% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{color}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{enumitem}
\usepackage{afterpage}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Exploring LLM-based Student Simulation for Metacognitive Cultivation}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

% \author{First Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\\And
%   Second Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\}

\author{
 \textbf{Haoxuan Li\textsuperscript{1}},
 \textbf{Jifan Yu\textsuperscript{1}},
 \textbf{Xin Cong\textsuperscript{1}},
 \textbf{Yang Dang\textsuperscript{1}},
\\
 \textbf{Yisi Zhan\textsuperscript{1}},
 \textbf{Huiqin Liu\textsuperscript{1}},
 \textbf{Zhiyuan Liu\textsuperscript{1}}
\\
 \textsuperscript{1}Tsinghua University
% \\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
}

\begin{document}
\maketitle
\begin{abstract}
Metacognitive education plays a crucial role in cultivating students' self-regulation and reflective thinking, providing essential support for those with learning difficulties through academic advising.
%Academic advising plays a crucial role in supporting students with learning difficulties by cultivating their metacognitive skills. 
Simulating students with insufficient learning capabilities using large language models offers a promising approach to refining pedagogical methods without ethical concerns. However, existing simulations often fail to authentically represent students' learning struggles and face challenges in evaluation due to the lack of reliable metrics and ethical constraints in data collection. To address these issues, we propose a pipeline for automatically generating and filtering high-quality simulated student agents. Our approach leverages a two-round automated scoring system validated by human experts and employs a score propagation module to obtain more consistent scores across the student graph. Experimental results demonstrate that our pipeline efficiently identifies high-quality student agents, and we discuss the traits that influence the simulation's effectiveness. By simulating students with varying degrees of learning difficulties, our work paves the way for broader applications in personalized learning and educational assessment.
\end{abstract}

\section{Introduction}

As is famously quoted by Joseph Addison \textit{"What sculpture is to a block of marble, education is to the human soul,"}, it emphasizes the transformative power of education in shaping individuals. Generations of educators have contributed to cultivating and "polishing" students' abilities to enhance their metacognitive skills, i.e., the skills about learning how to learn, rather than just transmitting specific knowledge \citep{tanner2012promoting}. Research indicates that well-developed metacognitive abilities are linked to improved learning outcomes \cite{schraw1995metacognitive, silver2023using}. To date, increasing attention has been devoted to their development beyond the implicit cultivation of these metacognitive skills in daily instructional activities. This has even led to the establishment of specialized \textbf{\textit{Academic Advising}} discipline, aiming at supporting students who encounter learning difficulties. However, this crucial domain continues to face an inherent challenge: while such interventions are beneficial, relying on real-world student participants to experiment with instructional methods risks unintended consequences. Such studies may inadvertently expose participants to psychological distress, privacy breaches, or academic disadvantages \citep{burns2008ethical, doody2018ethical}. Instead, simulating such "raw" students—who still require targeted support—can offer educators valuable insights into refining their pedagogical approaches without ethical concerns.

Large language models (LLMs) have recently been increasingly employed as human simulators in the social sciences \citep{gao2024large}. By replicating human behaviors \citep{qian2024chatdev, xu2023exploring, zhang2023exploring}, decision-making processes \citep{park2023choicemates}, and social interactions \citep{qian2024chatdev, chan2023chateval, hong2023metagpt}, LLM-based simulations enable researchers to conduct studies that would otherwise require extensive data collection or large-scale experiments \citep{wang2023recmind, zhang2024generative}. The success encourages a promising approach for studying academic advising by simulating student with LLM-based agents. Although some researchers have attempted to simulate students based on problem-solving behaviors \citep{ma2024students, lu2024generative} or dialogue interactions \citep{markel2023gpteach}, incorporating meta-cognitive abilities remains a challenge. 

\begin{figure*}[t]
  \includegraphics[width=\linewidth]{figs/Intro_fig_1_wide.pdf}
  \vspace{-11.5mm}
  \caption {Evaluation of Agent Student's Authenticity Using a Q\&A. We use colored words to present traits from the profile in the dialogue. The LLM Scorer identified some inconsistencies but deemed the agent's explanations reasonable. However, human experts concluded that the Agent's behavior deviated from the profile (colored in \textcolor{red}{red}).}
  \label{fig:Intro_fig}
  \vspace{-5.5mm}
\end{figure*}

Numerous technical obstacles need to be explored to effectively simulate "raw" students with insufficient learning capabilities. First, large language models (LLMs) are inherently optimized for generating correct answers and factual responses, making it difficult for them to exhibit genuine confusion or the typical struggles of real learners. Most existing student simulations are knowledge-based interactions, such as question answering in specific subjects, leaving aside critical aspects of learning strategies and metacognitive development. Consequently, these simulations often fail to represent students who experience genuine learning difficulties authentically.
Second, evaluating simulation realism remains resource-intensive. Current methods require substantial expert annotation to validate student authenticity. An alternative approach involves using LLMs to serve as judges or evaluators, yet this approach risks introducing biases. As shown in Figure \ref{fig:Intro_fig}, the LLM scorer detected certain inconsistencies, but found the agent's explanations plausible, while, on the contrary, human experts determined that the agent's behavior exhibited deviations from the profile.
Third, as mentioned above, collecting real-world data raises ethical concerns, making it impractical to evaluate or pre-train models by real data.


\begin{figure*}[t]
  \includegraphics[width=\linewidth]{figs/stu_sim_filter_framework_new.pdf}
  \caption {The pipeline automates the generation and selection of high-quality simulated student agents. It begins with random profile generation, followed by two rounds of automated scoring for profile and behavior consistency, partially validated by human experts. A graph module propagates scores across a student similarity graph constructed via a sentence encoder. Candidates are ranked and filtered based on propagated scores and finally selected by human experts through real academic advising test.}
  \label{fig:framework}
  \vspace{-5mm}
\end{figure*}

To address these challenges, we present a pipeline to automatically generate many simulated student agents and efficiently filter high-quality candidates, shown in Figure~\ref{fig:framework}. The pipeline begins by generating numerous random student profiles under minimal rule constraints. These profiles are then subjected to two rounds of automated scoring, assessing profile consistency and behavior consistency, and a subset of these scores is validated through limited human expert annotations. To fully leverage agent profile content and their latent relationships, we employ a graph neural network sub-module to propagate the scores across the student graph constructed by profile similarity embedded by a sentence encoder. Finally, candidates are ranked and filtered based on the propagated scores, resulting in a set of student agents with high consistency in profile content and alignment between behavior and profile. 
The contributions of our work are as follows,
\begin{itemize}[
  itemsep=0pt,      % 条目之间的行间距
  parsep=0pt,       % 条目与段落之间的间距
  topsep=0pt,       % 列表环境与上文之间的距离
]
    \item We extend LLM-based student simulation to the domain of metacognition, enabling the modeling of learning difficulties and providing empirical insights for personalized learning and pedagogical evaluation.
    \item We propose a novel pipeline for generating and filtering high-quality simulated students, reducing the reliance on human annotations.
    \item We conduct comprehensive experiments that demonstrate our pipeline's effectiveness and analyze key patterns influencing the quality of simulated students.
\end{itemize}


\section{Related Work}

Due to space limitations, we provide a more comprehensive discussion in the Appendix~\ref{app:related_work}.

\subsection{LLM-based Human Simulation}
LLMs have been increasingly used to simulate human behaviors across various domains, including social interactions \citep{gao2023s, qian2024chatdev}, economic decision-making \citep{horton2023large, zhao2023competeai}, and physical mobility \citep{jin2023surrealdriver, zou2023wireless}. Additionally, studies have explored hybrid models \citep{park2023choicemates, williams2023epidemic} and medical simulations \citep{du2024llms}. However, little work has been done on simulating students in academic advising.

\subsection{Student Simulation in Education}
LLMs have been leveraged to simulate students for educational purposes, such as training teaching assistants \citep{markel2023gpteach}, modeling student profiles \citep{lu2024generative}, and generating personality-driven student agents \citep{liu2024personality, jin2024teachtune, ma2024students}. However, simulating students with discrepancies in knowledge or learning skills remains unexplored.



\begin{figure}[t]
  \includegraphics[width=\columnwidth]{figs/student_profile.pdf}
  \caption{Student profiles include demographic information and other questionnaires related to studying. Detailed profile is provided in Appendix~\ref{app:profile}}
  \label{fig:student_profile}
  \vspace{-5mm}
\end{figure}
\section{Student Simulation Pipeline}
\subsection{Student profile Setting}
Simulating authentic "raw" students is pivotal for developing and evaluating academic tutoring, where direct collection of real-world data is impractical or ethically constrained. Therefore, we leverage prompt-based LLMs to simulate students with virtual profiles $P = \{p_1, p_2, \dots, p_N\}$ by random generation. As shown in Figure~\ref{fig:student_profile}, these profiles integrate (1) demographic information, (2) psychological traits derived from established frameworks (MBTI, Big Five \citep{de2000big}), (3) self-reported learning challenges (e.g., material understanding difficulties, anxiety), and (4) motivational factors including goal commitment and emotional states. The attribute selection aligns with guidelines \citep{gordon2011academic, folsom2015new} and empirical survey data from real educational contexts \citep{ma2024students,liu2024personality}, ensuring ecological validity while addressing ethical data collection constraints, providing a comprehensive representation of a real student’s characteristics.

However, randomly generated profiles may inherently contain internal contradictions and inconsistencies or exhibit low credibility, thereby undermining their utility for realistic simulations. For example, a profile might simultaneously indicate a high proficiency in mathematics while also describing a severe aversion to engaging in mathematical tasks.
We then introduce a pipeline designed to automatically generate a substantial number of simulated student agents and efficiently filter high-quality candidates. The pipeline initiates with the random generation of student profiles constrained by a minimal set of predefined rules, limiting the difference in Likert scale agreement of the same trait to no more than one point,
\begin{equation}
    \begin{aligned}
        Trait_i = (T_i^1, T_i^2, \dots, T_i^M), \\
        |T_i^k - T_i^\ell| \le d_{\max}, \forall k,\ell \in \mathcal{I},
    \end{aligned}
\end{equation}
where $\mathcal{I}\subseteq \{1,\dots,M\} $indexes traits subject to this difference limit, and $d_{\max}=1$. The profiles are then randomly sampled from this constrained space to avoid obvious contradictions.



\subsection{Two Round Scoring}

Subsequently, each profile undergoes two phases of automated scoring. The first scoring phase assesses profile consistency ($S_{p}$), wherein each profile is evaluated for internal logical coherence. The second phase examines behavioral consistency ($S_{b}$), ensuring the simulated behaviors align with the profile attributes.

\subsubsection{Profile Consistency Scoring}

This phase assesses the consistency of the profile ($S_{p}$), where a \textbf{Questioning Agent} $Q$ is introduced to identify potential contradictions or ambiguities by asking questions. 

Let $\mathcal{P} = \{p_1, p_2, \dots, p_N\}$ be the set of all student profiles under evaluation. Each profile $p_i$ includes demographic attributes and background information. $Q_i$ is the set of potential conflict points generated by a \textbf{Questioning Agent} $Q$ for $p_i$, and let $R_i$ be the set of corresponding responses the student agent provides.
A \textbf{Profile Scorer} $\rho_p$ then produces a consistency score $S_{p}(p_i)$ and an explanation $E_{p}(p_i)$ with a scoring instruction $\Theta_{p}$:
\begin{equation}
    S_{p}(p_i), E_{p}(p_i)= \rho_p\bigl(\Theta_{p};\, p_i,\, Q_i,\, R_i\bigr).
\end{equation}
A higher $S_{p}(p_i)$ indicates stronger internal coherence, implying that $R_i$ resolves or justifies more conflicts within $p_i$.

\subsubsection{Behavioral Consistency Scoring}

In the second phase, we evaluate behavioral consistency ($S_{b}$) to ensure that the behaviors exhibited by each student agent during interactions are consistent with profile attributes. 

We engaged each agent in a conversation with a \textbf{Dialogue Agent}, where the \textbf{Student Agent} is prompted with various open-ended topics, subtly encouraging the student to reveal information about their profile, traits, and learning strategies. 
Let $D_i$ be the set of conversation turns recorded for $p_i$. A \textbf{Behavior Scorer} $\rho_b$ then produces a behavior consistency score $S_{b}(p_i)$ and an explanation $E_{b}(p_i)$ with a scoring instruction $Theta_{b}$: \begin{equation} 
    S_{b}(p_i), E_{b}(p_i) = \rho_b\bigl(\Theta_{b};, p_i, D_i\bigr).
\end{equation}
A higher $S_{b}(p_i)$ signifies greater alignment between the agent’s interactive behavior and the attributes defined in its profile $p_i$. In practice, we set the turns of conversations to 15.

\begin{table*}[t]
    \centering
    \small
    \caption{Overall performance of the agent ranking pipeline under various scoring strategies. We compare the initial and propagated phases using profile score ($S_{p}$), behavior score ($S_{b}$), and their average, evaluated with Precision@K, NDCG@K, and Pairwise Accuracy for $K=5$ and $K=|\mathcal{C}|$.}
    \label{tab:overall_performance}
    \begin{tabular}{llcccccc}
    \toprule
    \multirow{2}{*}{\textbf{Score}} & \multirow{2}{*}{\textbf{Phase}} 
        & \multicolumn{3}{c}{\textbf{K=5}} 
        & \multicolumn{3}{c}{\textbf{K=}$|\mathcal{C}|$} \\
    \cmidrule(lr){3-5}\cmidrule(lr){6-8}
    & & \textbf{Prec.} & \textbf{NDCG} & \textbf{Pair Acc.}
      & \textbf{Prec.} & \textbf{NDCG} & \textbf{Pair Acc.} \\
    \midrule
    \multirow{2}{*}{$S_{p}$} 
        & Init & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0634 \\
        & Prop & 0.2 & 0.3664 & 0.1 & 0.4615 & 0.3695 & 0.0771 \\
    \midrule
    \multirow{2}{*}{$S_{b}$}
        & Init & 0.0 & 0.0787 & 0.3 & 0.1538 & 0.0999 & 0.0559 \\
        & Prop & 0.4 & 0.4200 & 0.2 & 0.4615 & 0.3858 & 0.0777 \\
    \midrule
    \multirow{2}{*}{$\mathrm{Avg}(S_{p}, S_{b})$}
        & Init & 0.0 & 0.0 & 0.0 & 0.2308 & 0.1021 & 0.0670 \\
        & Prop & 0.4 & 0.4200 & 0.2 & 0.4615 & 0.3866 & 0.0775 \\
    \bottomrule
    \end{tabular}
    \vspace{-5mm}
\end{table*}

\subsection{Graph-Based Score Propagation}

To effectively capture the semantic content of individual profiles and their latent interrelationships, we employ a graph neural network module \citep{you2020design} to propagate scores across a similarity-based graph structure.
We first employ a sentence encoder $\phi$ to derive normalized embeddings $\mathbf{e}_i$ for each profile $p_i$. 
\begin{equation}
    \phi: \mathcal{P} \rightarrow \mathbb{R}^d,\quad  \mathbf{u}_i = \phi(p_i), \mathbf{e}_i = \frac{\mathbf{u}_i}{\|\mathbf{u}_i\|_2}
\end{equation}
These embeddings facilitate the construction of a student similarity graph $G=(V,E)$, where each node corresponds to a profile, and edges $E $ are established based on a similarity threshold $\theta$. For simplicity, we employs bge-m3 \citep{chen2024bge}, one of the state-of-art encoders among benchmarks.
The adjacency matrix is formulated as,
\begin{equation}
    A_{ij} =  \begin{cases} 1, & \text{if } (p_i,p_j)\in E \\ 0, & \text{otherwise} \end{cases}
\end{equation}
Leveraging this graph structure, we integrate a propagation module to spread consistency scores across the network. 
Specifically, the initial scores $\mathbf{S}^{(0)}$ are iteratively updated as follows,
\begin{equation}
    \begin{aligned}
        \mathbf{S}^{(0)} & = [S_1, S_2, \dots, S_N]^T \\
        \tilde{\mathbf{A}} & = \mathbf{D}^{-\frac{1}{2}} \, \mathbf{A} \, \mathbf{D}^{-\frac{1}{2}}, \\
        \mathbf{S}^{(k+1)} & = \alpha \,\tilde{\mathbf{A}} \, \mathbf{S}^{(k)} + (1-\alpha)\,\mathbf{S}^{(0)},
    \end{aligned}
\end{equation}
where $\mathbf{D}$ denotes the diagonal matrix of $\mathbf{A}$. $k$ denotes the iteration step. We set $\alpha$ to 0.5 for simplification. This propagation allows for the refinement of scores by considering the influence of similar profiles within the graph.


\subsection{Interactive Test}

After score propagation, profiles are ranked and filtered based on their propagated scores $\mathbf{S}^{(K)}_p, \mathbf{S}^{(K)}_b$ after $K$ iterations, resulting in a curated set of candidate students exhibiting high consistency in both profile content and behavioral alignment. In practice, we constrain this curated set to those student agents $\mathcal{C}$ whose scores are higher than 8 out of 10. 

Finally, we conduct the interactive evaluation, engaging candidate student agents in multi-turn dialogues with academic advising experts. Each expert conducts a minimum of 15 conversational turns with an assigned agent while referencing its predefined profile. Annotators score profile conformity (1-100 scale) and provide justifications based on observed interactions. The annotation interface integrates agent profiles, real-time chat functionality, and evaluation metrics. Tutors initiate sessions by diagnosing academic challenges through open-ended questioning, then progressively apply intervention strategies from the academic support guidelines from guide books \citep{gordon2011academic, folsom2015new}, and finally evaluate the performance of student agents. More detailed information of experts and annotation instructions are provided in Appendix~\ref{app:Experimental-Settings}.

The whole pipeline ensures that the generated student agents not only adhere to logical consistency within their profiles but also exhibit behaviors that are coherent and aligned with their defined attributes, thereby facilitating effective simulations in educational contexts.

\vspace{-1.5mm}
\section{Experiments}
\vspace{-1.5mm}

We conducted a series of experiments and formulated several research questions to validate the proposed pipeline for selecting simulated student agents, investigate the roles of individual modules within the pipeline, and identify noteworthy phenomena during the selection process.
We provided detailed experimental settings and configurations of agents in Appendix~\ref{app:Experimental-Settings}.
% \begin{itemize}[
%   label={},         % 去掉前面的黑色圆点
%   itemsep=0pt,      % 条目之间的行间距
%   parsep=0pt,       % 条目与段落之间的间距
%   topsep=0pt,       % 列表环境与上文之间的距离
%   leftmargin=0pt    % 列表左侧不留缩进
% ]
%     \item \textbf{RQ1: }\textit{What is the discrepancy between the rankings of agents selected through the pipeline and those ranked by experts in the interactive test?}
%     \item \textbf{RQ2: }\textit{To what extent does the score propagation correct the initial scores?}
%     \item \textbf{RQ3: }\textit{Which trait in the profile remains after process by our pipeline?}
%     \item \textbf{RQ4: }\textit{How important are various traits on the final scoring?}
%     \item \textbf{RQ5: }\textit{Which trait does the LLM prioritize or overlook when evaluating the quality of simulations?}
% \end{itemize}

\begin{figure*}[t]
  \centering
  \begin{minipage}{\textwidth}
    \includegraphics[width=\textwidth]{figs/generated_profile_distribution.pdf}
  \end{minipage}
  
  \begin{minipage}{\textwidth}
    \includegraphics[width=\textwidth]{figs/candidate_profile_distribution.pdf}
  \end{minipage}
  \caption{The figure illustrates the distribution of key profile features of student agents before and after the pipeline filtering process. "BF-TC" represents the Big Five personality traits (O: Openness, C: Conscientiousness, E: Extraversion, A: Agreeableness, and N: Neuroticism). Other questionnaire items are labeled by their respective question numbers. The Four Trait Questionnaire subscales are distributed as follows: top-left (goal commitment), bottom-left (motivation), top-right (self-efficacy), and bottom-right (self-regulation).}
  \label{fig:profile_distribution}
\end{figure*}

\subsection{Pipeline Performances}
\textbf{RQ1: }\textit{How does the ranking of agents selected through the full pipeline compare to expert rankings in the interactive test?}

To analyze the extent to which the pipeline effectively selects agents aligned with expert preferences, we examine rankings derived from different scoring phases at various stages of the pipeline.
%To assess the discrepancy between the pipeline-based agent rankings and those provided by human experts in the interactive test, we rank agents using different scoring at distinct pipeline stages. 
In particular, we consider the profile score, the behavior score, and their average, evaluated at both the initial ($S_{p}, S_b$) and propagated phases ($S^{(K)}_p, S^{(K)}_b$). The rankings are compared using three metrics—Precision@K, NDCG@K, and Pairwise Accuracy—with $K$ set to 5 and to the total number of agents selected ($|\mathcal{C}|$). For all metrics, the higher the value, the more aligned to the human preferences. We elaborate on the metrics in Appendix~\ref{app:overall_performance_metrics}.

Table~\ref{tab:overall_performance} reports the ranking performance using different scores throughout the pipeline. The results reveal that scores computed in the initial scoring stage yield poor filtering performance across all metrics. In contrast, the average of $S_{p}$ and $S_{b}$ consistently improves the selection quality. Moreover, despite propagation improving top-K precision and NDCG, pairwise accuracy shows minimal gains (< 0.02 absolute improvement) between phases. It indicates that while propagation enhances absolute ranking positions, it preserves most relative agent pairwise relationships established during the two round scoring phase. The marginal improvement implies limited success in correcting initial rankings of pairwise agents through propagation.


\begin{figure}[t]
  \includegraphics[width=\columnwidth]{figs/Initial_vs_Propagated_vs_Human_violin.pdf}
  \caption{The distributions of the initial (\textcolor[RGB]{72,161,133}{green}), propagated (\textcolor[RGB]{216,110,69}{orange}), and human scores (\textcolor[RGB]{110,128,169}{blue}). Propagated scores are more aligned with that of human scores.}
\label{fig:Initial_vs_Propagated_vs_Human_violin}
\vspace{-5mm}
\end{figure}


\subsection{Score Propagation Validation}
\textbf{RQ2: }\textit{To what extent does the score propagation correct the initial scores?}

To validate the efficacy of the graph-based score propagation mechanism, we perform comparative analyses between the raw automated scores, the propagated scores, and the average human expert scores. Formally, let $\bar{S}_{\text{expert}}(p_i)$ denote the average score assigned by human experts for the student agent with profile $p_i$. 

As illustrated in Figure \ref{fig:Initial_vs_Propagated_vs_Human_violin}, we present the distribution of scores for all simulated student agents in both the initial and propagated stages, along with the distribution of human expert scores using violin plots. The score distribution after propagation aligns more closely with the human experts' score distribution. Notably, the propagated profile scores exhibit a top-heavy distribution mirroring that of human annotations, whereas human behavior scores display an inverse bottom-heavy pattern. This divergence suggests that the propagation module helps the pipeline better align with human experts on profile consistency evaluation than behavior consistency evaluation.

We then quantitatively measure the differences between $S^{(K)}(p_i)$ and the average experts' scores $\bar{S}_{\text{expert}}(p_i)$ using Mean Absolute Error (MAE).
% We measure the proximity of $S^{(K)}(p_i)$ to $\bar{S}_{\text{expert}}(p_i)$ using Mean Absolute Error (MAE):
% \begin{equation}
%     \text{MAE} = \frac{1}{N} \sum_{i=1}^{N} \lvert S^{(K)}(p_i) - \bar{S}_{\text{expert}}(p_i) \rvert
% \end{equation}
The results in Table \ref{tab:MAE_comparison} also demonstrate that the propagated scores have a reduced MAE with expert scores compared to the initial automated scores, thereby validating the effectiveness of the graph-based propagation mechanism.
Specifically, the profile MAE $\Delta_{profile}$ decreases from 1.007 to 0.6988, reflecting a 30.63\% improvement. Similarly, the behavior MAE $\Delta_{behavior}$ reduces from 1.6942 to 0.8453 with a 50.11\% improvement. These findings highlight the complementary strengths of LLMs and graph-based propagation. While LLMs provide assessments for explicit profile features, they require structural regularization through relational graphs to handle implicit relationships.

\begin{table}[t]
\centering
\caption{Mean Absolute Error comparison of initial and propagated scores compared to human scores.}
\label{tab:MAE_comparison}
\begin{tabular}{c|c|c|c}
\hline
MAE & Init & Propagated & Improv \\ \hline
$\Delta_{profile}$ & 1.007 & 0.6988 & +30.63\% \\
$\Delta_{behavior}$ & 1.6942 & 0.8453 & +50.11\% \\ \hline
\end{tabular}
\vspace{-5mm}
\end{table}

\subsection{Agent Profile Distribution}
\textbf{RQ3: }\textit{How does the distribution of profile traits shift throughout the pipeline?}

Figure~\ref{fig:profile_distribution} illustrates the distribution of key profile features of student agents before and after the pipeline filtering process. The initial distribution appears uniform, with minor differences across features. However, most feature distributions exhibit noticeable shifts in the final selected agents.

Among the Big Five personality traits, agents with low extraversion and high agreeableness are more prevalent in the final selection. It is because high extraversion often indicates strong energy and positivity, which may not align well with typical characteristics of students experiencing learning difficulties. In contrast, high agreeableness is more frequently observed, as agents tend to exhibit affirmative responses, making agents with high agreeableness more likely to receive higher alignment scores.
For the study questionnaire, q2 (\textit{Do you have trouble taking tests or completing assignments?}) shows a significantly higher proportion of "Yes" responses in the selected agents. It suggests that LLM-based student simulations better capture students with such difficulties, aligning with real-world academic advising practices where these challenges are common.
Notably, in the four trait questionnaire, the proportion of agents with strong self-efficacy and self-regulation increases substantially after filtering. It is because agents inherently display high self-efficacy and structured planning abilities in conversations, even though these traits may not typically align with students facing learning difficulties. Nevertheless, such agents still achieve high scores in behavior consistency evaluations.

\begin{figure}[t]
  \includegraphics[width=\columnwidth]{figs/feature_importance.pdf}
  \caption{Relative importance ranking of traits in profile, with those under the same level categorized by colors.}
  \label{fig:feature_importance}
  \vspace{-5mm}
\end{figure}

% \begin{figure*}[t]
%   \includegraphics[width=\textwidth]{figs/good case.pdf}
%   \caption{Examples of LLM scoring good cases for agent profile and behavior. Colored text highlights corresponding features in the profile. Highlighted text indicates the agent's deviation from the prompt instruction.}
%   \label{fig:good_case}
% \end{figure*}

\begin{figure*}[t]
  \centering
  \begin{minipage}{\textwidth}
    \includegraphics[width=\textwidth]{figs/good_case.pdf}
  \end{minipage}
  \vspace{-5mm}
  \begin{minipage}{\textwidth}
    \includegraphics[width=\textwidth]{figs/bad_case.pdf}
  \end{minipage}
  \caption{Examples of LLM scoring cases for agent profile and behavior: good case (top) and bad case (bottom). Colored text highlights corresponding features in the profile.}
  \label{fig:cases}
  \vspace{-5mm}
\end{figure*}


\subsection{Influence of Features in Profile}
\textbf{RQ4: }\textit{How important are the traits to final scores?}

To investigate the influence of the features in the agent profile on the final profile and behavior scores, we performed a hot encoding of the features of the profile and used a Random Forest model \citep{breiman2001random} to fit the propagated profile score and behavior score, where the size of test sets is 20\% and the number of trees is 100. We then ranked the features based on their relative importance\citep{archer2008empirical}, where the relative importance of each feature is calculated by dividing its importance score by the highest among all. 

As shown in Figure \ref{fig:feature_importance}, the profile features are categorized into five groups, with \textit{BF TC} containing both high-low value-based features and their corresponding descriptive features.
\textbf{Age} and \textbf{MBTI} personality type from the \textit{Basic Information} category have a significant impact on the scores. It indicates that both profile and behavior scorer pay more attention to these two traits when scoring the agents. Additionally, the overall influence of the \textit{Four Trait Questionnaire} is relatively high, with similar importance observed for the descriptive features of \textit{BF TC}. Compared to value-based features, descriptive features provide more detailed information to the scorer, aiding in assessing of the agent's consistency.
Notably, even the most important feature \textbf{Age} has a relatively low importance score of 0.0651 and 0.0644 for the profile and behavior scores, respectively. It suggests that individual features have limited influence on the final scores, and the interaction between multiple features plays a more critical and subtle role in determining the outcomes.


\subsection{Case Study}
\textbf{RQ5: }\textit{Which trait does the LLM prioritize or overlook when evaluating the quality of simulations?}

We showcase LLM scoring examples in Figure~\ref{fig:cases}, comparing a \textit{good case} and a \textit{bad case} in profile and behavior evaluation, accompanied by expert scores, agreement levels, and explanations. In the \textbf{good case} (top), the questioning agent $Q$ detects an inconsistency in the student's profile—17 years old while enrolled as a third-year Master's student. The student agent fails to provide a compelling justification for this contradiction. Additionally, the LLM accurately assesses the student agent's self-reported academic pressure, where the reasoning behind the discrepancy between daily and long-term pressure is somewhat reasonable. Regarding behavior evaluation, the scorer correctly identifies that the student agent violates the role of being a student who need academic advising. It demonstrates the model's sensitivity to instruction following and reveals its occasional struggle with adherence to long instructions.
In contrast, the \textbf{bad case} (bottom) illustrates a scenario where the LLM's evaluation deviates significantly from human judgment. The agent's profile presents conflicting traits, such as experiencing stress while reporting no difficulty in tests and assignments. The LLM questions these inconsistencies but assigns a relatively low score due to its strict conflict detection. However, expert annotations suggest that such contradictions are not inherently erroneous, but reflect common real-world cases where students manage stress effectively despite academic challenges. Furthermore, the LLM scorer underperforms behavior evaluation by misinterpreting the agent's conscientiousness and self-efficacy, leading to lower agreement scores with human experts. The results indicate that while LLM scorers is adept at identifying explicit conflicts, it may over-penalize cases where nuanced human explanations exist.

These findings suggest that the LLM tends to overemphasize explicit contradictions while occasionally overlooking the complexity of real-world student behaviors, reflected in certain cases' lower agreement scores with human experts. Future improvements should focus on enhancing the model's reasoning consistency and refining its instruction-following capabilities to align more closely with expert evaluations.

\section{Conclusion}
We propose a pipeline for automatically generating and filtering high-quality simulated student agents to improve the authenticity of learning difficulty simulations. Our approach contains a two-round automated scoring phase, validated by human experts to ensure the profile consistency and behavioral alignment of agents. We then employ a score propagation module to enhance consistency across the student graph. Experimental results show that our method effectively identifies high-quality agents, advancing the use of simulated students for broader educational applications.


\clearpage
\newpage
\section*{Limitations}
Our work explores the potential of automatically generating and filtering high-quality simulated student agents to enhance the authenticity of learning difficulty simulations. However, researchers and practitioners must consider several limitations and risks.
First, our approach focuses on prompt-based student simulation, particularly extending the scope to metacognition. We do not explore more complex agent simulation frameworks like workflow-based or retrieval-augmented generation (RAG) simulations. These approaches will be explored in future work to improve simulation fidelity further.
Second, some studies evaluate LLM-based human simulation by Turing tests \citep{aher2023using}, measuring the model's success rate in mimicking human-like responses. Such evaluations emphasize linguistic style. Our work prioritizes metacognitive abilities and learning limitations in simulation rather than stylistic authenticity. We explicitly addressed this distinction during expert annotation to ensure the evaluation remains aligned with our research objectives.
Third, our approach assumes that structured profiles can effectively capture student attributes and learning difficulties. However, real-world learning behaviors are highly dynamic and context-dependent. Future work should explore adaptive modeling techniques that better reflect the evolving nature of student cognition and engagement.

Additionally, ethical risks must be considered. The simulated student agents are generated based on training data, which may introduce biases, stereotypes, or unintended inaccuracies in student representations. These biases could affect the fairness and applicability of educational interventions. Researchers must remain vigilant in mitigating such risks through continuous evaluation, bias detection, and responsible deployment of simulated agents.

\bibliography{preprint}

\appendix

% \section{Example Appendix}
% \label{sec:appendix}

% This is an appendix.


\newpage
\section{Related Work}
\label{app:related_work}

\subsection{LLM-based Human Simulation}
Recent studies on human simulation have increasingly leveraged large language models (LLMs) to emulate complex behaviors across social, economic, and physical domains. In the social realm, \citet{gao2023s} and \citet{qian2024chatdev} demonstrated how LLM agents replicate interpersonal interactions and collaboration, enabling realistic group dynamics. From an economic perspective, \citet{horton2023large} and \citet{zhao2023competeai} showed that LLM-based simulations can capture market behaviors and decision-making patterns aligned with established economic theories. In the physical domain, \citet{jin2023surrealdriver} and \citet{zou2023wireless} employed LLM agents to model mobility and driving behaviors in virtual environments, highlighting human-like performance and adaptability. Hybrid approaches further integrate these dimensions, as illustrated by \citet{park2023choicemates} and \citet{williams2023epidemic}, showcasing the potential of LLM-driven simulations to generate intricate, multifaceted human-like behavior. In the medical context, \citet{du2024llms} introduced EvoPatient, an agent coevolution framework that models simulated patients and doctor agents engaging in multi-turn dialogues. Similar to simulating patients, academic advising, where an institutional representative provides guidance to a college student on academic, social, or personal matters \citep{kuhn2008historical}, helping students maximize their college experience \citep{miller2011structuring}. However, to the best of our knowledge, little work explores simulating students in academic advising using LLM.


% \subsection{Academic advising}
% Academic advising refers to situations where an institutional representative provides guidance to a college student on academic, social, or personal matters [1], helping students maximize their college experience [2]. Despite being complex and time-consuming, academic advising plays a crucial role, by Assiri et al. [3] highlighted its importance in enhancing student performance. With the development of information systems and conversational agents, many studies have integrated these technologies into academic advising, making it more thorough and comprehensive by considering various aspects of a student's educational journey [4].
% Wei et al. [5] developed a conversational agent to help students select academic subjects based on their capabilities and preferences. Bilquise and Shaalan [6] proposed a chatbot for academic advising that uses student data to recommend courses and inform advisors or students about potential academic risks. Bilquise et al. [7] created a conversational academic advisor that can answer questions about regulations and educational policies in multiple languages. Sweidan et al. [8] built a system to track students' learning progress and assist with course selection based on performance and owed courses. Kuhail et al. [9] implemented a chatbot that responds to questions about regulations and offers general support for academic issues.

% \subsection{Metacognitive capability meets language models}
% Language models have shown certain metacognitive abilities as they advance in capability. Some studies focus on evaluating the metacognitive capabilities of LLMs. [1] compares the metacognitive abilities of humans and LLMs in a simulated International Coaching Federation (ICF) exam, using scenario-based questions with best and worst response options. [2] defines and analyses declarative and procedural knowledge in LLMs, using hints derived from one type of knowledge to evaluate their impact on task performance. It demonstrates how pretraining affects the model's ability to utilize these knowledge types. [7] introduces metacognitive myopia as a cognitive-ecological framework to explain and address biases in Large Language Models (LLMs). It identifies five symptoms of metacognitive myopia and suggests incorporating metacognitive regulatory processes into LLMs to mitigate biases and improve human-machine interactions.
% Other research explores leveraging metacognitive theories to enhance LLM learning and reasoning. [3] examines whether LLMs possess metacognitive knowledge by tagging mathematical problems with skill labels of varying granularity, and whether retrieving similar problems based on identified skills improves the model’s performance. [4] [5] [6]
% Despite the success mentioned above, academic advising revolves around modeling and enhancing learners’ metacognitive abilities, where research on using LLMs for this purpose is still nascent. Current work focuses primarily on modeling human cognitive abilities (e.g., knowledge tracing) or using LLMs to support knowledge acquisition, with limited exploration of developing higher-order metacognitive skills.

\subsection{Student simulation in Education}
The emergence of LLMs has spurred extensive research on simulat at the individual level\citep{yu2024mooc, zhang2024simulating}. \citet{markel2023gpteach} uses LLMs to simulate students to train teaching assistants (TAs) for office hours. It categorizes TA-student interactions, performs A/B testing to evaluate system effectiveness, and addresses academic and personal issues like study methods and stress management. \citet{lu2024generative} models student profiles by simulating their responses to multiple-choice questions, predicting their performance, and evaluating their conceptual understanding. It highlights how LLM-based simulations can assist teachers in assessing the quality of their test items. \citet{liu2024personality} focuses on creating virtual students with diverse personality types (e.g., the Big Five or MBTI) to train teachers’ pedagogical skills. \citet{jin2024teachtune} evaluates teaching agents by simulating students with varying traits and knowledge levels, then generating conversations to assess agent responses. Its features resemble debugging tools, enabling real-time agent performance analysis across student profiles. \citet{ma2024students} constructs conversational datasets based on real teacher-student interactions, fine-tuning LLMs to simulate diverse student types in language learning. The virtual students underwent a Turing test, where some personalities closely resembled real humans, benefiting from high-quality, real-student data for fine-tuning. Despite the success they have achieved, simulating students with discrepancies in knowledge or learning skills remains unexplored.



\section{Experimental Settings}
\label{app:Experimental-Settings}

\subsection{Settings}
\subsubsection{Configurations}
We set all student agents to use \texttt{GPT-4o} as their backbone and scoring models. All API calls use the default official parameter settings\footnote{https://platform.openai.com/docs/api-reference/introduction}. We release the code and prompts for reproducibility~\footnote{https://anonymous.4open.science/r/student-simulation-0886}.


\subsubsection{Annotation Settings}
To ensure the reliability of automated scoring and to facilitate interactive testing for simulated students, we recruited a team of experienced experts with strong backgrounds in education from top universities in China. Each expert had at least one semester of experience as a teaching assistant or academic tutor and was compensated at a rate of 60 RMB/hour. All annotations and interactive tests were conducted in English.

In the \emph{profile score} and \emph{behavior score} annotation tasks, we first presented annotators with the entire interaction between the student agent and the scorer, along with the student agent’s profile information. Following the same instructions used by the LLM scorer, annotators evaluated the consistency of the agent’s profile and behavior. After providing their ratings, annotators were shown the LLM scorer’s results and explanations. They then indicated their level of agreement with the LLM scorer’s assessments.

\subsection{Pipeline Performance Metrics}
\label{app:overall_performance_metrics}

The evaluation of ranking performance relies on three widely used metrics: Precision@K, Normalized Discounted Cumulative Gain at K (NDCG@K), and Pairwise Accuracy. These metrics capture different aspects of ranking quality, ensuring a comprehensive assessment of the proposed approach.

Precision@K measures the proportion of relevant agents among the top $K$ positions in the ranking. Formally, it is computed as: 
\begin{equation} 
\text{Precision@K} = \frac{|{r_i \in R_K \cap G}|}{K}, \end{equation} 
where $R_K$ denotes the set of top $K$ agents according to the pipeline ranking, and $G$ represents the set of agents deemed relevant based on expert annotations. A higher Precision@K indicates a greater concentration of relevant agents in the top-ranked positions.

NDCG@K extends the evaluation by incorporating graded relevance, ensuring that highly relevant agents are given more weight when positioned near the top of the ranking. It is defined as: 
\begin{equation} 
\text{NDCG@K} = \frac{1}{\text{IDCG@K}} \sum_{i=1}^{K} \frac{2^{\text{rel}(r_i)} - 1}{\log_2 (i+1)}, \end{equation} 
where $\text{rel}(r_i)$ denotes the relevance score of agent $r_i$, and $\text{IDCG@K}$ represents the maximum possible DCG@K value obtained from an ideal ranking where all agents are perfectly ordered by their relevance scores. The logarithmic discount factor penalizes lower-ranked relevant agents, emphasizing the importance of placing highly relevant agents at the top of the ranking. Higher NDCG@K values indicate rankings that more closely align with the ideal ordering.

Pairwise Accuracy evaluates ranking correctness at the level of agent pairs by comparing the relative ordering produced by the system against the ground-truth order determined by expert annotations. It is given by: \begin{equation} 
\text{Pair Acc.} = \frac{\sum_{(i,j)\in P} \mathbf{1}\big[(r_i - r_j)(g_i - g_j) > 0\big]}{|P|}, \end{equation} 
where $P$ represents the set of all possible agent pairs, and $\mathbf{1}(\cdot)$ is an indicator function that returns one if the system correctly preserves the relative ordering of a pair and zero otherwise. A higher pairwise accuracy indicates better agreement between the system and expert-annotated rankings.

Together, these metrics provide a comprehensive evaluation of ranking quality. Precision@K focuses on binary relevance at a fixed cutoff, NDCG@K captures position-sensitive graded relevance, and Pairwise Accuracy measures local ranking consistency.

\section{Profile Settings}
\label{app:profile}

\begin{figure}[t]
  \includegraphics[width=\columnwidth]{figs/app_student_profile.pdf}
  \caption{Student profile includes demographic information and other studying-related questionnaires.}
  \label{app:student_profile}
\end{figure}

The virtual student profiles are constructed through four demographic attributes and two psychological assessments, as visualized in Figure~\ref{app:student_profile}. Demographic components include:

\begin{itemize}
[
  itemsep=0pt,      % 条目之间的行间距
  parsep=0pt,       % 条目与段落之间的间距
  topsep=0pt,       % 列表环境与上文之间的距离
]
    \item \textbf{Basic Attributes}: Gender, age (17-28 years covering typical undergraduate/graduate student ranges), academic major (62 most prevalent disciplines across Science, Engineering, Social Science and Arts), and academic standing (freshman to third-year master).
    \item \textbf{Psychological Profiles}: MBTI (16 personality types) and Big Five trait compositions with the corresponding descriptions.
\end{itemize}

Four multi-dimensional questionnaires capture learning characteristics:
\begin{itemize}
[
  itemsep=0pt,      % 条目之间的行间距
  parsep=0pt,       % 条目与段落之间的间距
  topsep=0pt,       % 列表环境与上文之间的距离
]
    \item \textbf{Learning Traits}: 12 Likert-scale items (1-5 agreement) measuring self-regulation, motivation, engagement, and information processing styles. Each dimension contains three semantically equivalent questions for response consistency verification.
    \item \textbf{Academic Challenges}: 7 common learning obstacles across four domains: cognitive (e.g., material understanding), metacognitive (e.g., learning strategies), social (e.g., seeking for help), and affective (e.g., stress management). 
\end{itemize}


\end{document}
