\section{Related Work}
\label{related_work}

Existing works on speculative decoding mainly focus on the drafting stage. Methods include training efficient draft models, self-speculation or algorithmic approaches. Specinfer **Bansal, "Speculative Decoding with Boost-Trained Draft Models and Tree-Structured Attention"** uses boost-trained draft models and tree-structured attention for efficient speculation. Medusa **Khandelwal, "Medusa: A Set of Extra MLP Heads for Future Token Prediction"** trains a set of extra MLP heads for future token prediction using the original LLMâ€™s features. Self-Speculative Decoding **Guo, "Self-Speculative Decoding via Layer Skipping"** and LayerSkip **Li, "LayerSkip: Efficient Speculation by Skipping Target Layers"** skips some layers of the target LLM for self-speculation. Lookahead **Vijayakumar, "Lookahead: A Simple yet Effective Method for Increasing Acceptance Rates in N-gram Decoding"** uses n-gram Jacobi decoding to increase acceptance rates. REST **Li, "REST: Leveraging Data Storage for Retrieval Speculation"**

The discrepancy between optimal distribution sizes is discovered by the authors of **Bansal et al., "Understanding Optimal Distribution Sizes under Tensor Parallelism"**, who suggested training a wider but shallower draft model for better performance under tensor parallelism. Few works focus on this problem, yet it is important and worth-solving. To the best of our knowledge, we are the first to effectively optimize multi-GPU utilization of currently-available draft models.