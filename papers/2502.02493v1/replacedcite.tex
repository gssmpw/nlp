\section{Related Work}
\label{related_work}

Existing works on speculative decoding mainly focus on the drafting stage. Methods include training efficient draft models, self-speculation or algorithmic approaches. Specinfer ____ uses boost-trained draft models and tree-structured attention for efficient speculation. Medusa ____ trains a set of extra MLP heads for future token prediction using the original LLMâ€™s features. Self-Speculative Decoding ____ and LayerSkip ____ skips some layers of the target LLM for self-speculation. Lookahead ____ uses n-gram Jacobi decoding to increase acceptance rates. REST ____ leverages a data storage for retrieval speculation.

The discrepancy between optimal distribution sizes is discovered by the authors of ____, who suggested training a wider but shallower draft model for better performance under tensor parallelism. Few works focus on this problem, yet it is important and worth-solving. To the best of our knowledge, we are the first to effectively optimize multi-GPU utilization of currently-available draft models.