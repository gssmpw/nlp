\section{Related Work}
\label{related_work}

Existing works on speculative decoding mainly focus on the drafting stage. Methods include training efficient draft models, self-speculation or algorithmic approaches. Specinfer \cite{miao2024specinfer} uses boost-trained draft models and tree-structured attention for efficient speculation. Medusa \cite{cai2024medusa} trains a set of extra MLP heads for future token prediction using the original LLMâ€™s features. Self-Speculative Decoding \cite{zhang2023draft} and LayerSkip \cite{elhoushi2024layer} skips some layers of the target LLM for self-speculation. Lookahead \cite{fubreak} uses n-gram Jacobi decoding to increase acceptance rates. REST \cite{he2023rest} leverages a data storage for retrieval speculation.

The discrepancy between optimal distribution sizes is discovered by the authors of \cite{chen2023accelerating}, who suggested training a wider but shallower draft model for better performance under tensor parallelism. Few works focus on this problem, yet it is important and worth-solving. To the best of our knowledge, we are the first to effectively optimize multi-GPU utilization of currently-available draft models.