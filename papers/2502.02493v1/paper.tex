\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{multirow}

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{submit2025} with \usepackage[nohyperref]{submit2025} above.
\usepackage{hyperref}

% self added packages
\usepackage{tabularx}
\usepackage{makecell}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{submit2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{submit2025}
% \usepackage{submit2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \submittitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\submittitlerunning{EasySpec: Layer-Parallel Speculative Decoding for Efficient Multi-GPU Utilization}

\begin{document}

\twocolumn[
\submittitle{EasySpec: Layer-Parallel Speculative Decoding for Efficient Multi-GPU Utilization}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the submit2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\submitsetsymbol{equal}{*}

\begin{submitauthorlist}
\submitauthor{Yize Wu}{iscas,ucas}
\submitauthor{Ke Gao}{iscas}
\submitauthor{Yanjun Wu}{iscas}

\end{submitauthorlist}

\submitaffiliation{iscas}{Institute of Software Chinese Academy of Sciences, Beijing, China}
\submitaffiliation{ucas}{University of Chinese Academy of Sciences, Beijing, China}

\submitcorrespondingauthor{Yanjun Wu}{yanjun@iscas.ac.cn}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\submitkeywords{Large Language Models, speculative decoding, distributed inference}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \submitEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
%\printAffiliationsAndNotice{\submitEqualContribution} % otherwise use the standard text.

\begin{abstract}
Speculative decoding is an effective and lossless method for Large Language Model (LLM) inference acceleration. It employs a smaller model to generate a draft token sequence, which is then verified by the original base model. In multi-GPU systems, inference latency can be further reduced through tensor parallelism (TP), while the optimal TP size of the draft model is typically smaller than that of the base model, leading to GPU idling during the drafting stage. To solve this problem, we propose EasySpec, a layer-parallel speculation strategy that optimizes the efficiency of multi-GPU utilization. EasySpec breaks the sequential execution order of layers in the drafting model, enabling multi-layer parallelization across devices, albeit with some induced approximation errors. After each drafting-and-verification iteration, the draft model’s key-value (KV) cache is calibrated in a single forward pass, preventing long-term error accumulation at minimal additional latency. We evaluated EasySpec on several mainstream open-source LLMs, using smaller versions of models from the same series as drafters. The results demonstrate that EasySpec can achieve a peak speedup of 4.17x compared to vanilla decoding, while preserving the original distribution of the base LLMs. Specifically, the drafting stage can be accelerated by up to 1.62x with a maximum accuracy drop of only 7\%, requiring no training or fine-tuning on the draft models.
\end{abstract}



\section{Introduction}
\label{introduction}

\begin{figure}[htb]
\vskip 0.2in
\begin{center}
\centering
\subfigure[]{
\centerline{\includegraphics[width=\columnwidth]{architecture.eps}}
\label{fig:EasySpec-subfig-a}
}
\subfigure[]{
\centerline{\includegraphics[width=\columnwidth]{gpu_time.eps}}
\label{fig:EasySpec-subfig-b}
}
\caption{Overview of EasySpec. (a) Model architecture of layer-parallel speculation. (b) GPU runtime of decoding in the distributed system. SD enables token-level parallelism (fewer gray blocks), while causing multi-GPU under-utilization during the drafting stage. EasySpec solves the problem by layer-parallel speculation.}
\label{fig:EasySpec}
\end{center}
\vskip -0.2in
\end{figure}



Transformer-based Large Language Models (LLMs) have demonstrated remarkable problem-solving abilities across various domains \cite{vaswani2017attention,dubey2024llama,yang2024qwen2technicalreport,yang2024qwen2,achiam2023gpt}. However, as the parameter size continues to grow, the time-consuming process of auto-regressive decoding poses a significant barrier to deploying large models in latency-sensitive applications \cite{kim2024speculative,cai2024medusa}.

Various effective approaches \cite{hinton2015distilling,kim2021bert,gu2023mamba} have been proposed to reduce inference latency, including speculative decoding \cite{leviathan2023fast,chen2023accelerating} and tensor-parallel (TP) distribution \cite{shoeybi2019megatron}. Speculative decoding employs a smaller model to generate a draft token sequence, and uses token-level parallelism to conduct non-autoregressive verification, ensuring no shifting of the original model's output distribution. In contrast, TP distribution leverages cross-device parallelism by partitioning computational workloads across multiple devices (usually GPUs) and synchronizing the results subsequently, which is also lossless.

Combining speculative decoding with TP distribution achieves even greater acceleration rates. However, integrating a draft model into a distributed system is not trivial \cite{cai2024medusa}. Since the parameter size of the draft model is typically much smaller (around or less than 1/10) than that of the base model, the optimal TP size (the number of workload segments) is correspondingly smaller \cite{chen2023accelerating}, meaning that the draft model would run fastest when dispatched on one or a subset of GPUs, leaving other GPUs idle (see \cref{fig:EasySpec-subfig-b} and \cref{tab:test_time_TP_size}). Consequently, multi-GPU computational resources are under-utilized during the drafting stage.

We identify the primary cause of such inefficiency as the lack of parallelism between the draft model's layers: while tensor operations within one layer can be parallelized by TP, the layers themselves are restricted to be executed sequentially, one after another and from bottom to top, for generating a ``precise'' result of inference. However, the drafting result is never required to be precise, as it is only used for token parallelism and does not directly impact the final output—the verification result does (see \cref{subsec:speculative_decoding}). Therefore, strictly following the execution order is unnecessary, while a ``fuzzy'' but faster layer execution strategy could be preferable than the precise one, as long as it can sufficiently approximate the drafting result. 

Based on this insight, we propose EasySpec, a layer-parallel speculation strategy for optimizing the efficiency of drafting-stage multi-GPU utilization. EasySpec introduces \textbf{fuzzy speculation} by breaking the restriction of the execution order and running multiple attention layers simultaneously, with the most recent hidden state as input to all of them (\cref{fig:EasySpec-subfig-a}). This layer-level parallelism enables more devices to participate in the drafting stage, as the data dependencies between some consecutive layers are removed (\cref{alg:lp_fuzzy_speculation}).

Fuzzy speculation may introduce approximation errors, as the inference procedure differs from how the model is pre-trained. To prevent long-term error accumulation in the key-value (KV) cache of the draft model, we perform a process called \textbf{bonus calibration} after each iteration of drafting-and-verification. Firstly, the accepted tokens are concatenated to the bonus token to form a token sequence. This sequence is then re-input to the draft model, where a standard layer-sequential forward pass is executed, and the KV cache is updated with the precise values. This calibration step applies token parallelism, which is typically used only in the verification stage, to the drafting stage as well. Therefore, it incurs minimal additional latency—roughly equivalent to that of a standard forward pass.

We evaluate our method on two widely-used open-source LLMs, Llama-3-70B-Instruct and Qwen2-72B-Instruct, as well as a state-of-the-art LLM, Llama-3.3-70B-Instruct, and two task-specific models Qwen2-Math-72B-Instruct and Qwen2.5-Coder-32B-Instruct \cite{hui2024qwen25codertechnicalreport}. The draft models are selected from the same series as their corresponding large models. The evaluation results show that our method can achieve a peak speedup of 4.17x over vanilla decoding. Specifically, the drafting stage can be accelerated by up to 1.62x with no greater than 7\% drop of accuracy, requiring no additional training or fine-tuning on the existing draft models.

\section{Preliminary}
\label{preliminary}

\subsection{Speculative decoding}
\label{subsec:speculative_decoding}

Speculative decoding is a two-stage non-autoregressive decoding method for inference acceleration. The two stages are namely drafting and verification, which are iteratively executed as the inference proceeds. At time step $t$, the input token sequence is $X$. In the drafting stage, a smaller and faster model $M^{\prime}$ is employed as the draft model. $M^{\prime}$ auto-regressively runs $n$ times, generating a speculation token sequence $X,x^{\prime}_{t+1},...,x^{\prime}_{t+n}$ and probability distributions $p^{\prime}_{t+1},...,p^{\prime}_{t+n}$. The original large model $M$ then takes the whole sequence as input and conducts a single-forward verification, the outputs of which are $p_{t+1},...,p_{t+n+1}$. The acceptance probability of token $x^{\prime}_{i}$ is $\min(1, \frac{p_{i}(x^{\prime}_{i})}{p^{\prime}_{i}(x^{\prime}_{i})})$. After the verification sampling, the sequence of $m$ accepted tokens $x_{t+1},...,x_{t+m}$ (the same as $x^{\prime}_{t+1},...,x^{\prime}_{t+m}$) will be appended to the final output, along with a ``bonus'' token $x_{t+m+1}$ (called bonus because it is a by-product of the verification process) which can be sampled from the distribution $p$:
\begin{equation}
p = \left\{
    \begin{aligned}
    & norm(\max(0, p_{t+m+1} - p^{\prime}_{t+m+1})) & (m<n) \\
    & p_{t+m+1} & (m = n)
    \end{aligned}
    \right.
\end{equation}

Through this whole verification process, the final output token sequence aligns with the original distribution of the base model. Details of this algorithm are in \cref{appendix_sec:sd}.


\subsection{Tensor-Parallel Distributed Inference}

Tensor parallelism is a technique for deploying large models across multiple devices (GPUs). As for TP distributed inference, tensors are split up into multiple chunks, and each of these chunks is dispatched on a unique GPU. During computation operations, each GPU works on its respective chunk independently at first, and the results are synchronized afterwards to ensure that the final result is identical to the original. In modern multi-GPU systems, advanced communication hardware and protocols enable TP to be applied for inference acceleration.

\subsection{Device Characteristics}

Modern GPUs feature powerful computation capacities and hence short computation latency. Therefore, matrix computation involved in LLM inference is usually bottlenecked by memory bandwidth or control-flow overheads \cite{kim2023squeezellm}. In multi-GPU systems, it can also be bounded by communication and synchronization overheads.

Formally, we denote the workload of 1-token forward as $W$, the latency of executing $W$ on a single GPU as $T_{single}(W)$, and the additional overhead of TP distribution as $T_{add}$. The workload of $s$-token parallelism is $sW$, and $W/s$ the workload of $s$-sized TP. If $W$ is relatively small compared to the computation power, we can expect the following relationships:
\begin{equation}
\label{eq:token_parallelism}
    T_{single}(W) \approx T_{single}(sW) < sT_{single}(W)
\end{equation}
\begin{equation}
\label{eq:forced_tp}
    T_{single}(W/s) + T_{add} > T_{single}(W)
\end{equation}

The effectiveness of token-level parallelism can be illustrated by \cref{eq:token_parallelism}, which is leveraged by speculative decoding for acceleration. \cref{eq:forced_tp} highlights the impact of communication and control-flow overheads in distributed inference, which constrains multi-GPU utilization of the draft model.

\section{Method}
\label{method}

In this section, we present the details of how EasySpec enhances multi-GPU parallelism to accelerate the drafting stage of speculative decoding. The two key steps—fuzzy speculation and bonus calibration—are specified within each segment.

\subsection{Fuzzy Speculation}
\label{subsec:fuzzy_speculation}

As mentioned in \cref{introduction}, the standard speculation process restricts the layers to be executed sequentially. \cref{alg:sequential_precise_speculation} demonstrates the execution procedure of $N$ consecutive layers. If we eliminate $h^{\prime}_{i}$ in the algorithm, we have 
\begin{equation}
\label{equation:data_dependency}
    h_{i+1} = h_{i} + Attnoutput_{i} + MLPoutput_{i}
\end{equation}
, which illustrates the data dependencies between the input of layer $i+1$ and the output of layer $i$. In other words, before the outputs of the previous attention layer ($Attnoutput_{i}$) and MLP layer ($MLPoutput_{i}$) are computed, nothing can be done with the upper layer $i+1$ (and $i+2$ to $N$ as well).

\begin{algorithm}[t]
   \caption{Sequential Precise Speculation}
   \label{alg:sequential_precise_speculation}
\begin{algorithmic}
    \STATE {\bfseries Input:} hidden state $h$, $N$ consecutive attention layers $Attn_1$,$\cdots$,$Attn_{N}$ and MLP layers $MLP_{1}$,$\cdots$,$MLP_{N}$
    \STATE $h_{1} = h$
    \FOR{$i=1$ {\bfseries to} $N$}
    \STATE $Attnoutput_{i} = Attn_{i}(h_{i})$
    \STATE $h^{\prime}_{i} = h_{i} + Attnoutput_{i}$
    \STATE $MLPoutput_{i} = MLP_{i}(h^{\prime}_{i})$
    \STATE $h_{i+1} = h^{\prime}_{i} + MLPoutput_{i}$
    \ENDFOR
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[t]
   \caption{Layer-Parallel Fuzzy Speculation}
   \label{alg:lp_fuzzy_speculation}
\begin{algorithmic}
    \STATE {\bfseries Input:} hidden state $h$, $N$ consecutive attention layers $Attn_1$,$\cdots$,$Attn_{N}$ and MLP layers $MLP_{1}$,$\cdots$,$MLP_{N}$
    \STATE $h_{1} = h$
    \FOR{$i=1$ {\bfseries to} $N$}
    \STATE $Attnoutput_{i} = Attn_{i}(h_{1})$ (parallel)
    \ENDFOR
    \FOR{$i=1$ {\bfseries to} $N$}
    \STATE $h^{\prime}_{i} = h_{i} + Attnoutput_{i}$
    \STATE $MLPoutput_{i} = MLP_{i}(h^{\prime}_{i})$
    \STATE $h_{i+1} = h^{\prime}_{i} + MLPoutput_{i}$
    \ENDFOR
\end{algorithmic}
\end{algorithm}

Such a restriction on the execution order is necessary when the draft model is directly used for inference. Executing the model differently would shift the output probability distributions $p^{\prime}$ to $p^{\prime\prime}$, undermining the quality of generated content. However, the situation is different when the draft model is just used for speculation. Regardless of how $p^{\prime\prime}$ deviate from the original, as long as we set the acceptance probabilities to $\min(1, p/p^{\prime\prime})$, the final output distribution will theoretically remain the same as the base model. This leads to a potential optimization of speculation strategy: if the output distribution $p^{\prime}$ can be efficiently approximated by $p^{\prime\prime}$, such that $p^{\prime\prime} \approx p^{\prime}$, the overall process may speedup, while the final outputs remain lossless. That is to say, a slightly fuzzy but faster approach for speculation could outperform the precise one.

We propose an effective approximation method for fuzzy speculation. Inspired by \cite{lee2024infinigen}, we observe that the attention layers' outputs can be well approximated by substituting the input $h_{i}$ with $h_{1}$ for all $1<i\leq N$. \cref{alg:lp_fuzzy_speculation} shows the detailed steps of this method. From the perspective of model architectures, the input of the attention layers is not the most recent hidden states, but rather those from several layers prior (\cref{fig:EasySpec-subfig-a}). With this modification of layer execution strategy, we break the data dependencies between $N$ consecutive attention layers: once $h_{j}$ is computed for some $j$, all attention layers between $j$ to $j+N$ can be executed with no data dependencies among them, thereby achieving a layer-level concurrency. In a multi-GPU system, these attention layers can run simultaneously on different devices (\cref{fig:EasySpec-subfig-b}). 

Denote the execution time of sequential and fuzzy speculation over these $N$ attention layers as $T_{seq}$ and $T_{fuzzy}$, we have
\begin{equation}
\begin{split}
    T_{seq} = NT_{single}(A) \\
    T_{fuzzy} = T_{single}(A) + T_{add}
\end{split}
\end{equation}
where $A$ is the workload of one attention layer. Unlike the traditional TP method, the latency of running an attention layer is long enough to compensate for the additional overhead, that is $(N-1)T_{single}(A) > T_{add}$ for $N>1$, resulting in $T_{fuzzy} < T_{seq}$. The MLP layers are executed in the original sequential order, while we can run identical copies of them on all devices to eliminate communication, fully utilizing the multi-GPU computational resources.

Fuzzy speculation will inevitably result in some loss of speculation accuracy. In order to maintain a higher acceptance rate, EasySpec incorporates tree attention \cite{miao2024specinfer}, a technique commonly used in existing speculative decoding methods \cite{cai2024medusa,li2024eagle,li2024eagle2}. Tree attention can boost the acceptance rate by increasing the number of draft token sequences. As shown in \cref{fig:tree-attention}, the drafted tokens are structured as a tree, with each path corresponding to a sequence. With 2 forward passes, 6 draft token sequences are generated. Speculation and verification for all the sequences can be performed in a single forward pass using a 2D tree-attention mask.

\begin{figure}[t]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{tree_attention.eps}}
\caption{Illustration of tree attention.}
\label{fig:tree-attention}
\end{center}
\vskip -0.2in
\end{figure}

\begin{figure}[t]
\vskip 0.2in
\begin{center}
\centerline{
    \includegraphics[width=\columnwidth]{calibration_details_w.eps}
}
\caption{Illustration of bonus calibration.}
\label{fig:bonus-calibration}
\end{center}
\vskip -0.2in
\end{figure}

\subsection{Bonus Calibration}

Formally, the speculation of token $j$ can be denoted as $x^{\prime}_{j}, p^{\prime}_{j}, kv^{\prime}_{j-1} = M^{\prime}(x^{\prime}_{i<j-1}, kv^{\prime}_{i<j-1},x^{\prime}_{j-1})$. Fuzzy speculation induces errors to the value of $x^{\prime}$, $p^{\prime}$ and $kv^{\prime}$. The errors in $x^{\prime}$ and $p^{\prime}$ are limited within a single draft-verify iteration, since the rejected tokens and all output distributions will be discarded after verification. However, the errors in $kv^{\prime}_{j-1}$ will affect all subsequent generations after $j$, leading to long-term error accumulation that can severely degrade the accuracy.  

The precise values of $kv^{\prime}$ are required for preventing such error accumulation. Nevertheless, obtaining these values demands running the model's layers sequentially, which is exactly what we try to optimize through fuzzy speculation. To solve this dilemma, we propose an efficient way of obtaining precise KV values with minimal additional latency. At the beginning of a standard drafting stage, the bonus token $x_{t+m+1}$ from the last iteration is input to the draft model, for the speculation of $x^{\prime}_{t+m+2}$. This is a 1-token forward pass and also memory-bounded, as discussed in \cref{subsec:speculative_decoding}. Therefore, similar to the verification stage, token parallelism can also be applied here. By concatenating the accepted token sequence $x_{t+1},\cdots,x_{t+m}$ with the bonus token $x_{t+m+1}$ and re-inputting this entire sequence $x_{t+1},\cdots,x_{t+m}, x_{t+m+1}$ to the draft model, the precise KV values $kv^{\prime}_{t+1},\cdots,kv^{\prime}_{t+m}$ (and $kv^{\prime}_{t+m+1}$) can be produced through a single sequential forward pass. According to \cref{eq:token_parallelism}, the latency of such a forward is nearly equal to a 1-token sequential forward. 

We propose bonus calibration based on the method above. As shown in Figure \ref{fig:bonus-calibration}, we firstly discard all fuzzy KV items, regardless of whether the corresponding tokens are accepted or rejected (e.g. the green items 1 and 2 are also discarded). Then, a token-parallel forward is conducted to refill the KV cache with precise values. From the perspective of execution, the precise KV values are by-products of the next-token speculation of the bonus token, similar to how the bonus token itself is produced by the verification. This is the reason why we call this process ``bonus'' as well. Bonus calibration can generate a precise candidate token and distribution, while subsequent rounds of speculation can revert to be fuzzy. The overall latency increases slightly, from $nT_{fuzzy}$ to $T_{seq} + (n-1)T_{fuzzy}$ ($n$ is the speculation length), while the elimination of errors is crucial and substantial.


\section{Experiment}
\label{experiment}

\textbf{Models and Benchmarks.} We evaluated EasySpec on Llama-3-70B-Instruct and Qwen2-72B-Instruct, as well as a state-of-the-art model, Llama-3.3-70B-Instruct, and two task-specific models Qwen2-Math-72B-Instruct and Qwen2.5-Coder-32B-Instruct. Llama-3-8B-Instruct serves as the draft model for the Llama family, while Qwen2-7B-Instruct, Qwen2-Math-7B-Instruct and Qwen2.5-Coder-7B-Instruct are drafters for their respective base models. The benchmarks include a variety of tasks: language understanding (MMLU\cite{hendrycks2020measuring}), code generation (HumanEval\cite{chen2021evaluating}), math reasoning (MATH\cite{hendrycks2021measuring}), instruction following (IFEval\cite{zhou2023instruction}) and multilingual language usage (MGSM\cite{shi2022language}). We also evaluated our method on Spec-Bench \cite{xia2024unlocking} for comparison with related work.

\textbf{Environments and Configurations.} We implemented our distributed inference system using the Pytorch \cite{paszke2019pytorch} distributed library. The experiments were conducted on 8$\times$A100 GPUs. Unless specified otherwise, the layer-parallel size $N$ is set to 4, as it is optimal for most of the models tested. The detailed layer-parallel strategies are listed in \cref{appendix_sec:layer_parallel_strategies}.

\textbf{Baseline Settings.} We use the following baselines for comparison: vanilla auto-regressive decoding (vanilla), tensor-parallel distributed decoding (TP), speculative decoding in the distributed system (+SD), and tree attention (+tree). Additionally, we compare our work with EAGLE-2 \cite{li2024eagle2}.

\textbf{Metrics.} We use token throughput of single-batch inference as the performance metric. We also measure the running time per 100 tokens of the drafting and verification stages, which demonstrates the effectiveness of acceleration more clearly. Acceptance rates are also recorded and presented.


\subsection{Test-Time Distribution Sizes}

\cref{tab:test_time_TP_size} shows the token throughput of some typical tested models at different TP sizes. The benchmark consists of 40 data items from MMLU. While the large models benefit from multi-GPU distribution, the draft models even experience a slight slowdown when the TP size is greater than 1, confirming that the under-utilization of multi-GPU resources cannot be solved by traditional TP. We use 1 as the test-time distribution size for all small models. Since the throughputs of the large models at TP sizes of 2, 4, and 8 are nearly identical, the TP size of all large models is set to 8 for simplicity.

\begin{table}[t]
\begin{center}
\begin{small}
\centering
\caption{Token throughput of models at different TP sizes on MMLU.}
\vskip 0.15in
\begin{tabular}{ccccc}
\toprule
Model  & TP=1 & TP=2 & TP=4 & TP=8 \\
\midrule
L3-8B  & 36.76 & 33.90 & 32.31 & 32.39 \\
L3-70B  & 8.39 & 13.00 & 13.23 & 13.23 \\
Q2-7B  &  37.16 & 36.46 & 37.12 & - \\
Q2-72B  & 8.49 & 13.03 & 13.01 & 12.89 \\
\bottomrule
\end{tabular}
\end{small}
\end{center}
\vskip -0.1in
\label{tab:test_time_TP_size}   
\end{table}


\begin{table*}[hp]
\centering
\caption{Running time and acceptance rates of two mainstream models across datasets.}
\vskip 0.15in
\fontsize{9pt}{10pt}\selectfont
\begin{center}
\begin{tabular}{cccccccccc}
\toprule
~ & ~ & \multicolumn{4}{c}{Llama-3-70B(8B)-Instruct} & \multicolumn{4}{c}{Qwen2-72B(7B)-Instruct} \\
Dataset & Method & d & v & total & $\alpha$ & d & v & total & $\alpha$ \\
\midrule
\multicolumn{10}{c}{temperature=0} \\
\midrule
% MMLU
\multirow{5}*{MMLU}  & vanilla & - & - & 11.61 & - & - & - & 11.88 & -  \\
~ & TP &  - &  - & 7.57(1.53x)  &  - &  - &  - &  7.59(1.56x) &  - \\
~ & +sd &  3.52 &  2.15 & 5.67(2.05x) &  0.57 &  3.32 &  2.25 & 5.57(2.13x)  &  0.52 \\
~ & +tree &  2.52 &  1.59 & 4.11(2.82x) &  0.88 &  2.38 &  1.62 & 4.01(2.96x)  &  0.85 \\
~ & EasySpec &  1.70($\uparrow$1.48x) &  1.73 &  \textbf{3.44(3.38x)} &  0.82 &  1.65($\uparrow$1.44x) &  1.70 &  \textbf{3.35(3.55x)} &  0.80 \\
\midrule
% humaneval
\multirow{5}*{HumanEval}  & vanilla & - & - & 11.79 & - & - & - & 11.98 & -  \\
~ & TP &  - &  - & 7.58(1.55x)  &  - &  - &  - &  7.64(1.57x) &  - \\
~ & +sd &  2.93 &  1.79 & 4.72(2.50x)  &  0.74 &  2.82 &  1.83 &  4.65(2.58x) &  0.69 \\
~ & +tree &  2.53 &  1.58 &  4.11(2.87x) &  0.92 &  2.26 &  1.51 &  3.77(3.18x) &  0.95 \\
~ & EasySpec &  1.61($\uparrow$1.57x) &  1.57 &  \textbf{3.24(3.64x)} &  0.87 &  1.48($\uparrow$1.52x) &  1.54 & \textbf{3.02(3.97x)}  &  0.91 \\
\midrule
% MATH
\multirow{5}*{MATH}  & vanilla & - & - & 11.52 & - & - & - & 11.82 & -  \\
~ & TP &  - &  - & 7.56(1.52x)  &  - &  - &  - &  7.68(1.54x) &  - \\
~ & +sd &  2.96 &  1.74 &  4.70(2.45x) &  0.73 &  2.48 &  1.65 & 4.13(2.86x)  &  0.78 \\
~ & +tree &  2.50 &  1.47 & 3.97(2.90x)  &  0.95 &  2.20 &  1.45 &  3.65(3.24x) &  0.96 \\
~ & EasySpec &  1.58($\uparrow$1.58x) &  1.55 & \textbf{3.13(3.68x)}  &  0.91 &  1.44($\uparrow$1.52x) &  1.47 & \textbf{2.91(4.06x)}  &  0.95 \\
\midrule
% IFEval
\multirow{5}*{IFEval}  & vanilla & - & - & 11.29 & - & - & 11.59 & - & -  \\
~ & TP &  - &  - & 7.55(1.50x)  &  - &  - &  - &  7.64(1.52x) &  - \\
~ & +sd &  3.68 &  2.16 &  5.84(1.93x) &  0.55 &  4.24 &  2.80 &  7.04(1.64x) &  0.39 \\
~ & +tree &  2.53 &  1.55 & 4.09(2.76x)  &  0.89 &  2.80 &  1.84 & 4.65(2.49x)  &  0.72 \\
~ & EasySpec &  1.68($\uparrow$1.51x) &  1.65 & \textbf{3.33(3.39x)}  & 0.82 &  1.87($\uparrow$1.50x) &  1.94 & \textbf{3.81(3.04x)}  &  0.67 \\
\midrule
% MGSM
\multirow{5}*{MGSM}  & vanilla & - & - & 11.65 & - & - & 11.92 & - & -  \\
~ & TP &  - &  - & 7.55(1.54x)  &  - &  - &  - &  7.63(1.56x) &  - \\
~ & +sd &  2.66 &  1.61 &  4.27(2.73x) &  0.80 &  2.62 &  1.75 &  4.37(2.73x) &  0.72 \\
~ & +tree &  2.45 &  1.48 & 3.93(2.96x)  &  0.96 &  2.12 &  1.50 &  3.62(3.29x) &  0.94 \\
~ & EasySpec &  1.55($\uparrow$1.58x) &  1.51 & \textbf{3.06(3.80x)}  &  0.93 &1.54($\uparrow$1.37x) &  1.57 &  \textbf{3.11(3.83x)} &  0.88 \\

\midrule
\multicolumn{10}{c}{temperature=0.8} \\
\midrule

% T=0.8
% MMLU
\multirow{5}*{MMLU}  & vanilla & - & - &  11.62 & - & - & - &   & -  \\
~ & TP &  - &  - &  7.59(1.53x)  &  - &  - &  - &  7.66(1.56x)  &  - \\
~ & +sd & 3.09  & 1.93  & 5.02(2.32x) &  0.67 &  2.90 & 1.93  &  4.82(2.47x) &  0.65 \\
~ & +tree &  2.49 &  1.54 & 4.02(2.89x) &  0.94 &  2.14 & 1.48  & 3.62(3.30x)  & 0.95\\
~ & EasySpec &  1.59($\uparrow$1.56x) & 1.63  & \textbf{3.22(3.61x)} & 0.89  & 1.49($\uparrow$1.43x)  & 1.54  & \textbf{3.03(3.94x)}  & 0.93 \\
\midrule
% HumanEval
\multirow{5}*{HumanEval}  & vanilla & - & - &  11.79 & - & - & - & 12.14  & -  \\
~ & TP &  - &  - &  7.64(1.54x)  &  - &  - &  - &  7.65(1.59x)  &  - \\
~ & +sd & 2.94  & 1.81  & 4.75(2.48x) &  0.73 & 2.54  &  1.63 &  4.17(2.91x) &   0.80\\
~ & +tree &  2.47 & 1.49  & 3.95(2.98x) &  0.96 &  2.08 &  1.45 & 3.53(3.44x)  & 0.97\\
~ & EasySpec &  1.53($\uparrow$1.62x) & 1.49 & \textbf{3.02(3.90x)} & 0.94  &  1.45($\uparrow$1.44x) &  1.46 &  \textbf{2.91(4.17x)} & 0.97\\
\midrule
% MATH
\multirow{5}*{MATH}  & vanilla & - & - &  11.56 & - & - & 11.86 &   & -  \\
~ & TP &  - &  - &  7.54(1.53x)  &  - &  - &  - &  7.63(1.55x)  &  - \\
~ & +sd & 2.81  & 1.65  &  4.46(2.59x) & 0.78  &  2.44 &  1.54 &  3.98(2.98x) & 0.85  \\
~ & +tree &  2.44 & 1.45  & 3.89(2.97x) & 0.97  &  2.09 &  1.40 &  3.50(3.39x) & 0.99\\
~ & EasySpec & 1.52($\uparrow$1.61x)  & 1.48  & \textbf{3.00(3.86x)} & 0.94  &  1.43($\uparrow$1.47x) &  1.44 &  \textbf{2.87(4.13x)} & 0.98\\
\midrule
% IFEval
\multirow{5}*{IFEval}  & vanilla & - & - & 11.30  & - & - & 11.63 &   & -  \\
~ & TP &  - &  - &  7.53(1.50x)  &  - &  - &  - &  7.72(1.51x)  &  - \\
~ & +sd & 3.08  &  1.87 & 4.94(2.29x) & 0.67  &  3.24 & 2.14  & 5.38(2.16x)  &   0.56\\
~ & +tree & 2.45  &  1.46 & 3.91(2.89x) &  0.97 &  2.17 &  1.51 &  3.68(3.16x) & 0.91\\
~ & EasySpec & 1.52($\uparrow$1.61x)  &  1.50 & \textbf{3.03(3.73x)} & 0.92  &  1.53($\uparrow$1.42x) & 1.58  &  \textbf{3.11(3.73x)} & 0.87 \\
\midrule
% MGSM
\multirow{5}*{MGSM}  & vanilla & - & - & 11.67  & - & - & 12.02 &   & -  \\
~ & TP &  - &  - &  7.55(1.55x)  &  - &  - &  - &  7.65(1.57x)  &  - \\
~ & +sd & 2.54  &  1.55 & 4.10(2.85x) & 0.84  & 2.68  & 1.72  &  4.41(2.73x) &  0.76 \\
~ & +tree &  2.30 &  1.44 &  3.73(3.13x)& 0.98  &  2.08 &  1.45 &  3.53(3.41x) & 0.97\\
~ & EasySpec & 1.52($\uparrow$1.51x)  &  1.49 &  \textbf{3.01(3.88x)}&  0.96 &  1.46($\uparrow$1.42x) &  1.50 & \textbf{2.96(4.06x)}  & 0.94\\
  \bottomrule
\end{tabular}
\end{center}
\vskip -0.1in
\label{tab:main_results}
\end{table*}

\subsection{Main Results}

\subsubsection{Effectiveness}

To illustrate the effectiveness of EasySpec, we conducted experiments on Llama-3-70B-Instruct and Qwen2-72B-Instruct across datasets, with temperatures=0 and 0.8. The results are shown in \cref{tab:main_results}. We measure the running time per 100 tokens of the drafting (d) and verification (v) stages and the total process (total). $\alpha$ represents the acceptance rates. 

The results show that EasySpec consistently accelerates the overall execution of speculative decoding across all datasets. As the verification stage (the large models) can be accelerated by TP, the drafting stage becomes the bottleneck, accounting for nearly 2/3 of the total running time. EasySpec accelerates the drafting stage by up to 1.62x, sometimes surpassing the acceleration rates of TP for the large models (around 1.5x), meaning that the optimized drafting-stage multi-GPU utilization is as much efficient as TP can achieve on the verification stage. Meanwhile, the drop of speculation accuracy is no more than 7\%, suggesting that fuzzy speculation is sufficiently accurate in most cases.

We also evaluated EasySpec on task-specific models: Qwen2-Math-72B-Instruct and Qwen2.5-Coder-32B-Instruct. \cref{table:task_specific_model_results} shows the results of acceleration on the specific-task datasets MATH and HumanEval. On these datasets, the original speculation results of the draft models is already highly accurate (with the acceptance rates greater than 97\%), indicating that the speculation is inherently simple. As a result, fuzzy speculation incurs virtually no drop in accuracies, achieving an ideal drafting-stage acceleration with little-to-no cost.

\begin{table}[t]
\caption{Running time and acceptance rates of task-specific models on datasets of their corresponding tasks.}
\label{table:task_specific_model_results}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{tabular}{cccccc}
\toprule
&  & \multicolumn{4}{c}{Q2.5-Coder-32B(7B)-Instruct} \\
\midrule
Datasets & Methods & d & v & total & $\alpha$ \\
\midrule
\multirow{3}*{\makecell[c]{Human\\Eval}} & vanilla & -& -& 7.13 & - \\
~ & +tree & 2.15 & 1.23 & 3.38 & 0.97 \\
~ & EasySpec & 1.43 & 1.23 & 2.66  & 0.97 \\
~ & ~ & -0.72 & +0.00 & -0.72 & -0.00 \\
\midrule
&  & \multicolumn{4}{c}{Q2-Math-72B(7B)-Instruct} \\
\multirow{3}*{MATH} & vanilla & -& -& 11.55 & - \\
~ & +tree & 2.05 &1.51 & 3.55 & 0.99 \\
~ & EasySpec & 1.40& 1.52& 2.91 & 0.98 \\
~ & ~ & -0.65 & +0.01 & -0.64 & -0.01 \\
\bottomrule
\end{tabular}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

\subsubsection{Generalization and Stability}

An intermediate question is that whether we can use an even smaller model to reduce drafting latency. Firstly, such a model often does not exist within the same series, while training a smaller draft model is non-trivial and the trained model often lacks the ability of generalization across base models and datasets \cite{fubreak}. Secondly, an overly smaller model tends to exhibit unstable speculation performance, while models in the same series inherently have an alignment in prediction behaviors due to shared tokenizer, pretraining corpora and similar training process \cite{xia2024unlocking}.

To clarify the above statements, we compare our method with EAGLE-2 \cite{li2024eagle2}, a typical tiny-drafter speculative decoding method. EAGLE-2 uses a well-trained one-layer draft model (the parameter size is around 1B), aiming for extreme drafting latency reduction. We evaluated both methods on Spec-Bench \cite{xia2024unlocking}, where EAGLE-2 outperforms all existing methods according to the leaderboard. Besides average token throughput, we also calculate the variances of throughput across data items, for better demonstration of stability and generalization capability. The temperature is set to 0 for both methods.

Firstly we choose Llama-3-70B-Instruct as the base model. The results in the top part of \cref{table:comparision} show that, the EasySpec-accelerated 8B drafter achieves throughput comparable to, or even surpassing, the well-trained drafter of EAGLE-2, with no training or fine-tuning on either the target distribution or fuzzy approximation. This suggests that a smaller model from the same series, if existing, is good enough to deliver satisfactory performance. On the other hand, EasySpec exhibits significantly smaller variances across all datasets, indicating that a relatively larger draft model can offer more stable and predictable performance. We attribute this stability and generalization capability to the higher speculation accuracy, which reaches nearly 0.8 in contrast to the less than 0.4 accuracy of EAGLE-2. Even trained upon the target tasks, the smaller drafter is inherently incapable of highly accurate speculation due to its limited parameter size.

To further demonstrate the generalization capability across base models, we tested both methods on Llama-3.3-70B-Instruct, as there is no smaller model in its series and both methods need to generalize. The results in the bottom part of \cref{table:comparision} illustrate that EasySpec greatly outperforms EAGLE-2 across all types of tasks, with less than a 10\% drop of accuracy compared to the upper results. In contrast, both the average throughput and the acceptance rates of EAGLE-2 drop significantly.

\begin{table}[t]
\caption{Average token throughput (variances) and acceptance rates of EasySpec and EAGLE-2 on Spec-Bench.}
\label{table:comparision}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{tabular}{ccccc}
\toprule
& \multicolumn{2}{c}{EAGLE-2} & \multicolumn{2}{c}{EasySpec}\\
\midrule
Task Type & Throughput & $\alpha$ & Throughput & $\alpha$\\
\midrule
\multicolumn{5}{c}{Llama-3-70B-Instruct} \\
\midrule
MT & 33.81(27.09) & 0.35 & 31.33(4.75) & 0.80 \\
TL & 30.44(13.12) & 0.34 & 31.69(6.86) & 0.83 \\
SUM & 33.26(11.30) & 0.36 & 31.13(3.34) & 0.83 \\
QA & 30.98(21.69) & 0.31& 31.32(6.80) & 0.79 \\
MR & 34.37(12.84) & 0.39& 34.43(4.72) & 0.85 \\
RAG & 33.7(32.99) & 0.40 & 29.91(8.19) & 0.83 \\
\midrule
\multicolumn{5}{c}{Llama-3.3-70B-Instruct} \\
\midrule
MT & 23.38(13.86) & 0.20 & 29.52(15.50) & 0.76 \\
TL & 18.60(5.96) & 0.14 & 28.00(12.43) & 0.73 \\
SUM & 23.22(4.16) & 0.19 & 31.05(4.48) & 0.83 \\
QA & 21.31(6.35) & 0.17& 29.29(9.46) & 0.75 \\
MR & 26.95(5.78) & 0.25& 33.79(6.36) & 0.84 \\
RAG & 23.03(16.41) & 0.20 & 28.45(16.65) & 0.77 \\
\bottomrule
\end{tabular}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

\begin{figure*}[t]
\begin{center}
\centering
\centerline{\includegraphics[width=\textwidth]{ablation_study.eps}}
\end{center}
\caption{Token throughput and acceptance rates of Llama-3-70B(8B)-Instruct on MMLU under different configurations of EasySpec.}
\label{fig:ablation_study}
\end{figure*}

\subsection{Approximation Precision}

The feasibility of fuzzy speculation relies on the approximation precision of hidden states inside the draft model, which can be measured by cosine similarity. \cref{table:cosine_similarity} shows the average cosine similarities between the precise and fuzzy-approximated hidden states. The tested drafter is Llama-3-8B-Instruct and the benchmark is MMLU. The evaluated hidden states include input hidden states $h$, outputs of attention layers $Attnoutput$, and queries(q), keys(k) and values(v) inside attention heads. We set layer-parallel sizes to 2,3,4 for diverse approximation granularity. The results show that, as more layers are parallelized, the cosine similarity decreases but stays above 0.8, indicating a sufficiently high precision of approximation. Notably, the queries and keys are well-approximated with cosine similarities approaching 1. This is critical for maintaining precision of other hidden states and outputs, as errors in attention weights scale exponentially.


\begin{table}[t]
\caption{Average cosine similarities between precise and approximated hidden states. The tested model and testbench are Llama-3-8B-Instruct and MMLU.}
\label{table:cosine_similarity}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{tabular}{cccccc}
\toprule
LP size & $h$ & q & k & v & $Attnoutput$ \\
\midrule
2 & 0.93 & 0.98 & 0.99 & 0.92 & 0.93 \\
3 & 0.89 & 0.97 & 0.98 & 0.86 & 0.88 \\
4 & 0.86 & 0.96 & 0.97 & 0.82 & 0.83 \\
\bottomrule
\end{tabular}
\end{small}
\end{center}
\vskip -0.1in
\end{table}


\subsection{Ablation study}

We conducted an ablation study on 3 aspects of EasySpec: tree attention width, layer-parallel (LP) size and the presence or absence of calibration. We varied the tree width across 1,4,8,12 and LP size from 1 to 5. We measure token throughput and acceptance rates of Llama-3-70B(8B)-Instruct on MMLU under these different configurations, and present the results in \cref{fig:ablation_study}. 

As the LP size increases from 1, the token throughput improves due to better multi-GPU utilization of fuzzy speculation. However, when the LP size becomes too large, the speculation results are excessively fuzzy, leading to a noticeable decline in acceptance rates and hence token throughput. The optimal LP size is influenced by the tree width: a wider tree increases the likelihood of existence of correct tokens, thus allowing for a fuzzier and faster speculation (a larger LP size). On the other hand, even without tree attention (width=1), EasySpec can also accelerate the inference with layer parallelism (see the green line in \cref{fig:ablation_study}).

Bonus calibration plays a critical role in preserving speculation accuracy. When the LP size is 1, calibration is unnecessary, since there are no approximation errors in the KV cache. However, as the LP size increases, for every combination of tree width and LP size, bonus calibration can significantly improve token throughput and acceptance rates. Bonus calibration also slows the decline in acceptance rates with increasing LP sizes (see the right part of \cref{fig:ablation_study}). All the above results prove that the trade-off between precision of KV values and additional latency is reasonable and effective.


\section{Related Work}
\label{related_work}

Existing works on speculative decoding mainly focus on the drafting stage. Methods include training efficient draft models, self-speculation or algorithmic approaches. Specinfer \cite{miao2024specinfer} uses boost-trained draft models and tree-structured attention for efficient speculation. Medusa \cite{cai2024medusa} trains a set of extra MLP heads for future token prediction using the original LLM’s features. Self-Speculative Decoding \cite{zhang2023draft} and LayerSkip \cite{elhoushi2024layer} skips some layers of the target LLM for self-speculation. Lookahead \cite{fubreak} uses n-gram Jacobi decoding to increase acceptance rates. REST \cite{he2023rest} leverages a data storage for retrieval speculation.

The discrepancy between optimal distribution sizes is discovered by the authors of \cite{chen2023accelerating}, who suggested training a wider but shallower draft model for better performance under tensor parallelism. Few works focus on this problem, yet it is important and worth-solving. To the best of our knowledge, we are the first to effectively optimize multi-GPU utilization of currently-available draft models.

\section{Conclusion}
\label{conclusion}

In this paper, we propose EasySpec, a layer-parallel speculation strategy designed to optimize the efficiency of multi-GPU utilization during the drafting stage of speculative decoding. EasySpec introduces two modifications to the original speculation process: fuzzy speculation and bonus calibration. Fuzzy speculation breaks the sequential layer execution order and enables multi-layer parallelization, while bonus calibration applies token parallelism to the drafting stage for mitigating long-term error accumulation. EasySpec achieves consistent acceleration across models and datasets, requiring no training or fine-tuning on the existing draft models.



\newpage

\section*{Impact Statement}
This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.

% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
\bibliography{paper}
\bibliographystyle{submit2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{Speculative Decoding}
\label{appendix_sec:sd}

Speculative decoding consists of iterations of drafting and verification. The detailed algorithm of a single iteration is present in \cref{alg:speculative_decoding}.
\begin{algorithm}[H]
   \caption{An Iteration of Speculative Decoding}
   \label{alg:speculative_decoding}
\begin{algorithmic}
    \STATE {\bfseries Input:} input token sequence $X$, drafting length $m$, drafting model $M^{\prime}$, base model $M$
    \FOR{$i=1$ {\bfseries to} $m$}
    \STATE $x^{\prime}_{i}, p^{\prime}_{i} = M^{\prime}(X,x^{\prime}_{j<i})$
    \ENDFOR
    \STATE $p_{1}, \cdots,p_{m}, p_{m+1} = M(X,[x^{\prime}_{1}, \cdots, x^{\prime}_{m}])$
    \STATE sample $r_{1}, \cdots,r_{m}$ independently from $U(0,1)$
    \FOR{$i=1$ {\bfseries to} $m$}
    \IF {$r_{i} > \frac{p_{i}(x^{\prime}_{i})}{p^{\prime}_{i}(x^{\prime}_{i})}$}
    \STATE $n = i - 1$ ($n$ is the accepted length)
    \STATE {\bfseries break} (rejected)
    \ELSE
    \STATE $x_{i} = x^{\prime}_{i}$ (accepted)
    \ENDIF
    \ENDFOR
    \IF {$n < m$}
    \STATE $p_{bonus}=norm(\max(0, p_{n}-p^{\prime}_{n}))$
    \ELSE
    \STATE $p_{bonus}=p_{n+1}$
    \ENDIF
    \STATE sample $x_{bonus}$ from $p_{bonus}$
    \STATE {\bfseries return} $X,x_{1},\cdots,x_{n},x_{bonus}$
\end{algorithmic}
\end{algorithm}

For any distribution $p^{\prime}$, the final output token sequence is equivalent to sampling them directly from $p$. The proof is specified in many existing works (e.g. \cite{leviathan2023fast}).

\section{Layer-Parallel Strategies}
\label{appendix_sec:layer_parallel_strategies}

The detailed layer-parallel strategies of different models and LP sizes are listed in \cref{tab:layer-parallel strategies}. The 'a-b' in the table represents that the input hidden states $h_a, h_{a+1},\cdots,h_{b}$ are all equal to $h_{a}$. We empirically discovered that the input of layer 0 (the word embedding) can hardly approximate input of other layers, so layer 0 is never bounded with any other layers. All the tested draft models from the Qwen family have 28 layers, and the parallel strategy for them is identical.

\begin{table}[H]
    \centering
    \caption{Layer-parallel strategies for draft models. 'Qwen'represents all Qwen draft models.}
    \begin{tabular}{ccc}
    \toprule
       Model  & $N$ & Strategy \\
    \midrule
       \multirow{4}*{Llama-3-8B-Instruct}  & 2 & 0, 1-2, 3-4, ..., 29-30, 31\\
       ~ & 3 & 0, 1-3, 4-6, ..., 25-27, 28-30, 31 \\
       ~ & 4 &  0, 1-3, 4-7, 8-11, ..., 24-27, 28-30, 31 \\
       ~ & 5 &  0, 1-3, 4-8, 9-13, ..., 24-28, 29-30, 31 \\
    \midrule
       Qwen  & 4 & 0, 1-3, 4-7, 8-11, ..., 24-26, 27 \\
    \bottomrule
    \end{tabular}
    \label{tab:layer-parallel strategies}
\end{table}

Note that the optimal layer-parallel strategy depends on specific models and datasets, and we have not elaborated on finding more fine-grained strategies. Modifying these strategies could lead to improved performance.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for submit-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
