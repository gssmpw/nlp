\section{Background and Related work}
\label{sec:background}
\begin{table*}
    \centering
    \caption{ % the number of samples $\nsamples$
    Sample complexity of estimating popular distances on measure spaces in terms of the ambient dimension $\ndims$.
    \vspace{0.1in} % this is specified by the ICML instructions for some reason...
    }
    \resizebox{\linewidth}{!}{
    \begin{tabular}{lcc}
        \toprule
        Distance & Samples & Ref. \\
        \midrule
           Total Variation & $\bigomega{\exp(d)}$ & \cite{devroye2018total,wolfer2021statistical} \\
%        Operator Infinity Norm & $\bigomega{}$ & \\
        Hellinger (Jeffreys)  & $\bigomega{\exp(d)}$ & \cite{devroye2018total,wolfer2021statistical} \\
        Wasserstein-1 & $\infty$ & Thm 5 \cite{lee2023computability} \\
        Wasserstein-2 & $\bigomega{\exp(d)}$ & \cite{dudley1969speed,fournier2015rate,weed2019sharp} \\
        Wasserstein-$\infty$ & $\bigomega{\exp(d)}$ & \cite{fournier2015rate,liu2018rate,weed2019sharp} \\
        % Multi-Dimensional Subset Scan & $\bigomega{}$ & \\ -- this is not a distance
        Maximum Mean Discrepancy & $\bigomega{\exp(d)}$ & \cite{NIPS2016_5055cbf4} \\
        \midrule
        Maximum Subgroup Discrepancy & $\bigoh{\ndims}$ w.h.p. & This paper (Eq. \ref{eq:smpl_thm_pr2} in \cref{sec:complexity}) \\
        \bottomrule
    \end{tabular}
    \label{tab:samplecomplexity}
    }
\end{table*}

\subsection{Distances on Measure Spaces}
\label{relateddistances}

There are numerous distances on measure spaces used in applied probability, including (in the approximately chronological order)
Total Variation (TV, \cite{zbMATH02706589}),
Hellinger distance \cite{hellinger1909neue},
Kullback–Leibler (KL) divergence \cite{KL} and its variants,
Wasserstein-2 \cite{vaserstein1969markov,dudley1969speed} and its variants such as Wasserstein-1 \cite{vaserstein1969markov},  and
Maximum Mean Discrepancy (MMD, \cite{gretton2012kernel}).
As we outline below,   most of these methods have exponential sample complexity in terms of the number of dimensions.
As can be seen in Table~\ref{tab:samplecomplexity}, most of these distances come with either undecidability results \cite{lee2023computability} or
exponential lower bounds on their sample complexity in the worst case.
For MMD, while \cite{gretton2012kernel} claimed polynomial sample complexity,  \cite{NIPS2016_5055cbf4} explained the lower bounds on sample complexity under strong assumptions.
%Wasserstein-1 \cite{fournier2015rate,weed2019sharp,panaretos2019statistical},
%Wasserstein-2 \cite{dudley1969speed,fournier2015rate,weed2019sharp,panaretos2019statistical},
%Wasserstein-$\infty$ \cite{fournier2015rate,liu2018rate,weed2019sharp,panaretos2019statistical},
%TV \cite{devroye2018total,wolfer2021statistical,arbas2023polynomial}, operator infinity norm \cite{wolfer2021statistical}, and a variety of divergences including Sinkhorn \cite{genevay2019sample,quang2021convergence}.
Yet for other methods, such as Hellinger and Jeffreys distances, % TODO should there be more sth like (a.k.a., Jeffreys) check their "sameness"
high sample complexity follows from their relationship to TV distance.
We refer to \cite{panaretos2019statistical} for a thorough survey.

% Some basic properties of TV may be found in \cite{sublinear_methods}, Lectures 11,12.
Obviously, one can consider additional assumptions, such as having cardinality of the support (which scales with $\exp(d)$ in general), bounded by a constant. Testing TV closeness in time sublinear in the support was derived in
\cite{chan2014optimal}.
Entropy estimation bounds were obtained in \cite{valiant2011estimating}.
\cite{feng2023simple,bhattacharyya2022approximating} give an algorithm for estimating TV between just \emph{product} measures, polynomial in product dimension.
%\cite{bhattacharyya2022approximating} show that computing TV exactly is hard.
%Some transport distance bounds were given in \cite{tahmasebi2023sample}.
Likewise, one can consider smoothness of the measures and certain invariance properties \cite{chen2023sample,tahmasebisample}, or focus on Gaussian distributions \cite{hsu2024polynomial} only.
Ising type models testing was explored in \cite{kandiros2023learning}. While these assumptions are of considerable interest,
it is not easy to test that those assumptions hold in real-world data sets.

%Also, even with smoothness assumptions, one can obtain error $n^{-{1/d}}$
%in some cases \cite{dudley1969speed}.

\subsection{Subgroup and Intersectional Fairness}

The notion of subgroups gave rise to the
%Distances on measure spaces underlie
 work on subgroup fairness \cite{kearns2018preventing}, and underlies the work on intersectional fairness \cite{foulds2020intersectional,gohar2023survey}.
%Note that there are a number of other terms used, including intersectional group fairness.
In particular, in the legal scholarship, Intersectional Fairness ideas go back to the work of
Crenshaw \cite{crenshaw2013demarginalizing}, but remain a subject of lively debate \cite{collins2020intersectionality} to the present day.

In the algorithmic fairness literature,
the sample complexity of certain fairness estimates (statistical parity, false positive fairness) was considered in \cite{kearns2018preventing}. The notion of distance, which we consider here, is conceptually different, as it concerns data quality (see Section \ref{sec:intro}) rather than a fairness test of a particular given classifier. Moreover,
the algorithms of \cite{kearns2018preventing} were developed with \emph{linear} subgroups in mind, and consequently evaluated on linear subgroups \cite{kearns2019empirical}.
In particular, these algorithms require certain specific heuristics (oracles), that are mainly developed for the linear case.
We note that such linear subgroups are considerably less interpretable and less suitable for real-world applications compared to the intersectional subgroups that we consider here.


In a related direction, multidimensional subset scanning methods systematically sift through potentially large collections of subgroups to find anomalous or biased regions. Although these approaches vary in the exact objective -- ranging from pinpointing classifier miscalibration \cite{zhang2016identifying} to detecting compact anomalies via penalties \cite{speakman2015penalized} or scanning multiple data streams \cite{neil2013fast} -- they share a focus on likelihood-based scoring and efficient “fast subset” searches. While these techniques are adept at finding one or more ``highest-scoring'' subgroups, their goals and scoring mechanisms are distinct from distance-based comparisons, positioning them as complementary.


%\jiricomment{Show the definition of Statistical Parity Subgroup Fairness.
%Check \cite{hsu2024polynomial} to see how this relates to our work.
%}
%