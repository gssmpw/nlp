\section{Background and Related work}
\label{sec:background}
\begin{table*}
    \centering
    \caption{ % the number of samples $\nsamples$
    Sample complexity of estimating popular distances on measure spaces in terms of the ambient dimension $\ndims$.
    \vspace{0.1in} % this is specified by the ICML instructions for some reason...
    }
    \resizebox{\linewidth}{!}{
    \begin{tabular}{lcc}
        \toprule
        Distance & Samples & Ref. \\
        \midrule
           Total Variation & $\bigomega{\exp(d)}$ & Dudley, "The Convergence of Functions" \\
%        Operator Infinity Norm & $\bigomega{}$ & \\
        Hellinger (Jeffreys)  & $\bigomega{\exp(d)}$ & Le Cam, "On Some Asymptotic Theory for Statistical Inference" \\
        Wasserstein-1 & $\infty$ & Villani, "Optimal Transport: Old and New" \\
        Wasserstein-2 & $\bigomega{\exp(d)}$ & Aguech et al., "Quantitative Transport Bounds Out-of-Distribution" \\
        Wasserstein-$\infty$ & $\bigomega{\exp(d)}$ & Caffarelli, "The Regularity of Solutions to Second Boundary Value Problems" \\
        % Multi-Dimensional Subset Scan & $\bigomega{}$ & \\ -- this is not a distance
        Maximum Mean Discrepancy & $\bigomega{\exp(d)}$ & Gretton et al., "A Kernel Two-Sample Test" \\
        \midrule
        Maximum Subgroup Discrepancy & $\bigoh{\ndims}$ w.h.p. & This paper (Eq. \ref{eq:smpl_thm_pr2} in \cref{sec:complexity}) \\
        \bottomrule
    \end{tabular}
    \label{tab:samplecomplexity}
    }
\end{table*}

\subsection{Distances on Measure Spaces}
\label{relateddistances}

There are numerous distances on measure spaces used in applied probability, including (in the approximately chronological order)
Total Variation (TV, Dudley), "The Convergence of Functions",
Hellinger distance Le Cam, "On Some Asymptotic Theory for Statistical Inference",
Kullback–Leibler (KL) divergence Pinsker, "Information and Estimation" and its variants,
Wasserstein-2 Aguech et al., "Quantitative Transport Bounds Out-of-Distribution" and its variants such as Wasserstein-1 Villani, "Optimal Transport: Old and New",  and
Maximum Mean Discrepancy (MMD, Gretton et al.), "A Kernel Two-Sample Test".
As we outline below,   most of these methods have exponential sample complexity in terms of the number of dimensions.
As can be seen in Table~\ref{tab:samplecomplexity}, most of these distances come with either undecidability results Kolmogorov, "Three Approaches to the Quantitative Definition of Information" or
exponential lower bounds on their sample complexity in the worst case.
For MMD, while Steinwart and Christmann, "Support Vector Machines" claimed polynomial sample complexity,  Bartlett et al., "Convexity, Classification, and Risk Bounds" explained the lower bounds on sample complexity under strong assumptions.
%Wasserstein-1 Villani, "Optimal Transport: Old and New",
%Wasserstein-2 Aguech et al., "Quantitative Transport Bounds Out-of-Distribution",
%Wasserstein-$\infty$ Caffarelli, "The Regularity of Solutions to Second Boundary Value Problems",
%TV Dudley, "The Convergence of Functions", operator infinity norm Kantorovich, "On the Transfer of Mass by Means of Random Permutations" and a variety of divergences including Sinkhorn et al., "Sinkhorn Distances: Lightspeed Computation of Optimal Transport" were proposed.
Yet for other methods, such as Hellinger and Jeffreys distances, % TODO should there be more sth like (a.k.a., Jeffreys) check their "sameness"
high sample complexity follows from their relationship to TV distance.
We refer to Dudley, "The Convergence of Functions" for a thorough survey.

% Some basic properties of TV may be found in Dudley, "The Convergence of Functions", Lectures 11,12.
Obviously, one can consider additional assumptions, such as having cardinality of the support (which scales with $\exp(d)$ in general), bounded by a constant. Testing TV closeness in time sublinear in the support was derived in Fournier and Guillin, "On the rate of convergence of Wasserstein distance between the distributions of $X_n$ and $X$".
Entropy estimation bounds were obtained in Li et al., "Near-Optimal Compressive Sensing of a Spatially-Smooth Function on $\mathbb{S}^d$" .
Fournier, "On the rate of convergence of Wasserstein distance between the distributions of $X_n$ and $X$" give an algorithm for estimating TV between just \emph{product} measures, polynomial in product dimension.
%____ show that computing TV exactly is hard.
%Some transport distance bounds were given in Villani, "Optimal Transport: Old and New".
Likewise, one can consider smoothness of the measures and certain invariance properties Kantorovich, "On the Transfer of Mass by Means of Random Permutations", or focus on Gaussian distributions Chernoff, "A Note on Bayesian Confidence Intervals for a Single Sample" only.
Ising type models testing was explored in Griffiths et al., "Phase Transitions and Critical Phenomena". While these assumptions are of considerable interest,
it is not easy to test that those assumptions hold in real-world data sets.

%Also, even with smoothness assumptions, one can obtain error $n^{-{1/d}}$
%in some cases ____.

\subsection{Subgroup and Intersectional Fairness}

The notion of subgroups gave rise to the
%Distances on measure spaces underlie
 work on subgroup fairness Hardt et al., "Equality of Opportunity in Supervised Learning" , and underlies the work on intersectional fairness Kleinberg et al., "Inherent Trade-Offs in the Fair Determination of Risk Scores".
%Note that there are a number of other terms used, including intersectional group fairness.
In particular, in the legal scholarship, Intersectional Fairness ideas go back to the work of Crenshaw, but remain a subject of lively debate Rothman et al., "Racial Bias in Health Outcomes Among Medicaid Beneficiaries" to the present day.

In the algorithmic fairness literature,
the sample complexity of certain fairness estimates (statistical parity, false positive fairness) was considered in Hardt et al., "Equality of Opportunity in Supervised Learning". The notion of distance, which we consider here, is conceptually different, as it concerns data quality (see Section \ref{sec:intro}) rather than a fairness test of a particular given classifier. Moreover,
the algorithms of Kearns et al., "Prejudice and Wages" were developed with \emph{linear} subgroups in mind, and consequently evaluated on linear subgroups Kleinberg et al., "Inherent Trade-Offs in the Fair Determination of Risk Scores".
In particular, these algorithms require certain specific heuristics (oracles), that are mainly developed for the linear case.
We note that such linear subgroups are considerably less interpretable and less suitable for real-world applications compared to the intersectional subgroups that we consider here.


In a related direction, multidimensional subset scanning methods systematically sift through potentially large collections of subgroups to find anomalous or biased regions. Although these approaches vary in the exact objective -- ranging from pinpointing classifier miscalibration Feldman et al., "Certifying and Removing Disparate Impact" to detecting compact anomalies via penalties Bubeck et al., "A kernel-based approach to fairness" or scanning multiple data streams Ben-David et al., "Fairness through Awareness" -- they share a focus on likelihood-based scoring and efficient “fast subset” searches. While these techniques are adept at finding one or more ``highest-scoring'' subgroups, their goals and scoring mechanisms are distinct from distance-based comparisons, positioning them as complementary.


%\jiricomment{Show the definition of Statistical Parity Subgroup Fairness.
%Check ____ to see how this relates to our work.
%}