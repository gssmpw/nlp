\section{Background and Related work}
\label{sec:background}
\begin{table*}
    \centering
    \caption{ % the number of samples $\nsamples$
    Sample complexity of estimating popular distances on measure spaces in terms of the ambient dimension $\ndims$.
    \vspace{0.1in} % this is specified by the ICML instructions for some reason...
    }
    \resizebox{\linewidth}{!}{
    \begin{tabular}{lcc}
        \toprule
        Distance & Samples & Ref. \\
        \midrule
           Total Variation & $\bigomega{\exp(d)}$ & ____ \\
%        Operator Infinity Norm & $\bigomega{}$ & \\
        Hellinger (Jeffreys)  & $\bigomega{\exp(d)}$ & ____ \\
        Wasserstein-1 & $\infty$ & Thm 5 ____ \\
        Wasserstein-2 & $\bigomega{\exp(d)}$ & ____ \\
        Wasserstein-$\infty$ & $\bigomega{\exp(d)}$ & ____ \\
        % Multi-Dimensional Subset Scan & $\bigomega{}$ & \\ -- this is not a distance
        Maximum Mean Discrepancy & $\bigomega{\exp(d)}$ & ____ \\
        \midrule
        Maximum Subgroup Discrepancy & $\bigoh{\ndims}$ w.h.p. & This paper (Eq. \ref{eq:smpl_thm_pr2} in \cref{sec:complexity}) \\
        \bottomrule
    \end{tabular}
    \label{tab:samplecomplexity}
    }
\end{table*}

\subsection{Distances on Measure Spaces}
\label{relateddistances}

There are numerous distances on measure spaces used in applied probability, including (in the approximately chronological order)
Total Variation (TV, ____),
Hellinger distance ____,
Kullback–Leibler (KL) divergence ____ and its variants,
Wasserstein-2 ____ and its variants such as Wasserstein-1 ____,  and
Maximum Mean Discrepancy (MMD, ____).
As we outline below,   most of these methods have exponential sample complexity in terms of the number of dimensions.
As can be seen in Table~\ref{tab:samplecomplexity}, most of these distances come with either undecidability results ____ or
exponential lower bounds on their sample complexity in the worst case.
For MMD, while ____ claimed polynomial sample complexity,  ____ explained the lower bounds on sample complexity under strong assumptions.
%Wasserstein-1 ____,
%Wasserstein-2 ____,
%Wasserstein-$\infty$ ____,
%TV ____, operator infinity norm ____, and a variety of divergences including Sinkhorn ____.
Yet for other methods, such as Hellinger and Jeffreys distances, % TODO should there be more sth like (a.k.a., Jeffreys) check their "sameness"
high sample complexity follows from their relationship to TV distance.
We refer to ____ for a thorough survey.

% Some basic properties of TV may be found in ____, Lectures 11,12.
Obviously, one can consider additional assumptions, such as having cardinality of the support (which scales with $\exp(d)$ in general), bounded by a constant. Testing TV closeness in time sublinear in the support was derived in
____.
Entropy estimation bounds were obtained in ____.
____ give an algorithm for estimating TV between just \emph{product} measures, polynomial in product dimension.
%____ show that computing TV exactly is hard.
%Some transport distance bounds were given in ____.
Likewise, one can consider smoothness of the measures and certain invariance properties ____, or focus on Gaussian distributions ____ only.
Ising type models testing was explored in ____. While these assumptions are of considerable interest,
it is not easy to test that those assumptions hold in real-world data sets.

%Also, even with smoothness assumptions, one can obtain error $n^{-{1/d}}$
%in some cases ____.

\subsection{Subgroup and Intersectional Fairness}

The notion of subgroups gave rise to the
%Distances on measure spaces underlie
 work on subgroup fairness ____, and underlies the work on intersectional fairness ____.
%Note that there are a number of other terms used, including intersectional group fairness.
In particular, in the legal scholarship, Intersectional Fairness ideas go back to the work of
Crenshaw ____, but remain a subject of lively debate ____ to the present day.

In the algorithmic fairness literature,
the sample complexity of certain fairness estimates (statistical parity, false positive fairness) was considered in ____. The notion of distance, which we consider here, is conceptually different, as it concerns data quality (see Section \ref{sec:intro}) rather than a fairness test of a particular given classifier. Moreover,
the algorithms of ____ were developed with \emph{linear} subgroups in mind, and consequently evaluated on linear subgroups ____.
In particular, these algorithms require certain specific heuristics (oracles), that are mainly developed for the linear case.
We note that such linear subgroups are considerably less interpretable and less suitable for real-world applications compared to the intersectional subgroups that we consider here.


In a related direction, multidimensional subset scanning methods systematically sift through potentially large collections of subgroups to find anomalous or biased regions. Although these approaches vary in the exact objective -- ranging from pinpointing classifier miscalibration ____ to detecting compact anomalies via penalties ____ or scanning multiple data streams ____ -- they share a focus on likelihood-based scoring and efficient “fast subset” searches. While these techniques are adept at finding one or more ``highest-scoring'' subgroups, their goals and scoring mechanisms are distinct from distance-based comparisons, positioning them as complementary.


%\jiricomment{Show the definition of Statistical Parity Subgroup Fairness.
%Check ____ to see how this relates to our work.
%}
%