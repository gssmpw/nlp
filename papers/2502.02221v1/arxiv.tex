\documentclass{article}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{hyperref}
\usepackage[numbers]{natbib}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

\usepackage{stmaryrd}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}


\input{defs}
\newcommand{\mcX}{\mathcal{X}}
\newcommand{\mcF}{\mathcal{F}}
\newcommand{\mcG}{\mathcal{G}}
\newcommand{\mcY}{\mathcal{Y}}
\newcommand{\mcD}{\mathcal{D}}
\newcommand{\mcP}{\mathcal{P}}
\newcommand{\mcL}{\mathcal{L}}
\newcommand{\mcB}{\mathcal{B}}

\usepackage{xcolor}
\newcommand{\dist}{\mbox{dist}}
\newcommand{\argmin}{\mbox{argmin}}

\newcommand\K{\mathbb{K}}\newcommand\N{\mathbb{N}}\newcommand\Z{\mathbb{Z}}\newcommand\Q{\mathbb{Q}}\newcommand\R{\mathbb{R}}\newcommand\C{\mathbb{C}}
\newtheorem{condition}{Condition}[section]
\newtheorem{fact}{Fact}[section]
\newtheorem{claim}{Claim}[section]
\newtheorem{example}{Example}[section]


\title{Bias Detection via Maximum Subgroup Discrepancy}
\date{\today}
\author{Jiří Němeček
\and Mark Kozdoba
\and Illia Kryvoviaz
\and Tomáš Pevný
\and Jakub Mareček}

\begin{document}
\maketitle

\begin{abstract}
Bias evaluation is fundamental to trustworthy AI, both in terms of checking data quality and in terms of checking the outputs of AI systems. In testing data quality, for example, one may study a distance of a given dataset, viewed as a distribution, to a given ground-truth reference dataset.
However, classical metrics, such as the Total Variation and the Wasserstein distances, are known to have high sample complexities and, therefore, may fail to provide meaningful distinction in many practical scenarios.

In this paper, we propose a new notion of distance, the Maximum Subgroup Discrepancy (MSD). In this metric, two distributions are close if, roughly, discrepancies are low for all feature subgroups. While the number of subgroups may be exponential, we show that the sample complexity is linear in the number of features, thus making it feasible for practical applications. Moreover,
we provide a practical algorithm for the evaluation of the distance, based on Mixed-integer optimization (MIO). We also note that the proposed distance is easily interpretable, thus providing clearer paths to fixing the biases once they have been identified. It also provides guarantees for all subgroups.
Finally, we empirically evaluate, compare with other metrics,  and demonstrate the above properties of MSD on real-world datasets.
\end{abstract}


\section{Introduction}
\label{sec:intro}

Regulatory frameworks, such as the AI Act \cite{AIAct} in Europe, suggest that one needs to measure data quality, including bias detection in training data,
as well as to detect bias in the output of the AI system,
but provide no suggestions as to what bias measures to use.
This is the case of the very recent IEEE Standard for Algorithmic Bias Considerations \cite{10851955} and earlier NIST white papers \cite{schwartz2022towards}  too, where the latter stops at the ``majority of fairness metrics are observational as they can be expressed
using probability statements involving the available random variables''.

At the most basic level, one could imagine bias detection as a two-sample problem in statistics, where, given two sets of samples, one asks whether they come from the same distribution.
In practice, the two sets of samples often do not come from the same distribution, but one would like to have an estimate of the distance between the two distributions.
The distance estimate, as any other statistical estimate \cite{tsybakov2009nonparametric}, comes with an error.
One would like the error in the estimate to be much smaller than the estimated value
for the bias detection to be credible, stand up in any court proceedings, etc.

%\notemk{a paragraph about other measures of distance and their sample complexity.}

In intersectional fairness, one would like to estimate bias for all protected subgroups.
This faces three challenges:
(1) the number of subgroups is exponential in the number of protected attributes,
(2) estimating many distances on measure spaces has sample complexity exponential in the ambient dimension for each subgroup, and
(3) some subgroups may have too few samples to estimate the bias well in terms of estimation error and significance.
Let us now consider these challenges in more detail.

Sample complexity is the number of samples that makes it possible to estimate a quantity to a given error.
A lower bound on sample complexity then suggests the largest known number of samples universally required to reach a given error.
The sample complexity of bias estimation depends on the distance between distributions (measures) used.
The accuracy improves in the number of samples taken,
but the rate of improvement depends on the dimension.
See Table \ref{tab:samplecomplexity} for a brief summary of the commonly considered distances and their sample complexity and Section
\ref{relateddistances} for references and discussion.
As is often the case in high-dimensional probability, the ``curse of dimensionality'' suggests that the number of samples to a given error grows exponentially with the dimension.

Not only is sample complexity a lower bound on the runtime \cite{lee2023computability}, but one must often test for bias for all subgroups, of which there are exponentially many (all combinations of protected attributes).
% Sample complexity of bias estimate is important for several reasons:
% first, the sample complexity is a lower bound on the runtime, even in cases where this is decidable  \cite{lee2023computability}.
% Second, many regulatory frameworks require the bias to be tested for all subgroups, of which there may be exponentially many in the number of protected attributes.
Together, this could lead to doubly exponential time complexity, becoming intractable even for a few dimensions. Thus, one would like to detect bias for a single subgroup with polynomial complexity or reason about the joint problem.
% Unless one wishes to run a bias detection with a doubly-exponential run-time, one should like to have polynomial complexity of the bias detection for a single subgroup or reason about the joint problem.
Additionally, the number of samples representing a certain subgroup decreases with the number of protected attributes defining it. Smaller sample complexity enables us to reliably detect bias for smaller subgroups.


In this paper, we \textbf{(i)} introduce the Maximum Subgroup Discrepancy
(MSD) distance between distributions. Compared to the standard distances discussed above,  we \textbf{(ii)} show that this distance has a manageable sample complexity (linear in the number of protected attributes). We also note that in contrast to other distances, the MSD is \emph{interpretable}, and thus provides actionable information on how the bias in the data should be mitigated once found. It also provides a guarantee in the form of an upper bound on bias over all subgroups. Next,  we \textbf{(iii)} develop a new Mixed-integer optimization (MIO) formulation for the evaluation of MSD. Finally,  we \textbf{(iv)} numerically validate the estimation stability and the dependence on the sample size for MSD and demonstrate its advantage over the Total Variation and Wasserstein metrics on 10 real-world datasets, which were developed with the view of studying fairness.
We show that MSD requires exponentially fewer samples to obtain a good estimate of the true value of the distance.
Taken together, we believe that these contributions
suggest MSD as a theoretically grounded and practically useful bias detection method.


\subsection{Problem description}
\label{sec:problem_statement}
We now discuss the MSD metric in slightly more detail.
Suppose we are given two probability distributions $\posdistr$ and $\negdistr$ over an input space $\mcX \subseteq \RR^\ndims$.
A subset of the features, denoted $\prot \subseteq \dims$,  will be considered to be the \emph{protected attributes},
such as for instance age, gender, ethnicity, and education.
A \emph{subgroup} is a subset of a population with a given fixed value of one or more protected attributes (e.g., black women).
Formally, a subgroup $\subg$, defined by attributes $\prot$ and their values $V=\Set{v_p}_{p \in \prot} \subseteq \RR^{\Abs{\prot}}$, is the set
\begin{equation}
    \subgs = \Set{x = (x_1, \ldots, x_d) \in \mcX \setsep x_p = v_p \text{ for $p \in \prot$}}.
\end{equation}
Two distributions will be considered similar if all subgroups have similar weights in the two distributions. Specifically, we define the \emph{Maximum Subgroup Discrepancy}  as
\begin{equation}
\MSD[\distance]\posdistr\negdistr\prot = \sup_{{\subg \in \subgs}} \distance({\posdistr(S),  \negdistr(S)}),%\right|,
\end{equation}
where $S$ ranges over all possible subgroups and $\distance$ behaves similar to a distance on measure spaces over the input space, $\distance: \mathcal{M}(\mcX) \times \mathcal{M}(\mcX) \to \RR$.
In this paper, we assume that the attributes are categorical\footnote{This is always possible by applying standard quantization procedures.} and thus the set of possible subgroups is finite. However, as mentioned earlier, this set will generally be of exponential size in the number of attributes.


% We utilize a distance measure  suitable for probability distributions.



% \paragraph{Notation}
% Let $\mcX \subseteq \RR^\ndims$ be the input data space of $\ndims$ features and $x \in \mcX$ an input sample. We split the input features into two sets $\dims = \prot \cup \nonprot$ where $\prot$ and $\nonprot$ are the sets of protected and non-protected features, respectively.

% The set of inputs belonging to the subgroup $\subg \in \subgs[\prot]$ is then $\Xsubg = \{x \in \mcX \mid \forall i \in \subg, x_i = 1\}$.

% The \emph{Maximal Subgroup Discrepancy} (MSD) uses a distance measure $\distance$ over distributions $\posdistr$ and $\negdistr$, parametrized by a subgroup $\subg \in \subgs$. MSD is defined as the distance between the distributions considering input vectors that belong to a given subgroup $\subg$ with the highest distance over all subgroups $\subgs$.


For example, in evaluating data quality, $\posdistr$ could be training data, seen as an empirical distribution and $\negdistr$ could be a distribution based on census data. Alternatively, we can consider distributions of a target variable of an ML model in auditing its fairness: $\posdistr$ for the positive class and $\negdistr$ for the negative. We can then find a subgroup that is most disadvantaged (in terms of $\MSD
\posdistr\negdistr\prot$) by the model.


The rest of this paper is organized as follows:
The background and prior work are discussed in Section
 \ref{sec:background}. In Section \ref{sec:max_norm} we introduce the MSD, discuss a number of its properties, and prove the sample complexity bounds.  In Section \ref{sec:estimation} we introduce the MSD estimation algorithm, and Section \ref{sec:experiments} is dedicated to the empirical evaluation. Finally, we conclude the paper in Section \ref{sec:conclusion}.


\section{Background and Related work}
\label{sec:background}
\begin{table*}
    \centering
    \caption{ % the number of samples $\nsamples$
    Sample complexity of estimating popular distances on measure spaces in terms of the ambient dimension $\ndims$.
    \vspace{0.1in} % this is specified by the ICML instructions for some reason...
    }
    \resizebox{\linewidth}{!}{
    \begin{tabular}{lcc}
        \toprule
        Distance & Samples & Ref. \\
        \midrule
           Total Variation & $\bigomega{\exp(d)}$ & \cite{devroye2018total,wolfer2021statistical} \\
%        Operator Infinity Norm & $\bigomega{}$ & \\
        Hellinger (Jeffreys)  & $\bigomega{\exp(d)}$ & \cite{devroye2018total,wolfer2021statistical} \\
        Wasserstein-1 & $\infty$ & Thm 5 \cite{lee2023computability} \\
        Wasserstein-2 & $\bigomega{\exp(d)}$ & \cite{dudley1969speed,fournier2015rate,weed2019sharp} \\
        Wasserstein-$\infty$ & $\bigomega{\exp(d)}$ & \cite{fournier2015rate,liu2018rate,weed2019sharp} \\
        % Multi-Dimensional Subset Scan & $\bigomega{}$ & \\ -- this is not a distance
        Maximum Mean Discrepancy & $\bigomega{\exp(d)}$ & \cite{NIPS2016_5055cbf4} \\
        \midrule
        Maximum Subgroup Discrepancy & $\bigoh{\ndims}$ w.h.p. & This paper (Eq. \ref{eq:smpl_thm_pr2} in \cref{sec:complexity}) \\
        \bottomrule
    \end{tabular}
    \label{tab:samplecomplexity}
    }
\end{table*}

\subsection{Distances on Measure Spaces}
\label{relateddistances}

There are numerous distances on measure spaces used in applied probability, including (in the approximately chronological order)
Total Variation (TV, \cite{zbMATH02706589}),
Hellinger distance \cite{hellinger1909neue},
Kullback–Leibler (KL) divergence \cite{KL} and its variants,
Wasserstein-2 \cite{vaserstein1969markov,dudley1969speed} and its variants such as Wasserstein-1 \cite{vaserstein1969markov},  and
Maximum Mean Discrepancy (MMD, \cite{gretton2012kernel}).
As we outline below,   most of these methods have exponential sample complexity in terms of the number of dimensions.
As can be seen in Table~\ref{tab:samplecomplexity}, most of these distances come with either undecidability results \cite{lee2023computability} or
exponential lower bounds on their sample complexity in the worst case.
For MMD, while \cite{gretton2012kernel} claimed polynomial sample complexity,  \cite{NIPS2016_5055cbf4} explained the lower bounds on sample complexity under strong assumptions.
%Wasserstein-1 \cite{fournier2015rate,weed2019sharp,panaretos2019statistical},
%Wasserstein-2 \cite{dudley1969speed,fournier2015rate,weed2019sharp,panaretos2019statistical},
%Wasserstein-$\infty$ \cite{fournier2015rate,liu2018rate,weed2019sharp,panaretos2019statistical},
%TV \cite{devroye2018total,wolfer2021statistical,arbas2023polynomial}, operator infinity norm \cite{wolfer2021statistical}, and a variety of divergences including Sinkhorn \cite{genevay2019sample,quang2021convergence}.
Yet for other methods, such as Hellinger and Jeffreys distances, % TODO should there be more sth like (a.k.a., Jeffreys) check their "sameness"
high sample complexity follows from their relationship to TV distance.
We refer to \cite{panaretos2019statistical} for a thorough survey.

% Some basic properties of TV may be found in \cite{sublinear_methods}, Lectures 11,12.
Obviously, one can consider additional assumptions, such as having cardinality of the support (which scales with $\exp(d)$ in general), bounded by a constant. Testing TV closeness in time sublinear in the support was derived in
\cite{chan2014optimal}.
Entropy estimation bounds were obtained in \cite{valiant2011estimating}.
\cite{feng2023simple,bhattacharyya2022approximating} give an algorithm for estimating TV between just \emph{product} measures, polynomial in product dimension.
%\cite{bhattacharyya2022approximating} show that computing TV exactly is hard.
%Some transport distance bounds were given in \cite{tahmasebi2023sample}.
Likewise, one can consider smoothness of the measures and certain invariance properties \cite{chen2023sample,tahmasebisample}, or focus on Gaussian distributions \cite{hsu2024polynomial} only.
Ising type models testing was explored in \cite{kandiros2023learning}. While these assumptions are of considerable interest,
it is not easy to test that those assumptions hold in real-world data sets.

%Also, even with smoothness assumptions, one can obtain error $n^{-{1/d}}$
%in some cases \cite{dudley1969speed}.

\subsection{Subgroup and Intersectional Fairness}

The notion of subgroups gave rise to the
%Distances on measure spaces underlie
 work on subgroup fairness \cite{kearns2018preventing}, and underlies the work on intersectional fairness \cite{foulds2020intersectional,gohar2023survey}.
%Note that there are a number of other terms used, including intersectional group fairness.
In particular, in the legal scholarship, Intersectional Fairness ideas go back to the work of
Crenshaw \cite{crenshaw2013demarginalizing}, but remain a subject of lively debate \cite{collins2020intersectionality} to the present day.

In the algorithmic fairness literature,
the sample complexity of certain fairness estimates (statistical parity, false positive fairness) was considered in \cite{kearns2018preventing}. The notion of distance, which we consider here, is conceptually different, as it concerns data quality (see Section \ref{sec:intro}) rather than a fairness test of a particular given classifier. Moreover,
the algorithms of \cite{kearns2018preventing} were developed with \emph{linear} subgroups in mind, and consequently evaluated on linear subgroups \cite{kearns2019empirical}.
In particular, these algorithms require certain specific heuristics (oracles), that are mainly developed for the linear case.
We note that such linear subgroups are considerably less interpretable and less suitable for real-world applications compared to the intersectional subgroups that we consider here.


In a related direction, multidimensional subset scanning methods systematically sift through potentially large collections of subgroups to find anomalous or biased regions. Although these approaches vary in the exact objective -- ranging from pinpointing classifier miscalibration \cite{zhang2016identifying} to detecting compact anomalies via penalties \cite{speakman2015penalized} or scanning multiple data streams \cite{neil2013fast} -- they share a focus on likelihood-based scoring and efficient “fast subset” searches. While these techniques are adept at finding one or more ``highest-scoring'' subgroups, their goals and scoring mechanisms are distinct from distance-based comparisons, positioning them as complementary.


%\jiricomment{Show the definition of Statistical Parity Subgroup Fairness.
%Check \cite{hsu2024polynomial} to see how this relates to our work.
%}
%\section{Prerequisites}

\subsection{Learning DNF}
A (protected) subgroup is naturally defined as a conjunction of a few feature-value pairs, e.g., \texttt{sex = "F" AND race = "black"}. A disjunction of multiple conjunctions (i.e., a union of subgroups) is a logical formula in Disjunctive Normal Form (DNF).

The study of learning DNF formulas is a fundamental part of theoretical computer science. Despite its polynomial sample complexity, finding a general algorithm with polynomial-time complexity has proven elusive in many settings.
Since the breakthrough reduction to polynomial threshold functions of \cite{klivans2001learning}, which matched the 1968 lower bound of \cite{minsky1988perceptrons},
 \cite{daniely2016complexity} helped understand the complexity-theoretic barriers.
See also \cite{shalev2014understanding} for an overview.


\subsection{Mixed-integer optimization}
Mixed-integer optimization (MIO, \cite{wolseyIntegerProgramming2021}) is a powerful framework for modeling and solving mathematical optimization problems, where some decision variables take values from a discrete set while others are continuously valued.
Despite MIO being a general framework for solving NP-hard problems, the MIO solvers are speeding up by approximately 22\% every year, \emph{excluding} hardware speedups \cite{kochProgressMathematicalProgramming2022}.
We use the abbreviation MIO, though we consider only mixed-integer \emph{linear} formulations.


MIO is used in machine learning, especially when one optimizes over discrete measures or decisions. This includes learning logical rules, including DNFs. \citet{malioutovExactRuleLearning2013} learn DNFs through a sequential generation of terms using an LP relaxation of an MIO formulation. Later, \citet{wangLearningOptimizedOrs2015} used MIO to optimize full DNFs, and \citet{suLearningSparseTwolevel2016} introduced a formulation with Hamming distance as an alternative objective function. A crucial improvement to the scalability of exact learning of DNFs was the BRCG \cite{dash2018boolean} utilizing column generation to generate candidate terms. Recently, MIO was utilized to learn a DNF classifier with fairness constraints \cite{lawlessInterpretableFairBoolean2023}. Importantly, we optimize only a single term (conjunction), representing the maximally discrepant subgroup, to evaluate bias. We do not perform predictions.

% \notejn{mio formulations of DNF - BRCG, Rudin's work

% OneRule source - \cite{malioutovExactRuleLearning2013}
% % formulate it then relax to LP

% Rudin \cite{wangLearningOptimizedOrs2015}
% % use MIO

% 2level - cca DNF used by us - though not here... \cite{suLearningSparseTwolevel2016}
% % add hamming distance?

% BRCG \cite{dash2018boolean} + AIX360 implementation \cite{arya2019one}
% % column gen

% Fair DNFs \cite{lawlessInterpretableFairBoolean2023}
% % current, for fairness
% }


As an aside, note that \cite{nair2021changed} (see also \cite{haldar2023interpretable}) use DNFs to compare two \emph{models}.
This is done by building two DNFs as proxy models and comparing them.
However, we could also directly compare the distributions of the two models, as proposed here.
% In this case, we would add the label provided by the model as another feature, and compare the distributions.
% \emph{Talk to Rahul about that.}

\section{Maximum Subgroup Discrepancy}
\label{sec:max_norm}
%In this Section we formally introduce one particular Maximum Subgroup Discrepancy distance and derive the associated sample complexity results.

Let $\mcX \subset \RR^d$ be an input space with $\ndims$ features and let $\prot \subseteq \dims$ be a subset of features that are protected attributes. Throughout the rest of the paper, we assume that the protected attributes are binary, which can always be achieved by quantization and one-hot encodings.
For a vector $x\in \RR^d$ and
$p \in \prot$ let
$x_p \in \Set{0,1}$ denote its $p$-th coordinate,
and let $\bar{x}_p := 1-x_p$ denote its logical negation.
The set of functions $\mcL = \Set{x_p,\bar{x}_p}_{p \in \prot}$ is called \emph{literals}.
A \emph{term} is a conjunction of literals. That is, for a subset $S \subset \mcL$, the term $\chi_S$ is defined as a function $\RR^d \rightarrow \Set{0,1}$ given by
\begin{equation}
    \chi_S(x) = \prod_{s \in S} s(x).
\end{equation}
The subsets $S \subset \mcL$ are called \emph{subgroups}, and with some abuse of notations we also refer to the corresponding parts of the data defined by $S$,
\begin{equation}
    \Xsubg = \Set{x \in \RR^d \setsep \chi_S(x) = 1},
\end{equation}
as subgroups.
The set of all possible subgroups $S \subset \mcL$ is denoted by $\subgs$.

For a distribution $\mu$ on $\RR^d$ and a function $f:\RR^d \rightarrow \RR$ denote by $\mu(f)$ the integral $\mu(f) = \int f(x) d\mu(x)$.

With this notation, the $\MSDc$ distance on feature set $\prot$ between two distributions, $\mu,\nu$, is defined as
\begin{equation}
\label{eq:main_msd_def}
    \MSDc(\mu,\nu;\prot) = \sup_{S \in \subgs} \Abs{\mu(\chi_S) - \nu(\chi_S)}.
\end{equation}
In words, as discussed earlier, two distributions are similar w.r.t MSD if all subgroups induced by $\prot$ have similar weights in both measures.

Let us now discuss the relation between MSD and the two standard distances - the $\ell_{\infty}$ and $\ell_1$, or equivalently, the TV distance.
Define the base terms as
\begin{equation}
    \mcB_{\prot} = \Set{ \prod_{p\in \prot} l_p \setsep \text{ where $l_p = x_p$ or $l_p = \bar{x}_p$} } \subset \subgs.
\end{equation}
That is, the terms in $\mcB_{\prot}$ correspond to all possible different values the projection of $x$ onto features $\prot$ might have. Equivalently, base terms correspond
to subgroups where \emph{all} protected attributes have a specified value (rather than the more general specification of only part of the values).  Clearly, there are precisely
$2^{\Abs{\prot}}$ base terms.

The $\ell_{\infty}$ norm may then be defined as
\begin{equation}
\label{eq:sup_def}
    \norm{\mu - \nu}_{\infty} = \sup_{S \in \mcB} \Abs{\mu(\chi_S) - \nu(\chi_S)}.
\end{equation}
That is, we consider the maximal weight difference over the atoms in $\prot$.

Let $\mcF_{\prot,\infty}$ be the set of functions $f: \RR^d \rightarrow \RR$ that depend only on the coordinates in $\prot$, and are bounded by $1$. Then, the \emph{total variation}\footnote{marginalised to $\prot$} (TV),  may be written as
\begin{flalign}
    \label{eq:tv_line1}
    \norm{\mu - \nu}_{TV} &= \half \sup_{f \in \mcF_{\prot,\infty}}
    \Abs{\mu(f) - \nu(f)}
    \\
    \label{eq:tv_line2}
    &= \half \sum_{S \in \mcB} \Abs{\mu(\chi_S) - \nu(\chi_S)},
\end{flalign}
where \eqref{eq:tv_line1} and \eqref{eq:tv_line2} are the dual and the standard definitions of the $\ell_1$ (and hence the TV) norms.

By comparing the definition \eqref{eq:main_msd_def} with \eqref{eq:sup_def} and \eqref{eq:tv_line1}, we see that
\begin{equation}
    \norm{\mu-\nu}_{\infty} \leq \MSDc(\mu,\nu;\prot) \leq
    \norm{\mu-\nu}_{TV},
\end{equation}
where the second inequality follows since all terms in $\subgs$ are clearly functions bounded by 1.

We thus observe that $\MSDc(\mu,\nu;\prot)$ is a stronger distance than $\ell_{\infty}$, which allows us to consider subgroups with partially specified values. At the same time, it is not as strong as the total variation, which requires the \emph{sum} over all base term differences to be small (rather than each of the differences being small individually). On the other hand, as discussed in Section \ref{sec:intro}, TV is, in fact, too strong to be practically computable due to its high sample complexity, while $\MSDc$ has a sample complexity linear in the number of attributes.


\subsection{Sample Complexity}
\label{sec:complexity}

In this section, we prove Theorem \ref{thm:main_sample_thm},
which allows us to quantify the error of estimating
$\MSDc(\mu,\nu;\prot)$ from finite samples of these distributions.

Let $\Set{x_i}_{i\leq N_1}$ be $N_1$ independent samples from $\mu$, and $\Set{x'_i}_{i\leq N_2}$ be $N_2$ independent samples from $\nu$.  Define the corresponding \emph{empirical distributions}, $\hat{\mu},\hat{\nu}$ by
\begin{equation}
    \hat{\mu} = \frac{1}{N_1} \sum_{i} \delta_{x_{i}}
    \text{ and } \hat{\nu} = \frac{1}{N_2} \sum_{i} \delta_{x'_{i}},
\end{equation}
where $\delta_{x}$ is an atomic distribution at point $x$, with weight $1$.


\begin{theorem}
\label{thm:main_sample_thm}
Fix $\delta>0$ and set $N = \min(N_1,N_2)$. Then with probability at least $1-2\delta$ over the samples,
\begin{equation}
    \MSDc(\mu,\nu;\prot) \leq \MSDc(\hat{\mu},\hat{\nu};\prot) +
    4\sqrt{\frac{2\Abs{\prot} + \log \frac{2}{\delta}}{2N}}.
\end{equation}
\end{theorem}
In words, if the number of samples $N$ is of the order of the number of protected attributes, $\Abs{\prot}$, or larger, we can estimate $\MSDc(\mu,\nu;\prot)$ by computing the $\MSDc$ on the empirical distributions, $\MSDc(\hat{\mu},\hat{\nu};\prot)$.
\begin{proof}
For any term $S \in \subgs$ by the triangle inequality we have
\begin{flalign*}
\Abs{\mu(S) - \nu(S)} \leq
&\Abs{\hat{\mu}(S) - \hat{\nu}(S) } \\
&+
\Abs{\mu(S) - \hat{\mu}(S) } +
\Abs{\nu(S) - \hat{\nu}(S) }.
\end{flalign*}
Therefore, taking suprema over $S$, we have
\begin{flalign*}
\label{eq:smpl_thm_pr1}
\MSDc(\mu,\nu;\prot) \leq &\MSDc(\hat{\mu},\hat{\nu};\prot) \\
&+
\sup_{S\in \subgs} \Abs{\hat{\mu}(S) - \mu(S)} \\
&+
\sup_{S\in \subgs} \Abs{\hat{\nu}(S) - \nu(S)}.
\end{flalign*}
The quantity $\sup_{S\in \subgs} \Abs{\hat{\mu}(S) - \mu(S)}$
describes the deviation of the empirical mean from the true mean over the $\subgs$  family of functions. Therefore,
by the uniform concentration results for bounded finite families (see, for instance, Theorem 2.13 in \cite{mohri2018foundations}), with probability at least $1- \delta$ over $\Set{x_i}$, we have
\begin{equation}
\label{eq:smpl_thm_pr2}
\sup_{S\in \subgs} \Abs{\hat{\mu}(S) - \mu(S)} \leq
2\sqrt{\frac{\log \Abs{\subgs} + \log \frac{2}{\delta}}{2N_1}}.
\end{equation}

Next, to estimate $\Abs{\subgs}$, observe that for every $p\in \prot$, a term either contains $x_p$, or $\bar{x}_p$, or neither. Thus there are at most $3^{\Abs{\prot}}$ terms. In particular, we have
\begin{equation}
\label{eq:smpl_thm_pr3}
    \log \Abs{\subgs} \leq 2 \Abs{\prot}.
\end{equation}

Finally, the proof is completed by repeating the argument for $\nu$, combining the results above, and using the union bound.
\end{proof}

\subsection{MSD Estimation As Classification}
To estimate the MSD for empirical distributions $\hat{\mu},\hat{\nu}$, it is more convenient to rephrase the maximization problem \eqref{eq:main_msd_def} as minimization of a classification loss, where the classifiers are taken from the family of terms, $\subgs$. As we detail in Section \ref{sec:estimation}, this point of view allows us to incorporate various useful ideas from the field of DNF classification into our MIO-based minimization algorithm.


To recast the problem as classification, denote by $\Set{x_i}_{i\leq N_1}$ and the $\Set{x'_j}_{j\leq N_2}$ datasets sampled from $\mu$ and $\nu$ respectively. Assign a label $y=1$ to all samples from $\mu$ and $y=0$ to all samples from $\nu$.
% Let $\istrue{\cdot}$ operator evaluate to 1 if the statement inside the brackets is true and to 0 otherwise.
Let $c(a,b)$ be the binary classification loss (0-1 loss),
$c(a,b) = 0 \text{ if } a=b \text{ and } 1 \text{ otherwise}$.

Then, for a binary classifier $f$ of a label $y$ as above, the binary classification loss would be
\begin{flalign*}
    &\Expsubidx{x\sim \mu} c(f(x),1) + \Expsubidx{x\sim \nu} c(f(x),0) \\
    % &\Expsubidx{x\sim \mu} \istrue{f(x) = 0} + \Expsubidx{x\sim \nu} \istrue{f(x) = 1} \\
    &=  \Expsubidx{x\sim \mu} \Brack{1-f(x)} +
    \Expsubidx{x\sim \nu} f(x)  \\
    &= 1 -\Expsubidx{x\sim \mu} f(x)
    +\Expsubidx{x\sim \nu} f(x) \\
    &= 1 - \Brack{ \mu(f) - \nu(f)}
\end{flalign*}
Thus, minimizing the classification loss is equivalent to maximizing the integral difference $\mu(f) - \nu(f)$, on which MSD is based.
Minimizing the classification loss with flipped labels (i.e., $y^\prime = 1 - y$) is equivalent to maximizing the opposite difference ($\nu(f) - \mu(f)$), enabling us to find the maximal absolute difference.

Note that the role of the classifier here is different from that of typical classifiers in algorithmic fairness, as $f$ is not required to be fair in any way. Rather, its role is similar to an \emph{adversary} in adversarial machine learning and is required to distinguish the two datasets as well as possible.


\section{Practical Estimation}
\label{sec:estimation}

In Section \ref{sec:max_norm}, we have described a measure with practically feasible sample complexity requirements. However, there is still a question of how to estimate it in practice. The challenge lies in the cardinality of $\subgs$, which is exponential in the number of protected attributes. See Figure \ref{fig:enumerative} for an illustration.

Thus, there is a question of how to estimate the full $\MSDc$ distance effectively. To this end, recall that subgroups $\subg \in \subgs$ constitute \emph{terms}, i.e., conjunctions of literals formed on a subset of features $\prot$.
% Finding interesting literals from data has been considered recently in
% \cite{dash2018boolean},\cite{arya2019one}, where a DNF classification model is fitted to labeled data.
And we also showed that one can formulate the $\MSDc$ distance maximization using two classification problems.


\begin{figure}[t]
    \centering
    \includegraphics[width=0.5\linewidth]{figures/sample_complexity/2025-01-30_enumeration.pdf}
    \caption{Illustrative example. To naively look for a subgroup for which two distributions differ most, one would have to enumerate all subgroups. We show the number of subgroups in real datasets and the mean number of subgroups (subsets of the datasets). Each distance metric can be computed within 10-minute time frame. This is compared to the proposed MIO-based $\MSDc$, which does not enumerate subgroups and thus scales better.
    }
    % Proportion of subgroups evaluated within 10 minutes and the total number of subgroups, utilizing an enumeration approach with a variety of real datasets and bias measures (except for the proposed approach, which is not enumerative). Note that all found subgroups with less than 10 samples are counted but not considered in computation to make it comparable.
    % The best enumerative approach is capable of considering 1000 times fewer sgs - extrapolating the time linearly, one would need about a full week to finish
    % and this is doable - most likely due to the limit on the number of samples and a small sample size.
    \label{fig:enumerative}
\end{figure}

By using the methods of \cite{wangLearningOptimizedOrs2015,arya2019one,wei2019generalized}, we could learn a DNF that distinguishes between $\posdistr$ and $\negdistr$. Note that by definition, every term in such a DNF would correspond to some subgroup $\subg$ on which $\posdistr$ and $\negdistr$ differ.
Since we search for a single member $\subg \in \subgs$, we do not need to search for a full DNF, we rather need a single term -- single conjunction.
% Further, we have shown that we can approximate the $\MSDdiff$ by $\MSD[\text{diff}]{\hat{\posdistr}}{\hat{\negdistr}}\prot$ well, and that it is equivalent to optimizing 0-1 loss in our classification settings.

To ensure that by minimizing the 0-1 loss, we indeed optimize the same notion as searching for the subgroup with maximal disparity, we must have balanced classes, i.e., an equal number of samples for each label. Alternatively, we can weigh the 0-1 losses of samples from a given class by $1/\text{number of samples of a given class}$.
We can thus run any conjunction-learning algorithm to obtain a subgroup and the loss $L_1$, then flip the labels $y_i$, and optimize again to get the loss $L_2$. Then $\MSD[\text{diff}]{\hat{\posdistr}}{\hat{\negdistr}}\prot = 1 - \min(L_1, L_2)$, assuming the found solutions of the classification problems are globally optimal, i.e., the found subgroups (terms) have the minimal loss out of all in $\subgs$.



\subsection{MIO formulation}

To find the globally optimal subgroup (i.e., conjunction or 1-term DNF), one can utilize Mixed-Integer Optimization (MIO). Let $\datapos$ and $\dataneg$ be sets of indices of samples from $\posdistr$ and $\negdistr$, respectively, and let $\data = \datapos \cup \dataneg$ be the set of indices of all samples.
Our formulation is related to the 0-1 error DNF formulation of \cite{suLearningSparseTwolevel2016}, but we search for a single term only and use a different objective, which leads to us also having to use further constraints.

Since we maximize the non-linear absolute difference, we must reformulate it using an auxiliary variable. We define variable $o$ as the absolute value objective we maximize. We bound it from above by the two potential values of the absolute value, such that it takes the higher of them.  The entire formulation is
\begin{subequations}
\label{eq:mio}
\begin{align}
    \max \; & o \\
    \mathrm{s.t.} \; & o \le \frac{1}{\Abs{\datapos}} \sum_{i \in \datapos}{\yhat} - \frac{1}{\Abs{\dataneg}} \sum_{i \in \dataneg}{\yhat} + 2b \label{eq:mio_abs1} \\
    & o \le \frac{1}{\Abs{\dataneg}} \sum_{i \in \dataneg}{\yhat} - \frac{1}{\Abs{\datapos}} \sum_{i \in \datapos}{\yhat} + 2(1-b) \label{eq:mio_abs2}  \\
    & \yhat \le 1 - (\usevar - x_{i,j} \cdot \usevar) \hspace{1cm} i \in \data, \; j \in \prot \label{eq:mio_positive} \\
    & \yhat \ge 1 - \sum_{j \in \prot}(\usevar - x_{i,j} \usevar) \hspace{1.8cm} i \in \data \label{eq:mio_negative} \\
    & \sum_{i \in \data} \yhat \ge \minSize &
    \label{eq:mio_lowerbound} \\
    & 0 \le \yhat \le 1 \hspace{4.1cm}  i \in \data \label{eq:mio_yhatbounds}\\
    & \usevar, b \in \{0, 1\} \hspace{3.7cm} j \in \prot,
\end{align}
\end{subequations}
where $\usevar$ is equal to 1 if and only if feature $j$ is present in the conjunction and $\yhat \in \{0,1\}$ are variables representing whether each sample belongs to the subgroup (i.e., is classified as 1). Indeed, $\yhat$ takes only binary values, due to the constraints on its value, together with the fact that $x_{i,j}$ and $\usevar$ are binary.

The first two constraints---\eqref{eq:mio_abs1} and \eqref{eq:mio_abs2}---formulate the absolute value objective. The binary variable $b$ serves to relax one of the upper bounds, to allow the objective variable $o$ to be constrained by the higher bound out of the two. The value 2 is a tight ``big-M'' value in this context since the difference of the two means takes values from $[-1, 1]$.

Third constraint \eqref{eq:mio_positive} forces $\yhat = 0$ for samples that have $x_{i,j} = 0$ but $\usevar = 1$ for some feature $j \in \prot$, i.e., when the literal for sample $x_i$ on feature $j$ is not satisfied. Since this is a conjunction, a single such event means that the sample will not belong to the subgroup (i.e., will be classified as 0).
The fourth constraint \eqref{eq:mio_negative} ensures $\yhat = 1$ when all literals are satisfied by the sample $x_i$.

Finally, the last constraint \eqref{eq:mio_lowerbound} introduces a lower bound on the number of samples that form a feasible subgroup. This can be used to restrict the search to subgroups that are reliably represented in the sampled data.
% The set of constraints \eqref{eq:mio_force_negerr} is only necessary if $\minSize > 0$. It forces errors for negative samples to 0 when appropriate to ensure that \eqref{eq:mio_lowerbound} is not satisfied incorrectly by setting the error to 1 for some sample $i$ even when the sample is not satisfied by the conjunction.

% Note that although the variables $\errvar$ are defined on a real domain, in the optimal solution, they can only take values in $\{0, 1\}$ because we minimize over their (positively weighted) sum, and they are bounded by 0 or 1 from below.
If all $\usevar = 0$, the formulation represents an empty conjunction that always returns true, i.e., $\yhat = 1$ for all $i \in \data$. This represents the trivial subgroup containing all samples.
% It is trivial to see that the objective function is equivalent to the derived loss equation \eqref{eq:zerooneloss}.

By solving this MIO problem, we find the subgroup with the highest absolute difference in probability of belonging to $\posdistr$ or $\negdistr$, i.e., the empirical estimate of $\MSDdiff$.

\section{Experiments}
\label{sec:experiments}
% To provide an empirical evaluation of the capabilities of our approach, we perform the following experiment.
Our main goal is to measure the distance between distributions, focusing on protected subgroups.
We thus wish to compare $\MSDc$ to other measures of distances on distributions,
as per Table \ref{tab:samplecomplexity}.
%namely Wasserstein-1  \cite{vaserstein1969markov}, Wasserstein-2 \cite{vaserstein1969markov,dudley1969speed}, Total Variation (TV, \cite{zbMATH02706589}) and Maximum Mean Discrepancy (MMD, \cite{gretton2012kernel}).
Since the data we consider is binary or categorical, we utilize the overlap kernel function $k(x_{i_1},x_{i_2}) = \sum_{j = 1}^{\ndims} \istrue{x_{i_1,j} = x_{i_2,j}} / \ndims$ for the MMD evaluation.
We use our own implementation of TV and MMD, and for W$_1$ and W$_2$, we utilize POT \cite{flamary2021pot}, an open-source Python library for the computation of optimal transport.

Additionally, we would like to compare the evaluation of $\MSDc$ utilizing the proposed MIO formulation to other DNF optimization algorithms. We choose the classical algorithm Ripper \cite{ripper} and BRCG \cite{dash2018boolean} as a modern DNF learner.
Since there is no public implementation of the original BRCG, we take the non-MIO re-implementation of BRCG, called BRCG-light \cite{arya2019one} and the Ripper algorithm available in the AIX360 library \cite{arya2019one}. We modify both implementations to return a single-term DNF, to ensure comparability to our method.
As addressed earlier, we run each method two times, once to minimize error and the second time to maximize it (minimize the error with flipped labels). In case one of the distributions has more samples, we subsample it to have an equal number of samples for each distribution. This makes minimizing the 0-1 loss equivalent to finding the $\MSDc$.

To solve the proposed MIO formulation \eqref{eq:mio}, we model it using the Pyomo library \cite{bynumPyomoOptimizationModeling2021} allowing easy use of a variety of solvers. We use the Gurobi solver \cite{gurobi}. We set the $\minSize$ parameter to 10, allowing only subgroups with at least ten samples, although one could also use \eqref{eq:smpl_thm_pr2} and a fixed probability level $\delta$.

\subsection{Datasests}
To showcase the intended use of our method, we use the US Census data via the Folktables library \cite{ding2021retiring}. Specifically, we use two families of five datasets.

The first family consists of the five pre-defined prediction tasks: ACSIncome, ACSPublicCoverage, ACSMobility, ACSEmployment, and ACSTravelTime. We compare the distributions of samples with $y_i = 1$ and $y_i = 0$ on the data of residents of California. For example, in ACSIncome, this means comparing the distributions between people earning more than \$50,000 a year and people earning less, similar to the well-known Adult dataset. For more details see the original paper \cite{ding2021retiring}. To evaluate the distance comparably between methods, we only consider protected attributes, the list of which is in Appendix \ref{app:protected}.

The second family of datasets aims at comparing distributions of population between different US states. We take attributes that could be considered protected (e.g., sex, race, disability, age) and take data for five distinct pairs of states. The state pairs are as follows: Hawaii v. Maine, California v. Wyoming, Mississippi v. New Hampshire, Maryland v. Mississippi, and Louisiana v. Utah. The state selection aims to compare demographically, socioeconomically, and culturally different states, allowing for a comparison of diverse populations across the United States. The complete list of used attributes is in Appendix \ref{app:protected}.

All used data is 1-year horizon data from the survey year 2018. We work with binary data, where we binarize continuous values to 10 equally spaced bins and semantically reduce the cardinalities of certain subgroups. The sample sizes span from around 10,000 to 380,000 and the number of protected features spans from 3 (with around 1,200 possible subgroups) to 14 (with more than $5 \cdot 10^9$ subgroups). Details are in Appendix \ref{app:data}.

\subsection{Setup}
We aim to see how the number of samples influences the value of the distance. Ideally, one would need only a few samples to compute the true distance of the distributions.
For that, each dataset is subsampled to form 5 (smaller) datasets, with sizes forming a geometric sequence from 1000 samples to the maximum number of samples for the given dataset.
Each method is evaluated on each dataset.
We ran each experiment configuration five times with different random seeds.

Each experiment was run on a cluster. Each node had 32 GB of RAM and either AMD EPYC 7543 or Intel Xeon Scalable Gold 6146 processor. Each distance computation had a time limit of 10 minutes.
All code is openly available on GitHub\footnote{\url{https://github.com/Epanemu/DNF_bias}}.
%\jiricomment{@Illia will clean up the code}.

\subsection{Results}
Since most evaluated methods have different notions of distance, it is difficult to compare them directly. Thus, we present relative distances w.r.t. the best estimate of the distance. In other words, we divide each of the values by the mean distance computed on the entire dataset. Or, if that value cannot be computed, we take the mean value on the second-largest number of samples. For original distance measure comparison, see Figure \ref{fig:base} in Appendix \ref{app:base_distances}.

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figures/sample_complexity/2025-01-30final_complexity_relative.pdf}
    \caption{Mean relative distance measured by each method. The datasets in the top row are standard datasets provided by the Folktables library. The bottom row shows datasets from the US Census, where we consider many protected attributes and compare their distributions of a pair of US states. Dashed lines represent methods that find a subgroup and for which we report the $\MSDc$ measure. The color bands represent the standard deviation. Ideally, each relative distance estimate should be 1, meaning that the estimate is the same as the estimate on the highest number of samples.}
    \label{fig:rel_dist}
\end{figure*}

The means over 5 random seeds are presented in Figure \ref{fig:rel_dist}. We report Ripper and BRCG results on different numbers of samples because these methods require equal sample sizes for both distributions, so we subsample the distribution for which we have more samples. This is especially pronounced in the California X Wyoming comparison due to the difference in population size. The color bands represent the standard deviations.
%
The BRCG and Ripper often vary in the quality of the distance estimate and have a rather high standard deviation. This is due to the methods being influenced by the randomness of the data. However, nonzero deviation on the last sample size (where data is the same for all seeds) suggests an effect of randomness within the algorithm or the ordering of the data.
%
Clearly, TV and Wasserstein-based measures do not converge quickly to the final value. It is possible that the best estimate is still far from the true distance measure. This shows that we cannot be certain of the true value of the distance measure.

Finally, MMD shows comparable performance to our MIO-based $\MSDc$, both in deviation and in convergence. While this seems to empirically point to low sample complexity on these datasets, one cannot be certain that this holds always. In comparison, this is the case for our $\MSDc$, with high probability.

The MIO-based $\MSDc$ almost always finds the global optimum within 10 minutes.
% The only exceptions were the datasets California X Wyoming, Maryland X Mississippi, and Louisiana X Utah, where 8, 3, and 2 setups did not prove optimality in time, respectively.
% This happened for the setups with many dimensions and many samples, which was expected.
The only exceptions were 13 seeded setups on three datasets with both many protected attributes and many samples, which could be expected.
Luckily, we show that we do not need a high number of samples to provide a good estimate of $\MSDc$.
Additionally, providing the solver with an optimal solution on the smaller sample size could be used to prune some branches in the solver and would likely help with runtime, even on bigger instances.
In addition, the strong performance of our method means that we are also able to recover the correct subgroup with maximal distance even with exponentially fewer samples than the maximum over the dataset.

\section{Conclusion}
\label{sec:conclusion}
We pointed out the difficulties of evaluating the distance of probability distributions by current methods, especially in the context of evaluating bias for intersectional subgroups. We proposed a family of distance measures MSD$_\Delta$ as the maximal distance $\distance$ over all subgroups.

We further defined a specific $\MSDc$ and proved its linear sample complexity, in contrast to the exponential sample complexities of existing methods. We validated this on datasets of real US Census data with varying numbers of protected attributes and sample sizes.
Only MMD showed empirically comparable performance to our MIO-based $\MSDc$, but it does not provide as good of an interpretation, that could be useful in bias evaluation.

The MIO formulation also outperforms more general DNF learners, which also lack guarantees due to not being global optimizers.
In addition, the global optimality of MIO does not necessarily come at a disadvantage as is otherwise common. That is because the linear sample complexity means that we do not need exponentially many samples, thus the formulation remains smaller and solves faster.

\paragraph{Future work}
As we have shown, evaluating bias in subgroups is an intrinsically difficult task. There are many challenges that require more work, one of which is the training of ML models under intersectional fairness constraints. We believe $\MSDc$ could be a good measure for learning with fairness guarantees.
Additionally, there may be other $\distance$ functions that could be efficiently used for MSD$_\distance$ measures, e.g., the SPSF \cite{kearns2018preventing}.

\section*{Impact Statement}
% does not count towards the 8-page limit
By its nature, the positive potential societal impact of Algorithmic Fairness is well understood, and consequently, the field has been intensively developed in the past decade.
The choice of a bias measure has a substantial impact on the evaluation of any AI system. Our bias measure facilitates the study of intersectional fairness of \cite{crenshaw2013demarginalizing}, which would be intractable with many other means of evaluation of bias.

\section*{Acknowledgment}
This work has received funding from the European Union’s Horizon Europe research and innovation
programme under grant agreement No. 10107056.

\bibliographystyle{plainnat}
% \bibliographystyle{icml2025}
\bibliography{refs.bib}

\clearpage
\onecolumn

\appendix
\section{Appendix}
\subsection{Datasets description}
\label{app:data}
We utilize 2 types of datasets, both constructed from the US Census data - American Community Survey (ACS). The first 5 (with names starting with ACS) represent tasks constructed by the creators of the folktables library \cite{ding2021retiring}. We simply access it, taking 1-year survey data of Californians for the year 2018.

The datasets of the other type are constructed by us using the folktables API from the same data source. We select five pairs of distinctive US states and compare their data distributions. We simply take a random half of the available data for each state and combine it to form our dataset. Each dataset has the same attributes, listed in Table \ref{tab:attrs}.

The dataset statistics are in Table \ref{tab:datasets}. Note the Hawaii X Maine dataset for an illustration of the difficulty of assessing the subgroup bias. It is quite difficult to evaluate the bias of more than 400,000x more groups than there are samples.

\begin{table}
    \centering
    \caption{Datasets, sorted by sample size. Datasets starting with ACS are the original tasks, provided by the folktables library \cite{ding2021retiring}. (CA) means that data is taken from the state of California. The bottom half of the datasets are constructed by us from the US Census data, through the folktables library. We compare states using all of the protected attributes from Table \ref{tab:attrs}. Datasets are in the order in which they appear in the plots. The number of subgroups may change even for the same attributes when some categorical value is missing from the data for one dataset/task and not for the other. \vspace{0.1in}}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{lccc}
    \toprule
         Name & Number of samples & Protected attributes & Number of subgroups \\
         \midrule
         ACSIncome (CA) & 195,665 & 4 & 1,229 \\
         ACSPublicCoverage (CA) & 138,554 & 12 & 56,008,799 \\
         ACSMobility (CA) & 80,329 & 11 & 11,567,699 \\
         ACSEmployment (CA) & 378,817 & 11 & 18,381,599 \\
         ACSTravelTime (CA) & 172,508 & 6 & 47,339 \\
         \midrule
         Hawaii X Maine & 13,837 & 14 & 5,772,124,799 \\
         California X Wyoming & 192,278 & 14 & 5,772,124,799 \\
         Mississippi X New Hampshire & 21,452 & 14 & 5,772,124,799 \\
         Maryland X Mississippi & 44,482 & 14 & 5,772,124,799 \\
         Louisiana X Utah & 37,595 & 14 & 5,772,124,799 \\
        \bottomrule
    \end{tabular}
    }
    \label{tab:datasets}
\end{table}

\subsubsection{Protected attributes}
\label{app:protected}
Below, we list the enumerate the attributes for each dataset. For more discussion and statistics of the attributes, see Table \ref{tab:attrs}.

\begin{itemize}
    \item ACSIncome (CA): \texttt{AGEP}, \texttt{POBP}, \texttt{SEX}, \texttt{RAC1P}
    \item ACSPublicCoverage (CA): \texttt{AGEP}, \texttt{SEX}, \texttt{DIS}, \texttt{CIT}, \texttt{MIL}, \texttt{ANC}, \texttt{NATIVITY}, \texttt{DEAR}, \texttt{DEYE}, \texttt{DREM}, \texttt{FER}, \texttt{RAC1P}
    \item ACSMobility (CA): \texttt{AGEP}, \texttt{SEX}, \texttt{DIS}, \texttt{CIT}, \texttt{MIL}, \texttt{ANC}, \texttt{NATIVITY}, \texttt{DEAR}, \texttt{DEYE}, \texttt{DREM}, \texttt{RAC1P}
    \item ACSEmployment (CA): \texttt{AGEP}, \texttt{DIS}, \texttt{SEX}, \texttt{CIT}, \texttt{MIL}, \texttt{ANC}, \texttt{NATIVITY}, \texttt{DEAR}, \texttt{DEYE}, \texttt{DREM},  \texttt{RAC1P}
    \item ACSTravelTime (CA): \texttt{AGEP}, \texttt{SEX}, \texttt{DIS}, \texttt{RAC1P}, \texttt{CIT}, \texttt{POVPIP}
    \item State X State: Every dataset with pairs of states has all 14 attributes shown in Table \ref{tab:attrs}.
\end{itemize}


\begin{table}
    \centering
    \caption{Protected attributes. Most attributes are commonly considered protected, with multiple being related to race and nationality. We consider Military service protected, due to existing laws against discrimination based on veteran status. The rationale behind protecting the feature of recently giving birth is linked to not uncommon discrimination in the workplace based on the prospect of women being in fertile age. For deeper insight, refer to the official documentation of the ACS data source. \vspace{0.1in}}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{llll}
    \toprule
         ACS Code & Name & Number of values & Reduced number of values \\
         \midrule
        \texttt{SEX} & Sex & 2 & 2 \\
        \texttt{RAC1P} & Race & 9 & 9 \\
        \texttt{AGEP} & Age & continuous & 10 (binned) \\
        \texttt{POBP} & Place of birth & 221 & 6 (grouped by continent) \\
        \texttt{DIS} & Disability & 2 & 2 \\
        \texttt{CIT} & Citizenship & 5 & 5 \\
        \texttt{MIL} & Military service & 5 & 5 \\
        \texttt{ANC} & Ancestry & 4 & 4 \\
        \texttt{NATIVITY} & Foreign or US native & 2 & 2 \\
        \texttt{DEAR} & Difficulty hearing & 2 & 2 \\
        \texttt{DEYE} & Difficulty seeing & 2 & 2 \\
        \texttt{DREM} & Cognitive difficulty & 3 & 3 \\
        \texttt{FER} & Gave birth last year & 3 & 3 \\
        \texttt{POVPIP} & Income / Poverty threshold & continuous & 10 (binned) \\
        \bottomrule
    \end{tabular}
    }
    \label{tab:attrs}
\end{table}

\subsection{More results}
\label{app:more_res}

\subsubsection{Original distances}
\label{app:base_distances}
In addition to the results in the main body of the paper, we present the results on the true distances, before ``normalization'' by the best-estimated distance, in Figure \ref{fig:base}. It is indeed difficult to compare the methods together, one might just comment on the distances separately.
For example, note the range of estimates of TV, oftentimes, the value changes by 0.3 from the first to last estimate, which is essentially a third of the range of feasible values TV can take.

Also, one can notice that Wasserstein metrics and the MMD are not well-suitable for the evaluation of intersectional bias by comparing results on Hawaii X Maine with Mississippi X New Hampshire. Wasserstein metrics, as well as MMD, evaluate the distance between 0.15 and 0.2 for both datasets, while the most discrepant subgroup has the $\MSDc$ distance of cca 0.7 in one and cca 0.3 in the other, less than a half.

Lastly, notice that our proposed MIO-based measure always finds the highest $\MSDc$ out of the three optimizers of conjunctions. This is due to the global optimality of the solutions. Out of the other two methods, Ripper seems to be performing better.
%\begin{figure}
%    \centering
%    \includegraphics[width=0.6\linewidth]{figures/sample_complexity/2025-01-30_enumeration.pdf}
%    \caption{Comparison of the number of considered subgroups, compared to the total number of subgroups.
    % The best enumerative approach is capable of considering 1000 times fewer sgs - extrapolating the time linearly, one would need about a full week to finish
    % and this is doable - most likely due to the limit on the number of samples and a small sample size.
%    }
%    \label{fig:enum}
%\end{figure}

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figures/sample_complexity/2025-01-30final_complexity_base.pdf}
    \caption{Original distances for all methods.}
    \label{fig:base}
\end{figure*}

\subsubsection{RSE}
Finally, we present the results by evaluating and plotting the Relative Standard Error (RSE) in Figure \ref{fig:RSE}. It shows the stability of the estimate over various seeds. The methods evaluating $\MSDc$ seem to generally struggle more, which might be a feature of the measure, taking the supremum rather than some mean value. This could likely make the measure more ``volatile'' using different seeds.
Note, however, that MMD has comparable RSE, despite being a measure computed as a mean value.

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figures/sample_complexity/2025-01-30final_complexity_RSE.pdf}
    \caption{Relative standard error of all methods.}
    \label{fig:RSE}
\end{figure*}

\end{document}