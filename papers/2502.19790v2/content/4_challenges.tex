\section{Current Challenges}\label{sec:status_quo}


We identify three challenges in the status quo of training data management with current open-source infrastructure.

\textbf{\underline{Challenge 1:} High engineering effort for data preparation.} 
The current approach to preparing training data sets involves many manual offline steps with general-purpose data processing and scripting frameworks (\Crefwl{fig:intro-worklfow}{a}).
For the offline cleaning step (\Cref{subsec:back-datamix}), ML engineers typically leverage data processing frameworks like Spark~\cite{Zaharia2016Spark}, Beam~\cite{Akidau2015DataflowBeam}, Data-Juicer~\cite{Chen2024DataJuicer}, or datatrove~\cite{Penedo2024DataTrove}.

The subsequent data mixing can happen offline or online.
If data is mixed offline, engineers write ad hoc scripts  which create a new mixed copy of the cleaned dataset for each training run.
This makes it difficult to get a quick sense of how a data mixture will impact model training when exploring different mixing policies.
Some existing data loaders support online data mixing.
However, they only implement it across directories, i.e., they assign a weight to each directory and sample data according to the weights.
Hence, the directories need to reflect the property we want to mix on, e.g., one directory per language or data source.
This inflexible approach neither supports switching the property we mix on nor supports specifying hierarchical mixtures across arbitrary properties, e.g., specify a proportion between Wikipedia and movie subtitles, \emph{and} between English and German (c.f.~\Cref{tab:intro-features}).

\textbf{\underline{Challenge 2:} Inefficient data management on filesystems.} 
Training data is typically stored and managed as files without a proper data management system, which can lead to \textit{storage overhead,  performance bottlenecks, and consistency issues}.
Both the source data and mixed training set are typically stored in formats such as \texttt{jsonl} or \texttt{parquet} files in a shared, distributed filesystem.
Existing data loaders just wrap around the filesystem and are therefore limited by its constraints (\Cref{tab:intro-features}).
Filesystems are commonly used because current database management systems are not natively built for foundation model training data~\cite{Wang2023LLMDataSurvey}.
Using a proper DBMS for such data would require ML developers to define table schemas and to orchestrate dataflow from the DBMS to the model training, in addition to administration overhead to operate the DBMS itself.
This situation is problematic, as (i) both the metadata and actual payloads are commonly duplicated across training jobs, (ii) filesystems do not provide an efficient interfaces to query data, (iii) there is no native way of tracking which model was trained on which data if it is accessed via general-purpose filesystem calls.
Some frameworks like Megatron~\cite{Shoeybi2019Megatron,Narayanan2021Megatron} even require pre-tokenized data, which leads to duplication of the even bigger tokenized data files.




\textbf{\underline{Challenge 3:} Rapidly evolving research.} 
How to train the best model on a given dataset is an active area of research, with many new techniques such as dynamic mixing emerging.
Offline preparation of the mixture or online mixing based on fixed directory weights does not support dynamic mixture at all.
Even for researchers who are familiar with the latest mixing techniques, implementing modern mixing algorithms in a training pipeline is a painful, tedious, and error-prone task.
The codebases for mixing algorithms are often tailored directly to the training framework, as well as the data collection and properties used in the respective papers.
This hinders the adoption of new methods and makes researching, reproducing, and comparing different strategies difficult.
