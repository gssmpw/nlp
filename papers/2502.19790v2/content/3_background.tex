\section{Background}\label{sec:background}




Foundation models are large-scale deep learning models suitable for a variety of tasks~\cite{Bommasani2022FMs,CMA2013FM}.
We focus on text-generation models, i.e., autoregressive large language models (LLMs) and multimodal vision-language models (VLMs).
As of 2025, most such models are based on the Transformer architecture~\cite{Vaswani2017Attention}.
They are trained on huge corpora of training data in a self-supervised manner to maximize the likelihood of predicting the tokens of a training sequence.

\textbf{Training phases.} Training is structured into \emph{pre-training} and \emph{post-training} phases.
In pre-training, we train a randomly initialized model on a general purpose data corpus (\Cref{subsec:back-datamix}) to derive a \emph{base model}.
In post-training, common steps include \emph{supervised finetuning} (SFT) and \emph{alignment}.
The data used in SFT is human-selected for a specific task.
In this paper, we focus on the pre-training use case.



\textbf{Distributed training.} Training foundation models requires distributing computation across multiple GPUs. %
Training frameworks typically employ 3D parallelism~\cite{BenNun2019Parallel,Hoefler20243D}, consisting of pipeline parallelism (PP), i.e., partioning the model layers between devices~\cite{Narayanan2021Megatron,Huang2019GPipe,Narayanan2019PipeDream}, tensor parallelism (TP), i.e., splitting individual tensor operations within layers across devices~\cite{Shazeer2018MeshTF,Shoeybi2019Megatron}, and data parallelism (DP), i.e., replication of the model across device groups.
PP and TP together are referred to as \emph{model parallelism}.
Nodes within the same DP group process identical inputs, while nodes across DP groups receive different data.
As an extension to DP, fully-sharded data parallelism (FSDP) shards model parameters, gradients, and optimizer states across data-parallel workers~\cite{Zhao2023FSDP}.


\subsection{Training Data and Data Mixing}\label{subsec:back-datamix}

The data used for pre-training stems from data collections that include data from various sources, such as Wikipedia, Common Crawl dumps, or arXiv papers.
Public examples of such collections include
RedPajama~\cite{together2023redpajama}, Dolma~\cite{Soldaini2024Dolma}, and FineWeb~\cite{Penedo2024FineWeb}.
Besides the aggregation of data from different sources, data engineers typically clean the data, which typically involves deduplicating, filtering (e.g., the removal of personal identifiable information - PII), and applying classifiers to the data samples (e.g., to obtain a toxicity score for each sample).
Longpre et al.~\cite{Longpre2024Guide} and Penedo et al.~\cite{Penedo2024FineWeb} provide a great overview of the processing steps and their impact. 

\textbf{Data properties and mixtures.} Each sample in the resulting collection has properties, such as its source (e.g., Wikipedia) or its language (e.g., English).
Beyond the filtering operations, ML engineers need to define a \emph{data mixture}.
A mixture describes how the data is mixed based on their characteristics, i.e., we might train on 50\,\% data from Common Crawl and 50\,\% from movie subtitles. 
The data can be combined based on multiple characteristics simultaneously.
For instance, besides Common Crawl and movie subtitles, we might also use 80\,\% French and 20\,\% Italian data.
The granularity on which the mixture is ensured depends on each training setup and there is no common agreement.
For example, a mixture could be ensured within a batch, or across a window of several batches.

\textbf{Mixing algorithms.}
Selecting the best mixture is critical for model performance~\cite{chen2023skillit,ye2024datamixinglaws,xie2024doremi,Shen2024SlimPajamDC}. 
We differentiate \emph{static mixtures}, i.e., mixtures that remain constant over the entire training job, and \emph{dynamic mixtures}, i.e., mixtures that change during the training job.
In the last years, several algorithms for finding the best static mixture or how to adjust the mixture dynamically during training have been proposed.
Algorithms such as \textsc{DoReMi}~\cite{xie2024doremi} or the data mixing laws~\cite{ye2024datamixinglaws} find the a static mixture via small proxy models.

Curriculum learning is an example of a pre-defined dynamic mixture.
Xu et al.~\cite{Xu2024Currciulum} order samples from \emph{easy} to \emph{hard} to improve alignment.
The \textsc{SmolLM2} model was trained on 4 stages of mixtures~\cite{Allal2025SmolLM2}.
Multilingual models are often first trained on English data, and then data from other languages is included~\cite{Richburg2024MLFinetune,Xu2024MultilingualShift}.

Beyond such pre-defined schedules, there is also work on adapting the data mixture to the model training dynamics, e.g., by increasing the weight of data domains which have high loss.
Albalak et al.~\cite{Albalak2024OnlineMixing} use a multi-armed bandit strategy to adjust the mixture.
\textsc{Skill-it} orders \enquote{skills} based on model feedback~\cite{chen2023skillit}.
\textsc{Aioli} builds upon \textsc{Skill-it} and provides a unified framework for estimating the best mixture during training~\cite{Chen2024Aioli}.
\textsc{PiKE} relies on gradient interactions~\cite{Li2025Pike}.
In this paper, we use Adaptive Data Optimization (ADO)~\cite{Jiang2024ADO} as an example of a dynamic mixing algorithms.


\subsubsection{Adaptive Data Optimization} \label{subsubsec:back-ado}

Adaptive Data Optimization (ADO) is a dynamic mixing algorithm that adjusts the data mixture during training based on the model's learning progress on each domain~\cite{Jiang2024ADO}. 
The key idea is to prioritize domains where the model shows rapid improvement while considering how much each domain contributes to its own progress.
ADO uses neural scaling laws to model how the loss $L_k$ of each domain $k$ decreases with the number of training samples $n$. 
To this end, it fits a power law $\hat{L}_k(n) = \varepsilon_k + \beta_k n^{-\alpha_k}$ \emph{for each domain}.
Here, $\varepsilon_k$ represents the irreducible loss of the domain, $\beta_k$ is a scaling factor, and $\alpha_k$ determines how quickly the loss decreases.
The parameters are re-fitted during training.
The algorithm combines two components to determine the mixture weights.
First, it estimates the learning speed for each domain using the derivative of the scaling law.
Second, ADO maintains a credit assignment score $\lambda_k(t)$ that indicates how much each domain contributes to its own progress, based on its recent sampling frequency.
These components are combined with a prior (initial) distribution $\mu_k$ to compute an intermediate preference distribution  $\rho_k(t)$.
To ensure stability, the final distribution $\pi_k(t)$ is then computed as a weighted average between $\rho_k(t)$ and $\pi_k(t)$'s temporal average.
Additionally, ADO enforces a minimum sampling probability for each domain. 
For more details, we refer to Jiang et al.~\cite{Jiang2024ADO}.
