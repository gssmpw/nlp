\section{Introduction}\label{sec:intro}

Large language and vision models (often called foundation models) have become omnipresent in our daily life.
They show enormous capabilities in a diverse set of tasks~\cite{Brown2020LLMFewShot,Bommasani2022FMs,OpenAI2024GPT4,dubey2024llama3herdmodels,radford2019language}, such as assistance with writing and coding,  video understanding, and even agentic interaction with the world.
The training of such language and vision models (LLMs/VLMs) presents new challenges for managing training data due to the ever-growing sizes of models and datasets.
To achieve high accuracy, state-of-the-art models train over trillions of tokens from aggregated data collections such as RedPajama~\cite{together2023redpajama}, Dolma~\cite{Soldaini2024Dolma}, or FineWeb~\cite{Penedo2024FineWeb}.
For example, Meta's Llama 3.3 70B model is trained on a corpus of 15 trillion tokens~\cite{dubey2024llama3herdmodels,Llama33ModelCard}.
These collections are built based on data from various sources, such as Wikipedia or Common Crawl dumps.

\begin{figure}
    \begin{adjustbox}{trim=0cm 0.4cm 0cm 0cm}
                \includesvg[width=\linewidth]{img/intro_hellaswag.svg}
    \end{adjustbox}
    \caption{Dynamically adjusting the mixture using the ADO algorithm improves pre-training performance on HellaSwag over the default static mixture across model scales.}
    \vspace{-0.5cm}
    \label{fig:intro-motivation}
\end{figure}

Training data is typically stored on distributed filesystems in GPU clusters or data lakes in the cloud, and managed manually and ad hoc by ML engineers (\Crefwl{fig:intro-worklfow}{a}). 
Selecting the right proportions out of this data with particular characteristics (e.g., language, topic, source) for training is critical for model performance~\cite{chen2023skillit,ye2024datamixinglaws,xie2024doremi}.
Individual users write ad hoc scripts to process the training data, filter relevant subsets with the properties of interest, often pre-tokenize it, and then mix it for their use case, e.g., they might want to train on 50\,\% data from Wikipedia and 50\,\% from movie subtitles.
This reflects our experience working with ML researchers as part of the Swiss AI initiative which aims to develop open-source LLMs~\cite{SwissAI}.
We confirmed through discussions with industry teams that such setups are common.
This can get complex as training data may need to be mixed based on a variety of characteristics.
For example, in addition to satisfying source data proportions (Wikipedia vs. movie subtitles in the previous example), we may also want the training data to be 80\,\% in English and 20\,\% in German. 
Recent works show that the data mixture should also be adjusted during training.
The \textsc{SmolLM2} model is trained with four stages of data mixtures~\cite{Allal2025SmolLM2}.
Algorithms such as Adaptive Data Optimization (ADO)~\cite{Jiang2024ADO}, \textsc{Aioli}~\cite{Chen2024Aioli}, \textsc{PiKE}~\cite{Li2025Pike}, and \textsc{Skill-It}~\cite{chen2023skillit} even propose to adjust the mixture dynamically based on the model behavior (e.g., loss per domain) during training.
~\Cref{fig:intro-motivation} shows that using ADO can increase the performance of LLMs over a static mixture across different scales on the downstream HellaSwag benchmark~\cite{Zellers2019HellaSwag}.

Today, there is no open-source system that automatically manages vast amounts of training data based on fine-grained characteristics.
Implementing data filtering and subsequent mixing requires users to manually keep track of metadata. 
At least in the open-source world, this is typically done as part of the directory structure of the filesystem, e.g., there is one subdirectory per source dataset, and then we sample from each directory (\Crefwl{fig:intro-worklfow}{a}).
This approach is limited because each data sample has multiple properties that can be used to determine whether it should be used for training. 
Filesystems fundamentally do not offer the right interface for managing training data and mixing, as they do not provide declarative query interfaces or a native way to track which model was trained on what data.
Running a data processing job to fully materialize the mixed training set for each training run has lots of overhead and leads to data duplication. 
A streaming-oriented approach that selects, tokenizes, and mixes data on the fly provides much more flexibility and eases experimentation.
Utilizing a full-fledged DBMS for tracking data properties would burden ML engineers with complexities such as database administration, schema design, and performance tuning.
We need a lightweight data plane that enables users to declaratively query and mix data based on arbitrary properties, independent of the filesystem structure, and to adjust this mixture dynamically.



\begin{figure}
   \centering
   \subfloat[a][Without \mixtera: More developer, CPU, and disk resources needed.]{\includegraphics[width=0.95\linewidth]{img/before_mixtera.pdf} \label{fig:intro-worklfow:a}} \\
   \vspace{-0.3cm}
   \subfloat[b][With \mixtera: Declarative mixture specification and low overhead.]{\includegraphics[width=0.95\linewidth]{img/after_mixtera.pdf} \label{fig:intro-worklfow:b}}
   \vspace{-0.1cm}
   \caption{\mixtera{} needs less offline processing and scripting.} \label{fig:intro-worklfow}
   \vspace{-0.65cm}
 \end{figure}

\begin{table*}[b]
\caption{Feature comparison of \mixtera{} and other open-source data loaders.}
\vspace{-0.3cm}
\small
\label{tab:intro-features}
\begin{tabular}{@{}cllll@{}}
\toprule
\textbf{}                                           & \multicolumn{1}{c}{\textbf{\mixtera}}                                       & \multicolumn{1}{c}{\textbf{HF Datasets}}                                        & \multicolumn{1}{c}{\textbf{WebDatasets}}          & \multicolumn{1}{c}{\textbf{Mosaic Streaming}}                           \\ \midrule
\multicolumn{1}{c|}{\textbf{File formats}}          & \begin{tabular}[c]{@{}l@{}}jsonl(.zst), parquet,\\ webdataset\end{tabular} & \begin{tabular}[c]{@{}l@{}}jsonl(.zst), parquet,\\ webdataset, csv\end{tabular} & webdataset                                        & \begin{tabular}[c]{@{}l@{}}Mosaic Data Shard,\\ jsonl, csv\end{tabular} \\
\multicolumn{1}{c|}{\textbf{Static filtering}}      & {\color[HTML]{2C7BB6} declarative}                                         & {\color[HTML]{FDAE61} using map UDFs}                                           & {\color[HTML]{FDAE61} using map UDFs}             & {\color[HTML]{FDAE61} using map UDFs}                                   \\
\multicolumn{1}{c|}{\textbf{Static mixtures}}       & {\color[HTML]{2C7BB6} on all properties}                                   & {\color[HTML]{FDAE61} on filesystem dirs.}                                      & {\color[HTML]{FDAE61} on filesystem dirs.}        & {\color[HTML]{FDAE61} on filesystem dirs.}                              \\
\multicolumn{1}{c|}{\textbf{Dynamic mixtures}}      & {\color[HTML]{2C7BB6} on all properties}                                   & {\color[HTML]{D7191C} no}                                                       & {\color[HTML]{D7191C} no}                         & {\color[HTML]{D7191C} no}                                               \\
\multicolumn{1}{c|}{\textbf{Native 3D parallelism}} & {\color[HTML]{2C7BB6} yes}                                                 & {\color[HTML]{FDAE61} yes, manual rank handling}                                & {\color[HTML]{FDAE61} data parallel only}         & {\color[HTML]{FDAE61} yes, for specific rank order}                     \\
\multicolumn{1}{c|}{\textbf{Checkpointing}}         & {\color[HTML]{2C7BB6} yes}                                                 & {\color[HTML]{FDAE61} using TorchData}                                          & {\color[HTML]{FDAE61} by replaying, not natively} & {\color[HTML]{2C7BB6} yes}                                              \\ \bottomrule
\end{tabular}
\end{table*}

We present \mixtera, a data plane that can be deployed on top of existing LLM/VLM training data collections.
It is a centralized, read-only layer and can be declaratively queried from training clients.
As shown in~\Crefwl{fig:intro-worklfow:b}, it reduces the amount of materialization and data scripting necessary for training jobs.
While existing data loaders as outlined in~\Cref{tab:intro-features} are limited by filesystem constraints, \mixtera{} supports arbitrary properties independent of the filesystem, and makes it easy for model engineers to experiment with different filter criteria, fixed learning curricula, and fully dynamic mixing algorithms that learn the mixture during training.
\mixtera{} follows a client-server model.
For a training job, the server, which indexes and manages the samples, first statically filters out the relevant samples, and then continuously distributes \emph{chunks} to the clients.
Chunks are fixed-size collections of \emph{pointers} to samples in files, and adhere to the current mixture.
The clients then fetch the relevant samples and return the relevant data to the training loop.
We design and implement \mixtera{} tailored to the needs for foundation model training, and make the following key contributions:
\begin{enumerate}[leftmargin=*,nosep]

\item \mixtera{} enables users to declaratively specify mixtures across properties independent of the filesystem structure and training frameworks by indexing all samples and their properties.


\item \mixtera{} enables dynamically changing the data mixture and the properties determining the mixture during training. 
It achieves this by generating and streaming chunks, i.e., fixed-size lists of pointers to samples following the current mixture.





\item We show \mixtera{} does not bottleneck training and is versatile, enabling model accuracy improvements for text-only and text-vision models.
As an example, we demonstrate how to implement the ADO dynamic data mixing algorithm in \mixtera{} and its positive impact on model accuracy.



\end{enumerate}

