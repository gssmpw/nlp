\section{Evaluation}\label{sec:eval}
We evaluate \mixtera{} to answer the following questions:


\begin{enumerate}[leftmargin=*,nosep]%
    \item How can we integrate dynamic mixing algorithms into \mixtera{} and what role do mixtures play for model accuracy?    

    
    \item How does \mixtera's throughput compare to other data loaders and how well does it scale?
\end{enumerate}


\begin{table}
\caption{Model configurations.}
\vspace{-0.3cm}
\label{tab:models}
\begin{adjustbox}{max width=\linewidth}
\centering
\begin{tabular}{@{}crrrrrr@{}}
\toprule
           & \multicolumn{1}{c}{\textbf{Hid. Dim.}} & \multicolumn{1}{c}{\textbf{Interm. Dim.}} & \multicolumn{1}{c}{\textbf{KV-Hds.}} & \multicolumn{1}{c}{\textbf{Q-Hds.}} & \multicolumn{1}{c}{\textbf{Layers}} & \multicolumn{1}{c}{\textbf{RoPE-$\theta$}} \\ \midrule
{\ul 162M} & 768                                    & 2\,048                                    & 12                                   & 12                                  & 12                                  & 10\,000                                    \\
{\ul 1.6B} & 2\,048                                 & 5\,464                                    & 16                                   & 16                                  & 24                                  & 10\,000                                    \\
{\ul 3.6B} & 3\,072                                 & 8\,192                                    & 8                                    & 24                                  & 28                                  & 500\,000                                   \\ \bottomrule
\end{tabular}
\end{adjustbox}
\end{table}

We explore the first question in a dynamic mixture case study on LLMs (\Cref{subsec:eval-ado}) and a static mixture case study for VLMs (\Cref{subsec:eval-multimodal}); the second question is explored in throughput benchmarks (\Cref{subsec:eval-perf}).
We conduct our experiments on the Clariden partition of the Alps supercomputer of the Swiss National Supercomputer Centre.
Clariden consists of HPE Cray Supercomputing EX254n blades, each hosting two Quad GH200 nodes.
Each node contains 4 interconnected groups of a Grace CPU with 72 cores, 128\,GB DRAM, and a H100 Hopper GPU with 96\,GB of HBM.
The nodes are connected using 200\,Gb/s HPE Slingshot interconnect.
We refer to Fusco et al.~\cite{Fusco2024GH} for a more detailed analysis of the Alps supercomputer.
The machines run Ubuntu Server 24.01 LTS with kernel 5.14.21, and we build upon the NVIDIA NGC 25.01 container with Python 3.12, a nightly build of PyTorch 2.6, NVIDIA driver 550.54.15 and CUDA 12.8.
We add support for \mixtera{} and other data loaders on our fork of torchtitan (commit \texttt{d989842})\footnote{Available at \url{https://github.com/eth-easl/torchtitan-mixtera}.}~\cite{Liang2024Torchtitan}.
Torchtitan is part of the Pytorch ecosystem and straightforward to set up. %
We use Llama3-like models whose configurations can be found in \Cref{tab:models}.
The 162M and 1.6B models are based on Jiang et al.~\cite{Jiang2024ADO}, while 3.6B follows Meta's Llama 3.2 model.
They do not have the same parameter count as torchtitan does not tie the weight embeddings.
Our training and benchmarking data is based on The Pile~\cite{Gao2020Pile}.
We heuristically split long samples with more than 3000 words, with max. 10\,k samples per file.

\subsection{Dynamic Mixing using ADO}\label{subsec:eval-ado}



We first demonstrate how to implement dynamic mixing algorithms in \mixtera{}, taking the ADO algorithm (\Cref{subsubsec:back-ado}) as an example. We show how dynamic mixing can improve model performance during pre-training. Dynamic mixing algorithms are an active area of research~\cite{Jiang2024ADO,Chen2024Aioli,chen2023skillit,Li2025Pike,Albalak2024OnlineMixing} and \mixtera{} facilitates exploration.

\begin{table*}
\caption{Task performance and perplexities across models, checkpoints, and mixtures. $\uparrow$/$\downarrow$ indicate higher/lower is better.}
\vspace{-0.3cm}
\label{tab:eval-downstream-ado}
\begin{adjustbox}{max width=0.99\linewidth}
\begin{tabular}{lllcccccccc||cc}
\toprule
Model & Steps & Mixture & HellaSwag~$\scriptstyle\uparrow$ & WinoGrande~$\scriptstyle\uparrow$ & ARC-E~$\scriptstyle\uparrow$ & ARC-C~$\scriptstyle\uparrow$ & LogiQA2~$\scriptstyle\uparrow$ & Lambada (OAI)~$\scriptstyle\uparrow$ & OpenBookQA~$\scriptstyle\uparrow$ & PIQA~$\scriptstyle\uparrow$ & SlimP. PPL~$\scriptstyle\downarrow$ & Pile PPL~$\scriptstyle\downarrow$ \\
\midrule
\multirow{6}{*}{162M} & \multirow{3}{*}{15\,k} & ADO & \textbf{0.28} & 0.49 & 0.43 & \textbf{0.20} & \textbf{0.23} & \textbf{0.29} & \textbf{0.19} & \textbf{0.61} & 58.19 & \textbf{66.88} \\
 &  & Default & \textbf{0.28} & \textbf{0.52} & \textbf{0.44} & 0.19 & \textbf{0.23} & \textbf{0.29} & 0.18 & \textbf{0.61} & 59.13 & 69.52 \\
 &  & Natural & \textbf{0.28} & \textbf{0.52} & 0.41 & 0.18 & \textbf{0.23} & 0.26 & 0.16 & \textbf{0.61} & \textbf{57.54} & 69.85 \\
 & \multirow{3}{*}{30\,k} & ADO & \textbf{0.29} & 0.50 & \textbf{0.44} & 0.19 & 0.23 & 0.31 & \textbf{0.17} & 0.62 & \textbf{52.62} & \textbf{61.82} \\
 &  & Default & 0.28 & 0.52 & \textbf{0.44} & \textbf{0.20} & 0.23 & \textbf{0.34} & 0.16 & 0.62 & 53.55 & 64.39 \\
 &  & Natural & \textbf{0.29} & \textbf{0.54} & \textbf{0.44} & 0.19 & \textbf{0.24} & 0.31 & 0.16 & \textbf{0.63} & 52.78 & 65.03 \\
\midrule
\multirow{6}{*}{1B} & \multirow{3}{*}{15\,k} & ADO & \textbf{0.37} & \textbf{0.54} & 0.51 & \textbf{0.23} & \textbf{0.25} & \textbf{0.52} & \textbf{0.20} & \textbf{0.70} & 30.07 & \textbf{32.20} \\
 &  & Default & 0.35 & \textbf{0.54} & \textbf{0.52} & \textbf{0.23} & 0.22 & 0.47 & 0.17 & 0.68 & \textbf{29.34} & 33.02 \\
 &  & Natural & 0.35 & \textbf{0.54} & \textbf{0.52} & \textbf{0.23} & 0.23 & 0.50 & 0.17 & 0.68 & 31.31 & 34.40 \\
 & \multirow{3}{*}{30\,k} & ADO & \textbf{0.40} & 0.55 & 0.55 & \textbf{0.26} & \textbf{0.23} & 0.54 & \textbf{0.23} & \textbf{0.71} & 25.89 & 29.05 \\
 &  & Default & 0.38 & \textbf{0.56} & \textbf{0.56} & 0.24 & 0.22 & 0.56 & 0.21 & 0.70 & \textbf{25.10} & \textbf{28.65} \\
 &  & Natural & 0.38 & \textbf{0.56} & 0.55 & 0.25 & 0.22 & \textbf{0.57} & 0.21 & 0.69 & 26.55 & 29.83 \\
\midrule
\multirow{4}{*}{3B} & \multirow{2}{*}{15\,k} & ADO & \textbf{0.42} & \textbf{0.56} & \textbf{0.57} & \textbf{0.25} & \textbf{0.24} & \textbf{0.59} & \textbf{0.21} & \textbf{0.71} & \textbf{25.78} & \textbf{23.00} \\
 &  & Default & 0.38 & 0.54 & 0.52 & \textbf{0.25} & \textbf{0.24} & 0.50 & 0.20 & 0.68 & 26.21 & 24.73 \\
 & \multirow{2}{*}{30\,k} & ADO & \textbf{0.45} & \textbf{0.59} & \textbf{0.60} & \textbf{0.28} & \textbf{0.24} & \textbf{0.59} & \textbf{0.24} & \textbf{0.73} & \textbf{22.22} & \textbf{20.52} \\
 &  & Default & 0.42 & 0.56 & 0.59 & 0.26 & \textbf{0.24} & 0.57 & 0.23 & 0.70 & 22.52 & 21.79 \\
\midrule
\bottomrule
\end{tabular}
\end{adjustbox}
\end{table*}


\textbf{\mixtera{} implementation.} 
The original ADO implementation updates the current mixture $\pi$ after every step and samples the next batch based on this distribution.
The distribution is updated based on the per-domain loss from the previous step. 
This does not fully match \mixtera, which can only use a new mixture when generating a new chunk.
Each chunk then may yield several batches of data with the same mixture.
We still send the per-domain losses on every training step at the client to update ADO's internal state at the server.
Whenever a new chunk is generated, the current mixture $\pi$ from ADO is queried (\Cref{alg:chunk-generation}), and the server generates a chunk  according to that mixture.
This introduces some slack since we do not use a new mixture on every batch.
However, due to the stochastic nature of sampling, we do not find this to be an issue.


The only change needed in training clients is the implementation of a per-domain loss.
For this, the loss function (e.g., cross-entropy) needs to be called without reduction, which gives a loss \emph{per token}.
As \mixtera{} provides which token belongs to which domain, we can aggregate the losses \emph{per domain}.
We then perform an all-reduce operation across all training nodes to get the global per-domain losses and send this to the server.

While the original implementation of ADO is fully tied to their training framework and data setup, \mixtera{} is agnostic to the training framework.
While developing, we switched from nanotron~\cite{HFNanotron} to torchtitan~\cite{Liang2024Torchtitan}.
No changes in \mixtera{}  were required.
Only the per-domain loss module at the training framework is required.
This showcases the benefit of having a system like \mixtera{} that decouples the mixing from the training framework.
 
\textbf{Training setup.}
We base our setup on Jiang et al.~\cite{Jiang2024ADO}. 
In addition to their 162M and 1.6B architectures, to test how ADO scales to the latest model architecture, we try a 3.6B model (\Cref{tab:models}) following \texttt{Llama-3.2-3B} from Meta, including its tokenizer.
The 162M and 1.6B models use the \texttt{EleutherAI/GPT-NeoX-20B} tokenizer.
All models use a sequence length of 2048.
For ADO, we follow the codebase and discard the first 500 steps for fitting the scaling laws, start with fitting the scaling laws at step 1\,000, and then re-fit the laws every 1\,000 steps with a subsampling frequency of 10.
We also follow the codebase and \enquote{use the same step size} for all domains.
Instead of using the count of how often a domain has been sampled to fit the scaling laws like in the paper, we follow the ADO codebase and average the total sample counts evenly across all domains.
We train all models with non-strict token-level mixtures, a learning rate of 0.001 with a linear warmup for 500 steps and linear decay until the final step, and the AdamW optimizer, for 30\,000 steps.
For 162M/1.6B, we use 64 GPUs with a microbatch size of 32, and for 3.6B we use 128 GPUs with a microbatch size of 16, resulting in a global batch size of 2048 and total 125\,B tokens.
As mixtures, we test ADO, the default weights as in the DoReMi paper~\cite{xie2024doremi}, and on 162M/1.6B, the \enquote{natural} weights by Jiang et al.~\cite{Jiang2024ADO} which are optimized for the NeoX tokenizer. %

\textbf{Evaluation metrics.}
We follow Jiang et al.~\cite{Jiang2024ADO} and report both downstream task performance as well as perplexity.
For downstream tasks, we report performance on HellaSwag~\cite{Zellers2019HellaSwag}, WinoGrande~\cite{Sakaguchi2021WinoGrande}, ARC-Easy and ARC-Challenge~\cite{Clark2018ARC}, LogiQA2~\cite{Liu2023LogiQA}, Lambada OpenAI~\cite{Paperno2016Lambada}, OpenBookQA~\cite{Mihaylov2018OpenbookQA}, and PIQA~\cite{Bisk2020PIQA}.
For perplexity, we report the average unweighted token perplexity on (i) the validation set of The Pile~\cite{Gao2020Pile}, and on (ii) SlimPajama~\cite{Cerebras2023SlimPajama} as a dataset we did not train on.
We collect all metrics using EleutherAI's lm-eval-harness~\cite{Gao2023Harness}, and use the unnormalized accuracy.


\textbf{ADO algorithm performance overview.}
\Cref{tab:eval-downstream-ado} shows the performance of all models and mixtures for a checkpoint after 15\,k and 30\,k steps.
We mark in bold the best value within a model/step group.
In contrast to Jiang et al.~\cite{Jiang2024ADO}, we find that on the 162M model, the default mixture is quite competitive across benchmarks.
For the final checkpoint (30\,k), ADO achieves the best (lowest) perplexity on both SlimPajama and The Pile, but it does not consistently beat the static mixtures as reported in the ADO paper.
However, analysis of the textual outputs reveals that the quality of generated text from this small model is limited across all configurations.

For the larger 1B model, ADO is particulary strong on the intermediate checkpoint, which shows an improvement in time-to-accuracy.
For the final checkpoint, on some benchmarks, the static mixtures perform on-par or marginally better than ADO, but on others, such as HellaSwag, ADO clearly beats the static mixtures.

On the 3B model, ADO beats the static default mixture on both checkpoints across all benchmarks.
While Jiang et al.~\cite{Jiang2024ADO} report that ADO performs best on the smaller scale model, we find that ADO performs better on larger models.
Since ADO's official repository is tightly coupled with the training framework, even after corresponding with the authors, we were not able to identify the root cause of this, partly also because their code is bound to training on specific cloud instances.
This shows that a common codebase for data mixing decoupled from the training framework is beneficial and help developers integrate the latest algorithms into their setups (\Cref{sec:status_quo}).

\begin{figure*}
    \centering
    \begin{adjustbox}{trim=0cm 0.4cm 0cm 0cm}
                \includesvg[width=\textwidth]{img/1b_over_time.svg}
    \end{adjustbox}
    \caption{Performance of the 1B model on HellaSwag, OpenBookQA, and ARC-Easy, measured every 2\,500 steps.} 
    \label{fig:bm-over-time-1b}
    \vspace{-0.3cm}
    \Description{TODO!}
\end{figure*}

\textbf{Performance over time.} To demonstrate how different tasks behave over the training, we show the performance of the 1B model on HellaSwag, OpenBookQA, and ARC-Easy for all training checkpoints in~\Cref{fig:bm-over-time-1b}.
Every benchmark exhibits different behavior.
For HellaSwag, we can clearly see that ADO consistently increases its margin over the static mixtures.
For OpenBookQA, ADO performs well especially in the intermediate checkpoints, and the static mixtures greatly improve towards the end.
For ARC-Easy, all mixtures behave similarly.
This motivates future research on data mixing using \mixtera. 
For example, we might be able to use intermediate evaluations instead of loss to dynamically adjust the mixture.

\begin{figure}
    \centering
    \begin{adjustbox}{trim=0cm 0.4cm 0cm 0cm}
                \includesvg[width=0.95\linewidth]{img/1b_mixture.svg}
    \end{adjustbox}
    \caption{Mixture for the 1B model for the 6 largest domains.} 
    \label{fig:mix-over-time-1b}
    \Description{TODO!}
    \vspace{-0.5cm}
\end{figure}

\textbf{Mixture over time.} We showcase the mixture over time for the six largest domains in~\Cref{fig:mix-over-time-1b}.
Around step 21\,k, we run out of data for the Books3 domain, and \mixtera's best-effort algorithm re-distributes the weight  proportionally to the other domains.
This happens because The Pile's samples are very imbalanced in size, i.e., the average sample in Books3 has around 538\,KiB, while the average Pile-CC sample has 4\,KiB.
While we split long samples heuristically, there still is some waste due to token-level mixtures.
Unlike Jiang et al.~\cite{Jiang2024ADO}, we do not observe that other models/tokenizers lead to different mixtures. %
Notably, the more parameters, the higher the weight of Books3, and the lower the weight of PubMed Central.
On the 3.6B model, OpenWebText2's weight even surpasses PubMed Central's weight before step 5\,000.

\textbf{Takeaways.} 
Dynamic mixture algorithms can improve model accuracy.
ADO scales beyond the 1.6B model tested in the original paper, beating the default weights on the 3.6B model on all benchmarks.
Our experiments also demonstrate that a synchronous algorithm, which uses a new mixture at each training step, adapts to \mixtera's chunking system.
We observe some discrepancies on the 162M model, which highlights the importance of having a common, open-source data mixing platform like \mixtera{} that facilities debugging and reproducibility.




\subsection{Multimodal LLaVA Finetuning}\label{subsec:eval-multimodal}
We are not aware of prior work on the impact of data mixtures on VLMs.
To showcase \mixtera's multimodal capabilities, we evaluate the impact of static mixtures for finetuning a LLaVA-style model~\cite{liu2024improvedbaselinesvisualinstruction}.
The LLaVA framework trains an adapter between a pre-trained image encoder and a pre-trained LLM, and then fine-tunes the adapter and LLM  on visual instruction-following data.


\textbf{Training setup.} We base our training setup on the TinyLlaVA Factory codebase by Jia et al.~\cite{jia2024tinyllavafactorymodularizedcodebase}.
We rely on a recipe from the Factory and use \texttt{google/siglip-so400m-patch14-384}~\cite{Alabdulmohsin2023Siglip400m} as the vision encoder, \texttt{TinyLlama/TinyLlama-1.1B-Chat-v1.0}~\cite{Zhang2024TinyLlama} as the LLM, and a 2-layer MLP as the adapter~\cite{FactoryRecipe}.
We train all models on one GH200 node with 4 data parallel GPUs.
For pre-training, we use a global batch size of 512 with a learning rate of 0.001, and for finetuning use a global batch size of 128 with a learning rate of 0.00002.
We use a chunk size of 256, a cosine learning rate scheduler and the Adam optimizer.
We follow Liu et al.~\cite{liu2024improvedbaselinesvisualinstruction,LlaVARepo} and pre-train the adapter on a 558\,k subset of the LAION-CC-SBU dataset with BLIP captions.
For finetuning, we follow the TinyLLava Factory \enquote{LLaVA dataset} and use 665\,k samples from six datasets (COCO~\cite{Lin2014COCO}, GQA~\cite{Hudson2019GQA}, OCR-VQA~\cite{Mishra2019OCRVQA}, TextVQA~\cite{Singh2019TextVQA}, and VisualGenome (VG)~\cite{Krishna2017VisualGenome}, and LLaVA's text-only SFT annotations~\cite{liu2024improvedbaselinesvisualinstruction})~\cite{FactoryDataPrep}.
We pre-train the adapter once, and then vary the proportions of the finetuning datasets.
We randomly generate 256 mixtures for finetuning.
Since the number of datapoints in comparison to LLM training is small, we use a best-effort mixture and ensure we go through all samples exactly once.
All models see the same data, but in different order.


\textbf{Benchmarks.}
We evaluate the models on the  GQA~\cite{Hudson2019GQA}, SQA-IMG~\cite{Lu2022ScienceQAIMG}, TextVQA~\cite{Singh2019TextVQA}, POPE~\cite{Li2023POPE}, MME~\cite{Fu2024MME}, and MMMU~\cite{Yue2024MMMU} benchmarks.
We collect all metrics using TinyLLaVA Factory.

\textbf{Results.} In~\Cref{tab:vlm-perf}, we show the results reported by Jia et al.~\cite{jia2024tinyllavafactorymodularizedcodebase}, the results we obtain using the inferring mixture (\Cref{subsubsec:impl-chunkgen}), and three mixtures out of the generated mixtures that perform well.
While the inferring mixture does not achieve their reported results, this could be either due to different data dynamics in their training, or due to a different evaluation setup we cannot reproduce as their model weights are not public.
Nevertheless, in particular on MME/MMMU/POPE, the mixtures outperform the baseline.
Mix. \#252 is weighted towards TextVQA (29.1\,\%) and OCR-VQA (19.5\,\%), with equal proportions of COCO (21.9\%) and VG (21.9\,\%); GQA (4.0\,\%) and SFT annotations  (3.6\,\%) contribute minimally.
Mix. \#107 places a large emphasis on SFT annotations (28.9\,\%) and VG (27.4\,\%), followed by COCO (25.5\,\%); GQA (5.3\,\%) and TextVQA (5.1\,\%) have lower representation, while OCR-VQA (7.8\,\%) remains a minor component.
Mix. \#155 prioritizes OCR-VQA (30.3\,\%) and TextVQA (25.7\,\%), and SFT annotations (22.7\,\%); VG (13.2\,\%) and COCO (6.9\,\%) are underrepresented, while GQA (1.2\,\%) is  least utilized.
Overall, despite seeing the same samples globally, the mixtures play a big role for model accuracy.
Exploring mixtures for pre-training of VLMs and adapting dynamic mixtures to VLMs  are promising future work directions.


\begin{table}
\centering
\caption{VLM scores for hand-picked mixtures.}
\vspace{-0.3cm}
\label{tab:vlm-perf}
\begin{adjustbox}{max width=\linewidth}
\begin{tabular}{l|cccccc}
\toprule
\textbf{Model} & \textbf{SQA-IMG} & \textbf{TxtVQA} & \textbf{GQA} & \textbf{MME} & \textbf{MMMU} & \textbf{POPE} \\
\midrule
\textbf{Jia et al.~\cite{jia2024tinyllavafactorymodularizedcodebase}} & 64.0 & 49.6 & 58.6 & 1256.5 & 28.3 & 86.3 \\
\textbf{Infer. Mix.} & 55.88 & 37.92 & 54.63 & 1238.76 & \textbf{29.4} & \textbf{86.6} \\
\textbf{Mix. \#252} & 63.01 & 43.87 & 56.14 & \textbf{1268.05} & \textbf{30.4} & 85.7 \\
\textbf{Mix. \#107} & 58.50 & 45.18 & 58.36 & \textbf{1283.62} & \textbf{30.1} & 85.54 \\
\textbf{Mix. \#155} & 57.14 & 42.37 & 57.14 & \textbf{1290.06} & \textbf{29.3} & \textbf{86.63} \\
\bottomrule
\end{tabular}
\end{adjustbox}
\vspace{-0.5cm}
\end{table}








\subsection{Throughput Benchmarks}\label{subsec:eval-perf}

We now focus on throughput and compare \mixtera{} with other data loaders across various configurations.
\mixtera's goal is to avoid data stalls, i.e., training throughput should not be reduced because we are waiting for data~\cite{Mohan2021DataStalls,Murray2021tfdata,Kuchnik2022Plumber, Zhao2022MetaRM}.
Hence, we measure throughput in tokens per second in actual workloads.
Note that \emph{smaller models} and \emph{more data parallelism} increase the pressure on the data loader, while larger models reduce the pressure.
If a data loader can sustain training small models at scale on a high-end platform like the GH200, its performance is sufficient for other scenarios as well.

Besides \mixtera{} with chunk sizes of 512, 1024, and 2048, we benchmark the iterable \texttt{HuggingFaceDataset} by Torchtitan (\textsc{Hf-Iter}), a mapped version (\textsc{Hf-Map}),  the Mosaic StreamingDataset~\cite{Mosaicml2022streaming} (\textsc{Mosaic}). %
The difference between \textsc{Hf-Iter} and \textsc{Hf-Map} is that similar to \mixtera, \textsc{Hf-Iter} loads and tokenizes the data on the fly, while \textsc{Hf-Map}  preprocesses all data, including tokenization.
We evaluate throughput on the 162M model since larger models only lead to lower throughput.
We always use FSDP since Torchtitan only enables \texttt{bfloat16} training if sharding is used.
We activate compilation, disable activation checkpointing, and use fused AdamW.
We measure throughput for 30 steps, discarding the first step.
We repeat all measurements three times, i.e., in total we have 3x30 steps.
We use the huggingface \texttt{EleutherAI/gpt-neox-20b} tokenizer for all data loaders.
We store all data on the \texttt{Iopsstor} SSD-backed Lustre DFS in the Alps cluster.

\begin{figure}
    \centering
    \begin{adjustbox}{trim=0cm 0.4cm 0cm 0cm}
                \includesvg[width=\linewidth]{img/singlenode_boxplot.svg}
    \end{adjustbox}
    \caption{Data loader throughput depending on the number of workers. The number in brackets indicates the chunk size.} 
    \label{fig:singlenode-tp}
    \Description{TODO!}
\end{figure}


\textbf{Single-node.} We train on a single GH200 node with 4 GPUs.
We use 2 data parallel replicates and shards.
In~\Cref{fig:singlenode-tp} we show the throughput for the data loaders depending on the number of data workers.
We test up to 16 workers, where 0 workers indicate that data is loaded in the same process as the main training loop.
We only show results up to 4 workers due as throughput does not increase further.
The error bars show the standard deviation of the throughput.
Without data workers, \mixtera{} has the highest average throughput; with one worker, the other data loaders reach their peak performance.

\mixtera{} performs worse than the other loaders with a lower number of workers due to the random access into the files. 
For every sample, it needs to open the file, seek to the correct position, and load the data, instead of bulk-transferring all the data as the other data loaders can do.
This leads to the higher variance in throughput indicated by the error bars, which overall leads to a lower average.
However, with a higher number of workers, the variance in data access gets hidden.
When using 4+ workers, \mixtera{} performs almost identical to the other data loaders.
Additionally, increasing the chunk size also helps, and for the 0 worker case, \mixtera{} with a chunk size of 1024 or 2048 even has a higher average throughput.

This benchmark setup is quite extreme, as we train a very small 162M model on an extremely fast GPU.
Nevertheless, just by adding some workers, which  comes at no overhead beyond some CPU resources, \mixtera{} reaches competitive throughput despite its more complicated model.




\textbf{Scaling out.} We investigate how the data loaders scale for larger training jobs with higher data parallelism.
We scale up to 64 GH200 nodes with a total of 256 data parallel GPUs.
We find that using a maximum number of 16 replication GPUs works best.
For 4, 8, and 16 GPUs, we use half the GPUs as replication, and shard across the rest.
The results with 8 data workers can be found in~\Cref{fig:eval-scaling}.
All data loaders scale perfectly linear.
For larger number of GPUs, the throughput variance increases a bit for all systems.
We attribute this to the random assignment of nodes in the cluster by the slurm scheduler across the 3 repetitions.
We do not test pipeline or tensor parallelism as (i) this is not necessary for the 162M model (and a larger model would only stress the data loaders less), (ii) the data loaders besides \mixtera{} do not easily support 3D parallelism, (iii) increasing data parallelism increases the load on the system more.



\textbf{File formats.} All previous benchmarks use uncompressed \texttt{jsonl} data.
We test compressed \texttt{jsonl} (\texttt{jsonl.zst}), \texttt{parquet}, and the \texttt{webdatasets} format in the data loaders that support them.
Notably, only \mixtera{} supports all of these formats.
We find that the underlying file format does not impact the training throughput and therefore omit a plot.
This observation holds across different numbers of data workers, where we also observe minimal performance variation among the data loaders.


\begin{figure}
    \centering
    \begin{adjustbox}{trim=0cm 0.4cm 0cm 0cm}
                \includesvg[width=0.95\linewidth]{img/scaling_boxplot.svg}
    \end{adjustbox}
    \caption{Data loader throughput when increasing the number of data parallel nodes.} 
    \label{fig:eval-scaling}
    \vspace{-0.5cm}
    \Description{TODO!}
\end{figure}

\subsubsection{Query execution}\label{subsubsec:eval-exec}

\mixtera{} executes the query in DuckDB and generates the \texttt{ChunkerIndex} before we can start training.
On our Pile dataset with 241\,M samples, DuckDB takes 29\,s, and preparing the index takes 18\,s due to our C++ implementation.
Optionally, we can also persist an initial state checkpoint of the query, which due to serialization of nested Python dictionaries takes 13\,min.
This is only necessary if checkpointing should be supported.
Future checkpoints can then be stored in milliseconds.
While the streaming \textsc{Hf-Iter} data loader can basically start streaming data immediately, the \textsc{Hf-Map} data loader, the default in nanotron, loads and tokenizes the data first, taking 2\,h 51\,min for this dataset.
However, what needs to be considered is that for streaming data loaders such as \textsc{Hf-Iter} or \textsc{Mosaic}, in many scenarios, users would also need to run more offline preprocessing, e.g., to perform static filtering, or reshuffling the data if we want to mix on a different property.
\mixtera{} avoids this offline preprocessing completely, and, besides an optional state checkpoint, starts streaming data in under a minute.

