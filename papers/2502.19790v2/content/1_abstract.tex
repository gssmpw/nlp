State-of-the-art large language and vision models are trained over trillions of tokens that are aggregated from a large variety of sources.
As training data collections grow, manually managing the samples becomes time-consuming, tedious, and prone to errors.
Yet recent research shows that the data mixture and the order in which samples are visited during training can significantly influence model accuracy.
We build and present \mixtera, a data plane for foundation model training that enables users to declaratively express which data samples should be used in which proportion and in which order during training.
\mixtera{} is a centralized, read-only layer that is deployed on top of existing training data collections and can be declaratively queried.
It operates independently of the filesystem structure and supports mixtures across arbitrary properties (e.g., language, source dataset) as well as dynamic adjustment of the mixture based on model feedback.
We experimentally evaluate \mixtera{} and show that our implementation does not bottleneck training and scales to 256 GH200 superchips.
We demonstrate how \mixtera{} supports recent advancements in mixing strategies by implementing the  proposed Adaptive Data Optimization (ADO) algorithm in the system and evaluating its performance impact.
We also explore the role of mixtures for vision-language models.
