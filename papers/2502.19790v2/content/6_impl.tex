\section{Implementation}\label{sec:implementation}

We explain how sample metadata is managed (\Cref{subsec:design-dc-mgmt}) at the server, how it executes queries and creates chunks (\Cref{subsec:design-queryexec}), how those chunks are parsed at the client (\Cref{subsec:design-clientside}), and give  details on \mixtera's integration into training frameworks (\Cref{subsec:design-integration}).

\subsection{Sample Management}\label{subsec:design-dc-mgmt}


\mixtera{} manages samples in a \texttt{MixteraDataCollection} (MDC).
It keeps track of all samples and their properties.
The MDC leverages DuckDB~\cite{Raasveldt2019DuckDB} as the robust and flexible metadata management system underlying it.
Every registered sample has properties according to a schema defined by a \texttt{MetadataParser}, e.g., language.
Representing the structure of training data, \mixtera{} initializes the database with three tables for source datasets, files, and samples.
The source dataset can also be ignored and stored as a schema property instead, to support collections such as The Pile~\cite{Gao2020Pile} where data from different datasets is contained within the same files.

\textbf{\texttt{MetadataParser}s.} A \texttt{MetadataParser} is a class that defines a schema for a data collection and, given a sample, extracts and returns the properties according to the schema.
A schema is a list of properties which have a type (e.g., string or enum), a nullable field, and a multiple field, describing whether a single sample can take multiple values for this properties, e.g., multiple languages.
\mixtera{} maps this Python schema to a proper database schema, comes with a set of pre-defined parsers for common datasets, and enables users to define custom parsers.
For example, there is a parser for The Pile~\cite{Gao2020Pile}.
Every sample has the property \texttt{pile\_set\_name} describing the source dataset.
This is a non-nullable, non-multiple, enum property, since we know the limited set of possible values.

\textbf{Data types.} Based on the parser, \mixtera{} adjusts the DuckDB sample table.
Properties marked as multiple get mapped to DuckDB's list type, e.g., a multiple string property gets mapped to \texttt{VARCHAR[]}.
Enum properties are appropriately inserted into DuckDB.
Using enum instead of strings for properties where all values are known beforehand speeds up query execution in DuckDB.%

\textbf{Insertion.} When inserting (registering) data, the MDC accumulates all relevant files, and prepares the DuckDB table schema.
Then, all files are inserted into the files table, and a pool of workers in parallel processes all files using the \texttt{MetadataParser} to extract the sample metadata.
We then aggregrate the worker results and insert it into the database, as DuckDB does not support insertion from multiple Python process workers.
We found that converting the worker results to columnar pyarrow in-memory tables and then inserting into the samples table has the highest throughput.

\subsection{Server-Side Query Execution}\label{subsec:design-queryexec}

After receiving a query from the client, the server executes it in two phases.
The first phase is performed via the MDC DuckDB-abstraction and applies static filters to identify all potentially relevant samples, e.g., all non-English samples are filtered out, and groups consecutive samples into intervals.
The second phase constructs a specialized data structure called \texttt{ChunkerIndex} that enables efficient, mixture-aware chunk generation.

\subsubsection{SQL generation and interval detection} 
After receiving an object representation of the query (c.f.~\Cref{listing:mixtera-query}), similar to ORM frameworks like sqlalchemy, the \mixtera{} server generates a base SQL query from this object.
This query returns a table in which each row represents a sample that the user is interested in.  
\mixtera{} ensures that the generated SQL matches the MDC's table schema, e.g., whether a property can have multiple values or not.

A key challenge for \mixtera{} is efficient random access to samples within files.
File formats like jsonl or parquet are optimized for sequential reading rather than random access. 
To address this, \mixtera{} implements an interval-based approach: the server wraps the base filtering query in an outer query that identifies continuous ranges of samples sharing identical properties within the same file.
Consider the following example result of a base filtering query:

\begin{center}
\begin{footnotesize}
\begin{tabular}{@{}cccc@{}}
\toprule
Sample ID & File ID & Language & License \\
\midrule
1 & 1 & JavaScript & MIT \\
2 & 1 & JavaScript & MIT \\
3 & 1 & JavaScript & MIT \\
4 & 1 & Python & Apache \\
5 & 1 & Python & Apache \\
1 & 2 & Python & Apache \\
\bottomrule
\end{tabular}
\end{footnotesize}
\end{center}

Instead of treating these as six individual samples, \mixtera{} identifies three intervals:
\begin{itemize}
    \item Interval 1: Samples 1-3 (File 1, JavaScript, MIT)
    \item Interval 2: Samples 4-5 (File 1, Python, Apache)
    \item Interval 3: Sample 1 (File 2, Python, Apache)
\end{itemize}

Even though samples 4-5 (file 1) and 1 (file 2) share the same properties (Python, Apache), they are in different files and thus form separate intervals.
The primary key is formed by the sample and file ID together.
\mixtera{} constructs a SQL query that processes the data in multiple stages:

\begin{enumerate}[leftmargin=*,nosep]
    \item First, \mixtera{} establishes a Common Table Expression (CTE) named \texttt{base\_data} that contains the filtered samples:
    \begin{minted}[frame=lines,framesep=2mm,baselinestretch=0.8,bgcolor=LightGray,fontsize=\footnotesize]{sql}
WITH base_data AS (
    -- Our generated base filtering query here, e.g.,
    SELECT * FROM samples WHERE license = 'MIT'),
    \end{minted}

    \item Next, \mixtera{} identifies breaks in the sample sequence using window functions. The \texttt{grouped\_samples} CTE calculates the difference between consecutive sample IDs within groups sharing the same properties:
    \begin{minted}[frame=lines,framesep=2mm,baselinestretch=0.8,bgcolor=LightGray,fontsize=\footnotesize]{sql}
grouped_samples AS (
    SELECT *, sample_id - LAG(sample_id, 1, sample_id)
    OVER (PARTITION BY file_id, lang, license 
            ORDER BY sample_id) AS diff
    FROM base_data),
    \end{minted}
    Here, a \texttt{diff} value of 1 indicates consecutive samples, while any other value indicates a break in the sequence.

    \item The \texttt{intervals} CTE then groups the sequences into intervals:
    \begin{minted}[frame=lines,framesep=2mm,baselinestretch=0.8,bgcolor=LightGray,fontsize=\footnotesize]{sql}
intervals AS (
    SELECT file_id, lang, license,
        SUM(CASE WHEN diff != 1 THEN 1 ELSE 0 END)
            OVER (PARTITION BY file_id, lang, license 
                  ORDER BY sample_id) AS group_id,
        MIN(sample_id) as int_strt, MAX(sample_id)+1 as int_end
    FROM grouped_samples
    GROUP BY file_id, lang, license, diff, sample_id)
    \end{minted}
    The \texttt{group\_id} is incremented when there is a break in the sequence, creating unique identifiers for each interval.

    \item Finally, \mixtera{} aggregates the results to get the final intervals:
    \vspace{-0.5cm}
    \begin{minted}[frame=lines,framesep=2mm,baselinestretch=0.8,bgcolor=LightGray,fontsize=\footnotesize]{sql}
SELECT file_id, lang, license, group_id,
  MIN(int_strt) as interval_start, MAX(int_end) as interval_end
FROM intervals
GROUP BY file_id, lang, license, group_id
ORDER BY file_id, interval_start;
    \end{minted}
\end{enumerate}

While we initially implemented the interval aggregation within Python, letting DuckDB optimize and parallelize the calculation of the interval is more efficient.
This optimization can only improve I/O if samples within files are clustered by properties and not randomly distributed.
For example, if a user has a file structure based on the source dataset and mixes based on that, \mixtera{} can heavily leverage the range optimization and just read, e.g., all Wikipedia samples from the sample file.
If the user decides to now mix on language as well, there might only be local clusters within the files.
Since \mixtera{} is read-only by design, it cannot re-shuffle data.

\subsubsection{Chunk generation}\label{subsubsec:impl-chunkgen}
The chunk generation algorithm is based on the \texttt{ChunkerIndex} data structure, which organizes sample ranges by their properties.
This data structure is generated based on the output from the interval query.
Before explaining its construction, we first introduce the key abstraction that enables flexible property matching: \texttt{MixtureKey}.

\textbf{\texttt{MixtureKey} abstraction.} A \texttt{MixtureKey} represents a set of properties and their values, e.g., language being English and license being MIT.
A property in a key can have multiple values.
A key \emph{matches} another if their lists of values intersect for all properties they share.
This matching is crucial because the resulting interval table from DuckDB contains the full cross-product of all properties---a sample might have values for language, license, size, topic, and more---while a mixture specification typically considers only a subset of these properties.
For example, a mixture might specify \texttt{language:English} to select English text regardless of license, while the underlying samples are stored with full property information (e.g., \texttt{language:English,German;license:CC} and \texttt{language:English;license:MIT}).
The flexible matching enables finding all samples that satisfy certain properties while ignoring irrelevant attributes they might have.
It also allows us to define mixtures on \emph{multiple properties} with \emph{multiple values}, instead of being limited to a single property (c.f.~\Cref{sec:status_quo}).
To ensure deterministic behavior, we implement a total ordering over keys based on the number of properties, property names, and their values.
We sometimes refer to a specific \texttt{MixtureKey} as a \emph{domain}, e.g., the key for \texttt{language:English} defines the domain for English samples.


\textbf{The \texttt{ChunkerIndex}.} The \texttt{ChunkerIndex} is a nested mapping from \texttt{MixtureKey}s to sample locations.
We refer to keys in the \texttt{ChunkerIndex}  as \emph{component keys}.
For each unique combination of all available properties and values, the index maps to dataset IDs, which in turn map to file IDs and finally to sorted lists of intervals of samples which share the properties and values as specified in the \texttt{MixtureKey}.
While the index maintains the complete property information of samples, the matching semantics allows us to efficiently query arbitrary subsets of these properties.
Consider a simplified example where we represent \texttt{MixtureKey}s as strings.
A fragment of the \texttt{ChunkerIndex} might look like:
\vspace{-0.2cm}
\begin{minted}[frame=lines,framesep=2mm,baselinestretch=0.8,bgcolor=LightGray,fontsize=\footnotesize]{python}
{ "language:JavaScript,HTML;license:MIT": {
    ds_1: {
      file_1: [(1,4), (10,15)],  # half (right) open ranges
      file_2: [(1,2)]
    }},
  "language:Python;license:Apache": { ds_1: { file_1: [(1,2)] }}}
\end{minted}

In this example, a query for \texttt{language:JavaScript} would match the first key despite it having the additional license property and two assigned languages.
This demonstrates how the \texttt{MixtureKey} matching allow to work with the full property/value cross-product in the index while supporting mixtures on subsets of properties.

\textbf{Building the ChunkerIndex.} The index is built in parallel in a C++ extension, processing the interval table from DuckDB provided in Apache Arrow format. 
Operating on the Arrow table in Python would be too slow due to Global Interpreter Lock (GIL) constraints.
Using \texttt{multiprocessing} to circumvent the GIL would require expensive pickling of nested dictionaries.
Our C++ implementation uses multithreading and only acquires the GIL at the end when creating the final Python result object.

Each C++ worker thread maintains a local index for a subset of the data. 
For each row (interval), each worker constructs a C++-representation of the \texttt{MixtureKey}, inserts the interval into its local index under this key, maintaining sorted order within each file's interval list.
After parallel processing, the local indices are merged, combining interval lists while preserving their sorted order.
In the end, we convert the index to Python objects, which often  is  the most expensive operation of this process.

\textbf{Chunk generation.} A mixture in \mixtera{} is represented as a mapping from \texttt{MixtureKey}s to their target proportions within a chunk.
Given a chunk size that describes how many samples each chunk should contain, these proportions are converted to absolute counts following the largest remainders quota method.
Users can set a mixture to be \emph{strict}, requiring exact proportions, or \emph{best-effort}, allowing to continue to generate chunks even if the mixture cannot be exactly fulfilled.
The chunk generation algorithm (\Cref{alg:chunk-generation}) iteratively constructs chunks that conform to the current mixture.
This algorithm supports dynamic mixture, as the mixture can be changed between chunks.

\begin{algorithm}[t]
\caption{Chunk generation algorithm. Some early exits and details are omitted for readability.}
\label{alg:chunk-generation}
Initialize remaining\_counts from mixture\;
chunk $\gets \emptyset$\;
progress $\gets$ true\;
\While{$\exists \text{key}: \text{remaining\_counts[key]} > 0 \text{\textbf{ and }progress}$}{
    progress $\gets$ false\;
    \ForEach{mixture\_key in remaining\_counts}{
        \ForEach{component\_key in chunker\_index}{
            \If{mixture\_key matches component\_key}{
                Take up to remaining\_counts[mixture\_key] samples from chunker\_index[component\_key]\;
                \If{$\text{got }>0\text{ samples}$}{
                    Add samples to chunk\;
                    Update remaining\_counts\;
                    progress $\gets$ true\;
                }
            }
        }
        \If{$\text{remaining\_counts[key]} > 0$ \textbf{and} is best-effort}{
            Redistribute remaining counts to other keys\; %
        }
    }
}
\If{$\forall \text{key}: \text{remaining\_counts[key]} = 0$}{
\Return chunk\;
}
\end{algorithm}

Similar to the \texttt{ChunkerIndex}, a chunk is also structured as a hierarchical dict with keys and files pointing to lists of ranges.
This chunk can be parsed on a training node by loading the sample ranges from the files (\Cref{subsec:design-clientside}).
While the \texttt{ChunkerIndex} uses all properties/values as \texttt{MixtureKey}s (component keys), a chunk's keys are identical to the mixture's keys, which might only contain a subset of properties.

For each key in the mixture, the algorithm keeps track of how many samples we still need to put into the chunk that is currently being generated (\texttt{remaining\_sizes}).
For each key in the mixture (line 6), it checks whether it matches a component key in the chunker index (lines 7-8).
If we find a match, we try and obtain samples (ranges) from the \texttt{ChunkerIndex} for this component key (lines 9+).

A call to obtain samples for a component key can return fewer samples than requested, e.g., if we are looking for JavaScript data and we need 5 samples, but we only have 3 JavaScript/MIT licensed samples, the according component key can only return 3 samples.
Requesting $n$ samples is implemented as requesting $m$ \emph{ranges} (from potentially multiple files) such that the overall number of samples in the returned list of ranges is $\leq n$.
The lists of ranges per file are merged into the existing sorted list of ranges.
\mixtera's implementation uses Python generators that yield ranges of samples and accept the number of samples needed as input through the Python generator's send mechanism.
This hides the complexity of the \emph{take samples} operation.
It also allows for efficient, stateful iteration over the available ranges while maintaining control over how many samples are returned in each request.
There is one generator per component key that returns the ranges containing $N$ samples based on the \texttt{ChunkerIndex} but ensures ranges are split such that never too much data is returned.

If after traversing all component keys we did not find sufficient samples for a key in the mixture, in strict mode, chunk generation fails.
In best-effort mode, the algorithm redistributes any unfulfilled counts to the remaining mixture components proportionally to their original ratios.
For example, if we need 100 JavaScript samples but only find 80, the remaining 20 samples would be proportionally distributed among other components.
To avoid infinite loops, we only distribute samples to keys on which we were able to find any samples in the last iteration.

\mixtera's implementation of this algorithm ensures determinism  because (1) the mixture's keys are processed in a consistent order and (2) when multiple component keys match a mixture key, they are considered in a deterministic order based on a seeded shuffle of all possible keys.
This approach ensures that identical queries with identical mixtures always produce identical chunks, important for training debugging and reproducibility~\cite{Zhuang2022Randomness,Cooper2022Determinism,Qian2021Seeds,Karamcheti2021Mistral,TensorflowDeterminism}.

\mixtera{} also supports an alternative, \emph{arbitrary chunk} generation algorithm.
It does not require any specified mixture and builds up chunks by iterating over the component keys, moving on to the next one when one key is depleted.
This can be useful if the data is completely property-free and guarantees maximum throughput as all ranges are consecutive.

\textbf{Mixture types.} 
All mixture classes implemented in \mixtera{} share a common interface that converts their specifications into a mapping from \texttt{MixtureKey}s to sample counts per chunk, used by the chunk generation algorithm:
\begin{itemize}[leftmargin=*,nosep]
    \item[--] \underline{Static Mixture:} Users explicitly specify fixed proportions for different property combinations (\Cref{listing:mixtera-query}).
    This supports arbitrary properties and is not limited by, e.g., directory boundaries.
    
    \item[--] \underline{Inferring Mixture:} Automatically derives mixture proportions from the data distribution in the query result:
    This is useful when users want to maintain the natural distribution of properties.
    
    \item[--] \underline{Hierarchical Mixture:} An advanced static mixture that allows to specify nested property relationships. 
    For example, users can define that 50\,\% of the data should be legal texts, and within that, 60\,\% should be in English and 40\% in French.
    \mixtera{} automatically flattens this hierarchy into appropriate \texttt{MixtureKey}s.
    
    \item[--] \underline{Mixture Schedule:} A \enquote{meta mixture} that allows for temporal changes in mixture composition by defining a sequence of mixtures that activate at specific training steps. 
    This enables curriculum learning with predefined schedules.
    
    \item[--] \underline{Dynamic Mixture:} Allows adaptation of mixture proportions during training based on feedback (e.g., loss) from the model. 
    If an algorithm is already supported by \mixtera{} (e.g., ADO), it can be used directly.
\end{itemize}


\textbf{Chunk distribution.}  
In distributed training, it is important to guarantee that all nodes within the same data parallel group operate on exactly the same input tensors.
\mixtera's \texttt{ChunkDistributor} wraps around the chunk generation component, and hands out chunks correctly to the training nodes, i.e., the same chunks in the same order to nodes within the same group, and different chunks to nodes in different groups for data parallelism.
To this end, the clients need to register at the server with their respective node and group identifiers.
To avoid redundant serialization overhead, the distributor caches chunks in serialized form until all nodes in a group have received them.

\subsubsection{Networking}
We implement a  TCP-based client-server protocol.
The server uses Python's asyncio framework to handle multiple concurrent client connections.
The protocol is message-based, with each message consisting of a task identifier followed by task-specific payload data.
Tasks for example include the execution of a query or sending out a new chunk to a client.
To handle network issues gracefully, the client implementation includes automatic reconnection with exponential backoff and configurable timeouts.
Larger data transfers, such as chunk transmission, use efficient streaming to avoid memory issues.

\subsection{Client-Side Reading}\label{subsec:design-clientside}

\mixtera's client-side abstractions offer a generator that yields data samples for a query at the server.
This generator internally follows a two-level nested iteration pattern: an outer iteration over chunks and an inner iteration over samples within each chunk.
The outer iteration hides the complexity of network transfer and chunk generation, while the inner iteration hides the complexity of going from pointers in the chunk to actual samples.
We discuss this inner step, i.e., how, given a chunk, we yield sample payloads.

\textbf{Processing modes.} A chunk can be processed in three mixture processing modes with different trade-offs.
The modes influence in what order samples are yielded, i.e., in what granularity the mixture is guaranteed, and whether string samples or tokenized sequences are yielded.
All modes begin by instantiating one \emph{active iterator} per property combination.
These iterators traverse files and ranges for their respective properties in a randomized order while maintaining sequential reading within consecutive ranges for I/O efficiency.

 
\textbf{Overall mixture mode.} This mode processes active iterators in a randomized round-robin fashion until depletion.
This ensures the mixture ratio is maintained at the chunk level.

\textbf{Window mixture mode.} This mode guarantees the mixture on a window smaller than the chunk size.
Similarly to chunk generation, we determine how many samples per property we yield within a window.
We then go through the properties in a randomized, round robin fashion until a window has been yielded, and start again.
This mode can operate in best-effort (continues after mixture cannot be guaranteed) or strict mode (stops at the first window where the mixture cannot be maintained).
In strict mode, the number of overall samples yielded from the chunk might be smaller than the chunk size.

\textbf{Tokenized mixture mode.} This mode extends the strict window mode.
It wraps the active iterators with a \emph{tokenizing iterator} that takes the incoming string samples, tokenizes them, and yields tokenized samples (integer lists) with the correct sequence length.
By setting the window size equal to the chunk size we guarantee that each chunk at least yields one window of \emph{tokenized samples}.
This mode is important for handling imbalanced collection with significantly varying text lengths.
For example, in the The Pile~\cite{Gao2020Pile}, an average sample from the \texttt{Books3} domain is much longer than one from the \texttt{PubMed Abstracts} domain.

Without tokenization, one string sample may correspond to potentially order-of-magnitudes more tokenized samples, which can heavily disturb the actual mixture seen during training.
While this mode ensures precise mixture ratios at the token level, it may result in partial utilization of longer samples.
This is an inherent issue of unbalanced datasets, and while \mixtera{} provides flexibility to handle it, the best approach is to process the datasets such that samples are (roughly) of similar size.


\textbf{File reading.} The active iterators wrapped by the processing iterators shuffle the file order but maintain sequential reading within files.
This acknowledges that formats like \texttt{jsonl} and \texttt{parquet} are optimized for sequential rather than random access, and enables us to linearly iterate through the sorted lists of ranges per file.
The complexity of reading different file formats internally is hidden by abstractions for each format.
Using the \texttt{xopen} library we support both compressed and uncompressed \texttt{jsonl}.
We optimize the reading of \texttt{parquet} files by calculating and loading only the relevant the row groups, and build upon pyarrow's \texttt{parquet}-batched-reading implementation.
WebDatasets is the only format supporting random access to samples, and we implement support using the \texttt{wids} library.
The format also gives us the option to store text-image pairs, as it can contain different modalities.
To mitigate latency from initial file operations that we observed on distributed filesystems, \mixtera{} employs a prefetching iterator that uses background threads to hide file opening latency.

\textbf{Determinism.} All random operations are seeded based on the current chunk, ensuring identical behavior across nodes.
Combined with the server-side chunk generation and distribution, this guarantees that \emph{all clients within a data parallel group yield exactly the same samples in exactly the same order}.
This property is crucial for both reproducibility across runs and correctness in distributed training.
We validated this using a suite of integration tests as well as the dataloader verification test provided by nanotron.

\textbf{Dataset abstractions.} 
Training frameworks typically require specific dataset interfaces that supports \texttt{multiprocessing} with worker processes.
Besides a general purpose interface, \mixtera{} offers a class extending torch's \texttt{IterableDataset}, and a class compatible with the huggingface API.
Each worker process at each node operates on its own chunk.


\subsection{Framework Integration}\label{subsec:design-integration}

\mixtera{} integrates into the training framework for checkpointing and transferring model feedback (e.g., per-domain loss).

\textbf{Checkpointing.} \mixtera's API offers a function to be called on checkpoint.
In order to restore from checkpoint, we need to know (i) which chunks have been handed out to which nodes and (ii) which data loader worker processes have yielded how many samples for each node.
\mixtera{} implements (i) using the \texttt{ChunkDistributor}, which caches the query and the current state on checkpoint and is able to restore the in-memory state of the iterators for chunk generation based on this information.
For (ii) the \texttt{MixteraTorchDataset} uses a shared memory segment to share with the main training process the status of the data loader workers.
When we restore a checkpoint, we restore the state at the server, hand out the last chunks to each worker that they were working on, and then at the workers discard the previously yielded samples.

\textbf{Training feedback.} Dynamic mixing algorithms require a loss per property domain.
\mixtera{} offers a simple function to forward this information to the server.
Users still need to adjust the training framework.
The loss implementation needs to be adjusted s.t. it is not immediately reduced but stored per domain.
The per-domain losses along all training nodes need to be synchronized, e.g., via all-reduce, before passing it to \mixtera.

