\section{Conclusion and Future Work}\label{sec:conclusion}

Managing training data at scale requires data management support.
We present \mixtera, a training framework-agnostic data plane for foundation model training.
Using its chunking mechanism, it supports dynamic mixtures across arbitrary properties.
We demonstrate \mixtera's performance and scalability, as well as the importance of mixtures on both LLMs and VLMs.
For future work, it is interesting to extend the analysis to the cloud, i.e., using a service like S3 as the underlying storage layer.
\mixtera{} also lays the foundation to implement lineage tracking on which model was trained on which data, as it is a centralized access point.
From a ML perspective, the impact of data composition and quality on model performance is still under-explored, and \mixtera{} provides a foundation to conduct future research on dynamic and static mixing algorithms.
