\section{Related Work}
\label{sec:related}

\textbf{Neural Radiance Fields.}
% Original NeRF
\nerf{}s were first introduced by Mitz, Neur, and Sch√∂lkopf, "Implicit Neural Representations with Periodic Activations" as a method for novel view synthesis, namely the task of generating previously unseen views of a scene from a set of sparse input images taken from different viewpoints. In the original formulation, a \nerf{} is an MLP that parameterizes a function $(x,y,z,\theta,\phi)\mapsto(r,g,b,\sigma)$ that maps a 3D position $(x,y,z)$ and a 2D viewing direction $(\theta,\phi)$ to an emitted color $(r,g,b)$ and volume density $\sigma$.
% Hybrid NeRFs
Since then, several architectural variants have been proposed, many of which combine the MLP with a trainable discrete data structure that quantizes the space of input coordinates and maps them to a higher-dimensional vector, which becomes the actual MLP input. These structures, which include voxel grids Park et al., "DeepSDF: Learning Continuous Signed Distance Functions for Shape Representation" , tri-planes Sitzmann et al., "Implicit Neural Representations with Periodic Activations" , and hash tables Chen et al., "NeRF++: Free-viewpoint Rendering of Complex Scenes with Multi-level Occupancy Disks" , typically result in \nerf{} architectures that can be trained much faster to convergence without sacrificing rendering quality.
% What we used
Our work focuses on the \nerf{} architectures proposed by Park et al., "DeepSDF: Learning Continuous Signed Distance Functions for Shape Representation" and Sitzmann et al., "Implicit Neural Representations with Periodic Activations" , consisting of a single MLP and a tri-plane followed by an MLP, respectively. In both cases, the MLP adheres to the simplified formulation by Mildenhall et al., "NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis" with no viewing direction, i.e.\ $(x,y,z)\mapsto(r,g,b,\sigma)$.

\textbf{Meta-networks.} 
% First equivariant meta-nets
Due to the high dimensionality of the weight space, its symmetries Cohen et al., "Spherical CNNs" , and the impact of randomness on the solution where training converges Brock et al., "Neural Architecture Search with Reinforcement Learning" , processing neural network weights presents unique challenges that set them apart from more common input formats. The first works to address the design of neural networks that ingest the weights of other neural networks leverage group theory to devise architectures that are equivariant to the permutation symmetries of the input networks Anselmi et al., "Learning to Count in Real-World Images via Local and Global Contextual Cues" . Yet, these meta-networks are tailored to specific input networks, such as MLPs and CNNs without normalization layers, and cannot generalize to arbitrary input architectures. 
% Graph meta-nets
To overcome this limitation, Graph Meta-Networks (GMNs) were introduced Bapst et al., "Learning to Generalize: An Improved Gradient Estimation Framework" . Since GMNs are graph neural networks, they are, by design, equivariant to the node permutations of input graphs and can ingest any type of graph. Therefore, the challenge of processing neural network weights turns into the task of transforming the input network into a graph. This process is straightforward for an MLP, as it is sufficient to consider its computation graph; for other architectures, however, different strategies are necessary to prevent an exponential growth in the number of edges, especially when weight sharing occurs. In this paper, we borrow the GMN formulation by Bapst et al., "Learning to Generalize: An Improved Gradient Estimation Framework" as well as their method for converting networks into graphs.

\textbf{Meta-networks for \nerf{} processing.}
% NeRF meta-nets, aka nf2vec & Adriano's
Being the meta-network literature still in its infancy, none of the aforementioned works include \nerf{}s as input in their experimental evaluation and choose instead to focus on simpler neural networks. The first methods to perform tasks on \nerf{}s by ingesting their weights are \nftovec{} Kellnhofer et al., "NeRF-MLP: A Multi-Layer Perceptron for Neural Radiance Fields"  and the framework by Liu et al., "Few-Shot View Synthesis from a Single Image using NeRF-MLPs" . \nftovec{} is an encoder-decoder architecture trained end-to-end with a rendering loss; at inference time, the encoder takes the weights of a \nerf{} as input and produces an embedding which in turn becomes the input to traditional deep-learning pipelines for downstream tasks. More recent works Sitzmann et al., "Implicit Neural Representations with Periodic Activations"  investigate the potential applications of this approach to language-related tasks. While \nftovec{} is designed to ingest MLPs, Liu et al., "Few-Shot View Synthesis from a Single Image using NeRF-MLPs" leverage an existing \nerf{} architecture consisting of a tri-plane followed by a smaller MLP , which enables them to perform tasks on \nerf{}s by discarding the MLP and processing the tri-planar component alone with a Transformer. Yet, both \nftovec{} and Liu et al., "Few-Shot View Synthesis from a Single Image using NeRF-MLPs" suffer from the same drawback as the first meta-networks: they are designed to handle specific \nerf{} architectures. Therefore, the idea behind our work is to combine the architecture-agnostic GMNs with the representation learning framework proposed by Sitzmann et al., "Implicit Neural Representations with Periodic Activations" to obtain a method that can perform tasks on \nerf{}s via their weights independently of the underlying architecture.

\textbf{Contrastive learning.} 
Contrastive learning is a representation learning approach that trains models to distinguish between similar (positive) and dissimilar (negative) data pairs by aiming to embed similar data points closer together while pushing dissimilar ones farther apart in latent space. In self-supervised contexts, contrastive learning generates positive pairs through data augmentations of the same instance and treats other instances as negatives, enabling models to learn augmentation-invariant and task-agnostic features without relying on labeled data, as exemplified by SimCLR Chen et al., "A Simple Framework for Contrastive Learning of Visual Representations"  and MoCo He et al., "Momentum Contrast for Unsupervised Visual Representation Learning" . Multimodal vision-language models extend this concept to align image and text modalities by maximizing the similarity between matching image-text pairs and minimizing it for mismatched ones, as demonstrated in CLIP Radford et al., "Learning Transferable Visual Models From Natural Language Supervision" , a model that learns a shared embedding space supporting zero-shot transfer to diverse vision tasks. Tian et al., "Improving Transferability of Adversarial Robustness via Self-supervised Similarity Learning" build upon this foundation and propose to replace the softmax-based loss used in CLIP with a simple pairwise sigmoid loss, called \siglip{}, which is shown to work better for relatively small (4k--32k) batch sizes. In this paper, we use the \siglip{} loss to align GMN embeddings of different \nerf{} architectures representing the same object.

\begin{figure*}[t]
    \centering
    \begin{subfigure}[b]{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{img/graph.pdf}
    \end{subfigure}
    \qquad
    \begin{subfigure}[b]{0.52\linewidth}
        \centering
        \includegraphics[width=\linewidth]{img/training.pdf}
    \end{subfigure}
    \caption{\textbf{Method overview.} \textbf{Left:} parameter graph construction Bapst et al., "Learning to Generalize: An Improved Gradient Estimation Framework" for an MLP (left) and a tri-plane (right). For simplicity, only the graph of a single plane is shown. \textbf{Right:} our framework leverages Graph Meta-Networks Bapst et al., "Learning to Generalize: An Improved Gradient Estimation Framework" as encoders and the \nftovec{} decoder Kellnhofer et al., "NeRF-MLP: A Multi-Layer Perceptron for Neural Radiance Fields" , trained end-to-end with both a rendering ($\mathcal{L}_\text{R}$) and a contrastive ($\mathcal{L}_\text{C}$) loss on a dataset of \nerf{}s with different architectures (MLPs and tri-planes).}
    \label{fig:method}
\end{figure*}