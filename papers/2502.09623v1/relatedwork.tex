\section{Related Work}
\label{sec:related}

\textbf{Neural Radiance Fields.}
% Original NeRF
\nerf{}s were first introduced by \citealp{mildenhall2020nerf}\ as a method for novel view synthesis, namely the task of generating previously unseen views of a scene from a set of sparse input images taken from different viewpoints. In the original formulation, a \nerf{} is an MLP that parameterizes a function $(x,y,z,\theta,\phi)\mapsto(r,g,b,\sigma)$ that maps a 3D position $(x,y,z)$ and a 2D viewing direction $(\theta,\phi)$ to an emitted color $(r,g,b)$ and volume density $\sigma$.
% Hybrid NeRFs
Since then, several architectural variants have been proposed, many of which combine the MLP with a trainable discrete data structure that quantizes the space of input coordinates and maps them to a higher-dimensional vector, which becomes the actual MLP input. These structures, which include voxel grids \cite{liu2020neural}, tri-planes \cite{chan2022efficient}, and hash tables \cite{muller2022instant, barron2023zip}, typically result in \nerf{} architectures that can be trained much faster to convergence without sacrificing rendering quality.
% What we used
Our work focuses on the \nerf{} architectures proposed by \citealp{ramirez2024deep}\ and \citealp{cardace2024neural}, consisting of a single MLP and a tri-plane followed by an MLP, respectively. In both cases, the MLP adheres to the simplified formulation by \citealp{mildenhall2020nerf}\ with no viewing direction, i.e.\ $(x,y,z)\mapsto(r,g,b,\sigma)$.

\textbf{Meta-networks.} 
% First equivariant meta-nets
Due to the high dimensionality of the weight space, its symmetries \cite{hecht1990algebraic}, and the impact of randomness on the solution where training converges \cite{entezari2022the, ainsworth2023git}, processing neural network weights presents unique challenges that set them apart from more common input formats. The first works to address the design of neural networks that ingest the weights of other neural networks leverage group theory to devise architectures that are equivariant to the permutation symmetries of the input networks \cite{navon2023equivariant, zhou2023permutation, zhou2023neural}. Yet, these meta-networks are tailored to specific input networks, such as MLPs and CNNs without normalization layers, and cannot generalize to arbitrary input architectures. 
% Graph meta-nets
To overcome this limitation, Graph Meta-Networks (GMNs) were introduced \cite{lim2024graph, kofinas2024graph}. Since GMNs are graph neural networks, they are, by design, equivariant to the node permutations of input graphs and can ingest any type of graph. Therefore, the challenge of processing neural network weights turns into the task of transforming the input network into a graph. This process is straightforward for an MLP, as it is sufficient to consider its computation graph; for other architectures, however, different strategies are necessary to prevent an exponential growth in the number of edges, especially when weight sharing occurs. In this paper, we borrow the GMN formulation by \citealp{lim2024graph}\ as well as their method for converting networks into graphs.

\textbf{Meta-networks for \nerf{} processing.}
% NeRF meta-nets, aka nf2vec & Adriano's
Being the meta-network literature still in its infancy, none of the aforementioned works include \nerf{}s as input in their experimental evaluation and choose instead to focus on simpler neural networks. The first methods to perform tasks on \nerf{}s by ingesting their weights are \nftovec{} \cite{ramirez2024deep} and the framework by \citealp{cardace2024neural}. \nftovec{} is en encoder-decoder architecture trained end-to-end with a rendering loss; at inference time, the encoder takes the weights of a \nerf{} as input and produces an embedding which in turn becomes the input to traditional deep-learning pipelines for downstream tasks. More recent works \cite{ballerini2024connecting, amaduzzi2024llana} investigate the potential applications of this approach to language-related tasks. While \nftovec{} is designed to ingest MLPs, \citeauthor{cardace2024neural}\ leverage an existing \nerf{} architecture consisting of a tri-plane followed by a smaller MLP \cite{chan2022efficient}, which enables them to perform tasks on \nerf{}s by discarding the MLP and processing the tri-planar component alone with a Transformer. Yet, both \nftovec{} and \citeauthor{cardace2024neural}\ suffer from the same drawback as the first meta-networks: they are designed to handle specific \nerf{} architectures. Therefore, the idea behind our work is to combine the architecture-agnostic GMNs with the representation learning framework proposed by \citeauthor{ramirez2024deep} to obtain a method that can perform tasks on \nerf{}s via their weights independently of the underlying architecture.

\textbf{Contrastive learning.} 
Contrastive learning is a representation learning approach that trains models to distinguish between similar (positive) and dissimilar (negative) data pairs by aiming to embed similar data points closer together while pushing dissimilar ones farther apart in latent space. In self-supervised contexts, contrastive learning generates positive pairs through data augmentations of the same instance and treats other instances as negatives, enabling models to learn augmentation-invariant and task-agnostic features without relying on labeled data, as exemplified by SimCLR \cite{chen2020simple} and MoCo \cite{he2020momentum}. Multimodal vision-language models extend this concept to align image and text modalities by maximizing the similarity between matching image-text pairs and minimizing it for mismatched ones, as demonstrated in CLIP \cite{radford2021learning}, a model that learns a shared embedding space supporting zero-shot transfer to diverse vision tasks. \citealp{zhai2023sigmoid}\ build upon this foundation and propose to replace the softmax-based loss used in CLIP with a simple pairwise sigmoid loss, called \siglip{}, which is shown to work better for relatively small (4k--32k) batch sizes. In this paper, we use the \siglip{} loss to align GMN embeddings of different \nerf{} architectures representing the same object.

\begin{figure*}[t]
    \centering
    \begin{subfigure}[b]{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{img/graph.pdf}
    \end{subfigure}
    \qquad
    \begin{subfigure}[b]{0.52\linewidth}
        \centering
        \includegraphics[width=\linewidth]{img/training.pdf}
    \end{subfigure}
    \caption{\textbf{Method overview.} \textbf{Left:} parameter graph construction \cite{lim2024graph} for an MLP (left) and a tri-plane (right). For simplicity, only the graph of a single plane is shown. \textbf{Right:} our framework leverages Graph Meta-Networks \cite{lim2024graph} as encoders and the \nftovec{} decoder \cite{ramirez2024deep}, trained end-to-end with both a rendering ($\mathcal{L}_\text{R}$) and a contrastive ($\mathcal{L}_\text{C}$) loss on a dataset of \nerf{}s with different architectures (MLPs and tri-planes).}
    \label{fig:method}
\end{figure*}