\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
%\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{lipsum}
\usepackage{multirow}
\usepackage{multicol}
\usepackage[table]{xcolor}
\usepackage{subcaption}
\usepackage[inline]{enumitem}
\usepackage{siunitx}
\sisetup{output-exponent-marker=\ensuremath{\mathrm{e}}}

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page),
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}

\newcommand{\nerf}{NeRF}
\newcommand{\nftovec}{\texttt{nf2vec}}
\newcommand{\siglip}{SigLIP}
\newcommand{\mlp}{\texttt{MLP}}
\newcommand{\trip}{\texttt{TRI}}
\newcommand{\both}{\texttt{BOTH}}
\newcommand{\mlpl}{\texttt{MLP-2L}}
\newcommand{\mlph}{\texttt{MLP-32H}}
\newcommand{\tripl}{\texttt{TRI-2L}}
\newcommand{\triph}{\texttt{TRI-32H}}
\newcommand{\tripwh}{\texttt{TRI-16W}}
\newcommand{\tripc}{\texttt{TRI-8C}}
\newcommand{\lr}{$\mathcal{L}_\text{R}$}
\newcommand{\lc}{$\mathcal{L}_\text{C}$}
\newcommand{\lrc}{$\mathcal{L}_{\text{R}+\text{C}}$}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Embed Any NeRF}

\begin{document}

\twocolumn[
\icmltitle{Embed Any NeRF: Graph Meta-Networks\\for Neural Tasks on Arbitrary NeRF Architectures}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols; otherwise, they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance, which is preferred.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Francesco Ballerini}{unibo}
\icmlauthor{Pierluigi Zama Ramirez}{unibo}
\icmlauthor{Samuele Salti}{unibo}
\icmlauthor{Luigi Di Stefano}{unibo}
\end{icmlauthorlist}

\icmlaffiliation{unibo}{University of Bologna, Italy}

\icmlcorrespondingauthor{Francesco Ballerini}{francesco.ballerini4@unibo.it}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text, to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if you don't need to mention the equal contribution
%\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Neural Radiance Fields (NeRFs) have emerged as a groundbreaking paradigm for representing 3D objects and scenes by encoding shape and appearance information into the weights of a neural network. Recent works have shown how such weights can be used as input to frameworks processing them to solve deep learning tasks. Yet, these frameworks can only process NeRFs with a specific, predefined architecture. In this paper, we present the first framework that can ingest NeRFs with multiple architectures and perform inference on architectures unseen at training time. We achieve this goal by training a Graph Meta-Network in a representation learning framework. Moreover, we show how a contrastive objective is conducive to obtaining an architecture-agnostic latent space. In experiments on both MLP-based and tri-planar NeRFs, our approach demonstrates robust performance in classification and retrieval tasks that either matches or exceeds that of existing frameworks constrained to single architectures, thus providing the first architecture-agnostic method to perform tasks on NeRFs by processing their weights.
\end{abstract}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{img/teaser.pdf}
    \caption{\textbf{Framework overview.} Our representation learning framework leverages a Graph Meta-Network \cite{lim2024graph} encoder to map weights of \nerf{}s with different architectures to a latent space where \nerf{}s representing similar objects are close to each other, independently of their architecture. The resulting embeddings are then used as input to downstream pipelines for either classification or retrieval tasks.}
    \label{fig:teaser}
\end{figure}

\section{Introduction}
\label{sec:intro}

% What is a NeRF
Neural Radiance Fields (\nerf{}s) \cite{mildenhall2020nerf} have emerged over the last few years as a new paradigm for representing 3D objects and scenes \cite{xie2022neural}. A \nerf{} is a neural network trained on a collection of images to map 3D coordinates to color and density values, which can then be used to synthesize novel views of the underlying object or scene via volume rendering.
% Advantages of NeRF representation
Due to their continuous nature, \nerf{}s can encode an arbitrary number of images at any resolution into a finite number of neural network weights, thus decoupling the number of observations and their spatial resolution from the memory required to store the 3D representation. As a result, \nerf{}s hold the potential to become a standard tool for storing and communicating 3D information, as supported by the recent publication of several \nerf{} datasets \cite{de2023scannerf, hu2023nerf, ramirez2024deep, cardace2024neural}.

% Advantages of processing NeRFs via their weights
With the rise of \nerf{}s as a new data format, whether and how it is possible to perform traditional deep-learning tasks on them has become an increasingly relevant research question. The naive solution to this problem involves rendering views of the underlying object from its \nerf{} representation and leveraging existing neural architectures designed to process images. However, this procedure requires additional computation time and several decisions that are likely to impact its outcome, such as the number of views to render, their viewpoint, and their resolution. A more elegant and efficient approach explored by recent works relies instead on performing tasks on \nerf{}s by processing their weights as input, therefore requiring no rendering step.
% nf2vec
This strategy is adopted by \nftovec{} \cite{ramirez2024deep}, a representation learning framework that learns to map MLP \nerf{} weights to latent vectors by minimizing a rendering loss. These vectors are then used as input to standard deep-learning pipelines for downstream tasks.
% Rendering loss
%By minimizing a rendering loss that quantifies the pixel-wise error between the signal reconstructed from the latent representation and the one reconstructed from the radiance field itself, \nftovec{} learns a latent space where distances measure how similar the underlying 3D objects are in shape and/or color.
% Adriano's
A related approach is proposed by \citealp{cardace2024neural}, who, instead of employing traditional MLPs, leverage tri-planar \nerf{} representations \cite{chan2022efficient}, where input coordinates are projected onto three orthogonal planes of learnable features to compute the input for an MLP.

% How to process different architectures? GMNs come to the rescue.
Both \nftovec{} and the method by \citeauthor{cardace2024neural}, however, perform tasks with neural frameworks designed to ingest a specific type of \nerf{} architecture (i.e.\ MLPs with predefined hidden dimensions in \nftovec{} and tri-planes with set spatial resolution in \citeauthor{cardace2024neural}), which makes them unable to process arbitrary input architectures. In the context of a research domain where new \nerf{} designs are constantly being explored \cite{xie2022neural}, this limitation strongly hinders their applicability. The issue of handling arbitrary input architectures has recently been studied in the broader research field on \emph{meta-networks}, i.e.\ neural networks that process other neural networks as input. Specifically, Graph Meta-Networks (GMNs) have been proposed \cite{lim2024graph, kofinas2024graph}, namely Graph Neural Networks (GNNs) that can ingest any neural architecture as long as it can be first converted into a graph. Yet, these works do not experiment with \nerf{}s as input to their GMNs.

% Our contribution(s)
Motivated by the potential of GMNs to process diverse \nerf{} architectural designs, we investigate whether a GMN encoder can learn a latent space 
%that interprets different neural architectures representing the same 3D object as alternative augmented instances of the same underlying signal, i.e.\ a latent space 
where distances reflect the similarity between the actual content of the radiance fields rather than their specific neural parameterization. Our empirical study reveals that this latent space organization cannot be achieved by solely relying on a rendering loss \cite{ramirez2024deep}, as such loss alone causes different \nerf{} architectures to aggregate into distinct clusters in the embedding space, even when they represent the same underlying object. To overcome this limitation, we draw inspiration from the contrastive learning literature and introduce a \siglip{} loss term \cite{zhai2023sigmoid} that places pairs of \nerf{}s with different architectures representing the same object closer to each other in latent space while pushing other pairs further apart. Combined with a rendering loss, this approach produces an architecture-agnostic embedding space organized by class and instance. We then show that our encoder produces latent representations that serve as effective inputs for downstream tasks such as classification and retrieval.
%, where our performance matches or surpasses that of competing \nerf-processing methods that, unlike our framework, can only handle specific architectures.
\cref{fig:teaser} outlines our framework key components when \nerf{} architectures are MLP \cite{ramirez2024deep} and tri-plane \cite{cardace2024neural}. 

Our contributions can be summarized as follows:
{\setlist{nosep}
\begin{itemize}[leftmargin=*]
    \item To the best of our knowledge, we are the first to perform tasks on \nerf{}s by processing their weights with an approach that is agnostic to their architecture;
    \item Our framework is also the first that can handle \nerf{} architectures unseen at training time;
    \item By means of a contrastive learning objective, we enforce a latent space where \nerf{}s with similar content are close to each other, regardless of their architecture;
    \item Our method can seamlessly handle arbitrary \nerf{} architectures while achieving comparable or superior results to previous methods operating on single architectures.
\end{itemize}}

\section{Related Work}
\label{sec:related}

\textbf{Neural Radiance Fields.}
% Original NeRF
\nerf{}s were first introduced by \citealp{mildenhall2020nerf}\ as a method for novel view synthesis, namely the task of generating previously unseen views of a scene from a set of sparse input images taken from different viewpoints. In the original formulation, a \nerf{} is an MLP that parameterizes a function $(x,y,z,\theta,\phi)\mapsto(r,g,b,\sigma)$ that maps a 3D position $(x,y,z)$ and a 2D viewing direction $(\theta,\phi)$ to an emitted color $(r,g,b)$ and volume density $\sigma$.
% Hybrid NeRFs
Since then, several architectural variants have been proposed, many of which combine the MLP with a trainable discrete data structure that quantizes the space of input coordinates and maps them to a higher-dimensional vector, which becomes the actual MLP input. These structures, which include voxel grids \cite{liu2020neural}, tri-planes \cite{chan2022efficient}, and hash tables \cite{muller2022instant, barron2023zip}, typically result in \nerf{} architectures that can be trained much faster to convergence without sacrificing rendering quality.
% What we used
Our work focuses on the \nerf{} architectures proposed by \citealp{ramirez2024deep}\ and \citealp{cardace2024neural}, consisting of a single MLP and a tri-plane followed by an MLP, respectively. In both cases, the MLP adheres to the simplified formulation by \citealp{mildenhall2020nerf}\ with no viewing direction, i.e.\ $(x,y,z)\mapsto(r,g,b,\sigma)$.

\textbf{Meta-networks.} 
% First equivariant meta-nets
Due to the high dimensionality of the weight space, its symmetries \cite{hecht1990algebraic}, and the impact of randomness on the solution where training converges \cite{entezari2022the, ainsworth2023git}, processing neural network weights presents unique challenges that set them apart from more common input formats. The first works to address the design of neural networks that ingest the weights of other neural networks leverage group theory to devise architectures that are equivariant to the permutation symmetries of the input networks \cite{navon2023equivariant, zhou2023permutation, zhou2023neural}. Yet, these meta-networks are tailored to specific input networks, such as MLPs and CNNs without normalization layers, and cannot generalize to arbitrary input architectures. 
% Graph meta-nets
To overcome this limitation, Graph Meta-Networks (GMNs) were introduced \cite{lim2024graph, kofinas2024graph}. Since GMNs are graph neural networks, they are, by design, equivariant to the node permutations of input graphs and can ingest any type of graph. Therefore, the challenge of processing neural network weights turns into the task of transforming the input network into a graph. This process is straightforward for an MLP, as it is sufficient to consider its computation graph; for other architectures, however, different strategies are necessary to prevent an exponential growth in the number of edges, especially when weight sharing occurs. In this paper, we borrow the GMN formulation by \citealp{lim2024graph}\ as well as their method for converting networks into graphs.

\textbf{Meta-networks for \nerf{} processing.}
% NeRF meta-nets, aka nf2vec & Adriano's
Being the meta-network literature still in its infancy, none of the aforementioned works include \nerf{}s as input in their experimental evaluation and choose instead to focus on simpler neural networks. The first methods to perform tasks on \nerf{}s by ingesting their weights are \nftovec{} \cite{ramirez2024deep} and the framework by \citealp{cardace2024neural}. \nftovec{} is en encoder-decoder architecture trained end-to-end with a rendering loss; at inference time, the encoder takes the weights of a \nerf{} as input and produces an embedding which in turn becomes the input to traditional deep-learning pipelines for downstream tasks. More recent works \cite{ballerini2024connecting, amaduzzi2024llana} investigate the potential applications of this approach to language-related tasks. While \nftovec{} is designed to ingest MLPs, \citeauthor{cardace2024neural}\ leverage an existing \nerf{} architecture consisting of a tri-plane followed by a smaller MLP \cite{chan2022efficient}, which enables them to perform tasks on \nerf{}s by discarding the MLP and processing the tri-planar component alone with a Transformer. Yet, both \nftovec{} and \citeauthor{cardace2024neural}\ suffer from the same drawback as the first meta-networks: they are designed to handle specific \nerf{} architectures. Therefore, the idea behind our work is to combine the architecture-agnostic GMNs with the representation learning framework proposed by \citeauthor{ramirez2024deep} to obtain a method that can perform tasks on \nerf{}s via their weights independently of the underlying architecture.

\textbf{Contrastive learning.} 
Contrastive learning is a representation learning approach that trains models to distinguish between similar (positive) and dissimilar (negative) data pairs by aiming to embed similar data points closer together while pushing dissimilar ones farther apart in latent space. In self-supervised contexts, contrastive learning generates positive pairs through data augmentations of the same instance and treats other instances as negatives, enabling models to learn augmentation-invariant and task-agnostic features without relying on labeled data, as exemplified by SimCLR \cite{chen2020simple} and MoCo \cite{he2020momentum}. Multimodal vision-language models extend this concept to align image and text modalities by maximizing the similarity between matching image-text pairs and minimizing it for mismatched ones, as demonstrated in CLIP \cite{radford2021learning}, a model that learns a shared embedding space supporting zero-shot transfer to diverse vision tasks. \citealp{zhai2023sigmoid}\ build upon this foundation and propose to replace the softmax-based loss used in CLIP with a simple pairwise sigmoid loss, called \siglip{}, which is shown to work better for relatively small (4k--32k) batch sizes. In this paper, we use the \siglip{} loss to align GMN embeddings of different \nerf{} architectures representing the same object.

\begin{figure*}[t]
    \centering
    \begin{subfigure}[b]{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{img/graph.pdf}
    \end{subfigure}
    \qquad
    \begin{subfigure}[b]{0.52\linewidth}
        \centering
        \includegraphics[width=\linewidth]{img/training.pdf}
    \end{subfigure}
    \caption{\textbf{Method overview.} \textbf{Left:} parameter graph construction \cite{lim2024graph} for an MLP (left) and a tri-plane (right). For simplicity, only the graph of a single plane is shown. \textbf{Right:} our framework leverages Graph Meta-Networks \cite{lim2024graph} as encoders and the \nftovec{} decoder \cite{ramirez2024deep}, trained end-to-end with both a rendering ($\mathcal{L}_\text{R}$) and a contrastive ($\mathcal{L}_\text{C}$) loss on a dataset of \nerf{}s with different architectures (MLPs and tri-planes).}
    \label{fig:method}
\end{figure*}

\section{Method}
\label{sec:method}

We tackle the challenging problem of embedding NeRFs parameterized by different neural architectures with a representation learning framework consisting of an encoder and a decoder trained end-to-end with a combination of rendering and contrastive objectives, where the encoder is implemented by a Graph Meta-Network (GMN). After training, the frozen encoder converts \nerf{} weights into an embedding that can serve as input to standard deep-learning pipelines for downstream tasks. In the remainder of this section, we will detail each framework component. Further details are provided in \cref{app:details}.

\textbf{From \nerf{}s to graphs.} In order for a \nerf{} to be ingested by the encoder, it must be first converted into a graph. The naive approach to perform this conversion would be to adopt the standard computation graph formulation, namely representing a neural network as a Directed Acyclic Graph (DAG) where nodes are activations and edges hold weight values. However, computation graphs scale poorly with the number of activations in networks with weight-sharing schemes, as a single weight requires multiple edges, one for each activation it affects. This limitation has motivated \citealp{lim2024graph} to introduce the \emph{parameter graph} representation, where each weight is associated with a single edge of the graph instead of multiple edges. Furthermore, differently from computation graphs, parameter graphs may include additional node and edge features that increase the expressive power of the meta-networks processing them, such as layer index, node index within a layer, node or edge type (e.g.\ either weight or bias), and position within a kernel or grid. \citealt{lim2024graph} describe the parameter graph construction of several common neural layers, including spatial parameter grids such as those used in many \nerf{} architectures \cite{liu2020neural, chan2022efficient, muller2022instant}. These per-layer subgraphs can then be concatenated to form the overall parameter graph. In this paper, we are specifically interested in graph representations of MLPs (hence, linear layers) and tri-planes \cite{chan2022efficient}. The parameter subgraph of a linear layer coincides with its computation graph, except for biases, which are modeled by including an additional node for each layer, connected to every other neuron in the layer with an edge containing the bias value. Grids like those used in tri-planes are instead modeled by one node for each spatial location and one node for each feature channel, where each spatial node is connected via an edge to every channel node; the channel nodes, together with one node for each input coordinate, become the first layer of the MLP parameter graph that follows. An example of graph construction for an MLP and a tri-planar grid is shown in \cref{fig:method} (left).
Our framework leverages the parameter graph representation by converting input \nerf{}s to parameter graphs before feeding them to the encoder.

\textbf{Encoder.} Our framework's encoder is the GMN proposed by \citealp{lim2024graph}, i.e.\ a standard message-passing Graph Neural Network (GNN) \cite{battaglia2018relational} with node and edge features but no global graph-level feature. Node features are updated by message-passing along neighbors and contribute to updating the features of the edges connecting them. The final embedding is obtained via an average pooling of the edge features. Notably, as the encoder is a GNN, it can process any input graph and, hence, any neural network that has undergone the previously described parameter-graph conversion, thereby allowing our framework to handle any \nerf{} architecture for which a graph representation is known.

\textbf{Decoder.} Our framework leverages the decoder first introduced by \nftovec{} \cite{ramirez2024deep}, which takes as input the concatenation of the embedding produced by the encoder alongside a frequency encoding \cite{mildenhall2020nerf} of a 3D point $(x,y,z)$ and outputs a learned approximation of the radiance field value $(r,g,b,\sigma)$ at that point. Thus, the combination of decoder and embedding can itself be seen as a conditioned neural radiance field. Inspired by \citealp{park2019deepsdf}, the decoder architecture is a simple succession of linear layers intertwined with ReLU activations and a single skip connection from the input to halfway through the network.

\textbf{Training.} The encoder and the decoder are trained end-to-end with a combination of two loss terms, which, from now on, will be referred to as \emph{rendering loss} and \emph{contrastive loss}.
% Rendering loss def
The rendering loss is the one used by \citealp{ramirez2024deep} to train the \nftovec{} framework and can be described as follows. Consider a \nerf{} $\mathcal{N}\colon\mathbf{x}\mapsto(\mathbf{c},\sigma)$ with $\mathbf{x}=(x,y,z)$ and $\mathbf{c}=(r,g,b)$ and let $I\in\mathcal{I}$ be one of the images $\mathcal{N}$ was trained on, with corresponding camera pose and intrinsic parameters. Let $\mathcal{X}=\{\mathbf{x}_i\}$ be the set of points sampled along a ray cast from camera $I$ into the scene and passing through pixel $\mathbf{p}$ on the image plane, and let $C_\mathcal{N}(\mathbf{p})$ be the color computed via volume rendering by accumulating the contributions of the $(\mathbf{c}_i,\sigma_i)$ output values of $\mathcal{N}$ for all inputs $\mathbf{x}_i\in\mathcal{X}$. Analogously, let $C_\text{D}(\mathbf{p})$ be the color computed with the output values produced by the decoder when the encoder takes $\mathcal{N}$ as input. The rendering loss associated with \nerf{} $\mathcal{N}$ is then defined as
\begin{equation}\label{eq:rend}
    \mathcal{L}_\text{R}(\mathcal{N})=\sum_{I\in\mathcal{I}}\sum_{\mathbf{p}\in \mathcal{S}(I)}\operatorname{smoothL1}(C_\mathcal{N}(\mathbf{p}), C_\text{D}(\mathbf{p}))
\end{equation}
where $\mathcal{S}(\mathcal{I})$ is a subset of pixels sampled from $\mathcal{I}$ and $\operatorname{smoothL1}$ is the smooth L1 loss \cite{girshick2015fast}. More precisely, \citeauthor{ramirez2024deep}\ compute one rendering loss term for foreground and one for background pixels and express $\mathcal{L}_\text{R}$ as a weighted sum of the two.
% Batch def
Let $\mathcal{M}$ and $\mathcal{T}$ be two \nerf{} architectures, e.g.\ MLP-based \cite{ramirez2024deep} and tri-planar \cite{cardace2024neural}, respectively, and consider a dataset of \nerf{}s of 3D objects where each object $j$ appears twice, once as $\mathcal{M}_j$ and then as $\mathcal{T}_j$. Given a mini-batch $\mathcal{B}=\{(\mathcal{M}_1,\mathcal{T}_1),(\mathcal{M}_2,\mathcal{T}_2,),\dots\}$, the rendering loss of $\mathcal{B}$ is the average rendering loss of each \nerf{} in $\mathcal{B}$, i.e.
\begin{equation} \label{eq:render}
    \mathcal{L}_\text{R}=\frac{1}{2|\mathcal{B}|}\sum_j\sum_{\mathcal{N}\in(\mathcal{M}_j,\mathcal{T}_j)}\mathcal{L}_\text{R}(\mathcal{N})
\end{equation}
where $\mathcal{L}_\text{R}(\mathcal{N})$ is the one defined in \cref{eq:rend}.
% Contrastive loss def
Our contrastive loss follows instead the definition by \citealp{zhai2023sigmoid}, namely
\begin{equation} \label{eq:contrastive}
    \mathcal{L}_\text{C}=-\frac{1}{|\mathcal{B}|}\sum_{j=1}^{|\mathcal{B}|}\sum_{k=1}^{|\mathcal{B}|}\ln\frac{1}{1+e^{-\ell_{jk}(t\mathbf{u}_j\cdot\mathbf{v}_k+b)}}
\end{equation}
where $\ell_{jk}=1$ if $j=k$ and $-1$ otherwise, $t$ and $b$ are learnable scalar hyper-parameters, and $\mathbf{u}_j$ and $\mathbf{v}_k$ are L2-normalized encoder embeddings of $\mathcal{M}_j$ and $\mathcal{T}_k$, respectively. Finally, the combined loss is
\begin{equation} \label{eq:combined}
    \mathcal{L}_{\text{R}+\text{C}}=\mathcal{L}_\text{R}+\lambda\mathcal{L}_\text{C},
\end{equation}
where $\lambda$ is a fixed hyper-parameter. \cref{fig:method} (right) shows an overview of our training procedure.
% Rendering loss rationale
The purpose of the rendering loss $\mathcal{L}_\text{R}$ is to guide the encoder in learning embeddings that, rather than encoding the input \nerf{} weights, capture meaningful information about the appearance of the 3D object represented by those weights; this is achieved by promoting decoder output values that lead to rendered object views that closely resemble those rendered from the \nerf{} itself. Therefore, $\mathcal{L}_\text{R}$ enforces a latent space where 
\nerf{}s representing similarly shaped and/or colored objects (e.g.\ those belonging to the same class) are closer than those that differ in appearance. However, $\mathcal{L}_\text{R}$ by itself is not able to capture the notion that different \nerf{} architectures representing the same object provide equivalent parameterizations of the same underlying radiance field and should thus be mapped to the same point in latent space. Instead, for any given class, $\mathcal{L}_\text{R}$ creates distinct clusters associated with different architectures, as shown in \cref{fig:tsne} (left).
% Contrastive loss rationale
To overcome this limitation and encourage the creation of an embedding space organized according to the content of the underlying signal rather than its neural network parameterization, we introduce the contrastive loss $\mathcal{L}_\text{C}$, which leverages the availability at training time of \nerf{}s of the same object parameterized by different architectures to learn to treat them as alternative but equivalent representations of the same data point. 
Hence, the combination of $\mathcal{L}_\text{R}$ and $\mathcal{L}_\text{C}$ should result in a latent space where \nerf{}s of similar objects are close to each other, independently of their \nerf{} architecture; in other words, architectural differences should not lead to greater distances between embeddings, only differences in appearance should. This claim is experimentally validated in \cref{fig:tsne} (middle) and further discussed in \cref{sec:latent}.

\textbf{Inference.} At inference time, a single forward pass of the encoder converts the parameter graph of a \nerf{} into a latent vector, which we then use as input to standard deep-learning pipelines for classification and retrieval tasks, as outlined in \cref{fig:teaser} and detailed in \cref{sec:classification,sec:retrieval}.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.8\linewidth]{img/tsne_mlp_trip.pdf}
    \caption{\textbf{t-SNE plots.} 2D projections of the latent space created by the GMN encoder when trained on a dataset of both MLP-based and tri-planar \nerf{}s of ShapenetRender objects \cite{xu2019disn}. Only test-set embeddings are shown.
    \label{fig:tsne}}
\end{figure*}

\section{Experiments}
\label{sec:experiments}

\textbf{Dataset.} All our experiments are performed on two datasets of \nerf{}s: the one by \citealp{ramirez2024deep} and the one by \citealp{cardace2024neural}. They both consist of \nerf{}s trained on ShapenetRender \cite{xu2019disn}, a dataset providing RGB images of synthetic 3D objects together with their class label. The difference between these two datasets lies in the \nerf{} architecture used to create them: in \citeauthor{ramirez2024deep}, a \nerf{} is an MLP, whereas in \citeauthor{cardace2024neural}\ is a tri-plane followed by an MLP.
% Need to say it: otherwise cannot explain unseen architectures
Both \citeauthor{ramirez2024deep}'s and \citeauthor{cardace2024neural}'s MLPs have 3 hidden layers with dimension 64, and \citeauthor{cardace2024neural}'s tri-planes have spatial resolution $32\times32$ and 16 channels. Our experimental evaluation is based on three alternative training sets:
\begin{enumerate*}[label=(\roman*)]
    \item One consisting of \citeauthor{ramirez2024deep}'s MLP-based \nerf{}s only, which will be referred to as \mlp{};
    \item One consisting of \citeauthor{cardace2024neural}'s tri-planar \nerf{}s only, which will be referred to as \trip{};
    \item The union of \citeauthor{ramirez2024deep}'s and \citeauthor{cardace2024neural}'s datasets, i.e.\ a dataset where each ShapenetRender object appears twice, once as an MLP-based and then as a tri-planar \nerf{}, which will be referred to as \both{}.
\end{enumerate*}

\textbf{Models.} In order to assess the impact of the different losses on training, we introduce a distinction between three versions of our framework, depending on the learning objective:
\begin{enumerate*}[label=(\roman*)]
    \item $\mathcal{L}_\text{R}$, where the framework has been trained only with the rendering loss of \cref{eq:render};
    \item $\mathcal{L}_{\text{R}+\text{C}}$, where the framework has been trained with a combination of rendering and contrastive losses as in \cref{eq:combined}. We set $\lambda=\num{2e-2}$, as it leads to similar magnitudes in the two terms;
    \item $\mathcal{L}_\text{C}$, where the framework has been trained only with the contrastive loss of \cref{eq:contrastive}.
\end{enumerate*}

\textbf{Single vs multi-architecture.} Our method is the first neural processing framework able to handle multiple \nerf{} architectures. Thus, the natural setting to test it is when it is trained on \both{}; we will refer to this scenario as the \emph{multi-architecture setting}. Yet, our method can also be used when the input \nerf{}s all share the same architecture, e.g.\ when our framework has been trained on either the \mlp{} or the \trip{} dataset only, by dropping the contrastive loss. Therefore, to test its generality, we perform experiments in such a scenario, which we will refer to as the \emph{single-architecture setting}. In this setting, our approach can be compared against previous methods (\citeauthor{ramirez2024deep, cardace2024neural}) that can only handle specific architectures.

\begin{table}[t]
    \centering
    \resizebox{0.8\linewidth}{!}{
    \begin{tabular}{lcrrr}
        \toprule
        && \multicolumn{3}{c}{Accuracy (\%) $\uparrow$} \\
        \cmidrule(lr){3-5}
        Method & Classifier Training Set & \both & \mlp & \trip \\
        \midrule
        \nftovec & \multirow{5}{*}{\both} & --- & --- & --- \\
        \citeauthor{cardace2024neural} && --- & --- & --- \\
        $\mathcal{L}_\text{R}$ (ours) && \textbf{93.5} & \textbf{93.5} & \textbf{93.5} \\
        $\mathcal{L}_{\text{R}+\text{C}}$ (ours) && 91.9 & 91.8 & 91.9 \\
        $\mathcal{L}_\text{C}$ (ours) && 86.2 & 87.3 & 85.1 \\
        \cmidrule(lr){1-5}
        \nftovec & \multirow{5}{*}{\mlp} & --- & --- & --- \\
        \citeauthor{cardace2024neural} && --- & --- & --- \\
        $\mathcal{L}_\text{R}$ (ours) && 49.9 & \textbf{93.6} & 6.2 \\
        $\mathcal{L}_{\text{R}+\text{C}}$ (ours) && \textbf{82.1} & 92.4 & 71.9 \\
        $\mathcal{L}_\text{C}$ (ours) && 80.6 & 87.5 & \textbf{73.6} \\
        \cmidrule(lr){1-5}
        \nftovec & \multirow{5}{*}{\trip} & --- & --- & --- \\
        \citeauthor{cardace2024neural} && --- & --- & --- \\
        $\mathcal{L}_\text{R}$ (ours) && 64.5 & 35.5 & \textbf{93.4} \\
        $\mathcal{L}_{\text{R}+\text{C}}$ (ours) && 81.1 & 70.1 & 92.1 \\
        $\mathcal{L}_\text{C}$ (ours) && \textbf{83.7} & \textbf{82.4} & 85.1 \\
        \bottomrule
    \end{tabular}}
    \caption{\textbf{\nerf{} classification (multi-architecture).} The encoder is trained on \both{}; the classifier is trained on the datasets in column 2 and tested on those in columns 3--5.}
    \label{tab:classification-multi}
\end{table}

\subsection{Latent Space Analysis}
\label{sec:latent}

To study the impact of different learning objectives on the organization of the resulting \nerf{} latent space, we apply the t-SNE dimensionality reduction \cite{vandermaaten08vis} to the embeddings computed by our framework on \nerf{}s belonging to the test set of the \both{} dataset. \cref{fig:tsne} shows the resulting bi-dimensional plots.
%
Some interesting patterns can be noted. When trained to minimize \lr{} alone (\cref{fig:tsne}, left), the encoder creates an embedding space that is clustered by class, even if class labels have not been used at training time. This behavior is a byproduct of the loss, which enforces \nerf{}s encoding similar shapes and colors to be nearby in the latent space. Another byproduct one could expect is that the same object under different \nerf{} architectures is projected into the same embedding, since the decoder output, and therefore the condition computed by the encoder, has to be the same for both \nerf{}s. However, this turns out not to be the case. Instead, two distinct clusters emerge for each class, each corresponding to a  \nerf{} architecture, and these clusters are further away from each other than clusters sharing the same underlying architecture. This outcome shows that \lr{} alone does not directly encourage the model to align \nerf{} embeddings regardless of the input architecture.
%
Conversely, when trained to minimize \lc{} alone (\cref{fig:tsne}, right), the encoder creates a single cluster per class, causing input architectures to be indistinguishable in the plot. However, gaps between classes reduce, especially for objects with similar shapes and colors, like \emph{chairs} and \emph{tables} or \emph{airplanes} and \emph{watercrafts}. 
%
\lrc{} (\cref{fig:tsne}, middle) strikes a balance between these latent space properties. Compared to \lr{}, one macro-cluster per class is present, although for some classes, like \emph{airplane} or \emph{car}, it splits into sub-clusters according to the \nerf{} architecture. In other words, the latent space represents more the content than the architectures of the \nerf{}s, but it is not completely architecture-agnostic as when minimizing \lc{} alone. Compared to \lc{}, classes are more separated, although not as much as with \lr{} alone. Thus, we expect \lr{} to be the best choice for tasks where the separation between classes is paramount; \lc{}, on the other hand, is likely to be the most effective objective for tasks where strong invariance to the \nerf{} architecture is required; \lrc{}, finally, should provide good performance across all tasks.

\begin{table}[t]
    \centering
    \resizebox{0.75\linewidth}{!}{
    \begin{tabular}{lcc}
        \toprule
        Method & Encoder Training Set & Accuracy (\%) $\uparrow$ \\
        \midrule
        \nftovec & \multirow{3}{*}{\mlp} & 92.1 \\
        \citeauthor{cardace2024neural} && --- \\
        $\mathcal{L}_\text{R}$ (ours) && \textbf{93.6} \\
        \cmidrule(lr){1-3}
        \nftovec & \multirow{3}{*}{\trip} & --- \\
        \citeauthor{cardace2024neural} && 93.1 \\
        $\mathcal{L}_\text{R}$ (ours) && \textbf{94.0} \\
        \bottomrule
    \end{tabular}}
    \caption{\textbf{\nerf{} classification (single-architecture).} The classifier is trained and tested on the same dataset the encoder is trained on.}
    \label{tab:classification-single}
\end{table}
\subsection{\nerf{} Classification}
\label{sec:classification}

Once our framework has been trained, \nerf{} classification is performed by learning a downstream classifier $\mathcal{C}$ to predict the labels of the embeddings produced by the encoder; an overview of this procedure is outlined in \cref{fig:teaser}. Since the multi-architecture setting requires evaluating methods trained on \both{}, neither one of the previous works, i.e.\ \nftovec{} \cite{ramirez2024deep} and \citealp{cardace2024neural}, can be applied in this scenario, as they can be trained only on \mlp{} and \trip{}, respectively. As a result, the only available methods for multi-architecture \nerf{} classification are the three variants of our framework, i.e.\ \lr{}, \lrc{}, and \lc{}. In the single-architecture setting, instead, previous methods can be trained and evaluated on the datasets corresponding to the \nerf{} architecture they can ingest, whereas neither \lrc{} nor \lc{} can be applied, as there is a single architecture and, therefore, no positive pairs to compute the contrastive loss. \cref{tab:classification-multi} shows \nerf{} classification results in the multi-architecture setting. When $\mathcal{C}$ has also been trained on \both{}, \lr{} performs the best. This result is consistent with the better separation between the clusters corresponding to the different classes provided by \lr{}, as shown in \cref{fig:tsne} (left) and discussed in \cref{sec:latent}. This trend can also be observed, as expected, when $\mathcal{C}$ is trained on either \mlp{} or \trip{} and tested on that same dataset, as this scenario replicates the previous one with smaller datasets: as shown by \cref{fig:tsne} (left),  \lr{} achieves the best separation between classes of \nerf{}s parametrized by the same architecture. On the other hand, the introduction of a contrastive objective in the loss (i.e.\ \lrc{} and \lc{}) is key to performance whenever $\mathcal{C}$ is trained on a single-architecture dataset (e.g.\ \trip{}) and tested on another including different architectures (e.g.\ \mlp{}{} or \both{}), as clusters of objects belonging to the same class but parameterized by different \nerf{} architectures are closer in the embedding space than with \lr{} alone. In particular, \lc{} provides the best accuracy whenever the classifier training and test sets feature different architectures (\trip{} vs \mlp{}, \mlp{} vs \trip{}). Overall, we achieve remarkable accuracies, both in the easier case of test sets whose architectures are part of the training set of $\mathcal{C}$ and in the more challenging case of generalizing $\mathcal{C}$ across architectures, a result which validates the effectiveness of the proposed architecture-agnostic representation learning framework. 
%
As shown in \cref{tab:classification-single}, in the single-architecture setting, i.e.\ when both the encoder and the classifier are trained and tested on either \mlp{} or \trip{}, our method outperforms both \nftovec{} and \citeauthor{cardace2024neural}. Hence, beyond its original aim, it also turns out to be a competitive alternative to realize a single-architecture framework.

\begin{table}[t]
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{lcrrrrrr}
        \toprule
        && \multicolumn{6}{c}{Accuracy (\%) $\uparrow$} \\
        \cmidrule(lr){3-8}
        \multirow{2}{*}{Method} & Classifier & \multirow{2}{*}{\mlpl} & \multirow{2}{*}{\mlph} & \multirow{2}{*}{\tripl} & \multirow{2}{*}{\triph} & \multirow{2}{*}{\tripwh} & \multirow{2}{*}{\tripc} \\
        & Training Set &&&&&& \\
        \midrule
        \nftovec{} & \multirow{5}{*}{\both} & --- & --- & --- & --- & --- & --- \\
        \citeauthor{cardace2024neural} && --- & --- & --- & --- & --- & --- \\
        \lr{} (ours) && \textbf{92.1} & \textbf{87.4} & \textbf{92.7} & \textbf{89.0} & 30.1 & \textbf{75.0} \\
        \lrc{} (ours) && 87.4 & 85.8 & 90.3 & 86.4 & 63.7 & 21.3 \\
        \lc{} (ours) && 81.0 & 76.7 & 81.6 & 75.1 & \textbf{78.0} & 11.4 \\
        \cmidrule(lr){1-8}
        \nftovec{} & \multirow{5}{*}{\mlp} & --- & --- & --- & --- & --- & --- \\
        \citeauthor{cardace2024neural} && --- & --- & --- & --- & --- & --- \\
        \lr{} (ours) && \textbf{91.1} & 83.3 & 6.1 & 9.8 & 4.6 & 7.3 \\
        \lrc{} (ours) && 87.1 & \textbf{84.8} & 66.7 & 59.5 & 49.0 & \textbf{12.3} \\
        \lc{} (ours) && 80.1 & 77.0 & \textbf{76.3} & \textbf{63.3} & \textbf{75.9} & 10.4 \\
        \cmidrule(lr){1-8}
        \nftovec{} & \multirow{5}{*}{\trip} & --- & --- & --- & --- & --- & --- \\
        \citeauthor{cardace2024neural} && --- & --- & --- & --- & --- & --- \\
        \lr{} (ours) && 32.6 & 32.0 & \textbf{92.0} & \textbf{89.1} & 37.5 & \textbf{56.0} \\
        \lrc{} (ours) && 58.7 & 54.4 & 89.6 & 85.2 & 61.7 & 15.0 \\
        \lc{} (ours) && \textbf{76.2} & \textbf{70.9} & 80.6 & 74.7 & \textbf{74.1} & 11.1 \\
        \bottomrule
    \end{tabular}}
    \caption{\textbf{\nerf{} classification of unseen architectures (multi-architecture).} The encoder is trained on \both{}; the classifier is trained on the datasets in column 2 and tested on those in columns 3--8, containing \nerf{} architectures unseen at training time.}
    \label{tab:classification-multi-unseen}
\end{table}

\begin{table}[t]
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{lcrrrrrr}
        \toprule
        & \multicolumn{6}{c}{Accuracy (\%) $\uparrow$} \\
        \cmidrule(lr){3-8}
        \multirow{2}{*}{Method} & Encoder & \multirow{2}{*}{\mlpl} & \multirow{2}{*}{\mlph} & \multirow{2}{*}{\tripl} & \multirow{2}{*}{\triph} & \multirow{2}{*}{\tripwh} & \multirow{2}{*}{\tripc} \\
        & Training Set &&&&& \\
        \midrule
        \nftovec{} & \multirow{3}{*}{\mlp} & 63.7 & --- & --- & --- & --- & --- \\
        \citeauthor{cardace2024neural} && --- & --- & --- & --- & --- & --- \\
        \lr{} (ours) && \textbf{91.8} & \textbf{83.7} & \textbf{10.9} & \textbf{6.1} & \textbf{7.0} & \textbf{4.9} \\
        \cmidrule(lr){1-8}
        \nftovec{} & \multirow{3}{*}{\trip} & --- & --- & --- & --- & --- & --- \\
        \citeauthor{cardace2024neural} && --- & --- & \textbf{93.5} & \textbf{92.0} & --- & 68.6 \\
        \lr{} (ours) && \textbf{10.1} & \textbf{10.0} & 92.6 & 82.5 & \textbf{22.9} & \textbf{72.8} \\
        \bottomrule
    \end{tabular}}
    \caption{\textbf{\nerf{} classification of unseen architectures (single-architecture).} The classifier is trained on the same dataset as the encoder and tested on the datasets in columns 3--8, containing \nerf{} architectures unseen at training time.}
    \label{tab:classification-single-unseen}
\end{table}

\textbf{Unseen architectures.} To further assess the ability of our framework to perform tasks on arbitrary \nerf{} architectures, we create additional test sets of \nerf{}s featuring architectures different  than those in \mlp{} and \trip{}:
\begin{enumerate*}[label=(\roman*)]
    \item \mlpl, i.e.\ \mlp{} where MLPs have 2 hidden layers instead of 3;
    \item \mlph, i.e.\ \mlp{} where MLPs have hidden dimension 32 instead of 64;
    \item \tripl, i.e.\ \trip{} where MLPs have 2 hidden layers instead of 3;
    \item \triph, i.e.\ \trip{} where MLPs have hidden dimension 32 instead of 64;
    \item \tripwh, i.e.\ \trip{} where tri-planes have spatial resolution $16\times16$ instead of $32\times32$;
    \item \tripc, i.e.\ \trip{} where tri-planes have  8 channels instead of 16.
\end{enumerate*}
We then evaluate our method on these new test sets without changing the previous multi and single-architecture settings, which implies that neither the encoder nor the classifier sees these architectures at training time.  %Again, neither \nftovec{} nor \citealp{cardace2024neural} can be included in the multi-architecture evaluation. 
Multi-architecture classification results are shown in \cref{tab:classification-multi-unseen}. Remarkably, a trend coherent to that of \cref{tab:classification-multi} can be noticed, namely \lr{} tends to perform best or close to best when the classifier $\mathcal{C}$ is trained either on \both{} or on an architecture similar to the unseen one presented at test time (e.g.\ trained on \mlp{} and tested on \mlpl{} or trained on \trip{} and tested on \tripl{}), whereas \lc{} tends to prevail when changing the architecture type. A notable exception is \tripwh{}, where the contrastive loss consistently yields the best results. Some other interesting patterns can be noted: reducing the number of hidden layers of an MLP has less impact on accuracy than reducing the hidden dimension of layers, both for the MLP-only case and when the MLP is part of a tri-plane; changing hyperparameters of tri-planes is the most disruptive change, especially if the number of channels is reduced. These patterns can be the basis of future investigation into this topic. 
Overall, our framework trained on \both{} proves to be quite robust and provides the first working solution to the problem of embedding \nerf{}s with unseen architectures.
%
In the single-architecture setting, shown in \cref{tab:classification-single-unseen}, \lr{} performs much better than \nftovec{} in the only case in which it can be tested. Instead, \lr{} performs worse than \citeauthor{cardace2024neural}\ in \tripl{} and \triph{}, i.e.\ \trip{} versions that change the MLP architecture. This is due to the fact that, when performing classification, \citeauthor{cardace2024neural} processes the tri-plane alone and discards the MLP, which makes their approach invariant to changes in the MLP architecture. Yet, since their method is tied to the tri-plane quantization, it cannot process \tripwh{} and is less robust to the reduction in the number of channels.
%
t-SNE visualizations of seen and unseen MLPs are provided in \cref{app:details}.  

\begin{table}[t]
    \centering
    \resizebox{0.97\linewidth}{!}{
    \begin{tabular}{lllrrr}
        \toprule
        &&& \multicolumn{3}{c}{Recall@$k$ (\%) $\uparrow$} \\
        \cmidrule(lr){4-6}
        Method & Query & Gallery & $k=1$ & $k=5$ & $k=10$ \\
        \midrule
        \nftovec{} \cite{ramirez2024deep} & \multirow{5}{*}{\both} & \multirow{5}{*}{\both} & --- & --- & --- \\
        \citealp{cardace2024neural} &&& --- & --- & --- \\
        \lr{} (ours) &&& 0.0 & 0.0 & 0.0 \\
        \lrc{} (ours) &&& 11.2 & 32.6 & 46.3 \\
        \lc{} (ours) &&& \textbf{29.0} & \textbf{57.8} & \textbf{70.7} \\
        \cmidrule(lr){1-6}
        \nftovec{} \cite{ramirez2024deep} & \multirow{5}{*}{\mlp} & \multirow{5}{*}{\trip} & --- & --- & --- \\
        \citealp{cardace2024neural} &&& --- & --- & --- \\
        \lr{} (ours) &&& 1.2 & 4.1 & 6.6 \\
        \lrc{} (ours) &&& 41.2 & 70.2 & 80.9 \\
        \lc{} (ours) &&& \textbf{45.5} & \textbf{74.9} & \textbf{85.2} \\
        \cmidrule(lr){1-6}
        \nftovec{} \cite{ramirez2024deep} & \multirow{5}{*}{\trip} & \multirow{5}{*}{\mlp} & --- & --- & --- \\
        \citealp{cardace2024neural} &&& --- & --- & --- \\
        \lr{} (ours) &&& 2.7 & 8.4 & 12.4 \\
        \lrc{} (ours) &&& 42.2 & 69.6 & 79.7 \\
        \lc{} (ours) &&& \textbf{44.1} & \textbf{75.2} & \textbf{85.1} \\
        \bottomrule
    \end{tabular}}
    \caption{\textbf{Instance-level \nerf{} retrieval.} The encoder is trained on \both{}. Query and gallery belong to their corresponding test sets. $k$-NN is computed with cosine distances.}
    \label{tab:retrieval-instance}
\end{table}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{img/retrieval.pdf}
    \caption{\textbf{\nerf{} retrieval (\lc{} qualitative results).} Green circles denote \nerf{}s representing the same object as the query, but with a different architecture, i.e.\ cases in which the instance-level retrieval goal has been accomplished (for $k=5$). Notice a failure case on the third row of the top figure. See \cref{app:qualitatives} for an extended version of this figure.}
    \label{fig:retrieval}
\end{figure}

\subsection{\nerf{} Retrieval}
\label{sec:retrieval}

The embeddings produced by the trained encoder can also be used to perform retrieval tasks via $k$-nearest neighbor search, as outlined in \cref{fig:teaser}. In particular, we define \emph{instance-level} retrieval as follows: given a \nerf{} embedding of a given object (a.k.a.\ the \emph{query}) and a \emph{gallery} of \nerf{} embeddings, the goal is to find the embedding in the gallery that represents the same object as the query but encodes a different \nerf{} architecture. Naturally, this task can only exist in the multi-architecture setting, as there need to be (at least) two instances of the same object, and cannot thus be performed by either \nftovec{} or \citealp{cardace2024neural}. We evaluate the performance on this task with the recall@$k$ metric \cite{wang2017deep}, defined in this context as the percentage of queries whose $k$ nearest neighbors contain the \nerf{} representing the same object as the query. In \cref{app:retrieval}, we also show results of \emph{class-level} \nerf{} retrieval, which confirm the trend observed in the classification results of \cref{sec:classification}, i.e.\ that the task benefits from the presence of a contrastive objective whenever the dataset on which it is performed requires reasoning across architectures (e.g.\ \mlp{} query and \trip{} gallery). \cref{tab:retrieval-instance} shows the instance-level \nerf{} retrieval results. It is immediately clear that, when query and gallery belong to \both{}, \lr{} is unable to perform the task: the latent space organization shown in \cref{fig:tsne} (left) and discussed in \cref{sec:latent} prevents \lr{} from being capable of recognizing the same object represented by different \nerf{} architectures. \lr{}'s performance when the (query, gallery) pair is (\mlp, \trip) or (\trip, \mlp) is slightly better (as the gallery is more constrained and the chance of missing the target instance is thus lower), but still very poor nonetheless. On the other hand, this is the task where the contribution of the contrastive loss emerges the most, which confirms our intuition that \lc{} favors an architecture-agnostic latent space, as discussed in \cref{sec:latent}. \lc{} outperforms \lr{} in all scenarios, with \lrc{} being somewhat in the middle for (\both, \both) while being closer to \lc{} for (\mlp, \trip) and (\trip, \mlp).  \cref{fig:retrieval} displays qualitatively that the organization of the latent space captures the \nerf{} similarity in color and shape.

\section{Conclusions and Future Work}
\label{sec:conclusions}
In this paper, we explored the task of creating a representation learning framework that directly processes \nerf{} weights. Our framework, based on a Graph Meta-Network encoder ingesting the \nerf{} parameter graph, is able, for the first time, to
\begin{enumerate*}[label=(\roman*)]
    \item handle any input neural architecture and
    \item process architectures unseen at training time.
\end{enumerate*}
We investigated a rendering and a contrastive loss to train it and showed that they serve complementary purposes, favoring either class-level separability or invariance to the \nerf{} architecture. %We experimentally characterized the pros and cons of both losses and their combination in several downstream tasks for the case of handling \nerf{}s implemented as MLPs or tri-planes. 
When tested on \nerf{}s implemented as MLP and tri-planes, our framework delivers excellent performance for the multi-architecture setting, and it is as effective as or better than previous methods when trained and tested on a specific \nerf{} architecture. 
The main limitation of our study lies in its experimental validation, currently limited to the two \nerf{} architectures considered in previous work  \citep{ramirez2024deep,cardace2024neural}, which, however, are not state-of-the-art. Verifying its effectiveness on architectures based on hash tables \citep{muller2022instant, barron2023zip} that are routinely used by practitioners would enhance its practical usefulness and adoption. 
%Another interesting extension would be to adapt the framework to embed neural fields other than \nerf{}s, e.g.\ to explore architecture-agnostic representation learning for signed-distance or occupancy fields. 
Finally, future work should also explore language-related tasks for \nerf{}s, such as the creation of \nerf{} assistants \citep{amaduzzi2024llana} that are architecture-agnostic.

% Acknowledgements should only appear in the accepted version.
% \section*{Acknowledgements}

% \section*{Impact Statement}

% This paper presents work whose goal is to advance the field of machine learning. There may be potential societal consequences
% of our work, none of which we feel must be specifically highlighted here.

\bibliography{main}
\bibliographystyle{icml2025}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage

\appendix
\onecolumn

In this appendix, we discuss additional experiments and provide a detailed overview of the datasets and the architecture of our method.

\section{Class-level \nerf{} Retrieval}
\label{app:retrieval}

Class-level \nerf{} retrieval results in the multi-architecture setting are shown in \cref{tab:retrieval-class-multi}. Following the definition by \citealp{wang2017deep}, we call \emph{class recall}@$k$ the percentage of queries whose $k$ nearest neighbors contain at least one gallery element with the same class as the query. \cref{tab:retrieval-class-multi} shows that different combinations of query and gallery lead to analogous results to those observed in \cref{tab:classification-multi} for different classifier training and test sets: contrastive variants \lrc{} and \lc{} largely outperform \lr{} whenever query and gallery belong to different architectures, i.e.\ when the (query, gallery) pair is either (\mlp, \trip) or (\trip, \mlp), due to the organization of the latent space they induce (\cref{fig:tsne}). In the (\both, \both) case, on the other hand, results are comparable across models, with a larger gap only for \lr{} at $k=1$. This is also consistent with previous results, where \lr{} was shown to perform the best when the classifier is tested on \both{} (\cref{tab:classification-multi}). 

\begin{table}[h]
    \centering
    \resizebox{0.6\linewidth}{!}{
    \begin{tabular}{lllrrr}
        \toprule
        &&& \multicolumn{3}{c}{Class Recall@$k$ (\%) $\uparrow$} \\
        \cmidrule(lr){4-6}
        Method & Query & Gallery & $k=1$ & $k=5$ & $k=10$ \\
        \midrule
        \nftovec{} \cite{ramirez2024deep} & \multirow{5}{*}{\both} & \multirow{5}{*}{\both} & --- & --- & --- \\
        \citealp{cardace2024neural} &&& --- & --- & --- \\
        \lr{} (ours) &&& \textbf{85.0} & 95.6 & 97.5 \\
        \lrc{} (ours) &&& 82.5 & 96.2 & 98.2 \\
        \lc{} (ours) &&& 82.7 & \textbf{96.5} & \textbf{98.5} \\
        \cmidrule(lr){1-6}
        \nftovec{} \cite{ramirez2024deep} & \multirow{5}{*}{\mlp} & \multirow{5}{*}{\trip} & --- & --- & --- \\
        \citealp{cardace2024neural} &&& --- & --- & --- \\
        \lr{} (ours) &&& 42.7 & 77.6 & 87.3 \\
        \lrc{} (ours) &&& \textbf{86.3} & \textbf{98.4} & \textbf{99.3} \\
        \lc{} (ours)&&& 85.3 & 98.0 & \textbf{99.3} \\
        \cmidrule{1-6}
        \nftovec{} \cite{ramirez2024deep} & \multirow{5}{*}{\trip} & \multirow{5}{*}{\mlp} & --- & --- & --- \\
        \citealp{cardace2024neural} &&& --- & --- & --- \\
        \lr{} (ours) &&& 54.0 & 79.1 & 86.8 \\
        \lrc{} (ours) &&& \textbf{87.1} & 97.5 & 98.9 \\
        \lc{} (ours) &&& 83.8 & \textbf{97.6} & \textbf{99.1} \\
        \bottomrule
    \end{tabular}}
    \caption{\textbf{Class-level NeRF retrieval (multi-architecture).} The encoder is trained on \both{}. Query and gallery belong to their corresponding test sets. $k$-NN is computed with cosine distances.}
    \label{tab:retrieval-class-multi}
\end{table}

\section{Additional Experimental Details}
\label{app:details}

\textbf{\nerf{} datasets.} As described in \cref{sec:experiments}, the \mlp{} and \trip{} datasets used throughout our experimental evaluation are the \nerf{} datasets proposed by \citealp{ramirez2024deep} and \citealp{cardace2024neural}, respectively. They both train their \nerf{}s on ShapenetRender \cite{xu2019disn}, a dataset featuring RGB images and class labels of synthetic objects belonging to 13 classes of the ShapeNetCore dataset \cite{shapenet}: \emph{airplane}, \emph{bench}, \emph{cabinet}, \emph{car}, \emph{chair}, \emph{display}, \emph{lamp}, \emph{speaker}, \emph{rifle}, \emph{sofa}, \emph{table}, \emph{phone}, and \emph{watercraft}. For each object, the dataset contains 36 $224\times224$ images. We follow \citeauthor{ramirez2024deep}'s train-val-test split without augmentations, consisting of 31744 \nerf{}s used for training, 3961 for validation, and 3961 for testing. The difference between \citealp{ramirez2024deep} and \citealp{cardace2024neural} lies in the \nerf{} architecture they use. \citeauthor{ramirez2024deep}'s are MLPs with 3 hidden layers of equal dimension 64 intertwined with ReLU activations. The input coordinates $(x,y,z)$ are mapped to a vector of size 144 through a frequency encoding \cite{mildenhall2020nerf} before being fed to the input layer of the MLP, whose outputs are the four $(r,g,b,\sigma)$ predicted radiance field values. No viewing direction $(\theta,\phi)$ is used. \citeauthor{cardace2024neural}'s \nerf{}s are instead made of a tri-plane with spatial resolution $32\times32$ and 16 channels followed by an MLP with 3 hidden layers of equal size 64 intertwined with sine activation functions \cite{sitzmann2020implicit}. The input coordinates $(x,y,z)$ are mapped to a vector of size 288 through a frequency encoding before being concatenated with the tri-plane output, resulting in a vector of size $288+16=304$ being fed to the input layer of the MLP, whose outputs are the four $(r,g,b,\sigma)$ predicted radiance field values. Again, no viewing direction $(\theta,\phi)$ is used. For our datasets of unseen architectures described in \cref{sec:classification}, we follow \citeauthor{ramirez2024deep}'s formulation for \mlpl{} and \mlph{} and \citeauthor{cardace2024neural}'s formulation for \tripl{}, \triph{}, \tripwh{}, and \tripc{}, except for the specific architectural choice that makes each dataset different from the training ones (e.g.\ 2 hidden layers instead of 3 in \mlpl{}).

\textbf{Encoder architecture.} Our framework's encoder is a slight re-adaptation of the graph meta-network used in \cite{lim2024graph} in their experiment called \emph{predicting accuracy for varying architectures}. It has the same network hyper-parameters: hidden dimension 128, 4 layers, a pre-encoder, 2 readout layers, and uses directed edges. However, differently from \citealp{lim2024graph}, before average-pooling the edge features, it maps them to vectors of size 1024 through a single linear layer, resulting in a final embedding of size 1024 as in \nftovec{} \cite{ramirez2024deep}.

\textbf{Decoder architecture.} We use the \nftovec{} decoder in its original formulation \cite{ramirez2024deep}. The input coordinates $(x,y,z)$ are mapped to a vector of size 144 through a frequency encoding \cite{mildenhall2020nerf} and concatenated with the 1024-dimensional embedding produced by the encoder. The resulting vector of size 1168 becomes the input of the decoder, which consists of 4 hidden layers with dimension 1024 intertwined with ReLU activations, with a skip connection that maps the input vector to a vector of size 1024 via a single linear layer + ReLU and sums it to the output of the second hidden layer. Finally, the decoder outputs the four $(r,g,b,\sigma)$ predicted radiance field values.

\textbf{Training.} Our encoder-decoder framework is trained end-to-end for 250 epochs with batch size 8, AdamW optimizer \cite{loshchilov2018decoupled}, one-cycle learning rate scheduler \cite{onecycle}, maximum learning rate \num{1e-4} and weight decay \num{1e-2}. These training hyper-parameters are the same for \lr{}, \lrc{}, and \lc{}. In \cref{eq:rend}, foreground and background pixels are weighted by 0.8 and 0.2, respectively, as done in \citealp{ramirez2024deep}. In \cref{eq:contrastive}, $t$ and $b$ are initialized to $10$ and $-10$, respectively, as done in \citealp{zhai2023sigmoid}. In \cref{eq:combined}, $\lambda$ is set to \num{2e-2}, as we experimentally observed that this choice leads to \lr{} and \lc{} values of the same order of magnitude.

\textbf{Classification.} In our classification experiments of \cref{sec:classification}, the classifier is a concatenation of 3 (linear $\rightarrow$ batch-norm $\rightarrow$ ReLU $\rightarrow$ dropout) blocks, where the hidden dimension of the linear layers are 1024, 512, and 256, respectively, and a final linear layer at the end computes the class logits. The classifier is trained via the cross-entropy loss for 150 epochs with batch size 256, AdamW optimizer, one-cycle learning rate scheduler, maximum learning rate \num{1e-4}, and weight decay \num{1e-2}. These same network and training hyper-parameters are used in the classification experiments of \citealp{ramirez2024deep}.

\begin{figure*}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{img/tsne_mlp_unseen.pdf}
    \caption{\textbf{t-SNE plots (MLP vs unseen MLPs).} Models are trained on \both{}. Classes refer to ShapenetRender objects \cite{xu2019disn}.}
    \label{fig:tsne-unseen}
\end{figure*}

\section{Additional Qualitative Results}
\label{app:qualitatives}

\textbf{t-SNE.} \cref{fig:tsne-unseen} shows a t-SNE plot \cite{vandermaaten08vis} that, differently from the one in \cref{fig:tsne}, features MLPs together with the unseen MLP variants. Two remarkable conclusions can be drawn:
\begin{enumerate*}[label=(\roman*)]
    \item \lr{} creates architecture-based sub-clusters for each class, not only for known architectures but also for those that it has not seen at training time;
    \item the contrastive objective pushes \nerf{}s of the same class with different MLP architectures closer to each other in the embedding space, although it has not been explicitly instructed to do so, as two of those architectures have not been seen at training time. This suggests that the contrastive models (especially \lc{}) have effectively generalized the notion of similarity between underlying object appearances of different \nerf{} parameterizations beyond the MLP and tri-plane architectures they have been trained on.
\end{enumerate*}

\textbf{Retrieval.} \cref{fig:retrieval-full} extends \cref{fig:retrieval} by showing additional qualitative retrieval results. It is worth noting that, although the 44--45\% recall@1 in \cref{tab:retrieval-instance} might suggest an undesirable outcome, by looking at all 10 nearest neighbors, one can appreciate the similarity in color and/or appearance compared to the query (e.g.\ similarly shaped white chairs, similar grey-colored planes, similar L-shaped sofas, and similarly rounded tables), even when the target is not among the first few neighbors. Our framework seems to have encoded both relevant information about the object's appearance and some invariance to the \nerf{} architecture that parameterizes it; nonetheless, the task of instance-level retrieval is challenging and far from being solved.

\section{Implementation and Hardware}
\label{app:implementation}
Our framework implementation is built upon the codebases by \citealp{ramirez2024deep} (for decoder and training) and \citealp{lim2024graph} (for the GMN architecture and graph conversion). We plan to publicly release our code upon paper acceptance. All our experiments were performed on a single NVIDIA GeForce RTX 3090 GPU. Training either \lr{} or \lrc{} took ${\sim}2$ weeks, whereas training \lc{} took ${\sim}4$ days.

\begin{figure}[b!]
    \centering
    \includegraphics[width=0.9\linewidth]{img/retrieval_full.pdf}
    \caption{\textbf{\nerf{} retrieval (\lc{} qualitative results).} Extended version of \cref{fig:retrieval}. Green circles denote \nerf{}s representing the same object as the query, but with a different architecture, i.e.\ cases in which the instance-level retrieval goal has been accomplished (for $k=10$).}
    \label{fig:retrieval-full}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}
