\section{Related Work}
\noindent
Offline model-based methods~\cite{DBLP:KidambiRNJ20,DBLP:SwazinnaUR21} learn an approximate dynamics model of the environment and utilize planning algorithms to search for optimal trajectories within that model.
However, research indicates that even small generation errors can significantly degrade the performance of multi-step rollouts, as generation errors compound, causing the model to deviate from the high-precision region after a few steps~\cite{DBLP:Talvitie17,DBLP:AsadiML18}.
To mitigate generation errors, some studies~\cite{DBLP:YuTYEZLFM20,DBLP:abs-2111-11097,DBLP:ZhengWXH23} employ multiple models to predict states and actions, using uncertainty quantification to eliminate samples with excessive errors.
In addition, some studies~\cite{chen2022lapo,DBLP:YangJZMS22,DBLP:MaT0M23} restrict the learning policy from accessing areas with significant differences between the learned and the real dynamics to prevent policy learning failures due to generation errors.

Recent studies~\cite{DBLP:WangLJZLZ21,DBLP:LyuLL22,DBLP:LuBTP23,DBLP:abs-2406-12550} suggest the use of fixed truncation techniques~\cite{DBLP:JannerFZL19} to generate shorter trajectories and thus reduce the impact of generation errors on generated data.
This generated data is then combined with the original data to create a new dataset, which can be utilized in model-free offline algorithms.
However, these studies neglect the influence of historical information on environmental dynamics, potentially resulting in the generation of data that fails to align with the environment.
% However, these studies overlook the impact of historical information and the pursuit of high rewards on the generated data.
Unlike the above methods, RT extends the offline dataset by capturing historical information between different locations in the input sequence and using reliability assessment to generate reliable trajectories.
Moreover, RT employs a cumulative reliability metric to dynamically adjust the trajectory length, enabling a more flexible mitigation of the adverse effects of accumulated generation errors.
% Unlike the above methods, RT extends offline data by capturing the interdependencies at various positions along the trajectories and using high reward sampling to generate high-return data. 
\\
\\
\textbf{Sequence modeling}.
% Recent works~\cite{DBLP:JannerLL21,DBLP:DT,DBLP:YamagataKS23,DBLP:BadrinathFNB23,DBLP:GaoWCKZ024} treat RL as a sequence modeling problem, thereby incorporating Transformers into RL.
Recent research~\cite{DBLP:JannerLL21,DBLP:DT,DBLP:YamagataKS23,DBLP:BadrinathFNB23,DBLP:GaoWCKZ024} shifts the paradigm of RL from traditional MDP to sequence modeling.
In particular, Trajectory Transformer (TT)~\cite{DBLP:JannerLL21} treats offline RL as a sequence modeling problem and further leverages the capabilities of sequence models by using beam search to incorporate states, actions, and rewards.
At the some time, Decision Transformer (DT)~\cite{DBLP:DT} learns the distribution of trajectories and predicts actions based on the given target reward and preceding states, rewards, and actions.
Q-Learning DT~\cite{DBLP:YamagataKS23} extends the use of DT in offline RL by introducing a value function, which relabels the returns in the training data using the dynamics planning results, and then uses the relabeled data to train the DT.
Multi-Game Decision Transformer (MGDT)~\cite{DBLP:LeeNYLFGFXJMM22} trains a Transformer to solve multiple Atari games, which eliminates the need for additional fine-tuning of unseen tasks.
Waypoint Transformer (WT)~\cite{DBLP:BadrinathFNB23} utilizes an architecture constructed based on the DT framework and conditioned on automatically generated waypoints to learn optimal Transformer policies.
Compared to these model-free approaches, we propose a model-based approach that generates high-return trajectories from suboptimal datasets using the Reliability-guaranteed Transformer framework  to facilitate policy learning in model-based RL.