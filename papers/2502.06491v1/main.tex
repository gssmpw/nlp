%%%% ijcai25.tex

\typeout{IJCAI--25 Instructions for Authors}

% These are the instructions for authors for IJCAI-25.

\documentclass{article}
\pdfpagewidth=8.5in
\pdfpageheight=11in

% The file ijcai25.sty is a copy from ijcai22.sty
% The file ijcai22.sty is NOT the same as previous years'
\usepackage{ijcai25}

% Use the postscript times font!
\usepackage{times}
\usepackage{soul}
\usepackage{url}
\usepackage[hidelinks]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[small]{caption}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[switch]{lineno}
\usepackage{subfigure}
\usepackage{amsfonts} % 或者
%\usepackage{amssymb}
% Comment out this line in the camera-ready submission
\linenumbers

\urlstyle{same}

% the following package is optional:
%\usepackage{latexsym}

% See https://www.overleaf.com/learn/latex/theorems_and_proofs
% for a nice explanation of how to define new theorems, but keep
% in mind that the amsthm package is already included in this
% template and that you must *not* alter the styling.
% \newtheorem{example}{Example}
% \newtheorem{theorem}{Theorem}
\newtheorem{defn}{Definition}
\newtheorem{thm}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{lemma}{Lemma}
% Following comment is from ijcai97-submit.tex:
% The preparation of these files was supported by Schlumberger Palo Alto
% Research, AT\&T Bell Laboratories, and Morgan Kaufmann Publishers.
% Shirley Jowell, of Morgan Kaufmann Publishers, and Peter F.
% Patel-Schneider, of AT\&T Bell Laboratories collaborated on their
% preparation.

% These instructions can be modified and used in other conferences as long
% as credit to the authors and supporting agencies is retained, this notice
% is not changed, and further modification or reuse is not restricted.
% Neither Shirley Jowell nor Peter F. Patel-Schneider can be listed as
% contacts for providing assistance without their prior permission.

% To use for other conferences, change references to files and the
% conference appropriate and use other authors, contacts, publishers, and
% organizations.
% Also change the deadline and address for returning papers and the length and
% page charge instructions.
% Put where the files are available in the appropriate places.


% PDF Info Is REQUIRED.

% Please leave this \pdfinfo block untouched both for the submission and
% Camera Ready Copy. Do not include Title and Author information in the pdfinfo section
\pdfinfo{
/TemplateVersion (IJCAI.2025.0)
}

% \title{Improving Model-Based Offline Reinforcement Learning with Reliability-Driven Sequence Modeling}
\title{Model-Based Offline Reinforcement Learning with \\ Reliability-Guaranteed Sequence Modeling}

% Single author syntax
% \author{
%     Shenghong He
%     \affiliations
%     \emails
% }

% Multiple author syntax (remove the single-author syntax above and the \iffalse ... \fi here)
% \iffalse
\author{
Shenghong He
\and
Chao Yu\and
Qian Lin\and
Yile Liang\and 
Donghui Li\And
Xuetao Ding\\
\affiliations
% $^1$First Affiliation\\
% $^2$Second Affiliation\\
% $^3$Third Affiliation\\
% $^4$Fourth Affiliation\\
\emails
% \{first, second\}@example.com,
% third@other.example.com,
% fourth@example.com
}
% \fi

\begin{document}

\maketitle

\nolinenumbers
\begin{abstract}
% \linenumbers
Model-based offline reinforcement learning (MORL) aims to learn a policy by exploiting a dynamics model derived from an existing dataset. 
% Applying conservative quantification to the dynamics model, most existing methods generate trajectories that are consistent with the distribution of real data in order to facilitate policy learning.
% Applying conservative quantification to the dynamics model, most existing methods use current information (i.e., the state and action at time step $t$) to generate next time data that approximates the real data distribution in order to facilitate policy learning.
Applying conservative quantification to the dynamics model, most existing works on MORL generate trajectories that approximate the real data distribution to facilitate policy learning by using current information (e.g., the state and action at time step $t$).
However, these works neglect the impact of historical information on environmental dynamics, leading to the generation of unreliable trajectories that may not align with the real data distribution.
% Additionally, existing works cannot effectively generate high-return trajectories, which can provide action sequences with high rewards for policy learning.
%Moreover, existing works cannot guarantee the generation of high-return trajectories (i.e., action sequences with high rewards) for policy learning, as they primarily imitate data transitions without effectively identifying states and actions that lead to higher cumulative returns.
% However, these methods simply focus on discrepancies between generated trajectories and real data distributions, but neglect the quality of generated trajectories (i.e., cumulative rewards). 
% Additionally, in order to avoid generating unreliable trajectories, existing methods typically limit the length of generated trajectories to a fixed value, which could possibly restrict the capacity for generating high-quality trajectories.
In this paper, we propose a new MORL algorithm \textbf{R}eliability-guaranteed \textbf{T}ransformer (RT), which can eliminate unreliable trajectories by calculating the cumulative reliability of the generated trajectory (i.e., using a weighted variational distance away from the real data).
Moreover, by sampling candidate actions with high rewards, RT can efficiently generate high-return trajectories from the existing offline data.
% RT captures historical information between different positions to generate data and introduces a reliable trajectory estimation to assess the reliability of the generated data. 
% In order to generate high-return trajectories, RT uses the log probabilities of high-reward actions as a heuristic to generate trajectories.
% In this paper, we propose \textbf{R}eliability-Guaranteed \textbf{T}ransformer (RT), which leverages historical information to generate reliable trajectories with high future returns.
% To solve the fixed truncation problem, RT also introduces a reliable trajectory estimation to evaluate the reliability of generated trajectories and adaptively truncate trajectories.
We theoretically prove the performance guarantees of RT in policy learning, and empirically demonstrate its effectiveness against state-of-the-art model-based methods  on several benchmark tasks.
\end{abstract}




\section{Introduction}



% Reinforcement learning (RL) achieves state-of-the-art performance in dealing with numerous sequential decision-making problems.
% The necessity for extensive interaction with the environment constrains the application of RL in numerous real-world domains, which often comes with high or dangerous costs.
% The requirement for interaction with the environment often incurs high or dangerous costs, which limits the application of RL in many real-world domains.
% However, the requirement for interaction with the environment can result in significant costs or dangers, limiting the application of RL in many real-world scenarios. 
% Offline RL learns policies from historical datasets, thereby avoiding the costs and risks associated with direct exploration.
% A major challenge in offline RL is the value estimation for counterfactual actions not present in the dataset, termed the out-of-distribution (OOD) issues~\cite{DBLP:PengHLZ23}.
% To address this challenge, researchers have proposed various solutions, which can be categorized into two major classes: model-free methods~\cite{DBLP:WangYW0Q24,DBLP:ZhangLLN0LO24} and model-based methods~\cite{DBLP:LuBTP23,DBLP:WuZYGGPLHPYJCCL24}.
% Model-free methods typically use conservative~\cite{DBLP:ZhangLLN0LO24} or constrained~\cite{DBLP:ZhangZWJ23} mechanisms to address the out-of-distribution issues. 
% The core idea is to avoid using actions in the Bellman update process that have not appeared in the dataset.

% Model-based offline reinforcement learning~\cite{DBLP:LuBTP23,DBLP:WuZYGGPLHPYJCCL24} involves learning dynamic models from offline datasets, which are then used to generate trajectories for policy learning without direct interaction with the environment xxxx.
Model-based offline reinforcement learning (MORL)~\cite{DBLP:KidambiRNJ20} aims to learn a policy by leveraging a dynamics model derived from an existing dataset, without requiring interaction with the environment.
% Model-based offline reinforcement learning (RL)  methods~\cite{DBLP:LuBTP23,DBLP:WuZYGGPLHPYJCCL24} extend the data distribution by fitting a dynamics model to solve the out-of-distribution (OOD) issues~\cite{DBLP:PengHLZ23}.
% which uses offline datasets to model the environment, predicting future state and rewards to learn a better policy.
% Then, 
% predicting state transfers and rewards to improve the generalization of the policy.
% However, research shows that directly applying model-based online RL methods (e.g., MBPO~\cite{DBLP:JannerFZL19} ) on offline datasets often yields unsatisfactory results, as model uncertainty cannot be eliminated through environmental interactions~\cite{DBLP:YuTYEZLFM20}.
Recent approaches often leverage uncertainty quantification~\cite{DBLP:YuTYEZLFM20,DBLP:WagenmakerSJ23,DBLP:BhardwajRSAHCK24} and value function penalization~\cite{yu2021combo,DBLP:SunZJLYY23,DBLP:LuisBVB023} to learn the dynamics model, ensuring that the data generated based on current information (e.g., state and action) align with the real data distribution.
% , reducing the impact of model errors on the policy learning.
% Some works~\cite{DBLP:WangLJZLZ21,DBLP:LyuLL22,DBLP:LuBTP23,DBLP:abs-2406-12550} utilize the learned dynamics models for offline data augmentation, thus learning a better policy
% Recent model-based offline RL methods often leverage uncertainty quantification~\cite{DBLP:YuTYEZLFM20,DBLP:WagenmakerSJ23,DBLP:BhardwajRSAHCK24} and penalizing the value function~\cite{yu2021combo,DBLP:SunZJLYY23, DBLP:conf/LuisBVB023} 

% However, these studies focus on minimizing the errors between the distribution of generated trajectories and real data, without considering the quality of the generated trajectories (i.e., the cumulative rewards of the generated trajectories).
% However, these methods typically use information (e.g., state and action) from the current moment to generate data, and they overlook the impact of historical information on environmental dynamics, which results in generated trajectories that may not exist in the real environment (i.e., unreliable trajectories).
However, these methods implicitly assume that the current information fully provides all relevant information for data generation at the next time step. Thus, unreliable trajectories that may not exist in the real environment can be generated due to neglect in considering the impact of historical information on the environmental dynamics.
For instance, considering a robot performing cleaning tasks in a complex room,
%At each time step $t$, the robot selects an action (i.e., movement direction or cleaning behavior) to maximize task completion efficiency.
at a historical time step $t-k$, the robot takes an action of moving a chair, causing a partial obstruction in the passage.
At the current time step $t$ , employing a generative model to generate trajectories could result in the robot passing through the chair due to the model's inability to remember the historical action of moving a chair.
This generated trajectory is unreliable because the robot cannot pass through the chair in the real environment.
% In addition, these studies ignore the importance of  high-return trajectories (i.e., the cumulative rewards of the generated trajectories) generated by the model, which is crucial for policy learning because high-return data can provide essential information about action sequences that lead to high rewards.
Furthermore, by simply focusing on imitating the observed data transitions, existing methods generally fail to identify states and actions that lead to high cumulative returns, thus significantly impairing the learning performance.
% Furthermore, despite focusing on imitating observed data transitions, these studies ignore the importance of high-return trajectories (i.e., trajectories with high cumulative rewards), which are crucial for policy learning as they reveal effective action sequences.

To address the above issues, in this paper, we propose a novel method called \textbf{R}eliability-guaranteed \textbf{T}ransformer (RT).
% Specifically, RT captures the dependencies between different positions in the input sequence to generate states, actions, and rewards, as state transitions may be related to actions or states much earlier in the sequence.
Specifically, taking advantage of the sequence modeling capabilities of transformer, RT captures historical information between different positions in the input sequence to generate data (e.g., states, actions, and rewards).
Subsequently, RT computes the variation distance between the generated data and the real data to derive a reliability value of the generated data for each time step,
% By performing a weighted sum of reliability values along the trajectory, the cumulative reliability of a trajectory can be approximated, which measures the feasibility of the generated trajectory in a real environment.
%A cumulative reliability of a trajectory is then derived as the weighted summation of the reliability values along the trajectory, serving as a measure of its feasibility within the environment.
and then incorporates a truncation metric based on the cumulative reliability along the trajectory to adaptively determine the trajectory length, so as to mitigate the accumulation of errors and consequently enhances the overall reliability of the generated sequences.
% RT employs a truncation metric based on cumulative reliability to adaptively determine the trajectory length, mitigating the adverse effects of generation errors.
% RT incorporates a truncation metric based on cumulative reliability to adaptively determine the trajectory length, which mitigates the accumulation of errors in generated sequences, enhancing the overall quality of the generated data.
% Then, RT introduces a reliability estimation mechanism that derives the generation reliability value for each time step based on the distribution variational error between the generated data and the real data.
% The cumulative reliability of a trajectory is defined as the weighted sum of the reliability values at each step, which measures the feasibility of the generated trajectory in the real environment.
% By leveraging the cumulative reliability, RT can also adaptively adjust the length of generated trajectories to alleviate the influence of generation errors.
%While RT can generate reliable trajectories through reliability-guaranteed sequence modeling, it is not effective in identifying the future returns of the generated data, which may result in a lack of high-return trajectories in the generated data.
Moreover, RT estimates the likelihood of future high returns for candidate actions and uses these estimations as generation conditions to generate high-return trajectories.
% Then, RT introduces a reliability estimation mechanism that derives a cumulative reliability value from the distribution error between the generated and real data.
% This cumulative reliability is a measure of the feasibility of generated trajectories in the real environment.
% Then, RT introduces a reliability estimation mechanism, which calculates the distribution error between the generated data and the real data and uses this error as a reliable value.
% Based on this reliable value, RT can gradually calculate the cumulative reliability of the generated data to determine the length of generated trajectories and eliminate unreliable trajectories.
% Additionally, RT can dynamically adjust the length of generated trajectories based on their cumulative reliability, ensuring a higher level of reliability.
% Then, RT evaluates the cumulative reliability along the trajectory based on the reliable values of the generated data to decide whether to continue generating operations.
% This generation mechanism effectively ensures that RT can generate reliable trajectories.
% Then, RT evaluates the cumulative reliability along the trajectory based on the reliable values of the generated data (i.e.,  reliability estimation)
% , deciding whether to continue generating operations, which effectively ensures the generation of reliable trajectories.
% Specifically, RT evaluates the cumulative reliability along the trajectory based on the reliable values of the generated data (i.e., reliability estimation), and decides whether to continue generating operations, which effectively ensures the generation of reliable trajectories xxxx.
% In order to generate high-return trajectories,
% Moreover, RT employs a binary classifier to assess the likelihood that a behavior leads to high rewards, and produces a probability distribution of future returns based on this reward. RT uses the probability distribution as the generation conditions to encourage the generation of high-return trajectories during the generation process.
% Then, RT samples from high probability xxx target returns to encourage the generation of high-quality trajectories in the generation process.
% xxxx To further enhance the generation of high-return trajectories, RT employs a backward generation mechanism to generate trajectories by randomly selecting a state from the trajectory as the starting point and modeling the states, actions, and cumulative rewards in a backward sequence, thus ensures that the generated trajectories include the actual goal state\footnote{Within the scope of this research, the goal state can be defined as any state along the trajectory that yields a maximal reward.} typically associated with high rewards.
% By utilizing these trajectories, the policy can learn the relationship between states and high rewards, as well as identify actions that effectively accomplish tasks, thereby optimizing long-term rewards.
% Extensive experiments demonstrate the superiority of RT, which not only generates high-quality trajectories but also significantly reduces unreliable state-action pairs.
% In addition, RT uses backward manner to ensure that generated trajectories contain goal states.
% To be specific, RT randomly selects a state from the trajectory as the starting point and models the states, actions, and cumulative rewards in a backward sequence.
% , and guides the model to generate high-return trajectories using expert returns.
% This backward manner ensures that the starting state (i.e., the goal state of the forward manner) is within the offline dataset, providing correct goal state information for subsequent policies.
% and achieving conservative generalization beyond the offline dataset~\cite{DBLP:WangLJZLZ21}.
% With these trajectories, subsequent RL methods can better learn the association between states and high returns, which helps optimize the long-term rewards of the policy.
% Finally, RT employs reliability trajectory truncation to adapt to different generated trajectory lengths and evaluate the reliability of generated trajectory.
% Finally, RT employs reliability trajectory truncation to assess the reliability of generated trajectories and incorporate it into the rewards of these trajectories, thereby penalizing state-action pairs with high distribution errors.
% Finally, RT employs reliable trajectory metric to adapt to different generated trajectory lengths.
% Finally, RT employs a reliable trajectory  estimation to ensure the reliability of the generated trajectories and address the limitations of fixed truncation.
% As shown in Fig~\ref{fig:beetle_smaple}, when a trajectory is generated, as shown by the green arrow, RT calculates the reliability value of the state-action pair at every time step. 
% Then, $u$ is used to compute pessimistic rewards~\cite{DBLP:KidambiRNJ20} for state-action pairs, which makes trajectories with high distributional errors have lower rewards, thus reducing their impact on policy learning.
% Then, the reliability value is employed to assess the cumulative reliability along the trajectory.
% If the cumulative reliability exceeds a predefined threshold ($\alpha$), RT automatically truncates the trajectory to ensure its reliability.
% This estimation not only prevents the generation of unreliable trajectories but also avoids the premature truncation of trajectories.
Our contributions are as follows:
\begin{itemize}

    % \item We explore the role of historical information in generating reliable data and leveraging the dynamic model to generate high-return trajectories for facilitating policy learning in offline RL.

    % \item We explore the role of leveraging historical information to generate reliable trajectories with high future returns for policy learning.

    \item We explore the role of leveraging historical information to generate reliable trajectories with high future returns to facilitate and stabilize policy learning in MORL.

    % \item We investigate the impact of using a dynamics model to generate high-return trajectories in offline RL, and employ backward sequence modeling and  high rewards as generation conditions for learning improvement.
    
    % \item We propose a reliability-driven model-based approach that improves the reliability of generated trajectories by leveraging cumulative reliability within sequence modeling, and generates high-return trajectories by maintaining a sustained high-reward condition.

    \item We propose RT that enhances trajectory reliability by incorporating cumulative reliability into sequence modeling and continuously selects actions with high rewards to generate high-return trajectories.

    \item Empirical results on various continuous control tasks demonstrate that our method  achieves superior learning  performance compared to existing baselines.
    
\end{itemize}


\section{Preliminaries}
\label{sec:preli}
\textbf{MDPs and offline RL}.
A Markov Decision Process (MDP) can be defined as $M=(S,A,R,P,\rho_0,\gamma)$, where $S$ is the state space, $A$ is the action space, $R(s, a)$ is the reward function, $P(s'|s,a)$ is the transition function, $\rho_0$ is the the initial state distribution, and $\gamma \in (0,1)$ is the discount factor.
A policy $\pi:S \times A \rightarrow [0,1]$ takes action $a$ at state $s$ with probability $\pi(a|s)$.
The goal of RL is to find the optimal policy $\pi^*$ that maximizes the expected return $\pi^* = \mathop{\text{argmax}}_\pi\limits \mathbb{E}_\pi[\sum_{t=0}^T \gamma^t R(s_t,a_t)]$.
In the offline RL setting, the agent only has access to a static dataset $ \mathcal{D}_{\text{env}} = \{ (s, a, r, s') \} $.
The agent's objective is to learn a policy  without interaction with the environment for any additional online exploration.
\\
\\
\textbf{Model-based offline  RL}.
Model-based offline RL methods use a dataset to learn the dynamics model $\hat{P}$, which is usually trained by maximum likelihood estimation: $\mathop{\min}_{\hat{P}}\limits \mathbb{E}_{(s,a,s') \sim D_{env}}[-\log \hat{P}(s'|s,a)]$.
% A model of the reward function $\hat{R}(s,a)$ can also be trained when the reward function is unknown.
Additionally, when the reward function is unknown, a model of the reward function $\hat{R}(s,a)$ can be trained.
Once the dynamics model is trained, samples generated by $\hat{P}$ are then placed into the model buffer $\mathcal{D}_{\text{model}}$, which is merged with the offline data $\mathcal{D}_{\text{env}} \cup \mathcal{D}_{\text{model}}$ to learn a policy.
In this work, we assume that the transition function of the environment is determined and propose RT to augment the offline data.
% The estimated MDP $\hat{M} = (S, A, \hat{R}, \hat{P}, \rho_0, \gamma)$ enables the application of any ~\cite{DBLP:ChenZX024,DBLP:DeyDD24} or planning~\cite{DBLP:000224,DBLP:Ashton24} method is applied to retrieve the best policy in the learned dynamics model $\hat{P}$.
% The estimated MDP $\hat{M} = (S, A, \hat{R}, \hat{P}, \rho_0, \gamma)$ has the same state and action spaces as the true MDP but utilizes the learned transition function and reward function. 
% Any RL~\cite{DBLP:ChenZX024,DBLP:DeyDD24} or planning~\cite{DBLP:000224,DBLP:Ashton24} method is applied to retrieve the best policy in the learned dynamics model $\hat{P}$ once the $\hat{P}$ has been established.
% In this study, we employ backward sequence modeling to generate trajectories and utilize a reliability trajectory truncation to determine the length of the generated trajectories. xxxx
% Then, the synthetic samples generated by $\hat{P}$ are placed into the model buffer $\mathcal{D}_{\text{model}}$ and the offline policy is updated using data sampled from  $\mathcal{D}_{\text{env}} \cup \mathcal{D}_{\text{model}}$.
% \\
% \\
% \textbf{Sequence Modeling}.




\section{Method}
%In this section, we describe the RT algorithm that utilizes the generated trajectories as auxiliary data to further augment the offline data.
% The overall training algorithm of RT is described in Appendix.
The overall process of RT is described in Algorithm~\ref{alg:RT_algo}.
RT first learns a dynamics model from the offline dataset and establishes a threshold for the reliability of the offline data (lines 1 to 11). 
Next, RT randomly selects a trajectory from the offline dataset and generates a reverse trajectory from any state along that trajectory.
This generated trajectory is then combined with the original data to create a new dataset, which is used to train the model-free algorithm (lines 12 to 21).
% RT consists of two primary stages: generating trajectories using learned sequence models, and augmenting the original offline dataset with these generated trajectories to facilitate policy learning. 


\begin{algorithm}[tb]
\caption{The RT algorithm}
\label{alg:RT_algo}
\textbf{Input}: Offline dataset $D_{env}$, generation number H, iteration N, model-free offline RL algorithm (e.g., BCQ)\\
\textbf{Parameter}: Randomly initialize RT parameters $\theta$, $\psi$  and $\phi$,\\
\textbf{Output}: offline RL algorithm
\begin{algorithmic}[1] %[1] enables line numbers
\STATE // Train RT
\FOR{i=0, $\cdots$, N}
\STATE Compute $\mathcal{L}_\tau$ using the dataset $D_{env}$
\STATE Updata RT network parameter $\theta$
\ENDFOR
\STATE // Train VAE to get the cumulative threshold $\alpha$
\FOR{for j=0, $\cdots$, N}
\STATE Compute $L(\psi,\phi)$ using offline dataset $D_{env}$
\STATE Update VAE network parameter $\psi$ and $\phi$
\ENDFOR
\STATE Use VAE to calculate the maximum distribution error of offline dataset to get a cumulative reliability threshold $\alpha$
\STATE //Generate trajectories
\STATE Initialize the buffer $D_{model}$
\FOR{i=0, $\cdots$, H}
\STATE Sample state $s_{t+1}$ from the $\tau$ in $D_{env}$
\STATE Generate backward trajectory \\ $\hat{\tau}=\{s_{t-k},a_{t-k},r_{t-k}\}_{k=0}^{t}$ using RT and reliability estimation.
\STATE Merge trajectories $\tau'=\hat{\tau}_{<T-t}+\tau_{\geq T-t}$
\STATE $D_{model} \leftarrow D_{model} \cup \tau'$
\ENDFOR
\STATE  Get composite dataset $D \leftarrow D_{env} \cup D_{model}$
\STATE Learn the model-free offline algorithm using $D$
\end{algorithmic}
\end{algorithm}


\subsection{Reliability-Guaranteed Sequence Generation}
% In this subsection, we present the details of the sequence modeling.
Inspired by previous sequence representation learning~\cite{DBLP:DT}, we treat each input trajectory $\tau$ as a sequence and add reward-to-go $R_t=\sum^T_{h=t} \gamma^{h-t}r_h$ as an auxiliary information at each time step $t$, which acts as future heuristics for further generating data. 
% Note that RT uses the backward trajectory sequence,
% Then, we model $\tau_{back}$ with the RT from the perspective of sequence modeling.
% Specifically, we use each trajectory $\tau_{back}=(s_T,a_T,r_T,R_T,\cdots,s_1,a_1,r_1,R_1)$ as an input sequence for RT and train it using the standard teacher-forcing~\cite{DBLP:HeWWST22}, which can be expressed as:
Specifically, each trajectory $\tau=(s_1,a_1,r_1,R_1,\cdots,s_T,a_T,r_T,R_T)$ is used as an input sequence for RT, which is trained using the standard teacher-forcing method~\cite{DBLP:HeWWST22}, as expressed by: 
\begin{equation}
\begin{split}
     \log P_\theta(\tau_t|\tau_{<t}) &= \log P_\theta(s_t|\tau_{<t})+\log P_\theta(a_{t}|s_t,\tau_{<t}) \\
    & +\log P_\theta(r_t|a_{t},s_t,\tau_{<t}) \\
    &+\log P_\theta(R_t|r_t,a_t,s_t,\tau_{<t}),
\end{split}
\end{equation}
where $\tau_{<t}$ represents the trajectory from the initial state to time step $t-1$, $\theta$ is the parameter of RT, and $P_\theta$ is the induced conditional probability.
The training objective is to maximize:
\begin{equation}
\label{eq:RT_loss}
    \mathcal{L}_\tau=\sum_{t=1}^{T} \log P_\theta(\tau_{t} | \tau_{<t}).
\end{equation}

However, during the sequence generation, the model generation error at moment $t$ is continuously accumulated by the subsequent generation process, which may cause the generated data to deviate from the distribution of the original data.
In order to solve this issue, we propose a  reliability estimation mechanism, which automatically determines the generated trajectory lengths based on the cumulative reliability along the trajectory and incorporates these reliability values into the pessimistic MDP~\cite{DBLP:BlanchetLZZ23} to provide performance bounds for the learned policy.
% that assesses the cumulative reliability of trajectories for automatically determining the length of trajectory generation and for imposing penalties on trajectories with high distribution errors, thereby enhancing their reliability.
% Next, we introduce some theoretical perspectives on our proposed reliability trajectory truncation.
We first define the cumulative reliability of trajectories and the truncation metric:
% xxxx Then, we define the cumulative reliability of trajectories and introduce a truncation metric xxxx:
\begin{defn}[\textbf{Cumulative Reliability}]
    Given a trajectory $\tau$, the cumulative reliability along the trajectory at step $t$ is defined as:
    \begin{equation}
    \begin{split}
    &\Gamma(s_t,\tau_{<t})=\mathbb{D}(P(\cdot|s_i,\tau_{<i}),\hat{P}(\cdot|s_i,\tau_{<i})) \\
    &=\sum^t_{i=1} \frac{\exp(e_i)}{\sum_{j=1}^n \exp(e_j)} \cdot D_{IST}(P(\cdot|s_i,a_i),\hat{P}(\cdot|s_i,a_i)),
    \end{split}
    \end{equation}
\end{defn}

\noindent \textit{where $\hat{P}$ is the estimated transition probability of the environment after training, $P$ is the true dynamics, $e$ is an attention value, and $D_{IST}$ represents the total variation distance~\cite{DBLP:HoY10a} between two distributions $\hat{P}$ and $P$.}
%\alpha_i = \frac{\exp(e_i)}{\sum_{j=1}^n \exp(e_j)}

\begin{defn}[\textbf{Truncation Metric}]
   Given a cumulative reliability $\Gamma$, the truncation metric at $t$ as:
    \begin{equation}
        U_t=\left \{ \begin{array}{lcl} 0 \quad (i.e., \ reliable) \ if \ \Gamma(s_t,\tau_{<t}) \leq \alpha \\ 1 \quad (i.e., unreliable) \ otherwise \end{array} \right. ,
    \end{equation}
\end{defn}
\noindent where \textit{$\alpha$ represents the cumulative threshold for generating reliable trajectories}.

$\Gamma$ provides a metric of the cumulative reliability of the trajectories generated by the model, while $U$ defines the truncation point of the generated trajectory. 
% Although the $U$ can explicitly provide a truncation point for generating trajectories, it cannot mitigate the impact of errors in the trajectory on subsequent policy learning.
% However, $\Gamma$ and $U$ are insufficient for analyzing the performance of policies learned from generated trajectories. 
% Additionally, they do not explicitly account for the generation errors in the generated trajectories, which can lead to the policy overestimating trajectories that have significant errors.
% However, $\Gamma$ and $U$ cannot directly quantify the impact of generation errors on subsequent policy learning.
To directly quantify the impact of generation errors on policy learning, inspired by previous works~\cite{DBLP:KidambiRNJ20,DBLP:BlanchetLZZ23,DBLP:ZhangLMY0WL23} that used pessimistic reward-based training policies,
we integrate the reliability values into the pessimistic MDP framework.
Specifically, the $\alpha$-pessimistic MDP is defined by the tuple $ \hat{M}_p = \{S, A, \hat{R}, \hat{P}_p, \rho_p, \gamma\} $, where $S$,$A$, $\rho_p$ and  $\gamma$ retain the same definitions as in the original MDP (see Sec.~\ref{sec:preli}).
The transition function and reward function are dynamically updated and evaluated using the truncation metric, defined as follows:

\begin{equation}
    \hat{P}(\cdot|s',\tau_{<t})=\left\{
    \begin{array}{lcl} 
         {0 \quad if\ U_t=1} \\
         {\hat{P}(\cdot|s_t,\tau_{<t}) \quad otherwise},
    \end{array} \right.
\end{equation}

\begin{equation}
\label{eq:a_mdp}
    \hat{R}(s_t,a_t)=\left\{
    \begin{array}{lcl}
         0 \quad if \ U_t=1 \\
         r(s_t,a_t)- \beta \frac{\Gamma(s_t,\tau_{<t})}{\alpha}) \quad otherwise, 
    \end{array}\right.
\end{equation}
\noindent where $\beta$ is the reliability penalty hyperparameters for each state-action pair $(s, a)$.
% By introducing the $\alpha$-pessimistic MDP, we incorporate conservatism into the reward function and the generated state-action sequence.


By introducing the $\alpha$-pessimistic MDP, we incorporate trajectory generation errors into the reward function, ensuring that trajectories with larger errors receive smaller reward values. In this way, the policy learns a lower Q-value when encountering such trajectories, thereby becoming more conservative and robust
~\cite{DBLP:KidambiRNJ20,DBLP:KumarZTL20}.
% If the truncation metric $U_t = 1$, that is, when the accumulated reliability $\Gamma$ at the step $t$  exceeds $\alpha$ , then the trajectory imagination will be terminated.
Moreover, If the truncation metric $U_t = 1$, indicating that the cumulative reliability $\Gamma$ at step $t$  exceeds the threshold $\alpha$, the trajectory generation process will be terminated.

%Next, we analyze the effectiveness of the above mechanism.
% Assuming the reward function is bounded, that is, $|R(s, a)| \leq r_{\text{max}}, \quad \forall (s, a)$, we can derive the performance bound between the $\alpha$-pessimistic MDP and the true MDP policy.
%based on the work~\cite{DBLP:KidambiRNJ20}.
In order to analyze the effect of integrating truncation metric into pessimistic MDP, we assume that $\tilde{r}$ is the maximum reward value in $\alpha$-pessimistic MDP, and then derive the performance bound between the $\alpha$-pessimistic MDP policy and the real MDP policy.
% Let $\tilde{r}$ be the maximum reward value in the $\alpha$-pessimistic MDP. Then we can derive the performance bound between the $\alpha$-pessimistic MDP and the true MDP policy.
% \begin{lemma}
% \label{le:pess}
%     Let $\tilde{r}$ be the maximum reward value in the $\alpha$-pessimistic MDP. The value of any policy $\pi$ in the original MDP $M$  and its $\alpha$-pessimistic counterpart $\hat{M}_p$ adheres to the following relationship:
%     \begin{equation}
%         \begin{split}
%             J_{\hat{\rho_0}}(\pi,\hat{M}_p) & \geq J_{\rho_0}(\pi,M)- \frac{2\tilde{r}}{1- \gamma } \cdot D_{TV}(\rho_0,\hat{\rho}_0)\\
%             &-\frac{2\gamma \tilde{r}}{(1-\gamma)^2}\cdot \alpha -\frac{\tilde{r}}{1-\gamma},
%         \end{split}
%     \end{equation}
%     \begin{equation}
%         \begin{split}
%             J_{\hat{\rho_0}}(\pi,\hat{M}_p) &\leq J_{\rho_0}(\pi,M)+ \frac{2\tilde{r}}{1- \gamma } \cdot D_{TV}(\rho_0,\hat{\rho}_0)\\
%             &+\frac{2\gamma \tilde{r}}{(1-\gamma)^2}\cdot \alpha.
%         \end{split}
%     \end{equation}
% \end{lemma}

% Refer to the Appendix for the proof of Theorem~\ref{th:pess}.
% Lemma~\ref{le:pess} indicates that the difference in the returns of any policy $\pi$ between the true MDP and the $\alpha$-pessimistic MDP primarily depends on the total variation distance $ D_{TV}$, the cumulative reliability threshold $\alpha$, and the upper bound of the pessimistic reward function.
% Based on this result, we can derive an immediate corollary

\begin{thm}
    Given $\pi^*$ as the optimal policy in the real MDP ($M$) and $\epsilon_\pi$ as the performance gap between the optimal policy and any policy in the $\alpha$-pessimistic MDP ($\hat{M}_p$), the performance lower bound of the policy $\pi$ learned  by Algorithm 1 is:
    \begin{equation}
        \begin{split}
            J_{\rho_0}(\pi,M)&\geq J_{\rho_0}(\pi^*,M)-\epsilon_\pi-\frac{4\tilde{r}}{1-\gamma}\cdot D_{IST}(\rho_0,\hat{\rho}_0)\\
            &-\frac{4\gamma \tilde{r}}{(1-\gamma)^2}\cdot \alpha-\frac{\tilde{r}}{1-\gamma},
        \end{split}
    \end{equation}
    \label{th:lower}
\end{thm}

Theorem~\ref{th:lower} demonstrates that incorporating cumulative reliable values into an \(\alpha\)-pessimistic MDP can provide a strict performance lower bound for a policy in the real MDP.
% Simply put, if the suboptimality of the policy learned in the pessimistic MDP is small (i.e., $\epsilon_\pi$ is small), then the suboptimality of the policy in the real MDP will also be small.
Simply put, if a policy learned is close to the optimal policy (i.e., $\epsilon_\pi$ is small) in the pessimistic MDP, then the policy is also close to the true optimal policy in the real MDP (see Appendix for detailed analysis).
% By utilizing the $\alpha$-pessimistic MDP, we incorporate the trajectory generation error into the reward function, which encourages subsequent policies to prioritize learning from trajectories with smaller errors.

However, the cumulative reliability and threshold $\alpha$ cannot be directly derived through the calculation of $D_{IST}$ because $P(\cdot|s,\tau_{<t})$ are typically unknown. 
% Additionally, the truncation threshold remains indeterminate xxxx.
To solve this issue, we employ a Variational Autoencoder (VAE)~\cite{DBLP:Li024,DBLP:ZhaoZFHSC24} to calculate the distribution error of $(s,a,s')$ as a replacement for the total variation distance  $D_{IST}(P(\cdot | s, a), \hat{P}(\cdot | s, a))$.
Specifically, we use a neural network to learn an encoder and a decoder. 
The encoder maps $x = (s, a, s')$ to a latent vector $z$, while the decoder reconstructs $x$ from $z$.
The conditional probability of the encoder is $ q(z | x)$ , and the constrained probability distribution of the decoder is $p(x | z)$. Then, the loss function of the VAE is expressed as:
\begin{equation}
    L(\psi,\phi)=-\mathbb{E}_{z\sim q(z|x)}[\log(p(x|z))]+D_{\text{KL}}(q(z|x)||p(z)),
\end{equation}
where $\psi$ is the encoder parameter, $\phi$ is the decoder parameter and $D_{\text{KL}}$ is KL divergence.
By learning the VAE, we identify the maximum reconstruction error in the offline data and set it as the threshold $\alpha$.
% Next, the states and actions obtained by the dynamics model use the VAE to calculate the reconstruction error as $D_{TV}$ to compute the pessimistic reward.
Next, the generated states and actions use the VAE to calculate the $D_{IST}$, which is then used as a reliable value for the generated data to compute the cumulative reliability of the trajectory.

\subsection{High-return Trajectory Generation  }

% This generative method, which learns the data distribution in a supervised manner, does not ensure the generation of high-quality data.
While the above method can generate more reliable trajectories, it does not ensure the generation of high-return trajectories.
% The above generation way can ensure that the generated trajectories align with the offline data distribution, but it cannot guarantee the generation of high-quality data.
% RT aims to control the generation of trajectories to consistently generate high return data (i.e., high-quality trajectories).
To address this issue,  we use the reward at time $t$ as a condition to guide the model to generate the action at $t+1$, which is similar to the discriminator-guided generation method used in language models~\cite{DBLP:0011LH024}.
Specifically, RT applies a binary classifier $\mathbb{P}(H_t |\cdots)$ to identify whether an action brings a high reward before taking an action at time $t$, and applies the Bayes' rule to approximate the high reward distribution $\mathbb{P}(R_t, \ldots | H_t) \propto \mathbb{P}(H_t | R_t, \ldots) \mathbb{P}(R_t, \ldots)$, where $H_t$ denotes the high reward at time $t$.
This probabilistic modeling~\cite{DBLP:LeeNYLFGFXJMM22} forms a simple autoregressive process in which $R_t$ is first generated based on a high and reasonable log-probability, and actions are then generated according to $P_\theta(a_t \mid R_t, \ldots)$, which can be considered a variant of the class-conditional model~\cite{DBLP:abs-1909-05858} that automatically takes actions with high rewards at each time step to generate high-return trajectories.

To further enhance the generation of high-return trajectories, RT employs a backward generation mechanism to generate trajectories by randomly selecting a state from the trajectory as the starting point and modeling the states, actions, and cumulative rewards in a backward sequence, thus ensures that the generated trajectories include the actual goal state\footnote{Within the scope of this research, the goal state can be defined as any state along the trajectory that yields a maximal reward.} typically associated with high rewards.
% By utilizing these trajectories, the policy can learn the relationship between states and high rewards, as well as identify actions that effectively accomplish tasks, thereby optimizing long-term rewards.
% xxxxx In addition, we employ backward generation, which starts from the goal state (e.g., a high reward state) and generates possible state-action sequences in reverse.
Specifically, each trajectory $\tau_{back}=(s_T,a_T,r_T,R_T,\cdots,s_1,a_1,r_1,R_1)$ is used as an input sequence for RT, which is expressed as follows: 
\begin{equation}
\begin{split}
     \log P_\theta(\tau_t|\tau_{>t}) &= \log P_\theta(s_t|\tau_{>t})+\log P_\theta(a_{t}|s_t,\tau_{>t}) \\
    & +\log P_\theta(r_t|a_{t},s_t,\tau_{>t}) \\
    &+\log P_\theta(R_t|r_t,a_t,s_t,\tau_{>t}),
\end{split}
\end{equation}
where $\tau$ denotes $\tau_{back}$, and $\tau_{>t}$ represents the trajectory from time step $t$ to the terminal state.
The training objective is to maximize:
\begin{equation}
\label{eq:RT_loss}
    \mathcal{L}_\tau=\sum_{t=1}^{T} \log P_\theta(\tau_{T-t+1} | \tau_{> T-t+1}).
\end{equation}

After the training process, the learned RT generates trajectories to augment the offline dataset $D$. For the original trajectories $\tau \in D $ in the original dataset, RT begins generating trajectories backward from time step $T'$:
\begin{equation}
    \begin{split}
        \hat{\tau} &=(\cdots,\hat{s}_{T-T'-1},\hat{a}_{T-T'-1},\hat{r}_{T-T'-1},\hat{R}_{T-T'-1}         \\&s_{T'},a_{T'},r_{T'},R_{T'},\cdots) \sim P_\theta(\tau_{<T-T'}|\tau_{\geq T-T'}),
    \end{split}
\end{equation}
where $\tau_{\geq T-T'}$ denotes the original trajectory truncated at time step $T-T'$.
Then, the generated sequence is concatenated with the truncated original trajectory, resulting in the formation of a new trajectory $\tau'=\hat{\tau}_{<T-T'}+\tau_{\geq T-T'}$, where $+$ is the concatenation operation. 
By adopting this approach, we are able to augment a solitary high-return trajectory into the trajectories, consequently enriching the offline dataset.

\section{Experiments}
% Our experiments aim to answer the following three questions xxx:
% 1) Does RT generate high-quality data compared to existing model-based augmentation methods?
%1) Does RT outperform existing model-based augmentation methods in terms of performance improvement?2) Can RT effectively control errors using backward sequence modeling and reliability truncation?3) Does RT generate trajectories that achieve higher rewards?4) How does RT compare to model-free methods and previous state-of-the-art model-based methods?5) How do the various components of RT influence its overall performance?
In this section, we first compare RT with existing mainstream offline RL methods across various domains in the D4RL benchmark~\cite{DBLP:abs-2004-07219}, including Maze2D, MuJoCo and  AntMaze, and then analyze the impact of different mechanisms or components of RT on its performance. Refer to the Appendix for more details about the environments.
% Maze2D is a simple 2D maze environment used to test reinforcement learning algorithms' navigation and path planning capabilities.
% MuJoCo is a high-performance physics simulation engine widely used in reinforcement learning for simulating complex mechanical systems and robotics tasks.
% AntMaze is a more complex maze environment that combines the MuJoCo-based quadruped robot (Ant).

% \begin{table*}[]
% \centering
% \begin{tabular}{llccccll}
% \hline
%               & Environment                                                                            & \multicolumn{1}{l}{MOPO}                                   & \multicolumn{1}{l}{MOReL}                                  & \multicolumn{1}{l}{MOPP}                                   & \multicolumn{1}{l}{RT+BCQ}                                             & RT+CQL                                                                  & RT+IQL                                                                 \\ \hline
% medium-replay & \begin{tabular}[c]{@{}l@{}}Hopper\\ Walker\\ Halfcheetah\end{tabular}                  & \begin{tabular}[c]{@{}c@{}}68.5\\ 40.3\\ 53.2\end{tabular} & \begin{tabular}[c]{@{}c@{}}93.8\\ 48.7\\ 39.8\end{tabular} & \begin{tabular}[c]{@{}c@{}}35.2\\ 23.6\\ 43.8\end{tabular} & \begin{tabular}[c]{@{}c@{}}99.3+7.2\\ 70.3+5.8\\ 47+3.5\end{tabular}   & \begin{tabular}[c]{@{}l@{}}106.3+3.5\\ 86.5+8.7\\ 75.6+7.8\end{tabular} & \begin{tabular}[c]{@{}l@{}}108.3+6.7\\ 94+5.7\\ 100+5.6\end{tabular}   \\ \hline
% sparse        & \begin{tabular}[c]{@{}l@{}}Maze2D-umaze\\ Maze2D-medium\\ Maze2D-large\end{tabular}    & \begin{tabular}[c]{@{}c@{}}1.2\\ 0.0\\ 0.9\end{tabular}    & \begin{tabular}[c]{@{}c@{}}0.0\\ 1.3\\ 0.0\end{tabular}    & \begin{tabular}[c]{@{}c@{}}2.1\\ 0.0\\ 0.6\end{tabular}    & \begin{tabular}[c]{@{}c@{}}54.3+5.6\\ 55.3+4.8\\ 40.8+5.2\end{tabular} & \begin{tabular}[c]{@{}l@{}}45.6+6.8\\ 50.3+4.8\\ 44.6+7.4\end{tabular}  & \begin{tabular}[c]{@{}l@{}}44.3+7.2\\ 48.3+6.7\\ 39.8+6.9\end{tabular} \\ \hline
% diverse       & \begin{tabular}[c]{@{}l@{}}Antmaze-umaze\\ Antmaze-medium\\ Antmaze-large\end{tabular} & \begin{tabular}[c]{@{}c@{}}0.0\\ 0.0\\ 0.0\end{tabular}    & \begin{tabular}[c]{@{}c@{}}0.0\\ 0.6\\ 0.0\end{tabular}    & \begin{tabular}[c]{@{}c@{}}0.0\\ 0.0\\ 0.0\end{tabular}    & \begin{tabular}[c]{@{}c@{}}71.3+4.5\\ 56.4+5.4\\ 46.8+3.9\end{tabular} & \begin{tabular}[c]{@{}l@{}}90.6+5.7\\ 58.4+6.5\\ 53.2+4.7\end{tabular}  & \begin{tabular}[c]{@{}l@{}}86.5+7.8\\ 80.2+5.9\\ 60.0+3.2\end{tabular} \\ \hline
% \end{tabular}
% \caption{Performance of RT and best performance of prior methods on the MuJoCo, Maze2D and Antmaze domains, on the normalized return metric proposed by D4RL benchmark}
% \label{tab:comp_resut}
% \end{table*}

% , and use the inverse temperature $\kappa$ to approximate the distribution of expert-level rewards (i.e., higher returns)
% \begin{equation}
%     P(R^t|expert^t, \cdots) \propto exp(kR^t)P(R^t).
% \end{equation}
% $\quad$

\subsection{Comparison Methods}
Since our goal is to augment offline datasets to improve the performance of offline RL algorithms, we compare RT with recent augmentation algorithms, including \textbf{ROMI}~\cite{DBLP:WangLJZLZ21}, \textbf{CABI}~\cite{DBLP:LyuLL22}),
\textbf{TATU}~\cite{DBLP:ZhangLMY0WL23} and \textbf{DStitch}~\cite{DBLP:LiSZL024}.
In addition, in order to intuitively understand the performance difference between RT and notable offline RL algorithms, RT is compared to the following notable methods, including offline model-free algorithms (i.e., \textbf{BCQ}~\cite{DBLP:FujimotoMP19}, \textbf{CQL}~\cite{DBLP:KumarZTL20}, \textbf{IQL}~\cite{DBLP:KostrikovNL22}, and \textbf{DT}~\cite{DBLP:DT}), as well as offline model-based algorithms (i.e., \textbf{MOPO}~\cite{DBLP:YuTYEZLFM20}, \textbf{MOReL}~\cite{DBLP:KidambiRNJ20} and \textbf{MOPP}~\cite{DBLP:ZhanZX22}.
More description of the above methods is provided in the Appendix.

\begin{figure*}[t]
    \centering
    \subfigure[BCQ]{
        \includegraphics[width=0.31\textwidth]{figure/BCQ_hopper_historgram.pdf}
    }
    \subfigure[CQL]{
        \includegraphics[width=0.31\textwidth]{figure/CQL_hopper_historgram.pdf}
    }
    \subfigure[IQL]{
        \includegraphics[width=0.31\textwidth]{figure/IQL_hopper_historgram.pdf}
    }
    % \subfigure[AntMaze]{
    %     \includegraphics[width=0.22\textwidth]{figure/BCQ_hopper_historgram.pdf}
    % }
    \caption{Performance of RT, ROMI, CABI, TATU and DStitch combined with different model-free algorithms (averaged over 10 random seeds).
    %(a) is the Hopper-medium task in MuJoco, (b) is the Walker-medium task in MuJoco, (c) denotes the sparse Maze2D-medium task in Maze2D and (d) represents the diverse Antmaze-medium task in AntMaze.} 
    %BCQ's cumulative rewards for the four tasks are 56.5, 53.8, 18.2 and 5.2 respectively. CQL's cumulative rewards for the four  tasks are 45.2, 56.9, 14.6 and 55.4 respectively. IQL's cumulative rewards for the four tasks are 67.1, 78.5, 10.6 and  respectively.
    }
    
    \label{fig:model-free_boost}
\end{figure*}

% \begin{table*}[]
% \centering
% \begin{tabular}{lcccccc}
% \hline
% Environment                   & \multicolumn{1}{l}{Model-free methods}                  & ROMI                                                       & CABI                                                       & TUAU                                                       & DStitich                                                   & RT                                                                     \\ \hline
% Hopper-medium          & \begin{tabular}[c]{@{}c@{}}BCQ\\ CQL\\ IQL\end{tabular} & \begin{tabular}[c]{@{}c@{}}55.2\\ 72.3\\ 78.5\end{tabular} & \begin{tabular}[c]{@{}c@{}}60.4\\ 70.9\\ 80.3\end{tabular} & \begin{tabular}[c]{@{}c@{}}62.3\\ 68.7\\ 81.3\end{tabular} & \begin{tabular}[c]{@{}c@{}}73.6\\ 79.3\\ \textbf{91.4}\end{tabular} & \begin{tabular}[c]{@{}c@{}}\textbf{75.6±7.3}\\ \textbf{83.4±6.8}\\ 90.2±3.5\end{tabular} \\ \hline
% Walker-medium          & \begin{tabular}[c]{@{}c@{}}BCQ\\ CQL\\ IQL\end{tabular} & \begin{tabular}[c]{@{}c@{}}72.2\\ 82.3\\ 88.5\end{tabular} & \begin{tabular}[c]{@{}c@{}}68.6\\ 78.9\\ 83.3\end{tabular} & \begin{tabular}[c]{@{}c@{}}78.9\\ 65.7\\ 89.4\end{tabular} & \begin{tabular}[c]{@{}c@{}}82.9\\ \textbf{91.2}\\ 92.4\end{tabular} & \begin{tabular}[c]{@{}c@{}}\textbf{83.2±2.5}\\ 90.4±7.2\\ \textbf{95.2±6.8}\end{tabular} \\ \hline
% sparse Maze2D-medium   & \begin{tabular}[c]{@{}c@{}}BCQ\\ CQL\\ IQL\end{tabular} & \begin{tabular}[c]{@{}c@{}}53.2\\ 22.3\\ 50.4\end{tabular} & \begin{tabular}[c]{@{}c@{}}43.2\\ 25.9\\ 20.3\end{tabular} & \begin{tabular}[c]{@{}c@{}}44.3\\ 21.6\\ 25.7\end{tabular} & \begin{tabular}[c]{@{}c@{}}20.3\\ 21.6\\ 19.4\end{tabular} & \begin{tabular}[c]{@{}c@{}}\textbf{55.3±4.8}\\ \textbf{50.3±4.8}\\ \textbf{48.3±6.7}\end{tabular} \\ \hline
% diverse Antmaze-medium & \begin{tabular}[c]{@{}c@{}}BCQ\\ CQL\\ IQL\end{tabular} & \begin{tabular}[c]{@{}c@{}}30.2\\ 25.3\\ 60.5\end{tabular} & \begin{tabular}[c]{@{}c@{}}43.1\\ 56.9\\ 71.3\end{tabular} & \begin{tabular}[c]{@{}c@{}}35.7\\ 41.2\\ 70.6\end{tabular} & \begin{tabular}[c]{@{}c@{}}\textbf{59.3}\\ 48.2\\ 73.7\end{tabular} & \begin{tabular}[c]{@{}c@{}}56.4±5.4\\ \textbf{58.4±6.5}\\ \textbf{80.2±5.9}\end{tabular} \\ \hline
% \end{tabular}
% \caption{Performance of RT, ROMI, CABI, TATU and DStitch combined with different model-free algorithms}
% \label{tab:agu_data}
% \end{table*}

\subsection{Results}
\noindent
\textbf{Comparison to augment methods}.
%We investigated the performance improvement of RT for model-free methods and compared it to the ROMI and CABI approaches.
% As shown in Fig.~\ref{fig:model-free_boost}, ROMI, CABI, and RT perform data augmentation on the BCQ, CQL, and IQL methods for tasks such as Hopper-medium, Walker-medium, sparse Maze2D-medium, and diverse Antmaze-medium.
ROMI, CABI, TATU, DStitch and RT are combined with model-free methods (e.g., BCQ, CQL and IQL) and evaluated on Hopper-medium, Walker-medium, sparse Maze2D-medium and diverse Antmaze-medium.
The experimental results shown in Fig.~\ref{fig:model-free_boost} demonstrate that RT consistently outperforms ROMI  CABI and TATU. 
% Besides, both ROMI and CABI show worse results than IQL alone due to the lack of ability to generate high-quality trajectories in the antmze media environment.
% Moreover, both ROMI and CABI show worse results than IQL alone due to the lack of ability to generate high-quality trajectories in the Antmze-media environment.
Moreover, both ROMI, CABI and TATU perform worse than IQL alone in the Antmaze-medium environment because their inability to generate high-return trajectories introduces a large amount of unrewarded data, leading to inaccurate policy constraints and then a suboptimal policy.
% While DStitch is capable of stitching together high and low reward trajectories from the data, it falls short in generating high-return trajectories in sparse reward environments with abundant suboptimal data, as exemplified by the Maze2D environment.
% Although DStitch can stitch high-reward trajectories and low-reward trajectories in the data together, it is unable to find suitable high-reward and low-reward trajectories for stitching in environments with a large amount of suboptimal data, as exemplified by the Maze2D environment.
Although DStitch can merge high-reward and low-reward trajectories, the presence of a high proportion of suboptimal data poses a significant challenge to its ability to find suitable trajectories for merging, as in the Maze2D environment.
In contrast, RT employs sequential modeling to generate high-return trajectories starting from the goal state and utilizes reliability estimation to automatically determine the length of generated trajectories, resulting in more robust performance across different environments.


% The results indicate that RT can achieve higher learning performance against ROMI and CABI, especially in challenging AntMaze where combining ROMI and CABI with IQL results in worse performance compared to using IQL. 

% In the sparse Maze2D-medium environment, RT continues to show superior performance, whereas both ROMI and CABI experience performance degradation. This degradation is particularly evident when ROMI and CABI are combined with CQL, resulting in worse outcomes than using CQL alone. The underlying reason is that ROMI and CABI generate non-reward trajectories in sparse environments, which not only fail to contribute to effective learning but also interfere with CQL’s policy learning from the offline dataset.
% Specifically, in the Hopper-medium, ROMI, CABI, and RT have overall achieved improvements over the BCQ, CQL and IQL algorithms, with ROMI showing the least improvement and RT showing the most significant enhancement.
%Specifically, in the Hopper-medium and Walker-medium, ROMI, CABI and RT show improvements compared to the BCQ, CQL and IQL, with RT exhibiting the most significant enhancement.
% xxx In the sparse Maze2D-medium, RT still shows the greatest improvement. 
% However, ROMI and CABI exhibit a degradation phenomenon (i.e., The performance of ROMI and CABI combined with CQL is lower than when only using CQL learning).
% xxxx However, combining ROMI and CABI with CQL results in worse performance than using CQL alone.
% However, ROMI and CABI exhibit a degradation in performance, where combining ROMI and CABI with CQL results in worse performance compared to using CQL.
% This result is because ROMI and CABI generate unreasonable state-action pairs (i.e., false state-action pairs) in a sparse environment, which interferes with CQL learning behavioral policies from offline dataset, thus leading to CQL performance degradation.
% This result is because ROMI and CABI produce no-reward trajectories in a sparse environment, and these random trajectories cannot be used for effective learning of CQL, and even interfere with CQL learning.
% xxx This outcome is due to ROMI and CABI generating non-reward trajectories in a sparse environment, which are ineffective for CQL improvement and may even hinder CQL learning.
% Unlike them, RT utilizes sequential modeling to generate trajectories from goal states, ensuring that the generated trajectories are effective for policy improvement.
% Unlike them, RT employs sequential modeling to generate trajectories starting from the goal state, ensuring all generated trajectories be included in the goal state. 
% Additionally, RT  uses reliability trajectory truncation to automatically determine the length of generated trajectories rather than using a fixed value, resulting in more robust performance across different environments.
%The above analysis still applies to the diverse Antmaze-medium.
% RT generates high-quality trajectories by sampling from the expert reward distribution, thereby enhancing policy learning.




\begin{figure}[t]
    \centering
    \subfigure[Walker]{
        \includegraphics[width=0.22\textwidth]{figure/medium_walker_box.pdf}
    }
    \subfigure[Halfcheetah]{
        \includegraphics[width=0.22\textwidth]{figure/medium_halfcheetah_box.pdf}
    }
    \subfigure[AntMaze]{
        \includegraphics[width=0.22\textwidth]{figure/antmaze_umaze_box.pdf}
    }
    \subfigure[Maze2D]{
        \includegraphics[width=0.22\textwidth]{figure/Maze2D-umaze_box.pdf}
    }
    \caption{Results of BC learned using trajectories generated by RT, ROMI, CABI, TATU and DStitch (averaged over 10 random seeds). 
    % The cumulative returns of BC in the above environment are 30.2, 3.2, 8.5 and -2.8 respectively.
    }
    \label{fig:BC_compar}
\end{figure}

% In order to evaluate the quality of the trajectories generated by RT, We employ Behavioral Cloning (BC)~\cite{DBLP:TorabiWS18} to learn a policy from the generated data and assess its performance , because The performance of BC is highly dependent on the quality and quantity of the expert data. 
Next, to evaluate the return of the trajectories generated by RT, we employ Behavioral Cloning (BC)~\cite{DBLP:TorabiWS18} to learn a policy from the generated data, as the effectiveness of BC is highly dependent on the quality and quantity of the high-return data.
We train ROMI, CABI, TATU, DStitch and RT on the Walker-medium, Halfcheetah-medium, Antmaze-umaze, and Maze2D-umaze environments, and then select 50 trajectories from these environments to generate new trajectories by sampling each selected trajectory 10 times for learning BC.
The results in Fig.~\ref{fig:BC_compar} show that RT shows strong learning performance, while ROMI, CABI, TATU and DStitch exhibit large performance fluctuations in different environments, which can be attributed to the fact that ROMI, CABI, TATU and DStitch prioritize generating trajectories that adhere to the original data distribution without considering the returns of the generated trajectories, preventing BC from learning an effective policy from the generated trajectories.
% This enhancement advantage is even more evident in AntMaze.
%Specifically, ROMI, CABI and RT effectively improve the performance of the BC algorithm in the Walker and Halfcheetah environments.
% xxxx However, in the sparse Maze2D-umaze environment, ROMI and CABI focus on generating trajectories that conform to the original data distribution without considering the returns of the generated trajectories, making it challenging to produce high-quality data.
% In the sparse Maze2D-umaze, ROMI and CABI focus on generating trajectories that conform to the distribution of the original data, without considering the generation of high-quality data.(i.e., in environments with sparse rewards, ROMI and CABI struggle to generate trajectories that include the goal state.).
% xxxx Therefore, they have little effect on BC learning.
In contrast, RT takes into account both the reliability and high-return of the data when generating trajectories, which allows to achieve better learning performance across different environments.


Furthermore, we employe T-SNE~\cite{van2008visualizing,DBLP:ChenWZDLL24} visualization to compare the distribution of states generated by RT with the original states, and evaluate the rewards of each state using a value function.
% the distribution of samples generated by RT is essentially consistent with that of the original data.
As shown in Fig.~\ref{fig:state_distribution}, the distribution of samples generated by RT is essentially consistent with that of the original data.
However, when looking at the non-overlapping regions of the generated and original states, we find that the generated data has much higher values than the original state. 
Those out-of-distribution and high-value regions reflect the ability of RT to generate high-return trajectories (refer to the Appendix for results of other environments).
% , and note that the high-value gains from single-step generalization can also be accumulated and propagated through time-steps to produce better facilitated policy learning.
\\

\begin{table*}[]
\centering
\begin{tabular}{llcccccccc}
\hline
              & Environment                                                                            & \multicolumn{1}{l}{BCQ}                                    & \multicolumn{1}{l}{CQL}                                    & \multicolumn{1}{l}{IQL}                                    & \multicolumn{1}{l}{DT}                                     & \multicolumn{1}{l}{MOPO}                                   & \multicolumn{1}{l}{MOReL}                                  & \multicolumn{1}{l}{MOPP}                                   & \multicolumn{1}{l}{RT+BCQ}                                             \\ \hline
medium-replay & \begin{tabular}[c]{@{}l@{}}Hopper\\ Walker\\ Halfcheetah\end{tabular}                  & \begin{tabular}[c]{@{}c@{}}33.3\\ 16.8\\ 39.2\end{tabular} & \begin{tabular}[c]{@{}c@{}}30.6\\ 15.8\\ 40.7\end{tabular} & \begin{tabular}[c]{@{}c@{}}96.8\\ \textbf{74.9}\\ 45.1\end{tabular} & \begin{tabular}[c]{@{}c@{}}83.6\\ 67.2\\ 40.3\end{tabular} & \begin{tabular}[c]{@{}c@{}}68.5\\ 40.3\\ 53.2\end{tabular} & \begin{tabular}[c]{@{}c@{}}93.8\\ 48.7\\ 39.8\end{tabular} & \begin{tabular}[c]{@{}c@{}}35.2\\ 23.6\\ 43.8\end{tabular} & \begin{tabular}[c]{@{}c@{}}\textbf{99.3$\pm$7.2}\\ 70.3$\pm$5.8\\ \textbf{47$\pm$3.5}\end{tabular}   \\ \hline
sparse        & \begin{tabular}[c]{@{}l@{}}Maze2D-umaze\\ Maze2D-medium\\ Maze2D-large\end{tabular}    & \begin{tabular}[c]{@{}c@{}}49.3\\ 18.2\\ 32.6\end{tabular} & \begin{tabular}[c]{@{}c@{}}19.1\\ 14.6\\ 17.2\end{tabular} & \begin{tabular}[c]{@{}c@{}}20.1\\ 10.6\\ 19.3\end{tabular} & \begin{tabular}[c]{@{}c@{}}52.6\\ 13.2\\ 3.4\end{tabular}  & \begin{tabular}[c]{@{}c@{}}1.2\\ 0.0\\ 0.9\end{tabular}    & \begin{tabular}[c]{@{}c@{}}0.0\\ 1.3\\ 0.0\end{tabular}    & \begin{tabular}[c]{@{}c@{}}2.1\\ 0.0\\ 0.6\end{tabular}    & \begin{tabular}[c]{@{}c@{}}\textbf{54.3$\pm$5.6}\\ \textbf{55.3$\pm$4.8}\\ \textbf{40.8$\pm$5.2}\end{tabular} \\ \hline
diverse       & \begin{tabular}[c]{@{}l@{}}Antmaze-umaze\\ Antmaze-medium\\ Antmaze-large\end{tabular} & \begin{tabular}[c]{@{}c@{}}48.8\\ 5.2\\ 1.9\end{tabular}   & \begin{tabular}[c]{@{}c@{}}83.6\\ 55.4\\ 13.6\end{tabular} & \begin{tabular}[c]{@{}c@{}}63.8\\ \textbf{71.3}\\ 44.2\end{tabular} & \begin{tabular}[c]{@{}c@{}}53.6\\ 44.2\\ 24.8\end{tabular} & \begin{tabular}[c]{@{}c@{}}0.0\\ 0.0\\ 0.0\end{tabular}    & \begin{tabular}[c]{@{}c@{}}0.0\\ 0.6\\ 0.0\end{tabular}    & \begin{tabular}[c]{@{}c@{}}0.0\\ 0.0\\ 0.0\end{tabular}    & \begin{tabular}[c]{@{}c@{}}\textbf{71.3$\pm$4.5}\\ 56.4$\pm$5.4\\ \textbf{46.8$\pm$3.9}\end{tabular} \\ \hline
\end{tabular}
\caption{Normalized returns of different methods on the D4RL benchmark (Bold black font indicates the highest return).}
\label{tab:comp_resut}
\end{table*}


\noindent\textbf{Comparison to notable offline methods}.
%RT is evaluated with other comparative methods in the D4RL benchmark task.
% and the experimental results are shown in Table~\ref{tab:comp_resut}.
Table~\ref{tab:comp_resut} shows the results of RT+BCQ compared with recent offline RL methods (refer to the Appendix for the  full results). 
% Our experiments show that RT combined with an offline model-free method like BCQ outperforms most offline RL baselines, 
The experiments demonstrate that RT can significantly enhance the performance of the original algorithm and outperform all the baselines.
It can be seen that the model-based methods have large performance fluctuations in different environments due to the failure in generating high-return trajectories from offline data; for instance, MOReL shows performance comparable to model-free methods in the MuJoCo environment but accumulates almost zero rewards in Maze2D and Antmaze.
RT uses Reliability-guaranteed sequence modeling to adaptively determine trajectory lengths and employs high reward as conditions to generate high-return trajectories from offline data, making it highly effective across diverse environments.

% However, model-based algorithms fail to perform well in Maze2D and AntMaze.
% This phenomenon occurs because model-based methods are unable to generate high-quality trajectories from offline data, thereby failing to better facilitate policy learning.
% However, model-based algorithms underperform in Maze2D and AntMaze due to their inability to generate high-quality trajectories from offline data, xxxx which fails to better facilitate policy learning.
% In contrast, RT generates high-quality trajectories and avoids guiding the agent into unreliable areas through backward sequence modeling and reliability trajectory truncation, thereby achieving better performance.
% In contrast, RT can ascertain that the endpoint of a trajectory exists within the environment through backward sequence modeling, xxxx which is crucial in maze environments.
% Furthermore, RT uses reliability trajectory truncation to adaptively determine trajectory lengths and employs expert-level reward to infer high-quality trajectories from offline data.
% xxxx Hence, RT can generate high-quality trajectories to facilitate policy learning.

\begin{figure}[]
    \centering
    \subfigure[States distribution]{
        \includegraphics[width=0.187\textwidth]{figure/distrubtion.pdf}
    }
    \subfigure[States return]{
        \includegraphics[width=0.25\textwidth]{figure/distrubtion_vales.pdf}
    }
    \caption{Visualization of generated data. (a) represents the distribution of the original states and the states generated by RT in Walker. (b) represents the cumulative return assessed using the state-value function, with brighter colors indicating higher returns.}
    \label{fig:state_distribution}
\end{figure}


\begin{table}[t]
\begin{tabular}{llccc}
\hline
        & Environment                                                                            & \multicolumn{1}{l}{BCQ}                                    & \multicolumn{1}{l}{FT+BCQ}                                             & \multicolumn{1}{l}{RT+BCQ}                                             \\ \hline
\rotatebox[origin=c]{90}{sparse}  & \begin{tabular}[c]{@{}l@{}}Maze2D-umaze\\ Maze2D-medium\\ Maze2D-large\end{tabular}    & \begin{tabular}[c]{@{}c@{}}49.3\\ 18.2\\ 32.6\end{tabular} & \begin{tabular}[c]{@{}c@{}}50.1$\pm$6.7\\ 42.6$\pm$4.9\\ 33.2$\pm$6.8\end{tabular} & \begin{tabular}[c]{@{}c@{}}\textbf{54.3$\pm$5.6}\\ \textbf{55.3$\pm$4.8}\\ \textbf{40.8$\pm$5.2}\end{tabular} \\ \hline
\rotatebox[origin=c]{90}{dense}   & \begin{tabular}[c]{@{}l@{}}Maze2D-umaze\\ Maze2D-medium\\ Maze2D-large\end{tabular}    & \begin{tabular}[c]{@{}c@{}}50.1\\ 41.3\\ 75.2\end{tabular} & \begin{tabular}[c]{@{}c@{}}55.3$\pm$6.7\\ 60.1$\pm$8.9\\ 83.4$\pm$6.2\end{tabular} & \begin{tabular}[c]{@{}c@{}}\textbf{68.9$\pm$8.5}\\ \textbf{80.6$\pm$8.2}\\ \textbf{98.6$\pm$5.8}\end{tabular} \\ \hline
\rotatebox[origin=c]{90}{fixed}   & Antmaze-umaze                                                                          & 79.5                                                       & 80.3$\pm$7.7                                                               & \textbf{83.7$\pm$8.9}                                                               \\ \hline
\rotatebox[origin=c]{90}{play}    & \begin{tabular}[c]{@{}l@{}}Antmaze-medium\\ Antmaze-large\end{tabular}                 & \begin{tabular}[c]{@{}c@{}}1.3\\ 2.1\end{tabular}          & \begin{tabular}[c]{@{}c@{}}32.2$\pm$6.5\\ 16.2$\pm$8.3\end{tabular}            & \begin{tabular}[c]{@{}c@{}}\textbf{40.6$\pm$6.4}\\ \textbf{23.2$\pm$5.6}\end{tabular}            \\ \hline
\rotatebox[origin=c]{90}{diverse} & \begin{tabular}[c]{@{}l@{}}Antmaze-umaze\\ Antmaze-medium\\ Antmaze-large\end{tabular} & \begin{tabular}[c]{@{}c@{}}48.8\\ 5.2\\ 1.9\end{tabular}   & \begin{tabular}[c]{@{}c@{}}63.5$\pm$4.6\\ 43.4$\pm$8.3\\ 16.6$\pm$9.6\end{tabular} & \begin{tabular}[c]{@{}c@{}}\textbf{71.3$\pm$4.5}\\ \textbf{56.4$\pm$5.4}\\ \textbf{46.8$\pm$3.9}\end{tabular} \\ \hline
\end{tabular}
\caption{Performance comparison between forward and backward generation}
\label{tab:ablation_RFT}
\end{table}


\subsection{Further Analysis}
\noindent\textbf{Generated trajectory analysis}.
% To gain a deeper understanding of RT's effectiveness in addressing edge cases, we construct a BoxBall game.
To better understand the effectiveness of RT in addressing unreliable trajectory problems, we construct a game called BoxBall, where the ball needs to move from the starting point to the goal state using up, down, left, and right actions while navigating around a wall in front of the goal state.
% Additional details about the environment are provided in the Appendix.

\begin{figure}[t]
    \centering
    \subfigure[Random]{
        \label{fig:boxball_random}
        \includegraphics[width=0.20\textwidth]{figure/boxball_random.pdf}
    }
    \subfigure[Forward]{
        \includegraphics[width=0.20\textwidth]{figure/boxball_forward.pdf}
    }
    \subfigure[Backward]{
        \includegraphics[width=0.20\textwidth]{figure/boxball_backward.pdf}
    }
    \subfigure[RT]{
        \label{fig:boxball_RT}
        \includegraphics[width=0.20\textwidth]{figure/boxball_RT.pdf}
    }
    \caption{Trajectory Visualization of BoxBall. The moving space of the ball is from 0 to 9, and it moves one grid at a time. The brown line beneath the red square represents the wall in the game. (a)$\sim$(d) denote the visualization trajectories of the random policy, forward model, backward model and RT, respectively.
    % xxxx (a) is the trajectory of a random policy. (b) is the trajectory generated by the forward model. (c) is the trajectory generated by the backward model. (d) represents the trajectory generated by the RT.
    %In BoxBall, the position of the ball is randomly initialized, and the ball moves to the red square (goal state) through four actions: up, down, left, and right. The moving space of the ball is from 0 to 9, and it moves one grid at a time. The brown line beneath the red square represents the wall in the game. (a) is the trajectory of a random policy. (b) is the trajectory generated by the forward model. (c) is the trajectory generated by the inverse model. (d) represents the trajectory generated by the RT. The x-axis and y-axis represent the space of movement
    }
    \label{fig:boxball_f}
\end{figure}

In this environment, we use DQN to collect a dataset and use it to train forward, backward, and RT models.
Then, we use the trained forward model, backward model and RT to generate and visualize 50 trajectories.
As shown in Fig.~\ref{fig:boxball_f}, the trajectory of the random strategy does not include the action of walking through the wall, while the trajectories generated by the forward model contain many unreliable trajectory (i.e., trajectories cross the wall) and fail to reach the goal state.
The values of unreliable trajectories may be arbitrarily misestimated, and then the erroneous values may be propagated throughout the state-action space leading to overestimation of Q-values during training.
Despite employing the same backward trajectory generation method, the backward model generates some unreliable trajectories due to cumulative errors and model lacks of ability to capture long-term dependencies.
RT utilizes sequence modeling techniques to effectively learn the distribution of trajectories and employs reliability estimation to adaptively determine the generated trajectories, which ensures that actions involving passing through walls do not occur.
A more detailed analysis of the generating reliable trajectory is discussed in the Appendix.
% xxxx Even if the starting state is not at a goal state, RT can generate trajectories that resemble the distribution of high return trajectories (i.e., contain goal state), such as the pink and aqua trajectories shown in Fig~\ref{fig:boxball_RT}.
% A more detailed analysis of the unreliable trajectory is discussed in the Appendix.
\\
\\
\textbf{Forward and backward analysis}.
In order to explore whether backward generation is more advantageous than forward generation,  we conduct forward and backward comparison experiments on Maze2D and Antmaze, where forward generation is donoted as FT.
% we conduct forward and backward manner xxxx studies to investigate whether the RT abilities arise from the backward modeling.
% xxxx by replacing the backward manner in RT with forward manner, denoted as FT.
% Table~\ref{tab:ablation_RFT} shows that RT+BCQ significantly outperforms FT+BCQ and BCQ, which means that the backward modeling mechanism is crucial for RT in the offline RL setting.
Table~\ref{tab:ablation_RFT} shows the experimental results for BCQ, FT+BCQ, and RT+BCQ with 10 random seeds. As can be seen, FT+BCQ provides superior performance in most environments, but not as significant as RT+BCQ.
For example, in Antmaze-large, where the maze area is extensive and the paths are complex, FT requires longer generation lengths to generate high-return trajectories, which in turn increases trajectory distribution error.
In contrast, RT is the backward generation, which enables safe generalization across all layouts (i.e., RT can ensure that the generated trajectories contain goal states), thus ensuring RT+BCQ significantly outperforms FT+BCQ and BCQ.
% RT+BCQ significantly outperforms FT+BCQ and BCQ, which means that the backward modeling mechanism is crucial for RT in the offline RL setting.
% xxx In Maze2D, RT+BCQ outperforms xxx all settings compared to BCQ.
% xxxx While FT+BCQ achieves superior performance in Maze2D-umaze, it underperforms in Maze2D-medium and Maze2D-large.
% Specifically, Maze2D-medium and Maze2D-large have more paths and obstacles.
% Therefore, although FT employs sequential modeling techniques, it is still challenging to generate trajectories that are beneficial for the improvement of the BCQ policy.
% Although FT employs sequential modeling techniques, it is still xxxx challenging to include actual goal states in the generated trajectories.
% xxx The inclusion of correct goal state information is crucial for subsequent policy learning, as its absence can degrade policy performance.
% RT is the backward manner, which enables safe generalization across all layouts (i.e., RT can ensure that the generated trajectories contain goal states).
% In Antmaze, RT+BCQ achieves superior performance across all layouts, while FT+BCQ and BCQ perform well only in Antmaze-medium.
% The maze is vast and the paths are crisscrossing in Antmaze-large, which hinders the forward manner of FT to generate trajectories.
% In Antmaze-large, where the maze area is extensive and the paths are complex, FT requires longer generation lengths to produce high-quality trajectories (i.e., those including the goal state), which in turn increases trajectory distribution error.
% However, RT has an advantage because it can generate backward trajectories from the goal state.
\\
\\
\noindent \textbf{Reliability estimation analysis}.
In order to explore the impact of reliability estimation on RT performance, we conduct experiments on Halfcheetah-medium-replay, Walker-medium-replay, dense Maze2D-larger and diverse Antmaze-large.
% RT is divided into two categories: fixed-length truncation and reliability trajectory truncation.
% The fixed-length truncation lengths are set to 3 (RT-F3) and 5 (RT-F5), respectively. 
RT is divided into two categories: fixed-length generation, with lengths set to 3 (RT-F3) and 5 (RT-F5), and reliability estimation.
Fig.~\ref{fig:IQL_boost} illustrates the impact of different generation length mechanisms on the convergence of the IQL learning process.
% RT performs truncation mechanism studies on Halfcheetah-medium-replay, Walker-medium-replay, dense Maze2D-larger, and diverse Antmaze-large to evaluate the impact of reliability trajectory truncation on performance.
% Specifically, RT is divided into two categories: fixed-length truncation and reliability trajectory truncation.
% The fixed-length truncation lengths are set to 3 (RT-F3) and 5 (RT-F5), respectively. 
% These truncation versions are trained on offline datasets from the above environments, and then a random point on a trajectory is selected to generate data.
% The generated dataset size is $\frac{1}{3}$ of the original dataset.
% % Then, the augmented dataset is merged with the original dataset, and these data are used to train IQL.
% Then, the generated dataset and the original dataset are merged into a new dataset, which is used to train IQL.
% Fig.~\ref{fig:IQL_boost} shows the convergence of IQL in different environments.
Both RT-F3 and RT-F5 can improve the performance of IQL in the Halfcheetah-medium-replay and Walker-medium-replay because of the sequence modeling, which ensures that the data generated by the models are consistent with the original offline data.
RT uses reliability estimation, which makes it more flexible and diverse when generating trajectories, thereby improving the performance of IQL.

In the dense Maze2D-large and diverse Antmaze-large environments, where maze layouts and paths become increasingly complex, shorter trajectories may fail to yield higher cumulative rewards, or may even decrease cumulative rewards.
% RT-F3 and RT-F5 cannot ensure the reliability of data through fixed-length truncation, resulting in limited performance improvement of IQL.
% Although RT-F3 and RT-F5 use fixed-length truncation to prevent generated data from exceeding the offline data distribution, this method limits the creation of high-quality trajectories, leading to a restricted improvement in IQL performance.
RT-F3 and RT-F5 generate shorter trajectories by using fixed-length generation to prevent the generated data from exceeding the distribution of offline data, which also limits the generation of high-return trajectories.
In contrast, RT employs a reliability estimation, which can automatically determine the length of the generated trajectories while ensuring that these state-action pairs remain within the distribution range of the original offline data.
Hence, RT can generate more high-return trajectories to facilitate IQL learning.
% In short, RT can have better data augmentation effect using reliability rule truncation.

\begin{figure}[t]
    \centering
    \subfigure[Halfcheetah]{
        \includegraphics[width=0.22\textwidth]{figure/Halfcheetah_IQL.pdf}
    }
    \subfigure[Walker]{
        \includegraphics[width=0.22\textwidth]{figure/walker_IQL.pdf}
    }
    \subfigure[Maze2D-large]{
        \includegraphics[width=0.22\textwidth]{figure/Maze2D_large_IQL.pdf}
    }
    \subfigure[AntMaze-large]{
        \includegraphics[width=0.22\textwidth]{figure/antmaze_large_IQL.pdf}
    }
    \caption{The impact of different truncation mechanisms on the performance of IQL. The x-axis represents the number of training episodes ($1\times 10^4$), and the y-axis represents the cumulative reward.}
    \label{fig:IQL_boost}
\end{figure}

\subsection{Related Work}
\noindent
Offline model-based methods~\cite{DBLP:KidambiRNJ20,DBLP:SwazinnaUR21} learn an approximate dynamics model of the environment and utilize planning algorithms to search for optimal trajectories within that model.
However, research indicates that even small generation errors can significantly degrade the performance of multi-step rollouts, as generation errors compound, causing the model to deviate from the high-precision region after a few steps~\cite{DBLP:Talvitie17,DBLP:AsadiML18}.
To mitigate generation errors, some studies~\cite{DBLP:YuTYEZLFM20,DBLP:abs-2111-11097,DBLP:ZhengWXH23} employ multiple models to predict states and actions, using uncertainty quantification to eliminate samples with excessive errors.
In addition, some studies~\cite{chen2022lapo,DBLP:YangJZMS22,DBLP:MaT0M23} restrict the learning policy from accessing areas with significant differences between the learned and the real dynamics to prevent policy learning failures due to generation errors.

Recent studies~\cite{DBLP:WangLJZLZ21,DBLP:LyuLL22,DBLP:LuBTP23,DBLP:abs-2406-12550} suggest the use of fixed truncation techniques~\cite{DBLP:JannerFZL19} to generate shorter trajectories and thus reduce the impact of generation errors on generated data.
This generated data is then combined with the original data to create a new dataset, which can be utilized in model-free offline algorithms.
However, these studies neglect the influence of historical information on environmental dynamics, potentially resulting in the generation of data that fails to align with the environment.
% However, these studies overlook the impact of historical information and the pursuit of high rewards on the generated data.
Unlike the above methods, RT extends the offline dataset by capturing historical information between different locations in the input sequence and using reliability assessment to generate reliable trajectories.
Moreover, RT employs a cumulative reliability metric to dynamically adjust the trajectory length, enabling a more flexible mitigation of the adverse effects of accumulated generation errors.
% Unlike the above methods, RT extends offline data by capturing the interdependencies at various positions along the trajectories and using high reward sampling to generate high-return data. 
\\
\\
\textbf{Sequence modeling}.
% Recent works~\cite{DBLP:JannerLL21,DBLP:DT,DBLP:YamagataKS23,DBLP:BadrinathFNB23,DBLP:GaoWCKZ024} treat RL as a sequence modeling problem, thereby incorporating Transformers into RL.
Recent research~\cite{DBLP:JannerLL21,DBLP:DT,DBLP:YamagataKS23,DBLP:BadrinathFNB23,DBLP:GaoWCKZ024} shifts the paradigm of RL from traditional MDP to sequence modeling.
In particular, Trajectory Transformer (TT)~\cite{DBLP:JannerLL21} treats offline RL as a sequence modeling problem and further leverages the capabilities of sequence models by using beam search to incorporate states, actions, and rewards.
At the some time, Decision Transformer (DT)~\cite{DBLP:DT} learns the distribution of trajectories and predicts actions based on the given target reward and preceding states, rewards, and actions.
Q-Learning DT~\cite{DBLP:YamagataKS23} extends the use of DT in offline RL by introducing a value function, which relabels the returns in the training data using the dynamics planning results, and then uses the relabeled data to train the DT.
Multi-Game Decision Transformer (MGDT)~\cite{DBLP:LeeNYLFGFXJMM22} trains a Transformer to solve multiple Atari games, which eliminates the need for additional fine-tuning of unseen tasks.
Waypoint Transformer (WT)~\cite{DBLP:BadrinathFNB23} utilizes an architecture constructed based on the DT framework and conditioned on automatically generated waypoints to learn optimal Transformer policies.
Compared to these model-free approaches, we propose a model-based approach that generates high-return trajectories from suboptimal datasets using the Reliability-guaranteed Transformer framework  to facilitate policy learning in model-based RL.


\section{Conclusion}
In this paper, we propose a novel model-based offline RL method RT to facilitate policy learning by generating reliable and high-return trajectories.
RT leverages Reliability-guaranteed sequence generation techniques and conditions the generation process on high rewards to generate high-return trajectories that are legitimate in the environment.
% By employing reliability-driven sequence generation techniques and conditioning the generation process on high rewards, RT enables the generation of high-return trajectories that are legitimate in the physical environment.
% Furthermore, RT employs a reliability trajectory estimation to adaptively truncate synthetic trajectories, thereby avoiding the negative influence arising from the inappropriate length of generated trajectories.
Extensive experiments demonstrate that RT can be effectively integrated with existing model-free algorithms and achieve better performance against existing offline RL benchmarks.
However, RT presupposes a fully observable state space in its empirical evaluations.  
For partially observable environments, RT may require additional techniques (e.g., Kalman Filters) to address the issue of incomplete information in order to accurately predict states and rewards. 
% Future work includes evaluation of RT in high-dimensional observation tasks.
Future work includes addressing this above issue and extending RT to multi-agent offline algorithms.





% \appendix

% \section*{Ethical Statement}

% There are no ethical issues.

% \section*{Acknowledgments}

% The preparation of these instructions and the \LaTeX{} and Bib\TeX{}
% files that implement them was supported by Schlumberger Palo Alto
% Research, AT\&T Bell Laboratories, and Morgan Kaufmann Publishers.
% Preparation of the Microsoft Word file was supported by IJCAI.  An
% early version of this document was created by Shirley Jowell and Peter
% F. Patel-Schneider.  It was subsequently modified by Jennifer
% Ballentine, Thomas Dean, Bernhard Nebel, Daniel Pagenstecher,
% Kurt Steinkraus, Toby Walsh, Carles Sierra, Marc Pujol-Gonzalez,
% Francisco Cruz-Mencia and Edith Elkind.


%% The file named.bst is a bibliography style file for BibTeX 0.99c
\bibliographystyle{named}
\bibliography{ijcai25}

\end{document}

