\section{Conclusion}
\label{sec: conclusion}
% What I need to write -> What's the problem of Thai legal QA system -> What did we propose -> What did we find from the three research questions -> What should future works continue on
One of the most significant challenges in implementing LLMs for Thai Legal QA systems is the lack of a standardized evaluation process. This issue arises due to the limited availability of Thai legal QA corpora and the absence of robust evaluation metrics. To address this, we introduce a novel benchmark dataset along with a corresponding task and evaluation framework named \textbf{NitiBench}.

Specifically, we construct two datasets: (1) \textbf{NitiBench-CCL} (derived from WangchanX-Legal-ThaiCCL), which covers general QA across 21 Thai financial law codes in its test split and 35 codes in its training split, and (2) \textbf{NitiBench-Tax}, which focuses on specialized QA involving real-world tax cases from the Thai Revenue Department, requiring extensive legal reasoning.

To complement this benchmark, we propose an evaluation framework that includes: (1) \textbf{Multi-label retrieval metrics}, in addition to traditional single-label metrics; (2) \textbf{An E2E task}, evaluating the system’s ability to generate correct answers consistent with ground truth while providing accurate legal citations; and (3) \textbf{E2E evaluation metrics}, measuring \textbf{Coverage} (how well the generated answer aligns with the ground truth), \textbf{Contradiction} (whether the generated answer contradicts the ground truth), and \textbf{Citation} (the accuracy of legal citations provided in the generated answer).

Using the proposed benchmark, we aim to address the research questions introduced in \S\ref{sec:introduction}. Through experiments outlined in \S\ref{subsec: setup_rq1}, we observed that our \textbf{hierarchy-aware} chunking approach slightly but consistently outperforms the system with the naive chunking strategy. This underscores the importance of integrating domain knowledge and understanding document structure when selecting a chunking strategy for optimal performance. Regarding the proposed \textbf{NitiLink}, we found that it notably improves context retrieval but does not enhance E2E performance on NitiBench-Tax queries. This may be attributed to the complexity of NitiBench-Tax queries, which require more detailed reasoning. For the NitiBench-CCL, NitiLink has a limited effect on improving context retrieval, as NitiBench-CCL queries often involve multiple non-hierarchically related sections, which NitiLink cannot resolve. However, those queries where NitiLink does provide an improvement show significant gains in E2E metrics.

In experiments described in \S\ref{subsec: setup_rq2}, we found that all retrieval models struggled with NitiBench-Tax. Among the retrievers, BGE-M3 fine-tuned on the WangchanX-Legal-ThaiCCL training split showed the highest performance. However, fine-tuning did not lead to performance improvements for NitiBench-Tax, and in some cases, it does not elicit performance improvements for NitiBench-CCL itself. Regarding the LLM component, no significant performance differences were observed between models, though Claude 3.5 Sonnet showed marginally better results. All tested LLMs, however, delivered mediocre performance on NitiBench-Tax. This issue is further illustrated in Table~\ref{subsubsec: e2e_best_result}, where even with a golden context provided, the LLMs still struggle to produce meaningful results on NitiBench-Tax queries, which demand complex reasoning. Additionally, the same table highlights limitations in the RAG pipeline on NitiBench-Tax. The performance is constrained by both the retriever model’s inherent limitations and the LLM's restricted ability to effectively leverage the correctly retrieved laws, as evidenced by the disparity between end-to-end recall and retriever recall.

We also evaluated the feasibility of using long-context LLMs (LCLMs) for both E2E tasks and retrieval tasks. LCLMs performed poorly in the E2E setting, showing subpar results. However, in the retrieval task, LCLMs performed reasonably well, although they still lagged behind embedding-based models, particularly when the number of retrieved documents was increased, highlighting the advantages of embedding-based approaches in this context.

Lastly, we address the limitations of our proposed benchmark, particularly with regard to the ambiguity of queries and the multi-label nature of the queries. We suggest potential methods to mitigate these issues in future work. Additionally, we propose the inclusion of legal reasoning evaluation in future assessments, offering a brief literature review on LLM reasoning evaluation to support this recommendation.


