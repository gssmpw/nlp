\section{Results}
% For chunking, It could be condensed -> First, show the final results (line chunking vs ideal chunking) and the naive chunking stats. Then, describe the trends and example of sections not in any naive chunking strategy.
\subsection{(RQ1) Impact of tailored components}
\label{subsec: rq1_result}

\subsubsection{Hierarchy-aware Chunking}
\label{subsubsec: chunking_result}

From the defined metrics that are used to quantify the information loss on each type of chunking in \S\ref{subsubsec: chunk_setup}, we can conclude that a good chunking strategy should minimize \textit{Fail Chunk Ratio}, \textit{Fail Section Ratio} and \textit{Uncovered Section Ratio}.
%
Minimizing these metrics will reduce the information loss of some sections or parts of sections that are missing from the chunks.
%
Additionally, \textit{Sections/Chunk} and \textit{Chunks/Section} should be close to 1 in order for sections \emph{not to be} split into multiple chunks and retain atomicity within each chunk.

We evaluate multiple configurations of chunking strategies, chunk sizes, and overlaps as described in \S\ref{subsubsec: chunk_setup} and present the average metrics for each chunking strategy in Table~\ref{table: chunking_by_type}. 
%
It is observed that the chunking strategy most closely resembling the output of our hierarchy-aware chunking strategy is line-based chunking. 

However, across all strategies, approximately 30\% of sections are not referenced in any chunks, and at least 41.7\% of sections are not fully contained within a single chunk. 
%
Further analysis indicates that around 20\% of all sections cannot be fully covered in a single chunk under any naive chunking strategy due to their extended length. 
%
This necessitates the retrieval model to retrieve multiple chunks to provide sufficient context.

An example of such a section is Section 44 of the \textbf{Emergency Decree on Digital Asset Businesses, B.E. 2561}, which cannot be fully covered in a single chunk across any naive chunking strategy. 
%
This is attributed to its lengthy content and the presence of multiple subsections separated by newline characters, which are commonly used as delimiters in many naive chunking approaches.

% Should this be translated?
\begin{quote}
    \textbf{Section 44 of Emergency Decree on Digital Asset Businesses, B.E. 2561}
    
    It shall be presumed that the following persons, who exhibit behavior involving the buying or selling of digital tokens or engaging in forward contracts related to digital tokens in an unusual manner for themselves, are persons who possess or are aware of inside information as defined under Section 42:
    
    (1) Holders of digital tokens exceeding 5\% of the total tokens sold in each series by the issuer of digital tokens. This includes digital tokens held by their spouses, cohabiting partners in the manner of husband and wife, and their minor children.
    
    (2) Directors, executives, controlling persons, employees, or staff members of the affiliated entities of the digital token issuer who are in positions or roles responsible for, or with access to, inside information.
    
    (3) Ascendants, descendants, adoptive parents, or adopted children of persons specified under Section 43.
    
    (4) Siblings sharing the same father and mother, or the same father or mother as persons specified under Section 43.
    
    (5) Spouses or cohabiting partners in the manner of husband and wife of persons specified under Section 43 or individuals listed under (3) or (4).
    
    The term \enquote{affiliated entities of the digital token issuer} under (2) refers to parent companies, subsidiaries, or associated companies of the digital token issuer, as defined by the criteria set forth by the SEC Board's announcements.
\end{quote}

\begin{table}[!ht]
\centering

\resizebox{\textwidth}{!}{%
\renewcommand{\arraystretch}{1.3} % This increases the cell height by 1.5 times
\small % or \scriptsize
\begin{tabular}{@{}lccccc@{}}
\toprule
\textbf{Chunking Strategy} & \multicolumn{1}{l}{\textbf{Section/Chunk $\rightarrow$1}} & \multicolumn{1}{l}{\textbf{Chunk/Section $\rightarrow$1}} & \multicolumn{1}{l}{\textbf{Fail Chunk Ratio $\downarrow$}} & \multicolumn{1}{l}{\textbf{Fail Section Ratio $\downarrow$}} & \multicolumn{1}{l}{\textbf{Uncovered Section Ratio $\downarrow$}} \\ \midrule
\cellcolor{lightgray}Hierarchy-aware  & \cellcolor{lightgray}{1.000}   & \cellcolor{lightgray}{1.000}    & \cellcolor{lightgray}{0.000}  & \cellcolor{lightgray}{0.000}  & \cellcolor{lightgray}{0.000}                                       \\
Character  & 3.098                             & 1.710                              & 0.819                                & 0.675                                  & 0.397                                       \\

Line       & \textbf{1.689}                    & \textbf{1.234}                    & \textbf{0.658}                       & \textbf{0.417}                         & \textbf{0.294}                              \\

Recursive  & \underline{1.793}                       &\underline{1.27}                        & \underline{0.741}                          & \underline{0.504}                            & \underline{0.381}                                 \\ \bottomrule
\end{tabular}
}
\caption{Information loss comparison between hierarchy-aware chunking compared to other naive chunking strategies. Since hierarchy-aware chunking consistently parses into a single law section, it was treated as an upper bound because no information loss occurred.}
\label{table: chunking_by_type}
\end{table}

For the specific configuration of line chunking that produces chunks most similar to hierarchy-aware chunking, we fix the chunking strategy while varying the chunk overlap and chunk size parameters. 
%
Increasing the chunk size results in more text per chunk, leading to higher \textbf{Sections/Chunk} and \textbf{Chunks/Section} values while reducing the \textbf{Fail Chunk Ratio}, \textbf{Fail Section Ratio}, and \textbf{Uncovered Section Ratio}. 
%
Similarly, increasing the overlap effectively increases the chunk size, producing comparable effects to directly increasing the chunk size.
%
Based on these observations, we select the optimal configuration for naive chunking as line chunking with a chunk size of 553 characters and a chunk overlap of 50 characters. The detailed results for this configuration are displayed in Appendix~\ref{appendix: chunk_hyper}.

Finally, the metrics for the selected naive chunking configuration are compared against hierarchy-aware chunking in Table~\ref{table: chunking_compare_metric}.

\begin{table}[!ht]
\centering

\resizebox{\textwidth}{!}{%
\renewcommand{\arraystretch}{1.3} % This increases the cell height by 1.5 times
\small % or \scriptsize
\begin{tabular}{@{}lccccc@{}}
\toprule
\textbf{Chunking Strategy} & \multicolumn{1}{l}{\textbf{Section/Chunk $\rightarrow$1}} & \multicolumn{1}{l}{\textbf{Chunk/Section $\rightarrow$1}} & \multicolumn{1}{l}{\textbf{Fail Chunk Ratio $\downarrow$}} & \multicolumn{1}{l}{\textbf{Fail Section Ratio $\downarrow$}} & \multicolumn{1}{l}{\textbf{Uncovered Section Ratio $\downarrow$}} \\ \midrule
Hierarchy-aware chunking  & 1.000   & 1.000    & 0.000  & 0.000  & 0.000                                       \\
Line chunking (553 chunk size and 50 chunk overlap)  & 1.956                             & 1.180                              & 0.521                                & 0.323                                  & 0.156                                       \\ \bottomrule
\end{tabular}
}
\caption{Information loss comparison between perfect chunking strategy (hierarchy-aware chunking) and the best naive chunking setup.}
\label{table: chunking_compare_metric}
\end{table}

% Next, we also show results on our benchmark as well
Apart from the evaluation of chunking in isolation in terms of information loss, we also present the evaluation results on our benchmark in Table~\ref{table: chunk_e2e_main}.

\begin{table}[ht!]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}lccccccc@{}}
    \toprule
    \textbf{Settings} & \textbf{Retriever Multi MRR ($\uparrow$)} & \textbf{Retriever Recall ($\uparrow$)} & \textbf{Coverage ($\uparrow$)} & \textbf{Contradiction ($\downarrow$)} & \textbf{E2E Recall ($\uparrow$)} & \textbf{E2E Precision ($\uparrow$)} & \textbf{E2E F1 ($\uparrow$)} \\
    \midrule
    Naïve Chunking            & 0.786 & 0.935 & 86.6 & \textbf{0.050} & 0.882 & 0.613 & 0.722 \\
    Hierarchy-aware Chunking  & \textbf{0.834} & \textbf{0.942} & \textbf{86.7} & 0.054 & \textbf{0.894} & \textbf{0.630} & \textbf{0.739} \\
    \bottomrule
\end{tabular}%
}
\caption{Effect of chunking configuration on E2E performance on NitiBench-CCL}
\label{table: chunk_e2e_main}
\end{table}


% From table~\ref{table: chunk_e2e_main}, the naive chunking strategy performs significantly worse than section-based chunking in terms of retrieval performance on both WCX and Tax Case datasets. This discrepancy likely stems from two factors: First, naive chunking discards chunks that do not fully contain a section. Second, it often splits single sections across multiple chunks, rendering these fragmented sections unusable for evaluation and practical application, as legal responses require complete sections.

% The E2E performance also agrees with the retrieval performance in that the section-based chunking significantly outperforms line chunking. Interestingly,with line chunking, end-to-end (E2E) recall (based on sections cited by the LLM) exceeds retrieval recall. This stems from two factors: 1) LLM sometimes cites unretrieved sections, either from its internal knowledge or through hallucination; and 2) Line chunking’s mapping of chunks to single, fully covered sections can lead to partial section coverage within a chunk, causing LLM to cite portions outside the mapped section. Consequently, line chunking’s E2E recall can surpass its retrieval recall.

From Table~\ref{table: chunk_e2e_main}, the naive chunking strategy performs worse than hierarchy-aware chunking in terms of retrieval performance. 
%
This discrepancy likely arises because naive chunks often contain content from multiple sections, introducing \enquote{noise} that can negatively impact the retrieval model's ranking of relevant documents.  

However, in terms of end-to-end (E2E) performance, the system using hierarchy-aware chunking only slightly outperforms the one using naive chunking. 
%
We suspect that this is because the LLM can effectively filter out the \enquote{noise} in the retrieved sections during answer generation. 
%
As a result, the coverage and contradiction scores are not significantly different between the two systems.
%
Nevertheless, there remains a discrepancy in the E2E citation score.  

In conclusion, \textbf{hierarchy-aware chunking achieves a slight but consistent advantage over the naive chunking strategy.}

\subsubsection{NitiLink}
\label{subsubsec: referencer_result}

The evaluation results of the experiment described in \S\ref{subsubsec: referencer_setup} are presented in Table~\ref{table: augmenter_e2e_main}. 
%
In the table, ``Ref Depth 1'' denotes a RAG system that incorporates a NitiLink component with a maximum depth of 1, while \enquote{No Ref} represents a RAG system without NitiLink. 
%
For the metrics, ``NitiLink'' indicates retrieval metrics calculated on the augmented context, which includes both the initially retrieved sections and the additional sections fetched by NitiLink.

% \begin{table}[!ht]
% \centering
% \caption{Effect of augmenter configuration on E2E performance}
% \renewcommand{\arraystretch}{1.5} % This increases the cell height by 1.5 times
% \label{table: augmenter_e2e_main}
% \begin{tabular}{@{}c|cc|cc@{}}
% \toprule
% Dataset                                  & \multicolumn{2}{c|}{Tax}        & \multicolumn{2}{c}{WCX}         \\ \midrule
% Setting                                  & Ref Depth 1    & No Ref         & Ref Depth 1    & No Ref         \\ \midrule
% Retriever MRR                            & 0.574          & 0.574          & 0.809          & 0.809          \\
% \multicolumn{1}{l|}{Retriever Multi MRR} & 0.333          & 0.333          & 0.809          & 0.809          \\
% Retriever Recall                         & 0.499          & 0.499          & 0.938          & 0.938          \\
% Referencer MRR                           & \textbf{0.582} & 0.574          & 0.800          & \textbf{0.809} \\
% Referencer Multi MRR                     & \textbf{0.345} & 0.333          & 0.800          & \textbf{0.809} \\
% Referencer Recall                        & \textbf{0.602} & 0.499          & \textbf{0.940} & 0.938          \\
% Coverage                                 & 45.0           & \textbf{50.0}  & \textbf{86.3}  & 85.2           \\
% Contradiction                            & 0.520          & \textbf{0.460} & \textbf{0.051} & 0.055          \\
% E2E Recall                               & \textbf{0.354} & 0.333          & \textbf{0.885} & 0.880          \\
% E2E Precision                            & 0.630          & \textbf{0.64}  & 0.579          & \textbf{0.601} \\
% E2E F1                                   & \textbf{0.453} & 0.438          & 0.700          & \textbf{0.714} \\ \bottomrule
% \end{tabular}%
% \end{table}
\begin{table}[!ht]
\centering
\renewcommand{\arraystretch}{1.3}
\newcommand{\gray}{\cellcolor{gray!15}}
\newcommand{\pos}[1]{\textcolor{darkgreen}{(#1\%)}}
\newcommand{\negv}[1]{\textcolor{red}{(#1\%)}}

\begin{tabular}{lcccccc}
\toprule
\multirow{2}{*}{\textbf{Metric}} & \multicolumn{3}{c}{\textbf{NitiBench-CCL}} & \multicolumn{3}{c}{\textbf{NitiBench-Tax}} \\ 
 & \textbf{No Ref} & \gray \textbf{Ref Depth 1} & $\Delta$ & \textbf{No Ref} & \gray \textbf{Ref Depth 1} & $\Delta$ \\ 
\midrule
\multicolumn{7}{c}{\textbf{Retriever Metrics}} \\ 
\midrule
MRR ($\uparrow$)       & \multicolumn{2}{c}{0.809} & -  & \multicolumn{2}{c}{0.574} & -  \\
Multi MRR ($\uparrow$) & \multicolumn{2}{c}{0.809} & -  & \multicolumn{2}{c}{0.333} & -  \\
Recall ($\uparrow$)    & \multicolumn{2}{c}{0.938} & -  & \multicolumn{2}{c}{0.499} & -  \\
\midrule
\multicolumn{7}{c}{\textbf{NitiLink Metrics}} \\ 
\midrule
MRR ($\uparrow$)             & 0.809  & \gray 0.800  & \negv{-1.11}  & 0.574  & \gray \textbf{0.582}  & \pos{+1.39}  \\
Multi MRR ($\uparrow$)       & 0.809  & \gray 0.800  & \negv{-1.11}  & 0.333  & \gray \textbf{0.345}  & \pos{+3.60}  \\
Recall ($\uparrow$)          & 0.938  & \gray \textbf{0.940}  & \pos{+0.21}  & 0.499  & \gray \textbf{0.602}  & \pos{+20.6}  \\
Coverage ($\uparrow$)        & 85.2   & \gray \textbf{86.3}  & \pos{+1.29}  & \textbf{50.0}   & \gray 45.0   & \negv{-10.0}  \\
Contradiction ($\downarrow$) & 0.055  & \gray \textbf{0.051}  & \pos{-7.27}  & \textbf{0.460}  & \gray 0.520  & \negv{+13.0}  \\
E2E Recall ($\uparrow$)      & 0.880  & \gray \textbf{0.885}  & \pos{+0.57}  & 0.333  & \gray \textbf{0.354}  & \pos{+6.31}  \\
E2E Precision ($\uparrow$)   & 0.601  & \gray 0.579  & \negv{-3.66}  & \textbf{0.640}  & \gray 0.630  & \negv{-1.56}  \\
E2E F1 ($\uparrow$)          & \textbf{0.714}  & \gray 0.700  & \negv{-1.96}  & 0.438  & \gray \textbf{0.453}  & \pos{+3.42}  \\
\bottomrule
\end{tabular}
\caption{Effect of NitiLink augmenter configuration on E2E performance. The $\Delta$ column shows the relative percentage change compared to ``No Ref'', with dark green indicating improvement and red indicating degradation.}
\label{table: augmenter_e2e_main}
\end{table}




The results from Table~\ref{table: augmenter_e2e_main} show that there is no clear significant advantage when employing NitiLink in a RAG system. 
%
The results also highlight the differing impacts of incorporating NitiLink across datasets.

\textbf{NitiBench-Tax} For this dataset, we can clearly see that the recall was substantially improved from 0.499 to 0.602.
%
The improvement of recall suggested that NitiLink does provide an additional correct law section to the retrieved documents. 
%
Despite significant improvement over recall, we only see marginal improvements over MRR and Multi MRR.
%
Since we're using a depth-first augmented strategy (see \S\ref{subsubsec: referencer_setup}), this suggested that the document that cited more positives by NitiLink is ranked at the bottom of the retrieved documents.
%
Surprisingly, despite a major improvement in recall, some E2E metrics declined.
%
This might be due to NitiBench-Tax's query complexity, which often demands advanced reasoning capabilities that the LLM, even with the correct documents, struggles to provide. 
%
Another reason that might affect the performance decline even with more relevant documents provided to the LLM is the longer context that the LLM needs to process due to the higher amount of content added by NitiLink.

\textbf{NitiBench-CCL} For NitiBench-CCL showed no significant change in retrieval metrics and most E2E metrics.
%
Incorporating NitiLink yields very little recall gain, while MRR is slightly lower. 
%
This means that NitiLink often pushed the positive lower in the ranking as we're using a depth-first augmentation strategy (see \S\ref{subsubsec: referencer_setup}).
%
We highlight several factors that might contribute to the limited recall gain in this dataset:
\begin{enumerate}
    \item \textbf{Binary recall nature:} NitiBench-CCL queries typically involve a single relevant law, making recall binary and thus harder to improve.
    %
    \item \textbf{Simplicity of NitiBench-CCL queries:} Simple, non-specific NitiBench-CCL queries often rely on many relevant law sections that are similar semantically rather than hierarchically. 
    %
    This is opposed to NitiBench-Tax, where referenced law sections are necessary for legal reasoning.
    %
    This simplicity stems from the fact that the dataset was created by letting the annotator craft a question based on a given law section.
    %
    This explicitly provides bias toward the dataset since the question was created without a referenced law section.
    %
    \item \textbf{Hierarchical limiation:} The hierarchical structure itself presents challenges. 
    %
    Although NitiLink augmented the retrieved law section mentioned in the retrieved document (children reference), it lacks a law section that references retrieved law sections (parent reference).  
    %
    Thus, this version of NitiLink that lacks the ability to fetch parent law sections could result in a suboptimal performance.
    %
    %\item The hierarchical structure itself presents challenges. Many sections in the Revenue Code lack hierarchical connections (41\% have no children, 45\% no parents, and 24\% neither), limiting the referencer's effectiveness. Furthermore, some queries (e.g., those about criminal penalties) require retrieving laws that reference multiple others. The current referencer, retrieving only children of initially retrieved laws, struggles with these, as it cannot retrieve the parent law (which might be the ground truth).
\end{enumerate}

Despite the limited recall gains in NitiBench-CCL, we can see that there's a slight improvement in coverage score as well as recall.
%
This suggests that even small recall improvements can enhance the LLM's ability to answer NitiBench-CCL queries effectively.

\subsection{(RQ2) Impact of Retriever and LLM}
\label{subsec: rq2_result}

\subsubsection{Retriever}
\label{subsubsec: retriever_result}

\textbf{NitiBench-CCL} 
%
Table~\ref{table: retrieval_wangchan} presents the retrieval performance of 8 models as described in \S\ref{subsubsec: retriever_setup} on NitiBench-CCL with hierarchy-aware chunking. 
%
Because each query has only one positive label (as mentioned in \S\ref{subsubsec: wcx_dataset}), the multi-hit-rate and multi-MRR metrics are equivalent to their single-label counterparts. This also applies to recall and hit rate as well. Thus, for this dataset, we only showed Recall (Recall@K) and MRR (MRR@K) since other metrics are considered redundant.

The best-performing model is the human-reranked fine-tuned BGE-M3, achieving an MRR@5 of 0.805. 
%
Close behind are the auto-reranked fine-tuned BGE-M3 (0.800 MRR@5) and the base BGE-M3 (0.579 MRR@5). 
%
BGE-M3's strong performance is likely due to its use of three embedding types for relevance calculation, further enhanced by fine-tuning on in-domain data. 
%
Notably, the auto-reranked version nearly matches human-reranked performance without requiring costly human annotation. 
%

\textbf{Based on these findings, we recommend a cost-effective in-domain adaptation pipeline, notably Auto-Finetuned BGE-M3, that uses a strong LLM to generate synthetic training pairs, retrieves top-k passages with BGE-M3, and then applies a BGE-M3 Reranker. 
%
As shown in the results, this approach closely matches human-reranked performance while significantly saving annotation costs in an in-domain setup.
}

The commercially available Cohere embedding model ranks just below the top-performing BGE-M3 models and is followed by the ColBERT-based and dense embedding models, JINA ColBERT v2 and JINA embeddings v3, respectively. 
%
Among the tested retrievers, NV-Embed v1 shows the lowest performance among non-baseline models (0.713 MRR@5), likely due to its decoder-based architecture and reliance on prefix instruction prompts. 
%
Overall, retrieval performance on NitiBench-CCL is strong, with most models delivering comparable results, except for NV-Embed v1 and BM25. 
%
However, despite this strong performance, a gap between hit-rate and MRR when $k=\{5,10\}$ indicates that \textbf{while relevant documents are frequently retrieved, they are not consistently ranked first, potentially impacting end-to-end performance.}

\begin{table}[!ht]
\centering

% \renewcommand{\arraystretch}{1.5}
\small
\begin{tabular}{@{}clcc@{}}
\toprule
\textbf{Top-K} & \textbf{Model} & \textbf{HR/Recall@k} & \textbf{MRR@k} \\ \midrule
\multirow{8}{*}{k=1} 
  & BM25                   & .481 & .481 \\
  & JINA V2                & .681 & .681 \\
  & JINA V3                & .587 & .587 \\
  & NV-Embed V1            & .492 & .492 \\
  & BGE-M3                 & .700 & .700 \\
  & Human-Finetuned BGE-M3 & \textbf{.735} & \textbf{.735} \\
  & Auto-Finetuned BGE-M3  & \underline{.731} & \underline{.731} \\
  & Cohere                 & .676 & .676 \\ \midrule
\multirow{8}{*}{k=5} 
  & BM25                   & .658 & .548 \\
  & JINA V2                & .852 & .750 \\
  & JINA V3                & .821 & .681 \\
  & NV-Embed V1            & .713 & .579 \\
  & BGE-M3                 & .880 & .773 \\
  & Human-Finetuned BGE-M3 & \textbf{.906} & \textbf{.805} \\
  & Auto-Finetuned BGE-M3  & \underline{.900} & \underline{.800} \\
  & Cohere                 & .870 & .754 \\ \midrule
\multirow{8}{*}{k=10} 
  & BM25                   & .715 & .556 \\
  & JINA V2                & .889 & .755 \\
  & JINA V3                & .875 & .688 \\
  & NV-Embed V1            & .776 & .587 \\
  & BGE-M3                 & .919 & .778 \\
  & Human-Finetuned BGE-M3 & \textbf{.938} & \textbf{.809} \\
  & Auto-Finetuned BGE-M3  & \underline{.934} & \underline{.804} \\
  & Cohere                 & .912 & .760 \\ \bottomrule
\end{tabular}
\caption{Retrieval Evaluation Result on NitiBench-CCL with hierarchy-aware chunking. Since the test split contains a single positive (as mentioned in \S \ref{subsubsec: wcx_dataset}), we collapsed metrics that are duplicated, such as HitRate (HR)/ Recall/ Multi-HitRate and MultiMRR / MRR.}
\label{table: retrieval_wangchan}
\end{table}


\textbf{NitiBench-Tax} Table~\ref{table: retrieval_tax} presents the retrieval performance of various models on NitiBench-Tax using hierarchy-aware chunking. 
%
Unlike NitiBench-CCL, this dataset includes multi-label queries, resulting in different values for single-label and multi-label metrics.

Overall performance is significantly lower on this dataset compared to NitiBench-CCL, likely due to the considerably longer and more nuanced queries in NitiBench-Tax.
%
JINA v3 and BGE-M3 (base, auto-fine-tuned, and human-finetuned) consistently perform among the top, achieving Multi-MRR@10 scores of 0.311, 0.354, 0.345, and 0.333, respectively. 
%
Conversely, JINA v2 and NV-Embed v1 consistently underperform compared to the baseline, potentially because NitiBench-Tax is out-of-distribution relative to their training data, considering the complexity of the query.
%
This is particularly evident with JINA v2, whose Multi-MRR@10 drops dramatically from 0.750 on NitiBench-CCL to 0.091 on NitiBench-Tax.

Similarly, the Human-Finetuned BGE-M3 variants are often outperformed by the base BGE-M3, suggesting different data distributions between NitiBench-CCL and NitiBench-Tax, hindering cross-dataset generalization. 
%
While some models achieve reasonable single-label hit rates, multi-label hit-rate performance is poor across all models. 
%
This, combined with low recall and significantly lower multi-label MRR compared to single-label MRR, indicates that while models can often retrieve some relevant documents, they struggle to retrieve all relevant documents for a given query. 
%
This limitation is critical, as comprehensive legal responses require consideration of all relevant legal sections. 
%
\textbf{Although the proposed pipeline (Human-Finetuned BGE-M3) performs strongly on in-domain data (as seen with NitiBench-CCL), these Tax Case results underscore the critical need for sufficiently diverse in-domain training data, since a narrow domain distribution can lead to inconsistent or contradictory outcomes in real-world settings.}

Despite its lower overall performance, NitiBench-Tax benefits more from increasing the number of top-k retrieved documents compared to NitiBench-CCL. 
%
Its hit rate and recall improve at a faster rate as more documents are retrieved compared to NitiBench-CCL.

\begin{table}[!ht]
\centering
\small
\begin{tabular}{@{}clccccc@{}}
\toprule
\textbf{Top-K} & \textbf{Model}                  & \textbf{HR@k}          & \textbf{Multi HR@k}    & \textbf{Recall@k}      & \textbf{MRR@k}         & \textbf{Multi MRR@k}   \\ \midrule
k=1   & BM25                   & .220          & .080          & .118          & .220          & .118          \\
      & JINA V2                & .140          & .040          & .068          & .140          & .068          \\
      & JINA V3                & .400          & .100          & .203          & .400          & .203          \\
      & NV-Embed V1            & .100          & .020          & .035          & .100          & .035          \\
      & BGE-M3                 & \underline{.500}    & \underline{.140}    & \underline{.269}    & \underline{.500}    & \underline{.269}    \\
      & Human-Finetuned BGE-M3 & .480          & \underline{.140}    & .255          & .480          & .255          \\
      & Auto-Finetuned BGE-M3  & \textbf{.520} & \textbf{.160} & \textbf{.281} & \textbf{.520} & \textbf{.281} \\
      & Cohere                 & .340          & .100          & .179          & .340          & .179          \\ \midrule
k=5   & BM25                   & .480          & .120          & .254          & .318          & .171          \\
      & JINA V2                & .200          & .080          & .114          & .165          & .085          \\
      & JINA V3                & \underline{.720}    & \textbf{.260} & \textbf{.448} & .508          & .297          \\
      & NV-Embed V1            & .200          & .020          & .081          & .126          & .050          \\
      & BGE-M3                 & \underline{.720}    & \underline{.240}    & \underline{.435}    & \underline{.580}    & \textbf{.337} \\
      & Human-Finetuned BGE-M3 & \textbf{.740} & .220          & .411          & .565          & .320          \\
      & Auto-Finetuned BGE-M3  & .700          & .200          & .382          & \textbf{.587} & \underline{.329}    \\
      & Cohere                 & .620          & .200          & .363          & .447          & .256          \\ \midrule
k=10  & BM25                   & .540          & .160          & .320          & .327          & .183          \\
      & JINA V2                & .240          & .100          & .147          & .171          & .091          \\
      & JINA V3                & \textbf{.840} & \underline{.340}    & \underline{.549}    & .524          & .311          \\
      & NV-Embed V1            & .220          & .040          & .097          & .128          & .052          \\
      & BGE-M3                 & \underline{.820}    & \textbf{.360} & \textbf{.555} & \underline{.593}    & \textbf{.354} \\
      & Human-Finetuned BGE-M3 & .800          & .280          & .499          & .574          & .333          \\
      & Auto-Finetuned BGE-M3  & .780          & .260          & .483          & \textbf{.600} & \underline{.345}    \\
      & Cohere                 & .680          & .200          & .414          & .454          & .263          \\ \bottomrule
\end{tabular}
\caption{Retrieval Evaluation Result on NitiBench-Tax with hierarchy-aware chunking. This split contains multiple positives per question.}
\label{table: retrieval_tax}
\end{table}



\subsubsection{LLM}
\label{subsubsec: llm_result}

The evaluation results of the experiments described in \S\ref{subsubsec: llm_setup} are presented in Table~\ref{table: llm_e2e_main_wcx} for NitiBench-CCL and Table~\ref{table: llm_e2e_main_tax} for NitiBench-Tax. 
%
Since experiments in \S\ref{subsubsec: referencer_result} do not provide conclusive results on whether the inclusion of NitiLink is necessary, we also vary the inclusion of NitiLink in this experiment as well.


\begin{table}[!ht]
\centering
\renewcommand{\arraystretch}{1.2}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}lcccccccc@{}}
\toprule
\textbf{Setting} & \textbf{NitiLink} & \textbf{Retriever MRR ($\uparrow$)} & \textbf{Retriever Recall ($\uparrow$)} & \textbf{E2E Recall ($\uparrow$)} & \textbf{E2E Precision ($\uparrow$)} & \textbf{E2E F1 ($\uparrow$)} & \textbf{Coverage ($\uparrow$)} & \textbf{Contradiction ($\downarrow$)} \\ \midrule
\multirow{3}{*}{\texttt{gpt-4o-2024-08-06}} 
& No Ref      & \multirow{3}{*}{0.809} & \multirow{3}{*}{0.938} & 0.880  & \textbf{0.601}  & \textbf{0.714}  & 85.2  & 0.055  \\
& \cellcolor{lightgray}Ref Depth 1 &                      &                     & \cellcolor{lightgray}0.885  & \cellcolor{lightgray}\underline{0.579}  & \cellcolor{lightgray}\underline{0.700}  & \cellcolor{lightgray}86.3  & \cellcolor{lightgray}0.051  \\
& $\Delta$    &                      &                     & \textcolor{darkgreen}{+0.6\%}  & \textcolor{red}{-3.7\%}  & \textcolor{red}{-2.0\%}  & \textcolor{darkgreen}{+1.3\%}  & \textcolor{darkgreen}{-7.3\%}  \\ \midrule
\multirow{3}{*}{\texttt{gemini-1.5-pro-002}} 
& No Ref      & \multirow{3}{*}{0.809} & \multirow{3}{*}{0.938} & 0.892  & 0.512  & 0.651  & 86.5  & 0.048  \\
& \cellcolor{lightgray}Ref Depth 1 &                      &                     & \cellcolor{lightgray}\underline{0.895}  & \cellcolor{lightgray}0.491  & \cellcolor{lightgray}0.634  & \cellcolor{lightgray}87.3  & \cellcolor{lightgray}\underline{0.042}  \\
& $\Delta$    &                      &                     & \textcolor{darkgreen}{+0.3\%}  & \textcolor{red}{-4.1\%}  & \textcolor{red}{-2.6\%}  & \textcolor{darkgreen}{+0.9\%}  & \textcolor{darkgreen}{-12.5\%} \\ \midrule
\multirow{3}{*}{\texttt{claude-3-5-sonnet-20240620}} 
& No Ref      & \multirow{3}{*}{0.809} & \multirow{3}{*}{0.938} & \textbf{0.901} & 0.444  & 0.595  & \textbf{89.7} & \textbf{0.040}  \\ 
& \cellcolor{lightgray}Ref Depth 1 &                      &                     & \cellcolor{lightgray}0.894  & \cellcolor{lightgray}0.443  & \cellcolor{lightgray}0.592  & \cellcolor{lightgray}\underline{89.5} & \cellcolor{lightgray}0.044  \\
& $\Delta$    &                      &                     & \textcolor{red}{-0.8\%} & \textcolor{red}{-0.2\%} & \textcolor{red}{-0.5\%}  & \textcolor{red}{-0.2\%}  & \textcolor{red}{+10.0\%}  \\ \midrule
\multirow{3}{*}{\texttt{typhoon-v2-70b-instruct}} 
& No Ref      & \multirow{3}{*}{0.809} & \multirow{3}{*}{0.938} & 0.862  & 0.537  & 0.662  & 81.2  & 0.076  \\
& \cellcolor{lightgray}Ref Depth 1 &                      &                     & \cellcolor{lightgray}0.845  & \cellcolor{lightgray}0.573  & \cellcolor{lightgray}0.683  & \cellcolor{lightgray}79.9  & \cellcolor{lightgray}0.080  \\
& $\Delta$    &                      &                     & \textcolor{red}{-2.0\%} & \textcolor{darkgreen}{+6.7\%} & \textcolor{darkgreen}{+3.2\%}  & \textcolor{red}{-1.6\%}  & \textcolor{red}{+5.3\%}  \\ \midrule
\multirow{3}{*}{\texttt{typhoon-v2-8b-instruct}} 
& No Ref      & \multirow{3}{*}{0.809} & \multirow{3}{*}{0.938} & 0.775  & 0.387  & 0.516  & 70.8  & 0.134  \\
& \cellcolor{lightgray}Ref Depth 1 &                      &                     & \cellcolor{lightgray}0.718  & \cellcolor{lightgray}0.385  & \cellcolor{lightgray}0.501  & \cellcolor{lightgray}68.5  & \cellcolor{lightgray}0.145  \\
& $\Delta$    &                      &                     & \textcolor{red}{-7.4\%} & \textcolor{red}{-0.5\%}  & \textcolor{red}{-2.9\%}  & \textcolor{red}{-3.3\%}  & \textcolor{red}{+8.2\%}  \\ \bottomrule
\end{tabular}
}
\caption{Effect of LLM configuration on E2E performance on NitiBench-CCL. $\Delta$ values are computed relative to No Ref and normalized to percentage change.}
\label{table: llm_e2e_main_wcx}
\end{table}


\begin{table}[!ht]
\centering
\renewcommand{\arraystretch}{1.2}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}lccccccccc@{}}
\toprule
{\textbf{Setting}} & {\textbf{NitiLink}} 
& {\textbf{Retriever MRR ($\uparrow$)}} 
& {\textbf{Retriever Multi MRR ($\uparrow$)}} 
& {\textbf{Retriever Recall ($\uparrow$)}} 
& {\textbf{E2E Recall ($\uparrow$)}} 
& {\textbf{E2E Precision ($\uparrow$)}} 
& {\textbf{E2E F1 ($\uparrow$)}} 
& {\textbf{Coverage ($\uparrow$)}} 
& {\textbf{Contradiction ($\downarrow$)}} \\ 
\midrule

%---------------- gpt-4o-2024-08-06 ----------------
\multirow{3}{*}{\texttt{gpt-4o-2024-08-06}} 
& No Ref            
  & \multirow{3}{*}{0.574} 
  & \multirow{3}{*}{0.333}  
  & \multirow{3}{*}{0.499}
  & 0.333 
  & \underline{0.640}
  & 0.438 
  & 50.0  
  & \underline{0.46} \\
& \cellcolor{lightgray}Ref Depth 1  
  & 
  & 
  & 
  & \cellcolor{lightgray}0.354 
  & \cellcolor{lightgray}0.630 
  & \cellcolor{lightgray}0.453 
  & \cellcolor{lightgray}45.0 
  & \cellcolor{lightgray}0.52 \\
& $\Delta$           
  & 
  & 
  & 
  & \textcolor{darkgreen}{+6.3\%}
  & \textcolor{red}{-1.6\%}
  & \textcolor{darkgreen}{+3.4\%}
  & \textcolor{red}{-10.0\%}
  & \textcolor{red}{+13.0\%} \\
\midrule

%---------------- gemini-1.5-pro-002 ----------------
\multirow{3}{*}{\texttt{gemini-1.5-pro-002}} 
& No Ref            
  & \multirow{3}{*}{0.574} & \multirow{3}{*}{0.333} & \multirow{3}{*}{0.499}
  & 0.361 & 0.308 & 0.332 
  & 44.0  & 0.48 \\
& \cellcolor{lightgray}Ref Depth 1  
  & 
  & 
  & 
  & \cellcolor{lightgray}0.354 
  & \cellcolor{lightgray}0.347 
  & \cellcolor{lightgray}0.351 
  & \cellcolor{lightgray}45.0 
  & \cellcolor{lightgray}0.48 \\
& $\Delta$           
  & 
  & 
  & 
  & \textcolor{red}{-1.9\%}
  & \textcolor{darkgreen}{+12.7\%}
  & \textcolor{darkgreen}{+5.7\%}
  & \textcolor{darkgreen}{+2.3\%}
  & \textcolor{black}{+0.0\%} \\
\midrule

%---------------- claude-3-5-sonnet-20240620 ----------------
\multirow{3}{*}{\texttt{claude-3-5-sonnet-20240620}} 
& No Ref            
  & \multirow{3}{*}{0.574} & \multirow{3}{*}{0.333} & \multirow{3}{*}{0.499 }
  & \underline{0.389} & 0.554 & \underline{0.457}
  & \underline{51.0}  & \textbf{0.44} \\
& \cellcolor{lightgray}Ref Depth 1  
  & 
  & 
  & 
  & \cellcolor{lightgray}\textbf{0.417} 
  & \cellcolor{lightgray}0.577 
  & \cellcolor{lightgray}\textbf{0.484} 
  & \cellcolor{lightgray}49.0 
  & \cellcolor{lightgray}0.56 \\
& $\Delta$           
  & 
  & 
  & 
  & \textcolor{darkgreen}{+7.2\%}
  & \textcolor{darkgreen}{+4.2\%}
  & \textcolor{darkgreen}{+5.9\%}
  & \textcolor{red}{-3.9\%}
  & \textcolor{red}{+27.3\%} \\
\midrule

%---------------- typhoon-v2-70b-instruct ----------------
\multirow{3}{*}{\texttt{typhoon-v2-70b-instruct}} 
& No Ref            
  & \multirow{3}{*}{0.574} & \multirow{3}{*}{0.333} & \multirow{3}{*}{0.499}
  & 0.326 & \textbf{0.662} & 0.437 
  & 42.0  & 0.58 \\
& \cellcolor{lightgray}Ref Depth 1  
  & 
  & 
  &  
  & \cellcolor{lightgray}0.333 
  & \cellcolor{lightgray}0.453 
  & \cellcolor{lightgray}0.384 
  & \cellcolor{lightgray}\textbf{54.0 }
  & \cellcolor{lightgray}\underline{0.46} \\
& $\Delta$           
  & 
  & 
  & 
  & \textcolor{darkgreen}{+2.1\%}
  & \textcolor{red}{-31.6\%}
  & \textcolor{red}{-12.1\%}
  & \textcolor{darkgreen}{+28.6\%}
  & \textcolor{darkgreen}{-20.7\%} \\
\midrule

%---------------- typhoon-v2-8b-instruct ----------------
\multirow{3}{*}{\texttt{typhoon-v2-8b-instruct}} 
& No Ref            
  & \multirow{3}{*}{0.574} & \multirow{3}{*}{0.333} & \multirow{3}{*}{0.499 }
  & 0.278 & 0.471 & 0.349 
  & 37.0  & 0.60 \\
& \cellcolor{lightgray}Ref Depth 1  
  & 
  & 
  & 
  & \cellcolor{lightgray}0.319 
  & \cellcolor{lightgray}0.561 
  & \cellcolor{lightgray}0.407 
  & \cellcolor{lightgray}35.0 
  & \cellcolor{lightgray}0.54 \\
& $\Delta$           
  & 
  & 
  & 
  & \textcolor{darkgreen}{+14.7\%}
  & \textcolor{darkgreen}{+19.1\%}
  & \textcolor{darkgreen}{+16.6\%}
  & \textcolor{red}{-5.4\%}
  & \textcolor{darkgreen}{-10.0\%} \\
\bottomrule
\end{tabular}
}
\caption{Effect of LLM configuration on E2E performance on NitiBench-Tax. 
\(\Delta\) values are computed relative to No Ref and normalized to percentage change.}
\label{table: llm_e2e_main_tax}
\end{table}


\textbf{NitiBench-CCL} On NitiBench-CCL, \texttt{claude-3.5-sonnet} excels in Coverage, Contradiction, and E2E Recall. 
%
However, the typhoon family of models struggles to match the performance of the closed-source models in this broader Thai legal QA domain. 
%
As seen with NitiBench-Tax, both \texttt{claude-3.5-sonnet} and \texttt{gemini-1.5-pro-002} exhibit low E2E Precision, leading to lower F1 scores compared to \texttt{gpt-4o}. 
%
This underscores a trade-off between recall and precision, particularly for \texttt{claude-3.5-sonnet} and \texttt{gemini-1.5-pro-002}. 
%
The causes of this precision drop are further analyzed in a later section.

\textbf{NitiBench-Tax} The results on NitiBench-Tax demonstrate that \texttt{claude-3.5-sonnet} also achieves top-2 performance across most end-to-end metrics. 
%
Interestingly, \texttt{typhoon-v2-70b-instruct}, an open-sourced model, delivers comparable results and outperforms others on NitiBench-Tax with the highest coverage score and E2E precision. 
%
However, its smaller variant, \texttt{typhoon-v2-8b-instruct}, ranks the lowest among the tested models.f
%`
Despite its limited parameter size, it manages to avoid falling significantly behind, showcasing a reasonable performance given its constraints.

Notably, both \texttt{claude-3.5-sonnet} and \texttt{gemini-1.5-pro-002} exhibit considerably lower E2E precision compared to \texttt{gpt-4o} and \texttt{typhoon-v2-70b-instruct}. This compromises their suitability for precision-critical applications, even though they excel in other areas. Additionally, the NitiLink module fails to consistently enhance performance, with mixed results indicating no definitive advantage in its current configuration.

\subsubsection{E2E results using best setups}
\label{subsubsec: e2e_best_result}

The results from experiments conducted under the four settings described in \S\ref{subsubsec: e2e_best_setup} are presented in Table~\ref{table: main_exp_full}. 
%
Both the Proposed RAG and Naive RAG settings utilize Human-Finetuned BGE-M3 as the retriever, as it demonstrated the best performance in previous experiments (Section~\ref{subsubsec: retriever_result}). 
%
Similarly, Claude 3.5 Sonnet is chosen as the generator based on its superior results in \S\ref{subsubsec: llm_result}. 
%
NitiLink is excluded in both settings due to its inconclusive impact on E2E performance (Section~\ref{subsubsec: referencer_result} and~\ref{subsubsec: llm_result}). 
%
We opted for this choice instead of the opposite because omitting NitiLink yields similar performance while reducing API costs. 
%
The primary distinction between Naive RAG and Proposed RAG lies in their chunking strategies: Naive RAG employs a naive chunking approach, whereas Proposed RAG utilizes hierarchy-aware chunking.

\begin{table}[!ht]
\centering
\renewcommand{\arraystretch}{1.2} % Increase cell height
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}lcccccccc@{}}
\toprule
{\textbf{Setting}} & \textbf{{Retriever MRR} ($\uparrow$)} & \textbf{{Retriever Multi-MRR} ($\uparrow$)} & \textbf{{Retriever Recall} ($\uparrow$)} & \textbf{{Coverage} ($\uparrow$)} & \textbf{{Contradiction} ($\downarrow$)} & \textbf{{E2E Recall} ($\uparrow$)} & \textbf{{E2E Precision} ($\uparrow$)} & \textbf{{E2E F1} ($\uparrow$)} \\ 
\midrule
\multicolumn{9}{c}{\textbf{NitiBench-CCL}} \\ \midrule
Parametric     
 & -     
 & -     
 & -     
 & 60.3     
 & 0.199   
 & 0.188   
 & 0.141   
 & 0.161  
\\
Na\"ive RAG    
 & 0.120 
 & 0.048 
 & 0.062 
 & 77.3     
 & 0.097   
 & 0.745   
 & 0.370   
 & 0.495  
\\
Proposed RAG   
 & 0.809 
 & 0.809 
 & 0.938 
 & \textbf{89.7}     
 & \textbf{0.040}   
 & \textbf{0.901}   
 & \textbf{0.444}   
 & \textbf{0.595}  
\\
\rowcolor{gray!15}
Golden Context 
 & 1.0   
 & 1.0   
 & 1.0   
 & 93.4     
 & 0.034   
 & 0.999   
 & 1.000   
 & 1.000  
\\ 
\midrule
\multicolumn{9}{c}{\textbf{NitiBench-Tax}} \\ \midrule
Parametric     
 & -     
 & -     
 & -     
 & 46.0     
 & 0.480   
 & \textbf{0.458}   
 & \textbf{0.629}   
 & \textbf{0.530}  
\\
Na\"ive RAG    
 & 0.120 
 & 0.048 
 & 0.062 
 & 50.0     
 & 0.460   
 & 0.306   
 & 0.463   
 & 0.368  
\\
Proposed RAG   
 & 0.574 
 & 0.333 
 & 0.499 
 & \textbf{51.0}     
 & \textbf{0.440}   
 & {0.389}   
 & {0.554}   
 & {0.457}  
\\
\rowcolor{gray!15}
Golden Context 
 & 1.0   
 & 1.0   
 & 1.0   
 & 52.0     
 & 0.460   
 & 0.694   
 & 1.000   
 & 0.820  
\\ 
\bottomrule
\end{tabular}
}
\caption{E2E Experiment results on NitiBench-CCL and NitiBench-Tax comparing various RAG setups on Human-Finetuned BGE-M3 retriever and \texttt{claude-3-5-sonnet-20240620} as a LLM.}
\label{table: main_exp_full}
\end{table}



\textbf{NitiBench-Tax} On NitiBench-Tax, the four main settings perform similarly, except for the parametric setting's slightly lower coverage and higher contradiction. Two key observations emerge:

First, the parametric setting achieves the second-highest E2E recall and precision despite lacking a retriever. 
%
To investigate this further, we inspected the cited law section generated by LLM. 
%
Surprisingly, we found that out of 105 law sections cited from LLM parametric knowledge, 58 of them aren't even retrieved by the best retriever. 
%
Among those 58 cited documents, 26 were in the correct law section. 
%
In contrast, only 5 of 101 sections cited by the proposed RAG system are \emph{not} retrieved. 
%
This indicates that retriever performance significantly constrains RAG systems, especially with complex queries like those in NitiBench-Tax. 
%
RAG system generators seem discouraged from using internal knowledge, which might sometimes provide better answers. 
%
Furthermore, the substantial disparity between retriever and E2E recall shows that the LLM often underutilizes relevant retrieved sections, particularly those containing primarily terminology (as discussed in \S\ref{subsec: retriever_re_error_analysis_tax}).
%
% We suspect that the high performance of parametric knowledge could be due to Thai law data or the case data from the revenue department website might be contaminated in the pretraining data since it's publicly available.
%
% However, we didn't further investigate this due to time constraint and we leave this to future works.

Second, Table~\ref{table: main_exp_full} (NitiBench-Tax) shows no clear relationship between E2E citation scores and coverage/contradiction. 
%
This suggests the LLM struggles to apply cited sections correctly in its reasoning, leading to incorrect or erroneously reasoned answers. 
%
This resembles the issue in \S\ref{subsubsec: referencer_result}, where improved retriever recall from NitiLink didn't consistently improve E2E metrics. 
%
Here, increased E2E citations with Claude 3.5 Sonnet don't necessarily improve coverage or contradiction.

\textbf{NitiBench-CCL} For NitiBench-CCL (Table~\ref{table: main_exp_full}), the results are as expected: parametric performs worst, followed by Naive RAG, then Proposed RAG, and finally Golden Context (best).

\subsection{(RQ3) Performance of Long-Context LLM (LCLM)}
\label{subsec: rq3_result}

The evaluation results of the Thai legal QA system based on LCLM are presented in Table~\ref{table: main_exp_sampled} in comparison to the same baselines in Table~\ref{table: main_exp_full}. 
%
The evaluation is conducted on a stratified 20\% sample of NitiBench-CCL and the full NitiBench-Tax. 
%
As detailed in \S\ref{subsec: setup_rq3}, the LCLM system processes all 35 Thai financial laws simultaneously as context, with special tokens inserted to serve as section identifiers. 
%
These tokens enable the LCLM to cite relevant sections explicitly when generating responses to queries.

\begin{table}[!ht]
\centering
\renewcommand{\arraystretch}{1.2} % Increase cell height
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}lcccccccc@{}}
\toprule
\textbf{Setting} & \textbf{Retriever MRR ($\uparrow$)} & \textbf{Retriever Multi-MRR ($\uparrow$)} & \textbf{Retriever Recall ($\uparrow$)} & \textbf{Coverage ($\uparrow$)} & \textbf{Contradiction ($\downarrow$)} & \textbf{E2E Recall ($\uparrow$)} & \textbf{E2E Precision ($\uparrow$)} & \textbf{E2E F1 ($\uparrow$)} \\ 
\midrule
\multicolumn{9}{c}{\textbf{NitiBench-CCL (20\% subsampled)}} \\ \midrule
Parametric     & -     & -     & -     & 60.6  & 0.198 & 0.197 & 0.147 & 0.169 \\
Na\"ive RAG    & 0.549 & 0.549 & 0.649 & 77.7  & 0.092 & 0.740 & 0.379 & 0.501 \\
Proposed RAG   & 0.825 & 0.825 & 0.945 & \textbf{90.1}  & \textbf{0.028} & \textbf{0.920} & 0.453 & 0.607 \\
LCLM (Gemini)  & -     & -     & -     & 83.2  & 0.063 & 0.765 & \textbf{0.514} & \textbf{0.615} \\
\rowcolor{gray!15}
Golden Context & 1.0   & 1.0   & 1.0   & 94.2  & 0.025 & 0.999 & 1.000 & 0.999 \\
\midrule
\multicolumn{9}{c}{\textbf{NitiBench-Tax}} \\ \midrule
Parametric     & -     & -     & -     & 46.0  & 0.480 & \textbf{0.458} & \textbf{0.629} & \textbf{0.530} \\
Na\"ive RAG    & 0.120 & 0.048 & 0.062 & 50.0  & 0.460 & 0.306 & 0.463 & 0.368 \\
Proposed RAG   & 0.574 & 0.333 & 0.499 & \textbf{51.0}  & \textbf{0.440} & 0.389 & 0.554 & 0.457 \\
LCLM (Gemini)  & -     & -     & -     & 36.0  & 0.620 & 0.410 & 0.484 & 0.444 \\
\rowcolor{gray!15}
Golden Context & 1.0   & 1.0   & 1.0   & 52.0  & 0.460 & 0.694 & 1.000 & 0.820 \\ 
\bottomrule
\end{tabular}
}
\caption{Experiment results on sampled NitiBench-CCL and full NitiBench-Tax. In the LCLM setup, we used the Gemini without retriever section, where full legislation books were parsed as a context.}
\label{table: main_exp_sampled}
\end{table}



From Table~\ref{table: main_exp_sampled}, the LCLM-based system performs comparably to the parametric setting on NitiBench-Tax and to the Naive RAG system on NitiBench-CCL. 
%
This performance gap may stem from degradation when processing extremely long contexts (1.2 million tokens). 
%
Regardless of the exact cause, the results suggest that while an LCLM-based Thai legal QA system is feasible, its performance remains significantly behind RAG-based counterparts, highlighting areas for further improvement.

Apart from utilizing LCLM to process the legislations and respond directly to the queries, we also explored using it as a retriever. 
%
As stated in \S\ref{subsec: setup_rq3}, Gemini 1.5 Pro is provided with all 35 legislations and tasked to retrieve 20 relevant laws given a query. 
%
This experiment is also conducted on the same sample of NitiBench-CCL as the previous experiment and the full NitiBench-Tax. 
%
The results are shown in Table~\ref{table: retrieval_wcx_lclm} and~\ref{table: retrieval_tax_lclm}.

\begin{table}[!ht]
\centering
\small
\begin{tabular}{@{}clcc@{}}
\toprule
\textbf{Top-K} & \textbf{Model} & \textbf{HR/Recall@k} & \textbf{MRR@k} \\ \midrule
\multirow{9}{*}{k=1} 
  & BM25                   & .480           & .480 \\
  & JINA V2                & .698           & .698 \\
  & JINA V3                & .601           & .601 \\
  & NV-Embed V1            & .496           & .496 \\
  & BGE-M3                 & .708           & .708 \\
  & Human-Finetuned BGE-M3 & \textbf{.757}  & \textbf{.757} \\
  & Auto-Finetuned BGE-M3  & \underline{.741} & \underline{.741} \\
  & Cohere                 & .707           & .707 \\
  & LCLM-as-a-retriever (Gemini)                   & .590           & .590 \\ \midrule
\multirow{9}{*}{k=5} 
  & BM25                   & .663           & .549 \\
  & JINA V2                & .858           & .761 \\
  & JINA V3                & .828           & .693 \\
  & NV-Embed V1            & .711           & .585 \\
  & BGE-M3                 & \underline{.888} & .779 \\
  & Human-Finetuned BGE-M3 & \textbf{.909}  & \textbf{.819} \\
  & Auto-Finetuned BGE-M3  & \textbf{.909}  & \underline{.807} \\
  & Cohere                 & .867           & .772 \\
  & LCLM-as-a-retriever (Gemini)                   & .776           & .667 \\ \midrule
\multirow{9}{*}{k=10} 
  & BM25                   & .733           & .559 \\
  & JINA V2                & .891           & .766 \\
  & JINA V3                & .878           & .700 \\
  & NV-Embed V1            & .794           & .596 \\
  & BGE-M3                 & .926           & .784 \\
  & Human-Finetuned BGE-M3 & \textbf{.945}  & \textbf{.824} \\
  & Auto-Finetuned BGE-M3  & \underline{.941} & \underline{.812} \\
  & Cohere                 & .913           & .778 \\
  & LCLM-as-a-retriever (Gemini)                   & .807           & .671 \\ \bottomrule
\end{tabular}
\caption{Retrieval Evaluation Result on a 20\% subset of NitiBench-CCL with hierarchy-aware chunking with Long-Context Retriever. Since the test split of NitiBench-CCL is single labeled, duplicated metrics (HR/Recall and MRR) have been collapsed.}
\label{table: retrieval_wcx_lclm}
\end{table}



\begin{table}[!ht]
\centering
\small
\begin{tabular}{@{}clccccc@{}}
\toprule
\textbf{Top-K} & \textbf{Model}                  & \textbf{HR@k}          & \textbf{Multi HR@k}    & \textbf{Recall@k}      & \textbf{MRR@k}         & \textbf{Multi MRR@k}   \\ \midrule
k=1   & BM25                   & .220          & .080          & .118          & .220          & .118          \\
      & JINA V2                & .140          & .040          & .068          & .140          & .068          \\
      & JINA V3                & .400          & .100          & .203          & .400          & .203          \\
      & NV-Embed V1            & .100          & .020          & .035          & .100          & .035          \\
      & BGE-M3                 & \underline{.500}    & \underline{.140}    & \underline{.269}    & \underline{.500}    & \underline{.269}    \\
      & Human-Finetuned BGE-M3 & .480          & \underline{.140}    & .255          & .480          & .255          \\
      & Auto-Finetuned BGE-M3  & \textbf{.520} & \textbf{.160} & \textbf{.281} & \textbf{.520} & \textbf{.281} \\
      & Cohere                 & .340          & .100          & .179          & .340          & .179          \\
      & LCLM                   & .480          & .120          & .227          & .480          & .227          \\ \midrule
k=5   & BM25                   & .480          & .120          & .254          & .318          & .171          \\
      & JINA V2                & .200          & .080          & .114          & .165          & .085          \\
      & JINA V3                & .720          & \underline{.260}    & \underline{.448}    & .508          & .297          \\
      & NV-Embed V1            & .200          & .020          & .081          & .126          & .050          \\
      & BGE-M3                 & .720          & .240          & .435          & \underline{.580}    & \underline{.337}    \\
      & Human-Finetuned BGE-M3 & \underline{.740}    & .220          & .411          & .565          & .320          \\
      & Auto-Finetuned BGE-M3  & .700          & .200          & .382          & \textbf{.587} & \underline{.329}    \\
      & Cohere                 & .620          & .200          & .363          & .447          & .256          \\
      & LCLM-as-a-retriever (Gemini)                   & \textbf{.760} & \textbf{.320} & \textbf{.515} & \textbf{.587} & \textbf{.370} \\ \midrule
k=10  & BM25                   & .540          & .160          & .320          & .327          & .183          \\
      & JINA V2                & .240          & .100          & .147          & .171          & .091          \\
      & JINA V3                & \textbf{.840} & \underline{.340}    & .549          & .524          & .311          \\
      & NV-Embed V1            & .220          & .040          & .097          & .128          & .052          \\
      & BGE-M3                 & \underline{.820}    & \textbf{.360} & \underline{.555}    & \underline{.593}    & \underline{.354}    \\
      & Human-Finetuned BGE-M3 & .800          & .280          & .499          & .574          & .333          \\
      & Auto-Finetuned BGE-M3  & .780          & .260          & .483          & \textbf{.600} & \underline{.345}    \\
      & Cohere                 & .680          & .200          & .414          & .454          & .263          \\
      & LCLM-as-a-retriever (Gemini)                   & .780          & \textbf{.360} & \textbf{.566} & .590          & \textbf{.379} \\ \bottomrule
\end{tabular}
\caption{Retrieval Evaluation Result on NitiBench-Tax with hierarchy-aware chunking. This split contains multiple positives per question.}
\label{table: retrieval_tax_lclm}
\end{table}


The results indicate that the LCLM retriever performs comparably to embedding-based retrievers on NitiBench-Tax, likely due to its superior reasoning capabilities. 
%
However, a noticeable performance gap exists when compared to the best retriever on NitiBench-CCL.
%
Additionally, increasing the number of retrieved documents for the LCLM yields minimal performance improvements relative to other models. 
%
We hypothesize that this limited gain is a result of LLMs' next-token prediction mechanism, which may hinder their ability to effectively retrieve and output relevant laws when those laws are distant from the query context or when the model attempts to generate relevant laws ranked lower in the retrieval order.










