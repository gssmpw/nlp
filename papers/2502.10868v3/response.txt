\section{Literature Review}
\label{sec:lit_review}
% Legal lit review: As decided, should start with previous benchmarking of RAG and what's difference from our benchmark, Thai legal structure - explain why it's important to consider these characteristics, RAG in legal domain - explain previous works, RAG vs LCLMs - explain why LCLM is rising and show previous attempts on using LCLM in other or legal domain

In this section, we review prior work on benchmarking QA systems, the application of RAG in the legal domain, and efforts to implement Long-Context LLMs (LCLMs) in QA systems. 
%
Additionally, given the unique characteristics of the Thai legal domain, we outline the structure of the Thai legal system to highlight the specific challenges associated with developing a Thai legal QA system.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Benchmarking}
\label{subsec: benchmarking}

% Still feels like it's missing a lot of things. Review with care 5555
Benchmarking plays a critical role in developing legal QA systems by providing standardized tasks and metrics to evaluate performance. 
%
It facilitates the comparison of different frameworks under consistent conditions and helps identify performance bottlenecks.

Popular benchmarks for legal QA in English include LexGlue, "LexGlue"__LegalBench, "LegalBench"_Dahl et al., "Quantifying Hallucinations in Legal Knowledge Models"_Magesh et al., "Assessing the Correctness and Groundedness of Responses in Commercial AI Legal Research Tools"_RAGAS, "A Reference-Free Retrieval-Augmented Generation Evaluation Framework"_Laban et al., "A Framework for LLMs to Identify Relevant Documents from Context".

Dahl et al. introduce a Hallucination QA dataset to quantify hallucinations in LLMs for legal knowledge. 
%
Tasks include reference-based questions (e.g., verifying case existence) and reference-free questions (e.g., identifying the central holding of a case). 
%
The hallucination rate is measured using binary classification metrics for reference-based tasks and contradictions in answers across various temperatures for reference-free tasks.

Although these works provide structured evaluations, many focus on specific sub-tasks rather than free-form QA, the primary goal of legal QA systems. 
%
Notable efforts for free-form legal QA evaluation include Magesh et al., "Assessing the Correctness and Groundedness of Responses in Commercial AI Legal Research Tools"__RAGAS, "A Reference-Free Retrieval-Augmented Generation Evaluation Framework"__Laban et al., "A Framework for LLMs to Identify Relevant Documents from Context".

% Before this, should review Thai legal QA corpus as well?
We conclude that a comprehensive legal QA benchmark should prioritize free-form QA tasks with clearly defined metrics, including:

\begin{itemize}
    \item{Correctness: The accuracy and relevance of the response.}
    \item{Contradiction: A measure of hallucination, defined by answers contradicting references.}
    \item{LLM Citation Quality: The ability of an LLM to filter and select relevant contexts from retrieved information.}
\end{itemize}

CLERC addresses the lack of publicly available legal datasets by constructing a pipeline built on the Caselaw Access Project (CAP)\footnote{\url{https://case.law/}} corpus for RAG evaluations. 
%
It introduces realistic tasks, novel queries, and tailored metrics like BLEU, ROUGE, Citation Recall, Precision, and False Positive Rate.
%
Their work serves as a model for building datasets in other languages that also lack the publicly available legal QA benchmark, such as Thai.

% Should review multi-label retrieval metrics?
In addition to end-to-end evaluations, assessing the retriever component of RAG-based systems is crucial for identifying system gaps. 
%
Traditional information retrieval metrics (Precision, Recall, F1-score) fall short in multi-label classifications required by Thai law, where multiple relevant legal sections must be retrieved. 
%
No prior work has developed multi-label variants of these metrics for Thai legal QA, emphasizing the need for tailored benchmarks in this context. 
%
Hence, a robust legal QA benchmark should also evaluate retriever performance for multi-label legal texts.

% Explain overall Thai legal framework, hightlight importance of Acts/Codes
\subsection{Thai Legal System}
\label{subsec: thai_legal}

Thailand's legal framework operates within a democratic, constitutional monarchy, adhering to a hierarchical structure. 
%
Lower-hierarchical laws must not be contradictory to the higher ones__Pongsawat et al., "Comparative Study of Thai and English Laws on Copyright Infringement"__. 
%
This hierarchy encompasses seven distinct levels, namely: (1) the Constitution, (2) Organic Laws, (3) Acts and Codes, (4) Emergency Decrees, (5) Royal Decrees, (6) Ministerial Regulations, and (7) Local Ordinances.

Within the Thai legal hierarchy, \textbf{Acts} holds a pivotal position, ranking immediately after the Constitution and Organic Laws. 
%
Acts represent primary legislation passed through a rigorous legislative process, ultimately requiring Royal Assent for enactment. 
%
While most laws are enacted as individual Acts, some areas of law are governed by comprehensive Codes, such as the \textit{Civil and Commercial Code}_\textit{Thailand's Civil Procedure Code}.

% intro to rags
RAG contains multiple components, the end-to-end performance relies on multiple design choices such as the embedding model used, the document chunking method, the number of documents to be retrieved, and so forth; the question arises as to whether we can solve this tradeoff among these choices. 
%
Long Context LLMs (LCLMs) have been an alternative to this problem as well__Google Gemini 1.5, "Google Gemini: A Large-Scale Pre-Trained Language Model"_.

% highlight several studies comparing these two approaches
Recent works have tried to conduct a qualitative study about the tradeoff between LCLM and RAG. 
%
LongBench, "A Benchmark for Long-Context Question Answering"__L-eval, "A Benchmark for Evaluating Long-Context Models"__ provide a more sophisticated long context benchmark compared to Needle-in-the-haystack. 
%
LOFT, "Overcoming the Curse of Large Document Length with LOFT"__Self-route, "Self-Routing Transformer for Question Answering"_NEPAQuAD1.0, "NEPAQuAD: A Benchmark for Evaluating Long-Context Models on Question Answering Tasks"_Summary of a haystack, "A Survey of Long-Context Models and Benchmarks for Question Answering"__ measures LCLMs performance compared to RAG as a baseline. 
%
Findings from these works suggested that although many proprietary models such as GPT4o, "GPT-4 Oracle: A Large-Scale Language Model"__Claude 3.5 Sonnet, "Claude 3.5: A State-of-the-Art Language Model for Natural Language Processing Tasks"_ are able to process up to over 128K and 200K context length respectively, under long context setup, the performance still can't match that of Gemini-1.5-pro__. 
%
____ also found that SQL reasoning is the only task RAG wins LCLM, and ____ points out that the LCLMs' performance is still subpar to humans in terms of both citation correctness and answer coverage.

% not sure that this paragraph should be here or problem statement (or both)?
% tan-> this should be fine as a loose end problem highlighted in this background session
%
Although the literature extensively explores the comparative performance of RAG and LCLMs across various domains, a crucial area remains unexplored: their application and evaluation within the legal domain. 
%
Numerous studies have meticulously investigated the strengths and weaknesses of RAG and LCLMs in handling complex tasks like question answering and reasoning. 
%
However, these investigations typically focus on general-purpose datasets and benchmarks. 
%
To date, no research has directly compared the efficacy of RAG and LCLMs in tackling the unique challenges inherent to the legal field. 
%
This gap underscores the need for future research to delve into the comparative performance of these paradigms, specifically within the legal domain, ultimately guiding the development of more effective and specialized legal NLP applications.