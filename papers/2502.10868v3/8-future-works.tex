\section{Future Works}
% What do I write in future works -> 1. On Limitation of WCX data 2. Reasoning 
\subsection{Limitations of WangchanX-Legal-ThaiCCL and NitiBench-CCL}
\label{subsec: wcx_limitation}

Our proposed dataset construction pipeline for WangchanX-Legal-ThaiCCL training data consists of two distinct processes: a semi-synthetic approach with human quality control (QC) for the training split and a fully human-annotated approach for the test split. 
%
This design choice helps manage costs, as creating a purely human-annotated training set is expensive. 
%
However, our test data construction pipeline still has some limitations.

\textbf{Ambiguous Queries from Single-Section Sampling}
%
This issue arises from the way dataset entries are constructed. 
%
Specifically, a single section is sampled from one of the 21 available legislation in the test set, and an expert annotator formulates a question and answer based on that section. 
%
However, annotators often focus solely on the sampled section's content without considering the specific conditions that make it unique compared to other similar sections on the same topic. 
%
This results in queries that are ambiguous and could be associated with multiple sections simultaneously. 
%
As discussed in \S\ref{subsec: llm_error_analysis}, this ambiguity leads to situations where the LLM incorporates multiple similar sections as relevant, even when the query was intended to target a single section. 

To address this issue, an additional step could be introduced in the pipeline. 
%
Instead of showing annotators only the targeted section, we could use an off-the-shelf retriever model or domain knowledge to retrieve similar sections. 
%
The annotators would then be asked to craft questions that are more specific to the targeted law and ensure that they are not easily confused with retrieved sections.

\textbf{The absence of truly multi-label queries} 
%
This applies to both the test (NitiBench-CCL) and training sets (of WangchanX-Legal-ThaiCCL). 
%
In the training set, multi-label ground truth is generated by allowing annotators to select relevant sections from retrieved documents. 
%
However, since the queries are initially derived from a single section, they are not inherently multi-label. 
%
As a result, our system evaluation lacks a mechanism to assess performance in scenarios requiring reasoning across multiple laws. 

To partially mitigate this, we constructed NitiBench-Tax, which includes genuinely multi-label queries.
%
However, it would be beneficial to refine the pipeline further by enabling annotators to create questions based on multiple legal provisions that are naturally grouped using heuristics. 
%
This adjustment would improve the dataset's ability to evaluate models in scenarios that require reasoning across multiple laws.

\textbf{Queries lack naturalness and differ from how everyday users might phrase questions in a Thai legal QA system} 
%
There are two potential approaches to address this issue.
%
One option is to implement an additional step in the system that transforms user queries into a style more aligned with NitiBench-CCL. 
%
Alternatively, the dataset construction pipeline could be adjusted to encourage annotators to use more informal language, or an LLM could be used to refine the annotators' phrasing to better reflect natural user input.

\subsection{Reasoning Evaluation}
\label{subsec: reasoning_evaluation}

Beyond the Coverage, Contradiction, and Citation scores proposed in our benchmark evaluation—measuring how well the generated answer covers the ground truth, whether it contradicts it, and how accurately it cites relevant sections—another crucial aspect of Legal QA evaluation is legal reasoning. 
%
Legal reasoning differs from general reasoning as it operates within a structured legal framework, relying on authoritative sources such as legislation and precedents while requiring precise interpretation of legal texts.
%
Unlike general reasoning, which is flexible and unconstrained, legal reasoning demands strict adherence to legal principles. 
%
Given its critical role in Legal QA, where the system's reasoning process is as important as the final answer, incorporating legal reasoning evaluation into the assessment framework would provide a more comprehensive measure of system performance.

% Review work -> However these are all works for reasoning in LLM -> Applying in Thai legal domain should require domain knowledge and concretize what would constitue a good reasoning process
Several studies have explored reasoning evaluation in LLMs.
%
ROSCOE \cite{roscoe} proposed a suite of metrics to assess reasoning chains, both with and without a gold standard reference. 
%
These metrics are categorized into four perspectives:

\begin{enumerate}
    \item \textbf{Semantic Alignment}: Measures how well a reasoning step is supported by the given context, primarily using cosine similarity.
    %
    \item \textbf{Semantic Similarity}: Similar to Semantic Alignment but evaluates the reasoning process as a whole rather than individual steps.
    %
    \item \textbf{Logical Inference}: Assesses logical fallacies between reasoning steps using Natural Language Inference (NLI) models.
    %
    \item \textbf{Language Coherence}: Evaluates the fluency and coherence of the reasoning chain using perplexity scores.
\end{enumerate}

RECEVAL \cite{receval} further refines reasoning evaluation by defining two key qualities of a good reasoning step: 

\begin{itemize}
    \item \textbf{Correctness}: Ensuring that reasoning steps do not contradict themselves or previous steps.
    %
    \item \textbf{Informativeness}: Ensuring that each step introduces new information contributing to the final answer.
\end{itemize}

This approach decomposes reasoning steps into atomic premise and conclusion statements. 
%
Correctness is measured using NLI models or Pointwise $\nu$-Information (PVI), which assesses entailment probability or the likelihood difference of generating conclusions with and without premises. 
%
Informativeness is measured by comparing PVI scores of reasoning chains with and without the current step.

While these methods rely on manually defined evaluation criteria, LLM Reasoner \cite{llmreasoners} automates this process by leveraging LLMs to categorize observed reasoning errors into evaluation criteria. 
%
These criteria are then used as prompts for automatic reasoning chain evaluation. 
%
This approach eliminates the need for manual criterion construction but requires training data with gold-standard reasoning chains.

Despite these advancements, reasoning evaluation for LLMs remains an ongoing challenge, particularly in the Thai legal domain. 
%
Key obstacles include defining what constitutes a good legal reasoning process and acquiring datasets that require complex legal reasoning beyond simple lookup-based answers.


