\section{Literature Review}
\label{sec:lit_review}
% Legal lit review: As decided, should start with previous benchmarking of RAG and what's difference from our benchmark, Thai legal structure - explain why it's important to consider these characteristics, RAG in legal domain - explain previous works, RAG vs LCLMs - explain why LCLM is rising and show previous attempts on using LCLM in other or legal domain

In this section, we review prior work on benchmarking QA systems, the application of RAG in the legal domain, and efforts to implement Long-Context LLMs (LCLMs) in QA systems. 
%
Additionally, given the unique characteristics of the Thai legal domain, we outline the structure of the Thai legal system to highlight the specific challenges associated with developing a Thai legal QA system.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Benchmarking}
\label{subsec: benchmarking}

% Still feels like it's missing a lot of things. Review with care 5555
Benchmarking plays a critical role in developing legal QA systems by providing standardized tasks and metrics to evaluate performance. 
%
It facilitates the comparison of different frameworks under consistent conditions and helps identify performance bottlenecks.

Popular benchmarks for legal QA in English include LexGlue____, a collection of datasets for tasks such as court opinion classification and law violation identification. 
%
Similarly, LegalBench____ contains tasks for evaluating legal reasoning in large language models (LLMs), including contract natural language inference (NLI) and determining whether a statement constitutes hearsay.

Dahl et al.____ introduce a Hallucination QA dataset to quantify hallucinations in LLMs for legal knowledge. 
%
Tasks include reference-based questions (e.g., verifying case existence) and reference-free questions (e.g., identifying the central holding of a case). 
%
The hallucination rate is measured using binary classification metrics for reference-based tasks and contradictions in answers across various temperatures for reference-free tasks.

Although these works provide structured evaluations, many focus on specific sub-tasks rather than free-form QA, the primary goal of legal QA systems. 
%
Notable efforts for free-form legal QA evaluation include Magesh et al.____, which uses LLM-as-a-judge metrics to assess the correctness and groundedness of responses in commercial AI legal research tools. 
%
Similarly, RAGAS____ offers a reference-free retrieval-augmented generation (RAG) evaluation framework, measuring Faithfulness, Answer Relevance, and Context Relevance. 
%
Laban et al.____ propose a framework where LLMs identify relevant documents from context—a critical task in RAG-based QA systems requiring models to digest retrieved information.

% Before this, should review Thai legal QA corpus as well?
We conclude that a comprehensive legal QA benchmark should prioritize free-form QA tasks with clearly defined metrics, including:

\begin{itemize}
    \item{Correctness: The accuracy and relevance of the response.}
    \item{Contradiction: A measure of hallucination, defined by answers contradicting references.}
    \item{LLM Citation Quality: The ability of an LLM to filter and select relevant contexts from retrieved information.}
\end{itemize}

CLERC____ addresses the lack of publicly available legal datasets by constructing a pipeline built on the Caselaw Access Project (CAP)\footnote{\url{https://case.law/}} corpus for RAG evaluations. 
%
It introduces realistic tasks, novel queries, and tailored metrics like BLEU, ROUGE, Citation Recall, Precision, and False Positive Rate.
%
Their work serves as a model for building datasets in other languages that also lack the publicly available legal QA benchmark, such as Thai.

% Should review multi-label retrieval metrics?
In addition to end-to-end evaluations, assessing the retriever component of RAG-based systems is crucial for identifying system gaps. 
%
Traditional information retrieval metrics (Precision, Recall, F1-score) fall short in multi-label classifications required by Thai law, where multiple relevant legal sections must be retrieved. 
%
No prior work has developed multi-label variants of these metrics for Thai legal QA, emphasizing the need for tailored benchmarks in this context. 
%
Hence, a robust legal QA benchmark should also evaluate retriever performance for multi-label legal texts.

% Explain overall Thai legal framework, hightlight importance of Acts/Codes
\subsection{Thai Legal System}
\label{subsec: thai_legal}

Thailand's legal framework operates within a democratic, constitutional monarchy, adhering to a hierarchical structure. 
%
Lower-hierarchical laws must not be contradictory to the higher ones____. 
%
This hierarchy encompasses seven distinct levels, namely: (1) the Constitution, (2) Organic Laws, (3) Acts and Codes, (4) Emergency Decrees, (5) Royal Decrees, (6) Ministerial Regulations, and (7) Local Ordinances.

Within the Thai legal hierarchy, \textbf{Acts} holds a pivotal position, ranking immediately after the Constitution and Organic Laws. 
%
Acts represent primary legislation passed through a rigorous legislative process, ultimately requiring Royal Assent for enactment. 
%
While most laws are enacted as individual Acts, some areas of law are governed by comprehensive Codes, such as the \textit{Civil and Commercial Code} or the \textit{Criminal Code}, which are structured compilations of related legal provisions.

Acts and Codes follow a multi-tiered structure, organized into levels including Book, Title, Chapter, Division, Section, Subsection, and Clause. 
%
Among these, \textbf{Section} level is the most fundamental unit, with each section articulating a specific legal rule or principle.

This granular structure serves a practical purpose, enabling legal professionals to efficiently navigate extensive codes, locate specific provisions, and understand their context within the broader legal framework. 
%
However, this complexity also raises challenges for the RAG framework. 
%
Specifically, it prompts the question of how to segment legislative documents in a manner that preserves the hierarchical structure while ensuring that each segment contains a meaningful and complete unit of the law.

Another notable characteristic of Thai law is the frequent use of inter-section references within legal texts. 
%
Sections often refer to other sections within the same legislation or in different legal documents.

Consider the following excerpt from the Criminal Code:
\begin{quote}
    \textit{Section 260: Whoever uses, sells, offers for sale, exchanges, or offers to exchange a ticket arising from an act as provided for in Section 258 or Section 259 shall be punished with imprisonment...}%____
    \label{quote: inter_ref}
\end{quote}

In this example, understanding Section 260 requires comprehension of the content of Sections 258 and 259, which are not provided within the immediate text. 
%
This inter-section referencing presents a significant challenge for traditional RAG systems, raising questions about how to handle referenced sections. 
%
Specifically, the design decision must consider whether these referenced sections can be automatically retrieved and augmented into the primary retrieved content to ensure a comprehensive understanding of the law.

From this understanding of the unique characteristics of the Thai legal system, we can see how these features contribute to the challenges of implementing LLMs in this domain. 
%
Specifically, it highlights the need to consider incorporating structural knowledge when segmenting legislation into chunks within the RAG framework, as this could help preserve the original content and enhance comprehensibility for retriever models. 
%
Moreover, given that Thai legislation frequently contains cross-references between sections, it raises an important question: how significant would the impact be of including nested sections as additional context for LLMs?  


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{RAG in legal practice}

{
RAG ____ is an approach designed to enhance the performance of a large language model (LLM) by using a separate retrieval model to fetch relevant documents, which are then used as a context, providing the necessary knowledge for the LLM to answer the question.

This section highlights several works applying RAG in legal practice.
}

\label{sec:raginlegal}
CBR-RAG____ is a framework that integrates Case-Based Reasoning (CBR) into RAG for answering legal questions.
%
CBR-RAG augments the original LLM query with contextually relevant cases retrieved using CBR’s indexing vocabulary and similarity knowledge containers, creating a richer prompt for the LLM. 
%
The authors evaluate CBR-RAG with different embeddings (BERT____, LegalBERT____, AnglEBERT____) and retrieval methods (intra, inter, hybrid) on the legal question-answering task. 
%
Their results demonstrate that CBR-RAG significantly improves the quality of generated answers, particularly with AnglEBERT and hybrid embedding retrieval, highlighting the effectiveness of this approach for knowledge-intensive tasks.

Ajmi____ offers a compelling exploration of how AI can bridge the "justice gap" by providing accessible legal information to those who lack traditional legal assistance. 
%
Their paper highlights the limitations of LLMs and advocates for the implementation of RAG to enhance accuracy and effectiveness. 
%
Drawing from a real-world implementation in the Nevada court system, this work demonstrates the feasibility and potential impact of RAG-assisted chatbots while maintaining a balanced perspective by acknowledging the limitations of AI and emphasizing the continuing need for human legal professionals.

Several commercial solutions also use RAG to power their legal assistant product.
%
For example, Lexis+ AI____ and Westlaw____ use RAG to make sure that the assistant response contains a properly cited legal document. 
%
Some localized legal-based assistant solutions, like Thanoy____, also use RAG on Thai legal documents to create a chatbot. 
%
{Even though some of the aforementioned products claimed to mitigate hallucination problems via RAG, ____ showed that hallucination still persists in some challenging cases.}


\subsection{RAG vs Long-Context LLMs}

% intro to long context llm as alternative to RAG
% \textcolor{blue}
{
Apart from using RAG to mitigate LLM hallucination by retrieving relevant documents as context, Long Context LLMs (LCLMs) have been an alternative to this problem as well____. 
%
Since RAG contains multiple components, the end-to-end performance relies on multiple design choices such as the embedding model used, the document chunking method, the number of documents to be retrieved, and so forth; the question arises as to whether we can solve this tradeoff among these choices. 
%
Long context is proposed as an alternative to RAG, which removes the necessity for tools such as a retrieval model or reranking model where the whole documents are parsed as a context to the prompt without any need for any retrieval module.
%
Google Gemini 1.5 ____ was the first model that introduced a very long context length support of up to 1M tokens in the flash version and 2M tokens in the pro version. 

% While there're also several attempts to expand the model context length for long context supports ____. 
}

% highlight several studies comparing these two approaches
% \textcolor{blue}
{
Recent works have tried to conduct a qualitative study about the tradeoff between LCLM and RAG. 
%
____ introduced a pressure test of the long context by inserting the context (needle) inside a very long document and then asking the model to answer the question based on the inserted context. 
%
However, this work is still limited to evaluating LCLM solely under naive assumptions. 
%
LongBench____ and L-eval____ provide a more sophisticated long context benchmark compared to Needle-in-the-haystack. 
%
LOFT____, Self-route____, NEPAQuAD1.0____, and Summary of a haystack____ measures LCLMs performance compared to RAG as a baseline. 
%
Findings from these works suggested that although many proprietary models such as GPT4o____ and Claude 3.5 Sonnet____ are able to process up to over 128K and 200K context length respectively, under long context setup, the performance still can't match that of Gemini-1.5-pro____. 
%
____ also found that SQL reasoning is the only task RAG wins LCLM, and ____ points out that the LCLMs' performance is still subpar to humans in terms of both citation correctness and answer coverage. 
}
% not sure that this paragraph should be here or problem statement (or both)?
% tan-> this should be fine as a loose end problem highlighted in this background session
%
Although the literature extensively explores the comparative performance of RAG and LCLMs across various domains, a crucial area remains unexplored: their application and evaluation within the legal domain. 
%
Numerous studies have meticulously investigated the strengths and weaknesses of RAG and LCLMs in handling complex tasks like question answering and reasoning. 
%
However, these investigations typically focus on general-purpose datasets and benchmarks. 
%
To date, no research has directly compared the efficacy of RAG and LCLMs in tackling the unique challenges inherent to the legal field. 
%
This gap underscores the need for future research to delve into the comparative performance of these paradigms, specifically within the legal domain, ultimately guiding the development of more effective and specialized legal NLP applications.