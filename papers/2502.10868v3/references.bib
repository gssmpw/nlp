%Introduction References
@misc{ailawyerLawyerYour,
	author = {ailawyer},
	title = {{A}{I} {L}awyer | {Y}our personal {A}{I} legal assistant},
	howpublished = {ailawyer.pro},
        url = {https://ailawyer.pro/},
	year = {},
	note = {[Accessed 14-01-2025]},
}

@misc{asklegalAskLegalbotLegal,
	author = {asklegal.bot},
	title = {{A}sk{L}egal.bot - {A}{I} {L}egal {H}elp},
	howpublished = {asklegal.bot},
        url = {https://asklegal.bot/},
	year = {},
	note = {[Accessed 14-01-2025]},
}

%Evaluation references
@misc{laban2024summaryhaystackchallengelongcontext,
      title={Summary of a Haystack: A Challenge to Long-Context LLMs and RAG Systems}, 
      author={Philippe Laban and Alexander R. Fabbri and Caiming Xiong and Chien-Sheng Wu},
      year={2024},
      eprint={2407.01370},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2407.01370}, 
}

@article{Dahl_2024,
   title={Large Legal Fictions: Profiling Legal Hallucinations in Large Language Models},
   volume={16},
   ISSN={1946-5319},
   url={http://dx.doi.org/10.1093/jla/laae003},
   DOI={10.1093/jla/laae003},
   number={1},
   journal={Journal of Legal Analysis},
   publisher={Oxford University Press (OUP)},
   author={Dahl, Matthew and Magesh, Varun and Suzgun, Mirac and Ho, Daniel E},
   year={2024},
   month=jan, pages={64–93} }

@misc{phan2024ragvslongcontext,
      title={RAG vs. Long Context: Examining Frontier Large Language Models for Environmental Review Document Comprehension}, 
      author={Hung Phan and Anurag Acharya and Sarthak Chaturvedi and Shivam Sharma and Mike Parker and Dan Nally and Ali Jannesari and Karl Pazdernik and Mahantesh Halappanavar and Sai Munikoti and Sameera Horawalavithana},
      year={2024},
      eprint={2407.07321},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2407.07321}, 
}

@misc{es2023ragasautomatedevaluationretrieval,
      title={RAGAS: Automated Evaluation of Retrieval Augmented Generation}, 
      author={Shahul Es and Jithin James and Luis Espinosa-Anke and Steven Schockaert},
      year={2023},
      eprint={2309.15217},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2309.15217}, 
}

@misc{gemini1.5,
      title={Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context}, 
      author={Machel Reid and Nikolay Savinov and Denis Teplyashin and Dmitry Lepikhin and Timothy Lillicrap and Jean-baptiste Alayrac and Radu Soricut and Angeliki Lazaridou and Orhan Firat and Julian Schrittwieser and et al.},
      year={2024},
      eprint={2403.05530},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2403.05530}, 
}

@misc{lee2024longcontextlanguagemodelssubsume,
      title={Can Long-Context Language Models Subsume Retrieval, RAG, SQL, and More?}, 
      author={Jinhyuk Lee and Anthony Chen and Zhuyun Dai and Dheeru Dua and Devendra Singh Sachan and Michael Boratko and Yi Luan and Sébastien M. R. Arnold and Vincent Perot and Siddharth Dalmia and Hexiang Hu and Xudong Lin and Panupong Pasupat and Aida Amini and Jeremy R. Cole and Sebastian Riedel and Iftekhar Naim and Ming-Wei Chang and Kelvin Guu},
      year={2024},
      eprint={2406.13121},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.13121}, 
}

@misc{gavin2024longinschallenginglongcontextinstructionbased,
      title={LongIns: A Challenging Long-context Instruction-based Exam for LLMs}, 
      author={Shawn Gavin and Tuney Zheng and Jiaheng Liu and Quehry Que and Noah Wang and Jian Yang and Chenchen Zhang and Wenhao Huang and Wenhu Chen and Ge Zhang},
      year={2024},
      eprint={2406.17588},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.17588}, 
}

% Aplications of RAG and LLM on legal practice

@misc{CBR-RAG,
      title={CBR-RAG: Case-Based Reasoning for Retrieval Augmented Generation in LLMs for Legal Question Answering}, 
      author={Nirmalie Wiratunga and Ramitha Abeyratne and Lasal Jayawardena and Kyle Martin and Stewart Massie and Ikechukwu Nkisi-Orji and Ruvan Weerasinghe and Anne Liret and Bruno Fleisch},
      year={2024},
      eprint={2404.04302},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2404.04302}, 
}

@misc{qasem2023exploitationllmbasedchatbotproviding,
      title={Towards the Exploitation of LLM-based Chatbot for Providing Legal Support to Palestinian Cooperatives}, 
      author={Rabee Qasem and Banan Tantour and Mohammed Maree},
      year={2023},
      eprint={2306.05827},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2306.05827}, 
}

@misc{devaraj2023developmentlegaldocumentaichatbot,
      title={Development of a Legal Document AI-Chatbot}, 
      author={Pranav Nataraj Devaraj and Rakesh Teja P V au2 and Aaryav Gangrade and Manoj Kumar R},
      year={2023},
      eprint={2311.12719},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2311.12719}, 
}

@misc{LLMlawsurvey,
      title={Large Language Models in Law: A Survey}, 
      author={Jinqi Lai and Wensheng Gan and Jiayang Wu and Zhenlian Qi and Philip S. Yu},
      year={2023},
      eprint={2312.03718},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2312.03718}, 
}

@inproceedings{jayakumar-etal-2023-large,
    title = "Large Language Models are legal but they are not: Making the case for a powerful {L}egal{LLM}",
    author = "Jayakumar, Thanmay  and
      Farooqui, Fauzan  and
      Farooqui, Luqman",
    editor = "Preo{\textcommabelow{t}}iuc-Pietro, Daniel  and
      Goanta, Catalina  and
      Chalkidis, Ilias  and
      Barrett, Leslie  and
      Spanakis, Gerasimos  and
      Aletras, Nikolaos",
    booktitle = "Proceedings of the Natural Legal Language Processing Workshop 2023",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.nllp-1.22",
    doi = "10.18653/v1/2023.nllp-1.22",
    pages = "223--229",
    abstract = "Realizing the recent advances from Natural Language Processing (NLP) to the legal sector poses challenging problems such as extremely long sequence lengths, specialized vocabulary that is usually only understood by legal professionals, and high amounts of data imbalance. The recent surge of Large Language Models (LLM) has begun to provide new opportunities to apply NLP in the legal domain due to their ability to handle lengthy, complex sequences. Moreover, the emergence of domain-specific LLMs has displayed extremely promising results on various tasks. In this study, we aim to quantify how general LLMs perform in comparison to legal-domain models (be it an LLM or otherwise). Specifically, we compare the zero-shot performance of three general-purpose LLMs (ChatGPT-3.5, LLaMA-70b and Falcon-180b) on the LEDGAR subset of the LexGLUE benchmark for contract provision classification. Although the LLMs were not explicitly trained on legal data, we observe that they are still able to classify the theme correctly in most cases. However, we find that their mic-F1/mac-F1 performance are upto 19.2/26.8{\%} lesser than smaller models fine-tuned on the legal domain, thus underscoring the need for more powerful legal-domain LLMs.",
}

@misc{gpt3.5,
      title={Language Models are Few-Shot Learners}, 
      author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
      year={2020},
      eprint={2005.14165},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2005.14165}, 
}

@misc{falcon,
      title={The Falcon Series of Open Language Models}, 
      author={Ebtesam Almazrouei and Hamza Alobeidli and Abdulaziz Alshamsi and Alessandro Cappelli and Ruxandra Cojocaru and Mérouane Debbah and Étienne Goffinet and Daniel Hesslow and Julien Launay and Quentin Malartic and Daniele Mazzotta and Badreddine Noune and Baptiste Pannier and Guilherme Penedo},
      year={2023},
      eprint={2311.16867},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2311.16867}, 
}

@misc{llama2,
      title={Llama 2: Open Foundation and Fine-Tuned Chat Models}, 
      author={Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and et al.},
      year={2023},
      eprint={2307.09288},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2307.09288}, 
}

@inproceedings{lexglue,
    title = "{L}ex{GLUE}: A Benchmark Dataset for Legal Language Understanding in {E}nglish",
    author = "Chalkidis, Ilias  and
      Jana, Abhik  and
      Hartung, Dirk  and
      Bommarito, Michael  and
      Androutsopoulos, Ion  and
      Katz, Daniel  and
      Aletras, Nikolaos",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.297",
    doi = "10.18653/v1/2022.acl-long.297",
    pages = "4310--4330",
    abstract = "Laws and their interpretations, legal arguments and agreements are typically expressed in writing, leading to the production of vast corpora of legal text. Their analysis, which is at the center of legal practice, becomes increasingly elaborate as these collections grow in size. Natural language understanding (NLU) technologies can be a valuable tool to support legal practitioners in these endeavors. Their usefulness, however, largely depends on whether current state-of-the-art models can generalize across various tasks in the legal domain. To answer this currently open question, we introduce the Legal General Language Understanding Evaluation (LexGLUE) benchmark, a collection of datasets for evaluating model performance across a diverse set of legal NLU tasks in a standardized way. We also provide an evaluation and analysis of several generic and legal-oriented models demonstrating that the latter consistently offer performance improvements across multiple tasks.",
}

@misc{SaulLM,
      title={SaulLM-7B: A pioneering Large Language Model for Law}, 
      author={Pierre Colombo and Telmo Pessoa Pires and Malik Boudiaf and Dominic Culver and Rui Melo and Caio Corro and Andre F. T. Martins and Fabrizio Esposito and Vera Lúcia Raposo and Sofia Morgado and Michael Desa},
      year={2024},
      eprint={2403.03883},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2403.03883}, 
}

@misc{mistral7b,
      title={Mistral 7B}, 
      author={Albert Q. Jiang and Alexandre Sablayrolles and Arthur Mensch and Chris Bamford and Devendra Singh Chaplot and Diego de las Casas and Florian Bressand and Gianna Lengyel and Guillaume Lample and Lucile Saulnier and Lélio Renard Lavaud and Marie-Anne Lachaux and Pierre Stock and Teven Le Scao and Thibaut Lavril and Thomas Wang and Timothée Lacroix and William El Sayed},
      year={2023},
      eprint={2310.06825},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.06825}, 
}

@misc{mmlu,
      title={Measuring Massive Multitask Language Understanding}, 
      author={Dan Hendrycks and Collin Burns and Steven Basart and Andy Zou and Mantas Mazeika and Dawn Song and Jacob Steinhardt},
      year={2021},
      eprint={2009.03300},
      archivePrefix={arXiv},
      primaryClass={cs.CY},
      url={https://arxiv.org/abs/2009.03300}, 
}

@misc{legalbert,
      title={LEGAL-BERT: The Muppets straight out of Law School}, 
      author={Ilias Chalkidis and Manos Fergadiotis and Prodromos Malakasiotis and Nikolaos Aletras and Ion Androutsopoulos},
      year={2020},
      eprint={2010.02559},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2010.02559}, 
}

@inproceedings{bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    editor = "Burstein, Jill  and
      Doran, Christy  and
      Solorio, Thamar",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}

@misc{anglebert,
      title={AnglE-optimized Text Embeddings}, 
      author={Xianming Li and Jing Li},
      year={2024},
      eprint={2309.12871},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2309.12871}, 
}

@software{llamaindex,
author = {Liu, Jerry},
doi = {10.5281/zenodo.1234},
month = {11},
title = {{LlamaIndex}},
url = {https://github.com/jerryjliu/llama_index},
year = {2022}
}

@software{langchain,
author = {Chase, Harrison},
title = {LangChain},
url = {https://github.com/langchain-ai/langchain},
month = {10},
year = {2022}
}

@misc{clerc,
      title={CLERC: A Dataset for Legal Case Retrieval and Retrieval-Augmented Analysis Generation}, 
      author={Abe Bohan Hou and Orion Weller and Guanghui Qin and Eugene Yang and Dawn Lawrie and Nils Holzenberger and Andrew Blair-Stanek and Benjamin Van Durme},
      year={2024},
      eprint={2406.17186},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.17186}, 
}

@misc{cap,
title = {Caselaw access project},
author = {Harvard Law School Library Innovation Lab},
url = {https://case.law/},
year = {2024},
note = {Accessed: 2024-08-05}
}

@misc{Ajmi2024,
  author = {Ajmi, Ayyoub},
  title = {Revolutionizing Access to Justice: The Role of {AI}-Powered Chatbots and Retrieval-Augmented Generation in Legal Self-Help},
  year = {2024},
  url = {https://irlaw.umkc.edu/faculty_works/949},
}

@book{basiclaw,
title = {Basic Knowledge of Law},
author = {Jumpa, Manit},
year = {2022},
edition = {21},
publisher = {Chulalongkorn University Press},
address = {Bangkok, Thailand},
isbn = {9789740337744},
language = {thai},
pages = {371},
}
%Problem statement

@misc{5chunking,
title = {The 5 Levels Of Text Splitting For Retrieval},
year = {2024},
month = {1},
day = {08},
author = {Greg Kamradt},
howpublished = {\url{https://www.youtube.com/watch?v=8OJC21T2SL4}},
}

@misc{gpt4o,
      title={GPT-4o System Card}, 
      author={Aaron Hurst and Adam Lerer and Adam P Goucher and Adam Perelman and Aditya Ramesh and Aidan Clark and AJ Ostrow and Akila Welihinda and Alan Hayes and Alec Radford and et al.},
      year={2024},
      eprint={2410.21276},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.21276}, 
}

@misc{claude3.5sonnet,
  title = {Claude 3.5 Sonnet Model Card Addendum},
  author = {Anthropic},
  year = {2024},
  url = {https://www.anthropic.com/news/claude-3-5-sonnet},
}

@misc{o1,
 title = {"OpenAI o1 System Card"},
 author = {"OpenAI"},
 year = {2024},
 url = {https://openai.com/index/openai-o1-system-card/}}

@misc{typhoon2,
      title={Typhoon 2: A Family of Open Text and Multimodal Thai Large Language Models}, 
      author={Kunat Pipatanakul and Potsawee Manakul and Natapong Nitarach and Warit Sirichotedumrong and Surapon Nonesung and Teetouch Jaknamon and Parinthapat Pengpun and Pittawat Taveekitworachai and Adisai Na-Thalang and Sittipong Sripaisarnmongkol and Krisanapong Jirayoot and Kasima Tharnpipitchai},
      year={2024},
      eprint={2412.13702},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.13702}, 
}

@misc{claude3,
  title={The Claude 3 Model Family: Opus, Sonnet, Haiku},
  author={Anthropic},
  year={2024},
  month = mar,
  note = {Technical report},
  url={https://www.anthropic.com/index/claude-3-technical-report}
}

@misc{llama3.1,
      title={The Llama 3 Herd of Models}, 
      author={Abhimanyu Dubey and Abhinav Jauhri and Abhinav Pandey and Abhishek Kadian and Ahmad Al-Dahle and Aiesha Letman and Akhil Mathur and Alan Schelten and Amy Yang and Angela Fan et al.},
      year={2024},
      eprint={2407.21783},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2407.21783}, 
}

@inproceedings{thlegalbert,
author={Wiratchawa, Kannika and Khunthong, Tanutcha and Intharah, Thanapong},
booktitle={2021 18th International Conference on Electrical Engineering/Electronics, Computer, Telecommunications and Information Technology (ECTI-CON)}, 
title={LegalBERT-th: Development of Legal Q\&A Dataset and Automatic Question Tagging}, 
year={2021},
month = {05},
volume={},
number={},
pages={1159-1162},
keywords={Law;Computational modeling;Bit error rate;Transfer learning;Prototypes;Tagging;Cleaning;Bidirectional Encoder Representations from Transformers;Legal Classification;Question Tagging;Legal Dataset;NLP Dataset},
doi={10.1109/ECTI-CON51831.2021.9454753}}

@misc{llmeval,
	author = {Jeffrey Ip},
	title = {{L}{L}{M} {E}valuation {M}etrics: {T}he {U}ltimate {L}{L}{M} {E}valuation {G}uide - {C}onfident {A}{I} --- confident-ai.com},
	howpublished = {confident-ai.com},
        url = {https://www.confident-ai.com/blog/llm-evaluation-metrics-everything-you-need-for-llm-evaluation},
	year = {},
	note = {[Accessed 08-08-2024]},
}

@inproceedings{papineni-etal-2002-bleu,
    title = "{B}leu: a Method for Automatic Evaluation of Machine Translation",
    author = "Papineni, Kishore  and
      Roukos, Salim  and
      Ward, Todd  and
      Zhu, Wei-Jing",
    editor = "Isabelle, Pierre  and
      Charniak, Eugene  and
      Lin, Dekang",
    booktitle = "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2002",
    address = "Philadelphia, Pennsylvania, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P02-1040",
    doi = "10.3115/1073083.1073135",
    pages = "311--318",
}

@misc{fu2023gptscoreevaluatedesire,
      title={GPTScore: Evaluate as You Desire}, 
      author={Jinlan Fu and See-Kiong Ng and Zhengbao Jiang and Pengfei Liu},
      year={2023},
      eprint={2302.04166},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2302.04166}, 
}

@misc{zhang2020bertscoreevaluatingtextgeneration,
      title={BERTScore: Evaluating Text Generation with BERT}, 
      author={Tianyi Zhang and Varsha Kishore and Felix Wu and Kilian Q. Weinberger and Yoav Artzi},
      year={2020},
      eprint={1904.09675},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1904.09675}, 
}

@misc{liu2023gevalnlgevaluationusing,
      title={G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment}, 
      author={Yang Liu and Dan Iter and Yichong Xu and Shuohang Wang and Ruochen Xu and Chenguang Zhu},
      year={2023},
      eprint={2303.16634},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2303.16634}, 
}

@misc{thanoy,
	author = {Kobkrit Viriyayudhakorn},
	title = {Thanoy {A}{I} {C}hatbot - Genius {A}{I} Lawyer},
	howpublished = {iapp.co.th},
        url = {https://iapp.co.th/thanoy},
	year = {2024},
	note = {[Accessed 13-08-2024]},
}

@misc{lexisnexis,
	author = {LexisNexis},
	title = {{L}exis{N}exis {L}aunches {L}exis+ {A}{I}, a {G}enerative {A}{I} {S}olution with {H}allucination-{F}ree {L}inked {L}egal {C}itations},
	howpublished = {lexisnexis.com},
        url = {https://www.lexisnexis.com/community/pressroom/b/news/posts/lexisnexis-launches-lexis-ai-a-generative-ai-solution-with-hallucination-free-linked-legal-citations},
	year = {2023},
	note = {[Accessed 13-08-2024]},
}

@misc{thomsonreutersAIpoweredLegal,
	author = {Sarah Strumberger},
	title = {{A}{I}-powered legal research: {W}here legal research meets generative {A}{I}},
	howpublished = {legal.thomsonreuters.com},
        url = {https://legal.thomsonreuters.com/blog/legal-research-meets-generative-ai/},
	year = {2023},
	note = {[Accessed 13-08-2024]},
}

@misc{leewayhertzAgentsLegal,
	author = {Akash Takyar},
	title = {{A}{I} agents for legal: {A}pplications, benefits, implementation and future trends --- leewayhertz.com},
	howpublished = {leewayhertz.com},
        url = {https://www.leewayhertz.com/ai-agent-for-legal/},
	year = {},
	note = {[Accessed 13-08-2024]},
}

@misc{magesh2024hallucinationfreeassessingreliabilityleading,
      title={Hallucination-Free? Assessing the Reliability of Leading AI Legal Research Tools}, 
      author={Varun Magesh and Faiz Surani and Matthew Dahl and Mirac Suzgun and Christopher D. Manning and Daniel E. Ho},
      year={2024},
      eprint={2405.20362},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2405.20362}, 
}

@misc{needleinahaystack,
	author = {Gregory Kamradt},
	title = {Needle in a Haystack},
	howpublished = {github.com},
        url={https://github.com/gkamradt/LLMTest_NeedleInAHaystack},
	year = {2023}
}


@misc{loft,
      title={Can Long-Context Language Models Subsume Retrieval, RAG, SQL, and More?}, 
      author={Jinhyuk Lee and Anthony Chen and Zhuyun Dai and Dheeru Dua and Devendra Singh Sachan and Michael Boratko and Yi Luan and Sébastien M. R. Arnold and Vincent Perot and Siddharth Dalmia and Hexiang Hu and Xudong Lin and Panupong Pasupat and Aida Amini and Jeremy R. Cole and Sebastian Riedel and Iftekhar Naim and Ming-Wei Chang and Kelvin Guu},
      year={2024},
      eprint={2406.13121},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.13121}, 
}

@misc{leval,
      title={L-Eval: Instituting Standardized Evaluation for Long Context Language Models}, 
      author={Chenxin An and Shansan Gong and Ming Zhong and Xingjian Zhao and Mukai Li and Jun Zhang and Lingpeng Kong and Xipeng Qiu},
      year={2023},
      eprint={2307.11088},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2307.11088}, 
}

@inproceedings{longbench,
    title = "{L}ong{B}ench: A Bilingual, Multitask Benchmark for Long Context Understanding",
    author = "Bai, Yushi  and
      Lv, Xin  and
      Zhang, Jiajie  and
      Lyu, Hongchang  and
      Tang, Jiankai  and
      Huang, Zhidian  and
      Du, Zhengxiao  and
      Liu, Xiao  and
      Zeng, Aohan  and
      Hou, Lei  and
      Dong, Yuxiao  and
      Tang, Jie  and
      Li, Juanzi",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.172",
    pages = "3119--3137",
    abstract = "Although large language models (LLMs) demonstrate impressive performance for many language tasks, most of them can only handle texts a few thousand tokens long, limiting their applications on longer sequence inputs, such as books, reports, and codebases. Recent works have proposed methods to improve LLMs{'} long context capabilities by extending context windows and more sophisticated memory mechanisms. However, comprehensive benchmarks tailored for evaluating long context understanding are lacking. In this paper, we introduce LongBench, the first bilingual, multi-task benchmark for long context understanding, enabling a more rigorous evaluation of long context understanding. LongBench comprises 21 datasets across 6 task categories in both English and Chinese, with an average length of 6,711 words (English) and 13,386 characters (Chinese). These tasks cover key long-text application areas including single-doc QA, multi-doc QA, summarization, few-shot learning, synthetic tasks, and code completion. All datasets in LongBench are standardized into a unified format, allowing for effortless automatic evaluation of LLMs. Upon comprehensive evaluation of 8 LLMs on LongBench, we find that: (1) Commercial model (GPT-3.5-Turbo-16k) outperforms other open-sourced models, but still struggles on longer contexts. (2) Scaled position embedding and fine-tuning on longer sequences lead to substantial improvement on long context understanding. (3) Context compression technique such as retrieval brings improvement for model with weak ability on long contexts, but the performance still lags behind models that have strong long context understanding capability.",
}

@misc{selfroute,
      title={Retrieval Augmented Generation or Long-Context LLMs? A Comprehensive Study and Hybrid Approach}, 
      author={Zhuowan Li and Cheng Li and Mingyang Zhang and Qiaozhu Mei and Michael Bendersky},
      year={2024},
      eprint={2407.16833},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2407.16833}, 
}

@misc{nepaquad,
      title={RAG vs. Long Context: Examining Frontier Large Language Models for Environmental Review Document Comprehension}, 
      author={Hung Phan and Anurag Acharya and Sarthak Chaturvedi and Shivam Sharma and Mike Parker and Dan Nally and Ali Jannesari and Karl Pazdernik and Mahantesh Halappanavar and Sai Munikoti and Sameera Horawalavithana},
      year={2024},
      eprint={2407.07321},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2407.07321}, 
}

@misc{originalRAG,
      title={Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks}, 
      author={Patrick Lewis and Ethan Perez and Aleksandra Piktus and Fabio Petroni and Vladimir Karpukhin and Naman Goyal and Heinrich Küttler and Mike Lewis and Wen-tau Yih and Tim Rocktäschel and Sebastian Riedel and Douwe Kiela},
      year={2021},
      eprint={2005.11401},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2005.11401}, 
}

#Retrieval Model stuff
@misc{jina-v3,
      title={jina-embeddings-v3: Multilingual Embeddings With Task LoRA}, 
      author={Saba Sturua and Isabelle Mohr and Mohammad Kalim Akram and Michael Günther and Bo Wang and Markus Krimmel and Feng Wang and Georgios Mastrapas and Andreas Koukounas and Andreas Koukounas and Nan Wang and Han Xiao},
      year={2024},
      eprint={2409.10173},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2409.10173}, 
}

@misc{jina-v2,
      title={Jina-ColBERT-v2: A General-Purpose Multilingual Late Interaction Retriever}, 
      author={Rohan Jha and Bo Wang and Michael Günther and Saba Sturua and Mohammad Kalim Akram and Han Xiao},
      year={2024},
      eprint={2408.16672},
      archivePrefix={arXiv},
      primaryClass={cs.IR},
      url={https://arxiv.org/abs/2408.16672}, 
}

@misc{colbert,
      title={ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT}, 
      author={Omar Khattab and Matei Zaharia},
      year={2020},
      eprint={2004.12832},
      archivePrefix={arXiv},
      primaryClass={cs.IR},
      url={https://arxiv.org/abs/2004.12832}, 
}

@misc{nvembed,
      title={NV-Embed: Improved Techniques for Training LLMs as Generalist Embedding Models}, 
      author={Chankyu Lee and Rajarshi Roy and Mengyao Xu and Jonathan Raiman and Mohammad Shoeybi and Bryan Catanzaro and Wei Ping},
      year={2024},
      eprint={2405.17428},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{li2023making,
      title={Making Large Language Models A Better Foundation For Dense Retrieval}, 
      author={Chaofan Li and Zheng Liu and Shitao Xiao and Yingxia Shao},
      year={2023},
      eprint={2312.15503},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{bge-m3,
      title={BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation}, 
      author={Jianlv Chen and Shitao Xiao and Peitian Zhang and Kun Luo and Defu Lian and Zheng Liu},
      year={2024},
      eprint={2402.03216},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{bm25,
author = {Robertson, Stephen and Zaragoza, Hugo},
year = {2009},
month = {01},
pages = {333-389},
title = {The Probabilistic Relevance Framework: BM25 and Beyond},
volume = {3},
journal = {Foundations and Trends in Information Retrieval},
doi = {10.1561/1500000019}
}

@misc{legalbench,
      title={LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models}, 
      author={Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher Ré and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},
      year={2023},
      eprint={2308.11462},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2308.11462}, 
}

@book{chuathai2023introduction,
  author    = {Somyot Chuathai},
  title     = {Introduction to Law},
  edition   = {30th},
  publisher = {Winyuchon},
  year      = {2023},
  pages     = {124-126}
}

@misc{criminal_code_section_260,
  author = "{The Kingdom of Thailand}",
  title = "{Section 260 of the Criminal Code B.E. 2565}",
  year = {2022},
  note = {Author's translation},
  howpublished = {Office of the Council of State of Thailand},
  volumn = {32}
}


@misc{roscoe,
      title={ROSCOE: A Suite of Metrics for Scoring Step-by-Step Reasoning}, 
      author={Olga Golovneva and Moya Chen and Spencer Poff and Martin Corredor and Luke Zettlemoyer and Maryam Fazel-Zarandi and Asli Celikyilmaz},
      year={2023},
      eprint={2212.07919},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2212.07919}, 
}

@misc{receval,
      title={ReCEval: Evaluating Reasoning Chains via Correctness and Informativeness}, 
      author={Archiki Prasad and Swarnadeep Saha and Xiang Zhou and Mohit Bansal},
      year={2023},
      eprint={2304.10703},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2304.10703}, 
}

@misc{socreval,
      title={SocREval: Large Language Models with the Socratic Method for Reference-Free Reasoning Evaluation}, 
      author={Hangfeng He and Hongming Zhang and Dan Roth},
      year={2024},
      eprint={2310.00074},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.00074}, 
}

@misc{llmreasoners,
      title={LLM Reasoners: New Evaluation, Library, and Analysis of Step-by-Step Reasoning with Large Language Models}, 
      author={Shibo Hao and Yi Gu and Haotian Luo and Tianyang Liu and Xiyan Shao and Xinyuan Wang and Shuhua Xie and Haodi Ma and Adithya Samavedhi and Qiyue Gao and Zhen Wang and Zhiting Hu},
      year={2024},
      eprint={2404.05221},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2404.05221}, 
}

@article{DBLP:journals/corr/abs-2006-15498,
  author       = {Jingtao Zhan and
                  Jiaxin Mao and
                  Yiqun Liu and
                  Min Zhang and
                  Shaoping Ma},
  title        = {RepBERT: Contextualized Text Embeddings for First-Stage Retrieval},
  journal      = {CoRR},
  volume       = {abs/2006.15498},
  year         = {2020},
  url          = {https://arxiv.org/abs/2006.15498},
  eprinttype    = {arXiv},
  eprint       = {2006.15498},
  timestamp    = {Wed, 16 Sep 2020 13:34:18 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2006-15498.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{10.1145/3397271.3401075,
author = {Khattab, Omar and Zaharia, Matei},
title = {ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT},
year = {2020},
isbn = {9781450380164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397271.3401075},
doi = {10.1145/3397271.3401075},
abstract = {Recent progress in Natural Language Understanding (NLU) is driving fast-paced advances in Information Retrieval (IR), largely owed to fine-tuning deep language models (LMs) for document ranking. While remarkably effective, the ranking models based on these LMs increase computational cost by orders of magnitude over prior approaches, particularly as they must feed each query-document pair through a massive neural network to compute a single relevance score. To tackle this, we present ColBERT, a novel ranking model that adapts deep LMs (in particular, BERT) for efficient retrieval. ColBERT introduces a late interaction architecture that independently encodes the query and the document using BERT and then employs a cheap yet powerful interaction step that models their fine-grained similarity. By delaying and yet retaining this fine-granular interaction, ColBERT can leverage the expressiveness of deep LMs while simultaneously gaining the ability to pre-compute document representations offline, considerably speeding up query processing. Crucially, ColBERT's pruning-friendly interaction mechanism enables leveraging vector-similarity indexes for end-to-end retrieval directly from millions of documents. We extensively evaluate ColBERT using two recent passage search datasets. Results show that ColBERT's effectiveness is competitive with existing BERT-based models (and outperforms every non-BERT baseline), while executing two orders-of-magnitude faster and requiring up to four orders-of-magnitude fewer FLOPs per query.},
booktitle = {Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {39–48},
numpages = {10},
keywords = {neural ir, efficiency, deep language models, bert},
location = {Virtual Event, China},
series = {SIGIR '20}
}