\section{Methodology}

In this section, we present NitiBench, a novel benchmark dataset specifically designed to evaluate Thai legal QA systems. 
%
We also outline an accompanying evaluation process, starting with a description of the end-to-end (E2E) task that the system must perform. 
%
Additionally, we introduce a set of carefully designed metrics to ensure a thorough and comprehensive assessment of the system's performance.

\subsection{Dataset}
\label{subsec: dataset}

As stated in \S\ref{subsec: benchmarking}, a good benchmark dataset should aim to directly measure a legal QA system's performance in a free-form QA task, which such systems are designed to excel at. 
%
Furthermore, it should explicitly include relevant legal documents to allow for the isolated evaluation of the retrieval component in a RAG-based legal QA system.
%
Therefore, we construct our datasets such that each entry consists of a query, corresponding relevant legal sections, and the answer. 
%
Formally, our dataset is represented as $\mathcal{D} = \{(\textbf{x}_1, y_1), ..., (\textbf{x}_N, y_N)\}$ where $\textbf{x}_i = (q, T)$. $q$ denotes a query or a question relevant to Thai law, $T = \{t_1, t_2, ..., t_K\}$ denotes positive documents relevant to $q$, and $y$ is an answer to the query based on the provided positives $T$.

% We develop two datasets: one for general Thai Financial Law, named \textbf{WangchanX Legal ThaiCCL Rag Dataset (WCX dataset)}, which includes both a training set for fine-tuning the retrieval model and the LLM, and a testing set for evaluation; and a specialized QA dataset for Thai Tax Law, named \textbf{Tax Case Dataset}.

We develop two datasets for assessing Thai legal QA systems: 
\begin{enumerate}
    \item {\textbf{NitiBench-CCL}}: This dataset represents general Thai Financial Law.
    %
    This test set was derived from WangchanX-Legal-ThaiCCL-RAG's test set where we apply additional postprocessing to the original dataset.
    %This dataset includes both a training set for fine-tuning the retrieval model and the LLM, and a testing set for evaluation.
    \item {\textbf{NitiBench-Tax}}: A specialized QA dataset for Thai Tax Law based on official government statement from \url{https://rd.go.th/}{Revenue Department website}.
\end{enumerate}

We also present \textbf{WangchanX Legal ThaiCCL RAG dataset} where its train split was used to finetune our retrieval model.

\subsubsection{WangchanX Legal ThaiCCL RAG Dataset}
\label{subsubsec: wcx_dataset}

We present \textbf{WangchanX Legal ThaiCCL RAG Dataset}\footnote{\url{https://huggingface.co/datasets/airesearch/WangchanX-Legal-ThaiCCL-RAG}} (WCX Dataset), a question-answering dataset specifically designed for RAG tasks in the field of Thai financial law containing 35 legislations. 
%
The goal of this dataset is to provide comprehensive datasets for finetuning both the retrieval model and LLM, which is a foundation for building a domain-specific Thai legal RAG system. 
%
The dataset also contains a separate test set annotated by the domain experts.

% Should add figure. Clear up on how it is multi-label? How are answers generated (What is provided)? How are questions generated?
\textbf{Curating Training Data.} Our training dataset is constructed using a semi-automated approach combining synthetic data generation with expert validation. 
%
First, we automatically generate questions from each legal section from the available 35 legislations within Thai Financial Laws, including the Civil and Commercial Code, Revenue Code, and various other financial laws (see \S\ref{tab:wcxccc}) using Gemini 1.5 Pro \cite{gemini1.5}. 
%
We then retrieve the top 5 most relevant sections using BGE-M3 embeddings \cite{bge-m3} before asking Legal experts to subsequently refine these retrieved sections alongside the original legal section the question is generated from. 
%
The Legal experts act as annotators, labeling each retrieved section as positive if relevant. Answers are then generated using \texttt{Meta-Llama-3-70B} \cite{llama3.1}, or \texttt{claude-3-sonnet} \cite{claude3} if Llama3 fails to produce an answer. 
%
The answer is conditioned on both the labeled positive context and the question. 
%
Next, another expert will review and can either accept the question-answer pairs, adjust the LLM-generated answer and question again to ensure accuracy and comprehensiveness, or completely reject and discard the entry.
%
The process of constructing the training dataset is outlined in Figure~\ref{fig:wcx_pipeline_train}.

\textbf{Curating Test Data.} 
%
Our test set, conversely, is constructed entirely manually and undergoes a rigorous two-tiered expert review. 
%
Legal experts craft questions and answers based on specific legal sections. 
%
A separate group of legal experts then reviews these question-answer pairs. 
%
The experts can either accept the constructed question-answer pair or provide feedback to the original annotators, who will improve the data entry or reject it completely.
%
The test set covers only 21 out of 35 legislations in the training set.

It is important to note a key difference between the training and test sets: the training set inherently allows for multiple relevant sections (multi-label) per question, reflecting real-world legal scenarios where multiple legal provisions might apply. 
%
Conversely, due to the complexity of curating fully human-annotated multi-label data, the expert-curated test set primarily focuses on single-label question-answer pairs.
%
This discrepancy presents a limitation, potentially leading to an overly optimistic evaluation of models trained on the multilabel training set when tested on the single-label test set. 
%
Future work will explore bridging this gap, potentially through incorporating multi-label annotations in the test set or developing evaluation metrics robust to label multiplicity variations.
%
Additionally, we ensure that each dataset entry contains only laws within the scope of the 35 Thai financial law codes under consideration and that no duplicate legal sections appear within a single entry.

Figure~\ref{fig:wcx_pipeline_test} shows the construction pipeline of the test set.

\textbf{NitiBench-CCL}\footnote{\url{https://huggingface.co/datasets/VISAI-AI/nitibench}} NitiBench-CCL extends the original WangchanX-ThaiLegal-CCL-RAG's test set by applying additioanl postprocessing step. Since the annotated contextual information includes the full content of relevant legal sections, we further preprocess the test set by extracting only the names of the referenced legal sections from the annotations and deduplicate entries with the same questions. 


% \textbf{Data Preprocessing.} 
%
As annotated responses often include explanations or reasoning, which are outside the scope of this evaluation, we utilize a LLM\footnote{Specifically, we use \texttt{gpt-4o-mini-2024-07-18}\cite{gpt4o} during September 2024} to extract only the essential answers without the accompanying rationale. 
%
Note that \textbf{we only apply this preprocessing to our experiment, and the released dataset still contains full answers.} 
%
Finally, to avoid the data leakage problem with the retriever fine-tuned on the train split of the dataset, only the test split is used for evaluation. 
%
The test set comprises 3,730 entries, referencing 21 out of 35 law codes present in the training set. 
%
Notably, 43\% of the queries pertain to the Civil and Commercial Code, and each query corresponds to a single relevant law section.

\textbf{Annotator Profile and Cost} 
%
Since we are curating a dataset specifically in the Thai legal domain, it is important to ensure that our annotators have a strong background in Thai legal knowledge. 
%
To achieve this, we recruited legal experts through law school professors via their available channels, such as their personal Facebook networks\footnote{\href{https://www.facebook.com/photo/?fbid=10230736147843884\&set=a.2166246952290}{https://www.facebook.com/photo/?fbid=10230736147843884\&set=a.2166246952290}}. 
%
We received a total of 97 applications and selected 34 annotators. 
%
Their occupations include law students, recent law school graduates, and employees at law firms.

We compensate annotators per completed task, which includes curating the training set, conducting quality checks, and curating the test set. 
%
Tasks are randomly assigned, and we adjust the distribution based on each annotator’s speed of completion. 
%
Payment is determined per task\footnote{To simplify the calculations, we use a conversion rate of 34 Thai baht per 1 US dollar.}, with each task compensated differently based on its difficulty. The tasks are as follows:

\begin{enumerate}
\item Rerank retrieved documents for the fine-tuning dataset: 5 THB (approximately \$0.15) per task.
%
\item Validate, correct, and reject the generated answers for both training and test data: 10 THB (approximately \$0.30) per task.
%
\item Create a question and answer based on a given law section (for the test set): 30 THB (approximately \$0.89) per task.
\end{enumerate}
%
The total cost spent solely on annotators is approximately 274,240 THB (roughly \$8076).

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{images/CCL_Training_Set.png}    
    \caption{Overall dataset construction pipeline for training set of \texttt{WangchanX-Legal-ThaiCCL-RAG} dataset}
    \label{fig:wcx_pipeline_train}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{images/CCL_Test_Set.png}    
    \caption{Overall dataset construction pipeline for test set of \texttt{WangchanX-Legal-ThaiCCL-RAG} dataset and NitiBench-CCL. The original test split of \texttt{WangchanX-Legal-ThaiCCL-RAG} was completed after the annotator accepted all question/answer pairs from another annotator. NitiBench-CCL further augments this test set to condense answers using GPT-4o mini and conduct further postprocessing.}
    \label{fig:wcx_pipeline_test}
\end{figure}

\begingroup
\renewcommand{\arraystretch}{1.4}
\begin{table}[]
\centering
\begin{tabular}{@{}p{0.6\linewidth}ccc@{}}
\toprule
\textbf{Legislation}                                                                                                                           & \textbf{Legal Terminology} & \textbf{Training} & \textbf{Test} \\ \midrule
Organic Act on Counter Corruption, B.E. 2561                                                                                          & organic law       & \checkmark        &      \\
Civil and Commercial Code                                                                                                             & code              & \checkmark        & \checkmark    \\
Revenue Code                                                                                                                          & code              & \checkmark        & \checkmark    \\
Act on Offenses Relating to Registered Partnerships, Limited Partnerships, Companies Limited, Associations and Foundations, B.E. 2499 & act               & \checkmark        & \checkmark    \\
Chamber of Commerce Act, B.E. 2509                                                                                                    & act               & \checkmark        & \checkmark    \\
Trade Association Act, B.E. 2509                                                                                                      & act               & \checkmark        & \checkmark    \\
Accounting Profession Act, B.E. 2547                                                                                                  & act               & \checkmark        & \checkmark    \\
Business Registration Act, B.E. 2499                                                                                                  & act               & \checkmark        & \checkmark    \\
Public Limited Companies Act, B.E. 2535                                                                                               & act               & \checkmark        & \checkmark    \\
Foreign Business Act, B.E. 2542                                                                                                       & act               & \checkmark        & \checkmark    \\
Accounting Act, B.E. 2543                                                                                                             & act               & \checkmark        & \checkmark    \\
Secured Transactions Act, B.E. 2558                                                                                                   & act               & \checkmark        & \checkmark    \\
Securities and Exchange Act, B.E. 2535                                                                                                & act               & \checkmark        & \checkmark    \\
Derivatives Act, B.E. 2546                                                                                                            & act               & \checkmark        & \checkmark    \\
Provident Fund Act, B.E. 2530                                                                                                         & act               & \checkmark        & \checkmark    \\
Trust for Transactions in Capital Market Act, B.E. 2550                                                                               & act               & \checkmark        & \checkmark    \\
Energy Industry Act, B.E. 2550                                                                                                        & act               & \checkmark        & \checkmark    \\
Energy Conservation Promotion Act, B.E. 2535                                                                                          & act               & \checkmark        & \checkmark    \\
Financial Institutions Business Act, B.E. 2551                                                                                        & act               & \checkmark        & \checkmark    \\
Petroleum Income Tax Act, B.E. 2514                                                                                                   & act               & \checkmark        & \checkmark    \\
Act Repealing the Agricultural Futures Trading Act, B.E. 2542 B.E. 2558                                                               & act               & \checkmark        &      \\
State Enterprise Development and Governance Act, B.E. 2562                                                                            & act               & \checkmark        &      \\
Government Procurement and Supplies Management Act, B.E. 2560                                                                         & act               & \checkmark        &      \\
State Enterprise Committee and Personnel Qualifications Standards Act, B.E. 2518                                                      & act               & \checkmark        &      \\
State Enterprise Labor Relations Act, B.E. 2543                                                                                       & act               & \checkmark        &      \\
State Enterprise Capital Act, B.E. 2542                                                                                               & act               & \checkmark        &      \\
Budget Procedure Act, B.E. 2561                                                                                                       & act               & \checkmark        &      \\
Act on Offences of Officials Working in State Agencies or Organizations, B.E. 2502                                                    & act               & \checkmark        &      \\
Act on the Management of Shares and Stocks of Ministers, B.E. 2543                                                                    & act               & \checkmark        &      \\
Fiscal Discipline Act, B.E. 2561                                                                                                      & act               & \checkmark        &      \\
National Economic and Social Development Act, B.E. 2561                                                                               & act               & \checkmark        &      \\
Act on Disciplinary Offenses of Government Officials Performing Duties in Agencies Other than Government Agencies, B.E. 2534          & act               & \checkmark        &      \\
Act on the Establishment of Government Organizations, B.E. 2496                                                                       & act               & \checkmark        &      \\
Emergency Decree on Special Purpose Juristic Person for Securitization, B.E. 2540                                                     & emergency decree  & \checkmark        & \checkmark    \\
Emergency Decree on Digital Asset Businesses, B.E. 2561                                                                               & emergency decree  & \checkmark        &      \\ \bottomrule
\end{tabular}

\caption{List of legislation distribution in both training and test splits in WangchanX Legal ThaiCCL RAG Dataset (sorted High to Low Legislative Rank, Alphabetical). The test column is the same as NitiBench-CCL since the data was derived from WangchanX Legal ThaiCCL RAG test set}
\label{tab:wcxccc}
\end{table}

\endgroup

\begin{figure}[H]
    \centering
    \begin{minipage}{0.46\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/wangchan_law_count.png}
    \end{minipage}%
    \hfill
    \begin{minipage}{0.26\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/wangchan_rel_count.png}
    \end{minipage}%
    \hfill
    \begin{minipage}{0.26\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/wangchan_query_count.png}
    \end{minipage}

    \caption{Visualization of the WangchanX Legal ThaiCCL RAG Dataset. From left to right: (1) Number of times each legislation is used as relevant context; (2) Histogram showing the frequency of sections used as relevant context in a single entry; (3) Histogram showing the length of each query in characters.}
    \label{fig:combined}
\end{figure}



\subsubsection{NitiBench-Tax}
\label{subsubsec: tax_dataset}

To evaluate the generalization capability of the system, we curated an additional dataset derived from publicly available resources in the Thai financial legal domain.
%
Specifically, this dataset was created by scraping tax-related cases from the Revenue Department's official website\footnote{https://www.rd.go.th}. 
%
These cases represent authentic inquiries or requests (with personally identifiable information removed) submitted to the department. 
%
Each case includes the original inquiry or request, the official response, and metadata such as the case ID and submission date.

We extracted references to legislative sections mentioned in both the inquiry and the response as case attributes using LLM\footnote{We use only \texttt{gpt-4o-mini-2024-07-18}~\cite{gpt4o}. The API was called during September 2024.} for any preprocessing steps involving the use of LLM used during constructing NitiBench-Tax. 
%
The dataset was filtered to retain only cases referencing laws within the 35 Thai financial law codes and to eliminate duplicate references within individual entries. 
%
Some cases, however, involve inquiries requesting discretionary decisions from the department, such as extensions for tax deadlines or tax exemptions, rather than informational responses based on statutory interpretation. 
%
Since these cases are outside the scope of our work, which focuses on law-based reasoning, they were identified using an LLM and subsequently removed.

Additionally, to align with our evaluation objectives, the department's responses were condensed to essential answers, excluding detailed explanations and rationales.
%
Finally, we restricted the dataset to cases from 2021 onward, reflecting the most recent legislative updates. 
%
The resulting NitiBench-Tax consists of 50 cases, predominantly related to the Revenue Code, with an average of three referenced legal sections per case. 
%
This dataset provides a challenging testbed for evaluating system performance in a specialized domain requiring nuanced legal reasoning and multi-label retrieval.

The complete dataset construction pipeline is outlined in Figure~\ref{fig:tax_pipeline}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{images/tax_case_test_set.png}
    
    \caption{Overall dataset construction pipeline for NitiBench-Tax}
    \label{fig:tax_pipeline}
\end{figure}




\begin{figure}[H]
    \centering
    \begin{minipage}{0.34\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/tax_law_count.png}
    \end{minipage}%
    \hfill
    \begin{minipage}{0.31\textwidth}
        \centering
        \includegraphics[width=0.7\textwidth]{images/tax_rel_count.png}
    \end{minipage}%
    \hfill
    \begin{minipage}{0.31\textwidth}
        \centering
        \includegraphics[width=0.7\textwidth]{images/tax_query_count.png}
    \end{minipage}

    \caption{Visualization of the NitiBench-Tax. From left to right: (1) Number of times each legislation is used as relevant context; (2) Histogram showing the number of sections used as relevant context in a single entry; (3) Histogram showing the length of each query in characters.}
    \label{fig:tax_combined}
\end{figure}

\subsection{Task}
\label{subsec: task}

For end-to-end evaluation, we adapt the task proposed by Laban et al.~\cite{laban2024summaryhaystackchallengelongcontext} to suit the Thai legal domain.
%
While the original task emphasized summarizing long contexts into bullet points with references, our QA-oriented dataset necessitates a different focus. 

Specifically, given a query and an optional context comprising legal sections, the system must generate an accurate, relevant answer and provide a list of cited legal sections used as references. 
%
This task assesses the system’s ability to select pertinent legal sections and perform legal reasoning to produce a complete, correct response.

\subsection{Metrics}
\label{subsec: metric}
Next, we describe the metrics used in both retrieval and E2E evaluation proposed to measure the performance of the Thai legal QA system in our benchmark.

\subsubsection{Retriever Metrics}
\label{subsubsec: retriever_metric}
As mentioned in \S\ref{sec:lit_review}, a good benchmark should be able to capture the performance of the retrieval component in RAG-based QA systems and should support multiple positive relevant document behaviors commonly observed in the legal domain. 
%
We modified the traditional information retrieval metrics, including Hit Rate, Recall, and MRR, which are designed for a single positive label to accommodate a multi-label scenario, enabling their use with datasets like ours that feature multiple positive labels.

\textbf{HitRate:} Let $N$ be the number of documents in a dataset and $k$ represents the number of top retrieved documents being evaluated, $T_i$ represents the set of positive relevant documents and $R_i^k$ represents the top-$k$ ranked retrieved documents of the $i^\text{th}$ sample. 
%
HitRate(HR) can be defined as

\begin{equation}
    \text{HitRate@k}=\frac{1}{N}\sum_{i=1}^{N}\mathbb{I}(R_i^k\subseteq T_i) 
\end{equation}

\textbf{Multi HitRate: } To ensure that hit rate supports multi-positive setup. 
%
We defined a multi-label version of the hit rate where the metric requires the system to retrieve all true relevant documents to be considered a hit. 
%
Formally

\begin{equation}
    \text{Multi-HitRate@k}=\frac{1}{N}\sum_{i=1}^{N}\mathbb{I}(T_i\subseteq R_i^k)
\end{equation}

\textbf{Recall: }As with HitRate, recall can be defined as

\begin{equation}
    \text{Recall@k}=\frac{1}{N}\frac{\sum_{i=1}^N |T_i\cap R_i^k|}{\sum_{i=1}^N |T_i|}
\end{equation}

\textbf{MRR and Multi-MRR: }In the traditional calculation of MRR with multi-label ground truth, the metrics can be written as follows:

\begin{equation}
    \text{MRR@k} = \frac{1}{N}\sum_{i=1}^N \frac{1}{\text{argmax}(T_i \cap R_i^k)} \label{eq:single_mrr}
\end{equation}

where $\text{argmax}(T_i\cap R_i^k)$ represents the highest rank number of correctly retrieved documents. 
%
The metric is zero if $|T_i \cap R_i^k| = 0$ (retrieved document contains no positive.

In some multi-label setup works, MRR was calculated under the assumption that any of the documents in the ground truth set $T$ is a sufficient context for the system to answer the question \cite{DBLP:journals/corr/abs-2006-15498, 10.1145/3397271.3401075}. 
%
However, this assumption is not true, especially in a legal domain where, sometimes, all relevant laws must be retrieved in order for the system to be able to answer the question. Therefore, the equation~\ref{eq:single_mrr} is augmented to Multi-MRR as follows:

\begin{equation}
    \text{Multi-MRR@k}=\frac{1}{N}\sum_{i=1}^N
    \left[
        \frac{\text{Recall@k}_{i}}{|T_i \cap R_i^k|}\sum_{j=1}^{|T_i\cap R_i^k|}
            \frac{1}{rank(d_j) - j + 1}
    \right]
\end{equation}

where $rank(d_j)$ represents the rank of the $j$-th matched documents, that is $d_j \in T_i \cap R^k_i$. 
%
This augmented calculation produces the highest scores when all of the relevant documents are retrieved and ranked at the top and, thus, can provide an evaluation metric that closely aligns with our performance goals.


\subsubsection{End-to-End Metrics}
\label{subsubsec: e2e_metric}

Building upon the metric suite employed by Laban et al.~\cite{laban2024summaryhaystackchallengelongcontext}, an additional metric inspired by Dahl et al.~\cite{Dahl_2024} is introduced to quantify hallucination rates. The following metrics are employed for end-to-end evaluation:

\textbf{Coverage:} This metric assesses the degree to which the generated answer semantically encompasses the ground truth response. 
%
Complete coverage of all points mentioned in the golden answer yields a score of 100, partial coverage results in a score of 50, and complete divergence is assigned a score of 0. 
%
Automatic evaluation is conducted using an LLM as a judge, following established methodologies (\cite{phan2024ragvslongcontext}, \cite{es2023ragasautomatedevaluationretrieval}).

\textbf{Citation: } This metric assesses the system's ability to ground answers in the provided context and its effectiveness in long-context retrieval. 
%
Specifically, given the ground truth answer, the generated response, and the relevant legal sections, we extract the cited sections from the generated answer.
%
This output, produced by the LLM, includes both the textual answer and citations in the form of a list of sections. 
%
We then compute the Recall, Precision, and F1 against the actual relevant sections. 
%
To distinguish these from similar retrieval metrics, these scores are often prefixed with "E2E," such as E2E Precision, E2E Recall, and E2E F1.

\textbf{Contradiction: } To assess hallucination, a simplified version of Dahl et al.'s \cite{Dahl_2024} approach is adopted. 
%
Rather than generating multiple answer trajectories and assessing whether there are contradictions among the generated trajectories, the generated answer is directly compared to the ground truth. 
%
A contradiction score of 0 indicates alignment between the two, while a score of 1 signifies direct disagreement.

In order to control the quality of judge LLM when assessing coverage and citation score, we iteratively refine the judge prompt to maximize the human agreements in a held-out set. 
%
In this held-out set, we sampled 150 entries from the NitiBench-Tax (without filtering cases from 2021 onwards) and 200 entries from the training set of WangchanX-Legal-ThaiCCL. 
%
The judge LLM is then prompted with the sampled queries to answer the inquiries along with cited laws. 
%
The LLM answer is then parsed to another LLM alongside the ground truth answer to generate the coverage and citation score. 
%
The generated coverage and citation scores are then compared with the ones assigned by a human annotator, given the queries, generated answers, and ground truth answers.
%
The agreement between LLM-generated scores and human-annotated scores is measured by precision, recall, and F1. 
%
The prompt for LLM-as-a-judge is then adjusted until it yields more than 0.8 F1 between human-annotated scores and LLM-generated ones.

The final agreement score between human-annotated scores and LLM-generated ones is displayed in Table~\ref{table: agreement_metric}. 
%
The LLM-as-a-judge score is generated by \texttt{gpt-4o-2024-08-06} \cite{gpt4o} model with a temperature of 0.3.

\begin{table}[H]
\centering



\begin{tabular}{@{}cccccc@{}}
\toprule
\textbf{Metric}                         & \textbf{Dataset} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-score} & \textbf{Support} \\ \midrule
\multirow{2}{*}{Coverage}      & Tax     & .83       & .83    & .83      & 150     \\
                               & CCL     & .88       & .88    & .88      & 200     \\ \midrule
\multirow{2}{*}{Contradiction} & Tax     & .92       & .91    & .91      & 150     \\
                               & CCL     & .98       & .97    & .98      & 200     \\ \bottomrule
\end{tabular}
\caption{Table displaying the weighted average precision, recall, and F1-score of LLM judge comparing to human experts in annotating coverage and contradiction score}
\label{table: agreement_metric}
\end{table}

The coverage agreement scores for the CCL and Tax are 0.88 and 0.83, respectively, which are lower than the contradiction agreement scores for both datasets. 
%
To further analyze this, we present the confusion matrices for each score in Tables~\ref{table: agreement_cm_wcx} and~\ref{table: agreement_cm_tax}.

\begin{table}[!ht]
    \centering
    \begin{minipage}{0.45\textwidth}
        \centering
        
        
        \resizebox{\textwidth}{!}{%
        \begin{tabular}{@{}lccc@{}}
        \toprule
                         & Predicted 0 & Predicted 50 & Predicted 100 \\ \midrule
        Ground Truth 0   & 8           & 2            & 3             \\
        Ground Truth 50  & 2           & 29           & 7             \\
        Ground Truth 100 & 1           & 9            & 139           \\ \bottomrule
        \end{tabular}
        }
        \caption{Confusion matrix for coverage agreement score on 200 WCX samples}
        \label{table: agreement_cm_wcx}
    \end{minipage}
    \hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        
        \resizebox{\textwidth}{!}{%
        \begin{tabular}{@{}lccc@{}}
        \toprule
                         & Predicted 0 & Predicted 50 & Predicted 100 \\ \midrule
        Ground Truth 0   & 43          & 5            & 1             \\
        Ground Truth 50  & 6           & 35           & 6             \\
        Ground Truth 100 & 2           & 5            & 47            \\ \bottomrule
        \end{tabular}
        }
        \caption{Confusion matrix for coverage agreement score on 150 NitiBench-Tax samples}
        \label{table: agreement_cm_tax}
    \end{minipage}
\end{table}

As observed in the confusion matrices, it is rare for the LLM-as-a-judge to misclassify a ground truth score of 0 as 100 or vice versa. 
%
Most errors occur in the confusion between 50 and 100, as well as between 0 and 50. 
%
We consider this acceptable since the boundaries between these scores can sometimes be subjective. 
%
Therefore, despite the agreement score not being as high as initially expected after multiple iterations, we conclude that it remains reliable, achieving at least 80\% accuracy for the coverage score and at least 90\% accuracy for the contradiction score.
