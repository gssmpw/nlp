\section{Introduction}
\label{sec:introduction}
% Outline would be something like this: 
% LLM in legal domain -> LLM in legal QA -> LLM in Thai legal QA -> Why it's hard to improve -> No standard benchmark -> No clear idea on the current gap
% We propose the evaluation method -> Describe the dataset property and metrics we defined
% With the proposed evaluation method -> Describe RQ1 (How traditional component of RAG affect the system performance) -> Describe problem of localization in Thai legal structure RQ2 (How crucial it is to have custom components to these?) -> Describe rise of LCLM RQ3 (How does LCLM fare in comparison with Normal RAG)

% LLM in legal domain
The application of large language models (LLMs) to legal domains holds immense potential, particularly in information retrieval and question-answering.
%
One prominent application of LLMs in the legal field is the development of LLM-powered legal research tools. 
%
These tools assist legal professionals with tasks such as a conversational search of legal documents, document summarization, and citation analysis. 
%
For example, leveraging the Retrieval-Augmented Generation (RAG) framework, \cite{lexisnexis,thomsonreutersAIpoweredLegal} created a reliable legal assistant chatbot, while \cite{leewayhertzAgentsLegal} proposed agentic workflows to address various legal challenges within a unified framework.

%Legal QA system in English
Another valuable LLM application is legal QA systems, which, unlike research tools designed for legal professionals, cater to the general public. 
%
These systems handle tasks such as providing general legal information and consulting on specific cases. 
%
To achieve this effectively, this type of system, mostly implemented in the RAG framework, must understand user queries accurately, retrieve or identify relevant legal documents, apply the identified information to the query, and respond in a factually correct and contextually relevant manner. 
%
Examples of LLM-based legal consulting systems include AI Lawyer \cite{ailawyerLawyerYour} and AskLegal.bot \cite{asklegalAskLegalbotLegal}, which offer LLM-powered legal guidance to consumers.

%Legal QA system in Thai
Despite the proliferation of legal QA systems in resource-rich languages like English, extending and adapting these systems to resource-constrained languages, such as Thai, remains a challenge. 
%
Thai Legal QA systems require careful design choices to balance trade-offs within the RAG framework and beyond. One example of such a system is Thanoy~\footnote{\href{https://iapp.co.th/thanoy}{https://iapp.co.th/thanoy}}~\cite{thanoy}, a Thai RAG-based legal QA system designed for the general public. 
%
However, even as a rare marketable example, Thanoy exhibits various errors, including incorrect legislation retrieval and hallucinations.

% as shown in \autoref{fig:thanoy}.
% cherry pick error case
% \begin{figure}[!ht]
%     \centering
%     \includegraphics[width=0.5\linewidth]{}
%     \caption{Caption}
%     \label{fig:thanoy}
% \end{figure}

%Main challenge in Thai Legal QA system
To shed light on legal QA systems under Thai laws, we investigate the main challenges of implementing such a system. 
%
One of the most significant challenges in implementing Thai Legal QA systems is the lack of standardized evaluation processes. 
%
This makes it difficult to compare systems objectively and to identify general performance gaps or bottlenecks. 
%
The difficulty arises for two primary reasons:  

\begin{itemize}
    \item \textbf{Limited Thai Legal QA Corpora}: The specificity and complexity of the Thai legal domain require substantial human effort to create annotated datasets, which are currently scarce.  
    
    \item \textbf{Inadequate Evaluation Metrics}: Widely used retrieval metrics, such as hit rate, mean reciprocal rank (MRR), and recall, are suitable for single-label retrieval tasks. 
    %
    However, the legal QA task oftentimes requires retrieving and applying multiple relevant documents simultaneously, making these traditional metrics insufficient. 
    %
    Additionally, evaluating end-to-end (E2E) performance poses a challenge due to the multi-faceted nature of evaluating LLM-based systems, as existing studies often use varied tasks and metrics, leading to inconsistencies and making systems difficult to compare comprehensively.
\end{itemize}

%Introduce our solution
To address this challenge, we present a novel benchmark dataset, NitiBench, along with corresponding task and evaluation metrics, specifically designed for assessing Thai Legal QA systems. 
%
The key features of this benchmark are as follows:  

\begin{itemize}  
    \item The benchmark comprises two datasets: the NitiBench-CCL Dataset, which covers general QA across 21 Thai financial law codes, and the Tax Case Dataset, which focuses on specialized QA involving real-world cases related to Thai tax issues requiring intense legal reasoning.  
    
    \item Each query in both datasets includes a question, an answer, and the relevant documents required to answer the question, enabling detailed evaluation of both the retrieval and E2E aspects.  
    
    \item The proposed retrieval metrics are designed to handle multi-label retrieval tasks, a critical requirement for effective legal QA.  
    
    \item Achieving a high E2E score on this benchmark requires a system to generate correct answers consistent with ground truth, avoid contradictions, and provide appropriate legal citations. 
    %
    These tasks and metrics simulate the legal research process, where accurate citations are as important as the correctness of the answer.  
\end{itemize}

Using the proposed benchmark, we conduct a comprehensive evaluation of legal QA systems built with current industry-standard components within the RAG framework. 
%
Specifically, we aim to answer three key research questions:

\textbf{(RQ1) What impact do components tailored to the Thai legal system structure have on a Thai legal QA system?}  

A significant challenge in building RAG-based Thai legal QA systems lies in the inherent hierarchical structure of Thai law, where sections\footnote{In this paper, ``section'' refers to a component in legislation, while we use ``\S'' to denote a section, subsection, or subsubsection in this document. For more information on Thai legal terminology, see \S\ref{subsec: thai_legal}.} often reference other sections within the same or different legal codes. 
%
To address this, we propose a \textbf{hierarchy-aware chunking}, a strategy that segments documents by legal sections, and \textbf{NitiLink}, a component that augments retrieved sections with additional referenced sections.  

Using our benchmark, we demonstrate that this chunking strategy outperforms naive line chunking strategy in both retrieval and E2E metrics, highlighting the importance of incorporating domain-specific knowledge when choosing chunking strategies. 
%
However, incorporating NitiLink does not improve E2E performance, likely due to the simplistic referencing criteria used in its implementation.

\textbf{(RQ2) How do the choices of retriever and LLM affect performance in a RAG-based Thai legal QA system?}  

This research question explores the impact of different retriever and LLM choices on the performance of a Thai legal QA system. 
%
Additionally, we examine the current performance gap for both retrievers and LLMs, comparing the RAG-based system with parametric baselines and RAG with a provided golden context.  

Our experiments reveal that all retrievers struggle with the Tax Case dataset, which contains complex queries requiring nuanced understanding. 
%
Moreover, fine-tuned retrievers perform poorly on out-of-domain queries and do not consistently show improved performance even on in-domain queries. 
%
For the LLM component, the Claude 3.5 Sonnet model \cite{claude3.5sonnet} achieves the best E2E performance, albeit by a small margin. 
%
However, all tested LLMs fail to perform well on the Tax Case dataset, even under the RAG with golden context setting, indicating that current LLMs lack sufficient legal reasoning capabilities.  

\textbf{(RQ3) How does a Long-Context LLM-based Thai legal QA system perform compared to RAG-based systems?}  

The advent of Long-Context LLMs (LCLMs) raises questions about their feasibility and effectiveness in the Thai legal domain, potentially challenging the need for traditional RAG systems. 
%
To investigate this, we evaluate a Thai legal QA system using LCLMs and compare its performance with RAG-based systems on our benchmark. 
%
Our results show that LCLMs still significantly underperform RAG-based systems, suggesting that LCLMs are not yet viable for implementing effective Thai legal QA systems.
%
This result contradicted previous findings that LCLM is better than RAG in some tasks \cite{laban2024summaryhaystackchallengelongcontext,selfroute,lee2024longcontextlanguagemodelssubsume} where LCLM is dominant in most setups except for SQL query \cite{lee2024longcontextlanguagemodelssubsume}.

Finally, we summarize our main contributions as follows:
\begin{enumerate}
    \item {We construct a new benchmark corpus for evaluating Thai legal QA systems, consisting of two datasets focused on the Thai financial and tax law domains.}
    
    \item {We design an evaluation process accompanying the constructed dataset, which includes defining the task for legal QA systems, adapting retriever metrics for multi-label scenarios, and establishing concrete end-to-end evaluation metrics. 
    %
    Our goal is to ensure that this evaluation process remains dataset-independent and can be applied to other Legal QA benchmark corpora.}
    
    \item {Using the proposed benchmark, we present our findings addressing the three key research questions outlined above.}
    % Dataset, Evaluation method, Key Insights
\end{enumerate}

