\section{Discussion}

\subsection{Bottleneck}
\label{subsubsec: bottleneck}
% Chunking and Retrieval -> Chunking by heirarchical is the best. Retrieval still struggles on Tax Case dataset. Fine-tuned model is not generalized. Case where fine-tuning does not help. Even retrieval good
The results from the previous section, as discussed in section \ref{subsec: chunking} , clearly demonstrate that the hierarchical chunking strategy significantly outperforms even the best naive chunking approach. This highlights the importance of incorporating domain knowledge and an understanding of document structure when selecting a chunking strategy to achieve optimal performance.

Next, from section \ref{subsec:re_result}, all retriever models continue to struggle with the Tax Case dataset due to the complexity of its queries, which require explicit reasoning and logical inference to break down tasks into more manageable components for retrieval. Furthermore, our results indicate minimal performance gains from including human input during fine-tuning, as evidenced by the marginal difference between Human-Finetuned BGE-M3 and Auto-Finetuned BGE-M3. Notably, the fine-tuned models' improved performance does not translate into a performance boost on the Tax Case dataset, where the base BGE-M3 still performs the best. This suggests that fine-tuning efforts should consider the specific domain and query format to achieve meaningful performance improvements.

Additionally, we explore the patterns of queries with unimproved performance, proposing various solutions to address these challenges. These include incorporating metadata into document chunks to reduce duplicate content retrieval, constructing knowledge graphs to retrieve hierarchically related documents, increasing the weight of ColBERT-based and sparse-embedding models to focus on finer details, and leveraging LLMs to break down complex queries into simpler, more manageable tasks.

Regarding the augmenter component as shown in section \ref{subsubsec: augmenter_e2e_result}, the proposed referencer, which retrieves child sections of the initially retrieved ones, significantly improves recall on the Tax Case dataset. However, it does not enhance end-to-end (E2E) performance. This limitation may be due to the complexity of the Tax Case queries, which require detailed reasoning by the LLM to fully utilize the additionally retrieved sections.

Conversely, the referencer does not improve retriever recall on the WCX dataset. This may stem from the nature of WCX queries, which often target single relevant laws. In these cases, retrieving neighboring sections beyond direct child sections could be more beneficial. Nevertheless, for subsets where recall is improved, the E2E performance also increases, likely due to the straightforward nature of WCX queries.

These findings suggest that the proposed referencer has room for further refinement. Improvements could include additional heuristic criteria for retrieval, such as considering distances based on the hierarchical structure of the law. Furthermore, the referencer appears most effective for queries that do not demand complex reasoning, indicating its potential for simpler retrieval tasks.

Regarding the performance of various LLMs, no significant differences were observed between models, although Claude 3.5 Sonnet showed slightly better results compared to others. All tested LLMs, however, performed mediocrely on the Tax Case dataset. This issue is further highlighted in Table \ref{subsubsec: main_table}, where even with golden context provided, the LLMs struggle to deliver meaningful performance on Tax Case queries, which demand complex reasoning. Additionally, the same table reveals limitations within the RAG pipeline on the Tax Case dataset. The performance is constrained by the retriever model's initial limitations and by the LLM's restricted ability to effectively utilize correctly retrieved laws, as evidenced by the disparity between end-to-end recall and retriever recall.

The feasibility of using long-context LLMs (LCLMs) was also evaluated for both end-to-end (E2E) tasks and retrieval tasks. LCLMs were found to struggle in the E2E setting, delivering subpar performance. However, they performed reasonably well in the retrieval task, although they fell short compared to embedding-based models, particularly in terms of performance improvements when increasing the number of retrieved documents.

\subsection{Positional Bias}
\label{subsubsec:positional_bias}
As mentioned in previous studies (\cite{laban2024summaryhaystackchallengelongcontext}, \cite{lee2024longcontextlanguagemodelssubsume}, \cite{gavin2024longinschallenginglongcontextinstructionbased}), a prominent phenomenon occurred in long-context language models is the "lost in the middle" problem where the model is not able to retrieve and understand the information deep in the long context. In this section, the effect of the relevant context position in the overall documents to the performance of the system is analysed on the sampled WCX dataset with long context Gemini 1.5 Pro ingesting 1.2 million tokens or 2.7 million characters. The resulting performance is binned every 100,000 characters by the maximum depth of the relevant laws that need to be retrieved and the coverage, contradiction and E2E F1 of each bin are averaged and plotted in figure \ref{fig: depth_performance}.

\begin{figure}[ht]
\centering
\includegraphics[width=\textwidth]{images/depth_performance.png}
\caption{Plot of performance grouped by the maximum depth of relevant context in the long context}
\label{fig: depth_performance}
\end{figure}

From the resulting plot, there is only a slight decrease in coverage score and slightly greater increase in contradiction score as the depth increases. However, there is a significant drop in E2E F1 score as the depth increases. Therefore, it can be concluded that the depth of the relevant laws does not affect the coverage and contradiction score of the system's response as much as its ability to cite applicable laws.  

% \subsubsection{Systematic Bias}
% \label{subsubsec:systematic_bias}

% Given the use of an LLM as a judge for coverage and contradiction scores, a rigorous evaluation of potential systematic biases in both the judge and the generator is essential. Bias is quantified by comparing scores assigned by human annotators and the LLM judge. [Result from this evaluation and analysis]



