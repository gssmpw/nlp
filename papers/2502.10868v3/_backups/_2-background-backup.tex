\section{Background}
\label{sec:background}
% Legal lit review
% Unlike domains like medicine with a more universal knowledge base, legal systems are inherently localized, each shaped by its unique history and societal context. 
Unlike fields such as medicine, which have a more universal knowledge base, legal systems are inherently localized, each shaped by its unique history and societal context.
In this section, we provide the background information essential to understanding the limitations and research gaps within the Thai legal domain, as well as the current applications of LLM-based solutions in legal practices.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Explain overall Thai legal framework, hightlight importance of Acts/Codes
\subsection{Thai Legal System}

\subsubsection{Thailand’s legal framework}
\label{sec:legal_system}
% explain Thai law hierarchy
Thailand's legal framework operates within a democratic, constitutional monarchy, adhering to a hierarchical structure \cite{basiclaw}. This hierarchy encompasses seven distinct levels:

\begin{enumerate}
    % รัฐธรรมนูญ
    \item \textbf{The Constitution}: The supreme law of the land, taking precedence over all other legal instruments. It defines the fundamental principles of governance, the relationship between different branches of government, and the rights and freedoms of citizens.
    % กฎหมายประกอบรัฐธรรมนูญ
    \item \textbf{Organic Laws}: While technically equal in weight to ordinary Acts and Codes, these laws hold a unique position due to their direct link to the Constitution. They provide detailed frameworks for key governance aspects as mandated by the Constitution and often necessitate specialized legislative processes.
    % พระราชบัญญัติ
    \item \textbf{Acts and Codes}: Legislation enacted by the Parliament, addressing specific areas of law or codifying existing legal principles.
    % พระราชกำหนด
    \item \textbf{Emergency Decrees}: Laws issued by the King upon the advice of the Cabinet, typically in urgent situations. They require subsequent parliamentary approval to remain in effect.
    % พระราชกฤษฎีกา
    \item \textbf{Royal Decrees}: Issued by the King on the advice of the Cabinet to provide detailed rules and regulations for implementing existing Acts.
    % กฎกระทรวง
    \item \textbf{Ministerial Regulations}: Regulations issued by individual ministers to provide further details and implementation guidelines for Acts and Royal Decrees.
    % ระเบียบ/ข้อบังคับ
    \item \textbf{Local Ordinances}: Rules and regulations established by local governments like municipalities and cities to address local concerns.
\end{enumerate}

% highlight importance of Act/Codes
Within the Thai legal hierarchy, \textbf{Acts} occupy a significant position, ranking immediately after the Constitution and Organic Laws. They represent primary legislation passed through a rigorous legislative process, ultimately requiring Royal Assent for enactment. While most legislation takes the form of individual Acts, some areas of law are addressed through comprehensive Codes.

% Codes splitted into multiple category - criminal, cival&commercial, etc. - and all Codes are equal
\st{While most legislation takes the form of individual Acts, some areas of law are addressed through }\textbf{Codes}, such as the \textit{Civil and Commercial Code} or the \textit{Criminal Code} (also referred to as the \textit{Penal Code}), consolidate related legal provisions into a single, structured document. This consolidation aims to provide a systematic and accessible framework for legal practitioners and the public alike. It's important to note that even though a Code represents a collection of related laws, it is given legal force through a dedicated Act of Parliament. Therefore, the enacting Act of a Code holds the true legal weight within the hierarchy.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Act/Codes structures explained
\subsubsection{Multi-Tiered Structure in Acts and Codes}
\label{multi-tiered}
Acts and Codes are meticulously organized, employing a multi-tiered structure to enhance clarity and accessibility. These tiers, using the Thai legal terminology, are:

\begin{enumerate}

    % 
    \item \textbf{Book}: This represents the broadest categorization, separating it into distinct thematic segments. For instance, the Criminal Code contains 3 books: (1) General Provisions, (2) Offenses, and (3) Petty Offenses.

    % 
    \item \textbf{Title}: Each book is further divided into titles, grouping together related legal concepts or principles. For example, within the Book dealing with offenses in the Criminal Code, a Title will focus on different type of offences e.g. \textit{Offenses against Property}.

    % 
    \item \textbf{Chapter}: Each chapter addressing a particular facet of the title. Continuing with the offenses against property, a Chapter will focus on the different types of offenses against property, such as \textit{Offences to Theft and Snaiching} or \textit{Offences of Extortion, Blackmail, Robbery and Gang-Robbery}.

    % 
    \item \textbf{Division}: Division are used to subdivide chapters, offering even greater granularity. This allows for a meticulous breakdown of complex legal concepts.

    % มาตรา
    \item \textbf{Section}: This forms the fundamental unit of Thai legislation, analogous to sections or articles in other legal systems. Each section articulates a specific legal rule or principle.

    % อนุมาตรา
    \item \textbf{Subsection}: To accommodate further detail, a section can be subdivided into subsections, denoted by numbers in parentheses. For example, section 2 (4) refers to subsection 4 within section 2.

    % วรรค
    \item \textbf{Clause}: Finally, for clarity and readability, individual section or subsection may be structured into paragraphs, denoted as clause.

\end{enumerate}

This granular structure, while seemingly complex, serves a practical purpose. It allows legal professionals to navigate voluminous codes with relative ease, locating specific provisions and understanding their context within the broader legal framework.

% RTGS or IPA ???
Moreover, the Thai legal system utilizes specific terminology to denote amendments and additions within the existing framework. Instead of renumbering entire sections, terms like \begin{thai}ทวิ\end{thai} - \textit{thawi} (second; bis), \begin{thai}ตรี\end{thai} - \textit{tri} (third; ter), and so forth, are added after the section number, indicating subsequent amendments. This practice preserves the original numbering while acknowledging legislative updates.

% This distinction between individual Acts and codified bodies of law, while primarily structural, highlights the importance of granular textual analysis for AI applications in the Thai legal domain. Accurately identifying the specific legal instrument, whether an individual Act or a provision within a larger Code, is crucial for accurate legal research and interpretation.

% This hierarchical structure, while ensuring legal clarity, poses a significant challenge for AI systems trained on general text. Traditional text chunking methods, often employed in RAG pipelines, risk fragmenting information vital for understanding the context and application of specific legal provisions. Retrieving a section of an Act without its corresponding Ministerial Regulation, for instance, could lead to incomplete or even misleading answers.

% Furthermore, the Thai legal system heavily relies on precedent established through judicial decisions. While not binding in the same way as common law systems, Supreme Court (Dika Court) rulings carry significant weight and influence subsequent judgments. This reliance on localized precedent further emphasizes the need for AI systems capable of capturing the nuances and context specific to Thai law.

% Therefore, effectively applying RAG to the Thai legal domain necessitates moving beyond generic text processing techniques. A deeper understanding of the legal system's structure, its reliance on local knowledge and precedent, and the interconnected nature of its various components is crucial for developing AI systems that can accurately and comprehensively address legal questions within the Thai context.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{LLM in Legal Practice}
\label{sec:llminlegal}

% some intro paragraph would be great
\textcolor{orange}{Although there aren't exists any research or publication that applies recent LLM technology towards Thai legal domain, many works have been explored in laws that is written in English.} Lai et al. \cite{LLMlawsurvey} explore the transformative potential of legal large language models (LLMs) in the legal industry, showcasing their applications in providing legal advice, conducting case analysis, and generating legal documents. However, they emphasize significant challenges to their widespread integration, including (1) the need for more comprehensive and high-quality legal datasets, (2) the difficulty of accurately interpreting legal concepts, and (3) the need to address algorithmic shortcomings such as interpretability, bias, and fairness. They further discuss the potential impact of LLMs on traditional judicial practices, such as the potential disruption of balance between prosecution and defense and the importance of ensuring access to data for all parties. The authors advocate for a responsible and ethical integration of legal LLMs, proposing future research directions aimed at data enhancement, algorithmic optimization, ethical considerations, and collaborative frameworks, ultimately aiming to ensure that these powerful tools are utilized for the benefit of society and the advancement of justice.


% Lai et al. \cite{LLMlawsurvey} provides a thorough examination of the emerging field of legal large language models (LLMs). The authors recognize the immense potential of LLMs in transforming the legal industry, showcasing their applications in providing legal advice, conducting case analysis, and generating legal documents. However, they also acknowledge the significant challenges associated with the integration of LLMs into the legal system, highlighting data limitations, algorithmic shortcomings, and the impact on traditional judicial practices. The paper emphasizes the need for more comprehensive and high-quality legal datasets, addressing issues of insufficient data acquisition, inadequate data sharing, and non-standardized legal documents. Furthermore, they discuss the difficulty of accurately interpreting legal concepts, which often carry inherent ambiguity and require context-dependent application. Algorithmic shortcomings are also addressed, specifically focusing on interpretability, ethics, bias and fairness, and algorithmic optimization. The authors stress the need to overcome the "black-box" nature of LLMs and improve their interpretability, especially in the context of judicial decision-making, where transparency and public trust are paramount. Ethical considerations, such as the potential for algorithmic bias and the influence of human bias in training data, are also discussed, highlighting the importance of developing techniques to mitigate bias and ensure fairness. Finally, the authors delve into the impact of LLMs on traditional judicial practices, discussing the potential for disrupting the balance between prosecution and defense, potentially leading to an imbalance of power. They also address the importance of ensuring parties’ access to data, as well as the need to expand and optimize the consulting capabilities of legal LLMs. The paper emphasizes the importance of developing accountability mechanisms to prevent political interference and promote transparency in the use of legal LLMs. In conclusion, the authors offer a comprehensive overview of the current state of legal LLMs, recognizing their potential and highlighting the challenges they face. They propose future research directions aimed at addressing these challenges, including data enhancement, algorithmic optimization, ethical considerations, and the development of collaborative frameworks. Ultimately, the authors advocate for a responsible and ethical integration of legal LLMs within the legal system, ensuring that these powerful tools are utilized for the benefit of society and the advancement of justice.

Jayakumar et al. \cite{jayakumar-etal-2023-large} investigates the performance of three large language models (LLMs) – ChatGPT-3.5 \cite{gpt3.5}, Falcon-180b \cite{falcon}, and LLaMA-2-70b \cite{llama2} on the LEDGAR subset of the LexGLUE benchmark \cite{lexglue} for contract provision classification. Despite not being trained on legal data, these general-purpose LLMs show a surprising ability to classify contract themes correctly in many cases. However, their performance falls significantly short of smaller models specifically fine-tuned on legal data, highlighting the need for specialized legal LLMs. The study observes that these LLMs struggle with semantically similar classes and labels with limited examples, emphasizing the importance of domain-specific data for robust legal language understanding. The findings suggest that while LLMs may offer benefits in terms of context length and parameter size, they cannot fully replace the effectiveness of smaller, fine-tuned models for specific legal tasks.

Al-Qasem et al. \cite{qasem2023exploitationllmbasedchatbotproviding} investigate the potential of ChatGPT-powered chatbots to provide legal support to Palestinian cooperatives. Their research showcases the development of a chatbot utilizing LlamaIndex \cite{llamaindex} to construct a comprehensive index of legal documents and question-answer data. This chatbot achieved a noteworthy 82\% accuracy in responding to 50 legal queries posed by a legal expert, highlighting its potential as a valuable resource for offering legal assistance to cooperatives and their members.

\cite{devaraj2023developmentlegaldocumentaichatbot} developed a legal document AI chatbot utilizing Langchain \cite{langchain} for natural language processing. The chatbot leverages Langchain's ability to process large documents and answer questions based on their semantic context, enabling users to ask questions about legal documents and receive responses tailored to the specific content. The project draws upon a variety of resources, including tutorials, documentation, and research papers to guide the development process, showcasing the feasibility of building an AI-powered legal chatbot.

SaulLM-7B \cite{SaulLM} is the first large language model (LLM) specifically designed for the legal domain, trained on a corpus of over 30 billion English legal tokens. It leverages the Mistral 7B \cite{mistral7b} architecture and undergoes a two-step training process: 1) continued pretraining on legal corpora to enhance its legal understanding and 2) instruction fine-tuning using legal datasets to improve performance on legal tasks. This work also introduce LegalBench-Instruct, an improved evaluation protocol for legal LLMs. They find that SaulLM-7B achieves state-of-the-art performance on LegalBench-Instruct, legal section of MMLU \cite{mmlu}, and perplexity measurement on various legal document types, showcasing its significant advancements in legal text comprehension and generation.

% what previous Legal LLM didn't do but we did
\textcolor{orange}{Despite an abundant resources of research done to establish a foundation research question on legal LLM in English, studies on challenges that apply LLM to a localized country such as Thailand remain unexplored. Our work aims to highlight these unexplored challenges and shed some light on current research gaps found in our experiments.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{RAG in legal practice}

{
RAG \cite{originalRAG} is an approach designed to enhance the performance of a large language model (LLM) by using a separate retrieval model to fetch relevant documents, which are then used as context to mitigate the hallucination problem. This section highlights several works that have applied RAG in legal practice.
}

\label{sec:raginlegal}
CBR-RAG \cite{CBR-RAG} is a framework that integrates Case-Based Reasoning (CBR) into RAG for legal question answering. CBR-RAG augments the original LLM query with contextually relevant cases retrieved using CBR’s indexing vocabulary and similarity knowledge containers, creating a richer prompt for the LLM. The paper evaluates CBR-RAG with different embeddings (BERT \cite{bert}, LegalBERT \cite{legalbert}, AnglEBERT \cite{anglebert}) and retrieval methods (intra, inter, hybrid) on the legal question-answering task. Results demonstrate that CBR-RAG significantly improves the quality of generated answers, particularly with AnglEBERT and hybrid embedding retrieval, highlighting the effectiveness of this approach for knowledge-intensive tasks.

% This paper \cite{clerc} introduce CLERC, a large-scale, high-quality dataset designed to tackle the crucial tasks of legal information retrieval and RAG of legal analyses. The authors address a significant gap in the field by providing a publicly available dataset built upon Caselaw Access Project (CAP) corpus\footnote{\url{https://case.law}} \cite{cap}, surpassing existing legal retrieval datasets in size. They employ an interdisciplinary approach, collaborating with legal professionals to define realistic tasks and evaluation metrics, ensuring the dataset's relevance to real-world legal applications. CLERC boasts a comprehensive preprocessing pipeline involving legal-specific tools and adheres to citation guidelines, ensuring high data quality. The authors propose novel task formulations and evaluation metrics, including distinct query types and metrics like Citation Recall, Precision, and False Positive rate specifically tailored for legal analysis generation. The paper presents comprehensive benchmarking results, evaluating various state-of-the-art retrieval and generation models on CLERC, revealing limitations of existing approaches and highlighting challenges posed by legal text. Despite the dataset's focus on U.S. Federal Law and the potential for OCR errors stemming from CAP corpus, the authors acknowledge these limitations and encourage future work to address geographical and domain specificity, as well as explore more sophisticated evaluation metrics for legal reasoning and factual accuracy in generated text. Furthermore, they raise important ethical considerations regarding potential biases in historical case law data, urging for responsible development and bias mitigation strategies in future AI systems.

CLERC \cite{clerc}, a large-scale, high-quality dataset built on the Caselaw Access Project (CAP) corpus\footnote{\url{https://case.law}} \cite{cap}, addresses the lack of publicly available datasets for legal information retrieval and RAG. Collaborating with legal professionals, the authors ensure real-world relevance through realistic tasks, novel query types, and tailored evaluation metrics like Citation Recall, Precision, and False Positive rate. The rigorous preprocessing pipeline, utilizing legal-specific tools and citation guidelines, guarantees high data quality. Comprehensive benchmarking reveals limitations of existing retrieval and generation models and highlights the challenges of legal text. Despite its focus on U.S. Federal Law and potential OCR errors from CAP, CLERC encourages future work on geographical and domain specificity, sophisticated evaluation metrics for legal reasoning and factual accuracy, and responsible AI development considering potential biases in historical case law data.

This \st{paper} \textcolor{orange}{work} \cite{Ajmi2024} offers a compelling exploration of how AI can bridge the "justice gap" by providing accessible legal information to those who lack traditional legal assistance. The paper highlights the limitations of LLMs and advocating for the implementation of RAG to enhance accuracy and effectiveness. Drawing from a real-world implementation in the Nevada court system, this work demonstrates the feasibility and potential impact of RAG-assisted chatbots, while maintaining a balanced perspective by acknowledging the limitations of AI and emphasizing the continued need for human legal professionals.

Several commercial solutions also use RAG to power their legal assistant product. For example, Lexis+ AI \cite{lexisnexis} and Westlaw \cite{thomsonreutersAIpoweredLegal} uses RAG to make sure that the assistant response contains a proper cited legal document. Some localized legal-based assistant solution, Thanoy \cite{thanoy}, also use RAG on Thai legal document to create a chatbot. \textcolor{orange}{Even though some of the aforementioned products claimed to mitigate hallucination problem via RAG, \cite{magesh2024hallucinationfreeassessingreliabilityleading} showed that hallucination still persists in some challenging case.}


\subsection{RAG vs Long-Context LLMs}

% intro to long context llm as alternative to RAG
% \textcolor{blue}
{
Apart from using RAG to mitigate LLM hallucination by retrieving relevant documents as context, Long Context LLMs (LCLMs) have been an alternative to this problem as well \cite{laban2024summaryhaystackchallengelongcontext, loft}. 
Since RAG contains multiple components, the end-to-end performance relies on multiple design choice such as the embedding model used, document chunking method, the number of document to be retrieved, and so forth, the question arise whether can we solve this tradeoffs among these choices. 
Long context is proposed as an alternative to RAG which removes the neccessity for tools such as retrieval model or reranking model where the whole documents are parsed as a context to the prompt without any needed of any retrieval module.
Google Gemini 1.5 \cite{gemini1.5} was the first model that introduce a very long context length support of up to 1M tokens in flash version and 2M tokens in pro version. 

% While there're also several attempts to expand the model context length for long context supports \cite{}. 
}

% highlight several studies comparing these two approaches
% \textcolor{blue}
{
Recent researches tried to make a qualitative study about the tradeoff between LCLM and RAG. \cite{needleinahaystack} was first introduce to pressure test the long context by inserting the context (needle) inside a very long document then ask the model to answer the question based on the inserted context. 
However, this work is still limited to evaluate LCLM solely under naive assumption. LongBench \cite{longbench}, L-eval \cite{leval} provides a more sophisticated long context benchmark compared to Needle-in-the-haystack. 
LOFT \cite{loft}, Self-route \cite{selfroute}, NEPAQuAD1.0 \cite{nepaquad}, and Summary of a haystack \cite{laban2024summaryhaystackchallengelongcontext} measures LCLMs performance compared to RAG as a baseline. Findings from these works suggested that although many proprietary model such as GPT4o \cite{gpt4o} and Claude 3.5 Sonnet \cite{claude3.5sonnet} are able to process up to over 128K and 200K context length respectively, under long context setup, the performance still can't match that of Gemini-1.5-pro \cite{leval, selfroute, laban2024summaryhaystackchallengelongcontext}. \cite{loft} also found that SQL reasoning is the only task RAG wins LCLM, and \cite{laban2024summaryhaystackchallengelongcontext} points out that these LCLMs are performance are still subpar to human in terms of both citation correctness and answer coverage. 
}

% not sure that this paragraph should be here or problem statement (or both)?
% tan-> this should be fine as a loose end problem highlighted in this background session
Although the literature extensively explores the comparative performance of RAG and LCLMs across various domains, a crucial area remains unexplored: their application and evaluation within the legal domain. Numerous studies have meticulously investigated the strengths and weaknesses of RAG and LCLMs in handling complex tasks like question answering and reasoning. However, these investigations typically focus on general-purpose datasets and benchmarks. To date, no research has directly compared the efficacy of RAG and LCLMs in tackling the unique challenges inherent to the legal field. This gap underscores the need for future research to delve into the comparative performance of these paradigms specifically within the legal domain, ultimately guiding the development of more effective and specialized legal NLP applications.