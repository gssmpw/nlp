\section{Experimental Setups}
\label{sec:evaluation}
% experiment setting @Non
To address the evaluation gap in Thai legal domain both on retrieval and generation task, we provide two tasks separately to address both capabilities of the system. We also conduct the experiment and showed that both RAG-based method as well as LCLM-based method are capable of performing both tasks and their results can be granularly evaluated. Apart from the retrieval and end-to-end task, we also design an experiment to measure the effectiveness of naive chunking strategy compared to the ideal one which divide the legislation by section.

This section begins with a description of the preprocessing steps applied to the WCX and Tax Case datasets to ensure high-quality test data. We then introduce the configurations of the Thai Legal QA systems used in our experiments. Following this, we outline a detailed evaluation framework designed to identify key bottlenecks encountered by large language models (LLMs) in the Thai legal domain. The framework starts with a description of the tasks and metrics developed to measure information loss from various naive chunking strategies. Next, we detail the retrieval task, its corresponding evaluation metrics, and the retrieval model configuration used in RAG-based systems. Finally, we present the end-to-end task that evaluates the overall system performance, including the methodology for initializing prompts for LLM-as-a-judge evaluations.

\subsection{Datasets}
\subsubsection{WCX Legal ThaiCCL Rag Dataset}
\label{subsubsec:wangchanx_dataset}

In our experiments, we utilize our WCX dataset to evaluate the efficacy of question-answering systems within the domain of Thai financial law. As described in section \ref{sec: wangchanx}, each dataset entry comprises a query simulating a user's inquiry, relevant contextual information extracted from the legal acts, and a corresponding human-annotated answer. 
% This chunk is a description of process used to filter data and getting the final dataset
Since the annotated contextual information includes the full content of relevant legal sections, we begin preprocessing the dataset by extracting only the names of the referenced legal sections from the annotations. Additionally, we ensure that each dataset entry contains only laws within the scope of the 36 Thai financial law codes under consideration and that no duplicate legal sections appear within a single entry.

As the annotated responses often include explanations or reasoning, which are outside the scope of this evaluation, we utilize a LLM to extract only the essential answers without the accompanying rationale. Finally, to align with the retriever fine-tuned on the train split of the dataset, only the test split is used for evaluation. The test set comprises 4,001 entries, referencing 21 out of 35 law codes present in the training set. Notably, 43\% of the queries pertain to the Civil and Commercial Code, and each query corresponds to a single relevant law section.
% Not sure if these should be moved to the dataset section.
% \st{The human-annotated answer is then further distilled by LLM into only essential part of the answer necessary for responding to the given query. The dataset contains both training data for fine-tuning embedding models and testing data. Only the test split of the dataset is used for evaluation. The test split contains 4k entries which use only 21 out of 35 legislation from the training set with 43\% being the Civil and Commercial Code and each query only consist of a single relevant law.}

\begin{figure}[H]
    \centering
    \begin{minipage}{0.46\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/wangchan_law_count.png}
    \end{minipage}%
    \hfill
    \begin{minipage}{0.26\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/wangchan_rel_count.png}
    \end{minipage}%
    \hfill
    \begin{minipage}{0.26\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/wangchan_query_count.png}
    \end{minipage}

    \caption{Visualization of the WCX Legal ThaiCCL Rag Dataset. From left to right: (1) Number of times each legislation is used as relevant context; (2) Histogram showing the amount of sections used as relevant context in a single entry; (3) Histogram showing the length of each query in characters.}
    \label{fig:combined}
\end{figure}

% A 200-entry sample from this dataset undergoes the following \textcolor{orange}{evaluation} process: 1) An LLM receives a simple chain-of-thought (CoT) prompt with few-shot examples and is tasked with answering the query and citing relevant legal sections. 2) Human annotators evaluate the LLM's response against the reference answer using the end-to-end (E2E) metrics described in Section \ref{subsubsec:e2e_metrics}. 3) An E2E evaluation prompt, using an LLM as a judge, generates initial E2E metric scores. 4) This prompt is iteratively refined until agreement between human-annotated and LLM-generated metrics reaches a predetermined threshold, indicating the evaluation prompt's readiness for final system evaluation.

\subsubsection{Tax Case Dataset}
\label{subsubsec:tax_dataset}

To evaluate the generalization capability of the system, we curated an additional dataset derived from publicly available resources in the Thai financial legal domain. Specifically, this dataset was created by scraping tax-related cases from the Revenue Department's official website\footnote{https://www.rd.go.th}. These cases represent authentic inquiries or requests (with personally identifiable information removed) submitted to the department. Each case includes the original inquiry or request, the official response, and metadata such as the case ID and submission date.

We extracted references to legislative sections mentioned in both the inquiry and the response as case attributes. The dataset was filtered to retain only cases referencing laws within the 36 Thai financial law codes and to eliminate duplicate references within individual entries. Some cases, however, involve inquiries requesting discretionary decisions from the department—such as extensions for tax deadlines or tax exemptions—rather than informational responses based on statutory interpretation. Since these cases are outside the scope of our work, which focuses on law-based reasoning, they were identified using a LLM and subsequently removed.

Additionally, to align with our evaluation objectives, the department's responses were condensed to essential answers, excluding detailed explanations and rationales. Finally, we restricted the dataset to cases from 2021 onward, reflecting the most recent legislative updates. The resulting tax case dataset consists of 50 cases, predominantly related to the Revenue Code, with an average of three referenced legal sections per case. This dataset provides a challenging testbed for evaluating system performance in a specialized domain requiring nuanced legal reasoning and multi-label retrieval.


\begin{figure}[H]
    \centering
    \begin{minipage}{0.34\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/tax_law_count.png}
    \end{minipage}%
    \hfill
    \begin{minipage}{0.31\textwidth}
        \centering
        \includegraphics[width=0.7\textwidth]{images/tax_rel_count.png}
    \end{minipage}%
    \hfill
    \begin{minipage}{0.31\textwidth}
        \centering
        \includegraphics[width=0.7\textwidth]{images/tax_query_count.png}
    \end{minipage}

    \caption{Visualization of the Tax Case Dataset. From left to right: (1) Number of times each legislation is used as relevant context; (2) Histogram showing the amount of sections used as relevant context in a single entry; (3) Histogram showing the length of each query in characters.}
    \label{fig:tax_combined}
\end{figure}

\subsection{Systems}
\label{subsec:systems}
\st{In this sections, systems focused on and evaluated in this work are described as follows:}\textcolor{orange}{This section describes different systems capable of conducting a Thai legal question answering that will be used in our experiments. Our systems include:}

\begin{enumerate}
    \item \textbf{Parametric Knowledge: }System with only instruction-tuned LLMs provided with no context and acts as a baseline for other systems due to it relying only on its training data.
    \item \textbf{Vanilla RAG: }System with traditional RAG components comprised of retrieval module with simple chunking method and generation module with no additional component. This system serves as the most basic approach for a RAG system in legal domain to be implemented.
    \item \textbf{Proposed RAG: }RAG system with our crafted components designed to be compatible with Thai law structure with specifically the aforementioned hierarchical chunking in retrieval module and referencer to retrieved all cross-referenced document as additional contexts.
    \item \textbf{RAG with golden context: }System with only generation module provided with prompt and ground truth context to measure the upper bound of the expected performance in the case of RAG system.
    \item \textbf{Long-Context LLM: }System with solely LLMs provided with the full 36 Thai legal acts as context alongside the prompt. Due to the number of tokens being roughly round 1.2 million tokens, the only model capable of ingesting the full context is Gemini-1.5 pro and only a subset of WCX stratified by the relevant law codes is evaluated due to budget limitation.
\end{enumerate}

\subsection{Chunking Selection}
\label{subsec:chunking_select}

A critical design decision when implementing a Retrieval-Augmented Generation (RAG) system is determining the appropriate document chunking strategy. In the context of the Thai legal domain, it is essential to incorporate the hierarchical structure of legislation into the chunking process to preserve the contextual relationships and structural integrity of the legal text.

This experiment aims to quantify the information loss associated with naive chunking strategies compared to an ideal chunking strategy, where legislation is segmented by legal sections. Specifically, we evaluate three common naive chunking approaches using varying chunk sizes and overlaps. The objective is to determine which naive strategy performs best according to our custom-defined metrics and to measure the performance gap between the best-performing naive strategy and the ideal section-based chunking approach. The chunking strategies evaluated in this experiment are as follows:
% In this work, we also aim to evaluate various chunking strategies of legal document as compared to the hierarchical chunking which we proposed and assumed to yield the best performance when being incorporated into RAG system due to it retaining the overall structure of the documents. Specifically,  we employ 3 chunking types with various chunk size and chunk overlap. The chunking types we used are as follows:
\begin{enumerate}
    \item \textbf{Character Chunking: }Chunking purely based on the number of characters.
    \item \textbf{Recursive Chunking: }Chunking based on various separators that may indicate the structure of the documents.
    \item \textbf{Line Chunking: }Chunking based on only newline characters.
\end{enumerate}
The chunk sizes we use in this experiment starts from 212 characters which is the 25th percentile of all sections' length to 553 characters which is the 75th percentile. We also include other chunk sizes in between the two value and one after the 75th percentile. As for the chunk overlap, the values of 50, 100, 150 and 200 are experimented with. For each chunking strategy, we first chunk each legal documents with metadata containing which sections it covers and whether it fully covers each of them or not. Then, the chunks that does not contain at least one full section are discarded to simplify retrieval process and make the comparison of same top k retrieval across each chunking strategies possible. Then, the following metrics are calculated for each chunking strategy
\begin{enumerate}
    \item \textbf{Section/Chunk: }Average amount of sections covered whether partially or fully in each chunk. For the hierarchical chunking, this metric is 1.0.
    \item \textbf{Chunk/Section: }Average amount of chunks that partially or fully covered any given section. For the hierarchical chunking, this metric is 1.0.
    \item \textbf{Fail Chunk Ratio: }Ratio of chunks that does not fully covered any section over all chunks. For the hierarchical chunking, this metric is 0.
    \item \textbf{Fail Section Ratio: }Ratio of sections that are not fully covered by any single chunk over all sections. For the hierarchical chunking, this metric is 0.
    \item \textbf{Uncovered Section Ratio: }Ratio of sections that are neither fully nor partially covered by any single chunk over all sections. For the hierarchical chunking, this metric is 0.
\end{enumerate}
The best chunking strategy will be applied as the retrieval component in Vanilla RAG system and compared with other systems.

\subsection{Retrieval Evaluation}
\label{subsec:re}

Another crucial part of a RAG system is the retrieval component. In this section, we describe how we set up the experiments on evaluating retrieval performance across various models on both dataset. Given a query from either WCX dataset or Tax Case dataset, the retrieval model is tasked to retrieved a top-k relevant documents from the available corpus. From this experiment, we aim to gather useful insights that give us a glimpse of problem of the retrieval component in RAG in Thai Legal domain such as the performance of each of the widely-used model on different domain and the effect of fine-tuned retrieval model on both in and out-of-domain data.

Furthermore, to our best knowledge, traditional retrieval metrics such as Mean Reciprocal Rank (MRR), Recall and Hit-rate are commonly employed in a single-label retrieval task. However, due to the presence of queries with multiple relevant documents in our dataset, it is crucial we adapt these metrics to ones suitable for measuring multi-label retrieval task.

\subsubsection{Models}
\label{subsubsec:re_models}
In this experiment, we choose 9 widely used retrieval models ranging from simple algorithm, ColBERT-based, hybrid-based, decoder-based, closed-source and long-context retriever. Specifically:
\begin{enumerate}
    \item \textbf{BM25} \cite{bm25}: A retrieval algorithm that ranks a set of documents based on the appearance of the terms in a given query in a document. This ranking algorithm does not consider the semantic of the document or the query and, thus, use as a baseline in our experiment.
    \item \textbf{JinaAI Colbert V2} \cite{jina-v2}: A ColBERT-based\cite{colbert} retrieval model that provides token-level embeddings and late interaction for information retrieval task.
    \item \textbf{JinaAI Embeddings V3} \cite{jina-v3}: A multilingual multi-task text embedding model applicable for multiple NLP tasks including retrieval, clustering, classification and text matching. Its architecture is based on an adapted version of XLM-Roberta and produce dense embeddings.
    \item \textbf{NV-Embed V1} \cite{nvembed}:  A generalist decoder-based embedding model trained with LLM as base model with novel techniques including latent vectors attention and two-stage instruction tuning aimed to enhance both retrieval and non-retrieval tasks.
    \item \textbf{BGE-M3} \cite{bge-m3}:  An multilingual embedding model with multi-functionality including dense retrieval, multi-vector retrieval and sparse retrieval. In this work, we weigh three scores obtained from three types of embedding for the retrieval task.
    \item \textbf{BGE-M3 human-rerank finetuned}: A version of BGE-M3 that is finetuned on the train split of our own Wangchan legal QA dataset. The positive context of each query is constructed from human reranking the retrieved context from BGE-M3. This model will be referred to as Human-Finetuned BGE-M3.
    \item \textbf{BGE-M3 auto-rerank finetuned}: A version of BGE-M3 that is finetuned on the train split of our own Wangchan legal QA dataset. The positive context of each query is constructed from using BGE-M3 Reranker V2\cite{} to compute relevant score between the query and each retrieved contexts. If the computed score is lower than a threshold of 0.8, the context is not considered positive. This model will be referred to as Auto-Finetuned BGE-M3.
    \item \textbf{Cohere Embeddings\footnote{https://cohere.com/embed}: } A commercialized text embedding model that produces dense embedding for multiple tasks including retrieval, classification and clustering. We use embed-multilingual-v3.0 in our experiment. 
    \item \textbf{Long-Context Retriever: } A long-context LLM is also evaluated on its capability to act as retriever in a RAG system as well. Specifically, the LLM (Gemini-1.5-pro) is provided with the content of 35 law codes and tasked to retrieve the most relevant sections given a query.
    
\end{enumerate}

\subsubsection{Metrics}
\label{subsubsec:re_metrics}

\textcolor{orange}{Since most retrieval metrics, such as Hit Rate, Recall, and MRR, are designed for a single positive label, we modified these metrics to accommodate a multi-label scenario, enabling their use with datasets like ours that feature multiple positive labels.}

\textbf{Hit-rate: }Let $N$ be the number of \st{samples}\textcolor{orange}{documents} in a dataset and $k$ represents the number of top retrieved documents being evaluated, $T_i$ represents the set of \st{true}\textcolor{orange}{positive} relevant documents and $R_i^k$ represents the top-k ranked retrieved documents. Hit-rate(HR) can be defined as

\begin{equation}
    \text{HitRate@k}=\frac{1}{N}\sum_{i=1}^{N}\mathbb{I}(R_i^k\subseteq T_i) 
\end{equation}

\textbf{Multi Hit-rate: } \st{We also proposed a multi-label version of hit-rate as well.}\textcolor{orange}{To ensure that hit rate supports multi-positive setup. We defined a multi-label version of hit rate where} \st{This version of} the metric requires the system to retrieve all true relevant documents to be considered a hit. Formally

\begin{equation}
    \text{Multi-HitRate@k}=\frac{1}{N}\sum_{i=1}^{N}\mathbb{I}(T_i\subseteq R_i^k)
\end{equation}

\textbf{Recall: }As with Hit-rate, recall can be defined as

\begin{equation}
    \text{Recall@k}=\frac{1}{N}\frac{\sum_{i=1}^N |T_i\cap R_i^k|}{\sum_{i=1}^N |T_i|}
\end{equation}

\textbf{MRR and Multi-MRR: }In traditional calculation of MRR with multi-label ground truth, the metrics can be written as follows:

\begin{equation}
    \text{MRR@k} = \frac{1}{N}\sum_{i=1}^N \frac{1}{\text{argmax}(T_i \cap R_i^k)} \label{eq:single_mrr}
\end{equation}

where $\text{argmax}(T_i\cap R_i^k)$ represents the highest rank number of correctly retrieved documents. This calculation assumes that any of the document in the ground truth set is a sufficient context for the system to answer the question. However, this assumption is not true especially in legal domain where, sometimes, all relevant laws must be retrieved in order for the system to be able to answer the question. Therefore, the equation \ref{eq:single_mrr} is augmented to Multi-MRR as follows:

\begin{equation}
    \text{Multi-MRR@k}=\frac{1}{N}\sum_{i=1}^N(\frac{1}{|T_i|}\sum_{j=1}^{|T_i\cap R_i^k|}(\frac{1}{rank(j) - j + 1})\times \text{Recall@k}_{i})
\end{equation}

where $rank(j)$ represents the rank of the $j$ matched documents. This augmented calculation produces highest scores when all of the relevant documents are retrieved and ranked at the top and, thus, can provide an evaluation metric that closely align with our performance goals.


\subsection{End-to-End Evaluation}
\label{subsec:e2e}

For end-to-end evaluation, we adapt the task introduced by Laban et al. \cite{laban2024summaryhaystackchallengelongcontext} to align with our specific domain. While the original task focused on summarizing long contexts into bullet points with references, our dataset’s question-answering (QA) nature necessitated a shift in focus. Specifically, given a query, the system is required to output relevant and accurate answer accompanied by a list of relevant legal sections which it uses as a reference to reach the final answer. This task is desgined to measure both the system's ability to choose relevant legal sections and its ability to perform legal reasoning from the chosen sections to reach its complete, relevant and accurate answer.

\subsubsection{Metrics}
\label{subsubsec:e2e_metrics}

Building upon the metric suite employed by Laban et al. \cite{laban2024summaryhaystackchallengelongcontext}, an additional metric inspired by Dahl et al. \cite{Dahl_2024} is introduced to quantify hallucination rates. The following metrics are employed for end-to-end evaluation:

\textbf{Coverage: } This metric assesses the degree to which the generated answer semantically encompasses the ground truth response. A perfect coverage of all points mentioned in the golden answer yields a score of 100, partial coverage results in a score of 50, and complete divergence is assigned a score of 0. Automatic evaluation is conducted using an LLM as a judge, following established methodologies (\cite{phan2024ragvslongcontext}, \cite{es2023ragasautomatedevaluationretrieval}).

\textbf{Citation: } This metric evaluates the system’s ability to ground answers in the provided context and its effectiveness in long-context retrieval. Specifically, given the ground truth answer, the generated response, and the relevant legal sections, we extract cited sections from the generated answer and compute the Recall, Precision and F1-score against the actual relevant sections. Scores range from 0 to 1.

\textbf{Contradiction: } To assess hallucination, a simplified version of Dahl et al.'s \cite{Dahl_2024} approach is adopted. Rather than generating multiple answer trajectories and assessing whether there are contradictions among the generated trajectories, the generated answer is directly compared to the ground truth. A contradiction score of 0 indicates alignment between the two, while a score of 1 signifies direct disagreement.

In order to ensure the quality of metrics provided via LLM-as-a-judge methods including coverage score and citation score, 150 entries from the Tax Case dataset without filtering cases from 2021 onwards and 200 entries from the WCX dataset are sampled. An LLM is, then, prompt with the sampled queries to answer the inquiries along with cited laws. The LLM answer is, then, parsed to another LLM alongside the ground truth answer to generate the coverage and citation score. The generated coverage and citation scores are then compared with the ones assigned by human annotator given the queries, generated answer and ground truth answer. The agreement between LLM-generated scores and human-annotated scores are measured by precision, recall and f1-score. The prompt for LLM-as-a-judge is then adjusted until it yields more than 0.8 f1-score between human-annotated score and LLM-generated ones.

The final agreement score between human-annotated score and LLM-generated ones are displayed in Table \ref{table: agreement_metric}. The LLM-as-a-judge score is generated by gpt-4o-2024-08-06 model with temperature of 0.3.

\begin{table}[H]
\centering

\caption{Table displaying the weighted average precision, recall and f1-score between metrics computed by LLM and annotated by human experts}
\label{table: agreement_metric}

\begin{tabular}{@{}c|ccccc@{}}
\toprule
Metric                         & Dataset & Precision & Recall & F1-score & Support \\ \midrule
\multirow{2}{*}{Coverage}      & Tax     & .83       & .83    & .83      & 150     \\
                               & WCX     & .88       & .88    & .88      & 200     \\ \midrule
\multirow{2}{*}{Contradiction} & Tax     & .92       & .91    & .91      & 150     \\
                               & WCX     & .98       & .97    & .98      & 200     \\ \bottomrule
\end{tabular}

\end{table}
