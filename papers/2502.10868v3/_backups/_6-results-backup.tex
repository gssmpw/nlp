\section{Result}
\subsection{Chunking}
\label{subsec: chunking}

We define several metrics to evaluate chunking performance. \textbf{Fail Chunk Ratio} represents the proportion of chunks not fully containing a single section. \textbf{Fail Section Ratio} represents the proportion of sections not fully contained within a single chunk. \textbf{Uncovered Section Ratio} represents the proportion of sections not covered by any chunk. \textbf{Sections/Chunk} (sections-per-chunk) indicates the average number of sections contained within a chunk, while \textbf{Chunks/Section} (chunks-per-section) indicates the average number of chunks required to cover a section. Ideally, Fail Chunk Ratio, Fail Section Ratio, and Uncovered Section Ratio should be minimized, while Sections/Chunk and Chunks/Section should be close to 1.


For chunking strategies, the chunking strategy that can generate chunks that resembles the chunks from our hierarchical chunking strategy the most is line chunking (see table \ref{table: chunking_by_type}). However, it can be noted that, in every strategy, around 30\% of sections are not mentioned at all in any chunks and at least 41.7\% of sections not fully covered in a single chunk. Upon further inspection, there are approximately 20\% of all sections that cannot be fully covered in a single chunk in any chunking strategy due to their extended length resulting in the retrieval model needing to retrieve multiple chunks to gather sufficient context. For example,

% \begin{center}
\begin{quote}
 %% Lorem ipsum from http://lorem.in.th/
\begin{thai}
% \centering
~~~~พระราชกำหนดการประกอบธุรกิจสินทรัพย์ดิจิทัล พ.ศ. 2561 มาตรา 44 ให้สันนิษฐานไว้ก่อนว่าบุคคลดังต่อไปนี้ ซึ่งมีพฤติกรรมการซื้อขายโทเคนดิจิทัล หรือเข้าผูกพันตามสัญญาซื้อขายล่วงหน้าที่เกี่ยวข้องกับโทเคนดิจิทัลที่ผิดไปจากปกติวิสัยของตนเป็นบุคคลซึ่งรู้หรือครอบครองข้อมูลภายในตามมาตรา 42

~~~~~~~~(1) ผู้ถือโทเคนดิจิทัลเกินร้อยละห้าของโทเคนดิจิทัลที่จำหน่ายได้แล้วทั้งหมดในแต่ละรุ่นของผู้เสนอขายโทเคนดิจิทัลโดยให้นับรวมโทเคนดิจิทัลที่ถือโดยคู่สมรสหรือผู้ที่อยู่กินด้วยกันฉันสามีภริยาและบุตรที่ยังไม่บรรลุนิติภาวะของผู้ถือโทเคนดิจิทัล

~~~~~~~~(2) กรรมการ ผู้บริหาร ผู้มีอำนาจควบคุมกิจการ พนักงาน หรือลูกจ้างของกิจการในกลุ่มของผู้เสนอขายโทเคนดิจิทัล ซึ่งอยู่ในตำแหน่งหรือสายงานที่รับผิดชอบข้อมูลภายในหรือที่สามารถเข้าถึงข้อมูลภายในได้

~~~~~~~~(3) บุพการี ผู้สืบสันดาน ผู้รับบุตรบุญธรรม หรือบุตรบุญธรรมของบุคคลตามมาตรา 43

~~~~~~~~(4) พี่น้องร่วมบิดามารดาเดียวกัน หรือพี่น้องร่วมบิดาหรือร่วมมารดาเดียวกันของบุคคลตามมาตรา 43

~~~~~~~~(5) คู่สมรสหรือผู้ที่อยู่กินด้วยกันฉันสามีภริยาของบุคคลตามมาตรา 43 หรือบุคคลตาม (3) หรือ (4)

~~~~กิจการในกลุ่มของผู้เสนอขายโทเคนดิจิทัลตาม (2) หมายความว่า บริษัทใหญ่ บริษัทย่อย หรือบริษัทร่วมของผู้เสนอขายโทเคนดิจิทัล ทั้งนี้ ตามหลักเกณฑ์ที่คณะกรรมการ ก.ล.ต. ประกาศกำหนด
\end{thai}
\end{quote}

    % \textbf{Quote 1:} Example of a section that cannot be contained in a single chunk in any chunking strategy due to its extended length.
% \end{center}

\begin{table}[H]
\centering
\resizebox{\textwidth}{!}{%
\renewcommand{\arraystretch}{1.3} % This increases the cell height by 1.5 times
\small % or \scriptsize
\begin{tabular}{@{}lccccc@{}}
\toprule
Chunking Strategy & \multicolumn{1}{l}{Section/Chunk $\rightarrow$1} & \multicolumn{1}{l}{Chunk/Section $\rightarrow$1} & \multicolumn{1}{l}{Fail Chunk Ratio $\downarrow$} & \multicolumn{1}{l}{Fail Section Ratio $\downarrow$} & \multicolumn{1}{l}{Uncovered Section Ratio $\downarrow$} \\ \midrule
Hierarchical  & \textbf{1.000}   & \textbf{1.000}    & \textbf{0.000}  & \textbf{0.000}  & \textbf{0.000}                                       \\
Character  & 3.098                             & 1.710                              & 0.819                                & 0.675                                  & 0.397                                       \\

Line       & \underline{1.689}                    & \underline{1.234}                    & \underline{0.658}                       & \underline{0.417}                         & \underline{0.294}                              \\

Recursive  & {1.793}                       &{1.27}                        & {0.741}                          & {0.504}                            & {0.381}                                 \\ \bottomrule
\end{tabular}
}
\caption{Chunking result by type of chunking}
\label{table: chunking_by_type}
\end{table}



% Table \ref{table: chunking_by_size} displays the metrics of each line chunking strategy in various chunk size. Higher amount of text is included in higher chunk size which results in higher section per chunk and chunk per section. Larger chunk also means that it is a lot less likely that any chunk does not fully cover any section or that any section is not fully covered in a single chunk. From observing the result in the table, we decide that the prioritised metric would be \textit{fail chunk ratio}, \textit{fail section ratio} and \textit{uncovered section ratio} then follows by both \textit{section/chunk} and \textit{chunk/section}. Therefore, the desirable chunk size to be employed in vanilla RAG should be 553 characters since it has the second lowest fail chunk ratio, fail section ratio and uncovered section ratio while not having as high section/chunk and chunk/section as chunk size of 600 characters.

Table \ref{table: chunking_by_size} presents the performance of various line chunking strategies across different chunk sizes. As expected, larger chunk sizes correlate with more text per chunk, resulting in higher Sections/Chunk and Chunks/Section values. Larger chunks also reduce the likelihood of incomplete section coverage within a single chunk. Based on these results, we prioritize minimizing {Fail Chunk Ratio}, {Fail Section Ratio}, and {Uncovered Section Ratio}, followed by Sections/Chunk and Chunks/Section. A chunk size of 553 characters offers the second-lowest values for the three prioritized metrics while maintaining reasonably low Sections/Chunk and Chunks/Section compared to a chunk size of 600 characters. Therefore, we select 553 characters as the optimal chunk size for vanilla RAG.

\begin{table}[H]
\centering
\resizebox{\textwidth}{!}{%
\renewcommand{\arraystretch}{1.5} % This increases the cell height by 1.5 times
% \small % or \scriptsize
\begin{tabular}{@{}lccccc@{}}
\toprule
Chunk Size & \multicolumn{1}{l}{Section/Chunk $\rightarrow$1} & \multicolumn{1}{l}{Chunk/Section $\rightarrow$1} & \multicolumn{1}{l}{Fail Chunk Ratio $\downarrow$} & \multicolumn{1}{l}{Fail Section Ratio $\downarrow$} & \multicolumn{1}{l}{Uncovered Section Ratio $\downarrow$} \\ \midrule

212        & \textbf{1.264}                    & \textbf{1.121}                    & 0.8                                  & 0.551                                  & 0.487                                       \\

250        & \underline{1.373}                       & \underline{1.174}                       & 0.768                                & 0.509                                  & 0.425                                       \\

300        & 1.497                             & 1.227                             & 0.725                                & 0.463                                  & 0.356                                       \\

350        & 1.613                             & 1.264                             & 0.684                                & 0.422                                  & 0.304                                       \\

466        & 1.87                              & 1.284                             & 0.597                                & 0.36                                   & 0.207                                       \\

553        & 2.048                             & 1.285                             & \underline{0.528}                          & \underline{0.315}                            & \underline{0.149}                                 \\

600        & 2.16                              & 1.285                             & \textbf{0.5}                         & \textbf{0.301}                         & \textbf{0.13}                               \\ \bottomrule
\end{tabular}
}
\caption{Line chunking strategy result by chunk size}
\label{table: chunking_by_size}
\end{table}

% Next, table \ref{table: chunking_by_overlap} displays the evaluation of line chunking strategies across multiple value of chunking overlap. Again, higher chunk overlap results in larger chunk of text which decreases the value of section per chunk and chunk per section. It also can be noted that the fail chunk ratio, fail section ratio and uncovered sections ratio all decreases as the value of chunk overlap increases. Therefore, we choose the chunk overlap value of 50 characters as the optimal chunk overlap for our vanilla RAG.

Table \ref{table: chunking_by_overlap} presents the evaluation of line chunking strategy with varying overlap. As overlap increases, the effective chunk size also increases, leading to lower Sections/Chunk and Chunks/Section values. Furthermore, increasing overlap reduces Fail Chunk Ratio, Fail Section Ratio, and Uncovered Section Ratio. Based on these results, we select an overlap of 50 characters as the optimal value for our vanilla RAG implementation.

\begin{table}[H]
\centering
\resizebox{\textwidth}{!}{%
\renewcommand{\arraystretch}{1.5} % This increases the cell height by 1.5 times
\small % or \scriptsize
\begin{tabular}{@{}lccccc@{}}
\toprule
Chunk Overlap & \multicolumn{1}{l}{Section/Chunk $\rightarrow$1} & \multicolumn{1}{l}{Chunk/Section $\rightarrow$1} & \multicolumn{1}{l}{Fail Chunk Ratio $\downarrow$} & \multicolumn{1}{l}{Fail Section Ratio $\downarrow$} & \multicolumn{1}{l}{Uncovered Section Ratio $\downarrow$} \\ \midrule

50            & \textbf{1.627}                    & \textbf{1.153}                    & \textbf{0.654}                       & 0.423                                  & 0.303                                       \\

100           & \underline{1.666}                       & \underline{1.183}                       & \underline{0.659}                          & 0.422                                  & 0.299                                       \\

150           & 1.717                             & 1.254                             & 0.661                                & \underline{0.417}                            & \underline{0.292}                                 \\

200           & 1.747                             & 1.347                             & 0.657                                & \textbf{0.407}                         & \textbf{0.283}                              \\ \bottomrule
\end{tabular}
}
\caption{Line chunking result by chunk overlap}
\label{table: chunking_by_overlap}
\end{table}

% Lastly, the stats of amount of sections fully covered in each chunk using the chosen chunking strategy of line chunking with chunk size of 553 and chunk overlap of 50 are displayed in table \ref{table: chunking_map_stats}.

Finally, Table \ref{table: chunking_map_stats} presents statistics on section coverage within each chunk using the selected line chunking strategy (chunk size: 553 characters, overlap: 50 characters).

\begin{table}[H]
\centering
\begin{tabular}{@{}
>{\columncolor[HTML]{FFFFFF}}l |
>{\columncolor[HTML]{FFFFFF}}l @{}}
\toprule
Count & 2610                                              \\ \midrule
Mean  & \multicolumn{1}{c}{\cellcolor[HTML]{FFFFFF}1.331} \\ \midrule
STD   & \multicolumn{1}{c}{\cellcolor[HTML]{FFFFFF}0.577} \\ \midrule
Min   & \multicolumn{1}{c}{\cellcolor[HTML]{FFFFFF}1.000} \\ \midrule
25\%  & \multicolumn{1}{c}{\cellcolor[HTML]{FFFFFF}1.000} \\ \midrule
50\%  & 1.000                                             \\ \midrule
75\%  & 2.000                                             \\ \midrule
Max   & 7.000                                            
\end{tabular}
\caption{Statistics on section coverage within each chunk}
\label{table: chunking_map_stats}
\end{table}

\subsection{Retrieval Results}
% TODO
% ใส่ table show เวลา inference ด้วย
\label{subsec:re_result}
\subsubsection{Retrieval Result on WangchanX Legal ThaiCCL Rag Dataset}
\label{subsubsec:re_result_wangchan}
% Table \ref{table: retrieval_wangchan} displays the overall retrieval results of all 8 retrieval models on WangchanX Legal ThaiCCL Rag Dataset (WCX) dataset with heirachical chunking. Due to the dataset only containing queries with one positive context, the multi-hit-rate and multi-mrr both have the same value as their single-lable counterparts. 

Table \ref{table: retrieval_wangchan} presents the retrieval performance of eight out of nine models excluding long-context retriever on the WangchanX Legal ThaiCCL Rag (WCX) Dataset using hierarchical chunking. Because each query has only one positive context, the multi-hit-rate and multi-MRR metrics are equivalent to their single-label counterparts.


% From the result table, the model that performs the best on this dataset is the human-reranked fine-tuned BGE-M3 which gains the performance of MRR@5 of 0.796 followed by the auto-reranked fine-tuned BGE-M3 with MRR@5 of 0.791 and the base version of BGE-M3 with MRR@5 of 0.764. This is not surprising since BGE-M3 utilises three types of embeddings to calculate relevancy of a given document and query and the model that is fine-tuned with in-domain data should always perform better than the pre-trained model. It can be noted that the auto-reranked version of fine-tuned BGE-M3 achieves approximately the same performance as the human-reranked one without the expensive cost of human annotation.

The best performing model is the human-reranked fine-tuned BGE-M3 \footnote{https://huggingface.co/airesearch/WangchanX-Legal-ThaiCCL-Retriever}, achieving an MRR@5 of 0.805. Close behind are the auto-reranked fine-tuned BGE-M3 (0.800 MRR@5) and the base BGE-M3 \footnote{https://huggingface.co/BAAI/bge-m3} (0.579 MRR@5). BGE-M3's strong performance is likely due to its use of three embedding types for relevance calculation, further enhanced by fine-tuning on in-domain data. Notably, the auto-reranked version nearly matches human-reranked performance without requiring costly human annotation.

% Following the top two models in terms of the performance is the Cohere multilingual embedding v3 model that is available commercially. Then, it is followed by JINA v2 and JINA v3 models which are ColBERT-based and Dense embeddings model that are less computationally expensive. Lastly, the NV-Embed v1 performs the worst out of all non-baseline models with MRR@5 of only 0.572. We suspect that it might be due to it being a decoder-based model which relied heavily on the prefix instruction prompt. Overall, the retrieval performance on WCX dataset is great and every models performs similarly except NV-Embed V1 and BM25.

The commercially available {Cohere Multilingual Embedding v3}\footnote{https://cohere.com/blog/introducing-embed-v3} follows the top BGE-M3 models, succeeded by the computationally efficient ColBERT-based and dense embedding models, JINA ColBERT v2 \footnote{https://huggingface.co/jinaai/jina-colbert-v2} and JINA embeddings v3 \footnote{https://huggingface.co/jinaai/jina-embeddings-v3}. NV-Embed v1 \footnote{https://huggingface.co/nvidia/NV-Embed-v1} exhibits the lowest performance among non-baseline models (0.713 MRR@5), potentially due to its decoder-based architecture and reliance on prefix instruction prompts. Overall, retrieval performance on the WCX dataset is strong, with most models exhibiting similar results except for NV-Embed v1 and BM25.


% Table for @5 golden retrieval
\begin{table}[!h]
\centering
\caption{Retrieval Evaluation Result on WangchanX Legal ThaiCCL Rag Dataset with hierarchical chunking}
\renewcommand{\arraystretch}{1.5} % This increases the cell height by 1.5 times
\label{table: retrieval_wangchan}
\begin{tabular}{@{}clccccc@{}}
\toprule
Top-K & Model                  & \multicolumn{1}{c}{HR@k} & \multicolumn{1}{c}{Multi HR@k} & \multicolumn{1}{c}{Recall@k} & \multicolumn{1}{c}{MRR@k} & \multicolumn{1}{c}{Multi MRR@k} \\ \midrule
k=1   & BM25                   & .481                     & .481                           & .481                         & .481                      & .481                            \\
      & JINA V2                & .681                     & .681                           & .681                         & .681                      & .681                            \\
      & JINA V3                & .587                     & .587                           & .587                         & .587                      & .587                            \\
      & NV-Embed V1            & .492                     & .492                           & .492                         & .492                      & .492                            \\
      & BGE-M3                 & .700                     & .700                           & .700                         & .700                      & .700                            \\
      & Human-Finetuned BGE-M3 & \textbf{.735}            & \textbf{.735}                  & \textbf{.735}                & \textbf{.735}             & \textbf{.735}                   \\
      & Auto-Finetuned BGE-M3  & \underline{.731}               & \underline{.731}                     & \underline{.731}                   & \underline{.731}                & \underline{.731}                      \\
      & Cohere                 & .676                     & .676                           & .676                         & .676                      & .676                            \\ \midrule
k=5   & BM25                   & .658                     & .658                           & .658                         & .548                      & .548                            \\
      & JINA V2                & .852                     & .852                           & .852                         & .750                      & .750                            \\
      & JINA V3                & .821                     & .821                           & .821                         & .681                      & .681                            \\
      & NV-Embed V1            & .713                     & .713                           & .713                         & .579                      & .579                            \\
      & BGE-M3                 & .880                     & .880                           & .880                         & .773                      & .773                            \\
      & Human-Finetuned BGE-M3 & \textbf{.906}            & \textbf{.906}                  & \textbf{.906}                & \textbf{.805}             & \textbf{.805}                   \\
      & Auto-Finetuned BGE-M3  & \underline{.900}               & \underline{.900}                     & \underline{.900}                   & \underline{.800}                & \underline{.800}                      \\
      & Cohere                 & .870                     & .870                           & .870                         & .754                      & .754                            \\ \midrule
k=10  & BM25                   & .715                     & .715                           & .715                         & .556                      & .556                            \\
      & JINA V2                & .889                     & .889                           & .889                         & .755                      & .755                            \\
      & JINA V3                & .875                     & .875                           & .875                         & .688                      & .688                            \\
      & NV-Embed V1            & .776                     & .776                           & .776                         & .587                      & .587                            \\
      & BGE-M3                 & .919                     & .919                           & .919                         & .778                      & .778                            \\
      & Human-Finetuned BGE-M3 & \textbf{.938}            & \textbf{.938}                  & \textbf{.938}                & \textbf{.809}             & \textbf{.809}                   \\
      & Auto-Finetuned BGE-M3  & \underline{.934}               & \underline{.934}                     & \underline{.934}                   & \underline{.804}                & \underline{.804}                      \\
      & Cohere                 & .912                     & .912                           & .912                         & .760                      & .760                            \\ \bottomrule

\end{tabular}
\end{table}


% ใส่กราฟ MRR เทียบกะ k ของทุก model ดูว่าอันไหนเพิ่มเยอะสุด
% Despite the excellent overall performance, there is a discrepancy between hit-rate and MRR which means that even though there is a relevant documents in the retrieved content, it sometimes is not ranked in the first position which might bring down the end-to-end performance. Furthermore, there is a  slight difference of increase in performance in each model caused by increase in top-k as shown in figure \ref{fig: wangchan_mrr_vs_topk}.

Despite the strong overall retrieval performance, a discrepancy between hit-rate and MRR suggests that while relevant documents are often retrieved, they are not consistently ranked first. This ranking inconsistency could negatively impact end-to-end performance. Furthermore, Figure \ref{fig: wangchan_mrr_vs_topk} reveals varying performance improvements across models as the top-k value increases, indicating differences in their ability to rank relevant documents higher within larger retrieval sets.

\begin{figure}[!h]
  \centering
  \includegraphics[width=0.7\textwidth]{images/wangchan_multimrr_topk.png}
  % \caption{Line chart showing increase in performance vs number of retrieved documents for each retrieval models}
  \caption{Multi-MRR vs. Number of Retrieved Documents (Top-k) for Each Retrieval Model}
  \label{fig: wangchan_mrr_vs_topk}
\end{figure}



% ใส่ ตาราง show performance by law code
% Lastly, the performance of the human-finetuned BGE-M3 on each law code is displayed in table \ref{table: retrieval_wangchan_by_code} with Support showing the amount of queries contain section in a given law code as relevant documents and Section Count showing the amount of sections in a given law code.

Finally, Table \ref{table: retrieval_wangchan_by_code} details the performance of the human-fine-tuned BGE-M3 model across individual legislation. The "Support" column indicates the number of queries for which a section within the given law code is relevant, while "Section Count" represents the total number of sections within that law code.

\begin{table}[]
\centering
\caption{Retrieval Evaluation Result by law code on WangchanX Legal ThaiCCL Rag Dataset}
\resizebox{\textwidth}{!}{%
\renewcommand{\arraystretch}{1.4} % This increases the cell height by 1.5 times
\label{table: retrieval_wangchan_by_code}
\begin{tabular}{@{}p{0.6\textwidth}cccc@{}}
\toprule
Law Code                                                                                                                              & \multicolumn{1}{c}{Multi MRR@10} & \multicolumn{1}{c}{Recall@10} & \multicolumn{1}{c}{Support} & \multicolumn{1}{c}{Section Count} \\ \midrule
Business Registration Act, B.E. 2499                                                                                                  & \textbf{0.955}                   & \textbf{1.00}                 & 11                          & 22                                \\
Foreign Business Act, B.E. 2542                                                                                                       & \underline{0.900}                      & \textbf{1.00}                 & 36                          & 46                                \\
Trade Association Act, B.E. 2509                                                                                                      & 0.883                            & 0.925                         & 40                          & 56                                \\
Accounting Profession Act, B.E. 2547                                                                                                  & 0.862                            & 0.922                         & 64                          & 78                                \\
Secured Transactions Act, B.E. 2558                                                                                                   & 0.848                            & 0.932                         & 74                          & 91                                \\
Revenue Code                                                                                                                          & 0.836                            & 0.911                         & 484                         & 361                               \\
Provident Fund Act, B.E. 2530                                                                                                         & 0.834                            & \underline{0.972}                   & 36                          & 51                                \\
Emergency Decree on Digital Asset Businesses, B.E. 2561                                                                               & 0.827                            & 0.962                         & 78                          & 100                               \\
Public Limited Companies Act, B.E. 2535                                                                                               & 0.825                            & 0.919                         & 186                         & 231                               \\
Act on Offenses Relating to Registered Partnerships, Limited Partnerships, Companies Limited, Associations and Foundations, B.E. 2499 & 0.819                            & 0.958                         & 48                          & 76                                \\
Financial Institutions Business Act, B.E. 2551                                                                                        & 0.813                            & 0.897                         & 165                         & 168                               \\
Derivatives Act, B.E. 2546                                                                                                            & 0.813                            & 0.927                         & 109                         & 156                               \\
Chamber of Commerce Act, B.E. 2509                                                                                                    & 0.811                            & 0.864                         & 22                          & 62                                \\
Civil and Commercial Code                                                                                                             & 0.807                            & 0.909                         & 1618                        & 1842                              \\
Trust for Transactions in Capital Market Act, B.E. 2550                                                                               & 0.805                            & 0.890                         & 73                          & 94                                \\
Emergency Decree on Special Purpose Juristic Person for Securitization, B.E. 2540                                                     & 0.798                            & 0.879                         & 33                          & 46                                \\
Securities and Exchange Act, B.E. 2535                                                                                                & 0.786                            & 0.898                         & 294                         & 499                               \\
Energy Conservation Promotion Act, B.E. 2535                                                                                          & 0.778                            & 0.948                         & 58                          & 68                                \\
Accounting Act, B.E. 2543                                                                                                             & 0.728                            & 0.895                         & 38                          & 45                                \\
Energy Industry Act, B.E. 2550                                                                                                        & 0.726                            & 0.843                         & 153                         & 155                               \\
Petroleum Income Tax Act, B.E. 2514                                                                                                   & 0.616                            & 0.764                         & 110                         & 115                               \\ \bottomrule

\end{tabular}
}
\end{table}

\subsubsection{Retrieval Result on Tax Case Dataset}
\label{subsubsec:re_result_tax}
% Table \ref{table: retrieval_tax} displays the overall result of each retrieval models on the Tax Case dataset with heirachical chunking. Unlike WCX dataset, this dataset contains multi-label queries and, thus, its multi-label metrics have different values from their single-label counterparts.

% From the table, the overall performance drops significantly as compared to the performance on WCX dataset due to this dataset containing considerably longer queries that requires more attention to details. The models that perform consistently in the top are JINA v3, BGE-M3, Auto-Finetuned BGE-M3 and Human-Finetuned BGE-M3 with Multi MRR@10 of 0.311, 0.354, 0.345 and 0.333 respectively. On the other hand, there are two models that perform consistently below the baseline, namely JINA v2 and NV-Embed v1 which might be due to the dataset being out-of-distribution from the train data of these models especially in the case of JINA v2 with which the performance such as Multi MRR@10 drops from 0.742 on WCX dataset to merely 0.091 on Tax Case dataset as shown in figure \ref{fig: tax_mrr_vs_wanchan}.

% Furthermore, the fine-tuned variants of BGE-M3 on the WCX train set are often outperformed by the base version of BGE-M3 which suggests that the WCX and Tax Case dataset are from a different distribution and a retrieval model trained on one distribution cannot generalise to the other. Next, focusing on the single-label metrics and their multi-label counterparts, despite some models achieving good performance in single-label hit-rate metric, all models fail to perform in multi-label hit-rate metric. This, combined with the low recall across all models and the fact that multi-label MRR is considerably lower than single-label MRR, suggests that even though the models can often retrieve a subset of relevant documents, the models can only seldomly retrieve all relevant documents to the query which can be fatal due to the fact that a legal response should be constructed and based on a every relevant law sections. In addition, from observing single-label MRR and multi-label MRR, the retrieved documents that are relevant are often not placed in the first rank but rather the second rank and below.

This section presents the retrieval performance of various models on the Tax Case dataset using hierarchical chunking. Unlike the WCX dataset, this dataset includes multi-label queries, resulting in different values for single-label and multi-label metrics.

Overall performance is significantly lower on this dataset compared to WCX, likely due to the considerably longer and more nuanced queries in the Tax Case dataset. JINA v3, BGE-M3 (base, auto-fine-tuned, and human-fine-tuned) consistently perform among the top, achieving Multi-MRR@10 scores of 0.311, 0.354, 0.345, and 0.333, respectively. Conversely, JINA v2 and NV-Embed v1 consistently underperform compared to the baseline, potentially because the Tax Case dataset is out-of-distribution relative to their training data. This is particularly evident with JINA v2, whose Multi-MRR@10 drops dramatically from 0.750 on WCX to 0.091 on the Tax Case dataset (see Figure \ref{fig: tax_mrr_vs_wanchan}).

Interestingly, the WCX-fine-tuned BGE-M3 variants are often outperformed by the base BGE-M3, suggesting different data distributions between the WCX and Tax Case datasets, hindering cross-dataset generalization. While some models achieve reasonable single-label hit-rates, multi-label hit-rate performance is poor across all models. This, combined with low recall and significantly lower multi-label MRR compared to single-label MRR, indicates that while models can often retrieve some relevant documents, they struggle to retrieve all relevant documents for a given query. This limitation is critical, as comprehensive legal responses require consideration of all relevant legal sections. Moreover, the difference between single-label and multi-label MRR suggests that even when relevant documents are retrieved, they are often not ranked highly.


\begin{table}[ht]
\centering
\caption{Retrieval Evaluation Result on Tax Case Dataset with hierarchical chunking}
\renewcommand{\arraystretch}{1.5} % This increases the cell height by 1.5 times
\label{table: retrieval_tax}
\begin{tabular}{@{}ccccccc@{}}
\toprule
Top-K & Model                  & HR@k          & Multi HR@k    & Recall@k      & MRR@k         & Multi MRR@k   \\ \midrule
k=1   & BM25                   & .220          & .080          & .118          & .220          & .118          \\
      & JINA V2                & .140          & .040          & .068          & .140          & .068          \\
      & JINA V3                & .400          & .100          & .203          & .400          & .203          \\
      & NV-Embed V1            & .100          & .020          & .035          & .100          & .035          \\
      & BGE-M3                 & \underline{.500}    & \underline{.140}    & \underline{.269}    & \underline{.500}    & \underline{.269}    \\
      & Human-Finetuned BGE-M3 & .480          & \underline{.140}    & .255          & .480          & .255          \\
      & Auto-Finetuned BGE-M3  & \textbf{.520} & \textbf{.160} & \textbf{.281} & \textbf{.520} & \textbf{.281} \\
      & Cohere                 & .340          & .100          & .179          & .340          & .179          \\
      & LCLM                   & .480          & .120          & .227          & .480          & .227          \\ \midrule
k=5   & BM25                   & .480          & .120          & .254          & .318          & .171          \\
      & JINA V2                & .200          & .080          & .114          & .165          & .085          \\
      & JINA V3                & .720          & \underline{.260}    & \underline{.448}    & .508          & .297          \\
      & NV-Embed V1            & .200          & .020          & .081          & .126          & .050          \\
      & BGE-M3                 & .720          & .240          & .435          & \underline{.580}    & \underline{.337}    \\
      & Human-Finetuned BGE-M3 & \underline{.740}    & .220          & .411          & .565          & .320          \\
      & Auto-Finetuned BGE-M3  & .700          & .200          & .382          & \textbf{.587} & .329          \\
      & Cohere                 & .620          & .200          & .363          & .447          & .256          \\
      & LCLM                   & \textbf{.760} & \textbf{.320} & \textbf{.515} & \textbf{.587} & \textbf{.370} \\ \midrule
k=10  & BM25                   & .540          & .160          & .320          & .327          & .183          \\
      & JINA V2                & .240          & .100          & .147          & .171          & .091          \\
      & JINA V3                & \textbf{.840} & \underline{.340}    & .549          & .524          & .311          \\
      & NV-Embed V1            & .220          & .040          & .097          & .128          & .052          \\
      & BGE-M3                 & \underline{.820}    & \textbf{.360} & \underline{.555}    & \underline{.593}    & \underline{.354}    \\
      & Human-Finetuned BGE-M3 & .800          & .280          & .499          & .574          & .333          \\
      & Auto-Finetuned BGE-M3  & .780          & .260          & .483          & \textbf{.600} & .345          \\
      & Cohere                 & .680          & .200          & .414          & .454          & .263          \\
      & LCLM                   & .780          & \textbf{.360} & \textbf{.566} & .590          & \textbf{.379} \\ \bottomrule
\end{tabular}
\end{table}

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.7\textwidth]{images/dataset_comparison.png}
  \caption{Stacked bar chart comparing MRR@10 performance between Tax Case dataset and WCX dataset across all models}
  \label{fig: tax_mrr_vs_wanchan}
\end{figure}

% Despite its overall unsatisfying performance, the increase in performance of hit-rate and recall as a result of higher number of retrieved documents are more rapid than the increase in performance on WCX dataset as shown in figure \ref{fig: tax_mrr_vs_topk}. Lastly, the performance of the human-finetuned BGE-M3 on each law code is displayed in table \ref{table: retrieval_tax_by_code} with Support showing the amount of queries contain section in a given law code as relevant documents and Section Count showing the amount of sections in a given law code.

Despite the lower overall performance, hit-rate and recall improve more rapidly with increasing numbers of retrieved documents on the Tax Case dataset compared to the WCX dataset (Figure \ref{fig: tax_mrr_vs_topk}). Finally, Table \ref{table: retrieval_tax_by_code} presents the performance of the human-fine-tuned BGE-M3 model across individual legislation within the Tax Case dataset.

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.7\textwidth]{images/tax_recall_topk.png}
  \caption{Line chart showing increase in performance vs number of retrieved documents for each retrieval models}
  \label{fig: tax_mrr_vs_topk}
\end{figure}

\begin{table}[ht]
\centering
\caption{Retrieval Evaluation Result by law code on Tax Case Dataset}
\renewcommand{\arraystretch}{1.5} % This increases the cell height by 1.5 times
\label{table: retrieval_tax_by_code}
\begin{tabular}{@{}ccccc@{}}
\toprule
Law Code                               & Multi MRR@10 & Recall@10 & Support & Section Count \\ \midrule
Revenue Code                           & 0.333        & 0.499     & 50      & 361           \\
Accounting Act, B.E. 2543              & 0.333        & 0.5       & 1       & 45            \\
Civil and Commercial Code              & 0.173        & 0.34      & 8       & 1842          \\
Securities and Exchange Act, B.E. 2535 & 0.065        & 0.333     & 1       & 499           \\ \bottomrule
\end{tabular}
\end{table}

\subsubsection{Effects of domain-specific chunking compared to naive chunking strategy.}
\label{subsubsec:re_result_chunk}
% In this section, the best naive chunking strategy obtain in section \ref{subsec:chunking_select} which is line chunking with chunk size of 553 characters and chunk overlap of 50 characters are evaluated with human-finetuned BGE-M3 retrieval model and compared with the performance of the same model on our hierarchical chunking strategy with 10 retrieved documents. The result is shown in table \ref{table: retrieval_chunk}. As observed from the result table, the performance of the retrieval models on naive chunking approaches is significantly lower than that on our hierarchical chunking strategy approach on both WCX and Tax Case dataset. This is partly due to the naive chunking approach leaving out chunk that does not fully contain any section. Another reason for this result is due to the fact that naive chunking often chunk a single section into multiple parts in multiple chunks and, thus, renders the separated section unusable and not counted towards any metrics since it is crucial for a system to give a legal response based on full detail of a section not a part of it. Therefore, as of now, we conclude that naive chunking approach is inferior to the our hierarchical chunking strategy approach even though the current retrieval evaluation is slightly biased. However, thorough end-to-end evaluation should be performed in order to firmly conclude the performance of two chunking approaches.

In this section, we compare the performance of the best naive chunking strategy (line chunking with 553-character chunk size and 50-character overlap) against our hierarchical chunking strategy using the human-fine-tuned BGE-M3 model and retrieving 10 documents. Results are presented in Table \ref{table: retrieval_chunk}.

As shown in the table \ref{table: retrieval_chunk}, naive chunking performance is significantly lower on both the WCX and Tax Case datasets. This discrepancy likely stems from two factors: First, naive chunking discards chunks that do not fully contain a section. Second, it often splits single sections across multiple chunks, rendering these fragmented sections unusable for evaluation and practical application, as legal responses require complete sections. Therefore, based on these retrieval results, our hierarchical chunking approach appears superior. However, it is important to acknowledge a potential bias in the current evaluation. A comprehensive end-to-end evaluation is necessary to definitively compare the two chunking strategies.

\begin{table}[!ht]
\centering
\caption{Retrieval Evaluation Result on naive chunking compared with hierarchical chunking}
\renewcommand{\arraystretch}{1.5} % This increases the cell height by 1.5 times
\label{table: retrieval_chunk}
\begin{tabular}{@{}c|cccccc@{}}
\toprule
Dataset                   & Chunking Strategy     & \multicolumn{1}{l}{HR@10} & \multicolumn{1}{l}{Multi HR@10} & \multicolumn{1}{l}{Recall@10} & \multicolumn{1}{l}{MRR@10} & \multicolumn{1}{l}{Multi MRR@10} \\ \midrule
\multirow{2}{*}{WCX}      & Hierarchical Chunking & \textbf{.938}             & \textbf{.938}                   & \textbf{.938}                 & \textbf{.809}              & \textbf{.809}                    \\
                          & Naive Chunking        & .658                      & .658                            & .658                          & .553                       & .553                             \\ \midrule
\multirow{2}{*}{Tax Case} & Hierarchical Chunking & \textbf{.800}             & \textbf{.280}                   & \textbf{.499}                 & \textbf{.574}              & \textbf{.333}                    \\
                          & Naive Chunking        & .180                      & .020                            & .062                          & .120                       & .048                             \\ \bottomrule
\end{tabular}
\end{table}

\subsubsection{Retrieval by Long-Context LLM}
\label{subsubsec:re_result_sample}

% Moreover, the feasibility of implementing retriever from long-context LLM is explored by providing the content of all 35 Thai Financial laws as the context along with the query and the LLM, specifically gemini-1.5-pro, is tasked to retrieve 20 relevant laws with reasoning on why it is relevant to the query. This method is only experimented in a 20\% subsample of WCX dataset stratified on the law code and full Tax Case dataset due to limitation in our budget. The result on the sampled WCX dataset and the Tax Case dataset are shown in table \ref{table: retrieval_tax_lclm} and {} with long-context LLM result shown as LCLM. 

We also explored using a long-context LLM (LCLM) as a retriever. We provided Gemini 1.5 Pro with all 35 legislation as context and tasked it with retrieving 20 relevant laws for each query, along with justifications for their relevance. Due to budget constraints, this experiment was limited to a stratified 20\% sample of the WCX dataset which translates to 746 data points and the full Tax Case dataset. Results are shown in Table \ref{table: retrieval_wcx_lclm} for the WCX dataset and \ref{table: retrieval_tax_lclm} for the sampled Tax Case dataset.

\begin{table}[ht]
\centering
\caption{Retrieval Evaluation Result on WCX Dataset with hierarchical chunking with Long-Context Retriever}
\renewcommand{\arraystretch}{1.5} % This increases the cell height by 1.5 times
\label{table: retrieval_wcx_lclm}
\begin{tabular}{@{}ccrrrrr@{}}
\toprule
Top-K                 & Model                  & \multicolumn{1}{c}{HR@k} & \multicolumn{1}{c}{Multi HR@k} & \multicolumn{1}{c}{Recall@k} & \multicolumn{1}{c}{MRR@k} & \multicolumn{1}{c}{Multi MRR@k} \\ \midrule
\multirow{9}{*}{k=1}  & BM25                   & .480                    & .480                          & .480                        & .480                     & .480                           \\
                      & JINA V2                & .698                    & .698                          & .698                        & .698                     & .698                           \\
                      & JINA V3                & .601                    & .601                          & .601                        & .601                     & .601                           \\
                      & NV-Embed V1            & .496                    & .496                          & .496                        & .496                     & .496                           \\
                      & BGE-M3                 & .708                    & .708                          & .708                        & .708                     & .708                           \\
                      & Human-Finetuned BGE-M3 & \textbf{.757}           & \textbf{.757}                 & \textbf{.757}               & \textbf{.757}            & \textbf{.757}                  \\
                      & Auto-Finetuned BGE-M3  & \underline{ .741}              & \underline{ .741}                    & \underline{ .741}                  & \underline{ .741}               & \underline{ .741}                     \\
                      & Cohere                 & .707                    & .707                          & .707                        & .707                     & .707                           \\
                      & LCLM                   & .590                    & .590                          & .590                        & .590                     & .590                           \\ \midrule
\multirow{9}{*}{k=5}  & BM25                   & .663                    & .663                          & .663                        & .549                     & .549                           \\
                      & JINA V2                & .858                    & .858                          & .858                        & .761                     & .761                           \\
                      & JINA V3                & .828                    & .828                          & .828                        & .693                     & .693                           \\
                      & NV-Embed V1            & .711                    & .711                          & .711                        & .585                     & .585                           \\
                      & BGE-M3                 & \underline{ .888}              & \underline{ .888}                    & \underline{ .888}                  & .779                     & .779                           \\
                      & Human-Finetuned BGE-M3 & \textbf{.909}           & \textbf{.909}                 & \textbf{.909}               & \textbf{.819}            & \textbf{.819}                  \\
                      & Auto-Finetuned BGE-M3  & \textbf{.909}           & \textbf{.909}                 & \textbf{.909}               & \underline{ .807}               & \underline{ .807}                     \\
                      & Cohere                 & .867                    & .867                          & .867                        & .772                     & .772                           \\
                      & LCLM                   & .776                    & .776                          & .776                        & .667                     & .667                           \\ \midrule
\multirow{9}{*}{k=10} & BM25                   & .733                    & .733                          & .733                        & .559                     & .559                           \\
                      & JINA V2                & .891                    & .891                          & .891                        & .766                     & .766                           \\
                      & JINA V3                & .878                    & .878                          & .878                        & .700                     & .700                           \\
                      & NV-Embed V1            & .794                    & .794                          & .794                        & .596                     & .596                           \\
                      & BGE-M3                 & .926                    & .926                          & .926                        & .784                     & .784                           \\
                      & Human-Finetuned BGE-M3 & \textbf{.945}           & \textbf{.945}                 & \textbf{.945}               & \textbf{.824}            & \textbf{.824}                  \\
                      & Auto-Finetuned BGE-M3  & \underline{ .941}              & \underline{ .941}                    & \underline{ .941}                  & \underline{ .812}               & \underline{ .812}                     \\
                      & Cohere                 & .913                    & .913                          & .913                        & .778                     & .778                           \\
                      & LCLM                   & .807                    & .807                          & .807                        & .671                     & .671                           \\ \bottomrule
\end{tabular}
\end{table}


\begin{table}[ht]
\centering
\caption{Retrieval Evaluation Result on Tax Case Dataset with hierarchical chunking with Long-Context Retriever}
\renewcommand{\arraystretch}{1.5} % This increases the cell height by 1.5 times
\label{table: retrieval_tax_lclm}
\begin{tabular}{@{}c|cccccc@{}}
\toprule
Top-K & Model                  & HR@k          & Multi HR@k    & Recall@k      & MRR@k         & Multi MRR@k   \\ \midrule
k=1   & BM25                   & .220          & .080          & .118          & .220          & .118          \\
      & JINA V2                & .140          & .040          & .068          & .140          & .068          \\
      & JINA V3                & .400          & .100          & .203          & .400          & .203          \\
      & NV-Embed V1            & .100          & .020          & .035          & .100          & .035          \\
      & BGE-M3                 & \underline{.500}    & \underline{.140}    & \underline{.269}    & \underline{.500}    & \underline{.269}    \\
      & Human-Finetuned BGE-M3 & .480          & \underline{.140}    & .255          & .480          & .255          \\
      & Auto-Finetuned BGE-M3  & \textbf{.520} & \textbf{.160} & \textbf{.281} & \textbf{.520} & \textbf{.281} \\
      & Cohere                 & .340          & .100          & .179          & .340          & .179          \\
      & LCLM                   & .480          & .120          & .227          & .480          & .227          \\ \midrule
k=5   & BM25                   & .480          & .120          & .254          & .318          & .171          \\
      & JINA V2                & .200          & .080          & .114          & .165          & .085          \\
      & JINA V3                & .720          & \underline{.260}    & \underline{.448}    & .508          & .297          \\
      & NV-Embed V1            & .200          & .020          & .081          & .126          & .050          \\
      & BGE-M3                 & .720          & .240          & .435          & \underline{.580}    & \underline{.337}    \\
      & Human-Finetuned BGE-M3 & \underline{.740}    & .220          & .411          & .565          & .320          \\
      & Auto-Finetuned BGE-M3  & .700          & .200          & .382          & \textbf{.587} & .329          \\
      & Cohere                 & .620          & .200          & .363          & .447          & .256          \\
      & LCLM                   & \textbf{.760} & \textbf{.320} & \textbf{.515} & \textbf{.587} & \textbf{.370} \\ \midrule
k=10  & BM25                   & .540          & .160          & .320          & .327          & .183          \\
      & JINA V2                & .240          & .100          & .147          & .171          & .091          \\
      & JINA V3                & \textbf{.840} & \underline{.340}    & .549          & .524          & .311          \\
      & NV-Embed V1            & .220          & .040          & .097          & .128          & .052          \\
      & BGE-M3                 & \underline{.820}    & \textbf{.360} & \underline{.555}    & \underline{.593}    & \underline{.354}    \\
      & Human-Finetuned BGE-M3 & .800          & .280          & .499          & .574          & .333          \\
      & Auto-Finetuned BGE-M3  & .780          & .260          & .483          & \textbf{.600} & .345          \\
      & Cohere                 & .680          & .200          & .414          & .454          & .263          \\
      & LCLM                   & .780          & \textbf{.360} & \textbf{.566} & .590          & \textbf{.379} \\ \bottomrule
\end{tabular}
\end{table}

% From the resulting table, it can be observed that the LCLM retriever demonstrates performance comparable to other embedding-based retrievers on the Tax Case dataset. However, a noticeable gap remains between the performance of LCLM and the best-performing retriever model on the WCX dataset. Additionally, the LCLM results from both tables indicate that increasing the number of retrieved documents yields only minimal performance improvements compared to those achieved by other models. This limited improvement is likely attributed to the next-token prediction nature of LLMs, which restricts their ability to effectively retrieve and output the final relevant laws when they are far removed from the query context.

The results show that the LCLM retriever performs comparably to other embedding-based retrievers on the Tax Case dataset which might be due to the LLM's higher reasoning capability than the embedding-based retriever, but there's a noticeable performance gap compared to the best retriever on the WCX dataset. Furthermore, increasing the number of retrieved documents for the LCLM yields minimal performance gains compared to other models. This limited improvement likely stems from LLMs' next-token prediction nature, which hinders their ability to effectively retrieve and output relevant laws when those laws are distant from the query context.

\subsubsection{Error Analysis}
\label{subsubsec:re_result_discussion}
In this section, a more detailed inspection of the retrieval result is conducted

% First, for WCX dataset, a set of queries that does not gain an improvement in performance, specifically MRR@10, from fine-tuning with in-domain dataset is examined. This subset of queries consists of around 780 data out of the total 400. Among the 780 queries, 540 queries achieve the same MRR@10 with base BGE-M3 and Human-Finetuned BGE-M3. 50 from 780 queries are sampled for further error analysis. We categorize the observed errors into 4 categories.

% \textbf{1. Hidden Hierarchical Information: } This type of retriever error is caused by the redundant and repetitive nature of Thai Financial Laws. Multiple section often conveys the same exact message with the difference only being the location in the legal hierarchy or the law code it is located in. For example, the difference between section 27 and 89/1 of the Revenue code, which are about additional tax charge of person who fails to pay taxes within the deadline, is that section 27 is located in \textbf{Chapter 2: Procedures Related to Tax Assessment} while section 89/1 is located in \textbf{Chapter 4: Value-Added Tax}. Without the information on the location of both sections in the legal hierarchy, there exists no method that can confidently discerns the two sections from each other.

Focusing first on the WCX dataset, we examine a subset of approximately 780 queries (out of 4000) that showed no MRR@10 improvement after fine-tuning with in-domain data. Within this subset, 540 queries exhibited identical MRR@10 scores for both the base and human-fine-tuned BGE-M3 models. A sample of 50 queries from this group was selected for further error analysis, revealing four distinct error categories.

\textbf{Hidden Hierarchical Information}: This error type arises from the redundancy and repetition within Thai Financial Law. Multiple sections often convey identical information, differing only in their location within the legal hierarchy or the specific law code. For example, section 27 and 89/1 of the Revenue Code both address additional tax charges for late payment, but section 27 resides in Chapter 2: Procedures Related to Tax Assessment, while section 89/1 belongs to Chapter 4: Value-Added Tax. Without \st{hierarchical context} \textcolor{orange}{providing a proper comprehensive definition in all law hierarchy as a context to the prompt}, differentiating these sections becomes extremely difficult.

\begin{quote}
\begin{thai}
"\textbf{ประมวลรัษฎากร มาตรา 27} บุคคลใดไม่เสียหรือนำส่งภาษีภายในกำหนดเวลาตามที่บัญญัติไว้ในหมวดต่าง ๆ แห่งลักษณะนี้เกี่ยวกับภาษีอากรประเมิน ให้เสียเงินเพิ่มอีกร้อยละ 1.5 ต่อเดือนหรือเศษของเดือนของเงินภาษี..."        
\end{thai}
\end{quote}

\begin{quote}
\begin{thai}
"\textbf{ประมวลรัษฎากร มาตรา 89/1} บุคคลใดไม่ชำระภาษีหรือนำส่งภาษีให้ครบถ้วนภายในกำหนดเวลาตามบทบัญญัติในหมวดนี้ ให้เสียเงินเพิ่มอีกร้อยละ 1.5 ต่อเดือน หรือเศษของเดือนของเงินภาษี..."           
\end{thai}
\end{quote}

% \begin{minipage}[t]{0.7\textwidth}
%     \begin{quote}
%         \begin{thai}
%         "\textbf{ประมวลรัษฎากร มาตรา 27} บุคคลใดไม่เสียหรือนำส่งภาษีภายในกำหนดเวลาตามที่บัญญัติไว้ในหมวดต่าง ๆ แห่งลักษณะนี้เกี่ยวกับภาษีอากรประเมิน ให้เสียเงินเพิ่มอีกร้อยละ 1.5 ต่อเดือนหรือเศษของเดือนของเงินภาษี..."        
%         \end{thai}
%     \end{quote}
% \end{minipage}
% \hfill
% \begin{minipage}[t]{0.7\textwidth}
%     \begin{quote}
%         \begin{thai}
%         "\textbf{ประมวลรัษฎากร มาตรา 89/1} บุคคลใดไม่ชำระภาษีหรือนำส่งภาษีให้ครบถ้วนภายในกำหนดเวลาตามบทบัญญัติในหมวดนี้ ให้เสียเงินเพิ่มอีกร้อยละ 1.5 ต่อเดือน หรือเศษของเดือนของเงินภาษี..."           
%         \end{thai}
%     \end{quote}
% \end{minipage}

% Another example of this problem is section 91/15 of Revenue Code and section 15 of Petroleum Income Tax Act (We also found that the Petroleum Income Tax Act have various sections that are very similar to sections in Revenue Code due to both of them revolving around Tax law but under different conditions). Both of these sections convey the message about the circumstance in which the assessment officer have the authority to assess tax, fine and charges for a person with the difference being only the law code they are in.

A further example involves section 91/15 of the Revenue Code and section 15 of the Petroleum Income Tax Act. Both address the circumstances under which an assessment officer can assess taxes, fines, and charges, differing primarily in their respective legislation. Notably, several sections within the Petroleum Income Tax Act closely resemble those in the Revenue Code, likely due to their shared focus on tax law, albeit under different conditions.

\begin{quote}
\begin{thai}
"\textbf{ประมวลรัษฎากร มาตรา 91/15} ประมวลรัษฎากร มาตรา 91/15 เจ้าพนักงานประเมินมีอำนาจประเมินภาษี เบี้ยปรับ และเงินเพิ่มตามหมวดนี้ ในเมื่อ
(1) ปรากฏแก่เจ้าพนักงานประเมินว่าผู้มีหน้าที่เสียภาษีมิได้ยื่นแบบแสดงรายการภายในเวลาที่กฎหมายกำหนด..."        
\end{thai}
\end{quote}

\begin{quote}
\begin{thai}
"\textbf{พระราชบัญญัติภาษีเงินได้ปิโตรเลียม พ.ศ. 2514 มาตรา 56} พระราชบัญญัติภาษีเงินได้ปิโตรเลียม พ.ศ. 2514 มาตรา 56 เจ้าพนักงานประเมินมีอำนาจประเมินภาษี เบี้ยปรับ และเงินเพิ่มตามพระราชบัญญัตินี้ เมื่อ
(1) ผู้มีหน้าที่ยื่นแบบแสดงรายการเงินได้มิได้ยื่นแบบแสดงรายการเงินได้ภายในเวลาที่กำหนด..."           
\end{thai}
\end{quote}

% \begin{minipage}[t]{0.45\textwidth}
%     \begin{quote}
%         \begin{thai}
%         "\textbf{ประมวลรัษฎากร มาตรา 91/15} ประมวลรัษฎากร มาตรา 91/15 เจ้าพนักงานประเมินมีอำนาจประเมินภาษี เบี้ยปรับ และเงินเพิ่มตามหมวดนี้ ในเมื่อ
% (1) ปรากฏแก่เจ้าพนักงานประเมินว่าผู้มีหน้าที่เสียภาษีมิได้ยื่นแบบแสดงรายการภายในเวลาที่กฎหมายกำหนด..."        
%         \end{thai}
%     \end{quote}
% \end{minipage}
% \hfill
% \begin{minipage}[t]{0.45\textwidth}
%     \begin{quote}
%         \begin{thai}
%         "\textbf{พระราชบัญญัติภาษีเงินได้ปิโตรเลียม พ.ศ. 2514 มาตรา 56} พระราชบัญญัติภาษีเงินได้ปิโตรเลียม พ.ศ. 2514 มาตรา 56 เจ้าพนักงานประเมินมีอำนาจประเมินภาษี เบี้ยปรับ และเงินเพิ่มตามพระราชบัญญัตินี้ เมื่อ
% (1) ผู้มีหน้าที่ยื่นแบบแสดงรายการเงินได้มิได้ยื่นแบบแสดงรายการเงินได้ภายในเวลาที่กำหนด..."           
%         \end{thai}
%     \end{quote}
% \end{minipage}

% There are two main approaches to tackle this problem. First, the hidden hierarchical information can be integrated directly into the content of the section, explicitly specifying which chapters, division and law code this section belongs to. Then, the retriever model should be fine-tuned to adapt to the new content format and ensure that the retriever model would take the hierarchical information into consideration when producing embeddings. Another approach to tackle this problem is to either utilize heuristic approach or other retriever models to retrieve only the subset of possible legislation, titles, divisions before retrieving at the section level which would limits the redundancy of retrieved sections.

Two main approaches can \textcolor{orange}{potentially} address this hierarchical information challenge. First, integrate this information directly into the section content, explicitly stating the chapter, division, and law code (or possibly it's comprehensive definition of each level). This requires retraining the retriever model to incorporate this new information into its embedding generation process. Alternatively, a heuristic approach or a separate retrieval model could pre-select relevant legislation, titles, and divisions before section-level retrieval, thus reducing redundancy.

% \textbf{2. Nested Structure: } This type of error arises from the nested structure of Thai financial law, where certain sections reference other sections either within the same law code or across different codes. As a result, some sections do not contain a complete, explicit explanation but instead point to other sections for additional details. For example, section 1409 of the Civil and Commercial Code mandates the analogical use of sections 552–555, 558, 562, and 563. However, section 1409 itself does not include the actual content of these referenced sections. Consequently, the retriever may struggle to find substantial information within section 1409 to match the query accurately and is likely to retrieve the referenced sections instead, as they contain the specific details and better align with the query's intent.

\textbf{Nested Structure}: This error type stems from the nested structure of Thai financial law, where sections often reference other sections within the same or different legislation. These referencing sections may lack complete explanations, instead pointing to other sections for details. For instance, section 1409 of the Civil and Commercial Code mandates the analogical application of sections 552–555, 558, 562, and 563 but does not contain their content. Consequently, retrievers might struggle to match queries with section 1409 directly, instead retrieving the referenced sections containing the relevant details.

\begin{quote}
    \begin{thai}
    "\textbf{ประมวลกฎหมายแพ่งและพาณิชย์ มาตรา 1409} ท่านให้นำบทบัญญัติแห่งประมวลกฎหมายนี้ว่าด้วยหน้าที่และความรับผิดของผู้เช่าอันกล่าวไว้ในมาตรา 552 ถึง 555 มาตรา 558, 562 และ 563 มาใช้บังคับโดยอนุโลม"        
    \end{thai}
\end{quote}

% Another frequently occurring examples involve sections specifying legal penalties. Often, to determine the penalty for a particular crime or to understand whether a specific case is punishable, one must first locate the section that prohibits the behavior and then find a separate section that defines the penalty for that prohibited act. This structure means that the penalty section itself lacks details about the crime, causing the retriever model to struggle in retrieving relevant penalty sections when asked about the punishment for a specific offense and tends to retrieve the section about the prohibition of the act instead. One example of this case is section 186 of Securities and Exchange Act which specifies the authority of the Securities and Exchange Commission (SEC) and section 291 of the same act which describes the punishment of any individual who fails to comply the orders within the said authority of SEC.

Another common example involves sections specifying penalties. Determining the penalty for a specific offense often requires locating the section prohibiting the behavior and a separate section defining the corresponding penalty. This separation can cause retrieval issues. For example, consider sections 186 and 291 of the Securities and Exchange Act. Section 186 specifies the authority of the Securities and Exchange Commission (SEC), while section 291 outlines penalties for non-compliance with SEC orders. A query about penalties for a specific offense might incorrectly retrieve section 186 (describing the SEC's authority) instead of section 291 (detailing the actual penalties) because the penalty section itself lacks details about the specific offense.

\begin{quote}
\begin{thai}
"\textbf{พระราชบัญญัติหลักทรัพย์และตลาดหลักทรัพย์ พ.ศ. 2535 มาตรา 186 } เพื่อป้องกันความเสียหายอันอาจเกิดแก่ประโยชน์ของประชาชนหรือเศรษฐกิจของประเทศ ให้คณะกรรมการ ก.ล.ต. มีอำนาจ
(1) ห้ามการซื้อขายหลักทรัพย์จดทะเบียนทั้งหมดในตลาดหลักทรัพย์เป็นการชั่วคราว..."        
\end{thai}
\end{quote}

\begin{quote}
\begin{thai}
"\textbf{พระราชบัญญัติหลักทรัพย์และตลาดหลักทรัพย์ พ.ศ. 2535 มาตรา 291 } ผู้ใดฝ่าฝืนหรือไม่ปฏิบัติตามคำสั่งที่กำหนดตามมาตรา 186 (2) ต้องระวางโทษจำคุกไม่เกินหนึ่งปี หรือปรับไม่เกินสามแสนบาท หรือทั้งจำทั้งปรับ"           
\end{thai}
\end{quote}


% \begin{minipage}[t]{0.45\textwidth}
%     \begin{quote}
%         \begin{thai}
%         "\textbf{พระราชบัญญัติหลักทรัพย์และตลาดหลักทรัพย์ พ.ศ. 2535 มาตรา 186 } เพื่อป้องกันความเสียหายอันอาจเกิดแก่ประโยชน์ของประชาชนหรือเศรษฐกิจของประเทศ ให้คณะกรรมการ ก.ล.ต. มีอำนาจ
% (1) ห้ามการซื้อขายหลักทรัพย์จดทะเบียนทั้งหมดในตลาดหลักทรัพย์เป็นการชั่วคราว..."        
%         \end{thai}
%     \end{quote}
% \end{minipage}
% \hfill
% \begin{minipage}[t]{0.45\textwidth}
%     \begin{quote}
%         \begin{thai}
%         "\textbf{พระราชบัญญัติหลักทรัพย์และตลาดหลักทรัพย์ พ.ศ. 2535 มาตรา 291 } ผู้ใดฝ่าฝืนหรือไม่ปฏิบัติตามคำสั่งที่กำหนดตามมาตรา 186 (2) ต้องระวางโทษจำคุกไม่เกินหนึ่งปี หรือปรับไม่เกินสามแสนบาท หรือทั้งจำทั้งปรับ"           
%         \end{thai}
%     \end{quote}
% \end{minipage}

% To mitigate this type of error, a knowledge graph should be created to capture the complete structure and relationships between each section. This graph will enhance the augmentation of retrieved content by including sections that reference the retrieved section or that the retrieved section mentions, thereby providing the LLM with more comprehensive content.

To mitigate these nested structure errors, a knowledge graph representing the relationships between sections could be employed. This would allow for augmenting retrieved content with referenced or referencing sections, providing the LLM with a more complete context.

% \textbf{3. Missable Details: } This error occurs in queries with small but crucial details that may be overlooked, especially by dense embedding models. For instance, in a query asking whether a person can act as a guarantor for another guarantor, the answer requires content from Section 682 of the Civil and Commercial Code, which states that a person may act as a surety, effectively serving as a guarantor for another guarantor. However, the top-ranked retrieved section is Section 680, which only defines a guarantor, suggesting that the model misses the specific nuance of a guarantor for a guarantor rather than just a general guarantor.

\textbf{Missable Details}: \st{This error arises when queries contain subtle yet crucial details that dense embedding models, in particular, may overlook. For example, a query about whether someone can guarantee another guarantor requires information from Section 682 of the Civil and Commercial Code, which permits such an arrangement. However, the retriever might prioritize Section 680, which merely defines a guarantor, missing the nuance of guaranteeing another guarantor.}\textcolor{prange}{This type of error occurs when subtle but critical details in a query are overlooked by dense embedding models, which often focus on general relevance instead of fine-grained nuances. For instance, consider a legal query about whether one guarantor can guarantee another guarantor. The correct answer relies on Section 682 of the Civil and Commercial Code, which explicitly allows such arrangements. However, the retriever might incorrectly prioritize Section 680, which only provides a general definition of a guarantor, thus missing the specific nuance required to address the query accurately.}

\begin{quote}
\begin{thai}
\textbf{Question: } "บุคคลสามารถเข้าค้ำประกันให้ผู้ค้ำประกันได้หรือไม่"\\
\textbf{Retrieved Section: } "ประมวลกฎหมายแพ่งและพาณิชย์ มาตรา 680
อันว่าค้ำประกันนั้น คือสัญญาซึ่งบุคคลภายนอกคนหนึ่ง เรียกว่า ผู้ค้ำประกัน ผูกพันตนต่อเจ้าหนี้คนหนึ่ง เพื่อชำระหนี้ในเมื่อลูกหนี้ไม่ชำระหนี้นั้น

อนึ่ง สัญญาค้ำประกันนั้น ถ้ามิได้มีหลักฐานเป็นหนังสืออย่างใดอย่างหนึ่งลงลายมือชื่อผู้ค้ำประกันเป็นสำคัญ ท่านว่าจะฟ้องร้องให้บังคับคดีหาได้ไม่"  \\

\textbf{Gold Section: } "ประมวลกฎหมายแพ่งและพาณิชย์ มาตรา 682 ท่านว่าบุคคลจะยอมเข้าเป็นผู้รับเรือน คือเป็นประกันของผู้ค้ำประกันอีกชั้นหนึ่ง ก็เป็นได้

ถ้าบุคคลหลายคนยอมตนเข้าเป็นผู้ค้ำประกันในหนี้รายเดียวกันไซร้ ท่านว่าผู้ค้ำประกันเหล่านั้นมีความรับผิดอย่างลูกหนี้ร่วมกัน แม้ถึงว่าจะมิได้เข้ารับค้ำประกันรวมกัน"
\end{thai}
\end{quote}

% \begin{minipage}[t]{0.5\textwidth}
%     \begin{quote}
%         \begin{thai}
%         \textbf{Question: } "บุคคลสามารถเข้าค้ำประกันให้ผู้ค้ำประกันได้หรือไม่"
%         \end{thai}
%     \end{quote}
% \end{minipage}
% \hfill
% \begin{minipage}[t]{0.5\textwidth}
%     \begin{quote}
%         \begin{thai}
%         "\textbf{Retrieved Section: } ประมวลกฎหมายแพ่งและพาณิชย์ มาตรา 680
% อันว่าค้ำประกันนั้น คือสัญญาซึ่งบุคคลภายนอกคนหนึ่ง เรียกว่า ผู้ค้ำประกัน ผูกพันตนต่อเจ้าหนี้คนหนึ่ง เพื่อชำระหนี้ในเมื่อลูกหนี้ไม่ชำระหนี้นั้น
% อนึ่ง สัญญาค้ำประกันนั้น ถ้ามิได้มีหลักฐานเป็นหนังสืออย่างใดอย่างหนึ่งลงลายมือชื่อผู้ค้ำประกันเป็นสำคัญ ท่านว่าจะฟ้องร้องให้บังคับคดีหาได้ไม่"   
%         \end{thai}
%     \end{quote}
% \end{minipage}

% To address this type of error, one approach is to adjust the weighting in a late-interaction embedding model, making the retriever more keyword-focused. This allows the model to better capture fine details in queries, reducing the likelihood of missing critical nuances.

One approach to address these missable detail errors involves adjusting the weighting in late-interaction embedding models to prioritize keyword matching. This increased focus on keywords can improve the model's ability to capture subtle nuances within queries.

% \textbf{4. Hard Queries: } This error stems from intricate queries that require multiple steps of reasoning to fully address the issue. Such errors are widespread and challenging to resolve through simple model fine-tuning. For instance, a query might ask whether a purchased ring must be returned to an individual claiming ownership who was not the original seller. This query is challenging because the model must first identify the ring as movable property before retrieving Section 1303 of the Civil and Commercial Code, which specifies that a possessor holds a superior right if they acquired the property with compensation and in good faith. However, current retrieval models struggle to incorporate this multi-step reasoning, often retrieving a more general section like Section 1332, which addresses whether a buyer must return purchased goods to the seller upon request.

\textbf{Complex Queries}: This category encompasses complex queries requiring multi-step reasoning. These errors are prevalent and difficult to resolve through simple fine-tuning. For example, a query about returning a purchased ring to someone claiming ownership (who wasn't the seller) requires multiple reasoning steps. The model must first recognize the ring as movable property, then retrieve section 1303 of the Civil and Commercial Code, which grants superior ownership rights to a good-faith purchaser for value. Current retrieval models struggle with this multi-step reasoning, often retrieving more general sections like Section 1332, which addresses returning purchased goods to the seller.

\begin{quote}
\begin{thai}
\textbf{Question: } "ซื้อแหวนมาจากคนหนึ่งได้รับของมาแล้ว ต่อมามีอีกคนอ้างว่าตนเป็นเจ้าของ ต้องคืนแหวนให้ผู้ที่มาทวงหรือไม่" \\
\textbf{Retrieved Section: } "ประมวลกฎหมายแพ่งและพาณิชย์ มาตรา 1332
บุคคลผู้ซื้อทรัพย์สินมาโดยสุจริตในการขายทอดตลาด หรือในท้องตลาด หรือจากพ่อค้าซึ่งขายของชนิดนั้น ไม่จำต้องคืนให้แก่เจ้าของแท้จริง เว้นแต่เจ้าของจะชดใช้ราคาที่ซื้อมา" \\
\textbf{Gold Section: } "ประมวลกฎหมายแพ่งและพาณิชย์ มาตรา 1303 ถ้าบุคคลหลายคนเรียกเอาสังหาริมทรัพย์เดียวกัน โดยอาศัยหลักกรรมสิทธิ์ต่างกันไซร้ ท่านว่าทรัพย์สินตกอยู่ในครอบครองของบุคคลใด บุคคลนั้นมีสิทธิยิ่งกว่าบุคคลอื่น ๆ แต่ต้องได้ทรัพย์นั้นมาโดยมีค่าตอบแทนและได้การครอบครองโดยสุจริต

ท่านมิให้ใช้มาตรานี้บังคับถึงสังหาริมทรัพย์ซึ่งระบุไว้ในมาตราก่อนและในเรื่องทรัพย์สินหาย กับทรัพย์สินที่ได้มาโดยการกระทำผิด"
\end{thai}
\end{quote}

% \begin{minipage}[t]{0.5\textwidth}
%     \begin{quote}
%         \begin{thai}
%         "\textbf{Question: } ซื้อแหวนมาจากคนหนึ่งได้รับของมาแล้ว ต่อมามีอีกคนอ้างว่าตนเป็นเจ้าของ ต้องคืนแหวนให้ผู้ที่มาทวงหรือไม่"        
%         \end{thai}
%     \end{quote}
% \end{minipage}
% \hfill
% \begin{minipage}[t]{0.5\textwidth}
%     \begin{quote}
%         \begin{thai}
%         "\textbf{Retrieved Section: } ประมวลกฎหมายแพ่งและพาณิชย์ มาตรา 1332
% บุคคลผู้ซื้อทรัพย์สินมาโดยสุจริตในการขายทอดตลาด หรือในท้องตลาด หรือจากพ่อค้าซึ่งขายของชนิดนั้น ไม่จำต้องคืนให้แก่เจ้าของแท้จริง เว้นแต่เจ้าของจะชดใช้ราคาที่ซื้อมา"   
%         \end{thai}
%     \end{quote}
% \end{minipage}

% To resolve this issue, it’s beneficial to introduce an extra step that analyzes and decomposes the query into multiple sub queries, each targeting a distinct aspect of the question. This preliminary step allows the model to engage in a “thought process,” inferring implicit information from the query or dividing complex issues into smaller, more manageable components. This approach enhances the retrieval model’s ability to retrieve more relevant and thorough content.

Addressing these complex queries requires an additional step to decompose the query into multiple sub-queries, each focusing on a specific aspect. This allows the model to perform a more nuanced "thought process," inferring implicit information and breaking down complex issues into smaller, more manageable parts, ultimately improving retrieval accuracy and comprehensiveness.

% In addition to the WCX dataset, a more comprehensive error analysis was also conducted on the Tax Case dataset.
% Initially, an analysis is conducted on the False Negatives and the False Negative Rate associated with the sections of the Revenue Code. The table \ref{table: retrieval_tax_section_fn} presents the five sections exhibiting the highest false negative rates and counts.

Beyond the WCX dataset, a detailed error analysis was also performed on the Tax Case dataset. This analysis focused on false negatives and false negative rates, particularly within sections of the Revenue Code. Table \ref{table: retrieval_tax_section_fn} highlights the five sections with the highest false negative rates and counts.

\begin{table}[]
\centering
\caption{Top 5 false negative and false negative ratio of sections in Revenue Code of Tax Case datasets}
\renewcommand{\arraystretch}{1.5} % This increases the cell height by 1.5 times
\label{table: retrieval_tax_section_fn}
\begin{tabular}{@{}ccc@{}}
\toprule
\multicolumn{1}{l}{Section} & \multicolumn{1}{l}{False Negative} & \multicolumn{1}{l}{False Negative Rate} \\ \midrule
77/2                        & 6                                  & 1.0                                     \\
77/1                        & 5                                  & 1.0                                     \\
2                           & 5                                  & 1.0                                     \\
3 octo                     & 4                                  & 1.0                                     \\
91/5                        & 2                                  & 1.0                                     \\ \bottomrule
\end{tabular}
\end{table}

% An examination of the content within these sections reveals that most contain either terminology definitions or very general statements of the law applicable in various situations. Due to the lack of specific details or conditions in these sections, coupled with the tax cases dataset primarily comprising real-world scenarios, the retriever model may struggle to retrieve these generic yet important sections. Instead, it is likely to favor sections that better match the provided context directly. This problem can be named as \textbf{"Generic Section Retrieval Challenge"}.

These sections largely contain either terminology definitions or broad legal overviews. Because they lack specific details and conditions, they pose a retrieval challenge: while important, their generality makes them a poor match for the concrete scenarios found in the tax case dataset. The retriever model, therefore, tends to favor sections with greater contextual relevance. We refer to this problem as the "Generic Section Retrieval Challenge."

% An example of this issue occurs when a query inquires whether the company should charge VAT for chemical products or medical supplies used in treating animals. Instead of retrieving the generic Section 77/2 of the Revenue Code, which simply states that any service or sale and importation of products is subject to VAT, the model retrieves other sections that discuss VAT exemptions or additional charges under specific conditions that may be tangentially related to the query. While some of these retrieved sections are indeed relevant, the retriever fails to provide a comprehensive set of documents necessary for valid legal reasoning.

For example, a query about VAT applicability to animal treatment chemicals and medical supplies might retrieve sections discussing specific VAT exemptions or surcharges, rather than the overarching section 77/2 of the Revenue Code, which simply states that all sales, imports, and services are subject to VAT. While some retrieved sections may be relevant, the model fails to surface the foundational Section 77/2, hindering comprehensive legal reasoning.



    \begin{quote}
        \begin{thai}
        \textbf{Question: } บริษัทประกอบกิจการโรงพยาบาลสัตว์ให้บริการเป็นสถานรักษาสัตว์ ได้มีการสั่งซื้อยาเพื่อมาใช้รักษาสัตว์ เมื่อบริษัทฯ คิดค่ารักษาสัตว์กับลูกค้า บริษัทฯ ได้แยกค่ายากับค่าบริการรักษาสัตว์ออกจากกัน บริษัทจึงขอหารือว่า
    1. บริษัทจะเรียกเก็บภาษีมูลค่าเพิ่มเฉพาะค่ารักษาสัตว์ แต่จะไม่เรียกเก็บภาษีมูลค่าเพิ่มสำหรับค่ายาที่ใช้รักษาสัตว์ ถูกต้องหรือไม่
    2. ค่าเคมีภัณฑ์ที่ใช้สำหรับการรักษาสัตว์ เช่น น้ำยาล้างแผล แอลกอฮอล์ น้ำเกลือ เป็นต้น บริษัทจะต้องเรียกเก็บภาษีมูลค่าเพิ่มจากลูกค้าหรือไม่
    3. ค่าเวชภัณฑ์ที่ใช้สำหรับรักษาสัตว์ เช่น สำลี สายน้ำเกลือ เข็มและไหมเย็บ เป็นต้น บริษัทจะต้องเรียกเก็บภาษีมูลค่าเพิ่มหรือไม่       

        \textbf{Retrieved Section: } ประมวลรัษฎากร มาตรา 83/10 ในการเสียภาษีมูลค่าเพิ่ม
(1) สำหรับสินค้าที่นำเข้า ให้กรมศุลกากรเรียกเก็บเพื่อกรมสรรพากร และในกรณีของตกค้างตามมาตรา 78/2 (3) ให้กรมศุลกากรหักภาษีมูลค่าเพิ่มพร้อมเบี้ยปรับเงินเพิ่มเพื่อกรมสรรพากรตามหลักเกณฑ์ที่อธิบดีกำหนด
(2) สำหรับการขายสินค้าหรือการให้บริการที่ต้องเสียภาษีสรรพสามิตด้วย ให้กรมสรรพสามิตเรียกเก็บเพื่อกรมสรรพากร

        \textbf{Gold Section: } ประมวลรัษฎากร มาตรา 77/2 การกระทำกิจการดังต่อไปนี้ในราชอาณาจักร ให้อยู่ในบังคับต้องเสียภาษีมูลค่าเพิ่ม ตามบทบัญญัติในหมวดนี้

(1) การขายสินค้าหรือการให้บริการโดยผู้ประกอบการ

(2) การนำเข้าสินค้าโดยผู้นำเข้า

การให้บริการในราชอาณาจักรให้หมายถึง บริการที่ทำในราชอาณาจักรโดยไม่คำนึงว่าการใช้บริการนั้นจะอยู่ในต่างประเทศหรือในราชอาณาจักร

การให้บริการที่ทำในต่างประเทศและได้มีการใช้บริการนั้นในราชอาณาจักรให้ถือว่าการให้บริการนั้น เป็นการให้บริการในราชอาณาจักร
        \end{thai}
    \end{quote}



% \begin{minipage}[t]{0.5\textwidth}
%     \begin{quote}
%         \begin{thai}
%         "\textbf{Question: } บริษัทประกอบกิจการโรงพยาบาลสัตว์ให้บริการเป็นสถานรักษาสัตว์ ได้มีการสั่งซื้อยาเพื่อมาใช้รักษาสัตว์ เมื่อบริษัทฯ คิดค่ารักษาสัตว์กับลูกค้า บริษัทฯ ได้แยกค่ายากับค่าบริการรักษาสัตว์ออกจากกัน บริษัทจึงขอหารือว่า
%     1. บริษัทจะเรียกเก็บภาษีมูลค่าเพิ่มเฉพาะค่ารักษาสัตว์ แต่จะไม่เรียกเก็บภาษีมูลค่าเพิ่มสำหรับค่ายาที่ใช้รักษาสัตว์ ถูกต้องหรือไม่
%     2. ค่าเคมีภัณฑ์ที่ใช้สำหรับการรักษาสัตว์ เช่น น้ำยาล้างแผล แอลกอฮอล์ น้ำเกลือ เป็นต้น บริษัทจะต้องเรียกเก็บภาษีมูลค่าเพิ่มจากลูกค้าหรือไม่
%     3. ค่าเวชภัณฑ์ที่ใช้สำหรับรักษาสัตว์ เช่น สำลี สายน้ำเกลือ เข็มและไหมเย็บ เป็นต้น บริษัทจะต้องเรียกเก็บภาษีมูลค่าเพิ่มหรือไม่"        
%         \end{thai}
%     \end{quote}
% \end{minipage}
% \hfill
% \begin{minipage}[t]{0.5\textwidth}
%     \begin{quote}
%         \begin{thai}
%         "\textbf{Retrieved Section: } ประมวลรัษฎากร มาตรา 83/10 ในการเสียภาษีมูลค่าเพิ่ม
% (1) สำหรับสินค้าที่นำเข้า ให้กรมศุลกากรเรียกเก็บเพื่อกรมสรรพากร และในกรณีของตกค้างตามมาตรา 78/2 (3) ให้กรมศุลกากรหักภาษีมูลค่าเพิ่มพร้อมเบี้ยปรับเงินเพิ่มเพื่อกรมสรรพากรตามหลักเกณฑ์ที่อธิบดีกำหนด
% (2) สำหรับการขายสินค้าหรือการให้บริการที่ต้องเสียภาษีสรรพสามิตด้วย ให้กรมสรรพสามิตเรียกเก็บเพื่อกรมสรรพากร"   
%         \end{thai}
%     \end{quote}
% \end{minipage}

% To mitigate this problem, it is important to determine whether abstracts and generic sections should be regarded as foundational knowledge for the LLM in every query within a RAG system, rather than being selectively retrieved for each query. Another potential solution is to implement keyword matching retrieval for the terminology sections, as the phrases defined in those sections are often directly found in the queries themselves.

% In addition to the \textbf{"Generic Section Retrieval Challenge"}, two other types of errors identified in the Tax Case dataset are as follows:

% \textbf{1. Wrong Law Code Retrieval: } The distribution of false positives in the retrieved sections at the law code level is examined. The results are presented in Table \ref{table: retrieval_tax_code_fp}.

Two strategies may mitigate this issue. First, we should consider whether abstract and general sections constitute foundational knowledge that the LLM requires for all queries within a RAG system, rather than retrieving them selectively. Second, keyword matching retrieval could prove effective for terminology sections, as query terms often directly match defined phrases.

Beyond the Generic Section Retrieval Challenge, our analysis of the Tax Case dataset revealed two additional error categories:

\textbf{Incorrect Legislation Retrieval:} Table \ref{table: retrieval_tax_code_fp} presents the distribution of false positives at the law code level.



% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
\begin{table}[!h]
\centering
\caption{False positive distribution on Tax Case dataset on law code level}
\renewcommand{\arraystretch}{1.5} % This increases the cell height by 1.5 times
\label{table: retrieval_tax_code_fp}
\begin{tabular}{@{}cc@{}}
\toprule
Law Code                                                                                                                    & False Positive \\ \midrule
Revenue Code                                                                                                                & 280            \\
Petroleum Income Tax Act, B.E. 2514                                                                                         & 30             \\
Civil and Commercial Code                                                                                                   & 21             \\
Securities and Exchange Act, B.E. 2535                                                                                      & 15             \\
\begin{tabular}[c]{@{}c@{}}Government Procurement and Supplies Management Act, B.E.\\ 2560\end{tabular}                     & 15             \\
Budget Procedure Act, B.E. 2561                                                                                             & 14             \\
Energy Industry Act, B.E. 2550                                                                                              & 12             \\
Business Registration Act, B.E. 2499                                                                                        & 10             \\
Public Limited Companies Act, B.E. 2535                                                                                     & 8              \\
Energy Conservation Promotion Act, B.E. 2535                                                                                & 5              \\
Trust for Transactions in Capital Market Act, B.E. 2550                                                                     & 5              \\
Financial Institutions Business Act, B.E. 2551                                                                              & 4              \\
National Economic and Social Development Act, B.E. 2561                                                                     & 3              \\
Accounting Profession Act, B.E. 2547                                                                                        & 3              \\
\begin{tabular}[c]{@{}c@{}}Act on the Management of Shares and Stocks of Ministers, B.E.\\ 2543\end{tabular}                & 2              \\
State Enterprise Development and Governance Act, B.E. 2562                                                                  & 2              \\
Fiscal Discipline Act, B.E. 2561                                                                                            & 2              \\
Accounting Act, B.E. 2543                                                                                                   & 1              \\
\begin{tabular}[c]{@{}c@{}}Emergency Decree on Special Purpose Juristic Person for\\ Securitization, B.E. 2540\end{tabular} & 1              \\
Provident Fund Act, B.E. 2530                                                                                               & 1              \\
Emergency Decree on Digital Asset Businesses, B.E. 2561                                                                     & 1              \\
Foreign Business Act, B.E. 2542                                                                                             & 1              \\
Derivatives Act, B.E. 2546                                                                                                  & 1              \\ \bottomrule
\end{tabular}
\end{table}

% Although the ground truth labels of the Tax Case dataset are derived from only four legislation, the actual false positives in the retrieved sections originate from a significant 20 legislation. This issue is akin to the problem of hidden hierarchical information in the WCX dataset, where various sections across different legislation convey similar messages. For example, Section 91/15 of the Revenue Code and Section 56 of the Petroleum Act both address the authority of tax assessors, differing only in the conditions under which the laws apply.

% This problem is exacerbated in the Tax Case dataset, as the queries are directed to the Revenue Department, often omitting certain details since the department presumably references only the Revenue Code. To address this issue, in addition to the previously suggested solutions of incorporating hidden information into the section content and utilizing level-based retrieval, it is recommended that if a query pertains to a specific domain requiring only a limited number of legislation, only sections from those relevant legislation should be considered for retrieval. This approach will help reduce false positives from unrelated legislation.

While the Tax Case dataset's ground truth labels span only 4 legislation, retrieved false positives originate from 20 different legislation. This mirrors the hidden hierarchical information problem observed in the WCX dataset, where similar concepts appear in different legislation. For instance, both Section 91/15 of the Revenue Code and Section 56 of the Petroleum Act address tax assessor authority, differing only in their conditions of application.

This issue is amplified in the Tax Case dataset because queries, directed to the Revenue Department, often omit details implicitly covered by the Revenue Code's scope. Besides previously suggested solutions such as incorporating hidden information into section content and employing level-based retrieval, we recommend restricting retrieval to sections within relevant legislation when queries pertain to a specific domain. This targeted approach should reduce false positives from unrelated codes.


    \begin{quote}
        \begin{thai}
        \textbf{ประมวลรัษฎากร มาตรา 91/15 } เจ้าพนักงานประเมินมีอำนาจประเมินภาษี เบี้ยปรับ และเงินเพิ่มตามหมวดนี้ ในเมื่อ
(1) ปรากฏแก่เจ้าพนักงานประเมินว่าผู้มีหน้าที่เสียภาษีมิได้ยื่นแบบแสดงรายการภายในเวลาที่กฎหมายกำหนด...
        
        \textbf{พระราชบัญญัติภาษีเงินได้ปิโตรเลียม พ.ศ. 2514 มาตรา 56} เจ้าพนักงานประเมินมีอำนาจประเมินภาษี เบี้ยปรับ และเงินเพิ่มตามพระราชบัญญัตินี้ เมื่อ
(1) ผู้มีหน้าที่ยื่นแบบแสดงรายการเงินได้มิได้ยื่นแบบแสดงรายการเงินได้ภายในเวลาที่กำหนด...         
        \end{thai}
    \end{quote}



% \begin{minipage}[t]{0.45\textwidth}
%     \begin{quote}
%         \begin{thai}
%         "\textbf{ประมวลรัษฎากร มาตรา 91/15 } เจ้าพนักงานประเมินมีอำนาจประเมินภาษี เบี้ยปรับ และเงินเพิ่มตามหมวดนี้ ในเมื่อ
% (1) ปรากฏแก่เจ้าพนักงานประเมินว่าผู้มีหน้าที่เสียภาษีมิได้ยื่นแบบแสดงรายการภายในเวลาที่กฎหมายกำหนด..."
%         \end{thai}
%     \end{quote}
% \end{minipage}
% \hfill
% \begin{minipage}[t]{0.45\textwidth}
%     \begin{quote}
%         \begin{thai}
%         "\textbf{พระราชบัญญัติภาษีเงินได้ปิโตรเลียม พ.ศ. 2514 มาตรา 56} เจ้าพนักงานประเมินมีอำนาจประเมินภาษี เบี้ยปรับ และเงินเพิ่มตามพระราชบัญญัตินี้ เมื่อ
% (1) ผู้มีหน้าที่ยื่นแบบแสดงรายการเงินได้มิได้ยื่นแบบแสดงรายการเงินได้ภายในเวลาที่กำหนด..."           
%         \end{thai}
%     \end{quote}
% \end{minipage}

% \textbf{Wrong Tax Type Retrieval: } In addition to false positive retrievals due to similar content, the retrieval model frequently struggles to identify the type of tax relevant to a query, particularly with complex inquiries. This issue resembles the hard queries error type found in the WCX dataset.

% For example, consider a query regarding how an employee working in a subsidiary company, who receives income from both the subsidiary and the parent company in Thailand, should handle their tax obligations. This query specifically concerns personal income tax, and the retrieved sections should include Section 41, 50, 48, and 56 of the Revenue Code, which address personal income tax, the company's obligations to withhold it before paying the employee's salary, and the calculation of tax on income received from a foreign company.

% However, the retrieval model instead returns sections related to corporate tax and export tax. This misalignment may result from the query containing multiple keywords such as "company," "corporate," and "foreign," which could lead the query embedding to focus on sections concerning corporate tax and export tax rather than personal income tax.


\textbf{Incorrect Tax Type Retrieval}: Beyond false positives due to similar content, the retrieval model frequently misidentifies the relevant tax type, especially in complex queries. This resembles the "hard query" error type observed in the WCX dataset.

For example, a query about the tax obligations of an employee in Thailand receiving income from both a subsidiary and its parent company (a personal income tax question) should retrieve Sections 41, 48, 50, and 56 of the Revenue Code, which address personal income tax, withholding obligations, and calculating tax on foreign income.

However, the model instead retrieves sections related to corporate and export taxes. This likely stems from keywords like "company," "corporate," and "foreign" influencing the query embedding, shifting its focus away from personal income tax.


\begin{quote}
    \begin{thai}
    \textbf{Question: } บริษัทจัดตั้งขึ้นตามกฎหมายประเทศไทย ประกอบธุรกิจเกี่ยวกับสิ่งทอ และมีการลงทุนในธุรกิจที่เกี่ยวข้องทั้งในประเทศไทย และในต่างประเทศ บริษัทได้แต่งตั้งให้ผู้บริหารหรือพนักงานไปดำรงตำแหน่งกรรมการในบริษัทลูกในต่างประเทศ โดยจะได้รับค่าตอบแทนเงินเดือน สวัสดิการต่าง ๆ จากหน้าที่งานของบริษัทในประเทศไทย โดยผ่านบัญชีเงินฝากของผู้บริหารหรือพนักงานในประเทศไทย และค่าตอบแทนจากการเป็นกรรมการในบริษัทลูกในต่างประเทศ ตามอัตราค่าตอบแทนเปรียบเทียบกับค่าตอบแทนในตำแหน่งเดียวกัน หรือเทียบเคียงของธุรกิจหรืออุตสาหกรรมประเภทเดียวกัน หรือให้เป็นไปตามข้อกำหนดในแต่ละประเทศ โดยบริษัทลูกในต่างประเทศจะจ่ายค่าตอบแทนให้แก่ผู้บริหารโดยตรง โดยผ่านบัญชีเงินฝากในต่างประเทศที่บริษัทลูกในต่างประเทศเปิดไว้ให้ บริษัทขอหารือ ดังนี้
1. กรณีบริษัทจ่ายเงินเดือนหรือค่าตอบแทนเฉพาะที่เกิดจากหน้าที่งานของนายจ้างในประเทศไทย บริษัทมีหน้าที่นำเงินได้ที่ผู้บริหารหรือพนักงานได้รับเฉพาะที่จ่ายจากหน้าที่งานที่ทำให้นายจ้างในประเทศไทยมารวมคำนวณเป็นรายได้ เพื่อนำส่งภาษีหัก ณ ที่จ่าย ตามมาตรา 50 (1) แห่งประมวลรัษฎากร ใช่หรือไม่
2. กรณีผู้บริหารหรือพนักงานที่ได้รับค่าตอบแทนจากการเป็นกรรมการในบริษัทลูกในต่างประเทศ และได้อยู่ในประเทศไทยในปีภาษีเดียวกันกับที่ได้รับเงินได้ค่าตอบแทนเกิน 180 วัน หากผู้บริหารหรือพนักงานคนดังกล่าว ไม่ได้นำเงินค่าตอบแทนที่ได้รับดังกล่าว เข้ามาในประเทศไทยในปีภาษีเดียวกัน ผู้บริหารคนดังกล่าวไม่มีหน้าที่ต้องเสียภาษีเงินได้บุคคลธรรมดาเนื่องจากหน้าที่งานที่ทำในต่างประเทศตามมาตรา 41 วรรคสอง แห่งประมวลรัษฎากร ใช่หรือไม่        
    
    \textbf{Retrieved Section: } ประมวลรัษฎากร มาตรา 76 ทวิ บริษัทหรือห้างหุ้นส่วนนิติบุคคลซึ่งตั้งขึ้นตามกฎหมายของต่างประเทศมีลูกจ้าง หรือผู้ทำการแทน หรือผู้ทำการติดต่อ ในการประกอบกิจการในประเทศไทย ซึ่งเป็นเหตุให้ได้รับเงินได้หรือผลกำไรในประเทศไทยให้ถือว่า บริษัทหรือห้างหุ้นส่วนนิติบุคคลนั้นประกอบกิจการในประเทศไทยและให้ถือว่าบุคคลผู้เป็นลูกจ้างหรือผู้ทำการแทน หรือผู้ทำการติดต่อเช่นว่านั้นไม่ว่าจะเป็นบุคคลธรรมดาหรือนิติบุคคลเป็นตัวแทนของบริษัทหรือห้างหุ้นส่วนนิติบุคคลซึ่งตั้งขึ้นตามกฎหมายของต่างประเทศและให้บุคคลนั้นมีหน้าที่และความรับผิดในการยื่นรายการและเสียภาษีตามบทบัญญัติในส่วนนี้เฉพาะที่เกี่ยวกับเงินได้หรือผลกำไรที่กล่าวแล้ว
ในกรณีที่กล่าวในวรรคแรกถ้าบุคคลผู้มีหน้าที่และความรับผิดในการยื่นรายการและเสียภาษีไม่สามารถจะคำนวณกำไรสุทธิเพื่อเสียภาษีตามบทบัญญัติในส่วนนี้ได้ให้นำบทบัญญัติว่าด้วยการประเมินภาษีตามมาตรา71 (1) มาใช้บังคับโดยอนุโลม
ในกรณีการประเมินตามความในมาตรานี้จะอุทธรณ์การประเมินก็ได้


    \end{thai}
\end{quote}

% \subsection{End-to-End Result}
% \label{subsec: e2e_result}
% The performance of the RAG system is evaluated comprehensively in an end-to-end manner. Each component of the system is examined independently: chunking divides documents into meaningful and concise segments, retrieval indexes and identifies relevant chunks in response to a query, augmenting refines queries and retrieved documents to enhance contextual relevance with explicit knowledge, and generation produces accurate responses based on the enriched input. To isolate the impact of each component, multiple configurations are applied to one component at a time, while the remaining components are kept fixed in a consistent setting. This methodology allows for a detailed analysis of how variations in individual components influence the overall performance of the RAG system, assuming the components operate independently.

% \subsubsection{Effect of Chunking on End-to-End performance}
% \label{subsubsec: chunk_e2e_result}
% First, the impact of the chunking strategy on the overall performance of the system is evaluated. The possible configurations for the chunking strategy consist of two settings, as described in Section \ref{subsubsec:re_result_chunk}. The first is line chunking, which divides each document based on newline characters, and the second is hierarchical chunking, which segments each document according to its sections. As for other components, the BGE-M3 human-rerank finetuned model is employed as a retriever with no augmenter component and with gpt-4o-2024-08-06 as the generator component with temperature of 0.5. The performance of these configurations is assessed on both the WCX and Tax case datasets, with the results presented in Table \ref{table: chunk_e2e_main}.

\subsection{End-to-End Result}
\label{subsec: e2e_result}
We evaluate the RAG system's performance end-to-end, examining each component—chunking, retrieval, augmentation, and generation—independently. To isolate each component's impact, we vary one component's configuration at a time while holding others constant. This allows detailed analysis of how individual component variations affect overall system performance, assuming component independence.

\subsubsection{Effect of Chunking on End-to-End performance}
\label{subsubsec: chunk_e2e_result}
We first evaluate the impact of chunking strategy. Two configurations are considered (as described in Section \ref{subsubsec:re_result_chunk}): line chunking (splitting documents by newline characters) and hierarchical chunking (splitting by section). For the other components, we use the BGE-M3 human-rerank fine-tuned model for retrieval, without augmentation, and gpt-4o-2024-08-06 for generation (temperature = 0.5). Table \ref{table: chunk_e2e_main} presents the performance of these configurations in the WCX and Tax Case datasets.

\begin{table}[h!]
\centering
\caption{Effect of chunking configuration on E2E performance}
\renewcommand{\arraystretch}{1.5} % This increases the cell height by 1.5 times
\label{table: chunk_e2e_main}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}c|ccccccccc@{}}
\toprule
Dataset               & Setting               & Retriever MRR  & \multicolumn{1}{l}{Retriever Multi MRR} & Retriever Recall & Coverage      & Contradiction  & E2E Recall     & E2E Precision  & E2E F1         \\ \midrule
                      & Naïve Chunking        & 0.120          & {\color[HTML]{333333} 0.048}            & 0.062            & 44.0          & 0.520          & 0.229          & 0.478          & 0.310          \\
\multirow{-2}{*}{Tax} & Hierarchical Chunking & \textbf{0.574} & {\color[HTML]{333333} 0.333}            & \textbf{0.499}   & \textbf{50.0} & \textbf{0.460} & \textbf{0.333} & \textbf{0.64}  & \textbf{0.438} \\ \midrule
                      & Naïve Chunking        & 0.553          & {\color[HTML]{333333} 0.553}            & 0.658            & 74.0          & 0.108          & 0.714          & 0.492          & 0.583          \\
\multirow{-2}{*}{WCX} & Hierarchical Chunking & \textbf{0.809} & {\color[HTML]{333333} 0.809}            & \textbf{0.938}   & \textbf{85.2} & \textbf{0.055} & \textbf{0.880} & \textbf{0.601} & \textbf{0.714} \\ \bottomrule
\end{tabular}%
}
\end{table}

% The results from the table show that the hierarchical chunking configuration significantly outperforms the naive chunking approach. Additionally, for naive chunking, the end-to-end (E2E) recall score, which is measured based on the sections cited by the LLM in its response, is higher than the recall of the retrieved documents. Upon further analysis, this phenomenon can be attributed to two factors: 1) The LLM often cites sections that were not retrieved, either due to its parameterized knowledge or hallucinated content; and 2) In the naive chunking strategy, where each chunk is mapped to a single fully covered section when calculating retriever performance, some sections may be partially covered within a chunk, leading the LLM to cite portions of the section that are not fully covered in that chunk. As a result, the E2E recall score for naive chunking can exceed that of the retrieval process.

% The number of samples where hierarchical chunking outperforms or underperforms the naive chunking strategy is also examined. For the Tax Case dataset, There is a comparable number of queries where each chunking strategy performs better than one another in terms of coverage score. In contrast, the difference in the number of queries where each chunking strategy performs better is more pronounced in the WCX dataset, where hierarchical chunking shows a distinct advantage.

The results demonstrate that hierarchical chunking significantly outperforms line chunking. Interestingly, with line chunking, end-to-end (E2E) recall (based on sections cited by the LLM) exceeds retrieval recall. This stems from two factors: 1) LLM sometimes cites unretrieved sections, either from its internal knowledge or through hallucination; and 2) Line chunking's mapping of chunks to single, fully covered sections can lead to partial section coverage within a chunk, causing LLM to cite portions outside the mapped section. Consequently, line chunking's E2E recall can surpass its retrieval recall.

We also analyzed the number of samples where each chunking strategy performed better. For the Tax Case dataset, the performance difference is relatively balanced. However, in the WCX dataset, hierarchical chunking demonstrates a clear advantage, outperforming line chunking in considerably more queries.

\begin{figure}[h!]
\centering
\begin{minipage}{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/chunk_vary_won_lost_tax.png}  % Replace with your figure file
    \caption{Bar chart showing amount of queries performing better under hierarchical and naive chunking on Tax Case Dataset}
    \label{fig:chunk_vary_won_lost_tax}
\end{minipage}
\hfill
\begin{minipage}{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/chunk_vary_won_lost_wcx.png}  % Replace with your figure file
    \caption{Bar chart showing amount of queries performing better under hierarchical and naive chunking on WCX Dataset}
    \label{fig:chunk_vary_won_lost_wcx}
\end{minipage}
\label{fig:chunk_vary_won_lost}
\end{figure}

% Further analysis reveals that the hierarchical chunking strategy outperforms the naive chunking approach because chunking by line often splits sections with multiple conditions, each located on a new line, into several smaller chunks. This segmentation makes it more difficult for the retriever to retrieve these smaller chunks effectively. For instance, in a query asking whether the sale of calcium hypochlorite to aquaculture farmers as a product used in fisheries for controlling, preventing, and eliminating microorganisms, parasites, etc., is exempt from VAT, the relevant section is Section 81 of the Revenue Code, which outlines the conditions for VAT exemption. With hierarchical chunking, RAG is able to retrieve this entire section and answer the query correctly. In contrast, under the naive chunking approach, Section 81 is divided into multiple chunks, leading RAG to retrieve only parts of the section, resulting in an incorrect answer.

Further analysis reveals hierarchical chunking's superiority stems from its ability to preserve section integrity. Line chunking often fragments sections containing multiple conditions (each on a new line) into smaller, harder-to-retrieve chunks. For example, consider a query about VAT exemption for calcium hypochlorite sold to aquaculture farmers for microorganism control. The relevant section is section 81 of the Revenue Code, which details exemption conditions. Hierarchical chunking allows the RAG system to retrieve the entire section and answer correctly. Line chunking, however, fragments Section 81, leading to only partial retrieval and an incorrect answer.

\begin{quote}
    \begin{thai}
    \textbf{Question: } บริษัท ก. นำเข้าสินค้าแคลเซียมไฮโปคลอไรต์ (CALCIUM HYPOCHLORITE) โดยมีวัตถุประสงค์เพื่อจำหน่ายให้กับผู้เลี้ยงสัตว์น้ำ เนื่องจากสินค้าดังกล่าวมีคุณสมบัติเป็นผลิตภัณฑ์ที่ใช้ในทางการประมง และการเพาะเลี้ยงสัตว์น้ำ เพื่อประโยชน์แก่การควบคุม ป้องกัน กำจัด เชื้อจุลินทรีย์ ปรสิต พืช หรือสัตว์อื่น และได้รับการขึ้นทะเบียนเป็นวัตถุอันตราย ตามบัญชีรายชื่อวัตถุอันตราย แนบท้ายประกาศกระทรวงอุตสาหกรรม เรื่อง บัญชีรายชื่อ วัตถุอันตราย พ.ศ. 2556 ลงวันที่ 28 สิงหาคม พ.ศ. 2556 บัญชี 2 ที่กรมประมงรับผิดชอบ (บัญชีรายชื่อฯ) ได้รับยกเว้นภาษีมูลค่าเพิ่มหรือไม่ 
    
        \textbf{Retrieved by Hierarchical Chunking} ประมวลรัษฎากร มาตรา 81 ให้ยกเว้นภาษีมูลค่าเพิ่มสำหรับการประกอบกิจการประเภทต่าง ๆ ดังต่อไปนี้
(1) การขายสินค้าที่มิใช่การส่งออก หรือการให้บริการดังต่อไปนี้...
(จ) การขายยาหรือเคมีภัณฑ์ที่ใช้สำหรับพืชหรือสัตว์ เพื่อบำรุงรักษาป้องกัน ทำลายหรือกำจัดศัตรูหรือโรคของพืชและสัตว์
        
        \textbf{Retrieved by Naive Chunking} ประมวลรัษฎากร มาตรา 81 ให้ยกเว้นภาษีมูลค่าเพิ่มสำหรับการประกอบกิจการประเภทต่าง ๆ ดังต่อไปนี้
(1) การขายสินค้าที่มิใช่การส่งออก หรือการให้บริการดังต่อไปนี้           
        \end{thai}
    \end{quote}


% Next, an investigation was conducted into queries where naive chunking performs better. In the Tax Case dataset, it was found that these queries do not exhibit an increased E2E recall with naive chunking, despite having higher coverage scores. Furthermore, these queries often involve binary questions, resulting in a 50\% chance for the RAG system to provide a correct answer. Based on these observations, it can be concluded that for this type of query in the Tax Case dataset, the LLM accesses only parts of the relevant documents, which increases the likelihood of a correct response. An example of this is in a query which asks whether two companies which has similar holdings are considered as related companies or not. The retrieved sections from naive chunking strategy do not contain the relevant sections. However, the answer is correct which is be due to pure chance since the query is binary.

We also investigated queries where line chunking performed better on Coverage score. In Tax Case dataset, these queries did not exhibit increased E2E recall despite higher coverage scores. Furthermore, these queries often involved binary questions, giving the RAG system a 50\% chance of answering correctly. This suggests that for such queries, LLM only accesses parts of the relevant documents, yet this partial access, combined with chance, increases the likelihood of a correct response. For example, given a query about whether two companies with similar holdings are considered related, the system might retrieve irrelevant sections under line chunking, yet still produce a correct answer due to the binary nature of the question.


\begin{figure}[h!]
\centering
\includegraphics[width=\textwidth]{images/chunk_vary_recall_diff.png}
\caption{Line plot showing difference in Recall of naive and hierarchical chunking in the case that naive chunking outperforms on Tax Case Dataset}
\end{figure}

\begin{quote}
    \begin{thai}
    \textbf{Question: } บริษัท ก. นำเข้าสินค้าแคลเซียมไฮโปคลอไรต์ (CALCIUM HYPOCHLORITE) โดยมีวัตถุประสงค์เพื่อจำหน่ายให้กับผู้เลี้ยงสัตว์น้ำ เนื่องจากสินค้าดังกล่าวมีคุณสมบัติเป็นผลิตภัณฑ์ที่ใช้ในทางการประมง และการเพาะเลี้ยงสัตว์น้ำ เพื่อประโยชน์แก่การควบคุม ป้องกัน กำจัด เชื้อจุลินทรีย์ ปรสิต พืช หรือสัตว์อื่น และได้รับการขึ้นทะเบียนเป็นวัตถุอันตราย ตามบัญชีรายชื่อวัตถุอันตราย แนบท้ายประกาศกระทรวงอุตสาหกรรม เรื่อง บัญชีรายชื่อ วัตถุอันตราย พ.ศ. 2556 ลงวันที่ 28 สิงหาคม พ.ศ. 2556 บัญชี 2 ที่กรมประมงรับผิดชอบ (บัญชีรายชื่อฯ) ได้รับยกเว้นภาษีมูลค่าเพิ่มหรือไม่
    
    \textbf{Naive chunk retrieved sections (Irrelevant): }
    
    ประมวลกฎหมายแพ่งและพาณิชย์ มาตรา 1014
    บรรดาสำนักงานสำหรับจดทะเบียนห้างหุ้นส่วนและบริษัททั้งหลายนั้น ให้รัฐมนตรี*เจ้ากระทรวงซึ่งบัญชาการจดทะเบียนห้างหุ้นส่วนและบริษัทเป็นผู้ออกกฎข้อบังคับจัดตั้งขึ้น 

    
    ประมวลกฎหมายแพ่งและพาณิชย์ มาตรา 1015
    ห้างหุ้นส่วนหรือบริษัทเมื่อได้จดทะเบียนตามบัญญัติแห่งลักษณะนี้แล้ว ท่านจัดว่าเป็นนิติบุคคลต่างหากจากผู้เป็นหุ้นส่วนหรือผู้ถือหุ้นทั้งหลายซึ่งรวมเข้ากันเป็นหุ้นส่วนหรือบริษัทนั้น 
    
    % ประมวลกฎหมายแพ่งและพาณิชย์ มาตรา 1016 
    \end{thai}
\end{quote}

% In the case of the WCX dataset, the superior performance of naive chunking is attributed to the ambiguity of the query, which causes RAG with hierarchical chunking to retrieve similar but irrelevant sections. However, because these similar, yet irrelevant sections are chunked differently in the naive chunking setting, they are not retrieved, allowing the relevant sections to be accessed instead. These ambiguous queries are a result of the annotation pipeline that chooses one section for annotator to build question on and give an answer. This process does not consider the possibility that multiple sections of the law are similar and differ only in a tiny aspect of the condition they depend on. As a result, some queries are not specific enough for only a single section to be applied to. One example of this problem is in a query asking what are the roles of SEC in terms of digital assets. The relevant law annotated by human annotator is section 11 of Emergency Decree on Digital Asset Businesses, B.E. 2561. The section retrieved by RAG under hierarchical chunking is section 10 which also states about the role of SEC as well and the generator for RAG in this setting use this as reference to answer the question incorrectly. However, since section 10 contains multiple newline characters, it is separated into multiple chunks under naive chunking strategy and, thus, not retrieved by RAG which uses section 11, which is the next most relevant section instead and answer the query correctly.

For the WCX dataset, line chunking's occasional superiority stems from query ambiguity. With hierarchical chunking, ambiguous queries can retrieve similar but irrelevant sections. Line chunking, by differently fragmenting these irrelevant sections, sometimes avoids their retrieval, allowing relevant sections to surface instead. This ambiguity arises from the annotation pipeline, which focuses on single sections for question generation, overlooking potential overlap between similar sections differing only in nuanced conditions. Consequently, some queries lack the specificity needed to target a single section. For example, a query about the SEC's role regarding digital assets should retrieve section 11 of the Emergency Decree on Digital Asset Businesses. However, hierarchical chunking might retrieve the similar but slightly different section 10, leading to an incorrect answer. Because Section 10 contains multiple newlines, line chunking fragments it, preventing its retrieval and allowing the RAG system to utilize the next most relevant section (section 11) and answer correctly.

\begin{quote}
    \begin{thai}
    \textbf{Question: } คณะกรรมการกำกับหลักทรัพย์และตลาดหลักทรัพย์ (ก.ล.ต.) มีหน้าที่เกี่ยวข้องกับสินทรัพย์ดิจิทัลหรือไม่
    
    \textbf{Section retrieved under hierarchical chunking setting: } พระราชกำหนดการประกอบธุรกิจสินทรัพย์ดิจิทัล พ.ศ. 2561 มาตรา 10 เพื่อประโยชน์ในการกำกับและควบคุมการออกและเสนอขายโทเคนดิจิทัลและการประกอบธุรกิจสินทรัพย์ดิจิทัล ให้คณะกรรมการ ก.ล.ต. มีหน้าที่และอำนาจวางนโยบายเกี่ยวกับการส่งเสริมและพัฒนา ตลอดจนกำกับและควบคุมสินทรัพย์ดิจิทัลและผู้ประกอบธุรกิจสินทรัพย์ดิจิทัลตามที่บัญญัติไว้ในพระราชกำหนดนี้ หน้าที่และอำนาจดังกล่าวให้รวมถึง
(1) ออกระเบียบ ข้อบังคับ ประกาศ คำสั่ง หรือข้อกำหนดเกี่ยวกับการออกและเสนอขายโทเคนดิจิทัลและการประกอบธุรกิจสินทรัพย์ดิจิทัล...

    \textbf{Section retrieved under line chunking setting: } พระราชกำหนดการประกอบธุรกิจสินทรัพย์ดิจิทัล พ.ศ. 2561 มาตรา 11 ในกรณีที่มีการดำเนินการใด ๆ เกี่ยวกับสินทรัพย์ดิจิทัลอันอาจก่อให้เกิดผลกระทบหรือความเสียหายอย่างรุนแรงต่อประโยชน์ของประชาชนในวงกว้าง ให้คณะกรรมการ ก.ล.ต. รายงานข้อเท็จจริงและประเมินผลกระทบหรือความเสียหายที่อาจเกิดขึ้น พร้อมทั้งวิเคราะห์ปัญหาและเสนอแนวทางการดำเนินการต่อรัฐมนตรีเพื่อพิจารณาโดยเร็ว 

    \end{thai}
\end{quote}

\subsubsection{Effect of Retriever on End-to-End performance}
\label{subsubsec: retriever_e2e_result}

% Next, the effect of each setting of retriever is examined in an E2E manner. The possible configurations of retrieval component are BGE-M3 human-rerank finetuned, JinaAI Embeddings V3, BM25 and Cohere embeddings. Other components of the system are fixed with hierarchical chunking, no augmentation and gpt-4o-2024-08-06 as the generator. The result is shown in table \ref{table: retrieval_e2e_main}.

Next, we examine the end-to-end impact of different retriever configurations: BGE-M3 human-rerank fine-tuned, JinaAI Embeddings V3, BM25, and Cohere embeddings. We fix the other components using hierarchical chunking, no augmentation, and gpt-4o-2024-08-06 for generation. The results are presented in Table \ref{table: retrieval_e2e_main}.

\begin{table}[ht]
\centering
\caption{Effect of retrieval configuration on E2E performance}
\renewcommand{\arraystretch}{1.5} % This increases the cell height by 1.5 times
\label{table: retrieval_e2e_main}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}c|ccccccccc@{}}
\toprule
Dataset               & Setting                & Retriever MRR        & \multicolumn{1}{l}{Retriever Multi MRR} & Retriever Recall & Coverage      & Contradiction  & E2E Recall     & E2E Precision  & E2E F1         \\ \midrule
                      & BM25                   & 0.327                & {\color[HTML]{333333} 0.183}            & 0.320            & 36.0          & 0.600          & 0.271          & 0.582          & 0.370          \\
                      & JINA V3                & \underline{0.524}          & {\color[HTML]{333333} \underline{0.311}}      & \textbf{0.549}   & \underline{47.0}    & \underline{0.520}    & \underline{0.340}    & \textbf{0.690} & \underline{0.456}    \\
                      & Cohere                 & 0.454                & {\color[HTML]{333333} 0.263}            & 0.414            & \underline{47.0}    & \underline{0.520}    & \textbf{0.361} & 0.642          & \textbf{0.462} \\
\multirow{-4}{*}{Tax} & Human-Finetuned BGE-M3 & \textbf{0.574}       & {\color[HTML]{333333} \textbf{0.333} }           & \underline{0.499}      & \textbf{50.0} & \textbf{0.460} & 0.333          & \underline{0.640}    & 0.438          \\ \midrule
                      & BM25                   & 0.556                & {\color[HTML]{333333} 0.556}            & 0.716            & 73.3          & 0.123          & 0.687          & 0.571          & 0.624          \\
                      & JINA V3                & 0.689                & {\color[HTML]{333333} 0.689}            & 0.875            & 82.1          & 0.065          & 0.827          & 0.592          & 0.690          \\
                      & Cohere                 & \underline{\textbf{0.760}} & {\color[HTML]{333333} \underline{0.760}}      & \underline{0.912}      & \underline{84.4}    & \underline{0.060}    & \underline{0.857}    & \underline{0.597}    & \underline{0.704}    \\
\multirow{-4}{*}{WCX} & Human-Finetuned BGE-M3 & \textbf{0.809}       & \textbf{0.809}                                   & \textbf{0.938}   & \textbf{85.2} & \textbf{0.055} & \textbf{0.880} & \textbf{0.601} & \textbf{0.714} \\ \bottomrule
\end{tabular}%
}
\end{table}

% The table indicates that the Human-Finetuned BGE-M3 achieves the best performance across all four settings, demonstrating comparable results to JINA V3 on the Tax Case dataset and Cohere on the WCX dataset. Furthermore, the relationships between Multi-MRR, a retriever metric, and five end-to-end metrics (coverage, contradiction, E2E recall, E2E precision, and E2E F1) are visualized in Figure \ref{fig: retriever_vary_corr}. The figure reveals a strong positive correlation between Multi-MRR and the end-to-end metrics, except for the contradiction score, which exhibits a negative correlation. This trend aligns with expectations, as the overall system performance improves when the retrieval component performs effectively.

The results in Table \ref{table: retrieval_e2e_main} show that the human-fine-tuned BGE-M3 model performs best overall, achieving comparable results to JinaAI V3 on the Tax Case dataset and Cohere embeddings on the WCX dataset. Figure \ref{fig: retriever_vary_corr} visualizes the relationship between Multi-MRR (a retriever metric) and five end-to-end metrics: coverage, contradiction, E2E recall, E2E precision, and E2E F1. The figure reveals a strong positive correlation between Multi-MRR and all end-to-end metrics except for contradiction, which shows a negative correlation. This is expected, as better retrieval generally leads to better overall system performance.

\begin{figure}[ht]
\centering
\includegraphics[width=\textwidth]{images/retriever_vary_corr.png}
\caption{Scatter plot of retrieval metrics vs E2E metrics}
\label{fig: retriever_vary_corr}
\end{figure}

% Next, the number of samples that each retriever model produces answer and achieves the highest score of each E2E metric on is investigated. From figure \ref{fig:retriever_vary_win_tax} and \ref{fig:retriever_vary_win_wcx}, it is shown that Human-Finetuned BGE-M3, or 'wcx' in the figure, has the highest number of samples that it achieves highest E2E metric on. However, there is a large part of both Tax Case Dataset and WCX dataset that have the same E2E metric across all 4 settings. We further examine this type of queries by taking a look at the retrieval recall of this subset compared to the retrieval recall of the full dataset. The result is shown in table \ref{table: retrieval_e2e_subset}

We then analyzed the number of samples where each retriever achieved the highest score for each end-to-end metric. Figures \ref{fig:retriever_vary_win_tax} and \ref{fig:retriever_vary_win_wcx}, in which we exclude the samples that exhibit maximum score of each metric since we want to focus on the gap in performance of the retriever, show that the human-fine-tuned BGE-M3 model (\textbf{wcx} in the figures) most frequently achieved the highest E2E metric scores. However, a portion of both datasets exhibited identical E2E metric scores across all four retriever configurations. To further investigate these queries, we compared their retrieval recall to the full dataset's retrieval recall, as shown in Table \ref{table: retrieval_e2e_subset}.

\begin{figure}[H]
\centering
\begin{minipage}{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/retriever_vary_win_tax.png}  % Replace with your figure file
    \caption{Bar chart showing amount of times each retriever achieves highest score on a sample on Tax Dataset}
    \label{fig:retriever_vary_win_tax}
\end{minipage}
\hfill
\begin{minipage}{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/retriever_vary_win_wcx.png}  % Replace with your figure file
    \caption{Bar chart showing amount of times each retriever achieves highest score on a sample on WCX Dataset}
    \label{fig:retriever_vary_win_wcx}
\end{minipage}
\end{figure}

\begin{table}[ht]
\centering
\caption{Retriever recall on subset of datasets that have the same coverage score across 4 settings compared to the full dataset}
\renewcommand{\arraystretch}{1.5} % This increases the cell height by 1.5 times
\label{table: retrieval_e2e_subset}
\begin{tabular}{@{}cccc@{}}
\toprule
Dataset              & Setting                & Retriever Recall on Tie samples & Retriever Recall on All samples \\ \midrule
\multirow{4}{*}{Tax} & BM25                   & 0.333                           & 0.320                           \\
                     & JINA V3                & 0.524                           & 0.549                           \\
                     & Cohere                 & 0.411                           & 0.414                           \\
                     & Human-Finetuned BGE-M3 & 0.399                           & 0.499                           \\ \midrule
\multirow{4}{*}{WCX} & BM25                   & 0.522                           & 0.716                           \\
                     & JINA V3                & 0.659                           & 0.875                           \\
                     & Cohere                 & 0.691                           & 0.912                           \\
                     & Human-Finetuned BGE-M3 & 0.731                           & 0.938                           \\ \bottomrule
\end{tabular}

\end{table}

% From Table \ref{table: retrieval_e2e_subset}, the Tax and WCX datasets exhibit distinct types of errors. In the Tax dataset, there is little difference between the retriever recall on the subset and the full dataset. This suggests that while the RAG system can retrieve the relevant documents, it struggles to effectively utilize their content for legal reasoning to generate accurate responses. This limitation arises because the queries in this dataset are complex and often require additional reasoning steps not explicitly outlined in the retrieved documents. One example of this is the query asking whether Mrs. A who operates a business investing in debt instruments and has entered into a private fund management agreement, appointing an asset management company (AMC) as her authorized representative and manager of her private fund, titled "Mrs. A by AMC." should be subjected to specific business tax on the interest of the company's investment in promissory notes. The relevant law of this query is section 91/2 of Revenue Code which states that carrying out business operations in the manner of a commercial bank, such as providing loans, guarantees, currency exchange, issuing, purchasing, or selling promissory notes, or transferring funds abroad through various methods is subjected to the specific business tax. However, since the section itself does not explicitly states about the interest of promissory notes and the system needs to reason further that interest from promissory notes also comes from commercial-bank-like operations which is subjected to tax, RAG system, which can retrieve this section correctly but unable to perform additional legal reasoning, assumes that this case is not subjected to the tax.

Table \ref{table: retrieval_e2e_subset} reveals distinct error patterns in Tax and WCX datasets. In the Tax dataset, the retrieval recall for the subset of queries with identical E2E scores across all retrievers is similar to the full dataset's recall. This suggests that while the RAG system the bottleneck of these queries lies in the fact that the LLM struggles to use retrieved documents effectively for legal reasoning and generating accurate answers. This limitation arises because the dataset's queries are complex and often require reasoning steps not explicitly stated in the retrieved documents. For example, consider a query about whether a private fund managed by an asset management company on behalf of an individual investing in debt instruments is subject to a specific business tax on the interest earned. The relevant law (Section 91/2 of the Revenue Code) states that commercial bank-like operations (e.g., lending, guarantees, dealing with promissory notes) are subject to this tax. However, the law doesn't explicitly mention interest from promissory notes. The system needs to reason that such interest also falls under commercial bank-like operations and is therefore taxable. The RAG system, while able to retrieve the correct section, fails to perform this additional reasoning and incorrectly concludes that the interest is not taxable.

\begin{quote}
    \begin{thai}
    \textbf{Question: } นาง ก. ประกอบธุรกิจลงทุนในตราสารหนี้ ได้ทำสัญญาจัดการกองทุนส่วนบุคคลแต่งตั้งให้บริษัทหลักทรัพย์จัดการกองทุน (บลจ.) เป็นผู้รับมอบอำนาจและเป็นผู้บริหารกองทุนส่วนบุคคล “นาง ก. โดย บลจ.” ซึ่ง บลจ. ได้ลงทุนด้วยการซื้อตั๋วแลกเงินจากบริษัทผู้ออกตั๋วและระบุชื่อ “นาง ก. โดย บลจ.” เป็นผู้ทรงตั๋วเงิน … ดอกเบี้ยที่ได้รับจากการลงทุนในตั๋วแลกเงินดังกล่าวต้องเสียภาษีธุรกิจเฉพาะหรือไม่
    
    \textbf{Relevant Laws: }
    
    ประมวลรัษฎากร มาตรา 91/2 ภายใต้บังคับมาตรา 91/4 การประกอบกิจการดังต่อไปนี้ในราชอาณาจักร ให้อยู่ในบังคับต้องเสียภาษีธุรกิจเฉพาะตามบทบัญญัติในหมวดนี้
    
(1) การธนาคาร ตามกฎหมายว่าด้วยการธนาคารพาณิชย์หรือกฎหมายเฉพาะ

(2) การประกอบธุรกิจเงินทุน ธุรกิจหลักทรัพย์ ธุรกิจเครดิตฟองซิเอร์ตามกฎหมายว่าด้วยการประกอบธุรกิจเงินทุน ธุรกิจหลักทรัพย์ และธุรกิจเครดิตฟองซิเอร์

(3)การรับประกันชีวิตตามกฎหมายว่าด้วยการประกันชีวิต

(4) การรับจำนำตามกฎหมายว่าด้วยโรงรับจำนำ

\textbf{(5) การประกอบกิจการโดยปกติเยี่ยงธนาคารพาณิชย์ เช่น การให้กู้ยืมเงินค้ำประกัน แลกเปลี่ยนเงินตรา ออก ซื้อ หรือขายตั๋วเงิน หรือรับส่งเงินไปต่างประเทศด้วยวิธีต่าง ๆ}

(6) การขายอสังหาริมทรัพย์เป็นทางค้าหรือหากำไร ไม่ว่าอสังหาริมทรัพย์นั้นจะได้มาโดยวิธีใดก็ตาม ทั้งนี้ เฉพาะที่เป็นไปตามหลักเกณฑ์ วิธีการ และเงื่อนไขตามที่กำหนดโดยพระราชกฤษฎีกา…
    \end{thai}
\end{quote}

% In contrast, the WCX dataset shows a significant difference between the retriever recall on the subset and the full dataset, indicating that the primary issue lies in the model's overall poorer performance on this subset of queries. This underperformance is likely due to factors similar to the four error categories discussed in Section \ref{subsubsec:re_result_discussion}. An additional observation in this subset involves cases where the retriever successfully retrieves the relevant documents, but the generator fails to produce accurate responses. Further investigation reveals two prominent reasons for this failure

% First, the generator attempts to summarize lengthy sections with multiple conditions into concise and meaningful answers but often fails to address all conditions or merges them incorrectly. This leads to a coverage score dropping from 100. This issue may stem from the prompt instructing the LLM to provide concise responses. For instance, a query asking in what cases the court can revoke a declaration of disappearance requires the system to retrieve section 63 of the Civil and Commercial Code, which states two conditions that the court can take this action, to answer. All RAG system under 4 settings can retrieve this section correctly. However, when the generator generates a response, it makes an attempt to summarize the section and leave out one of the condition. This type of error might be solved with prompting the LLM to determine if it should quote the answer directly for some questions or not before answering.

In contrast, WCX dataset shows a significant difference in retrieval recall between the subset and the full dataset, indicating deficient overall performance on this subset. This likely stems from issues similar to the four error categories discussed in Section \ref{subsubsec:re_result_discussion}. Additionally, within this subset, we observe cases where successful retrieval is followed by inaccurate answer generation

A pattern occurs when observing these queries. We observe that the generator often struggles to summarize long, multi-conditional sections concisely while addressing all conditions accurately. This often leads to incomplete or incorrectly merged conditions, lowering the coverage score from 100. The prompt's emphasis on conciseness may exacerbate this issue. For example, a query about revoking a declaration of disappearance requires retrieving Section 63 of the Civil and Commercial Code, which outlines two conditions for such revocation. All four RAG configurations retrieve this section correctly. However, the generator, in attempting to summarize, often omits one condition. Prompting the LLM to determine whether direct quotation is appropriate might mitigate this issue.

\begin{quote}
    \begin{thai}
    \textbf{Question: } ศาลสามารถถอนคำสั่งให้เป็นคนสาบสูญในกรณีใดบ้าง
    
    \textbf{Relevant Laws: }
    ประมวลกฎหมายแพ่งและพาณิชย์ มาตรา 63

    เมื่อบุคคลผู้ถูกศาลสั่งให้เป็นคนสาบสูญนั้นเองหรือผู้มีส่วนได้เสียหรือพนักงานอัยการร้องขอต่อศาล และพิสูจน์ได้ว่าบุคคลผู้ถูกศาลสั่งให้เป็นคนสาบสูญนั้นยังคงมีชีวิตอยู่ก็ดี หรือว่าตายในเวลาอื่นผิดไปจากเวลาดังระบุไว้ในมาตรา 62 ก็ดี ให้ศาลสั่งถอนคำสั่งให้เป็นคนสาบสูญนั้น แต่การถอนคำสั่งนี้ย่อมไม่กระทบกระเทือนถึงความสมบูรณ์แห่งการทั้งหลายอันได้ทำไปโดยสุจริตในระหว่างเวลาตั้งแต่ศาลมีคำสั่งให้เป็นคนสาบสูญจนถึงเวลาถอนคำสั่งนั้น
    บุคคลผู้ได้ทรัพย์สินมาเนื่องแต่การที่ศาลสั่งให้บุคคลใดเป็นคนสาบสูญ แต่ต้องเสียสิทธิของตนไปเพราะศาลสั่งถอนคำสั่งให้บุคคลนั้นเป็นคนสาบสูญ ให้นำบทบัญญัติว่าด้วยลาภมิควรได้แห่งประมวลกฎหมายนี้มาใช้บังคับโดยอนุโลม 

    \textbf{RAG Answer: }

    ศาลสามารถถอนคำสั่งให้เป็นคนสาบสูญได้เมื่อพิสูจน์ได้ว่าบุคคลนั้นยังคงมีชีวิตอยู่ หรือว่าตายในเวลาอื่นผิดไปจากเวลาที่ระบุไว้ในคำสั่งให้เป็นคนสาบสูญ 
    \end{thai}
\end{quote}

% ===========================REMOVE AFTER CLEANING WCX DATASET====================================
% % In some cases, the relevant laws are inherently confusing or ambiguous, causing the generator to misinterpret them and produce answers that contradict the reference. For example, in a query asking whether it is illegal to use the word \begin{thai}"ห้างหุ้นส่วน (Partnership)o"\end{thai} on signs, letters, or notices, the system needs to retrieve section 3 of Business Registration Act, B.E. 2499 to be able to answer the query. However, the actual content of this section is confusing by nature with the use of double negatives that causes the LLM to understand it in the opposite way and, thus, answer incorrectly.

% Second, inherent ambiguity or confusing language within the relevant laws can lead to misinterpretation and contradictory answers. For example, a query about the legality of using the Thai word \begin{thai}"ห้างหุ้นส่วน (Partnership)"\end{thai} on signs requires retrieving section 3 of the Business Registration Act. However, this section's language, with its double negatives, is inherently confusing, leading the LLM to misinterpret it and answer incorrectly.

% \begin{quote}
%     \begin{thai}
%     \textbf{Question: } ถ้าใช้ชื่อห้างหุ้นส่วนลงในป้ายชื่อ จดหมาย ใบแจ้งความ ผิดกฎหมายไหม
    
%     \textbf{Relevant Laws: }
%     พระราชบัญญัติกำหนดความผิดเกี่ยวกับห้างหุ้นส่วนจดทะเบียน ห้างหุ้นส่วนจำกัด บริษัทจำกัด สมาคม และมูลนิธิ พ.ศ. 2499 มาตรา 3 
    
%     ห้างหุ้นส่วนจดทะเบียน หรือห้างหุ้นส่วนจำกัดใดในกรณีใช้ชื่อในดวงตรา ป้ายชื่อ หนังสือบอกกล่าวป่าวร้อง จดหมาย ใบแจ้งความ หรือเอกสารอย่างอื่นเกี่ยวกับธุรกิจของห้างหุ้นส่วน
    
%     (1) ถ้าเป็นอักษรไทย ไม่ใช้คำว่า “ห้างหุ้นส่วนสามัญนิติบุคคล” หรือ “ห้างหุ้นส่วนจำกัด” ประกอบชื่อ แล้วแต่กรณี
    
%     (2) ถ้าเป็นอักษรต่างประเทศ ไม่ใช้คำซึ่งมีความหมายว่า “ห้างหุ้นส่วนสามัญนิติบุคคล” หรือ “ห้างหุ้นส่วนจำกัด” ตามประกาศของกระทรวงเศรษฐการประกอบชื่อ แล้วแต่กรณี
%     มีความผิดทางพินัยต้องชำระค่าปรับเป็นพินัยไม่เกินห้าหมื่นบาท และชำระค่าปรับเป็นพินัยอีกวันละไม่เกินห้าร้อยบาท* จนกว่าจะได้ปฏิบัติให้ถูกต้อง 
%     \end{thai}
% \end{quote}

\subsubsection{Effect of Augmenter on End-to-End performance}
\label{subsubsec: augmenter_e2e_result}

% The effect of the augmenter (referencer) was analyzed under two configurations. The first configuration, referred to as No-Referencer, utilized a Retrieval-Augmented Generation (RAG) model without any additional augmenter. The second configuration, Referencer Depth 1, employed an augmenter that retrieves immediate nested laws within the initially retrieved laws in a depth-first manner. To manage computational costs, the augmentation was limited to immediate nested laws, as increasing the depth of the referencer significantly expands the token count. For the remaining components, a golden chunking strategy was adopted, with a Human-Finetuned BGE-M3 serving as the retriever and GPT-4o-2024-08-06 as the generator. The results are presented in Table \ref{table: augmenter_e2e_main}.

We analyzed the augmenter's effect under two configurations: "No-Referencer" (standard RAG with no augmentation) and "Referencer Depth 1" (augmenting with nested laws retrieved depth-first from the initial retrieval). Augmentation depth was limited to one level for computational feasibility. For the other components, we used a "golden" (i.e., optimal) chunking strategy, the human-fine-tuned BGE-M3 retriever, and GPT-4o-2024-08-06 as the generator. Table \ref{table: augmenter_e2e_main} presents the results.

\begin{table}[ht]
\centering
\caption{Effect of augmenter configuration on E2E performance}
\renewcommand{\arraystretch}{1.5} % This increases the cell height by 1.5 times
\label{table: augmenter_e2e_main}
\begin{tabular}{@{}c|cc|cc@{}}
\toprule
Dataset                                  & \multicolumn{2}{c|}{Tax}        & \multicolumn{2}{c}{WCX}         \\ \midrule
Setting                                  & Ref Depth 1    & No Ref         & Ref Depth 1    & No Ref         \\ \midrule
Retriever MRR                            & 0.574          & 0.574          & 0.809          & 0.809          \\
\multicolumn{1}{l|}{Retriever Multi MRR} & 0.333          & 0.333          & 0.809          & 0.809          \\
Retriever Recall                         & 0.499          & 0.499          & 0.938          & 0.938          \\
Referencer MRR                           & \textbf{0.582} & 0.574          & 0.800          & \textbf{0.809} \\
Referencer Multi MRR                     & \textbf{0.345} & 0.333          & 0.800          & \textbf{0.809} \\
Referencer Recall                        & \textbf{0.602} & 0.499          & \textbf{0.940} & 0.938          \\
Coverage                                 & 45.0           & \textbf{50.0}  & \textbf{86.3}  & 85.2           \\
Contradiction                            & 0.520          & \textbf{0.460} & \textbf{0.051} & 0.055          \\
E2E Recall                               & \textbf{0.354} & 0.333          & \textbf{0.885} & 0.880          \\
E2E Precision                            & 0.630          & \textbf{0.64}  & 0.579          & \textbf{0.601} \\
E2E F1                                   & \textbf{0.453} & 0.438          & 0.700          & \textbf{0.714} \\ \bottomrule
\end{tabular}%
\end{table}

% The result table highlights differences in the impact of the referencer on retrieval metrics across datasets. For the Tax dataset, the referencer significantly increases recall but shows only marginal improvement in MRR and MultiMRR. This suggests that the additional nested laws retrieved by the augmenter are often located at the end of the query's ranking. Furthermore, despite the improvement in recall, the end-to-end performance deteriorates in some metrics. This outcome may stem from the complexity of queries in the Tax dataset, which often require advanced reasoning capabilities from the LLM. As a result, the LLM struggles to generate accurate responses even when the correct document is retrieved.

% For the WCX dataset, the referencer yields only a minimal increase in recall, and its use results in a decline in MRR. This decline occurs because augmenting the query often pushes the correct sections further down the ranked list. Several factors may explain the limited recall improvement for this dataset. Unlike the Tax dataset, WCX queries typically pertain to a single relevant law, making recall binary (0 or 1) and more challenging to enhance. Additionally, the simple and non-specific nature of WCX queries frequently leads the retriever to select multiple similar but non-hierarchically related sections. This observation raises the question on the importance of augmenting hierarchical information with handcrafted categorical labels in the knowledge graph for improved retrieval performance.

% The hierarchical structure of the WCX dataset also poses challenges. For instance, a substantial proportion of the Revenue Code lacks hierarchical connections: 41\% of sections have no children, 45\% have no parents, and 24\% lack both. Consequently, these sections cannot be retrieved by the referencer, limiting recall improvements for queries requiring such sections. Additionally, as noted in Section \ref{subsubsec:re_result_discussion}, certain queries—particularly those related to punishments for crimes—require the retrieval of laws that reference multiple others. The current referencer implementation, which retrieves only the children of initially retrieved laws, struggles with such queries because it cannot retrieve back to the parent law which is the ground truth resulting in non-improved retrieval performance.

% Despite the minimal recall gains in the WCX dataset, the referencer improves most end-to-end performance metrics. This suggests that even modest increases in recall can enhance the LLM's ability to answer queries more effectively for this dataset.

% The histograms depicting the difference between retriever recall and referencer recall for subsets of the datasets where the retriever recall is less than 1.0 are shown in Figures \ref{fig:augmenter_vary_recall_diff_tax} and \ref{fig:augmenter_vary_recall_diff_wcx}. The distributions reveal a larger proportion of cases with increased recall in the Tax dataset compared to the WCX dataset.

The results highlight differing impacts of the referencer on retrieval metrics across datasets. For the Tax dataset, the referencer significantly improves recall but only marginally affects MRR and Multi-MRR. This suggests that the augmented nested laws often appear lower in the retrieval ranking. Despite the recall improvement, some end-to-end metrics decline. This might be due to the Tax dataset's query complexity, which often demands advanced reasoning capabilities that the LLM, even with the correct documents, struggles to provide. Another reason that might effect the performance decline even with more relevant documents provided to the LLM is the longer context that the LLM needs to process due to higher amount of content added by the referencer.

For the WCX dataset, the referencer yields minimal recall improvement and even decreases MRR. Augmentation often pushes relevant sections lower in the ranking. Several factors contribute to the limited recall gain: 1) WCX queries typically involve a single relevant law, making recall binary and harder to improve; 2) Simple, non-specific WCX queries often lead to the retrieval of multiple similar but non-hierarchically related sections, raising questions about the importance of combining hierarchical information with handcrafted categorical labels in the knowledge graph; and 3) The WCX dataset's hierarchical structure itself presents challenges. Many sections in the Revenue Code lack hierarchical connections (41\% have no children, 45\% no parents, and 24\% neither), limiting the referencer's effectiveness. Furthermore, as noted in Section \ref{subsubsec:re_result_discussion}, some queries (e.g., those about criminal penalties) require retrieving laws that reference multiple others. The current referencer, retrieving only children of initially retrieved laws, struggles with these, as it cannot retrieve the parent law (which might be the ground truth).

Despite the limited recall gains in the WCX dataset, the referencer improves most end-to-end metrics. This suggests that even small recall improvements can enhance the LLM's ability to answer WCX queries effectively.

Figures \ref{fig:augmenter_vary_recall_diff_tax} and \ref{fig:augmenter_vary_recall_diff_wcx} (histograms of the difference between retriever and referencer recall when retriever recall is below 1.0) show a greater proportion of recall increases in the Tax dataset compared to the WCX dataset.

\begin{figure}[H]
\centering
\begin{minipage}{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/augmenter_vary_recall_diff_tax.png}  % Replace with your figure file
    \caption{Histogram of difference of referencer recall and retriever recall on Tax Case Dataset}
    \label{fig:augmenter_vary_recall_diff_tax}
\end{minipage}
\hfill
\begin{minipage}{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/augmenter_vary_recall_diff_wcx.png}  % Replace with your figure file
    \caption{Histogram of difference of referencer recall and retriever recall on WCX Dataset}
    \label{fig:augmenter_vary_recall_diff_wcx}
\end{minipage}
\end{figure}

% Next, the relationship between the maximum depth of referencer and the retrieval performance gains and the total number of sections parsed to generator is displayed in figure \ref{fig: augmenter_vary_retrieval_diff_tax} and \ref{fig: augmenter_vary_retrieval_diff_wcx}. For tax dataset, the gains in retrieval performance increases as maximum depth increases until it reaches the maximum value at approximately maximum depth of 6. However, this gains come with a disadvantage of longer context of around 60 sections per query. As for WCX, the gain in retrieval performance is minimal and stop increasing after maximum depth of 2.

We then examined the relationship between the referencer's maximum depth, retrieval performance gains, and the number of sections passed to the generator (see Figure \ref{fig: augmenter_vary_retrieval_diff_tax} and \ref{fig: augmenter_vary_retrieval_diff_wcx}). For the Tax dataset, retrieval performance improves with increasing depth, peaking around a depth of 6. However, this comes at the cost of increased context length, reaching approximately 60 sections per query. For the WCX dataset, retrieval gains are minimal and plateau after a depth of 2.


\begin{figure}[ht]
\centering
\includegraphics[width=\textwidth]{images/augmenter_vary_retrieval_diff_tax.png}
\caption{Plot of relationship between depth of referencer and retrieval performance and number of section per query on Tax dataset}
\label{fig: augmenter_vary_retrieval_diff_tax}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[width=\textwidth]{images/augmenter_vary_retrieval_diff_wcx.png}
\caption{Plot of relationship between depth of referencer and retrieval performance and number of section per query on WCX dataset}
\label{fig: augmenter_vary_retrieval_diff_wcx}
\end{figure}

% Furthermore, the plots illustrating the differences in end-to-end metrics between augmented and non-augmented subsets of queries with recall improvements from the referencer are presented in Figures \ref{fig:augmenter_vary_e2e_gain_tax} and \ref{fig:augmenter_vary_e2e_gain_wcx}. These plots reinforce the previously observed trend: improvements in retriever performance for the Tax dataset do not consistently translate to enhancements in end-to-end performance due to both complexity of queries and the increased context length, whereas in the WCX dataset, increases in retriever performance significantly boost end-to-end outcomes.

Figures \ref{fig:augmenter_vary_e2e_gain_tax} and \ref{fig:augmenter_vary_e2e_gain_wcx} show the differences in end-to-end metrics between augmented and non-augmented subsets of queries where the referencer improved recall. These plots confirm the previous trend: improved retrieval performance in the Tax dataset doesn't consistently translate to better end-to-end performance due to query complexity and increased context length. In contrast, retrieval improvements in the WCX dataset significantly enhance end-to-end results.

\begin{figure}[H]
\centering
\begin{minipage}{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/augmenter_vary_e2e_gain_tax.png}  % Replace with your figure file
    \caption{Difference between End-to-End performance on subset that has improved recall on Tax dataset}
    \label{fig:augmenter_vary_e2e_gain_tax}
\end{minipage}
\hfill
\begin{minipage}{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/augmenter_vary_e2e_gain_wcx.png}  % Replace with your figure file
    \caption{Difference between End-to-End performance on subset that has improved recall on WCX dataset}
    \label{fig:augmenter_vary_e2e_gain_wcx}
\end{minipage}
\end{figure}

\subsubsection{Effect of LLM on End-to-End performance}
\label{subsubsec: llm_e2e_result}

% The effect of language model (LLM) selection in a Retrieval-Augmented Generation (RAG) pipeline was evaluated under three distinct configurations: OpenAI's gpt-4o-2024-08-06, Anthropic's claude-3.5-sonnet-20240620, and Google's gemini-1.5-pro-002. All models were configured with a temperature of 0.3 and a maximum output token limit of 2048. Hierarchical chunking was employed as the text chunking strategy, while retrieval was performed using the Human-Finetuned BGE-M3 model. Two configurations of the augmenter component were included in the analysis, namely referencer with max depth 1 and no referencer, given that no significant differences were observed between the two setups in previous experiments. The experiment results are displayed in Table \ref{table: llm_e2e_main_tax} for Tax Dataset and Table \ref{table: llm_e2e_main_wcx} for WCX dataset.

We evaluate the impact of LLM selection in the RAG pipeline using three models: OpenAI's gpt-4o-2024-08-06, Anthropic's claude-3.5-sonnet-20240620, Google's gemini-1.5-pro-002 and two open-source Thai LLMs: typhoon-v2-70b-instruct and typhoon-v2-8b-instruct. All models are configured with a temperature of 0.5 and a maximum output token limit of 2048. We used hierarchical chunking and the human-fine-tuned BGE-M3 retriever. Given previous findings, we test both a referencer with maximum depth 1 and no referencer. Results are presented in Table \ref{table: llm_e2e_main_tax} (Tax Dataset) and Table \ref{table: llm_e2e_main_wcx} (WCX Dataset).

\begin{table}[ht]
\centering
\caption{Effect of LLM configuration on E2E performance on Tax Dataset}
\renewcommand{\arraystretch}{1.5} % This increases the cell height by 1.5 times
\label{table: llm_e2e_main_tax}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}c|ccccccccc@{}}
\toprule
Setting                                     & Referencer  & Retriever MRR & Retriever Multi MRR & Retriever Recall & Coverage      & Contradiction & E2E Recall     & E2E Precision  & E2E F1         \\ \midrule
\multirow{2}{*}{gpt-4o-2024-08-06}          & Ref Depth 1 & 0.574         & 0.333               & 0.499            & 45.0          & 0.52          & 0.354          & 0.630          & 0.453          \\
                                            & No Ref      & 0.574         & 0.333               & 0.499            & 50.0          & \underline{0.46}    & 0.333          & \underline{0.64}     & 0.438          \\ \midrule
\multirow{2}{*}{gemini-1.5-pro-002}         & Ref Depth 1 & 0.574         & 0.333               & 0.499            & 45.0          & 0.48          & 0.354          & 0.347          & 0.351          \\
                                            & No Ref      & 0.574         & 0.333               & 0.499            & 44.0          & 0.48          & 0.361          & 0.308          & 0.332          \\ \midrule
\multirow{2}{*}{claude-3-5-sonnet-20240620} & Ref Depth 1 & 0.574         & 0.333               & 0.499            & 49.0          & 0.56          & \textbf{0.417} & 0.577          & \textbf{0.484} \\
                                            & No Ref      & 0.574         & 0.333               & 0.499            & \underline{51.0}    & \textbf{0.44} & \underline{0.389}    & 0.554          & \underline{0.457}    \\ \midrule
\multirow{2}{*}{typhoon-v2-70b-instruct}    & Ref Depth 1 & 0.574         & 0.333               & 0.499            & \textbf{54.0} & \underline{0.46}    & 0.333          & 0.453          & 0.384          \\
                                            & No Ref      & 0.574         & 0.333               & 0.499            & 42.0          & 0.58          & 0.326          & \textbf{0.662} & 0.437          \\ \midrule
\multirow{2}{*}{typhoon-v2-8b-instruct}     & Ref Depth 1 & 0.574         & 0.333               & 0.499            & 35.0          & 0.54          & 0.319          & 0.561          & 0.407          \\
                                            & No Ref      & 0.574         & 0.333               & 0.499            & 37.0          & 0.60          & 0.278          & 0.471          & 0.349          \\ \bottomrule
\end{tabular}
}
\end{table}

\begin{table}[ht]
\centering
\caption{Effect of LLM configuration on E2E performance on WCX Dataset}
\renewcommand{\arraystretch}{1.5} % This increases the cell height by 1.5 times
\label{table: llm_e2e_main_wcx}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}c|ccccccccc@{}}
\toprule
Setting                                     & Referencer  & Retriever MRR & Retriever Multi MRR & Retriever Recall & Coverage      & Contradiction  & E2E Recall     & E2E Precision  & E2E F1         \\ \midrule
\multirow{2}{*}{gpt-4o-2024-08-06}          & Ref Depth 1 & 0.809         & 0.809               & 0.938            & 86.3          & 0.051          & 0.885          & \underline{0.579}    & \underline{0.700}    \\
                                            & No Ref      & 0.809         & 0.809               & 0.938            & 85.2          & 0.055          & 0.880          & \textbf{0.601} & \textbf{0.714} \\ \midrule
\multirow{2}{*}{gemini-1.5-pro-002}         & Ref Depth 1 & 0.809         & 0.809               & 0.938            & 87.3          & \underline{0.042}    & \underline{0.895}    & 0.491          & 0.634          \\
                                            & No Ref      & 0.809         & 0.809               & 0.938            & 86.5          & 0.048          & 0.892          & 0.512          & 0.651          \\ \midrule
\multirow{2}{*}{claude-3-5-sonnet-20240620} & Ref Depth 1 & 0.809         & 0.809               & 0.938            & \underline{89.5}    & 0.044          & 0.894          & 0.443          & 0.592          \\
                                            & No Ref      & 0.809         & 0.809               & 0.938            & \textbf{89.7} & \textbf{0.040} & \textbf{0.901} & 0.444          & 0.595          \\ \midrule
\multirow{2}{*}{typhoon-v2-70b-instruct}    & Ref Depth 1 & 0.809         & 0.809               & 0.938            & 79.9          & 0.080          & 0.845          & 0.573          & 0.683          \\
                                            & No Ref      & 0.809         & 0.809               & 0.938            & 81.2          & 0.076          & 0.862          & 0.537          & 0.662          \\ \midrule
\multirow{2}{*}{typhoon-v2-8b-instruct}     & Ref Depth 1 & 0.809         & 0.809               & 0.938            & 68.5          & 0.145          & 0.718          & 0.385          & 0.501          \\
                                            & No Ref      & 0.809         & 0.809               & 0.938            & 70.8          & 0.134          & 0.775          & 0.387          & 0.516          \\ \bottomrule
\end{tabular}
}
\end{table}

% From the results on the Tax dataset (Table \ref{table: llm_e2e_main_tax}), claude-3.5-sonnet demonstrates superior performance across most End-to-End (E2E) metrics, including Coverage, Contradiction, E2E Recall, and E2E F1 score. However, both claude-3.5-Sonnet and gemini-1.5-Pro-002 exhibit significantly lower E2E Precision compared to the gpt-4o model. This precision gap limits the overall effectiveness of these models in precision-critical applications, despite their advantages in other areas.

% Regarding the augmenter component, the inclusion of the referencer does not consistently improve performance. The End-to-End metrics show mixed results, with some models benefiting from the referencer while others exhibit a decline in performance. This variability underscores the lack of a definitive advantage in using the referencer in its current configuration.

% A similar trend is observed on the WCX dataset, where claude-3.5-sonnet maintains its dominance in Coverage, Contradiction, and E2E Recall metrics. However, as with the Tax dataset, both claude-3.5-sonnet and gemini-1.5-pro-002 perform poorly on E2E Precision, leading to lower F1 scores compared to the gpt-4o model. This pattern highlights a trade-off between achieving high recall and maintaining precision, particularly for claude-3.5-sonnet and gemini-1.5-pro-002. Further analysis on the drop of precision score is conducted in later section.

% In addition to the results presented in the main table analyzing the effect of LLMs, two additional metrics were calculated and are displayed in Table \ref{table: llm_e2e_recall_diff}. The first metric is the difference between retriever recall and End-to-End (E2E) recall, which reflects the ability of a Retrieval-Augmented Generation (RAG) system to effectively utilize relevant retrieved documents for legal reasoning and answer generation. A lower value for this metric indicates a higher capability of the model to leverage all relevant retrieved content efficiently.

% The second metric is the ratio of samples with zero E2E recall but non-zero coverage, which indicates the likelihood that a system can generate partially or fully correct answers without relying on correctly retrieved documents. This behavior may result from the LLM’s parametric knowledge or hallucinations.

% From the results, the claude-3.5-sonnet model achieves the lowest difference between retriever recall and E2E recall across both datasets, demonstrating its strong ability to utilize correctly retrieved documents for legal reasoning. This performance contributes to Claude’s high Coverage scores and low Contradiction rates. Regarding the hallucination ratio, the OpenAI model achieves the lowest percentage on the Tax dataset, while Claude and Gemini models perform best on the WCX dataset.

% Furthermore, the results indicate that the difference in recall on the Tax dataset is significantly higher than on the WCX dataset, suggesting that the Tax dataset poses greater challenges in applying correctly retrieved documents for reasoning and answer generation.


The results on the Tax dataset (Table \ref{table: llm_e2e_main_tax}) demonstrate that claude-3.5-sonnet achieves top-2 performance across most end-to-end metrics. Interestingly, the open-source model typhoon-v2-70b-instruct delivers comparable results, outperforming others on the Tax Case dataset with the highest coverage score and E2E precision. However, its smaller variant, typhoon-v2-8b-instruct, ranks the lowest among the tested models. Despite its limited parameter size, it manages to avoid falling significantly behind, showcasing a reasonable performance given its constraints.

Notably, both claude-3.5-sonnet and gemini-1.5-pro-002 exhibit considerably lower E2E precision compared to gpt-4o and typhoon-v2-70b-instruct. This compromises their suitability for precision-critical applications, even though they excel in other areas. Additionally, the referencer module fails to consistently enhance performance, with mixed results indicating no definitive advantage in its current configuration.

A similar trend is observed in the WCX dataset, where claude-3.5-sonnet excels in Coverage, Contradiction, and E2E Recall. However, the typhoon family of models struggles to match the performance of the closed-source models in this broader Thai legal QA domain. As seen with the Tax dataset, both claude-3.5-sonnet and gemini-1.5-pro-002 exhibit low E2E Precision, leading to lower F1 scores compared to gpt-4o. This underscores a trade-off between recall and precision, particularly for claude-3.5-sonnet and gemini-1.5-pro-002. The causes of this precision drop are further analyzed in a later section.

Table \ref{table: llm_e2e_recall_diff} presents two additional metrics: the difference between retriever recall and E2E recall which reflects the RAG system's ability to utilize retrieved documents for reasoning and answer generation, and the ratio of samples with zero E2E recall but non-zero coverage which indicates the likelihood of generating answers without relying on correctly retrieved documents, potentially due to parametric knowledge or hallucinations.

Claude-3.5-sonnet minimizes the recall difference across both datasets, demonstrating its strong ability to leverage retrieved documents for reasoning. This contributes to its high Coverage and low Contradiction scores. OpenAI achieves the lowest hallucination ratio on the Tax dataset, while Claude and Gemini perform best on the WCX dataset. As for Typhoon models, the 70b variant performs well in terms of the hallucination ratio in Tax dataset. However, both Typhoon models exhibit high recall differences on both datasets which brings their abilities to utilize correctly retrieved documents into question. The recall difference is significantly higher for the Tax dataset than the WCX dataset, suggesting that the Tax dataset poses greater challenges for applying the retrieved information to reasoning and answer generation.



\begin{table}[ht]
\centering
\caption{Recall Difference and Ratio of Hallucination by LLM}
\renewcommand{\arraystretch}{1.5} % This increases the cell height by 1.5 times
\label{table: llm_e2e_recall_diff}
\begin{tabular}{@{}c|ccc@{}}
\toprule
Dataset              & Model                      & Recall Difference & Zero Recall, Non Zero Coverage Ratio (Hallucination Rate) \\ \midrule
\multirow{5}{*}{Tax} & gpt-4o-2024-08-06          & 0.100             & 0.100                                \\
                     & claude-3.5-sonnet-20240620 & 0.095             & 0.160                                \\
                     & gemini-1.5-pro-002         & 0.102             & 0.140                                \\
                     & typhoon-v2-70b-instruct    & 0.148             & 0.120                                \\
                     & typhoon-v2-8b-instruct     & 0.246             & 0.200                                \\ \midrule
\multirow{5}{*}{WCX} & gpt-4o-2024-08-06          & 0.058             & 0.069                                \\
                     & claude-3.5-sonnet-20240620 & 0.036             & 0.060                                \\
                     & gemini-1.5-pro-002         & 0.045             & 0.058                                \\
                     & typhoon-v2-70b-instruct    & 0.076             & 0.079                                \\
                     & typhoon-v2-8b-instruct     & 0.163             & 0.120                                \\ \bottomrule
\end{tabular}
\end{table}

% An analysis of samples with high retriever recall but low Coverage scores reveals a recurring pattern where the LLM often omits intermediate reasoning steps, instead applying the retrieved laws directly without sufficient explanation. For example, in a query regarding the exemption of income from a husband in a foreign country to his wife in Thailand without a registered marriage, the correct legal reasoning involves multiple steps.

% First, the relevant retrieved law—Section 42—states that income received from moral responsibility or as gifts given during ceremonies or customs is exempt from tax. The system should then reason that the money given to the wife could be classified as moral responsibility. Additionally, the system should consider whether evidence of the unregistered marriage exists, as such evidence would validate the claim of moral responsibility, allowing the income to be exempted from tax.

% However, the analysis generated by the LLM bypasses this detailed reasoning process. Instead, the model prematurely concludes that the money is neither from tradition nor moral responsibility, asserting that the income is subject to tax. This response overlooks the possibility that a salary given by a husband to a wife could be considered a moral responsibility, failing to provide the necessary intermediate reasoning steps to support its conclusion.


Analyzing samples with high retriever recall but low coverage reveals a recurring pattern: LLM often omits intermediate reasoning steps, directly applying retrieved laws without sufficient explanation. For example, a query about exempting income sent from a foreign husband to his wife in Thailand (without a registered marriage) requires multi-step reasoning.

First, the relevant law (section 42 of the Revenue Code) exempts income received from moral obligation or gifts given during ceremonies or customs. The system should then reason whether the money sent to the wife qualifies as a moral obligation. Evidence of the unregistered marriage would support this claim and justify the exemption.

However, the LLM often skips this detailed reasoning. It prematurely concludes that the money isn't from tradition or moral obligation and is therefore taxable. This overlooks the possibility of the husband's payment being a moral obligation, failing to provide the necessary intermediate reasoning steps.


\begin{quote}
    \begin{thai}
    \textbf{Question: } 1. นาง ก. ปัจจุบันมีสามีเป็นชาวต่างชาติชื่อ Mr.A แต่มิได้จดทะเบียนสมรส
  2. กรมสรรพากรมีข้อมูล Exchange of Information จากสรรพากรต่างประเทศ (ประเทศ B) กรณีนาง ก. ผู้มีถิ่นที่อยู่ในประเทศไทยได้รับเงินของขวัญ (Gift) ในฐานะ แฟนสาวจาก Mr.A คิดเป็นเงินบาทไทยตามอัตราซื้อถัวเฉลี่ยเงินโอนของธนาคารแห่งประเทศไทย (ธปท.) ดังนี้
    2.1 จำนวน 1 ล้านบาท เมื่อวันที่ 20 มีนาคม 2560...
  4. เงินที่นาง ก. ได้รับในกรณีดังกล่าว เป็นเงินได้ที่ได้รับยกเว้นตามมาตรา 42 (28) แห่ง ประมวลรัษฎากรหรือไม่

    
    \textbf{Relevant Laws: }
    ประมวลรัษฎากร มาตรา 42
    ...
    (28) เงินได้ที่ได้รับจากการอุปการะโดยหน้าที่ธรรมจรรยาหรือจากการให้โดยเสน่หาเนื่องในพิธีหรือตามโอกาสแห่งขนบธรรมเนียมประเพณี ทั้งนี้ จากบุคคลซึ่งมิใช่บุพการี ผู้สืบสันดาน หรือคู่สมรส เฉพาะเงินได้ในส่วนที่ไม่เกินสิบล้านบาทตลอดปีภาษีนั้น
    ...

    \textbf{RAG Analysis: }

    From the given case details, we need to consider the following points:
    
    1. นาง ก. and Mr. A are not legally married, but they are in a relationship.
    
    2. นาง ก. received large sums of money as gifts from Mr. A in 2560, 2561, and 2562.
    
    3. นาง ก. also receives monthly allowances from Mr. A for household expenses.
    
    4. The question is whether these gifts are exempt under Section 42(28) of the Revenue Code.
    
    According to Section 42(28), income received from moral support or gifts given during ceremonies or on occasions according to custom traditions, from persons who are not ancestors, descendants, or spouses, is exempt from income tax, but only for the portion not exceeding 10 million baht throughout the tax year.
    
    In this case:
    1. Mr. A is not legally นาง ก.'s spouse, so he falls under the category of "persons who are not ancestors, descendants, or spouses".
    2. The gifts received in each year (1 million, 800,000, and 700,000 baht) are all below the 10 million baht threshold.
    3. However, these gifts do not seem to be given "during ceremonies or on occasions according to custom traditions". They appear to be regular gifts given in a relationship.

    \end{thai}
\end{quote}

% Another issue observed in WCX queries is that the LLM attempts to incorporate multiple similar retrieved laws into its answer. This arises from the ambiguous nature of the queries and the lack of context surrounding laws with similar content that must be applied under different conditions as specified in other sections of the legal code.

% For instance, in a query asking about the deadline for a taxpayer to pay taxes from the date they receive the tax assessment notification, the correct answer is 7 days, as referenced in Section 18 bis of the Revenue Code. However, due to the vagueness of the query and the absence of specific conditions to identify the most relevant law, the model incorporates both Section 18 ter of the Revenue Code and Section 44 of the Petroleum Income Tax, as both contain similar content but apply under different conditions.

% This issue will persist until the system is capable of engaging in multi-turn conversations to clarify the specific conditions and retrieve the most relevant documents more accurately.


Another issue in the WCX dataset is the LLM's tendency to incorporate multiple similar, retrieved laws into its answer. This stems from query ambiguity and the lack of context surrounding laws with similar content but different applicability conditions.

For example, a query about the tax payment deadline after receiving an assessment notification should retrieve Section 18 bis of the Revenue Code (specifying 7 days). However, due to the query's vagueness, the model might also incorporate Section 18 ter of the Revenue Code and Section 44 of the Petroleum Income Tax, both containing similar content but with different conditions.

This issue will likely persist until the system can engage in multi-turn conversations to clarify specific conditions and refine document retrieval.


\begin{quote}
    \begin{thai}
    \textbf{Question: } ผู้ต้องเสียภาษีต้องชำระภาษีภายในกี่วันตั้งแต่วันที่ได้รับแจ้งประเมินภาษี

    
    \textbf{Relevant Laws: }
    
    ประมวลรัษฎากร มาตรา 18 ทวิ 
    
    ในกรณีจำเป็นเพื่อรักษาผลประโยชน์ในการจัดเก็บภาษีอากร เจ้าพนักงานประเมินมีอำนาจที่จะประเมินเรียกเก็บภาษีจากผู้ต้องเสียภาษีก่อนถึงกำหนดเวลายื่นรายการได้ เมื่อได้ประเมินแล้วให้แจ้งจำนวนภาษีที่ต้องเสียไปยังผู้ต้องเสียและให้ผู้ต้องเสียภาษีชำระภาษีภายในเจ็ดวันนับแต่วันได้รับแจ้งการประเมิน ในกรณีนี้จะอุทธรณ์การประเมินก็ได้
    ภาษีที่ประเมินเรียกเก็บตามความในวรรคก่อน ให้ถือเป็นเครดิตของผู้ต้องเสียภาษีในการคำนวณภาษี
    ในการใช้อำนาจตามความในมาตรานี้ เจ้าพนักงานประเมินจะสั่งให้ยื่นรายการรตามแบบที่อธิบดีกำหนดด้วยก็ได้ 

    \textbf{RAG Analysis: }

    From the given question, we need to determine the timeframe for tax payment after receiving a tax assessment notice. According to the provided laws, there are different timeframes mentioned for various situations:
    
    1. Section 18 ตรี of the Revenue Code states that the taxpayer must pay the tax within 30 days from the date of receiving the assessment notice.
    
    2. Section 44 of the Petroleum Income Tax Act B.E. 2514 also mentions a 30-day period for tax payment after receiving the assessment notice.
    
    3. Section 18 ทวิ of the Revenue Code mentions a special case where the tax officer can assess and collect tax before the due date of filing a tax return. In this case, the taxpayer must pay within 7 days of receiving the assessment notice.
    
    Since the question doesn't specify a particular type of tax or situation, we should provide the general answer based on the most common timeframe mentioned in the laws. 

    \end{thai}
\end{quote}

% Lastly, as seen from the result in Table \ref{table: llm_e2e_main_tax}, the Gemini model performs significantly worse than other models on the E2E precision metric. Through the bar plot showing the amount of samples that LLM cites a certain amount of laws shown in figure \ref{fig: tax_citation_len}, it is observed that Gemini model tends to cite a lot more laws than other models even though its average output tokens is less than Claude model. A more detailed analysis shows that Gemini often cites laws that contain a few relevant keywords but are irrelevant overall. On the other hand, figure \ref{fig: wcx_citation_len} shows that, for the WCX dataset, Claude tends to cite a lot more laws than other models, which might be due to, as mentioned previously, the model trying to incorporate as many laws for every condition possible for vague queries as possible.

Finally, as shown in Table \ref{table: llm_e2e_main_tax}, the Gemini model performs significantly worse than others on E2E precision. Figure \ref{fig: tax_citation_len} reveals that Gemini cites considerably more laws than other models, despite having a shorter average output token length than Claude. Detailed analysis shows that Gemini often cites laws containing a few relevant keywords but lacking overall relevance. Conversely, Figure \ref{fig: wcx_citation_len} shows that Claude cites more laws than other models on average for the WCX dataset, likely due to its tendency to incorporate multiple laws for vaguely worded queries, as discussed previously.

\begin{figure}[ht]
\centering
\includegraphics[width=\textwidth]{images/tax_citation_len.png}
\caption{Plot of distribution of number of citations each model makes on Tax Case dataset}
\label{fig: tax_citation_len}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[width=\textwidth]{images/wcx_citation_len.png}
\caption{Plot of distribution of number of citations each model makes on WCX dataset}
\label{fig: wcx_citation_len}
\end{figure}

% \subsubsection{Main Experiments}
% \label{subsubsec: main_table}
% After determining the optimal configuration for each component of the Retrieval-Augmented Generation (RAG) system, the final experiments were conducted using four distinct settings: 

% 1. \textbf{Parametric:} This setting prompts the best LLM (claude-3.5-sonnet) to answer queries without providing additional context and acts as the lower baseline for the performance of the RAG system.  

% 2. \textbf{Naive RAG:} This approach employs a naive chunking method in conjunction with the best retriever (human-finetuned BGE-M3 model) and the best LLM without the inclusion of a referencer.  

% 3. \textbf{Proposed RAG:} This setting utilizes hierarchical chunking alongside the best retriever and LLM, also without a referencer.  

% 4. \textbf{Golden Context:} This setting provides the RAG system with golden context as annotated in the dataset and acts as the upper baseline for the performance of the RAG system.

% Furthermore,to investigate the capability of long-context LLMs as replacements for RAG systems and assess the reasoning ability of OpenAI's latest model, the Gemini 1.5 Pro model in a long-context setup and the OpenAI's o1-preview model were evaluated in an end-to-end manner.

% For the Gemini 1.5 Pro long-context setup, the entire set of 36 legislation was provided as input to the LLM, with markers delineating the start and end of each section. The model was required to interpret and utilize this context without the assistance of a retriever to answer queries. Due to the cost associated with these methods, the evaluation was restricted to a stratified 20\% subset of the WCX dataset, sampled based on the distribution of legislation, as well as the complete Tax Case dataset.

% The results for the both full and sampled WCX dataset along with the result on the entire tax case dataset are presented in Tables~\ref{table: main_exp_full} and~\ref{table: main_exp_sampled}, respectively.

\subsubsection{Main Experiments}
\label{subsubsec: main_table}
After determining the optimal configuration for each RAG component, we conducted final experiments using four distinct settings:

\begin{enumerate}
    \item \textbf{Parametric}: The best LLM (claude-3.5-sonnet) answers queries without additional context, serving as a lower performance baseline.
    \item \textbf{Naive RAG}: This uses naive chunking, the best retriever (human-fine-tuned BGE-M3), and the best LLM without a referencer.
    \item \textbf{Proposed RAG}: This uses hierarchical chunking, the best retriever, and the best LLM, also without a referencer.
    \item \textbf{Golden Context}: The RAG system receives the dataset's gold-standard context, serving as an upper performance baseline.
\end{enumerate}

To investigate long-context LLMs as RAG replacements and assess OpenAI's latest model's reasoning abilities, we also evaluated Gemini 1.5 Pro (long-context) and OpenAI's o1-preview model end-to-end.

For Gemini 1.5 Pro, all 35 legislation were provided as input, with markers delimiting each section. The model had to interpret and use this context without a retriever. Due to cost, this evaluation was limited to a stratified 20\% sample of the WCX dataset (the same subset used in experiment with long-context LLM as a retriever) and the full Tax Case dataset.

Results for the full and sampled WCX dataset, along with the Tax Case dataset, are presented in Tables \ref{table: main_exp_full} and \ref{table: main_exp_sampled}, respectively.




\begin{table}[ht]
\centering
\caption{Experiment results on full WCX dataset and full Tax Case dataset}
\renewcommand{\arraystretch}{1.5} % This increases the cell height by 1.5 times
\label{table: main_exp_full}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}cccccccccc@{}}
\toprule
Dataset                                   & Setting        & Retriever MRR & Retriever Multi-MRR & Retriever Recall & Coverage & Contradiction & E2E Recall & E2E Precision & E2E F1 \\ \midrule
\multicolumn{1}{c|}{\multirow{4}{*}{Tax}} & Parametric     & -             & -                   & -                & 46.0     & 0.480         & 0.458      & 0.629         & 0.530  \\
\multicolumn{1}{c|}{}                     & Naïve RAG      & 0.120         & 0.048               & 0.062            & 50.0     & 0.460         & 0.306      & 0.463         & 0.368  \\
\multicolumn{1}{c|}{}                     & Our RAG        & 0.574         & 0.333               & 0.499            & 51.0     & 0.440         & 0.389      & 0.554         & 0.457  \\
\multicolumn{1}{c|}{}                     & Golden Context & 1.0           & 1.0                 & 1.0              & 52.0     & 0.460         & 0.694      & 1.000         & 0.820  \\ \midrule
\multicolumn{1}{c|}{\multirow{4}{*}{WCX}} & Parametric     & -             & -                   & -                & 60.3     & 0.199         & 0.188      & 0.141         & 0.161  \\
\multicolumn{1}{c|}{}                     & Naïve RAG      & 0.120         & 0.048               & 0.062            & 77.3     & 0.097         & 0.745      & 0.370         & 0.495  \\
\multicolumn{1}{c|}{}                     & Our RAG        & 0.809         & 0.809               & 0.938            & 89.7     & 0.040         & 0.901      & 0.444         & 0.595  \\
\multicolumn{1}{c|}{}                     & Golden Context & 1.0           & 1.0                 & 1.0              & 93.4     & 0.034         & 0.999      & 1.000         & 1.000  \\ \bottomrule
\end{tabular}
}
\end{table}

\begin{table}[ht]
\centering
\caption{Experiment results on sampled WCX dataset and full Tax Case dataset}
\renewcommand{\arraystretch}{1.5} % This increases the cell height by 1.5 times
\label{table: main_exp_sampled}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}c|ccccccccc@{}}
\toprule
Dataset              & Setting                                                       & Retriever MRR & Retriever Multi-MRR & Retriever Recall & Coverage & Contradiction & E2E Recall & E2E Precision & E2E F1 \\ \midrule
\multirow{6}{*}{Tax} & Parametric                                                    & -             & -                   & -                & 46.0     & 0.480         & 0.458      & 0.629         & 0.530  \\
                     & Naïve RAG                                                     & 0.120         & 0.048               & 0.062            & 50.0     & 0.460         & 0.306      & 0.463         & 0.368  \\
                     & Our RAG                                                       & 0.574         & 0.333               & 0.499            & 51.0     & 0.440         & 0.389      & 0.554         & 0.457  \\
                     & Golden Context                                                & 1.0           & 1.0                 & 1.0              & 52.0     & 0.460         & 0.694      & 1.000         & 0.820  \\
                     & \begin{tabular}[c]{@{}c@{}}LCLM\\(Gemini)\end{tabular} & -             & -                   & -                & 36.0     & 0.620         & 0.410      & 0.484         & 0.444  \\
                     & \begin{tabular}[c]{@{}c@{}}O1\\ (Zero Shot)\end{tabular}      & 0.574         & 0.333               & 0.499            & 43.0     & 0.460         & 0.333      & 0.640         & 0.438  \\ \midrule
\multirow{6}{*}{WCX} & Parametric                                                    & -             & -                   & -                & 60.6     & 0.198         & 0.197      & 0.147         & 0.169  \\
                     & Naïve RAG                                                     & 0.549         & 0.549               & 0.649            & 77.7     & 0.092         & 0.740      & 0.379         & 0.501  \\
                     & Our RAG                                                       & 0.825         & 0.825               & 0.945            & 90.1     & 0.028         & 0.920      & 0.453         & 0.607  \\
                     & Golden Context                                                & 1.0           & 1.0                 & 1.0              & 94.2     & 0.025         & 0.999      & 1.0           & 0.999  \\
                     & \begin{tabular}[c]{@{}c@{}}LCLM\\ (Gemini)\end{tabular}       & -             & -                   & -                & 83.2     & 0.063         & 0.765      & 0.514         & 0.615  \\
                     & \begin{tabular}[c]{@{}c@{}}O1\\ (Zero-shot)\end{tabular}      & 0.825         & 0.825               & 0.945            & 83.3     & 0.044         & 0.886      & 0.466         & 0.611  \\ \bottomrule
\end{tabular}
}
\end{table}

% On Tax Case dataset, there is not much difference between the 4 main settings apart from the parametric setting achieving slightly lower coverage score and higher contradiction score than others. Two notable observations can be made about this result. 

% First, it is observed that the parametric setting achieves the second-highest end-to-end (E2E) recall and precision, despite the absence of a retriever and the lack of additional context beyond the queries. Detailed analysis reveals that 58 out of 105 sections cited by the parametric setting are not retrieved by even the best retriever in this study, with 26 of these 58 sections being relevant. In contrast, only 5 out of 101 sections cited by the proposed RAG system are not retrieved by the retriever. 

% These findings indicate that RAG systems are significantly constrained by the performance of the retriever, particularly in cases involving complex queries such as those in the Tax Case dataset. The generator in RAG systems appears discouraged from leveraging its internal knowledge, which, in certain instances, might provide more informative responses. 

% Moreover, a considerable disparity is observed between the retriever recall and the E2E recall, indicating that the LLM often fails to utilize relevant retrieved sections effectively. This issue is particularly pronounced in the case discussed in Section \ref{subsubsec:re_result_discussion}, where the model encounters difficulties in retrieving sections that predominantly consist of terminologies. Even in instances where such sections are successfully retrieved, the LLM frequently struggles to incorporate the retrieved content into its reasoning process.

% Next, from the table \ref{table: main_exp_full} in Tax Case dataset, there is no clear relationship between E2E citation scores and the coverage and contradiction score. This suggests that the LLM still struggles to apply the cited sections correctly in its reasoning chain leading to either incorrect response or correct response resulting from erroneous reasoning. This problem is similar to the one discussed in section \ref{subsubsec: augmenter_e2e_result} where it is shown that the improve in retriever recall from referencer does not lead to increase in E2E metrics. The difference is that, in this experiment, it can be observed that increase in E2E citations performance on Tax Case dataset does not necessarily increase the coverage and contradiction score when the system is implemented with Claude 3.5 Sonnet.

% As for result on WCX dataset in table \ref{table: main_exp_full}, the results are as expected with the parametric setting performing poorest followed by Naive RAG and Proposed RAG and, lastly, Golden context setting which performs the best.

% Lastly, the performances of long-context LLM (LCLM) and OpenAI's o1 model are reported in the table \ref{table: main_exp_sampled} with both model performing comparable to parametric setting in Tax Case dataset and naive RAG in WCX dataset. This might be due to the performance degradation with long context (1.2 million tokens) for LCLM and that the legal reasoning is a specific type of reasoning that requires different thought process as the one o1 model is capable of.


On the Tax Case dataset, the four main settings perform similarly, except for the parametric setting's slightly lower coverage and higher contradiction. Two key observations emerge:

First, the parametric setting achieves the second-highest E2E recall and precision despite lacking a retriever and additional context. Analysis reveals that 58 of 105 sections cited by the parametric setting aren't retrieved by even the best retriever, with 26 of those being relevant. In contrast, only 5 of 101 sections cited by the proposed RAG system are unretrieved. This indicates that retriever performance significantly constrains RAG systems, especially with complex queries like those in the Tax Case dataset. RAG system generators seem discouraged from using internal knowledge, which might sometimes provide better answers. Furthermore, the substantial disparity between retriever and E2E recall shows that the LLM often underutilizes relevant retrieved sections, particularly those containing primarily terminology (as discussed in section \ref{subsubsec:re_result_discussion}).

Second, Table \ref{table: main_exp_full} (Tax Case dataset) shows no clear relationship between E2E citation scores and coverage/contradiction. This suggests the LLM struggles to apply cited sections correctly in its reasoning, leading to incorrect or erroneously reasoned answers. This resembles the issue in Section \ref{subsubsec: augmenter_e2e_result}, where improved retriever recall from the referencer didn't consistently improve E2E metrics. Here, increased E2E citations with Claude 3.5 Sonnet don't necessarily improve coverage or contradiction.

For the WCX dataset (Table \ref{table: main_exp_full}), the results are as expected: parametric performs worst, followed by Naive RAG, then Proposed RAG, and finally Golden Context (best).

Finally, Table \ref{table: main_exp_sampled} shows the performance of the long-context LLM (LCLM) and OpenAI's o1 model. Both perform comparably to the parametric setting on the Tax Case dataset and Naive RAG on the WCX dataset. This might be due to performance degradation with long contexts (1.2m tokens) for the LCLM and the specialized nature of legal reasoning, which might differ from o1's capabilities.







