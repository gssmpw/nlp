@misc{Ajmi2024,
  author = {Ajmi, Ayyoub},
  title = {Revolutionizing Access to Justice: The Role of {AI}-Powered Chatbots and Retrieval-Augmented Generation in Legal Self-Help},
  year = {2024},
  url = {https://irlaw.umkc.edu/faculty_works/949},
}

@misc{CBR-RAG,
      title={CBR-RAG: Case-Based Reasoning for Retrieval Augmented Generation in LLMs for Legal Question Answering}, 
      author={Nirmalie Wiratunga and Ramitha Abeyratne and Lasal Jayawardena and Kyle Martin and Stewart Massie and Ikechukwu Nkisi-Orji and Ruvan Weerasinghe and Anne Liret and Bruno Fleisch},
      year={2024},
      eprint={2404.04302},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2404.04302}, 
}

@article{Dahl_2024,
   title={Large Legal Fictions: Profiling Legal Hallucinations in Large Language Models},
   volume={16},
   ISSN={1946-5319},
   url={http://dx.doi.org/10.1093/jla/laae003},
   DOI={10.1093/jla/laae003},
   number={1},
   journal={Journal of Legal Analysis},
   publisher={Oxford University Press (OUP)},
   author={Dahl, Matthew and Magesh, Varun and Suzgun, Mirac and Ho, Daniel E},
   year={2024},
   month=jan, pages={64–93} }

@misc{anglebert,
      title={AnglE-optimized Text Embeddings}, 
      author={Xianming Li and Jing Li},
      year={2024},
      eprint={2309.12871},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2309.12871}, 
}

@inproceedings{bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    editor = "Burstein, Jill  and
      Doran, Christy  and
      Solorio, Thamar",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}

@book{chuathai2023introduction,
  author    = {Somyot Chuathai},
  title     = {Introduction to Law},
  edition   = {30th},
  publisher = {Winyuchon},
  year      = {2023},
  pages     = {124-126}
}

@misc{claude3.5sonnet,
  title = {Claude 3.5 Sonnet Model Card Addendum},
  author = {Anthropic},
  year = {2024},
  url = {https://www.anthropic.com/news/claude-3-5-sonnet},
}

@misc{clerc,
      title={CLERC: A Dataset for Legal Case Retrieval and Retrieval-Augmented Analysis Generation}, 
      author={Abe Bohan Hou and Orion Weller and Guanghui Qin and Eugene Yang and Dawn Lawrie and Nils Holzenberger and Andrew Blair-Stanek and Benjamin Van Durme},
      year={2024},
      eprint={2406.17186},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.17186}, 
}

@misc{es2023ragasautomatedevaluationretrieval,
      title={RAGAS: Automated Evaluation of Retrieval Augmented Generation}, 
      author={Shahul Es and Jithin James and Luis Espinosa-Anke and Steven Schockaert},
      year={2023},
      eprint={2309.15217},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2309.15217}, 
}

@misc{gemini1.5,
      title={Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context}, 
      author={Machel Reid and Nikolay Savinov and Denis Teplyashin and Dmitry Lepikhin and Timothy Lillicrap and Jean-baptiste Alayrac and Radu Soricut and Angeliki Lazaridou and Orhan Firat and Julian Schrittwieser and et al.},
      year={2024},
      eprint={2403.05530},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2403.05530}, 
}

@misc{gpt4o,
      title={GPT-4o System Card}, 
      author={Aaron Hurst and Adam Lerer and Adam P Goucher and Adam Perelman and Aditya Ramesh and Aidan Clark and AJ Ostrow and Akila Welihinda and Alan Hayes and Alec Radford and et al.},
      year={2024},
      eprint={2410.21276},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.21276}, 
}

@misc{laban2024summaryhaystackchallengelongcontext,
      title={Summary of a Haystack: A Challenge to Long-Context LLMs and RAG Systems}, 
      author={Philippe Laban and Alexander R. Fabbri and Caiming Xiong and Chien-Sheng Wu},
      year={2024},
      eprint={2407.01370},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2407.01370}, 
}

@misc{legalbench,
      title={LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models}, 
      author={Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher Ré and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},
      year={2023},
      eprint={2308.11462},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2308.11462}, 
}

@misc{legalbert,
      title={LEGAL-BERT: The Muppets straight out of Law School}, 
      author={Ilias Chalkidis and Manos Fergadiotis and Prodromos Malakasiotis and Nikolaos Aletras and Ion Androutsopoulos},
      year={2020},
      eprint={2010.02559},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2010.02559}, 
}

@misc{leval,
      title={L-Eval: Instituting Standardized Evaluation for Long Context Language Models}, 
      author={Chenxin An and Shansan Gong and Ming Zhong and Xingjian Zhao and Mukai Li and Jun Zhang and Lingpeng Kong and Xipeng Qiu},
      year={2023},
      eprint={2307.11088},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2307.11088}, 
}

@inproceedings{lexglue,
    title = "{L}ex{GLUE}: A Benchmark Dataset for Legal Language Understanding in {E}nglish",
    author = "Chalkidis, Ilias  and
      Jana, Abhik  and
      Hartung, Dirk  and
      Bommarito, Michael  and
      Androutsopoulos, Ion  and
      Katz, Daniel  and
      Aletras, Nikolaos",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.297",
    doi = "10.18653/v1/2022.acl-long.297",
    pages = "4310--4330",
    abstract = "Laws and their interpretations, legal arguments and agreements are typically expressed in writing, leading to the production of vast corpora of legal text. Their analysis, which is at the center of legal practice, becomes increasingly elaborate as these collections grow in size. Natural language understanding (NLU) technologies can be a valuable tool to support legal practitioners in these endeavors. Their usefulness, however, largely depends on whether current state-of-the-art models can generalize across various tasks in the legal domain. To answer this currently open question, we introduce the Legal General Language Understanding Evaluation (LexGLUE) benchmark, a collection of datasets for evaluating model performance across a diverse set of legal NLU tasks in a standardized way. We also provide an evaluation and analysis of several generic and legal-oriented models demonstrating that the latter consistently offer performance improvements across multiple tasks.",
}

@misc{lexisnexis,
	author = {LexisNexis},
	title = {{L}exis{N}exis {L}aunches {L}exis+ {A}{I}, a {G}enerative {A}{I} {S}olution with {H}allucination-{F}ree {L}inked {L}egal {C}itations},
	howpublished = {lexisnexis.com},
        url = {https://www.lexisnexis.com/community/pressroom/b/news/posts/lexisnexis-launches-lexis-ai-a-generative-ai-solution-with-hallucination-free-linked-legal-citations},
	year = {2023},
	note = {[Accessed 13-08-2024]},
}

@misc{loft,
      title={Can Long-Context Language Models Subsume Retrieval, RAG, SQL, and More?}, 
      author={Jinhyuk Lee and Anthony Chen and Zhuyun Dai and Dheeru Dua and Devendra Singh Sachan and Michael Boratko and Yi Luan and Sébastien M. R. Arnold and Vincent Perot and Siddharth Dalmia and Hexiang Hu and Xudong Lin and Panupong Pasupat and Aida Amini and Jeremy R. Cole and Sebastian Riedel and Iftekhar Naim and Ming-Wei Chang and Kelvin Guu},
      year={2024},
      eprint={2406.13121},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.13121}, 
}

@inproceedings{longbench,
    title = "{L}ong{B}ench: A Bilingual, Multitask Benchmark for Long Context Understanding",
    author = "Bai, Yushi  and
      Lv, Xin  and
      Zhang, Jiajie  and
      Lyu, Hongchang  and
      Tang, Jiankai  and
      Huang, Zhidian  and
      Du, Zhengxiao  and
      Liu, Xiao  and
      Zeng, Aohan  and
      Hou, Lei  and
      Dong, Yuxiao  and
      Tang, Jie  and
      Li, Juanzi",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.172",
    pages = "3119--3137",
    abstract = "Although large language models (LLMs) demonstrate impressive performance for many language tasks, most of them can only handle texts a few thousand tokens long, limiting their applications on longer sequence inputs, such as books, reports, and codebases. Recent works have proposed methods to improve LLMs{'} long context capabilities by extending context windows and more sophisticated memory mechanisms. However, comprehensive benchmarks tailored for evaluating long context understanding are lacking. In this paper, we introduce LongBench, the first bilingual, multi-task benchmark for long context understanding, enabling a more rigorous evaluation of long context understanding. LongBench comprises 21 datasets across 6 task categories in both English and Chinese, with an average length of 6,711 words (English) and 13,386 characters (Chinese). These tasks cover key long-text application areas including single-doc QA, multi-doc QA, summarization, few-shot learning, synthetic tasks, and code completion. All datasets in LongBench are standardized into a unified format, allowing for effortless automatic evaluation of LLMs. Upon comprehensive evaluation of 8 LLMs on LongBench, we find that: (1) Commercial model (GPT-3.5-Turbo-16k) outperforms other open-sourced models, but still struggles on longer contexts. (2) Scaled position embedding and fine-tuning on longer sequences lead to substantial improvement on long context understanding. (3) Context compression technique such as retrieval brings improvement for model with weak ability on long contexts, but the performance still lags behind models that have strong long context understanding capability.",
}

@misc{magesh2024hallucinationfreeassessingreliabilityleading,
      title={Hallucination-Free? Assessing the Reliability of Leading AI Legal Research Tools}, 
      author={Varun Magesh and Faiz Surani and Matthew Dahl and Mirac Suzgun and Christopher D. Manning and Daniel E. Ho},
      year={2024},
      eprint={2405.20362},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2405.20362}, 
}

@misc{needleinahaystack,
	author = {Gregory Kamradt},
	title = {Needle in a Haystack},
	howpublished = {github.com},
        url={https://github.com/gkamradt/LLMTest_NeedleInAHaystack},
	year = {2023}
}

@misc{nepaquad,
      title={RAG vs. Long Context: Examining Frontier Large Language Models for Environmental Review Document Comprehension}, 
      author={Hung Phan and Anurag Acharya and Sarthak Chaturvedi and Shivam Sharma and Mike Parker and Dan Nally and Ali Jannesari and Karl Pazdernik and Mahantesh Halappanavar and Sai Munikoti and Sameera Horawalavithana},
      year={2024},
      eprint={2407.07321},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2407.07321}, 
}

@misc{originalRAG,
      title={Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks}, 
      author={Patrick Lewis and Ethan Perez and Aleksandra Piktus and Fabio Petroni and Vladimir Karpukhin and Naman Goyal and Heinrich Küttler and Mike Lewis and Wen-tau Yih and Tim Rocktäschel and Sebastian Riedel and Douwe Kiela},
      year={2021},
      eprint={2005.11401},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2005.11401}, 
}

@misc{selfroute,
      title={Retrieval Augmented Generation or Long-Context LLMs? A Comprehensive Study and Hybrid Approach}, 
      author={Zhuowan Li and Cheng Li and Mingyang Zhang and Qiaozhu Mei and Michael Bendersky},
      year={2024},
      eprint={2407.16833},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2407.16833}, 
}

@misc{thanoy,
	author = {Kobkrit Viriyayudhakorn},
	title = {Thanoy {A}{I} {C}hatbot - Genius {A}{I} Lawyer},
	howpublished = {iapp.co.th},
        url = {https://iapp.co.th/thanoy},
	year = {2024},
	note = {[Accessed 13-08-2024]},
}

@misc{thomsonreutersAIpoweredLegal,
	author = {Sarah Strumberger},
	title = {{A}{I}-powered legal research: {W}here legal research meets generative {A}{I}},
	howpublished = {legal.thomsonreuters.com},
        url = {https://legal.thomsonreuters.com/blog/legal-research-meets-generative-ai/},
	year = {2023},
	note = {[Accessed 13-08-2024]},
}

