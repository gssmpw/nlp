\section{Experimental Setups}
\label{sec:evaluation}
% experiment setting @Non
% What I need to write as intro -> We use the designed benchmark dataset and evaluation process to answer three research questions: RQ1: traditional components, RQ2: custom components, RQ3: LCLM vs RAG -> Dive into each -> RQ1: Start with retriever evaluation (all models we used) to find the best retriever -> Fix retriever and change LLM (find best LLM) -> present main table!! (Describe what setting we use). RQ2: Start with defining what we want to try: chunking -> How we chunk -> How we evaluate each chunks -> Each chunk affect retriever and E2E, referencer -> how we implement it -> what's the experiment to measure its effectiveness. RQ3: Start with models we used (O1 & Gemini) -> Sample -> See effect. Swap RQ2 and RQ1 should be more consistent since we want to show main table after traditional components !!
In this section, we outline the experimental setup used to leverage our benchmark dataset and the proposed evaluation process to address the aforementioned three key research questions:
%
\begin{description}
\item [\textbf{(RQ1)}:] What impact do components tailored to the Thai legal system structure have on a RAG-based Thai legal QA system?

\item [\textbf{(RQ2)}:] How do the choices of retriever and LLM affect performance in a RAG-based Thai legal QA system?

\item [\textbf{(RQ3)}:] How does a Long-Context LLM-based Thai legal QA system perform compared to RAG-based systems?
\end{description}

For each research question, we describe the purpose of the corresponding experiment, its design, and the models or components selected for evaluation. 
%
Note that for every E2E evaluation of RAG-based systems, we opt to use the top 10 retrieved documents as context for the generator, and the generator is provided with a few shot examples for the task. 
%
The few-shot examples are ensured to be from the entries not in the final test set for both the NitiBench-CCL (few-shot examples taken from WangchanX-Legal-ThaiCCL's train split) and NitiBench-Tax datasets. We use 3-shot examples in our experiments randomly sampled from our training data.

\subsection{(RQ1) Impact of tailored components}
\label{subsec: setup_rq1}
% What do I put in this section: Describe two main problems -> Describe chunking - how to measure -> Describe referencer - how to measure
As outlined in \S\ref{subsec: thai_legal}, implementing LLM-based applications in the Thai legal domain presents two key challenges: 
%
(1) Thai legislation is organized hierarchically, requiring domain expertise to segment these documents into coherent chunks suitable for a RAG framework and 
%
(2) Thai legislation frequently includes inter-section references, necessitating the inclusion of nested legal contexts in addition to the standard retrieved context within the RAG framework. 
%
In this research question, we aim to quantitatively display the effect of components crafted to address these problems on the performance of the RAG-based Thai legal QA system.

\subsubsection{Hierarchy-aware Chunking}
\label{subsubsec: chunk_setup}

To address the limitations of traditional chunking methods, which are largely structure-agnostic (e.g. split documents into multiple chunks by sliding window algorithm) leading to information loss in RAG systems, we introduce \textbf{Hierarchy-aware Chunking}. 
%
This approach leverages the multi-tiered structure inherent in Thai legislation. 
%
Rather than relying on arbitrary character counts or generic separators, it utilizes domain knowledge to segment the legislation so that each chunk contains a single complete section of the law. 
%
This ensures that each chunk comprehensively represents a self-contained legal concept or rule, preserving the integrity of the original document.
%
The hierarchy-aware chunking was done via exhaustive regular expression and rule-based.

To evaluate the effect of the proposed component, we first quantify the baseline of the naive chunking strategy.
%
Since the naive chunking method requires a selection of different approaches and parameters, we seek to find the combination that minimized the \textit{information loss} compared to our proposed hierarchy-aware chunking and use the best combination as the naive chunking baseline.
%
Specifically, we assess information loss among three common naive chunking approaches varying its parameters, such as chunk sizes and overlaps. 
%
%The goal is to identify the best-performing naive chunking strategy according to custom-defined metrics and to measure the performance gap between this strategy and the ideal section-based chunking approach. 
%
The chunking strategies considered are as follows:

\begin{enumerate}
    \item \textbf{Character Chunking: } Chunking is based purely on a fixed number of characters.
    \item \textbf{Recursive Chunking: } Chunking using various document structure-related separators.
    \item \textbf{Line Chunking: } Chunking based solely on newline characters.
\end{enumerate}

The chunk sizes in this experiment range from 212 characters (the 25th percentile of section lengths) to 553 characters (the 75th percentile), including intermediate values and one size beyond the 75th percentile. 
%
For chunk overlaps, the values 50, 100, 150, and 200 are tested. 

For each chunking strategy, we first chunk the legal documents and generate metadata indicating the sections each chunk covers and whether they are fully covered. 
%
Additionally, since the naive chunking strategy has no awareness of law section boundaries, the chunked text might either contain multiple law sections (if the section is shorter than the chunk size) or be incomplete (if the section is longer than the chunk size). 
%
This makes it hard to justify whether a retrieved incomplete chunk (partially containing law section content) is considered a correctly retrieved document. 
%
Therefore, to simply retrieve and enable a fair comparison of top-$k$ retrieval across strategies, chunks that do not fully cover at least one section are discarded based on the chunk metadata.
%
Next, the chunk's metadata are then used to measure the information loss of each naive chunking strategy.
%
We define \textit{information loss} of each naive chunking strategy using the following metrics:
%To simplify retrieval and enable a fair comparison of top-$k$ retrieval across strategies, chunks that do not fully cover at least one section are discarded. 
%
%The following metrics are then computed for each strategy:

\begin{enumerate}
    \item \textbf{Sections/Chunk: } The average number of sections (fully or partially covered) in each chunk. For hierarchy-aware chunking, this metric equals 1.0.
    \item \textbf{Chunks/Section: } The average number of chunks that fully or partially cover any given section. For hierarchy-aware chunking, this metric equals 1.0.
    \item \textbf{Fail Chunk Ratio: } The ratio of chunks that do not fully cover any section compared to all chunks. For hierarchy-aware chunking, this metric equals 0.
    \item \textbf{Fail Section Ratio: } The ratio of sections not fully covered by any single chunk, compared to all sections. For hierarchy-aware chunking, this metric equals 0.
    \item \textbf{Uncovered Section Ratio: } The ratio of sections neither fully nor partially covered by any single chunk, compared to all sections. For hierarchy-aware chunking, this metric equals 0.
\end{enumerate}

The naive chunking strategy that yields the best performance based on these metrics is then selected as the optimal naive chunking strategy and represents our naive chunking baseline.
%
After selecting the optimal naive chunking strategy based on the above metrics, we integrate it into a RAG system and compare it against the hierarchy-aware chunking approach. 
%
To ensure a fair comparison, we remove sections from the hierarchy-aware chunks that are not covered by the naive chunking strategy due to content length constraints.
%
Additionally, the dataset is augmented to include only relevant laws within the sections available in the naive chunking strategy.  

After filtering incomplete chunks, only 19 NitiBench-Tax entries and 2,625 NitiBench-CCL entries were left.
%
Given the limited size of the NitiBench-Tax subset, we perform evaluations solely on the NitiBench-CCL. 
%
Both retrieval and end-to-end (E2E) performance are assessed to comprehensively analyze the impact of the proposed component.

For retrieval evaluation, retrieved chunks from the RAG system using naive chunking are mapped to sections before computing retrieval metrics. 
%
The mapping process only associates chunks with sections that are fully contained within them. 
%
The retriever in this setup is a three-headed, human-finetuned BGE-M3 model, as described in \S\ref{subsubsec: retriever_setup}, while the LLM used is \texttt{gpt-4o-2024-08-06} with few-shot examples, as detailed in \S\ref{subsubsec: llm_setup}.

We note that this E2E experiment does not fully capture the advantages of hierarchy-aware chunking over naive chunking in real-world scenarios. 
%
In practical applications, the naive chunking strategy often retains chunks that do not fully cover any section, raising questions about the LLM's and retriever's ability to process and utilize fragmented sections effectively.
%
However, we structured the experiment this way to enable a more direct comparison between chunking strategies under the same top-$k$ setting.

\subsubsection{NitiLink}
\label{subsubsec: referencer_setup}

Another component we propose to address the issue of inter-references between sections in Thai legislation is \textbf{NitiLink}. 
%
This component is designed to recursively fetch additional sections referenced within the initially retrieved sections in a depth-first manner. 
%
For example, understanding Section 291 of the Securities and Exchange Act B.E. 2535 requires knowledge of Section 186 from the same legislation. 
%
Using the proposed NitiLink, the system retrieves Section 186, identified as a child of Section 291, along with any other children within Section 186 and deeper levels, as additional context for the generator in the RAG system.

\begin{quote}
    \textit{Section 291 of Securities and Exchange Act B.E. 2535: Anyone who violates or fails to comply with orders issued under Section 186(2) shall be liable to a penalty of imprisonment for not more than one year, a fine of not more than 300,000 baht, or both.}
\end{quote}

We emphasize that NitiLink is implemented in a depth-first manner. 
%
For example, if the initially retrieved sections are ranked as [Section A with reference to Section U and Section W, Section B with reference to Section X and Section U, Section C], NitiLink fetches the referenced sections, re-ranking the results as [Section A, Section U, Section W, Section B, Section X, Section C]. 
%
While this can degrade MRR if the referenced section is not a hit, we argue that referenced sections should rank higher than originally lower-ranked sections due to their direct relevance to the higher-ranked section, thus providing a more complete and continuous context for answering the query, especially in legal tasks requiring precise reasoning.

NitiLink allows for configuring a maximum depth to limit the extent of nested sections retrieved.
%
In this study, we set the maximum depth to 1 due to computational feasibility and budget constraints since more depth means a longer context is required. 
%
More studies on the impact of referencing depth are conducted in \S\ref{subsec: ref_depth_vs_retrieval_perf}.

To assess the impact of the NitiLink, we utilized the benchmark dataset and evaluation process to compare the performance of a RAG-based system with NitiLink (configured with a maximum depth of 1) against a system without this component. 
%
The evaluation was conducted for both retrieval and end-to-end (E2E) performance. 
%
When calculating the system with NitiLink, we combine both the referenced sections and the retrieved section, as this would allow us to analyze the impact of NitiLink directly on enhancing correctness. 
%
In other words, we attempt to see if incorporating the referenced sections would result in a more accurate retrieval.

For this experiment, we used hierarchy-aware chunking with three-headed Human-Finetuned BGE-M3 (described in \S\ref{subsubsec: referencer_setup}) as the retriever and \texttt{gpt-4o-2024-08-06} as the LLM with few-shot examples in the RAG system configuration.

\subsection{(RQ2) Impact of Retriever and LLM}
\label{subsec: setup_rq2}
% What do I put in this section?: Why is this RQ important? -> when implementing RAG there are lots of choices for these components -> we aim to quantify the performance difference of each choice and the current gap of industry-standard models -> Retriever - explain each model we use and cite -> LLM - explain each model we use and cite -> Furthermore, we also evaluate these systems in comparison to baseline like parametric, naive RAG and RAG with golden context
When implementing a RAG-based legal QA system, numerous design choices must be made for its components, with two of the most critical choices being the selection of the \textit{retriever model} and the \textit{LLM}. 
%
In this research question, we aim to analyze the performance gap and conduct a comparative evaluation of various retrievers and LLMs, focusing on both retrieval and E2E performance.

To answer this research question step-by-step, we decompose the experiment into three steps.
%
First, in \S\ref{subsubsec: retriever_setup}, we address the question \textit{How well do publicly and commercially available embedding models perform in Thai legal QA, and what are the limitations?}. 
%
Then, we seek to answer another question \textit{What is the performance of the available LLM suitable to answer Thai legal QA?} in \S\ref{subsubsec: llm_setup}.
%
Finally, we assess the effectiveness of RAG systems enhanced by the best setups from prior experiments against several baselines, including parametric knowledge QA, naive RAG, and RAG with golden context. 
%
This comprehensive evaluation provides insights into how current RAG systems perform relative to their theoretical lower and upper performance bound and is covered in \S\ref{subsubsec: e2e_best_setup}.

\subsubsection{Retriever}
\label{subsubsec: retriever_setup}
To quantify the performance gaps of current retrievers in the Thai Legal QA domain, we employ several widely-used retrieval models ranging from simple algorithms to ColBERT-based, hybrid-based, decoder-based, and closed-source retrievers within a RAG system. 
%
These models are evaluated on top-$k$ retrieval tasks using our benchmark dataset and the custom-designed multi-label retrieval metrics.

Specifically, the retriever models employed in these experiments are as follows:
\begin{enumerate}
    \item \textbf{BM25} \cite{bm25}: A retrieval algorithm that ranks a set of documents based on term frequency and inverse document frequency. 
    %
    It does not consider semantic similarity and serves as a baseline in our experiments.
    %
    \item \textbf{JinaAI Colbert V2\footnote{\url{https://huggingface.co/jinaai/jina-colbert-v2}}} \cite{jina-v2}: A ColBERT-based \cite{colbert} retrieval model that employs token-level embeddings and late interaction mechanisms tailored for information retrieval tasks.
    %
    \item \textbf{JinaAI Embeddings V3\footnote{\url{https://huggingface.co/jinaai/jina-embeddings-v3}}} \cite{jina-v3}: A multilingual, multi-task text embedding model designed for tasks such as retrieval, clustering, classification, and text matching. Its architecture is based on an adapted version of XLM-Roberta, producing dense embeddings.
    %
    \item \textbf{NV-Embed V1\footnote{\url{https://huggingface.co/nvidia/NV-Embed-v1}}} \cite{nvembed}: A decoder-based embedding model trained with an LLM as its base. 
    %
    It incorporates novel techniques such as latent vector attention and two-stage instruction tuning, enhancing performance on both retrieval and non-retrieval tasks.
    %
    \item \textbf{BGE-M3\footnote{\url{https://huggingface.co/BAAI/bge-m3}}} \cite{bge-m3}: A multilingual embedding model capable of dense, multi-vector, and sparse retrieval. 
    %
    For this work, we weight the scores obtained from three embedding types to enhance retrieval accuracy.
    %
    \item \textbf{Human-Finetuned BGE-M3\footnote{\url{https://huggingface.co/airesearch/WangchanX-Legal-ThaiCCL-Retriever}}}: A version of BGE-M3 finetuned on the train split of the WangchanX-Legal-ThaiCCL dataset (see \S\ref{subsubsec: wcx_dataset}). 
    %
    Positive contexts are derived from human reranking of initially retrieved contexts from BGE-M3.
    %
    \item \textbf{Auto-Finetuned BGE-M3}: Another version of BGE-M3 finetuned on the train split of the WangchanX-Legal-ThaiCCL dataset (see \S\ref{subsubsec: wcx_dataset}). 
    %
    Unlike the human-reranked version, positive contexts are determined using BGE-M3 Reranker V2 \cite{li2023making}\cite{bge-m3}, which computes relevance scores between queries and retrieved contexts. 
    %
    Contexts with scores below a threshold of 0.8 are excluded.
    %
    \item \textbf{Cohere Embeddings\footnote{\url{https://cohere.com/blog/introducing-embed-v3}}: } A commercialized text embedding model designed for tasks like retrieval, classification, and clustering. 
    %
    We use the \texttt{embed-multilingual-v3.0} variant in our experiments.
\end{enumerate}

Notably, for all BGE-M3-based models, we use similarity scores from all three heads of the model with weightings of 0.4, 0.4, and 0.2 for dense embeddings, multi-vector, and sparse embeddings, respectively. 

\subsubsection{LLM}
\label{subsubsec: llm_setup}
Next, assuming the effect of LLM choices in a RAG-based system is independent of the retriever choices, we assess the impact of different LLMs as generators on the end-to-end (E2E) performance of a RAG-based Thai legal QA system. 
%
We fix other design choices, such as the chunking strategy and retriever model, while varying only the LLM selection and inclusion of NitiLink, as experiments in \S\ref{subsec: setup_rq1} showed no clear effect.
%
Specifically, we implement RAG systems with \textit{hierarchy-aware} chunking and the three-headed \textit{Human-Finetuned BGE-M3} retriever, using the following LLM choices:

\begin{enumerate}
    \item \textbf{GPT 4o}\cite{gpt4o}: OpenAI's autoregressive model, trained in an end-to-end fashion and capable of processing text, audio, images, and video. 
    %
    Specifically, \texttt{gpt-4o-2024-08-06} is used in our experiment.
    %
    \item \textbf{Claude 3.5 Sonnet}\cite{claude3.5sonnet}: The latest iteration of Anthropic's LLM, which improves upon its predecessor in benchmarks ranging from math to coding and reasoning problems. 
    %
    Specifically, \texttt{claude-3-5-sonnet-20240620} is utilized in our experiment.
    %
    \item \textbf{Gemini 1.5 Pro}\cite{gemini1.5}: Google's LLM outperforms its previous version across multiple benchmarks and has a maximum context window of 2M tokens, surpassing many closed-source LLMs. 
    %
    Specifically, \texttt{gemini-1.5-pro-002} is used in our experiment.
    %
    \item \textbf{Typhoon V2 70b}\cite{typhoon2}: An LLM specifically optimized for Thai language, continually pre-trained from the 70b variant of Meta's Llama 3.1\cite{llama3.1}, with post-training techniques to enhance Thai language performance while retaining the model’s original capabilities. 
    %
    Specifically, \texttt{typhoon-v2-70b-instruct}\footnote{\url{https://huggingface.co/scb10x/llama3.1-typhoon2-70b-instruct}} is used in our experiment.
    %
    \item \textbf{Typhoon V2 8b}\cite{typhoon2}: An LLM optimized for Thai language, continually pre-trained from the 8b variant of Meta's Llama 3.1\cite{llama3.1}, with post-training techniques to enhance Thai language performance without sacrificing the original model’s capabilities. 
    %
    Specifically, \texttt{typhoon-v2-8b-instruct}\footnote{\url{https://huggingface.co/scb10x/llama3.1-typhoon2-8b-instruct}} is used in our experiment.
\end{enumerate}

These LLMs were selected because they represent the cutting-edge of both closed-source (GPT-4o, Gemini 1.5 Pro, and Claude 3.5 Sonnet) and open-source (Typhoon v2) models, with a focus on the Thai language for the open-source models. 
%
All LLMs are configured to generate a maximum of 2048 tokens with a temperature of 0.5, and the seed is set when possible to ensure consistent results. 
%
All LLMs are also provided with a few-shot examples as well.

% explain prompt


\subsubsection{E2E with best setups}
\label{subsubsec: e2e_best_setup}
In addition to assessing the impact of retriever and LLM choices on system performance, we aim to establish the performance of the current RAG system in comparison to both lower and upper bounds. 
%
Specifically, we evaluate various systems capable of handling the Thai legal QA task, as described in \S\ref{subsec: task}, and measure their retrieval and E2E performance. 
%
The systems included in our experiments are as follows:

\begin{enumerate}
    \item \textbf{Parametric Knowledge: } A system that uses only instruction-tuned LLMs without any additional context. 
    %
    This serves as the baseline, relying solely on the knowledge contained within the LLM’s training data.
    %
    \item \textbf{Naive RAG: } A basic RAG system with traditional components, consisting of a retrieval module utilizing the best-performing naive chunking method and a generation module without additional enhancements. Note that we did not modify the benchmark dataset to contain only relevant laws within the available sections in the naive chunks as done in Experiments in \S\ref{subsubsec: chunk_setup}. 
    %
    This represents the simplest implementation of a RAG system for the legal domain.
    \item \textbf{Proposed RAG: } A RAG system enhanced with custom components tailored to the structure of Thai law. 
    %
    This includes the hierarchy-aware chunking method for the retrieval module and NitiLink component to fetch all cross-referenced sections as additional context, provided these components demonstrably improve performance based on the experiments in \S\ref{subsec: setup_rq1}. In this setup, we did not remove chunks or sections that are not presented in the naive chunks, and the benchmark datasets are not modified unlike the modifications done in \S\ref{subsubsec: chunk_setup}
    %
    \item \textbf{RAG with Golden Context: } A system where only the generation module is used, provided with the prompt and ground truth context. 
    %
    This represents the upper bound of expected performance for a RAG system.
\end{enumerate}

It is important to note that the same retriever models are used for both the Vanilla RAG and the Proposed RAG systems to ensure fair comparisons. Additionally, the same LLM with few-shot examples used in Vanilla RAG and Proposed RAG is utilized for Parametric Knowledge without the relevant laws in the example.  

\subsection{(RQ3) Performance of Long-Context LLM (LCLM)}
\label{subsec: setup_rq3}
% What I need to write? Why LCLM is important -> How I set up the experiment. No mention of O1. Describe both LCLM as retriever and LCLM for E2E task
The rise of long-context large language models (LCLMs) presents new opportunities to potentially replace traditional RAG systems in the Thai legal domain. 
%
With their ability to process extensive context, LCLMs can ingest all 35 Thai financial laws within our benchmark dataset and use them as context to respond to queries. 
%
This capability could eliminate the need for a RAG framework in Thai legal QA systems. 
%
Among the LLMs discussed in \S\ref{subsubsec: llm_setup}, the only model capable of processing all 35 legislations is Gemini 1.5 Pro, which supports a maximum context window of 2 million tokens. 
%
This experiment evaluates the performance of an LCLM-based Thai legal QA system—specifically Gemini 1.5 Pro—when it directly intakes all 35 legislations as context with a few-shot examples. 
%
The system's E2E performance is then compared to that of other baseline systems described in \S\ref{subsubsec: e2e_best_setup} using our proposed benchmark.

Another noteworthy use case of LCLMs in Thai legal QA systems is their application as \textit{retrievers} within a RAG framework. 
%
In this approach, the traditional embedding-based retriever is replaced with an LCLM capable of ingesting all available documents (35 legislations) and is conditioned via a few-shot prompt to output the top-$k$ most relevant documents. 
%
This method leverages the reasoning capabilities of LLMs, allowing them to retrieve relevant documents even for queries requiring additional logical steps. 
%
We evaluate this setup using Gemini 1.5 Pro as the LCLM retriever and compare its retrieval performance to traditional retrieval models on our proposed benchmark.

Lastly, it is important to note that due to budget constraints, the experiments for this research question are conducted on a 20\% stratified subset of NitiBench-CCL (based on relevant legislations) and the full NitiBench-Tax.


