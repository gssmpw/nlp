In a cyber-security context, we assume that the supervisory control system can be under sensor attack.
In sensor attacks, an attacker hijacks and controls a subset of the sensors to reach the critical state.
In this manner, this attacker modifies the closed-loop behavior of the system.
In this section, we review the supervisory control under sensor deception attacks as in \citep{Su:2018,meira-goes:2020synthesis,meira-goes:2023dealing}.
We focus on explaining the probabilistic attacker model and defining the control system under attack.
% In the appendix, we describe in detail the construction of PDES $R_a/G_a$ as defined in \cite{meira-goes:2021synthesis}. 

\subsection{Sensor Attacker} 
We follow the probabilistic sensor attacker model introduced in \citep{meira-goes:2021synthesis}.
A sensor deception attacker compromises a subset of sensor events, denoted $\Sigma_a\subseteq \Sigma$.
This attacker can modify the readings of these compromised events by inserting fictitious events into or deleting event readings from the supervisor.
To identify the attacker actions, insertion and deletion events are modeled using the sets $\Sigma_i = \{ins(e) \mid e \in \Sigma_a\}$ and $\Sigma_d=\{del(e)\mid e \in \Sigma_a\}$, respectively.
The union of the insertion, deletion, and plant event sets, $\Sigma_m = \Sigma\cup\Sigma_i\cup\Sigma_d$, encompasses the event set of the system under attack. 
Formally, the attacker is defined as:

\begin{definition}[Attack strategy]\label{def:attack_str}
An attack strategy with compromised event set $\Sigma_a$ is defined as a partial map $A: \Sigma_m^* \times (\Sigma\cup \{\epsilon\})\rightarrow \Sigma_m^*$ that satisfies for any $t \in \Sigma^*_m$ and $e \in \Sigma\cup \{\epsilon\}$:
\begin{enumerate}
    \item $A(\epsilon,\epsilon) \in \Sigma_{i}^*$ and $A(t,\epsilon) = \epsilon$ for $t\neq \epsilon$
    \item If $e\in \Sigma_a$, then $A(t,e)\in \{e,del(e)\}\Sigma_i^*$
    \item If $e\in \Sigma\setminus\Sigma_a$, then $A(t,e)\in \{e\}\Sigma_i^*$
\end{enumerate}
\end{definition}
The attack strategy $A$ defines a deterministic action based on the last event executed $e$ and modification history $t$.
We extend the function $A$ to concatenate these modifications for any string $s\in \Sigma^*$: $A(\epsilon) = A(\epsilon,\epsilon)$ and $A(s) = A(s^{|s|-1})A(A(s^{|s|-1}),s[|s|])$.
With an abuse of notation, we assume that attack strategy $A$ is encoded as a DFA $A = (X_A, \delta_A, \Sigma_m, x_{0,A})$ as in \citep{meira-goes:2020synthesis,meira-goes2021synthesistac}.
In Appendix~\ref{app:A_aut}, we show the conditions for this encoding.
Intuitively, each state of $A$ encodes one attacker's decision: insertion, deletion, or no attack. 

To identify how the attack actions affect the plant $G$ and supervisor $S$, we define projection operators to reason about events in $\Sigma_m$ in different contexts.
% For example, an insertion event $ins(e)$ is seen by the supervisor as a legitimate event $e$ whereas the $\varepsilon$ (empty) event is executed by the plant since $ins(e)$ is a fictitious event created by the attacker. 
We define projector operator $\Pi^G$ ($\Pi^S$) that projects events in $\Sigma_m$ to events in $\Sigma$ generated by the plant (observed by the supervisor).
Formally, $\Pi^G$ outputs the event that is executed in $G$, i.e., $\Pi^G(ins(e)) = \varepsilon$ and $\Pi^G(del(e)) = \Pi^G(e) = e$. 
Similarly, $\Pi^S$ outputs the event observed by the supervisor, i.e, $\Pi^S(del(e)) = \varepsilon$ and $\Pi^S(ins(e)) = \Pi^S(e) = e$. 
% Lastly, we define the mask operator $\mask:\Sigma_m\rightarrow\Sigma$ that simply removes insertion and deletion ``marks" from events, i.e., $\mask(del(e)) = \mask(ins(e)) = \mask(e) = e$.



\begin{example}
% \rmg{This example should just focus on the attack strategies. We will need figures different than Fig.~5.
% In the next subsection, you can talk about the closed-loop behavior with these strategies.
% The int values should be only mentioned later (next section). We haven't introduced it yet.}
% Figure~\ref{fig:attack-str} depicts two attack strategies for our running example as DFAs $A_1$ and $A_2$. In the attack strategy $A_1$, illustrated in Figure~\ref{fig:M_a}, the attacker does not use memory. Starting from state 2, the attacker inserts the event $b_{\text{ins}}$ at state 1, creating the path $a(b_{\text{ins}})$ that leads to the unsafe state. By introducing $b_{\text{ins}}$ when the relative distance between two cars is one, the attacker misleads the system into believing the distance is safe. This false perception allows the system to advance, ultimately causing the two cars to collide and transitioning the system to the critical unsafe state, $X_{\text{crit},a}$.

% The probability of this path occurring in the attacked system is $0.1(1)=0.1$, while in the nominal system, it is $0.1(0.111...)=0.0111...$.

% In contrast, the second strategy ($A_2$) assumes the attacker has memory, enabling them to track past states. This allows for a more coordinated sequence of actions, such as $aba(b_{\text{ins}})$, which misleads the system over multiple transitions and eventually drives it to the unsafe state.

%  The probability of this sequence originating from the attacker is $0.1(0.111...) \cdot 0.1(1)=0.00111...$ in the attacked system and $0.1(0.111...) \cdot 0.1(0.111...)=0.000123...$ in the nominal system.

%  For the first strategy $A_1$, the intrusion detection value given by Equation~\ref{eq:attack_ratio} is computed as:
% \[
% \text{int}_{A_1} = \frac{0.1}{0.1 + 0.01111111111} \approx 0.9.
% \]

% For the second strategy $A_2$, the intrusion detection value is computed as:
% \[
% \text{int}_{A_2} = \frac{0.00111111111}{0.00111111111 + 0.000123} \approx 0.9003.
% \]

% Both values are approximately equal, indicating similar outcomes for the two attack strategies. This means that the $\lambda$-sda detectability value for these attack strategies is approximately $0.9$, as shown in Equation~\ref{eq:attack_ratio_sdad}.
% Although the attacker employs memory in the second strategy, the $\lambda$-sda detectability value remains approximately equal to that of the memoryless strategy. This indicates that even with memory, the attack remains detectable.
% \rmg{Talk about the two strategies.}
Figure~\ref{fig:attack-strategies} illustrates two attack strategies encoded as automata $A_1$, Fig.~\ref{fig:A1}, and $A_2$, Fig.~\ref{fig:A2}.
Since attack strategies act on observations from the plant, the encoding defines attack strategies for string $s\in \lang(A_1)$ such that $s[|s|] \in \Sigma\cup\Sigma_d$, i.e., the last event is the event observation.
For example, strings $b, a\text{ins}(b)\text{del}(b) \in \lang(A_1)$ define attack strategy $A_1(\epsilon,b)$ and $A_1(a\text{ins}(b),b)$.
After observing $s$, the attack strategy is defined for the last observed event in $s$ followed by any possible insertion in the automaton.
For instance, string $ca\in \lang(A_1)$ defines the attack strategy $A_1(c,a) = a\text{ins}(b)$ since $ca\text{ins}(b) \in \lang(A_1)$ and $\nexists e \in \Sigma_i$ such that $ca\text{ins}(b)e \in \lang(A_1)$.
Similarly, for string $aa\in \lang(A_2)$, the strategy is $A_2(a,a) = a\text{ins}(b)$.
On the other hand, for string $aa\text{ins}(b)\text{del}(b)$, the strategy is $A_2(aa\text{ins}(b),b) = \text{del}(b)$ since state $2$ does not have any feasible insertion event.

Both attack strategies $A_1$ and $A_2$ insert event $b$ when the relative distance between the vehicles is $1$.
The key difference between attackers $A_1$ and $A_2$ is when each attacker inserts event $b$. 
Attacker $A_1$ always inserts $b$ whenever the relative distance is $1$, i.e., immediately after observing $a$ the attacker inserts.
On the other hand, attacker $A_2$ inserts $b$ after the second time the relative distance is $1$.
In Fig.~\ref{fig:A2}, the attacker inserts event $b$ only after two events $a$ happened, i.e., the second time the relative distance among the cars is equal to $1$ as in Fig.~\ref{fig:M_n}. 
% is satisfied.
% For instance, consider a sequence starting with $c a$. 
% As shown in Fig.~\ref{fig:plant_G}, this sequence causes the relative distance between the two cars to become $1$. 
% In this situation, $A_1$ immediately responds by inserting event $b$, resulting in the trace $c a \text{ins}(b)$.

% Attacker $A_2$, however, behaves differently. 
% It does not insert event $b$ immediately after observing $a$; instead, it alternates its behavior in an ``on-and-off" pattern. 
% Consider another path, $c a b a$, where, according to Fig.~\ref{fig:plant_G}, the relative distance between the two cars becomes $1$. 
% The first time this condition is met, $A_2$ skips inserting $b$. 
% However, when the relative distance becomes $1$ again, it responds by inserting $b$. 
% As a result, the trace produced by $A_2$ is $c a b a \text{ins}(b)$. 
% This demonstrates that $A_2$'s behavior depends on the history of events, as it decides whether to insert $b$ based on past occurrences.

% Both attack strategies $A_1$ and $A_2$ ultimately lead the system to the critical state, but they operate differently. Attacker $A_1$ acts without memory—it inserts event $b$ whenever the relative distance condition is met, regardless of past events. In contrast, attacker $A_2$ has memory, meaning its decision to insert event $b$ depends on whether the condition has been satisfied before.
\end{example}

\begin{figure}[thpb]
\begin{subfigure}[t]{0.45\columnwidth}
\centering
\includegraphics[width=0.7\columnwidth]{A1.png}
\caption{Attacker Strategy $A_1$}
\label{fig:A1}
\end{subfigure}
\ 
\begin{subfigure}[t]{0.45\columnwidth}
\centering
\includegraphics[width=0.87\columnwidth]{Figs/A2.png}
\caption{Attacker Strategy $A_2$}
\label{fig:A2}
\end{subfigure}
\caption{Two Attack strategies $A_1$ and $A_2$}
\label{fig:attack-strategies}
\vspace{-2em}
\end{figure}

\subsection{Controlled System under Attack}

The sensor attacker disrupts the \emph{nominal} controlled system $R/G$.
A new controlled behavior is generated when the attack function $A$ is placed in the communication channel between the plant and the supervisor.
We define a new supervisor, denoted by $S_A$, that composes $S$ with the attack strategy $A$.
\begin{definition}[Attacked supervisor]
Given supervisor $S$, a set of compromised events $\Sigma_a\subseteq \Sigma$, and an attack strategy $A$. The attacked supervisor is defined for $s\in \Sigma^*$: 
\begin{equation}
S_A(s) = (S\circ \Pi^S\circ A)(s)
\end{equation}
\end{definition}
% Formally,  is the resulting control action, under attack, after string $s$ has been executed by the system\footnote{$\circ$ is the function composition operator.}.
Based on $S_A$ and $G$, the closed-loop system language under attack is defined as $\mathcal{L}(S_A/G)\subseteq \lang(G)$ as in \citep{Lafortune:2021}. 
The system $S_A/G$ denotes the closed-loop system language under attack, or simply the \emph{attacked system}.
Moreover, $S_A/G$ also generates a p-language in the same manner as $R/G$.
% The language $\lang(S_A/G)$ is defined over $\Sigma$; and it generates a p-language in the same manner as $R/G$.

\begin{example}
We compute the controlled language of the attacked systems $S_{A_1}/G$ and $S_{A_2}/G$, for $A_1$ and $A_2$ as shown in Fig.~\ref{fig:attack-strategies}.
Figure~\ref{fig:SA/G} depicts these two controlled systems under attack.
Let us analyze the controlled system under attack strategy $A_1$. 
At the beginning, $A_1$ does not insert any event, i.e., $A_1(\epsilon,\epsilon) = \epsilon$.
In this manner, $S_{A_1}(\epsilon) = S(\epsilon) = \{a,b,c\}$. 
For this reason, events $a,b$, and $c$ are present in the initial of the automaton in Fig.~\ref{fig:SA1/G}.

Next, let us assume that the plant executed event $a$ for which $A_1(\epsilon, a) = a\text{ins}(b)$.
Recall that after event $a$ the distance between the cars is $1$, i.e., state $1$ in $G$ in Fig.~\ref{fig:plant_G}. 
Attack strategy $A_1$ makes the supervisor return to state $2$ with $S_{A_1}(a) = S(ab) = \{a,b,c\}$, i.e., all events are enabled when the plant is in state $1$.
Thus, the controlled system under attack $A_1$ behaves exactly as in $G$ (open-loop), Fig.~\ref{fig:plant_G}.
A similar analysis can be made for the controlled system under attack $A_2$, Fig.~\ref{fig:SA2/G}.
However, the first time the plant reaches state $1$ in $G$, the controlled system behaves normally as in Fig.\ref{fig:M_n}.
After that, the controlled system under attack $A_2$ behaves as in $G$.
% Using attack strategy $A_1$, the attacker inserts $b$ whenever the relative distance condition is met. For instance, while the plant executes the sequence $c a$, the attacker modifies it to $c a \text{ins}(b)$. 
% The supervisor, unaware of this manipulation, observes the sequence as $c a b$ and interprets $b$ as a legitimate event. Based on this observation, the supervisor assumes that the relative distance between the two cars has increased to $2$, even though the actual distance remains $1$. 
% This misinterpretation can lead the supervisor to allow unsafe actions, such as permitting the plant to execute event $a$ from state $1$, which could result in a collision. 
% This demonstrates how the closed-loop behavior $S_{A_1}/G$ is compromised due to the attacker's manipulation of observed events.

% Attack strategy $A_2$, however, uses a different tactic. Unlike $A_1$, it does nothing the first time the relative distance between the two cars becomes $1$. The attacker allows the plant to execute $c a b$ unmodified during this phase. However, the second time the relative distance becomes $1$, $A_2$ inserts $b$, causing the plant’s execution trace $c a b a$ to be modified to $c a b a \text{ins}(b)$. The supervisor observes this manipulated sequence as $c a b a b$ and again assumes that the relative distance between the cars has increased to $2$, despite the actual distance still being $1$. This deception can lead the supervisor to make similarly unsafe decisions, such as allowing event $a$ to execute from state $1$, resulting in a collision. Thus, while $A_2$ operates differently by incorporating memory, it ultimately disrupts the closed-loop behavior $S_{A_2}/G$ in a manner comparable to $A_1$.


\end{example}
\begin{figure}[thpb]
\begin{subfigure}[t]{0.45\columnwidth}
\centering
\includegraphics[width=0.85\columnwidth]{Figs/plant_G.png}
\caption{$S_{A_1}/G$}
\label{fig:SA1/G}
\end{subfigure}
\ 
\begin{subfigure}[t]{0.45\columnwidth}
\centering
\includegraphics[width=0.87\columnwidth]{Figs/SA2-G.png}
\caption{$S_{A_2}/G$}
\label{fig:SA2/G}
\end{subfigure}
\caption{Controlled system under attack}
\label{fig:SA/G}
\vspace{-2em}
\end{figure}

% The sensor attacker disrupts the \emph{nominal} controlled system $R/G$.
% This attacker can modify the event observations the supervisor receives by inserting events that \emph{have not happened} in the plant or by deleting events that \emph{have occurred} in the plant.
% Based on these attack actions, an attacked plant $G_a$ and attacked supervisor $R_a$ are defined to include \emph{every possible attack action} with respect to $\Sigma_a$, i.e., ``all-out" attacker as in \citep{Carvalho:2018}. 
% The controlled system under sensor attacks is then defined by a PDES $R_a/G_a$ similar as $R/G$.
% Considering these attacker actions, we model the controlled system under sensor attacks by modifying the plant $G$ and supervisor $R$ as in \citep{meira-goes:2021synthesis}.

% The attacked plant $G_a$ is a copy of $G$ with more transitions based on compromised sensors $\Sigma_a$.
% Insertion events are introduced to $G_a$ as self-loops with probability $1$ since fictitious insertions do not alter the state of the plant with probability $1$.
% On the other hand, deletion events are defined in $G_a$ with the same probability as their legitimate events because the attacker can only delete an event if this event has been executed in the plant $G$.
% The following insertion and deletion transitions are added to $G_a$ for any $e\in \Sigma_a$.
% \begin{align}
% \delta_{G_a}(x,ins(e)) = x, \quad & P_{G_a}(x,ins(e),x) = 1\label{eq:ins_plant}\\
% \delta_{G_a}(x,del(e)) = y, \quad & P_{G_a}(x,del(e),y) = P_{G}(x,e,y), \text{ if } \delta_G(x,e)!\label{eq:del_plant}
% \end{align}
% Figure~\ref{fig:plant_Ga} shows the $G_a$ for our running example where these new transitions are highlighted in red.
% % For each transition $y = \delta_G(x,e)$ in $G$, the following deletion transitions are added to $G_a$.
% % \begin{align}
% % \end{align}
% % Equation~\ref{eq:del_plant} models attacker deletions with the same probability as their legitimate events.

% % Figure~\ref{fig:attacked-plant} shows the attacked plant for our running example where new events are highlighted in red.
% % For illustration purposes, we omit the transitions in states $0$ and $3$ since they are deadlock states.
% Similarly to the construction of $G_a$, we define the attacked supervisor $R_a$ as a DFA.
% For the supervisor, insertions are observed as legitimate events while deletions do not change the state of the supervisor.
% For any event $e\in \Sigma_a$, the following transitions are added to $R_a$ on top of those already in $R$.
% \begin{align}
% \delta_{R_a}(x,ins(e)) &= \delta_R(x,e) \text{ if } \delta_R(x,e)! \label{eq:ins_sup}\\
% \delta_{R_a}(x,del(e)) &= x \text{ if } \delta_R(x,e)! \label{eq:del_sup}
% \end{align}
% Figure~\ref{fig:supervisor_Ra} shows the attacked supervisor for our running example.
% % Again for illustration purposes, we omit transitions from state~$3$.

% The controlled system under attack is defined as $R_a/G_a := R_a||_p G_a$.
% Fig.~\ref{fig:supervisor_Ra} shows the system $R_a/G_a$ for our running example.
% Note that, the system $R_a/G_a$ includes every possible sensor attack action since the model considers the worst-case attack scenario in which the attacker can attack whenever it is possible.
% Although this model is simple, it is well-suited to the problem we investigating since we want to detect every possible attack strategy. 
% Next, we show how to specialize this general closed-loop behavior under all possible attacks to specific attack strategies.

% % The PDES $M_a$ defines the language of the attacked system in $\Sigma^*_m$, i.e., with the marks of the attacker modifications.
% % We define the set of critical states in $M_a$ as:
% % $$X_{crit,a} = \{x \in X_{M_a}| \exists s\in \lang(M_a) \text{ s.t. } x_{crit} = \delta_{G_a}(x_{0,G_a},s)\}$$


% % \begin{example}\label{ex:optimalattackedsystem}
% % Back to our running example, let the attacker compromise all $adv$ events, i.e., $\Sigma_a = \{r_{adv},m_{adv}\}$.
% % For simplicity, we omit the full model of the attacker.
% % Intuitively, the attacker inserts a fictitious move by $adv$, $ins(m_{adv})$, when the relative distance between the cars is equal to one.
% % After the insertion, $ego$'s supervisor believes that the relative distance is equal to $2$ which allows $ego$ to advance.
% % If $ego$ moves one cell closer to $adv$ immediately after this insertion, the two cars collide, i.e., state $0$ is in $X_{crit,a}$.
% % Figure~\ref{fig:M_a} depicts the attacked system $M_a$.



% \begin{figure}[htbp]
%     \centering
%     % Subfigure for the attacked plant Ga
%     \begin{subfigure}[b]{0.45\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{Figs/11.png}
%         \caption{Attacked plant $G_a$}
%         \label{fig:plant_Ga}
%     \end{subfigure}
%     \hfill
%     % Subfigure for the attacked supervisor Ra
%     \begin{subfigure}[b]{0.41\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{Figs/10.png}
%         \caption{Attacked supervisor $R_a$}
%         \label{fig:supervisor_Ra}
%     \end{subfigure}
%     \caption{Illustration of the attacked systems $G_a$ and $R_a$}
%     \label{fig:attacked_systems}
% \end{figure}
% %\\
% %\begin{subfigure}{0.9\columnwidth}
% %\centering
% %\includegraphics[width=0.9\columnwidth]{13.png}
% %\caption{Attacked system $M_a$}
% %\label{fig:M_a}
% %\end{subfigure}
% %\vspace{-1em}
% %\caption{Attacked systems}
% %\label{fig:AS}
% %\end{figure}
% %\vspace{-1em}
% % \end{example}




\subsection{Class of Attack Strategies}

So far, we have used the general definition of attack strategies, Def.~\ref{def:attack_str}.
Herein, we have additional constraints that an attack strategy needs to satisfy.
We say the attack strategy is \emph{complete} if the attack strategy is defined for every new event observation the plant generates.
Moreover, we assume that the attack strategy is \emph{consistent} if it does not insert an event disabled by the supervisor.
Lastly, we consider \emph{successful} attack strategies as the ones that can reach the critical state, i.e., strategies that can cause damage.
\begin{definition}[Complete, Consistent, and Successful Strategies \citep{meira-goes:2021synthesis}]\label{def:complete_attacker}
An attack strategy $A$ is \emph{complete} w.r.t. $G$ and $S$ if for any $s$ in $\lang(S_A/G)$, we have that $A(s)$ is defined.
$A$ is \emph{consistent} if for any $e\in \Sigma$, $s\in \lang(S_A/G)$ such that $se \in\lang(S_A/G)$ with $A(s,e) = t$, then $S(\Pi^S(A(s)t^i))!$ and $t[i+1]\in S(\Pi^S(A(s)t^i))$ for all $i\in [|t|-1]$.
Lastly, $A$ is \emph{successful} if $\exists s\in \lang(S_A/G)$ such that $\delta_G(x_{0,G},s) = x_{crit}$.
We denote by $\Psi_A$ as the set of all complete, consistent, and successful attack strategies.  
\end{definition}
% \rmg{FYI: I removed the part of memoryless for now. I am thinking if it is needed.}

% Although the set $\Psi_A$ already restricts the attack strategies space, there are other potential ways of limiting the attacker strategies.
% Let $\mathcal{A}\subseteq\Psi_A$ denotes the \emph{attack set}.
% For example, we could consider $\attackset = \{A_1, A_2\}$ where $A_1,A_2$ are the two attack strategies as in Fig.~\ref{fig:attack-str}.
% We can generalize for any finite set of attack strategies $\attackset = \{A_1,A_2,\dots, A_n\}$ for given attack strategies $A_i\in \Psi_A$ for $1 \leq i\leq n$.

% For this work, we will consider two large classes of attack strategies: memoryless and finite-memory attack strategies.
% A memoryless attack strategy only depends on the states of $G$ and $R$ and is independent of previous attack modifications.
% Intuitively, the attack strategy is memoryless when the \emph{same} action is selected when the controlled system is in states $x_G\in X_G$ and $x_R\in X_R$.

% \begin{definition}[Memoryless attack strategies]\label{def:mem-att-str}
% Attack strategy $A$ is a \emph{memoryless} attack strategy if for all $s_1,s_2\in \lang(S_A/G)$, $e\in \Sigma$, and $s_1e,s_2e\in \lang(S_A/G)$ such that $\delta_G(x_{0,G},s_1) = \delta_G(x_{0,G},s_2)$ and $\delta_R(x_{0,R},\Pi^S(A(s_1))) = \delta_R(x_{0,R},\Pi^S(A(s_2)))$, then $A(s_1,e) = A(s_2,e)$.
% The set of all memoryless attack strategies is defined as $\attackset_{mem}$
% \end{definition}
% \begin{example}
% % \rmg{Back to examples of $A_1$ and $A_2$. $A_1$ is memoryless whereas $A_2$ is finite-memory.}
% In the context of our running example, we assume an attacker capable of manipulating the event $\Sigma_a = \{b\}$. 
% Figure ~\ref{fig:attack-str} illustrates an attack strategy $A_1$, which represents a deterministic and memoryless attack strategy. 
% In both attack strategies, the attacker targets the event $b$, which allows transitions between states 1 and 2. 
% By deleting $b$ when the system is in state 1, the attacker prevents the system from moving to state 2, a safer state. 
% This forces the system to remain in state 1, where the likelihood of executing the unsafe action $a$ increases. 
% The deletion attack reduces the system's flexibility in choosing alternative paths, thus increasing the probability of reaching the critical state (state 0). 
% In addition, the attacker can insert event $b$ when the system is in state 1, misleading the system into believing it has transitioned to state 2. 
% This deception may result in the system allowing the execution of action $a$, which is unsafe in state 1 but permissible in state 2. 
% Consequently, the system is more likely to transition directly to the critical state (state 0). 
% \end{example}


