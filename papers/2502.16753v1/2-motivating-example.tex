Inspired by the problem described in \citep{meira-goes:2020towards}, we consider, as a motivating example, a scenario in which two vehicles are traveling on an infinite, discretized road. In this scenario, the vehicle in front, referred to as \emph{adv veh}, is manually driven, i.e., its actions are uncontrollable. 
The vehicle behind, referred to as \emph{ego veh}, is autonomous.
The two vehicles move in the same direction with an initial distance of 2 units between them.
When the relative distance between the vehicles becomes zero, a collision occurs.
% To simplify the scenario, \emph{adv veh} is considered to have moved out of range when the distance from \emph{ego veh} reaches three or more units.
% In this case, \emph{adv veh} is no longer considered a threat to \emph{ego veh}.


\begin{figure}[h]
    \centering
    \includegraphics[width=0.65\textwidth]{aut-veh-overview.pdf} 
    \caption{Overview of discrete collision avoidance modeling for autonomous vehicles} 
    \label{Example} 
\end{figure}

\noindent \textbf{Controlled system.} 
The goal of the $ego$ controller is to avoid collision with $adv$ by measuring the relative distance between the two cars.
Based on a discrete-state event-driven model of this system, we can use standard supervisory control theory techniques to synthesize a controller that ensures no collision.
Intuitively, this safe controller prevents $ego$ from moving ahead when the relative distance between the cars is equal to one. 

\noindent \textbf{System under attack.} 
Let us consider that $ego$ has been compromised by a sensor attacker.
The attacker hijacks the sensors in $ego$ aiming to cause a collision between $ego$ and $adv$. 
The attacker might use two different attack strategies to cause the collision between the cars.
The first attack strategy, $att_1$, will immediately insert a fictitious reading of relative distance equal to $2$ when $ego$ is only $1$ cell away from $adv$. 
In this manner, the controller allows $ego$ to move forward and collide with $adv$. 
On the other hand, the second attack strategy, $att_2$, only makes the insertion the second time $ego$ and $adv$ are $1$ cell away.
The first attack strategy ``eagerly" changes the nominal behavior to reach a collision whereas the second attack strategy allows the nominal behavior to happen before changing it.

\noindent \textbf{ID systems.}
Using probabilistic information about the system allows detection analysis beyond the logical ID systems.
We discuss ID mechanisms options below. 

\noindent \textbf{(1) Logical ID systems.} 
Logical ID mechanisms rely on monitoring the behavior of the controlled system to determine whether an attack has occurred or not. 
These mechanisms identify a sensor attack when the observed behavior deviates from the nominal behavior \citep{Carvalho:2018, Lima:2019, lin2024diagnosability}. 
% However, when an attacker manipulates sensor observations in such a way that it mimics nominal behavior, such malicious actions may go undetected. 
Since a relative distance of $2$ is possible in the collision avoidance system, both $att_1$ and $att_2$ strategies feed the ID with nominal behavior.
Thus, logical ID systems cannot detect strategies $att_1$ and $att_2$.

\noindent \textbf{(2) Probabilistic $\epsilon$-safety ID system.} 
Our previous works \citep{meira-goes:2020towards, Fahim2024-wodes} provided a probabilistic approach to detect a sensor attack strategy using the notion of $\epsilon$-safety. 
This notion compares the probability of generating a nominal behavior versus an attacked one.
% Let us consider that it is unlikely for $adv$ to immediately move ahead when $ego$ is $1$ cell away, i.e., a probabilistic model of the system in Fig.~\ref{Example}.
According to \citep{Fahim2024-wodes}, this system is $\epsilon$-safe with respect to strategy $att_1$ for a confidence level of $0.9$.
The ID detects $att_1$ since the attacker eagerly inserts the fictitious event without considering its probability changes.
However, $\epsilon$-safety regarding strategy $att_1$ does not provide any information about detecting strategy $att_2$.
We need to verify $att_2$ to guarantee that $\epsilon$-safety holds, i.e., a new run of the verification procedure.

\noindent \textbf{(2) Probabilistic $\lambda$-sensor-attacks ID system.} 
We present an enhanced probabilistic ID approach that goes beyond detecting a specific sensor attack strategy. 
The notion of $\lambda$-sensor-attacks detectability can effectively detect all possible sensor attack strategies.
In Section~\ref{sect:solution}, we show that the collision avoidance system in Fig.~\ref{Example} is $\lambda$-sensor-attacks detectable with confidence level $0.9$.
It means that both $att_1$ and $att_2$ can be detected when using probabilistic information of the system.
% Building on the foundations of our previous works, this new approach significantly expands its applicability and robustness by detecting a broad range of attack strategies, including all complete, consistent, and successful sensor attack strategies. 
% For instance, the new approach 
