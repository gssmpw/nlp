\section{Related Work}
% Sensor calibration has long been a cornerstone for achieving robust perception in autonomous systems. Early work in multi-sensor calibration—such as LiDAR-IMU and LiDAR–camera calibration—has largely focused on controlled, offline scenarios. Many methods in these areas rely on artificial targets or pre-structured environments to establish accurate correspondences \cite{cui2020,dhall2017}. Although target-based approaches can yield high precision, their dependence on manually deployed calibration objects renders them impractical for on-site applications, particularly in dynamic or unstructured environments.

Sensor calibration is a fundamental requirement for robust perception in autonomous systems. Early efforts in multi-sensor calibration, such as LiDAR-IMU, LiDAR-camera, and LiDAR-motor calibration, predominantly focused on controlled, offline environments. Many established methods in these domains rely on artificial targets or pre-structured settings to establish precise correspondences (e.g., \cite{cui2020, dhall2017,gao2019calibration}). While target-based approaches can achieve high accuracy, their dependence on manually deployed calibration objects limits their practicality for on-site applications, particularly in unstructured environments where such setups are often infeasible.

To overcome the limitations of target-based calibration, recent research has focused on targetless methods that leverage naturally occurring features such as edges, planes, and semantic boundaries from raw sensor data \cite{skaloud2006rigorous,park2020,liao2023se}. These approaches extract dense geometric or semantic features and employ complex nonlinear optimization to align sensor measurements, but they incur substantial computational overhead and are highly sensitive to scene structure; their performance may degrade in environments with sparse salient features or overly complex geometry. Moreover, many of these methods assume static or slowly varying conditions, limiting their real-time applicability on mobile platforms. For instance, Skaloud and Lichti \cite{skaloud2006rigorous} introduced a rigorous self-calibration technique for airborne laser scanning that, despite its foundational role, is not well suited for real-time mobile applications due to high complexity. Park et al. \cite{park2020} proposed a spatiotemporal calibration method for camera–LiDAR systems tailored to dynamic environments, yet it still demands dense feature extraction and intensive nonlinear optimization. In urban settings, Liao et al. \cite{liao2023se} presented SE-Calib, a semantic edge-based approach that enhances robustness through semantic constraints, but its reliance on high-quality semantic segmentation adds further computational burden and sensitivity to lighting variations. Similarly, Yuan et al. \cite{yuan2021} achieved pixel-level extrinsic calibration by aligning natural edge features across LiDAR and camera data, while Liu et al. \cite{liu2021} developed an annotation-free method based on semantic alignment loss; both methods, however, face challenges in computational efficiency and adaptability to dynamic scenes. Collectively, these studies underscore a critical trade-off in targetless calibration between eliminating artificial targets for greater deployment flexibility and managing the increased computational load and environmental sensitivity which motivates our development of LiMo-Calib that judiciously selects high-confidence geometric primitives and incorporates robust reweighting strategies to achieve efficient, accurate calibration in dynamic, on-site applications.

Recent work in rotating LiDAR calibration has largely focused on exploiting planar constraints to determine the sensor’s extrinsic parameters. For instance, Zeng et al. \cite{zeng2018} proposed an improved calibration method that optimizes plane data extracted from raw LiDAR scans to enhance accuracy and robustness; however, its reliance on the presence of prominent planar features and high computational cost limits its applicability in cluttered or sparse environments. Similarly, Olivka et al. \cite{olivka2016} addressed the calibration of short-range 2D laser range finders for 3D SLAM by leveraging environmental planar features. Although effective in structured settings, this approach is sensitive to noise and less suited for dynamic or large-scale applications. Furthermore, Alismail and Browning \cite{alismail2015automatic} proposed an automatic calibration algorithm that estimates spinning LiDAR internal parameters by assuming local planar surfaces, a strategy that fails when the plane assumption does not hold and imposes significant computational burdens. Kang and Doh \cite{kang2016full} presented a full-DOF calibration approach using a simple plane measurement to estimate all six degrees of freedom between a rotating LiDAR and its motor; although it performs well in ideal, structured environments, its accuracy diminishes in unstructured or noisy settings due to its dependence on precise plane extraction. Collectively, while these approaches offer effective calibration solutions for rotating LiDAR systems, they share common limitations in terms of heavy dependence on high computational complexity.

Our work diverges from these prior approaches by addressing the dual challenges of computational efficiency and robustness even in some unstructured environments. Rather than processing dense point clouds or requiring artificial targets, LiMo-Calib extracts reliable planar features from raw LiDAR scans using an adaptive, normal-based selection strategy combined with a reweighting mechanism that evaluates local plane fitting quality. This selective feature extraction not only reduces redundancy and computational overhead but also enhances calibration accuracy under high-frequency vibrations and dynamic motion. In doing so, our method bridges the gap between target-based precision and the practical demands of real-time, on-site calibration.