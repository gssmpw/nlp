\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
\usepackage{amssymb}
%\usepackage{txfonts}
%\usepackage{}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{amsmath,amsthm}
\usepackage{float}
\usepackage{enumerate}
\usepackage{epstopdf}
\usepackage{dblfloatfix}
\usepackage{caption}
\usepackage{amsfonts}
\usepackage{fancyhdr}
\usepackage{bbm}
\usepackage{cases}
\usepackage{cite}
\usepackage{breqn}
\usepackage{multirow}
\usepackage{mathrsfs}
\usepackage{algpascal}
\usepackage{algc}
\usepackage{algorithmicx}
\usepackage[ruled]{algorithm}
\usepackage{algpseudocode}
\usepackage{graphics}
\usepackage{epsfig}
\usepackage{mathrsfs} % the form of set
\usepackage{dsfont}
\usepackage{tabularx}
\usepackage{color}
\usepackage{booktabs}
\usepackage{subfigure}
\usepackage{flushend}

%\renewcommand{\captionlabeldelim}{.}
\renewcommand\d{\mathop{}\!\mathrm{d}}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}
\newcommand{\tocaption}{%
\setlength{\abovecaptionskip}{0pt}%
\setlength{\belowcaptionskip}{10pt}%
\caption}

\floatname{algorithm}{Alg.}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}

\floatname{algorithm}{\small{Alg.}}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\makeatletter
\newcommand{\multiline}[1]{%
  \begin{tabularx}{\dimexpr\linewidth-\ALG@thistlm}[t]{@{}X@{}}
    #1
  \end{tabularx}
}

\algdef{SE}[DOWHILE]{Do}{doWhile}{\algorithmicdo}[1]{\algorithmicwhile\ #1}%


\captionsetup{font=small,labelsep=period,justification=justified, singlelinecheck=false}
%\usepackage[letterpaper]{geometry}

% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
 \else
\fi


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor er-ror}

\begin{document}
\theoremstyle{definition} % Set the style of Theorem
\newtheorem{theorem}{Theorem}[section]
\newtheorem{definition}[theorem]{Definition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{example}[theorem]{Example}
\newtheorem{Proposition}[theorem]{Proposition}
\newtheorem{Corollary}[theorem]{Corollary}


\title{Memristor-Based Meta-Learning for Fast mmWave Beam Prediction in Non-Stationary Environments}

\author{
Yuwen Cao\textsuperscript{$\dag$},~\IEEEmembership{}Wenqin Lu\textsuperscript{$\dag$},~\IEEEmembership{}Tomoaki Ohtsuki\textsuperscript{\textdaggerdbl},~\IEEEmembership{}Setareh
Maghsudi\textsuperscript{$\diamond$},~\IEEEmembership{}Xue-Qin Jiang\textsuperscript{$\dag$},~\IEEEmembership{}Charalampos C. Tsimenidis\textsuperscript{$\star$}~\IEEEmembership{}\\
%
\IEEEauthorblockA{\textsuperscript{$\dag$}College of Information Science and Technology, Donghua University, Shanghai, China}
\IEEEauthorblockA{\textsuperscript{\textdaggerdbl}Department of Information and Computer Science, Keio University, Yokohama, Japan}
% \IEEEauthorblockA{\textsuperscript{$\diamond$}Department of Electrical Engineering and Information Technology, Ruhr-University Bochum \& Fraunhofer Heinrich Hertz Institute, Germany}
\IEEEauthorblockA{\textsuperscript{$\diamond$}Ruhr-University Bochum, Germany \& Fraunhofer Heinrich Hertz Institute, Germany}
\IEEEauthorblockA{\textsuperscript{$\star$}Department of Engineering, Nottingham Trent University, Nottingham, United Kingdom}

\thanks{
The work of Y. Cao was supported by
the National Natural Science Foundation of China under Grant
62301143, in part by
Shanghai Sailing Program under Grant 23YF1400800, in part by the Fundamental Research Funds
for the Central Universities under Grant 2232024D-38.
The work of S. Maghsudi was supported by the German Federal Ministry of Education and Research under Project 16KISK035 and 16KISK037.
}

}

% \markboth{IEEE XX XX XX,~Vol.~XX, No.~XX, XXX~2024}
% {}

\maketitle

\begin{abstract}
Traditional machine learning techniques have achieved great success in improving data-rate performance and reducing latency in millimeter wave (mmWave) communications.
However, these methods still face two key challenges: (i) their reliance on large-scale paired data for model training and tuning, which limits performance gains and makes beam predictions outdated, especially in multi-user mmWave systems with large antenna arrays, and (ii) meta-learning (ML)-based beamforming solutions are prone to overfitting when trained on a limited number of tasks. To address these issues,
we propose a memristor-based meta-learning (M-ML) framework for predicting mmWave beam in real time. The M-ML framework generates optimal initialization parameters during the training phase, providing a strong starting point for adapting to  unknown environments during the testing phase. By leveraging memory to store key data, M-ML ensures the predicted beamforming vectors are well-suited to episodically dynamic channel distributions, even when testing and training environments do not align. Simulation results show that our approach delivers high prediction accuracy in new environments, without relying on large datasets. Moreover, M-ML enhances the model's generalization ability and adaptability. 
\end{abstract}
\begin{IEEEkeywords}
Multi-user mmWave communications, meta learning, memristor-based meta-learning (M-ML), memory.
\end{IEEEkeywords}

\section{Introduction}
With the continuous advancement of technology, millimeter wave (mmWave) communication has emerged as a key technology for 5G communications \cite{Qualcomm2022,7876965}. With the continuous development of the 6G technology, there are higher requirements in terms of the data rate, reliability and delay \cite{8241348,9627726,10044183}. In this context,  accurately predicting mmWave beams between base stations (BSs) and mobile users has attracted significant research interest.
To reduce the training overhead incurred for beam prediction, the beamforming vectors can be transformed into multiple low-dimensional vectors, and the optimization of the beamforming vectors can usually be done by solving a weighted sum rate (WSR) maximization problem under total power constraints. However, WSR maximization is non-convex and NP-hard in practice \cite{4712693}.

To tackle this challenge, an exhaustive search algorithm, such as sector scanning, has been developed to identify the best beam \cite{Steinmetzer2017CompressiveMS,5756489}. Specifically, in \cite{Steinmetzer2017CompressiveMS}, two stations send detection frames to each other to measure the received signal strength of each sector, and then the sector with the highest signal strength is selected to establish a connection. 
Since the complexity of this algorithm increases linearly along with the number of predefined sectors, such algorithm is valid only for a limited number of sectors. 
To overcome the limitations in terms of scanning speed involved in traditional methods, \cite{9764610} uses machine learning for beam prediction, i.e., the beam prediction is accelerated by pre-training the mapping function. %which avoids the exhaustive search incurred by traditional methods \cite{Steinmetzer2017CompressiveMS,5756489}. 
Reference \cite{10096315} proposes a model-agnostic meta-learning (MAML), as a model-independent meta-learning (ML) algorithm, to train an initialized model parameter so that it can achieve good generalization on a new task by training one or more gradient updates. Reference 
\cite{9257198} develops a ML-based approach to solve the task mismatch problem by training in a multi-task environment to obtain a generalized initialization model, as well as to achieve fast beam prediction.
However, when encountering episodically dynamic non-stationary environments for training, we observe that the aforementioned ML-based solutions i) may face the risk of \textit{catastrophic forgetting}; ii) suffer from \textit{overfitting} when handling a limited number of training tasks.  
%

Against this background, we study the mmWave beam prediction in multiple-input single-output
(MISO) over downlink broadcast channel (MISO-BC) by exploiting the MAML framework. Specifically, we first decompose the high-dimensional beamforming matrix into low-dimentional components based on the weighted minimum mean squared error (WMMSE) algorithm, followed by formulating a low-dimensional component prediction problem. To address this challenging problem, we propose a memristor-based meta-learning (M-ML) framework for predicting mmWave beam in real time. Afterwards, we use these predicted components to precisely recover the beamformer. Notably, M-ML exploits memory to store key data for facilitating a swift adaption to episodically dynamic channel environment, thus yielding
superior performance.
Note that, the main contributions of this work are summarized as follows:
\begin{itemize}
\item [$\bullet$] In this paper, we formulate a joint beam prediction and memory set selection problem in MISO-BC for facilitating a fast beam prediction   
while improving the
modelâ€™s adaptability in new environments.
%
\item[$\bullet$] We propose a M-ML framework for predicting mmWave beam. Notably, the proposed framework enables high sum rate by prioritizing and sampling these data points with the high loss in memory set, followed by combining these samples with the new training data to participate in model updating and optimization in real-time manner.
%
\item[$\bullet$] Simulations are carried out in practical communication scenario which is episodically dynamic. Comparing M-ML with the state-of-the-art MAML, unsupervised training, and WMMSE methods, we deduce that adding memory to store data dynamically can improve the performance of the model very well in the testing process.
\end{itemize}
%

\section{System Model and Problem Formulation}
%
\subsection{MISO-BC Transmission System}
Consider a MISO-BC transmission system whereby the BS with $N$ antennas serves $K$ single-antenna users (UEs), as shown in Fig. \ref{fig:01}. Then, the signal observed at the $k$th UE can be expressed as
\begin{equation}
{{y}}_k=\mathbf{{h}}_k^H\mathbf{{v}}_k{s}_k+{\sum\nolimits_{i =1,i\neq k}^{K}\mathbf{{h}}_k^H\mathbf{{v}}_i}{s}_i+{n}_k,
\end{equation}
where $\mathbf{v}_i\in\mathbb{C}^{N_{}}$ is the transmit beamforming vector for the UE $i$.
${n}_i \sim\mathcal{CN}(\mathbf{0},\sigma^{2})$ represents the circularly symmetric Gaussian noise with zero mean and variance $\sigma^{2}$.
Let $s_i\sim$ $\mathcal{CN}(0,1)$ be the independent data symbol transmitted to the $i$th UE, and $\mathbf{h}_i\sim\mathcal{CN}(\mathbf{0},\mathbf{I}_N)$ denote the channel distribution between the BS and the $i$th UE. Herein, $\mathcal{CN}(\mu, \boldsymbol{\Sigma})$ denotes the multivariate complex Gaussian
distribution with mean $\mu$ and covariance matrix $\boldsymbol{\Sigma}$.

\begin{figure}[tp]
%\setlength{\abovecaptionskip}{-0.1cm}
%\setlength{\belowcaptionskip}{-0.5cm}
    \begin{center}
    \includegraphics[width=3.4cm]{fig/kuangjia_new.pdf}
    \end{center}
   \caption{Schematic diagram for the system model.}\label{fig:01}
\end{figure}

The instantaneous signal-to-interference-plus-noise-ratio (SINR) received at the UE $k$ is given by
\begin{equation}
\text{SINR}_k=
\frac{|\mathbf{h}_k^H\mathbf{v}_k|^{2}}{\sigma^{2} + \sum_{i=1,i\neq k}^K{|\mathbf{h}_k^H\mathbf{v}_i|}^{2}}.
\end{equation}
Let $\textit{P}$ be the maximum transmission power and $\alpha_k$ denote the system weight of UE $k$. For a given estimated channel distribution, the instantaneous WSR maximization problem involved in the MISO-BS transmission system under the total transmit power constraints can then be expressed as:
\begin{equation}
\begin{aligned}
\label{eq:003}
&\mathop{\text{maximize}}_{\mathbf{V}}\sum\nolimits_{k=1}^{K}{\alpha}_k\text{log}_2{(1+\text{SINR}_k)}\\
&s.t. \; { \mathrm{Tr}(\mathbf{VV}^H)\leq}{P},
\end{aligned}
\end{equation}
where $\mathrm{Tr}(\cdot)$ denotes the trace of a square matrix operation. $\mathbf{V} = [\mathbf{v}_{1}, \ldots,\mathbf{v}_{K}]\in\mathbb{C}^{N\times K}$ is the transmit beamforming matrix for $K$ UEs at each time instance. Note that the optimization problem (3) is difficult to solve for the following reasons: i) the problem (3) is non-convex and ii) using the WMMSE algorithm \cite{5756489} involves the time-consuming iteration process, thus rendering the estimated beamforming weights outdated as the small-scale fading varies in the order of milliseconds. Besides, it is pointed out by reference \cite{6832894} that, the optimum solution to the instantaneous WSR maximization problem (\ref{eq:003}) follows the structure of
\begin{align}
\label{eq:04}
\hat{\mathbf{v}}_k=\sqrt{p_k}\frac{(\mathbf{I}_N+\sum_{k=1}^K\frac{\lambda_k}{\sigma^2}\mathbf{h}_k\mathbf{h}_k^H)^{-1}\mathbf{h}_k}{||(\mathbf{I}_N+\sum_{k=1}^K\frac{\lambda_k}{\sigma^2}\mathbf{h}_k\mathbf{h}_k^H)^{-1}\mathbf{h}_k||_2},\forall k,
\end{align} 
where $\|\cdot\|_{2}$ means the $l_2$ norm operator. $\boldsymbol{\lambda} = [\lambda_{1},\ldots,\lambda_{K}]^{T}$ is a virtual power allocation vector satisfying that $\sum_{k=1}^{K} \lambda_{k} = \sum_{k=1}^{K} p_{k} = P$. However, such a solution, which estimates the optimal vector $\hat{\mathbf{v}}_{k}$ at BS by inputting the optimum $\{\boldsymbol{\lambda},\mathbf{p}\}$ predicted by using 
deep neural network (DNN)-based solutions \cite{9926157,10146432,9314253} into (\ref{eq:04}) will incur prohibitively high computational cost of $\mathcal{O}(N^{3})$, particularly for the large antenna array $N$ in a given scene.

\begin{figure}[tp]
%\setlength{\abovecaptionskip}{-0.2cm}
%\setlength{\belowcaptionskip}{-0.5cm}
    \begin{center}
    \includegraphics[width=8cm]{fig/meta3_new.eps}
    \end{center}
 \caption{An overview of model-agnostic meta-learning framework.}\label{fig:002}
\end{figure}

\begin{figure*}[t!]
%\setlength{\abovecaptionskip}{-0.2cm}
%\setlength{\belowcaptionskip}{-0.6cm}
    \begin{center}
    \includegraphics[width=17cm,height=6.4cm]{fig/mml_New.eps}
    \end{center}
   \caption{An overview of the training and testing phases of the M-ML framework.}\label{fig:03}
\end{figure*}

\subsection{Meta Learning-Inspired Beam Prediction}
%

As illustrated in Fig. \ref{fig:002}, the MAML framework in traditional ML-aided beam prediction starts by defining a set of tasks, each involving prediction from a set of channel realizations \cite{Finn2017ModelAgnosticMF}. Each task has corresponding training data (i.e., the support set denoted by $\mathcal{D}_{s}$) and validation data (i.e, the query set denoted by $\mathcal{D}_{q}$). MAML then uses \textit{two optimization loops} to obtain a meta-initialization for fast adaptation on new tasks. 

The inner loop is used to update the parameters of each task based on the relevant support set $\mathcal{D}_{s}$, while the outer loop is used to update the global neural network parameters based on the query sets $\mathcal{D}_{q}$ of all tasks. Let $Loss_{\mathcal{D}_{s}(k)}\left(\theta_{k}\right)$ be the performance metric for task $k$ in the inner loop.
Then, each task optimizes its parameters independently, typically using gradient descent technique to minimize the loss function on its support set $\mathcal{D}_{s}$ \cite{Finn2017ModelAgnosticMF}, i.e.,
\begin{align}
\label{eq:005}
\hat{\theta}_{k}=\mathop{\arg\min}\limits_{\theta_k} ~ Loss_{\mathcal{D}_{s}(k)}\left(\theta_{k}\right),\quad\forall k.
\end{align}  

In the outer loop, MAML updates the global network parameters with the goal of minimizing the sum of the loss functions of all tasks on their query set $\mathcal{D}_{q}$. Likewise, let $Loss_{\mathcal{D}_{q}}(\psi)$ be the performance metric for the tasks in the outer loop.
Once completing training, it enters the meta-testing phase for adaptation. 
In this phase, the pre-trained global network parameters are used to adapt to the new task.
% The process of updating the parameters of the new task is similar to an inner loop, but using the adaptation dataset. 
The parameters of the new task are updated based on the adaptation dataset and optimized via gradient descent technique \cite{Finn2017ModelAgnosticMF}, i.e.,
\begin{align}
\label{eq:006}
\hat{\psi}=\mathop{\arg\min}\limits_{\psi} ~ Loss_{\mathcal{D}_{q}}(\psi).
\end{align}

% \textcolor{red}{The core task of the meta-learning algorithm in the outer loop (outer loop) phase is to update, through an iterative process, the global network parameters $\varphi$, which serve as the initial point for all tasks, with the goal of finding a parameter configuration that adapts quickly on different tasks. This process focuses on optimizing the generalization ability of the model, i.e., its adaptability on new tasks. By calculating the average loss of the updated parameters in the inner loop (inner loop) over the query set of each task, the outer loop iteratively updates $\varphi$ to minimize this loss. This process does not involve updating the parameters individually for each specific task, but rather extracts generic features and knowledge from the meta-training dataset so that the model can achieve effective adaptation on new tasks with minimal data.}

% \textcolor{red}{Furthermore, the key to the outer loop is to capture commonalities across tasks and utilize these commonalities to guide the rapid adaptation of the model. This approach is particularly suitable for tasks in dynamic environments, where models need to frequently adapt to new situations. In this way, meta-learning algorithms are able to learn how to extract useful features and knowledge from previous tasks and migrate this knowledge to new tasks, thus enabling fast learning and adaptation on new tasks.}


By conducting the above two optimization loops, the optimal beam associated with the underlaying channel distribution then can be determined with low complexity. However, one of the main drawbacks of the MAML algorithm is that it is complicated in the training and testing phases. When encountering divergent training and testing channel distributions, MAML's adaptation process will become to be slow, thus severely degrading the accuracy performance.

\subsection{The Design Rational of Our Proposal}
%

When using a small amount of data, traditional ML based solutions are prone to overfitting. Moreover, since the new environment encountered may be quite different from the training environment when considering the small-scale fading, it is often difficult for the model to achieve optimum performance at the initial stage, and thus it takes longer time to adapt to a new scenario. In this paper, we propose an innovative memory set selection mechanism to update the data, specifically, selecting poorly performing data as a selected representative sample through a loss function defined in (\ref{eq:012}). Usually, these data points contain information that the model has not yet adapted to or understood in the new environment.
Accordingly, the latest test data and representative samples are combined to form a memory set $\mathcal{M}_{t}$, whose size is $M$. In this sense, the  memory set is dynamically updated at time instance $t+1$ according to
\begin{align}
\label{eq:0007}
    \mathcal{M}_{t+1}=\left\{D_{t}^{\left(i\right)}\right\}_{i\in\Omega_{Dt}}\cup\left\{M_{t}^{\left(i\right)}\right\}_{i\in\Omega_{Mt}},
\end{align}
where $D_{t}^{(i)}$ denotes the test data collected at time instance $t$, having the characteristics and patterns of
the new environment. 
$\Omega_{Dt}$ and $\Omega_{Mt}$ mean the index sets of the latest test data $\mathcal{D}_{t}$ and selected representative samples $\mathcal{M}_{t}$, respectively. 
The selection of data ensures that the selected samples capture key features of the environment. Then, the proposed M-ML framework updates the global network parameter in the testing phase by minimizing the sum of the loss functions
of all tasks on their updated query set $\mathcal{M}_{t}$ at the time instance $t$ under the total memory size constraint:
\begin{align}
\small
\label{eq:0008}
\begin{split}
&\mathop{\text{minimize}}_{\psi_{t}}\quad Loss_{\mathcal{M}_{t}}(\psi_{t})\\
&s.t. \; \left|\left\{{D}_{t}^{\left(i\right)}\right\}_{i\in\Omega_{Dt}}\cup\left\{{M}_{t}^{\left(i\right)}\right\}_{i\in\Omega_{Mt}}\right| \leq M, \;\; t=1,2,\ldots
\end{split}
\end{align}

Based on the above discussions and following the MAML-aided beam prediction approaches \cite{10096315,9257198}, we attempt to solve the joint beam prediction and the memory set selection problem in real time by following three consecutive stages: i) beamforming matrix decomposition, ii) M-ML model training and testing phases, iii) beamformer reconstruction. Accordingly, the joint beam prediction and the memory set selection problem is divided into the following cascaded subproblems:
\begin{itemize}
\item \textbf{Beamforming matrix decomposition:} We decompose the beamforming matrix $\mathbf{V}$ of size $N\times K$ into three low-dimensional components $\mathbf{w}$ of size $K\times $1 in (\ref{eq:0009}), $\mathbf{u}$ of size $K\times $1 in (\ref{eq:0010}), and $\mu$.   
%
\item \textbf{M-ML model training and testing:} M-ML predicts the low-dimentional components $\mathbf{w}_{}$,  $\mathbf{u}_{}$, and $\mu$ by training the model parameters $\theta_{u}^{l}$ in (\ref{eq:0013}), $\theta_{w}^{l}$ in (\ref{eq:0014}), and $\theta_{\mu}^{l}$ in (\ref{eq:0015}) using an updated dataset $\mathcal{M}_{t}$.   
%
\item \textbf{Beamformer reconstruction:} We reconstruct the beamforming matrix using these predicted components according to (\ref{eq:009}).
\end{itemize}

% The above joint beamforming and memory set selection optimization is summarized in Algorithm 1. More details are explained in Section III. 

% \alglanguage{pseudocode}
% \begin{algorithm}[!t]\small
% \caption{Summary of Joint Beam Prediction and Memory Set Selection}
% \label{alg:Framwork2}
% \begin{algorithmic}[1]
% % \Require
% % Initialize the memory set $M_0$ = $\varnothing$, memory set size $M$, counter of time slots $t$ = 0, the index sets of the selected samples $\Omega_{D_t}$, and the index sets of the selected samples $\Omega_{M_t}$
% % \Ensure The non-stationary file popularity $\hat{\mu}^{}_{1:F,1:T}$.
% \For{{$t$} = 1, 2, $\ldots$}
% \State \multiline{Decompose
% the beamforming matrix $\mathbf{V}_t$ into three low-dimensional components based on the WMMSE algorithm.}
% \State \multiline{Learn the decomposed low-dimensional components by using the updated memory set $\mathcal{M}_{t}$.}
% \State \multiline{Reconstruct the beamforming matrix $\hat{\mathbf{V}}_t$ towards the new channel distribution using the predicted results in step 3.}
% \EndFor
% \end{algorithmic}
% \end{algorithm}
% %-------------------------------->

% \begin{figure}[tp]
% % \setlength{\abovecaptionskip}{-0.5cm}
% % \setlength{\belowcaptionskip}{-0.2cm}
%     \begin{center}
%     \includegraphics[width=8cm]{fig/re3.pdf}
%     \end{center}
%    \caption{An overview of the meta-regularization framework.}\label{fig:02}
% \end{figure}


\section{M-ML Aided Beam Prediction}
%
In this Section, we propose an innovative M-ML framework for predicting mmWave beam in real time, as illustrated in Fig. \ref{fig:03}. The main objective of this framework is to combine the generalization ability of ML pre-trained models with the storage property of memristors. 
When deploying the pre-trained model into a new test environment, our approach utilizes the memory to dynamically store and process data in the new environment. 
%In this case, our approach can not only maintain high performance on the original task, but also show good generalization ability and robustness in the new test environment. 
More details are as follows.

\subsection{Beamforming Matrix Decomposition}
%
Along with an increasing number of transmitting antennas and users, estimating the beamforming vectors directly at BS by using DNN-based solutions \cite{9926157,10146432,9314253} aggravates the training burden of the neural networks. To tackle this challenge, we attempt to decompose the beamforming matrix  $\mathbf{V} = \left[\mathbf{v}_{1},\ldots,\mathbf{v}_{K}\right]$ into three low-dimensional components, i.e., $w_{k}$, $u_{k}$, and $\mu_{}$. 
The expressions of $w_k$ and $\mu_k$ are as follows:
\begin{equation}
\label{eq:0009}
w_k=
\frac{\sigma^2+{\sum_{j=1}^N\left|\mathbf{h}_k^H\mathbf{v}_j\right|}^{2}}{{\sigma}^{2} + \sum_{j=1,j\neq k}^N{\left|{\mathbf{h}_k^H}\mathbf{v}_j\right|}^{2}},
\end{equation}
\begin{equation}
\label{eq:0010}
u_k=
\frac{\mathbf{h}_k^H\mathbf{v}_k}{{\sigma}^{2} + \sum_{j=1}^N{\left|{\mathbf{h}_k^H}\mathbf{v}_j\right|}^{2}}.
\end{equation}
While $\mu$ $\geq$ 0 is a Lagrange multiplier that exists in the power constraints when finding the first-order optimality condition of the beamforming matrix in the WMMSE algorithm \cite{5756489}. 
%

In the training phase, we employ three independent neural networks, each of which is responsible for predicting three basic components of the beamforming vectors, i.e.,  $w_{k}$, ${u}_k$, and $\mu$. The training starts with a random initial value of $\mathbf{V}$, followed by the predictions of $w_k$ and $u_k$ using the current estimate of $\mathbf{V}$. %while $\mu$ is determined by a dichotomous search method, which is different from the neural network prediction approach employed in our method. 
Once finishing the above prediction process, these components are used to reconstruct the beamforming vector $\hat{\bar{\mathbf{v}}}_{k}$ of UE $k$ by means of  
\begin{equation}
\label{eq:009}
\hat{\bar{\mathbf{v}}}_k=\alpha_ku_kw_k{\mathbf{h}_k}\left(\mathbf{S}+\mu\mathbf{I}_{N}\right)^{-1},
\end{equation}
where $\mathbf{S} := \sum_{k=1}^K\alpha_k|u_k|^2w_k\mathbf{h}_k\mathbf{h}_k^H$.
%\textcolor{red}{Accordingly, the power constraint is satisfied by normalizing the transmit beamforming matrix $\hat{\bar{\mathbf{V}}}$ and then iteratively updating it according to the above equations.} 
%where $\alpha_k$ is the system weight of user $k$, which is a value controlled by the communication system.

It seems that the above updating process is similar to the WMMSE algorithm \cite{5756489}, which continues until a preset error criterion is reached. However, the difference is that: i) our approach simplifies the iterative process by directly predicting $u_k$, $w_k$, and $\mu$ through neural networks and then reconstructs the value of $\hat{\bar{\mathbf{V}}}$ based on these predictions.
ii) In the training phase, we specifically design the loss function, which is defined as follows: 
% \begin{equation}
% \label{eq:012}
% \begin{split}
% Loss(v)=&-\sum_{i=1}^{K}\text{log}~\text{det}\Big(\mathbf{I}+\\
% &\mathbf{h}_i\mathbf{v}_i\mathbf{v}_i^H\mathbf{h}_i^H\Big(\sum_{j\neq i}^{K}\mathbf{h}_j\mathbf{v}_j\mathbf{v}_j^H\mathbf{h}_j^H+\sigma^2\mathbf{I}\Big)^{-1}\Big),
% \end{split}
% \end{equation}
\begin{equation}
\label{eq:012}
\begin{split}
Loss(\mathbf{V})=&-\frac{1}{K}\sum\nolimits_{i=1}^{K}\text{log}~\Big(1+\\
&\mathbf{h}^{H}_i\mathbf{v}^{}_i\mathbf{v}^{H}_i\mathbf{h}_i\Big(\sigma^2 + \sum_{j\neq i}^{K}\mathbf{h}^{H}_j\mathbf{v}_j\mathbf{v}_j^H\mathbf{h}_j\Big)^{-1}\Big).
\end{split}
\end{equation}

\addtolength{\topmargin}{0.05in}

\subsection{Inner Model Training}
%
In each training cycle, we randomly select $N_s$ and $N_q$ data points from the channel realizations of each task to construct the support set $\mathcal{D}^i_s$ and the query set $\mathcal{D}^i_q$, respectively, where $i$ indicates the index of the channel groups. For each single training epoch, the parameters $\theta_u$, $\theta_w$, and $\theta_{\mu}$ experience total numbers of $N_t$ updates.
Likewise, define $\theta_u$, $\theta_w$, and $\theta_{\mu}$ as the parameters related to the inner models trained by a total number of $N_{t} \times (N_s + N_q)$ channel realization samples as entry to predict $\mathbf{u}$, $\mathbf{w}$, and $\mu$, respectively. Define $\varphi_u$,  $\varphi_w$, and  $\varphi_{\mu}$ as the parameters related to the outer models.


% In the inner loop, we process a total number of $N_t \times (N_s + N_q)$ channel realization samples as input to predict $u_{k}$, $w_k$, and $\mu$ for $\forall k\in\{1,\ldots,K\}$. 
% We then reconstruct the beamforming vectors and normalize them to satisfy the total power requirement in (\ref{eq:003}), and finally compute the sum rate as well as the loss for each user. 
The inner model parameters are updated following the criteria of 
\begin{equation}
\label{eq:0013}
\theta_{u}^{i}=\theta_{u}^{i-1}-a\nabla_{\theta_{u}}\sum\nolimits_{d=1}^{N_{s}}Loss(\mathbf{V}_{d}^{i}),
\end{equation}
\begin{equation}
\label{eq:0014}
\theta_{w}^{i}=\theta_{w}^{i-1}-a\nabla_{\theta_{w}}\sum\nolimits_{d=1}^{N_{s}}Loss(\mathbf{V}_{d}^{i}),
\end{equation}
\begin{equation}
\label{eq:0015}
\theta_{\mu}^{i}=\theta_{\mu}^{i-1}-a\nabla_{\theta_{\mu}}\sum\nolimits_{d=1}^{N_{s}}Loss(\mathbf{V}_{d}^{i}),
\end{equation}
where $\nabla_{x}$ means the partial derivative of the loss function $Loss(\cdot)$ against the parameter $x$. Besides, $a$ is the learning rate of the inner model and $\mathbf{V}_d^i$ represents the beamforming matrix realized for the $d$th channel in the $i$th set. At the beginning of each period, the parameter updates are influenced by the last set of data from the previous period. The parameters update after processing the last set of data in the current period is considered as the final parameter for that period.

\subsection{Outer Model Testing}
%
Once finishing the inner loop, the model enters the outer loop, and 
the corresponding outer model parameters are updated following the criteria of \footnote{
As a practical communication scenario which is episodically dynamic is considered in our MISO-BC systems, we add a superscript "e" in the outer model parameters.   
}
\begin{equation}
\label{eq:016}
\varphi_u^e=\theta_u^e-\beta\nabla_{\theta_u}\sum\nolimits_{i=1}^{N_t}\sum\nolimits_{d=1}^{N_q}Loss(\mathbf{V}_d^{e,i}),
\end{equation}
\begin{equation}
\label{eq:017}
\varphi_{w}^{e}=\theta_{w}^{e}-\beta\nabla_{\theta_{w}}\sum\nolimits_{i=1}^{N_{t}}\sum\nolimits_{d=1}^{N_{q}}Loss(\mathbf{V}_{d}^{e,i}),
\end{equation}
\begin{equation}
\label{eq:018}
\varphi_{\mu}^{e}=\theta_{\mu}^{e}-\beta\nabla_{\theta_{\mu}}\sum\nolimits_{i=1}^{N_{t}}\sum\nolimits_{d=1}^{N_{q}}Loss(\mathbf{V}_{d}^{e,i}),
\end{equation}
where the parameter $\beta$ is the learning rate of the outer model, and $\mathbf{V}_d^{e,i}$ accounts for the beamforming matrix corresponding to the $d$th data point of group $i$ at the $e$th epoch.

During the testing period, we dynamically add the test data $D_{t}^{(i)}$ collected in real time to the memory set $\mathcal{M}^{(i)}_{t}$.\footnote{These new data capture the important characteristics and subtle patterns of the new environment.} Subsequently, we combine these new data with the existing data into $\mathcal{M}^{(i)}_{t}$ which serves as the input to the neural networks to jointly participate in the next round of testing and training. This approach not only enriches the model's knowledge of the new environment, but also provides more information for the model to adjust and optimize its own parameters. 
%thus achieving more accurate prediction and higher performance in the new environment.

To further improve the model's adaptability in the new environment, we adopt a loss-based ranking strategy. According to (\ref{eq:012}), we evaluate and rank the losses incurred by the model during testing and identify the data points with the most severe losses, i.e., 
\begin{align}
\hat{\Omega}_{Mt} = \mathop{\arg\max}\limits_{\mathcal{M}_{t}} ~ Loss_{}(\mathbf{V}_{d}^{e,i}).
\end{align}
These data points usually contain information that the model has not yet adapted to or understood in the new environment, and thus have a high learning value. We prioritize these data points with the highest losses in memory set $\mathcal{M}_{t}$ to ensure that the model can focus on these difficult samples during the next round of training. Given that the BS's storage capacity (i.e., the memory size) is $M$, then the top $M-\pi(t)$ highest ranking channel groups are selected and added to the memory set $\mathcal{M}_{t}$, where $\pi(t)$ accounts for the samples selected from $D_{t}^{(i)}$.   
In this way, the model is not only able to quickly learn key features in new environments, but also gradually reduces its sensitivity to false predictions. This memory update strategy based on loss sensitivity effectively improves the model's adaptability and accuracy in the face of new and unknown environments, providing strong support for the robustness of meta-learning algorithms in practical applications. Note that the above procedure is summarized in the Algorithm 1.

\alglanguage{pseudocode}
\begin{algorithm}[!t]\small
\caption{\small{Proposed M-ML Aided Beam Prediction Algorithm}}
\label{alg:Framwork2}
\begin{algorithmic}[1]
\Require
Initialize the memory set $\mathcal{M}_0$ = $\varnothing$, memory set size $M$, beamforming matrix $\mathbf{V}$, counter of time slots $t$ = 0, the index sets of the selected samples $\Omega$.
\Ensure The index set $\hat{\bar{\Omega}}^{}_{M_{t}}$ and the recovered beamformer $\hat{\bar{\mathbf{V}}}_{}$.

\State
$\;\vartriangleright$ \textbf{Beamforming matrix decomposition}
\State \multiline{Decompose the beamforming matrix $\mathbf{V}$ into low-dimensional components $\mathbf{w}$, $\mathbf{u}$, and $\mu$. }
$\vartriangleright$ \textbf{Low-dimensional component prediction}
\For{{$T$} = 1, 2, $\ldots$}
%
\State \multiline{Select $N_s$ and $N_q$ data from the channel realizations of each task at random to construct the support set $\mathcal{D}^i_s$ and the query set $\mathcal{D}^{i}_{q}$, respectively, to form a memory set $\mathcal{M}_T = \mathcal{D}^i_s \cup \mathcal{D}^{i}_{q}$. 
 }
\For{$t = 1, \ldots, T$}
%
\State\multiline{Predict the instantaneous values of low-dimensional components $\mathbf{w}_{t}$ in (\ref{eq:0009}), $\mathbf{u}_{t}$ in (\ref{eq:0010}), and $\mu_{t}$.}
 \State \multiline{Reconstruct the beamforming matrix $\hat{\bar{\mathbf{V}}}_{t}$ based on these predicted low-dimensional components by (\ref{eq:009}).}
 \State \multiline{Update the inner model parameters $\theta^{t}_{u}$, $\theta^{t}_{w}$, and $\theta^{t}_{\mu}$ by minimizing the loss function $Loss(\hat{\bar{\mathbf{V}}}_{t})$ in (\ref{eq:012}).}
\EndFor
\For{$t = 1, \ldots, T$} 
\State \multiline{Predict the instantaneous value of low-dimensional components $\mathbf{w}_{t}$, $\mathbf{u}_{t}$, $\mu_{t}$, and reconstruct the beamforming matrix $\hat{\bar{\mathbf{V}}}_{t}$ based on these predicted low-dimensional components.}
%
\State \multiline{Update the outer model parameters $\varphi_u^e$, $\varphi_w^e$, and $\varphi_{\mu}^e$ by minimizing the loss function $Loss(\hat{\bar{\mathbf{V}}}_{t})$ in (\ref{eq:012}).}
\EndFor
\State \multiline{Select $\pi(t)$ samples from $D_{t}^{(i)}$ and then update the index set of the selected sample $\Omega_{D_{t}}$.}
\State \multiline{Identify the data points with the most severe
losses by searching the index
sets $\hat{\bar{\Omega}}_{M_{t}}$ according to
\begin{align}
    \small
\hat{\bar{\Omega}}_{M_{t}}
    \leftarrow
    \textrm{Rank}_{M-\pi\left(t\right)}\left(Loss(\hat{\bar{\mathbf{V}}}_{t})\right).
\end{align}
}
\State \multiline{Update the memory set $ \mathcal{M}_{T+1}\leftarrow\{D_{t}^{\left(i\right)}\}_{i\in\Omega_{Dt}}\cup\{M_{t}^{\left(i\right)}\}_{i\in\hat{\bar{\Omega}}_{Mt}}.
$}
\State \multiline{Normalize
the recovered beamformer to satisfy the total power requirement in (\ref{eq:003}) by
\begin{align}
\small
\label{equ:040914}
\hat{\bar{\mathbf{V}}}_{t} \leftarrow \frac{\hat{\bar{\mathbf{V}}}_t}{\|\hat{\bar{\mathbf{V}}}_t\|_{2}} \sqrt{P}.
\end{align}
}
\EndFor
\end{algorithmic}
\end{algorithm}
%-------------------------------->

% \alglanguage{pseudocode}
% \begin{algorithm}[!t]\small
% \caption{Proposed M-ML Aided Beam Prediction Framework}
% \label{alg:Framwork2}
% \begin{algorithmic}[1]
% \Require
% Initialize the memory set $M_0$ = $\varnothing$, memory set size $M$, beamforming matrix $\mathbf{V}$, counter of time slots $t$ = 0, the index sets of the selected samples $\Omega_{D_t}$, and the index sets of the selected samples $\Omega_{M_t}$
% \Ensure The index set $\hat{\Omega}^{}_{M_{t}}$ and the recovered beamforming matrix $\hat{\mathbf{V}}_{}$.
% \For{{$T$} = 1, 2, $\ldots$}
% % \State \multiline{Select the unsatisfied set $\mathcal{A}_{T}^{*}$ by conducting the unsatisfactory measure in Definition II.1, and maximize the cardinality of set $\mathcal{A}_{T,\mathcal{S}}^{*}$ from $\mathcal{A}_{T}^{*}$ within $T(\boldsymbol\tau)$.}
% \For{{$t$} = 1, $\ldots$, $\theta$}  $\;\vartriangleright$ \textbf{Beamforming matrix decomposition}
% \State \multiline{Decompose the beamforming matrix $\mathbf{V}$ into low-dimensional components $w_k$, $u_k$, and $\mu$ for $\forall k \in \{1,\ldots,K\}$. }
% \State \multiline{Calculate the values of $w_{k}$ and $u_k$ according to (\ref{eq:009}) and (\ref{eq:010}) using the current estimate of $\mathbf{V}_t$.}
% \State \multiline{Calculate the value of $\mu$ according to the WMMSE algorithm.}
% \EndFor
% \For{$t = 1, \ldots, \theta$}   $\vartriangleright$ \textbf{Low-dimension component learning}
% \State \multiline{Select $N_s$ and $N_q$ data from the channel realizations of each task at random to construct the support set $\mathcal{D}^t_s$ and the query set $\mathcal{D}^t_q$, respectively.}
% \State \multiline{Update the inner model parameters $\theta^{t}_{u}$, $\theta^{t}_{w}$, and $\theta^{t}_{\mu}$ based on (\ref{eq:0013}), (\ref{eq:0014}), and (\ref{eq:0015}), respectively.}
% %
% \State\multiline{Predict the instantaneous value of low-dimensional components $\mathbf{w}$ in (\ref{eq:009}), $\mathbf{u}$ in (\ref{eq:010}), and $\mu$ by minimizing the loss function $Loss(\mathbf{V}_d^{t})$ in ((\ref{eq:012})).}
% % \State \multiline{Estimate the files' popularity $\hat{\mu}^{}_{f,\theta} \leftarrow \frac{1}{T_{\theta}}\sum\nolimits_{t= \bar{t}_{m}}^{\bar{t}_{m}+T_{\theta}} \hat{q}_{f,t}$.}
% \EndFor
% %
% \State \multiline{Update the outer model parameters $\varphi_u^e$, $\varphi_w^e$, and $\varphi_{\mu}^e$ by using the gradient decent method according to (\ref{eq:016}), (\ref{eq:017}), and (\ref{eq:018}), respectively. Moreover, $\boldsymbol{\Theta}_{q}$ is given by
% \begin{align}
% \small
% \label{equ:040913}
% \boldsymbol{\Theta}_{q} \leftarrow  \nabla  F(\mathbf{x}_{i,j}, {y}_{i,j}; \boldsymbol{\rho}_{q,t}) - \frac{1}{Q} \sum\limits_{i \in \mathcal{A}^{*}_{T,\mathcal{S}}} \; {Q_{i} \cdot \mathbf{w}_{q,i,t} }.
% \end{align}}
% \EndFor
% \end{algorithmic}
% \end{algorithm}
% %-------------------------------->

\section{Simulation Results}
%
\subsection{Simulation Specification}
%
Our experiments are conducted in a MISO-BC system using traditional meta-learning (i.e., MAML \cite{Finn2017ModelAgnosticMF}), unsupervised learning, and meta-learning without pre-training, WMMSE \cite{5756489}, and the proposed M-ML aided beam prediction methods. 
We adopt the instantaneous WSR as the performance metric.
The MISO-BC system configuration used in our experiments consists of one BS and three users with the following parameters: $N$ = 3, and $K$ = 3. $N_s$ = $N_q$ = $N_t$ = 40 are set during the experiments. We adopt the Adam optimizer for optimization, and we set the learning rate to be $a$= 0.01 and $b$ = 0.001. For each neural network considered in this paper, the number of neurons in each layer (except for the output layer) is set to be 64.
Besides, for each user the weight $\alpha_k$ is set to be 1.

Both Rayleigh, Rician and Nakagami-$m$ fading channels are used for training. During the training of the experiments, the proportion of each of the two different types of channels is kept equal to 50$\%$ and the dataset with the size of 500 is trained using each learning method.

\begin{figure}[t!]
\setlength{\abovecaptionskip}{-0.1cm}
\setlength{\belowcaptionskip}{-0.5cm}
    \begin{center}
    \includegraphics[width=0.45\textwidth]{fig/rician21-eps1.pdf}
    \end{center}
 \caption{WSR performance comparison of distinct beam prediction approaches in Rician fading environment.}    \label{fig:5}
\end{figure}

 
\begin{figure}[t!]
\setlength{\abovecaptionskip}{-0.1cm}
\setlength{\belowcaptionskip}{-0.5cm}
    \begin{center}
    \includegraphics[width=0.45\textwidth]{fig/rayleigh21-eps1.pdf}
    \end{center}
 \caption{WSR performance comparison of distinct beam prediction approaches in Rayleigh fading environment.}    \label{fig:6}
\end{figure}

\subsection{WSR Performance Comparisons}
%
% For the memory-based meta-learning beam prediction in the model, during the experiments, we performed comparison tests by four methods: unsupervised learning, traditional meta-learning, WMMSE and no pre-training and only adding memristors.

In Figs. \ref{fig:5} and \ref{fig:6}, we first test the model in the same distribution as the training environment so as to show the performance of the model in the test environment. It is easy to see from both Figs. \ref{fig:5} and \ref{fig:6} that, the traditional MAML approach outperforms the WMMSE and unsupervised learning approaches at high signal-to-noise ratios (SNRs). In addition, our M-ML introduces a memorization mechanism based on the traditional MAML, which is designed to enhance the model's ability of rapid adaptation to i) the environment where both training and testing channels follow the same distribution and ii) the new environments. 
%Although the performance improvement is not significant in the test environment with the same distribution, this \textcolor{red}{indicates} potential room for improvement in the model's adaptability when encountering different environments.

Subsequently, in Figs. \ref{fig:7} and \ref{fig:8}, we extend the test range by choosing a test environment with a different distribution from the training environment, which is Nakagami-$m$ fading channel distribution with $m$=1 and $m$=10, to further evaluate the model's generalization ability. Simulation results show that all of the above mentioned methods suffer from WSR performance degradation when the models encounter mismatched training and testing channel distributions. However, the unsupervised learning method performs the worst performance in this case due to their lack of exploiting prior knowledge of the environment. We observe that the proposed M-ML scheme can significantly improve the model's adaptability and WSR performance in new environments. This is due to the loss sensitivity inspired memory set selection mechanism, which is helpful to store key data from memory and the latest data collected from the new environments. 
%The benefit of the improved adaptability of M-ML is not only reflected in the fast response to new environments, but also means that the model is able to maintain stable and reliable performance in a wider range of application scenarios. 
%Based on the above results, we deduce that M-ML can provide strong support for the robustness and flexibility of meta-learning in real-world applications.
\begin{figure}[t!]
\setlength{\abovecaptionskip}{-0.1cm}
\setlength{\belowcaptionskip}{-0.5cm}
    \begin{center}
    \includegraphics[width=0.45\textwidth]{fig/nami1d1-eps1.pdf}
    \end{center}
 \caption{WSR performance comparison of distinct beam prediction approaches in Nakagami-$m$ fading environment with $m$ = 1.}    \label{fig:7}
\end{figure}


\section{Conclusion}
In this paper, we proposed an innovative M-ML framework for predicting mmWave beam, aiming to significantly improve the model's adaptability in new environments. Our method extends the traditional MAML framework by introducing a core component, i.e., the memristor, which enhances the model's ability of dynamically learning and memorizing the features of new environments. Simulation results demonstrated that, our approach achieves significant performance gains in trained scenarios. By dynamically storing the data collected in the new environment into memory and fully utilizing them during subsequent training and testing, our model is able to swiftly adapt to changes in the new environment and achieve more accurate predictions.

\begin{figure}[t!]
\setlength{\abovecaptionskip}{-0.1cm}
\setlength{\belowcaptionskip}{-0.5cm}
    \begin{center}
    \includegraphics[width=0.45\textwidth]{fig/nami10d1-eps1.pdf}
    \end{center}
 \caption{WSR performance comparison of distinct beam prediction approaches in Nakagami-$m$ fading environment with $m$ = 10.}    \label{fig:8}
\end{figure}

\bibliographystyle{IEEEtran}%
\bibliography{ref1}

\end{document}