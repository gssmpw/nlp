\section{Related Work}
The concept of rationales was created in \cite{lei_rationalizing_2016} and its relation to mutual information was shown by \cite{chen_learning_2018}. There have been a variety of follow-up works tackling different issues faced by these networks. \citeauthor{yu_rethinking_2019} addressed the problem of rationale degeneration through the idea of complement control: minimizing the amount of information left in the complement of the rationale. \citeauthor{chang_game_2019} leveraged the idea of counterfactuals for rationales, but did these counterfactuals were different selections over a single input document. \citeauthor{chang_invariant_2020} used a variety of training dataset environments and invariant learning to find a rationale policy that generalizes across the domains. Our work builds most directly on \citeauthor{plyler_making_2021} which uses generative models to create a counterfactual dataset. \citeauthor{liu_d-separation_2023} and \citeauthor{zhang_towards_2023} have both taken a causal prospective on the rationalization problem where \cite{liu_d-separation_2023} builds counterfactuals during training by perturbing the rationales and \citeauthor{zhang_towards_2023} evaluated differences in predictions using the whole document versus the rationales. We selected MMI \cite{lei_rationalizing_2016} \cite{chen_learning_2018} and CDA \cite{plyler_making_2021} as methods to re-implement as baselines because our work is most directly built on these methods. We also re-implemented \cite{yu_rethinking_2019} because it directly generalizes to the rationalization over sentences setting while many methods are tied to the token-rationalization scheme. Its also important to point out that our data augmentation method is offline from model training and the $train\_rationale$ procedure in Algorithm \ref{alg:iter_cda} can generally be replaced with any rationalization strategy.

Prior work in the causal modeling community focused on identifying causal signals and helping models ignore spurious signals. Sun et al. 2021 shows that identifying such causal signals helps models become shift-invariant. Veitch et al. 2021 leverages causal inference and  proposes the idea of counterfactual invariance as a model requirement and training strategy for avoiding spurious correlations.

Counterfactual data augmentation was introduced as a method for controlling gender bias in datasets \cite{lu_gender_2020}. They hand-crafted a strategy for changing gender carrying terms in the datasets in such a way that the augmented dataset would have less gender bias. This idea has been built upon in the literature. \citeauthor{zeng_counterfactual_2020} created counterfactuals by swapping named entities in a training dataset, and we should note our method of shuffling rationales between documents is similar counterfactual generation process. \citeauthor{kaushik_learning_2020} used human annotators to generate counterfactual datasets and showed that this can help downstream models generalize out-of-domain and \citeauthor{deng_counterfactual_2023} adapted this strategy to active learning. \citeauthor{li_prompting_2024} explored prompting LLMs for counterfactual generation.