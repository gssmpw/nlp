
@inproceedings{yu_rethinking_2019,
	address = {Hong Kong, China},
	title = {Rethinking {Cooperative} {Rationalization}: {Introspective} {Extraction} and {Complement} {Control}},
	shorttitle = {Rethinking {Cooperative} {Rationalization}},
	url = {https://www.aclweb.org/anthology/D19-1420},
	doi = {10.18653/v1/D19-1420},
	language = {en},
	urldate = {2020-05-12},
	booktitle = {Proceedings of the 2019 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} and the 9th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({EMNLP}-{IJCNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Yu, Mo and Chang, Shiyu and Zhang, Yang and Jaakkola, Tommi},
	year = {2019},
	pages = {4092--4101},
	file = {Yu et al. - 2019 - Rethinking Cooperative Rationalization Introspect.pdf:/home/mitchell/Zotero/storage/243UUCFJ/Yu et al. - 2019 - Rethinking Cooperative Rationalization Introspect.pdf:application/pdf},
}

@inproceedings{lei_rationalizing_2016,
	address = {Austin, Texas},
	title = {Rationalizing {Neural} {Predictions}},
	url = {http://aclweb.org/anthology/D16-1011},
	doi = {10.18653/v1/D16-1011},
	language = {en},
	urldate = {2020-05-12},
	booktitle = {Proceedings of the 2016 {Conference} on {Empirical} {Methods} in {Natural}           {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Lei, Tao and Barzilay, Regina and Jaakkola, Tommi},
	year = {2016},
	pages = {107--117},
	file = {Lei et al. - 2016 - Rationalizing Neural Predictions.pdf:/home/mitchell/Zotero/storage/M7X9PB7G/Lei et al. - 2016 - Rationalizing Neural Predictions.pdf:application/pdf},
}

@article{jang_categorical_2017,
	title = {Categorical {Reparameterization} with {Gumbel}-{Softmax}},
	url = {http://arxiv.org/abs/1611.01144},
	abstract = {Categorical variables are a natural choice for representing discrete structure in the world. However, stochastic neural networks rarely use categorical latent variables due to the inability to backpropagate through samples. In this work, we present an efficient gradient estimator that replaces the non-differentiable sample from a categorical distribution with a differentiable sample from a novel Gumbel-Softmax distribution. This distribution has the essential property that it can be smoothly annealed into a categorical distribution. We show that our Gumbel-Softmax estimator outperforms state-of-the-art gradient estimators on structured output prediction and unsupervised generative modeling tasks with categorical latent variables, and enables large speedups on semi-supervised classification.},
	urldate = {2020-05-13},
	journal = {arXiv:1611.01144 [cs, stat]},
	author = {Jang, Eric and Gu, Shixiang and Poole, Ben},
	month = aug,
	year = {2017},
	note = {arXiv: 1611.01144},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/mitchell/Zotero/storage/WRAJRUE8/Jang et al. - 2017 - Categorical Reparameterization with Gumbel-Softmax.pdf:application/pdf;arXiv.org Snapshot:/home/mitchell/Zotero/storage/C6Q8HPTE/1611.html:text/html},
}

@article{bengio_estimating_2013,
	title = {Estimating or {Propagating} {Gradients} {Through} {Stochastic} {Neurons} for {Conditional} {Computation}},
	url = {http://arxiv.org/abs/1308.3432},
	abstract = {Stochastic neurons and hard non-linearities can be useful for a number of reasons in deep learning models, but in many cases they pose a challenging problem: how to estimate the gradient of a loss function with respect to the input of such stochastic or non-smooth neurons? I.e., can we "back-propagate" through these stochastic neurons? We examine this question, existing approaches, and compare four families of solutions, applicable in different settings. One of them is the minimum variance unbiased gradient estimator for stochatic binary neurons (a special case of the REINFORCE algorithm). A second approach, introduced here, decomposes the operation of a binary stochastic neuron into a stochastic binary part and a smooth differentiable part, which approximates the expected effect of the pure stochatic binary neuron to first order. A third approach involves the injection of additive or multiplicative noise in a computational graph that is otherwise differentiable. A fourth approach heuristically copies the gradient with respect to the stochastic output directly as an estimator of the gradient with respect to the sigmoid argument (we call this the straight-through estimator). To explore a context where these estimators are useful, we consider a small-scale version of \{{\textbackslash}em conditional computation\}, where sparse stochastic units form a distributed representation of gaters that can turn off in combinatorially many ways large chunks of the computation performed in the rest of the neural network. In this case, it is important that the gating units produce an actual 0 most of the time. The resulting sparsity can be potentially be exploited to greatly reduce the computational cost of large deep networks for which conditional computation would be useful.},
	urldate = {2020-05-13},
	journal = {arXiv:1308.3432 [cs]},
	author = {Bengio, Yoshua and Léonard, Nicholas and Courville, Aaron},
	month = aug,
	year = {2013},
	note = {arXiv: 1308.3432},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/mitchell/Zotero/storage/N4HGGWRC/Bengio et al. - 2013 - Estimating or Propagating Gradients Through Stocha.pdf:application/pdf;arXiv.org Snapshot:/home/mitchell/Zotero/storage/EDXSDFES/1308.html:text/html},
}

@article{mcauley_learning_2012,
	title = {Learning {Attitudes} and {Attributes} from {Multi}-{Aspect} {Reviews}},
	url = {http://arxiv.org/abs/1210.3926},
	abstract = {The majority of online reviews consist of plain-text feedback together with a single numeric score. However, there are multiple dimensions to products and opinions, and understanding the `aspects' that contribute to users' ratings may help us to better understand their individual preferences. For example, a user's impression of an audiobook presumably depends on aspects such as the story and the narrator, and knowing their opinions on these aspects may help us to recommend better products. In this paper, we build models for rating systems in which such dimensions are explicit, in the sense that users leave separate ratings for each aspect of a product. By introducing new corpora consisting of five million reviews, rated with between three and six aspects, we evaluate our models on three prediction tasks: First, we use our model to uncover which parts of a review discuss which of the rated aspects. Second, we use our model to summarize reviews, which for us means finding the sentences that best explain a user's rating. Finally, since aspect ratings are optional in many of the datasets we consider, we use our model to recover those ratings that are missing from a user's evaluation. Our model matches state-of-the-art approaches on existing small-scale datasets, while scaling to the real-world datasets we introduce. Moreover, our model is able to `disentangle' content and sentiment words: we automatically learn content words that are indicative of a particular aspect as well as the aspect-specific sentiment words that are indicative of a particular rating.},
	urldate = {2020-07-20},
	journal = {arXiv:1210.3926 [cs]},
	author = {McAuley, Julian and Leskovec, Jure and Jurafsky, Dan},
	month = oct,
	year = {2012},
	note = {arXiv: 1210.3926},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Information Retrieval},
	file = {arXiv Fulltext PDF:/home/mitchell/Zotero/storage/5SNUM834/McAuley et al. - 2012 - Learning Attitudes and Attributes from Multi-Aspec.pdf:application/pdf;arXiv.org Snapshot:/home/mitchell/Zotero/storage/QHEHM685/1210.html:text/html},
}

@article{chang_invariant_2020,
	title = {Invariant {Rationalization}},
	url = {http://arxiv.org/abs/2003.09772},
	abstract = {Selective rationalization improves neural network interpretability by identifying a small subset of input features -- the rationale -- that best explains or supports the prediction. A typical rationalization criterion, i.e. maximum mutual information (MMI), finds the rationale that maximizes the prediction performance based only on the rationale. However, MMI can be problematic because it picks up spurious correlations between the input features and the output. Instead, we introduce a game-theoretic invariant rationalization criterion where the rationales are constrained to enable the same predictor to be optimal across different environments. We show both theoretically and empirically that the proposed rationales can rule out spurious correlations, generalize better to different test scenarios, and align better with human judgments. Our data and code are available.},
	urldate = {2020-08-18},
	journal = {arXiv:2003.09772 [cs, stat]},
	author = {Chang, Shiyu and Zhang, Yang and Yu, Mo and Jaakkola, Tommi S.},
	month = mar,
	year = {2020},
	note = {arXiv: 2003.09772},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/mitchell/Zotero/storage/5MH8F7X6/Chang et al. - 2020 - Invariant Rationalization.pdf:application/pdf;arXiv.org Snapshot:/home/mitchell/Zotero/storage/P3NB7YPB/2003.html:text/html},
}

@article{bao_deriving_2018,
	title = {Deriving {Machine} {Attention} from {Human} {Rationales}},
	url = {http://arxiv.org/abs/1808.09367},
	abstract = {Attention-based models are successful when trained on large amounts of data. In this paper, we demonstrate that even in the low-resource scenario, attention can be learned effectively. To this end, we start with discrete human-annotated rationales and map them into continuous attention. Our central hypothesis is that this mapping is general across domains, and thus can be transferred from resource-rich domains to low-resource ones. Our model jointly learns a domain-invariant representation and induces the desired mapping between rationales and attention. Our empirical results validate this hypothesis and show that our approach delivers significant gains over state-of-the-art baselines, yielding over 15\% average error reduction on benchmark datasets.},
	urldate = {2020-09-03},
	journal = {arXiv:1808.09367 [cs]},
	author = {Bao, Yujia and Chang, Shiyu and Yu, Mo and Barzilay, Regina},
	month = aug,
	year = {2018},
	note = {arXiv: 1808.09367},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/home/mitchell/Zotero/storage/PSRJ6CG3/Bao et al. - 2018 - Deriving Machine Attention from Human Rationales.pdf:application/pdf;arXiv.org Snapshot:/home/mitchell/Zotero/storage/AJUDU2MV/1808.html:text/html},
}

@article{chen_learning_2018,
	title = {Learning to {Explain}: {An} {Information}-{Theoretic} {Perspective} on {Model} {Interpretation}},
	shorttitle = {Learning to {Explain}},
	url = {http://arxiv.org/abs/1802.07814},
	abstract = {We introduce instancewise feature selection as a methodology for model interpretation. Our method is based on learning a function to extract a subset of features that are most informative for each given example. This feature selector is trained to maximize the mutual information between selected features and the response variable, where the conditional distribution of the response variable given the input is the model to be explained. We develop an efficient variational approximation to the mutual information, and show the effectiveness of our method on a variety of synthetic and real data sets using both quantitative metrics and human evaluation.},
	urldate = {2020-09-05},
	journal = {arXiv:1802.07814 [cs, stat]},
	author = {Chen, Jianbo and Song, Le and Wainwright, Martin J. and Jordan, Michael I.},
	month = jun,
	year = {2018},
	note = {arXiv: 1802.07814},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/mitchell/Zotero/storage/PZJMBJ3Q/Chen et al. - 2018 - Learning to Explain An Information-Theoretic Pers.pdf:application/pdf;arXiv.org Snapshot:/home/mitchell/Zotero/storage/XF2ZZ7LB/1802.html:text/html},
}
@inproceedings{wang_latent_nodate,
author = {Wang, Hongning and Lu, Yue and Zhai, Chengxiang},
title = {Latent aspect rating analysis on review text data: a rating regression approach},
year = {2010},
isbn = {9781450300551},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1835804.1835903},
doi = {10.1145/1835804.1835903},
abstract = {In this paper, we define and study a new opinionated text data analysis problem called Latent Aspect Rating Analysis (LARA), which aims at analyzing opinions expressed about an entity in an online review at the level of topical aspects to discover each individual reviewer's latent opinion on each aspect as well as the relative emphasis on different aspects when forming the overall judgment of the entity. We propose a novel probabilistic rating regression model to solve this new text mining problem in a general way. Empirical experiments on a hotel review data set show that the proposed latent rating regression model can effectively solve the problem of LARA, and that the detailed analysis of opinions at the level of topical aspects enabled by the proposed model can support a wide range of application tasks, such as aspect opinion summarization, entity ranking based on aspect ratings, and analysis of reviewers rating behavior.},
booktitle = {Proceedings of the 16th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {783–792},
numpages = {10},
keywords = {experimentation, algorithms},
location = {Washington, DC, USA},
series = {KDD '10}
}


@article{williams_simple_2004,
	title = {Simple statistical gradient-following algorithms for connectionist reinforcement learning},
	volume = {8},
	journal = {Machine Learning},
	author = {Williams, R. J.},
	year = {2004},
	pages = {229--256},
}

@inproceedings{zeng_counterfactual_2020,
	address = {Online},
	title = {Counterfactual {Generator}: {A} {Weakly}-{Supervised} {Method} for {Named} {Entity} {Recognition}},
	shorttitle = {Counterfactual {Generator}},
	url = {https://www.aclweb.org/anthology/2020.emnlp-main.590},
	abstract = {Past progress on neural models has proven that named entity recognition is no longer a problem if we have enough labeled data. However, collecting enough data and annotating them are labor-intensive, time-consuming, and expensive. In this paper, we decompose the sentence into two parts: entity and context, and rethink the relationship between them and model performance from a causal perspective. Based on this, we propose the Counterfactual Generator, which generates counterfactual examples by the interventions on the existing observational examples to enhance the original dataset. Experiments across three datasets show that our method improves the generalization ability of models under limited observational examples. Besides, we provide a theoretical foundation by using a structural causal model to explore the spurious correlations between input features and output labels. We investigate the causal effects of entity or context on model performance under both conditions: the non-augmented and the augmented. Interestingly, we find that the non-spurious correlations are more located in entity representation rather than context representation. As a result, our method eliminates part of the spurious correlations between context representation and output labels. The code is available at https://github.com/xijiz/cfgen.},
	urldate = {2020-11-30},
	booktitle = {Proceedings of the 2020 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Zeng, Xiangji and Li, Yunliang and Zhai, Yuchen and Zhang, Yin},
	month = nov,
	year = {2020},
	pages = {7270--7280},
	file = {Full Text PDF:/home/mitchell/Zotero/storage/SKKINFZT/Zeng et al. - 2020 - Counterfactual Generator A Weakly-Supervised Meth.pdf:application/pdf},
}

@article{cover_elements_nodate,
	title = {{ELEMENTS} {OF} {INFORMATION} {THEORY}},
	language = {en},
	author = {Cover, Thomas M and Thomas, Joy A},
	pages = {774},
	file = {Cover and Thomas - ELEMENTS OF INFORMATION THEORY.pdf:/home/mitchell/Zotero/storage/27RJ5ECJ/Cover and Thomas - ELEMENTS OF INFORMATION THEORY.pdf:application/pdf},
}

@inproceedings{devlin_bert_2019,
	address = {Minneapolis, Minnesota},
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	url = {https://www.aclweb.org/anthology/N19-1423},
	doi = {10.18653/v1/N19-1423},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	urldate = {2021-01-27},
	booktitle = {Proceedings of the 2019 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {Volume} 1 ({Long} and {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = jun,
	year = {2019},
	pages = {4171--4186},
	file = {Full Text PDF:/home/mitchell/Zotero/storage/HJJI2JZG/Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf:application/pdf},
}

@article{chang_game_2019,
	title = {A {Game} {Theoretic} {Approach} to {Class}-wise {Selective} {Rationalization}},
	url = {http://arxiv.org/abs/1910.12853},
	abstract = {Selection of input features such as relevant pieces of text has become a common technique of highlighting how complex neural predictors operate. The selection can be optimized post-hoc for trained models or incorporated directly into the method itself (self-explaining). However, an overall selection does not properly capture the multi-faceted nature of useful rationales such as pros and cons for decisions. To this end, we propose a new game theoretic approach to class-dependent rationalization, where the method is speciﬁcally trained to highlight evidence supporting alternative conclusions. Each class involves three players set up competitively to ﬁnd evidence for factual and counterfactual scenarios. We show theoretically in a simpliﬁed scenario how the game drives the solution towards meaningful class-dependent rationales. We evaluate the method in single- and multi-aspect sentiment classiﬁcation tasks and demonstrate that the proposed method is able to identify both factual (justifying the ground truth label) and counterfactual (countering the ground truth label) rationales consistent with human rationalization. The code for our method is publicly available2.},
	language = {en},
	urldate = {2021-04-20},
	journal = {arXiv:1910.12853 [cs, stat]},
	author = {Chang, Shiyu and Zhang, Yang and Yu, Mo and Jaakkola, Tommi S.},
	month = oct,
	year = {2019},
	note = {arXiv: 1910.12853},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Chang et al. - 2019 - A Game Theoretic Approach to Class-wise Selective .pdf:/home/mitchell/Zotero/storage/EAGY6XET/Chang et al. - 2019 - A Game Theoretic Approach to Class-wise Selective .pdf:application/pdf},
}

@article{kaushik_learning_2020,
	title = {{LEARNING} {THE} {DIFFERENCE} {THAT} {MAKES} {A} {DIFFER}- {ENCE} {WITH} {COUNTERFACTUALLY}-{AUGMENTED} {DATA}},
	abstract = {Despite alarm over the reliance of machine learning systems on so-called spurious patterns, the term lacks coherent meaning in standard statistical frameworks. However, the language of causality offers clarity: spurious associations are due to confounding (e.g., a common cause), but not direct or indirect causal effects. In this paper, we focus on natural language processing, introducing methods and resources for training models less sensitive to spurious patterns. Given documents and their initial labels, we task humans with revising each document so that it (i) accords with a counterfactual target label; (ii) retains internal coherence; and (iii) avoids unnecessary changes. Interestingly, on sentiment analysis and natural language inference tasks, classiﬁers trained on original data fail on their counterfactually-revised counterparts and vice versa. Classiﬁers trained on combined datasets perform remarkably well, just shy of those specialized to either domain. While classiﬁers trained on either original or manipulated data alone are sensitive to spurious features (e.g., mentions of genre), models trained on the combined data are less sensitive to this signal. Both datasets are publicly available1.},
	language = {en},
	author = {Kaushik, Divyansh and Hovy, Eduard and Lipton, Zachary C},
	year = {2020},
	pages = {17},
	file = {Kaushik et al. - 2020 - LEARNING THE DIFFERENCE THAT MAKES A DIFFER- ENCE .pdf:/home/mitchell/Zotero/storage/AEHGPCM7/Kaushik et al. - 2020 - LEARNING THE DIFFERENCE THAT MAKES A DIFFER- ENCE .pdf:application/pdf},
}

@incollection{lu_gender_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Gender {Bias} in {Neural} {Natural} {Language} {Processing}},
	isbn = {978-3-030-62077-6},
	url = {https://doi.org/10.1007/978-3-030-62077-6_14},
	abstract = {We examine whether neural natural language processing (NLP) systems reflect historical biases in training data. We define a general benchmark to quantify gender bias in a variety of neural NLP tasks. Our empirical evaluation with state-of-the-art neural coreference resolution and textbook RNN-based language models trained on benchmark data sets finds significant gender bias in how models view occupations. We then mitigate bias with counterfactual data augmentation (CDA): a generic methodology for corpus augmentation via causal interventions that breaks associations between gendered and gender-neutral words. We empirically show that CDA effectively decreases gender bias while preserving accuracy. We also explore the space of mitigation strategies with CDA, a prior approach to word embedding debiasing (WED), and their compositions. We show that CDA outperforms WED, drastically so when word embeddings are trained. For pre-trained embeddings, the two methods can be effectively composed. We also find that as training proceeds on the original data set with gradient descent the gender bias grows as the loss reduces, indicating that the optimization encourages bias; CDA mitigates this behavior.},
	language = {en},
	urldate = {2021-10-24},
	booktitle = {Logic, {Language}, and {Security}: {Essays} {Dedicated} to {Andre} {Scedrov} on the {Occasion} of {His} 65th {Birthday}},
	publisher = {Springer International Publishing},
	author = {Lu, Kaiji and Mardziel, Piotr and Wu, Fangjing and Amancharla, Preetam and Datta, Anupam},
	editor = {Nigam, Vivek and Ban Kirigin, Tajana and Talcott, Carolyn and Guttman, Joshua and Kuznetsov, Stepan and Thau Loo, Boon and Okada, Mitsuhiro},
	year = {2020},
	doi = {10.1007/978-3-030-62077-6_14},
	keywords = {Deep learning, Fairness, Machine learning, Natural language processing},
	pages = {189--202},
	file = {Submitted Version:/home/mitchell/Zotero/storage/AQAC8TNZ/Lu et al. - 2020 - Gender Bias in Neural Natural Language Processing.pdf:application/pdf},
}

@article{veitch_counterfactual_2021,
	title = {Counterfactual {Invariance} to {Spurious} {Correlations}: {Why} and {How} to {Pass} {Stress} {Tests}},
	shorttitle = {Counterfactual {Invariance} to {Spurious} {Correlations}},
	url = {http://arxiv.org/abs/2106.00545},
	abstract = {Informally, a 'spurious correlation' is the dependence of a model on some aspect of the input data that an analyst thinks shouldn't matter. In machine learning, these have a know-it-when-you-see-it character; e.g., changing the gender of a sentence's subject changes a sentiment predictor's output. To check for spurious correlations, we can 'stress test' models by perturbing irrelevant parts of input data and seeing if model predictions change. In this paper, we study stress testing using the tools of causal inference. We introduce counterfactual invariance as a formalization of the requirement that changing irrelevant parts of the input shouldn't change model predictions. We connect counterfactual invariance to out-of-domain model performance, and provide practical schemes for learning (approximately) counterfactual invariant predictors (without access to counterfactual examples). It turns out that both the means and implications of counterfactual invariance depend fundamentally on the true underlying causal structure of the data -- in particular, whether the label causes the features or the features cause the label. Distinct causal structures require distinct regularization schemes to induce counterfactual invariance. Similarly, counterfactual invariance implies different domain shift guarantees depending on the underlying causal structure. This theory is supported by empirical results on text classification.},
	urldate = {2021-11-05},
	journal = {arXiv:2106.00545 [cs, stat]},
	author = {Veitch, Victor and D'Amour, Alexander and Yadlowsky, Steve and Eisenstein, Jacob},
	month = nov,
	year = {2021},
	note = {arXiv: 2106.00545},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/home/mitchell/Zotero/storage/7GQU6JID/2106.html:text/html;Veitch et al_2021_Counterfactual Invariance to Spurious Correlations.pdf:/home/mitchell/Zotero/storage/3WBC2K7E/Veitch et al_2021_Counterfactual Invariance to Spurious Correlations.pdf:application/pdf},
}

@article{yang_exploring_2021,
	title = {Exploring the {Efficacy} of {Automatically} {Generated} {Counterfactuals} for {Sentiment} {Analysis}},
	url = {http://arxiv.org/abs/2106.15231},
	abstract = {While state-of-the-art NLP models have been achieving the excellent performance of a wide range of tasks in recent years, important questions are being raised about their robustness and their underlying sensitivity to systematic biases that may exist in their training and test data. Such issues come to be manifest in performance problems when faced with out-of-distribution data in the field. One recent solution has been to use counterfactually augmented datasets in order to reduce any reliance on spurious patterns that may exist in the original data. Producing high-quality augmented data can be costly and time-consuming as it usually needs to involve human feedback and crowdsourcing efforts. In this work, we propose an alternative by describing and evaluating an approach to automatically generating counterfactual data for data augmentation and explanation. A comprehensive evaluation on several different datasets and using a variety of state-of-the-art benchmarks demonstrate how our approach can achieve significant improvements in model performance when compared to models training on the original data and even when compared to models trained with the benefit of human-generated augmented data.},
	urldate = {2022-01-12},
	journal = {arXiv:2106.15231 [cs]},
	author = {Yang, Linyi and Li, Jiazheng and Cunningham, Pádraig and Zhang, Yue and Smyth, Barry and Dong, Ruihai},
	month = jun,
	year = {2021},
	note = {arXiv: 2106.15231},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, \_tablet, Computer Science - Logic in Computer Science},
	file = {arXiv.org Snapshot:/home/mitchell/Zotero/storage/39MDAY92/2106.html:text/html;Yang et al_2021_Exploring the Efficacy of Automatically Generated Counterfactuals for Sentiment.pdf:/home/mitchell/Zotero/storage/E4F32RH4/Yang et al_2021_Exploring the Efficacy of Automatically Generated Counterfactuals for Sentiment.pdf:application/pdf},
}

@misc{pappagari_hierarchical_2019,
	title = {Hierarchical {Transformers} for {Long} {Document} {Classification}},
	url = {http://arxiv.org/abs/1910.10781},
	abstract = {BERT, which stands for Bidirectional Encoder Representations from Transformers, is a recently introduced language representation model based upon the transfer learning paradigm. We extend its fine-tuning procedure to address one of its major limitations - applicability to inputs longer than a few hundred words, such as transcripts of human call conversations. Our method is conceptually simple. We segment the input into smaller chunks and feed each of them into the base model. Then, we propagate each output through a single recurrent layer, or another transformer, followed by a softmax activation. We obtain the final classification decision after the last segment has been consumed. We show that both BERT extensions are quick to fine-tune and converge after as little as 1 epoch of training on a small, domain-specific data set. We successfully apply them in three different tasks involving customer call satisfaction prediction and topic classification, and obtain a significant improvement over the baseline models in two of them.},
	urldate = {2023-01-23},
	publisher = {arXiv},
	author = {Pappagari, Raghavendra and Żelasko, Piotr and Villalba, Jesús and Carmiel, Yishay and Dehak, Najim},
	month = oct,
	year = {2019},
	note = {arXiv:1910.10781 [cs, stat]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/home/mitchell/Zotero/storage/5WVHPUA9/1910.html:text/html;Pappagari et al_2019_Hierarchical Transformers for Long Document Classification.pdf:/home/mitchell/Zotero/storage/AUXY5NXG/Pappagari et al_2019_Hierarchical Transformers for Long Document Classification.pdf:application/pdf},
}

@inproceedings{plyler_making_2021,
	title = {Making a ({Counterfactual}) {Difference} {One} {Rationale} at a {Time}},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper_files/paper/2021/hash/f0f800c92d191d736c4411f3b3f8ef4a-Abstract.html},
	abstract = {Rationales, snippets of extracted text that explain an inference, have emerged as a popular framework for interpretable natural language processing (NLP). Rationale models typically consist of two cooperating modules: a selector and a classifier with the goal of maximizing the mutual information (MMI) between the "selected" text and the document label. Despite their promises, MMI-based methods often pick up on spurious text patterns and result in models with nonsensical behaviors. In this work, we investigate whether counterfactual data augmentation (CDA), without human assistance, can improve the performance of the selector by lowering the mutual information between spurious signals and the document label. Our counterfactuals are produced in an unsupervised fashion using class-dependent generative models. From an information theoretic lens, we derive properties of the unaugmented dataset for which our CDA approach would succeed. The effectiveness of CDA is empirically evaluated by comparing against several baselines including an improved MMI-based rationale schema on two multi-aspect datasets. Our results show that CDA produces rationales that better capture the signal of interest.},
	urldate = {2024-02-07},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Plyler, Mitchell and Green, Michael and Chi, Min},
	year = {2021},
	pages = {28701--28713},
	file = {Plyler et al_2021_Making a (Counterfactual) Difference One Rationale at a Time.pdf:/home/mitchell/Zotero/storage/8ZUXEV33/Plyler et al_2021_Making a (Counterfactual) Difference One Rationale at a Time.pdf:application/pdf},
}

@misc{loshchilov_decoupled_2019,
	title = {Decoupled {Weight} {Decay} {Regularization}},
	url = {http://arxiv.org/abs/1711.05101},
	abstract = {L\$\_2\$ regularization and weight decay regularization are equivalent for standard stochastic gradient descent (when rescaled by the learning rate), but as we demonstrate this is {\textbackslash}emph\{not\} the case for adaptive gradient algorithms, such as Adam. While common implementations of these algorithms employ L\$\_2\$ regularization (often calling it "weight decay" in what may be misleading due to the inequivalence we expose), we propose a simple modification to recover the original formulation of weight decay regularization by {\textbackslash}emph\{decoupling\} the weight decay from the optimization steps taken w.r.t. the loss function. We provide empirical evidence that our proposed modification (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam and (ii) substantially improves Adam's generalization performance, allowing it to compete with SGD with momentum on image classification datasets (on which it was previously typically outperformed by the latter). Our proposed decoupled weight decay has already been adopted by many researchers, and the community has implemented it in TensorFlow and PyTorch; the complete source code for our experiments is available at https://github.com/loshchil/AdamW-and-SGDW},
	language = {en},
	urldate = {2024-05-14},
	publisher = {arXiv},
	author = {Loshchilov, Ilya and Hutter, Frank},
	month = jan,
	year = {2019},
	note = {arXiv:1711.05101 [cs, math]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Mathematics - Optimization and Control},
	file = {Loshchilov and Hutter - 2019 - Decoupled Weight Decay Regularization.pdf:/home/mitchell/Zotero/storage/UZH3IKBR/Loshchilov and Hutter - 2019 - Decoupled Weight Decay Regularization.pdf:application/pdf},
}

@misc{touvron_llama_2023,
	title = {Llama 2: {Open} {Foundation} and {Fine}-{Tuned} {Chat} {Models}},
	shorttitle = {Llama 2},
	url = {http://arxiv.org/abs/2307.09288},
	abstract = {In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closedsource models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.},
	language = {en},
	urldate = {2024-05-14},
	publisher = {arXiv},
	author = {Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and Bikel, Dan and Blecher, Lukas and Ferrer, Cristian Canton and Chen, Moya and Cucurull, Guillem and Esiobu, David and Fernandes, Jude and Fu, Jeremy and Fu, Wenyin and Fuller, Brian and Gao, Cynthia and Goswami, Vedanuj and Goyal, Naman and Hartshorn, Anthony and Hosseini, Saghar and Hou, Rui and Inan, Hakan and Kardas, Marcin and Kerkez, Viktor and Khabsa, Madian and Kloumann, Isabel and Korenev, Artem and Koura, Punit Singh and Lachaux, Marie-Anne and Lavril, Thibaut and Lee, Jenya and Liskovich, Diana and Lu, Yinghai and Mao, Yuning and Martinet, Xavier and Mihaylov, Todor and Mishra, Pushkar and Molybog, Igor and Nie, Yixin and Poulton, Andrew and Reizenstein, Jeremy and Rungta, Rashi and Saladi, Kalyan and Schelten, Alan and Silva, Ruan and Smith, Eric Michael and Subramanian, Ranjan and Tan, Xiaoqing Ellen and Tang, Binh and Taylor, Ross and Williams, Adina and Kuan, Jian Xiang and Xu, Puxin and Yan, Zheng and Zarov, Iliyan and Zhang, Yuchen and Fan, Angela and Kambadur, Melanie and Narang, Sharan and Rodriguez, Aurelien and Stojnic, Robert and Edunov, Sergey and Scialom, Thomas},
	month = jul,
	year = {2023},
	note = {arXiv:2307.09288 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {Touvron et al. - 2023 - Llama 2 Open Foundation and Fine-Tuned Chat Model.pdf:/home/mitchell/Zotero/storage/M5ZJ38UA/Touvron et al. - 2023 - Llama 2 Open Foundation and Fine-Tuned Chat Model.pdf:application/pdf},
}

@misc{li_prompting_2024,
	title = {Prompting {Large} {Language} {Models} for {Counterfactual} {Generation}: {An} {Empirical} {Study}},
	shorttitle = {Prompting {Large} {Language} {Models} for {Counterfactual} {Generation}},
	url = {http://arxiv.org/abs/2305.14791},
	abstract = {Large language models (LLMs) have made remarkable progress in a wide range of natural language understanding and generation tasks. However, their ability to generate counterfactuals has not been examined systematically. To bridge this gap, we present a comprehensive evaluation framework on various types of NLU tasks, which covers all key factors in determining LLMs’ capability of generating counterfactuals. Based on this framework, we 1) investigate the strengths and weaknesses of LLMs as the counterfactual generator, and 2) disclose the factors that affect LLMs when generating counterfactuals, including both the intrinsic properties of LLMs and prompt designing. The results show that, though LLMs are promising in most cases, they face challenges in complex tasks like RE since they are bounded by task-specific performance, entity constraints, and inherent selection bias. We also find that alignment techniques, e.g., instruction-tuning and reinforcement learning from human feedback, may potentially enhance the counterfactual generation ability of LLMs. On the contrary, simply increasing the parameter size does not yield the desired improvements. Besides, from the perspective of prompt designing, task guidelines unsurprisingly play an important role. However, the chain-of-thought approach does not always help due to inconsistency issues.},
	language = {en},
	urldate = {2024-05-14},
	publisher = {arXiv},
	author = {Li, Yongqi and Xu, Mayi and Miao, Xin and Zhou, Shen and Qian, Tieyun},
	month = feb,
	year = {2024},
	note = {arXiv:2305.14791 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Li et al. - 2024 - Prompting Large Language Models for Counterfactual.pdf:/home/mitchell/Zotero/storage/DV258I72/Li et al. - 2024 - Prompting Large Language Models for Counterfactual.pdf:application/pdf},
}

@inproceedings{deng_counterfactual_2023,
	address = {Toronto, Canada},
	title = {Counterfactual {Active} {Learning} for {Out}-of-{Distribution} {Generalization}},
	url = {https://aclanthology.org/2023.acl-long.636},
	doi = {10.18653/v1/2023.acl-long.636},
	abstract = {We study the out-of-distribution generalization of active learning that adaptively selects samples for annotation in learning the decision boundary of classification. Our empirical study finds that increasingly annotating seen samples may hardly benefit the generalization. To address the problem, we propose Counterfactual Active Learning (CounterAL) that empowers active learning with counterfactual thinking to bridge the seen samples with unseen cases. In addition to annotating factual samples, CounterAL requires annotators to answer counterfactual questions to construct counterfactual samples for training. To achieve CounterAL, we design a new acquisition strategy that selects the informative factual-counterfactual pairs for annotation; and a new training strategy that pushes the model update to focus on the discrepancy between factual and counterfactual samples. We evaluate CounterAL on multiple public datasets of sentiment analysis and natural language inference. The experiment results show that CounterAL requires fewer acquisition rounds and outperforms existing active learning methods by a large margin in OOD tests with comparable IID performance.},
	urldate = {2024-05-14},
	booktitle = {Proceedings of the 61st {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Deng, Xun and Wang, Wenjie and Feng, Fuli and Zhang, Hanwang and He, Xiangnan and Liao, Yong},
	editor = {Rogers, Anna and Boyd-Graber, Jordan and Okazaki, Naoaki},
	month = jul,
	year = {2023},
	pages = {11362--11377},
	file = {Deng et al_2023_Counterfactual Active Learning for Out-of-Distribution Generalization.pdf:/home/mitchell/Zotero/storage/FIGXLU34/Deng et al_2023_Counterfactual Active Learning for Out-of-Distribution Generalization.pdf:application/pdf},
}

@article{liu_d-separation_2023,
	title = {D-{Separation} for {Causal} {Self}-{Explanation}},
	abstract = {Rationalization is a self-explaining framework for NLP models. Conventional work typically uses the maximum mutual information (MMI) criterion to find the rationale that is most indicative of the target label. However, this criterion can be influenced by spurious features that correlate with the causal rationale or the target label. Instead of attempting to rectify the issues of the MMI criterion, we propose a novel criterion to uncover the causal rationale, termed the Minimum Conditional Dependence (MCD) criterion, which is grounded on our finding that the non-causal features and the target label are d-separated by the causal rationale. By minimizing the dependence between the unselected parts of the input and the target label conditioned on the selected rationale candidate, all the causes of the label are compelled to be selected. In this study, we employ a simple and practical measure of dependence, specifically the KL-divergence, to validate our proposed MCD criterion. Empirically, we demonstrate that MCD improves the F1 score by up to 13.7\% compared to previous state-of-the-art MMI-based methods. Our code is available at: https://github.com/jugechengzi/Rationalization-MCD.},
	language = {en},
	author = {Liu, Wei and Wang, Jun and Wang, Haozhao and Li, Ruixuan and Deng, Zhiying and Zhang, Yuankai and Qiu, Yang},
	year = {2023},
	file = {Liu et al. - D-Separation for Causal Self-Explanation.pdf:/home/mitchell/Zotero/storage/YVS4XPZT/Liu et al. - D-Separation for Causal Self-Explanation.pdf:application/pdf},
}

@inproceedings{zhang_towards_2023,
	title = {Towards {Trustworthy} {Explanation}: {On} {Causal} {Rationalization}},
	shorttitle = {Towards {Trustworthy} {Explanation}},
	url = {https://proceedings.mlr.press/v202/zhang23ap.html},
	abstract = {With recent advances in natural language processing, rationalization becomes an essential self-explaining diagram to disentangle the black box by selecting a subset of input texts to account for the major variation in prediction. Yet, existing association-based approaches on rationalization cannot identify true rationales when two or more snippets are highly inter-correlated and thus provide a similar contribution to prediction accuracy, so-called spuriousness. To address this limitation, we novelly leverage two causal desiderata, non-spuriousness and efficiency, into rationalization from the causal inference perspective. We formally define a series of probabilities of causation based on a newly proposed structural causal model of rationalization, with its theoretical identification established as the main component of learning necessary and sufficient rationales. The superior performance of the proposed causal rationalization is demonstrated on real-world review and medical datasets with extensive experiments compared to state-of-the-art methods.},
	language = {en},
	urldate = {2024-05-14},
	booktitle = {Proceedings of the 40th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Zhang, Wenbo and Wu, Tong and Wang, Yunlong and Cai, Yong and Cai, Hengrui},
	month = jul,
	year = {2023},
	note = {ISSN: 2640-3498},
	pages = {41715--41736},
	file = {Zhang et al_2023_Towards Trustworthy Explanation.pdf:/home/mitchell/Zotero/storage/4SHHE6S6/Zhang et al_2023_Towards Trustworthy Explanation.pdf:application/pdf},
}

@article{banach_sur_1922,
	title = {Sur les opérations dans les ensembles abstraits et leur application aux équations intégrales},
	volume = {3},
	issn = {0016-2736, 1730-6329},
	url = {https://www.impan.pl/pl/wydawnictwa/czasopisma-i-serie-wydawnicze/fundamenta-mathematicae/all/3/0/92453/sur-les-operations-dans-les-ensembles-abstraits-et-leur-application-aux-equations-integrales},
	doi = {10.4064/fm-3-1-133-181},
	abstract = {Le but de cette note est d'établir quelques théorèmes valables pour différents champs fonctionnels.},
	language = {pl},
	urldate = {2024-05-15},
	journal = {Fundamenta Mathematicae},
	author = {Banach, Stefan},
	year = {1922},
	note = {Publisher: Instytut Matematyczny Polskiej Akademii Nauk},
	pages = {133--181},
	file = {Banach_1922_Sur les opérations dans les ensembles abstraits et leur application aux.pdf:/home/mitchell/Zotero/storage/8HIRZZSE/Banach_1922_Sur les opérations dans les ensembles abstraits et leur application aux.pdf:application/pdf},
}

@inproceedings{sun_recovering_2021,
	title = {Recovering {Latent} {Causal} {Factor} for {Generalization} to {Distributional} {Shifts}},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper_files/paper/2021/hash/8c6744c9d42ec2cb9e8885b54ff744d0-Abstract.html},
	abstract = {Distributional shifts between training and target domains may degrade the prediction accuracy of learned models, mainly because these models often learn features that possess only correlation rather than causal relation with the output. Such a correlation, which is known as ``spurious correlation'' statistically, is domain-dependent hence may fail to generalize to unseen domains. To avoid such a spurious correlation, we propose {\textbackslash}textbf\{La\}tent {\textbackslash}textbf\{C\}ausal {\textbackslash}textbf\{I\}nvariance {\textbackslash}textbf\{M\}odels (LaCIM) that specifies the underlying causal structure of the data and the source of distributional shifts, guiding us to pursue only causal factor for prediction. Specifically, the LaCIM introduces a pair of correlated latent factors: (a) causal factor and (b) others, while the extent of this correlation is governed by a domain variable that characterizes the distributional shifts. On the basis of this, we prove that the distribution of observed variables conditioning on latent variables is shift-invariant. Equipped with such an invariance, we prove that the causal factor can be recovered without mixing information from others, which induces the ground-truth predicting mechanism. We propose a Variational-Bayesian-based method to learn this invariance for prediction. The utility of our approach is verified by improved generalization to distributional shifts on various real-world data. Our code is freely available at {\textbackslash}url\{https://github.com/wubotong/LaCIM\}.},
	urldate = {2024-11-07},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Sun, Xinwei and Wu, Botong and Zheng, Xiangyu and Liu, Chang and Chen, Wei and Qin, Tao and Liu, Tie-Yan},
	year = {2021},
	pages = {16846--16859},
	file = {Sun et al_2021_Recovering Latent Causal Factor for Generalization to Distributional Shifts.pdf:/home/mitchell/Zotero/storage/VHSJ4KAG/Sun et al_2021_Recovering Latent Causal Factor for Generalization to Distributional Shifts.pdf:application/pdf},
}