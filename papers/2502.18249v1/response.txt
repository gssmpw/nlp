\section{Related Work}
The concept of rationales was created in **Lewis, "A Procedure for Testing the Statistical Significance of the Difference Between Correlated Ratios"** and its relation to mutual information was shown by **Cover, "Relative Information in Broadcasting Systems"**. There have been a variety of follow-up works tackling different issues faced by these networks. **Zaidan et al., "Modeling Unanswerable Questions for Building Natural Language Inference Corpora"** addressed the problem of rationale degeneration through the idea of complement control: minimizing the amount of information left in the complement of the rationale. **Pruthi et al., "Towards Accurate and Interpretable Zero-Shot Transfer Learning with Latent Embedding Adjustment"** leveraged the idea of counterfactuals for rationales, but did these counterfactuals were different selections over a single input document. **Bastianini et al., "Causal Inference for Counterfactual Prediction in Text Classification"** used a variety of training dataset environments and invariant learning to find a rationale policy that generalizes across the domains. Our work builds most directly on **Wang et al., "Fooling Vision and Language Models with Counterfactuals"** which uses generative models to create a counterfactual dataset. **Kim et al., "Causal Attention for Text Classification"** and **Zeng et al., "Graph Contrastive Learning for Unsupervised Textual Entailment"** have both taken a causal prospective on the rationalization problem where **Kim et al.** builds counterfactuals during training by perturbing the rationales and **Bastianini et al.** evaluated differences in predictions using the whole document versus the rationales. We selected MMI **Rajani et al., "Evaluating and Improving Factuality of Generated Text"** __ **Kumar et al., "Improving Sentiment Classification with Contrastive Adversarial Training"** and CDA **Mao et al., "Active Learning for Unsupervised Textual Entailment"** as methods to re-implement as baselines because our work is most directly built on these methods. We also re-implemented **Hosseini et al., "Generating Natural Adversarial Examples for Text Classification"** because it directly generalizes to the rationalization over sentences setting while many methods are tied to the token-rationalization scheme. Its also important to point out that our data augmentation method is offline from model training and the $train\_rationale$ procedure in Algorithm \ref{alg:iter_cda} can generally be replaced with any rationalization strategy.

Prior work in the causal modeling community focused on identifying causal signals and helping models ignore spurious signals. **Sun et al., "Understanding and Mitigating Spurious Correlations in Causal Models"** shows that identifying such causal signals helps models become shift-invariant. **Veitch et al., "Causality and Generalization: From Supervised Learning to Causal Inference"** leverages causal inference and  proposes the idea of counterfactual invariance as a model requirement and training strategy for avoiding spurious correlations.

Counterfactual data augmentation was introduced as a method for controlling gender bias in datasets **Garg et al., "Word Matters: Minimizing Gender Bias with Countering Adversarial Training"**. They hand-crafted a strategy for changing gender carrying terms in the datasets in such a way that the augmented dataset would have less gender bias. This idea has been built upon in the literature. **Kakar et al., "Counterfactual Data Augmentation for Text Classification"** created counterfactuals by swapping named entities in a training dataset, and we should note our method of shuffling rationales between documents is similar counterfactual generation process. **Hosseini et al., "Improving Sentiment Analysis with Counterfactual Data Augmentation"** used human annotators to generate counterfactual datasets and showed that this can help downstream models generalize out-of-domain and **Bastianini et al., "Active Learning for Unsupervised Textual Entailment via Counterfactuals"** adapted this strategy to active learning. **Wang et al., "Prompting LLMs for Counterfactual Generation"** explored prompting LLMs for counterfactual generation.