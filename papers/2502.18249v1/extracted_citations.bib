@article{chen_learning_2018,
	title = {Learning to {Explain}: {An} {Information}-{Theoretic} {Perspective} on {Model} {Interpretation}},
	shorttitle = {Learning to {Explain}},
	url = {http://arxiv.org/abs/1802.07814},
	abstract = {We introduce instancewise feature selection as a methodology for model interpretation. Our method is based on learning a function to extract a subset of features that are most informative for each given example. This feature selector is trained to maximize the mutual information between selected features and the response variable, where the conditional distribution of the response variable given the input is the model to be explained. We develop an efficient variational approximation to the mutual information, and show the effectiveness of our method on a variety of synthetic and real data sets using both quantitative metrics and human evaluation.},
	urldate = {2020-09-05},
	journal = {arXiv:1802.07814 [cs, stat]},
	author = {Chen, Jianbo and Song, Le and Wainwright, Martin J. and Jordan, Michael I.},
	month = jun,
	year = {2018},
	note = {arXiv: 1802.07814},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: Accepted to ICML 2018 as a long oral},
	file = {arXiv Fulltext PDF:/Zotero/storage/PZJMBJ3Q/Chen et al. - 2018 - Learning to Explain An Information-Theoretic Pers.pdf:application/pdf;arXiv.org Snapshot:/Zotero/storage/XF2ZZ7LB/1802.html:text/html},
}

@inproceedings{lei_rationalizing_2016,
	address = {Austin, Texas},
	title = {Rationalizing {Neural} {Predictions}},
	url = {http://aclweb.org/anthology/D16-1011},
	doi = {10.18653/v1/D16-1011},
	language = {en},
	urldate = {2020-05-12},
	booktitle = {Proceedings of the 2016 {Conference} on {Empirical} {Methods} in {Natural}           {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Lei, Tao and Barzilay, Regina and Jaakkola, Tommi},
	year = {2016},
	pages = {107--117},
	file = {Lei et al. - 2016 - Rationalizing Neural Predictions.pdf:/Zotero/storage/M7X9PB7G/Lei et al. - 2016 - Rationalizing Neural Predictions.pdf:application/pdf},
}

@article{liu_d-separation_2023,
	title = {D-{Separation} for {Causal} {Self}-{Explanation}},
	abstract = {Rationalization is a self-explaining framework for NLP models. Conventional work typically uses the maximum mutual information (MMI) criterion to find the rationale that is most indicative of the target label. However, this criterion can be influenced by spurious features that correlate with the causal rationale or the target label. Instead of attempting to rectify the issues of the MMI criterion, we propose a novel criterion to uncover the causal rationale, termed the Minimum Conditional Dependence (MCD) criterion, which is grounded on our finding that the non-causal features and the target label are d-separated by the causal rationale. By minimizing the dependence between the unselected parts of the input and the target label conditioned on the selected rationale candidate, all the causes of the label are compelled to be selected. In this study, we employ a simple and practical measure of dependence, specifically the KL-divergence, to validate our proposed MCD criterion. Empirically, we demonstrate that MCD improves the F1 score by up to 13.7\% compared to previous state-of-the-art MMI-based methods. Our code is available at: https://github.com/jugechengzi/Rationalization-MCD.},
	language = {en},
	author = {Liu, Wei and Wang, Jun and Wang, Haozhao and Li, Ruixuan and Deng, Zhiying and Zhang, Yuankai and Qiu, Yang},
	year = {2023},
	file = {Liu et al. - D-Separation for Causal Self-Explanation.pdf:/Zotero/storage/YVS4XPZT/Liu et al. - D-Separation for Causal Self-Explanation.pdf:application/pdf},
}

@incollection{lu_gender_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Gender {Bias} in {Neural} {Natural} {Language} {Processing}},
	isbn = {978-3-030-62077-6},
	url = {https://doi.org/10.1007/978-3-030-62077-6_14},
	abstract = {We examine whether neural natural language processing (NLP) systems reflect historical biases in training data. We define a general benchmark to quantify gender bias in a variety of neural NLP tasks. Our empirical evaluation with state-of-the-art neural coreference resolution and textbook RNN-based language models trained on benchmark data sets finds significant gender bias in how models view occupations. We then mitigate bias with counterfactual data augmentation (CDA): a generic methodology for corpus augmentation via causal interventions that breaks associations between gendered and gender-neutral words. We empirically show that CDA effectively decreases gender bias while preserving accuracy. We also explore the space of mitigation strategies with CDA, a prior approach to word embedding debiasing (WED), and their compositions. We show that CDA outperforms WED, drastically so when word embeddings are trained. For pre-trained embeddings, the two methods can be effectively composed. We also find that as training proceeds on the original data set with gradient descent the gender bias grows as the loss reduces, indicating that the optimization encourages bias; CDA mitigates this behavior.},
	language = {en},
	urldate = {2021-10-24},
	booktitle = {Logic, {Language}, and {Security}: {Essays} {Dedicated} to {Andre} {Scedrov} on the {Occasion} of {His} 65th {Birthday}},
	publisher = {Springer International Publishing},
	author = {Lu, Kaiji and Mardziel, Piotr and Wu, Fangjing and Amancharla, Preetam and Datta, Anupam},
	editor = {Nigam, Vivek and Ban Kirigin, Tajana and Talcott, Carolyn and Guttman, Joshua and Kuznetsov, Stepan and Thau Loo, Boon and Okada, Mitsuhiro},
	year = {2020},
	doi = {10.1007/978-3-030-62077-6_14},
	keywords = {Deep learning, Fairness, Machine learning, Natural language processing},
	pages = {189--202},
	file = {Submitted Version:/Zotero/storage/AQAC8TNZ/Lu et al. - 2020 - Gender Bias in Neural Natural Language Processing.pdf:application/pdf},
}

@inproceedings{plyler_making_2021,
	title = {Making a ({Counterfactual}) {Difference} {One} {Rationale} at a {Time}},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper_files/paper/2021/hash/f0f800c92d191d736c4411f3b3f8ef4a-Abstract.html},
	abstract = {Rationales, snippets of extracted text that explain an inference, have emerged as a popular framework for interpretable natural language processing (NLP). Rationale models typically consist of two cooperating modules: a selector and a classifier with the goal of maximizing the mutual information (MMI) between the "selected" text and the document label. Despite their promises, MMI-based methods often pick up on spurious text patterns and result in models with nonsensical behaviors. In this work, we investigate whether counterfactual data augmentation (CDA), without human assistance, can improve the performance of the selector by lowering the mutual information between spurious signals and the document label. Our counterfactuals are produced in an unsupervised fashion using class-dependent generative models. From an information theoretic lens, we derive properties of the unaugmented dataset for which our CDA approach would succeed. The effectiveness of CDA is empirically evaluated by comparing against several baselines including an improved MMI-based rationale schema on two multi-aspect datasets. Our results show that CDA produces rationales that better capture the signal of interest.},
	urldate = {2024-02-07},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Plyler, Mitchell and Green, Michael and Chi, Min},
	year = {2021},
	pages = {28701--28713},
	file = {Plyler et al_2021_Making a (Counterfactual) Difference One Rationale at a Time.pdf:/Zotero/storage/8ZUXEV33/Plyler et al_2021_Making a (Counterfactual) Difference One Rationale at a Time.pdf:application/pdf},
}

@inproceedings{yu_rethinking_2019,
	address = {Hong Kong, China},
	title = {Rethinking {Cooperative} {Rationalization}: {Introspective} {Extraction} and {Complement} {Control}},
	shorttitle = {Rethinking {Cooperative} {Rationalization}},
	url = {https://www.aclweb.org/anthology/D19-1420},
	doi = {10.18653/v1/D19-1420},
	language = {en},
	urldate = {2020-05-12},
	booktitle = {Proceedings of the 2019 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} and the 9th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({EMNLP}-{IJCNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Yu, Mo and Chang, Shiyu and Zhang, Yang and Jaakkola, Tommi},
	year = {2019},
	pages = {4092--4101},
	file = {Yu et al. - 2019 - Rethinking Cooperative Rationalization Introspect.pdf:/Zotero/storage/243UUCFJ/Yu et al. - 2019 - Rethinking Cooperative Rationalization Introspect.pdf:application/pdf},
}

