\section{RL Framework and Domain Knowledge}

We start with formulating the intervention decision making as an RL problem, where we underscore the challenge in the multiple time scales.
% We start with introducing the important variables in each component, and then formulate the clinical problem as a multi-agent reinforcement learning (MARL) problem.
% \subsection{Variables}
% In our study, each user stays for 98 days with 196 decision times in total. We first introduce important variables we plan to collect or optimize in ADAPTS-HCT. The variables are organized for each of the three components with a summary provided in Table \ref{tab:variables}.
%
% AYA component has twice-daily observations for $t=1,\dots,196$ decision times. On each $t$, we observe three variables $R_{t}^{\AYA}, A_{t}^{\AYA}, B_{t}^{\AYA}$ where $R_{t}^{\AYA} \in \{0, 1\}$ with $R_{t}^{\AYA} = 1$ representing AYA's adherence to medication at the decision time $t$. $A_t^{\AYA} \in \{0, 1\}$ represents the digital intervention provided to AYA with $A_t^{\AYA} = 1$ representing sending a positive psychology message. $B_t^{\AYA} \in \sR$ is AYA's perceived digital intervention burden about digital interventions at the time $t$.
%
% The care partner component observes three daily variables $R_{d}^{\CARE}, A_{d}^{\CARE}, B_{d}^{\CARE}$ for time index $d = 1, \dots, 98$. Here $R_d^{\CARE} \in \sR$ is the \textbf{psychological distress level} of the care partner (a continuous variable) at the end of day $d$. The psychological distress is measured through daily self-report. $A_d^{\CARE} \in \{0, 1\}$ represents the digital intervention provided to the care partner with $A_d^{\CARE} = 1$ representing sending a positive psychology message. Similar to the AYA component, $B_d^{\CARE} \in \sR$ is the continuous variable representing the level of the care partner's perceived digital intervention burden on day $d$. A higher $B_d^{\CARE}$ represents a higher perceived burden.
%
% The relationship component encompasses two weekly variables $A_{w}^{\EDGE}$ and $Y_{w}^{\EDGE}$ for time index $w = 1, \dots, 14$. Here $A_{w}^{\EDGE} \in \{0, 1\}$ is thegame intervention representing whether the dyad is offered to play a game at the beginning of a week, and $Y_{w}^{\EDGE} \in \{0, 1\}$ indicates the relationship quality between AYA's and their care partners measured in the end of the week through self-report. An $Y_{w}^{\EDGE} = 1$ indicates good relationship.
%
%
\begin{table}[pht]
    \centering
    \caption{Summary of variables about each target component}
    \label{tab:variables}
    \begin{tabular}{c|c|c|c}
    \toprule
    Target & Variable & Type & Description \\
    \midrule
    AYA & \makecell{$R_{w,d,t}^{\AYA}$ \\ $A_{w,d,t}^{\AYA}$ \\ $B_{w,d,t}^{\AYA}$} & \makecell{binary \\ binary \\ continuous} & \makecell{Medication adherence at time $t$ on day $d$ in week $w$ \\ Intervention at time $t$ on day $d$ in week $w$ \\ App burden at time $t$ on day $d$ in week $w$} \\
    \midrule
    Care partner & \makecell{$Y_{w,d}^{\CARE}$ \\ $A_{w,d}^{\CARE}$ \\ $B_{w,d}^{\CARE}$} & \makecell{continuous \\ binary \\ continuous} & \makecell{Psychological distress on day $d$ in week $w$ \\ Intervention on day $d$ in week $w$ \\ App burden on day $d$ in week $w$} \\
    \midrule
    Relationship & \makecell{$Y_w^{\EDGE}$ \\ $A_w^{\EDGE}$} & \makecell{binary \\ binary} & \makecell{Relationship quality at the end of week $w$ \\ Game intervention at the beginning of week $w$} \\
    \bottomrule
    % \vspace{10mm}
    \end{tabular}
    \end{table}
%
% \vspace{40mm}
%
%
%\sam{prior sentence was very unclear} 
HCT treatment is followed by  an outpatient 14-weeks twice-daily medication regimen. Decision times within the 14 weeks are denoted by $(w, d, t)$ where $w \in \{1, \dots, 14\}$ is the week index, $d \in \{1, \dots, 7\}$ is the day index, and $t \in \{1, 2\}$ is the decision window within a day. 
% In Table \ref{tab:variables}, we summarize the information collected from each component. 

\textbf{Primary goal.} The primary goal is to make decisions at each decision time $t$ to maximize cumulative sum of medication adherence $\sum_{w=1}^{14}\sum_{d=1}^{7}\sum_{t=1}^{2}R_{w,d,t}^{\AYA}$, where $R_{w,d,t}^{\AYA}$ is medication adherence at window $t$ on day $d$ in week $w$. See Table \ref{tab:variables}, for selected information that will be collected on the dyad. 

% A key challenge is the multiple time scales in both the action space and the observation space, which is visualized in Fig. \ref{fig:rl_framework}. 
% Specifically, both the action space and the observation space are different at different time scales.

\textbf{Action space.} All actions are binary (deliver versus do not deliver intervention content); see Tables~\ref{tab:interventions},\ref{tab:variables}. When the current time $(d = 1, t = 1)$ is the first decision time on the first day of the week, the agent chooses a three-dimensional action corresponding to all three interventions components. If the current time is the first time  on a day after the first day of the week $(d> 1, t = 1)$, the agent chooses a two-dimensional action corresponding to only the AYA intervention and the care partner components. At the second time on each day ($t=2$) the agent chooses a one-dimensional action corresponding to only the AYA intervention component. % We denote this dynamic action space at time $(w, d, t)$ as $\mathcal{A}_{w,d,t}$.

\textbf{Observation space.} Apart from the dynamic action space, we collect observations about different components at different time scales as well; see Table \ref{tab:variables}. At each time $(w, d, t)$, we collect the current medication adherence and digital intervention burden from the AYA component. In the end of each day $d$, we collect the psychological distress and digital intervention burden from the care partner component. In the end of a week $w$, we collect the relationship quality from questionnaires from both the AYA and the care partner.  

% \vspace{-3mm}

\section{Domain Knowledge through Causal Diagram} 

% \vspace{-3mm}

\begin{figure}[hpt]
\includegraphics[width=1\textwidth]{Plots/Causal_DAG/DAG_Dyad_Anotation.pdf}
    % \vspace{-0.8cm}
    \captionsetup{parskip=0pt}
    \caption{Causal diagram for ADAPTS-HCT intervention \textsuperscript{1}. We categorize the variables into three components: AYA component (marked in \textbf{black}), care partner component (marked in \textcolor{redorange}{red}), and relationship component (marked in \textcolor{grassgreen}{green}). Each component operates at different time scales. Variables in the AYA component evolve on a twice-daily basis, while the care partner component operates on a daily basis. The relationship component operates on a weekly basis. The arrows indicate the direct causal effects.}
    \vspace{5mm}
    \small\textsuperscript{1} In the causal inference literature, this is called a causal Directed Acyclic Graph (DAG), a graphical representation of causal relationships among a set of variables \cite{pearl2000models}.
    \label{fig:dag}
    % \vspace{-5mm}
    % \end{subfigure}
    % \caption{The RL framework and the causal diagram for ADAPTS-HCT intervention. \ziping{Call the middle one Relationship component.}
\end{figure}


% \ziping{This is a simplified causal graph. The variables that are most likely to have effects.}

% \ziping{We see the pathway in which the interventions can impact the adherence.}

% \ziping{xxx is expected to improve xxx, which in turn may decrease aya adherence.}
Our algorithm design is guided by domain knowledge encoded as the causal diagram in Fig. \ref{fig:dag}. 
% \sam{I think you should put a footnote about this being a causal DAG and reference Pearl--Daiqi did a good job referencing Pearl in her paper--check that paper out}. 
This diagram describes the scientific team's understanding of  the primary causal relationships between the variables in each component listed in Table \ref{tab:variables}.  Note that  the causal relationships are likely more complex and direct paths may exist between any two variables.  However the scientific team believes that these other paths are likely to be less detectable given the noise in digital intervention data.   We summarize the  primary pathways that interventions can take to effect the AYA's adherence in the following.  %these are not principles!
% We highlight that this is a simplified casual graph with arrows only between the key relationships. 

% Below we summarize the major mechanisms on the effect of interventions that guide the surrogate design.

% \ziping{I need some explanation here for each of the standpoints. See examples in the paper on Sepsis Treatment. Could Billie or Sung help us with this?}

% \subsubsection{Principle 1: conditional independence.} The variables in AYA component are independent of those in the care partner component conditioned on the relationship component (there is no direct arrows between the two components). Let $w(t)$ and $w(d)$ be the week index of time $t$ and day $d$ respectively. This principle favors the following feature construction choices: AYA feature at the time $t$ is $\phi_t^{\AYA} = \left(1, R_{t-1}^{\AYA}, B_{t-1}^{\AYA}, A_{w(t)}^{\EDGE}, R_{w(t)}^{\EDGE}, M_t\right)$, where $M_t = 1$ indicates that $t$ is a morning decision time, which ignores the care partner component variables. The care partner feature at the time $d$ is $\phi_d^{\CARE} = \left(1, R_{d-1}^{\CARE}, B_{d-1}^{\CARE}, A_{w(d)}^{\EDGE}, R_{w(d)}^{\EDGE}\right)$, which ignores the AYA component variables, and game feature at the time $w$ is $\phi_w^{\EDGE} = \left(1, B_{t(w)}^{\AYA}, B_{d(w)}^{\CARE}, Y_{w-1}^{\EDGE}, A_{w-1}^{\EDGE}, \bar{R}_{w-1}^{\CARE}, \bar{R}_{w-1}^{\AYA}\right)$. Here $\bar{R}_{w-1}^{\CARE}$ and $\bar{R}_{w-1}^{\AYA}$ are exponentially weighted average psychological distress and medication adherence in the week $w-1$.

% Fortunately, the monotonic mechanism should induce collaboration.
% \begin{itemize}
%     \item two variants: low signal pathway from rel to adh and stress to adh
%     \item stress to adh when relationship is good
% \end{itemize}
\begin{enumerate}
% \item \textbf{Principle 1:} The AYA component is conditional independent of the care partner component conditioned on the relationship component. This principle grounds the choice of the multi-agent algorithm we propose later.
\item \textbf{AYA intervention.} The AYA interventions $A_{w,d,t}^{\AYA}$ should directly influence the immediate AYA's adherence $R_{w,d,t}^{\AYA}$ (black arrows). 


\item \textbf{Game intervention.} The game intervention $A_{w}^{\EDGE}$ has two pathways by which it is expected to effect  AYA's adherence. First, $A_{w}^{\EDGE}$ is expected to increase the AYA's burden $B_{w,d,t}^{\AYA}$ throughout the week $w$.  And AYA's burden $B_{w,d,t}^{\AYA}$ is expected to decrease the AYA's adherence $R_{w,d,t}^{\AYA}$ (\textcolor{darkblue}{blue arrows}). Second, the game intervention $A_{w}^{\EDGE}$ is expected to effect   next week AYA's adherence $R_{w+1,d,t}^{\AYA}$ by improving the end of the week relationship quality $Y_{w}^{\EDGE}$ (\textcolor{grassgreen}{green arrows}). 

% \ziping{Remind them to look into the figure.}

% Relationship quality is known to have non-negative effect on the AYA's adherence. As shown in Fig. \ref{fig:dag}, the relationship quality and the AYA burden block all the paths from game intervention to AYA's adherence. 

\item 
\textbf{Care partner intervention.} The care-partner intervention $A_{w,d}^{\CARE}$ is expected to effect the AYA's adherence indirectly. First, $A_{w,d}^{\CARE}$ should decrease the care partner's psychological distress $Y_{w,d}^{\CARE}$, which should increase the end of week relationship quality $Y_{w}^{\EDGE}$ (\textcolor{yellow}{yellow arrows}). Second, $A_{w,d}^{\CARE}$ should increase the care partner's burden $B_{w,d}^{\CARE}$, which should decrease the the end of week relationship quality $Y_{w}^{\EDGE}$ (\textcolor{darkblue}{blue arrows}). 

% This principle favors the following reward function choice: $r_d^{\CARE} = -R_d^{\CARE}$ for the care partner agent.
\end{enumerate}

We further note that the variables from different components are generally independent conditioned on the bottleneck variables, e.g., the relationship quality that blocks all the paths from the care partner variables to the AYA's adherence. This forms the basis of our multi-agent RL design.


% \ziping{Mention that the three components are independent unless through bottleneck variables: e.g., }

% \vspace{-3mm}

\section{Proposed Multi-Agent RL Approach}

% A key observation from Fig. \ref{fig:dag} is that many variables from different components are independent conditioned on some key bottleneck variables for example, the relationship quality blocks all the paths from the care partner variables to the AYA's adherence. 
The conditional independence property observed from Fig. \ref{fig:dag} motivates us to design a multi-agent RL (MARL) comprising three agents: the AYA agent, the care partner agent, and the relationship agent. Each makes decisions at different time scales for their own component.

% The approach is hierarchical: the game agent's decision on the game intervention is included in the AYA and care partner agents' observation space. 

% Our goal is to learn quickly to benefit as many dyads as possible.
The MARL approach allows us to tailor the agent design choices for each agent to optimize the learning speed. Our base RL algorithm for each agent is Randomized Least Square Value Iteration (RLSVI) \cite{osband2016generalization}, which has been proven as stable in deployment of mobile health applications \cite{trella2024deployed,ghosh2024miwaves}. Additionally, we use linear models, which helps in discussions of the algorithm and its parameters with domain scientists. 

We construct agent-specific features based on Fig. \ref{fig:dag}. Specifically, the AYA agent's model uses its own variables ($B_{w, d, t}^{\AYA}, R_{w, d, t-1}^{\AYA}$) and the variables in the relationship component $(Y_{w-1}^{\EDGE}, A_{w}^{\EDGE})$. Similarly, the care partner agent uses its own variables, as well as the variables in the relationship component. The relationship agent's model uses $Y_{w-1}^{\EDGE}$, and previous weeks' $B_{w-1, 7, 2}^{\AYA}$, $B_{w-1, 7}^{\CARE}$, as well as a weighted average of AYA adherence and care partner distress in the past week.
% , an algorithm that explores the environment by acting optimally w.r.t a randomly sampled value function. 
% The sampling-based algorithm is shown to perform better in mobile health applications \cite{trella2024deployed,ghosh2024miwaves}. 
% \ziping{Stable in deployment, similar to Bayesian linear regression. Help with discussion with domain scientists. }

% Each agent optimizes the discounted sum of rewards with a discount factor $\gamma^{\AYA}= \gamma^{\CARE} = 0.5$ and $\gamma^{\EDGE} = 0$ for the AYA, care partner, and relationship agents, respectively. The discount factor $\gamma^{\EDGE} = 0$ is to reduce the learning complexity of the relationship agent due to limited data (14 points for one dyad). As we have more data for the AYA and care partner agents, we set $\gamma^{\AYA} = \gamma^{\CARE} = 0.5$.

% \vspace{-3mm}

\subsection{Surrogate Reward Function Design Through Domain Knowledge}


Typical MARL \cite{oroojlooy2023review} with independent learners considers agents making decisions without communication. In our study, the lack of communication is due to the different time scales--the relationship agent that makes decisions in the beginning of a week may not predict the AYA and care partner agents' decisions throughout the week. This may prevent the agents from \textbf{collaborating}. For example, the relationship agent may choose to always intervene so as to improve the relationship quality (the primary goal of the game intervention), which may not be optimal for the AYA's adherence.

Furthermore, the effects of care partner intervention and the game intervention are highly delayed. The game intervention improves end of week relationship quality with a significant delayed effect onto the adherence in the next week.
% The relationship agent's discount factor $\gamma^{\EDGE} = 0$ means that the decision-making focuses on its effect on the immediate reward instead of its delayed effect. 
The care partner intervention (positive messages for the care partner) is designed to mitigate the care partner's psychological distress, which only has indirect and delayed effects on the AYA's adherence. 

To address the above two issues, we engineer the reward function to account for the delayed effects and across-component effects of each intervention component to promot collaboration. Similar reward engineering in the context of digital interventions is discussed in \cite{trella2023reward}. Our approach is distinct in that we explore the principles for incorporating domain knowledge to guide the reward function design.

% Surrogate reward design is critical for the above RL framework. The game agent's discount factor $\gamma^{\EDGE} = 0$ means that the decision-making focuses on its effect on the immediate reward instead of its delayed effect. The care partner intervention (positive psychology messages to the care partner) is designed to mitigate the psychological distress, which only has indirect and delayed effects on the AYA's adherence. Therefore, we need to engineer the reward function to take into account the delayed effect of the game intervention and the care partner intervention. Similar reward engineering in the context of digital interventions is discussed in \cite{trella2023reward}. Our approach is distinct in that we explore the principles on incorporating domain knowledge to guide the reward function design.

% \ziping{Add short review on the reward engineering or reward shaping literature. Talk about why it is important to tailor the reward function for each agent here.}

% Design for the game agent:
% \begin{itemize}
%     \item First fit a linear model $f^{\EDGE}$ to predict the sum of medication adherence within a week using the $(A_{w}^{\EDGE}, Y_{w-1}^{\EDGE}, B_{t(w)-1}^{\AYA})$ as features.
%     \item Engineer the reward to take care the delayed effect: 
%     $$
%         r_w^{\EDGE} = f^{\EDGE}(A_{w}^{\EDGE}, Y_{w-1}^{\EDGE}, B_{t(w)-1}^{\AYA}) + \gamma \argmax_{a} f^{\EDGE}(a, Y_{w}^{\EDGE}, B_{t(w+1)-1}^{\AYA}).
%     $$
%     Here $\gamma$ is a tuning parameter.
% \end{itemize}

% Design for the care partner agent:
% \begin{itemize}
%     \item First fit a linear model $f^{\CARE}$ to predict .
%     \item The goal of the care partner agent is to minimize the psychological distress, while managing the digital intervention burden.
%           $$
%             r_d^{\CARE} = R_{t(d)}^{\AYA} + R_{t(d)+1}^{\AYA} - R_d^{\CARE} - \gamma B_{d}^{\CARE}
%           $$
% \end{itemize}


\textbf{Domain knowledge informed surrogate reward functions.} We introduce the surrogate reward functions for the relationship agent and the care partner agent. As informed by Fig. \ref{fig:dag}, the delayed effect of the game intervention is through the relationship quality and the AYA burden. 
% This motivates us to fit a linear model to predict the sum of medication adherence within week $w$, $\sum_{d=1}^{7} \sum_{t=1}^{2} R_{w,d,t}^{\AYA}$, using $(1, Y_{w-1}^{\EDGE}, {B}_{w, 1, 1}^{\AYA})^{\top}$ and their interactions with $A_{w}^{\EDGE}$ as the covariates. 
This motivates us to fit a linear model to predict the sum of medication adherence within week $w$, $\sum_{d=1}^{7} \sum_{t=1}^{2} R_{w,d,t}^{\AYA}$, using $(1, Y_{w-1}^{\EDGE}, {B}_{w, 1, 1}^{\AYA}, A_{w}^{\EDGE}, A_w^{\EDGE} \cdot Y_{w-1}^{\EDGE})$ as the covariates. 
% The working model is:
% \begin{align}
%     \sum_{d=1}^{7} \sum_{t=1}^{2} R_{w,d,t}^{\AYA} = & \vbeta_0^{\top} \bm{S}_w^{\EDGE} + \vbeta_1^{\top} \bm{S}_{w}^{\EDGE} \cdot A_{w}^{\EDGE} + \epsilon_w^{\EDGE},
% \end{align}
% where $\epsilon_w^{\EDGE}$ is the error term.
% which we denote as $\phi_{w}^{\EDGE}(a)$ with $A_w^{\EDGE} = a$.  
% \sam{DAG indicates that predictors are from prior week, $w-1$.   ${B}_{w, 1, 1}^{\CARE}$ does not have 3 subscripts. Should be ${B}_{w-1, 1}^{\CARE}$ }
% Similar notation is used for the care partner burden. 
To account for the delayed effect, we engineer the surrogate reward function for the relationship agent as:
% \begin{align}
% r_w^{\EDGE} = & (1, Y_{w-1}^{\EDGE}, {B}_{w, 1, 1}^{\AYA})\vbeta_0 + (1, Y_{w-1}^{\EDGE}, {B}_{w, 1, 1}^{\AYA})\vbeta_1 \cdot A_{w}^{\EDGE} + \nonumber \\ 
% & (1, Y_{w}^{\EDGE}, {B}_{w+1, 1, 1}^{\AYA}) \vbeta_0 +  \max_{a \in \{0, 1\}} (1, Y_{w}^{\EDGE}, {B}_{w+1, 1, 1}^{\AYA}) \vbeta_1 \cdot a, \label{equ:game_rwd}
% \end{align}
$r_{w}^{\EDGE} = (1, Y_{w-1}^{\EDGE}, {B}_{w, 1, 1}^{\AYA}, A_{w}^{\EDGE}, A_w^{\EDGE} \cdot Y_{w-1}^{\EDGE})\vbeta^{\EDGE}  
 + \max_{a}(1, Y_{w}^{\EDGE}, {B}_{w+1, 1, 1}^{\AYA}, a, a \cdot Y_{w}^{\EDGE}) \vbeta^{\EDGE},$ where $\vbeta^{\EDGE} \in \sR^{5}$ are Bayesian linear regression estimates. 
 \footnote{We choose the prior mean to reflect our guesses on the sign the coefficients. The prior variance is chosen so the prior mean dominates until around the 5th dyads. The complete prior is provided in Appendix.} 
% \end{align} \sam{above formula is too much like math.  Just write out the linear formula.  Will be easier on reader!  I'm not sure if the reader will realize that the label is the number of 1/2 days during the week $w$ for which the AYA adhered.}
The above reward yields a two-step greedy policy, which is a good enough approximation for the total sum of the medication adherence. We opt for a simple, linear model here because the bias trade-off is justified by the faster learning and reduction in noise. 
% \ziping{We prefer simpler models in order to learn fast to trade-off bias and variance.}

The design of the care partner agent is similar. A key observation is that the end of the week relationship quality blocks all the paths from the care partner variables to the AYA's adherence. Thus, we fit a linear model to predict the end of week relationship quality $Y_{w+1}^{\EDGE}$ using $(1, Y_{w,d}^{\CARE}, B_{w,d+1}^{\CARE}, Y_{w-1}^{\EDGE}, A_{w,d}^{\CARE})$ as covariates. The surrogate reward function is:
    $r_{w, d}^{\CARE} =  (1, Y_{w,d}^{\CARE}, B_{w,d+1}^{\CARE}, Y_{w-1}^{\EDGE}, A_{w,d}^{\CARE}) \vbeta^{\CARE}$, 
where $\vbeta^{\CARE} \in \sR^{5}$ are Bayesian linear regression estimates. 
% We update the estimates in real-time based on the current data.
% We denote the covariates as $\phi_{w,d}^{\CARE}$. Then we engineer the reward:
% \begin{align}
% r_{w,d}^{\CARE} = f^{\CARE}(\phi_{w,d}^{\CARE}).
% \end{align}
% \sam{ again no reason for confusing notation.} 
% The care partner agent is essentially maximizing the end of the week relationship quality, and the collaboration between agents is guaranteed due to the monotonic relationship between the end of the week relationship quality and the AYA's adherence.


% \paragraph{Informative prior.} We use the domain knowledge to construct the informative prior for the above regression models to accelerate learning. The prior means of $f^{\EDGE}$ and $f^{\CARE}$ are set to be $\pm 1$ for the main effects and $\pm 0.5$ for the interactions terms with the signs based on our guesses on whether the variables are positively or negatively correlated with the dependent variable. The prior variance are identity matrices $I_6 \sigma^2_{\EDGE}$ and $I_8 \sigma^2_{\CARE}$ with $\sigma^2_{\EDGE} = \sigma^2_{\CARE} = 0.1$. The value 0.1 is chosen so the prior mean dominates until around the 5th dyads. The complete prior is shown in Table \ref{tab:prior} in Appendix \ref{app:algo}.

% \subsubsection{Other designs.} The designs on base algorithm and pooling are not directly informed by the domain knowledge. We will make algorithm selection based on the simulation results. The first kind is the choice of the base algorithm. The base algorithm is about the exploration strategy, which affects the learning speed and the ability to uncover the optimal policy. The base algorithm also determines the planning horizon, which is about how far the agent looks ahead to make decisions. Specifically, we consider the following choices:
% \begin{enumerate}
%     \item Thompson Sampling (TS) for Bandit \cite{russo2018tutorial}: TS that only maximizes the immediate reward. Since taking account for the delayed effect is important, we always add a penalty on the app burden onto the immediate reward.
%     \item Infinite-horizon RLSVI: RLSVI that maximizes the exponentially discounted sum of medication adherence over infinite horizon \cite{osband2016generalization}.
%     \item Finite-horizon RLSVI: RLSVI that maximizes the sum of medication adherence within a week \cite{osband2016generalization}.
% \end{enumerate}
% % \ziping{find references for these algorithms. Why do we choose them?}

% Another important design choice is whether we should pool data across dyads. Pooling data across dyads is a common practice in RL for digital interventions \cite{trella2024deployed} to overcome the challenge of small sample size.
% We consider the following choices:
% \begin{enumerate}
%     \item No pooling across dyads: agent makes decisions for each dyad independently.
%     \item Pooling across dyads: agent pools data across dyads to learn a single model.
%     \item Empirical Bayes? \ziping{let us discuss whether we want to include this.}
% \end{enumerate}