% % \vspace{-3mm}
\section{Results}

% In this section, we evaluate the performance of the proposed MARL approach. 
% We start with an overview of the dyadic environment within which we evaluate the algorithm.

% \subsection{The Dyadic Environment}
\label{sec:testbed}

% \ziping{Call it the Dyadic Environment}

We simulate a \textit{dyadic environment} to evaluate the performance of the proposed framework. The environment design should replicate the noise level and structure that we expect to encounter in the forthcoming ADAPTS-HCT clinical trial. 
% Additionally, the environment should reflect the complexity of the optimal strategy induced by the dyadic structure. For instance, the agent should benefit from collaboration across components, and naive policies that maximizes the immediate goal may not be optimal.

Our environment is based on Roadmap 2.0 dataset involving 171 dyads, each consisting of a patient undergone HCT (target person) and a care partner. Roadmap 2.0 provides daily positive psychology interventions to the care partner only. Roadmap 2.0 collects wearable devices data, for example, physical activity, and self-report data, for example, mood score.
% Roadmap 2.0 involves 171 dyads, each consisting of a patient undergone HCT (target person) and a care partner. 


% in the dyad had the Roadmap mobile app on their smartphone and wore a Fitbit wrist tracker. The Fitbit wrist tracker recorded physical activity, heart rate, and sleep patterns. Furthermore, each participant was asked to self-report their mood via the Roadmap app every evening. A list of variables in Roadmap 2.0 is reported in Table \ref{tab:roadmap_variable} in supplementary material \ref{app:testbed}.

% Roadmap 2.0 data is suitable for constructing the dyadic environment for developing the RL algorithm for ADAPTS-HCT in that Roadmap 2.0 has the same dyadic structure about the participants--post-HCT cancer patients and their care partner. Moreover, Roadmap 2.0 encompasses some context variables that align with those to be collected in ADAPTS-HCT, for example, the daily self-reported mood score, or can be used to construct proxies for the variables to be collected in ADAPTS-HCT, for example, the daily self-reported mood score. 

% We follow the testbed design in \cite{li2023dyadic}, which also uses the Roadmap 2.0 data, but primarily focuses on AYA intervention and Game intervention. 
We build upon the environment design in \cite{li2023dyadic}, which also uses the Roadmap 2.0 data, but primarily focuses on AYA and relationship intervention component. We extend the environment to include the care partner intervention component. Specifically, we fit a separate multi-variate linear model for each component's outcome (ie., $R_{w,d,t}^{\AYA}, Y_{w,d}^{\CARE}, Y_w^{\EDGE}$) in the dataset. These models simulate the user trajectories under no intervention. 


% with the AR(1) working correlation using the generalized estimating equation (GEE) approach \cite{ziegler2010generalized,hojsgaard2006r} that allows us to simulate user trajectories under no intervention. \ziping{For all the R and Y's.}

To simulate outcomes under treatments, we impute the treatment effects of the interventions and the effects of app burden, so the induced standard treatment effects (STE) \footnote{STE here is defined as the difference in the mean of the primary outcomes under the proposed intervention package and these under no intervention, which is further standardized by the standard deviation under no intervention. } are around 0.15, 0.3, and 0.5. These STEs are commonly seen in behavioral science studies \cite{cohen2013statistical}. A complete description and code of the dyadic environment is provided in supplementary material \footnote{\url{https://github.com/StatisticalReinforcementLearningLab/ADAPTS-HCT-AIME}}. 

% Lastly, we verify that the dyadic environment indeed requires collaboration. supplementary material \ref{app:evidence-collab} details our experiment showing that each agent impacts the performance of other agents in the environment. 

% The testbed simulates a trial by randomly sampling dyads from the dataset, and simulate their trajectories based on the actions selected by the RL algorithm. The testbed details are described in supplementary material \ref{app:testbed}. Our experiments primarily focus on the three vanilla testbeds corresponding to the three STEs.

% % \vspace{-3mm}

\subsection{Cumulative Adherence Improvement}

% \ziping{Cumulative adherence.}

% We simulate 25 dayds

% We evaluate the following three algorithms on the designed simulation testbed to understand the benefits of multi-agent algorithm and the benefits of surrogate reward design.
% \begin{enumerate}
%     \item \texttt{SingleAgent}: runs a single agent that selects actions for all components.
%     \item \texttt{MultiAgent}: runs multiple agents that select actions for each component.
%     \item \texttt{MultiAgent+SurrogateRwd}: runs multiple agents that select actions for each component, and uses the surrogate reward functions learned in (\ref{equ:game_rwd},\ref{equ:care_rwd}).
% \end{enumerate}

% \subsection{Cumulative Rewards Improvement}

We simulate 25 dyads, the planned sample size in the upcoming pilot study, by sampling dyads sequentially with replacement from Roadmap 2.0 dataset. Each dyad is simulated for 14 weeks. We implement the following three algorithms: \texttt{SingleAgent}, \texttt{MultiAgent}, and \texttt{MultiAgent+SurrogateRwd}. Here the \texttt{SingleAgent} is the algorithm that trains a single agent that outputs all the three types of actions. The \texttt{MultiAgent} is the proposed MARL algorithm using the adherence as the learning reward signal for all three agents. The \texttt{MultiAgent+SurrogateRwd} is the proposed MARL algorithm using the surrogate reward functions.
%learned in (\ref{equ:game_rwd}) and (\ref{equ:care_rwd}). 
The full details of the algorithms are described in supplementary material.% \ref{app:algo}.

We compare the proposed RL algorithms with a random policy, where $P(A_{w,d,t}^{\AYA}=1)= P(A_{w,d}^{\CARE}=1) = P(A_w^{\EDGE}=1)\equiv0.5$ in terms of the cumulative adherence improvement. 
% The algorithm learns much faster for higher effect sizes. 

We also observe that all the algorithms can make more significant improvement over the random policy under a higher STE. \texttt{SingleAgent} takes longer to learn due to the larger number of parameters compared to \texttt{MultiAgent}. We also see an advantage of using surrogate rewards through an increased cumulative adherence  at all levels of STE. Notice that for a low STE, the learning is slow, which is intuitive given that the the signal-to-noise ratio is low in such an environment. Additional ablation studies and analysis on the collaborating behavior is provided in the supplementary material. 
% \footnote{\url{https://github.com/StatisticalReinforcementLearningLab/ADAPTS-HCT-AIME}}.

\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.32\textwidth}
        % \rotatebox{90}{\quad Mediator 0}
        % \includegraphics[width=0.91\textwidth]{Plots/Experiments/015/All_Rewards_Mediator0.pdf}
        % \rotatebox{90}{\quad \quad Mediator 1}
        \includegraphics[width=1\textwidth]{Plots/Experiments/015/All_Rewards_Mediator1.pdf}
        % \rotatebox{90}{\quad \quad Mediator 2}
        % \includegraphics[width=0.91\textwidth]{Plots/Experiments/015/All_Rewards_Mediator2.pdf}
        \caption{STE 0.15}
    \end{subfigure}
    \begin{subfigure}[b]{0.32\textwidth}
        % \includegraphics[width=1\textwidth]{Plots/Experiments/03/All_Rewards_Mediator0.pdf}
        \includegraphics[width=1\textwidth]{Plots/Experiments/03/All_Rewards_Mediator1.pdf}
        % \includegraphics[width=1\textwidth]{Plots/Experiments/03/All_Rewards_Mediator2.pdf}
        \caption{STE 0.3}
    \end{subfigure}
    \begin{subfigure}[b]{0.32\textwidth}
        
        % \includegraphics[width=1\textwidth]{Plots/Experiments/05/All_Rewards_Mediator0.pdf}
        \includegraphics[width=1\textwidth]{Plots/Experiments/05/All_Rewards_Mediator1.pdf}
        % \includegraphics[width=1\textwidth]{Plots/Experiments/05/All_Rewards_Mediator2.pdf}
        \caption{STE 0.5}
    \end{subfigure}
    \caption{Cumulative adherence improvement over the uniform random policy for all three components under dyadic environments with different STEs. The confidence interval is the standard deviation based on 1000 independent runs.} 
    % \vspace{-3mm}
    % \sam{label for y-axis is very confusing.....!} \sam{this figure is exactly what I thought we would see.  The closer to 0 the STE gets the less advantage of the multi-agent approach.  Why doe you think this does not make sense?  is it because in figure (1,1) the rewards for single agent decrease?   Doesn't single agent have to learn many many weights?   If so then figure (1,1) makes sense....}}
    % \ziping{We were seeing that even the our algorithm was worse than random policy. We found a bug in the single agent. Now it makes more sense. }
\end{figure}

% \vspace{-3mm}

% \subsection{Validating Collaboration} As discussed in Section \ref{sec:testbed}, the dyadic environment requires collaboration in order to optimize the overall performance, while collaboration is a major concern in MARL with independent learners. In this section, we validate that the collaboration of \texttt{MultiAgent+SurrogateRwd} is achieved by learning the surrogate reward functions that account for the other agent's behavior. To see this, we visualize the learned surrogate reward coefficients in (\ref{equ:game_rwd}, \ref{equ:care_rwd}) under the dyadic environment with STE 0.5 in Figure \ref{fig:rwd_coef}. Among the relationship reward coefficients, the coefficient for the main effect of weekly relationship intervention $A_{w}^{\EDGE}$ is significantly negative. That is the relationship agent accounts for the negative effect of weekly intervention caused by the increased app burden during a game-on week, while the effect of the relationship intervention when the relationship is good is positive. Similarly, the care partner distress coefficient is negative. That is the care partner agent accounts for the positive effect of reducing distress onto weekly relationship.

% % \vspace{-3mm}

% \begin{figure}[hpt]
%     \centering
%     \includegraphics[width=0.6\textwidth]{Plots/RwdCof/combined_theta.pdf}
%     % % \vspace{-3mm}
%     \caption{Learned surrogate reward coefficients under STE 0.5.}
%     % \vspace{5mm}
%     \label{fig:rwd_coef}
% \end{figure}

% % \vspace{-5mm}

% To further validate collaboration, we train each individual agent in the \texttt{MultiAgent+SurrogateRwd} algorithm over 1000 dyads under the STE 0.5 environment, while fixing the randomization probability of the other agents. We observe that the average probability of sending an intervention for the game agent and the care partner agent decreases as the overall dosages of the other agents increase. This indicates that the agents learn to \textit{reduce} the intervention probability when the other agents are more likely to send an intervention. More details are provided in supplementary material \ref{app:additional_results}.

% % \vspace{-3mm}

\section{Discussion}

% % \vspace{-2mm}

In this paper, we propose an MARL algorithm that effectively learns to optimize delivery of the ADAPTS-HCT digital interventions. While this presents a significant step towards preparing for the ADAPTS-HCT clinical trial, several challenges remain to be addressed. First, in the real-clinical trial, the participants are recruited incrementally with significant overlaps, whereas our dyadic environment assumes a simple sequential recruitment. Second, the clinical trial study emphasizes the need for after-study analysis, such as causal inference on treatment effects, which often requires smooth allocation functions \cite{zhang2024replicable}. Additionally, there is room to further improve algorithm performance. For example, our proposed algorithm pools data across dyads to reduce learning variance but does not account for heterogeneity across dyads. The algorithm may benefit from a more flexible pooling, e.g., a random effect model.

% % \vspace{-3mm}

% To further validate collaboration, we investigate the agent's behavior under extreme simulation testbed designs, where the game intervention has a direct negative effect onto AYA adherence, and the care partner intervention has a direct negative effect onto next week relationship. We train the \texttt{MultiAgent+SurrogateRwd}, \texttt{MultiAgent}, and \texttt{SingleAgent} with over 200 dyads. The later is an approximation to the optimal policy. The average probability of sending an intervention for \texttt{MultiAgent+SurrogateRwd} is as low as 0.18, and 0.19 over the last 15 dyads. It is worth-noting that \texttt{MultiAgent} still have a sampling probability of 0.68 for the care partner agent because it does not account for the delayed effect through relationship.

% \vspace{-3mm}