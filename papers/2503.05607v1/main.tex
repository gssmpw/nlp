\documentclass[journal=accacs,manuscript=article]{achemso}
\usepackage{amssymb}
\usepackage{amsmath,bm}
\usepackage{graphicx,color}
\usepackage[version=3]{mhchem} 
%\usepackage[english]{babel}
%\usepackage{academicons}
\usepackage{xcolor}%,hyperref}
\usepackage{soul}
\sethlcolor{yellow}
\usepackage{multirow}


\newcommand{\B}[1]{{\bm{#1}}}%% Bold Roman & Greek Lower & Upper Case
\newcommand{\R}[1]{{\textrm{#1}}}
\newcommand{\orcid}[1]{\href{https://orcid.org/#1}{\textcolor[HTML]{A6CE39}{\aiOrcid}}}
\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand{\blue}[1]{\textcolor{blue}{#1}}


\author{Joyjit Chattoraj}
\email{joyjit_chattoraj@ihpc.a-star.edu.sg}
\affiliation{Computing and Intelligence, Institute of High Performance Computing, Agency for Science Technology and Research, Singapore 138632, Republic of Singapore}
\author{Brahim Hamadicharef}
\affiliation{Computing and Intelligence, Institute of High Performance Computing, Agency for Science Technology and Research, Singapore 138632, Republic of Singapore}
\author{Teo Shi Chang}
\affiliation{Department of Catalysis and Green Process Engineering, Institute of Sustainability for Chemicals, Energy and Environment, Agency for Science Technology and Research, Singapore 627833, Republic of Singapore} 
\author{Yingzhi Zeng}
\affiliation{Materials Science and Chemistry, Institute of High Performance Computing, Agency for Science Technology and Research, Singapore 138632, Republic of Singapore}
\author{Chee Kok Poh}
\affiliation{Department of Catalysis and Green Process Engineering, Institute of Sustainability for Chemicals, Energy and Environment, Agency for Science Technology and Research, Singapore 627833, Republic of Singapore}
\author{Luwei Chen}
\affiliation{Department of Catalysis and Green Process Engineering, Institute of Sustainability for Chemicals, Energy and Environment, Agency for Science Technology and Research, Singapore 627833, Republic of Singapore}
\author{Teck Leong Tan}
\affiliation{Materials Science and Chemistry, Institute of High Performance Computing, Agency for Science Technology and Research, Singapore 138632, Republic of Singapore}

\title{AceWGS: An LLM-Aided Framework to Accelerate Catalyst Design for Water-Gas Shift Reactions} % Featuring Data Extraction, Article Comprehension, and Inverse Modelling}
\keywords{Water-Gas Shift (WGS), Catalyst Design, Inverse Modelling, Large Language Model (LLM), Generative AI}
\begin{document}

\begin{abstract}
While the Water-Gas Shift (WGS) reaction plays a crucial role in hydrogen production for fuel cells, finding suitable catalysts to achieve high yields for low-temperature WGS reactions remains a persistent challenge.
Artificial Intelligence (AI) has shown promise in accelerating catalyst design by exploring vast candidate spaces, however, two key gaps limit its effectiveness. First, AI models primarily train on numerical data, which fail to capture essential text-based information, such as catalyst synthesis methods. Second, the cross-disciplinary nature of catalyst design requires seamless collaboration between AI, theory, experiments, and numerical simulations, often leading to communication barriers. 
To address these gaps, we present AceWGS, a Large Language Models (LLMs)-aided framework to streamline WGS catalyst design. AceWGS interacts with researchers through natural language, answering queries based on four features: (i) answering general queries, (ii) extracting information about the database comprising WGS-related journal articles, (iii) comprehending the context described in these articles, and (iv) identifying catalyst candidates using our proposed AI inverse model. We presented a practical case study demonstrating how AceWGS can accelerate the catalyst design process. 
AceWGS, built with open-source tools, offers an adjustable framework that researchers can readily adapt for a range of AI-accelerated catalyst design applications, supporting seamless integration across cross-disciplinary studies.
\end{abstract}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
\label{sec:intro}
% Role of WGS reactions for hydrogen fuels, challenges bottle-necks
The increasing global energy demand and reliance on carbon-based fuels significantly contribute to environmental pollution. Hydrogen, produced from renewable sources, offers a sustainable, carbon-free alternative and plays a key role in decarbonizing the global energy system, especially through fuel cell technologies. 
Hydrogen used in fuel cells is commonly produced through hydrocarbon reforming processes, which often yield undesirable by-products. For example, in proton exchange membrane fuel cells, carbon monoxide (CO) is a common contaminant in the hydrogen fuel, arising as a by-product of production methods such as steam methane reforming.
This CO must be entirely removed to protect the anode catalyst. 
The Water-Gas Shift (WGS) reaction, a catalytic process between CO and H$_2$O to produce H$_2$ and CO$_2$, is seen as the solution. For fuel cells, WGS catalysts need to be highly stable, active, and able to function without special pretreatment or regeneration to achieve maximum CO conversion at low temperatures. Noble metal catalysts are potential candidates for such applications~\cite{Oensan2007, Park2009, Ebrahimi2020, ZhouLiu2023, ShuiJZZLH2023}.

% AI-accelerated catalsyt design, 
    % Benifits
    % state-of-the-art
    % bottlenecks (unutilised text information such as synthesis, catalyst structures, communication)
Artificial Intelligence (AI) has become an increasingly appealing approach for catalyst design, as advanced AI models can uncover complex relationships between numerous variables, enabling them to explore and exploit the vast design space~\cite{Toyao2020, Benavides2024}.
In the context of AI research on the WGS reaction, Odaba\c{s}i et al.~\cite{Odabasi2014} conducted the first comprehensive study. They developed a database using data mining techniques, comprising 4,360 experimental data points and 81 features, including catalyst compositions, preparation methods, reaction conditions, and CO conversion. These data were extracted from 84 research articles published between 2002 and 2012.
They also developed AI models to predict CO conversion based on 80 other features. Several subsequent studies have utilized this database, proposing various AI models and techniques to more effectively capture the correlation between catalyst characteristics and CO conversion~\cite{Avsar2017, Cavalcanti2019, Suzuki2019, Smith2020, Garcia2023, golder2024machine}.

In the recent past, the authors of this article developed a theory-guided AI model, training it on the same database while incorporating thermodynamic equilibrium constraints through a custom loss function. They demonstrated that their AI model strictly adhered to the thermodynamic equilibrium principle, leading to more accurate and robust predictions~\cite{Chattoraj2022}. 
Following this work, the authors compiled a new WGS database by extracting data from 82 articles published between 2013 and 2021. This effort produced 8,908 individual records with 99 features, covering 10 different base metals, 27 supports, 16 promoters, 32 preparation methods, 13 reaction conditions, and carbon monoxide conversion percentages.
Furthermore, they developed an inverse model that integrates the theory-guided AI model with a particle swarm optimization method. This inverse model can explore and utilize the new database to identify suitable catalysts for low-temperature WGS reactions based on the design constraints set by the researchers~\cite{Chattoraj2023}.

The primary limitation of the AI models that are mentioned above is their reliance solely on numerical data. For example, they reduce complex catalyst preparation methods, such as wet impregnation, to simple categorical variables in a {\it yes} or {\it no} format, whereas, catalyst synthesis involves a series of detailed, multi-step processes. Therefore, when an inverse model predicts a catalyst design, it cannot provide the corresponding step-by-step synthesis procedure, which is crucial for practical implementation. This limitation underscores the need to utilize the textual content of relevant research articles.

Another limitation of current AI-accelerated catalyst design is the necessity for cross-disciplinary collaboration. Establishing a comprehensive AI pipeline, beginning with database preparation, data cleaning, feature selection, and extraction, and extending to training AI models, including inverse models, and validating predictions through simulations and experiments, often requires the involvement of multiple researchers with diverse expertise. This complexity can hinder the efficiency of the research process, as it demands effective communication and coordination among specialists in fields such as chemistry, materials science, computer science, and engineering. Consequently, the multifaceted nature of AI-driven catalyst design can create challenges in workflow integration and knowledge transfer, potentially slowing the advancement of innovative catalyst designs.

% Large Language Model
    % Benefits
    % LLM-RAG state-of-the-art from chemistry view point e.g. ChemCrow
    % bottlenecks (ethics, not open source papers, privacy, ...)
Large Language Models (LLMs), such as ChatGPT, could offer significant advantages in addressing existing limitations in AI-accelerated catalyst design. These models analyze vast amounts of text data, utilizing deep learning techniques including neural networks and transformers to comprehend context, predict words, and generate human-like responses. By learning complex patterns from the data, LLMs produce coherent and contextually relevant answers during conversations. The integration of Retrieval-Augmented Generation (RAG) enhances this capability by connecting LLMs with external knowledge bases, enabling access to up-to-date information for generating reliable outputs. 
 The synergy of LLMs and RAG, referred to as LLM-RAG, has profound implications for scientific research. LLM-RAG systems can provide accurate and dependable answers to scientific inquiries by leveraging the reasoning capabilities of LLMs alongside real-time information retrieval. RAG's ability to manage long contexts and its high interpretability make it particularly suitable for complex, integrative, or summary questions that necessitate processing large volumes of material~\cite{fan2024survey, kumar2024can,si2024can}.

Kim et al.~\cite{kim2024large} fine-tuned LLMs to predict the synthesizability of inorganic compounds and select synthesis precursors, demonstrating promising performance compared to specialized AI models.
Wang et al.~\cite{wang2024catalm} introduced a large language model specifically tailored for the domain of electrocatalytic materials, demonstrating its potential to enhance human-AI collaboration in catalyst knowledge exploration and design.
Bran et al~\cite{bran2024augmenting} developed ChemCrow, a chemistry-focused LLM-RAG system using GPT-4 and 18 expert-designed tools. The system enhances organic synthesis, drug discovery, and materials design performance, effectively automating chemical processes and bridging the gap between experimental and computational chemistry.


% AceWGS
    % Demand to have a home-grown copilot for assisting catalyst design research    
    % brief description of AceWGS
    % Value of this work
In this article, we introduce an LLM-RAG framework, AceWGS, that aims to accelerate the design of noble metal catalysts for the WGS reaction. 
AceWGS maximizes the utilization of both text and numerical data from the research articles on WGS experiments involving noble metals as catalysts, while also demonstrating the advantages of combining traditional AI models with LLMs.
The framework comprises four key features: (i) a generic querying tool that addresses researcher queries related to WGS reactions and AI methods, (ii) an extraction tool that retrieves information from a local WGS database of 82 research articles, (iii) a comprehension tool that provides insights from individual WGS research articles, and (iv) an AI inverse modelling tool that employs a theory-guided AI model to identify suitable catalyst designs. 

AceWGS is built using open-source software and moderate-sized LLMs (e.g., Llama3 with 8 billion parameters), ensuring adaptability and ease of implementation. This design enables researchers to follow the methods outlined in this article to develop their LLM-assisted frameworks. Our approach will facilitate seamless cross-disciplinary research in AI-accelerated material design.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Methodology}
\label{sec:method}
\begin{figure}
    \centering
    \includegraphics[width=0.85\linewidth]{Fig1.png}
    \caption{AceWGS framework utilizing a graphical user interface (GUI), large language models (LLMs) with retrieval augmented generation (RAG) to accelerate catalyst design for Water-Gas Shift Reactions (WGS).}
    \label{Fig:AceWGS}
\end{figure}
\subsection{AceWGS}
AceWGS consists of two primary components: a graphical user interface (GUI) and a core LLM-RAG framework, which incorporates four key features along with a central module, called "Switch" (see Fig.~\ref{Fig:AceWGS}). The module functions as a rule-based system that selects and manages which feature to activate based on researcher queries. For example, when the query specifies "inverse model", Switch configures the environment to assist researchers in identifying optimal catalyst compositions and reaction conditions. 

The entire AceWGS is built upon Python (version $3.11$)~\cite{Python}, providing the foundation for all components. The graphical user interface (GUI) is developed using Tkinter~\cite{Tkinter}, offering researchers an intuitive way to interact with the framework. LangChain (version $0.3$)~\cite{Langchain} is used to orchestrate large language models (LLMs) and local databases, enabling seamless retrieval-augmented generation (RAG) processes. Ollama (version $0.3.11$)~\cite{Ollama} manages the execution of LLMs, with open-source models such as Llama installed locally on a Ubuntu machine, facilitating efficient and flexible model deployment for the LLM-RAG framework.

The following sections describe the methodologies behind each of the four features: answering general queries, extracting database information, comprehending research articles, and AI inverse modelling.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Answer General Queries}
\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{Fig2.png}
    \caption{(a) Feature-1 contains an LLM model to answer general queries. (b) A typical answer generated by Feature-1 is based on a catalyst-related question. (c) Similarly, a typical answer generated by Feature-1 is based on an AI-related question.}
    \label{Fig:general}
\end{figure}
The "Answer General Queries" feature is designed to answer various general inquiries, including WGS reactions, catalysts, machine learning, inverse modelling, generative AI, LLMs, and RAGs. Leveraging open-source LLM models, this feature delivers domain-specific responses with high accuracy. Its primary purpose is to facilitate interdisciplinary communication, allowing researchers from different fields to access relevant information from outside their areas of expertise quickly. The framework is straightforward, utilizing an LLM, with typical question-and-answer examples shown in Fig.~\ref{Fig:general}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Extract Database Information}
\begin{figure}
    \centering
    \includegraphics[width=0.85\linewidth]{Fig3.png}
    \caption{(a) Feature-2 consists of two tools: (i) an LLM agent that takes a data frame and a customized prompt as inputs, and (ii) an execution tool that runs the Python command suggested by the LLM agent. (b) Typical responses generated by Feature-2 in answer to two questions designed to retrieve information from the local database.}
    \label{Fig:extract}
\end{figure}
The "Extract Database Information" feature helps researchers retrieve qualitative, quantitative, and statistical information from a local database of 82 articles on WGS experiments involving noble metal catalysts. To achieve the objective, we first constructed a data frame using pandas library~\cite{pandas}, which contains seven metadata fields for each article: reference ID, publication year, title, abstract, journal name, author names, and digital object identifier (DOI). We then formulated a prompt describing the data frame and its seven metadata fields. The data frame and the prompt were inputted into an inbuilt LLM agent, {\it create\_pandas\_dataframe\_agent}~\cite{dfAgent}, within LangChain, which is designed to interpret data frames and perform operations such as data retrieval, and filtering, based on customized queries. The agent generates suggested Python commands, which are then passed to an execution tool that runs these commands and displays the corresponding results (see Fig.~\ref{Fig:extract}(a)). 

Fig.~\ref{Fig:extract}(b) presents two sample queries and corresponding answers about the local database, demonstrating the utility of this feature in efficiently extracting insights. By retrieving and organizing relevant data, this tool aids researchers in evaluating the strengths and limitations of the database, facilitating informed decision-making for further analysis.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Comprehend Articles}
\begin{figure}
    \centering
    \includegraphics[width=0.85\linewidth]{Fig4.png}
    \caption{(a) Feature-3 is an LLM-RAG that takes a vector retriever tool and a customized prompt as inputs. (b) The workflow of the vector retriever tool. (c) Typical responses generated by Feature-3 in answer to four questions set to comprehend a research article.}
    \label{Fig:comprehend}
\end{figure}
The "Comprehend Articles" feature allows researchers to retrieve information from any of the 82 articles stored in the local database. To use the feature, researchers first input the reference ID of the desired article into the GUI, and then proceed to pose questions. The system processes each query using an LLM-RAG framework, which is integrated with a customized prompt and a vector retriever tool (see Fig.~\ref{Fig:comprehend}(a)). The prompt guides the LLM-RAG system in identifying the comprehension task and generating an appropriate response. The vector retriever tool allows the system to search and retrieve the most relevant sections of the article efficiently, ensuring that the LLM can accurately focus on and answer specific questions by accessing the semantically related portions of the document.

The workflow of the vector retriever tool is illustrated in Fig.~\ref{Fig:comprehend}(b). The process begins with text extraction (often referred to as "chunking") from 82 articles from their portable document format (PDF). 
Each PDF is segmented into a list of 1000-character texts with a 150-character overlap between adjacent segments using pdfMiner~\cite{PDFMiner} and {\it RecursiveCharacterTextSplitter} module of LangChain.
%RecursiveCharacterTextSplitter module of langchain, chunk-size 1000, and chunk-overlap 150. 
 
 Next, the extracted list of text segments is converted into numbers, specifically vector embeddings, for further processing by an LLM. The vectorization is executed using the {\it mxbai-embed-large} model of OllamaEmbeddings~\cite{OllamaEmbeddings}, and the resulting embeddings are stored in a vector database built with FAISS~\cite{FAISS}.

We demonstrated an example of the feature in Fig.~\ref{Fig:comprehend}(c), where we selected an article from our database, identified as reference 4. The generated answers provided insights into several key aspects, including the number and types of catalysts, the reaction temperatures for conducting WGS experiments, and the corresponding results. Specifically, in this case, the feature enabled the identification of the best-performing catalysts.
% One application could be the first screening to build a numerical dataset to train AI models.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%/.

\subsection{AI Inverse Modelling}
\begin{figure}
    \centering
    \includegraphics[width=0.85\linewidth]{Fig5.png}
    \caption{(a) Feature-4 contains three tools: (i) the Parameter Settings tool, a GUI where researchers can set the required catalyst design parameters, e.g., base metals, supports, promoters, preparation methods, and reaction conditions; (ii) the Inverse Model, which searches for the best catalytic candidates based on the set design parameters; and (iii) the Prompt-guided LLM, which takes the outputs of the Inverse Model and explains them in a natural language manner. (b) The Inverse Model framework, where the model initially takes a set of parameters, predicts the CO conversion percentage using our theory-guided AI model then performs particle swarm optimization to search for catalytic candidates. (c) A typical query on inverse modelling results in a sequence of responses starting from setting parameters, stating the status of inverse modelling, and finally displaying the solution.
    }
    \label{Fig:inverse}
\end{figure}
The "AI Inverse Modelling" feature searches for optimal catalytic candidates and reaction conditions according to the design constraints specified by researchers. This feature consists of three tools: (i) the Parameter Settings tool, a GUI where researchers can design a catalyst by selecting base metals, supports, promoters, and catalyst preparation methods, and can set reaction conditions; (ii) the Inverse Model, which identifies the best catalytic candidates based on these parameters; and (iii) the Prompt-guided LLM, which interprets and explains the outputs of the Inverse Model tool in natural language (see Fig.~\ref{Fig:inverse}(a)).

The Inverse Model tool is sophisticated, integrating our previously proposed pre-trained theory-guided AI model with a particle swarm optimization algorithm (see Fig.~\ref{Fig:inverse}(b)). The theory-guided AI model accepts catalytic compositions (numerical variables), preparation methods (binary features), and reaction conditions (numerical variables) as inputs. It outputs the predicted CO conversion percentage, the uncertainty in this prediction, and the thermodynamic equilibrium conversion for CO. The particle swarm optimization algorithm further explores and exploits these predictions to iteratively identify the optimal solution. Detailed descriptions and source codes for the model are provided in~\cite{Chattoraj2023}.

The output of the Inverse Model tool is passed to a prompt-guided LLM as a string. The LLM receives specific instructions via a prompt on how to interpret the string and then generates a concise explanation, simplifying the content into natural language in no more than 200 words. Translating the raw output string into natural language allows researchers to easily understand complex data and results, making the information more accessible for their work. 

A typical example of the AI Inverse Modelling feature is shown in Fig.~\ref{Fig:inverse}(c), where on the Parameter Settings we design a catalyst with platinum (Pt) as a base metal, cerium oxide (CeO$_2$) as support, nickel (Ni) as promoter, wet impregnation as the preparation method, and set the range of reaction temperature between 300 and 350~$^\circ$C. 
The Parameters Settings passed these initial parameters to the Inverse Model, which then found the catalyst weight percentage and optimal reaction conditions that can achieve the highest CO conversion. 
The final catalytic parameters were passed to the prompt-guided LLM which then expressed the raw parameters into natural language.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Results}
\begin{table}
    \centering
    \begin{tabular}{|c|c|c|c|}
        \hline
         Model    &  General Queries &  Extraction & Comprehension \\
         \hline
         Llama2   &      2.9            &  3.0 &  3.17\\
         \hline
         Llama3   &      3.6            &  3.8 &  3.17\\
         \hline
         Llama3.1 &      4.3            &  4.1 &  3.08\\
         \hline
         Gemma2   &      4.6            &  4.3 &  3.50\\
         \hline
    \end{tabular}
    \caption{The performance of the four LLMs is presented as the average Likert score across all questions, with a maximum possible score of 5 and a minimum possible score of 1.}
    \label{Table:results}
\end{table}
In this study, we evaluate the performance of four moderately sized open-source large language models (LLMs), including Llama2 (7 billion parameters, 3.8 GB), Llama3 (8 billion parameters, 4.7 GB), Llama3.1 (8 billion parameters, 4.7 GB), and Gemma2 (9 billion parameters, 5.4 GB), across three distinct tasks of AceWGS: Answer General Queries, Extract Database Information, and Comprehend Articles. Note that we did not assess the fourth task, AI Inverse Modelling, as the role of a prompt-guided LLM in this feature is primarily auxiliary. 
We configured the LLMs with the following default parameters: ${\it temperature} = 0$, ${\it top\_k} = 10$, and ${\it top\_p} = 0.5$, to ensure that the generated responses prioritize accuracy and factual correctness over creativity, as maintaining the integrity of scientific information was the primary objective.
 %Ollama(model=model_name, top_k=10, top_p=0.5, temperature=0.0)
 %temperature	The temperature of the model. Increasing the temperature will make the model answer more creatively. (Default: 0.8)
%top_k	Reduces the probability of generating nonsense. A higher value (e.g. 100) will give more diverse answers, while a lower value (e.g. 10) will be more conservative. (Default: 40)	int	top_k 40
%top_p	Works together with top-k. A higher value (e.g., 0.95) will lead to more diverse text, while a lower value (e.g., 0.5) will generate more focused and conservative text. (Default: 0.9)
 
To evaluate the performance of the four LLMs for the feature Answer General Queries, we posed 10 questions, consisting of five on WGS reactions and noble metal catalysts, one on AI, one on inverse modelling, and three on LLM and RAG. Each question was evaluated using a 5-point Likert scale, where the criteria included: 1 (incorrect), 2 (poor), 3 (acceptable), 4 (good), and 5 (very good). Given the descriptive nature of the questions, there is no single correct answer, however, the answer can be potentially evaluated as incorrect. We found that Llama2 had the lowest average score of 2.9 across the ten questions, followed by Llama3, Llama3.1, and Gemma2 (see Table~\ref{Table:results}).

Similarly, for the feature, Extract Database Information, we prepared 10 questions and evaluated the performance of the LLMs using the 5-point Likert scale. In this case, each question had a single accurate answer, which could receive a score of 5, while incorrect answers were scored as 1. The evaluation revealed a performance improvement in the following order: LLama2, Llama3, Llama3.1, and Gemma2, respectively (see Table~\ref{Table:results}).  

The feature, Comprehend Articles, is an LLM-RAG system, and its effectiveness depends on several critical tasks, including accurate text segmentation, vector embeddings, retrieval search, and generation. This study focuses on the performance of the four LLMs primarily responsible for the generation task. A set of 12 questions was designed, with 4 questions each for information retrieval from three selected articles~\cite{petallidou2013water,runxia2016cu,cai2021hydrogen}. Most of these questions allow for broad but precise answers, which were evaluated using a Likert scale ranging from 1 to 5. 
Unlike the trends observed in the previous two features, the performance of the three Llama models did not improve with the newer versions for the comprehension task. However, Gemma2 consistently outperformed the other models, maintaining its superiority across various features
(see Table~\ref{Table:results}).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Case study}
In this section, we demonstrate the practical utility of the AceWGS framework by presenting a case study centered on catalyst design for the WGS reaction. Specifically, we aim to identify a noble metal catalyst capable of achieving maximum CO conversion at temperatures below 300~$^\circ$C.

% 
\subsection*{Extract Database Information}
First, identifying potential catalytic candidates reported in the literature is essential. In this scenario, we aim to find the most recent articles available in our local WGS database. This can be accomplished using the "Extract Database Information" feature of AceWGS. The typical flow of questions and corresponding answers from this feature is shown below, illustrating how researchers can efficiently gather relevant data.\\ 

\makebox[\textwidth][c]{%
    \fbox{%
        \parbox{0.9\textwidth}{%
        \textbf{Question:} Extract the journal names for all papers that were published in the year 2021. \\ 
        \textbf{Answer:}  ['Nature', 'Energy \& Fuels', 'Nanomaterials', 'Catalysis Today', 'Journal of Catalysis', 'Journal of Catalysis', 'Catalysts', 'Heliyon', 'International Journal of Energy Research', 'Catalysts'] \\
        \textbf{Question:} Retrieve the reference and title of all papers published in the year 2021 in the journal Nature.\\
        \textbf{Answer:} R71 \ 2021 \ Nature \ A stable low-temperature H2-production catalyst by crowding Pt on $\alpha$-MoC. \\
        \textbf{Question:} Retrieve papers where the string 'MoC' is mentioned in the abstract in the exact same form.\\
        \textbf{Answer:} \\
        \ \ R51 \ 2017 \ Science \  Atomic\-layered Au clusters on $\alpha$-MoC as catalysts for the low-temperature water-gas shift reaction \\
        \ \ R71 \ 2021 \ Nature \ A stable low-temperature H2-production catalyst by crowding Pt on $\alpha$-MoC 
        }
    }
} \\
\\

At this stage, we identified two articles~\cite{yao2017atomic, zhang2021stable} that report gold and platinum-based catalysts supported by $\alpha$-molybdenum carbide. From their abstracts, it is clear that both catalysts show superior performance at the desired low-temperature limit, indicating that these studies warrant further exploration.

\subsection*{Comprehend Articles}
To gain a deeper understanding of the two identified articles, we employed the Comprehend Articles feature of AceWGS. The typical workflow of questions and their corresponding answers is illustrated below. \\

\makebox[\textwidth][c]{%
    \fbox{%
        \parbox{0.9\textwidth}{%
         \textbf{Question:} Comprehend the article of reference ID R71.\\
         \textbf{Answer:} Ready to retrieve information from the article R71. \\  
         Title: A stable low-temperature H$_2$-production catalyst by crowding Pt on $\alpha$-MoC \\
         \textbf{Question:} Extract the name of the catalysts mentioned in the article. \\
         \textbf{Answer:} The catalysts mentioned in the article are Pt/{$\alpha$}-MoC catalysts with varying Pt loadings.\\  
         \textbf{Question:} Find the name of the catalyst synthesis or preparation method. \\
         \textbf{Answer:} The catalyst synthesis method used is incipient wetness impregnation (IWI).  \\
         \textbf{Question:} Provide a step-by-step synthesis method for the catalyst as described in the article.  \\
         \textbf{Answer:} The synthesis of Pt/$\alpha$-MoC catalysts involves several steps: \\
        1. Synthesis of $\alpha$-MoC: Ammonium molybdate tetrahydrate is calcined to form MoO$_3$, which is then heated in ammonia and subsequently in a methane/hydrogen mixture to produce $\alpha$-MoC.\\
        2. Reduction of $\alpha$-MoC: The synthesized $\alpha$-MoC is reduced in a hydrogen/nitrogen mixture at 523 K for 60 minutes to create surface anchoring sites for platinum. \\
        3. Impregnation with Pt Precursor: A solution of H$_2$PtCl$_6$-6H$_2$O is mixed with the reduced $\alpha$-MoC powder, followed by drying in a vacuum oven. \\
        4. Reduction and Activation: The impregnated sample is mildly reduced at 623 K for 1 hour under hydrogen flow. Then, it's exposed to a methane/hydrogen mixture and heated to 863 K for 2 hours to reactivate the catalyst. 
    }
    }   
} \\
\\

Accordingly, by employing the Comprehend Articles feature, we gathered essential information regarding reaction conditions, catalytic performance for CO conversion, and other significant details from both studies identified through the data extraction process. With these insights, we now seek to identify gaps in the existing literature that will further guide us toward future research directions.      

\subsection*{Answer General Queries}
To facilitate research gap identification and idea generation, we pose the following question to the feature Answer General Queries of AceWGS: \\

\makebox[\textwidth][c]{%
    \fbox{%
        \parbox{0.9\textwidth}{%
         \textbf{Question:} Provide one or two catalyst design ideas based on the two existing catalysts: i. Pt $\alpha$-MoC catalyst and ii. Au $\alpha$-MoC catalyst. \\
         \textbf{Answer:} Here are a couple of catalyst design ideas: \\
         1. Alloying:  Combine Pt and Au in an $\alpha$-MoC support to potentially leverage the high activity of Pt at lower temperatures and the stability of Au at higher temperatures. This could create a catalyst with a broader operating temperature range.\\ 
         2. Tuning Particle Size Distribution: Explore varying the size distribution of Pt or Au nanoparticles on the $\alpha$-MoC support.  The text highlights that different sizes (isolated atoms, clusters, particles) can influence activity and stability. Optimizing this distribution might lead to enhanced performance. 
   }
    }   
} \\
\\

Both ideas presented above are quite compelling. We further investigate the first concept of "Alloying" by utilizing our AI Inverse Modelling feature.

\subsection*{AI Inverse Modelling}
Once we input platinum, gold, and $\alpha$-molybdenum carbide as the catalyst combination, select incipient wetness impregnation (IWI) as the preparation method, and set the reaction temperature below 300~$^\circ$C, the AI Inverse Modelling feature of AceWGS generates the following catalytic solution:\\

\makebox[\textwidth][c]{%
    \fbox{%
        \parbox{0.9\textwidth}{%
         \textbf{Question:} Run inverse model. \\
         \textbf{Answer:} Set catalyst design parameters in the pop-up box. \\
         (call the Parameters Settings tool)\\
         The inverse model is running. It might take a while to finish ... \\
         Finished. \\
         \textbf{Answer:} Found a catalytic solution of Pt (4.26\%), Au (3.09\%) with the support of $\alpha$-MoC (92.64\%) that can achieve maximum 95.07\% (error $\pm$ 0.79\%) CO conversion at 200~$^\circ$C. The catalyst preparation method is incipient wetness impregnation (IWI). The initial feed gases are CO (0.1\%), H$_2$O (6.18\%), CO$_2$ (5\%), H$_2$ (0.15\%), and N$_2$ (88.57\%). The time on stream is 1 hour. The ratio of catalyst weight to feed flow rate is 1 mg min/ml. 
   }
    }   
} \\
\\

The generated results now provide a comprehensive catalyst formulation, detailing the exact weight percentages for each catalyst component, the specific volume ratios of initial feed gases, and optimized reaction conditions. In addition to identifying the preparation method (IWI), AceWGS supplies an in-depth synthesis protocol, as retrieved through the Comprehend Articles feature. This level of detail enhances reproducibility and provides critical insights for further experimentation and validation.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion}
In conclusion, we present AceWGS, a versatile LLM-RAG framework designed to streamline AI-driven catalyst discovery for the water-gas shift (WGS) reaction. AceWGS provides four key features that enhance cross-disciplinary collaboration, enable efficient retrieval of state-of-the-art information, and optimize textual and numerical data extraction from literature, ultimately identifying promising catalytic candidates in a fraction of the time required by traditional methods. Purposefully constructed with open-source tools and moderate-sized LLMs, AceWGS establishes a prototype accessible to researchers with standard computational resources. It allows them to adapt and extend the framework for accelerated, cross-domain research in AI-powered materials design.

In the future, We will focus on expanding AceWGS by integrating advanced features, automating data retrieval directly from the literature, and streamlining dataset preparation for AI model training. These enhancements aim to improve efficiency and support more sophisticated AI-driven workflows for catalyst design.
   


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Acknowledgement}
This research is supported by Agency for Science Technology and Research (A*STAR) $\langle$C222812014$\rangle$.


%\appendix

%\section{Sample Appendix Section}
%\label{sec:sample:appendix}


%% If you have bibdatabase file and want bibtex to generate the
%% bibitems, please use
%%
\bibliographystyle{elsarticle-num} 
%\bibliography{WGS}

\begin{thebibliography}{10}
\expandafter\ifx\csname url\endcsname\relax
  \def\url#1{\texttt{#1}}\fi
\expandafter\ifx\csname urlprefix\endcsname\relax\def\urlprefix{URL }\fi
\expandafter\ifx\csname href\endcsname\relax
  \def\href#1#2{#2} \def\path#1{#1}\fi

\bibitem{Oensan2007}
Z.~{\.{I}}. \"Onsan, Catalytic {Processes} for {Clean} {Hydrogen} {Production} from {Hydrocarbons}, Turk J Chem 31~(5) (2007) 531--550, publisher: The Scientific and Technological Research Council of Turkey.

\bibitem{Park2009}
E.~D. Park, D.~Lee, H.~C. Lee, Recent progress in selective {CO} removal in a {H2}-rich stream, Catalysis Today 139~(4) (2009) 280--290.
\newblock \href {https://doi.org/10.1016/j.cattod.2008.06.027} {\path{doi:10.1016/j.cattod.2008.06.027}}.

\bibitem{Ebrahimi2020}
P.~Ebrahimi, A.~Kumar, M.~Khraisheh, A review of recent advances in water-gas shift catalysis for hydrogen production, Emergent Materials 3~(6) (2020) 881--917.
\newblock \href {https://doi.org/10.1007/s42247-020-00116-y} {\path{doi:10.1007/s42247-020-00116-y}}.

\bibitem{ZhouLiu2023}
L.~Zhou, Y.~Liu, S.~Liu, H.~Zhang, X.~Wu, R.~Shen, T.~Liu, J.~Gao, K.~Sun, B.~Li, J.~Jiang, For more and purer hydrogen-the progress and challenges in water gas shift reaction, Journal of Energy Chemistry 83 (2023) 363--396.
\newblock \href {https://doi.org/10.1016/j.jechem.2023.03.055} {\path{doi:10.1016/j.jechem.2023.03.055}}.

\bibitem{ShuiJZZLH2023}
Z.~Shui, G.~Jiang, M.~Zhao, Z.~Yang, G.~Li, Z.~Hao, Recent advances in atomically dispersed metal catalysts for low-temperature water-gas shift reaction, Current Opinion in Chemical Engineering 41 (2023) 100929.
\newblock \href {https://doi.org/10.1016/j.coche.2023.100929} {\path{doi:10.1016/j.coche.2023.100929}}.

\bibitem{Toyao2020}
T.~Toyao, Z.~Maeno, S.~Takakusagi, T.~Kamachi, I.~Takigawa, K.-i. Shimizu, Machine {Learning} for {Catalysis} {Informatics}: {Recent} {Applications} and {Prospects}, ACS Catalysis 10~(3) (2020) 2260--2297.
\newblock \href {https://doi.org/10.1021/acscatal.9b04186} {\path{doi:10.1021/acscatal.9b04186}}.

\bibitem{Benavides2024}
J.~Benavides-Hern\'andez, F.~Dumeignil, From characterization to discovery: Artificial intelligence, machine learning and high-throughput experiments for heterogeneous catalyst design, ACS Catalysis 14~(15) (2024) 11749--11779.
\newblock \href {https://doi.org/10.1021/acscatal.3c06293} {\path{doi:10.1021/acscatal.3c06293}}.

\bibitem{Odabasi2014}
{\c{C}}.~Odaba\c{s}{\i}, M.~E. G\"unay, R.~Y{\i}ld{\i}r{\i}m, Knowledge extraction for water gas shift reaction over noble metal catalysts from publications in the literature between 2002 and 2012, International Journal of Hydrogen Energy 39~(11) (2014) 5733--5746.
\newblock \href {https://doi.org/10.1016/j.ijhydene.2014.01.160} {\path{doi:10.1016/j.ijhydene.2014.01.160}}.

\bibitem{Avsar2017}
E.~Av\c{s}ar, Dimensionality reduction for predicting {CO} conversion in water gas shift reaction over {Pt}-based catalysts using support vector regression models, International Journal of Hydrogen Energy 42~(36) (2017) 23326--23333.
\newblock \href {https://doi.org/10.1016/j.ijhydene.2016.12.091} {\path{doi:10.1016/j.ijhydene.2016.12.091}}.

\bibitem{Cavalcanti2019}
F.~M. Cavalcanti, M.~Schmal, R.~Giudici, R.~M. Brito~Alves, A catalyst selection method for hydrogen production through {Water}-{Gas} {Shift} {Reaction} using artificial neural networks, Journal of Environmental Management 237 (2019) 585--594.
\newblock \href {https://doi.org/10.1016/j.jenvman.2019.02.092} {\path{doi:10.1016/j.jenvman.2019.02.092}}.

\bibitem{Suzuki2019}
K.~Suzuki, T.~Toyao, Z.~Maeno, S.~Takakusagi, K.-i. Shimizu, I.~Takigawa, Statistical {Analysis} and {Discovery} of {Heterogeneous} {Catalysts} {Based} on {Machine} {Learning} from {Diverse} {Published} {Data}, ChemCatChem 11~(18) (2019) 4537--4547.
\newblock \href {https://doi.org/10.1002/cctc.201900971} {\path{doi:10.1002/cctc.201900971}}.

\bibitem{Smith2020}
A.~Smith, A.~Keane, J.~A. Dumesic, G.~W. Huber, V.~M. Zavala, A machine learning framework for the analysis and prediction of catalytic activity from experimental data, Applied Catalysis B: Environmental 263 (2020) 118257.
\newblock \href {https://doi.org/10.1016/j.apcatb.2019.118257} {\path{doi:10.1016/j.apcatb.2019.118257}}.

\bibitem{Garcia2023}
F.~{Garcia-Escobar}, S.~{Nishimura}, K.~{Takahashi}, {Data-Driven Design and Understanding of Noble Metal-Based Water--Gas Shift Catalysts from Literature Data}, {The Journal of Physical Chemistry C} 127~(13) (2023) 6152--6166.
\newblock \href {https://doi.org/10.1021/acs.jpcc.2c09132} {\path{doi:10.1021/acs.jpcc.2c09132}}.

\bibitem{golder2024machine}
R.~Golder, S.~Pal, K.~Ray, et~al., Machine learning-enhanced optimal catalyst selection for water-gas shift reaction, Digital Chemical Engineering 12 (2024) 100165.
\newblock \href {https://doi.org/10.1016/j.dche.2024.100165} {\path{doi:10.1016/j.dche.2024.100165}}.

\bibitem{Chattoraj2022}
J.~Chattoraj, B.~Hamadicharef, J.~F. Kong, M.~K. Pargi, Y.~Zeng, C.~K. Poh, L.~Chen, F.~Gao, T.~L. Tan, Theory-guided machine learning to predict the performance of noble metal catalysts in the water-gas shift reaction, ChemCatChem 14~(16) (2022) e202200355.
\newblock \href {https://doi.org/10.1002/cctc.202200355} {\path{doi:10.1002/cctc.202200355}}.

\bibitem{Chattoraj2023}
J.~Chattoraj, B.~Hamadicharef, Y.~N.~A. Syadzali, G.~O. Limantara, Y.~Zeng, C.~K. Poh, L.~Chen, T.~L. Tan, Preparation of a water--gas shift database to evaluate the performance of noble metal catalysts using theory-guided machine learning, ACS Catalysis 13~(21) (2023) 14334--14345.
\newblock \href {https://doi.org/10.1021/acscatal.3c04467} {\path{doi:10.1021/acscatal.3c04467}}.

\bibitem{fan2024survey}
W.~Fan, Y.~Ding, L.~Ning, S.~Wang, H.~Li, D.~Yin, T.-S. Chua, Q.~Li, A survey on rag meeting llms: Towards retrieval-augmented large language models, in: Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, 2024, pp. 6491--6501.
\newblock \href {https://doi.org/10.1145/3637528.367147} {\path{doi:10.1145/3637528.367147}}.

\bibitem{kumar2024can}
S.~Kumar, T.~Ghosal, V.~Goyal, A.~Ekbal, \href{https://arxiv.org/abs/2409.06185}{Can large language models unlock novel scientific research ideas?}, arXiv preprint arXiv:2409.06185 (2024).
\newblock \href {https://doi.org/10.48550/arXiv.2409.06185} {\path{doi:10.48550/arXiv.2409.06185}}.
\newline\urlprefix\url{https://arxiv.org/abs/2409.06185}

\bibitem{si2024can}
C.~Si, D.~Yang, T.~Hashimoto, \href{https://arxiv.org/abs/2409.04109}{{Can LLMs Generate Novel Research Ideas? A Large-Scale Human Study with 100+ NLP Researchers}}, arXiv preprint arXiv:2409.04109 (2024).
\newblock \href {https://doi.org/10.48550/arXiv.2409.04109} {\path{doi:10.48550/arXiv.2409.04109}}.
\newline\urlprefix\url{https://arxiv.org/abs/2409.04109}

\bibitem{kim2024large}
S.~Kim, Y.~Jung, J.~Schrier, Large language models for inorganic synthesis predictions, Journal of the American Chemical Society 146~(29) (2024) 19654--19659.
\newblock \href {https://doi.org/10.1021/jacs.4c05840} {\path{doi:10.1021/jacs.4c05840}}.

\bibitem{wang2024catalm}
L.~Wang, X.~Chen, Y.~Du, Y.~Zhou, Y.~Gao, W.~Cui, \href{CoRR abs/2405.17440}{Catalm: Empowering catalyst design through large language models}, arXiv preprint arXiv:2405.17440 (2024).
\newline\urlprefix\url{CoRR abs/2405.17440}

\bibitem{bran2024augmenting}
A.~M.~Bran, S.~Cox, O.~Schilter, C.~Baldassari, A.~D. White, P.~Schwaller, Augmenting large language models with chemistry tools, Nature Machine Intelligence 6 (2024) 525--535.
\newblock \href {https://doi.org/10.1038/s42256-024-00832-8} {\path{doi:10.1038/s42256-024-00832-8}}.

\bibitem{Python}
\href{https://www.python.org/downloads/}{{Python Software Foundation}} (2024 (accessed 1st November 2024)).
\newline\urlprefix\url{https://www.python.org/downloads/}

\bibitem{Tkinter}
\href{https://docs.python.org/3.11/library/tkinter.html}{{Tkinter - Python interface to Tcl/Tk}} (2024 (accessed 1st November 2024)).
\newline\urlprefix\url{https://docs.python.org/3.11/library/tkinter.html}

\bibitem{Langchain}
\href{https://www.langchain.com}{{LangChain - Applications that can reason}} (2024 (accessed 1st November 2024)).
\newline\urlprefix\url{https://www.langchain.com}

\bibitem{Ollama}
\href{https://ollama.com}{{Ollama - Get up and running with large language models}} (2024 (accessed 1st November 2024)).
\newline\urlprefix\url{https://ollama.com}

\bibitem{pandas}
\href{https://pandas.pydata.org}{{pandas - Python Data Analysis Library}} (2024 (accessed 1st November 2024)).
\newline\urlprefix\url{https://pandas.pydata.org}

\bibitem{dfAgent}
\href{https://python.langchain.com/api\_reference/experimental/\\ agents/langchain\_experimental.agents.agent\_toolkits.\\ pandas.base.create\_pandas\_dataframe\_agent.html\\ \#langchain\_experimental.agents.agent\_toolkits.pandas.base.\\ create\_pandas\_dataframe\_agent}{{Pandas Dataframe Agent}} (2024 (accessed 1st November 2024)).
\newline\urlprefix\url{https://python.langchain.com/api\_reference/experimental/\\ agents/langchain\_experimental.agents.agent\_toolkits.\\ pandas.base.create\_pandas\_dataframe\_agent.html\\ \#langchain\_experimental.agents.agent\_toolkits.pandas.base.\\ create\_pandas\_dataframe\_agent}

\bibitem{PDFMiner}
\href{https://pypi.org/project/pdfminer/}{{PDFMiner - Text extraction tool for PDF documents}} (2024 (accessed 1st November 2024)).
\newline\urlprefix\url{https://pypi.org/project/pdfminer/}

\bibitem{OllamaEmbeddings}
\href{https://python.langchain.com/docs/integrations/text\_embedding/ollama/}{{Ollama Embeddings}} (2024 (accessed 1st November 2024)).
\newline\urlprefix\url{https://python.langchain.com/docs/integrations/text\_embedding/ollama/}

\bibitem{FAISS}
\href{https://ai.meta.com/tools/faiss/}{{FAISS - Facebook AI Similarity Search}} (2024 (accessed 1st November 2024)).
\newline\urlprefix\url{https://ai.meta.com/tools/faiss/}

\bibitem{petallidou2013water}
K.~C. Petallidou, K.~Polychronopoulou, S.~Boghosian, S.~Garcia-Rodriguez, A.~M. Efstathiou, Water--gas shift reaction on pt/ce1--x ti x o2- $\delta$: The effect of ce/ti ratio, The Journal of Physical Chemistry C 117~(48) (2013) 25467--25477.
\newblock \href {https://doi.org/10.1021/jp406059h} {\path{doi:10.1021/jp406059h}}.

\bibitem{runxia2016cu}
H.~Runxia, W.~Dandan, Z.~Keduan, W.~Bin, H.~Zhong, H.~Jiang, L.~Na, L.~Quansheng, Cu-mn catalysts modified by rare earth lantnaum for low temperature water-gas shift reaction, Journal of Rare Earths 34~(10) (2016) 994--1003.
\newblock \href {https://doi.org/10.1016/S1002-0721(16)60126-6} {\path{doi:10.1016/S1002-0721(16)60126-6}}.

\bibitem{cai2021hydrogen}
G.~Cai, Y.~He, H.~Ren, Y.~Zhan, C.~Chen, Y.~Luo, L.~Jiang, Hydrogen production via water--gas shift reaction by cu/sio2 catalyst: A case study of ceo2 doping, Energy \& Fuels 35~(4) (2021) 3521--3528.
\newblock \href {https://doi.org/10.1021/acs.energyfuels.0c04192} {\path{doi:10.1021/acs.energyfuels.0c04192}}.

\bibitem{yao2017atomic}
S.~Yao, X.~Zhang, W.~Zhou, R.~Gao, W.~Xu, Y.~Ye, L.~Lin, X.~Wen, P.~Liu, B.~Chen, et~al., Atomic-layered au clusters on $\alpha$-moc as catalysts for the low-temperature water-gas shift reaction, Science 357~(6349) (2017) 389--393.
\newblock \href {https://doi.org/10.1126/science.aah432} {\path{doi:10.1126/science.aah432}}.

\bibitem{zhang2021stable}
X.~Zhang, M.~Zhang, Y.~Deng, M.~Xu, L.~Artiglia, W.~Wen, R.~Gao, B.~Chen, S.~Yao, X.~Zhang, et~al., A stable low-temperature h2-production catalyst by crowding pt on $\alpha$-moc, Nature 589~(7842) (2021) 396--401.
\newblock \href {https://doi.org/10.1038/s41586-020-03130-6} {\path{doi:10.1038/s41586-020-03130-6}}.

\end{thebibliography}

% \end{thebibliography}
\end{document}
\endinput
%%
%% End of file `elsarticle-template-num.tex'.










