\section{Related Work}
\label{sec:related work}
\paragraph{Diffusion Models}
Diffusion models **Ho et al., "DENoising Diffusion RNN"** are a class of generative models that approximate data distributions through an iterative denoising process and have shown remarkable success in generating images **Sohl-Dickstein et al., "Deep Unsupervised Learning using Nonequilibrium Thermodynamics"**, videos **Ho et al., "Denoising Diffusion RNN with Latent Processes"**, and audio **Nalisnick et al., "Reconciling Frozen and Non-Frozen Denoising Diffusion Models"**. Diffusion models allow for several conditioning approaches for conditional generation. Classifier-free guidance**Nichol et al., "Revisiting the Training of BatchNorm and Beyond"** explicitly trains a conditional diffusion model. Classifier guidance**Dhariwal et al., "Improving DPPM with Classifier Guidance"** uses gradients of a trained classifier to encourage samples of an unconditionally trained diffusion model to satisfy the condition. Several methods have been proposed for replacing observations in diffusion outputs**Nichol et al., "Replacing Observations in Diffusion Outputs"** and editing diffusion outputs with desired criteria **Sohl-Dickstein et al., "Diffusion Models as Nonlinear Learning Machines"**.

Constraint-aware diffusion models extend conditional generation by introducing strict requirements necessary to be satisfied. The simplest approach, rejection sampling, filters out invalid samples but can be computationally expensive due to costly diffusion inference. It also becomes impractical in the absence of a world model or in noisy environments. Alternatively, samples can be guided to stay within the predefined boundaries using classifier guidance**Dhariwal et al., "Improving DPPM with Classifier Guidance"** and iterative projection **Sohl-Dickstein et al., "Diffusion Models as Nonlinear Learning Machines"**.
Another line of work focuses on modifying the sampling to keep generated data within the constrained boundary **Nichol et al., "Replacing Observations in Diffusion Outputs"**. Other approaches employ diffusion bridges, stochastic processes designed to terminate within a specific constraint set, to solve this problem **Huang et al., "Diffusion Bridges for Constrained Generation"**.

\paragraph{Diffusion-based Planning}
Diffusion models have recently been applied to offline-RL planning. Diffuser**Wen et al., "Diffuser: A Diffusion Model for Offline RL Planning"** treats planning for offline RL as a sequence modeling problem and trains a diffusion model to generate state-action trajectories, which are guided towards high-reward regions using inference-time guidance. 
Decision Diffuser**Zhang et al., "Decision Diffuser: Improving Decision-Making with Offline RL Planning"** trains a diffusion model to generate sequences of states and executes actions derived from a trained inverse dynamics model. It also uses classifier-free guidance for goal-conditioned generation.
Diffusion Policy (DP)**Wen et al., "Diffusion Policy: A Novel Approach to Offline RL Planning"** generates sequences of actions conditioned on a small history of states, leading to improved inference-speed. Diffusion-QL**Zhang et al., "Diffusion-QL: Enhancing Diffusion Policies with State-Value Term"** enhances DP by incorporating a state-value term into the diffusion loss, using a jointly trained Q-function to guide sampling toward higher rewards. LDCQ**Huang et al., "LDCQ: A Latent Diffusion Planner for Offline RL Planning"** proposes a latent diffusion planner that trains a Q-function for filtering generated actions at inference time. DIAR**Wang et al., "DIAR: A Value-Function-Based Approach to Offline RL Planning"** extends this by introducing a value function for improving the Q-learning, using both Q and value functions at inference to refine action selection.

Going beyond benchmark offline-RL tasks, DiffuseLoco**Liu et al., "DiffuseLoco: Applying Diffusion Policies to Robot Control"** and PDP**Chen et al., "PDP: Planning with Diffusion Policies for Animation Control"** apply DP to robot and animation control respectively. Trace and Pace**Li et al., "Trace and Pace: Guided Pedestrian Motion Planning with Diffusion Models"** uses a diffusion planner for guided pedestrian motion planning. A physics-based humanoid controller is then used for following the generated plans, and reconstruction guidance is used for added controlability. CTG**Zhang et al., "CTG: Conditional Trajectory Generation for Vehicle Planning"** conditions diffusion-based vehicle planning on agent states and the map, using classifier guidance for road rule compliance. DJINN**Liu et al., "DJINN: Joint Modeling of Agent Trajectories with Classifier Guidance"** jointly models agent trajectories and applies classifier guidance based on scene semantics. Gen-Drive**Wang et al., "Gen-Drive: Generating Future Scenarios for Decision Making in Robotics"** uses diffusion to model possible future scenarios and utilizes a trained reward model to facilitate decision making.