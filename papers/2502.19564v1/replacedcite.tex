\section{Related Work}
\label{sec:related work}
\paragraph{Diffusion Models}
Diffusion models ____ are a class of generative models that approximate data distributions through an iterative denoising process and have shown remarkable success in generating images ____, videos ____, and audio ____. Diffusion models allow for several conditioning approaches for conditional generation. Classifier-free guidance____ explicitly trains a conditional diffusion model. Classifier guidance____ uses gradients of a trained classifier to encourage samples of an unconditionally trained diffusion model to satisfy the condition. Several methods have been proposed for replacing observations in diffusion outputs____ and editing diffusion outputs with desired criteria ____. 

Constraint-aware diffusion models extend conditional generation by introducing strict requirements necessary to be satisfied. The simplest approach, rejection sampling, filters out invalid samples but can be computationally expensive due to costly diffusion inference. It also becomes impractical in the absence of a world model or in noisy environments. Alternatively, samples can be guided to stay within the predefined boundaries using classifier guidance____ and iterative projection ____.
Another line of work focuses on modifying the sampling to keep generated data within the constrained boundary ____. Other approaches employ diffusion bridges, stochastic processes designed to terminate within a specific constraint set, to solve this problem ____.

\paragraph{Diffusion-based Planning}
Diffusion models have recently been applied to offline-RL planning. Diffuser____ treats planning for offline RL as a sequence modeling problem and trains a diffusion model to generate state-action trajectories, which are guided towards high-reward regions using inference-time guidance. 
Decision Diffuser____ trains a diffusion model to generate sequences of states and executes actions derived from a trained inverse dynamics model. It also uses classifier-free guidance for goal-conditioned generation.
Diffusion Policy (DP)____ generates sequences of actions conditioned on a small history of states, leading to improved inference-speed. Diffusion-QL____ enhances DP by incorporating a state-value term into the diffusion loss, using a jointly trained Q-function to guide sampling toward higher rewards. LDCQ____ proposes a latent diffusion planner that trains a Q-function for filtering generated actions at inference time. DIAR____ extends this by introducing a value function for improving the Q-learning, using both Q and value functions at inference to refine action selection.

Going beyond benchmark offline-RL tasks, DiffuseLoco____ and PDP____ apply DP to robot and animation control respectively. Trace and Pace____ uses a diffusion planner for guided pedestrian motion planning. A physics-based humanoid controller is then used for following the generated plans, and reconstruction guidance is used for added controlability. CTG____ conditions diffusion-based vehicle planning on agent states and the map, using classifier guidance for road rule compliance. DJINN____ jointly models agent trajectories and applies classifier guidance based on scene semantics. Gen-Drive____ uses diffusion to model possible future scenarios and utilizes a trained reward model to facilitate decision making.