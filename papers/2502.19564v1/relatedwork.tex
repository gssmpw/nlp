\section{Related Work}
\label{sec:related work}
\paragraph{Diffusion Models}
Diffusion models \citep{sohl2015deep, ho2020denoising, song2020denoising, nichol2021improved} are a class of generative models that approximate data distributions through an iterative denoising process and have shown remarkable success in generating images \citep{dhariwal2021diffusion, ramesh2022hierarchical, rombach2022high}, videos \citep{ho2022video, harvey2022flexible}, and audio \citep{kong2020diffwave}. Diffusion models allow for several conditioning approaches for conditional generation. Classifier-free guidance~\cite{ho2022classifier} explicitly trains a conditional diffusion model. Classifier guidance~\cite{dhariwal2021diffusion} uses gradients of a trained classifier to encourage samples of an unconditionally trained diffusion model to satisfy the condition. Several methods have been proposed for replacing observations in diffusion outputs~\cite{lugmayr2022repaint, ho2022video} and editing diffusion outputs with desired criteria ~\cite{meng2021sdedit, parmar2023zero}. 

Constraint-aware diffusion models extend conditional generation by introducing strict requirements necessary to be satisfied. The simplest approach, rejection sampling, filters out invalid samples but can be computationally expensive due to costly diffusion inference. It also becomes impractical in the absence of a world model or in noisy environments. Alternatively, samples can be guided to stay within the predefined boundaries using classifier guidance~\citep{pmlr-v235-naderiparizi24a} and iterative projection ~\citep{christopher2024constrained}.
Another line of work focuses on modifying the sampling to keep generated data within the constrained boundary \citep{lou2023reflected, fishman2024metropolis, liu2024mirror}. Other approaches employ diffusion bridges, stochastic processes designed to terminate within a specific constraint set, to solve this problem \citep{liu2023learning}.

\paragraph{Diffusion-based Planning}
Diffusion models have recently been applied to offline-RL planning. Diffuser~\citep{janner2022diffuser} treats planning for offline RL as a sequence modeling problem and trains a diffusion model to generate state-action trajectories, which are guided towards high-reward regions using inference-time guidance. 
Decision Diffuser~\citep{ajay2022conditional} trains a diffusion model to generate sequences of states and executes actions derived from a trained inverse dynamics model. It also uses classifier-free guidance for goal-conditioned generation.
Diffusion Policy (DP)~\citep{chi2023diffusion} generates sequences of actions conditioned on a small history of states, leading to improved inference-speed. Diffusion-QL~\citep{wang2022diffusion} enhances DP by incorporating a state-value term into the diffusion loss, using a jointly trained Q-function to guide sampling toward higher rewards. LDCQ~\citep{venkatraman2023reasoning} proposes a latent diffusion planner that trains a Q-function for filtering generated actions at inference time. DIAR~\citep{park2024diar} extends this by introducing a value function for improving the Q-learning, using both Q and value functions at inference to refine action selection.

Going beyond benchmark offline-RL tasks, DiffuseLoco~\citep{huang2024diffuseloco} and PDP~\citep{truong2024pdp} apply DP to robot and animation control respectively. Trace and Pace~\citep{rempeluo2023tracepace} uses a diffusion planner for guided pedestrian motion planning. A physics-based humanoid controller is then used for following the generated plans, and reconstruction guidance is used for added controlability. CTG~\cite{zhong2023guided} conditions diffusion-based vehicle planning on agent states and the map, using classifier guidance for road rule compliance. DJINN~\cite{niedoba2024diffusion} jointly models agent trajectories and applies classifier guidance based on scene semantics. Gen-Drive~\citep{huang2024gen} uses diffusion to model possible future scenarios and utilizes a trained reward model to facilitate decision making.