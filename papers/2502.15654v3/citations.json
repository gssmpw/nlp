[
  {
    "index": 0,
    "papers": [
      {
        "key": "shumailov2023curse",
        "author": "Shumailov, Ilia and Shumaylov, Zakhar and Zhao, Yiren and Gal, Yarin and Papernot, Nicolas and Anderson, Ross",
        "title": "{The curse of recursion: Training on generated data makes models forget}"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "dohmatob2024tale",
        "author": "Elvis Dohmatob and Yunzhen Feng and Pu Yang and Francois Charton and Julia Kempe",
        "title": "{A Tale of Tails: Model Collapse as a Change of Scaling Laws}"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "shumailov2023curse",
        "author": "Shumailov, Ilia and Shumaylov, Zakhar and Zhao, Yiren and Gal, Yarin and Papernot, Nicolas and Anderson, Ross",
        "title": "{The curse of recursion: Training on generated data makes models forget}"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "dohmatob2024tale",
        "author": "Elvis Dohmatob and Yunzhen Feng and Pu Yang and Francois Charton and Julia Kempe",
        "title": "{A Tale of Tails: Model Collapse as a Change of Scaling Laws}"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "shumailov2024ai",
        "author": "Shumailov, Ilia and Shumaylov, Zakhar and Zhao, Yiren and Papernot, Nicolas and Anderson, Ross and Gal, Yarin",
        "title": "{AI models collapse when trained on recursively generated data}"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "gerstgrasser_is_2024",
        "author": "Gerstgrasser, Matthias and Schaeffer, Rylan and Dey, Apratim and Rafailov, Rafael and Sleight, Henry and Hughes, John and Korbak, Tomasz and Agrawal, Rajashree and Pai, Dhruv and Gromov, Andrey and others",
        "title": "{Is Model Collapse Inevitable? Breaking the Curse of Recursion by Accumulating Real and Synthetic Data}"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "guo-etal2024-curious",
        "author": "Guo, Yanzhu  and\nShang, Guokan  and\nVazirgiannis, Michalis  and\nClavel, Chlo{\\'e}",
        "title": "{The Curious Decline of Linguistic Diversity: Training Language Models on Synthetic Text}"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "bertrand2024on",
        "author": "Quentin Bertrand and Joey Bose and Alexandre Duplessis and Marco Jiralerspong and Gauthier Gidel",
        "title": "{On the Stability of Iterative Retraining of Generative Models on their own Data}"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "alemohammad_self-consuming_2023",
        "author": "Sina Alemohammad and Josue Casco-Rodriguez and Lorenzo Luzi and Ahmed Imtiaz Humayun and Hossein Babaei and Daniel LeJeune and Ali Siahkoohi and Richard Baraniuk",
        "title": "{Self-Consuming Generative Models Go MAD}"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "gerstgrasser_is_2024",
        "author": "Gerstgrasser, Matthias and Schaeffer, Rylan and Dey, Apratim and Rafailov, Rafael and Sleight, Henry and Hughes, John and Korbak, Tomasz and Agrawal, Rajashree and Pai, Dhruv and Gromov, Andrey and others",
        "title": "{Is Model Collapse Inevitable? Breaking the Curse of Recursion by Accumulating Real and Synthetic Data}"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "feng_beyond_2024",
        "author": "Yunzhen Feng and Elvis Dohmatob and Pu Yang and Francois Charton and Julia Kempe",
        "title": "{Beyond Model Collapse: Scaling Up with Synthesized Data Requires Reinforcement}"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "zhang_regurgitative_2024",
        "author": "Zhang, Jinghui and Qiao, Dandan and Yang, Mochen and Wei, Qiang",
        "title": "{Regurgitative training: The value of real data in training large language models}"
      }
    ]
  }
]