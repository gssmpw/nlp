@article{gerstgrasser_is_2024,
  title={{Is Model Collapse Inevitable? Breaking the Curse of Recursion by Accumulating Real and Synthetic Data}},
  author={Gerstgrasser, Matthias and Schaeffer, Rylan and Dey, Apratim and Rafailov, Rafael and Sleight, Henry and Hughes, John and Korbak, Tomasz and Agrawal, Rajashree and Pai, Dhruv and Gromov, Andrey and others},
  journal={arXiv preprint arXiv:2404.01413},
  year={2024}
}

@inproceedings{guo-etal2024-curious,
    title ={{The Curious Decline of Linguistic Diversity: Training Language Models on Synthetic Text}},
    author = "Guo, Yanzhu  and
      Shang, Guokan  and
      Vazirgiannis, Michalis  and
      Clavel, Chlo{\'e}",
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Findings of the Association for Computational Linguistics: NAACL 2024",
    month = jun,
    year = "2024",
    url = "https://aclanthology.org/2024.findings-naacl.228/",
    doi = "10.18653/v1/2024.findings-naacl.228",
    pages = "3589--3604",
    abstract = "This study investigates the consequences of training language models on synthetic data generated by their predecessors, an increasingly prevalent practice given the prominence of powerful generative models. Diverging from the usual emphasis on performance metrics, we focus on the impact of this training methodology on linguistic diversity, especially when conducted recursively over time. To assess this, we adapt and develop a set of novel metrics targeting lexical, syntactic, and semantic diversity, applying them in recursive finetuning experiments across various natural language generation tasks in English. Our findings reveal a consistent decrease in the diversity of the model outputs through successive iterations, especially remarkable for tasks demanding high levels of creativity. This trend underscores the potential risks of training language models on synthetic text, particularly concerning the preservation of linguistic richness. Our study highlights the need for careful consideration of the long-term effects of such training approaches on the linguistic capabilities of language models."
}

@article{shumailov2023curse,
  title={{The curse of recursion: Training on generated data makes models forget}},
  author={Shumailov, Ilia and Shumaylov, Zakhar and Zhao, Yiren and Gal, Yarin and Papernot, Nicolas and Anderson, Ross},
  journal={arXiv preprint arXiv:2305.17493},
  year={2023}
}

@article{shumailov2024ai,
  title={{AI models collapse when trained on recursively generated data}},
  author={Shumailov, Ilia and Shumaylov, Zakhar and Zhao, Yiren and Papernot, Nicolas and Anderson, Ross and Gal, Yarin},
  journal={Nature},
  volume={631},
  number={8022},
  pages={755--759},
  year={2024},
}

@article{zhang_regurgitative_2024,
  title={{Regurgitative training: The value of real data in training large language models}},
  author={Zhang, Jinghui and Qiao, Dandan and Yang, Mochen and Wei, Qiang},
  journal={arXiv preprint arXiv:2407.12835},
  year={2024}
}

