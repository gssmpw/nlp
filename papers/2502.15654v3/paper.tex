% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

\usepackage{bbm}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

\usepackage{amsmath}
\usepackage{amsfonts}

\usepackage[inkscapelatex=false]{svg}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

\usepackage{booktabs}

\usepackage{algorithm}
\usepackage{algpseudocode}

% improves the aesthetics of text in the typewriter font.
\usepackage{inconsolata}

% Including images in your LaTeX document requires adding additional package(s)
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{enumitem}

\usepackage{balance}


% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Machine-generated text detection prevents language model collapse}

  % \author{George Drayson \\
  % Center for Artificial Intelligence\\ 
  % Department of Computer Science \\
  % University College London, UK \\
  % \texttt{george.drayson.23@ucl.ac.uk} \\\And
  % Vasileios Lampos \\
  % AI Centre \\
  % Department of Computer Science \\
  % University College London, UK \\
  % \texttt{v.lampos@ucl.ac.uk} \\}

% copying from another paper
\author{George Drayson, Emine Yilmaz, Vasileios Lampos\\
  Centre for Artificial Intelligence\\
  Department of Computer Science\\
  University College London, UK\\
  \{\texttt{george.drayson.23},\,\texttt{emine.yilmaz},\,\texttt{v.lampos}\}\texttt{@ucl.ac.uk}}
  
  
\begin{document}
\maketitle
\begin{abstract}
As Large Language Models (LLMs) become increasingly prevalent, their generated outputs are proliferating across the web, risking a future where machine-generated content dilutes human-authored text. Since online data is the primary resource for LLM pre-training, subsequent models could be trained on an unknown portion of synthetic samples. This will lead to model collapse, a degenerative process whereby LLMs reinforce their own errors, and ultimately yield a declining performance. In this study, we investigate the impact of decoding strategy on model collapse, analysing the characteristics of text at each model generation, the similarity to human references, and the resulting model performance. Using the decoding strategies that lead to the most significant degradation, we evaluate model collapse in more realistic scenarios where the origin of the data (human or synthetic) is unknown. We train a machine-generated text detector and propose an importance sampling approach to alleviate model collapse. Our method is validated on two LLM variants (\texttt{GPT-2} and \texttt{SmolLM2}) on the open-ended text generation task. We demonstrate that it can not only prevent model collapse but also improve performance when sufficient human-authored samples are present.
\end{abstract}

\section{Introduction}
%
Large Language Models (LLMs) can generate high-quality, human-like language across a wide range of applications. A key factor that drives their capabilities is the vast amount of data used to train them, which is predominantly based on text published on the web \cite{wenzeketal-2020-ccnet}.
%
The extensive adoption of LLMs will inevitably result in an ever-increasing amount of synthetic (machine-generated) data that will co-exist alongside or even dominate over human-generated text~\cite{dohmatob2024tale}, especially within online ecosystems such as social media platforms, news websites, and digital encyclopedias. 
%
Hence, there are legitimate concerns as to the effect this will have on future generations of language models trained on a mixed set of human and synthetic corpora.

Several works have attempted to simulate this scenario by recursively training language models on LLM-generated output~\cite{shumailov2023curse, briesch_large_2024, alemohammad_self-consuming_2023}. They show that this recursive training loop affects model performance, reflected by an increased perplexity among other metrics.
% 
The outcome of this recursive training process is referred to as ``\emph{model collapse}''~\cite{shumailov2023curse}, a degenerative process caused by training on synthetic data from previous generations, leading to compounded errors and models that have converged to a low variance distribution that is significantly dissimilar to the original model trained on human data. This leads to models with significant performance degradation~\cite{alemohammad_self-consuming_2023} and a drastic loss in diversity~\cite{briesch_large_2024, guo-etal2024-curious, alemohammad_self-consuming_2023}. Related literature has proposed some ways to mitigate against this, such as using synthetic samples as negative guidance in the image domain~\cite{alemohammad_self_improving_2024}, selecting synthetic samples based on high perplexity scores~\cite{feng_beyond_2024}, token-level editing \cite{zhu2024synthesize} or filtering low-quality samples~\cite{zhang_regurgitative_2024}. However, in these prior works, the models are either trained on entirely synthetic data or the labels of the samples are known a priori which is not necessarily reflective of more realistic settings.

Our work explores the scenario where the origin of the samples is unknown (human or synthetic). We propose a method for preventing model collapse by a guided resampling of the training samples using a machine-generated text detector. Our method is motivated by prior work~\cite{bertrand2024on,alemohammad_self-consuming_2023} which highlighted that when the proportion of human data in the training set is at a sufficient level, model collapse can be prevented. In addition to the current task-focused metrics used to assess model collapse, we evaluate the quality of the data from each model generation and its semantic resemblance to the original human samples. In this context, we also explore the effect of decoding strategy on the extent of model collapse and look into the characteristics of the synthetic data that could be causing this. 

\noindent Our contributions can be summarised as follows:
%
\begin{enumerate}[label=(\alph*), leftmargin=*, topsep=0.5pt, itemsep=-4pt]
    \item we evaluate model collapse from three perspectives: task performance, model generation quality, and semantic similarity to human text,
    \item we show that model collapse is significantly affected by the choice of decoding strategy, demonstrating large discrepancies in performance and data quality,
    \item we train a machine-generated text detector to provide calibrated confidence estimates for the origin of the training samples,
    \item we propose a method that uses the detector's outputs to prevent model collapse, and
    \item we present experiments on two LLMs across a range of decoding strategies.
\end{enumerate}

\section{Prior work on model collapse}
\label{sec:model_collapse}
%
Model collapse is a degenerative process in which models recursively trained on generational data exhibit a drop in performance compared to a model trained on the original human distribution~\cite{shumailov2023curse}. In the early stages of recursive training, information is lost at the tails of the distribution and eventually, the output distribution converges to a point estimate with very little variance, resulting in a model that cannot be restored back to the original generation trained on human data. This effect can also be viewed as a change to neural scaling laws, in which there reaches a point where training on additional synthetic samples does not improve model performance and learning plateaus~\cite{dohmatob2024tale}.

It has been argued that the two causes for this behaviour are finite sampling error leading to information being lost at the tails of the distribution, and functional approximation error introducing non-zero likelihoods outside of the support of the original distribution~\cite{shumailov2023curse}. Additionally, \citet{dohmatob2024tale} theorised that the choice of generation algorithm is another contributing factor to model collapse. However, this has not been empirically evaluated in the case of LLMs, where decoding strategies that truncate the probability could have a significant impact. Currently, model collapse in LLMs has been studied with a fixed decoding strategy and model degradation has been mostly assessed using task performance metrics such as perplexity~\cite{shumailov2024ai} and test loss~\cite{gerstgrasser_is_2024}. Interestingly,~\citet{guo-etal2024-curious} also looks into the diversity of the generated text. Hence, in our study, we have chosen to study model collapse across three perspectives: the quality of the generated text (including diversity and readability), its similarity to human text, and the model task performance.

% \subsection{Prior work on preventing model collapse}
%
Recent studies have explored methods for mitigating model collapse. \citet{bertrand2024on}~and~\citet{alemohammad_self-consuming_2023} show that when a high enough proportion of human data is added to the training samples at each iteration, model collapse in diffusion models can be avoided. In the computational linguistics domain,~\citet{gerstgrasser_is_2024} showed that by accumulating all cross-generational data and combining it with the original human data, model collapse can be significantly mitigated. \citet{feng_beyond_2024}~and~\citet{zhang_regurgitative_2024} investigated model collapse in a purely synthetic setting. They proposed heuristic methods for selecting samples, achieving performance akin to training on the original human data. However, in these prior works, the models are trained on either entirely synthetic data or the true labels of the samples are known. In our work, we investigate how to avoid model collapse in a more realistic setting where the training data is mixed and the origin (human or synthetic) of the samples 
% and hence the class distribution 
is unknown.

\section{Methods}
\label{sec:background}
In this section, we provide an overview of the methods and metrics used in our experiments, including the details of the machine-generated text detector.

\subsection{Language models and decoding strategies}
\label{subsec:language_model}
%
Language models learn a probability distribution $p_\theta\!\left(\mathbf{x}\right)$ for a sequence of tokens $\mathbf{x}\!=\!\{x_1, \dots, x_{|\mathbf{x}|}\}$, where $\theta$ denotes the model parameters. In this work, we study open-ended text generation, in which a sequence of $m$ tokens, $\mathbf{x}\!=\!\{x_1,\dots,x_m\}$, is provided as context and the task is to generate a continuation, $\mathbf{\hat{x}}\!=\!\{\hat{x}_{1},\dots,\hat{x}_{c}\}$, from the model's probability distribution, as in
%
\begin{equation}
    p_{\theta}\!\left(\hat{\mathbf{x}}\right) = \prod_{i=1}^{c} p_{\theta}\!\left(\hat{x}_i\mid \{\mathbf{x},\hat{\mathbf{x}}_{<i}\}\right).
    \label{eq:language_modeling}
\end{equation}
%
Tokens are selected from the probability distribution at each step by following a decoding strategy, resulting in a text sample $\{\mathbf{x}, \hat{\mathbf{x}}\}$. 

There are two main categories of decoding strategies, deterministic and stochastic. The former is designed to maximise the joint probability of the generated sequence, e.g. by selecting the most probable token at each step (greedy decoding) or keeping track of multiple candidate text sequences and selecting the most probable (beam search). This reduces the likelihood of converging to locally optimal but globally suboptimal sequences.
%
Stochastic methods, on the other hand, produce less repetitive and more human-like text~\cite{Holtzman2020The}. The simplest stochastic method samples directly from the distribution $p_{\theta}$. A more nuanced approach, referred to as top-$k$ decoding~\cite{fanetal-2018-hierarchical}, samples from the $k$ most probable tokens to avoid text generation from the tail of $p_{\theta}$. Nucleus sampling dynamically truncates the vocabulary to the highest probability tokens by thresholding the cumulative probability mass with a parameter $\eta\!\in\![0, 1]$~\cite{Holtzman2020The}.
% $p$
Alternatively, the probability mass can be skewed towards high-probability outcomes by deploying temperature, controlled by parameter $\tau\!\in\![0, 1]$~\cite{ackley_learning_1985}.

\subsection{Recursive LLM training}
\label{subsec:recursive_training}
%
Similarly to~\citet{shumailov2024ai}~and~\citet{dohmatob2024tale}, we simulate model collapse by training a language model recursively on its own generated output (entirely or partially, depending on our underlying hypothesis) for a fixed number of generations. This process is described in Algorithm~\ref{alg:recursive_training}. Recursive training commences by fine-tuning a pre-trained language model, $p_{\theta}$, using a dataset consisting of $n$ human-generated samples, $\mathcal{D}_{\text{H}}\!=\!\{\mathbf{x}_s\}_{s=1}^{n}$. This results in a model $p^0$, where `$0$' denotes the stage of the entire process (generation).\footnote{For enhanced notational clarity, we choose to drop parameter $\theta$ for the recursively produced LLMs. However, we clarify that $\theta$ is updated in each generation.} We then use a set of $n$ context sequences, $\mathcal{X}\!=\!\{\mathbf{x}_1, \dots, \mathbf{x}_n\}$ (one for each sample in $\mathcal{D}_{\text{H}}$), to generate a set of continuation sequences, $\mathcal{\hat{X}}\!=\!\{\mathbf{\hat{x}}_1, \dots, \mathbf{\hat{x}}_n\}$, where $\hat{\mathbf{x}}_s\!\sim\!p^0$ (see also section~\ref{subsec:language_model}).
%
The human-generated context together with the LLM-generated continuation sequences form a new synthetic dataset, $\mathcal{D}_{\text{S}}^1$ (here `$1$' is used to denote that this dataset will be used to fine-tune a language model in the next generation).

Subsequently, successive rounds of recursive training are carried out. In each generation $i$ with $i\!\ge\!1$, the original language model $p_{\theta}$ is fine-tuned using (an ever changing) synthetic dataset $\mathcal{D}_{\text{S}}^i$ to obtain $p^i$. Then, we prompt $p^i$ with context sequences $\mathcal{X}$ to generate a new synthetic dataset $\mathcal{D}_{\text{S}}^{i\!+\!1}$ that will be used to fine-tune $p_{\theta}$ in generation $i\!+\!1$.

\begin{algorithm}[t]
\caption{Recursive LLM training}
\label{alg:recursive_training}
\begin{algorithmic}[1]
\State \textbf{Input:} Human text samples $\mathcal{D}_{\text{H}}\!=\!\{\mathbf{x}_s\}_{s=1}^{n}$, pre-trained language model $p_{\theta}$ \hfill
\State Obtain $p^0$ by fine-tuning $p_{\theta}$ using $\mathcal{D}_{\text{H}}$ \hfill
\For{$i\!=\!1, \dots, G$}
    \State $\mathcal{D}_{\text{S}}^{i}\!=\!\{\mathbf{x}_s, \hat{\mathbf{x}}_s\}_{s=1}^{n} , \text{ where } \hat{\mathbf{x}}_s \sim p^{i-1}$ \hfill
    \State Obtain $p^i$ by fine-tuning $p_{\theta}$ using $\mathcal{D}_{\text{S}}^{i}$ \hfill
    % \State where $\mathcal{D}^i = \alpha \mathcal{D}_{\text{H}} + \beta \mathcal{D}_{\text{S}}^{i-1} + \sum_{l=1}^{i-2} \gamma \mathcal{D}_{\text{S}}^{l}$ \hfill
\EndFor
\State \textbf{Outputs:} $p^i \, (i\!\ge\!0)$, $\mathcal{D}_{\text{S}}^{i} \, (i\!\ge\!1)$
\end{algorithmic}
\end{algorithm}

\subsection{LLM performance}
\label{subsec:LLM_performance}
%
We evaluate model collapse by fine-tuning and testing models on the open-ended text generation task, emulating the setup proposed by~\citet{shumailov2024ai}~and~\citet{dohmatob2024tale} to facilitate comparison. We assess language model performance in terms of perplexity and evaluation accuracy. Perplexity measures how well the model predicts unseen text, with lower values indicating better performance. Accuracy, in this context, reflects the proportion of correctly predicted tokens, providing a direct measure of the model's effectiveness in generating accurate language outputs.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.7\linewidth]{images/legend_full.pdf}
    
    \begin{tabular}{c@{\hspace{0.25cm}}c@{\hspace{1cm}}c@{\hspace{0.25cm}}c@{}}
        \includegraphics[width=0.2\linewidth]{images/gpt2_perplexity.pdf} &
        \includegraphics[width=0.2\linewidth]{images/gpt2_acc.pdf} &
        \includegraphics[width=0.2\linewidth]{images/smollm_perplexity.pdf} &
        \includegraphics[width=0.2\linewidth]{images/smollm_acc.pdf}\\
        \multicolumn{2}{c}{\scriptsize\texttt{GPT-2}} & \multicolumn{2}{c}{\scriptsize\texttt{SmolLM2}}
    \end{tabular}
    \caption{Perplexity and accuracy over generations $0$ to $9$ of fully synthetic recursive training.}
    \label{fig:fully_synthetic}
\end{figure*}

\subsection{Metrics for LLM text generation quality}
\label{subsec:qualitative_metrics}
%
We complement performance metrics with more qualitative ones drawn on the generated text outputs and their similarity to human references, to obtain a holistic understanding of LLM collapse.\\[2pt]
%
\textbf{Diversity} (\texttt{D}) takes into account the sequence-level repetition at different $n$-gram levels of a document. Higher scores are reflective of more lexically diverse text. We use the following formulation:
\begin{equation}
\texttt{D}\!\left(\hat{\mathbf{x}}\right)=\prod_{n=2}^{4} \left( 1 - \frac{\lvert \text{unique } n\text{-grams}(\hat{\mathbf{x}}) \rvert}{\lvert \text{total } n\text{-grams}(\hat{\mathbf{x}}) \rvert} \right) \, .
\end{equation}
%
\textbf{MAUVE}~\cite{pillutla2021mauve} measures the distribution similarity between the original human text and the generated text. It is computed using the Kullback–Leibler (KL) divergence between the two text distributions in the embedding space of an LLM. To perform this, we use a random sample of $1{,}000$ documents of human and machine-generated text. A higher score indicates that the model generates more human-like text.\\[2pt]
%
\textbf{Self-BLEU}~\cite{zhu2018texygen} evaluates the BLEU score \cite{papineni2002bleu} of each document compared to all other documents in the generation set, providing a metric for how repetitive the model is for different outputs. Due to the computational expense of evaluating Self-BLEU we constrain the calculation to a random sample of $1{,}000$ documents and evaluate up to an $n$-gram size of $4$. A lower Self-BLEU score indicates higher text diversity.\\[2pt]
%
\textbf{Readability} is evaluated using the Flesch-Kincaid Reading-Ease score \cite{flesch1948new}, which evaluates how difficult a passage is to understand based on the number of words, sentences, and syllables. We implement the metric using the \texttt{textstat} package.\footnote{\texttt{textstat} Python package, \href{https://textstat.org/}{textstat.org}.} Lower scores indicate more complex text that contains longer sentences and verbose language.

\subsection{Machine-generated text detection}
\label{subsec:detector}
%
Machine-generated text detection methods can be divided into neural-based~\cite{hu2023radar,bhattacharjee2023conda} and metric-based approaches~\cite{mitchell2023detectgpt,hans2024spotting}. The former use statistical features, often extracted from surrogate LLMs, to detect machine-generated text, whereas the latter are based on machine learning such as fine-tuning a small pre-trained language model with a binary classification head. Here we deploy a neural classifier due to reported state-of-the-art (SOTA) performance on relevant machine-generated text detection benchmarks~\cite{wang2024semeval, mage}. We use the output logits as a form of classification confidence.

Our detector is based on an encoder-only transformer model with a sequence classification head that maps the CLS token representation to logits, $\mathbf{z}_i$, which are converted to pseudo-probabilities using a sigmoid function, $\sigma$. As LLM training is considerably resource intensive, any data filtering or sampling methods must be able to efficiently process large quantities of data with minimal computational overhead~\cite{wenzek-etal-2020-ccnet}. With this in consideration, we evaluated the base variants of $3$ pre-trained language models with under $200$ million parameters: \texttt{RoBERTa}~\cite{liu_roberta_2019} and \texttt{DeBERTav3}~\cite{he2023debertav} due to their SOTA performance in machine-generated text detection~\cite{mage, wangetal-2024-m4gt} and \texttt{ModernBERT}~\cite{warner_smarter_2024} as a more recent variant that has achieved superior performance on a range of benchmarks. The added advantage of \texttt{ModernBERT} is the large context window ($8{,}192$ tokens), and its superior computational speed and memory efficiency \cite{warner_smarter_2024}. See Appendix~\ref{sec:ai_text_detection} for more details. Despite their strong performance, as with all modern neural networks, the confidence estimates are poorly calibrated~\cite{guo2017calibration}, i.e. the probability estimates are not representative of the true likelihood. To mitigate overconfidence in the predictions, we applied label smoothing. Additionally, we used temperature scaling to further calibrate the model's predictions. Given the logit vector $\mathbf{z}_i$, the new confidence prediction is $\sigma \left(\mathbf{z}_i / T\right)$ where $T$ is a learnable temperature parameter.
% optimised using negative log-likelihood loss on a validation set.
% on the validation split of the training dataset. 

\input{tables/data_quality_decoding_strategies_all}

\section{The impact of decoding strategies on model collapse}
\label{sec:exp_decoding}
%
We carry out recursive training as described in Section \ref{subsec:recursive_training} on the open-ended text generation task by fine-tuning LLMs on the WikiText-2 dataset \cite{merity2016pointer} with \texttt{GPT-2}~\cite{radford2019language} and % Hugging Face's recently released
\texttt{SmolLM2}~\cite{smollm2}. The Wikipedia articles are segmented into non-overlapping chunks of $512$ tokens, where the first $256$ are used as the context ($\mathbf{x}$), and the remaining $256$ as the continuation ($\hat{\mathbf{x}}$). We conduct full fine-tuning for $1$ epoch and, to avoid information leakage between generation and training, define cross-entropy loss only on the generated sequence of each sample~\cite{dohmatob2024tale}. Additional details can be found in Appendix~\ref{sec:model_collapse_training}. As described in section~\ref{subsec:recursive_training}, we recursively train language models on the previous generation's output. We evaluate a range of decoding strategies to assess the effect on model collapse: greedy decoding ($\tau=0$), $5$-way beam search, pure sampling ($\tau=1$), temperature ($\tau=0.9$), top-$k$ (with $k=50$), and nucleus sampling ($\eta=0.95$). The hyperparameter settings for these decoding strategies were based on recommendations from prior work~\cite{Holtzman2020The, shumailov2024ai, arias_decoding_2024}.

Figure~\ref{fig:fully_synthetic} depicts the perplexity and evaluation accuracy on the WikiText-2 test set for every model generation. Additionally, we obtain scores for the qualitative metrics using the outputs generated by the model (i.e. $\{\hat{\mathbf{x}}\}_{s=1}^n$ of $\mathcal{D}_{\text{S}}^i$ in Algorithm~\ref{alg:recursive_training}), and enumerate them in Table~\ref{tab:data_quality_fully_synthetic} for generations $0$ and $9$. We first notice that deterministic decoding results in the most significant model collapse. While stochastic sampling methods appear to follow a linear relationship in model degradation across generations, the rate of collapse is accelerated with greedy decoding and beam search which then starts to plateau in later generations. At generation $0$, deterministic strategies yield significantly less fluent and more repetitive text, with MAUVE scores less than $5\%$ and diversity scores less than $20\%$. The gap in generation quality between deterministic and stochastic strategies in the open-ended text generation task has been explored in related literature~\cite{Holtzman2020The,pillutla2021mauve}. Here we see how it leads to exacerbated model collapse. While deterministic methods are rarely used in open-ended generation, we included them for completeness and due to the choice of beam-search for the experiments of~\citet{shumailov2024ai}, but exclude them in subsequent experiments due to their unrealistic collapse.

Pure sampling approaches appear to have affected the language models in contrasting ways. For \texttt{GPT-2}, sampling directly from the probability distribution leads to diverse and fluent text in generation $0$, but training recursively on these outputs yields the worst test perplexity among all stochastic methods, and generated text that is significantly dissimilar to human outputs (MAUVE of $7.18$). With \texttt{SmolLM2}, on the other hand, pure sampling led to the smallest drop in model task performance and the closest similarity to the human reference on average across all metrics.

Importantly, stochastic decoding with temperature leads to the most repetitive outputs after recursive training. We observed a $~70\%$ decrease in diversity for both models and the farthest distribution semantically from the human reference for \texttt{SmolLM2}, showing significant model collapse. Performance with top-$k$ sampling, on the other hand, was consistent across models, with the smallest drop in diversity and Self-BLEU, the closest resulting text to the human reference, and a smaller drop in test performance compared to nucleus sampling.

In our subsequent experiments on preventing model collapse, we seek to validate that our method can work in the most extreme scenario while also facilitating direct comparison between models. For this reason, we evaluate the models using the worst-performing stochastic decoding method (pure sampling for \texttt{GPT-2} and temperature sampling for \texttt{SmolLM2}) and top-$k$ decoding due to its consistent performance across models.
% that enables direct comparison.

\section{Preventing model collapse}
\label{sec:preventing_collapse}

So far we have carried out recursive training in a setting where models are trained exclusively on the outputs of the previous generation without implicitly including any human-generated samples. We now turn our focus to the partially synthetic setting, a more realistic scenario where human data make up a portion of the training dataset and the synthetic data is a mix of the samples produced across generations. The training dataset for generation $i$, $\mathcal{D}^{i}$, consists of the aggregation of $3$ samples:
%
\begin{equation}\label{eq:di}
\begin{split}
    \mathcal{D}^{i} \sim & \,\, {\text{\small sample}_{i \ge 1}} \,\, \mathcal{D}_{\text{H}},\alpha \\
                         & \,\, {\text{\small sample}_{i \ge 1}} \,\, \mathcal{D}_{\text{S}}^{i}, \beta\\
                         & \,\, {\text{\small sample}_{i \ge 2}} \,\, \left\{\mathcal{D}_{\text{S}}^{i-1},\dots,\mathcal{D}_{\text{S}}^{1}\right\}, \frac{\gamma}{(i-1)} \, ,
\end{split}
\end{equation}
%
where $\alpha$, $\beta$, and $\gamma\!\in\![0, 1]$ are mixing coefficients that affect the distribution of human and machine-generated data as well as the proportion of cross-generational data in the training set. 

We explore the following settings: (i) fully synthetic $\left(\alpha\!=\!0,\beta\!=\!1,\gamma\!=\!0\right)$, where training data consists entirely of synthetic samples from the previous generation, (ii) partially synthetic $\left(\alpha\!>\!0, \beta\!=\!1, \gamma\!=\!0\right)$, where the same proportion of human data is added to the training data at every generation, and (iii) partially synthetic with cross-generational data $\left(\alpha\!>\!0, \beta\!>\!0, \gamma\!>\!0\right)$ which is the same as above, but with the synthetic data accumulated across generations.
%
% \begin{enumerate}[label=(\arabic*), leftmargin=*, topsep=0.5pt, itemsep=-4pt]
%    \item fully synthetic $\left(\alpha\!=\!0,\beta\!=\!1,\gamma\!=\!0\right)$, where training data consists entirely of synthetic samples from the previous generation,
%    \item partially synthetic $\left(\alpha\!>\!0, \beta\!=\!1, \gamma\!=\!0\right)$, where the same proportion of human data is added to the training data at every generation, and
%    \item partially synthetic with cross-generational data $\left(\alpha\!>\!0, \beta\!>\!0, \gamma\!>\!0\right)$ which is the same as above, but with the synthetic data accumulated across generations.
%\end{enumerate}
%
We evaluate our method in the partially synthetic setting and vary the mixing coefficients $\alpha$, $\beta$, and $\gamma$, however, our method does not assume access to the values of the mixing coefficients and hence the data distribution. To prevent model collapse when the origin of each training sample is unknown, we train a machine-generated detector that estimates the likelihood of text origin (section~\ref{subsec:perf_detector}). We then use this information to conduct weighted sampling (section~\ref{subsec:detector_sampling}) that ultimately mitigates model collapse.

\input{figures/collapse_prevention}

\input{tables/detectors}

\subsection{Machine-generated text detection performance}
\label{subsec:perf_detector}
%
We trained and evaluated \texttt{RoBERTa}, \texttt{DeBERTav3} and \texttt{ModernBERT} on the MAGE dataset \cite{mage}, a machine-generated text detection benchmark based on documents from $10$ domains which have been used to generate text from $27$ LLMs. We adopt the preset training / validation / test splits ($80\%$/$10\%$/$10\%$). We also test on the more demanding out-of-distribution test set that contains human-curated text from $4$ unseen domains and machine-generated samples by an unseen LLM (\texttt{GPT-4}). Each model was fine-tuned for 5 epochs using a binary cross-entropy loss with early stopping. More details on the model training can be found in Appendix~\ref{sec:ai_text_detection}. Performance is enumerated in Table~\ref{tab:ai_detectors}. \texttt{ModernBERT} yielded the best classification performance on both aforementioned test sets with an AUC of $.986$ and $.943$, respectively. This is comparable to the top-performing model evaluated by \citet{mage}, Longformer, which achieved an in-distribution and out-of-distribution AUC of $.99$ and $.94$, respectively.

\subsection{Informed sampling of training data}
\label{subsec:detector_sampling}
%
Our goal is to sample a new training dataset at each iteration of recursive training which does not result in model collapse. We consider that a language model has collapsed if the addition of synthetic samples in the training data leads to degraded model performance compared to training exclusively on the human samples. Given text inputs $\mathbf{x}_i$, $i\!\in\!\{1,\dots,n\}$, with an unknown distribution of human and synthetic samples, the machine-generated text detector yields predictions $q\!\left(\mathbf{x}_i\right)$, $\forall i$.

Our approach deploys weighted sampling with replacement, using the detector's prediction probabilities as weights. As the detector has been trained on an unbalanced dataset ($29\%$ human samples), the predictions are biased towards attributing text as machine-generated, reflected in the optimal classification threshold of $0.8674$ ($0$/$1$: human/machine-generated). To ameliorate this, we apply a bias term $b\!\ge\!1$ (see Appendix~\ref{subsec:bias_term}) to the probabilities, followed by normalising the weights using
%
\begin{equation}
   w_i = \frac{\left(1-q\!\left(\mathbf{x}_i\right)\right)^{b}}{\sum_{j=1}^{n} \left(1 - q\!\left(\mathbf{x}_j\right)\right)^b} \, ,
\end{equation}
%
where $w_i\!\in\![0,1]$ denotes the weight for sample $\mathbf{x}_i$, $\forall i$. From the $n$ weighted training samples, we draw $k\!\times\!n$ samples with replacement. We have set $k$ to $1.5$ to allow for a $50\%$ upsampling of the training data. To avoid oversampling of specific samples we also set the maximum number of selections for an individual sample to $10$. In this way, we obtain a revised set of samples that we use in our recursive training regime.

\input{tables/mitigation_results_diff_3}

\subsection{Results on collapse prevention}
\label{subsec:collapse_prevention_results}

As explained in Section \ref{sec:exp_decoding}, we assess our approach by adopting the decoding strategy that caused the most significant model collapse, i.e. pure sampling for \texttt{GPT-2} and temperature sampling for \texttt{SmolLM2}, and for a more direct comparison, we also conduct experiments using top-$k$ decoding. At each generation $i$, we compare against the baseline of training on all samples in the pool of data $\mathcal{D}^i$ (Eq.~\ref{eq:di}). We also provide an ``Oracle'' performance which represents a perfect machine-generated text detector that filters all synthetic samples.



%We initially set $\alpha\!=\!1$, $\beta\!=\!1$, and $\gamma\!=\!0$ to maintain an equal proportion of human and synthetic samples (we also provide results for $\alpha\!=\!0.5$, $\beta\!=\!1$, and $\gamma\!=\!0$ in Appendix~\ref{fig:partially_synthetic_a05}). 

We evaluate recursive training in the partially synthetic setting under $3$ mixing coefficient settings ($\alpha\!=\!1$, $\beta\!=\!1$, $\gamma\!=\!0$, $\alpha\!=\!0.5$, $\beta\!=\!1$, $\gamma\!=\!0$ and $\alpha\!=\!\beta\!=\!\gamma\!=\!0.5$). The task performance, data quality, and detector accuracy metrics over $9$ generations of partially synthetic training are depicted in Figure~\ref{fig:partially_synthetic_a1} for the scenario when there is an equal proportion of human and synthetic samples (we also provide results for the $2$ additional scenarios in Appendix~\ref{fig:partially_synthetic_a05},\ref{fig:partially_synthetic_a05_g05} ). Weighted sampling (as described in section~\ref{subsec:detector_sampling}) appears to prevent model collapse, while the baseline degrades (higher perplexity, lower test accuracy). Our method also alleviates any effects on the readability and diversity of the synthetic samples. The detectability of the baseline text, on the other hand, is increasing as it becomes more dissimilar to human text. Interestingly, our method improves performance compared to training exclusively on human data (Oracle) for both language models and respective decoding strategies. These outcomes demonstrate both the value of using synthetic data in LLM training, but also the importance of selecting the right synthetic samples while maintaining a high proportion of human data.

\begin{figure}[!b]
    \includegraphics[width=\linewidth]{images/legend_perplexity_dist.pdf} \hfill
  \includegraphics[width=0.49\linewidth]{images/top_k_baseline_perplexity.pdf} \hfill
  \includegraphics[width=0.49\linewidth]{images/top_k_mit_perplexity.pdf}
  \includegraphics[width=0.48\linewidth]{images/sampling_baseline_perplexity.pdf} \hfill
  \includegraphics[width=0.48\linewidth]{images/sampling_mit_perplexity.pdf}
  \caption {Distribution of perplexity for machine-generated text of \texttt{GPT-2} at generations $0$, $1$, and $9$ in the partially synthetic scenario $\left(\alpha\!=\!0.5,\beta\!=\!1,\gamma\!=\!0\right)$. Perplexity is evaluated using the model trained on the human text ($p^0$).}
  \label{fig:perplexity_dist}
\end{figure}

Table~\ref{tab:data_quality_mitigation} enumerates the percentage difference for the final models under the baseline strategy vs. using our approach. We also enumerate the exact scores in Table~\ref{tab:mitigation_percentages}. Our method improves the model output quality across all metrics compared to the baseline except for sampling with \texttt{GPT-2}. In this case, the diversity of the text increases dramatically using the baseline method, however, this is accompanied by a significant drop in MAUVE and readability compared to our method. We observe that mixing cross-generational data has an insignificant effect on the extent of model collapse compared to training solely on the previous generation, contrasting with the findings of~\citet{gerstgrasser_is_2024}. However, we note that~\citet{gerstgrasser_is_2024} did not constrain the sample size and trained on all data available. Our experiments adopt the more realistic and less extreme setting, sampling the pool of data under mixing scenarios.
% a fixed-size dataset using mixing coefficients.

Similar to~\citet{shumailov2024ai}, we also study the perplexity distribution of the synthetic data at each generation using model $p^0$ that was trained on human data. Figure~\ref{fig:perplexity_dist} depicts these distributions for generations $0$, $1$, and $9$ for the \texttt{GPT-2} model with top-$k$ and pure sampling, compared to the  baseline. For top-$k$ decoding, similarly to~\citet{shumailov2024ai}, we observe that for the baseline perplexity shifts towards regions of lower scores and the distribution becomes more peaked, showing signs of early model collapse. For pure sampling, on the other hand, we observe that the baseline distribution shifts to higher perplexity scores and becomes more spread out. This is an interesting finding that demonstrates that by removing truncation from the decoding strategy, the narrowing effect of model collapse is diminished and instead, model collapse is reflected by long-tail incoherent text that is completely distinct from the original human samples. By deploying our mitigation strategy, however, we observe very little change in the perplexity distribution for both sampling strategies, and also improved data quality metrics to the baseline, as seen in Table~\ref{tab:data_quality_mitigation}. 

\section{Conclusion}

This work investigates model collapse across three dimensions: model performance, data quality, and resemblance to human samples. Through our analysis we found that the extent of model collapse and the effect on the data distribution is influenced by the decoding strategy. Truncating can lead to peaked distributions and repetitive models while sampling directly can result in high perplexity and verbose outputs with low resemblance to human data. Using the decoding strategies that resulted in the most extreme collapse, we evaluated the partially synthetic scenario, where human data is mixed into the training data.
% , but in an unknown proportion. 
We designed a novel method to mitigate model collapse based on resampling the training distribution using the predictions of a machine-generated text detector. We have validated our method on two popular LLMs (\texttt{GPT-2} and \texttt{SmolLM2}) across a range of decoding strategies, showing that we can prevent model collapse in all cases. When there is an equal ratio of human to synthetic samples in the training pool, our method results in improved model performance compared to training only on the human data.

\section*{Limitations}

The primary limitation of this study is the evaluation of relatively small LLMs ($117$M and $360$M parameters) compared to larger models, which often have billions of parameters, limiting the generalisability of these findings. However, recent work~\cite{dohmatob2024strong} suggests that model collapse is even more pronounced in larger models, indicating that our proposed approach could be even more beneficial in such cases. Future work could extend this analysis to larger models to validate these findings.

Additionally, as in previous studies~\cite{shumailov2023curse, dohmatob2024tale}, we assess LLMs exclusively in a fine-tuning setting rather than pre-training from scratch. While pre-training experiments could provide deeper insights, the computational cost and complexity of training large-scale models from the ground up make such an approach impractical in our case. Nevertheless, given that model collapse has been primarily evaluated in LLMs from a fine-tuning setting, the conclusions made in this work still align with the current body of research.

Another limitation is that our study focuses primarily on open-ended text generation tasks. While this is a crucial area for understanding model collapse, our findings may not fully generalise to other domains, such as structured prediction or code generation, where the impact of model collapse may manifest differently. Future work could explore whether our resampling method remains effective across these domains.

\section*{Acknowledgments}
G. Drayson is funded by the EPSRC grant ``AI Centre for Doctoral Training in Foundational Artificial Intelligence'' (EP/S021566/1). V. Lampos would like to acknowledge the support from the EPSRC grant ``Digital Health Hub for Antimicrobial Resistance'' (EP/X031276/1).

\section*{Ethics Statement}
The authors declare no competing interests. The Wikitext-2 dataset \cite{merity2016pointer} used for the experiments is licensed under the Creative Commons Attribution-ShareAlike License (CC BY-SA 4.0). \texttt{GPT-2} is licensed under the MIT license and \texttt{SmolLM2} is licensed under APACHE 2.0. AI assistants were not used for this research.

\bibliography{custom, anthology}

\balance
\clearpage
\nobalance

\appendix
\setcounter{table}{0}
\renewcommand{\thetable}{S\arabic{table}}%
\setcounter{figure}{0}
\renewcommand{\thefigure}{S\arabic{figure}}%
\setcounter{equation}{0}
\renewcommand{\theequation}{S\arabic{equation}}%

\section*{\Large\centering Appendix}
\vspace{0.2in}

\section{Machine-generated text detection}
\label{sec:ai_text_detection}

\subsection{Dataset}
\label{sec:ai_dataset}

We trained and evaluated the machine-generated text detectors on the MAGE dataset~\cite{mage}, which is based on documents from $10$ domains: opinion statements (CMV \& Yelp reviews dataset), news articles (XSum \& TLDR dataset), question answering (ELI5), story generation (Reddit WritingPrompts \& ROC), commonsense reasoning (HellaSwag), knowledge illustration (SQuAD) and Scientific writing (SciGen). The authors sampled $1{,}000$ texts from each domain (apart from opinion statements and news articles with $804$ and $777$ samples respectively) and generated text using $27$ LLMs from $7$ model families, which include OpenAI, LLaMA, GLM, FLAN-T5, OPT, BigScience and EleutherAI. For each human-written sample in the dataset, they generate a machine-generated version by providing the first $30$ tokens of human-written text as context to the LLM. In addition, for the OpenAI models, they implemented two other prompt strategies for relevant domains: `topical' prompts such as an argument or news title and `specified' prompts which contain information about the domain source. This results in $33{,}000$ ($27{,}000\!+\!3\times2\times1{,}000$) machine-generated samples per source before processing and filtering. The authors split the dataset into train, validation and test splits in the ratio $80:10:10$. To mitigate data imbalance in the validation and test sets they sample additional human data from each data source. The resulting test set contains $28{,}741$ human and $28{,}078$ machine-generated samples ($49\%$ machine-generated). The training set, however, is $71\%$ machine-generated. The total dataset consists of $154{,}078$ human-written and $294{,}381$ machine-generated texts. In addition to the test set, we also evaluate our detector on their more challenging test set containing text from four unseen domains (CNN/DailyMail, DialogSum, PubMedQA, IMDB) and generated by an unseen model (\texttt{GPT-4}). This out-of-distribution test set contains $762$ human and $800$ machine-generated samples.

\begin{figure}[t]
  \includegraphics[width=0.48\linewidth]{images/vary_alpha_perpl} \hfill
  \includegraphics[width=0.48\linewidth]{images/vary_alpha_acc}
  \label{fig:vary_alpha}
  \caption {Wikitext test set performance (perplexity and accuracy) for varying mixing coefficients ($\alpha$, $\beta$, $\gamma$) across generations $0$ to $9$ using pure sampling decoding with \texttt{GPT-2}.}
\end{figure}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.85\linewidth]{images/legend.pdf}
    
    \begin{tabular}{@{}c@{\hspace{0.1cm}}c@{}c@{}c@{}c@{}c@{}}
        \raisebox{1cm}{\rotatebox{90}{\small \texttt{GPT-2}}} &
        \includegraphics[width=0.19\linewidth]{images/a05_gpt2_perplexity.pdf} &
        \includegraphics[width=0.19\linewidth]{images/a05_gpt2_acc.pdf} &
        \includegraphics[width=0.19\linewidth]{images/a05_gpt2_readability.pdf} &
        \includegraphics[width=0.19\linewidth]{images/a05_gpt2_diversity.pdf} &
        \includegraphics[width=0.19\linewidth]{images/a05_gpt2_detector.pdf} \\
        
        \raisebox{0.9cm}{\rotatebox{90}{\small \texttt{SmolLM2}}} &
        \includegraphics[width=0.19\linewidth]{images/a05_smollm_perplexity.pdf} &
        \includegraphics[width=0.19\linewidth]{images/a05_smollm_acc.pdf} &
        \includegraphics[width=0.19\linewidth]{images/a05_smollm_readability.pdf} &
        \includegraphics[width=0.19\linewidth]{images/a05_smollm_diversity.pdf} &
        \includegraphics[width=0.19\linewidth]{images/a05_smollm_detector.pdf} \\
    \end{tabular}
    
    \caption{\texttt{GPT-2} (top) and \texttt{SmolLM2} (bottom) under partially synthetic recursive training $\left(\alpha\!=\!0.5,\beta\!=\!1,\gamma\!=\!0\right)$ for 10 generations. The baseline is equivalent to training on all the data in the pool and the Oracle performance represents a perfect AI text detector that filters all synthetic samples. }
    \label{fig:partially_synthetic_a05}
\end{figure*}

\subsection{Pre-trained models}

Robustly Optimized \texttt{BERT} pre-training Approach (\texttt{RoBERTa}) by~\citet{liu2019-roberta} improves on the pre-training phase of \texttt{BERT}~\cite{devlin-2019}, an encoder-only transformer model that leverages masked language models to enable pre-trained deep bidirectional representations. The \texttt{RoBERTa} model optimised the pre-training procedure for \texttt{BERT} by training the model for longer and on more data, changing the masking pattern, and removing the next sentence prediction objective. We use the base variant which has $125$M parameters.

Decoding-enhanced BERT with Disentangled Attention (\texttt{DeBERTav3}) by~\citet{he2023debertav}, is a BERT-based encoder only model enhanced with disentangled attention. \texttt{DeBERTav3} model improves on \texttt{DeBERTa} by using Enhanced Mask Decoding and an \texttt{ELECTRA}-style pre-training objective, Replaced Token Detection, instead of Masked Language Modelling. We use the base variant which contains $86$M backbone parameters with an embedding layer of $98$M parameters.

\texttt{ModernBERT}~\cite{warner_smarter_2024} is a recent addition to the encoder-only transformer models that has been designed to increase downstream performance and efficiency on GPUs, particularly in long-context scenarios due to its $8{,}192$ native sequence length. The model was trained on $2$ trillion tokens and improves on the original \texttt{BERT} architecture with rotary positional embeddings (RoPE), unpadding, GeGLU layers and alternating local-global attention demonstrating SOTA performance amongst encoder models across a range of classification and retrieval tasks. We conduct experiments with the base variant, which contains $150$M parameters.


\subsection{Hyperparameters}
\label{sec:ai_hyperparameters}

Each model was fine-tuned for 5 epochs with an early stopping patience of 3 epochs which is based on the macro average F1 score on the validation set and with no warm-up. Models were evaluated on the validation set after every epoch which was used to select the best model for evaluation. Optimisation was performed using AdamW with a $\beta_1$, $\beta_2$, $\epsilon$, and weight decay of $0.9$, $0.98$, $10^{-6}$, and $10^{-2}$, respectively. These parameters were chosen based on the recommended parameters of \citet{warner_smarter_2024}. The label smoothing parameter $\alpha$ was set to 0.1. The seed was fixed at $42$ and a training batch size of $16$ and validation and test batch size of $64$ were used for all experiments. The learning rate was set based on a hyperparameter sweep over % [$1e^{-5}$, $1.5e^{-5}$, $2e^{-5}$, $3e^{-5}$, $4e^{-5}$].
[$1$, $1.5$, $2$, $3$, $4$]$\times 10^{-5}$.
For ModernBERT the best-performing learning rate was $10^{-5}$. The classification threshold was selected to maximise the macro average F1 score on the validation set.

\section{Model collapse experiments}

\subsection{Dataset}

All our model collapse experiments use the raw variant of the WikiText-2 dataset \cite{merity2016pointer}. We train models on the training set, consisting of $36,718$ documents and evaluate on the test set of $4,358$ documents. The WikiText dataset was extracted from the Good or Featured article criteria specified by editors on Wikipedia and only covers the English language.

\subsection{LLMs}

The Generative Pre-trained Transformer 2 (\texttt{GPT-2})~\cite{radford2019language} is a decoder-only transformer-based language model. \texttt{GPT-2} demonstrated that large-scale language models could perform various language tasks without task-specific training. We use the base variant, which contains $117$M parameters. \texttt{SmolLM2}~\cite{smollm2} is a family of compact and efficient language models developed by Hugging Face, available in three sizes: $135$M, $360$M, and $1.7$B parameters. We utilise the $360$M parameter variant in our experiments.

\subsection{Hyperparameters}
\label{sec:model_collapse_training}
%
In our experiments, we conduct full fine-tuning using a learning rate of $5\!\times\!10^{-5}$, batch size of $8$ and a dropout rate of $0.1$. For the AdamW optimizer, we use $\beta_1$, $\beta_2$ and $\epsilon$ of $0.9$, $0.999$ and $10^{-8}$ respectively. Each model was trained for $1$ epoch with the hyperparameters fixed for all experiments. We conducted $10$ iterations of recursive training.

\section{Informed sampling}

\subsection{Bias term}
\label{subsec:bias_term}

To correct for the bias in the classifier’s confidence estimates toward predicting text as machine-generated, we introduce a bias factor $b$ based on the classification threshold:
%
\begin{equation}
b = 1 + \frac{\text{threshold}}{1-\text{threshold}}.
\end{equation}
This formulation is motivated by the odds ratio at the decision boundary, where $\frac{\text{threshold}}{1-\text{threshold}}$ represents the relative likelihood of a sample being classified as machine-generated versus human-generated at the threshold. By adding $1$, we ensure that $b\!\ge 1$, maintaining a minimum correction factor of 1. For a classifier with \( \text{threshold} > 0.5 \), this formulation increases the selection probability of underrepresented samples, counteracting the bias introduced by the classifier’s skewed confidence distribution.



\begin{figure*}[!t]
    \centering
    \includegraphics[width=0.85\linewidth]{images/legend.pdf}
    
    \begin{tabular}{@{}c@{\hspace{0.1cm}}c@{}c@{}c@{}c@{}c@{}}
        \raisebox{1cm}{\rotatebox{90}{\small \texttt{GPT-2}}} &
        \includegraphics[width=0.19\linewidth]{images/a05_g05_gpt2_perplexity.pdf} &
        \includegraphics[width=0.19\linewidth]{images/a05_g05_gpt2_acc.pdf} &
        \includegraphics[width=0.19\linewidth]{images/a05_g05_gpt2_readability.pdf} &
        \includegraphics[width=0.19\linewidth]{images/a05_g05_gpt2_diversity.pdf} &
        \includegraphics[width=0.19\linewidth]{images/a05_g05_gpt2_detector.pdf} \\
        
        \raisebox{0.9cm}{\rotatebox{90}{\small \texttt{SmolLM2}}} &
        \includegraphics[width=0.19\linewidth]{images/a05_g05_smollm_perplexity.pdf} &
        \includegraphics[width=0.19\linewidth]{images/a05_g05_smollm_acc.pdf} &
        \includegraphics[width=0.19\linewidth]{images/a05_g05_smollm_readability.pdf} &
        \includegraphics[width=0.19\linewidth]{images/a05_g05_smollm_diversity.pdf} &
        \includegraphics[width=0.19\linewidth]{images/a05_g05_smollm_detector.pdf} \\
    \end{tabular}
    
    \caption{\texttt{GPT-2} (top) and \texttt{SmolLM2} (bottom) under partially synthetic recursive training with cross-generational data $\left(\alpha\!=\!0.5,\beta\!=\!0.5,\gamma\!=\!0.5\right)$ for 10 generations. The baseline is equivalent to training on all the data in the pool and the Oracle performance represents a perfect AI text detector that filters all synthetic samples. }
    \label{fig:partially_synthetic_a05_g05}
\end{figure*}

\input{tables/mitigation_results_all_3}

\end{document}