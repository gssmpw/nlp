\section{Prior work on model collapse}
\label{sec:model_collapse}
%
Model collapse is a degenerative process in which models recursively trained on generational data exhibit a drop in performance compared to a model trained on the original human distribution____. In the early stages of recursive training, information is lost at the tails of the distribution and eventually, the output distribution converges to a point estimate with very little variance, resulting in a model that cannot be restored back to the original generation trained on human data. This effect can also be viewed as a change to neural scaling laws, in which there reaches a point where training on additional synthetic samples does not improve model performance and learning plateaus____.

It has been argued that the two causes for this behaviour are finite sampling error leading to information being lost at the tails of the distribution, and functional approximation error introducing non-zero likelihoods outside of the support of the original distribution____. Additionally, ____ theorised that the choice of generation algorithm is another contributing factor to model collapse. However, this has not been empirically evaluated in the case of LLMs, where decoding strategies that truncate the probability could have a significant impact. Currently, model collapse in LLMs has been studied with a fixed decoding strategy and model degradation has been mostly assessed using task performance metrics such as perplexity____ and test loss____. Interestingly,____ also looks into the diversity of the generated text. Hence, in our study, we have chosen to study model collapse across three perspectives: the quality of the generated text (including diversity and readability), its similarity to human text, and the model task performance.

% \subsection{Prior work on preventing model collapse}
%
Recent studies have explored methods for mitigating model collapse. ____~and____ show that when a high enough proportion of human data is added to the training samples at each iteration, model collapse in diffusion models can be avoided. In the computational linguistics domain,____ showed that by accumulating all cross-generational data and combining it with the original human data, model collapse can be significantly mitigated. ____~and____ investigated model collapse in a purely synthetic setting. They proposed heuristic methods for selecting samples, achieving performance akin to training on the original human data. However, in these prior works, the models are trained on either entirely synthetic data or the true labels of the samples are known. In our work, we investigate how to avoid model collapse in a more realistic setting where the training data is mixed and the origin (human or synthetic) of the samples 
% and hence the class distribution 
is unknown.