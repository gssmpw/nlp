% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@article{shumailov2024ai,
  title={{AI models collapse when trained on recursively generated data}},
  author={Shumailov, Ilia and Shumaylov, Zakhar and Zhao, Yiren and Papernot, Nicolas and Anderson, Ross and Gal, Yarin},
  journal={Nature},
  volume={631},
  number={8022},
  pages={755--759},
  year={2024},
}

@article{zhu2024synthesize,
  title={How to Synthesize Text Data without Model Collapse?},
  author={Zhu, Xuekai and Cheng, Daixuan and Li, Hengli and Zhang, Kaiyan and Hua, Ermo and Lv, Xingtai and Ding, Ning and Lin, Zhouhan and Zheng, Zilong and Zhou, Bowen},
  journal={arXiv preprint arXiv:2412.14689},
  year={2024}
}

@inproceedings{
dohmatob2024tale,
title={{A Tale of Tails: Model Collapse as a Change of Scaling Laws}},
author={Elvis Dohmatob and Yunzhen Feng and Pu Yang and Francois Charton and Julia Kempe},
booktitle={International Conference on Machine Learning},
year={2024},
url={https://openreview.net/forum?id=KVvku47shW}
}

@inproceedings{
Holtzman2020The,
title={{The Curious Case of Neural Text Degeneration}},
author={Ari Holtzman and Jan Buys and Li Du and Maxwell Forbes and Yejin Choi},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=rygGQyrFvH}
}

@article{shumailov2023curse,
  title={{The curse of recursion: Training on generated data makes models forget}},
  author={Shumailov, Ilia and Shumaylov, Zakhar and Zhao, Yiren and Gal, Yarin and Papernot, Nicolas and Anderson, Ross},
  journal={arXiv preprint arXiv:2305.17493},
  year={2023}
}

@inproceedings{
bertrand2024on,
title={{On the Stability of Iterative Retraining of Generative Models on their own Data}},
author={Quentin Bertrand and Joey Bose and Alexandre Duplessis and Marco Jiralerspong and Gauthier Gidel},
booktitle={International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=JORAfH2xFd}
}

@inproceedings{
alemohammad_self-consuming_2023,
title={{Self-Consuming Generative Models Go MAD}},
author={Sina Alemohammad and Josue Casco-Rodriguez and Lorenzo Luzi and Ahmed Imtiaz Humayun and Hossein Babaei and Daniel LeJeune and Ali Siahkoohi and Richard Baraniuk},
booktitle={International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=ShjMHfmPs0}
}

@article{briesch_large_2024,
  title={{Large language models suffer from their own output: An analysis of the self-consuming training loop}},
  author={Briesch, Martin and Sobania, Dominik and Rothlauf, Franz},
  journal={arXiv preprint arXiv:2311.16822},
  year={2023}
}

@article{gerstgrasser_is_2024,
  title={{Is Model Collapse Inevitable? Breaking the Curse of Recursion by Accumulating Real and Synthetic Data}},
  author={Gerstgrasser, Matthias and Schaeffer, Rylan and Dey, Apratim and Rafailov, Rafael and Sleight, Henry and Hughes, John and Korbak, Tomasz and Agrawal, Rajashree and Pai, Dhruv and Gromov, Andrey and others},
  journal={arXiv preprint arXiv:2404.01413},
  year={2024}
}

@inproceedings{
feng_beyond_2024,
title={{Beyond Model Collapse: Scaling Up with Synthesized Data Requires Reinforcement}},
author={Yunzhen Feng and Elvis Dohmatob and Pu Yang and Francois Charton and Julia Kempe},
booktitle={International Conference on Machine Learning 2024 Workshop on Theoretical Foundations of Foundation Models},
year={2024},
url={https://openreview.net/forum?id=iqoqtNyVta}
}

@article{zhang_regurgitative_2024,
  title={{Regurgitative training: The value of real data in training large language models}},
  author={Zhang, Jinghui and Qiao, Dandan and Yang, Mochen and Wei, Qiang},
  journal={arXiv preprint arXiv:2407.12835},
  year={2024}
}

@article{ackley_learning_1985,
  title={{A learning algorithm for Boltzmann machines}},
  author={Ackley, David H and Hinton, Geoffrey E and Sejnowski, Terrence J},
  journal={Cognitive science},
  volume={9},
  number={1},
  pages={147--169},
  year={1985},
  publisher={Elsevier}
}

@article{meister_locally_2023,
  title={{Locally typical sampling}},
  author={Meister, Clara and Pimentel, Tiago and Wiher, Gian and Cotterell, Ryan},
  journal={Transactions of the Association for Computational Linguistics},
  volume={11},
  pages={102--121},
  year={2023},
  publisher={MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~â€¦}
}


@inproceedings{li_contrastive_2023,
    title = {{Contrastive Decoding: Open-ended Text Generation as Optimization}},
    author = "Li, Xiang Lisa  and
      Holtzman, Ari  and
      Fried, Daniel  and
      Liang, Percy  and
      Eisner, Jason  and
      Hashimoto, Tatsunori  and
      Zettlemoyer, Luke  and
      Lewis, Mike",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.687/",
    doi = "10.18653/v1/2023.acl-long.687",
    pages = "12286--12312",
    abstract = "Given a language model (LM), maximum probability is a poor decoding objective for open-ended generation, because it produces short and repetitive text. On the other hand, sampling can often produce incoherent text that drifts from the original topics. We propose contrastive decoding (CD), a reliable decoding approach that optimizes a contrastive objective subject to a plausibility constraint. The contrastive objective returns the difference between the likelihood under a large LM (called the expert, e.g. OPT-13B) and a small LM (called the amateur, e.g. OPT-125M), and the constraint ensures that the outputs are plausible. CD is inspired by the fact that the failures of larger LMs (e.g., repetition, inco- herence) are even more prevalent in smaller LMs, and that this difference signals which texts should be preferred. CD requires zero additional training, and produces higher quality text than decoding from the larger LM alone. It also works across model scales (OPT-13B and GPT2-1.5B) and significantly outperforms four strong decoding algorithms (e.g., nucleus, top-k) in automatic and human evaluations across wikipedia, news and story domains."
}

@article{seddik_how_2024,
  title={{How bad is training on synthetic data? a statistical analysis of language model collapse}},
  author={Seddik, Mohamed El Amine and Chen, Suei-Wen and Hayou, Soufiane and Youssef, Pierre and Debbah, Merouane},
  journal={arXiv preprint arXiv:2404.05090},
  year={2024}
}

@article{smollm2,
  title={{SmolLM2: When Smol Goes Big--Data-Centric Training of a Small Language Model}},
  author={Allal, Loubna Ben and Lozhkov, Anton and Bakouch, Elie and Bl{\'a}zquez, Gabriel Mart{\'\i}n and Penedo, Guilherme and Tunstall, Lewis and Marafioti, Andr{\'e}s and Kydl{\'\i}{\v{c}}ek, Hynek and Lajar{\'\i}n, Agust{\'\i}n Piqueres and Srivastav, Vaibhav and others},
  journal={arXiv preprint arXiv:2502.02737},
  year={2025}
}

@article{
su_contrastive_2023,
title={{Contrastive Search Is What You Need For Neural Text Generation}},
author={Yixuan Su and Nigel Collier},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2023},
url={https://openreview.net/forum?id=GbkWw3jwL9},
note={}
}

@inproceedings{arias_decoding_2024,
    title = {{Decoding Decoded: Understanding Hyperparameter Effects in Open-Ended Text Generation}},
    author = "Garces Arias, Esteban  and
      Li, Meimingwei  and
      Heumann, Christian  and
      Assenmacher, Matthias",
    editor = "Rambow, Owen  and
      Wanner, Leo  and
      Apidianaki, Marianna  and
      Al-Khalifa, Hend  and
      Eugenio, Barbara Di  and
      Schockaert, Steven",
    booktitle = "Proceedings of the 31st International Conference on Computational Linguistics",
    month = jan,
    year = "2025",
    url = "https://aclanthology.org/2025.coling-main.668/",
    pages = "9992--10020",
    abstract = "Decoding strategies for generative large language models (LLMs) are a critical but often underexplored aspect of text generation tasks. Guided by specific hyperparameters, these strategies aim to transform the raw probability distributions produced by language models into coherent, fluent text. In this study, we undertake a large-scale empirical assessment of a range of decoding methods, open-source LLMs, textual domains, and evaluation protocols to determine how hyperparameter choices shape the outputs. Our experiments include both factual (e.g., news) and creative (e.g., fiction) domains, and incorporate a broad suite of automatic evaluation metrics alongside human judgments. Through extensive sensitivity analyses, we distill practical recommendations for selecting and tuning hyperparameters, noting that optimal configurations vary across models and tasks. By synthesizing these insights, this study provides actionable guidance for refining decoding strategies, enabling researchers and practitioners to achieve higher-quality, more reliable, and context-appropriate text generation outcomes."
}

@article{pillutla2021mauve,
  title={{Mauve: Measuring the gap between neural text and human text using divergence frontiers}},
  author={Pillutla, Krishna and Swayamdipta, Swabha and Zellers, Rowan and Thickstun, John and Welleck, Sean and Choi, Yejin and Harchaoui, Zaid},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={4816--4828},
  year={2021}
}

@inproceedings{zhu2018texygen,
  title={{Texygen: A benchmarking platform for text generation models}},
  author={Zhu, Yaoming and Lu, Sidi and Zheng, Lei and Guo, Jiaxian and Zhang, Weinan and Wang, Jun and Yu, Yong},
  booktitle={The 41st international ACM SIGIR conference on research \& development in information retrieval},
  pages={1097--1100},
  year={2018}
}

@inproceedings{papineni2002bleu,
  title={{Bleu: a method for automatic evaluation of machine translation}},
  author={Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
  booktitle={Proceedings of the 40th annual meeting of the Association for Computational Linguistics},
  pages={311--318},
  year={2002}
}

@article{flesch1948new,
  title={{A new readability yardstick.}},
  author={Flesch, Rudolph},
  journal={Journal of applied psychology},
  volume={32},
  number={3},
  pages={221},
  year={1948},
  publisher={American Psychological Association}
}

@inproceedings{su-etal-2023-detectllm,
    title = {{DetectLLM: Leveraging Log Rank Information for Zero-Shot Detection of Machine-Generated Text}},
    author = "Su, Jinyan  and
      Zhuo, Terry  and
      Wang, Di  and
      Nakov, Preslav",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-emnlp.827/",
    doi = "10.18653/v1/2023.findings-emnlp.827",
    pages = "12395--12412",
    abstract = "With the rapid progress of Large language models (LLMs) and the huge amount of text they generate, it becomes impractical to manually distinguish whether a text is machine-generated. The growing use of LLMs in social media and education, prompts us to develop methods to detect machine-generated text, preventing malicious use such as plagiarism, misinformation, and propaganda. In this paper, we introduce two novel zero-shot methods for detecting machine-generated text by leveraging the Log-Rank information. One is called DetectLLM-LRR, which is fast and efficient, and the other is called DetectLLM-NPR, which is more accurate, but slower due to the need for perturbations. Our experiments on three datasets and seven language models show that our proposed methods improve over the state of the art by 3.9 and 1.75 AUROC points absolute. Moreover, DetectLLM-NPR needs fewer perturbations than previous work to achieve the same level of performance, which makes it more practical for real-world use. We also investigate the efficiency-performance trade-off based on users' preference for these two measures and provide intuition for using them in practice effectively. We release the data and the code of both methods in https://github.com/mbzuai-nlp/DetectLLM."
}


@InProceedings{liu_roberta_2019,
  title = 	 {{P}o{WER}-{BERT}: Accelerating {BERT} Inference via Progressive Word-vector Elimination},
  author =       {Goyal, Saurabh and Choudhury, Anamitra Roy and Raje, Saurabh and Chakaravarthy, Venkatesan and Sabharwal, Yogish and Verma, Ashish},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {3690--3699},
  year = 	 {2020},
  volume = 	 {119},
  month = 	 {13--18 Jul},
  pdf = 	 {http://proceedings.mlr.press/v119/goyal20a/goyal20a.pdf},
  url = 	 {https://proceedings.mlr.press/v119/goyal20a.html},
  abstract = 	 {We develop a novel method, called PoWER-BERT, for improving the inference time of the popular BERT model, while maintaining the accuracy. It works by: a) exploiting redundancy pertaining to word-vectors (intermediate transformer block outputs) and eliminating the redundant vectors. b) determining which word-vectors to eliminate by developing a strategy for measuring their significance, based on the self-attention mechanism. c) learning how many word-vectors to eliminate by augmenting the BERT model and the loss function. Experiments on the standard GLUE benchmark shows that PoWER-BERT achieves up to 4.5x reduction in inference time over BERT with &lt; 1% loss in accuracy. We show that PoWER-BERT offers significantly better trade-off between accuracy and inference time compared to prior methods. We demonstrate that our method attains up to 6.8x reduction in inference time with &lt; 1% loss in accuracy when applied over ALBERT, a highly compressed version of BERT. The code for PoWER-BERT is publicly available at https://github.com/IBM/PoWER-BERT.}
}

@inproceedings{wang2024semeval,
    title = "{S}em{E}val-2024 Task 8: Multidomain, Multimodel and Multilingual Machine-Generated Text Detection",
    author = "Wang, Yuxia  and
      Mansurov, Jonibek  and
      Ivanov, Petar  and
      Su, Jinyan  and
      Shelmanov, Artem  and
      Tsvigun, Akim  and
      Mohammed Afzal, Osama  and
      Mahmoud, Tarek  and
      Puccetti, Giovanni  and
      Arnold, Thomas",
    editor = {Ojha, Atul Kr.  and
      Do{\u{g}}ru{\"o}z, A. Seza  and
      Tayyar Madabushi, Harish  and
      Da San Martino, Giovanni  and
      Rosenthal, Sara  and
      Ros{\'a}, Aiala},
    booktitle = "Proceedings of the 18th International Workshop on Semantic Evaluation",
    month = jun,
    year = "2024",
    url = "https://aclanthology.org/2024.semeval-1.279/",
    doi = "10.18653/v1/2024.semeval-1.279",
    pages = "2057--2079",
    abstract = "We present the results and the main findings of SemEval-2024 Task 8: Multigenerator, Multidomain, and Multilingual Machine-Generated Text Detection. The task featured three subtasks. Subtask A is a binary classification task determining whether a text is written by a human or generated by a machine. This subtask has two tracks: a monolingual track focused solely on English texts and a multilingual track. Subtask B is to detect the exact source of a text, discerning whether it is written by a human or generated by a specific LLM. Subtask C aims to identify the changing point within a text, at which the authorship transitions from human to machine. The task attracted a large number of participants: subtask A monolingual (126), subtask A multilingual (59), subtask B (70), and subtask C (30). In this paper, we present the task, analyze the results, and discuss the system submissions and the methods they used. For all subtasks, the best systems used LLMs."
}

@inproceedings{mage,
    title = {{MAGE: Machine-generated Text Detection in the Wild}},
    author = "Li, Yafu  and
      Li, Qintong  and
      Cui, Leyang  and
      Bi, Wei  and
      Wang, Zhilin  and
      Wang, Longyue  and
      Yang, Linyi  and
      Shi, Shuming  and
      Zhang, Yue",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics",
    month = aug,
    year = "2024",
    url = "https://aclanthology.org/2024.acl-long.3/",
    doi = "10.18653/v1/2024.acl-long.3",
    pages = "36--53",
    abstract = "Large language models (LLMs) have achieved human-level text generation, emphasizing the need for effective deepfake text detection to mitigate risks like the spread of fake news and plagiarism. Existing research has been constrained by evaluating detection methods o specific domains or particular language models. In practical scenarios, however, the detector faces texts from various domains or LLMs without knowing their sources. To this end, we build a comprehensive testbed by gathering texts from diverse human writings and deepfake texts generated by different LLMs. Empirical results on mainstream detection methods demonstrate the difficulties associated with detecting deepfake text in a wide-ranging testbed, particularly in out-of-distribution scenarios. Such difficulties align with the diminishing linguistic differences between the two text sources. Despite challenges, the top-performing detector can identify 84.12{\%} out-of-domain texts generated by a new LLM, indicating the feasibility for application scenarios."
}

@inproceedings{
he2023debertav,
title={{De{BERT}aV3: Improving De{BERT}a using {ELECTRA}-Style Pre-Training with Gradient-Disentangled Embedding Sharing}},
author={Pengcheng He and Jianfeng Gao and Weizhu Chen},
booktitle={International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=sE7-XhLxHA}
}

@article{alemohammad_self_improving_2024,
  publtype={informal},
  author={Sina Alemohammad and Ahmed Imtiaz Humayun and Shruti Agarwal and John P. Collomosse and Richard G. Baraniuk},
  title={{Self-Improving Diffusion Models with Synthetic Data}},
  year={2024},
  cdate={1704067200000},
  journal={CoRR},
  volume={abs/2408.16333},
  url={https://doi.org/10.48550/arXiv.2408.16333}
}

@article{dohmatob2024strong,
  title={{Strong model collapse}},
  author={Dohmatob, Elvis and Feng, Yunzhen and Subramonian, Arjun and Kempe, Julia},
  journal={arXiv preprint arXiv:2410.04840},
  year={2024}
}


@article{warner_smarter_2024,
  title={{Smarter, better, faster, longer: A modern bidirectional encoder for fast, memory efficient, and long context finetuning and inference}},
  author={Warner, Benjamin and Chaffin, Antoine and Clavi{\'e}, Benjamin and Weller, Orion and Hallstr{\"o}m, Oskar and Taghadouini, Said and Gallagher, Alexis and Biswas, Raja and Ladhak, Faisal and Aarsen, Tom and others},
  journal={arXiv preprint arXiv:2412.13663},
  year={2024}
}

@inproceedings{guo2017calibration,
  title={{On calibration of modern neural networks}},
  author={Guo, Chuan and Pleiss, Geoff and Sun, Yu and Weinberger, Kilian Q},
  booktitle={International conference on machine learning},
  pages={1321--1330},
  year={2017},
}

@inproceedings{kenton2019bert,
  title={{Bert: Pre-training of deep bidirectional transformers for language understanding}},
  author={Kenton, Jacob Devlin Ming-Wei Chang and Toutanova, Lee Kristina},
  booktitle={Proceedings of naacL-HLT},
  year={2019},
  organization={Minneapolis, Minnesota}
}

@article{merity2016pointer,
  title={Pointer sentinel mixture models},
  author={Merity, Stephen and Xiong, Caiming and Bradbury, James and Socher, Richard},
  journal={arXiv preprint arXiv:1609.07843},
  year={2016}
}

@inproceedings{mitchell2023detectgpt,
  title={{Detectgpt: Zero-shot machine-generated text detection using probability curvature}},
  author={Mitchell, Eric and Lee, Yoonho and Khazatsky, Alexander and Manning, Christopher D and Finn, Chelsea},
  booktitle={International Conference on Machine Learning},
  pages={24950--24962},
  year={2023},
}

@article{hans2024spotting,
  title={{Spotting LLMs with binoculars: Zero-shot detection of machine-generated text}},
  author={Hans, Abhimanyu and Schwarzschild, Avi and Cherepanova, Valeriia and Kazemi, Hamid and Saha, Aniruddha and Goldblum, Micah and Geiping, Jonas and Goldstein, Tom},
  journal={arXiv preprint arXiv:2401.12070},
  year={2024}
}

@article{hu2023radar,
  title={{Radar: Robust ai-text detection via adversarial learning}},
  author={Hu, Xiaomeng and Chen, Pin-Yu and Ho, Tsung-Yi},
  journal={Advances in neural information processing systems},
  volume={36},
  pages={15077--15095},
  year={2023}
}


@inproceedings{bhattacharjee2023conda,
  title={{ConDA: Contrastive Domain Adaptation for AI-generated Text Detection}},
  author={Bhattacharjee, Amrita and Kumarage, Tharindu and Moraffah, Raha and Liu, Huan},
  booktitle={Proceedings of the 13th International Joint Conference on Natural Language Processing},
  pages={598--610},
  year={2023}
}

@inproceedings{devlin-2019,
    title ={{BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}},
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    editor = "Burstein, Jill  and
      Doran, Christy  and
      Solorio, Thamar",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics",
    month = jun,
    year = "2019",
    url = "https://aclanthology.org/N19-1423/",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
}

@inproceedings{guo-etal2024-curious,
    title ={{The Curious Decline of Linguistic Diversity: Training Language Models on Synthetic Text}},
    author = "Guo, Yanzhu  and
      Shang, Guokan  and
      Vazirgiannis, Michalis  and
      Clavel, Chlo{\'e}",
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Findings of the Association for Computational Linguistics: NAACL 2024",
    month = jun,
    year = "2024",
    url = "https://aclanthology.org/2024.findings-naacl.228/",
    doi = "10.18653/v1/2024.findings-naacl.228",
    pages = "3589--3604",
    abstract = "This study investigates the consequences of training language models on synthetic data generated by their predecessors, an increasingly prevalent practice given the prominence of powerful generative models. Diverging from the usual emphasis on performance metrics, we focus on the impact of this training methodology on linguistic diversity, especially when conducted recursively over time. To assess this, we adapt and develop a set of novel metrics targeting lexical, syntactic, and semantic diversity, applying them in recursive finetuning experiments across various natural language generation tasks in English. Our findings reveal a consistent decrease in the diversity of the model outputs through successive iterations, especially remarkable for tasks demanding high levels of creativity. This trend underscores the potential risks of training language models on synthetic text, particularly concerning the preservation of linguistic richness. Our study highlights the need for careful consideration of the long-term effects of such training approaches on the linguistic capabilities of language models."
}

@article{liu2019-roberta,
  title={{Roberta: A robustly optimized bert pretraining approach}},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}

@article{radford2019language,
  title={{Language models are unsupervised multitask learners}},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  year={2019}
}

@inproceedings{fanetal-2018-hierarchical,
    title = "Hierarchical Neural Story Generation",
    author = "Fan, Angela  and
      Lewis, Mike  and
      Dauphin, Yann",
    editor = "Gurevych, Iryna  and
      Miyao, Yusuke",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2018",
    url = "https://aclanthology.org/P18-1082/",
    doi = "10.18653/v1/P18-1082",
    pages = "889--898",
    abstract = "We explore story generation: creative systems that can build coherent and fluent passages of text about a topic. We collect a large dataset of 300K human-written stories paired with writing prompts from an online forum. Our dataset enables hierarchical story generation, where the model first generates a premise, and then transforms it into a passage of text. We gain further improvements with a novel form of model fusion that improves the relevance of the story to the prompt, and adding a new gated multi-scale self-attention mechanism to model long-range context. Experiments show large improvements over strong baselines on both automated and human evaluations. Human judges prefer stories generated by our approach to those from a strong non-hierarchical model by a factor of two to one."
}

@inproceedings{wangetal-2024-m4gt,
    title = "{M}4{GT}-Bench: Evaluation Benchmark for Black-Box Machine-Generated Text Detection",
    author = "Wang, Yuxia  and
      Mansurov, Jonibek  and
      Ivanov, Petar  and
      Su, Jinyan  and
      Shelmanov, Artem  and
      Tsvigun, Akim  and
      Mohammed Afzal, Osama  and
      Mahmoud, Tarek  and
      Puccetti, Giovanni  and
      Arnold, Thomas  and
      Aji, Alham  and
      Habash, Nizar  and
      Gurevych, Iryna  and
      Nakov, Preslav",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics",
    month = aug,
    year = "2024",
    url = "https://aclanthology.org/2024.acl-long.218/",
    doi = "10.18653/v1/2024.acl-long.218",
    pages = "3964--3992",
    abstract = "The advent of Large Language Models (LLMs) has brought an unprecedented surge in machine-generated text (MGT) across diverse channels. This raises legitimate concerns about its potential misuse and societal implications. The need to identify and differentiate such content from genuine human-generated text is critical in combating disinformation, preserving the integrity of education and scientific fields, and maintaining trust in communication. In this work, we address this problem by introducing a new benchmark based on a multilingual, multi-domain and multi-generator corpus of MGTs {---} M4GT-Bench. The benchmark is compiled of three tasks: (1) mono-lingual and multi-lingual binary MGT detection; (2) multi-way detection where one need to identify, which particular model generated the text; and (3) mixed human-machine text detection, where a word boundary delimiting MGT from human-written content should be determined. On the developed benchmark, we have tested several MGT detection baselines and also conducted an evaluation of human performance. We see that obtaining good performance in MGT detection usually requires an access to the training data from the same domain and generators. The benchmark is available at https://github.com/mbzuai-nlp/M4GT-Bench."
}

@inproceedings{wenzeketal-2020-ccnet,
    title = "{CCN}et: Extracting High Quality Monolingual Datasets from Web Crawl Data",
    author = "Wenzek, Guillaume  and
      Lachaux, Marie-Anne  and
      Conneau, Alexis  and
      Chaudhary, Vishrav  and
      Guzm{\'a}n, Francisco  and
      Joulin, Armand  and
      Grave, Edouard",
    editor = "Calzolari, Nicoletta  and
      B{\'e}chet, Fr{\'e}d{\'e}ric  and
      Blache, Philippe  and
      Choukri, Khalid  and
      Cieri, Christopher  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Isahara, Hitoshi  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, H{\'e}l{\`e}ne  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios",
    booktitle = "Proceedings of the Twelfth Language Resources and Evaluation Conference",
    month = may,
    year = "2020",
    url = "https://aclanthology.org/2020.lrec-1.494/",
    pages = "4003--4012",
    language = "eng",
    ISBN = "979-10-95546-34-4",
    abstract = "Pre-training text representations have led to significant improvements in many areas of natural language processing. The quality of these models benefits greatly from the size of the pretraining corpora as long as its quality is preserved. In this paper, we describe an automatic pipeline to extract massive high-quality monolingual datasets from Common Crawl for a variety of languages. Our pipeline follows the data processing introduced in fastText (Mikolov et al., 2017; Grave et al., 2018), that deduplicates documents and identifies their language. We augment this pipeline with a filtering step to select documents that are close to high quality corpora like Wikipedia."
}