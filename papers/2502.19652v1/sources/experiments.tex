\section{ Supplementary Experiments and Analysis}
\label{appendix:experiments-robust}
% \subsection{Settings and baselines}



\begin{comment}
In this study, we provide comprehensive evaluation for our benchmark tasks, as shown in Table \ref{appendix-table:baselines-for-robust-RL-4settings}.

\begin{table}[ht]
\centering
\begin{tabular}{c|c}
\hline
\diagbox[dir=NW,width=7em,height=2em]{\hspace{-7pt}Baselines}{Tasks\hspace{-3pt}} & Robustness settings \\
\hline
SAC \citep{haarnoja2018soft}  & \multirow{2}{*}{Standard RL's robustness evaluation} \\
PPO \citep{schulman2017proximal} & \\
\hline
OMPO \citep{luoompo2024} & \multirow{2}{*}{Robust RL's robustness evaluation} \\
RSC \citep{ding2024seeing} & \\
\hline
PCRPO \citep{gu2024balance} & \multirow{2}{*}{Safe RL's robustness evaluation} \\
CRPO \citep{xu2021crpo} & \\
\hline
MAPPO \citep{yu2022surprising} & \multirow{2}{*}{Multi-agent RL's robustness evaluation} \\
IPPO \citep{de2020independent} & \\
\hline
\end{tabular}
\caption{Experiment baselines to evaluate task effectiveness.}
\label{appendix-table:baselines-for-robust-RL-4settings}
\end{table}


% \begin{table}[ht]
% \centering
% \begin{tabular}{cc}
% \hline
% \diagbox[dir=NW,width=7em,height=2em]{\hspace{-7pt}Baselines}{Task\hspace{-3pt}} & Tasks \\
% \hline
% PPO \citep{schulman2017proximal}, SAC \citep{haarnoja2018soft} & Standard RL's robustness evaluation  \\
% OMPO \citep{luoompo2024}, RSC \citep{ding2024seeing}  &  Robust RL's  robustness evaluation  \\
% PCRPO \citep{gu2024balance}, CRPO \citep{xu2021crpo} &  Safe RL's  robustness evaluation  \\
% MAPPO \citep{yu2022surprising}, IPPO \citep{de2020independent} &  Multi-agent RL's  robustness evaluation  \\
% \hline
% \end{tabular}
% \caption{Experiment baselines to evaluate task effectiveness.
% }
% \label{table:baselines-for-robust-RL}
% \end{table}





\paragraph{Attack on 4 sources}: 1. 4 separate experiments on different sources: state, reward, action, environment.  2. multiple sources are attacked, a combination of them
% \begin{itemize}
%     \item 
%     \item 
% \end{itemize}


\paragraph{Attack on 4 modes.}
random mode (state), adversarial mode (state), internal dynamics and external dynamics (environment).

\paragraph{Frequency}

\paragraph{LLM adversarial attack compared to other adversarial attacks.}

\paragraph{Safe RL and multi-agent RL attacks}
\end{comment}

% \textcolor{cyan}{Done:}


% \textcolor{blue}{robust mujoco,
% robust dynamics, environments,
% robust dexterous hands, 
% robust safety,  Robust RL with LLMs, Robust MARL.}


% \textcolor{blue}{robust manipulation, Robust Semantics RL,}




\subsection{Supplementary for evaluation robustness of standard RL}
\label{appendix:sup-robust-of-standard-rl}


As shown in Figures \ref{fig:clear-show-robust-mujoco-halfcheetah-v4-state-action-attack-training}, they demonstrates the robustness of PPO in the HalfCheetah-v4 environment under various adversarial conditions. Each graph presents the average episode reward across training steps, contrasting the performance of the standard PPO algorithm against its adaptations under diverse adversarial attack parameters. Specifically, the figure for in-Training Attack on Reward (Figure \ref{fig:clear-show-robust-mujoco-halfcheetah-v4-state-action-attack-training} (a)) investigates how modifications to the rewards during training influence the learning performance, employing multiple levels of perturbation. Moreover, the graph for Post-Training Attack on Reward (Figure \ref{fig:clear-show-robust-mujoco-halfcheetah-v4-state-action-attack-training} (b)) assesses how the trained policy withstands alterations to the reward signals post-training. The experimental results suggest that training an RL agent with disturbances and then testing it in ideal environments may lead to improved reward performance in test scenarios. Similarly, we conducted an experiment to evaluate the robustness of another popular RL baseline, SAC. As shown in Figure \ref{fig:clear-show-robust-mujoco-halfcheetah-v4-state-attack-training-sac}, the performance of SAC degrades under a disturbance attack.

% Each graph employs lines to depict various scenarios, with shaded areas indicating confidence intervals or variability across different trials. 

This experiment aids in understanding the stability and robustness of RL policies under adversarial conditions, which is pivotal for deploying these models in real-world scenarios where they may encounter unexpected or adversarial changes in input data.

\begin{figure}[tb!]
 \centering
%  \subcaptionbox{During-Training Attack on State}
%   {
% \includegraphics[width=0.31\linewidth]{figures/robust-mujoco/state/HalfCheetah-v4/clearly-training-attack-state-HalfCheetah-v4_3seeds.pdf}
% }    
%  \subcaptionbox{During-Training Attack on Action}
%   {
% \includegraphics[width=0.31\linewidth]{figures/robust-mujoco/action/clearly_training-attack-action-HalfCheetah-v4_3seeds.pdf}
% }    
%  \subcaptionbox{During-Training Attack on Reward}
%   {
% \includegraphics[width=0.31\linewidth]{figures/robust-mujoco/reward/clearly_training-attack-reward-HalfCheetah-v4_3seeds.pdf}
% }    
%  \subcaptionbox{Post-Training Attack on State}
%   {
% \includegraphics[width=0.31\linewidth]{figures/robust-mujoco/state/HalfCheetah-v4/clearly_after_training_attack-state-HalfCheetah-v4_3seed.pdf}
% }    
%  \subcaptionbox{Post-Training Attack on Action}
%   {
% \includegraphics[width=0.31\linewidth]{figures/robust-mujoco/action/clearly_after_training_attack-action-HalfCheetah-v4_3seed.pdf}
% }    
 \subcaptionbox{In-Training Attack on Reward}
  {
\includegraphics[width=0.441\linewidth]{figures/robust-mujoco/reward/clearly_training-attack-reward-HalfCheetah-v4_3seeds.pdf}
}    
 \subcaptionbox{Post-Training Attack on Reward}
  {
\includegraphics[width=0.441\linewidth]{figures/robust-mujoco/reward/clearly_after_training_attack-reward-HalfCheetah-v4_3seed.pdf}
}    
 	\caption{\normalsize HalfCheetah-v4 robustness: training attack,reward. 
  Specifically, in experiment (a), we train the PPO algorithm under conditions: without a reward attack, and with a reward attack involving Gaussian noise with standard deviations of 0.1 and 0.5, respectively. In both reward attack scenarios, the noise has a mean of 0,  with attack noise standard deviations of 0.1 and 0.15, respectively.
   In experiment (b), we test the trained PPO models that are attacked during training with reward attacks, using standard deviations of 0.1 and 0.15. After the attack-based training, the models are evaluated in environments without any attacks.
 	} 
  \label{fig:clear-show-robust-mujoco-halfcheetah-v4-state-action-attack-training}
 \end{figure} 


\begin{figure}[tb!]
 \centering
 % \subcaptionbox{Post-Training Attack on Reward}
  {
\includegraphics[width=0.5\linewidth]{figures/robust-mujoco/sac/sac-same-clearly-training-attack-state-HalfCheetah-v4_3seeds.pdf}
}    
 	\caption{\normalsize Evaluation SAC robustness on HalfCheetah-v4 tasks. 
 	} 
  \label{fig:clear-show-robust-mujoco-halfcheetah-v4-state-attack-training-sac}
 \end{figure} 
 


%  \begin{figure}[htbp!]
%  \centering
%  % \subcaptionbox{}
%   {
% \includegraphics[width=0.5\linewidth]{figures/robust-mujoco/state/HalfCheetah/HalfCheetah-v5_3seeds.pdf}
% }    
%  	\caption{\normalsize HalfCheetah-v5 robust state.
%  	} 
%   \label{fig:robust-mujoco-halfcheetah-v5-state}
%  \end{figure} 


 \begin{comment}
\begin{figure}[htbp!]
 \centering
 \subcaptionbox{During-Training Attack on State}
  {
\includegraphics[width=0.31\linewidth]{figures/robust-mujoco/state/HalfCheetah-v4/training-attack-state-HalfCheetah-v4_3seeds.pdf}
}    
 \subcaptionbox{During-Training Attack on Action}
  {
\includegraphics[width=0.31\linewidth]{figures/robust-mujoco/state/HalfCheetah-v4/training-attack-action-HalfCheetah-v4_3seeds.pdf}
}    
 \subcaptionbox{During-Training Attack on Reward}
  {
\includegraphics[width=0.31\linewidth]{figures/robust-mujoco/reward/training-attack-reward-HalfCheetah-v4_3seeds.pdf}
}    
 \subcaptionbox{Post-Training Attack on State}
  {
\includegraphics[width=0.31\linewidth]{figures/robust-mujoco/state/HalfCheetah-v4/attack-state-HalfCheetah-v4_3seeds.pdf}
}    
 \subcaptionbox{Post-Training Attack on Action}
  {
\includegraphics[width=0.31\linewidth]{figures/robust-mujoco/state/HalfCheetah-v4/attack-action-HalfCheetah-v4_3seeds.pdf}
}    
 \subcaptionbox{Post-Training Attack on Reward}
  {
\includegraphics[width=0.31\linewidth]{figures/robust-mujoco/reward/attack-eva-reward-HalfCheetah-v4_3seeds.pdf}
}    
 	\caption{\normalsize HalfCheetah-v4 robustness: training attack on state, action, and reward. S denotes state, A denotes action, and R denotes reward. Note, in (f), train with reward attack, test w/o reward attack.
 	} 
  \label{fig:robust-mujoco-halfcheetah-v4-state-action-attack-training}
 \end{figure} 
 \end{comment}

\begin{comment}
\begin{figure}[htbp!]
 \centering
 \subcaptionbox{During-Training Attack on State}
  {
\includegraphics[width=0.31\linewidth]{figures/robust-mujoco/state/Ant-v4/Ant-v4_State Attack_3seeds.pdf}
}    
 \subcaptionbox{During-Training Attack on Action}
  {
\includegraphics[width=0.31\linewidth]{figures/robust-mujoco/state/Ant-v4/Ant-v4_Action Attack_3seeds.pdf}
}   
\subcaptionbox{During-Training Attack on Reward}
  {
\includegraphics[width=0.31\linewidth]{figures/robust-mujoco/state/Ant-v4/Ant-v4_Reward Attack_3seeds.pdf}
} 

 \subcaptionbox{Post-Training Attack on State}
  {
\includegraphics[width=0.31\linewidth]{figures/robust-mujoco/state/Ant-v4/after_training_Ant-v4_State Attack_3seeds.pdf}
}    
 \subcaptionbox{Post-Training Attack on Action}
  {
\includegraphics[width=0.31\linewidth]{figures/robust-mujoco/state/Ant-v4/after_training_Ant-v4_Action Attack_3seeds.pdf}
}   
\subcaptionbox{Post-Training Attack on Reward (train with reward attack, test w/o reward attack)}
  {
\includegraphics[width=0.31\linewidth]{figures/robust-mujoco/state/Ant-v4/after_training_Ant-v4_Reward Attack_3seeds.pdf}
} 

\caption{\normalsize Ant-v4 robustness: training attack on state, action, and reward. S denotes state, A denotes action and R denotes reward.
} 
  \label{fig:robust-mujoco-ant-v4-state-action-attack-training-main}
 \end{figure} 
\end{comment}

%  \begin{figure}[htbp!]
%  \centering
%  \subcaptionbox{}
%   {
% \includegraphics[width=0.45\linewidth]{figures/robust-mujoco/state/HalfCheetah-v4/attack-state-HalfCheetah-v4_3seeds.pdf}
% }    
%  \subcaptionbox{}
%   {
% \includegraphics[width=0.45\linewidth]{figures/robust-mujoco/state/HalfCheetah-v4/attack-action-HalfCheetah-v4_3seeds.pdf}
% }    
%  	\caption{\normalsize HalfCheetah-v4 robustness: attack on state and action. S denotes state and A denotes action.
%  	} 
%   \label{fig:robust-mujoco-halfcheetah-v4-state-action-attack}
%  \end{figure} 




\subsection{Supplementary for evaluation robustness of Safe RL}
\label{appendix:sup-robust-of-safe-rl}

 % As depicted in Figures \ref{fig:robust-safety-mujoco-pcrpo-4settings}(a)-(h), we implement PCRPO \citep{gu2024balance}, a state-of-the-art safe reinforcement learning (RL) algorithm, in robust safety-critical tasks. We selected a representative task from robust safe RL to assess the effectiveness of the safe RL algorithm. Specifically, we introduce a disruptor to adversarially attack the Walker2d robot's observations during training, as shown in Figures \ref{fig:robust-safety-mujoco-pcrpo-4settings}(a)-(b). Despite these adversarial attacks, PCRPO's reward performance remains robust. The attacks follow a Gaussian distribution with a mean of 0 and variances of 0.01, 0.05, 0.1, and 0.15, mirroring the algorithm's safety performance. Similarly, applying the same distribution to attack the robot's actions does not degrade the algorithm’s reward performance; surprisingly, it improves over scenarios without adversarial attacks. This suggests that the attack enhances policy learning diversity and aids in exploring more effective policies.

  As depicted in Figures \ref{fig:robust-safety-mujoco-pcrpo-4settings}(a) and (b), we implement PCRPO \citep{gu2024balance} and CRPO \citep{xu2021crpo}, SOTA safe RL algorithms, in robust safety-critical tasks. We selected a representative task from robust safe RL to assess the effectiveness of the safe RL algorithm. Specifically, we introduce a disruptor to  attack the Walker2d robot's observations during training, as shown in Figures \ref{fig:robust-safety-mujoco-pcrpo-4settings}(a)-(b). Under these adversarial attacks, the reward performance of both PCRPO and CRPO degrades. The attacks follow a Gaussian distribution with a mean of 0 and standard deviation of 0.3, highlighting the importance of considering disturbance testing before deploying safe RL models in real-world applications.
  
  % Similarly, applying the same distribution to attack the robot's actions does not degrade the algorithm’s reward performance; surprisingly, it improves over scenarios without adversarial attacks. This suggests that the attack enhances policy learning diversity and aids in exploring more effective policies.

\begin{figure}[tb!]
 \centering
 \subcaptionbox*{(a) Attack on safe state}
  {
\includegraphics[width=0.44\linewidth]{figures/safety-mujoco/compare-crpo/crpo_during_state-2sr-limit-40-slack-5-SafetyWalker2d-v4-reward_3seeds.pdf}
}     
\subcaptionbox*{(b) Attack on safe state}
  {
\includegraphics[width=0.44\linewidth]{figures/safety-mujoco/compare-crpo/crpo_during_state_2sr-limit-40-slack-5-SafetyWalker2d-v4-costs_3seeds.pdf}
}  
\caption{\normalsize Robust Safety RL Tasks.
} 
  \label{fig:robust-safety-mujoco-pcrpo-4settings}
 \end{figure} 


 \begin{comment}
\begin{figure}[tb!]
 \centering
 \subcaptionbox*{(a) Attack on Safe State}
  {
\includegraphics[width=0.31\linewidth]{figures/safety-mujoco/compare-crpo/crpo_during_state-2sr-limit-40-slack-5-SafetyWalker2d-v4-reward_3seeds.pdf}
}     
 \subcaptionbox*{(c) Attack on Safe Action}
  {
\includegraphics[width=0.31\linewidth]{figures/safety-mujoco/compare-crpo/crpo_during_action-2sr-limit-40-slack-5-SafetyWalker2d-v4-reward_3seeds.pdf}
}     
 \subcaptionbox*{(e) Attack on Safe Cost Signal}
  {
\includegraphics[width=0.31\linewidth]{figures/safety-mujoco/compare-crpo/crpo_during_cost-2sr-limit-40-slack-5-SafetyWalker2d-v4-reward_3seeds.pdf}
}    
\subcaptionbox*{(b) Attack on Safe State}
  {
\includegraphics[width=0.31\linewidth]{figures/safety-mujoco/compare-crpo/crpo_during_state_2sr-limit-40-slack-5-SafetyWalker2d-v4-costs_3seeds.pdf}
}  
\subcaptionbox*{(d) Attack on Safe Action}
  {
\includegraphics[width=0.31\linewidth]{figures/safety-mujoco/compare-crpo/crpo_during_action_2sr-limit-40-slack-5-SafetyWalker2d-v4-costs_3seeds.pdf}
}  
\subcaptionbox*{(f) Attack on Robot Cost Signal}
  {
\includegraphics[width=0.31\linewidth]{figures/safety-mujoco/compare-crpo/crpo_during_cost_2sr-limit-40-slack-5-SafetyWalker2d-v4-costs_3seeds.pdf}
}   
\caption{\normalsize Robust Safety RL Tasks.
} 
  \label{fig:robust-safety-mujoco-pcrpo-4settings}
 \end{figure} 
 \end{comment}

\begin{comment}
 \begin{figure}[htbp!]
 \centering
 \subcaptionbox{Post-Training Attack on Safe Robot's action}
  {
\includegraphics[width=0.44\linewidth]{figures/safety-mujoco/eva_A_2sr-limit-40-slack-5-SafetyWalker2d-v4-reward_3seeds.pdf}
}    
\subcaptionbox{Post-Training Attack on Safe Robot's action}
  {
\includegraphics[width=0.44\linewidth]{figures/safety-mujoco/eva-action-2sr-limit-40-slack-5-SafetyWalker2d-v4-costs_3seeds.pdf}
}    
\caption{\normalsize Robust Safety RL Tasks.
} 
  \label{fig:robust-safety-mujoco-pcrpo-post-only}
 \end{figure} 
\end{comment}

%  \begin{figure}[htbp!]
%  \centering
%  \subcaptionbox{During-Training Attack on Robot}
%   {
% \includegraphics[width=0.31\linewidth]{figures/safety-mujoco/2sr-limit-40-slack-5-SafetyWalker2d-v4-reward_3seeds.pdf}
% }    
% \subcaptionbox{During-Training Attack on Robot}
%   {
% \includegraphics[width=0.31\linewidth]{figures/safety-mujoco/2sr-limit-40-slack-5-SafetyWalker2d-v4-costs_3seeds.pdf}
% }    
% \subcaptionbox{During-Training Attack on Robot}
%   {
% \includegraphics[width=0.31\linewidth]{figures/safety-mujoco/action-2sr-limit-40-slack-5-SafetyWalker2d-v4-reward_3seeds.pdf}
% }    
% \subcaptionbox{During-Training Attack on Robot}
%   {
% \includegraphics[width=0.31\linewidth]{figures/safety-mujoco/action-2sr-limit-40-slack-5-SafetyWalker2d-v4-costs_3seeds.pdf}
% }    
% \subcaptionbox{During-Training Attack on Robot}
%   {
% \includegraphics[width=0.31\linewidth]{figures/safety-mujoco/reward-2sr-limit-40-slack-5-SafetyWalker2d-v4-reward_3seeds.pdf}
% }
% \subcaptionbox{During-Training Attack on Robot}
%   {
% \includegraphics[width=0.31\linewidth]{figures/safety-mujoco/reward-2sr-limit-40-slack-5-SafetyWalker2d-v4-costs_3seeds.pdf}
% }  
% \subcaptionbox{During-Training Attack on Robot}
%   {
% \includegraphics[width=0.31\linewidth]{figures/safety-mujoco/cost-2sr-limit-40-slack-5-SafetyWalker2d-v4-reward_3seeds.pdf}
% }  
% \subcaptionbox{During-Training Attack on Robot}
%   {
% \includegraphics[width=0.31\linewidth]{figures/safety-mujoco/cost-2sr-limit-40-slack-5-SafetyWalker2d-v4-costs_3seeds.pdf}
% }  
% % \subcaptionbox{Post-Training Attack on Robot}
% %   {
% % \includegraphics[width=0.31\linewidth]{figures/safety-mujoco/eva_S_2sr-limit-40-slack-5-SafetyWalker2d-v4-reward_3seeds.pdf}
% % }    
% % \subcaptionbox{Post-Training Attack on Robot}
% %   {
% % \includegraphics[width=0.31\linewidth]{figures/safety-mujoco/eva-state-2sr-limit-40-slack-5-SafetyWalker2d-v4-costs_3seeds.pdf}
% % }    
% \subcaptionbox{Post-Training Attack on Robot}
%   {
% \includegraphics[width=0.31\linewidth]{figures/safety-mujoco/eva_A_2sr-limit-40-slack-5-SafetyWalker2d-v4-reward_3seeds.pdf}
% }    
% \subcaptionbox{Post-Training Attack on Robot}
%   {
% \includegraphics[width=0.31\linewidth]{figures/safety-mujoco/eva-action-2sr-limit-40-slack-5-SafetyWalker2d-v4-costs_3seeds.pdf}
% }    
% \caption{\normalsize Robust Safety RL Tasks.
% } 
%   \label{fig:robust-safety-mujoco-pcrpo}
%  \end{figure} 

\subsection{Supplementary for evaluation robustness of Multi-Agent RL}
\label{appendix:sup-robust-of-ma-rl}

 As shown in Figures \ref{appendix-fig:robust-multi-agent-mujoco-halfcheetah-2x3-4settings} (d), (e), and (f), we investigate partial state, action, and reward attacks on MAPPO, where only a subset of agents or aspects is attacked. These figures show a smaller drop in performance, indicating partial attacks are less harmful compared to full attacks (See Figure \ref{fig:robust-multi-agent-mujoco-halfcheetah-2x3-4settings}). 
 
 % For each scenario, MAPPO tends to maintain a higher reward than IPPO, showing better robustness to adversarial attacks. 



\begin{figure}[tb!]
 \centering
%  \subcaptionbox{Attack \textbf{all state}}
%   {
% \includegraphics[width=0.31\linewidth]{figures/marl/ma-mappo-ippo-training-attack-state-MA-HalfCheetah-v4_3seeds.pdf}
% }    
%  \subcaptionbox{Attack \textbf{all action}}
%   {
% \includegraphics[width=0.31\linewidth]{figures/marl/ma-mappo-ippo-training-attack-action-MA-HalfCheetah-v4_3seeds.pdf}
% }   
% \subcaptionbox{Attack \textbf{all reward}}
%   {
% \includegraphics[width=0.31\linewidth]{figures/marl/ma-mappo-ippo-training-attack-reward-MA-HalfCheetah-v4_3seeds.pdf}
% } 
\subcaptionbox{Attack \textbf{partial state}}
  {
\includegraphics[width=0.31\linewidth]{figures/marl/partial/part-ma-mappo-training-attack-state-MA-HalfCheetah-v4_3seeds.pdf}
} 
\subcaptionbox{Attack \textbf{partial action}}
  {
\includegraphics[width=0.31\linewidth]{figures/marl/partial/part-ma-mappo-training-attack-action-MA-HalfCheetah-v4_3seeds.pdf}
} 
\subcaptionbox{Attack \textbf{reward signal}}
  {
\includegraphics[width=0.31\linewidth]{figures/marl/partial/part-ma-mappo-training-attack-reward-MA-HalfCheetah-v4_3seeds.pdf}
} 
\caption{\normalsize Multi-Agent HalfCheetah-2x3 robustness: training attack on state, action, and reward for all the two agents. S denotes state, A denotes action and R denotes reward, P denotes partial attacks. Some of agents are attacked with various attack factors.
} 
  \label{appendix-fig:robust-multi-agent-mujoco-halfcheetah-2x3-4settings}
 \end{figure} 

%  \begin{figure}[htbp!]
%  \centering
%  \subcaptionbox{During-Training Attack on State}
%   {
% \includegraphics[width=0.31\linewidth]{figures/mappo/HalfCheetah-2x3_State Attack_3seeds.pdf}
% }    
%  \subcaptionbox{During-Training Attack on Action}
%   {
% \includegraphics[width=0.31\linewidth]{figures/mappo/HalfCheetah-2x3_Action Attack_3seeds.pdf}
% }   
% \subcaptionbox{During-Training Attack on Reward}
%   {
% \includegraphics[width=0.31\linewidth]{figures/mappo/HalfCheetah-2x3_Reward Attack_3seeds.pdf}
% } 

%  \subcaptionbox{Post-Training Attack on State}
%   {
% \includegraphics[width=0.31\linewidth]{figures/mappo/after_training_HalfCheetah-2x3_State Attack_3seeds.pdf}
% }    
%  \subcaptionbox{Post-Training Attack on Action}
%   {
% \includegraphics[width=0.31\linewidth]{figures/mappo/after_training_HalfCheetah-2x3_Action Attack_3seeds.pdf}
% }   
% \subcaptionbox{Post-Training Attack on Reward (train with reward attack, test w/o reward attack)}
%   {
% \includegraphics[width=0.31\linewidth]{figures/mappo/after_training_HalfCheetah-2x3_Reward Attack_3seeds.pdf}
% } 

% \caption{\normalsize Multi-Agent HalfCheetah-2x3 robustness: training attack on state, action, and reward for all the two agents. S denotes state, A denotes action and R denotes reward.
% } 
%   \label{fig:robust-multi-agent-mujoco-halfcheetah-2x3}
%  \end{figure} 

%   \begin{figure}[htbp!]
%  \centering
%  \subcaptionbox{During-Training Attack on State}
%   {
% \includegraphics[width=0.31\linewidth]{figures/mappo/partial/HalfCheetah-2x3_Partial State Attack_3seeds.pdf}
% }    
%  \subcaptionbox{During-Training Attack on Action}
%   {
% \includegraphics[width=0.31\linewidth]{figures/mappo/partial/HalfCheetah-2x3_Partial Action Attack_3seeds.pdf}
% }   
% \subcaptionbox{During-Training Attack on Reward}
%   {
% \includegraphics[width=0.31\linewidth]{figures/mappo/partial/HalfCheetah-2x3_Partial Reward Attack_3seeds.pdf}
% } 

%  \subcaptionbox{Post-Training Attack on State}
%   {
% \includegraphics[width=0.31\linewidth]{figures/mappo/partial/after_training_HalfCheetah-2x3_Partial State Attack_3seeds.pdf}
% }    
%  \subcaptionbox{Post-Training Attack on Action}
%   {
% \includegraphics[width=0.31\linewidth]{figures/mappo/partial/after_training_HalfCheetah-2x3_Partial Action Attack_3seeds.pdf}
% }   
% \subcaptionbox{Post-Training Attack on Reward (train with reward attack, test w/o reward attack)}
%   {
% \includegraphics[width=0.31\linewidth]{figures/mappo/partial/after_training_HalfCheetah-2x3_Partial Reward Attack_3seeds.pdf}
% } 

% \caption{\normalsize Multi-Agent HalfCheetah-2x3 robustness: training attack on state, action, and reward for one of two agents. S denotes state, A denotes action and R denotes reward.
% } 
%   \label{fig:robust-multi-agent-mujoco-halfcheetah-2x3-partial}
%  \end{figure} 

\begin{comment}
\subsection{Supplementary for evaluation robustness with LLMs}
\label{appendix:sup-robust-of-llms-task}

\begin{figure}[htbp!]
 \centering
%  \subcaptionbox{Different frequency attacks}
%   {
% \includegraphics[width=0.44\linewidth]{figures/llm-attack/training-llm-attack-state-Ant-v4_3seeds.pdf}
% }     
 \subcaptionbox{Different source and level attacks}
  {
\includegraphics[width=0.31\linewidth]{figures/llm-attack/training-llm-compare-attack-state-Ant-v4_3seeds.pdf}
}   
 \subcaptionbox{Different source with different frequency attacks}
  {
\includegraphics[width=0.31\linewidth]{figures/llm-attack/same-training-llm-compare-attack-state-Ant-v4_3seeds.pdf}
}  
 \subcaptionbox{Different source with same frequency attacks}
  {
\includegraphics[width=0.31\linewidth]{figures/llm-attack/sameF-training-llm-compare-attack-state-Ant-v4_3seeds.pdf}
}  
%  \subcaptionbox{Same source distribution with same frequency attacks}
%   {
% \includegraphics[width=0.44\linewidth]{figures/llm-attack/uniform-sameF-training-llm-compare-attack-state-Ant-v4_3seeds.pdf}
% }  
  \label{fig:robust-ppo-mujoco-llms}
 \end{figure} 
\end{comment}




\begin{comment}
 \subsection{Robust Tasks with Shift Attacks}

 \begin{figure}[htbp!]
 \centering
 \subcaptionbox{During-Training Attack}
  {
\includegraphics[width=0.31\linewidth]{figures/robust-mujoco/shift/shift-training-attack-state-HalfCheetah-v4_3seeds.pdf}
}    
 \subcaptionbox{During-Training Attack}
  {
\includegraphics[width=0.31\linewidth]{figures/robust-mujoco/shift/shift-training-attack-action-HalfCheetah-v4_3seeds.pdf}
}    
 \subcaptionbox{During-Training Attack}
  {
\includegraphics[width=0.31\linewidth]{figures/robust-mujoco/shift/shift-training-attack-reward-HalfCheetah-v4_3seeds.pdf}
}    
\caption{\normalsize Robust HalfCheetah Tasks with shift attacks.
} 
  \label{fig:robust-ppo-halfcheetah-shift-attacks}
 \end{figure} 
\end{comment}



\subsection{Frequency Attack}

We offer interactive modes that support step-wise, variable interactions between disruptors, agents, and environments, allowing users to apply perturbations at any point in time and in any manner they choose. As shown in Figure \ref{fig:framework-overview-robust-rl-attack-modes-frequency}, the frequency of attacks on tasks is illustrated. Perturbations can occur at various points during the training and testing phases, with different frequencies. 


\label{appendix-frequency:robust-mujoco}
As shown in Figure \ref{appendix-fig:robust-ppo-ant-frequency-attacks}, we provide the results of robustness evaluations on the Ant-v4 task under frequency-based adversarial attacks. The figure consists of two subplots, each examining the performance of PPO-based algorithms under different attack levels and frequencies.
In Figure (a), we explore the impact of varying attack intensities at a fixed attack frequency (every 50 steps) targeting the agent's actions. As shown, PPO without adversarial intervention achieves the highest episode rewards. However, as the attack intensity increases (PPO-F50-A-0.01, PPO-F50-A-0.05, PPO-F50-A-0.1), the performance declines progressively. The highest intensity attack (PPO-F50-A-0.1) results in the most significant reduction in rewards, indicating a substantial performance drop under stronger attacks.
In Figure (b), we examine the effect of varying attack frequencies while keeping the attack intensity constant. Here, PPO-F50-S-0.15 and PPO-F100-S-0.15 represent attacks occurring every 50 and 100 steps, respectively. The results indicate that more frequent attacks (PPO-F50-S-0.15) lead to a larger decline in episode rewards compared to less frequent attacks (PPO-F100-S-0.15). This suggests that attack frequency plays a critical role in determining the robustness of PPO algorithms.
Overall, these findings demonstrate that both the intensity and frequency of attacks significantly affect the performance of RL agents, with higher intensities and more frequent attacks causing greater degradation in task performance.

   \begin{figure}[tb!]
 \centering
%  \subcaptionbox{}
%   {
% \includegraphics[width=0.42\linewidth]{figures/framework/attack-modes.pdf}
% }    
%  \subcaptionbox{}
  {
\includegraphics[width=0.57\linewidth]{figures/framework/attack-frequency.pdf}
}    
 	\caption{\normalsize Different levels of robust RL's attack frequency.
 	} 
  \label{fig:framework-overview-robust-rl-attack-modes-frequency}
 \end{figure} 

\begin{figure}[tb!]
 \centering
\subcaptionbox{Different level of attacks with same frequency}
  {
\includegraphics[width=0.44\linewidth]{figures/frequency/mujoco/action-frequency50-Ant-v4_3seeds.pdf}
}     
\subcaptionbox{Different level of frequency  with same attack}
  {
\includegraphics[width=0.44\linewidth]{figures/frequency/mujoco/state0-15-frequency-Ant-v4_3seeds.pdf}
}       
\caption{\normalsize Robust Ant Tasks with Frequency attacks.
} 
  \label{appendix-fig:robust-ppo-ant-frequency-attacks}
 \end{figure} 

\begin{comment}
\begin{figure}[htbp!]
 \centering
 \subcaptionbox{During-Training Attack}
  {
\includegraphics[width=0.31\linewidth]{figures/frequency/mujoco/state-frequency50-Ant-v4_3seeds.pdf}
}        
\subcaptionbox{During-Training Attack}
  {
\includegraphics[width=0.31\linewidth]{figures/frequency/mujoco/state-frequency100-Ant-v4_3seeds.pdf}
}        
\subcaptionbox{During-Training Attack}
  {
\includegraphics[width=0.31\linewidth]{figures/frequency/mujoco/state-frequency500-Ant-v4_3seeds.pdf}
}       
\subcaptionbox{During-Training Attack}
  {
\includegraphics[width=0.31\linewidth]{figures/frequency/mujoco/action-frequency50-Ant-v4_3seeds.pdf}
}       
\subcaptionbox{During-Training Attack}
  {
\includegraphics[width=0.31\linewidth]{figures/frequency/mujoco/action-frequency100-Ant-v4_3seeds.pdf}
}       
\subcaptionbox{During-Training Attack}
  {
\includegraphics[width=0.31\linewidth]{figures/frequency/mujoco/action-frequency250-Ant-v4_3seeds.pdf}
}       
\subcaptionbox{During-Training Attack}
  {
\includegraphics[width=0.31\linewidth]{figures/frequency/mujoco/action-frequency500-Ant-v4_3seeds.pdf}
}       
\subcaptionbox{During-Training Attack}
  {
\includegraphics[width=0.31\linewidth]{figures/frequency/mujoco/state0.15-frequency-Ant-v4_3seeds.pdf}
}       
\caption{\normalsize Robust Ant Tasks with Frequency attacks.
} 
  \label{fig:robust-ppo-ant-frequency-attacks}
 \end{figure} 
\end{comment}


% \subsection{Semantic Attack}


% \subsection{Robust Dexterous Hand Tasks}

% \begin{figure}[htbp!]
%  \centering
%  \subcaptionbox{During-Training Attack on Robot}
%   {
% \includegraphics[width=0.44\linewidth]{figures/dexterous-hand/HandReach/HandReach-v2_3seeds.pdf}
% }    
%  \subcaptionbox{During-Training Attack on Robot}
%   {
% \includegraphics[width=0.44\linewidth]{figures/dexterous-hand/HandReach/Action-HandReach-v2_3seeds.pdf}
% }    
% \caption{\normalsize Robust Dexterous Hand Tasks.
% } 
%   \label{fig:robust-ppo-dexterous-handreach-v2}
%  \end{figure} 


% \subsection{Comparison Experiments of Different Adversary Attack Settings}

% \begin{figure}[tb!]
%  \centering
%   \subcaptionbox{Frequency as 100 adversary attack}
%   {
% \includegraphics[width=0.43\linewidth]{figures/llm-attack/rebuttal/rebuttal-com-constraint-compareself-500-sameF-training-llm-compare-state-Ant-v4_3seeds.pdf}
% }  
% %  \subcaptionbox{Compare LLMs with Gaussian attack}
% %   {
% % \includegraphics[width=0.31\linewidth]{figures/llm-attack/finished/constraint_sameF-training-llm-compare-attack-state-Ant-v4_3seeds.pdf}
% % }     
%  \subcaptionbox{Frequency as 500 adversary attack}
%   {
% \includegraphics[width=0.43\linewidth]{figures/llm-attack/rebuttal/rebuttal-com-constraint-uniform0dot8-sameF-training-llm-compare-state-Ant-v4_3seeds.pdf}
% }  
% \caption{\normalsize \colorbox{yellow}{Compare LLM adversary attack with uniform adversary attack.}
% } 
% \label{fig:robust-ppo-mujoco-llms-main-clean-re-rebuttal-com}
%  \end{figure} 

% \clearpage
\section{Other Settings of the Framework}
\label{appendix-framework:settings}

% In this section, we will introduce the sources of attacks, modes of attack, and attack frequency, respectively. 

% Those perturbation events can be generally categorized from three different perspectives:

% \subsection{Attack Sources} 

% As shown in Figure \ref{fig:framework-overview-robust-rl}, we provide the overview for our robust sources, which are leveraged to perturb/attack agents.

% \begin{itemize}
%     \item Agent's observed state: The agent observes a noisy/attacked 'state' $\widetilde{s}_t$ (diverge from the real state $s_t$) and use it as the input of its policy to determine the action. 
%     \item Agent's observed reward: The agent observes a noisy/attacked 'reward' $\widetilde{r}_t$ (differ from the real immediate reward ($r_t$) obtained from the environment) and construct their policy according to it.
%     \item Action: The action $a_t$ chosen by the agent is contaminated before sent to the environment. Namely, a perturbed action $\widetilde{a}_t$ serves as the input of the environment for the next step.
%     \item Environment: an environment includes both immediate reward function $r$ and dynamic function $P_t$. An agent may interact with a shifted or unstationary environment.
% \end{itemize}



% \begin{lstlisting}[language=Python, caption=A simple example of random attack on state., label=lst:python-state-source]
% if args.noise_factor == "state":
%     if args.noise_type == "gauss":
%         observation = self._get_obs() + random.gauss(mu, sigma) 
%     elif args.noise_type == "shift":
%         observation = self._get_obs() + args.noise_shift
%     else:
%         observation = self._get_obs()        
% else:
%     observation = self._get_obs()
% \end{lstlisting}

% \begin{figure}[htbp!]
%  \centering
%  \subcaptionbox{}
%   {
% \includegraphics[width=0.43\linewidth]{figures/framework/robust-factor-framework.pdf}
% }    
%  \subcaptionbox{}
%   {
% \includegraphics[width=0.43\linewidth]{figures/framework/LLM-adversary.pdf}
% }    
%  	\caption{\normalsize A framework of robust factors (a) and a framework of LLM adversary attacks (b).
%  	} 
%   \label{fig:framework-overview-robust-rl-factors}
%  \end{figure} 
  
  
 
  
  
  
  
% \subsection{Attack Modes} 

% As shown in Figure \ref{fig:framework-overview-robust-rl-attack-modes-frequency} (a), the diagram illustrates various modes of perturbations applied to tasks. Each attack mode represents a different level of challenge for robust tasks. For instance, `Set Arbitrarily' could be considered an easy robust task, followed by `Random Attack', `Semantic-Domain-Shifted', and `Adversarial Attack', with the latter likely being the most challenging.

% % As illustrated in Figure \ref{fig:framework-overview-robust-rl-attack-modes}, this diagram depicts the various modes of perturbations applied to the tasks. Different attack modes denote different challenging robust tasks. FOr example, Set arbitrarily could be the easy robust task, following by the modes,  Random attack, Semantic-domain-shifted, and Adversarial attack; Adversarial attack may be the most challenging tasks.

% \begin{figure}[htbp!]
%  \centering
%  \subcaptionbox{Gaussian Distribution Attack}
%   {
% \includegraphics[width=0.31\linewidth]{figures/attack-factors/fold-line-case-gauss-plot-attack-factors.pdf}
% }    
%  \subcaptionbox{Uniform distribution attack}
%   {
% \includegraphics[width=0.31\linewidth]{figures/attack-factors/fold-line-case-uniform-plot-attack-factors.pdf}
% } 
%  \subcaptionbox{Sin(x) distribution attack}
%   {
% \includegraphics[width=0.31\linewidth]{figures/attack-factors/fold-line-case-sin-plot-attack-factors.pdf}
% }   
% \caption{\normalsize Different distributions based attack.
% } 
%   \label{fig:attack-distributions-on-state}
%  \end{figure} 

% \begin{itemize}
%     \item Set arbitrarily: An environment can be set to any fixed one within some pre-scribed uncertainty set of the environments. 
%     \item Random attack: the nominal variable will be added by some random noise following some distributions, such as Gaussian, or uniform distribution (an example is shown in Figure \ref{fig:attack-distributions-on-state}). This mode can be used to all perturbation sources.        
%     \item Semantic-domain-shifted: We offer some partially-similar environment/tasks while with some semantic diversity (such as different goals) for domain generalization or transfer learning tasks.
%     \item Adversarial attack: an adversarial attacker will choose the perturbed output within some admissible set to degrade the agent's performance. This mode can be used to the perturbations towards observation and action.
% \end{itemize}

%   \begin{figure}[htbp!]
%  \centering
%  \subcaptionbox{}
%   {
% \includegraphics[width=0.42\linewidth]{figures/framework/attack-modes.pdf}
% }    
%  \subcaptionbox{}
%   {
% \includegraphics[width=0.47\linewidth]{figures/framework/attack-frequency.pdf}
% }    
%  	\caption{\normalsize (a) Different levels of robust RL's attack modes; (b) Different levels of robust RL's attack frequency.
%  	} 
%   \label{fig:framework-overview-robust-rl-attack-modes-frequency}
%  \end{figure} 



 
  
  
%   \begin{figure}[htbp!]
%  \centering
%  % \subcaptionbox{}
%   {
% \includegraphics[width=0.43\linewidth]{figures/framework/LLM-adversary.pdf}
% }    
%  	\caption{\normalsize A framework of robust factors (a) and a framework of LLM adversary attacks (b).
%  	} 
%   \label{fig:framework-overview-robust-rl-factors}
%  \end{figure} 
  
  
  
% \subsection{Attack Frequency} 

% As shown in Figure \ref{fig:framework-overview-robust-rl-attack-modes-frequency}, the frequency of attacks on tasks is illustrated. Perturbations can occur at various points during the training and testing phases, with different frequencies. We offer interactive modes that support step-wise, variable interactions between disruptors, agents, and environments, allowing users to apply perturbations at any point in time and in any manner they choose.

% it illustrates the frequency of attacks on tasks. Viewed through the lens of time, the perturbations can happen at different period during training and testing process, even with different frequency. We provide interactive modes that support step-wise varying interaction between disruptors, agents, and environments. So the user can choose to apply perturbations at any point in the dimension of time in any way. 


%   \begin{figure}[htbp!]
%  \centering
%  \subcaptionbox{}
%   {
% \includegraphics[width=0.55\linewidth]{figures/framework/attack-frequency.pdf}
% }    
%  	\caption{\normalsize Different levels of robust RL's attack frequency.
%  	} 
%   \label{fig:framework-overview-robust-rl-attack-frequemcy}
%  \end{figure} 
 



\subsection{Benchmark Features}
\label{appendix-framework:benchmark-features}
The features of the benchmark are as follows:

\textbf{High Modularity:} It is designed for flexible adaptation to a variety of research needs, featuring high modularity to support a wide range of experiments.

\textbf{Wide Coverage of :} It provides a comprehensive set of tasks to evaluate robustness across different RL scenarios. An overview of the task list is shown in Figure \ref{fig:tasks+disruptors}.

% An example is shown in Table \ref{tab:robustness-task-list-mujoco}.







\textbf{High Compatibility:} It can be seamless and compatible with a wide range of existing environments.  An example is shown in Listing \ref{lst:python-interface-example}. Moreover, this benchmark supports vectorized environments, which means it can be useful to enable parallel processing of multiple environments for efficient experimentation.



\begin{lstlisting}[language=Python, caption=An example of python interface, label=lst:python-interface-example]
from robust_gymnasium.configs.robust_setting import get_config
args = get_config().parse_args()
action = env.action_space.sample()
robust_input = {"action": action, "robust_config": args}
observation, reward, terminated, truncated, info = env.step(robust_input)
\end{lstlisting}



\textbf{Support for New Gym API:} It fully supports the latest standards in Gym API, facilitating easy integration and expansion.

\textbf{Adversarial Attack with LLMs:} We feature an approach that leverages LLMs as adversary policies. An example is shown in Listing \ref{lst:python-llms-adversary-example}.

\begin{lstlisting}[language=Python, caption=An example of LLMs for robust learning, label=lst:python-llms-adversary-example]
prompt = "This is about a robust reinforcement learning setting; we want you as an adversary policy. If the current reward exceeds the previous reward value, please input some observation noise to disturb the environment and improve the learning algorithm's robustness." "The noise should be in this area:" +str((args.region_low, args.region_high))+ ", the current reward:" + str(reward) + ", the previous reward is" + str(self.previous_reward) + "please slightly revise the current environment state values:" + str(observation) + ", just output the revised state with its original format" "do not output any other things."
prompt_state = gpt_call(prompt)
observation = prompt_state
\end{lstlisting}






% \subsection{Robustness Type Design}
% \subsubsection{Random Attack}
% \subsubsection{Adversary Attack}
% \subsubsection{Arbitrary Set}
% \subsubsection{Semantic Domain shift}

% \subsection{Task Design}
% \label{appendix:tasks-design-full}



% %  \begin{figure}[htbp!]
% %  \centering
% %  \subcaptionbox{}
% %   {
% % \includegraphics[width=0.55\linewidth]{figures/framework/LLM-adversary.pdf}
% % }    
% %  	\caption{\normalsize A framework of LLM adversary attacks.
% %  	} 
% %   \label{fig:framework-overview-robust-rl-factors-llm-adversary}
% %  \end{figure} 

% % \textbf{Benchmark Tasks:}

% \paragraph{Robust Random Attack on Dynamics or Shape MuJoCo Tasks:} Tackle complex simulations with enhanced robustness.  
% As illustrated in Figures \ref{fig:robust-mujoco-dynamics-ant}(a)-(g), Figure \ref{fig:robust-mujoco-dynamics-ant}(a) depicts the Ant robot in its original training configuration. However, post-training, the robot is subjected to various adversarial factors, including environmental changes and deliberate attacks. These perturbations alter the robot's shape and mass distribution, resulting in configurations that deviate significantly from the training model, as shown in Figures \ref{fig:robust-mujoco-dynamics-ant}(b)-(g). Consequently, the performance of the trained Ant robot model may degrade when faced with these altered conditions. 

% Similar robust robot settings are depicted in Figures \ref{fig:robust-mujoco-dynamics-halfcheetah}(a)-(e). Figure \ref{fig:robust-mujoco-dynamics-halfcheetah}(a) shows the original training configuration of the HalfCheetah robot. Figures \ref{fig:robust-mujoco-dynamics-halfcheetah}(b)-(e) then illustrate the HalfCheetah robot after it has been subjected to various attacks, resulting in altered robot models that deviate from the initial training model.
% The inclusion of these additional figures for the HalfCheetah robot further denotes that robust robotics systems must be able to maintain performance even when the physical characteristics of the robot platform have been perturbed or attacked, whether through environmental factors or adversarial attacks. Analyzing the impact of such changes on the robot's behavior is crucial for developing effective and reliable control policies.

% % Other similar robust robot settings are shown in Figures \ref{fig:robust-mujoco-dynamics-halfcheetah}(a)-(e), in which \ref{fig:robust-mujoco-dynamics-halfcheetah}(a) shows the original training HalfCheetah robot, Figures \ref{fig:robust-mujoco-dynamics-halfcheetah}(b)-(e) show the attacked HalfCheetah robots.

% \begin{figure}[tb!]
%  \centering
%   \subcaptionbox{}
%   {
% \includegraphics[width=0.22\linewidth]{figures/mujoco-dynamics/ant-original.png}
% }    
%  \subcaptionbox{}
%   {
% \includegraphics[width=0.265\linewidth]{figures/mujoco-dynamics/robust-ant-dynamics.png}
% }    
%  \subcaptionbox{}
%   {
% \includegraphics[width=0.225\linewidth]{figures/mujoco-dynamics/robust-ant-dynamics-02.png}
% }    
%  \subcaptionbox{}
%   {
% \includegraphics[width=0.225\linewidth]{figures/mujoco-dynamics/robust-ant-dynamics-03.png}
% }    
%  \subcaptionbox{}
%   {
% \includegraphics[width=0.22\linewidth]{figures/mujoco-dynamics/robust-ant-dynamics-04.png}
% }    
%  \subcaptionbox{}
%   {
% \includegraphics[width=0.25\linewidth]{figures/mujoco-dynamics/robust-ant-dynamics-05.png}
% }    
%  \subcaptionbox{}
%   {
% \includegraphics[width=0.22\linewidth]{figures/mujoco-dynamics/robust-ant-dynamics-06.png}
% }    
%  	\caption{\normalsize Examples of robust MuJoCo dynamics: Robust Ant tasks.
%  	} 
%   \label{fig:robust-mujoco-dynamics-ant}
%  \end{figure} 


% \begin{figure}[tb!]
%  \centering
% %  \subcaptionbox{}
% %   {
% % \includegraphics[width=0.205\linewidth]{figures/mujoco-dynamics/halfcheetah/halfcheetah-01.png}
% % }    
%  \subcaptionbox{}
%   {
% \includegraphics[width=0.265\linewidth]{figures/mujoco-dynamics/halfcheetah/halfcheetah-02.png}
% }    
%  \subcaptionbox{}
%   {
% \includegraphics[width=0.225\linewidth]{figures/mujoco-dynamics/halfcheetah/halfcheetah-03.png}
% }    
%  \subcaptionbox{}
%   {
% \includegraphics[width=0.27\linewidth]{figures/mujoco-dynamics/halfcheetah/halfcheetah-04.png}
% }    
%  \subcaptionbox{}
%   {
% \includegraphics[width=0.18\linewidth]{figures/mujoco-dynamics/halfcheetah/halfcheetah-05.png}
% }    
%  \subcaptionbox{}
%   {
% \includegraphics[width=0.3\linewidth]{figures/mujoco-dynamics/halfcheetah/halfcheetah-06.png}
% }    
%  	\caption{\normalsize Examples of robust MuJoCo dynamics: Robust HalfCheetah tasks.
%  	} 
%   \label{fig:robust-mujoco-dynamics-halfcheetah}
%  \end{figure} 
 
% % Random attack on state:

% % \begin{align}
% % S' = S + S_g, \ \  S_g \in \mathcal{N}(\mu_s, \sigma_s) 
% % \end{align}

% % \begin{align}
% % S' = S + S_u, \ \ S_u \sim \mathcal{U}(a_s, b_s) 
% % \end{align}

% % \begin{align}
% % S' = S + S_{sh}, \ \ S_{sh} = c_s
% % \end{align}

% % Random attack on action:

% % \begin{align}
% % A' = A + A_g, \ \
% % A_g \in \mathcal{N}(\mu_a, \sigma_a) 
% % \end{align}

% % \begin{align}
% % A' = A + A_u, \ \
% % A_u \sim \mathcal{U}(a_a, b_a) 
% % \end{align}

% % \begin{align}
% % A' = A + A_{sh}, \ \
% % A_{sh} = c_a
% % \end{align}

% % Random attack on reward:

% % \begin{align}
% % R' = R + R_g, \ \
% % R_g \in \mathcal{N}(\mu_r, \sigma_r) 
% % \end{align}

% % \begin{align}
% % R' = R + R_u, \ \
% % R_u \sim \mathcal{U}(a_r, b_r) 
% % \end{align}

% % \begin{align}
% % R' = R + R_{sh}, \ \ R_{sh} = c_r
% % \end{align}


% \begin{table}[h]
%     \centering
%     \begin{tabular}{cccc}
%         \hline
%         Robust Type & Robust State & Robust Action & Robust Reward \\
%         \hline
%         Random Attack & $S' = S + S_g $ & $A' = A + A_g$ & $R' = R + R_g$ \\
%         % \hline
%         Set Attack & $S' = S + S_u$& $A' = A + A_u$ & $R' = R + R_u$ \\
%         % \hline
%         % semantic Attack & $\backslash$ & Data 3.3 & Data 3.4 \\
%         % % \hline
%         % Adversary Attack & Data 4.2 & Data 4.3 & Data 4.4 \\
%         \hline
%     \end{tabular}
%     \caption{Random and set attack on state, action and reward, where $S_g \in \mathcal{N}(\mu_s, \sigma_s)$, $S_u \sim \mathcal{U}(a_s, b_s)$, $
% A_g \in \mathcal{N}(\mu_a, \sigma_a)$, $
% A_u \sim \mathcal{U}(a_a, b_a)$, $
% R_g \in \mathcal{N}(\mu_r, \sigma_r)$, $
% R_u \sim \mathcal{U}(a_r, b_r)$.}
%     \label{tab:placeholder_label}
% \end{table}


% Each of these robust tasks incorporates robust elements such as robust observations, actions, reward signals, and dynamics to evaluate the robustness of RL algorithms.

% % Robust robot manipulation tasks: As shown in Figures \ref{fig:robust-robosuite-door-random-attack-environments}(a)-(i), where Figure \ref{fig:robust-robosuite-door-random-attack-environments}(a) denotes the original training environments, Figures \ref{fig:robust-robosuite-door-random-attack-environments}(b)-(i) shows the attacked environments, which means the environments are attacked by some factors or agent. Specifically, Figures \ref{fig:robust-robosuite-door-random-attack-environments}(i) is the most challenging task in robot open door task, where a random manipulation robot (the white robot) will disturb the yellow-black robot action and states during the yellow-black robot achieve its goals.

% \paragraph{Robust Random Attacked Robot Manipulation Environment Tasks:}
% Figures \ref{fig:robust-robosuite-door-random-attack-environments}(a)-(i) illustrate various scenarios in robot manipulation tasks. Figure \ref{fig:robust-robosuite-door-random-attack-environments}(a) represents the original training environment. In contrast, Figures \ref{fig:robust-robosuite-door-random-attack-environments}(b)-(i) depict environments that have been subjected to various forms of perturbation or interference, simulating potential real-world challenges or adversarial actions.
% Of particular note is Figure \ref{fig:robust-robosuite-door-random-attack-environments}(i), which denotes the most challenging scenario in the robot door-opening task. In this setting, a white robot, acting as an interfering agent, actively disrupts the actions and states of the yellow-black robot as it attempts to accomplish its objective. This scenario tests the robustness of the primary robot's control policy in the face of dynamic, unpredictable obstacles.
% These diverse environments demonstrate the importance of developing robust manipulation strategies that can adapt to unexpected changes and interference in real-world applications.




% \begin{figure}[tb!]
%  \centering
%   \subcaptionbox{}
%   {
% \includegraphics[width=0.17\linewidth]{figures/robosuite/door-07.png}
% }   
%   \subcaptionbox{}
%   {
% \includegraphics[width=0.14\linewidth]{figures/robosuite/door-02.png}
% }   
%  \subcaptionbox{}
%   {
% \includegraphics[width=0.14\linewidth]{figures/robosuite/door-03.png}
% }   
%  \subcaptionbox{}
%   {
% \includegraphics[width=0.132\linewidth]{figures/robosuite/door-04.png}
% }   
%  \subcaptionbox{}
%   {
% \includegraphics[width=0.147\linewidth]{figures/robosuite/door-05.png}
% }   
%  \subcaptionbox{}
%   {
% \includegraphics[width=0.228\linewidth]{figures/robosuite/door-06.png}
% }   
%  \subcaptionbox{}
%   {
% \includegraphics[width=0.12\linewidth]{figures/robosuite/door-08.png}
% }   
%  \subcaptionbox{}
%   {
% \includegraphics[width=0.14\linewidth]{figures/robosuite/door-09.png}
% }   
%  \subcaptionbox{}
%   {
% \includegraphics[width=0.233\linewidth]{figures/robosuite/door-01.png}
% }    
%  	\caption{\normalsize Examples of robust Robosuite: Random attack on environments.
%  	} 
%   \label{fig:robust-robosuite-door-random-attack-environments}
%  \end{figure} 

% \paragraph{Robust Semantic-Domain Shifted Robot Manipulation Tasks:}
% Figures \ref{fig:robust-robosuite-door-semantic-sttack}(a)-(d) illustrate semantic-domain shifted attacks in robot manipulation tasks, specifically in door-opening scenarios. These figures demonstrate a dynamic environment where the position of the door handles changes based on the robot's proximity to the door:

% (1) When the robot is far from the door, the handle is consistently positioned on the bottom right side of the door.
% As the robot approaches within a certain proximity threshold to the door, the handle's position shifts unpredictably to either the top left or top right side of the door.
% (2) This semantic-domain shift presents a unique challenge for robotic systems, requiring adaptive decision-making capabilities. The robot must recognize the change in the handle's position and adjust its approach and manipulation strategy accordingly. Such scenarios test the robustness of the robot's visual processing, spatial awareness, and RL algorithms in the face of dynamic, context-dependent environmental changes.

% These examples underscore the importance of developing robust and adaptable robotic systems capable of handling unexpected semantic changes in their operational environment, a crucial factor for successful deployment in real-world, dynamic settings.

% % Robust Semantic-Domain Shifted Robot Manipulation Tasks: As shown in Figures \ref{fig:robust-robosuite-door-semantic-sttack}(a)-(d), which are semantic-domain shifted attack robot manipulation in door open tasks. Specifically, if the robot is far from the door, the door handle will appear on the bottom right side. However, if the robot is within a certain distance from the door, the handle will appear on either the top left or top right side.

%  \begin{figure}[htbp!]
%  \centering
%  \subcaptionbox{}
%   {
% \includegraphics[width=0.11\linewidth]{figures/robosuite/door-10.png}
% }    
%   \subcaptionbox{}
%   {
% \includegraphics[width=0.155\linewidth]{figures/robosuite/door-11.png}
% }   
%  \subcaptionbox{}
%   {
% \includegraphics[width=0.145\linewidth]{figures/robosuite/door-12.png}
% }   
%  \subcaptionbox{}
%   {
% \includegraphics[width=0.165\linewidth]{figures/robosuite/door-13.png}
% }   
%  	\caption{\normalsize Examples of robust Robosuite---semantic attacks.
%  	} 
%   \label{fig:robust-robosuite-door-semantic-sttack}
%  \end{figure}

%  % As illustrated in \ref{fig:random-set-arbitrary-tasks}, 

%  \paragraph{Robust Random Attack and Set Attack on all Robot Tasks:} These tasks encompass a variety of robust challenges: (1) Robust Box2D Tasks: Engage with 2D physics environments specifically designed for robustness evaluation. (2) Robust Robot Manipulation Tasks: Conduct robust manipulation using Kuka and Franka robots. (3) Robust Safety Tasks: Emphasize safety within robustness assessments. (4) Robust Android Hand Tasks: Tackle sophisticated hand manipulation challenges in robust settings. (5) Robust Dexterous Tasks: Enhance robust capabilities in dexterous robotics. (6) Robust Fetch Manipulation Tasks: Perform robust object manipulation with Fetch robots. (7) Robust Robot Kitchen Tasks: Execute robust manipulations in kitchen environments using robots. (8) Robust Maze Tasks: Navigate mazes with robust robots. (9) Robust Multi-Agent Tasks: Promote robust coordination among multiple agents.

% % \paragraph{Robust Random Attack and Set Attack on all Robot Tasks:} A shown in Figure \ref{fig:random-set-arbitrary-tasks}, these tasks include: (1) Robust Box2D Tasks: Engage with 2D physics environments designed for robustness evaluation. (2) Robust Robot Manipulation Tasks: Robust robotic manipulation with Kuka and Franka robots. (3) Robust Safety Tasks: Prioritize safety in robustness evaluation. (4) Robust Android Hand Tasks: Explore sophisticated hand manipulation challenges in robust settings. (5) Robust Dexterous Tasks: Advance the robust capabilities in dexterous robotics. (6) Robust Fetch Manipulation Tasks: Robust object manipulation with Fetch robots. (7) Robust Robot Kitchen Tasks: Robust manipulation in Kitchen environments with robots. (8) Robust Maze Tasks: Robust navigation robots. (9) Robust Multi-Agent Tasks: Facilitate robust coordination among multiple agents.

% \begin{comment}
% \begin{figure}[tb!]
%  \centering
%  \subcaptionbox{}
%   {
% \includegraphics[width=0.265\linewidth]{figures/random-robust/robust-andreas-hands.png}
% }    
%  \subcaptionbox{}
%   {
% \includegraphics[width=0.285\linewidth]{figures/random-robust/robust-hands-03.png}
% }    
%  \subcaptionbox{}
%   {
% \includegraphics[width=0.275\linewidth]{figures/random-robust/robust-hands.png}
% }    
%  \subcaptionbox{}
%   {
% \includegraphics[width=0.328\linewidth]{figures/random-robust/robust-hands02.png}
% }    
%  \subcaptionbox{}
%   {
% \includegraphics[width=0.267\linewidth]{figures/random-robust/robust-kitchen.png}
% }    
%  \subcaptionbox{}
%   {
% \includegraphics[width=0.257\linewidth]{figures/random-robust/robust-navigation.png}
% }    
%  \subcaptionbox{}
%   {
% \includegraphics[width=0.27\linewidth]{figures/random-robust/robust-pusher.png}
% }    
%  \subcaptionbox{}
%   {
% \includegraphics[width=0.3\linewidth]{figures/random-robust/robust-torch.png}
% }   
%  \subcaptionbox{}
%   {
% \includegraphics[width=0.204\linewidth]{figures/random-robust/Robust-Humanoid.png}
% }   
%  	\caption{\normalsize Examples of random and set arbitrary robust tasks.
%  	} 
%   \label{fig:random-set-arbitrary-tasks}
%  \end{figure} 
% \end{comment}
 
\subsection{Robust Non-Stationary Tasks:} 
\label{appendix:sup-robust-of-robust-rl-task}
Inspired by OMPO \citep{luoompo2024}, we provide various task settings to evaluate policy robustness, as illustrated in Figure \ref{fig:robust-non-stationary-ours}. During policy learning, we introduce adversarial attacks during walking or running tasks by altering robot dynamics and environmental conditions. For instance, we stochastically adjust the robot's gravity and the environment's wind speed, introducing uncertain disturbances during policy learning. Additionally, we stochastically modify the robot's physical shape throughout the learning process to test and enhance policy robustness.


 \begin{figure}[tb!]
 \centering
 \subcaptionbox{}
  {
\includegraphics[width=0.3\linewidth]{figures/non-stationary/different-task-settings.pdf}
} 
 \subcaptionbox{}
  {
\includegraphics[width=0.3\linewidth]{figures/non-stationary/different-road.pdf}
} 
 \subcaptionbox{}
  {
\includegraphics[width=0.3\linewidth]{figures/non-stationary/task-disturbance.pdf}
} 
 	\caption{\normalsize Examples of robust non-stationary tasks \citep{luoompo2024}.
 	} 
  \label{fig:robust-non-stationary-ours}
 \end{figure} 

 
% Non-Stationary Ant-v5 tasks


% \begin{figure}[htbp!]
%  \centering
%  \subcaptionbox{}
%   {
% \includegraphics[width=0.9905\linewidth]{figures/non-stationary/robust-non-stationary-tasks.png}
% }    
%  	\caption{\normalsize Examples of robust non-stationary tasks \citep{luoompo2024}.
%  	} 
%   \label{fig:robust-non-stationary}
%  \end{figure} 






% \lxs{1. Robust to environment dynamics (semantic meaning); 2. flexible to interact with environment --- have access to choose enrivonment to interact (fixed task, but possibly shifted noise level/noise type).}
% \begin{itemize}
%     \item Evaluation for some baselines: SAC, PPO, (robust RL baselines). 
%     \begin{itemize}
%         \item Train on non-noise environment, test on different shifted/noisy environment with different noisy level/noise type
%         \item Different metrics: 1) Worst-case performance over all environments in an uncertainty set; 2) Average performance over all environments; 3) environment range is large: robust to state, action, robot broken, or 
%     \end{itemize}
% \end{itemize}

% \begin{itemize}
%     \item Single agent: continuous --- mujoco, Dexterous Hand, robotics manipulation, maze; discrete --- Box2D
%     \item Multi-agent: Mujoco
%     \item Safety: Mujoco
% \end{itemize}
% \subsection{Dataset Design}
% offline dataset
% \begin{itemize}
%     \item prepare non-noisy dataset from some fixed environment. Example, train from these offline dataset. but test in different dataset
%     \item prepare noisy dataset, as augmented data to help training (include the algorithms to generate those noisy data)
% \end{itemize}


 % \subsection{Supplementary for evaluation robustness of robust RL}


% as illustrated in Figure \ref{fig:ompo-robust-non-stationary-ant-humanoid-walker2d-v5}(a)

Specifically, in non-stationary Ant-v5 Tasks, during each step, we introduce noise into the agent's dynamics by attacking factors like the Ant robot's gravity and the wind speed in the robot's environment. As demonstrated in Equation (\ref{eq:non-stationary-ant-initial-deter}) for attacks at initial and training steps, we introduce deterministic perturbations to the Ant robot, such as variations in gravity and environmental wind speed, the pseudo code is shown in Listing \ref{lst:python-ant-non-stationary-initial-step}. Furthermore, Equation (\ref{eq:non-stationary-ant-initial-stochastic}) is for initial noise, and Equation (\ref{eq:non-stationary-ant-training-stochastic}) is for noise during training we use these Equarions to consider the incorporation of stochastic disturbances into the Ant robot model, again including factors like gravity fluctuations and wind speed variations, the pseudo code is shown in Listing \ref{lst:python-ant-non-stationary-during-training}. Apart from wind and gravity disturbances, we also investigate the robot shape disturbances during policy learning, as shown in Equations (\ref{eq:non-stationary-walker-initial-stochastic})-(\ref{eq:non-stationary-hopper-training-stochastic}), and an example of pseudo code is shown in Listing \ref{lst:python-walker-non-stationary-during-training}.

At the initial and training steps, if we choose non-stationary attack as deterministic noise,
% \begin{align}
% \text{Gravity} &= 14.715, \nonumber \\
% \text{Wind} &= 1.0. 
% \label{eq:non-stationary-ant-initial-deter}
% \end{align}

\begin{equation}
\text{Ant deterministic noise}=\left\{
\begin{aligned}
\text{Gravity} &= 14.715,  \\
\text{Wind} &= 1.0. 
\end{aligned}
\right.
\label{eq:non-stationary-ant-initial-deter}
\end{equation}

\text{if we choose non-stationary attack as stochastic noise,}
% \begin{align}
% \text{Gravity} &\sim \text{Uniform}(9.81, 19.82), \nonumber \\
% \text{Wind} &\sim \text{Uniform}(0.8, 1.2).

% \end{align}

\begin{equation}
\text{Ant and Humanoid stochastic noise at initial steps}=\left\{
\begin{aligned}
\text{Gravity} &\sim \text{Uniform}(9.81, 19.82),  \\
\text{Wind} &\sim \text{Uniform}(0.8, 1.2).
\end{aligned}
\right.
\label{eq:non-stationary-ant-initial-stochastic}
\end{equation}




% \text{If } \texttt{config.deter\_noise} \text{ is true:}

% \begin{align}
% \text{Gravity} &= 14.715, \nonumber \\
% \text{Wind} &= 1.0.
% \label{eq:non-stationary-ant-training-deter}
% \end{align}

During training steps, if we choose non-stationary attack as stochastic noise, where $i_{\text{episode}}$ denotes the training step number,
\begin{equation}
\text{Ant and Humanoid noise during training}\\
=\left\{
\begin{aligned}
\text{Gravity} &= 14.715 + 4.905 \cdot \sin\left(0.5 \cdot i_{\text{episode}}\right),  \\
\text{Wind} &= 1.0 + 0.2 \cdot \sin\left(0.5 \cdot i_{\text{episode}}\right).
\end{aligned}
\right.
\label{eq:non-stationary-ant-training-stochastic}
\end{equation}

\begin{equation}
% \label{appendix-shape:walker-initial}
\text{Walker stochastic noise at initial steps}=\left\{
\begin{aligned}
\text{Torso Length} &\sim \text{Uniform}(0.1, 0.3),  \\
\text{Foot Length} &\sim \text{Uniform}(0.05, 0.15).
\end{aligned}
\right.
\label{eq:non-stationary-walker-initial-stochastic}
\end{equation}

% \begin{equation}
% \text{Walker Deterministic noises}=\left\{
% \begin{aligned}
% \text{Torso Length} = 0.2 \\
% \text{Foot Length} = 0.1
% \end{aligned}
% \right.
% \label{eq:non-stationary-walker-initial-deterministic}
% \end{equation}


\begin{equation}
\text{Walker Stochastic noise}=\left\{
\begin{aligned}
\text{Torso Length} = 0.2 + 0.1 \sin(0.3 \cdot i_\text{episode}) \\
\text{Foot Length} = 0.1 + 0.05 \sin(0.3 \cdot i_\text{episode})
\end{aligned}
\right.
\label{eq:non-stationary-walker-training-stochastic}
\end{equation}

\begin{equation}
\text{Hopper stochastic noise at initial steps}=\left\{
\begin{aligned}
\text{Torso Length} &\sim \text{Uniform}(0.3, 0.5),  \\
\text{Foot Length} &\sim \text{Uniform}(0.29, 0.49).
\end{aligned}
\right.
\label{eq:non-stationary-hopper-initial-stochastic}
\end{equation}

% \begin{equation}
% \text{Hopper Deterministic noises}=\left\{
% \begin{aligned}
% \text{Torso Length} &= 0.4, \\
% \text{Foot Length} &= 0.39.
% \end{aligned}
% \right.
% \label{eq:non-stationary-hopper-initial-deterministic}
% \end{equation}

\begin{equation}
\text{Walker Stochastic noise}=\left\{
\begin{aligned}
\text{Torso Length} &= 0.4 + 0.1 \cdot \sin(0.2 \cdot i_{\text{episode}}), \\
\text{Foot Length} &= 0.39 + 0.1 \cdot \sin(0.2 \cdot i_{\text{episode}}).
\end{aligned}
\right.
\label{eq:non-stationary-hopper-training-stochastic}
\end{equation}

% \text{If } \texttt{config.deter\_noise} \text{ is true:}
% \[
% \begin{aligned}
% \text{Torso Length} &= 0.4, \\
% \text{Foot Length} &= 0.39.
% \end{aligned}
% \]
% \text{Otherwise:}
% \[
% \begin{aligned}
% \text{Torso Length} &= 0.4 + 0.1 \cdot \sin(0.2 \cdot i_{\text{episode}}), \\
% \text{Foot Length} &= 0.39 + 0.1 \cdot \sin(0.2 \cdot i_{\text{episode}}).
% \end{aligned}
% \]



\begin{lstlisting}[language=Python, caption=An example of Non-stationary Ant python code for initial steps., label=lst:python-ant-non-stationary-initial-step]
if config.deter_noise:
    gravity = 14.715
    wind = 1.
else:
    gravity = np.random.uniform(9.81, 19.82)
    wind = np.random.uniform(0.8, 1.2)
\end{lstlisting}

\begin{lstlisting}[language=Python, caption=An example of Non-stationary Ant python code for training steps., label=lst:python-ant-non-stationary-during-training]
if config.deter_noise:
    gravity = 14.715
    wind = 1.
else:
    gravity = 14.715 + 4.905 * np.sin(0.5 * i_episode)
    wind = 1. + 0.2 * np.sin(0.5 * i_episode)
\end{lstlisting}

% \begin{lstlisting}[language=Python, caption=An example of Non-stationary Walker python code for initial steps., label=lst:python-walker-non-stationary-initial-step]
% if config.deter_noise:
%     torso_len = 0.2
%     foot_len = 0.1
% else:
%     torso_len = np.random.uniform(0.1, 0.3)
%     foot_len = np.random.uniform(0.05, 0.15)
% \end{lstlisting}

\begin{lstlisting}[language=Python, caption=An example of Non-stationary Walker python code for training steps., label=lst:python-walker-non-stationary-during-training]
if config.deter_noise:
    torso_len = 0.2
    foot_len = 0.1
else:
    torso_len = 0.2 + 0.1 * np.sin(0.3 * i_episode)
    foot_len = 0.1 + 0.05 * np.sin(0.3 * i_episode)
\end{lstlisting}
                        

            

% \begin{figure}[htbp!]
%  \centering
%  \subcaptionbox{During-Training Attack on Robot Dynamics (Robot gravity and wind change)}
% %    {
% % \includegraphics[width=0.44\linewidth]{figures/ompo/Non-Stationary-Ant-v5_3seeds.pdf}
% % }    
%   {
% \includegraphics[width=0.31\linewidth]{figures/ompo/Ant/only-all-episode-Non-Stationary-Ant-v5_3seeds.pdf}
% }    
% %  \subcaptionbox{During-Training Attack on Robot Shape}
% %   {
% % \includegraphics[width=0.44\linewidth]{figures/ompo/deter-Non-Stationary-Humanoid-v5_3seeds.pdf}
% % }   
%  \subcaptionbox{During-Training Attack on Robot Shape}
% %   {
% % \includegraphics[width=0.44\linewidth]{figures/ompo/deter-Non-Stationary-Walker2d-v5_3seeds.pdf}
% % }   
%   {
% \includegraphics[width=0.31\linewidth]{figures/ompo/walker/all-episode-deter-Non-Stationary-Walker2d-v5_3seeds.pdf}
% } 
%  \subcaptionbox{During-Training Attack on Robot Shape}
% %   {
% % \includegraphics[width=0.44\linewidth]{figures/ompo/deter-Non-Stationary-Hopper-v5_3seeds.pdf}
% % }   
%   {
% \includegraphics[width=0.31\linewidth]{figures/ompo/hopper/all-episode-Non-Stationary-Hopper-v5_3seeds.pdf}
% }   
% \caption{\normalsize Non-stationary adversary attacks on Ant-v5 Tasks.
% } 
%   \label{fig:ompo-robust-non-stationary-ant-humanoid-walker2d-v5}
%  \end{figure} 
