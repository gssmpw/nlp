\section{Experiments and Analysis}


% \paragraph{Baselines.}
\name offers a variety of tasks for comprehensively evaluating the robustness of different RL paradigms. We demonstrate its flexibility by constructing robust RL tasks based on various task bases, incorporating disruptions with different types, modes, and frequencies, and evaluating several SOTA algorithms on these tasks. In addition to benchmarking existing algorithms, we also highlight an adversarial disruption mode that leverages LLMs. Examples of robust RL tasks are shown in Figure \ref{fig:attack-distributions-mujoco-ant-wind-humanoid-gravity}. More details about the experiments can be found in Appendix~\ref{appendix-parameters-settings-experiments}.

\paragraph{Benchmark RL algorithms.}
Specifically, we benchmark several SOTA algorithms in their corresponding robust RL tasks: 
{\bf Standard RL:} Proximal Policy Optimization (PPO) \citep{schulman2017proximal}, Soft Actor-Critic (SAC) \citep{haarnoja2018soft}; {\bf Robust RL:} Occupancy-Matching Policy Optimization (OMPO) \citep{luoompo2024}, Robust State-Confounded SAC (RSC) \citep{ding2024seeing}, Alternating Training with Learned Adversaries (ATLA) \citep{zhang2021robust}, and Deep
Bisimulation for Control (DBC) \citep{zhang2021learning}; {\bf Safe RL:} Projection Constraint-Rectified Policy Optimization (PCRPO) \citep{gu2024balance}, Constraint-Rectified Policy Optimization (CRPO) \citep{xu2021crpo}; {\bf Multi-Agent RL:} Multi-Agent PPO (MAPPO) \citep{yu2022surprising},  Independent PPO (IPPO) \citep{de2020independent}. 


\paragraph{Evaluation processes.} We mainly focus on two evaluation settings: {\em In-training}: the disruptor simultaneously affects the agent and environment during both training and testing at each time step. This process is typically used in robotics to address sim-to-real gaps by introducing potential noise during training; 2) {\em Post-training}: the disruptor only impacts the agent and environment during testing, mimicking scenarios where learning algorithms are unaware of testing variability.

\paragraph{Robust metrics.} In this work, we usually use the performance in the original (deployment) environment as the robust metric for evaluations. While there are many different formulations of the robust RL objective (robust metrics), such as risk-sensitive metrics (e.g., CVaR) \citep{chan2019measuring}, and the worst-case or average performance when the environment shifts \citep{zouitine2024rrls}.

% and Figure \ref{fig:attack-distributions-on-state-standard}

% \begin{table}[ht]
% \centering
% \begin{tabular}{cc}
% \hline
% \diagbox[dir=NW,width=7em,height=2em]{\hspace{-7pt}Baselines}{Task\hspace{-3pt}} & Robustness settings   \\
% \hline
% PPO \citep{schulman2017proximal}, SAC \citep{haarnoja2018soft} & Standard RL's robustness evaluation  \\
% OMPO \citep{luoompo2024}, RSC \citep{ding2024seeing}  &  Robust RL's  robustness evaluation  \\
% PCRPO \citep{gu2024balance}, CRPO \citep{xu2021crpo} &  Safe RL's  robustness evaluation  \\
% MAPPO \citep{yu2022surprising}, IPPO \citep{de2020independent} &  Multi-agent RL's  robustness evaluation  \\
% \hline
% \end{tabular}
% \caption{Experiment baselines to evaluate task effectiveness.
% }
% \label{table:baselines-for-robust-RL-4settings}
% \end{table}



\begin{figure}[tb!]
 \centering
 \subcaptionbox{Attack on robot wind}
  {
\includegraphics[width=0.31\linewidth]{figures/attack-factors/finished/Ant-wind-uniform-disturb.png}
}    
 \subcaptionbox{Attack on robot gravity}
  {
\includegraphics[width=0.31\linewidth]{figures/attack-factors/finished/humanoid-gravity-disturb.png}
}   
 \subcaptionbox{Attack on robot torso length}
  {
\includegraphics[width=0.31\linewidth]{figures/attack-factors/finished/Hopper-robust-torso-length.png}
}  
\caption{\normalsize Adversary attack on robot environments, dynamics and shape  with different distributions (We can also attack on robot state space, action space and reward signal, etc.).
} 
  \label{fig:attack-distributions-mujoco-ant-wind-humanoid-gravity}
 \end{figure} 
\begin{comment}
\begin{figure}[tb!]
 \centering
 \subcaptionbox{Gaussian Distribution Attack}
  {
\includegraphics[width=0.31\linewidth]{figures/attack-factors/fold-line-case-gauss-plot-attack-factors.pdf}
}    
 \subcaptionbox{Uniform Distribution Attack}
  {
\includegraphics[width=0.31\linewidth]{figures/attack-factors/fold-line-case-uniform-plot-attack-factors.pdf}
} 
 \subcaptionbox{Sin(x) Distribution Attack}
  {
\includegraphics[width=0.31\linewidth]{figures/attack-factors/fold-line-case-sin-plot-attack-factors.pdf}
}   
\caption{\normalsize Adversary attack on State with different distributions (Can also attack on action space and reward signal, etc.).
} 
  \label{fig:attack-distributions-on-state-standard}
 \end{figure} 
\end{comment}


\subsection{Evaluation of standard RL baselines}\label{sec:benchmark-standard}
% and \ref{fig:robust-mujoco-ant-v4-state-action-attack-training-main-standard}
To begin, we evaluate two types of robust RL tasks: one with an observation disruptor (affecting the agent's observed state) and the other with an action disruptor (affecting the action), both subjected to random disturbances at varying levels.
We benchmark the performance of standard RL baselines—PPO \citep{schulman2017proximal} and SAC \citep{haarnoja2018soft}—on robust RL tasks based on the representative HalfCheetah-v4 task from Gymnasium-MuJoCo, as partially shown in Figure~\ref{fig:clear-show-robust-mujoco-halfcheetah-v4-state-action-attack-training-standard}.  Here, S=0.1 indicates that the random disturbance over the state follows a Gaussian distribution with a mean of 0 and a standard deviation of $0.1$ (resp.$0.15$). The same applies for A=0.1 or A=0.15. Figures~\ref{fig:clear-show-robust-mujoco-halfcheetah-v4-state-action-attack-training-standard} (a)-(b) and Figure~\ref{fig:clear-show-robust-mujoco-halfcheetah-v4-state-action-attack-training-standard} (c)-(d) present the results from two different evaluation processes—In-training and Post-training, respectively. The results show that as the disturbance level increases, the performance of the baselines degrades quickly, particularly when the training process is unaware of potential disturbances (as seen in the Post-training results). More experiments, including those using disturbances over reward or the results for SAC, can be found in Appendix \ref{appendix:sup-robust-of-standard-rl}.

% Figure~\ref{fig:clear-show-robust-mujoco-halfcheetah-v4-state-action-attack-training-standard} (a) and (b)  illustrate perturbations in the algorithm's state and action during training. We explore different perturbation levels (e.g., S=0.1, S=0.15) to evaluate their effects on learning efficacy. Next, Figures \ref{fig:clear-show-robust-mujoco-halfcheetah-v4-state-action-attack-training-standard} (c) and (d) show scenarios that apply state and action perturbations after the model's training to assess the robustness of the trained policy against post-training attacks.The experimental results indicate that as the attack level increases, the performance of the RL models degrades. This highlights the importance of ensuring RL model robustness before deploying them in real-world applications. More experiments (e.g., reward attack and SAC experiments) settings are provided in Appendix \ref{appendix:sup-robust-of-standard-rl}.

% In experiment (f), we test the trained PPO models that are attacked during training with reward attacks, using variances of 0.1 and 0.15. After attack-based training, the models are evaluated in environments without attacks. More experiments and specifical settings are provided in Appendix \ref{appendix:sup-robust-of-standard-rl}.

% Specifically, the graph for in-training attack on Reward (Figure \ref{fig:clear-show-robust-mujoco-halfcheetah-v4-state-action-attack-training-standard} (c)) investigates how modifications to the rewards during training influence the learning performance, employing multiple levels of perturbation (R=0.1, R=0.15, etc.).

% In particular, the graph for the post-training attack on reward (Figure \ref{fig:clear-show-robust-mujoco-halfcheetah-v4-state-action-attack-training-standard} (f)) evaluates how the trained policy withstands alterations in reward signals after training. Each graph employs lines to depict various scenarios, with shaded areas indicating confidence intervals or variability across different trials. This experiment aids in understanding the stability and robustness of reinforcement learning policies under adversarial conditions, which is pivotal for deploying these models in real-world scenarios where they may encounter unexpected or adversarial changes in input data.

% In experiment (a), we train the PPO algorithm under three conditions: without a state attack, and with a state attack involving Gaussian noise with variances of 0.1 and 0.5, respectively. In both state attack scenarios, the noise has a mean of 0. Similarly, attacks on the action and reward are presented in experiments (b) and (c), respectively.  
%   In experiments (d) and (e), we apply attacks on the trained PPO model targeting the state and action, with attack noise variances of 0.1 and 0.15, respectively.
%    In experiment (f), we test the trained PPO models that are attacked during training with reward attacks, using variances of 0.1 and 0.15. After the attack-based training, the models are evaluated in environments without any attacks.

\begin{figure}[b!]
 \centering
 \subcaptionbox*{(a) In-training attack}
  {
\includegraphics[width=0.23\linewidth]{figures/robust-mujoco/state/HalfCheetah-v4/clearly-training-attack-state-HalfCheetah-v4_3seeds.pdf}
}    
 \subcaptionbox*{(b) In-training attack}
  {
\includegraphics[width=0.23\linewidth]{figures/robust-mujoco/action/clearly_training-attack-action-HalfCheetah-v4_3seeds.pdf}
}    
%  \subcaptionbox*{(c) During-Training Attack}
%   {
% \includegraphics[width=0.23\linewidth]{figures/robust-mujoco/reward/clearly_training-attack-reward-HalfCheetah-v4_3seeds.pdf}
% }    
 \subcaptionbox*{(c) Post-training attack}
  {
\includegraphics[width=0.23\linewidth]{figures/robust-mujoco/state/HalfCheetah-v4/clearly_after_training_attack-state-HalfCheetah-v4_3seed.pdf}
}    
 \subcaptionbox*{(d) Post-training attack}
  {
\includegraphics[width=0.23\linewidth]{figures/robust-mujoco/action/clearly_after_training_attack-action-HalfCheetah-v4_3seed.pdf}
}    
%  \subcaptionbox*{(f) Post-Training Attack}
%   {
% \includegraphics[width=0.23\linewidth]{figures/robust-mujoco/reward/clearly_after_training_attack-reward-HalfCheetah-v4_3seed.pdf}
% }    
 	\caption{\normalsize 
   Adversary attack on state and action space in robust HalfCheetah-v4 tasks. S denotes attack on state and A denotes attack on action.
  % , and R denotes reward. 
   % Clearly show: HalfCheetah-v4 robustness: training attack on state, action, and reward. S denotes state, A denotes action, and R denotes reward. Note, in (f), train with reward attack, test w/o reward attack. \\
 	} 
  \label{fig:clear-show-robust-mujoco-halfcheetah-v4-state-action-attack-training-standard}
 \end{figure} 



\subsection{Evaluation of robust RL baselines}
In this section, we evaluate robust RL tasks using an environment disruptor under two representative modes: internal dynamic shift and external disturbance. The robust RL tasks are based on various task bases, including Ant-v5 and Hopper-v5 from Gymnasium-MuJoCo, as well as DoorCausal-v1 and LiftCausal-v1 from Robosuite, utilizing the In-training evaluation process.

Specifically, Figure \ref{fig:ompo-robust-non-stationary-ant-humanoid-walker2d-v5-4-settings}(a-b) displays the performance of the robust RL baseline OMPO across two tasks with internal dynamic shifts: (a) Ant-v5 with varying gravity and wind strength, and (b) Hopper-v5 with changes to the robot model's shape, including torso and foot length. Experimental settings can be found in \eqref{eq:non-stationary-ant-training-stochastic} and \eqref{eq:non-stationary-walker-training-stochastic} in Appendix \ref{appendix:sup-robust-of-robust-rl-task}. The results indicate that OMPO's performance significantly declines in non-stationary environments compared to stationary conditions without disturbances.

Figures \ref{fig:ompo-robust-non-stationary-ant-humanoid-walker2d-v5-4-settings}(c-d) illustrate the performance of three robust RL baselines (RSC, ATLA, DBC) in two tasks from Robosuite involving disruptions on the environment with external semantic disturbances. In the DoorCausal task, the initial distance of the door from the robot and the height of the door handle are varied in a correlated manner. In the CausalLift task, both the position and color of the object to be lifted are changed together according to specific patterns. RSC demonstrates greater robustness than ATLA and DBC, maintaining stable reward trajectories throughout the training process. However, RSC's training efficiency may need further improvement, as it generates augmentation data during policy learning.


% we investigate the effectivness of robust RL in semantic tasks: In the DoorCausal task, when the door's initial distance from the robot is beyond a certain threshold, the door handle will appear in a high position; when the door is close to the robot, the handle will be in a low position. In the CausalLift task, when the lift object is initialized randomly in the right areas on the table, its color will change to red; otherwise, it remains green. Specifically, we compare the robustness of three robust RL algorithms—RSC \citep{ding2024seeing}, ATLA \citep{zhang2021robust}, and DBC \citep{zhang2021learning}—under semantic attacks in the DoorCausal-v1 and LiftCausal-v1 environments. The RSC algorithm demonstrates superior robustness, maintaining stable reward trajectories throughout the training process. However, we may need to further improve RSC training efficiency since it generates augmentation data during policy learning.
% Overall, the results in this figure highlight the varying levels of robustness that robust RL algorithms exhibit when exposed to non-stationary and semantic perturbations. 

% In this section, as shown in Figure \ref{fig:ompo-robust-non-stationary-ant-humanoid-walker2d-v5-4-settings}, we provide different attack modes applied to robust RL environments, such as Ant-v5 and Hopper-v5 tasks, as well as a semantic attack on DoorCausal-v1. These experiments evaluate the robustness of robust RL algorithms, OMPO \citep{luoompo2024}, RSC \citep{ding2024seeing} and other algorithms under dynamic environmental changes and adversarial conditions. In Figure (a), OMPO is evaluated in the Ant-v5 environment under Stochastic and fixed wind and gravity disturbances on the system’s dynamics, in which varying disturbances  lead to more degradation in reward performance.
% Figure (b) investigates the impact of robot shape distrubances on the Hopper-v5 environment, focusing on shape perturbations. As in Ant-v5, fixed shape distrubance results in higher rewards.
% Figure (c) and (d) compares the robustness of three robust R: algorithms, RSC \citep{ding2024seeing}, ATLA \citep{zhang2021robust}, and DBC \citep{zhang2021learning}—under Semantic Attacks in the DoorCausal-v1 and LiftCausal-v1 environments. The RSC algorithm shows superior robustness, maintaining stable rewards across training steps.
% Overall, this figure demonstrates the varying degrees of resilience that OMPO and other algorithms exhibit when exposed to non-stationary and adversarial perturbations across different environments and attack scenarios. More experiments and specifical settings are provided in Appendix \ref{appendix:experiments-robust}.


\begin{figure}[tb!]
 \centering
 \subcaptionbox{Attack wind \& gravity}
  {
\includegraphics[width=0.23\linewidth]{figures/ompo/finished/clean_wind_gravity-all-episode-Non-Stationary-Ant-v5_3seeds.pdf}
}    
%  \subcaptionbox{Attack shape}
%   {
% \includegraphics[width=0.23\linewidth]{figures/ompo/finished/clean_action-gauss-only-all-episode-Non-Stationary-Ant-v5_3seeds.pdf}
% }   
 \subcaptionbox{Attack shape}
  {
\includegraphics[width=0.23\linewidth]{figures/ompo/finished/clean_deter-Non-Stationary-Hopper-v5_3seeds.pdf}
}    
\subcaptionbox{Attack position}
  {
\includegraphics[width=0.23\linewidth]{figures/semantic/rsc-DoorCausal-v1_3seeds.pdf}
}    
\subcaptionbox{Attack color}
  {
\includegraphics[width=0.23\linewidth]{figures/semantic/rsc-LiftCausal-v1_3seeds.pdf}
}   
\caption{\normalsize (a-b): Internal dynamic shift attacks on Ant-v5 and Hopper-v5 tasks. (c-d): External semantic attacks on Robosuite tasks.
% : In the DoorCausal task, when the door's initial distance from the robot is beyond a certain threshold, the door handle will appear in a high position; when the door is close to the robot, the handle will be in a low position. In the CausalLift task, when the lift object is initialized randomly in certain areas on the table, its color will change to red; otherwise, it remains green.
} 
  \label{fig:ompo-robust-non-stationary-ant-humanoid-walker2d-v5-4-settings}
 \end{figure} 

%  \begin{figure}[tb!]
%  \centering
% \subcaptionbox{DoorCausal}
%   {
% \includegraphics[width=0.31\linewidth]{figures/semantic/rsc-DoorCausal-v1_3seeds.pdf}
% }       
% \subcaptionbox{LiftCausal}
%   {
% \includegraphics[width=0.31\linewidth]{figures/semantic/rsc-LiftCausal-v1_3seeds.pdf}
% }    
% \subcaptionbox{StackCausal}
%   {
% \includegraphics[width=0.31\linewidth]{figures/semantic/only-gauss-rsc-StackCausal-v1_3seeds.pdf}
% }    
% \caption{\normalsize RSC \citep{ding2024seeing}: Robust DoorCausal Tasks with semantic attacks.
% } 
%   \label{fig:robust-rsc-doorcausal-semantic-attacks}
%  \end{figure} 


 \begin{comment}

 \begin{figure}[tb!]
 \centering
 \subcaptionbox{Attack on Robot Dynamics}
%    {
% \includegraphics[width=0.44\linewidth]{figures/ompo/Non-Stationary-Ant-v5_3seeds.pdf}
% }    
  {
\includegraphics[width=0.31\linewidth]{figures/ompo/Ant/only-all-episode-Non-Stationary-Ant-v5_3seeds.pdf}
}    
%  \subcaptionbox{During-Training Attack on Robot Shape}
%   {
% \includegraphics[width=0.44\linewidth]{figures/ompo/deter-Non-Stationary-Humanoid-v5_3seeds.pdf}
% }   
%  \subcaptionbox{Attack on Robot Shape}
%   {
% \includegraphics[width=0.44\linewidth]{figures/ompo/deter-Non-Stationary-Walker2d-v5_3seeds.pdf}
% }   
%   {
% \includegraphics[width=0.31\linewidth]{figures/ompo/walker/all-episode-deter-Non-Stationary-Walker2d-v5_3seeds.pdf}
% } 
 \subcaptionbox{Attack on Robot Shape}
%   {
% \includegraphics[width=0.44\linewidth]{figures/ompo/deter-Non-Stationary-Hopper-v5_3seeds.pdf}
% }   
  {
\includegraphics[width=0.31\linewidth]{figures/ompo/hopper/all-episode-Non-Stationary-Hopper-v5_3seeds.pdf}
}   
 \subcaptionbox{Attack on Robot Action}
  {
\includegraphics[width=0.31\linewidth]{figures/ompo/Ant/action-gauss-only-all-episode-Non-Stationary-Ant-v5_3seeds.pdf}
}    
\caption{\normalsize \textbf{OMPO:} Non-stationary adversary attacks on Ant-v5 Tasks.
} 
  \label{fig:ompo-robust-non-stationary-ant-humanoid-walker2d-v5-4-settings}
 \end{figure} 
 
\begin{figure}[tb!]
 \centering
 \subcaptionbox{Attack on Robot State}
  {
\includegraphics[width=0.31\linewidth]{figures/ompo/Ant/state-gauss-only-all-episode-Non-Stationary-Ant-v5_3seeds.pdf}
}    
 \subcaptionbox{Attack on Robot Action}
  {
\includegraphics[width=0.31\linewidth]{figures/ompo/Ant/action-gauss-only-all-episode-Non-Stationary-Ant-v5_3seeds.pdf}
}    

\caption{\normalsize \textbf{OMPO:} Non-stationary adversary attacks on Ant-v5 Tasks.
} 
  \label{fig:ompo-robust-non-stationary-ant-v5-4-settings-compare-gauss}
 \end{figure} 

  \begin{figure}[tb!]
 \centering
\subcaptionbox{DoorCausal}
  {
\includegraphics[width=0.31\linewidth]{figures/semantic/rsc-DoorCausal-v1_3seeds.pdf}
}       
\subcaptionbox{LiftCausal}
  {
\includegraphics[width=0.31\linewidth]{figures/semantic/rsc-LiftCausal-v1_3seeds.pdf}
}    
\subcaptionbox{StackCausal}
  {
\includegraphics[width=0.31\linewidth]{figures/semantic/only-gauss-rsc-StackCausal-v1_3seeds.pdf}
}    
\caption{\normalsize RSC \citep{ding2024seeing}: Robust DoorCausal Tasks with semantic attacks.
} 
  \label{fig:robust-rsc-doorcausal-semantic-attacks}
 \end{figure} 
 \end{comment}


 



 \subsection{Evaluation of safe RL baselines}
Two safe RL baselines, PCRPO \citep{gu2024balance} and CRPO \citep{xu2021crpo}, are benchmarked on robust safety-critical tasks using the In-training evaluation process. Specifically, we assess two types of robust RL tasks based on Walker2d from Gymnasium-MuJoCo:  (a) an action-disruption attacks the agent's action with different levels; (b) the agent's observe immediate safety cost is disturbed in different levels. These attacks follow a Gaussian distribution with a mean of 0 and standard deviations of 0.15 or 0.3 for both the action and the observed cost. The outcomes and safety costs for these tasks are presented in Figures \ref{fig:robust-safety-mujoco-pcrpo-4settings}(a-b) and Figures \ref{fig:robust-safety-mujoco-pcrpo-4settings}(c-d), respectively. The performance of CRPO quickly degrades when disruptions occur, while PCRPO demonstrates greater robustness against disturbances in either action or observed cost. Notably, PCRPO's performance under disturbance surpasses its performance without disturbance, suggesting that introducing appropriate disturbances during training may enhance overall performance. Due to space limitations, additional results can be found in Appendix \ref{appendix:sup-robust-of-safe-rl}.
 


% reward performance remains more robust than that of CRPO. The attacks follow a Gaussian distribution with a mean of 0 and standard deviations of 0.15 and 0.3, mirroring the algorithm's safety performance. Similarly, applying the same distribution to attack the robot's cost signal does not significantly degrade PCRPO's reward performance. Surprisingly, it even outperforms scenarios without adversarial attacks. This suggests that the attack may enhance policy learning diversity, helping to explore more effective policies. More experiments are provided in Appendix \ref{appendix:sup-robust-of-safe-rl}.

\begin{figure}[tb!]
 \centering
%  \subcaptionbox*{(a) Attack on Robot State}
%   {
% \includegraphics[width=0.31\linewidth]{figures/safety-mujoco/compare-crpo/crpo_during_state-2sr-limit-40-slack-5-SafetyWalker2d-v4-reward_3seeds.pdf}
% }     
 \subcaptionbox*{(a) Attack \textbf{safe action}}
  {
\includegraphics[width=0.23\linewidth]{figures/safety-mujoco/compare-crpo/crpo_during_action-2sr-limit-40-slack-5-SafetyWalker2d-v4-reward_3seeds.pdf}
}     
\subcaptionbox*{(b) Attack \textbf{safe action}}
  {
\includegraphics[width=0.23\linewidth]{figures/safety-mujoco/compare-crpo/crpo_during_action_2sr-limit-40-slack-5-SafetyWalker2d-v4-costs_3seeds.pdf}
}  
 \subcaptionbox*{(c) Attack \textbf{cost signal}}
  {
\includegraphics[width=0.23\linewidth]{figures/safety-mujoco/compare-crpo/crpo_during_cost-2sr-limit-40-slack-5-SafetyWalker2d-v4-reward_3seeds.pdf}
}  
\subcaptionbox*{(d) Attack \textbf{cost signal}}
  {
\includegraphics[width=0.23\linewidth]{figures/safety-mujoco/compare-crpo/crpo_during_cost_2sr-limit-40-slack-5-SafetyWalker2d-v4-costs_3seeds.pdf}
}   
% \subcaptionbox*{(b) Attack on Robot State}
%   {
% \includegraphics[width=0.31\linewidth]{figures/safety-mujoco/compare-crpo/crpo_during_state_2sr-limit-40-slack-5-SafetyWalker2d-v4-costs_3seeds.pdf}
% }  
\caption{\normalsize Robust safe RL rasks: Random disturbances over either the action or the agent's observed immediate cost feedback.
} 
  \label{fig:robust-safety-mujoco-pcrpo-4settings}
 \end{figure} 

\begin{comment}
\begin{figure}[tb!]
 \centering
 \subcaptionbox*{(a) Attack on Robot State}
  {
\includegraphics[width=0.31\linewidth]{figures/safety-mujoco/compare-crpo/crpo_during_state-2sr-limit-40-slack-5-SafetyWalker2d-v4-reward_3seeds.pdf}
}     
 \subcaptionbox*{(c) Attack on Robot Action}
  {
\includegraphics[width=0.31\linewidth]{figures/safety-mujoco/compare-crpo/crpo_during_action-2sr-limit-40-slack-5-SafetyWalker2d-v4-reward_3seeds.pdf}
}     
 \subcaptionbox*{(e) Attack on Robot Cost Signal}
  {
\includegraphics[width=0.31\linewidth]{figures/safety-mujoco/compare-crpo/crpo_during_cost-2sr-limit-40-slack-5-SafetyWalker2d-v4-reward_3seeds.pdf}
}    
\subcaptionbox*{(b) Attack on Robot State}
  {
\includegraphics[width=0.31\linewidth]{figures/safety-mujoco/compare-crpo/crpo_during_state_2sr-limit-40-slack-5-SafetyWalker2d-v4-costs_3seeds.pdf}
}  
\subcaptionbox*{(d) Attack on Robot Action}
  {
\includegraphics[width=0.31\linewidth]{figures/safety-mujoco/compare-crpo/crpo_during_action_2sr-limit-40-slack-5-SafetyWalker2d-v4-costs_3seeds.pdf}
}  
\subcaptionbox*{(f) Attack on Robot Cost Signal}
  {
\includegraphics[width=0.31\linewidth]{figures/safety-mujoco/compare-crpo/crpo_during_cost_2sr-limit-40-slack-5-SafetyWalker2d-v4-costs_3seeds.pdf}
}   
\caption{\normalsize Robust Safety RL Tasks.
} 
  \label{fig:robust-safety-mujoco-pcrpo-4settings}
 \end{figure} 
\end{comment}

 \begin{comment}
\begin{figure}[htbp!]
 \centering
 \subcaptionbox{During-Training Attack on Robot}
  {
\includegraphics[width=0.31\linewidth]{figures/safety-mujoco/2sr-limit-40-slack-5-SafetyWalker2d-v4-reward_3seeds.pdf}
}    
\subcaptionbox{During-Training Attack on Robot}
  {
\includegraphics[width=0.31\linewidth]{figures/safety-mujoco/2sr-limit-40-slack-5-SafetyWalker2d-v4-costs_3seeds.pdf}
}    
\subcaptionbox{During-Training Attack on Robot}
  {
\includegraphics[width=0.31\linewidth]{figures/safety-mujoco/action-2sr-limit-40-slack-5-SafetyWalker2d-v4-reward_3seeds.pdf}
}    
\subcaptionbox{During-Training Attack on Robot}
  {
\includegraphics[width=0.31\linewidth]{figures/safety-mujoco/action-2sr-limit-40-slack-5-SafetyWalker2d-v4-costs_3seeds.pdf}
}    
\subcaptionbox{During-Training Attack on Robot}
  {
\includegraphics[width=0.31\linewidth]{figures/safety-mujoco/reward-2sr-limit-40-slack-5-SafetyWalker2d-v4-reward_3seeds.pdf}
}
\subcaptionbox{During-Training Attack on Robot}
  {
\includegraphics[width=0.31\linewidth]{figures/safety-mujoco/reward-2sr-limit-40-slack-5-SafetyWalker2d-v4-costs_3seeds.pdf}
}  
\subcaptionbox{During-Training Attack on Robot}
  {
\includegraphics[width=0.31\linewidth]{figures/safety-mujoco/cost-2sr-limit-40-slack-5-SafetyWalker2d-v4-reward_3seeds.pdf}
}  
\subcaptionbox{During-Training Attack on Robot}
  {
\includegraphics[width=0.31\linewidth]{figures/safety-mujoco/cost-2sr-limit-40-slack-5-SafetyWalker2d-v4-costs_3seeds.pdf}
}  
\subcaptionbox{Post-Training Attack on Robot}
  {
\includegraphics[width=0.31\linewidth]{figures/safety-mujoco/eva_S_2sr-limit-40-slack-5-SafetyWalker2d-v4-reward_3seeds.pdf}
}    
\subcaptionbox{Post-Training Attack on Robot}
  {
\includegraphics[width=0.31\linewidth]{figures/safety-mujoco/eva-state-2sr-limit-40-slack-5-SafetyWalker2d-v4-costs_3seeds.pdf}
}    
\subcaptionbox{Post-Training Attack on Robot}
  {
\includegraphics[width=0.31\linewidth]{figures/safety-mujoco/eva_A_2sr-limit-40-slack-5-SafetyWalker2d-v4-reward_3seeds.pdf}
}    
\subcaptionbox{Post-Training Attack on Robot}
  {
\includegraphics[width=0.31\linewidth]{figures/safety-mujoco/eva-action-2sr-limit-40-slack-5-SafetyWalker2d-v4-costs_3seeds.pdf}
}    
\caption{\normalsize Robust Safety RL Tasks.
} 
  \label{fig:robust-safety-mujoco-pcrpo-4settings}
 \end{figure} 
 \end{comment}


 \subsection{Evaluation of multi-Agent RL baselines}
We evaluate two MARL baselines: Multi-Agent PPO (MAPPO) \citep{yu2022surprising} and Independent PPO (IPPO) \citep{de2020independent} on MA-HalfCheetah-v4 from MAMoJoCo under various disruption settings affecting the agents' observed states, actions, and rewards. Using the In-training evaluation process, as shown in Figure \ref{fig:robust-multi-agent-mujoco-halfcheetah-2x3-4settings}, we apply disruptions to all agents. The results indicate that the performance of both MAPPO and IPPO degrades accordingly as the disruptions occur. Additionally, we conduct experiments involving \textbf{partial disruptions} on a subset of agents within the multi-agent system; further details can be found in Appendix \ref{appendix:sup-robust-of-ma-rl}.

% For each scenario, MAPPO tends to maintain a higher reward than IPPO, showing better robustness to adversarial attacks. 

% Figures \ref{fig:robust-multi-agent-mujoco-halfcheetah-2x3-4settings} (d), (e), and (f) focus on partial state, action, and reward attacks, where only a subset of agents or aspects is attacked. These figures show a smaller drop in performance, indicating partial attacks are less harmful compared to full attacks. For each scenario, MAPPO tends to maintain a higher reward than IPPO, showing better robustness to adversarial attacks. More experiments and specifical settings are provided in Appendix \ref{appendix:sup-robust-of-ma-rl}.
% The colored regions represent the standard deviation, indicating variability across multiple training runs.

\begin{figure}[htbp!]
 \centering
 \subcaptionbox{Attack \textbf{all agents' state}}
  {
\includegraphics[width=0.31\linewidth]{figures/marl/ma-mappo-ippo-training-attack-state-MA-HalfCheetah-v4_3seeds.pdf}
}    
 \subcaptionbox{Attack \textbf{all agents' action}}
  {
\includegraphics[width=0.31\linewidth]{figures/marl/ma-mappo-ippo-training-attack-action-MA-HalfCheetah-v4_3seeds.pdf}
}   
\subcaptionbox{Attack \textbf{reward signal}}
  {
\includegraphics[width=0.31\linewidth]{figures/marl/ma-mappo-ippo-training-attack-reward-MA-HalfCheetah-v4_3seeds.pdf}
} 
% \subcaptionbox{Attack \textbf{partial state}}
%   {
% \includegraphics[width=0.31\linewidth]{figures/marl/partial/part-ma-mappo-training-attack-state-MA-HalfCheetah-v4_3seeds.pdf}
% } 
% \subcaptionbox{Attack \textbf{partial action}}
%   {
% \includegraphics[width=0.31\linewidth]{figures/marl/partial/part-ma-mappo-training-attack-action-MA-HalfCheetah-v4_3seeds.pdf}
% } 
% \subcaptionbox{Attack \textbf{partial reward}}
%   {
% \includegraphics[width=0.31\linewidth]{figures/marl/partial/part-ma-mappo-training-attack-reward-MA-HalfCheetah-v4_3seeds.pdf}
% } 
\caption{\normalsize Multi-Agent HalfCheetah-2x3 robustness: training attack on state, action, and reward for all the two agents. S denotes state, A denotes action and R denotes reward.
} 
  \label{fig:robust-multi-agent-mujoco-halfcheetah-2x3-4settings}
 \end{figure} 

\begin{comment}
\begin{figure}[htbp!]
 \centering
 \subcaptionbox{Attack \textbf{all agents' state}}
  {
\includegraphics[width=0.31\linewidth]{figures/marl/ma-mappo-ippo-training-attack-state-MA-HalfCheetah-v4_3seeds.pdf}
}    
 \subcaptionbox{Attack \textbf{all agents' action}}
  {
\includegraphics[width=0.31\linewidth]{figures/marl/ma-mappo-ippo-training-attack-action-MA-HalfCheetah-v4_3seeds.pdf}
}   
\subcaptionbox{Attack \textbf{reward signal}}
  {
\includegraphics[width=0.31\linewidth]{figures/marl/ma-mappo-ippo-training-attack-reward-MA-HalfCheetah-v4_3seeds.pdf}
} 
% \subcaptionbox{Attack \textbf{partial state}}
%   {
% \includegraphics[width=0.31\linewidth]{figures/marl/partial/part-ma-mappo-training-attack-state-MA-HalfCheetah-v4_3seeds.pdf}
% } 
% \subcaptionbox{Attack \textbf{partial action}}
%   {
% \includegraphics[width=0.31\linewidth]{figures/marl/partial/part-ma-mappo-training-attack-action-MA-HalfCheetah-v4_3seeds.pdf}
% } 
% \subcaptionbox{Attack \textbf{partial reward}}
%   {
% \includegraphics[width=0.31\linewidth]{figures/marl/partial/part-ma-mappo-training-attack-reward-MA-HalfCheetah-v4_3seeds.pdf}
% } 
\caption{\normalsize Multi-Agent HalfCheetah-2x3 robustness: training attack on state, action, and reward for all the two agents. S denotes state, A denotes action and R denotes reward.
} 
  \label{fig:robust-multi-agent-mujoco-halfcheetah-2x3-4settings}
 \end{figure} 
\end{comment}

 % The all agents are attacked (a, b and c) or some of agents are attacked (d, e and f) with various adversary attack factors.

 % , P denotes partial attacks. 

 \begin{comment}
\begin{figure}[htbp!]
 \centering
 \subcaptionbox{During-Training Attack on State}
  {
\includegraphics[width=0.31\linewidth]{figures/mappo/HalfCheetah-2x3_State Attack_3seeds.pdf}
}    
 \subcaptionbox{During-Training Attack on Action}
  {
\includegraphics[width=0.31\linewidth]{figures/mappo/HalfCheetah-2x3_Action Attack_3seeds.pdf}
}   
\subcaptionbox{During-Training Attack on Reward}
  {
\includegraphics[width=0.31\linewidth]{figures/mappo/HalfCheetah-2x3_Reward Attack_3seeds.pdf}
} 

 \subcaptionbox{Post-Training Attack on State}
  {
\includegraphics[width=0.31\linewidth]{figures/mappo/after_training_HalfCheetah-2x3_State Attack_3seeds.pdf}
}    
 \subcaptionbox{Post-Training Attack on Action}
  {
\includegraphics[width=0.31\linewidth]{figures/mappo/after_training_HalfCheetah-2x3_Action Attack_3seeds.pdf}
}   
\subcaptionbox{Post-Training Attack on Reward (train with reward attack, test w/o reward attack)}
  {
\includegraphics[width=0.31\linewidth]{figures/mappo/after_training_HalfCheetah-2x3_Reward Attack_3seeds.pdf}
} 

\caption{\normalsize Multi-Agent HalfCheetah-2x3 robustness: training attack on state, action, and reward for all the two agents. S denotes state, A denotes action and R denotes reward.
} 
  \label{fig:robust-multi-agent-mujoco-halfcheetah-2x3-4settings}
 \end{figure} 
 \end{comment}

 \begin{comment}
 \begin{figure}[htbp!]
 \centering
 \subcaptionbox{During-Training Attack on State}
  {
\includegraphics[width=0.31\linewidth]{figures/mappo/partial/HalfCheetah-2x3_Partial State Attack_3seeds.pdf}
}    
 \subcaptionbox{During-Training Attack on Action}
  {
\includegraphics[width=0.31\linewidth]{figures/mappo/partial/HalfCheetah-2x3_Partial Action Attack_3seeds.pdf}
}   
\subcaptionbox{During-Training Attack on Reward}
  {
\includegraphics[width=0.31\linewidth]{figures/mappo/partial/HalfCheetah-2x3_Partial Reward Attack_3seeds.pdf}
} 

%  \subcaptionbox{Post-Training Attack on State}
%   {
% \includegraphics[width=0.31\linewidth]{figures/mappo/partial/after_training_HalfCheetah-2x3_Partial State Attack_3seeds.pdf}
% }    
%  \subcaptionbox{Post-Training Attack on Action}
%   {
% \includegraphics[width=0.31\linewidth]{figures/mappo/partial/after_training_HalfCheetah-2x3_Partial Action Attack_3seeds.pdf}
% }   
% \subcaptionbox{Post-Training Attack on Reward (train with reward attack, test w/o reward attack)}
%   {
% \includegraphics[width=0.31\linewidth]{figures/mappo/partial/after_training_HalfCheetah-2x3_Partial Reward Attack_3seeds.pdf}
% } 

\caption{\normalsize Multi-Agent HalfCheetah-2x3 robustness: training attack on state, action, and reward for one of two agents. S denotes state, A denotes action and R denotes reward.
} 
  \label{fig:robust-multi-agent-mujoco-halfcheetah-2x3-partial-4settings}
 \end{figure} 
 \end{comment}

 \begin{comment}
\subsection{Robust Tasks with Shift Attacks}

 \begin{figure}[htbp!]
 \centering
 \subcaptionbox{During-Training Attack}
  {
\includegraphics[width=0.31\linewidth]{figures/robust-mujoco/shift/shift-training-attack-state-HalfCheetah-v4_3seeds.pdf}
}    
 \subcaptionbox{During-Training Attack}
  {
\includegraphics[width=0.31\linewidth]{figures/robust-mujoco/shift/shift-training-attack-action-HalfCheetah-v4_3seeds.pdf}
}    
 \subcaptionbox{During-Training Attack}
  {
\includegraphics[width=0.31\linewidth]{figures/robust-mujoco/shift/shift-training-attack-reward-HalfCheetah-v4_3seeds.pdf}
}    
\caption{\normalsize Robust HalfCheetah Tasks with shift attacks.
} 
  \label{fig:robust-ppo-halfcheetah-shift-attacks-4settings}
 \end{figure} 
 \end{comment}

 \subsection{Adversarial disturbance through LLMs}

% In this benchmark, we employ LLMs as adversarial agents to evaluate the robustness of  RL algorithms across a variety of attack settings. Figure \ref{fig:robust-ppo-mujoco-llms-main-clean-re} illustrates the results of LLM-driven adversarial attacks on the Ant-v4 environment, targeting the state space. We explore several attack configurations, including uniform, Gaussian, frequency-based, and source-specific attacks, as shown in the subfigures. Specifically, \textbf{Figure (a)} illustrates the impact of uniform LLM-based attacks, where adversarial agents introduce uniform perturbations into the agent's state. Despite the presence of these attacks, the PPO baseline maintains a relatively stable performance trajectory, albeit with slower progress as compared to the non-attacked setting. \textbf{Figure (b)} compares the effects of Gaussian-distributed adversarial noise and LLM-based attacks, at different noise levels. The results indicate that while the introduction of Gaussian noise ($\sigma = 0.01$ and $\sigma = 0.1$) impairs performance, models subjected to larger noise levels ($\sigma = 0.1$) experience more significant degradation. \textbf{Figure (c)} analyzes the effect of varying attack frequencies, where LLMs apply adversarial policies at different intervals. Higher frequency attacks (PPO-S-100F) tend to result in more significant performance degradation, yet the PPO baseline still achieves reasonable learning progress, highlighting its relative resilience to these types of perturbations. \textbf{Figure (d)} investigates the impact of different LLM attack sources, including uniformly distributed attacks and random perturbations. The results suggest that uniform adversarial policies have a more pronounced negative impact on learning performance, whereas random attacks, though detrimental, result in a less severe decline in performance.
% Across all configurations, LLM-based adversarial policies pose substantial challenges to the learning process, especially in scenarios involving frequent and highly variable attacks. Future research needs to focus on harnessing LLM-driven adversarial interventions to enhance the robustness of RL, particularly before deploying these models in diverse and real-world applications. More experiments and specifical settings are provided in Appendix \ref{appendix:experiments-robust}.

In addition to benchmarking various existing RL algorithms, this section demonstrates the adversarial disturbance mode by leveraging a featured approach with LLMs. As shown in Figure \ref{fig:robust-ppo-mujoco-llms-main-clean-re}, we evaluate the performance of PPO on Ant-v4 with adversarial disruptions to the agent's observed state. Different attack configurations are employed, including comparisons to uniform noise and testing varying frequencies. Here, ``C[0.2–0.8]'' indicates that the noise level from the LLM is constrained within the [0.2, 0.8] range; ``100F'' (resp. ``500F") signifies that the agent is attacked every 100 (resp. 500) steps; and ``U[0.2–0.8]" represents noise drawn from a uniform distribution $\cU(0.2,0.8)$. The results show that LLM-based attacks lead to a more significant performance drop for PPO compared to that using uniform distribution (Figure \ref{fig:robust-ppo-mujoco-llms-main-clean-re}(a)). {Figure (b)} examines how varying attack frequencies affect performance, revealing that higher-frequency attacks (PPO-S-100F) result in greater performance degradation. Due to space constraints, additional frequency experiments on other robust tasks based on Gymnasium-MuJoCo using PPO are provided in Appendix \ref{appendix-frequency:robust-mujoco}.


% Figure \ref{fig:robust-ppo-mujoco-llms-main-clean-re} presents the results of LLM-driven adversarial attacks on the Ant-v4 environment, focusing on the state space. Several attack configurations, including uniform, Gaussian, and frequency-based are explored. Figure (a) illustrates the effect of LLM-based attacks, where adversarial LLM agents introduce perturbations into the state space. ``C[0.2–0.8]" denotes that the LLM noise is constrained within the [0.2, 0.8] range. ``S" indicates that we attack the agent's state. ``100F" denotes that we attack the agent every 100 steps, and ``500F" denotes that we attack the agent every 500 steps. ``U[0.2–0.8]" means that we use a uniform distribution to attack the agent's state within the [0.2, 0.8] interval. Despite the presence of these disruptions, the PPO baseline maintains a stable performance, although progress is slower compared to the non-attacked scenario. Moreover, LLM-based attacks result in worse performance for PPO compared to uniform-based attacks. {Figure (b)} compares Gaussian-distributed noise at different levels with LLM-based attacks. Models subjected to higher noise levels (standard deviation $ = 0.1$) exhibit greater performance degradation than those with smaller noise levels (standard deviation $= 0.01$). {Figure (c)} examines how varying attack frequencies affect performance. Higher-frequency attacks (PPO-S-100F) result in more significant performance degradation, yet the PPO model still shows robustness, continuing to learn. 

% % {Figure (d)} analyzes the impact of different LLM attack sources, revealing that uniform adversarial policies result in a more substantial performance decline compared to random attacks, which cause a less severe performance drop.

% Across all configurations, LLM-based adversarial policies pose a powerful ability to attack agents. Future research needs to focus on harnessing LLM-driven adversarial interventions to enhance the robustness of RL, particularly before deploying these models in diverse and real-world applications. Due to space limitations, we provide additional frequency experiments on robust MuJoCo using PPO in Appendix \ref{appendix-frequency:robust-mujoco}.

% More experiments and specifical settings are provided in Appendix \ref{appendix:sup-robust-of-llms-task}.

% Overall, LLM-driven adversarial policies challenge the robustness of RL algorithms, particularly under high-intensity, frequent attacks.





% Nevertheless, PPO-S-100F demonstrates a greater degree of robustness against such perturbations.

% \begin{itemize}
    % \item \textbf{Figure (a)}: This subfigure illustrates the impact of uniform LLM-based attacks, where adversarial policies introduce uniform perturbations into the agent's state. Despite the presence of these attacks, the PPO baseline maintains a relatively stable performance trajectory, albeit with slower progress as compared to the non-attacked setting.

    % \item \textbf{Figure (b)}: This subfigure compares the effects of Gaussian-distributed adversarial noise, generated by LLMs, at different noise levels. The results indicate that while the introduction of Gaussian noise ($\sigma = 0.01$ and $\sigma = 0.1$) impairs performance, models subjected to larger noise levels ($\sigma = 0.1$) experience more significant degradation. Nevertheless, PPO-S-100F demonstrates a greater degree of robustness against such perturbations.

    % \item \textbf{Figure (c)}: This subfigure analyzes the effect of varying attack frequencies, where LLMs apply adversarial policies at different intervals. Higher frequency attacks (PPO-S-500F) tend to result in more significant performance degradation, yet the PPO baseline still achieves reasonable learning progress, highlighting its relative resilience to these types of perturbations.

    % \item \textbf{Figure (d)}: This subfigure investigates the impact of different LLM attack sources, including uniformly distributed attacks and random perturbations. The results suggest that random adversarial policies have a more pronounced negative impact on learning performance, whereas uniform attacks, though detrimental, result in a less severe decline in performance.
% \end{itemize}

\begin{figure}[tb!]
 \centering
  \subcaptionbox{LLMs v.s. uniform}
  {
\includegraphics[width=0.43\linewidth]{figures/llm-attack/finished/constraint-uniform0dot8-sameF-training-llm-compare-state-Ant-v4_3seeds.pdf}
}  
%  \subcaptionbox{Compare LLMs with Gaussian attack}
%   {
% \includegraphics[width=0.31\linewidth]{figures/llm-attack/finished/constraint_sameF-training-llm-compare-attack-state-Ant-v4_3seeds.pdf}
% }     
 \subcaptionbox{Varying frequencies of attacking}
  {
\includegraphics[width=0.43\linewidth]{figures/llm-attack/finished/constraint-compareself-100-500-sameF-training-llm-compare-state-Ant-v4_3seeds.pdf}
}  
\caption{\normalsize LLM-based attacks with different settings.
} 
\label{fig:robust-ppo-mujoco-llms-main-clean-re}
 \end{figure} 


\begin{comment}
\begin{figure}[tb!]
 \centering
  \subcaptionbox{Compare LLMs with uniform attack}
  {
\includegraphics[width=0.31\linewidth]{figures/llm-attack/finished/clean-uniform0dot8-sameF-training-llm-compare-state-Ant-v4_3seeds.pdf}
}  
 \subcaptionbox{Compare LLMs with Gaussian attack}
  {
\includegraphics[width=0.31\linewidth]{figures/llm-attack/finished/clean_sameF-training-llm-compare-attack-state-Ant-v4_3seeds.pdf}
}     
 \subcaptionbox{Frequency of LLM attack}
  {
\includegraphics[width=0.31\linewidth]{figures/llm-attack/finished/clean_DF-same-training-llm-attack-state-Ant-v4_3seeds.pdf}
}  
%  \subcaptionbox{Different LLM attack sources}
%   {
% \includegraphics[width=0.23\linewidth]{figures/llm-attack/finished/clean_uniform-random-100F-training-llm-attack-state-Ant-v4_3seeds.pdf}
% }  
\caption{\normalsize LLM-based attacks with different settings.
} 
\label{fig:robust-ppo-mujoco-llms-main-clean-re}
 \end{figure} 
\end{comment}

 \begin{comment}
\begin{figure}[htbp!]
 \centering
  \subcaptionbox{Same source distribution with same frequency attacks}
  {
\includegraphics[width=0.31\linewidth]{figures/llm-attack/uniform-sameF-training-llm-compare-attack-state-Ant-v4_3seeds.pdf}
}  
 \subcaptionbox{Different frequency attacks}
  {
\includegraphics[width=0.31\linewidth]{figures/llm-attack/training-llm-attack-state-Ant-v4_3seeds.pdf}
}     
 \subcaptionbox{Different source and level attacks}
  {
\includegraphics[width=0.31\linewidth]{figures/llm-attack/training-llm-compare-attack-state-Ant-v4_3seeds.pdf}
}   
 \subcaptionbox{Different source with different frequency attacks}
  {
\includegraphics[width=0.31\linewidth]{figures/llm-attack/same-training-llm-compare-attack-state-Ant-v4_3seeds.pdf}
}  
 \subcaptionbox{Different source with same frequency attacks}
  {
\includegraphics[width=0.31\linewidth]{figures/llm-attack/sameF-training-llm-compare-attack-state-Ant-v4_3seeds.pdf}
}  
  \label{fig:robust-ppo-mujoco-llms-main}
 \end{figure} 

 \begin{figure}[htbp!]
 \centering
 \subcaptionbox{Attack State}
  {
\includegraphics[width=0.31\linewidth]{figures/frequency/mujoco/state-frequency50-Ant-v4_3seeds.pdf}
}        
\subcaptionbox{Attack State}
  {
\includegraphics[width=0.31\linewidth]{figures/frequency/mujoco/state-frequency100-Ant-v4_3seeds.pdf}
}        
\subcaptionbox{Attack State}
  {
\includegraphics[width=0.31\linewidth]{figures/frequency/mujoco/state-frequency500-Ant-v4_3seeds.pdf}
}       
\subcaptionbox{Attack Action}
  {
\includegraphics[width=0.31\linewidth]{figures/frequency/mujoco/action-frequency50-Ant-v4_3seeds.pdf}
}       
\subcaptionbox{Attack Action}
  {
\includegraphics[width=0.31\linewidth]{figures/frequency/mujoco/action-frequency100-Ant-v4_3seeds.pdf}
}       
\subcaptionbox{Attack Action}
  {
\includegraphics[width=0.31\linewidth]{figures/frequency/mujoco/action-frequency250-Ant-v4_3seeds.pdf}
}       
\subcaptionbox{Attack Action}
  {
\includegraphics[width=0.31\linewidth]{figures/frequency/mujoco/action-frequency500-Ant-v4_3seeds.pdf}
}       
\subcaptionbox{Attack State}
  {
\includegraphics[width=0.31\linewidth]{figures/frequency/mujoco/state0.15-frequency-Ant-v4_3seeds.pdf}
}       
\caption{\normalsize Robust Ant Tasks with Frequency attacks.
} 
  \label{fig:robust-ppo-ant-frequency-attacks-main}
 \end{figure} 
 \end{comment}