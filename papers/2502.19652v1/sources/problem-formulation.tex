\section{A Unified Robust Reinforcement Learning Framework}\label{sec:framework}



 
We begin by presenting a robust RL framework that unifies various robust RL tasks explored in the literature, including combinations of these paradigms. We outline the framework in the context of single-agent RL and then extend it to encompass broader classes of RL tasks, such as safe RL and multi-agent RL.
 
\paragraph{Background: Markov decision process (MDP).}\label{sec:background}

A single-agent RL problem is formulated as a finite-horizon Markov decision process (MDP), represented by the tuple $\cM = \big(\mathcal{S}, \mathcal{A}, T, P^\no, r^\no \big)$, where $\cS$ and $\cA$ denote the (possibly infinite) state and action spaces, and $T$ is the horizon length. The nominal transition kernel $P^0= \{P^0_t \}_{1\leq t\leq T}$ defines the environmental dynamics: $P^\no_t(s' \mymid s, a)$ gives the probability of transitioning from state $s$ to state $s'$ given action $a$ at time step $t$. The reward function $r^\no= \{r^\no_t\}_{1 \leq t \leq T}$ represents the immediate reward at time step $t$, given the current state $s$ and action $a$.

% processing process of standard RL
% The standard agent-environment interaction process proceeds as follows: at each time step $t \in [T]$, the agent observes the current state $s_t$ and the immediate reward $r_{t}(s_{t-1}, a_{t-1})$ provided by the environment. The agent then selects an action $a_t \sim \pi_t( \cdot \mid s_t)$, based on its policy $\pi_t: \cS \rightarrow \Delta(\cA)$, which specifies the probability distribution over actions in $\cA$ given state $s_t$. The environment receives the action $a_t$, transitions to the next state $s_{t+1}$ according to the transition probability $P^\no_t(s_{t+1} \mid s_t, a_t)$, and provides the immediate reward $r^\no_{t+1}(s_t, a_t)$ as feedback. The state $s_{t+1}$ and reward $r^\no_{t+1}$ are then sent back to the agent for the decision-making process at next step $t+1$.

  \begin{figure}[htbp!]
 \centering
 % \subcaptionbox{}
  {
\includegraphics[width=0.85\linewidth]{figures/framework/overview-framework2.png}
}    
 \caption{\normalsize The overview of a finite-horizon MDP with disruptors. } 
  \label{fig:framework-overview-robust-rl}
 \end{figure} 


\subsection{A unified robust RL framework: MDPs with disruption}

\label{sec:robust-RL-formulation}
To proceed, we introduce an additional disruption module that represents potential uncertainties or disturbances that impact different stages of the agent-environment interaction process (MDP). 
This module provides a categorized summary of the types of uncertainty addressed in prior robust RL studies.
\paragraph{Disruptors.} We introduce each type in detail as follows:

\begin{itemize}
\item \textit{Observation-disruptor.} An agent’s observations may not perfectly reflect the true status of the environment due to factors like sensor noise and time delays. To model this sensing inaccuracy, we introduce an additional module—the observation-disruptor—which determines the agent’s observations from the environment: {\em Agents' observed state $\widetilde{s}_t$:} The observation-disruptor takes the true current state $s_t$ as input and outputs a perturbed state $\widetilde{s}_t = D_{\mathsf{s}}(s_t)$. The agent uses $\widetilde{s}_t$ as input to its policy to select an action; {\em Agents' observed reward $\widetilde{r}_t$:} The observation-disruptor takes the real immediate reward $r_t$ as input and outputs a perturbed reward $\widetilde{r}_t = D_{\mathsf{r}}(r_t)$. The agent observes $\widetilde{r}_t$ and updates its policy accordingly.

\item \textit{Action-disruptor.} The real action $a_t$ chosen by the agent may be altered before or during execution in the environment due to implementation inaccuracies or system malfunctions. The action-disruptor models this perturbation, outputting a perturbed action $\widetilde{a}_t = D_{\mathsf{a}}(a_t)$, which is then executed in the environment for the next step.

\item \textit{Environment-disruptor.} 
Recall that a task environment consists of both the internal dynamic model and the external workspace it interacts with, characterized by its transition dynamics $P$ and reward function $r$. The environment during training can differ from the real-world environment due to factors such as the sim-to-real gap, human and natural variability, external disturbances, and more. We attribute this potential nonstationarity to an environment-disruptor, which determines the actual environment $(P, r)$ the agent is interacting with at any given moment. These dynamics may differ from the nominal environment $(P^0, r^0)$ that the agent was originally expected to interact with.

\end{itemize}


% the {\em observation-disruptor}, with functions $D_{\mathsf{s}}(\cdot)$ and $D_{\mathsf{r}}(\cdot)$, perturbs the agent's observations (states and rewards); the {\em action-disruptor} perturbs actions using function $D_{\mathsf{a}}(\cdot)$; \colorbox{yellow}{the {\em environment-disruptor} modifies the environment,} resulting in a shifted transition kernel $P$ and reward function $r$. Further details on the disruptors are provided in the next subsection.

\paragraph{MDPs with Disruption.}
As shown in Fig.~\ref{fig:framework-overview-robust-rl}, a robust RL problem can be formulated as a finite-horizon MDP with an additional disruption module $\cM_{\mathsf{dis}} = \big(\mathcal{S}, \mathcal{A}, T, P, r, D_{\mathsf{s}}(\cdot), D_{\mathsf{r}}(\cdot), D_{\mathsf{a}}(\cdot)\big)$, abbreviated as \textbf{\fname}. It consists of three potential disruptors introduced above. Specifically, the interaction process between an agent and an MDP with disruption (Fig.~\ref{fig:framework-overview-robust-rl}) unfolds as follows: at each time step $t \in [T]$, the (possibly perturbed) environment outputs the current state $s_t$ and reward $r_t$. The {\em observation-disruptor} then perturbs these, sending the modified state $\widetilde{s}_t = D_{\mathsf{s}}(s_t)$ and reward $\widetilde{r}_t = D_{\mathsf{r}}(r_t)$ to the agent. Based on these, the agent selects an action $a_t \sim \pi_t( \cdot \mid \widetilde{s}_t)$, according to its policy $\pi=\{\pi_t\}_{1\leq t\leq T}$, where $\pi_t: \cS \rightarrow \Delta(\cA)$ defines the probability distribution over actions in $\cA$ given the observed state $\widetilde{s}_t$. The {\em action-disruptor} then perturbs this action to $\widetilde{a}_t = D_{\mathsf{a}}(a_t)$, which is then sent to a perturbed environment governed by the {\em environment-disruptor}, based on the reference --- nominal environment $(P^0, r^0)$. The environment then transitions to the next state $s_{t+1} \sim P_t(\cdot \mid s_t, \widetilde{a}_t)$ and provides the reward $r_{t+1}(s_t, \widetilde{a}_t)$, which becomes the input for the observation-disruptor in the next step $t+1$. \looseness =-1

\paragraph{Goal.}
For any \fname, the objective is to learn a policy (action selection rule) $\pi =\{\pi_t\}_{1\leq t\leq T}$ that maximizes long-term cumulative rewards, represented by the value function $\{V_t^\pi\}_{1\leq t\leq T}: \cS\mapsto \mathbb{R}$:
\begin{align}
    \max_{\pi} V_{t}^{\pi}(s)= \mathbb{E}\left[\sum_{k=t}^{T}  r_{k}\left(s_{k}, \widetilde{a}_{k}\right) \bigg|  \pi,(P,r), s_t=s\right].
\end{align}
Here, the expectation is taken over the trajectories generated by executing the policy $\pi$ under the perturbed transition kernels and reward functions $(P, r)$.





%   \begin{figure}[htbp!]
%  \centering
%  \subcaptionbox{}
%   {
% \includegraphics[width=0.42\linewidth]{figures/framework/attack-modes.pdf}
% }    
%  \subcaptionbox{}
%   {
% \includegraphics[width=0.47\linewidth]{figures/framework/attack-frequency.pdf}
% }    
%  	\caption{\normalsize Different levels of robust RL's attack modes (a); Different levels of robust RL's attack frequency (b).
%  	} 
%   \label{fig:framework-overview-robust-rl-attack-modes-frequency}
%  \end{figure} 


%\paragraph{The operation frequency of disruptors.}
In addition to disruption modes, the \fname allows disruptors to operate flexibly over time during the interaction process. Disruptors can act at different frequencies, such as step-wise, episode-wise, or at varying intervals.



 


% \subsection{Extended framework for safe RL and multi-agent RL}
% Robustness concerns are not limited to single-agent RL tasks; they are widespread and even more critical in other RL applications. Here, we extend our robust RL framework to two additional task sets: one involving safety constraints and the other involving multi-agent scenarios with strategic objectives.





% \paragraph{Safe RL: constrained MDP with disruption.}
% A safe RL problem is typically formulated as a finite-horizon constrained MDP (CMDP), represented as $\mathcal{CM}= \big( \cS, \cA, T, P^\no, r^\no, c, b \big)$. In addition to the components introduced in standard RL, there are extra components for safety constraints:  $c=\{c_t\}_{1\leq t\leq T}$, where $c_t = (c^1_t,\cdots c^n_t): \cS\times \cA \mapsto \mathbb{R}^n$ represents the immediate cost function assigned to state-action pairs at each time step $t$ (higher values indicate higher risks); and $b = (b^1, \cdots, b^n): \mathbb{R}^n$ represents the safety constraint thresholds. The goal of the agent is to learn a policy that maximizes the value function associated with rewards while satisfying the safety constraints:
% \begin{align}
%     \forall 1\leq i\leq n: \quad V_{\mathsf{c},1}^{\pi}(s)=\mathbb{E}\left[\sum_{k=1}^{T}  c_{k}^i\left(s_{k}, \widetilde{a}_{k}\right) \bigg| \pi, (P^\no,r^\no), s_1=s\right] \leq b^i.
% \end{align}

% To integrate the disruption module, all disruptors applicable to standard RL (Sec.~\ref{sec:disruptors}) are also effective here. Additionally, the environment-disruptors can perturb the cost function $c$ and safety constraint thresholds $b$, creating varied risky scenarios.

% \paragraph{Multi-agent RL (MARL): Markov games with disruption.} A MARL problem, involving $n$ strategic agents in a shared environment, is formulated as a Markov game  $\mathcal{MG}=\big\{ \cS,  \{\cA^i\}_{1 \le i \le n}, T, \bP^{\no}, \{r^i\}_{1\leq i\leq n} \big\}.$ Here, $\cA^i$ represents the action space of the $i$-th agent for all $1 \leq i \leq n$. The joint action profile of all agents is denoted by $\bm{a} \in \prod_{1\leq i\leq n} \cA^i$. The transition kernel $\bP^\no = {\bP^{\no}_t}_{1\leq t\leq T}$, where $\bP^{\no}t: \cS \times \prod{i=1}^n \cA^i \mapsto \Delta(\cS)$, defines the probability of transitioning from state $s \in \cS$ to next state $s' \in \cS$ given the joint action $\bm{a}$ at time step $t$. \looseness = -1

% MARL can also incorporate our disruption module, where the disruptors from single-agent RL (Sec.~\ref{sec:disruptors}) are applied to each agent in the system. Each agent has its own observation and action disruptors, while a shared environment disruptor affects the common environment. In MARL, challenges related to robustness and safety are even more complex, such as when all agents are disrupted by system noise or when only a subset of agents experiences jamming or malfunctions.




 
  
  
  
  

  
  