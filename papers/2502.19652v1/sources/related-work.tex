\newpage
\section{Related Works}


\paragraph{Related RL benchmarks.}
% they lack a general benchmark
To the best of our knowledge, \citet{zouitine2024rrls} is the only existing benchmark designed specifically for robustness evaluations, with the same goal of this work. It introduced six continuous control tasks in Gymnasium-MuJoCo, designed to address environmental shifts. A clear lack of standardized benchmarks is present that offer a wide range of diverse tasks and
account for uncertainty and disruptions over multiple stages throughout the interaction process, (not only the environment). Such a comprehensive evaluation platform is essential for the community to evaluate existing efforts and inspire new algorithms. Robust-Gymnaisum fills the gaps for robust evaluation of RL as a unified modular benchmark that supports over sixty diverse tasks in robotics and control for comprehensive evaluation, and accounting for different types of uncertainty and disruptions across multiple stages of the interaction process.

Moreover, enhancing robustness against environment shifts can be seen as a slight generalization to unseen tasks or environments. A non-exhaustive list of relevant benchmarks includes: a domain generalization benchmark in offline RL \citep{mediratta2023generalization}, Meta-World for meta-RL \citep{yu2020meta}, a generalization benchmark for robot manipulation \citep{pumacay2024colosseum}, SustainGym --- generalization for sustainable energy systems \citep{yeh2024sustaingym}, continual robot learning \citep{wolczyk2021continual}, lifelong robot learning \citep{liu2024libero}, skill manipulation robot learning \citep{mu2021maniskill}, safe RL \citep{ray2019benchmarking, yuan2022safe, gu2023safe}, multi-task/objective RL \citep{mendez2022composuite, gu2025safe}, human-robot collaboration tasks \citep{puig2024habitat}, dynamic algorithm configuration \citep{eimer2021dacbench}, RL in JAX \citep{bonnet2024jumanji}, procedurally
generated environments \citep{kuttler2020nethack}, DM control \citep{tunyasuvunakool2020dm_control}, arcade learning environments \citep{bellemare2013arcade}, and others \citep{marklund2020wilds,yao2022wild}.


\paragraph{RL works involving tasks for robust evaluation.}
Although not primarily focusing on building a benchmark for robust RL, there exists a lot of prior works or benchmarks that involves tasks for robust evaluation. While they typically support a few robust evaluation tasks associated with only one disruption type, which is not sufficient for comprehensive evaluations for robustness in real-world applications.


Specifically, there exists a lot of benchmarks for different RL problems, such as standard RL, safe RL, multi-agent RL, offline RL, and etc. These benchmarks either don't have robust evaluation tasks, or only have a narrow range of tasks for robust evaluation (since robust evaluation is not their primary goals), such as \citet{duan2016benchmarking} support 5 tasks with robust evaluations in control. Besides, there are many existing robust RL works that involve tasks for robust evaluations, while they often evaluate one-off and a narrow range of tasks in specific domains,  such as 8 tasks for robotics and control \citep{ding2023seeing}, 9 robot and control tasks in StateAdvRL \citep{zhang2020robust}, 5 robust RL tasks in RARL \citep{pinto2017robust}, a 3D bin-packing task \citep{pan2023adjustable}. Since their primary goal is to design robust RL algorithms, but not a platform to evaluate the algorithms.


% Despite recent efforts, current robust RL works often evaluate proposed algorithms in overly ideal scenarios, typically involving only one type of disruption at a time. These evaluations are often one-off and limited to a narrow range of tasks in specific domains, such as classical control, robot path planning \citep{bagnell2001solving}, autonomous driving \citep{wymann2000torcs}, investment planning \citep{mannor2016robust}, and 3D bin-packing \citep{pan2023adjustable}.



\paragraph{Robustness in  single-agent RL.}
Robustness is a key principle in designing RL algorithms, as training processes are often idealized and limited in data and scenarios, while real-world environments are changeable, unpredictable, and highly diverse. An emerging body of work focuses on developing robust RL algorithms that can withstand potential uncertainties, perturbations, and attacks during real-world execution. These efforts can largely be categorized under our unified robust RL framework (Sec.~\ref{sec:framework}), which formulates uncertainty events affecting the agent-environment interaction as behaviors of three types of disruptors. Our proposed \name encompasses all types of robust RL tasks within this framework, providing a flexible and comprehensive platform for evaluating and developing robust RL algorithms.

% make all the prior works into our framework
Specifically, prior works typically involve one type of disruptors: \citet{zhang2020robust,zhang2021robust,han2022solution,qiaoben2021strategically,sun2021exploring,xiong2022defending} studied the uncertainty of agent's observed state, controlled by the observation-disruptor who can add restricted noise or perform adversarial attack; \citet{tessler2019action,tan2020robustifying} considered the robustness w.r.t. the uncertainty of the action, where the action is possibly distorted by the action-disruptor abruptly or smoothly before forwarding to the environment to be executed; A large amount of prior works focus on dealing with the perturbation/shift on the environmental controlled by the environment-disruptor --- includes the reward function, the dynamics, or the task itself, ranging from theory \citep{iyengar2005robust,xu2012distributionally,wolff2012robust,kaufman2013robust,ho2018fast,smirnova2019distributionally,ho2021partial,goyal2022robust,derman2020distributional,tamar2014scaling,badrinath2021robust} to applications \citep{pinto2017robust,pattanaik2017robust,tanabe2022max,ding2023seeing}. Besides them, only a few works consider more complex scenarios that more than one disruptors are involved \citep{mandlekar2017adversarially}. See \citet{moos2022robust} for a recent review.








% Robust RL has attracted a lot of attentions due to the improving RL techniques gradually enable the prototypes to real applications, which makes the uncertainty or sim-to-real gaps an urgent challenge. 



\paragraph{Robustness in safe RL and multi-agent RL.}
Besides the class of standard single-agent RL, robustness in RL algorithms are ubiquitously demanded and has emerges a growing line of works for other RL problems such as partially observable Markov decision processes (POMDPs) \citep{cubuktepe2021robust}, safe RL \citep{liu2022robustness,sun2024constrained,zhang2024distributionally, gu2024roscom, gu2024review} and multi-agent RL \citep{vial2022robust,han2022solution,he2023robust,zhou2023robustness,zhang2023safe,zhang2021robust}. Additional challenges arise when combining robustness requirements with issues such as safety constraints and strategic interactions, which are often understudied and lack standardized benchmarks for evaluation. Our \name not only provides single-agent RL tasks but also encompasses a broader range of RL paradigms, including safe RL and multi-agent RL. This enables a faster and more comprehensive process for designing and evaluating robust RL algorithms across a wider array of RL tasks.



% \paragraph{LLM for robustness in RL.}