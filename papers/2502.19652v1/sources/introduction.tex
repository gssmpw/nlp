%!TEX root = ./../../main.tex
\section{Introduction}


% \url{https://anonymous.4open.science/r/Robust-Gymnasium-E532}

% RL is important in sequential making tasks robustness is important in RL
% current status of it
% So we propose a general framework that include all possible perturbations
% Curren\input{sources/appendix-detail-task}t works: usually target one of these types; still a lot of tasks can't be solved; and no benchmark for testing for different kinds of tasks.

% our vision


% motivate RL 
Reinforcement learning (RL) is a popular learning framework for sequential decision-making based on trial-and-error interactions with an unknown environment, achieving success in a variety of applications, such as games \citep{mnih2015human,vinyals2019grandmaster}, energy systems \citep{chen2022reinforcement}, finance and trading \citep{park2015adaptive,davenport2016overview}, and large language model alignment \citep{koubaa2023gpt,ziegler2019fine}.
% robotics and control \citep{kober2013reinforcement,mnih2013playing,sun2016guaranteed}


% robust RL is important
Despite recent advances in standard RL, its practical application remains limited due to concerns over robustness and safety. Specifically, policies learned in idealized training environments often fail catastrophically in real-world scenarios due to various factors such as the sim-to-real gap \citep{pinto2017robust}, uncertainty \citep{bertsimas2019adaptive}, noise, and even malicious attacks \citep{zhang2020robust,klopp2017robust,mahmood2018benchmarking}. Robustness is key to deploying RL in real-world applications, especially in high-stakes or high-cost fields such as autonomous driving \citep{ding2023survey}, clinical trials \citep{liu2015robustness}, robotics \citep{li2021reinforcement}, and semiconductor manufacturing \citep{kozak2023stability}. Towards this, Robust RL seeks to ensure resilience in the face of the complexity and variability of both the physical world \citep{bertsimas2019adaptive} and human behavior \citep{tversky1974judgment,arthur1991designing}.
% especially in sequential decision process where the influence of disruptions could accumulate.



% advanced generalization
% \item {\em advanced generalization:} The generalization capability is at the forefront of the next era of artificial intelligence (AI), especially with the development of large language models which has demonstrated the potential in various areas, including robotics, code generation, and personal AI assistants. However, creating a generalizable solution is challenging since only limited situations and tasks can be covered during the training process, where the data is scarce compared to the tremendous variability, complexity, and unpredictability in real-world applications that we aim to address.


% Current status of robust RL: need more works. However, the solution for one type is not perfect; for more types are even worse; testing scenarios are limited

Robust RL policies currently fall short of the requirement for broad deployment. Disruptions or interventions can occur at various stages of the agent-environment interaction, affecting the agent's observed state \citep{zhang2020robust,zhang2021robust,han2022solution,sun2021exploring,xiong2022defending}, observed reward \citep{xu2006robustness}, action \citep{huang2017adversarial}, and the environment (transition kernel) \citep{iyengar2005robust,pinto2017robust} and existing robust RL policies are vulnerable to such real-world failures \citep{mandlekar2017adversarially}.  This vulnerability is, in part, a result of the fact that policies are designed to address only one specific type of disruption (e.g., over the observed state), among other technical limitations \citep{ding2024seeing}. More critically, robust RL policies are often evaluated in distinct, one-off environments that can be narrow or over-fitted to the proposed algorithms. The absence of standardized benchmarks is a key bottleneck to progress in robust RL. Ideally, a benchmark should offer a wide range of diverse tasks for comprehensive evaluation and account for uncertainty and disruptions over multiple stages throughout the interaction process.

While numerous RL benchmarks exist, including a recent one focused on robustness to environment shifts \citep{zouitine2024rrls}, none are specifically designed for comprehensively evaluating robust RL algorithms. To address this gap, we present \name \footnote{Website with the introduction, code, and examples: \url{https://robust-gym.github.io/}}, a unified, highly modular benchmark for robust RL.  This open-source tool enables flexible construction of diverse tasks, facilitating the evaluation and development robust RL algorithms. Our main contributions are:

% Many successful examples show that developing a comprehensive benchmark can evidently boost the progress of a scientific area, such as in computer vision and single-agent RL \citep{deng2009imagenet,brockman2016openai}. Therefore, to promote robust RL algorithms that can in realistic applications with all potential disruptors, we aim to build a comprehensive robust RL benchmark to encourage more exploration and accelerate more rigorous experimental methodology.  This work builds a task library on the robust RL framework named \name \footnote{GitHub with introduction, code, and tutorials: \url{https://robust-rl.github.io/}} to promote robust and applicable solutions for the sequential decision making society. \name allows the researchers to flexibly choose the combinations of diverse kinds of tasks with various types of perturbations/attacks, leading to abundant settings for both training and testing. The main contributions are summarized as below:


 \begin{figure}[t]
 \centering
 % \subcaptionbox{}
  {
\includegraphics[width=0.9\linewidth]{figures/framework/tasks-illustration2.png}
}    
 	\caption{\normalsize The overview of \name. For more details, please visit the \href{https://robust-rl.github.io/}{website}.
 	} 
  \label{fig:tasks-overview-robust-rl}
 \end{figure} 

\begin{itemize}
    \item We introduce a unified framework for robust RL, encompassing diverse disruption types within a modular agent-environment interaction process (detailed in  Sec.~\ref{sec:framework}). This framework enables the development of \name, a benchmark that comprises over sixty diverse tasks in robotics and control, safe RL, and multi-agent RL; and
    % featuring different robot models (e.g., arms, dexterous hands, humanoid), environments (e.g., kitchen, outdoors), and task objectives (e.g., navigation, manipulation); 
    includes a wide range of disruptions targeting different stages/sources (agent observations, actions, and the environment) with varying modes (e.g., random or adversarial disturbances, environmental shifts) and frequencies. This is a unified benchmark specifically designed for robust RL, providing a foundational tool for evaluating and developing robust algorithms. \looseness = -1
     % We provide detailed tutorials and an easy-to-use API for users to flexibly choose or construct new robust RL tasks during both the training and testing processes.

    \item We conduct a comprehensive evaluation of several state-of-the-art (SOTA) baselines from standard RL, robust RL, safe RL, and multi-agent RL using representative tasks in \name. Our findings reveal that current algorithms often fall short of expectations in challenging tasks, even under single-stage disruptions, highlighting the need for new robust RL approaches. Furthermore, our experiments demonstrate the flexibility of \name by encompassing tasks with disruptions across all stages and four disturbance modes, including an adversarial model using a large language model (LLM). This illustrates the potential of LLMs in robust RL research.
    
\end{itemize}



 

% \begin{table}[t]
% 	\begin{center}
% \resizebox{\textwidth}{!}{
% \begin{tabular}{c|c|c|c|c|c|c|c|c|c}
% \hline
% \toprule
% 	\multicolumn{3}{c|}{\multirow{3}{*}{Class of Tasks}}  &  \multicolumn{2}{c|}{Observed state} &   \multicolumn{2}{c|}{Observed reward} &  \multicolumn{2}{c|}{Action}  & Environment \tabularnewline
% 	\cline{4-10}
% 	\multicolumn{3}{c|}{} & \multirow{2}{*}{Random} &  Adversarial  & \multirow{2}{*}{Random} &  Adversarial & \multirow{2}{*}{Random} &  Adversarial &  Dynamics \tabularnewline
% 	\multicolumn{3}{c|}{} & & (LLM) & & (LLM) & & (LLM) & Disturbance\tabularnewline
% \hline
% \toprule
%   \multirow{6}{*}{Single-agent}&   \multirow{2}{*}{ Control } & Box2D &  \multicolumn{3}{c}{\multirow{2}{*}{$\frac{S^2A (1+\sigma)}{  (1-\gamma)^4 \varepsilon^2} $
% } } \tabularnewline
% &  &   & \multicolumn{3}{c}{} \tabularnewline 
% \cline{2-5} & \multirow{2}{*}{ \citet{yang2021towards}   \vphantom{$\frac{1^{7}}{1^{7^{7}}}$} }  & \multicolumn{3}{c}{ \multirow{2}{*}{$\frac{S^2A (1+\sigma)^2}{  (\sqrt{1+\sigma} - 1)^2  (1-\gamma)^4 \varepsilon^2}$ }  } \tabularnewline
% &  &  \multicolumn{3}{c}{} \tabularnewline 
% \cline{2-5}
% &  \multirow{2}{*}{ {\bf This paper}  \vphantom{$\frac{1^{7}}{1^{7^{7}}}$} } &    \multicolumn{3}{c}{ \multirow{2}{*}{ $\frac{SA (1 + \ror ) }{  (1-\gamma)^4 \varepsilon^2} $}  }  \tabularnewline
% &  &  \multicolumn{3}{c}{} \tabularnewline 
% \hline
% \toprule
%  \multirow{2}{*}{Safety} &  \multirow{2}{*}{\citet{yang2021towards} } \vphantom{$\frac{1^{7}}{1^{7^{7}}}$} & \multirow{2}{*}{$\frac{SA}{(1-\gamma)^3 \varepsilon^2}$}  & \multicolumn{2}{c}{\multirow{2}{*}{$ \frac{SA}{(1-\gamma)^2\sigma \varepsilon^2}$} }   \tabularnewline
% &  &  & \multicolumn{2}{c}{}  \tabularnewline  
% \hline 
% \toprule
%  \multirow{2}{*}{Mutli-agent} &  \multirow{2}{*}{\citet{yang2021towards} } \vphantom{$\frac{1^{7}}{1^{7^{7}}}$} & \multirow{2}{*}{$\frac{SA}{(1-\gamma)^3 \varepsilon^2}$}  & \multicolumn{2}{c}{\multirow{2}{*}{$ \frac{SA}{(1-\gamma)^2\sigma \varepsilon^2}$} }   \tabularnewline
% &  &  & \multicolumn{2}{c}{}  \tabularnewline 
% \hline 
% \toprule
% \end{tabular}
% }

% 	\end{center}
% 	\caption{An overview of all the tasks available in \name.} 
% \label{tab:chi2}

% \end{table}


