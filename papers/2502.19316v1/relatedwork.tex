\section{Related Work}
\label{section:related_work}
% Due to the domain shift which leads to performance degradation, domain adaptation has been extensively researched. In the context of conventional domain adaptation, there is a source domain $\D_s = \{X_s, Y_s\}$ and a target domain $\D_t = \{X_t\}$, and they follow different distributions, \textit{i.e.}, $P(X_s) \neq P(X_t)$. This can be easily caused by different generation processes, such as using different devices under different environments. Generally, the source task is the same as the target task, thus, $Y_s$ and $Y_t$ are in the same label space $Y$. Semi-supervised domain adaptation~\cite{SemiSupervisedDA} assumes that there are a few labeled data in the target domain. On the other hand, unsupervised domain adaptation~\cite{AsymTriTraining} refers to the case in which the data in the target domain are completely unlabeled, which is more practical in real-world situation. Thus, we follow the same setting with unsupervised domain adaptation. In this section, we focus on recent unsupervised domain adaptation methods based on Convolutional Neural Networks (CNNs) due to its superior performance.
In this section, we focus on recent unsupervised domain adaptation methods based on Convolutional Neural Networks (CNNs) due to its superior performance.

Most domain adaptation methods mitigate the distribution discrepancy between domains according to~\cite{TheoryDA}. The expected error on the target domain is bounded by: 1) the expected error on the source domain; 2) the domain discrepancy between the source and target domains; and 3) a shared expected loss which is expected to be small~\cite{LearnSemanticRepresentation}. The expected error on the source domain can be minimized by using labeled data in the source domain. Thus the core task becomes to minimize the discrepancy between domains. %Many works follow this direction and achieve remarkable progresses. 
Deep Domain Confusion (DDC)~\cite{DDC} and Deep Adaptation Networks (DAN)~\cite{DAN} adopt maximum mean discrepancy~\cite{KernelMMD} on the final multiple layers to enforce the distribution similarity between source and target features. Joint Adaptation Networks (JAN)~\cite{JAN} uses the joint maximum mean discrepancy to align the joint distributions among multiple layers. Deep CORAL~\cite{DeepCoral} use feature covariance to measure the domain discrepancy. Philip \textit{et al.}~\cite{AssocDA} enforce the associations of similar features within two domains. In addition to these methods of measuring distribution discrepancy, maximizing the domain confusion via adversarial training can be used to align distributions. Domain Adversarial Neural Network (DANN)~\cite{DANN} introduces a domain classifier and renders the extracted features from two domains indistinguishable by a gradient reversal layer~\cite{RevGral}. These adversarial training based methods show effective adaptation performances~\cite{DSN,CondAdvDA}. Pinheiro \textit{et al.} include the adversarial loss and a similarity-based classifier~\cite{SimilarityUDA} to improve the model generalization. To integrate category information into the learning of domain-invariant features, Multi-Adversarial Domain Adaptation (MADA)~\cite{MADA} adopts multiple domain discriminators which correspond to each category. In~\cite{MCD}, instead of relying on a domain discriminator, Saito \textit{et al.} propose two task classifiers to align distributions by minimizing their discrepancy.~\cite{SWD} adopts sliced Wasserstein metric to measure the dissimilarity of classifiers.

Inspired by GAN~\cite{GAN}, recent works achieve feature distribution alignment based on a generative model. Sankaranarayanan \textit{et al.} propose a GenerateToAdapt model~\cite{GenToAdapt} which induces the extracted source or target embeddings to produce source-like images, such that the extracted features are expected to be domain-invariant. DuplexGAN~\cite{DuplexGAN} uses two discriminators for two domains to ensure that the extracted features can generate images on both domains based on a domain code. Image-to-image translation~\cite{Pix2Pix} provides a new direction for domain adaptation, which achieves the distribution alignment in the data space. In the absence of paired domain data, preserving the content will be non-trivial, and several recent works perform unsupervised image-to-image translation by including an extra constraint between input and the transformed output. SimGAN~\cite{SimGAN} employs a reconstruction loss between them, while PixelDA~\cite{PixelDA} and DTN~\cite{DTN} encourage the output to have the same class label and the semantic features as input, respectively. CoGAN~\cite{CoGAN} and UNIT~\cite{UNIT} learn a feature space based on shared or non-shared strategies to perform cross-domain generation. Zhu \textit{et al.} propose CycleGAN~\cite{CycleGAN} which involves bi-directional translations with a cycle-consistency loss, which enforces the condition that the translated image can be mapped back to input. DiscoGAN~\cite{DiscoGAN} and DualGAN~\cite{DualGAN} share the same idea and achieve promising unsupervised image translation performance. CyCADA~\cite{CYCADA} is based on CycleGAN and delivers good performance on multiple domain adaptation tasks.

Additionally, some works further explore using unlabeled target data to improve generalization by co-training~\cite{colearn}, pseudo-labeling~\cite{AsymTriTraining,CRST}, and entropy regularization~\cite{DIRT-T}. Some recent works focus on open set adaptation problems~\cite{UniversalDA}. However, these works require source data during adaptation. Thus, most previous works are not applicable to the proposed model adaptation problem. Some incremental learning works~\cite{LearningWoMemorize,LearningWoForget} are relevant to us, but they need labeled target data for new tasks. In this paper, we propose to simply use the unlabeled target dataset to adapt the pre-trained model to the target domain.