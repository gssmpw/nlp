\section{Related Work}
\label{section:related_work}
% Due to the domain shift which leads to performance degradation, domain adaptation has been extensively researched. In the context of conventional domain adaptation, there is a source domain $\D_s = \{X_s, Y_s\}$ and a target domain $\D_t = \{X_t\}$, and they follow different distributions, \textit{i.e.}, $P(X_s) \neq P(X_t)$. This can be easily caused by different generation processes, such as using different devices under different environments. Generally, the source task is the same as the target task, thus, $Y_s$ and $Y_t$ are in the same label space $Y$. Semi-supervised domain adaptation____ assumes that there are a few labeled data in the target domain. On the other hand, unsupervised domain adaptation____ refers to the case in which the data in the target domain are completely unlabeled, which is more practical in real-world situation. Thus, we follow the same setting with unsupervised domain adaptation. In this section, we focus on recent unsupervised domain adaptation methods based on Convolutional Neural Networks (CNNs) due to its superior performance.
In this section, we focus on recent unsupervised domain adaptation methods based on Convolutional Neural Networks (CNNs) due to its superior performance.

Most domain adaptation methods mitigate the distribution discrepancy between domains according to____. The expected error on the target domain is bounded by: 1) the expected error on the source domain; 2) the domain discrepancy between the source and target domains; and 3) a shared expected loss which is expected to be small____. The expected error on the source domain can be minimized by using labeled data in the source domain. Thus the core task becomes to minimize the discrepancy between domains. %Many works follow this direction and achieve remarkable progresses. 
Deep Domain Confusion (DDC)____ and Deep Adaptation Networks (DAN)____ adopt maximum mean discrepancy____ on the final multiple layers to enforce the distribution similarity between source and target features. Joint Adaptation Networks (JAN)____ uses the joint maximum mean discrepancy to align the joint distributions among multiple layers. Deep CORAL____ use feature covariance to measure the domain discrepancy. Philip \textit{et al.}____ enforce the associations of similar features within two domains. In addition to these methods of measuring distribution discrepancy, maximizing the domain confusion via adversarial training can be used to align distributions. Domain Adversarial Neural Network (DANN)____ introduces a domain classifier and renders the extracted features from two domains indistinguishable by a gradient reversal layer____. These adversarial training based methods show effective adaptation performances____. Pinheiro \textit{et al.} include the adversarial loss and a similarity-based classifier____ to improve the model generalization. To integrate category information into the learning of domain-invariant features, Multi-Adversarial Domain Adaptation (MADA)____ adopts multiple domain discriminators which correspond to each category. In____, instead of relying on a domain discriminator, Saito \textit{et al.} propose two task classifiers to align distributions by minimizing their discrepancy.____ adopts sliced Wasserstein metric to measure the dissimilarity of classifiers.

Inspired by GAN____, recent works achieve feature distribution alignment based on a generative model. Sankaranarayanan \textit{et al.} propose a GenerateToAdapt model____ which induces the extracted source or target embeddings to produce source-like images, such that the extracted features are expected to be domain-invariant. DuplexGAN____ uses two discriminators for two domains to ensure that the extracted features can generate images on both domains based on a domain code. Image-to-image translation____ provides a new direction for domain adaptation, which achieves the distribution alignment in the data space. In the absence of paired domain data, preserving the content will be non-trivial, and several recent works perform unsupervised image-to-image translation by including an extra constraint between input and the transformed output. SimGAN____ employs a reconstruction loss between them, while PixelDA____ and DTN____ encourage the output to have the same class label and the semantic features as input, respectively. CoGAN____ and UNIT____ learn a feature space based on shared or non-shared strategies to perform cross-domain generation. Zhu \textit{et al.} propose CycleGAN____ which involves bi-directional translations with a cycle-consistency loss, which enforces the condition that the translated image can be mapped back to input. DiscoGAN____ and DualGAN____ share the same idea and achieve promising unsupervised image translation performance. CyCADA____ is based on CycleGAN and delivers good performance on multiple domain adaptation tasks.

Additionally, some works further explore using unlabeled target data to improve generalization by co-training____, pseudo-labeling____, and entropy regularization____. Some recent works focus on open set adaptation problems____. However, these works require source data during adaptation. Thus, most previous works are not applicable to the proposed model adaptation problem. Some incremental learning works____ are relevant to us, but they need labeled target data for new tasks. In this paper, we propose to simply use the unlabeled target dataset to adapt the pre-trained model to the target domain.