\section{Method}
\label{sec:method}

\begin{figure*}[t!]
  \centering
  \includegraphics[width=1.0\linewidth]{figures/method-figure.png}
  \vspace{-7mm}
  \caption{\textbf{Overview.} \method consists of an API generation stage and a program synthesis stage. The Signature \& Implementation Agents generate an API that is used by the Program Agent to produce a program to answer the question, executed by the Execution Agent.}
  \label{fig:overview}
  \vspace{-3mm}
\end{figure*}

At the core of our approach is a dynamic API generated by LLMs that can be extended to address new queries that require novel skills. The goal of the API is to break down complex reasoning problems into simpler subproblems with general modules that can be used during program synthesis. Our approach consists of an API Generation stage and a Program Synthesis stage, illustrated in \cref{fig:overview}.

\myparit{Vision Specialists.}
During program execution on the image, we employ vision models for solving visual subtasks:
Molmo's~\cite{molmo} pointing model and GroundingDINO~\cite{groundingdino} are used to localize objects prompted with text ($\texttt{loc}$), SAM~\cite{sam} returns the bounding box from the object's mask prompted with Molmo's points ($\texttt{get\_2D\_object\_size}$), UniDepth~\cite{unidepth} estimates the depth at an image location ($\texttt{depth}$), GPT4o is utilized as a VQA module to query object attributes (color, material) from an image with the target object bounding box overlayed ($\texttt{vqa}$). We initialize the API with these functions. The API also includes $\texttt{same\_object}$ that computes the overlap of two object bounding boxes to determine if the objects are the same.

\subsection{API Generation}
\input{algos/algo1.tex}
\cref{algo:api} describes the API Generation. Here, the \textbf{Signature Agent} and the \textbf{Implementation Agent} collaborate to define and implement new functions \emph{as needed} to aid in solving the queries. 
First, the Signature Agent receives a batch of $N$ queries ($N=15$), \emph{without answers}, and is instructed to produce general method signatures for subproblems that could arise when answering those kinds of queries. The Implementation Agent then implements the signatures in Python.
Examples of signatures and their implementations are shown in \cref{fig:overview}.

\myparit{Prompting the Signature Agent.}
The agent receives the current API state as docstrings so it avoids duplicating existing methods. We observed that our Signature Agent performed better without in-context examples as it produced a more diverse API with wider potential functionality. 

\myparit{Prompting the Implementation Agent.}
The Implementation Agent receives all other signatures in the API along with the signature it needs to implement, so it can use other API methods in its implementation, enabling a hierarchy in the API. In contrast to the Signature Agent, providing in-context examples significantly enhances the Implementation Agent’s output, as implementation prioritizes accuracy over diversity. We refer to these examples as \emph{weak} in-context learning (ICL), as they guide correct method implementation in Python, unlike \emph{strong} ICL, which breaks down queries into full programs. Prompts for both agents and weak-ICL examples are found in the Appendix. 

\myparit{Depth-First Implementation}.
Once a method is implemented from its signature, the Test Agent, a Python interpreter, runs it using placeholder inputs. If a runtime error occurs, the Test Agent signals the Implementation Agent to revise it with the exception message. However, if the implementation relies on another yet-to-be-implemented API method, the test run cannot proceed. In this case, the Implementation Agent traverses an implicit dependency graph, depth-first, ensuring that prerequisite methods are implemented first (see Algo.~\ref{algo:api}).

Consider the following example where the signatures \texttt{get\_color}, \texttt{find\_objects\_by\_color}, \texttt{count\_objects\_left\_of}, and \texttt{is\_left\_of}, are defined by the Signature Agent, in that order. First, the Implementation Agent will implement \texttt{get\_color}, the Test Agent will be called, and barring  no runtime errors, the method will be complete. Then, the implementation for \texttt{find\_objects\_by\_color} uses \texttt{get\_color}, which is implemented, so the Test Agent only checks for Python errors. If \texttt{count\_objects\_left\_of} attempts to use \texttt{is\_left\_of}, the Test Agent will detect that \texttt{is\_left\_of} is not implemented and recursively call the Implementation Agent to implement \texttt{is\_left\_of}, followed by \texttt{count\_objects\_left\_of}.

In the event a cycle in the dependency graph is persistent after attempting the implementation of those methods 5 times, the methods in the cycle are deleted. Empirically, we rarely detect such cycles, which can be attributed to the Signature Agent producing multiple signatures at once, tending to avoid proposing signatures that overlap in function.

\subsection{Program Synthesis}
\input{algos/algo2.tex} 
The \textbf{Program Agent} receives the generated API and a single question as input. Its task is to generate Python code that leverages the API to solve the question. The Execution Agent, another Python interpreter, executes the program line-by-line. In the event of a Python error, it provides the Program Agent with the exception, and a new program is generated. This is repeated at most 5 times, after which the program returns an execution error.

\myparit{Prompting the Program Agent.} 
Following the success of Chain-of-Thought (CoT) prompting~\cite{cot}, we instruct the Program Agent to create a plan before generating the corresponding program. In-context examples boost the Program Agent’s performance. However, unlike \visprog~\cite{visprog} and \viper~\cite{vipergpt} that use strong-ICL, we use API-agnostic natural language instructions since the API is not predefined, making it impossible to provide full program examples. These instructions help for the same reason as with the Implementation Agent, to focus on correctness. The prompt for the Program Agent is provided in the Appendix.

\myparit{Test \& Execution Agent vs Critics.}
In modern library learning, LLM agents, or critics, evaluate the quality and utility of learned functions. Our Test and Execution Agents also assess method quality, but we opt for deterministic critics that leverage the full Python runtime, signaling LLM Agents with Python exceptions in case of errors.
