\clearpage
\setcounter{page}{1}
\maketitlesupplementary

\appendix
% %% ----- TABLE ----- %%
\input{tables/table_all_results_sup}
\input{tables/table_runtime}

The Appendix includes the prompts used for all agents, additional qualitative examples of \method on \clevr, \ourbench, and GQA, and a supplemental qualitative analysis with standard deviations to compare the robustness of approaches.

\section{Prompts}
\mypar{Predefined Module Signatures.} \cref{fig:clevr_predefined} and \cref{fig:omni3d_predef} show the docstrings of the predefined modules 
for \clevr and \ourbench respectively, which are used to initialize the dynamic API. We note that the two prompts are virtually identical, with the exception of the \texttt{get\_2D\_object\_size} method, which we omit from our experiments on \clevr as the dataset defines size as either \texttt{small} or \texttt{large}. In~\cref{fig:predefined_implementation}, we provide the Python implementation for all of the predefined modules.

\mypar{Signature Agent Prompt.} \cref{fig:signature_agent_prompt} contains the prompt used for the Signature Agent for both \clevr and \ourbench. We prompt the LLM to only generate signatures for methods when necessary, as we found this avoids redundant methods with minor changes to previously defined methods. We impose that the name of new methods start with an underscore, to prevent the common failure case of methods sharing names with variables previously defined. 

\mypar{Implementation Agent Prompt.} \cref{fig:implementation_prompt_clevr} and \cref{fig:implementation_prompt_omni3d} contain the prompts used for the Implementation agent on \clevr and \ourbench respectively. The prompts contain \emph{Weak ICL} examples, illustrating how to implement a model signature and use the pre-defined modules correctly for simpler queries. This is in contrast to \emph{Strong ICL} examples in \visprog and \viper, which provide complete program examples for full queries using a predefined API. In our framework, where agents dynamically generate the API, \emph{Strong ICL} is not feasible. 

Additionally, the prompts feature \emph{Pseudo ICL} in the form of natural language instructions and tips. Similarly to the predefined modules, the prompts differ between \clevr and \ourbench as the latter considers metric sizes and not a binary \texttt{small} or \texttt{large} as in \clevr. Consequently, we found it necessary to include natural language definitions and instructions for reasoning about 2D and 3D dimensions in the Implementation prompt on \ourbench.

\mypar{Program Agent Prompt.} In \cref{fig:program_prompt_clevr} and \cref{fig:program_prompt_omni3d} we show the prompts for the Program Agent on \clevr and \ourbench respectively. In the prompt for \clevr, we include a list of all available attributes. In both prompts, we include \emph{Pseudo ICL} in the form of natural language examples and instructions. For the \ourbench prompt, we additionally include tips and definitions for handling 2D and 3D dimensions.

\section{Additional Quantitative Analysis}
\mypar{Experimental Variability.} \cref{tab:main_results} in the main paper reports the mean performance of all methods across $3$ runs. \cref{tab:stdevs} reports the standard deviation on \clevr and \ourbench across the same $3$ runs. \method's variation is comparable to the VLMs on \ourbench, but slightly higher than program synthesis methods on both benchmarks. However, \method significantly outperforms ViperGPT and VisProg, even when accounting for this variation.

\mypar{Runtime.} \cref{tab:runtime} reports runtime in seconds for our Agents on an A100 GPU. Notably, when running our method on $1000$+ questions, the Signature and Implementation Agents \emph{only run once}, therefore their runtime becomes negligible to total inference runtime.

\section{More information on \ourbench}
On images sourced from Omni3D~\cite{omni3d} we collect a set of challenging questions with the help of human annotators. We omit using templates for questions, as done by others~\cite{cambrian1,thinkinginspace,spatial}, to avoid template overfitting, and instead instruct annotators to directly ask questions in free-form natural language, focusing on the scene, object layout and object sizes. 
We discard questions that are simplistic, \eg ``Is there a sofa in the image?" or ``Is the sofa behind the table?", and only keep queries which involve complex inference steps in 2D and 3D.
\ourbench queries roughly target the following areas of reasoning: relative size and dimensions with hypotheticals, spatial relationships and depth reasoning, relative proportions and alignments, and interaction with other objects. Queries from \ourbench can be browsed in \href{https://glab-caltech.github.io/vadar/omni3d-bench.html}{https://glab-caltech.github.io/vadar/omni3d-bench.html}.

We compute answers for questions using the 3D annotations provided in Omni3D~\cite{omni3d}.
Since the questions are not templated and thus don't follow rule-based instructions, we collect answers manually by sourcing the 3D annotations provided by the dataset for each image.
This results in 500 \emph{unique} and challenging image-question-answer tuples that test diverse aspects of 3D spatial reasoning. 
The diversity and complexity of \ourbench is showcased by the examples in~\cref{fig:fig1}, \cref{fig:main_results} and \cref{fig:omni_examples}.

\ourbench complements \clevr when assessing 3D spatial understanding. While \clevr uses templated questions, enabling the creation of a large volume of image-question-answer pairs, \ourbench focuses on diverse and complex reasoning tasks in free-form language. 
Together, \clevr and \ourbench provide a comprehensive test for models' 3D spatial reasoning capabilities. This is evidenced by the relatively low performance of modern state-of-the-art AI models on these benchmarks, achieving only 20-40\% accuracy.

\section{Comparison to VSI-Bench}
\label{sec:vsi-bench}
\input{tables/table_vsi_bench}

Concurrent to our work is VSI-Bench~\cite{thinkinginspace}, a video understanding benchmark that focuses on spatial reasoning. 
VSI-Bench targets 3D reasoning, but it differs from \ourbench in three critical ways:
First, it focuses on video understanding and retrieving the appropriate frame to answer a given query.
Second, while queries in VSI-Bench target 3D object attributes, they query absolute measurements, such as \emph{``What is the height of the chair?"}.
Monolithic VLMs when prompted with such questions resort to object priors. For example, \gpt4o says: \emph{``A chair tends to be 30-40 inches tall"}. 
In contrast, \ourbench introduces hypotheticals that require reasoning over scene attributes, evaluating true 3D spatial reasoning, \eg, \emph{``If the table is 2 meters wide, how tall is the chair?"}.
Third, VSI-Bench queries are templated, which can lead to biased conclusions due to template overfitting.

We compare \method on VSI-Bench. To decouple frame retrieval from image-based reasoning, we create a variant of the benchmark by sourcing a subset of $75$ queries with the associated frame that contains the information necessary to address the query.
We call this subset VSI-Bench-img. 
\cref{tab:vsi-bench-results} reports \method's performance and compares to Gemini1.5-Pro, which authors report to be the best VLM on the set.
From \cref{tab:vsi-bench-results} we observe that \method performs on par with the industry-leading Gemini1.5-pro. 
Importantly, \method's performance on VSI-Bench-img is 10\% higher than on \ourbench (40.4 vs 50.1) which highlights the more challenging nature of our benchmark.

\section{Qualitative Examples on \clevr}

\cref{fig:clevr_examples} shows additional qualitative examples on \clevr. The correct example showcases the use of API methods for repeated tasks and accurately determining spatial relations. The incorrect example highlights a failure to use same object to exclude the original reference object when the questions asks for ``another'' object.

\section{Qualitative Examples on \ourbench}

\cref{fig:omni_examples} shows additional qualitative examples on \ourbench. Our method is able to correctly estimate 3D distances by scaling depth based on the reference scale given in the question. An instance where such scaling is done incorrectly is shown in the last example. 

\section{Qualitative Examples on GQA}

\cref{fig:gqa_examples} shows qualitative examples on GQA~\cite{gqa}. Our method is able to identify and locate key objects necessary to answer questions. It is extremely explicit, locating the nearest person in the top right example using pixel distance from the tree. Some GQA questions have ambiguous answers, where the shape of the pot is generically ``round" and the frame of reference for spatial relations is not entirely clear (\ie, which man in the last example?).  


%% ------------------%%
\begin{figure*}[!ht]
    \centering
    \includegraphics[width=0.49\linewidth]{figures/appendix_examples/Slide22.png}
    \includegraphics[width=0.49\linewidth]{figures/appendix_examples/Slide23.png}
    \vspace{-2mm}
    \caption{\method program outputs on \clevr.}
    \label{fig:clevr_examples}
    \vspace{-2mm}
\end{figure*}

% HERE: 
\begin{figure*}[!ht]
    \centering
    \includegraphics[width=0.32\linewidth]{figures/appendix_examples/Slide24.png}
    \includegraphics[width=0.32\linewidth]{figures/appendix_examples/Slide25.png}
    \includegraphics[width=0.32\linewidth]{figures/appendix_examples/Slide26.png}
    \vspace{-2mm}
    \caption{\method program outputs on \ourbench.}
    \label{fig:omni_examples}
    \vspace{-2mm}
\end{figure*}
% HERE: GQA figure

\begin{figure*}[!ht]
    \centering
    \includegraphics[width=0.49\linewidth]{figures/appendix_examples/Slide27.png}
    \includegraphics[width=0.49\linewidth]{figures/appendix_examples/Slide28.png}
    \includegraphics[width=0.49\linewidth]{figures/appendix_examples/Slide29.png}
    \includegraphics[width=0.49\linewidth]{figures/appendix_examples/Slide30.png}
    \vspace{-2mm}
    \caption{\method program outputs on GQA~\cite{gqa}.}
    \label{fig:gqa_examples}
    \vspace{-2mm}
\end{figure*}
%% ------------------%%


%% -------- PROMPTS -----------%%
\input{figures/prompt_figures/predef_docstrings_clevr}
\input{figures/prompt_figures/predef_docstrings_omni3d}
\input{figures/prompt_figures/predefined_implementation}
\input{figures/prompt_figures/signature_prompt}
\input{figures/prompt_figures/api_prompt_clevr}
\input{figures/prompt_figures/api_prompt_omni3d}
\input{figures/prompt_figures/program_prompt_clevr}
\input{figures/prompt_figures/program_prompt_omni3d}
%% -----------------------------%%