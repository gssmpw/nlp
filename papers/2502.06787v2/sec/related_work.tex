\section{Related Work}
\label{sec:related}

Our work draws from areas of language modeling, visual program synthesis and library learning. 

\mypar{VLMs for Spatial Reasoning.}
LLMs~\cite{gpt4,gemini,llama3,claude} are trained on large corpora of text, including domain specific languages (DSLs) such as Python. Their multi-modal variants incorporate images and are additionally trained on image-text pairs showing impressive results for visual captioning and vision question-answering (VQA)~\cite{vqa}. 
Despite their strong performance, their ability to reason beyond category-level semantic queries is limited.
Recent work~\cite{cambrian1,whatsup} shows that VLMs suffer on visual tasks such as grounding spatial relationships and inferring object-centric attributes. SpatialRGPT~\cite{spatialrgpt} and SpatialVLM~\cite{spatial} use data synthesis pipelines to generate templated queries for spatial understanding. We compare to SpatialVLM and show that it struggles to tackle 3D spatial reasoning queries.

\mypar{Visual Program Synthesis.}
Recent advances in visual reasoning have led to methods which improve upon the capabilities of vision-based models by composing them symbolically via program synthesis. 
\visprog~\cite{visprog} prompts an LLM to generate an executable program of a specified DSL that calls and combines vision specialists -- OwlViT~\cite{owlvit} for object detection, CLIP~\cite{clip} for classification, and ViLT~\cite{vilt} for VQA.
\viper~\cite{vipergpt} directly generates Python code by providing a Python API specification to the LLM agent and adds MiDaS~\cite{midas} as the vision specialist for depth estimation, in addition to GLIP~\cite{glip} and X-VLM~\cite{x-vlm} for vision-language tasks.
Both approaches rely on a pre-defined DSL, which narrows the scope of applicability and makes these methods difficult to extend to a wider range of queries.
Similar to \viper, we use Python as the interface for our LLM agents, but we don't define the API a-priori.
We instead rely on our agentic workflow to generate the API needed to tackle complex spatial reasoning queries.
We compare to \viper and \visprog and show that both struggle to generate accurate programs for complex queries, often completely ignoring part of the query. 

\mypar{Library Learning.}
An emerging field in LLM research focuses on the dynamic creation and extension of a set of reusable functions during problem-solving. 
Early work on library learning predates the use of LLMs \cite{ellis2023dreamcoder,valkov2018houdini,lake2015human},
and focuses on a common architecture of iteratively proposing new programs and synthesizing commonly used components into a library.
Modern approaches follow this same paradigm, but use LLMs to accelerate the synthesis of useful programs, applied to gaming \cite{wang2023voyager}, 3D graphics scripting \cite{hu2024scenecraft}, theorem proving \cite{thakur2024context}, and symbolic regression \cite{grayeli2024symbolic}. 

\mypar{Neuro-symbolic AI} generates interpretable symbolic components for complex tasks 
and has been explored for a wide range of fields, including spatial reasoning~\cite{mao2019neuro}, grounding of 3D point clouds~\cite{ns3d}, mechanistic modeling in scientific domains \cite{grayeli2024symbolic,sun2022neurosymbolic}, logical reasoning \cite{olausson2023linc}, amongst other areas. 
Closer to us is the logic-enhanced LLM, \leftm~\cite{whatsleft}, that uses a dynamic DSL of first order logic structures and differentiably executes them using domain-specific modules. These modules, instantiated as MLPs, ground spatial concepts, \eg \emph{``is left of"}, and are \emph{trained with supervision}.
On \clevr, \method, which is \emph{training-free}, achieves the same performance as \leftm when trained with $\geq 10,000$ training samples. 
A benefit of our training-free approach is that it scales to new domains where 3D supervision is hard to acquire, as we show on our \ourbench.

\mypar{Spatial Reasoning Benchmarks.}
Existing benchmarks test aspects of visual reasoning with free-form language~\cite{laurent2024lab, arc}.
We focus on natural-image based ones. 
VQA~\cite{vqa} introduced the task of visual question answering.
GQA~\cite{gqa} is a popular large-scale VQA benchmark with questions that pertain to object and attribute recognition, of mostly a single-step inference -- \emph{``What color is the cat next to the chair?"}, \emph{``What type of vehicle is on top of the road?"}, \emph{``Do the wildflowers look ugly?"}. 
RefCOCO~\cite{refcoco} targets object localization with referring expressions such as \emph{``the man in a red shirt"}. 
What's up~\cite{whatsup} quantifies comprehension of basic 2D spatial relations such as \emph{``left of"} and \emph{``above"}.
These benchmarks evaluate aspects of visual reasoning, but critically omit 3D understanding. 
Q-Spatial Bench~\cite{qspatial} focuses solely on absolute 3D measurements.
Cambrian-1~\cite{cambrian1} proposes a VQA benchmark repurposing images and annotations from Omni3D~\cite{omni3d}, but its queries focus on the relative depth and depth ordering of objects with (2 or 3)-choice questions.
Our benchmark also repurposes Omni3D annotations, but in contrast to Cambrian-1, we design more complex queries that extend beyond depth ordering and multiple choice. Concurrent to our work, VSI-Bench~\cite{thinkinginspace} introduces a video understanding benchmark focused on spatial relationships, which we discuss extensively in Appendix \ref{sec:vsi-bench}.
