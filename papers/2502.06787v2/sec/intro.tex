\section{Introduction}
\label{sec:intro}
Consider \cref{fig:fig1}. 
Here, a person or an agent wants to determine the radius of the mirror in the image, given that the table is 20 meters tall. 
Answering this question requires visual reasoning, a crucial step toward achieving general-purpose AI.
Visual reasoning enables machines to analyze and make sense of the visual world. 
Humans rely heavily on visual cues to navigate complex environments, interact with objects and make informed decisions.
Our goal is to build intelligent agents that can do the same.
Recent advances in AI have produced vision and language models (VLMs)~\cite{gpt4, claude, molmo, gemini} that can answer questions from images. 
Although impressive, these models excel primarily at category-level semantic understanding. 
% \eg, ``Is there a mirror in the image?". 
Their performance significantly declines when tasked with spatial understanding within the three-dimensional world~\cite{spatial, cambrian1, whatsup}. 

Returning to \cref{fig:fig1}, to answer the query, an AI agent must first locate the relevant objects,  determine their dimensions in pixel space, use their depth to calculate their 3D sizes, and finally compute the mirror's radius using the table's height.
This is a complex sequence of tasks, involving multiple steps of understanding, grounding, and inference. 
\gpt4o~\cite{gpt4}, a state-of-the-art VLM trained on extensive datasets, gives a wrong final answer.

To address the complexity of 3D spatial reasoning tasks, we propose a system of agents working collaboratively to create executable programs for a given image. 
Our approach leverages LLM agents that \emph{dynamically} define and expand a domain-specific language (DSL) \emph{as needed}, generating new functions, skills and reasoning, in two phases: the \textbf{API Generation} and the \textbf{Program Synthesis} stage.
Vision specialists -- an object detector, a depth estimator and object attribute predictor -- help the agents execute the program.
We name our approach \method, as it integrates Visual, Agentic, Dynamic AI for Reasoning.
\method belongs in the family of visual program synthesis methods, like \viper~\cite{vipergpt} and \visprog~\cite{visprog}, but addresses a key limitation in these approaches: their reliance on a static, human-defined DSL, which restricts them to a predefined range of functionality.
This limitation is evident in~\cref{fig:fig1}, where \viper generates an incomplete, inaccurate program and \visprog defaults to a holistic visual question answer (VQA) approach for answering the query.
\method's output in~\cref{fig:fig1} demonstrates its ability to tackle a wider range of visual queries.

We evaluate 3D spatial reasoning using challenging benchmarks designed for rigorous assessment of 3D understanding. Our evaluation includes \clevr~\cite{clevr} and our newly introduced benchmark, \ourbench, based on Omni3D~\cite{omni3d}; \cref{fig:fig1} shows an example. Both datasets emphasize visual queries involving relative depth, size, and object location, often conditioned on measurement hypotheses, requiring grounding and 3D inference. This contrasts with previous spatial reasoning benchmarks like GQA~\cite{gqa}, which primarily emphasize appearance-based reasoning.

% What do we achieve?
At a high level, \method roughly mirrors the workflow of a software engineer when defining, implementing, and testing new software solutions for a given problem. Leveraging its agentic design, \method autonomously defines and implements functions such as \texttt{\_find\_closest\_object\_3D}, \texttt{\_is\_behind}, \texttt{\_count\_objects\_by\_attributes\_and\_position}, \texttt{\_is\_left\_of}, and more. These functions are used by the Program Agent, resulting in more concise programs, less output tokens and thus a lower likelihood of errors from LLM-generated predictions.
We empirically show that \method outperforms a \emph{no-API} agent by 6\%, highlighting the value of general, reusable, functions within an API.
Moreover, we show that our generated API significantly surpasses a static, human-defined API used in~\cite{visprog, vipergpt}, by more than 20\% on CLEVR.
\method performs competitively with state-of-the-art VLMs, on \ourbench, while also providing executable programs. 

Considering the rapid progress in AI, one might wonder if methods like \method can dominate monolithic VLMs in 3D spatial reasoning. One clear advantage of \method is its ability to generate interpretable programs. However, our experiments highlight another key potential. Improving VLMs for 3D reasoning would require extensive datasets of image-question-answer tuples with 3D information, an onerous endeavor. In contrast, our experiments show that if the component vision models -- an object detector, an attribute predictor and depth estimator -- were replaced with oracle versions, \method would achieve 83.0\% accuracy, 24\% higher from the best VLM.
This indicates that \method is bottlenecked by the performance of its vision specialists. Thus, an alternative path to scaling 3D spatial reasoning could be through improving specialized vision models, which tackle a simpler problem than general-purpose VQA and for which training data is more readily available. 

