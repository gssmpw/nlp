\section{Experiments}
\label{sec:experiments}\

We evaluate our approach on challenging spatial reasoning benchmarks, demonstrating that a dynamically generated API outperforms the static, human-defined APIs in ViperGPT~\cite{vipergpt} and VisProg~\cite{visprog} by a large margin. Additionally, we compare against state-of-the-art monolithic VLMs trained on billions of (image, question, answer) samples, showing that our method competes favorably and even surpasses them on certain question types while offering interpretable reasoning steps for complex queries.


\subsection{A Benchmark for Spatial Reasoning in 3D}
We evaluate 3D spatial reasoning using \clevr, and our newly introduced benchmark, \ourbench.

\mypar{\clevr}~\cite{clevr} consists of (image, question, answer) tuples. Each image contains 2-10 objects of 3 different shapes, 8 colors, 2 materials, and 2 sizes. Despite the simplicity of the scenes, the questions in \clevr are complex, \eg, \emph{``There is a large ball right of the large metal sphere that is left of the large object that is behind the small brown sphere; what color is it?"}. Our \clevr benchmark contains 1,155 samples, 400 of which require a numerical answer, 399 are yes/no questions, and 356 are multiple-choice questions.

\mypar{\ourbench} is sourced from Omni3D~\cite{omni3d}, a dataset of images from diverse real-world scenes with 3D object annotations. We repurpose images from Omni3D to a VQA benchmark, with questions about 3D information portrayed in the image, such as \emph{``If the height of the front most chair is 6 meters in 3D, what is the height in 3D of the table in the image?"} and \emph{``How many bottles would you have to stack on top of each other to make a structure as tall in 3D as the armchair?"}. \ourbench complements \clevr with \emph{non-templated} queries pertaining to 3D locations and sizes of objects. 
Our queries test 3D reasoning, as they require grounding objects in 3D and combining predicted attributes to reason about distances and dimensions in three dimensions. \ourbench consists of 500 extremely challenging (image, question, answer) tuples. 

We compare our proposed benchmark to GQA~\cite{gqa}, a popular visual reasoning dataset. GQA derives queries from scene graphs which primarily pertain to the visual appearance and attributes of objects. Example queries in GQA are \emph{``Is there a red truck or bus?"}, \emph{``Is the field short and brown?"} and \emph{``Is the chair in the top part of the image?"}. These are significantly simpler to queries in \clevr and \ourbench which involve multiple steps of grounding and inference in two- and three- dimensions.

%% ----- TABLE ----- %%
\input{tables/table_allres}
\input{tables/table_oracle}
%% ------------------%%

\subsection{Results on Spatial Reasoning in 3D}

\cref{tab:main_results} compares our approach, \method, to state-of-the-art VLMs and Program Synthesis methods. \cref{fig:left_vs_ours} additionally compares to the neuro-symbolic \leftm~\cite{whatsleft}.
\method uses \gpt4o with a temperature of $0.7$ for all agents.

\mypar{VLMs vs \method.} VLMs, such as \gpt4o~\cite{gpt4}, \claude~\cite{claude}, \gemini~\cite{gemini}, \llama-11B~\cite{llama3}, and Molmo-7B~\cite{molmo}, are monolithic models trained on vast image-question-answer datasets, likely including samples with spatial and 3D information. We expect them to perform well on related tasks. We also compare to SpaceMantis~\cite{spatial, mantis}, the most recent and largest SpatialVLM~\cite{spatial} variant, finetuned on data with 3D information. We analyze performance based on three answer types: yes/no, multiple-choice, and numerical answers. For queries with floating point answers, we report MRA~\cite{thinkinginspace} with thresholds $\mathcal{C} = \{0.5, 0.55,...,0.95\}$ for outputs $\hat{y}$ and ground truth $y$: $\mathcal{MRA} = \frac{1}{|\mathcal{C}|} \sum_{\theta \in \mathcal{C}} \mathbbm{1} \bigg(\frac{|\hat{y} - y|}{y} < 1 - \theta \bigg)$

% \begin{equation}
%     \mathcal{MRA} = \frac{1}{|\mathcal{C}|} \sum_{\theta \in \mathcal{C}} \mathbbm{1} \bigg(\frac{|\hat{y} - y|}{y} < 1 - \theta \bigg)
% \end{equation}


%% ----- FIGURE ----- %%
\begin{figure}[t!]
    \vspace{-5mm}
    \centering
    \includegraphics[width=\linewidth]{figures/left_vs_ours.png}
    \vspace{-8mm}
    \caption{\textbf{\leftm~\cite{whatsleft} vs \method on \clevr.} \leftm requires supervision. We vary the amount of training data (x-axis) and report accuracy (y-axis). \method requires \emph{no} supervision but takes in 15 queries \emph{without answers} to guide the creation of the API. \method outperforms \leftm trained with $\leq 10,000$ supervised examples.}
    \label{fig:left_vs_ours}
    \vspace{-6mm}
\end{figure}
%% -------------------%%


%% ----- FIGURE ----- %%
\begin{figure*}[t!!]
    \centering
    \includegraphics[width=0.49\linewidth]{figures/figure2/img1.png}
    \includegraphics[width=0.49\linewidth]{figures/figure2/img4.png}
    \includegraphics[width=0.49\linewidth]{figures/figure2/img3.png}
    \includegraphics[width=0.49\linewidth]{figures/figure2/img2.png}
    \includegraphics[width=0.49\linewidth]{figures/figure2/img5.png}
    \includegraphics[width=0.49\linewidth]{figures/figure2/img6.png}
    \vspace{-3mm}
    \caption{\textbf{Program outputs for \visprog, \viper and \method.} For each example, we show the query, the input image, and the method's program generations. Queries are from our benchmark and pertain to 3D understanding of scenes. Zoom-in to read the programs.}
    \label{fig:main_results}
    \vspace{-5mm}
\end{figure*}
%% -------------------%%

From \cref{tab:main_results}, we observe that
on \clevr, \gpt4o, \claude, and \gemini perform best on average while \method slightly outperforms VLMs on numeric (by 1.0\%) and yes/no answers (by 2.3\%), while providing interpretable execution traces.
On \ourbench, \method is behind GPT4o by just $2\%$ and outperforms all other VLMs by more than $5\%$. \llama-11B and Molmo-7B perform worse among VLMs likely due to their smaller size. 

\mypar{\viper vs \visprog vs \method.} 
\method outperforms both methods on both \clevr and \ourbench by more than 20\%. \visprog and \method use \gpt4o as their LLM; \viper uses GPT-3.5 as it performed better.

Separating program correctness from execution accuracy, \cref{tab:oracle_results} provides comparisons to \viper and \visprog when vision specialists are replaced with oracle ones. 
On \clevr, we use an Oracle Execution Agent that leverages the true scene annotations to provide the correct output automatically.
For \ourbench, we use a smaller subset of 50 queries and manually verify program correctness as ground truth 3D information is not available for all objects in the scene.
The results reveal that with oracle vision specialists, \method achieves an accuracy of 83.0\% on \clevr and 94.4\% on \ourbench, compared to \viper's 42.6\% and 54.9\%, and \visprog's 39.9\% and 66.0\% respectively.
This suggests that \method supports a wider variety of queries, thanks to the dynamically generated API by our LLM agents, as opposed to the static, human-defined API in \viper and \visprog. Our API allows for flexible integration of vision specialists, avoiding human biases -- \eg, as in \visprog, where the pre-defined API guides the LLM to define ``behind'' by cropping the image above.

The high accuracy of \method with oracle vision specialists -- more than 20\% above \claude on \clevr and more than 40\% above \gpt4o on \ourbench -- suggests a promising path to scaling 3D spatial reasoning: improving specialized vision models. These models are easier to train than general-purpose VLMs, as they address simpler tasks with more accessible training data.

\cref{fig:main_results} shows programs generated by the methods. We observe that \viper and \visprog tend to resort to direct VQA calls when questions are complex, as opposed to generating programs. 
In addition, \viper often tends to produce incomplete programs, ignoring a significant portion of the query. 
Finally, both \viper and \visprog often confuse above-behind and below-in front. 
This seems to be a semantic error for \viper that uses a depth estimation module, like us, and a conceptual design error by \visprog that implements \texttt{CROP\_BEHIND} to crop above in the image.

\mypar{\leftm~\cite{whatsleft} vs \method.}
We also compare to the logic-enhanced neuro-symbolic approach \leftm~\cite{whatsleft}, which uses trained modules to ground visual concepts in images, such as ``\texttt{is left of}''. Unlike \leftm, our approach is entirely training-free, while \leftm requires extensive supervision for module training. \cref{fig:left_vs_ours} reports the performance of \leftm on the \clevr dataset when trained (to convergence) with varying training set sizes (x-axis). Although our approach does not require any explicit supervision, our API agent uses a small sample ($=15$) of \emph{questions only}, \emph{without answers}, to construct the API. 
According to \cref{fig:left_vs_ours}, we outperform \leftm trained with $\leq 10,000$ examples on \clevr.
Notably, it is not possible to evaluate \leftm on \ourbench due to its reliance on a large, domain-specific training set with appropriate 3D supervision, which is difficult to obtain for this benchmark or in general. 
This highlights an added advantage of our method: its ability to scale to new domains without the need for training. 


\mypar{Results on GQA.}
We report results on GQA~\cite{gqa}, a widely used benchmark for spatial reasoning. 
As noted earlier, GQA queries emphasize object appearance and attributes, and primarily require one-step inference.
Questions in GQA include \emph{“What size is the doughnut the person is eating?”} and \emph{“Who is sitting in front of the water?”}. 
\cref{table:qga} compares \gpt4o, \viper, \visprog, and \method.
We observe different relative model performance compared to \cref{tab:main_results}.
Given the nature of GQA, it is not surprising that a monolithic and performant VLM like \gpt4o would perform well, which our results confirm. Among the program synthesis methods, we observe that \method and \visprog achieve comparable performance, while \viper shows a drop in accuracy. 
A deeper dive into the output programs shows that \visprog relies on image-wide VQA calls in 34\% of cases, whereas \method does so only 24\% of the time.
The limitations of GQA queries in evaluating 3D spatial reasoning highlight the need for our proposed benchmark, which better assesses 3D understanding and exposes the weaknesses of current methods.
%% ----- TABLE ----- %%
\input{tables/table_gqa.tex}
%% ------------------%%

\subsection{Ablations}
We turn to ablations to quantify the effectiveness of the agentic design and prompting in our approach. To reduce costs from \gpt4o, we experiment on a randomly selected CLEVR subset. \cref{table:ablations_agents} compares the following variants:

%% ----- TABLE ----- %%
\input{tables/table_ablations_agents.tex}
%% ------------------%%

\myparit{No-API Agent} is a single agent instructed to directly create programs for queries without defining an API of reusable methods. Comparison to this variant shows the value of an API. 
\cref{fig:noapi_vs_ours} shows a common reasoning error by the \emph{No-API Agent}, which confuses depth with left/right; our approach, by implementing reusable methods, invokes the appropriately named method that is accurately implemented.
The example reiterates that spatial reasoning relies on correctness, supporting \method's design to build an accurate API \textit{before} program synthesis, over library learning, that discovers a potentially incorrect library \textit{after} program synthesis.

\myparit{API Agent} is our approach without any prompting instructions or ICL examples.
We incrementally add our two prompting techniques: 
(1) \emph{Weak ICL} examples guide the Implementation Agent to use the pre-defined modules. 
(2) \emph{Pseudo ICL} provides pseudo-code examples and instructions in \emph{natural language} to the Implementation and Program Agent, respectively, that demonstrate how to handle intricate queries. We provide the prompts in the Appendix. 

From \cref{table:ablations_agents} we observe that the No-API Agent performs the worst, while our prompting techniques via weak ICL examples and instructions achieve the best performance.

%% ----- FIGURE ----- %%
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/noapi_vs_ours.png}
    \begin{minipage}{0.4\linewidth}
    \centering
    (a) \emph{No-API} Agent
    \end{minipage}
    \begin{minipage}{0.58\linewidth}
    \centering
    (b) \method
    \end{minipage}
    \vspace{-3mm}
    \caption{(a) The \emph{No-API} agent produces longer programs and is prone to errors, often mistakenly using depth for left/right comparisons. (b) In contrast, our agentic \method creates shorter programs by leveraging methods from the API.}
    \label{fig:noapi_vs_ours}
    \vspace{-6mm}
\end{figure}
%% -------------------%%