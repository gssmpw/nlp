\clearpage

\begin{figure*}[t]
\begin{psmall}
def loc(self, image, object_prompt):
    pts = molmo(image, "point to the " + object_prompt)
    if len(pts) == 0:
        # No points found
        return []
    return pts

def vqa(image, question, x, y):
    mask = sam_2([x, y], "foreground")  # get sam2 mask at x,y
    bbox = bbox_from_mask(mask)  # bbox around sam2 mask
    boxed_image = overlay_box_on_image(image, bbox)  # original image with bbox overlaid
    result = gpt4o(boxed_image, question)
    return result

def depth(image, x, y):
    depth_pred = unidepth(image)["depth"]  # Predict depth map over image
    depth_x_y = depth_pred[y, x]
    return depth_x_y

def same_object(image, x_1, y_1, x_2, y_2):
    mask_1 = sam_2([x_1, y_1], "foreground")  # get sam2 mask for point 1
    mask_2 = sam_2([x_2, y_2], "foreground")  # get sam2 mask for point 2
    obj_1_bbox = bbox_from_mask(mask_1)  # bbox around sam2 mask
    obj_2_bbox = bbox_from_mask(mask_2)  # bbox around sam2 mask
    return iou(obj_1_bbox, obj_2_bbox) > 0.92

def get_2D_object_size(image, x, y):
    mask = sam_2([x, y], "foreground")  # get sam2 mask at x,y
    bbox = bbox_from_mask(mask)  # bbox around sam2 mask
    width = abs(box[0] - box[2])
    height = abs(box[1] - box[3])
    return width, height
\end{psmall}
\caption{\textbf{Python Implementation of Predefined Modules.} \method uses Molmo~\cite{molmo} for object detection, SAM2~\cite{sam} for segmentation, GPT4o~\cite{gpt4} for VQA, and UniDepth~\cite{unidepth} for depth estimation.} 
\label{fig:predefined_implementation}
\end{figure*}