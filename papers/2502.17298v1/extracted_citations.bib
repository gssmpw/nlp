@article{chen2024retraining,
  title={Retraining-Free Merging of Sparse Mixture-of-Experts via Hierarchical Clustering},
  author={Chen, I and Liu, Hsu-Shen and Sun, Wei-Fang and Chao, Chen-Hao and Hsu, Yen-Chang and Lee, Chun-Yi and others},
  journal={arXiv preprint arXiv:2410.08589},
  year={2024}
}

@inproceedings{isik2023gptzip,
  title={Gpt-zip: Deep compression of finetuned large language models},
  author={Isik, Berivan and Kumbong, Hermann and Ning, Wanyi and Yao, Xiaozhe and Koyejo, Sanmi and Zhang, Ce}

@article{li2023merge,
  title={Merge, then compress: Demystify efficient SMoe with hints from its routing policy},
  author={Li, Pingzhi and Zhang, Zhenyu and Yadav, Prateek and Sung, Yi-Lin and Cheng, Yu and Bansal, Mohit and Chen, Tianlong},
  journal={arXiv preprint arXiv:2310.01334},
  year={2023}
}

@article{liu2024bitdelta,
  title={BitDelta: Your Fine-Tune May Only Be Worth One Bit},
  author={Liu, James and Xiao, Guangxuan and Li, Kai and Lee, Jason D and Han, Song and Dao, Tri and Cai, Tianle},
  journal={arXiv preprint arXiv:2402.10193},
  year={2024}
}

@article{liu2024efficient,
  title={Efficient expert pruning for sparse mixture-of-experts language models: Enhancing performance and reducing inference costs},
  author={Liu, Enshu and Zhu, Junyi and Lin, Zinan and Ning, Xuefei and Blaschko, Matthew B and Yan, Shengen and Dai, Guohao and Yang, Huazhong and Wang, Yu},
  journal={arXiv preprint arXiv:2407.00945},
  year={2024}
}

@misc{lu2024experts,
      title={Not All Experts are Equal: Efficient Expert Pruning and Skipping for Mixture-of-Experts Large Language Models}, 
      author={Xudong Lu and Qi Liu and Yuhui Xu and Aojun Zhou and Siyuan Huang and Bo Zhang and Junchi Yan and Hongsheng Li},
      year={2024},
      eprint={2402.14800},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{ping2024deltacome,
      title={Delta-CoMe: Training-Free Delta-Compression with Mixed-Precision for Large Language Models}, 
      author={Bowen Ping and Shuo Wang and Hanqing Wang and Xu Han and Yuzhuang Xu and Yukun Yan and Yun Chen and Baobao Chang and Zhiyuan Liu and Maosong Sun},
      booktitle={Thirty-eighth Conference on Neural Information Processing Systems},
      year={2024},
}

@article{xie2024moe,
  title={MoE-Pruner: Pruning Mixture-of-Experts Large Language Model using the Hints from Its Router},
  author={Xie, Yanyue and Zhang, Zhi and Zhou, Ding and Xie, Cong and Song, Ziang and Liu, Xin and Wang, Yanzhi and Lin, Xue and Xu, An},
  journal={arXiv preprint arXiv:2410.12013},
  year={2024}
}

@article{yang2024moe,
  title={MoE-I2: Compressing Mixture of Experts Models through Inter-Expert Pruning and Intra-Expert Low-Rank Decomposition},
  author={Yang, Cheng and Sui, Yang and Xiao, Jinqi and Huang, Lingyi and Gong, Yu and Duan, Yuanlin and Jia, Wenqi and Yin, Miao and Cheng, Yu and Yuan, Bo},
  journal={arXiv preprint arXiv:2411.01016},
  year={2024}
}

@article{yao2023deltazip,
  title={DeltaZip: Multi-Tenant Language Model Serving via Delta Compression},
  author={Yao, Xiaozhe and Klimovic, Ana},
  journal={arXiv preprint arXiv:2312.05215},
  year={2023}
}

