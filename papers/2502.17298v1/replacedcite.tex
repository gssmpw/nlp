\section{Related Work}
\label{sec:related_works}

\textbf{Mixture of Experts Compression} methods (see Table~\ref{tab:Related}) reduce parameter redundancy and minimize storage in MoE models. For example,  MoE-Pruner____ achieves compression by pruning weights based on their activations and router importance. However, these unstructured methods typically provide only limited inference acceleration. For structured pruning, NAEE____ skips non-redundant experts and trims unimportant weight connections, while MoE-I$^2$____ combines inter-expert pruning with intra-expert low-rank decomposition. Yet, these methods involve serious loss of expert knowledge, requiring additional fine-tuning. Our approach differs from these methods by avoiding the direct removal of experts and no need for retraining. Expert merging methods like EEP____ introduce a two-stage pipeline where experts are first pruned and then merged into consolidated representations. Similarly, MC-SMoE____ groups experts based on routing policies and merges each group into a single expert. But  merging experts inherently reduces the diversity of the model, potentially harming its ability to generalize across diverse input distributions. Methods like HC-SMoE____ mitigate retraining requirements but are still limited by the trade-off between compression and preserving the model's capacity. In contrast, our framework strategically isolates shared knowledge into a base weight while retaining expert-specific variations as delta weights.  In addition, our semi-dynamic pruning and other techniques are also not exist in the previous methods.


\textbf{Delta compression} in LLMs has emerged as a critical technique to reduce the storage and computational costs of deploying multiple fine-tuned models by compressing the differences (delta weights) between a base model and its fine-tuned variants. Recent advancements, GPT-Zip____ and BitDelta____  successfully quantize the delta weights into ultra-low bit. Delta-CoMe____ employs mixed-precision quantization to the varying singular vectors of decomposed delta weights. Another approach, DeltaZip____ develop a multi-tenant serving system by compressing delta weights. In contrast to these quantization and system-level works, \textbf{we not only first introduce delta compression into MoE compression, but also propose new techniques like truncation-aware SVD and semi-dynamic pruning,} achieving the optimal performance-efficiency trade-off.






\begin{figure*}[t] 
    \centering 
    \includegraphics[width=1\textwidth]{pic/main_fig.pdf} 
    \caption{Overall  Process of $D^2$-MoE. We  first merge original expert weights into a shared base weight, weighted according to their Fisher importance. Then, we derive delta weights and compress them using Singular Value Decomposition (SVD). Finally, we apply a two-step pruning strategy: static column-wise pruning followed by dynamic column-wise pruning to further optimize the base weight.}  
    \label{fig:main}
     %vspace{-5mm}
\end{figure*}