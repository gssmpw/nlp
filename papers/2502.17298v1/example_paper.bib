
@article{Asvd,
  title={Asvd: Activation-aware singular value decomposition for compressing large language models},
  author={Yuan, Zhihang and Shang, Yuzhang and Song, Yue and Wu, Qiang and Yan, Yan and Sun, Guangyu},
  journal={arXiv preprint arXiv:2312.05821},
  year={2023}
}



@article{merge_then_compress,
  title={Merge, then compress: Demystify efficient SMoe with hints from its routing policy},
  author={Li, Pingzhi and Zhang, Zhenyu and Yadav, Prateek and Sung, Yi-Lin and Cheng, Yu and Bansal, Mohit and Chen, Tianlong},
  journal={arXiv preprint arXiv:2310.01334},
  year={2023}
}

@article{RegMean,
  title={Dataless knowledge fusion by merging weights of language models},
  author={Jin, Xisen and Ren, Xiang and Preotiuc-Pietro, Daniel and Cheng, Pengxiang},
  journal={arXiv preprint arXiv:2212.09849},
  year={2022}
}

@article{TIES,
  title={Ties-merging: Resolving interference when merging models},
  author={Yadav, Prateek and Tam, Derek and Choshen, Leshem and Raffel, Colin A and Bansal, Mohit},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{Fisher,
  title={Merging models with fisher-weighted averaging},
  author={Matena, Michael S and Raffel, Colin A},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={17703--17716},
  year={2022}
}

@incollection{Bengio+chapter2007,
author = {Bengio, Yoshua and LeCun, Yann},
booktitle = {Large Scale Kernel Machines},
publisher = {MIT Press},
title = {Scaling Learning Algorithms Towards {AI}},
year = {2007}
}

@article{Hinton06,
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
journal = {Neural Computation},
pages = {1527--1554},
title = {A Fast Learning Algorithm for Deep Belief Nets},
volume = {18},
year = {2006}
}

@book{goodfellow2016deep,
title={Deep learning},
author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
volume={1},
year={2016},
publisher={MIT Press}
}



%%% Background
@article{DBLP:journals/corr/abs-2303-18223,
    author       = {Wayne Xin Zhao and
                  Kun Zhou and
                  Junyi Li and
                  Tianyi Tang and
                  Xiaolei Wang and
                  Yupeng Hou and
                  Yingqian Min and
                  Beichen Zhang and
                  Junjie Zhang and
                  Zican Dong and
                  Yifan Du and
                  Chen Yang and
                  Yushuo Chen and
                  Zhipeng Chen and
                  Jinhao Jiang and
                  Ruiyang Ren and
                  Yifan Li and
                  Xinyu Tang and
                  Zikang Liu and
                  Peiyu Liu and
                  Jian{-}Yun Nie and
                  Ji{-}Rong Wen},
    title        = {A Survey of Large Language Models},
    journal      = {CoRR},
    volume       = {abs/2303.18223},
    year         = {2023}
}
@article{DBLP:journals/corr/abs-2306-02781,
    author       = {Roberto Gozalo{-}Brizuela and
                  Eduardo C. Garrido{-}Merch{\'{a}}n},
    title        = {A survey of Generative {AI} Applications},
    journal      = {CoRR},
    volume       = {abs/2306.02781},
    year         = {2023}
}

@article{wan2023efficient,
    title={Efficient Large Language Models: A Survey},
    author={Wan, Zhongwei and Wang, Xin and others},
    year={2023},
    journal={arXiv preprint arXiv:2312.03863},
}
@misc{zhou2024survey,
      title={A Survey on Efficient Inference for Large Language Models}, 
      author={Zixuan Zhou and Xuefei Ning and Ke Hong and Tianyu Fu and Jiaming Xu and Shiyao Li and Yuming Lou and Luning Wang and Zhihang Yuan and Xiuhong Li and Shengen Yan and Guohao Dai and Xiao-Ping Zhang and Yuhan Dong and Yu Wang},
      year={2024},
      eprint={2404.14294},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@article{wang2024iot,
    title={IoT in the Era of Generative AI: Vision and Challenges},
    author={Wang, Xin and Wan, Zhongwei and Hekmati, Arvin and Zong, Mingyu and Alam, Samiul and Zhang, Mi and Krishnamachari, Bhaskar},
    journal={arXiv preprint arXiv:2401.01923},
    year={2024}
}
@article{DBLP:journals/corr/abs-2308-07633,
    author       = {Xunyu Zhu and
                  Jian Li and
                  Yong Liu and
                  Can Ma and
                  Weiping Wang},
    title        = {A Survey on Model Compression for Large Language Models},
    journal      = {CoRR},
    volume       = {abs/2308.07633},
    year         = {2023}
}
%%% Baseline
@inproceedings{DBLP:conf/nips/MaFW23,
    author       = {Xinyin Ma and
                  Gongfan Fang and
                  Xinchao Wang},
    title        = {LLM-Pruner: On the Structural Pruning of Large Language Models},
    booktitle    = {NeurIPS},
    year         = {2023}
}
@article{DBLP:journals/corr/abs-2210-17323,
    author       = {Elias Frantar and
                  Saleh Ashkboos and
                  Torsten Hoefler and
                  Dan Alistarh},
    title        = {{GPTQ:} Accurate Post-Training Quantization for Generative Pre-trained
                  Transformers},
    journal      = {CoRR},
    volume       = {abs/2210.17323},
    year         = {2022}
}
@article{DBLP:journals/corr/abs-2312-05821,
    author       = {Zhihang Yuan and
                  Yuzhang Shang and
                  Yue Song and
                  Qiang Wu and
                  Yan Yan and
                  Guangyu Sun},
    title        = {{ASVD:} Activation-aware Singular Value Decomposition for Compressing
                  Large Language Models},
    journal      = {CoRR},
    volume       = {abs/2312.05821},
    year         = {2023}
}
@inproceedings{DBLP:conf/iclr/HsuHCLSJ22,
    author       = {Yen{-}Chang Hsu and
                  Ting Hua and
                  Sungen Chang and
                  Qian Lou and
                  Yilin Shen and
                  Hongxia Jin},
    title        = {Language model compression with weighted low-rank factorization},
    booktitle    = {{ICLR}},
    publisher    = {OpenReview.net},
    year         = {2022}
}
%%% Other compression
@inproceedings{DBLP:conf/icml/FrantarA23,
    author       = {Elias Frantar and
                  Dan Alistarh},
    title        = {SparseGPT: Massive Language Models Can be Accurately Pruned in One-Shot},
    booktitle    = {{ICML}},
    series       = {Proceedings of Machine Learning Research},
    volume       = {202},
    pages        = {10323--10337},
    publisher    = {{PMLR}},
    year         = {2023}
}
@article{DBLP:journals/corr/abs-2306-08543,
    author       = {Yuxian Gu and
                  Li Dong and
                  Furu Wei and
                  Minlie Huang},
    title        = {Knowledge Distillation of Large Language Models},
    journal      = {CoRR},
    volume       = {abs/2306.08543},
    year         = {2023}
}
@inproceedings{DBLP:conf/icml/XiaoLSWDH23,
    author       = {Guangxuan Xiao and
                  Ji Lin and
                  Micka{\"{e}}l Seznec and
                  Hao Wu and
                  Julien Demouth and
                  Song Han},
    title        = {SmoothQuant: Accurate and Efficient Post-Training Quantization for
                  Large Language Models},
    booktitle    = {{ICML}},
    series       = {Proceedings of Machine Learning Research},
    volume       = {202},
    pages        = {38087--38099},
    publisher    = {{PMLR}},
    year         = {2023}
}
@inproceedings{DBLP:conf/acl/HsiehLYNFRKLP23,
    author       = {Cheng{-}Yu Hsieh and
                  Chun{-}Liang Li and
                  Chih{-}Kuan Yeh and
                  Hootan Nakhost and
                  Yasuhisa Fujii and
                  Alex Ratner and
                  Ranjay Krishna and
                  Chen{-}Yu Lee and
                  Tomas Pfister},
    title        = {Distilling Step-by-Step! Outperforming Larger Language Models with
                  Less Training Data and Smaller Model Sizes},
    booktitle    = {{ACL} (Findings)},
    pages        = {8003--8017},
    publisher    = {Association for Computational Linguistics},
    year         = {2023}
}
%%% LLM
@article{DBLP:journals/corr/abs-2302-13971,
    author       = {Hugo Touvron and
                  Thibaut Lavril and
                  Gautier Izacard and
                  Xavier Martinet and
                  Marie{-}Anne Lachaux and
                  Timoth{\'{e}}e Lacroix and
                  Baptiste Rozi{\`{e}}re and
                  Naman Goyal and
                  Eric Hambro and
                  Faisal Azhar and
                  Aur{\'{e}}lien Rodriguez and
                  Armand Joulin and
                  Edouard Grave and
                  Guillaume Lample},
    title        = {LLaMA: Open and Efficient Foundation Language Models},
    journal      = {CoRR},
    volume       = {abs/2302.13971},
    year         = {2023}
}
@inproceedings{DBLP:conf/nips/BrownMRSKDNSSAA20,
    author       = {Tom B. Brown and
                  Benjamin Mann and
                  Nick Ryder and
                  Melanie Subbiah and
                  Jared Kaplan and
                  Prafulla Dhariwal and
                  Arvind Neelakantan and
                  Pranav Shyam and
                  Girish Sastry and
                  Amanda Askell and
                  Sandhini Agarwal and
                  Ariel Herbert{-}Voss and
                  Gretchen Krueger and
                  Tom Henighan and
                  Rewon Child and
                  Aditya Ramesh and
                  Daniel M. Ziegler and
                  Jeffrey Wu and
                  Clemens Winter and
                  Christopher Hesse and
                  Mark Chen and
                  Eric Sigler and
                  Mateusz Litwin and
                  Scott Gray and
                  Benjamin Chess and
                  Jack Clark and
                  Christopher Berner and
                  Sam McCandlish and
                  Alec Radford and
                  Ilya Sutskever and
                  Dario Amodei},
    title        = {Language Models are Few-Shot Learners},
    booktitle    = {NeurIPS},
    year         = {2020}
}
@article{DBLP:journals/corr/abs-2307-09288,
    author       = {Hugo Touvron and
                  Louis Martin and
                  Kevin Stone and
                  Peter Albert and
                  Amjad Almahairi and
                  Yasmine Babaei and
                  Nikolay Bashlykov and
                  Soumya Batra and
                  Prajjwal Bhargava and
                  Shruti Bhosale and
                  Dan Bikel and
                  Lukas Blecher and
                  Cristian Canton{-}Ferrer and
                  Moya Chen and
                  Guillem Cucurull and
                  David Esiobu and
                  Jude Fernandes and
                  Jeremy Fu and
                  Wenyin Fu and
                  Brian Fuller and
                  Cynthia Gao and
                  Vedanuj Goswami and
                  Naman Goyal and
                  Anthony Hartshorn and
                  Saghar Hosseini and
                  Rui Hou and
                  Hakan Inan and
                  Marcin Kardas and
                  Viktor Kerkez and
                  Madian Khabsa and
                  Isabel Kloumann and
                  Artem Korenev and
                  Punit Singh Koura and
                  Marie{-}Anne Lachaux and
                  Thibaut Lavril and
                  Jenya Lee and
                  Diana Liskovich and
                  Yinghai Lu and
                  Yuning Mao and
                  Xavier Martinet and
                  Todor Mihaylov and
                  Pushkar Mishra and
                  Igor Molybog and
                  Yixin Nie and
                  Andrew Poulton and
                  Jeremy Reizenstein and
                  Rashi Rungta and
                  Kalyan Saladi and
                  Alan Schelten and
                  Ruan Silva and
                  Eric Michael Smith and
                  Ranjan Subramanian and
                  Xiaoqing Ellen Tan and
                  Binh Tang and
                  Ross Taylor and
                  Adina Williams and
                  Jian Xiang Kuan and
                  Puxin Xu and
                  Zheng Yan and
                  Iliyan Zarov and
                  Yuchen Zhang and
                  Angela Fan and
                  Melanie Kambadur and
                  Sharan Narang and
                  Aur{\'{e}}lien Rodriguez and
                  Robert Stojnic and
                  Sergey Edunov and
                  Thomas Scialom},
    title        = {Llama 2: Open Foundation and Fine-Tuned Chat Models},
    journal      = {CoRR},
    volume       = {abs/2307.09288},
    year         = {2023}
}
@article{DBLP:journals/corr/abs-2205-01068,
  author       = {Susan Zhang and
                  Stephen Roller and
                  Naman Goyal and
                  Mikel Artetxe and
                  Moya Chen and
                  Shuohui Chen and
                  Christopher Dewan and
                  Mona T. Diab and
                  Xian Li and
                  Xi Victoria Lin and
                  Todor Mihaylov and
                  Myle Ott and
                  Sam Shleifer and
                  Kurt Shuster and
                  Daniel Simig and
                  Punit Singh Koura and
                  Anjali Sridhar and
                  Tianlu Wang and
                  Luke Zettlemoyer},
  title        = {{OPT:} Open Pre-trained Transformer Language Models},
  journal      = {CoRR},
  volume       = {abs/2205.01068},
  year         = {2022}
}
@article{DBLP:journals/corr/abs-2310-06825,
    author       = {Albert Q. Jiang and
                  Alexandre Sablayrolles and
                  Arthur Mensch and
                  Chris Bamford and
                  Devendra Singh Chaplot and
                  Diego de Las Casas and
                  Florian Bressand and
                  Gianna Lengyel and
                  Guillaume Lample and
                  Lucile Saulnier and
                  L{\'{e}}lio Renard Lavaud and
                  Marie{-}Anne Lachaux and
                  Pierre Stock and
                  Teven Le Scao and
                  Thibaut Lavril and
                  Thomas Wang and
                  Timoth{\'{e}}e Lacroix and
                  William El Sayed},
    title        = {Mistral 7B},
    journal      = {CoRR},
    volume       = {abs/2310.06825},
    year         = {2023}
}
@misc{vicuna2023,
    title = {Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90\%* ChatGPT Quality},
    url = {https://lmsys.org/blog/2023-03-30-vicuna/},
    author = {Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E. and Stoica, Ion and Xing, Eric P.},
    month = {March},
    year = {2023}
}
%%% Dataset
@misc{eval-harness,
  author       = {Gao, Leo and Tow, Jonathan and Abbasi, Baber and Biderman, Stella and Black, Sid and DiPofi, Anthony and Foster, Charles and Golding, Laurence and Hsu, Jeffrey and Le Noac'h, Alain and Li, Haonan and McDonell, Kyle and Muennighoff, Niklas and Ociepa, Chris and Phang, Jason and Reynolds, Laria and Schoelkopf, Hailey and Skowron, Aviya and Sutawika, Lintang and Tang, Eric and Thite, Anish and Wang, Ben and Wang, Kevin and Zou, Andy},
  title        = {A framework for few-shot language model evaluation},
  month        = 12,
  year         = 2023,
  publisher    = {Zenodo},
  version      = {v0.4.0},
  doi          = {10.5281/zenodo.10256836},
  url          = {https://zenodo.org/records/10256836}
}
@inproceedings{DBLP:conf/emnlp/MihaylovCKS18,
  author       = {Todor Mihaylov and
                  Peter Clark and
                  Tushar Khot and
                  Ashish Sabharwal},
  title        = {Can a Suit of Armor Conduct Electricity? {A} New Dataset for Open
                  Book Question Answering},
  booktitle    = {{EMNLP}},
  pages        = {2381--2391},
  publisher    = {Association for Computational Linguistics},
  year         = {2018}
}
@inproceedings{DBLP:conf/aaai/SakaguchiBBC20,
  author       = {Keisuke Sakaguchi and
                  Ronan Le Bras and
                  Chandra Bhagavatula and
                  Yejin Choi},
  title        = {WinoGrande: An Adversarial Winograd Schema Challenge at Scale},
  booktitle    = {{AAAI}},
  pages        = {8732--8740},
  publisher    = {{AAAI} Press},
  year         = {2020}
}
@inproceedings{DBLP:conf/acl/ZellersHBFC19,
  author       = {Rowan Zellers and
                  Ari Holtzman and
                  Yonatan Bisk and
                  Ali Farhadi and
                  Yejin Choi},
  title        = {HellaSwag: Can a Machine Really Finish Your Sentence?},
  booktitle    = {{ACL} {(1)}},
  pages        = {4791--4800},
  publisher    = {Association for Computational Linguistics},
  year         = {2019}
}
@inproceedings{DBLP:conf/naacl/AminiGLKCH19,
  author       = {Aida Amini and
                  Saadia Gabriel and
                  Shanchuan Lin and
                  Rik Koncel{-}Kedziorski and
                  Yejin Choi and
                  Hannaneh Hajishirzi},
  title        = {MathQA: Towards Interpretable Math Word Problem Solving with Operation-Based
                  Formalisms},
  booktitle    = {{NAACL-HLT} {(1)}},
  pages        = {2357--2367},
  publisher    = {Association for Computational Linguistics},
  year         = {2019}
}
@article{DBLP:journals/jmlr/RaffelSRLNMZLL20,
    author       = {Colin Raffel and
                  Noam Shazeer and
                  Adam Roberts and
                  Katherine Lee and
                  Sharan Narang and
                  Michael Matena and
                  Yanqi Zhou and
                  Wei Li and
                  Peter J. Liu},
    title        = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text
                  Transformer},
    journal      = {J. Mach. Learn. Res.},
    volume       = {21},
    pages        = {140:1--140:67},
    year         = {2020}
}
@inproceedings{DBLP:conf/iclr/MerityX0S17,
    author       = {Stephen Merity and
                  Caiming Xiong and
                  James Bradbury and
                  Richard Socher},
    title        = {Pointer Sentinel Mixture Models},
    booktitle    = {{ICLR} (Poster)},
    publisher    = {OpenReview.net},
    year         = {2017}
}
@article{DBLP:journals/coling/MarcusSM94,
    author       = {Mitchell P. Marcus and
                  Beatrice Santorini and
                  Mary Ann Marcinkiewicz},
    title        = {Building a Large Annotated Corpus of English: The Penn Treebank},
    journal      = {Comput. Linguistics},
    volume       = {19},
    number       = {2},
    pages        = {313--330},
    year         = {1993}
}
@inproceedings{DBLP:conf/aaai/BiskZLGC20,
    author       = {Yonatan Bisk and
                  Rowan Zellers and
                  Ronan Le Bras and
                  Jianfeng Gao and
                  Yejin Choi},
    title        = {{PIQA:} Reasoning about Physical Commonsense in Natural Language},
    booktitle    = {{AAAI}},
    pages        = {7432--7439},
    publisher    = {{AAAI} Press},
    year         = {2020}
}

@misc{alpaca,
    author = {Rohan Taori and Ishaan Gulrajani and Tianyi Zhang and Yann Dubois and Xuechen Li and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto },
    title = {Stanford Alpaca: An Instruction-following LLaMA model},
    year = {2023},
    publisher = {GitHub},
    journal = {GitHub repository},
    howpublished = {\url{https://github.com/tatsu-lab/stanford_alpaca}},
}

%%% technique
@inproceedings{DBLP:conf/iclr/HuSWALWWC22,
    author       = {Edward J. Hu and
                  Yelong Shen and
                  Phillip Wallis and
                  Zeyuan Allen{-}Zhu and
                  Yuanzhi Li and
                  Shean Wang and
                  Lu Wang and
                  Weizhu Chen},
    title        = {LoRA: Low-Rank Adaptation of Large Language Models},
    booktitle    = {{ICLR}},
    publisher    = {OpenReview.net},
    year         = {2022}
}
@article{GOLUB1987317,
    title = {A generalization of the Eckart-Young-Mirsky matrix approximation theorem},
    journal = {Linear Algebra and its Applications},
    volume = {88-89},
    pages = {317-327},
    year = {1987},
    issn = {0024-3795},
    doi = {https://doi.org/10.1016/0024-3795(87)90114-5},
    url = {https://www.sciencedirect.com/science/article/pii/0024379587901145},
    author = {G.H. Golub and Alan Hoffman and G.W. Stewart},
    abstract = {The Eckart-Young-Mirsky theorem solves the problem of approximating a matrix by one of lower rank. However, the approximation generally differs from the original in all its elements. In this paper it is shown how to obtain a best approximation of lower rank in which a specified set of columns of the matrix remains fixed. The paper concludes with some applications of the generalization.}
}

%%%% Dataset
@inproceedings{DBLP:conf/emnlp/BowmanAPM15,
    author       = {Samuel R. Bowman and
                  Gabor Angeli and
                  Christopher Potts and
                  Christopher D. Manning},
    title        = {A large annotated corpus for learning natural language inference},
    booktitle    = {{EMNLP}},
    pages        = {632--642},
    publisher    = {The Association for Computational Linguistics},
    year         = {2015}
}
@article{DBLP:journals/corr/abs-1803-05457,
    author       = {Peter Clark and
                  Isaac Cowhey and
                  Oren Etzioni and
                  Tushar Khot and
                  Ashish Sabharwal and
                  Carissa Schoenick and
                  Oyvind Tafjord},
    title        = {Think you have Solved Question Answering? Try ARC, the {AI2} Reasoning
                  Challenge},
    journal      = {CoRR},
    volume       = {abs/1803.05457},
    year         = {2018}
}
@inproceedings{DBLP:conf/acl-iwp/DolanB05,
    author       = {William B. Dolan and
                  Chris Brockett},
    title        = {Automatically Constructing a Corpus of Sentential Paraphrases},
    booktitle    = {IWP@IJCNLP},
    publisher    = {Asian Federation of Natural Language Processing},
    year         = {2005}
}
@article{DBLP:journals/corr/abs-1804-07461,
    author       = {Alex Wang and
                  Amanpreet Singh and
                  Julian Michael and
                  Felix Hill and
                  Omer Levy and
                  Samuel R. Bowman},
    title        = {{GLUE:} {A} Multi-Task Benchmark and Analysis Platform for Natural
                  Language Understanding},
    journal      = {CoRR},
    volume       = {abs/1804.07461},
    year         = {2018}
}
@article{DBLP:journals/tacl/WarstadtSB19,
    author       = {Alex Warstadt and
                  Amanpreet Singh and
                  Samuel R. Bowman},
    title        = {Neural Network Acceptability Judgments},
    journal      = {Trans. Assoc. Comput. Linguistics},
    volume       = {7},
    pages        = {625--641},
    year         = {2019}
}
@inproceedings{DBLP:conf/emnlp/SocherPWCMNP13,
    author       = {Richard Socher and
                  Alex Perelygin and
                  Jean Wu and
                  Jason Chuang and
                  Christopher D. Manning and
                  Andrew Y. Ng and
                  Christopher Potts},
    title        = {Recursive Deep Models for Semantic Compositionality Over a Sentiment
                  Treebank},
    booktitle    = {{EMNLP}},
    pages        = {1631--1642},
    publisher    = {{ACL}},
    year         = {2013}
}
@inproceedings{DBLP:conf/naacl/WilliamsNB18,
    author       = {Adina Williams and
                  Nikita Nangia and
                  Samuel R. Bowman},
    title        = {A Broad-Coverage Challenge Corpus for Sentence Understanding through
                  Inference},
    booktitle    = {{NAACL-HLT}},
    pages        = {1112--1122},
    publisher    = {Association for Computational Linguistics},
    year         = {2018}
}
@inproceedings{DBLP:conf/naacl/ClarkLCK0T19,
    author       = {Christopher Clark and
                  Kenton Lee and
                  Ming{-}Wei Chang and
                  Tom Kwiatkowski and
                  Michael Collins and
                  Kristina Toutanova},
    title        = {BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions},
    booktitle    = {{NAACL-HLT} {(1)}},
    pages        = {2924--2936},
    publisher    = {Association for Computational Linguistics},
    year         = {2019}
}

@book{DBLP:books/siam/Meyer00,
    author       = {Carl Dean Meyer},
    title        = {Matrix Analysis and Applied Linear Algebra},
    publisher    = {{SIAM}},
    year         = {2000}
}

@article{DBLP:journals/corr/abs-2306-00978,
    author       = {Ji Lin and
                  Jiaming Tang and
                  Haotian Tang and
                  Shang Yang and
                  Xingyu Dang and
                  Song Han},
    title        = {{AWQ:} Activation-aware Weight Quantization for {LLM} Compression
                  and Acceleration},
    journal      = {CoRR},
    volume       = {abs/2306.00978},
    year         = {2023}
}


@misc{xiao2024smoothquant,
      title={SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models}, 
      author={Guangxuan Xiao and Ji Lin and Mickael Seznec and Hao Wu and Julien Demouth and Song Han},
      year={2024},
      eprint={2211.10438},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{frantar2023gptq,
      title={GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers}, 
      author={Elias Frantar and Saleh Ashkboos and Torsten Hoefler and Dan Alistarh},
      year={2023},
      eprint={2210.17323},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{lin2024awq,
      title={AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration}, 
      author={Ji Lin and Jiaming Tang and Haotian Tang and Shang Yang and Wei-Ming Chen and Wei-Chen Wang and Guangxuan Xiao and Xingyu Dang and Chuang Gan and Song Han},
      year={2024},
      eprint={2306.00978},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}



@inproceedings{li2024merge,
    title={Merge, Then Compress: Demystify Efficient {SM}oE with Hints from Its Routing Policy},
    author={Pingzhi Li and Zhenyu Zhang and Prateek Yadav and Yi-Lin Sung and Yu Cheng and Mohit Bansal and Tianlong Chen},
    booktitle={The Twelfth International Conference on Learning Representations},
    year={2024},
    url={https://openreview.net/forum?id=eFWG9Cy3WK}
}

@misc{shazeer2017outrageously,
      title={Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer}, 
      author={Noam Shazeer and Azalia Mirhoseini and Krzysztof Maziarz and Andy Davis and Quoc Le and Geoffrey Hinton and Jeff Dean},
      year={2017},
      eprint={1701.06538},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{jiang2024mixtral,
      title={Mixtral of Experts}, 
      author={Albert Q. Jiang and Alexandre Sablayrolles and Antoine Roux and Arthur Mensch and Blanche Savary and Chris Bamford and Devendra Singh Chaplot and Diego de las Casas and Emma Bou Hanna and Florian Bressand and Gianna Lengyel and Guillaume Bour and Guillaume Lample and Lélio Renard Lavaud and Lucile Saulnier and Marie-Anne Lachaux and Pierre Stock and Sandeep Subramanian and Sophia Yang and Szymon Antoniak and Teven Le Scao and Théophile Gervet and Thibaut Lavril and Thomas Wang and Timothée Lacroix and William El Sayed},
      year={2024},
      eprint={2401.04088},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{chen2022taskspecific,
      title={Task-Specific Expert Pruning for Sparse Mixture-of-Experts}, 
      author={Tianyu Chen and Shaohan Huang and Yuan Xie and Binxing Jiao and Daxin Jiang and Haoyi Zhou and Jianxin Li and Furu Wei},
      year={2022},
      eprint={2206.00277},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@article{fedus2022switch,
  title={Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity},
  author={Fedus, William and Zoph, Barret and Shazeer, Noam},
  journal={Journal of Machine Learning Research},
  volume={23},
  number={120},
  pages={1--39},
  year={2022}
}

@misc{rajbhandari2022deepspeedmoe,
      title={DeepSpeed-MoE: Advancing Mixture-of-Experts Inference and Training to Power Next-Generation AI Scale}, 
      author={Samyam Rajbhandari and Conglong Li and Zhewei Yao and Minjia Zhang and Reza Yazdani Aminabadi and Ammar Ahmad Awan and Jeff Rasley and Yuxiong He},
      year={2022},
      eprint={2201.05596},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{artetxe2022efficient,
      title={Efficient Large Scale Language Modeling with Mixtures of Experts}, 
      author={Mikel Artetxe and Shruti Bhosale and Naman Goyal and Todor Mihaylov and Myle Ott and Sam Shleifer and Xi Victoria Lin and Jingfei Du and Srinivasan Iyer and Ramakanth Pasunuru and Giri Anantharaman and Xian Li and Shuohui Chen and Halil Akin and Mandeep Baines and Louis Martin and Xing Zhou and Punit Singh Koura and Brian O'Horo and Jeff Wang and Luke Zettlemoyer and Mona Diab and Zornitsa Kozareva and Ves Stoyanov},
      year={2022},
      eprint={2112.10684},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@article{dai2024deepseekmoe,
  title={Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models},
  author={Dai, Damai and Deng, Chengqi and Zhao, Chenggang and Xu, RX and Gao, Huazuo and Chen, Deli and Li, Jiashi and Zeng, Wangding and Yu, Xingkai and Wu, Y and others},
  journal={arXiv preprint arXiv:2401.06066},
  year={2024}
}



@misc{krajewski2024scaling,
      title={Scaling Laws for Fine-Grained Mixture of Experts}, 
      author={Jakub Krajewski and Jan Ludziejewski and Kamil Adamczewski and Maciej Pióro and Michał Krutul and Szymon Antoniak and Kamil Ciebiera and Krystian Król and Tomasz Odrzygóźdź and Piotr Sankowski and Marek Cygan and Sebastian Jaszczur},
      year={2024},
      eprint={2402.07871},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{ma2023llmpruner,
      title={LLM-Pruner: On the Structural Pruning of Large Language Models}, 
      author={Xinyin Ma and Gongfan Fang and Xinchao Wang},
      year={2023},
      eprint={2305.11627},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{2019t5,
    author = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
    title = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
    journal = {arXiv e-prints},
    year = {2019},
    archivePrefix = {arXiv},
    eprint = {1910.10683},
}

@InProceedings{ai2:winogrande,
title = {WinoGrande: An Adversarial Winograd Schema Challenge at Scale},
authors={Keisuke, Sakaguchi and Ronan, Le Bras and Chandra, Bhagavatula and Yejin, Choi
},
year={2019}
}

@inproceedings{gordon-etal-2012-semeval,
    title = "{S}em{E}val-2012 Task 7: Choice of Plausible Alternatives: An Evaluation of Commonsense Causal Reasoning",
    author = "Gordon, Andrew  and
      Kozareva, Zornitsa  and
      Roemmele, Melissa",
    editor = "Agirre, Eneko  and
      Bos, Johan  and
      Diab, Mona  and
      Manandhar, Suresh  and
      Marton, Yuval  and
      Yuret, Deniz",
    booktitle = "*{SEM} 2012: The First Joint Conference on Lexical and Computational Semantics {--} Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation ({S}em{E}val 2012)",
    month = "7-8 " # jun,
    year = "2012",
    address = "Montr{\'e}al, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/S12-1052",
    pages = "394--398",
}

@misc{mihaylov2018suit,
      title={Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering}, 
      author={Todor Mihaylov and Peter Clark and Tushar Khot and Ashish Sabharwal},
      year={2018},
      eprint={1809.02789},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{zellers2019hellaswag,
      title={HellaSwag: Can a Machine Really Finish Your Sentence?}, 
      author={Rowan Zellers and Ari Holtzman and Yonatan Bisk and Ali Farhadi and Yejin Choi},
      year={2019},
      eprint={1905.07830},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{hendrycks2021measuring,
      title={Measuring Massive Multitask Language Understanding}, 
      author={Dan Hendrycks and Collin Burns and Steven Basart and Andy Zou and Mantas Mazeika and Dawn Song and Jacob Steinhardt},
      year={2021},
      eprint={2009.03300},
      archivePrefix={arXiv},
      primaryClass={cs.CY}
}

@misc{brown2020language,
      title={Language Models are Few-Shot Learners}, 
      author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
      year={2020},
      eprint={2005.14165},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{touvron2023llama,
      title={Llama 2: Open Foundation and Fine-Tuned Chat Models}, 
      author={Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and Dan Bikel and Lukas Blecher and Cristian Canton Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel Kloumann and Artem Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and Ranjan Subramanian and Xiaoqing Ellen Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zheng Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom},
      year={2023},
      eprint={2307.09288},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{jiang2023mistral,
      title={Mistral 7B}, 
      author={Albert Q. Jiang and Alexandre Sablayrolles and Arthur Mensch and Chris Bamford and Devendra Singh Chaplot and Diego de las Casas and Florian Bressand and Gianna Lengyel and Guillaume Lample and Lucile Saulnier and Lélio Renard Lavaud and Marie-Anne Lachaux and Pierre Stock and Teven Le Scao and Thibaut Lavril and Thomas Wang and Timothée Lacroix and William El Sayed},
      year={2023},
      eprint={2310.06825},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{kaplan2020scaling,
      title={Scaling Laws for Neural Language Models}, 
      author={Jared Kaplan and Sam McCandlish and Tom Henighan and Tom B. Brown and Benjamin Chess and Rewon Child and Scott Gray and Alec Radford and Jeffrey Wu and Dario Amodei},
      year={2020},
      eprint={2001.08361},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@inproceedings{openai2024gpt4,
  title={GPT-4 Technical Report},
  author={OpenAI Josh Achiam and Steven Adler and Sandhini Agarwal and Lama Ahmad and Ilge Akkaya and Florencia Leoni Aleman and Diogo Almeida and Janko Altenschmidt and Sam Altman and Shyamal Anadkat and Red Avila and Igor Babuschkin and Suchir Balaji and Valerie Balcom and Paul Baltescu and Haim-ing Bao and Mo Bavarian and Jeff Belgum and Irwan Bello and Jake Berdine and Gabriel Bernadett-Shapiro and Christopher Berner and Lenny Bogdonoff and Oleg Boiko and Madelaine Boyd and Anna-Luisa Brakman and Greg Brockman and Tim Brooks and Miles Brundage and Kevin Button and Trevor Cai and Rosie Campbell and Andrew Cann and Brittany Carey and Chelsea Carlson and Rory Carmichael and Brooke Chan and Che Chang and Fotis Chantzis and Derek Chen and Sully Chen and Ruby Chen and Jason Chen and Mark Chen and Benjamin Chess and Chester Cho and Casey Chu and Hyung Won Chung and Dave Cummings and Jeremiah Currier and Yunxing Dai and Cory Decareaux and Thomas Degry and Noah Deutsch and Damien Deville and Arka Dhar and David Dohan and Steve Dowling and Sheila Dunning and Adrien Ecoffet and Atty Eleti and Tyna Eloundou and David Farhi and Liam Fedus and Niko Felix and Sim'on Posada Fishman and Juston Forte and Is-abella Fulford and Leo Gao and Elie Georges and Christian Gibson and Vik Goel and Tarun Gogineni and Gabriel Goh and Raphael Gontijo-Lopes and Jonathan Gordon and Morgan Grafstein and Scott Gray and Ryan Greene and Joshua Gross and Shixiang Shane Gu and Yufei Guo and Chris Hallacy and Jesse Han and Jeff Harris and Yuchen He and Mike Heaton and Johannes Heidecke and Chris Hesse and Alan Hickey and Wade Hickey and Peter Hoeschele and Brandon Houghton and Kenny Hsu and Shengli Hu and Xin Hu and Joost Huizinga and Shantanu Jain and Shawn Jain and Joanne Jang and Angela Jiang and Roger Jiang and Haozhun Jin and Denny Jin and Shino Jomoto and Billie Jonn and Heewoo Jun and Tomer Kaftan and Lukasz Kaiser and Ali Kamali and Ingmar Kanitscheider and Nitish Shirish Keskar and Tabarak Khan and Logan Kilpatrick and Jong Wook Kim and Christina Kim and Yongjik Kim and Hendrik Kirchner and Jamie Ryan Kiros and Matthew Knight and Daniel Kokotajlo and Lukasz Kondraciuk and Andrew Kondrich and Aris Konstantinidis and Kyle Kosic and Gretchen Krueger and Vishal Kuo and Michael Lampe and Ikai Lan and Teddy Lee and Jan Leike and Jade Leung and Daniel Levy and Chak Ming Li and Rachel Lim and Molly Lin and Stephanie Lin and Ma-teusz Litwin and Theresa Lopez and Ryan Lowe and Patricia Lue and Anna Makanju and Kim Malfacini and Sam Manning and Todor Markov and Yaniv Markovski and Bianca Martin and Katie Mayer and Andrew Mayne and Bob McGrew and Scott Mayer McKinney and Christine McLeavey and Paul McMillan and Jake McNeil and David Medina and Aalok Mehta and Jacob Menick and Luke Metz and Andrey Mishchenko and Pamela Mishkin and Vinnie Monaco and Evan Morikawa and Daniel P. Mossing and Tong Mu and Mira Murati and Oleg Murk and David M'ely and Ashvin Nair and Reiichiro Nakano and Rajeev Nayak and Arvind Neelakantan and Richard Ngo and Hyeonwoo Noh and Ouyang Long and Cullen O'Keefe and Jakub W. Pachocki and Alex Paino and Joe Palermo and Ashley Pantuliano and Giambattista Parascandolo and Joel Parish and Emy Parparita and Alexandre Passos and Mikhail Pavlov and Andrew Peng and Adam Perelman and Filipe de Avila Belbute Peres and Michael Petrov and Henrique Pond{\'e} de Oliveira Pinto and Michael Pokorny and Michelle Pokrass and Vitchyr H. Pong and Tolly Powell and Alethea Power and Boris Power and Elizabeth Proehl and Raul Puri and Alec Radford and Jack W. Rae and Aditya Ramesh and Cameron Raymond and Francis Real and Kendra Rimbach and Carl Ross and Bob Rotsted and Henri Roussez and Nick Ryder and Mario D. Saltarelli and Ted Sanders and Shibani Santurkar and Girish Sastry and Heather Schmidt and David Schnurr and John Schulman and Daniel Selsam and Kyla Sheppard and Toki Sherbakov and Jessica Shieh and Sarah Shoker and Pranav Shyam and Szymon Sidor and Eric Sigler and Maddie Simens and Jordan Sitkin and Katarina Slama and Ian Sohl and Benjamin D. Sokolowsky and Yang Song and Natalie Staudacher and Felipe Petroski Such and Natalie Summers and Ilya Sutskever and Jie Tang and Nikolas A. Tezak and Madeleine Thompson and Phil Tillet and Amin Tootoonchian and Elizabeth Tseng and Preston Tuggle and Nick Turley and Jerry Tworek and Juan Felipe Cer'on Uribe and Andrea Vallone and Arun Vijayvergiya and Chelsea Voss and Carroll L. Wainwright and Justin Jay Wang and Alvin Wang and Ben Wang and Jonathan Ward and Jason Wei and CJ Weinmann and Akila Welihinda and Peter Welinder and Jiayi Weng and Lilian Weng and Matt Wiethoff and Dave Willner and Clemens Winter and Samuel Wolrich and Hannah Wong and Lauren Workman and Sherwin Wu and Jeff Wu and Michael Wu and Kai Xiao and Tao Xu and Sarah Yoo and Kevin Yu and Qim-ing Yuan and Wojciech Zaremba and Rowan Zellers and Chong Zhang and Marvin Zhang and Shengjia Zhao and Tianhao Zheng and Juntang Zhuang and William Zhuk and Barret Zoph},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:257532815}
}


@misc{aminabadi2022deepspeed,
      title={DeepSpeed Inference: Enabling Efficient Inference of Transformer Models at Unprecedented Scale}, 
      author={Reza Yazdani Aminabadi and Samyam Rajbhandari and Minjia Zhang and Ammar Ahmad Awan and Cheng Li and Du Li and Elton Zheng and Jeff Rasley and Shaden Smith and Olatunji Ruwase and Yuxiong He},
      year={2022},
      eprint={2207.00032},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{shoeybi2020megatronlm,
      title={Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism}, 
      author={Mohammad Shoeybi and Mostofa Patwary and Raul Puri and Patrick LeGresley and Jared Casper and Bryan Catanzaro},
      year={2020},
      eprint={1909.08053},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{frantar2023optimal,
      title={Optimal Brain Compression: A Framework for Accurate Post-Training Quantization and Pruning}, 
      author={Elias Frantar and Sidak Pal Singh and Dan Alistarh},
      year={2023},
      eprint={2208.11580},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{merity2016pointer,
      title={Pointer Sentinel Mixture Models},
      author={Stephen Merity and Caiming Xiong and James Bradbury and Richard Socher},
      year={2016},
      eprint={1609.07843},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{jaiswal2024ffnskipllm,
      title={FFN-SkipLLM: A Hidden Gem for Autoregressive Decoding with Adaptive Feed Forward Skipping}, 
      author={Ajay Jaiswal and Bodun Hu and Lu Yin and Yeonju Ro and Shiwei Liu and Tianlong Chen and Aditya Akella},
      year={2024},
      eprint={2404.03865},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@article{gong2024llm,
  title={LLM-QBench: A Benchmark Towards the Best Practice for Post-training Quantization of Large Language Models},
  author={Gong, Ruihao and Yong, Yang and Gu, Shiqiao and Huang, Yushi and Zhang, Yunchen and Liu, Xianglong and Tao, Dacheng},
  journal={arXiv preprint arXiv:2405.06001},
  year={2024}
}


@misc{shazeer2020glu,
      title={GLU Variants Improve Transformer}, 
      author={Noam Shazeer},
      year={2020},
      eprint={2002.05202},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}



@article{pan2023smoothquant+,
  title={SmoothQuant+: Accurate and Efficient 4-bit Post-Training WeightQuantization for LLM},
  author={Pan, Jiayi and Wang, Chengcan and Zheng, Kaifu and Li, Yangguang and Wang, Zhenyu and Feng, Bin},
  journal={arXiv preprint arXiv:2312.03788},
  year={2023}
}


@article{sharify2024combining,
  title={Combining multiple post-training techniques to achieve most efficient quantized LLMs},
  author={Sharify, Sayeh and Xu, Zifei and Wang, Xin and others},
  journal={arXiv preprint arXiv:2405.07135},
  year={2024}
}


@article{liu2023llm,
  title={Llm-fp4: 4-bit floating-point quantized transformers},
  author={Liu, Shih-yang and Liu, Zechun and Huang, Xijie and Dong, Pingcheng and Cheng, Kwang-Ting},
  journal={arXiv preprint arXiv:2310.16836},
  year={2023}
}


@article{jacobs1991adaptive,
  title={Adaptive mixtures of local experts},
  author={Jacobs, Robert A and Jordan, Michael I and Nowlan, Steven J and Hinton, Geoffrey E},
  journal={Neural computation},
  volume={3},
  number={1},
  pages={79--87},
  year={1991},
  publisher={MIT Press}
}

@article{jordan1994hierarchical,
  title={Hierarchical mixtures of experts and the EM algorithm},
  author={Jordan, Michael I and Jacobs, Robert A},
  journal={Neural computation},
  volume={6},
  number={2},
  pages={181--214},
  year={1994},
  publisher={MIT Press}
}

@article{eigen2013learning,
  title={Learning factored representations in a deep mixture of experts},
  author={Eigen, David and Ranzato, Marc'Aurelio and Sutskever, Ilya},
  journal={arXiv preprint arXiv:1312.4314},
  year={2013}
}



@article{riquelme2021scaling,
  title={Scaling vision with sparse mixture of experts},
  author={Riquelme, Carlos and Puigcerver, Joan and Mustafa, Basil and Neumann, Maxim and Jenatton, Rodolphe and Susano Pinto, Andr{\'e} and Keysers, Daniel and Houlsby, Neil},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={8583--8595},
  year={2021}
}

@article{lepikhin2020gshard,
  title={Gshard: Scaling giant models with conditional computation and automatic sharding},
  author={Lepikhin, Dmitry and Lee, HyoukJoong and Xu, Yuanzhong and Chen, Dehao and Firat, Orhan and Huang, Yanping and Krikun, Maxim and Shazeer, Noam and Chen, Zhifeng},
  journal={arXiv preprint arXiv:2006.16668},
  year={2020}
}

@inproceedings{du2022glam,
  title={Glam: Efficient scaling of language models with mixture-of-experts},
  author={Du, Nan and Huang, Yanping and Dai, Andrew M and Tong, Simon and Lepikhin, Dmitry and Xu, Yuanzhong and Krikun, Maxim and Zhou, Yanqi and Yu, Adams Wei and Firat, Orhan and others},
  booktitle={International Conference on Machine Learning},
  pages={5547--5569},
  year={2022},
  organization={PMLR}
}



@misc{llama-moe-2023,
  title={LLaMA-MoE: Building Mixture-of-Experts from LLaMA with Continual Pre-training},
  author={LLaMA-MoE Team},
  year={2023},
  month={Dec},
  url={https://github.com/pjlab-sys4nlp/llama-moe}
}

@article{xue2024openmoe,
  title={Openmoe: An early effort on open mixture-of-experts language models},
  author={Xue, Fuzhao and Zheng, Zian and Fu, Yao and Ni, Jinjie and Zheng, Zangwei and Zhou, Wangchunshu and You, Yang},
  journal={arXiv preprint arXiv:2402.01739},
  year={2024}
}



@inproceedings{clark2022unified,
  title={Unified scaling laws for routed language models},
  author={Clark, Aidan and de Las Casas, Diego and Guy, Aurelia and Mensch, Arthur and Paganini, Michela and Hoffmann, Jordan and Damoc, Bogdan and Hechtman, Blake and Cai, Trevor and Borgeaud, Sebastian and others},
  booktitle={International conference on machine learning},
  pages={4057--4086},
  year={2022},
  organization={PMLR}
}

@inproceedings{gururangan2022demix,
  title={DEMix Layers: Disentangling Domains for Modular Language Modeling},
  author={Gururangan, Suchin and Lewis, Mike and Holtzman, Ari and Smith, Noah A and Zettlemoyer, Luke},
  booktitle={Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={5557--5576},
  year={2022}
}

@inproceedings{rajbhandari2022deepspeed,
  title={Deepspeed-moe: Advancing mixture-of-experts inference and training to power next-generation ai scale},
  author={Rajbhandari, Samyam and Li, Conglong and Yao, Zhewei and Zhang, Minjia and Aminabadi, Reza Yazdani and Awan, Ammar Ahmad and Rasley, Jeff and He, Yuxiong},
  booktitle={International conference on machine learning},
  pages={18332--18346},
  year={2022},
  organization={PMLR}
}

@inproceedings{lewis2021base,
  title={Base layers: Simplifying training of large, sparse models},
  author={Lewis, Mike and Bhosale, Shruti and Dettmers, Tim and Goyal, Naman and Zettlemoyer, Luke},
  booktitle={International Conference on Machine Learning},
  pages={6265--6274},
  year={2021},
  organization={PMLR}
}

@article{roller2021hash,
  title={Hash layers for large sparse models},
  author={Roller, Stephen and Sukhbaatar, Sainbayar and Weston, Jason and others},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={17555--17566},
  year={2021}
}

@article{zhou2022mixture,
  title={Mixture-of-experts with expert choice routing},
  author={Zhou, Yanqi and Lei, Tao and Liu, Hanxiao and Du, Nan and Huang, Yanping and Zhao, Vincent and Dai, Andrew M and Le, Quoc V and Laudon, James and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={7103--7114},
  year={2022}
}

@article{zoph2022st,
  title={St-moe: Designing stable and transferable sparse expert models},
  author={Zoph, Barret and Bello, Irwan and Kumar, Sameer and Du, Nan and Huang, Yanping and Dean, Jeff and Shazeer, Noam and Fedus, William},
  journal={arXiv preprint arXiv:2202.08906},
  year={2022}
}

@article{puigcerver2023sparse,
  title={From sparse to soft mixtures of experts},
  author={Puigcerver, Joan and Riquelme, Carlos and Mustafa, Basil and Houlsby, Neil},
  journal={arXiv preprint arXiv:2308.00951},
  year={2023}
}

@article{huang2024harder,
  title={Harder Tasks Need More Experts: Dynamic Routing in MoE Models},
  author={Huang, Quzhe and An, Zhenwei and Zhuang, Nan and Tao, Mingxu and Zhang, Chen and Jin, Yang and Xu, Kun and Chen, Liwei and Huang, Songfang and Feng, Yansong},
  journal={arXiv preprint arXiv:2403.07652},
  year={2024}
}

@article{shen2023moduleformer,
  title={Moduleformer: Learning modular large language models from uncurated data},
  author={Shen, Yikang and Zhang, Zheyu and Cao, Tianyou and Tan, Shawn and Chen, Zhenfang and Gan, Chuang},
  journal={arXiv preprint arXiv:2306.04640},
  year={2023}
}

@inproceedings{chen2022sparse,
  title={Sparse MoE as the New Dropout: Scaling Dense and Self-Slimmable Transformers},
  author={Chen, Tianlong and Zhang, Zhenyu and JAISWAL, AJAY KUMAR and Liu, Shiwei and Wang, Zhangyang},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2022}
}

@article{pan2024dense,
  title={Dense Training, Sparse Inference: Rethinking Training of Mixture-of-Experts Language Models},
  author={Pan, Bowen and Shen, Yikang and Liu, Haokun and Mishra, Mayank and Zhang, Gaoyuan and Oliva, Aude and Raffel, Colin and Panda, Rameswar},
  journal={arXiv preprint arXiv:2404.05567},
  year={2024}
}

@inproceedings{he-etal-2023-pad,
    title = "{PAD}-Net: An Efficient Framework for Dynamic Networks",
    author = "He, Shwai  and
      Ding, Liang  and
      Dong, Daize  and
      Liu, Boan  and
      Yu, Fuqiang  and
      Tao, Dacheng",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.803",
    doi = "10.18653/v1/2023.acl-long.803",
    pages = "14354--14366",
    abstract = "Dynamic networks, e.g., Dynamic Convolution (DY-Conv) and the Mixture of Experts (MoE), have been extensively explored as they can considerably improve the model{'}s representation power with acceptable computational cost. The common practice in implementing dynamic networks is to convert the given static layers into fully dynamic ones where all parameters are dynamic (at least within a single layer) and vary with the input. However, such a fully dynamic setting may cause redundant parameters and high deployment costs, limiting the applicability of dynamic networks to a broader range of tasks and models. The main contributions of our work are challenging the basic commonsense in dynamic networks and proposing a partially dynamic network, namely PAD-Net, to transform the redundant dynamic parameters into static ones. Also, we further design Iterative Mode Partition to partition dynamic and static parameters efficiently. Our method is comprehensively supported by large-scale experiments with two typical advanced dynamic architectures, i.e., DY-Conv and MoE, on both image classification and GLUE benchmarks. Encouragingly, we surpass the fully dynamic networks by $+0.7\%$ top-1 acc with only 30{\%} dynamic parameters for ResNet-50 and $+1.9\%$ average score in language understanding with only 50{\%} dynamic parameters for BERT. Code will be released at: \url{https://github.com/Shwai-He/PAD-Net}.",
}

@misc{song2023powerinfer,
      title={PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU}, 
      author={Yixin Song and Zeyu Mi and Haotong Xie and Haibo Chen},
      year={2023},
      eprint={2312.12456},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{xue2024moeinfinity,
      title={MoE-Infinity: Activation-Aware Expert Offloading for Efficient MoE Serving}, 
      author={Leyang Xue and Yao Fu and Zhan Lu and Luo Mai and Mahesh Marina},
      year={2024},
      eprint={2401.14361},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{awq,
  title={AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration},
  author={Lin, Ji and Tang, Jiaming and Tang, Haotian and Yang, Shang and Chen, Wei-Ming and Wang, Wei-Chen and Xiao, Guangxuan and Dang, Xingyu and Gan, Chuang and Han, Song},
  booktitle={MLSys},
  year={2024}
}

@article{gptq,
  title={{GPTQ}: Accurate Post-training Compression for Generative Pretrained Transformers}, 
  author={Elias Frantar and Saleh Ashkboos and Torsten Hoefler and Dan Alistarh},
  year={2022},
  journal={arXiv preprint arXiv:2210.17323}
}

@article{wanda,
  title={A Simple and Effective Pruning Approach for Large Language Models}, 
  author={Sun, Mingjie and Liu, Zhuang and Bair, Anna and Kolter, J. Zico},
  year={2023},
  journal={arXiv preprint arXiv:2306.11695}
}

@article{sparsegpt,
  title={{SparseGPT}: Massive Language Models Can Be Accurately Pruned in One-Shot}, 
  author={Elias Frantar and Dan Alistarh},
  year={2023},
  journal={arXiv preprint arXiv:2301.00774}
}

@misc{jacob2017quantization,
      title={Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference}, 
      author={Benoit Jacob and Skirmantas Kligys and Bo Chen and Menglong Zhu and Matthew Tang and Andrew Howard and Hartwig Adam and Dmitry Kalenichenko},
      year={2017},
      eprint={1712.05877},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{han2016deep,
      title={Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding}, 
      author={Song Han and Huizi Mao and William J. Dally},
      year={2016},
      eprint={1510.00149},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{muzio2024seermoe,
      title={SEER-MoE: Sparse Expert Efficiency through Regularization for Mixture-of-Experts}, 
      author={Alexandre Muzio and Alex Sun and Churan He},
      year={2024},
      eprint={2404.05089},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{lu2024experts,
      title={Not All Experts are Equal: Efficient Expert Pruning and Skipping for Mixture-of-Experts Large Language Models}, 
      author={Xudong Lu and Qi Liu and Yuhui Xu and Aojun Zhou and Siyuan Huang and Bo Zhang and Junchi Yan and Hongsheng Li},
      year={2024},
      eprint={2402.14800},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{vaswani2017attention,
    title={Attention is all you need},
    author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
    journal={Advances in neural information processing systems},
    volume={30},
    year={2017}
}




@misc{li2024lorap,
      title={LoRAP: Transformer Sub-Layers Deserve Differentiated Structured Compression for Large Language Models}, 
      author={Guangyan Li and Yongqiang Tang and Wensheng Zhang},
      year={2024},
      eprint={2404.09695},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{
    lee2021layeradaptive,
    title={Layer-adaptive Sparsity for the Magnitude-based Pruning},
    author={Jaeho Lee and Sejun Park and Sangwoo Mo and Sungsoo Ahn and Jinwoo Shin},
    booktitle={International Conference on Learning Representations},
    year={2021},
    url={https://openreview.net/forum?id=H6ATjJ0TKdf}
}

@inproceedings{
    bai2022dual,
    title={Dual Lottery Ticket Hypothesis},
    author={Yue Bai and Huan Wang and ZHIQIANG TAO and Kunpeng Li and Yun Fu},
    booktitle={International Conference on Learning Representations},
    year={2022},
    url={https://openreview.net/forum?id=fOsN52jn25l}
}



@misc{mod,
      title={Mixture-of-Depths: Dynamically allocating compute in transformer-based language models}, 
      author={David Raposo and Sam Ritter and Blake Richards and Timothy Lillicrap and Peter Conway Humphreys and Adam Santoro},
      year={2024},
      eprint={2404.02258},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{layerskip,
      title={LayerSkip: Enabling Early Exit Inference and Self-Speculative Decoding}, 
      author={Mostafa Elhoushi and Akshat Shrivastava and Diana Liskovich and Basil Hosmer and Bram Wasti and Liangzhen Lai and Anas Mahmoud and Bilge Acun and Saurabh Agarwal and Ahmed Roman and Ahmed A Aly and Beidi Chen and Carole-Jean Wu},
      year={2024},
      eprint={2404.16710},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{ribar2024sparq,
      title={SparQ Attention: Bandwidth-Efficient LLM Inference}, 
      author={Luka Ribar and Ivan Chelombiev and Luke Hudlass-Galley and Charlie Blake and Carlo Luschi and Douglas Orr},
      year={2024},
      eprint={2312.04985},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{zhang2023h2o,
      title={H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models}, 
      author={Zhenyu Zhang and Ying Sheng and Tianyi Zhou and Tianlong Chen and Lianmin Zheng and Ruisi Cai and Zhao Song and Yuandong Tian and Christopher Ré and Clark Barrett and Zhangyang Wang and Beidi Chen},
      year={2023},
      eprint={2306.14048},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{2019t5,
    author = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
    title = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
    journal = {arXiv e-prints},
    year = {2019},
    archivePrefix = {arXiv},
    eprint = {1910.10683},
}


@misc{geminiteam2024gemini,
      title={Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context}, 
      author={Gemini Team},
      year={2024},
      eprint={2403.05530},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{ramesh2021zeroshot,
      title={Zero-Shot Text-to-Image Generation}, 
      author={Aditya Ramesh and Mikhail Pavlov and Gabriel Goh and Scott Gray and Chelsea Voss and Alec Radford and Mark Chen and Ilya Sutskever},
      year={2021},
      eprint={2102.12092},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{du2023sida,
      title={SiDA: Sparsity-Inspired Data-Aware Serving for Efficient and Scalable Large Mixture-of-Experts Models}, 
      author={Zhixu Du and Shiyu Li and Yuhao Wu and Xiangyu Jiang and Jingwei Sun and Qilin Zheng and Yongkai Wu and Ang Li and Hai "Helen" Li and Yiran Chen},
      year={2023},
      eprint={2310.18859},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{kudugunta2021distillation,
      title={Beyond Distillation: Task-level Mixture-of-Experts for Efficient Inference}, 
      author={Sneha Kudugunta and Yanping Huang and Ankur Bapna and Maxim Krikun and Dmitry Lepikhin and Minh-Thang Luong and Orhan Firat},
      year={2021},
      eprint={2110.03742},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{cheng2020survey,
      title={A Survey of Model Compression and Acceleration for Deep Neural Networks}, 
      author={Yu Cheng and Duo Wang and Pan Zhou and Tao Zhang},
      year={2020},
      eprint={1710.09282},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{liang2021pruning,
      title={Pruning and Quantization for Deep Neural Network Acceleration: A Survey}, 
      author={Tailin Liang and John Glossner and Lei Wang and Shaobo Shi and Xiaotong Zhang},
      year={2021},
      eprint={2101.09671},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{zhu2017prune,
      title={To prune, or not to prune: exploring the efficacy of pruning for model compression}, 
      author={Michael Zhu and Suyog Gupta},
      year={2017},
      eprint={1710.01878},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{nagel2021white,
      title={A White Paper on Neural Network Quantization}, 
      author={Markus Nagel and Marios Fournarakis and Rana Ali Amjad and Yelysei Bondarenko and Mart van Baalen and Tijmen Blankevoort},
      year={2021},
      eprint={2106.08295},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@InProceedings{ai2:winogrande,
title = {WinoGrande: An Adversarial Winograd Schema Challenge at Scale},
authors={Keisuke, Sakaguchi and Ronan, Le Bras and Chandra, Bhagavatula and Yejin, Choi
},
year={2019}
}

@inproceedings{wang2019glue,
  title={{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},
  author={Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R.},
  note={In the Proceedings of ICLR.},
  year={2019}
}

@misc{mihaylov2018suit,
      title={Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering}, 
      author={Todor Mihaylov and Peter Clark and Tushar Khot and Ashish Sabharwal},
      year={2018},
      eprint={1809.02789},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{hendrycks2021measuring,
      title={Measuring Massive Multitask Language Understanding}, 
      author={Dan Hendrycks and Collin Burns and Steven Basart and Andy Zou and Mantas Mazeika and Dawn Song and Jacob Steinhardt},
      year={2021},
      eprint={2009.03300},
      archivePrefix={arXiv},
      primaryClass={cs.CY}
}

@misc{zellers2019hellaswag,
      title={HellaSwag: Can a Machine Really Finish Your Sentence?}, 
      author={Rowan Zellers and Ari Holtzman and Yonatan Bisk and Ali Farhadi and Yejin Choi},
      year={2019},
      eprint={1905.07830},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{clark2018think,
      title={Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge}, 
      author={Peter Clark and Isaac Cowhey and Oren Etzioni and Tushar Khot and Ashish Sabharwal and Carissa Schoenick and Oyvind Tafjord},
      year={2018},
      eprint={1803.05457},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@misc{clark2019boolq,
      title={BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions}, 
      author={Christopher Clark and Kenton Lee and Ming-Wei Chang and Tom Kwiatkowski and Michael Collins and Kristina Toutanova},
      year={2019},
      eprint={1905.10044},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{bisk2019piqa,
      title={PIQA: Reasoning about Physical Commonsense in Natural Language}, 
      author={Yonatan Bisk and Rowan Zellers and Ronan Le Bras and Jianfeng Gao and Yejin Choi},
      year={2019},
      eprint={1911.11641},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{men2024shortgpt,
      title={ShortGPT: Layers in Large Language Models are More Redundant Than You Expect}, 
      author={Xin Men and Mingyu Xu and Qingyu Zhang and Bingning Wang and Hongyu Lin and Yaojie Lu and Xianpei Han and Weipeng Chen},
      year={2024},
      eprint={2403.03853},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{dettmers2023qlora,
      title={QLoRA: Efficient Finetuning of Quantized LLMs}, 
      author={Tim Dettmers and Artidoro Pagnoni and Ari Holtzman and Luke Zettlemoyer},
      year={2023},
      eprint={2305.14314},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{gao2020pile,
      title={The Pile: An 800GB Dataset of Diverse Text for Language Modeling}, 
      author={Leo Gao and Stella Biderman and Sid Black and Laurence Golding and Travis Hoppe and Charles Foster and Jason Phang and Horace He and Anish Thite and Noa Nabeshima and Shawn Presser and Connor Leahy},
      year={2020},
      eprint={2101.00027},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{zhou2023lima,
      title={LIMA: Less Is More for Alignment}, 
      author={Chunting Zhou and Pengfei Liu and Puxin Xu and Srini Iyer and Jiao Sun and Yuning Mao and Xuezhe Ma and Avia Efrat and Ping Yu and Lili Yu and Susan Zhang and Gargi Ghosh and Mike Lewis and Luke Zettlemoyer and Omer Levy},
      year={2023},
      eprint={2305.11206},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{yu2023metamath,
  title={MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models},
  author={Yu, Longhui and Jiang, Weisen and Shi, Han and Yu, Jincheng and Liu, Zhengying and Zhang, Yu and Kwok, James T and Li, Zhenguo and Weller, Adrian and Liu, Weiyang},
  journal={arXiv preprint arXiv:2309.12284},
  year={2023}
}

@misc{alpaca,
  author = {Rohan Taori and Ishaan Gulrajani and Tianyi Zhang and Yann Dubois and Xuechen Li and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto },
  title = {Stanford Alpaca: An Instruction-following LLaMA model},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/tatsu-lab/stanford_alpaca}},
}

@misc{he2015deep,
      title={Deep Residual Learning for Image Recognition}, 
      author={Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
      year={2015},
      eprint={1512.03385},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{ba2016layer,
      title={Layer Normalization}, 
      author={Jimmy Lei Ba and Jamie Ryan Kiros and Geoffrey E. Hinton},
      year={2016},
      eprint={1607.06450},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@article{liu2023diversifying,
  title={Diversifying the mixture-of-experts representation for language models with orthogonal optimizer},
  author={Liu, Boan and Ding, Liang and Shen, Li and Peng, Keqin and Cao, Yu and Cheng, Dazhao and Tao, Dacheng},
  journal={arXiv preprint arXiv:2310.09762},
  year={2023}
}

@inproceedings{zhu2023zero,
  title={Zero-shot Sharpness-Aware Quantization for Pre-trained Language Models},
  author={Zhu, Miaoxi and Zhong, Qihuang and Shen, Li and Ding, Liang and Liu, Juhua and Du, Bo and Tao, Dacheng},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  pages={11305--11327},
  year={2023}
}

@article{chen2024db,
  title={DB-LLM: Accurate Dual-Binarization for Efficient LLMs},
  author={Chen, Hong and Lv, Chengtao and Ding, Liang and Qin, Haotong and Zhou, Xiabin and Ding, Yifu and Liu, Xuebo and Zhang, Min and Guo, Jinyang and Liu, Xianglong and others},
  journal={arXiv preprint arXiv:2402.11960},
  year={2024}
}

@inproceedings{he2022sparseadapter,
  title={SparseAdapter: An Easy Approach for Improving the Parameter-Efficiency of Adapters},
  author={He, Shwai and Ding, Liang and Dong, Daize and Zhang, Jeremy and Tao, Dacheng},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2022},
  pages={2184--2190},
  year={2022}
}

@inproceedings{he2023merging,
  title={Merging Experts into One: Improving Computational Efficiency of Mixture of Experts},
  author={He, Shwai and Fan, Run-Ze and Ding, Liang and Shen, Li and Zhou, Tianyi and Tao, Dacheng},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  pages={14685--14691},
  year={2023}
}


@inproceedings{he2023sd,
  title={Sd-conv: Towards the parameter-efficiency of dynamic convolution},
  author={He, Shwai and Jiang, Chenbo and Dong, Daize and Ding, Liang},
  booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
  pages={6454--6463},
  year={2023}
}




@article{zhang2024diversifying,
  title={Diversifying the Expert Knowledge for Task-Agnostic Pruning in Sparse Mixture-of-Experts},
  author={Zhang, Zeliang and Liu, Xiaodong and Cheng, Hao and Xu, Chenliang and Gao, Jianfeng},
  journal={arXiv preprint arXiv:2407.09590},
  year={2024}
}

@article{li2024examining,
  title={Examining Post-Training Quantization for Mixture-of-Experts: A Benchmark},
  author={Li, Pingzhi and Jin, Xiaolong and Cheng, Yu and Chen, Tianlong},
  journal={arXiv preprint arXiv:2406.08155},
  year={2024}
}



@article{xue2024moe,
  title={Moe-infinity: Activation-aware expert offloading for efficient moe serving},
  author={Xue, Leyang and Fu, Yao and Lu, Zhan and Mai, Luo and Marina, Mahesh},
  journal={arXiv preprint arXiv:2401.14361},
  year={2024}
}

@article{cai2024shortcut,
  title={Shortcut-connected Expert Parallelism for Accelerating Mixture-of-Experts},
  author={Cai, Weilin and Jiang, Juyong and Qin, Le and Cui, Junwei and Kim, Sunghun and Huang, Jiayi},
  journal={arXiv preprint arXiv:2404.05019},
  year={2024}
}

@article{liu2024me,
  title={ME-Switch: A Memory-Efficient Expert Switching Framework for Large Language Models},
  author={Liu, Jing and Gong, Ruihao and Zhang, Mingyang and He, Yefei and Cai, Jianfei and Zhuang, Bohan},
  journal={arXiv preprint arXiv:2406.09041},
  year={2024}
}


@article{wang2024svd,
  title={Svd-llm: Truncation-aware singular value decomposition for large language model compression},
  author={Wang, Xin and Zheng, Yu and Wan, Zhongwei and Zhang, Mi},
  journal={arXiv preprint arXiv:2403.07378},
  year={2024}
}

@article{yin2023outlier,
  title={Outlier weighed layerwise sparsity (owl): A missing secret sauce for pruning llms to high sparsity},
  author={Yin, Lu and Wu, You and Zhang, Zhenyu and Hsieh, Cheng-Yu and Wang, Yaqing and Jia, Yiling and Pechenizkiy, Mykola and Liang, Yi and Wang, Zhangyang and Liu, Shiwei},
  journal={arXiv preprint arXiv:2310.05175},
  year={2023}
}


@article{frantar-gptq,
  title={{GPTQ}: Accurate Post-training Compression for Generative Pretrained Transformers}, 
  author={Elias Frantar and Saleh Ashkboos and Torsten Hoefler and Dan Alistarh},
  year={2022},
  journal={arXiv preprint arXiv:2210.17323}
}

@article{cai2024survey,
  title={A survey on mixture of experts},
  author={Cai, Weilin and Jiang, Juyong and Wang, Fan and Tang, Jing and Kim, Sunghun and Huang, Jiayi},
  journal={arXiv preprint arXiv:2407.06204},
  year={2024}
}
@article{ren2023pangu,
  title={{Pangu-{$\Sigma$}: Towards trillion parameter language model with sparse heterogeneous computing}},
  author={Ren, Xiaozhe and Zhou, Pingyi and Meng, Xinfan and Huang, Xinjing and Wang, Yadao and Wang, Weichao and Li, Pengfei and Zhang, Xiaoda and Podolskiy, Alexander and Arshinov, Grigory and others},
  journal={arXiv preprint arXiv:2303.10845},
  year={2023}
}
@inproceedings{chen2023lifelong,
  title={{Lifelong language pretraining with distribution-specialized experts}},
  author={Chen, Wuyang and Zhou, Yanqi and Du, Nan and Huang, Yanping and Laudon, James and Chen, Zhifeng and Cui, Claire},
  booktitle={International Conference on Machine Learning},
  pages={5383--5395},
  year={2023},
  organization={PMLR}
}
@article{antoniak2023mixture,
  title={{Mixture of Tokens: Efficient LLMs through Cross-Example Aggregation}},
  author={Antoniak, Szymon and Jaszczur, Sebastian and Krutul, Micha{\l} and Pi{\'o}ro, Maciej and Krajewski, Jakub and Ludziejewski, Jan and Odrzyg{\'o}{\'z}d{\'z}, Tomasz and Cygan, Marek},
  journal={arXiv preprint arXiv:2310.15961},
  year={2023}
}
@inproceedings{
zhang2024llamaadapter,
title={{{LL}a{MA}-Adapter: Efficient Fine-tuning of Large Language Models with Zero-initialized Attention}},
author={Renrui Zhang and Jiaming Han and Chris Liu and Aojun Zhou and Pan Lu and Hongsheng Li and Peng Gao and Yu Qiao},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=d4UiXAHN2W}
}
@inproceedings{wang-etal-2022-adamix,
    title = {{{A}da{M}ix: Mixture-of-Adaptations for Parameter-efficient Model Tuning}},
    author = "Wang, Yaqing  and
      Agarwal, Sahaj  and
      Mukherjee, Subhabrata  and
      Liu, Xiaodong  and
      Gao, Jing  and
      Awadallah, Ahmed Hassan  and
      Gao, Jianfeng",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.388",
    doi = "10.18653/v1/2022.emnlp-main.388",
    pages = "5744--5760"
}
@inproceedings{ma2018modeling,
  title={{Modeling task relationships in multi-task learning with multi-gate mixture-of-experts}},
  author={Ma, Jiaqi and Zhao, Zhe and Yi, Xinyang and Chen, Jilin and Hong, Lichan and Chi, Ed H},
  booktitle={Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery \& data mining},
  pages={1930--1939},
  year={2018}
}
@inproceedings{wei2021finetuned,
  title={{Finetuned Language Models are Zero-Shot Learners}},
  author={Wei, Jason and Bosma, Maarten and Zhao, Vincent and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M and Le, Quoc V},
  booktitle={International Conference on Learning Representations},
  year={2021}
}
@inproceedings{xiao2021ernie,
  title={{ERNIE-GEN: an enhanced multi-flow pre-training and fine-tuning framework for natural language generation}},
  author={Xiao, Dongling and Zhang, Han and Li, Yukun and Sun, Yu and Tian, Hao and Wu, Hua and Wang, Haifeng},
  booktitle={Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence},
  pages={3997--4003},
  year={2021}
}
@article{shuster2022blenderbot,
  title={{Blenderbot 3: a deployed conversational agent that continually learns to responsibly engage}},
  author={Shuster, Kurt and Xu, Jing and Komeili, Mojtaba and Ju, Da and Smith, Eric Michael and Roller, Stephen and Ung, Megan and Chen, Moya and Arora, Kushal and Lane, Joshua and others},
  journal={arXiv preprint arXiv:2208.03188},
  year={2022}
}
@article{lialin2023scaling,
  title={{Scaling down to scale up: A guide to parameter-efficient fine-tuning}},
  author={Lialin, Vladislav and Deshpande, Vijeta and Rumshisky, Anna},
  journal={arXiv preprint arXiv:2303.15647},
  year={2023}
}
@inproceedings{ma2018entire,
  title={{Entire space multi-task model: An effective approach for estimating post-click conversion rate}},
  author={Ma, Xiao and Zhao, Liqin and Huang, Guan and Wang, Zhi and Hu, Zelin and Zhu, Xiaoqiang and Gai, Kun},
  booktitle={The 41st International ACM SIGIR Conference on Research \& Development in Information Retrieval},
  pages={1137--1140},
  year={2018}
}
@inproceedings{tang2020progressive,
  title={{Progressive layered extraction (ple): A novel multi-task learning (mtl) model for personalized recommendations}},
  author={Tang, Hongyan and Liu, Junning and Zhao, Ming and Gong, Xudong},
  booktitle={Proceedings of the 14th ACM Conference on Recommender Systems},
  pages={269--278},
  year={2020}
}
@article{raposo2024mixture,
  title={{Mixture-of-Depths: Dynamically allocating compute in transformer-based language models}},
  author={Raposo, David and Ritter, Sam and Richards, Blake and Lillicrap, Timothy and Humphreys, Peter Conway and Santoro, Adam},
  journal={arXiv preprint arXiv:2404.02258},
  year={2024}
}
@inproceedings{hu2021lora,
  title={{LoRA: Low-Rank Adaptation of Large Language Models}},
  author={Hu, Edward J and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu and others},
  booktitle={International Conference on Learning Representations},
  year={2021}
}
@inproceedings{houlsby2019parameter,
  title={{Parameter-efficient transfer learning for NLP}},
  author={Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and De Laroussilhe, Quentin and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain},
  booktitle={International Conference on Machine Learning},
  pages={2790--2799},
  year={2019},
  organization={PMLR}
}
@article{zhang2023mosa,
  title={{MoSA: Mixture of Sparse Adapters for Visual Efficient Tuning}},
  author={Zhang, Qizhe and Zou, Bocheng and An, Ruichuan and Liu, Jiaming and Zhang, Shanghang},
  journal={arXiv preprint arXiv:2312.02923},
  year={2023}
}
@inproceedings{chen-etal-2022-revisiting,
    title = {{Revisiting Parameter-Efficient Tuning: Are We Really There Yet?}},
    author = "Chen, Guanzheng  and
      Liu, Fangyu  and
      Meng, Zaiqiao  and
      Liang, Shangsong",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.168",
    doi = "10.18653/v1/2022.emnlp-main.168",
    pages = "2612--2626",
    abstract = "Parameter-Efficient Tuning (PETuning) methods have been deemed by many as the new paradigm for using pretrained language models (PLMs). By tuning just a fraction amount of parameters comparing to full model finetuning, PETuning methods claim to have achieved performance on par with or even better than finetuning. In this work, we take a step back and re-examine these PETuning methods by conducting the first comprehensive investigation into the training and evaluation of them. We found the problematic validation and testing practice in current studies, when accompanied by the instability nature of PETuning methods, has led to unreliable conclusions. When being compared under a truly fair evaluation protocol, PETuning cannot yield consistently competitive performance while finetuning remains to be the best-performing method in medium- and high-resource settings. We delve deeper into the cause of the instability and observed that the number of trainable parameters and training iterations are two main factors: reducing trainable parameters and prolonging training iterations may lead to higher stability in PETuning methods.",
}
@article{huang2023lorahub,
  title={{Lorahub: Efficient cross-task generalization via dynamic lora composition}},
  author={Huang, Chengsong and Liu, Qian and Lin, Bill Yuchen and Pang, Tianyu and Du, Chao and Lin, Min},
  journal={arXiv preprint arXiv:2307.13269},
  year={2023}
}
@article{luo2024moelora,
  title={{Moelora: Contrastive learning guided mixture of experts on parameter-efficient fine-tuning for large language models}},
  author={Luo, Tongxu and Lei, Jiahe and Lei, Fangyu and Liu, Weihao and He, Shizhu and Zhao, Jun and Liu, Kang},
  journal={arXiv preprint arXiv:2402.12851},
  year={2024}
}
@article{gao2022parameter,
  title={{Parameter-efficient mixture-of-experts architecture for pre-trained language models}},
  author={Gao, Ze-Feng and Liu, Peiyu and Zhao, Wayne Xin and Lu, Zhong-Yi and Wen, Ji-Rong},
  journal={arXiv preprint arXiv:2203.01104},
  year={2022}
}
@article{gao2024higher,
  title={{Higher Layers Need More LoRA Experts}},
  author={Gao, Chongyang and Chen, Kezhen and Rao, Jinmeng and Sun, Baochen and Liu, Ruibo and Peng, Daiyi and Zhang, Yawen and Guo, Xiaoyuan and Yang, Jie and Subrahmanian, VS},
  journal={arXiv preprint arXiv:2402.08562},
  year={2024}
}
@article{eigen2013learning,
  title={{Learning factored representations in a deep mixture of experts}},
  author={Eigen, David and Ranzato, Marc'Aurelio and Sutskever, Ilya},
  journal={arXiv preprint arXiv:1312.4314},
  year={2013}
}
@article{liu2022few,
  title={{Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning}},
  author={Liu, Haokun and Tam, Derek and Muqeeth, Mohammed and Mohta, Jay and Huang, Tenghao and Bansal, Mohit and Raffel, Colin A},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={1950--1965},
  year={2022}
}
@article{riquelme2021scaling,
  title={{Scaling vision with sparse mixture of experts}},
  author={Riquelme, Carlos and Puigcerver, Joan and Mustafa, Basil and Neumann, Maxim and Jenatton, Rodolphe and Susano Pinto, Andr{\'e} and Keysers, Daniel and Houlsby, Neil},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={8583--8595},
  year={2021}
}

@article{chowdhery2023palm,
  title={{Palm: Scaling language modeling with pathways}},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={Journal of Machine Learning Research},
  volume={24},
  number={240},
  pages={1--113},
  year={2023}
}
@article{shen2023mixture,
  title={{Mixture-of-experts meets instruction tuning: A winning combination for large language models}},
  author={Shen, Sheng and Hou, Le and Zhou, Yanqi and Du, Nan and Longpre, Shayne and Wei, Jason and Chung, Hyung Won and Zoph, Barret and Fedus, William and Chen, Xinyun and others},
  journal={arXiv preprint arXiv:2305.14705},
  year={2023}
}
@article{fedus2022switch,
  title={{Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity}},
  author={Fedus, William and Zoph, Barret and Shazeer, Noam},
  journal={Journal of Machine Learning Research},
  volume={23},
  number={120},
  pages={1--39},
  year={2022}
}

@misc{qwen1.5,
    title = {{Introducing Qwen1.5}},
    url = {https://qwenlm.github.io/blog/qwen1.5/},
    author = {Qwen Team},
    month = {February},
    year = {2024}
}
@article{mckinzie2024mm1,
  title={{Mm1: Methods, analysis \& insights from multimodal llm pre-training}},
  author={McKinzie, Brandon and Gan, Zhe and Fauconnier, Jean-Philippe and Dodge, Sam and Zhang, Bowen and Dufter, Philipp and Shah, Dhruti and Du, Xianzhi and Peng, Futang and Weers, Floris and others},
  journal={arXiv preprint arXiv:2403.09611},
  year={2024}
}
@misc{Grok-1,
    title = {{Grok-1}},
    url = {https://github.com/xai-org/grok-1},
    author = {xAI},
    month = {March},
    year = {2024}
}

@article{dai2024deepseekmoe,
  title={{Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models}},
  author={Dai, Damai and Deng, Chengqi and Zhao, Chenggang and Xu, RX and Gao, Huazuo and Chen, Deli and Li, Jiashi and Zeng, Wangding and Yu, Xingkai and Wu, Y and others},
  journal={arXiv preprint arXiv:2401.06066},
  year={2024}
}
@misc{qwen_moe,
    title = {{Qwen1.5-MoE: Matching 7B Model Performance with 1/3 Activated Parameters"}},
    url = {https://qwenlm.github.io/blog/qwen-moe/},
    author = {Qwen Team},
    month = {February},
    year = {2024}
}
@article{bi2024deepseek,
  title={{Deepseek llm: Scaling open-source language models with longtermism}},
  author={Bi, Xiao and Chen, Deli and Chen, Guanting and Chen, Shanhuang and Dai, Damai and Deng, Chengqi and Ding, Honghui and Dong, Kai and Du, Qiushi and Fu, Zhe and others},
  journal={arXiv preprint arXiv:2401.02954},
  year={2024}
}
@article{jiang2023mistral,
  title={{Mistral 7B}},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
  journal={arXiv preprint arXiv:2310.06825},
  year={2023}
}

@misc{gpt-3.5-turbo,
  author = {OpenAI},
  title = {{Chatgpt: Optimizing language models for dialogue}},
  year = {2022},
  publisher = {OpenAI},
  journal = {OpenAI Blog},
  howpublished = {\url{https://openai.com/blog/chatgpt}}
}
@article{touvron2023llama,
  title={{Llama 2: Open foundation and fine-tuned chat models}},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@article{jiang2024mixtral,
  title={{Mixtral of experts}},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Roux, Antoine and Mensch, Arthur and Savary, Blanche and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Hanna, Emma Bou and Bressand, Florian and others},
  journal={arXiv preprint arXiv:2401.04088},
  year={2024}
}
@article{fedus2022review,
  title={{A review of sparse expert models in deep learning}},
  author={Fedus, William and Dean, Jeff and Zoph, Barret},
  journal={arXiv preprint arXiv:2209.01667},
  year={2022}
}

@article{kirkpatrick2017overcoming,
  title={{Overcoming catastrophic forgetting in neural networks}},
  author={Kirkpatrick, James and Pascanu, Razvan and Rabinowitz, Neil and Veness, Joel and Desjardins, Guillaume and Rusu, Andrei A and Milan, Kieran and Quan, John and Ramalho, Tiago and Grabska-Barwinska, Agnieszka and others},
  journal={Proceedings of the national academy of sciences},
  volume={114},
  number={13},
  pages={3521--3526},
  year={2017},
  publisher={National Acad Sciences}
}
@article{zheng2022survey,
  title={{A survey of recommender systems with multi-objective optimization}},
  author={Zheng, Yong and Wang, David Xuejun},
  journal={Neurocomputing},
  volume={474},
  pages={141--153},
  year={2022},
  publisher={Elsevier}
}
@inproceedings{zhou2020unified,
  title={{Unified vision-language pre-training for image captioning and vqa}},
  author={Zhou, Luowei and Palangi, Hamid and Zhang, Lei and Hu, Houdong and Corso, Jason and Gao, Jianfeng},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={34},
  number={07},
  pages={13041--13049},
  year={2020}
}
@article{uppal2022multimodal,
  title={{Multimodal research in vision and language: A review of current and emerging trends}},
  author={Uppal, Shagun and Bhagat, Sarthak and Hazarika, Devamanyu and Majumder, Navonil and Poria, Soujanya and Zimmermann, Roger and Zadeh, Amir},
  journal={Information Fusion},
  volume={77},
  pages={149--171},
  year={2022},
  publisher={Elsevier}
}
@article{baltruvsaitis2018multimodal,
  title={{Multimodal machine learning: A survey and taxonomy}},
  author={Baltru{\v{s}}aitis, Tadas and Ahuja, Chaitanya and Morency, Louis-Philippe},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={41},
  number={2},
  pages={423--443},
  year={2018},
  publisher={IEEE}
}
@inproceedings{ngiam2011multimodal,
  title={{Multimodal deep learning}},
  author={Ngiam, Jiquan and Khosla, Aditya and Kim, Mingyu and Nam, Juhan and Lee, Honglak and Ng, Andrew Y},
  booktitle={Proceedings of the 28th international conference on machine learning (ICML-11)},
  pages={689--696},
  year={2011}
}
@article{dosovitskiy2020image,
  title={{An image is worth 16x16 words: Transformers for image recognition at scale}},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}
@article{zoph2022st,
  title={{St-moe: Designing stable and transferable sparse expert models}},
  author={Zoph, Barret and Bello, Irwan and Kumar, Sameer and Du, Nan and Huang, Yanping and Dean, Jeff and Shazeer, Noam and Fedus, William},
  journal={arXiv preprint arXiv:2202.08906},
  year={2022}
}
@article{zhou2022mixture,
  title={{Mixture-of-experts with expert choice routing}},
  author={Zhou, Yanqi and Lei, Tao and Liu, Hanxiao and Du, Nan and Huang, Yanping and Zhao, Vincent and Dai, Andrew M and Le, Quoc V and Laudon, James and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={7103--7114},
  year={2022}
}
@article{dou2023art,
  title={{The Art of Balancing: Revolutionizing Mixture of Experts for Maintaining World Knowledge in Language Model Alignment}},
  author={Dou, Shihan and Zhou, Enyu and Liu, Yan and Gao, Songyang and Zhao, Jun and Shen, Wei and Zhou, Yuhao and Xi, Zhiheng and Wang, Xiao and Fan, Xiaoran and others},
  journal={arXiv preprint arXiv:2312.09979},
  year={2023}
}
@article{jiang2024survey,
  title={{A Survey on Large Language Models for Code Generation}},
  author={Jiang, Juyong and Wang, Fan and Shen, Jiasi and Kim, Sungju and Kim, Sunghun},
  journal={arXiv preprint arXiv:2406.00515},
  year={2024}
}
@article{yoo2024hyperclova,
  title={{HyperCLOVA X Technical Report}},
  author={Yoo, Kang Min and Han, Jaegeun and In, Sookyo and Jeon, Heewon and Jeong, Jisu and Kang, Jaewook and Kim, Hyunwook and Kim, Kyung-Min and Kim, Munhyong and Kim, Sungju and others},
  journal={arXiv preprint arXiv:2404.01954},
  year={2024}
}
@article{kaplan2020scaling,
  title={{Scaling laws for neural language models}},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}
@article{hoffmann2022training,
  title={{Training compute-optimal large language models}},
  author={Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others},
  journal={arXiv preprint arXiv:2203.15556},
  year={2022}
}
@article{zadouri2023pushing,
  title={{Pushing mixture of experts to the limit: Extremely parameter efficient moe for instruction tuning}},
  author={Zadouri, Ted and {\"U}st{\"u}n, Ahmet and Ahmadian, Arash and Ermi{\c{s}}, Beyza and Locatelli, Acyr and Hooker, Sara},
  journal={arXiv preprint arXiv:2309.05444},
  year={2023}
}
@article{dou2023loramoe,
  title={{Loramoe: Revolutionizing mixture of experts for maintaining world knowledge in language model alignment}},
  author={Dou, Shihan and Zhou, Enyu and Liu, Yan and Gao, Songyang and Zhao, Jun and Shen, Wei and Zhou, Yuhao and Xi, Zhiheng and Wang, Xiao and Fan, Xiaoran and others},
  journal={arXiv preprint arXiv:2312.09979},
  year={2023}
}
@article{xue2022one,
  title={{One student knows all experts know: From sparse to dense}},
  author={Xue, Fuzhao and He, Xiaoxin and Ren, Xiaozhe and Lou, Yuxuan and You, Yang},
  journal={arXiv preprint arXiv:2201.10890},
  year={2022}
}
@article{chen2024llava,
  title={{Llava-mole: Sparse mixture of lora experts for mitigating data conflicts in instruction finetuning mllms}},
  author={Chen, Shaoxiang and Jie, Zequn and Ma, Lin},
  journal={arXiv preprint arXiv:2401.16160},
  year={2024}
}
@article{liu2023moelora,
  title={{Moelora: An moe-based parameter efficient fine-tuning method for multi-task medical applications}},
  author={Liu, Qidong and Wu, Xian and Zhao, Xiangyu and Zhu, Yuanshao and Xu, Derong and Tian, Feng and Zheng, Yefeng},
  journal={arXiv preprint arXiv:2310.18339},
  year={2023}
}
@inproceedings{aghajanyan2021intrinsic,
  title={{Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning}},
  author={Aghajanyan, Armen and Gupta, Sonal and Zettlemoyer, Luke},
  booktitle={Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
  pages={7319--7328},
  year={2021}
}
@article{zhu2022uni,
  title={{Uni-perceiver-moe: Learning sparse generalist models with conditional moes}},
  author={Zhu, Jinguo and Zhu, Xizhou and Wang, Wenhai and Wang, Xiaohua and Li, Hongsheng and Wang, Xiaogang and Dai, Jifeng},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={2664--2678},
  year={2022}
}
@inproceedings{liu2021swin,
  title={{Swin transformer: Hierarchical vision transformer using shifted windows}},
  author={Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={10012--10022},
  year={2021}
}

@inproceedings{jiang2023adamct,
  title={{AdaMCT: adaptive mixture of CNN-transformer for sequential recommendation}},
  author={Jiang, Juyong and Zhang, Peiyan and Luo, Yingtao and Li, Chaozhuo and Kim, Jae Boum and Zhang, Kai and Wang, Senzhang and Xie, Xing and Kim, Sunghun},
  booktitle={Proceedings of the 32nd ACM International Conference on Information and Knowledge Management},
  pages={976--986},
  year={2023}
}

@article{li2024uni,
  title={{Uni-MoE: Scaling Unified Multimodal LLMs with Mixture of Experts}},
  author={Li, Yunxin and Jiang, Shenyuan and Hu, Baotian and Wang, Longyue and Zhong, Wanqi and Luo, Wenhan and Ma, Lin and Zhang, Min},
  journal={arXiv preprint arXiv:2405.11273},
  year={2024}
}

@article{lin2024moe,
  title={{Moe-llava: Mixture of experts for large vision-language models}},
  author={Lin, Bin and Tang, Zhenyu and Ye, Yang and Cui, Jiaxi and Zhu, Bin and Jin, Peng and Zhang, Junwu and Ning, Munan and Yuan, Li},
  journal={arXiv preprint arXiv:2401.15947},
  year={2024}
}

@article{he2021fastmoe,
  title={{Fastmoe: A fast mixture-of-expert training system}},
  author={He, Jiaao and Qiu, Jiezhong and Zeng, Aohan and Yang, Zhilin and Zhai, Jidong and Tang, Jie},
  journal={arXiv preprint arXiv:2103.13262},
  year={2021}
}

@article{hwang2023tutel,
  title={{Tutel: Adaptive mixture-of-experts at scale}},
  author={Hwang, Changho and Cui, Wei and Xiong, Yifan and Yang, Ziyue and Liu, Ze and Hu, Han and Wang, Zilong and Salas, Rafael and Jose, Jithin and Ram, Prabhat and others},
  journal={Proceedings of Machine Learning and Systems},
  volume={5},
  year={2023}
}

@article{gale2023megablocks,
  title={{Megablocks: Efficient sparse training with mixture-of-experts}},
  author={Gale, Trevor and Narayanan, Deepak and Young, Cliff and Zaharia, Matei},
  journal={Proceedings of Machine Learning and Systems},
  volume={5},
  year={2023}
}

@inproceedings{zhai2023smartmoe,
  title={{$\{$SmartMoE$\}$: Efficiently Training $\{$Sparsely-Activated$\}$ Models through Combining Offline and Online Parallelization}},
  author={Zhai, Mingshu and He, Jiaao and Ma, Zixuan and Zong, Zan and Zhang, Runqing and Zhai, Jidong},
  booktitle={2023 USENIX Annual Technical Conference (USENIX ATC 23)},
  pages={961--975},
  year={2023}
}

@article{lepikhin2020gshard,
  title={{Gshard: Scaling giant models with conditional computation and automatic sharding}},
  author={Lepikhin, Dmitry and Lee, HyoukJoong and Xu, Yuanzhong and Chen, Dehao and Firat, Orhan and Huang, Yanping and Krikun, Maxim and Shazeer, Noam and Chen, Zhifeng},
  journal={arXiv preprint arXiv:2006.16668},
  year={2020}
}

@inproceedings{singh2023hybrid,
  title={{A Hybrid Tensor-Expert-Data Parallelism Approach to Optimize Mixture-of-Experts Training}},
  author={Singh, Siddharth and Ruwase, Olatunji and Awan, Ammar Ahmad and Rajbhandari, Samyam and He, Yuxiong and Bhatele, Abhinav},
  booktitle={Proceedings of the 37th International Conference on Supercomputing},
  pages={203--214},
  year={2023}
}

@inproceedings{zheng2022alpa,
  title={{Alpa: Automating inter-and $\{$Intra-Operator$\}$ parallelism for distributed deep learning}},
  author={Zheng, Lianmin and Li, Zhuohan and Zhang, Hao and Zhuang, Yonghao and Chen, Zhifeng and Huang, Yanping and Wang, Yida and Xu, Yuanzhong and Zhuo, Danyang and Xing, Eric P and others},
  booktitle={16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22)},
  pages={559--578},
  year={2022}
}

@article{shoeybi2019megatron,
  title={{Megatron-lm: Training multi-billion parameter language models using model parallelism}},
  author={Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan},
  journal={arXiv preprint arXiv:1909.08053},
  year={2019}
}

@inproceedings{narayanan2021efficient,
  title={{Efficient large-scale language model training on gpu clusters using megatron-lm}},
  author={Narayanan, Deepak and Shoeybi, Mohammad and Casper, Jared and LeGresley, Patrick and Patwary, Mostofa and Korthikanti, Vijay and Vainbrand, Dmitri and Kashinkunti, Prethvi and Bernauer, Julie and Catanzaro, Bryan and others},
  booktitle={Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
  pages={1--15},
  year={2021}
}

@article{smith2022using,
  title={{Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model}},
  author={Smith, Shaden and Patwary, Mostofa and Norick, Brandon and LeGresley, Patrick and Rajbhandari, Samyam and Casper, Jared and Liu, Zhun and Prabhumoye, Shrimai and Zerveas, George and Korthikanti, Vijay and others},
  journal={arXiv preprint arXiv:2201.11990},
  year={2022}
}

@inproceedings{rajbhandari2020zero,
  title={{Zero: Memory optimizations toward training trillion parameter models}},
  author={Rajbhandari, Samyam and Rasley, Jeff and Ruwase, Olatunji and He, Yuxiong},
  booktitle={SC20: International Conference for High Performance Computing, Networking, Storage and Analysis},
  pages={1--16},
  year={2020},
  organization={IEEE}
}

@article{li2021sequence,
  title={{Sequence parallelism: Long sequence training from system perspective}},
  author={Li, Shenggui and Xue, Fuzhao and Baranwal, Chaitanya and Li, Yongbin and You, Yang},
  journal={arXiv preprint arXiv:2105.13120},
  year={2021}
}

@article{korthikanti2023reducing,
  title={{Reducing activation recomputation in large transformer models}},
  author={Korthikanti, Vijay Anand and Casper, Jared and Lym, Sangkug and McAfee, Lawrence and Andersch, Michael and Shoeybi, Mohammad and Catanzaro, Bryan},
  journal={Proceedings of Machine Learning and Systems},
  volume={5},
  year={2023}
}

@article{jacobs2023deepspeed,
  title={{Deepspeed ulysses: System optimizations for enabling training of extreme long sequence transformer models}},
  author={Jacobs, Sam Ade and Tanaka, Masahiro and Zhang, Chengming and Zhang, Minjia and Song, Leon and Rajbhandari, Samyam and He, Yuxiong},
  journal={arXiv preprint arXiv:2309.14509},
  year={2023}
}


@inproceedings{narayanan2019pipedream,
  title={{PipeDream: generalized pipeline parallelism for DNN training}},
  author={Narayanan, Deepak and Harlap, Aaron and Phanishayee, Amar and Seshadri, Vivek and Devanur, Nikhil R and Ganger, Gregory R and Gibbons, Phillip B and Zaharia, Matei},
  booktitle={Proceedings of the 27th ACM symposium on operating systems principles},
  pages={1--15},
  year={2019}
}

@article{huang2019gpipe,
  title={{Gpipe: Efficient training of giant neural networks using pipeline parallelism}},
  author={Huang, Yanping and Cheng, Youlong and Bapna, Ankur and Firat, Orhan and Chen, Dehao and Chen, Mia and Lee, HyoukJoong and Ngiam, Jiquan and Le, Quoc V and Wu, Yonghui and others},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}


@inproceedings{rajbhandari2021zero,
  title={{Zero-infinity: Breaking the gpu memory wall for extreme scale deep learning}},
  author={Rajbhandari, Samyam and Ruwase, Olatunji and Rasley, Jeff and Smith, Shaden and He, Yuxiong},
  booktitle={Proceedings of the international conference for high performance computing, networking, storage and analysis},
  pages={1--14},
  year={2021}
}

@inproceedings{qi2023zero,
  title={{Zero Bubble Pipeline Parallelism}},
  author={Qi, Penghui and Wan, Xinyi and Huang, Guangxing and Lin, Min},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2023}
}

@inproceedings{ren2021zero,
  title={{$\{$Zero-offload$\}$: Democratizing $\{$billion-scale$\}$ model training}},
  author={Ren, Jie and Rajbhandari, Samyam and Aminabadi, Reza Yazdani and Ruwase, Olatunji and Yang, Shuangyan and Zhang, Minjia and Li, Dong and He, Yuxiong},
  booktitle={2021 USENIX Annual Technical Conference (USENIX ATC 21)},
  pages={551--564},
  year={2021}
}


@article{nie2022hetumoe,
  title={{HetuMoE: An efficient trillion-scale mixture-of-expert distributed training system}},
  author={Nie, Xiaonan and Zhao, Pinxue and Miao, Xupeng and Zhao, Tong and Cui, Bin},
  journal={arXiv preprint arXiv:2203.14685},
  year={2022}
}

@inproceedings{he2022fastermoe,
  title={{Fastermoe: modeling and optimizing training of large-scale dynamic pre-trained models}},
  author={He, Jiaao and Zhai, Jidong and Antunes, Tiago and Wang, Haojie and Luo, Fuwen and Shi, Shangfeng and Li, Qin},
  booktitle={Proceedings of the 27th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
  pages={120--134},
  year={2022}
}

@inproceedings{rajbhandari2022deepspeed,
  title={{Deepspeed-moe: Advancing mixture-of-experts inference and training to power next-generation ai scale}},
  author={Rajbhandari, Samyam and Li, Conglong and Yao, Zhewei and Zhang, Minjia and Aminabadi, Reza Yazdani and Awan, Ammar Ahmad and Rasley, Jeff and He, Yuxiong},
  booktitle={International conference on machine learning},
  pages={18332--18346},
  year={2022},
  organization={PMLR}
}

@article{chen2022ta,
  title={{Ta-moe: Topology-aware large scale mixture-of-expert training}},
  author={Chen, Chang and Li, Min and Wu, Zhihua and Yu, Dianhai and Yang, Chao},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={22173--22186},
  year={2022}
}

@article{zhang2024mpmoe,
  title={{MPMoE: Memory Efficient MoE for Pre-trained Models with Adaptive Pipeline Parallelism}},
  author={Zhang, Zheng and Xia, Yaqi and Wang, Hulin and Yang, Donglin and Hu, Chuang and Zhou, Xiaobo and Cheng, Dazhao},
  journal={IEEE Transactions on Parallel and Distributed Systems},
  year={2024},
  publisher={IEEE}
}

@article{nie2023flexmoe,
  title={{Flexmoe: Scaling large-scale sparse pre-trained model training via dynamic device placement}},
  author={Nie, Xiaonan and Miao, Xupeng and Wang, Zilong and Yang, Zichao and Xue, Jilong and Ma, Lingxiao and Cao, Gang and Cui, Bin},
  journal={Proceedings of the ACM on Management of Data},
  volume={1},
  number={1},
  pages={1--19},
  year={2023},
  publisher={ACM New York, NY, USA}
}

@article{yao2024exploiting,
  title={{Exploiting Inter-Layer Expert Affinity for Accelerating Mixture-of-Experts Model Inference}},
  author={Yao, Jinghan and Anthony, Quentin and Shafi, Aamir and Subramoni, Hari and others},
  journal={arXiv preprint arXiv:2401.08383},
  year={2024}
}

@article{cai2024shortcut,
  title={{Shortcut-connected Expert Parallelism for Accelerating Mixture-of-Experts}},
  author={Cai, Weilin and Jiang, Juyong and Qin, Le and Cui, Junwei and Kim, Sunghun and Huang, Jiayi},
  journal={arXiv preprint arXiv:2404.05019},
  year={2024}
}

@article{shen2022se,
  title={{Se-moe: A scalable and efficient mixture-of-experts distributed training and inference system}},
  author={Shen, Liang and Wu, Zhihua and Gong, WeiBao and Hao, Hongxiang and Bai, Yangfan and Wu, HuaChao and Wu, Xinxuan and Bian, Jiang and Xiong, Haoyi and Yu, Dianhai and others},
  journal={arXiv preprint arXiv:2205.10034},
  year={2022}
}

@article{yi2023edgemoe,
  title={{Edgemoe: Fast on-device inference of moe-based large language models}},
  author={Yi, Rongjie and Guo, Liwei and Wei, Shiyun and Zhou, Ao and Wang, Shangguang and Xu, Mengwei},
  journal={arXiv preprint arXiv:2308.14352},
  year={2023}
}

@article{hwang2023pre,
  title={{Pre-gated MoE: An Algorithm-System Co-Design for Fast and Scalable Mixture-of-Expert Inference}},
  author={Hwang, Ranggi and Wei, Jianyu and Cao, Shijie and Hwang, Changho and Tang, Xiaohu and Cao, Ting and Yang, Mao and Rhu, Minsoo},
  journal={arXiv preprint arXiv:2308.12066},
  year={2023}
}

@inproceedings{ma2022bagualu,
  title={{BaGuaLu: targeting brain scale pretrained models with over 37 million cores}},
  author={Ma, Zixuan and He, Jiaao and Qiu, Jiezhong and Cao, Huanqi and Wang, Yuanwei and Sun, Zhenbo and Zheng, Liyan and Wang, Haojie and Tang, Shizhi and Zheng, Tianyu and others},
  booktitle={Proceedings of the 27th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
  pages={192--204},
  year={2022}
}

@article{xue2024openmoe,
  title={{Openmoe: An early effort on open mixture-of-experts language models}},
  author={Xue, Fuzhao and Zheng, Zian and Fu, Yao and Ni, Jinjie and Zheng, Zangwei and Zhou, Wangchunshu and You, Yang},
  journal={arXiv preprint arXiv:2402.01739},
  year={2024}
}

@article{shazeer2018mesh,
  title={{Mesh-tensorflow: Deep learning for supercomputers}},
  author={Shazeer, Noam and Cheng, Youlong and Parmar, Niki and Tran, Dustin and Vaswani, Ashish and Koanantakool, Penporn and Hawkins, Peter and Lee, HyoukJoong and Hong, Mingsheng and Young, Cliff and others},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@article{ott2019fairseq,
  title={{fairseq: A fast, extensible toolkit for sequence modeling}},
  author={Ott, Myle and Edunov, Sergey and Baevski, Alexei and Fan, Angela and Gross, Sam and Ng, Nathan and Grangier, David and Auli, Michael},
  journal={arXiv preprint arXiv:1904.01038},
  year={2019}
}

@article{artetxe2021efficient,
  title={{Efficient large scale language modeling with mixtures of experts}},
  author={Artetxe, Mikel and Bhosale, Shruti and Goyal, Naman and Mihaylov, Todor and Ott, Myle and Shleifer, Sam and Lin, Xi Victoria and Du, Jingfei and Iyer, Srinivasan and Pasunuru, Ramakanth and others},
  journal={arXiv preprint arXiv:2112.10684},
  year={2021}
}

@article{jordan1994hierarchical,
  title={{Hierarchical mixtures of experts and the EM algorithm}},
  author={Jordan, Michael I and Jacobs, Robert A},
  journal={Neural computation},
  volume={6},
  number={2},
  pages={181--214},
  year={1994},
  publisher={MIT Press}
}

@article{jacobs1991adaptive,
  title={{Adaptive mixtures of local experts}},
  author={Jacobs, Robert A and Jordan, Michael I and Nowlan, Steven J and Hinton, Geoffrey E},
  journal={Neural computation},
  volume={3},
  number={1},
  pages={79--87},
  year={1991},
  publisher={MIT Press}
}

@article{shazeer2017outrageously,
  title={{Outrageously large neural networks: The sparsely-gated mixture-of-experts layer}},
  author={Shazeer, Noam and Mirhoseini, Azalia and Maziarz, Krzysztof and Davis, Andy and Le, Quoc and Hinton, Geoffrey and Dean, Jeff},
  journal={arXiv preprint arXiv:1701.06538},
  year={2017}
}


@article{collobert2001parallel,
  title={{A parallel mixture of SVMs for very large scale problems}},
  author={Collobert, Ronan and Bengio, Samy and Bengio, Yoshua},
  journal={Advances in Neural Information Processing Systems},
  volume={14},
  year={2001}
}

@article{shahbaba2009nonlinear,
  title={{Nonlinear models using Dirichlet process mixtures.}},
  author={Shahbaba, Babak and Neal, Radford},
  journal={Journal of Machine Learning Research},
  volume={10},
  number={8},
  year={2009}
}

@inproceedings{deisenroth2015distributed,
  title={{Distributed gaussian processes}},
  author={Deisenroth, Marc and Ng, Jun Wei},
  booktitle={International conference on machine learning},
  pages={1481--1490},
  year={2015},
  organization={PMLR}
}

@article{theis2015generative,
  title={{Generative image modeling using spatial lstms}},
  author={Theis, Lucas and Bethge, Matthias},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}


@article{rasmussen2001infinite,
  title={{Infinite mixtures of Gaussian process experts}},
  author={Rasmussen, Carl and Ghahramani, Zoubin},
  journal={Advances in neural information processing systems},
  volume={14},
  year={2001}
}

@inproceedings{aljundi2017expert,
  title={{Expert gate: Lifelong learning with a network of experts}},
  author={Aljundi, Rahaf and Chakravarty, Punarjay and Tuytelaars, Tinne},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={3366--3375},
  year={2017}
}

@inproceedings{du2022glam,
  title={{Glam: Efficient scaling of language models with mixture-of-experts}},
  author={Du, Nan and Huang, Yanping and Dai, Andrew M and Tong, Simon and Lepikhin, Dmitry and Xu, Yuanzhong and Krikun, Maxim and Zhou, Yanqi and Yu, Adams Wei and Firat, Orhan and others},
  booktitle={International Conference on Machine Learning},
  pages={5547--5569},
  year={2022},
  organization={PMLR}
}

@inproceedings{clark2022unified,
  title={{Unified scaling laws for routed language models}},
  author={Clark, Aidan and de Las Casas, Diego and Guy, Aurelia and Mensch, Arthur and Paganini, Michela and Hoffmann, Jordan and Damoc, Bogdan and Hechtman, Blake and Cai, Trevor and Borgeaud, Sebastian and others},
  booktitle={International conference on machine learning},
  pages={4057--4086},
  year={2022},
  organization={PMLR}
}


@misc{dbrx,
    title = {{Introducing DBRX: A New State-of-the-Art Open LLM}},
    url = {https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm},
    author = {Databricks},
    month = {March},
    year = {2024}
}

@misc{snowflake,
    title = {{Snowflake Arctic: The Best LLM for Enterprise AI — Efficiently Intelligent, Truly Open}},
    author = {Snowflake AI Research Team},
    month = {April},
    year = {2024}
}


@article{shen2023moduleformer,
  title={{Moduleformer: Learning modular large language models from uncurated data}},
  author={Shen, Yikang and Zhang, Zheyu and Cao, Tianyou and Tan, Shawn and Chen, Zhenfang and Gan, Chuang},
  journal={arXiv preprint arXiv:2306.04640},
  year={2023}
}

@article{pan2024dense,
  title={{Dense Training, Sparse Inference: Rethinking Training of Mixture-of-Experts Language Models}},
  author={Pan, Bowen and Shen, Yikang and Liu, Haokun and Mishra, Mayank and Zhang, Gaoyuan and Oliva, Aude and Raffel, Colin and Panda, Rameswar},
  journal={arXiv preprint arXiv:2404.05567},
  year={2024}
}




@inproceedings{wu2023mole,
  title={{MoLE: Mixture of LoRA Experts}},
  author={Wu, Xun and Huang, Shaohan and Wei, Furu},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2023}
}

@article{bengio2015conditional,
  title={{Conditional computation in neural networks for faster models}},
  author={Bengio, Emmanuel and Bacon, Pierre-Luc and Pineau, Joelle and Precup, Doina},
  journal={arXiv preprint arXiv:1511.06297},
  year={2015}
}

@article{rosenbaum2017routing,
  title={{Routing networks: Adaptive selection of non-linear functions for multi-task learning}},
  author={Rosenbaum, Clemens and Klinger, Tim and Riemer, Matthew},
  journal={arXiv preprint arXiv:1711.01239},
  year={2017}
}

@article{rosenbaum2019routing,
  title={{Routing networks and the challenges of modular and compositional computation}},
  author={Rosenbaum, Clemens and Cases, Ignacio and Riemer, Matthew and Klinger, Tim},
  journal={arXiv preprint arXiv:1904.12774},
  year={2019}
}



@article{davis2013low,
  title={{Low-rank approximations for conditional feedforward computation in deep neural networks}},
  author={Davis, Andrew and Arel, Itamar},
  journal={arXiv preprint arXiv:1312.4461},
  year={2013}
}

@article{bengio2013estimating,
  title={{Estimating or propagating gradients through stochastic neurons for conditional computation}},
  author={Bengio, Yoshua and L{\'e}onard, Nicholas and Courville, Aaron},
  journal={arXiv preprint arXiv:1308.3432},
  year={2013}
}

@inproceedings{almahairi2016dynamic,
  title={{Dynamic capacity networks}},
  author={Almahairi, Amjad and Ballas, Nicolas and Cooijmans, Tim and Zheng, Yin and Larochelle, Hugo and Courville, Aaron},
  booktitle={International Conference on Machine Learning},
  pages={2549--2558},
  year={2016},
  organization={PMLR}
}


@article{nie2021evomoe,
  title={{Evomoe: An evolutional mixture-of-experts training framework via dense-to-sparse gate}},
  author={Nie, Xiaonan and Miao, Xupeng and Cao, Shijie and Ma, Lingxiao and Liu, Qibin and Xue, Jilong and Miao, Youshan and Liu, Yi and Yang, Zhi and Cui, Bin},
  journal={arXiv preprint arXiv:2112.14397},
  year={2021}
}

@inproceedings{puigcerver2023sparse,
  title={{From Sparse to Soft Mixtures of Experts}},
  author={Puigcerver, Joan and Ruiz, Carlos Riquelme and Mustafa, Basil and Houlsby, Neil},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2023}
}

@article{wu2023omni,
  title={{Omni-SMoLA: Boosting Generalist Multimodal Models with Soft Mixture of Low-rank Experts}},
  author={Wu, Jialin and Hu, Xia and Wang, Yaqing and Pang, Bo and Soricut, Radu},
  journal={arXiv preprint arXiv:2312.00968},
  year={2023}
}

@article{muqeeth2023soft,
  title={{Soft merging of experts with adaptive routing}},
  author={Muqeeth, Mohammed and Liu, Haokun and Raffel, Colin},
  journal={arXiv preprint arXiv:2306.03745},
  year={2023}
}

@article{raffel2020exploring,
  title={{Exploring the limits of transfer learning with a unified text-to-text transformer}},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={Journal of machine learning research},
  volume={21},
  number={140},
  pages={1--67},
  year={2020}
}

@article{zhong2024lory,
  title={{Lory: Fully Differentiable Mixture-of-Experts for Autoregressive Language Model Pre-training}},
  author={Zhong, Zexuan and Xia, Mengzhou and Chen, Danqi and Lewis, Mike},
  journal={arXiv preprint arXiv:2405.03133},
  year={2024}
}

@article{yang2021m6,
  title={{M6-t: Exploring sparse expert models and beyond}},
  author={Yang, An and Lin, Junyang and Men, Rui and Zhou, Chang and Jiang, Le and Jia, Xianyan and Wang, Ang and Zhang, Jie and Wang, Jiamang and Li, Yong and others},
  journal={arXiv preprint arXiv:2105.15082},
  year={2021}
}

@inproceedings{dua2022tricks,
  title={{Tricks for Training Sparse Translation Models}},
  author={Dua, Dheeru and Bhosale, Shruti and Goswami, Vedanuj and Cross, James and Lewis, Mike and Fan, Angela},
  booktitle={Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={3340--3345},
  year={2022}
}


@article{kim2021scalable,
  title={{Scalable and efficient moe training for multitask multilingual models}},
  author={Kim, Young Jin and Awan, Ammar Ahmad and Muzio, Alexandre and Salinas, Andres Felipe Cruz and Lu, Liyang and Hendy, Amr and Rajbhandari, Samyam and He, Yuxiong and Awadalla, Hany Hassan},
  journal={arXiv preprint arXiv:2109.10465},
  year={2021}
}




@article{williams1992simple,
  title={{Simple statistical gradient-following algorithms for connectionist reinforcement learning}},
  author={Williams, Ronald J},
  journal={Machine learning},
  volume={8},
  pages={229--256},
  year={1992},
  publisher={Springer}
}

@article{hazimeh2021dselect,
  title={{Dselect-k: Differentiable selection in the mixture of experts with applications to multi-task learning}},
  author={Hazimeh, Hussein and Zhao, Zhe and Chowdhery, Aakanksha and Sathiamoorthy, Maheswaran and Chen, Yihua and Mazumder, Rahul and Hong, Lichan and Chi, Ed},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={29335--29347},
  year={2021}
}

@inproceedings{chen2023mod,
  title={{Mod-squad: Designing mixtures of experts as modular multi-task learners}},
  author={Chen, Zitian and Shen, Yikang and Ding, Mingyu and Chen, Zhenfang and Zhao, Hengshuang and Learned-Miller, Erik G and Gan, Chuang},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={11828--11837},
  year={2023}
}

@article{roller2021hash,
  title={{Hash layers for large sparse models}},
  author={Roller, Stephen and Sukhbaatar, Sainbayar and Weston, Jason and others},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={17555--17566},
  year={2021}
}

@inproceedings{lewis2021base,
  title={{Base layers: Simplifying training of large, sparse models}},
  author={Lewis, Mike and Bhosale, Shruti and Dettmers, Tim and Goyal, Naman and Zettlemoyer, Luke},
  booktitle={International Conference on Machine Learning},
  pages={6265--6274},
  year={2021},
  organization={PMLR}
}


@inproceedings{gururangan2022demix,
  title={{DEMix Layers: Disentangling Domains for Modular Language Modeling}},
  author={Gururangan, Suchin and Lewis, Mike and Holtzman, Ari and Smith, Noah A and Zettlemoyer, Luke},
  booktitle={Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={5557--5576},
  year={2022}
}

@inproceedings{kudugunta2021beyond,
  title={{Beyond Distillation: Task-level Mixture-of-Experts for Efficient Inference}},
  author={Kudugunta, Sneha and Huang, Yanping and Bapna, Ankur and Krikun, Maxim and Lepikhin, Dmitry and Luong, Minh-Thang and Firat, Orhan},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2021},
  pages={3577--3599},
  year={2021}
}

@article{fan2021beyond,
  title={{Beyond english-centric multilingual machine translation}},
  author={Fan, Angela and Bhosale, Shruti and Schwenk, Holger and Ma, Zhiyi and El-Kishky, Ahmed and Goyal, Siddharth and Baines, Mandeep and Celebi, Onur and Wenzek, Guillaume and Chaudhary, Vishrav and others},
  journal={Journal of Machine Learning Research},
  volume={22},
  number={107},
  pages={1--48},
  year={2021}
}

@article{costa2022no,
  title={{No language left behind: Scaling human-centered machine translation}},
  author={Costa-juss{\`a}, Marta R and Cross, James and {\c{C}}elebi, Onur and Elbayad, Maha and Heafield, Kenneth and Heffernan, Kevin and Kalbassi, Elahe and Lam, Janice and Licht, Daniel and Maillard, Jean and others},
  journal={arXiv preprint arXiv:2207.04672},
  year={2022}
}

@inproceedings{zuo2021taming,
  title={{Taming Sparsely Activated Transformer with Stochastic Experts}},
  author={Zuo, Simiao and Liu, Xiaodong and Jiao, Jian and Kim, Young Jin and Hassan, Hany and Zhang, Ruofei and Gao, Jianfeng and Zhao, Tuo},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@inproceedings{zhou2023brainformers,
  title={{Brainformers: Trading simplicity for efficiency}},
  author={Zhou, Yanqi and Du, Nan and Huang, Yanping and Peng, Daiyi and Lan, Chang and Huang, Da and Shakeri, Siamak and So, David and Dai, Andrew M and Lu, Yifeng and others},
  booktitle={International Conference on Machine Learning},
  pages={42531--42542},
  year={2023},
  organization={PMLR}
}

@inproceedings{dai2022stablemoe,
  title={{StableMoE: Stable Routing Strategy for Mixture of Experts}},
  author={Dai, Damai and Dong, Li and Ma, Shuming and Zheng, Bo and Sui, Zhifang and Chang, Baobao and Wei, Furu},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={7085--7095},
  year={2022}
}

@inproceedings{ye2022eliciting,
  title={{Eliciting and Understanding Cross-task Skills with Task-level Mixture-of-Experts}},
  author={Ye, Qinyuan and Zha, Juan and Ren, Xiang},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2022},
  pages={2567--2592},
  year={2022}
}

@article{wei2024skywork,
  title={{Skywork-MoE: A Deep Dive into Training Techniques for Mixture-of-Experts Language Models}},
  author={Wei, Tianwen and Zhu, Bo and Zhao, Liang and Cheng, Cheng and Li, Biye and L{\"u}, Weiwei and Cheng, Peng and Zhang, Jianhao and Zhang, Xiaoyu and Zeng, Liang and others},
  journal={arXiv preprint arXiv:2406.06563},
  year={2024}
}

@article{wu2022residual,
  title={{Residual mixture of experts}},
  author={Wu, Lemeng and Liu, Mengchen and Chen, Yinpeng and Chen, Dongdong and Dai, Xiyang and Yuan, Lu},
  journal={arXiv preprint arXiv:2204.09636},
  year={2022}
}

@inproceedings{komatsuzaki2022sparse,
  title={{Sparse Upcycling: Training Mixture-of-Experts from Dense Checkpoints}},
  author={Komatsuzaki, Aran and Puigcerver, Joan and Lee-Thorp, James and Ruiz, Carlos Riquelme and Mustafa, Basil and Ainslie, Joshua and Tay, Yi and Dehghani, Mostafa and Houlsby, Neil},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2022}
}

@article{wei2023skywork,
  title={{Skywork: A more open bilingual foundation model}},
  author={Wei, Tianwen and Zhao, Liang and Zhang, Lichang and Zhu, Bo and Wang, Lijie and Yang, Haihua and Li, Biye and Cheng, Cheng and L{\"u}, Weiwei and Hu, Rui and others},
  journal={arXiv preprint arXiv:2310.19341},
  year={2023}
}

@article{tan2024scattered,
  title={{Scattered Mixture-of-Experts Implementation}},
  author={Tan, Shawn and Shen, Yikang and Panda, Rameswar and Courville, Aaron},
  journal={arXiv preprint arXiv:2403.08245},
  year={2024}
}

@inproceedings{zheng2023pit,
  title={{Pit: Optimization of dynamic sparse deep learning models via permutation invariant transformation}},
  author={Zheng, Ningxin and Jiang, Huiqiang and Zhang, Quanlu and Han, Zhenhua and Ma, Lingxiao and Yang, Yuqing and Yang, Fan and Zhang, Chengruidong and Qiu, Lili and Yang, Mao and others},
  booktitle={Proceedings of the 29th Symposium on Operating Systems Principles},
  pages={331--347},
  year={2023}
}

@inproceedings{tan2023sparse,
  title={{Sparse Universal Transformer}},
  author={Tan, Shawn and Shen, Yikang and Chen, Zhenfang and Courville, Aaron and Gan, Chuang},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  pages={169--179},
  year={2023}
}

@inproceedings{xue2022go,
  title={{Go wider instead of deeper}},
  author={Xue, Fuzhao and Shi, Ziji and Wei, Futao and Lou, Yuxuan and Liu, Yong and You, Yang},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={36},
  number={8},
  pages={8779--8787},
  year={2022}
}

@article{huang2023experts,
  title={{Experts weights averaging: A new general training scheme for vision transformers}},
  author={Huang, Yongqi and Ye, Peng and Huang, Xiaoshui and Li, Sheng and Chen, Tao and Ouyang, Wanli},
  journal={arXiv preprint arXiv:2308.06093},
  year={2023}
}

@article{chen2022task,
  title={{Task-specific expert pruning for sparse mixture-of-experts}},
  author={Chen, Tianyu and Huang, Shaohan and Xie, Yuan and Jiao, Binxing and Jiang, Daxin and Zhou, Haoyi and Li, Jianxin and Wei, Furu},
  journal={arXiv preprint arXiv:2206.00277},
  year={2022}
}

@inproceedings{li2022branch,
  title={{Branch-Train-Merge: Embarrassingly Parallel Training of Expert Language Models}},
  author={Li, Margaret and Gururangan, Suchin and Dettmers, Tim and Lewis, Mike and Althoff, Tim and Smith, Noah A and Zettlemoyer, Luke},
  booktitle={First Workshop on Interpolation Regularizers and Beyond at NeurIPS 2022},
  year={2022}
}

@article{sukhbaatar2024branch,
  title={{Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM}},
  author={Sukhbaatar, Sainbayar and Golovneva, Olga and Sharma, Vasu and Xu, Hu and Lin, Xi Victoria and Rozi{\`e}re, Baptiste and Kahn, Jacob and Li, Daniel and Yih, Wen-tau and Weston, Jason and others},
  journal={arXiv preprint arXiv:2403.07816},
  year={2024}
}

@article{chi2022representation,
  title={{On the representation collapse of sparse mixture of experts}},
  author={Chi, Zewen and Dong, Li and Huang, Shaohan and Dai, Damai and Ma, Shuming and Patra, Barun and Singhal, Saksham and Bajaj, Payal and Song, Xia and Mao, Xian-Ling and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={34600--34613},
  year={2022}
}

@inproceedings{chen2022sparse,
  title={{Sparse MoE as the New Dropout: Scaling Dense and Self-Slimmable Transformers}},
  author={Chen, Tianlong and Zhang, Zhenyu and JAISWAL, AJAY KUMAR and Liu, Shiwei and Wang, Zhangyang},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2022}
}


@inproceedings{wang2023fusing,
  title={{Fusing Models with Complementary Expertise}},
  author={Wang, Hongyi and Polo, Felipe Maia and Sun, Yuekai and Kundu, Souvik and Xing, Eric and Yurochkin, Mikhail},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2023}
}

@inproceedings{zhang2022mixture,
  title={{Mixture of Attention Heads: Selecting Attention Heads Per Token}},
  author={Zhang, Xiaofeng and Shen, Yikang and Huang, Zeyu and Zhou, Jie and Rong, Wenge and Xiong, Zhang},
  booktitle={Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  pages={4150--4162},
  year={2022}
}

@inproceedings{ainslie2023gqa,
  title={{GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints}},
  author={Ainslie, Joshua and Lee-Thorp, James and de Jong, Michiel and Zemlyanskiy, Yury and Lebron, Federico and Sanghai, Sumit},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  pages={4895--4901},
  year={2023}
}
@article{han2024parameter,
  title={{Parameter-efficient fine-tuning for large models: A comprehensive survey}},
  author={Han, Zeyu and Gao, Chao and Liu, Jinyang and Zhang, Sai Qian and others},
  journal={arXiv preprint arXiv:2403.14608},
  year={2024}
}
@article{dao2022flashattention,
  title={{Flashattention: Fast and memory-efficient exact attention with io-awareness}},
  author={Dao, Tri and Fu, Dan and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={16344--16359},
  year={2022}
}
@inproceedings{kwon2023efficient,
  title={{Efficient memory management for large language model serving with pagedattention}},
  author={Kwon, Woosuk and Li, Zhuohan and Zhuang, Siyuan and Sheng, Ying and Zheng, Lianmin and Yu, Cody Hao and Gonzalez, Joseph and Zhang, Hao and Stoica, Ion},
  booktitle={Proceedings of the 29th Symposium on Operating Systems Principles},
  pages={611--626},
  year={2023}
}
@article{shen2024jetmoe,
  title={{JetMoE: Reaching Llama2 Performance with 0.1 M Dollars}},
  author={Shen, Yikang and Guo, Zhen and Cai, Tianle and Qin, Zengyi},
  journal={arXiv preprint arXiv:2404.07413},
  year={2024}
}

@article{chen2022towards,
  title={{Towards understanding mixture of experts in deep learning}},
  author={Chen, Zixiang and Deng, Yihe and Wu, Yue and Gu, Quanquan and Li, Yuanzhi},
  journal={arXiv preprint arXiv:2208.02813},
  year={2022}
}

@inproceedings{chowdhury2023patch,
  title={{Patch-level routing in mixture-of-experts is provably sample-efficient for convolutional neural networks}},
  author={Chowdhury, Mohammed Nowaz Rabbani and Zhang, Shuai and Wang, Meng and Liu, Sijia and Chen, Pin-Yu},
  booktitle={International Conference on Machine Learning},
  pages={6074--6114},
  year={2023},
  organization={PMLR}
}

@article{shen2023scaling,
  title={{Scaling vision-language models with sparse mixture of experts}},
  author={Shen, Sheng and Yao, Zhewei and Li, Chunyuan and Darrell, Trevor and Keutzer, Kurt and He, Yuxiong},
  journal={arXiv preprint arXiv:2303.07226},
  year={2023}
}

@article{mustafa2022multimodal,
  title={{Multimodal contrastive learning with limoe: the language-image mixture of experts}},
  author={Mustafa, Basil and Riquelme, Carlos and Puigcerver, Joan and Jenatton, Rodolphe and Houlsby, Neil},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={9564--9576},
  year={2022}
}

@inproceedings{zhang2023robust,
  title={{Robust Mixture-of-Expert Training for Convolutional Neural Networks}},
  author={Zhang, Yihua and Cai, Ruisi and Chen, Tianlong and Zhang, Guanhua and Zhang, Huan and Chen, Pin-Yu and Chang, Shiyu and Wang, Zhangyang and Liu, Sijia},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={90--101},
  year={2023}
}

@inproceedings{wang2020deep,
  title={{Deep mixture of experts via shallow embedding}},
  author={Wang, Xin and Yu, Fisher and Dunlap, Lisa and Ma, Yi-An and Wang, Ruth and Mirhoseini, Azalia and Darrell, Trevor and Gonzalez, Joseph E},
  booktitle={Uncertainty in artificial intelligence},
  pages={552--562},
  year={2020},
  organization={PMLR}
}

@inproceedings{gross2017hard,
  title={{Hard mixtures of experts for large scale weakly supervised vision}},
  author={Gross, Sam and Ranzato, Marc'Aurelio and Szlam, Arthur},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={6865--6873},
  year={2017}
}

@article{jiang2024lancet,
  title={{Lancet: Accelerating Mixture-of-Experts Training via Whole Graph Computation-Communication Overlapping}},
  author={Jiang, Chenyu and Tian, Ye and Jia, Zhen and Zheng, Shuai and Wu, Chuan and Wang, Yida},
  journal={arXiv preprint arXiv:2404.19429},
  year={2024}
}

@inproceedings{choi2023smop,
  title={{SMoP: Towards Efficient and Effective Prompt Tuning with Sparse Mixture-of-Prompts}},
  author={Choi, Joon-Young and Kim, Junho and Park, Jun-Hyung and Mok, Wing-Lam and Lee, SangKeun},
  booktitle={The 2023 Conference on Empirical Methods in Natural Language Processing},
  year={2023}
}
@article{li2024mixlora,
  title={{MixLoRA: Enhancing Large Language Models Fine-Tuning with LoRA based Mixture of Experts}},
  author={Li, Dengchun and Ma, Yingzi and Wang, Naizheng and Cheng, Zhiyuan and Duan, Lei and Zuo, Jie and Yang, Cal and Tang, Mingjie},
  journal={arXiv preprint arXiv:2404.15159},
  year={2024}
}
@inproceedings{mao2022unipelt,
  title={{UniPELT: A Unified Framework for Parameter-Efficient Language Model Tuning}},
  author={Mao, Yuning and Mathias, Lambert and Hou, Rui and Almahairi, Amjad and Ma, Hao and Han, Jiawei and Yih, Scott and Khabsa, Madian},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={6253--6264},
  year={2022}
}

@article{devlin2018bert,
  title={{Bert: Pre-training of deep bidirectional transformers for language understanding}},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{brown2020language,
  title={{Language models are few-shot learners}},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}


@inproceedings{glorot2011deep,
  title={{Deep sparse rectifier neural networks}},
  author={Glorot, Xavier and Bordes, Antoine and Bengio, Yoshua},
  booktitle={Proceedings of the fourteenth international conference on artificial intelligence and statistics},
  pages={315--323},
  year={2011},
  organization={JMLR Workshop and Conference Proceedings}
}

@article{hendrycks2016gaussian,
  title={{Gaussian error linear units (gelus)}},
  author={Hendrycks, Dan and Gimpel, Kevin},
  journal={arXiv preprint arXiv:1606.08415},
  year={2016}
}

@article{shazeer2020glu,
  title={{Glu variants improve transformer}},
  author={Shazeer, Noam},
  journal={arXiv preprint arXiv:2002.05202},
  year={2020}
}

@article{zhang2019root,
  title={{Root mean square layer normalization}},
  author={Zhang, Biao and Sennrich, Rico},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}


@article{su2024roformer,
  title={{Roformer: Enhanced transformer with rotary position embedding}},
  author={Su, Jianlin and Ahmed, Murtadha and Lu, Yu and Pan, Shengfeng and Bo, Wen and Liu, Yunfeng},
  journal={Neurocomputing},
  volume={568},
  pages={127063},
  year={2024},
  publisher={Elsevier}
}

@article{gou2023mixture,
  title={{Mixture of cluster-conditional lora experts for vision-language instruction tuning}},
  author={Gou, Yunhao and Liu, Zhili and Chen, Kai and Hong, Lanqing and Xu, Hang and Li, Aoxue and Yeung, Dit-Yan and Kwok, James T and Zhang, Yu},
  journal={arXiv preprint arXiv:2312.12379},
  year={2023}
}


@article{wu2024yuan,
  title={{Yuan 2.0-M32: Mixture of Experts with Attention Router}},
  author={Wu, Shaohua and Luo, Jiangang and Chen, Xi and Li, Lingjun and Zhao, Xudong and Yu, Tong and Wang, Chao and Wang, Yue and Wang, Fei and Qiao, Weixu and others},
  journal={arXiv preprint arXiv:2405.17976},
  year={2024}
}

@misc{llama-moe-2023,
  title={{LLaMA-MoE: Building Mixture-of-Experts from LLaMA with Continual Pre-training}},
  author={LLaMA-MoE Team},
  year={2023},
  month={Dec},
  url={https://github.com/pjlab-sys4nlp/llama-moe}
}

@article{han2024fusemoe,
  title={{FuseMoE: Mixture-of-Experts Transformers for Fleximodal Fusion}},
  author={Han, Xing and Nguyen, Huy and Harris, Carl and Ho, Nhat and Saria, Suchi},
  journal={arXiv preprint arXiv:2402.03226},
  year={2024}
}

@article{dat2023homoe,
  title={{HOMOE: A Memory-Based and Composition-Aware Framework for Zero-Shot Learning with Hopfield Network and Soft Mixture of Experts}},
  author={Dat, Do Huu and Mao, Po Yuan and Nguyen, Tien Hoang and Buntine, Wray and Bennamoun, Mohammed},
  journal={arXiv preprint arXiv:2311.14747},
  year={2023}
}

@inproceedings{zhang2022moefication,
  title={{MoEfication: Transformer Feed-forward Layers are Mixtures of Experts}},
  author={Zhang, Zhengyan and Lin, Yankai and Liu, Zhiyuan and Li, Peng and Sun, Maosong and Zhou, Jie},
  booktitle={Findings of the Association for Computational Linguistics: ACL 2022},
  pages={877--890},
  year={2022}
}

@inproceedings{zuo2022moebert,
  title={{MoEBERT: from BERT to Mixture-of-Experts via Importance-Guided Adaptation}},
  author={Zuo, Simiao and Zhang, Qingru and Liang, Chen and He, Pengcheng and Zhao, Tuo and Chen, Weizhu},
  booktitle={Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={1610--1623},
  year={2022}
}

@misc{deepseekv2,
      title={{DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model}}, 
      author={DeepSeek-AI},
      year={2024},
      eprint={2405.04434},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{lieber2024jamba,
  title={{Jamba: A hybrid transformer-mamba language model}},
  author={Lieber, Opher and Lenz, Barak and Bata, Hofit and Cohen, Gal and Osin, Jhonathan and Dalmedigos, Itay and Safahi, Erez and Meirom, Shaked and Belinkov, Yonatan and Shalev-Shwartz, Shai and others},
  journal={arXiv preprint arXiv:2403.19887},
  year={2024}
}
@inproceedings{diao2023mixture,
  title={{Mixture-of-Domain-Adapters: Decoupling and Injecting Domain Knowledge to Pre-trained Language Models' Memories}},
  author={Diao, Shizhe and Xu, Tianyang and Xu, Ruijia and Wang, Jiawei and Zhang, Tong},
  booktitle={The 61st Annual Meeting Of The Association For Computational Linguistics},
  year={2023}
}
@article{zhu2023sira,
  title={{Sira: Sparse mixture of low rank adaptation}},
  author={Zhu, Yun and Wichers, Nevan and Lin, Chu-Cheng and Wang, Xinyi and Chen, Tianlong and Shu, Lei and Lu, Han and Liu, Canoee and Luo, Liangchen and Chen, Jindong and others},
  journal={arXiv preprint arXiv:2311.09179},
  year={2023}
}
@article{liu2024intuition,
  title={{Intuition-aware Mixture-of-Rank-1-Experts for Parameter Efficient Finetuning}},
  author={Liu, Yijiang and Zhang, Rongyu and Yang, Huanrui and Keutzer, Kurt and Du, Yuan and Du, Li and Zhang, Shanghang},
  journal={arXiv preprint arXiv:2404.08985},
  year={2024}
}

@inproceedings{wu2024mixture,
title={{Mixture of Lo{RA} Experts}},
author={Xun Wu and Shaohan Huang and Furu Wei},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=uWvKBCYh4S}
}

@article{gu2023mamba,
  title={{Mamba: Linear-time sequence modeling with selective state spaces}},
  author={Gu, Albert and Dao, Tri},
  journal={arXiv preprint arXiv:2312.00752},
  year={2023}
}
@inproceedings{li2021prefix,
  title={{Prefix-Tuning: Optimizing Continuous Prompts for Generation}},
  author={Li, Xiang Lisa and Liang, Percy},
  booktitle={Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
  pages={4582--4597},
  year={2021}
}
@inproceedings{chen2023sparse,
title={{Sparse MoE as the New Dropout: Scaling Dense and Self-Slimmable Transformers}},
author={Tianlong Chen and Zhenyu Zhang and AJAY KUMAR JAISWAL and Shiwei Liu and Zhangyang Wang},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=w1hwFUb_81}
}

@article{hendrycks2020measuring,
  title={{Measuring massive multitask language understanding}},
  author={Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2009.03300},
  year={2020}
}

@article{cobbe2021training,
  title={{Training verifiers to solve math word problems}},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and others},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}

@article{hendrycks2021measuring,
  title={{Measuring mathematical problem solving with the math dataset}},
  author={Hendrycks, Dan and Burns, Collin and Kadavath, Saurav and Arora, Akul and Basart, Steven and Tang, Eric and Song, Dawn and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2103.03874},
  year={2021}
}

@article{chen2021evaluating,
  title={{Evaluating large language models trained on code}},
  author={Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and others},
  journal={arXiv preprint arXiv:2107.03374},
  year={2021}
}



@article{vaswani2017attention,
  title={{Attention is all you need}},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{lu2019vilbert,
  title={{Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks}},
  author={Lu, Jiasen and Batra, Dhruv and Parikh, Devi and Lee, Stefan},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{zhou2022learning,
  title={{Learning to prompt for vision-language models}},
  author={Zhou, Kaiyang and Yang, Jingkang and Loy, Chen Change and Liu, Ziwei},
  journal={International Journal of Computer Vision},
  volume={130},
  number={9},
  pages={2337--2348},
  year={2022},
  publisher={Springer}
}
@inproceedings{zhang2021vinvl,
  title={{Vinvl: Revisiting visual representations in vision-language models}},
  author={Zhang, Pengchuan and Li, Xiujun and Hu, Xiaowei and Yang, Jianwei and Zhang, Lei and Wang, Lijuan and Choi, Yejin and Gao, Jianfeng},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={5579--5588},
  year={2021}
}
@article{zhu2023minigpt,
  title={{Minigpt-4: Enhancing vision-language understanding with advanced large language models}},
  author={Zhu, Deyao and Chen, Jun and Shen, Xiaoqian and Li, Xiang and Elhoseiny, Mohamed},
  journal={arXiv preprint arXiv:2304.10592},
  year={2023}
}


@article{wei2022chain,
  title={{Chain-of-thought prompting elicits reasoning in large language models}},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}

@article{ouyang2022training,
  title={{Training language models to follow instructions with human feedback}},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@article{wei2022emergent,
  title={{Emergent abilities of large language models}},
  author={Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and others},
  journal={arXiv preprint arXiv:2206.07682},
  year={2022}
}

@article{achiam2023gpt,
  title={{Gpt-4 technical report}},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@inproceedings{ostapenko2023case,
  title={{A Case Study of Instruction Tuning with Mixture of Parameter-Efficient Experts}},
  author={Ostapenko, Oleksiy and Caccia, Lucas and Su, Zhan and Le Roux, Nicolas and Charlin, Laurent and Sordoni, Alessandro},
  booktitle={NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following},
  year={2023}
}

@article{ding2022delta,
  title={{Delta tuning: A comprehensive study of parameter efficient methods for pre-trained language models}},
  author={Ding, Ning and Qin, Yujia and Yang, Guang and Wei, Fuchao and Yang, Zonghan and Su, Yusheng and Hu, Shengding and Chen, Yulin and Chan, Chi-Min and Chen, Weize and others},
  journal={arXiv preprint arXiv:2203.06904},
  year={2022}
}

@article{zeng2024adamoe,
  title={AdaMoE: Token-Adaptive Routing with Null Experts for Mixture-of-Experts Language Models},
  author={Zeng, Zihao and Miao, Yibo and Gao, Hongcheng and Zhang, Hao and Deng, Zhijie},
  journal={arXiv preprint arXiv:2406.13233},
  year={2024}
}

@article{li2022lazy,
  title={The lazy neuron phenomenon: On emergence of activation sparsity in transformers},
  author={Li, Zonglin and You, Chong and Bhojanapalli, Srinadh and Li, Daliang and Rawat, Ankit Singh and Reddi, Sashank J and Ye, Ke and Chern, Felix and Yu, Felix and Guo, Ruiqi and others},
  journal={arXiv preprint arXiv:2210.06313},
  year={2022}
}

@inproceedings{zhang2023emergent,
  title={Emergent Modularity in Pre-trained Transformers},
  author={Zhang, Zhengyan and Zeng, Zhiyuan and Lin, Yankai and Xiao, Chaojun and Wang, Xiaozhi and Han, Xu and Liu, Zhiyuan and Xie, Ruobing and Sun, Maosong and Zhou, Jie},
  booktitle={Findings of the Association for Computational Linguistics: ACL 2023},
  pages={4066--4083},
  year={2023}
}

@inproceedings{qiu2024unlocking,
  title={Unlocking Emergent Modularity in Large Language Models},
  author={Qiu, Zihan and Huang, Zeyu and Fu, Jie},
  booktitle={Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)},
  pages={2638--2660},
  year={2024}
}

@article{he2024mixture,
  title={Mixture of A Million Experts},
  author={He, Xu Owen},
  journal={arXiv preprint arXiv:2407.04153},
  year={2024}
}

@article{lample2019large,
  title={Large memory layers with product keys},
  author={Lample, Guillaume and Sablayrolles, Alexandre and Ranzato, Marc'Aurelio and Denoyer, Ludovic and J{\'e}gou, Herv{\'e}},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@article{shi2024unchosen,
  title={Unchosen Experts Can Contribute Too: Unleashing MoE Models' Power by Self-Contrast},
  author={Shi, Chufan and Yang, Cheng and Zhu, Xinyu and Wang, Jiahao and Wu, Taiqiang and Li, Siheng and Cai, Deng and Yang, Yujiu and Meng, Yu},
  journal={arXiv preprint arXiv:2405.14507},
  year={2024}
}

@inproceedings{he2023pad,
  title={PAD-Net: An Efficient Framework for Dynamic Networks},
  author={He, Shwai and Ding, Liang and Dong, Daize and Liu, Boan and Yu, Fuqiang and Tao, Dacheng},
  booktitle={Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={14354--14366},
  year={2023}
}

@inproceedings{he2023merging,
  title={Merging Experts into One: Improving Computational Efficiency of Mixture of Experts},
  author={He, Shwai and Fan, Run-Ze and Ding, Liang and Shen, Li and Zhou, Tianyi and Tao, Dacheng},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  pages={14685--14691},
  year={2023}
}

@inproceedings{li2023pace,
  title={PaCE: Unified Multi-modal Dialogue Pre-training with Progressive and Compositional Experts},
  author={Li, Yunshui and Hui, Binyuan and Yin, ZhiChao and Yang, Min and Huang, Fei and Li, Yongbin},
  booktitle={Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={13402--13416},
  year={2023}
}

@article{guo2024dynamic,
  title={Dynamic Mixture of Experts: An Auto-Tuning Approach for Efficient Transformer Models},
  author={Guo, Yongxin and Cheng, Zhenglin and Tang, Xiaoying and Lin, Tao},
  journal={arXiv preprint arXiv:2405.14297},
  year={2024}
}

@inproceedings{shi2024schemoe,
  title={ScheMoE: An Extensible Mixture-of-Experts Distributed Training System with Tasks Scheduling},
  author={Shi, Shaohuai and Pan, Xinglin and Wang, Qiang and Liu, Chengjian and Ren, Xiaozhe and Hu, Zhongzhe and Yang, Yu and Li, Bo and Chu, Xiaowen},
  booktitle={Proceedings of the Nineteenth European Conference on Computer Systems},
  pages={236--249},
  year={2024}
}

@inproceedings{shi2023pipemoe,
  title={PipeMoE: Accelerating Mixture-of-Experts through Adaptive Pipelining},
  author={Shi, Shaohuai and Pan, Xinglin and Chu, Xiaowen and Li, Bo},
  booktitle={IEEE INFOCOM 2023-IEEE Conference on Computer Communications},
  pages={1--10},
  year={2023},
  organization={IEEE}
}

@article{punniyamurthy2023optimizing,
  title={Optimizing Distributed ML Communication with Fused Computation-Collective Operations},
  author={Punniyamurthy, Kishore and Hamidouche, Khaled and Beckmann, Bradford M},
  journal={arXiv preprint arXiv:2305.06942},
  year={2023}
}

@inproceedings{caiflextron,
  title={Flextron: Many-in-One Flexible Large Language Model},
  author={Cai, Ruisi and Muralidharan, Saurav and Heinrich, Greg and Yin, Hongxu and Wang, Zhangyang and Kautz, Jan and Molchanov, Pavlo},
  booktitle={Forty-first International Conference on Machine Learning}
}

@inproceedings{zhang2024m3oe,
  title={M3oE: Multi-Domain Multi-Task Mixture-of Experts Recommendation Framework},
  author={Zhang, Zijian and Liu, Shuchang and Yu, Jiaao and Cai, Qingpeng and Zhao, Xiangyu and Zhang, Chunxu and Liu, Ziru and Liu, Qidong and Zhao, Hongwei and Hu, Lantao and others},
  booktitle={Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  pages={893--902},
  year={2024}
}

@article{zhao2024hypermoe,
  title={HyperMoE: Towards Better Mixture of Experts via Transferring Among Experts},
  author={Zhao, Hao and Qiu, Zihan and Wu, Huijia and Wang, Zili and He, Zhaofeng and Fu, Jie},
  journal={arXiv preprint arXiv:2402.12656},
  year={2024}
}

@article{du2024mogu,
  title={MoGU: A Framework for Enhancing Safety of Open-Sourced LLMs While Preserving Their Usability},
  author={Du, Yanrui and Zhao, Sendong and Zhao, Danyang and Ma, Ming and Chen, Yuhan and Huo, Liangyu and Yang, Qing and Xu, Dongliang and Qin, Bing},
  journal={arXiv preprint arXiv:2405.14488},
  year={2024}
}

@article{xu2024meteora,
  title={MeteoRA: Multiple-tasks Embedded LoRA for Large Language Models},
  author={Xu, Jingwei and Lai, Junyu and Huang, Yunpeng},
  journal={arXiv preprint arXiv:2405.13053},
  year={2024}
}

@inproceedings{yu2024mario,
  title={Language Models are Super Mario: Absorbing Abilities from Homologous Models as a Free Lunch},
  author={Yu, Le and Yu, Bowen and Yu, Haiyang and Huang, Fei and Li, Yongbin},
  booktitle={International Conference on Machine Learning},
  year={2024},
  organization={PMLR}
}
@inproceedings{hu2022lora,
title={Lo{RA}: Low-Rank Adaptation of Large Language Models},
author={Edward J Hu and yelong shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=nZeVKeeFYf9}
}
@inproceedings{isik2023gptzip,
  title={Gpt-zip: Deep compression of finetuned large language models},
  author={Isik, Berivan and Kumbong, Hermann and Ning, Wanyi and Yao, Xiaozhe and Koyejo, Sanmi and Zhang, Ce},
  booktitle={Workshop on Efficient Systems for Foundation Models@ ICML2023},
  year={2023}
}
@inproceedings{frantar2023gptq,
title={{OPTQ}: Accurate Quantization for Generative Pre-trained Transformers},
author={Elias Frantar and Saleh Ashkboos and Torsten Hoefler and Dan Alistarh},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=tcbBPnfwxS}
}
@article{yao2023deltazip,
  title={DeltaZip: Multi-Tenant Language Model Serving via Delta Compression},
  author={Yao, Xiaozhe and Klimovic, Ana},
  journal={arXiv preprint arXiv:2312.05215},
  year={2023}
}
@article{liu2024bitdelta,
  title={BitDelta: Your Fine-Tune May Only Be Worth One Bit},
  author={Liu, James and Xiao, Guangxuan and Li, Kai and Lee, Jason D and Han, Song and Dao, Tri and Cai, Tianle},
  journal={arXiv preprint arXiv:2402.10193},
  year={2024}
}
@article{ryu2023lowrank,
  title={Efficient Storage of Fine-Tuned Models via Low-Rank Approximation of Weight Residuals},
  author={Ryu, Simo and Seo, Seunghyun and Yoo, Jaejun},
  journal={arXiv preprint arXiv:2305.18425},
  year={2023}
}
@inproceedings{chee2023quip,
title={Qu{IP}: 2-Bit Quantization of Large Language Models With Guarantees},
author={Jerry Chee and Yaohui Cai and Volodymyr Kuleshov and Christopher De Sa},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=xrk9g5vcXR}
}
@inproceedings{shao2024omniquant,
title={OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models},
author={Wenqi Shao and Mengzhao Chen and Zhaoyang Zhang and Peng Xu and Lirui Zhao and Zhiqian Li and Kaipeng Zhang and Peng Gao and Yu Qiao and Ping Luo},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=8Wuvhh0LYW}
}
@article{cobbe2021verifier,
  title={Training verifiers to solve math word problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and others},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}
@article{hendrycksmath2021,
  title={Measuring Mathematical Problem Solving With the MATH Dataset},
  author={Dan Hendrycks and Collin Burns and Saurav Kadavath and Akul Arora and Steven Basart and Eric Tang and Dawn Song and Jacob Steinhardt},
  journal={NeurIPS},
  year={2021}
}
@article{chen2021evaluating,
  title={Evaluating large language models trained on code},
  author={Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and others},
  journal={arXiv preprint arXiv:2107.03374},
  year={2021}
}
@article{austin2021program,
  title={Program synthesis with large language models},
  author={Austin, Jacob and Odena, Augustus and Nye, Maxwell and Bosma, Maarten and Michalewski, Henryk and Dohan, David and Jiang, Ellen and Cai, Carrie and Terry, Michael and Le, Quoc and others},
  journal={arXiv preprint arXiv:2108.07732},
  year={2021}
}
@article{zhang2023safetybench,
      title={SafetyBench: Evaluating the Safety of Large Language Models with Multiple Choice Questions}, 
      author={Zhexin Zhang and Leqi Lei and Lindong Wu and Rui Sun and Yongkang Huang and Chong Long and Xiao Liu and Xuanyu Lei and Jie Tang and Minlie Huang},
      journal={arXiv preprint arXiv:2309.07045},
      year={2023}
}
@inproceedings{lin2022truthfulqa,
  title={TruthfulQA: Measuring How Models Mimic Human Falsehoods},
  author={Lin, Stephanie and Hilton, Jacob and Evans, Owain},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={3214--3252},
  year={2022}
}
@inproceedings{singh2019towards,
  title={Towards vqa models that can read},
  author={Singh, Amanpreet and Natarajan, Vivek and Shah, Meet and Jiang, Yu and Chen, Xinlei and Batra, Dhruv and Parikh, Devi and Rohrbach, Marcus},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={8317--8326},
  year={2019}
}
@inproceedings{hudson2019gqa,
  title={Gqa: A new dataset for real-world visual reasoning and compositional question answering},
  author={Hudson, Drew A and Manning, Christopher D},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={6700--6709},
  year={2019}
}
@article{touvron2023llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}
@misc{xu2023wizardlm,
      title={WizardLM: Empowering Large Language Models to Follow Complex Instructions}, 
      author={Can Xu and Qingfeng Sun and Kai Zheng and Xiubo Geng and Pu Zhao and Jiazhan Feng and Chongyang Tao and Daxin Jiang},
      year={2023},
      eprint={2304.12244},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@inproceedings{li2018measuring,
  title={Measuring the Intrinsic Dimension of Objective Landscapes},
  author={Li, Chunyuan and Farkhoor, Heerad and Liu, Rosanne and Yosinski, Jason},
  booktitle={International Conference on Learning Representations},
  year={2018}
}
@inproceedings{mahoney2019traditional,
  title={Traditional and heavy tailed self regularization in neural network models},
  author={Mahoney, Michael and Martin, Charles},
  booktitle={International Conference on Machine Learning},
  pages={4284--4293},
  year={2019},
  organization={PMLR}
}
@article{luo2023wizardmath,
  title={Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct},
  author={Luo, Haipeng and Sun, Qingfeng and Xu, Can and Zhao, Pu and Lou, Jianguang and Tao, Chongyang and Geng, Xiubo and Lin, Qingwei and Chen, Shifeng and Zhang, Dongmei},
  journal={arXiv preprint arXiv:2308.09583},
  year={2023}
}
@article{wei2023magicoder,
  title={Magicoder: Source code is all you need},
  author={Wei, Yuxiang and Wang, Zhe and Liu, Jiawei and Ding, Yifeng and Zhang, Lingming},
  journal={arXiv preprint arXiv:2312.02120},
  year={2023}
}
@article{luo2023wizardcoder,
  title={Wizardcoder: Empowering code large language models with evol-instruct},
  author={Luo, Ziyang and Xu, Can and Zhao, Pu and Sun, Qingfeng and Geng, Xiubo and Hu, Wenxiang and Tao, Chongyang and Ma, Jing and Lin, Qingwei and Jiang, Daxin},
  journal={arXiv preprint arXiv:2306.08568},
  year={2023}
}
@article{liu2024visual,
  title={Visual instruction tuning},
  author={Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
  journal={Advances in neural information processing systems},
  volume={36},
  year={2024}
}
@article{jiang2023mistral,
  title={Mistral 7B},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
  journal={arXiv preprint arXiv:2310.06825},
  year={2023}
}
@inproceedings{wang2023openchat,
  title={OpenChat: Advancing Open-source Language Models with Mixed-Quality Data},
  author={Wang, Guan and Cheng, Sijie and Zhan, Xianyuan and Li, Xiangang and Song, Sen and Liu, Yang},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2023}
}
@inproceedings{yu2023metamath,
  title={MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models},
  author={Yu, Longhui and Jiang, Weisen and Shi, Han and Jincheng, YU and Liu, Zhengying and Zhang, Yu and Kwok, James and Li, Zhenguo and Weller, Adrian and Liu, Weiyang},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2023}
}
@article{lin2023awq,
  title={Awq: Activation-aware weight quantization for llm compression and acceleration},
  author={Lin, Ji and Tang, Jiaming and Tang, Haotian and Yang, Shang and Dang, Xingyu and Han, Song},
  journal={arXiv preprint arXiv:2306.00978},
  year={2023}
}
@inproceedings{dettmers2023spqr,
  title={SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression},
  author={Dettmers, Tim and Svirschevski, Ruslan A and Egiazarian, Vage and Kuznedelev, Denis and Frantar, Elias and Ashkboos, Saleh and Borzunov, Alexander and Hoefler, Torsten and Alistarh, Dan},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2023}
}
@article{xu2024onebit,
  title={OneBit: Towards Extremely Low-bit Large Language Models},
  author={Xu, Yuzhuang and Han, Xu and Yang, Zonghan and Wang, Shuo and Zhu, Qingfu and Liu, Zhiyuan and Liu, Weidong and Che, Wanxiang},
  journal={arXiv preprint arXiv:2402.11295},
  year={2024}
}
@inproceedings{shen2024agile,
  title={Agile-Quant: Activation-Guided Quantization for Faster Inference of LLMs on the Edge},
  author={Shen, Xuan and Dong, Peiyan and Lu, Lei and Kong, Zhenglun and Li, Zhengang and Lin, Ming and Wu, Chao and Wang, Yanzhi},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={38},
  pages={18944--18951},
  year={2024}
}
@article{bablani2023efficient,
  title={Efficient and effective methods for mixed precision neural network quantization for faster, energy-efficient inference},
  author={Bablani, Deepika and Mckinstry, Jeffrey L and Esser, Steven K and Appuswamy, Rathinakumar and Modha, Dharmendra S},
  journal={arXiv preprint arXiv:2301.13330},
  year={2023}
}
@inproceedings{yao2021hawq,
  title={Hawq-v3: Dyadic neural network quantization},
  author={Yao, Zhewei and Dong, Zhen and Zheng, Zhangcheng and Gholami, Amir and Yu, Jiali and Tan, Eric and Wang, Leyuan and Huang, Qijing and Wang, Yida and Mahoney, Michael and others},
  booktitle={International Conference on Machine Learning},
  pages={11875--11886},
  year={2021},
  organization={PMLR}
}
@article{ryu2023efficient,
  title={Efficient Storage of Fine-Tuned Models via Low-Rank Approximation of Weight Residuals},
  author={Ryu, Simo and Seo, Seunghyun and Yoo, Jaejun},
  journal={arXiv preprint arXiv:2305.18425},
  year={2023}
}
@inproceedings{aghajanyan2021intrinsic,
  title={Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning},
  author={Aghajanyan, Armen and Gupta, Sonal and Zettlemoyer, Luke},
  booktitle={Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
  pages={7319--7328},
  year={2021}
}
@inproceedings{li2021prefix,
  title={Prefix-Tuning: Optimizing Continuous Prompts for Generation},
  author={Li, Xiang Lisa and Liang, Percy},
  booktitle={Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
  pages={4582--4597},
  year={2021}
}
@article{cobbe2021training,
  title={Training verifiers to solve math word problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and others},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}
@inproceedings{wu2023mole,
  title={MoLE: Mixture of LoRA Experts},
  author={Wu, Xun and Huang, Shaohan and Wei, Furu},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2023}
}
@misc{chen2021codex,
      title={Evaluating Large Language Models Trained on Code}, 
      author={Mark Chen and Jerry Tworek and Heewoo Jun and Qiming Yuan and Henrique Ponde de Oliveira Pinto and Jared Kaplan and Harri Edwards and Yuri Burda and Nicholas Joseph and Greg Brockman and Alex Ray and Raul Puri and Gretchen Krueger and Michael Petrov and Heidy Khlaaf and Girish Sastry and Pamela Mishkin and Brooke Chan and Scott Gray and Nick Ryder and Mikhail Pavlov and Alethea Power and Lukasz Kaiser and Mohammad Bavarian and Clemens Winter and Philippe Tillet and Felipe Petroski Such and Dave Cummings and Matthias Plappert and Fotios Chantzis and Elizabeth Barnes and Ariel Herbert-Voss and William Hebgen Guss and Alex Nichol and Alex Paino and Nikolas Tezak and Jie Tang and Igor Babuschkin and Suchir Balaji and Shantanu Jain and William Saunders and Christopher Hesse and Andrew N. Carr and Jan Leike and Josh Achiam and Vedant Misra and Evan Morikawa and Alec Radford and Matthew Knight and Miles Brundage and Mira Murati and Katie Mayer and Peter Welinder and Bob McGrew and Dario Amodei and Sam McCandlish and Ilya Sutskever and Wojciech Zaremba},
      year={2021},
      eprint={2107.03374},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@inproceedings{tillet2019triton,
  title={Triton: an intermediate language and compiler for tiled neural network computations},
  author={Tillet, Philippe and Kung, Hsiang-Tsung and Cox, David},
  booktitle={Proceedings of the 3rd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages},
  pages={10--19},
  year={2019}
}
@misc{wei2024tmaccpurenaissancetable,
      title={T-MAC: CPU Renaissance via Table Lookup for Low-Bit LLM Deployment on Edge}, 
      author={Jianyu Wei and Shijie Cao and Ting Cao and Lingxiao Ma and Lei Wang and Yanyong Zhang and Mao Yang},
      year={2024},
      eprint={2407.00088},
      archivePrefix={arXiv},
      primaryClass={cs.DC},
      url={https://arxiv.org/abs/2407.00088}, 
}
@misc{guo2024fastmatrixmultiplicationslookup,
      title={Fast Matrix Multiplications for Lookup Table-Quantized LLMs}, 
      author={Han Guo and William Brandon and Radostin Cholakov and Jonathan Ragan-Kelley and Eric P. Xing and Yoon Kim},
      year={2024},
      eprint={2407.10960},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2407.10960}, 
}

@article{jiang2024deltadq,
  title={DeltaDQ: Ultra-High Delta Compression for Fine-Tuned LLMs via Group-wise Dropout and Separate Quantization},
  author={Jiang, Yanfeng and Yang, Zelan and Chen, Bohua and Li, Shen and Li, Yong and Li, Tao},
  journal={arXiv preprint arXiv:2410.08666},
  year={2024}
}
@inproceedings{ping2024deltacome,
      title={Delta-CoMe: Training-Free Delta-Compression with Mixed-Precision for Large Language Models}, 
      author={Bowen Ping and Shuo Wang and Hanqing Wang and Xu Han and Yuzhuang Xu and Yukun Yan and Yun Chen and Baobao Chang and Zhiyuan Liu and Maosong Sun},
      booktitle={Thirty-eighth Conference on Neural Information Processing Systems},
      year={2024},
}

@inproceedings{wang-etal-2024-lora-flow,
    title = "{L}o{RA}-Flow: Dynamic {L}o{RA} Fusion for Large Language Models in Generative Tasks",
    author = "Wang, Hanqing  and
      Ping, Bowen  and
      Wang, Shuo  and
      Han, Xu  and
      Chen, Yun  and
      Liu, Zhiyuan  and
      Sun, Maosong",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.695",
    doi = "10.18653/v1/2024.acl-long.695",
    pages = "12871--12882",
    abstract = "LoRA employs lightweight modules to customize large language models (LLMs) for each downstream task or domain, where different learned additional modules represent diverse skills. Combining existing LoRAs to address new tasks can enhance the reusability of learned LoRAs, particularly beneficial for tasks with limited annotated data. Most prior works on LoRA combination primarily rely on task-level weights for each involved LoRA, making different examples and tokens share the same LoRA weights. However, in generative tasks, different tokens may necessitate diverse skills to manage. Taking the Chinese math task as an example, understanding the problem description may depend more on the Chinese LoRA, while the calculation part may rely more on the math LoRA. To this end, we propose LoRA-Flow, which utilizes dynamic weights to adjust the impact of different LoRAs. The weights at each step are determined by a fusion gate with extremely few parameters, which can be learned with only 200 training examples. Experiments across six generative tasks demonstrate that our method consistently outperforms baselines with task-level fusion weights. This underscores the necessity of introducing dynamic fusion weights for LoRA combination.",
}


@InProceedings{pmlr-v202-li23ap,
  title = 	 {{L}o{S}parse: Structured Compression of Large Language Models based on Low-Rank and Sparse Approximation},
  author =       {Li, Yixiao and Yu, Yifan and Zhang, Qingru and Liang, Chen and He, Pengcheng and Chen, Weizhu and Zhao, Tuo},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {20336--20350},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/li23ap/li23ap.pdf},
  url = 	 {https://proceedings.mlr.press/v202/li23ap.html},
  abstract = 	 {Transformer models have achieved remarkable results in various natural language tasks, but they are often prohibitively large, requiring massive memories and computational resources. To re- duce the size and complexity of these models, we propose LoSparse (Low-Rank and Sparse ap- proximation), a novel model compression tech- nique that approximates a weight matrix by the sum of a low-rank matrix and a sparse matrix. Our method combines the advantages of both low- rank approximations and pruning, while avoid- ing their limitations. Low-rank approximation compresses the coherent and expressive parts in neurons, while pruning removes the incoherent and non-expressive parts in neurons. Pruning enhances the diversity of low-rank approxima- tions, and low-rank approximation prevents prun- ing from losing too many expressive neurons. We evaluate our method on natural language under- standing, question answering, and natural lan- guage generation tasks. We show that it signif- icantly outperforms existing compression meth- ods. Our code is publicly available at https: //github.com/yxli2123/LoSparse}
}


@article{lee2024stun,
  title={STUN: Structured-Then-Unstructured Pruning for Scalable MoE Pruning},
  author={Lee, Jaeseong and Qiao, Aurick and Campos, Daniel F and Yao, Zhewei and He, Yuxiong and others},
  journal={arXiv preprint arXiv:2409.06211},
  year={2024}
}


@inproceedings{kornblith2019similarity,
  title={Similarity of neural network representations revisited},
  author={Kornblith, Simon and Norouzi, Mohammad and Lee, Honglak and Hinton, Geoffrey},
  booktitle={International conference on machine learning},
  pages={3519--3529},
  year={2019},
  organization={PMLR}
}



% Journals

% First the Full Name is given, then the abbreviation used in the AMS Math
% Reviews, with an indication if it could not be found ries



@comment{Aixin Liu and Bei Feng and Bing Xue and Bingxuan Wang and Bochao Wu and Chengda Lu and Chenggang Zhao and Chengqi Deng and Chenyu Zhang and Chong Ruan and Damai Dai and Daya Guo and Dejian Yang and Deli Chen and Dongjie Ji and Erhang Li and Fangyun Lin and Fucong Dai and Fuli Luo and Guangbo Hao and Guanting Chen and Guowei Li and H. Zhang and Han Bao and Hanwei Xu and Haocheng Wang and Haowei Zhang and Honghui Ding and Huajian Xin and Huazuo Gao and Hui Li and Hui Qu and J. L. Cai and Jian Liang and Jianzhong Guo and Jiaqi Ni and Jiashi Li and Jiawei Wang and Jin Chen and Jingchang Chen and Jingyang Yuan and Junjie Qiu and Junlong Li and Junxiao Song and Kai Dong and Kai Hu and Kaige Gao and Kang Guan and Kexin Huang and Kuai Yu and Lean Wang and Lecong Zhang and Lei Xu and Leyi Xia and Liang Zhao and Litong Wang and Liyue Zhang and Meng Li and Miaojun Wang and Mingchuan Zhang and Minghua Zhang and Minghui Tang and Mingming Li and Ning Tian and Panpan Huang and Peiyi Wang and Peng Zhang and Qiancheng Wang and Qihao Zhu and Qinyu Chen and Qiushi Du and R. J. Chen and R. L. Jin and Ruiqi Ge and Ruisong Zhang and Ruizhe Pan and Runji Wang and Runxin Xu and Ruoyu Zhang and Ruyi Chen and S. S. Li and Shanghao Lu and Shangyan Zhou and Shanhuang Chen and Shaoqing Wu and Shengfeng Ye and Shengfeng Ye and Shirong Ma and Shiyu Wang and Shuang Zhou and Shuiping Yu and Shunfeng Zhou and Shuting Pan and T. Wang and Tao Yun and Tian Pei and Tianyu Sun and W. L. Xiao and Wangding Zeng and Wanjia Zhao and Wei An and Wen Liu and Wenfeng Liang and Wenjun Gao and Wenqin Yu and Wentao Zhang and X. Q. Li and Xiangyue Jin and Xianzu Wang and Xiao Bi and Xiaodong Liu and Xiaohan Wang and Xiaojin Shen and Xiaokang Chen and Xiaokang Zhang and Xiaosha Chen and Xiaotao Nie and Xiaowen Sun and Xiaoxiang Wang and Xin Cheng and Xin Liu and Xin Xie and Xingchao Liu and Xingkai Yu and Xinnan Song and Xinxia Shan and Xinyi Zhou and Xinyu Yang and Xinyuan Li and Xuecheng Su and Xuheng Lin and Y. K. Li and Y. Q. Wang and Y. X. Wei and Y. X. Zhu and Yang Zhang and Yanhong Xu and Yanhong Xu and Yanping Huang and Yao Li and Yao Zhao and Yaofeng Sun and Yaohui Li and Yaohui Wang and Yi Yu and Yi Zheng and Yichao Zhang and Yifan Shi and Yiliang Xiong and Ying He and Ying Tang and Yishi Piao and Yisong Wang and Yixuan Tan and Yiyang Ma and Yiyuan Liu and Yongqiang Guo and Yu Wu and Yuan Ou and Yuchen Zhu and Yuduan Wang and Yue Gong and Yuheng Zou and Yujia He and Yukun Zha and Yunfan Xiong and Yunxian Ma and Yuting Yan and Yuxiang Luo and Yuxiang You and Yuxuan Liu and Yuyang Zhou and Z. F. Wu and Z. Z. Ren and Zehui Ren and Zhangli Sha and Zhe Fu and Zhean Xu and Zhen Huang and Zhen Zhang and Zhenda Xie and Zhengyan Zhang and Zhewen Hao and Zhibin Gou and Zhicheng Ma and Zhigang Yan and Zhihong Shao and Zhipeng Xu and Zhiyu Wu and Zhongyu Zhang and Zhuoshu Li and Zihui Gu and Zijia Zhu and Zijun Liu and Zilin Li and Ziwei Xie and Ziyang Song and Ziyi Gao and Zizheng Pan}
@article{liu2024deepseek,
  title={DeepSeek-V3 Technical Report},
  author={DeepSeek-AI and others},
  journal={arXiv preprint arXiv:2412.19437},
  year={2024}
}


%% expert parallel

@article{qian2024eps,
  title={EPS-MoE: Expert Pipeline Scheduler for Cost-Efficient MoE Inference},
  author={Qian, Yulei and Li, Fengcun and Ji, Xiangyang and Zhao, Xiaoyu and Tan, Jianchao and Zhang, Kefeng and Cai, Xunliang},
  journal={arXiv preprint arXiv:2410.12247},
  year={2024}
}

@inproceedings{yao2024exploiting,
  title={Exploiting inter-layer expert affinity for accelerating mixture-of-experts model inference},
  author={Yao, Jinghan and Anthony, Quentin and Shafi, Aamir and Subramoni, Hari and Panda, Dhabaleswar K DK},
  booktitle={2024 IEEE International Parallel and Distributed Processing Symposium (IPDPS)},
  pages={915--925},
  year={2024},
  organization={IEEE}
}

@article{li2024optimizing,
  title={Optimizing Mixture-of-Experts Inference Time Combining Model Deployment and Communication Scheduling},
  author={Li, Jialong and Tripathi, Shreyansh and Rastogi, Lakshay and Lei, Yiming and Pan, Rui and Xia, Yiting},
  journal={arXiv preprint arXiv:2410.17043},
  year={2024}
}

@article{wu2024lazarus,
  title={Lazarus: Resilient and elastic training of mixture-of-experts models with adaptive expert placement},
  author={Wu, Yongji and Qu, Wenjie and Tao, Tianyang and Wang, Zhuang and Bai, Wei and Li, Zhuohao and Tian, Yuan and Zhang, Jiaheng and Lentz, Matthew and Zhuo, Danyang},
  journal={arXiv preprint arXiv:2407.04656},
  year={2024}
}


@article{HEAX,
  title={HEAX-MOE: Efficient and Heterogeneous-aware MoE Acceleration with ZERO Computation Redundancy},
  author={Luo, Shuqing and Peng, Jie and Li, Pingzhi and Wang, Hanrui and Chen, Tianlong},
  journal={arXiv preprint arXiv:2411.01288},
  year={2024}
}


@article{luo2024texttt,
  title={HEXA-MoE: Efficient and Heterogeneous-aware MoE Acceleration with ZERO Computation Redundancy},
  author={Luo, Shuqing and Peng, Jie and Li, Pingzhi and Wang, Hanrui and Chen, Tianlong},
  journal={arXiv preprint arXiv:2411.01288},
  year={2024}
}

@article{li2024locmoe,
  title={Locmoe: A low-overhead moe for large language model training},
  author={Li, Jing and Sun, Zhijie and He, Xuan and Zeng, Li and Lin, Yi and Li, Entong and Zheng, Binfan and Zhao, Rongqian and Chen, Xin},
  journal={arXiv preprint arXiv:2401.13920},
  year={2024}
}

@ARTICLE{10494556,
  author={Zhang, Zheng and Xia, Yaqi and Wang, Hulin and Yang, Donglin and Hu, Chuang and Zhou, Xiaobo and Cheng, Dazhao},
  journal={IEEE Transactions on Parallel and Distributed Systems}, 
  title={MPMoE: Memory Efficient MoE for Pre-Trained Models With Adaptive Pipeline Parallelism}, 
  year={2024},
  volume={35},
  number={6},
  pages={998-1011},
  keywords={Training;Computational modeling;Memory management;Pipeline processing;Graphics processing units;Transformers;Pipelines;Mixture of experts;pipeline parallelism;distributed training;memory redundancy;performance model},
  doi={10.1109/TPDS.2024.3385639}}


@INPROCEEDINGS{10621327,
  author={Pan, Xinglin and Lin, Wenxiang and Shi, Shaohuai and Chu, Xiaowen and Sun, Weinong and Li, Bo},
  booktitle={IEEE INFOCOM 2024 - IEEE Conference on Computer Communications}, 
  title={Parm: Efficient Training of Large Sparsely-Activated Models with Dedicated Schedules}, 
  year={2024},
  volume={},
  number={},
  pages={1880-1889},
  keywords={Training;Schedules;Costs;Processor scheduling;Computational modeling;Prototypes;Graphics processing units;Large Language Models;Distributed Training;Mixture-of-Experts;Task Scheduling},
  doi={10.1109/INFOCOM52122.2024.10621327}}


@inproceedings{10.1145/3627703.3650083,
author = {Shi, Shaohuai and Pan, Xinglin and Wang, Qiang and Liu, Chengjian and Ren, Xiaozhe and Hu, Zhongzhe and Yang, Yu and Li, Bo and Chu, Xiaowen},
title = {ScheMoE: An Extensible Mixture-of-Experts Distributed Training System with Tasks Scheduling},
year = {2024},
isbn = {9798400704376},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3627703.3650083},
doi = {10.1145/3627703.3650083},
booktitle = {Proceedings of the Nineteenth European Conference on Computer Systems},
pages = {236–249},
numpages = {14},
keywords = {Distributed Deep Learning, Large Language Model, Mixture-of-Experts, Scheduling},
location = {Athens, Greece},
series = {EuroSys '24}
}

@inproceedings{10.1145/3603269.3604869,
author = {Liu, Juncai and Wang, Jessie Hui and Jiang, Yimin},
title = {Janus: A Unified Distributed Training Framework for Sparse Mixture-of-Experts Models},
year = {2023},
isbn = {9798400702365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3603269.3604869},
doi = {10.1145/3603269.3604869},
booktitle = {Proceedings of the ACM SIGCOMM 2023 Conference},
pages = {486–498},
numpages = {13},
keywords = {distributed training, mixture of experts, deep learning},
location = {New York, NY, USA},
series = {ACM SIGCOMM '23}
}

@INPROCEEDINGS{10228874,
  author={Shi, Shaohuai and Pan, Xinglin and Chu, Xiaowen and Li, Bo},
  booktitle={IEEE INFOCOM 2023 - IEEE Conference on Computer Communications}, 
  title={PipeMoE: Accelerating Mixture-of-Experts through Adaptive Pipelining}, 
  year={2023},
  volume={},
  number={},
  pages={1-10},
  keywords={Training;Adaptation models;Codes;Computational modeling;Clustering algorithms;Data models;Computational efficiency;Distributed Deep Learning;Communication-Efficient Training;Mixture-of-Experts;Pipelining},
  doi={10.1109/INFOCOM53939.2023.10228874}}

@inproceedings {288705,
author = {Jiamin Li and Yimin Jiang and Yibo Zhu and Cong Wang and Hong Xu},
title = {Accelerating Distributed {MoE} Training and Inference with Lina},
booktitle = {2023 USENIX Annual Technical Conference (USENIX ATC 23)},
year = {2023},
isbn = {978-1-939133-35-9},
address = {Boston, MA},
pages = {945--959},
url = {https://www.usenix.org/conference/atc23/presentation/li-jiamin},
publisher = {USENIX Association},
month = jul
}


@inproceedings {288691,
author = {Mingshu Zhai and Jiaao He and Zixuan Ma and Zan Zong and Runqing Zhang and Jidong Zhai},
title = {{SmartMoE}: Efficiently Training {Sparsely-Activated} Models through Combining Offline and Online Parallelization},
booktitle = {2023 USENIX Annual Technical Conference (USENIX ATC 23)},
year = {2023},
isbn = {978-1-939133-35-9},
address = {Boston, MA},
pages = {961--975},
url = {https://www.usenix.org/conference/atc23/presentation/zhai},
publisher = {USENIX Association},
month = jul
}

@article{chen2022ta,
  title={Ta-moe: Topology-aware large scale mixture-of-expert training},
  author={Chen, Chang and Li, Min and Wu, Zhihua and Yu, Dianhai and Yang, Chao},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={22173--22186},
  year={2022}
}

@article{nie2023flexmoe,
  title={Flexmoe: Scaling large-scale sparse pre-trained model training via dynamic device placement},
  author={Nie, Xiaonan and Miao, Xupeng and Wang, Zilong and Yang, Zichao and Xue, Jilong and Ma, Lingxiao and Cao, Gang and Cui, Bin},
  journal={Proceedings of the ACM on Management of Data},
  volume={1},
  number={1},
  pages={1--19},
  year={2023},
  publisher={ACM New York, NY, USA}
}


@inproceedings{10.1145/3503221.3508418,
author = {He, Jiaao and Zhai, Jidong and Antunes, Tiago and Wang, Haojie and Luo, Fuwen and Shi, Shangfeng and Li, Qin},
title = {FasterMoE: modeling and optimizing training of large-scale dynamic pre-trained models},
year = {2022},
isbn = {9781450392044},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503221.3508418},
doi = {10.1145/3503221.3508418},
booktitle = {Proceedings of the 27th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
pages = {120–134},
numpages = {15},
keywords = {distributed deep learning, parallelism, performance modeling},
location = {Seoul, Republic of Korea},
series = {PPoPP '22}
}

@article{hwang2023tutel,
  title={Tutel: Adaptive mixture-of-experts at scale},
  author={Hwang, Changho and Cui, Wei and Xiong, Yifan and Yang, Ziyue and Liu, Ze and Hu, Han and Wang, Zilong and Salas, Rafael and Jose, Jithin and Ram, Prabhat and others},
  journal={Proceedings of Machine Learning and Systems},
  volume={5},
  pages={269--287},
  year={2023}
}

@article{lepikhin2020gshard,
  title={Gshard: Scaling giant models with conditional computation and automatic sharding},
  author={Lepikhin, Dmitry and Lee, HyoukJoong and Xu, Yuanzhong and Chen, Dehao and Firat, Orhan and Huang, Yanping and Krikun, Maxim and Shazeer, Noam and Chen, Zhifeng},
  journal={arXiv preprint arXiv:2006.16668},
  year={2020}
}

@article{nie2022hetumoe,
  title={HetuMoE: An efficient trillion-scale mixture-of-expert distributed training system},
  author={Nie, Xiaonan and Zhao, Pinxue and Miao, Xupeng and Zhao, Tong and Cui, Bin},
  journal={arXiv preprint arXiv:2203.14685},
  year={2022}
}

@article{he2021fastmoe,
  title={Fastmoe: A fast mixture-of-expert training system},
  author={He, Jiaao and Qiu, Jiezhong and Zeng, Aohan and Yang, Zhilin and Zhai, Jidong and Tang, Jie},
  journal={arXiv preprint arXiv:2103.13262},
  year={2021}
}

@inproceedings{ibrahim2023comet,
  title={Comet: Learning cardinality constrained mixture of experts with trees and local search},
  author={Ibrahim, Shibal and Chen, Wenyu and Hazimeh, Hussein and Ponomareva, Natalia and Zhao, Zhe and Mazumder, Rahul},
  booktitle={Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  pages={832--844},
  year={2023}
}

@ARTICLE{10528887,
  author={Yu, Dianhai and Shen, Liang and Hao, Hongxiang and Gong, Weibao and Wu, Huachao and Bian, Jiang and Dai, Lirong and Xiong, Haoyi},
  journal={IEEE Transactions on Services Computing}, 
  title={MoESys: A Distributed and Efficient Mixture-of-Experts Training and Inference System for Internet Services}, 
  year={2024},
  volume={17},
  number={5},
  pages={2626-2639},
  keywords={Training;Computational modeling;Biological system modeling;Data models;Web and internet services;Memory management;Load modeling;Distributed inference;distributed training;large models for internet services;MoE},
  doi={10.1109/TSC.2024.3399654}}




@article{shazeer2017outrageously,
  title={Outrageously large neural networks: The sparsely-gated mixture-of-experts layer},
  author={Shazeer, Noam and Mirhoseini, Azalia and Maziarz, Krzysztof and Davis, Andy and Le, Quoc and Hinton, Geoffrey and Dean, Jeff},
  journal={arXiv preprint arXiv:1701.06538},
  year={2017}
}

@article{fedus2022switch,
  title={Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity},
  author={Fedus, William and Zoph, Barret and Shazeer, Noam},
  journal={Journal of Machine Learning Research},
  volume={23},
  number={120},
  pages={1--39},
  year={2022}
}


@article{roller2021hash,
  title={Hash layers for large sparse models},
  author={Roller, Stephen and Sukhbaatar, Sainbayar and Szlam, Arthur and Weston, Jason},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={17555--17566},
  year={2021}
}




@article{gupta2024lynx,
  title={Lynx: Enabling Efficient MoE Inference through Dynamic Batch-Aware Expert Selection},
  author={Gupta, Vima and Sinha, Kartik and Gavrilovska, Ada and Iyer, Anand Padmanabha},
  journal={arXiv preprint arXiv:2411.08982},
  year={2024}
}



@InProceedings{DeepSpeedmoe,
  title = 	 {DeepSpeed-MoE: Advancing Mixture-of-Experts Inference and Training to Power Next-Generation {AI} Scale},
  author =       {Rajbhandari, Samyam and Li, Conglong and Yao, Zhewei and Zhang, Minjia and Aminabadi, Reza Yazdani and Awan, Ammar Ahmad and Rasley, Jeff and He, Yuxiong},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {18332--18346},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/rajbhandari22a/rajbhandari22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/rajbhandari22a.html},
}

@inproceedings{BaGuaLu,
author = {Ma, Zixuan and He, Jiaao and Qiu, Jiezhong and Cao, Huanqi and Wang, Yuanwei and Sun, Zhenbo and Zheng, Liyan and Wang, Haojie and Tang, Shizhi and Zheng, Tianyu and Lin, Junyang and Feng, Guanyu and Huang, Zeqiang and Gao, Jie and Zeng, Aohan and Zhang, Jianwei and Zhong, Runxin and Shi, Tianhui and Liu, Sha and Zheng, Weimin and Tang, Jie and Yang, Hongxia and Liu, Xin and Zhai, Jidong and Chen, Wenguang},
title = {BaGuaLu: targeting brain scale pretrained models with over 37 million cores},
year = {2022},
isbn = {9781450392044},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503221.3508417},
doi = {10.1145/3503221.3508417},
booktitle = {Proceedings of the 27th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
pages = {192–204},
numpages = {13},
keywords = {artificial intelligence, heterogeneous architecture, mixture of experts, supercomputers},
location = {Seoul, Republic of Korea},
series = {PPoPP '22}
}


@inproceedings{lewis2021base,
  title={Base layers: Simplifying training of large, sparse models},
  author={Lewis, Mike and Bhosale, Shruti and Dettmers, Tim and Goyal, Naman and Zettlemoyer, Luke},
  booktitle={International Conference on Machine Learning},
  pages={6265--6274},
  year={2021},
  organization={PMLR}
}

@inproceedings {Brainstorm,
author = {Weihao Cui and Zhenhua Han and Lingji Ouyang and Yichuan Wang and Ningxin Zheng and Lingxiao Ma and Yuqing Yang and Fan Yang and Jilong Xue and Lili Qiu and Lidong Zhou and Quan Chen and Haisheng Tan and Minyi Guo},
title = {Optimizing Dynamic Neural Networks with Brainstorm},
booktitle = {17th USENIX Symposium on Operating Systems Design and Implementation (OSDI 23)},
year = {2023},
isbn = {978-1-939133-34-2},
address = {Boston, MA},
pages = {797--815},
url = {https://www.usenix.org/conference/osdi23/presentation/cui},
publisher = {USENIX Association},
month = jul
}

@article{moeecr,
  title={Mixture-of-experts with expert choice routing},
  author={Zhou, Yanqi and Lei, Tao and Liu, Hanxiao and Du, Nan and Huang, Yanping and Zhao, Vincent and Dai, Andrew M and Le, Quoc V and Laudon, James},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={7103--7114},
  year={2022}
}


@inproceedings{HiDup,
  title={Accelerating large-scale distributed neural network training with SPMD parallelism},
  author={Zhang, Shiwei and Diao, Lansong and Wu, Chuan and Wang, Siyu and Lin, Wei},
  booktitle={Proceedings of the 13th Symposium on Cloud Computing},
  pages={403--418},
  year={2022}
}


@inproceedings{GatingDropout,
  title={Gating dropout: Communication-efficient regularization for sparsely activated transformers},
  author={Liu, Rui and Kim, Young Jin and Muzio, Alexandre and Hassan, Hany},
  booktitle={International Conference on Machine Learning},
  pages={13782--13792},
  year={2022},
  organization={PMLR}
}


@inproceedings{DeepSpeedted,
  title={A hybrid tensor-expert-data parallelism approach to optimize mixture-of-experts training},
  author={Singh, Siddharth and Ruwase, Olatunji and Awan, Ammar Ahmad and Rajbhandari, Samyam and He, Yuxiong and Bhatele, Abhinav},
  booktitle={Proceedings of the 37th International Conference on Supercomputing},
  pages={203--214},
  year={2023}
}


@inproceedings {Alpa,
author = {Lianmin Zheng and Zhuohan Li and Hao Zhang and Yonghao Zhuang and Zhifeng Chen and Yanping Huang and Yida Wang and Yuanzhong Xu and Danyang Zhuo and Eric P. Xing and Joseph E. Gonzalez and Ion Stoica},
title = {Alpa: Automating Inter- and {Intra-Operator} Parallelism for Distributed Deep Learning},
booktitle = {16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22)},
year = {2022},
isbn = {978-1-939133-28-1},
address = {Carlsbad, CA},
pages = {559--578},
url = {https://www.usenix.org/conference/osdi22/presentation/zheng-lianmin},
publisher = {USENIX Association},
month = jul
}

%%expert offloading

@article{tang2024hobbit,
  title={HOBBIT: A Mixed Precision Expert Offloading System for Fast MoE Inference},
  author={Tang, Peng and Liu, Jiacheng and Hou, Xiaofeng and Pu, Yifei and Wang, Jing and Heng, Pheng-Ann and Li, Chao and Guo, Minyi},
  journal={arXiv preprint arXiv:2411.01433},
  year={2024}
}
@article{he2024expertflow,
  title={ExpertFlow: Optimized Expert Activation and Token Allocation for Efficient Mixture-of-Experts Inference},
  author={He, Xin and Zhang, Shunkang and Wang, Yuxin and Yin, Haiyan and Zeng, Zihao and Shi, Shaohuai and Tang, Zhenheng and Chu, Xiaowen and Tsang, Ivor and Soon, Ong Yew},
  journal={arXiv preprint arXiv:2410.17954},
  year={2024}
}
@article{zhong2024adapmoe,
  title={AdapMoE: Adaptive Sensitivity-based Expert Gating and Management for Efficient MoE Inference},
  author={Zhong, Shuzhang and Liang, Ling and Wang, Yuan and Wang, Runsheng and Huang, Ru and Li, Meng},
  journal={arXiv preprint arXiv:2408.10284},
  year={2024}
}

@article{du2024sida,
  title={SiDA: Sparsity-Inspired Data-Aware Serving for Efficient and Scalable Large Mixture-of-Experts Models},
  author={Du, Zhixu and Li, Shiyu and Wu, Yuhao and Jiang, Xiangyu and Sun, Jingwei and Zheng, Qilin and Wu, Yongkai and Li, Ang and Li, Hai and Chen, Yiran},
  journal={Proceedings of Machine Learning and Systems},
  volume={6},
  pages={224--238},
  year={2024}
}

@article{xue2024moe,
  title={Moe-infinity: Activation-aware expert offloading for efficient moe serving},
  author={Xue, Leyang and Fu, Yao and Lu, Zhan and Mai, Luo and Marina, Mahesh},
  journal={arXiv preprint arXiv:2401.14361},
  year={2024}
}

@article{kamahori2024fiddler,
  title={Fiddler: CPU-GPU Orchestration for Fast Inference of Mixture-of-Experts Models},
  author={Kamahori, Keisuke and Gu, Yile and Zhu, Kan and Kasikci, Baris},
  journal={arXiv preprint arXiv:2402.07033},
  year={2024}
}

@article{yuan2024efficient,
  title={Efficient Inference Offloading for Mixture-of-Experts Large Language Models in Internet of Medical Things},
  author={Yuan, Xiaoming and Kong, Weixuan and Luo, Zhenyu and Xu, Minrui},
  journal={Electronics},
  volume={13},
  number={11},
  pages={2077},
  year={2024},
  publisher={MDPI}
}


@article{eliseev2023fast,
  title={Fast inference of mixture-of-experts language models with offloading},
  author={Eliseev, Artyom and Mazur, Denis},
  journal={arXiv preprint arXiv:2312.17238},
  year={2023}
}

@article{huang2023towards,
  title={Towards MoE Deployment: Mitigating Inefficiencies in Mixture-of-Expert (MoE) Inference},
  author={Huang, Haiyang and Ardalani, Newsha and Sun, Anna and Ke, Liu and Lee, Hsien-Hsin S and Sridhar, Anjali and Bhosale, Shruti and Wu, Carole-Jean and Lee, Benjamin},
  journal={arXiv preprint arXiv:2303.06182},
  year={2023}
}

@article{yi2023edgemoe,
  title={Edgemoe: Fast on-device inference of moe-based large language models},
  author={Yi, Rongjie and Guo, Liwei and Wei, Shiyun and Zhou, Ao and Wang, Shangguang and Xu, Mengwei},
  journal={arXiv preprint arXiv:2308.14352},
  year={2023}
}

@article{kong2023serving,
  title={Serving MoE Models on Resource-constrained Edge Devices via Dynamic Expert Swapping},
  author={Kong, Rui and Li, Yuanchun and Feng, Qingtian and Wang, Weijun and Kong, Linghe and Liu, Yunxin},
  journal={arXiv preprint arXiv:2308.15030},
  year={2023}
}

@article{xue2024wdmoe,
  title={WDMoE: Wireless Distributed Large Language Models with Mixture of Experts},
  author={Xue, Nan and Sun, Yaping and Chen, Zhiyong and Tao, Meixia and Xu, Xiaodong and Qian, Liang and Cui, Shuguang and Zhang, Ping},
  journal={arXiv preprint arXiv:2405.03131},
  year={2024}
}

@misc{cao2024moelight,
      title={MoE-Lightning: High-Throughput MoE Inference on Memory-constrained GPUs}, 
      author={Shiyi Cao and Shu Liu and Tyler Griggs and Peter Schafhalter and Xiaoxuan Liu and Ying Sheng and Joseph E. Gonzalez and Matei Zaharia and Ion Stoica},
      year={2024},
      eprint={2411.11217},
      archivePrefix={arXiv},
      primaryClass={cs.DC},
      url={https://arxiv.org/abs/2411.11217}, 
}



@misc{readmoe,
      title={Read-ME: Refactorizing LLMs as Router-Decoupled Mixture of Experts with System Co-Design}, 
      author={Ruisi Cai and Yeonju Ro and Geon-Woo Kim and Peihao Wang and Babak Ehteshami Bejnordi and Aditya Akella and Zhangyang Wang},
      year={2024},
      eprint={2410.19123},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.19123}, 
}


@INPROCEEDINGS{dynnoffload,
  author={Ren, Jie and Xu, Dong and Yang, Shuangyan and Zhao, Jiacheng and Li, Zhicheng and Navasca, Christian and Wang, Chenxi and Xu, Harry and Li, Dong},
  booktitle={2024 IEEE International Symposium on High-Performance Computer Architecture (HPCA)}, 
  title={Enabling Large Dynamic Neural Network Training with Learning-based Memory Management}, 
  year={2024},
  volume={},
  number={},
  pages={788-802},
  keywords={Training;Tensors;Computational modeling;Memory management;Neural networks;Graphics processing units;Predictive models;Memory Management;Dynamic Neural Network;Neural Network Training},
  doi={10.1109/HPCA57654.2024.00066}}

@article{song2024promoe,
  title={ProMoE: Fast MoE-based LLM Serving using Proactive Caching},
  author={Song, Xiaoniu and Zhong, Zihang and Chen, Rong},
  journal={arXiv preprint arXiv:2410.22134},
  year={2024}
}





%%% loading balancing

@article{cong2024prediction,
  title={Prediction Is All MoE Needs: Expert Load Distribution Goes from Fluctuating to Stabilizing},
  author={Cong, Peizhuang and Yuan, Aomufei and Chen, Shimao and Tian, Yuxuan and Ye, Bowen and Yang, Tong},
  journal={arXiv preprint arXiv:2404.16914},
  year={2024}
}

@INPROCEEDINGS{10319949,
  author={Wang, Wei and Lai, Zhiquan and Li, Shengwei and Liu, Weijie and Ge, Keshi and Liu, Yujie and Shen, Ao and Li, Dongsheng},
  booktitle={2023 IEEE International Conference on Cluster Computing (CLUSTER)}, 
  title={Prophet: Fine-grained Load Balancing for Parallel Training of Large-scale MoE Models}, 
  year={2023},
  volume={},
  number={},
  pages={82-94},
  keywords={Training;Schedules;Systematics;Computational modeling;Predictive models;Load management;Throughput;mixture of experts;distributed training},
  doi={10.1109/CLUSTER52292.2023.00015}}
@article{cai2024shortcut,
  title={Shortcut-connected Expert Parallelism for Accelerating Mixture-of-Experts},
  author={Cai, Weilin and Jiang, Juyong and Qin, Le and Cui, Junwei and Kim, Sunghun and Huang, Jiayi},
  journal={arXiv preprint arXiv:2404.05019},
  year={2024}
}

% hardware



@inproceedings{Lin2024flame,
author = {Lin, Xuanda and Tian, Huinan and Xue, Wenxiao and Ma, Lanqi and Cao, Jialin and Zhang, Manting and Yu, Jun and Wang, Kun},
title = {FLAME: Fully Leveraging MoE Sparsity for Transformer on FPGA},
year = {2024},
isbn = {9798400706011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649329.3656507},
doi = {10.1145/3649329.3656507},
booktitle = {Proceedings of the 61st ACM/IEEE Design Automation Conference},
articleno = {248},
numpages = {6},
location = {San Francisco, CA, USA},
series = {DAC '24}
}




@inproceedings{Kim2024monde,
author = {Kim, Taehyun and Choi, Kwanseok and Cho, Youngmock and Cho, Jaehoon and Lee, Hyuk-Jae and Sim, Jaewoong},
title = {MoNDE: Mixture of Near-Data Experts for Large-Scale Sparse Models},
year = {2024},
isbn = {9798400706011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649329.3655951},
doi = {10.1145/3649329.3655951},
booktitle = {Proceedings of the 61st ACM/IEEE Design Automation Conference},
articleno = {221},
numpages = {6},
location = {San Francisco, CA, USA},
series = {DAC '24}
}


@article{Liang2022m3vit,
  title={M3ViT: Mixture-of-experts vision transformer for efficient multi-task learning with model-accelerator co-design},
  author={Fan, Zhiwen and Sarkar, Rishov and Jiang, Ziyu and Chen, Tianlong and Zou, Kai and Cheng, Yu and Hao, Cong and Wang, Zhangyang and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={28441--28457},
  year={2022}
}





@INPROCEEDINGS{Sarkar2024edgemoe,
  author={Sarkar, Rishov and Liang, Hanxue and Fan, Zhiwen and Wang, Zhangyang and Hao, Cong},
  booktitle={2023 IEEE/ACM International Conference on Computer Aided Design (ICCAD)}, 
  title={Edge-MoE: Memory-Efficient Multi-Task Vision Transformer Architecture with Task-Level Sparsity via Mixture-of-Experts}, 
  year={2023},
  volume={},
  number={},
  pages={01-09},
  keywords={Computational modeling;Graphics processing units;Bandwidth;Parallel processing;Multitasking;Transformers;Real-time systems},
  doi={10.1109/ICCAD57390.2023.10323651}}



@article{Yun2024Duplex,
  title={Duplex: A Device for Large Language Models with Mixture of Experts, Grouped Query Attention, and Continuous Batching},
  author={Yun, Sungmin and Kyung, Kwanhee and Cho, Juhwan and Choi, Jaewan and Kim, Jongmin and Kim, Byeongho and Lee, Sukhan and Sohn, Kyomin and Ahn, Jung Ho},
  journal={MICRO},
  year={2024}
}



@INPROCEEDINGS{Park2024spacemate,
  author={Park, Gwangtae and Song, Seokchan and Sang, Haoyang and Im, Dongseok and Han, Donghyeon and Kim, Sangyeob and Lee, Hongseok and Yoo, Hoi-Jun},
  booktitle={2024 IEEE International Solid-State Circuits Conference (ISSCC)}, 
  title={20.8 Space-Mate: A 303.5mW Real-Time Sparse Mixture-of-Experts-Based NeRF-SLAM Processor for Mobile Spatial Computing}, 
  year={2024},
  volume={67},
  number={},
  pages={374-376},
  keywords={Simultaneous localization and mapping;Three-dimensional displays;Graphics processing units;Glass;Mobile handsets;Real-time systems;Solid state circuits},
  doi={10.1109/ISSCC49657.2024.10454487}}

@article{buhlmann2012bagging,
  title={Bagging, boosting and ensemble methods},
  author={B{\"u}hlmann, Peter},
  journal={Handbook of computational statistics: Concepts and methods},
  pages={985--1022},
  year={2012},
  publisher={Springer}
}

@article{li2023adaptive,
  title={Adaptive gating in mixture-of-experts based language models},
  author={Li, Jiamin and Su, Qiang and Yang, Yitao and Jiang, Yimin and Wang, Cong and Xu, Hong},
  journal={arXiv preprint arXiv:2310.07188},
  year={2023}
}

@misc{liu2024adamolefinetuninglargelanguage,
      title={AdaMoLE: Fine-Tuning Large Language Models with Adaptive Mixture of Low-Rank Adaptation Experts}, 
      author={Zefang Liu and Jiahua Luo},
      year={2024},
      eprint={2405.00361},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2405.00361}, 
}

@article{guo2024dynamic,
  title={Dynamic mixture of experts: An auto-tuning approach for efficient transformer models},
  author={Guo, Yongxin and Cheng, Zhenglin and Tang, Xiaoying and Tu, Zhaopeng and Lin, Tao},
  journal={arXiv preprint arXiv:2405.14297},
  year={2024}
}

@inproceedings{yang2024xmoe,
  title={XMoE: Sparse Models with Fine-grained and Adaptive Expert Selection},
  author={Yang, Yuanhang and Qi, Shiyi and Gu, Wenchao and Wang, Chaozheng and Gao, Cuiyun and Xu, Zenglin},
  booktitle={Findings of the Association for Computational Linguistics ACL 2024},
  pages={11664--11674},
  year={2024}
}

@article{park2024learning,
  title={Learning More Generalized Experts by Merging Experts in Mixture-of-Experts},
  author={Park, Sejik},
  journal={arXiv preprint arXiv:2405.11530},
  year={2024}
}

@article{sukhbaatar2024branch,
  title={Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM},
  author={Sukhbaatar, Sainbayar and Golovneva, Olga and Sharma, Vasu and Xu, Hu and Lin, Xi Victoria and Rozi{\`e}re, Baptiste and Kahn, Jacob and Li, Daniel and Yih, Wen-tau and Weston, Jason and others},
  journal={arXiv preprint arXiv:2403.07816},
  year={2024}
}

@article{li2022branch,
  title={Branch-train-merge: Embarrassingly parallel training of expert language models},
  author={Li, Margaret and Gururangan, Suchin and Dettmers, Tim and Lewis, Mike and Althoff, Tim and Smith, Noah A and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:2208.03306},
  year={2022}
}

@misc{aghdam2024damoedynamicexpertallocation,
      title={DA-MoE: Towards Dynamic Expert Allocation for Mixture-of-Experts Models}, 
      author={Maryam Akhavan Aghdam and Hongpeng Jin and Yanzhao Wu},
      year={2024},
      eprint={2409.06669},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2409.06669}, 
}

@inproceedings{zhao-etal-2024-hypermoe,
    title = "{H}yper{M}o{E}: Towards Better Mixture of Experts via Transferring Among Experts",
    author = "Zhao, Hao  and
      Qiu, Zihan  and
      Wu, Huijia  and
      Wang, Zili  and
      He, Zhaofeng  and
      Fu, Jie",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.571/",
    doi = "10.18653/v1/2024.acl-long.571",
    pages = "10605--10618",
    abstract = "The Mixture of Experts (MoE) for language models has been proven effective in augmenting the capacity of models by dynamically routing each input token to a specific subset of experts for processing. Despite the success, most existing methods face a challenge for balance between sparsity and the availability of expert knowledge: enhancing performance through increased use of expert knowledge often results in diminishing sparsity during expert selection. To mitigate this contradiction, we propose HyperMoE, a novel MoE framework built upon Hypernetworks. This framework integrates the computational processes of MoE with the concept of knowledge transferring in multi-task learning. Specific modules generated based on the information of unselected experts serve as supplementary information, which allows the knowledge of experts not selected to be used while maintaining selection sparsity. Our comprehensive empirical evaluations across multiple datasets and backbones establish that HyperMoE significantly outperforms existing MoE methods under identical conditions concerning the number of experts. Our code is publicly available at https://github.com/Bumble666/Hyper{\_}MoE"
}

@article{ding2024xft,
  title={XFT: Unlocking the Power of Code Instruction Tuning by Simply Merging Upcycled Mixture-of-Experts},
  author={Ding, Yifeng and Liu, Jiawei and Wei, Yuxiang and Zhuo, Terry Yue and Zhang, Lingming},
  journal={arXiv preprint arXiv:2404.15247},
  year={2024}
}

@article{bengio2013estimating,
  title={Estimating or propagating gradients through stochastic neurons for conditional computation},
  author={Bengio, Yoshua and L{\'e}onard, Nicholas and Courville, Aaron},
  journal={arXiv preprint arXiv:1308.3432},
  year={2013}
}

@inproceedings{zhuang2024litemoe,
  title={LiteMoE: Customizing On-device LLM Serving via Proxy Submodel Tuning},
  author={Zhuang, Yan and Zheng, Zhenzhe and Wu, Fan and Chen, Guihai},
  booktitle={Proceedings of the 22nd ACM Conference on Embedded Networked Sensor Systems},
  pages={521--534},
  year={2024}
}

@inproceedings{almahairi2016dynamic,
  title={Dynamic capacity networks},
  author={Almahairi, Amjad and Ballas, Nicolas and Cooijmans, Tim and Zheng, Yin and Larochelle, Hugo and Courville, Aaron},
  booktitle={International conference on machine learning},
  pages={2549--2558},
  year={2016},
  organization={PMLR}
}

@article{dong2020survey,
  title={A survey on ensemble learning},
  author={Dong, Xibin and Yu, Zhiwen and Cao, Wenming and Shi, Yifan and Ma, Qianli},
  journal={Frontiers of Computer Science},
  volume={14},
  pages={241--258},
  year={2020},
  publisher={Springer}
}

@article{he2023merging,
  title={Merging experts into one: Improving computational efficiency of mixture of experts},
  author={He, Shwai and Fan, Run-Ze and Ding, Liang and Shen, Li and Zhou, Tianyi and Tao, Dacheng},
  journal={arXiv preprint arXiv:2310.09832},
  year={2023}
}

@article{chen2024retraining,
  title={Retraining-Free Merging of Sparse Mixture-of-Experts via Hierarchical Clustering},
  author={Chen, I and Liu, Hsu-Shen and Sun, Wei-Fang and Chao, Chen-Hao and Hsu, Yen-Chang and Lee, Chun-Yi and others},
  journal={arXiv preprint arXiv:2410.08589},
  year={2024}
}

@article{wang2023fusing,
  title={Fusing models with complementary expertise},
  author={Wang, Hongyi and Polo, Felipe Maia and Sun, Yuekai and Kundu, Souvik and Xing, Eric and Yurochkin, Mikhail},
  journal={arXiv preprint arXiv:2310.01542},
  year={2023}
}




@article{artetxe2021efficient,
  title={Efficient large scale language modeling with mixtures of experts},
  author={Artetxe, Mikel and Bhosale, Shruti and Goyal, Naman and Mihaylov, Todor and Ott, Myle and Shleifer, Sam and Lin, Xi Victoria and Du, Jingfei and Iyer, Srinivasan and Pasunuru, Ramakanth and others},
  journal={arXiv preprint arXiv:2112.10684},
  year={2021}
}

@article{xue2022one,
  title={One student knows all experts know: From sparse to dense},
  author={Xue, Fuzhao and He, Xiaoxin and Ren, Xiaozhe and Lou, Yuxuan and You, Yang},
  journal={arXiv preprint arXiv:2201.10890},
  year={2022}
}

@article{chen2022task,
  title={Task-specific expert pruning for sparse mixture-of-experts},
  author={Chen, Tianyu and Huang, Shaohan and Xie, Yuan and Jiao, Binxing and Jiang, Daxin and Zhou, Haoyi and Li, Jianxin and Wei, Furu},
  journal={arXiv preprint arXiv:2206.00277},
  year={2022}
}

@article{shen2023moduleformer,
  title={Moduleformer: Learning modular large language models from uncurated data},
  author={Shen, Yikang and Zhang, Zheyu and Cao, Tianyou and Tan, Shawn and Chen, Zhenfang and Gan, Chuang},
  journal={arXiv preprint arXiv:2306.04640},
  year={2023}
}

@article{huang2023experts,
  title={Experts weights averaging: A new general training scheme for vision transformers},
  author={Huang, Yongqi and Ye, Peng and Huang, Xiaoshui and Li, Sheng and Chen, Tao and He, Tong and Ouyang, Wanli},
  journal={arXiv preprint arXiv:2308.06093},
  year={2023}
}


@article{jin2024moh,
  title={MoH: Multi-Head Attention as Mixture-of-Head Attention},
  author={Jin, Peng and Zhu, Bo and Yuan, Li and Yan, Shuicheng},
  journal={arXiv preprint arXiv:2410.11842},
  year={2024}
}

@article{pan2024dense,
  title={Dense Training, Sparse Inference: Rethinking Training of Mixture-of-Experts Language Models},
  author={Pan, Bowen and Shen, Yikang and Liu, Haokun and Mishra, Mayank and Zhang, Gaoyuan and Oliva, Aude and Raffel, Colin and Panda, Rameswar},
  journal={arXiv preprint arXiv:2404.05567},
  year={2024}
}

@article{shen2024jetmoe,
  title={Jetmoe: Reaching llama2 performance with 0.1 m dollars},
  author={Shen, Yikang and Guo, Zhen and Cai, Tianle and Qin, Zengyi},
  journal={arXiv preprint arXiv:2404.07413},
  year={2024}
}



@article{zhang2022mixture,
  title={Mixture of attention heads: Selecting attention heads per token},
  author={Zhang, Xiaofeng and Shen, Yikang and Huang, Zeyu and Zhou, Jie and Rong, Wenge and Xiong, Zhang},
  journal={arXiv preprint arXiv:2210.05144},
  year={2022}
}

@article{zhang2021enhancing,
  title={Enhancing mixture-of-experts by leveraging attention for fine-grained recognition},
  author={Zhang, Lianbo and Huang, Shaoli and Liu, Wei},
  journal={IEEE Transactions on Multimedia},
  volume={24},
  pages={4409--4421},
  year={2021},
  publisher={IEEE}
}

@article{jin2024moe++,
  title={Moe++: Accelerating mixture-of-experts methods with zero-computation experts},
  author={Jin, Peng and Zhu, Bo and Yuan, Li and Yan, Shuicheng},
  journal={arXiv preprint arXiv:2410.07348},
  year={2024}
}

@article{luo2024moelora,
  title={Moelora: Contrastive learning guided mixture of experts on parameter-efficient fine-tuning for large language models},
  author={Luo, Tongxu and Lei, Jiahe and Lei, Fangyu and Liu, Weihao and He, Shizhu and Zhao, Jun and Liu, Kang},
  journal={arXiv preprint arXiv:2402.12851},
  year={2024}
}

@article{muzio2024seer,
  title={SEER-MoE: Sparse Expert Efficiency through Regularization for Mixture-of-Experts},
  author={Muzio, Alexandre and Sun, Alex and He, Churan},
  journal={arXiv preprint arXiv:2404.05089},
  year={2024}
}

@inproceedings{hwang2024pre,
  title={Pre-gated moe: An algorithm-system co-design for fast and scalable mixture-of-expert inference},
  author={Hwang, Ranggi and Wei, Jianyu and Cao, Shijie and Hwang, Changho and Tang, Xiaohu and Cao, Ting and Yang, Mao},
  booktitle={2024 ACM/IEEE 51st Annual International Symposium on Computer Architecture (ISCA)},
  pages={1018--1031},
  year={2024},
  organization={IEEE}
}

@inproceedings{xiongscomoe,
  title={SCoMoE: Efficient Mixtures of Experts with Structured Communication},
  author={Xiong, Deyi and others},
  booktitle={The Eleventh International Conference on Learning Representations}
}


@article{kim2022says,
  title={Who Says Elephants Can't Run: Bringing Large Scale MoE Models into Cloud Scale Production},
  author={Kim, Young Jin and Henry, Rawn and Fahim, Raffy and Awadalla, Hany Hassan},
  journal={arXiv preprint arXiv:2211.10017},
  year={2022}
}

@article{xie2024moe,
  title={MoE-Pruner: Pruning Mixture-of-Experts Large Language Model using the Hints from Its Router},
  author={Xie, Yanyue and Zhang, Zhi and Zhou, Ding and Xie, Cong and Song, Ziang and Liu, Xin and Wang, Yanzhi and Lin, Xue and Xu, An},
  journal={arXiv preprint arXiv:2410.12013},
  year={2024}
}

@article{zhang2024diversifying,
  title={Diversifying the expert knowledge for task-agnostic pruning in sparse mixture-of-experts},
  author={Zhang, Zeliang and Liu, Xiaodong and Cheng, Hao and Xu, Chenliang and Gao, Jianfeng},
  journal={arXiv preprint arXiv:2407.09590},
  year={2024}
}

@article{liu2024efficient,
  title={Efficient expert pruning for sparse mixture-of-experts language models: Enhancing performance and reducing inference costs},
  author={Liu, Enshu and Zhu, Junyi and Lin, Zinan and Ning, Xuefei and Blaschko, Matthew B and Yan, Shengen and Dai, Guohao and Yang, Huazhong and Wang, Yu},
  journal={arXiv preprint arXiv:2407.00945},
  year={2024}
}

@article{yang2024laco,
  title={Laco: Large language model pruning via layer collapse},
  author={Yang, Yifei and Cao, Zouying and Zhao, Hai},
  journal={arXiv preprint arXiv:2402.11187},
  year={2024}
}

@article{lu2024not,
  title={Not All Experts are Equal: Efficient Expert Pruning and Skipping for Mixture-of-Experts Large Language Models},
  author={Lu, Xudong and Liu, Qi and Xu, Yuhui and Zhou, Aojun and Huang, Siyuan and Zhang, Bo and Yan, Junchi and Li, Hongsheng},
  journal={arXiv preprint arXiv:2402.14800},
  year={2024}
}

@article{sarkar2024revisiting,
  title={Revisiting SMoE Language Models by Evaluating Inefficiencies with Task Specific Expert Pruning},
  author={Sarkar, Soumajyoti and Lausen, Leonard and Cevher, Volkan and Zha, Sheng and Brox, Thomas and Karypis, George},
  journal={arXiv preprint arXiv:2409.01483},
  year={2024}
}

@article{lee2024stun,
  title={STUN: Structured-Then-Unstructured Pruning for Scalable MoE Pruning},
  author={Lee, Jaeseong and Qiao, Aurick and Campos, Daniel F and Yao, Zhewei and He, Yuxiong and others},
  journal={arXiv preprint arXiv:2409.06211},
  year={2024}
}

@article{he2024demystifying,
  title={Demystifying the Compression of Mixture-of-Experts Through a Unified Framework},
  author={He, Shwai and Dong, Daize and Ding, Liang and Li, Ang},
  journal={arXiv preprint arXiv:2406.02500},
  year={2024}
}

@article{chowdhury2024provably,
  title={A Provably Effective Method for Pruning Experts in Fine-tuned Sparse Mixture-of-Experts},
  author={Chowdhury, Mohammed Nowaz Rabbani and Wang, Meng and Maghraoui, Kaoutar El and Wang, Naigang and Chen, Pin-Yu and Carothers, Christopher},
  journal={arXiv preprint arXiv:2405.16646},
  year={2024}
}



@article{huang2024mc,
  title={MC-MoE: Mixture Compressor for Mixture-of-Experts LLMs Gains More},
  author={Huang, Wei and Liao, Yue and Liu, Jianhui and He, Ruifei and Tan, Haoru and Zhang, Shiming and Li, Hongsheng and Liu, Si and Qi, Xiaojuan},
  journal={arXiv preprint arXiv:2410.06270},
  year={2024}
}

@article{kim2023mixture,
  title={Mixture of Quantized Experts (MoQE): Complementary Effect of Low-bit Quantization and Robustness},
  author={Kim, Young Jin and Fahim, Raffy and Awadalla, Hany Hassan},
  journal={arXiv preprint arXiv:2310.02410},
  year={2023}
}

@article{frantar2023qmoe,
  title={Qmoe: Practical sub-1-bit compression of trillion-parameter models},
  author={Frantar, Elias and Alistarh, Dan},
  journal={arXiv preprint arXiv:2310.16795},
  year={2023}
}

@article{imani2024mixture,
  title={Mixture of experts with mixture of precisions for tuning quality of service},
  author={Imani, HamidReza and Amirany, Abdolah and El-Ghazawi, Tarek},
  journal={arXiv preprint arXiv:2407.14417},
  year={2024}
}

@article{li2024examining,
  title={Examining Post-Training Quantization for Mixture-of-Experts: A Benchmark},
  author={Li, Pingzhi and Jin, Xiaolong and Cheng, Yu and Chen, Tianlong},
  journal={arXiv preprint arXiv:2406.08155},
  year={2024}
}

@article{yuancompressed,
  title={Compressed MoE ASR Model Based on Knowledge Distillation and Quantization},
  author={Yuan, Yuping and You, Zhao and Feng, Shulin and Su, Dan and Liang, Yanchun and Shi, Xiaohu and Yu, Dong}
}



@article{shu2024llava,
  title={Llava-mod: Making llava tiny via moe knowledge distillation},
  author={Shu, Fangxun and Liao, Yue and Zhuo, Le and Xu, Chenning and Zhang, Lei and Zhang, Guanghao and Shi, Haonan and Chen, Long and Zhong, Tao and He, Wanggui and others},
  journal={arXiv preprint arXiv:2408.15881},
  year={2024}
}

@article{kim2024ladimo,
  title={Ladimo: Layer-wise distillation inspired moefier},
  author={Kim, Sungyoon and Kim, Youngjun and Moon, Kihyo and Jang, Minsung},
  journal={arXiv preprint arXiv:2408.04278},
  year={2024}
}
@techreport{salinas2022knowledge,
  title={Knowledge distillation for mixture of experts models in speech recognition},
  author={Salinas, Felipe Cruz and Kumatani, Kenichi and Gmyr, Robert and Liu, Linquan and Shi, Yu},
  year={2022},
  institution={Technical Report MSR-TR-2022-6, Microsoft Research, May 2022. https://www~…}
}

@article{yang2024moe,
  title={MoE-I2: Compressing Mixture of Experts Models through Inter-Expert Pruning and Intra-Expert Low-Rank Decomposition},
  author={Yang, Cheng and Sui, Yang and Xiao, Jinqi and Huang, Lingyi and Gong, Yu and Duan, Yuanlin and Jia, Wenqi and Yin, Miao and Cheng, Yu and Yuan, Bo},
  journal={arXiv preprint arXiv:2411.01016},
  year={2024}
}

%%%%sota
@article{sun2024hunyuan,
  title={Hunyuan-Large: An Open-Source MoE Model with 52 Billion Activated Parameters by Tencent},
  author={Xingwu Sun and Yanfeng Chen and Yiqing Huang and Ruobing Xie and Jiaqi Zhu and Kai Zhang and Shuaipeng Li and Zhen Yang and Jonny Han and Xiaobo Shu and Jiahao Bu and Zhongzhi Chen and Xuemeng Huang and Fengzong Lian and Saiyong Yang and Jianfeng Yan and Yuyuan Zeng and Xiaoqin Ren and Chao Yu and Lulu Wu and Yue Mao and Jun Xia and Tao Yang and Suncong Zheng and Kan Wu and Dian Jiao and Jinbao Xue and Xipeng Zhang and Decheng Wu and Kai Liu and Dengpeng Wu and Guanghui Xu and Shaohua Chen and Shuang Chen and Xiao Feng and Yigeng Hong and Junqiang Zheng and Chengcheng Xu and Zongwei Li and Xiong Kuang and Jianglu Hu and Yiqi Chen and Yuchi Deng and Guiyang Li and Ao Liu and Chenchen Zhang and Shihui Hu and Zilong Zhao and Zifan Wu and Yao Ding and Weichao Wang and Han Liu and Roberts Wang and Hao Fei and Peijie Yu and Ze Zhao and Xun Cao and Hai Wang and Fusheng Xiang and Mengyuan Huang and Zhiyuan Xiong and Bin Hu and Xuebin Hou and Lei Jiang and Jianqiang Ma and Jiajia Wu and Yaping Deng and Yi Shen and Qian Wang and Weijie Liu and Jie Liu and Meng Chen and Liang Dong and Weiwen Jia and Hu Chen and Feifei Liu and Rui Yuan and Huilin Xu and Zhenxiang Yan and Tengfei Cao and Zhichao Hu and Xinhua Feng and Dong Du and Tinghao Yu and Yangyu Tao and Feng Zhang and Jianchen Zhu and Chengzhong Xu and Xirui Li and Chong Zha and Wen Ouyang and Yinben Xia and Xiang Li and Zekun He and Rongpeng Chen and Jiawei Song and Ruibin Chen and Fan Jiang and Chongqing Zhao and Bo Wang and Hao Gong and Rong Gan and Winston Hu and Zhanhui Kang and Yong Yang and Yuhong Liu and Di Wang and Jie Jiang},
  journal={arXiv preprint arXiv:2411.02265},
  year={2024}
}


@article{jiang2024mixtral,
  title={Mixtral of experts},
  author={Albert Q. Jiang and Alexandre Sablayrolles and Antoine Roux and Arthur Mensch and Blanche Savary and Chris Bamford and Devendra Singh Chaplot and Diego de las Casas and Emma Bou Hanna and Florian Bressand and Gianna Lengyel and Guillaume Bour and Guillaume Lample and Lélio Renard Lavaud and Lucile Saulnier and Marie-Anne Lachaux and Pierre Stock and Sandeep Subramanian and Sophia Yang and Szymon Antoniak and Teven Le Scao and Théophile Gervet and Thibaut Lavril and Thomas Wang and Timothée Lacroix and William El Sayed},
  journal={arXiv preprint arXiv:2401.04088},
  year={2024}
}

@article{dai2024deepseekmoe,
  title={Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models},
  author={Damai Dai and Chengqi Deng and Chenggang Zhao and R. X. Xu and Huazuo Gao and Deli Chen and Jiashi Li and Wangding Zeng and Xingkai Yu and Y. Wu and Zhenda Xie and Y. K. Li and Panpan Huang and Fuli Luo and Chong Ruan and Zhifang Sui and Wenfeng Liang},
  journal={arXiv preprint arXiv:2401.06066},
  year={2024}
}


@comment{Aixin Liu and Bei Feng and Bin Wang and Bingxuan Wang and Bo Liu and Chenggang Zhao and Chengqi Dengr and Chong Ruan and Damai Dai and Daya Guo and Dejian Yang and Deli Chen and Dongjie Ji and Erhang Li and Fangyun Lin and Fuli Luo and Guangbo Hao and Guanting Chen and Guowei Li and H. Zhang and Hanwei Xu and Hao Yang and Haowei Zhang and Honghui Ding and Huajian Xin and Huazuo Gao and Hui Li and Hui Qu and J. L. Cai and Jian Liang and Jianzhong Guo and Jiaqi Ni and Jiashi Li and Jin Chen and Jingyang Yuan and Junjie Qiu and Junxiao Song and Kai Dong and Kaige Gao and Kang Guan and Lean Wang and Lecong Zhang and Lei Xu and Leyi Xia and Liang Zhao and Liyue Zhang and Meng Li and Miaojun Wang and Mingchuan Zhang and Minghua Zhang and Minghui Tang and Mingming Li and Ning Tian and Panpan Huang and Peiyi Wang and Peng Zhang and Qihao Zhu and Qinyu Chen and Qiushi Du and R. J. Chen and R. L. Jin and Ruiqi Ge and Ruizhe Pan and Runxin Xu and Ruyi Chen and S. S. Li and Shanghao Lu and Shangyan Zhou and Shanhuang Chen and Shaoqing Wu and Shengfeng Ye and Shirong Ma and Shiyu Wang and Shuang Zhou and Shuiping Yu and Shunfeng Zhou and Size Zheng and T. Wang and Tian Pei and Tian Yuan and Tianyu Sun and W. L. Xiao and Wangding Zeng and Wei An and Wen Liu and Wenfeng Liang and Wenjun Gao and Wentao Zhang and X. Q. Li and Xiangyue Jin and Xianzu Wang and Xiao Bi and Xiaodong Liu and Xiaohan Wang and Xiaojin Shen and Xiaokang Chen and Xiaosha Chen and Xiaotao Nie and Xiaowen Sun and Xiaoxiang Wang and Xin Liu and Xin Xie and Xingkai Yu and Xinnan Song and Xinyi Zhou and Xinyu Yang and Xuan Lu and Xuecheng Su and Y. Wu and Y. K. Li and Y. X. Wei and Y. X. Zhu and Yanhong Xu and Yanping Huang and Yao Li and Yao Zhao and Yaofeng Sun and Yaohui Li and Yaohui Wang and Yi Zheng and Yichao Zhang and Yiliang Xiong and Yilong Zhao and Ying He and Ying Tang and Yishi Piao and Yixin Dong and Yixuan Tan and Yiyuan Liu and Yongji Wang and Yongqiang Guo and Yuchen Zhu and Yuduan Wang and Yuheng Zou and Yukun Zha and Yunxian Ma and Yuting Yan and Yuxiang You and Yuxuan Liu and Z. Z. Ren and Zehui Ren and Zhangli Sha and Zhe Fu and Zhen Huang and Zhen Zhang and Zhenda Xie and Zhewen Hao and Zhihong Shao and Zhiniu Wen and Zhipeng Xu and Zhongyu Zhang and Zhuoshu Li and Zihan Wang and Zihui Gu and Zilin Li and Ziwei Xie}
@misc{deepseekai2024deepseekv2strongeconomicalefficient,
      title={DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model}, 
      author={DeepSeek-AI and others},
      year={2024},
      eprint={2405.04434},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2405.04434}, 
}

@article{abdin2024phi,
  title={Phi-3 technical report: A highly capable language model locally on your phone},
  author={Marah Abdin and Jyoti Aneja and Hany Awadalla and Ahmed Awadallah and Ammar Ahmad Awan and Nguyen Bach and Amit Bahree and Arash Bakhtiari and Jianmin Bao and Harkirat Behl and Alon Benhaim and Misha Bilenko and Johan Bjorck and Sébastien Bubeck and Martin Cai and Qin Cai and Vishrav Chaudhary and Dong Chen and Dongdong Chen and Weizhu Chen and Yen-Chun Chen and Yi-Ling Chen and Hao Cheng and Parul Chopra and Xiyang Dai and Matthew Dixon and Ronen Eldan and Victor Fragoso and Jianfeng Gao and Mei Gao and Min Gao and Amit Garg and Allie Del Giorno and Abhishek Goswami and Suriya Gunasekar and Emman Haider and Junheng Hao and Russell J. Hewett and Wenxiang Hu and Jamie Huynh and Dan Iter and Sam Ade Jacobs and Mojan Javaheripi and Xin Jin and Nikos Karampatziakis and Piero Kauffmann and Mahoud Khademi and Dongwoo Kim and Young Jin Kim and Lev Kurilenko and James R. Lee and Yin Tat Lee and Yuanzhi Li and Yunsheng Li and Chen Liang and Lars Liden and Xihui Lin and Zeqi Lin and Ce Liu and Liyuan Liu and Mengchen Liu and Weishung Liu and Xiaodong Liu and Chong Luo and Piyush Madan and Ali Mahmoudzadeh and David Majercak and Matt Mazzola and Caio César Teodoro Mendes and Arindam Mitra and Hardik Modi and Anh Nguyen and Brandon Norick and Barun Patra and Daniel Perez-Becker and Thomas Portet and Reid Pryzant and Heyang Qin and Marko Radmilac and Liliang Ren and Gustavo de Rosa and Corby Rosset and Sambudha Roy and Olatunji Ruwase and Olli Saarikivi and Amin Saied and Adil Salim and Michael Santacroce and Shital Shah and Ning Shang and Hiteshi Sharma and Yelong Shen and Swadheen Shukla and Xia Song and Masahiro Tanaka and Andrea Tupini and Praneetha Vaddamanu and Chunyu Wang and Guanhua Wang and Lijuan Wang and Shuohang Wang and Xin Wang and Yu Wang and Rachel Ward and Wen Wen and Philipp Witte and Haiping Wu and Xiaoxia Wu and Michael Wyatt and Bin Xiao and Can Xu and Jiahang Xu and Weijian Xu and Jilong Xue and Sonali Yadav and Fan Yang and Jianwei Yang and Yifan Yang and Ziyi Yang and Donghan Yu and Lu Yuan and Chenruidong Zhang and Cyril Zhang and Jianwen Zhang and Li Lyna Zhang and Yi Zhang and Yue Zhang and Yunan Zhang and Xiren Zhou},
  journal={arXiv preprint arXiv:2404.14219},
  year={2024}
}

@article{liu2024grin,
  title={GRIN: GRadient-INformed MoE},
  author={Liu, Liyuan and Kim, Young Jin and Wang, Shuohang and Liang, Chen and Shen, Yelong and Cheng, Hao and Liu, Xiaodong and Tanaka, Masahiro and Wu, Xiaoxia and Hu, Wenxiang and others},
  journal={arXiv preprint arXiv:2409.12136},
  year={2024}
}

@article{yang2024qwen2,
  title={Qwen2 technical report},
  author={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jianxin Yang and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Xuejing Liu and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhifang Guo and Zhihao Fan},
  journal={arXiv preprint arXiv:2407.10671},
  year={2024}
}

@article{muennighoff2024olmoe,
  title={OLMoE: Open Mixture-of-Experts Language Models},
  author={Niklas Muennighoff and Luca Soldaini and Dirk Groeneveld and Kyle Lo and Jacob Morrison and Sewon Min and Weijia Shi and Pete Walsh and Oyvind Tafjord and Nathan Lambert and Yuling Gu and Shane Arora and Akshita Bhagia and Dustin Schwenk and David Wadden and Alexander Wettig and Binyuan Hui and Tim Dettmers and Douwe Kiela and Ali Farhadi and Noah A. Smith and Pang Wei Koh and Amanpreet Singh and Hannaneh Hajishirzi},
  journal={arXiv preprint arXiv:2409.02060},
  year={2024}
}

@article{xue2024openmoe,
  title={Openmoe: An early effort on open mixture-of-experts language models},
  author={Xue, Fuzhao and Zheng, Zian and Fu, Yao and Ni, Jinjie and Zheng, Zangwei and Zhou, Wangchunshu and You, Yang},
  journal={arXiv preprint arXiv:2402.01739},
  year={2024}
}

@article{wei2024skywork,
  title={Skywork-MoE: A Deep Dive into Training Techniques for Mixture-of-Experts Language Models},
  author={Tianwen Wei and Bo Zhu and Liang Zhao and Cheng Cheng and Biye Li and Weiwei Lü and Peng Cheng and Jianhao Zhang and Xiaoyu Zhang and Liang Zeng and Xiaokun Wang and Yutuan Ma and Rui Hu and Shuicheng Yan and Han Fang and Yahui Zhou},
  journal={arXiv preprint arXiv:2406.06563},
  year={2024}
}


@article{wu2024yuan,
  title={Yuan 2.0-M32: Mixture of Experts with Attention Router},
  author={Shaohua Wu and Jiangang Luo and Xi Chen and Lingjun Li and Xudong Zhao and Tong Yu and Chao Wang and Yue Wang and Fei Wang and Weixu Qiao and Houbo He and Zeru Zhang and Zeyu Sun and Junxiong Mao and Chong Shen},
  journal={arXiv preprint arXiv:2405.17976},
  year={2024}
}

@article{lieber2024jamba,
  title={Jamba: A hybrid transformer-mamba language model},
  author={Opher Lieber and Barak Lenz and Hofit Bata and Gal Cohen and Jhonathan Osin and Itay Dalmedigos and Erez Safahi and Shaked Meirom and Yonatan Belinkov and Shai Shalev-Shwartz and Omri Abend and Raz Alon and Tomer Asida and Amir Bergman and Roman Glozman and Michael Gokhman and Avashalom Manevich and Nir Ratner and Noam Rozen and Erez Shwartz and Mor Zusman and Yoav Shoham},
  journal={arXiv preprint arXiv:2403.19887},
  year={2024}
}

@inproceedings{zhu2024llama,
  title={Llama-moe: Building mixture-of-experts from llama with continual pre-training},
  author={Zhu, Tong and Qu, Xiaoye and Dong, Daize and Ruan, Jiacheng and Tong, Jingqi and He, Conghui and Cheng, Yu},
  booktitle={Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  pages={15913--15923},
  year={2024}
}

@article{costa2022no,
  title={No language left behind: Scaling human-centered machine translation},
  author={Costa-juss{\`a}, Marta R and Cross, James and {\c{C}}elebi, Onur and Elbayad, Maha and Heafield, Kenneth and Heffernan, Kevin and Kalbassi, Elahe and Lam, Janice and Licht, Daniel and Maillard, Jean and others},
  journal={arXiv preprint arXiv:2207.04672},
  year={2022}
}

@misc{qwen_moe,
    title = {Qwen1.5-MoE: Matching 7B Model Performance with 1/3 Activated Parameters},
    url = {https://qwenlm.github.io/blog/qwen-moe/},
    author = {Qwen Team},
    month = {February},
    year = {2024}
}

@misc{DBRX,
    title = {Introducing DBRX: A New State-of-the-Art Open LLM},
    url = {https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm},
    author = {The Mosaic Research Team},
    month = {March},
    year = {2024}
}

@misc{Arctic,
    title = {Snowflake Arctic: The Best LLM for Enterprise AI — Efficiently Intelligent, Truly Open},
    url = {https://www.snowflake.com/en/blog/arctic-open-efficient-foundation-language-models-snowflake/},
    author = {Snowflake AI Research},
    month = {April},
    year = {2024}
}

@misc{Grok-1,
    title = {Open Release of Grok-1},
    url = {https://x.ai/blog/grok-os},
    author = {xAI},
    month = {March},
    year = {2024}
}


@article{csordas2023switchhead,
  title={Switchhead: Accelerating transformers with mixture-of-experts attention},
  author={Csord{\'a}s, R{\'o}bert and Pi{\k{e}}kos, Piotr and Irie, Kazuki and Schmidhuber, J{\"u}rgen},
  journal={arXiv preprint arXiv:2312.07987},
  year={2023}
}

@article{zhang2024bam,
  title={BAM! Just Like That: Simple and Efficient Parameter Upcycling for Mixture of Experts},
  author={Zhang, Qizhen and Gritsch, Nikolas and Gnaneshwar, Dwaraknath and Guo, Simon and Cairuz, David and Venkitesh, Bharat and Foerster, Jakob and Blunsom, Phil and Ruder, Sebastian and Ustun, Ahmet and others},
  journal={arXiv preprint arXiv:2408.08274},
  year={2024}
}

@inproceedings{peng-etal-2020-mixture,
    title = "A Mixture of h - 1 Heads is Better than h Heads",
    author = "Peng, Hao  and
      Schwartz, Roy  and
      Li, Dianqi  and
      Smith, Noah A.",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.587",
    doi = "10.18653/v1/2020.acl-main.587",
    pages = "6566--6577"
}

@article{tan2023sparse,
  title={Sparse universal transformer},
  author={Tan, Shawn and Shen, Yikang and Chen, Zhenfang and Courville, Aaron and Gan, Chuang},
  journal={arXiv preprint arXiv:2310.07096},
  year={2023}
}

@article{csordas2024moeut,
  title={MoEUT: Mixture-of-Experts Universal Transformers},
  author={Csord{\'a}s, R{\'o}bert and Irie, Kazuki and Schmidhuber, J{\"u}rgen and Potts, Christopher and Manning, Christopher D},
  journal={arXiv preprint arXiv:2405.16039},
  year={2024}
}

@article{li2023merge,
  title={Merge, then compress: Demystify efficient SMoe with hints from its routing policy},
  author={Li, Pingzhi and Zhang, Zhenyu and Yadav, Prateek and Sung, Yi-Lin and Cheng, Yu and Bansal, Mohit and Chen, Tianlong},
  journal={arXiv preprint arXiv:2310.01334},
  year={2023}
}

@article{gao2022parameter,
  title={Parameter-efficient mixture-of-experts architecture for pre-trained language models},
  author={Gao, Ze-Feng and Liu, Peiyu and Zhao, Wayne Xin and Lu, Zhong-Yi and Wen, Ji-Rong},
  journal={arXiv preprint arXiv:2203.01104},
  year={2022}
}



@article{wang2023bitnet,
  title={Bitnet: Scaling 1-bit transformers for large language models},
  author={Wang, Hongyu and Ma, Shuming and Dong, Li and Huang, Shaohan and Wang, Huaijie and Ma, Lingxiao and Yang, Fan and Wang, Ruiping and Wu, Yi and Wei, Furu},
  journal={arXiv preprint arXiv:2310.11453},
  year={2023}
}

@article{frantar2022gptq,
  title={Gptq: Accurate post-training quantization for generative pre-trained transformers},
  author={Frantar, Elias and Ashkboos, Saleh and Hoefler, Torsten and Alistarh, Dan},
  journal={arXiv preprint arXiv:2210.17323},
  year={2022}
}


@article{gu2023knowledge,
  title={Knowledge distillation of large language models},
  author={Gu, Yuxian and Dong, Li and Wei, Furu and Huang, Minlie},
  journal={arXiv preprint arXiv:2306.08543},
  year={2023}
}
@article{agarwal2023gkd,
  title={Gkd: Generalized knowledge distillation for auto-regressive sequence models},
  author={Agarwal, Rishabh and Vieillard, Nino and Stanczyk, Piotr and Ramos, Sabela and Geist, Matthieu and Bachem, Olivier},
  journal={arXiv preprint arXiv:2306.13649},
  year={2023}
}

@article{men2024shortgpt,
  title={Shortgpt: Layers in large language models are more redundant than you expect},
  author={Men, Xin and Xu, Mingyu and Zhang, Qingyu and Wang, Bingning and Lin, Hongyu and Lu, Yaojie and Han, Xianpei and Chen, Weipeng},
  journal={arXiv preprint arXiv:2403.03853},
  year={2024}
}

@inproceedings{li2023losparse,
  title={Losparse: Structured compression of large language models based on low-rank and sparse approximation},
  author={Li, Yixiao and Yu, Yifan and Zhang, Qingru and Liang, Chen and He, Pengcheng and Chen, Weizhu and Zhao, Tuo},
  booktitle={International Conference on Machine Learning},
  pages={20336--20350},
  year={2023},
  organization={PMLR}
}

@article{yuan2023asvd,
  title={Asvd: Activation-aware singular value decomposition for compressing large language models},
  author={Yuan, Zhihang and Shang, Yuzhang and Song, Yue and Wu, Qiang and Yan, Yan and Sun, Guangyu},
  journal={arXiv preprint arXiv:2312.05821},
  year={2023}
}

@article{wang2024svd,
  title={Svd-llm: Truncation-aware singular value decomposition for large language model compression},
  author={Wang, Xin and Zheng, Yu and Wan, Zhongwei and Zhang, Mi},
  journal={arXiv preprint arXiv:2403.07378},
  year={2024}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, A},
  journal={Advances in Neural Information Processing Systems},
  year={2017}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@article{chowdhery2023palm,
  title={Palm: Scaling language modeling with pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={Journal of Machine Learning Research},
  volume={24},
  number={240},
  pages={1--113},
  year={2023}
}


@article{dosovitskiy2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}

@InProceedings{pmlr-v202-dehghani23a,
  title = 	 {Scaling Vision Transformers to 22 Billion Parameters},
  author =       {Dehghani, Mostafa and Djolonga, Josip and Mustafa, Basil and Padlewski, Piotr and Heek, Jonathan and Gilmer, Justin and Steiner, Andreas Peter and Caron, Mathilde and Geirhos, Robert and Alabdulmohsin, Ibrahim and Jenatton, Rodolphe and Beyer, Lucas and Tschannen, Michael and Arnab, Anurag and Wang, Xiao and Riquelme Ruiz, Carlos and Minderer, Matthias and Puigcerver, Joan and Evci, Utku and Kumar, Manoj and Steenkiste, Sjoerd Van and Elsayed, Gamaleldin Fathy and Mahendran, Aravindh and Yu, Fisher and Oliver, Avital and Huot, Fantine and Bastings, Jasmijn and Collier, Mark and Gritsenko, Alexey A. and Birodkar, Vighnesh and Vasconcelos, Cristina Nader and Tay, Yi and Mensink, Thomas and Kolesnikov, Alexander and Pavetic, Filip and Tran, Dustin and Kipf, Thomas and Lucic, Mario and Zhai, Xiaohua and Keysers, Daniel and Harmsen, Jeremiah J. and Houlsby, Neil},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {7480--7512},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/dehghani23a/dehghani23a.pdf},
  url = 	 {https://proceedings.mlr.press/v202/dehghani23a.html}
}

@inproceedings{zhang2021vit,
  title={ViT-YOLO: Transformer-based YOLO for object detection},
  author={Zhang, Zixiao and Lu, Xiaoqiang and Cao, Guojin and Yang, Yuting and Jiao, Licheng and Liu, Fang},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={2799--2808},
  year={2021}
}

@inproceedings{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={International conference on machine learning},
  pages={8748--8763},
  year={2021},
  organization={PMLR}
}

@article{li2021align,
  title={Align before fuse: Vision and language representation learning with momentum distillation},
  author={Li, Junnan and Selvaraju, Ramprasaath and Gotmare, Akhilesh and Joty, Shafiq and Xiong, Caiming and Hoi, Steven Chu Hong},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={9694--9705},
  year={2021}
}

@inproceedings{wang2022ofa,
  title={Ofa: Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework},
  author={Wang, Peng and Yang, An and Men, Rui and Lin, Junyang and Bai, Shuai and Li, Zhikang and Ma, Jianxin and Zhou, Chang and Zhou, Jingren and Yang, Hongxia},
  booktitle={International conference on machine learning},
  pages={23318--23340},
  year={2022},
  organization={PMLR}
}

@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={OpenAI},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}


@misc{Claude,
    title = {The Claude 3 Model Family: Opus, Sonnet, Haiku},
    url = {https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf},
    author = {Anthropic},
    year = {2023}
}


@article{kaplan2020scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}
@article{alabdulmohsin2022revisiting,
  title={Revisiting neural scaling laws in language and vision},
  author={Alabdulmohsin, Ibrahim M and Neyshabur, Behnam and Zhai, Xiaohua},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={22300--22312},
  year={2022}
}

@inproceedings{cherti2023reproducible,
  title={Reproducible scaling laws for contrastive language-image learning},
  author={Cherti, Mehdi and Beaumont, Romain and Wightman, Ross and Wortsman, Mitchell and Ilharco, Gabriel and Gordon, Cade and Schuhmann, Christoph and Schmidt, Ludwig and Jitsev, Jenia},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={2818--2829},
  year={2023}
}

@article{zhou2024survey,
  title={A survey on efficient inference for large language models},
  author={Zhou, Zixuan and Ning, Xuefei and Hong, Ke and Fu, Tianyu and Xu, Jiaming and Li, Shiyao and Lou, Yuming and Wang, Luning and Yuan, Zhihang and Li, Xiuhong and others},
  journal={arXiv preprint arXiv:2404.14294},
  year={2024}
}

@article{yuan2024llm,
  title={Llm inference unveiled: Survey and roofline model insights},
  author={Yuan, Zhihang and Shang, Yuzhang and Zhou, Yang and Dong, Zhen and Zhou, Zhe and Xue, Chenhao and Wu, Bingzhe and Li, Zhikai and Gu, Qingyi and Lee, Yong Jae and others},
  journal={arXiv preprint arXiv:2402.16363},
  year={2024}
}

@article{xu2024survey,
  title={A survey of resource-efficient llm and multimodal foundation models},
  author={Xu, Mengwei and Yin, Wangsong and Cai, Dongqi and Yi, Rongjie and Xu, Daliang and Wang, Qipeng and Wu, Bingyang and Zhao, Yihao and Yang, Chen and Wang, Shihe and others},
  journal={arXiv preprint arXiv:2401.08092},
  year={2024}
}
@article{bai2024beyond,
  title={Beyond efficiency: A systematic survey of resource-efficient large language models},
  author={Bai, Guangji and Chai, Zheng and Ling, Chen and Wang, Shiyu and Lu, Jiaying and Zhang, Nan and Shi, Tingwei and Yu, Ziyang and Zhu, Mengdan and Zhang, Yifei and others},
  journal={arXiv preprint arXiv:2401.00625},
  year={2024}
}

@inproceedings{du2022glam,
  title={Glam: Efficient scaling of language models with mixture-of-experts},
  author={Du, Nan and Huang, Yanping and Dai, Andrew M and Tong, Simon and Lepikhin, Dmitry and Xu, Yuanzhong and Krikun, Maxim and Zhou, Yanqi and Yu, Adams Wei and Firat, Orhan and others},
  booktitle={International Conference on Machine Learning},
  pages={5547--5569},
  year={2022},
  organization={PMLR}
}







@ARTICLE{MetaPi,
  author={Hampshire, J.B. and Waibel, A.},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={The Meta-Pi network: building distributed knowledge representations for robust multisource pattern recognition}, 
  year={1992},
  volume={14},
  number={7},
  pages={751-769},
  doi={10.1109/34.142911}}
  
@inproceedings{NIPS20009fdb62f9,
 author = {Tresp, Volker},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 pages = {},
 publisher = {MIT Press},
 title = {Mixtures of Gaussian Processes},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/file/9fdb62f932adf55af2c0e09e55861964-Paper.pdf},
 volume = {13},
 year = {2000}
}

@article{Eigen2013LearningFR,
  title={Learning Factored Representations in a Deep Mixture of Experts},
  author={David Eigen and Marc'Aurelio Ranzato and Ilya Sutskever},
  journal={CoRR},
  year={2013},
  volume={abs/1312.4314},
  url={https://api.semanticscholar.org/CorpusID:11492613}
}




@article{articleJacobs,
author = {Jacobs, Robert and Jordan, Michael and Nowlan, Steven and Hinton, Geoffrey},
year = {1991},
month = {02},
pages = {78-88},
title = {Adaptive Mixture of Local Expert},
volume = {3},
journal = {Neural Computation},
doi = {10.1162/neco.1991.3.1.79}
}







@article{li2024llm,
  title={Llm inference serving: Survey of recent advances and opportunities},
  author={Li, Baolin and Jiang, Yankai and Gadepally, Vijay and Tiwari, Devesh},
  journal={arXiv preprint arXiv:2407.12391},
  year={2024}
}

@article{li2024large,
  title={Large language model inference acceleration: A comprehensive hardware perspective},
  author={Li, Jinhao and Xu, Jiaming and Huang, Shan and Chen, Yonghua and Li, Wen and Liu, Jun and Lian, Yaoxiu and Pan, Jiayi and Ding, Li and Zhou, Hao and others},
  journal={arXiv preprint arXiv:2410.04466},
  year={2024}
}

@article{treviso2023efficient,
  title={Efficient methods for natural language processing: A survey},
  author={Treviso, Marcos and Lee, Ji-Ung and Ji, Tianchu and Aken, Betty van and Cao, Qingqing and Ciosici, Manuel R and Hassid, Michael and Heafield, Kenneth and Hooker, Sara and Raffel, Colin and others},
  journal={Transactions of the Association for Computational Linguistics},
  volume={11},
  pages={826--860},
  year={2023},
  publisher={MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~…}
}

@article{wan2023efficient,
  title={Efficient large language models: A survey},
  author={Wan, Zhongwei and Wang, Xin and Liu, Che and Alam, Samiul and Zheng, Yu and Liu, Jiachen and Qu, Zhongnan and Yan, Shen and Zhu, Yi and Zhang, Quanlu and others},
  journal={arXiv preprint arXiv:2312.03863},
  year={2023}
}


@article{fedus2022review,
  title={A review of sparse expert models in deep learning},
  author={Fedus, William and Dean, Jeff and Zoph, Barret},
  journal={arXiv preprint arXiv:2209.01667},
  year={2022}
}

@article{202408.0583,
	doi = {10.20944/preprints202408.0583.v2},
	url = {https://doi.org/10.20944/preprints202408.0583.v2},
	year = 2024,
	month = {August},
	publisher = {Preprints},
	author = {Arpita Vats and Rahul Raja and Vinija Jain and Aman Chadha},
	title = {The Evolution of Mixture of Experts: A Survey from Basics to Breakthroughs},
	journal = {Preprints}
}




@article{polikar2012ensemble,
  title={Ensemble learning},
  author={Polikar, Robi},
  journal={Ensemble machine learning: Methods and applications},
  pages={1--34},
  year={2012},
  publisher={Springer}
}

@inproceedings{Demonstrative,
author = {Wei, Jingyu and Su, Yi and Xu, Kele and Zeng, Lingbin and Liu, Bo and Wang, Huaimin},
title = {Demonstrative Instruction Following in Multimodal LLMs via Integrating Low-Rank Adaptation with Ensemble Learning},
year = {2024},
isbn = {9798400706868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3664647.3688995},
doi = {10.1145/3664647.3688995},
booktitle = {Proceedings of the 32nd ACM International Conference on Multimedia},
pages = {11435–11441},
numpages = {7},
keywords = {ensemble learning, instruction following, interleaved image-text, low-rank adaptation, multimodal large language model},
location = {Melbourne VIC, Australia},
series = {MM '24}
}

@InProceedings{10.1007/3-540-45014-9_1,
author="Dietterich, Thomas G.",
title="Ensemble Methods in Machine Learning",
booktitle="Multiple Classifier Systems",
year="2000",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="1--15",
abstract="Ensemble methods are learning algorithms that construct a set of classifiers and then classify new data points by taking a (weighted) vote of their predictions. The original ensemble method is Bayesian averaging, but more recent algorithms include error-correcting output coding, Bagging, and boosting. This paper reviews these methods and explains why ensembles can often perform better than any single classifier. Some previous studies comparing ensemble methods are reviewed, and some new experiments are presented to uncover the reasons that Adaboost does not overfit rapidly.",
isbn="978-3-540-45014-6"
}



@article{zhao2023bagging,
  title={Bagging and boosting fine-tuning for ensemble learning},
  author={Zhao, Changming and Peng, Ruimin and Wu, Dongrui},
  journal={IEEE Transactions on Artificial Intelligence},
  year={2023},
  publisher={IEEE}
}

@article{kasneci2024enriching,
  title={Enriching Tabular Data with Contextual LLM Embeddings: A Comprehensive Ablation Study for Ensemble Classifiers},
  author={Kasneci, Gjergji and Kasneci, Enkelejda},
  journal={arXiv preprint arXiv:2411.01645},
  year={2024}
}

@article{del2023skipdecode,
  title={Skipdecode: Autoregressive skip decoding with batching and caching for efficient llm inference},
  author={Del Corro, Luciano and Del Giorno, Allie and Agarwal, Sahaj and Yu, Bin and Awadallah, Ahmed and Mukherjee, Subhabrata},
  journal={arXiv preprint arXiv:2307.02628},
  year={2023}
}

@article{wang2024q,
  title={Q-sparse: All large language models can be fully sparsely-activated},
  author={Wang, Hongyu and Ma, Shuming and Wang, Ruiping and Wei, Furu},
  journal={arXiv preprint arXiv:2407.10969},
  year={2024}
}

@inproceedings{liu2024moe,
  title={When moe meets llms: Parameter efficient fine-tuning for multi-task medical applications},
  author={Liu, Qidong and Wu, Xian and Zhao, Xiangyu and Zhu, Yuanshao and Xu, Derong and Tian, Feng and Zheng, Yefeng},
  booktitle={Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  pages={1104--1114},
  year={2024}
}

@article{liu2023moelora,
  title={Moelora: An moe-based parameter efficient fine-tuning method for multi-task medical applications},
  author={Liu, Qidong and Wu, Xian and Zhao, Xiangyu and Zhu, Yuanshao and Xu, Derong and Tian, Feng and Zheng, Yefeng},
  journal={arXiv preprint arXiv:2310.18339},
  year={2023}
}

@inproceedings{dou2024loramoe,
  title={LoRAMoE: Alleviating world knowledge forgetting in large language models via MoE-style plugin},
  author={Dou, Shihan and Zhou, Enyu and Liu, Yan and Gao, Songyang and Shen, Wei and Xiong, Limao and Zhou, Yuhao and Wang, Xiao and Xi, Zhiheng and Fan, Xiaoran and others},
  booktitle={Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={1932--1945},
  year={2024}
}


@article{gao2024higher,
  title={Higher layers need more lora experts},
  author={Gao, Chongyang and Chen, Kezhen and Rao, Jinmeng and Sun, Baochen and Liu, Ruibo and Peng, Daiyi and Zhang, Yawen and Guo, Xiaoyuan and Yang, Jie and Subrahmanian, VS},
  journal={arXiv preprint arXiv:2402.08562},
  year={2024}
}


@inproceedings{chowdhury2023patch,
  title={Patch-level routing in mixture-of-experts is provably sample-efficient for convolutional neural networks},
  author={Chowdhury, Mohammed Nowaz Rabbani and Zhang, Shuai and Wang, Meng and Liu, Sijia and Chen, Pin-Yu},
  booktitle={International Conference on Machine Learning},
  pages={6074--6114},
  year={2023},
  organization={PMLR}
}

@inproceedings{wang2020deep,
  title={Deep mixture of experts via shallow embedding},
  author={Wang, Xin and Yu, Fisher and Dunlap, Lisa and Ma, Yi-An and Wang, Ruth and Mirhoseini, Azalia and Darrell, Trevor and Gonzalez, Joseph E},
  booktitle={Uncertainty in artificial intelligence},
  pages={552--562},
  year={2020},
  organization={PMLR}
}




@article{sun2022vaqf,
  title={Vaqf: Fully automatic software-hardware co-design framework for low-bit vision transformer},
  author={Sun, Mengshu and Ma, Haoyu and Kang, Guoliang and Jiang, Yifan and Chen, Tianlong and Ma, Xiaolong and Wang, Zhangyang and Wang, Yanzhi},
  journal={arXiv preprint arXiv:2201.06618},
  year={2022}
}


@inproceedings{you2023vitcod,
  title={Vitcod: Vision transformer acceleration via dedicated algorithm and accelerator co-design},
  author={You, Haoran and Sun, Zhanyi and Shi, Huihong and Yu, Zhongzhi and Zhao, Yang and Zhang, Yongan and Li, Chaojian and Li, Baopu and Lin, Yingyan},
  booktitle={2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA)},
  pages={273--286},
  year={2023},
  organization={IEEE}
}

@INPROCEEDINGS{9773212,
  author={Zhou, Minxuan and Xu, Weihong and Kang, Jaeyoung and Rosing, Tajana},
  booktitle={2022 IEEE International Symposium on High-Performance Computer Architecture (HPCA)}, 
  title={TransPIM: A Memory-based Acceleration via Software-Hardware Co-Design for Transformer}, 
  year={2022},
  volume={},
  number={},
  pages={1071-1085},
  keywords={Computational modeling;Memory management;Bandwidth;Parallel processing;Transformers;Throughput;Hardware;Processing in-memory;Near-data processing;Transformer;Domain-specific acceleration;Software-hardware co-design},
  doi={10.1109/HPCA53966.2022.00082}}


@article{schuman2017survey,
  title={A survey of neuromorphic computing and neural networks in hardware},
  author={Schuman, Catherine D and Potok, Thomas E and Patton, Robert M and Birdwell, J Douglas and Dean, Mark E and Rose, Garrett S and Plank, James S},
  journal={arXiv preprint arXiv:1705.06963},
  year={2017}
}

@article{schuman2022opportunities,
  title={Opportunities for neuromorphic computing algorithms and applications},
  author={Schuman, Catherine D and Kulkarni, Shruti R and Parsa, Maryam and Mitchell, J Parker and Date, Prasanna and Kay, Bill},
  journal={Nature Computational Science},
  volume={2},
  number={1},
  pages={10--19},
  year={2022},
  publisher={Nature Publishing Group US New York}
}


@article{liang2023unleashing,
  title={Unleashing the potential of llms for quantum computing: A study in quantum architecture design},
  author={Liang, Zhiding and Cheng, Jinglei and Yang, Rui and Ren, Hang and Song, Zhixin and Wu, Di and Qian, Xuehai and Li, Tongyang and Shi, Yiyu},
  journal={arXiv preprint arXiv:2307.08191},
  year={2023}
}

@article{steane1998quantum,
  title={Quantum computing},
  author={Steane, Andrew},
  journal={Reports on Progress in Physics},
  volume={61},
  number={2},
  pages={117},
  year={1998},
  publisher={IOP Publishing}
}

@article{padmanaban2024quantum,
  title={Quantum Computing and AI in the Cloud},
  author={Padmanaban, Harish},
  journal={Journal of Computational Intelligence and Robotics},
  volume={4},
  number={1},
  pages={14--32},
  year={2024}
}


@article{mao2024green,
  title={Green edge AI: A contemporary survey},
  author={Mao, Yuyi and Yu, Xianghao and Huang, Kaibin and Zhang, Ying-Jun Angela and Zhang, Jun},
  journal={Proceedings of the IEEE},
  year={2024},
  publisher={IEEE}
}


@article{cruz2024innovating,
  title={Innovating for Tomorrow: The convergence of SE and Green AI},
  author={Cruz, Lu{\'\i}s and Gutierrez, Xavier Franch and Mart{\'\i}nez-Fern{\'a}ndez, Silverio},
  journal={arXiv preprint arXiv:2406.18142},
  year={2024}
}


@article{raman2024green,
  title={Green and sustainable AI research: an integrated thematic and topic modeling analysis},
  author={Raman, Raghu and Pattnaik, Debidutta and Lathabai, Hiran H and Kumar, Chandan and Govindan, Kannan and Nedungadi, Prema},
  journal={Journal of Big Data},
  volume={11},
  number={1},
  pages={55},
  year={2024},
  publisher={Springer}
}


@inproceedings{alizadeh2024green,
  title={Green AI: A Preliminary Empirical Study on Energy Consumption in DL Models Across Different Runtime Infrastructures},
  author={Alizadeh, Negar and Castor, Fernando},
  booktitle={Proceedings of the IEEE/ACM 3rd International Conference on AI Engineering-Software Engineering for AI},
  pages={134--139},
  year={2024}
}



@inproceedings{zhu2024lightening,
  title={Lightening-transformer: A dynamically-operated optically-interconnected photonic transformer accelerator},
  author={Zhu, Hanqing and Gu, Jiaqi and Wang, Hanrui and Jiang, Zixuan and Zhang, Zhekai and Tang, Rongxing and Feng, Chenghao and Han, Song and Chen, Ray T and Pan, David Z},
  booktitle={2024 IEEE International Symposium on High-Performance Computer Architecture (HPCA)},
  pages={686--703},
  year={2024},
  organization={IEEE}
}

@article{ning2024photonic,
  title={Photonic-electronic integrated circuits for high-performance computing and ai accelerators},
  author={Ning, Shupeng and Zhu, Hanqing and Feng, Chenghao and Gu, Jiaqi and Jiang, Zhixing and Ying, Zhoufeng and Midkiff, Jason and Jain, Sourabh and Hlaing, May H and Pan, David Z and others},
  journal={Journal of Lightwave Technology},
  year={2024},
  publisher={IEEE}
}

@article{radovanovic2022carbon,
  title={Carbon-aware computing for datacenters},
  author={Radovanovi{\'c}, Ana and Koningstein, Ross and Schneider, Ian and Chen, Bokan and Duarte, Alexandre and Roy, Binz and Xiao, Diyue and Haridasan, Maya and Hung, Patrick and Care, Nick and others},
  journal={IEEE Transactions on Power Systems},
  volume={38},
  number={2},
  pages={1270--1280},
  year={2022},
  publisher={IEEE}
}

@article{faiz2023llmcarbon,
  title={Llmcarbon: Modeling the end-to-end carbon footprint of large language models},
  author={Faiz, Ahmad and Kaneda, Sotaro and Wang, Ruhan and Osi, Rita and Sharma, Prateek and Chen, Fan and Jiang, Lei},
  journal={arXiv preprint arXiv:2309.14393},
  year={2023}
}

@article{fu2024llmco2,
  title={LLMCO2: Advancing Accurate Carbon Footprint Prediction for LLM Inferences},
  author={Fu, Zhenxiao and Chen, Fan and Zhou, Shan and Li, Haitong and Jiang, Lei},
  journal={arXiv preprint arXiv:2410.02950},
  year={2024}
}


@article{nguyen2024libmoe,
  title={LIBMoE: A Library for comprehensive benchmarking Mixture of Experts in Large Language Models},
  author={Nguyen, Nam V and Doan, Thong T and Tran, Luong and Nguyen, Van and Pham, Quang},
  journal={arXiv preprint arXiv:2411.00918},
  year={2024}
}

@article{fu2024moe,
  title={MoE-CAP: Cost-Accuracy-Performance Benchmarking for Mixture-of-Experts Systems},
  author={Fu, Yao and Jiang, Yinsicheng and Huang, Yeqi and Nie, Ping and Lu, Zhan and Xue, Leyang and He, Congjie and Sit, Man-Kit and Xue, Jilong and Dong, Li and others},
  journal={arXiv preprint arXiv:2412.07067},
  year={2024}
}



@misc{Llm-perf,
      title={Llm-perf leaderboard}, 
      author={Huggingface},
      year={2023},
      url={https://huggingface.co/spaces/optimum/llm-perf-leaderboard}, 
}

@misc{Llm-Artificial,
      title={Artificial analysis llm performance leaderboard}, 
      author={Artificial Analysis},
      year={2024},
      url={https://artificialanalysis.ai/leaderboards/models}, 
}



@inproceedings{reddi2020mlperf,
  title={Mlperf inference benchmark},
  author={Reddi, Vijay Janapa and Cheng, Christine and Kanter, David and Mattson, Peter and Schmuelling, Guenther and Wu, Carole-Jean and Anderson, Brian and Breughe, Maximilien and Charlebois, Mark and Chou, William and others},
  booktitle={2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA)},
  pages={446--459},
  year={2020},
  organization={IEEE}
}



@inproceedings{narayanan2021efficient,
  title={Efficient large-scale language model training on gpu clusters using megatron-lm},
  author={Narayanan, Deepak and Shoeybi, Mohammad and Casper, Jared and LeGresley, Patrick and Patwary, Mostofa and Korthikanti, Vijay and Vainbrand, Dmitri and Kashinkunti, Prethvi and Bernauer, Julie and Catanzaro, Bryan and others},
  booktitle={Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
  pages={1--15},
  year={2021}
}

@inproceedings{rajbhandari2020zero,
  title={Zero: Memory optimizations toward training trillion parameter models},
  author={Rajbhandari, Samyam and Rasley, Jeff and Ruwase, Olatunji and He, Yuxiong},
  booktitle={SC20: International Conference for High Performance Computing, Networking, Storage and Analysis},
  pages={1--16},
  year={2020},
  organization={IEEE}
}

@article{jia2019beyond,
  title={Beyond data and model parallelism for deep neural networks.},
  author={Jia, Zhihao and Zaharia, Matei and Aiken, Alex},
  journal={Proceedings of Machine Learning and Systems},
  volume={1},
  pages={1--13},
  year={2019}
}

@inproceedings{xu2023mmbench,
  title={MMBench: Benchmarking End-to-End Multi-modal DNNs and Understanding Their Hardware-Software Implications},
  author={Xu, Cheng and Hou, Xiaofeng and Liu, Jiacheng and Li, Chao and Huang, Tianhao and Zhu, Xiaozhi and Niu, Mo and Sun, Lingyu and Tang, Peng and Xu, Tongqiao and others},
  booktitle={2023 IEEE International Symposium on Workload Characterization (IISWC)},
  pages={154--166},
  year={2023},
  organization={IEEE}
}

@inproceedings{liu2024loraexit,
  title={LoRAExit: Empowering Dynamic Modulation of LLMs in Resource-limited Settings using Low-rank Adapters},
  author={Liu, Jiacheng and Tang, Peng and Hou, Xiaofeng and Li, Chao and Heng, Pheng-Ann},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2024},
  pages={9211--9225},
  year={2024}
}

@article{hou2024improving,
  title={Improving Efficiency in Multi-modal Autonomous Embedded Systems through Adaptive Gating},
  author={Hou, Xiaofeng and Xu, Cheng and Li, Chao and Liu, Jiacheng and Tang, Xuehan and Cheng, Kwang-Ting and Guo, Minyi},
  journal={IEEE Transactions on Computers},
  year={2024},
  publisher={IEEE}
}

@article{team2024gemini,
  title={Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context},
  author={Team, Gemini and Georgiev, Petko and Lei, Ving Ian and Burnell, Ryan and Bai, Libin and Gulati, Anmol and Tanzer, Garrett and Vincent, Damien and Pan, Zhufeng and Wang, Shibo and others},
  journal={arXiv preprint arXiv:2403.05530},
  year={2024}
}

@inproceedings{kwon2023efficient,
  title={Efficient Memory Management for Large Language Model Serving with PagedAttention},
  author={Woosuk Kwon and Zhuohan Li and Siyuan Zhuang and Ying Sheng and Lianmin Zheng and Cody Hao Yu and Joseph E. Gonzalez and Hao Zhang and Ion Stoica},
  booktitle={Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles},
  year={2023}
}




@misc{pytorch,
      title={PyTorch}, 
      author={Meta AI},
      year={2024},
      url={https://pytorch.org/}, 
}


@misc{TensorFlow,
      title={TensorFlow}, 
      author={Google Brain Team},
      year={2024},
      url={http://tensorflow.org/}, 
}


@misc{cai2024mocsystemefficientfaulttolerance,
      title={MoC-System: Efficient Fault Tolerance for Sparse Mixture-of-Experts Model Training}, 
      author={Weilin Cai and Le Qin and Jiayi Huang},
      year={2024},
      eprint={2408.04307},
      archivePrefix={arXiv},
      primaryClass={cs.DC},
      url={https://arxiv.org/abs/2408.04307}, 
}


@misc{llamacpp,
      title={Llama.cpp}, 
      author={Georgi Gerganov},
      year={2023},
      url={https://github.com/ggerganov/llama.cpp}, 
}



@misc{transformers,
      title={Transformers: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow}, 
      author={Huggingface},
      year={2023},
      url={https://github.com/huggingface/transformers}, 
}

@misc{deepspeed,
      title={DeepSpeed}, 
      author={Microsoft},
      year={2023},
      url={https://github.com/microsoft/DeepSpeed}, 
}
@misc{fairseq,
      title={Fairseq}, 
      author={Facebook AI Research},
      year={2019},
      url={https://github.com/facebookresearch/fairseq}, 
}



@misc{FasterTransformer,
      title={FasterTransformer}, 
      author={NVIDIA},
      year={2019},
      url={https://github.com/NVIDIA/FasterTransformer}, 
}
@inproceedings{sheng2023flexgen,
  title={Flexgen: High-throughput generative inference of large language models with a single gpu},
  author={Sheng, Ying and Zheng, Lianmin and Yuan, Binhang and Li, Zhuohan and Ryabinin, Max and Chen, Beidi and Liang, Percy and R{\'e}, Christopher and Stoica, Ion and Zhang, Ce},
  booktitle={International Conference on Machine Learning},
  pages={31094--31116},
  year={2023},
  organization={PMLR}
}

@article{shen2022se,
  title={Se-moe: A scalable and efficient mixture-of-experts distributed training and inference system},
  author={Shen, Liang and Wu, Zhihua and Gong, WeiBao and Hao, Hongxiang and Bai, Yangfan and Wu, HuaChao and Wu, Xinxuan and Bian, Jiang and Xiong, Haoyi and Yu, Dianhai and others},
  journal={arXiv preprint arXiv:2205.10034},
  year={2022}
}

@article{liu2025optimizing,
  title={Optimizing Distributed Deployment of Mixture-of-Experts Model Inference in Serverless Computing},
  author={Liu, Mengfan and Wang, Wei and Wu, Chuan},
  journal={arXiv preprint arXiv:2501.05313},
  year={2025}
}

@article{skliar2024mixture,
  title={Mixture of Cache-Conditional Experts for Efficient Mobile Device Inference},
  author={Skliar, Andrii and van Rozendaal, Ties and Lepert, Romain and Boinovski, Todor and van Baalen, Mart and Nagel, Markus and Whatmough, Paul and Bejnordi, Babak Ehteshami},
  journal={arXiv preprint arXiv:2412.00099},
  year={2024}
}

%datasets of finetuning
@article{wang2018glue,
  title={Glue: A multi-task benchmark and analysis platform for natural language understanding},
  author={Wang, Alex},
  journal={arXiv preprint arXiv:1804.07461},
  year={2018}
}

@article{rajpurkar2016squad,
  title={Squad: 100,000+ questions for machine comprehension of text},
  author={Rajpurkar, P},
  journal={arXiv preprint arXiv:1606.05250},
  year={2016}
}

@article{zhou2024lima,
  title={Lima: Less is more for alignment},
  author={Zhou, Chunting and Liu, Pengfei and Xu, Puxin and Iyer, Srinivasan and Sun, Jiao and Mao, Yuning and Ma, Xuezhe and Efrat, Avia and Yu, Ping and Yu, Lili and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{yu2023metamath,
  title={Metamath: Bootstrap your own mathematical questions for large language models},
  author={Yu, Longhui and Jiang, Weisen and Shi, Han and Yu, Jincheng and Liu, Zhengying and Zhang, Yu and Kwok, James T and Li, Zhenguo and Weller, Adrian and Liu, Weiyang},
  journal={arXiv preprint arXiv:2309.12284},
  year={2023}
}

@inproceedings{longpre2023flan,
  title={The flan collection: Designing data and methods for effective instruction tuning},
  author={Longpre, Shayne and Hou, Le and Vu, Tu and Webson, Albert and Chung, Hyung Won and Tay, Yi and Zhou, Denny and Le, Quoc V and Zoph, Barret and Wei, Jason and others},
  booktitle={International Conference on Machine Learning},
  pages={22631--22648},
  year={2023},
  organization={PMLR}
}

@article{krizhevsky2009learning,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, Alex and Hinton, Geoffrey and others},
  year={2009},
  publisher={Toronto, ON, Canada}
}

@article{russakovsky2015imagenet,
  title={Imagenet large scale visual recognition challenge},
  author={Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and others},
  journal={International journal of computer vision},
  volume={115},
  pages={211--252},
  year={2015},
  publisher={Springer}
}

@article{hendrycks2020measuring,
  title={Measuring massive multitask language understanding},
  author={Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2009.03300},
  year={2020}
}



@misc{taori2023stanford,
  title={Stanford alpaca: An instruction-following llama model},
  author={Taori, Rohan and Gulrajani, Ishaan and Zhang, Tianyi and Dubois, Yann and Li, Xuechen and Guestrin, Carlos and Liang, Percy and Hashimoto, Tatsunori B},
  year={2023}
}

@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={Journal of machine learning research},
  volume={21},
  number={140},
  pages={1--67},
  year={2020}
}

@inproceedings{socher2013recursive,
  title={Recursive deep models for semantic compositionality over a sentiment treebank},
  author={Socher, Richard and Perelygin, Alex and Wu, Jean and Chuang, Jason and Manning, Christopher D and Ng, Andrew Y and Potts, Christopher},
  booktitle={Proceedings of the 2013 conference on empirical methods in natural language processing},
  pages={1631--1642},
  year={2013}
}

@inproceedings{dolan2005automatically,
  title={Automatically constructing a corpus of sentential paraphrases},
  author={Dolan, Bill and Brockett, Chris},
  booktitle={Third international workshop on paraphrasing (IWP2005)},
  year={2005}
}

@inproceedings{khashabi2018looking,
  title={Looking beyond the surface: A challenge set for reading comprehension over multiple sentences},
  author={Khashabi, Daniel and Chaturvedi, Snigdha and Roth, Michael and Upadhyay, Shyam and Roth, Dan},
  booktitle={Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)},
  pages={252--262},
  year={2018}
}

@inproceedings{roemmele2011choice,
  title={Choice of plausible alternatives: An evaluation of commonsense causal reasoning},
  author={Roemmele, Melissa and Bejan, Cosmin Adrian and Gordon, Andrew S},
  booktitle={2011 AAAI spring symposium series},
  year={2011}
}

@article{sakaguchi2021winogrande,
  title={Winogrande: An adversarial winograd schema challenge at scale},
  author={Sakaguchi, Keisuke and Bras, Ronan Le and Bhagavatula, Chandra and Choi, Yejin},
  journal={Communications of the ACM},
  volume={64},
  number={9},
  pages={99--106},
  year={2021},
  publisher={ACM New York, NY, USA}
}

@inproceedings{yang2015wikiqa,
  title={Wikiqa: A challenge dataset for open-domain question answering},
  author={Yang, Yi and Yih, Wen-tau and Meek, Christopher},
  booktitle={Proceedings of the 2015 conference on empirical methods in natural language processing},
  pages={2013--2018},
  year={2015}
}

@article{yang2018hotpotqa,
  title={HotpotQA: A dataset for diverse, explainable multi-hop question answering},
  author={Yang, Zhilin and Qi, Peng and Zhang, Saizheng and Bengio, Yoshua and Cohen, William W and Salakhutdinov, Ruslan and Manning, Christopher D},
  journal={arXiv preprint arXiv:1809.09600},
  year={2018}
}

@misc{minimax2025minimax01scalingfoundationmodels,
      title={MiniMax-01: Scaling Foundation Models with Lightning Attention}, 
      author={MiniMax and Aonian Li and Bangwei Gong and Bo Yang and Boji Shan and Chang Liu and Cheng Zhu and Chunhao Zhang and Congchao Guo and Da Chen and Dong Li and Enwei Jiao and Gengxin Li and Guojun Zhang and Haohai Sun and Houze Dong and Jiadai Zhu and Jiaqi Zhuang and Jiayuan Song and Jin Zhu and Jingtao Han and Jingyang Li and Junbin Xie and Junhao Xu and Junjie Yan and Kaishun Zhang and Kecheng Xiao and Kexi Kang and Le Han and Leyang Wang and Lianfei Yu and Liheng Feng and Lin Zheng and Linbo Chai and Long Xing and Meizhi Ju and Mingyuan Chi and Mozhi Zhang and Peikai Huang and Pengcheng Niu and Pengfei Li and Pengyu Zhao and Qi Yang and Qidi Xu and Qiexiang Wang and Qin Wang and Qiuhui Li and Ruitao Leng and Shengmin Shi and Shuqi Yu and Sichen Li and Songquan Zhu and Tao Huang and Tianrun Liang and Weigao Sun and Weixuan Sun and Weiyu Cheng and Wenkai Li and Xiangjun Song and Xiao Su and Xiaodong Han and Xinjie Zhang and Xinzhu Hou and Xu Min and Xun Zou and Xuyang Shen and Yan Gong and Yingjie Zhu and Yipeng Zhou and Yiran Zhong and Yongyi Hu and Yuanxiang Fan and Yue Yu and Yufeng Yang and Yuhao Li and Yunan Huang and Yunji Li and Yunpeng Huang and Yunzhi Xu and Yuxin Mao and Zehan Li and Zekang Li and Zewei Tao and Zewen Ying and Zhaoyang Cong and Zhen Qin and Zhenhua Fan and Zhihang Yu and Zhuo Jiang and Zijia Wu},
      year={2025},
      eprint={2501.08313},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2501.08313}, 
}