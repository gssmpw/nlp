[
  {
    "index": 0,
    "papers": [
      {
        "key": "xie2024moe",
        "author": "Xie, Yanyue and Zhang, Zhi and Zhou, Ding and Xie, Cong and Song, Ziang and Liu, Xin and Wang, Yanzhi and Lin, Xue and Xu, An",
        "title": "MoE-Pruner: Pruning Mixture-of-Experts Large Language Model using the Hints from Its Router"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "lu2024experts",
        "author": "Xudong Lu and Qi Liu and Yuhui Xu and Aojun Zhou and Siyuan Huang and Bo Zhang and Junchi Yan and Hongsheng Li",
        "title": "Not All Experts are Equal: Efficient Expert Pruning and Skipping for Mixture-of-Experts Large Language Models"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "yang2024moe",
        "author": "Yang, Cheng and Sui, Yang and Xiao, Jinqi and Huang, Lingyi and Gong, Yu and Duan, Yuanlin and Jia, Wenqi and Yin, Miao and Cheng, Yu and Yuan, Bo",
        "title": "MoE-I2: Compressing Mixture of Experts Models through Inter-Expert Pruning and Intra-Expert Low-Rank Decomposition"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "liu2024efficient",
        "author": "Liu, Enshu and Zhu, Junyi and Lin, Zinan and Ning, Xuefei and Blaschko, Matthew B and Yan, Shengen and Dai, Guohao and Yang, Huazhong and Wang, Yu",
        "title": "Efficient expert pruning for sparse mixture-of-experts language models: Enhancing performance and reducing inference costs"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "li2023merge",
        "author": "Li, Pingzhi and Zhang, Zhenyu and Yadav, Prateek and Sung, Yi-Lin and Cheng, Yu and Bansal, Mohit and Chen, Tianlong",
        "title": "Merge, then compress: Demystify efficient SMoe with hints from its routing policy"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "chen2024retraining",
        "author": "Chen, I and Liu, Hsu-Shen and Sun, Wei-Fang and Chao, Chen-Hao and Hsu, Yen-Chang and Lee, Chun-Yi and others",
        "title": "Retraining-Free Merging of Sparse Mixture-of-Experts via Hierarchical Clustering"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "isik2023gptzip",
        "author": "Isik, Berivan and Kumbong, Hermann and Ning, Wanyi and Yao, Xiaozhe and Koyejo, Sanmi and Zhang, Ce",
        "title": "Gpt-zip: Deep compression of finetuned large language models"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "liu2024bitdelta",
        "author": "Liu, James and Xiao, Guangxuan and Li, Kai and Lee, Jason D and Han, Song and Dao, Tri and Cai, Tianle",
        "title": "BitDelta: Your Fine-Tune May Only Be Worth One Bit"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "ping2024deltacome",
        "author": "Bowen Ping and Shuo Wang and Hanqing Wang and Xu Han and Yuzhuang Xu and Yukun Yan and Yun Chen and Baobao Chang and Zhiyuan Liu and Maosong Sun",
        "title": "Delta-CoMe: Training-Free Delta-Compression with Mixed-Precision for Large Language Models"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "yao2023deltazip",
        "author": "Yao, Xiaozhe and Klimovic, Ana",
        "title": "DeltaZip: Multi-Tenant Language Model Serving via Delta Compression"
      }
    ]
  }
]