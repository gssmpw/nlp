\section{Experiments}

% To validate the advancements of the proposed \sysname{} framework, we conducted experiments focusing on two key aspects: (1) Main Results: a comparison with state-of-the-art methods to demonstrate the effectiveness of \sysname{}, and (2) Ablation Study: an evaluation of the proposed format optimizer and scoring system to assess their contributions.


\subsection{Experimental Setups}

\textbf{Dataset and Models.}
% We select evaluation tasks from various domains, incorporating widely recognized benchmarks: the reasoning datasets GSM8K~\citep{gsm8k} and MATH500~\citep{math,lightman2023let}, the multiple-choice dataset ARC-Challenge~\citep{clark2018thinksolvedquestionanswering}, and a classification task from the Big-Bench benchmark~\citep{srivastava2023beyond}.
% Our model selection also includes a diverse range: Mistral-7B-v0.1~\citep{jiang2023mistral7b} and LLaMA-3.1-8B~\citep{grattafiori2024llama3herdmodels}, as well as instruction-tuned models such as LLaMA-3-8B-Instruct~\citep{llama3} and Phi-3-Mini-Instruct~\citep{abdin2024phi3technicalreporthighly}. This selection ensures a comprehensive assessment of \sysname{}'s performance across diverse model types and task formats. Furthermore, we utilized GPT4 (2024-05-01-preview) as an optimizer for content optimization and format generation~\citep{openai2024gpt4technicalreport}.
To rigorously evaluate \sysname{}, we selected a diverse set of tasks and models. Our benchmark tasks span various domains and complexities, including: 
\begin{itemize}[itemsep=-4pt, topsep=0pt]
    \item \textbf{Reasoning}: GSM8K~\citep{gsm8k} and MATH500~\citep{math,lightman2023let} which require complex mathematical reasoning abilities.
    \item \textbf{Multiple-choice}: ARC-Challenge \citep{clark2018thinksolvedquestionanswering}, demanding understanding and selection among alternatives.
    \item \textbf{Classification}: The \textit{Implicatures} task from the Big-Bench benchmark~\citep{srivastava2023beyond} to evaluate classification proficiency.
\end{itemize}
Our model selection includes a mix of foundational and instruction-tuned models to understand the generalizability of our approach:
\begin{itemize}[itemsep=-4pt, topsep=0pt]
    \item \textbf{Foundational Models}: Mistral-7B-v0.1 \citep{jiang2023mistral7b} and LLaMA-3.1-8B~\citep{grattafiori2024llama3herdmodels} represent pre-trained models.
    \item \textbf{Instruction-Tuned Models}: LLaMA-3-8B-Instruct \citep{llama3} and Phi-3-Mini-Instruct~\citep{abdin2024phi3technicalreporthighly} represent models specifically fine-tuned for instruction following.
\end{itemize}
Furthermore, we use GPT-4 (2024-05-01-preview) as the LLM optimizer for content mutation and format generation~\citep{openai2024gpt4technicalreport}.

% \subsubsection{Settings for \sysname{}}
\noindent \textbf{Implementation Details.}
The training process involved $20$ iterative rounds, each consisting of content and format optimization.
% Content optimizer
During content optimization, case-diagnosis and Monte Carlo sampling each generate 4 prompts per round. A set of 40 test cases is used, with 5 correct and incorrect cases leveraged for case-diagnosis. The number of prompt-structured components decreases progressively from 4 to 1, narrowing the search space over time to enhance efficiency.
% Format optimizer
For format optimization, 4 UCT-selected formats and 4 newly generated formats are used to generate new prompts. The coefficient in the UCT selection process $\alpha$ is set to $1e-3$. 
Beam search, with a budget of 8, is employed during mutations to ensure effective exploration. Eval data sizes are configured as 50, 300, 500, and 500 for BigBench-Classification, MATH500, GSM8K, and ARC-Challenge, respectively. 
The best performing prompt on the evaluation set for each method was selected and reported on the test set.

\noindent \textbf{Baselines.} 
To evaluate the effectiveness of \sysname{}, we compared againt several commonly used and popular baselines.
GrIPS~\citep{prasad2023grips} performs syntactic phrase-level edits in instruction, representing a non-LLM-based optimization approach. APE~\citep{zhou2023ape} and ProTeGi~\citep{C_2023EMNLP_APO} both employ LLM to optimize prompt content, but differ in mutation strategy. APE adopts an instruction induction approach, while ProTeGi leverages test cases feedback with LLM to guide the mutation process.
SAMMO~\citep{C_2024EMNLP_sammo} introduces a structured framework that incorporates a preliminary format mutation strategy, which relies on random selection from a predefined format pool.
This choice of baselines enables a comprehensive assessment of \sysname{}'s capabilities against various types of optimization approaches. All methods were evaluated using consistent experimental configurations to ensure a fair comparison.


\noindent \textbf{Initial Prompts.} 
% For baseline prompts, except for GRIPS, which requires an initial instruction, we use a reasonable non-instructional prompt with one in-context example for other tasks and models. Specifically, for reasoning tasks like GSM8K and MATH500, we include one example with chain-of-thought to enhance reasoning. 
% Additionally, we report commonly used baseline prompt for them including 8-shot prompt for GSM8K and 4-shot prompt for MATH500.
% Detailed configurations of the initial prompts are provided in the Appendix \ref{init}. 
To establish a reasonable starting point, we employed a single in-context example without any further instruction as the initial prompt for each model and task, except for GrIPS which requires an initial instruction. Chain-of-Thought examples were employed for the reasoning tasks.  We also report common baseline prompts, including 8-shot for GSM8K and 4-shot for MATH500. A comprehensive list of our initial prompts is in Appendix \ref{init}.

\subsection{Main Results}
Table~\ref{tbl:mainresults} summarizes the performance of \sysname{} in comparison with several state-of-the-art methods across four datasets.
The results highlight the superior performance of \sysname{}, significantly outperforming the baseline prompt as well as competing methods.
% Overall
We observed that pre-trained models exhibit greater sensitivity to prompt formatting, leading to substantial improvements when optimized by \sysname{}.
Notably, optimized prompts for pre-trained models tend to be longer and incorporate more in-context examples, suggesting that these characteristics better align with the optimization needs of pre-trained models (see Appendix \ref{apx:shot}).
In contrast, instruction-tuned models display relatively more robust results and smaller gains, likely due to their inherent adaptability and generalization.

% Reasoning
For the reasoning tasks, GSM8K and MATH, prompt optimization is especially impactful due to the sensitivity of these tasks to prompt structure.
\sysname{}, which integrates unified content and format optimization, delivers significant performance gains. Specifically, the improvement for GSM8K is more evident compared to the more challenging MATH task, where the inherent complexity limits the magnitude of improvement.
Moreover, feedback-based methods like ProTeGi, SAMMO, and \sysname{}, consistently outperform the other baselines because they leverage iterative feedback for prompt refinement. In contrast, GRIPS, which is limited to phrase-level mutations, exhibits marginal improvements. These results underline the effectiveness of the integrated optimization strategy adopted by \sysname{}.  The selected optimal prompts discovered by our approach can be found in Appendix \ref{opt_prompts}.


% The results clearly demonstrate the superior performance of \sysname{}, underscoring its effectiveness in prompt optimization.
% 首先，我们发现与没有经过prompt engineering 的baseline prompt相比，我们的方法大幅提升了准确度。同时，（与其他method相比）我们的方法也优势很大

% Global的:
% （1）我们发现pre-trained model对format更敏感，相对于instruction-tuned models.
% （2）进一步研究prompt，发现：提升更大的pretrain model的prompt：1. example和longer text； 2. instruction-tune的模型，不需要那么多。
% 结论：可能是因为instruction-tuned models适配了更多的任务... 导致他们can benefit less substantially from \sysname{}.

% reasoning：
% reasoning task非常challenging，对prompt更敏感。prompt的影响更显著。我们的方法with unified content and format optimization 的prompt optimization效果更好。特别的， 对于简单的（GSM8K）提升会更明显，相对于难的（MATH）。
% ProTeGi、SAMMO和\sysname{}都是基于feedback对content做修改的，我们可以看出这类方法对reasoning task会更加有效。同时，我们可以发现GRIPS这种对文字做修改的prompt对reasoning task的提升微乎其微。基本上和baseline持平。

% Selected searched optimal prompts are shown in Appendix \ref{opt_prompts}.

\subsection{Ablation Study}

\noindent \textbf{Impact of the Format Optimizer.}
% \sysname{} integrates a comprehensive format optimization process, combining novel format generation and a UCT-based format selection strategy.
\sysname{} incorporates a unique format optimization process, leveraging LLM for format generation and a UCT-based strategy for format selection.
% To evaluate its effectiveness, we conducted comparisons under two alternative settings: (1) \sysname{}$_c$, where only the prompt content is optimized during each iteration while keeping the format fixed, and (2) \sysname{}$_c$+format, where the optimal prompt searched by \sysname{}$_c$ is subsequently subjected to a separate format optimization process.
To evaluate its effectiveness, we evaluated two variations of our method: (1) \sysname{}$_c$, which optimizes content while keeping format fixed, and (2) \sysname{}$_c$+Format, which first optimizes content, then performs a separate format optimization step.
% employs a content-format unified optimization for prompt. The optimization for format 
% The dual-optimizer design in \sysname{} allows us to evaluate the impact of the format optimizer by removing it during each iteration, thereby assessing the contribution of iterative format optimization to the overall prompt optimization process.
% \key{rephrase, intro}
% As shown in Table \ref{tab:ablation_format},
% we compared with two settings: Only prompt content is optimized in each iteration (\sysname{}$_c$), and the optimal prompt searched by \sysname{}$_c$ is further subjected to a separate format optimization process, denoted as (\sysname{}$_c$+format).
% we include ProTegi~\citep{C_2023EMNLP_APO} which optimizes only the content as a baseline. 
% we denote \sysname{}$_c$ as the variant where only prompt content is optimized in each iteration, and \sysname{}$_c$+format as the method where the optimal prompt searched by \sysname{}$_c$ is further subjected to a separate format optimization process.
% As shown in Table \ref{tab:ablation_format}, the results demonstrate that neither of these variants outperforms the full \sysname{} approach, which incorporates integrated content and format optimization. This finding highlights the effectiveness of format optimization and necessity of the integrated search mechanism.
Table~\ref{tab:ablation_format} shows that both \sysname{}$_c$ and \sysname{}$_c$+Format underperform compared to the full \sysname{} approach, highlighting the importance of the integrated content and format optimization approach. The need for a joint optimization process which addresses the interdependence of content and format is essential for prompt optimization.
% This finding highlights the necessity of the integrated search mechanism.
% and shows that incorporating format mutation significantly enhances the upper bound of content optimization, underscoring its critical role in achieving superior performance.
% incorporating format mutation significantly enhances the upper bound of content optimization.

% To validate the effectiveness of format mutation in our proposed \sysname{} method, we conducted a series of ablation studies. 

\begin{table}[h]
  \centering
  \resizebox{0.5\textwidth}{!}{
  {\footnotesize
  \begin{tabular}{l|l|c|c}
    \hline
    \textbf{Task} & \textbf{Method} & \textbf{LLaMA-3.1-8B} & \textbf{LLaMA-3-8B-Instruct} \\
    \hline
      \multirow{4}{*}{GSM8K}   & ProTeGi          &  54.74    &   75.36  \\
                               & \sysname{}$_{c}$   &  {58.07}   &   {77.71}     \\
                     & \sysname{}$_c$+Format & 61.94      &   79.30       \\
                     & \textbf{\sysname{}}         &  \textbf{63.38}    &   \textbf{80.74}  \\
    \hline
   \multirow{4}{*}{BBC}                   & ProTeGi          &  81.00    &   82.00   \\
              & \sysname{}$_{c}$   &  85.00    &   {85.00}   \\
                       & \sysname{}$_c$+Format &  {88.00}     &   89.00       \\
                     & \textbf{\sysname{}}         &  \textbf{90.00}    &   \textbf{91.00}   \\
    \hline
  \end{tabular}
  }
  }
  \vspace{-1ex}
  \caption{Ablation study of the format optimizer and content optimizer. \sysname{}${c}$ performs content optimization with a fixed format. \sysname{}${c}$+Format performs format optimization after content optimization.}
  % \caption{Ablation study across different models, methods, and tasks. \sysname{}$_{c}$: only involves content optimization in \sysname{} with format fixed. {CFPO$_c$+format}}
  \label{tab:ablation_format}
  \vspace{-2ex}
\end{table}

\noindent \textbf{Effectiveness of Format Generation.}
% \sysname{} incorporates a format exploration mechanism designed to dynamically generate novel formats during the optimization process, aiming to explore superior prompt structures beyond those available in the initial format pool.
% To evaluate its effectiveness,
% We compared \sysname{} against a baseline that exclusively selects formats directly from the initial pool without generating new ones to evaluate the format generation.
We compared the full \sysname{} approach against a variant that uses format from initial format pool without using LLM for generation.
As presented in Table~\ref{tab:ablation_format_gen}, \sysname{} with format generation consistently outperforms the baseline relying solely on the initial pool.
These results demonstrate the effectiveness of the proposed format exploration mechanism in enhancing both the quality and diversity of prompts.
% As shown in Table~\ref{tab:ablation_format_gen}, the generated formats in certain cases outperform those from the initial pool. This demonstrates the efficacy of the proposed format exploration mechanism in uncovering superior prompt structures.

\begin{table}[h]
  \centering
  \resizebox{0.5\textwidth}{!}{
  {\footnotesize
  \begin{tabular}{l|l|c|c}
    \hline
    \textbf{Task} & \textbf{Method} & \textbf{LLaMA-3.1-8B} & \textbf{LLaMA-3-8B-Instruct} \\
    \hline
    \multirow{2}{*}{{GSM8K}} & w/o Format Gen        & 62.70       & 78.85  \\
                                    & with Format Gen       & \textbf{63.38}       & \textbf{80.74}  \\
    \hline
    \multirow{2}{*}{{BBC}}   & w/o Format Gen        & 88.00       &  {87.00} \\
                                    & with Format Gen       & \textbf{90.00}       & \textbf{91.00}  \\
    \hline
  \end{tabular}
  }
  }
  \vspace{-1ex}
  \caption{Impact of format generation during prompt optimization.}
  \label{tab:ablation_scoring}
  \vspace{-2ex}
\end{table}

% \noindent \textbf{Effectiveness of the Format Scoring System.}
\noindent \textbf{Effectiveness of Format Selection.}
% \sysname{} leverages a UCT-based selection strategy that balances exploitation and exploration during the optimization process.
% To assess its effectiveness, 
% We compared our format selection strategy with two alternative settings: random selection from the format pool and deterministic selection without exploration (\textit{i.e.},  $\alpha=0$ in Eq. (\ref{eq:uct})). 
We further evaluated our UCT-based format selection process, compared it to a random selection from the format pool and a greedy selection without exploration (using $\alpha=0$ in Eq. (\ref{eq:uct})). 
As presented in Table\ref{tab:ablation_scoring}, \sysname{} consistently achieves the best performance across all experimental settings, demonstrating the efficacy of the UCT-based selection strategy.

\begin{table}[h]
  \centering
  \resizebox{0.5\textwidth}{!}{
  {\footnotesize
  \begin{tabular}{l|l|c|c}
    \hline
    \textbf{Task} & \textbf{Method} & \textbf{LLaMA-3.1-8B} & \textbf{LLaMA-3-8B-Instruct} \\
    \hline
    % random select
    % UCT only Q c=0
    % ours
                     & Random          &  62.40        &   78.82      \\
      GSM8K          & UCT($\alpha=0$)    &   {63.23}      &   79.08      \\
                     & UCT(ours)    &  \textbf{63.38}        &   \textbf{80.74}      \\
    \hline
                     & Random       &   85.00       &   87.00      \\
      BBH          & UCT($\alpha=0$)    &  86.00        &   88.00      \\
                     & UCT(ours)    &  \textbf{90.00}        &   \textbf{91.00}      \\
    \hline
  \end{tabular}
  }
  }
  \vspace{-1ex}
  \caption{Impact of different format selection strategies during optimization.}
  \label{tab:ablation_format_gen}
  \vspace{-2ex}
\end{table}

% \sysname{} employs a UCT-based scoring system, which consider exploitation and exploration in the same time.
% To evaluate its effectiveness, we compared it against two alternative settings: randomly selection from format pool and deterministic selection without exploration (i.e., $c=0$ in Eq.\ref{eq:uct}). 
% As shown in Table~\ref{tab:ablation_scoring}, \sysname{} achieved best performance, demonstrating the effectiveness of UCT-based scoring system.
% \sysname{} uses a UCT-based scoring system to search prompt, 

% Table~\ref{tab:ablation_scoring} highlights the advantages of our designed scoring system. By using UCT-based selection, which balances exploration and exploitation, our approach outperforms both random selection and deterministic selection without exploration (c=0).
% 先讲这个场景下的变量是什么（\sysname{}用UCT scoring system去做search），setting（为了探究Scoring system的作用，我们设定了random和只用Q即c=0的两种setting），结果（发现我们的分数最高，说明这种scoring的设置最有效）


\noindent \textbf{Effectiveness of the Content Optimizer.}
% As shown in Table. \ref{tab:ablation_format}, we include ProTegi~\citep{C_2023EMNLP_APO} which optimizes only the content as a baseline, our \sysname{}$_{c}$, which leverages structured prompting and integrates correct cases for feedback, achieves substantial performance improvements.
As presented in Table \ref{tab:ablation_format}, we include ProTeGi~\citep{C_2023EMNLP_APO}, a baseline that optimizes only the content. In contrast, our \sysname{}$_{c}$, which incorporates structured prompting and integrates correct cases for diagnosis, achieves significant performance improvements, which highlights the effectiveness of our content optimization strategy.