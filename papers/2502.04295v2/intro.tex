\section{Introduction}

\begin{figure}[h]
    \vspace{-2ex}
    \includegraphics[width=\columnwidth]{fig/teaser6.png}
    \vspace{-3.5ex}
    \caption{
    The crucial role of prompt formatting and its interaction with content. \textbf{(A)}: Model-specific format biases: Illustrates the performance sensitivity of two LLMs to different format styles on the GSM8K task, showing substantial variability in the effectiveness of 10 randomly selected formats. \textbf{(B)}: For seven different prompt contents evaluated across 24 distinct formats, performance variations show the complex, interdependent relationship between prompt content and structure, demonstrating that no single format universally maximizes effectiveness.}
    \vspace{-3ex}
\label{fig:teaser}
\end{figure}

\begin{figure*}[t]
  \vspace{-2ex}
  \includegraphics[width=\linewidth]{fig/pipeline3.png}
  \vspace{-3ex}
  \caption {Illustration of the \sysname{} pipeline within a single iteration round. In the initial Component-wise Content Optimization stage, case-diagnosis and Monte-Carlo sampling are employed for content mutation. Subsequently, the Format Optimization stage identifies the most suitable format for each content candidate. The yellow dashed line indicates where the LLM optimizer is employed to guide the optimization process.}
  \label{fig:pipeline}
  \vspace{-2ex}
\end{figure*}

% Large Language models (LLM) have demonstrated impressive achievements in various fields~\cite{openai2024gpt4technicalreport}. In the applications of LLMs, prompting sentence serves as a communication bridge connecting human users or developers and the LLM systems. Crafting effective prompts is paramount for deriving accurate and relevant responses from LLMs for specific tasks. Studies have shown that expert-designed prompts could significantly enhance LLM performance~\citep{brown2020incontextlearning, wei2023chainofthought,A_2024_prompt_survey}.
Large Language Models (LLMs) have demonstrated impressive achievements across various domains~\cite{openai2024gpt4technicalreport}. The effectiveness of LLMs in real-world applications is fundamentally dependent on the design of effective prompts, which serve as an essential interface between human users or developers and the LLM system. Studies have shown that expert-designed prompts could significantly enhance LLM performance~\citep{brown2020incontextlearning, wei2023chainofthought,A_2024_prompt_survey}.

% However, manual design of prompts presents significant challenges, primarily due to the high sensitivity of LLMs to prompt characteristics, such as text and structure, which varies across different models and tasks~\cite{salinas2024butterflyeffectalteringprompts,zhuo2024prosaassessingunderstandingprompt,Jiang2022promptMaker, chi2023JohnnyPrompt, C_2024ICLR_formatspread}. To reduce the complexity for prompt optimization, the advanced LLMs are introduced to serve as optimizers, automatically adapting and evolving prompts to improve the effectiveness~\cite{C_2023EMNLP_APO, C_2024EMNLP_sammo, C_2024ICLR_LLMasOPT}.
However, manual design of prompts presents significant challenges, primarily due to the high sensitivity of LLMs to subtle variations in prompt characteristics, including both textual content and structural format~\citep{Jiang2022promptMaker, chi2023JohnnyPrompt, salinas2024butterflyeffectalteringprompts}. These sensitivities are further complicated by variations across different models and tasks~\cite{zhuo2024prosaassessingunderstandingprompt, C_2024ICLR_formatspread}. To alleviate these difficulties, automated prompt optimization techniques, often leveraging the power of LLMs themselves, have proven to be an effective approach to adapt and refine prompts~\cite{C_2023EMNLP_APO, C_2024EMNLP_sammo, C_2024ICLR_LLMasOPT}. However, existing research primarily focuses on optimizing \textit{prompt content}, while overlooking a critical and largely unexplored dimension: the \textbf{prompt formatting}.

% Despite these innovative approaches, there remains a critical yet under-explored aspect in current literature: \textbf{the formatting of prompts}. 
% Our preliminary evaluations provide key insights into the role of format in prompt optimization. As illustrated in Figure~\ref{fig:teaser}(A), we have found that different models exhibit distinct preferences for prompt content and format. Notably, some formats that excel on one model may under-perform or even fail on another, indicating sophisticated model-specific format preferences~\cite{C_2024ICLR_formatspread}. Furthermore, Figure~\ref{fig:teaser}(B) demonstrates a complex interplay between the content and format of prompts, where no single format consistently outperforming others across different contents. This indicates the absence of a universally superior format, and highlights the impracticality of predefining a \textit{"best"} format for all contents. Consequently, optimizing prompt content and format must be treated as an integrated process rather than independent efforts, which emphasizes the necessity of a joint search approach to concurrently explore both aspects in prompt optimization. 
Our preliminary investigations, as illustrated in Figure~\ref{fig:teaser}, provide valuable insights into the role of prompt format in prompt optimization. We have observed that different LLMs display distinct preferences, with some formats performing well on one model but failing on another. This suggests sophisticated, model-specific format biases~\cite{C_2024ICLR_formatspread}. Furthermore, we have identified a complex interplay between prompt content and format, where no single format consistently outperforms others across all contents. This lack of a universally optimal format highlights the impracticality of predefining format, and underscores the need for a joint optimization approach that treats prompt content and format as interdependent variables.

\begin{figure*}[t]
  \vspace{-2ex}
  \includegraphics[width=\linewidth]{fig/template2.png}
  \vspace{-4.5ex}
  \caption {An illustrative example of our Structured Prompt Template. This template systematically organizes the prompt into distinct components, each serving a specific functional role. When formulating a prompt, the template first employs a Query format to present examples and queries, and then integrates all content components via the Prompt Renderer to construct the comprehensive prompt string.}
  \vspace{-2ex}
  \label{fig:template}
\end{figure*}

% In this paper, we propose a novel methodology, \fullsysname{}, which simultaneously addresses the optimization of both prompt content and format through an integrated process of alternating refinement. \sysname{} employs distinct strategies for content and format optimization, tailored to address the unique challenges presented by the distinct search spaces of content and format. Specifically, for content optimization, \sysname{} utilizes a strategy that incorporates performance feedback, such as outcomes from test cases, to guide the evolution of natural language mutations in a direction that enhances prompt effectiveness. Format optimization, on the other hand, navigates through a discrete set of format options without clear directional indicators for improvement, for which \sysname{} designs an effective format optimizer with an efficient approach to identify promising prompts.
To address these limitations, we introduce \textbf{\fullsysname{}}, an innovative methodology that concurrently optimizes both prompt content and format through an iterative refinement process. \sysname{} employs distinct optimization strategies tailored to the unique search spaces of content and format. Content optimization is guided by performance feedback and Monte Carlo sampling, leveraging natural language mutations to enhance prompt effectiveness. For format optimization, \sysname{} explores a discrete set of format options through a dynamic exploration strategy designed to identify optimal formats without requiring a prior knowledge.

% The format optimizer of \sysname{} embodies the principle of structured thinking, addressing two key dimensions: the \textit{Prompt Renderer}, which organizes of all components within a prompt~\citep{he2024doespromptformattingimpact}, and the \textit{Query Format}, which presents the in-context learning examples and queries~\citep{voronov2024mindformatconsistentevaluation,salinas2024butterflyeffectalteringprompts}. By integrating these dimensions, \sysname{} defines a structured template that distinguishes between content and format types, enabling the efficient identification of prompt content and format.
Specifically, \sysname{}'s format optimizer leverages the principles of structured thinking, operating along two key dimensions: the \textit{Prompt Renderer}, which governs the organizational structure of all components within a prompt~\citep{he2024doespromptformattingimpact}, and the \textit{Query Format}, which dictates the presentation of in-context learning examples and queries~\citep{voronov2024mindformatconsistentevaluation,salinas2024butterflyeffectalteringprompts}. By integrating these two dimensions, \sysname{} defines a structured template that effectively distinguishes between content and format types, enabling the efficient identification of high-performing prompts.

% Our contributions are threefold: (1) We introduce \fullsysname{}, a groundbreaking methodology that iteratively optimizes both the content and format of prompts. (2) We introduce an efficient and effective strategy for format optimization, which iteratively generates new formats and employs a scoring system to select the most suitable format from a dynamically expanding pool. (3) Our extensive evaluation of \sysname{} across a range of tasks and with multiple leading open-source LLMs showcases its superior capability in improving LLM performance in diverse scenarios.
Our primary contributions are threefold: (1) We propose \textbf{\sysname{}}, an innovative approach to simultaneously optimizes prompt content and format using an iterative process. (2) We introduce an efficient strategy for dynamic format optimization that generates new formats in an iterative manner and evaluates formats instance through a scoring system to select the best option. (3) Through extensive evaluations across diverse tasks and multiple open-source LLMs, we demonstrate that \sysname{} consistently improves LLM performance in a measurable and effective manner.