# 命名按照 "J/C/B/A_年份期刊会议名_关键词"
# J: journal, C: Conference, B:Book, A:Arxiv.
# 如果文章发表，尽量去发表的官网上导出引用，不保留DOI和url 
# 如果是arxiv上，在arxiv官网上导出misc格式的条目 (尽量不引用arxiv文献)

# 书籍(book)包括author, title, publisher, year

# 会议文章(inproceedings)包括author, title, booktitle, year ,pages (会议的title直接写简写，定义在了strings.bib) 

# 期刊文章(article)包括 author, title, journal, volume, number, year, pages (其中number指期刊当年的issue号码（第几期），有些引用里没有直接给出，需要去期刊官网上搜索all issues根据volum号码数是当年第几期) 


@article{J_2024TMLR_moreagent,
      title={More agents is all you need},
      author={Li, Junyou and Zhang, Qin and Yu, Yangbin and Fu, Qiang and Ye, Deheng},
      journal={Transactions on Machine Learning Research},
      year={2024},
}

@misc{A_2024_AFlow,
      title={AFlow: Automating Agentic Workflow Generation}, 
      author={Jiayi Zhang and Jinyu Xiang and Zhaoyang Yu and Fengwei Teng and Xionghui Chen and Jiaqi Chen and Mingchen Zhuge and Xin Cheng and Sirui Hong and Jinlin Wang and Bingnan Zheng and Bang Liu and Yuyu Luo and Chenglin Wu},
      year={2024},
}

@misc{A_2024_GReaTer,
      title={GReaTer: Gradients over Reasoning Makes Smaller Language Models Strong Prompt Optimizers}, 
      author={Sarkar Snigdha Sarathi Das and Ryo Kamoi and Bo Pang and Yusen Zhang and Caiming Xiong and Rui Zhang},
      year={2024},
}

@inproceedings{C_2024ICLR_promptagent,
    author={Wang, Xinyuan and Li, Chenxi and Wang, Zhen and Bai, Fan and Luo, Haotian and Zhang, Jiayou and Jojic, Nebojsa and Xing, Eric P and Hu, Zhiting},
    title = {PromptAgent: Strategic Planning with Language Models Enables Expert-level Prompt Optimization},
    booktitle = ICLR,
    year = 2024,
}

@misc{A_2024_MAGIC,
      title={MAGIC: Generating Self-Correction Guideline for In-Context Text-to-SQL}, 
      author={Arian Askari and Christian Poelitz and Xinye Tang},
      year={2024},
      eprint={2406.12692},
}

@misc{A_2023_optimize_code,
      title={Language Models Can Teach Themselves to Program Better}, 
      author={Patrick Haluptzok and Matthew Bowers and Adam Tauman Kalai},
      year={2023},
}

@misc{A_2024_tool_making,
      title={Large Language Models as Tool Makers}, 
      author={Tianle Cai and Xuezhi Wang and Tengyu Ma and Xinyun Chen and Denny Zhou},
      year={2024},
      eprint={2305.17126},
}

@misc{A_2024_prompt_survey,
      title={The Prompt Report: A Systematic Survey of Prompting Techniques}, 
      author={Sander Schulhoff and Michael Ilie and Nishant Balepur and Konstantine Kahadze and Amanda Liu and Chenglei Si and Yinheng Li and Aayush Gupta and HyoJung Han and Sevien Schulhoff and Pranav Sandeep Dulepet and Saurav Vidyadhara and Dayeon Ki and Sweta Agrawal and Chau Pham and Gerson Kroiz and Feileen Li and Hudson Tao and Ashay Srivastava and Hevander Da Costa and Saloni Gupta and Megan L. Rogers and Inna Goncearenco and Giuseppe Sarli and Igor Galynker and Denis Peskoff and Marine Carpuat and Jules White and Shyamal Anadkat and Alexander Hoyle and Philip Resnik},
      year={2024}
}

@incollection{Bengio+chapter2007,
author = {Bengio, Yoshua and LeCun, Yann},
booktitle = {Large Scale Kernel Machines},
publisher = {MIT Press},
title = {Scaling Learning Algorithms Towards {AI}},
year = {2007}
}

@article{Hinton06,
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
journal = {Neural Computation},
pages = {1527--1554},
title = {A Fast Learning Algorithm for Deep Belief Nets},
volume = {18},
year = {2006}
}

@book{goodfellow2016deep,
title={Deep learning},
author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
volume={1},
year={2016},
publisher={MIT Press}
}

@misc{promptbreeder,
      title={Promptbreeder: Self-Referential Self-Improvement Via Prompt Evolution}, 
      author={Chrisantha Fernando and Dylan Banarse and Henryk Michalewski and Simon Osindero and Tim Rocktäschel},
      year={2023},
      eprint={2309.16797},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2309.16797}, 
}

@inproceedings{C_2024ICLR_LLMasOPT,
    author = {Chengrun Yang and Xuezhi Wang and Yifeng Lu and Hanxiao Liu and Quoc V. Le and Denny Zhou and Xinyun Chen},
    title = {{Large Language Models as Optimizers}},
    booktitle = ICLR,
    year = 2024
}

@misc{wei2023chainofthought,
      title={Chain-of-Thought Prompting Elicits Reasoning in Large Language Models}, 
      author={Jason Wei and Xuezhi Wang and Dale Schuurmans and Maarten Bosma and Brian Ichter and Fei Xia and Ed Chi and Quoc Le and Denny Zhou},
      year={2023},
      eprint={2201.11903},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2201.11903}, 
}

 @inproceedings{C_2024ICLR_formatspread,
    author = {Melanie Sclar and Yejin Choi and Yulia Tsvetkov and Alane Suhr},
    title = {{Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting}},
    booktitle = ICLR,
    year = 2024,
}

@inproceedings{Jiang2022promptMaker,
author = {Jiang, Ellen and Olson, Kristen and Toh, Edwin and Molina, Alejandra and Donsbach, Aaron and Terry, Michael and Cai, Carrie J},
title = {PromptMaker: Prompt-based Prototyping with Large Language Models},
year = {2022},
isbn = {9781450391566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491101.3503564},
doi = {10.1145/3491101.3503564},
abstract = {Prototyping is notoriously difficult to do with machine learning (ML), but recent advances in large language models may lower the barriers to people prototyping with ML, through the use of natural language prompts. This case study reports on the real-world experiences of industry professionals (e.g. designers, program managers, front-end developers) prototyping new ML-powered feature ideas via prompt-based prototyping. Through interviews with eleven practitioners during a three-week sprint and a workshop, we find that prompt-based prototyping reduced barriers of access by substantially broadening who can prototype with ML, sped up the prototyping process, and grounded communication between collaborators. Yet, it also introduced new challenges, such as the need to reverse-engineer prompt designs, source example data, and debug and evaluate prompt effectiveness. Taken together, this case study provides important implications that lay the groundwork toward a new future of prototyping with ML.},
booktitle = {Extended Abstracts of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {35},
numpages = {8},
location = {New Orleans, LA, USA},
series = {CHI EA '22}
}

@misc{chi2023JohnnyPrompt,
    title={ Why Johnny Can’t Prompt: How Non-AI Experts Try (and Fail) to Design LLM Prompts },
    author={ J. D. Zamfirescu-Pereira and Richmond Y. Wong and Bjoern Hartmann and Qiang Yang },
    year={ 2023 },
    doi={ 10.1145/3544548.3581388 },  
  }

@inproceedings{brown2020incontextlearning,
author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
title = {Language models are few-shot learners},
year = {2020},
booktitle = NIPS,
}

@inproceedings{C_2023EMNLP_APO,
      author={Reid Pryzant and Dan Iter and Jerry Li and Yin Tat Lee and Chenguang Zhu and Michael Zeng},
      title={Automatic Prompt Optimization with "Gradient Descent" and Beam Search}, 
    booktitle = EMNLP,
    year = 2023,
    pages = {7957–7968},
}


@inproceedings{C_2024EMNLP_sammo,
    title = {Symbolic Prompt Program Search: A Structure-Aware Approach to Efficient Compile-Time Prompt Optimization},
    author = {Schnabel, Tobias and Neville, Jennifer},
    booktitle = EMNLP,
    year = {2024},
    pages = {670--686},
}

@misc{zhou2023ape,
      title={Large Language Models Are Human-Level Prompt Engineers}, 
      author={Yongchao Zhou and Andrei Ioan Muresanu and Ziwen Han and Keiran Paster and Silviu Pitis and Harris Chan and Jimmy Ba},
      year={2023},
      eprint={2211.01910},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2211.01910}, 
}

@misc{fernando2023promptbreeder,
      title={Promptbreeder: Self-Referential Self-Improvement Via Prompt Evolution}, 
      author={Chrisantha Fernando and Dylan Banarse and Henryk Michalewski and Simon Osindero and Tim Rocktäschel},
      year={2023},
      eprint={2309.16797},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2309.16797}, 
}

@misc{hu2022knowledgeableprompttuning,
      title={Knowledgeable Prompt-tuning: Incorporating Knowledge into Prompt Verbalizer for Text Classification}, 
      author={Shengding Hu and Ning Ding and Huadong Wang and Zhiyuan Liu and Jingang Wang and Juanzi Li and Wei Wu and Maosong Sun},
      year={2022},
      eprint={2108.02035},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2108.02035}, 
}


@inproceedings{C_2024ICLR_DSPy,
  title={DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines},
  author={Khattab, Omar and Singhvi, Arnav and Maheshwari, Paridhi and Zhang, Zhiyuan and Santhanam, Keshav and Vardhamanan, Sri and Haq, Saiful and Sharma, Ashutosh and Joshi, Thomas T. and Moazam, Hanna and Miller, Heather and Zaharia, Matei and Potts, Christopher},
  booktitle = ICLR,
  year={2024}
}

@misc{prasad2023grips,
      title={GrIPS: Gradient-free, Edit-based Instruction Search for Prompting Large Language Models}, 
      author={Archiki Prasad and Peter Hase and Xiang Zhou and Mohit Bansal},
      year={2023},
      eprint={2203.07281},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2203.07281}, 
}

@inproceedings{C_2024COLM_stop,
      author={Eric Zelikman and Eliana Lorch and Lester Mackey and Adam Tauman Kalai},
      title={{Self-Taught Optimizer (STOP): Recursively Self-Improving Code Generation}}, 
    booktitle = COLM,
    year = 2024,
}
}


@misc{opsahlong2024dspy,
      title={Optimizing Instructions and Demonstrations for Multi-Stage Language Model Programs}, 
      author={Krista Opsahl-Ong and Michael J Ryan and Josh Purtell and David Broman and Christopher Potts and Matei Zaharia and Omar Khattab},
      year={2024},
      eprint={2406.11695},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.11695}, 
}


@inproceedings{C_2023ICLR_TEMPERA,
      author={Tianjun Zhang and Xuezhi Wang and Denny Zhou and Dale Schuurmans and Joseph E. Gonzalez},
      title={TEMPERA: Test-Time Prompting via Reinforcement Learning}, 
    booktitle = ICLR,
    year = 2023,
}

@misc{A_2024_self_evolution_survey,
      title={A Survey on Self-Evolution of Large Language Models}, 
      author={Zhengwei Tao and Ting-En Lin and Xiancai Chen and Hangyu Li and Yuchuan Wu and Yongbin Li and Zhi Jin and Fei Huang and Dacheng Tao and Jingren Zhou},
      year={2024},
      eprint={2404.14387},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2404.14387}, 
}

@inproceedings{kocsis2006bandit,
  title={Bandit based monte-carlo planning},
  author={Kocsis, Levente and Szepesv{\'a}ri, Csaba},
  booktitle={European conference on machine learning},
  pages={282--293},
  year={2006},
  organization={Springer}
}


@misc{O_2023_CRISPE,
  author       = {Matt Nigh},
  title        = {ChatGPT3 Free Prompt List},
  year         = {2023},
  howpublished = {\url{https://github.com/mattnigh/ChatGPT3-Free-Prompt-List}},
}

@@misc{O_2023_Autogpt,
    title = {Auto-GPT},
    author = {WHO},
    year = {2023},
    howpublished = {\url{https://github.com/Significant-Gravitas/AutoGPT}},
}

@misc{A_2024_LangGPT,
      title={LangGPT: Rethinking Structured Reusable Prompt Design Framework for LLMs from the Programming Language}, 
      author={Ming Wang and Yuanzhong Liu and Xiaoyu Liang and Songlian Li and Yijie Huang and Xiaoming Zhang and Sijia Shen and Chaofeng Guan and Daling Wang and Shi Feng and Huaiwen Zhang and Yifei Zhang and Minghui Zheng and Chi Zhang},
      year={2024},
      eprint={2402.16929},
}

% Jiahang
@misc{he2024doespromptformattingimpact,
      title={Does Prompt Formatting Have Any Impact on LLM Performance?}, 
      author={Jia He and Mukund Rungta and David Koleczek and Arshdeep Sekhon and Franklin X Wang and Sadid Hasan},
      year={2024},
      eprint={2411.10541},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2411.10541}, 
}

@misc{openai2024gpt4technicalreport,
      title={GPT-4 Technical Report}, 
      author={OpenAI},
      year={2024},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2303.08774}, 
}

@misc{abdin2024phi3technicalreporthighly,
      title={Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone}, 
      author={Microsoft},
      year={2024},
      eprint={2404.14219},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2404.14219}, 
}


@misc{sahoo2024systematicsurveypromptengineering,
      title={A Systematic Survey of Prompt Engineering in Large Language Models: Techniques and Applications}, 
      author={Pranab Sahoo and Ayush Kumar Singh and Sriparna Saha and Vinija Jain and Samrat Mondal and Aman Chadha},
      year={2024},
      eprint={2402.07927},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2402.07927}, 
}

@misc{zhuo2024prosaassessingunderstandingprompt,
      title={ProSA: Assessing and Understanding the Prompt Sensitivity of LLMs}, 
      author={Jingming Zhuo and Songyang Zhang and Xinyu Fang and Haodong Duan and Dahua Lin and Kai Chen},
      year={2024},
      eprint={2410.12405},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.12405}, 
}

@misc{salinas2024butterflyeffectalteringprompts,
      title={The Butterfly Effect of Altering Prompts: How Small Changes and Jailbreaks Affect Large Language Model Performance}, 
      author={Abel Salinas and Fred Morstatter},
      year={2024},
      eprint={2401.03729},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2401.03729}, 
}

@misc{openai_guide,
author = {OpenAI},
  title = {Prompt Generation},
  howpublished = {\url{https://platform.openai.com/docs/guides/prompt-generation/}},
  year = {2024},
}
@misc{google2024Promptingguide101,
  author = {Google},
  title = {Prompting Guide 101},
  howpublished = {\url{https://workspace.google.com/resources/ai/writing-effective-prompts/}},
  year = {2024},
}


@misc{jiang2023mistral7b,
      title={Mistral 7B}, 
      author={Albert Q. Jiang and Alexandre Sablayrolles and Arthur Mensch and Chris Bamford and Devendra Singh Chaplot and Diego de las Casas and Florian Bressand and Gianna Lengyel and Guillaume Lample and Lucile Saulnier and Lélio Renard Lavaud and Marie-Anne Lachaux and Pierre Stock and Teven Le Scao and Thibaut Lavril and Thomas Wang and Timothée Lacroix and William El Sayed},
      year={2023},
      eprint={2310.06825},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.06825}, 
}


@misc{grattafiori2024llama3herdmodels,
      title={The Llama 3 Herd of Models}, 
      author={Meta},
      year={2024},
      eprint={2407.21783},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2407.21783}, 
}

@misc{llama3,
	title={Introducing Meta Llama3: The most capable openly available LLM to date}, 
	author={Meta},
	year={2024},
 url={https://ai.meta.com/blog/meta-llama-3/}
}

@article{gsm8k,
  title={Training verifiers to solve math word problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and others},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}

@article{math,
	title={Measuring mathematical problem solving with the math dataset},
	author={Hendrycks, Dan and Burns, Collin and Kadavath, Saurav and Arora, Akul and Basart, Steven and Tang, Eric and Song, Dawn and Steinhardt, Jacob},
	journal={arXiv preprint arXiv:2103.03874},
	year={2021}
}
@article{lightman2023let,
  title={Let's verify step by step},
  author={Lightman, Hunter and Kosaraju, Vineet and Burda, Yura and Edwards, Harri and Baker, Bowen and Lee, Teddy and Leike, Jan and Schulman, John and Sutskever, Ilya and Cobbe, Karl},
  journal={arXiv preprint arXiv:2305.20050},
  year={2023}
}
@misc{mathshepherd,
	title={Math-Shepherd: Verify and Reinforce LLMs Step-by-step without Human Annotations}, 
	author={Peiyi Wang and Lei Li and Zhihong Shao and R. X. Xu and Damai Dai and Yifei Li and Deli Chen and Y. Wu and Zhifang Sui},
	year={2024},
	eprint={2312.08935},
	archivePrefix={arXiv},
	primaryClass={cs.AI},
	url={https://arxiv.org/abs/2312.08935}, 
}

@misc{clark2018thinksolvedquestionanswering,
      title={Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge}, 
      author={Peter Clark and Isaac Cowhey and Oren Etzioni and Tushar Khot and Ashish Sabharwal and Carissa Schoenick and Oyvind Tafjord},
      year={2018},
      eprint={1803.05457},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/1803.05457}, 
}
@article{srivastava2023beyond,
  title={Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models},
  author={BIG-bench authors},
  journal={Transactions on Machine Learning Research},
  issn={2835-8856},
  year={2023},
  url={https://openreview.net/forum?id=uyTL5Bvosj},
  note={}
}

@misc{voronov2024mindformatconsistentevaluation,
      title={Mind Your Format: Towards Consistent Evaluation of In-Context Learning Improvements}, 
      author={Anton Voronov and Lena Wolf and Max Ryabinin},
      year={2024},
      eprint={2401.06766},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2401.06766}, 
}

@misc{lu2024aiscientistfullyautomated,
      title={The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery}, 
      author={Chris Lu and Cong Lu and Robert Tjarko Lange and Jakob Foerster and Jeff Clune and David Ha},
      year={2024},
      eprint={2408.06292},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2408.06292}, 
}

@misc{hu2024automateddesignagenticsystems,
      title={Automated Design of Agentic Systems}, 
      author={Shengran Hu and Cong Lu and Jeff Clune},
      year={2024},
      eprint={2408.08435},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2408.08435}, 
}

@inproceedings{voronov-etal-2024-mind,
    title = "Mind Your Format: Towards Consistent Evaluation of In-Context Learning Improvements",
    author = "Voronov, Anton  and
      Wolf, Lena  and
      Ryabinin, Max",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2024",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-acl.375/",
    doi = "10.18653/v1/2024.findings-acl.375",
    pages = "6287--6310"
}