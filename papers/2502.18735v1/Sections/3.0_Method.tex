\section{Preliminaries}

\begin{figure}[!t]
\centering
\includegraphics[width=1\columnwidth]{Figures/method_figure2.png}
\caption{A detailed summary of QueryAdapter, the proposed framework for responding to natural language queries with an adapted VLM. The method is split into four steps; object captioning and storage, training data selection, adaptation and object retrieval.
% The first input to the system is a task command from the user (for example, water the plant for me). An LLM is then used to decompose the request into a set of target classes required to complete the task. These classes are passed to our TaskAdapter module, which leverages unlabelled objects from previous deployments to quickly optimise the pre-trained VLM for detecting the target objects. The robot then uses the adapted VLM in its online perception system to retrieve objects from the current scene that are relevant to the task. Upon execution of the task, the stored objects can be updated with the object segments from the current scene and the process repeated when a new task arises.
% More specifically, stored objects from previous deployments are used in a novel self-training approach to optimise a light-weight adapter network. 
}
\vspace{-1.5em}
\label{method_figure}
\end{figure}

\subsection{Problem Formulation}
\label{prob_form}
We consider the scenario where a robot collects from the current scene $j$ a sequence of posed RGB-D images. The following formulation can also be applied to a single posed image without losing generality. As per ConceptGraphs \cite{conceptgraphs}, pixel segmentation masks $m_{x}$ are firstly extracted for each image. Image crops $c_{x}$ for each segment are then passed to an image encoder $g_{I}\left(\cdot \right)$ to produce an open-vocabulary feature $I_{x}$. This image encoder has a corresponding text encoder $g_{T}\left(\cdot \right)$ that can produce text features $T_{q}$ in the same embedding space as $I_{x}$. Depth, pose and the camera intrinsics are then used to project each pixel mask into the world frame, generating a point cloud $p_{x}$. The result of this process is that for the current scene, we obtain a set of $X$ object segments each defined by a segmentation mask $m_{x}$, image crop $c_{x}$, open-vocabulary image feature $I_{x}$ and point cloud $p_{x}$:
\begin{equation}
    O_{j} = \left\{\left( m_{x}, c_{x}, I_{x}, p_{x} \right)\right\}^{X}_{x=1}
\end{equation}
% \textit{Object Retrieval from Natural Language: }
Given a natural language query $Q$, the robot must retrieve a relevant object from the set $O_{j}$. The retrieved object can then be localised within the scene using the pre-computed point cloud $p_{x}$. We further assess a more complex version of this task where the robot is given a natural language task description $Q_{t}$ that it must complete in the environment. In response to such a query, the robot must return a set of objects from $O_{j}$ that are required to complete the task.

\subsection{Object Retrieval with Using Cosine Similarity}
\label{sec:retrieval}
To retrieve objects related to a natural language query, the query $Q$ can be passed to the text encoder $g_{T}\left(\cdot \right)$ to produce a text feature $T_q$:
\begin{equation}
    T_q = g_{T}\left(Q \right)
    \label{eq:text_encoder}
\end{equation}
The similarity $s_x$ between an object feature $I_x$ and the text feature $T_q$ can then be calculated as:
\begin{equation}
    s_x = \mathbb{S}(I_x, T_q)
    \label{eq:cosine_sim}
\end{equation}
where $\mathbb{S}(\cdot, \cdot)$  denotes cosine similarity. The object segment $x$ with the highest similarity $s_x$ can then be returned in response to the query. Alternatively, the top $k$ segments can be returned as required.

% \subsection{Object Retrieval with Using LLMs}
% Other works use an LLM but 1) merging segments to create distinct instances 2) captioning each instance using a MLLM and 3) using the MLLM

\section{QueryAdapter Method}
\label{sec:adaptation}
Next, we define our QueryAdapter framework for adapting the pre-trained VLM in response to a natural language query $Q$. We separate this into four steps that are described in the following sections and summarised in Figure \ref{method_figure}.

\subsection{Object Captioning and Storage}
Our proposed robotic vision system relies on a set of previously observed objects to perform adaptation. Furthermore, captions are required for these objects to produce negative labels for improving adaptation in the presence of open-query objects. Upon completion of deployment in scene $j$, a caption $\hat{c}_{x}$ is produced for each object segment in $O_{j}$ by passing the extracted image crops to a MLLM.
\begin{equation}
    \hat{c}_{x} = MLLM\left( c_{x}\right)
\end{equation}
These captions are added to the set of object segments, and the image crops removed for efficient storage. Furthermore, we add the index for the current scene. The set of object segments in the scene thus becomes:
\begin{equation}
    O_{j} = \left\{\left(j, m_{x}, \hat{c}_{x}, I_{x}, p_{x} \right)\right\}^{X}_{x=1}
\end{equation}
Note that the use of a MLLM in this process is computationally expensive, which is why we perform this operation offline, after deployment. Lastly, the updated set of objects $O_{j}$ are added to the current set of stored object $S_{j}$. This update rule is defined as:
\begin{equation}
    S_{j+1} = S_{j}+O_{j}
\end{equation}
In our experiments, we do not iteratively deploy the robot across many scenes to generate the stored objects. Instead, we produce $S_{j}$ using a predefined set of $j$ scenes.

\begin{figure*}[t!]
\quad
    \begin{subfigure}[t]{0.3\textwidth}
    \centering
    \includegraphics[trim={0.3cm 0 -0.5cm 0},clip,width=6cm]{Figures/task_queries_bar_charts_2.png}
    \caption{Comparison of QueryAdapter with methods based on 3DSGs for task-oriented object retrieval.}
    \end{subfigure}
\quad
    \begin{subfigure}[t]{0.3\textwidth}
    \centering
    \includegraphics[trim={0.3cm 0 -0.5cm 0},clip,width=6cm]{Figures/adapter_bar_charts_2.png}
    \caption{Comparison of QueryAdapter with unsupervised VLM adapters using small sets of target classes.}
    \end{subfigure}
\quad
    \begin{subfigure}[t]{0.3\textwidth}
    \centering
    \includegraphics[trim={0.3cm 0 -0.5cm 0},clip,width=6cm]{Figures/ablation_bar_charts_2.png}
    \caption{Ablation of QueryAdatpter using small sets of target classes.}
    \end{subfigure}
\caption{Comparison of QueryAdapter with state-of-the-art unsupervised VLM adapters and 3DSG methods.}
\label{comparison_results}
\vspace{-1.5em}
\end{figure*}

\subsection{Training Data Selection}
\label{sec:training_data}
The next step aims to use the stored objects $S_{j}$ to create training data for adapting the VLM to the query $Q$. Firstly, a LLM is used to decompose the query into a set of target classes $\left\{C_{t} \right\}_{t=1}^{T}$ required to fulfil the request:
\begin{equation}
    \left\{C_{t} \right\}_{t=1}^{T} = LLM\left( Q\right)
\end{equation}
% Alternatively, the target classes $C_{t}$ could be directly defined by the user. However, our method demonstrates that more complex adaptation behaviour is possible using the LLM.   
Then, we use the caption $\hat{c}_{x}$ for each object in $S_{j}$ to produce a set of negative classes $\left\{C_{n} \right\}_{n=1}^{N}$.
The nouns are extracted from each caption, and the most common $N$ nouns used to define the set of negative labels. 
To avoid overlap between the negative and target classes, an LLM is used to remove negative classes that have a direct synonym in the target classes. 
The concatenation of the resulting negative and target classes defines the total set of classes $\left\{C_{a} \right\}_{a=1}^{A}$ used to adapt the model:
\begin{equation}
    \left\{C_{a} \right\}_{a=1}^{A} = \left\{C_{t} \right\}_{t=1}^{T} + \left\{C_{n} \right\}_{n=1}^{N}
\end{equation}
Lastly, we select a subset of the stored objects as samples for performing adaptation. This is done to reduce training time and focus adaptation towards the target classes.
For each scene $j$, the top $k$ most similar object segments are retrieved for each target class as described in Section \ref{sec:retrieval}.
% We do this per scene to ensure that the same process can be applied regardless of the size of the stored objects. 
The retrieved object segments, which are analogous to pseudo-labels, are then added to the set of filtered objects to be used for adaptation:
\begin{equation}
    F_{j} = topk\left(S_{j}, \left\{C_{t} \right\}_{t=1}^{T} \right), F_{j} \subset S_{j}
\end{equation}

\subsection{Adaptation}
Next, we use the filtered set of object segments $F_{j}$ and the adaptation classes $\left\{C_{a} \right\}_{a=1}^{A}$ to adapt the VLM. Following CoOp \cite{coop}, we freeze the text and image encoders and instead optimise the text prompts used to perform classification. Specifically, we define a set of $m$ learnable word vectors $\left\{\left[V_{i} \right]\right\}_{i=1}^{m}$ to generate a prompt of the form ``[V$_1$], [V$_2$], \dots, [V$_m$], [CLASS]'' for each adaptation class. As per Eq. (\ref{eq:text_encoder}), the learnable prompts for each adaptation class can be passed to the text encoder to generate corresponding text features $\left\{T_{a} \right\}_{a=1}^{A}$. The probability that an object segment with embedding $I_{x}$ belongs to class $a$ can then be defined using the softmax operation:
\begin{equation} 
\label{eq:clip}
p_a(x) = \frac{\exp(\mathbb{S}(I_x, T_a)/ \tau)} {\sum_{i=1}^{A} \exp(\mathbb{S}(I_x, T_i)/\tau)},
\end{equation}
where $\mathbb{S}(\cdot, \cdot)$ denotes the cosine similarity and $\tau$ is the temperature parameter. If using labelled data to perform adaptation, a standard image classification loss could in turn be applied to optimise $\left\{\left[V_{i} \right]\right\}_{i=1}^{m}$. However, to leverage the unlabelled object segments in $F_{j}$, we implement the unsupervised loss proposed by UEO \cite{ueo}. This loss aims to minimise entropy for samples with a confident classification while maximising it for uncertain samples. To achieve this, the maximum softmax probability score returned via Eq. (\ref{eq:clip}) is used as an estimate of confidence, denoted $w\left(x\right)$. The following loss is then used to achieve simultaneous entropy minimisation and maximisation:
\begin{equation}
\mathcal{L} = \sum_{x \in \mathcal{B}_t} \widetilde{w}(x) \mathcal{H}(p(x)) - \mathcal{H}(\bar{p}),\; 
\label{eq:went2}
\end{equation}
where $\mathcal{B}_t$ is a training mini-batch sampled from $F_{j}$, $\widetilde{w}(x)$ is the normalised value of $w(x)$ across the mini-batch and $\mathcal{H}(\cdot)$ is the Shannon entropy of a probability distribution. Furthermore, $\bar{p}$ is the inversely weighted average of predictions for each sample within the mini-batch:
\begin{equation}
\bar{p} = \sum_{x \in \mathcal{B}_t} \frac{p(x)}{\widetilde{w}(x)}
\end{equation}
We find that when relatively few classes $\left\{C_{a} \right\}_{a=1}^{A}$ are defined for adaptation, the estimation of confidence $w(x)$ and entropy $\mathcal{H}(p(x))$ are much less reliable. By utilising additional negative classes $\left\{C_{n} \right\}_{n=1}^{N}$ during adaptation, we aim to obtain a better prediction for these values. Furthermore, by selecting only those samples similar to the target classes $\left\{C_{t} \right\}_{t=1}^{T}$, we aim to reduce the complexity of the open-query problem, allowing the entropy maximisation term of Eq. (\ref{eq:went2}) to dominate.

\subsection{Object Retrieval}
Once adaptation is performed, which needs to occur quickly, the optimized prompt vectors $\left\{\left[V_{i} \right]\right\}_{i=1}^{m}$ can be used to retrieve object segments from the scene. We prepend these vectors to the target classes $\left\{C_{t} \right\}_{t=1}^{T}$ to generate the optimised prompts $\left\{P_{t} \right\}_{t=1}^{T}$. We can then use these prompts to retrieve object segments relevant to the natural language query, as described in Section \ref{sec:retrieval}.

% Passing these prompts through the text encoder as per  Eq. (\ref{eq:text_encoder}), we get text features $\left\{T_{t} \right\}_{t=1}^{T}$. Finally, to retrieve object segments relevant to the task description, we simply apply Eq. (\ref{eq:cosine_sim}) for each text feature $T_{t}$.