\section{RELATED WORK}
\subsection{Language-Image Pre-training}
The availability of captioned images has allowed the joint training of image and text encoders using natural language supervision \cite{clip}. The seminal work in this space is Contrastive Language-Image Pretraining (CLIP), which uses a contrastive loss to produce similar embeddings for image-text pairs \cite{clip}. The resulting VLM can be used to perform image classification with open-vocabulary classes \cite{clip}, and exhibits superior resistance to domain shift compared with supervised pre-training methods \cite{clip, grounding_with_text}. 
% A variety of methods extend have extended this capability to perform dense prediction tasks such as object detection \cite{glip, owl, detic} and semantic segmentation \cite{lseg, dense_clip, ZegCLIP, segment_anything}. 
However, a domain shift exists between the large-scale, internet data used to train these VLMs and the raw image streams collected by a robot \cite{seal} (Figure \ref{domain_shift}). Consequently, a pre-trained VLM is unlikely to perform optimally in robotic deployment environments.

\subsection{Object Retrieval from Natural Language}
A range of robotic vision systems have been proposed for responding to natural language queries in a real-world scenes \cite{clio, conceptgraphs, hovsg, bare, ovoslam}. These systems are distinct from traditional object detection and mapping systems, as they do not aim to classify each object at the time of observation \cite{kimera, multi-tsdfs}. Instead, they look to maintain a generic representation of the environment that can be utilised later to respond to natural language queries \cite{vlmap, lerf}. To solve this problem, foundational VLMs are used to produce objects segmentations \cite{segment_anything} and open-vocabulary features \cite{clip, detic, glip, owl, lseg} from a stream of posed RGB-D images. While these segments can be directly used to respond to queries \cite{bare}, they are often fused across frames using projective geometry and feature similarity to produce distinct object instances \cite{conceptgraphs, hovsg, clio, ovoslam}. In turn, relationships between instances can be predicted to generate a complete 3D Scene Graph (3DSG) \cite{conceptgraphs, hovsg, ovoslam}. 

\begin{figure}[!t]
\centering
\includegraphics[width=1\columnwidth]{Figures/hook_figure_4.png}
\caption{Our proposed framework for rapidly adapting a pre-trained VLM to detect objects relevant to a natural language query. Given a new query, we use an LLM to generate a set of ``target classes'' required to fulfill the request. Unlabelled data collected by the robot in previous deployments is then used to align VLM features with these target classes. As a final step, the adapted model is used to detect the target classes in the current scene.
}
\vspace{-1.5em}
\label{simple_method}
\end{figure}

To respond to natural language queries, the cosine similarity between text and object embeddings can be used to retrieve relevant objects. Alternatively, Multi-modal Large-Language Models (MLLMs) \cite{llava, llava3d} can generate object captions and select those that are relevant to a particular query \cite{conceptgraphs}. What is common across these systems is that they rely on pre-trained vision-language models that have not been adapted for use in robotic deployment environments. In response, we propose an approach to adapt a VLM to a particular natural language query, improving the retrieval of relevant objects.

\subsection{Adaptive Embodied Object Detection}
Several works aim to close the domain gap between the large-scale, internet data used to train a VLM and the raw image streams collected by a robot \cite{seal}. Embodied Active Learning (EAL) methods use the spatial-temporal consistency of a scene as a learning signal to perform adaptation \cite{seal, eal_semseg, self_improving, move_to_see}. However, existing EAL methods focus on using closed-vocabulary object detectors. Instead, we aim to use previously observed objects to quickly adapt a VLM to open-vocabulary concepts. Attempts have also been made to optimise the fusion of CLIP features across different views of the scene \cite{ovoslam, bare, eod}. Unlike these systems, we rely solely on unlabelled data collected by the robot to perform adaptation, and do not require the definition of a closed-set of classes. 

% \begin{figure}[h]
% \centering
% \includegraphics[width=0.56\textwidth]{Figures/Chapter-2/vlm_domain_shift.png}
% \caption[Domain Shift for VLMs]{\label{fig:vlm_domain_shift} An example of the captioned image data from the LAION-5B dataset (top). Such datasets contain images that are not representative of robotic deployment, where diverse tasks must be performed using the data stream collected by a robot. Image adapted from \cite{laion5b}.}
% \end{figure}

\subsection{Parameter-efficient Transfer Learning of VLMs}
Recent work has focussed on adapting a VLM to particular downstream tasks without altering the pre-trained image and text encoders \cite{clip_adapter, coop, cocoop, multi_modal_adapter, vpt, tip_adapter}. This allows adaptation to be performed with limited data, without damaging the representations learnt during large-scale pre-training. The most basic approach, fitting a linear probe on top of the image encoder, was explored in the original work on CLIP \cite{clip}. Prompt tuning \cite{coop} formalised the task of parameter-efficient adaptation of VLMs and proposed learning a set of context embeddings that can be prepended to the tokenised class names to enhance image classification performance. Various other adapters have since been proposed to augment the text features \cite{coop, cocoop, tip_adapter} and image features \cite{clip_adapter, vpt} using limited labelled data. 

Motivated by the Unsupervised Domain Adaptation (UDA) literature \cite{domain_theory, uda_survey, udaod1, udaod2}, attempts have been made to adapt CLIP using only unlabelled data \cite{ueo, upl}. Unsupervised Prompt Learning (UPL) \cite{upl} selects the top $k$ confident samples for each class in the unlabelled data, and uses them as pseudo-labels to fine-tune the system via prompt tuning \cite{coop}. Universal Entropy Optimisation (UEO) \cite{ueo} extends the standard unsupervised learning task to consider the existence of OOD images in the training data. They propose a learning objective that minimises entropy for confident samples while maximising it for low-confidence samples, and use this to perform prompt tuning. However, both UEO and UPL require the definition of a closed-set of classes, which is undesirable when the robot is expected to respond to diverse natural language queries. Furthermore, we find that these approaches fail when applied to a robotic data stream with many open-query objects.  

% While promising, we find that this method performs poorly for complex natural language queries and in the presence of many OOD samples. Universal Entropy Optimisation (UEO) extends the standard unsupervised learning task to consider the existance of OOD images in the training data. They propose a learning objective that minimises entropy for confident samples while maximising it for low-confidence samples, and use this to perform prompt tuning. However, as it is designed for image classification, entropy minimisation assumes that each object has only one correct label. Furthermore, we find that this approach degrades significantly when facing a significant number of OOD samples. In response to these limitations, we propose a novel pseudo-labelling approach designed specifically to deal with these challenges. Similar to UPL, we select the top-k confident objects for each core concept as candidate pseudo-labels. We then pass these candidate objects to a MLLM to produce a caption and decide if the candidate pseudo-label is likely to be correct. This way, we significantly reduce the number of OOD samples in our training data. We then use these pseudo-labels to train an adapter to perform object retrieval, significantly improving performance on the core concepts without compromising zero-shot performance.

\subsection{OOD and Open-set Detection with VLMs}
VLMs are trained to classify open-vocabulary concepts, making them robust to open-set conditions. However, the act of defining a query set introduces closed-set assumptions, in turn making VLMs vulnerable to open-set \cite{open_vlm, bare} or OOD \cite{ueo} objects. Recent work attempts to overcome this by using random words and embeddings as negative classes \cite{open_vlm}, and by adapting CLIP to better express prediction uncertainty \cite{probvlm}. 
In this work, we study a specialised version of this problem where objects unrelated to a specific query (termed open-query objects) must be rejected during adaptation.
% In turn, we extend existing work in unsupervised VLM adaptation \cite{ueo} to deal with such objects, which we term open-query objects. 
In response, we generate captions for all objects in the training data and use the most common class names as negative classes. As a further step to filter our open-query objects and improve learning efficiency, we only the use the top $k$ previously observed objects for adaptation.
