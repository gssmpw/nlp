\section{RESULTS}
\subsection{Experimental Settings}

% \begin{figure}[!t]
% \centering
% \includegraphics[width=1\columnwidth]{Figures/task_queries_bar_charts.png}
% \caption{
% Comparison of the proposed robotic vision paradigm with ConceptGraphs for task-oriented object retrieval in the Scannet++ dataset.
% }
% \label{task_oriented}
% \end{figure}

\textbf{Dataset Preparation:} Following similar work \cite{bare, ovoslam}, we utilise scenes from the Scannet++ dataset \cite{scannet++} for both adaptation and evaluation. The standard split that assigns 230 scenes to training and 50 scenes to testing is used. We pre-process the raw data for each scene to produce the set of object segments $O_{j}$ as described in Section~\ref{prob_form}. We use SegmentAnything \cite{segment_anything} to produce segmentation masks for each image and CLIP \cite{clip} to generate the open-vocabulary embeddings. The number of scenes $j$ used for adaptation is by default defined as 70, however we test the sensitivity of this variable in later experiments.

% \textbf{Evaluation of Task-oriented Adaptation: }We utilise two distinct experimental approaches to assess task-oriented adaptation using Scannet++ \cite{scannet++}. Firstly, we define a set of natural language task descriptions and use the top-100 most common classes to label the relevant objects required to complete the task. 
% % In response to a task description, we use the segments from $n$ training scenes to apply TaskAdapter as per Section III. 
% The robot can then be evaluated on its ability to retrieve objects that are in the labelled set of relevant objects. This experiment results in 158 task queries relating to 329 relevant objects for evaluating our overall robotic vision pipeline. These annotated tasks will be made available on our github repository.

\textbf{Evaluation of Task-oriented Object Retrieval:}
To assess object retrieval in response to complex natural language queries, we define a set of task descriptions that can be completed in the Scannet++ test scenes \cite{scannet++}. In turn, the ``relevant classes'' required to complete each task are defined using the top-100 common classes in the dataset. In scenes where all relevant classes are present, the robot is evaluated on its ability to retrieve these objects in response to the query. This experiment results in 158 queries relating to 329 relevant objects for evaluating QueryAdapter. \footnote{The annotated queries and code for performing evaluation will be made available on our github repository on acceptance.\label{myfootnote}}

% Given a task description, we use the object segments from $j$ training scenes to apply TaskAdapter as per Section III. We then perform object retrieval using the adapted model, making sure to only evaluate a task in scenes where a correct solution is possible. 
To evaluate segments retrieved by the robot, we use the ground truth point cloud to assign a class label to each segment. We firstly assign each point in the object segment the label of the closest ground-truth point, before assigning the most common label across all points to the segment. 
% In addition, if more than 5 objects are retrieved in response to a query, we randomly sample 5 to preform evaluation.
Using these ground truth labels, we can calculate the proportion of relevant classes that were recalled in response to the query. We report the average of this metric across all tasks as the Average Task Recall (ATR).

\textbf{Optimisation Procedure:}
To avoid biasing the method towards the queries used for evaluation, we optimise QueryAdapter using a different experimental approach. To simulate adapting to natural language queries, we randomly generate small sets of target classes that the robot must adapt to. We use the most common object classes in Scannet++ to define eight sets of six target classes. These target classes can then be used to perform adaptation as per Section~\ref{sec:adaptation}, with the query decomposition step skipped. 
% Following the example of existing work, we also generate a second set of pseudo-tasks using affordance queries \cite{conceptgraphs}. These are generated by asking an LLM to define the most common use case for each class. 
We report the recall@1 averaged across all target classes, which comprises a significant number of object queries ($>$1500). This allows us to optimise QueryAdapter without overfitting to particular types of natural language query. Furthermore, we use this dataset to compare with existing unsupervised VLM adapters.

% \begin{figure}[!t]
% \centering
% \includegraphics[width=1\columnwidth]{Figures/adapter_bar_charts.png}
% \caption{
% Comparison of TaskAdapter with unsupervised VLM adapters for class and affordance queries in the Scannet++ dataset.
% }
% \label{adapters}
% \end{figure}

% \textbf{Evaluation Procedure: }
% Given a task query or pseudo-task, we use the object segments from $j$ training scenes to apply TaskAdapter as per Section III. We then perform object retrieval using the adapted model, making sure to only evaluate a query in scenes where a correct solution is possible. To evaluate each segment retrieved by the robot, we use the ground truth point cloud to assign a class label. We firstly assign each point in the segment point cloud the label of the closest ground-truth point, before assigning the most common label across all points to the segment. 

% For task queries, a retrieved object that receives a label relevant to the task is considered a true positive, with the inverse deemed a false positive. Failure to retrieve a segment for a particular relevant object is deemed a false negative. The precision@1 and recall@1 of the robot is thus calculated across all tasks. For pseudo-tasks containing class or affordance queries, we evaluate each query separately and report the recall@1 across all pseudo-tasks.

% \textit{Evaluation of Class and Affordance Queries: }We also use the Scannet++ dataset to evaluate TaskAdapter on object class and affordance queries. The class queries consist of top-50 class names, with corresponding affordance queries generated by asking an LLM to define the the most common use case for each class. For both class and affordance queries, we create 8 distinct \textit{pseudo-tasks} that each contain 6 queries. We can then apply TaskAdapter to these pseudo-tasks as per the task-oriented queries. That is, the segments from $n$ training scenes are used to optimise an adapter for each pseudo-task. Then, we assess object retrieval for each query in the pseudo-task in all evaluation scenes where the query object is present, and report the recall@1 across each pseudo-task. This process allows us to evaluate TaskAdapter on a significant number of natural language queries ($>$1500), exceeding that studied in similar work \cite{conceptgraphs}.

\textbf{Baseline methods: }We firstly compare QueryAdapter with other unsupervised VLM adapters from the literature. UPL \cite{upl} uses top $k$ pseudo-labelling and cross-entropy loss to perform prompt learning. UEO \cite{ueo} uses the same self-training loss as QueryAdapter, but without negative labels or top $k$ filtering to deal with the many OOD objects in the unlabelled data stream. We also compare with an approach to object retrieval from natural language based on 3DSGs. We implement ConceptGraphs \cite{conceptgraphs} and perform object retrieval using both the object captions and cosine similarity.

\textbf{Implementation details: }For all experiments, we use the \textit{ViT-H-14} CLIP model from Openclip \cite{clip} as the pre-trained VLM. For the prompt learner \cite{coop}, we use four context vectors initialised with the prompt ``a photo of a'' and the temperature parameter $\tau$ is set to 0.01. We train the adapters with the Adam optimiser for 50 epochs on a single A100 GPU, with a batch size of 256 and learning rate of 0.0005. For QueryAdapter, we use the optimal setting of $k=8$ and 100 negative queries as standard. For UPL, we also find that $k=8$ is optimal. The number of negative labels is set as $N=100$. We use \textit{Llama-3-8B-Instruc} model as the LLM and \textit{llava-v1.6-vicuna-7b} as the captioning system for QueryAdapter and ConceptGraphs.\footnote{Examples of the prompts for both these models will be made available on our github repository on acceptance.\label{myfootnote}}

% \begin{figure}[!t]
% \centering
% \includegraphics[width=1\columnwidth]{Figures/ablation_bar_charts.png}
% \caption{
% Ablation of TaskAdatpter for class and affordance queries in the Scannet++ dataset.
% }
% \label{ablation}
% \end{figure}

\begin{figure*}[t!]
\vspace{-1em}
\quad
    \begin{subfigure}[t]{0.3\textwidth}
    \centering
    \includegraphics[trim={0.3cm 0.5cm -0.5cm 0},clip,width=6cm]{Figures/taskadapter_topk_with_time_2.png}
    \caption{Impact of top $k$ setting on QueryAdapter performance and training time.}
    \end{subfigure}
\quad
    \begin{subfigure}[t]{0.3\textwidth}
    \centering
    \includegraphics[trim={0.3cm 0.5cm -0.5cm 0},clip,width=6cm]{Figures/negative_queries_with_time_2.png}
    \caption{Impact of the number of negative classes on QueryAdapter performance and training time.}
    \end{subfigure}
\quad
    \begin{subfigure}[t]{0.3\textwidth}
    \centering
    \includegraphics[trim={0.3cm 0.5cm -0.5cm 0},clip,width=6cm]{Figures/training_data_size_with_time_2.png}
    \caption{Impact of the number of scenes used for adaptation on QueryAdapter performance and training time.}
    \end{subfigure}
\caption{Impact of key parameters on QueryAdapter performance and training time using the small sets of target classes. The purple solid lines show the performance of the adapted model on the target classes. 
The purple dotted lines refers to performance of the pre-trained system on the target classes. 
The orange solid line shows the time taken to perform adaptation.}
\label{paramter_sensitivity}
\vspace{-1.5em}
\end{figure*}

\vspace{-0.3em}
\subsection{Task-oriented Object Retrieval}
We firstly assess the ability of our overall framework to respond to complex, task-oriented queries (Figure \ref{comparison_results}a). We compare our pipeline with ConceptGraphs \cite{conceptgraphs}, an approach for object retrieval from natural language based on 3DSGs. This approach incrementally merges object segments and features to produce a set of 3D objects. As in our approach, objects can be retrieved based on their cosine similarity with the query (CG Cosine). For more abstract queries, a caption is produced for each object and an LLM used to retrieve relevant objects (CG Captions). Owing to the inconsistent captions produced for each object, CG Captions performs poorly when responding to our task-based queries (Figure \ref{comparison_results}a). In particular, when relevant objects are missing from the 3DSG, the LLM is prone to producing an obscure plan in an attempt to respond to the query using the objects present. Using the LLM to first produce a set of target classes required to fulfil the query as per our approach avoids such confabulation. Using these target classes to query the 3DSG via cosine similarity thus produces a significant increase in average task recall (Figure \ref{comparison_results}a). However, this approach remains worse then querying the raw segments directly. This indicates that when merging object segments and features to produce a compact 3DSG, valuable information may be lost. Lastly, we see that QueryAdapter produces a further 6.7\% increase in average task recall on the task-oriented queries. This amounts to a 17.6\% increase in recall relative to using CG Captions. This result highlights the value of both QueryAdapter and our overall robotic vision paradigm in responding to complex natural language queries.

\vspace{-0.3em}
\subsection{Comparison of Unsupervised VLM Adapters}
We use the small sets of target classes to conduct a detailed comparison of unsupervised VLM adapters for query-oriented adaptation (Figure \ref{comparison_results}b). Due to the large number of open-query objects in the training set, UEO performs very poorly in this setting. UPL performs better, generating minor improvement relative to using the pre-trained model. 
% However, to make noticable difference to object retrieval, an alternative adaptation approach is required. In particular, new strategies are needed to deal with the many OOD objects present in the unlabelled data stream collected by the robot. 
However, QueryAdapter is the strongest approach, producing a 7.9\% improvement in recall@1 relative to the pre-trained model. This emphasises the importance of addressing the many open-query objects present in the raw data stream used for adaptation. 
% In practical terms, this result further shows that QueryAdapter can be used to improve the performance of a pre-trained VLM on a small set of object classes deemed relevant to the robot. 
Furthermore, it highlights that our negative labelling method, top $k$ object selection and the UEO loss are all integral to the effective operation of QueryAdapter. 

% Furthermore, it can be used to drastically improve recall for more abstract affordance queries that were previusly thought challenging for CLIP-based retrieval systems \cite{conceptgraphs}.
% We also find that TaskAdapter is less sensitive than UPL to the value of top-k (Figure \ref{paramter_sensitivity}a-b). We attribute this to the use of negative labels and the UEO loss in addition to top-k pseudo-labelling. 

% \begin{figure}[t!]
% \centering
%     \begin{subfigure}[t]{0.22\textwidth}
%     \centering
%     \includegraphics[trim={0.5cm 0 -0.5cm 0},clip,width=4.5cm]{Figures/upl_topk.png}
%     \caption{Sensitivity of UPL to top-k setting.}
%     \end{subfigure}
% \quad
%     \begin{subfigure}[t]{0.22\textwidth}
%     \centering
%     \includegraphics[trim={0.5cm 0 -0.5cm 0},clip,width=4.5cm]{Figures/taskadapter_topk.png}
%     \caption{Sensitivity of TaskAdapter to top-k setting.}
%     \end{subfigure}
% \quad
%     \begin{subfigure}[t]{0.22\textwidth}
%     \centering
%     \includegraphics[trim={0.5cm 0 -0.5cm 0},clip,width=4.5cm]{Figures/negative_queries.png}
%     \caption{Impact of the number of negative labels on TaskAdapter performance.}
%     \end{subfigure}
% \quad
%     \begin{subfigure}[t]{0.22\textwidth}
%     \centering
%     \includegraphics[trim={0.5cm 0 -0.5cm 0},clip,width=4.5cm]{Figures/training_data_size.png}
%     \caption{Impact of the number of training scenes on TaskAdapter performance.}
%     \end{subfigure}
% \caption{Sensitivity of TaskAdapter and UPL to key parameters. The purple and green dotted lines refer to performance of the pre-trained system on class and affordance queries.}
% \label{paramter_sensitivity}
% \end{figure}

\vspace{-0.3em}
\subsection{Ablation Study}
We also use the small sets of target classes to perform an ablation study of the proposed QueryAdapter (Figure \ref{comparison_results}c). We firstly assess the impact of running QueryAdapter without negative labels, which we term top $k$ only. 
% This approach performs well for the affordance queries, but actually harms performance for class queries relative to using the pre-trained model. 
This approach harms performance of the pre-trained model, emphasising the need for additional strategies to deal with open-query objects. In turn, the addition of our negative labelling approach is shown to improve recall@1 by 10.1\%. 
We also assess the inverse of this, where QueryAdapter is run with the negative labels but without top $k$ object selection. 
% Notably, this approach performs better for affordance queries than TaskAdapter. 
This approach is slightly worse than QueryAdapter, demonstrating that top $k$ object selection provides a small performance benefit in addition to improving efficiency. 
Lastly, we assess the impact of using random words as negative labels \cite{open_vlm}. This approach leads to a reduction of 5.2\% on recall@1 relative to QueryAdapter, further emphasising the value of using object captions as negative labels. 
% These results our negative labelling method, top $k$ object selection and the UEO loss are all integral to the effective operation of QueryAdapter. 

\vspace{-0.3em}
\subsection{Practical Considerations}
To minimise downtime of the robot, adaptation of the VLM to the current query needs to be performed as quickly as possible. Ultimately, QueryAdapter can produce an adapted VLM in a few minutes, which we argue is sufficient for many applications (Figure \ref{paramter_sensitivity}). However, there are several parameters that impact training time, such as top $k$, number of training scenes $j$ and number of negative labels $N$. A larger value for $k$ leads to more training samples being used, directly increasing training time (Figure \ref{paramter_sensitivity}a). However, this does not necessarily improve performance, as a larger $k$ introduces segments that are more likely to be OOD \cite{upl}. In practice, top $k=8$ generates optimal performance while maintaining low run-time. Secondly, the number of negative labels increases the time taken to calculate the UEO loss. There is a clear trade-off here, as using more negative labels tends to generate better performance (Figure \ref{paramter_sensitivity}b). We recommend using around 50 to 100 labels, as at this point performance appears to plateau. Lastly, we see that using more scenes for training increases both the training time and performance of QueryAdapter (Figure \ref{paramter_sensitivity}c). 
% However, even with only a few scenes, the adapted model performs better than the pre-trained VLM. 
This highlights the potential for QueryAdapter to be used in a practical continual learning setting, where as more scenes are explored by the robot its response to natural language queries will improve. 

\begin{table}[t]
\centering
\caption{Performance of QueryAdapter in alternative deployment scenarios. Small sets of target classes are used to evaluate performance on affordance-based queries in Scannet++. Additionally, the performance of the adapted VLMs produced for task-oriented queries in the Scannet++ dataset are evaluated on scenes from Ego4D. }
\begin{tabular}{l|l|l}
\hline
Method & Affordance  & Ego4D \\ \hline
QueryAdapter   & \textbf{30.84 (+10.64)} & \textbf{33.06 (+8.11)} \\ \hline

Pre-trained   &  20.19  & 24.95   \\ \hline
\end{tabular}
\vspace{-1.5em}
\label{tab:other}
\end{table}

\vspace{-0.3em}
\subsection{Alternative Deployment Scenarios}
We additionally explore the performance of QueryAdapter in alternative deployment settings (Table \ref{tab:other}). Firstly, we evaluate the potential for our method to improve performance on affordance queries. Such abstract queries are known to be challenging for CLIP-based retrieval systems, motivating the use of MLLMs in some work \cite{conceptgraphs}. To evaluate affordance queries, we perform adaptation using a new set of target classes where the object classes are replaced with common object affordances. These are generated by asking an LLM to define the most common use case for each class.
% Object retrieval is then evaluated for these affordance queries using the pre-trained and adapted model. 
In this setting, the adapted model generates an improvement in recall@1 of 10.6\%. Evidently, adaptation strategies such as QueryAdapter can be used to align CLIP features with more abstract concepts, potentially avoiding the need to use computationally expensive methods such as MLLMs.   

Lastly, we assess the ability of the adapted models to generalise across datasets and application domains. We take the adapted VLMs trained on the Scannet++ dataset and evaluate them on scenes from the Ego4D dataset \cite{ego4d, paco}. This dataset contains footage from wearable cameras showing people completing common manipulation tasks. Our evaluation procedure with this dataset remains the same as when using ScanNet++ to assess task-oriented queries. The only change is that we update the set of relevant classes for each query using the Ego4D labels, and a bounding-box Intersection over Union of 50 is used to associate ground-truth labels to predicted segments. Despite never having seen Ego4D data, the adapted model improves average task recall by 8.1\% in these scenes. This emphasises that QueryAdapter is robust to the visual appearance changes that can occur between datasets. Furthermore, this result highlights the potential for QueryAdapter to improve the execution of common manipulation tasks.

\vspace{-0.5em}
\subsection{Limitations and Future Work}
This work raises several directions for improving how VLMs are adapted for robotic deployment. Firstly, despite requiring only a few minutes to train, there may be opportunities to further improve the efficiency of QueryAdapter. For example, there may be solutions in the Test-Time Training (TTT) literature, which aims to perform adaptation online using a stream of images \cite{improved_tta}. However, how to perform query-oriented adaptation with such approaches remains unexplored. The incremental use of QueryAdapter could also be investigated in more detail. In particular, different adaptation strategies may be optimal in low data scenarios \cite{coop, cocoop} in comparison to when training data is plentiful \cite{udaod2}. Lastly, the robotic vision framework proposed in this paper is yet to be integrated with downstream methods of task execution. This process could have interesting implications for how open-vocabulary robotic vision systems are evaluated. For example, it is unclear how existing open-vocabulary systems would respond if a queried object is not present in the scene. 
