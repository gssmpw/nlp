%!TEX root = ../main.tex

\section{Correlative and Term Sparsity}
\label{sec:general-sparsity}

This section begins with essential notation and preliminaries. We first introduce chordal graphs and efficient algorithms for automatic chordal extension. We then present key definitions and properties of sparse matrices. Next, we discuss two fundamental types of sparsity in polynomial optimization: correlative sparsity and term sparsity. Building on these concepts, we present the sparse moment-SOS hierarchy.

To provide a tutorial-style exposition of the mathematical machinery, we use a contact trajectory optimization problem as the running example.

\textbf{Example 2.1.} (Double Integrator With Soft Wall)

\hy{Need a figure for this simple example}

\begin{example}[Double Integrator with Soft Wall]
	\label{ex:di-soft-wall}
	Denote system's state as $(x, v)$, control input as $u$, two wall's force as $(\lam{1}, \lam{2})$, the dynamics is:
\begin{align}
	\min \quad & \sum_{k = 0} ^ {N - 1} u_k^2 + \x[k+1]^2 + v_{k+1}^2 \\
	\text{s.t.} \quad & \x[k+1] - \x[k] = \dt \cdot v_k \\
	& v_{k+1} - v_k = \frac{\dt}{m} \cdot (u_k + \lam{1}[k] - \lam{2}[k]) \\
	& u_{\max}^2 - u_k^2 \ge 0 \\
	& 0 \le \lam{1}[k] \perp \frac{\lam{1}[k] }{k_1} + d_1 + \x[k] \ge 0 \\
	& 0 \le \lam{2}[k] \perp \frac{\lam{2}[k] }{k_2} + d_2 - \x[k] \ge 0 \\
	& x_0 = x_{init} \quad \text{and} \quad v_0 = v_{init}
\end{align} 
where $m$, $\dt$, $k_1$, $k_2$, $d_1$, $d_2$, $u_{\max}$, $x_{init}$ are constants.
\end{example}

\subsection{Notation and Preliminaries}	

Let $\mvx = (x_1, \dots, x_n)$ be a tuple of variables and $\R[\mvx] = \R[x_1,\dots,x_n]$. A monomial is defined as $\mvx^\mva = x_1^{\alpha_ n1}x_1^{\alpha_2} \cdots x_n^{\alpha_n}$. A polynomial in $\mvx$ can be written as $f(\mvx) = \sum_{\mva \in \bbN^n} f_\mva \mvx^\mva$ with coefficients $f_\mva \in \R$. We denote the set of all polynomials with degree less than or equal to $d$ as $\R_{d}[\mvx]$. 

The support of $f$ is defined by $\text{supp}(f) = \left\{ \mva\in \bbN^n\mid f_\mva \ne 0 \right\}$. The set of all variables contained in $f$ is defined by $\text{var}(f)$.

Given a polynomial $f(\mvx)\in \R[\mvx]$, if there exist polynomials $f_1(\mvx), \dots, f_k(\mvx)$ such that $f(\mvx) = \sum_{i=1}^k f_i^2(\mvx)$, then we say that $f(\mvx)$ is a \emph{sum of squares (SOS)} polynomial. The set of all SOS polynomials is denoted by $\Sigma[\mvx]$. Similarly, the set of all SOS polynomials that its degree is less than or equal to $2d$ is denoted by $\Sigma_{2d}[\mvx]$. 

Assume $\mvx^{\bbN_d^n}$ is the \emph{standard monomial basis}. Then for $f\in \Sigma_{2d}[\mvx]$, there exist a \emph{Gram matrix} $Q$ such that $Q$ is PSD matrix and $f = (\mvx^{\bbN_d^n})^T Q \mvx^{\bbN_d^n}$. 

Define $[m] = \cbrace{1,\dots,m}$. For convenience, we abuse notation in the sequel and denote the \emph{standard monomial basis} $\mvx^{\bbN_d^n}$ by ${[\mvx]}^n_d$. Given an index set $I\subseteq [n]$, let $\mvx(I)= (x_i, i\in I)$ and ${[\mvx(I)]}_d$ denote the standard monomial basis of the subspace spanned by the variables $x_i, i\in I$.


In SPOT, we consider the following generally defined polynomial optimization problem (POP):
\begin{equation}
	\rho^\star = \min_{\mvx\in \R^n}\left\{
        f(\mvx)\mymid 
        \substack{g_1(\mvx)\ge 0, \dots, g_{m_{ineq}}(\mvx)\ge 0,\\
        h_1(\mvx) = 0, \dots, h_{m_{eq}}(\mvx) = 0}
    \right\} 
\end{equation}
where the objective function $f$ and constraints $g_i, h_i$ are all polynomials.





\subsection{Chordal graphs and Chordal Extension}


\hy{Show the graph for Example~\ref{ex:di-soft-wall}}

In this subsection, we introduce chordal graphs, their properties, and algorithms for chordal extension~\cite{Blair1993book-chordal-clique}.

An undirected graph $G(V, E)$ consists of a vertex set $V={v_1, v_2, \dots, v_n}$ and an edge set $E \subseteq {(v_i, v_j) \mid v_i, v_j\in V, v_i\ne v_j }$. Graphs can be represented by adjacency matrices $A \in \mathbb{R}^{n \times n}$, where $a_{ij} = 1$ if $(v_i, v_j) \in E$ and $a_{ij} = 0$ otherwise. The neighbors of a vertex $v_i$ can be identified from the $i$-th row of $A$, and its degree is determined by counting the non-zero entries in that row.

\textbf{Definition 2.1.} A graph $G = (V, E)$ is \emph{chordal} if every cycle of four or more vertices has a chordâ€”an edge connecting two non-adjacent vertices in the cycle. Formally, for any induced cycle $C \subseteq V$ with $|C| \geq 4$, there exists an edge $(v_i, v_j) \in E$ such that $v_i, v_j \in C$ and $(v_i, v_j)$ is not part of the cycle $C$.

A graph $G'(V', E')$ is called a chordal extension of graph $G(V, E)$ if $V' = V$ and $E\subseteq E'$. The objective of chordal extension is to add the minimum number of edges to $G$ to make it chordal. This process plays a crucial role in various applications, particularly in sparse matrix computations.

A clique in a graph $G(V, E)$ is a subset of vertices $C \subseteq V$ where every pair of vertices is connected by an edge. 
A clique is maximal if it is not properly contained within any other clique in $G$. Actually, the maximal cliques of a chordal graph can be enumerated efficiently in linear time interms of the number of noeds and edges~\cite{Delbert1965pjm-matrix-and-graph, Hans2010iac-treewidth-computations, golumbic2004algorithmic}. 
The identification of all maximal cliques in a chordal graph plays a fundamental role in exploiting sparsity patterns.

In SPOT, we implement five approaches to obtain maximal cliques for a general graph. In the simplest cases, users can either manually specify the cliques or use the trivial choice of considering all vertices as a single maximal clique. Additionally, we implement three automatic chordal extension algorithms that identify maximal cliques from the resulting chordal graph.

The first automatic approach constructs disjoint complete subgraphs by connecting all vertices within each connected component. The other two approaches employ greedy algorithms that aim to minimize the number of additional edges required to obtain a chordal graph.

We now present the Minimum Degree algorithm for constructing chordal extensions and identifying maximal cliques. Algorithm 1 implements the chordal extension by iteratively selecting vertices with minimum degree from the remaining graph and adding necessary edges to ensure chordality. 

\begin{algorithm}
	\small
	\caption{MD Chordal Extension}
	\begin{algorithmic}[1]
	\REQUIRE Graph $G(V,E)$ with $n$ vertices
	\ENSURE Chordal graph $G'$, elimination order $\pi$
	\STATE Initialize $G' \leftarrow G$, $H \leftarrow H$, $R \leftarrow V$
	\FOR{$i = 1$ to $n$}
		\STATE Find vertex $v \in R$ with minimum degree in $H$
		\STATE $\pi(v) \leftarrow i$
		\STATE $N \leftarrow \{u \in R : (v,u) \in E(H)\}$ \COMMENT{unprocessed neighbors}
		\FOR{each pair $(u,w) \in N \times N, u \neq w$}
			\IF{$(u,w) \notin E'$}
				\STATE Add edge $(u,w)$ to $G'$ and $H$
				\STATE Remove edges $(v, u)$ in H, $\forall u \in N$
			\ENDIF
		\ENDFOR
		\STATE $R \leftarrow R \setminus \{v\}$
	\ENDFOR
	\RETURN $G'$, $\pi$
	\end{algorithmic}		
\end{algorithm}

\textbf{Remark 2.1.} An alternative approach is to select vertices based on the number of additional edges required to maintain chordality. We implement this Minimum Fill-in (MF) chordal extension algorithm in our code, which aims to minimize the total number of edges added during the chordal extension process.

Once the chordal extension is constructed, the next step is to identify the maximal cliques in the chordal graph $G'(V, E')$. Algorithm 2 identifies these cliques through a two-phase process. 
First, for each vertex $v \in V$, we construct a clique containing $v$ and its neighbors that appear later in the elimination order $\pi$, i.e., ${u \mid \pi(u) > \pi(v)}$. Then, examining these cliques in descending order of size, we add a clique to the set of maximal cliques if it is not contained within any previously identified maximal clique.

\begin{algorithm}
	\small
	\caption{Maximal Cliques Identification}
	\begin{algorithmic}[1]
	\REQUIRE Chordal graph $G'(V,E')$, elimination order $\pi$
	\ENSURE Set of maximal cliques $\mathcal{C}$
	\STATE Initialize candidate cliques $\mathcal{K} \leftarrow \emptyset$
	\FOR{each $v \in V$}
    	\STATE $C_v \leftarrow \{v\} \cup \{u \mid (v,u) \in E' \text{ and } \pi(u) > \pi(v)\}$
   		\STATE $\mathcal{K} \leftarrow \mathcal{K} \cup \{C_v\}$
	\ENDFOR
	\STATE Sort $\mathcal{K}$ in descending order by clique size
	\STATE Initialize maximal cliques $\mathcal{C} \leftarrow \emptyset$
	\FOR{each $C \in \mathcal{K}$}
    	\IF{$C$ is not contained in any clique in $\mathcal{C}$}
        	\STATE $\mathcal{C} \leftarrow \mathcal{C} \cup \{C\}$
    	\ENDIF
	\ENDFOR
	\RETURN $\mathcal{C}$
	\end{algorithmic}
\end{algorithm}

Additionally, we implement the maximal chordal extension approach that generates the largest possible maximal cliques in the graph. This method employs a union-find algorithm to merge connected components into maximal cliques. The resulting graph consists of several disjoint complete subgraphs, where every pair of vertices within each subgraph is connected by an edge. 

Here's an example to illustrate the minimal degree and maximal chordal extension.

\begin{example}[An example of chordal extension]
	We apply the minimum degree maximal extension algorithm to graph (a). The elimination order $A$, $C$, $D$, $B$, $E$, $F$, and adding two edges $(B,D)$, $(B,F)$, we obtain the chordal extension graph shown in (b). The maximal cliques of the resulting chordal graph are $\cbrace{A,B,D}$, $\cbrace{B,D,E}$, $\cbrace{B,E,F}$, and $\cbrace{B,C,F}$.
	\input{figures/sparsity/chordal-extension.tex}
\end{example}


\subsection{Sparse Matrix}

A sparse matrix is a matrix where most elements are zero. More precisely, a matrix is considered sparse when the number of non-zero elements is significantly smaller than the total number of elements. Sparse matrices frequently arise in scientific computing, engineering problems, and data science, particularly when dealing with large-scale systems.

Let $\mbS^n$ denote the space of $n \times n$ symmetric matrices, and $\mbS^n_+$ denote the cone of $n \times n$ symmetric positive semidefinite matrices. For matrices $A, B$ of the same dimensions, their Hadamard (elementwise) product is defined as $(A \circ B){ij} = A{ij}B_{ij}$.

Given a graph $G(V, E)$, we define:
\begin{equation*}
\mbS_G = \cbrace{Q\in \mbS^{|V|}\mymid Q_{\beta\gamma} = Q_{\gamma\beta} = 0, \forall \beta\ne\gamma, (\beta, \gamma)\notin E}
\end{equation*}
where the rows and columns of $Q \in \mathbf{S}_G$ are indexed by $V$.

Let $\Pi_G$ be the projection form $\mbS^{|V|}$ to the subspace $\mbS_G$. Specifically, forall $Q\in \mbS^{|V|}$:
\begin{equation}
	\Pi_G(Q) = \begin{cases}
		Q_{\beta\gamma}, & \beta = \gamma \text{or} (\beta, \gamma)\in E\\
		0, & \text{otherwise}
	\end{cases}
\end{equation}

\textbf{Theorem 2.1.} (Grone et al.,1984~\cite{Grone1984lina-positive-definite})
Let $G(V, E)$ be a chordal graph and define $\Pi_G(\mathbf{S}^{|V|}_+) = \cbrace{\Pi_G(Q) \mymid Q \in \mathbf{S}^{|V|}_+}$. Let $I_1, \ldots, I_t$ be the list of maximal cliques of $G$. Then $Q \in \Pi_G(\mathbf{S}^{|V|}_+)$ if and only if $Q[I_i] \succeq 0$ for all $i\in[t]$, where $Q[I_i]$ denotes the principal submatrix of $Q$ indexed by $I_i$.

\subsection{Correlative Sparsity}
Correlative sparsity is corresponds to the variables of POP which is exploited by partitioning and regrouping the variables into cliques according to the links between them~\cite{Waki2006siam-sos-semidefinite-relaxation}.

\textbf{Definition 2.2.} (Correlative Sparsity Pattern Graph). We call the graph $G^{csp}(V, E)$ the correlative sparsity pattern graph of a POP with variables $\mvx \in \R^n$ if $V = [n]$ and $(i, j) \in E$ if one of the following holds:
\begin{enumerate}
    \item $\exists \mva \in \text{supp}(f), \quad\text{s.t.}\quad \alpha_i, \alpha_j > 0$
    \item $\exists k \in [m_{ineq}], \quad\text{s.t.}\quad x_i, x_j \in \text{var}(g_k)$
    \item $\exists k \in [m_{eq}], \quad\text{s.t.}\quad x_i, x_j \in \text{var}(h_k)$
\end{enumerate}

Using Algorithm that present above, we can obtain the chordal extension $(G^{csp})'$ and its maximal cliques $\cbrace{I_l}_{l=1}^{p}$. We then partition the constraint polynomials $g_1, \dots, g_{m_{ineq}}$ and $h_1, \dots, h_{m_{eq}}$ into groups $\left\{ g_j\mymid j\in \calG_l \right\}$ and $\left\{ h_j\mymid j\in \calH_l \right\}$. Here, $\mathcal{G}_l$ and $\mathcal{H}_l$, $l=1,\dots,p$ index the inequality and equality constraints respectively, satisfying:
\begin{enumerate}
    \item $\forall j \in \calG_l, \text{var}(g_j)\subseteq I_l$ 
    \item $\forall j \in \calH_l, \text{var}(h_j)\subseteq I_l$
\end{enumerate}

Now we introduce Moment-SOS Hierarchy with correlative sparsity. 

\textbf{Definition 2.2} (\emph{Riesz linear functional})
With $\mvy = (y_\mva)_{\mva}$ be a sequence indexed by the standard monomial basis $\sqbk{\mvx}^n$ of $\R[\mvx]$, The Riesz linear functional $L_{\mvy} \colon \R[\mvx]\to \R$ is defined as:
\begin{equation*}
    f = \sum_{\mva}f_{\mva}\mvx^\mva \mapsto \sum_{\mva}f_{\mva}y_{\mva}, \quad \forall f(\mvx) \in \R[\mvx]
\end{equation*}

The notation of $L_{\mvy}$ can be naturally extended to polynomial vectors and matrices, as illustrated in the following example:

\textbf{Example 2.1.} Let $n = 3$, $I = \left\{ 1,3 \right\}$. Then $\sqbk{\mvx}_1 = \left[1;x_1;x_2;x_3\right]$ and $\sqbk{\mvx(I)}_2 = \left[ 1;x_1;x_3;x_1^2;x_1x_3;x_3^2 \right]$. Applying $L_{\mvy}$, we have

\begin{align*}
	& L_{\mvy}((x_1 + x_3)\cdot \sqbk{\mvx(I)}_2) = \\
	& L_{\mvy}\left( 
		\sqbk{
			\begin{array}{c}
				x_1 + x_3\\
				x_1^2 + x_1x_3\\
				x_1x_3 + x_3^2\\
				x_1^3 + x_1^2x_3\\
				x_1^2x_3 + x_1x_3^2\\
				x_1x_3^2 + x_3^3
			\end{array}
		}
	 \right) = 
	 \sqbk{
		\begin{array}{c}
			y_{1,0,0} + y_{0,0,1}\\
			y_{2,0,0} + y_{1,0,1}\\
			y_{1,0,1} + y_{0,0,2}\\
			y_{3,0,0} + y_{2,0,1}\\
			y_{2,0,1} + y_{1,0,2}\\
			y_{1,0,2} + y_{0,0,3}
		\end{array}
	 }\\[1em]
	 & L_{\mvy}(\sqbk{\mvx}_1\sqbk{\mvx}_1^T) = \\
	 & L_{\mvy}\left(
		\sqbk{
			\begin{array}{cccc}
				1 & x_1 & x_2 & x_3\\
				x_1 & x_1^2 & x_1x_2 & x_1x_3\\
				x_2 & x_1x_2 & x_2^2 & x_2x_3\\
				x_3 & x_1x_3 & x_2x_3 & x_3^2
			\end{array}
		} \right) = 
		\sqbk{
			\begin{array}{cccc}
				y_{0,0,0} & y_{1,0,0} & y_{0,1,0} & y_{0,0,1}\\
				y_{1,0,0} & y_{2,0,0} & y_{1,1,0} & y_{1,0,1}\\
				y_{0,1,0} & y_{1,1,0} & y_{0,2,0} & y_{0,1,1}\\
				y_{0,0,1} & y_{1,0,1} & y_{0,1,1} & y_{0,0,2}
			\end{array}
		}
	\end{align*}

Let $d_j^g = \ceil{\text{deg}(g_j)/2}$, $j = 1,\dots,m_{ineq}$ and $d_j^h = \text{deg}(h_j)$, $j = 1,\dots,m_{eq}$. Then define $d_{\min} = \max\cbrace{ \ceil{\text{deg}(f)}/2, \cbrace{2d_j^g}_{j\in \ceil{m_{ineq}}}, \cbrace{d_h^g}_{j\in \ceil{m_{eq}}}}$

\textbf{Definition 2.3} For a fixed integer $d \geq d_{\min}$, we define the following polynomial matrices and vectors associated with each maximal clique $I_l$:
\begin{align}
	M_d(I_l) &= {\sqbk{\mvx(I_l)}_d^n}^T \sqbk{\mvx(I_l)}_d^n, l\in \sqbk{p}\\
	M_d(g_j, I_l) &= g_j{\sqbk{\mvx(I_l)}_{d-d_j^g}^n}^T \sqbk{\mvx(I_l)}_{d-d_j^g}^n, j\in \calG_l, l\in \sqbk{p}\\
	H_d(h_j, I_l) &= h_j\sqbk{\mvx(I_l)}_{2d-d_j^h}^n, j\in \calH_l, l\in \sqbk{p} \label{eq:gs:H}
\end{align}


Now we present the Moment-SOS Hierarchy with correlative sparsity for the POP.

\textbf{Proposition 2.1.} (\emph{Moment Relaxation with Correlative Sparsity}) Let  $g_0 \coloneqq 1, d_0^g \coloneqq 0$, then $M_d(g_0, I_l) = M_d(I_l)$.

For any integer $d \geq d_{\min}$, the moment hierarchy with correlative sparsity is defined as:
\begin{align}
	\begin{cases}
		\min & L_y(f) \\
		\text{s.t.} & L_y\left( M_d(g_j, I_l) \right) \succeq 0, \forall j\in \cbrace{0} \cup \calG_l, l\in [p]\\
		& L_y\left( H_d(h_j, I_l) \right) = 0, \forall j\in \calH_l, l\in [p]\\
		& y_{\mathbf{0}} = 1 
	\end{cases}
\end{align}
We call $d$ the \emph{relaxation order}.

\textbf{Remark 2.1.} We denote this problem as $p_d^{cs}$ and let $\rho_d$ be its optimal value. Under appropriate compactness assumptions~\cite{Lasserre2006siam-convergent-sdp-relaxation}, the sequence ${\rho_d}$ converges monotonically to the global optimum $\rho^\star$ of the original POP.


\subsection{Term Sparsity}
While correlative sparsity focuses on relationships between variables, term sparsity addresses relationships between monomials. Specifically, term sparsity is exploited by partitioning monomial bases into blocks based on the monomial connections present in the input polynomial system~\cite{wang2021siam-tssos}.

In practice, both types of sparsity can significantly reduce the size of SDPs involved in the Moment-SOS Hierarchy. Naturally, combining correlative and term sparsity can further enhance scalability. In this subsection, we focus on how these two types of sparsity can be integrated, rather than discussing term sparsity in isolation~\cite{wang2022tms-cs-tssos}.

Let ${I_l}, \ l\in [p]$ be the maximal cliques of the chordal extension of the correlative sparsity pattern graph $G^{csp}$, with $n_l \coloneqq |I_l|$. Let $\mathcal{G}_l$ and $\mathcal{H}_l$, $l\in [p]$ be defined as in the previous section on correlative sparsity. The variables $\mvx$ are then regrouped into subsets $\mvx(I_1), \dots, \mvx(I_l)$. Define:
\begin{align}
	\msA\coloneqq \text{supp}(f)\cup\bigcup_{j = 1}^{m_{ineq}}\text{supp}(g_j)\cup\bigcup_{j = 1}^{m_{eq}}\text{supp}(h_j)\cup (2\bbN)^n
\end{align}
Where $(2\bbN)^n$ is defined as $\cbrace{2\mva \mymid \mva\in \bbN^n}$
% We Fix the relaxation order $d\ge d_{\min}$, $d_0\coloneqq 0$ and $g_0\coloneqq 1$. 
% Let $\msA_l\coloneqq \cbrace{\mva\in\msA \mymid \text{supp}(\mva) \subseteq I^{cs}_l}$.
% Note that we embed $\bbN^{n_l}$ into $\bbN^n$ via the map $\mva = (\alpha_i) \mapsto \mva' = (\alpha_i')$ which satisfies
% \begin{align*}
% 	\alpha_i' = \begin{cases}
% 		\alpha_i & \text{if } i \in I^{cs}_l\\
% 		0 & \text{otherwise}
% 	\end{cases}
% \end{align*}

To exploit term sparsity of inequality constraints in the moment-SOS hierarchy, we proceed in two steps:

\begin{enumerate}
	\item \textbf{Support Extension: }
	
	For each $l \in [p]$ and $j \in \{0\} \cup \mathcal{G}_l$, define graph $G_{d,l,j}^g$ with:
	\begin{enumerate}
		\item Nodes: $V_{d,l,j} = |[\mathbf{x}(I_l)]_{d-d_j^g}^n|$
		\item Edges: 
		\begin{equation*}
			E_{d,l,j} = \cbrace{(\beta, \gamma)\mymid \text{supp}( M_d(g_j, I_l)_{\beta\gamma})\cap \msA \ne 0}
		\end{equation*}
	\end{enumerate}

	\item \textbf{Chordal Extension: } 
	
	Let $G'_{d,l,j}$ be the chordal extension of $G_{d,l,j}^g$, and define $B_{d,l,j}^g$ as its adjacency matrix.
\end{enumerate}

For equality constraints, we only perform support extension (as we deal with vectors rather than matrices):

\textbf{Support Extension: } Define

\begin{align}
	B_{d,l,j}^h(\beta) = \begin{cases}
		1, & \text{supp}(H_d(h_j, I_l)_{\beta}) \cap \msA \ne \emptyset\\
		0, & \text{otherwise}
	\end{cases}
\end{align}

\begin{example}[An example for support extension]
	% The csp graph itself is already a chordal graph. Let us consider cliques $\cbrace{x_0, v_0, \lambda_{1,0}, \lambda_{2,0}}$ and $\cbrace{x_0, x_1, v_0}$ in the exploration of correlative sparsity.
	% \input{figures/sparsity/csp-graph.tex}

	With relaxation order $d = 2$ and assume $I_1 = \cbrace{x_0, v_0, \lambda_{1,0}, \lambda_{2,0}}$ $I_2 = \cbrace{x_0, x_1, v_0}$. let $g_1 = \lambda_{1,0} + 1 + x_0$. This gives us graph $G_{2,1,1}^g$ and $G_{2,2,0}^g$. Then we can do chordal extension on them. 
	% $$
	% B_{2,1,1}^g = 
	% \begin{pmatrix}
	% 	1 & 1 & 1 & 1 & 1\\
	% 	1 & 1 & 0 & 1 & 1\\
	% 	1 & 0 & 1 & 0 & 0\\
	% 	1 & 1 & 0 & 1 & 0\\
	% 	1 & 1 & 0 & 0 & 1
	% \end{pmatrix}
	% $$
	% \input{figures/sparsity/support-extension.tex}
\end{example}


\textbf{Proposition 2.2.} (\emph{Moment Relaxation with Combined Sparsity})
The sparse moment relaxation incorporating both correlative and term sparsity is defined as:	

\begin{align}
	\begin{cases}
		\min & L_y(f) \\
		\text{s.t.} & L_y\left( M_d(g_j, I_l) \right) \circ B_{d,l,j}^g \in \Pi_{G'_{d,l,j}}(S^{|V_{d,l,j}|}_{+}), \forall j\in \cbrace{0} \cup \calG_l, l\in [p]\\
		& L_y\left( H_d(h_j, I_l) \right) \circ B_{d,l,j}^h = 0, \forall j\in \calH_l, l\in [p]\\
		& y_{\mathbf{0}} = 1 
	\end{cases}
\end{align}

\textbf{Remark 2.2.}
Let this problem be denoted as $p_d^{cs-ts}$. By iteratively applying support extension and chordal extension in term sparsity, we obtain new $B_{d,l,j}^g$ and $B_{d,l,j}^h$, leading to new multi-block SDP formulations based on the moment hierarchy. Define the iteration count $k$ as the \emph{sparse order} and let $\rho_d^k$ be the optimal value of the resulting problem. The sequence $\cbrace{\rho_d^k}_{k \geq 1}$ is monotonically non-decreasing and satisfies $\rho_d^k \geq \rho_d$ for all $k$.

\textbf{Remark 2.3.}
We have flexibility in choosing the type of chordal extension for term sparsity. If maximal chordal extension is used, with a fixed relaxation order $d$ and without correlative sparsity (i.e., taking $\cbrace{I_l}_{l=1}^p = \cbrace{[n]}$, $p=1$), then $\rho_d^k$ will monotonically converge in finitely many steps to the optimal value of the corresponding dense moment relaxation problem~\cite{wang2021siam-tssos}.

Moreover, using cliques generated by minimum degree (MD) or minimum fill (MF) algorithms can lead to smaller SDP problems, resulting in faster computation times, albeit potentially at the cost of reduced accuracy.

\textbf{Remark 2.4.}
For the large-scale contact-rich problems considered in this paper, we fix the sparse order $k$ to 1, as higher orders increase computational complexity while order 1 consistently achieves satisfactory accuracy.














