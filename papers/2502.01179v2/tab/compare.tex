\begin{table}[!htp]
\centering
\vspace{-1.2em}
\caption{\label{tab:intro}
\textbf{Comparison of full parameter tuning, LoRA, and activation editing methods.} We compare the intervention components, the percentage of new parameters introduced, the data efficiency, and robustness across different tasks. We use LLaMA-3~\cite{dubey2024llama} to compute the parameters. Note that full parameter tuning$^{*}$ does not introduce new parameters.
}
\resizebox{\textwidth}{!}{
\begin{tabular}{l|rcccc}
\toprule
       & \textbf{Params (\%)} & \textbf{Intervention}  &  \textbf{Dynamic Localization?} & \textbf{Data Efficient?} & \textbf{Robust?} \\
\midrule
Full Parameter Tuning$^{*}$ & 100\% &  - & -  & No & No \\
LoRA~\cite{hu2021lora} & 0.826\% & - & -  & No & No \\
\midrule
BitFit~\cite{ben-zaken-etal-2022-bitfit} & 0.0800\% & Bias Term & No & No & No \\
RED~\cite{wu-etal-2024-advancing}    & 0.0040\% & MLP Layer & No & Yes & No \\
REFT~\cite{wu2024reft}   & 0.0300\% & Hidden Representation & No & Yes & No \\
LOFIT~\cite{yin2024lofit}  & 0.0002\% & Attention & No & Yes & No \\
\midrule
\textbf{\jola{}}    & 0.0002\% & Attention & Yes & Yes & Yes \\
\bottomrule
\end{tabular}
}
\end{table}