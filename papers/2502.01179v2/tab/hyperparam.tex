\begin{table}[!thp]
\caption{
\label{tab:hyperparam}
Hyperparameter configurations for the baseline methods evaluated in our experiments. These settings are used across multiple tasks to assess model performance in low-resource settings, as discussed in Sections~\ref{sec:intro} and Section~\ref{sec:exp}.
}
\resizebox{\textwidth}{!}{
\begin{tabular}{l|l|p{8cm}}
\toprule
\textbf{Baseline} & \textbf{Hyperparameter} & \textbf{Values} \\
\midrule
\multirow{2}{*}{BitFit~\cite{ben-zaken-etal-2022-bitfit}} & Bias Moudule & bias of Q,K and V from attention/bias of LayerNorm from attention outputs/bias of LayerNorm from hidden outputs \\ \cmidrule{2-3}
 & Learning Rate & 1e-4/ 5e-4 \\
\midrule
\multirow{2}{*}{RED~\cite{wu-etal-2024-advancing}} & Rank & 8 / 16 \\
\cmidrule{2-3} 
 & Learning Rate & 5e-5/ 2e-4 / 6e-2 \\
\midrule
REPE~\cite{zou2023representation} & method & Representation Reading / Representation Control \\
\midrule
\multirow{2}{*}{ReFT~\cite{wu2024reft}} & Prefix + suffix posotion & p7 + s7 / p11 + s11 \\
\cmidrule{2-3} 
 & Layers & all / 4,6,10,12,14,18,20,22/3,9,18,24 \\
\midrule
\multirow{2}{*}{LoFIT~\cite{yin2024lofit}} & number of attention heads & 32/64/128 \\
\cmidrule{2-3} 
 & Learning Rate & 5e-4 / 5e-3 \\
\bottomrule
\end{tabular}
}
\end{table}
