\begin{table*}[!thp]
\centering
\caption{\label{tab:main_res}
\textbf{Main Results:} Average performance comparison (Accuracy, BLEU, Rouge-L, BERTScore) of different activation editing methods across reasoning, understanding, and generation tasks for LLaMA-3.1 and Qwen-2.5 models. The best results in each category are highlighted in bold.
}
% \vspace{1em}
% \small
 \resizebox{\textwidth}{!}{
\begin{tabular}{l|ccccc|ccccc}
\toprule
           & \multicolumn{5}{c}{\textbf{Llama-3.1-8B-Instruct}}                                  & \multicolumn{5}{c}{\textbf{Qwen2.5-7B-Instruct}}                                   \\
\cmidrule(lr){2-6}\cmidrule(lr){7-11}
           & \textbf{Reasoning} & \textbf{Understanding} & \multicolumn{3}{c}{\textbf{Generation}} & \textbf{Reasoning} & \textbf{Understanding} & \multicolumn{3}{c}{\textbf{Generation}} \\
\cmidrule(lr){4-6}\cmidrule(lr){9-11}
           & ACC ($\uparrow$) & ACC ($\uparrow$) & BLEU ($\uparrow$) & Rouge-L ($\uparrow$) & BERTScore ($\uparrow$) & ACC ($\uparrow$) & ACC ($\uparrow$) & BLEU ($\uparrow$) & Rouge-L ($\uparrow$) & BERTScore ($\uparrow$)        \\
\midrule
zero\_shot & 53.70 & 40.00 & 12.56 & 36.70 & 77.23 & 78.65 & 37.21 & 14.03 & 34.29 & 78.52 \\
LoRA       & 66.58 & 42.07 & 13.27 & 36.97 & 77.74 & 78.28 & 46.22 & 19.46 & 45.34 & 82.40 \\
\midrule
BitFit     & 63.05 & 35.02 & 9.25  & 28.81 & 74.83 & 69.25 & 28.72 & 13.47 & 33.10 & 77.89 \\
RED        & 46.19 & 37.33 & 11.24 & 32.40 & 76.24 & 71.52 & 38.76 & 12.81 & 34.75 & 77.52 \\
REPE       & 63.61 & 35.54 & 8.49  & 27.61 & 74.30 & 69.85 & 29.15 & 12.19 & 33.07 & 76.98 \\
REFT       & 65.95 & 40.89 & 12.60 & 36.89 & 77.21 & 72.69 & 47.74 & 16.02 & 37.40 & 79.74 \\
LOFIT      & 56.19 & 27.76 & 11.88 & 32.09 & 76.71 & 69.93 & 43.13 & 12.31 & 34.68 & 77.16 \\
\midrule
Our        & \textbf{70.55} & \textbf{47.00} & \textbf{17.07} & \textbf{40.65} & \textbf{80.54} & \textbf{82.40} & \textbf{51.57} & \textbf{24.00} & \textbf{50.23} & \textbf{85.90} \\
\bottomrule
\end{tabular}
}   
\end{table*}