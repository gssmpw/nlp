\begin{table}[!thp]
\caption{\label{tab:llama_reason_full}
The accuracy of LLaMA-3 across various commonsense reasoning tasks, comparing different baseline methods and our proposed method (\jola{}).
}
\resizebox{\textwidth}{!}{
\begin{tabular}{lccccccccc}
\toprule
                    & \textbf{ARC-c} & \textbf{ARC-e} & \textbf{BoolQ} & \textbf{HellaSwag} & \textbf{OBQA}  & \textbf{PIQA}  & \textbf{SIQA}  & \textbf{WinoGrande} & \textbf{\# AVG} \\
\midrule
\textbf{zero\_shot} & 59.56          & 65.40          & 41.99          & 45.19              & 54.80          & 76.01          & 42.78          & 43.88               & 53.70           \\
\textbf{LoRA}~\cite{hu2021lora}       & 70.13          & 77.85          & 56.37          & 66.18              & 73.38          & 71.36          & 63.42          & 53.97               & 66.58           \\
\midrule
\textbf{BitFit}~\cite{ben-zaken-etal-2022-bitfit}     & 64.17          & 72.35          & 49.69          & 63.48              & 74.07          & 71.24          & 58.14          & 51.28               & 63.05           \\
\textbf{RED}~\cite{wu-etal-2024-advancing}        & 39.67          & 56.20          & 9.69           & 40.81              & 50.75          & 70.09          & 50.46          & 51.84               & 46.19           \\
\textbf{REPE}~\cite{zou2023representation}       & 61.34          & 74.07          & 53.41          & 60.32              & 76.06          & 73.18          & 60.53          & 49.95               & 63.61           \\
\textbf{REFT}~\cite{wu2024reft}       & 66.36          & 77.37          & 53.34          & 63.96              & 75.43          & 73.50          & 62.27          & 55.36               & 65.95           \\
\textbf{LOFIT}~\cite{yin2024lofit}      & 57.10          & 78.59          & 43.69          & 42.01              & 61.73          & 53.96          & 56.37          & 56.10               & 56.19           \\
\midrule
\textbf{\jola{}}        & \textbf{74.66} & \textbf{80.13} & \textbf{62.17} & \textbf{70.69}     & \textbf{76.20} & \textbf{76.01} & \textbf{66.22} & \textbf{58.33}      & \textbf{70.55} \\
\bottomrule
\end{tabular}
}
\end{table}

\begin{table}[!thp]
\caption{\label{tab:qwen_reason_full}
The accuracy of Qwen-2.5 across various commonsense reasoning tasks, comparing different baseline methods and our proposed method (\jola{}).
}
\resizebox{\textwidth}{!}{
\begin{tabular}{lccccccccc}
\toprule
                    & \textbf{ARC-c} & \textbf{ARC-e} & \textbf{BoolQ} & \textbf{HellaSwag} & \textbf{OBQA}  & \textbf{PIQA}  & \textbf{SIQA}  & \textbf{WinoGrande} & \textbf{\# AVG} \\
\midrule
\textbf{zero\_shot} & 88.14 & 94.70 & 55.87 & 82.42 & 81.80 & \textbf{87.38} & \textbf{76.56} & 62.35 & 78.65 \\
\textbf{LoRA}~\cite{hu2021lora}       & 85.30 & 92.04 & 66.20 & 83.30 & 82.09 & 84.53 & 71.25 & 61.53 & 78.28 \\
\midrule
\textbf{BitFit}~\cite{ben-zaken-etal-2022-bitfit}     & 75.73 & 85.78 & 53.05 & 75.37 & 70.08 & 78.63 & 66.08 & 49.23 & 69.25 \\
\textbf{RED}~\cite{wu-etal-2024-advancing}        & 84.23 & 86.72 & 55.74 & 75.09 & 73.05 & 79.00 & 67.28 & 51.08 & 71.52 \\
\textbf{REPE}~\cite{zou2023representation}       & 78.90 & 83.51 & 55.49 & 74.18 & 68.38 & 81.45 & 63.20 & 53.72 & 69.85 \\
\textbf{REFT}~\cite{wu2024reft}       & 79.29 & 87.57 & 58.88 & 77.72 & 71.96 & 82.41 & 69.66 & 54.01 & 72.69 \\
\textbf{LOFIT}~\cite{yin2024lofit}      & 78.32 & 84.25 & 54.14 & 75.00 & 72.53 & 79.27 & 66.60 & 49.30 & 69.93 \\
\midrule
\textbf{\jola{}}        & \textbf{88.31} & \textbf{95.29} & \textbf{68.10} & \textbf{88.53} & \textbf{86.40} & 87.05 & 75.79 & \textbf{69.69} & \textbf{82.40} \\
\bottomrule
\end{tabular}
}
\end{table}


\begin{table}[!thp]
\caption{\label{tab:llama_mmlu_full}
The performance of LLaMA-3 across multiple domains in the MMLU-Pro benchmark.
}
\resizebox{\textwidth}{!}{
\begin{tabular}{lccccccccccccccc}
\toprule
                    & \textbf{Biology} & \textbf{Business} & \textbf{Chemistry} & \textbf{Computer Science} & \textbf{Economics} & \textbf{Engineering} & \textbf{Health} & \textbf{History} & \textbf{Law} & \textbf{Math} & \textbf{Other} & \textbf{Philosophy} & \textbf{Physics} & \textbf{Psychology} & \textbf{\#AVG} \\
\midrule
\textbf{zero\_shot} & \textbf{75}      & 31                & 27                 & \textbf{36}               & 53                 & 26                   & 55              & 46               & 31           & 20            & 34             & 43                  & 24               & 59                  & 40             \\
\textbf{LoRA}       & 65               & 35                & 35                 & 30                        & 47                 & 40                   & 54              & 51               & 36           & 31            & 32             & 36                  & 44               & 53                  & 42             \\
\midrule
\textbf{BitFit}     & 62               & 31                & 20                 & 30                        & 49                 & 23                   & 47              & 43               & 20           & 22            & 24             & 45                  & 21               & 53                  & 35             \\
\textbf{RED}        & 59               & 26                & 26                 & 31                        & 52                 & 26                   & \textbf{61}     & 46               & 34           & 23            & 33             & 42                  & 21               & 41                  & 37             \\
\textbf{REPE}       & 67               & 30                & 22                 & 30                        & \textbf{56}        & 26                   & 49              & 37               & 24           & 14            & 24             & 35                  & 28               & 55                  & 36             \\
\textbf{REFT}       & 71               & 31                & 31                 & 35                        & 57                 & 26                   & 55              & 45               & 31           & 23            & 32             & \textbf{46}         & 29               & 61                  & 41             \\
\textbf{LOFIT}      & 55               & 17                & 13                 & 29                        & 37                 & 32                   & 29              & 49               & 37           & 16            & 19             & 16                  & 7                & 33                  & 28             \\
\midrule
\textbf{\jola{}}        & 70               & \textbf{42}       & \textbf{43}        & 34                        & 53                 & \textbf{43}          & 55              & \textbf{54}      & \textbf{40}  & \textbf{37}   & \textbf{40}    & 39                  & \textbf{46}      & \textbf{62}         & \textbf{47}   \\
\bottomrule
\end{tabular}
}
\end{table}


\begin{table}[!thp]
\caption{\label{tab:qwen_mmlu_full}
The performance of Qwen-2.5 across multiple domains in the MMLU-Pro benchmark.
}
\resizebox{\textwidth}{!}{
\begin{tabular}{lccccccccccccccc}
\toprule
                    & \textbf{Biology} & \textbf{Business} & \textbf{Chemistry} & \textbf{Computer Science} & \textbf{Economics} & \textbf{Engineering} & \textbf{Health} & \textbf{History} & \textbf{Law} & \textbf{Math} & \textbf{Other} & \textbf{Philosophy} & \textbf{Physics} & \textbf{Psychology} & \textbf{\#AVG} \\
\midrule
\textbf{zero\_shot} & 71          & 20          & 17          & 36          & 55          & 17          & 46          & 44          & 27          & 13          & 44          & 48          & 19          & 64          & 37          \\
\textbf{LoRA}       & 68          & 32          & 36          & 45          & 58          & 36          & 48          & 53          & 34          & 40          & 33          & 40          & 50          & 74          & 46          \\
\midrule
\textbf{BitFit}     & 49          & 25          & 13          & 17          & 40          & 26          & 25          & 30          & 9           & 18          & 25          & 29          & 29          & 66          & 29          \\
\textbf{RED}        & 73          & 21          & 20          & 40          & 56          & 23          & 49          & 43          & 28          & 15          & \textbf{45} & 46          & 20          & 65          & 39          \\
\textbf{REPE}       & 57          & 32          & 20          & 22          & 42          & 23          & 17          & 14          & 21          & 34          & 22          & 9           & 27          & 68          & 29          \\
\textbf{REFT}       & 74          & 38          & 37          & 46          & 55          & 29          & 53          & 50          & 37          & 36          & 42          & 44          & 54          & 74          & 48          \\
\textbf{LOFIT}      & 73          & 25          & 23          & 43          & 58          & 35          & 51          & 47          & 29          & 27          & 44          & \textbf{49} & 31          & 67          & 43          \\
\midrule
\textbf{\jola{}}        & \textbf{75} & \textbf{41} & \textbf{39} & \textbf{49} & \textbf{62} & \textbf{38} & \textbf{56} & \textbf{57} & \textbf{42} & \textbf{45} & 42          & 45          & \textbf{55} & \textbf{76} & \textbf{52} \\
\bottomrule
\end{tabular}
}
\end{table}


\begin{table}[!thp]
\caption{\label{tab:llama_gem_full}
The performance of LLaMA-3 across various natural language generation tasks (Commen\_Gen, E2E\_NLG, WEB\_NLG, and Xsum), using BLEU, ROUGE-L, and BERTScore as evaluation metrics.
}
\resizebox{\textwidth}{!}{
\begin{tabular}{lllllllllllll}
\toprule
                    & \multicolumn{3}{c}{\textbf{Commen\_Gen}}         & \multicolumn{3}{c}{\textbf{E2E\_NLG}}            & \multicolumn{3}{c}{\textbf{WEB\_NLG}}            & \multicolumn{3}{c}{\textbf{Xsum}}               \\
\cmidrule(lr){2-4}\cmidrule(lr){5-7}\cmidrule(lr){8-10}\cmidrule(lr){11-13}
                    & BLEU           & Rouge-L        & BertScore      & BLEU           & Rouge-L        & BertScore      & BLEU           & Rouge-L        & BertScore      & BLEU          & Rouge-L        & BertScore      \\
\midrule
\textbf{zero\_shot} & 16.19          & 46.59          & 79.69          & 8.26           & 27.47          & 74.10          & 21.65          & 52.11          & 83.79          & 4.14          & 20.65          & 71.35          \\
\textbf{LoRA}       & 18.17          & 49.54          & 81.15          & 13.15          & 39.75          & 77.50          & 19.53          & 34.50          & 82.18          & 2.25          & 24.11          & 70.12          \\
\midrule
\textbf{BitFit}     & 13.16          & 31.02          & 77.51          & 9.25           & 31.28          & 74.77          & 12.25          & 40.25          & 76.86          & 2.35          & 12.68          & 70.19          \\
\textbf{RED}        & 17.19          & 45.41          & 80.43          & 10.31          & 30.44          & 75.51          & 14.45          & 42.62          & 78.43          & 2.99          & 11.14          & 70.60          \\
\textbf{REPE}       & 11.24          & 30.15          & 76.15          & 8.12           & 25.46          & 74.01          & 12.36          & 42.36          & 76.94          & 2.25          & 12.46          & 70.12          \\
\textbf{REFT}       & 20.22          & 48.26          & 82.69          & 12.60          & 32.71          & 77.11          & 13.09          & 43.36          & 77.46          & 4.49          & 23.22          & 71.58          \\
\textbf{LOFIT}      & 12.17          & 30.53          & 76.81          & 14.77          & 38.88          & 78.66          & 18.12          & 
\textbf{46.38} & 81.11          & 2.47          & 12.57          & 70.26          \\
\midrule
\textbf{Our}        & \textbf{23.13} & \textbf{53.47} & \textbf{84.93} & \textbf{15.54} & \textbf{42.52} & \textbf{79.22} & \textbf{24.39} & 38.09          & \textbf{85.92} & \textbf{5.24} & \textbf{28.50} & \textbf{72.07} \\
\bottomrule
\end{tabular}
}
\end{table}

\begin{table}[!thp]
\caption{\label{tab:qwen_gem_full}
The performance of Qwen-2.5 across various natural language generation tasks (Commen\_Gen, E2E\_NLG, WEB\_NLG, and Xsum), using BLEU, ROUGE-L, and BERTScore as evaluation metrics.
}
\resizebox{\textwidth}{!}{
\begin{tabular}{lllllllllllll}
\toprule
                    & \multicolumn{3}{c}{\textbf{Commen\_Gen}}         & \multicolumn{3}{c}{\textbf{E2E\_NLG}}            & \multicolumn{3}{c}{\textbf{WEB\_NLG}}            & \multicolumn{3}{c}{\textbf{Xsum}}               \\
\cmidrule(lr){2-4}\cmidrule(lr){5-7}\cmidrule(lr){8-10}\cmidrule(lr){11-13}
                    & BLEU           & Rouge-L        & BertScore      & BLEU           & Rouge-L        & BertScore      & BLEU           & Rouge-L        & BertScore      & BLEU          & Rouge-L        & BertScore      \\
\midrule
\textbf{zero\_shot} & 14.58                    & 41.85                       & 78.53                         & 8.08                     & 25.63                       & 73.97                         & 31.13                    & 56.11                       & 91.40                         & 2.32                     & 13.59                       & 70.17                         \\
\textbf{LoRA}       & 17.16                    & 52.39                       & 80.40                         & 23.39                    & 46.65                       & 85.14                         & 31.00                    & 55.50                       & 91.30                         & 6.29                     & 26.84                       & 72.77                         \\
\midrule
\textbf{BitFit}     & 14.92                    & 40.16                       & 78.77                         & 15.25                    & 35.03                       & 79.01                         & 21.49                    & 43.27                       & 83.66                         & 2.23                     & 13.94                       & 70.11                         \\
\textbf{RED}        & 13.91                    & 41.75                       & 78.04                         & 8.25                     & 26.55                       & 74.09                         & 26.58                    & 54.64                       & 87.67                         & 2.50                     & 16.06                       & 70.28                         \\
\textbf{REPE}       & 11.46                    & 41.01                       & 76.31                         & 13.11                    & 30.46                       & 77.47                         & 21.94                    & 46.25                       & 84.01                         & 2.26                     & 14.58                       & 70.13                         \\
\textbf{REFT}       & 15.84                    & 41.37                       & 79.43                         & 18.05                    & 35.43                       & 81.07                         & 25.04                    & 48.93                       & 86.44                         & 5.15                     & 23.85                       & 72.01                         \\
\textbf{LOFIT}      & 11.23                    & 40.73                       & 76.15                         & 9.17                     & 28.47                       & 74.72                         & 26.47                    & 54.50                       & 87.58                         & 2.36                     & 15.02                       & 70.19                         \\
\midrule
\textbf{Our}        & \textbf{21.12}           & \textbf{57.54}              & \textbf{83.38}                & \textbf{28.32}           & \textbf{52.60}              & \textbf{89.08}                & \textbf{35.32}           & \textbf{58.54}              & \textbf{94.99}                & \textbf{11.24}           & \textbf{32.25}              & \textbf{76.15}     \\
\bottomrule
\end{tabular}
}
\end{table}