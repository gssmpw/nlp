\section{Related Work}
\label{sec:related_work}
\noindent\textbf{Low-Resource Fine-tuning.}
Recent advancements in LLMs have transformed a wide range of NLP tasks~\cite{zhao2023survey}. However, efficiently adapting these models to diverse applications remains challenging, especially in low-resource settings. Parameter-efficient fine-tuning (PEFT) methods~\cite{hu2021lora,dettmers2024qlora}, which update a small subset of parameters or integrate new modules~\cite{houlsby2019parameter}, achieve performance comparable to full fine-tuning across various tasks~\cite{wang2024parameter}. Yet, mitigating overfitting in low-resource scenarios remains a key challenge. Activation editing techniques~\cite{ben-zaken-etal-2022-bitfit,wu-etal-2024-advancing,wu2024reft,yin2024lofit} offer a lightweight approach to model adaptation, often argued to be more data-efficient than standard PEFT methods.

\noindent\textbf{Pruning.}
Neural network pruning~\cite{chengneuralpruning2024} aims to reduce model complexity and computational demands by removing less important or redundant components. Our approach builds on pruning techniques, specifically expected-$L_0$ regularization~\cite{louizos2018learning}. However, rather than pruning heads, our goal is to modify a selected subset of heads while keeping the rest intact.  Subnetwork pruning techniques (e.g., \citealp{frantar2023sparsegpt};\citealp{sun2023simple}) seek to identify an effective subnetwork, often tailored to a specific task, domain, or language. However, their primary objective is typically to match the performance of the full model rather than to specialize it for a particular task.

\noindent\textbf{Sparse Fine-tuning.}
Sparse finetuning~\cite{dao2022monarch,thangarasa2023spdf} is a technique for adapting LLMs to specific tasks or datasets while only updating a small subset of the model's parameters.
Our approach shares similarities with sparse fine-tuning, a technique commonly used in multilingual modeling~\cite{nooralahzadeh2023improving,choenni-etal-2024-examining}, where languages are typically assumed to be encoded modularly. Sparse fine-tuning identifies specific components (e.g., heads), fine-tunes all their parameters, and discards the others.
In contrast, \jola{} adjusts the activations of selected components while keeping the rest intact.
While the goal of sparse fine-tuning is often to match the performance of the full model using a smaller version, our aim is not only to reduce model size but to enhance performance over the full model.