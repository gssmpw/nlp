\section{Results}
\label{sec:res}
This section evaluates the performance of our proposed method, \jola{}, in comparison with various baselines.
Table~\ref{tab:main_res} presents the average performance for all methods across the three tasks, while Figure~\ref{fig:main_res} illustrates the results for individual subtasks.
More detailed numerical results can be found in Appendix~\ref{appendix:full_res}.

%% main res table
\input{tab/main_res}
%%

\noindent\textbf{Performance of Activation-Based Baselines.}
Activation editing baselines exhibit varying levels of success across tasks, but their sensitivity to hyperparameter selection and layer intervention limits their consistency.
For example, BitFit~\cite{ben-zaken-etal-2022-bitfit} demonstrates notable sensitivity to the placement of bias terms within the model.
Adjusting bias terms in dropout layers or attention mechanisms results in performance fluctuations, particularly in low-data scenarios.
Similarly, RED~\cite{wu-etal-2024-advancing} depends on the specific positions where scaling and bias vectors are introduced, leading to inconsistent results.
REPE~\cite{zou2023representation} is highly sensitive to the quality of activation representations across tasks, making it challenging to generalize its performance.
ReFT~\cite{wu2024reft} achieves moderate success by intervening on selected layers but faces challenges in determining the optimal number and choice of layers.
LoFIT~\cite{yin2024lofit}, while effective in leveraging task-relevant attention heads, struggles to maintain consistency across tasks.

\noindent\textbf{Performance of LoRA.}
LoRA achieves noticeable improvements over zero-shot baselines and, somewhat surprisingly, outperforms previous activation editing methods across all tasks when its rank hyperparameter is appropriately tuned. 
In tasks such as natural language generation, LoRA achieves higher BLEU and ROUGE-L scores, highlighting its ability to generate coherent outputs.

\noindent\textbf{Performance of \jola{}}
Our proposed method, \jola{}, consistently outperforms all baselines across the three tasks by a significant margin.
This can be attributed to \jola{}'s dynamic gated attention mechanism, which allows for adaptive activation of attention heads.
Unlike LoFIT~\cite{yin2024lofit}, which requires manual selection of attention heads, \jola{}'s mechanism enables less relevant heads to gradually ``die off" during training, improving robustness and adaptability.
In commonsense reasoning, \jola{} achieves an average improvement of 3.97\% over the best-performing baseline (LoRA) in LLaMA-3, as shown in Table~\ref{tab:main_res}.
For natural language understanding, \jola{} demonstrates consistent performance across diverse domains in the MMLU-Pro benchmark~\cite{wang2024mmlu} across all 14 subtasks as illustrated in Figure~\ref{fig:main_res}.
In natural language generation tasks, \jola{} achieves higher BLEU, ROUGE-L and BERTScore scores compared to activation-based baselines and LoRA.