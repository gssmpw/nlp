\section{Background}
\label{sec:background}
Activation editing in LLMs modifies intermediate activation outputs to steer model behavior.
We categorize existing approaches into three types based on the transformation function applied to activations.
Given an activation output $z_t^{(l,i)} \in \mathbb{R}^{d_l}$ for $i$th component at layer $l$, the general transformation is:
\begin{equation}
    z_t^{(l,i)'} = f(z_t^{(l,i)}),
\end{equation}
where $f(\cdot)$ determines the intervention type:
\begin{compactitem}
\item \textbf{Additive methods} apply a bias vector $a^{i}_{l} \in \mathbb{R}^{d_l}$ : $z_t^{(l,i)'} = z_t^{(l,i)} + a^{(l,i)}$.
\item \textbf{Multiplicative methods} scale activations as $z_t^{(l,i)'} = m^{(l,i)} \odot z_t^{(l,i)}$, where $m^{(l,i)} \in \mathbb{R}^{d_l}$ and $\odot$ is an element-wise product.
\item \textbf{Hybrid methods} combine both transformations: $z_t^{(l,i)'} = m^{(l,i)} \odot z_t^{(l,i)} + a^{(l,i)}$.
\end{compactitem}

Existing methods follow these paradigms but often rely on fixed selections of components for modification, limiting adaptability.
For example, BitFit~\cite{ben-zaken-etal-2022-bitfit} updates bias terms, while RED~\cite{wu-etal-2024-advancing} employs per-dimension scaling vectors and bias vectors. ReFT~\cite{wu2024reft} applies fine-tuned low-rank hidden states with MLP layers, and LoFIT~\cite{yin2024lofit} intervenes in selected attention heads with additive bias vectors but requires manual selection.