\section{Conclusion}
\label{sec:conclusion}
In this paper, we introduce~\jola{}, a novel approach for low-resource fine-tuning that jointly learns to dynamically localize the attention heads to be targeted for intervention and the best practice for editing the activation outputs with multiplicative scaling vectors and/or additive bias vectors.
We observe that attention heads are more effective than other model components in activation editing, offering a novel perspective for future research in this area.
Extensive experiments and ablation studies demonstrate the robustness of our method in low-data scenarios and across varying model sizes, highlighting the importance of joint component selection and activation editing.