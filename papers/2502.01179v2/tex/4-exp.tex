%% main res fifure
\input{figure/main_res}
%%

\section{Experiments}
\label{sec:exp}

\subsection{Datasets and Tasks}
\label{subsec:data_task}
We evaluate \jola{} on three diverse tasks: commonsense reasoning, natural language understanding, and natural language generation.
Additional details regarding the datasets can be found in Appendix~\ref{appendix:dataset}.

\paragraph{Commonsense Reasoning.}
For commonsense reasoning, we utilize a widely adopted benchmark~\cite{hu-etal-2023-llm,wu2024reft} containing 8 datasets: ARC-c and ARC-e~\cite{clark2018think}, BoolQ~\cite{clark-etal-2019-boolq}, HellaSwag~\cite{zellers-etal-2019-hellaswag}, OBQA~\cite{mihaylov-etal-2018-suit}, PIQA~\cite{bisk2020piqa}, SIQA~\cite{sap-etal-2019-social}, and WinoGrande~\cite{sakaguchi2021winogrande}. 
These datasets consist of multiple-choice questions, where the model must directly generate the correct option without providing explanations. 

\paragraph{Natural Language Understanding.}
We evaluate on the MMLU-Pro benchmark~\cite{wang2024mmlu}, covering 14 domains: Biology, Business, Chemistry, Computer Science, Economics, Engineering, Health, History, Law, Math, Philosophy, Physics, Psychology, and Others. 
Each task requires selecting the correct answer from ten options, testing the model's broad knowledge and reasoning capabilities.

\paragraph{Natural Language Generation.}
For generation tasks, we select 4 datasets from GEM benchmark~\cite{gehrmann-etal-2022-gemv2}, including CommonGen~\cite{lin-etal-2020-commongen} for concept-to-sentence generation, E2E~\cite{novikova-etal-2017-e2e} and WebNLG~\cite{gardent-etal-2017-creating} for data-to-text generation, and XSum~\cite{narayan-etal-2018-dont} for abstractive summarization of long documents. 
This selection ensures a diverse evaluation of generation tasks, including coherence, informativeness, and abstraction.

\subsection{Baselines}
We compare \jola{} against a range of state-of-the-art baselines:
(1) \textbf{Zero-Shot}: Direct evaluation of pre-trained large language models (LLMs) without fine-tuning, including LLaMA-3~\cite{dubey2024llama} and Qwen-2.5~\cite{yang2024qwen2}.
(2) \textbf{Parameter-Efficient Fine-Tuning}: LoRA~\cite{hu2021lora}, a method for efficient fine-tuning by injecting trainable low-rank updates into the model's weights.
(3) \textbf{Activation editing during training}: BitFiT~\cite{ben-zaken-etal-2022-bitfit}, a method that fine-tunes only the bias terms of the model; RED~\cite{wu-etal-2024-advancing}, which adds scaling and bias vectors to the outputs of MLP layer; ReFT~\cite{wu2024reft}, which directly intervenes on task-specific hidden states with MLP Layers, and LOFIT~\cite{yin2024lofit}, a two-stage method that selects task-relevant attention heads and applies bias tuning.
(4) \textbf{Activation editing during infernece}: 
REPE~\cite{zou2023representation}, which modifies representations derived from contrastive prompts.

\subsection{Implementation}
We conduct experiments using the \textit{Llama-3.1-8B-Instruct} (8B) and \textit{Qwen2.5-7B-Instruct} (7B) models as the primary base models.
Both are publicly available via the Huggingface repository\footnote{\url{https://huggingface.co}}. 
To study the impact of model size, we also experiment with smaller (\textit{Llama-3.2-1B-Instruct}, \textit{Llama-3.2-3B-Instruct}) and larger (\textit{Llama-3.1-70B-Instruct}) model variants.
For all datasets, we sample 200 examples to simulate low-resource scenarios, with further analysis of data size effects provided in Section~\ref{sec:analysis}. 
The prompt templates used in our method are also included in the Appendix~\ref{appendix:prompt}.
In all baseline experiments, we observe that the choice of hyperparameters significantly affected performance across different tasks.
To address this, we conduct a hyperparameter search for each method, selecting five hyperparameters and averaging the results.
The final outcomes are presented in Table~\ref{tab:main_res} and Figure~\ref{fig:main_res}.
More details on the training setup, computational resources, and hyperparameter selection process are provided in Appendix~\ref{appendix:exp_config}.

\subsection{Evaluation Metrics}
We employ exact match accuracy as the evaluation metric for commonsense reasoning and natural language understanding tasks. 
For natural language generation, we use BLEU~\cite{papineni-etal-2002-bleu}, ROUGE-L~\cite{lin-2004-rouge} and BERTScore~\cite{zhang2019bertscore} scores as implemented in the GEM benchmark~\cite{gehrmann-etal-2022-gemv2}.