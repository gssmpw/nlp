\section{Method}
\label{sec:method}
In this section, we introduce \jola{}, a novel approach for fine-tuning LLMs in low-resource settings.
We first identify two key challenges in existing approaches and present an analysis to better motivate our method (Section~\ref{subsec:motivation}).
We then propose a gated dynamic attention head selection mechanism to address these limitations (Section~\ref{subsec:localize}).
Figure~\ref{fig:framework} illustrates the comparison of previous activation editing approaches and \jola{}.

\input{figure/diff_components}

\subsection{Motivation}
\label{subsec:motivation}
Activation editing methods have demonstrated success in modifying Transformer components such as bias terms~\cite{ben-zaken-etal-2022-bitfit}, MLP layers~\cite{wu-etal-2024-advancing}, low-rank hidden state subspaces~\cite{wu2024reft}, and specific attention heads~\cite{yin2024lofit}. However, two critical questions remain underexplored:
\textbf{Q1:} Which Transformer components are most crucial for effective activation editing?
\textbf{Q2:} What combination of multiplicative and additive operations yields the best performance for intervention?
Existing approaches predefine the components to edit and rely on fixed intervention strategies, such as simple multiplicative scaling, which limits adaptability and can lead to inconsistent performance across tasks, especially in low-resource scenarios.  
To address these questions, we conduct controlled experiments to compare the effectiveness of editing different Transformer components and analyze the relative contributions of multiplicative and additive operations.\footnote{Details on experimental setup and datasets are provided in Section~\ref{sec:exp}.}

\paragraph{Q1: Component Selection.} 
We evaluate activation editing across four Transformer components: bias terms, MLP layers, hidden states, and attention heads. Figure~\ref{fig:diff_comp} indicates that attention heads are the most impactful module to target.
Unlike other modules, which primarily refine intermediate representations, attention heads encode key semantic relationships and play a crucial role in reasoning and task adaptation~\cite{ren-etal-2024-identifying}.
Moreover, intervening in multiple components often degrades performance, reinforcing the need for targeted attention head modifications.

%% scaling and offset vectors
\input{figure/scaling_offset}

\paragraph{Q2: Scaling vs. Offset Operations.} 
Activation editing typically involves two operations: scaling (multiplicative) and offset (additive) adjustments.
To evaluate their relative importance, we conduct ablation studies isolating each operation.
As shown in Figure~\ref{fig:scale_off}, bias offsets consistently contribute more to performance improvements than scaling.
We hypothesize that this behavior arises because the bias directly adjusts the latent representations, facilitating fine-grained task-specific adaptation while retaining the features of the pre-trained model.
In contrast, scaling modifies the activations uniformly, which may introduce unintended distortions.
These findings motivate our approach: \jola{} incorporates both operations but prioritizes offset interventions for more effective adaptation.

\subsection{Joint Localization and Editing}
\label{subsec:localize}
Based on our insights from Section~\ref{subsec:motivation}, \jola{} focuses on adaptive attention head interventions to maximize activation editing effectiveness.
Existing methods like LoFIT~\cite{yin2024lofit} require manual hyperparameter tuning to select the number of attention heads and cannot adjust the chosen heads during training. Moreover, their head selection criteria do not necessarily align with interventions. For instance, LoFIT employs multiplicative variables to determine head selection before restarting training with additive-only interventions. To address these limitations, we propose a method that jointly learns which heads to modify while optimizing intervention parameters (i.e., vectors $m^{(l,i)}$ and $a^{(l,i)}$). 

We extend the hybrid intervention method from Section~\ref{sec:background} by introducing two scalar gates, \( g_{a}^{(l,i)} \) and \( g_{m}^{(l,i)} \), both in \([0,1]\). This results in the transformation:
\begin{equation}  
    \label{eq:gate_editing}  
    z_t^{(l,i)^{\prime}} =  (\mathbf{1} + g_{m}^{(l,i)} \cdot m^{(l,i)}) \odot z_{t}^{(l,i)} + g_{a}^{(l,i)} \cdot a^{(l,i)},  
\end{equation} 
where \( \mathbf{1} \in \mathbb{R}^{d_l} \) is a vector of ones. The transformation is designed so that when both gates are closed ($g_{a}^{(l,i)} = g_{m}^{(l,i)} = 0$), it reduces to the identity map, effectively disabling the intervention for that head. By using separate gates, the model can learn to apply additive and multiplicative modifications independently.

Since our goal is to apply activation editing to a small, adaptively chosen subset of heads, we encourage the gates to be exactly zero where intervention is unnecessary. To achieve this, we use expected-$L_0$ regularization, a technique originally introduced by \citet{louizos2018learning} for pruning neural network weights. This approach has since been successfully applied to tasks such as head pruning~\cite{voita-etal-2019-analyzing} and extracting reasoning paths in graph neural networks~\cite{schlichtkrull2020interpreting}.

During training, each gate is modeled as a stochastic variable drawn from a Hard-Concrete distribution~\cite{louizos2018learning},
\begin{align}
g_{a}^{(l,i)} &\sim P(g_{a}^{(l,i)} \mid \phi_a^{(l,i)}), \quad
g_{m}^{(l,i)} \sim P(g_{m}^{(l,i)} \mid \phi_m^{(l,i)}) \end{align}

The Hard-Concrete distribution is a mixed discrete-continuous distribution over 
$[0, 1]$, with point masses at $0$ and $1$ and a continuous density over $(0, 1)$. The closed-form probability of a gate being non-zero (e.g., $1 - P(g_{a}^{(l,i)} = 0 \mid \phi_a^{(l,i)})$), is used to define a sparsity-inducing regularizer:

\begin{align}
L_C(\phi) = \sum_{l,i} & \Big( 1 - P(g_{a}^{(l,i)} = 0 \mid \phi_a^{(l,i)}) \\ & + 1 - P(g_{m}^{(l,i)} = 0 \mid \phi_m^{(l,i)}) \Big).
\end{align}

The overall training objective balances task-specific performance with sparsity:  
\begin{equation}  
    L(\mathbf{m}, \mathbf{a}, \phi) = L_{xent}(\mathbf{m}, \mathbf{a}) + \lambda L_C(\phi),  
\end{equation}  
where $L_{xent}$ is the standard cross-entropy loss, and $\lambda$ controls the trade-off between performance and sparsity. Optimization is performed over all intervention parameters: $\mathbf{\phi}$, $\mathbf{m}$ and $\mathbf{a}$.

As with parameter pruning~\cite{louizos2018learning}, the expected value of each gate can be computed. The interventions with very low expected gate value (i.e., $\mathbb{E}[g_m^{(l,i)}] < \epsilon$) can be 
disregarded with no effect on \jola{} performance. For the remaining heads, the gate is set during inference to its expected value.

By dynamically selecting and adapting attention head interventions, \jola{} achieves efficient and effective activation editing, overcoming the limitations of previous methods.
Our approach ensures robust, data-efficient adaptation across diverse tasks, making it well-suited for low-resource settings.