\section{Introduction}
\label{sec:intro}
Parameter-efficient fine-tuning (PEFT; \citealp{han2024parameter}) methods are widely used to adapt large language models (LLMs). However, popular PEFT approaches like LoRA \cite{hu2021lora} often struggle in low-resource settings with only a few hundred examples. Inspired by advances in interpretability research~\cite{vig2020causal,zhang2023towards}, {\it activation editing} techniques~\cite{wu-etal-2024-advancing,yin2024lofit} have emerged as an alternative. These methods modify model activations to adapt LLMs to new tasks, leveraging the intuition that LLMs encode many semantically meaningful properties in a  coordinate-aligned (or even disentangled) manner. The activations can then be adjusted with simple operations such as pruning, rescaling or addition. 
Activation editing method avoid more complex transformations, such as the MLPs used in the original Adapters~\cite{houlsby2019parameter}. 
Editable components in activation editing  include bias terms~\cite{ben-zaken-etal-2022-bitfit}, MLP layer outputs~\cite{wu-etal-2024-advancing}, hidden states within MLP layers~\cite{wu2024reft}, and attention head outputs~\cite{yin2024lofit}.

Compared to standard PEFT methods like LoRA~\cite{hu2021lora}, activation editing modifies significantly fewer parameters. For example, in our experiments, the optimal LoRA configuration altered 0.826\% of LLaMA-3 8B’s~\cite{dubey2024llama} parameters, whereas LOFIT~\cite{yin2024lofit} updated only 0.002\%.\footnote{Detailed comparisons are provided in Appendix~\ref{appendix:compare}.} This drastic reduction makes activation editing particularly appealing for low-resource scenarios, where only a few hundred training examples are available.

%% framework figure
\input{figure/framework}
%%


However, activation editing’s effectiveness is highly sensitive to the choice of modules. %to intervene. 
This selection is typically determined either by fixed hyperparameters -- specifying which layers and component types to modify~\cite{ben-zaken-etal-2022-bitfit,wu2024reft} -- or by additional methods that estimate the importance of different model components for a given task~\cite{yin2024lofit}. 
Furthermore, existing approaches vary in their intervention strategies (e.g., additive vs. multiplicative modifications), with no clear consensus on which method is most effective across tasks and models.
As a result, performance tends to be inconsistent across different datasets and models (see Figure~\ref{fig:main_res} and Table~\ref{tab:main_res}).

To address these limitations, we propose \textbf{Joint Localization and Activation Editing} (\textbf{\jola{}}), a method that, for a given task, jointly learns (1) which components to edit, (2) whether to apply additive, multiplicative, or combined interventions, and (3) the optimal intervention parameters -- the vectors applied as additive offsets or multiplicative scalings to modules' outputs. Rather than relying on fixed heuristics or manual selection, \jola{} dynamically identifies the most relevant components and applies targeted modifications to their activations.

To achieve this, \jola{} uses HardConcrete gates with expected-L0 regularization, a technique previously employed for parameter~\cite{louizos2018learning} and component pruning~\cite{voita-etal-2019-analyzing}. 
This method encourages sparsity, ensuring that only a small subset of components is selected for editing, thereby reducing the number of interventions and, thus, the method’s effective parameter count. We also observe that it appears sufficient to focus on heads' outputs rather than other component types, further reducing the parameter counts and enhancing the simplicity of the method. By combining additive offsets and multiplicative scalings, \jola{} provides a flexible, data-efficient adaptation strategy. 

We evaluate \jola{} across three benchmark categories: commonsense reasoning, natural language understanding, and natural language generation.
Experimental results on 26 tasks from the benchmarks~\cite{hu-etal-2023-llm, wang2024mmlu, gehrmann-etal-2022-gemv2} demonstrate that \jola{} consistently outperforms existing methods in low-resource settings (as shown in Figure~\ref{fig:main_res}), delivering robust performance across various data scales and model sizes.

In summary, our contributions are as follows:
(i) We introduce \jola{}, a novel activation editing approach that jointly optimizes the selection of intervention components and the intervention strategy, specifically tailored for low-resource scenarios.
(ii) We demonstrate that \jola{} achieves stable and consistent performance across diverse tasks, addressing key limitations of existing methods. We further validate its effectiveness across different data scales and model sizes.
(iii) We provide new insights into the role of attention heads in activation editing, showing that they are the most impactful components for fine-tuning.