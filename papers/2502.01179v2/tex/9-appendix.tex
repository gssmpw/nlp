\section{Comparison of the number of parameters
%Compare 
with Full Parameter Fine-Tuning, PEFT and Activation Editing}
\label{appendix:compare}
To better demonstrate the advantages of our approach over full parameter fine-tuning, LoRA~\cite{hu2021lora}, and existing activation editing methods, we compare them across five key aspects: the percentage of modified parameters, the intervention modules, dynamic localization of intervention modules, data efficiency, and robustness across various tasks.
Using the fine-tuning of LLaMA-3~\cite{dubey2024llama} as an example, as shown in Table~\ref{tab:intro}, compared to full parameter fine-tuning and LoRA, activation editing requires editing only a small fraction of parameters, significantly fewer than the parameters from LoRA.
Activation editing is also a data-efficient approach, often effective even with just 100 examples.
However, we find that existing activation editing methods lack robustness across different datasets, primarily due to their reliance on manually configuring intervention module parameters, which are highly sensitive to task-specific variations.
In contrast, our proposed method, \jola{}, achieves comparable parameter efficiency to the best activation editing methods (e.g., LoFIT \cite{yin2024lofit}), while also delivering robust performance across tasks in low-resource settings.
This is due to dynamically adjusting the intervention module configurations rather than relying on manual parameter tuning.

%% Table: comparison
% \vspace{-1em}
\input{tab/compare}
%% Table: Dataset
\input{tab/dataset}

\section{Datasets}
\label{appendix:dataset}
We conduct experiments across three tasks: commonsense reasoning~\cite{hu-etal-2023-llm}, natural language understanding~\cite{wang2024mmlu}, and natural language generation~\cite{gehrmann-etal-2022-gemv2}.
Table~\ref{tab:dataset} provides a brief overview of the sub-datasets or sub-tasks within the three benchmarks evaluated.
The commonsense reasoning task is framed as a multiple-choice problem, where the correct answer is selected from 2 to 4 possible options.
The natural language understanding task also follows a multiple-choice format, but with ten options.
The natural language generation task, on the other hand, is an end-to-end text generation task, where unstructured data (such as commonsense concepts or data) is converted into coherent text.
In the training phase, we simulate a low-resource scenario by using 200 examples.
Section~\ref{subsec:data_model_size} further explores experiments with varying numbers of samples.
To ensure consistency across experiments, we used the same random seed (seed$=42$) for data sampling, ensuring identical training samples in all runs.

%% Table: prompt
\input{tab/prompt}
%% parameter search
\input{tab/hyperparam}

\section{Prompt Setting}
\label{appendix:prompt}
Recent studies~\cite{he2024does,lai-etal-2024-llms} have highlighted the substantial impact of prompt design on model performance.
In our experiments, we adopt the same prompt configurations as~\citet{hu-etal-2023-llm} for the commonsense reasoning benchmark, and used the prompts from the original paper for the MMLU-Pro benchmark~\cite{wang2024mmlu}.
For the GEM benchmark~\cite{gehrmann-etal-2022-gemv2}, where the original paper did not provide the prompt settings, we utilized commonly used prompts curated from PromptSource\footnote{\url{https://github.com/bigscience-workshop/promptsource}}.
To ensure reproducibility of our results, we present the prompts employed in our experiments in Table~\ref{tab:prompt}.

\section{Experiment Configurations}
\label{appendix:exp_config}
\subsection{Training Setup}
\label{appendix:traiing_config}
We conduct all experiments using the HuggingFace Transformers\footnote{\url{https://github.com/huggingface/transformers}} library and fine-tuned the models with the TRL toolkit\footnote{\url{https://github.com/huggingface/trl}}.
The AdamW optimizer~\cite{loshchilov2017decoupled} was used for fine-tuning, with $\epsilon = 1e-6$ and one epoch of warm-up.
In addition, we employ an exponentially decaying~\cite{li2019exponential} learning rate schedule, defined by the following formula:
\begin{equation}
    \text{lr}(t) = \text{lr}_0 \cdot \lambda^{t} \cdot e^{-\text{decay} \cdot t}
\end{equation}
where $\text{lr}_0$ is the initial learning rate $lr_{0}$set to $5 \times 10^{-4}$ , $\lambda$ is 0.1, and the decay rate is 0.01.
For the gating units, we used a temperature of 0.33 in the Gumbel Softmax~\cite{jang2017categorical}.
Fine-tuning was performed in full precision for the 7B, 8B, 1B, and 3B models, while for the 70B model, we applied 4-bit quantization to enable single-precision fine-tuning.

\subsection{Computational Resources}
\label{appendix:comp_resource}
All experiments for the 1B, 3B, 8B, and 13B models were conducted on a single NVIDIA A100 80GB GPU server.
The 70B model, described in Section~\ref{subsec:data_model_size}, was evaluated on an NVIDIA H100 94GB GPU server.
As an example, with the 8B LLaMA-3 model, \jola{} converged within 2 GPU hours on most tasks in the low-resource setting, using only 200 training samples.

\subsection{Hyperparameter Search for Baselines}
\label{appendix:hyper_search}
As discussed in Section~\ref{sec:intro} and Section~\ref{sec:exp}, the performance of baseline methods in low-resource settings is highly sensitive to hyperparameters across different tasks.
However, it is impractical to conduct hyperparameter searches for each task individually, given that we evaluate 26 tasks in total, and performing a separate search for each would be time-consuming.
To mitigate this, we use five different sets of hyperparameters during the baseline experiments.
The average results of these five configurations are reported in Table~\ref{tab:main_res} and Figure~\ref{fig:main_res}.
The specific hyperparameters used for each method are detailed in Table~\ref{tab:hyperparam}.

\section{Full Results Across all Tasks}
\label{appendix:full_res}
Due to page limitations, we present the average performance across the 26 tasks in Table~\ref{tab:main_res} and Figure~\ref{fig:main_res}.
In this section, we provide detailed performance metrics for each individual task.
Specifically, Table~\ref{tab:llama_reason_full} reports the accuracy of LLaMA-3 on the commonsense reasoning task, while Table~\ref{tab:qwen_reason_full} presents the accuracy of Qwen-2.5 on the same task.
Table~\ref{tab:llama_mmlu_full} shows the accuracy of LLaMA-3 on the natural language understanding task, and Table~\ref{tab:qwen_mmlu_full} shows the corresponding accuracy for Qwen-2.5.
Finally, Table~\ref{tab:llama_gem_full} presents the BLEU, ROUGE-L, and BERTScore for LLaMA-3 on the natural language generation task, with Table~\ref{tab:qwen_gem_full} displaying the corresponding metrics for Qwen-2.5.

%% full result
\input{tab/full_res}

\section{Case Study}
\label{appendix:case_study}
To provide an intuitive evaluation of the advantages of our method, we select one representative case from each of the tasks: commonsense reasoning, natural language understanding, and natural language generation.
The results generated by the baseline and our approach are presented below.

%% case study
\input{tab/case}





