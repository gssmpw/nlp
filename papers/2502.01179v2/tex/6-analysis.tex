\section{Analysis}
\label{sec:analysis}
In this section, we present a detailed analysis of \jola{} through ablation studies (Section~\ref{subsec:ablation_1}, Section~\ref{subsec:ablation_2}, and Section~\ref{subsec:ablation_3}), an exploration of gate status during training (Section~\ref{subsec:gate_states}), and evaluations across varying data and model sizes (Section~\ref{subsec:data_model_size}).
Unless otherwise specified, the analyses in this section are conducted on selected tasks, including \textit{SIQA}, \textit{WinoGrande}, \textit{Law}, \textit{Physics}, \textit{E2E\_NLG}, and \textit{WEB\_NLG}.
In addition, we provide a case study to better visualize the advantages of \jola{} in Appendix~\ref{appendix:case_study}.

\subsection{Ablation 1: Gate Mechanism}
\label{subsec:ablation_1}
Dynamic gating attention head selection is central to the performance of \jola{}, as detailed in Section~\ref{subsec:localize}.
To evaluate its necessity, we compare models with and without the gating mechanism.
As illustrated in Figure~\ref{fig:abalation_gate}, the gating mechanism provides a significant improvement to task performance.
We speculate that this improvement arises because certain attention heads can be modified more effectively to achieve the desired behavior in a generalizable way, whereas modifying others may disrupt the model. 
The gating mechanism can selectively adjust the activation outputs of relevant attention heads, avoiding excessive or unnecessary edits that could harm performance.
In contrast, models without this gating mechanism fail to differentiate between ``editable" and less ``editable" heads, resulting in performance instability.

\input{figure/ablation_gate}


\subsection{Ablation 2: Number of Gates}
\label{subsec:ablation_2}
In Equation~\ref{eq:gate_editing}, we employ separate gating units for the scaling vector and the bias vector.
To investigate the impact of this design, we compare configurations where each vector has its own gate with configurations where both vectors share a single gate.
As illustrated in Figure~\ref{fig:gate_num}, although the shared gating configuration achieves a performance improvement over the zero-shot baseline, it underperforms compared to the configuration with separate gates.
This suggests that the choice of intervention should depend on what role the head plays in a given task.
Using independent gating units enables fine-grained control over each vectorâ€™s contribution, facilitating more precise task-specific adjustments and preventing over-modification of the activation outputs.

\input{figure/gate_num}

\subsection{Ablation 3: Different Head Selection Strategies}
\label{subsec:ablation_3}
Head selection is a critical component of \jola{}'s design.
To evaluate whether alternative selection strategies could achieve similar outcomes, we compare \jola{} with three established methods:
(1) \textbf{SMP}~\cite{zhang2021know}, which trains a separate pruner to rank and identify attention heads that are less important for the task;
(2) \textbf{DSP}~\cite{li-etal-2021-differentiable}, which employs Gumbel-Softmax~\cite{jang2017categorical} to iteratively select the top-K heads; and
(3) \textbf{PASS}~\cite{ding2024pass}, which uses robust optimization to enforce deterministic sparsity.

As shown in Figure~\ref{fig:ablation_pruning}, \jola{} outperforms these methods, especially in low-resource scenarios.
SMP's reliance on large datasets for training the pruner makes it ill-suited for sparse data.
DSP's iterative selection process is highly sensitive to noisy gradients from small datasets, leading to unstable or incorrect selection decisions.
While PASS achieves deterministic sparsity, its regularization objective overfits to limited data distributions, resulting in suboptimal gate decisions.
By contrast, \jola{}'s stochastic gating mechanism effectively balances exploration and exploitation, allowing it to adaptively identify important heads even in low-data settings.

\input{figure/ablation_pruning}

\subsection{Gate Status During Traning}
\label{subsec:gate_states}

To further investigate the dynamic gating mechanism, we analyzed the probability of the $g_m$ and $g_a$ gates being ``closed" (i.e., having a value of 0) during training on the OBQA dataset~\cite{mihaylov-etal-2018-suit}.
As shown in Figure~\ref{fig:gate_states}, both gates start in the ``open" state (at batch=0), allowing all attention heads to contribute to the learning process.
The probability of the gates being closed increases during training, which reflects the gradual closing of redundant attention heads.
This behavior is consistent with our expectations, i.e., attention heads that encode task-relevant information remain active early in training, while less important heads are pruned as their representations converge.

\input{figure/gate_states}

\subsection{Further Analysis}
\label{subsec:data_model_size}
\paragraph{Different Data Size}
To evaluate \jola{}'s robustness across varying data scales, we conduct experiments using 100 to 1000 training examples sampled from the SIQA and WinoGrande datasets.
Figure~\ref{fig:data_size} shows that \jola{} consistently outperforms baselines even with only 100 examples, demonstrating its efficacy in extreme low-resource settings.
Performance improves steadily with increasing data, highlighting the method's adaptability to diverse levels of data availability.
This scalability across data sizes underscores the practicality of \jola{} for real-world applications where labeled data is often limited.

\input{figure/data_size}

\paragraph{Different Model Size}
To evaluate the scalability of \jola{} with respect to model size, we test three variants: \textit{Llama-3.2-1B-Instruct}, \textit{Llama-3.2-3B-Instruct}, and \textit{Llama-3.1-70B-Instruct}.
As shown in Table~\ref{tab:model_size}, \jola{} consistently delivers significant performance improvements across all model sizes.
Notably, larger models benefit more substantially from \jola{}'s dynamic selection mechanism, as they inherently possess greater redundancy in attention heads.
This finding highlights \jola{}'s scalability and effectiveness in optimizing large-scale models while maintaining robust performance in low-data scenarios.

\input{tab/model_size}

