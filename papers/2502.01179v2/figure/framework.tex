\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{images/framework.pdf}
    \caption{\label{fig:framework}
    Comparison of previous representative activation editing methods with proposed \jola{}. (a) includes BitFIT~\cite{ben-zaken-etal-2022-bitfit}, which fine-tunes only the bias term; RED~\cite{wu-etal-2024-advancing} introduces scaling and bias vectors in the MLP layer; ReFT~\cite{wu2024reft}, which fine-tunes the hidden layer representations; and LoFIT~\cite{yin2024lofit} intervenes with attention heads in two steps. (b) \jola{} introduces a gating mechanism that dynamically selects and locates attention heads to modify the activation outputs.
}
\vspace{-1em}
\end{figure*}