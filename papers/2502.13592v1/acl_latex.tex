% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.
\PassOptionsToPackage{table,xcdraw,dvipsnames}{xcolor}
\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}
\usepackage{comment}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

%\usepackage[table,xcdraw,dvipsnames]{xcolor} % Required for coloring cells
%\usepackage{colortbl} % Required for row and cell coloring

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{\textit{Don't Stop the Multi-Party!} \\ On Generating Synthetic Multi-Party Conversations with Constraints}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{Nicol\`o Penzo\textsuperscript{1,2}, Marco Guerini\textsuperscript{1}, Bruno Lepri\textsuperscript{1}, Goran Glava\v{s}\textsuperscript{3}, Sara Tonelli\textsuperscript{1}\\\\
  $^1$ Fondazione Bruno Kessler, Italy\\
  $^2$ University of Trento, Italy \\
  $^3$ Center For Artificial Intelligence and Data Science, University of Würzburg, Germany\\
}
%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle



\begin{abstract}

Multi-Party Conversations (MPCs) are widely studied across disciplines, with social media as a primary data source due to their accessibility. However, these datasets raise privacy concerns and often reflect  platform-specific properties. For example, interactions between speakers may be limited due to rigid platform structures (e.g., threads, tree-like discussions), which yield overly simplistic interaction patterns (e.g., as a consequence of "reply-to" links). This work explores the feasibility of generating diverse MPCs with instruction-tuned Large Language Models (LLMs) by providing deterministic constraints such as dialogue structure and participants’ stance. We investigate two complementary \textit{strategies} of leveraging LLMs in this context: \textsc{(i.)} \textit{LLMs as MPC generators}, where we task the LLM to generate a whole MPC at once and \textsc{(ii.)} \textit{LLMs as MPC parties}, where the LLM generates one turn of the conversation at a time, provided the conversation history. We next introduce an analytical framework to evaluate compliance with the constraints, content quality, and interaction complexity for both strategies. Finally, we assess the quality of obtained MPCs via human annotation and LLM-as-a-judge evaluations. We find stark differences among LLMs, with only some being able to generate high-quality MPCs. We also find that turn-by-turn generation yields better conformance to constraints and higher linguistic variability than generating MPCs in one pass. Nonetheless, our structural and qualitative evaluation indicates that both generation strategies can yield high-quality MPCs.

\end{abstract}

\section{Introduction}\label{sec:intro}

Multi-Party Conversations (MPCs), i.e., conversations involving more than two participants \cite{branigan2006perspectives}, have been studied across multiple disciplines. Research in conversational analysis and linguistics has focused on modeling interaction dynamics \cite{sacks1978simplest, wilson1984models},  identifying participant roles \cite{malouf1995towards}, or mapping emergent structural patterns in discourse \cite{10.1353/sof.2003.0055}. These studies highlight both complexity and diversity of real-world MPCs, where factors like turn-taking, speaker alignment, and social context shape the flow of conversation.

The collection of MPC data has, however, strongly shifted from in-person and online meetings to social media platforms \cite{mahajan-shaikh-2021-need}, where large-scale data is more accessible. However, this shift has introduced several confounding factors. Social media platforms often enforce a one-to-one reply structure, overlooking implicit addressees and simplifying interaction dynamics; in natural conversations, in contrast, a turn is often directed to multiple participants and the conversational structure is more dynamic. As a result, MPC corpora derived from social media platforms often lack structural diversity, which severely limits their utility in analyzing real-world conversational phenomena \cite{weiEtAl2023}. 
This, in turn, affects generation capabilities of current Large Language Models (LLMs). For LLMs, trained on conversations from social media and predominantly used in two-party interactions (i.e. human-assistant use-cases), MPCs represent a distributional shift, resulting in their underwhelming performance in natural MPC contexts \cite{tan-etal-2023-chatgpt, penzo-etal-2024-llms}. The next generation of LLMs is, however, expected to engage in MPCs and excel in tasks like identifying the appropriate speaker to respond to \cite{weiEtAl2023}, summarizing meetings \cite{kirstein-etal-2024-tell} or even managing multi-agent scenarios \cite{wu2023autogenenablingnextgenllm}. Recent studies have explored their performance in social contexts \cite{10.1162/coli_a_00502, chang2024llmsgeneratestructurallyrealistic}, emphasizing the need for large, representative datasets to train this novel generation of LLMs and to ensure robustness across diverse and less frequent interaction patterns \cite{lee2024towards}. \citet{zhou-etal-2024-real} explicitly highlight multi-party interactions as a key challenge for future LLM research. 

One possible remedy for the lack of structural 
diversity in MPCs derived from social media data is to synthesize MPCs, by \textit{explicitly constraining LLMs} to generate MPCs with specific characteristics.
In particular, generated MPCs should include diverse conversations, encompass various interaction structures, topics and contexts and capture rich speaker-addressee relationships, allowing for multi-addressee interactions, in order to reflect real-world conversational complexity.

In this paper, we propose generating synthetic MPCs using LLMs guided by constrains related to the above capabilities. We explore two generation strategies: \textsc{(i.)} One-Long (OL) generation, where the LLM produces an entire MPC in a single step, and \textsc{(ii.)} Turn-by-Turn (TT) generation, which constructs the conversation sequentially, one turn at a time. A comparison of resulting MPCs from both strategies highlights the (potential) discrepancies between \textsc{(i.)} what LLMs believe human MPCs look like (OL) and \textsc{(ii.)} how they behave as participants in an MPC (TT).
We propose a novel evaluation framework that combines several quantitative and qualitative dimensions of generated MPCs, focusing on the extent of LLMs' compliance to provided content and structural constraints. 

We address the following three key research questions:

\vspace{1mm}
\noindent \textbf{RQ(1)}: Can LLMs be leveraged to generate large synthetic MPC datasets while maintaining compliance with predefined constraints on dialogue structure and participants' stance?

\vspace{1mm}
\noindent \textbf{RQ(2)}: Which generation strategy (One-Long vs. Turn-by-Turn) produces higher-quality MPCs?

\vspace{1mm}
\noindent\textbf{RQ(3)}: How can we effectively evaluate the variety and quality of the generated MPCs?

\vspace{1mm}
We test four popular LLMs and identify Llama3.1 \cite{dubey2024llama3herdmodels} and Qwen2.5 \cite{yang2024qwen2technicalreport} as the best MPCs for complying the most with constraints. Turn-by-Turn seems to generate more constraint-compliant MPCs than One-Long. Moreover, the MPCs produced by TT exhibit greater lexical variability and semantic coherence. The generated MPCs also present a higher structural complexity than a widely-used corpus of `real' conversations \cite{ouchi-tsuboi-2016-addressee}. Finally, a qualitative evaluation shows that both TT and OL can produce high-quality MPCs, rendering the choice of the LLM more important than the choice of generation strategy. \footnote{The code for generation and evaluation of the MPCs is available at the Github repository: \url{https://github.com/dhfbk/Constrained-SyntheticMPC}.}

\section{Related Work}
\citet{mahajan-shaikh-2021-need} categorize MPC corpora into three types: Spoken Unscripted, Spoken Scripted, and Written. In this work, we address Spoken Unscripted and Written MPCs. Spoken Unscripted MPCs typically occur in in-person discussions, with spontaneous interactions. Written MPCs, on the other hand, are characteristic of online platforms: here conversations unfold asynchronously, often due to platform-specific constraints. While social media allow for rapid collection of large-scale written MPCs, these datasets often come with incomplete interaction metadata. Many datasets record only explicit reply-to relationships, neglecting implicit addressees and richer conversational dynamics \cite{ouchi-tsuboi-2016-addressee, zhang-etal-2018-conversations, chang-danescu-niculescu-mizil-2019-trouble}. \citet{weiEtAl2023} point out that well-known MPC corpora \cite{ritter-etal-2010-unsupervised, baumgartner2020pushshiftredditdataset, lowe-etal-2015-ubuntu} are useful for response generation, but not for more interactive tasks. Only most recent efforts focus on capturing conversational dynamics (i.e., go beyond text content) \cite{hua-etal-2024-get}.

Structural analyses of social communication networks have primarily focused on interaction patterns across multiple conversations \cite{panzarasa2009patterns, coletto2017motif, 10.1145/3178876.3186139, felmlee2021dyads}. This confirms the relevance of such structures in studying conversation dynamics. However, our focus is on interactions emerging within a single conversation rather than across multiple discussions, applying the same structural analysis techniques. 

To the best of our knowledge, the only existing attempt at generating synthetic MPCs was made by \citet{chen-etal-2023-places}. However, their work primarily focused on conversations involving at most three participants, limiting the complexity of interactions. In contrast, our study explores the generation of MPCs with four or more participants, leading to more elaborate discussion dynamics. While this increased complexity allows for richer conversational structures, it also introduces a higher likelihood of generation errors, necessitating a rigorous evaluation process to assess the quality and consistency of the generated dialogues.

\section{Synthetic MPCs Generation}

\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{latex/images/example_turn.pdf}
    \caption{Example of a turn in a synthetic MPC.}
    \label{fig:example_turn}
    \vspace{-1em}
\end{figure}

In our framework, a Multi-Party Conversation (MPC) consists of an ordered sequence of turns, where each turn includes the speaker information (\textit{who} wrote the turn), the message (\textit{what} the textual content of the turn is), and the addressees (to \textit{whom} the turn is directed), see for example Figure \ref{fig:example_turn}. 
In this section, we first introduce the two generation strategies we test (Section \ref{subsec:modalities_pres}), followed by the topics chosen for the MPCs (Section \ref{subsec:topic_to_discuss}), and finally the constraints specified in the instructions for generating MPCs (Section \ref{subsec:conv_constr}).

\subsection{Generation Strategies}\label{subsec:modalities_pres}
We test two strategies for generating MPCs using instruction-based models. 
Our main goal is to determine whether LLMs behave differently when asked to generate an MPC as a unique narrative compared to acting as an interactive participant within the conversation. With this motivation, we use each LLM in two generation strategies: 

\textbf{One-Long generation strategy (OL).} 
The LLM is prompted to generate the entire conversation in one pass. In this strategy, generation starts with a system input prompt that defines all the constraints and the task, asking then to generate the entire conversation. This strategy follows a one-step, long-generation process, based on a single input context.

\textbf{Turn-by-Turn generation strategy (TT).}  
Here the LLM is prompted to generate the conversation incrementally, provided the conversation history. The model is prompted multiple times to perform one of three tasks:  
\textsc{(i.)} generate a speaker, \textsc{(ii.)} generate interactions between a speaker and addressees (given the speaker), or \textsc{(iii.)} generate a message (given the interaction).
The process begins with a system prompt specifying the constraints and these three tasks. The model is first prompted to generate each speaker and assign them a stance on a controversial topic. Then, the LLM generates a sequence of interactions and messages (one at a time), iteratively augmenting the MPC: this means that the context provided to the LLM increases monotonically in size with consecutive turns. 


\subsection{Topics} \label{subsec:topic_to_discuss}

To generate a controlled set of synthetic MPCs, we identify a set of controversial topics to encourage more polarized and clear statements from speakers based on their assigned stance.
Specifically, following \citet{li-etal-2024-llms-speak}, we select $38$ topics and create two stance statements for each topic: one reflecting a \textit{progressive} perspective and the other a \textit{conservative} perspective. The statements are created trying to avoid potential biases such as framing statements negatively or using specific terms exclusively in one category. 
Finally, we instruct the LLMs to generate conversations based on each of the resulting 76 statements. We provide the full list of topics in Appendix \ref{app:topic_to_discuss}.

\subsection{Conversation Constraints}\label{subsec:conv_constr}

To ensure that the generated conversations feature rich interaction patterns with diverse dynamics, we instruct the model to follow specific constraints (we provide details about how this was operationalized in prompts in Appendix \ref{appendix:system_prompts}).

\textbf{Output Format}: to enable automated analysis, the generated output must respect a structured \texttt{JSON} format with all the information needed. So, each generated MPC must be a dictionary with two primary keys, namely  \texttt{conversation} and \texttt{speakers}. The \texttt{conversation} field must include a list of dictionaries, each with specific fields such as  \texttt{speaker}'s name, turn \texttt{message} and \texttt{addressees}, i.e. the list of participants in the conversation to whom the message is directed. The \texttt{speakers} field includes the speaker's \texttt{name} and the \texttt{stance} with respect to the conversation topic. 

\textbf{Interactions}: 
these constraints refer to three requirements in the generated MPCs -- all speakers appearing in the interactions must be present in the speakers' list (i.e., the LLM should not invent a new speaker half way through the conversation); 
\texttt{addressees} must cover at least once also the role of \texttt{speaker};  self-interactions, i.e. speakers sending a message to themselves, are not admitted.

\textbf{Speaker's Contribution}: all speakers in the \texttt{speakers} field must be authors of at least one turn in the conversation.

\textbf{Number of Speakers}: In order to enable complex interaction structures, each MPC must involve between 4 and 6 speakers.

\textbf{Number of Messages}: Each generated MPC must include 15 messages across all speakers, with a maximum of 50 words per message.

\textbf{Speaker's Stance}: We specify the exact number of speakers for each stance (e.g., 2 with the \textit{pro} and 3 with \textit{against} stance). 

We additionally request that the first turn always addresses all participants: this ensures that the generated interaction graph is connected, as required for structural analysis (see Section \ref{subsec:glob}).

\section{Evaluation Framework}\label{sec:analysis}

We design an evaluation framework aimed at assessing different aspects of the generated MPCs. It is composed of four blocks, which we detail below. 

\subsection{Compliance with Constraints}\label{subsec:filter}

The first dimension considered in the evaluation framework is to what extent the synthetic MPCs comply with the format and structural constraints given in the prompt. For each generated MPC, this framework must verify:
\textsc{(i.)} the correctness of the \textit{Output Format}; \textsc{(ii.)} the correctness of the \textit{Interactions}; \textsc{(iii.)} the \textit{Contribution} of each speaker; \textsc{(iv.)} the \textit{Number of Speakers}; \textsc{(v.)} the \textit{Number of Messages}; \textsc{(vi.)} the distribution of the \textit{Stance of the Speakers}.

All the computed values must be compliant with the constraints presented in Section \ref{subsec:conv_constr}. Only for the Number of Messages, we relax the constraint by considering valid MPCs including less than 15 turns if they contain at least $2$ messages per speaker. Indeed, after a manual check of the generated MPCs, we noticed that shorter or longer conversations may still represent high-quality data. Each value is computed separately and then used to identify how many MPCs comply with \textit{all} these constraints.

\begin{figure*}
    \centering
    \includegraphics[width=0.7\linewidth]{latex/images/structural_analysis.pdf}
    \caption{Overview of the metrics considered in our structural analysis.}
    \label{fig:struct_metrics}
\end{figure*}


\subsection{Analysis of Language Variability}\label{subsec:content_analysis}

A key risk for synthetic datasets is to suffer from low linguistic variability, due to repetitive examples obtained when using similar prompts (even if stochastic decoding is used), an issue already highlighted for dialogical settings 
\cite{occhipinti-etal-2024-fine}.
On the other hand, while generated MPCs should ideally be lexically rich, they should also be semantically coherent, i.e. different MPCs about the same topic should exhibit a certain degree of semantic similarity.  

To control for these aspects, we compute the following three metrics:

\textbf{Repetition Rate} \cite{bertoldi-etal-2013-cache}, which has already been used in synthetic conversational scenarios in the past \citep{bonaldi-etal-2022-human}, measures the rate of non-singleton n-grams within a cluster of MPCs.  

\textbf{String Similarity} between pairs of turns is computed using thefuzz library\footnote{\url{https://github.com/seatgeek/thefuzz}} and is based on Lehvenstein distance. 

\textbf{Semantic Coherence} between pairs of turns is computed by first embedding each turn with SentenceBERT-all-MiniLM-L6-v2 \cite{reimers-gurevych-2019-sentence} and then calculating pairwise cosine similarity.

The above metrics are computed across all generated MPCs about the same topic. More specifically,  Repetition Rate is computed among all MPCs in the same topic-based cluster and then averaged across all clusters to assess linguistic diversity. String Similarity and Semantic Coherence are calculated across all turns in all possible conversation pairs on the same topic, following an all-vs-all comparison strategy. For each topic we keep only the 5 highest similarity scores and average them, then we compute the average of all topic-level scores.

\subsection{Interaction Structure Analysis}\label{subsec:glob}

To describe and quantify the structural complexity of interactions in the generated MPCs, we compute a series of network metrics, focusing on node-level properties, dyads (pairs of nodes) and triads (triplets of nodes), according to standard practices in interaction network analysis \cite{pauksztat2011speaks, felmlee2021dyads}. 
Following \citet{penzo-etal-2024-llms}, we represent MPC interactions with an \textit{unweighted undirected graph} $G_{u}$ and a \textit{weighted directed graph} $G_d$. 

To measure the average activity of a node in the conversation, we compute two metrics. First, the \textbf{Average Degree Centrality} in $G_u$, denoted as $deg_{avg}(G_{u})$, represents the average number of speakers each participant interacts with, regardless of direction. Second, the \textbf{Average Out-going Degree} in $G_d$, denoted as $outdeg_{avg}(G_{d})$, captures the average number of speakers each participant interacts with having a specific direction. Figure \ref{fig:struct_metrics} provides a visual representation of $deg_{avg}(G_{u})$ (graph A) and $outdeg_{avg}(G_{d})$ (graph B). Both averages are computed across all the nodes in the conversation and normalized according to their maximum possible values.

When two speakers, $s_1$ and $s_2$, reply to each other, they form a \textit{cycle} \cite{coletto2017motif}, represented by a directed edge $e_1$ from $s_1$ to $s_2$ and a reciprocal edge $e_2$ from $s_2$ to $s_1$ (graph C in Figure \ref{fig:struct_metrics}). If this back-and-forth exchange continues multiple times, the edge weights $w(e_1)$ and $w(e_2)$ will both become $>$ $1$. We refer to such recurring exchange as \textit{consistent cycles} (graph D in Figure \ref{fig:struct_metrics}). Based on this, we compute the \textbf{Reciprocity} $R(G_{d})$, i.e., the total number of cycles between two nodes over all pairs of nodes in $G_d$, and the \textbf{Consistent Reciprocity} $R^w(G_{d})$, i.e. the number of consistent cycles between two nodes over all pairs of nodes in $G_d$. Finally, to quantify how often speakers build "triads" of interactions, we compute the \textbf{Transitivity} $T(G_{u})$ (graph E in Figure \ref{fig:struct_metrics}).

For all these metrics, higher values indicate more complex interactions in a conversation. Indeed, higher reciprocity (consistent or not) suggests more frequent back-and-forth exchanges. Again, higher average degree values means that speakers engage with more participants, while greater transitivity reflects denser connections, leading to the creation of more interconnected speaker groups \cite{pauksztat2011speaks}.

\subsection{Qualitative Evaluation}\label{subsec:hum_eval}

As a final assessment, we evaluate MPCs qualitatively. We  run both a small-scale human evaluation and an ``LLM as a judge'' assessment \cite{gu2024survey} for analyses on a large scale.  

We ask an expert human annotator and an LLM to rate a given MPC along the following dimensions with a Likert Scale from $1$ to $5$, inspired by the scores proposed in \citet{chen-etal-2023-places}: \textsc{(i.)} \textit{naturalness}, i.e., the quality of the overall flow, tone, and word choice in the conversation; 
\textsc{(ii.)} \textit{argumentability}, i.e., how well the conversation presents reasoned and well-argued positions;
\textsc{(iii.)} \textit{speaker's  stance consistency}, i.e., whether all speakers maintain their assigned stance at the beginning of the conversation; 
\textsc{(iv.)} \textit{speaker's  stance evolution}, i.e., whether speakers demonstrate a realistic and logical evolution of their stance during the conversation or maintain their stance consistently;
\textsc{(v.)} \textit{addressee correctness}, i.e., whether the assigned addressees align with the conversation context and are logically appropriate; 
\textsc{(vi.)} \textit{addressee preciseness}, i.e., whether addressees are precise and contextually appropriate (messages should target the smallest relevant group of individuals). 

\section{Experimental settings}
\begin{table*}[h!]
\centering
\small
\begin{tabular}{|l|cc|cc|cc|cc|}
\hline
\textbf{Model}     & \multicolumn{2}{c|}{\textbf{Llama3.1}} & \multicolumn{2}{c|}{\textbf{Qwen2.5}} & \multicolumn{2}{c|}{\textbf{Ministral}} & \multicolumn{2}{c|}{\textbf{OLMo2}}  \\ 
     \hline
\textit{Generation strategy}&  \textit{OL} &  \textit{TT} &  \textit{OL}   & \textit{TT} & \textit{OL}  &  \textit{TT} & \textit{OL} & \textit{TT} \\ \hline
Output Format     & $81\,026$       & $99\,524$       & $93\,136$       & $\textbf{102\,164}$      & $16\,045$       & $35\,918$        & $440$         & $93\,529$ \\  
Interactions & $80\,963$      & $95\,916$       & $93\,082$       & $\textbf{102\,110}$     & 16\,020       & $13\,524$       & $437$         & $72\,660$    \\
Number of Messages & $80\,978$     & $72\,072$       & $93\,016$       & $\textbf{102\,160}$     & $15\,971$       & $13\,444$        & $439$         & $73\,546$ \\
Number of Speakers   & $30\,332$       & $99\,524$       & $40\,201$      & $\textbf{102\,156}$      & $10\,490$       & $13\,381$        & $217$         & $73\,747$ \\
Stance of the Speakers  & $20\,167$       & $\textbf{99\,328}$       & $23\,550$       & $86\,214$       & $4\,538$        & $1\,067$         & $92$          & $63\,727$               \\
Contribution  & $74\,760$      & $\textbf{97\,763}$       & $87\,006$       & $92\,777$       & $15\,936$       & $18\,674$        & $168$         & $30\,860$               \\ \hline
\rowcolor{Gray!35} All Constraints      & $15\,550$       & $68\,246$       & $20\,853$      & $\textbf{79\,740}$       & $4\,452$        & 897          & $46$          & $19\,893$               \\ \hline
\end{tabular}
\caption{Number of generated MPCs that are compliant with each constraint, out of the full set $102\,600$ generations for each LLM and strategy (i.e. OL = One-Long generation, TT = Turn-by-Turn generation). The final amount of MPCs (last row) is the amount of generations that satisfy all the constraints.}
\label{tab:constraints_satisf}
\end{table*}
To generate synthetic MPCs, we compare four different instruction-based models, chosen for their comparable parameter sizes and compatibility with the same prompt design. The models include  Llama3.1-8B-Instruct \cite{dubey2024llama3herdmodels}, Qwen2.5-7B-Instruct \cite{yang2024qwen2technicalreport}, Ministral-8B-Instruct\footnote{\url{https://mistral.ai/news/ministraux/}}, and OLMo-2-7B-Instruct \cite{olmo20242olmo2furious}.
For each generation strategy (One-Long or Turn-by-Turn, see Section \ref{subsec:modalities_pres}) we develop three distinct system prompts combining a more or less schematic task description and different examples of the output format. For details we refer to Appendix \ref{appendix:system_prompts}.
For each combination of constraints, topic and system prompt, we generate $75$ conversations to account for the potential variety of structures. As hyperparameters, we use temperature $0.7$, mixed top p and top k decoding with $p= 0.9$ and $k=40$. In total we obtaine $102\,600$ synthetic MPCs for each model and generation strategy. 


\section{Evaluation Results}

We evaluate the generated MPCs for each dimension included in the evaluation framework (Section  \ref{sec:analysis}).

\subsection{Evaluation of Compliance with Constraints}\label{subsec:result_constr}

We first address \textbf{RQ(1)}, aimed at assessing whether synthetic MPCs can comply with the predefined constraints described in Section \ref{subsec:filter}. The results of the analysis are reported in Table \ref{tab:constraints_satisf}. We compare the output generated by the four different LLMs, each following two strategies for generation (i.e. One-Long vs. Turn-by-Turn). We report the number of generated MPCs, out of the $102\,600$ in the initial set, that were generated in compliance with the given constraint. 

This first evaluation shows that Qwen2.5 is the best model to comply with the constraints, followed by Llama3.1. Indeed, focusing on the best generation strategy, 77\% of the MPCs generated by the former comply with all constraints, while for Llama 3.1 this percentage drops to 66\%. Ministral and OLMo2, instead, fail to satisfy all constraints in the vast majority of generated conversations. Concerning the generation strategy, TT generation is overall better at complying with almost all the constraints.

The constraints where most settings encountered significant challenges were the \textit{Number of Speakers} and  \textit{Stance of the Speakers}. However, TT seems to be able to mitigate these issues for LLMs except Ministral. 
Based on these findings, in the remainder of this work we will focus on Llama3.1 and Qwen2.5 and perform all analyses on the subset of MPCs that satisfy all constraints.


\subsection{Results of Language Variability}

Table \ref{tab:content_analysis} summarizes the results on language variability (as described in Section \ref{subsec:content_analysis}). 

\begin{table}[t]
    \centering
    \small
    \begin{tabular}{|l|cc|cc|}
    \hline
  \textbf{Model} & \multicolumn{2}{c|}{\textbf{Llama3.1}} & \multicolumn{2}{c|}{\textbf{Qwen2.5}}  \\ \hline
  \textit{Gener. Strategy}  & \textit{OL}      & \textit{TT}       & \textit{OL}    & \textit{TT} \\ \hline
          Avg. \# words & $11.94$ & $26.58$ & $9.67$ & $14.15$\\\hline

        RepetitionRate ($\downarrow$) & $18.08$ & $11.07$ & $14.43$ & $13.35$\\
        StringSimilarity ($\downarrow$) & $65.51$ & $53.88$ & $63.22$ & $58.38$\\
        SemanticCoherence ($\uparrow$) & $0.496$ & $0.565$ & $0.475$ & $0.529$ \\
        \hline
    \end{tabular}
    \caption{Results of language variability analysis.}
    \label{tab:content_analysis}
\end{table}

The analysis shows that linguistic variability at surface level is lower when MPCs are generated in a single pass (OL generation) and also semantic coherence is lower compared to Turn-by-Turn generation (TT) for both models, i.e., Llama3.1 and Qwen2.5. This is probably due to the fact that in TT settings, the LLM is explicitly required to generate a turn by taking into account what immediately precedes it, building a coherent conversation step by step. 
Llama3.1 generates less repetitive MPCs at surface level, despite their turns being on average longer than Qwen2.5's. Also semantic coherence is better for Llama3.1 in all settings. 

\begin{figure*}
    \centering
    \includegraphics[width=0.9\linewidth]{latex/images/global_stats3.pdf}
    \caption{Empirical Cumulative Density Function (ECDF) of structural analysis of synthetic MPCs from Qwen2.5-TT. All the boxes goes from $0$ to $1$ on both vertical axis (density) and horizontal axis (value of the metric).}
    \label{fig:glob_struct}
\end{figure*}

\subsection{Results of Structure Analysis}\label{subsec:global-analysis-expl}
 We report the results of the structure analysis for Qwen2.5 -- TT, i.e. the model providing the highest number of synthetic MPCs, in Figure \ref{fig:glob_struct}. Results for Qwen2.5 -- OL and for Llama3.1 exhibit similar patterns, which are detailed in the Appendix \ref{appendix:global_extended}.

Since one of our goals is to assess how synthetic MPCs compare to \textit{real} MPCs in terms of structural complexity, we perform the same structure analysis on $13\,714$ MPCs extracted from the UbuntuIRC dataset \citep{ouchi-tsuboi-2016-addressee}, a widely used corpus of conversations from an online forum about software issues and troubleshooting. This subset was extracted using the strategy in \citet{penzo-etal-2024-llms} to obtain all non-overlapping conversations with 15 messages and 4, 5, or 6 speakers, ensuring each conversation formed a single connected-component interaction. 
For each of the five network metrics introduced in Section \ref{subsec:glob}, we plot in Figure \ref{fig:glob_struct} the Empirical Cumulative Density Function (ECDF) obtained by analysing synthetic MPCs with 4, 5 or 6 speakers (i.e. nodes) and on all generated MPCs, and we compare them with ECDF for UbuntuIRC. 

For all metrics, higher values indicate more complex interactions. As shown by the median values, the UbuntuIRC dataset consistently exhibits lower values across all statistics. Compared to UbuntuIRC, speakers in our synthetic MPCs tend to interact with more participants. Also, pairs of speakers tend to have more back-and-forth dynamics and groups of speakers tend to be more interconnected.
Additionally, in our dataset, the distribution of conversations with varying numbers of participants closely mirrors the overall average, with no notable deviations. This finding holds for all the model-strategy combinations and all metrics.


\subsection{Qualitative Evaluation}\label{subsec:qualitative_res}
The last analysis focuses on the quality of the generated conversations and is conducted both manually and automatically. 
Ideally, using LLM-as-a-judge would allow us to quickly evaluate all synthetic MPCs with limited effort. However, we could not be sure about the quality of this multi-dimensional evaluation when performed automatically. 
So, we first select $96$  MPCs ($24$ per model and generation strategy) via stratified sampling balanced across topic and stance. 
We then ask an expert annotator with background in philosophy and extensive experience with linguistic annotation to evaluate for each MPC the six dimensions described in Section \ref{subsec:hum_eval} such as addressee correctness, stance consistency, etc. 
After annotating a first batch of 32 MPCs, a discussion phase took place in which doubts were clarified with the help of a second annotator and reconciliations took place if needed.
The average values assigned to each dimension on a Likert scale between $1$ (poor quality) and $5$ (perfect quality) on the 96 MPCs are reported in Table \ref{tab:human_res}. We observe that all dimensions have been evaluated positively, especially \textit{Naturalness} and \textit{Speaker's Stance Evolution}. The most challenging dimension is \textit{Addressee Preciseness}, which is the only item with an average score below $4$. Neither of the two LLMs is consistently better. Neither of the generation strategies (OL vs. TT) is superior to the other w.r.t all evaluation dimensions either. 

 

\begin{table}[]
    \centering
    \small
    \begin{tabular}{|l|cc|cc|}
    \hline
  \textbf{Model} & \multicolumn{2}{c|}{\textbf{Llama3.1}} & \multicolumn{2}{c|}{\textbf{Qwen2.5}}  \\ \hline
  \textit{Generation Strategy} & \textit{OL}      & \textit{TT}       & \textit{OL}    & \textit{TT}  \\ \hline
        Naturalness  & $\textbf{4.30}$ & $\textbf{4.30}$ & $3.92$ & $3.75$ \\
        Argumentability  & $\textbf{4.00}$ & $3.92$ & $3.67$ & $3.21$ \\
        Addressee Correctness  & $3.96$ & $4.04$ & $3.92$ & $\textbf{4.25}$ \\
        Addressee Preciseness  & $3.54$ & $\textbf{3.83}$ & $3.71$ & $3.29$ \\
        Stance Consistency  & $4.08$ & $3.63$ & $3.63$ & $\textbf{4.17}$ \\
        Stance Evolution& $4.38$ & $\textbf{4.83}$ & $4.50$ & $4.63$ \\ \hline
    \end{tabular}
    \caption{Results from expert human annotator on 96 MPCs (24 for each model-strategy combination).}
    \label{tab:human_res}
\end{table}

We complement this manual analysis with a large-scale one using LLM-as-a-judge with OpenAI's o3-mini model.\footnote{\url{https://openai.com/index/openai-o3-mini/}} We first assess whether it can be reliably used to evaluate all six dimensions above. We therefore launch LLM-as-a-judge on the same 96 MPCs which were manually evaluated and measure human-LLM agreement via Krippendorff’s alpha \cite{krippendorff1computing} and Spearman's correlation (full details in Table  \ref{tab:agreement_human-llm} in Appendix \ref{appendix:agreement}). While Spearman's correlation highlights a positive correlation between LLM and human annotator on all dimensions except for \textit{Addressee Preciseness}, Krippendorf's alpha results are less consistent. However, for both measures the two dimensions that show the highest agreement between human annotator and LLM-as-a-judge are \textit{Speaker Stance Consistency} (Krippendorf's alpha $0.78$, Spearman's correlation $0.76$ with $p < 0.001$), i.e. whether the speakers comply with the assigned stance when entering the conversation, and \textit{Speaker's Stance Evolution} (Krippendorf's alpha $0.29$, Spearman's correlation $0.25$ with $p < 0.05$), i.e. whether the speakers follow a logical and realistic evolution of their stance throughout the conversation. We therefore carry out a large-scale evaluation only for these two aspects using LLM-as-a-judge on 800 conversations (200 per model and generation strategy). Results are reported in Table \ref{tab:llm_judge_res} and, similar to the human evaluation, show that Llama3.1 and Qwen2.5 are comparable in terms of performance and that they are able to generate MPCs that present realistic evolution of speakers' stance with both generation strategies. 

 \begin{table}[h]
    \centering
    \small
    \begin{tabular}{|l|cc|cc|}
    \hline
  \textbf{Model} & \multicolumn{2}{c|}{\textbf{Llama3.1}} & \multicolumn{2}{c|}{\textbf{Qwen2.5}} \\ \hline
    \textit{Generation Strategy}  & \textit{OL}      & \textit{TT}       & \textit{OL}    & \textit{TT}  \\ \hline
        Stance Consistency  & $\textbf{4.15}$ & $3.76$ & $3.99$ & $3.64$ \\
        Stance Evolution  & $4.64$  & $4.46$ & $4.62$ & $\textbf{4.68}$\\ \hline
    \end{tabular}
    \caption{Results with LLM as a judge on 800 MPCs (200 for each model-strategy combination).}
    \label{tab:llm_judge_res}
    \vspace{-1em}
\end{table}



\section{Discussion}

The analyses from the previous sections allow us to address the three research questions from Section \ref{sec:intro}. 
With respect to \textbf{RQ(1)}, targeting the possibility to generate synthetic MPCs following predefined constraints, our evaluation shows that models with comparable parameter sizes can yield very different performances. In this respect, Qwen2.5 is by far the best performing LLM in our pool followed by Llama3.1. Indeed, Qwen2.5 is able to  generate 77\% MPCs compliant with \textit{all} the constraints given in the prompt (with Ministral this percentage drops to less than 1\%). The reason behind this difference in performance cannot be clearly identified but it is likely to depend on the quality of pretraining data. 
Looking at other dimensions, however, there is no clear winner between Qwen2.5 and Llama3.1. Although Llama generates less repetitive and semantically more coherent MPCs, our qualitative evaluation does not favor either model. 

As regards \textbf{RQ(2)}, aimed at finding the best generation strategy between OL and TT, we observe that generating MPCs in a Turn-by-Turn fashion is consistently better in terms of compliance with given constraints. This can be related to recent advancements in handling long contexts: generating shorter, multi-step outputs can be more precise and reduce errors compared to relying on a single, long-generation output. However, this advantage comes at the cost of longer computational times (in our experiments, TT took around 12 times more than OL). Using TT reduces also the repetitiveness of MPCs while generating conversations that are more semantically coherent than OL. Our qualitative evaluation, in contrast, renders TT and OL similarly viable. 

To address \textbf{RQ(3)}, concerning how we can effectively evaluate the quality of generated MPCs along different dimensions, we present a framework composed by four evaluation blocks, each targeting a specific aspect of MPCs. Beside linguistic variety, coherence and qualitative dimensions such as naturalness and stance evolution, we introduce a novel assessment of the structure of synthetic MPCs. We consider five network metrics and compute empirical cumulative density function to compare them with the same values calculated from real MPCs. We show that it is possible to steer the interaction structure in generated conversations, which paves the way to the large-scale creation of high-quality MPCs with much more complex interactions than what social media datasets offer.   

\section{Conclusion}
MPCs are widely studied using social media data because of its abundance and accessibility. Due to platform constraints and inherently asynchronous communication, however, such datasets poorly reflect the structural diversity of natural MPCs. 
In this work, we investigated the viability of generating varied MPCs with LLMs, showing that (some) LLMs can indeed generate MPCs that conform to structural constraints (e.g., number of speakers and their stances). Models such as Llama3.1 and Qwen2.5 can yield high-quality MPCs under varied constraints, both when prompted to \textsc{(i.)} generate the whole MPC at once or \textsc{(ii.)} one turn at a time, given all preceding turns in context. 
This makes LLMs suitable for synthesizing large-scale datasets for various types of conversations, addressing the diversity of real-world MPCs. Synthesized data can then be further leveraged to fine-tune smaller models for various discriminative tasks (e.g., next speaker or addressee prediction). Our future efforts will exactly focus on synthesizing use-case-specific MPCs and evaluating their utility when used as fine-tuning data for smaller discriminative models. 

\section*{Limitations}
Our work presents some limitations. First of all, we focus only on English, and the topics we select are typical of US-centric  polarized debates such as universal healthcare, right to abortion and death penalty. It is possible that precisely because of these divisive topics, speakers in generated MPCs were able to discuss in a consistent way w.r.t. the assigned stance. In the future, it would be interesting to extend our analysis also to topics on which speakers can have more nuanced views, that are probably more challenging for LLMs to imitate. 

In this first set of experiments, we generated MPCs with 4, 5 or 6 speakers, and with a maximum length of 15 turns. It may be worth investigating whether looser constraints, allowing more or less speakers, or longer and shorter conversations, can lead to the creation of more `natural' MPCs and whether the evaluation results would still hold. 


\section*{Ethical Statement}
One of the main reasons behind research on synthetic data is the need to comply with privacy concerns, especially when working with conversations extracted from social media. We share the same concern and we argue that generated MPCs could alleviate ethical issues related to sharing personal information online. This applies in particular to conversations about sensitive topics such as abortion or death penalty, like the ones that we generate in our experiments. Still, we acknowledge that the problem is not fully solved since basically all best performing LLMs are currently trained on social media data, and synthetic MPCs could include personal data as well \cite{li2024llm}. Also, the creation of synthetic MPCs is not exempt from possible negative impact, for instance when used for training malicious agents in social conversation scenarios. This is a drawback   applicable to the whole research field of synthetic corpora generation for conversational AI and we advocate for more discussions on such risks within the NLP community.

\section*{Acknowledgments}

We thank Sebastiano Vecellio Salto for his contribution to the  evaluation activities. 

The work of BL was partially supported by the NextGenerationEU Horizon Europe Programme, grant number 101120237 - ELIAS and grant number 101120763 - TANGO. 
BL and ST were also supported by the PNRR project FAIR - Future AI Research (PE00000013). NP’s activities are part of the network of excellence of the European Laboratory for Learning and Intelligent Systems (ELLIS). The work is a result of his research visit at the Chair for Natural Language Processing, Center For Artificial Intelligence and Data Science, University of W\"{u}rzburg, Germany, under the supervision of GG. The visiting has been partially funded by the Erasmus+ Traineeship programme.

\bibliography{anthology, latex/custom}

\appendix

\section{System Prompts}\label{appendix:system_prompts}

We report in Figure \ref{fig:ol_system} and Figure \ref{fig:ol_output} respectively the two versions of the \texttt{Task Description} and the two versions of the \texttt{Output Format} (same structure, just different example) for the One-Long (OL) generation strategy. In Figure \ref{fig:tt_system} and Figure \ref{fig:tt_output} we report the same for the Turn-by-Turn (TT) generation strategy. 

For both strategies, the three System Prompts are obtained by concatenating: \textsc{(i.)} \texttt{Task Description 1} + \texttt{Output Format 1}; \textsc{(ii.)} \texttt{Task Description 2} + \texttt{Output Format 1};  \textsc{(iii.)} \texttt{Task Description 1} + \texttt{Output Format 2}. 

Finally, in Figure \ref{fig:conv_example} we report a synthetic Multi-Party-Conversation according to the guidelines (the \texttt{topic} field has been added in post-processing for context). In the System Prompt, the  assignment of stance among the speakers has $6$ possible  distributions (pro-topic vs counter-topic): $2$ vs $2$, $3$ vs $2$, $2$ vs $3$, $2$ vs $4$, $3$ vs $3$, $4$ vs $2$.

\begin{figure*}
    \centering
    \includegraphics[width=0.7\linewidth]{latex/images/ol_system.pdf}
    \caption{The two versions of \texttt{Task Description} for the One-Long generation strategy}
    \label{fig:ol_system}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=0.7\linewidth]{latex/images/ol_output.pdf}
    \caption{The two examples of \texttt{Output Format} for the One-Long generation strategy}
    \label{fig:ol_output}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=0.7\linewidth]{latex/images/tt_system.pdf}
    \caption{The two versions of \texttt{Task Description} for the Turn-by-Turn generation strategy}
    \label{fig:tt_system}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=0.7\linewidth]{latex/images/tt_output.pdf}
    \caption{The two examples of \texttt{Output Format} for the Turn-by-Turn generation strategy}
    \label{fig:tt_output}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=0.9\linewidth]{latex/images/conv_example.pdf}
    \caption{Example of Synthetic Multi-Party Conversation}
    \label{fig:conv_example}
\end{figure*}

\section{Further Analysis}\label{appendix:further_analysis}

\subsection{List of Topics} \label{app:topic_to_discuss}

The topics on which LLMs were prompted to generate MPCs (see Section \ref{subsec:topic_to_discuss}) are reported in Table \ref{tab:progressist_conservatist}. They consist in $76$ paired statements based on $38$ topics, in which each statement reflects a more conservative or progressive point of view with respect to the given topic.

The $38$ topics were manually selected from the ones provided by \citet{li-etal-2024-llms-speak} (freely available in the related Github repository).\footnote{\url{https://github.com/tianyi-lab/DEBATunE}}
Specifically, we picked the most polarizing topics, in order to foster more clear-cut stances during the generation of MPCs. 

\subsection{General Statistics of Final Set of Synthetic MPCs}
In Figure \ref{fig:general-stats} we report the general statistics of the synthetic MPCs that satisfy the constraints according to Section \ref{subsec:result_constr}. We report for each model-strategy combination: \textsc{(i.)} the average number of addressees per turn; \textsc{(ii.)} the number of users; \textsc{(iii.)} the stance assignment; \textsc{(iv.)} the number of turns.

Most of the combinations satisfy the strict requirements of exactly $15$ messages. Only Llama3.1-TT tends to generate a lot of examples of shorter conversations. 


\subsection{Extended Results from Global s
Structure Analysis}\label{appendix:global_extended}
In Figure \ref{fig:all-global} we report all the plots related to the $5$ structural metrics presented in Section \ref{subsec:glob}, for all the model-strategy combinations (where the models are Llama3.1 and Qwen2.5). All the plots show similar shapes, and all the combinations present a more complex structure than MPCs in the UbuntuIRC dataset \cite{ouchi-tsuboi-2016-addressee}.


\begin{table*}[h]
\centering
\small
\begin{tabular}{|p{7cm}|p{7cm}|}
\hline
\textbf{Progressive} & \textbf{Conservative} \\ \hline
ban of targeted killing & allowance of targeted killing \\ \hline
ban of the death penalty & allowance of the death penalty \\ \hline
recognition of the right to abortion & ban of abortion \\ \hline
recognition of the right to euthanasia & ban of euthanasia \\ \hline
recognition of Palestinian state & non-recognition of Palestinian state \\ \hline
ban of mandatory military service & mandatory military service \\ \hline
ban of nuclear weapons & support for nuclear weapons \\ \hline
mandatory sex education in schools & optional sex education in schools \\ \hline
guarantee of online teaching & mandatory in-person teaching \\ \hline
fight to climate change & opposition to regulations for action on climate change \\ \hline
incentives for renewable energy & incentives for energy from fossil fuels \\ \hline
ban of facial recognition technology & incentives for facial recognition technology \\ \hline
incentives for AI research & opposition to AI research incentives \\ \hline
mandatory vaccination for children & optional vaccination for children \\ \hline
ban of animal testing & allowance of animal testing \\ \hline
incentives for organ donation & opposition to organ donation incentives \\ \hline
ban of racial profiling & allowance of racial profiling \\ \hline
incentives for immigration and asylum & support to immigration contrast and stricter asylum rules \\ \hline
universal healthcare & support to private healthcare \\ \hline
legalization of marijuana & ban of marijuana \\ \hline
legalization of same-sex marriage & ban of same-sex marriage \\ \hline
legalization of surrogate motherhood & ban of surrogate motherhood \\ \hline
programme for the reduction of the gender pay gap & increase of the gender pay gap in favor of men \\ \hline
limitation to gun ownership & right to unrestricted gun ownership \\ \hline
holocaust remembrance mandatory in schools & optional holocaust remembrance in schools \\ \hline
ban of zoos & support for zoos \\ \hline
protection of endangered species & opposition to endangered species protection \\ \hline
organization of pride parades & ban of pride parades \\ \hline
allowance of tattoos & ban of tattoos \\ \hline
cohabitation of couples before marriage & mandatory marriage before cohabitation \\ \hline
ban of arranged marriages & right to arranged marriages \\ \hline
US staying in NATO & US leaving NATO \\ \hline
Germany staying in EU & Germany leaving the EU \\ \hline
mandatory acceptance of mobile payments & ban of mobile payments \\ \hline
lowering university tuition fees & increase in university tuition fees \\ \hline
mandatory cameras on police officers & freedom of police officers to refuse cameras \\ \hline
freedom of blasphemy & punishment for blasphemy \\ \hline
legalization of adoption by same-sex couples & ban of adoption by same-sex couples \\ \hline
\end{tabular}
\caption{List of topics, paired according to their Progressive and Conservative version.}
\label{tab:progressist_conservatist}
\end{table*}


\begin{figure*}
    \centering
    \includegraphics[width=0.95\linewidth]{latex/images/general_statistics3.pdf}
    \caption{General statistics of the resulting MPC for Llama3.1 and Qwen2.5 on both generation strategies. The statistics reported are (from the top): \textsc{(i.)} average number of addressees per turn, \textsc{(ii.)} number of users, \textsc{(iii.)} stance assignment, \textsc{(iv.)} number of turns.}
    \label{fig:general-stats}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=0.95\linewidth]{latex/images/all_global_structures3.pdf}
    \caption{Empirical Cumulative Density Function (ECDF) of structural analysis on the synthetic MPCs from Llama3.1 and Qwen2.5, with both generation strategies, i.e. One-Long and Turn-by-Turn generation. The statistics reported are (top to bottom): \textsc{(i.)} Average Degree Centrality, \textsc{(ii.)} Average Out-Going Degree, \textsc{(iii.)} Transitivity, \textsc{(iv.)} Reciprocity, \textsc{(v.)} Consistent Reciprocity. Average Degree Centrality and Average Out-Going Degree are normalized. In this way, all values on the vertical axis (density) and on the horizontal axis (value of the metric) are included between $0$ and $1$.}
    \label{fig:all-global}
\end{figure*}


\section{Technical Report}

All the experiments have been run on Ampere A40 GPUs, which present 48GB of VRAM. We used the vLLM library \cite{kwon2023efficient} for speeding up the inference time. In Table \ref{tab:models} we report the links to the models  and the repositories we used.

In order to compute the structural metrics, we used the tools from the \texttt{NetworkX}\footnote{\url{https://networkx.org/documentation/stable/}} library \cite{osti_960616}. Instead, for computing the Krippendorf-alpha we used the implementation from \citet{castro-2017-fast-krippendorff} for interval data. For the Spearman correlation, we used the SciPy library \cite{2020SciPy-NMeth}. For the human evaluation, we used Argilla\footnote{\url{https://github.com/argilla-io/argilla/}} for creating the annotation platform and we used the User Interface provided on HuggingFace spaces.\footnote{\url{https://huggingface.co/argilla}} For the LLM-as-a-judge evaluation, we employed the OpenAI API for reasoning models.\footnote{\url{https://platform.openai.com/docs/guides/text-generation}} We also used the official guide to perform prompt refinement.\footnote{\url{https://platform.openai.com/docs/guides/prompt-generation}} Our results can be rerun by paying less than $\$20.00$. We used both ChatGPT\footnote{\url{https://openai.com/index/chatgpt/}} and Copilot\footnote{\url{https://github.com/features/copilot}} for help in the coding process.


\begin{table*}[]
    \centering
    \begin{tabular}{|l|r|}
        \hline
        Model & Repository \\
        \hline
        Llama-3.1-8B & \url{https://huggingface.co/meta-llama/Llama-3.1-8B} \\
        Qwen2.5-7B-Instruct & \url{https://huggingface.co/Qwen/Qwen2.5-7B-Instruct}\\
        Ministral-8B-Instruct & \url{https://huggingface.co/mistralai/Ministral-8B-Instruct-2410} \\
        OLMo2-7B-Instruct & \url{https://huggingface.co/allenai/OLMo-2-1124-7B-Instruct}\\
        \hline
    \end{tabular}

        \begin{tabular}{|l|r|}
        \hline
        Model & Context length \\
        \hline
        Llama-3.1-8B & $128$k \\
        Qwen2.5-7B-Instruct & $131$k\\
        Ministral-8B-Instruct & $131$k \\
        OLMo2-7B-Instruct & $4$k\\
        \hline
    \end{tabular}
    
    \caption{Upper - repositories of the model used for generating the synthetic MPCs. Lower - context length of each model.}
    \label{tab:models}
\end{table*}

\section{Human and LLM-as-a-judge Agreement} \label{appendix:agreement}

As reported in Section \ref{subsec:qualitative_res}, we compute the Inter-Annotator Agreement on a batch of $96$ MPCs with Krippendorf's alpha and  Spearman correlation between the human expert annotator and the LLM (results in Table \ref{tab:agreement_human-llm}). For the two scores with higher Krippendorf's alpha, i.e., \textit{Speaker Stance Consistency and Speaker Stance Evolution}, we employed the LLM-as-a-judge on large scale. 


\begin{table}[]
    \centering
    \small
    \begin{tabular}{|l|c|c|}
    \hline
  \textbf{Coefficients} & Krippendorf & Spearman \\ \hline
        Naturalness  & $-0.24$ & $0.05$\\
        Argumentability  & $0.14$ & $0.22^*$\\
        Addressee Correctness  & $0.18$ & $0.30^*$\\
        Addressee Preciseness  &  $-0.08$ & $-0.12$\\ \hline
        Stance Consistency  & $0.78$ & $0.76^{**}$\\
        Stance Evolution  &  $0.29$ & $0.25^*$ \\ \hline
    \end{tabular}
    \caption{LLM as a judge -- human expert agreement/correlation. For  Spearman's correlation, (*) highlights that the correlation is statistically significant ($p < 0.05$). Instead, (**) corresponds to a correlation that is highly statistically significant ($p < 0.001$).}
    \label{tab:agreement_human-llm}
\end{table}

\section{Guidelines for Human Evaluation}

For human annotation, we designed a careful documentation to be used as  guidelines. We report in Figure \ref{fig:human_eval_1} the introduction, in Figure \ref{fig:human_eval_2} the Platform Description (with a screenshot of the view), in Figure \ref{fig:human_eval_3} the Scores Description.
One annotator performed his task as part of an internship, while the second annotator is regularly employed at the authors' institution. The effort required, on average, $2.5$ hours for a batch of $32$ items.

\begin{figure*}
    \centering
    \includegraphics[width=0.9\linewidth]{latex/images/human_eval_1.pdf}
    \caption{Overview of guidelines for human evaluation}
    \label{fig:human_eval_1}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=0.9\linewidth]{latex/images/human_eval_2.pdf}
    \caption{Platform description from human evaluation guidelines}
    \label{fig:human_eval_2}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=0.9\linewidth]{latex/images/human_eval_3.pdf}
    \caption{Description of the scores from the human evaluation guidelines}
    \label{fig:human_eval_3}
\end{figure*}


\end{document}
