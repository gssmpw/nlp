
\documentclass[manuscript,screen,nonacm]{acmart}

\usepackage{comment}
%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
%\setcopyright{acmlicensed}
%\copyrightyear{2025}
%\acmYear{2025}
%\acmDOI{XXXXXXX.XXXXXXX}

%% These commands are for a PROCEEDINGS abstract or paper.
%\acmConference[Conference acronym 'XX]{Make sure to enter the correct conference title from your rights confirmation emai}{June 03--05,2018}{Woodstock, NY}
%%
%%  Uncomment \acmBooktitle if the title of the proceedings is different
%%  from ``Proceedings of ...''!
%%
%%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%%  June 03--05, 2018, Woodstock, NY}
\acmISBN{978-1-4503-XXXX-X/18/06}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}


%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Mapping the Regulatory Learning Space for the EU AI Act}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.

\author{Dave Lewis}
\authornotemark[1]
\email{dave.lewis@adaptcentre.ie}
\affiliation{%
  \institution{ADAPT Centre at Trinity College Dublin}
  \country{Ireland}
}

\author{Marta Lasek-Markey}
\affiliation{%
  \institution{ADAPT Centre at Trinity College Dublin}
  \country{Ireland}
}
\email{mlasek@tcd.ie}

\author{Delaram Golpayegani}
\affiliation{%
  \institution{ADAPT Centre at Trinity College Dublin}
  \country{Ireland}
}
\email{delaram.golpayegani@adaptcentre.ie}


\author{Harshvardhan J. Pandit}
\affiliation{%
  \institution{ADAPT Centre at Dublin City University}
  \country{Ireland} }
\email{harshvardhan.pandit@adaptcentre.ie}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Lewis et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
The EU's AI Act represents the world first transnational AI regulation with concrete enforcement measures. It builds upon existing EU mechanisms for product health and safety regulation, but extends it to protect fundamental rights and by addressing AI as a horizontal technology that is regulated across multiple vertical application sectors. These extensions introduce uncertainties in terms of how the technical state of the art will be applied to AI system certification and enforcement actions, how horizontal technical measures will map into vertical enforcement responsibilities and the degree to which different fundamental rights can be protected via product regulation measures that must be applied with consistency across EU Member States. We argue that these uncertainties, coupled with the fast changing nature of AI technology and the relative immaturity of the state of the art in fundamental rights risk management, when compared to health and safety concerns in product regulation, require the implementation of the AI Act to place a strong emphasis on comprehensive and rapid \textit{regulatory learning}. We define parameterised axes for the regulatory learning space set out in the Act and describe a layered system of different learning arenas where the population of oversight authorities, value chain participants and affected stakeholders may interact to apply and learn from technical, organisational and legal implementation measures. We conclude by exploring how existing open data policies and practices in the EU can be adapted to support rapid and effective regulatory learning in a transparent manner that supports the development of trust in and predictability of regulated AI. We also discuss how the Act may result in a regulatory turn in the research of AI fairness, accountability and transparency towards investigations into implementations of and interactions between different fundamental rights protections and reproducible and accountable models of metrology for AI risk assessment and treatment.    
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%

\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10010147.10010178.10010187</concept_id>
       <concept_desc>Computing methodologies~Knowledge representation and reasoning</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10003456.10003462.10003588.10003589</concept_id>
       <concept_desc>Social and professional topics~Governmental regulations</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10002951.10003260.10003309.10003315.10003314</concept_id>
       <concept_desc>Information systems~Resource Description Framework (RDF)</concept_desc>
       <concept_significance>300</concept_significance>
       </concept>
   <concept>
       <concept_id>10011007.10010940.10011003.10010117</concept_id>
       <concept_desc>Software and its engineering~Interoperability</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10010147.10010178</concept_id>
       <concept_desc>Computing methodologies~Artificial intelligence</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10010405.10010455.10010458</concept_id>
       <concept_desc>Applied computing~Law</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10002951.10002952</concept_id>
       <concept_desc>Information systems~Data management systems</concept_desc>
       <concept_significance>300</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computing methodologies~Knowledge representation and reasoning}
\ccsdesc[500]{Social and professional topics~Governmental regulations}
\ccsdesc[300]{Information systems~Resource Description Framework (RDF)}
\ccsdesc[500]{Software and its engineering~Interoperability}
\ccsdesc[500]{Computing methodologies~Artificial intelligence}
\ccsdesc[500]{Applied computing~Law}
\ccsdesc[300]{Information systems~Data management systems} 

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{EU AI Act, regulatory learning, trustworthy AI, AI risk management, fundamental rights impact assessment, AI sandboxes}

%\received{20 February 2007}
%\received[revised]{12 March 2009}
%\received[accepted]{5 June 2009}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}
Research into fairness, accountability and transparency of algorithms has focussed on normalised approaches to achieving ethical or trustworthy AI systems in response to societal concerns of the risks AI systems pose. Studies across the rapidly growing body of work that offer guidelines on ethical AI to developers and users of such systems demonstrate a seeming degree of consensus on key principles, including  human rights and agency, privacy, non-discrimination, safety and robustness, transparency and accountability~\cite{hagendorff_ethics_2020, jobin_global_2019,fjeld_principled_2020, correa_worldwide_2023}. Other studies have indicated, however, that this consensus in discourse breaks down as attempts are made to turn common principles into workable tools. This is seen in the tendency toward techno-centric solutions, such as explainable AI or differential privacy that may confound calls for more direct inspection of algorithmic and data processing design decisions, the distance between decision-making on technical measures for ethical AI from that its governance and the lack of involvement by societal actors in technical decision-making \cite{palladino_biased_2023}. A body of analysis has emerged that both posits efforts of ``ethics-washing'' as a distraction from harmful behaviour and raises criticisms of attempts to integrate ethical reasoning into AI governance \cite{schultz_digital_2024}. At the same time, the pace of advancement in capabilities of AI technology, which has rapidly accelerated especially in relation to Large Language Models (LLM) and Generative AI (GenAI), has injected uncertainty about the risks they present \cite{grace_thousands_2024}.  

In considering legal grounding for AI ethics, models of co-regulation—where responsibility for delivering ethical AI between industry, users and government needs to ground the legal definition of the ethics principles in democratic legitimacy—compliance should be achieved in a stable manner and the regulatory process should be implemented efficiently \cite{ferretti_institutionalist_2022}. In April 2021, the EU became the first jurisdiction to propose a comprehensive AI regulation~\cite{mazzini2023proposal}. This took a risk-based approach modelled based on the existing EU scheme for harmonising product health and safety regulation across Member States participating in the EU internal market, known as the New Legislative Framework (NLF), and therefore benefitted from established rules on regulatory framework for regulatory transparency and accountability, laid out in the so-called ``Blue Guide''~\cite{eu-nlf}. This grounding in the NLF was retained through the legislation's progress through the European Parliament and its trilogue negotiation with the European Council (representing Member States) and the European Commission (EC). Additionally, the AI Act also has a fundamental rights underpinning whereby references to ethical principles have been supplemented with, or in certain instances replaced by, references to the protection of fundamental rights in the context of AI. However, this process also presented an eloquent demonstration of the pacing problem when provisions were added to the initial draft to handle the way that the parallel advances in LLM and GenAI potentially enable the most powerful classes of ``General Purpose AI'' (GPAI) to evade the mechanism of the NLF which are predicated on specific uses. With the AI Act coming into law in August 2024 \cite{noauthor_regulation_2024}, market actors, regulators and civil society are presented with a concrete timetable within which the many remaining uncertainties in realising its provisions present a substantial challenge \cite{kaizenner_ai_2024}. For example, the two year grace period in advance of the EU's General Data Protection Regulation (GDPR) came into force in 2018 benefitted from experience garnered from the similar provisions of the Data Protection Directive, which though subject to a less strict enforcement regime, did include largely the same provisions, e.g. data minimisation and informed consent as a legal basis for processing personal data. Thus there was a basis in terms of known technical measures, tools and competencies that could be drawn upon. 

While implementation of the AI Act will benefit from experience, technical measures, tools and competencies established under the different regulations under the NLF, in some important respects, it faces the some serious challenges, discussed in the following. Firstly, the AI Act attempt to regulate AI as a horizontal technology using an enforcement mechanisms that is conducted on a sectoral basis under the NLF, e.g. under specific product safety regulations such as the Medical Device Regulation (MDR). Secondly, the AI Act expands the scope seen in current regulation harmonised under the NFL from protection of health and safety to include protection of fundamental rights. In this paper, we explore these uncertainties introduced by these novel aspects of AI Act and argue that given the pacing problem in establishing technical measures for ethical AI, the Act should not be viewed at this point as a complete framework for the required transparency and accountability, but as a means for learning how to populate such a framework with appropriate technical, organisational and regulatory measures. Therefore, while it may seem premature to introduce regulation when the required technical measures and ethical frameworks are relatively immature, one can argue that the real benefit of the AI Act is that it delivers a concrete motivation to rapidly invest in and build consensus on these unknowns, while acknowledging they will be subject to ongoing change as the technology and the risk it poses rapidly advance. 

Considering the AI Act as a learning framework in this way therefore may portend a ``regulatory turn'' in AI ethics research and innovation, where questions focus on the efficient operation of the Act and its effectiveness in protecting fundamental rights. Such a regulatory turn may have import far beyond the EU, as it has extra-territorial effect on AI value chain actors aiming to put AI products or components onto the EU Internal Market, because fundamental rights serve as the basis for ethical protection in many jurisdictions and existing international treaties and because the democratic legitimacy and transparency of its enforcement may prove attractive to those buying or subject to AI systems outside of the EU in comparison to other normative frameworks for AI \cite{lewis_global_2020}. In this paper, we therefore build on our analysis of the main uncertainties arising from the AI Act by defining a multi-axis learning space and a layered categorisation of learning arenas that could be used to structure and maximise the shared efficiency of learning under the current complex structures defined in the Act. We conclude with some concrete recommendation of how to improve information flow between actors in different learning arena with the aim of accelerating shared learning to better respond to the pace of advances in AI.


\section{AI Act and Standards}
The EU AI Act as adopted into law takes a risk-based approach with technical requirements that fall primarily on AI providers that aim to place an AI system or general purpose AI model on the market or into service in the EU in specified application area and organisations that deploy such AI offerings, i.e. AI Deployers. The requirements are structured through the ex-ante application of quality management (Article 17) and risk management techniques (Article 9) and involve technical measures for data governance and data protection (Article 10), technical documentation and record keeping (Articles 11 and 12), transparency (Article 13) and human oversight (Article 14) in AI system operation, and implementation of accuracy, robustness and cybersecurity measures (Article 15). Though these technical measures are similar to those shared between existing EC guidelines on trustworthy AI and those proposed by technical companies, their integration into the legally binding framework of the Act frames concrete questions on whether these technical measures will adequately deliver the Act's goals of a well-function internal market in AI-based good and services with high levels of protections for health, safety and fundamental rights (Recital 1) \cite{evers_talking_2024}. Adopting the well-established pattern from the NLF, the legal enforcement of technical measures combines ex-ante certification of AI-based products classified as ``high-risk'' prior to release, with ex-post monitoring and surveillance of such products once released and in use. The process of certification required depends of the class of the high-risk AI system in question, of which the Act defines two sets. The first related to the use of AI in products that are already subject to harmonised product regulation and listed in Annex I of the Act, which would receive certification on conformance to the Act via the certification mechanisms established therein. The second relates to the use of AI in applications not subject to existing product regulation but are deemed to represent risks needing oversight and listed in Annex III of the Act, for which self-certification of compliance will suffice, apart from the area of remote biometric identification (Annex III, point 1) for which a new certification mechanism will be established. The ex-post enforcement of requirements on high-risk AI systems involves planned monitoring of deployed systems by providers, adherence of deployers to the instructions of use provided with the system and surveillance of these by a network of Market Surveillance Authorities (MSA). If an MSA have reason to believe that a high-risk AI system placed on the market does not comply with the Act's requirements, it has the power to obtain detailed technical documentation and resources from the provider, up to and including copies of the system code in order to make determination of breaches of requirements. Depending on the severity of any assessed breaches, an MSA may require corrective measure to be made to the system or its instructions for use or that the system is removed from the market and may also impose financial penalties on the provider or deployer. 

The ability to unambiguously verify that a high-risk AI system employs appropriate technical measures to mitigate risks to health, safety and fundamental rights relies on investigation of documentation.  Technical documentation, testing and instructions for use are therefore central to the consistent and effective implementation of the Act, and doing so efficiently is vital to minimising the regulatory burden of complying with the Act. However, the Act is limited to essential high-level requirements for technical measures and how they should be undertaken stated in horizontal terms independent of sectoral context. Instead, in common with some other regulations under the NLF, the Act allows compliance with requirements to be demonstrated by provider in terms of a presumption of conformity gained by adherence to separate technical standards. These technical standards can be either Harmonised Standards developed by European Standards Organisations (ESOs) or if that is not possible or insufficient then via Common Specification to be developed by the EC \cite{tartaro2023towards}. The EC has already issued a request for harmonised standards in 10 areas to the ESOs, addressing different aspects of the required technical measures \cite{standardisationrequest}. Response to these requests are being actively addressed by a Joint Technical Committee of the ESO's CEN and CENELEC (JTC21), which is charter to develop European Standards for AI. This approach to developing more unambiguous specifications of technical measures—  than currently offered by the text of the Act—presents benefits in that ESO can leverage expertise of industry practitioners and current technical knowledge as well as draw upon existing international standards so that resulting specification are technically correct, reflective or recent state of the art and more likely to be readily implementable by industry actors. However, this presents several additional uncertainties in the implementation of the Act:

\begin{itemize}
  \item \textit{Lack of representation in standards development:} Standards development is a meticulous, lengthy and highly technical process so the bulk of participation comes from large multinational companies that sustain teams of suitably-experienced experts, whereas other less well-resourced experts from NGOs, SMEs or academia struggle to maintain effective level of contribution \cite{cuccuru_interest_2019}. This raises concerns that the resulting AI harmonised standards from CEN/CENELEC JTC21 may not sufficiently represent the interests of citizen and SMEs in favour of the large companies that the Act must also regulate \cite{veale_demystifying_2021}. Further, JTC21 is able to adopt standards directly from ISO/IEC, where the subcommittee on AI (SC42) has been developing AI standards since 2018, many of which align to topics in the EC's harmonised standards request, e.g. ISO/IEC 23894 on AI risk management \cite{noauthor_isoiec_nodate-6} and ISO/IEC 24029-2 on AI robustness  \cite{noauthor_isoiec_nodate-5}. However, as these are developed by experts from a global membership of national standards bodies rather than just the European-based ones, the representation of \textit{weaker} European actors may be further diluted. 
  \item \textit{Lack of free access to harmonised standards:} Standards bodies such as ISO/IEC and CEN/CENELEC rely on charging for access as means of funding to maintain their business model of convening and overseeing consensus formation and quality of standards projects. Standards needed for regulatory compliance, such as harmonised standards in the EU, are often a well-selling part of their portfolio. However, as harmonised standards essentially enter EU law once accepted by the EC, this raises the anomalous situation where these standards become part of the legal framework while they are behind a paywall. While this may not have caused much controversy for harmonised standards related to technical industrial safety, as the standards begin to address consumer safety and especially broad questions of fundamental rights protections this situation became untenable and resulted in court actions \cite{kamara_general_2022}, which progressed to the European Court of Justice delivered its judgment in Case C-588/21 P that there was an overriding public interest in free access to harmonised standards. As of writing, negotiation between the EC and CEN/CENELEC and ISO/IEC have not reached a resolution.
  \item \textit{Developing standards with reference to legal requirements and ethical norms:} ISO/IEC standards are prohibited from referencing compliance to any national or regional legislation or value sets to avoid introducing barriers to trade in these specification. CEN/CENELEC also does no allow specific legal provision entering its standards, and alignment between clauses of standards and regulation requirement is only introduced through a specific form an annex. In addition to these provisions, there are broader arguments of lack of democratic legitimacy in standards development that generally mean that standards bodies limit themselves to non-normative technical reports when addressing ethical issues \cite{noauthor_isoiec_nodate-1}.
  
  \item \textit{Integrating horizontal and vertical standards development:} A key benefit of basing harmonised standards on International ISO/IEC standards is the support offered for regulatory certification regimes, where conformity assessment is based on the ISO 17000 series of standards. This provides a standard way for authorities to recognise certification assessment bodies as competent to certify against a given regulation and its harmonised standards, a process known as ``notification'' (i.e. competent authorities assess certification providers and ``notify'' them if deemed qualified). Complementing this scheme is the harmonised management system standard (MSS) template defined by ISO/IEC~\cite{noauthor_iso_guide2}. This common template facilitates organisations combining adoption of MSSs from different standards committees into a unified organisational management system, e.g. ISO/IEC 27001 for information security and ISO 42001 which is the MSS developed by SC42 for AI from the harmonised management system. This however offers no guarantee that similar concepts or control measures included from different ISO/IEC standards committees are not in conflict, as they are often developed at different times and as committees addressing horizontal technologies such as AI are typically chartered separately to those addressing sectorial standards, such as ISO/TC 210 on health related products. The ideal solution is for a joint Committee to be formed between separate committees, however as there are two sets of consensus forming and voting communities involved, these are typically slower to establish and complete standards project. To date, the harmonised standards request to JTC21 has only sought horizontal harmonised standards from JTC21, so the development of harmonised standards specific to AI use in a vertical domain such as medical devices is a distant prospect.
\end{itemize}

\section{Fundamental Rights and Legal Uncertainties of the AI Act}
 This paper argues that regulatory learning in the context of the AI Act should begin, at the most general level, by adequately identifying the role of fundamental rights and addressing issues related to the so-called direct effect of EU law. This section will briefly outline the three sources of fundamental rights protection in the EU with particular focus on direct effect of the EU Charter of Fundamental Rights. It will then consider the application of the Charter to the AI Act. \par
 As EU integration progressed over the years, so did protection of fundamental rights. Consequently, within the EU legal order, there is currently no single uniform source of fundamental rights protection ~\cite{spaventa2018should}. Instead, Article 6 TEU identifies three sources thereof, in chronological order: 1) constitutional traditions shared by the Member States; 2) the European Convention for the Protection of Human Rights and Fundamental Freedoms (ECHR), which all EU Member States are parties to but the European Union is not; and 3) the EU Charter of Fundamental Rights (CFR). The latter has been gaining in importance since the Treaty of Lisbon's entry into force (2009), however, the scope of application of the Charter is limited by its so-called horizontal provisions, particularly Article 51. With Lenaerts, we note that the CFR rights are ``addressed to the institutions, bodies, offices and agencies of the Union with due regard for the principle of subsidiarity and to the Member States only when they are implementing Union law''~\cite{lenaerts2012exploring}. Therefore, while the provisions of the Charter bind the EU itself and the Member States, and this has been particularly evident in policymaking, its impact in litigation has been limited due to issues with direct effect of the Charter~\cite{frantziou2015horizontal, frantziou2019horizontal}. To be capable of direct effect, i.e. to create rights for individuals, a provision of EU law must fulfil the following three conditions laid down in the CJEU case 26/62 \textit{Van Gend en Loos}: it must be 1) clear, 2) precise and 3) unconditional and, therefore, not require any further implementing measures. \par
 In relation to CFR rights, the CJEU has to date only considered direct effect of four provisions. Consequently, Article 21 CFR (right to non-discrimination), Article 31(2) CFR (right to maximum working hours, to daily and weekly rest periods and to annual leave) and Article 47 CFR (right to an effective remedy and to a fair trail) have been deemed as having direct effect~\cite{prechal2020horizontal}. Conversely, Article 27 CFR (workers' right to information and consultation) has been denied direct effect in case C‑176/12 \textit{AMS}, where the CJEU stated, in para 45, that ``for this article to be fully effective, it must be given more specific expression in European Union or national law''. Therefore, it does require additional implementing measures. Regarding the remainder of the CFR, it is not clear which of the rights might be capable of direct effect and, thus, whether individuals might be able to successfully rely on them. \par
 Consequently, it is possible that in conflicts involving fundamental rights in the context of the AI Act, the other sources of protection, i.e. the European Convention on Human Rights which is binding on all the EU Member States, and indeed the national Constitutions of the Member States, might also become relevant. As the EU and the Council of Europe are in the process of negotiating EU's accession to the ECHR, a rebuttable presumption of equivalent protection between the two systems, laid down in Article 52(3) of the CFR, continues to apply ~\cite{de2012doctrine}. Indeed, the European Court of Human Rights (ECtHR) has delivered judgments where violations of the Convention were found, e.g. in relation to unjustified processing of biometric data (case \textit {Glukhin v Russia}, Application no. 11519/20). In this vein, the body of ECtHR case law constitutes yet another important resource for regulatory learning pertaining to fundamental rights.  
  \par

While potentially all fundamental rights, including CFR rights, might fall within the scope of application of the AI Act, the Act explicitly refers to 17 fundamental rights in Recital 48 of the Preamble, which had also been mentioned in the Proposal for the AI Act (COM(2021) 206 final). This implies that the European Commission expects these to be of particular relevance in the context of AI, even though it does not grant them any additional legal standing. These rights are: 1) the right to human dignity; 2) respect for private and family life; 3) protection of personal data; 4) freedom of expression and information; 5) freedom of assembly and of association; 6) the right to non-discrimination; 7) the right to education; 8) consumer protection; 9) workers’ rights; 10) the rights of persons with disabilities; 11) gender equality; 12) intellectual property rights; 13) the right to an effective remedy and to a fair trial; 14) the right of defence and the presumption of innocence; 15) the right to good administration; 16) children's rights; 17) environmental protection. \par
As discussed above, it is unclear which of these fundamental rights beyond the right to non-discrimination, the right to an effective remedy and to a fair trial, as well the specific workers' right to rest, might have direct effect. The lack of direct effect might constitute a barrier to effective enforcement of fundamental rights, particularly in horizontal disputes involving non-state actors, i.e. private parties~\cite{viljanen2023horizontal, fornasier2015impact}. In the context of the AI Act, disputes involving fundamental rights might potentially concern such types of deployers of high-risk AI systems as private employers, insurance companies or banks. Regarding fundamental rights that do not have direct effect, e.g. the workers' right to information and consultation, it appears that such non-state actors would not be bound by the Charter. 
\par Many of the fundamental rights enshrined in the Charter have further been concretised in secondary EU legislation, some of which also has direct effect and would, therefore, be binding on private parties. A prime example is the right to the protection of personal data guaranteed by the General Data Protection Regulation 2016/679 (GDPR), referenced multiple times across the AI Act, as well as other legislation, such as the Law Enforcement Directive 2016/680 or the ePrivacy Directive 2002/58. Other examples of secondary legislation mentioned in the AI Act which is connected to various CFR rights include Regulation 2024/900 on the transparency and targeting of political advertising; Framework Directive 2002/14 on employee information and consultation or the Platform Workers Directive 2024/2831. Additionally, the entire body of EU consumer legislation gives effect to Article 38 of the Charter (the right to consumer protection), including the Medical Devices Regulation 2017/745, the Representative Actions Directive 2020/1828 and the Unfair Commercial Practices Directive 2005/29 to name a few examples mentioned in the AI Act. Furthermore, there is other relevant EU secondary legislation which, albeit not explicitly referenced in the AI Act, provides additional layers of protection of various Charter rights. A pertinent example is the EU non-discrimination law consisting of the Framework Directive 2000/78, the Racial Equality Directive 2000/43 and the Gender Recast Directive 2006/54, where implications of AI and algorithmic management in employment are significant ~\cite{wachter2021fairness, xenidis2020eu}.   
\par
Given the existing regulatory maze, as well as the uncertainties surrounding the status of fundamental rights and the Charter in the context of the AI Act, We can therefore draw up the following list of questions that may need to be prioritised in ensuring predictable and consistent enforcement of the Act:
\begin{enumerate}
  \item Will all rights under the CFR be equally protected under the AI Act?
  \item What role will Market Surveillance Authorities (MSAs) play in enforcing the AI Act in relation to identifying whether a high-risk AI system has breached fundamental rights protections and enforcement of corrective measures?
  \item How will the enforcement powers of MSAs in enforcing rights protections (executed in concert with national bodies protecting fundamental rights as per Article 77 of the AI Act) interplay with those of notified bodies, national competent authorities?
  \item What role and weight will be placed on the Fundamental Rights Impact Assessment (FRIA) conducted by AI deployers and AI providers (Article 27) in enforcing fundamental rights protections under the Act?
\end{enumerate}

\section{Mapping a Regulatory Learning Space for the AI Act}
Regulatory learning is a means by which legally grounded technology regulation can respond and adapt to rapid advances in the capabilities and use of the technologies subject to regulation. It can feature in measures taken by the EU to implement smart regulation\cite{noauthor_communication_2010}, which aim to ensure regulation achieves its policy objective with the minimum regulatory burden. The approach aims to address the whole policy cycle, from regulation design to its implementation, enforcement, evaluation, and revision, while also ensuring responsibility for the cycle is effectively shared between the European institutions and Member States and that the voices of the citizens and stakeholders most impacted by the policy are fully heard. The AI Act includes the requirement for the Member States to establish Regulatory Sandboxes (Article 57, 58, 59), which are formal arenas where regulators and prospective regulated actors can assess and report on technical measures and their documentation with time and scope-limited constraints on regulatory enforcement to inform improvement and implementation on both sides. Previous experience with sandboxes in the financial sector also indicates that some benefits, in terms of resolving ambiguities in requirements and terminology, can be achieved through collaborative learning arenas that do not need to operate under the ambit of regulators \cite{buckley_building_2020}. The EC's Joint Research Centre has highlighted, for example, how mechanisms such as living labs and testbeds can also offer alternative arenas for regulatory learning with different degrees of technology maturity, regulatory involvement, resources models, access routes and stakeholder engagement \cite{kert_regulatory_2022}. These can also offer learning opportunities for governance mechanisms beyond appropriate technical measures \cite{engels_testing_2019}.  While the AI Act does not directly address these other forms of regulatory experimentation arenas, it does include oversight rules for testing of technical measures using human subjects (Articles 60, 61).

These trends in adoption of regulatory learning coupled with the uncertainties in the Act's use of horizontal measures in vertical enforcement and fundamental rights protections indicate that a broad and structured framework for regulatory learning is required to enable the efficient, timely and stakeholder-inclusive system for resolving uncertainties and adapting to technological advances. We propose such a framework by identifying: i)  a taxonomy of actors who will participate, ii) axes for a three-dimensional space within which regulatory learning activities can be organised and iii) a nine-layer system of grouping classes of regulatory learning arenas with similar competencies and outcomes based on the structure of provisions in the Act. The aim is to provide a structure framework for understanding and improving the efficiency of information exchange between different actors and the regulatory arenas in which they participate. 
\begin{itemize}    
  \item \textit{Value-chain actors:} These are actors that participate in a value chain where any member is subject to the provisions of the Act. This includes any operator of AI systems or models subject to the Act, defined in Article 3(8) to be providers, deployers, product manufacturers and also authorised representatives, distributors and importers of AI systems sourced in jurisdictions outside the EU. It also includes the suppliers of data, AI models and other components, which operators are expected to have written agreements on their responsibilities to support operators' compliance requirements (Article 25).
  \item \textit{Oversight Authorities:} This consists of bodies and agencies that make decisions on the policy lifecycle for the Act, including those involved in investigating and making decisions on compliance and in the development of official guidelines, opinions, delegated/implementing acts, or standards associated with the Act.  At the Member State level, this includes the national competent authorities to be appointed in each for the market surveillance and notifying of certification bodies for the AI Act (Article 70), and certification bodies notified in the state, MSA-related to other harmonised regulations listed in Annex I, authorities identified as protecting fundamental rights in relation to the Act (Article 77), national courts and government. At the European level, this includes the EC, the AI Office and its subgroupings (Article 64), the European AI Board (Article 65), the Advisory forum and panel of independent experts serving the EC and Board (Article s 67 and 68), Union testing service (Article 84), Union coordination bodies for sectorial-MSAs, certification bodies and fundamental rights protections bodies, EU agencies for Fundamental Rights and cybersecurity (ENISA), the ESOs, the CJEU and the European Council and Parliament in the roles related to delegated and implementing acts (Article 97) and revision (Article 112) of the Act.
  \item \textit{Affected Stakeholders:} This is any natural person who may benefit or perceive that they may benefit from the Act's protections of their health, safety, fundamental rights including democracy, the rule of law and environmental protection, any grouping of such persons or any non-governmental organisations that serve to ensure and enhance these protections. While this actor type is offered relatively little formal role in the Act beyond the right to lodge a complaint with an MSA on rights protection breach (Article 85), to request an explanation of an AI decision (Article 86) and to benefit from whistle-blower protections (Article 87), thier participation in the design of technical and governance measures are key but still challenging to implement~\cite{groves_going_2023}.
\end{itemize}

Information exchange between such actors could include: that between regulated actors and regulators to predictably determine compliance with the Act's requirements; that which must support transitivity on AI quality and risk assessment upstream along the value chain from suppliers to providers and deployers; and that exchanged between value chain actors, oversight authorities and affected stakeholder to conduct public interest investigation and comparison, e.g. on appropriate thresholds for measures of particular types of harm used in testing AI systems.

This set of actors and the information exchanges between them present an extremely large space for conducting activities that may result in useful regulatory learning outcomes. This presents challenges for the organisation of such activities in terms of what to prioritise, how to enact the principle of subsidiarity between making decisions at the Member State or Union level, which actors to involve, how to maximise learning and how to avoid wasteful duplication of activities. We propose that some of these issues can be addressed by structuring the space for learning activities along three characteristics or axes. Firstly, \textit{Protections:} addressing separately the 50 fundamental rights of the CFR, with an additional focus on health, safety, democracy, the rule of law and environmental protection, as emphasised in the Act. \textit{Secondly}, AI System Types as relevant to the Act, namely:  \textit{prohibited} as per Article 5(1), \textit{permitted/exempt prohibited} described in Article 5(1f) and 5(1g), \textit{Annex I high-risk} classified under one of the 20 vertical sectorial regulations in that Annex, \textit{Annex III high-risk}, \textit{Annex III non-high-risk} classified under the 8 areas or 24 sub areas defined therein, (Article 6(3)), \textit{transparency-required} according to Article 50,  \textit{AI systems other than high-risk} that are subject to voluntary application of the codes of conduct (Article 95). These seven types of regulated AI systems are not fully mutually exclusive, as one can envisage systems subject to more than one, e.g. vehicles involved in critical infrastructure or a high-risk category that is also subject to transparency or voluntary guidelines. \textit{Thirdly}, learning activities can be classified according to interactions of information between actors that their outcomes inform. This granularity therefore favours activities that require agreement or communication between actors' types over ones that can be resolved internally by an actor and therefore will not benefit from cooperative organisation of learning activities. Though we do not aim to present a comprehensive list, interactions on this axis could include: a deployer generating a fundamental rights impact assessment; a provider releasing a high-risk AI system, a deployer employing a high-risk AI system; a deployer employing a high-risk AI system in a way that involves substantial change; a GPAI model provider releasing a model with systematic risk; an AI provider employing a GPAI model with systematic risk; a deployer employing a GPAI model with systematic risk; a known but mitigated risk in an AI system materialising after deployment, or materialisation of a previously unknown risk in an AI system.

\section{Layers of Regulatory Learning}
To further organise the space for regulatory learning, we identify nine distinct learning levels (LL1-9) to classify regulatory learning activities, the actors involved in learning arenas at each level, the learning arena types in which they engage and the objective of that learning engagement and possible exchangeable learning outcomes.

\par
\textit{LL1: Individual Learning:}This level consists of individual learning in arenas related to personal knowledge and skills, conducted for example through formal courses, experiential learning \cite{madaio_learning_2024} and competency reviews. The individual learners from value-chain actors would include the staff of providers and deployers, as well as related suppliers meeting the AI literacy requirements of Article 4 \cite{long2020ailiteracy}. Another arena would involve officers of oversight authorities whose professional competence was  certification, bodies acting as notified bodies, national competent authorities (Atricle 70(3)), including notifying bodies (Article 31(11)), MSA and fundamental rights protection bodies who would need to demonstrate professional competence needed for their employers to maintain their oversight role, e.g. knowledge of provisions of the Act, standards and codes of conduct and up-to-date awareness of the scientific state of the art in AI risk types and their mitigation techniques. For Affected Stakeholders, learning arenas may be established for members of the general public, especially those from vulnerable groups like children [UNESCO], for example in response to needs of informed consent in user trials or through public policy to expand public awareness of their rights under the Act. Other learning arena would then be those that design curricula, content and any assessment for learning by individuals. Such arenas are already emerging in professional bodies such as the International Association of Privacy Professions\footnote{https://iapp.org/resources/topics/eu-ai-act/} and at JTC21 in preliminary investigation into  "competence requirements for AI ethicist professionals" and "Guidance for upskilling organisations on AI ethics and social concerns".  
\par
\textit{LL2: Organisational Learning} consists of organisations in learning arenas within their own organisational operations, including those involving management of relationships with suppliers, customers, oversight authorities and affected stakeholders.  Where organisations are high-risk AI providers, they need to demonstrate implementation of a quality management system for which the ISO/IEC MSS provides a suitable template. The MSS template may also offer a well-established organisational learning system that can be adopted by any organisation in the AI value chain, even those not required to implement one as providers of a high-risk AI system. Learning outcomes may include: understanding of risks related to the organsiation's involvement in specific AI value chain; understanding and outcome- and cost-optimised use of risk identification and mitigation techniques and, over the long term, the effectiveness of different risk management and other management activities in achieving both compliance with regulatory requirements and the organisation's goals. Consumers of the learning are primarily the indvidual organisation. However, organisations may also opt to voluntarily share learnings with those they cooperate with, e.g. suppliers, customers and competitors within the context of cooperative frameworks such as trade associations, testbeds, living labs, regulatory sandboxes or standards committees, with a particular focus on SMEs and start-ups and public sector organisations. Further, learning may be facilitated by management consulting or auditing services providers, who may also then integrate valuable learning into the services offered to other clients, while retaining confidentiality firewalls between activities with different clients.
\par
\textit{LL3: Vertical Enforcement Learning} consists of actors involved in compliance in the vertical sectors defined in Annex I and Annex III of the Act. Such learning arenas could include instances of product certification and instances of market surveillance activities (including incident reporting, investigation by MSA and regulatory enforcement and actions). This is undertaken largely at the Member State level by notified bodies and MSA as oversight authorities interacting with providers and deployers as value chain actors. The number of notified bodies in a Member State depends on whether a certification body would seek to be a notified body under the Act, and sometimes this is undertaken by a body supported by the state where there is a public policy need in that state. MSA include the national competent authority appointed for the AI Act and the MSA in place for the Annex I regulations plus bodies responsible for the fundamental rights protection. For example, in Ireland while the national competent authority for the AI Act has not at the time of writing been announced, there is a notified body to serve the certification needs of the large medical device sector operating, there are 10 different agencies or government departments covering the MSA role for the 20 Annex I regulations and 9 fundamental rights protection bodies have been announced for the Act. At the EU level, additional learning arenas operate based on reporting, coordination and cooperation of the national oversight actors. The European AI Board has subcommittees to coordinate between the certification related bodies and surveillance related bodies in each state. In addition, Annex I market surveillance in each state cooperates via an Administrative Coordination Group, though it is unclear how these will interact with the surveillance subgroup of the European AI Board. Other arenas for vertical learning involve oversight authorities, value chains actors and potentially affected stakeholders are sectorial regulatory sandboxes, testbeds or living labs undertaking trial with human users or sector-specific technical standards committees (though as noted above none yet harmonised, standard development is purely horizontal). Member States are required to establish at least one regulatory sandbox under the Act, but it is not clear how the coverage of different vertical sectors will be coordinated between states in a way that maximises learning but minimises wasteful duplication. Learning outcomes may include: understanding of sector-specific risks and the effectiveness of risk assessment techniques encountered; agreement on acceptable risk thresholds in specific sectors; agreement on appropriate risk mitigation measures for different risk types and of acceptable levels and forms of technical documentation and of incident reporting. Consumers of these learning would be the participant in these arenas including Member State oversight authorities providing certification, conducting market surveillance or undertaking fundamental rights protection activities and, subject to agreement, the broader cohort value chain actors and affected stakeholder in a sector.
\par
\textit{LL4: Horizontal Learning}: It is as yet unclear where the balance of learning arena activity will be between the Member State and European levels. In the states, the national competence authorities will be the hub for coordination and dissemination of learning based on translation of the Act into national law and policies and tailoring of support to the needs of the preponderance of sectors, providers and deployers in the state, as well as demands from coordinated industry groups, SMEs, citizens and public sector organisations and garnered from local vertical learning arenas in LL3. At the European level the learning arena will be centred on the EC and the AI Office as they consult and gather evidence on the formation of required guidelines, e.g. in relation to compliance in public procurement, the determination of substantial change to an AI system by deployers or reduced technical documentation requirements for SMEs. Further, the ESO form horizontal learning arenas as they develop and revise harmonised standards request, develop complementary horizontal standards sought by AI providers and deployers and adopt additional standards from appropriate international bodies,such as SC42 deployers. Other standards fora without the mandate to produce harmonised standards may still offer useful arenas for consultation and development of technical standards, e.g. machine readable AI software documentation package from the Linux Foundation SPDX project. The learning outcome at this level are therefore horizontal guidelines, implementing/delegated acts and standards. 
\par
\textit{LL5: Learning on General Purpose AI} involves all actors types and is centred at the AI Office, which is already undertaking a large consultation with over 1000 participants on the required code of practice for the GPAI \footnote{https://digital-strategy.ec.europa.eu/en/policies/ai-code-practice}.
\par
\textit{LL6: Learning on Voluntary Codes of Conduct} represents the arena that previously made up the breadth of AI ethics guidelines discussed previously. It will continue to involve all types of actors, though the challenge will be progressing to a common code of conduct amidst some many competing guidelines, many representing different conflicting interests as seen in debates on ethics washing. A key consideration here will be whether a regulatory turn in AI ethics will refocus voluntary codes more specifically on fundamental rights protections to leverage the learning and evidence that must arise from the compliance actions for high-risk and GPAI systems.
\par
\textit{LL7: Learning for the Legislative Review of the Act} consists of the learning arenas where the enforcement of the AI Act and its implementing or delegated acts are considered, developed or refined, e.g. for changing the areas covered by Annex I and III or the derogation on high-risk classification in Article 6(3). Learning outcomes are therefore in the form of implementing and delegated acts \cite{novelli_robust_2024}\cite{kaizenner_ai_2024} and evaluation and review of the Act (Article 112). It involves the EC, the AI Office, as well as the Council and Parliament. Learning from LL3 and regulatory sandboxes therein can feed into this assessment and review of legal frameworks~\cite{oecd2023sandbox}, though it seems unlikely sandboxes would be resourced purely for this purpose.
\par
\textit{LL8: Learning on the Interplay of the Act and Related Digital and Product Legislation}  consists of actors engaged in learning arenas that consider the interplay between the AI Act and other legislation, including national legislation, other EU legislation such as the Digital Services Act, cybersecurity and privacy legislation (GDPR, the Cyber Resilience Act, Network and Information Systems (NIS2) Directive), Liability Legislation (General Product Safety Regulation, the Product Liability Directive, class actions via the Representative Action Directive), Corporate Sustainability Reporting Directive on environmental and social reporting and the proposed AI Liability Directive, Open Digital Market Legislation which impacts data value chains (Digital Market Act, Data Act, Payment Services Directive 2), Data and Knowledge Leverage Legislation (Data Governance Act for data pooling/sharing, the Health Data Spaces Act and Interoperable Europe Act for the public sector). This also includes arenas assessing interplay with other non-EU legislation including AI regulation of major trading partners, e.g. via the Council of Europe, UNESCO, OECD, Interparliamentary Union. Learning outcomes could include: revealing unintended interplay between the AI Act and other legalisation that confounds either's enforcement; places where enforcement of one regulation already adequately offers outcomes intended by the other; or areas where the interplay and potential precedent between enforcement of different legislation needs further clarification. Consumers of such learning would include national bodies or the EU who are in a position to propose legislative amendments that could address issues raised.
\par
\textit{LL9: Learning on the Interplay of the Act and Fundamental Rights Protections} consists of actors engaging in learning arenas where the protections of fundamental rights in relation to AI are considered and if necessary revised. Such arenas could include cases brought before national courts, the CJEU or where a MSA or the AI Office makes a ruling based on a potential fundamental rights breach. Learning outcomes could include: clarification of circumstances under which the AI Act does or does not support protection of specific fundamental rights; judgment on the relative importance when different rights protections come into conflict (including the freedom to conduct a business) in the protection of other rights; where fundamental rights are protected by other national or EU legislation rather than the provisions of the AI Act; or areas where revised or additional fundamental rights formulations or additional secondary legislation are needed to fully protect fundamental rights from impact of AI. Learning can be facilitated by cataloging judgments from existing rights protecting frameworks such as GDPR Supervisory Authorities\footnote{https://gdprhub.eu} and the ECtHR.\footnote{https://huridocs.org/} Consumers of such learning could be national bodies, the EC  who is in a position to propose new fundamental rights protection legislation or modifications to the AIA implementation or that of other rights-protecting legislation, and the Council of Europe. 


%Academic tools surveys:
%AI Risks: https://airisk.mit.edu/ 
%AI Auditing Tools: https://doi.org/10.48550/arXiv.2401.14462 
%Some practice emerging from application of industry guidelines:
%EU HLEG ALTAI: https://altai.insight-centre.org/ 
%Microsoft Azure Harms Modelling: https://learn.microsoft.com/en-us/azure/architecture/guide/responsible-innovation/harms-modeling/ 
%IBM; https://www.ibm.com/impact/ai-ethics 
%Voluntary nature leads to concerns of Ethics Washing:
%Schultz, M.D., Conti, L.G. \& Seele, P. Digital ethicswashing: a systematic review and a process-perception-outcome framework. AI Ethics (2024). https://doi.org/10.1007/s43681-024-00430-9 
%OpenAI: Voluntary self-governance in practice ….
%Open question of how AI Act codes will develop, BUT high-risk regulation offers ‘gold standard’





%Flexibly in the Act through Delegated Power to EC, reviewed annually, 
%e.g. prohibited classes, Annex III high risk classes and derogations
%2nd Aug 2028, and every four year after – EC report to EP
%New areas for Annex III
%Transparency
%Supervision and Governance
%Implementation resources
%Use of penalties
%Harmonised standards and common specification
%Impact on SME market entrants
%Other periodic reviews:
%Voluntary Code of Conduct
%Energy efficient GPAI
%EC can propose amendments and report of sufficiency of powers
%2 Aug 2031: EC overall evaluation report



\section{Information Exchange for Regulatory Learning}
For actors involved in the AI value chain, the interactions with stakeholders can be categorised based on whether they occur within the same learning loop (\textit{intra-loop}) i.e. with other actors implementing or involved in similar situations with respect to the AI Act, or across learning loops (\textit{inter-loop}) - such as where actors are receiving technologies from upstream providers, or are providing downstream implementers with products and services. The primary distinguishing factor between intra- and inter-loop is the shared context - actors within the same learning loop will have a shared understanding of their abilities, processes, and risks, and therefore can share pertinent information that relies on common interpretation. However, actors sharing information across learning loops are required to understand the context and abilities of the intended recipients, and develop approaches to facilitate shared understanding and consistent interpretation. 
% For example, actors operating in vertical (sectorial) implementations of AI systems can share information with each other by relying and reusing the existing sectorial terms and common knowledge. But when the same actors interact with actors in horizontal (general-purpose) implementations, both must first establish common terminology and interpretations before they can share information with each other.

% @HP: presuming this table will have its own explanation para? Where to find its values or is it to be created?



With this view, we can consider the AI Act as also advocating the collection and use of specific knowledge to both determine and ensure compliance, as well as to promote a culture of organisational oversight through the use of this information. An example of this is Articles 9-15 where specific categories of information are required to be maintained and used towards indicated processes. However, given that the AI Act ecosystem (or the AI value chain) will mostly consist of multiple entities who develop, provide, and use AI systems, and based on the application of the learning loops from before, we can see how the information required to be maintained by the AI Act will also be required to be shared with other stakeholders outside of the immediate organisational context. At the same time, the specific nature of this information (e.g. highly technical or legal) also shows challenges in effectively utilising it within the organisation where there will be different levels of expertise and the decision-making person may not always be capable of understanding it.

% For simplicity, consider the following two challenges: (1) How can highly technical information about an AI system be shared between technical experts, which may or may not be in the same organisation, in a manner that assures completeness and consistent interpretation? (2) How can highly technical information about an AI system be used to create a summarised overview for decision makers, which may or may not be in the same organisation, in a manner that assures access to underlying technical information if desired?

% AI Act is a Learning Framework
% 	Provisional, extensible, progressively specific semantic vocabularies
% 	Mappable & comparable Parallel development through name spaces
Thus, we point out that effective enforcement of the AI Act depends on the information sharing of relevant stakeholders, and that doing so efficiently is a necessary regulatory learning in this space. However, the primary challenge in developed shared information modelling approaches is the divergence in use-cases and requirements, particularly across sectors. For this, we therefore require a common `upper ontology' based on the AI Act itself which provides a common terminology and basis for developing further approaches that can be provisional while the AI Act is being initially interpreted, and then progressively be extended as certainty of regulatory interpretation is achieved. For such approaches developed in parallel across use-cases, it would be desirable to share information by `aligning' or `mapping' the information so that it can be `translated' and utilised conveniently.

To achieve all of this, we believe the best solution is the use of standards-based information models and processes developed to share them - specifically semantic web standards and their use in data portals that enable cataloging information and deploying a shared infrastructure that promotes data availability and leads to innovative solutions based on its reuse. The EU's vision of `data spaces' is grounded in a similar argument which entirely relies on shared information and processes to achieve greater efficiency through consistency in shared contexts.
The EU Commission's European Interoperability Framework (EIF) % ref: European Interoperability Framework – Implementation Strategy SWD(2017) 112 final
was developed with a similar goal for creating interoperable digital public services. We posit that a similar approach for the AI value chain, based on a focus on achieving the necessary knowledge exchange for AI Act compliance, should be considered to be an essential part of regulatory learning in this space.
Another example is the EU's eProcurement Ontology % ref https://docs.ted.europa.eu/epo-home/
which provides a standardised ontology of concepts for interoperable modelling of procurement data and processes based on EU regulations and directives. The ontology caters to common terminology between stakeholders, and enables the development of common tooling, platforms, and services that function both before and after the award. A similar effort for the implementation of the AI Act can be envisioned that functions `end-to-end' across the value chain and assists in the definition of interoperable information for facilitating sharing between stakeholders, and to formalise the exchange of knowledge necessary for regulatory learning within and across the defined learning loops. More importantly, it can function as the common technical foundation for the implementation of future regulations and directives which will build on top of the AI Act in both horizontal (general) and vertical (sectorial) directions. 

Thus, we argue that the development of an interoperable framework is an investment in the regulation and will aid regulatory learning processes for both organisations and policy makers by giving them a common point of reference for developing further obligations and smoothing over reporting obligations. This type of `harmonisation' is critically required given the recent Draghi report % ref: https://commission.europa.eu/topics/strengthening-european-competitiveness/eu-competitiveness-looking-ahead_en
outlined reduction in regulatory reporting as one of the key factors essential to ensuring EU competitiveness, and where the Commission's response agrees with this view as it has tabled an `omnibus regulation' in its agenda for the 2025 work program that reduces reporting and red-tape by 25\%. %ref: https://ec.europa.eu/transparency/documents-register/detail?ref=SEC(2024)2508&lang=en
We interpret this as the Commission aiming to make regulatory enforcement more efficient rather than diluting the regulatory obligations and protection of fundamental rights and freedoms. Thus, our argument for using regulatory learning as a tool to systematise compliance processes across the ecosystem and across regulations should be considered an important factor for such future policy-making decisions. 
% SC42 Standards focus on process vs info exchange 
% 	Mapping Act and harmonised standard into workable interoperability specifications
% 	Semantic interoper., Serialisation and Web API integration

To achieve this, we envision the reuse of existing standardisation processes, such as in ISO/IEC, CEN/CENELEC, and W3C to develop appropriate agreements on information exchange formats and processes based on using them. This also provides a way for the integration of information required or involved in the use of harmonised standards for the AI Act within the proposed interoperability framework.


% Complex value chains
% 	Specialisable upper layer models for permissionless innovation in different sectors
% 	Upper layer models, serialisation, API and access control
% Need for broad stakeholder engagement
% 	Open access availability, especially for SME, NGO, public sector 
% 	OSS tools for queries, constraints, rules, catalogues & user interfaces



\section{Discussion and Recommendations}

 In this paper, we have argued that the EU AI Act presents two major areas of uncertainty that need to be addressed in order to efficiently and consistently achieve the joint aims of enabling innovation and adoption of AI technology while protecting societal values: the gap between horizontal technical measures and vertical enforcement measures and the means by which fundamental rights protections can be consistently enforced across the EU. We acknowledge that the Act employs many measures for addressing these and other current implementation uncertainties over time, and we argue that the design of decision-making and advisory fora established to support the Act should reflect the need to operate as an efficient, consistent and stakeholder-engaged regulatory learning framework. We therefore propose axes for a regulatory learning space and suggest how this might be broken into layers to more clearly identify and mobilise the learning arena best positioned to address specific uncertainties, while also understanding the information flow that can be supported to enable learning at all layers to keep pace with advances in their areas of competence. Based on our analysis of this layered learning space we make the following recommendations:
\begin{itemize}
  \item \textit{Oversight authorities:} Maximise the use of good transparency practices to improve the efficiency, timeliness and predictability of the exchange of regulatory learning outcomes, including standardised mechanisms for cataloging of documentation and data from learning and use of data spaces. Appropriate information sharing governance policies for such data spaces must balance commercial, maturity or security-related restrictions on the scope of sharing with goals, reduction in barriers to AI innovation by SMEs and the public sector; and the rights of the public and representative groups to engage freely with the development of practices for AI risk management.
  \item \textit{Technical Standards Organisations:} Develop mechanisms to enable more rapid and responsive consultation and consensus-building mechanisms for experts from AI and other specialisations such as those addressing sectorial harmonised standards for certification as well as complementary horizontal activities, especially in data management, testing, documentation and cybersecurity. This could also be extended to support for developing machine-readable verification suites.
\end{itemize}

 
%Multistakeholder engagement must represent citizen views on fundamental rights impact: reporting and redress, public observatories, incidents logs, legitimate regulatory learning
%Semantic Web Technologies offer FAIR, open, extensible, decentralized models for AI Risk and Documentation 
%Standardized tools needed for regulatory info automation support for scaleable compliance and surveillance


%Establishing a semantic superhighway for AI Regulatory Learning

%    Prioritization for concept and artifact definitions
 
%   Prioritites for persistent identifiers
%        REgulation but also implementating acts and guidelines - ELI
%        Technical stadnards - FISO SSOS
%        AI systems 
%        AI risks and controls
%        FR risks
%        public procurement requirements (Interop Europe Act requirement)
%    Potential harms in open sharing
    






%%
%% The acknowledgments section is defined using the "acks" environment
%% (and NOT an unnumbered section). This ensures the proper
%% identification of the section in the article metadata, and the
%% consistent spelling of the heading.
\begin{acks}
This project has received funding from the ADAPT SFI Centre for Digital Media Technology, which is funded by Research Ireland through the SFI Research Centres Programme and is co-funded under the European Regional Development Fund (ERDF) through Grant\#13/RC/2106\_P2. 
\end{acks}

%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{references}


%%
%% If your work has an appendix, this is the place to put it.
\appendix



\end{document}
\endinput
%%
%% End of file `sample-manuscript.tex'.
