
\documentclass[manuscript,screen,review,anonymous]{acmart}

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmlicensed}
\copyrightyear{2018}
\acmYear{2018}
\acmDOI{XXXXXXX.XXXXXXX}

%% These commands are for a PROCEEDINGS abstract or paper.
\acmConference[Conference acronym 'XX]{Make sure to enter the correct
  conference title from your rights confirmation emai}{June 03--05,
  2018}{Woodstock, NY}
%%
%%  Uncomment \acmBooktitle if the title of the proceedings is different
%%  from ``Proceedings of ...''!
%%
%%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%%  June 03--05, 2018, Woodstock, NY}
\acmISBN{978-1-4503-XXXX-X/18/06}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}


%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Mapping the Regulatory Learning Space for the EU AI Act}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.

\author{Dave Lewis}
\authornotemark[1]
\email{dave.lewis@adaptcentre.ie}
\affiliation{%
  \institution{ADAPT Centre at Trinity College Dublin}
  \country{Ireland}
}

\author{Marta Lasek-Markey}
\affiliation{%
  \institution{ADAPT Centre at Trinity College Dublin}
  \country{Ireland}
}
\email{mlasek@tcd.ie}

\author{Delaram Golapayegani}
\affiliation{%
  \institution{ADAPT Centre at Trinity College Dublin}
  \country{Ireland}
}
\email{delaram.golpayegani@adaptcentre.ie}


\author{Harshvardhan J. Pandit}
\affiliation{%
  \institution{ADAPT Centre at Dublin City University}
  \country{Ireland} }
\email{harshvardhan.pandit@adaptcentre.ie}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Trovato et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
The EU's AI Act represents the world first regulation that specifically targets Artificial Intelligence (AI) technologies. It extends existing EU mechanisms for product health and safety regulations to protect fundamental rights by addressing AI as a horizontal technology that is regulated across multiple vertical application sectors. Such extensions introduce inherent uncertainties regarding how generic (horizontal) technical measures map into sectorial (vertical), involved certifications, and consistency in product regulations  across EU Member States - especially in how they address fundamental rights. We argue that these uncertainties, coupled with the rapid progression of AI technologies and the immaturity of managing impacts on fundamental rights as compared to product regulation, necessitates comprehensive and rapid regulatory learning for the AI Act to be successful. We provide the first formalised regulatory learning framework for assisting stakeholders across the AI value chain by identifying `learning arenas' where oversight authorities, value chain participants, and affected stakeholders interact to develop and learn from implementation of technical, organisational, and legal measures. 
% @HP: Added below statement on why this approach is good based on sections in paper
By considering such `separations' in learning approaches, we identify how each cohort can best utilise its position in the value chain and interactions with other cohorts to facilitate enforcement of the AI Act and ultimately for doing their part in managing risks to fundamental rights.
We ground the applicability of this framework by exploring how existing open data policies and practices in the EU regulatory framework can be adapted to support rapid and effective regulatory learning in a transparent manner that supports the development of trust in and predictability of regulated AI. 
% @HP: this point seems detached from the rest of the argument above - I cannot reconcile it even if I try to rephrase it
We also discuss how the Act may result in a regulatory turn in the research of AI fairness, accountability and transparency towards investigations into implementations of and interactions between different fundamental rights protections and reproducible and accountable models of metrology for AI risk assessment and treatment.    
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
<ccs2012>
 <concept>
  <concept_id>00000000.0000000.0000000</concept_id>
  <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
  <concept_significance>500</concept_significance>
 </concept>
 <concept>
  <concept_id>00000000.00000000.00000000</concept_id>
  <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
  <concept_significance>300</concept_significance>
 </concept>
 <concept>
  <concept_id>00000000.00000000.00000000</concept_id>
  <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
 <concept>
  <concept_id>00000000.00000000.00000000</concept_id>
  <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Do Not Use This Code~Generate the Correct Terms for Your Paper}
\ccsdesc[300]{Do Not Use This Code~Generate the Correct Terms for Your Paper}
\ccsdesc{Do Not Use This Code~Generate the Correct Terms for Your Paper}
\ccsdesc[100]{Do Not Use This Code~Generate the Correct Terms for Your Paper}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Do, Not, Us, This, Code, Put, the, Correct, Terms, for,
  Your, Paper}

%\received{20 February 2007}
%\received[revised]{12 March 2009}
%\received[accepted]{5 June 2009}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}
Research into fairness, accountability and transparency of algorithms has focussed on normalised approaches to achieving ethical or trustworthy AI systems in response to societal concerns of the risks they pose. Studies across the rapidly growing body of work that offer guidelines on ethical AI to developers and users of such systems demonstrate a seeming degree of consensus on key principles, including XXX \cite{hagendorff_ethics_2020}\cite{jobin_global_2019}\cite{fjeld_principled_2020}\cite{correa_worldwide_2023}. 
Other studies have indicated however that this seem consensus in discourse breaks down as attempts are made to turn common principles into workable tools \cite{???}. % @HP: Citation needed for statement
This is seen in: the tendency toward techno-centric solutions such as explainable AI or differential privacy that may confound calls for more direct inspection of algorithmic and data processing design decisions; the distance between decision-making on technical measures for ethical AI from that its governance and the lack of involvement by societal actors in technical decision-making \cite{palladino_biased_2023}. A body of analysis has emerged that both posits efforts of `ethics-washing' as a distraction from harmful behaviour and criticism attempts to integrate ethical reasoning into AI governance \cite{schultz_digital_2024}.    

At the same time the capability of AI technology has rapidly accelerated, especially in relation to Large Language Models (LLMs) and Generative AI (GenAI) injected uncertainty about the pace of advance of the capabilities and therefore the risks they present \cite{grace_thousands_2024}. In this sense the AI ethics research community is presented with its own version of the pacing problem that faces effort to regulate fast advancing technologies such as AI, where changes in the form and impact of technology outpace our ability to form evidence-based consensus on how to mitigate its risk\cite{fenwick_regulation_2017}.

on hte rapid development of AIAs the research are assoicated with fairness the major changes brought by the Ai Act
    introduction of fundamental rights protections to product regulation and the likely regulatory turn in the field of AI Ethics
    relative immaturity of the field vs previous digital and product regulation
    the role of technical standards and guidelines 
    use of horizontal regulation - interplay with vertical

The uncertainty of the AI Act and importance of regulatory learning

% @HP: ensure we are grounding of motivation in terms of problem and required solution, current state of this, and then linking it to the contributions of this work

Contributions: This paper offers a layered framework for AI Act regulatory learning space 

% @HP: the rest of the paper is structured as follows ...

\section{Current Landscape regarding Standards and Fundametal Rights Protections in AI Act} % @HP: turned this into a 'state of the art' type section, as otherwise the paper doesn't have one

\subsection{AI act, Standards and Guidelines} % @HP: section on standards could be placed after section on rights to reflect importance of application (one is broader and the other is narrower to the act)

benefits:
Standards request reflects international consensus in areas of interest
Standards embody industry technical consensus
ISO management system standards well aligned with existing EU product certification processes

challenges:
International standards (ISO/IEC) cannot directly address specific jurisdictional needs/norms
Terminology and concepts differ between regulation and standards
Regulation define legal compliance (more ‘shall’/’must’) vs standards often focus on process norms (‘should’/’may’)
Different author communities: legislators vs. technical experts
Different revision processes, authorities and timelines
EC-specific standards may diverge from international standards

\subsection{Fundamental Rights and Legal Uncertainties of the AI Act} % @HP: this section is probably longer - we may need space for other sections. Shorten it?
The enactment of the EU AI Act in the form of Regulation (EU) 2024/1689 will result in a complex and delicate interplay between various layers of legislation, both on EU and national level. Article 1 of the Act stipulates a twofold aim: first, to improve the functioning of the internal market and promote the uptake of human-centric and trustworthy AI; and second, to ensure a high level of protection of health, safety, fundamental rights enshrined in the EU Charter of Fundamental Rights, including democracy, the rule of law and environmental protection, against the harmful effects of AI systems in the Union and supporting innovation. As has previously been the case with EU legislation designed to benefit both the internal market on the one hand and the citizens and their rights on the other, Article 1 of the AI Act carries a degree of inherent tension that will require a balancing act between various conflicting interests ~\cite{garben2020balancing, syrpis2008economic}. The social leg of the AI Act's twofold aim can further be broken down into: 1) health and safety; and 2) fundamental rights, including European values enshrined in Article 2 of the Treaty on the European Union (TEU), notably democracy, the rule of law, etc. Although the former has been extensively regulated in the EU product safety legislation, compliance with the latter considerations, i.e., fundamental rights and European values, is likely to pose significant challenges.

As EU integration progressed over the years, so did protection of fundamental rights. Consequently, within the EU legal order, there is currently no single uniform source of fundamental rights protection ~\cite{spaventa2018should}. Instead, Article 6 TEU identifies three sources thereof, in chronological order: 1) constitutional traditions shared by the Member States; 2) the European Convention for the Protection of Human Rights and Fundamental Freedoms (ECHR), which all EU Member States are parties to but the European Union is not; and 3) the EU Charter of Fundamental Rights (CFR). The Charter containing a catalogue of 50 fundamental rights constitutes the most recent addition to the EU framework, proclaimed in 2000 and binding since 2009. However, the scope of application of the Charter is limited by its so-called horizontal provisions, particularly Article 51. With Lenaerts, we note that the CFR rights are 'addressed to the institutions, bodies, offices and agencies of the Union with due regard for the principle of subsidiarity and to the Member States only when they are implementing Union law'~\cite{lenaerts2012exploring}. 

Therefore, while the provisions of the Charter bind the EU itself and the Member States, and this has been particularly evident in policymaking, its impact in litigation has been limited due to issues with direct effect of the Charter~\cite{frantziou2015horizontal, frantziou2019horizontal}. It is worth reiterating that to be capable of direct effect, i.e. to create rights for individuals, a provision of EU law must fulfill the following three conditions laid down in the CJEU case 26/62 \textit{Van Gend en Loos}: it must be 1) clear, 2) precise and 3) unconditional and, therefore, not require any further implementing measures. In relation to CFR rights, the CJEU has to date only considered direct effect of four provisions. Consequently, Article 21 CFR (right to non-discrimination), Article 31(2) CFR (right to maximum working hours, to daily and weekly rest periods and to annual leave) and Article 47 CFR (right to an effective remedy and to a fair trail) have been deemed as having direct effect~\cite{prechal2020horizontal}. Conversely, Article 27 CFR (workers' right to information and consultation) has been denied direct effect in case C‑176/12 \textit{AMS}. Regarding the remainder of the CFR, it is not clear which of the rights might be capable of direct effect and, thus, whether individuals might be able to successfully rely on them. Consequently, it is possible that in conflicts involving fundamental rights in the context of the AI Act, the other sources of protection, i.e. the European Convention on Human Rights, and indeed the national Constitutions of the Member States, might also become relevant. Lastly, as regards European values going beyond individual fundamental rights, such as democracy and the rule of law, in recent years Article 2 TEU has been gaining in importance. On this note, recent CJEU case law pertaining to the rule of law, e.g. in the context of the independence of the judiciary in Hungary and Poland, has exposed the limits of Article 2 as potential grounds of protection of European values going beyond the individual citizens' rights enshrined in the Charter~\cite{spieker2019breathing, spieker2023eu, hogan2024towards}. 

While potentially all fundamental rights, including CFR rights, might fall within the scope of application of the AI Act, the Act explicitly refers to 17 fundamental rights in Recital 48 of the Preamble, which had also been mentioned in the Proposal for the AI Act (COM(2021) 206 final). This implies that the European Commission expects these to be of particular relevance in the context of AI, even though it does not grant them any additional legal standing. These rights are: 1) the right to human dignity; 2) respect for private and family life; 3) protection of personal data; 4) freedom of expression and information; 5) freedom of assembly and of association; 6) the right to non-discrimination; 7) the right to education; 8) consumer protection; 9) workers’ rights; 10) the rights of persons with disabilities; 11) gender equality; 12) intellectual property rights; 13) the right to an effective remedy and to a fair trial; 14) the right of defence and the presumption of innocence; 15) the right to good administration; 16) children's rights; 17) environmental protection. 

As discussed above, it is unclear which of these fundamental rights beyond the right to non-discrimination, the right to an effective remedy and to a fair trial, as well the specific workers' right to rest, might have direct effect. The lack of direct effect might constitute a barrier to effective enforcement of fundamental rights, particularly in horizontal disputes involving non-state actors, i.e. private parties ~\cite{viljanen2023horizontal, fornasier2015impact}. In the context of the AI Act, disputes involving fundamental rights might potentially concern such types of deployers of high-risk AI systems as private employers, insurance companies or banks. Regarding fundamental rights that do not have direct effect, e.g. the workers' right to information and consultation, it appears that such non-state actors would not be bound by the Charter. 

Many of the fundamental rights enshrined in the Charter have further been concretised in secondary EU legislation, some of which also has direct effect and would, therefore, be binding on private parties. A prime example is the right to the protection of personal data guaranteed by the General Data Protection Regulation 2016/679 (GDPR), referenced multiple times across the AI Act, as well as other legislation, such as the Law Enforcement Directive 2016/680 or the ePrivacy Directive 2002/58. Other examples of secondary legislation mentioned in the AI Act which is connected to various CFR rights include Regulation 2024/900 on the transparency and targeting of political advertising; Framework Directive 2002/14 on employee information and consultation or the Platform Workers Directive 2024/2831. Additionally, the entire body of EU consumer legislation gives effect to Article 38 of the Charter (the right to consumer protection), including the Medical Devices Regulation 2017/745, the Representative Actions Directive 2020/1828 and the Unfair Commercial Practices Directive 2005/29 to name a few examples mentioned in the AI Act. Furthermore, there is other relevant EU secondary legislation which, albeit not explicitly referenced in the AI Act, provides additional layers of protection of various Charter rights. A pertinent example is the EU non-discrimination law consisting of the Framework Directive 2000/78, the Racial Equality Directive 2000/43 and the Gender Recast Directive 2006/54, where implications of AI and algorithmic management in employment are significant ~\cite{wachter2021fairness, xenidis2020eu}.   


Given the existing regulatory maze, as well as the uncertainties surrounding the status of fundamental rights and the Charter in the context of the AI Act, We can therefore draw up the following list of questions that may need to be prioritised in ensuring predictable and consistent enforcement of the Act:
\begin{enumerate}
  \item Will all rights under the CFR be equally protected under the AI Act?
  \item What role will Market Surveillance Authorities (MSA) play in enforcing the AI Act in relation to identifying whether a high-risk AI system has breached FR protections and enforcement of corrective measures?
  \item How will the enforcement powers of MSA in enforcing FR protections (executed in concert with national bodies protecting fundamental rights as per Article 77 of the AI Act) interplay with those of notified bodies, national competence authoritises?
  \item What role and weight will be placed on the FRIA conducted by AI deployers and AI providers (Art. 27) in enforcing FR protections under the Act?
\end{enumerate}

% @HP: need to link this section's end to some contribution or establishment of research question type of statement. At the moment, it does not 'naturally' lead to the next section on learning space.




\section{Mapping a Regulatory Learning Space for the AI Act}

\subsection{Understanding Regulatory Learning across AI Value Chain}
% @HP: would be good to also state what is our definition/interpretation of regulatory learning, how it is created, who uses it, and what are existing examples of it - either here in this section or in the introduction. References to use:
% Regulatory learning in experimentation spaces https://publications.jrc.ec.europa.eu/repository/handle/JRC130458 which talks about experimenting to create regulatory learning
% New Commission Staff Working Document sheds light on experimentation spaces for regulatory learning https://research-and-innovation.ec.europa.eu/news/all-research-and-innovation-news/new-commission-staff-working-document-sheds-light-experimentation-spaces-regulatory-learning-2023-07-25_en -- this mentions the AI Act and sandboxes 
Breakdown of different actor types
    Value chain actors
    Oversight Authorities
    Affected Stakeholders

Interaction between Actors:
    Complex info exchange between AI Providers/Deployers & Authorities

    Value Chain Transitivity of Quality/Risk Assessments and incident reports

    Public Interest comparisons of FRIA/incidents/risks assessment

Axes to define the Learning Space of the AI Act.

[we can elaborate of these axes in the following subsections - Types or AI systems comes form the AI Act iteself, fortypes of protection perhaps we can elaborate on the broader legal basese for these in EU law]

\subsection{Types of AI Systems and General-Purpose AI Models}

 While the scope of the AI Act covers both AI systems and general-purpose AI models, the process for governing general-purpose AI models seems separate from product safety regulation that leverages the New Legislative Framework (NLF) structure. This implies a divergence between risk-based classification for AI systems and general-purpose AI models under the Act. Regarding AI systems, we identified the following 7 types of AI systems: \textit{prohibited} as per Art. 5(1), \textit{permitted/exempt prohibited} described in Art. 5(1f) and 5(1g), \textit{Annex I high-risk}, \textit{Annex III high-risk}, \textit{Annex III non-high-risk} based on the conditions specified in Art.6 (3), \textit{transparency-required} according to Art. 50, %(transparency-required also applies to genAI)
\textit{AI systems other than high-risk} that are subject to voluntary application of the codes of conduct as per Art. 95. These 7 types of systems can be mapped into the 4-level risk hierarchy that is commonly used by the European Commission\footnote{See e.g. \url{https://digital-strategy.ec.europa.eu/en/policies/regulatory-framework-ai}}. %However, the illustration of the risk levels, which are unacceptable risk, high-risk, transparency risk, minimal risk, in separate levels with no overlaps can be misleading, given that high-risk AI systems can also be subject to the transparency obligations.[they are not mutually exclusive] %maybe add a figure from Dave's slides here 
The risk-based categories for general-purpose AI models seems more straightforward with two categories defined on the basis of presence of \textit{systemic risk}. However, the challenge lies in interoperating the notion of ``systemic risk'' and determining whether the model present such risks. [add a reference here]
%General-purpose AI systems that ``generating synthetic audio, image, video or text content'' are subject to transparency obligations (Art. 50(2)).

 
%Prohibited
%Annex I
%Annex III
%Non-High Risk
%GPAI
%Transparency 
%Subject to voluntary codes of conduct


\subsection{Types of Protection}

Safety
Health
Fundamental Rights
Democracy
Environment


\subsection{Types of Value Chain use cases}

Deployer FRIA
HRAI Provider-Deployer
Public Procurement of HRAI
GPAI with Systematic Risk
GPAI Provider-HRAI Provider
GPAI Provider—HRAI Deployer
HRAI Deployer Substantial Change
Risk Materialisation/ Incident Reporting


\section{The Nine Layers of Regulatory Learning}

\subsection{LL1: Individual Learning}
% @HP: clarify here what 'individual' refers to - I know its a person, but individual may get misinterpreted as lay person or end user as well. Would 'professional learning' fit the context as a better term? It is already used for education to refer to learning for teachers etc.
AI Act Literacy Requirements – exploding market
AI Competencies for certification (JTC21 WG4)
    Prerequisite for indemnification?
    Needed to secure innovation investment?
    lerning on the job -cite{madaio_learning_2024}

The sufficient level of AI literacy can be seen as a set of individual competencies ~\cite{long2020ailiteracy}
%UNESCO's K-12 AI curricula 


To fulfil their tasks, national competent authorities, notifying authority, and notified bodies require sufficient competent personnel, with expertise in relevant areas to the AI Act, including AI technologies, data, fundamental rights, health and safety risks (See Art. 28(7), Art. 31(11), and Art. 70(3)). 


\subsection{LL2: Organisational Learning}

ISO/IEC 42001: AI Management System Standard

Note: JTC21 has not adopted 42001 directly, but is defining its own QMS that makes extensive reference to 42001


\subsection{LL3: Vertical Enforcement Learning}

AI Act is a Regulatory Learning Framework
    Providers must assess and treat risks
    For severe risks with known treatments – external certification required
    Otherwise providers can self-certify
    If Risks materialize post-deployment, products can be removed, correctives demanded and fines levied
    Learning on new risks shared across market & authorities
    Regulator Leaning via Sandboxes and Human Trials accelerate learning and sharing knowledge on risks


\subsection{LL4: Horizontal Learning}

%Summary of norizontal harmonised stadnards request: 
Given that AI Act follows the NLF structure, it only defines the \emph{essential requirements} whose interpretation and implementation is expected to be supported by \emph{harmonised standards} (Art. 40) \cite{mazzini2023proposal, tartaro2023towards}. In the implementing decision on a standardisation request \cite{standardisationrequest}, the Commission has called upon CEN (European Committee for Standardisation) and CENELEC (European Committee for Electrotechnical Standardisation) to develop the required harmonised standards in relation to the AI Act. With a deadline in April 2025, CEN and CENELEC are delegated to create European standard(s) and/or European standardisation deliverable(s).

Enforcement Interoperability: Technical Document and Bill of Materials Standards over life cycle
    Ecma TC54-TG2 is chartered with the standardization of Package URL (PURL) https://ecma-international.org/task-groups/tc54-tg2/ https://github.com/package-url/purl-spec 
    CycloneDX - Machine Learning Bill of Materials (ML-BOM) https://cyclonedx.org/capabilities/mlbom/ 
    SPDX - AI Profile https://spdx.dev/learn/areas-of-interest/ai/ 
    Implementing AI Bill of Materials (AI BOM) with SPDX 3.0 https://www.linuxfoundation.org/research/ai-bom 
    SPDX 3.0.1 JSON-LD & Turtle files https://spdx.github.io/spdx-spec/v3.0.1/serializations/ 


\subsection{LL5: Learning on General Purpose AI}

AI Office: GPAI Consultation: closed 18th Sept 24
    General-purpose AI models: transparency and copyright-related provisions
    General-purpose AI models with systemic risk: risk taxonomy, assessment and mitigation
    Reviewing and monitoring the Codes of Practice for general-purpose AI models
Large consultative activity ongoing (1000+ members)
    2 May’25  Codes of Practice in place
    2 Aug’25 GPAI General Purpose Obligation in force


\subsection{LL6: Learning on Voluntary Codes of Conduct}

Only arena for AI Ethics up to this point
Wide Range of Guidelines and Principles:
Corrêa, N.K., Camila Galvão, C., Santos, J.W., Del Pino, C., Pontes Pinto, E., Barbosa, C., Massmann, D., Mambrini, R., Galvão, L., Terem, E., de Oliveira, N. (2023) Worldwide AI ethics: A review of 200 guidelines and recommendations for AI governance, Patterns, Volume 4, Issue 10, https://doi.org/10.1016/j.patter.2023.100857
Academic tools surveys:
AI Risks: https://airisk.mit.edu/ 
AI Auditing Tools: https://doi.org/10.48550/arXiv.2401.14462 
Some practice emerging from application of industry guidelines:
EU HLEG ALTAI: https://altai.insight-centre.org/ 
Microsoft Azure Harms Modelling: https://learn.microsoft.com/en-us/azure/architecture/guide/responsible-innovation/harms-modeling/ 
IBM; https://www.ibm.com/impact/ai-ethics 
Voluntary nature leads to concerns of Ethics Washing:
Schultz, M.D., Conti, L.G. & Seele, P. Digital ethicswashing: a systematic review and a process-perception-outcome framework. AI Ethics (2024). https://doi.org/10.1007/s43681-024-00430-9 
OpenAI: Voluntary self-governance in practice ….
Open question of how AI Act codes will develop, BUT high-risk regulation offers ‘gold standard’


\subsection{LL7: Learning for the Legislative Review of the Act}

Flexibly in the Act through Delegated Power to EC, reviewed annually, 
e.g. prohibited classes, Annex III high risk classes and derogations
2nd Aug 2028, and every four year after – EC report to EP
New areas for Annex III
Transparency
Supervision and Governance
Implementation resources
Use of penalties
Harmonised standards and common specification
Impact on SME market entrants
Other periodic reviews:
Voluntary Code of Conduct
Energy efficient GPAI
EC can propose amendments and report of sufficiency of powers
2 Aug 2031: EC overall evaluation report


while sandboxes play an important role in organisation learning(?), from the regulators' perspective,  they are seen as instruments for assessing and reviewing legal frameworks~\cite{oecd2023sandbox}. This implies that experimentation and learning within AI sandboxes can provide the necessary evidence to signal the need for reviewing the Act. - The Act's reporting obligations in regard to AI regulatory sandboxes supports this  (Art. 57)

\subsection{LL8: Learning on the Interplay of the Act and Related Digital and Product Legislation}

Digital Service Act (DSA): 
    Possibly scope overlap in administration of democracy, 
    DSA learning in algorithm investigatory mechanisms
Privacy and Cybersecurity
    GDPR: Data Protection Impact Assessment and AIA Fundamental Rights Impact Assessment (FRIA - Art 27), and AI audits from GDPR: \url{https://www.edpb.europa.eu/our-work-tools/our-documents/support-pool-experts-projects/ai-auditing_en }
    Sandboxes that jointly test compliance with multiple legal regimes, for example the requirements of the AI Act and the GDPR~\cite{Baldini2024regulatorysandbox}
    Cyber Resilience Act (CRA):  product focus - AIA requirements on Accuracy, Robustness and Cybersecurity (Art 15)
    Network and Information Systems (NIS2) Directive – Annex III critical infrastructure

Liability Legislation:
    General Product Safety Regulation – includes software
    Product Liability Directive – includes mental health
    Representative Action Directive – collective redress
    Corporate Sustainability Reporting Directive (CSRD) – environmental and social reporting
    Proposed AI Liability Directive
Open Digital Market Legislation: impacts data value chains
    Digital Market Act
    Data Act – Internet of Things data
    Payment Services Directive 2
Data and Knowledge Leverage Legislation:
    Data Governance Act: Mechanisms for legal basis for data pooling/sharing
    Health Data Spaces Act
    Interoperable Europe Act: Interoperability specification sharing in public sector

Legislation with legislation outside EU and MS
Trans-national agreements:
Council of Europe, UNESCO, OECD, Interparliamentary Union
Trading partner: UK, US, China
Stability, Politics
Brussels effect?
AI Act less ‘portable’ than GDPR – highly interconnected with other legislation
Digital sovereignty?
Non-tariff trade barriers?

%Relation to national law in each Member State?
\subsection{LL9: Learning on the Interplay of the Act and Fundamental Rights Protections}

Interplay with existing EU regulation protecting fundamental rights
Need a clear mapping
Citizen Literacy & Reporting breaches 
https://commission.europa.eu/aid-development-cooperation-fundamental-rights/your-rights-eu/how-report-breach-your-rights_en 
Interplay with Member State Regulation Protecting Fundamental Rights
Comfort with a certification body or market surveillance bodies in other MSs determining details of EU-wide FR protections, 
e.g. categories of gender in AI used for hiring?
Civil Society tools:
https://gdprhub.eu/ 
https://huridocs.org/


\section{Information Exchange for Regulatory Learning}

For actors involved in the AI value chain, the interactions with stakeholders can be categorised based on whether they occur within the same learning loop (\textit(intra-loop}) i.e. with other actors implementing or involved in similar situations with respect to the AI Act, or across learning loops (\textit{inter-loop}) - such as where actors are receiving technologies from upstream providers, or are providing downstream implementers with products and services. The primary distinguishing factor between intra- and inter-loop is the shared context - actors within the same learning loop will have a shared understanding of their abilities, processes, and risks, and therefore can share pertinent information that relies on common interpretation. However, actors sharing information across learning loops are required to understand the context and abilities of the intended recipients, and develop approaches to facilitate shared understanding and consistent interpretation. 
% For example, actors operating in vertical (sectorial) implementations of AI systems can share information with each other by relying and reusing the existing sectorial terms and common knowledge. But when the same actors interact with actors in horizontal (general-purpose) implementations, both must first establish common terminology and interpretations before they can share information with each other.

% @HP: presuming this table will have its own explanation para? Where to find its values or is it to be created?
\begin{table}
    \centering
    \begin{tabular}{|c|c|c|c|c|c|c|} \hline 
    % @HP: changed "parties" to "entities" for abstraction
       Learning Loop Level  & Entities & Timing & Learning Goals & AIA Provisions & Learning Measures & \\ \hline 
       LL1: Individual  &  &  &  &  &  & \\ \hline 
       LL2: Organisational  &  &  &  &  &  & \\ \hline 
       LL3: Vertical &  &  &  &  &  & \\ \hline 
       LL4: Horizontal &  &  &  &  &  & \\ \hline 
       LL5: GPAI &  &  &  &  &  & \\ \hline 
       LL6: Voluntary Code of Conduct  &  &  &  &  &  & \\ \hline 
       LL6: AI Act &  &  &  &  &  & \\ \hline 
       LL8: Other Regulation  &  &  &  &  &  & \\ \hline 
       LL9: Fundamental Rights  &  &  &  &  &  & \\ \hline
    \end{tabular}
    \caption{Summary of Characteristics of Learning Loop Levels}
    \label{tab:my_label}
\end{table}

% \subsection{Inter-loop Information Sharing}

% [here we make arguement for why open knowledge graphs are a suitable and effective tool for infromaiton sharing in support of regulatory learning, citing existign EU and LOD useage,  DPV, delarma's phd and other examples, ]

% @HP: the below para can be taken out for space
If we look at conventional norms for sharing information, they are oriented towards manual processes and rely on `unstructured' formats, typically those that are text-heavy - such as MS-Word or PDF documents. While these are convenient to communicate and use (e.g. print or share with another human), they are inherently `limited' due to their inability to be used reliably in technological processes without a lot of additional effort and resources. Ironically, LLMs themselves might be suggested as a solution to this situation, however, as of now, we do not consider them capable of replacing a human who can read and enact complex interpretations, and further develop contextual knowledge by using the information. For example, a Data Protection Officer (DPO) under GDPR must interpret documents to determine what the organisation's personal data processing activities are, and from it identify tasks that could include remedying potential compliance violations. In this, the GDPR foresees the need for such information to be documented (as it does in Article 30), and for this information to be used by the DPO to support other organisational processes (for example, in Article 39).

With this view, we can consider the AI Act as also advocating the collection and use of specific knowledge to both determine and ensure compliance, as well as to promote a culture of organisational oversight through the use of this information. An example of this is Articles 9-15 where specific categories of information are required to be maintained and used towards indicated processes. However, given that the AI Act ecosystem (or the AI value chain) will mostly consist of multiple entities who develop, provide, and use AI systems, and based on the application of the learning loops from before, we can see how the information required to be maintained by the AI Act will also be required to be shared with other stakeholders outside of the immediate organisational context. At the same time, the specific nature of this information (e.g. highly technical or legal) also shows challenges in effectively utilising it within the organisation where there will be different levels of expertise and the decision-taking person may not always be capable of understanding it.

% For simplicity, consider the following two challenges: (1) How can highly technical information about an AI system be shared between technical experts, which may or may not be in the same organisation, in a manner that assures completeness and consistent interpretation? (2) How can highly technical information about an AI system be used to create a summarised overview for decision makers, which may or may not be in the same organisation, in a manner that assures access to underlying technical information if desired?

% AI Act is a Learning Framework
% 	Provisional, extensible, progressively specific semantic vocabularies
% 	Mappable & comparable Parallel development through name spaces
Thus, we point out that the effective enforcement of the AI Act depends on the information sharing of relevant stakeholders, and that doing so efficiently is a necessary regulatory learning in this space. However, the primary challenge in developed shared information modelling approaches is the divergence in use-cases and requirements, particularly across sectors. For this, we therefore require a common `upper ontology' based on the AI Act itself which provides a common terminology and basis for developing further approaches that can be provisional while the AI Act is being initially interpreted, and then progressively be extended as certainty of regulatory interpretation is achieved. For such approaches developed in parallel across use-cases, it would be desirable to share information by `aligning' or `mapping' the information so that it can be `translated' and utilised conveniently.

To achieve all of this, we believe the best solution is the use of standards-based information models and processes developed to share them - specifically semantic web standards and their use in data portals that enable cataloguing information and deploying a shared infrastructure that promotes data availability and leads to innovative solutions based on its reuse. The EU's vision of `data spaces' is grounded in a similar argument which entirely relies on shared information and processes to achieve greater efficiency through consistency in shared contexts \cite{data-spaces}. % ref: https://doi.org/10.1016/j.dib.2024.110969
The EU Commission's European Interoperability Framework (EIF) % ref: European Interoperability Framework – Implementation Strategy SWD(2017) 112 final
was developed with a similar goal for creating interoperable digital public services. We posit that a similar approach for the AI value chain, based on a focus on achieving the necessary knowledge exchange for AI Act compliance, should be considered to be an essential part of regulatory learning in this space.
% SC42 Standards focus on process vs info exchange 
% 	Mapping Act and harmonised standard into workable interoperability specifications
% 	Semantic interoper., Serialisation and Web API integration
To achieve this, we envision the reuse of existing standardisation processes, such as in ISO, CEN/CENELEC, and W3C to develop appropriate agreements on information exchange formats and processes based on using them. This also provides a way for the integration of information required or involved in the use of harmonised standards for the AI Act within the proposed interoperability framework.

There are existing efforts within the state of the art where information modelling and exchange based on semantic web standards has been developed - in particular for use with GDPR \cite{} % ref: Analysis of ontologies and policy languages to represent information flows in GDPR https://doi.org/10.3233/SW-223009 ; ref: DPV ; 
and where such efforts have also been successfully used to aid in the interpretation of the AI Act's high-risk categorisations \cite{} % ref: Facct to be high risk
and to develop a shared knowledge base of risks involved in AI systems \cite{}. % ref: AIRO/VAIR
However, what is missing is to formally recognise their role in the regulatory learning process, and to explore how they can be used to assist specific actors and processes within each learning loop, and then how they can be used to share these intra-loop learnings with other learning loops.
% Complex value chains
% 	Specialisable upper layer models for permissionless innovation in different sectors
% 	Upper layer models, serialisation, API and access control
% Need for broad stakeholder engagement
% 	Open access availability, especially for SME, NGO, public sector 
% 	OSS tools for queries, constraints, rules, catalogues & user interfaces

% @HP: based on above, I'm not sure how the below table ties in, but I'm thinking it could be something like we identified the following broad information categories, and this is how we think they apply or are involved in for each learning loop?
    \begin{table}
        \centering
        \begin{tabular}{|l|c|c|c|c|c|c|c|c|c|} \hline 
            Information Artefacts Shared & LL1 & LL2 & LL3 & LL4 & LL5 & LL6 & LL7 & LL8 & LL9 \\ \hline 
            AI DB &  &  &  &  &  &  &  &  & \\ \hline 
            Incident Reports &  &  &  &  &  &  &  &  & \\ \hline 
            Risk/Mitigation Registry &  &  &  &  &  &  &  &  & \\ \hline 
            FR Harms &  &  &  &  &  &  &  &  & \\ \hline 
            Sandbox Results &  &  &  &  &  &  &  &  & \\ \hline 
            Human Trial Results &  &  &  &  &  &  &  &  & \\ \hline 
            Public Procurement Specs &  &  &  &  &  &  &  &  & \\ \hline 
            MSA Assessments &  &  &  &  &  &  &  &  & \\ \hline 
            NCA Rulings &  &  &  &  &  &  &  &  & \\ \hline 
            AI Office Ruling &  &  &  &  &  &  &  &  & \\ \hline 
            Certification Results &  &  &  &  &  &  &  &  & \\ \hline
        \end{tabular}
        \caption{Summary of Sources and Sinks of Information - with some compact actor class coding and AIA article ref where relevant and perhaps an indication of whether this is required or recommended or just a possibility}
        \label{tab:my_label}
    \end{table}

\subsection{}

\section{Discussion and Recommendations}

AI systems are now regulated in the EU
New risks management and documentation obligations on AI Providers and AI Deployers
Requires lots of complex, multi-party information exchange 
Many legal uncertainties, lots of regulatory learning needed – a ‘regulatory turn’ in AI Ethics
Multistakeholder engagement must represent citizen views on fundamental rights impact: reporting and redress, public observatories, incidents logs, legitimate regulatory learning
Semantic Web Technologies offer FAIR, open, extensible, decentralized models for AI Risk and Documentation 
Standardized tools needed for regulatory info automation support for scaleable compliance and surveillance

Recommendtions

Establishing a semantic superhighway for AI Regulatory Learning
    ADaoption of FAIR principles, learning from open data approaches in the EU
    Open versus controlled data exchange
    Prioritization for concept and artifact definitions
    Models of catalogues and federated sharing systems - DCAT-AP 4 AI
    Prioritites for persistent identifiers
        REgulation but also implementating acts and guidelines - ELI
        Technical stadnards - FISO SSOS
        AI systems 
        AI risks and controls
        FR risks
        public procurement requirements (Interop Europe Act requirement)
    Potential harms in open sharing
    






%%
%% The acknowledgments section is defined using the "acks" environment
%% (and NOT an unnumbered section). This ensures the proper
%% identification of the section in the article metadata, and the
%% consistent spelling of the heading.
\begin{acks}

\end{acks}

%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{references}


%%
%% If your work has an appendix, this is the place to put it.
\appendix



\end{document}
\endinput
%%
%% End of file `sample-manuscript.tex'.
