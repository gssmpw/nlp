\section{Related Work}

\paragraph{Fairness Issues in Generative AI}   

Fairness concerns in generative AI often arise from biases in training data and non-representative model outputs. \citet{xiang2024fairness} highlights how data bias leads to representational harm and legal challenges, while \citet{ghassemi2024limiting} emphasizes its impact on racial and gender disparities in AI-driven cancer care. \citet{luccioni2024stable} and \citet{teo2024measuring} assess social bias in diffusion models, proposing improved fairness measurement techniques. These studies underscore fairness as both a technical and societal issue.

\paragraph{Bias Detection}

With the increasing use of LLMs, bias detection has gained attention. OccuGender~\cite{chen2024causally} benchmark assesses gender bias in occupational contexts, while \citet{zhao2024gender} examines cultural and linguistic variations in gender bias. BiasAlert~\cite{fan2024biasalert} is a human-knowledge-driven bias detection tool, and \citet{wilson2024gender} highlights LLM-induced bias in resume screening, disproportionately affecting black males. BiasAsker~\citet{wan2023biasasker} constructs a dataset of 841 groups and 5,021 biased properties. These works emphasize the need for diverse evaluation methods and bias mitigation strategies.
Bias detection in T2I models is also emerging. \citet{qiu2023gender} investigates gender biases in image captioning metrics, proposing a hybrid evaluation approach. BiasPainter~\cite{wang2024new} is a framework for quantifying social biases by analyzing demographic shifts in generated images. \citet{wan2024survey} provides a comprehensive review of biases in T2I models, identifying mitigation gaps and advocating for human-centered fairness approaches. These studies contribute to improving fairness in generative AI.

\paragraph{Fairness-Accuracy Trade-Off} 

Balancing fairness and accuracy remains a key challenge. \citet{ferrara2023fairness} and \citet{wang2021understanding} highlight this trade-off, noting that fairness improvements may reduce accuracy. They propose multi-dimensional Pareto optimization to navigate this balance, offering theoretical insights into model performance trade-offs.

\paragraph{Improving Fairness}

To mitigate biases, researchers have proposed various techniques. \citet{jiang2024mitigating} and \citet{shen2023finetuning} improve fairness through fine-tuning and enhanced semantic consistency, while \citet{friedrich2023fair} and \citet{li2023fair} introduce bias adjustment and fair mapping methods. \citet{su2023manifold} develops a ``flow-guided sampling'' approach to reduce bias without modifying model architecture. These methods provide practical strategies for fairness enhancement.