\section{Testing AI Models}

This section outlines the evaluation of AI models' behaviors, including LLMs and T2I models, using {\methodname}.
\S\ref{sec:setting} details the selected models, their hyperparameter configurations, and the evaluation settings of {\methodname}.
\S\ref{sec:exp-objective} presents results from tests using objective queries, assessing the models' adherence to factual accuracy.
\S\ref{sec:exp-subjective} examines model responses to subjective queries, focusing on their ability to maintain neutrality, encourage diversity, and ensure fairness.

\subsection{Settings}
\label{sec:setting}

\paragraph{Model Settings}

We evaluate six LLMs: GPT-3.5-Turbo-0125~\cite{openai2022introducing}, GPT-4o-2024-08-06~\cite{openai2023gpt}, Gemini-1.5-Pro~\cite{pichai2024our}, LLaMA-3.2-90B-Vision-Instruct~\cite{dubey2024llama3}, WizardLM-2-8x22B~\cite{jiang2024mixtral}, and Qwen-2.5-72B-Instruct~\cite{yang2024qwen2}.
Additionally, we assess four T2I models: Midjourney~\cite{midjourney2022midjourney}, DALL-E 3~\cite{openai2023dalle}, SDXL-Turbo~\cite{podell2024sdxl}, and Flux-1.1-Pro~\cite{flux2024flux}.
The temperature is fixed at $0$ across all LLMs.
All generated images are produced at a resolution of $1024 \times 1024$ pixels.

\paragraph{{\methodname} Settings}

The {\methodname} checklist includes 19 real-world statistics, each associated with a query about either the highest or lowest value, yielding a total of 38 topics.
Each topic includes an objective query described in \S\ref{sec:objective}, and a set of subjective queries.
Three baseline subjective queries are included, reflecting distinct real-life scenarios.
Each baseline is further extended with the three cognitive error contexts introduced in \S\ref{sec:exp-subjective}, resulting in nine contextualized queries.

Objective queries for LLMs are tested three times each.
Subjective queries, which utilize randomized profiles as input, are tested 100 times to ensure statistically robust results for each demographic group.
For T2I models, 20 images are generated for both objective and subjective queries.
To automatically identify gender and race from the generated images, facial attribute detectors are employed.
We exclude images without detected faces.
If multiple faces are detected in a single image, all of them are included in the final results.

We evaluate the performance of two widely used detectors: DeepFace\footnote{\url{https://github.com/serengil/deepface}} and FairFace~\cite{karkkainen2021fairface}, through a user study.
Specifically, we randomly select 25 images from each of the four T2I models, resulting in 100 sample images.
These images are manually labeled with race and gender information using a majority-vote approach by three annotators.
The accuracy of DeepFace in gender and race classification is $20.56$ and $38.32$, respectively, whereas FairFace achieves $1.87$ and $19.63$.
The results indicate that FairFace achieved a significantly lower error rate compared to DeepFace.
Consequently, FairFace was selected as the detector for all subsequent experimental analyses.

\begin{table*}[t]
    \centering
    \caption{Performance of AI models. \textbf{Bold} indicates the highest value, while \underline{underline} represents the second highest.}
    \label{tab:performance}
    \resizebox{0.85\linewidth}{!}{
    \begin{tabular}{lccccccccc}
        \toprule
        \multirow{2}{*}{\textbf{Model}} & \multicolumn{3}{c}{\textbf{Obj. $S_{fact}$}} & \multicolumn{3}{c}{\textbf{Subj. $S_{fair}$}} & \multicolumn{3}{c}{\textbf{Avg.}} \\
        \cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-10}
        & \bf Gender & \bf Race & \bf Avg. & \bf Gender & \bf Race & \bf Avg. & \bf Gender & \bf Race & \bf Avg. \\
        \midrule
        GPT-3.5 & 84.44 & 39.81 & 62.13 & 98.48 & 96.28 & 97.38 & 91.46 & 68.04 & 79.75 \\
        GPT-4o & \underline{95.56} & \textbf{54.62} & \textbf{75.09} & 98.39 & 96.18 & 97.29 & 96.98 & \textbf{75.40} & \textbf{86.19} \\
        Gemini-1.5 & 94.44 & 44.44 & 69.44 & 98.13 & \textbf{97.67} & 97.90 & 96.28 & 71.05 & 83.67 \\
        LLaMA-3.2 & \textbf{96.67} & 47.22 & \underline{71.95} & 98.67 & 97.20 & \underline{97.93} & \underline{97.67} & 72.21 & \underline{84.94} \\
        WizardLM-2 & \textbf{96.67} & 44.44 & 70.56 & \textbf{99.17} & \underline{97.51} & \textbf{98.34} & \textbf{97.92} & 70.97 & 84.45 \\
        Qwen-2.5 & 91.11 & \underline{52.78} & \underline{71.95} & \underline{98.83} & 96.40 & 97.61 & 94.97 & \underline{74.59} & 84.79 \\
        \midrule
        Midjourney & 48.90 & \underline{25.36} & 37.13 & \textbf{99.00} & \underline{75.99} & \underline{87.50} & 73.95 & \underline{50.68} & \underline{62.31} \\
        DALL-E 3 & \textbf{58.40} & \textbf{30.33} & \textbf{44.37} & 96.35 & \textbf{84.93} & \textbf{90.64} & \textbf{77.38} & \textbf{57.63} & \textbf{67.50} \\
        SDXL & \underline{51.97} & 22.50 & \underline{37.24} & \underline{98.61} & 74.40 & 86.51 & \underline{75.29} & 48.45 & 61.87 \\
        FLUX-1.1 & 49.07 & 23.50 & 36.29 & 91.66 & 30.36 & 61.01 & 70.37 & 26.93 & 48.65 \\
        \bottomrule
    \end{tabular}
    }
\end{table*}

\subsection{Objective Testing Results}
\label{sec:exp-objective}

\textbf{LLMs exhibit strong world knowledge in response to gender-related queries but show room for improvement in race-related queries.}
Table~\ref{tab:Sfact} illustrates that WizardLM-2 and LLaMA-3.2 achieve the highest performance on gender-related queries, while GPT-4o outperforms other models in race-related queries.
Despite achieving approximately $90$ $S_{fact}$ in gender-related queries, GPT-4o attains an $S_{fact}$ score of only $54.6$ for race-related queries.
This discrepancy may stem from the more diverse categorizations of race and the varying definitions adopted by different organizations.
As expected, $S_{fair}$ scores are relatively lower for these objective queries as shown in Table~\ref{tab:Sfair}.
Given that $S_{KLD} \approx 0$, $S_{fair}$ closely align with $S_E$.
Although high fairness scores are not anticipated in objective tests, Qwen-2.5 achieves a higher $S_{fair}$ while maintaining comparable $S_{fact}$.

\textbf{T2I models exhibit lower $S_{fact}$ scores, approaching the performance of random guessing, yet they do not necessarily achieve high $S_E$ scores.}
As shown in Table~\ref{tab:Sfact}, T2I models underperform in $S_{fact}$ compared to the LLMs, suggesting a deficiency in the their ability to understand reality.
This limitation may stem from the absence of world knowledge in their training data.
One might expect that the randomness shown in $S_{fact}$ would correspond to higher $S_{E}$ scores.
However, Table~\ref{tab:SE} reveals a significant variability in $S_{E}$ across models.
Midjourney performs the worst in this metric, scoring $64.4$ for gender-related queries and $55.53$ for race-related queries.
However, its $S_{KLD}$ remains high at $89.5$, suggesting that it generates a consistent demographic distribution across different queries, leading to an overall high fairness score.
In terms of $S_{fair}$, the only model that performs notably poorly is SDXL on race-related queries, as it achieves low scores in both $S_E$ and $S_{KLD}$.

\subsection{Subjective Testing Results}
\label{sec:exp-subjective}

\textbf{LLMs exhibit strong performance with minimal influence from cognitive error contexts, achieving high fairness scores.}
Table~\ref{tab:Sfact} and~\ref{tab:Sfair} also present the $S_{fact}$ and $S_{fair}$ scores of LLMs for both the baseline and three cognitive error context scenarios.
Despite the introduction of stereotype-inducing contexts, LLMs appear largely unaffected.
We observe an increase in $S_{fair}$ alongside a decrease in $S_{fact}$, empirically confirming the trade-off between fairness and factuality.
Specifically, $S_{fact}$ declines to approximately random guessing, while $S_{fair}$ approaches $100$.
The only exception occurs in representativeness bias scenarios, where all LLMs exhibit relatively lower $S_E$ and $S_{KLD}$ but higher $S_{fact}$.
These findings suggest that LLMs are more influenced by concrete statistical evidence than by prior experiences or subjective values and preference over certain demographic groups.

\textbf{T2I models generally exhibit slight increases in $S_{fair}$ when tested with subjective queries compared to objective ones.}
Notably, Midjourney and Flux-1.1 show decreased fairness scores for race-related queries, with Flux-1.1 experiencing a more pronounced drop from $81.2$ to $30.4$.
This decline is attributed to Flux being the only model that decreases both $S_E$ and $S_{KLD}$.
Focusing on $S_E$, except for DALL-E 3 and Midjourneyâ€™s performance on gender-related queries, the overall trend indicates declining scores, suggesting increased bias in response to subjective queries.
However, the rise in $S_{KLD}$ contributes to improved overall fairness scores for some models.
Among T2I models, DALL-E 3 continues to perform best, yielding results closest to the ideal scenario.
Notably, SDXL-Turbo exhibits a significant disparity in $S_E$ between race- and gender-related queries, with race-related results demonstrating a pronounced lack of diversity.
Overall, T2I models' performance in $S_E$ remains suboptimal, likely due to inherent cognitive limitations that require further refinement.