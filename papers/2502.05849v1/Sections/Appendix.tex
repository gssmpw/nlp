\onecolumn
\appendix

\section{Proof of the Accuracy-Entropy Trade-Off}
\label{sec:proof}

When the accuracy of a $k$-choice query is $a$, the distribution of responses from a LLM should follow $\{p_1, \cdots, p_{i-1}, a, p_{i+1}, \cdots, p_k\}$, where the ground truth for this query is $i$ and $p_i = a$.
We aim to maximize:
\begin{equation}
    - \sum_{\substack{j=1, \cdots, k \\ j \neq i}} p_j \log p_j - a \log a,
\end{equation}
subject to the constraint:
\begin{equation}\label{eq:2}
    \sum_{\substack{j=1, \cdots, k \\ j \neq i}} p_j = 1 - a.
\end{equation}
The Lagrangian function is defined as:
\begin{equation}
    \mathcal{L}(p_1, \dots, p_{i - 1}, p_{i + 1}, \dots, p_k, \lambda) = - \sum_{\substack{j=1, \cdots, k \\ j \neq i}} p_j \log p_j + \lambda \left( \sum_{\substack{j=1, \cdots, k \\ j \neq i}} p_j - (1 - a) \right).
\end{equation}
By taking the derivative with respect to each $p_j$ and setting it to zero, we obtain:
\begin{align}
    \frac{\partial \mathcal{L}}{\partial p_j} = - (\log p_j + 1) + \lambda & = 0, \\
    \log p_j & = \lambda - 1, \\
    p_j & = e^{\lambda - 1}.
\end{align}
Considering the constraint in Eq.~\ref{eq:2}, we have:
\begin{align}
    (k - 1) \cdot e^{\lambda - 1} & = 1 - a, \\
    e^{\lambda - 1} & = \frac{1 - a}{k - 1}, \\
    p_j & = \frac{1 - a}{k - 1}, \forall j \in \{1, \cdots, k\}, j \neq i.
\end{align}
Thus, the expected maximum entropy is:
\begin{align}
    & - (k - 1) \frac{1 - a}{k - 1} \log \frac{1 - a}{k - 1} - a \log a, \\
    = & - (1 - a) \log \frac{1 - a}{k - 1} - a \log a.
\end{align}

\clearpage

\section{Quantitative Results}
\label{sec:scores}

In all figures in this section, ``S-B'' denotes the base scenario in subjective queries. `S-R`'' denotes the scenarios with contexts of representativeness bias. ``S-A'' represents the scenarios with contexts of attribution error. ``S-G'' represents the scenarios with contexts of in-group/out-group bias. ``O'' and ``S'' denote objective queries and subjective queries, respectively.

\begin{table*}[h!]
    \centering
    \caption{$S_{fact}$ of all LLMs and T2I models using both objective and subjective queries. \textbf{Bold} indicates the highest value, while \underline{underline} represents the second highest.}
    \label{tab:Sfact}
    \resizebox{1.0\linewidth}{!}{
    \begin{tabular}{llccccc|lcc}
        \toprule
        & \textbf{(a) LLM} & \textbf{O} & \textbf{S-B} & \textbf{S-R} & \textbf{S-A} & \textbf{S-G} & \textbf{(b) T2I Model} & \textbf{O} & \textbf{S} \\
        \midrule
        \multirow{6}{*}{\rotatebox{90}{\bf Gender}} & GPT-3.5-Turbo-0125 & 84.44 & \underline{53.33} & \textbf{67.24} & 53.17 & 53.35 & Midjourney & 48.90 & \underline{51.10} \\
        & GPT-4o-2024-08-06 & \underline{95.56} & \textbf{54.39} & 63.88 & \textbf{54.81} & \textbf{57.03} & DALL-E 3 & \textbf{58.40} & \textbf{55.83} \\
        & Gemini-1.5-Pro & 94.44 & 52.35 & 66.22 & \underline{54.52} & 53.31 & SDXL-Turbo & \underline{51.97} & 48.37 \\
        & LLaMA-3.2-90B-Vision-Instruct & \textbf{96.67} & 53.18 & 64.78 & 52.87 & 52.76 & Flux-1.1-Pro & 49.07 & 48.67 \\
        & WizardLM-2-8x22B & \textbf{96.67} & 52.63 & 64.64 & 52.90 & \underline{55.13} \\
        & Qwen-2.5-72B-Instruct & 91.11 & 53.30 & \underline{66.65} & 52.08 & 54.12 \\
        \midrule
        \multirow{6}{*}{\rotatebox{90}{\bf Race}} & GPT-3.5-Turbo-0125 & 39.81 & \textbf{33.33} & \textbf{48.78} & 28.71 & \underline{30.73} & Midjourney & \underline{25.36} & \underline{22.36} \\
        & GPT-4o-2024-08-06 & \textbf{54.62} & 29.73 & 47.09 & \underline{29.59} & 30.46 & DALL-E 3 & \textbf{30.33} & \textbf{27.78} \\
        & Gemini-1.5-Pro & 44.44 & 31.28 & 42.94 & \textbf{30.39} & \textbf{31.04} & SDXL-Turbo & 22.50 & 19.75 \\
        & LLaMA-3.2-90B-Vision-Instruct & 47.22 & \underline{31.62} & 45.71 & 28.23 & 29.54 & Flux-1.1-Pro & 23.50 & 21.08 \\
        & WizardLM-2-8x22B & 44.44 & 27.44 & 45.48 & 27.42 & 29.79 \\
        & Qwen-2.5-72B-Instruct & \underline{52.78} & 26.04 & \underline{48.63} & 28.31 & 30.53 \\
        \bottomrule
    \end{tabular}
    }
\end{table*}

\begin{table*}[h!]
    \centering
    \caption{$S_{fair}$ of all LLMs and T2I models using both objective and subjective queries. \textbf{Bold} indicates the highest value, while \underline{underline} represents the second highest.}
    \label{tab:Sfair}
    \resizebox{1.0\linewidth}{!}{
    \begin{tabular}{llccccc|lcc}
        \toprule
        & \textbf{(a) LLM} & \textbf{O} & \textbf{S-B} & \textbf{S-R} & \textbf{S-A} & \textbf{S-G} & \textbf{(b) T2I Model} & \textbf{O} & \textbf{S} \\
        \midrule
        \multirow{6}{*}{\rotatebox{90}{\bf Gender}} & GPT-3.5-Turbo-0125 & \textbf{21.43} & 99.86 & 94.10 & \textbf{99.98} & \underline{99.96} & Midjourney & 96.25 & \textbf{99.00} \\
        & GPT-4o-2024-08-06 & 3.06 & 99.81 & 94.23 & 99.85 & 99.68 & DALL-E 3 & 92.54 & 96.35 \\
        & Gemini-1.5-Pro & 3.06 & 99.89 & 92.86 & 99.86 & 99.89 & SDXL-Turbo & \underline{97.89} & \underline{98.61} \\
        & LLaMA-3.2-90B-Vision-Instruct & 6.12 & \textbf{99.94} & 94.78 & \underline{99.97} & \textbf{99.97} & Flux-1.1-Pro & \textbf{98.72} & 91.66 \\
        & WizardLM-2-8x22B & \underline{9.18} & \underline{99.91} & \textbf{96.90} & 99.94 & 99.91 \\
        & Qwen-2.5-72B-Instruct & \textbf{21.43} & 99.89 & \underline{95.52} & 99.96 & 99.94 \\
        \midrule
        \multirow{6}{*}{\rotatebox{90}{\bf Race}} & GPT-3.5-Turbo-0125 & \underline{13.49} & 97.80 & 90.34 & 99.16 & 97.80 & Midjourney & \underline{81.65} & \underline{75.99} \\
        & GPT-4o-2024-08-06 & 3.54 & 98.59 & 89.35 & 98.50 & 98.27 & DALL-E 3 & \textbf{82.88} & \textbf{84.93} \\
        & Gemini-1.5-Pro & 6.02 & \textbf{98.86} & \textbf{94.42} & 98.89 & \underline{98.49} & SDXL-Turbo & 62.85 & 74.40 \\
        & LLaMA-3.2-90B-Vision-Instruct & \textbf{13.93} & \underline{98.70} & 92.55 & 99.06 & \underline{98.49} & Flux-1.1-Pro & 81.19 & 30.36 \\
        & WizardLM-2-8x22B & 12.21 & 98.49 & \underline{93.80} & \underline{99.23} & \textbf{98.50} \\
        & Qwen-2.5-72B-Instruct & 9.56 & 98.59 & 89.31 & \textbf{99.40} & 98.28 \\
        \bottomrule
    \end{tabular}
    }
\end{table*}

\begin{table*}[h!]
    \centering
    \caption{$S_{E}$ of all LLMs and T2I models using both objective and subjective queries. \textbf{Bold} indicates the highest value, while \underline{underline} represents the second highest.}
    \label{tab:SE}
    \resizebox{1.0\linewidth}{!}{
    \begin{tabular}{llccccc|lcc}
        \toprule
        & \textbf{(a) LLM} & \textbf{O} & \textbf{S-B} & \textbf{S-R} & \textbf{S-A} & \textbf{S-G} & \textbf{(b) T2I Model} & \textbf{O} & \textbf{S} \\
        \midrule
        \multirow{6}{*}{\rotatebox{90}{\bf Gender}} & GPT-3.5-Turbo-0125 & \textbf{21.43} & 97.45 & 83.88 & \underline{98.88} & \underline{98.58} & Midjourney & 64.36 & 74.43 \\
        & GPT-4o-2024-08-06 & 3.06 & 97.10 & 83.85 & 97.57 & 96.39 & DALL-E 3 & \underline{82.24} & \textbf{87.30} \\
        & Gemini-1.5-Pro & 3.06 & \underline{97.86} & 82.00 & 97.61 & 97.83 & SDXL-Turbo & 81.90 & \underline{82.85} \\
        & LLaMA-3.2-90B-Vision-Instruct & 6.12 & \textbf{98.32} & 84.73 & \textbf{98.89} & \textbf{98.88} & Flux-1.1-Pro & \textbf{85.28} & 67.12 \\
        & WizardLM-2-8x22B & \underline{9.18} & 97.73 & \textbf{88.39} & 98.46 & 98.11 \\
        & Qwen-2.5-72B-Instruct & \textbf{21.43} & 97.51 & \underline{86.18} & 98.60 & 98.32 \\
        \midrule
        \multirow{6}{*}{\rotatebox{90}{\bf Race}} & GPT-3.5-Turbo-0125 & \underline{13.49} & 92.96 & 83.12 & 95.71 & 93.02 & Midjourney & 55.53 & 55.32 \\
        & GPT-4o-2024-08-06 & 3.54 & 94.28 & 82.33 & 93.95 & 93.95 & DALL-E 3 & \textbf{79.21} & \textbf{74.83} \\
        & Gemini-1.5-Pro & 6.02 & \textbf{94.96} & \underline{86.58} & 94.98 & 94.25 & SDXL-Turbo & 45.98 & 39.75 \\
        & LLaMA-3.2-90B-Vision-Instruct & \textbf{13.93} & \underline{94.61} & 84.62 & 95.29 & \underline{94.30} & Flux-1.1-Pro & \underline{68.74} & \underline{57.40} \\
        & WizardLM-2-8x22B & 12.21 & 94.29 & \textbf{86.82} & \underline{95.85} & \textbf{94.58} \\
        & Qwen-2.5-72B-Instruct & 9.56 & 94.35 & 81.69 & \textbf{96.48} & 94.04 \\
        \bottomrule
    \end{tabular}
    }
\end{table*}

\begin{table*}[h!]
    \centering
    \caption{$S_{KLD}$ of all LLMs and T2I models using both objective and subjective queries. \textbf{Bold} indicates the highest value, while \underline{underline} represents the second highest.}
    \label{tab:SKLD}
    \resizebox{1.0\linewidth}{!}{
    \begin{tabular}{llccccc|lcc}
        \toprule
        & \textbf{(a) LLM} & \textbf{O} & \textbf{S-B} & \textbf{S-R} & \textbf{S-A} & \textbf{S-G} & \textbf{(b) T2I Model} & \textbf{O} & \textbf{S} \\
        \midrule
        \multirow{6}{*}{\rotatebox{90}{\bf Gender}} & GPT-3.5-Turbo-0125 & $<10^{-6}$ & 94.66 & 63.4 & \textbf{97.79} & \underline{96.99} & Midjourney & \underline{89.48} & \textbf{96.10} \\
        & GPT-4o-2024-08-06 & $<10^{-6}$ & 93.54 & 64.28 & 93.82 & 91.04 & DALL-E 3 & 57.98 & 71.26 \\
        & Gemini-1.5-Pro & $<10^{-6}$ & 94.75 & 60.31 & 93.95 & 94.78 & SDXL-Turbo & 88.33 & \underline{91.91} \\
        & LLaMA-3.2-90B-Vision-Instruct & $<10^{-6}$ & \textbf{96.22} & 65.77 & \underline{97.49} & \textbf{97.25} & Flux-1.1-Pro & \textbf{91.33} & 74.64 \\
        & WizardLM-2-8x22B & $<10^{-6}$ & \underline{95.82} & \textbf{73.26} & 96.13 & 95.30 \\
        & Qwen-2.5-72B-Instruct & $<10^{-6}$ & 95.65 & \underline{67.62} & 96.85 & 96.33 \\
        \midrule
        \multirow{6}{*}{\rotatebox{90}{\bf Race}} & GPT-3.5-Turbo-0125 & $<10^{-6}$ & 68.77 & 42.76 & 80.50 & 68.52 & Midjourney & \textbf{58.73} & \underline{46.26} \\
        & GPT-4o-2024-08-06 & $<10^{-6}$ & 75.34 & 39.75 & 75.18 & 71.43 & DALL-E 3 & 17.67 & 40.12 \\
        & Gemini-1.5-Pro & $<10^{-6}$ & \textbf{77.42} & \textbf{58.43} & 77.92 & \textbf{73.74} & SDXL-Turbo & 31.23 & \textbf{57.52} \\
        & LLaMA-3.2-90B-Vision-Instruct & $<10^{-6}$ & \underline{75.83} & 51.56 & 80.06 & \underline{73.51} & Flux-1.1-Pro & \underline{39.82} & 30.29 \\
        & WizardLM-2-8x22B & $<10^{-6}$ & 73.51 & \underline{53.00} & \underline{81.48} & 72.39 \\
        & Qwen-2.5-72B-Instruct & $<10^{-6}$ & 75.12 & 41.61 & \textbf{82.92} & 71.11 \\
        \bottomrule
    \end{tabular}
    }
\end{table*}

\begin{table*}[h!]
    \centering
    \caption{$d$: Distance to the theoretical maximum of all LLMs and T2I models using both objective and subjective queries. \textbf{Bold} indicates the lowest value, while \underline{underline} represents the second lowest.}
    \label{tab:distance}
    \resizebox{1.0\linewidth}{!}{
    \begin{tabular}{llcccccc|lccc}
        \toprule
        & \textbf{(a) LLM} & \textbf{O} & \textbf{S-B} & \textbf{S-R} & \textbf{S-A} & \textbf{S-G} & \bf Avg. & \textbf{(b) T2I Model} & \textbf{O} & \textbf{S} & \bf Avg. \\
        \midrule
        \multirow{6}{*}{\rotatebox{90}{\bf Gender}} & GPT-3.5-Turbo-0125 & 11.89 & 2.18 & 4.80 & \textbf{0.82} & \underline{1.07} & 4.15 & Midjourney & 29.14 & 23.27 & 26.21 \\
        & GPT-4o-2024-08-06 & 4.10 & 2.26 & 7.44 & 1.69 & 2.00 & 3.50 & DALL-E 3 & \textbf{12.61} & \textbf{10.51} & \textbf{11.56} \\
        & Gemini-1.5-Pro & 5.20 & 3.55 & 5.99 & 1.70 & 1.74 & 3.64 & SDXL-Turbo & 17.14 & \underline{16.52} & \underline{16.83} \\
        & LLaMA-3.2-90B-Vision-Instruct & \underline{2.59} & \textbf{1.37} & 6.18 & \underline{0.86} & \textbf{0.89} & \underline{2.38} & Flux-1.1-Pro & \underline{14.58} & 27.49 & 21.04 \\
        & WizardLM-2-8x22B & \textbf{2.14} & \underline{2.04} & \underline{3.85} & 1.28 & \underline{1.07} & \textbf{2.08} \\
        & Qwen-2.5-72B-Instruct & 5.37 & 2.14 & \textbf{3.82} & 1.27 & 1.16 & 2.75 \\
        \midrule
        \multirow{6}{*}{\rotatebox{90}{\bf Race}} & GPT-3.5-Turbo-0125 & 53.17 & 5.51 & \underline{5.79} & \underline{3.99} & 6.21 & 14.93 & Midjourney & 41.97 & 44.05 & 43.01 \\
        & GPT-4o-2024-08-06 & \underline{42.97} & \underline{5.21} & 7.49 & 5.56 & 5.38 & \underline{13.32} & DALL-E 3 & \textbf{19.40} & \textbf{24.44} & \textbf{21.92} \\
        & Gemini-1.5-Pro & 51.72 & 6.66 & 7.53 & 6.95 & 5.36 & 15.64 & SDXL-Turbo & 50.80 & 56.98 & 53.89 \\
        & LLaMA-3.2-90B-Vision-Instruct & 46.20 & \textbf{4.45} & 6.58 & 4.48 & \underline{5.23} & 13.39 & Flux-1.1-Pro & \underline{25.74} & \underline{30.36} & \underline{28.05} \\
        & WizardLM-2-8x22B & 49.42 & 5.57 & \textbf{4.98} & 4.02 & \textbf{4.91} & 13.78 \\
        & Qwen-2.5-72B-Instruct & \textbf{42.67} & 5.63 & 6.96 & \textbf{3.29} & 5.27 & \textbf{12.76} \\
        \bottomrule
    \end{tabular}
    }
\end{table*}

\clearpage

\section{Illustration of Diverse Scenarios in Subjective Queries}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.85\linewidth]{Figures/diversity.pdf}
    \caption{{\methodname} offers diverse scenarios in subjective queries to evaluate models' fairness.}
    \label{fig:diversity}
\end{figure}

\clearpage

\section{Visualization of Model Performance}

\begin{figure*}[h!]
  \centering
  \subfloat[LLMs tested with objective queries.]{
    \includegraphics[width=0.48\textwidth]{Figures/llm-obj.pdf}
    \label{fig:llm-obj}
  }
  \subfloat[T2I Models tested with objective queries.]{
    \includegraphics[width=0.48\textwidth]{Figures/t2i-obj.pdf}
    \label{fig:t2i-obj}
  } \\
  \subfloat[LLMs tested with subjective queries.]{
    \includegraphics[width=0.48\textwidth]{Figures/llm-subj.pdf}
    \label{fig:llm-subj}
  }
  \subfloat[T2I Models tested with subjective queries.]{
    \includegraphics[width=0.48\textwidth]{Figures/t2i-subj.pdf}
    \label{fig:t2i-subj}
  }
  \caption{$S_{fair}$ and $S_{fact}$ of six LLMs and four T2I models using {\methodname}.}
\end{figure*}

\begin{figure*}[h!]
  \centering
  \subfloat[w/o context (base scenarios).]{
    \includegraphics[width=0.48\textwidth]{Figures/base.pdf}
    \label{fig:llm-subj-base}
  }
  \subfloat[w/ Representativeness bias contexts.]{
    \includegraphics[width=0.48\textwidth]{Figures/representativeness.pdf}
    \label{fig:llm-subj-representativeness}
  } \\
  \subfloat[w/ Attribution error contexts.]{
    \includegraphics[width=0.48\textwidth]{Figures/attribution.pdf}
    \label{fig:llm-subj-attribution}
  }
  \subfloat[w/ In-group/out-group bias contexts.]{
    \includegraphics[width=0.48\textwidth]{Figures/group.pdf}
    \label{fig:llm-subj-group}
  }
  \caption{$S_{fair}$ and $S_{fact}$ of six LLMs using subjective queries with different contexts.}
\end{figure*}

\clearpage

% \section{Domains and Scenarios Covered in {\methodname}}

% \begin{figure}[h!]
%     \centering
%     \includegraphics[width=0.75\linewidth]{Figures/diversity.pdf}
%     \caption{Diverse scenarios covered by subjective queries in {\methodname}.}
%     \label{fig:cover}
% \end{figure}

% \clearpage

\section{Racial Information in the Statistics}
\label{sec:def}

\begin{table}[h!]
    \centering
    \caption{Racial classifications for each statistic. \textbf{Asian} includes Asian, Pacific Islander, and Native Hawaiian. \textbf{Black} is sometimes called Africa American. \textbf{Hispanic} is sometimes called Latino/Latina. Other categories, such as ``Multiple Races'' and ``Other'', are omitted.}
    \label{tab:statistics-class}
    % \resizebox{1.0\linewidth}{!}{
    \begin{tabular}{llll}
    \toprule
    & \bf Statistics & \bf Gender & \bf Race \\
    \midrule
    \multirow{6}{*}{\rotatebox{90}{\bf Economic}} & Employment Rate & Female, Male & Asian, Black, Hispanic, White \\
    & Unemployment Rate & Female, Male & Asian, Black, Hispanic, White \\
    & Weekly Income & Female, Male & Asian, Black, Hispanic, White \\
    & Poverty Rate & Female, Male & Asian, Black, Hispanic, White \\
    & Homeownership Rate & N/A & Asian, Black, Hispanic, White \\
    & Homelessness Rate & Female, Male & Asian, Black, Hispanic, White \\
    \midrule
    \multirow{5}{*}{\rotatebox{90}{\bf Social}} & Educational Attainment & Female, Male & Asian, Black, Hispanic, White \\
    & Voter Turnout Rate & N/A & Asian, Black, Hispanic, White \\
    & Volunteer Rate & Female, Male & N/A \\
    & Crime Rate & Female, Male & Asian, Black, Hispanic, White \\
    & Insurance Coverage Rate & Female, Male & Asian, Black, Hispanic, White \\
    \midrule
    \multirow{8}{*}{\rotatebox{90}{\bf Health}} & Life Expectancy & Female, Male & Asian, Black, Hispanic, White \\
    & Mortality Rate & Female, Male & Asian, Black, Hispanic, White \\
    & Obesity Rate & N/A & Asian, Black, Hispanic, White \\
    & Diabetes Rate & Female, Male & Asian, Black, Hispanic, White \\
    & HIV Rate & Female, Male & Asian, Black, Hispanic, White \\
    & Cancer Incidence Rate & Female, Male & Asian, Black, Hispanic, White \\
    & Influenza Hospitalization Rate & N/A & Asian, Black, Hispanic, White \\
    & COVID-19 Mortality Rate & Female, Male & Asian, Black, Hispanic, White \\
    \bottomrule
    \end{tabular}
    % }
\end{table}