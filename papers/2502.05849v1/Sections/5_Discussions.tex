\section{Discussion}

\begin{table*}[t]
    \centering
    \caption{Percentage of cases where LLMs' choices are in the same demographic group with the contexts, averaged across all statistics. \textbf{Bold} indicates the lowest value, while \underline{underline} represents the second lowest.}
    \label{tab:context}
    \resizebox{1.0\linewidth}{!}{
    \begin{tabular}{lcccccccccccc}
        \toprule
        \multirow{2}{*}{\textbf{Model}} & \multicolumn{2}{c}{\textbf{R. Bias High}} & \multicolumn{2}{c}{\textbf{R. Bias Low}} & \multicolumn{2}{c}{\textbf{Attr. Err.}} & \multicolumn{2}{c}{\textbf{In-G. Bias}} & \multicolumn{2}{c}{\textbf{Out-G. Bias}} & \multicolumn{2}{c}{\textbf{Avg. Increase}} \\
        \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9} \cmidrule(lr){10-11} \cmidrule(lr){12-13}
        & \bf Gender & \bf Race & \bf Gender & \bf Race & \bf Gender & \bf Race & \bf Gender & \bf Race & \bf Gender & \bf Race & \bf Gender & \bf Race \\
        \midrule
        GPT-3.5 & 69.10 & 53.33 & 65.38 & 44.23 & \textbf{54.04} & 41.18 & 53.47 & 35.14 & \textbf{52.57} & \textbf{78.78} & $\uparrow$8.91 & $\uparrow$15.53 \\
        GPT-4o & \textbf{66.26} & 49.58 & \underline{61.55} & 44.66 & \underline{54.98} & 40.09 & \textbf{50.99} & \underline{29.80} & 55.76 & 80.38 & \textbf{$\uparrow$7.91} & $\uparrow$13.90 \\
        Gemini-1.5 & 69.65 & \textbf{44.37} & 62.79 & \textbf{41.49} & 55.85 & \textbf{35.37} & 54.47 & \textbf{28.87} & 56.08 & 81.54 & $\uparrow$9.77 & \textbf{$\uparrow$11.32} \\
        LLaMA-3.2 & \underline{67.18} & 49.72 & 62.42 & \underline{41.76} & 55.78 & \underline{39.30} & 54.51 & 32.38 & 55.17 & \underline{80.08} & $\uparrow$9.01 & $\uparrow$13.65 \\
        WizardLM-2 & 68.16 & \underline{45.62} & \textbf{61.13} & 45.33 & 55.18 & 39.42 & 53.32 & 31.07 & 55.57 & 80.29 & \underline{$\uparrow$8.67} & \underline{$\uparrow$13.35} \\
        Qwen-2.5 & 69.94 & 52.19 & 63.37 & 45.06 & 57.19 & 43.73 & \underline{52.79} & 30.83 & \underline{54.18} & 80.09 & $\uparrow$9.49 & $\uparrow$15.38 \\
        \bottomrule
    \end{tabular}
    }
\end{table*}

\subsection{Cognitive Errors in LLMs}

We are particularly interested in whether large language models (LLMs) are influenced by cognitive error contexts, specifically how these contexts affect their decision-making.
To investigate this, we calculate the percentage of instances in which LLMs’ responses align with the demographic group shown in recent news for attribution error test cases.
For representativeness bias, we compute the percentage where LLMs select the highest/lowest demographic group in response to corresponding questions.
For in-group and out-group bias, we analyze two distinct conditions:
(1) whether positive attributes are associated with in-groups—for example, when asked about a positive statistic such as a low crime rate, whether the LLM selects an option corresponding to its assigned identity; and
(2) whether negative attributes are associated with out-groups—for instance, when asked about a negative statistic such as a high crime rate, whether the LLM selects an option differing from its assigned identity.

Table~\ref{tab:context} shows the results, with detailed gender and race results.
The baseline for gender is $50\%$, while it is $25\%$ for race, except in the out-group bias scenario, where it is $75\%$.
The last column presents the increase relative to this baseline.
GPT-4o and Gemini-1.5 exhibit the least susceptibility to cognitive errors related to gender and race, respectively, yet they are still affected in $7.9\%$ and $11.3\%$ of cases.
For representativeness bias, LLMs are more significantly influenced, with an increase of $11.1\%\sim28.3\%$ over the baseline.
In summary, the context of subjective queries influence model behavior, eliciting biases or cognitive errors, highlighting the need for further improvements.