\section{Related Work}
\label{sec: app-related-work}

\paragraph{Process vs.~Outcome Supervision}
Our work is motivated by the empirical effectiveness of process supervision over outcome supervision, particularly in language model reasoning tasks \citep{cobbe2021training,uesato2022solving,lightman2023let}.
To address the challenges of cost and scalability in obtaining human-annotated process labels, recent approaches \citep{wang2024math,luo2024improve,setlur2024rewarding} have developed automated methods to generate process supervision from outcome-based signals, leveraging Q-functions and advantage functions under specific policies.
When data is provided in the form of preferences, outcome supervision is sometimes conducted with implicit rewards, as seen in works such as Direct Preference Optimization \citep{rafailov2023direct,lambert2024rewardbench,zhong2024dpo,yuan2024free}.


\paragraph{RL with Trajectory Feedback}
A closely related line of theoretical work is reinforcement learning with trajectory feedback or aggregate bandit feedback (or, bandit and semi-bandit feedback) \citep{neu2013efficient,efroni2021reinforcement,chatterji2021theory,chen2022human,cassel2024near,lancewicki2025near}, where the learner only receives trajectory-level feedback at the end of each episode. This line of work also includes preference-based RL \citep{pacchiano2021dueling,zhu2023principled,wu2023making,zhan2023provable}, which operates on trajectory-level pair preferences. While most existing works in this area focus on online exploration settings, our paper investigates offline learning and analyzes the statistical relationship between process (step-level) and outcome (trajectory-level) feedback.


\paragraph{Offline Reinforcement Learning}
Our work is most closely related to offline (batch) reinforcement learning in the classical reinforcement learning literature.
The paradigm of reinforcement learning with process-supervised data is essentially an offline RL problem, where a rich body of existing theoretical results \citep[e.g.,][]{munos2003error,antos2008learning,farahmand2010error,chen2019information,xie2020q,jin2021pessimism,xie2021batch,xie2021bellman,uehara2021pessimistic,cheng2022adversarially,xie2022armor,bhardwaj2023adversarial} can be applied to our paper, either directly for the process supervision case, or serving as subroutines in our \cref{alg:transfomration-2} for the outcome supervision case.
Within these results, \citet{chen2019information,xie2020q} develop model-free algorithms under all-policy coverage conditions, while \citet{xie2021batch} proposes a model-free approach requiring only realizability assumptions but with stronger coverage requirements. \citet{xie2021bellman,cheng2022adversarially} investigate model-free offline RL under partial coverage settings, and \citet{uehara2021pessimistic,xie2022armor,bhardwaj2023adversarial} address model-based offline RL with partial coverage.

\paragraph{Off-Policy Evaluation}
Our work also connects to the rich literature on off-policy evaluation (OPE) in reinforcement learning. A central challenge in OPE is the change of measure problem, where extensive research \citep{liu2018breaking,xie2019towards,nachum2019dualdice,uehara2020minimax} has investigated the significant distinction between state-action coverage and trajectory coverage conditions. These findings highlight the significance of our main results, particularly our change of trajectory measure lemma.


\paragraph{Reward Shaping and Internal Rewards}
A related but distinct line of research focuses on augmenting sparse reward functions to improve learning efficiency. Reward shaping techniques have been extensively employed as a method for providing denser learning signals while preserving optimal policies \citep[e.g.,][]{ng1999policy,trott2019keeping,gupta2022unpacking}. Similarly, intrinsic rewards based on prediction errors of environment dynamics \citep{pathak2017curiosity} or random networks \citep{burda2018exploration} have been proposed to tackle sparse reward settings. However, these approaches differ fundamentally from our work in their objectives -- while reward shaping and intrinsic rewards aim to improve exploration in online RL by modifying the reward landscape, our analysis focuses on the statistical properties of learning in the offline setting where the data distribution is fixed.