\documentclass{article}
\usepackage[utf8]{inputenc}

\usepackage{etoolbox}
\newcommand{\arxiv}[1]{\iftoggle{icml}{}{#1}}
\newcommand{\icml}[1]{\iftoggle{icml}{#1}{}}
\newtoggle{icml}

\usepackage{txie}
\icml{\usepackage{icml2025}}

\arxiv{
\usepackage{geometry}
\geometry{letterpaper, margin=1in}
}
\input{macro}
\input{command}

\usepackage{parskip}

\usepackage[T1]{fontenc}
\usepackage[scaled=.91]{helvet}
\usepackage{inconsolata}
\usepackage{microtype}


\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1.}}


\newcommand{\titlestr}{Do We Need to Verify Step by Step? Rethinking Process Supervision from a Theoretical Perspective}
\newcommand{\icmltitlestr}{Do We Need to Verify Step by Step?\\Rethinking Process Supervision from a Theoretical Perspective}

\arxiv{\addtocontents{toc}{\protect\setcounter{tocdepth}{0}}}


\arxiv{\title{\titlestr{}}
\author{
Zeyu Jia
\\
\normalsize
\href{mailto:zyjia@mit.edu}{\texttt{zyjia@mit.edu}}
\and
Alexander Rakhlin
\\
\normalsize
\href{mailto:rakhlin@mit.edu}{\texttt{rakhlin@mit.edu}}
\and
Tengyang Xie
\\
\normalsize
\href{mailto:tx@cs.wisc.edu}{\texttt{tx@cs.wisc.edu}}
}
\date{\today}
}

\icml{\icmltitlerunning{Process vs. Outcome Supervision}}

\begin{document}

\arxiv{
\maketitle
}

\icml{
    \twocolumn[
        \icmltitle{\texorpdfstring{\icmltitlestr{}}{\titlestr{}}}
        
        
        
        \icmlsetsymbol{equal}{*}
        
        \begin{icmlauthorlist}
        \icmlauthor{Firstname1 Lastname1}{equal,yyy}
        \icmlauthor{Firstname2 Lastname2}{equal,yyy,comp}
        \icmlauthor{Firstname3 Lastname3}{comp}
        \icmlauthor{Firstname4 Lastname4}{sch}
        \icmlauthor{Firstname5 Lastname5}{yyy}
        \icmlauthor{Firstname6 Lastname6}{sch,yyy,comp}
        \icmlauthor{Firstname7 Lastname7}{comp}
        \icmlauthor{Firstname8 Lastname8}{sch}
        \icmlauthor{Firstname8 Lastname8}{yyy,comp}
        \end{icmlauthorlist}
        
        \icmlaffiliation{yyy}{Department of XXX, University of YYY, Location, Country}
        \icmlaffiliation{comp}{Company Name, Location, Country}
        \icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}
        
        \icmlcorrespondingauthor{Firstname1 Lastname1}{first1.last1@xxx.edu}
        \icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}
        
        \icmlkeywords{Machine Learning, ICML}
        
        \vskip 0.3in
        ]
        
        
        
        \printAffiliationsAndNotice{\icmlEqualContribution} %
}


\begin{abstract}
    As large language models have evolved, it has become crucial to distinguish between process supervision and outcome supervision---two key reinforcement learning approaches to complex reasoning tasks.
    While process supervision offers intuitive advantages for long-term credit assignment, the precise relationship between these paradigms has remained an open question.
    Conventional wisdom suggests that outcome supervision is fundamentally more challenging due to the trajectory-level coverage problem, leading to significant investment in collecting fine-grained process supervision data.
\arxiv{


}
\icml{\quad}
    In this paper, we take steps towards resolving this debate.
    Our main theorem shows that, under standard data coverage assumptions, \emph{reinforcement learning through outcome supervision is no more statistically difficult than through process supervision}, up to polynomial factors in horizon.
    At the core of this result lies the novel \emph{Change of  Trajectory Measure Lemma}---a technical tool that bridges return-based trajectory measure and step-level distribution shift.
    Furthermore, for settings with access to a verifier or a rollout capability, we prove that any policy's advantage function can serve as an optimal process reward model, providing a direct connection between outcome and process supervision.
    These findings suggest that the empirically observed performance gap---if any---between outcome and process supervision likely stems from algorithmic limitations rather than inherent statistical difficulties, potentially transforming how we approach data\arxiv{ collection} and algorithm design for reinforcement learning.
\end{abstract}






\section{Introduction}

Reward signals play a central role in reinforcement learning, and it has been hypothesized that intelligence and its associated abilities could emerge naturally from the simple principle of reward maximization \citep{silver2021reward}.
Over the past decade, this idea has been powerfully demonstrated across diverse AI systems. In specialized domains like AlphaGo Zero \citep{silver2017mastering}, superhuman performance has been achieved by maximizing simple, well-defined environmental reward signals. The paradigm has also proven transformative for general-purpose AI systems, particularly in training large language models (LLMs) using reinforcement learning \citep{ouyang2022training,bai2022training,jaech2024openai}. However, for these more open-ended systems, the challenge of reward specification is significantly more complex, requiring reward signals to be learned from human-annotated data through reward modeling rather than being manually specified.

This challenge of reward specification has led to the emergence of two fundamental supervision paradigms in reinforcement learning \citep[e.g.,][]{uesato2022solving,lightman2023let}:
\begin{itemize}
    \item \emph{Outcome supervision:} Reward feedback is provided only after the final output, based on the final outcomes or---in the case of LLMs---overall quality of the model's chain-of-thought (CoT).
    \item \emph{Process supervision:} Fine-grained reward feedback is provided based on the quality of each intermediate step (e.g., correctness of each step in the CoT in the case of LLMs).
\end{itemize}

The choice between these paradigms represents a fundamental trade-off in reinforcement learning system design. Process supervision offers several intuitive advantages: it provides more granular feedback, enables better interpretability of model decisions, and potentially allows for more efficient credit assignment in long reasoning chains. These benefits have led to significant investment in collecting step-by-step feedback data, despite the substantial human effort required \citep{lightman2023let}. The granular nature of process supervision also aligns with how humans often learn and teach—through step-by-step guidance rather than just final outcomes.

However, the high cost of collecting process supervision data raises important questions about its necessity. Outcome supervision, while providing less detailed feedback, offers practical advantages in terms of data collection efficiency and scalability. It also reflects various natural settings of human learning where detailed step-by-step feedback may not be available (e.g., game-playing). Recent empirical advances in automated process supervision derived from outcome supervision \citep{wang2024math,luo2024improve,zhong2024dpo,yuan2024free,setlur2024rewarding} or in directly learning from outcome supervision \citep{guo2025deepseek} suggest that the statistical benefits of process supervision might not be as fundamental as previously thought.

This tension between process and outcome supervision touches on deeper questions in machine learning and cognitive science: How much explicit supervision is truly necessary for effective learning? Can systems learn optimal behavior from sparse reward signals, or is step-by-step guidance fundamentally necessary? Understanding these questions has important implications not just for practical system design, but also for our broader understanding of learning and intelligence.


\subsection{Our Results}

This paper examines the statistical performance of reinforcement learning under outcome supervision---an emerging paradigm that has garnered significant attention in large language model research. Our findings challenge conventional wisdom that outcome supervision is inherently more difficult than process supervision due to its coarser feedback.
\begin{enumerate}[label=(\arabic*)]
\item Our main results (\cref{sec: orm-reward}) demonstrate that given a dataset of trajectories with only cumulative rewards (as in outcome supervision), we can transform the data into trajectories with per-step rewards, while only paying an additive error that scales with the \textit{state-action concentrability}. As a result, we can transform any algorithm that takes trajectories with per-step rewards as input into an algorithm that takes trajectories with total rewards as input, with essentially no loss in statistical performance up to polynomial factors of the horizon.
\item We also provide (\cref{sec: advantage}) a theoretical analysis of using Q-functions or advantage functions as reward functions, a popular approach in practice for mimicking process supervision from outcome supervision data \citep[e.g.,][]{wang2024math,luo2024improve,setlur2024rewarding}. We prove that the advantage function of \emph{any policy} can serve as an optimal process reward model; in contradistinction, using Q-functions could lead to sub-optimal results.
\end{enumerate}
Beyond these two main messages, we make the following contributions:
\begin{enumerate}[label=(\arabic*)]
    \item Our key technical contribution---\emph{Change of Trajectory Measure Lemma} (\cref{lem:orm-reward})---is applicable beyond our main results. The change of measure is a fundamental operation in analyzing off-policy evaluation \citep[e.g.,][]{uehara2020minimax}, offline reinforcement learning \citep[e.g.,][]{xie2021bellman}, and out-of-domain generalization \citep{dong2022first}. To our knowledge, \cref{lem:orm-reward} presents the first result concerning trajectory-level change of measure via step-level distribution shift.
    \item We also extend our main results to the setting of preference-based reinforcement learning (\cref{sec: dpo}). Namely, we transform preference-based trajectory data, generated according to the Bradley-Terry model, into a dataset of trajectories with per-step reward. In particular, for direct preference optimization \citep{rafailov2023direct}, we improve the previous analyses and show that its sample complexity only scales with the state-action concentrability coefficients instead of trajectory concentrability coefficients---potentially, an exponential improvement.
\end{enumerate}


\subsection{Notation}
We use $a_n\lesssim b_n$ or $a_n = \mathcal{O}(b_n)$ whenever there exists some universal positive constant $c$ such that $a_n\le c\cdot b_n$. We use $\sigma: \RR\to [0, 1]$ to denote the sigmoid function $x\mapsto\sigma(x) = \exp(x)/(1 + \exp(x))$.







\section{Background}
In this section, we introduce key prerequisite concepts. We begin with the basics of Markov Decision Processes (\pref{sec: prem-mdp}). We then discuss the two aforementioned supervision paradigms in reinforcement learning (\pref{sec: prem-reward}). Finally, we review the concepts of state-action and trajectory concentrability (\pref{sec: prem-concentrability}).

\subsection{Markov Decision Processes}\label{sec: prem-mdp}
An MDP $M$ consists of a tuple $(\calS, \calA, P, r^\star, H)$. Here $H\in \ZZ_+$ denotes the horizon, $\calS = \cup_{h=1}^H \calS_h$ denotes the layered state space, $\calA$ denotes the action space, $P = (P_1, \cdots, P_H)$ with $P_h: \calS_h\times\calA\to \Delta(\calS_{h+1})$ denoting the transition model, and $r^\star: \calS\times\calA\to [0, 1]$ 
is the deterministic ground truth reward model.
For simplicity, we let $s_1\in\Scal_1$ be a fixed initial state.

For a trajectory $\tau = (s_1, a_1, s_2, a_2, \cdots, s_H, a_H)$, we write $r(\tau)\coloneqq \sum_{h=1}^H r(s_h, a_h)$ to denote the total reward accumulated along the trajectory under deterministic reward model $r:\Scal\times \Acal\to\RR$ (which can be the ground truth reward model $r^\star$ or any learned reward model $\widehat r$). A (Markov) policy $\pi: \Scal \to \Delta(\Acal)$ is a mapping from the state space $\calS$ to a distribution on the action space $\calA$. The notation $\PP_{\tau\sim \pi}$ and $\EE_{\tau\sim \pi}$ stands for the probability and expectation with respect to trajectories $\tau$ sampled according to policy $\pi$ within the transition model given by $P$, starting from a fixed state $s_1$. For any given policy $\pi$, the occupancy measure of any state $s_h\in \calS_h$ in layer $h$ and any state-action pair $(s_h, a_h)\in \calS_h\times\calA$ are defined, respectively, as $d^\pi(s_h) \coloneqq \PP_{\tau\sim \pi} (s_h\in \tau)$ and $d^\pi(s_h, a_h) \coloneqq \PP_{\tau\sim \pi} ((s_h, a_h)\in \tau)$.
Additionally, we define the trajectory occupancy measure for any trajectory $\tau$, $d^\pi(\tau) \coloneqq \PP_{\tau'\sim \pi}(\tau = \tau')$, as well as $\pi(\tau) \coloneqq \prod_{(s_h, a_h)\in \tau} \pi(a_h \mid s_h)$ (note that $d^\pi(\tau)$ and $\pi(\tau)$ are different when transitions is stochastic). For any policy $\pi$, we use $J(\pi)$ to denote the expected total reward of trajectories collected by $\pi$ under the ground truth reward $r^\star$, i.e., $J(\pi) \coloneqq \EE_{\tau\sim \pi}[r^\star(\tau)]$.
For a specific reward model $r$, we use $J_{r}(\pi)$ to denote the expected total reward of trajectories collected by $\pi$ under reward $r$, i.e., $J_{r}(\pi) \coloneqq \EE_{\tau\sim \pi}[r(\tau)]$. We assume that $r^\star(\tau)\in [0,1]$ for any trajectory $\tau$.

\subsection{Outcome Supervision and Process Supervision}\label{sec: prem-reward}

This paper focuses on two basic supervision paradigms in reinforcement learning: \emph{process supervision} and \emph{outcome supervision}. We analyze these approaches through the lens of \emph{statistical complexity} rather than algorithmic implementation details. 

The distinction lies in the temporal resolution of available reward signals:
\begin{itemize}
    \item Process supervision provides step-wise rewards during trajectory collection. More precisely, the offline data has the form
    \icml{\begin{small}
    \begin{align}
    \label{eq:def-prm-data}
    \Dcal_P \coloneqq\{(s_1, a_1, r_1, s_2, a_2, r_2, \cdots, s_H, a_H, r_H)\},
    \end{align}
    \end{small}}
    \arxiv{\begin{align}
    \label{eq:def-prm-data}
    \Dcal_P \coloneqq\{(s_1, a_1, r_1^\star, s_2, a_2, r_2^\star, \cdots, s_H, a_H, r_H^\star)\},
    \end{align}}
    where $r_h = r^\star(s_h, a_h)$ denotes the ground truth reward value at step $h$. 
    This setting is compelling compared to outcome supervision, especially for complex multi-step reasoning problems, as it provides more precise feedback that can pinpoint the  location of suboptimal actions and allows to correct for cases where the agent makes mistakes in the middle of the reasoning path but reaches the correct final answer \citep{uesato2022solving,lightman2023let}.

    \item Outcome supervision reveals only the cumulative rewards for complete trajectories. The data for this setting has the form
    \icml{\begin{small}
    \begin{align}
    \label{eq:def-orm-data}
    \Dcal_O \coloneqq\{(s_1, a_1, s_2, a_2, \cdots, s_H, a_H, R)\},
    \end{align}
    \end{small}}
    \arxiv{\begin{align}
    \label{eq:def-orm-data}
    \Dcal_O \coloneqq\{(s_1, a_1, s_2, a_2, \cdots, s_H, a_H, R)\},
    \end{align}}
    where $R = \sum_{h=1}^H r^\star(s_h, a_h)$ denotes the total reward along the trajectory $(s_1,a_1,...,s_H,a_H)$.
    
    We also consider  preference learning,  where the learner only has access to trajectory-level pairwise preferences, as an extension of outcome supervision. Our main results are applicable to both cases, and are discussed in \cref{sec: orm-reward}. The problem of  reinforcement learning from human preferences  \citep[e.g.,][]{christiano2017deep} or human feedback \citep[e.g.,][]{ouyang2022training} typically falls under this paradigm, where only the trajectory-level supervision signal is available.
\end{itemize}


Our analysis reveals the temporal resolution distinction described above does not inherently create statistical complexity gaps when proper coverage conditions hold. Our results formalize this insight in the following two settings: (1) Offline RL with different reward signal resolutions (\cref{sec: orm-reward}), and (2) Online RL with verifier/rollout access (\cref{sec: advantage}).









\paragraph{Outcome-Supervised and Process-Supervised Reward Models}

Two other commonly used terms related to this paper are outcome-supervised reward models (ORMs) and process-supervised reward models (PRMs). ORMs are learned to evaluate the final outcomes of whole trajectories, corresponding to the value $R$ in \cref{eq:def-orm-data}. PRMs are learned to evaluate the intermediate rewards of each state-action pair, corresponding to the values $r^\star(s_h,a_h)$ in \cref{eq:def-prm-data} for all $h\in [H]$.

In this paper, we use ORMs and PRMs to refer specifically to different algorithmic approaches for learning reward models, rather than the underlying supervision paradigms discussed earlier, as learning ORMs or PRMs does not necessarily require the underlying data to be collected under the same supervision paradigm.



\subsection{State-Action Coverage and Trajectory Coverage}\label{sec: prem-concentrability}

The coverage condition---typically referred to as a bounded \emph{concentrability coefficient} \citep{munos2003error,antos2008learning,farahmand2010error,chen2019information,jin2021pessimism,xie2021batch,xie2021bellman,bhardwaj2023adversarial}---has played a central role in the theory of offline (or, batch) reinforcement learning, and has recently gained growing attention in online reinforcement learning through a related concept of a \emph{coverability coefficient} \citep{xie2022role,liu2023can,amortila2024harnessing,amortila2024scalable}.

In this paper, we use coverage conditions to capture the statistical complexity of different supervision paradigms. To motivate the importance of coverage notions, consider the following approach for imputing the missing rewards in outcome supervision. Suppose we minimize the trajectory-level regression objective over a class of reward functions $\Rcal=\{r:\Scal\times\Acal\to\RR\}$,
\begin{equation}\label{eq: solution-r-bar}
    \icml{\textstyle}
    \hr = \argmin_{r\in \calR} \sum_{(\tau, R)\in \Dcal_O} \left(r(\tau) - R\right)^2,
\end{equation}
where the outcome supervision data $\calD_O$ are collected by some reference policy $\pioff$. With the learned reward model $\widehat r$, we can now employ an offline RL method of our choice. However, we have a (standard in the literature) mismatch: while $\big| \sum_{h=1}^H \widehat{r}(s_h,a_h) - \sum_{h=1}^H r^\star(s_h,a_h) \big|$ is small for trajectories collected by $\pioff$, we care about the error $\big| J(\pi) - J_{\widehat r}(\pi) \big|$ for some policy $\pi$ that differs from $\pioff$, where $J(\pi)$ and $J_{\widehat r}(\pi)$ are defined in \cref{sec: prem-mdp}. A naive approach for capturing such a change of trajectory measure is to use the  \emph{trajectory concentrability coefficient},
\begin{equation}\label{eq: def-t-coverage}
    \Ctraj(\pi, \pioff) \coloneqq \sup_{\tau} \frac{d^\pi(\tau)}{d^{\pioff}(\tau)},
\end{equation}
where the supremum is over all possible trajectories.

This trajectory concentrability coefficient is usually considered to be prohibitively large, and its limitation has been widely studied in the literature on off-policy evaluation \citep[e.g.,][]{liu2018breaking,xie2019towards,nachum2019dualdice,uehara2020minimax}. An alterative approach is to express upper bounds (if possible) in terms of the \emph{state-action concentrability coefficient}, commonly used in offline policy learning literature \citep[e.g.,][]{munos2003error,antos2008learning,farahmand2010error,chen2019information} and defined as follows:
\begin{equation}\label{eq: def-s-coverage}
    \Cs(\pi, \pioff) \coloneqq \max_{h\in [H]}\sup_{s_h\in \calS_h, a\in \calA} \frac{d^\pi(s_h, a_h)}{d^{\pioff}(s_h, a_h)}.
\end{equation}
The state-action concentrability coefficient is always smaller than the trajectory concentrability coefficient, and the difference can be exponential. This is because $d^\pi(s_h, a_h)$ aggregates over all trajectories that pass through $(s_h, a_h)$, and thus
\begin{align*}
    \frac{d^\pi(s_h, a_h)}{d^{\pioff}(s_h, a_h)} =  \frac{\sum_{\tau: (s_h, a_h)\in \tau} d^\pi(\tau)}{\sum_{\tau: (s_h, a_h)\in \tau} d^{\pioff}(\tau)} \le \sup_{\tau} \frac{d^\pi(\tau)}{d^{\pioff}(\tau)},
\end{align*}
for all $h\in [H]$ and $(s_h, a_h)\in \calS_h\times\calA$.

It is straightforward to see that using the state-action concentrability coefficient is usually sufficient for the process supervision paradigm, the setting prevalent in the offline RL literature. The intuition is that process supervision allows us to achieve small estimation error at the state-action level, which only poses a state-action level change of measure during the subsequent policy learning step. \emph{Perhaps surprisingly, we show that state-action concentrability is also sufficient for the outcome supervision case.}

\paragraph{Single-Policy and All-Policy Coverage}

Another important concept in the offline RL literature is the distinction between single-policy and all-policy concentrability. Using the state-action concentrability coefficient as an example, single-policy concentrability refers to the coverage of a specific target policy (such as the optimal policy $\pi^\star$), defined as $\Cs(\pi^\star, \pioff)$. In contrast, all-policy concentrability considers the coverage of all policies in a policy class $\Pi$, defined as $\Cs(\Pi, \pioff)\coloneqq\sup_{\pi\in\Pi}\Cs(\pi, \pioff)$.

Single-policy versus all-policy coverage is one of the central questions in the offline RL literature. For more details, we refer the reader to \citet{chen2019information,xie2021bellman,jiang2024offline}, but this is not the focus of this paper. In the following sections, we will present results using all-policy coverage conditions for simplicity, and defer the single-policy case to the appendix.
















\section[Outcome and Process Supervision: Similar Statistical Guarantees]{\fontdimen2\font=0.7\fontdimen2\font\fontdimen3\font=0.7\fontdimen3\font\fontdimen4\font=0.7\fontdimen4\font\mbox{Outcome and Process Supervision: Similar Statistical Guarantees}}\label{sec: orm-reward}





\subsection{Learning a Reward Model from Total Reward}\label{sec: transform}




We present a simple approach to estimate rewards in an outcome supervision dataset of the form \cref{eq:def-orm-data} using least squares regression, assuming that the learner has access to a class of reward models $\calR$. This transformation allows the learner to use outcome supervision data with methods designed for process reward data, as detailed below. 
We have the following theorem for the least squares estimate of the rewards:
\begin{theorem}\label{thm: orm-prm-transformation}
    Suppose the dataset $\calD_O$ is collected i.i.d. according to policy $\pioff$ in the MDP $M = (\calS, \calA, P, r^\star, H)$ with the ground truth reward model $r^\star\in \calR$. Then, with probability at least $1 - \delta$, for any policy $\pi$, 
    the PRM reward model $\hr$ computed by \pref{eq: solution-r-bar} satisfies 
    \[
    \left|J_{\hr}(\pi) - J(\pi)\right|\lesssim H^{3/2}\cdot \sqrt{\frac{\Cs(\pi, \pioff)\cdot \log(|\calR|/\delta)}{|\Dcal_O|}},
    \]
    where $\Cs(\pi, \pioff)$ is the state-action concentrability coefficient defined in \cref{eq: def-s-coverage}.
\end{theorem}
The proof of \pref{thm: orm-prm-transformation} is deferred to \cref{sec: app-transform}. \pref{thm: orm-prm-transformation} yields an approach for transforming any offline RL algorithm which takes trajectories with per-step reward into an offline RL algorithm which takes trajectories with total reward as input. More precisely, we split the outcome supervised data, use the first part to estimate the reward function via least squares, and then use this estimate to impute the missing rewards on the second part of the data. We summarize this basic transformation in the following algorithm.

\begin{algorithm}
    \caption{Offline Outcome-to-Process Transformation}\label{alg:transfomration-2}
    \begin{algorithmic}[1]
        \State\textbf{Input: } Offline dataset with total rewards $\calD_O = \{(s_1, a_1, \cdots, s_H, a_H, R)\}$, 
        Reward model class $\calR$, Offline RL Algorithm $\frakA$%
        \State Split $\calD_O$ into two datasets $\calD_{O}^1$ and $\calD_{O}^2$ of equal size. 
        \State Compute $\hr$ by solving \pref{eq: solution-r-bar} with dataset $\calD_O^1$ and the reward class $\calR$.
        \State Construct dataset $\calD_P = \{(s_1, a_1, r_1, \cdots, s_H, a_H, r_H)$ from $\calD_O^2$, where $\forall (s_1, a_1, \cdots, s_H, a_H, R)\in \calD_O^2$,
        $$r_h = \hr(s_h, a_h),\qquad \forall h\in [H].$$
        \State Call algorithm $\frakA$ with dataset $\calD_P$ and output learned policy $\pihat$.
        \State \textbf{Output: } Learned policy $\pihat$.
    \end{algorithmic}
\end{algorithm}


\begin{corollary}\label[corollary]{corr: alg-transform}
    Fix a policy set $\Pi$ which contains the optimal policy $\pi^\star$. Suppose the offline RL algorithm $\frakA$ always outputs a policy $\pihat\in\Pi$ with error at most $\epsilon_{\alg}$, i.e., $J(\pi^\star) - J(\pihat)\le \epsilon_{\alg}$, with probability at least $1 - \delta$. Then the policy output by \pref{alg:transfomration-2} satisfies with probability at least $1 - 2\delta$, %
    \begin{align*}
        \icml{&~}\max_{\pi}J(\pi) - J(\pihat)
        \icml{\\}
        \lesssim \icml{&~} \epsilon_{\alg} + H^{3/2}\cdot \sqrt{\frac{\Cs(\Pi, \pioff)\cdot \log(|\calR|/\delta)}{|\calD_O|}},
    \end{align*}
where $\Cs(\Pi, \pioff)\coloneqq\sup_{\pi\in\Pi}\Cs(\pi, \pioff)$.
\end{corollary}

Notice that the bound in the above theorem suffers from all-policy concentrability, regardless of which algorithm $\frakA$ is used with the transformed data. This occurs because the transformation fixes the learned reward model, requiring us to account for the distribution shift between $\pioff$ and the data-dependent policy $\pihat$ which can be any policy in the policy class $\Pi$.
This is a common issue in classical offline RL without specific methods like pessimism, particularly for the case of partial coverage. However, the concept of pessimism can also be applied to the outcome supervision setting in our paper, where we learn a specific reward model for each policy, as commonly done in the (process-supervision) offline RL literature \citep[e.g.,][]{xie2022armor,cheng2022adversarially,uehara2021pessimistic,bhardwaj2023adversarial}.
Following this approach, we can transform model-based offline RL algorithms that use pessimism \citep{xie2022armor,bhardwaj2023adversarial} into algorithms that employ outcome supervision data. The sample complexity of these transformed algorithms scales with the single-policy concentrability $\Cs(\pi^\star, \pioff)$, which depends only on the optimal policy $\pi^\star$. We defer further details to \cref{sec: app-armor}.


\paragraph{Statistical Efficiency}
We now argue that there is no significant statistical edge for process supervision paradigm compared to the outcome supervision paradigm in the offline setting.
The latter corresponds to standard offline RL problems \citep{levine2020offline,jiang2024offline}, for which a rich body of work exists analyzing sample complexity. Our ``equivalence'' argument primarily focus on coverage conditions, since different coverage notions (e.g., state-action-level vs.~trajectory-level) can lead to exponential differences, as discussed in \cref{sec: prem-concentrability}. While our results establish equivalence with respect to coverage conditions, we acknowledge they may still be subject to polynomial factors of $H$; removing such factors is an avenue for further research.

If we consider the worst-case scenario, it is easy to see that any algorithm which outputs an $\epsilon$-optimal policy requires at least $\sup_{\pi} \Cs(\pi, \pioff) / \epsilon^2$ number of samples. To see this, we may consider the two-armed bandit with action $a_1$ and $a_2$, and $r(a_1) = \pm\epsilon, r(a_2) = 0$, and policy $\pi = \mathrm{Unif}(\{a_1, a_2\})$. In the meantime, many classical offline RL algorithms, such as Fitted Q-Iteration \citep{antos2008learning,munos2008finite}, the theoretical backbone of Deep Q-Network (DQN), require sample complexity that scales with $\Cs(\Pi, \pioff)/\epsilon^2$ for obtaining an $\epsilon$-optimal policy \citep{chen2019information,xie2020q}. Hence \pref{corr: alg-transform} provides a transformation with the same sample complexity as in these works (up to polynomial in horizon factors), when encountering outcome supervision reward data.

As for the instance-dependent case (corresponding to the single-policy coverage discussed in \cref{sec: prem-concentrability}), the lower bound result in \citet{xie2021policy} shows that any algorithm requires at least $\Cs(\pi^\star, \pioff) / \epsilon^2$ number of samples to output an $\epsilon$-optimal policy, which only depends on the coverage of optimal policy $\pi^\star$. Recent offline RL algorithms \citep{xie2021bellman,cheng2022adversarially,uehara2021pessimistic,bhardwaj2023adversarial} indeed reach that sample complexity in terms of the single-policy concentrability $\Cs(\pi^\star, \pioff)$. Our results presented in \cref{sec: app-armor} match the upper bound of these offline RL algorithms for the process supervision case and also enjoy the same sample complexity depending on the single-policy concentrability $\Cs(\pi^\star, \pioff)$.









\subsection{Change of Trajectory Measure}

The proof of \pref{thm: orm-prm-transformation} relies on the following key change of trajectory measure lemma, which states that changing the measure of trajectory returns can be done at the price of state-action concentrability, up to logarithmic and polynomial-in-horizon factors. This lemma will be used for bounding the error between the true reward model $r^\star$ and the learned reward model $\hr$. Thus, by setting $f = r^\star - \hr$, we only need to show that the expectation of the absolute value of $f(\tau)\coloneqq\sum_{(s_h,a_h)\sim\tau}f(s_h,a_h)$ is small for $\tau\sim \pi$ when controlling under $\pioff$.
\begin{lemma}\label[lemma]{lem:orm-reward}(Change of Trajectory Measure Lemma)
    For MDP $M = (\calS, \calA, T, f, H)$ with any function $f: \calS\times\calA\to [-1, 1]$, for any two policies $\pi$ and $\pioff$, we have
    \begin{align*}
        \frac{\EE_{\tau\sim \pi} [f(\tau)^2]}{\EE_{\tau \sim \pioff} [f(\tau)^2]}\lesssim H^3 \Cs(\pi, \pioff),
    \end{align*}
    where $\Cs(\pi, \pioff)$ is defined in \pref{eq: def-s-coverage}. Additionally, the following holds without the extraneous log factors:
    $$\EE_{\tau\sim \pi}\left[|f(\tau)|\right] \lesssim \sqrt{H^3\Cs(\pi, \pioff)\cdot \EE_{\tau\sim \pioff} [f(\tau)^2]}.$$
\end{lemma}
This lemma reveals a perhaps surprising insight: when the squared sum of some state-action value functions $[\sum_{(s_h,a_h)\sim\tau}f(s_h,a_h)]^2$ is small under the off-policy trajectory distribution $\tau\sim\pioff$, we only need to account for state-action-level distribution shifts between $\pioff$ and $\pi$ to bound the same squared sum under $\tau\sim\pi$. This holds true even though controlling such trajectory sums theoretically cannot prevent cases where individual terms have equal and large magnitude but opposite signs (i.e., where $|a|=|b|>0$ but $a+b=0$).
We provide a proof sketch of \pref{lem:orm-reward} in the following. The detailed proof is deferred to \cref{app: app-orm-reward-change}.

\subsection{Proof Sketch of \pref{lem:orm-reward}}

In this section, we outline the key insights behind the proof of \pref{lem:orm-reward}. The central observation is that controlling the trajectory-level variance of $f$ under a reference policy $\pioff$ implies an automatic control of variance on prefixes and suffixes over the entire trajectory, with only a polynomial overhead in the horizon length $H$. This seemingly simple fact leads to perhaps surprisingly strong guarantees.

\paragraph{Insight I: Trajectory-level bound controls the second moment on prefixes and suffixes}
At first glance, small value $\bigl\lvert f(\tau)\bigr\rvert$ over the entire trajectory $\tau$ does \emph{not} obviously guarantee that the value of $f$ on either (i) every prefix $\tau_{1:h}$ or (ii) every suffix $\tau_{h+1:H}$ is small. In principle, large positive and large negative portions of a single trajectory could ``cancel'' each other out, resulting in a small overall sum $|f(\tau)|=|f(\tau_{1:h})+f(\tau_{h+1:H})|$.

Crucially, however, thanks to the \emph{Markov property}, we can argue that if $f$ has small second moment (under $\pioff$) and if a state $s_h$ is visited sufficiently often by $\pioff$, $f$ cannot have high variance on the prefix (leading up to $s_h$) and suffix (following $s_h$).
Indeed, if the value of $f$ on the prefix (or suffix) has large variance, then conditioned on passing through $s_h$ that is visited sufficiently often by $\pioff$, the value of $f$ on the entire trajectory also has large variance, which directly implies the large variance (hence, large second moment) of $f(\tau)$. 
Hence, even though the trajectory-level bound looks coarse, it forces each state $s_h$ to have relatively stable partial sums in both the prefix and suffix directions under $\pioff$.

\paragraph{Insight II: Layer-by-layer ``locking'' with only state-action coverage}
Next, we want to argue that if \emph{all} states in a trajectory satisfy the above low-variance property (we call such states ``good'' states), then the reward of the entire trajectory cannot have large absolute value. We call this the ``locking in'' property here for brevity. In the following, we argue that ``locking in'' happens with high probability, even under policy $\pi$.

According to the earlier argument, ``bad'' states (opposite of ``good'' states) cannot have large visitation probability under $\pioff$. Then, by the definition of $\Cs(\pi, \pioff)$, which upper bounds the probability ratio between $\pi$ and $\pioff$ at any state, we conclude that such bad states also have low probability under $\pi$, up to a factor of $\Cs(\pi,\pioff)$. Thus, we avoid exponential blow-up over the horizon because we only ``pay'' for distribution shift at each individual $(s,a)$, rather than for entire trajectories:
$\Pr_{\tau \sim \pi}(\text{bad}) \leq \Cs(\pi,\pioff)\cdot\bP_{\tau \sim \pioff}(\text{bad})$.

Hence, with high probability, $\pi$ visits only ``good'' states throughout its trajectory, ensuring that each layer $h$ ``locks in'' a small partial sum (as both its prefix and suffix have low variance).\footnote{Our formal proof also needs to consider the ``good'' state-action-state tuples, which are similar to ``good'' states but involve the $(s_h,a_h,s_{h+1})$ tuple. We omit the details here for brevity, and readers can refer to the full proof in \cref{app: app-orm-reward-change}.}
When we stitch these layers from $h = 1$ to $h = H$, the entire sum $f(\tau)$ is guaranteed to have small absolute values.









\subsection{Extension to Preference-Based Reinforcement Learning}\label{sec: dpo}

In the previous section, we studied the statistical complexity of outcome supervision under the data format of \cref{eq:def-orm-data}, where the outcome reward is provided at the end of each trajectory. Preference-based reinforcement learning \citep[e.g.,][]{knox2008tamer,akrour2012april,wirth2017survey} represents another well-established paradigm that extends outcome supervision and is commonly employed for learning from human preferences \citep[e.g.,][]{griffith2013policy,christiano2017deep,stiennon2020learning,ouyang2022training}.

Recent work on implicit reward modeling through single-step contrastive learning approaches \citep[DPO;][]{rafailov2023direct} aims to eliminate the need for explicit reward modeling. While extensive prior work \citep[e.g.,][]{zhan2023provable,liu2024provably,song2024importance,zhang2024self} has focused on the sample complexity of these implicit reward modeling approaches, most existing bounds rely on trajectory-level change of measure.
These results are also considered to depend on the trajectory concentrability under naive simplifications, which can grow exponentially with the horizon length (see detailed discussion in \cref{sec: prem-concentrability} and Section 3.2 of \citealt{xie2024exploratory}).

In this section, we extend our main results to the preference-based reinforcement learning setting. As a direct application of our Change of Trajectory Measure Lemma (\cref{lem:orm-reward}), we first provide a sample complexity bound of preference based RL which only scales with state-action concentrability instead of trajectory concentrability, a result applicable to standard explicit reward modeling approaches as well as implicit reward modeling approaches (i.e., DPO).

In preference-based RL, we suppose that for any trajectory $\tau$, the total reward along $\tau$ satisfies $r(\tau)\in [0, \rmax]$. To form the dataset $\calD$ of preferences, the learner collects two reward-free trajectories $\tau = (s_1, a_1, s_2, a_2, \cdots, s_H, a_H)$ and $\tau' = (s_1', a_1', s_2', a_2', \cdots, s_H', a_H')$ according to policy $\piref$, and receives the information about the order $(\tau_+, \tau_-)$ of $\tau,\tau'$, based on the preference $y\sim \PP(\tau \succ \tau')$. We adopt the Bradley-Terry model \citep{bradley1952rank}:
\begin{equation}\label{eq: bt-reward}
    \PP(\tau \succ \tau') = \frac{\exp(r(\tau))}{\exp(r(\tau)) + \exp(r(\tau'))}.
\end{equation}
The labeled preference dataset $\calD$ consists of ordered samples $(\tau_+, \tau_-)$, where both trajectories are collected according to policy $\piref$ and labeled according to the Bradley-Terry model \pref{eq: bt-reward}. 


\subsubsection{Improved Analysis of Preference-Based RL with Explicit Reward Modeling}

We first provide the analysis for the case where an explicit reward modeling procedure is used for preference-based RL.
Suppose the learner is given the preference-based dataset $\calD_\prefer$ and  a reward class $\calR$, where for every reward model $r\in \calR$ and any trajectory $\tau$, $r(\tau)\in [0, V_{\max}]$. In the following result, an analogue of \pref{thm: orm-prm-transformation}, we transform the preference-based dataset into the reward model via maximum likelihood rather than the method of least squares.

\begin{theorem}\label{thm: preference}
    Suppose $\calD_\prefer = \{(\tau_+, \tau_-)\}$ contains i.i.d. pairs of sequences collected according to $\piref$ and ordered according to \cref{eq: bt-reward} with $r^\star\in\calR$.
    Let $$\hr = \argmin_{r\in \calR} \sum_{(\tau+,\tau_-)\in \calD_\prefer} \log \sigma(r(\tau_+) - r(\tau_-))$$
    be the maximum likelihood estimate.
    With probability at least $1 - \delta$, for any two policies $\pi,\pi'\in\Pi$, 
    it holds that
    \arxiv{
    \begin{equation}\label{eq: thm-preference}
        \EE_{\tau\sim \pi, \tau'\sim \pi'}[|r^\star(\tau) - r^\star(\tau') - \hr(\tau) + \hr(\tau')|]\lesssim H^{3/2}\rmax e^{2\rmax}\cdot \sqrt{\frac{(\Cs(\pi, \piref)\vee \Cs(\pi', \piref))\log(|\Pi|/\delta)}{|\calD|}}.
    \end{equation}
    }
    \icml{
    \begin{align*}
        &\hspace{-0.5cm}\EE_{\tau\sim \pi, \tau'\sim \pi'}[|r^\star(\tau) - r^\star(\tau') - \hr(\tau) + \hr(\tau')|]\\
        & \lesssim H^{3/2}\rmax e^{2\rmax}\\
        &\quad\cdot \sqrt{\frac{(\Cs(\pi, \piref)\vee \Cs(\pi', \piref))\log(|\Pi|/\delta)}{|\calD|}}.\numberthis\label{eq: thm-preference}
    \end{align*}
    }
\end{theorem}

The proof of \cref{thm: preference} is deferred to \cref{sec: dpo-proof}.
Notice that with \pref{eq: thm-preference}, if we could obtain $\pihat$ as the optimal policy under reward model $\hr$ (otherwise we can just pay for the sub-optimality as in \cref{corr: alg-transform}), then with high probability $\pihat$ is a near-optimal policy. To see this, we choose $\pi = \pihat = \argmax_{\pi} J_{\hr}(\pihat)$ and $\pi' = \pi^\star = \argmax_{\pi} J(\pihat)$, then with probability at least $1 - \delta$,
\arxiv{
\begin{align*}
    J(\pi^\star) - J(\pihat) & \le J(\pi^\star) - J(\pihat) - J_{\hr}(\pi^\star) + J_{\hr}(\pihat) = \EE_{\tau\sim \pihat, \tau'\sim \pi^\star}[r^\star(\tau') - r^\star(\tau) - \hr(\tau') + \hr(\tau)]\\
    & \le \EE_{\tau\sim \pihat, \tau'\sim \pi^\star}[|r^\star(\tau') - r^\star(\tau) - \hr(\tau') + \hr(\tau)|]\\
    & \lesssim H^{3/2}\rmax e^{2\rmax}\cdot \sqrt{\frac{(\Cs(\pihat, \piref)\vee \Cs(\pi^\star, \piref))\log(|\Pi|/\delta)}{|\calD|}},
\end{align*}
}
\icml{
\begin{align*}
    &~ J(\pi^\star) - J(\pihat)\\
    \le &~ J(\pi^\star) - J(\pihat) - J_{\hr}(\pi^\star) + J_{\hr}(\pihat)\\
    = &~ \EE_{\tau\sim \pihat, \tau'\sim \pi^\star}[r^\star(\tau') - r^\star(\tau) - \hr(\tau') + \hr(\tau)]\\
    \le &~ \EE_{\tau\sim \pihat, \tau'\sim \pi^\star}[|r^\star(\tau') - r^\star(\tau) - \hr(\tau') + \hr(\tau)|]\\
    \lesssim &~ H^{3/2}\rmax e^{2\rmax} \icml{\\ & \quad} \cdot \sqrt{\frac{(\Cs(\pihat, \piref)\vee \Cs(\pi^\star, \piref))\log(|\Pi|/\delta)}{|\calD|}}
    \\
    \leq &~ H^{3/2}\rmax e^{2\rmax} \cdot \sqrt{\frac{\Cs(\Pi, \pioff)\log(|\Pi|/\delta)}{|\calD|}},
\end{align*}
}
Therefore, as we acquire enough samples, $J(\pi^\star) - J(\pihat)$ converges to zero with high probability, implying that $\pihat$ is a near-optimal policy. This convergence requires the same condition of bounded state-action concentrability as in \cref{corr: alg-transform}.


\subsubsection{Improved Analysis of DPO Algorithm}



We now extend our main results to the implicit reward modeling setting and analyze the sample complexity of the DPO algorithm \citep{rafailov2023direct}. DPO is a popular implicit reward algorithm that converts the two-step process of reward modeling and policy optimization into a single-step contrastive learning problem. DPO is commonly used in the token-level setup of LLMs, where actions (tokens) are directly appended to states (contexts) \citep{rafailov2023direct,rafailov2024r}. In this case, the state-action concentrability coefficient essentially reduces to trajectory-level concentrability, as the last state is contains the trajectory.
However, recent work indicates that DPO-style algorithms are  applicable beyond the token-level setup, e.g., in environments with deterministic transition dynamics but still Markovian states, e.g., in robotics \citep{hejna2023contrastive,xie2024exploratory}. In these settings, our bounds with only state-action concentrability can be substantially tighter than existing trajectory-level ones.


Following \citet{xie2024exploratory}, we assume deterministic ground-truth transition dynamics and consider the following KL-regularized objective \citep{xiong2023gibbs, ye2024theoretical,xie2024exploratory}: for some positive number $\beta$,
\arxiv{
\begin{equation}\label{eq: def-J-beta}
    \Jbeta(\pi) \coloneqq J(\pi) - \beta\KL(\pi(\tau)\ \|\ \piref(\tau)) = \EE_{\tau\sim \pi}\left[r^\star(\tau) - \beta\log\frac{\pi(\tau)}{\piref(\tau)}\right].
\end{equation}
}
\icml{
\begin{equation}\label{eq: def-J-beta}
\begin{aligned}
    \Jbeta(\pi) \coloneqq &~ J_{r}(\pi) - \beta\KL(\pi(\tau)\ \|\ \piref(\tau))\\
    = &~ \EE_{\tau\sim \pi}\left[r(\tau) - \beta\log\frac{\pi(\tau)}{\piref(\tau)}\right].
\end{aligned}
\end{equation}
}
The policy $\pi_\beta^\star$ which maximizes $\Jbeta(\pi)$ in \pref{eq: def-J-beta} satisfies $\pi_\beta^\star(\tau)\propto \piref(\tau)\exp\left(r^\star(\tau) / \beta\right)$ for any trajectory $\tau$. It is easy to verify that $\pi_\beta^\star$ is a Markov policy. We assume the learner has access to a Markov policy class $\Pi\subset \calS^\calA$, and aims to find a policy $\pihat$ that is nearly optimal with respect to the policy class $\Pi$, i.e.
$$\max_{\pi\in \Pi} \Jbeta(\pi) - \Jbeta(\pihat)\le \epsilon.$$

The DPO algorithm \citep{rafailov2023direct} takes the dataset $\calD = \{(\tau_+, \tau_-)\}$ as input, and outputs the policy $\pihat\in\Pi$ which maximizes the log likelihood, i.e., \icml{$\pihat$ is obtained by solving}
\arxiv{\begin{equation}\label{eq: dpo}
    \pihat\leftarrow \argmin_{\pi\in \Pi}\left\{\sum_{(\tau_+, \tau_-)\in \calD} \log\left[\sigma\left(\beta \log\frac{\pi(\tau_+)}{\piref(\tau_+)} - \beta\log\frac{\pi(\tau_-)}{\piref(\tau_-)}\right)\right]\right\}.
\end{equation}}
\icml{
\begin{small}
\begin{equation}\label{eq: dpo}
\begin{aligned}
    \argmin_{\pi\in \Pi}\Bigg\{\sum_{(\tau_+, \tau_-)\in \calD} \log\bigg[\sigma\bigg(\beta \log\frac{\pi(\tau_+)}{\piref(\tau_+)} - \beta\log\frac{\pi(\tau_-)}{\piref(\tau_-)}\bigg)\bigg]\Bigg\}
\end{aligned}
\end{equation}
\end{small}}

In the following, we provide a refined analysis of the sample complexity of this algorithm. We first make the following assumptions.
\begin{assumption}[Policy Realizability]\label{ass: dpo-realizability}
    The optimal policy is in the policy class $\Pi$, i.e., $\pi^\star_\beta\in \Pi$.
\end{assumption}
Policy realizability is a minimal assumption for sample-efficient reinforcement learning and is necessary for establishing many standard results \citep{agarwal2019reinforcement,lattimore2020bandit,foster2023foundations}.

In addition, we make the following assumptions of bounded trajectory concentrability.

\begin{assumption}\label{ass: dpo-traj-coverage}
    For any policy $\pi\in \Pi$ and trajectory $\tau$,
    $\left|\log \frac{\pi(\tau)}{\piref(\tau)}\right| \le \frac{\rmax}{\beta}.$
\end{assumption}
This assumption is commonly used in the literature on implicit reward modeling approaches for preference-based reinforcement learning \citep[e.g.,][]{rosset2024direct,xie2024exploratory}. The intuition behind this assumption is that we treat $\log \frac{\pi(\tau)}{\piref(\tau)}$ as an implicit value function,\footnote{In fact, $\log \nicefrac{\pi(\tau)}{\piref(\tau)}$ corresponds to a more complex combination of value functions and rewards. We refer readers to \citet{watson2023coherent,rafailov2024r,xie2024exploratory} for further details.} so that this assumption essentially plays the same role as bounded value functions in the analysis.

If the above assumptions hold, we have the following upper bound on the sample complexity of the DPO algorithm in terms of the state-action concentrability.
\begin{theorem}\label{thm: dpo}
    Suppose \cref{ass: dpo-realizability,ass: dpo-traj-coverage} hold, with probability at least $1 - \delta$, the output $\pihat$ of the DPO algorithm in \pref{eq: dpo} satisfies
    \begin{align*}
    \icml{&~} \Jbeta(\pi_\beta^\star) - \Jbeta(\hpi)
    \icml{\\}
    \lesssim \icml{&~} H^{3/2}\rmax e^{2\rmax}\cdot \sqrt{\frac{\sup_{\pi\in\Pi}\Cs(\pi, \pioff)\log(|\Pi|/\delta)}{|\calD|}}.
    \end{align*}
\end{theorem}
The proof of \pref{thm: dpo} is deferred to \cref{sec: dpo-proof}.
Note that, the exponential dependence on the reward range $\rmax$ is a fundamental characteristic of the Bradley-Terry model, not an artifact of our analysis. Indeed, this exponential dependence appears consistently across the theoretical literature on RLHF  \citep[e.g.,][]{zhu2023principled,zhan2023provable,rosset2024direct,xie2024exploratory,das2024provably}.



\section{Advantage Function Learning with Rollouts}\label{sec: advantage}

In this section, we analyze a common empirical strategy for converting outcome-supervised data into process supervision by leveraging online rollouts. The central observation is that, given access to an environment that returns final outcomes, one can initiate rollouts from individual state-action pairs and use the resulting outcomes to approximate their “quality.” Multiple works have adopted variations of this idea, relying on Q-functions \citep[e.g.,][]{wang2024math}, advantage functions \citep[e.g.,][]{setlur2024rewarding}, or other specialized value estimators \citep[e.g.,][]{luo2024improve}.

Although these methods have demonstrated empirical promise, their theoretical properties remain relatively unexplored. Establishing a theoretical foundation could reveal the assumptions and conditions under which these methods are effective and enable principled comparisons to alternative reward modeling approaches. In what follows, we present (to our knowledge) the first theoretical study of advantage-based reward learning with online rollouts. We show that the advantage function of \emph{any} policy can serve as a valid process-based reward model, recovering the same optimal policy as the original environment. By contrast, we also prove a lower bound indicating that simply using the Q-function can fail: in certain cases, the Q-function-based reward model produces suboptimal or undesired policies.





\subsection{Algorithm and Upper Bounds}

For MDP is $M = (\calS, \calA, P, r, H)$, suppose the transition model $P$ is known to the learner, but the reward model $r$ is unknown. For any given policy $\mu$, we define the advantage function $A^\mu: \calS\times\calA\to \RR$ as 
\icml{\begin{small}
\begin{equation}\label{eq: def-advantage-function}
    A^\mu(s, a) = Q_h^\mu(s, a) - V_h^\mu(s),\arxiv{\qquad}\icml{~} \forall h\in [H], s\in \calS_h, a\in \calA,
\end{equation}
\end{small}}
\arxiv{\begin{equation}\label{eq: def-advantage-function}
    A^\mu(s, a) = Q_h^\mu(s, a) - V_h^\mu(s),\arxiv{\qquad}\icml{~} \forall h\in [H], s\in \calS_h, a\in \calA,
\end{equation}}
where $Q_h^\mu$ and $V_h^\mu$ denote the $Q$-function and value function of policy $\mu$.

\citet{setlur2024rewarding} find an approximation $\hr$ to $A^\mu$ and then invoke a policy gradient algorithm for the MDP with transition model $P$ and reward function $\hr$, yielding a policy $\hpi$. We provide a theoretical guarantee for this approach by showing that the performance gap of $\hpi$ to the optimal policy can be upper bounded in terms of the error $\epsilon_{\sf stat}$ of approximating the advantage function and the error $\epsilon_{\sf alg}$ of optimizing the policy. Before stating the result, we define the concentrability coefficient with respect to distribution $\nu$,
    \begin{equation}\label{eq: def-C-nu}
        \Cs(\nu)\coloneqq \sup_{\pi} \sup_{s\in \calS, a\in \calA}\frac{d^\pi(s, a)}{\nu(s, a)},
    \end{equation}
    where the outer supremum is over all possible policies.

\begin{theorem}\label{thm: advantage}
    Suppose there exists some policy $\mu$ and distribution $\nu\in \Delta(\calS\times\calA)$ such that 
    \begin{equation}\label{eq: epsilon-stat}
        \EE_{(s, a)\sim \nu}\left[|\hr(s, a) - A^\mu(s, a)|^2\right]\le \epsilon_{\sf stat}.
    \end{equation}
    If policy $\pihat$ satisfies
    \begin{equation}\label{eq: epsilon-alg}
        \max_{\pi} J_{\hr}(\pi) - J_{\hr}(\pihat) \leq \varepsilon_{\sf alg},
    \end{equation}
    then it also satisfies
    $$\max_{\pi} J(\pi) - J(\pihat) \le 2H\sqrt{\Cs(\nu)}\cdot \epsilon_{\sf stat} + \epsilon_{\sf alg}.$$
\end{theorem}
The proof of \pref{thm: advantage} is deferred to \cref{sec: advantage-proof}.
Intuitively, the proof follows from the performance difference lemma \citep{kakade2002approximately}, which states that for any policies $\pi$ and $\mu$:
$J(\pi) - J(\mu) = H\cdot\E_{(s_h,a_h) \sim d^\pi}[ A^\mu(s_h,a_h)]$.
This implies that maximizing $J_{A^\mu}$ (treating $A^\mu$ as the reward function) is equivalent to maximizing $J$, since they differ only by the constant term $J(\mu)$. Therefore, both optimization problems yield the same optimal policy.



    There are several ways of obtaining an estimate $\hr$ of the advantage function $A^\mu$ that satisfies \pref{eq: epsilon-stat}. One commonly used approach is Monte-Carlo sampling, for instance as in  \citet{setlur2024rewarding}. In detail, this approach first collects a dataset $\calD$ of data $(s, a, \widehat{A})$, where $(s, a)\in \calS\times \calA$ and $\widehat{A}$ is calculated via rollout from $(s, a)$ under policy $\mu$, serving as an approximation to the advantage function of policy $\mu$ at $(s, a)$. Next, we fit a reward function $\hr$ in a reward class to the dataset $\calD$. Then, as long as the reward class realizes the ground truth advantage function $A^\mu$, the reward function $\hr$ satisfies \pref{eq: epsilon-stat} with high probability.





\subsection{Lower Bound on Failure of Using Q-Functions}
\pref{thm: advantage} indicates that the MDP with reward function set to be the advantage function of any policy $\mu$ has the same best policy as the original MDPs. One may wonder whether the same holds for the $Q$-function as well. In this section, we disprove this by providing a hard MDP with best policy $\pi^\star$, and a policy $\mu$, so that the best policy of the MDP with reward function $Q^\mu$ is not $\pi^\star$.
\begin{theorem}\label{thm: hard-case}
    There exists an MDP $M = (\calS, \calA, P, r, H)$, and a policy $\mu\in \calA^\calS$, such that 
    $$\max_{\pi}J_{r}(\pi) - J_{r}(\hpi)\ge \frac{1}{3},$$
    for
    $\hpi = \argmax_{\pi} J_{Q^\mu}(\pi).$
    Here $Q^\mu: \calS\times\calA\to \RR$ is the $Q$-function of MDP $M$.
\end{theorem}
The proof of \pref{thm: hard-case} is deferred to \cref{sec: advantage-proof}. 

\pref{thm: hard-case} indicates that the widely used approach in large language model training, whereby an approximate $Q$-function is learned in place of the reward model, is theoretically incorrect and possibly outputs undesired policies. In contrast, using the advantage function as the reward model is theoretically justified.


\input{related_works}


\section{Conclusion}
In this paper, we present a way of transforming data in the setting of outcome supervision into the data in the setting of process supervision. This transformation enables us to design offline algorithms in the outcome supervision model from the large pool of algorithms that use process supervision. This transformation extends to preference-based algorithms such as DPO. Beyond this transformation, we also provide theoretical guarantees for algorithms using an approximate advantage function as the reward function.

While our transformation scheme works for most of the offline algorithms, the theoretical guarantees require that the outcome supervision data are collected offline. How to construct similar transformation for online data or online algorithms is left for future work.

\icml{\section*{Impact Statement}
This work contributes to advancing the field of Machine Learning through theoretical analysis and insights. While our research focuses on foundational understanding rather than direct applications, we acknowledge that theoretical advancements in machine learning can have broad societal implications. However, given the theoretical nature of this work, we believe these potential impacts do not warrant specific discussion in this context.}
\arxiv{
\section*{Acknowledgements}
ZJ and AR acknowledge support of the Simons Foundation and the NSF through awards DMS-2031883 and PHY-2019786, as well as from the ARO through award W911NF-21-1-0328.
}






\bibliographystyle{plainnat}
\bibliography{ref}



\clearpage

\appendix
\onecolumn
\allowdisplaybreaks

\begin{center}
{\LARGE Appendix}
\end{center}


\arxiv{
\renewcommand{\contentsname}{}
\addtocontents{toc}{\protect\setcounter{tocdepth}{2}}
{\hypersetup{hidelinks}
\tableofcontents
}
}




\section{Missing Proofs in \pref{sec: orm-reward}}\label[appendix]{sec: app-orm-reward}
\subsection{Proof of Change of Trajectory Measure Lemma}\label[appendix]{app: app-orm-reward-change}
\begin{proof}[\cpfname{lem:orm-reward}]
    In the following proof, for any trajectory $\tau = (s_1, a_1, s_2, a_2, \cdots, s_H, a_H)$ and $1\le u \le v\le H$, we use $\tau_{u:v}$ to denote the partial trajectory $(s_u, a_u, s_{u+1}, a_{u+1}, \cdots, s_v, a_v)$. We use $f(\tau_{u:v})\coloneqq \sum_{h=u}^v f(s_h, a_h)$ to denote the cumulative reward  in this segment. Without loss of generality we assume for any trajectory $\tau$, $f(\tau)\in [-1, 1]$. Since the state space of the MDP is layered, we write $\{s_h\in \tau\}$ for $s_h\in \calS_h$ to denote the event that $s_h$ is the time-$h$ element of the trajectory $\tau$. We let 
    $$L(\eta) = \bP_{\tau\sim \pioff}\left[|f(\tau)|\ge \eta\right]$$ 
    for every $\eta > 0$. For every real number $r\in \RR$ and $\eta, p > 0$, we define the following sets:
    \begin{align*} 
        \calS_h^{\uparrow}(r, \eta, p) &\coloneqq \left\{s_h\in \calS_h: \bP_{\tau\sim \pioff}\left(\left|r - f(\tau_{h: H})\right|\le \eta\ \Big{|}\ s_h\in \tau\right)\ge 1 - p\right\},\\
        \text{and}\quad\calS_h^{\downarrow}(r, \eta, p) &\coloneqq \left\{s_h\in \calS_h: \bP_{\tau\sim \pioff}\left(\left|r - f(\tau_{1:h-1})\right|\le \eta \ \Big{|}\ s_h\in \tau\right)\ge 1 - p\right\}.
    \end{align*}
    Intuitively, $\calS_h^\downarrow(r, \eta, p)$ denotes subset of states in $\calS_h$ where, conditionally on arriving at that state under policy $\pioff$, with probability at least $1-p$, the total reward collected in the first $(h - 1)$ steps is $\eta$-close to $r$. Similarly, $\calS_h^\uparrow(r, \eta, p)$ denotes subset of states in $\calS_h$ where, conditionally on arriving at that state under policy $\pioff$, the probability that the total reward collected in the last $(H - h + 1)$ steps is $\eta$-close to $r$ is at least $1-p$.

    We further define sets
    \[\calS_h^\uparrow(\eta, p) = \cup_{r\in \RR} \calS_h^\uparrow(r, \eta, p),\quad \calS_h^\downarrow(\eta, p) = \cup_{r\in \RR} \calS_h^\downarrow(r, \eta, p).\]
    We can now upper bound the occupancy measure of those states outside the set $\calS_h^\uparrow(\eta, p)$ or $\calS_h^\downarrow(\eta, p)$. This property is summarized in the following claim:

    \paragraph{Claim 1} We have the following upper bounds on the occupancy measure outside $\calS_h^\uparrow (\eta, p)$ and  $\calS_h^\downarrow(\eta, p)$:
    \begin{equation}\label{eq:s-up-down}
        \bP_{\tau\sim \pioff}(\tau \cap \calS_h \not\subset \calS_h^\uparrow(\eta, p))\le \frac{L(\eta)}{p}\quad\text{and}\quad \bP_{\tau\sim \pioff}(\tau\cap \calS_h\not\subset  \calS_h^\downarrow(\eta, p))\le \frac{L(\eta)}{p}.
    \end{equation}

    \paragraph{Proof} We only prove the first inequality, as the proof of the second inequality is similar. Notice that for any state $s_h\in \calS_h\backslash \calS_h^\uparrow(\eta, p)$, according to the definition of $\calS_h^\uparrow(\eta, p)$ we have for any $r\in \RR$,
    \[\bP_{\tau\sim \pioff}\left(\left|r - f(\tau_{h:H})\right|\le \eta\ \Big{|}\ s_h\in \tau\right) < 1 - p.\]
    According to the Markov property, when sampling $\tau\sim \pioff$, $\tau_{1:h-1}\indep \tau_{h:H}$ conditioned on $s_h \in \tau$, which implies
    \begin{align*} 
        &\hspace{-0.5cm} \bP_{\tau\sim \pioff}\left(\left|f(\tau)\right|\le \eta\ \Big{|}\ s_h\in \tau\right)\\
        & = \EE_{\tau\sim \pioff}\left[\bP_{\tau\sim \pioff}\left[\left|f(\tau_{h:H}) - \left(-f(\tau_{1:h-1})\right)\right|\le \eta\ \Big{|}\ \tau_{1:h-1}, s_h\in \tau\right]\ \Big{|}\  s_h\in \tau\right]\le 1 - p.
    \end{align*}
    Hence we have
    \[L(\eta) = \bP_{\tau\sim \pioff}\left[|f(\tau)|\ge \eta\right]\ge \bP_{\tau\sim \pioff}(\tau\cap \calS_h\not\subset \calS_h^\uparrow(\eta, p))\cdot p,\]
    which implies the first inequality of \pref{eq:s-up-down}. The second inequality of \pref{eq:s-up-down} follows similarly. \qed
    
    Next, for every real number $r\in \RR$ and $\eta, p > 0$, we define the set 
    \begin{align*} 
        \calS_h(r, \eta, p) &\coloneqq \left\{s_h\in \calS_h: \bP_{\tau\sim \pioff}\left(\left|r - f(\tau_{1:h-1})\right|\le \eta\  \text{and}\  \left|r + f(\tau_{h: H})\right|\le \eta\ \Big{|}\ s_h\in \tau\right)\ge 1 - p\right\}.
    \end{align*}
    This set denotes subset of states in $\calS_h$ where conditioned on arriving at that state under policy $\pioff$, the probability that the first $(h-1)$ steps' total reward is $\eta$-close to $r$ and also the last $(H-h+1)$ steps' total reward is $\eta$-close to $-r$ is less than $p$. We further define the set
    \[\calS_h(\eta, p) = \cup_{r\in \RR} \calS_h(r, \eta, p),\]
    Then we have the following claim, which shows that the occupancy measure of states outside $\calS_h(\eta, p)$ is also upper bounded:
    \paragraph{Claim 2} We have the following upper bounds on the occupancy measure outside $\calS_h(\eta, p)$:
    \begin{equation}\label{eq:s-total}
        \bP_{\tau\sim \pioff}(\tau\cap \calS_h\not\in \calS_h(\eta, p))\le \frac{L(2\eta/3)}{1-p} + \frac{4L(\eta/3)}{p}.
    \end{equation}

    \paragraph{Proof} For state $s_h\in \calS_h\backslash \calS_h(\eta, p)$ but $s_h\in \calS_h^\uparrow(\eta/3, p/2)\cap \calS_h^\downarrow(\eta/3, p/2)$, there exists some $r^\uparrow(s_h)\in \RR$ and $r^\downarrow(s_h)\in \RR$ such that
    \begin{align*} 
        \bP_{\tau\sim \pioff}\left(\left|f(s_h)^\downarrow - f(\tau_{1:h-1})\right|\le \frac{\eta}{3}\ \Big{|}\ s_h\in \tau\right) \ge 1 - \frac{p}{2}\ \ \text{and}\ \ \bP_{\tau\sim \pioff}\left(\left|f(s_h)^\uparrow - f(\tau_{h:H})\right|\le \frac{\eta}{3}\ \Big{|}\ s_h\in \tau\right) \ge 1 - \frac{p}{2}.
    \end{align*}
    By union bound we have that for any such $s_h$,
    \[\bP_{\tau\sim \pioff}\left(\left|r^\downarrow(s_h) - f(\tau_{1:h-1})\right|\le \frac{\eta}{3}\quad  \text{and}\quad  \left|r^\uparrow(s_h) - f(\tau_{h:H})\right|\le \frac{\eta}{3}\ \Big{|}\ s_h\in \tau\right)\ge 1 - p.\]
    If for some $s_h\in \calS_h\backslash \calS_h(\eta, p)$, we have $|r^\downarrow(s_h) + r^\uparrow(s_h)|\le \frac{4\eta}{3}$, then by letting $r = \frac{r^\downarrow(s_h) - r^\uparrow(s_h)}{2}$, we obtain
    \begin{align*} 
        &\hspace{-0.5cm} \bP_{\tau\sim \pioff}\left(\left|r - f(\tau_{1:h-1})\right|\le \eta\  \text{and}\  \left|r + f(\tau_{h:H})\right|\le \eta\ \Big{|}\ s_h\in \tau\right)\\
        & \ge \bP_{\tau\sim \pioff}\left(\left|r^\downarrow(s_h) - f(\tau_{1:h-1})\right|\le \frac{\eta}{3}\  \text{and}\  \left|r^\uparrow(s_h) - f(\tau_{h:H})\right|\le \frac{\eta}{3}\ \Big{|}\ s_h\in \tau\right)\\
        & \ge 1 - p.
    \end{align*}
    This contradicts the definition of $\calS_h\backslash \calS_h(\eta, p)$. Hence for any $s_h\in (\calS_h\backslash \calS_h(\eta, p))\cap (\calS_h^\uparrow(\eta/3, p/2)\cap \calS_h^\downarrow(\eta/3, p/2))$, we always have $|r^\downarrow(s_h) + r^\uparrow(s_h)| > \frac{4\eta}{3}$, which implies that
    \begin{align*} 
        \bP_{\tau\sim \pioff}\left(\left|f(\tau)\right| \ge \frac{2\eta}{3}\ \Big{|}\ s_h\in \tau\right) &\ge \bP_{\tau\sim \pioff}\left(\left|r^\downarrow(s_h) - f(\tau_{1:h-1})\right|\le \frac{\eta}{3}\  \text{and}\  \left|r^\uparrow(s_h) - f(\tau_{h:H})\right|\le \frac{\eta}{3}\ \Big{|}\ s_h\in \tau\right)\\
        & \ge 1 - p.
    \end{align*}
    Further notice that
    \begin{align*} 
        L(2\eta/3) & = \bP_{\tau\sim \pioff}\left(|f(\tau)|\ge 2\eta/3\right)\\
        & \ge \bP_{\tau\sim \pioff}(\tau\cap \calS_h\subset  (\calS_h\backslash \calS_h(\eta, p))\cap (\calS_h^\uparrow(\eta/3, p/2)\cap \calS_h^\downarrow(\eta/3, p/2)))\cdot (1 - p).
    \end{align*}
    Hence we obtain
    \[\bP_{\tau\sim \pioff}(\tau\cap \calS_h\subset (\calS_h\backslash \calS_h(\eta, p))\cap (\calS_h^\uparrow(\eta/3, p/2)\cap \calS_h^\downarrow(\eta/3, p/2)))\le \frac{L(2\eta/3)}{1-p}.\]
    Additionally, according to our previous claim, we have 
    \[\bP_{\tau\sim \pioff}(\tau\cap\calS_h\subset \calS_h^\uparrow(\eta/3, p/2))\le \frac{2L(\eta/3)}{p}\quad\text{and}\quad \bP_{\tau\sim \pioff}(\tau\cap \calS_h\subset \calS_h^\downarrow(\eta/3, p/2))\le \frac{2L(\eta/3)}{p}.\]
    Combining the above three inequalities, we obtain that 
    \[\bP_{\tau\sim \pioff}(\tau\cap\calS_h\not\subset \calS_h(\eta, p))\le \frac{L(2\eta/3)}{1-p} + \frac{4L(\eta/3)}{p},\]
    and \pref{eq:s-total} is verified. \qed

    We next define the following sets of ``good'' state-action-state tuples:
    \begin{align*} 
        \calU_h & \coloneqq \{(s_h, a_h, s_{h+1}): s_h\in \calS_h, a_h\in \calA_h, s_{h+1}\in \calS_{h+1},\\
        & \qquad \exists r, r'\in \RR\text{ such that } s_h\in \calS_h(r, \eta, p), s_{h+1}\in \calS_{h+1}(r', \eta, p)\text{ and }\left|f(s_h, a_h) + r - r'\right|\le 3\eta\}
    \end{align*}
    and also `bad' state-action-state tuples:
    \begin{align*} 
        \calU_h' & \coloneqq \{(s_h, a_h, s_{h+1}): s_h\in \calS_h, a_h\in \calA_h, s_{h+1}\in \calS_{h+1},\\
        & \qquad \forall r, r'\in \RR\text{ such that }s_h\in \calS_h(r, \eta, p), s_{h+1}\in \calS_{h+1}(r', \eta, p)\text{ and }\left|f(s_h, a_h) + r - r'\right|\ge 3\eta\}.
    \end{align*}
    We will show that under policy $\pioff$, the encountered state-action-state pairs are `good', i.e., belong to $\calU_h$, with high probability. Notice that the complement of set $\calU_h$ satisfies
    \begin{equation}\label{eq:u-subset}
        (\calU_h)^c\subset\ \calU_h'\cup \{(s_h, a_h, s_{h+1}): s_h\in \calS_h\backslash \calS_h(\eta, p)\}\cup \{(s_h, a_h, s_{h+1}): s_{h+1}\in \calS_{h+1}\backslash \calS_{h+1}(\eta, p)\},
    \end{equation}
    and according to the last claim we have 
    \begin{align*} 
        & \bP_{\tau\sim \pioff}(\tau\cap\calS_h\not\subset \calS_h(\eta, p))\le \frac{L(2\eta/3)}{1-p} + \frac{4L(\eta/3)}{p}\\
        &\hspace{-0.5cm}\text{and}\quad \bP_{\tau\sim \pioff}(\tau\cap\calS_{h+1}\not\subset \calS_{h+1}(\eta, p))\le \frac{L(2\eta/3)}{1-p} + \frac{4L(\eta/3)}{p}. \numberthis \label{eq:s-l}
    \end{align*}
    We next upper bound $\bP_{\tau\sim \pioff}(\tau\cap \calU_h'\neq \emptyset)$, which is summarized into the following claim.
    \paragraph{Claim 3} We have the following upper bounds on the occupancy measure of $\calU_h'$:
    \begin{equation}\label{eq:u}
        \bP_{\tau\sim \pioff}(\tau\cap \calU_h'\neq \emptyset)\le \frac{L(\eta)}{1-2p}
    \end{equation}

    \paragraph{Proof} According to the Markov property, we have 
    \begin{align*} 
        & (s_1, a_1, \cdots, s_{h-1}, a_{h-1}) \indep (a_h, s_{h+1})\quad\text{conditioned on }s_h\\
        & \hspace{-0.5cm} \text{and}\quad (s_{h+1}, a_{h+1}, \cdots, s_{H}, a_{H}) \indep (s_h, a_h)\quad\text{conditioned on }s_{h+1}.
    \end{align*}
    Hence for any $(s_h, a_h, s_{h+1})\in \calU_h'$, there exists $r$ and $r'$ such that $s_h\in \calS_h(r, \eta, p)$ and $s_{h+1}\in \calS_{h+1}(r', \eta, p)$, which implies
    \begin{align*}
        \bP_{\tau\sim \pioff}\left(\left|f(\tau_{1:h-1}) - r\right|\le \eta\ \Big{|}\ (s_h, a_h, s_{h+1})\in \tau\right) = \bP_{\tau\sim \pioff}\left(\left|f(\tau_{1:h-1}) - r\right|\le \eta\ \Big{|}\ s_h\in \tau\right)\ge 1- p
    \end{align*}
    and 
    \begin{align*}
        \bP_{\tau\sim \pioff}\left(\left|f(\tau_{h:H}) + r'\right|\le \eta\ \Big{|}\ (s_h, a_h, s_{h+1})\in \tau\right) = \bP_{\tau\sim \pioff}\left(\left|f(\tau_{h:H}) + r'\right|\le \eta\ \Big{|}\ s_{h+1}\in \tau\right)\ge 1- p.
    \end{align*}
    By union bound, we obtain
    \[\bP_{\tau\sim \pioff}\left(\left|f(\tau_{1:h-1}) - r\right|\le \eta\text{ and }\left|f(\tau_{h:H}) + r'\right|\le \eta\ \Big{|}\ (s_h, a_h, s_{h+1})\in \tau\right)\ge 1 - 2p.\]
    Additionally $(s_h, a_h, s_{h+1})\in \calU_h'$ implies $\left|f(s_h, a_h) + r - r'\right|\ge 3\eta$. Therefore, for any $(s_h, a_h, s_{h+1})\in \calU_h'$,
    \[\bP_{\tau\sim \pioff}\left(\left|f(\tau)\right|\ge \eta\ \Big{|}\ (s_h, a_h, s_{h+1})\in \tau\right)\ge 1 - 2p.\]
    This leads to
    \[L(\eta) = \bP_{\tau\sim \pioff}\left(|f(\tau)|\ge \eta\right)\ge \bP_{\tau\sim \pioff}\left(\tau\cap \calU_h'\neq\emptyset\right)\cdot (1-2p),\]
    and \pref{eq:u} follows. \qed

    Combining \pref{eq:u} and the two inequalities in \pref{eq:s-l}, and in view of \pref{eq:u-subset}, we obtain
    \[\bP_{\tau\sim \pioff}(\tau\cap \calU_h = \emptyset)\le \frac{2L(2\eta/3)}{1-p} + \frac{8L(\eta/3)}{p} + \frac{L(\eta)}{1 - 2p}.\]
    Next notice from the definition of state-action concentrability, we have for any policy $\pi$ and layer $h\in [H]$,
    \begin{align*} 
        &\hspace{-0.5cm}\sup_{s_h\in \calS_h, a_h\in \calA, s_{h+1}\in \calS_{h+1}} \frac{d^\pi(s_h, a_h, s_{h+1})}{d^{\pioff}(s_h, a_h, s_{h+1})}\\
        & = \sup_{s_h\in \calS_h, a_h\in \calA, s_{h+1}\in \calS_{h+1}} \frac{d^\pi(s_h, a_h)\cdot T(s_{h+1}\mid s_h, a_h)}{d^{\pioff}(s_h, a_h)\cdot T(s_{h+1}\mid s_h, a_h)}\\
        & = \sup_{s_h\in \calS_h, a_h\in \calA} \frac{d^\pi(s, a)}{d^{\pioff}(s, a)}\le \Cs(\pi, \pioff),
    \end{align*}
    which implies that for any policy $\pi$ and layer $h\in [H]$, 
    \begin{align*} 
        \bP_{\tau\sim \pi}(\tau\cap \calU_h = \emptyset) & = \sum_{(s_h, a_h, s_{h+1})\in (\calS_h\times\calA\times\calS_{h+1})\backslash \calU_h} d^\pi(s_h, a_h, s_{h+1})\\ 
        & \le \Cs(\pi, \pioff)\cdot \sum_{(s_h, a_h, s_{h+1})\in (\calS_h\times\calA\times\calS_{h+1})\backslash \calU_h} d^{\pioff}(s_h, a_h, s_{h+1})\\
        & = \Cs(\pi, \pioff)\cdot \bP_{\tau\sim \pioff} (\tau\cap \calU_h = \emptyset)\\
        &\le \Cs(\pi, \pioff)\cdot\left(\frac{2L(2\eta/3)}{1-p} + \frac{8L(\eta/3)}{p} + \frac{L(\eta)}{1 - 2p}\right).
    \end{align*}
    Hence by union bound, we have for any policy $\pi$, 
    \[\bP_{\tau\sim \pi}(\tau\cap \calU_h\neq \emptyset,\ \forall h\in [H])\ge 1 - H\Cs(\pi, \pioff)\cdot\left(\frac{2L(2\eta/3)}{1-p} + \frac{8L(\eta/3)}{p} + \frac{L(\eta)}{1 - 2p}\right).\]

    Finally, we have the last claim showing that if for all $h\in [H]$, $(s_h, a_h, s_{h+1})\in \calU_h$, then the total reward of the trajectory can be upper bounded.
    
    \paragraph{Claim 4} For any trajectory $\tau = (s_1, a_1, \cdots, s_H, a_H)$ where $(s_h, a_h, s_{h+1})\in \calU_h$ for every $h$, we have 
    \[\left|f(\tau)\right|\le 5H\eta.\]
    \paragraph{Proof} We define tuple $u_h = (s_h, a_h, s_{h+1})$ for $h\in [H]$. According to the definition of $\calU_h$, there exist real numbers $r(u_h)\in \RR$ and $r'(u_{h+1})\in \RR$ such that for any $h\in [H]$,
    \[s_h\in \calS_h(r(u_h), \eta, p),\quad s_{h+1}\in \calS_h(r'(u_h), \eta, p),\quad \text{and}\quad |f(s_h, a_h) + r(u_h) - r'(u_h)|\le 3\eta.\]
    Compare the condition on $s_h\in \calS_h(r(u_h), \eta, p)$ with $s_h\in \calS_h(r'(u_{h-1}), \eta, p)$, we have
    \[\bP_{\tau\sim \pioff}\left(\left|r(u_h) - f(\tau_{1:h-1})\right|\le \eta \ \Big{|}\ s_h\in \tau\right)\ge 1 - p\ \text{ and }\ \bP_{\tau\sim \pioff}\left(\left|r'(u_{h-1}) - f(\tau_{1:h-1})\right|\le \eta \ \Big{|}\ s_h\in \tau\right)\ge 1 - p.\]
    When $p < 1/2$, by union bound we obtain 
    $$\bP_{\tau\sim \pioff}\left(\left|r(u_h) - f(\tau_{1:h-1})\right|\le \eta \text{ and }\left|r'(u_{h-1}) - f(\tau_{1:h-1})\right|\le \eta\ \Big{|}\ s_h\in \tau\right)\ge 1 - 2p > 0,$$
    which implies that there exists a trajectory $\tau$ such that $\left|r(u_h) - f(\tau_{1:h-1})\right|\le \eta$ and $\left|r'(u_{h-1}) - f(\tau_{1:h-1})\right|\le \eta$ both hold. Hence we obtain
    \[|r(u_h) - r'(u_{h-1})|\le |r(u_h) - f(\tau_{1:h-1})| + |f(\tau_{1:h-1}) - r'(u_{h-1})|\le \eta + \eta = 2\eta.\]
    Therefore, we obtain that 
    \begin{align*}
        |f(\tau)| = \left|\sum_{h=1}^H f(s_h, a_h)\right|\le 3H\eta + \left|\sum_{h=1}^H \{r'(u_h) - r(u_h)\}\right|\le 5H\eta + |r'(u_H) - r(u_1)| = 5H\eta,
    \end{align*}
    where the last equality uses the fact that $s_{H+1}$ is the notational terminal state and $s_1$ is in the first layer. \qed

    According to the previous proofs, we obtain that 
    \[\bP_{\tau\sim \pi}\left(\left|f(\tau)\right|\ge 5H\eta\right)\le H\Cs(\pi, \pioff)\cdot\left(\frac{2L(2\eta/3)}{1-p} + \frac{8L(\eta/3)}{p} + \frac{L(\eta)}{1 - 2p}\right).\]
    By choosing $p = 1/3$ and replacing $\eta$ by $\eta/(5H)$, we have
    \begin{align*} 
        \bP_{\tau\sim \pi}\left(\left|f(\tau)\right|\ge \eta\right) & \le H\Cs(\pi, \pioff)\cdot \left(6L(2\eta/(15H)) + 24L(\eta/(15H)) + 3L(\eta/(5H))\right)\\
        & \stackrel{(i)}{\le} 33H\Cs(\pi, \pioff)\cdot L(\eta/(15H)) = 33H\Cs(\pi, \pioff)\cdot \bP_{\tau\sim \pioff}\left(\left|f(\tau)\right|\ge \eta/(15H)\right),
    \end{align*}
    where $(i)$ uses the fact that $L(\eta)$ is monotonically decreasing with $\eta$. Hence we obtain
    \begin{align*} 
        \EE_{\tau\sim \pi}\left[f(\tau)^2\right] & \stackrel{(i)}{=} \int_{0}^1 2\eta\cdot \bP_{\tau\sim \pi}\left(\left|f(\tau)\right|\ge \eta\right)d\eta\\
        & \le \int_0^1 2\eta\cdot 33H\Cs(\pi, \pioff)\cdot \bP_{\tau\sim \pioff}\left(\left|f(\tau)\right|\ge \eta/(15H)\right)d\eta\\
        & \le \int_0^{15H} 2\eta\cdot 33H\Cs(\pi, \pioff)\cdot \bP_{\tau\sim \pioff}\left(\left|f(\tau)\right|\ge \eta/(15H)\right)d\eta\\
        & = 7425H^3\Cs(\pi, \pioff)\cdot \int_{0}^1 2(\eta/(15H))\cdot \bP_{\tau\sim \pioff}\left(\left|f(\tau)\right|\ge \eta/(15H)\right)d(\eta/(15H))\\
        & = 7425H^3\Cs(\pi, \pioff)\cdot \EE_{\tau\sim \pioff}[f(\tau)^2],
    \end{align*}
    where $(i)$ uses integration by parts and also the assumption that for any trajectory $\tau$, $f(\tau)\in [-1, 1]$. Additionally, we upper bound the expected total reward under policy $\pi$ with Cauchy-Schwarz inequality: 
    \begin{align*}
        \EE_{\tau\sim \pi}[|f(\tau)|] \le \sqrt{\EE_{\tau\sim \pi}[f(\tau)^2]} \lesssim \sqrt{H^3\Cs(\pi, \pioff)\cdot \EE_{\tau\sim \pioff} \left[f(\tau)^2\right]}.
    \end{align*}
\end{proof}

\subsection{Missing Proofs in \pref{sec: transform}}\label[appendix]{sec: app-transform}

\begin{proof}[\cpfname{thm: orm-prm-transformation}]
    For any reward model $r$, we use $r^\star[r] = r - r^\star$ to denote the difference between reward model $r$ and $r^\star$. Then we have 
    $$J_{r^\star[r]}(\pi) = J_{r}(\pi) - J(\pi),$$
    For any reward $r\in \calR$, we also have
    $$\sum_{\tau\in \calD_\orm}\left(r(\tau) - r^\star(\tau)\right)^2 = \sum_{\tau\in \calD_\orm}\left(r^\star[r](\tau)\right)^2.$$
    We notice that according to our assumption on the MDPs, for any trajectory $\tau$ and reward model $r$, $r(\tau)\in [0, 1]$, hence $r^\star[r](\tau)\in [-1, 1]$. According to \citet[Lemma A.3]{foster2021statistical} and union bound over $\calR$, with probability $1 - p$ we have for any $r\in \calR$,
    \begin{align*}
        \left|\frac{1}{|\calD_\orm|}\sum_{\tau\in \calD_\orm}\left(r^\star[r](\tau)\right)^2 - \EE_{\tau\sim \pioff}\left[\left(r^\star[r](\tau)\right)^2\right]\right| \le \frac{1}{2}\cdot \EE_{\tau\sim \pioff}\left[\left(r^\star[r](\tau)\right)^2\right] + \frac{4\log(2|\calR|/p)}{|\calD_\orm|}. \numberthis \label{eq: r-0-formula} 
    \end{align*}
    When \pref{eq: r-0-formula} holds for all $r\in \calR$, since $\hr$ is the solution of optimization problem \pref{eq: solution-r-bar}, we have 
    \begin{align*} 
        \EE_{\tau\sim \pioff}\left[\left(r^\star[\hr](\tau)\right)^2\right] & \le \frac{2}{|\calD_\orm|}\sum_{\tau\in \calD_\orm}\left(r^\star[\hr](\tau)\right)^2  + \frac{8\log(2|\calR|/p)}{|\calD_\orm|}\\
        & \le \frac{2}{|\calD_\orm|}\sum_{\tau\in \calD_\orm}\left(r^\star[r^\star](\tau)\right)^2  + \frac{8\log(2|\calR|/p)}{|\calD_\orm|}\\
        & \le 3\EE_{\tau\sim \pioff}\left[\left(r^\star[r^\star](\tau)\right)^2\right] + \frac{16\log(2|\calR|/p)}{|\calD_\orm|}\\
        & = \frac{16\log(2|\calR|/p)}{|\calD_\orm|},
    \end{align*}
    where the last equation uses $r^\star[r^\star] = r^\star - r^\star = 0$. Therefore, according to \pref{lem:orm-reward}, with probability at least $1 - p$ we have for any policy $\pi$,
    $$|J_{\hr}(\pi) - J(\pi)| = |J_{r^\star[\hr]}(\pi)|\lesssim \sqrt{H^3\cdot \Cs(\pi, \pioff)\cdot  \EE_{\tau\sim \pioff}\left[\left(r^\star[\hr](\tau)\right)^2\right]}\lesssim H^{3/2}\cdot \sqrt{\frac{\Cs(\pi, \pioff)\cdot \log(|\calR|/p)}{|\calD_\orm|}}.$$
\end{proof}

\begin{proof}[\cpfname{corr: alg-transform}]
    Suppose $\pi^\star$ to be the best policy of the true MDP $M^\star$. By \pref{thm: orm-prm-transformation}, with probability at least $1 - \delta$, for any policy $\pi$ we have 
    \[
    \left|J(\pi) - J_{\hr}(\pi)\right|\lesssim \sqrt{\frac{H^3\Cs(\pi, \pioff)\cdot \log(|\Rcal|/\delta)}{|\calD_1|}}.
    \]
    Especially since $\pi^\star, \pihat\in \Pi$, we have
    \[
    \left|J(\pi^\star) - J_{\hr}(\pi^\star)\right|\lesssim \sqrt{\frac{H^3\Cs(\pi^\star, \pioff)\cdot \log(|\calR|/\delta)}{|\calD_1|}}\le \sqrt{\frac{H^3\sup_{\pi\in \Pi}\Cs(\pi^\star, \pioff)\cdot \log(|\calR|/\delta)}{|\calD_1|}}
    \]
    and also 
    $$\left|J(\pihat) - J_{\hr}(\pihat)\right|\lesssim \sqrt{\frac{H^3\Cs(\pihat, \pioff)\cdot \log(|\calR|/\delta)}{|\calD_1|}}\le \sqrt{\frac{H^3\sup_{\pi\in \Pi}\Cs(\pi^\star, \pioff)\cdot \log(|\calR|/\delta)}{|\calD_1|}}.$$
    Next, by calling algorithm $\frakA$, with probability at least $1 - \delta$ we have 
    $$\max_{\pi} J_{\hr}(\pi) - J_{\hr}(\pihat)\le \epsilon_{\alg}(|\calD|_2, \delta),$$
    Hence with probability at least $1 - 2\delta$,  
    \begin{align*}
        \max_{\pi} J(\pi) - J(\pihat) & = J(\pi^\star) - J(\pihat) \\
        & \le J_{\hr}(\pi^\star) - J_{\hr}(\pihat) + \calO\left(\sqrt{\frac{H^3\Cs(\Pi, \pioff)\cdot \log(|\calM|/\delta)}{|\calD_1|}}\right)\\
        & \le \max_{\pi} J_{\hr}(\pi) - J_{\hr}(\pihat) + \calO\left(\sqrt{\frac{H^3\Cs(\Pi, \pioff)\cdot \log(|\calM|/\delta)}{|\calD_1|}}\right)\\
        & \lesssim \epsilon_{\alg}(|\calD|_2, \delta) + \sqrt{\frac{H^3\Cs(\Pi, \pioff)\cdot \log(|\calM|/\delta)}{|\calD_1|}}.
    \end{align*}
\end{proof}


\subsection{Missing Proofs in \pref{sec: dpo}}\label[appendix]{sec: dpo-proof}

In the following, we will prove \pref{thm: preference} and \pref{thm: dpo}. Before the proofs, we first present several useful lemmas.
\begin{lemma}[Lemma A.1 in \citealt{song2024importance}]\label{lem: dpo-proof-1}
    Suppose \pref{ass: dpo-realizability} holds, and $\pihat$ satisfies
    \begin{equation}\label{eq: condition-pihat}
        \pihat = \argmax_{\pi\in \Pi} \EE_{\tau\sim \pi}\left[\hr(\tau) - \log\frac{\pi(\tau)}{\piref(\tau)}\right]
    \end{equation}
    then we have 
    $$\Jbeta(\pi_\beta^\star) - \Jbeta(\pihat) \le \EE_{\tau\sim \pi_\beta^\star, \ttau\sim \pihat}\left[r^\star(\tau) - r^\star(\ttau) - \hr(\tau) + \hr(\ttau)\right].$$
\end{lemma}
\begin{proof}[\cpfname{lem: dpo-proof-1}]
    We calculate 
    \begin{align*}
        \Jbeta(\pi_\beta^\star) - \Jbeta(\pihat) & = \EE_{\tau\sim \pi_\beta^\star}\left[r^\star(\tau) - \log\frac{\pi_\beta^\star(\tau)}{\piref(\tau)}\right] - \EE_{\tau\sim \pihat}\left[r^\star(\tau) - \log\frac{\hpi(\tau)}{\piref(\tau)}\right]\\
        & = \EE_{\tau\sim \pi_\beta^\star}\left[\hr(\tau) - \log\frac{\pi_\beta^\star(\tau)}{\piref(\tau)}\right] - \EE_{\tau\sim \pihat}\left[\hr(\tau) - \log\frac{\hpi(\tau)}{\piref(\tau)}\right]\\
        & \qquad + \EE_{\tau\sim \pi_\beta^\star}[r^\star(\tau) - \hr(\tau)] - \EE_{\tau\sim \pihat}[r^\star(\tau) - \hr(\tau)]\\
        & \stackrel{(i)}{\le} \EE_{\tau\sim \pi_\beta^\star}[r^\star(\tau) - \hr(\tau)] - \EE_{\tau\sim \pihat}[r^\star(\tau) - \hr(\tau)]\\
        & = \EE_{\tau\sim \pi_\beta^\star, \ttau\sim \pihat}\left[r^\star(\tau) - r^\star(\ttau) - \hr(\tau) + \hr(\ttau)\right],
    \end{align*}
    where in $(i)$ we uses \pref{eq: condition-pihat} and the realizability assumption \pref{ass: dpo-realizability}.
\end{proof}

\begin{lemma}[Lemma C.5 in \citealt{xie2024exploratory}]\label[lemma]{lem: dpo-proof-2}
    We define reward model $\hr$: 
    $$\hr(\tau) = \log\frac{\pihat(\tau)}{\piref(\tau)},$$
    where $\pihat$ is given in \pref{eq: dpo}. Then with probability at least $1 - \delta$, we have 
    $$\EE_{\tau, \ttau\simiid \piref}\left[\left(r^\star(\tau) - r^\star(\ttau) - \hr(\tau) + \hr(\ttau)\right)^2\right]\le \kappa^2\cdot \frac{2\log(|\Pi|/\delta)}{|\calD|},$$
    where $\kappa = 16\rmax e^{2\rmax}$.
\end{lemma}

\begin{lemma}\label[lemma]{lem: dpo-proof-3}
    For MDP $M = (\calS, \calA, T, r, H)$ with reward function $f: \calS\times\calA\to [-1, 1]$, then for any policy $\pi$ and $\tpi$, we have 
    $$\EE_{\tau\sim \pi, \ttau\sim \tpi}[|f(\tau) - f(\ttau)|] \lesssim \sqrt{H^3(\Cs(\pi, \pioff)\vee \Cs(\tpi, \pioff))\cdot \EE_{\tau, \ttau\simiid \pioff} \left[\left(f(\tau) - f(\ttau)\right)^2\right]},$$
    where $\Cs(\pi, \pioff)$ is defined in \pref{eq: def-s-coverage}.
\end{lemma}
\begin{proof}[\cpfname{lem: dpo-proof-3}]
    Suppose the layered state space representation is $\calS = \cup_{h=1}^H \calS_h$. We construct a new MDP $M'$ with horizon $2H$. The state spaces of $M'$ among layer $1$ to $H$, and among layer $H+1$ to $2H$, are both $\calS_1, \cdots, \calS_H$. The action space of $\calM'$ is $\calA$. The transition model of $M'$ follows transition model $T$ in the first $H$ layers, and then transits to the initial state $s_1$ of $M$ and follows transition model $T$ again in the last $H$ layers as well. The reward model $g$ in the first $H$ layers are set to be $f$ and in the last $H$ layers are set to be $-f$.

    We define the policy $\pi'$ of MDP $M'$, which follows policy $\pi$ in the first $H$ layers, and follows policy $\tpi$ in the last $H$ layers. We further define the policy $\pioff'$ of MDP $M'$, which follows policy $\pioff$ in the first $H$ layers, and in the last $H$ layers as well. Then it is easy to see that 
    $$\EE_{\tau\sim \pi, \ttau\sim \tpi}[|f(\tau) - f(\ttau)|] = \EE_{\tau'\sim \pi'}[|g(\tau')|],$$
    where the right hand side is the expected rewards in the new MDP $M'$. We also have 
    $$\EE_{\tau, \ttau\simiid \pioff} \left[\left(f(\tau) - f(\ttau))^2\right)^2\right] = \EE_{\tau'\sim \pioff} \left[g(\tau')^2\right].$$
    Next, according to the construction of $M'$, we have 
    $$\Cs(\pi, \pioff)\vee \Cs(\tpi, \pioff) = \sup_{h\in [2H]}\sup_{s\in \calS_h, a\in \calA} \frac{d^{\pi'}(s, a)}{d^{\pioff'}(s, a)}.$$
    Hence according to \pref{lem:orm-reward}, we have 
    \begin{align*} 
        \EE_{\tau'\sim \pi'}[|g(\tau')|] & \lesssim \sqrt{H^3\cdot \sup_{h\in [2H]}\sup_{s\in \calS_h, a\in \calA} \frac{d^{\pi'}(s, a)}{d^{\pioff'}(s, a)}\cdot \EE_{\tau'\sim \pioff} \left[g(\tau')^2\right]}\\
        & = \sqrt{H^3(\Cs(\pi, \pioff)\vee \Cs(\tpi, \pioff))\cdot \EE_{\tau'\sim \pioff} \left[g(\tau')^2\right]},
    \end{align*}
    which implies that 
    $$\EE_{\tau\sim \pi, \ttau\sim \tpi}[|f(\tau) - f(\ttau)|] \lesssim \sqrt{H^3(\Cs(\pi, \pioff)\vee \Cs(\tpi, \pioff))\cdot \EE_{\tau, \ttau\simiid \pioff} \left[\left(f(\tau) - f(\ttau)\right)^2\right]}.$$
\end{proof}

Now we are ready to prove \pref{thm: preference} and \pref{thm: dpo}.
\begin{proof}[\cpfname{thm: preference}]
    According to \cref{lem: dpo-proof-2,lem: dpo-proof-3}, for any policy $\pi$, with probability at least $1 - \delta$,
    \begin{align*}
        &\hspace{-0.5cm} \EE_{\tau\sim \pi, \tau'\sim \pi'}[|r^\star(\tau) - r^\star(\tau') - \hr(\tau) + \hr(\tau')|]\\
        & \lesssim \sqrt{H^3\cdot (\Cs(\pi, \piref)\vee \Cs(\pi', \piref))\cdot \EE_{\tau, \ttau\simiid \piref}\left[\left(r^\star(\tau) - r^\star(\ttau) - \hr(\tau) + \hr(\ttau)\right)^2\right]}\\
        & \lesssim  H^{3/2}\rmax e^{2 \rmax}\cdot \sqrt{\frac{(\Cs(\pi, \piref)\vee \Cs(\pi', \piref))\log(|\calR|/\delta)}{|\calD|}},
    \end{align*}
    where the second line uses \pref{lem: dpo-proof-2} with the class of policies $\{\pihat: \pihat(\tau)\propto \piref(\tau)\exp (r(\tau))\ \forall r\in \calR\}$.
\end{proof}

\begin{proof}[\cpfname{thm: dpo}]
    Let $\pihat$ to be the policy given in \pref{eq: dpo}. We define the reward model $\hr$ as
    $$\hr(\tau) = \log\frac{\pihat(\tau)}{\piref(\tau)}.$$
    Then it is easy to see that $\pihat$ is the solution of \pref{eq: condition-pihat} according to the above reward model. Since $\pihat\in \Pi$ and $\pi_\beta^\star\in \Pi$ according to \pref{ass: dpo-realizability}, with probability at least $1 - \delta$ we have 
    \begin{align*} 
        \Jbeta(\pi_\beta^\star) - \Jbeta(\pihat) & \stackrel{(i)}{\le} \EE_{\tau\sim \pi_\beta^\star, \ttau\sim \pihat}\left[r^\star(\tau) - r^\star(\ttau) - \hr(\tau) + \hr(\ttau)\right]\\
        & \stackrel{(ii)}{\lesssim} \sqrt{H^3\cdot \Cs(\Pi, \piref)\cdot \EE_{\tau, \ttau\simiid \piref}\left[\left(r^\star(\tau) - r^\star(\ttau) - \hr(\tau) + \hr(\ttau)\right)^2\right]}\\
        & \stackrel{(iii)}{\lesssim} H^{3/2}\rmax e^{2 \rmax}\cdot \sqrt{\frac{\Cs(\Pi, \piref)\log(|\Pi|/\delta)}{|\calD|}},
    \end{align*}
    where $(i)$ uses \pref{lem: dpo-proof-1}, $(ii)$ uses \pref{lem: dpo-proof-3} with the reward model to be $r - \hr$, and $(iii)$ uses \pref{lem: dpo-proof-2}.
\end{proof}

\section{Analysis of ARMOR with Total Reward}\label[appendix]{sec: app-armor}
ARMOR is an offline RL algorithm introduced by \citet{xie2022armor}. Given $n$ trajectories of data in the form of $(s_1, a_1, r_1, \cdots, s_H, a_H, r_H)\sim \pioff$, they proved that for the optimal policy $\pi^\star$, the output policy $\pihat$ of their algorithm satisfies
\begin{align}
    \label{eq:armor-error-bound}
    J(\pi^\star) - J(\pihat)\lesssim \sqrt{\frac{H^2\cdot C(\pi^\star, \pioff)\log(|\calM| / \delta)}{n}}.
\end{align}
Notably, this error bound scales with the best-policy concentrability $C(\pi^\star, \pioff)$ rather than the all-policy concentrability $\sup_{\pi} C(\pi, \pioff)$ shown in \pref{corr: alg-transform}. A natural question arises: can we develop a variant of ARMOR that takes data in the form of trajectory plus total reward, i.e., $(s_1, a_1, \cdots, s_H, a_H, R)$, while maintaining the sample complexity that scales with best-policy concentrability instead of all-policy concentrability? In this section, we answer this question affirmatively by presenting and analyzing such a variant of the ARMOR algorithm.

\begin{algorithm}[!htp]
    \caption{ARMOR with Total Rewards}\label{alg: armor}
    \begin{algorithmic}[1]
        \State\textbf{Input: } Batch data $\calD$, model class $\calM$, parameter $\alpha$.
        \State Construct version space 
        $$\calM_\alpha = \left\{M\in \calM: \max_{M'\in \calM} \calL_\calD(\calM') - \calL_\calD(\calM)\le \alpha\right\},$$
        where for MDP model $M$ with transition model $P_M$ and reward function $r_M$, 
        $$\calL_\calD(M)\coloneqq \sum_{(s_1, a_1, \cdots, s_H, a_H, R)\in \calD} \left[\sum_{h=1}^{H-1}\log P_M(s_{h+1}\mid s_h, a_h) - \left(\sum_{h=1}^H r_M(s_h, a_h) - R\right)^2\right]$$
        \State Output the best policy with pessimism:
        $$\pihat = \argmax_{\pi} \min_{M\in \calM_\alpha} J_M(\pi),$$
        where $J_M(\pi)$ denotes the value function of policy $\pi$ under MDP $M$
    \end{algorithmic}
\end{algorithm}

Suppose the learner is given a model class $\calM$, which realizes the ground truth model $M^\star$. The learner also have access to some offline batched data $\calD$, which is collected from policy $\pioff$. Specifically, $\calD$ consists of i.i.d.~sampled trajectories $\tau = (s_1, a_1, \cdots, s_H, a_H)$ together with its total reward $R = r(\tau)$. Each of the trajectories is collected by executing $\pioff$ under the ground truth MDP $M^\star$.

We consider \pref{alg: armor}, which is a variant of the ARMOR algorithm introduced in \citet{xie2022armor} after specifically taylored for data of trajectories with total reward. We have the following guarantee on its sample complexity, which only relies on the single policy concentrability $\Cs(\pi^\star, \pioff)$.
\begin{theorem}\label{thm: armor}
    Suppose the model class $\calM$ realizes the ground truth model $M^\star$. Then there exists some positive constant $c$ such that for any $\delta > 0$, by letting $\alpha = c\cdot \log(|\calM|/\delta)$, with probability at least $1 - \delta$, the output of \pref{alg: armor} satisfies 
    $$J_{M^\star}(\pi^\star) - J_{M^\star}(\pihat)\lesssim \sqrt{\frac{H^3\cdot \Cs(\pi^\star, \pioff)\log(|\calM| / \delta)}{n}}$$
\end{theorem}
\begin{proof}[\cpfname{thm: armor}]
    Adopting the similar way as \citet{xie2022armor}, we let
    $$\ell_\calD(M) = \prod_{(s_1, a_1, \cdots, s_H, a_H, R)\in \calD} \prod_{h=1}^{H-1} P_M(s_{h+1}, s_h, a_h).$$
    Then according to \citet[Lemma 8]{xie2022armor}, with probability at least $1 - \delta$, we have 
    $$\max_{M\in \calM} \log \ell_\calD(M) - \ell_\calD(M^\star)\le \log\left(\frac{|\calM|}{\delta}\right),$$
    where $M^\star$ denotes the ground truth model. Additionally, according to \citet[Theorem A.1]{xie2021bellman} (by letting $\gamma = 0$ and merge states or actions across the entire horizon into one state or action), we have 
    $$\sum_{(\tau, R)\in \calD}\left(r_{M^\star}(\tau) - R\right)^2 - \min_{M\in \calM}\sum_{(\tau, R)\in \calD}\left(r_M(\tau) - R\right)^2\lesssim \log\left(\frac{|\calM|}{\delta}\right).$$
    Combining the above inequalities together, we obtain that 
    $$\max_{M\in \calM}\calL_\calD(M) - \calL_\calD(M^\star)\lesssim \log\left(\frac{|\calM|}{\delta}\right).$$
    Hence with our choice of $\alpha$, with probability at least $1 - \delta$ we have $M^\star\in \calM$. Next, by \citet[Lemma 7]{xie2022armor}, with probability at least $1 - \delta$, for any $M\in \calM$,
    \begin{align*}
        &\hspace{-0.5cm} \EE_{(s_1, a_1, \cdots, s_H, a_H)\sim \pioff} \left[\sum_{h=1}^{H-1} \TV(P_M(s_{h+1}\mid s_h, a_h), P_{M^\star}(s_{h+1}\mid s_h, a_h)) + \left(\sum_{h=1}^H r_M(s_h, a_h) - r_{M^\star}(s_h, a_h)\right)^2\right]\\
        & \lesssim \frac{\max_{M'\in \calM} \calL_{\calD}(M') - \calL_\calD(M^\star) + \log(|\calM|/\delta)}{n}\lesssim \frac{\log(|\calM|/\delta)}{n}. \numberthis\label{eq: armor-analysis}
    \end{align*}

    Finally, according to the optimality of $\pihat$, if letting $\widehat{M} = \argmin_{M\in \calM} J_M(\pihat)$, we have 
    \begin{align*} 
        J_{M^\star}(\pi^\star) - J_{M^\star}(\pihat) & \le J_{M^\star}(\pi^\star) - \min_{M\in \calM} J_M(\pihat)\\
        & \stackrel{(i)}{=} J_{M^\star}(\pi^\star) - \max_{\pi} \min_{M\in \calM} J_M(\pihat)\\
        & \le \max_{M\in \calM} \left\{J_{M^\star}(\pi^\star) - J_{M}(\pi^\star)\right\},
    \end{align*}
    where $(i)$ uses the definition of $\pihat$ in \pref{alg: armor}. According to simulation lemma \citep[e.g.,][Lemma 7]{uehara2021pessimistic}, we have
    \begin{align*} 
        \left|J_{M^\star}(\pi^\star) - J_M(\pi^\star)\right| & \le \sum_{h=1}^{H-1}\EE_{(s_h, a_h)\sim d^{\pi^\star}}[\TV(P_M(s_{h+1}\mid s_h, a_h), P_{M^\star}(s_{h+1}\mid s_h, a_h))]\\
        & \qquad + \EE_{\tau\sim \pi^\star}[|r_{M^\star}(\tau) - r_{M}(\tau)|]\\
        & \le H^{1/2}\cdot \sqrt{\Cs(\pi^\star, \pioff)\sum_{h=1}^{H-1}\EE_{(s_h, a_h)\sim d^{\pioff}}[\TV(P_M(s_{h+1}\mid s_h, a_h), P_{M^\star}(s_{h+1}\mid s_h, a_h))^2]}\\
        & \qquad + \EE_{\tau\sim \pi^\star}[|r_{M^\star}(\tau) - r_{M}(\tau)|],
    \end{align*}
    where the last inequality uses the Cauchy-Schwarz inequality and the definition of state-action concentrability. Additionally, according to \pref{lem:orm-reward}, we have 
    $$\EE_{\tau\sim \pi^\star}[|r_{M^\star}(\tau) - r_{M}(\tau)|]\lesssim H^{3/2}\sqrt{\Cs(\pi^\star, \pioff)\cdot \EE_{\tau\sim \pioff}[(r_{M^\star}(\tau) - r_M(\tau))^2]}.$$
    Therefore, with probability at least $1 - \delta$, 
    \begin{align*} 
        &\quad J_{M^\star}(\pi^\star) - J_{M^\star}(\pihat)\le \max_{M\in \calM}\{|J_{M^\star}(\pi^\star) - J_M(\pi^\star)|\}\lesssim \sqrt{H^3 \Cs(\pi^\star, \pioff)}\\
        &\cdot \sqrt{\EE_{(s_1, a_1, \cdots, s_H, a_H)\sim \pioff} \left[\sum_{h=1}^{H-1} \TV(P_M(s_{h+1}\mid s_h, a_h), P_{M^\star}(s_{h+1}\mid s_h, a_h))\right] + \EE_{\tau\sim \pioff}\left(r_M(\tau) - r_{M^\star}(\tau)\right)^2}\\
        & \lesssim \sqrt{\frac{H^3 \Cs(\pi^\star, \pioff)\log(|\calM|/\delta)}{n}}
    \end{align*}
    where the last inequality is due to \pref{eq: armor-analysis}.
\end{proof}

We observe that similar to the ARMOR algorithm, the sample complexity of \pref{alg: armor} scales with the single-policy concentrability $\Cs(\pi^\star, \pioff)$ rather than the all-policy concentrability $\sup_{\pi} \Cs(\pi, \pioff)$ as shown in \cref{thm: armor}. However, unlike ARMOR, \cref{thm: armor} requires the assumption that the total reward is bounded by $[0,1]$. Without this assumption, if we include the scale of total reward in the proof of \cref{thm: armor}, we would directly obtain a sample complexity of $\sqrt{\frac{H^5\cdot \Cs(\pi^\star, \pioff)\log(|\calM| / \delta)}{n}}$. Comparing this with ARMOR's sample complexity (\cref{eq:armor-error-bound}), we observe a gap of $H^3$ (when obtaining an $\varepsilon$-sub-optimality bound) in terms of the horizon dependency.

We suspect this gap arises fundamentally from the extra horizon dependency in the Change of Trajectory Measure Lemma (\cref{lem:orm-reward}). To illustrate this, consider how changing measures affects complexity (ignoring log terms):
\begin{itemize}
    \item Change of state-action measure: For any function $g: \Scal \times \calA \to \RR$,
    \begin{align*}
        \frac{\sum_{h=1}^{H}\EE_{(s_h,a_h)\sim d^\pi} [g(s_h,a_h)^2]}{\sum_{h=1}^{H}\EE_{(s_h,a_h)\sim d^{\pioff}} [g(s_h,a_h)^2]} \leq \max_{h\in [H]}\frac{\EE_{(s_h,a_h)\sim d^\pi} [g(s_h,a_h)^2]}{\EE_{(s_h,a_h)\sim d^{\pioff}} [g(s_h,a_h)^2]} \leq \max_{h\in [H]}\sup_{s_h\in \calS_h, a_h\in \calA} \frac{d^\pi(s_h, a_h)}{d^{\pioff}(s_h, a_h)}.
    \end{align*}
    \item Change of trajectory measure (\cref{lem:orm-reward}): For any function $f: \calS \times \calA \to \RR$,
    \begin{align*}
        \frac{\EE_{\tau\sim \pi} [f(\tau)^2]}{\EE_{\tau \sim \pioff} [f(\tau)^2]} \lesssim H^3 \max_{h\in [H]}\sup_{s_h\in \calS_h, a\in \calA} \frac{d^\pi(s_h, a_h)}{d^{\pioff}(s_h, a_h)}.
    \end{align*}
\end{itemize}

While this gap in horizon dependency raises an interesting theoretical question, the precise polynomial dependence on horizon is not the main focus of our paper. We leave a more thorough investigation of this horizon dependency gap between outcome and process supervision as an interesting direction for future work.



\section{Missing Proofs in \pref{sec: advantage}}\label[appendix]{sec: advantage-proof}
\subsection{Proof of \pref{thm: advantage}}
First we present a lemma.

\begin{lemma}\label{lem: advantage}
    For any MDP $M = (\calS, \calA, P, r, H)$ and policy $\mu$, if we let $A^\mu: \calS\times\calA\to \RR$ to be the advantage function of policy $\mu$ (defined in \pref{eq: def-advantage-function}), then for any policy $\pi$, we have 
    $$J_{A^\mu}(\pi) = J_r(\pi) - J_r(\mu).$$
\end{lemma}
\begin{proof}[\cpfname{lem: advantage}]
    In view of the performance difference lemma \citep{kakade2002approximately}, we have
    \begin{align*}
    J_r(\pi) - J_r(\mu) = &~ H\cdot \E_{(s,a) \sim d^\pi}\left[ Q^\mu(s,a) - Q^\mu(s,\mu) \right]
    \\
    = &~ H\cdot \E_{(s,a) \sim d^\pi}\left[ Q^\mu(s,a) - V^\mu(s) \right]
    \\
    = &~ H\cdot \E_{(s,a) \sim d^\pi}\left[ A^\mu(s,a)\right]\\
    = &~ J_{A^\mu}(\pi).
    \end{align*}
\end{proof}

Next we present the proof of \pref{thm: advantage}.
\begin{proof}[\cpfname{thm: advantage}]
    For any policy $\pi$, we decompose
    \begin{align*}
        J_r(\pi) - J_r(\hpi) & = J_r(\pi) - J_{\hr}(\pi) + J_{\hr}(\pi) - J_{\hr}(\hpi) + J_{\hr}(\hpi) - J_r(\hpi)\\
        & \stackrel{(i)}{=} J_{A^{\mu}}(\pi) - J_{\hr}(\pi) + J_{\hr}(\pi) - J_{\hr}(\hpi) + J_{\hr}(\hpi) - J_{A^\mu}(\hpi), \numberthis \label{eq: J-pi-hpi}
    \end{align*}
    where in $(i)$ we use \pref{lem: advantage}. Next, we notice that for any policy $\pi$, we can write down the value of policy $\pi$ as the inner product between the occupancy measures of policy $\pi$ and the reward function, which implies that
    \begin{align*} 
        J_{A^\mu}(\pi) - J_{\hr}(\pi) & = H\cdot \sum_{s\in \calS}\sum_{a\in \calA} d^\pi(s, a)\cdot (A^\mu(s, a) - \hr(s, a))\\
        & \stackrel{(i)}{\le } H\cdot \sqrt{\sum_{s\in \calS}\sum_{a\in \calA} d^\pi(s, a)\cdot (A^\mu(s, a) - \hr(s, a))^2}\\
        & \stackrel{(ii)}{\le } H\cdot \sqrt{\Cs(\nu)\cdot \EE_{(s, a)\sim \nu} \left[(A^\mu(s, a) - \hr(s, a))^2\right]}\\
        & \stackrel{(iii)}{\le } H\sqrt{\Cs(\nu)}\cdot \epsilon_{\sf stat},
    \end{align*}
    where $(i)$ uses Cauchy-Schwarz inequality, $(ii)$ adopts the definition of $\Cs(\nu)$ in \pref{eq: def-C-nu}, and finally in $(iii)$ we use \pref{eq: epsilon-stat}. Additionally, according to \pref{eq: epsilon-alg} we have 
    $$J_{\hr}(\pi) - J_{\hr}(\hpi)\le \max_{\pi} J_{\hr}(\pi) - J_{\hr}(\hpi)\le \epsilon_{\sf alg}.$$
    Bringing these inequalities back to \pref{eq: J-pi-hpi}, we obtain that 
    $$J_r(\pi) - J_r(\hpi)\le 2H\sqrt{\Cs(\nu)}\cdot \epsilon_{\sf stat} + \epsilon_{\sf alg}.$$
\end{proof}

\subsection{Proof of \pref{thm: hard-case}}
\begin{proof}[\cpfname{thm: hard-case}]
    We construct $M$ as follows: We let $H = 2$, $\calA = \{0, 1\}$, and $\calS = \calS_1\cup \calS_2$ where $\calS_1 = \{a\}$ and $\calS_2 = \{b, c\}$. We further define the transition model $P$ as
    $$P(b \mid a, 0) = 1\quad\text{and}\quad P(c\mid a, 1) = 1,$$
    and the reward function $R$ as 
    $$R(a, 0) = R(a, 1) = 0,\quad\text{and}\quad  R(b, 0) = 1, R(b, 1) = 0,\quad \text{and}\quad R(c, 0) = \frac{2}{3}, R(c, 1) = \frac{1}{2}.$$
    The best policy $\pi^\star$ of this MDP $M$ satisfies
    $$\pi^\star(a) = \pi^\star(b) = \pi^\star(c) = 0,$$
    hence $J_r(\pi^\star) = 1$. We next choose policy $\mu$ as $\mu(a) = 0$, $\mu(b) = \mu(c) = 1$. Then we can calculate that 
    \begin{align*}
        Q^\mu(b, 0) = R(b, 0) = 1, & \quad  Q^\mu(b, 1) = R(b, 1) = 0,\\
        Q^\mu(c, 0) = R(c, 0) = \frac{2}{3}, & \quad Q^\mu(c, 1) = R(c, 1) = \frac{1}{2},\\
        Q^\mu(a, 0) = R(a, 0) + Q^\mu(b, \mu(b)) = 0,&\quad Q^\mu(a, 1) = R(a, 1) + Q^\mu(c, \mu(c)) = \frac{1}{2}.
    \end{align*}
    Therefore, the greedy policy $\hpi$ with respect to the MDP with reward function to be $Q^\mu$ is 
    $$\hpi(a) = 1, \quad \hpi(b) = 0,\quad \hpi(c) = 0,$$
    which satisfies 
    $$\max_{\pi}J_r(\pi) - J_r(\hpi) = 1 - \frac{2}{3} = \frac{1}{3}.$$
\end{proof}
\begin{remark}
    With the same choice of MDP $M$ and policy $\mu$ in the above proof, we can calculate the advantage function $A^\mu$ as  
    \begin{align*}
        A^\mu(b, 0) = 1, \quad  A^\mu(b, 1) = 0,\quad \text{and}\quad A^\mu(c, 0) = \frac{1}{6}, \quad A^\mu(c, 1) = 0,\quad \text{and}\quad 
        A^\mu(a, 0)  = 0,\quad A^\mu(a, 1) = \frac{1}{2}.
    \end{align*}
    And it is easy to verify that the greedy policy with respect to the MDP with reward function to be $A^\mu$ coincides with $\pi^\star$. 
\end{remark}

\end{document}
