\begin{table*}[t]
    \centering
    \begin{tabular}[t]{ccccccccc}
        \toprule[\thick pt]
        \multirow{2}{*}{Dataset} & \multirow{2}{*}{H} & \multicolumn{1}{c}{No adapter} & \multicolumn{5}{c}{with adapter} \\
        \cmidrule(lr){3-3} 
        \cmidrule(lr){4-8}
         & & \moment & \pca & $\texttt{LinearAE}$ & $\texttt{dropoutLAE}$ & $\texttt{LinearVAE}$ & \vae \\
        \midrule[\thick pt]
        \multirow{2}{*}{ETTh1} & 96 & $0.411_{\pm 0.012}$ & $0.433_{\pm 0.001}$ & $0.402_{\pm 0.002}$ & $\mathbf{0.395_{\pm 0.003}}$ & $0.400_{\pm 0.001}$ & $0.404_{\pm 0.001}$ \\
         & 192 & $\mathbf{0.431_{\pm 0.001}}$ & $0.440_{\pm 0.000}$  & $0.452_{\pm 0.002}$ & $0.446_{\pm 0.001}$ & $0.448_{\pm 0.002}$ & $\mathbf{0.431_{\pm 0.001}}$  \\
        \cmidrule(lr){1-8}
        \multirow{2}{*}{Illness} & 24 & $2.902_{\pm 0.023}$ & $2.98_{\pm 0.001}$ & $2.624_{\pm 0.035}$ & $2.76_{\pm 0.061}$ & $2.542_{\pm 0.036}$ & $\mathbf{2.461_{\pm 0.008}}$ \\
         & 60  & $3.000_{\pm 0.004}$ & $3.079_{\pm 0.000}$ & $3.110_{\pm 0.127}$ & $2.794_{\pm 0.015}$ & $\mathbf{2.752_{\pm 0.040}}$ & $2.960_{\pm 0.092}$\\
         \cmidrule(lr){1-8}
        \multirow{2}{*}{Weather} & 96 & $0.177_{\pm 0.010}$ & $0.176_{\pm 0.000}$ & $0.169_{\pm 0.000}$ & $\mathbf{0.156_{\pm 0.001}}$ & $0.161_{\pm 0.001}$ & $0.187_{\pm 0.001}$ \\
         & 192 & $0.202_{\pm 0.000}$ & $0.208_{\pm 0.001}$ & $\mathbf{0.198_{\pm 0.001}}$ & $0.200_{\pm 0.001}$ & $0.204_{\pm 0.000}$ & $0.226_{\pm 0.000}$  \\
        \cmidrule(lr){1-8}
        \multirow{2}{*}{ExchangeRate} & 96 & $\mathbf{0.130_{\pm 0.011}}$ & $0.147_{\pm 0.000}$ & $0.167_{\pm 0.013}$ & $\mathbf{0.130_{\pm 0.011}}$ & $0.243_{\pm 0.039}$ & $0.455_{\pm 0.010}$ \\
         & 192 & $\mathbf{0.210_{\pm 0.002}}$ & $0.222_{\pm 0.000}$ & $0.304_{\pm 0.005}$ & $0.305_{\pm 0.013}$ & $0.457_{\pm 0.020}$ & $0.607_{\pm 0.021}$ \\
        \bottomrule[\thick pt]
    \end{tabular}
    \caption{Performance comparison between the baseline $\moment$ model without adapters against different adapter architectures (\pca, $\texttt{LinearAE}$, $\texttt{dropoutLinearAE}$, $\texttt{LinearVAE}$, and \vae), for multivariate long-term forecasting with different horizons $H$. We display the average test MSE $\pm$ standard error obtained on $3$ runs with different seeds. \textbf{Best} results are in bold, with lower values indicating better performance.}
    \label{table:results}
\end{table*}

\section{Experiments \& Results}
\label{sec:exps}

In this section, we empirically demonstrate the quantitative and qualitative superiority of \adapts in multivariate long-term time series forecasting on common benchmarks. We show that in most of the considered tasks (datasets and forecasting horizons,) our framework improves the performance of \moment, a commonly used time series forecasting foundation model. The implementation details are provided in~\cref{appendix:implem}.

\subsection{Time series forecasting}

\noindent\textbf{Datasets.} Our experiments are conducted on four publicly available real-world multivariate time series datasets, commonly used for long-term forecasting \citep{ilbert2024samformer, wu2021autoformer, chen2023tsmixer, nie2023a, Zeng2022AreTE}. These datasets include the Electricity Transformer Temperature dataset (ETTh1) \citep{zhou2021informer}, ExchangeRate \citep{lai2018modelinglongshorttermtemporal}, Weather \citep{weather}, and Influenza-like Illness \citep{illness}. All time series are segmented with an input length of $L = 512$, prediction horizons $H \in [96, 192]$ and $H \in [24, 60]$ for the Illness dataset, and a stride of 1, meaning each subsequent window is shifted by one step. These datasets (detailed in \cref{appendix:datasets}) originate from various application domains, enabling a comprehensive evaluation of our framework across diverse real-world scenarios.

\noindent\textbf{Baseline.} We compare our method against the vanilla application of the foundation model \moment$_{\text{small}}$ from the $\moment$ family of models \citep{goswami2024moment}. This means that for each dataset, we apply \moment$_{\text{small}}$ independently to each feature. Additionally, we compare our learning-based adapters against \pca, an adapter that has been used in the literature for model-based reinforcement learning \citep{benechehab2025zeroshot} and time series classification \citep{feofanov2024adapters}.

% \noindent\textbf{Evaluation.} All adapters are trained to minimize the MSE loss defined in \cref{eq:prob1}, following the training procedure in \cref{alg:training}. The average MSE on the test set, together with the standard error over 5 runs with different seeds is reported. Additional details and results, including the Mean Absolute Error (MAE), can be found in \abdelhakim{[cref]}. Except specified otherwise, all other results are also obtained over 5 runs with different seeds.

\begin{figure}[ht]
\centering
\includegraphics[width=\columnwidth]{figures/n_comp_comparison.pdf}
\caption{Impact of the number of components on model performance. The dashed line indicates $\moment$ performance without adapters, the shaded area its standard deviation, and the vertical line the number of original features.}
\label{fig:dimred}
\end{figure}

\begin{figure*}[t]
% \vskip 0.2in
\centering
\begin{subfigure}[b]{0.24\textwidth}
         \centering
 \includegraphics[width=\textwidth]{figures/repr_pca_Illness.pdf}
 \label{fig:subfig_pca}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.24\textwidth}
 \centering
 \includegraphics[width=\textwidth]{figures/repr_pca_Illness_dropoutLinearAE.pdf}
 \label{fig:subfig_dropout}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.24\textwidth}
 \centering
 \includegraphics[width=\textwidth]{figures/repr_pca_Illness_linearVAE.pdf}
 \label{fig:subfig_linearvae}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.24\textwidth}
 \centering
 \includegraphics[width=\textwidth]{figures/repr_pca_Illness_VAE.pdf}
 \label{fig:subfig_vae}
\end{subfigure}
\vfill
\vskip -0.1in
\begin{subfigure}[b]{0.5\textwidth}
 \centering
 \includegraphics[width=\textwidth]{figures/legend_colormaps.pdf}
\end{subfigure}
\caption{Visualization of the latent representation obtained by different adapters \rebuttal{(with number of components equal to 2)} on Illness($H=24$). Shaded colors indicate the time dimension, with lighter colors representing earlier timesteps.}
\label{fig:latent}
%\end{center}
% \vskip -0.2in
\end{figure*}

\noindent\textbf{\adapts improves the performance of \moment.} We present the forecasting error measured by the Mean Squared Error (MSE) in \cref{table:results} and the Mean Absolute Error (MAE) in \cref{appendix:results}. On the ETTh1 dataset with a prediction horizon $H=96$, all adapter-based variants outperform the baseline \moment model, with $\texttt{dropoutLinearAE}$ achieving the best performance, showing an 8\% improvement. Similar results are observed for the Illness dataset, where all adapters improve over the baseline. Notably, the $\texttt{VAE}$ achieves a significant 15\% improvement, reducing the MSE from $2.902$ to $2.461$ at $H=24$. In the Weather dataset, the $\texttt{dropoutLinearAE}$ adapter shows the best improvement across all adapter architectures for $H=96$, while its deterministic counterpart, $\texttt{LinearAE}$, takes the lead at $H=192$. The results on the ExchangeRate dataset are mixed, with some adapters matching the baseline performance ($\texttt{dropoutLinearAE}$ at $H=96$) while others show degraded performance, particularly at a longer prediction horizon ($H=192$), which is also observed for the ETTh1 dataset. Overall, \adapts improves the forecasting accuracy of \moment in 5 out of the 8 considered tasks, matches its performance in 2, and degrades performance in 1 task.

\subsection{Dimensionality Reduction}

\cref{fig:dimred} illustrates the impact of varying latent space dimensions on forecasting performance across different adapters. For the ETTh1 dataset with a $96$-step horizon, all adapter architectures achieve optimal performance at 7 components (matching the original feature count), with MSE values consistently lower than the baseline. Notably, at just 5 components, all adapters (except the \pca baseline) match the baseline score, demonstrating the suitability of our framework for low-resource setups through dimensionality reduction. The Illness dataset ($H=24$) presents more compelling results, as the \vae adapter achieves significantly optimal performance with only 2 components, underscoring the potential of our approach for cost-effective adaptation of time series foundation models. Ultimately, we find that expanding dimensionality beyond the original feature count does not yield further improvements, as no adapter shows notable enhancements past this point.

\subsection{Interpretability of the latent representations}

\cref{fig:latent} compares the representation learning capabilities of different adapters on the Illness($H=24$) dataset, focusing on their ability to distinguish between training and test data. To visualize the raw dataset, we employ PCA for dimensionality reduction, retaining only two principal components, which is justified by the 95.6\% explained variance. When representing the training and test datasets in the space of the first two principal components, we observe a clear distribution shift, potentially complicating the forecasting task for the baseline foundational model. In contrast, using \adapts results in well-overlapping Gaussian distributions for the training and test data in the latent space. This demonstrates our framework's ability to enforce a structured, isotropic representation that mitigates distribution shift. This effect is particularly pronounced with the \vae adapter and, to a lesser extent, with $\texttt{LinearVAE}$ and $\texttt{dropoutLinearAE}$.

The findings emphasize the advantages of \vae in managing distribution shift, a critical challenge in time series representation learning. By modeling uncertainty and enforcing a continuous latent space, \vae enhance generalization, making them especially valuable for real-world applications where test distributions differ from training data. This aligns with the objective of utilizing adapters in foundational models to optimize zero-shot performance, ensuring robustness across various tasks without extensive fine-tuning.

\subsection{On the calibration of the probabilistic adapters}

\begin{wrapfigure}{r}{0.2\textwidth}
  \begin{center}
  \vskip -0.1in
    \includegraphics[width=0.2\textwidth]{figures/calibration.pdf}
  \end{center}
  \caption{Reliability diagram for the first feature of the ETTh1 ($H=96$) dataset using $\texttt{LinearVAE}$.}
  \label{fig:calibration}
\end{wrapfigure}

To evaluate the calibration of our adapter-based probabilistic forecasters, we use quantile calibration as depicted in the reliability diagram in \cref{fig:calibration}. In an ideal scenario, a well-calibrated probabilistic forecast should align with the red dashed diagonal, indicating that the empirical proportion of observations falls within the predicted quantiles at the expected rate. The overall conclusion is that we observe a gradual deviation from ideal calibration as the prediction horizon increases (darker shades). While early prediction horizons display reasonably well-calibrated predictions, longer-horizon forecasts systematically underestimate uncertainty, as shown by the curve falling below the diagonal. This indicates that observed values exceed predicted quantiles more frequently than expected, suggesting that the predictive distribution becomes too narrow, resulting in overconfident forecasts.

\subsection{Ablation studies}

\noindent\textbf{Influence of $\sigma$ and $\beta$ in the \vae Adapter.} \cref{fig:beta_sigma} illustrates an ablation study examining the $\beta$ parameter in $\beta$-\vae and the noise scale $\sigma$ of the likelihood model applied to the prediction $\hat{\bY}$, assessing their effects on MSE and Expected Calibration Error (ECE). The MSE heatmap (left) demonstrates that increasing $\beta$ generally diminishes MSE, with the lowest values observed at $\beta = 2.0$ and $\beta = 4.0$, particularly for higher $\log \sigma^2$. This indicates that stronger regularization through $\beta$ can enhance forecasting accuracy, possibly due to the disentangling effect of regularization towards a prior distribution with statistically independent components. Conversely, the ECE heatmap (right) shows that higher $\beta$ and $\log \sigma^2$ values result in lower calibration error, with optimal results at $\beta = 4.0$ and $\log \sigma^2 = 3.0$. This outcome is anticipated, as larger values of $\beta$ and $\sigma$ mitigate overfitting, where the model tends to exhibit overconfidence in its predictions. Additionally, it is observed that maintaining a fixed $\sigma$ during training generally outperforms including it in the optimization loop, a configuration denoted as \emph{auto} in \cref{fig:beta_sigma}.

\begin{figure}[ht]
\centering
\includegraphics[width=\columnwidth]{figures/beta_sigma.pdf}
\caption{$\beta$ and $\log \sigma^2$ \vae hyperparameters ablation on the Illness($H=24$) dataset. For reference, the \moment baseline score on this task is $2.902_{\pm 0.023}$.}
\label{fig:beta_sigma}
\end{figure}

\noindent\textbf{$\texttt{LinearAE}$ components.} The ablation study presented in \cref{fig:enc_dec} examines the performance of different components of the linear autoencoder adapter ($\texttt{LinearAE}$) across three datasets: ETTh1, Weather, and ExchangeRate. The figure compares the full linear autoencoder with its encoder-only ($\texttt{LinearEncoder}$) and decoder-only ($\texttt{LinearDecoder}$) variants. Overall, the results reveal that the decoder component of the linear autoencoder plays the most important role in minimizing the forecasting error across all datasets. The encoder-only variant's contribution varies, being more impactful in the Weather dataset compared to ETTh1 and ExchangeRate. These findings highlight the significance of the decoder in the $\texttt{LinearAE}$ adapter and suggest that, in the deterministic case, a decoder might be sufficient to capture feature dependencies. 

\begin{figure}[ht]
\centering
\includegraphics[width=\columnwidth]{figures/encoder_decoder.pdf}
\caption{LinearAE components ablation.}
\label{fig:enc_dec}
\end{figure}

Nevertheless, as shown in our previous experiments, particularly \cref{table:results}, probabilistic adapters generally outperformed the deterministic ones. This underscores the importance of the encoder as well, which is responsible for approximating the posterior distribution in the latent space—a mechanism inherent to our probabilistic framework.

