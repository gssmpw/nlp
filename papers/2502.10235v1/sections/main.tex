\section{Adapters}
\label{sec:adapters}

\subsection{Problem setup}

Consider a multivariate long-term time series forecasting task, represented by: a data matrix $\bX \in \RR^{L \times D}$ where $L$ is the context window size and $D$ is the multivariate time series dimensionality, and a target matrix $\bY \in \RR^{H \times D}$, where $H$ is the forecasting horizon. 
We denote by $\vx_d \in \RR^{L \times 1}$ (respectively $\vy_d \in \RR^{H \times 1}$) the $d$-th component of the input (respectively target) multivariate time series.

Our goal is to use a frozen pre-trained univariate time series foundation model denoted as $\fm: \RR^{L \times 1} \to \RR^{H \times 1}$ (\cref{fig:pipeline}) and exploit the information stored in its weights to achieve the best forecasting performance, measured by the mean squared error (MSE) loss:

\begin{equation}
\label{eq:metric}
\Loss = \| \bY - \fm(\bX)\|_{\mathrm{F}}^2
\end{equation}


On multivariate time series, for simplicity, we denote by $\fm(\bX)$ the application of $\fm$ to each channel independently, in which case the loss can be written as: $\frac{1}{D} \sum_{d=1}^D \|\vy_d - \fm(\vx_d)\|_2^2$.

We now formally define an adapter, a tool by means of which we aim to best use the foundation model $\fm$ for multivariate forecasting:

\begin{definition}[adapter]
\label{def:adapter}

An adapter is a feature-space transformation $\varphi: \RR^{D} \to \RR^{D'}$ that is applied to the data prior to the foundation model\footnote{In practice, $\varphi$ is applied on matrices $\bX$ in $\RR^{L \times D}$. This denotes the application of $\varphi$ on each row of $\bX$.}. The forecast is then obtained by transforming the predictions back to the original feature space:
\[\hat{\bY}(\bX; \varphi) = \varphi^{-1} \big( \fm(\varphi(\bX)) \big)\]

\end{definition}

According to this definition, an adapter is valid only if the inverse transformation $\varphi^{-1}: \RR^{D'} \to \RR^D$, such that $\forall \vx \in \RR^D$, $\varphi^{-1} \circ \varphi (\vx) = \vx$, is well-defined on $\RR^{D'}$. In the rest of the paper, we relax this condition by naming the direct transformation as \emph{encoder} ($\varphi \triangleq \enc$), and respectively, the inverse transformation as \emph{decoder} ($\varphi^{-1} \triangleq \dec$). In this case, the predictions obtained after the application of the adapter become: $\hat{\bY}(\bX; \enc, \dec) = \dec \big( \fm(\enc(\bX)) \big)$.

We note that in the literature, there exist alternatives to adapt to the multivariate setting \citep{zhang2023crossformer,zhou2023onefitsall}, but we have chosen this family of adapters due to their high flexibility as: (a) any foundation model can be plugged-in, (b) no requirement of fine-tuning due to feature-level transformations~\citep{feofanov2024adapters}, (c) adaptation to the computation budget by defining the number of encoded channels.

\noindent\textbf{Optimality of an adapter.} In order for an adapter to be useful, it has to achieve a lower forecasting error than the identity baseline. In fact, the loss defined in \cref{eq:metric} corresponds to the forecasting loss obtained by using an adapter implementing the identity matrix $\mathbf{I}$. Therefore, we define the optimality of the adapter based on improving the forecasting error of the identity baseline:

\[ \Loss \geq \Loss(\varphi) = \|\bY - \varphi^{-1} \big( \fm(\varphi(\bX)) \big)\|_{\mathrm{F}}^2 \]

\subsection{Theoretical analysis}
\label{subsec:theory}

The purpose of this section is to study the optimization problem that the adapter $\varphi$ is aiming to solve:

\begin{equation}
\label{eq:prob1}
    \varphi^* = \argmin_\varphi \|\bY - \varphi^{-1} \big( \fm(\varphi(\bX)) \big)\|_{\mathrm{F}}^2
\end{equation}

Under mild assumptions on the adapter function class and the backbone foundation model $\fm$, we aim at characterizing the optimal solution $\varphi^*$ and prove that it realizes the optimality condition: $\Loss(\varphi^*) \leq \Loss$.

We first consider the linear case where we constrain the adapter $\varphi$ to the class of linear transformations, parametrized by a matrix $\bW_{\varphi} \in \RR^{D \times D}$: $\varphi(\bX) = \bX \bW_{\varphi}$.

\begin{assumption}
\label{ass:invertible}
$\bW_{\varphi}$ has full rank: $\text{rank}(\bW_{\varphi}) = D$, insuring its invertibility.
\end{assumption}

\begin{assumption}
\label{ass:fm}
For ease of derivation, we consider a similar linear parametrization for the foundation model: $\fm(\bX) =  \bW_{\text{FM}}^\top \bX + \vb_{\text{FM}} \mbf{1}^\top$ where $\bW_{\text{FM}} \in \RR^{L \times H}$, $\vb_{\text{FM}} \in \RR^H$, and $\mbf{1}$ a vector of ones of dimension $D$.
\end{assumption}

\begin{proposition}[Optimal linear adapter]
\label{prop:solution}
Under \cref{ass:invertible} and \cref{ass:fm}, the \emph{closed-form} solution of the problem: 

\begin{equation}
\label{eq:prob2}
    \Loss (\bW_{\varphi}) = \|\bY - \big( \bW_{\text{FM}}^\top \bX \bW_{\varphi} + \vb_{\text{FM}} \mbf{1}^\top \big) \bW_{\varphi}^{-1}\|_F^2
\end{equation}

writes as:

\begin{equation}
\label{eq:solution}
    \bW_{\varphi}^* = (\bB^\top \bA)^{+} \bB^\top \bB
\end{equation}

where $\bW_{\varphi}^* = \argmin_{\bW_{\varphi} \in \mathcal{GL}_D(\RR)} \Loss (\bW_{\varphi})$, $\bA = \bY -  \bW_{FM}^\top \bX$, $\bB = \vb_{FM} \mbf{1}^\top$, and $(\bB^{\top} \bA)^{+}$ denoting the \emph{pseudo-inverse} operator.

\end{proposition}

\begin{proof}
    The result follows by differentiating $\Loss (\bW_{\varphi})$ with respect to $\bW_{\varphi}$, and solving the Euler equation: $\nabla_{\bW_{\varphi}} \Loss = 0$. The detailed proof is deferred to \cref{appendix:theory}.
\end{proof}

\begin{remark}
In this case, the fact that the matrix $\bB = \vb_{FM} \mbf{1}^\top$ have identical columns renders the matrix $\bB^\top \bA$ degenerate (with $rank(\bB^\top \bA) = 1$). In practice, we add a positive constant to the diagonal in order to numerically stabilize the matrix inversion: $\bW_{\varphi}^* = (\bB^\top \bA + \lambda \mbf{I})^{-1} \bB^\top \bB$, with $\lambda > 0$. In \cref{subsec:motiv} we show that we are able to reach an optimal solution regardless of this added regularization.
\end{remark}

\subsection{Working example}
\label{subsec:motiv}

\noindent\textbf{Synthetic data.} \rebuttal{Our synthetic dataset comprises two multivariate time series, one with several independent components and the other with linearly correlated ones (\cref{fig:linear_synthetic})}, designed to evaluate a linear feature-space transformation. The data generation process creates five (\emph{uncorrelated}) base signals—sinusoids with distinct frequencies, amplitudes, and \rebuttal{\iid} noise— and derives eight additional channels through linear combinations of these bases with additive Gaussian noise of different magnitude ($\sigma \in (0.1, 0.2, 0.5)$). This construction provides a controlled environment where the ground truth relationship between channels is known: the underlying data manifold is effectively five-dimensional, \rebuttal{but in the correlated case} the observed eight-dimensional multivariate time series includes varying levels of noise and linear mixing. 

\noindent\textbf{Randomly generated linear FMs.} The experimental setup in \cref{fig:linear_synthetic} consists in randomly sampling the linear parameters of a toy foundation model: $\bW_{\text{FM}}$ and $b_{\text{FM}}$. To simulate a realistic scenario, we use Glorot-uniform initialization distribution as it would be the case in neural network-based architectures. We then compute the closed-form solution $\bW_{\varphi}^*$ \rebuttal{(\cref{eq:solution})} on raw data $\bX$, and compare the resulting loss value with the baseline (using the identity matrix $\mathbf{I}$ as adapter) and the $\pca$-only adapter. 

\begin{figure}[ht]
\centering
\includegraphics[width=\columnwidth]{figures/linear_synthetic.pdf}
\caption{\textbf{Optimality of $\bW_{\varphi}^*$}. Comparing the MSE obtained with $\bW_{\varphi}^*$ against the baseline, for 1000 randomly generated linear FM.}
\label{fig:linear_synthetic}
\end{figure}

\cref{fig:linear_synthetic} shows that in the case of uncorrelated data (\emph{left} column) $\pca$ is equivalent to the identity matrix, while the solution $\bW_{\varphi}^*$ to the problem $\Loss (\bW_{\varphi})$ reaches an order of magnitude better forecasting loss. In the correlated case, we observe that $\pca$ has a similar performance to the optimal solution. This example motivates the adapter idea through the existence of better linear transformations than the identity matrix in the case of linear foundation models.

\section{\adapts: Adapters for Probabilistic Multivariate Time Series Forecasting}
\label{sec:prob_adapters}

In this section, we introduce families of adapters that verify the conditions stated in \cref{def:adapter}. Furthermore, we extend this definition to include probabilistic variants of adapters, useful for uncertainty quantification on the FM predictions.

\subsection{Families of adapters}
\label{subsec:families}

\rebuttal{Our framework accommodates various families of transformations that can serve as adapters. Initially, we define linear AutoEncoders and subsequently extend them to their deep non-linear counterparts. Ultimately, we introduce the probabilistic adapter framework, encompassing Variational AutoEncoders (\vae) and $\texttt{Dropout}$-based families of adapters.}

%finally similarly to the experiments in the previous sections. Then we introduce more expressive , and their variational counterparts $\vae$. 
% Following the observations regarding $\pca$ in the linear case, we opt for $\beta$-$\vae$, a $\vae$ variant that favors disentangled representations in the latent space. Finally, we experiment with Normalizing Flows (practically RealNVP), which are a natural choice given the invertibility property of adapters.

\noindent\textbf{Linear AutoEncoders.} \rebuttal{In addition to the linear setup introduced in \cref{subsec:theory},} we extend Linear AutoEncoders to provide a simple yet effective method for dimensionality reduction while preserving the temporal relationships within time series data. In this more general case, the encoder compresses the multivariate time series $ \bX $ into a potentially lower-dimensional representation $ \bZ = \bX \bW_{\theta_{\enc}} $, where $ W \in \mathbb{R}^{D \times D'} $ is the linear transformation matrix, and $D' \leq D$. The decoder reconstructs the forecast to the original feature space after prediction as $ \hat{\bY} = \fm(\bZ) \bW_{\theta_{\dec}} $. Finally, the parameters of the encoder $\theta_{\enc}$ and the decoder $\theta_{\dec}$ are jointly optimized to minimize the objective in \cref{eq:prob1}.

\noindent\textbf{Deep non-linear AutoEncoders.} Deep non-linear AutoEncoders extend their linear counterparts by employing multiple layers of non-linear transformations. The encoder maps the input $ \bX $ to a latent space $ \bZ = \enc(\bX; \theta_{\enc}) $, where $ \enc $ is parameterized by a deep neural network. Similarly, the decoder reconstructs the predictions of the foundation model in the latent space:  $ \hat{\bY} = \dec(\fm(\bZ); \theta_{\dec})$.

Besides AutoEncoders, Normalizing Flows~\citep{Kobyzev_2021} such as \texttt{RealNVP}~\citep{dinh2017densityestimationusingreal} are a valid choice in the context of adapters, thanks to their inherently invertible nature. However, their training may be challenging due to various optimization related concerns. We defer a discussion on Normalizing Flows as adapters to \cref{appendix:flows}.

\subsection{Probabilistic Adapters}

We now discuss an alternative to the optimization of adapters, which is based on a Bayesian treatment of their parameters. 
There are many options on how to carry out inference over these parameters, and we can draw from the literature on Bayesian inference for neural networks \citep{PapamarkouICML24}. 

Considering a FM which yields point predictions, the appeal of Bayesian adapters is that they enable probabilistic predictions, which can be used for uncertainty quantification.
Note that this is the case for models such as \chronos and \moirai, which output a distribution over the time series continuous values\footnote{In the case of \chronos, this distribution is obtained through a categorical distribution (with \emph{softmax} probabilities) over a tokenized space of the time series values.}.
For deterministic FMs such as $\moment$ \citep{goswami2024moment}, a Bayesian treatment of adapters yields an ensemble of such predictions, which is key for accounting for the predictive uncertainty.

\noindent\textbf{Inference.} Recalling that $\theta$ represents the set of parameters of encoder ($\enc_\theta$) and decoder ($\dec_\theta$), we can attempt to obtain the posterior distribution over these parameters through Bayes theorem \cite{Gelman13}:
\begin{equation*}
p(\theta | \bY, \bX) \propto p(\bY | \bX, \theta) p(\theta)
\text{,}
\end{equation*}
where $p(\theta)$ is the prior distribution over the parameters and $p(\bY | \bX, \theta)$ the likelihood, with $\bY, \bX$ representing a training dataset in this context.
Alternatively, we can rather treat the latent representation $\bZ$ as stochastic, where the interest is now to characterize the following posterior: 
\begin{equation*}
p(\bZ | \bY, \bX) \propto p(\bY | \bX, \bZ) p(\bZ) \text{.}
\end{equation*}

In these two formulations, the posterior distribution over the parameters, is instrumental in obtaining predictive distributions useful for uncertainty quantification.
For instance, in the case of inference over $\theta$, 
%by integrating out model parameters. Therefore, 
for new test data $\bY^{*}, \bX^{*}$ we obtain:

\begin{equation*}
p(\bY^{*} | \bX^{*}, \bY, \bX) = \int p(\bY^{*} | \bX^{*}, \theta) p(\theta | \bY, \bX) d\theta \text{.}
\end{equation*}

Characterizing the posterior analytically, however, is intractable and we need to resort to approximations. 
The literature on Bayesian inference offers various strategies, which can be adapted to neural networks \citep{PapamarkouICML24}, including variational inference \citep{Graves11}, Laplace approximations \citep{Yang24}, and Markov chain Monte Carlo (MCMC) \citep{Chen14,Tran22}.

Within the \adapts framework, we focus in particular on variational inference (VI) for \vae adapters and on Monte Carlo dropout \citep{Gal16} as an approximate form of VI for carrying out inference over $\theta$. 
%In practice, instead of being Bayesian about the model parameters \( \theta \), we take a Bayesian perspective on the latent space. This means that we model the uncertainty of the latent representations \( \bZ \) rather than directly over \( \theta \). This perspective naturally leads to Variational AutoEncoders (\vae) as a choice of probabilistic adapters.

\noindent\textbf{Variational AutoEncoders.} Following the Bayesian perspective on adapters, $\vae$ assume a prior distribution over the latent representation $ \bZ $, typically $ \mathcal{N}(0, \mathbf{I}) $. The encoder then outputs parameters of the posterior distribution $ q_\phi(\bZ|\bX) $, and in our context, the decoder generates reconstructions of predictions $\hat{\bY} \sim p_\theta(\bY|\bX,\fm(\bZ)) $ where $\theta$ parametrize a likelihood model $p$. We then define the training objective of the $\vae$, which brings together the forecasting loss and a regularization term, in a similar way to the \emph{evidence lower bound} (ELBO)~\citep{Kingma2013AutoEncodingVB} objective:

\begin{proposition}[$\vae$ adapter training objective]
\label{prop:vae}
The training objective for the $\vae$ adapter is the maximization of an \emph{ELBO}-like lower bound on the marginal likelihood of the target $\bY$:
\begin{equation*}
\begin{split}
\log p_\theta(\bY|\bX, \fm) \geq & \mathbb{E}_{q_\phi(\bZ|\bX)} \left[ \log p_\theta(\bY|\bX, \fm(\bZ)) \right] \\
& - \mathrm{KL}\left(q_\phi(\bZ|\bX) \,\|\, p(\bZ)\right),
\end{split}
\end{equation*}
where $\mathrm{KL}$ denotes the Kullback-Leibler divergence. 
\end{proposition}

The derivation of this lower bound and a discussion on the implications of each term of the loss are deferred to~\cref{appendix:vae_proof}.

\begin{remark}
In practice, we use the Gaussian likelihood as our likelihood model: $p_\theta(\bY|\bX, \fm(\bZ)) = \mathcal{N}(\bY; \hat{\bY}, \sigma^2 \mathbf{I})$, with $\hat{\bY} = \dec_\theta(\fm(\bZ))$. In this case the forecasting loss term boils down to the MSE objective in \cref{eq:prob1} up to a multiplicative and additive noise-related constants: $\log \mathcal{N}(\bY; \hat{\bY}, \sigma^2 \mathbf{I}) = -\frac{1}{2\sigma^2} \| \bY - \hat{\bY} \|^2_{\mathrm{F}} - \frac{H D}{2} \log(2\pi\sigma^2)$. Notice that one can also learn a model of the noise where $ \dec_\theta(\fm(\bZ)) = [\mu_\theta(\bY|\bX, \fm(\bZ)), \sigma_\theta(\bY|\bX, \fm(\bZ))]$.
\end{remark}

\begin{remark}
The $\mathrm{KL}$ divergence regularization term can be multiplied by a scaling factor $\beta$ to control the disentanglement—independence of the latent representation components. This results in $\beta$-\vae~\citep{higgins2017betavae}, which is what we use in practice while referring to it as the \vae adapter throughout the paper. 
\end{remark}

\noindent\textbf{Dropout as approximate VI.} Dropout~\citep{srivastava14a} can be interpreted as a form of variational inference, where a variational distribution is imposed over the weights of a neural network \citep{Gal16}. Specifically, applying dropout during training corresponds to approximating a posterior over the weights using a Bernoulli distribution. This perspective allows the deterministic models introduced in \cref{subsec:families}, such as Linear AutoEncoders, to be transformed into probabilistic models by introducing stochasticity through dropout. 

% By treating dropout as a variational approximation, these models can capture uncertainty in their predictions while preserving their original structure, enabling a natural extension from deterministic to probabilistic inference frameworks.

Treating adapters in a Bayesian manner while keeping the FM fixed aligns with the concept of partially stochastic Bayesian neural networks, which provides theoretical guarantees on universal conditional density estimation \citep{Sharma23}. This framework ensures that the model can approximate any conditional density, provided that stochasticity is introduced early enough in the architecture and that the number of stochastic units matches or exceeds the output dimension. Using probabilistic adapters, We comply with these conditions by making the encoder stochastic, allowing the learned latent space to capture uncertainty while leveraging the FM’s fixed parameters.




% \begin{definition}
% \label{def:inj}
% A function $f:X \to Y$ is injective if for any $x,y\in X$ different, $f(x)\ne f(y)$.
% \end{definition}
% Using \cref{def:inj} we immediate get the following result:
% \begin{proposition}
% If $f$ is injective mapping a set $X$ to another set $Y$, 
% the cardinality of $Y$ is at least as large as that of $X$
% \end{proposition}
% \begin{proof} 
% Left as an exercise to the reader. 
% \end{proof}
% \cref{lem:usefullemma} stated next will prove to be useful.
% \begin{lemma}
% \label{lem:usefullemma}
% For any $f:X \to Y$ and $g:Y\to Z$ injective functions, $f \circ g$ is injective.
% \end{lemma}
% \begin{theorem}
% \label{thm:bigtheorem}
% If $f:X\to Y$ is bijective, the cardinality of $X$ and $Y$ are the same.
% \end{theorem}
% An easy corollary of \cref{thm:bigtheorem} is the following:
% \begin{corollary}
% If $f:X\to Y$ is bijective, 
% the cardinality of $X$ is at least as large as that of $Y$.
% \end{corollary}
% \begin{assumption}
% The set $X$ is finite.
% \label{ass:xfinite}
% \end{assumption}
% \begin{remark}
% According to some, it is only the finite case (cf. \cref{ass:xfinite}) that is interesting.
% \end{remark}