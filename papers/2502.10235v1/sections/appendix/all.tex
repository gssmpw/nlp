\section{Theoretical analysis}
\label{appendix:theory}

\subsection{Proof of \cref{prop:solution}}
\label{appendix:solution_proof}
We begin by restating the proposition, and its underlying assumptions:

\begin{assumption}
$\bW_{\varphi}$ has full rank: $\text{rank}(\bW_{\varphi}) = D$, insuring its invertibility.
\end{assumption}

\begin{assumption}
For ease of derivation, we consider a similar linear parametrization for the foundation model: $\fm(\bX) =  \bW_{\text{FM}}^\top \bX + \vb_{\text{FM}} \mbf{1}^\top$ where $\bW_{\text{FM}} \in \RR^{L \times H}$, $\vb_{\text{FM}} \in \RR^H$, and $\mbf{1}$ a vector of ones of dimension $D$.
\end{assumption}

\begin{proposition}[Optimal linear adapter]
Under \cref{ass:invertible} and \cref{ass:fm}, the \emph{closed-form} solution of the problem: 

\begin{equation}
    \Loss (\bW_{\varphi}) = \|\bY - \big( \bW_{\text{FM}}^\top \bX \bW_{\varphi} + \vb_{\text{FM}} \mbf{1}^\top \big) \bW_{\varphi}^{-1}\|_F^2
\end{equation}

writes as:

\begin{equation}
    \bW_{\varphi}^* = (\bB^\top \bA)^{+} \bB^\top \bB
\end{equation}

where $\bW_{\varphi}^* = \argmin_{\bW_{\varphi} \in \mathcal{GL}_D(\RR)} \Loss (\bW_{\varphi})$, $\bA = \bY -  \bW_{FM}^\top \bX$, $\bB = \vb_{FM} \mbf{1}^\top$, and $(\bB^{\top} \bA)^{+}$ denoting the \emph{pseudo-inverse} operator.

\end{proposition}

\begin{proof}

We begin by expanding the loss function:
\begin{align*}
    \Loss (\bW_{\varphi}) &= \|\bY - (\bW_{\text{FM}}^\top \bX \bW_{\varphi} + \vb_{\text{FM}} \mbf{1}^\top) \bW_{\varphi}^{-1} \|_F^2 \\
    &= \| \bA - \bB \bW_{\varphi}^{-1} \|_F^2
\end{align*}

where $\bA = \bY -  \bW_{FM}^\top \bX$ and $\bB = \vb_{FM} \mbf{1}^\top$. Expanding the Frobenius norm:
\begin{equation*}
    \Loss (\bW_{\varphi}) = \text{Tr} \left( (\bA - \bB \bW_{\varphi}^{-1})^\top (\bA - \bB \bW_{\varphi}^{-1}) \right)
\end{equation*}

Taking the gradient with respect to $\bW_{\varphi}^{-1}$ yields:

\begin{equation*}
    \frac{\partial \Loss}{\partial \bW_{\varphi}^{-1}} = - 2 \bB^\top \bA + 2 \bB^\top \bB \bW_{\varphi}^{-1}
\end{equation*}

Knowing that $\bW_{\varphi}$ is invertible, We have that: $ \frac{\partial \Loss}{\partial \bW_{\varphi}} =  - \bW_{\varphi}^{-\top} \frac{\partial \Loss}{\partial \bW_{\varphi}^{-1}} \bW_{\varphi}^{-\top}$

hence

\begin{equation*}
    \frac{\partial \Loss}{\partial \bW_{\varphi}} = -2 \bW_{\varphi}^{-T} \left( \bB^\top \bA - \bB^\top \bB \bW_{\varphi}^{-1} \right) \bW_{\varphi}^{-T}.
\end{equation*}

Setting $\frac{\partial \Loss}{\partial \bW_{\varphi}} = 0$ and multiplying both sides by $\bW_{\varphi}^\top$, we obtain:
\begin{equation*}
    \bB^\top \bA = \bB^\top \bB \bW_{\varphi}^{-1}.
\end{equation*}
Multiplying both sides by $\bW_{\varphi}$:
\begin{equation*}
     \bB^\top \bA \bW_{\varphi} = \bB^\top \bB.
\end{equation*}
Finally applying the pseudo-inverse to solve for $\bW_{\varphi}$ gives our final result:
\begin{equation*}
    \bW_{\varphi}^* = (\bB^\top \bA)^{+} \bB^\top \bB.
\end{equation*}

Given the convexity of $\Loss (\bW_{\varphi})$ (which follows from the convexity of the Frobenius norm $\norm{\cdot}_{\mathrm{F}}^2$, the inverse operation, and an affine transformation), we conclude that $\bW_{\varphi}^*$ is a global solution for \cref{eq:prob2}.

\begin{remark}
    We make use of the pseudo-inverse due to the current construction of the matrix $\bB$ (with identical rows) which implies that the product $\bB^\top \bA$ is degenerate. To bypass this limitation and further ensure the invertibility of $\bW_{\varphi}^*$, we can revisit the definition of the foundation model in \cref{ass:fm} to include channel dependent biases and ensure a full rank matrix $\bB$.
\end{remark}

\end{proof}
    
\subsection{Proof of \cref{prop:vae}}
\label{appendix:vae_proof}

To derive the evidence lower bound (ELBO) used in the training objective of the VAE adapter, we start from the marginal likelihood of the observed data $\bY$ given the inputs $\bX$ and foundation model $\fm$. The marginal likelihood is expressed as:
\begin{equation}
\log p_\theta(\bY|\bX, \fm) = \log \int p_\theta(\bY|\bX, \fm(\bZ)) p(\bZ) \, d\bZ,
\end{equation}
where $\bZ$ is the latent variable, $p_\theta(\bY|\bX, \fm(\bZ))$ is the likelihood model parameterized by $\theta$, and $p(\bZ)$ is the prior distribution over the latent variable $\bZ$.

Direct optimization of this marginal likelihood is generally intractable due to the integration over $\bZ$. To make this optimization feasible, we introduce a variational distribution $q_\phi(\bZ|\bX)$, parameterized by $\phi$, as an approximation to the true posterior $p_\theta(\bZ|\bX, \bY, \fm)$. Using $q_\phi(\bZ|\bX)$, we can reformulate the log-marginal likelihood as follows:
\begin{align}
\log p_\theta(\bY|\bX, \fm) 
&= \log \int q_\phi(\bZ|\bX) \frac{p_\theta(\bY|\bX, \fm(\bZ)) p(\bZ)}{q_\phi(\bZ|\bX)} \, d\bZ \\
&= \log \mathbb{E}_{q_\phi(\bZ|\bX)} \left[ \frac{p_\theta(\bY|\bX, \fm(\bZ)) p(\bZ)}{q_\phi(\bZ|\bX)} \right].
\end{align}

Using Jensen's inequality, we can derive a lower bound on this log-marginal likelihood:
\begin{align}
\log p_\theta(\bY|\bX, \fm) 
&\geq \mathbb{E}_{q_\phi(\bZ|\bX)} \left[ \log \frac{p_\theta(\bY|\bX, \fm(\bZ)) p(\bZ)}{q_\phi(\bZ|\bX)} \right] \\
&= \mathbb{E}_{q_\phi(\bZ|\bX)} \left[ \log p_\theta(\bY|\bX, \fm(\bZ)) \right] 
    - \mathbb{E}_{q_\phi(\bZ|\bX)} \left[ \log \frac{q_\phi(\bZ|\bX)}{p(\bZ)} \right].
\end{align}

The second term can be rewritten as the Kullback-Leibler (KL) divergence between the variational posterior $q_\phi(\bZ|\bX)$ and the prior $p(\bZ)$:
\begin{equation}
\mathrm{KL}\left(q_\phi(\bZ|\bX) \,\|\, p(\bZ)\right) = \mathbb{E}_{q_\phi(\bZ|\bX)} \left[ \log \frac{q_\phi(\bZ|\bX)}{p(\bZ)} \right].
\end{equation}

Substituting this into the inequality, we obtain the evidence lower bound (ELBO):
\begin{equation}
\log p_\theta(\bY|\bX, \fm) \geq \mathbb{E}_{q_\phi(\bZ|\bX)} \left[ \log p_\theta(\bY|\bX, \fm(\bZ)) \right] 
    - \mathrm{KL}\left(q_\phi(\bZ|\bX) \,\|\, p(\bZ)\right).
\end{equation}

The ELBO consists of two terms:
\begin{itemize}
    \item The \emph{forecasting} term, $\mathbb{E}_{q_\phi(\bZ|\bX)} \left[ \log p_\theta(\bY|\bX, \fm(\bZ)) \right]$, which measures how well the model can reconstruct $\bY$ given the latent variable $\bZ$.
    \item The \emph{regularization} term, $\mathrm{KL}\left(q_\phi(\bZ|\bX) \,\|\, p(\bZ)\right)$, which encourages the variational posterior to stay close to the prior distribution $p(\bZ)$.
\end{itemize}

Thus, maximizing the ELBO provides a tractable way to train the parameters $\theta$ and $\phi$ by optimizing the balance between forecasting accuracy and latent space regularization.
\qed

\section{Normalizing Flows}
\label{appendix:flows}

Normalizing Flows make use of invertible transformations to map a simple base distribution (e.g. Gaussian) to a complex data distribution. Each transformation $ T $ is designed to maintain invertibility and efficient Jacobian computation. The transformation is applied iteratively: $ \bZ = T_k \circ T_{k-1} \circ \dots \circ T_1(\bX) $. Current Normalizing Flow instantiations (e.g. \texttt{RealNVP}) make use of generic invertible transformations such as \emph{coupling flows}; the latters can be parametrized using a neural network leading to powerful non-linear generative models that are trained to maximize the data log-likelihood:
\[
\log p(\bX) = \log p(\bZ) + \sum_{i=1}^k \log \left| \det \frac{\partial T_i(\cdot;\theta)}{\partial \bZ_{i-1}} \right|
\]
where $\theta$ denote the parameters of the non-linear parametrization of the invertible transformations $T_i$, and $\bZ_{i-1}$ is the output of the transformation $T_{i-1}$.
In the context of time series adapters, we directly optimize the parameters of the transformations based on their direct and inverse application on the time series forecasting problem:

\begin{equation*}
\begin{split}
\Loss_{\text{flow}} = \| \bY - &T^{-1}_{1} \circ T^{-1}_{2} \circ \dots \circ T^{-1}_k( \\ &\fm \big( T_k \circ T_{k-1} \circ \dots \circ T_1(\bX; \theta) \big); \theta) \|_F^2
\end{split}
\end{equation*}

where the encoder is represented by the series of direct transformations: $\enc(\cdot) = T_k \circ T_{k-1} \circ \dots \circ T_1(\cdot; \theta)$, and respectively the decoder by the series of inverse transformations $\dec(\cdot) = T^{-1}_{1} \circ T^{-1}_{2} \circ \dots \circ T^{-1}_k(\cdot; \theta)$.

As defined here, Normalizing Flows suffer from the constraint of keeping the same dimension in both original and learned representation space. For this purpose, we investigate coupling a normalizing flow with a linear encoder-decoder type of architecture to enable dimensionality reduction prior to applying the transformation $T_i$. The parameters of the additional encoder and decoder are then jointly trained to optimize the learning objective $\Loss_{\text{flow}}$.

Given that the parameters of the encoder and the decoder are shared in Normalizing Flows, the gradient-based optimization within our framework receives conflicting directions due to gradient flow from both the direct and inverse transformations simultaneously. We discovered that this adapter construction was challenging to optimize in practice, and we defer the exploration of this direction to future research endeavors.


% \section{Probabilistic Adapters as Partially Stochastic Bayesian Neural Networks}

% \maurizio{
% An interesting question is what properties we should expect to obtain from this approach, given that our goal is to operate with adapters {\em without} inferring the parameters of FMs.  
% It turns out that treating adapters in a Bayesian fashion, while keeping the FM fixed, can be seen as an instantiation of a partially stochastic Bayesian neural network, which has recently been studied in \citet{Sharma23}.
% Remarkably, this work offers theoretical guarantees on the level of stochasticity needed in order to be able to model any conditional density in the output. 
% }


% \maurizio{
% In our work we can leverage such important theoretical development of Bayesian neural networks in order to design adapters satisfying the property of universal conditional density approximation.
% In particular, (i) we need to ensure that stochasticity is established early enough in the architecture so that the model has the possibility to map this into any conditional density in the output. 
% Then, (ii) we need to ensure that the number of stochastic units is at least as large as the dimension of the output. 
% }


% \maurizio{
% We can easily design adapters following these constraints, and as a result we have a guarantee of universal conditional density approximation.
% Making the encoder stochastic, regardless on the treatment of the decoder (either optimized or inferred) is a simple way to ensure that the FM combined with the adapter is a universal approximator of conditional densities.
% It is also possible to reduce the level of stochasticity of the encoder down to $D$, which is the dimensionality of the multivariate time series, meaning that we can treat $D$ variables in a Bayesian way while optimizing the rest; alternatively, we can simply introduce $D$ random variables which are concatenated to the input, and the encoder parameters are then optimized. 
% See \citet{Sharma23} for a discussion on valid ways to satisfy the conditions of the universal conditional density approximation theorem.
% }

% \maurizio{
% An important note on the theory is that the conditions expressed in \citet{Sharma23} do not specify the optimal level of stochasticity for a given problem.  
% Much like the universal approximation theorem for neural networks, the theory establishes sufficient conditions for guaranteeing universal approximation. 
% However, choosing the right level of stochasticity and architecture remains a model-selection problem.
% }


\section{Experimental setup}
\label{appendix:exp_setup}

\subsection{Datasets}
\label{appendix:datasets}

\begin{table}[htbp]
    \centering
    \caption{Characteristics of the multivariate time series datasets used in our experiments with various sizes and dimensions.}
    \label{tab:dataset_description}
    \scalebox{1}{
    \begin{tabular}{lcccc}
        \toprule
         Dataset & ETTh1 & Illness & ExchangeRate & Weather\\
         \midrule
         \# features & $7$ & $7$ & $8$ & $21$\\
         \# time steps & $13603$ & $169$ & $6791$ & $51899$ \\
         Granularity & 1 hour & 1 week & 1 day & 10 minutes \\
         (Train, Val, Test) & (8033, 2785, 2785) & (69, 2, 98)  & (4704, 665, 1422) & (36280, 5175, 10444) \\
         \bottomrule
    \end{tabular}
    }
\end{table}

\subsection{Implementation details}
\label{appendix:implem}

In this section, we describe the full \adapts framework, starting from the data preprocessing, the training algorithm, and the hyperparameters optimization.

\paragraph{Preprocessing.} Given that the adapter as defined in \cref{def:adapter} is a feature space transformation, we start by rescaling (\emph{StandardScaler} and \emph{MinMaxScaler}) the data where all the timesteps are regarded as data points. To account for the temporal specificities in each batch, we use Reversible Instance Normalization (RevIn) \citep{kim2022reversible} that has been proven to mitigate time-related distribution shifts in time series problems. Finally, and following the observation that $\pca$ when composed with a linear adapter showed the best result in the case of correlated data (\cref{fig:linear_synthetic}), we include the possibility of applying \emph{full-component} $\pca$ as part of our data pre-processing pipeline.

% We describe the adapter training loop in \cref{alg:training}.
\paragraph{Training parameters.} After the pre-processing phase, we proceed to split the data into a \emph{train-validation-test} sets, where the validation set serves as a tool to select the best hyperparameters for the adapter. The resulting adapter that is instantiated with the optimal hyperparameters is then tested against the unseen test dataset. For all of our experiments, we first train the linear forecasting head of \moment (referred to as \emph{Linear Probing} in \cite{goswami2024moment}) with the Adam optimizer \citep{kingma2017adammethodstochasticoptimization}, a batch size of $32$, a one cycle scheduler starting with $0.001$ as learning rate. Once the forecasting linear head is trained, we freeze its parameters and proceed to training the adapter. This is done using the Adam optimizer, a batch size of $32$, a reduce on plateau scheduler starting with $0.001$ as learning rate.

% \begin{algorithm}[ht]
%    \caption{Training a model using the multi-step loss}
%    \label{alg:training}
% \begin{algorithmic}
%    \STATE {\bfseries Input:} model $\hat{p}_\theta$, trajectory $\tau=(s_{t+i},a_{t+i})_{i=0}^h$, loss function $L$, horizon $h$, normalized weights $\{\alpha_i\}_{i=1}^h$
%    \STATE Initialize the loss $l = 0$
%    \STATE $s=s_t$
%    \FOR{$i=0$ {\bfseries to} $h-1$}
%    \STATE Sample from the model $\hat{s}_{t+i+1} \to \hat{p}_\theta(s, a_{t+i})$
%    \STATE Update the loss $l \mathrel{+}= \alpha_i \cdot L(\hat{s}_{t+i+1}, s_{t+i+1})$
%    \STATE $s=\hat{s}_{t+i+1}$
%    \ENDFOR
%    \STATE Update $\theta$ using a gradient step minimizing $l$
% \end{algorithmic}
% \end{algorithm}


\paragraph{Hyperparameter optimization.} In order to select the best hyperparameters for the adapter architecture we use \emph{Ray tune}~\citep{liaw2018tuneresearchplatformdistributed} with the Heteroscedastic and Evolutionary Bayesian Optimisation solver (HEBO) \citep{Cowen-Rivers2022-HEBO} engine, reporting the average mean squred error (MSE) from \emph{k-fold} cross validation. \cref{table:hypers} shows the default hyperparameters for each considered adapter.

\begin{table}[!ht]
  \scriptsize
  \caption{Adapters hyperparameters.}
  \label{table:hypers}
  \centering
  \begin{tabular}{lllll}
    \toprule
    adapter
    & \texttt{LinearAE}
    & \texttt{DropoutLinearAE}
    & \texttt{LinearVAE}
    & \texttt{VAE}
    \\
    \cmidrule(r){1-5}
    p dropout
    & $-$   
    & $0.1$ 
    & $-$ 
    & $-$ 
    \\
    Number of layers
    & $-$   
    & $-$ 
    & $-$ 
    & $2$ 
    \\
    Hidden dimension
    & $-$   
    & $-$ 
    & $-$ 
    & $128$
    \\
    $\beta$
    & $-$   
    & $-$ 
    & $0.5$
    & $0.5$
    \\
    $\sigma$
    & $-$   
    & $-$ 
    & $1.0$
    & $1.0$
    \\
    \bottomrule
  \end{tabular}
\end{table}



\section{Additional results}
\label{appendix:results}


\subsection{$\moment$ applied to synthetic data.}
\label{appendix:moment_synth}

To validate the adapter \emph{optimality} condition with large non-linear foundation models, we use $\moment$ \citep{goswami2024moment}. The optimal linear adapter in this case minimizes the following intractable objective:

\begin{equation}
\label{eq:moment_linear}
    \Loss (\bW_{\varphi}) = \|\bY - f_{\text{$\moment$}}\big(\bX \bW_{\varphi} \big) \bW_{\varphi}^{-1}\|_{\mathrm{F}}^2
\end{equation}

To approximately solve this optimization problem, we instantiate $\bW_{\varphi}$ as a single-linear-layer encoder denoted $\enc_\theta$, and respectively the inverse transformation $\bW_{\varphi}^{-1}$ as a single-linear-layer decoder denoted $\dec_\theta$. We then use gradient-based optimization of the parameters $\theta$ using the Adam optimizer, aiming at solving the following optimization problem:

\begin{equation}
\label{eq:theta}
    \theta^* = \argmin_\theta \|\bY - \dec_\theta \big( f_{\text{$\moment$}}(\enc_\theta (\bX)) \big)\|_{\mathrm{F}}^2
\end{equation}

\begin{figure}
     \centering
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
        \includegraphics[width=\textwidth]{figures/moment_synthetic_independent.pdf}
         \caption{Independent}
         \label{fig:indp}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
        \includegraphics[width=\textwidth]{figures/moment_synthetic_correlated.pdf}
         \caption{Correlated}
         \label{fig:corr}
     \end{subfigure}
        \caption{$\moment$ on simulated independent data.}
        \label{fig:moment_synthetic}
\end{figure}

\cref{fig:moment_synthetic} shows the performance gain obtained by optimizing a linear adapter on \emph{$\moment$-small} foundation model. Unlike the tractable case, we observe that in both data modalities (independent and correlated data), $\pca$ has little to no improvement over the identity baseline, while $\varphi_{\theta^*}$ reaches an order of magnitude better solution. This confirms our intuition about the existence of a better solution than the identity matrix, even in the case of real-world complex foundation models.

% \subsubsection{Physical system data}

% To further strengthen our findings, we apply the same protocol as before to dynamics trajectories simulated from a Markovian physical system: \emph{Pendulum}. \abdelhakim{[Introduce pendulum from gym, add figure, and comment]}. 

\subsection{Mean Absolute Error}

\begin{table*}[ht]
    \centering
    \begin{tabular}[t]{cccccccc}
        \toprule[\thick pt]
        \multirow{2}{*}{Dataset} & \multirow{2}{*}{H} & \multicolumn{1}{c}{No adapter} & \multicolumn{5}{c}{with adapter} \\
        \cmidrule(lr){3-3} 
        \cmidrule(lr){4-8}
         & & $\text{Moment}_{\text{small}}$ & pca & linear & dropout & linear VAE & VAE \\
        \midrule[\thick pt]
        \multirow{2}{*}{ETTh1} & 96 & $0.422_{\pm 0.006}$ & $0.440_{\pm 0.000}$ & $0.423_{\pm 0.003}$ & $\mathbf{0.415_{\pm 0.002}}$ & $0.420_{\pm 0.001}$ & $0.426_{\pm 0.001}$ \\
         & 192  &  $\mathbf{0.436_{\pm 0.000}}$ & $0.445_{\pm 0.000}$ & $0.449_{\pm 0.003}$ & $0.450_{\pm 0.001}$ & $0.451_{\pm 0.001}$ & $0.444_{\pm 0.001}$  \\
        \cmidrule(lr){1-8}
        \multirow{2}{*}{Illness} & 24 & $1.143_{\pm 0.007}$ & $1.163_{\pm 0.001}$ & $2.624_{\pm 0.035}$ & $1.156_{\pm 0.016}$ & $1.074_{\pm 0.011}$ & $\mathbf{1.057_{\pm 0.012}}$ \\
         & 60  & $1.149_{\pm 0.001}$ & $1.161_{\pm 0.001}$ & $1.227_{\pm 0.030}$ & $1.173_{\pm 0.015}$ & $1.112_{\pm 0.021}$ & $\mathbf{1.105_{\pm 0.021}}$ \\
         \cmidrule(lr){1-8}
        \multirow{2}{*}{Weather} & 96 & $0.232_{\pm 0.010}$ & $0.235_{\pm 0.000}$ & $0.226_{\pm 0.000}$ & $0.212_{\pm 0.001}$ & $\mathbf{0.218_{\pm 0.001}}$ & $0.243_{\pm 0.001}$ \\
         & 192 & $\mathbf{0.251_{\pm 0.001}}$ & $0.260_{\pm 0.001}$ & $\mathbf{0.251_{\pm 0.001}}$ & $\mathbf{0.251_{\pm 0.000}}$ & $0.255_{\pm 0.000}$ & $0.274_{\pm 0.000}$ \\
        \cmidrule(lr){1-8}
        \multirow{2}{*}{ExchangeRate} & 96 & $\mathbf{0.252_{\pm 0.010}}$ & $0.264_{\pm 0.000}$ & $0.308_{\pm 0.010}$ & $0.269_{\pm 0.012}$ & $0.376_{\pm 0.031}$ & $0.488_{\pm 0.003}$ \\
         & 192 & $\mathbf{0.329_{\pm 0.001}}$ & $0.335_{\pm 0.000}$ & $0.415_{\pm 0.002}$ & $0.419_{\pm 0.010}$ & $0.513_{\pm 0.010}$ & $0.585_{\pm 0.008}$ \\
        \bottomrule[\thick pt]
    \end{tabular}
    \caption{Performance comparison between the baseline $\moment$ model without adapters against different adapter architectures (\pca, $\texttt{LinearAE}$, $\texttt{dropoutLAE}$, $\texttt{LinearVAE}$, and \vae), for multivariate long-term forecasting with different horizons $H$. We display the average test MAE $\pm$ standard error obtained on $3$ runs with different seeds. \textbf{Best} results are in bold, with lower values indicating better performance.}
    \label{table:results_mae}
\end{table*}

% \begin{figure}[ht]
% \centering
% \includegraphics[width=0.5\textwidth]{figures/result_ETTh1.pdf}
% \caption{ETTh1.}
% \label{fig:etth1}
% \end{figure}

% \begin{figure}[ht]
% \centering
% \includegraphics[width=0.5\textwidth]{figures/result_ETTh2.pdf}
% \caption{ETTh2.}
% \label{fig:etth2}
% \end{figure}

% \begin{figure}[ht]
% \centering
% \includegraphics[width=0.5\textwidth]{figures/result_Weather.pdf}
% \caption{Weather.}
% \label{fig:weather}
% \end{figure}

% \begin{figure}[ht]
% \centering
% \includegraphics[width=0.5\textwidth]{figures/result_ExchangeRate.pdf}
% \caption{ExchangeRate.}
% \label{fig:ExchangeRate}
% \end{figure}

% \begin{figure}[ht]
% \centering
% \includegraphics[width=0.5\textwidth]{figures/result_Illness.pdf}
% \caption{Illness.}
% \label{fig:Illness}
% \end{figure}