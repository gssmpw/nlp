\section{Related work}
\label{sec:related}

\noindent\textbf{Time Series Foundational Models.} Over the past two years, a plethora of foundation models have been proposed with a particular focus on time series forecasting. Some of these models like $\texttt{GPT4TS}$~\citep{zhou2023onefitsall} and $\texttt{Time-LLM}$~\citep{jin2023time} \rebuttal{``reprogram''} a Large Language Model to the forecasting setting by freezing most of its layers and fine-tuning additional time series-specific modules to a new downstream task. The majority of these time series FMs including \texttt{Lag-Llama}~\citep{rasul2024lagllama}, $\chronos$~\citep{ansari2024chronos}, $\moirai$~\citep{liu2024moirai}, \texttt{TimesFM} \citep{das2024a} and \moment~\citep{goswami2024moment} are trained from scratch on a large volume of time series data.

\noindent\textbf{Adapters.} The multivariate setting presents a significant challenge for time series FMs, as different tasks involve varying numbers of channels\footnote{Throughout the paper, the words \emph{features}, \emph{channels}, and \emph{components} are used interchangeably to refer to the number of variates in a multivariate time series, represented as $D$ in \cref{sec:adapters}.}. To the best of our knowledge, the only model that naturally accommodates any number of channels is $\moirai$~\citep{liu2024moirai}, which, however, suffers from high computational demand due to processing all channels flattened in the transformer simultaneously, leading to a quadratic memory complexity w.r.t. to the number of channels. Most foundation models, instead, treat each one of these independently, which, as noted by~\citet{feofanov2024adapters}, remains computationally expensive when full fine-tuning is required. For classification tasks with hundreds or thousands of features, they demonstrated that simple adapters like the rotation matrix obtained through Principal Components Analysis ($\pca$) mitigate this issue. At the same time,~\citet{benechehab2025zeroshot} showed that $\pca$ preserves channel interactions by learning new disentangled components. However, in both cases, $\pca$ provided little improvement over independent processing, leaving room for further enhancements. In the context of tabular regression, foundation models such as \citep[$\texttt{TabDPT}$]{ma2024tabdptscalingtabularfoundation} also use \pca to adapt to a variable number of features. 

Less related to our work, \citet{cheng2016_gpadapter} use a Gaussian process adapter in the context of irregular time series classification. In other domains, adapters have been used for multimodal (text-time series) representation learning~\citep{zhang2024dualtimedualadaptermultimodallanguage} and computer vision~\citep{LI2025, yin2023adapterneedtuningvisual, pan2022}.

% \noindent\textbf{Multivariate time series representation learning.} [fill here].










% \begin{itemize}
%     \item \vasilii{GPT4TS\citep{zhou2023onefitsall} leverages a pre-trained LLM (GPT-2 in their case) for diverse time-series tasks including forecasting. Their approach handles the multivariate setting through the tokenization block, which has to be trained for each task similarly to adapters. \href{https://github.com/DAMO-DI-ML/One_Fits_All}{code}.}
%     \item \citep[\textbf{TIME-LLM}]{jin2023time}. Introduces "reprogramming" large language models for time series, using pre-prompts and a frozen LLM, while training a time series-specific module.

%     \item \citep[\textbf{Lag-Llama}]{rasul2024lagllama}. Presents a foundation model with 2.45M parameters optimized for probabilistic time series forecasting.

%     \item \citep[\textbf{MOMENT}]{goswami2024moment}. Provides open-source time series models with parameter sizes of 37.9M, 113M, and 346M.

%     \item \citep[\textbf{TimesFM}]{das2024a}. A 200M parameter model by Google, focusing on decoder-only architecture for time series forecasting.
    
%     \item \citep[\textbf{UniTS}]{gao2024building}. .
    
% \end{itemize}