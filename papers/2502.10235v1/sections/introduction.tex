\section{Introduction}
\label{sec:intro}

\begin{figure}[t]
     \centering
     \begin{subfigure}[b]{\columnwidth}
         \centering
        \includegraphics[width=\textwidth]{figures/traj.pdf}
         \caption{}
         \label{fig:traj}
     \end{subfigure}
     \vfill
     \begin{subfigure}[b]{\columnwidth}
         \centering
        \includegraphics[width=\textwidth]{figures/pipeline.png}
         \caption{}
         \label{fig:pipeline}
     \end{subfigure}
\caption{(a) Augmenting $\moment$ time series foundation model with the \adapts framework provides \emph{probabilistic} and \emph{more accurate} predictions. (b) \textbf{The \adapts framework:} The input time series is transformed through a feature space transformation $\varphi$ that maps into a stochastic latent space. The prediction is then conducted using a pre-trained FM before transforming back the predicted, now distribution, to the original feature space. The fire symbol indicate trainable weights while the snowflake implicates that the parameters of the FM are kept frozen.
}
\label{fig:main}
\end{figure}

Time series forecasting is a well-established machine learning problem that involves analyzing sequential data to predict future trends based on historical patterns. Two key challenges frequently arise in this context: (a) time series are often multivariate, incorporating multiple descriptive features~\citep{wei2019multivariate}, and (b) estimating the uncertainty of a forecast is equally important, requiring probabilistic model outputs~\citep{gneiting2014probabilistic}. These challenges are particularly relevant in real-world applications where risk assessment depends on reliable forecasts, such as healthcare~\citep{jones2012improved}, finance~\citep{groen2013real}, energy management~\citep{zhang2014review,nowotarski2018recent}, and weather prediction~\citep{palmer2012towards,bi2023accurate}.  

Existing foundation models (FMs) for time series forecasting, such as \chronos~\citep{ansari2024chronos}, are typically trained for univariate forecasting tasks due to tractability constraints, as the wide range of real world time series problems typically have different numbers of features. Even without discretization, handling multivariate time series directly within these models (\moment~\citep{goswami2024moment}, \moirai~\citep{liu2024moirai}) remains computationally challenging due to the high-dimensional dependencies among features. This limitation raises a fundamental question: how can we leverage existing pre-trained univariate FMs to enable probabilistic forecasting for multivariate time series?  

To address this, we introduce \adapts, a novel framework designed to augment FMs with probabilistic adapters. As illustrated in Figure~\ref{fig:main}, \adapts applies a stochastic feature transformation that maps the input time series into a latent space, where predictions are made using the frozen FM. Our framework sets itself apart from existing literature by enforcing an invertibility constraint on the adapter, allowing predictions to be transformed back into the original feature space. Beyond enhancing forecasting accuracy, the integration of stochasticity into the adapter's latent representation ensures that the model captures uncertainty, thereby improving both calibration and robustness.

Our approach leads to several novel insights and contributions, which we summarize as follows:  

\begin{enumerate}
    \item \textbf{Multivariate FM adaptation.} We introduce a principled methodology for adapting existing pre-trained univariate FMs to multivariate probabilistic forecasting, resulting in the \adapts framework. 
    \item \textbf{Theoretical foundations of adapters.} We provide a theoretical analysis to support the necessity of adapters, starting with the analytically tractable case of linear adapters and linear FMs. We then build on the literature on partially stochastic Bayesian neural networks to introduce probabilistic adapters.  
    \item \textbf{Empirical validation.} We conduct extensive experiments on multivariate time series forecasting benchmarks, demonstrating that our approach improves forecasting accuracy baseline methods. We also analyze the interpretability of the learned latent representation and show that adapters enable cost-effective adaptation by reducing the dimensionality of the feature space. 
\end{enumerate}

The rest of this paper is organized as follows: ~\cref{sec:related} discusses related work, ~\cref{sec:adapters} details the problem setup and the theoretical analysis on linear adapters. ~\cref{sec:prob_adapters} extends our framework to probabilistic adapters, and ~\cref{sec:exps} showcases experimental results. Finally, we conclude with limitations and future directions in ~\cref{sec:discussion}.