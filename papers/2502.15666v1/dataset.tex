\subsection{Initial Dataset}
In this study, we begin with purely human-written texts (HWT) and refine them using various large language models (LLMs). Building on the work of \citet{zhang2024llm}, we utilize HWT samples from their `MixSet' dataset. These samples are carefully selected based on two key criteria: (1) they were created prior to the widespread adoption of LLMs, and (2) they span six distinct domains. For clarity, we refer to this baseline HWT dataset as the `No-Polish-HWT' set. This set comprises 300 samples, with 50 samples per domain (details in Table \ref{tab:dataset_hwt_basic}).


\subsection{Dataset Preparation}
As we generate the AI-polished versions of our No-Polish-HWT samples, we adjust the level of AI/LLM involvement. We employ two distinct polishing strategies: --
\begin{enumerate}
    \item \textbf{Degree-Based Polishing:} The LLM is prompted to refine the text in four varying degrees of modification: -- (1) extremely-minor, (2) minor, (3) slightly-major, and (4) major.

    \item \textbf{Percentage-Based Polishing:} The LLM is instructed to modify a fixed percentage $(p\%)$ of words in a given text. The percentage is systematically varied across the following values: $p\% = \{1, 5, 10, 20, 35, 50, 75 \}\%$.
\end{enumerate}

As a result, each HWT sample is transformed into 11 distinct AI-polished variants. For the LLM-polishing, we employ four different models: GPT-4o, Llama3.1-70B, Llama3-8B, and Llama2-7B. Each model is carefully prompted to generate the highest-quality output, preserving the original semantics of the text (details provided in Appendix \ref{app:llm_prompt}). Figure \ref{fig:extreme_minor_gpt_sample} illustrates a randomly selected sample from our dataset, which has been polished by GPT-4o with an extremely minor modification.

% Consequently, for each HWT sample, we consider $11$ different variants of an AI-polished text.
% As our LLM polisher, we consider three different models -- GPT-4o, LLaMa3-8B-Instruct, and LLaMa2-7B-chat models. Each LLM is systematically prompted to get the best quality output (details in Appendix \ref{}). Figure \ref{fig:extreme_minor_gpt_sample} shows a random sample from our dataset that was polished by the GPT-4o with extremely minor modification.


\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{images/samples/gpt_extreme_minor1_new.pdf}
    \caption{Random sample from our APT Eval dataset. Original HWT on left; Polished version on right.}
    \label{fig:extreme_minor_gpt_sample}
\end{figure}


\subsection{Dataset Analysis}
To assess the differences and deviations between pure HWT and AI-polished text, we employ three key metrics: Cosine semantic similarity, Jaccard distance, and Levenshtein distance. For semantic similarity, we compute the cosine similarity between the embeddings of the original and AI-polished texts (APT) using the BERT-base model. To ensure that the polished samples retain a strong resemblance to the original text, we filter out any samples with a semantic similarity below $0.85$. Figure \ref{fig:sem_sim_gpt4} shows the distribution for APTs (degree-based) with their mean value (see all metrics, and plots in Appendix \ref{app:data_analysis}).

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{images/data_plots/multiple_semantic_similarity_polish_type_gpt.pdf}
    \caption{Distribution of Semantic Similarity for Degree-based AI-Polished Texts by GPT-4o.}
    \label{fig:sem_sim_gpt4}
\end{figure}

\begin{table}[h]
\centering
\resizebox{\linewidth}{!}{%
\begin{tabular}{l|llll|c}
\multirow{2}{*}{\textbf{Polish Type}}                                    & \multicolumn{4}{c|}{\textbf{Polisher LLM}} & \multirow{2}{*}{\textbf{Total}} \\ \cline{2-5}
 &
  \textbf{GPT-4o} &
  \textbf{\begin{tabular}[c]{@{}l@{}}Llama3.1\\ -70B\end{tabular}} &
  \textbf{\begin{tabular}[c]{@{}l@{}}Llama3\\ -8B\end{tabular}} &
  \textbf{\begin{tabular}[c]{@{}l@{}}Llama2\\ -7B\end{tabular}} &
   \\ \hline
\textbf{\begin{tabular}[c]{@{}l@{}}no-polish / \\ pure HWT\end{tabular}} & -         & -        & -        & -        & 300                             \\ \hline
\textbf{\begin{tabular}[c]{@{}l@{}}Degree-\\ based\end{tabular}}         & 1152      & 1085     & 1125     & 744      & 4406                            \\ \hline
\textbf{\begin{tabular}[c]{@{}l@{}}Percentage-\\ based\end{tabular}}     & 2072      & 2048     & 1977     & 1282     & 7379                            \\ \hline
\textbf{Total}                                                           & 3224      & 3133     & 3102     & 2026     & \textbf{11785}                 
\end{tabular}%
}
\caption{Our APT Eval Dataset}
\label{tab:apt_eval}
\end{table}

After filtering, the final APT-Eval dataset consists of \textbf{11.7K} samples, providing a robust benchmark for evaluating AI-text detection systems. Table \ref{tab:apt_eval} shows the total number of samples for each strategy and polisher (more details in table \ref{tab:apt_eval_details}).
%After the analysis and filtration, we get our final AI-polished dataset of size \textbf{$11.7K$}, named \textbf{`APT-Eval'} dataset. 


