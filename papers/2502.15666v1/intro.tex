%Previous Intro Motivation
% The rapid advancement of LLMs has transformed the landscape of text generation, enabling artificial intelligence to produce human-like text with remarkable fluency. This progress has raised concerns about the detectability of AI-generated content, leading to the development of various AI-text detectors \citep{gehrmann2019gltr, mitchell2023detectgpt, hu2023radar}. These detectors aim to distinguish between human-written text (HWT) and AI-generated text, ensuring authenticity in academic, journalistic, and online discourse. 

% However, an emerging challenge in this space is the phenomenon of AI-polished text, where human-authored content undergoes refinement using AI tools. Unlike fully AI-generated text, AI-polished text retains the core essence of human writing while subtly enhancing its style, grammar, and coherence.


% The ability of current AI-text detectors to accurately classify AI-polished text remains largely under-explored. While past research distinguishes fully AI-generated from human-authored text, many now use LLMs for refinement rather than generation. This raises concerns: misclassifying minimally polished text may unfairly penalize human writers, while failing to detect extensive AI assistance questions detection reliability.



% New Intro Motivation
The rapid advancement of LLMs has enabled AI to generate highly fluent, human-like text, raising concerns about detectability and prompting the development of various AI-text detectors \citep{gehrmann2019gltr, mitchell2023detectgpt, hu2023radar}.
However, the distinction between AI-generated and human-written text remains a gray area, particularly when human-authored content is refined using AI tools. \textit{If a human-written text is slightly polished by AI, should it still be classified as human-written, or does it become AI-generated?} Misclassifying such text can lead to false plagiarism accusations and unfair penalties, especially when detectors flag minimally polished content as AI-generated.\footnote{\href{https://www.usatoday.com/story/life/health-wellness/2025/01/22/college-students-ai-allegations-mental-health/77723194007/}{USAToday News on false AI allegations}}

Additionally, reports claiming that a large percentage of online content is AI-generated --such as assertions that \textit{"$40\%$ of Medium articles are AI-written"} -- often fail to account for AI-polished text.\footnote{\href{https://tinyurl.com/3jxktwkr}{Wired news on Medium}}$^{,}$\footnote{\href{https://tinyurl.com/594ydhaj}{NewsBytes report on Medium}} These sweeping claims risk misrepresenting the actual extent of AI involvement, leading to misleading statistics and misplaced skepticism about human authorship. Motivated by these issues, our study systematically examines how AI-text detectors respond to AI-polished text and whether their classifications are both accurate and fair.

To investigate this issue, we introduce the \textbf{AI-Polished-Text Evaluation (APT-Eval)} dataset of size \textbf{11.7K}, which systematically examines how AI-text detectors respond to varying degrees of AI involvement in human writing. Our dataset is built from pre-existing human-written samples that are refined using different LLMs, such as GPT-4o \citep{openaiGPT4}, Llama3-70B \citep{dubey2024llama}, etc., applying degree and percentage based modifications. This allows us to assess how detectors respond to minor and major AI polishing. We analyze the classification accuracy, false positive rates, and domain-specific sensitivities of \textbf{11 state-of-the-art detectors}, spanning model-based, metric-based, and commercial systems.


Our findings reveal critical weaknesses in existing AI-text detection systems. AI-text detectors exhibit alarmingly high false positive rates, often flagging very minimally polished text as AI-generated. Furthermore, detectors struggle to differentiate between minor and major AI refinements, suggesting that they may not be as reliable as previously assumed. We also uncover biases against smaller or older LLMs, where polishing done by less advanced models is more likely to be flagged than text refined by state-of-the-art LLMs. Furthermore, our study highlights inconsistencies in detection accuracy across different text domains, raising questions about bias and reliability. By shedding light on these issues, our research provides valuable insights into the evolving challenges of AI-assisted writing and the limitations of current AI-text detection methodologies. Our code and dataset are publicly available: \url{https://github.com/ShoumikSaha/ai-polished-text.git}

%By making our code and dataset publicly available, we aim to advance research on AI-polished text detection.

%Even minor AI-polish can lead to alarmingly high false positive rates, suggesting that current detectors struggle to differentiate between human writing and lightly polished content. Furthermore, our study highlights inconsistencies in detection accuracy across different LLMs and text domains, raising questions about bias and reliability. By shedding light on these issues, our research provides valuable insights into the evolving challenges of AI-assisted writing and the limitations of current AI-text detection methodologies.