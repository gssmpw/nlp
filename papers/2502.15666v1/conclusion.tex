%Our study reveals critical limitations in current AI-text detection systems when faced with AI-polished human-written text. We found that detectors exhibit high false positive rates, often misclassifying even minimally polished text as AI-generated. Additionally, most detectors struggle to differentiate between varying degrees of AI involvement, with some failing to recognize substantial refinements while over-penalizing minor edits. We also identified biases where older or smaller language models are more likely to be flagged, raising concerns about fairness in AI-assisted writing detection.  These findings emphasize the need for more nuanced detection methodologies that can account for varying degrees of AI involvement in writing. As AI tools become increasingly integrated into everyday writing practices, detection systems must evolve to ensure fairness and accuracy. To foster further research, we publicly release our AI-Polished-Text Evaluation (APT-Eval) dataset and encourage the development of more robust and context-aware detection models.

Our study exposes key flaws in AI-text detectors when handling AI-polished text, showing high false positive rates and difficulty distinguishing minor from major AI refinements. Detectors also exhibit biases against older or smaller models, raising fairness concerns. We highlight the need for more nuanced detection methods and release our APT-Eval dataset to support further research.