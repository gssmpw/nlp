While our study provides valuable insights into the challenges of AI-text detection for AI-polished texts, several limitations should be acknowledged. First, our dataset, APT-Eval, is built using a specific set of LLMs (GPT-4o, Llama3-70B, etc.), which may not fully represent the diversity of AI models available. Future research should explore a broader range of models to assess generalizability. Additionally, while our dataset spans six distinct domains of human-written text (HWT), incorporating more domains could provide a more comprehensive evaluation of AI-text detection across different writing contexts.

Second, our findings highlight biases in detection models, particularly against smaller or older LLMs, but further investigation is needed to understand the root causes of these biases. Moreover, while this study focuses on identifying limitations in current AI-text detection systems, the development of more nuanced, fine-grained detection frameworks remains an open challenge. Future work should explore adaptive AI-text detectors capable of distinguishing varying levels of AI involvement, ensuring both accuracy and fairness in AI-assisted writing evaluation.