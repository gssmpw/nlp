\section{Introduction}
\label{sec:intro}   

In recent years, the need to align and transfer information across modalities has grown substantially, especially for tasks involving complex 3D environments. Such a capability enables knowledge and experience transfer across modalities. For example, knowing the layout of kitchens in computer-aided design (CAD) format will provide guidance on how to build a new kitchen, such that it follows the layout of the most similar CAD floorplan.


\begin{figure}
    \includegraphics[width=\linewidth]{fig/teaser.pdf}%}
    \caption{\textbf{\textit{\project{}} is a cross-modal alignment method for 3D scenes that learns a unified, modality-agnostic embedding space, enabling a range of tasks.} For example, given the 3D CAD model of a query scene and a database of reconstructed point clouds, CrossOver can retrieve the closest matching point cloud and, if object instances are known, it can identify the individual locations of furniture CAD models with matched instances in the retrieved point cloud, using brute-force alignment. This capability has direct applications in virtual and augmented reality.}
    \label{fig:teaser}
\end{figure} 

Current multi-modal approaches tackle 3D data alignment of individual objects across modalities \cite{zhang2021pointclip,xue2022ulip,xue2023ulip2,pointbind}, \textit{without including and considering scene context}, making them challenging to extend effectively for scene-level understanding. These methods typically assume fully aligned, consistent datasets, where each modality is perfectly corresponding to all others for each object. However, real-world scenarios rarely provide such complete modality pairings. For example, a video of a room and its CAD model might share some spatial alignment but differ in data characteristics and object instances (hereby referred to as \textit{instances}) represented in the data (e.g., some instances could be missing in one modality, which is common between real-world scenes and their CAD models). Also, achieving consistent instance segmentation across modalities is nearly impossible in practice. Thus, these approaches struggle when certain modalities are missing or incomplete, limiting their flexibility in practical applications \cite{baltruvsaitis2018multimodal}.


We address the inherent limitations of strict object-level modality alignment by introducing a \textit{flexible scene-level} modality alignment approach that operates without prior information during inference (\eg, semantic instance segmentation), unlike the current methods \cite{sarkar2023sgaligner,xie2024sg}. Our method, namely \textbf{\textit{\project{}}} (Fig. ~\ref{fig:teaser}), enables the learning of cross-modal behaviors and relationships, such as identifying similar objects or scenes across different modalities, like the virtual CAD scene based on a video of a real room. This capability extends beyond instance-level matching towards a \textit{unified, modality-agnostic understanding} that supports seamless cross-modal interactions at the scene level.

\textit{\project{}} focuses on aligning five key scene modalities---RGB images, real-world point clouds, CAD models, floorplan images, and text descriptions, in the \textit{feature} space---going beyond the RGB-PC-Text triplets of prior work. Importantly, it is designed with the assumption that not all modalities are available for every data point. By employing a flexible training strategy, we allow \project{} to leverage any available modality during training, without requiring fully aligned data across all modalities. This approach enables our encoders to learn emergent modality alignments, supporting cross-modal traversals even in cases with missing data. Our work is grounded in three key contributions:
\begin{itemize}
\item \textbf{Dimensionality-Specific Encoders:} We introduce 1D, 2D, and 3D encoders tailored to each modality's dimensionality, removing the need for explicit 3D scene graphs or semantic labels during inference. This optimizes feature extraction for each modality and avoids reliance on consistent semantics, which is often hard to obtain.
\item \textbf{Three-Stage Training Pipeline:} Our pipeline progressively builds a modality-agnostic embedding space. First, object-level embeddings capture fine-grained modality relationships. Next, scene-level training develops unified scene representations without requiring all object pairs to align. Finally, dimensionality-specific encoders create semantic-free cross-modal embeddings.
\item \textbf{Emergent Cross-Modal Behavior:} \project{} learns emergent modality behavior, despite not being explicitly trained on all pairwise modalities. It recognizes, \eg, that \textit{Scene$_i$} in the image modality corresponds to \textit{Scene$_i$} in the floorplan modality or its point cloud to the text one, without these modality pairs being present in training.
\end{itemize}

This unified, modality-agnostic embedding space enables diverse tasks such as object localization and cross-modal scene retrieval, offering a flexible, scalable solution for real-world data that may lack complete pairings.