\clearpage
\setcounter{section}{0}
\counterwithin{table}{section}
\renewcommand{\thesection}{\Alph{section}}

\maketitlesupplementary

\begin{abstract}
In the supplementary material, we provide:
\begin{enumerate}[leftmargin=12pt,itemsep=0em]
    \item Impact of scaling up data (Sec.~\ref{sec:data_scaleup})
    \item Results on training with all pairwise modalities (Sec.~\ref{sec:all_pairwise_modality})
    \item Results on same modality scene retrieval (Sec.~\ref{sec:same_modal_scene_retrieval})
    \item Results on scene retrieval with one modality input to the scene-level encoder (Sec.~\ref{sec:scene_encoder_avg})
    \item Results on cross-modal coarse visual localization (Sec.~\ref{sec:sgloc_compare})
    \item Additional qualitative results on scene retrieval (Sec.~\ref{sec:qualitative_res})
    \item Details on the camera view sampling algorithm (Sec.~\ref{sec:cam_view_sampling})
    \item Analysis of inference runtime (Sec.~\ref{sec:runtime_analysis})
    \item Further details on the experimental setup (Sec.~\ref{sec:exp_details})

\end{enumerate}
\end{abstract}

\section{Data Scale-up Improvements}
\label{sec:data_scaleup}
We investigate the impact of scaling up training data by merging different datasets and its effect on \project{}'s performance, particularly for instance- and scene-level matching recall. Figure~\ref{fig:instance_matching_scaleup} demonstrates the advantages of joint training on the ScanNet and 3RScan datasets compared to training on each dataset individually. Please note that 3RScan includes only the $\mathcal{I}$, $\mathcal{P}$, and $\mathcal{R}$ modalities. Joint training significantly enhances scene-level recall performance and also improves instance-level recall. These results highlight CrossOver's ability to effectively leverage diverse data sources, enabling better generalization across varying scenes and object arrangements, ultimately boosting overall performance.
\input{tab/scannet_instance_matching_scaleup}

\section{All Pairwise Modality Training}
\label{sec:all_pairwise_modality}
As mentioned in Sec. \ref{sec:object_encoder_training} of the main paper, training with all pairwise modality combinations, as in prior work~\cite{xue2023ulip2,pointbind}, directly aligns all modality pairs in a shared embedding space. However, this approach underperforms compared to alignment with a single reference modality, as evidenced by the results in Tabs.~\ref{tab:allpairloss_comparison_scannet} and ~\ref{tab:allpairloss_comparison_scan3r}. Note that \textit{`Ours'} results are copied from Fig. \ref{fig:instance_matching} of the main paper. The key limitation of aligning all modality pairs lies in its added complexity, which dilutes focus and leads to lower scene-level recall metrics. In contrast, intra-modal alignment enhances robustness, particularly in cases of missing modality inputs, by concentrating learning on specific modality relationships. This focused alignment not only improves performance but also facilitates \textit{emergent modality} behavior. Similar insight is also noticed when training the unified encoders with the raw scene data using all pairwise modalities, namely $\mathbf{F}_{1D}$, $\mathbf{F}_{2D}$, $\mathbf{F}_{3D}$ and $\mathbf{F}_{\mathcal{S}}$. This is shown as `All Pairs' in Tabs. \ref{tab:cross_modal_scene_retrieval_scannet_scenenc} and \ref{tab:cross_modal_scene_retrieval_scan3r_scenenc}.


\input{tab/allpair_loss_scannet}
\input{tab/allpair_loss_scan3r}

\section{Same-Modal Scene Retrieval}
\label{sec:same_modal_scene_retrieval}
We present results for \textit{same-modality scene retrieval} in Tabs.~\ref{tab:same_modal_scene_retrieval_scannet} and \ref{tab:same_modal_scene_retrieval_scan3r}, evaluated on the ScanNet and 3RScan datasets. Metrics include scene category recall, temporal recall, and intra-category recall. Our method is compared to ULIP-2~\cite{xue2023ulip2}, PointBind~\cite{pointbind}, and our instance baseline. The instance baseline is not evaluated on the floorplan modality $\mathcal{F}$ due to the lack of floorplan representation at the instance level. Additionally, the scene-level encoder combines \textit{all} instance modalities to generate the $\mathcal{F_S}$ encoding, utilizing ground truth instance segmentation that is consistent across all modalities. This can serve as an upper bound of performance for our method. 
Results indicate that individual modalities in our method are closely aligned within the embedding space, despite the cross-modal training objective. Consistent with cross-modal results, our method performs better than the instance baseline in most cases, highlighting the importance of scene-level understanding. Moreover, it achieves significantly better or comparable performance to ULIP-2 and PointBind. Notably, our method achieves 100\% accuracy on the intra-category recall metric in all modalities, consistently distinguishing the same, \eg, \textit{kitchen} among a database of \textit{kitchens}, with ULIP-2 following closely. ULIP-2 and PointBind show decreased performance on the text referral $\mathcal{R}$ modality, likely due to training on simple object descriptions (e.g., “a point cloud of a chair”) without scene context. Finally, while our scene-level encoder excels when all modalities are available, challenges arise with missing modalities, as discussed in Sec.~\ref{sec:scene_encoder_avg}.
\input{tab/scannet_scene_retrieval_same_modal}
\input{tab/scan3r_scene_retrieval_same_modal}

\begin{figure*}
    \centering
    \includegraphics[width=\columnwidth]{fig/camera_view_abl_scannet.pdf}
    \vspace{-20pt}
    \caption{\textbf{Cross-Modal $\mathcal{I} \rightarrow \mathcal{P}$ Scene Retrieval on \textit{ScanNet}.} Plots showcase scene matching recall (Recall), category recall, temporal recall, and intra-category recall for different number of camera views evaluated on several Top-$k$ matches. Note that maximum $k$ differs per recall since the amount of eligible matches depends on the criteria for each recall type: scene similarity, category, temporal changes.}
    \label{fig:camera_view_abl_scannet}
\end{figure*}

\section{Uni-modal Scene-Level Encoder Inference}
\label{sec:scene_encoder_avg}
\input{tab/scannet_scene_retrieval_cross_modal_scenenc_avg}
In Sec.~\ref{sec:unified_encoder_training} of the main paper, we highlighted two key advantages of unified dimensionality encoders over the scene-level encoder: (i) they eliminate the need for instance-level modalities or instance information, and (ii) the scene-level encoder struggles when provided with only a single modality (uni-modal) instead of all. To validate the latter, cross-modal scene retrieval results are presented in Tabs. \ref{tab:cross_modal_scene_retrieval_scannet_scenenc} and \ref{tab:cross_modal_scene_retrieval_scan3r_scenenc}. Our method significantly outperforms the uni-modal scene-level encoder in most cases, underscoring the effectiveness and value of the unified modality encoders.
\input{tab/scan3r_scene_retrieval_cross_modal_scenenc_avg}

\section{Cross-Modal Coarse Visual Localization}
\label{sec:sgloc_compare}
\input{tab/sgloc_comparison}

We evaluate our method on the task of cross-modal coarse visual localization of a single image against a database of multi-modal reference maps, comparing it to SceneGraphLoc~\cite{miao2024scenegraphloc} and its baselines LipLoc~\cite{shubodh2024lip} and LidarCLIP~\cite{lidarclip2024} on the 3RScan dataset. SceneGraphLoc uses 3D scene graphs during inference as the multi-modal reference maps, incorporating object instance point clouds, their attributes and relationships, and the scene's structure (for a formal definition of these modalities we point the reader to~\cite{sarkar2023sgaligner,miao2024scenegraphloc}). For a fair comparison, we use the 2D unified dimensionality encoder to process the input image into an $\mathcal{F}_{2D}$ feature vector, which is then compared to the $\mathcal{F}_{S}$ feature vectors of all scenes in the database, extracted by our scene-level encoder. As shown in Tab. \ref{tab:sgloc_comparison}, despite encoding less information in our multi-modal maps, our method performs competitively with SceneGraphLoc.

\section{Qualitative Results}
\label{sec:qualitative_res}

We present additional qualitative results in Figs. \ref{fig:visual_comparison_add_success} and \ref{fig:visual_comparison_add_failure} for cross-modal scene retrieval of the pairwise modalities $\mathcal{F} \rightarrow \mathcal{P}$. Fig. \ref{fig:visual_comparison_add_success} illustrates a success case for our method, where the correct scene is retrieved in the first match. In contrast, PointBind~\cite{pointbind} and our instance baseline fail to retrieve the correct scene within the first four matches. Notably, our instance baseline does not retrieve any bedrooms. Fig. \ref{fig:visual_comparison_add_failure} illustrates a failure case of our method. Despite this, it successfully retrieves all office scenes with a layout similar to the query one. In comparison, the baselines also fail to retrieve the correct scene but instead find matches in bedrooms and meeting rooms. Fig. \ref{fig:visual_comparison_scan3r} shows success and failure cases on 3RScan dataset for cross-modal scene retrieval of the pairwise modalities $\mathcal{R} \rightarrow \mathcal{P}$.

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{fig/camera_view_abl_scan3r.pdf}
    \vspace{-20pt}
    \caption{\textbf{Cross-Modal $\mathcal{I} \rightarrow \mathcal{P}$ Scene Retrieval on \textit{3RScan}.} Plots showcase scene matching recall (Recall) and temporal recall for different number of camera views.}
    \label{fig:camera_view_abl_scan3r}
\end{figure}
\vspace{-2pt}

\section{Camera View Sampling}
\label{sec:cam_view_sampling}

To sample camera views for the unified 2D encoder (Sec. \ref{sec:unified_encoder_training} of the main paper), we represent each camera pose as a $7D$ grid, combining its $3D$ translation and quaternion-based rotation ($4$ quaternion $+$ $3$ translation components). Our method selects $N$ camera poses to maximize $3D$ spatial separation in rotation and translation. Starting with a random pose, we iteratively select the pose farthest from \textit{all} previously chosen ones. This method ensures an even and diverse sampling of camera viewpoints across the scene. We analyze the impact of the number of selected cameras and present results for $N$ values of $1, 5, 10$, and $20$) in Figs. \ref{fig:camera_view_abl_scannet} and \ref{fig:camera_view_abl_scan3r}. The results show that performance stabilizes after $N=10$, with additional frames providing only slight improvements, indicating full scene coverage is not necessary for training \project{}. Consequently, we use $N=10$ for all reported results in our method.

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{fig/camera_view_sampling_visualisation.pdf}
    \caption{\textbf{Camera View Sampling Visualisation on ScaNnet dataset.} Here, we visualise the $N=20$ selected views (in \textcolor{purple}{purple} projected onto the ground truth scene mesh) using  our camera sampling method. Although, the selected cameras may not cover the entire scene, they are spatially well-separated.}
    \label{fig:camera_view_sampling}
\end{figure*}  

\section{Runtime Analysis}
\label{sec:runtime_analysis}
Our scene retrieval model consists of $1.5$B-parameter. On an NVIDIA 4090 GPU, our model takes $1.01$s $\pm 0.26$s for a single modality and $1.98$s for all modalities in $1D$, $2D$ and $3D$. It can be adapted to lightweight encoders for faster inference in compute-limited scenarios, with potential performance trade-off.

\section{Experimental Setup Details}
\label{sec:exp_details}
\noindent \textbf{Location Encoding \& Instance Spatial Relationships.} Given $\mathcal{P}_i$, we compose features $f_i^\mathcal{P}$ and the location $l_i$ (ie, $3$D position, length, width and height) to form instance tokens $\hat{f_i^\mathcal{P}}$~\cite{3dvista}. A similar process is followed for $\mathcal{M}_i$. Since we do not use scene graph representations, for instance modality $\mathcal{P}$, we embed the pairwise spatial relationships between objects in a spatial transformer~\cite{jia2024sceneverse,3dvista} to encode the scene layout and context. For any two objects $\mathcal{O}_i$ and $\mathcal{O}_j$ present in a scene, we define relationship $s_{ij} = [d_{ij}, sin(\theta_h), cos(\theta_h), sin(\theta_v), cos(\theta_v)]$, where $d_{ij}$ is the Euclidean distance between the centroids of objects $i$ and $j$, and $\theta_h$ and $\theta_v$ are the horizontal and vertical angles of the line connecting the centers of objects $i$ and $j$. The pairwise spatial feature matrix $S = \{s_{ij}\}$ is used to update the attention weights in the self-attention layers of the transformer.

\noindent \textbf{Evaluation Setup.} Our results are reported on the \textit{validation} sets of ScanNet \cite{dai2017scannet} and 3RScan \cite{wald2019rio}, as their corresponding \textit{test} sets lack public annotations or is unavailable. For the experiments in Sec.~\ref{sec:sgloc_compare}, we follow the dataset split provided by SceneGraphLoc~\cite{miao2024scenegraphloc} to ensure fairness.

\noindent \textbf{Implementation.} Inspired by CLIP~\cite{Radford2021LearningTV}, we adopt an embedding space of size $768$, consistent across instance-level, scene-level, and unified training stages. Each model is trained for 300 epochs on an NVIDIA GeForce RTX 4090 Ti GPU using the AdamW optimizer~\cite{Loshchilov2017DecoupledWD} with a learning rate of $1e-3$, and a cosine annealing scheduler with warm restarts. To fine-tune the pre-trained encoders (BLIP~\cite{Li2022BLIPBL}, DinoV2~\cite{oquab2023dinov2, darcet2023vitneedreg}, and I2PMAE \cite{Zhang2022Learning3R}), we employ a 2-layer MLP projection head with dropout and Layer Normalization~\cite{miao2024scenegraphloc, girdhar2023imagebind}. The $\tau$ parameter in the contrastive loss formulation is treated as a learnable parameter. Consistent with practices in \cite{jia2024sceneverse}, we pre-train object-level and scene-level encoders and freeze them during unified dimensionality encoder training. 

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{fig/scene_retrieval_vis_add_success.pdf}
    \caption{\textbf{Cross-Modal Scene Retrieval \textcolor{spidergreen}{Success} Qualitative Results on ScanNet.} Given a scene in query modality $\mathcal{F}$, we aim to retrieve the same scene in target modality $\mathcal{P}$. While PointBind and the Instance Baseline do not retrieve the correct scene within the top-4 matches, \project{} identifies it as the top-1 match.}
    \label{fig:visual_comparison_add_success}
    \vspace{-5pt}
\end{figure*}  

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{fig/scene_retrieval_vis_add_failure.pdf}
    \caption{\textbf{Cross-Modal Scene Retrieval \textcolor{red}{Failure} Qualitative Results on ScanNet.} Given a scene in query modality $\mathcal{F}$, we aim to retrieve the same scene in target modality $\mathcal{P}$. While the baselines also fail to retrieve the same scene, CrossOver ($k=2$) and PointBind ($k=3$) retrieve a temporal scan as match.}
    \label{fig:visual_comparison_add_failure}
    \vspace{-5pt}
\end{figure*} 

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{fig/scene_retrieval_scan3r.pdf}
    \caption{\textbf{Cross-Modal Scene Retrieval Qualitative Results on 3RScan. Top row - \textcolor{spidergreen}{Success}, Bottom row - \textcolor{red}{Failure}.} Given a scene in query modality $\mathcal{R}$, we aim to retrieve the same scene in target modality $\mathcal{P}$. Temporal scenes cluster naturally in the embedding space. However, query referrals may retrieve scans with similar objects across different scenes, especially when not discriminative enough (bottom).}
    \label{fig:visual_comparison_scan3r}
    \vspace{-5pt}
\end{figure*}