\vspace{-10pt}
\begin{abstract}
Multi-modal 3D object understanding has gained significant attention, yet current approaches often assume complete data availability and rigid alignment across all modalities. We present \project{}, a novel framework for cross-modal 3D \textit{scene} understanding via flexible, scene-level modality alignment. Unlike traditional methods that require aligned modality data for every object instance, \project{} learns a unified, modality-agnostic embedding space for scenes by aligning modalities -- RGB images, point clouds, CAD models, floorplans, and text descriptions -- with relaxed constraints and without explicit object semantics. Leveraging dimensionality-specific encoders, a multi-stage training pipeline, and emergent cross-modal behaviors, \project{} supports robust scene retrieval and object localization, even with missing modalities. Evaluations on ScanNet and 3RScan datasets show its superior performance across diverse metrics, highlighting \project{}â€™s adaptability for real-world applications in 3D scene understanding. 

\end{abstract}
\vspace{-1.0em}