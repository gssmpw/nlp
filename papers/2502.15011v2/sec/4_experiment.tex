\section{Experiments}
\label{sec:experiment}

\textbf{Datasets.}
We train and evaluate \project{} on ScanNet \cite{dai2017scannet} and 3RScan~\cite{wald2019rio}. We choose ScanNet for providing comprehensive coverage of all modalities, and 3RScan for including more data on temporal scenes. For both, we use the \textit{object referrals} from SceneVerse~\cite{jia2024sceneverse}, which is a million-scale 3D vision-language dataset with $68$K 3D indoor scenes comprising indoor scene understanding datasets and $2.5M$ vision-language pairs. In all evaluations, we use a model trained across all datasets (details in Supp.).

\textbf{ScanNet \cite{dai2017scannet}} is an RGB-D video dataset containing 2.5 million views in more than $1500$ scenes, annotated with 3D camera poses, surface reconstructions, and instance-level semantic segmentation; we obtain images and 3D point clouds. For mesh $\mathcal{M}$ and floorplan $\mathcal{F}$, we use the Scan2CAD~\cite{Avetisyan_2019_CVPR} dataset, which provides annotated keypoint pairs between CAD models from ShapeNet~\cite{Chang2015ShapeNetAI} and their counterpart objects in the scans. \textbf{3RScan \cite{wald2019rio}} benchmarks instance relocalization, featuring $1428$ RGB-D sequences across $478$ indoor scenes, including rescans of the latter after object relocation. It provides annotated 2D and 3D instance segmentation, camera trajectories, and reconstructed scan meshes. We obtain images and point clouds.

\begin{figure*}
    \centering
    \includegraphics[width=0.8\linewidth]{fig/visual_scene_retrieval.pdf}
    \caption{\textbf{Cross-Modal Scene Retrieval Qualitative Results on ScanNet.} Given a scene in query modality $\mathcal{F}$, we aim to retrieve the same scene in target modality $\mathcal{P}$. While PointBind and the Instance Baseline do not retrieve the correct scene within the top-4 matches, \project{} identifies it as the top-1 match. Notably, temporal scenes appear close together in \project{}â€™s embedding space (\eg, $k=2$, $k=3$), with retrieved scenes featuring similar object layouts to the query scene, such as the red couch in $k=4$.}
    \label{fig:visual_comparison}
    \vspace{-10pt}
\end{figure*} 

\noindent
\textbf{Evaluation Metrics.}
%\label{sec:evaluation_metrics}
We assess the quality of our representation by quantifying its ability to identify the same instance $\mathcal{O}_i$ or scene $\mathcal{S}_i$ across modalities, $\mathcal{Q}_j$ and $\mathcal{Q}_k$. Extending image feature matching evaluation~\cite{lowe2004, sarlin20superglue}, we compute the \textit{instance matching recall} as the ratio of correctly identified $\mathcal{O}_i$ matches, given a database of instances. Additionally, we evaluate \textit{scene-level (instance) matching recall} at thresholds of $25\%$, $50\%$, and $75\%$, indicating how many objects from a scene in modality $\mathcal{Q}_j$ out of the total objects in the same scene we can match in modality $\mathcal{Q}_k$. This combined measure shows instance matching failure within a scene.

We further evaluate the challenging task of cross-modal scene retrieval within a database. For example, given a query point cloud of a scene, we aim to retrieve its corresponding 2D floorplan. This analysis includes multiple levels: (i) \textit{scene matching recall}, or the model's ability to retrieve the exact scene $\mathcal{S}_i$; (ii) \textit{scene category recall} to test retrieval of a scene from the same category (\eg, retrieving \textit{any} kitchen when given a kitchen query in a multi-category database); (iii) \textit{temporal recall} to evaluate whether the model can recover the same scene captured at a different time, accounting for potential object movement or removal; and (iv) \textit{intra-category recall}, which assesses retrieval of a specific scene within a single-category database (\eg, retrieving a particular kitchen from only kitchen scenes). This last metric uniquely requires a different database.

\subsection{Instance Retrieval}
\noindent \textbf{Cross-Modal Instance Matching.}  Our goal is instance matching within the same scene where multiple instances of the same furniture (\eg, \textit{two identical chairs}) are commonly present. We showcase our results on \textit{ScanNet} and \textit{3RScan} datasets in Fig.~\ref{fig:instance_matching}. We compare \project{} with pretrained multi-modal methods ULIP-2~\cite{xue2023ulip2} and PointBind~\cite{pointbind} and our instance-level multi-modal encoder to highlight the importance of scene-level understanding in a cross-modal embedding space. As shown in Fig.~\ref{fig:spider_instancematching}, our performance on ScanNet is robust across modalities, while baselines exhibit varying results. Current multi-modal methods are large pretrained models with strong text encoders that boost performance for \textit{referral}-based retrieval. While prior work trains on all pairwise modalities, we selectively train only in reference to the image modality ($\mathcal{I}$). Yet, we still achieve robust performance across all modalities, even without direct interactions during training. Emergent interactions are in \textcolor{spidergreen}{green}. Similar trends appear in Fig. \ref{fig:table_scene_level_instance_matching} for scene-level matching.

\input{tab/scan3r_matching}

\noindent \textbf{Temporal Instance Matching.} Although not part of the learning objective, we evaluate \project{}'s effectiveness on temporal point cloud-based instance retrieval (same-modal) using scans acquired at different time intervals, with scene changes like object displacement and rearrangement. Tab.~\ref{tab:temporal_instance_matching_3rscan} shows a comparison on the \textit{3RScan} dataset, highlighting our method's superior performance. This is a large gain, lying in the strong representational power of our multi-modal embedding space, which allows the encoder to efficiently extract each instance's spatial and geometric features in dynamic scenes. Moreover, our method, while primarily evaluated in the same-modal setting, also demonstrates superior performance in the cross-modal scenario, shown in the second half of Tab.~\ref{tab:temporal_instance_matching_3rscan}, further underlining the importance of scene-level multi-modal alignment to handle temporal variations in indoor scene understanding.

\input{tab/scannet_scene_retrieval_recall}
\input{tab/scannet_scene_retrieval_cross_modal}
\input{tab/scan3r_scene_retrieval_cross_modal}


\subsection{Cross-Modal Scene Retrieval}
We compare our cross-modal scene retrieval results with \cite{xue2023ulip2,pointbind} and our instance-level baseline. Since prior work does not address this task, we adapt their methods by averaging object embeddings per modality to create scene representations, treating our baseline similarly. Unlike \project{}, these methods rely on semantic instance segmentation. \textit{Scene matching recall} results on \textit{ScanNet} (Fig.~\ref{fig:cross_modal_scene_retrieval_scannet_same_recall_graph}) show that our unified encoders, not relying on semantics, consistently outperform prior methods in all pairwise modalities and surpass our baseline. Detailed results on \textit{ScanNet} and \textit{3RScan} are in Tabs.~\ref{tab:cross_modal_scene_retrieval_scannet} and~\ref{tab:cross_modal_scene_retrieval_scan3r}. Our method achieves overall scene understanding, even with small-scale object reconfigurations, as shown by its high temporal recall. The lower performance of pretrained methods may stem from training biases that limit their robustness with real-world data, such as incomplete point clouds and blurry images. Qualitative results are in Fig. \ref{fig:visual_comparison}.


\subsection{Missing Modalities}
\label{sec:missing_modalities}
To demonstrate \project{}'s ability to capture emergent modality behavior with non-overlapping training data points, we train \project{} using different data repositories for each modality pair. Specifically, we use the \textit{ScanNet} dataset and split the image repository into two chunks of varying sizes. Training on image-point cloud ($\mathcal{I} \rightarrow \mathcal{P}$) and image-mesh ($\mathcal{I} \rightarrow \mathcal{M}$) using each chunk respectively, we expect to see an emergent behavior between point cloud and mesh ($\mathcal{P} \rightarrow \mathcal{M}$). The results (Tab.~\ref{tab:ablation_miss_mod_scannet}) include top-$1$ and top-$3$ instance matching recall, as well as \textit{same} and \textit{diff} recall for evaluating intra- (e.g., identical chairs) and inter- (e.g., a chair and a table) object category performance within a scene. Although partial data availability decreases recall, our $\mathcal{P} \rightarrow \mathcal{M}$ matching only decreases by $3\%$ even when using $25\%$~$\mathcal{I} \rightarrow \mathcal{P}$. This scenario is common in real-world applications, where certain modalities might be scarce.

\input{tab/ablation_missing_data}