\section{Method}
\label{sec:method}
Given a 3D scene $\mathcal{S}$ represented by various modalities, denoted as $\mathcal{Q} = \{ \mathcal{I}, \mathcal{P}, \mathcal{M}, \mathcal{R}, \mathcal{F} \}$, our objective is to develop a unified, modality-agnostic representation that maps independent modalities capturing the same 3D scene to a common point in the embedding space. Here, $\mathcal{I}$ is a set of RGB images, $\mathcal{P}$ is a real-world reconstruction as a point cloud, $\mathcal{M}$ is a digital mesh representation from computer aided design (CAD), $\mathcal{R}$ is textual data describing $\mathcal{S}$ within its surroundings, and $\mathcal{F}$ is a rasterized floorplan.

Our proposed framework facilitates robust interactions across different modalities at both the comprising instances and scene levels, enhancing the multi-modal (\eg, pointcloud $\mathcal{P}$ and floorplan $\mathcal{F}$) and same modal (e.g., textual data $\mathcal{R}$) understanding of 3D environments. We structure the development of the embedding space progressively, beginning with instance-level multi-modal interactions and culminating in scene-level multi-modal interactions without requiring prior knowledge, such as semantic information about constituent instances. An overview of \project{} is shown in Fig. \ref{fig:architecture}. To demonstrate the capabilities of this unified, modality-agnostic embedding space, we evaluate:
\begin{enumerate}
    \item{\textbf{Cross-modal instance retrieval}}: Given an observed modality $\mathcal{Q}_{j}$ of a query instance $\mathcal{O}_i$ in a scene $\mathcal{S}$ (\eg, mesh $\mathcal{M}$ or pointcloud $\mathcal{P}$), we aim to retrieve any other modality $\mathcal{Q}_{k}$ representing $\mathcal{O}_i$ within $\mathcal{S}$.
    \item{\textbf{Cross-modal scene retrieval}}: Given a scene $\mathcal{S}_i$ represented by modality $\mathcal{Q}_j$ (\eg, image $\mathcal{I}$ or floorplan $\mathcal{F}$), we aim to retrieve another modality $\mathcal{Q}_{k}$ representing $\mathcal{S}_i$.
\end{enumerate}


\subsection{Instance-Level Multi-Modal Interactions}
\label{sec:object_encoder_training}

First, we describe the pipeline used for learning a multi-modal embedding space for independent instances.
This will provide a basis for the scene-level embeddings. 
We process each of the 1D ($\mathcal{R}$), 2D ($\mathcal{I}$), and 3D ($\mathcal{P}$ and $\mathcal{M}$) instance modalities with corresponding encoders\footnote{The $\mathcal{F}$ modality is not used when learning an instance-level embedding since there is no notion of a floorplan in this scenario.}:

\noindent \textbf{1D Encoder.} 
An instance $\mathcal{O}_i$ can be represented by its textual context in a scene $\mathcal{S}$, using descriptions like \textit{``The chair is in front of the lamp"} and \textit{``The chair is left of the table"}. We term these descriptions as \textit{object referrals}~\cite{jia2024sceneverse} and encode each referral as $f_{ij}^\mathcal{R}$ using the pre-trained text encoder BLIP~\cite{Li2022BLIPBL}, where $i$ is the instance of interest (\eg, chair) and $j$ represents another instance in the scene (\eg, lamp, table, or another chair). Practically, we collect $k$ object referrals per instance, resulting in $F_i^\mathcal{R} = \{ f_{i1}^{\mathcal{R}}, \ldots, f_{ik}^{\mathcal{R}} \}$. To create a single feature vector $f_{i}^{\mathcal{R}}$ representing the instanceâ€™s context, we apply average pooling over $F_i^\mathcal{R}$ .

\noindent \textbf{2D Encoder.} Given a collection $I_\mathcal{S}$ of images capturing a scene $\mathcal{S}$, we integrate multi-view and per-view multi-level visual embeddings for each $\mathcal{O}_i$ to encode $f_i^\mathcal{I}$. Inspired by~\cite{miao2024scenegraphloc}, for each $\mathcal{O}_i$, we select the top $K_{view}$ defined by largest visibility of  $\mathcal{O}_i$ among $I_\mathcal{S}$ and calculate multi-level bounding boxes around $\mathcal{O}_i$ $\{b_{v,l} \; | \; l \in [0, L)\}$ within each view $v$. A pre-trained DinoV2~\cite{oquab2023dinov2, darcet2023vitneedreg} encoder processes the image crops defined by $b_{v,l}$ to give us the \texttt{[CLS]} tokens per crop~\cite{yang2024denoising}. Subsequent average pooling operations aggregate these tokens into a singular feature vector $f_i^\mathcal{I}$. In contrast to~\cite{miao2024scenegraphloc}, we do not assume available camera poses.

\noindent \textbf{3D Encoder.} Given instance $\mathcal{O}_i$ and its corresponding real-world \textit{point cloud} $\mathcal{P}_i$ and \textit{shape mesh} $\mathcal{M}_i$, we extract instance features $\Bar{f_i^\mathcal{P}}$ and $\Bar{f_i^\mathcal{M}}$ using a pretrained I2PMAE~\cite{Zhang2022Learning3R} point cloud encoder. Importantly, we do not utilize the semantic class~\cite{jia2024sceneverse, 3dvista} of $\mathcal{O}_i$ in these operations. We concatenate the 3D location of $\mathcal{P}_i$ and $\mathcal{M}_i$ to $\Bar{f_i^\mathcal{P}}$ and $\Bar{f_i^\mathcal{M}}$, respectively, to form the instance tokens $\hat{f_i^\mathcal{P}}$ and $\hat{f_i^\mathcal{M}}$. To introduce partial scene-level reasoning, we incorporate interactions between instances by integrating the instance tokens and encoding the pairwise spatial relationships of an instance with all others in $\mathcal{S}$ within a transformer network. Similar to~\cite{jia2024sceneverse}, we employ spatial-attention-based transformers, following~\cite{3dvista, chenlanguage2022}, to generate $f_i^\mathcal{P}$ and $f_i^\mathcal{M}$. Details about the 3D location and spatial relationships are in Supp. For the mesh modality $\mathcal{M}$, we sample points on the mesh surface to enable input to a point cloud encoder. We encode neither the 3D location nor the spatial pairwise relation among instances, as we do not assume that the meshes are aligned with the scene geometry.

All pre-trained encoders, which are frozen during training, are followed by trainable projection layers. During training, after encoding each modality, we apply a contrastive loss to enforce alignment of modality features within a joint embedding space. Unlike prior work that requires full data modality alignment~\cite{pointbind, xue2023ulip2} or semantic scene graph \cite{sarkar2023sgaligner,miao2024scenegraphloc}, \project{} accommodates the practical challenge that not all modalities may always be available by not requiring the presence of all modalities simultaneously. Instead, it aligns all other modality embeddings with image space $\mathcal{I}$. The loss function can be defined as:
%
\begin{equation}
\vspace{-2pt} 
    \mathcal{L}_{\mathcal{O}_i} = \mathcal{L}_{f_i^I, f_i^\mathcal{P}} + \mathcal{L}_{f_i^I, f_i^\mathcal{M}} + \mathcal{L}_{f_i^I, f_i^\mathcal{R}}.
\end{equation}

 During \textit{training}, \project{} requires a base modality for every instance, to align other modalities with its feature space. We choose images $\mathcal{I}$ as the base modality due to their availability and strong encoder priors, though any supported modality can serve this role. Crucially, \textit{no modality availability assumptions are made during inference}, allowing any query-target modality pair. Our experiments (see Supp.) show that aligning to a single reference modality, rather than using all pairwise combinations as in prior work, improves performance.


\subsection{Scene-Level Multi-Modal Interactions}
\label{sec:scene_encoder_training}
We distill knowledge from instance-level modality encoders to scene-level encoders, allowing us to leverage instance-based insights during training and enabling scene-level retrieval at inference without relying on 3D scene graphs or semantic instance information across modalities.

\noindent \textbf{Multi-modal Scene Fusion.} Given the instance features $f_i^\mathcal{R}$, $f_i^\mathcal{I}$, $f_i^\mathcal{P}$, and $f_i^\mathcal{M}$ for each instance $\mathcal{O}_i$ in scene $\mathcal{S}$, we compute each of the scene level features $f^\mathcal{R}$, $f^\mathcal{I}$, $f^\mathcal{P}$, and $f^\mathcal{M}$ by first applying average pooling per modality to the features of all instances in $\mathcal{S}$. We then perform a weighted fusion of these pooled features to learn a fixed-size multi-modal embedding $\mathbf{F}_\mathcal{S}$:
%
\begin{equation}
\mathbf{F}_{\mathcal{S}} = \sum_{q \in \mathcal{Q}} \left[ \frac{\exp(w_q)}{\sum_{j \in \mathcal{Q} \setminus q} \exp(w_j)} f^q \right],
\end{equation}
%
where  $j, q \in \mathcal{Q}$, $w_q$ and $w_j$ are modality-wise trainable attention weights. We use an MLP head to project the dimensionality to our final representation space, resulting in an embedding that serves as a unified scene representation, capturing interactions across all modalities. In practice, this representation is flexible, adapting to data availability and specifically to any missing modalities.


\subsection{Unified Dimensionality Encoders}
\label{sec:unified_encoder_training}

The above scene-level encoder provides a unified, modality-agnostic embedding space; however, it requires semantic instance information consistent across modalities during inference, which is challenging to obtain in practice. To eliminate this need, we design a single encoder per modality dimensionality (\ie 1D, 2D, and 3D) that directly processes raw data without needing additional information. Moreover,
our experiments (Supp.) show that the scene-level encoder
needs all modalities at inference to perform reasonably. 

\noindent \textbf{1D Encoder.} Similar to Sec. \ref{sec:object_encoder_training}, we use \textit{object referrals} to describe scene context~\cite{3dvista}. We randomly sample $t=10$ referrals per scene and use a text encoder to form $\mathbf{F}_{1D}$.

\noindent \textbf{2D Encoder.} Here, we consider both RGB and floorplan images. The floorplan $\mathcal{F}$ is represented as a top-view orthographic projection image of the $3D$ layout with geometrically aligned shape meshes for furniture instances. Since a scene can be captured with multiple RGB images $\mathcal{I}_\mathcal{S}$, we employ a naive key-frame selection strategy to sample $N=10$ multi-view images (see Supp.). We process the images using a DinoV2~\cite{oquab2023dinov2} encoder and concatenate the output \texttt{[CLS]} token and aggregated patch embeddings to form $\mathbf{F}_{2D}^i, i \in N$. We pass each $\mathbf{F}_{2D}^i$ via an MLP projection head and apply average pooling to generate $F_{2D}$. In practice, we use the same encoder with shared weights for both RGB images $\mathcal{I}_\mathcal{S}$ and floorplan $\mathcal{F}$; \ie, inputs are not distinguished between RGB and floorplan during training. This is the first use of the floorplan modality in \project{} and there is \textit{no pairwise modality interaction} during training between it and the image modality, unlike other modalities.

\noindent \textbf{3D Encoder.} We utilize a sparse convolutional architecture with a residual network as the encoder, built with the Minkowski Engine~\cite{choy20194d}. Given an input point cloud $P \in \mathbb{R}^{N \times 3}$ containing $N$ points, it is first quantized into $M_0$ voxels represented as $V \in \mathbb{R}^{M_0 \times 3}$. The model then produces a full-resolution output feature map $\mathbf{F}_{3D} \in \mathbb{R}^{M_0 \times D}$.

\par The goal is to align each of the unified dimensionality encoders with the scene-level multi-modal encoder. The loss function for unified training becomes:
\begin{equation}
    \mathcal{L}_s = \alpha \mathcal{L}_{\mathbf{F}_{\mathcal{S}}, \mathbf{F}_{1D}} + \beta \mathcal{L}_{\mathbf{F}_{\mathcal{S}}, \mathbf{F}_{2D}} + \gamma \mathcal{L}_{\mathbf{F}_{\mathcal{S}}, \mathbf{F}_{3D}},
\end{equation}
where, $\alpha$, $\beta$, and $\gamma$ are learnable hyper-parameters.

Thus, our combined loss is as follows:
\begin{equation}
    \mathcal{L} = \mathcal{L}_s + \sum_{\mathcal{O}_i \in \mathcal{S}} \mathcal{L}_{\mathcal{O}_i}
    \label{eq:loss_objective}
\end{equation}

\subsection{Loss Definition} Given $q = G({\mathcal{Q}^m}_{i})$ and $k = H({\mathcal{Q}^n}_i)$, $i \in \mathcal{B}$, two different encoder outputs for modalities $\mathcal{Q}^m$ and $\mathcal{Q}^n$ in minibatch $\mathcal{B}$, we use a contrastive loss similar to \cite{girdhar2023imagebind}:
\begin{equation}
    \vspace{-4pt}
    \mathcal{L}_{q, k} = - \log \frac{exp(q_i^Tk_i / \tau)}{exp(q_i^Tk_i / \tau) +  \sum_{j \neq i}{exp(q_i^Tk_j / \tau)}}.
\end{equation}
Here, $\tau$ is a learnable temperature parameter, to modulate similarity between positive pairs. We consider every example $j \neq i$ in a minibatch $\mathcal{B}$ as a negative example. In practice, we use a symmetric loss for better convergence: $\mathcal{L}_{q, k} + \mathcal{L}_{k, q}$. Although we pair each modality with the most prevalent one (\ie, $\mathcal{I}$) to avoid the need for fully aligned modalities per data point during training, there are cases where not all modality pairs are available for a given data point. To enhance \project{}'s flexibility, we account for these scenarios by masking the corresponding loss term for any unavailable modality pairs. 

\subsection{Inference}
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{fig/cross_modal_scene_ret.pdf}
    \caption{\textbf{Cross-modal Scene Retrieval Inference Pipeline.} Given a query modality ($\mathcal{P}$) that represents a scene, we obtain with the corresponding dimensionality encoder its feature vector ($\mathcal{F}_{3D}$) in the shared cross-modal embedding space. We identify the closest feature vector ($\mathcal{F}_{2D}$) in the target modality ($\mathcal{F}$) and retrieve the corresponding scene from a database of scenes in $\mathcal{F}$.}  
    \label{fig:cross_modal_retrieval_schema}
\end{figure} 
\vspace{-1mm}

After training \project{} with the loss objective defined in Eq. \ref{eq:loss_objective}, we use the embedding feature vectors for retrieval tasks. Given a scene $S$ containing $\mathcal{O} = \{ \mathcal{O}_i \}$ instances each represented by one or more modalities from $\mathcal{Q}$, we use our instance-level multi-modal encoders to perform cross-modal retrieval. Given $\mathcal{O}_i$ in query modality $\mathcal{Q}_j$ and all other instances in target modality $\mathcal{Q}_k$, the goal is to retrieve the $\mathcal{O}_i$ in $\mathcal{Q}_k$. For scene retrieval, we apply a similar approach using our unified dimensionality encoders, except that instead of instances, we retrieve entire scenes. A schematic diagram for one modality pair is shown in Fig.~\ref{fig:cross_modal_retrieval_schema}.

\input{tab/Scannet_instance_matching}