\section{Conclusion}
\label{sec:conclusion}
In summary, this work introduces \project{}, a framework for flexible, scene-level cross-modal alignment without the need for semantic annotations or perfectly aligned data. \project{} leverages a unified embedding space centered on image features, allowing it to generalize across unpaired modalities and outperform existing methods in cross-modal scene retrieval and instance matching on real-world datasets. This approach addresses the limitations of traditional multi-modal models and holds promise for practical applications in areas like robotics, AR/VR, and construction monitoring.  Although \project{} excels in cross-modal instance matching, its scene retrieval generalizability could benefit from training on diverse indoor and outdoor datasets. \project{} assumes a base modality per dataset, advancing prior work requiring perfect modality alignment. Further relaxation is a promising direction. Finally, exploring its embedding space for downstream scene understanding remains a key area. Future research can explore how our approach can be applied to dynamic scene reconstruction and real-time navigation, thus leading to interactive and immersive mixed-reality experiences. 