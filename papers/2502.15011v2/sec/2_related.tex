\section{Related Work}
\label{sec:related_work}

\textbf{Multi-modal Representation Learning} aims to bridge data modalities by learning shared embeddings for cross-modal understanding and retrieval.
A seminal work in this area is CLIP~\cite{Radford2021LearningTV}, which popularized the contrastive training objective to learn a joint image-text embedding space. This framework has been extended to various tasks, such as video retrieval~\cite{Luo2021CLIP4Clip}, unified vision-language modeling~\cite{Luo2020UniVL}, and cross-modal alignment~\cite{gao2021clip,Ma2022XCLIP}. In the 3D domain, PointCLIP~\cite{zhang2021pointclip} applied CLIP to point clouds by projecting them into multi-view depth maps, leveraging pretrained 2D knowledge. Subsequent research has focused on multi-modality alignment, \eg ImageBind~\cite{girdhar2023imagebind} aligns six modalities in the 2D domain and shows the power of such representation for generative tasks. In 3D, ULIP~\cite{xue2022ulip} and its successor ULIP-2~\cite{xue2023ulip2} aim to learn unified representations among images, texts, and point clouds. Point-Bind~\cite{pointbind} extends ImageBind~\cite{girdhar2023imagebind} to 3D by aligning specific pairs of modalities using an InfoNCE loss~\cite{oord2018representation}. While these methods effectively capture object-level data, they struggle to differentiate similar instances within a scene, primarily focusing on isolated objects rather than complex scenes. Experiments in Section~\ref{sec:experiment} demonstrate this limitation.


\begin{figure*}[ht!]
    \centering
    \includegraphics[trim=0 0 0 0,clip,width=0.95\linewidth]{fig/Architecture.pdf}
    \caption{\textbf{Overview of \project{}.} Given a scene $\mathcal{S}$ and its instances $\mathcal{O}_i$ represented across different modalities $\mathcal{I}, \mathcal{P}, \mathcal{M}, \mathcal{R}, \mathcal{F}$, the goal is to align all modalities within a shared embedding space. The \textit{Instance-Level Multimodal Interaction} module captures modality interactions at the instance level within the context of a scene. This is further enhanced by the \textit{Scene-Level Multimodal Interaction} module, which jointly processes all instances to represent the scene with a single feature vector $\mathcal{F_S}$. The \textit{Unified Dimensionality Encoders} eliminate dependency on precise semantic instance information by learning to process each scene modality independently while interacting with $\mathcal{F_S}$.}
    \label{fig:architecture}
    \vspace{-1mm}
\end{figure*}

A common limitation of these approaches is the assumption of \textit{perfect modality alignments} and \textit{complete data} for each instance, often relying on datasets like ShapeNet55~\cite{Chang2015ShapeNetAI}. 
This assumption is impractical for real-world scenarios where data is often incomplete or not well-matched due to occlusions, dynamic changes, sensor limitations, or capture errors, such as in construction sites or robot navigation.
Our work, \project{}, addresses these challenges using real-world datasets consisting of incomplete point clouds and noisy images captured using affordable sensors. Unlike prior methods, we do not require perfect modality alignments or complete data (e.g., point clouds).

\noindent \textbf{3D Scene Understanding} has driven extensive work on text-to-image and point cloud based instance localization and alignment within large maps~\cite{Arandjelovi2015NetVLADCA,camnet2019,text2pos2022}. Techniques like NetVLAD~\cite{Arandjelovi2015NetVLADCA} and CamNet~\cite{camnet2019} enable place recognition and image-based localization by extracting global image descriptors. Recent work has leveraged 3D scene graphs for enhanced scene understanding~\cite{armeni20193d,rosinol20203d,kim20193}, with methods like SGAligner~\cite{sarkar2023sgaligner} and SG-PGM~\cite{xie2024sg} facilitating scene alignment through 3D scene graph matching. For dynamic instance matching across long-term sparse environments, LivingScenes~\cite{zhu2023living} parses an evolving 3D environment with an object-centric formulation. For cross-modal retrieval, approaches like ScanRefer~\cite{chen2020scanrefer} and ReferIt3D~\cite{achlioptas2020referit_3d} localize objects in 3D scenes via natural language but rely on detailed annotations and fixed modality pairs. Methods like 3DSSG~\cite{3DSSG2020} and ``Where Am I"~\cite{Chen2024WhereAI} extend scene retrieval across images and natural language using 3D scene graphs, yet they depend heavily on semantic annotations. SceneGraphLoc~\cite{miao2024scenegraphloc} performs image-to-scene-graph matching, using semantic information. Our approach diverges from these by removing the need for semantics or explicit scene graphs, instead leveraging dimensionality-specific encoders and modality-agnostic embeddings for scene understanding without prior semantic knowledge. 

\noindent \textbf{Handling Missing Modalities} and noisy data is a key challenge in multi-modal learning~\cite{baltruvsaitis2018multimodal}. Traditional approaches often assume full data availability, limiting their real-world applicability. Some methods address missing data through modality imputation or robust models~\cite{tsai2018learning,wu2024comprehensive}. Baltrusaitis \etal~\cite{baltruvsaitis2018multimodal} highlight that many methods lack flexibility for incomplete or noisy data. Our framework tackles this by allowing independent mapping of each modality into a shared embedding space, enabling flexible cross-modal interactions in unstructured environments with sparse or unaligned data. Furthermore, emergent behavior in multi-modal models, such as generalizing and inferring relationships beyond training data~\cite{Radford2021LearningTV,girdhar2023imagebind}, are promoted by structuring training around image embeddings as a common representation. By mapping other modalities into this shared space, \project{} fosters organic cross-modal relationships, enabling unified understanding across diverse data types. 