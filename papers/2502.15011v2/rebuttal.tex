\documentclass[10pt,twocolumn,letterpaper]{article}
\usepackage[rebuttal]{cvpr}

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}

% Import additional packages in the preamble file, before hyperref
\input{preamble}

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\definecolor{cvprblue}{rgb}{0.21,0.49,0.74}
\definecolor{spidergreen}{RGB}{10,156,10}
\usepackage[pagebackref,breaklinks,colorlinks,allcolors=cvprblue]{hyperref}

% If you wish to avoid re-using figure, table, and equation numbers from
% the main paper, please uncomment the following and change the numbers
% appropriately.
%\setcounter{figure}{2}
%\setcounter{table}{1}
%\setcounter{equation}{2}

% If you wish to avoid re-using reference numbers from the main paper,
% please uncomment the following and change the counter value to the
% number of references you have in the main paper (here, 100).
%\makeatletter
%\apptocmd{\thebibliography}{\global\c@NAT@ctr 100\relax}{}{}
%\makeatother

\definecolor{darkgreen}{RGB}{0,150,0}

\newcommand{\Rone}{\text{\textcolor{blue}{\textbf{y1TH}}}\xspace}
\newcommand{\Rtwo}{\text{\textcolor{red}{\textbf{jp9m}}}\xspace}
\newcommand{\Rthree}{\text{\textcolor{darkgreen}{\textbf{iRkj}}}\xspace}

%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\paperID{7644} % *** Enter the Paper ID here
\def\confName{CVPR}
\def\confYear{2025}

\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{\project{}: 3D Scene Cross-Modal Alignment}  % **** Enter the paper title here

\maketitle
\thispagestyle{empty}

We thank the reviewers for their insightful comments and suggestions and for finding our proposed framework ``\textit{novel and meaningful}" (\Rtwo), ``\textit{very good}" and ``\textit{reasonable}" (\Rthree) ``\textit{flexible and scalable}" (\Rone), ``\textit{practical}" (\Rone), ``\textit{inspiring}" to following directions (\Rthree), and having ``\textit{robust}" (\Rone) and ``\textit{superior}" (\Rtwo, \Rthree) performance, as well as the paper ``\textit{well-organized}" (\Rthree), ``\textit{well-written and easy to follow}" (\Rtwo), and with ``\textit{clear descriptions and figures}" (\Rthree). Below, we address the comments. We will incorporate all feedback in the manuscript.
%\rule{\columnwidth}{0.4pt}

\noindent
[\Rtwo{}, \Rthree{}] \textbf{Image modality is always needed for instance-level interactions.} During \textit{training}, \project{} requires a base modality for every instance, to align other modalities with its feature space. We choose images as the base modality due to their availability and strong encoder priors, though any supported modality can serve this role. Crucially, \textit{no modality availability assumptions are made during inference}, allowing any query-target modality pair.

\noindent
[\Rone{}, \Rthree{}] \textbf{The performance on outdoor environments remain unclear.} Although focused on \textit{indoor} 3D scene understanding, \project{} utilizes strong pre-trained encoders and maps them into a shared space for cross-modal interaction. This approach can straightforwardly extend to outdoor scenes by training on such datasets, advancing toward "foundation" cross-modal scene alignment models. We will clarify the indoor focus in the text.

\noindent
[\Rtwo{}] \textbf{Camera sampling may not cover fully the entire scene.} Yes, it may not cover the entire scene. Our method selects $N$ camera poses to maximize $3D$ spatial separation in rotation and translation. Starting with a random pose, we iteratively select the pose farthest from \textit{all} previously chosen ones. This strategy achieves high accuracy (Fig. 3, suppl.), with performance saturating above $10$ views, indicating full scene coverage is not necessary for training \project{}. We will add visualization of selected cameras.

\noindent
[\Rone{}] \textbf{Rendering For Multi-view Image Modality.} Due to the sim-to-real gap, using rendered multi-view images for training impacts real-world performance (Tab.~\ref{tab:render_instance_matching_scannet}). We rendered 10 views per object mesh $\mathcal{M}$ and used real images when meshes were unavailable, for $\approx 70\%$ objects.


\begin{table}[ht!]
        \vspace{-8pt}
	\centering
	\resizebox{\linewidth}{!}{
   \begin{tabular}{l|cc|ccc}
    \toprule
       \cellcolor[HTML]{EEEEEE}{\textit{$\mathcal{I} \rightarrow \mathcal{P}$}} &  \multicolumn{2}{c|}{\textbf{Instance-level Recall} $\uparrow$} & \multicolumn{3}{c}{\textbf{Scene-level Recall} $\uparrow$} \\  %& \multicolumn{2}{R}
     \midrule\arrayrulecolor{black} 
      \multicolumn{1}{l|}{\textbf{Rendered Images}} & \textbf{top-1} & \textbf{top-3} & \textbf{R@25}\% & \textbf{R@50}\% & \textbf{R@75}\%  \\
    \midrule
    \hfill \checkmark \hfill & 52.62 & 80.15 & 95.42 & 63.07 & 4.9 \\
    \hfill \times \hfill & \fs 63.60 & \fs 88.51 & \fs 98.39 & \fs 79.74 & \fs 24.18 \\
    \bottomrule
    
    \end{tabular}
}
    \vspace{-10pt}
    \caption{\textbf{Instance Matching on \emph{Scannet}.} Training using rendered images does not generalise well in real-world evaluation.}\label{tab:render_instance_matching_scannet}
\end{table}

\vspace{-12pt}
\noindent
[\Rone{}] \textbf{Qualitative results and failure case analysis on 3RScan dataset.} Temporal scenes cluster naturally in the embedding space (Fig.\ref{fig:visual_comparison_scan3r}-top). However, query referrals may retrieve scans with similar objects across different scenes, especially when not discriminative enough (bottom).

\noindent
[\Rthree{}] \textbf{Requirements on the amount of training data.} \project{} encodes object- and scene-level features using frozen pre-trained text, image, and point cloud encoders. Fig. 1 (suppl.) shows ScanNet training achieves strong results, improved by adding 3RScan. We will simplify Fig. 2 (main) to highlight each module. Future work could train on multiple datasets for a more expressive embedding space.

\noindent
[\Rone{}, \Rthree{}] \textbf{Computational Complexity and Real-Time Deployment.} The $1.5$B-parameter model runs on an NVIDIA 4090 GPU, taking $1.01 \pm 0.26$s for a single modality and $1.98$s for all modalities. It can be adapted to lightweight encoders for faster inference in compute-limited scenarios, with potential performance trade-off.

\vspace{-10pt}
\begin{figure}[ht!]
    \centering
    \includegraphics[width=\linewidth]{fig/scene_retrieval_scan3r.pdf}
    \vspace{-10pt}
    \caption{\textbf{Referral ($\mathcal{R}$) $\rightarrow$ Point Cloud ($\mathcal{P}$) Scene Retrieval on \emph{3RScan}.} Top row - \textcolor{spidergreen}{Success}, Bottom row - \textcolor{red}{Failure}.}
    \label{fig:visual_comparison_scan3r}
    \vspace{-12pt}
\end{figure} 

\vspace{-7pt}
\noindent
[\Rtwo{}] \textbf{Emergent capabilities in object localization.} We are a bit unsure what the reviewer refers to with this comment. Our focus is cross-modal 3D scene alignment in the \textit{feature} space, matching objects or scenes across modalities; rather than direct object localization. Through temporal instance matching experiment, we demonstrate the effectiveness on scenes captured at different times, with object displacement and rearrangement. The emergent behavior we refer to lies in matching objects across different modalities (e.g., RGB images $\mathcal{I}$ and mesh $\mathcal{M}$) despite being unpaired during training. While the matches can be used as input for localization, the current method is designed for matching where we significantly improve upon baselines. Spatial localization based on these matches can be achieved using ICP or a learning-based method for 3D data, with similar techniques for feature matching and localization in 1D and 2D. We acknowledge that the term 'localization' was incorrectly used and will update the terminology.

% Our focus is cross-modal 3D scene alignment in the \textit{feature} space, matching objects or scenes across modalities rather than direct object localization. Our temporal instance matching experiment demonstrates effectiveness on scenes captured at different times with object displacement and rearrangement. The emergent behavior lies in matching objects across modalities (e.g., RGB and point cloud) despite being unpaired during training. While these matches can aid localization tasks, our method is designed for matching and significantly outperforms baselines. Spatial alignment can be achieved via ICP, learning-based 3D methods, or similar techniques in 1D/2D. We will correct the misuse of "localization" in the final version.

\noindent
[\Rthree{}] \textbf{Limitations of the method.}  Although \project{} excels in cross-modal instance matching, its scene retrieval generalizability could benefit from training on diverse indoor and outdoor datasets. \project{} assumes a base modality per dataset, advancing prior work requiring perfect modality alignment. Further relaxation is a promising direction. Finally, exploring its embedding space for downstream scene understanding remains a key area.
\vspace{-12pt}

% %%%%%%%%% REFERENCES
% {
%     \small
%     \bibliographystyle{ieeenat_fullname}
%     \bibliography{main}
% }

\end{document}
