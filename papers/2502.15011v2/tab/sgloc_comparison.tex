\begin{table*}[t]
  \centering
  \resizebox{0.6\textwidth}{!}{
   \begin{tabular}{l| ccc | ccc}
    \toprule
    \textbf{Method} & \multicolumn{6}{c}{\textbf{Static Scenario}} \\
    \midrule\arrayrulecolor{black} 
    & \multicolumn{3}{c}{\textbf{R out of $10$}  $\uparrow$}
    & \multicolumn{3}{c}{\textbf{R out of $50$}  $\uparrow$} \\   
    \midrule\arrayrulecolor{black} 
    & \textbf{top-1} & \textbf{top-5} & \textbf{top-10} &
    \textbf{top-1} & \textbf{top-5} & \textbf{top-10} \\
    \midrule\arrayrulecolor{black} 
    LidarCLIP~\cite{lidarclip2024} & 16.30 & 41.40 & 60.60 & 4.70 & 11.00 & 16.30 \\
    LipLoc~\cite{shubodh2024lip} & 14.00 & 35.80 & 57.90 & 2.00 & 8.60 & 15.20 \\
    SceneGraphLoc~\cite{miao2024scenegraphloc} & \fs53.60 & \fs81.90 & \fs92.80 & \fs30.20 & \fs50.20 & \fs61.20 \\
    Ours & \nd46.00 & \nd77.97 & \nd90.58 & \nd18.69 & \nd39.16 & \nd51.62 \\
    \bottomrule
    \end{tabular}
    }
    \caption{\textbf{Cross-Modal Coarse Visual Localization on \textit{3RScan}.} Given a single image of a scene, the goal is to retrieve the corresponding scene from a database of multi-modal maps. We evaluate following the experimental setup in SceneGraphLoc~\cite{miao2024scenegraphloc} and compare our method to it and its baselines. Despite encoding less information in our multi-modal maps, our method performs competitively with SceneGraphLoc.}
    \label{tab:sgloc_comparison}
\end{table*}