\section{Related Works}
\textbf{Multilingual Capabilities of LLMs:}
LLM performance varies across languages due to imbalanced pre-training data volume.  
However, even predominantly English-centric models ____ exhibit some degree of multilingual capability ____,
potentially due to the unintentional ingestion of multilingual data during pretraining ____.
Meanwhile, many recent LLMs have expanded their language coverage ____.
Despite these inherent multilingual capabilities, 
extending them to downstream tasks in low-resource settings ____ remains challenging. 

\noindent
\textbf{Multilingual Representation Alignment:}
Enhancing meaningful cross-lingual relationships between model representations has been a well-studied area in the context of many tasks,
including intermediate tasks such as 
bilingual lexicon induction ____ and
sentence embeddings ____,
as well as more direct applications like 
information retrieval ____ and translation ____.
In the context of LLMs, 
____ 
use linear projections learned offline to align non-English representations with English ones during decoding.
Our work differs in that our alignment objective is parameterized by the same weights as task-specific fine-tuning, 
and is directly applicable to multilingual fine-tuning.
____ align LLM top-layer representations specifically for the task of semantic textual similarity (STS).
Different from this work, they do not consider cross-lingual transfer in downstream tasks or explore intermediate LLM layers for alignment.

\noindent
\textbf{LLM Representation Analysis:}
Several recent works have analyzed LLM internal representations with
geometric analysis of representation spaces ____,
probing classifiers ____,
or logit lens analysis ____.
In particular,
____ identify “semantic hubs” in LLM middle layers, which integrate information from various data types.
Our findings are orthogonal to their work on multi-modality.