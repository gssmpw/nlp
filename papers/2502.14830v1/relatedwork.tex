\section{Related Works}
\textbf{Multilingual Capabilities of LLMs:}
LLM performance varies across languages due to imbalanced pre-training data volume.  
However, even predominantly English-centric models \cite{DBLP:journals/corr/abs-2307-09288} exhibit some degree of multilingual capability \cite{aycock-bawden-2024-topic,yuan-etal-2024-vocabulary},
potentially due to the unintentional ingestion of multilingual data during pretraining \cite{briakou-etal-2023-searching}.
Meanwhile, many recent LLMs have expanded their language coverage \cite{grattafiori2024llama3herdmodels,qwen2025qwen25technicalreport}.
Despite these inherent multilingual capabilities, 
extending them to downstream tasks in low-resource settings \cite{adelani-etal-2024-comparing,iyer-etal-2024-exploring} remains challenging. 

\noindent
\textbf{Multilingual Representation Alignment:}
Enhancing meaningful cross-lingual relationships between model representations has been a well-studied area in the context of many tasks,
including intermediate tasks such as 
bilingual lexicon induction \cite{zhang-etal-2017-adversarial} and
sentence embeddings \cite{feng-etal-2022-language,li-etal-2023-dual},
as well as more direct applications like 
information retrieval \cite{DBLP:journals/tmlr/IzacardCHRBJG22} and translation \cite{pham-etal-2019-improving,pan-etal-2021-contrastive}.
In the context of LLMs, 
\citet{wang2024bridginglanguagegapslarge} 
use linear projections learned offline to align non-English representations with English ones during decoding.
Our work differs in that our alignment objective is parameterized by the same weights as task-specific fine-tuning, 
and is directly applicable to multilingual fine-tuning.
\citet{wu-etal-2024-representational} align LLM top-layer representations specifically for the task of semantic textual similarity (STS).
Different from this work, they do not consider cross-lingual transfer in downstream tasks or explore intermediate LLM layers for alignment.

\noindent
\textbf{LLM Representation Analysis:}
Several recent works have analyzed LLM internal representations with
geometric analysis of representation spaces \cite{razzhigaev-etal-2024-shape,lee2024multimodalfoundationmodelsencode},
probing classifiers \cite{wang-etal-2024-probing-emergence,li2025exploringmultilingualprobinglarge},
or logit lens analysis \cite{wu2024semantichubhypothesislanguage}.
In particular,
\citet{wu2024semantichubhypothesislanguage} identify “semantic hubs” in LLM middle layers, which integrate information from various data types.
Our findings are orthogonal to their work on multi-modality.