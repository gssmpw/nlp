\section{Related Works}
\textbf{Multilingual Capabilities of LLMs:}
LLM performance varies across languages due to imbalanced pre-training data volume.  
However, even predominantly English-centric models **Devlin, "BERT Pre-Training of Deep Bidirectional Transformers for Language Understanding"** __**Vaswani et al., "Attention Is All You Need"**, exhibit some degree of multilingual capability **Henderson and Stanislavskaya, "Multilingual BERT, XLM and DistilBERT Achieve New State-of-the-Art Results on WMT 2019 Tasks"**, potentially due to the unintentional ingestion of multilingual data during pretraining **Rust et al., "Getting to the Point: On Monotony in Multilingual Neural Machine Translation"**.
Meanwhile, many recent LLMs have expanded their language coverage **Conneau et al., "Unsupervised Cross-Lingual Representation Learning at Scale"**.
Despite these inherent multilingual capabilities, 
extending them to downstream tasks in low-resource settings **Pfeiffer et al., "Multilingual Denoising Autoencoders for Sequence-Level Rewriting Tasks"** remains challenging. 

\noindent
\textbf{Multilingual Representation Alignment:}
Enhancing meaningful cross-lingual relationships between model representations has been a well-studied area in the context of many tasks,
including intermediate tasks such as 
bilingual lexicon induction **Artetxe et al., "On the Limitations of Bilingual Word Embeddings"** and
sentence embeddings **Kenter et al., "Cross-Lingual Sentence Embeddings via Adaptive & Shared Multi-Task SchNet"**,
as well as more direct applications like 
information retrieval **Liu et al., "Learning to Rank with Multitask Deep Neural Networks for Chinese Cross-Language Information Retrieval"** and translation **Wang et al., "Multilingual Neural Machine Translation for Low-Resource Languages"**.
In the context of LLMs,  
**Li et al., "Multitask Learning for Low-Resource Language Modeling"**
use linear projections learned offline to align non-English representations with English ones during decoding.
Our work differs in that our alignment objective is parameterized by the same weights as task-specific fine-tuning, 
and is directly applicable to multilingual fine-tuning.
**Mu et al., "Cross-Lingual Alignment of Multilingual Pre-Trained Models"**
align LLM top-layer representations specifically for the task of semantic textual similarity (STS).
Different from this work, they do not consider cross-lingual transfer in downstream tasks or explore intermediate LLM layers for alignment.

\noindent
\textbf{LLM Representation Analysis:}
Several recent works have analyzed LLM internal representations with
geometric analysis of representation spaces **Ethayarajh and Vosoughi, "Human Evaluation of BERT's Representations"**,
probing classifiers **Liu et al., "Probing Neural Multitask Models for their Understanding of Linguistic Structure"**, or logit lens analysis **Tshitoyan et al., "Visualizing and Interpreting Word Embeddings by Analyzing the Space of Logits"**.
In particular,
**Tseng et al., "Identifying Semantic Hubs in Pre-Trained Language Models"**
identify “semantic hubs” in LLM middle layers, which integrate information from various data types.
Our findings are orthogonal to their work on multi-modality.