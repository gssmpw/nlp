\documentclass[lettersize,journal]{IEEEtran}
\usepackage{amsmath,amsfonts, bm}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{cite}
%\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
% updated with editorial comments 8/9/2021

% new commands
\newcommand{\real}{\mathbb{R}}
\newcommand{\nat}{\mathbb{N}}
\newcommand{\argmin}{\mathrm{arg~min}}
\newcommand{\diag}{\mathrm{diag}}
\newcommand{\A}{\mathbf{A}}
\newcommand{\E}{\mathbf{E}}
\newcommand{\e}{\mathbf{e}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\X}{\mathbf{X}}
\newcommand{\ba}{\mathbf{a}}
\newcommand{\bS}{\mathbf{S}}
\newcommand{\s}{\mathbf{s}}

\newcommand{\esad}{\E\text{-}\mathrm{SAD}_\s}
\newcommand{\xsad}{\X\text{-}\mathrm{SAD}_\s}

\newtheorem{theorem}{Theorem}
\newtheorem{assumption}{Assumption}
\newtheorem{definition}{Definition}


\begin{document}

\title{A Two-step Linear Mixing Model for Unmixing \\ under Hyperspectral Variability}

\author{Xander Haijen, Bikram Koirala, Xuanwen Tao, and Paul Scheunders
        % <-this % stops a space
\thanks{Xander Haijen, Bikram Koirala, Xuanwen Tao, and Paul Scheunders are with the Imec-Visionlab research group, Department of Physics, University of Antwerp.}% <-this % stops a space
\thanks{Manuscript submitted to the IEEE transactions on image processing on 21 Feb 2025.}
}

% The paper headers
\markboth{}%
{Haijen \MakeLowercase{\textit{et al.}}: A Two-step Linear Mixing Model for Unmixing under Hyperspectral Variability}

% \IEEEpubid{0000--0000/00\$00.00~\copyright~2021 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.

\maketitle

\begin{abstract}
Spectral unmixing is an important task in the research field of hyperspectral image processing. It can be thought of as a regression problem, where the observed variable (i.e., an image pixel) is to be found as a function of the response variables (i.e., the pure materials in a scene, called endmembers). The Linear Mixing Model (LMM) has received a great deal of attention, due to its simplicity and ease of use in, e.g., optimization problems. Its biggest flaw is that it assumes that any pure material can be characterized by one unique spectrum throughout the entire scene. In many cases this is incorrect: the endmembers face a significant amount of spectral variability caused by, e.g., illumination conditions, atmospheric effects, or intrinsic variability. Researchers have suggested several generalizations of the LMM to mitigate this effect. However, most models lead to ill-posed and highly non-convex optimization problems, which are hard to solve and have hyperparameters that are difficult to tune. 
In this paper, we propose a two-step LMM that bridges the gap between model complexity and computational tractability. We show that this model leads to only a mildly non-convex optimization problem, which we solve with an interior-point solver. This method requires virtually no hyperparameter tuning, and can therefore be used easily and quickly in a wide range of unmixing tasks. We show that the model is competitive and in some cases superior to existing and well-established unmixing methods and algorithms. We do this through several experiments on synthetic data, real-life satellite data, and hybrid synthetic-real data.
\end{abstract}

\begin{IEEEkeywords}
Remote sensing, hyperspectral unmixing, spectral variability, two-step linear mixing model, ELMM, SLMM, interior-point method
\end{IEEEkeywords}

\section{Introduction}
\input{introduction}

\section{Related work}
\input{related}

\section{Proposed model (2LMM)}
\input{proposed_model}

\section{Experiments with synthetic data}
\input{experiments_synthetic}

\section{Experiments with real data}
\input{experiments_real}

\section{Conclusion}
In this work, we have presented the 2LMM, a novel physically motivated two-step linear mixing model that mitigates the effect of spectral variability. The model bridges the gap between model complexity and computational tractability. A key feature of the 2LMM is that it leads to a mildly non-convex unmixing problem, which we solve using an interior-point method. Experiments on synthetic and real hyperspectral data show that the 2LMM achieves competitive performance against existing methods and exhibits robustness to deviations from its underlying assumptions.


\section*{Acknowledgments}
The research presented in this paper is funded by the Research Foundation-Flanders - project G031921N. Bikram Koirala is a postdoctoral fellow of the Research Foundation Flanders, Belgium (FWO: 1250824N-7028). The authors  acknowledge the team of Daniele Cerra at DLR for the development of the DLR HySU  dataset.

\bibliographystyle{IEEEtran}
\bibliography{main}

{\appendices
\section{Assumption on the feasible set $\mathcal{X}$}
In order for the barrier function to be well-defined we require that the problem (\ref{eq: interior point problem}) admits at least one strictly feasible solution \cite{boyd_convex_2004}:
\begin{assumption}
    The feasible set $\mathcal{X} \subseteq \real^D$ is nonempty and the problem (\ref{eq: interior point problem}) is \textbf{strictly feasible}, i.e. 
        \[
    \exists \Bar{\x} \in \real^D: g_i(\Bar{\x}) > 0, \forall i, ~\mathbf{C}\Bar{\x} = \mathbf{b}, ~\Bar{\x} > 0.
        \]
\end{assumption}

\section{Newton's method for equality-constrained optimization}
For a fixed value of $\mu$, we can formulate the Karush-Kuhn-Tucker (KKT) optimality conditions for the problem (\ref{eq: barrier problem}) and solve them. The KKT conditions start from the Lagrangian $\mathcal{L}_\mu(\x, \bm{\lambda})$ \cite[Ch. 5]{boyd_convex_2004} and read
\begin{equation} \label{eq: kkt system}
    \begin{aligned}
        \nabla_{\bm{\lambda}} \mathcal{L}_\mu (\x, \bm{\lambda}) &= 0 \\
        \nabla_\x \mathcal{L}_\mu (\x, \bm{\lambda}) &= 0.
    \end{aligned}
\end{equation}
This is a system of nonlinear equations, and can be solved using Newton's method for nonlinear equations. For this, define
\[
\textbf{F}_\mu(\x, \bm{\lambda}) = \left( \begin{array}{c}
     - \nabla_{\bm{\lambda}} \mathcal{L}_\mu (\x, \bm{\lambda}) \\ \nabla_\x \mathcal{L}_\mu (\x, \bm{\lambda})
\end{array} \right).
\]
A step $\Delta^k = (\Delta_\x^k, \Delta_{\bm{\lambda}}^k)$ is found by solving
\[
\mathbf{J}_{\textbf{F}_\mu}(\x_k, \mu_k) \Delta^k = - \textbf{F}_\mu(\x_k, \bm{\lambda}_k)
\]
where $\mathbf{J}_{\textbf{F}_\mu}$ is the Jacobian matrix. The solution is then updated using a line search procedure:
\begin{align*}
    \x_{k+1} &= \x_k + \alpha_k \Delta_\x^k \\
    \bm{\lambda}_{k+1} &= \bm{\lambda}_k + \alpha_k \Delta_{\bm{\lambda}}^k
\end{align*}
where the step size $\alpha_k$ is determined using a backtracking line search algorithm.


\section{Convergence}
A formal convergence proof is beyond the scope of this paper, so instead we sketch a convergence analysis based on \cite{boyd_convex_2004}. The analysis consists of two parts, proving convergence of the inner (Newton) loop and outer loop, respectively. Fix the following sequence for the barrier parameter $\mu$: $\mu_0, \nu \mu_0, \nu^2 \mu_0, \ldots$ for $0 < \nu < 1$.

\subsubsection*{Inner loop} 
We make the following assumptions:
\begin{assumption} Consider the problem (\ref{eq: interior point problem}) and its corresponding barrier problem (\ref{eq: barrier problem}). The barrier problem can always be solved using Newton's method, or equivalently:
        \begin{enumerate}
            \item $f(\x)$ and $g_i(\x)$ are closed on $\mathcal{X}$, i.e., the set
            \[
            \{\x \in \mathcal{X} \mid f(\x) \leq \alpha\}
            \]
            is closed for any $\alpha \in \real$, similarly for $g_i(\x)$.
            \item For all $\x \in \mathcal{X}$, we have $\| \x \|_2^2 \leq R^2$ for some $R < +\infty$.
        \end{enumerate}
\end{assumption}
It follows from Assumption 2 that each barrier problem can be solved using Newton's method in a finite number of steps. Bounding the number of steps is hard without making additional assumptions on the problem. If we assume $B(\x, \mu)$ is closed and \textit{self-concordant} for all $\mu \leq \mu_0$, and assume the sublevel sets of the problem (\ref{eq: interior point problem}) are bounded, we can provide an upper bound on the required number of Newton steps. If we solve the problem to an accuracy of $\epsilon_\mathrm{N} > 0$, then we need at most
\[
\frac{I}{\gamma}(\nu - 1 - \log \nu) + \log_2 \log_2 \frac{1}{\epsilon_\mathrm{N}}
\]
steps, where $\gamma$ is a constant determined by the backtracking line search procedure, and $I$ is the number of inequality constraints.

\subsubsection*{Outer loop} 
If the barrier problem (\ref{eq: barrier problem}) can be minimized using Newton's method for the sequence $\{\mu_k\}_{k \geq 0}$ as mentioned above, then we can achieve a desired accuracy $\epsilon_\mathrm{B} > 0$ after
\[
\left\lceil \frac{\log \left( I \mu_0 / \epsilon_\mathrm{B} \right)}{\log 1/\nu}\right\rceil + 1
\]
steps. Therefore, since we can solve both the outer problem and inner problem in finitely many steps, we can guarantee that the algorithm will always converge to a locally optimal solution in finite time.

\section{Slack variables}
We replace all logarithms of the form $\log (g_i(\x))$ by the constrained form \cite{geletu_introduction_nodate} 
\[
\log \sigma_i~\mathrm{s.t.}~g_i(\x)-\sigma_i = 0.
\]
For the two scaling factor approach, this leads to the modified barrier function 
\begin{multline*}
    \Tilde{B}(\A_\s, \s_\E) = J(\A_\s, \s_\E) - \mu \Big( \sum_{k=1}^K ( \log \sigma_k^+ + \log \sigma_k^-) + \\
                \sum_{k=1}^K \sum_{n=1}^N (\log \sigma_{nk} + \log a_{nk}) \Big)
\end{multline*}
and the optimization problem
\begin{equation*}
\begin{aligned}
        \min &~\Tilde{B}(\A_\s, \s_\E) \\
    \mathrm{s.t.} &~ s_{\e_k} - \underline{S} - \sigma^+_k = 0, \quad \forall k\\
                &~\overline{S} - s_{\e_k} - \sigma^-_k = 0, \quad \forall k \\
                &~\overline{S} - a_{nk} - \sigma_{nk} = 0, \quad \forall n, k
\end{aligned}
\end{equation*}
which is equivalent to the original barrier problem (\ref{eq: barrier problem}).
}
\newpage




\vfill

\end{document}


