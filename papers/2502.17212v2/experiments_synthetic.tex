In this section, we compare the three newly proposed algorithms to each other, and to several well-established methods based on the LMM, SLMM and ELMM. 

Throughout these experiments, results will be validated with the following metrics.
To validate the reconstruction error, the reconstruction Root Mean Square Error (RMSE) 
is defined as:
\[
\mathrm{RMSE}_\mathbf{X} = \sqrt{\frac{1}{NP} \sum_{n=1}^N \|\mathbf{\x}_n - \mathbf{\hat{\x}}_n\|^2_2}
\]
where $\hat{\x}$ denotes an estimated pixel and $\x$ denotes a measured pixel. 
Similarly, the reconstruction Spectral Angle Distance (SAD) in degrees is defined as:
\[
\mathrm{SAD}_\mathbf{X} = \frac{1}{NP} \sum_{n=1}^N \frac{\mathbf{x}_n^\top \mathbf{\hat{x}}_n}{\|\mathbf{x}_n\|_2 \|\mathbf{\hat{x}}_n\|_2} \times \frac{180^\circ}{\pi}.
\]

To validate the performance of the abundance estimation, we define the abundance RMSE as:
\[
\mathrm{RMSE}_\mathbf{A} = \sqrt{\frac{1}{NK} \sum_{n=1}^N \|\mathbf{a}_n - \mathbf{\hat{\ba}}_n\|^2_2}
\]
where $\hat{\ba}$ denotes an estimated abundance vector and $\ba$ is a ground truth abundance vector. 

To validate the performance of the scaling estimation, 
when unmixing 2LMM-generated data with 2LMM, the scaling RMSE is defined as:
\[
\mathrm{RMSE}_\mathbf{s} = \frac{1}{K+N}\|\mathbf{s} - \mathbf{\hat{\s}}\|_2
\]
where $\s = [\s_\E^\top~\s_\X^\top]^\top$ is the $(K+N)$-dimensional vector containing all scaling factors. However, the scaling RMSE might give a wrong picture about the estimation accuracy. This is because every pixel is influenced by both the EM scaling factors $\s_\E$ and its pixel scaling factor $\s_{\x_n}$. It is of no importance for the final result whether the largest scaling happens in the first scaling step or the second scaling step, as long as the \textit{resulting} scaling in each pixel is correct. This is not taken into account by the RMSE, so it will wrongly penalize correct scaling factors. For this, we propose the following error metric, which looks at the EM scaling step and pixel scaling step separately, and verifies whether the estimated vectors are scaled versions of the actual vectors, thus incorporating this indifference to how the scaling is distributed over the two steps. We call these error metrics the EM scaling SAD $\esad$ and the pixel scaling SAD $\xsad$:
\begin{align*}
    \esad &= \frac{1}{K} \frac{\s_\E^\top\hat{\s}_\E}{\|\s_\E\|_2\|\hat{\s}_\E\|_2} \times \frac{180°}{\pi} \\
    \xsad &= \frac{1}{N} \frac{\s_\X^\top\hat{\s}_\X}{\|\s_\X\|_2\|\hat{\s}_\X\|_2} \times \frac{180°}{\pi}
\end{align*}
This metric is a more truthful representation of the observable \textit{result} of the scaling on the pixels.

\subsection{Data}
We selected three EMs (asphalt (gds367), brick (gds350) and cardboard (gds371)) from the United States Geological Survey (USGS) spectral library \cite{kokaly_usgs_2017}, which contain 2152 spectral bands from the visible to the short-wave infrared range (200 nm to 2,500 nm). Their reflectance is shown in Fig. \ref{fig: usgs ems}.  For  computational considerations, we selected 224 equidistant bands for each EM. We call these reference EMs $\E_0$.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{Figs/usgs_ems.jpg}
    \caption{The EMs used for generating the synthetic data: asphalt (gds367), brick (gds350) and cardboard (gds371).}
    \label{fig: usgs ems}
\end{figure}

We generated synthetic abundance maps based on \emph{Gaussian Random Fields} (GRFs). Gaussian random fields can be thought of as spatially correlated Gaussian randomness \cite{kozintsev_computations_1999, noauthor_hyperspectral_nodate}, and they are a popular choice for generating synthetic hyperspectral data. We generate abundance maps using GRFs designed to comply with the ANC and ASC. The ground truth abundances are called $\A_\mathrm{gt}$. The scaling factors were drawn from the uniform distribution $\mathcal{U}([0.5; 1.5])$. The choice for this range is based on physical arguments, limiting the scaling factors to a meaningful range, as very large scaling factors or scaling factors close to zero are physically unrealistic.  The synthetic images were then designed to either comply with the 2LMM or the ELMM.

\subsubsection{2LMM-generated variability}
For generating synthetic data according to the 2LMM, we generate $N + K$ scaling factors, and group them in vectors $\s_\E$ and $\s_\X$. Then we generate the $n$-th pixel as
\[
\x_n = \E_0 \mathrm{diag}(\s_\E) \ba_{\mathrm{gt}, n} s_{\x_n}.
\]
We do not add any noise to the image.

\subsubsection{ELMM-generated variability}
For generating synthetic data according to the ELMM, we generate $NK$ scaling factors and combine these into $N$ vectors of dimension $K$, $\s_n, n=1,\ldots, N$. Then we generate the $n$-th pixel as
\[
\x_n = \E_0 \diag(\s_n) \ba_{\mathrm{gt}, n}.
\]
We do not add any noise to the image.

\subsection{Influence of the bounds $\underline{S}$ and $\overline{S}$} \label{sec: influence of bounds}
In this first experiment, a $50 \times 50$ synthetic image is generated with 2LMM-generated variability. 
We first validated the performance of the 3 proposed approaches to solve the optimization. We observed that the $\mathrm{2LMM}_\mathrm{norm}$ approach fails, because it consumes too much memory. During execution, it produces an out-of-memory error and is terminated by the operating system. The angle approach $\mathrm{2LMM}_\mathrm{angle}$ does not crash, but it is extremely slow and does not converge. It was automatically terminated after 1 million function evaluations, at which point it had run for approximately 4.5 hours. Based on this observation, we will use the two scaling factor approach $\mathrm{2LMM}$ for further experimentation.

To examine the effect of the bounds on the resulting estimates, we vary the lower and upper bounds $\underline{S}$ and $\overline{S}$. 
The bounds were taken to be $\left[ \frac{1}{\alpha}, \alpha \right]$ for different values of $\alpha > 1$. The results are shown in Table \ref{tab:small image grf}.
\begin{table}[t]
\caption{Unmixing results on a  $50 \times 50$ synthetic image with 2LMM-generated variability. The bounds $[\underline{S}, \overline{S}]$ are given by $[\frac{1}{\alpha}, \alpha]$. The best results are highlighted in bold.}
\centering
        \begin{tabular}{|r|cccccc|}
        \hline
        $\alpha$    & 100    & 10              & 5      & 2               & 4/3             & 10/9   \\ \hline \hline
        RMSE$_\X$   & 1e-6   & \textbf{9e-7}   & 2e-6   & 2e-6            & 2e-6            & 0.0042 \\ \hline
        SAD$_\X$    & 1e-4   & \textbf{4e-5}   & 1e-4   & 1e-4            & 1e-4            & 0.1063 \\ \hline
        RMSE$_\A$   & 0.0239 & 0.0239          & 0.0239 & 0.0239          & \textbf{0.0222} & 0.0951 \\ \hline
        RMSE$_\s$   & 57.804 & 4.8513          & 1.9099 & \textbf{0.1511} & 0.2039          & 0.3170 \\ \hline
        $\esad$     & 4.3382 & 4.3395          & 4.3393 & 4.3364          & \textbf{3.8441} & 18.260 \\ \hline
        $\xsad$     & 2.4598 & 2.4600          & 2.4602 & \textbf{2.4591} & 2.4976          & 8.9428 \\ \hline
        \end{tabular}
\label{tab:small image grf}
\end{table}
First, one can observe that the results are overall the best when the chosen bounds (i.e., $[\underline{S}, \overline{S}] = [0.5, 2]$) are closest to the actual range of scale values (i.e., $[0.5,1.5]$). However, the results
are not overly sensitive to changes of the bounds $\underline{S}$ and $\overline{S}$ and there is a fairly broad range of choices that lead to similar results. 
Nevertheless, the results suggest that the bounds should not be chosen too tight, as this will result in a feasible set that is very small. As a result, many good solutions will fall outside the feasible set, ending up with a poor solution. This is the case when $[\underline{S}, \overline{S}] = [\frac{9}{10}, \frac{10}{9}]$ where the abundance RMSE,  the reconstruction RMSE, the reconstruction SAD and the scaling SADs are higher than the cases with looser bounds.
When the bounds are chosen wider than the actual scaling range, the abundance estimates remain accurate, and the  reconstruction error remains low, but the scaling RMSE is very high. However, this is not an issue, since the scaling SADs are still low, so the total scaling is still estimated accurately.
In conclusion, since the priority is accurate abundance estimation, it is crucial to select sufficiently wide bounds that encompass a realistic range of scalings.

%This suggests that there is a balance in between, where the method is given enough freedom to explore possible solutions, but is still guided by bounds that are appropriately chosen. Given these observations, we continue in what follows with fixed bounds $[\underline{S}, \overline{S}] = [\frac{1}{2}; 2]$.


\subsection{Comparison to LMM, SLMM and ELMM}

In this experiment, synthetic data are generated using reference endmembers $\E_0$ and GRF-generated ground truth abundances $\A_\mathrm{gt}$. The image size is $100 \times 100$. The required number of scaling factors is sampled from the uniform distribution $\mathcal{U}([0.5, 1.5])$.
The bounds of 2LMM are chosen accordingly as $[\underline{S}, \overline{S}] = [0.5, 2]$.
We compare the performance of 2LMM to several well-established unmixing methods, more precisely: LMM (solved with FCLSU), SLMM (solved with CLSU) and ELMM (solved with alternating least-squares and ADMM, as described in \cite{drumetz_blind_2016}).
For the ELMM-based method, we test two variants: WS-ELMM, where the method is \textit{warm-started}, i.e., initialized with the abundance estimates from CLSU, and CS-ELMM, where the method is \textit{cold-started}, i.e., initialized with uniform abundance estimates $\frac{1}{K}$. 

\subsubsection{Performance under 2LMM-generated variability}
In this first experiment, the synthetic image is generated with 2LMM-generated variability. We compare the results of the 2LMM method to the three models mentioned above.  The results are shown in Table \ref{tab: results 2lmm lowvar}, along with the computation times. 

Overall, the 2LMM method is the best performing method at a reasonable cost. Given the fact that the ELMM is a model that is rich enough to describe any dataset with 2LMM-generated variability without modeling error, it is quite surprising that WS-ELMM fails to perform better than CLSU, which will have a possibly large model mismatch since it is too simple to describe most 2LMM-based models.

\begin{table}[t]
    \caption{Experimental results for synthetic data with  2LMM-generated variability. The best errors are highlighted in bold.}
    \begin{center}
    \begin{tabular}{|r|ccccc|}
    \hline
               & FCLSU  & CLSU   & WS-ELMM & CS-ELMM & 2LMM  \\ \hline \hline
    RMSE$_\X$  & 0.0167 & 0.0027 & 0.0129  & 0.0089  & \textbf{2e-6}   \\ \hline
    SAD$_\X$   & 3.9934 & 0.0715 & 1.0199  & 1.6937  & \textbf{0.0002}   \\ \hline
    RMSE$_\A$  & 0.2190 & 0.0919 & 0.0913  & 0.2598  & \textbf{0.0135}    \\ \hline
    $\Delta t$ & 17     & 16     & 31      & 96      & 48                 \\ \hline
    \end{tabular}
    \end{center}
    \label{tab: results 2lmm lowvar}
\end{table}

\begin{table}[t]
    \caption{Experimental results for synthetic data with  ELMM-generated variability. The best errors are highlighted in bold.}
    \begin{center}
    \begin{tabular}{|r|ccccc|}
    \hline
               & FCLSU  & CLSU   & WS-ELMM & CS-ELMM & 2LMM   \\ \hline \hline
    RMSE$_\X$  & 0.0207 & 0.0039 & 0.0118  & 0.0149  & \textbf{2e-7}   \\ \hline
    SAD$_\X$   & 1.7413 & 0.1044 & 0.5676  & 2.0905  & \textbf{2e-6}    \\ \hline
    RMSE$_\A$  & 0.1513 & 0.0744 & 0.0740  & 0.2307  & \textbf{0.0693}    \\ \hline
    $\Delta t$ & 19     & 17     & 26      & 70      & 62                 \\ \hline
    \end{tabular}
    \end{center}
    \label{tab:results elmm lowvar}
\end{table}

\subsubsection{Performance under ELMM-generated variability}
The experiment from the previous paragraph is repeated, but this time with the variability generated according to the ELMM. The results are shown in Table \ref{tab:results elmm lowvar}. Again, 2LMM performed the best overall.  
Regarding the abundance estimation, 2LMM, CLSU and WS-ELMM perform similarly. Interestingly, WS-ELMM only performs as good as CLSU, even though CLSU has a significant model mismatch, while ELMM is rich enough to describe the scene exactly, and is initialized using the CLSU estimates. The estimates of CS-ELMM are very poor, meaning that ELMM relies heavily on a good initial estimate. 
Because 2LMM is only mildly non-convex and because the cost function of 2LMM (Eq. \ref{eq: two scaling factor}) only consists of the reconstruction error, the local interior-point solver is able to find a (close to) global minimum for this problem, with a reconstruction RMSE that is very close to zero. This is not the case for WS-ELMM and CS-ELMM, since ELMM is highly non-convex, and its cost function includes regularization terms as well. This means that, even if a global minimum of the ELMM cost function was obtained, it is very unlikely to coincide with a near-zero reconstruction RMSE.
Lastly, FCLSU does not perform very well due to considerable model mismatch. 
