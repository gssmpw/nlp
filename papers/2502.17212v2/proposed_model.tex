\subsection{Motivation and model description} 

We propose a new model, which bridges the gap between the simple SLMM, and the rich, but complicated ELMM. The proposed model, which we call the \textbf{two-step linear mixing model} (2LMM), is a physically motivated model that uses reference EMs extracted from the image or provided in a spectral library.

The 2LMM balances the computational ease of the SLMM and the model complexity of the ELMM. The model is not as complicated as the ELMM, leading to better-posed optimization problems. On the other hand, it is richer than the SLMM, so it can model more diverse scenes.

Using the reference EMs $\E$, the model is constructed as follows. As a first scaling step, the EMs are scaled independently of each other, but in the same way across the entire image. Then, the EMs are  linearly combined to form unscaled pixels. The second scaling step then consists of scaling each mixed pixel independently. See Fig. \ref{fig: 2lmm concept} for a conceptual representation of the 2LMM mixing process.  The 2LMM can still model material-specific variability, but in a more constrained way than the ELMM. 

The 2LMM is constructed with the following acquisition scenario in mind. Assume that reference EMs have been obtained, either from the image itself, or from some spectral library. The acquisition conditions to generate  the reference signatures might differ significantly from the acquisition conditions of  the image, such that the reference EMs will be scaled versions of the actual EMs in the image. If the EEA extracts an EM from a heavily illuminated region, it will have to be scaled to obtain the actual EM in standard conditions. This is especially true when reference EMs are obtained from a spectral library. The first scaling step of the EMs  corrects for this effect. The pixel scaling step  then further corrects for any pixel-wise illumination differences. 

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{Figs/2LMM.pdf}
    \caption{A graphical representation of the 2LMM model assumption. \textbf{1.} The reference EMs (blue lines) are scaled independently (red lines). \textbf{2.} The scaled EMs are mixed to form the unscaled pixels in the image. \textbf{3.} Each pixel is scaled independently to form the final image.}
    \label{fig: 2lmm concept}
\end{figure}

\subsection{Mathematical formulation}

Let $\s_\E \in \real^K$ be the EM scaling vector representing the first scaling step, and $s_{\x_n}$ a pixel-dependent scaling factor representing the second scaling step. Then the $n$-th pixel in the 2LMM is given by:
\begin{equation*}
    \x_n = \E\mathrm{diag}(\s_\E)\ba_n s_{\x_n}.
\end{equation*}
We can combine this for all pixels. Let 
\[
\s_\X = [s_{\x_1}~s_{\x_2}~\cdots~s_{\x_N}]^\top
\]
denote the pixel scaling vector. Then the 2LMM at the image level is given by:

\begin{equation} \label{eq: 2lmm cost function}
    \X = \E\mathrm{diag}(\s_\E)\A\mathrm{diag}(\s_\X).
\end{equation}



\subsection{Performing unmixing with the 2LMM}
Consider a non-convex FCLSU problem for the 2LMM:
\begin{equation}
\begin{aligned}
    \min_{\A, \s_\E, \s_\X} &~\|\Hat{\X} - \E \diag(\s_\E)\A\diag(\s_\X)\|_F^2 \\
    \text{s.t.} &~ 0 \leq \A \leq 1, \mathbf{1}^\top \A = \mathbf{1} \\
    &~ \underline{S} \leq \s_\E \leq \overline{S}, \quad \underline{S} \leq \s_\X \leq \overline{S}
\end{aligned}
\end{equation}
where the box bounds $\underline{S}, \overline{S} > 0$ can be used to constrain the scaling variables to a user-specified interval. Naturally, $\underline{S} < \overline{S}$. The motivation for introducing these box bounds is both physical and mathematical. First, it allows us to constrain the scaling factors to a physically meaningful range, since in many cases credible assumptions can be made about the magnitude of the scaling factors. Secondly, it makes the problem easier to solve mathematically, since it reduces the size of the search space, and therefore reduces the probability of finding a sub-optimal solution to the non-convex cost function.

The quantity in which we are interested is the abundance matrix $\A$. The estimates of the scaling variables $\s_\E$ and $\s_\X$ are less important. However, this increase in variables might lead to worse optimization results because the problem becomes more complex. Therefore, we propose the following optimization strategies to (partially) avoid the need to estimate the scaling factors.

\subsubsection{Scaling-independent optimization}

The cost function for the model (\ref{eq: 2lmm cost function}) can be written as a sum over the different pixels:
\[
J(\A, \s_\E, \s_\X) = \sum_{n=1}^N \| \hat{\x}_n - \E \diag(\s_\E) \ba_n s_{\x_n}\|_2^2.
\]
To remove the need to estimate the pixel scaling factors, we may now divide both terms by their norm, to obtain the norm-divided cost function:
\begin{equation} \label{eq: norm division cost function}
    \Tilde{J}(\A, \s_\E) = \sum_{n=1}^N \left\| \frac{\hat{\x}_n}{\|\hat{\x}_n\|_2} - \frac{\E \diag(\s_\E) \ba_n}{\|\E \diag(\s_\E) \ba_n\|_2}\right\|_2^2
\end{equation}
This cost function defines the \textit{norm division approach}:
\begin{equation} \label{eq: norm division}
   (\mathrm{2LMM}_\mathrm{norm}):~
   \begin{aligned}
        \min_{\A, \s_\E} &~\sum_{n=1}^N \left\| \frac{\hat{\x}_n}{\|\hat{\x}_n\|_2} - \frac{\E \diag(\s_\E) \ba_n}{\|\E \diag(\s_\E) \ba_n\|_2}\right\|_2^2 \\
    \text{s.t.} &~ 0 \leq \A \leq 1,~\mathbf{1}^\top \A = \mathbf{1}  \\
    &~ \underline{S} \leq \s_\E \leq \overline{S}
\end{aligned}
\end{equation}
By normalizing the two terms in Eq. (\ref{eq: norm division cost function}), we discard the length of the vectors in order to minimize the \textit{angle} between them. We can do this more explicitly. Using the standard inner product in $\real^D$:
\[
\langle \mathbf{u}, \mathbf{v} \rangle = \sum_{d=1}^D u_d v_d = \mathbf{u}^\top \mathbf{v},
\]
the angle between two vectors $\mathbf{u}$ and $\mathbf{v}$ is given by:
\[
\angle(\mathbf{u},\mathbf{v}) = \arccos \left( \frac{\mathbf{u}^\top \mathbf{v}}{\|\mathbf{u}\|_2 \|\mathbf{v}\|_2}\right).
\]
This suggests using the following cost function:
\begin{equation}
J(\A, \s_\E) = \sum_{n=1}^N \arccos \left( \frac{(\E \diag (\s_\E) \ba_n)^\top \hat{\x}_n}{\|\E \diag (\s_\E) \ba_n)\|_2 \|\hat{\x_n}\|_2 }\right)    
\end{equation}
where the factor $s_{\x_n}$ is canceled because it is a scalar. The arc cosine makes this a highly nonlinear function. By removing the arc cosine, the negative of the argument needs to be minimized (since the derivative of the arc cosine is always negative). Therefore we define the new cost function as:
\begin{equation} \label{eq: angle cost function}
    \Tilde{J}(\A, \s_\E) = - \sum_{n=1}^N \frac{(\E \diag (\s_\E) \ba_n)^\top \hat{\x}_n}{\|\E \diag (\s_\E) \ba_n)\|_2 \|\hat{\x}_n\|_2 }
\end{equation}
and the corresponding optimization problem, which we will call the \textit{angle approach}:
\begin{equation}
    (\mathrm{2LMM}_\mathrm{angle}):~
    \begin{aligned}
            \min_{\A, \s_\E} &~- \sum_{n=1}^N \frac{(\E \diag (\s_\E) \ba_n)^\top \hat{\x}_n}{\|\E \diag (\s_\E) \ba_n)\|_2 \|\hat{\x}_n\|_2 } \\
            \text{s.t.} &~ 0 \leq \A \leq 1,~\mathbf{1}^\top \A = \mathbf{1}  \\
                        &~ \underline{S} \leq \s_\E \leq \overline{S}.
    \end{aligned}
\end{equation}
Conceptually, this makes the optimization problem simpler, since we reduce the parameter space by $N$ dimensions. However, there are some drawbacks, especially from a computational perspective. Since both approaches have variables in both the numerator and denominator, every function evaluation will involve a very expensive division operation. When a division is performed on a computer, it involves an iterative process of subtractions and comparisons, which is notoriously slow \cite[Ch. 3.4]{patterson_computer_2017} and can produce considerable numerical errors. Additionally, this can lead to excessive memory requirements. Therefore, we propose a third approach, which avoids expensive divisions, and is inspired by CLSU.
\subsubsection{Two scaling factor approach}
A third approach combines the matrices $\A$ and $\s_\X$ of the cost function (\ref{eq: 2lmm cost function}) into one matrix $\A_\s$, and drops the ASC. This leads to the optimization problem
\begin{equation} \label{eq: two scaling factor}
  (\mathrm{2LMM}):~ \begin{aligned}
        \min_{\A_\s, \s_\E} &~\|\Hat{\X} - \E \diag(\s_\E)\A_\s\|_F^2 \\
    \text{s.t.} &~ 0 \leq \A_\s \leq \overline{S},  \\
    &~ \underline{S} \leq \s_\E \leq \overline{S}
\end{aligned}
\end{equation}
The actual abundances and pixel scaling factors are easily recovered using the normalization step (\ref{eq: normalization}). We will refer to this approach by the model name, 2LMM, or by calling it the \emph{two scaling factor approach}.

\subsection{Optimization algorithm}
To perform spectral unmixing using the 2LMM, we will use an interior-point (IP) method. Interior-point methods can solve general nonlinear constrained minimization problems \cite{geletu_introduction_nodate}. 

\subsubsection{Concept of IP methods}
In what follows we give a high-level overview of the IP method. Formal assumptions and discussions which are not essential to understanding the concept of the algorithm are deferred to the appendices. Consider the general problem:
\begin{equation} \label{eq: interior point problem}
   \begin{aligned}
    \min_{\x \in \real^D} &~f(\x) \\ \mathrm{s.t.}&~g_i(\x) \geq 0,  &i=1,2,\ldots, I \\ &~\mathbf{C}\x = \mathbf{b}\\ &~\x\geq0
    \end{aligned} 
\end{equation}
where $f$ need not be convex. Furthermore, define the feasible set $\mathcal{X}$ as the set of all points that satisfy the constraints:
\[
\mathcal{X} = \left\{ \x \in \real^D \Bigg| \begin{array}{c}
     g_i(\x) \geq 0,~i=1,2,\ldots, I \\ \mathbf{C}\x = \mathbf{b}\\ \x \geq 0
\end{array}\right\}.
\]
 The main idea of the IP method is to replace  the inequality constraints in the cost function with penalty terms  that approach infinity as the argument approaches the boundary of the feasible set, and that are very small when the argument falls within the feasible set. For the IP algorithm to work, we require the feasible set $\mathcal{X}$ to be \textit{large enough} (see App. A for a formal assumption), so that it allows the definition of a \textbf{barrier function}, in this case a log barrier function:
\[
B(\x, \mu) = f(\x) - \mu \left( \sum_{i=1}^I \log(g_i(\x)) + \sum_{d = 1}^D \log(x_d)\right)
\]
A {barrier function} is a function that approaches $f(\x)$ as $\mu$ decreases but still approaches $+\infty$ as $\x$ approaches the boundary of the feasible set (see Fig. \ref{fig:barrier fct}). Instead of the original problem (\ref{eq: interior point problem}), one can now consider the problem:
\begin{equation} \label{eq: barrier problem}
\begin{aligned}
    \min_{\x \in \real^D} &~B(\x, \mu) \\
    \mathrm{s.t.}&~\mathbf{C}\x = \mathbf{b}.
\end{aligned}
\end{equation}

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{Figs/barrier_fct.jpg}
    \caption{A barrier function for $f(x) = 2(x - 0.7)^2$ and $\mathcal{X} = [0, 1]$. As the value of $\mu$ decreases, the barrier function approaches $f(x)$ while still approaching infinity at the boundaries.}
    \label{fig:barrier fct}
\end{figure}

The interior-point method is an extension to the barrier method, where the problem (\ref{eq: barrier problem}) is solved several times using Newton's method for a decreasing sequence of nonnegative numbers $\{\mu_k\}_{k \geq 0}$. This way, we obtain a sequence $\{(\x_{\mu_k}, \bm{\lambda}_{\mu_k})\}_{k \geq 0}$ of optimal solutions, where $\bm{\lambda}_k$ is the dual variable or Lagrange multiplier (see App. B). This sequence is called the \emph{primal-dual path}. Under mild assumptions, the primal-dual path converges to a locally optimal solution of the problem (\ref{eq: interior point problem}) (see App. C for a more elaborate convergence discussion). The full conceptual algorithm is shown in Algorithm \ref{alg: interior point}. For more information on interior-point and related methods, see \cite[Ch. 11]{boyd_convex_2004}.

\begin{algorithm}[t]
\caption{Interior-point algorithm}\label{alg: interior point}
\begin{algorithmic}
\FOR {$k = 0, 1, 2, \ldots$}
\STATE {Construct the barrier function $B(\x, \mu_k)$}
\STATE {Solve the problem (\ref{eq: barrier problem}) using, e.g., Newton's method}
\STATE {Call the solutions $(\x_{\mu_k}, \bm{\lambda}_{\mu_k})$}
\IF {some termination criterion is met}
\STATE{Terminate and return $\x^\star = \x_{\mu_k}$}
\ENDIF
\ENDFOR
\end{algorithmic}
\end{algorithm}



\subsubsection{The IP method for 2LMM}
Consider the two scaling factor approach (\ref{eq: two scaling factor}), with the cost function
\[
J(\A_\s, \s_\E) = \|\hat{\X} - \E \diag(\s_\E)\A_\s\|_F^2
\]
and the constraints $0 \leq \A_\s \leq \overline{S}$ and $\underline{S} \leq \s_\E \leq \overline{S}$. There are no equality constraints. The inequality constraints can be written as:
\begin{equation}
    \begin{aligned}
    s_{\e_k} - \underline{S} & \geq 0, \quad \overline{S} - s_{\e_k} \geq 0, & \quad \forall k  \\
    a_{nk} & \geq 0, \quad  \overline{S} - a_{nk} \geq 0, & \quad \forall n, k
    \end{aligned}
\end{equation}
leading to the barrier function
\begin{multline*}
    B(\A_\s, \s_\E, \mu) = J(\A_\s, \s_\E) -   \\
    \mu \Big(\sum_{k=1}^K (\log (s_{\e_k} - \underline{S}) + \log (\overline{S} - s_{\e_k})) + \\
    \sum_{k=1}^K \sum_{n=1}^N (\log (\overline{S} - a_{nk}) + \log (a_{nk}))\Big).
\end{multline*}
We have omitted the terms $\log (s_{\e_k})$ since they are redundant. The barrier function for the other approaches is similar, and given by
\begin{multline*}
    \Tilde{B}(\A, \s_\E, \mu) = \Tilde{J}(\A, \s_\E) -   \\
    \mu \Big(\sum_{k=1}^K (\log (s_{\e_k} - \underline{S}) + \log (\overline{S} - s_{\e_k})) + \\
    \sum_{k=1}^K \sum_{n=1}^N (\log (1 - a_{nk}) + \log (a_{nk}))\Big)
\end{multline*}
with $\Tilde{J}(\A, \s_\E)$ either the norm division cost function (\ref{eq: norm division cost function}) or the angle cost function (\ref{eq: angle cost function}). To improve the numerical behavior of the algorithm, slack variables are often introduced in the barrier function (see App. D for the modified cost function in case of the two scaling factor approach). 

\subsubsection{Implementation}
Several general-purpose software implementations of the interior-point algorithm exist. For the two scaling factor approach 2LMM, we use the Ipopt solver \cite{wachter_line_2005} interfaced through the Julia programming language. For the norm division approach $\mathrm{2LMM}_\mathrm{norm}$ and the angle approach $\mathrm{2LMM}_\mathrm{angle}$, we use \textsc{Matlab}'s interior-point solver, implemented in the $\texttt{lsqnonlin}$ and $\texttt{fmincon}$ functions from the \textsc{Matlab} Optimization Toolbox. All experiments were run on a desktop computer with a 32-core Intel i9 CPU with a 3-level cache and 64 GiB RAM (DIMM).