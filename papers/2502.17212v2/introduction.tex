\IEEEPARstart{H}{yperspectral} imaging  (HSI) has been widely used as an alternative to high-spatial-resolution RGB images in remote sensing for the detection of, e.g., terrestrial features and ground cover classification \cite{shaw_spectral_2003}. As HSI typically lacks sufficient spatial resolution, an important task is \textbf{spectral unmixing} (SU), i.e., the subpixel  estimation of the coverages of pure materials. A major challenge during SU is the fact that the signatures of the pure materials vary throughout the image, due to various causes. As a result, researchers have focused much attention on developing unmixing methods that can mitigate this effect.

\subsection{Causes of hyperspectral variability}

The causes of hyperspectral variability can be grouped into four categories \cite{borsoi_spectral_2021}. The first is \emph{atmospheric effects}, due to the absorption of light by gases, especially water vapor. The second is \emph{intrinsic variability} in endmembers, both in space and in time. As an example, consider the leafs of trees or plants. The pigment concentration, mesophyll structure and water content all affect the way in which light is reflected in a highly nonlinear way \cite{jacquemoud_prospect_1990, karabourniotis_optical_2021}. Therefore, the leaves of single tree can all have widely different spectral signatures. Moreover, their spectral signatures will depend on the season, and even on the time of the day. Next, \emph{illumination effects} can also cause spectral variability. If certain areas are illuminated more prominently than others, the signatures of endmembers will be different. This effect can also be wavelength-dependent, e.g., when the illumination mostly comes from scattered radiation, in which case the short wavelengths have more strength. Finally, the slope of the terrain and the \emph{varying topography} also impact the spectral reflectance. This is because the intensity of the reflected light depends on the incident and reflectance angles, and is also wavelength- and material-dependent.

\subsection{Unmixing under hyperspectral variability}

The linear mixing model (LMM) serves as the basis for many SU models and techniques. The basic assumption of the LMM is that the surface within a given image is covered by a low number of distinct pure materials that have relatively constant spectral signatures. These distinct materials are called the \textit{endmembers} (EMs) \cite{keshava_spectral_2002}. The LMM assumes that the reflected spectrum in each pixel can be described as a linear combination of the pure materials.  The relative area covered by any given EM is known as its \textit{fractional abundance}. 
%If most of the spectral variability in the scene can be explained by EMs appearing in different concentrations, then it follows that the reflected spectrum in each pixel can be described as a linear combination of the pure materials. This model is called the \textbf{linear mixing model} (LMM).
Performing unmixing using the LMM is usually done by minimizing the Euclidean distance between the measured spectrum and a reconstructed spectrum. This leads to a convex minimization problem, which is separable per pixel. As a consequence, the problem can be solved efficiently and in a parallelized way.

However, the LMM is not suited for performing unmixing when hyperspectral variability is present. It has a significant model mismatch, which leads to poor results. As a result, researchers have developed a wide variety of methods to mitigate the effects of hyperspectral variability \cite{borsoi_spectral_2021}. In these methods, EM signatures either originate from a spectral library, or they can be obtained from the image using an Endmember Extraction Algorithm (EEA). Designing robust and reliable EEAs is an interesting and active field of research in its own right. For an overview, see \cite{kale_hyperspectral_2019}. The most important SU methods are summarized below.

\subsubsection{Methods that use spectral libraries}
These methods assume the availability of a spectral library, which is an overcomplete collection of multiple signatures for each EM that represent the possible variability for that EM.
The simplest method that uses spectral libraries is Multiple Endmember Spectral Mixture Analysis (MESMA) \cite{roberts_mapping_1998}. MESMA assumes an LMM in each pixel, and for every pixel it finds the best fitting EM signatures using least-squares. This is a combinatorial problem, so it can quickly become prohibitively large. Several variants exist, but they are only effective for small spectral libraries. As a way to reduce the computational burden of MESMA, sparse unmixing was proposed. Only one least-squares problem per pixel is solved, while enforcing abundance sparsity to limit the number of signatures that are actually contributing to the pixel. A well-known method for sparse unmixing is the Sparse Unmixing by variable Splitting and Augmented Lagrangian (SUnSAL) \cite{bioucas-dias_alternating_2010}, of which several variants exist \cite{iordache_total_2012}.


\subsubsection{Parametric physics-based models}
Physics-based models explicitly model the physical interaction of light with a material, described by a (relatively low) number of parameters. The SAIL model \cite{verhoef_light_1984}, the PROSPECT model \cite{jacquemoud_prospect_1990} and the combined PROSAIL model \cite{jacquemoud_prospect_2009}  model leaf optical properties based on a limited amount of biophysical parameters such as the refractive index, the leaf mesophyll structure and the spectral absorption coefficients. The community standard for modeling topographic effects is Hapke's model \cite{hapke_theory_2012, heylen_review_2014}. Hapke's model uses the Single Scattering Albedo (SSA) of a material, in combination with incident and reflected angles, to determine the reflectance spectrum of a material. All of the methods above tend to be very complicated and very difficult, if not impossible, to invert. In the context of SU, this leads to extremely challenging and ill-posed optimization problems. 

\subsubsection{Parametric physically motivated models}
Since full physics-based models are too complicated, simpler models, which are not physics-based, but physically motivated, are often used \cite{borsoi_spectral_2021}. Starting from the LMM, several consequent generalizations have been proposed. 
Some approaches model the variability as a scaling of the EM signatures, which is especially suited for illumination- and topography-induced variability. The Scaled LMM (SLMM) includes a pixel-wise scaling factor. This simple model leads to a convex optimization problem. The Extended LMM (ELMM) is a generalization of the SLMM, and it includes a scaling factor for every EM in every pixel \cite{drumetz_blind_2016}. Next, the Generalized LMM (GLMM) incorporates wavelength-specific effects, and includes additional scaling factors for each spectral band \cite{imbiriba_generalized_2018}. 
The variability can also be modeled as an additive perturbation, accounting for intrinsic variability, and inter-class variability, i.e., variability due to the presence of unknown materials. The Perturbed LMM (PLMM) includes a pixel-wise additive perturbation on the EMs \cite{thouvenin_hyperspectral_2016}. The Augmented LMM (ALMM) combines the pixel-wise scaling factor from the SLMM with the additive perturbation of the PLMM \cite{hong_augmented_2019}.
Most of these models are non-convex and are solved using an Alternating Direction Method of Multipliers (ADMM). The ADMM is a distributed optimization algorithm that arose due to a need for efficient distributed optimization algorithms for processing large datasets \cite{boyd_distributed_2011}.

\subsubsection{Models jointly estimating EMs and abundances}
The performance of the models above depends heavily on the quality of the reference EMs. When they are provided a priori, we have no control over their quality. If they are obtained from the image, many EEAs rely on the pure pixel assumption, i.e., there is at least one pure pixel for each EM in the image. This assumption is often not met, making the extracted EMs unreliable. Because of this, it has been proposed to jointly estimate the EMs and the abundances, also called \textit{blind unmixing}. This allows the final EM estimates to vary from the initial estimates, which can help account for intrinsic variability and it can mitigate the effect of the absence of pure pixels. Performing blind unmixing is usually done using constrained nonnegative matrix factorization (NMF) \cite{lee_learning_1999}, where the image matrix is factored into a product of two or more nonnegative matrices, while conforming to, e.g., sparsity or smoothness constraints. In \cite{drumetz_spectral_2020}, an NMF-inspired cost function is used for optimization on oblique manifolds. Starting from some initial EM guess, a \textit{maximum a posteriori} (MAP) estimator is derived which allows the final EMs to vary from the initial reference EMs. The authors in \cite{song_weighted_2022} use an ADMM-inspired algorithm for NMF with weighted total variation regularization. Other approaches using TV regularization include \cite{qin_blind_2021, he_total_2017}. In \cite{yuan_projection-based_2015} a projection-based NMF algorithm is used to select relevant EM signatures and to promote sparsity. Lastly, many works make use of norm regularization (e.g., \cite{qian_hyperspectral_2011, sigurdsson_hyperspectral_2014, sun_blind_2022}).

%\subsubsection{Statistical methods}
%Statistical or Bayesian methods assume that the EMs are random vectors, following some distribution parametrized by the reference EMs and possibly some additional parameters. The most common choice for the distribution is a multivariate normal distribution, and the model is known as the \textit{normal compositional model} (NCM). It assumes a random vector with some variance around a reference EM. The problem is then solved using a stochastic expectation-maximization (SEM) approach \cite{stein_application_2003}. Due to the use of the normal distributions, the SEM can be solved efficiently with well-established algorithms \cite{neal_view_1998}. Additional quantities can be enforced, such as sparsity or smoothness, by slightly altering the method. Next to the normal distribution, it is also possible to consider other distributions, such as the beta distribution (leading to the beta compositional model, BCM \cite{zare_spectral_2013}).

\subsubsection{Model-free methods}
Model-free methods do not assume any underlying model on the spectral variability. As a consequence, they make few assumptions on the endmember models, but they can lead to problems of high non-convexity and high complexity. The main approach for model-free methods is to use another distance metric than the Euclidean distance, as the Euclidean distance is very sensitive to scaling. Such distance metrics include the Spectral Angle Distance (SAD) and the Spectral Correlation Measure (SCM) \cite{chen_generalization_2009}. Due to the high non-convexity, it is difficult to use these metrics in unaltered from. Therefore, the authors in \cite{kizel_stepwise_2017} derived a simplified SAD-inspired cost function and performed SU using a semi-analytic gradient projection algorithm.


\subsubsection{Machine Learning methods}
Like in many fields of HSI, machine learning (ML) methods have been used to perform SU in the presence of variability. In recent years, deep learning methods have become very popular. Most available techniques are based on autoencoder (AE) networks, which can automatically learn low-dimensional embeddings and reconstruct the original data.
In \cite{chen_dsfc-ae_2024}, a new deep shared fully connected autoencoder (DSFC-AE) unmixing network was developed. 
Authors in \cite{shi_deep_2022} presented a variational AE-based model for spatial-spectral unmixing with EM variability, by linking the generated EMs to the probability distributions of endmember bundles extracted from the hyperspectral image, and used adversarial learning to learn realistic EMs. 
In \cite{borsoi_dynamical_2023}, a recurrent neural network (RNN) based model is designed to handle both spatial and temporal variability, inspired by the Generalized LMM. Another work that was based on a specific physical model is \cite{cheng_hyperspectral_2023}. The authours designed a scaled-and-perturbed LMM (SPLMM) and used it in combination with a multi-stream feed-forward neural network. Authors in \cite{gao_proportional_2024} used a similar physical model, called the Proportional Perturbation Model (PPM).
In \cite{zhang_spectral_2022}, a two-stream convolutional encoder-decoder network, one for learning the abundances and one for learning variability coefficients, was used to explicitly account for variability. The attention mechanism, originally used in encoder-decoder architectures for natural language processing, was used in \cite{su_multi-attention_2023} for unmixing under hyperspectral variability to discover global spatial features, and to exploit redundancy in the spectral bands. 
In \cite{gao_reversible_2024} a reversible generative network was developed to make the EM learning process more stable. The authors combined a CNN-based abundance estimation module with a reversible EM learning module. 

ML methods often lack interpretability and explainability, and their solutions are not physically motivated, even though some efforts have been made to design (partially) interpretable networks. The authors in \cite{hong_endmember-guided_2022} and \cite{lyngdoh_hyperspectral_2022} used a two-stream Siamese deep network, with an EM network to explicitly model variable EM signatures and provide more interpretable unmixing solutions. Similarly, \cite{zheng_blind_2024} used a physics-driven model to perform joint unmixing with a two-stream AE network, consisting of an EM extractor and an abundance estimator. Despite these efforts, most ML approaches remain a black box. A second drawback is their high training cost.

\subsection{Contribution}

Most of the developed models accounting for EM variability lead to ill-posed and highly non-convex optimization problems which are hard to solve, and which often have hyperparameters that are difficult to tune. In this paper, we propose a parametric physically motivated model, which is a two-step LMM that bridges the gap between model complexity and computational tractability. We show that this model leads to a mildly non-convex optimization problem, solvable with an interior-point solver. This method requires virtually no hyperparameter tuning, and can therefore be easily and quickly used in a wide range of unmixing tasks. 

\subsection{Outline}
The remaining of this article is structured as follows: in the next section, the related models are described. Section III is devoted to the description of the proposed model, along with the proposed  optimization procedure. Experiments are conducted on synthetic data in section IV, and  on real data in section V. Section VI concludes the work.