\section{Implementation \& Evaluation}
\label{sec:evaluation}
This section outlines the implementation details and evaluates the performance of both the MIDR and MFSP predictors.

\subsection{MIDR Predictor Evaluation}
\label{subsec:mdrp_eval}

\subsubsection{Implementation details}
To explore the impact of model capacity on performance, we implemented two versions of the MIDR predictor: the {\em{small predictor}} and the {\em{large predictor}}. Both models share an identical network architecture but differ in the dimensions of the embedding vectors used for graph nodes and edges. The small and large predictors employ embedding dimensions of $32$ and $64$, respectively. These variations allow for evaluating trade-offs between model complexity, computational efficiency, and predictive accuracy. The detailed configurations of each network component, all utilizing ReLU as the activation function in their MLP layers, are outlined below:
\begin{itemize}
    \item {\bf{Initial encoders for nodes and edges}}: Two distinct single-hidden-layer MLPs serve as encoders for nodes and edges, each tailored to its respective input features. The dimensions of their hidden layers are the same as the output dimension. Consequently, for the small predictor, with both node and edge embedding dimensions set to $32$, the node encoder layers are configured as $[13, 32, 32]$ for the input features, hidden layer, and output layer, respectively. Similarly, the edge encoder layers are $[9, 32, 32]$. Refer to \secref{subsubsec:input_attributes} for the input attribute counts. For the large predictor, the embedding dimensions are increased to $64$, and the layer configurations are adjusted accordingly. This design ensures the encoders effectively process distinct feature sets for nodes and edges while seamlessly integrating with the GNN architecture.
    \item {\bf{Message function}}, $\phi^k(\cdot)$: A single-hidden-layer MLP is used for the message function. Its input is the concatenation of the embeddings of the source node and the edge. The output dimension matches the node embedding dimension. The hidden layer dimension for the small and large predictors is $64$ and $128$, respectively.
    \item {\bf{Aggregation operation}}, $\bigoplus$: The ``max'' function is used as the aggregation operation. It selects the maximum value from the neighboring node messages. This approach (compared to other aggregation methods through averaging or summation) aligns with a conservative engineering design philosophy, prioritizing the most critical structural information for robust analysis and decision-making.
    \item {\bf{Update function}}, $\gamma^k(\cdot)$: It combines the previous node embedding with the processed aggregated message. Specifically, the aggregated message for node $i$, $\vec{\tilde{v}}_{i}^k$, is processed through a single-hidden-layer MLP, $\tilde{\gamma}^k(\cdot)$, whose output dimension matches the node embedding dimension. The update rule is as follows: 
    \begin{equation}
        \gamma^k\left(\vec{v}_{i}^{k-1}, \vec{\tilde{v}}_{i}^k\right) = \vec{v}_{i}^{k-1} + \tilde{\gamma}^k\left(\vec{\tilde{v}}_{i}^k\right).
    \end{equation}
    The hidden layer dimension of $\tilde{\gamma}^k(\cdot)$ for the small and large predictors is $64$ and $128$, respectively.
    \item {\bf{EU function}}, $\zeta\left(\cdot\right)$: A single-hidden-layer MLP is used for updating edge attributes. The input is the concatenation of the two adjacent nodes. Thus, the input dimension is twice the node embedding dimension. The output dimension corresponds to the edge embedding dimension. The hidden layer dimension for the small and large predictors is $32$ and $64$, respectively.
    \item {\bf{Task head-1, task head-2 \& pooling function}}: Task heads are implemented as linear layers. The pooling function combines node embedding into a graph embedding by concatenating the mean and max pooling results. Consequently, the input dimension for task head-1 is twice the node embedding dimension, while for task head-2, it equals the node embedding dimension. Recall that task head-1 and task head-2 predict the IDR as a node-level outcome and MIDR as a structure-level outcome, respectively, i.e., a scalar value is output as the final prediction for each head.
\end{itemize}

The number of layers in the GNN is aligned with the number of stories in the building structure. As the maximum number of stories of the considered buildings in this study is 7 (refer to \secref{subsec:geometry_generation}), we set the GNN to possess 7 layers. Note that to process a $k$-story structure, only first $k$ layers of the entire GNN is used (refer to \secref{subsubsec:handle_structural_data}). For the small predictor configuration, the total parameter count is approximately $7.1 \times 10^4$, while for the large predictor configuration, it reaches about $2.8 \times 10^5$. In the absence of the EU functionality, the parameter counts are marginally reduced to approximately $6.8\times 10^4$ for the small predictor and $2.7 \times 10^5$ for the large predictor.

\subsubsection{Evaluation metrics}
We evaluate the performance of the MIDR predictor using three metrics: Mean Squared Error (MSE), Mean Absolute Error (MAE), and Spearman's rank correlation coefficient. \revise{MSE and MAE are respectively defined as follows:
\begin{equation}
    \text{MSE} = \frac{1}{\sum_{m=1}^{M}N_{m}} \sum_{m=1}^{M} \sum_{n=1}^{N_{m}} \left( \widehat{d}_{m,n, \max} - d_{m,n, \max}^{\, \gt} \right)^2,
\end{equation}
\begin{equation}
    \text{MAE} = \frac{1}{\sum_{m=1}^{M}N_{m}} \sum_{m=1}^{M} \sum_{n=1}^{N_{m}} \left| \widehat{d}_{m,n, \max} - d_{m,n, \max}^{\, \gt} \right|,
\end{equation}
}
where $d_{m,n, \max}^{\, \gt}$ is the ground truth MIDR of structure $m$ under fire scenario $n$, obtained from OpenSeesRT, and $\widehat{d}_{m,n, \max}$ is the corresponding prediction from the MIDR predictor.

\revise{
Since our ultimate goal is to train an argmaxer to identify the MFSP, rather than merely predicting the MIDR values, accurately assessing the relative ranking of MIDRs across different fire scenarios for each structure is critical. MSE and MAE can be good metrics for characterizing the closeness between the predicted MIDRs and the ground truth MIDRs. However, they do not adequately capture the ranking performance. Therefore, we introduce the Spearman's rank correlation coefficient, $\rho_s$, which measures the {\em{monotonic relationship}} between two random variables. Formally, let $X$ and $Y$ be two random variables, and let $R_{x}$ and $R_{Y}$ denote their corresponding rank-transformed values based on the observed samples. The Spearman’s rank correlation coefficient is defined as:}
\begin{equation}
    \rho_s\left(X,Y\right) \triangleq \rho\left( R_{X}, R_{Y} \right) = \frac{\text{cov}\left( R_{X}, R_{Y} \right)}{\sigma_{R_{X}}\sigma_{R_{Y}}},
    \label{eq:Spearman}
\end{equation}
\revise{
where $\rho(\cdot,\cdot)$ is the Pearson correlation coefficient, $\text{cov}\left(R_{X}, R_{Y}\right)$ is the covariance between the ranked values, and $\sigma_{R_{X}}$ \& $\sigma_{R_{Y}}$ denote their respective standard deviations. Unlike Pearson's correlation, which measures linear relationships, Spearman’s correlation assesses how well the orderings of the data match, making it especially suitable for ordinal data or for relationships that are monotonic but not necessarily linear. The range of $\rho_s$ is $[-1, +1]$, with values close to $+1$, $0$, and $-1$ respectively indicating strong positive, weak or no, and strong negative monotonic associations.
}

\revise{
In this paper, for structure $m$ with $N_m$ fire scenarios, we treat the ground truth and predicted MIDRs as realizations of underlying random variables. Specifically, we define the ground truth sample as $X\equiv\left\{d_{m, 1, \max}^{\, \gt}, d_{m, 2, \max}^{\, \gt}, \ldots, d_{m, N_m, \max}^{\, \gt}\right\}$, obtained from OpenSeesRT simulations and the prediction samples as $Y\equiv\left\{\widehat{d}_{m, 1, \max}, \widehat{d}_{m, 2, \max}, \ldots, \widehat{d}_{m, N_m, \max}\right\}$, produced by the MIDR predictor.}
The final evaluation metric is the average $\rho_s$ across all structures, providing a robust measure of the prediction quality for the MFSP identification.

\subsubsection{Evaluation results}
For training and evaluating the MIDR predictor, we exclusively use the labeled dataset, splitting it into $80\%$ for training and $20\%$ for testing. Various GNN configurations and training methods are compared, with the results summarized in \tabref{tab:mdrp_eval}.
\begin{table}[h!]
    % comments for tracking the NN model and results
    \centering
    \caption{Evaluation results of the MIDR predictor (numbers in boldface indicate best results).}
    \begin{tabular}{lcccc}
        \toprule
        Method & Network & MAE & MSE & $\rho_s$  \\
        \midrule
        Strawman 1 & \multirow{5}{*}{Small} & $1.017$ & $1.707$ & $0.162$ \\  % 20241223134702spZMgx
        Strawman 2: No EU &  & $0.320$ & $0.232$ & $0.670$ \\  % 20241223125955Tscgpd
        Strawman 2: EU &  & $0.314$ & $0.203$ & $0.689$ \\  % 20241223125824kHCDYl
        Proposed: No EU &  & $0.300$ & $0.201$ & $0.685$ \\  % 20241223140810npSUcn
        \textbf{Proposed: EU} &  & $\mathbf{0.278}$ & $\mathbf{0.170}$ & $\mathbf{0.725}$ \\  % 20241223135224iwoQMK
        \midrule
        Strawman 1 & \multirow{5}{*}{Large} & $0.305$ & $0.202$ & $0.649$ \\ % 20241223205519gIBaaG
        Strawman 2: No EU &  & $0.305$ & $0.201$ & $0.678$ \\ % 20241222203631GWJOab
        Strawman 2: EU &  & $0.317$ & $0.216$ & $0.704$ \\ % 20241222200010mwIyfh
        Proposed: No EU &  & $0.289$ & $0.175$ & $0.707$ \\ % 20241223150855HDWgFJ
        \textbf{Proposed: EU} &  & $\mathbf{0.272}$ & $\mathbf{0.169}$ & $\mathbf{0.742}$ \\ % 20241223151005bPJuxs
        \bottomrule
    \end{tabular}
    \label{tab:mdrp_eval}
\end{table}

\paragraph{Overall accuracy performance analysis}
In \tabref{tab:mdrp_eval}, the methods Strawman 1, Strawman 2, and Proposed are described in \secref{subsec:transfer_learning} and illustrated in \figref{fig:transfer_learning}. For Strawman 2 and Proposed methods, we further assess the impact of enabling the EU functionality. Two key observations are summarized below:
\begin{itemize}
    \item {\bf{EU functionality}}: Comparisons between cases with and without EU reveal that enabling EU consistently enhances model performance. Specifically, the inclusion of EU increases $\rho_s$ in all cases, and reduces both MAE and MSE for not only the proposed predictors but also the Strawman 2 predictor based on small network.
    \item {\bf{Proposed method}}: Among all configurations, the proposed method with EU achieves the best performance, with $\rho_s$ scores of $0.725$ and $0.742$ for the small and large networks, respectively. This indicates that the proposed method better captures the structural sensitivity ranking across various fire scenarios when predicting the MIDR.
\end{itemize}

\figref{fig:midrp_cdf_rho_s_and_examples}(a) presents the Complementary Cumulative Distribution Function (CCDF) of $\rho_s$, i.e., ${\overline{{\cal{F}}}}_{\rho_s}(a)={\cal{P}}(\rho_s \ge a)$, where $\cal{P}$ indicates the probability, for the proposed method with EU. Key observations include:
\begin{itemize}
\item For the small and large predictors (solid lines), $83.2\%$ of structures achieve a $\rho_s > 0.50$, and $64.4\%$ (small predictor) and $66.7\%$ (large predictor) of structures achieve a $\rho_s > 0.75$. Additionally, the large predictor improves on the worst case, where the smallest $\rho_s$ increases from $-0.86$ for the small predictor to $-0.58$ for the large predictor.
\item MIDR predictors perform much better on severe cases (dashed lines), defined for a structure when the ground truth MIDR exceeds $2\%$ under at least one fire case, in comparison with all cases (solid lines). In severe cases, for the small predictor, $98.0\%$ of structures achieve a $\rho_s > 0.50$, and $89.3\%$ exceed $0.75$ with an average of $0.90$. For the large predictor, $98.7\%$ of structures achieve a $\rho_s > 0.50$, while $90.0\%$ exceed $0.75$ with an average of $0.91$.
\end{itemize}
These results indicate that the proposed method performs exceptionally well under severe cases. This is consistent with our expectations. When building structures are less sensitive to fire, errors in MIDR prediction have a relatively small impact. However, the risk of MIDR prediction errors becomes critical when structures are sensitive to fire. 

\begin{figure*}[h!] 
    \centering
    \includegraphics[width=\linewidth]{figures/midrp_cdf_rho_s_and_examples.pdf}
    \caption{Representative MIDR predictor results: (a) CDF of $\rho_s$ of proposed MIDR predictors for two different sizes (severe cases for structures whose ground truth MIDR is larger than $2\%$ in at least one fire case); (b) and (c) show the ground truth and predicted MIDR of 3 example structures (\texttt{ex1}, \texttt{ex2} \& \texttt{ex3}) for the small and large predictors, respectively.}
    \label{fig:midrp_cdf_rho_s_and_examples}
\end{figure*}

\paragraph{Exemplary illustration \& time efficiency}
Here, we present the ground truth MIDR and predicted MIDR for three example structures under the small (\figref{fig:midrp_cdf_rho_s_and_examples}(b)) and the large (\figref{fig:midrp_cdf_rho_s_and_examples}(c)) predictors. In these examples, \texttt{ex1} and \texttt{ex2} represent severe cases, while \texttt{ex3} remains relatively stable with low MIDR across all fire scenarios. From these figures, for both predictors, the predicted results for \texttt{ex1} and \texttt{ex2} closely match the ground truth, achieving $\rho_s > 0.92$. For \texttt{ex3}, the performance of both predictors is lower but the large predictor outperforms the small one, achieving a $\rho_s > 0.70$. \revise{
It is to be noted that the Spearman's rank correlation coefficient does not evaluate the closeness between predicted and ground truth values, but rather evaluates the monotonic relationship. To clarify, we compare \texttt{ex1} with \texttt{ex3} in \figref{fig:midrp_cdf_rho_s_and_examples}(b) where \texttt{ex3} points are closely located alongside the ideal $45^\circ$ line, but has a lower $\rho_s$, because the ranks of the predicted MIDRs do not match those of ground truth MIDRs. Instead, although \texttt{ex1} points are deviating from the ideal line, due to the match of ranks, \texttt{ex1} holds a much higher $\rho_s$. Considering the consequent task of training an argmaxer, the $\rho_s$ is an important metric to evaluate the performance of an MIDR predictor.
}

{\blockRevise
The differentiable agent shows great improvement on the time efficiency compared with traditional FEA. On our experimental machine, with 2 cores of Skylake CPU (2.1 GHz) and one GTX2080TI GPU, the average time consumption for predicting the MIDR for the 3 example structures is listed in \tabref{tab:midrp_time}, where the number of nodes and edges are also listed for reference. Note that the values for OpenSeesRT are the time consumption for just one fire scenario, and the values for the agent are the time consumption for all 30 fire scenarios. We clearly see that the conventional FEA simulation's efficiency is highly affected by the number of nodes and edges, while these numbers have less impact on the agent's time consumption. The most important finding, and as expected, is that the GNN-based agent is much faster than the FEA simulations, with more than 3 orders of magnitude difference, i.e., OpenSeesRT requires even tens of seconds for one fire scenario simulation while the agent requires only several milliseconds (ms). Note that to obtain accurate time consumption of the agent, we run the agent for 1000 times and report their average run time.
\begin{table}[h!]
    \centering
    \caption{\revise{Average time consumption of different methods to obtain MIDR for 3 examples in Figures \ref{fig:midrp_cdf_rho_s_and_examples}(b) \& (c).}}
    \begin{tabular}{l|cc|ccc}
        \toprule
        \multirow{2}{*}{\texttt{ex}} & Node & Edge & OpenSeesRT & \multicolumn{2}{c}{Agent (1 batch of 30 fires)} \\ 
        \cmidrule(lr){5-6}
        & \# & \# & (1 fire) (ms) & Small (ms) & Large (ms) \\
        \midrule
        \texttt{1} & 331 & 728 & 70,950 & 8.63 & 12.64  \\  
        \texttt{2} & 286 & 618 & 48,340 & 8.59 & 11.00  \\  
        \texttt{3} & 59  & 101 & 6,590  & 4.12 & 4.29 \\  
        \bottomrule
    \end{tabular}
    \label{tab:midrp_time}
\end{table}

In conclusion, the proposed method with EU functionality demonstrates exceptional performance across all metrics for both accuracy and time efficiency, particularly for severe cases. These findings highlight the robustness and effectiveness of the method in supporting fire safety assessments for building structures. The high values of $\rho_s$, and the extremely low time costs also demonstrate the reliability and the efficiency of using the MIDR predictor as an agent for pseudo-labeling and as a direct indicator for training the MFSP predictor. 
} % blockRevise

\subsection{MFSP Predictor Evaluation}
\label{subsec:mfsp_eval}
To evaluate the MFSP predictor, we adopt the proposed method with EU as the final model for the MIDR predictor and utilize it to train the MFSP predictor. As described in \secref{subsec:mfspp_transfer_learning}, we consider two training strategies:
\begin{itemize}
    \item {\bf{De novo training}}: Training a completely new network from scratch.
    \item {\bf{TL with GNN}}: Reusing the GNN module, including the initial encoders, from the MIDR predictor. 
\end{itemize}
Additionally, two loss functions are evaluated: 
\begin{itemize}
    \item {\bf{MSE loss}}: Focusing on accuracy of predicted coordinates.
    \item {\bf{Hybrid loss}}: A combination of MSE loss and the MIDR-based loss for better ranking consistency.
\end{itemize}
The network architecture of the MFSP predictor is almost identical to that of the MIDR predictor, consisting of an initial encoder, a GNN, pooling layers, and an MLP task head. There are only a few differences as follows:
\begin{itemize}
    \item {\bf{Final MLP head}}: The task head in the MFSP predictor replaces the linear layer in the MIDR predictor with a single-hidden-layer MLP. The hidden layer's dimension matches the graph embedding (twice the node embedding dimension), and the output layer has a dimension of 3, representing the predicted coordinates $(x, y, z)$. A ``sigmoid'' activation function is applied to normalize the predicted coordinates to the range $[0,1]$, aligned with the building normalized spatial dimensions.
    \item {\bf{De Novo training}}: The initial node input features include only the spatial coordinates of the nodes. Fire point information is excluded, as no prior knowledge from the MIDR predictor is utilized in this case.
    \item {\bf{TL with GNN training}}: The GNN module and initial encoders from the MIDR predictor are directly reused. Moreover, the fire point information is replaced with a VFP to ensure consistency in the input structure. This enables the network to generalize its understanding of the structural information without relying on specific fire locations.
\end{itemize}
By evaluating these configurations and loss functions, the MFSP predictor aims to efficiently and accurately predict the MFSP within the building.

\subsubsection{Evaluation metrics}
\label{sec:7.2.1}
\revise{
In \secref{subsec:mfspp_pseudo_label}, we introduce the ``pseudo-labeling'' method, which generates pseudo ground truth coordinates for the MFSP using the well-trained MIDR predictor. These pseudo-labels are used to evaluate the MFSP predictor through various metrics, capturing both point-level and room-level performances. Specifically, they are generated by selecting the fire point with the highest predicted MIDR for each structure, and used to train and evaluate the MFSP predictor. This approach allows leveraging the unlabeled data effectively, as the MIDR predictor provides a computationally efficient and accurate surrogate for the FEA simulations.
}

For a given structure $m$, the distance error between predicted MFSP and pseudo ground truth MFSP is defined as: 
\begin{equation}
    e_{m} = \sqrt{\left( x_{m}^{\, \pgt} - \widehat{x}_{m} \right)^2 + \left( y_{m}^{\, \pgt} - \widehat{y}_{m} \right)^2 + \left( z_{m}^{\, \pgt} - \widehat{z}_{m} \right)^2},
\end{equation}
where $\widehat{x}_{m}$, $\widehat{y}_{m}$, $\widehat{z}_{m}$ represent the $x$, $y$, $z$ coordinates predicted by the MFSP predictor, while $x_{m}^{\, \pgt}$, $y_{m}^{\, \pgt}$, $z_{m}^{\, \pgt}$ denote the pseudo ground truth MFSP coordinates obtained using the MIDR predictor. This metric provides a direct measure of the prediction accuracy at the point level. However, as the pseudo ground truth is defined at a room-level granularity, its reliability may vary depending on the building's size and complexity. For practical evaluation, two room-level metrics are introduced, namely, room distance error and room rank. These metrics treat rooms as the basic unit of measurements, reflecting how well the MFSP predictor identifies the Most Fire-Sensitive Room (MFSR), i.e., the room where the MFSP is located.

{\bf{Room distance error}} measures the distance between the predicted MFSR and the pseudo ground truth MFSR, assuming all edges (beams and columns) of the building structure have unit length. For structure $m$, let the room dimensions be $\tilde{x}_{m}$, $\tilde{y}_{m}$, $\tilde{z}_{m}$ for the respective length, width, and height. The coordinates of the center point of the predicted MFSR are denoted as $\left( \widehat{x}_{m}', \widehat{y}_{m}', \widehat{z}_{m}'\right)$. The room distance is then computed as:
\begin{equation}
    \tilde{e}_{m} = \sqrt{\left( \frac{x_{m}^{\, \pgt} - \widehat{x}_{m}'}{\tilde{x}_{m}} \right)^2 + \left( \frac{y_{m}^{\, \pgt} - \widehat{y}_{m}'}{\tilde{y}_{m}} \right)^2 + \left( \frac{z_{m}^{\, \pgt} - \widehat{z}_{m}'}{\tilde{z}_{m}} \right)^2}.
\end{equation}
This metric provides a scaled distance measure, accounting for the room size and structure dimensions.

{\bf{Room rank}} evaluates the ranking of the room containing the predicted MFSP based on the pseudo-labeled MIDR values. \revise{For structure $m$ with $N_{m}$ pseudo fire cases, we sort the pseudo-labeled room-center MIDRs in a descending order, and index the rooms by the fire case occurring at each room's center, i.e., re-number the rooms to satisfy $d_{m,1, \max}^{\, \pgt} > d_{m,2, \max}^{\, \pgt} > \cdots > d_{m,N_{m}, \max}^{\, \pgt}$}. If the predicted MFSP falls into the $l$-th ranked room, the room rank is accordingly defined as $r_{m} = l$. Lower ranks indicate better prediction performance, as the predicted MFSR with such low rank is closer to the MFSP.
% For example, if there are 4 rooms, originally numbered as room 1,2,3,4.
% In pseudo labeling process, we get four fire cases, corresponding to the fire happening at the center of each room. So we have $d_{m,i,\max}^{\pgt}, i\in \{1,2,3,4\}$. But note that here it is not sorted.
% Then, we sort the pseudo MIDRs in descending order, and may have 
% $d_{m,3,\max}^{\pgt} > d_{m, 4,\max}^{\pgt} > d_{m, 1,\max}^{\pgt} > d_{m, 2,\max}^{\pgt}$. But for ease of refering, we just RE-NUMBER the rooms, to have $d_{m,1,\max}^{\pgt} > d_{m, 2,\max}^{\pgt} > d_{m, 3,\max}^{\pgt} > d_{m, 4,\max}^{\pgt}$.
%  In this way, if the predicted MFSR is room 2, it means that the fire happending in the center of this room leads to the 2nd most severe MIDR. So the $r_m = 2$.

In addition to the distance error, room distance error, and room rank, the predicted MIDR at the identified MFSP can also serve as an evaluation metric. Higher MIDR values reflect the predictor's ability to identify the MFSPs, directly correlating with the severity of the structural response. These evaluation metrics collectively assess the MFSP predictor's performance, balancing point-level precision, room-level granularity, and the functional implications of predictions. The inclusion of room distance error and room rank ensures that the evaluation aligns with practical building-level assessments, where rooms are typically used as reference units for structural analysis and fire safety planning.

\subsubsection{Evaluation results}
Using the metrics defined in the previous section, we evaluate the performance of the MFSP predictor by calculating average quantities across all structures. Since the MIDR predictor provides a computationally efficient agent model to rapidly generate pseudo labels for unlabeled data, the large unlabeled dataset can be effectively utilized for training the MFSP predictor. For this evaluation, the unlabeled dataset is divided into $80\%$ for training and $20\%$ for testing. The evaluation results for various training methods and loss functions are summarized in \tabref{tab:mfspp_eval}. For the ``Hybrid loss'', Equation (\ref{eq:hybrid}), the weight for the MSE term is fixed at $w_2 = 1 \, \mathrm{m}^{-2}$, while the weight for the MIDR term is varied as $w_1=10$, $50$, or $100$. In \tabref{tab:mfspp_eval}, the TL with GNN, Hybrid results for $w_1=50$, using both small and large networks, are taken as the reference cases for the listed \revise{multipliers $\mu$ in parentheses, i.e., $(\mu \times)$}.

\begin{table*}[h!]
    % comments for tracking the NN model and results
    \centering
    \caption{Evaluation results of MFSP predictor for different training methods and loss functions. \revise{Numbers in boldface indicate best results; values in parentheses show performance multiples relative to the reference configuration, i.e., TL with GNN, Hybrid ($w_1=50$).}}
    \begin{tabular}{lcccccc}
        \toprule
        Method & Loss & Network  & Avg. distance error (m) & Avg. room distance error & Avg. room rank & Avg. MIDR (\%)  \\
        \midrule
        De novo training  & MSE               & \multirow{8}{*}{Small} & $5.49~(0.93\times)$ & $1.28~(1.08\times)$ & $9.09~(1.14\times)$ & $2.40~(0.98\times)$ \\ % 20250105100017XRaXOW
        De novo training  & Hybrid ($w_1=10$) &                        & $5.65~(0.96\times)$ & $1.25~(1.06\times)$ & $9.11~(1.15\times)$ & $2.42~(0.98\times)$ \\ % 20250105095948pGVIWz
        De novo training  & Hybrid ($w_1=50$) &                        & $6.10~(1.04\times)$ & $1.24~(1.05\times)$ & $8.50~(1.07\times)$ & $2.45~(1.00\times)$ \\ % 20250107110409XOzvBW
        De novo training  & Hybrid ($w_1=100$)&                        & $6.26~(1.06\times)$ & $1.23~(1.04\times)$ & $8.59~(1.08\times)$ & $2.42~(0.99\times)$ \\ % 20250108212357ninGVp
        TL with GNN       & MSE               &                        & $\mathbf{5.18~(0.88\times)}$ & $1.21~(1.03\times)$ & $8.51~(1.07\times)$ & $2.40~(0.98\times)$ \\ % 20250105081538oOtGkV
        TL with GNN       & Hybrid ($w_1=10$) &                        & $5.31~(0.90\times)$ & $\mathbf{1.18~(1.00\times)}$ & $8.19~(1.03\times)$ & $2.43~(0.99\times)$ \\ % 20250105095817VQgMkg
        TL with GNN       & Hybrid ($w_1=50$) &                        & $5.89~(1.00\times)$ & $\mathbf{1.18~(1.00\times)}$ & $\mathbf{7.95~(1.00\times)}$ & $\mathbf{2.46~(1.00\times)}$ \\ % 20250107110409bxJeIQ
        TL with GNN       & Hybrid ($w_1=100$)&                        & $6.16~(1.05\times)$ & $1.21~(1.03\times)$ & $\mathbf{7.95~(1.00\times)}$ & $2.45~(1.00\times)$ \\ % 20250108213456qsaPpE
        \midrule
        De novo training  & MSE               & \multirow{8}{*}{Large} & $5.41~(0.92\times)$ & $1.27~(1.10\times)$ & $8.02~(1.19\times)$ & $2.37~(0.97\times)$ \\ % 20250105085555YXnJEb
        De novo training  & Hybrid ($w_1=10$) &                        & $5.64~(0.96\times)$ & $1.25~(1.09\times)$ & $7.58~(1.13\times)$ & $2.40~(0.98\times)$ \\ % 20250105085442rareqa
        De novo training  & Hybrid ($w_1=50$) &                        & $6.21~(1.06\times)$ & $1.22~(1.06\times)$ & $6.96~(1.03\times)$ & $2.43~(0.99\times)$ \\ % 20250108182703XlsTEb
        De novo training  & Hybrid ($w_1=100$)&                        & $6.46~(1.10\times)$ & $1.23~(1.07\times)$ & $7.27~(1.08\times)$ & $2.42~(0.99\times)$ \\ % 20250108213448BCTRLE
        TL with GNN       & MSE               &                        & $\mathbf{4.93~(0.84\times)}$ & $\mathbf{1.15~(1.00\times)}$ & $7.18~(1.07\times)$ & $2.40~(0.98\times)$ \\ % 20250105085354tvfKkR
        TL with GNN       & Hybrid ($w_1=10$) &                        & $5.21~(0.89\times)$ & $\mathbf{1.15~(1.00\times)}$ & $7.12~(1.06\times)$ & $2.41~(0.98\times)$ \\ % 20250105121813gYQvuj
        TL with GNN       & Hybrid ($w_1=50$) &                        & $5.87~(1.00\times)$ & $\mathbf{1.15~(1.00\times)}$ & $\mathbf{6.73~(1.00\times)}$ & $\mathbf{2.45~(1.00\times)}$ \\ % 20250108182814YyaMPD
        TL with GNN       & Hybrid ($w_1=100$)&                        & $6.23~(1.06\times)$ & $1.18~(1.03\times)$ & $7.20~(1.07\times)$ & $2.43~(0.99\times)$ \\ % 20250108220805QoJsCc
        \bottomrule
    \end{tabular}
    \label{tab:mfspp_eval}
\end{table*}
\begin{figure*}[h!]
    \includegraphics[width=\linewidth]{figures/eval_mfspp_cdf.pdf}
    \caption{CDF of distance error, room distance error, and room rank for the small (a to c) and large (d to f) MFSP predictors.}
    \label{fig:eval_mfspp_cdf} 
\end{figure*}

\begin{table*}[ht]
    \centering
    \caption{Key CDF values (\%) of different metrics to evaluate the MFSP predictor (numbers in boldface indicate best results).}
    \begin{tabular}{lcccccccc}
        \toprule
        Method & Loss & Network  & $e_{m} \leq 5$ m & $e_{m} \leq 10$ m & $\tilde{e}_{m}\leq \sqrt{2}$ & $\tilde{e}_{m}\leq 2$ &  $r_{m} \leq 5$ & $r_{m}\leq 10$ \\
        \midrule
        De novo training  & MSE               & \multirow{4}{*}{Small} & $63.69$ & $83.46$ & $69.95$ & $77.83$ & $63.91$ & $75.24$ \\
        De novo training  & Hybrid ($w_1=50$) &                        & $63.75$ & $82.19$ & $71.10$ & $77.98$ & $66.61$ & $76.83$ \\
        TL with GNN       & MSE               &                        & $65.28$ & $\mathbf{84.34}$ & $71.75$ & $\mathbf{79.66}$ & $66.93$ & $76.67$ \\
        TL with GNN       & Hybrid ($w_1=50$) &                        & $\mathbf{65.40}$ & $83.21$ & $\mathbf{72.38}$ & $79.51$ & $\mathbf{68.98}$ & $\mathbf{78.85}$ \\
        \midrule
        De novo training  & MSE               & \multirow{4}{*}{Large} & $66.74$ & $82.16$ & $70.88$ & $78.26$ & $68.58$ & $78.42$ \\
        De novo training  & Hybrid ($w_1=50$) &                        & $66.83$ & $81.63$ & $73.00$ & $79.29$ & $71.57$ & $80.50$ \\
        TL with GNN       & MSE               &                        & $\mathbf{69.10}$ & $\mathbf{84.30}$ & $73.47$ & $79.76$ & $72.28$ & $81.47$ \\
        TL with GNN       & Hybrid ($w_1=50$) &                        & $68.10$ & $83.21$ & $\mathbf{73.93}$ & $\mathbf{80.32}$ & $\mathbf{74.06}$ & $\mathbf{83.21}$ \\
        \bottomrule
    \end{tabular}
    \label{tab:mfspp_eval_cdf}
\end{table*}

\paragraph{Average performance}
Comparing the De novo training and TL with GNN methods, the latter outperforms the former in all metrics. For both small and large predictors, TL with GNN achieves lower average distance error, average room distance error, and average room rank, while also attaining higher average MIDR values compared to De novo training using the same loss function. This indicates that the TL with GNN approach effectively leverages the information embedded in the MIDR predictor's GNN module to enhance the performance of the MFSP predictor.

We compare the MSE loss with Hybrid loss within the TL with GNN method, using the Hybrid ($w_1=50$) case as an example. For the average MIDR metric, the Hybrid loss explicitly calculates MIDR, achieving a higher value than the MSE loss by up to $(2.46-2.40)\left/2.40\right.=2.50\%$ for the small predictor, with a smaller improvement for the large predictor. Conversely, as the MSE loss is computed based on the pseudo ground truth MFSP (similar to the distance error metric), it achieves a lower average distance error than the that of the Hybrid loss by up to $16\%$ for the large predictor. Interestingly, despite the average room distance error and room rank being calculated relative to the pseudo ground truth MFSP, the Hybrid loss performs similar to or even better than the MSE loss. This is likely attributed to the pseudo labels being accurate at the room-level granularity, even though the MFSP is not necessarily located at the center of a room. As a result, the MSE and Hybrid loss methods show varying performances across the distance error and room distance error metrics.

The Hybrid loss introduces a trade-off between achieving higher MIDR and lower distance error by adjusting the weights $w_1$ and $w_2$, which reflect the degree of reliance on the pseudo labels. In this analysis, the weight for the MSE term is fixed at $w_2=1$. By varying the weight for the MIDR term ($w_1$) from 10 to 100, the reliance on pseudo labels decreases as $w_1$ increases. This trade-off is evident when comparing results for $w_1=10$ and $w_1=50$, where $w_1=10$ results in about $10\%$ lower distance error but also yields a slightly lower MIDR, reduced by about $1\%$. When $w_1$ is set to a very large value, such as $w_1=100$, the reliance on the pseudo labels becomes minimal, and the difficulty of training an ``argmaxer'', as discussed in \secref{subsec:mfspp_pseudo_label}, negatively impacts the MFSP predictor's performance. For the average MIDR metric, the performance with $w_1=100$ is worse than that achieved with $w_1=50$.

Taking TL with GNN and Hybrid loss ($w_1=50$) as an example, the average room distance error reaches 1.15, indicating that the predicted MFSP is, on average, located within 1.15 rooms of the ground truth MFSP. Additionally, the average room rank is $6.73$ indicating that the predicted MFSP is approximately the 6th or 7th most fire-sensitive room among all rooms in the building, with an average of over 87 rooms in each structure in this study. These low values suggest that the MFSP predictor accurately identifies the MFSPs in the building.

\paragraph{Quantile performance}
To further illustrate the performance of the MFSP predictor, we plotted the Cumulative Distribution Function (CDF), defined as ${\cal{F}}_{X}(a)={\cal{P}}(X \le a)$, where $\cal{P}$ indicates the probability and $X$ is a random variable representing the different metrics, i.e., distance error, room distance error, and room rank. Figures~\ref{fig:eval_mfspp_cdf}(a to c) display the results for the small predictor, while Figures~\ref{fig:eval_mfspp_cdf}(d to f) present the results for the large predictor. The key CDF values are summarized in \tabref{tab:mfspp_eval_cdf}. In \figref{fig:eval_mfspp_cdf}, the CDF curves of TL with GNN generally lie above those of De novo training, demonstrating that the former outperforms the latter in most cases. Besides, TL with GNN requires much less computational resources in training because it leverages a pre-trained GNN instead of training from scratch. The pre-trained model provides a well-initialized starting point, requiring only fine-tuning to adapt to the target task. This further highlights the efficiency and superiority of the approach of TL with GNN.

From \tabref{tab:mfspp_eval_cdf}, for the small network using TL with GNN and Hybird loss ($w_1=50$), $65.40\%$ of the cases achieve a distance error of less than $5$ meters. When the threshold is extended to $10$ meters, this ratio increases to $83.21\%$, surpassing the $82.19\%$ achieved with De novo training. Regarding room distance error, $\tilde{e}_{m}$, the large network using TL with GNN and Hybird loss achieves the highest probability of $73.93\%$ for $\tilde{e}_{m} \leq \sqrt{2}$, indicating that the predicted MFSP is either within the same room as the pseudo ground truth MFSP or in an adjacent room. Notably, for normalized rooms treated as unit-length squares (\secref{sec:7.2.1}), the longest distance between the centers of two diagonally adjacent rooms is $\sqrt{2}$. When the error threshold is further relaxed to $\tilde{e}_{m} \leq 2$, this probability increases to $80.32\%$, reflecting a high level of accuracy. For room rank, the large network employing TL with GNN and Hybird loss achieves the highest Top-5 accuracy, defined as the fraction of instances where the predicted MFSR ranks among the top 5 pseudo ground truth MFSRs, i.e., ${\cal{P}}\left(r_m \leq 5\right)$. This accuracy reaches $74.06\%$, representing an improvement of $2.49\%$ over De novo training. When using MSE loss, the TL with GNN method achieves a Top-5 accuracy of $72.28\%$, i.e., $3.7\%$ higher than that of De novo training. These results highlight the robustness of the TL with GNN approach, particularly when paired with Hybrid loss, in accurately identifying the most fire-sensitive locations within buildings.

In summary, the TL with GNN approach consistently outperforms De novo training across all evaluation metrics, highlighting the effectiveness of leveraging the MIDR predictor's GNN module to enhance the performance of the MFSP predictor. Two notable trade-offs arise when selecting the loss function.
The first is between $w_1$ and $w_2$ within the Hybrid loss to include reliance  on the room-level granularity of pseudo ground truth MFSP. 
The second is between MSE loss and Hybrid loss for higher efficiency and accuracy. While the Hybrid loss achieves slightly better results in most metrics (except for distance error), it incurs a higher computational cost due to the additional GNN forward pass required to calculate the MIDR loss and the backward pass through the MIDR predictor to calculate the gradient of the loss $L_{\text{MIDR}}$. In contrast, the MSE loss avoids this additional computational cost, making it more efficient but potentially less accurate in certain scenarios. This trade-off between computational cost and accuracy should be carefully considered based on the specific requirements and constraints of the application. If computational efficiency is a priority, the MSE loss may be more suitable. However, if higher accuracy in identifying sensitive fire points is critical, the Hybrid loss could provide a more robust solution.