\section{MIDR Predictor}
\label{sec:mdrp}
The MIDR predictor functions as a differentiable agent for the FEA simulators. Leveraging GNNs for structural representation, the predictor integrates Transfer Learning (TL) to optimize data utilization while focusing on the MIDR prediction. Additionally, an innovative EU module, to update the edge attributes of the GNNs, is introduced to capture changes in the finite element attributes during fire scenarios.

\subsection{GNNs}
\label{subsec:gnn}

\subsubsection{Overview of GNNs}
Building structures and graphs share an intrinsic similarity: structural nodes correspond to graph nodes, and structural elements (e.g., beams and columns) map naturally to graph edges. This analogy forms the basis for utilizing GNNs in structural data processing. A typical GNN consists of multiple stacked layers, where the output of one layer serves as the input for the next. Each  layer executes three core operations --message passing, aggregation \& update-- expressed as follows:
\begin{equation}
    \label{eq:gnn_overview}
    \vec{v}_i^k = \gamma^k \left( \vec{v}_i^{k-1}, \bigoplus_{j \in \mathcal{N}(i)} \phi^k \left( \vec{v}_i^{k-1}, \vec{v}_j^{k-1}, \vec{e}_{j,i} \right) \right),
\end{equation}
where $\gamma^k(\cdot)$ and $\phi^k(\cdot)$ represent the update and message functions, respectively, $\bigoplus$ is the aggregation operation, e.g., sum, mean, or max, $\mathcal{N}(i)$ denotes the set of neighboring nodes of node $i$, and $\vec{v}_i^k$ and $\vec{e}_{j,i}$ are the attributes of node $i$ at the $k$-th layer and of the edge from node $j$ to node $i$, respectively. Here, both $\gamma^k(\cdot)$ and $\phi^k(\cdot)$ are Multi-Layer Perceptrons (MLPs), i.e., fully connected NNs. \revise{The operations $\gamma(\cdot) ,\phi(\cdot) \, \& \, \bigoplus$ are not related to certain nodes or edges, indicating that the GNN architecture inherently supports variable-sized graphs through parameter-sharing mechanisms. Each graph is processed independently, with identical NN operations applied to all nodes and edges regardless of their number.} \figref{fig:gnn_illustration} is a schematic of a GNN with $K$ layers, illustrating the role of node and edge encoders, as well as the EU mechanism. The shown GNN incorporates initial node and edge encoders to transform the input features into higher-dimensional spaces and utilizes an EU mechanism to dynamically update the edge attributes during each layer to capture changes of the finite elements during fire scenarios.
\begin{figure*}[h!]
    \centering
    \includegraphics[width=1\linewidth]{figures/gnn_illustration.pdf}
    \caption{Illustration of a $K$-layer GNN architecture, demonstrating information flow within the network for a graph with $N$ nodes.}
    \label{fig:gnn_illustration}
\end{figure*}

\subsubsection{Input attributes}
\label{subsubsec:input_attributes}
In our framework, gravity and thermal loads are applied to the beams and columns. These elements dictate the information encapsulated in the nodes and edges of the graph representation.

\paragraph{Node attributes} 
% The information associated with each node $i$ in the structure includes geometric details, specifically the 3D coordinates $(x_i, y_i, z_i)$. These coordinates are essential for representing the spatial configuration of the structure. % Additionally, since floor height, $h_i$, is a critical factor in fire scenarios, it is also included as a key node attribute.
\revise{The coordinates of the $i$-th node in 3 Dimensions (3D) $(x_i, y_i, z_i)$, essential to represent the spatial configuration of the structure, are included as node attributes. Additionally, its floor level $h_i$, a critical factor to define the fire scenarios, is included as another node attribute. For ease of description, we call the four-tuple $\left(x_i, y_i, z_i, h_i\right)$ extended coordinates of node $i$. To account for the fire location, the extended coordinates of the fire source $\left(x_{\subfire}, y_{\subfire}, z_{\subfire}, h_{\subfire}\right)$, containing the 3D coordinates and the corresponding floor level of the fire, are incorporated into the node attributes. Note that $h_i$ and $h_{\subfire}$ are integers.
To enhance the model's ability to capture fire-related information, we compute the differences between the node's and fire source's extended coordinates $\left(x_i - x_{\subfire}, y_i - y_{\subfire}, z_i - z_{\subfire}, h_i - h_{\subfire}\right)$.
Finally, the Euclidean distance between the $i$-th node and the fire location is calculated as $\sqrt{(x_i-x_{\subfire})^2 + (y_i-y_{\subfire})^2 + (z_i - z_{\subfire})^2}$. 
% according to Equation (\ref{eq:Ls}) with $x=x_i$, $y=y_i$, and $z=z_i$.
These features collectively form the initial attributes of each node $i$, denoted as $\vec{v}_{i}^{\supinit}$, comprising a total of 13 features, i.e., $4$ for the extended coordinates of node $i$, $4$ for the extended coordinates of fire source, $4$ for the differences between these two sets of the extended coordinates, and $1$ for the Euclidean distance.
}

\paragraph{Edge attributes} The edges between nodes represent the structural elements (beams and columns) and encapsulate the following attributes:
\begin{itemize}
    \item {\bf{Material properties}}: Young's modulus \& yield strength at ambient temperature, and the strain-hardening ratio, \revise{counting up to 3 dimensions.}
    \item {\bf{Geometric properties}}: Length, floor level, and element orientation. Note that the orientation is encoded using a {\em{one-hot representation}} for alignment along the $x$-axis, $y$-axis, or $z$-axis. \revise{Therefore, the geometric properties count up to 5 dimensions. In this study and for simplicity, we use square sections for the beams and columns, requiring no additional attributes for the orientation of these sections.}
    \item {\bf{Gravity loads}}: {\revise{The magnitude of the applied load on the element counts as the 9-th dimension.}}
\end{itemize}
The initial attributes of each edge $(i,j)$, denoted as $\vec{e}_{i,j}^{\supinit}$, are represented by 9-dimensional vector combining these three sets of attributes.

\paragraph{Feature encoding} The raw node and edge attributes are not directly input into the GNN. Instead,  encoders are employed to map the initial attributes into higher-dimensional spaces, which improve the model's ability for feature representation as follows: 
\begin{itemize}
    \item {\bf{Node encoder}}: Extends the 13-dimensional node attributes $\vec{v}_{i}^{\supinit}$ to $\vec{v}_{i}^0$ with a higher dimensionality, e.g., 64 dimensions. The superscript $0$ indicates it is the input of the first GNN layer.
    \item {\bf{Edge encoder}}: Maps the 9-dimensional edge attributes $\vec{e}_{i,j}^{\supinit}$ to $\vec{e}_{i,j}$ with a higher-dimensional representation, e.g., 32 dimensions. 
\end{itemize}
\revise{
After extending the dimensions of node and edge features using encoders, their dimensions remain consistent across different layers of the GNN. This simplifies the architecture of the NN design, where different layers have similar structures, all with the same input and output dimensions. This characteristic of unchanged dimensions allows for the ease of extracting edge or node features from any layer of the GNN and performing the same processing for the MIDR prediction, as detailed in \secref{subsubsec:handle_structural_data}. If we do not use encoders to expand the dimensions and instead maintain consistent dimensions across different layers of the GNN, i.e., node attributes remain at 13 dimensions, the model capacity decreases and the performance of the predictor accordingly declines.
}
% These encoders, implemented as simple single-hidden-layer MLPs, ensures that the feature dimensions across different layers remain consistent. This consistency is crucial because the number of GNN layers a structure passes through is determined by its maximum number of stories, consistent dimensionality allows for seamless information flow and extraction across layers. This hierarchical encoding approach ensures that the GNN effectively captures the structural and fire-related information necessary for accurate MIDR predictions.

\subsubsection{Handling structural data}
\label{subsubsec:handle_structural_data}
A significant challenge in GNNs is the {\em{over-smoothing}}, where during each layer's update process, a node's attributes aggregate information from its neighboring nodes. Consequently, increasing the number of layers causes node attributes to homogenize, losing their specificity and  negatively impacting the model's performance. This issue is particularly pronounced in building structures with varying story counts and number of nodes, such as 2-story vs. 7-story buildings. If both are passed through a 7-layer GNN, the node attributes of the 2-story building would experience severe over-smoothing. 
\revise{
In simple terms, when there are too many message-passing layers, the characteristics of all nodes tend to be consistent. However, too few layers lead to the problem of incomplete perception, i.e., nodes cannot obtain information from distant nodes.
}
To mitigate this problem, we adopt the method from \cite{chou_structgnn_2024}, restricting the number of GNN layers for each graph, i.e., building structure herein, to the building's number of stories. 
\revise{
In this way, the number of layers is properly set, neither too many nor too few, ensuring that critical information about fire propagation is retained in the node attributes without over-smoothing.
}

Beams and columns exhibit distinct load characteristics that influence how they are represented in graph-based model of a structure. Beams primarily bear gravity loads that act perpendicular to their span direction, which aligns with their structural function in predominantly resisting bending moments and shear forces. Under thermal expansion due to fire, these loads affect both end nodes of the beam. As a result, when representing beams in a graph model, it is intuitive to treat them as {\em{undirected edges}}, reflecting their bidirectional influence on the connected nodes. {\revise{Columns, on the other hand, experience gravity loads along their axial direction, directed downward, which is consistent with their role in mainly supporting vertical loads in addition to bending moments and shear forces due to their frame action with the floor beams}}. When a column deforms under loads (due to thermal expansion, lateral loads, or gravity), the displacements at any point in the column influence the displacements above it, which continues sequentially upward along the column length. However, in the mathematical formulation of the FEA, the computation involves solving global system of equations that inherently account for all inter-dependencies between elements and boundary conditions simultaneously, not sequentially. Accordingly, representing columns as {\em{directed edges}} might initially seem appropriate, as it allows separate transmission of force and displacement information. However, in GNNs, undirected graphs are commonly implemented by duplicating directed edges and reversing their directions to ensure bidirectional information flow. Given this practical implementation in GNNs, both beams and columns are treated as {\bf{undirected edges}} in our graph model. This approach simplifies the representation while preserving the necessary information flow for structural analysis.

\subsubsection{Edge update mechanism}
\label{subsubsec:edge_update}
Typically GNNs often do not update edge attributes, $\vec{e}_{i,j}$, during the iterative update process in each layer. However, in the context of fire scenarios, the attributes of beams and columns (e.g., material properties and geometrical characteristics) evolve with temperature changes, directly affecting the MIDR computations. To address this need, we introduce an EU module that dynamically updates edge attributes during each layer's computation. After updating node attributes via message passing in each layer of the GNN, the EU module utilizes the attributes of the two adjacent nodes and the edge itself to update the edge attributes. This ensures that the edge features reflect the ongoing structural changes due to fire conditions. 

With the EU module, the standard GNN operation in Equation \eqref{eq:gnn_overview} is modified to include EUs. The updated node attributes are expressed as follows:
\begin{equation}
    \label{eq:gnn_with_edge_update}
    \vec{v}_i^k = \gamma^k \left( \vec{v}_i^{k-1}, \bigoplus_{j \in \mathcal{N}(i)} \phi^k \left( \vec{v}_i^{k-1}, \vec{v}_j^{k-1}, \vec{e}_{j,i}^{k-1} \right) \right).
\end{equation}
The EU module updates the edge attributes as follows:
\begin{equation}
    \label{eq:edge_update}
    \vec{e}_{j,i}^k = \zeta \left( \vec{v}_i^{k}, \vec{v}_j^{k} \right) + \vec{e}_{j,i}^{k-1},
\end{equation}
where $\zeta(\cdot)$ is the update function for edge attributes, implemented as a single-hidden-layer MLP. The inclusion of the EU module enables:
\begin{enumerate}
    \item {\bf{Dynamic adaptation}}: Capture temperature-induced changes in beam and column properties during each GNN layer's computations.
    \item {\bf{Enhanced modeling}}: Simultaneously updates node and edge attributes, resulting in a more accurate representation of the structural state.
    \item {\bf{Improved performance}}: Provides the GNN with the ability to better model the structural transformations under fire scenarios, leading to more precise MIDR predictions.
\end{enumerate}    
By iteratively updating edge attributes, the EU module effectively integrates the evolving structural properties into the GNN, enhancing its predictive capabilities in fire scenarios.

\subsection{TL-based Network Training}
\label{subsec:transfer_learning}
\revise{The proposed GNN framework only computes an embedding vector for each node. Therefore, to obtain the predicted MIDR, additional operations (layers) are needed. Two straightforward methods to aggregate the node-level results to a graph-level prediction can be applied. These two baseline methods are referred to as Strawman 1 \& 2 and they only differ in where and what to aggregate, as discussed below:}
% To predict the IDR, an additional MLP is employed to map the node embedding vector to the corresponding IDR value. We explore two baseline training and inference methods, referred to as Strawman 1 and Strawman 2, as illustrated in \figref{fig:transfer_learning}:
\begin{itemize}
    \item {\textbf{Strawman 1}} \revise{aggregates the node embeddings in a graph into a single graph embedding, using a {\em{pooling operation}}, which predicts MIDR with an additional MLP.}
    \item {\textbf{Strawman 2}} \revise{predicts the IDR of each node using node embeddings and an additional MLP. Then it aggregates node-level IDR predictions by selecting MIDR from them.}
    % the IDR for each node is directly predicted using its embedding, and the maximum value is selected as the structure's MIDR.
\end{itemize}

While Strawman 1 focuses solely on predicting the MIDR, it fails to leverage the IDR labels available for individual node data. Consequently, this approach risks overfitting when the dataset is small. On the other hand, Strawman 2 uses all node-level IDR labels, but the inclusion of non-MIDR nodes during training may dilute the model's focus on predicting the MIDR. To overcome these limitations, we propose a TL-based training method that integrates the strengths of both approaches. By framing the problem as two related tasks --predicting node-level IDRs and structure-level MIDR-- we adopt a multi-task TL approach with a two-step training procedure, \figref{fig:transfer_learning}. 
\begin{figure}[h!]
    \centering
    \includegraphics[width=1\linewidth]{figures/transfer_learning.pdf}
    \caption{Different training strategies (head-1 \& head-2 are MLPs for different tasks, e.g., predicting IDR of each node in a structure and MIDR of the structure.}
    \label{fig:transfer_learning}
\end{figure}

In the first step, the GNN is trained to predict the IDR of individual nodes, using an MLP referred to as {\bf{task head-1}} as in Strawman 2, \figref{fig:transfer_learning}. During this step, the GNN learns to extract global structural features. The loss function for this step is defined as follows:
\begin{equation}
    \label{eq:mdr_loss_task1}
    \begin{aligned}
        & L_{\text{task-1}}\left(\Theta_{\text{GNN}}, \Theta_{\text{head-1}}\right)\\
        & \qquad=\frac{1}{\sum_{m=1}^{M} N_{m} I_{m}}\sum_{m=1}^{M} \sum_{n=1}^{N_{m}} \sum_{i=1}^{I_{m}} \left( d_{m,n,i}^{\, \gt} - \widehat{d}_{m,n,i} \right)^2,
    \end{aligned}
\end{equation}  
where $d_{m,n,i}^{\, \gt}$ is the ground truth IDR for node $i \in \left(1,2,\dots,I_{m}\right)$ in structure $m \in (1,2,\dots,M)$ computed via OpenSeesRT under fire scenario $n \in \left(1,2,\dots,N_{m}\right)$, $\widehat{d}_{m,n,i}$ is the predicted IDR from the model, $\Theta_{\text{GNN}}$ is the set of parameters of the GNN, and $\Theta_{\text{head-1}}$ is the set of parameters of task head-1. The optimization objective for this step is as follows:  
\begin{equation}
    \label{eq:mdr_task1}
    \min_{\Theta_{\text{GNN}}, \Theta_{\text{head-1}}} L_{\text{task-1}}\left(\Theta_{\text{GNN}}, \Theta_{\text{head-1}}\right).
\end{equation}  

In the second step, the trained GNN parameters from the first step are reused as a {\em{feature extractor}}. The MLP for node-level IDR prediction (task head-1) is replaced with a new MLP, referred to as {\bf{task head-2}} as in Strawman 1, \figref{fig:transfer_learning}, to predict the structure MIDR. During this step, task head-2 is first trained independently, followed by {\em{fine-tuning}} the entire GNN. The MIDR for structure $m$ under fire scenario $n$ is defined as:  
\begin{equation}
    d_{m,n,\max} \triangleq \max_{i=1,2,\dots,I_{m}} d_{m,n,i}.
\end{equation}  
The loss function for this step is defined as follows:
\begin{equation}
    \label{eq:mdr_loss_task2}
    \begin{aligned}
        & L_{\text{task-2}}\left(\Theta_{\text{GNN}}, \Theta_{\text{head-2}}\right)\\
        & \qquad=\frac{1}{\sum_{m=1}^{M} N_{m}}\sum_{m=1}^{M} \sum_{n=1}^{N_{m}} \left( d_{m,n,\max}^{\, \gt} - \widehat{d}_{m,n,\max} \right)^2,
    \end{aligned}
\end{equation}  
where $d_{m,n,\max}^{\, \gt}$ is the ground truth MIDR for structure $m$ under fire scenario $n$ computed using OpenSeesRT, $\widehat{d}_{m,n,\max}$ is the predicted MIDR from the model, and $\Theta_{\text{head-2}}$ is the set of parameters of task head-2. The optimization objective for this step is as follows:
\begin{equation}
    \label{eq:mdr_task2}
    \min_{\Theta_{\text{GNN}}, \Theta_{\text{head-2}}} L_{\text{task-2}}\left(\Theta_{\text{GNN}}, \Theta_{\text{head-2}}\right).
\end{equation}  

The proposed TL-based methods offers these benefits:
\begin{enumerate}
    \item {\bf{Effective utilization of data}}: Leverages node-level IDR labels while focusing on MIDR prediction.
    \item {\bf{Improved generalization}}: Enhances the model's ability to generalize across diverse fire scenarios and structural configurations.
    \item {\bf{Accurate MIDR predictions}}: Integrates global structural information for precise MIDR predictions, outperforming the individual Strawman methods 1 \& 2.
\end{enumerate}
By combining node-level and structure-level predictions in a two-step training pipeline, the TL-based approach achieves a robust balance between learning granular details and capturing global structural patterns. 
