\documentclass{article}


\usepackage{arxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{threeparttable}
\graphicspath{ {./images/} }


\title{CAST: Cross Attention based multimodal fusion of Structure and Text for materials property prediction}


\author{
    Jaewan Lee \\
  Materials Intelligence Lab\\
  LG AI Research\\
  Seoul, Republic of Korea \\
  \texttt{jaewan.lee@lgresearch.ai} \\
  %% examples of more authors
   \And
    Changyoung Park\\
  Materials Intelligence Lab\\
  LG AI Research\\
  Seoul, Republic of Korea \\
  \texttt{changyoung.park@lgresearch.ai} \\
  \And
  Hongjun Yang \\
  Materials Intelligence Lab\\
  LG AI Research\\
  Seoul, Republic of Koreal \\
  \texttt{hongjun.yang@lgresearch.ai} \\
  \And
  Sungbin Lim \\
  Department of statistics\\
  Korea University\\
  Seoul, Republic of Korea \\
  \texttt{sunbin@korea.ac.kr} \\
  \And
  Sehui Han \\
  Materials Intelligence Lab\\
  LG AI Research\\
  Seoul, Republic of Korea\\
  \texttt{hansse.han@lgresearch.ai} \\
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}

\begin{document}
\maketitle
\begin{abstract}
Recent advancements in AI have revolutionized property prediction in materials science and accelerating material discovery. Graph neural networks (GNNs) stand out due to their ability to represent crystal structures as graphs, effectively capturing local interactions and delivering superior predictions. However, these methods often lose critical global information, such as crystal systems and repetitive unit connectivity. To address this, we propose CAST, a cross-attention-based multimodal fusion model that integrates graph and text modalities to preserve essential material information. CAST combines node- and token-level features using cross-attention mechanisms, surpassing previous approaches reliant on material-level embeddings like graph mean-pooling or [CLS] tokens. A masked node prediction pretraining strategy further enhances atomic-level information integration. Our method achieved up to 22.9\% improvement in property prediction across four crystal properties including band gap compared to methods like CrysMMNet and MultiMat. Pretraining was key to aligning node and text embeddings, with attention maps confirming its effectiveness in capturing relationships between nodes and tokens. This study highlights the potential of multimodal learning in materials science, paving the way for more robust predictive models that incorporate both local and global information.
\end{abstract}


\section{Introduction}
Designing AI-based predictive models has become an essential tool for accelerating the discovery and optimization of advanced materials. Traditional approaches rely heavily on domain expertise and heuristic methods, often supplemented by Density Functional Theory(DFT) calculations. While DFT is widely used for its accuracy in predicting material properties, its computational expense and time requirements limit its scalability for high-throughput material discovery, especially given the complexity and diversity of material systems. With the advent of machine learning(ML) techniques, particularly Graph Neural Networks(GNNs), researchers have been able to model material structures with unprecedented accuracy and speed by representing them as graphs and capturing intricate local interactions\cite{xie2018crystal, gasteiger2020directional, gasteiger2021gemnet, choudhary2021atomistic, ruff2024connectivity, merchant2023scaling}. Despite these advancements, the process of converting material structures into graph representations inherently leads to the loss of crucial information, such as crystal symmetries and the connectivity of repetitive structural units, which are critical to certain material properties.
This issue is inherent when using GNNs. To address this limitation, multimodal learning can be leveraged to complement the lost information by incorporating additional modalities. While multimodal learning has been extensively studied in fields such as vision, language, and speech \cite{wang2023large, xu2023multimodal, park2022graph}, its application in the materials, especially inorganic crystals, remains relatively underexplored.\cite{das2023crysmmnet, munjal2024lattice, ozawa2024graph, ock2024unimat, lee2023clcs}
One primary reason for this gap is the relative scarcity of data suitable for multimodal learning in materials science. However, the recent development of tools like Crystallographer\cite{ganose2019robocrystallographer}, an API that generates textual descriptions from crystal structures, has enabled some initial studies in this area. Crystallographer generates text descriptions based on rules derived from structural information, including global and semi-global features that are often lost when structures are converted into graph representations. This advancement has opened new opportunities for integrating multimodal approaches in materials science research.

Techniques such as various attention methods\cite{Tan2019LXMERTLC, Li2022BLIPBL, Li2023BLIP2BL} and gradient modulation \cite{Peng2022BalancedML, Zhang2023MultimodalRL} often drive advancements in multimodal learning within the Vision-Language and molecule domains.
In contrast, materials science primarily relies on simpler approaches, such as concatenation or contrastive pretraining methods like CLIP\cite{radford2021learning}. Notable examples are CrysMMNet\cite{das2023crysmmnet} and MultiMat\cite{moro2024multimodallearningmaterials}, which are optimized to predict properties of materials.
CrysMMNet encodes structure and text using separate modality encoders, then concatenates the embeddings to predict material properties. MultiMat simultaneously leverages structure, text, density of states(DOS), and charge density, employing contrastive learning to pretrain a GNN encoder before applying it to downstream tasks. 
Both approaches demonstrated performance improvements over unimodal GNNs, but their reliance on coarse-level modality combination introduces limitations which may fail to fully exploit the intricate relationships between modalities, potentially resulting in suboptimal performance. 

In contrast, we propose a \textbf{CAST}, \textbf{Cross Attention-based multimodal fusion of Structure and Text}, which integrates graph and text modalities at a fine-grained level, ensuring profound mutual interactions. Our method incorporates node- and token-level features using cross-attention mechanisms, which are proposed in a Transformer\cite{vaswani2017attention}, allowing the model to learn more complex relationships between the structural and textual representations. Inspired by the success of masked object prediction in the language domain \cite{radford2018improving, kenton2019bert, liu2019roberta}, we applied a Masked Node Prediction (MNP) pretraining strategy by masking a subset of nodes in the graph and training the model to predict the masked nodes using information from their neighboring nodes and corresponding text tokens. Through extensive experimentation, we demonstrate that our approach outperforms unimodal models as well as multimodal methods like CrysMMNet and MultiMat, achieving performance gains of up to 22.9\% across four property prediction tasks. Analyses of cross-attention maps further reveal that pretraining enables attention heads to effectively capture a diverse range of node-token relationships, which are critical for accurate predictions. By addressing the inherent limitations of existing models, our work establishes a robust framework for multimodal learning in materials science, paving the way for more accurate predictive models. 

\section{Results and discussion}
% 

\subsection{Data}
For training and evaluation, data was downloaded from the Materials Project\cite{jain2013commentary} database and rigorously filtered to ensure quality and relevance. The filtering criteria were inspired by the data cleaning methods used in MatBench\cite{dunn2020benchmarking} and further enhanced with additional constraints to ensure practical applicability and facilitate efficient screening processes for real-world use. The applied filters are as follows:
\newline
\noindent\textbf{MatBench Filtering Criteria:} % 들여쓰기 제거
\begin{list}{$\bullet$}{\setlength{\leftmargin}{2em}}
    \item Remove entries with a formation energy or energy above the convex hull greater than 150 meV.
    \item Exclude entries where $G_{\text{Voigt}}, G_{\text{Reuss}}, G_{\text{VRH}}, K_{\text{Voigt}}, K_{\text{Reuss}},$ or $K_{\text{VRH}}$ are less than or equal to zero.
    \item Remove entries that do not satisfy the conditions $G_{Reuss} < G_{VRH} < G_{Voigt}$ or $K_{Reuss} < K_{VRH} < K_{Voigt}$.
    \item Exclude entries containing noble gases.

\end{list}
\vspace{\baselineskip}
\noindent \textbf{Additional Filtering:}
\begin{list}{$\bullet$}{\setlength{\leftmargin}{2em}}
    \item Exclude entries with a formation energy greater than -10 eV.
    \item Remove entries with shear modulus or bulk modulus values exceeding 1000 GPa.
\end{list}
\vspace{\baselineskip}
\noindent For regression tasks, we used four properties(total energy, bandgap, shear modulus and bulk modululs). Each dataset comprised 114,398, 65,367, 9,423 and 9,423 instances, respectively. Detailed statistical analyses of the regression data for each target property can be found in the Table \ref{tab:statistic_analysis}. For the pretraining stage, we leveraged the total energy data, which had the largest volume of samples.

\subsection{Text generation}
While the Materials Project directly provides structural information in cif formats, textual descriptions of material structures were generated using the Robocrystallographer API\cite{ganose2019robocrystallographer}. This allows for the automatic conversion of structural data into textual descriptions, which we processed using the default settings. For instances where text could not be generated, we adapted our approach by passing only the [CLS] token through the text encoder. The proportion of cases where text generation failed varies by property but remains below 0.2\% for all properties. Therefore, it did not pose a significant concern. Statistical analyses, including the number of text tokens and the proportion of successfully generated texts, are detailed in the Table\ref{tab:statistic_analysis}.



\subsection{Framework}

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{figures/figure1_overview.png}
  % \fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
  \caption{Overall framework of CAST 
 \textbf{(a) Pretraining.} \; During pretraining, a subset of nodes is masked with a 50\% probability. The structure encoder processes the graph to generate node embeddings, while the corresponding text is encoded into token-level embeddings using a language model (LM). A cross-attention mechanism enables node embeddings to interact with text token embeddings, aligning nodes with relevant textual descriptions. The model learns to predict the masked node's element type, aligning structural and textual features. 
 \textbf{(b) Finetuning.} \; In the finetuning stage, the pretrained model is adapted for material property prediction by replacing the classification block with a regression block. The structural and textual embeddings, aligned during pretraining, are leveraged to predict material properties accurately through cross-attention and interaction layers.}
  \label{fig:figure1}
\end{figure}
CAST was trained in two stages, (1) MNP pretraining and (2) finetuning with MP data, as illustrated in Fig\ref{fig:figure1}. To embed each modality, we utilized a randomly initialized coGN\cite{ruff2024connectivity}  to generate node embeddings from the graph and MatSciBERT\cite{gupta2022matscibert}, pretrained on a large corpus of peer-reviewed materials science publications, to get text token embeddings.


\subsubsection{Pretraining: Masked Node Prediction(MNP)}

A subset of nodes in the graph is masked with a 50\% probability, and the structure encoder processes the graph to generate embeddings for each node. Simultaneously, the corresponding text is passed through a text encoder to produce token-level embeddings. The node embeddings(as queries) and the text token embeddings(as keys and values) interact through a cross-attention mechanism. This interaction allows the node embeddings to gather information from the text token embeddings through the cross-attention mechanism. As the model learns to predict the masked node's element type, it simultaneously aligns the node embeddings with the most relevant text tokens. This process enhances the model's ability to represent and utilize cross-modal relationships. 

\subsubsection{Finetuning}

The pretrained model architecture is preserved, with the classification block replaced by a regression block to align with the predictive objective. By leveraging the structural and textual embeddings aligned during pretraining, the model is optimized to predict material properties with improved accuracy. This enhancement stems directly from the pretrained cross-modal understanding, which enables a more comprehensive integration of multimodal features.

\subsection{Comparison of Predictive Performance }
%  성능 다 나오면 쓰기
CAST was compared against unimodal models(coGN\cite{ruff2024connectivity} and MatSciBERT\cite{gupta2022matscibert}) and other multimodal methods, CrysMMNet(concatenation)\cite{das2023crysmmnet} and MultiMat(pretrained with contrastive learning)\cite{moro2024multimodallearningmaterials}. In the original studies, CrysMMNet and MultiMat utilized different encoders for their respective architectures. However, to ensure a fair comparison of fusion methods, we standardized the encoders by using coGN as the structure encoder and MatSciBERT as the text encoder for training both models. In MultiMat, contrastive learning is originally conducted using four modalities, but in this study, we used only structure and text for training. While CrysMMNet originally froze the text encoder during training, we evaluated both the frozen approach and a fine-tuning approach using LoRA \cite{hu2021lora}, a widely adopted technique for efficient model fine-tuning. Since the MatSciBERT was not pretrained for regression tasks, incorporating LoRA allowed us to explore and compare a broader range of multimodal scenarios within the concatenation framework. Our approach demonstrated the best performance in predicting three out of four properties, except for bulk modulus, where it achieved results only 0.001 lower than the best-performing method, CrysMMNet(LoRA fine-tuning), making it nearly equivalent in performance.
Notably, the test MAE loss for total energy prediction was approximately 23\% lower than that of the second best model, CrysMMNet(LoRA ft). 
Details on the implementation of each model are provided in the Method section. When comparing single-modality models, the text based MatSciBERT showed superior performance in predicting total energy, while coGN excelled in bandgap prediction. For modulus predictions, the performance of the two models was nearly identical. These results suggest that certain modalities are better suited for predicting specific properties, highlighting the importance for developing multimodal predictive models that leverage the strengths of multiple modalities.


Among multimodal approaches, the strong performance of CAST can be attributed to its ability to integrate information from both modalities in a fine-grained manner at the token level and to align the two modalities through pretraining. However, when comparing CAST (w/o pretraining) with other methods, it was observed that cross-attention alone was insufficient to achieve superior performance across all properties. In contrast, CAST with pretraining showed consistently improved predictive performance across all properties, outperforming both other methods and its w/o pretraining counterpart. Specifically, for bandgap prediction, most multimodal methods, including CAST (w/o pretraining), performed worse than the unimodal coGN. However, pretraining mitigated this issue, underscoring its critical role in multimodal learning. 
With CrysMMNet, fine-tuning with LoRA outperformed using a frozen language model for most properties, except for bandgap. This suggests that, since MatSciBERT was not optimized for property regression during pretraining, the addition of low-rank matrices through fine-tuning provided beneficial adjustments that improved performance. On the other hand, the MultiMat(pretrained with contrastive learning) approach negatively affected the parameters of the GNN during pretraining, resulting in degraded performance compared to the unimodal coGN model. In the case of the MultiMat paper, performance improved when different GNNs and text encoders were used. Further research is required to determine whether this improvement stems from changes in the dataset or the choice of encoders. The superior performance of the concatenation approach over contrastive learning suggests that leveraging both modalities directly together can have complementary effects. This is likely because, during fine-tuning, GNNs pretrained with contrastive learning do not directly utilize the text modality, limiting their ability to fully benefit from multimodal information.

\begin{table}
\label{tab:model evaluation}
\centering
\resizebox{\textwidth}{!}{
  \begin{tabular}[width=\textwidth]{c c c c c}
    \toprule
    Models  & Total Energy &  Bandgap & log(Shear Modulus)  & log(Bulk Modulus)  \\
    \midrule
    coGN & 0.673(0.118) & 0.381(0.006) & 0.091(0.006) & 0.050(0.001)\\
    MatSciBERT & 0.390(0.047) & 0.420(0.002) & 0.089(0.002) & 0.053(0.001) \\
    \midrule
    CrysMMNet(frozen LM)\tnote{*} & 0.615(0.121) & \underline{0.369(0.005)} & 0.085(0.003) & 0.047(0.001) \\
    CrysMMNet(LoRA ft)\tnote{*} & 0.332(0.056) & 0.416(0.005) & 0.073(0.004) &  \textbf{0.038(0.001)}\\
    MultiMat\tnote{*} & 1.095(0.086) & 0.414(0.003) & 0.089(0.002) & 0.055(0.002) \\
    CAST(w/o pretraining) & \underline{0.277(0.051)} & 0.394(0.003) & \underline{0.072(0.002)} & 0.042(0.002)\\
    \textbf{CAST(ours)} & \textbf{0.256(0.045)} & \textbf{0.354(0.002)} & \textbf{0.069(0.001)} & \underline{0.039(0.0003)}\\
    \bottomrule
\end{tabular}
}
\caption{This table shows the predictive performance of five models in terms of MAE. We tested using three different random seeds, with the mean values presented as numbers and the standard deviation values shown in parentheses. Bold text indicates the best-performing model. The second-highest performance is indicated with an underline. \tnote{*}In the original studies, CrysMMNet and MultiMat utilized different encoders for their respective architectures. However, to ensure a fair comparison of fusion methods, we standardized the encoders by using coGN as the structure encoder and MatSciBERT as the text encoder for training both models. In MultiMat, contrastive learning is originally conducted using four modalities, but in this study, we used only structure and text for training.}
\end{table}

\subsection{Further analysis of pretraining effects}
\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{figures/figure2_attentionmap.png}
  \caption{\textbf{A example of attention map}
  \; To analyze whether the node tokens in the cross-attention heads effectively attend to diverse text tokens, we brought one example from the test set(mp-1963).
  % we examined four examples from the test set (mp-2025704, 644607, 9254, 27840). 
  In the case of CAST(w/o pretraining), we observed that node tokens tend to attend to a limited variety of text tokens, often assigning similar importance to the same tokens. This behavior frequently results in vertical linear patterns. In contrast, for the CAST(w/ pretraining), node tokens demonstrate a broader and more varied attention to different text tokens compared to the CAST(w/o pretraining). This indicates pretraining improved cross-modal interaction between modalities.
  Other examples can be seen in Supplementary Information.}
  \label{fig:figure2}
\end{figure}

We conducted an analysis to investigate how pretraining contributed to the observed improvement in predictive performance. Using a test set example, we examined the attention maps generated by the cross-attention mechanism (Fig. \ref{fig:figure2}). In the figure, the x-axis represents the text tokens acting as keys, while the y-axis corresponds to the query tokens. Each cell indicates the attention score, with brighter colors reflecting higher scores and stronger associations.

The first row, labeled “CAST(w/o pretraining)”, shows the attention maps extracted from CAST (w/o pretraining), whereas the second row, “CAST(w/ pretraining)”, represents the attention maps obtained from CAST. In the “CAST(w/o pretraining)” case, we observed that all nodes predominantly attended to the same text tokens, resulting in stripe-like attention patterns. In contrast, the CAST(w/ pretraining) exhibited a more diverse attention distribution, where nodes attended to a broader range of text tokens rather than focusing on just a few. This indicates that pretraining enables the model to establish meaningful associations between node tokens and text tokens, allowing each query node to attend more diversely to relevant text tokens. Additional examples are provided in the Supplementary Information.


\begin{figure} 
  \centering
  \includegraphics[width=\textwidth]{figures/figure3_similarity_dist.png}
  \caption{\textbf{Distributions of similarities} 
  \; To generalize these observations, we analyzed the distribution of similarities in the attention maps between node tokens across the dataset. Each row represents how a node token references text tokens.  We calculated the cosine similarity of attention values between all pairs of nodes (combinations).
  In the CAST(w/o pretraining), the similarities were mostly concentrated near 1, with a high degree of focus that intensified as layers progressed. On the other hand, the CAST(w/ pretraining) exhibited a wider distribution of similarities, with less severe concentration as layers deepened compared to the from-scratch case. These findings indicate that pretraining enables node tokens to reference a variety of text tokens, allowing them to receive more relevant and diverse information. This diversity in information flow directly enhances the model’s performance on downstream regression tasks, demonstrating the effectiveness of pretraining in improving the overall learning process.}
  \label{fig:figure3}
\end{figure}

To generalize these observations, we analyzed the distribution of similarities in the attention maps between node tokens across the dataset(Fig \ref{fig:figure3}). 
In the given example, the material is represented as a graph with four nodes (Al, Te, Te, Te). To quantify how diversely node tokens reference text tokens, we calculated the cosine similarity of attention values between all pairs of nodes (combinations). This analysis offers a quantitative measure of how pretraining enhances the diversity of text token references by node tokens.
In the CAST(w/o pretraining), the similarities were predominantly concentrated near 1, indicating a high degree of focus that intensified as layers progressed. On the other hand, the pretrained model exhibited a wider distribution of similarities, with less severe concentration as layers deepened compared to the CAST(w/o pretraining).

These findings confirm that pretraining enables node tokens to reference a variety of text tokens, allowing them to gather more diverse and relevant information. This validates that the phenomenon observed in Fig \ref{fig:figure2} is not limited to few examples but reflects an overall trend acrooss the dataset. The ability of node tokens to reference a diverse range of relevant text tokens demonstrates that the model has effectively captured the token-level relationships between modalities, ultimately contributing to improved downstream performance. 


\section{Conclusion}
In this study, we proposed \textbf{CAST}(Cross Attention based multimodal fusion of Structure and Text), a multimodal learning framework for material property prediction that integrates structural and textual data at a fine-grained level. Through extensive experimentation, we demonstrated that our approach outperforms unimodal baselines(coGN and MatSciBERT) and other multimodal models(Concatenation and Contrastive learning) across multiple material property prediction tasks. Our results highlight the critical role of cross attention and pretraining in aligning structural and textual modalities. The Masked Node Prediction(MNP) pretraining strategy significantly improved the model’s ability to align node tokens with their relevant text tokens, as evidenced by the diverse and meaningful attention patterns observed in the cross-attention maps. This alignment not only improved the accuracy of property predictions, particularly for challenging properties such as bandgap, but also contributed to the stability of the learning process, reducing performance variability across random seeds. 

Despite these achievements, there are several areas where the framework could be further improved to enhance its applicability and efficiency. First, the reliance on language models for text encoding and attention mechanisms for modality fusion leads to increased computational costs, particularly as the number of tokens grows. This poses challenges for scalability when dealing with datasets taht include significantly larger or more complex structural and textual modalities. Additionally, the framework is more sensitive to data quality compared to traditional unimodal materials datasets, because both modalities must be well-represented and paired. Lastly, the text encoder employed in this study is limited to processing only 512 tokens. Employing a text encoder capable of handling longer text sequences could further enhance performance, as demonstrated in previous research\cite{alampara2024mattext}.
Addressing these limitations in future research could involve the development of more efficient fusion strategies, exploration of lightweight or advanced encoders, and application of the framework to more diverse multimodal datasets and a broader range of material properties.


\section{Methods}
\subsection{Statistical analysis of data}
%  data statistics 갖다 붙이기 -> 걍 이미지로 갖다 붙이자 (table로 하니까 삐져나감)
The table presents a comprehensive summary of the dataset statistics for various material properties, including total energy, bandgap, the logarithm of shear modulus, and the logarithm of bulk modulus, along with their respective train, valid, and test splits. For each property, key characteristics of the graph and text representations are detailed, such as the mean and standard deviation of the number of graph nodes and text tokens. The text existence rate indicates the proportion of samples with successfully generated textual descriptions, with absolute counts provided in parentheses. Furthermore, the table reports the mean and standard deviation of the target property values, offering insights into the distributions within the dataset. These statistics underscore the variation in structural and textual complexities across properties and highlight the near-complete availability of text descriptions, enabling the seamless integration of multimodal information for predictive modeling tasks.

\begin{table}[htbp]
\scriptsize
\centering
\resizebox{\textwidth}{!}{
    \begin{tabular}{|l|l|l|l|l|l|l|l|l|}
    \hline
     & \multicolumn{2}{c|}{\textbf{Total Energy}} & \multicolumn{2}{c|}{\textbf{Bandgap}} & \multicolumn{2}{c|}{\textbf{log(Shear Modulus)}} & \multicolumn{2}{c|}{\textbf{log(Bulk Modulus)}} \\ \hline
     \textbf{Data type} & \textbf{train} & \textbf{val/test} & \textbf{train} & \textbf{val/test} & \textbf{train} & \textbf{val/test} & \textbf{train} & \textbf{val/test} \\ \hline 
    Dataset size & 91,520 & 22,878 & 52,371 & 12,996 & 7,513 & 1,910 & 7,513 & 1,910 \\ \hline
    \# of node mean(std) & 17.28(28.59) & 17.10(28.34) & 22.95(34.95) & 22.78(34.76)  & 3.67(2.03) & 3.58(2.39) & 3.67(2.03) & 3.58(2.39)\\ \hline
    \# of tokens mean(std) & 1,796.65(3574.83) & 1,783.56(3516.19) & 2,326.0(4137.72) & 2,323.2(4109.76) & 250.83(272.62) & 250.16(327.55) & 250.83(272.62) & 250.16(327.55) \\ \hline
    Text existence rate & 99.9\%(91,437) & 99.9\%(22,855) & 99.9\%(52,298) & 99.8\%(12,974)& 100\%(7,513) & 100\%(1,910) & 100\%(7,513) & 100\%(1,910) \\ \hline
    Y mean(std) & -9.14(7.22) & -9.26(7.38) & 2.17(1.54) & 2.15(1.55) & 1.54(0.39) & 1.55(0.38) & 1.87(0.38) & 1.88(0.37)  \\ \hline
\end{tabular}
}
\caption{\textbf{Statistical analysis of data} \quad The table provides a comprehensive summary of the dataset statistics across different material properties, including total energy, bandgap, logarithm of shear modulus, and logarithm of Bulk Modulus, as well as their respective data splits for train and val/test.}
\label{tab:statistic_analysis}
\end{table}



\subsection{Encoders for each modality}
\subsubsection{Structure Encoder : coGN}
By leveraging the inherent symmetry of crystals, coGN \cite{ruff2024connectivity} employs an asymmetric unit cell representation to reduce the number of graph nodes, thereby improving computational efficiency. The proposed Nested Line Graph Network (NLGN) architecture further enhances the GNN framework through optimized message passing. This approach has demonstrated superior performance across most tasks within the MatBench benchmark dataset \cite{dunn2020benchmarking}. Key factors contributing to these performance improvements include optimized connectivity and enriched message-passing capabilities. The methods introduced by coGN are broadly applicable for modeling crystal structures, providing a systematic framework for comparing and designing diverse GNN architectures. Given its state-of-the-art (SOTA) performance on MatBench and computational efficiency, coGN is considered to have robust encoding capabilities, making it an ideal choice as the GNN encoder for our work.


\subsubsection{Text Encoder: MatSciBERT}
MatSciBERT\cite{gupta2022matscibert} is a domain-specific language model designed for materials science, trained on a large corpus of peer-reviewed materials science publications. It excels at information extraction tasks by effectively interpreting the unique notations and terminology prevalent in materials science literature. Evaluations on tasks related to materials such as abstract classification, named entity recognition, and relation extraction have demonstrated that MatSciBERT outperforms general scientific language models, such as SciBERT. The pretrained and fine-tuned models are publicly available, serving as a valuable resource for the materials science community. We hypothesize that leveraging a model with a deep understanding of materials science will significantly enhance predictive performance. Therefore, we adopted MatSciBERT as the text encoder in our framework.

\subsection{Multimodal fusion methods}
We compared regression performance using three approaches: concatenation, contrastive learning-based pretraining, and CAST(cross attention). Among these, CAST demonstrated the best prediction performance across four properties. While concatenation and contrastive learning utilize instance-level multimodal  features, CAST leverages token-level features. To ensure a fair comparison, we standardized the training hyperparameters across all methods.

For training, we employed a periodic cosine scheduling strategy to adjust the learning rate. To enhance training stability, we included a warm-up phase for the initial 1,000 steps. For contrastive learning pretraining, we set the batch size to 360, following the configuration used in the MultiMat paper\cite{moro2024multimodallearningmaterials}, as larger batch sizes are more efficient for this approach. For other pretraining task and regression tasks, a batch size of 64 was used.

For consistency, we employed coGN\cite{ruff2024connectivity} and MatSciBERT\cite{gupta2022matscibert} as the structure and text encoders across all methods. Accordingly, the feature dimensions of node tokens and text tokens were set to 128 and 768, respectively. 

\subsubsection{CAST(proposed method)}
CAST, based on cross-attention, follows a two-step process: pretraining and finetuning. Unlike previous studies \cite{das2023crysmmnet, moro2024multimodallearningmaterials, munjal2024lattice} in the materials domain that focused on multimodal fusion at the instance level, we hypothesize that token-level fusion can achieve superior performance. To enable interaction at the token level, we employ the cross-attention mechanism of transformer models \cite{vaswani2017attention}.

During the pretraining phase, a subset of graph nodes is randomly masked with a 50\% probability. The structure encoder processes the graph to generate embeddings for each node, while the corresponding text is passed through a text encoder to produce token-level embeddings. These embeddings interact through a cross-attention mechanism, where the node embeddings serve as queries, and the text token embeddings act as keys and values. This interaction allows the node embeddings to incorporate contextual information from the text. As the model learns to predict the element types of the masked nodes, it simultaneously aligns node embeddings with the most relevant text tokens. The fusion module consists of four cross attention layers with eight attention heads, each with an attention dimension of 128. The classification block is a single linear layer.

Following the pretraining phase, the model undergoes a finetuning stage specifically designed for property prediction tasks. In this phase, the pretrained model is preserved, but the classification block is replaced with a regression block, also a single linear layer, to align with the predictive objective. By leveraging the structural and textual embeddings aligned during pretraining, the model is optimized to predict material properties with heightened accuracy. This improvement stems from the pretrained cross-modal understanding, which facilitates a more comprehensive integration of multimodal features. As shown in Table \ref{tab:model evaluation}, our method demonstrates more stable and accurate predictive performance compared to other multimodal approaches and unimodal models.


\subsubsection{Concatenation(CrysMMNet)}
CrysMMNet\cite{das2023crysmmnet}, to the best of our knowledge, is the first approach to leverage multimodal learning for material property prediction. Their model employs ALIGNN as the structure encoder and MatSciBERT as the text encoder. In this framework, the graph structure is processed through a graph encoder to generate graph embeddings, while textual descriptions are passed through a text encoder and a projection layer to produce text embeddings. These representations are then fused to jointly model the input modalities and predict crystal properties. Notably, the text encoder is frozen during training in their method, and the study does not evaluate the model’s performance when the text encoder is not frozen. To address this gap, we conducted additional experiments, comparing the performance of the Concat method with a frozen language model (LM) and with LoRA fine-tuning. The results of these experiments are presented in Table \ref{tab:model evaluation}.


\subsubsection{Contrastive Learning(MultiMat)}
In MultiMat, contrastive learning between multimodal representations is utilized to pretrain the structure encoder, similar to the approach used in CLIP\cite{radford2021learning}. Their study reported that applying the pretrained graph encoder to downstream tasks improves performance compared to training from scratch. MultiMat leverages four modalities—crystal structures, density of states (DOS), and charge density, whereas our study focuses on comparing different modality fusion strategies.
To ensure a fair comparison with other methods, we pretrained the model using only structural information and textual descriptions. Furthermore, while MultiMat utilizes PotNet \cite{lin2023efficient} as the structure encoder and MatBERT \cite{walker2021impact} as the text encoder, we adapted the model by employing the coGN structure encoder and MatSciBERT text encoder. This alignment with the configurations of other methods ensures consistent evaluation across models.

-----------------


\bibliographystyle{unsrt}  
\bibliography{references}  %%% Remove comment to use the external .bib file (using bibtex).
%%% and comment out the ``thebibliography'' section.


%%% Comment out this section when you \bibliography{references} is enabled.
% \begin{thebibliography}{1}

% \bibitem{kour2014real}
% George Kour and Raid Saabne.
% \newblock Real-time segmentation of on-line handwritten arabic script.
% \newblock In {\em Frontiers in Handwriting Recognition (ICFHR), 2014 14th
%   International Conference on}, pages 417--422. IEEE, 2014.

% \bibitem{kour2014fast}
% George Kour and Raid Saabne.
% \newblock Fast classification of handwritten on-line arabic characters.
% \newblock In {\em Soft Computing and Pattern Recognition (SoCPaR), 2014 6th
%   International Conference of}, pages 312--318. IEEE, 2014.

% \bibitem{hadash2018estimate}
% Guy Hadash, Einat Kermany, Boaz Carmeli, Ofer Lavi, George Kour, and Alon
%   Jacovi.
% \newblock Estimate and replace: A novel approach to integrating deep neural
%   networks with existing applications.
% \newblock {\em arXiv preprint arXiv:1804.09028}, 2018.

% \end{thebibliography}


\end{document}
