% \section{Main Results}
% First, we show that the two timescale mean square are related to their asymptotic covariances
% \begin{equation}
%     \lim_{t \to \infty} \alpha_t^{-1} \mathbb{E} \tilde{x}_t \tilde{x}_t^T = \Sigma_{ff} , \; 
%     \lim_{t \to \infty} \gamma_t^{-1} \mathbb{E} \hat{y}_t \hat{y}_t^T = \Sigma_{ss} , 
%     \; 
%      \lim_{t \to \infty} \gamma_t^{-1} \mathbb{E} \hat{y}_t \tilde{x}_t^T = \Sigma_{sf} .
% \end{equation}
% The asymptotic covariances were evaluated in \citep{konda2004convergence}, and we show in Sections \ref{sec:fast_mse}--\ref{sec:slow_mse} that the errors at finite $t \geq 1$ are of order
% \begin{equation}
%     \mathrm{Tr}\mathbb{E}\tilde{x}_t \tilde{x}_t^T = \mathcal{O}\left(\alpha_t \mathrm{Tr} \Sigma_{ff} + \gamma_t\right), 
%     \;
%     \mathrm{Tr} \mathbb{E}\hat{y}_t \hat{y}_t^T = \mathcal{O}\left(\gamma_t \mathrm{Tr} \Sigma_{ss} + \gamma_t \left(\alpha_t \mathrm{Tr} \Sigma_{ff} + \gamma_t \mathrm{Tr} A_{sf} \Sigma_{fs}\right) \right) .
% \end{equation}
% The MSE of $\hat{x}_t$ is then recovered using $x_t - x^* = \hat{x}_t + (x_\infty (y_t) - x^*) = \hat{x}_t + H \hat{y}_t$ to get
% \begin{align*}
%     \mathbb{E}\left(x_t - x^*\right) \left(x_t - x^* \right)^T 
%     = \mathbb{E} \tilde{x}_t \tilde{x}_t^T
%     + (L_t - H) \mathbb{E} \hat{y}_t \hat{y}_t^T (L_t - H)^T
%     - (L_t - H) \mathbb{E} \tilde{x}_t \hat{y}_t^T (L_t - H)^T .    
% \end{align*}
% Combining with $\lVert L_t \rVert = \mathcal{O}(\gamma_t/\alpha_t)$, this yields the rate
% \begin{equation}
%     \mathbb{E} \lVert x_t - x^*\rVert^2 = \mathcal{O}\left(\alpha_t \mathrm{Tr} \Sigma_{ff} 
%     + \gamma_t \mathrm{Tr} (H \Sigma_{ss} H^T + 2 H \Sigma_{sf})
%     + \gamma_t \mathrm{Tr} (H \Sigma_{fs} H^T) \right).
% \end{equation}
% \begin{theorem}\label{thm:mse}
%     Let $\Sigma$ be the asymptotic covariance of $(x_t - x^*, y_t - y^*)$, evaluated as
%     \begin{align*}
%         A_{ff} \Sigma_{ff} + \Sigma_{ff} A_{ff}^T &= \Gamma_{ff} , \\
%         A_{ff} \Sigma_{fs} + \Sigma_{ff} A_{sf}^T &= \Gamma_{fs} , \\ 
%         \Delta \Sigma_{ss} + \Sigma_{ss} \Delta^T - \bar{\gamma} \Sigma_{ss} + A_{sf} \Sigma_{fs} + \Sigma_{sf} A_{sf}^T &= \Gamma_{ss} .
%     \end{align*}
%     % Let $\nu_{ff} = \mathrm{Tr} A_{ff}, \nu_\Delta = \mathrm{Tr} \Delta$, and $M_f$ all be positive (problem-dependent) constants.
%     Under Assumptions \ref{assumption:first}--\ref{assumption:last},
%     \begin{align*}
%         % ff finite.
%         \mathbb{E} \lVert x_t - x^*\rVert^2 &= \mathcal{O}\left( \alpha_t \left(\frac{\nu_{ff}}{\mu_{ff}} \mathrm{Tr} \Sigma_{ff} \right) 
%         + \gamma_t\right) \\
%         % + \mathcal{O}\left(\gamma_t \mathrm{Tr}\left(\Sigma_{ss} + \Sigma_{sf} A_{sf}^T \right)\right) \\
%         % + \gamma_t M_f \mathrm{Tr}\left(H \Sigma_{ss} H^T + 2 H \Sigma_{sf}\right) + o\left(\frac{1}{t\alpha_t}\right) \\
%         \mathbb{E} \lVert y_t - y^* \rVert^2 &= 
%         \mathcal{O}\left(\gamma_t \left(\frac{\nu_\Delta}{\mu_\Delta}  \mathrm{Tr} \Sigma_{ss} \right) + 
%          \gamma_t \left(\alpha_t \mathrm{Tr} \Sigma_{ff} + \gamma_t \mathrm{Tr} A_{sf} \Sigma_{fs} \right)\right)  .
%         % \mathcal{O}\left(\alpha_t^2 \gamma_t\right) .
%     \end{align*}
% \end{theorem}


% Our next result is a non-asymptotic CLT for the Polyak-Ruppert averages $\bar{z}_n = (\bar{x}_n, \bar{y}_n)$.
% Utilizing the MSE rates of $x_t - x^*$ and $y_t - y^*$, we are able to show rates of convergence in the Wasserstein-1 distance defined below for random variables $X$ and $Y$
% \input{definitions/wasserstein_distance}\unskip
% Define the asymptotic covariance $\bar{\Sigma} = \lim_{n \to \infty} n^{1/2} \mathbb{E} \bar{z}_n \bar{z}_n^T$, which is evaluated to be
% \begin{equation}
%     \bar{\Sigma} =  DP \Gamma (DP)^T
% \end{equation}
% with $G = A_{ff} - A_{fs} A_{ss}^{-1} A_{sf}$ and $\Delta = A_{ss} - A_{sf} A_{ff}^{-1} A_{fs}$ given by Schur complements of $A_{ss}$ and $A_{ff}$, respectively, and
% \begin{equation} 
%     D = \begin{pmatrix}
%         G^{-1} & 0 \\ 0 & \Delta^{-1}
%     \end{pmatrix}, 
%     P = \begin{pmatrix}
%         I & - A_{fs} A_{ss}^{-1}
%         \\ 
%         - A_{sf} A_{ff}^{-1} & I
%     \end{pmatrix} .
% \end{equation}    
% Our non-asymptotic CLT in Theorem \ref{thm:clt} states that 
% \begin{equation}\label{eq:WassersteinBound}
%     d_1 \left(\sqrt{n}(\bar{z}_n - z^*), \bar{\Sigma}^{1/2} Z\right) = \mathcal{O}\left(
%     %     \frac{1}{\sqrt{n}} \left(
%     %     n^{a/2} + n^{a - b/2} + n^{b/2}
%     %     % n^{b/2} + n^{a - b/2} + n^{b-a}
%     %     \right)
%     % \right) ,
%         \frac{1}{\sqrt{n}}\left(n^{a/2}, n^{a-b}, n^{b/2}\right) ,
%     \right)
% \end{equation}
% where $Z \sim \mathcal{N}(0, I)$ is a standard normal variable.
% As long as $a, b$ satisfy Assumption \ref{assumption:steps}, we then obtain as an immediate consequence the optimal finite time bound for two timescale stochastic approximation: 
% \begin{equation}
%     \mathbb{E}\lVert \bar{z}_n - z^* \rVert \leq \frac{1}{\sqrt{n}} \mathbb{E}\lVert \bar{\Sigma}^{1/2} Z \rVert + \frac{1}{\sqrt{n}} d_1 \left(\sqrt{n}(\bar{z}_n - z^*), \bar{\Sigma}^{1/2} Z \right) 
%     = \frac{1}{\sqrt{n}} \mathbb{E} \lVert \bar{\Sigma}^{1/2} Z\rVert + o(n^{-1/2}) .
%     % = \mathcal{O}\left( n^{-1/2} \mathbb{E} \lVert \bar{\Sigma}^{1/2} Z \rVert\right) .
% \end{equation}
% This matches the $1/\sqrt{n}$ rate achieved by the theoretical baseline's error sequence $\{z^{opt}_n - z^*\}$, and the constant $\lVert \bar{\Sigma} \rVert^{1/2}$ is tighter than the $\sqrt{\mathrm{Tr} \Sigma^*}$ resulting from mean square analysis. 

% Here


% It is instructive to compare our result above with the optimal achievable rate.
% % Recall that the optimal baseline algorithm yields iterates $\{z^{opt}_n\}$ with a mean-square error
% $\mathbb{E}\lVert z^{opt}_n - z^* \rVert^2 = n^{-1} \mathrm{Tr}\bar{\Sigma}$.
% % Again, this is a theoretical baseline that cannot be outperformed by any TTSA algorithm that is unaware of the system parameters $A$.
% Using Jensen's inequality, the size of the expected norm can thus be estimated to be
% \begin{align*}
%     \mathbb{E} \lVert z^{opt}_n - z^* \rVert \leq n^{-1/2} \mathrm{Tr} \bar{\Sigma} .
% \end{align*}
% % In contrast, the expected norm in Eq. \eqref{eq:main_expected_norm_sketch} which we derive is stronger, matching the exact norm (in expectation) of the optimal TTSA algorithm.
% This comparison illustrates how our analysis of the Wasserstein-1 distance offers a deeper understanding of the Polyak-Ruppert average of TTSA.



% We note that the expected norm bounds can be hard to obtain when using the decoupling in \citep{konda2004convergence} and the induction step in \citep{polyakJuditsky} commonly used for m.s. analysis. 
% Instead, we use an alternative decomposition used by \citet{mokkadem2006convergence} and the non-asymptotic CLT Theorem in \citep{srikant2024CLT} to prove non-asymptotic convergence in distribution.  








% \subsection{Overview of Non-Asymptotic Central Limit Theorem and Expected Norms}
