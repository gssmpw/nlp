\section{Proof Sketches for Theorems \ref{thm:mse} and \ref{thm:clt}}
The analysis of two-time-scale algorithms is difficult because of the coupling between the fast- and slow-time-scale iterates.
Methods for establishing finite-time MSE bounds on TSA are often based on a decoupling technique introduced by \citet{konda2004convergence}. 
The complexity involved in the MSE analysis is further accentuated when analyzing the averaged iterates, because it involves the inner product between the averages which can be intractable. 
We observe that the Wasserstein-1 distance between errors achieved by TSA-PR and the limiting Gaussian variables can be made tractable by decomposing the TSA-PR errors as standardized martingale sums and weighted averages of the TSA following \citep{mokkadem2006convergence}. 
While \citet{mokkadem2006convergence} prove asymptotic convergence by establishing that the TSA converges almost surely to zero and that the martingales converge in distribution, proving the rates of convergence requires a different approach. 
By considering the Wasserstein-1 distance as the metric, we exploit the useful property in Lemma \ref{lem:slutsky} to bound the Wasserstein-1 distance with that of standardized martingale sums and terms that decay to zero in expected norm by Theorem \ref{thm:mse}. 
The martingale component converges in distribution to a non-zero limit, hence determines the limiting distribution.
It is shown that the expected error achieved by TSA dominates the bound on the Wasserstein-1 distance.
% Convergence of TTSA determines the rate of decay of the metric. 


\textbf{Proof of Theorem \ref{thm:mse}:}
Our mean square analysis starts by using the decoupling technique from \citep{konda2004convergence}, which is the cornerstone of two-time-scale analysis \citep{kaledin2020finite,haque2023tightfinitetimebounds}.
A derivation of the details is provided in Section \ref{sec:recursion}.
% The common technique of writing the second moments as a contraction leads to sub-optimal constants, whereas we show that the difference between the second moments and their asymptotic covariances is contractive. 
Recall the definition of the residues
\begin{equation}
    \hat{x}_t \coloneqq x_t - x_\infty (y_t), \quad \hat{y}_t \coloneqq y_t - y^* ,
\end{equation}
which measure the deviation from the steady-state solution $x_\infty (y)$ of the fast-time-scale system satisfying $F(x_\infty (y), y) = 0$ for every $y$.
Using a coordinate transformation 
\begin{equation}
    \tilde{x}_t = \hat{x}_t + L_t \hat{y}_t ,
\end{equation}
the recursions for $(\tilde{x}_t, \hat{y}_t)$ can then be written as updates with respect to a new system $B$ obtained by a change of basis on the system $A$:
\begin{equation}\label{eq:recursion_expression}
    \begin{split}
        \tilde{x}_{t+1} &= \tilde{x}_t - \alpha_t (B_t^{ff} \tilde{x}_t + B_t^{fs} \hat{y}_t) + \alpha_t W_t + \gamma_t (L_{t+1} + A_{ff}^{-1} A_{fs}) V_t ,
        \\
        \hat{y}_{t+1} &= \hat{y}_t - \gamma_t (B_t^{sf} \tilde{x}_t + B_t^{ss} \hat{y}_t) + \gamma_t V_t ,        
    \end{split}
\end{equation}
where the specific expression for the transformed system $B$ is deferred to Sections \ref{sec:fast_mse}--\ref{sec:slow_mse}.
When $\{L_t\}$ is defined recursively with initial condition $L_0 = 0$ as
\begin{equation}\label{eq:Lt}
    L_{t+1} \coloneqq \left(L_t - \alpha_t A_{ff} L_t + \gamma_t A_{ff}^{-1} A_{fs}(\Delta - A_{sf} L_t)\right) (I - \gamma_t (\Delta - A_{sf} L_t))^{-1}  ,
\end{equation}
the sequence $\{B_t^{fs}\}$ in Eq. \eqref{eq:recursion_expression} is zero for every $t$, effectively removing the explicit dependency of the fast-time-scale $\tilde{x}_t$ from the slow-time-scale $\hat{y}_t$.
A derivation for Eq. \eqref{eq:Lt} is provided in Section \ref{sec:recursion}
The recursion is shown to be well defined in Proposition \ref{prop:Lt_welldefined}, and the mean square error of the transformed iterate $\tilde{x}_t$ is then analyzed in Section \ref{sec:fast_mse}. 
Specifically, we show that for some constants $M_f', M_s' > 0$, 
\begin{equation}
    \begin{split}
        \lVert \mathbb{E} [\tilde{x}_{t+1} \tilde{x}_{t+1}^T ] - \alpha_{t+1} \Sigma_{ff} \rVert &\leq \left(1 - \alpha_t \frac{\mu_{ff}}{2} + \gamma_t M_f'\right) \lVert \mathbb{E} \tilde{x}_t \tilde{x}_t^T - \alpha_t \Sigma_{ff} \rVert + \mathcal{O}\left(\alpha_t \gamma_t \right)
            , 
        \\ 
        \lVert \mathbb{E} [\hat{y}_{t+1} \hat{y}_{t+1}^T ] - \gamma_{t+1} \Sigma_{ss} \rVert &\leq \left(1 - \gamma_t \frac{\mu_\Delta}{2} + \frac{\gamma_t^2}{\alpha_t} M_s'\right) \lVert \mathbb{E}\hat{y}_t \hat{y}_t^T - \gamma_t \Sigma_{ss} \rVert + \mathcal{O}\left(\gamma_t^2\right) .
    \end{split}
\end{equation}
Once this is shown, a standard induction step can be applied with the choice of step sizes in Assumption \ref{assumption:steps} to obtain Theorem \ref{thm:mse}.


The second moment of $x_t - x^*$ is then recovered using the identities $x_t - x^* = \hat{x}_t + (x_\infty (y_t) - x^*) = \hat{x}_t + H \hat{y}_t$ to get
\begin{align*}
    \mathbb{E}\left(x_t - x^*\right) \left(x_t - x^* \right)^T 
    = \mathbb{E} \tilde{x}_t \tilde{x}_t^T
    + (H - L_t ) \mathbb{E} \hat{y}_t \hat{y}_t^T (H - L_t)^T
    + \mathbb{E} \tilde{x}_t \hat{y}_t^T (H - L_t)^T .    
\end{align*}
It is shown in Sections \ref{sec:joint_mse} and \ref{sec:slow_mse} that $\lVert \mathbb{E} \tilde{x}_t \hat{y}_t^T \rVert = \mathcal{O}(\gamma_t)$ and $\lVert \mathbb{E} \hat{y}_t \hat{y}_t^T \rVert = \mathcal{O}(\gamma_t)$.
Proposition \ref{prop:Lt_welldefined} establishes that $\lVert L_t \rVert$ is uniformly bounded, from which we recover the mean square error of $x_{t+1} - x^*$ as
\begin{equation}\label{eq:fast_notransformation_mse}
    \mathrm{Tr} \mathbb{E} (x_{t+1} - x^*) (x_{t+1} - x^*) 
    \leq
    \alpha_t \mathrm{Tr} \Sigma_{ff} 
    + \mathcal{O}\left(\gamma_t \right) .
    % + \gamma_t \left(M_f (I + \Sigma_{ff}) + H \Sigma_{ss} H^T + \Sigma_{fs} H^T\right)
    % + o(\gamma_t) .
    % + \gamma_t H \left(\Sigma_{ss} -  H^T 
    % + \mathcal{O}\left(\gamma_t \mathrm{Tr} (H \Sigma_{ss} H^T + H \Sigma_{sf})
    % + \gamma_t \mathrm{Tr} (H \Sigma_{fs} H^T) \right).
\end{equation}
This completes the mean square error bounds. 


\textbf{Proof of Theorem \ref{thm:clt}:}
Without loss of generality, let $x^* = 0$ and $y^* = 0$.
In Section \ref{sec:clt}, following \citet{mokkadem2006convergence}, we then express the PR averages as a telescoped sum
\begin{equation}
    \begin{split}
        G \bar{x}_n  &= \frac{1}{n} \sum_{t=1}^n (W_t - A_{fs} A_{ss}^{-1} V_t) + \frac{1}{n} \sum_{t=1}^n \alpha_t^{-1} (x_{t} - x_{t+1}) - \frac{1}{n} \sum_{t=1}^n \gamma_t^{-1} A_{fs} A_{ss}^{-1} (y_t - y_{t+1}) \\ 
        \Delta \bar{y}_n &= \frac{1}{n} \sum_{t=1}^n (V_t - A_{sf} A_{ff}^{-1} W_t) 
        + \frac{1}{n}\sum_{t=1}^n A_{sf} A_{ff}^{-1} \alpha_t^{-1} (x_{t} - x_{t+1})
        + \frac{1}{n}\sum_{t=1}^n \gamma_t^{-1} (y_{t} - y_{t+1})  .    
    \end{split}
    \label{eq:polyak_ruppert_expression}
\end{equation}
After scaling Eq. \eqref{eq:polyak_ruppert_expression} by $\sqrt{n}$, we see that the first terms are standardized sums of martingales which converge in distribution. 
To obtain their rates of convergence, we use the quantitative bound from \citep{srikant2024CLT} on the Wasserstein-1 distance between a standardized sum of martingales and its limiting Gaussian variable. 
\begin{lemma}\label{lem:CLT}
    Let $\{N_t\}$ be a martingale difference sequence in $\mathbb{R}^d$ satisfying $\mathbb{E}N_t N_t^T = \Gamma$ and $\mathbb{E}\lVert N_t \rVert^{2 + \beta}<\infty$ for every $t \geq 1$ and some $\beta \in (0, 1)$. 
    For a standard Gaussian vector $Z$, it holds that
    \begin{equation}
        d_1 \left(n^{-1/2} \sum_{t=1}^n N_t , \Gamma^{1/2} Z \right) \leq \mathcal{O}\left(\frac{d}{1 - \beta}\right) \frac{\lVert \Gamma^{1/2} \rVert \lVert \Gamma^{-1/2}\rVert^{2 + \beta}}{n^{\beta/2}} .
    \end{equation}
\end{lemma}
Next, we relate the telescoped terms in Eq. \eqref{eq:polyak_ruppert_expression} to their respective errors in expectation as
\begin{equation}
    \begin{split}
        \mathbb{E}\lVert \sum_{t=1}^n \alpha_t^{-1} (x_t - x_{t+1}) \rVert 
            &\leq 
        \alpha_1^{-1} \mathbb{E} \lVert x_1 \rVert + \alpha_n^{-1} \mathbb{E}\lVert x_n \rVert + \alpha_1^{-1} \sum_{t=1}^n t^{a-1} \mathbb{E}\lVert x_t \rVert  
        \\
        \mathbb{E}\lVert \sum_{t=1}^n \gamma_t^{-1} (y_t - y_{t+1}) \rVert 
            &\leq 
        \gamma_1^{-1} \mathbb{E} \lVert y_1 \rVert + \gamma_n^{-1} \mathbb{E}\lVert y_n \rVert + \gamma_1^{-1} \sum_{t=1}^n t^{b-1} \mathbb{E}\lVert y_t \rVert .
    \end{split}
    \label{eq:pr_expression}
\end{equation}
The expected errors of $x_t$ and $y_t$ for every $t \leq n$ are recovered from Theorem \ref{thm:mse} and Jensen's inequality, which gives $\mathbb{E}\lVert x_t \rVert = \mathcal{O}(t^{-a/2})$ and $\mathbb{E}\lVert y_t \rVert = \mathcal{O}(t^{-b/2})$.
Combining, we conclude that the sum of telescoped sums in Eq. \eqref{eq:pr_expression} grows with $n^{a/2}, n^{b/2}$, and $n^{a-b/2}$, where the $a-b/2$ is obtained from the $\gamma_{n+1}$ dependence of $\mathbb{E}\lVert \hat{x}_n \rVert^2$ in Eq. \eqref{eq:fast_notransformation_mse}.
% on the convergence of $\mathrm{Tr} (\mathbb{E} \hat{x}_{n+1} \hat{x}_{n+1}^T - \alpha_{n+1} \Sigma_{ff})$ deduced from Theorem \ref{thm:mse}. 



To combine these results, we then use the following property of the Wasserstein-1 distance.
\begin{lemma}\label{lem:slutsky}
    % I.e. this is Lindeberg's decomposition.
    Consider two random sequences $\{X_t\}, \{Y_t\}$ such that $d_1 (X_t, X) \leq r_t$ for some random variable $X$ and $\mathbb{E}\lVert Y_t \rVert \leq r'_t$.
    Then, $d_1 (X_t + Y_t, X) \leq r_t + r'_t$.
\end{lemma}
% To finish, we use the quantitative bound for convergence of vector martingale sums to a normal distribution in Theorem 1 of \citep{srikant2024CLT}.
Theorem \ref{thm:clt} is then obtained by applying Lemma \ref{lem:slutsky} to bound the distance between $G \bar{x}_n$ and its Gaussian limit with the Wasserstein-1 distance between the martingale terms in Eq. \eqref{eq:polyak_ruppert_expression} and the expected norms in Eq. \eqref{eq:pr_expression}. 

% Applying Lemma \ref{lem:CLT} to the first term in Eq. \eqref{eq:polyak_ruppert_expression} 
% Together with Eq. \eqref{eq:polyak_ruppert_expression}, we obtain Theorem \ref{thm:clt}.
% where the bound is dominated by the mean absolute errors of the last iterates. 


To obtain Corollary \ref{cor:mae}, we then use $h(x) = \lVert x \rVert$ as the 1-Lipschitz test function for the Wasserstein-1 distance in Eq. \eqref{eq:wasserstein_definition} to obtain
\begin{equation}
    \begin{split}
        \mathbb{E}\lVert G \bar{x}_n\rVert - n^{-1/2} \mathbb{E} \lVert (G \bar{\Sigma}_{ff} G^T)^{1/2} Z_1 \rVert 
        &\leq d_1 \left(G \bar{x}_n, n^{-1/2}(G \bar{\Sigma}_{ff} G^T)^{1/2} Z_1 \right) = \mathcal{O}(n^{b/2 - 1}) ,
        \\
        \mathbb{E}\lVert \Delta \bar{y}_n \rVert - n^{-1/2} \mathbb{E} \lVert (\Delta \bar{\Sigma}_{ss} \Delta^T)^{1/2} Z_2 \rVert 
        &\leq d_1 \left(\Delta \bar{y}_n, n^{-1/2} (\Delta \bar{\Sigma}_{ss} \Delta^T)^{1/2} Z_2 \right) = \mathcal{O}(n^{b/2 - 1}) .
    \end{split}
\end{equation}
Because $b < 1$, we have that $\mathbb{E} \lVert G \bar{x}_n \rVert$ and $\mathbb{E} \lVert \Delta \bar{y}_n \rVert$ are both $\mathcal{O}(n^{-1/2})$.
% , where the inequality in the above equation uses that the function  is 1-Lipschitz and therefore is a valid test function in the definition of the Wasserstein-1 distance (Eq. \eqref{eq:wasserstein_definition}).  
The lower bound is obtained by using the test function $-\lVert x \rVert$. 