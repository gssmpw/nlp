\section{Discussion}
The mean absolute error derived in Eq. \eqref{eq:mae} is asymptotically optimal. 
% Optimal rate for Polyak-Ruppert averaging. 
Several remarks follow. 


\textbf{Optimality of the rate $n^{-1}$:} % Obtained by Polyak-Ruppert averaging. 
First, we are the first to obtain the optimal finite time bound on $n^{-1}$ for TTSA. 
All of these papers \citep{kaledin2020finite,dalal2018finite} are only for last iterates. 
The paper \citep{mokkadem2006convergence} is only asymptotic. 


\textbf{Constants:} % Comparison with higher moment bounds. 
Higher moment bounds, even in the case of single timescale algorithms, are obtained in \citep{srikant2019,wainwrightBartlettJordan,durmus2024finite} for constant step sizes. 
The resulting bounds involve a correction term because constant step sizes are not asymptotically optimal, and are expressed in terms of the trace of the covariance matrix. 
While error bounds on the higher moments ensure convergence in a strong sense, they fail to capture the correct constants when considering the mean absolute error. 



\textbf{Central Limit Theorems:} % Only established for single timescale algorithms.
But there are no finite time central limit theorems for TTSA.
\citet{samsonov2024gaussian,srikant2024CLT}


The important consequence of optimal mean absolute error in Eq. \eqref{eq:mae} was obtainable only because we chose the Wasserstein-1 distance. 
Berry-Esseen type bounds established for single timescale in \citep{samsonov2024gaussian} do not give such bounds. 
Higher moment bounds \citep{durmus2024finite} are expressed in terms of the trace, whereas deducing an MAE in terms of the expected norm requires additional work. 


{\color{red}
Discuss a few single timescale CLTs, to highlight how ours is the first ttsa result.

Methods like \citep{durmus2024finite} only give second moments and higher (fixed step size); but they do not get the optimal MAE.
The optimal MAE is only attained via the proper choice of test function $h$, which admits regulatory properties due to Stein's equation.

Our choice of metric is what gives us the optimal rate.
}


Maybe also mention the implication to Kolmogorov Smirnov test? 
Ie distributions match. 


% We have derived quantitative bounds on the central limit theorem for two timescale stochastic approximation. 
% A major consequence was that the optimal mean absolute error in Eq. \eqref{eq:mae} was derived. 
% Here we discuss 






{\color{red}Maybe we can demonstrate a few examples/applications, and instantiate our bounds here? 
Related work can also come here if adequate. 
\begin{enumerate}
    \item Construct $A$ and the distribution of $W, V$ such that $\lVert \bar{\Sigma} \rVert \ll \mathrm{Tr} \bar{\Sigma}$.
    This would occur if the condition number is large, so that the trace is $d$ but the operator norm is $\sqrt{d}$. 
    The paper \citep{samsonov2024gaussian} (very recent: NeurIPS 2024) seems to analyze Berry-Esseen CLT for single timescale, iid SA Polyak-Ruppert average. 
    Their bound is $\sqrt{\mathrm{Tr} \Sigma}$. 
    Hence, we can position our paper by comparing with this. 
    But theirs concerns the Berry-Essen (test functions are indicators), whereas ours is for Waasserstein 1 distance. 
    \item Similar to above, we could also mention implications of Wasserstein-1 distance for various 1-Lipschitz test functions. This would serve to demonstrate benefit over Berry-Esseen-type bounds. 
    \item \citet{durmus2024finite} obtains high probability bounds for $p \geq 2$ moments. Of course, this is expressed in terms of $\mathrm{Tr}$. Again, it may be worth emphasizing how all of these are loose in this constant; moreover, high probability bounds may not be tight. 
    \item \citep{anastasiou2019normal} is another paper regarding Stein's method for PR averaged single timescale CLT. Their dimension dependence is worse, with $d^2$ than \citep{srikant2024CLT} and their assumptions might be a bit tighter. 
    \item Discretized forward-backward diffusion problems? 
\end{enumerate}
}

% This part can be dedicated to the expected norm? 
% It can also be a subsection. 


% If we don't do averaging, we get bad rates and it may even diverge. 
% Step size for averaging uses this choice, whereas last iterates use another set of step sizes. 


% Slow last iterate converges nearly optimally but with bad constant. 
% Fast iterate converges at a poor rate. 
% But averaging makes both converge fast. 


% \subsection{Comparing Asymptotic Covariances: Ongoing}
% Can we show that 

% The asymptotic covariances $\bar{\Sigma}$ and $\Sigma$ can be compared as follows.
% First,
% \begin{align*}
%     G \bar{\Sigma} G  =  \Gamma_{ff} =  A_{ff} \Sigma_{ff} + \Sigma_{ff} A_{ff}^T  
% \end{align*}
%     % \geq 
%     % 2\lVert A_{ff}^{1/2} \Sigma_{ff} A_{ff}^{1/2,T} \rVert .
% From the Lyapunov equation for $\Sigma_{ff}$, we have that (\citet{bhatia2009positive}, Eq. 5.25)

