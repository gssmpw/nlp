\section{Remaining Tasks and Notes}
% Main results are: 
% \begin{enumerate}
%     \item Optimal MSE rates (Lemmas \ref{lem:fast_mse}, \ref{lem:joint_mse}, \ref{lem:slow_mse}).
%     The rates are optimal in the sense of Cramer-Rao: A sum of iid random variables has mse matching the covariance trace (trivially true). 
%     Therefore, stochastic approximation has to have a MSE bounded by the covariance trace (of the signal). 
%     Mine matches this lower bound.
    
%     \item More precisely, I derive a lower bound that matches the upper bound up to multiplicative constants.    
%     My method seems to make it easy to derive lower bounds as well. I have derived the lower bound for the fast iterate, see Lemma \ref{lem:fast_lb}.
%     \item If desired, I can proceed to derive the LBs for the joint and slow iterates.
% \end{enumerate}


% Notes for discussion:
% \begin{enumerate}
%     \item (Step size) The requirement $b < 1$ is needed for the Polyak-Ruppert average to converge at a fast rate (converge at all for our CLT technique). 
%     This is the case even in STSA. 
%     The requirement $a < b$ is needed for the fast last iterate to converge; this is used for instance to ensure that $1 - \alpha_t \nu_{ff} + \gamma_t m_{ff} \leq \alpha_t/2$ up to constant multiples. 
%     \item I don't think I need to prove the MSE of the Polyak-Ruppert average. 
%     It can be overcome by setting $\gamma_1 \ll \alpha_1$ or something. 
    
%     This was to motivate how our technique provides more information than a MSE analysis. 
%     Instead, I included an ``optimal baseline algorithm'' whose MSE is optimal (depends on trace of covariance). 
%     The benefit of our technique is illustrated by how even for this optimal algorithm, Jensen's inequality + MSE bounds can only yield the sub-optimal expected norm. 
%     On the other hand, our expected norm matches this optimal algorithm's expected norm. 
% \end{enumerate}
To do:
\begin{enumerate}
    % \item It's clear where I use $a < b < 1$ (see ``useful inequalities''). 
    % But where do I use $a , b > 1/2$? 
    % Experiments suggest that even last iterates converge when $a, b < 1/2$. 
    % I don't think I require $a, b > 1/2$, but rather just $b > a > 0$; a similar allowance is obtained in \citep{dalal2018finite}, Remark 2.
    
    \item Additional related works? 
    Should we add a section on discussion/applications, instantiating our bound for certain problems? 
    For example, constructing $A, \Gamma$ such that the asymptotic covariance has large MSE but small MAE (expected norm vs. trace; i.e. large eigengap).
    It may also be useful to numerical validate how large this can be in characterizing statistical tests. 
    If we also simulate these examples, we can just put them under the Experiments section. 

    

    % (Presentation): (1) Introduction, (2) Problem Statement, (3) Main Results (Statements: Finite MSE, Our expected norm, LB on last iterates), (4) Numerical Experiments, (5) Conclusion. (6) Proofs and auxiliary results in Appendix. 
    % Conclusion could include discussions that ideally, we want the Polyak-Ruppert analysis to hold for $a, b \leq 1/2$ as well. 
    % But our requirement stems from how we use MSE results for CLT. 

    \item 
    Experiments
    \begin{enumerate}
        \item I've noticed that small step size results in last iterate performing better than averaged. 

        
        \item Effect on $\alpha_t, \gamma_t$ orders: Can we see transient behavior in the initial steps? 
        \item Do the step sizes really have no effect after long iterations? Looking at figures truncated to e.g. $t \geq 100$, are the curves similar for various values of $a, b \in (1/2, 1)$?
        \item Is the requirement $a, b > 1/2$ an analysis artifact? 
        This is required for the MSE analysis, but may not be required for the Polyak-Ruppert averaging if analyzed directly without MSE last iterates as an intermediate result. 
        Check through simulations if setting $a, b = 1/4$ (maybe $b > a$) that the last iterates diverge, and that the Polyak-Ruppert average converges at the same rate. 
        \item Comparison of Polyak-Ruppert average with the optimal baseline; do they match asymptotically? When does the gap start to decrease?
        \item Our lower and upper bounds are in terms of spectrums of $A_{ff}$ and $\Delta$; more specifically, their trace. 
        If we fix $\mu, \nu$, but change other properties (e.g. sample eigenvalues u.a.r from this bound and eigenvector matrix u.a.r), do we see different behaviors?
    \end{enumerate}
\end{enumerate}
