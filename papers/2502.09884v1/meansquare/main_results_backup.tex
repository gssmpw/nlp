\subsection{Main Results}
{\color{red}This can be a entire section where we state the main Lemmas and Theorems.}
Recall $x_\infty (y)$ is the steady-state solution for the fast timescale system $F(x, y)$.
For mean-square analysis, we consider a linear transformation of the residues
\begin{equation}
    \hat{x}_t \coloneqq x_t - x_\infty (y_t), \quad \hat{y}_t \coloneqq y_t - y^* 
\end{equation}
using some coordinate transformation $L_t$ such that
\begin{equation}
    \tilde{x}_t = \hat{x}_t + L_t \hat{y}_t .
\end{equation}
A suitable choice of $L_t$ is defined recursively with initial condition $L_0=0$ as
\begin{equation}\label{eq:Lt}
    L_{t+1} \coloneqq \left(L_t - \alpha_t A_{ff} L_t + \gamma_t A_{ff}^{-1} A_{fs}(\Delta - A_{sf} L_t)\right) (I - \gamma_t (\Delta - A_{sf} L_t))^{-1}  .
\end{equation}
This particular coordinate transformation, if well-defined, results in a fast timescale $\tilde{x}_t$ which is independent of $\hat{y}_t$:
\begin{align*}
    \tilde{x}_{t+1} &= (I - \alpha_t B_t^{ff}) \tilde{x}_t + \alpha_t W_t + \gamma_t (L_{t+1} + A_{ff}^{-1} A_{fs}) V_t ,
    \numberthis \label{eq:fast_recursion_preview}
    \\
    B^{ff}_t &= \frac{\gamma_t}{\alpha_t}(L_{t+1} + A_{ff}^{-1} A_{fs}) A_{sf} + A_{ff} .    
\end{align*}
A derivation of this result is given in Section \ref{sec:recursion_mse}.
Equation \eqref{eq:Lt} is indeed well-defined at some point $t \geq 1$ when $\gamma_t$ is sufficiently as noted after Assumption \ref{assumption:steps}, whose condition is verified in Proposition \ref{prop:Lt_welldefined}.
We show in Lemma \ref{lem:Ltsize} that $\lVert L_t \rVert = \mathcal{O}\left(\frac{\gamma_t}{\alpha_t}\right)$, which implies $\lim_{t \to \infty} \lVert \tilde{x}_t - \hat{x}_t \rVert = 0$ whenever $\lim_{t \to \infty} \lVert \hat{y}_t \rVert$ exists.
The recursion in Eq. \eqref{eq:fast_recursion_preview} can then be analyzed using single timescale analysis. 
The rate established for $\mathbb{E} \tilde{x}_t \tilde{x}_t^T$ is then used to analyze the cross timescale $\mathbb{E} \tilde{x}_t \hat{y}_t^T$, and in turn both are used to analyze $\mathbb{E} \hat{y}_t \hat{y}_t^T$.
Specifically, we show that the two timescale mean square errors are related to their asymptotic covariances
\begin{equation}
    \lim_{t \to \infty} \alpha_t^{-1} \mathbb{E} \tilde{x}_t \tilde{x}_t^T = \Sigma_{ff} , \; 
    \lim_{t \to \infty} \gamma_t^{-1} \mathbb{E} \hat{y}_t \hat{y}_t^T = \Sigma_{ss} , 
    \; 
     \lim_{t \to \infty} \gamma_t^{-1} \mathbb{E} \hat{y}_t \tilde{x}_t^T = \Sigma_{sf} .
\end{equation}
The asymptotic covariances were evaluated in \citep{konda2004convergence}, and we show in Sections \ref{sec:fast_mse}--\ref{sec:slow_mse} that the errors at finite $t \geq 1$ are of order
\begin{equation}
    \mathrm{Tr}\mathbb{E}\tilde{x}_t \tilde{x}_t^T = \mathcal{O}\left(\alpha_t \mathrm{Tr} \Sigma_{ff} + \gamma_t\right), 
    \;
    \mathrm{Tr} \mathbb{E}\hat{y}_t \hat{y}_t^T = \mathcal{O}\left(\gamma_t \mathrm{Tr} \Sigma_{ss} + \gamma_t \left(\alpha_t \mathrm{Tr} \Sigma_{ff} + \gamma_t \mathrm{Tr} A_{sf} \Sigma_{fs}\right) \right) .
\end{equation}
The MSE of $\hat{x}_t$ is then recovered using $x_t - x^* = \hat{x}_t + (x_\infty (y_t) - x^*) = \hat{x}_t + H \hat{y}_t$ to get
\begin{align*}
    \mathbb{E}\left(x_t - x^*\right) \left(x_t - x^* \right)^T 
    = \mathbb{E} \tilde{x}_t \tilde{x}_t^T
    + (L_t - H) \mathbb{E} \hat{y}_t \hat{y}_t^T (L_t - H)^T
    - (L_t - H) \mathbb{E} \tilde{x}_t \hat{y}_t^T (L_t - H)^T .    
\end{align*}
Combining with $\lVert L_t \rVert = \mathcal{O}(\gamma_t/\alpha_t)$, this yields the rate
\begin{equation}
    \mathbb{E} \lVert x_t - x^*\rVert^2 = \mathcal{O}\left(\alpha_t \mathrm{Tr} \Sigma_{ff} 
    + \gamma_t \mathrm{Tr} (H \Sigma_{ss} H^T + 2 H \Sigma_{sf})
    + \gamma_t \mathrm{Tr} (H \Sigma_{fs} H^T) \right).
\end{equation}
\begin{theorem}\label{thm:mse}
    Let $\Sigma$ be the asymptotic covariance of $(x_t - x^*, y_t - y^*)$, evaluated as
    \begin{align*}
        A_{ff} \Sigma_{ff} + \Sigma_{ff} A_{ff}^T &= \Gamma_{ff} , \\
        A_{ff} \Sigma_{fs} + \Sigma_{ff} A_{sf}^T &= \Gamma_{fs} , \\ 
        \Delta \Sigma_{ss} + \Sigma_{ss} \Delta^T - \bar{\gamma} \Sigma_{ss} + A_{sf} \Sigma_{fs} + \Sigma_{sf} A_{sf}^T &= \Gamma_{ss} .
    \end{align*}
    % Let $\nu_{ff} = \mathrm{Tr} A_{ff}, \nu_\Delta = \mathrm{Tr} \Delta$, and $M_f$ all be positive (problem-dependent) constants.
    Under Assumptions \ref{assumption:first}--\ref{assumption:last},
    \begin{align*}
        % ff finite.
        \mathbb{E} \lVert x_t - x^*\rVert^2 &= \mathcal{O}\left( \alpha_t \left(\frac{\nu_{ff}}{\mu_{ff}} \mathrm{Tr} \Sigma_{ff} \right) 
        + \gamma_t\right) \\
        % + \mathcal{O}\left(\gamma_t \mathrm{Tr}\left(\Sigma_{ss} + \Sigma_{sf} A_{sf}^T \right)\right) \\
        % + \gamma_t M_f \mathrm{Tr}\left(H \Sigma_{ss} H^T + 2 H \Sigma_{sf}\right) + o\left(\frac{1}{t\alpha_t}\right) \\
        \mathbb{E} \lVert y_t - y^* \rVert^2 &= 
        \mathcal{O}\left(\gamma_t \left(\frac{\nu_\Delta}{\mu_\Delta}  \mathrm{Tr} \Sigma_{ss} \right) + 
         \gamma_t \left(\alpha_t \mathrm{Tr} \Sigma_{ff} + \gamma_t \mathrm{Tr} A_{sf} \Sigma_{fs} \right)\right)  .
        % \mathcal{O}\left(\alpha_t^2 \gamma_t\right) .
    \end{align*}
\end{theorem}


Our next result is a non-asymptotic CLT for the Polyak-Ruppert averages $\bar{z}_n = (\bar{x}_n, \bar{y}_n)$.
Utilizing the MSE rates of $x_t - x^*$ and $y_t - y^*$, we are able to show rates of convergence in the Wasserstein-1 distance defined below for random variables $X$ and $Y$
\input{definitions/wasserstein_distance}\unskip
Define the asymptotic covariance $\bar{\Sigma} = \lim_{n \to \infty} n^{1/2} \mathbb{E} \bar{z}_n \bar{z}_n^T$, which is evaluated to be
\begin{equation}
    \bar{\Sigma} =  DP \Gamma (DP)^T
\end{equation}
with $G = A_{ff} - A_{fs} A_{ss}^{-1} A_{sf}$ and $\Delta = A_{ss} - A_{sf} A_{ff}^{-1} A_{fs}$ given by Schur complements of $A_{ss}$ and $A_{ff}$, respectively, and
\begin{equation} 
    D = \begin{pmatrix}
        G^{-1} & 0 \\ 0 & \Delta^{-1}
    \end{pmatrix}, 
    P = \begin{pmatrix}
        I & - A_{fs} A_{ss}^{-1}
        \\ 
        - A_{sf} A_{ff}^{-1} & I
    \end{pmatrix} .
\end{equation}    
Our non-asymptotic CLT in Theorem \ref{thm:clt} states that 
\begin{equation}\label{eq:WassersteinBound}
    d_1 \left(\sqrt{n}(\bar{z}_n - z^*), \bar{\Sigma}^{1/2} Z\right) = \mathcal{O}\left(
    %     \frac{1}{\sqrt{n}} \left(
    %     n^{a/2} + n^{a - b/2} + n^{b/2}
    %     % n^{b/2} + n^{a - b/2} + n^{b-a}
    %     \right)
    % \right) ,
        \frac{1}{\sqrt{n}}\left(n^{a/2}, n^{a-b}, n^{b/2}\right) ,
    \right)
\end{equation}
where $Z \sim \mathcal{N}(0, I)$ is a standard normal variable.
As long as $a, b$ satisfy Assumption \ref{assumption:steps}, we then obtain as an immediate consequence the optimal finite time bound for two timescale stochastic approximation: 
\begin{equation}
    \mathbb{E}\lVert \bar{z}_n - z^* \rVert \leq \frac{1}{\sqrt{n}} \mathbb{E}\lVert \bar{\Sigma}^{1/2} Z \rVert + \frac{1}{\sqrt{n}} d_1 \left(\sqrt{n}(\bar{z}_n - z^*), \bar{\Sigma}^{1/2} Z \right) 
    = \frac{1}{\sqrt{n}} \mathbb{E} \lVert \bar{\Sigma}^{1/2} Z\rVert + o(n^{-1/2}) .
    % = \mathcal{O}\left( n^{-1/2} \mathbb{E} \lVert \bar{\Sigma}^{1/2} Z \rVert\right) .
\end{equation}
This matches the $1/\sqrt{n}$ rate achieved by the theoretical baseline's error sequence $\{z^{opt}_n - z^*\}$, and the constant $\lVert \bar{\Sigma} \rVert^{1/2}$ is tighter than the $\sqrt{\mathrm{Tr} \Sigma^*}$ resulting from mean square analysis. 




% It is instructive to compare our result above with the optimal achievable rate.
% % Recall that the optimal baseline algorithm yields iterates $\{z^{opt}_n\}$ with a mean-square error
% $\mathbb{E}\lVert z^{opt}_n - z^* \rVert^2 = n^{-1} \mathrm{Tr}\bar{\Sigma}$.
% % Again, this is a theoretical baseline that cannot be outperformed by any TTSA algorithm that is unaware of the system parameters $A$.
% Using Jensen's inequality, the size of the expected norm can thus be estimated to be
% \begin{align*}
%     \mathbb{E} \lVert z^{opt}_n - z^* \rVert \leq n^{-1/2} \mathrm{Tr} \bar{\Sigma} .
% \end{align*}
% % In contrast, the expected norm in Eq. \eqref{eq:main_expected_norm_sketch} which we derive is stronger, matching the exact norm (in expectation) of the optimal TTSA algorithm.
% This comparison illustrates how our analysis of the Wasserstein-1 distance offers a deeper understanding of the Polyak-Ruppert average of TTSA.



% We note that the expected norm bounds can be hard to obtain when using the decoupling in \citep{konda2004convergence} and the induction step in \citep{polyakJuditsky} commonly used for m.s. analysis. 
% Instead, we use an alternative decomposition used by \citet{mokkadem2006convergence} and the non-asymptotic CLT Theorem in \citep{srikant2024CLT} to prove non-asymptotic convergence in distribution.  








% \subsection{Overview of Non-Asymptotic Central Limit Theorem and Expected Norms}
