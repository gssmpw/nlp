\section{Non-Asymptotic CLT for TSA-PR}\label{sec:clt}
We establish finite-time bounds on the Wasserstein-1 distance defined in Eq. \eqref{eq:wasserstein_definition} between the deviation of the Polyak-Ruppert average $(\bar{x}_n, \bar{y}_n)$ around the solution (zeros) and the limiting Gaussian vector.
Here we restate Theorem \ref{thm:clt} for reference.
\begin{theorem}
    \CLT
\end{theorem}
The asymptotic covariances $\bar{\Sigma}_{ff}$ and $\bar{\Sigma}_{ss}$ satisfy
\begin{equation}
    \begin{split}
        G \bar{\Sigma}_{ff} G^T
        &= \Gamma_{ff} + A_{fs} A_{ss}^{-1} \Gamma_{ss} (A_{fs} A_{ss}^{-1})^T 
        - \left(\Gamma_{fs} (A_{fs} A_{ss}^{-1})^T + A_{fs} A_{ss}^{-1} \Gamma_{sf}\right)  \\
        \Delta \bar{\Sigma}_{ss} \Delta^T
        &= \Gamma_{ss} + A_{sf} A_{ff}^{-1} \Gamma_{ff} (A_{sf} A_{ff}^{-1})^T
        - \left(\Gamma_{sf} (A_{sf} A_{ff}^{-1})^T + A_{sf} A_{ff}^{-1} \Gamma_{fs}\right)         
    \end{split} . \label{eq:ttsapr_covariance}
\end{equation}


\subsection{Proof of Theorem \ref{thm:clt}}
\textbf{Step 1:} We write the recursions \eqref{eq:ttsa} in the form
% We follow \citep{mokkadem2006convergence} and write the fast and slow timescale recursions as
\begin{align*}
    G x_t = (W_t - A_{fs} A_{ss}^{-1} V_t) + \alpha_t^{-1} (x_t - x_{t+1}) - \gamma_t^{-1} A_{fs} A_{ss}^{-1} (y_t - y_{t+1}) \numberthis \label{eq:fast_last} \\ 
    \Delta y_t = (V_t - A_{sf} A_{ff}^{-1} W_t) + \gamma_t^{-1} (y_t - y_{t+1}) - A_{sf} A_{ff}^{-1} \alpha_t^{-1} (x_{t+1} - x_t) ,
\end{align*}
where $G, \Delta$ are the Schur complements
\begin{align*}
    G = A_{ff} - A_{fs} A_{ss}^{-1} A_{sf} , \quad
    \Delta = A_{ss} - A_{sf} A_{ff}^{-1} A_{fs} .
\end{align*}
\begin{proof}    
    Let us start with the fast-time-scale variable update
    \begin{align*}
        x_{t+1} &= x_t - \alpha_t (A_{ff} x_t + A_{fs} y_t - W_t) 
        \\
        \Rightarrow x_t &= (\alpha_t A_{ff})^{-1} (x_{t+1} - x_t) - A_{ff}^{-1} (A_{fs} y_t - W_t) .
    \end{align*}
    Substituting into the slow-time-scale variable's recursion,
    \begin{align*}
        y_{t+1} &= y_t - \gamma_t \left(
            A_{sf} x_t + A_{ss} y_t - V_t 
        \right)
        \\ 
        &= y_t - \gamma_t A_{sf} \left( A_{ff}^{-1} \alpha_t^{-1} (x_{t+1} - x_t) \right) + \gamma_t A_{sf} A_{ff}^{-1} A_{fs} y_t - \gamma_t A_{sf} A_{ff}^{-1} W_t
        - \gamma_t A_{ss} y_t + \gamma_t V_t .
    \end{align*}
    Using $\Delta = A_{ss} - A_{sf} A_{ff}^{-1} A_{fs}$, we have 
    \begin{align*}
         y_{t+1} &= (I - \gamma_t \Delta) y_t - \frac{\gamma_t}{\alpha_t} A_{sf} A_{ff}^{-1} (x_{t+1} - x_t) + \gamma_t (V_t - A_{sf} A_{ff}^{-1} W_t)  \numberthis \label{eq:prelim}\\
        \Leftrightarrow 
         \gamma_t \Delta y_t &=  y_t - y_{t+1} - \frac{\gamma_t}{\alpha_t} A_{sf} A_{ff}^{-1} (x_{t+1} - x_t) + \gamma_t (V_t - A_{sf} A_{ff}^{-1} W_t).
    \end{align*}
    Dividing both sides by the step size $\gamma_t$, we have
    \begin{equation}
        \Delta y_t = \gamma_t^{-1} (y_t - y_{t+1}) - \alpha_t^{-1} A_{sf} A_{ff}^{-1} (x_{t+1} - x_t) +(V_t - A_{sf} A_{ff}^{-1} W_t) .
    \end{equation}

    We repeat the same steps for the fast iterate:
    Using that 
    \begin{align*}
        A_{ss} y_t = \gamma_t^{-1} (y_t - y_{t+1}) - (A_{sf} x_t - V_t) ,
    \end{align*}
    we have by substitution
    \begin{align*}
        x_{t+1} &= x_t - \alpha_t (A_{ff} x_t - W_t) - \alpha_t A_{fs} y_t 
        \\
        &= x_t - \alpha_t A_{ff} x_t + \alpha_t W_t - \alpha_t A_{fs} A_{ss}^{-1}\left(\gamma_t^{-1} (y_t - y_{t+1}) - (A_{sf} x_t - V_t) \right) 
        \\ & = 
        x_t - \alpha_t \left(A_{ff} - A_{fs} A_{ss}^{-1} A_{sf} \right) x_t + \alpha_t (W_t - A_{fs} A_{ss}^{-1} V_t) - \frac{\alpha_t}{\gamma_t} A_{fs} A_{ss}^{-1} (y_t - y_{t+1}) .
    \end{align*}
    Denoting $G = A_{ff} - A_{fs} A_{ss}^{-1} A_{sf}$, we have 
    \begin{equation}
        G x_t = \alpha_t^{-1} (x_t - x_{t+1}) + (W_t - A_{fs} A_{ss}^{-1} V_t) - \gamma_t^{-1} A_{fs} A_{ss}^{-1} (y_t - y_{t+1}) .
    \end{equation}
\end{proof}
The Polyak-Ruppert averages $\bar{x}_n, \bar{y}_t$ are obtained by taking the average on both sides:
\begin{align*}
    G \bar{x}_n &= \frac{1}{n} \sum_{t=1}^n (W_t - A_{fs} A_{ss}^{-1} V_t) + \frac{1}{n} \sum_{t=1}^n \alpha_t^{-1} (x_{t} - x_{t+1}) - \frac{1}{n} \sum_{t=1}^n \gamma_t^{-1} A_{fs} A_{ss}^{-1} (y_t - y_{t+1}) \\ 
    \Delta \bar{y}_n &= \frac{1}{n} \sum_{t=1}^n (V_t - A_{sf} A_{ff}^{-1} W_t) 
    + \frac{1}{n}\sum_{t=1}^n A_{sf} A_{ff}^{-1} \alpha_t^{-1} (x_{t} - x_{t+1})
    + \frac{1}{n}\sum_{t=1}^n \gamma_t^{-1} (y_{t} - y_{t+1})  .    
\end{align*}


\textbf{Step 2:} Quantitative bounds on the central limit theorem. 

Asymptotic convergence of $\sqrt{n} (\bar{x}_n, \bar{y}_n)$ to a normal distribution can then be proved by showing that (1) the first noise terms above satisfy Lindeberg's condition and that (2) the remaining terms converge to zero with probability 1.
Slutsky's theorem then implies that the sum $N_t + E_t \to N + c$ when the noise $N_t$ converges in distribution to a random variable $N$ and the error terms $E_t$ converge to a constant $c$ with probability 1. 



For non-asymptotic analysis, we use the mean-square results for the last iterates to obtain finite-time bounds on the telescoped weighted differences above, where they converge in probability to 0.
Next, we use a bound on the Wasserstein-1 distance between the first terms and a normal random variable. 
These results can be combined using Lindeberg's decomposition to obtain finite-time bounds on the distance between $\bar{x}_n$ and $\bar{y}_n$ to a Gaussian random variable. 


Lemma \ref{lem:slutsky} resembles Slutsky's theorem for a sum of two convergent random sequences, but can be used to obtain finite-time bounds on the Wasserstein-1 distance when the random sequences being summed converge with respect to the Wasserstein-1 distance and mean absolute error. 
% \begin{lemma}[Lemma \ref{lem:slutsky}]
% \begin{lemma}[Lemma \ref{lem:slutsky}]
%     Consider two random sequences $\{X_t\}, \{Y_t\}$ such that $d_1 (X_t, X) \leq r_t$ for some random variable $X$ and $\mathbb{E}\lVert Y_t \rVert \leq r'_t$ for some constant $Y$.
%     Then, $d_1 (X_t + Y_t, X) \leq r_t + r'_t$.
% \end{lemma}
\begin{lemma*}
    (Restated Lemma \ref{lem:slutsky})
    Consider two random sequences $\{X_t\}, \{Y_t\}$ such that $d_1 (X_t, X) \leq r_t$ for some random variable $X$ and $\mathbb{E}\lVert Y_t \rVert \leq r'_t$.
    Then, $d_1 (X_t + Y_t, X) \leq r_t + r'_t$.
\end{lemma*}
\begin{proof}
    By the definition \eqref{eq:wasserstein_definition}, we have that
    \begin{align*}
        d_1 (X_t + Y_t, X) &= \sup_{h \in \mathrm{Lip}_1} \mathbb{E}[h(X_t + Y_t) - h(X)] 
        \\ 
        &= \mathbb{E}\left[h(X_t + Y_t) - h(X_t)\right] + \mathbb{E}h(X_t) - \mathbb{E} h(X) 
        \\ & \leq 
        \mathbb{E}\lVert Y_t \rVert + d_1 (X_t, X) 
        ,
    \end{align*}
    where the last inequality uses that $h$ in 1-Lipschitz. 
    This statement demonstrates that the Wasserstein-1 distance between a sum and its limit can be decomposed into mean absolute errors and the Wasserstein-1 distance of a summand. 
\end{proof}
Next, we use a non-asymptotic central limit theorem (Theorem 1, \citet{srikant2024CLT}) for martingale differences, simplified for our application.
\begin{lemma*}
    % {\color{red}Revise the statement in the main draft;}
    (Restated Lemma \ref{lem:CLT})
    Let $\{N_t\}$ be a martingale difference sequence satisfying Assumption \ref{assumption:noise}. 
    Denoting $Z \sim \mathcal{N}(0, I)$ to be the standard Gaussian vector, we have that
    \begin{equation}
        d_1 \left(n^{-1/2} \sum_{t=1}^n N_t, \Gamma^{1/2} Z\right) \leq 
        \mathcal{O}\left(\frac{d}{1-\beta}\right) \frac{\lVert \Gamma^{1/2} \rVert }{n^{\beta / 2}} \left(\lVert \Gamma^{-1/2} \rVert^{2 + \beta} + \lVert \Gamma^{-1/2} \rVert^\beta \right).
    \end{equation}
\end{lemma*}
% \begin{proof}
%     Assumption \ref{assumption:noise} implies that all $\leq 2 + \beta$ moments of $N_t$ are finite. 
%     Therefore, the assumptions in Theorem 1 of \citep{srikant2024CLT} are satisfied.
%     % This is a special case of Theorem 1 in \citep{srikant2024CLT}.     
% \end{proof}
It remains to show that the expected norm of the remaining terms decay to zero.
Observe that 
\begin{align*}
    \sum_{t=1}^n \alpha_t^{-1} (x_t - x_{t+1}) = \frac{x_1}{\alpha_1} - \frac{x_{n+1}}{\alpha_{n}} + \sum_{t=1}^{n-1} (\alpha_{t+1}^{-1} - \alpha_{t}^{-1}) x_t .
\end{align*}
The step sizes in Assumption \ref{assumption:steps} satisfy
\begin{align*}
    \alpha_{t+1}^{-1} - \alpha_{t}^{-1} \leq (t \alpha_t)^{-1} ,
\end{align*}
and we have from triangle inequality that
\begin{equation}\label{eq:fast_telescope}
    \mathbb{E} \lVert \sum_{t=1}^n \alpha_t^{-1} (x_t - x_{t+1}) \rVert \leq \alpha_1^{-1} \mathbb{E}\lVert x_1 \rVert  + \alpha_n^{-1} \mathbb{E}\lVert x_{n+1} \rVert + (\alpha_1)^{-1} \sum_{t=1}^n t^{a - 1} \mathbb{E} \lVert  x_t \rVert .
\end{equation}
Repeating for $\{y_t\}$, we have
\begin{equation}\label{eq:slow_telescope}
    \mathbb{E} \lVert \sum_{t=1}^n \gamma_t^{-1} (y_t - y_{t+1}) \rVert \leq \gamma_1^{-1} \mathbb{E} \lVert y_1 \rVert + \gamma_n^{-1} \mathbb{E} \lVert y_{n+1} \rVert + (\gamma_1)^{-1} \sum_{t=1}^n t^{b - 1} \mathbb{E} \lVert  y_t \rVert .
\end{equation}
We have from Theorem \ref{thm:mse} and Jensen's inequality that
\begin{align*}
    \mathbb{E}\lVert x_{t} \rVert = \mathcal{O}(\sqrt{\alpha_t} + \sqrt{\gamma_t}), 
    \quad 
    \mathbb{E} \lVert y_{t} \rVert = \mathcal{O}(\sqrt{\gamma_t}) .
\end{align*}
Substituting, we obtain
\begin{align*}
    \mathbb{E} \lVert \sum_{t=1}^n \alpha_t^{-1} (x_t - x_{t+1}) \rVert + \mathbb{E}\lVert \sum_{t=1}^n \gamma_t^{-1} (y_t - y_{t+1}) \rVert 
    = \mathcal{O}\left(n^{a/2} + n^{a-b/2} + n^{b/2}\right) .
\end{align*}
Combining with Lemma \ref{lem:CLT} and \ref{lem:slutsky} yields Theorem \ref{thm:clt}.
Similar to how we included the transient term $\sqrt{\gamma_t}$ for $\mathbb{E}\lVert x_t\rVert$ to obtain the $n^{a-b/2}$ in the above equation, a refined inspection of all rates in the proof of Theorem \ref{thm:mse} illustrate how to control the transient terms in the Wasserstein-1 distance for the Polyak-Ruppert averaged errors in Theorem \ref{thm:clt}. 


