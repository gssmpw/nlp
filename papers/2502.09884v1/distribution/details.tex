\section{Details}\label{sec:details}

{\color{red}Finish above and discard below, leaving only the important ones.
Then go on to slow iterate.
}
\textbf{Terms 2 \& 3:}
\begin{lemma}
    Proposition 3 in \citep{kaledin2020finite} and/or how $\mathbb{E}[\lVert \delta_t^{(2)} \rVert^2] = \mathcal{O}(\alpha_t)$ as in Eq. 4.1 \citep{konda2004convergence}.
    \begin{equation}
        \mathbb{E}[\lVert \tilde{x} \rVert^2] = \mathcal{O}(\alpha_t) .
    \end{equation}    

    Prop. 3 in \citep{kaledin2020finite} has $Cov(\tilde{x}_t)$, but they assume $Cov(W_t) \leq \mathcal{O}(1) + Cov(\tilde{x}_t) + Cov(\hat{y}_t)$ in the operator norms, which is kind of strange.
    Following through, we should be able to get a similar inequality except without this condition on the noise. 
    Note that $\mathbb{E}[\lVert \tilde{x}_t \rVert^2] = \mathrm{Tr} Cov(\tilde{x}_t)$ denoted as $M^{\tilde{w}_t}$ int heir paper. 
\end{lemma}
Now we use that {\color{red}$L_t$ is uniformly bounded} and that {\color{red}$\Phi$ is uniformly bounded} so that that the second and third terms are, up to a multiplicative constant, no larger than
\begin{align*}
    % \frac{1}{T}\sum_{j=0}^{T-1} \mathbb{E}[\lVert \tilde{x}_j \rVert^2] = \frac{1}{T}\sum_j \alpha_j 
    % \Rightarrow 
    \mathbb{E}\lVert \frac{1}{\sqrt{T}} \sum_{j=0}^{T-1} (\Phi_{jT} + A_{ff}^{-1})(A_{ff} - B_j^{ff}) \tilde{x}_j \rVert 
    \\ \leq 
    \sqrt{\frac{1}{T} \lVert \Phi + A_{ff}^{-1} \rVert^2 
    \lVert (L_\infty + A_{ff}^{-1}A_{fs})A_{sf} \rVert^2 \sum_{j=0}^{T-1} \frac{\gamma_j^2}{\alpha_j^2} \mathbb{E} \lVert \tilde{x}_j \rVert^2}
    \\ 
    = \mathcal{O}\left(\sqrt{\frac{1}{T} \sum_j \frac{\gamma_j^2}{\alpha_j}}\right) 
    = \mathcal{O} (T^{-5/12}) 
    , \quad \gamma_j = j^{-1}, \alpha_j = t^{-2/3}
    .
\end{align*}
{\color{red}Above I need to be careful about norm squared of sum vs. sum of norm squares.}

\begin{proposition}
    $\lVert L_{t+1} + A_{ff}^{-1} A_{fs} \rVert$ is bounded by a universal constant (using that $L$ is bounded universally). 
    When $(\alpha_j - \alpha_{j+1})/\alpha_j \leq \frac{\delta}{j+2}$ (following Srikant's Appendix B; but for our problem we may want a different condition e.g. Assumption 3.3 in \citep{polyakJuditsky}), 
    then 
    \begin{equation}
        \frac{1}{T}\sum_{j=0}^{T-1} \lVert \Phi_{jT} \rVert^2 = \mathcal{O}(t^{-(1-\delta)}).
    \end{equation}
\end{proposition}
\begin{proof}
    {\color{red}To do: State the precise bounds.}
    $\Phi$ is studied in \citep{polyakJuditsky} and \citep{konda2004convergence}. 
    I am proving in Lemma \ref{lem:unweighted_norm} a bound on the unweighted norm of $L_t$, whereas the references \citep{konda2004convergence,kaledin2020finite} only prove in the weighted norms.
    Since $L_t$ is bounded, we have by triangle inequality that $\lVert L_{t+1} + A_{ff}^{-1} A_{fs} \rVert$ is also bounded. 
    Last iterate version of $\Phi$ (which differs from my $\Phi$ in that it has an additional summation) is studied in \citep{kaledin2020finite} Lemmas 12 \& 14, with some useful inequalities that may appear when analyzing the slow iterates. 
\end{proof}


\textbf{Terms 4 \& 5: }
{\color{red}Come back to condition on filtrations more carefully.}
Term 4 can be written as a telescopic sum of Martingale differences $\{X_T\}$ with initial condition $X_0 = 0$:
\begin{align*}
    X_T &\coloneqq \sum_{j=0}^{T-1} \frac{\gamma_j}{\alpha_j} \Phi_{jT} (L_{j+1} + A_{ff}^{-1} A_{fs}) V_j \\ 
    X_{T+1} - X_{T} &= \frac{\gamma_T}{\alpha_T} (L_{j+1} + A_{ff}^{-1} A_{fs})V_T
    \\
    \Rightarrow 
    \mathbb{E}[\lVert X_{T}\rVert^2 ] &\leq \sum_{j=0}^{T-1} \mathbb{E}[\lVert X_{j+1} - X_j \rVert^2 | H_j] 
    \\ &= \sum_{j=0}^{T-1} \left(\frac{\gamma_j}{\alpha_j}\right)^2 \mathrm{Tr}\left((L_{j+1} - A_{ff}^{-1} A_{fs}) \Gamma_{ss} (L_{j+1} - A_{ff}^{-1} A_{fs})^T\right) .
\end{align*}
The last term can be simplified by using the circular shift invariance of trace, and then appyling known inequalities to $L_t$.
A similar procedure yields the same rate of decay for term 5. 
Therefore, dividing by $T$ and taking the square root, we have that 
\begin{equation}
    \text{Terms 4 \& 5} \, = \mathcal{O}\left(
        \sqrt{\frac{1}{T} \sum_j \frac{\gamma_j^2}{\alpha_j^2}}
    \right)
    \sim \mathcal{O}(T^{-1/3})
\end{equation}
{\color{red}This looks very slow.}


\textbf{Term 6} is used because the original sum has time-varying weights.
Because we can't apply CLT directly due to the time-varying weights, we split into terms 6 and 7.
Here we need that $\Phi$ decays sufficiently fast.
This behaves similarly to the analysis in Step 4 of \citep{srikant2024CLT}, and we need to show that the square norm of Term 6 decays sufficiently fast.
Note that here we need a condition stronger than uniform bound on $\Phi$.
See Appendix B in \citep{srikant2024CLT}.

Summary: Follow Step 4 in \citep{srikant2024CLT} to get $T^{-(1-\delta)}$ where $\alpha_j - \alpha_{j+1} \sim \delta \alpha_j$. 
% As long as $\mathbb{E}[\lVert W_t \rVert]$ is bounded by a constant (e.g. $\sqrt{\mathrm{Tr} \Gamma_{ff}}$), this term is $\mathcal{O}(T^{-1/2})$.  


\textbf{Term 7:}
We then apply CLT to Term 7.
 Theorem 1 in \citep{srikant2024CLT} states that the term 7 satisfies 
\begin{align*}
    \expect{h(A_{ff}^{-1} \sum_j W_j) - h(\Sigma_{ff}^{-1/2} Z)} 
    \\
    \leq 
    \frac{1}{\sqrt{T}} \sum_{t=1}^T \Big(
        C_1 (d, \beta) \frac{\lVert \Sigma_{ff}^{1/2} \rVert}{(T - t + 1)^{(1+\beta)/2}} \expect{\lVert \Sigma_{ff}^{-1/2} W_t \rVert^{2 + \beta}} 
        \\
        + C_2(d, \beta) \frac{\lVert \Sigma_{ff}^{1/2} \rVert}{(T - t + 1)^{(1+\beta)/2}} \expect{\lVert \Sigma_{ff}^{-1/2} W_t \rVert^{ \beta}} 
        \\
        - \frac{1}{\sqrt{T}}\sum_{t=1}^T \frac{1}{T - n + 1} \mathrm{Tr}\left(
            A_k (\Sigma_\infty^{-1/2} E[\Sigma_k] \Sigma_\infty^{-1/2} - I)
        \right)
    \Big)    
\end{align*}
Here, $\tilde{Z}_k \sim \mathcal{N}(0, \sqrt{\frac{n-k}{n-k+1}} I)$ and $A_k = \mathbb{E}[\nabla^2 f(\tilde{Z}_k)]$ where $f$ is the solution to the Stein's equation that satisfies
\begin{equation}
    \lVert \nabla^2 f(x) - \nabla^2 f(y) \rVert_{op} \leq \tilde{C}_1(d, \beta) \lVert x - y \rVert^\beta L .
\end{equation}
Note that this implies
\begin{align*}
    \lVert A_k \rVert \leq \lVert \nabla^2 f(0) \rVert + \tilde{C}_1 (d, \beta) L \lVert \mathbb{E}[ \tilde{Z}_k ] \rVert^\beta.
\end{align*}
{\color{red}Actually here I'm using (1) in Lemma 1. 
But I'll need to use statement (3) as stated in the end of Theorem 1 proof to get the correct bound.}

{\color{red}Need to write out third term.
Is $\lVert A_k \rVert_{op} \leq C \lVert \Sigma_{\infty}^{1/2} \rVert_{op} \sqrt{n -k +1}$ or $n -k +1$? Theorem says former, but Theorem 2 proof seems to use latter.  
If it's the former, then we actually get faster rates, so I'm not sure why \citep{srikant2024CLT} used the weaker condition. Check his proof. 
}
It remains to show that every term is small enough so that convergence is applied.

{\color{blue}Complete the next steps!}
Now suppose
\begin{assumption}
    \begin{align*}
        \mathbb{E}[\lVert W_t \rVert^{\beta}] \leq M_{\beta} , \quad
        \mathbb{E}[\lVert W_t \rVert^{2+\beta}] \leq M_{2+\beta}         
    \end{align*} 
\end{assumption}
so that the first two terms in the CLT are bounded. 
It remains to analyze the last term. 




{\color{blue}Combine everything and see what the convergence result looks like.
I'm anticipating something like:
}
$\Sigma_{ff} = A_{ff}^{-1} \Gamma_{ff} A_{ff}^{-1}$
\begin{equation}
    d(\tilde{x}_t, \Sigma_{ff}^{1/2} Z) = \min \left\{T^{-1/3}, \text{Term 7} \right\},
\end{equation}
where the first one comes from rate of term 6. 
Note this isn't too bad, since \citep{srikant2024CLT} Theorem 3 is of order $T^{-1/4}$.
{\color{blue}
    Theorem 1 is of order $\frac{\log T}{\sqrt{T}}$; Theorem 2 is of order $T^{-1/2}$.
    Theorem 3 is of order $T^{-1/4}$.
}


