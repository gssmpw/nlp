\subsection{Size of Transformation $L_k$}
Using the same definition of $L_t$ as in \citep{konda2004convergence}, we have
\begin{equation}
    L_{t+1} = (I - \alpha_t A_{ff}) L_t + \gamma_t \left(
        A_{ff}^{-1} A_{fs} B^{ss}_t + (I - \alpha_t A_{ff})L_t B^{ss}_t
    \right)(I - \gamma_t B^{ss}_t)^{-1} .
\end{equation}
We have the following bound on the size of $L_t$ (\citet{konda2004convergence} Lemma A.1, \citet{kaledin2020finite} Lemma 17-19).
\begin{lemma}\label{lem:Ltsize}
    Let $Q \succ 0$ be the unique solution to the Lyapunov equation
    \begin{equation}
        A_{ff}^T Q + Q A_{ff} = I ,
    \end{equation}
    where $-A_{ff}$ is H\"{u}rwitz. Then for every 
    \begin{equation}
        0 \leq \alpha_t \leq \frac{1}{2} \lVert A_{ff}\rVert_Q^{-2} \lVert Q \rVert^{-2} ,
    \end{equation}
    where the norm $\lVert \cdot \rVert_Q$ is defined as
    \begin{equation}
        \lVert A \rVert_Q^2 = \sup_x \frac{x^T A^T Q A x}{x^T Q x} ,
    \end{equation}
    we have
    \begin{align*}
        \lVert I - \alpha_t A_{ff} \rVert_Q^2 \leq 1 - \frac{1}{2} \lVert Q \rVert^{-2} .
    \end{align*}
    Moreover, this implies that 
    \begin{align*}
        \lVert L_{t} \rVert_Q \leq ? .
    \end{align*}
    {\color{red}Complete using Lemma 17 - 19 in \citep{kaledin2020finite} and Lemma A.1 in \citep{konda2004convergence}.}
\end{lemma}



\subsection{Matrix Functions and Inequalities}
Here we would like to approximate $\hat{\bar{z}}_T$ with 
\begin{equation}
    \frac{1}{T}\sum_{t=1}^T \left(
        \prod_{k=1}^t (I - D_k) \hat{z}_0 + \sum_{j=1}^t \prod_{k=j+1}^t (I - D_k) N_j 
    \right),
\end{equation}
where $D_k = R_k - [L_k]$ is the lower-triangular component of $R_k$.
The norm on the approximation error is then 
\begin{align*}
    \left\lVert \left(\prod_k (I - R_k) \hat{z}_0 + \sum_j \prod_k (I - R_k) N_j \right) - \left(\prod_k (I - D_k) \hat{z}_0 + \sum_j \prod_k (I - D_k) N_j \right) \right\rVert 
\end{align*}    
If we are able to show that 
\begin{equation}
    \prod_k (I - R_k) = \prod_k (I - D_k) (1 + o(k)) ,
\end{equation}
then we obtain that the approximation error is bounded above by
\begin{equation}
    \left\lVert \prod_k (1 + o(k)) \hat{z}_0 + \sum_j \prod_k (1 + o(k)) N_j \right\rVert .    
\end{equation}
If $o(k) \leq 1/k$ and $o(k) < D_k$, then the approximation is asymptotically an equality. 


To show this, we use the identity
\begin{align*}
    (I - R_k) = (I - D_k)(I + (I-D_k)^{-1}(D_k - R_k)) 
\end{align*}
and then show that $(I - D_k)^{-1} (D_k - R_k) = (I - D_k)^{-1} [L_k]$ is small. 
Notice that because $I - D_k$ is lower triangular, we can use Schur complements to get that the upper left block of $(I - D_t)^{-1}$ is 
\begin{align*}
    (I - \alpha_t B_t^{ff})^{-1} .
\end{align*}
Therefore, we need to bound the operator norm of
\begin{align*}
    I + (I - D_t)^{-1}(D_t - R_t) = I + (I - \alpha_t B_t^{ff})^{-1} L_t  .
\end{align*}
Using either \citep{konda2004convergence} or \citep{kaledin2020finite}, we have that 
{\color{red}To prove: }
If the conditions in Lemma \ref{lem:Ltsize} are met so that
\begin{equation}
    \lVert L_k \rVert_Q \leq \sum_{j=t_0}^{t-1} \alpha_t \exp \left(-O \left(\sum_{k=j+1}^{t} \alpha_k \right)\right) O\left(\frac{\gamma_j}{\alpha_j}\right) 
\end{equation}
where {\color{red}Q is the Lypaunov solution of $A_{ff}$}, then 
\begin{equation}
    \lVert \hat{z}_T - \left(\prod_k (I - D_k) \hat{z}_0 + \sum_j \prod_k (I - D_k) N_j \right) \rVert_Q \leq 
    \sum_
\end{equation}


for {\color{red}$Q \approx (I - D_k)^{-1}$}, then we have that {\color{red}(with $(I - D_k)^{-1} [L_k] \approx \lVert L_k \rVert_Q$)}, the approximation error is given by
\begin{align*}
    \prod_k (I - R_k) = \prod_k (I - D_k) \prod_k (I + o(k)) .
\end{align*}
For this approximation error to converge, we require that $o(k)$ is strictly less than $1/k$ so that the last product converges.
This convergence holds if and only if $o(k) < 1/k$; otherwise if $o(k) = 1/k$ then $\prod_k (1+1/k)$ diverges. 

{\color{red}Prove the red terms above.}

{\color{blue}Notes: $I - D_k$ is easy to analyze. 
We nee to see if we can use the norm induced by $(I - D_k)^{-1}$ which is positive definite for large enough $k$, and then show that the induced norm is precisely this quantity,.
}




\begin{remark}
    At first sight, it seems that $I - R_k \approx \exp(-R_k)$ would work. 
    However, the error accumulates when taking the product, and the Taylor remainder gives a term that scales with $\lVert \sum_k R_k \rVert$ (even for scalar case). 
    Because this diverges, the approximation is not valid.
    Instead, the above approximation leaves the term $D_k$ that scales accordingly with $I - R_k$ so that the approximation error decays.
\end{remark}
% Now using that
% \begin{align*}
%     \lVert \exp X - (I + X)\rVert \leq \sum_{r=2}^\infty \frac{\lVert X\rVert^r}{r!} 
%     \leq \frac{\lVert X\rVert^2}{2} \frac{1}{1 - \lVert X \rVert} ,
%     % = \exp (\lVert X \rVert) - (1 + \lVert X \rVert) ,
% \end{align*}
% for $X = (I - D_k)^{-1}(D_k - R_k)$,
% we have
% \begin{align*}
%     \lVert \exp ((I-D_k)^{-1}(D_k - R_k)) - (I + (I-D_k)^{-1}(D_k - R_k))\rVert 
%     \\ \leq \frac{\lVert (I-D_k)^{-1}(D_k - R_k) \rVert^2}{2} \frac{1}{1 - \lVert (I-D_k)^{-1}(D_k - R_k)\rVert} .
% \end{align*}
% Applying to the product, we have that
% \begin{align*}
%     \lVert \prod_{k=j+1}^t (I + (I - R)^{-1}(R-R_k)) - \prod_{k=j+1} \exp (I + (I-R)^{-1}(R-R_k))\rVert \leq \prod_{k=j+1}^t
% \end{align*}
% {\color{blue}
%     Now finish off to get the bound on the product. 
%     Boudning $I - R_k$ directly results in an error that grows exponentially with $\sum_k \lVert R_k\rVert$. 
%     But instead this gives our desired exponential rate, and gives a bound on the approximation of this desired rate as a product of sequences $1 + a_k$ where $a_k \to 1$.
% }

% {\color{red}If the error bounds turn out to work only for sufficiently large $t \geq t_0$, then we can get a bound for $t \geq t_0$ and use the partial product up to $t_0$. As long as this is constant, the asymptotic product will dominate. 
% }

% \begin{lemma}
%     Let $X$ be a H\"{u}rwitz matrix with spectral norm $\rho$. 
%     Then, the first order approximation of the matrix exponential $\exp X$ is given by a contour integral
%     \begin{equation}
%         \exp \left(X\right) - \left(I + X\right) = \frac{1}{2\pi j} \oint e^z \left(z I - X\right)^{-1} \left(z X\right)^{2} dz 
%     \end{equation}
%     over a closed contour with radius at least $\rho$. 
%     Moreover, the following upper bound holds:
%     \begin{equation}
%         \lVert \exp X - \left(I + X\right) \rVert \leq \frac{\lVert X \rVert^2}{\rho^3}
%     \end{equation}
% \end{lemma}


% There are two approximations to $\prod_{k=1}^t (I - R_k)$.
% One is to approximate the inside term $I - R_k \approx \exp(-R_k)$ from which we apply the product, and the other is to approximate $\prod_{k=1}^t(I- R_k) \approx I - \sum_k R_k$ by direct evaluation and discarding the terms that have multiple $R_k$ multiplied. 


% Taylor's remainder theorem  applied to first order approximation:
% \begin{equation}
%     \left\lVert \exp \left(-\sum_{k=1}^t R_k \right) - \left(I - \sum_{k=1}^t R_k\right) \right\rVert \leq
%         \frac{\lVert \sum_k R_k\rVert^{2}}{2} 
%         % \frac{3}{3 - \lVert \sum_k R_k \rVert}  .
% \end{equation}
% \begin{proof}
%     From the definition of matrix exponential, we have that the difference is the 2nd order remainder 
%     \begin{align*}
%         \exp \left(-\sum_{k=1}^t R_k \right) - \left(I - \sum_{k=1}^t R_k\right) = \sum_{m=2}^\infty \frac{(-1)^m}{m!} \left(\sum_k R_k\right)^m .
%     \end{align*}
%     {\color{blue}Find a reference on matrix analysis to show the following remainder:
%     }
%     \begin{equation}
%         E_2 (A) =   \int_0^1 e^{ -t A }A^2(1-t)dt ,
%     \end{equation}
%     which we can use for $A = \sum_k R_k$.
%     {\color{red}Some possible references are commented below}
%     % https://www.anandinstitute.org/pdf/Roger_A.Horn.%20_Matrix_Analysis_2nd_edition(BookSee.org).pdf
%     % https://epubs.siam.org/doi/10.1137/1.9780898717778
   
% \end{proof}


% {\color{red}Now we need the following deviation}
% \begin{equation}
%     \left\lVert \prod_{k=1}^t (I - R_k) - \left(I - \sum_k R_k\right) \right\rVert 
%     \leq 
%     \lVert \exp \left(-  \sum_k R_k  \right)\rVert
% \end{equation}
% {\color{red}This may be done by approximating writing out the approximation of $I - R_k$ and applying product? }
% \begin{proof}
%     {\color{blue}If the matrix $R_k$ is invertible, then we can take the matrix logarithm. If the real parts of the eigenvalues are non-negative, then there is a unique principal logarithm.}
%     Assuming these conditions,
%     \begin{align*}
%         \prod_{k=1}^t (I - R_k) = \exp \sum_{k=1}^t \log (I - R_k) .
%     \end{align*}
%     % \begin{align*}
%     %     \prod_{k=1} (I - R_k) = I - \sum_k R_k + \sum_{i < j} R_i R_j - \sum_{i < j < l} R_i R_j R_l + \cdots \\
%     %     \Rightarrow 
%     %     \lVert \prod_{k=1}^t (I - R_k) - \left(I - \sum_k R_k\right) \rVert \leq  \sum_{m=2}^\infty \frac{1}{m!} \lVert \sum_k R_k \rVert^m
%     %     \\
%     %     = \exp \left(\lVert \sum_k R_k \rVert \right) .
%     % \end{align*}
% \end{proof}


% Then we can combine with Triangular inequality to get
% \begin{equation}
%     \lVert \exp (-\sum_k R_k) - \prod (I - R_k)\rVert \leq ?
% \end{equation}
% From this we have that
% \begin{align*}
%     \hat{\bar{z}}_T = \frac{1}{T}\sum_t \left(
%         \exp \left(-\sum_{k} R_k\right) \hat{z}_0 + \sum_j \exp \left(-\sum_k R_k\right) N_j
%     \right)
%     \\
%     + \frac{1}{T}\sum_t \left(
%         \left(\prod_k (I - R_k) - \exp(-\sum_k R_k)\right) \hat{z}_0 + 
%         \sum_j \left(\prod_k (I - R_k) - \exp(-\sum_k R_k)\right) N_j
%     \right) ,
% \end{align*}
% where the first term can be analyzed more easily and the latter term can be bounded almost surely using the remainder terms of the matrix exponential. 




\subsection{Convergent Sequence}
Since {\color{red}(to be shown above)} $o(k)$ decays sufficiently fast, we can now focus on the easier-to-analyze
\begin{equation}
    \prod_k (I - D_k) \hat{z}_0 + \sum_j \prod_k (I - R_k) N_j ,
\end{equation}
or the Polyak-Ruppert average of this quantity.


The term $\sum_j \prod_k (I - R_k) N_j$ can be analyzed using Lemma 12 in \citep{kaledin2020finite}, and the first term's convergence should be obtained by getting a bound on $\lVert I - D_t\rVert$ in the form of $I - \lVert D_t \rVert$ for sufficiently small $D_t$, where $\lVert D_t\rVert$ can be easily analyzed from the diagonal entries of $D_t$.

{\color{blue}TODO: We know the deterministic part decays exponentially fast by definition of $D_k$.
It remains to show that the average weighted Martingale converges in distribution.
The main difficulty in applying \citep{srikant2024CLT} is that $R_k$ and $N_j$ both depend on time-varying terms. 
But hopefully we can go through the proof and see that they converge. 
As Thinh mentioned, this may not be trivial since the O-U process requires time-invariant coefficient matrix. 
}


