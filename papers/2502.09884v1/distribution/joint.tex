% \section{Main Results}
\section{Two Time-Scale Convergence}
This is left as the next step. 
Once we have a term-by-term convergence result in distribution, we will need to refine the proof so that it works for the two together. 
The idea is to re-derive the decomposition for two time-scale iterates jointly, where the individual iterate convergence will be given by the previous section.
Then, we can substitute these expression to that required to analyze the cross terms.
{\color{red}Below is based on discussions before deciding to do each time-scale separately.}



Using this transformation, we have the state transition terms given by
\begin{align*}
    B_t = \begin{pmatrix}
        \alpha_t B_t^{ff} & \alpha_t B_t^{fs} \\ \gamma_t A_{sf} & \gamma_t B_t^{ss}
    \end{pmatrix}  
\end{align*}
with the systems $B$ defined as
\begin{align*}
    B_t^{ff} = \frac{\gamma_t}{\alpha_t}(L_{t+1} + A_{ff}^{-1} A_{fs}) A_{sf} + A_{ff} \\ 
    B_t^{fs} = \alpha_t^{-1} \left(L_t - L_{t+1}\right) + \frac{\gamma_t}{\alpha_t}\left(L_{t+1} + A_{ff}^{-1} A_{fs}\right) B_t^{ss} - A_{ff} L_t \\ 
    B_t^{ss} = \Delta - A^{sf} L_t .
\end{align*}
Here we see that if $L_t \to 0$ in the limit $t \to \infty$, we have $-B$ is asymptotically H\"{u}rwitz by inspecting the block-diagonal of the asymptotically lower triangular matrix.
The noise term is
\begin{equation}
    N_t = \begin{pmatrix}
        \alpha_t \xi_t + \gamma_t (A_{ff}^{-1} A_{fs} + L_{t+1}) \psi_t \\ \gamma_t \psi_t 
    \end{pmatrix} 
\end{equation}



For finite-time analysis, it will be useful to define scaled versions of the above quantities.
Specifically, let's use bar to normalize matrices so that they don't go to 0 asymptotically:
\begin{equation}
    B_t = S_t \bar{B}_t, S_t = \begin{pmatrix}
        \alpha_t I & 0 \\ 0 & \gamma_t I 
    \end{pmatrix}, 
    \bar{B}_t = \begin{pmatrix}
        B_t^{ff} & B_t^{fs} \\ A_{sf} & B_t^{ss}
    \end{pmatrix} 
    \to \bar{B}_\infty = \begin{pmatrix}
        A_{ff} & 0 \\ A_{sf} & \Delta
    \end{pmatrix} .
\end{equation}
We can also write the noise term as
\begin{align*}
    N_t = \left(S_t + \gamma_t M_k
    \right) \begin{pmatrix}
        \xi_t \\ \psi_t
    \end{pmatrix},
    M_k = \begin{pmatrix}
        0 &  (A_{ff}^{-1} A_{fs} + L_{k+1}) \\ 0 & 0 
    \end{pmatrix} .
\end{align*}
From this, we can write 
\begin{equation}\label{eq:master}
    \hat{z}_t = \prod_{k=1}^t (I - B_k) \hat{z}_0 + \sum_j \prod_k \left(I - S_k \bar{B}_k\right) \left(S_j + M_j\right) \begin{pmatrix}
        \xi_j \\ \psi_j 
    \end{pmatrix} .
\end{equation}


Note that $\hat{\bar{z}}_T = T^{-1} \sum_t \hat{z}_t$ is an adequate approximation of the Polyak-Ruppet average of $(x_t, y_t)$, where the error of approximation decays with the average of $\{L_t\}$ (see backup CLT.tex).
So for the following sections, we will focus on analyzing $\hat{z}_t$ before moving on to the Polyak-Ruppert average.


{\color{blue}
To do: Derive formal approximation error bounds and verify that they decay fast enough.
Most of the remaining analysis can be completed by generalizing scalar-versions of inequalities in \citep{konda2004convergence,kaledin2020finite} to matrices. 
The reason we need such generalizations is that convergence in distribution requires keeping track of the exact expressions as much as possible, whereas mean-squared convergence results can at some point replace operators with scalar versions of their operator norms. 
}

\subsection{Noise Decomposition}
We will write the noise in the form
\begin{equation}
    \sum_j \prod_k \left(I - S_k \bar{B}_k\right) \left(S_j + M_j\right) \begin{pmatrix}
        \xi_j \\ \psi_j 
    \end{pmatrix} = 
    \bar{B}_\infty^{-1} \sum_j \begin{pmatrix}
        \xi_t \\ \psi_t
    \end{pmatrix}
    + \text{Transient Terms}
\end{equation}
where the Transient Terms are error bounds that are shown to decay to 0.


First note that $S_k$ in \eqref{eq:master} is diagonal hence commutes with any matrix, which we use to get
\begin{equation}
    \sum_j \prod_k (I - S_k \bar{B}_k) S_k \begin{pmatrix}
        \xi_j \\ \psi_j 
    \end{pmatrix}
    = \sum_j S_j \prod_k (I - S_k \bar{B}_k) \begin{pmatrix}
        \xi_j \\ \psi_j 
    \end{pmatrix} .
\end{equation}
We would like to approximate the noise terms with 
\begin{align*}
    \sum_j S_j \prod_k (I - S_k \bar{B}_\infty) \begin{pmatrix}
        \xi_j \\ \psi_j 
    \end{pmatrix} , 
\end{align*}
which resembles the $\sum_j \gamma_j (1 - \gamma_j A)$ shown to converge to $A^{-1}$ for a H\"{u}rwitz matrix $-A$ in \citep{konda2004convergence}.
Specifically, we have that $-\bar{B}_\infty$ defined above is H\"{u}rwitz, and we need to generalize this for a matrix $S_j$ in place of scalar $\gamma_j$.
{\color{red}TODO: Prove the rate of the above term.
If we succeed, we should be able to prove convergence of the Polyak-Ruppert average.
}


Assuming the above gives a nice analysis, we would like to obtain an error bound on 
\begin{equation}
    \prod_k (I - S_k \bar{B}_k) - \prod_k (I - S_k \bar{B}_\infty) .
\end{equation}
Note that the LHS terms can be factorized as
\begin{align*}
    I - S_k \bar{B}_k = (I - S_k \bar{B}_\infty) (I - (I - S_k \bar{B}_\infty)^{-1}(S_k \bar{B}_k - S_k \bar{B}_\infty)).
\end{align*}
{\color{red}To Prove: A formal proof for the below approximation. 
Currently, this is based on the fact that if the last term on RHS is a scalar, or if we can somehow isolate its operator norm, then we get an error rate of}
\begin{equation}
    \lVert \prod_k (I - S_k \bar{B}_\infty) - \prod_k (I - S_k \bar{B}_k)\rVert \approx \lVert \prod_k \left(I - (I - S_k \bar{B}_\infty)^{-1}(S_k \bar{B}_k - S_k \bar{B}_\infty)\right) - I \rVert,
\end{equation}
where $S_k \to 0$ and so $I - S_k \bar{B}_\infty \to I$, and the transient term $\bar{B}_t - \bar{B}_\infty$ is the transient component
\begin{align*}
    S_t \bar{B}_t - S_t \bar{B}_\infty = S_t \begin{pmatrix}
        \frac{\gamma_t}{\alpha_t}(L_{t+1} + A_{ff}^{-1} A_{fs})A_{sf} & B_t^{fs} \\ 0 & -A_{sf} L_t 
    \end{pmatrix} .
\end{align*}
{\color{red}At this point, we need that}
\begin{align*}
    \lVert \prod_k (I - (I - S_k \bar{B}_\infty)^{-1}(S_k \bar{B}_k - S_k \bar{B}_\infty)) \rVert
        \approx 
    \prod_k (1 - o(k))
\end{align*}
which holds {\color{red}wrt a weighted norm} when the minus term is sufficiently small (see \citet{kaledin2020finite} Eq. 8 or Lemma 17). 
For this error to decay to 0, we need $o(k) \leq 1/k$.
{\color{red}To do: Combine to recover the error rate of the approximation above from this.}



Assuming we can prove the above, we now have that with $\Xi_j = [\xi_j, \psi_j]$ that
\begin{align*}
    \sum_j \prod_k (I - S_k \bar{B}_k) (S_j + M_j) \Xi_j 
    = \sum_j S_j \prod_k (I - S_k \bar{B}_k) \Xi_j + \sum_j \prod_k (I - S_k \bar{B}_k) M_j \Xi_j 
    \\
    \approx \sum_j S_j \prod_k (I - S_k \bar{B}_\infty) \Xi_j + \sum_j \prod_k (I - S_k \bar{B}_k) M_j \Xi_j 
    \\
    = \bar{B}_\infty^{-1} \sum_j \Xi_j + \sum_j \left(S_j \prod_k (I - S_k \bar{B}_\infty) - \bar{B}_{\infty}^{-1}\right) \Xi_j 
    + \sum_j \prod_k (I - S_K \bar{B}_k) M_j \Xi_j .
\end{align*}
{\color{red}
My intention now is to show that the (1) we can apply CLT to the first one, (2) Second term we are working on an error bound in the above section, and (3) $M_j$ is upper triangular with eigenvalues of 0 and $\lVert \prod_k (I - S_k \bar{B}_k) Z \rVert \leq \prod_k (1 - s_k)$ as used in \citep{konda2004convergence,kaledin2020finite}.
}

The reason we can suspect
\begin{align*}
    \sum_j S_j \prod_{k=j+1}^t (I - S_k \bar{B}_\infty) \to \bar{B}_{\infty}^{-1}
\end{align*}
is that (1) if $S_k$ is understood to be a scalar step-size, then \citep{konda2004convergence} has this type of result (their $\alpha$ and $w$); (2) Again viewing $S_j$ to be a scalar step-size, we have (after switching sums in Polyak Ruppert avg) $s_t \sum_j \prod_k (1 - s_k) \approx s_t \sum_j \exp(-\sum_{k=j+1}^t s_k) = s_t (\sum_k s_k r_\infty)^{-1} \approx r_\infty^{-1}$ (refine this argument in matrix form). 
The first reason from \citep{konda2004convergence} is sufficinet to see this case. 
Note that in this case, we are saying that the approximation is such that for every finite time $t$, we have a constant bound on the distance to its limit.
