\section{Slow Iterate}

\subsection{Why Same Steps as in Fast Isn't Good Enough for Slow}
Using induction,
\begin{align*}
    &\hat{y}_{t} 
    = \prod_{k=1}^t (I - \gamma_k \Delta) \hat{y}_0 
    + \sum_{j=1}^t \gamma_j \prod_{j=k+1}^t (I - \gamma_k \Delta) A_{sf} L_j \hat{y}_j 
    \\ 
    &+ \sum_{j=1}^t \gamma_j \prod_{j=k+1}^t (I - \gamma_k \Delta) A_{sf} \tilde{x}_j 
    + \sum_{j=1}^t \gamma_j \prod_{j=k+1}^t (I - \gamma_k \Delta)V_j 
    \\
    & \hat{\mu}_{yn} = n^{-1} \sum_{t=1}^n \prod_{k=1}^t (I - \gamma_k \Delta) \hat{y}_0 + n^{-1} \sum_{t=1}^n \gamma_t \sum_{j=t+1}^n \prod_{k=j+1}^n (I - \gamma_k \Delta) A_{sf} L_t \hat{y}_t 
\\ & + n^{-1} \sum_{t=1}^n \gamma_t \sum_{j=t+1}^n \prod_{k=t+1}^n (I - \gamma_k \Delta) A_{sf} \tilde{x}_t + n^{-1} \sum_{t=1}^n \gamma_t \sum_{j=t+1}^n \prod_{k=t+1}^n (I - \gamma_k \Delta) V_t
    ,
\end{align*}
where we changed the order of summation as earlier.
Overloading $\Phi_{tn}$ to be defined with the step-size schedule $\{\gamma_t\}$ and $\Delta$, we then repeat the same analysis for 
\begin{align*}
    \sqrt{n} \hat{\mu}_{yn} = \frac{1}{\sqrt{n}} \sum_{t=1}^n \prod_{k=1}^t (I - \gamma_k \Delta) \hat{y}_0 + \frac{1}{\sqrt{n}} \sum_{t=1}^n \Phi_{tn} A_{sf} L_t \hat{y}_t + \frac{1}{\sqrt{n}} \sum_{t=1}^n \Delta^{-1} A_{sf} L_t \hat{y}_t 
    \\
    + \frac{1}{\sqrt{n}} \sum_{t=1}^n \Phi_{tn} A_{sf} \tilde{x}_t + \frac{1}{\sqrt{n}} \sum_{t=1}^n \Delta^{-1} A_{sf} \tilde{x}_t + 
    \frac{1}{\sqrt{n}} \sum_{t=1}^n \Phi_{tn} V_t + \frac{1}{\sqrt{n}} \sum_{t=1}^n \Delta^{-1} V_t .
\end{align*}
{\color{red}To proceed with the analysis, we need to understand the behavior of $\Phi$ with respect to $\gamma$, i.e. when $\delta = 1$. Is it still uniformly bounded?}
{\color{red}Assuming $\Phi$ is uniformly bounded for $\gamma$, }
the terms are 
\begin{enumerate}
    \item Second term summand $\lVert L_t \hat{y}_t \rVert \sim t^{-1 + \delta} t^{-1/2}$ which results in a sum followed by normalization to get $(\delta - 1/2)^{-1} n^{-1 + \delta}$.
    Third term is the same as this.
    \item Fourth and fifth terms (especially fifth) don't decay fast enough: using $\tilde{x}_t \sim t^{-\delta/2}$, we have that after normalization that the sum if $n^{(1 - \delta)/2}$ which grows with $n$. 
    We're not using that $Cov(\tilde{x}_t, \hat{y}_t) \asymp t^{-1}$, which is perhaps what we need to use. 
    {\color{blue}For $\gamma_t = t^{-1}$, we have $\lVert \Phi \rVert \sim \log n$ simply by integrating. 
    So we need to write out the terms using a different decomposition such that the fourth and fifth terms do not appear. 
    } 
\end{enumerate}
We can invoke Theorem 1 in \citep{srikant2024CLT} to get $n^{-\beta / 2}$ for the last term.



Overloading $\Phi_{tn}$ to be defined with the step-size schedule $\{\gamma_t\}$ and $\Delta$ as
\begin{equation}
    \Phi_{tn} = \gamma_t \sum_{j=t+1}^n \prod_{k=t+1}^j (I - \gamma_k \Delta) - \Delta^{-1},
\end{equation}
we then repeat the same analysis for 
\begin{align*}
    \sqrt{n} \hat{\mu}_{yn} = \frac{1}{\sqrt{n}} \sum_{t=1}^n \prod_{k=1}^t (I - \gamma_k \Delta) \hat{y}_0 + \frac{1}{\sqrt{n}} \sum_{t=1}^n \Phi_{tn} A_{sf} L_t \hat{y}_t + \frac{1}{\sqrt{n}} \sum_{t=1}^n \Delta^{-1} A_{sf} L_t \hat{y}_t 
    \\
    + \frac{1}{\sqrt{n}} \sum_{t=1}^n \Phi_{tn} A_{sf} \tilde{x}_t + \frac{1}{\sqrt{n}} \sum_{t=1}^n \Delta^{-1} A_{sf} \tilde{x}_t + 
    \frac{1}{\sqrt{n}} \sum_{t=1}^n \Phi_{tn} V_t + \frac{1}{\sqrt{n}} \sum_{t=1}^n \Delta^{-1} V_t .
\end{align*}
Note that $\Phi_{tn}$ is not analyzed in \citep{srikant2024CLT,polyakJuditsky} because $\gamma_t \propto t^{-1}$.
Let us roughly calculate the rate of $\Phi_{tn}$ using $\gamma_t \propto (c t)^{-1}$:
\begin{align*}
    \lVert \gamma_t \sum_{j=t+1}^n \prod_{k=t+1}^j (I - \gamma_k \Delta) \rVert \leq \gamma_t \sum_{j=t+1}^n \prod_{k=t+1}^j (1 - c \gamma_k) 
    \propto t^{-1} \sum_{j=t+1}^n \frac{t+1}{j}
    % \\ 
    \approx \log \frac{n}{t+1}
    % \\
    % \approx \frac{\log }
    % \leq \frac{1}{t} \sum_{j=t+1}^n \exp\left(-c \log \frac{j}{t+1}\right) \\ 
    % = \frac{1}{t} \sum_{j=t+1}^n \left(\frac{j}{t+1}\right)^c
    % \leq t^{-1 - c} \sum_{j=t+1}^n j^c 
    % \approx t^{-1 - c} \left(n^{1+c} - t^{1+c}\right) ,
\end{align*}
where the first inequality uses that $\lVert I - \gamma_t \Delta \rVert \leq 1 - c \gamma_t$ by relating the weighted norm with the unweighted norm.
The second inequality holds with $\gamma_t = t^{-1}$ and $c \in (0, 1)$.
Note that this approximation shows that $\lVert \Phi_{tn}\rVert$ does not decay. 
As a result, the recursion in the current form is not contractive.


Even if $\Phi_{tn}$ is shown to exhibit contractive properties, the fourth and fifth terms are problematic: Using $\tilde{x}_t \sim t^{-\delta/2}$ we have that $n^{-1/2} \sum_t t^{-\delta / 2} \sim n^{1/2 - \delta/2}$ which grows with $n$.
Therefore, we try to improve our analysis for the fourth and fifth terms in the next part.


% \subsubsection{Attempt 3: Re-scaling}
% {\color{blue}This works, but not to the correct distribution (covariance is off). 
% Rate matches that of fast iterate. 
% Correction: Problem persists, since we only get the ratio in front of $\Phi_{tn}$, not in front of $\Delta^{-1}$. 
% }
% % Our analysis relies on the fast iterates converging for every $y$.
% % Therefore, we can try to write $\tilde{x}_t = x_\infty(\hat{y}_t) + (\tilde{x}_t - x_\infty (\hat{y}_t))$:
% % \begin{align*}
% %     \hat{y}_{t+1} = (I - \gamma_t B_t^{ss}) \hat{y}_t - \gamma_t A_{sf} (x_t - x_\infty (y_t) + L_t \hat{y}_t) + \gamma_t V_t .
% % \end{align*}
% % {\color{red}Not sure how to proceed; the $x_t$ term still poses a problem of being too slow. 
% % I was hoping that we can bring out the $\hat{y}_t$ term from inside $x_\infty$, since $\hat{y}_t$ decays fast. 
% % }
% \begin{align*}
%     \hat{y}_{t+1} = (I - \alpha_t B_t^{ss}) \hat{y}_t + (\alpha_t - \gamma_t) B_t^{ss}\hat{y}_t + \gamma_t \tilde{x}_t + \gamma_t V_t
%     \\
%     = (I - \alpha_t \Delta) \hat{y}_t + \alpha_t (\Delta - B_t^{ss}) \hat{y}_t + (\alpha_t - \gamma_t)B_t^{ss} \hat{y}_t + \gamma_t \tilde{x}_t + \gamma_t V_t .
% \end{align*}
% Defining
% \begin{align*}
%     \Phi_{tn} = \alpha_t \sum_{j=t+1}^n \prod_{k=t+1}^j (I - \alpha_t \Delta) - \Delta^{-1} ,
% \end{align*}
% we can write out the terms as {\color{red}Check calculations! I think there should be a $\gamma_t$ coefficient. }
% \begin{align*}
%     \bar{y}_n = \bar{b}_n + \frac{1}{n}\sum_{t=1}^n (\frac{\gamma_t}{\alpha_t} \Phi_{tn} + \Delta^{-1}) (\Delta - B_t^{ss}) \hat{y}_t - \frac{1}{n} \sum_{t=1}^n (\frac{\gamma_t}{\alpha_t} \Phi_{tn} + \Delta^{-1}) B_t^{ss} \hat{y}_t \\
%     + 
%     \frac{1}{n}\sum_{t=1}^n (\frac{\gamma_t}{\alpha_t} \Phi_{tn} + \Delta^{-1}) \tilde{x}_t + Noise .
% \end{align*}
% Therefore using $\sum_{t=1}^n \lVert \Phi_{tn} \rVert^2 = O(n^\delta)$ as before, the $n^{-1/2} \sum_t \frac{\gamma_t}{\alpha_t}\tilde{x}_t$ term goes away at rate established for the fast iterate, which is $n^{\delta/2}$.
% {\color{red}Problem: Applying telescoping we obtain convergence to normal distribution with covariance $\Delta^{-1} \Gamma_{ss} \Delta^{-T}$, which may not be the correct one. 
% We can derive the correct covariance through recursion following \citep{konda2004convergence}.
% But then what is the flaw in the above argument?
% There is a chance that everything is correct, and that for the Polyak-Ruppert average we indeed get the simple $\Delta^{-1} \Gamma_{ss} \Delta^{-T}$ term instead of the more complicated covariance established in \citep{konda2004convergence}.
% }



% \subsubsection{{\color{red}New!} Handling individual terms: Linear Innovations / LMMSE}
% {\color{blue}Summary of idea and reason for failure:}
% The $\tilde{x}_t$ term decays too slow, and we need a different decomposition so that individual terms exhibit fast convergence, in the sense that when added and divided by $\sqrt{n}$ the norm decays to 0 (without the $\gamma_t$ term).
% Here, $\tilde{x}_t \sim t^{-\delta/2}$ becomes $n^{-1/2} \sum_t t^{-\delta / 2} \sim n^{1/2 - \delta / 2}$ which grows with $n$.


% In hopes that we can exploit the covariance between $\tilde{x}_t$ and $\hat{y}_t$, because this term decays fast, we can try to approximate $\tilde{x}_t$:
% \begin{align*}
%     \tilde{x}_t = Cov(\tilde{x}_t, \hat{y}_t) Cov(\hat{y}_t)^{-1} \hat{y}_t + (\tilde{x}_t - Cov(\tilde{x}_t, \hat{y}_t) Cov(\hat{y}_t)^{-1} \hat{y}_t ),
% \end{align*}
% which resembles the LMMSE.
% While the former term has norm of order $\hat{y}_t \sim \gamma_t$ (which still isn't fast enough, but is faster), the latter term understood as the error term in LMMSE has variance
% \begin{align*}
%     Var(e) = Var(\tilde{x}_t) - Cov(\tilde{x}_t, \hat{y}_t) Cov(\hat{y}_t)^{-1} Cov(\hat{y}_t, \tilde{x}_t) ,
% \end{align*}
% which is of order $Var \tilde{x}_t \sim \alpha_t$. 
% Therefore, this doesn't eliminate the slowness of $\tilde{x}_t$.




% {\color{red}
% The $\Sigma^{ff}_t$ term has a second order $\gamma_t$ and is thus neglected for covariance. 
% Ie the covariance depends only between $\hat{y}_t$ (because of the $I$) and inter-covariances. 
% So it is unclear how we can get rid of the $x_t$ term in the analysis, which results in rates that is too slow. 

% \textbf{Mean-square analysis / covariance methods will all be successful because they can get rid of the $\gamma_t^2$ term. 
% But for Stein's method analysis, we need to write $\hat{y}_t$ or $\bar{y}_n$ as a linear combination whose individual terms has expected norms that converge to zero.}
% }

% The issue with the above decomposition is that $\tilde{x}_t$ appears to decay slowly, whereas there may be a different decomposition that uses some correlation between $\tilde{x}_t$ and $\hat{y}_t$.
% Specifically, we aren't using the covariance matrix $Cov(\tilde{x}_t, \hat{y}_t)$ whose magnitude decays fast at rate $\gamma_t$.


% To this end, we introduce $E[\tilde{x}_t | \hat{y}_t]$ and obtain the following recursion for the slow iterates:
% \begin{align*}
%     \hat{y}_{t+1} &= (I - \gamma_t B_t^{ss}) \hat{y}_t 
%     - \gamma_t A_{sf} \left(\tilde{x}_t - E[\tilde{x}_t | \hat{y}_t] \right) + \gamma_t E[\tilde{x}_t | \hat{y}_t] + \gamma_t V_t .
% \end{align*}
% Let us use the LMMSE $E[\tilde{x}_t | \hat{y}_t] = H_t \hat{y}_t$, where we now have
% \begin{align*}
%     \hat{y}_{t+1} &= (I - \gamma_t (B_t^{ss} + A_{sf} H_t))\hat{y}_t - \gamma_t A_{sf} (\tilde{x}_t - H_t \hat{y}_t) + \gamma_t V_t , \\ 
%     H_t &= Cov(\tilde{x}_t, \hat{y}_t) Cov^{-1} (\hat{y}_t) \to \Sigma_{fs} \Sigma_{ss}^{-1} .
% \end{align*}
% The role of introducing a LMMSE $H_t \hat{y}_t$ is to re-write the recursion above using some matrix $Q_t$, where 
% \begin{align*}
%     \hat{y}_{t+1} = (I - \gamma_t Q_t) \hat{y}_t + \gamma_t (Q_t - B_t^{ss} - A_{sf} H_t) \hat{y}_t - \gamma_t A_{sf} (\tilde{x}_t - H_t \hat{y}_t) + \gamma_t V_t .
% \end{align*}
% Since the above is satisfied for any $Q_t$, we can check for which values of $Q_t$ the covariance of $\hat{y}_t$ matches that in \citep{konda2004convergence}:
% \begin{align*}
%     \Sigma_{t+1}^{ss} = &\Sigma_t^{ss} - \gamma_t (Q_t \Sigma_t^{ss} + \Sigma_t^{ss} Q_t^T) 
%     \\
%     &+ \gamma_t^2 (Q_t - \Delta - A_{sf} H_t) \Sigma_t^{ss}(Q_t - \Delta - A_{sf} H_t)^T
%     - \gamma_t^2 A_{sf} Cov(\tilde{x}_t - H_t \hat{y}_t) A_{sf}^T + \gamma_t^2 \Gamma_{ss}
%     \\
%     &+ \gamma_t \left(\Sigma_t^{ss} (Q_t - B_t^{ss} - A_{sf} H_t)^T +  (Q_t - B_t^{ss} - A_{sf} H_t) \Sigma_t^{ss}\right)
%     \\
%     &+ \gamma_t \left( Cov(\hat{y}_t, A_{sf}(\tilde{x}_t - H_t \hat{y}_t)) + Cov(A_{sf} (\tilde{x}_t - H_t \hat{y}_t), \hat{y}_t)\right)
%     \\
%     &+ \gamma_t^2 Q_t \Sigma_t^{ss} Q_t^T
%     - \gamma_t^2 Cov( A_{sf} L_t \hat{y}_t) 
%     + \gamma_t^2 \left(Q_t \Sigma_t^{ss} (Q_t - B_t^{ss} - A_{sf} H_t) + (Q_t - B_t^{ss} - A_{sf} H_t) \Sigma_t^{ss} Q_t^T\right)
%     \\
%     &+ \gamma_t^2 \left(Cov(Q_t \hat{y}_t, A_{sf} (\tilde{x}_t - H_t \hat{y}_t)) + Cov(A_{sf} (\tilde{x}_t - H_t \hat{y}_t, Q_t \hat{y}_t)\right)  .
% \end{align*}
% Similar to how \citet{konda2004convergence} neglect the higher-order $\gamma_t$ terms, we can also use that $\Sigma_t^{ss} \approx \gamma_t \Sigma_{ss}$ and hide the higher order terms to get the limit $\Sigma_{t+1}^{ss} = \Sigma_t^{ss}$ so that the lower-order terms (that depends on $\gamma_t \Sigma_t^{ss}$ and $\gamma_t^2 \Gamma_{ss}$, discrading those with $\gamma_t^2 \Sigma_t^{ss} \sim \gamma_t^3$)
% \begin{align*}
%     0 = -(Q \Sigma_{ss} + \Sigma_{ss} Q_t^T) + \Sigma_{ss} (Q - \Delta - A_{sf} H)^T + (Q - \Delta - A_{sf} H) \Sigma_{ss}
%     \\
%     + 
%     \Sigma_{sf} A_{sf}^T + A_{sf} \Sigma_{fs} - (\Sigma_{ss} H^T A_{sf}^T + A_{sf} H \Sigma_{ss}) 
%     + \Gamma_{ss}
%     \\
%     =
%     - (\Delta + A_{sf} H) \Sigma_{ss} - \Sigma_{ss} (\Delta + A_{sf} H)^T + \Gamma_{ss}
%     \\
%     = - (\Delta \Sigma_{ss} + \Sigma_{ss} \Delta^T) - (A_{sf} \Sigma_{fs} + \Sigma_{fs} A_{sf}^T)  + \Gamma_{ss}
%     \\
%     \Leftrightarrow \Delta \Sigma_{ss} + \Sigma_{ss} \Delta^T +  (A_{sf} \Sigma_{fs} + \Sigma_{fs} A_{sf}^T)  = \Gamma_{ss} ,
% \end{align*}
% where we used that $H_t \to \Sigma_{fs} \Sigma_{ss}^{-1}$.
% {\color{red}
% This gives the correct $\Sigma_{ss}$ as in Eq. 2.6 in \citep{konda2004convergence} except above is missing a $-\bar{\gamma} \Sigma_{ss}$ term. 
% I think this term can be recovered if instead of above, we use the more precise limit $\Sigma_{t+1}^{ss} = \frac{\gamma_{t+1}}{\gamma_t} \Sigma_t^{ss} \Rightarrow \Sigma_{t+1}^{ss} - \Sigma_t^{ss} = -1 + \frac{\gamma_{t+1}}{\gamma_t}$. 
% }
% Here we see that any $Q_t$ can be chosen as long as $\gamma_t^3 Q_t \ll \gamma_t^2$ so that the higher order terms are indeed negligible. 

% Now we will need to choose $Q_t$ so that it is Hurwitz, from which we can prove decay of the induction
% \begin{align*}
%     \bar{y}_n = \bar{b}_n + \frac{1}{n} \sum_{t=1}^n \Phi_{tn} (Q_t - B_t^{ss} - A_{sf} H_t) \hat{y}_t + \frac{1}{n} \sum_{t=1}^n \Phi_{tn} A_{sf} (\tilde{x}_t - H_t \hat{y}_t) + \frac{1}{n} \sum_{t=1}^n \Phi_{tn} V_t \\ 
%     + \frac{1}{n} \sum_{t=1}^n Q^{-1} (Q_t - B_t^{ss} - A_{sf} H_t) \hat{y}_t + \frac{1}{n} \sum_{t=1}^n Q^{-1} A_{sf} (\tilde{x}_t - H_t \hat{y}_t) + \frac{1}{n}\sum_{t=1}^n Q^{-1} V_t ,
% \end{align*}
% where we overload notation for 
% \begin{align*}
%     \Phi_{tn} = \gamma_t \sum_{j=t+1}^n \prod_{k=t+1}^j (I - \gamma_k Q_k) .
% \end{align*}
% {\color{red}What if we choose $Q^{-1} = \Sigma_{ss}^{1/2} \Gamma_{ss}^{-1/2}$ and prove that for this choice we have $\Phi_{tn} (\tilde{x}_t - H_t \hat{y}_t)$ decays sufficiently fast?
% The reason for this choice is so that we can apply CLT to the last term. 
% But the problem is $Q_t$ is then not necessarily H\"{u}rwitz. 
% }
% Ultimately, we will get a different covariance matrix for $\bar{y}_n$ because $\Sigma_{ss}$ is the covariance for the last iterate.


% % If $\Delta + A_{sf} H_t$ is H\"{u}rwitz, then we have that the norm of $I - ()$ can be bounded using weighted norm with weight satisfying Lyapunov equation. 
% % {\color{red}Let us define $\Phi_{tn}$} and write
% % \begin{equation}
% %     \sqrt{n} \hat{\mu}_{yn} = \frac{1}{\sqrt{n}} \sum_{t=1}^n \prod_{k=1}^t (I - \gamma_k (\Delta + A_{sf} H_t )) \hat{y}_0 + \frac{1}{\sqrt{n}} \sum_{t=1}^n \Phi_{tn} A_{sf} (\tilde{x}_t - H_t \hat{y}_t) + \frac{1}{\sqrt{n}} \sum_{t=1}^n \Phi_{tn} V_t ,
% % \end{equation}
% % where $\Phi_{tn}$ {\color{red}A modified version} has to satisfy the last iterate's covariance \citep{konda2004convergence}
% % \begin{equation}
% %     Cov(\Phi_{tn} V_t) \approx \Sigma_{ss}: \Delta \Sigma_{ss} + \Sigma_{ss} \Delta^T - \bar{\gamma} \Sigma_{ss} = - (A_{sf} \Sigma_{fs} + \Sigma_{sf} A_{sf}^T) + \Gamma_{ss} , 
% % \end{equation}
% % where $\bar{\gamma} = \lim_{t \to \infty} \gamma_{t+1}^{-1} - \gamma_t^{-1}$.
% % {\color{red}Note that this is a rough estimate, since the covariance of Polyak-Ruppert average on the slow iterate is not established in literature.}
% % Using the LMMSE property (\citet{HajekRandomProcess} Eq. 3.7)
% % \begin{equation}
% %     Cov(\tilde{x}_t - E[\tilde{x}_t | \hat{y}_t]) = Cov(\tilde{x}_t) - Cov(\tilde{x}_t, \hat{y}_t) Cov^{-1} (\hat{y}_t) Cov(\hat{y}_t, \tilde{x}_t) ,
% % \end{equation}
% % \textbf{\color{blue}Most important part: can we show that the $\tilde{x}_t$ covariance terms cancel out to obtain a rate of $Cov (\hat{y}_t) \sim \gamma_t$?}
% % Note that this implies 
% % \begin{align*}
% %     \alpha_t \Sigma_{ff} - \gamma_t H_t \Sigma_{fs} = O(\gamma_t) \Leftrightarrow 
% %     H_t = \frac{\alpha_t}{\gamma_t} (\Sigma_{ff} \Sigma_{fs}^{-1} + O(\gamma_t)) .
% % \end{align*}
% % {\color{blue}Since in this case we have that $H_t \gg \Delta$, perhaps we need to use induction by defining $\Phi$ with respect to $H_t$ excluding $\Delta$?}
% % If this is all true, then we can proceed with the former method of obtaining a bound on the expected norm of individual terms.


% Note that by definition of LMMSE, we have from Eq. 3.7 in \citep{hajekRandomProcess} that
% \begin{align*}
%     Cov(\tilde{x}_t - H_t \hat{y}_t ) = Cov(\tilde{x}_t) - Cov(\tilde{x}_t, \hat{y}_t) Cov(\hat{y}_t)^{-1} Cov(\hat{y}_t, \tilde{x}_t) .
% \end{align*}
% In the limit, we already know the RHS terms to be
% \begin{align*}
%     Cov(\tilde{x}_t - H_t \hat{y}_t ) \to \alpha_t \Sigma_{ff} - \gamma_t \Sigma_{fs} \Sigma_{ss}^{-1} \Sigma_{sf} .
% \end{align*}
% {\color{blue}The following may be useful:
% In the univariate case ($d = 1$), we have that
% }
% \begin{align*}
%     Cov(\tilde{x}_t - E[\tilde{x}_t | \hat{y}_t]) &= (1 - \rho_t^2) Var(\tilde{x}_t) \sim (1 - \rho_t^2) \alpha_t \\
%     E \lVert E[\tilde{x}_t | \hat{y}_t] \rVert &\leq \sqrt{Var E[\tilde{x}_t | \hat{y}_t]} \leq \sqrt{Var \tilde{x}_t} \sim \sqrt{\alpha_t} ,
% \end{align*}
% where the second line uses Jensen, law of total variance, and the MSE of $\tilde{x}_t$. 
% If we obtain a similar expression for the matrix case with $\rho_t^2 \geq 1 - t^{-1}$, then the covariance of LMMSE error term decays fast enough as 
% \begin{align*}
%     n^{-1/2} \sum_{t=1}^n \sqrt{(1 - \rho_t^2) \alpha_t} = n^{-1/2} \sum_{t=1}^n t^{-1/2 - \delta / 2} \sim n^{-\delta / 2}
% \end{align*}
% which decays for every $\delta > 0$ (this is the corresponding CLT rate if this term dominates).
% {\color{red}Notice that if this is true, then we have that while fast term benefits from small $\delta$, the slow term here benefits from large $\delta$.}



