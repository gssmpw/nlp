

\section{{\color{red}Delete after completing above sections:} Linear Two Time-Scale Stochastic Approximation}
{\color{red}These sections currently contain some more details from \citep{konda2004convergence} and \citep{kaledin2020finite}. 
These are useful in proving auxiliary lemmas for the mean-square convergence rates.
I can safely delete this once the above sections are complete. 
}
Consider finding the roots $(x^*, y^*)$ of the linear systems
\begin{align*}
    &F(x, y) = A_{ff} x + A_{fs} y , 
    &G(x, y) = A_{sf} x + A_{ss} y    \\
    &F(x^*, y^*) = 0 , 
    &G(x^*, y^*) = 0  .
    % F(x, y) &= A_{ff} x + A_{fs} y , 
    % G(x, y) &= A_{sf} x + A_{ss} y    
    % \\
    % F(x^*, y^*) &= 0 , 
    % G(x^*, y^*) &= 0  .
\end{align*}
In many applications, the exact operators $F$ and $G$ are either not available or expensive to compute, and the parameters $x$ and $y$ are updated using stochastic iterates
\begin{equation}\label{eq:updates}
    x_{t+1} = x_t - \alpha_t (F(x_t, y_t) - W_t) , \quad y_{t+1} = y_t - \gamma_t (G(x_t, y_t) - V_t) 
\end{equation}
with step-sizes $\{(\alpha_t, \gamma_t)\}$ such that $\gamma_t/\alpha_t = o(t)$, where $x_t$ and $y_t$ are called the ``fast'' and ``slow'' iterates.


To analyze the convergence of $(x_t, y_t) \to (x^*, y^*)$, it is common to assume or impose some structure on the fast and slow systems \citep{konda2004convergence} which we state below.
Observe that there exists a solution $x_\infty: \mathcal{Y} \to \mathcal{X}$ such that $F(x_\infty (y), y) = 0$ for every $y \in \mathcal{Y}$ given by $x_{\infty} (y) = -A_{ff}^{-1} A_{fs} y$.
Our first assumption is that the fast and slow systems are both asymptotically stable, i.e. $x_t \to x_\infty (y)$ for every fixed $y$ and $y_t \to y^*$ under $x_\infty (y_t)$.
To relate this assumption on dynamics of the systems to the system's parameters, observe that $y^*$ is a global attractor iff for $\hat{y} = y - y^*$, the system
\begin{align*}
    y - y^* = G(x_\infty (y), y) - G(x_{\infty}(y^*), y^*) =
    % A_{sf} x_\infty (\hat{y}) + A_{ss} \hat{y} = 
    (A_{ss} -A_{sf} A_{ff}^{-1} A_{fs}) \hat{y} 
\end{align*}
is stable for every $y$.
Therefore, we assume the following:
\begin{assumption}
    The matrices $-A_{ff}$ and $-\Delta = -(A_{ss} - A_{sf} A_{ff}^{-1} A_{fs})$ are H\"{u}rwitz.
\end{assumption}


Denote $\hat{x}_t = x_t - x_\infty (y_t)$ and $\hat{y}_t = y_t - y^*$.
To analyze the convergence of $\hat{x}_t \to 0$, we follow \citep{konda2004convergence,kaledin2020finite} and choose a linear transformation $\tilde{x}_t = \hat{x}_t + L_t \hat{y}_t$ to treat $\tilde{x}_t$ as a single time-scale stochastic approximation.
Observe that for any $L_t$, the recursions can be written as
\begin{align*}
    \tilde{x}_{t+1} &= (I - \alpha_t B^{ff}_t) \tilde{x}_t + \alpha_t B^{fs}_t \hat{y}_t + \alpha_t W_t + \gamma_t (L_{t+1} + A_{ff}^{-1} A_{fs}) V_t \\ 
    \hat{y}_{t+1} &= (I - \gamma_t B^{ss}_t) \hat{y}_t -\gamma_t A_{sf} \tilde{x}_t + \gamma_t V_t ,
\end{align*}
where
\begin{align}
    B^{ff}_t &= \frac{\gamma_t}{\alpha_t}(L_{t+1} + A_{ff}^{-1} A_{fs}) A_{sf} + A_{ff} , \quad 
    B^{ss}_t = \Delta - A_{sf} L_t , \\
    B^{fs}_t &= \frac{1}{\alpha_t}(L_t - L_{t+1}) + \frac{\gamma_t}{\alpha_t}(L_{t+1} + A_{ff}^{-1} A_{fs}) B^{ss}_t - A_{ff} L_t .
\end{align}
By defining the sequence $\{L_t\}$ recursively with $L_0 = 0$ as
\begin{equation}
    L_{t+1} \coloneqq \left(L_t - \alpha_t A_{ff} L_t + \gamma_t A_{ff}^{-1} A_{fs}(\Delta - A_{sf} L_t)\right) (I - \gamma_t (\Delta - A_{sf} L_t))^{-1}  ,
\end{equation}
the transformed systems simplify to $B^{fs}_t = 0$, and $\tilde{x}_t$ can be treated as a single time-scale update.
The recursion \eqref{eq:Lt} is well-defined for the following choice of step-sizes $\{(\alpha_t, \gamma_t)\}$ :
\begin{proposition}
    When step sizes are chosen so that
    \begin{equation}\label{eq:stepsize}
        \gamma_0 \leq \min \frac{1}{2} \left\{ \frac{1}{\lVert Q_\Delta \rVert^2 \lVert \Delta\rVert^2_{Q_\Delta}}, \frac{\lVert Q_\Delta\rVert^2}{\lVert \Delta \rVert_{Q_\Delta} \lVert Q_\Delta\rVert^2 + 1} \right\}
        , \quad 
        \alpha_0 \leq \frac{1}{2 \lVert Q_{ff}\rVert^2 \lVert A_{ff}\rVert_{Q_ff}^2} ,
    \end{equation}    
    there exists a sequence of matrices $\{L_t\}$ with $L_0 = 0$ such that $I - \gamma_t (\Delta - A_{sf}L_t)$ is non-singular. 
    Consequently, there exists a sequence satisfying the recursion \eqref{eq:Lt} for all $t >0$.
\end{proposition}
Using this choice of initial step size and setting $\gamma_t = \gamma_0 t^{-1}, \alpha_t = \alpha_0 t^{-\delta}$ for some $\delta \in (0, 1)$ (valid for arguments in \citet{polyakJuditsky}; need $\delta > 1/2$ for arguments in \citet{konda2004convergence}), we will then use the transformation \eqref{eq:Lt} and write the fast iterate as
\begin{equation}
    \tilde{x}_{t+1} = (I - \alpha_t B^{ff}_t) \tilde{x}_t + \alpha_t W_t + \gamma_t (L_{t+1} + A_{ff}^{-1} A_{fs})V_t .
\end{equation}



