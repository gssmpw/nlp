% \section{Decoupled Analysis}
% \subsection{Notations}
% We use $Q_A$ to denote the solution to the Lyapunov equation 
% \begin{equation}
%     A Q + Q A^T = I 
% \end{equation}
% for a stable matrix $A$. 
% The weighted operator $\lVert \cdot \rVert_{Q, Q'}$ is defined as
% \begin{equation}
%     \lVert A \rVert_{Q, Q'} = \sup_{x \neq 0} \frac{x^T A^T Q' A x}{x^T Q x} = \frac{\inp{A x}{Ax}_{Q'}}{\inp{x}{x}_{Q}} .
% \end{equation}
% A single subscript $Q$ is used when $Q = Q'$, indicating that the domain and co-domain inner product weights are identical.


% {\color{red}
% Below proofs will require fixes on (1) summation indices (e.g. step size starting from 0 vs. PR average from 1) and (2) precise orders of taking Martingale expectations.
% }

% \subsection{Choice of Coordinate Transformation}
\subsection{Polyak-Ruppert Average}
In this paper, we focus on the Polyak-Ruppert averages $\mu_{xn} \coloneqq n^{-1} \sum_{t=1}^n x_t$ and $\mu_{yn} \coloneqq n^{-1} \sum_{t=1}^n y_t$.
We then analyze the convergence of $(\mu_{xn}, \mu_{yn})$ to $(x_\infty (\mu_{yn}), y^*)$ by defining the residues
\begin{align*}
    \hat{\mu}_{xn} &\coloneqq \mu_{xn} - x_\infty (\mu_{yn}) = \frac{1}{n}\sum_{t=1}^n (x_t - x_\infty (y_t))  = \frac{1}{n}\sum_{t=1}^n \hat{x}_t, 
    \\
    \hat{\mu}_{yn} &\coloneqq \mu_{yn} - y^* .
\end{align*}
To utilize the aforementioned technique of transforming $\hat{x}_t$ to a single time-scale update, we require that the convergence of $\hat{\mu}_{xn}$ can be recovered from that of the decoupled iterate $n^{-1} \sum_{t=1}^n \tilde{x}_t$:
\begin{align*}
    \hat{\mu}_{xn} = \frac{1}{n}\sum_{t=1}^n (\tilde{x}_t - L_t y_t) 
    % \Rightarrow 
    % \lVert \hat{\mu}_{xn} \rVert 
    % \\
    \Rightarrow 
    \mathbb{E} \lVert \hat{\mu}_{xn} \rVert 
        \leq 
    % \frac{1}{n} \mathbb{E}\sum_{t=1}^n \lVert  \tilde{x}_t \rVert + \mathbb{E}\lVert \frac{1}{n}\sum_{t=1}^n L \hat{y}_t \rVert + \mathbb{E}\lVert \frac{1}{n}\sum_{t=1}^n (L_t - L) \hat{y}_t \rVert 
    % \\
        % & \leq
    % O(n^{-1} \sum_{t=1}^n \sqrt{\alpha_t}) 
    \frac{1}{n} \mathbb{E} \lVert \sum_{t=1}^n \tilde{x}_t \rVert
    % + \lVert L\rVert  \mathbb{E} \lVert \hat{\mu}_{yn} \rVert 
    + \frac{1}{n} \mathbb{E} \lVert \sum_{t=1}^n L_t \hat{y}_t \rVert .
    % + \sqrt{\frac{1}{n^2} \sum_{t=1}^n \lVert  \hat{y}_t \rVert^2}
    % + \frac{1}{n} (n \max_{t} \lVert L_t - L \rVert^2)^{1/2}  \sqrt{\sum_{t=1}^n \lVert \hat{y}_t \rVert^2}
    % \\ &= 
    % O(n^{-\delta/2 }) 
    % \frac{1}{n} \sum_{t=1}^n \lVert  \tilde{x}_t \rVert
    % + O(\lVert \hat{\mu}_{yn} \rVert ) + O(\sqrt{\frac{\log n}{n}}) ,
\end{align*}
Here we show that the last term decays sufficiently fast.
\begin{lemma}[Lemma A.1 \citet{konda2004convergence}; Lemma C.4 in \citep{haque2023tightfinitetimebounds}]\label{lem:Ltsize}
    {\color{red}(For a suitable choice of step-sizes) $\lVert L_t \rVert \leq \gamma_t / \alpha_t$.}
\end{lemma}
\begin{proof}
    Recall the definition of $B_t^{ss} = \Delta - A_{sf} L_t$.
    The recursion \eqref{eq:Lt} can be expressed as
    \begin{equation}
        L_{t+1} = (I - \alpha_t A_{ff}) L_t + \gamma_t D_t (L_t) , \;
        D_t (L_t) = \left(A_{ff}^{-1} A_{fs} + (I - \alpha_t A_{ff}) L_t \right)B_t^{ss} \left(I - \gamma_t B_t^{ss}\right)^{-1} .
    \end{equation}    
    {\color{red}Show $D_t$ is bounded; weighted norm allows us to write a recursive inequality (triangular); Then by induction we have the result.}
\end{proof}
Together with the fact that $\mathbb{E}\lVert \hat{y}_t \rVert^2 = O(\gamma_t)$, we have a bound on the last term
\begin{align*}
    \frac{1}{n}\sum_{t=1}^n \mathbb{E}\lVert L_t \hat{y}_t \rVert \leq \frac{1}{n} \sum_{t=1}^n \frac{\gamma_t}{\alpha_t} \sqrt{\gamma_t} = O\left(n^{\delta - 3/2}\right) .
\end{align*}
In the next section, we obtain a bound on the remaining $n^{-1} \sum_{t=1}^n \tilde{x}$ term.

\subsection{Fast Iterate Convergence}\label{sec:fast}
In this section, we establish a non-asymptotic central limit theorem for the Polyak-Ruppert average $n^{-1} \sum_{t=1}^n x_t$ to the solution $x_\infty (\mu_{yn})$, where $\mu_{yn}$ is the Polyak-Ruppert average of the slow iterate:
\begin{theorem}\label{thm:CLT_fast}
    Let $\{W_t\}$ be a martingale difference sequence satisfying 
    \begin{align*}
        \mathbb{E}\lVert W_t \rVert^{2 + \beta} < \infty, \quad \mathbb{E} W_t W_t^T = \Gamma_{ff} , \quad \forall t
    \end{align*}
    for some $\beta \in (0, 1)$. 
    The Polyak-Ruppert average $\mu_{xn}$ of the fast iterates $\{x_t\}$ in the updates \eqref{eq:updates} with step-sizes $\alpha_t \propto t^{-\delta}, \delta \in (1/2, 1)$ and $\gamma_t \propto t^{-1}$ converges in distribution to a normal random variable $Z \sim \mathcal{N}(0, \Sigma_{ff})$ with $\Sigma_{ff} = A_{ff}^{-1}\Gamma_{ff} A_{ff}^{-T}$, with a rate measured by the Wasserstein-1 distance
    \begin{equation}
        \sup_{h \in Lip_1} \mathbb{E}\left[
            h\left(n^{1/2} \left(\mu_{xn} - x_\infty (\mu_{yn}) \right) \right) - h\left(Z\right)
        \right] \leq 
        C(\delta, \beta, d) \sqrt{\frac{\mathrm{Tr} \Sigma_{ff}}{n^{1 - \delta}}} ,
    \end{equation}
    where $C(\delta, \beta, d)$ is a constant defined for all $\delta \in (1/2, 1)$, $\beta \in (0, 1)$, and state-space dimension $d$.
\end{theorem}
\begin{remark}
    This indicates that $\mathbb{E}\lVert \mu_{xn} - x_\infty (\mu_{yn}) \rVert n^{-1} \mathrm{Tr} \Sigma_{ff}$. 
    The rate of convergence is giving more insights. 
\end{remark}



The remainder of this section is dedicated to the proof of Theorem \ref{thm:CLT_fast}. 
Our technique is to decouple the fast residue $x_t - x_\infty (y_t)$ from the slow iterate $y_t$ using a linear transformation, and then to approximate the resulting variable as a sum of martingale differences from which we show that the error terms' first moments decay to zero individually. 
By the non-asymptotic central limit theorem in \citep{srikant2024CLT}, we then establish convergence in distribution to a random variable whose covariance matches the optimal covariance of single time-scale stochastic approximation \citep{polyakJuditsky}.

\subsubsection{Proof}
Recall the recursion for $\tilde{x}_t$:
\begin{align*}
    \tilde{x}_{t+1} 
    &= (I - \alpha_t A_{ff}) \tilde{x}_t + \alpha_t (A_{ff} - B^{ff}_t) \tilde{x}_t
    - \gamma_t (L_{t+1} + A_{ff}^{-1} A_{fs}) V_{t} - \alpha_t W_{t} 
    \\
    &= (I - \alpha_t A_{ff}) \tilde{x}_t + \gamma_t (L_{t+1} + A_{ff}^{-1} A_{fs}) A_{sf}  \tilde{x}_t
    - \gamma_t (L_{t+1} + A_{ff}^{-1} A_{fs}) V_{t} - \alpha_t W_{t} ,
\end{align*}
where we use
$A_{ff} - B_j^{ff} = \frac{\gamma_j}{\alpha_j}(L_{j+1} + A_{ff}^{-1} A_{fs}) A_{sf}$.
By induction, 
\begin{align}
    \tilde{x}_t 
    &= \prod_{k=0}^{t-1} (I - \alpha_k A_k^{ff}) \tilde{x}_0 
    + \sum_{j=0}^{t-1}  \gamma_j  \prod_{k=0}^{t-1} (I - \alpha_k A_{ff})(L_{j+1} + A_{ff}^{-1} A_{fs}) A_{sf} \tilde{x}_j
    \\
    &+ \sum_{j=0}^{t-1} \gamma_j \prod_{k=j+1}^{t-1} (I - \alpha_k A_{ff}) (L_{j+1} + A_{ff}^{-1} A_{fs}) V_j 
    % \\
    + \sum_{j=0}^{t-1} \alpha_j \prod_{k=j+1}^{t-1} (I - \alpha_k A_{ff}) W_j .    \label{eq:xtilde}
\end{align}
Using a change of summation $\sum_{t=1}^{n} \sum_{j=1}^{t} a_{j,t} = \sum_{j=1}^{n} \sum_{t=j+1}^{n} a_{j,t}$, we obtain for the Polyak-Ruppert average $n^{-1} \sum_{t=1}^n \tilde{x}_t$ that
\begin{align*}
    &\frac{1}{\sqrt{n}} \sum_{t=1}^n \tilde{x}_t 
    = \frac{1}{\sqrt{n}} \sum_{t=1}^n \prod_{k=0}^{t-1}(I - \alpha_k A_{ff}) \tilde{x}_0 
    \\ 
    & + \frac{1}{\sqrt{n}}\sum_{j=1}^{n} \frac{\gamma_j}{\alpha_j} \alpha_j \sum_{t=j+1}^n \prod_{k=j+1}^{t} (I - \alpha_k A_{ff}) (L_{j+1} + A_{ff}^{-1} A_{fs})A_{sf} \tilde{x}_j      \\ & 
    + \frac{1}{\sqrt{n}}\sum_{j=1}^{n} \frac{\gamma_j}{\alpha_j} \alpha_j \sum_{t=j+1}^n \prod_{k=j+1}^{t} (I - \alpha_k A_{ff}) (L_{j+1} + A_{ff}^{-1} A_{fs})A_{sf} V_j
    \\ &
    - \frac{1}{\sqrt{n}}\sum_{j=1}^{n} \alpha_j \sum_{t=j+1}^n \prod_{k=j+1}^{t} (I - \alpha_k A_{ff}) W_j . \numberthis \label{eq:xtilde_induction}
\end{align*}
A recurring quantity in the iterative expression for the scaled Polyak-Ruppert average $n^{-1/2} \sum_{t=1}^n \tilde{x}_t$ is the operator
\begin{equation}
    \Phi_{tn} = \alpha_t \sum_{j=t+1}^n \prod_{k=t+1}^{j} (I - \alpha_k A_{ff}) - A_{ff}^{-1} ,
\end{equation}
which is studied in \citep{polyakJuditsky,srikant2024CLT,kaledin2020finite}. 
Equation \eqref{eq:xtilde_induction} can then be simplified to 
\begin{align*}
    % \sqrt{n} \hat{\mu}_{xn} - \frac{1}{\sqrt{n}} \sum_{t=1}^n L_t \hat{y}_t 
    \frac{1}{\sqrt{n}} \sum_{t=1}^n \tilde{x}_t
    &= \frac{1}{\sqrt{n} \alpha_0} \Phi_{1n} \tilde{x}_0 
    \\ &+
    \frac{1}{\sqrt{n}} \sum_{t=1}^n \frac{\gamma_t}{\alpha_t} \left(\Phi_{tn} + A_{ff}^{-1}\right) (L_{t+1}+A_{ff}^{-1} A_{fs}) A_{sf} \tilde{x}_t
    \\
    &+ \frac{1}{\sqrt{n}} \sum_{t=1}^n \frac{\gamma_t}{\alpha_t} \left(\Phi_{tn} + A_{ff}^{-1}\right) (L_{t+1} + A_{ff}^{-1} A_{fs}) A_{sf} V_t 
    \\
    &+ \frac{1}{\sqrt{n}} \sum_{t=1}^n \Phi_{tn}W_t 
    + \frac{1}{\sqrt{n}} \sum_{t=1}^n A_{ff}^{-1} W_t.
\end{align*}
Here we state two properties that we will use to prove convergence in distribution:
\begin{lemma}\label{lem:Phi}
    The operator $\Phi_{tn}$ is uniformly bounded for all $t \in [n]$ and satisfies
    \begin{equation}
        \alpha_n \sum_{t=1}^n \lVert \Phi_{tn} \rVert^2 = \mathcal{O}(1) .
    \end{equation}
    See \citep{srikant2024CLT} for the square sum bound. 
\end{lemma}
To analyze the expected norm of each term above, we use the following mean squared errors on the last iterate $\tilde{x}_t$.
\begin{lemma}[Last Iterate Mean Square Error, Theorem 2.6 \citet{konda2004convergence}]\label{lem:LastFast}
    % For any $\alpha_t = t^{-\delta}$ with $\delta \in (0, 1)$ {\color{blue}(when using \citep{polyakJuditsky} vs. $\delta \in (1/2, 1)$ when using arguments from \citep{konda2004convergence})}, 
    For any $\alpha_t \propto t^{-\delta}$ with $\delta \in (1/2, 1)$, 
    \begin{equation}
        \lim_{t \to \infty} \left(\mathbb{E} \lVert \tilde{x}_t \rVert^2 - \alpha_t \mathrm{Tr} \Sigma_{ff}\right) = 0 ,
    \end{equation}
    where the covariance $\Sigma_{ff}$ satisfies the Lyapunov equation
    \begin{equation}
        A_{ff} \Sigma_{ff} + \Sigma_{ff} A_{ff}^T = \Gamma_{ff} .
    \end{equation}
\end{lemma}
Lastly, we will use the non-asymptotic central limit theorem from \citep{srikant2024CLT} for a sum of martingales.
\begin{lemma}[Theorem 1, \citet{srikant2024CLT}]\label{lem:CLT}
    Consider a martingale difference sequence $\{W_t\}$ with covariance $\Sigma = \lim_{n \to \infty} n^{-1} \sum_{t=1}^n \mathbb{E}W_t W_t^T$ and finite moment $\mathbb{E}[\lVert W_t \rVert^{2 + \beta}] < \infty$ for all $t$, the Wasserstein-1 distance between $n^{-1/2} \sum_{t=1}^n W_t$ and $Z$ with $Z \sim \mathcal{N}(0, \Sigma)$ is given by
    \begin{align*}
        d\left(n^{-1/2} \sum_{t=1}^n W_t, Z\right) \leq C(\beta) n^{-\beta / 2} ,
    \end{align*}
    where $C(\beta) \to \infty$ as $\beta \to 1$.
\end{lemma}
We use the above results to obtain bounds on the individual terms.
\begin{remark}[{\color{red}Optimal rate with respect to $\delta$}]
    Below we will see a behavior that depends on $\delta$ that is different from \citep{srikant2024CLT}. 
    The reason is that his model incurs the trade-off because of Markovian $A$, where the last iterate benefits from larger $\delta$.

    In two-timescale stochastic approximation, Lemma \ref{lem:LastFast} shows that the MSE $E\lVert \tilde{x}_t \rVert = n^{-\delta}$ also benefits using a small step-size, i.e. large $\delta$.
    However, the transformation results in a term $\gamma_t / \alpha_t \tilde{x}_t$ which actually grows with $\delta$.
    Moreover, the remaining terms have rates dominated by $\sum_t \lVert \Phi_{tn} \rVert^2 = n^\delta$, which also grows with $\delta$.
    Therefore, we see below that the order-rates will improve with smaller $\delta$, with a restriction that $\delta \not \in \{0, 1/2, 1\}$ because of the appearing constants.
\end{remark}



\begin{enumerate}
    \item First term is deterministic:
    \begin{align*}
        \frac{1}{\sqrt{n}} \sum_{t=1}^n \prod_{k=0}^{t-1}(I - \alpha_k A_{ff}) \tilde{x}_0 .
    \end{align*}
    When the step-size is sufficiently small ({\color{red}ref: \citet{kaledin2020finite} for weighted norm; we can relate to unweighted norm}), this term decays exponentially fast.    
    But because all the other terms will be much slower, even the very crude bound of
    $n^{-1/2} \lVert \Phi_{0n}/\alpha_0 \rVert \mathbb{E}[\lVert \tilde{x}_0\rVert] = O(n^{-1/2})$ is also sufficient for our purpose.
    
    \item Second term: Let's ignore $L_{j+1} + A_{ff}^{-1} A_{fs}$ which is bounded. 
    Using that $\lVert \Phi_{tn} + A_{ff}^{-1} \rVert$ is bounded, the second term's rate is determined by 
    \begin{align*}
        &\mathbb{E} \frac{1}{\sqrt{n}} \lVert \sum_{t=1}^n \frac{\gamma_t}{\alpha_t}  \tilde{x}_t \rVert 
        \leq \frac{1}{\sqrt{n}} \sum_{t=1}^n \mathbb{E} \lVert \frac{\gamma_t}{\alpha_t} \tilde{x}_t \rVert 
        \\
        \leq
        n^{-1/2} \sum_{t=1}^n t^{\delta - 1} \sqrt{\mathbb{E} \lVert \tilde{x}_t \rVert^2}
        &= n^{-1/2} \sum_{t=1}^n t^{\delta/2 - 1}
        = \frac{2}{\delta} n^{\frac{1}{2}(\delta - 1)} .
    \end{align*}
    where we use $\gamma_t/\alpha_t = t^{-1 + \delta}$ and that $\mathbb{E}\lVert \tilde{x}_t \rVert \leq \sqrt{Var \tilde{x}_t} = O(\sqrt{\alpha_t}) = O(n^{\delta/2})$.

    % An alternative to (a) above is to use variance directly
    
    {\color{red}
    Notice that this is why increasing $\delta$ is bad: While it would have had a benefit without the ratio $\gamma_t / \alpha_t$ term, we see that the ratio results in an error term that overall is worsened with larger $\delta$.
    I believe there should also be a constant that prevents us from getting $\delta \to 1/2$ for the MSE rate, which should appear from the \citep{konda2004convergence} MSE analysis. 
    }

    {\color{blue}Is it possible to improve this rate by showing $\mathbb{E}\lVert n^{-1} \sum_{t=1}^n \tilde{x}_t \rVert^2 = O(n^{-1})$?}
    % \begin{align*}
    %     \mathbb{E}\lVert \frac{1}{\sqrt{n}} \sum_{t=1}^n \frac{\gamma_t}{\alpha_t} \tilde{x}_t \rVert \leq \sqrt{\sum_{t=1}^n \frac{\gamma_t^2}{\alpha_t^2}} \sqrt{\mathbb{E} \lVert \frac{1}{n} \sum_{t=1}^n \tilde{x}_t \rVert^2}
    %     = n^{-\delta + 1/2} n^{-1/2} = n^{-\delta} .
    % \end{align*}
    % {\color{blue}This would indicate a trade-off, where this term benefits from larger $\delta \to 1$; we can't set it to 1 because of the constant $(1 - \delta)^{-1}$ appearing from $\sum_t t^{-\delta}$.}
    
    \item Third term involving $V_j$: 
    We are interested in the rate of 
    \begin{equation}
        \frac{1}{\sqrt{n}} \mathbb{E} \lVert \sum_{t=1}^n \frac{\gamma_t}{\alpha_t} V_t \rVert \leq \frac{1}{\sqrt{n}} \sqrt{\mathbb{E}\lVert \sum_{t=1}^n t^{-1 + \delta} V_t \rVert^2}.
    \end{equation}
    Because $\{V_t\}$ is a martingale difference sequence, the cross-term expectations are zero and we have
    \begin{align*}
         \frac{1}{\sqrt{n}} \mathbb{E} \lVert \sum_{t=1}^n \frac{\gamma_t}{\alpha_t} V_t \rVert \leq 
         n^{-1/2} \sqrt{\sum_{t=1}^n t^{-2 + 2\delta} \mathbb{E} \lVert V_t \rVert^2}
         = n^{-1/2} \sqrt{\mathrm{Tr} \Gamma_{ss} \frac{n^{-1 + 2\delta}}{2\delta - 1}}
         = \sqrt{\frac{\mathrm{Tr} \Gamma_{ss}}{2\delta - 1}} n^{-1 + \delta} .
    \end{align*}
    Note that this rate is always faster than $n^{-1/2}$ for any choice of $\delta \in (1/2, 1)$.
        
    \item Last term is factorized into
    \begin{align*}
        \frac{1}{\sqrt{n}}\sum_{t=1}^{n} \Phi_{tn} W_t + \frac{1}{\sqrt{n}} \sum_{t=1}^{n} A_{ff}^{-1} W_t .
    \end{align*}
    Using Lemma \ref{lem:CLT}, the last term converges to the normal distribution $\mathcal{N}(0, A_{ff}^{-1} \Gamma_{ff} A_{ff}^{-t}$ at rate $C(\beta) n^{-\beta/2}$. 
    
    
    Using Lemma \ref{lem:Phi} to get $\sum_{t=1}^n \lVert \Phi_{tn} \rVert^2 = O(n^{\delta})$, the first term behaves as
    \begin{align*}
        \mathbb{E}\lVert \frac{1}{\sqrt{n}} \sum_{t=1}^n \Phi_{tn} W_t \rVert 
        \leq \frac{1}{\sqrt{n}} \sqrt{\sum_{t=1}^n \lVert \Phi_{tn} \rVert^2 \mathbb{E} \lVert W_t \rVert^2}
        \leq 
        \frac{1}{n^{1/2 - \delta /2}} \sqrt{\mathrm{Tr} \Gamma_{ff}}
        % n^{-1/2} n^{\delta /2} \sqrt{\mathrm{Tr} \Gamma_{ff}} 
        % = O(n^{-\frac{1}{2}(1 - \delta)})
        .
        % \leq \sqrt{\frac{1}{n} \sum_t \lVert \Phi_{tn} \rVert^2 \mathrm{Tr} \Gamma_{ff}}
        % \leq 
        % \sqrt{n^{\delta-1} \mathrm{Tr} \Gamma_{ff}} 
        % = O(n^{(\delta-1)/2} \sqrt{\Gamma_{ff}}) .
    \end{align*}
    This is the same rate as the second term depending on $\tilde{x}_t$ with constants $1/\sqrt{\delta}$ and $1/(1-\delta)$ resulting from integration.    
\end{enumerate}
Representing $n^{-1/2} \sum_{t=1}^n \tilde{x}_t$ as a telescopic sum as in \citep{srikant2024CLT}, we have the following convergence to the random variable $Z \sim \mathcal{N}(0, A^{-1} \Gamma_{ff} A_{ff}^{-T})$:
\begin{equation}
    d\left(n^{-1/2} \sum_{t=1}^n \tilde{x}_t, Z\right) = O\left( \frac{n^{-1/2 + \delta / 2}}{\sqrt{2 \delta - 1}}\right) ,
\end{equation}
where we used that the combination of rates obtained above is dominated by
\begin{equation}
    \frac{n^{-1/2 + \delta/2}}{\delta (1 - \delta)}  + \frac{n^{-1 + \delta}}{\sqrt{2 \delta - 1}} \asymp
    n^{-1/2}\left(
        n^{\delta / 2} + \frac{n^{-1/2 + \delta}}{\sqrt{2 \delta -1}}
    \right)
    = 
    O\left( \frac{n^{-1/2 + \delta / 2}}{\sqrt{2 \delta - 1}}\right)
    .
\end{equation}
To recover the fast residue $n^{1/2} (\bar{x}_n - x_\infty (\bar{y}_n))$,
% \begin{align*}
%     n^{-1/2} \sum_{t=1}^n \hat{x}_t = n^{-1/2} \sum_{t=1}^n (x_t - x_\infty (y_t)) = \sqrt{n}(\bar{x}_t - x_\infty (\bar{y}_n)) ,
% \end{align*}
we recall $n^{-1/2} \sum_{t=1}^n \mathbb{E}\lVert L_t \hat{y}_t \rVert = O(n^{\delta - 1})$ to obtain
\begin{align*}
    \frac{1}{\sqrt{n}} \mathbb{E} \left\lVert \left(\sum_{t=1}^n x_t - x_\infty (y_t)\right) - \sum_{t=1}^n \tilde{x}_t \right\rVert = O(n^{\delta - 1}) ,
\end{align*}
which is much faster than the above rate.


