\section{Main Results}
In this section, we state a finite-time bound on the Wasserstein-1 distance between TSA-PR, scaled appropriately, and its Gaussian limit. 
We highlight the practical significance of this result by proving lower- and upper-bounds on the expected error. 
To this end, we first prove that the second moments of TSA error converge to zero at rates determined by the choice of step sizes. 


Recall the notations $\hat{x}_n = x_n - x^*, \hat{y}_n = y_n - y^*$ to denote the deviation of TSA $(x_n, y_n)$ from the solution $(x^*,y^*)$.
\begin{theorem}\label{thm:mse}
    \MSE
\end{theorem}
A proof is provided in Sections \ref{sec:fast_mse}--\ref{sec:slow_mse}.
The finite-time bounds in Theorem \ref{thm:mse} are needed to derive our next result in Theorem \ref{thm:clt}. 
Specifically, it is implied that
\begin{equation}\label{eq:lbub_mse}
    \lim_{n\to \infty} \lVert \mathbb{E}\hat{x}_{n+1} \hat{x}_{n+1}^T - \alpha_{n+1} \Sigma_{ff} \rVert = 0 , 
    \quad
    \lim_{n \to \infty} \lVert \mathbb{E}\hat{y}_{n+1} \hat{y}_{n+1}^T - \gamma_{n+1} \Sigma_{ss} \rVert = 0 .
\end{equation}
Note that the nature of Theorem 2 is similar to Theorem 4.1 in \citep{haque2023tightfinitetimebounds}. 
However, the latter is not directly applicable to our setting.
Our presentation provides a more convenient step to establish the convergence rates in Theorem \ref{thm:clt}. 
In addition, we develop a simple proof technique for Theorem \ref{thm:mse} compared to existing proofs for finite-time mean square error rates. 
% The above theorem is stronger than the more common upper bounds on the trace of $\mathbb{E}\hat{x}_{n+1} \hat{x}_{n+1}^T$ and $\mathbb{E}\hat{y}_{n+1} \hat{y}_{n+1}^T$ because the above also implies a lower bound on the second moments, confirming that the analysis is tight.
% Therefore, Theorem \ref{thm:mse} captures how the second moment of the errors are determined by the step sizes $\alpha_t$ and $\gamma_t$.
% indicates that the correct rate of decays are $\alpha_t$ and $\gamma_t$ in the second moment, 



Next, we present a non-asymptotic CLT for TSA-PR.
To our knowledge, \citet{mokkadem2006convergence} are the only authors who analyzed TSA-PR in the context of two-time-scale SA, where the authors prove its asymptotic behavior.
% However, the rates of convergence have not been studied. 
Utilizing the error bounds in Theorem \ref{thm:mse} for TSA, we establish a non-asymptotic CLT on the deviation $\sqrt{n}(\bar{z}_n - z^*)$ from a Gaussian variable $\Sigma^{1/2} Z$ by obtaining a finite-time bound on the Wasserstein-1 distance
\begin{equation}\label{eq:wasserstein_definition}
        d_1 \left(\sqrt{n}(\bar{z}_n - z^*), \Sigma^{1/2} Z\right) 
        \coloneqq \sup_{h \in \mathrm{Lip}_1} \mathbb{E}\left[h\left(\sqrt{n}(\bar{z}_n - z^*)\right) - h \left(\Sigma^{1/2} Z \right) \right]
        .    
\end{equation}
The limiting distribution of TSA-PR involves the Schur complement $G = A_{ff} - A_{fs} A_{ss}^{-1} A_{ss}$ of $A_{ss}$, as well as the Schur complement $\Delta$ defined in Section \ref{sec:preliminaries}.
\begin{theorem}\label{thm:clt}
    \CLT
\end{theorem}
A closed-form solution for $\bar{\Sigma}_{ff}$ and $\bar{\Sigma}_{ss}$ is given in Section \ref{sec:clt}, Eq. \eqref{eq:ttsapr_covariance}.
Because $a < b$, the dominant term is $n^{b/2}$, but we explicitly state the dependence on $a, b$, and $a-b/2$ which decays with $\gamma_t/\alpha_t$ to illustrate their roles. 
% \begin{remark}
% % Expected norm of the limiting Gaussian.
%    % Characterizing the distribution of the error can be more useful than moment bounds when assessing the ensemble behavior of realized trials.
%     %From the limiting distribution, it can be hypothesized that the expected error is related to the expected norm of the limiting Gaussian variable.
%     %This connection is formally established in Corollary \ref{cor:mae}.
%     % , which is possible because of our choice of the Wasserstein-1 distance.
%     % , rather than Berry-Esseen or other weaker metrics. 
% \end{remark}
By restricting the test functions $h$ in the Wasserstein-1 distance in Eq. \eqref{eq:wasserstein_definition} to be $h(x) \in \{-\lVert x \rVert, \lVert x \rVert\}$, we have the following lower and upper bounds on the expected error.
\begin{corollary}\label{cor:mae}
    Let $Z_1$ and $Z_2$ be standard Gaussian vectors. 
    Under Assumptions \ref{assumption:first}--\ref{assumption:last},
    \begin{equation}\label{eq:mae}
        \begin{split}
            \Big \lvert \mathbb{E}\lVert G(\bar{x}_n - x^*) \rVert - \frac{1}{\sqrt{n}} \mathbb{E}\lVert (G \bar{\Sigma}_{ff} G^T)^{1/2} Z_1 \rVert 
            \Big \rvert = o(n^{-1/2})
            \\ 
            \Big \lvert \mathbb{E}\lVert \Delta (\bar{y}_n - y^*) \rVert - \frac{1}{\sqrt{n}} \mathbb{E}\lVert (\Delta \bar{\Sigma}_{ss} \Delta^T)^{1/2} Z_2 \rVert \Big \rvert = o(n^{-1/2})    
            % \mathbb{E}\lVert G(\bar{x}_n - x^*) \rVert = \frac{1}{\sqrt{n}} \mathbb{E}\lVert (G \bar{\Sigma}_{ff} G^T)^{1/2} Z_1 \rVert + o(n^{-1/2})
            % \\ 
            % \mathbb{E}\lVert \Delta (\bar{y}_n - y^*) \rVert = \frac{1}{\sqrt{n}} \mathbb{E}\lVert (\Delta \bar{\Sigma}_{ss} \Delta^T)^{1/2} Z_2 \rVert + o(n^{-1/2})    
        \end{split}
        .
    \end{equation}    
\end{corollary}
The bounds on the expected error is tight, in the sense that the expected error is bounded above by rate $n^{-1/2}$, and similar to Eq. \eqref{eq:lbub_mse}, both $\mathbb{E}\lVert \bar{x}_n - x^*\rVert$ and $\mathbb{E}\lVert \bar{y}_n - y^* \rVert$ are lower bounded by the expected norm of the limiting Gaussian vector times the rate $n^{-1/2}$.
Replacing the norm of the Gaussian vector with the trace of its covariance would yield $\mathcal{O}(n^{-1/2})$ instead of the $o(n^{-1/2})$ above. 
Theorem \ref{thm:clt} shows that the step sizes determine the quality of a Gaussian approximation of $\bar{z}_n - z^*$ at any $n \geq 1$, whereas Corollary \ref{cor:mae} shows that the expected error decays at rate $n^{-1/2}$, with the step sizes $1/2 < a < b < 1$ determining only the transient behavior. 

\begin{remark}\label{rem:comparison_with_mse}
% Comparing with MSE bounds. 
    As mentioned in Section \ref{sec:literature}, a finite-time performance guarantee on TSA-PR has not been established in literature.
    Second moment bounds, which are common metrics of interest in the analysis of stochastic approximation algorithms, are inevitably tied to the trace of an algorithm's asymptotic covariance. 
    We emphasize that such bounds only provide a conservative estimate of $n^{-1/2} \sqrt{\mathrm{Tr} \bar{\Sigma}}$ on the expected error.
    % The difference between $\sqrt{\mathrm{Tr}\bar{\Sigma}}$ and the norm of the corresponding Gaussian vector can grow proportionally to $\sqrt{d}$, where $d$ is the dimension of the Gaussian vector. 
\end{remark}

%Although we establish a relatively strong notion of convergence, with respect to the Wasserstein-1 distance, it is worth comparing the practical significance of its implication to the mean absolute error with existing high probability bounds.

% {\color{red}Include an example that demonstrates this gap, where MAE is much smaller than MSE. 
% % The gap may be extremely large for high dimensional vectors, for example when .
% This example should be constructed by considering a special case of $A$ and $\Gamma$, not just the limit. 
% But in this route, we see that if the limit is a Gaussian variable which can be decomposed into two Gaussian $Z_1 \sim N(0, \sigma_1^2 I)$ and $Z_2 \sim N(0, \sigma_2^2 I)$ which occurs with prob $p$ and $1-p$, then the expected norm is $(p \sigma_1 + (1-p) \sigma_2) \sqrt{d} \sqrt{2/\pi}$. 
% But if we apply Jensen's inequality, then we get $\sqrt{d (p \sigma_1^2 + (1-p) \sigma_2^2)}$. 
% When $\sigma_1 \ll \sigma_2$, then we see that (via numerical plug-ins) that the MAE is much better than Jensen + MSE.
% }






% \subsection{Optimal Asymptotic Covariance}
\subsection{Discussion}
To understand the significance of the Wasserstein-1 distance error bounds achieved by TSA-PR, it is useful to compare with an oracle algorithm that has access to the system parameters $A$.
Such an algorithm has been considered as a baseline in prior work \citep{polyakJuditsky,konda2004convergence,mokkadem2006convergence}.



Consider a modification of TSA in \eqref{eq:ttsa_general}, where the algorithm is allowed to design two gain matrices $Q_{ff}, Q_{ss}$ and estimate the solution $(x^*, y^*)$ with the sequence $\{(x_t^\dagger, y_t^\dagger)\}$ updated as
\begin{equation}
    \begin{split}
        x_{t+1}^{\dagger} &= x_t^{\dagger} -  \alpha_t Q_{ff} \left(A_{ff} x_t^\dagger + A_{fs} y_t^\dagger - W_t\right) \\
        y_{t+1}^{\dagger} &= y_t^{\dagger} -  \gamma_t Q_{ss} \left(A_{sf} x_t^\dagger + A_{ss} y_t^\dagger - V_t \right) .  
    \end{split}
    \label{eq:optimal_update}        
\end{equation}
A constraint is imposed on $Q_{ff}$ and $Q_{ss}$ so that the modified system with the gain matrix is asymptotically stable. 
It was shown in \citep{mokkadem2006convergence} that if the step sizes are chosen as $\alpha_t = \mathcal{O}(t^{-a})$ and $\gamma_t = \mathcal{O}(t^{-b})$ with $a \in (1/2, 1)$, the slow-time-scale $\{y_t^\dagger\}_{t=1}^n$ converges in distribution to the solution $y^*$:
\begin{align*}
    \sqrt{n} (y_n^\dagger - y^*) \to \mathcal{N}(0, \Sigma_{ss}(Q)) ,
\end{align*}
where $\Sigma_{ss} (Q_{ss})$ is the solution to the Lyapunov equation
\begin{equation}\label{eq:slow_optimal_lyapunov}
    \begin{split}
        \left(Q_{ss} \Delta - \frac{1}{2} I \right) \Sigma_{ss} (Q_{ss}) + \Sigma_{ss} (Q_{ss}) \left(\Delta^T Q_{ss}^T - \frac{1}{2}I\right) 
    \\
        = Q_{ss} \left(\Gamma_{ss} + A_{sf} A_{ff}^{-1} \Gamma_{ff} (A_{sf} A_{ff}^{-1})^T - (A_{sf} A_{ff}^{-1} \Gamma_{fs} + (A_{sf} A_{ff}^{-1} \Gamma_{fs})^T)\right) Q_{ss}^T .
    \end{split}
\end{equation}
A similar result holds for $x_t^\dagger$ when it is updated as the slow-time-scale variable (reversing the step size decay rates), where the other Schur complement $G$ appears in place of $\Delta$ in the above equation.
The minimum mean square error and the corresponding asymptotic covariance matrix is obtained by solving
\begin{align*}
    \min_{Q_{ff}} \mathrm{Tr} \Sigma_{ff} (Q_{ff}) \eqcolon \mathrm{Tr} \Sigma_{ff}^*, 
    \quad 
    \min_{Q_{ss}} \mathrm{Tr} \Sigma_{ss}(Q_{ss}) \eqcolon \mathrm{Tr} \Sigma_{ss}^* ,
\end{align*}
which can be shown to satisfy
\begin{equation}\label{eq:optimal_covariance}
    \begin{split}
        G \Sigma_{ff}^* G^T 
        &= \Gamma_{ff} + A_{fs} A_{ss}^{-1} \Gamma_{ss} (A_{fs} A_{ss}^{-1})^T 
        - \left(\Gamma_{fs} (A_{fs} A_{ss}^{-1})^T + A_{fs} A_{ss}^{-1} \Gamma_{sf}\right)  \\
        \Delta \Sigma_{ss}^* \Delta^T 
        &= \Gamma_{ss} + A_{sf} A_{ff}^{-1} \Gamma_{ff} (A_{sf} A_{ff}^{-1})^T
        - \left(\Gamma_{sf} (A_{sf} A_{ff}^{-1})^T + A_{sf} A_{ff}^{-1} \Gamma_{fs}\right)    
    \end{split} .
\end{equation}
This matches the asymptotic covariance achieved by TSA-PR.
But in this case, the above covariances are not obtained simultaneously because one variable has to be updated at a faster time scale. 
This is in contrast to TSA-PR, which achieves the minimum asymptotic mean square errors for both time scales simultaneously and at the same rate. 
Next, we explain a gap in literature in the context of achievable rates.

\textbf{Improved Rates via TSA-PR:}
\citet{konda2004convergence} proved that TSA converges asymptotically in m.s., where it was shown that
\begin{equation}\label{eq:asymptotic_rates}
    \lim_{n \to \infty} \alpha_n^{-1} \mathbb{E} (x_n - x^*) (x_n - x^*)^T = \Sigma_{ff}, 
    \quad 
    \lim_{n \to \infty} \gamma_n^{-1} \mathbb{E} (y_n - y^*) (y_n - y^*)^T = \Sigma_{ss} ,
\end{equation}
with $\Sigma_{ff}, \Sigma_{ss}$ defined in Eq. \eqref{eq:covariances}. 
Finite-time counterparts to the above asymptotic convergence were established by \citet{dalal2018finite,kaledin2020finite,haque2023tightfinitetimebounds}, where it was shown that
\begin{equation}\label{eq:finite_literature}
    \mathbb{E}(x_n - x^*) (x_n - x^*)^T = \mathcal{O}\left(\alpha_n\right) \Sigma_{ff} , \quad 
    \mathbb{E}(y_n - y^*) (y_n - y^*)^T = \mathcal{O}\left(\gamma_n \right) \Sigma_{ss} .    
\end{equation}
The MSE rates, obtained by taking the trace of the above, demonstrate that the asymptotic rates in Eq. \eqref{eq:asymptotic_rates} are achievable.
Our upper bound on the norm of the difference in Theorem \ref{thm:mse} is stronger than Eq. \eqref{eq:finite_literature} in that it also proves a lower bound, namely that the mean square errors decay no faster than $\alpha_n$ and $\gamma_n$.
These bounds suggest that the expected error rate achieved by TSA is of order $\mathcal{O}(n^{-a/2})$ and $\mathcal{O}(n^{-b/2})$, where $a, b$ satisfy Assumption \ref{assumption:steps}. 


The scaling factor of $\sqrt{n}$ in $\bar{x}_n - x^*$ and $\bar{y}_n - y^*$ in Theorem \ref{thm:clt} suggests that TSA-PR enjoys faster rates of convergence than TSA. 
Moreover, its asymptotic covariance cannot be improved upon even when the model parameters are known, as seen for $(x_n^\dagger, y_n^\dagger)$. 
Therefore, we conclude that the $n^{-1/2}$ rate in Corollary \ref{cor:mae} cannot be improved upon without a trade-off in covariance or other modes of convergences. 



Next, we describe the tightness of the bound in Corollary \ref{cor:mae}. 
First, note that the result conveys both an upper and lower bound on $\mathbb{E}\lVert \bar{x}_n - x^* \rVert$ and $\mathbb{E}\lVert \bar{y}_n -y^* \rVert$, indicating that the guarantee is asymptotically tight. 
Knowing that the limit of $\sqrt{n} (\bar{x}_n - x^*, \bar{y}_n - y^*)$ is the Gaussian vector $\bar{\Sigma}^{1/2} Z$, the best possible bound on the second moment is of the form 
\begin{equation}
    \mathbb{E}\lVert \bar{x}_n - x^* \rVert^2 = \frac{1}{n} \mathrm{Tr} \bar{\Sigma}_{ff} + o(n^{-1}) ,
    \quad
    \mathbb{E}\lVert \bar{y}_n -y^* \rVert^2 = \frac{1}{n} \mathrm{Tr}\bar{\Sigma}_{ss} + o(n^{-1}) 
    .
\end{equation}
Without additional information, the expected error can be deduced using Jensen's inequality to obtain
\begin{align*}
    \mathbb{E}\lVert \bar{x}_n - x^* \rVert = \frac{1}{\sqrt{n}} \sqrt{\mathrm{Tr} \bar{\Sigma}_{ff}} + o(n^{-1/2}) ,
    \quad
    \mathbb{E}\lVert \bar{y}_n - y^* \rVert = \frac{1}{\sqrt{n}} \sqrt{\mathrm{Tr} \bar{\Sigma}_{ss}} + o(n^{-1/2}) .
\end{align*}
But Corollary \ref{cor:mae} establishes $\mathbb{E}\lVert \bar{x}_n - x^*\rVert = \Theta (n^{-1/2} \mathbb{E}\lVert \bar{\Sigma}_{ff} Z_1 \rVert)$ and a similar improvement for $\mathbb{E}\lVert \bar{y}_n - y^*\rVert$.
Therefore, it is interesting to see that the use of the Wassertein-1 distance in our CLT captures the expected error precisely up to exact constants. 
