\section{Related Work}
\label{sec:literature}
In this section, we highlight the differences between our results and prior work.
References in the following discussion may assume a model with conditions that differ from ours (Assumptions \ref{assumption:first}--\ref{assumption:last}), but nonetheless we refer to their results when they are comparable. 
The norms $(\lVert x_n - x^* \rVert, \lVert y_n - y^* \rVert)$ and $(\lVert \bar{x}_n - x^* \rVert, \lVert \bar{y}_n - y^*\rVert)$ are referred to as the errors achieved by TSA and TSA-PR, respectively. 


\subsubsection{Polyak-Ruppert Averaging in Two-Time-Scale Algorithms}\label{sec:PR_TTSA}
Understanding the optimal rates that can be achieved by a class of algorithms is important for algorithm design. 
The study of optimal rates in stochastic approximation has been a central focus since \citep{RobbinsMonro}. 
In the context of single-time-scale algorithms, the question of whether the optimal rate could be achieved was resolved by \citet{polyakJuditsky}, using the PR averaging scheme as proposed by \citet{ruppert,polyak}.
Moreover, it was shown that the minimum asymptotic covariance is obtained. 
The PR averaging scheme was extended to two-time-scale algorithms by \citet{mokkadem2006convergence}, where the authors established asymptotic normality of TSA-PR.



All the above results are asymptotic. 
The widespread application of stochastic approximation algorithms has spurred interest in understanding finite-time bounds. 
For single-time-scale algorithms, finite-time bounds on the averaged iterates are known when the step size is constant \citep{srikant19,mou2020linearstochasticapproximationfinegrained,durmus2024finite}.
These bounds involve correction terms because constant step sizes are not asymptotically optimal. 
Finite-time bounds for decaying step sizes was established in \citep{srikant19,chen2022finite,srikant2024CLT}.
Less is known for the more general case of two-time-scale algorithms. 
Existing finite-time bounds are known only for the TSA without the averaging in Eq. \eqref{eq:ttsa_general}; see \citep{dalal2018finite,kaledin2020finite,haque2023tightfinitetimebounds}. 
Here, we establish the first finite-time bound for the averaged iterates (TSA-PR) generated by two-time-scale algorithms with decreasing step sizes. 
The rates shown to be achieved by TSA-PR significantly improves that achieved by TSA, akin to the single-time-scale case. 




\subsubsection{Limit Theorems and Quantitative Bounds}
Central limit theorems have been established for both SA and TSA algorithms \citep{polyakJuditsky,mokkadem2006convergence,hu2024central}.
But these results are asymptotic, and therefore cannot be applied to rigorously test the significance of repeated trials that halt after a finite number of iterations. 
Non-asymptotic CLTs were established in \citep{anastasiou2019normal,samsonov2024gaussian,srikant2024CLT}, which capture the normality behavior of single-time-scale algorithms.
In this paper (Theorem~\ref{thm:clt}), we establish a finite-time bound on the Wasserstein-1 metric for TSA-PR using the martingale CLT in \citep{srikant2024CLT}. 
The reason for considering the Wasserstein-1 distance as in \citep{srikant2024CLT} is that weaker notions of distance considered in \citep{anastasiou2019normal,samsonov2024gaussian} are not strong enough to deduce explicit bounds such as the expected error.   
In Corollary \ref{cor:mae}, we show that convergence in the Wasserstein-1 distance can be used to establish both a lower and upper bound on the expected error, capturing its correct magnitude up to exact constants.