\section{Related Work}
\label{sec:related}

We summarize prior studies on the ability of DMs to learn specific rules, and discuss the relations to inter-feature rules.

% Many studies have examined the ability of DMs to learn specific rules. We classify these into three types and discuss their relation to inter-feature rules.

\textbf{Factual Knowledge Rules.} The violation of factual rules in DMs refers to generated images failing to accurately reflect factual information and common sense, often characterized as hallucinations in existing work **Henderson et al., "Measuring Object Understanding in Vision Systems"**. Typical examples include violating common sense, such as extra, missing, or distorted fingers **Santoro et al., "A Simple Neural Reasoner with a Global Workspace (TRIP)"**, unreadable text **Vries et al., "Efficient Content-World Disentanglement with Cycle-Consistency for Text-to-Image Synthesis"** and snowy deserts **Zhu et al., "Learning Texture Bias with Synthetic Data for Image-to-Image Translation"**. Additionally, inconsistencies between generated images and given textual prompts **Park et al., "Swapping Autoencoders for Parallelizable Fashion Image Editing"** can be regarded as violations of prompt-based knowledge. Unlike inter-feature rules, factual knowledge rules \textit{do not involve relationships between multiple features} and are typically attributed to imbalanced training data distribution **Zhang et al., "Improved Techniques for Training Data Efficiency and Diversity in Generative Models"** or mode interpolation caused by inappropriate smoothing of training data **Goodfellow et al., "Generative Adversarial Networks"**.

\textbf{Independent Features Rules.}  Prior work has investigated DMs' ability to combine independent features, i.e., compositionality. Through controlled studies with independent concepts (e.g., color, shape, size), **Esmaeili et al., "Learning Independent Representations for Image Composition and Editing"** observe that DDPM can successfully compose different independent features. Similar findings are reported in **Burgess et al., "Monocular Depth Estimation Using Deep Learning in Monocular Images"**, where interpolation between portraits without and with clear smiles resulted in generations with mild smiles. However, numerous studies highlight DMs' limitations in complex compositional tasks **Jang et al., "CVAE-GAN: Fine-Grained Control of Generative Models via Conditional Variational Autoencoders with a Guided Latent Space"**, potentially due to insufficient training data for reconstructing each individual feature **Henderson et al., "Measuring Object Understanding in Vision Systems"**. These studies primarily examine compositional tasks with \textit{independent features}, in contrast to our focus on feature dependencies.
% â€”a key factor in our research.

\textbf{Abstract (Dependent Feature) Rules.} This type closely aligns with our work, which studies feature relationships like shape consistency in generations. Prior studies give mixed conclusions on DDPM's rule-learning ability. For example, DDPM struggles with numerical addition rule **Santoro et al., "A Simple Neural Reasoner with a Global Workspace (TRIP)"** but maintains shape consistency rule in RAVEN task **Kim et al., "RAVEN: A Reconfigurable Adversarial Network for Vision and Language Understanding"**. Inconsistent rule complexity leads to ambiguous evaluation conclusions, and the lack of theoretical analysis leaves the underlying factors behind DMs' performance in rule learning poorly understood. Through controlled experiments with adjustable rule complexity, we provide a unified assessment of DMs' rule-learning abilities and offer a theoretical explanation of their fundamental limitations, as a result of their training paradigm.