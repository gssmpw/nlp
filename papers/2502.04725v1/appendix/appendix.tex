\appendix

 \onecolumn
% \begin{center}
%     \Large{\textbf{Appendix}}
% \end{center}

% \etocdepthtag.toc{mtappendix}
% \etocsettagdepth{mtchapter}{none}
% \etocsettagdepth{mtappendix}{subsection}
% % \tableofcontents
% \newpage


\section{Low FID and Worse Inter-Feature Learning: A Gaussian Mixture Case}
\label{app:Low FID and Worse Inter-Feature Leaning: A Gaussian Mixture Case}

In this section, we provide a toy example based on the Gaussian Mixture Distribution to explain how low FID and incorrect inter-feature relationships can coexist. This supports the point that even though DMs may perform excellently on classical metrics such as FID, this does not necessarily mean they can perfectly learn the hidden inter-feature rules.

Consider a 2-dimensional population, i.e., the true distribution \( p(x, y) \), which is a Gaussian Mixture Model (GMM) with two components as:
\begin{align}
\label{eq:estimated_dist}
    p({x}, {y}) = \mathcal{F}(\boldsymbol{\mu}_p,\boldsymbol{\Sigma}_p)= \frac{1}{2} \cdot \mathcal{N}\left(\begin{bmatrix} 1 \\ 1 \end{bmatrix}, \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}\right) + \frac{1}{2} \cdot \mathcal{N}\left(\begin{bmatrix} -1 \\ -1 \end{bmatrix}, \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}\right).
\end{align}
where we can have 
\begin{align*}
    \boldsymbol{\mu}_p &= \frac{1}{2} \cdot \begin{bmatrix} 1 \\ 1 \end{bmatrix} + \frac{1}{2} \cdot \begin{bmatrix} -1 \\ -1 \end{bmatrix} =\begin{bmatrix} 0 \\ 0 \end{bmatrix}
\end{align*}
and the covaraince matirx as
\begin{align*}
    \boldsymbol{\Sigma}_p &= \sum_{i=1}^2 w_i \left( \boldsymbol{\Sigma}_i + (\boldsymbol{\mu}_i - \boldsymbol{\mu}_q)(\boldsymbol{\mu}_i - \boldsymbol{\mu}_q)^\top \right) \\
    &= 0.5 \cdot \left( \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} + \begin{bmatrix} 1 & 1 \\ 1 & 1 \end{bmatrix} \right) + 0.5 \cdot \left( \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} + \begin{bmatrix} 1 & 1 \\ 1 & 1 \end{bmatrix} \right) = \begin{bmatrix} 2 & 1 \\ 1 & 2 \end{bmatrix}.
\end{align*}
We assume the estimated data distribution learned by DMs is a joint Gaussian distribution:
\begin{align}
\label{eq:true_dist}
    q(\hat{x}, \hat{y}) =\mathcal{N}(\boldsymbol{\mu}_q,\boldsymbol{\Sigma}_q) =\mathcal{N}\left(\begin{bmatrix} 0 \\ 0 \end{bmatrix}, \begin{bmatrix} 2 & 1 \\ 1 & 2 \end{bmatrix}\right).
\end{align}
With means and covariance matrices of true distributon $p$ and estimated distribution $q$ are identical, that is \(\boldsymbol{\mu}_p = \boldsymbol{\mu}_q = [0, 0]^\top\) and \(\boldsymbol{\Sigma}_p = \boldsymbol{\Sigma}_q = \begin{bmatrix} 2 & 1 \\ 1 & 2 \end{bmatrix}\), we easily have the FID between \( p(x, y) \) and \( p(\hat{x}, \hat{y}) \) is computed as:
\begin{align}
    \text{FID} &= \left\| \boldsymbol{\mu}_p - \boldsymbol{\mu}_q \right\|_2^2 + \text{Tr}\left( \boldsymbol{\Sigma}_p + \boldsymbol{\Sigma}_q - 2 \left( \boldsymbol{\Sigma}_p \boldsymbol{\Sigma}_q \right)^{1/2} \right)\notag\\
    &=\left\| 0 - 0 \right\|_2^2 + \text{Tr}\left( \begin{bmatrix} 2 & 1 \\ 1 & 2 \end{bmatrix} + \begin{bmatrix} 2 & 1 \\ 1 & 2 \end{bmatrix} - 2 \begin{bmatrix} 2 & 1 \\ 1 & 2 \end{bmatrix} \right)\notag\\
    &= 0
\end{align}
Although the FID is small (i.e., 0), the inter-feature relationships between $x$ and $y$ in true and estimated distribution are fundamentally different. In the true distribution, $x$ and $y$ are independent within each Gaussian component but exhibit dependence in the overall distribution due to the mixture of components. In the estimated distribution $q(\hat{x}, \hat{y})$, $\hat{x}$ and $\hat{y}$ are dependent with $\text{Cov}(x, y) = 1$. Therefore, low FID does not imply a correct recovery of the inter-feature rules.
\begin{figure}
\setlength{\abovecaptionskip}{-1cm}
  \centering
  \includegraphics[width=1.\textwidth, height=0.3\textheight]{figures/syth_more_compressed.pdf}
% width=1.0\textwidth, 
\vspace*{-8mm}
  \caption{\textbf{Evaluating More Mainstream DMs on Real-World Inter-Feature Rules.} We evaluate more mainstream DMs on scenarios with inter-feature rules, with $5$ random generations and manual selection of unreasonable samples. Despite their success in metrics like FID, none of these DMs achieve complete correctness in cases involving inter-feature relationships. }
  \label{fig:syth_more}
  % \vspace{-0.2cm}
\end{figure}

\begin{table}[]
	\centering
	\caption{\textbf{Real-World Inter-Feature Rules.} For each scenario containing inter-feature rules, \cref{tab:prompt} provides detailed prompts and annotates the existing inter-feature relationships. By comparing the genuine inter-feature relationships with those in generated images, we can evaluate DMs' ability to learn inter-feature relationships.}
 \label{tab:prompt}
 \vspace{0pt} 
	\begin{tabular}{m{0.95\linewidth}}
		\toprule
		\textbf{[Spatial Rule] (a) Light shadow:}\\
		Prompt 1: {A desert scene with a majestic palace under a bright sun.} \\
        Inter-Feature Rule 1: \texttt{Sun position affects palace's shadow direction.}\\
		Prompt 2: \texttt{The moonlight casts a clear shadow of a tall tree onto the ground.} \\
        Inter-Feature Rule 2: {Moon position affects tree's shadow direction}\\
		\midrule
		\textbf{[Spatial Rule] (b) Reflection/Refraction}\\
		Prompt 1: \texttt{A plush lion toy in front of the mirror. Its front side is facing the camera. There is its reflection in the mirror.} \\
        Inter-Feature Rule 1: {The lion toy's orientation relative to the mirror determines its reflection's orientation.}\\
        Prompt 2: \texttt{A transparent glass bottle partially submerged in a calm, clear pool of water. The upper half of the bottle extends above the water's surface and the lower half of the bottle is submerged.} \\
        Inter-Feature Rule 2: {The water surface's position dictates the bottle's shape distortion.}\\
		\midrule
		\textbf{[Spatial Rule] (c) Semantics}\\
		Prompt 1: \texttt{A field of sunflowers under a clear blue sky with the sun shining brightly above.} \\
        Inter-Feature Rule 1: {Sun direction dictates sunflower orientation.}\\
        Prompt 2: \texttt{A paintbrush fully loaded with paint, making a stroke on a blank white canvas.} \\
        Inter-Feature Rule 2: {Brush tip color matches canvas paint.}\\
  \midrule
  \textbf{[Non-Spatial Rule] (d) Size -Texture} \\
  	Prompt 1: \texttt{The cross-section of a sturdy tree, covered with annual rings.} \\
        Inter-Feature Rule 1: {The diameter of a tree is related to its growth rings.}\\
        Prompt 2: \texttt{A nautilus fossil, showing its intricate spiral shell structure with visible growth chambers.} \\
        Inter-Feature Rule 2: {Nautilus fossil size correlates with spiral patterns.}\\
    \midrule
    \textbf{[Non-Spatial Rule] (e) Size/Region- Color} \\
  	Prompt 1: \texttt{An artistic representation showing the expanded star phase and cooling star  phase of the same star.} \\
        Inter-Feature Rule 1: {Celestial body size and color should align, exemplified by red giants and white dwarfs.}\\
        Prompt 2: \texttt{A burning red candle in a dark with the flame, which is vibrant, dynamic, and glowing intensely against the darkness.} \\
        Inter-Feature Rule 2: {Candle flame color varies with distance from the wick.}\\
    \midrule
  \textbf{[Non-Spatial Rule] } (f) Color - Color\\
 	Prompt 1: \texttt{Two Eclectus parrots, showcasing the striking sexual dimorphism of the species.} \\
        Inter-Feature Rule 1: {Eclectus parrots' body and beak colors match—green and yellow for males, red and black for females.}\\
        Prompt 2: \texttt{A male Poecilia reticulata and a female Poecilia reticulata are swimming gracefully in a clear, freshwater aquarium, showcasing the striking sexual dimorphism of the species} \\
        Inter-Feature Rule 2: {Guppies' body and tail colors match—males are equally colorful in both.}\\
		\bottomrule
	\end{tabular}%
\end{table}%
\section{Details and More Example on Real-Wold Hidden Inter-Feature Rules}
\label{app:Details and More Example on Real-Wold Hidden Inter-Feature Rules}
\cref{tab:prompt} provides a detailed description of the prompts for each case in \cref{fig:real_synthetic} and \cref{fig:syth_more}, including scenarios with inter-feature rules and the corresponding rules themselves. We also consider more DMs such as \texttt{SDXL}\footnote{https://fal.ai/models/fal-ai/fast-lightning-sdxl} \cite{podell2023sdxl}, \texttt{Flux.1.1 Ultra}\footnote{https://fal.ai/models/fal-ai/flux-pro/v1.1-ultra} \cite{flux2023}, \texttt{DALL$\cdot$E 3}\footnote{https://chatgpt.com/g/g-iLoR8U3iA-dall-e3}  \cite{betker2023improving}, and VAR-based \cite{VAR} text-to-image model \texttt{Infinity}\footnote{https://github.com/FoundationVision/Infinity?tab=readme-ov-file} \cite{Infinity} in the evaluation.
By comparing these rules, we observe that most mainstream DMs fail in some or all scenarios. For instance, in the \textit{Reflection/Refraction} scenario, none of the DMs successfully generate plausible images: the reflected toy in the mirror faces the camera just like the real one, and the submerged bottle shows no refraction. Our evaluation covers both classic latent diffusion models (e.g., \texttt{SD-3.5 Large}) and the latest next-scale prediction-based diffusion models (e.g., \texttt{Infinity}). Surprisingly, none of them can perfectly handle these inter-feature relationships, highlighting the widespread limitation of DMs in this regard.
% \subsection{Detailed Evaluation Methods for Four Synthetic Tasks}
% \label{app:Detailed Evaluation Methods for Four Synthetic Tasks}
\section{Details and More Example on Synthetic Tasks}
\label{app:Details and More Example on Synthetic Tasks}
This section presents supplementary details and examples regarding our synthetic datasets.

\textbf{Task A} generates synthetic images featuring a simple outdoor scene composed of a vertical pole, a sun, and their corresponding shadow. The height of the pole is randomly selected within the range of \([6.4, 12.8]\) pixels, which corresponds to \([20\%, 40\%]\) of the total image size \((32 \times 32\) pixels). The sun's horizontal position is sampled from two predefined distance intervals: far distances \((0-6\) pixels or \(26-32\) pixels) and near distances \((10-16\) pixels or \(16-22\) pixels), ensuring a varied distribution of sun locations. The shadow length is computed using the formula:
\begin{align}
    \text{shadow\_length} = \frac{\text{pole\_height} \times |\text{sun\_distance}|}{\text{sun\_height} - \text{pole\_height}}
\end{align}

where the sun height is determined as twice the pole height, clipped within \([9.6, 25.6]\) pixels (30\%-80\% of the image size). Colors for the sun, pole, and shadow are randomly selected from predefined HSV (Hue-Saturation-Value) ranges: Sun color (yellowish tones) has a hue in \([0, 30]\), saturation in \([100, 255]\), and value in \([200, 255]\). Pole color (blue-green tones) has a hue in \([90, 150]\), saturation in \([100, 255]\), and value in \([100, 255]\). Shadow color (dark tones like black, brown, gray) has a hue in \([0, 180]\), saturation in \([0, 50]\), and value in \([50, 150]\). 


\textbf{Task B}  generates synthetic images containing two rectangular objects placed within a \(32 \times 32\) pixel space. The first rectangle's position and size are determined as follows: its leftmost position \( l_1 \) is chosen randomly from the range \([0, 9.6]\) pixels ($30\%$ of the image width), and its height \( l_2 \) is chosen randomly from \([6.4, 19.2]\) pixels ($20\%$ to 60\% of the image height). The color of the first rectangle is randomly selected from a yellowish hue range with hue \([0, 30]\), saturation \([100, 255]\), and value \([200, 255]\) in HSV space. The second rectangle's position is determined by \( h_1 \), which is chosen randomly within a range dependent on \( l_1 \). Specifically, \( h_1 \) is sampled from the range \([l_1 + 6.4, 25.6]\) pixels (ensuring \( h_1 > l_1 \)). The height of the second rectangle \( h_2 \) is calculated based on the first rectangle's height \( l_2 \), ensuring the relation \( l_1{h_1} = {h_2}{l_2} \). The color of the second rectangle is chosen randomly from a blue-green hue range with hue \([90, 150]\), saturation \([100, 255]\), and value \([100, 255]\) in HSV space.

\textbf{Task C}  generates images containing two circles: one large and one small. The large circle's diameter is randomly chosen between $10\%$ and $30\%$ of the image size, and the small circle's diameter is determined to be \(\sqrt{2}\) times the diameter of the large circle. The colors of the circles are randomly selected from predefined color ranges in the HSV color space. Specifically, the large circle is assigned a color from the blue-green hue range, with hue values between $90$ and $150$, saturation between $100$ and $255$, and brightness between $100$ and $255$. The small circle is assigned a color from the yellowish hue range, with hue values between $0$ and $30$, saturation between $100$ and $255$, and brightness between $200$ and $255$. The circles are randomly positioned such that they are adjacent to each other—either on the left, right, top, or bottom of the large circle. 
\begin{figure}[]
  \centering  \includegraphics[width=0.95\textwidth, height=0.43\textheight]{figures/case_vis_train.pdf}
  \vspace*{-5mm}
  \caption{\textbf{Synthetic Data Examples.} We present synthetic samples in four synthetic tasks, with annotations of features of interest and Ratio calculations. The target Ratios for Tasks A, B, C, and D are $1$, $1$, $\sqrt{2}$, and $1.5$, respectively.}
  \label{fig:case_vis_train}
\end{figure}

\textbf{Task D} generates images containing two squares: one smaller and one larger. The small square's size is randomly chosen to be between 30\% and 70\% of the top half of the image's size. The larger square's size is then set to be $1.5$ times the size of the small square. The color of the small square is randomly selected from a yellowish hue range, with hue values between $0$ and $30$, saturation between $100$ and $255$, and brightness between $200$ and $255$. The color of the large square is randomly chosen from a blue-green hue range, with hue values between $90$ and $150$, saturation between $100$ and $255$, and brightness between $100$ and $255$. The position of the squares is determined within specific regions of the image. The top half and the bottom half of the image are divided into distinct regions, with the small square being placed in the top half and the large square in the bottom half. The exact position of each square is randomly chosen within its respective region, while ensuring that the squares do not exceed the image's boundaries. Both squares are positioned such that they do not overlap with each other and remain entirely within the image frame.


\section{More Synthetic Tasks Setup and Results}
\subsection{More Details of Experimental Setup}
\label{app:Details of DMs' Training}
We use the U-Net architecture as the denoising network, consisting of several down-sampling and up-sampling blocks, each with two convolutional layers followed by ReLU activation. Each down-sampling block incorporates a Self-Attention mechanism and skip connections to preserve fine details. Pooling layers are used to reduce spatial dimensions and capture abstract features. A final $1 \times 1$ convolution layer produces the denoised output image. We use AdamW \cite{loshchilov2017decoupled} as the optimizer with a learning rate of $3e-4$. The noisy steps are set to $T = 1000$, with a linear noise schedule ranging from $1e-4$ to $2e-2$. For Tasks A, B, C, and D, the sample sizes are $4000$, $2000$, $2000$, and $2000$, respectively, and the input data size is $(3, 32, 32)$. The training is performed on a single NVIDIA A800 GPU for $400$, $800$, $1600$, and $1000$ epochs, respectively.
\subsection{More Results of Synthetic Tasks}
\label{app:More Results of Synthetic Tasks}
This section provides additional details to complement the experimental results in \cref{sec:results}. Notably, to ensure more accurate quality assessment of generated images, we upscale the $32 \times 32$ images to $128 \times 128$ during evaluation. This allows the training data to precisely exhibit the expected rule patterns, thereby enabling more reliable evaluation of the generated samples.
\paragraph{Generations that Violate Coarse-Grained Rules.}\cref{tab:coarse-grained rule} illustrates the DDPM's ability to learn coarse-grained rules. We observe that in all four synthetic tasks, the number of samples violating the coarse-grained rules is almost zero, except for Task A, where one generated sample, shown in \cref{fig:example_sundow}, has the sun and shadow on the same side of the pole.
\begin{wrapfigure}{r}{0.25\textwidth}
% \vspace{-.2in}
\begin{center}
    \includegraphics[width=0.23\textwidth]{figures/sunshadowweight_1108.pdf}
\end{center}
\vspace{-0.2in}
\caption{For Task A, while all training samples have the sun and shadow on opposite sides, DDPM generates one sample violating this coarse-grained rule where the sun and shadow appear on the same side.} 
\vspace{-0.5in}
\label{fig:example_sundow} 
\end{wrapfigure}
% \cref{fig:s_d} then visualizes the representations of samples that do not violate the coarse rules (i.e.,sun and shadow on opposite sides) and those that do violate the coarse rules (i.e.,sun and shadow on the same side) after being processed by CLIP and reduced to lower dimensions using UMAP. We observe that their distributions are distant from each other. This may help explain why it is challenging for DMs to violate the coarse rules, as it essentially becomes an extrapolation problem. DMs struggle to generate sufficiently distant out-of-distribution samples from the limited in-distribution training data, making it difficult to violate the coarse rules.
\begin{figure}[]
  \centering  \includegraphics[width=0.95\textwidth, height=0.43\textheight]{figures/case_vis.pdf}
  \vspace*{-5mm}
  \caption{\textbf{Examples of Rule-violating Generations.} We present samples generated by DDPM that violate fine-grained rules in four synthetic tasks, with annotations of features of interest and Ratio calculations. The target Ratios for Tasks A, B, C, and D are $1$, $1$, $\sqrt{2}$, and $1.5$, respectively.}
  \label{fig:case_vis}
\end{figure}
\paragraph{Generations that Violate Fine-Grained Rules.}We then proceed to show the samples generated by DDPMs that do not satisfy the fine rules in \cref{fig:case_vis}, and highlight the features of interest using the evaluation method developed in \cref{sec:setup}.
\paragraph{Generations that Satisfy Fine-Grained Rules.}Here, we use two coordinate systems: a 4D representation capturing key features $(l_1,l_2,h_1,h_2)$ and a 13D representation that additionally encodes the RGB colors of the sun, pole, and shadow. This dual-coordinate analysis allows us to distinguish whether differences between generated and training samples arise from structural variations or merely from different color combinations within similar structures \cite{okawa2024compositional}. We then compute the Euclidean distances between each generated sample and its nearest neighbor in both 4D and 13D spaces. As a supplement to the DDPM memory experiment in \cref{sec:results}, \cref{fig:taska_nearest_neighbors} presents the three nearest neighbors in the training data for high-quality generated samples (with ratios in $[0.99, 1.01]$) in both 4-dimensional and 13-dimensional coordinates. We observe that the 4-dimensional coordinates effectively capture the spatial structure of the nearest neighbors in the training data, while the 13-dimensional coordinates provide a more comprehensive understanding of the similarity of the generated samples, accounting for both color and structure.
\begin{figure}[]
  \centering  \includegraphics[width=0.7\textwidth, height=0.9\textheight]{figures/taska_nearest_neighbors.pdf}
  \vspace*{-3mm}
  \caption{\textbf{Generations that Violate Fine-Grained Rules.} Taking Task A as an example, we show $10$ high-quality generated samples and their Top-$3$ nearest neighbors from the training data. The first column visualizes the generated samples, while columns $2$-$4$ display the Top-$3$ nearest neighbors from training data in 4D coordinates, where similarity mainly reflects spatial structure. Columns $5$-$7$ show the top-3 nearest neighbors in 13D coordinates, where similarity primarily reflects object colors.}
\label{fig:taska_nearest_neighbors}
\end{figure}
\subsection{More Setting of Synthetic Tasks}
\label{app:More Setting of Synthetic Tasks}
In this section, we consider additional factors, such as more powerful model architectures and larger training datasets, to evaluate the diffusion model's ability to learn precise rules in Task A. Furthermore, detailed experimental results not included in \cref{sec:results}, such as samples that violate coarse rules, will be presented in this section.
\paragraph{More Training Epochs.}Taking Task A as an example, \cref{fig:taska_rule_all} shows the impact of more training epochs on learning fine-grained rules. We observe that as the number of training epochs increases, the DDPM's ability to learn fine-grained rules improves significantly from $200$ to $400$ epochs, with $R^2$ increasing from $0.19$ to $0.85$. This indicates that the relationship between $l_1h_2$ and $l_2h_1$ is better described by the linear model. However, even as the training continues up to $4000$ epochs, there is no noticeable improvement in the model’s ability to learn the fine-grained rules, as reflected by the slight changes in the fitted line coefficients and $R^2$ remaining around $0.85$.
\begin{figure}[]
  \centering  \includegraphics[width=0.9\textwidth, height=0.43\textheight]{figures/taska_rule_all.pdf}
  \vspace*{-5mm}
  \caption{\textbf{The learning capability of DDPM for fine-grained rules across training epochs.} We observe that as epochs increase from $200$ to $4000$, DDPM's ability to learn fine-grained rules shows no significant improvement, as evidenced by the stable Estimation line and $R^2$. This suggests that increasing training iterations does not alleviate DMs' difficulty in learning fine-grained rules.  The visualized generated samples fall within the interval $[2.5\%, 97.5\%]$.}
  \label{fig:taska_rule_all}
\end{figure}
\paragraph{More Model Architectures.}
Then, we consider the factor modle architectures and use more powerful backbones, DiT \cite{peebles2023scalable} and SiT \cite{ma2024sit}, to replace U-Net as the denoising network. Specifically, we consider two sizes of DiT and SiT: DiT Small with 33M parameters and patch size (DiT-S/2), DiT Base with 130M parameters and patch size (DiT-B/2), SiT Small with 33M parameters and patch size (SiT-S/2), and SiT Base with 130M parameters and patch size (SiT-B/2). Keeping the number of training epochs, noise time steps, and other hyperparameters consistent, we find that, compared to the 14M parameter U-Net, the parameter count of SiT and DiT has increased by $2$ to $10$ times. However, as revealed in \cref{fig:more_archi}, although all models follow coarse rules, the deficiency in DDPM's ability to learn fine-grained rules does not significantly improve with the increase in parameter count, and there is even a slight decrease in performance with DiT-S/2.
\begin{figure*}[]
\centering
    \hfill
    \subfigure[33M, DiT-S/2]{\includegraphics[width=0.24\textwidth]{figures/taska_dits2.pdf}}
    \hfill
    \subfigure[130M, DiT-B/2]{\includegraphics[width=0.24\textwidth]{figures/taska_ditb2.pdf}}
    \hfill
    \subfigure[33M, SiT-S/2]{\includegraphics[width=0.24\textwidth]{figures/taska_sits2.pdf}}
    \hfill
    \subfigure[130M, DiT-B/2]{\includegraphics[width=0.24\textwidth]{figures/taska_sitb2.pdf}}
    \hfill
\vspace{-0.1in}
\caption{\textbf{DDPM's capability in learning fine-grained rules with more powerful backbones.} Even with larger and more advanced denoising networks, DDPM still cannot avoid generating samples that violate fine-grained rules. This indicates that DDPM's inability to learn fine-grained rules is decoupled from model architecture. The visualized generated samples fall within the  interval $[2.5\%, 97.5\%]$.}
\vspace{-0.15in}
\label{fig:more_archi}
\end{figure*}

\paragraph{More Training Data.}Next, we consider the impact of training data size. For Task A, we gradually increase the sample size from $4000$ to $20000$ to $40000$ and observe whether increasing the sample size improves the DMs' ability to learn rules. \cref{fig:training_size_20000} and \cref{fig:training_size_40000} show that the increase in sample size does not enable DMs to learn fine-grained rules better, as evidenced by the almost unchanged $R^2$ and the fitted linear model. Similarly, we do not observe DMs violating coarse rules with large samples.
\begin{figure}[t!]
\centering
    \hfill
    \subfigure[{Training Data Size $20000$}]{\label{fig:training_size_20000}\includegraphics[width=0.3\linewidth]{figures/tasha_20000.pdf}}
    \hfill
    \subfigure[\label{fig:training_size_40000}{Training Data Size $40000$}]{\includegraphics[width=0.3\linewidth]{figures/tasha_40000.pdf}}
    \hfill
     \subfigure[{Image Size $64 \times 64$}]{\label{fig:training_size_64t64}\includegraphics[width=0.3\linewidth]{figures/tasha_64_times_64.pdf}}
    \hfill
    %  \subfigure[{Image Size $64 \times 64$}]{\label{fig:test_acc}\includegraphics[width=0.23\linewidth]{figures/test_acc.pdf}}
    % \hfill
\vspace{-0.15in}
\caption{\textbf{DDPM's capability in learning fine-grained rules with increased training samples and larger image sizes.} We observe that increasing training samples and image sizes does not significantly improve DDPM's ability to learn fine-grained rules, as evidenced by the stable Estimation line and $R^2$. This suggests that neither expanding the training dataset nor increasing image resolution alleviates DMs' difficulty in learning fine-grained rules. The visualized generated samples fall within the  interval $[2.5\%, 97.5\%]$.}
\vspace{-0.15in}
\label{fig:training_datasize}
\end{figure}
\paragraph{More Image Size Choice.}Our final consideration is image size. In the main text, the images are only $32 \times 32$. Existing studies suggest that low-resolution images may lead to the loss of details in diffusion models' generation \cite{chen2024image,niu2024acdmsr,li2024scalability}. Therefore, we consider larger input resolutions of $(3,64,64)$, as shown in \cref{fig:training_size_64t64}. We observe almost no improvement in the DMs' ability to learn underlying rules with generated samples that do not violate coarse rules. Due to computational constraints, we were unable to explore even higher resolutions. But it is clear that for a relatively simple task like Task A, which does not contain rich semantics, DMs are unable to recover the underlying feature relationships even at the $64 \times 64$ resolution. This itself highlights the difficulty DMs face in learning hidden features.



\section{Proofs}


\begin{proof}[Proof of Theorem \ref{thm:score}]
Given the independence, we can write $p_t(\bx_t) = p_t(\bx_t^{(1)}, \bx_t^{(2)}) p_t(\bx_t^{(3)}, ..., \bx_t^{(P)})$. We derive $p_t(\bx_t^{(1)}, \bx_t^{(2)})$ as follows. First, we notice that $\bx_t^{(1)} | \zeta \sim \gN(\alpha_t \zeta \bu, \beta_t^2 \bI)$ and $\bx_t^{(2)} |  \zeta \sim \gN(\alpha_t  (1-\zeta) \bv, \beta_t^2 \bI)$. Then we obtain 
\begin{align*}
    p_t(\bx_t^{(1)}, \bx_t^{(2)}) = \sE_{\gD_\zeta} [ \gN( \bmu_t(\zeta), \beta_t^2 \bI_{2d} ) ]
\end{align*}
where we denote $\bmu_t(\zeta) = [\alpha_t  \zeta \bu^\top, \alpha_t  (1- \zeta) \bv^\top ]^\top$. 

Thus the score can be computed as $\nabla \log p_t(\bx_t) = [\nabla \log p_t(\bx_t^{(1)}, \bx_t^{(2)})^\top, \nabla \log p_t(\bx_t^{(3)}, ..., \bx_t^{(P)})^\top]^\top$ where $\nabla \log p_t(\bx_t^{(1)}, \bx_t^{(2)})^\top \in \sR^{2d}$ and can be derived as 
\begin{align*}
    \nabla \log p_t(\bx_t^{(1)}, \bx_t^{(2)}) = \frac{\nabla p_t(\bx_t^{(1)}, \bx_t^{(2)})}{p_t(\bx_t^{(1)}, \bx_t^{(2)})} &= \frac{\sE_{\gD_\zeta} [ \nabla \gN( \bx_t; \bmu_t(\zeta), \beta_t^2 \bI_{2d}) ]}{\sE_{\gD_\zeta} [ \gN( \bx_t;\bmu_t(\zeta), \beta_t^2 \bI_{2d}) ]} \\
    &= \frac{\sE_{\gD_\zeta} \big[ -\gN(\bx_t; \bmu_t(\zeta), \beta_t^2 \bI_{2d})  \beta_t^{-2} \big(  \bx_t - \bmu_t(\zeta) \big)  \big]}{\sE_{\gD_\zeta} [ \gN( \bx_t; \bmu_t(\zeta), \beta_t^2 \bI_{2d}) ]} \\
    &= - \beta_t^{-2} \bx_t + \beta_t^{-2} \sE_{\gD_\zeta} [  \pi_t(\zeta, \bx_t) \bmu_t(\zeta)  ] \\
    &= - \beta_t^{-2} \bx_t + \alpha_t \beta_t^{-2} \begin{bmatrix}
        \sE_{\gD_\zeta} [\pi_t(\zeta, \bx_t) \zeta  ] \bu \\
        \sE_{\gD_\zeta} [\pi_t(\zeta, \bx_t) (1-\zeta) ] \bv 
    \end{bmatrix} 
\end{align*}
where with a slight abuse of notation, we let $\bx_t = [\bx_t^{(1)\top}, \bx_t^{(2)\top}]^\top$ and denote
\begin{equation*}
    \pi_t(\zeta, \bx_t) = \frac{\gN(\bx_t; \bmu_t( \zeta), \bSigma_t)}{\sE_{D_\zeta} [\gN(\bx_t; \bmu_t( \zeta), \bSigma_t)]}.
\end{equation*}
\end{proof}



% \begin{align*}
%     L(\bW_t) = \frac{1}{2n} \sum_{i=1}^n\sum_{p=1}^P \sE_{\beps_{t,i}^{(p)}}  \Big\| s_{w}^{(p)}(\bx_t)  - \beps_{t,i}^{(p)} \Big\|^2 = \frac{1}{2n} \sum_{i=1}^n\sum_{p=1}^P \sE_{\beps_{t,i}^{(p)}} \Big\| \sigma( \langle \bw_t^{(p)}, \bx_{t,i}^{(p)} \rangle )  \bw_t^{(p)} - \frac{1}{\beta_t^2} \bx_{t,i}^{(p)} - \beps_{t,i}^{(p)} \Big\|^2
% \end{align*}
% where $\bx_{t,i}^{(p)} = \alpha_t \bx_{0,i}^{(p)} + \beta_t \beps_{t,i}^{(p)}$. 

% \highlight{
% \begin{condition}
% \label{cond:diff}
% We assume the following conditions hold.
% \begin{enumerate}[leftmargin=0.3in]
%     \item The dimension $d$ satisfies $d = \widetilde \Omega(1)$.

%     \item The initialization  $\sigma_0$ satisfies ${ \widetilde O(n \sigma_\xi^{-1} d^{-1}) \leq} \sigma_0 \leq \widetilde O( \min\{   \| \bmu \|^{-1}, { \| \bmu\|^{-2} n^{-1} \sigma_\xi d^{1/2}}, \sigma_\xi^{-1} d^{-1/2}, \\ { \sigma_\xi m^{-1/6} n^{-1/2}, n\sigma_\xi^{-1} d^{-3/4}  }\} )$. 

%     \item The noise coefficients $\alpha_t, \beta_t$ satisfy $\alpha_t, \beta_t = \Theta(1)$. 
% \end{enumerate}
% \end{condition}
% }




\begin{proof}[Proof of Theorem \ref{them:multi_poly}]
According to the decomposition of the rule-respecting error in terms of bias and variance, we have $\gE_{\rm mse} = \gE_{\rm bias}^2  + \gE_{\rm variance}$, where we compute 
\begin{align*}
    \gE_{\rm bias} &= \left| \sum_{r=1}^m \sE \Big[ \sigma \big( \langle \bw_r^{(1)}, \bx_t^{(1)} \rangle \big) \Big] \langle \bw_r^{(1)}, \bu \rangle + \sum_{r=1}^m \sE \Big[ \sigma \big( \langle \bw_{r,t}^{(2)} , \bx_t^{(2)} \rangle \big) \Big] \langle \bw_{r,t}^{(2)}, \bv\rangle - \frac{\alpha_t}{\beta_t^2}\right| \\
    \gE_{\rm variance} &= {\rm Var} \Big(  \sum_{r=1}^m \sigma( \langle \bw_{r,t}^{(1)}, \bx_t^{(1)} \rangle  )  \langle \bw_{r,t}^{(1)}, \bu \rangle + \sum_{r=1}^m \sigma( \langle \bw_{r,t}^{(2)}, \bx_t^{(2)} \rangle  )  \langle \bw_{r,t}^{(2)}, \bv \rangle  \Big) 
    % &\geq \sE_\zeta \Big[ {\rm Var}_{|\zeta} \Big(  \sum_{r=1}^m \sigma( \langle \bw_{r,t}^{(1)}, \bx_t^{(1)} \rangle  )  \langle \bw_{r,t}^{(1)}, \bu \rangle  \Big) + {\rm Var}_{|\zeta} \Big(  \sum_{r=1}^m \sigma( \langle \bw_{r,t}^{(2)}, \bx_t^{(2)} \rangle  )  \langle \bw_{r,t}^{(2)}, \bv \rangle  \Big)  \Big]
    % &= {\rm Var}\Big(\sum_{r=1}^m \sigma( \langle \bw_{r,t}^{(1)}, \bx_t^{(1)} \rangle  )  \langle \bw_{r,t}^{(1)}, \bu \rangle \Big) + {\rm Var} \Big(  \sum_{r=1}^m \sigma( \langle \bw_{r,t}^{(2)}, \bx_t^{(2)} \rangle  )  \langle \bw_{r,t}^{(2)}, \bv \rangle \Big) + 2 {\rm Cov} \\
    % &=\sE [  ( \psi_t(\bx_t) )^2]  -  \big(\sE [\psi_t(\bx_t)]\big)^2 \\
    % &= \sum_{r=1}^m \sE \Big[ \sigma( \langle \bw_{r,t}^{(1)}, \bx_t^{(1)} \rangle  )^2 \Big] \langle \bw_{r,t}^{(1)}, \bu \rangle^2 + 2\sum_{r=1}^m \sum_{r'\neq r} \sE\Big[ \sigma\big( \langle \bw_{r,t}^{(1)}, \bx_{t}^{(1)} \rangle \big) \sigma( \langle \bw_{r',t}^{(1)}, \bx_t^{(1)} \rangle ) \Big] \langle \bw_{r,t}^{(1)}, \bu \rangle \langle \bw_{r',t}^{(1)}, \bu  \rangle \\
    % &\quad + \sum_{r=1}^m \sE \Big[ \sigma( \langle \bw_{r,t}^{(2)}, \bx_t^{(2)} \rangle  )^2 \Big] \langle \bw_{r,t}^{(2)}, \bv \rangle^2 + 2 \sum_{r=1}^m \sum_{r'\neq r} \sE\Big[ \sigma\big( \langle \bw_{r,t}^{(2)}, \bx_{t}^{(2)} \rangle \big) \sigma( \langle \bw_{r',t}^{(2)}, \bx_t^{(2)} \rangle ) \Big] \langle \bw_{r,t}^{(2)}, \bv \rangle \langle \bw_{r',t}^{(2)}, \bv  \rangle \\
    % &\quad + 2 \sum_{r=1}^m \sum_{r'=1}^m \sE \Big[ \sigma(\langle \bw_{r,t}^{(1)}, \bx_t^{(1)}  \rangle) \sigma(\langle \bw_{r',t}^{(2)}, \bx_t^{(2)} \rangle) \Big] \langle \bw_{r,t}^{(1)}, \bu\rangle \langle \bw_{r,t}^{(2)}, \bv \rangle \\
    % &\quad - \sum_{r=1}^m \sE \Big[  \sigma(\langle \bw_{r,t}^{(1)} , \bx_t^{(1)}  \rangle)  \Big]^2 \langle \bw_r^{(1)},\bu \rangle^2 - 2 \sum_{r=1}^m \sum_{r'\neq r} \sE\Big[ \sigma\big( \langle \bw_{r,t}^{(1)}, \bx_{t}^{(1)} \rangle \big) \Big] \sE \Big[ \sigma( \langle \bw_{r',t}^{(1)}, \bx_t^{(1)} \rangle ) \Big] \langle \bw_{r,t}^{(1)}, \bu \rangle \langle \bw_{r',t}^{(1)}, \bu  \rangle  \\
    % &\quad - \sum_{r=1}^m \sE \Big[  \sigma(\langle \bw_{r,t}^{(2)} , \bx_t^{(2)}  \rangle)  \Big]^2 \langle \bw_r^{(2)},\bv \rangle^2 - 2 \sum_{r=1}^m \sum_{r'\neq r} \sE\Big[ \sigma\big( \langle \bw_{r,t}^{(2)}, \bx_{t}^{(2)} \rangle \big) \Big] \sE \Big[ \sigma( \langle \bw_{r',t}^{(2)}, \bx_t^{(2)} \rangle ) \Big] \langle \bw_{r,t}^{(2)}, \bv \rangle \langle \bw_{r',t}^{(2)}, \bv  \rangle \\
    % &\quad - 2 \sum_{r=1}^m \sum_{r'=1}^m \sE \Big[ \sigma(\langle \bw_{r,t}^{(1)}, \bx_t^{(1)}  \rangle) \Big] \Big[ \sigma(\langle \bw_{r',t}^{(2)}, \bx_t^{(2)} \rangle) \Big] \langle \bw_{r,t}^{(1)}, \bu\rangle \langle \bw_{r,t}^{(2)}, \bv \rangle
\end{align*}
where we use the law of total variance and denote ${\rm Var}_{|\zeta} = {\rm Var}(\cdot |\zeta)$.

Given the gradient direction only consists of $\bu, \bw_{r,t}^{(1),0}$ and $\bv, \bw_{r,t}^{(2),0}$ respectively for the two patches, we can decompose the weights $\bw_{r,t}^{(1)}, \bw_{r,t}^{(2)}$ into
\begin{align*}
    \bw_{r,t}^{(1)} = \phi_{r,t} \bw_{r,t}^0 + \gamma_{r,t} \bu \\
    \bw_{r,t}^{(2)} = \varphi_{r,t} \bw_{r,t}^0 + \varsigma_{r,t} \bu
\end{align*}
for $r \in [m]$. In addition, we decompose for each $p = 1,2$
\begin{align*}
    \beps_t^{(p)} = {\beps}_{t,-}^{(p)} +  \beps_{t, \perp}^{(p)} = \gP^{(p)}_0 \beps_t^{(p)} + (I_d - \gP^{(p)}_0) \beps_{t}^{(p)}
\end{align*}
where $\gP^{(p)}_0$ denotes the projection onto the span of $\{ \bw^{(p),0}_{1,t}, ..., \bw^{(p),0}_{m,t}  \}$. Then we can write for the first patch
\begin{align*}
    &\sum_{r=1}^m \sigma(\langle \bw_{r,t}^{(1)}, \bx_t^{(1)} \rangle) \langle \bw_{r,t}^{(1)}, \bu \rangle \\
    &= \sum_{r=1}^m\sigma( \langle \phi_{r,t} \bw_{r,t}^0 + \gamma_{r,t} \bu, \alpha_t \zeta \bu + \beta_t ( {\beps}_{t,-}^{(1)} +  \beps_{t, \perp}^{(1)}  ) \rangle ) \langle \phi_{r,t} \bw_{r,t}^0 + \gamma_{r,t} \bu , \bu  \rangle\\
    &= \sum_{r=1}^m\sigma\Big(  \phi_{r,t} \alpha_t \zeta \langle  \bw_{r,t}^0, \bu \rangle + \gamma_{r,t} \alpha_t \zeta + \beta_t \langle \phi_{r,t} \bw_{r,t}^0 + \gamma_{r,t} \bu, \beps_{t,-}^{(1)} \rangle + \beta_t \gamma_{r,t} \langle \bu, \beps_{t,\perp}^{(1)} \rangle \Big) \Big( \phi_{r,t} \langle \bw_{r,t}^0 ,\bu\rangle + \gamma_{r,t} \Big) \\
    &= \widetilde \sigma^{(1)}( \langle \bu, \beps_{t, \perp}^{(1)} \rangle )
\end{align*} 
where $\widetilde \sigma^{(1)}(\cdot)$ is a polynomial with coefficients depending on $\gamma_{r,t}, \phi_{r,t}, \alpha_t, \beta_t, \zeta$. Similarly, we can write for the second patch that 
\begin{align*}
    &\sum_{r=1}^m \sigma(\langle \bw_{r,t}^{(2)}, \bx_t^{(2)} \rangle) \langle \bw_{r,t}^{(2)}, \bv \rangle  = \widetilde \sigma^{(2)} (\langle \bv, \beps_{t,\perp}^{(2)} \rangle)
\end{align*}
where $\widetilde \sigma^{(2)}(\cdot)$ is a polynomial of the same form as $\widetilde \sigma^{(1)}(\cdot)$ except that $\phi_{r,t}, \gamma_{r,t}, \zeta$ is respectively replaced with $\varphi_{r,t}, \varsigma_{r,t}, 1- \zeta$.
Then we can lower bound the variance by 
\begin{align*}
    \gE_{\rm variance} 
    &\geq \sE_\zeta \Big[ {\rm Var}_{|\zeta} \Big(  \sum_{r=1}^m \sigma( \langle \bw_{r,t}^{(1)}, \bx_t^{(1)} \rangle  )  \langle \bw_{r,t}^{(1)}, \bu \rangle  \Big) + {\rm Var}_{|\zeta} \Big(  \sum_{r=1}^m \sigma( \langle \bw_{r,t}^{(2)}, \bx_t^{(2)} \rangle  )  \langle \bw_{r,t}^{(2)}, \bv \rangle  \Big)  \Big] \\
    &\geq \sE_{\zeta, \beps_{t,-}^{(1)}, \beps_{t,-}^{(2)}} \Big[ {\rm Var}_{|\zeta, \beps_{t,-}^{(1)}} \big( \widetilde \sigma^{(1)}( \langle \bu, \beps_{t, \perp}^{(1)} \rangle ) \big) +  {\rm Var}_{|\zeta, \beps_{t,-}^{(2)}} \big( \widetilde \sigma^{(2)}( \langle \bv, \beps_{t, \perp}^{(2)} \rangle ) \big) \Big]
\end{align*}
where we use law of total variance. 
\end{proof}






\begin{lemma}[\cite{cao2022benign}]
\label{lemma:init_scale}
If $\bw_t^0 \sim \gN(0, \sigma_0^2 \bI)$, we have with probability at least $1-\delta$
\begin{align*}
    &\sigma_0^2 d (1 - \widetilde O(d^{-1/2})) \leq \| \bw_{t}^0 \|^2 \leq \sigma_0^2 d (1 + \widetilde O(d^{-1/2})) \\
    &|\langle \bw_{t}^0, \bu \rangle| \leq \sqrt{2 \log(8/\delta)} \sigma_0,  \\
     &|\langle \bw_{t}^0, \bv \rangle| \leq \sqrt{2 \log(8/\delta)} \sigma_0, 
\end{align*}
\end{lemma}
% \begin{proof}[Proof of Lemma \ref{lemma:init_scale}]
% Let $\delta = 0.1$. Because at initialization $\bw_{r,t}^0 \sim \gN(0, \sigma_0^2 \bI)$, by Bernstein's inequality, with probability at least $1 - \delta/5$, we have 
% \begin{align*}
%     | \| \bw_{r,t}^0  \|^2_2  - \sigma_0^2 d|  = O(  \sigma_0^2 \sqrt{d \log(10/\delta)} )
% \end{align*}
% In addition, by Gaussian tail bound, we have with probability at least $ 1- \delta/5$ that 
% \end{proof}




\begin{proof}[Proof of Theorem \ref{thm:main_linear}]
Let $L^{(p)} (\bW_t) = \sE_{\beps_{t}^{(p)}, \bx_{0}^{(p)}} \|  s_w^{(p)}(\bx_t^{(p)}) - \beps_{t}^{(p)} \|^2$. Then the loss can be written as
\begin{align*}
    L(\bW_t) = \sum_{p=1}^P L^{(p)} (\bW_t) =  \sum_{p=1}^P \sE_{\beps_{t}^{(p)}, \bx_{0}^{(p)}} \Big\|  s_w^{(p)}(\bx_t^{(p)}) - \beps_{t}^{(p)} \Big\|^2 = \sum_{p=1}^P  \sE_{\beps_{t}^{(p)}, \bx_t^{(p)}}  \Big\|  \langle \bw_t^{(p)}, \bx_{t}^{(p)} \rangle   \bw_t^{(p)} - \frac{1}{\beta_t^2} \bx_{t}^{(p)} - \beps_{t}^{(p)} \Big\|^2
\end{align*}
We first simplify the loss as follows, where we omit the superscript and consider a single patch due to that each patch is independent and weights are separated.
\begin{align*}
    &\sE_{\beps_{t,i}} \Big\| \langle \bw_t, \bx_{t,i} \rangle \bw_t - \frac{1}{\beta_t^2} \bx_{t,i} - \beps_{t,i}  \Big\|^2 \\
    &= \underbrace{\sE_{\beps_{t,i}} \Big\|  \langle \bw_t, \bx_{t,i} \rangle \bw_t  \Big\|^2}_{I_1} + \underbrace{\sE_{\beps_{t,i}} \Big\|  \frac{1}{\beta_t^2} \bx_{t,i} + \beps_{t,i} \Big\|^2}_{I_2} - 2 \underbrace{\sE_{\beps_{t,i}} \big[ \langle \bw_t  , \bx_{t,i} \rangle \langle \bw_t, \frac{1}{\beta_t^2 }\bx_{t,i} + \beps_{t,i} \rangle \big]}_{I_3}
\end{align*}
where we can compute each term following \citep{han2024feature} as  
\begin{align*}
    &I_1 = \sE_{\beps_{t,i}} [ \langle \bw_t, \bx_{t,i} \rangle^2 ] \| \bw_t \|^2 = \Big( \alpha_t^2 \langle \bw_t, \bx_{0,i} \rangle^2 + \beta_t^2 \| \bw_t \|^2 \Big) \| \bw_t \|^2 \\
    &I_2 = \sE_{\beps_{t,i}} \Big[  \frac{\alpha_t^2}{\beta_t^4} \| \bx_{0,i}  \|^2 + \big( 1 + \frac{1}{\beta_t}\big)^2 \| \beps_{t,i} \|^2  \Big] =  \frac{\alpha_t^2}{\beta_t^4 } \| \bx_{0,i}\|^2 + \Big( 1 + \frac{1}{\beta_t} \Big)^2 d \\
    &I_3  = \frac{\alpha_t}{\beta_t^2} \sE_{\beps_{t,i}} \Big[ \langle \bw_t, \bx_{t,i} \rangle \Big] \langle \bw_t, \bx_{0,i} \rangle + \Big( \frac{1}{\beta_t} +1 \Big) \sE_{\beps_{t,i}} \big[ \langle \bw_t, \bx_{t,i} \rangle \langle \bw_t, \beps_{t,i} \rangle \big]  = \frac{\alpha_t^2}{\beta_t^2}  \langle \bw_t, \bx_{0,i} \rangle^2 + (1 + \beta_t) \| \bw_t \|^2.
\end{align*}
This suggests  
\begin{align*}
    \sE_{\beps_{t,i}} \Big\| \langle \bw_t, \bx_{t,i} \rangle \bw_t - \frac{1}{\beta_t^2} \bx_{t,i} - \beps_{t,i}  \Big\|^2 = \Big( \alpha_t^2 \langle \bw_t, \bx_{0,i} \rangle^2 + \beta_t^2 \| \bw_t \|^2 \Big) \| \bw_t \|^2 - \frac{2\alpha_t^2}{\beta_t^2}  \langle \bw_t, \bx_{0,i} \rangle^2 - 2(1 + \beta_t) \| \bw_t \|^2 + I_2
\end{align*}
where $I_2$ is a constant independent of $\bw_t$. Then we obtain the loss for the first two patches as 
\begin{align*}
    &L^{(1)}(\bw_t^{(1)}) = \big( \alpha_t^2 \sE [\zeta^2] \langle \bw_t^{(1)}, \bu \rangle^2 + \beta_t^2 \| \bw_t^{(1)} \|^2 \big) \| \bw_t^{(1)} \|^2 - \frac{2 \alpha_t^2}{\beta_t^2 } \sE [\zeta^2] \langle \bw_t^{(1)}, \bu \rangle^2 - 2 (1 + \beta_t) \| \bw_t^{(1)} \|^2 + I_2 \\
    &L^{(2)}(\bw_t^{(2)}) = \big( \alpha_t^2 \sE [(1-\zeta)^2] \langle \bw_t^{(2)}, \bv \rangle^2 + \beta_t^2 \| \bw_t^{(2)} \|^2 \big) \| \bw_t^{(2)} \|^2 - \frac{2 \alpha_t^2}{\beta_t^2 } \sE [(1-\zeta)^2] \langle \bw_t^{(2)}, \bv \rangle^2 - 2 (1 + \beta_t) \| \bw_t^{(2)}\|^2 + I_2.
\end{align*}
We next analyze the training dynamics of the gradient descent on the first patch. The second patch follows from similar analysis. For notation clarity, we omit the superscript. 

The gradient for the first patch can be computed as
\begin{align*}
    \nabla L^{(1)}(\bw_t) &=  \| \bw_t \|^2( 2\alpha_t^2 \sE [\zeta^2] \langle \bw_t, \bu \rangle \bu + 2\beta_t^2 \bw_t ) + 2 \Big( \alpha_t^2 \sE [\zeta^2] \langle \bw_t, \bu \rangle^2 + \beta_t^2 \| \bw_t \|^2 \Big) \bw_t \\
    &\quad - \frac{2\alpha_t^2}{\beta_t^2} \sE [\zeta^2] \langle \bw_t, \bu \rangle \bu - 2 (1 + \beta_t) \bw_t.
\end{align*}

It is noticed that the gradient only consists of directions of $\bw_t^0$ and $\bu$. It suffices to track the gradient descent dynamics projected to the two directions $\bu$ and $ \widetilde \bw_{t}^0$, where $\widetilde \bw_t^0 = \bw_t^0 - \langle \bw_t^0, \bu \rangle \bu$, i.e.,
\begin{align*}
    \langle \bw_{t}^{k+1}, \bu \rangle  &= \langle \bw_{t}^{k}, \bu \rangle - \eta \langle \nabla^{(1)} L(\bw_t), \bu \rangle  \\
    &= \left(1  + \eta \Big( 2 \alpha_t^2 \beta_t^{-2} \sE[\zeta^2] + 2(1 + \beta_t) - 2 \alpha_t^2\sE[\zeta^2] \| \bw_t^k \|^2 - 4 \beta_t^2 \| \bw_t^k \|^2 - 2 \alpha_t^2 \sE[\zeta^2] \langle \bw_t^k, \bu\rangle^2 \Big) \right) \langle \bw_t^k, \bu \rangle \\
    \langle \bw_{t}^{k+1}, \widetilde \bw_t^0 \rangle \| \widetilde\bw_t^0\|^{-1}  &= \langle \bw_{t}^{k}, \widetilde\bw_t^0 \rangle \| \widetilde\bw_t^0\|^{-1} - \eta \langle \nabla^{(1)} L(\bw_t), \widetilde \bw_t^0 \rangle \| \widetilde\bw_t^0\|^{-1} \\
    &=\left( 1  + \eta \Big( 2 (1+ \beta_t ) - 4 \beta_t^2 \| \bw_t^k \|^2 - 2 \alpha_t^2 \sE[\zeta^2] \langle \bw_t^k, \bu \rangle^2  \Big) \right) \langle \bw_t^k, \widetilde \bw_t^0 \rangle \| \widetilde\bw_t^0\|^{-1} 
\end{align*}
It is clear that gradients become zero only when $\Theta(\| \bw_{t}^k\|^2 + \langle \bw_t^k, \bu \rangle^2) = \Theta(1)$. This suggests that before convergence, $\| \bw_t^k \|^2 = o(1)$ given the initialization is small, i.e., $\sigma_0 = O(d^{-1/2})$. We can then verify that $\langle \nabla^{(1)} L(\bw_t), \bu \rangle \geq \langle \nabla^{(1)} L(\bw_t), \widetilde \bw_t^0 \rangle + C$ for some constant $C$. 

In addition, suppose we decompose $\bw_{t}^k = \phi_t^k \widetilde \bw_t^0 + \gamma_t^k \bu$, we can see 
\begin{align*}
    &\phi_t^k = \langle \bw_t^k,  \widetilde \bw_t^0 \rangle \| \bw_t^0 \|^{-2}, \quad \gamma_t^k = \langle \bw_t^k, \bu \rangle
\end{align*}
which then implies 
\begin{align*}
    \| \bw_t^k \|^2 = \langle \bw_t^k, \widetilde \bw_t^0 \rangle^2 \| \widetilde \bw_t^0 \|^{-2} + \langle \bw_t^k, \bu \rangle^2.
\end{align*}
This combined with the fact that $\langle \nabla^{(1)} L(\bw_t), \bu \rangle \geq \langle \nabla^{(1)} L(\bw_t), \widetilde \bw_t^0 \rangle + C$ suggests that $\langle \bw_{t}^{k+1}, \widetilde \bw_t^0 \rangle \| \widetilde\bw_t^0\|^{-1}$ cannot increase to $\Theta(1)$ without $\langle \bw_t^k, \bu \rangle$ reaching $\Theta(1)$. Thus at stationary point, we must have both $\langle \bw_t^k, \bu \rangle, \| \bw_t^k\|^2 = \Theta(1)$.


Next, we analyze the stationary point. Given the gradient only consists of directions $\bw_t^k$ and $\bu$, we have for any stationary point $\bw_t$, it satisfies
\begin{align*}
    \langle\nabla L^{(1)}(\bw_t) , \bw_t \rangle &=\| \bw_t \|^2( 2\alpha_t^2 \sE [\zeta^2] \langle \bw_t, \bu \rangle^2 + 2\beta_t^2 \| \bw_t\|^2 ) + 2 \Big( \alpha_t^2 \sE [\zeta^2] \langle \bw_t, \bu \rangle^2 + \beta_t^2 \| \bw_t \|^2 \Big) \| \bw_t \|^2 \\
    &\quad - \frac{2\alpha_t^2}{\beta_t^2} \sE [\zeta^2] \langle \bw_t, \bu \rangle^2 - 2 (1 + \beta_t)  \| \bw_t \|^2 = 0 \\
    \langle\nabla L^{(1)}(\bw_t) , \bu \rangle &= \| \bw_t \|^2( 2\alpha_t^2 \sE [\zeta^2] \mu^2  \langle \bw_t, \bu \rangle + 2\beta_t^2 \langle \bw_t, \bu \rangle ) + 2 \Big( \alpha_t^2 \sE [\zeta^2] \langle \bw_t, \bu \rangle^2 + \beta_t^2 \| \bw_t \|^2 \Big) \langle \bw_t, \bu \rangle \\
    &\quad - \frac{2\alpha_t^2}{\beta_t^2} \sE [\zeta^2] \mu^2 \langle \bw_t, \bu \rangle  - 2 (1 + \beta_t) \langle \bw_t , \bu \rangle = 0
\end{align*}

\begin{figure}[]
  \centering  \includegraphics[width=0.95\textwidth, height=0.4\textheight]{figures/contrastive_example.pdf}
  \vspace*{-5mm}
  \caption{\textbf{Constructed contrastive data} includes three classes, which differ in fine-grained rules.}
  \label{fig:case_contrastive}
\end{figure}


We solve the stationary equalities as 
\begin{align*}
    &\| \bw_t^{(1)} \|^2 = \frac{6 \alpha_t^2 \beta_t^{-2} \sE[\zeta^2] + 6 + 2 \beta_t \pm \sqrt{4 \alpha_t^4 \beta_t^{-4} (\sE [\zeta^2])^2 + (56 + 16 \beta_t) \alpha_t^2 \beta_t^{-2} \sE[\zeta^2] + 28 + 16 \beta_t + 4 \beta_t^2 }}{8 \alpha_t^2 \sE[\zeta^2] + 8 \beta_t^2 }\\
    &\langle\bw_t^{(1)}, \bu \rangle^2 = \frac{\| \bw_t^{(1)}\|^2}{\alpha_t^2\sE[\zeta^2]} \frac{1 + \beta_t - 2 \beta_t^2 \| \bw_t^{(1)} \|^2}{2  \| \bw_t^{(1)} \|^2 - \beta_t^{-2} }
\end{align*}
Similarly, we can compute and solve the stationary point for the second patch where $\sE[\zeta^2]$ is replaced with $\sE[(1- \zeta)^2]$. 

We then compute the bias error as 
\begin{align*}
    \gE_{\rm bias}
    &= \sE_{\beps_{t}, \bx_0} \Big[  \langle \bw_t^{(1)}, \bx_t^{(1)} \rangle \langle \bw_t^{(1)}, \bu \rangle +  \langle \bw_t^{(2)} , \bx_t^{(2)} \rangle  \langle \bw_t^{(2)}, \bv \rangle  \Big] - \frac{\alpha_t}{\beta_t^2 }\\
    &= \Big| \alpha_t \sE[\zeta] \langle \bw_t^{(1)}, \bu \rangle^2 + \alpha_t \sE[1-\zeta] \langle \bw_t^{(2)}, \bv\rangle^2 - \frac{\alpha_t}{\beta_t^2}  \Big| \\
    &=\left| \frac{\sE[\zeta] \| \bw_t^{(1)}  \|^2}{\alpha_t \sE[\zeta^2]} \frac{1 + \beta_t - 2 \beta_t^2 \| \bw_t^{(1)} \|^2}{2  \| \bw_t^{(1)} \|^2 - \beta_t^{-2} } + \frac{\sE[1-\zeta] \| \bw_t^{(2)}  \|^2}{\alpha_t \sE[(1-\zeta)^2]} \frac{1 + \beta_t - 2 \beta_t^2 \| \bw_t^{(2)} \|^2}{2  \| \bw_t^{(2)} \|^2 - \beta_t^{-2} } - \frac{\alpha_t}{\beta_t^2} \right| = C_0({\sE[\zeta], \sE[\zeta^2], \alpha_t, \beta_t})
\end{align*}
It can be easily verified that there exists a constant bias $C_0$ that depends on $\sE[\zeta], \sE[\zeta^2], \alpha_t, \beta_t$. In addition, we compute the variance as 
\begin{align*}
    \gE_{\rm variance} &= \Big( \alpha_t^2 \sE[\zeta^2] \langle \bw_t^{(1)}, \bu \rangle^2 + \beta_t^2 \|\bw_t^{(1)} \|^2\Big) \langle \bw_t^{(1)}, \bu \rangle^2 + \Big( \alpha_t^2 \sE[(1-\zeta)^2] \langle \bw_t^{(2)}, \bv \rangle^2 + \beta_t^2 \| \bw_t^{(2)}\|^2 \Big) \langle \bw_t^{(2)}, \bv \rangle^2 \\
    &\quad + 2 \alpha_t^2 \sE[\zeta(1- \zeta)] \langle \bw_t^{(1)}, \bu \rangle^2 \langle \bw_t^{(2)}, \bv \rangle^2  - \Big( \alpha_t \sE[\zeta] \langle \bw_t^{(1)}, \bu \rangle^2 + \alpha_t \sE[1-\zeta] \langle \bw_t^{(2)}, \bv\rangle^2  \Big)^2 \\
    &= \alpha_t^2  {\rm Var}(\zeta) \langle \bw_{t}^{(1)}, \bu \rangle^4 + \alpha_t^2 {\rm Var}(1-\zeta) \langle \bw_{t}^{(2)}, \bv \rangle^4 - 2 \alpha_t^2 {\rm Cov}(\zeta, 1- \zeta) \langle \bw_t^{(1)}, \bu \rangle^2 \langle \bw_t^{(2)}, \bv \rangle^2  \\
    &\quad + \beta_t^2 \| \bw_t^{(1)} \|^2 \langle \bw_t^{(1)}, \bu \rangle^2 + \beta_t^2 \| \bw_t^{(2)} \|^2 \langle \bw_t^{(2)}, \bv \rangle^2\\
    &=  \alpha_t^2 {\rm Var}\Big( \zeta \langle\bw_{t}^{(1)}, \bu \rangle^2 - (1-\zeta) \langle\bw_t^{(2)}, \bv  \rangle^2 \Big)  +  \beta_t^2 \| \bw_t^{(1)} \|^2 \langle \bw_t^{(1)}, \bu \rangle^2 + \beta_t^2 \| \bw_t^{(2)} \|^2 \langle \bw_t^{(2)}, \bv \rangle^2 \\
    &= C_1(\sE[\zeta], \sE[\zeta^2], \alpha_t, \beta_t) > 0
\end{align*}
where we see $\sE[A^2] - \sE[A]^2 = {\rm Var}(A) \geq 0$ for arbitrary random variable $A$. 
\end{proof}



\section{Experiment Details on Synthetic Data with Two-layer Diffusion Model}
\label{app:synthe_two_layer}

In order to verify the theoretical claims on DMs failing to precisely recover the inter-feature rule \eqref{eq:hidden_rule} (in Section \ref{sec:Theory}), we conduct numerical experiments on a two-layer diffusion model on a two-patch data distribution.

Specifically we set $\bx = [\bx^{(1)\top}, \bx^{(2)\top}]$ where $\bx^{(1)} = \zeta \bu$, $\bx^{(2)\top} = (1-\zeta) \bv$. Here we set $\bu = [1, 0,\cdots0] \in \sR^d$, $\bv = [0,1,0,\cdots0] \in \sR^d$ with $d = 100$. The score network follows the structure in \eqref{eq:score_network_main} where we consider $\sigma(\cdot)$ to be ReLU, linear, quadratic and cubic activation functions. We set network width $m = 20$. To simulate the DDPM loss in expectation, for each epoch, we sample $n = 1000$ input data $\bx_{0,i}, i \in [n]$ and for each data we sample $n_\epsilon = 1000$ standard Gaussian noise $\beps_{t,i,j}, i,j \in [1000]$, and consider minimizing the empirical loss 
\begin{equation*}
    L(\bW_t) = \frac{1}{n n_\epsilon} \sum_{i = 1}^n \sum_{j=1}^{n_\epsilon} \sum_{p=1}^2 \| s_w(\bx_{t,i,j}^{(p)}) - \beps_{t,i,j}^{(p)} \|^2
\end{equation*}
where $\bx_{t,i,j}^{(p)} = \alpha_t \bx_{0,i}^{(p)} + \beta_t \beps_{t,i,j}^{(p)}$, $p =1,2$. We use gradient descent to train the score network for $5000$ epochs. We consider $\alpha_t = \exp(-t)$ and $\beta_t = \sqrt{1-\exp(-2t)}$ where we set $t = 0.2, 0.4, 0.6, 0.8$. We then check whether learned diffusion models learn the ground-truth rule \eqref{eq:hidden_rule} by plotting the distribution of $\psi_t(\bx_t)$ against $\alpha_t/\beta_t^2$. The distribution of $\psi_t(\bx_t)$ is estimated with $5000$ samples $\bx_t$. 


\section{Details of Mitigation Strategies}
\subsection{Details of Guided Diffusion}
\label{app:Details of Guided Diffusion}
Guided Diffusion is a common strategy that trains an additional classifier to guide DDPM generation towards desired samples during the sampling process.
\begin{figure*}[]
\centering
    \hfill
    \subfigure[Task A]{\includegraphics[width=0.24\textwidth]{figures/taska_contrastive_training.pdf}}
    \hfill
    \subfigure[Task B]{\includegraphics[width=0.24\textwidth]{figures/taskb_contrastive_training.pdf}}
    \hfill
    \subfigure[Task C]{\includegraphics[width=0.24\textwidth]{figures/taskc_contrastive_training.pdf}}
    \hfill
    \subfigure[Task D]{\includegraphics[width=0.24\textwidth]{figures/taskd_contrastive_training.pdf}}
    \hfill
\vspace{-0.1in}
\caption{\textbf{Construction of contrastive training data.} For each task, we build a three-class dataset where Class $1$ represents samples satisfying fine-grained rules, while Classes $0$ and $2$ represent samples that only satisfy coarse-grained rules. Based on these constructed contrastive datasets, we train classifiers as additional guidance to improve DDPM's generation.}
\vspace{-0.15in}
\label{fig:more_contrastive}
\end{figure*}


\paragraph{Training Details and Results.} This section includes the details of training classifiers with contrastive learning as guidance. \cref{fig:case_contrastive} visualizes the constructed contrastive data, where each dataset includes three sample types that differ only in fine-grained rules and appear nearly identical at a glance. \cref{fig:more_contrastive} visualizes the contrastive datasets constructed for each of the four synthetic tasks. The classifier training for each task is treated as a three-class classification problem with $2000$ positive samples (class $1$) and $2000$ samples per negative class (classes $0$ and $2$). We use U-Net as the classifier architecture, trained for $20000$ iterations with a learning rate of $3e-4$, and a contrastive learning weight $\lambda = 1$. Beyond standard guided diffusion, we dynamically adjust guidance weights (gradient scales) with a piecewise strategy where guidance is activated only in the final $20$ denoising steps. The weight linearly increases from $0$ to predefined gradient scale factors ($7$ for Tasks A/C, $10$ for Tasks B/D). Through comparation of constant versus piecewise weighting, we report optimal strategies: Task A,B,D for standard sweighting method and Task C employs the piecewise weighting method. As noted in Section \ref{sec:limitation}, training high-accuracy classifiers is not easy in our problem, as evidenced by the accuracy of the training data for Tasks A, B, C, and D being $0.57$, $0.51$, $0.55$, and $0.63$, respectively.
\begin{figure*}[]
\centering
    \hfill
    \subfigure[Task A]{\includegraphics[width=0.24\textwidth]{figures/taska_clip.pdf}}
    \hfill
    \subfigure[Task B]{\includegraphics[width=0.24\textwidth]{figures/taskb_clip.pdf}}
    \hfill
    \subfigure[Task C]{\includegraphics[width=0.24\textwidth]{figures/taskc_clip.pdf}}
    \hfill
    \subfigure[Task D]{\includegraphics[width=0.24\textwidth]{figures/taskd_clip.pdf}}
    \hfill
\vspace{-0.1in}
\caption{\textbf{CLIP representation of contrastive training data}. For each task, we use the CLIP model to extract its representations and apply UMAP for dimensionality reduction. We observe that the contrastive data is nearly inseparable, which presents a challenge for training the classifier.}
\vspace{-0.15in}
\label{fig:more_contrastive_clip}
\end{figure*}
\paragraph{NT-Xent Loss.} NT-Xent Loss (Normalized Temperature-scaled Cross Entropy Loss) \cite{sohn2016improved} is commonly used in contrastive learning to measure the similarity between positive pairs (similar samples) and distinguish them from negative pairs (dissimilar samples). 
\begin{align}
    \mathcal{L}_{\text{NT-Xent}}(i, j) = -\log \left( \frac{\exp(\text{sim}(\mathbf{z}_i, \mathbf{z}_j)/\tau)}{\sum_{k=1}^{2N} \mathbf{1}_{[k \neq i]} \exp(\text{sim}(\mathbf{z}_i, \mathbf{z}_k)/\tau)} \right),
\end{align}
Where \( \mathbf{z}_i \) and \( \mathbf{z}_j \) are the embeddings of the \(i\)-th and \(j\)-th samples, \( \text{sim}(\mathbf{z}_i, \mathbf{z}_j) = \frac{\mathbf{z}_i^\top \mathbf{z}_j}{\|\mathbf{z}_i\| \|\mathbf{z}_j\|} \) is the cosine similarity and \( \tau \) is the temperature parameter that scales the similarity which  we set $\tau = 0.5$ in our experiments.
\subsection{Details of Filtered DDPM}
Filtered DDPM is a more straightforward strategy that uses a classifier trained on raw images to filter DDPM generations, keeping only samples predicted to satisfy fine-grained rules.
\paragraph{Training Details and Results.}Based on the contrastive data constructed in \cref{fig:more_contrastive}, we split the training and test data in an 80:20 ratio and directly train a three-way classifier in the raw image space, using MLP, ResNet-8, and U-Net architectures. 
\begin{wrapfigure}{r}{0.25\textwidth}
% \vspace{-.2in}
\begin{center}
    \includegraphics[width=0.98\linewidth]{figures/test_acc.pdf}
\end{center}
\vspace{-0.2in}
\caption{Test accuracy of different model architectures on contrastive datasets.} 
\vspace{-0.5in}
\label{fig:test_acc} 
\end{wrapfigure}
These models are trained for $100$ epochs with a learning rate of $3e-4$. As shown in \cref{fig:test_acc}, the classifiers achieve accuracy between $60\%$ and $80\%$. While they outperform classifiers trained for guided diffusion due to the noise-free setting, they still fail to achieve $100\%$ accuracy, even for these simple synthesis tasks. Additionally, \cref{fig:more_contrastive_clip} shows the representations extracted by CLIP \cite{radford2021learning} for each synthetic task, followed by dimensionality reduction using UMAP \cite{mcinnes2018umap}. We observe that the data from different categories in the contrastive data is difficult to distinguish, which presents a challenge for training the classifier. Based on test accuracy, we use the trained MLP model to filter DDPM generations for Tasks A and B, keeping only samples predicted as Class 1 (satisfying fine-grained rules). For Tasks C and D, we use the U-Net model for filtering.

% \begin{figure}[t!]
%     \centering
%     \includegraphics[width=0.2\linewidth]{figures/test_acc.pdf}
%     \vspace{-0.1in}
%     \caption{Caption}
%     \label{fig:test_acc} 
% \end{figure}
% \begin{figure}[t!]
% \centering
%     \hfill
%     \subfigure[Test Acyuracy.]{\label{fig:test_acc} \includegraphics[width=0.235\textwidth]{figures/test_acc.pdf}}
%     \hfill
% \vspace{-0.2in}
% \caption{\cref{fig:taskd_contrastive_training} shows the construction of contrastive data of Task D, where Class $1$ samples satisfy fine-grained rules, while Classes $0$ and $2$ only conform to coarse-grained ones. \cref{fig:taskd_clip} shows CLIP representations (reduced by UMAP \cite{mcinnes2018umap}) of three classes in Task D are indistinguishable.}
% \vspace{-0.15in}
% \end{figure}

