% Consider a lasso optimization procedure with potentially distinct regularization penalties:
% \begin{align}
%     \hat{\beta} = \arg\min_{\beta}\{\|y-X\beta\|^2_2+\sum_{i=1}^{N}\lambda_i|\beta_i|\}.
% \end{align}
\subsection{Supervised Data-Driven Learning}\label{subsec:supervised}
We consider a generic data-driven supervised learning procedure. Given a dataset \( \mathcal{D} \) consisting of \( n \) data points \( (x_i, y_i) \in \mathcal{X} \times \mathcal{Y} \) drawn from an underlying distribution \( p(\cdot|\theta) \), our goal is to estimate parameters \( \theta \in \Theta \) through a learning procedure, defined as \( f: (\mathcal{X} \times \mathcal{Y})^n \rightarrow \Theta \) 
that minimizes the predictive error on observed data. 
Specifically, the learning objective is defined as follows:
\begin{align}
\hat{\theta}_f := f(\mathcal{D}) = \arg\min_{\theta} \mathcal{L}(\theta, \mathcal{D}),
\end{align}
where \( \mathcal{L}(\cdot,\mathcal{D}) := \sum_{i=1}^{n} \mathcal{L}(\cdot, (x_i, y_i))\), and $\mathcal{L}$ is a loss function quantifying the error between predictions and true outcomes. 
Here, $\hat{\theta}_f$ is the parameter that best explains the observed data pairs \( (x_i, y_i) \) according to the chosen loss function \( \mathcal{L} (\cdot) \).

\paragraph{Feature Selection.}
Feature selection aims to improve model \( f \)'s predictive performance while minimizing redundancy. 
%Formally, given data \( X \), response \( y \), feature set \( \mathcal{F} \), loss function \( \mathcal{L}(\cdot) \), and a feature limit \( k \), the objective is:
% \begin{align}
% \mathcal{S}^* = \arg \min_{\mathcal{S} \subseteq \mathcal{F}, |\mathcal{S}| \leq k} \mathcal{L}(y, f(X_\mathcal{S})) + \lambda R(\mathcal{S}),
% \end{align}
% where \( X_\mathcal{S} \) is the submatrix of \( X \) for selected features \( \mathcal{S} \), \( \lambda \) is a regularization parameter, and \( R(\mathcal{S}) \) penalizes feature redundancy.
 State-of-the-art techniques fall into four categories: (i) filter methods, which rank features based on statistical properties like Fisher score \citep{duda2001pattern,song2012feature}; (ii) wrapper methods, which evaluate model performance on different feature subsets \citep{kohavi1997wrappers}; (iii) embedded methods, which integrate feature selection into the learning process using techniques like regularization \citep{tibshirani1996LASSO,lemhadri2021lassonet}; and (iv) hybrid methods, which combine elements of (i)-(iii) \citep{SINGH2021104396,li2022micq}. This paper focuses on embedded methods via Lasso, benchmarking against approaches from (i)-(iii).

\subsection{Language Modeling}
% The objective of language modeling is to learn a probability distribution \( p_{LM}(x) \) over sequences of text \( x = (X_1, \ldots, X_{|x|}) \), such that \( p_{LM}(x) \approx p_{text}(x) \), where \( p_{text}(x) \) represents the true distribution of natural language. This process involves estimating the likelihood of token sequences across variable lengths and diverse linguistic structures.
% Modern large language models (LLMs) are trained on vast datasets spanning encyclopedias, news, social media, books, and scientific papers \cite{gao2020pile}. This broad training enables them to generalize across domains, learn contextual knowledge, and perform zero-shot learning—tackling new tasks using only task descriptions without fine-tuning \cite{brown2020gpt3}.
Language modeling aims to approximate the true distribution of natural language \( p_{\text{text}}(x) \) by learning \( p_{\text{LM}}(x) \), a probability distribution over text sequences \( x = (X_1, \ldots, X_{|x|}) \). Modern large language models, trained on diverse datasets \citep{gao2020pile}, exhibit strong generalization across domains, acquire contextual knowledge, and perform zero-shot learning—solving new tasks using only task descriptions—or few-shot learning by leveraging a small number of demonstrations \citep{brown2020gpt3}.
\paragraph{Retrieval-Augmented Generation (RAG).} Retrieval-Augmented Generation (RAG) enhances the performance of generative language models by  integrating a domain-specific information retrieval process  \citep{lewis2020retrieval}. The RAG framework comprises two main components: \textit{retrieval}, which extracts relevant information from external knowledge sources, and \textit{generation}, where an LLM generates context-aware responses using the prompt combined with the retrieved context. Documents are indexed through various databases, such as relational, graph, or vector databases \citep{khattab2020colbert, douze2024faiss, peng2024graphretrievalaugmentedgenerationsurvey}, enabling efficient organization and retrieval via algorithms like semantic similarity search to match the prompt with relevant documents in the knowledge base. RAG has gained much traction recently due to its demonstrated ability to reduce incidence of hallucinations and boost LLMs' reliability as well as performance \citep{huang2023hallucination, zhang2023merging}. 
 
% image source: https://medium.com/@bindurani_22/retrieval-augmented-generation-815c1ae438d8
\begin{figure}
    \centering
\includegraphics[width=1.03\linewidth]{fig/fig1.pdf}
\vspace{-0.6cm}
\scriptsize 
    \caption{Retrieval Augmented Generation (RAG) based $\ell_1$-norm weights (penalty factors) for Lasso. Only feature names---no training data--- are included in LLM prompt.} 
    \label{fig:rag}
\end{figure}
% However, for the RAG model to be effective given the input token constraints of the LLM model used, we need to effectively process the retrieval documents through a procedure known as \textit{chunking}.

\subsection{Task-Specific Data-Driven Learning}
LLM-Lasso aims to bridge the gap between data-driven supervised learning and the predictive capabilities of LLMs trained on rich metadata. This fusion not only enhances traditional data-driven methods by incorporating key task-relevant contextual information often overlooked by such models, but can also be especially valuable in low-data regimes, where the learning algorithm $f:\mathcal{D}\rightarrow\Theta$ (seen as a map from datasets $\mathcal{D}$ to the space of decisions $\Theta$) is susceptible to overfitting.

The task-specific data-driven learning model $\tilde{f}:\mathcal{D}\times\mathcal{D}_\text{meta}\rightarrow\Theta$ can be described as a metadata-augmented version of $f$, where a link function $h(\cdot)$ integrates metadata (i.e. $\mathcal{D}_\text{meta}$) to refine the original learning process. This can be expressed as:
\[
\tilde{f}(\mathcal{D}, \mathcal{D}_\text{meta}) := \mathcal{T}(f(\mathcal{D}),  h(\mathcal{D}_{\text{meta}})),
\]
where the functional $\mathcal{T}$ takes the original learning algorithm $f(\mathcal{D})$ and transforms it into a task-specific learning algorithm $\tilde{f}(\mathcal{D}, \mathcal{D}_\text{meta})$ by incorporating the metadata $\mathcal{D}_\text{meta}$. 
% In particular, the link function $h(\mathcal{D}_{\text{meta}})$ provides a structured mechanism summarizing the contextual knowledge.

There are multiple approaches to formulate $\mathcal{T}$ and $h$.
%to ``inform" the data-driven model $f$ of %meta knowledge. 
For instance, LMPriors \citep{choi2022lmpriorspretrainedlanguagemodels} designed $h$ and $\mathcal{T}$ such that $h(\mathcal{D}_{\text{meta}})$ first specifies which features to retain (based on a probabilistic prior framework), and then $\mathcal{T}$ keeps the selected features and removes all the others from the original learning objective of $f$. 
Note that this approach inherently is restricted as it selects important features solely based on $\mathcal{D}_\text{meta}$ without seeing $\mathcal{D}$.

In contrast, we directly embed task-specific knowledge into the optimization landscape through regularization by introducing a structured inductive bias. This bias guides the learning process toward solutions that are consistent with metadata-informed insights, without relying on explicit probabilistic modeling. Abstractly, this can be expressed as:
\begin{align}
    \!\!\!\!\!\hat{\theta}_{\tilde{f}} := \tilde{f}(\mathcal{D},\mathcal{D}
    _\text{meta})= \arg\min_{\theta} \mathcal{L}(\theta, \mathcal{D}) + \lambda R(\theta, \mathcal{D}_{\text{meta}}),
\end{align}
where \( \lambda \) is a regularization parameter, \( R(\cdot) \) is a regularizer, and $\theta$ is the prediction parameter.
%We explain our framework with more details in the following section.


% Our research diverges from both aforementioned approaches by positioning the LLM not as a standalone feature selector but as an enhancement to data-driven models through an embedded feature selection method, L-LASSO. L-LASSO incorporates domain expertise—auxiliary natural language metadata about the task—via the LLM-informed LASSO penalty, which is then used in statistical models to enhance predictive performance. This method integrates the rich, context-sensitive insights of LLMs with the rigor and transparency of statistical modeling, bridging the gap between data-driven and knowledge-driven feature selection approaches. To approach this task, we need to tackle two key components: (i). train an LLM that is expert in the task-specific knowledge; (ii). inform data-driven feature selector LASSO with LLM knowledge.

% In practice, this involves combining techniques like prompt engineering and data engineering to develop an effective framework for integrating metadata into existing data-driven models. We will go through this in detail in Section \ref{mthd} and \ref{experiment}.

