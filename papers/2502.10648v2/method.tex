In this section, we introduce the LLM-Lasso framework by outlining its two key components: (i) the core statistical model that integrates an LLM-informed penalty into Lasso; and (ii) the general pipeline for training a task-specific LLM on expert knowledge base. An overview schematic is shown in  Figure \ref{fig:rag}.

\subsection{The LLM-Lasso}\label{subsec:llm_lasso}
We focus on the supervised learning framework introduced earlier in Section \ref{subsec:supervised} with input feature $X \in \mathbb{R}^{n \times p}$ and response $Y \in \mathbb{R}^n$. %In scenarios with a large number of predictors \( p \) relative to the number of observations \( n \)—a common situation in fields such as genomics, biology, and medicine — shrinkage methods like Lasso are commonly employed to perform feature selection and reduce overfitting in high-dimensional data.
The Lasso is a shrinkage method that places an $\ell_1$ penalty on the coefficient, which causes some of the coefficient to be exactly zero. The objective function of the Lasso is given by:
\begin{align}
    \underset{\beta}{\min} \left\{\frac{1}{2} \sum_{i=1}^n (y_i - \beta_0 - x_i^\top \beta)^2 + \lambda \sum_{j=1}^p |\beta_j| \right\}.
\end{align}

To incorporate prior knowledge of the relationship between \( X \) and \( Y \) into the learning of a prediction model \( f: (\mathcal{X} \times \mathcal{Y})^n \rightarrow \Theta \), one can enhance the Lasso by assigning penalty factors to each coefficient in the \( \ell_1 \) penalty \citep{zou2006adaptive}. The objective function of the Lasso with penalty factors is: 
% we don't bold vectors. notations need to be consistent.
\begin{align}
    \underset{\beta}{\min} \left\{\frac{1}{2} \sum_{i=1}^n (y_i - \beta_0 -x_i^\top \beta)^2 + \lambda \sum_{j=1}^p w_j |\beta_j| \right\}.
\end{align}
While penalty factors can be manually assigned based on prior knowledge, this approach becomes impractical when \( p \) is large. To address this, we leverage LLMs to streamline the integration of task-specific knowledge by generating LLM-informed penalty factors or importance scores for all predictors using domain-specific insights. The key modeling challenge is determining how best to effectively inform the underlying data-driven shrinkage method. In the following, we introduce two approaches for modeling LLM-informed penalty factors, whose performance is evaluated through simulation studies in Section \ref{sec:sim}.


\paragraph{Inverse importance penalty factors.}
First, simply taking the inverse of the importance scores and their powers is useful. In this case, the penalty factors would take the form $(\mathcal{I}_j)^{-\eta}$, where $\mathcal{I}_j$ is the importance score for feature $j$ and the power $\eta \geq 0$ would be determined via cross-validation. A large $\eta$ would indicate heavy reliance of the LLM-Lasso on the importance scores, whereas a value of $\eta$ close to 0 would indicate minimal reliance on the importance scores.% talk about interpretation and significance of this procedure.

\paragraph{ReLU-form penalty factors.}
Another approach to defining the penalty factors involves interpolating between penalty factors derived from the LLM and those used in Lasso regression with equal $\ell_1$-norm weights by applying a rectification operation. Specifically, we use a rectified linear unit (ReLU) to achieve this. Suppose $\tilde{w}_{(j)}$ is the penalty factor after ReLU-form processing of the $j$\textsuperscript{th} most important feature, as determined from the scores obtained from the LLM. Here, the largest penalty factor (and therefore the penalty factor of the least important feature) would be $\tilde{w}_{(p)}$, which is greater than 1. We can define $\tilde{w}_{(j)}$ as $\tilde{w}_{(j)} = 1 + \frac{(j - (1 - \gamma)p)_+}{\gamma p} \cdot (\tilde{w}_{(p)} - 1)$,
% %
% \begin{align}
%     w_{\tilde{j}} = 
%     \begin{cases}
%     \frac{j-p+\gamma p}{\gamma p}(w_{\tilde{p}}-1)+1, & \text{if $j>(1-\gamma)p$}\\
%     1, & \mbox{if $j\leq(1-\gamma)p$},
%     \end{cases}
% \end{align}
\noindent where $\gamma \in (0, 1)$ is the ReLU threshold. The lower the importance of a feature, the more likely its coefficient will be 0. The threshold $\gamma$ can be chosen via cross-validation. 
% Graphically, the relationship between $\tilde{\beta}_{\tilde{j}}$ and $\hat{\beta}_{\tilde{j}}$ for $\gamma=0.5$ and for different values of $j$ is as shown in Figure~\ref{fig:beta_plot}.
% % better way to formulate/place the graph?
% \begin{figure}[H]
%     \centering
%     \includegraphics[width=0.5\linewidth]{figures/beta_plot.png}
% \caption{\scriptsize Relationship between $\tilde{\beta}_{\tilde{j}}$ and $\hat{\beta}_{\tilde{j}}$ for $\gamma=0.5$ and for $j \in \{1,2,\cdots 0.5p\}$, $j=0.75p$, and $j=p$. We assume that $0.5p$ and $0.75p$ are positive integers.}
% \label{fig:beta_plot}
% \end{figure}




\subsection{Task-Specific LLM}\label{subsec:task_specific}
To develop a task-specific LLM that provides accurate answers grounded in rigorous and extensive domain knowledge, we focus on two key aspects: prompt engineering and knowledge-base embedding via RAG. % adding memorization if we have progress for scalability.  

\subsubsection{Prompt Engineering.}\label{subsec:prompt} % we currently emply CoT. Should we try self-consistency encoding
% black-box access means that prompting is a large part of the control over LLMs' predictive power.

Prompting is an efficient and effective approach for adapting pretrained LLMs to tackle new tasks not encountered during training \citep{radford2019language, liu2023prompting}. In our experiment, we employ a zero-shot approach for large-scale experiment on biomedical dataset, where the acquisition of ground truth is often infeasible, and a few-shot approach for small-scale experiments. By default, we use greedy decoding—i.e., sampling with temperature \( T=0 \)—due to its simplicity and deterministic behavior, making it well-suited for replication and ablation studies. In addition, we incorporate chain-of-thought (CoT) prompting \citep{wei2022chain}, a technique shown to significantly enhance performance on complex reasoning tasks.

\begin{figure}[h]
    \centering    \includegraphics[width=1.05\linewidth]{fig_new/prompt.png}
    \scriptsize
    \caption{An example task description ($\mathcal{A}$) prompt.}
    \label{fig:prompt}
\end{figure}

For all classification tasks, our full prompt template consists of three components—user, retriever (if RAG is used), and system—and is defined as follows:
\begin{align*}
\mathcal{P}^{\text{full}} = \text{prompt}(\mathcal{Q}^{\text{user}}(\mathcal{A}(\phi,c)), \mathcal{C}^{\text{retriever}}(k,\mathcal{R}(\phi,c)), \mathcal{H}^{\text{system}}), 
\end{align*}

where (i). $\mathcal{Q}^{\text{user}}$ stands for user query, which is comprised of $\mathcal{A}$, a task description prompt that takes features $\phi$ and categories $c$ as inputs; (ii). $\mathcal{C}^{\text{retriever}}$ represents the top $k$ retrieved contexts via a semantic similarity search of retrieval prompt $\mathcal{R}(\phi,c)$ with the retrieval knowledge base; and (iii). $\mathcal{H}^{\text{system}}$ summarizes past queries and responses, enacted through a conversational buffer. Under this framework, prompt engineering consists of three components: $\mathcal{A}$ (task description), and $\mathcal{R}$ (retrieval prompt). Component $\mathcal{A}$ follows the general structure in Figure \ref{fig:prompt}, where it is composed of a background description of the dataset, the assigned the task, and formatting instructions. We refer the readers to Appendix \ref{subsec:prompt_constr} for a more detailed description.

% , while this section focuses on our approach to constructing $\mathcal{E}$. In practice, the design of $\mathcal{E}$ is critical to the performance of the LLM. To develop an effective strategy, we investigate three methods for providing the LLM with an interpretation of the penalty term to enhance its performance: the Bayesian approach, the ReLU-form approach, and the adversarial approach.

% \paragraph{The Bayesian Approach for Prompt Construction.}
% The Lasso with penalty factors can be interpreted from a Bayesian perspective, where the penalty factor serves as the scaling parameter of a Laplace prior. A larger penalty factor results in a tighter distribution around zero, encouraging sparsity. Under this framework, the corresponding prompt is: 

% \begin{center}
% \noindent\fbox{%
%     \parbox{0.45\textwidth}{%
% I would like you to provide penalty factors greater than or equal to 0 to use on each coefficient of a Lasso estimator based on domain knowledge for a regression or classification task. Suppose \(\beta_k\) is the regression coefficient for feature \(k\). We interpret Lasso with penalty factors \(\lambda_k\) as yielding a maximum a posteriori estimate under Laplace priors with parameters \(\lambda_k\). 
% This means that, before observing the data, the ratio of log-tail probabilities
% $\log P(\|\beta_i\| > t) / \log P(\|\beta_j\| > t)$
% is equal to \(\lambda_i / \lambda_j\) for each \(i, j\) and for all \(t\). Therefore, the penalty factors represent relative log-tail probabilities of coefficients. For example, if feature \(A\) has a penalty factor of \(\lambda\) and feature \(B\) has a penalty factor of \(2\lambda\), this implies that the log-likelihood of the absolute value of the regression coefficient for \(A\) exceeding any threshold is twice that of \(B\). Thus, the larger the penalty factor for a coefficient, the less ``important" the coefficient is.

%     }%
% }
% \end{center}

% \paragraph{The ReLU-form Approach for Prompt Construction.} Another prompting framework for interpreting the penalty factor is to directly discribe the process which we code our underlying Lasso model with penalty using the ReLU-form penalty. We defer the prompt to Appendix \ref{subsec:prompt_constr}.

% \paragraph{The Adversarial Approach for Prompt Construction.} The penalty factor can also be interpreted as part of an adversarial game to enhance out-of-sample prediction robustness. Here, the penalty factor scales the cost of perturbing covariates under a weighted \( \ell_{\infty} \) norm. Larger penalty factors make changes to a covariate more ``expensive," limiting adversarial alterations, while smaller factors make them cheaper, reflecting lower importance. The adversary operates within a fixed budget, distributing total weights across covariates to balance importance and vulnerability. We defer the respective prompt to Appendix \ref{subsec:prompt_constr}.

\subsubsection{Knowledge-base Embedding via RAG}\label{subsec:knowledge}
We use the standard RAG pipeline to create an optional task-specific knowledge base embedding for our prediction task. 
We provide a brief overview of what RAG is and our specific pipeline.
RAG provides LLMs with informative contextual information by selectively choosing the relevant documents from a database. This is critical, as today's LLMs still struggle to handle very long contexts and generally cannot take an entire database as part of the input prompt.
We now describe the specific RAG pipeline we  use. 

\paragraph{Preprocessing (Embedding and Indexing)}
Given a knowledge base consist of $N$ text documents, $\{D_i\}_{i=1}^N$, we obtain their $d$-dimensional semantic embeddings $\{d_i\}_{i=1}^N = \{E(D_i)\}_{i=1}^N$ via an embedding function $E: \text{Text} \rightarrow \mathbb{R}^d$. 
Here, we use the \texttt{OpenAI} embeddings off-the-shelf \citep{openai_embeddings}. 
Once the semantic embedding vectors are obtained, we apply the the Hierarchical Navigable Small World (HNSW) algorithm \citep{malkov2018hnsw}, implemented in \texttt{chromadb}, to enable sublinear complexity for semantic similarity search.

\paragraph{Retrieval} 
% Given a query, 
% to the task in query by amplifying the user prompt with context $\mathcal{C}$. 

% This function maps each document to a $d$-dimensional embedding vector. 
% Using $E$, we construct a vectorized representation of the knowledge base, , and store these embeddings in a vector store database along with metadata to provide context for each document $d_i$.
At retrieval time, given a query vector $q \in \mathbb{R}^d$, the semantic similarity between $q$ and the stored embeddings $\{d_i\}_{i=1}^N$ is computed as $\text{Sim}(q,d_i) = \frac{q^T d_i}{\|q\|_2\|d_i\|_2}.$
The top $k$ documents with the highest similarity scores are retrieved and supplied as context $\mathcal{C}$.

Throughout the paper, we adhere to the following naming convention: \texttt{LLM-Lasso (Plain)} refers to a pipeline without RAG, while \texttt{LLM-Lasso (RAG)} denotes a pipeline incorporating RAG.
The performance of RAG in our framework highly depends on the retrieval prompt and the relevance of the retrieved documents. 
Figure \ref{fig:rag} illustrates \texttt{LLM-Lasso (RAG)}.
Due to space constraints, a detailed discussion is provided in Appendix \ref{appdx:imp}.

%To optimize this, we survey techniques such as query translation, active retrieval, as well as the construction of a specialized retrieval prompt $\mathcal{R}$ for similarity search, replacing the original query.


%MAYBE TODO
% \subsubsection{Memory-Retention} Sampling bias, etc.

% \subsubsection{Fine-tuning.}\label{subsec:finetune} Fine-tuning uses a pre-trained model, such as OpenAI’s GPT series, as a foundation. The process
% involves further training on a smaller, domain-specific dataset. This approach builds upon the model’s pre-existing knowledge, enhancing performance on specific tasks with reduced data and computational requirements. \textcolor{red}{[TODO.]}