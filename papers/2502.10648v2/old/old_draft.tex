\section{The L-LASSO}
We focus on the linear regression model, where, given a response variable \( y \) and a set of predictor variables \( x_1, \ldots, x_p \), the relationship is modeled as: 
\[
y = \beta_0 + \sum_{i=1}^{p} \beta_i x_i.
\]
In scenarios with a large number of predictors \( p \) relative to the number of observations \( N \)—a common situation in fields such as clinical research—shrinkage regression models like LASSO are commonly employed to perform feature selection and reduce overfitting in high-dimensional data. In this paper, we focus on the standard LASSO regression model, enhanced by distinct regularization penalties informed by a language model (LLM) to guide feature selection.

The LASSO estimator is defined as:
\begin{align}\label{eq:LASSO_est}
    \hat{\beta} = \arg\min_{\beta} \left\{ \|y - X \beta\|^2_2 + \sum_{i=1}^{p} \lambda^{\text{LLM}}_i |\beta_i| \right\},
\end{align}
where \( \{\lambda^{\text{LLM}}_i\}_{i=1}^{p} \) represents a set of LLM-informed penalty coefficients for each predictor \( \beta_i \), chosen based on importance scores derived from specific prompting techniques. These penalties help focus the model on features deemed more relevant by the LLM, potentially enhancing interpretability and performance.

The sum of squared errors (SSE) for model \eqref{eq:LASSO_est} can be expressed as:
\begin{align}\label{eq:LASSO_sse}
    \sum_{i=1}^{N} \left( y_i - \beta_0 - \sum_{j=1}^{p} \beta_j x_{ij} \right)^2 + \sum_{j=1}^{p} \lambda^{\text{LLM}}_j |\beta_j|.
\end{align}
% Define x and y.
% What about negative importance score?
\paragraph{LLM-Informed LASSO Penalty.} A critical challenge lies in defining an effective mapping, $\varphi : \mathbb{R}^{+} \rightarrow \mathbb{R}$, which translates the importance score $I_i$ provided by the LLM for the $i$-th feature into an LLM-informed penalty $\lambda^{\text{LLM}}_i = \varphi(I_i)$. This penalty should seamlessly integrate the LLM’s meta-knowledge with the inherent structure and data-driven insights of traditional LASSO, yielding a model that leverages both sources of expertise effectively.

\subsection{Alternative Formulation: Weighted-L-LASSO.} Another way to inform LASSO through LLM could be by weighting the weight matrix of the model. In the case of multi-class classification tasks, we can discuss with respect to two setups: (i). the multitask LASSO with squared error loss and (ii). multinomial logistic regression with logisitc loss. We continue our discussions of the following under the setup: we are given an $n$ by $p$ matrix $X$, with $n$ denoting the number of samples and $p$ the number of features, a target matrix $Y$ that is $n$ by $c$, where $c$ is the total number of class that we are asked to classify. Our goal is to come up with an effective predictor matrix $W$ of shape $p$ by $c$. 

\paragraph{Weighted Multi-task L-LASSO.} In multitask LASSO, we handle multiple related tasks by fitting a separate regression model for each task while promoting sparsity across tasks. This setup minimizes the squared error loss across all tasks, with an \( \ell_{1,2} \)-norm (Definition \ref{def:norm12}) regularization applied to the weight matrix \( W \). Specifically, we minimize:
     \[
     \min_{W} \frac{1}{2n} \| Y - X W \|_F^2 + \lambda \| W \|_{1,2}
     \]
     where \( Y \) is the target matrix with one column per task, \( X \) is the feature matrix, and \( W \) is the weight matrix. The \( \ell_{1,2} \)-norm regularization promotes shared sparsity, encouraging some features to have zero weights across all tasks.

     To incorporate feature importance from an LLM, we can adjust the model weight matrix \( W \) with an importance matrix \( P \), which is $p$ by $c$, obtained by prompting the LLM. This importance matrix \( P \) contains feature importance scores for each class, reflecting the relevance of each feature for each class. We then define a \textit{Weighted-L-LASSO} model by applying an entrywise product between \( W \) and \( P \), effectively scaling the weights by feature importance.

     In multitask LASSO, this adjustment modifies the squared error loss as follows:
     \[
     \min_{W} \frac{1}{2n} \| Y - XW \|_F^2 + \lambda \| W \circ P \|_{1,2}
     \]
     where \( W \circ P \) represents the elementwise product of \( W \) with \( P \). This scaling allows us to prioritize features differently across tasks based on their importance scores.
     
\paragraph{Weighted Multinomial L-LASSO.}
   In the general multinomial logistic regression with LASSO, we adapt this approach for multi-class classification, where the goal is to predict a discrete label across several classes. Instead of squared error loss, we use logistic loss to model the probability of each class and add an \( \ell_{1,2} \)-norm regularization to encourage sparsity across the class weight matrix. The objective function for multinomial logistic regression is:
     \begin{align}
     \begin{split}
     \min_{W} &-\frac{1}{n} \sum_{i=1}^n \sum_{j=1}^c Y_{ij} \log \left( \frac{\exp((X W)_{ij})}{\sum_{k=1}^c \exp((X W)_{ik})} \right) \\ &+ \lambda \| W \|_{1,2}
     \end{split}
     \end{align}
     where \( Y \) is a one-hot encoded matrix representing the classes, and \( W \) is the weight matrix for each class. The regularization promotes feature sparsity across all classes.

     Similar to othe weighted multi-task L-LASSO, we can weight the weight matrix in the multinomial L-LASSO using the importance matrix $I$ in the same manner by similarly adjusting the logistic loss term:
     \begin{align}
     \begin{split}
     \min_{W} &-\frac{1}{n} \sum_{i=1}^n \sum_{j=1}^c Y_{ij} \log \left( \frac{\exp((XW)_{ij})}{\sum_{k=1}^c \exp((XW)_{ik})} \right) \\ &+ \lambda \|W \circ P\|_{1,2}
     \end{split}
     \end{align}
     where the importance matrix \( P \) weights the contributions of each feature for each class. By modifying \( W \) in this way, the Weighted-L-LASSO model incorporates the LLM-derived feature importance into the regularization framework, improving feature selection based on domain-specific knowledge and potentially enhancing model interpretability.

     \paragraph{A multinomial variant: one-versus-all.} One variant we can potentially look at in addition to the general multi-class LASSO is the one-versus-all binary logisitc regression setup. 