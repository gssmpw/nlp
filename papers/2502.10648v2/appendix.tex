% \section{Key Definitions}
% \begin{definition}[The \( \ell_{1,2} \)-norm] \label{def:norm12}
%     The \( \ell_{1,2} \)-norm of a weight matrix \( W \in \mathbb{R}^{p \times c} \), where \( p \) is the number of features and \( c \) is the number of tasks, is defined as:

% \[
% \| W \|_{1,2} = \sum_{i=1}^p \| W_{i,:} \|_2
% \]

% where \( W_{i,:} \) represents the \( i \)-th row of \( W \), and \( \| W_{i,:} \|_2 \) is the \( \ell_2 \)-norm of this row, calculated as:

% \[
% \| W_{i,:} \|_2 = \sqrt{\sum_{j=1}^c W_{ij}^2}
% \]

% Thus, the \( \ell_{1,2} \)-norm is the sum of the \( \ell_2 \)-norms of each row of \( W \).

% \end{definition}

\begin{figure*}[htbp]
    \centering
    \includegraphics[width=0.8\linewidth]{fig/simulations_test_error.png}
    \caption{ Test error in simulations}
    \label{fig:simulations_test_error}
\end{figure*}

\begin{figure*}[htbp]
    \centering
    \includegraphics[width=0.8\linewidth]{fig/simulations_AUROC.png}
    \caption{AUROC in simulations}
    \label{fig:simulations_AUROC}
\end{figure*}
\section{Penalty Factor Simulations Details}\label{appdx:sim}
We run simulations to find the adequate form of penalty factors. Based on the simulations, we use the inverse importance penalty factors to compare the LLM-Lasso to the baseline models. The details can be found in \ref{appdx:sim}. using datasets outlined in Table \ref{tab:simulation_data}. The data are split into the importance score-generating set and the cross-validation set. The hypothetical importance scores are generated by running a Lasso regression on the score-generating set and assigning the absolute values of the coefficient of each feature is assigned to be the score of that feature. Then, the hypothetical importance scores are scaled so that the maximum score was 1 and the minimum score is 0.1. On the cross-validation set, we run the hypothetical LLM-Lasso using different forms of penalty factors: (i). the inverse of the importance scores and their powers and (ii). ReLU penalty factors with different thresholds. For ReLU penalty factors, we set the maximum penalty factor such that the least important feature received a coefficient of 0 for all values of $\lambda$. We perform 5-fold cross-validation across the hyperparameter $\gamma \in (0.1, 0.2, \ldots, 0.9)$ for the ReLU penalty factors and $\eta \in (0, 1, \ldots, 10)$ for penalty factors of the form $\mathcal{I}^{-\eta}$. We obtain cross-validation misclassification rates across the spectrum of regularization parameters $\lambda$ with a $\lambda_\text{min}$ to $\lambda_\text{max}$ ratio of 0.01. Cross-validation is performed such that the difference in the area of the plot of the misclassification rate across numbers of features with respect to the Lasso is maximized. The best misclassification rate is obtained for each number of features selected. We perform the above for 10 data splits and plot the mean of the best misclassification rate for each number of features. The above procedure is repeated for areas under the receiver-operating characteristic curve (AUROCs) as the cross-validation metric. \begin{table}[h!]
\small
\centering
\begin{tabular}{lrr}
\toprule
\textbf{Dataset} & \textbf{n} & \textbf{p} \\
\midrule
Cancer microarray \cite{ramaswamy2001multiclass}  & 52 & 1000 \\
Small-round-blue-cell tumor \cite{khan2001classification} & 83 & 1000 \\
Lung cancer \cite{spira2007airway} & 187 & 1000 \\
\bottomrule
\end{tabular}
\caption{Summary of simulation datasets for penalty factor form.}
\label{tab:simulation_data}
\end{table}
 
The datasets we use in the simulations are summarized in Table \ref{tab:simulation_data}. In the simulations, we perform the task of classifying samples into tumor tissue or healthy tissue (lung cancer dataset) or cancer subtypes (cancer microarray dataset, rhabdomyosarcoma vs others; small-round-blue-cell tumor (SRBCT) dataset, lymphoma vs leukemia) using gene expression levels. We select features with the top 1000 variances as predictors. The simulation show an advantage of the inverse importance penalty factors over the ReLU penalty factors, as well as compared to the Lasso (Figures \ref{fig:simulations_test_error} and  \ref{fig:simulations_AUROC}). Thus, in the experiments, we use the inverse importance penalty factors to compare the LLM-Lasso to baseline models.

\section{Prompt Construction}\label{subsec:prompt_constr} 

Not only is prompting shown to be significant to the performance of LLMs
Throughout our experiment with the biomedical dataset, we set the system message to the generation LLM as ``assistant," with instruction: ``you are an expert assistant with access to gene and cancer knowledge."

As we recall in Section \ref{subsec:prompt} that our full prompt follows the following structure: 
\begin{align*}
\mathcal{P}^{\text{full}} = \text{prompt}(\mathcal{Q}^{\text{user}}(\mathcal{A}(\phi,c)), \mathcal{C}^{\text{retriever}}(k,\mathcal{R}(\phi,c)), \mathcal{H}^{\text{system}}). 
\end{align*}
The design choice for the user therefore primarily resides in (i). the construction of the task description prompt $\mathcal{A}(\phi,c)$ and (ii). the construction of a customized retrieval prompt $\mathcal{R}(\phi,c)$ in the case when RAG is used. 
\subsubsection{Task Description} The general format of text description follows Figure \ref{fig:prompt}. However, there are many ways one can format each of the three sections, that is, background description, a task description, and formatting rules. In the following, we go through each component in depth.
\paragraph{Background Description.}
We include the following key elements in our background description prompt:
\begin{itemize}
    \item Meta-data of the dataset. This includes details on how the data is collected, number of samples, and number of features.
    \item User Intention. This includes a description of our goal for data analysis. For conducting classification experiments using LLM-Lasso, for instance, we remark: \textit{``We wish to build a statstical (Lasso) model that classifies samples into category diffuse large B-cell lymphoma (DLBCL) and follicular lymphoma (FL)."}
\end{itemize}
\paragraph{Task Description.} The task section specifies the exact request made to the generation LLM. For LLM-Lasso, this involves a description of the penalty factors. As penalty factors can be less intuitive to understand than the straight-forward importance scores,
through our experiments, we experimented with a number of prompts to describe to the LLM the meaning of ``penalty factors" in an effort to boost prediction performance by facilitating understanding. To this end, we employ four prompting strategies—bayesian, ReLU, adversarial, and \texttt{o1-}generated —to guide the interpretation of the penalty factors. We found that this part of the prompt has a direct and considerable impact on the predictive ability of the LLM. 
% \begin{itemize}
%     \item \textit{Task Description.} This includes a detailed description of the task and objectives, along with key information about the dataset, such as the number of samples and features. 
%     \item \textit{Output Format Instructions.} To facilitate streamlined score scraping, we provide explicit output format instructions for LLMs. We observed that smaller LLMs often struggle to adhere to these instructions. In such cases, it is beneficial to:
%     (i). make the instructions exceptionally clear; (ii). use a few-shot examples to illustrate the desired response format; (iii). adopt a firmer tone to emphasize the importance of strictly following the instructions.
% \end{itemize} 
We discuss each of the three component in detail in the following.

\paragraph{I. The Bayesian Approach for Prompt Construction.}
The Lasso with penalty factors can be interpreted from a Bayesian perspective, where the penalty factor serves as the scaling parameter of a Laplace prior. A larger penalty factor results in a tighter distribution around zero, encouraging sparsity. Under this framework, the corresponding prompt for the oncology prediction task is: 

\begin{center}
\noindent\fbox{%
    \parbox{0.45\textwidth}{%
I would like you to provide penalty factors greater than or equal to 0 to use on each coefficient of a Lasso estimator based on domain knowledge for a regression or classification task. Suppose \(\beta_k\) is the regression coefficient for feature \(k\). We interpret Lasso with penalty factors \(\lambda_k\) as yielding a maximum a posteriori estimate under Laplace priors with parameters \(\lambda_k\). 
This means that, before observing the data, the ratio of log-tail probabilities
$\log P(\|\beta_i\| > t) / \log P(\|\beta_j\| > t)$
is equal to \(\lambda_i / \lambda_j\) for each \(i, j\) and for all \(t\). Therefore, the penalty factors represent relative log-tail probabilities of coefficients. For example, if feature \(A\) has a penalty factor of \(\lambda\) and feature \(B\) has a penalty factor of \(2\lambda\), this implies that the log-likelihood of the absolute value of the regression coefficient for \(A\) exceeding any threshold is twice that of \(B\). Thus, the larger the penalty factor for a coefficient, the less ``important" the coefficient is.

    }%
}
\end{center}

\paragraph{II. The ReLU-form Approach for Prompt Construction.} Another prompting framework for interpreting the penalty factor is to directly discribe the process which we code our underlying Lasso model with penalty using the ReLU-form penalty.

\begin{center}
\noindent\fbox{%
    \parbox{0.45\textwidth}{%
We plan to use your scores with a Lasso-regularized multinomial classifier, implemented via the R package \texttt{glmnet}. The scores will generate penalty factors (weights on the $\ell_1$ norm), which will be used in \texttt{glmnet}. Higher importance genes will be assigned smaller penalty factors, while lower importance genes will receive larger penalty factors.

Let \texttt{xall} denote the feature matrix (number of observations by number of genes) and \texttt{yall} the multinomial class outcome. Similarly, let \texttt{xtest} and \texttt{ytest} be the test set feature matrix and class outcome, respectively.

Let \texttt{scores} be the $p$-vector of gene importance scores provided by ChatGPT.

The details of our plan are implemented in the following R code: [omitted]
    }%
}
\end{center}

\paragraph{III. The Adversarial Approach for Prompt Construction.} The penalty factor can also be interpreted as part of an adversarial game to enhance out-of-sample prediction robustness. Here, the penalty factor scales the cost of perturbing covariates under a weighted \( \ell_{\infty} \) norm. Larger penalty factors make changes to a covariate more ``expensive," limiting adversarial alterations, while smaller factors make them cheaper, reflecting lower importance. The adversary operates within a fixed budget, distributing total weights across covariates to balance importance and vulnerability. 

\begin{center}
\noindent\fbox{%
    \parbox{0.45\textwidth}{%
You are tasked with helping perform a what-if (adversarial) analysis to improve out-of-sample prediction on a logistic regression model for classification. Here is how this analysis works: (1). For each sample, every covariate (gene expression level) can be modified (increased or decreased), but the cost of changing each covariate is scaled by a weight that we assign now. (2). The “size” of a change to a single sample is measured by the weighted \( L_{\infty} \) norm: if \( \delta_i \) is the change to covariate \( i \), and \( w_i \) is the weight for covariate \( i \), then the size of the change is: $\max_i(|\delta_i| \times w_i).$ Across the dataset, the average of these sizes is constrained by a fixed budget. (3). A larger weight on covariate \( i \) makes changes to that covariate more “expensive” to the adversary, limiting how drastically it can be altered under the same overall budget. A smaller weight makes it cheaper to perturb that feature, which might be acceptable if the gene is less important. (4). You must distribute a total of 100 weight units among all covariates: $\sum_i w_i = 100.$
Given this setup, your job is to choose weights for each predictor. Your goal is to provide a plausible weighting scheme that balances the importance of each predictor against potential adversarial changes.
    }%
}
\end{center}
\paragraph{IV. o1-Generated Approach for Prompt Construction.} In addition to using different theoretical angles to explain to the generation LLM the notion of penalty factors, we consult advanced LLMs, such as o1 from \texttt{OpenAI} for their advice on constructing a prompt to explain the penalty factors in a way that would be most conducive for an LLM to perform:
\begin{center}
\noindent\fbox{%
    \parbox{0.45\textwidth}{%
Provide penalty factors for each of the genes. These penalty factors should be integers between 2 and 5 (inclusive), where:
2 indicates a gene strongly associated with ``{category}" (i.e., it should be penalized the least by Lasso).
5 indicates a gene with minimal relevance to ``{category}" (i.e., it should be penalized the most by Lasso).
    }%
}
\end{center}
We note that the range $(2,5)$ is arbitrary and can be chosen by the user. It is noticeably that the o1-generated prompt is significantly shorter and simpler than the other three approaches and focuses on direct instructions (i.e more important features should be penalized more) using use-case examples rather than attempting to explain the intuition behind the penalty factors. While the o1-generated prompt seems to introduce no in-depth understanding of penalty factors that the shallow level, empirically, we find that this prompt consistently encourages better prediction performance across a range of LLMs from simple to advanced on penatly factor production for feature selection. Overall, our empirical findings suggest that: o1-generated prompt $>$ Bayesian prompt $>$ ReLU prompt $>$ Adversarial prompt, where we use a descending order of performance.

\paragraph{Output Format Instructions.}
Our experiments revealed that selecting appropriate output format instructions is crucial not only for the accuracy of the score collection process but also for maintaining the quality of the scores produced. This is especially important for smaller models with fewer parameters (e.g., \texttt{llama-3-8b-instruct}), which often struggle to follow prompt instructions and understand the concepts and guidance provided.

In practice, we found that directly using text responses and providing LLMs with clear text formatting rules is more effective in regulating their behavior and ensuring a smooth score collection process compared to requesting responses in raw \texttt{JSON} format, as commonly used in \texttt{LangChain}'s pipeline. For all LLMs, we attach a format instruction to the end of every prompt, with slight modifications tailored to the specific task. Below is an example of a format instruction used for the task of outputting penalty factors for gene selection in cancer or lymphoma prediction.

\noindent\adjustbox{margin=0.3em, fbox}{%
    \parbox{0.45\textwidth}{%
        Formatting Rules:
        \begin{enumerate}
            \item Score Representation: Use a direct floating-point number (e.g., 0.5). Avoid scientific notation (e.g., 10**(-2) or 1e-2) and additional formatting.
            \item Include All Genes: Assign a penalty factor for every gene in the input list, preserving the order of input.
            \item Reasoning: After each penalty factor, add a concise reasoning about the gene’s role in predicting \{category\}.
            \item Consistency: Ensure uniform formatting. Example:
            \begin{quote}
            \texttt{AASS: 1} \\
            Reasoning: This gene is highly expressed in cancer pathways and has been associated with \{category\}. Assigned a low penalty factor. \\
            \texttt{BRCA1: 5} \\
            Reasoning: BRCA1 is not significantly relevant for \{category\}. Assigned a high penalty factor.
            \end{quote}
        \end{enumerate}
        Do not include disclaimers about lacking full data; rely on general cancer genomics and pathway relevance.
    }%
}
\vspace{0.5cm}

As outlined in the formatting prompt, three strategies were found to be particularly effective:
\begin{enumerate}
\item Highlighting common errors: We include a list of frequent formatting mistakes made by LLMs, identified through trial and error. These include, for example, using scientific notation instead of floating-point numbers, which complicates the score collection algorithm, and applying inconsistent additional formatting to the scores.
\item Providing examples: Examples demonstrating the desired score and explanation format significantly improve the LLMs' understanding of the task. This is particularly important when querying for penalty factors instead of importance scores. While a dedicated prompt explains the concept of penalty factors, smaller models like \texttt{llama-3-8b-instruct} often struggle with the counterintuitive nature of penalty scores—where lower values indicate higher significance and vice versa. Including examples of both low and high penalty scores helps address this challenge and ensures better compliance. 
\item Using a firm tone: We employ strict language to enforce adherence to the rules. Commands such as ``Do not say that it’s not possible to compute precise penalty factors without access to the actual gene expression values" and ``Responses not following these guidelines will be considered invalid" have proven effective in ensuring LLMs behave consistently and follow the guidance provided.
\end{enumerate}

Figure \ref{fig:full_prompt} is an example of the full user prompt used in the study of classifying patients into DLBCL and FL, which employs o1-generated explanation of penalty factors. 

\begin{figure}[h]
    \centering    \includegraphics[width=1\linewidth]{fig_new/full_prompt.png}
    \caption{Example of a full user prompt for experiment study DLBCL vs FL.}
    \label{fig:full_prompt}
\end{figure}

\subsubsection{Retrieval Prompt}
The default pipeline in \texttt{Langchain} for retrieval query is to directly perform semantic similarity search on the user's original prompt to the generation LLM. This becomes problematic, however, when the main user prompt is large and overshadows the important information that sheds light on what documents should be retrieved. As an example, when passing in directly the full user prompt for retrieval in the high-dimensional oncology classification tasks, semantic similarity search sometimes retrieves information on the description of the dataset, for example, contexts regarding cfDNA fragmentation pattern and EPIC-Seq, instead of what we are actually curious about, that is, the relevance of certain gene, say AASS, with classifying lymphoma subtypes, say, diffuse large B-cell lymphoma (DLBCL) and follicular lymphoma (FL). 

In order to pinpoint the retriever to the specific retrieval documents that are actually relevant to the task at hand, we use a customized retrieval prompt. For the lymphoma classification tasks using micro-array gene data, due to the high-dimensional nature of the dataset, we batch process the genes (see Appendix \ref{sec:exp_supp} for discussion) and use the following prompt that takes each gene $g_i \in \{g_1,...,g_B\}$ in each batch of size $B$ and the target classification category $c$:

\noindent\adjustbox{margin=0.3em, fbox}{%
    \parbox{0.45\textwidth}{%
Retrieve information about gene \{g\}, category \{c\}, especially in the context of \{g\}’s relevance to \{c\}.
    }%
}
\vspace{1cm}

An example prompt using this format is show in Figure \ref{fig:retrieval_prompt}.

\begin{figure}[h]
    \centering    \includegraphics[width=1\linewidth]{fig_new/retrieval_prompt.png}
    \caption{Example of a completed retrieval prompt.}
    \label{fig:retrieval_prompt}
\end{figure}

For each pass of retrieval search with gene and lymphoma pair, we retrieve top $k$ relevant documents. After collecting the contexts for all the genes in the batch, we then filter for the unique documents and then append them to the full prompt in prompt component $\mathcal{C}$. It turns out that the specific implementation of this procedure is an art: we want to strike a balance between overwhelming the generation LLM with long-context and potentially minimally informative documents and excessive cautious retrieval that does not inform the LLM by much. 

\section{Experiment Details}\label{appdx:exp}
% \subsection{Baseline Details}
% We give a more detailed introduction to each baseline feature selection methods we surveyed. 
% \subsubsection{LLM-Scores} LLM-Scores is part of the three proposed text-based LLM feature selector by \cite{jeong2024llmselectfeatureselectionlarge}.



% Table \ref{tab:genetics-literature} shows whether genes with high feature contributions in the FL vs DLBCL and MCL vs DLBCL experiments are included in previous literature in cancer genetics. 

% \begin{table*}[htbp] 
% \small
% \centering 
% \begin{tabular}{|l|c|c|c|c|} 
% \hline 
% \textbf{Gene Name (FL)} & \cite{valvert2021low} & \cite{zhang2022novel} & \cite{bobee2020combining} \\ 
% \hline BACH2 & & & \\ 
% \hline AICDA & & & \\ 
% \hline PLCG2 & & & \\ 
% \hline HLA-DQA1 & & & \\ 
% \hline SIGLEC6 & & \checkmark & \\ 
% \hline FOXP1 & \checkmark & & \checkmark \\ 
% \hline FDCSP & & & \\ 
% \hline POLO & & & \\ 
% \hline GPR183 & & & \\ 
% \hline SERPINA9 & & & \checkmark \\ 
% \hline IL9 & & & \\ 
% \hline CD22 & & & \checkmark \\ 
% \hline GAS1 & & & \\ 
% \hline HHLA1 & & & \\ 
% \hline HSPA12A & & & \\ 
% \hline IL7 & & & \\ 
% \hline CCL19 & & & \\ 
% \hline S100A7L2 & & & \\ 
% \hline ANKFN1 & & & \\ 
% \hline TPSAB1 & & & \\ 
% \hline NDN & & & \\ 
% \hline CD1E & & & \\ 
% \hline OR10X1 & & & \\ 
% \hline FOXM1 & & & \\ 
% \hline CD3D & & & \checkmark \\ 
% \hline CKS1B & & & \\ 
% \hline CCN2 & & & \\ 
% \hline CD1B & & & \\ 
% \hline \textbf{Gene Name (MCL)} & \cite{valvert2021low} & \cite{zhang2022novel} & \cite{bobee2020combining} \\ 
% \hline KMT2D & & & \\ 
% \hline AICDA & & & \\ 
% \hline FOXP1 & \checkmark & & \checkmark \\ 
% \hline BCL6 & \checkmark & & \checkmark \\
% \hline PDPN & & & \\ 
% \hline RGS1 & & & \\ 
% \hline SIGLEC6 & & \checkmark & \\ 
% \hline CD44 & \checkmark & & \\
% \hline KIT & & & \\
% \hline PRDM1 & & & \checkmark \\
% \hline IL7 & & & \\ 
% \hline ROR1 & & & \\
% \hline ICOSLG & & & \\
% \hline KCNH8 & & & \\
% \hline TNFRSF8 (CD30) & \checkmark & & \checkmark \\
% \hline AHR & & & \\
% \hline CKS1B & & & \\ 
% \hline XCR1 & & & \\
% \hline MPZ & & & \\
% \hline ITGB1 & & & \\
% \hline SNORAS52 & & & \\
% \hline CD52 & & & \\
% \hline RPS6KL1 & & & \\
% \hline DNAH6 & & & \\
% \hline LIMS1 & & & \\
% \hline 
% \end{tabular} 
% \caption{Genes with high features contributions in the FL vs DLBCL and MCL vs DLBCL experiments included in previous literature} 
% \label{tab:genetics-literature}
% \end{table*}



\subsection{Model Details}\label{subsec:model_details}
We provide more details of the LLMs sampled. Table \ref{tab:cutoff} summarizes the cut-off dates in each LLMs in used.

\begin{table*}[!ht]
\centering
\begin{tabular}{l l l l}
\toprule
\textbf{Model Name} & \textbf{Company} & \textbf{Cut-off Date} & \textbf{Source} \\
\midrule
GPT-3.5 (Turbo) & OpenAI & 2021.09 & \href{https://platform.openai.com/docs/models#gpt-3-5-turbo}{Source} \\
GPT-4o (2024-08-06) & OpenAI & 2023.10 & \href{https://platform.openai.com/docs/models\#gpt-4o}{Source} \\
o1 & OpenAI & 2023.10 & \href{https://platform.openai.com/docs/models\#o1}{Source} \\
Llama-3-8B & Meta & 2023.12 & \href{https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct}{Source} \\
Llama-3.1-405B & Meta & 2023.12 & \href{https://huggingface.co/meta-llama/Llama-3.1-405B-Instruct}{Source} \\
DeepSeek-R1 & DeepSeek & 2024.07 & \href{https://www.deepseek.com/}{Source} \\
Qwen Models-72B & Alibaba & 2023.09 & \href{https://huggingface.co/Qwen/Qwen2-72B}{Source} \\
\bottomrule
\end{tabular}
\caption{Surveyed LLMs Cutoff Dates Overview}
\label{tab:cutoff}
\end{table*}

\subsection{Dataset Details}\label{subapp:dataset-details}
In this section, we give more details on the datasets used in the Experiment Section.
\subsubsection{Small-scale Experiment Datasets}\label{subsec:small_scale_data}
We source a wide range of small-scale datasets for feature selection in classification and regression. We use * to indicate the datasets that are released after the cutoff dates for all models sampled (see Table \ref{tab:cutoff} for an overview of the model cutoff dates). For all small-scale datasets, we remove features whose values are not numerical and not categorical and remove rows and columns with missing values. We remark that the purpose of the small-scale experiment is not meant to demonstrate performance on the specific task but rather to show case the ability of the feature selector candidate, even in the absence of some potentially informative features and data.

Figure \ref{fig:spotify} describes the model ablation study on the spotify regression dataset for feature selection. As we can see, the GPT-4o model we used in Figure \ref{fig:small_scale} is not the top performing model, yet we still outperformed the sampled feature selection baselines. This demonstrates once again the strength of our method. In addition, as we have seen in Figure \ref{fig:experiments_test_error}, Deepseek r1 continues to dominate in performance against other models and smaller models such as LlaMa-3-8b-instruct and GPT-3.5 has worse performance comparing to their more advanced model counterparts.

\begin{figure}[h]
    \centering    \includegraphics[width=1\linewidth]{fig_new/spotifymodel.png}
    \caption{Model ablation on the Spotify dataset with test error rate computed at $~25\%$ of total features.}
    \label{fig:spotify}
\end{figure}

\subsubsection{Large-scale Experiment Datasets}\label{subsec:large_scale_data}
The datasets used in the large-scale experiments are outlined in Table \ref{tab:data}, where $n,p$ denotes resp. sample size and number of features. We note that the dataset has not been published and is currently confidential. Therefore, we do not disclose further details. 

\begin{table}[h!]
\small
\centering
\begin{tabular}{lrr}
\toprule
\textbf{Dataset} & \textbf{n} & \textbf{p} \\
\midrule
Lymphoma (FL vs DLBCL) & 130 & 1592 \\ Lymphoma (MCL vs DLBCL) & 161 & 1592 \\ Lymphoma (cHL vs DLBCL) & 196 & 1592 \\
% ETP T-ALL \cite{liu2017genomic} & 189 & 1000 \\
\bottomrule
\end{tabular}
\caption{Summary of large-scale experiment datasets.}
\label{tab:data}
\end{table}

\subsection{Supplemental Experiment Results}


\subsubsection{Feature contributions}\label{subsec:feat_cont}
Figure \ref{fig:feature_contribution} shows heatmaps of feature contributions in the FL vs DLBCL experiment for GPT-4o and o1 RAG LLM-Lasso.
Although the o1 model performs very well in terms of misclassification error and AUROC (Figure \ref{fig:experiments_test_error}), the genes selected by GPT-4o may align more with the oncology literature, as per the discussion in the Feature Contribution paragraph of Section \ref{sec:evaluation}.

\begin{figure}[h!]
    \centering
    % \includegraphics[width=0.8\linewidth]{fig/feature_contribution.png}
    \includegraphics[width=0.9\linewidth]{fig_new/feature-contribution-DLBCL_FL-model-comparison.png}
    \caption{Heatmaps of feature contributions in the FL vs DLBCL experiment for GPT-4o and o1 LLM-Lasso, with and without RAG.}
    \label{fig:feature_contribution}
\end{figure}

% distinguish between concepts and features
\begin{table}[!ht]
\centering
\begin{tabular}{l l l l l}
\toprule
\textbf{Dataset} & \textbf{Year} & \textbf{n} & \textbf{p} & \textbf{Source} \\
\midrule
$\text{Spotify}^*$ & 2024 & 4600 & 29 & \href{https://www.kaggle.com/datasets/nelgiriyewithana/most-streamed-spotify-songs-2024/data}{Source} \\
$\text{Wine}$ & 2009 & 6497 & 11 & \href{https://www.kaggle.com/datasets/yasserh/wine-quality-dataset}{Source} \\
$\text{Diabetes}$ & 1998 & 768 & 8 & \href{https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database}{Source} \\
$\text{Bank}$ & 2012 & 45211 & 51 & \href{https://archive.ics.uci.edu/dataset/222/bank+marketing}{Source} \\
$\text{Glioma}$ & 2022 & 839 & 23 & \href{https://archive.ics.uci.edu/dataset/759/glioma+grading+clinical+and+mutation+features+dataset}{Source}\\
\midrule
\end{tabular}
\caption{Summary of small-scale experiment datasets.}
\label{tab:small_data}
\end{table}

\subsubsection{Deferred Plots}\label{subsec:deferred_plot}
In this subsection, we present the deferred plots in the large-scale experiment section. Figure \ref{fig:auroc_large_scale} illustrates the AUROC performance of our model against various baseline across the three high-dimensional lymphomal datasets. It is evident that the strong performance demonstrated by Figure \ref{fig:experiments_test_error} carries over to the AUROC metric.
\begin{figure*}[h!]
    \centering
    % \includegraphics[width=0.8\linewidth]{fig/feature_contribution.png}
    \includegraphics[width=1\linewidth]{fig_new/AUROC_large_scale.png}
    \caption{AUROC performance across 10 splits for the lymphoma datasets.}
    \label{fig:auroc_large_scale}
\end{figure*}


\section{Implementation Details}\label{appdx:imp}

\subsection{Handling Token Limits in LLMs}\label{subsec:token}
Input and output token limits in closed-source pretrained LLMs pose significant challenges, especially when querying large sets of predictors, as they restrict the information processed or returned in a single interaction. The absence of memory retention further complicates output aggregation. This limitation affects both closed-source GPT models via the OpenAI API and cloud-hosted open-source models, which also lack persistent memory.

However, ensuring that an LLM has sufficient output tokens is critical for its performance. For instance, \cite{wei2022chain} showed that step-by-step reasoning improves effectiveness, and we observe that LLMs struggle under tight token limits or when limits are exceeded (see Appendix \ref{appdx:imp}). To handle large feature sizes, batch-querying with an appropriate batch size is necessary to stay within token limits. However, this approach introduces challenges: without memory retention, the LLM cannot access previously processed features or their scores, leading to inconsistencies when aggregating batch results and potential scale mismatches. To address these issues and ensure accurate feature diagnoses while preserving essential output tokens, we propose two strategies that require no fine-tuning or parameter modifications.

\paragraph{Text-based Summary.} A straightforward approach to address this challenge is to batch-query the features while enabling memory retention in the LLM by augmenting the user query $\mathcal{Q}^{\text{user}}$ with a summarization of chat history $\mathcal{H}^{\text{system}}$, stored in a conversation buffer constrained by the max token limit. Several open-source \texttt{Python} packages support this functionality. In our implementation, we use \texttt{LangChain}'s \texttt{ConversationBufferMemory}. While not ideal for score-collection scenarios—since summarization often omits full scores and context due to token constraints—we find that including memory increases the likelihood of the LLM assigning scores on a consistent scale and provides marginal improvements in prediction performance (See Appendix \ref{appdx:imp} for more details). 

% The following was not explored yet
\paragraph{Statistical Estimation.} Another approach to address this problem is to use statistical techniques to infer the true score from batch scores without injecting memory into each batch. We introduce the following method. We note that to balance batch size and the number of queries, we heuristically select a batch size of $\lceil \sqrt{p} \rceil$, where $p$ is the total number of features and $\lceil \cdot\rceil$ is the ceiling notation.
%We explore two methods: scaling and regression. To start, one can formalize the problem as the following [TODO].  For both methods, to balance batch size and the number of queries, we heuristically select a batch size of $\lceil \sqrt{p} \rceil$, where $p$ is the total number of features. 

\textit{Scaling.} Given batch scores $B_1(s), \dots, B_{\lceil \sqrt{p} \rceil}(s)$, the scaling method involves selecting the maximum score from each batch, $s_{\text{max},1}, \dots, s_{\text{max},\lceil \sqrt{p} \rceil}$, and passing these maximum scores as a new batch to the LLM for rescoring, yielding $\tilde{s}_{\text{max},1}, \dots, \tilde{s}_{\text{max},\lceil \sqrt{p} \rceil}$. The final score is then computed by weighting and concatenating the batch scores. Specifically, each batch $B_i$ is weighted by $\frac{\tilde{s}_{\text{max,}i}}{\sum{j=1}^{\lceil \sqrt{p} \rceil} \tilde{s}_{\text{max,}j}}$, which is the normalized rescored maximum candidate from that batch relative to the rescored maximum candidates across all batches. 

% \textit{Regression.} [TODO]

\subsection{Score Collection}\label{subsec:score_collection}
LLMs, especially smaller models can make formatting mistakes. For instance, they may not including all necessary genes, or may include extra genes (e.g., ones mentioned in retrieval context) as part of the genes to score.
For OpenAI models, we use structured outputs to directly receive the scores as a \texttt{Python} object.
In models where this streamlined score collection feature is not available, we rely on the output formatting from the prompt (see Appendix \ref{subsec:prompt_constr}) and search for floating point scores that immediately flow the double-asterisk-colon sign (with or without space).
If we fail to collect scores, we retry until the correct scores for the batch are collected.

\subsection{\texttt{R} implementation}\label{appdx:r-impl}
Once the importance scores are obtained from the LLM, the LLM-Lasso can be implemented in \texttt{R} \cite{Rpackage}. One can pass the penalty factors, transformed into the form of choice, such as the ReLU-form or inverse importance and their powers ($\mathcal{I}^{-\eta}$), into the \texttt{cv.glmnet} function in the package \texttt{glmnet} \cite{glmnet}. The penalty factors can be passed into an argument called \texttt{penalty.factor}, which specifies the penalty factors to be assigned to each feature. 

\subsection{\texttt{Python} Implementation}\label{app:python-implementation}
The full end-to-end pipeline of LLM-Lasso is implemented in \texttt{Python}.
The score collection is done via \texttt{OpenAI} APIs for GPT models and o1, and via \texttt{OpenRouter} otherwise.
\texttt{Langchain} is used for the retrieval component of RAG.

For computing the data-driven baseline metrics (such as mutual information and MRMR), we first produce a set of randomly generated $50/50$ train and test splits and save them to CSV files.
These splits are used for both LLM-Lasso and each data-driven baseline.
Then, baseline scores can be computed via our \texttt{Python} implementations, relying on \texttt{scikit-learn}.

The implementation of the LLM-Lasso model, given the importance scores, is based on the package \texttt{adelie} \cite{yang2024fastscalablepathwisesolvergroup}.
We have a custom fork of \texttt{adelie} that adds AUROC and misclassification error metrics to the output of \texttt{adelie.cv.cv\_grpnet}.
For penalty factors, we consider powers of the inverse importance ($\mathcal{I}^{-n}$, where $n$ is some small integer).
Cross-validation, with folds determined internally by \texttt{adelie}, determines which power of the inverse importance to use.
Results are averaged across the same folds as used to compute the baselines.

For more details, refer to our open-sourced Git Repository. %TODO

\section{Experiment Supplements}\label{sec:exp_supp}

\subsection{Evaluation of RAG}\label{subsec:rag_eval}
\begin{figure}[htbp]
    \scriptsize 
    {\parindent0.7em \indent Prompt}
    
    \begin{tcolorbox}[
        on line, colframe=darkgray,colback=mypurple,
        boxrule=0.8pt,arc=7pt,boxsep=0pt,left=6pt,
        right=6pt,top=6pt,bottom=6pt
    ]
    Can you concisely tell me about the gene BCL6?
    \end{tcolorbox}
    
    \vspace{0.5em}
    
    {\parindent0.7em Without RAG}

    \begin{tcolorbox}[
        on line, colframe=darkgray,colback=mygray,
        boxrule=0.8pt,arc=7pt,boxsep=0pt,left=6pt,
        right=6pt,top=6pt,bottom=6pt
    ]
    BCL6 (B-cell lymphoma 6) is a gene that encodes a transcriptional repressor protein involved in the regulation of immune responses. It plays a critical role in the development and function of B cells, particularly in the formation of germinal centers where B cells mature and undergo antibody diversification. BCL6 is crucial for processes like somatic hypermutation and class switch recombination. Abnormal expression or translocations involving BCL6 are associated with certain types of lymphomas, especially diffuse large B-cell lymphoma (DLBCL). As a transcriptional repressor, BCL6 regulates the expression of genes involved in cell cycle control, apoptosis, and differentiation
    \end{tcolorbox}

    \vspace{0.5em}
    
    {\parindent0.7em With RAG}

    \begin{tcolorbox}[
        on line, colframe=darkgray,colback=pink,
        boxrule=0.8pt,arc=7pt,boxsep=0pt,left=6pt,
        right=6pt,top=6pt,bottom=6pt
    ]
    The BCL6 gene encodes a 79-kD protein that functions as a zinc finger-transcription factor, primarily involved in B-cell differentiation. It is predominantly expressed in the B-cell lineage, specifically in mature B cells and germinal centers. BCL6 is implicated in the pathogenesis of certain B-cell lymphomas, particularly diffuse large B-cell lymphoma (DLBL), where its expression may be deregulated due to chromosomal translocations affecting the 3q27 region. These translocations can lead to fusion with other genes, such as the Ikaros gene, potentially contributing to lymphoma development by preventing normal B-cell differentiation.
    \end{tcolorbox}    
    \caption{Output of GPT-4o responses, with and without RAG.}
    \label{fig:rag_gpt_outputs}
\end{figure}

To illustrate the benefits of RAG, we query GPT-4o with the question ``Can you concisely tell me about the gene BCL6?'' twice: once without RAG, and once with the OMIM RAG pipeline described in Section \ref{sec:omim-knowledge-base}.
The corresponding outputs are displayed in Figure \ref{fig:rag_gpt_outputs}.
Without RAG, the response is more generic, whereas the RAG-enhanced response is more detailed and scientific, specifically describing how BCL6 is related to lymphoma and citing interactions with proteins and other genes.

We, however, find that there are cases where RAG fails to improve performance.
There are several explanations for this.
In some cases, the RAG knowledge base is not compatible with the downstream classification or regression task.
This can cause few documents to be retrieved, in which case RAG can only marginally improve performance.
For some tasks, e.g., for DLBCL vs. FL, irrelevant documents are retrieved.
This harms performance by increasing the context that the LLM has to parse, while requiring it to sift through the context for relevant details.
We also notice that some models can be overly reliant on the documents retrieved, assigning high penalty factors to relevant genes and citing that the genes did not appear in the provided context.
This can be problematic in cases where the retriever fails to return all relevant documents, or the knowledge base lacks information about many genes.

\subsection{Scalability of Our Method} \label{appdx:gene-batches}
In this section, we discuss challenges and insights related to generating penalty factors for all 1592 genes in the Lymphoma dataset (Table \ref{tab:data}).
First, difficulties with long contexts (and context limits for some models) prevent us from generating all scores with a single query (see Section \ref{subsec:token} for discussion).
Instead, we generate scores in batches of $40$ genes, where $40 \approx \sqrt{1592}$ balances batch size with number of queries.
As a result, it is essential to ensure consistency in the penalty factors produced across batches (i.e., that a penalty factor of $5$ corresponds to the same degree of relevance across batches).
Without explicit handling of consistency, the LLM-produced penalty factors exhibit mean shifts between batches, as well as differences in orders of magnitude.

To this extent, we constrain the penalty factors to be in a pre-determined range, which we encode in our prompt (see Figure \ref{fig:prompt}).
Empirically, the range $2-5$ (inclusive) produces good results for the tasks in Table \ref{tab:data}.
If the range is too small, the penalty factors produced are close to those used in plain Lasso, preventing a large improvement of LLM-Lasso over Lasso.
If the range is too large, some models (e.g., GPT-4o) provide penalty factors too close to the extremes.
This can lead to convergence issues in the downstream algorithm, and increase the impact of spuriously low or high scores.