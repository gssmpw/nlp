In this section, we demonstrate the effectiveness of our proposed framework, LLM-Lasso, through a series of experiments. These include small-scale tests ($\sim 20$ features) and large-scale experiments ($> 1000$ features) which leverage an unpublished biomedical dataset.

% double check parameters
\subsection{Model Details}\label{subsec:model} For the experiment, we sample a combination of closed-source and open-source LLMs:
\begin{compactenum}
    \item o1 \citep{openai_gpt_o1} $-$\footnote{We note that the official parameter counts for closed-source OpenAI models have not been disclosed. Therefore, we omit the model parameter counts.},
    \item GPT-4o \citep{openai2023gpt4}: $-$,
    \item GPT-3.5 \citep{openai2023gpt35}: $-$,
    \item DeepSeek-R1 \citep{deepseek_r1}: 671B parameters,
    \item LlaMa-3.1 \citep{llama_405b}: 405B parameters,
    \item LlaMA-3 \citep{llama3_8b_instruct}: 8B parameters,
    \item Qwen Models \citep{qwen_models}: 72B parameters.
\end{compactenum}
We use all GPT models via \texttt{OpenAI} API calling and all open-source models via \texttt{OpenRouter} API calling via cloud-based inference. We implement RAG using the \texttt{langchain-community} \citep{langchain_community} code-base and a self-query retriver as our base method for query construction via \texttt{Chroma} vectorstore.

\subsection{Baselines}\label{sec:baselines} To robustly evaluate our model's performance, we compare it against baselines from both LLM-based feature selectors and traditional data-driven feature selection methods, with representatives chosen from each of the three main categories, that is, filter, wrapper, and embedded:
\begin{compactenum}
    \item LLM-Score \citep{jeong2024llmselectfeatureselectionlarge}.
    \item Filtering by Mutual Information (MI) \citep{lewis1992feature}.
    \item Recursive Feature Elimination (RFE) \citep{guyon2002gene}.
    \item Minimum Redundancy Maximum Relevance selection (MRMR) \citep{ding2005minimum}.
    \item Lasso \citep{tibshirani1996LASSO}.
    \item Random feature selection.
\end{compactenum}
For standalone feature selectors such as LLM-Select, MI, RFE, MRMR, and Random, we follow the evaluation procedures outlined in \cite{jeong2024llmselectfeatureselectionlarge} to ensure a fair comparison: approximately 10\% of the total features are selected using each method, and their performance is evaluated by measuring the test performance of a downstream $\ell_2$-penalized logistic regression model, with hyperparameters chosen via grid search and cross-validation. 
% we used the models to select 100 features and created a Lasso model with the selected features, with a $\lambda_{min}$ to $\lambda_{max}$ ratio of 0.01. The models were evaluated following the same procedure as the LLM-Lasso.

% need to follow the exact procedure of llm-select.
\begin{figure*}[htp]
    \centering    \includegraphics[width=1.08\linewidth]{fig_new/large-scale-final.png}
    \vspace{-0.5em}
    \caption{Large-Scale Experiments on Lymphoma Datasets: LLM-Lasso vs. Baselines and Model Ablation Across LLMs.
    The model ablations display mean misclassification  and (one minus) AUROC at $20$ features, with error bars for the standard deviation.
    Lasso is plotted for reference.}
\label{fig:experiments_test_error}
\end{figure*}
\subsection{Small-Scale Experiments}\label{subsec:samll_scale}
We begin with a preliminary evaluation of our plain LLM-based method against baselines using small-scale, low-dimensional public datasets across various domains. This includes three binary classification datasets (\texttt{Bank}, \texttt{Diabetes}, \texttt{Glioma}) and two regression datasets (\texttt{Wine Quality}, $\texttt{Spotify 2024}^*$). An asterisk (*) denotes a dataset published after the pretraining-data cutoff for all sampled LLMs (see Table \ref{tab:cutoff}), included to mitigate concerns about pretraining-data memorization. A summary of the datasets used can be found in Table \ref{tab:small_data}. We follow the evaluation procedures outlined in Section \ref{sec:baselines} for standalone feature selectors and in Section \ref{sec:large-scale-experiments} for Lasso-based models. To ensure a fair assessment in the presence of class imbalance, we report the error rate across ten splits along with the AUROC. As shown in Figure \ref{fig:small_scale}, GPT-4o-based LLM-Lasso consistently outperforms all sampled datasets and baselines, even when not using the best-performing LLM (see Appendix \ref{sec:exp_supp} for a model ablation study on \texttt{Spotify}).

\begin{figure}[h]
    \centering    \includegraphics[width=1\linewidth]{fig_new/small-scale-final.png}
        \vspace{-1em}
    \caption{Small-scale experiments on public datasets using GPT-4o for LLM-Lasso (Plain) and LLM-Score.}
    \label{fig:small_scale}
\end{figure}

\subsection{Large-Scale Experiments}\label{sec:large-scale-experiments}
Gene expression levels can aid in cancer diagnosis and prediction. Moreover, identifying genes predictive of specific cancers or subtypes enhances our understanding of cancer pathophysiology and may facilitate drug discovery by prioritizing key predictive genes. To demonstrate the application of our proposed framework and to show that the strong performance of LLM-Lasso carries over to high-dimensional, complex datasets, we consider the task of cancer diagnosis and classification with gene expression data as features across a range of biomedical tasks using subsets of the following unpublished lymphoma dataset. 

\paragraph{Lymphoma (Unpublished)}
Follicular lymphoma (FL) is a relatively indolent form of lymphoma that usually does not require intervention, but it could occasionally transform into the more aggressive diffuse large B-cell lymphoma (DLBCL). Using an unpublished dataset of 1592 gene expression levels from 130 lymphoma samples, we use LLM-Lasso to classify tumor samples into DLBCL and FL. Though less clinically significant, we also perform the task of classifying 161 lymphoma samples into DLBCL and mantle cell lymphoma (MCL) and classifying 196 samples into DLBCL and classical Hodgkin lymphoma (cHL) using 1592 gene expression levels. The datasets used are summarized in Table \ref{tab:data}.


\subsubsection{Building a Knowledge-Base for RAG}\label{sec:omim-knowledge-base}
We utilize \href{https://www.omim.org/}{OMIM} (Online Mendelian Inheritance in Man), an open-source database of human genes and their disease associations, to build our RAG knowledge base. Using the OMIM API, we extract gene symbols, preferred titles, clinical synopses, and detailed genetic and phenotypic relationships, storing the data in structured \texttt{JSON} format for efficient retrieval. The \texttt{JSON} data is indexed, chunked using a recursive text splitter, and ingested into \texttt{Chroma} database to populate the vector store. See Appendix \ref{appdx:imp} for more implementation details.

% \begin{figure*}[htbp]
%     \centering
%     \includegraphics[width=0.8\linewidth]{fig/experiments_AUROC.png}
%     \caption{\scriptsize AUROC of experiments}
%     \label{fig:exp}
% \end{figure*}



% use present tense
% move some description here into captions of figures.

% \paragraph{ETP T-ALL (published dataet).}
% Early T-cell precursor T-lineage acute lymphoblastic leukemia (ETP T-ALL) is a high-risk subtype of T-lineage acute lymphoblastic leukemia (T-ALL) that is challenging to diagnose due to its distinct immunophenotypic profile. Using a dataset of 1000 gene expression levels from 189 T-ALL samples \cite{liu2017genomic}, we used LLM-Lasso to classify tumor samples into ETP T-ALL and non-ETP T-ALL.

\subsubsection{Evaluation}\label{sec:evaluation}
\paragraph{Evaluation of LLM performance}\label{para:LLM_perform}
We evaluate the performance of LLMs in two parts: (i). the performance comparison of the models surveyed in Section \ref{subsec:model}, and (ii). the performance of RAG in of retrieval quality and relevance.

The histograms of Figure \ref{fig:experiments_test_error} display the average misclassification error and AUROC of LLM-Lasso at 20 features, for the models listed in Section \ref{subsec:model}.
Lasso is plotted as a baseline.
Larger and more powerful models generally perform better, especially with RAG.
Some key exceptions are LLaMa-3-8b, which achieves the second-lowest test error without RAG, and DeepSeek-R1, for which RAG degrades performance. 
We hypothesize that some models have more nuanced abilities to parse the medical documents provided by RAG, whereas others are harmed by the increased context from the retrieved documents.

As shown in the misclassification error plots of Figure \ref{fig:experiments_test_error}, LLM-Lasso RAG outperforms the plain LLM-Lasso in all tasks except DLBCL vs. FL.
Appendix \ref{subsec:rag_eval} provides an illustrative example justifying this, in which a RAG-enhanced GPT-4o model cites concrete gene interactions.
RAG, however, does not unilaterally improve performance.
This can result from the knowledge base being ill-suited to the task, issues with long contexts, and retrieval of irrelevant information, as is further discussed in Appendix \ref{subsec:rag_eval}.

Scaling LLM generation of penalty factors to over a thousand genes also presents unique challenges, as we discuss in Appendix \ref{appdx:gene-batches}.
\paragraph{Evaluation of Prediction Performance}\label{para:prediction_perf}
To test the prediction performance of the LLM-Lasso, the data is centered and split into the training set and the test set. On the training set, we perform 10-fold cross-validation across the hyperparameter $\eta \in (0,1, 2, \ldots, 10)$ for penalty factors of the form $\mathcal{I}^{-\eta}$. Using the hyperparameter with the lowest cross-validation misclassification rate, we evaluate model performance (RAG LLM-Lasso, plain LLM-Lasso, and baselines) on the test set. For each number of selected features, we record the best misclassification rate, repeating the process across 10 random splits and plotting the mean. The same procedure is applied using test metric AUROC.
Results from large-scale experiments are shown in Figure \ref{fig:experiments_test_error}, with a close-up comparison of RAG-enhanced LLM-Lasso vs. Lasso in Figure \ref{fig:lasso_comp}. RAG LLM-Lasso outperforms both the baselines and plain LLM-Lasso, achieving lower misclassification rates and higher AUROC with fewer selected genes.
\begin{figure}[h]
    \centering    \includegraphics[width=1\linewidth]{fig_new/lasso-vs-llm-lasso.png}
    \vspace{-1.5em}
    \caption{Close-up comparison of Lasso vs. RAG-enhanced LLM-Lasso on various Lymphoma datasets.}
    \label{fig:lasso_comp}
\end{figure}
\paragraph{Feature Contribution}
In the experiments we run multiple LLM-Lasso regressions, and thus we are unable to extract a single list of selected features and their coefficients. For better interpretability, we introduce a feature contribution metric that takes the proportion that each feature appears across the full path of the number of features. A feature contribution of 1 means the feature appeared in all the models, while that of 0 means the feature appeared in none of the models. We create heatmaps of the union of genes with top 10 feature contributions for the Lasso, Plain LLM-Lasso, and RAG LLM-Lasso, as well as the polarity of the coefficients, represented as letters in the heatmaps (``F'' coefficients in the direction of FL and ``D'' for DLBCL) (Figure \ref{fig:feature_contribution}). In the clinically relevant problem of classifying FL and DLBCL, there are several genes with high feature contributions that have relevance in cancer genomics and hematology/oncology, especially in the GPT-4o LLM-Lasso heatmap. For example, \textit{AICDA}, \textit{BCL2}, and \textit{BCL6}, all of which have high feature contributions in the RAG LLM-Lasso, have been implicated in the transformation of FL to DLBCL \cite{lossos2004aid, green2013hierarchy}. In the o1 LLM-Lasso heatmap, although \textit{AICDA} is included as the top gene in the RAG LLM-Lasso, many of the other genes are less relevant to the DLBCL literature \cite{pasqualucci2018genetics}. 

% Furthermore, \textit{CD5}, the top gene for RAG LLM-Lasso in the MCL vs DLBCL experiment, is a hallmark of MCL \cite{liu2002cd5}. Thus, LLM-Lasso can help prioritize features that can be useful in the classification of the target. Finally, ETP T-ALL is characterzed by weak or absent expression of \textit{CD5}, as well as mutations in genes such as \textit{FLT3} and \textit{PTEN}, both of which are prioritized in the RAG LLM-Lasso \cite{liu2017genomic, zhang2012genetic}.  


% \begin{table}[H]
% \centering
% \begin{tabular}{|l|l|l|l|}
% \hline
% \textbf{Model} & \textbf{FL} & \textbf{MCL} & \textbf{ETP T-ALL}\\
% \hline
% Permuted LLM-Lasso & 2.43 & 1.73 & 1.59 \\
% Plain LLM-Lasso & 1.47 & 1.81 & 2.14 \\
% RAG LLM-Lasso & 1.79 & 2.29 & 2.21 \\
% \hline
% \end{tabular}
% \caption{Mean cross-validated powers of hyperparameter $\mathcal{I}^{-i}$ \textcolor{red}{NOTE: update with hyperparameters of final model}}
% \label{tab:hyperparameters}
% \end{table}

%\newpage



