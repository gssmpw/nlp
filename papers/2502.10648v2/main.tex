\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{adjustbox}
\usepackage{mathtools}
\usepackage{comment}
\usepackage{booktabs} % for professional tables
\usepackage{paralist}
\usepackage{tcolorbox}
\definecolor{pink}{HTML}{faedff}
\definecolor{mypurple}{HTML}{e6e0ff}
\definecolor{mygray}{HTML}{eeeeee}

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[arxiv]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{LLM-Lasso: A Robust Framework for Domain-Informed Feature Selection and Regularization}

\begin{document}

\twocolumn[
\icmltitle{LLM-Lasso: A Robust Framework for Domain-Informed\\ Feature Selection and Regularization}

% LLM-LASSO: Bridging Statistical Learning and Metadata-Driven Insights for Robust Feature Selection

% LLM-LASSO: A Scalable Framework for Domain-Informed Regularization

\icmlsetsymbol{first}{*}
\icmlsetsymbol{last}{$\dagger$}
\begin{icmlauthorlist}
\icmlauthor{Erica Zhang}{first,yyy}
\icmlauthor{Ryunosuke Goto}{first,ggg}
\icmlauthor{Naomi Sagan}{mmm}
\icmlauthor{Jurik Mutter}{aaa}
\icmlauthor{Nick Phillips}{aaa}
\icmlauthor{Ash Alizadeh}{aaa}
\icmlauthor{Kangwook Lee}{www}
\icmlauthor{Jose Blanchet}{yyy}
\icmlauthor{Mert Pilanci}{last,mmm}
\icmlauthor{Robert Tibshirani}{last,ggg,rrr}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}


\icmlaffiliation{yyy}{Department of Management Science and Engineering, Stanford University, Stanford, USA}
\icmlaffiliation{ggg}{Department of Biomedical Data Science, Stanford University School of Medicine, Stanford, USA}
\icmlaffiliation{aaa}{Divisions of Oncology and Hematology, Stanford University School of Medicine, Stanford, USA}
\icmlaffiliation{rrr}{Department of Statistics, Stanford University, Stanford, USA}
\icmlaffiliation{mmm}{Department of Electrical Engineering, Stanford University, Stanford, USA}
\icmlaffiliation{www}{Department of Electrical and Computer Engineering, University of Wisconsin-Madison, Madison, USA}

% \icmlcorrespondingauthor{Erica Zhang}{yz4232@stanford.edu}
% \icmlcorrespondingauthor{Ryunosuke Goto}{rgoto@stanford.edu}
%\icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Large Language Models, Feature Selection, Lasso}

\vskip 0.3in

]

\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.


\begin{abstract}
We introduce LLM-Lasso, a novel framework that leverages large language models (LLMs) to guide feature selection in Lasso $\ell_1$ regression. Unlike traditional methods that rely solely on numerical data, LLM-Lasso incorporates domain-specific knowledge extracted from natural language, enhanced through a retrieval-augmented generation (RAG) pipeline, to seamlessly integrate data-driven modeling with contextual insights. Specifically, the LLM generates penalty factors for each feature, which are converted into weights for the Lasso penalty using a simple, tunable model. Features identified as more relevant by the LLM receive lower penalties, increasing their likelihood of being retained in the final model, while less relevant features are assigned higher penalties, reducing their influence. Importantly, LLM-Lasso has  an internal validation step that determines how much to trust the contextual knowledge in our prediction pipeline. Hence it addresses key challenges in robustness, making it suitable for mitigating potential inaccuracies or hallucinations from the LLM. In various biomedical case studies, LLM-Lasso outperforms standard Lasso and existing feature selection baselines, all while ensuring the LLM operates without prior access to the datasets. To our knowledge, this is the first approach to effectively integrate conventional feature selection techniques directly with LLM-based domain-specific reasoning.
\end{abstract}

%\section{Introduction}
\section{Introduction}\label{sec:intro}
\input{intro}
% \section{Notation}
% \input{notation}
% % \vspace{-0.2cm}
\section{Preliminaries}\label{prelim}
% \vspace{-0.2cm}
\input{preliminary}
% \vspace{-0.25cm}
\section{Methodology}\label{mthd}
\input{method}
\section{Simulations}\label{sec:sim}
\input{simulation}
\section{Experiments}\label{experiment}
\input{experiment}
\section{Discussion and Conclusion}\label{conclude}
\input{conclusion}
%\newpage
\section*{Author Contributions}
\textbf{Erica Zhang} proposed and implemented the complete RAG pipeline for LLM-Lasso, including scraping and curating the OMIM-based knowledge base for retrieval (Sections \ref{subsec:task_specific}, \ref{sec:omim-knowledge-base}). She developed the entire \texttt{Python} code base spanning all surveyed LLMs and baseline methods (Sections \ref{subsec:model}-\ref{subsec:samll_scale}). She conducted the small-scale experiments and performed various model ablation studies with Naomi (Figures \ref{fig:experiments_test_error}-\ref{fig:lasso_comp}). Additionally, she led the \texttt{Python}-based large-scale experiments and was largely responsible for LLM or task-specific data-driven learning related sections in the manuscript (Sections\ref{sec:intro}-\ref{prelim}, Appendices \ref{subsec:prompt_constr}, \ref{subsec:model_details}-\ref{subapp:dataset-details}, \ref{subsec:token}-\ref{subsec:score_collection}).

\textbf{Ryunosuke Goto} led efforts to develop the statistical model of the pipeline, including the evaluation of various penalty factor forms through simulations and development of the cross-validation pipeline for determining the hyperparameter for each penalty factor form (Sections \ref{subsec:llm_lasso}, \ref{subsec:penalty_fac}, \ref{sec:large-scale-experiments}, Appendix \ref{appdx:sim}). He proposed the use of OMIM and contributed to prompt engineering. He led large-scale experiment efforts, including preparing the dataset, applying the LLM-Lasso in \texttt{R}, and visualizing and interpreting the results with Jurik, Nick, and Ash (Section \ref{conclude}, Appendix \ref{subsec:large_scale_data}, \ref{subsec:feat_cont}, \ref{appdx:r-impl}). 

\textbf{Naomi Sagan} implemented the regularized Lasso and the cross-validation pipeline in \texttt{Python} (Appendix \ref{app:python-implementation}), performed the adversarial data corruption simulations (Section \ref{subsec:adversarial}), and contributed to the LLM-Lasso evaluation process (Section \ref{para:LLM_perform}). She and Erica were responsible for boosting LLM performance, including prompt engineering and improvements/customizations to the RAG pipeline (Appendix \ref{sec:exp_supp}), and they finalized \texttt{Python}-based experiments in Section \ref{sec:large-scale-experiments} (Figures \ref{fig:experiments_test_error}-\ref{fig:lasso_comp}). She helped contribute to the experimental portion of the manuscript.

\textbf{Jurik Mutter} provided the dataset for the large-scale experiments in Section \ref{sec:large-scale-experiments} and assisted in interpreting the biological implications of the data derived from the LLM-Lasso analysis.
Participated in occasional meetings and offered constructive feedback throughout the research process.

\textbf{Nick Phillips} assisted in providing validation dataset for the large-scale experiments in Section \ref{sec:large-scale-experiments} and assisted in interpreting the biological implications of the data derived from the LLM-lasso analysis. He participated in meetings and offered feedback throughout the research process.

\textbf{Ash Alizadeh} helped provide the original motivation for the noninvasive cancer classification problem from blood samples. He conceived and proposed the use of and calculation of inferred expression of selected genes of interest that he curated toward this goal using EPIC-Seq using plasma cell-free DNA. His lab provided the corresponding data from multi-cancer classification cohort.  He participated in regular meetings, he guided fellows and students working on the project, and he provided comments on the form and details of the manuscript, including biological interpretations.

\textbf{Kangwook Lee} participated in the initial idea conception, assisted the students in designing controlled experiments, and provided comments on the manuscript.

\textbf{Jose Blanchet} participated in weekly meetings, and helped in guiding students working on the project. In collaboration with others authors helped write the Introduction and assisted in writing and editorial work of the manuscript. Proposed adversarial prompt.

\textbf{Mert Pilanci} participated in the conception of the original idea, and helped develop the regularization and prompting strategy. He attended weekly meetings, provided guidance to the students on the project and offered feedback on the manuscript.

\textbf{Robert Tibshirani} helped to conceive the original idea and helped develop the statistical model for the pipeline. He participated in weekly meetings, and guided students working on the project. 
He provided comments on the form and details of the manuscript.

\paragraph{Acknowledgement.}
R.T. was supported by the National Institutes of Health (NIH) grant 5R01EB001988-16 and the National Science Foundation (NSF) grant 19DMS1208164. M.P. was supported in part by the NSF CAREER Award under Grant CCF-2236829; in part by the U.S. Army Research Office Early Career Award under Grant W911NF-21-1-0242; in part by the Office of Naval Research under Grant N00014-24-1-2164. K.L. is supported by NSF CAREER Award CCF2339978, an Amazon Research Award, and a grant from FuriosaAI.
\section*{Impact Statement}
This paper contributes to advancing machine learning and statistics by improving the robustness of LLM-based feature selection, particularly in the biomedical domain. By reducing susceptibility to overfitting, our approach enhances the reliability and generalizability of feature selection methods, improving the trustworthiness and interpretability of AI. These improvements have positive implications for prioritizing features in biomedicine, potentially leading to novel biomedical discoveries.

\bibliography{ref}
\bibliographystyle{icml2025}
\newpage
\appendix\label{appendix}
\input{appendix}


\end{document}

