\section{Obfuscation Detection Framework}
\label{sec:Methodology}

We developed a machine learning-based framework to detect whether an app is being obfuscated or not, followed by what tool it has used for obfuscation and, finally, what type of obfuscation(s) are present. Our framework consists of a bank of classifiers that use the same Android APK-level features.

\begin{figure*}[!h]
    \centering
    \subfloat[Training and testing]{%
        \includegraphics[width=\linewidth]{Figures/Method_1V1.pdf}%
        \label{fig:overview_1a}
    }\\
    \subfloat[Large-scale analysis]{%
        \includegraphics[width=\linewidth]{Figures/Method_2V2.pdf}%
        \label{fig:overview_1b}
    }
    \caption{Overall experiment process: Training and testing and large-scale investigation.}
    \label{fig:overview}
\end{figure*}


\subsection{Classifier Banks}
\label{sec:classifier_bank}

Our framework consists of three classifier types as illustrated in Figure~\ref{fig:overview_1a} for  \textit{i) Obfuscation Detection, ii) Obfuscation Tool Detection, and iii) Obfuscation Technique Detection}.

Using our training and validation sets ({\bf cf.} Section~\ref{sec:building_dataset}), we tested several models, such as MLP, SVM, Random Forest, and Decision Trees. We selected the best model for each task and further tuned the hyper-parameters using grid search.


Our models and features are comparable to existing work in obfuscation detection~\cite{wermke2018large, wang2017changed, mirzaei2019androdet, conti2022obfuscation} and provide similar performance. Here, our methodological contribution is the comprehensive framework, which facilitates subsequent large-scale longitudinal analysis of obfuscation adoption.


\begin{itemize}
    \item \textbf{Obfuscation Detector}: Our Obfuscation Detector is a binary MLP classifier that makes a prediction of whether a given Android APK is obfuscated or not. 


    \item \textbf{Obfuscation Tool Detector}: We use a bank of three binary Random Forest classifiers for obfuscation tool detection making decisions: \textit{1. ProGuard vs. Other}, \textit{2. Allatori vs. Other}, \textit{3. DashO vs. Other}.
    Each classifier assigns a probability to each tool: Proguard, DashO, and Allatori. Based on the highest probability, we decide which tool the APK uses. We chose to use a bank of classifiers rather than a single multi-class classifier to handle the other category more effectively, given the absence of training data for unknown obfuscation tools. This approach simplifies decision-making, as they do not need to distinguish between multiple classes. Also, it allows easy scalability by adding new classifiers as data from other tools becomes available.

    \item \textbf{Obfuscation Technique Detector}: Similarly, we train a bank of three binary Random Forest classifiers for Obfuscation Technique Detection. We focused on the most commonly used obfuscation techniques that were discussed in Section~\ref{sec:obfustexchniques} and in previous works~\cite{zhang2021android, conti2022obfuscation, guo2022survey, dong2018understanding, wermke2018large}; i) Identifier Renaming (IR), ii) Control Flow Modification (CF), and iii) String Encryption (SE). Given an APK, these classifiers predict whether it is obfuscated with each of these techniques. If the probability given by a classifier is higher than 0.5, we categorize the APK as obfuscated using the relevant technique. 
    
    
\end{itemize}

\subsection{Feature Engineering}
\label{sec:features}

We use 37 features of three types to represent an Android APK. They are primarily related to obfuscation techniques we focus on, as summarised in Table~\ref{tab:feature_list}. These features were selected based on prior works~\cite{mirzaei2019androdet, conti2022obfuscation}. We use Androguard~\cite{desnos2018androguard} to extract Identifier names, Strings and Instructions from APKs' DEX files. We calculate the percentages of class names, method names, field names, and other strings based on their lengths, the presence of special characters, and the presence of numeric characters. Specifically, we count the occurrences of names with lengths of 1, 2, 3, 4, and greater than 4, as well as those containing special characters, numeric characters, or both. For example, Feature 1, described in Equation~\eqref{feature1}, represents the percentage of class names with a length of 1 relative to the total number of class names in a given APK. Equation~\eqref{feature5} calculates the percentage of class names containing special characters relative to the total number of class names. We performed similar calculations for other attributes, as detailed in Table 2. For instructions, we selected five specific instructions: \texttt{nop, goto, invoke, if, and move}. We then calculated the percentage of these selected instructions out of the total number of available instructions, as shown in Equation~\eqref{feature_ins}.


\begin{table}[]
\caption{Extracted feature list}
\label{tab:feature_list}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{ccl}
\hline
\textbf{\begin{tabular}[c]{@{}c@{}}Obfuscation \\ Category\end{tabular}} & \textbf{Category} & \multicolumn{1}{c}{\textbf{Feature}} \\ \hline
\multirow{6}{*}{\begin{tabular}[c]{@{}c@{}}Identifier\\ Renaming\end{tabular}} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Class\\ Names\end{tabular}} & \begin{tabular}[c]{@{}l@{}}Feature 1 - 5:\\ Percentage of class names of length 1, 2, 3, 4 and \textgreater{} 4\end{tabular} \\ \cline{3-3} 
 &  & \begin{tabular}[c]{@{}l@{}}Feature 6 - 8:\\ Percentage of class names containing special characters, \\numeric characters, or both\end{tabular} \\ \cline{2-3} 
 & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Method\\ Names\end{tabular}} & \begin{tabular}[c]{@{}l@{}}Feature 9 - 13:\\ Percentage of method names of length 1, 2, 3, 4 and \textgreater{} 4\end{tabular} \\ \cline{3-3} 
 &  & \begin{tabular}[c]{@{}l@{}}Feature 14 - 16:\\ Percentage of class names containing special characters, \\numeric characters, or both\end{tabular} \\ \cline{2-3} 
 & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Field\\ Names\end{tabular}} & \begin{tabular}[c]{@{}l@{}}Feature 17 - 21:\\ Percentage of field names of length 1, 2, 3, 4 and \textgreater{} 4\end{tabular} \\ \cline{3-3} 
 &  & \begin{tabular}[c]{@{}l@{}}Feature 22 - 24:\\ Percentage of class names containing special characters, \\numeric characters, or both\end{tabular} \\ \hline
\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}String\\ Encryption\end{tabular}} & \multirow{2}{*}{Strings} & \begin{tabular}[c]{@{}l@{}}Feature 25 - 29:\\ Percentage of other strings of length 1, 2, 3, 4 and \textgreater{} 4\end{tabular} \\ \cline{3-3} 
 &  & \begin{tabular}[c]{@{}l@{}}Feature 30 - 32:\\ Percentage of class names containing special characters, \\numeric characters, or both\end{tabular} \\ \hline
\begin{tabular}[c]{@{}c@{}}Control\\ Flow\end{tabular} & \multicolumn{1}{l}{Instructions} & \begin{tabular}[c]{@{}l@{}}Feature Ins 33 - 37:\\ Percentage of nop,  goto,  invoke,  if, and move instructions\end{tabular} \\ \hline
\end{tabular}%
}
\end{table}




\begin{equation}
    \label{feature1}
    \text{Feat. 1} = \frac{\text{No. of class names with length 1}}{\text {Total number of class names}}\times 100
\end{equation}

\begin{equation}
    \label{feature5}
    \text{Feat. 6} = \frac{\text{No. of class names consist of special chars}}{\text {Total number of class names}}\times 100
\end{equation}

\begin{equation}
    \label{feature_ins}
    \text{Feat. Ins 33} = \frac{\text{No. of \textbf{nop} Instructions}}{\text {Total number of Instructions}}\times 100
    % {Feature\ Ins1 = \frac{No.\ of \ nop\ instructions}{Total\ no.\ of\ instructions}}
\end{equation}


\subsection{Building the Ground-truth Dataset}
\label{sec:building_dataset}

To build our training dataset, we downloaded app source codes from the F-droid repository~\cite{fdroid}. We imported each source code to Android Studio and disabled any obfuscation in the \texttt{build.gradle} to produce non-obfuscated samples. 


To create obfuscated samples, we used the same projects imported from F-droid and employed ProGuard~\cite{proguard}, Allatori~\cite{allatori}, and DashO~\cite{dasho} as obfuscation tools. While Proguard is free, for Allatori, we used the educational version, which has the same features as the commercial version. For DashO, we used the 7-day evaluation licence provided to us by PreEmptive Inc., which again has the full features of the commercial version. To train the Obfuscation Technique detector, we created sets of APKs by applying each technique individually, thereby ensuring separate datasets for each technique.

Furthermore, we also use the AndroOBFS dataset~\cite{androobfs}, comprising malware APKs obfuscated with ObfuscAPK~\cite{obfuscapk}. Our aim is to select a subset of APKs from AndroOBFS to validate our classifiers and verify that they perform well with unseen obfuscation tools. Finally, we obtained a set of random APKs from the Google Play Store and manually labelled them for obfuscation, serving as an additional validation dataset. Since we don't know which tool was used to obfuscate these apps, we label them only as obfuscated or not.


Using manually created obfuscated and non-obfuscated APKs \textbf{(MC-APKs)}, AndroOBFS APKs, and 50 Google Play Store APKs \textbf{(GP)}, we generated several datasets to train and validate each classifier. The summary of these data subsets is discussed below.

\subsubsection{Datasets for Obfuscation Detector}

We curated four datasets using MC-APKs, AndroOBFS data, and 50 GP APKs:
\begin{itemize}
    \item {\bf D1}: Training and testing dataset (349 MC-APKs; 80\% for training and parameter tuning, and 20\% for testing). 
    \item {\bf D2:} Unseen evaluation dataset (135 MC-APKs).
    \item {\bf D3:} Random subset of AndroOBFS (270 APKs).
    \item {\bf D4:} Manually labelled 50 Google Play APKs.
\end{itemize}

Using F-Droid~\cite{fdroid} Android projects, we manually created 87 non-obfuscated APKs and 397 obfuscated APKs (MC-APKs). Due to the limited number of ground truth APKs, we divided these manually created APKs into two sets, D1 and D2, as detailed in Table~\ref{tab:obfuscation_detection_dataset}. We utilized D1 as our training and testing dataset, reserving D2 as an unseen validation dataset to assess the generalizability of our models. To further enhance the validation of generalizability, we incorporated the AndroOBFS dataset~\cite{androobfs}. We randomly selected D3 from the AndroOBFS dataset as our second validation set. As shown in Table~\ref{tab:obfuscation_detection_dataset}, all APKs in the AndroOBFS dataset are obfuscated; no non-obfuscated APKs were included. To strengthen the validation of our obfuscation detector, we also randomly selected 50 APKs from the Google Play Store and manually labelled them. We examined the identifier names of each APK to identify any anomalies or deviations in natural language. APKs were labelled as obfuscated if anomalies were observed in the identifier names; otherwise, they were labelled as non-obfuscated. Out of the 50 APKs, 33 were classified as obfuscated and 17 as non-obfuscated after manual labelling.

\begin{table}[!h]
    \centering
    \caption{Obfuscation Detector Dataset}
    \label{tab:obfuscation_detection_dataset}
    \begin{tabular}{p{2.0cm}ccc}
        \hline
        \textbf{Dataset} & \textbf{Obfuscated} & \textbf{N-Obfuscated} &\textbf{Total}\\
        \hline
        D1 & 274& 75& 349\\  
        D2 & 123& 12& 135\\ 
        D3 & 270& 0& 270\\ 
        D4 & 33& 17& 50\\
        \hline
    \end{tabular}
\end{table}

\subsubsection{Datasets for Obfuscation Tool Detector}

We use MC-APKs and AndroOBFS apps to create two datasets:
\begin{itemize}
    \item {\bf D5:} Training and testing dataset (312 MC-APKs; 80\% for training and parameter tuning, and 20\% for testing).
    \item {\bf D6:} Unseen evaluation dataset (50 MC-APKs + 30 AndroOBFS)
\end{itemize}

We excluded all non-obfuscated APKs from the manually created APK dataset (from MC-APKs) because they could not be categorized under any obfuscation tools. Additionally, we removed APKs that were obfuscated using a combination of two tools, as it was challenging to classify such APKs under a single tool. Consequently, we had 362 obfuscated APKs available for tool detection. Similar to the previous scenario, we divided this dataset into D5 and D6, using D5 for training and testing the tool detection model, and keeping D6 as the unseen validation set. To ensure that our model can accurately classify the ``Other'' category, we randomly selected 30 APKs from the AndroOBFS dataset and combined them with D6, as detailed in Table~\ref{tab:tool_detection_dataset}. We used D6 as our unseen validation dataset.

\begin{table}[!h] 
    \caption{Obfuscation Tool Detector Dataset}
    \label{tab:tool_detection_dataset}
    \begin{tabular}{p{1.5cm}ccccc} \hline
        Dataset & ProGuard & DashO & Allatori & Obfuscapk & Total\\ \hline
        D5 & 68 & 162 & 82 & 0 & 312\\
        D6 & 15& 20 & 15& 30& 80\\
        \hline
    \end{tabular}
\end{table}

\subsubsection{Datasets for Obfuscation Technique Detector}

We used 376 MC-APKs and AndroOBFS apps to create three datasets:
\begin{itemize}
    \item {\bf D7:} Training and testing dataset (324 MC-APKs; 80\% for training and parameter tuning, and 20\% for testing).
    \item {\bf D8:} Unseen evaluation dataset (52 MC-APKs)
    \item {\bf D9:} Random subset of AndroOBFS (90 each for IR, CF, and SE).
\end{itemize}
In line with the tool detection process, we removed non-obfuscated APKs and irregularly combined APKs from the original dataset (from MC-APKs) to create a set of 376 manually obfuscated APKs for technique detection. As with the previous cases, we split this dataset into two subsets: D7 for training and testing, and D8 for unseen evaluation. The number of APKs in each dataset is detailed in Table~\ref{tab:technique_detection_dataset}. Additionally, since the AndroOBFS dataset provides APKs labelled with obfuscation techniques, we randomly selected an extra set of APKs (D9) from it to further validate the generalizability of our method.

\begin{table}[!h] 
    \caption{Obfuscation Technique Detector ataset}
    \label{tab:technique_detection_dataset}
    \begin{tabular}{p{0.8cm}cccccccc} \hline
        Dataset & IR & CF & SE & IR\&CF & IR\&SE & CF\&SE & All & \textbf{Total}\\
        \hline
        D7 & 45 & 36 & 36 & 23 & 23 & 23 & 138 & 324\\
        D8 & 15 & 7 & 7 & 2 & 2 & 4 & 15 & 52\\
        D9 & 90 & 90 & 90 & - & - & - & - & 270\\
        \hline
    \end{tabular}
\end{table}

Note that there are different numbers of APKs in the MC-APKs dataset for the three detectors. This is because we excluded all non-obfuscated APKs and APKs created using multiple tools (i.e., ProGuard and Allatori combined) from the MC-APKs when creating the tool detector and technique datasets. Additionally, to balance the unseen evaluation set, we used only 30 APKs from AndroOBFS in D6. 


\subsection{Performance of the Classifiers}
\label{sec:training_and_validation}

\subsubsection{{Obfuscation Detector}}

We show the performance of the obfuscation detector on different datasets in Table~\ref{tab:binary_cls_results}. Our results are comparable to those of OBFUSCAN~\cite{wermke2018large}, which reported similar findings. However, OBFUSCAN targets only ProGuard-obfuscated APKs, whereas our tool can handle APKs obfuscated by various obfuscators. To assess how well our obfuscation detector works with unseen data (i.e., not from a split of the training and test set), we evaluated it on D2, D3 and D4 as well. While the performance dropped somewhat, detector accuracy was still in the range of 87\%--92\%, suggesting its suitability for the large-scale analysis.


\begin{table}[h]
\caption{Obfuscation detection - Results}
\label{tab:binary_cls_results}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{ccccc}
\hline
\textbf{Dataset} & \textbf{Accuracy} & \textbf{Precision} & \textbf{Recall} & \textbf{F1 Score} \\ \hline
D1 (Test Set) & 0.97 & 0.96 & 1.00 & 0.98 \\
D2 & 0.87 & 1.00 & 0.85 & 0.92 \\
D3 & 0.88 & 1.00 & 0.88 & 0.94 \\
D4 & 0.92 & 1.00 & 0.88 & 0.94 \\ \hline
Avg. & 0.91 & 0.99 & 0.90 & 0.95 \\ \hline
\end{tabular}%
}
\end{table}

\subsubsection{{Obfuscation Tool Detector Bank}}

Each classifier in our bank determines if an APK is obfuscated using a specific tool (ProGuard, Allatori, DashO) or another tool (Other). Here, it is crucial to assess that each classifier is not only classifying its own target tool usage correctly but also any other tool usage must be classified as other. For instance, if an APK is obfuscated with \emph{DashO}, \textit{ProGuard vs. Other} and \textit{Allatori vs. Other} should classify it as \emph{Other}, while \textit{DashO vs. Other} should detect it as \emph{DashO}. For APKs from AndroOBFS, all three classifiers should classify them as \emph{Other}. Therefore, we use macro versions of Precision, Recall, and F1 to evaluate performance. We summarize the results in Table~\ref{tab:tool_results}.


On the test set (D5), our classifier bank achieves an average of 99\% which is comparable to previous work~\cite{wang2017changed}. However, we highlight that~\cite{wang2017changed} operates in a closed-set setting and, as such, does not have the means to categorize unknown obfuscators accurately. To further validate how well the classifiers perform in detecting unknown tools, we evaluated them on D6. We achieved an average accuracy of 88\% showing that the obfuscation tool detector indeed works well with unknown tools and classifies them as others with high accuracy.


\begin{table}[h]
\caption{Obfuscation tool detection - Results}
\label{tab:tool_results}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{clcccc}
\hline
\multirow{2}{*}{\textbf{Dataset}} & \multicolumn{1}{c}{\multirow{2}{*}{\textbf{Classifier}}} & \multicolumn{3}{c}{\textbf{Macro}} & \multirow{2}{*}{\textbf{Acc.}} \\ \cline{3-5}
 & \multicolumn{1}{c}{} & \textbf{P} & \textbf{R} & \textbf{F1} &  \\ \hline
\multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}D5 \\ (Test Set)\end{tabular}} & ProGuard vs. Other & 1.00 & 1.00 & 1.00 & 1.00 \\
 & Allatori vs. Other & 1.00 & 1.00 & 1.00 & 1.00 \\
 & DashO vs. Other & 0.98 & 0.97 & 0.97 & 0.97 \\ \hline
\multirow{3}{*}{D6} & ProGuard vs. Other & 0.82 & 0.91 & 0.86 & 0.90 \\
 & Allatori vs. Other & 0.89 & 0.91 & 0.90 & 0.94 \\
 & DashO vs. Other & 0.84 & 0.93 & 0.87 & 0.89 \\ \hline
\end{tabular}%
}
\end{table}

\subsubsection{{Obfuscation Technique Detector Bank}}

Similarly, we show the performance of the obfuscation technique detector bank in Table~\ref{tab:technique_results}. On the test set (D7), we achieved an average accuracy of 88\% which is comparable to prior works~\cite{kuhnel2015fast, conti2022obfuscation, mirzaei2019androdet}. Our technique outperforms the method described in~\cite{mirzaei2019androdet}. While~\cite{kuhnel2015fast} focuses solely on identifier renaming, our method addresses a broader range of popular obfuscation techniques. Additionally, although the results of AndrODet*~\cite{conti2022obfuscation} for the training and test datasets are comparable to those of our method, they retrained their model before evaluating it on unseen data. This approach converts the unseen data into seen data, which undermines the generalizability of their method. Such generalizability is crucial for conducting a comprehensive large-scale study.


Further, to examine the generalizability of our method, we conducted two validation studies using D8 and D9, which are unseen and differently distributed from the original training/test dataset. Still, the classifier bank achieves average accuracies of 88\% and 80\%.


\begin{table}[h]
\caption{Obfuscation technique detection - Results}
\label{tab:technique_results}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{cccccc}
\hline
\textbf{Dataset} & \textbf{Classifier} & \textbf{Precision} & \textbf{Recall} & \textbf{F1} & \textbf{Accuracy} \\ \hline
\multirow{3}{*}{D7 (Test Set)} & (IR) & 0.98 & 0.89 & 0.93 & 0.91 \\
 & (CF) & 0.87 & 0.91 & 0.89 & 0.85 \\
 & (SE) & 0.92 & 0.92 & 0.92 & 0.88 \\ \hline
\multirow{3}{*}{D8} & (IR) & 1.00 & 0.88 & 0.94 & 0.92 \\
 & (CF) & 0.82 & 0.96 & 0.89 & 0.86 \\
 & (SE) & 0.83 & 0.93 & 0.88 & 0.86 \\ \hline
\multirow{3}{*}{D9} & (IR) & 1.00 & 0.84 & 0.92 & 0.84 \\
 & (CF) & 1.00 & 0.78 & 0.88 & 0.78 \\
 & (SE) & 1.00 & 0.79 & 0.88 & 0.79 \\ \hline
\end{tabular}%
}
\end{table}


In summary, we presented an obfuscation detection framework comprising three machine learning models developed to detect obfuscation, obfuscation tools, and techniques. We evaluated the capabilities of these models to perform large-scale analysis using various ground-truth datasets, which are employed in the large-scale analysis as illustrated in Figure~\ref{fig:overview_1b}. In addition, we have publicly released the source codes of our classifiers along with its best models and ground-truth datasets.\footnote{\href{https://github.com/NSS-USYD/Obfuscation-Large_Analysis}{https://github.com/NSS-USYD/Obfuscation-Large\_Analysis}}
