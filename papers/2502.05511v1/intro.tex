\section{Introduction}
\label{sec:intro}


The online paging problem is a fundamental problem in %
the area of online
algorithms. There are $n$ pages in slow memory, and requests for these pages arrive in a sequential manner. We are allowed to maintain a fast cache of size $k$, which initially comprises of some $k$ pages. At time step $t$, if the page that is requested, say $p_t$, exists in the cache, this corresponds to a cache hit: we suffer a zero cost, the cache stays as is, and we move on to the next request. Otherwise, we incur a cache miss/page fault, suffer a unit cost, and are forced to evict some page from the cache so as to bring $p_t$ from the slow memory into the cache. A paging algorithm/policy is then specified by how it chooses the page to evict whenever it suffers a cache miss.

An optimal offline algorithm for this problem is the algorithm that has entire knowledge of the page request sequence, and thereafter makes its eviction choices in a way that minimizes the total number of cache misses it incurs. The classical Farthest-in-Future algorithm \citep{belady1966study}, which at any cache miss, evicts the page that is next requested latest in the remainder of the sequence, is known to be an optimal offline policy. In the spirit of competitive analysis as introduced by \cite{sleator1985amortized}, one way to benchmark any online algorithm (i.e., one which does not have knowledge of the page request sequence) is to compute the ratio of the number of page faults that the algorithm suffers, to the number of page faults that the optimal offline algorithm suffers, on a \textit{worst-case} sequence of page requests. It is known that any deterministic online algorithm has a worst-case ratio that is at least $k$, while any randomized online algorithm has a worst-case ratio of $\Omega(\log k)$.

Taking a slightly more optimistic view, one can also consider benchmarking an algorithm against the best \textit{online} algorithm, that does not know beforehand the realized sequence of page requests. Additionally, in the context of beyond worst-case analysis, one can impose assumptions on the sequence of page requests to more accurately model the behavioral characteristics of real-world page requests on computers. For example, page requests generally follow the \textit{locality of reference} principle, either in time (the next time a particular page is requested is close to its previous time) or in space (the page that will be requested next is likely to be in a nearby memory location).%

The Markov Paging model, introduced by \cite{karlin1992markov}, is one of the ways to model such locality of reference in the page requests. The model assumes that page requests are drawn from a (known) Markov chain over the entire set of $n$ pages. Once such a distributional assumption over the page requests is imposed, one can ask how an online paging algorithm fares, in comparison to the optimal online algorithm that suffers the smallest number of page misses \textit{in expectation} over the draw of a page sequence from the distribution. As it turns out, one can compute such an optimal online algorithm, but only in time exponential in the cache size $k$. Therefore, one seeks efficient polynomial-time algorithms that are approximately optimal.

In 1994, \cite{lund1994ip} proposed an extremely elegant randomized algorithm for the Markov Paging problem, known as the dominating distribution algorithm. The dominating distribution algorithm, whenever it suffers a cache miss, evicts a random page drawn from a special distribution, which has the property that a randomly drawn page is likely to be next requested latest among all the existing pages in the cache. This algorithm, which has since become a popular textbook algorithm due to its simplicity (e.g., see Chapter 5 in \cite{borodin2005online}, or the lecture notes by \cite{roughgardennotes}), runs in time polynomial in $k$; furthermore, \cite{lund1994ip} show that the algorithm suffers only 4 times more cache misses in expectation than the optimal online algorithm. Since then, this has remained state-of-the-art---we do not know any other polynomial-time algorithms that achieve a better performance guarantee. It has nevertheless been conjectured that the above guarantee for the dominating distribution algorithm is suboptimal (e.g., see Section 6.2 in \cite{roughgardennotes}).


Our main result in this work establishes an improved upper bound for the dominating distribution algorithm.

\begin{theorem}
    \label{thm:dominating-distribution-improved-bound}
    The dominating distribution algorithm suffers at most 2 times more cache misses in expectation compared to the optimal online algorithm in the Markov Paging model.%
\end{theorem}
In fact, as mentioned in \cite{lund1994ip}, our guarantee for the performance of the dominating distribution algorithm holds more generally for \textit{pairwise-predictive} distributions. These are distributions for which one can compute any time, for any pair of pages $p$ and $q$ in the cache, the probability that $p$ is next requested before $q$ in the future (conditioned on the most recent request).

While the dominating distribution algorithm is a randomized algorithm, \cite{lund1994ip} also propose and study a simple deterministic algorithm for the Markov Paging model, known as the median algorithm. On any cache miss, the median algorithm evicts the page in cache that has the largest median time at which it is requested next. \cite{lund1994ip} show that the median algorithm suffers at most 5 times more cache misses in expectation than the optimal online algorithm---to the best of our knowledge, this is the state of the art for deterministic algorithms. As an additional convenient consequence of our improved analysis, we are also able to improve the performance guarantee for the median algorithm.

\begin{theorem}
    \label{thm:median-improved-bound}
    The median algorithm suffers at most 4 times more cache misses in expectation compared to the optimal online algorithm in the Markov Paging model.
\end{theorem}

Given the improved performance guarantee in \Cref{thm:dominating-distribution-improved-bound}, one can ask: is this the best bound that we can show for the dominating distribution algorithm? Or is there hope to show that it is \textit{the} optimal online algorithm? A detail here is that the dominating distribution algorithm is defined as any algorithm satisfying a specific property (see \eqref{eqn:dominating-distribution-property}); in particular, there can exist different dominating distribution algorithms. Our upper bound (\Cref{thm:dominating-distribution-improved-bound}) holds uniformly for \textit{all} dominating distribution algorithms. However, our next theorem shows a lower bound that at least one of these is significantly suboptimal. 

\begin{theorem}
    \label{thm:dominating-distribution-lower-bound}
    There exists a dominating distribution algorithm that suffers at least $1.5907$ times more cache misses in expectation compared to the optimal online algorithm in the Markov Paging model.
\end{theorem}

\Cref{thm:dominating-distribution-lower-bound} shows that one cannot hope to show optimality uniformly for all dominating distribution algorithms. To the best of our knowledge, no such lower bounds for the dominating distribution algorithm have been previously shown. While we believe that the bound in \Cref{thm:dominating-distribution-improved-bound} is the correct bound, it is an interesting open direction to close the gap between the upper and lower bound. %

Finally, we also consider a setting where the algorithm does not exactly know the Markov chain generating the page requests, but only has access to a dataset of past page requests. In this case, one can use standard estimators from the literature to approximate the transition matrix of the Markov chain from this dataset. But can an approximate estimate of the transition matrix translate to a tight competitive ratio for a paging algorithm, and if so, how large of a dataset does it require to achieve it? The robustness of the analysis\footnote{In fact, we borrow this robustness from the original analysis of \cite{lund1994ip}.} of \Cref{thm:dominating-distribution-improved-bound} lets us derive a precise learning-theoretic result (\Cref{thm:learning-markov-chains} in \Cref{sec:robustness}) showing that the dominating distribution algorithm, when trained on a dataset of size $O(n^2/\eps^2)$, is no more than $\frac{2}{1-2\eps}$ times worse than the optimal online algorithm on a fresh sequence of page requests. 

This result is especially relevant in the context of data-driven and learning-augmented algorithms. For instance, by treating the learned Markov chain as a learned oracle and combining it with a worst-case approach, such as the Marker algorithm, similarly to the approach proposed in~\citep{mahdian2012online,lykouris2021competitive}, the resulting algorithm achieves $O(1)$-consistency and $O(\log k)$-robustness guarantees for the online paging problem.

\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|c|c|}\hline
    Algorithm & Reference & Upper Bound & Lower Bound \\\hline
    \multirow{4}{*}{} & \multirow{2}{*}{\cite{lund1994ip}} & 4 & \multirow{2}{*}{-} \\
      Dominating Distribution  &  & (Theorem 3.5) & \\ \cline{2-4}
      (Randomized) & \multirow{2}{*}{This Work} & \textbf{2} & \textbf{1.5907} \\
      & & (\Cref{thm:dominating-distribution-improved-bound}) & (\Cref{thm:dominating-distribution-lower-bound}) \\ \hline 
    \multirow{4}{*}{} & \multirow{2}{*}{\cite{lund1994ip}} & 5 & \textbf{1.511}\footnotemark  \\
      Median  &  & (Theorem 2.4) & (Theorem 5.4) \\ \cline{2-4}
      (Deterministic) & \multirow{2}{*}{This Work} & \textbf{4} & \multirow{2}{*}{-} \\
      & & (\Cref{thm:median-improved-bound}) & \\ \hline
    \end{tabular}
    \caption{State of the art for Markov Paging (best-known bounds are in bold). Both the median and the dominating distribution algorithm are due to \cite{lund1994ip}.}
    \label{table:summary}
\end{table}
\footnotetext{\cite{lund1994ip} only provide a proof for a lower bound of 1.4, but mention that they also obtained 1.511.}

\subsection{Other Related Work}
\label{sec:related-work}



There is a rich literature by this point on studying the online paging problem under assumptions on the permissible page requests that are supposed to model real-world scenarios (see e.g., the excellent surveys by \cite{irani2005competitive} and \cite{dorrigiv2005survey}). In particular, several models are motivated by the aforementioned locality of reference principle. The access graph model, proposed originally by \cite{borodin1991competitive}, and developed further in the works of \cite{irani1992strongly}, \cite{fiat1995randomized} and \cite{fiat1997truly}, assumes an underlying graph on the pages, which a request sequence is supposed to abide by. Namely, if a page $p_t$ has been requested at time $t$, then only the neighbors of $p_t$ in the graph may be requested at time $t+1$. In this sense, the Markov Paging model can be thought of as a probabilistic variant of the access graph model.
\cite{torng1998unified} models locality of reference based on the ``working sets'' concept, introduced by~\cite{denning1968working}, and also shows how a finite lookahead of the page sequence can be used to obtain improved guarantees. A related form of competitive analysis, proposed by \cite{koutsoupias2000beyond}, assumes that there is a family of valid distributions, and page sequences are drawn from some distribution belonging to this family. The goal of an algorithm is then to be simultaneously competitive with the optimal algorithm for every distribution in the family. A particular family of interest suggested by \cite{koutsoupias2000beyond} (which is incomparable to the Markov Paging model) is that of \textit{diffuse adversaries}, which has since been developed further in follow-up works by \cite{young1998bounding, young2000line} and \cite{becchetti2004modeling}. Finally, \cite{angelopoulos2009paging} study paging under the \textit{bijective analysis} framework, which compares the performance of two algorithms under bijections on page request sequences.




