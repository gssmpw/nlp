\input{appendix/more_related_work}

\section{Additional Algorithms}\label{sec:add_algorithms}

\begin{algorithm}[t]
\caption{Basic Next-Word Prediction Model $M_{\cT, \cA, \cD}$.}
\label{alg:basic-model}
\begin{algorithmic}
\STATE {\bf Parameters:}\begin{itemize}[itemsep=-3pt]
    \item $\cT : \Vtoken \to \R^d$: token embedding layer,
    \item $\cA : (\R^d)^{\le N_{\max}} \to \R^d$: transformer model,
    \item $\cD : \R^d \to \Delta_{\Vtoken}$: prediction head.
\end{itemize}
\STATE {\bf Input:} $(\sigma_1, \ldots, \sigma_m) \in \Vtoken^*$ for $m \le N_{\max}$.
\STATE {\bf Output:} Probability distribution over next token $\hat{\sigma}_{m+1}$.
\FOR{$i=1, \ldots, m$}
    \STATE $\be_i \gets \cT(\sigma_i)$ : Input embedding per token
\ENDFOR
\STATE $\be_{\mathrm{out}} \gets \cA(\be_1, \ldots, \be_m)$: Output embedding.
\RETURN $D(\be_{\mathrm{out}})$.
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[h]
\caption{Next-word prediction with \SCONE  $M_{\cT, \Vfgram, \train{\Afgram}|\infer{\cF}, \Amain, \cD}$}
\label{alg:scone-model}
\begin{algorithmic}
\STATE {\bf Parameters:}\begin{itemize}[itemsep=-3pt]
    \item $\cT : \Vtoken \to \R^d$: token embedding layer,
    \item $\Vfgram \subseteq \Vtoken^{[2,n]}$: set of f-grams,
    \item \textbf{\textsc{Training}}: f-gram transformer model
    \begin{itemize}[leftmargin=8mm,label=$\triangleright$,itemsep=-3pt,topsep=-4pt]
        \item \train{$\Afgram : (\R^d)^{\le n} \to \R^d$},
    \end{itemize}
    \item \textbf{\textsc{Inference}}: f-gram embedding layer
    \begin{itemize}[leftmargin=8mm,label=$\triangleright$,itemsep=-3pt,topsep=-4pt]
        \item \infer{$\cF : \Vfgram \to \R^d$}.
    \end{itemize}
    \item $\Amain : (\R^d)^{\le N_{\max}} \to \R^d$: main transformer model
    \item $\cD : \R^d \to \Delta_{\Vtoken}$: Prediction head.
\end{itemize}
\STATE {\bf Input:} $(\sigma_1, \ldots, \sigma_m) \in \Vtoken^*$ for $m \le N_{\max}$.
\STATE {\bf Output:} Probability distribution over next token $\hat{\sigma}_{m+1}$.
\STATE $(\be_1, \ldots, \be_m) \gets F_{\cT, \Vfgram, \train{\Afgram} | \infer{\cF}}(\sigma_1, \ldots, \sigma_m)$ (\Cref{alg:scone-first-stage})
\STATE $\be_{\mathrm{out}} \gets \Amain(\be_1, \ldots, \be_m)$
\RETURN $D(\be_{\mathrm{out}})$.
\end{algorithmic}
\end{algorithm}

In \Cref{sec:preliminary}, we discuss a simple next-word prediction model, \( M_{\cT, \cA, \cD} \), consisting of a token embedding layer \( \cT \), a transformer model \( \cA \), and a prediction head \( \cD \). This model takes a token sequence \( (\sigma_1, \ldots, \sigma_m) \), with each token from the token vocabulary \( \Vtoken \), and produces a probability distribution for the next token. We provide the pseudocode for \( M_{\cT, \cA, \cD} \) in \Cref{alg:basic-model}.





In \Cref{alg:scone-first-stage} in \Cref{sec:method}, we present the pseudocode for \SCONE's process of generating contextualized f-gram embeddings. Next, we describe the end-to-end next-word prediction process using \SCONE (\Cref{alg:scone-model}). Specifically, the process, denoted as $M_{\cT, \Vfgram, \train{\Afgram} | \infer{\cF}, \Amain, \cD}$, takes an input sequence $(\sigma_1, \ldots, \sigma_m) \in \Vtoken^*$ and produces a distribution over the next token $\hat{\sigma}_{m+1}$. Note that in \Cref{alg:scone-model}, f-gram embeddings are generated with $\Afgram$ during training and retrieved from a lookup table $\cF$ during inference.



\section{Challenges of Scaling Vocabulary Size in Embedding Layers}
\label{sec:scale_vocab}

Scaling the vocabulary size is the most straightforward way to enlarge an embedding layer, but we find that larger vocabularies degrade performance beyond a certain threshold and significantly increase accelerator usage during decoding. We pre-train GPT-2 models \citep{radford2019language} with three sizes of non-embedding parameters: 85M (small), 302M (medium), and 708M (large) on the WebText dataset \citep{openwebtext}, testing six vocabulary sizes ranging from 32,768 to 2,097,152. The tokenizers are trained using the BPE algorithm \citep{gage1994new, sennrich2015neural}. We follow the implementation in \citet{tao2024scaling}, which allows token merges across word boundaries. Each model is trained on 80 billion tokens. Since larger vocabularies produce fewer tokens for the same dataset, they effectively enable models to process more data. Additional implementation details are provided in Appendix~\ref{subsec:webtext_details}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.49\linewidth]{figs/openwebtext_vocab_size_bpc_loss.pdf}
    \caption{BPC of three model sizes on the validation set (lower is better). For all three model sizes, BPC initially improves as vocabulary size increases but eventually deteriorates.}
    \label{fig:bpc_vary_vocab}
\end{figure}

Figure~\ref{fig:bpc_vary_vocab} presents the average bits per character (BPC) on the WebText validation set. We report BPC instead of cross-entropy loss because the latter is sensitive to vocabulary size, with larger vocabularies typically producing higher losses. BPC, by contrast, is a common vocabulary-insensitive metric for comparing models trained with different tokenizers \citep{huang2024compression}. We observe that BPC for all three models initially improves with larger vocabulary sizes but eventually deteriorates.


\begin{figure}[h]
    \centering
    \includegraphics[width=0.49\linewidth]{figs/openwebtext_vocab_size_update_freq.pdf}
    \caption{Percentages of tokens (y-axis) that receive more than a given number of updates (x-axis), measured over 100 million training tokens. As the vocabulary size increases, tokens receive increasingly sparse updates.}
    \label{fig:update_freq_vary_vocab}
\end{figure}


Figure~\ref{fig:update_freq_vary_vocab} shows the percentages  of tokens that receive more than a given number of updates over 100 million training tokens. In standard embedding layers, gradients are directly backpropagated to the embedding vectors. With a fixed number of training tokens, larger vocabularies lead to fewer updates per token. For a vocabulary size of 2,097,152, only 7.3\% of tokens receive more than 100 updates, compared to 97.6\% for a vocabulary size of 32,768. This suggests that the performance drop for larger vocabularies may stem from sparse updates to per-token embedding vectors.

\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\linewidth]{figs/openwebtext_vocab_size_params_memory.pdf}
    \caption{Number of embedding layer parameters on the GPU and corresponding GPU memory usage. Computational costs increase linearly with vocabulary size.}
    \label{fig:gpu_cost_vary_vocab}
\end{figure}



In addition to performance degradation, increasing the vocabulary size significantly raises accelerator usage during the inference stage. This is because predicting the next token involves running a linear layer and softmax operation across the entire vocabulary to identify the closest embedding. Figure~\ref{fig:gpu_cost_vary_vocab} illustrates that both the number of embedding layer parameters stored on the GPU and the GPU memory cost increase linearly with vocabulary size. These costs are measured using a batch size of 1, a sequence length of 1024, and 16-bit precision. 







\section{Additional Experiments}
\label{sec:addition_results}

\subsection{More Results for Training on WebText}
\label{subsec:more_exp_webtext}

\begin{figure}
\centering
\begin{minipage}{.46\textwidth}
  \centering
  \includegraphics[width=.95\linewidth]{figs/openwebtext_maxngramsize_eval_loss_WebText.pdf}
  \captionof{figure}{Effect of the maximum f-gram length in $\Vfgram$, evaluated on the WebText validation split. }
  \label{fig:scale_max_ngram_size_appendix}
\end{minipage}
\hspace{0.05\textwidth}
\begin{minipage}{.46\textwidth}
  \centering
  \includegraphics[width=.93\linewidth]{figs/openwebtext_ngrammodel_size_eval_loss_WebText.pdf}
  \captionof{figure}{Evaluation perplexity on WebText as a function of the size of $\Afgram$.}
  \label{fig:scale_ngram_model_appendix}
\end{minipage}
\end{figure}


\paragraph{\boldmath Varying Maximum f-gram Length.}  In \Cref{subsec:max_ngram_size}, we discuss the impact of varying the maximum f-gram length in $\Vfgram$ and present results on Wikitext-103. We observe that a relatively small maximum length is sufficient, as long as it is not too small, otherwise, the number of available \ngram{n}s for ranking becomes too limited. Here, in \Cref{fig:scale_max_ngram_size_appendix}, we show the corresponding results on WebText, which exhibit similar trends. The left $y$-axis represents the evaluation loss (averaged over three seeds), with the leftmost star indicating baseline performance. The right $y$-axis shows the average length of matched f-grams. As the maximum size increases, the loss initially decreases but then plateaus with some fluctuations. Meanwhile, the matched length rises initially before stabilizing for larger values. 



\paragraph{\boldmath Varying $\Afgram$ Model Size.} In \Cref{subsec:ngram_model_size}, we discuss the impact of varying the size of $\Afgram$ on evaluation perplexity for Wikitext-103. We find that increasing the model size leads to further performance improvements for a fixed $|\Vfgram|$. In \Cref{fig:scale_ngram_model_appendix}, we present the results on WebText, which show a similar trend. Model sizes in the legend correspond to inference-time sizes on accelerators. Dashed lines and stars on the left represent baseline performance.  The evaluation perplexity improves as the size of $\Afgram$ grows.




\subsection{More Results for Training on OLMo Corpus}
\label{subsec:more_exp_dolma}

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.85\linewidth]{figs/olmo_all_loss_curves.pdf}
    \caption{Average perplexity on the OLMo evaluation mixture throughout training. Models with \SCONE enabled converge later, indicating stronger capacity, and achieve better perplexity.}
    \label{fig:olmo_all_training_curves}
\end{figure}

\paragraph{Loss Curves over Training.} In \Cref{sec:exps_dolma}, we pre-train four model variants with sizes of 0.7B, 1B, 1.3B, and 1.9B, evaluating \SCONE on the first three. Each model is trained for 200B tokens, uniformly sampled from the OLMo-tokenized training corpus. The training token count is roughly 10 times more than the compute-optimal token count for a 1B model suggested by \citet{hoffmann2022training}, to ensure near-convergence. \Cref{fig:olmo_all_training_curves} shows the evaluation loss throughout training. The loss curves indicate that models trained with \SCONE converge later, suggesting that it effectively expands model capacity.

\section{Implementation Details}\label{sec:implementation_details}

Here, we provide additional implementation details. While f-gram lookup is efficient for inference, it creates a bottleneck during training since at training time transformer models process all token positions in parallel. This leads to GPU idle time when fetching the longest matching f-gram on the fly. To remove this bottleneck, after we construct the set of f-grams ($\Vfgram$), we pre-scan the training sequences to tag the longest matching length for each token. During training, we can then directly retrieve the corresponding f-gram for forward computation with the $\Afgram$ model.

For the $\Afgram$ model, we use an absolute position embedding layer where the maximum position equals the longest \ngram{n} in $\Vfgram$. Within each batch, all f-grams are padded to the longest \ngram{n} length in that batch. We train all models with the bfloat16 precision.


\subsection{WebText}
\label{subsec:webtext_details}

\begin{table}[h]
    \centering
    \begin{tabular}{r|r r r}
        \toprule
        \textbf{Parameters (million)} & \textbf{d\_model} & \textbf{ffw\_size} & \textbf{n\_layers} \\
        \midrule
        128  & 1024  & 4096  & 6   \\
        204  & 1024  & 4096  & 12     \\
        491  & 1536  & 6144  & 12     \\
        759  & 1536  & 6144  & 24     \\
        589  & 1536  & 6144  & 18     \\
        1099  & 1536  & 6144  & 36    \\
        \bottomrule
    \end{tabular}
    \caption{Baseline model configurations for pre-training on WebText. For constructing the f-gram model ($\Afgram$), we vary the number of layers in the 128M, 491M, and 589M variants and discard the token embedding layer.}
    \label{tab:webtext_model_configs}
\end{table}


For pre-training on WebText \citep{openwebtext}, we follow \citet{radford2019language} and set the batch size and sequence length to 512 and 1024, respectively. \citet{radford2019language} do not specify the number of training tokens or optimizer details. We train the models for 80B tokens, roughly doubling the count in \citet{radford2018improving}. For optimization, we use AdamW \citep{loshchilov2017decoupled} with a weight decay of 0.1. Following \citet{hoffmann2022training}, we set the maximum learning rate to $2\times 10^{-4}$ and apply a cosine learning rate scheduler. We list the model configurations in \Cref{tab:webtext_model_configs}.



\subsection{OLMo Tokenized Training Corpus}
\label{subsec:dolma_details}

\begin{table}[h]
    \centering
    \begin{tabular}{r|r r r}
        \toprule
        \textbf{Parameters (million)} & \textbf{d\_model} & \textbf{ffw\_size} & \textbf{n\_layers} \\
        \midrule
         711 &  2048  &  8192  &  12   \\
        1014  &  2048   &  8192  &  18     \\
        1316  &   2048 &  8192   &  24     \\
        1920  &  2048  &  8192  &  36     \\
        \bottomrule
    \end{tabular}
    \caption{Baseline model configurations for pre-training on OLMo corpus. For constructing the f-gram model ($\Afgram$), we use the 711M and 1920M configurations and discard the token embedding layers.}
    \label{tab:olmo_model_configs}
\end{table}


For pre-training on the OLMo tokenized training corpus, we follow the optimizer settings for the 1B variant in \citet{OLMo} \footnote{\url{https://github.com/allenai/OLMo/blob/v0.4.0/configs/official/OLMo-1B.yaml\#L40}}. All models use a sequence length of 2048 and are trained on 200B tokens from sequences uniformly sampled from the corpus. We use DeepSpeed \citep{deepspeed_repo} with ZeRO stage 1 to reduce GPU memory usage. ZeRO stage 1 partitions the optimizer state across GPUs. Our hardware supports training models up to a training-time size of ~3B parameters. We list the model configurations in \Cref{tab:olmo_model_configs}.