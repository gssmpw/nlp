\section{Additional Related Work}\label{sec:add_related_work}

Mixture of Experts (MoE) and Memory Layers  are two well-established methods for scaling language models within a fixed FLOPS budget, as discussed below.

\paragraph{Mixture of Experts} MoE   replaces traditional feedforward layers with parallel `expert' layers, activating only one (or a few) per token via a lightweight router \citep{shazeer2017outrageously,lepikhin2021gshard,fedus2022switch,jiang2024mixtral,he2024mixture}. This allows scaling by increasing the number of experts without increasing active experts per token. However, all experts must reside on the accelerator, leading to higher memory usage.

\paragraph{Memory Layers} Memory layers store large sets of embeddings (continuous vectors) and retrieve nearest-neighbor embeddings during the forward pass via (approximate) similarity search \citep{weston2014memory,sukhbaatar2015end,lample2019large,berges2024memory}. These retrieved embeddings contribute to computations without adding to FLOPS. Despite advancements in similarity search \citep{lample2019large,johnson2019billion}, memory layers still need to reside on accelerators, which increases memory demands to impractical levels at larger scales \citep{berges2024memory}. Moreover, the embeddings in memory layers are typically updated through backpropagation, which also introduces sparse update challenges as memory scales.

Our approach focuses on input embedding layers, which we demonstrate can be efficiently offloaded from accelerators. This ensures constant memory usage and fixed FLOPS on the accelerator during inference.  Additionally, by modifying only the input embedding layer, our method integrates seamlessly with both MoE and memory layer techniques.





