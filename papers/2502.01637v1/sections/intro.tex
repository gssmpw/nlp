

\section{Introduction}\label{sec:intro}

Embedding layers in language models map discrete tokens to continuous vector representations \citep{mikolov2013efficient, sennrich2015neural}. These layers can be implemented as lookup tables, enabling efficient retrieval of embeddings using hash- or tree-based data structures.
This allows embedding layers to be offloaded to main memory or even secondary storage (e.g., disk) with minimal impact on inference speed. This is desirable, as main memory and secondary storage are significantly more cost-effective than accelerators (e.g., GPUs and TPUs~\citep{memory_disk_price}). These advantages drive our exploration of methods for scaling up embedding layers.

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.85\linewidth]{figs/olmo_headline.pdf}
    \caption{\small \textbf{(Top)} Perplexity (lower is better) on the OLMo \citep{OLMo} evaluation mixture. Inference-time FLOPS refer to the forward pass computation cost for four model sizes (0.7B, 1B, 1.3B, and 1.9B). With 10M f-grams, the 1.3B model matches the 1.9B baseline, while with 1B f-grams, the 1B model surpasses it. \textbf{(Bottom)} End-to-end token generation speed on a single A100 using vLLM \citep{kwon2023efficient}. Storing f-gram embeddings in main memory introduces negligible latency, while NVMe storage slows generation slightly but does not create a bottleneck.}
    \label{fig:olmo_scaling_result}
\end{figure}



\begin{figure*}[ht]
    \centering
    \includegraphics[width=.95\linewidth]{figs/ngram-embedding-illustration.pdf}
    \caption{Illustration of \SCONE (with a maximum \ngram{n} length of $3$). The term {\em f-grams} refers to the set of frequent \ngram{n}s (\Cref{sec:method}).}
    \label{fig:method_overview}
\end{figure*}



However, scaling the embedding layer by simply increasing the vocabulary size has limited benefits. The first issue is the coupling between the input embedding layer and the output (logits) layer:  (i) It is common to share weights between the input embedding and the output layer. In this case, the weights already reside in the accelerator memory as they are needed for logits computation, which eliminates the benefits of offloading the input embedding. (ii) 
Even when the weight \emph{parameters} are not shared, the weight \emph{shapes} are tied to the vocabulary size and embedding dimension. When scaling the input embedding by increasing the vocabulary size, as explored in prior studies \citep{wang2019improving, zheng2021allocating, liang2023xlm, tao2024scaling}, the inference cost also increases due to the size growth of the output embedding that is used to  compute logits \citep{dagan2024getting}. This scaling becomes computationally impractical beyond a vocabulary size of a few hundred thousands. 

The second issue is that the benefits of scaling the vocabulary diminish, even when computational costs can be mitigated by advances in accelerators or smarter algorithms~\citep{joulin2017efficient,shim2017svd}: Scaling leads to a large number of \emph{tail tokens}, that have low frequency in the training corpus. The training of their (input and output) embeddings  receives very few updates which results in representations that are of lower quality \citep{liao2021efficient, dou2024sailor}.  
Our experiments with GPT-2 models \citep{radford2019language} pre-trained on WebText \citep{openwebtext} confirm these limitations: Only 7.3\% of embedding vectors in a 2M vocabulary receive more than 100 updates over 100M training tokens, compared to 97.6\% for a 32K vocabulary. Additionally, we observe performance degradation and a linear increase in accelerator memory usage when the vocabulary size exceeds 1M (\Cref{sec:scale_vocab}).

\smallskip\noindent\textbf{Our contributions.}

In this paper we propose a novel approach to disentangle the input and output embeddings, bypassing those issues and enabling the effective input embedding scaling with minimal additional inference cost. Instead of increasing the size of the vocabulary, we augment each token in the base vocabulary with \ngram{n} contextualized variants. The \ngram{n}s are selected from a predefined set of frequently occurring \ngram{n}s, that we refer to as \emph{f-grams}. Those contextualized tokens are only used in input embedding computation, allowing us to build an augmented input embedding table with billions of entries without impacting the computation cost of the output layer. Furthermore, the embeddings for those contextualized tokens are generated from an embedding transformer model, referred to as \emph{f-gram model}, that is jointly trained (see \Cref{fig:method_overview} for an illustration). This allows us to obtain rich contextualized representations without being subjected to the sparse tail phenomenon of naively increasing the vocabulary size. 

With this novel design, we introduce two new directions for improving model performance: (i) increasing the number of cached f-gram embeddings and (ii) scaling up the f-gram model for learning those embeddings. Notably, both approaches preserve inference-time FLOPs. These directions enable us to fully leverage the precomputation and offloading of contextualized embeddings. We demonstrate that a 1B parameter model with \SCONE outperforms a baseline model requiring $\sim$2$\times$ more inference-time FLOPs. \Cref{fig:olmo_scaling_result} presents representative results from \Cref{sec:exps_dolma}.

To summarize, our contributions are as follows:
\begin{itemize}[nosep,topsep=-4pt]
    \item We propose \textsc{Scone}, a scalable approach to expand the embedding layer in language models (\Cref{sec:method}).
    \item We conduct extensive experiments to evaluate design choices and validate our method in large-scale pre-training setups (\Cref{sec:exps_openwebtext}). 
\end{itemize}

