


\section{Preliminaries}\label{sec:preliminary}

We focus on pre-training decoder-only language models using the causal language modeling objective, a standard recipe to train modern language models \citep{radford2019language,brown2020language}. This involves predicting the next token based solely on preceding tokens, enabling these models to handle diverse text generation tasks. 


We introduce formal notations to describe such model architectures. Following \citet{phuong22formal}, we prioritize clarity and omit details that are not essential for describing our method.
We assume that a vocabulary $\Vtoken$ of tokens has already been defined.
The {\em token embedding layer} is parameterized by a function $\cT : \Vtoken \to \R^d$ that maps any token in $\Vtoken$ to an embedding vector in $\R^{d}$, where $d$ is the embedding dimension. 
We abstractly view a transformer model as $\cA : (\R^{d})^{\le N_{\max}} \to \R^{d}$, which maps a sequence of vectors in $\R^d$ of length at most $N_{\max}$ to a single embedding vector\footnote{The output embedding vector is typically the last token embedding, which is commonly used for next-token prediction. In this work, for the f-gram model, we use the last token embedding as the contextualized embedding of an input token.} in $\R^d$. The size of a transformer model refers to the number of parameters in the model.
A prediction head $\cD : \R^d \to \Delta_{\Vtoken}$ maps the embedding vector to a probability distribution over tokens in the vocabulary $\Vtoken$ (where $\Delta_{\Vtoken}$ denotes the probability simplex). Collectively these pieces yield a basic next-word prediction model $M_{\cT,\cA,\cD}$, which, given an input sequence $(\sigma_1, \ldots, \sigma_m) \in \Vtoken^m$ of tokens, produces a distribution over the next token $\hat{\sigma}_{m+1}$, which can be used for auto-regressive sequence prediction. For completeness, we present the pesudocode of the basic next-word prediction model in \Cref{alg:basic-model} (\Cref{sec:add_algorithms}).




{\em Remarks on Token Embedding Layers.}
The number of parameters in a token embedding layer $\cT : \Vtoken \to \R^d$ is the product of the vocabulary size $|\Vtoken|$, which ranges from a few dozen to hundreds of thousands in current models \citep{radford2019language, dubey2024llama}, and the embedding dimension $d$, which is typically ranges in a few thousands \citep{brown2020language, touvron2023llama}. 
Often, the prediction head $\cD : \R^d \to \Delta_{\Vtoken}$ computes the similarity between the model's output embedding and all vocabulary embeddings to predict the next token, thereby sharing the embeddings in the token embedding layer.

Embedding layers admit highly efficient implementations. The key-value pairs can be organized with hash- or tree-based data structures, which allow the embedding layer to operate with access cost that is either constant or logarithmic in the number of embedding vectors.
These low cost methods, in principle, allow the embedding layer to be offloaded from accelerators with minimal latency impact (see \Cref{subsec:inference_cost}). Most implementations, however, store the token embedding layer in accelerator memory, since the token embeddings are also required for applying the prediction head.