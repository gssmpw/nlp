\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{figs/openwebtext_maxngramsize_eval_loss_Wikitext-103.pdf}
    \caption{Effect of the maximum f-gram length $n$ in $\Vfgram$, on perplexity and matched length. The left $y$-axis shows perplexity (averaged over three seeds), where the leftmost star indicates baseline performance. The right $y$-axis shows the average length of matched f-grams. The perplexity decreases as we increase the maximum length from 2 to 4, but then plateaus with some fluctuation. Similarly, the average matched length initially rises but stabilizes after size 4.}
    \label{fig:scale_max_ngram_size}
\end{figure}

\section{Experimental Evaluation}\label{sec:exps_openwebtext}

\subsection{Design Choices}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{figs/openwebtext_numgrams_eval_loss.pdf}
    \caption{Evaluation perplexity as a function of $|\Vfgram|$. Model sizes in the legend correspond to the main model sizes, including the token embedding layer. The dashed lines and leftmost stars indicate baseline performance.  Perplexity decreases overall with increasing sizes of $\Vfgram$. }

    \label{fig:scale_num_ngrams}
\end{figure*}

We analyze three key hyperparameters: (i) the maximum f-gram length in $\Vfgram$, (ii) the number of f-grams used $|\Vfgram|$, and (iii) the $\Afgram$ model size. We use the released GPT-2 tokenizer, which has $|\Vtoken| = 50,\!257$, and train on the WebText dataset \citep{openwebtext}. The tokenized corpus  contains 9B training tokens, from which we extract f-grams using the method in \Cref{subsec:key_discovery}.

We consider three main model sizes with 76M, 340M, and 510M non-embedding parameters. Including the token embedding layer, the total parameter count increases to 128M, 419M, and 589M, respectively. The embedding dimensions for these models are 1024, 1536, and 1536, respectively. These models are either trained using only the token embedding layer as baselines or with an additional $\Afgram$ when \SCONE is applied.  Following \citet{radford2019language}, we use a batch size of $512$ and a sequence length of $1024$. Since \citet{radford2019language} do not specify the number of training steps, we train all models for 80B tokens, roughly twice the number of training tokens in \citet{radford2018improving}. For evaluation, we use the validation split of WebText and WikiText-103 \citep{merity2016pointer}, one of the largest downstream datasets in \citet{radford2019language}. Additional implementation details are in \Cref{subsec:webtext_details}.



\subsubsection{Varying the Maximum f-gram Length}\label{subsec:max_ngram_size}


We explore the effect of varying the maximum length $n$ of f-grams in $\Vfgram$. We vary $n$ from 2 to 8 while fixing the total number of f-grams to $|\Vfgram|=$20M. This means that we obtain a {\em frequency cutoff} (the minimum frequency of an \ngram{n} in $\Vfgram$) that increases with $n$: This value was $7$ for $n=2$ and 108 for $n=8$.
We then measure for each $n$ (i) evaluation perplexity and (ii) the average length of a matched f-grams on Wikitext-103. Our findings are reported in \Cref{fig:scale_max_ngram_size}. We observe that evaluation perplexity increases for $n$ between $2$ and  $4$ and then plateaus with some fluctuations.
A similar trend is observed for the {\em average match length}, which is the average length of f-gram found in \SCONE method (\Cref{alg:scone-first-stage}) for each token in the evaluation set: it rises from 2 to 4 before stabilizing. This is likely because longer f-grams occur less frequently than shorter ones after ranking. Even with a higher maximum \ngram{n} length, most selected entries remain short. Additionally, longer f-grams from the training corpus are less likely to match downstream data. Findings on the validation split of WebText  (\Cref{subsec:more_exp_webtext}) exhibit a similar pattern, though the average matched length plateaus later, at length 6.

Considering these findings, for the experiments in the remainder of this paper, we set the maximum f-gram length to $n=5$ unless stated otherwise.

\subsubsection{Varying the Number of f-grams}


We observe consistent improvements in language modeling performance as we scale up $|\Vfgram|$.  To implement the f-gram model $\Afgram$, we replicate the baseline model architecture but remove the token embedding layer. This results in the size of $\Afgram$ matches the baseline model's non-embedding parameters. 

\Cref{fig:scale_num_ngrams} shows the evaluation perplexity as $|\Vfgram|$, the number of f-gram embeddings, increases from 512K to 100M. On the WebText validation split, the perplexity decreases consistently as the number of f-gram embeddings increases. Similarly, on WikiText-103, the perplexity generally decreases with more f-gram embeddings, though minor fluctuations are observed. 

In \Cref{fig:scale_num_ngrams}, we include three additional baselines where the non-embedding parameters of the three main models are doubled, resulting in models with 204M, 759M, and 1099M parameters for the original 128M, 419M, and 589M models, respectively. This ensures that the total parameter count of each baseline matches the training-time parameter count when \SCONE is applied. With 100M f-gram embeddings, the 419M and 589M models trained with \SCONE match or surpass the performance of the 759M and 1099M baselines, respectively, despite using only half as many non-embedding parameters during inference.

\subsubsection{Varying the Size of the $\Afgram$ Model}\label{subsec:ngram_model_size}

\begin{figure}[ht]
    \centering
    \includegraphics[width=1.0\linewidth]{figs/openwebtext_ngrammodel_size_eval_loss_Wikitext-103.pdf}
    \caption{Evaluation perplexity on Wikitext-103 as a function of the size of $\Afgram$. Model sizes in the legend correspond to the main model sizes, including the token embedding layer. Dashed lines and stars on the left represent baseline performance.  The perplexity improves as the size of $\Afgram$ grows.}
    \label{fig:scale_ngram_model}
\end{figure}

We observe that, for a fixed $|\Vfgram|$, scaling up the $\Afgram$ model size provides a new way to improve language modeling performance. We vary the model size by changing the number of layers in the main model architecture. For each $\Amain$ model size, we evaluate four $\Afgram$ model sizes: 0.5x, 1x, 2x, and 3x the non-embedding parameters of the main model. We set $|\Vfgram|$ to be 100M. \Cref{fig:scale_ngram_model} presents the evaluation perplexity on Wikitext-103. The observations on WebText validation split are similar, and we present the results in \Cref{subsec:more_exp_webtext}.

The results in \Cref{fig:scale_ngram_model} show that the perplexity generally decreases as the $\Afgram$ model size increases, although the improvements become smaller as the model size grows larger. For instance, with the 419M main model, a 170M $\Afgram$ model improves the perplexity from 26.1 to 23.4, outperforming the 589M baseline (24.7) by a clear margin. Further scaling of the $\Afgram$ model to 1020M (resulting in 1439M total parameters during training) lowers the perplexity to 22.1, which is slightly higher than the 1099M baseline (21.9). This suggests that scaling up the $\Afgram$ model size initially provides a better scaling curve, but beyond a certain size, it yields a less optimal scaling curve compared to directly scaling up $\Amain$. However, the latter also increases inference-time FLOPS, whereas scaling $\Afgram$ does not, as it is replaced with an off-accelerator lookup table during inference. This highlights our method as a novel way to leverage additional training compute while maintaining fixed inference-time compute.
