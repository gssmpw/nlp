\subsection{Scaling Up the Training Corpus}\label{sec:exps_dolma}

\begin{table*}[htbp]
\caption{Perplexity (lower is better) on the OLMo evaluation mixture. All models are trained for 200B tokens. \SCONE consistently improves language modeling performance across all evaluation corpora. With 10M $\Vfgram$, a 1.3B model matches the performance of the 1.9B baseline. Similarly, with 1B $\Vfgram$, a 1B model matches the 1.9B baseline. }
\label{table:olmo_breakdown}
\centering
\setlength{\tabcolsep}{5pt}
\resizebox{1.0\linewidth}{!}{
\begin{tabular}{c|cccccccccccc}
\toprule
 \textbf{Model size} & \textbf{c4-en} & \textbf{books} & \textbf{common-crawl} & \textbf{pes2o} & \textbf{reddit} & \textbf{stack} & \textbf{wiki} & \textbf{ice} & \textbf{m2de-s2orc} & \textbf{pile} & \textbf{wikitext-103} & \textbf{Average} \\
 \midrule
 \midrule
  \textbf{1B baseline} &  16.813 &  21.570 & 16.752 & 11.682 & 22.612 &  3.360 & 14.453  & 15.281  &  27.900 & 10.429   & 16.053  & 16.082 \\
  \midrule
 +10M $\Vfgram$ (0.6B $\Afgram$) &  16.087 &  20.963 & 16.039 & 11.270 & 21.797 &  3.274 & 13.777  & 14.979  &  26.361 & 10.128   & 15.371  & 15.459 \\
   \midrule
  +10M $\Vfgram$ (1.8B $\Afgram$) &  15.727 &  20.429 & 15.473 & 11.124 & 21.388 &  3.231 & 13.454  & 14.709  &  25.785 & 9.956   & 15.104  & 15.125 \\
  \midrule
  +1B $\Vfgram$ (0.6B $\Afgram$) &   15.846 &  20.593 & 15.684 & 11.071 & 21.411 &  3.213 & 13.543  & 14.702  &  26.026 & 9.889   & 15.077  & 15.187 \\
  \midrule
  +1B $\Vfgram$ (1.8B $\Afgram$) &  15.158 &  19.680 & 14.857 & 10.761 & 20.757 &  3.133 & 12.964  & 14.220  &  24.958 & 9.553   & 14.354  & 14.581 \\
 \midrule
 \midrule
  \textbf{1.3B baseline} & 15.994 &  20.157 & 15.921 & 11.148 & 21.634 &  3.248 & 13.721  & 14.651  &  26.583 & 9.927   & 15.143  & 15.284 \\
  \midrule
  +10M $\Vfgram$ (0.6B $\Afgram$) &  15.509 &  19.816 & 15.407 & 10.887 & 21.022 &  3.192 & 13.260  & 14.372  &  25.450 & 9.757   & 14.616  & 14.844 \\
  \midrule
  +10M $\Vfgram$ (1.8B $\Afgram$) & 15.193 &  19.587 & 14.995 & 10.795 & 20.735 &  3.171 & 13.071  & 14.272  &  25.258 & 9.674   & 14.438  & 14.654 \\
  \midrule
  +1B $\Vfgram$ (0.6B $\Afgram$) & 15.270 &  19.510 & 15.106 & 10.707 & 20.763 &  3.139 & 13.073  & 14.177  &  25.009 & 9.546   & 14.397  & 14.609 \\
  \midrule
  +1B $\Vfgram$ (1.8B $\Afgram$) & 14.803&  18.996 & 14.541 & 10.502 & 20.296 &  3.085 & 12.637  & 13.971  &  24.533 & 9.357   & 13.971  & 14.245 \\
  \midrule
  \midrule
  \textbf{1.9B baseline} & 15.270 &  19.017 & 15.184 & 10.719 & 20.752 &  3.163 & 13.119  & 14.095  &  25.461 & 9.570   & 14.229  & 14.598 \\
\bottomrule
\end{tabular}
}
\end{table*}


We demonstrate that the two new scaling aspects of \SCONE, namely size of $|\Vfgram|$ and parameters in $\Afgram$, apply to large-scale pre-training. The cutoff frequencies are 21,956 for 10M f-grams and 70 for 1B f-grams.

We use a base architecture with 18 decoder blocks and 1B parameters. Following the OLMo-1B architecture, we set a context length $N_{\max} = 2048$
and embedding dimension $d=2048$. To explore parameter scaling, we vary the number of layers, creating four model variants: 0.7B, 1.0B, 1.3B, and 1.9B. \SCONE is evaluated on the first three, while the 1.9B model serves solely as a baseline. We test \SCONE with two sizes of $\Afgram$, 0.6B and 1.8B, matching the non-embedding parameters of the 0.7B and 1.9B variants. All models are trained on 200B tokens, with sequences of tokens sampled uniformly from the corpus. While \citet{hoffmann2022training} suggests $\sim$20B tokens for a compute-optimal 1B model, we use a larger corpus to approach convergence. Training loss curves are provided in \Cref{subsec:more_exp_dolma}, with implementation details in \Cref{subsec:dolma_details}.



\Cref{fig:olmo_scaling_result} presents the perplexity on the OLMo evaluation mixture\footnote{\url{https://github.com/allenai/OLMo/blob/v0.4.0/configs/official/OLMo-1B.yaml\#L90}}, which comprises 11 diverse corpora, including web crawl data, literature, online forums, scientific writing, coding, and more. \Cref{table:olmo_breakdown} presents the performance breakdown for the 1B, 1.3B, and 1.9B variants. In \Cref{fig:olmo_scaling_result}, the X-axis represents inference-time forward FLOPS per sequence, computed following the breakdown in \citet{hoffmann2022training}. Results indicate that increasing both $|\Vfgram|$ and the size of $\Afgram$ consistently  improves performance across all evaluation corpora. Additionally, in \Cref{fig:olmo_scaling_result} we report end-to-end token generation speed using  the vLLM framework \citep{kwon2023efficient} with a batch size of 1, showing that even for large values of $|\Vfgram|$, embedding retrieval is not a bottleneck.

For a representative finding, in the 1B model variant, the baseline achieves an average perplexity of 16.082. Setting $|\Vfgram|$ to 10M improves the perplexity to 15.459 with a 0.6B $\Afgram$ model and to 15.125 with a 1.8B $\Afgram$ model, the later outperforming the 1.3B baseline (15.284). Increasing $|\Vfgram|$ to 1B further improves perplexity to 15.187 and 14.581 for the 0.6B and 1.8B $\Afgram$ models, respectively, surpassing the 1.9B baseline (14.598) despite requiring only half the inference-time FLOPS.
