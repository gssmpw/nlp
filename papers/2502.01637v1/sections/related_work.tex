\section{Related Work}\label{sec:related_work}

We discuss additional related work, including Mixture of Experts and Memory Layers, two established methods for scaling language models under a fixed FLOPS budget, in \Cref{sec:add_related_work}.


\smallskip\noindent\textbf{Contextualized Word Embeddings.} Words can have different meanings depending on context. Prior work has incorporated context into word embeddings, either from the entire sequence \citep{mccann2017learned,peters2018deep} or short \ngram{n}s \citep{gupta2019better}, before applying them to downstream tasks. Modern language models inherently use contextualized token embeddings, leveraging attention mechanisms. In this study, we extend the embedding layer to include contextualized f-gram embeddings for each token. A key novelty is that our approach allows embeddings to be precomputed and offloaded from accelerators, providing contextual embeddings for each token without increasing inference-time FLOPS.

\smallskip\noindent\textbf{Scaling of Vocabulary Size.} \citet{tao2024scaling} show that larger models benefit from larger vocabularies, aligning with the trend of advanced language models whose vocabulary sizes have increased from a few dozen thousand \citep{devlin2019bert,radford2019language} to a few hundred thousand \citep{google2024gemma,adler2024nemotron,dubey2024llama,deepseek2024deepseek}. However, the optimal vocabulary size predicted by \citet{tao2024scaling} is still much smaller than the model size, e.g., a vocabulary size of 216K for a 70B  parameter model. These findings motivate us to extend the embedding layer without changing the vocabulary size, fully exploiting the lookup nature of the input embedding layer.

\smallskip\noindent\textbf{Tokenization in Language Models.} Our method assumes a predefined vocabulary from a trained tokenizer. Several popular algorithms exist for training tokenizers \citep{sennrich2015neural, wu2016google, kudo2018subword, kudo2018sentencepiece}. In this work, we use a BPE tokenizer, following prior seminal works \citep{radford2019language, touvron2023llama}. However, our method is not tied to any specific tokenization algorithm and can be applied seamlessly to others.

Tokenization-free language models have also been widely explored \citep{kim2016character, choe2019bridging, xue2022byt5, yu2023megabyte, wang2024mambabyte, deiseroth2024t, meta2024large, pagnoni2024byte}. While we have not tested our method on tokenization-free models, we believe our core idea—introducing an off-accelerator embedding layer by precomputing embeddings for frequent input patterns—remains applicable.


\smallskip\noindent\textbf{A Concurrent Work.} We learned about a concurrent study by \citet{huang2025over} upon the release of our work. They also explore scaling the embedding layer in language models. Both works propose decoupling the input embedding layer size from the output layer by introducing an additional embedding layer at input, and observe similar performance gains as the size of the additional embedding layer increases. However, the method in \citet{huang2025over} differs significantly from ours, introducing trade-offs that we discuss below.

\citet{huang2025over} instantiate the additional embedding layer during training. Similar to our work, they also leverage a short preceding \ngram{n} to determine the embedding for each input token. A key distinction is that they use all \ngram{n}s without differentiating by frequency. Since the number of \ngram{n}s is typically much larger than the number of embedding vectors, they hash each \ngram{n} to obtain an index in the additional embedding layer, leading to multiple \ngram{n}s sharing the same embedding vector. We speculate that this also mitigates the sparse update issue discussed in \Cref{sec:scale_vocab}.

The method proposed in \citet{huang2025over} has both advantages and drawbacks compared to ours. On the positive side, it directly backpropagates gradients to an instantiated embedding layer, eliminating the need for a separate embedding model. However, instantiating a large embedding layer during training introduces other challenges. At training, transformer models process all tokens in parallel, generating a high volume of read and write queries to the additional embedding layer. This makes offloading the layer from the accelerator less practical, thereby poses significant memory challenges for accelerators. While \citet{huang2025over} mitigate this by sharding the embedding layer across accelerators via tensor parallelism, this still limits the number of embeddings, with the largest tested configuration in \citet{huang2025over} being 12.8M. In contrast, our method avoids instantiating embeddings during training, allowing us to scale to 1B embeddings. Additionally, our design of $\Afgram$ introduces a new axis for leveraging extra training compute while keeping both the additional embedding layer size and inference-time FLOPS fixed.
