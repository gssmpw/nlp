\section{Conclusion}\label{sec:conclusion}

We introduce \SCONE, a scalable approach for generating \ngram{n} contextualized embeddings for each input token without increasing inference-time FLOPS. These embeddings are learned during training and cached in off-accelerator storage for inference. SCONE enables two new aspects for scaling language models: (1) scaling the number of cached contextualized embeddings and (2) scaling the model size for learning them, both while maintaining fixed inference-time FLOPS. This is especially useful for latency-sensitive applications \citep{jones2021scaling,snell2024scaling} and reducing serving costs. Future work could explore scaling embedding layers for other modalities; for instance, recent research underscores the importance of vocabulary size in visual modeling \citep{yu2023language}.