\section{\SCONE Architecture}
\label{sec:method}

\begin{algorithm}[t]
\caption{Constructing a set of f-grams $\Vfgram$.}
\label{alg:n-gram-vocab}
\begin{algorithmic}
\STATE {\bf Parameters:} $S$: desired size of $\Vfgram$.
\STATE {\bf Input:} $\{(\sigma_1, \ldots, \sigma_{N_{\max}})^{(i)}\}$ : token sequences from training set, where each sequence is from $\Vtoken^{N_{\max}}$.
\STATE {\bf Output:} $\Vfgram \subseteq \Vtoken^{[2,n]}$: set of f-grams of size $S$.
\FOR{$k = 2, \ldots, n$}
    \FOR{$\omega := (\sigma^{'}_1, \ldots, \sigma^{'}_k) \in \Vtoken^{k}$}
        \STATE $C_{\omega} \gets$ the number of times $\omega$ appears in all sequences $\{(\sigma_1, \ldots, \sigma_{N_{\max}})^{(i)}\}$.
    \ENDFOR
\ENDFOR
\STATE Let $\omega_1, \omega_2, \ldots $ be list of elements of $\bigcup_{k=2}^n \Vtoken^{k}$, sorted such that $C_{\omega_1} \ge C_{\omega_2} \ge \cdots$, breaking ties arbitrarily.
\RETURN $\{\omega_1, \ldots, \omega_{S}\}$: set of f-grams of size $S$ 
\end{algorithmic}
\end{algorithm}

We introduce the \SCONE architecture that uses a new embedding layer for frequently occurring \ngram{n}s of tokens. \Cref{fig:method_overview} shows the high-level approach.

First we construct a set $\Vfgram \subseteq \Vtoken^{[2, n]} := \bigcup_{k=2}^n \Vtoken^{k}$ of frequently occurring \ngram{k}s of length at most $n$, that we term {\em f-grams}; throughout this paper, we use $n$ to denote the maximum length of f-grams considered. \Cref{alg:n-gram-vocab} illustrates this process. That said, \Cref{alg:n-gram-vocab} is for illustration purposes only; in practice, we use a more efficient implementation requiring only $(n-1)$ linear scans over the training corpus, as detailed in \Cref{subsec:key_discovery}. Notably, the counting and ranking process resembles continuing the training of a BPE tokenizer \citep{sennrich2015neural} with the existing vocabulary.

\begin{algorithm}[t]
\caption{\SCONE method $F_{\cT, \Vfgram, \train{\Afgram} | \infer{\cF}}$.}
\label{alg:scone-first-stage}
\begin{algorithmic}
\STATE {\bf Parameters:}\begin{itemize}[itemsep=-2pt]
    \item $\cT : \Vtoken \to \R^d$: token embedding layer,
    \item $\Vfgram \subseteq \Vtoken^{\le n}$: set of f-grams,
    \item \textbf{\textsc{Training}}: f-gram transformer model
    \begin{itemize}[leftmargin=8mm,label=$\triangleright$,itemsep=-3pt,topsep=-4pt]
        \item \train{$\Afgram : (\R^d)^{\le n} \to \R^d$},
    \end{itemize}
    \item \textbf{\textsc{Inference}}: f-gram embedding layer
    \begin{itemize}[leftmargin=8mm,label=$\triangleright$,itemsep=-3pt,topsep=-4pt]
        \item \infer{$\cF : \Vfgram \to \R^d$}.
    \end{itemize}
\end{itemize}
\STATE {\bf Input:} A sequence of $m$ tokens $(\sigma_1, \ldots, \sigma_m) \in \Vtoken^m$.
\STATE {\bf Output:} Embeddings $(\be_1, \ldots, \be_m) \in (\mathbb{R}^d)^m$.
\FOR{$i = 1, 2, \ldots, m$}
    \STATE $j \gets$ smallest $j' < i$ s.t. $(\sigma_{j'}, \ldots, \sigma_i) \in \Vfgram$ if such a $j'$ exists, otherwise $i$.
    \IF{$j = i$}
        \STATE $\be_i \gets \cT(\sigma_i)$.
    \ELSE
        \STATE $\be_i \gets \begin{cases}
            \Afgram(\cT(\sigma_{j}), \ldots, \cT(\sigma_i)) & \text{at training}\\
            \cF(\sigma_{j}, \ldots, \sigma_i) & \text{at inference}
        \end{cases}$
    \ENDIF
\ENDFOR
\RETURN $(\be_1, \ldots, \be_m)$
\end{algorithmic}
\end{algorithm}


Next, we define the \SCONE method, which maps a given sequence of tokens to a sequence of embedding vectors. \SCONE method behaves differently at training and at inference, as described in \Cref{alg:scone-first-stage}. At training, it is parameterized by an {\em f-gram transformer model} $\Afgram$. However, at inference, it is parameterized instead by an {\em f-gram embedding layer} $\cF : \Vfgram \to \R^d$ that maps the set of f-grams obtained above to embedding vectors. This embedding layer is implemented by caching the outputs of $\Afgram$ for all f-grams in $\Vfgram$ and storing them in a hash- or tree-based data structure for efficient retrieval.





The embeddings produced by \SCONE are then passed to a standard transformer model $\Amain : (\R^d)^{\le N_{\max}} \to \R^d$, referred to as the \emph{main model}, followed by a prediction head $\cD : \R^d \to \Delta_{\Vtoken}$. Together, these components form the end-to-end process for next-word prediction with \SCONE. We present the corresponding pseudocode in \Cref{alg:scone-model} in \Cref{sec:add_algorithms}.








In the rest of this section, we will discuss the motivations behind these design decisions and provide further implementation details.


\subsection{\boldmath BPE-Style Discovery of f-grams}
\label{subsec:key_discovery}
The construction of $\Vfgram$, as defined in \Cref{alg:n-gram-vocab}, can be implemented efficiently with $(n-1)$ linear scans over the training corpus.
We perform one scan for each $k \in [2, n]$, starting with \ngram{2}s.
In subsequent scans, we impose a minimum frequency threshold of 5 to reduce memory usage. At the $(k+1)_{th}$ scan, the set of \ngram{k}s from the previous scan allows us to skip any \ngram{(k+1)} candidates that cannot meet the threshold. Specifically, if an \ngram{(k+1)} surpasses the threshold, its $k$-suffix or prefix must appear at least as many times. \Cref{fig:num_ngrams} shows how the number of unique \ngram{k}s (up to \ngram{6}s) grows as the training corpus scales from a few billion to one trillion tokens. Finally, all found \ngram{k}s (for $k \in [2, n]$) are ranked by frequency, and the top $S$ are selected as keys, where $S=|\Vfgram|$ is the target number of f-grams.

Our procedure for counting and ranking the \ngram{n}s is analogous to continuing a BPE tokenizerâ€™s training on an existing vocabulary. In each BPE iteration \citep{gage1994new,sennrich2015neural}, the frequencies of all token pairs (\ngram{2}s) are counted and the most frequent pair is merged to form a new token, expanding the vocabulary by one. However, merging and recounting pairs repeatedly is prohibitively expensive for large corpora. Instead, we simply collect and sort all \ngram{n}s up to a small $n$. 
\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{figs/olmo_num_ngrams.pdf}
    \caption{Number of unique $2$- to $6$-grams that appear at least five times. We vary the size of the corpus by uniformly sampling sequences from the OLMo tokenized training corpus \citep{soldaini2024dolma}.} 
    \label{fig:num_ngrams}
\end{figure}

\subsection{\boldmath Learning f-gram Embeddings with $\Afgram$}
\label{subsec:train_embedding}

We motivate our use of $\Afgram$ by considering the alternative of directly backpropagating gradients to a large embedding table.
The issue with the alternative approach is that it does not exploit the dependencies between \ngram{n}s and therefore suffers from fewer updates per embedding. We explored this by pre-training GPT-2 models with token vocabulary sizes ranging from 32K to 2M. As vocabulary size increases, token embedding updates become sparser, which eventually degrades performance.  For example, when training over 100M tokens, 97.6\% of the tokens in a 32K vocabulary receive more than 100 updates. In contrast, with a vocabulary of 2M, only 7.3\% of the tokens reach this threshold.  See \Cref{sec:scale_vocab} for more details. The sparsity makes it extremely challenging to learn an embedding table by directly backpropagating gradients to the embeddings.
\SCONE solve this problem by parameterizing the embeddings with a f-gram transformer $\Afgram$, avoiding the sparse update issue.


\SCONE jointly trains the $\Afgram$ model with the main model $\Amain$ and the token embedding layer $\cT$. This overcomes the sparse updates issue but also introduces additional compute costs. For each $\omega\in\Vfgram$, the computation is the same as that of  processing a short sequence of length $|\omega|$ through a standard transformer. Since $|\omega|$ is small ($|\omega|\leq 5$ for most of our experiments), the primary overhead comes from the feed-forward layer.  


During inference, the f-gram embedding layer $\cF$ can be precomputed and stored in a lookup table, offloaded to CPU memory or secondary storage for efficient retrieval. Meanwhile, the token embedding layer $\cT$ remains on the accelerator for decoding. This design leverages the low complexity of embedding layers to enrich token representations without increasing decoding costs.

Importantly, \SCONE introduces a novel approach to improving performance under a fixed inference-time FLOPS budget. Prior work shows that increasing training compute beyond a compute-optimal threshold yields diminishing returns for a fixed model size \citep{hoffmann2022training}. A common strategy to utilize extra training compute is scaling up both model size and compute, but this typically raises inference-time FLOPS as well. In contrast, our method allows the $\Afgram$ model to benefit from greater training compute without increasing inference-time FLOPS.

\subsection{Space Usage and Query Latency}
\label{subsec:inference_cost}

We evaluate the space usage and query latency of the f-gram embedding layer under various configurations.  We show that latency is not a bottleneck for  language model inference and the space costs are low due to the use of relatively inexpensive system memory or secondary storage.

Using the setup described in \Cref{sec:exps_dolma}, we set the maximum \ngram{n} length to 5 and experiment with $|\Vfgram|$ being 10M, 100M, and 1B with embedding dimension of $d = 2048$ with 16-bit precision per floating point value. Experiments were conducted on a workstation with 64 Intel Xeon CPU cores and 512 GB of memory. Space and latency were measured for both in-memory and on-disk storage. In memory, embeddings are stored as a single matrix with a hash dictionary mapping f-grams to indices, while on-disk storage uses the Lightning Memory-Mapped Database \citep{lmdb} to directly store f-gram and embedding pairs on NVMe solid-state drives.


\begin{table} [h]
    \caption{Space usage of the f-gram embedding layer $\cF$, along with cost for memory and NVMe solid-state drives \citep{memory_disk_price}.}
\label{tbl:space}
\centering
\small
\renewcommand{\arraystretch}{1.85}
\begin{tabular}{ P{2.0cm}|P{2.4cm} | P{2.4cm} }
 \hline \hline
  \# of \ngram{n}s       & System memory & Solid-state drive 		 \\\hline
$10^{7}$    &   41.4 GB  &	  77.3 GB 	\\\hline
$10^{8}$    &   413.6 GB  &  766.8 GB 	\\\hline
$10^{9}$    &   (does not fit)  &	7665.4 GB 	\\\hline\hline

Price (per GB) & $\sim$ 2 USD  & $\sim$ 0.1 USD  \\\hline \hline
\end{tabular} 
\end{table}

\Cref{tbl:space} summarizes the space usage for both storage methods. In both cases, the space required increases linearly with the number of embedding vectors. 
The 10M and 100M f-gram embedding layers are able to fit within CPU memory, with the 10M layer requiring 41.4 GB. For on-disk storage, there is additional overhead as the same 10M layer occupies 77.3 GB storage.


\begin{figure}[ht]
    \centering
    \includegraphics[width=1.0\linewidth]{figs/olmo_query_latency.pdf}
    \caption{Amortized per-token query latency (ms), averaged over 100,000 batches. The latency spike from batch size 1 to 2 when reading from system memory is due to batch operator overhead, which is less pronounced for solid-state drives. }

    \label{fig:latency}
\end{figure}

\Cref{fig:latency} shows the latency of retrieving embeddings with different batch sizes. Latency is measured as the end-to-end time from loading a batch of tokens to the f-gram embeddings (\Cref{alg:scone-first-stage}) being ready on GPU. CPU cache is cleared before each test, and up to 4 queries are made per token to identify the longest matching \ngram{n} (with a maximum length of 5). For in-memory storage, sequential queries suffice as they are not the bottleneck. In contrast, for on-disk storage, we make parallel  queries to the database. At a batch size of 1, the latency for a 10M f-gram embedding layer on the NVMe drive is 1.1ms, increasing to 2.3ms for a 1B f-gram embedding layer. This is well below the latency threshold for LLM inference, as typical commercial APIs offer a generation speed of $\sim$100 tokens per second, corresponding to $\sim$10ms per token \citep{generation_speed}. Larger batch sizes further improve efficiency, with a batch size of 16 reducing the amortized per-token latency to 0.5ms. In-memory access is much faster:
for a 100M f-gram embedding layer and a batch size of 16, the amortized per-token latency is only 0.017ms.






