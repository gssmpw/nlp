%%% Constants that control formatting.
\def\Blind{0}
\def\CameraReady{1}
\newcommand{\version}{\CameraReady}

\def\Comments{0}  % Set to 0 for hiding todo comments and restoring margin

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}

\usepackage{subcaption}
\usepackage{booktabs} % for professional tables
\usepackage{makecell}
% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}

\usepackage[hang,flushmargin]{footmisc} % footnote no indentation

\usepackage[noend]{algorithmic} % Or \usepackage[noend]{algpseudocode}
% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}




\ifnum\version=\Blind
    \usepackage{sty_files/icml2025}            % Blind submissions
\else
    \usepackage[accepted]{sty_files/icml2025}  % Camera ready
\fi

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{array}       % For defining custom column types (e.g., P{})
\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}
\usepackage{diagbox}     % For the \diagbox command (diagonal split cells)
\usepackage{caption}     % For styling captions (optional, included by default)

% if you use cleveref..
\usepackage[capitalize,noabbrev,nameinlink]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\newcommand{\ngram}[1]{$#1$-gram}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Scaling Embedding Layers in Language Models}

\makeatletter
\def\@fnsymbol#1{\ensuremath{\ifcase#1\or \dagger \or  \ddagger\or
   \mathsection\or  \text{*}\or \mathparagraph \or  \| \or **\or \dagger\dagger
   \or \ddagger\ddagger \else\@ctrerr\fi}}
\makeatother



%%%% Custom macros:
\input{sty_files/macros}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
%\icmltitlerunning{\boldmath Scaling Transformer Models Vocabulary using $n$-gram Embeddings}
\renewcommand{\thefootnote}{\fnsymbol{footnote}} 
\begin{document}

% When comments are enabled, reset page number and hide it on the "Violation" page.
\mytodo{
\setcounter{page}{1}
\thispagestyle{empty}
}

\twocolumn[
\icmltitle{Scaling Embedding Layers in Language Models} 
\icmlsetsymbol{equal}{*}


\begin{icmlauthorlist}
\icmlauthor{Da Yu$^{\dagger}$ }{}
\icmlauthor{Edith Cohen$^{\dagger}$ }{}
\icmlauthor{Badih Ghazi$^{\dagger}$ }{}
\icmlauthor{Yangsibo Huang$^{\dagger}$ }{}
\icmlauthor{Pritish Kamath$^{\dagger}$}{}
\\
\icmlauthor{Ravi Kumar$^{\dagger}$}{}
\icmlauthor{Daogao Liu$^{\dagger}$}{}
\icmlauthor{Chiyuan Zhang$^{\dagger}$}{}
\end{icmlauthorlist}


\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{} % otherwise use the standard text.
\newcommand{\SCONE}{\textsc{Scone}\xspace}
\begin{abstract}
We propose \SCONE (\textbf{S}calable, \textbf{C}ontextualized, \textbf{O}ffloaded, \textbf{N}-gram \textbf{E}mbedding), a method for extending input embedding layers to enhance language model performance as layer size scales. To avoid increased decoding costs, \SCONE retains the original vocabulary while introducing embeddings for a set of frequent \ngram{n}s. These embeddings provide contextualized representation for each input token and are learned with a separate model during training. During inference, they are precomputed and stored in off-accelerator memory with minimal impact on inference speed. \SCONE enables two new scaling strategies: increasing the number of cached \ngram{n} embeddings and scaling the model used to learn them, all while maintaining fixed inference-time FLOPS. We show that scaling both aspects allows \SCONE to outperform a 1.9B parameter baseline across diverse corpora, while using only half the inference-time FLOPS.
\footnotetext[1]{Google. Correspondence to:\\ \texttt{\{dayuwork,edco,pritishk,chiyuan\}@google.com}.\\}
\end{abstract}

\renewcommand{\thefootnote}{\arabic{footnote}}


\setlength{\textfloatsep}{5pt}

\input{sections/intro}
\input{sections/preliminary}
\input{sections/method}
\input{sections/exps_desgin_choices}
\input{sections/exps_dolma}
\input{sections/related_work}
\input{sections/conclusion}
\ifnum\version=\CameraReady % Acknowledgements should only appear in the accepted version.
\section*{Acknowledgements}

The authors would like to thank Andrew Tomkins for his helpful feedback on an early draft.
\fi



{\small
\bibliography{refs}
\bibliographystyle{sty_files/icml2025}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\input{appendix/appendix}


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
