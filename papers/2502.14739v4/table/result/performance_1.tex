\definecolor{color11}{rgb}{1, 0.8, 0.8}  % 红色20%混合
\definecolor{color12}{rgb}{1, 0.9, 0.9}  % 红色10%混合
\definecolor{color21}{RGB}{255, 224, 127}  % 更亮的黄色调，温暖
\definecolor{color22}{RGB}{255, 239, 179}  % 浅黄色调，亮度增加
\definecolor{color31}{RGB}{198, 230, 195}  % 更亮的绿色，柔和清新
\definecolor{color32}{RGB}{224, 239, 225}  % 更亮的浅绿色，清新且明亮

{
\linespread{1}
\begin{table}[H]
\scriptsize
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{p{4cm}<{\raggedright\arraybackslash}*{7}{p{2cm}<{\centering\arraybackslash}}}
\toprule
\textbf{Model} & \textbf{Overall} & \textbf{Overall} & \textbf{Overall} & \textbf{Overall} & \textbf{Easy} & \textbf{Middle} & \textbf{Hard}\\
& \textbf{(sample)} & \textbf{(subfield)} & \textbf{(field)} & \textbf{(discipline)} & \textbf{(sample)} & \textbf{(sample)} & \textbf{(sample)}\\
\midrule
\rowcolor{color11}
\multicolumn{8}{c}{\textbf{\textit{Reasoning Models}}}\\
\midrule
\rowcolor{color12}
DeepSeek-R1 &\boxed{61.82} & \boxed{62.61} & \boxed{61.23} & \textbf{59.95} & \underline{63.59} & \boxed{63.63} &\boxed{56.87} \\
\rowcolor{color12}
o1-2024-12-17 &\textbf{60.24} & \underline{61.25} & \underline{59.94} & \underline{59.44} & \textbf{64.40} & \underline{61.44} &\underline{53.67} \\
\rowcolor{color12}
DeepSeek-R1-Zero &\textbf{60.24} & \textbf{61.62} & \textbf{60.95} & \boxed{60.99} & \boxed{65.06} & \textbf{62.61} &50.99 \\
\rowcolor{color12}
o3-mini-2025-01-31-high &\underline{55.22} & 54.94 & 52.11 & 48.32 & 53.05 & 56.09 &\textbf{56.16} \\
\rowcolor{color12}
o3-mini-2025-01-31-medium &52.69 & 52.66 & 49.95 & 46.07 & 51.30 & 53.79 &52.37 \\
\rowcolor{color12}
o3-mini-2025-01-31-low &48.03 & 48.51 & 45.89 & 42.63 & 48.80 & 50.21 &43.53 \\
\rowcolor{color12}
o1-mini-2024-09-12 &45.22 & 45.46 & 42.53 & 39.33 & 46.77 & 47.34 &40.00 \\
\rowcolor{color12}
QwQ &43.59 & 44.40 & 43.19 & 41.63 & 46.46 & 47.40 &34.07 \\
\midrule
\rowcolor{color21}
\multicolumn{8}{c}{\textbf{\textit{Chat Models}}}\\
\midrule
\rowcolor{color22}
Doubao-1.5-pro-32k-250115 &\boxed{55.09} & \boxed{56.55} & \boxed{55.62} & \boxed{54.39} & \underline{57.70} & \boxed{60.15} &\boxed{43.80} \\
\rowcolor{color22}
Doubao-1.5-pro-32k-241225 &\textbf{50.93} & \underline{52.41} & \underline{51.76} & 51.24 & 53.54 & \textbf{56.56} &\underline{38.70} \\
\rowcolor{color22}
qwen-max-2025-01-25 &\underline{50.08} & \textbf{52.75} & \textbf{52.47} & \underline{51.65} & \textbf{58.16} & \underline{54.95} &33.09 \\
\rowcolor{color22}
claude-3-5-sonnet-20241022 &48.16 & 51.38 & 51.23 & \textbf{53.15} & \boxed{59.04} & 51.91 &29.99 \\
\rowcolor{color22}
gemini-2.0-flash &47.73 & 48.70 & 47.80 & 46.10 & 53.06 & 49.56 &\textbf{38.84} \\
\rowcolor{color22}
DeepSeek-V3 &47.40 & 49.10 & 48.31 & 47.35 & 55.63 & 50.11 &33.86 \\
\rowcolor{color22}
MiniMax-Text-01 &45.11 & 47.46 & 46.97 & 47.06 & 54.51 & 48.60 &28.98 \\
\rowcolor{color22}
gpt-4o-2024-11-20 &44.40 & 47.62 & 47.50 & 48.84 & 56.84 & 48.75 &23.50 \\
\rowcolor{color22}
Llama-3.1-405B-Instruct &43.14 & 46.43 & 45.83 & 47.35 & 56.06 & 46.31 &23.70 \\
\rowcolor{color22}
gpt-4o-2024-08-06 &41.64 & 44.79 & 44.91 & 46.29 & 55.22 & 45.11 &20.98 \\
\rowcolor{color22}
Qwen2.5-72B-Instruct &40.75 & 43.66 & 43.32 & 42.10 & 48.84 & 45.42 &24.10 \\
\rowcolor{color22}
Mistral-Large-Instruct-2411 &40.65 & 43.38 & 43.13 & 43.37 & 52.92 & 43.28 &22.81 \\
\rowcolor{color22}
qwen-max-2024-09-19 &39.96 & 42.93 & 42.16 & 41.62 & 50.23 & 43.63 &22.60 \\
\rowcolor{color22}
gpt-4o-2024-05-13 &39.76 & 43.19 & 43.13 & 45.23 & 53.37 & 42.38 &20.45 \\
\rowcolor{color22}
Qwen2.5-32B-Instruct &38.76 & 41.18 & 40.40 & 39.43 & 47.42 & 43.05 &22.13 \\
\rowcolor{color22}
Llama-3.3-70B-Instruct &37.69 & 40.56 & 40.15 & 41.12 & 49.68 & 40.68 &19.55 \\
\rowcolor{color22}
phi-4 &37.65 & 39.59 & 38.61 & 37.66 & 45.43 & 40.91 &23.69 \\
\rowcolor{color22}
Qwen2.5-14B-Instruct &35.15 & 37.72 & 37.41 & 36.07 & 44.82 & 37.90 &19.97 \\
\rowcolor{color22}
Llama-3.1-70B-Instruct &34.86 & 38.94 & 39.18 & 40.57 & 48.22 & 37.85 &15.22 \\
\rowcolor{color22}
Yi-Lightning &33.42 & 36.57 & 36.45 & 36.92 & 43.38 & 35.32 &19.35 \\
\rowcolor{color22}
Mixtral-8x22B-Instruct-v0.1 &29.23 & 32.14 & 32.28 & 32.82 & 42.52 & 29.73 &13.82 \\
\rowcolor{color22}
Qwen2.5-7B-Instruct &28.78 & 30.78 & 30.37 & 30.63 & 37.77 & 30.98 &15.23 \\
\rowcolor{color22}
gemma-2-27b-it &27.43 & 30.50 & 30.42 & 31.30 & 40.90 & 27.45 &12.64 \\
\rowcolor{color22}
Yi-1.5-34B-Chat &26.03 & 28.81 & 28.84 & 29.08 & 36.99 & 26.74 &12.81 \\
\rowcolor{color22}
Mistral-Small-Instruct-2409 &25.89 & 28.46 & 28.69 & 28.59 & 37.93 & 25.89 &12.70 \\
\rowcolor{color22}
gemma-2-9b-it &24.04 & 26.89 & 27.06 & 27.74 & 37.81 & 23.05 &10.60 \\
\rowcolor{color22}
Qwen2.5-3B-Instruct &23.31 & 25.45 & 25.86 & 25.57 & 33.10 & 23.50 &12.24 \\
\rowcolor{color22}
Yi-1.5-9B-Chat &23.17 & 25.32 & 25.65 & 26.07 & 32.75 & 24.05 &11.19 \\
\rowcolor{color22}
K2-Chat &22.47 & 24.59 & 24.61 & 24.61 & 30.58 & 21.95 &14.44 \\
\rowcolor{color22}
Mixtral-8x7B-Instruct-v0.1 &22.10 & 24.76 & 24.73 & 26.36 & 34.19 & 20.52 &11.49 \\
\rowcolor{color22}
granite-3.1-8b-instruct &20.83 & 22.85 & 22.92 & 22.26 & 29.48 & 19.79 &13.09 \\
\rowcolor{color22}
Llama-3.1-8B-Instruct &20.50 & 24.07 & 24.52 & 26.12 & 32.82 & 20.37 &7.22 \\
\rowcolor{color22}
Yi-1.5-6B-Chat &19.24 & 21.32 & 21.39 & 22.21 & 27.90 & 18.83 &10.44 \\
\rowcolor{color22}
Qwen2.5-1.5B-Instruct &18.82 & 20.91 & 20.75 & 22.11 & 27.41 & 18.19 &10.45 \\
\rowcolor{color22}
OLMo-2-1124-13B-Instruct &18.66 & 20.46 & 20.60 & 21.80 & 27.10 & 17.85 &10.74 \\
\rowcolor{color22}
gemma-2-2b-it &18.61 & 19.91 & 19.97 & 20.50 & 26.40 & 16.95 &12.85 \\
\rowcolor{color22}
granite-3.1-2b-instruct &17.92 & 19.02 & 19.11 & 19.58 & 23.94 & 17.58 &11.87 \\
\rowcolor{color22}
Mistral-7B-Instruct-v0.3 &17.82 & 19.64 & 19.65 & 20.09 & 26.37 & 16.64 &10.41 \\
\rowcolor{color22}
MAP-Neo-7B-Instruct-v0.1 &17.05 & 18.52 & 18.42 & 18.70 & 23.26 & 16.62 &10.95 \\
\rowcolor{color22}
OLMo-2-1124-7B-Instruct &16.81 & 18.08 & 18.57 & 18.85 & 22.80 & 15.82 &11.90 \\
\rowcolor{color22}
Qwen2.5-0.5B-Instruct &10.77 & 11.92 & 12.47 & 13.47 & 14.90 & 10.88 &6.07 \\
\midrule
\rowcolor{color31}
\multicolumn{8}{c}{\textbf{\textit{Base Models}}}\\
\midrule
\rowcolor{color32}
Qwen2.5-72B & \boxed{34.33} & \boxed{38.08} & \boxed{38.70} & \boxed{39.54} & \boxed{46.20} & \boxed{38.12} &\textbf{15.01} \\
\rowcolor{color32}
Qwen2.5-32B & \textbf{33.16} & \textbf{36.52} & \textbf{37.33} & \textbf{38.29} & \textbf{45.12} & \textbf{36.58} &14.34 \\
\rowcolor{color32}
DeepSeek-V3-Base & \underline{32.14} & \underline{34.79} & \underline{34.58} & \underline{34.71} & 41.28 & \underline{34.50} &\boxed{18.20} \\
\rowcolor{color32}
Qwen2.5-14B & 30.19 & 33.33 & 34.14 & 34.54 & \underline{42.27} & 31.44 &\underline{14.85} \\
\rowcolor{color32}
Yi-1.5-34B & 27.62 & 30.78 & 31.03 & 32.55 & 39.68 & 27.95 &13.86 \\
\rowcolor{color32}
Llama-3.1-70B & 27.22 & 30.52 & 31.28 & 32.55 & 40.78 & 26.95 &12.78 \\
\rowcolor{color32}
Qwen2.5-7B & 25.36 & 28.19 & 28.73 & 29.60 & 36.58 & 25.94 &12.10 \\
\rowcolor{color32}
Llama-3.1-405B & 25.23 & 28.09 & 28.33 & 30.15 & 37.58 & 25.12 &11.86 \\
\rowcolor{color32}
gemma-2-27b & 24.49 & 27.35 & 27.96 & 28.58 & 36.26 & 24.07 &12.27 \\
\rowcolor{color32}
Yi-1.5-9B & 23.10 & 25.40 & 25.69 & 26.17 & 32.52 & 22.98 &12.96 \\
\rowcolor{color32}
gemma-2-9b & 22.56 & 25.19 & 25.47 & 26.26 & 33.88 & 21.34 &12.20 \\
\rowcolor{color32}
Mixtral-8x22B-v0.1 & 22.41 & 24.71 & 25.04 & 25.02 & 32.78 & 21.67 &12.26 \\
\rowcolor{color32}
Mixtral-8x7B-v0.1 & 21.76 & 24.92 & 25.45 & 27.36 & 33.66 & 20.32 &11.13 \\
\rowcolor{color32}
K2 & 20.92 & 23.62 & 23.88 & 24.65 & 30.97 & 20.01 &11.40 \\
\rowcolor{color32}
Yi-1.5-6B & 20.20 & 22.10 & 22.56 & 23.44 & 28.37 & 19.64 &12.20 \\
\rowcolor{color32}
Qwen2.5-3B & 20.14 & 22.81 & 23.30 & 24.42 & 30.42 & 19.81 &9.40 \\
\rowcolor{color32}
Llama-3.1-8B & 19.93 & 22.42 & 22.77 & 23.87 & 30.33 & 18.94 &10.16 \\
\rowcolor{color32}
Mistral-7B-v0.3 & 19.48 & 21.50 & 21.81 & 22.27 & 27.62 & 18.65 &11.96 \\
\rowcolor{color32}
Qwen2.5-1.5B & 17.17 & 19.31 & 19.80 & 21.35 & 24.52 & 16.79 &9.74 \\
\rowcolor{color32}
granite-3.1-2b-base & 16.18 & 17.90 & 17.91 & 18.09 & 23.13 & 14.91 &10.67 \\
\rowcolor{color32}
OLMo-2-1124-13B & 16.07 & 18.75 & 19.82 & 21.37 & 27.24 & 14.41 &6.57 \\
\rowcolor{color32}
MAP-Neo-7B & 15.76 & 17.48 & 18.26 & 19.54 & 22.86 & 14.64 &9.83 \\
\rowcolor{color32}
granite-3.1-8b-base & 15.69 & 16.98 & 16.79 & 16.65 & 20.40 & 15.65 &10.60 \\
\rowcolor{color32}
OLMo-2-1124-7B & 15.15 & 17.62 & 18.30 & 19.60 & 24.43 & 13.83 &7.15 \\
\rowcolor{color32}
gemma-2-2b & 13.57 & 14.98 & 15.43 & 16.21 & 19.12 & 13.47 &7.63 \\
\rowcolor{color32}
Qwen2.5-0.5B & 10.74 & 11.88 & 12.09 & 13.25 & 14.41 & 10.78 &6.65 \\

\bottomrule
\end{tabular}
}
\captionsetup{font=footnotesize}
\caption{\textbf{Detailed Performance Overview on \benchmark - Pivot Table 1.} 
LLMs are scored sample-wise, subfield-wise, field-wise, and discipline-wise levels to ensure fair assessment despite imbalanced question counts. 
The columns Easy(sample), Middle(sample), and Hard(sample) represent average scores according to difficulty. 
The highest score in each column is indicated with a \boxed{box}; the second-best score is in \textbf{bold}, and the third-best score is \underline{underlined}.}
\label{performance1_all}
\end{table}
}
