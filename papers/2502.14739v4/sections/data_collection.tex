\section{Data Collection}\label{sec:data_collection}
We solicit difficult questions from well-educated experts and crowd-sourcing annotators, where we consider experts as individuals having or pursuing a PhD, as in GPQA~\citep{rein2023gpqa}, and crowd-sourcing annotators as undergraduates and master students from top-tier Chinese universities, i.e.mainly from Tsinghua University, Peking University, Zhejiang University, Beihang University, and Chinese Academy of Sciences.
\autoref{Fig: Data Collection} shows the three major stages of \benchmark's data collection pipeline: Source Screening, Transcription, and Quality Inspection, which are separately detailed in \autoref{Sec: Source-Screening}, \autoref{Sec: Transcription}, and \autoref{Sec: Quality-Inspection}.
First, experts select credible resources of different
disciplines’ questions.
Second, crowd-sourcing annotators revise the raw questions from credible resources to candidate questions.
Finally, a rigorous human-LLM collaboration quality inspection process is adopted to select difficult and reliable questions from candidate questions.

\begin{figure*}[ht]
\begin{center}
\includegraphics[width=\linewidth]{pic/main_v2.pdf}
\end{center}
\caption{Data Collection Process of \benchmark.}
\label{Fig: Data Collection}
\end{figure*}

\subsection{Source Screening}
\label{Sec: Source-Screening}

\begin{tcolorbox}[colframe=boxcolor,colback=white,boxrule=0.5mm,arc=0mm]
{\Large \textcolor{bulbcolor}{\faLightbulbO}}  \textbf{Lessons:}
\begin{itemize}
    \item Crowd-Sourcing Annotators are not capable of collecting credible resources for multiple-choice question annotation with high expertise requirements.
    \item The questions and answers (\textbf{QAs}) on exercise websites are not always reliable, even sometimes these QAs are claimed verified.
    \item Multiple-choice questions modified from calculation and reasoning problems usually are more discriminatory than the original multi-choice questions available online.
\end{itemize}
\end{tcolorbox}

During the source screening stage, only expert annotators are allowed to collect credible resources of different disciplines’ questions to guarantee the reliability and difficulty of the raw questions.
In the early stage of collecting candidate questions of \benchmark, we trust crowd-sourcing annotators to collect credible resources themselves.
However, the candidate questions based on the resources found by the crowd-sourcing annotators are always judged too easy or unreliable by expert annotators.
As a result, a significant portion of early funding is wasted on ineffective questions annotated by crowd-sourcing annotators.
Additionally, we point out that QAs on exercise websites are not always reliable. 

In the early stage of \benchmark collection process, some annotators, even expert annotators, trust exercise websites that serve as corroboration of their reasoning process and answers.
In the subsequent quality inspection stage, this proved to be a costly mistake, leading us to spend a significant amount of time and cost correcting erroneous answers derived from online exercise websites.
Furthermore, we find that many SOTA LLMs, such as GPT-4o, o1-mini, and Gemini-flash, exhibit a high frequency of consistency in both process and answers with the erroneous processes and answers from several online exercise websites. 
The observations reveal that \textbf{the reliability of the solutions provided by online exercise websites is limited and there is a significant risk of data leakage}.

Expert annotators are asked to provide raw questions from credible resources with screenshots for further annotation in the source screening stage.
The screenshot greatly eases the workload of quality inspection.
We observe that the efficiency of quality inspection is greatly improved with the provided source screenshot.
The priority order for selecting original questions is:
\begin{itemize}
    \item Example problems with solutions from textbooks.
    \item Calculation and reasoning-needed questions with solutions from websites.
    \item Reasoning-needed multiple-choice questions with solutions from websites.
    \item General multiple-choice questions with solutions from websites.
    \item Questions only with answers but deemed correct by expert annotators.
\end{itemize}
A sampled list of credible resources certified by expert annotators is provided in 
% \autoref{Sec: Book List} and
\autoref{Sec: Benchmark List} for reference.

\subsection{Transcription}
\label{Sec: Transcription}

\begin{figure*}[ht]
\begin{center}
\includegraphics[width=\linewidth]{pic/rephrase_1_27_3.pdf}
\end{center}
\caption{Rewriting Samples of Questions Requiring the Selection of Correct or Incorrect Options.}
\label{Fig: rewriting process}
\end{figure*}

\begin{tcolorbox}[colframe=boxcolor,colback=white,boxrule=0.5mm,arc=0mm]
{\Large \textcolor{bulbcolor}{\faLightbulbO}}  \textbf{Lessons:}
\begin{itemize}
    \item Crowd-sourcing annotators have low accuracy in judging generated distractors. For question types like selecting correct or incorrect options, it is easy to generate flawed distractors, requiring unified rewriting at this stage.
\end{itemize}
\end{tcolorbox}
During the transcription stage, crowd-sourcing annotators are asked to revise the original questions into candidate questions.
Specifically,  the following operations are performed:
\begin{itemize}
    \item Translate non-English questions into English with academic language.
    \item Convert non-multiple-choice questions into multiple-choice format.
    \item Standardize the rewriting of questions requiring the selection of correct or incorrect statements, as shown in \autoref{Fig: rewriting process}.
    \item Include region-specific information where necessary, such as specifying the country for laws mentioned in the questions, except for universally accepted rules.
\end{itemize}
The questions requiring the selection of correct or incorrect statements must be standardized as shown in \autoref{Fig: rewriting process}.
Because we notice that even SOTA LLMs, e.g. Claude-3.5-Sonnet, GPT-4o-0806, suffer from generating correct suitable confounders for questions requiring the selection of correct or incorrect statements.
The full transcription tutorial is provided in \autoref{appendix: annotation tutorial}.


\subsection{Quality Inspection}
\label{Sec: Quality-Inspection}

\begin{tcolorbox}[colframe=boxcolor,colback=white,boxrule=0.5mm,arc=0mm]
{\Large \textcolor{bulbcolor}{\faLightbulbO}}  \textbf{Lessons:}
\begin{itemize}
    \item Questions where LLMs choose the same incorrect option are highly suspicious.
    \item Cases where multiple or all SOTA LLMs make the same error often indicate that the LLMs have memorized explanations from incorrect exercise websites, based on SOTA LLMs' responses.
\end{itemize}
\end{tcolorbox}
We refer to the data quality inspection and filtering methods used in LIME and MMLU-Redux~\cite{zhu2024lime,gema2024we,wu2024comparativestudyreasoningpatterns}, etc. The quality inspection process consists of three substages: \textbf{Rule-based Quality Inspection}, \textbf{LLM-based Quality Inspection}, and \textbf{Expert-based Quality Inspection}.

Candidate questions with clear formatting issues are identified and filtered out by rule-based quality inspection. 
The full checklist of rule-based quality inspection refers to \autoref{Appendix: Rule-Based Check}.
We then adopt several SOTA LLMs to generate responses and additional tags to these reserved candidate questions.
LLM-based quality inspection includes validity checks, negative and extreme inquiry detection, multimodal exclusion, field relevance evaluation, completeness assessment, and discrimination tagging based on SOTA LLMs' responses.
It provides an estimation of not only the correctness but also the discrimination of the candidate questions.
Finally, we ask the expert annotators to re-annotate the suspicious candidate questions.
The checklist for selecting suspicious candidate questions refer to \autoref{Appendix: LLM-Based Screening}.
We adopt GPT-4o-2024-08-06, Gemini-2.0-flash, Doubao-1.5-pro-32k-250115, Claude-3.5-Sonnet, DeepSeek-R1, QwQ, Qwen-2.5-72B-Instruct as SOTA LLMs for selecting suspicious candidate questions.
During the re-annotation process, following the rules stated in GPQA~\citep{rein2023gpqa}, expert annotators are asked to review and solve the given candidate questions with unrestricted access to the web.
They spend over 30 minutes on each candidate question according to the post-annotation interview.
The tutorial for manual quality review refers to \autoref{Appendix: Manual Quality Review}.


