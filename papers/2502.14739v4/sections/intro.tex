\section{Introduction}
Large language models (\textbf{LLMs}) have greatly changed human life.
LLMs are seen as the next technological singularity and proved to surpass human performance in many areas~\citep{phan2025hle}, significantly improving work efficiency.
However, the measurement of LLMs' accessibility on various real-world professionalism remains an unresolved issue, especially in the long-tailed fields with less attention, such as light industry, agriculture, and
various service-related disciplines.
Many popular benchmarks, such as MMLU~\citep{hendrycks2020measuring}, GPQA~\citep{rein2023gpqa}, and MMLU-pro~\citep{wang2024mmlupro}, evaluate LLMs' abilities across different fields, while mainly focus on common fields like mathematics, physics, chemistry, biology, and law, limiting these benchmarks' practical significance on many real-world professionalisms.
These benchmarks fail to cover the diverse and long-tail knowledge accumulated by humans.
Moreover, large language models have achieved very high scores on these benchmarks, making them lose their value as challenging frontiers.

To address the gap, we introduce \textbf{\benchmark}, a comprehensive evaluation at the boundaries of human knowledge covering the evaluation of 285 graduate-level disciplines' knowledge and reasoning capacities.
% Meaning
\benchmark provides at least 50 questions for each graduate-level disciplines to guarantee its accessibility on various real-world professionalism.
% Reliability
\benchmark is developed by a large-scale human-LLM collaboration system, with crowd-sourcing annotators, experts, and state-of-the-art (\textbf{SOTA}) LLMs participating in, and then verified by a rigorous 3-stage quality inspection process, to guarantee its reliability.
% Results as a Proof for its challenging
Moreover, \benchmark is qualified as a challenging frontier for SOTA reasoning LLMs, instruct LLMs, and base LLMs, 
where the best LLMs (e.g., o1 and Deepseek-R1) only achieve a score of around 60.

% 介绍我们的标注流程，并且强调我们share了很多标注的经验
% 记得和下文产生关联，refer to 下文的sections
For building \benchmark, we propose a large-scale human-LLM collaboration system and share the valuable lessons learned in the paper.
We divide the annotation system of \benchmark into three major stages: \textbf{Source Screening}, \textbf{Transcription}, and \textbf{Quality Inspection}.
\textbf{During the source screening stage}, expert annotators collect credible resources of different disciplines' questions to guarantee the reliability and difficulty of the raw questions.
\textbf{During the transcription stage}, crowd-sourcing annotators are asked to revise or translate the raw questions to multiple-choice questions, generate complementary confusion options, and estimate the difficulty and reliability of these questions.
Crowd-sourcing annotators estimate the difficulty and reliability of candidate questions based on both expert judgments and the accuracy of LLMs' responses during the annotation process.
Crowd-sourcing annotators, rigorous real-time plagiarism checks with existing candidate questions, and a robust filtering system based on SOTA LLMs are adopted in the transcription stage to reduce the waste of funding and expert manpower.
\textbf{During the quality inspection stage}, we adopt a rigorous three-stage quality inspection process:
\begin{itemize}
    \item We select suspicious candidate questions based on a checklist of LLMs' responses. 
    \item Expert annotators review the suspicious candidate questions with unrestricted access to the web and revise these questions.
    \item The easy questions are further tailored based on the accuracy of LLMs' responses to guarantee the discrimination of \benchmark.
\end{itemize}


% 分享基于\benchmark Results的insights
We share several major insights based on the evaluation results of \benchmark:


\begin{itemize}
 \item  \textbf{Reasoning capacities matter}. The reasoning models (e.g., DeepSeek-R1, o1-2024-12-17) achieve the best performance in \textbf{\benchmark}. 
    \item  \textbf{Instruction tuning is very helpful}. 
    % to improve the performance of SuperGPQA. Notably,
    For example,
    the results (47.40, 40.75) of DeepSeek-V3 and Qwen2.5-72B-Instruct are better than the results (32.14, 34.33) of DeepSeek-V3-Base and Qwen2.5-72B a lot, respectively.
    % \item The performance gap of DeepSeek-R1 and DeepSeek-R1-Zero is relatively small. In \autoref{performance2}), the DeepSeek-R1 is better than  DeepSeek-R1-Zero in two disciplines (i.e., Science and Engineering). In other disciplines, the DeepSeek-R1-Zero  is better than DeepSeek-R1 
    \item \textbf{More powerful LLMs lead to more balanced results}.
    On different difficulties. the results of simple, middle, and hard splits of DeepSeek-R1 are 63.59, 63.63, and 56.87. In contrast, the results of easy, middle, and hard splits of Qwen2.5-14B-Instruct are 44.82, 37.90, and 19.97.

    \item \textbf{Models are better in newer versions.}
    % New versions of close-sourced LLMs achieve significant improvements on SuperGPQA. 
    For example,  the results of GPT-4o-2024-11-20,  GPT-4o-2024-08-06, and GPT-4o-2024-05-13 are 44.40, 41.64, and 39.76, respectively. 
    % \item These fully open-sourced LLMs (e.g., MAP-Neo-7B, OLMo-2-1124-7B) are still lower than the corresponding open-sourced powerful LLMs with similar sizes (e.g., Qwen2.5-7B).
\end{itemize}
