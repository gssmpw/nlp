\section{Experiments}

\subsection{Baseline Models}
We evaluate 6 reasoning models (o3-mini has three modes), 28 chat models and 17 base models on \benchmark, which includes closed-source models, open-source models, and fully open-source models. The reasoning models include Deepseek-R1 and Deepseek-R1-Zero~\citep{guo2025deepseek}, o1 and o1-mini~\citep{openai2024o1}, QwQ~\citep{qwq-32b-preview}, and o3-mini~\citep{openai2023o3mini} series models. The chat models include closed-source models such as Doubao-1.5-pro, Qwen-max, Claude-3.5~\citep{claude35addendum}, Gemini~\citep{geminiteam2024gemini15unlockingmultimodal}, GPT-4o~\citep{openai2024gpt4o}, Yi-Lightning~\citep{wake2024yi}, and open-source models like MiniMax-Text-01~\citep{li2025minimax}, Qwen2.5~\citep{yang2024qwen2} series, Llama-3.1~\citep{dubey2024llama} series, Mistral~\citep{jiang2023mistral} and Mixtral~\citep{jiang2024mixtral} series, Gemma-2 ~\citep{team2024gemma}series, Yi-1.5~\citep{Young2024YiOF} series, Phi4~\citep{phi-4}, and Granite-3.1~\citep{granite2024granite}. Additionally, there are fully open-source models like MAP-Neo~\citep{zhang2024mapneo} and OLMo-2~\citep{olmo20242}. According to our test results, these models still show a significant gap when compared to industry standards level. The base models include Qwen2.5~\citep{yang2024qwen2} series, Deepseek-V3~\citep{liu2024deepseek}, Yi-1.5~\citep{Young2024YiOF} series, Llama-3.1~\citep{dubey2024llama} series, Gemma-2~\citep{team2024gemma} series, and Mistral~\citep{jiang2023mistral} and Mixtral~\citep{jiang2024mixtral} series. Reasoning models and chat models are evaluated using a zero-shot approach, while base models are assessed using a five-shot evaluation.  Specifically, the five-shot evaluation for base models follow a similar methodology to MMLU-Pro. The specific prompts employed for both the zero-shot and five-shot evaluations are detailed in \autoref{appendix:Evaluation Prompt}. For all main results, the temperature is set to 0. The maximum number of new tokens is set to 32K for reasoning models, while for all other models, it is set to 4K. More model results can be found in \autoref{appendix: all results}, and all detailed results are provided in \autoref{appendix: model score}.

%包含了闭源模型、开源模型以及完全开源的模型，Reason models包括Deepseek-R1与Deepseek-R1-Zero，o1与o1-mini，QWQ 以及o3-mini的系列模型。chat models则包括Doubao-1.5-pro,qwen-max,claude,gemini,gpt-4o,Yi-Lighting等闭源模型,以及MiniMax-Text-01，Qwen2.5系列,Llama系列，Mistral 以及Mixtral系列，gemma系列，Yi-1.5系列等模型，此外还包括Map-Neo,OLMO-2,以及granite这类全开源的模型，这类模型从我们测试的结果看来和工业界的水平以及frontier_lab的水平还有极大的差距。Base models则包括qwen2.5系列，Deepseek-V3系列，Yi系列，Llama系列，gemma系列，以及Mistral和Mixtral系列等模型
\subsection{Main Results}\label{sec:results}

We present the performances of the baselines on different levels, difficulties (\autoref{performance1}) and disciplines (\autoref{performance2}).
\input{table/result/performance_1_main}
\input{table/result/performance_2_main}
% and we have the following findings:

% \begin{itemize}
%     \item We observe that the top-performed reasoning models (e.g., DeepSeek-R1, o1-2024-12-17) achieve the best overall performance in \benchmark. 
%     \item  Instruction tuning is very important to improve the performance of \benchmark. Notably, the results (47.40, 40.75) of DeepSeek-V3 and Qwen2.5-72B-Instruct are better than the results (32.14, 34.33) of DeepSeek-V3-Base and Qwen2.5-72B a lot, respectively.
%     \item The performance gap of DeepSeek-R1 and DeepSeek-R1-Zero is relatively small. In \autoref{performance2}, the DeepSeek-R1 is better than  DeepSeek-R1-Zero only in two disciplines (i.e., Science and Engineering). In other disciplines, the DeepSeek-R1-Zero is slightly better than DeepSeek-R1, which leaves the optimal training paradigm of the reason models an open question.
%     \item New versions of close-sourced LLMs achieve significant improvements on \benchmark. For example,  the results of GPT-4o-2024-11-20,  GPT-4o-2024-08-06 and GPT-4o-2024-05-13 are 44.40, 41.64 and 39.76, respectively. 
%     \item More powerful LLMs achieve more balanced results on different difficulties. For example, the results of simple, middle and easy splits of DeepSeek-R1 are 63.59, 63.63 and 56.87. In contrast, the results of simple, middle and easy splits of Qwen2.5-14B-Instruct are 44.82, 37.90 and 19.97.
%     \item The development of the fully open-sourced LLMs (e.g., MAP-Neo-7B and OLMo-2-1124-7B) still lags behind to the SOTA open-weight LLMs with similar sizes (e.g., Qwen2.5-7B). It can be observed that 
% \end{itemize}

\paragraph{Overall Results.} In general, the top-performed reasoning models (e.g., DeepSeek-R1, o1-2024-12-17) achieve the best overall performance in \benchmark. The effectiveness of instruction tuning to improve the performances is once again verified in the benchmark. For instance, the results (47.40, 40.75) of DeepSeek-V3 and Qwen2.5-72B-Instruct are significantly better than the results (32.14, 34.33) of DeepSeek-V3-Base and Qwen2.5-72B, respectively.
More powerful LLMs achieve more balanced results on different difficulties. For example, the results of simple, middle and hard splits of DeepSeek-R1 are 63.59, 63.63 and 56.87. In contrast, the results of simple, middle and hard splits of Qwen2.5-14B-Instruct are 44.82, 37.90 and 19.97.

\paragraph{Observations for Reasoning Models.} 
Surprisingly, the performance gap between DeepSeek-R1 and DeepSeek-R1-Zero is relatively small. In \autoref{performance2}, the DeepSeek-R1 only beats DeepSeek-R1-Zero in two disciplines (i.e., Science and Engineering). In the other disciplines, the DeepSeek-R1-Zero is slightly better than DeepSeek-R1, which leaves the optimal training paradigm of the reason models an open question.
% 基于学科Table 1和学科Table 2，O1是大概正常的，O3-mini和O1-mini在非Science和Engineer的学科和简单题上崩了掉了。存在潜在的overfit的问题。 [Reason模型的结论]
From the performances across different dimensions, compared to the o1-2024-12-17 model, the newer o1-mini and o3-mini models show decreasing scores except in science and engineering, suggesting a potential data leakage.
Moreover, for different versions of o3-mini (o3-mini-2025-01-31-high, o3-mini-2025-01-31-medium, o3-mini-2025-01-31-low), we observe obvious performance gaps for different difficulties.


\paragraph{Advantages from Pre-training Corpus.}
The newer versions of proprietary LLMs achieve significant improvements on \benchmark, considering the incremental growing of qwen-max series (2024-09-19$\rightarrow$2025-01-25: 39.96 $\rightarrow$50.08) and GPT-4o series (2024-05-13 $\rightarrow$ 2024-08-06 $\rightarrow$ 2024-11-20: 39.76 $\rightarrow$ 41.64 $\rightarrow$ 44.40). Such incremental performances following the chronological order suggest that the developers of proprietary models highly value the incorporation of long-tailed knowledge.
Moreover, we conjecture that the LLMs from Chinese firms (e.g., Qwen and Doubao) generally show superior performances is partially because their data collection pipelines are more aligned to \benchmark, i.e., a considerable ratio of the references are translated from Chinese textbooks.

\paragraph{Progress of Open-source.} 
The \benchmark also reveals a pessimistic open-source progress from the research community.
In the dimension of pre-training corpus curation, the fully open-sourced LLMs (e.g., MAP-Neo-7B and OLMo-2-1124-13B) perform similarly and lag behind to other non fully open ones in similar sizes (e.g., Qwen2.5-7B). 
% The development of such fully open-source models still lags behind to the SOTA open-weight LLMs with similar sizes (e.g., Qwen2.5-7B). 
It can also be observed that, compared to the proprietary models, most of the open-weight LLMs except for the DeepSeek-R1 series are not satisfactory in our benchmark, especially the hard questions.

\paragraph{Difficulty-Specific Capabilities.}  
The difficulty stratification in \benchmark reveals distinct capability patterns between reasoning-focused and knowledge-oriented LLMs. As shown in \autoref{performance1}, the \textbf{hard} split specifically challenges models' reasoning capacities, while \textbf{easy} and \textbf{middle} splits better reflect factual knowledge mastery. For instance, the o3-mini series exhibits lower scores than Doubao-1.5-pro-32k-250115 on easy and middle splits, yet surpasses it significantly on hard questions. This dichotomy suggests that:  
\begin{itemize}
    \item \textbf{Chat-oriented LLMs} (e.g., Doubao series) excel at knowledge recall for common professional questions but struggle with complex reasoning in long-tail domains.  
    \item \textbf{Reasoning-specialized models} demonstrate superior performance on hard questions through enhanced logical processing, despite potential compromises in broad knowledge coverage.  
\end{itemize}
This differentiation validates \benchmark's design rationale -- using difficulty levels as diagnostic tools to dissect complementary capabilities in modern LLMs.

%\input{table/result/performance_1_main}
%\input{table/result/performance_2_main}

\subsection{Further Analysis}
% 不同学科的top3 model
% 不包括o3-mini

% \input{table/result/xiaolong_analysis}

\begin{figure}[t]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{pic/xiaolong_analysis/qwen_comparison_subfield.pdf}
        \caption{Impact of subfield information.}
        \label{fig:subfield}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{pic/xiaolong_analysis/model_performance_robustness.pdf}
        \caption{Model robustness analysis.}
        \label{fig:robustness}
    \end{subfigure}
    \caption{(a) Accuracy comparison of Qwen2.5 models (0.5B–72B) with and without subfield information in prompts. Larger models benefit more from additional context. 
    (b) Robustness evaluation across 24 semantically equivalent prompts, indicating larger models exhibit higher stability with lower variance.}
    \label{fig:subfield_robustness}
\end{figure}


\paragraph{Effect of Subfield Information in Prompts.}
To investigate the impact of subfield information on model performance, we conduct zero-shot evaluations under two conditions: (1) zero-shot-with-subfield, where the prompt includes a description of the problem's subfield, and (2) zero-shot-without-subfield, where no such information is provided. The prompts of zero-shot-with-subfield are shown in \autoref{appendix:Impact of Subfield Information}.
We evaluate Qwen2.5-Instruct models ranging from 0.5B to 72B parameters across these two settings.  
\textbf{\autoref{fig:subfield} show that incorporating subfield information generally leads to improved performance, particularly for larger models}. 
For example, Qwen2.5-72B-Instruct achieves an accuracy of 41.93\% in the zero-shot-with-subfield setting, compared to 40.82\% without subfield annotations. 
Similarly, Qwen2.5-32B-Instruct improves from 39.13\% to 39.65\%, and Qwen2.5-14B-Instruct sees a minor increase from 35.36\% to 35.78\%, \textbf{suggesting that additional contextual information helps larger models refine their reasoning by narrowing down the relevant domain}.  
However, for smaller models like Qwen2.5-0.5B-Instruct and Qwen2.5-1.5B-Instruct, the introduction of subfield information does not lead to a noticeable gain in accuracy. Qwen2.5-0.5B-Instruct performs slightly worse with subfield annotations (10.62\% vs. 11.12\%), while Qwen2.5-1.5B-Instruct shows near-identical performance (18.09\% vs. 18.62\%), \textbf{indicating that smaller models may lack the capacity to leverage fine-grained domain-specific cues effectively, relying more on general knowledge retrieval rather than contextual domain disambiguation}.  


\paragraph{Robustness Analysis.}
Benchmark evaluations can be significantly influenced by slight variations in prompts, leading to inconsistencies in model ranking and overall assessment reliability. 
To investigate this phenomenon, we conduct robustness experimentacross Qwen2.5-Instruct models (0.5B$\sim$72B), employing 24 distinct yet semantically equivalent prompts in a zero-shot setting, using the same evaluation parameters as in the main results.
Specifically, we employ 4 types of initial prompts and 6 types of question formats, resulting in a combination of 24 different prompt styles to verify the robustness of our bench.
The combination of initial prompt 1 and question format 1 is the default prompt for our evaluation. The detailed prompts are shown in the \autoref{appendix:robustness}.
\textbf{\autoref{fig:robustness} shows that larger models exhibit increased robustness, as a steady improvement in accuracy from 9.80\% of Qwen2.5-0.5B-Instruct to 41.15\% of Qwen2.5-72B-Instruct while variance decreases ($\sigma = 1.37$ for the smallest model, dropping to $\sigma \approx 0.27 \sim 0.36$ in the largest models). }
This demonstrates the our evaluation framework is more stable with \textbf{a maximum standard deviation SD of 1.37\%}, that mitigates prompt-induced instability and provides a more reliable basis for model assessment. 







\begin{figure}[t]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{pic/xiaolong_analysis/BON.pdf}
        \caption{Best of N (BoN) performance comparison}
        \label{fig:bon}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{pic/xiaolong_analysis/Majority_Voting.pdf}
        \caption{Majority Voting performance comparison }
        \label{fig:Majority_Voting}
    \end{subfigure}
    \caption{Performance comparison of Qwen2.5-72B-Instruct and Doubao-1.5-pro-32k-20250115 under two ensembling strategies: (a) Best of N (BoN) and (b) Majority Voting. BoN shows that Qwen2.5-72B-Instruct benefits more from increased sampling, whereas Majority Voting favors Doubao-1.5-pro-32k-20250115 due to its more consistent output distribution. }
    \label{fig:bon_Majority_Voting}
\end{figure}


\paragraph{Best of N (BoN) Analysis.}
BoN (Best of N) is a strategy that selects the highest-quality response from N independent generations which utilize stochastic sampling to improve overall performance. 
In our experiments, we evaluate Qwen2.5-72B-Instruct and Doubao-1.5-pro-32k-20250115 under BoN settings ranging from $N=1$ to $N=32$, with results presented in \autoref{fig:bon}.
\textbf{Qwen2.5-72B-Instruct exhibits a steeper BoN curve compared to Doubao-1.5-pro-32k-20250115}, suggesting that Qwen benefits more significantly from multiple sampling attempts, likely due to a higher variance in response quality. 
Conversely, \textbf{Doubao-1.5-pro-32k-20250115 shows stronger initial performance but a more gradual BoN gain}, implying that its response distribution is more consistent but less opportunistic in leveraging multiple trials.
Notably, \textbf{while Doubao maintains a lead in early BoN values ($N \leq 15$), Qwen2.5-72B-Instruct surpasses it around Bo24 and continues to outperform at Bo32}, indicating that for scenarios where extensive sampling is feasible, Qwen2.5-72B-Instruct demonstrates a greater ability to exploit high-quality outputs.




\paragraph{Majority Voting Analysis.}\footnote{Both the BoN and Majority Voting analyses are conducted using the same set of inference results, generated with temperature = 0.7 and repeated for 32 independent runs.}
Majority Voting is a strategy that selects the most frequently generated response from multiple independent runs. We evaluate Majority Voting using Qwen2.5-72B-Instruct and Doubao-1.5-pro-32k-20250115 shown in \autoref{fig:Majority_Voting}. When multiple options receive the same highest number of votes, the answer is considered correct if the correct option is among them.
We show that \textbf{Doubao-1.5-pro-32k-20250115 consistently outperforms Qwen2.5-72B-Instruct across all voting sizes, exhibiting a stable performance around 55-57\%}. 
In contrast, \textbf{Qwen2.5-72B-Instruct demonstrates fluctuations, particularly for lower N, with performance largely remaining in the 40-45\% range}, indicating that Doubao generates more consistent responses across independent runs.


% Conversely, Qwen’s performance variability suggests a higher response diversity, which may lead to instability when aggregating outputs via simple voting.

% Interestingly, as N increases, Doubao-1.5-Pro-32k stabilizes around 55.50\%, while Qwen-2.5-72B does not show noticeable improvements, implying that the latter does not benefit as much from frequency-based selection. This suggests that for scenarios requiring high consistency and reliability, Doubao-1.5-Pro-32k is better suited for Majority Voting, whereas Qwen-2.5-72B may require alternative ensembling methods to fully utilize its diverse response space.





% \subsection{Robustness of Evaluation Result}
% \paragraph{performance variation}
% \paragraph{prompt with field name}

\subsection{Analysis of Disciplinary Discrimination Power}
\label{sec:discrimination}
\definecolor{softblue}{rgb}{0.88, 0.95, 1.0} % Soft blue
\definecolor{softred}{HTML}{FCE4D6}

To systematically evaluate the discrimination power across disciplines, we employ two complementary analytical approaches: \textbf{descriptive statistics} and \textbf{discrimination indices analysis}. The descriptive statistics approach examines the distribution characteristics of model performance within each discipline through three key metrics:
\begin{itemize}
\item \textbf{Mean Accuracy}: Reflects the overall difficulty level of the discipline.
\item \textbf{Standard Deviation (SD)}: Measures the dispersion of model performance.
\item \textbf{Coefficient of Variation (CV)}: Normalizes the standard deviation by mean accuracy, enabling cross-discipline comparison.
\end{itemize}

The discrimination indices analysis complements this by comparing performance extremes through:
\begin{itemize}
\item \textbf{High-Low Group Difference ($\Delta$)}: Calculates the mean accuracy gap between the top 3 and bottom 3 models in each discipline.
\end{itemize}

~\autoref{tab:full_analysis} presents complete results with group comparisons across all 13 disciplines, highlighting key patterns through color coding.

\begin{table}[htbp]
\centering
\small
\begin{tabular}{@{}lcccccc@{}}
\toprule
Discipline & \multicolumn{3}{c}{Descriptive Statistics} & \multicolumn{3}{c}{Discrimination Indices Analysis} \\
\cmidrule(lr){2-4} \cmidrule(lr){5-7}
 &\textbf{Mean Acc}. & \textbf{SD} & \textbf{CV} & \textbf{High} &\textbf{Low} & $\Delta$ \\
\midrule
\rowcolor{softblue}
Engineering & 53.93 & 5.75 & 0.107 & 60.85 & 47.72 & 13.13 \\
Philosophy & 55.56 & 7.33 & 0.132 & 62.92 & 46.59 & 16.33 \\
Medicine & 54.42 & 6.44 & 0.118 & 60.94 & 46.38 & 14.57 \\
Economics & 58.25 & 6.96 & 0.120 & 65.86 & 49.83 & 16.04 \\
Science & 54.52 & 6.86 & 0.126 & 62.39 & 46.94 & 15.45 \\
\rowcolor{softred}
Law & 56.92 & 7.17 & 0.126 & 65.29 & 48.68 & 16.62 \\
\rowcolor{softred}
History & 48.28 & 8.45 & 0.175 & 58.26 & 39.07 & 19.19 \\
Education & 52.15 & 5.94 & 0.114 & 57.51 & 44.15 & 13.36 \\
\rowcolor{softblue}
Military Science & 53.85 & 4.99 & 0.093 & 59.51 & 47.97 & 11.55 \\
\rowcolor{softblue}
Management & 52.73 & 5.21 & 0.099 & 58.02 & 47.04 & 10.98 \\
Literature \& Arts & 46.94 & 6.58 & 0.140 & 55.03 & 40.02 & 15.02 \\
Agronomy & 46.97 & 5.58 & 0.119 & 53.06 & 40.27 & 12.78 \\
Sociology & 57.83 & 7.33 & 0.127 & 66.20 & 50.35 & 15.85 \\
\bottomrule
\end{tabular}
\caption{Comprehensive Discrimination Analysis with Several Key Evaluation Metrics (\textbf{Mean Acc.}: Mean Accuracy, \textbf{SD}: Standard Deviation, \textbf{CV}: Coefficient of Variation, $\Delta$: High-Low Group Difference).}
\label{tab:full_analysis}
\end{table}

Our systematic analysis reveals two distinct patterns in disciplinary discrimination power:
\begin{itemize}
\item \textbf{High-discrimination disciplines}: History (SD=8.45, CV=0.175, $\Delta$=19.19) and Law (SD=7.17, CV=0.126, $\Delta$=16.62) demonstrate the strongest differentiation capacity, indicating models exhibit substantially varied performance in these domains.

\item \textbf{Low-discrimination disciplines}: Military Science (SD=4.99, CV=0.093, $\Delta$=11.55), Engineering (SD=5.75, CV=0.107, $\Delta$=13.13), and Management (SD=5.21, CV=0.099, $\Delta$=10.98) exhibit performance convergence among top models.
\end{itemize}

The observed dichotomy between humanities and STEM disciplines emerges from fundamental differences in knowledge representation. The heightened discrimination in humanities (History CV=0.175) likely originates from:
\begin{itemize}
\item Context-dependent reasoning requiring real-world knowledge synthesis.
\item Cultural nuance interpretation demands.
\item Ethical judgment variance in open-ended scenarios.
\end{itemize}

Conversely, the performance convergence in STEM fields (Engineering CV=0.107) reflects:
\begin{itemize}
\item Standardized problem-solving patterns in technical domains.
\item Mathematical consistency in training corpora.
\item Concentrated optimization efforts by model developers.
\end{itemize}

This finding validates our experimental design hypothesis: when evaluating top-performing models (per-discipline top 10 selection), humanities disciplines better reveal capability differences due to their complexity beyond pattern recognition, while STEM metrics approach performance ceilings. Our results emphasize the critical need for comprehensive cross-domain evaluation frameworks to fully capture models' heterogeneous capabilities beyond technical domains.