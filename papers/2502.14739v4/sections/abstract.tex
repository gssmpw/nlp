\begin{abstract}

% Large language models have demonstrated high expertise in multiple mainstream disciplines, such as mathematics, physics, and computer science.
% However, in this paper, we observe that human knowledge could cover more than 200 specialized disciplines, exceeding the knowledge scope far beyond the existing benchmarks.
% The proficiency of large models in many specialized disciplines remains insufficiently measured, which poses a risk of being overlooked, particularly in fields such as light industry, agriculture, and various service-related disciplines.
Large language models (LLMs) have demonstrated remarkable proficiency in mainstream academic disciplines such as mathematics, physics, and computer science. However, human knowledge encompasses over 200 specialized disciplines, far exceeding the scope of existing benchmarks. The capabilities of LLMs in many of these specialized fields—particularly in light industry, agriculture, and service-oriented disciplines—remain inadequately evaluated.
% This paper introduces \textbf{\benchmark}, an encyclopedic benchmark at the boundaries of human knowledge covering evaluation of 285 graduate-level disciplines' knowledge and reasoning capacities.
% Additionally, by introducing a highly interactive Human-LLM collaborative mechanism, \benchmark excludes the trivial and noisy questions based on both LLMs' responses and experts' comments.
To address this gap, we present \textbf{\benchmark}, a comprehensive benchmark that evaluates graduate-level knowledge and reasoning capabilities across 285 disciplines. Our benchmark employs a novel Human-LLM collaborative filtering mechanism to eliminate trivial or ambiguous questions through iterative refinement based on both LLM responses and expert feedback.
% Our experimental results show that the results of existing top-performed LLMs still need to improve significantly on the knowledge domain. Notably, the reasoning LLM (i.e., DeepSeek-R1) achieves the best performance with  61.82 accuracy on \benchmark. Moreover, we share unparalleled annotation management experience with more than 80 expert annotators and large-scale interactive Human-LLM collaborative systems participating for the benefit of similar large-scale research projects' effective practice.
Our experimental results reveal significant room for improvement in the performance of current state-of-the-art LLMs across diverse knowledge domains ( \emph{e.g.}, the reasoning-focused model DeepSeek-R1 achieved the highest accuracy of 61.82\% on \benchmark), highlighting the considerable gap between current model capabilities and artificial general intelligence. Additionally, we present comprehensive insights from our management of a large-scale annotation process, involving over 80 expert annotators and an interactive Human-LLM collaborative system, offering valuable methodological guidance for future research initiatives of comparable scope.
\end{abstract}
