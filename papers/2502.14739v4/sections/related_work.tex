\section{Related Work}

\subsection{Large Language Models}

The landscape of natural language processing has been transformed by recent breakthroughs in Large Language Models (LLMs)~\citep{zhang2024mapneo,Young2024YiOF}. The introduction of GPT-3 marked a significant milestone, showcasing its ability to interpret tasks and examples from textual inputs with minimal prior training. 
Recently, the latest generation of LLMs (e.g., GPT-4~\citep{gpt4}, Claude-3.5~\footnote{\url{https://www.anthropic.com/news/claude-3-5-sonnet}}, Gemini~\citep{gemini1}, and Llama-3~\citep{llama3modelcard}), have exhibited remarkable progress in sophisticated reasoning across diverse fields.
To comprehensively evaluate and challenge the expanding capabilities of these advanced AI systems, we present \benchmark. This novel benchmark is specifically crafted to probe the knowledge boundaries of existing LLMs.
% cognitive processing and knowledge breadth in cutting-edge language models, providing a rigorous assessment framework for the evolving landscape of artificial intelligence.

% Recent advancements in LLMs have significantly propelled the field of natural language processing.
% GPT-3 [7] demonstrated robust few-shot prediction capabilities, interpreting tasks and examples from
% natural language inputs. Subsequent models like InstructGPT [28], which employ human-feedback
% reinforcement learning, have achieved strong user instruction-following capability. More recent
% models including GPT-4o, GPT-4, Claude-3, Gemini, and Llama-3, have shown notable improvements
% in complex reasoning across various domains. To rigorously assess and push the capabilities of these
% LLMs, we introduce MMLU-Pro, a new benchmark designed to test the upper limits of reasoning
% and knowledge in advanced language models.

\subsection{LLM Benchmarks}


Recently, the development of the Large Language Model (LLM) has been transformed by the introduction of various benchmarks~\citep{cobbe2021training,Wang2024MTUBenchAM,bai2024mt}. Notable examples include GLUE~\citep{DBLP:conf/iclr/WangSMHLB19} and its successor SuperGLUE~\citep{DBLP:conf/nips/WangPNSMHLB19}, which have been instrumental in propelling advancements in language comprehension tasks. These foundational benchmarks paved the way for more specialized assessments, such as MMLU~\citep{hendrycks2020measuring}, HotpotQA \citep{DBLP:conf/emnlp/Yang0ZBCSM18}, BigBench~\citep{DBLP:journals/corr/abs-2206-04615}, HellaSwag~\citep{DBLP:conf/acl/ZellersHBFC19},  CommonsenseQA~\citep{DBLP:conf/naacl/TalmorHLB19}, KOR-Bench~\citep{ma2024korbenchbenchmarkinglanguagemodels},
SimpleQA~\citep{simpleqa} and Chinese SimpleQA~\citep{csimpleqa}. These newer benchmarks have expanded the evaluation scope to encompass content generation, knowledge understanding, and complex reasoning abilities.
% To facilitate comparative analysis of diverse LLMs, platforms like the OpenLLM Leaderboard and OpenCompass have emerged. However, the rapid progress in LLM capabilities has led to a clustering of top scores on these leaderboards, with models such as GPT-4 achieving near-perfect results across multiple benchmarks. This phenomenon underscores the pressing need for more demanding evaluation metrics to adequately probe the boundaries of LLM capabilities.
% Recent research has uncovered a vulnerability in current benchmarks: LLM performance can be significantly affected by minor alterations in prompt style or phrasing. This instability is partly attributable to the inherent characteristics of the models, but is also exacerbated by the prevalent four-option multiple-choice question (MCQ) format. This standard format may not provide sufficient challenge or differentiation between high-performing systems, potentially leading to overestimation of model capabilities.
% To address these limitations, 
% Therefore,
While numerous benchmarks have been developed to evaluate LLMsâ€™ capabilities and alignment with human values, these have often focused narrowly on performance within singular
tasks or domains. 
To enable a more comprehensive LLM assessment,
we propose \benchmark to scale the LLM evaluation to 285 graduate-level disciplines and provide a comprehensive and fine-grained analysis of foundation models.

% For instance, the GLUE benchmark, which aggregates various tasks, has seen models surpass human-level performance. Building on this, MMLU was created to assess model proficiency across 57 subjects spanning STEM, humanities, and social sciences, helping to identify areas of weakness. Similarly, BIG-bench was developed, offering over 200 diverse tasks for comprehensive evaluation.

% In recent years, the development of various benchmarks has significantly enhanced the evaluation of
% Large Language Models (LLMs). For instance, GLUE [37] and its successor SuperGLUE [38], have
% played a pivotal role in advancing language understanding tasks, setting the stage for more specialized
% evaluations. Other recent benchmarks, including MMLU [18], HELM [22], BigBench [32], HellaSwag [45], and the AI2 Reasoning Challenge (ARC) [12], have broadened the scope by assessing
% capabilities across language generation, knowledge understanding, and complex reasoning [9].
% To facilitate performance comparison across diverse LLMs, several leaderboards have been established, such as the OpenLLM Leaderboard [27] and OpenCompass [14]. However, as LLM
% capabilities have rapidly advanced, leaderboard scores have become increasingly concentrated at the
% top, with models like GPT-4 achieving near-perfect scores on multiple benchmarks. This trend highlights the urgent need for more challenging benchmarks to fully test the limits of LLM capabilities.
% Recent studies have revealed that the performance of Large Language Models (LLMs) on current
% benchmarks is not robust to minor perturbations [25, 31]. Specifically, slight variations in the style or
% phrasing of prompts can lead to significant shifts in model scores. Beyond the inherent non-robustness
% of the models themselves, the typical four-option format of multiple-choice questions (MCQs) also
% contributes to this instability in model scoring. This format may not sufficiently challenge the models
% or differentiate between closely performing systems, leading to potential overestimations of their
% capabilities. Our new benchmark, MMLU-Pro, aims to address these issues by introducing more
% complex questions and increasing the number of answer choices from four to ten, thus enhancing
% performance differentiation and reducing the likelihood of correct guesses by chance.



