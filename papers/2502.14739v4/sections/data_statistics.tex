
\section{Statistics}

% 1. Introduce the \benchmark in general, a comprehensive benchmark to probe the ability upper bound of SOTA LLMs. Emphasize the distinctive characteristics: large number of questions (compared to the similar work), the large number of options (averge 9.67 options per question, across diverse domains based on a comprehensive disciplinary taxonomy).
% 2. Discuss about the overall distribution of question numbers across different disciplines, especicially the uneven STEM and non STEM ratio. Justify the distribution, and highlight the advancement of benchmark. Important arguments: 2.1. There were similar quantity of reference books sourced for each among the 285 subfields during data annotation process, but more questions are derivated from the STEM disciplines following the requirements of question quality and difficulty (to both human annotator and SOTA LLMs). 2.2. Even the non STEM disciplines have a relatively small numbers of questions in \benchmark, we are still able to distinguish different performance levels of the SOTA LLMs in these domains (refer to a result subsection).
% 3. Discuss about the distribution of difficulty level (LLM-based difficulty judgement) and the ratio of the calculation tasks requiring understanding and reasoning on the expressions.
% 4. Discuss the length of question and options (note that the answer option does not show a distinct difference from the general option stats).
% 5. Highlight the comprehensiveness of the benchmark again (across different dimensions mentioned above), (How do we leverage visualization like tSNE of the QA embeddings to show this?)


\begin{table}[!ht]
    \centering
\small
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}lcccccccccc@{}}
\toprule
\multirow{2}{*}{Discipline} & \multirow{2}{*}{\#Num.}  & \multicolumn{3}{c}{Question \#Tokens} & \multicolumn{3}{c}{Answer \#Tokens } & \multicolumn{2}{c}{Options} &\multirow{2}{*}{Cal. Rate} \\
\cmidrule(lr){3-5} \cmidrule(lr){6-8} \cmidrule(lr){9-10} 
& & Max & Min & Avg & Max & Min & Avg & \#Num. &  \#Tokens \\
\midrule
Engineering & 7892 & 715 & 4 & 67.26 & 352 & 1 & 13.86 & 9.76 & 13.67 & 55.40\% \\
Medicine & 2755 & 447 & 4 & 39.06 & 89 & 1 & 7.57 & 9.67 & 7.38 & 3.34\% \\
Science & 9838 & 623 & 5 & 69.77 & 372 & 1 & 16.89 & 9.71 & 16.75 &66.34\% \\
Philosophy & 347 & 573 & 5 & 44.63 & 71 & 1 & 8.93 & 9.63 & 8.42 & 0.29\%\\
Military Science & 205 & 336 & 7 & 30.03 & 50 & 1 & 6.76 & 9.29 & 6.40 & 3.41\%\\
Economics & 314 & 314 & 5 & 36.15 & 98 & 1 & 7.85 & 9.86 & 7.12 & 20.50\%\\
Management & 501 & 308 & 7 & 41.87 & 217 & 1 & 7.77 & 9.72 & 6.62 & 3.79\% \\
Sociology & 143 & 107 & 6 & 25.31 & 75 & 1 & 7.34 & 9.87 & 6.91 & 2.10\% \\
Literature & 1676 & 869 & 5 & 35.80 & 88 & 1 & 6.82 & 8.78 & 6.75 & 0.60\% \\
History & 674 & 539 & 6 & 28.38 & 125 & 1 & 5.32 & 9.64 & 5.49 & 1.04\% \\
Agriculture & 485 & 331 & 7 & 27.25 & 33 & 1 & 5.32 & 9.91 & 5.24 & 1.44\% \\
Law & 656 & 560 & 6 & 66.38 & 89 & 1 & 12.17 & 9.73 & 11.45 &0.46\%\\
Education & 484 & 173 & 7 & 23.35 & 37 & 1 & 5.74 & 9.78 & 5.39 & 0.41\%\\
\hline
Overall & 26529 & 869 & 4 & 58.42 & 372 & 1 & 12.86 & 9.67 & 12.64 &42.33\%\\
\bottomrule
\end{tabular}}
\caption{Statistics of \benchmark. Tokens are calculated with Tiktoken using \texttt{\href{https://huggingface.co/BEE-spoke-data/cl100k_base}{cl100k\_base}} encoding.}
\label{tab:overall_stats}
\end{table}
% \footnote{\url{https://huggingface.co/BEE-spoke-data/cl100k_base}}

\input{table/discipline_1}
\input{table/discipline_2}

% We present \benchmark, a comprehensive benchmark dataset designed to evaluate the knowledge retrieval and question-answering capabilities of large language models. The benchmark comprises 26,579 samples, featuring questions that vary significantly in length and complexity. The questions range from concise queries of just 4 tokens to extensive inquiries spanning up to 869 tokens, with an average length of 58.38 tokens. This wide range in question length reflects the diverse nature of real-world graduate-level queries and challenges models to handle brief and detailed inquiries effectively.

% Another notable characteristic of \benchmark is its structured multiple-choice format, where each question is accompanied by 4 to 10 possible answers, with an average of 9.66 options per question. The answers demonstrate considerable variation in length, ranging from single-token responses to more elaborate explanations of up to 372 tokens, while maintaining a concise average length of 12.85 tokens.

% We present the basic statistics of \benchmark in \autoref{tab:overall_stats}. The benchmark encompasses the \textbf{comprehensive 13 distinct academic disciplines}, with a notably uneven distribution of questions across fields. Science and Engineering dominate the dataset, containing 9,838 (37.1\%) and 7,892 (29.7\%) questions respectively, collectively accounting for approximately two-thirds of the total 26,529 questions. Medicine follows as the third largest discipline with 2,755 questions (10.4\%), while Literature contributes 1,676 questions (6.3\%). The remaining disciplines each constitute less than 3\% of the dataset, with Sociology having the smallest representation at 143 questions (0.5\%). 
% This distribution reflects our deliberate focus on STEM fields, where questions often require complex reasoning and calculation abilities, as evidenced by the high calculation rates in Science (66.34\%) and Engineering (55.40\%). The expertise-driven data collection process, involving annotators with advanced degrees in their respective fields, ensures the questions represent graduate-level complexity and domain-specific challenges.
% The pronounced skew towards STEM disciplines, particularly Science (37.1\%) and Engineering (29.7\%)(detailed in \autoref{tab:detailed_discipline_1} and \autoref{tab:detailed_discipline_2}), reflects a deliberate emphasis in our annotation process . 
% Annotators with expertise in these fields consistently identified and selected \textit{more challenging questions}, likely due to several factors. First, these disciplines often contain well-defined problems that can rigorously test an LLM's capabilities in mathematical reasoning, physical principles application, and complex problem-solving. 
% The high calculation rates in Science (66.34\%) and Engineering (55.40\%) support this observation, indicating that annotators preferentially selected quantitative problems that challenge both knowledge recall and computational abilities. Additionally, the graduate-level textbooks in these fields typically contain abundant end-of-chapter problems and worked examples, providing annotators with a rich source of sophisticated questions. 
% However, with non-STEM questions in relatively small quantities, the benchmark can still reflect significantly distinct performance of different models \textcolor{blue}{(YL: add evidence from Tianyu)}, which suggests the robustness of our annotation pipeline.

% A distinctive feature of \benchmark is its coverage of long-tail disciplines that are often underrepresented in existing LLM benchmarks \textcolor{blue}{(YL: add evidence)}. While smaller in volume, the inclusion of fields such as Sociology (143 questions), Philosophy (347 questions), and Military Science (205 questions) represents a deliberate effort to assess LLMs' capabilities across the full spectrum of academic knowledge. This comprehensive approach differentiates \benchmark from many existing benchmarks that focus primarily on mainstream subjects or computer science-adjacent fields. The presence of these specialized disciplines, despite their smaller representation, enables evaluation of LLMs' understanding in areas requiring nuanced conceptual reasoning and domain-specific expertise. This is particularly valuable as it helps assess whether LLMs can handle not just the dominant academic disciplines but also more specialized fields that may require different types of reasoning or knowledge structures. The relatively small number of questions in these disciplines reflects both the specialized nature of these fields and the challenge of finding expert annotators in these areas, but their inclusion provides crucial insight into LLMs' breadth of capability.


% Regarding length characteristics, we observe significant variations across disciplines in both question and answer lengths. Literature exhibits the highest maximum question length at 869 tokens, followed by Engineering (715 tokens) and Science (623 tokens), suggesting these fields often require extensive context or detailed problem setups. However, the average question lengths tell a different story, with Law (66.38 tokens), Science (69.77 tokens), and Engineering (67.26 tokens) showing consistently higher means, while Education (23.35 tokens) and Sociology (25.31 tokens) tend toward more concise questions. For answer lengths, Science and Engineering again lead with maximum lengths of 372 and 352 tokens respectively, while their average answer lengths (16.89 and 13.86 tokens) indicate that most responses are relatively succinct. Notably, the option characteristics remain relatively consistent across disciplines, with the number of options ranging narrowly between 8.78 (Literature) and 9.91 (Agriculture), suggesting a standardized multiple-choice format throughout the benchmark. The average option token length varies from 5.24 (Agriculture) to 16.75 (Science), correlating roughly with the complexity and technical nature of each discipline.

We propose \benchmark, designed as a comprehensive benchmark to probe the upper bounds of state-of-the-art Large Language Models' capabilities. With 26,529 questions spanning 13 disciplines, 72 fields, and 285 subfields, it substantially surpasses existing benchmarks in both scale and taxonomic depth. Compared to similar ``hard'' benchmarks such as GPQA (448 questions) and MMLU-Pro (12,032 questions), \benchmark not only contains a larger question pool but also features a more challenging format with an average of 9.67 options per question, significantly higher than the conventional 4-option format (\emph{e.g.}, MMLU).

\paragraph{Comprehensiveness and Discrimination.} As revealed in \autoref{tab:overall_stats}, the distribution of questions across disciplines reveals a notable concentration in STEM fields, with Science (9,838 questions), Engineering (7,892 questions), and Medicine (2,755 questions) collectively accounting for 77.2\% of the benchmark. While this distribution might appear uneven at first glance, it emerges from our rigorous question collection and validation process. During the data annotation phase, we source a comparable number of reference books for 285 subfields (the subfields are detailed in \autoref{tab:discipline_1} and \autoref{tab:discipline_2}). Note that we also use some open-source datasets for supplementation of \benchmark in \autoref{Sec: Benchmark List}. However, the STEM disciplines yielded more questions meeting our stringent quality and difficulty criteria aforementioned in \autoref{sec:data_collection}, where the questions and options are filtered through rigorous rule-based, model-based and human-based pipeline. This natural emergence of STEM-heavy distribution aligns with the benchmark's goal of probing LLMs' upper-bound capabilities in complex reasoning tasks.
Despite the relatively smaller representation of non-STEM disciplines (\emph{e.g.}, Philosophy: 347, Literature: 1,676, History: 674 questions), our experiments demonstrate that these subsets effectively discriminate various SOTA LLMs' performance levels (detailed in \autoref{sec:results}). This once again validates the discriminative power of our benchmark across all domains, regardless of sample size.

\paragraph{Difficulty.} The difficulty distribution across disciplines 
% (\autoref{fig:discipline_difficulty_distribution} \textcolor{blue}{(YL: refine the figure}) 
(\autoref{tab:discipline_difficulty_distribution})
reveals varying levels of complexity. In STEM fields, we observe a more balanced distribution of difficulty levels. For instance, Engineering questions are distributed as 31.1\% hard, 43.9\% middle, and 25.0\% easy, while Science shows 42.8\% hard, 42.0\% middle, and 15.2\% easy. Non-STEM disciplines generally show a different pattern, with a higher proportion of easy and middle-difficulty questions. Notably, 42.33\% of all questions require mathematical calculations or formal reasoning, with Science (66.34\%) and Engineering (55.40\%) showing the highest calculation rates.

\begin{figure*}[ht]
\begin{center}
\includegraphics[width=0.9\linewidth]{pic/tsne/tsne_perp100_lr500_iter1000.pdf}
\end{center}
\caption{The Visualization of the Text Embeddings of Question-Answer Pairs Across Disciplines.}
\label{fig:tsne}
\end{figure*}
\input{table/difficulty}


\begin{figure}[ht]
\centering
% \begin{minipage}{0.5\textwidth} % 设置第一张图的宽度
    \centering
    % \vspace{0.75cm}  % 向下移动左侧图片，调整距离
    \includegraphics[width=0.7\columnwidth, trim={0 0 -5cm 0}, clip]{pic/heatmap_benchmark.pdf}
    \caption{The Correlation Coefficients between Different Benchmarks. For MMLU, we record the results under the 5-shot setting.}
    \label{fig:correlation_benchmark}
% \end{minipage}%
\end{figure}


\paragraph{Sentence Length.} Question and answer length analysis reveals substantial variation across disciplines. The average question length is 58.42 tokens, with Literature questions showing the highest maximum length (869 tokens) and Engineering questions averaging 67.26 tokens. The ``answer'' options maintain a length pattern similar to the general options across different disciplines, averaging 12.86 tokens per option. Such a consistent length distribution across options aligns with one of the requirements of error option curation, which tries to confuse the models by providing confusing options in similar lengths.



\paragraph{Semantic Visualization.} As shown in \autoref{fig:tsne}, we employ t-SNE visualization of question-answer pair embeddings to visualize the distribution \benchmark \footnote{We use the \texttt{gte-large-en-v1.5}~\citep{li2023towards,zhang2024mgte} encoding model and set the t-SNE parameters as: perplexity 100, learning rate 500, and 1000 iteration. For clearer visualization, we randomly select maximum 1000 samples from each disciplines.} and to further show the comprehensiveness of it.
The resulting visualization demonstrates clear clustering patterns across disciplines while maintaining substantial overlap in conceptual spaces.
The Engineering and Science demonstrate the highest degree of embedding overlap, suggesting strong semantic similarities in their Q\&A patterns. The humanities cluster (Literature, Philosophy, History) shows diffuse boundaries but maintains distinct centroids. The Military Science exhibits relatively isolated embedding patterns, indicating unique domain-specific language.
This analysis shows aligning conclusions from \autoref{fig:correlation_subclass} and reveals that the semantic structure of the \benchmark pairs reflects both the traditional organization of academic disciplines and their natural intellectual relationships, effectively capturing both domain-specific knowledge and cross-disciplinary connections.
% with quantifiable patterns of overlap and distinction in their linguistic characteristics. 
% suggesting that our benchmark effectively captures both domain-specific knowledge and cross-disciplinary connections.

 
\begin{figure}[ht]
% \begin{minipage}{0.5\textwidth} % 设置第二张图的宽度
    \centering
    \includegraphics[width=0.8\columnwidth, trim={0 0 -4cm 0}, clip]{pic/heatmap_subclass.pdf}
    \caption{The Correlation Between Disciplines in \benchmark.}
    \label{fig:correlation_subclass}
% \end{minipage}
\end{figure}

% \begin{figure}[htbp]
%     \centering
%     \begin{subfigure}[b]{0.22\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{pic/pie_difficulty/Agriculture.pdf}
%         \caption{Agriculture.}
%     \end{subfigure}
%     % \hfill
%     \begin{subfigure}[b]{0.22\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{pic/pie_difficulty/Economics.pdf}
%         \caption{Economics.}
%     \end{subfigure}
%     \begin{subfigure}[b]{0.22\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{pic/pie_difficulty/Education.pdf}
%         \caption{Education.}
%     \end{subfigure}
%     \\
%     \begin{subfigure}[b]{0.22\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{pic/pie_difficulty/Engineering.pdf}
%         \caption{Engineering.}
%     \end{subfigure}
%     % \hfill
%     \begin{subfigure}[b]{0.22\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{pic/pie_difficulty/History.pdf}
%         \caption{History.}
%     \end{subfigure}
%     \begin{subfigure}[b]{0.22\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{pic/pie_difficulty/Law.pdf}
%         \caption{Law.}
%     \end{subfigure}
%     \\
%     \begin{subfigure}[b]{0.22\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{pic/pie_difficulty/Literature.pdf}
%         \caption{Literature.}
%     \end{subfigure}
%     % \hfill
%     \begin{subfigure}[b]{0.22\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{pic/pie_difficulty/Management.pdf}
%         \caption{Management.}
%     \end{subfigure}
%     \begin{subfigure}[b]{0.22\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{pic/pie_difficulty/Medicine.pdf}
%         \caption{Medicine.}
%     \end{subfigure}
%     \\
%         \begin{subfigure}[b]{0.22\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{pic/pie_difficulty/Military Science.pdf}
%         \caption{Military Science.}
%     \end{subfigure}
%     % \hfill
%     \begin{subfigure}[b]{0.22\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{pic/pie_difficulty/Philosophy.pdf}
%         \caption{Philosophy.}
%     \end{subfigure}
%     \begin{subfigure}[b]{0.22\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{pic/pie_difficulty/Science.pdf}
%         \caption{Science.}
%     \end{subfigure}
%     \begin{subfigure}[b]{0.22\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{pic/pie_difficulty/Sociology.pdf}
%         \caption{Sociology.}
%     \end{subfigure}
%     \caption{The Difficulty Distribution of Field-Subfield Across Disciplines. }
%     % \tikz\draw[fill=boxcolorgreen, draw=boxcolorgreen, thick, rounded corners=10pt] (0,0) rectangle (0.6,0.3);
%     % hard, 
%     % \tikz\draw[fill=boxcolororange, draw=boxcolororange, thick, rounded corners=10pt] (0,0) rectangle (0.6,0.3);
%     % middle, 
%     % \tikz\draw[fill=boxcolorblue, draw=boxcolorblue, thick, rounded corners=10pt] (0,0) rectangle (0.6,0.3); easy.}
%     \label{fig:discipline_difficulty_distribution}
% \end{figure}
