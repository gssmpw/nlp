% ****** Start of file apssamp.tex ******
%
%   This file is part of the APS files in the REVTeX 4.2 distribution.
%   Version 4.2a of REVTeX, December 2014
%
%   Copyright (c) 2014 The American Physical Society.
%
%   See the REVTeX 4 README file for restrictions and more information.
%
% TeX'ing this file requires that you have AMS-LaTeX 2.0 installed
% as well as the rest of the prerequisites for REVTeX 4.2
%
% See the REVTeX 4 README file
% It also requires running BibTeX. The commands are as follows:
%
%  1)  latex apssamp.tex
%  2)  bibtex apssamp
%  3)  latex apssamp.tex
%  4)  latex apssamp.tex
%
\documentclass[%
 reprint,
superscriptaddress,
%groupedaddress,
%unsortedaddress,
%runinaddress,
%frontmatterverbose,
%preprint,
%preprintnumbers,
%nofootinbib,
%nobibnotes,
%bibnotes,
 amsmath,amssymb,
 longbibliography,
%linenumbers,
%pra,
 prb,
%prx,
%rmp,
%prstab,
%prstper,
%floatfix,
]{revtex4-2}

% \usepackage{array}
% \usepackage{amsmath}

\usepackage[ruled]{algorithm2e}
\usepackage[noend]{algpseudocode}
\usepackage{wasysym}
%\usepackage{algorithmicx,algorithm}
\usepackage{graphicx}% Include figure files
\usepackage{dcolumn}% Align table columns on decimal point
\usepackage{bm}% bold math
\DeclareMathAlphabet{\mathpzc}{OT1}{pzc}{m}{it}
%\usepackage{hyperref}% add hypertext capabilities
%\usepackage[mathlines]{lineno}% Enable numbering of text and display math
%\linenumbers\relax % Commence numbering lines

%\usepackage[showframe,%Uncomment any one of the following lines to test
%%scale=0.7, marginratio={1:1, 2:3}, ignoreall,% default settings
%%text={7in,10in},centering,
%%margin=1.5in,
%%total={6.5in,8.75in}, top=1.2in, left=0.9in, includefoot,
%%height=10in,a5paper,hmargin={3cm,0.8in},
%]{geometry}

\begin{document}

\preprint{APS/123-QED}

\title{Generalized Lanczos method for systematic optimization of neural-network quantum states}

\author{Jia-Qi Wang}
\affiliation{Department of Physics, Renmin University of China, Beijing 100872, China}
\affiliation{Key Laboratory of Quantum State Construction and Manipulation (Ministry of Education), Renmin University of China, Beijing 100872, China}

\author{Rong-Qiang He}\email{rqhe@ruc.edu.cn}
\affiliation{Department of Physics, Renmin University of China, Beijing 100872, China}
\affiliation{Key Laboratory of Quantum State Construction and Manipulation (Ministry of Education), Renmin University of China, Beijing 100872, China}

\author{Zhong-Yi Lu}\email{zlu@ruc.edu.cn}
\affiliation{Department of Physics, Renmin University of China, Beijing 100872, China}
\affiliation{Key Laboratory of Quantum State Construction and Manipulation (Ministry of Education), Renmin University of China, Beijing 100872, China}
\affiliation{Hefei National Laboratory, Hefei 230088, China}

\date{\today}% It is always \today, but any date may be explicitly specified

\begin{abstract}
  Recently, artificial intelligence for science has made significant inroads into various fields of natural science research. In the field of quantum many-body computation, researchers have developed numerous ground state solvers based on neural-network quantum states (NQSs), achieving ground state energies with accuracy comparable to or surpassing traditional methods such as variational Monte Carlo methods, density matrix renormalization group, and quantum Monte Carlo methods. Here, we combine supervised learning, reinforcement learning, and the Lanczos method to develop a systematic approach to improving the NQSs of many-body systems, which we refer to as the NQS Lanczos method. The algorithm mainly consists of two parts: the supervised learning part and the reinforcement learning part. Through supervised learning, the Lanczos states are represented by the NQSs. Through reinforcement learning, the NQSs are further optimized. We analyze the reasons for the underfitting problem and demonstrate how the NQS Lanczos method systematically improves the energy in the highly frustrated regime of the two-dimensional Heisenberg $J_1$-$J_2$ model. Compared to the existing method that combines the Lanczos method with the restricted Boltzmann machine, the primary advantage of the NQS Lanczos method is its linearly increasing computational cost.
\end{abstract}

% Through supervised training, this algorithm uses NQSs to represent the basis of the Lanczos method. We analyze the effects of underfitting in the training process and propose a reinforcement learning scheme for the amplitudes, which we refer to as the reinforcement-learning Lanczos (RLL) algorithm. Building on this, we propose an iterative procedure for the SLL algorithm to approach the ground state.

\keywords{neural-network quantum states, the Lanczos method, supervised learning, reinforcement learning}%Use showkeys class option if keyword display desired

\maketitle

%\tableofcontents


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%--------------------------------- Section I ----------------------------------%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
\label{sec:introduction}
The ground state of a quantum many-body system has long been one of the central topics in condensed matter physics. It is crucial for understanding fundamental physical properties. When the number of particles is large, the existing methods fail to find the exact ground state due to the ``exponential wall'' problem.

To obtain the ground state of a many-body system, many methods have been developed in the past, including the variational Monte Carlo (VMC) methods~\cite{Sandvik2007VBBasis,Sandvik2010_VBBasis+LoopUpdate,Sorella2013_VMC,Sandvik2007_VMC+TNS,Tagliacozzo2009_TreeTensorNetwork}, density matrix renormalization group~\cite{GongShouShu2014_DMRG,Stoudenmire2012_2DsystemDMRG,Sandvik2018_DMRG}, and the quantum Monte Carlo methods~\cite{Sandvik1997SSEQMC,Sandvik2010_VBBasis+LoopUpdate}, and have effectively advanced the field. In recent years, with the increasing use of artificial intelligence technologies in quantum many-body physics, researchers have started using neural networks to represent many-body wave functions, known as neural-network quantum states (NQSs). NQS-based methods are being rapidly developed~\cite{Juan2021PRX_NQSReview,Nomura2023RBMNQSReview,Hermann2023_NQSChemistryReview,Medvidovic2024NQSReview,Lange2024NQSReview}, whose performances are even surpassing those of traditional approaches.

NQSs were first introduced in 2017 by Carleo and Troyer~\cite{GiuseppeCarleo2017_RBM}, who applied restricted Boltzmann machines (RBMs)  with stochastic reconfiguration (SR) optimization~\cite{Sorella2007SR} to the field of quantum many-body computation. Since then, researchers have begun to study the problems~\cite{Claudio2020_NQS_SignProblem,westerhout2023SignStructure,Westerhout2020_difference_Sign_amp,Chen2022_SignStructure-NeuralNetwork} encountered in the optimization process and try to use new network architectures~\cite{Cai2018_ANN,Pfau2020Ferminet,HibatAllah2020_RNN_NQS,kochkov2021GNN,fu2022latticeCNN,roth2023_GCNN,Rende2024DeepViT,Lange2024RNN_NQS} and optimization algorithms~\cite{Chen2024MinSR,Drissi2024SecondOrderOptimization}. For example, Pfau \textit{et al.}~\cite{Pfau2020Ferminet} incorporated the antisymmetry of the exchange of fermions into the neural network and gave a ground state solver for small molecular systems. Westerhout \textit{et al.}~\cite{Westerhout2020_difference_Sign_amp} studied the generalization capabilities of NQSs in the Hilbert space and explored the optimization of both the amplitude and sign of the many-body wave functions for frustrated systems. Roth \textit{et al.}~\cite{roth2023_GCNN} incorporated group symmetries into convolutional neural networks (CNNs), enhancing the ability of CNN to handle two-dimensional lattice systems. Chen and Heyl~\cite{Chen2024MinSR} introduced the minimum-step stochastic reconfiguration (MinSR) algorithm, an improvement on the SR algorithm based on imaginary time evolution. The accuracy of the ground state energy obtained by the test (on the two-dimensional Heisenberg model with $J_2/J_1 = 0.5$) in Ref.~\cite{Chen2024MinSR} surpassed the one by traditional methods, highlighting the significant potential of NQSs in solving quantum many-body problems. However, the trade-off is that the network parameters quickly grow to millions. For example, in the latest simulations of the 10 Ã— 10 square-lattice Heisenberg $J_1$-$J_2$ model, Rende \textit{et al.}~\cite{Rende2024DeepViT} used 267,720 parameters, while Chen and Heyl~\cite{Chen2024MinSR} used 1,071,488 parameters. The approach of increasing the number of network parameters is not sustainable. As the network size increases, the computational cost increases dramatically, while the returns diminish rapidly.

The Lanczos method~\cite{Lanczos1952Method} is an approach to obtain accurate ground state or low-lying excitations of small quantum systems. It was combined with VMC methods~\cite{sorella2001VMC_Lanczos} and tensor networks~\cite{huang2018tensornetwork_Lanczos} for treating larger systems. The Lanczos method starts from an arbitrary state (which is not orthogonal to the ground state) and progressively constructs new states that are orthogonal to the previous states in the Krylov space. An orthogonal basis set can be constructed by these Lanczos states. The Hamiltonian matrix in this basis becomes tridiagonal. By diagonalizing it, an approximate ground state, represented as a superposition of these basis states, is obtained. The corresponding eigenvalue gives the approximate ground state energy.

In 2022, Chen \textit{et al.}~\cite{chen2022lanczos} applied the Lanczos method to two-dimensional lattice systems, where wave functions are represented by RBMs, successfully improving the energies. However, this method needs to calculate the expectations $\langle H^{2i + 1} \rangle$, where $H$ is the Hamiltonian, causing the computational cost to increase exponentially with step $i$ of the Lanczos method.

In this paper, we propose an alternative implementation of the Lanczos method, called the NQS Lanczos method, which consists of two parts: the supervised-learning Lanczos (SLL) and the reinforcement-learning Lanczos (RLL) algorithms. The SLL algorithm further consists of two parts: supervised training and diagonalization. Through supervised training, the SLL algorithm represents Lanczos states with NQSs, thereby avoiding the calculation of the expectations $\langle H^{2i + 1} \rangle$. When the supervised learning is terminated, a set of basis NQSs are constructed. Then a Hamiltonian matrix is constructed with these basis NQSs. Diagonalizing the Hamiltonian matrix, a superposition state is obtained. The RLL algorithm uses a reinforcement learning scheme for the amplitude part of the superposition state. When the RLL algorithm is terminated, an optimized superposition state and the improved energy are obtained. Using the optimized superposition state as a new state for the next iteration, the full loop of the NQS Lanczos method is achieved by repeating the procedure.

We tested the NQS Lanczos method in high-frustration regions of the two-dimensional Heisenberg $J_1$-$J_2$ model ($ 0.5 \lesssim J_2/J_1 \lesssim 0.6$) on square lattices with linear size $L=4,6 $, and $10$. The results show that the NQS Lanczos method can significantly improve the energies of the systems.

The organization of this paper is as follows. In Section~\ref{sec:method}, we first introduce the structure of the NQSs used for supervised learning. Next, we show the details of the supervised learning, including the loss functions and the optimization strategy employed. Following this, we introduce a reinforcement learning scheme specifically designed for the amplitude component. Finally, building on the aforementioned steps, we present the complete procedure of the NQS Lanczos method. In Section~\ref{sec:results}, we show the calculated results for the two-dimensional Heisenberg $J_1$-$J_2$ model and provide an analysis and discussion. Lastly, in Section~\ref{sec:summary}, we discuss the strengths and limitations of the NQS Lanczos method and provide an outlook for future improvements.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%--------------------------------- Section â…¡ ----------------------------------%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[t!]
  \includegraphics[width=16cm]{Lanczos_network.pdf}
  \caption{Schematic of the structure of the output layers of the sign network and the amplitude network. $v_{\text{out}}$ represents the output of the output layer, and $k_{\text{out}-1}$ denotes the convolutional kernel of the output layer. The flow labeled by symmetry means data argumentation by symmetry operations, which includes the spin-flip and the $\mathcal{C}_{\mathrm{4v}}$ symmetries. The output layer of the amplitude network has a single channel, while that of the sign network has two channels, which are used for the binary classification task of the sign (positive or negative). The outputs of the sign and amplitude network are denoted as $S$ and $A$, respectively. The corresponding wave function can be written as $\psi = S \cdot e^{A}$.}\label{fig:net}
\end{figure*}


\section{Method}
\label{sec:method}
In this section, we introduce a representation of the NQS. Based on this representation, we introduce the details of the SLL algorithm and design the loss function and optimization strategy. To address issues arising from underfitting in the SLL algorithm, we propose an RLL scheme to optimize the states further. The section concludes with an overview of the complete NQS Lanczos method.


\subsection{Neural-network quantum states}
One challenge in the optimization of NQS is the difficulty in handling the sign structure~\cite{Claudio2020_NQS_SignProblem,westerhout2023SignStructure}. In Ref.~\cite{Westerhout2020_difference_Sign_amp}, the optimization problems related to the sign and amplitude for frustrated systems were analyzed. Ref.~\cite{Jqwang2024aCNN} explored the case where only the amplitude is optimized, using a fixed sign structure. Choosing an appropriate neural-network architecture will facilitate the optimization. Various network architectures, such as RBM~\cite{Salakhutdinov2009DeepRBM}, multilayer perceptron~\cite{Rumelhart1986MLP}, recurrent neural network~\cite{Alex2013DeepRNN}, CNN~\cite{1Lecun1998CNN_LeNet}, and vision transformer~\cite{Dosovitskiy2021ViT}, have been used to represent NQSs~\cite{Lange2024NQSReview}. Among these, CNNs have been favored, with their intrinsic translation equivariance. Here we choose to use the real-valued CNN used in Ref.~\cite{Jqwang2024aCNN}, denoted as aCNN. The network is divided into two parts: the sign network ($\text{SNet}$) and the amplitude network ($\text{ANet}$).

The aCNN employs deep CNNs with shortcut connections as its main architecture. The shortcut connection is described in Ref.~\cite{he2015ResnetV1} (referred to as ResNet-v1). We have modified it to incorporate the structure presented in Ref.~\cite{he2016ResnetV2} (referred to as ResNet-v2). It has been widely demonstrated in the field of machine learning that ResNet-v2 is superior to ResNet-v1 in terms of network stability and expressiveness. The input to both the sign network and the amplitude network is a spin configuration $\sigma$ of the system. The only difference between the sign network and the amplitude network lies in the output layer, as shown in Fig.~\ref{fig:net}. 
The output layer of the amplitude network has only one channel, while the sign network has two. By calculating the mean value of the elements within the output channel of the amplitude network, the amplitude output is obtained and denoted as $A$. Meanwhile, for the sign network, the mean of each channel is computed to obtain $s_0$ and $s_1$, which are used for the binary classification task. The output of the sign network is denoted as $S$. Together, the sign and amplitude networks form a representation of a wave function. This representation is used in the following sections to represent the wave function of the Lanczos state at step $i$, and the wave function is denoted as $\psi_i = S_i \cdot e^{A_i}$.

Additionally, the implementation of lattice symmetries is divided into two parts. Translation symmetry is easily realized by maintaining a constant hidden layer size and averaging over the output layer. Spatial rotation, mirror reflection, and spin-flip symmetries are realized by performing data augmentation on the input configurations and averaging the outputs. This corresponds to the flow labeled by symmetry in Fig.~\ref{fig:net}.


\subsection{The supervised-learning Lanczos algorithm}
The Lanczos method~\cite{Lanczos1952Method} was first introduced by Cornelius Lanczos in 1952. In 2013, Hu \textit{et al.}~\cite{Sorella2013_VMC} used this approach to further improve the states obtained by VMC in the calculation of many-body systems, achieving promising results at that time.

In this work, we use the Lanczos state (denoted as $|\psi_i\rangle$) at Lanczos step $i$ as the target of supervised learning. Through supervised learning, the sign network and amplitude network of the NQS are optimized so that the NQS can be used as an approximate representation of $|\psi_i\rangle$. The NQS and supervised learning are combined to implement the Lanczos procedure. The procedure is as follows.

i. Give an initial state $|\psi_0\rangle$. This state can be arbitrary (which is not orthogonal to the ground state). In this work, we use aCNN as the initial state, denoted as $|\psi^{\text{net}}_0\rangle$.

ii. Define $|v_1 \rangle = H | \psi^{\text{net}}_0 \rangle - a_0 | \psi^{\text{net}}_0 \rangle$, where 
$a_0 = {\langle \psi^{\text{net}}_0 | H | \psi^{\text{net}}_0 \rangle} / {\langle \psi^{\text{net}}_0 | \psi^{\text{net}}_0\rangle}$ is the expectation of the Hamiltonian on the state $|\psi^{\text{net}}_0\rangle$. Let $ b_1 |\psi^\mathrm{trg}_1 \rangle = |v_1 \rangle $, where $b_1 = \sqrt{{\langle v_1 |v_1 \rangle} / {\langle \psi^{\text{net}}_0 | \psi^{\text{net}}_0 \rangle}}$ is the normalization factor that normalizes $|v_1 \rangle$ with respect to $|\psi^{\text{net}}_0 \rangle$, namely $\langle \psi^\mathrm{trg}_1 | \psi^\mathrm{trg}_1 \rangle  = \langle \psi^{\text{net}}_0|\psi^{\text{net}}_0 \rangle$. The Lanczos state $|\psi^\mathrm{trg}_1\rangle$ is set as the target state to be learned. Through Monte Carlo sampling, the coefficients $a_0$ and $b_1$ can be calculated. Then the target state $|\psi^\mathrm{trg}_1\rangle$ can be expressed, with the coefficients $a_0$, $b_1$, and the initial state $|\psi_0\rangle$.

iii. Take a configuration $\sigma$ as a training sample for supervised learning. The sign and modulus of $\psi^\mathrm{trg}_1(\sigma)$ are taken as labels for the supervised learning of the sign network and the amplitude network, respectively. Then, the sign network $\text{SNet}_1$ and the amplitude network $\text{ANet}_1$ can be trained through supervised learning. Once the losses converge, the wave function $\psi^{\text{net}}_1 = S_1 \cdot e^{A_1}$ of the Lanczos state is obtained, that is, the Lanczos state is represented by a NQS.

iv. Define 
\begin{align}
    |v_{i+1} \rangle = H | \psi^{\text{net}}_{i} \rangle - a_{i} | \psi^{\text{net}}_{i} \rangle - b_{i} | \psi^{\text{net}}_{i-1} \rangle,
    \label{eq:v_i}
\end{align}
where
\begin{align}
    a_i &= \frac{\langle\psi^{\text{net}}_i|H|\psi^{\text{net}}_i\rangle}{\langle\psi^{\text{net}}_i|\psi^{\text{net}}_i\rangle}.
    \label{eq:a}
\end{align}
The Lanczos state $| \psi_{i+1}^\mathrm{trg} \rangle$ is constructed as % by the equation
\begin{align}
    |\psi_{i+1}^\mathrm{trg} \rangle &=  \frac{|v_{i+1} \rangle} {b_{i+1}},
    \label{eq:psi_i}
\end{align} where
\begin{align} 
    b_{i+1} = \sqrt{\frac{\langle v_{i+1} |v_{i+1} \rangle}{\langle \psi^{\text{net}}_0|\psi^{\text{net}}_0\rangle}}.
    \label{eq:b}
\end{align}
The coefficients $a_i$ and $b_{i+1}$ are calculated through Monte Carlo sampling.

v. Train a sign network $\text{SNet}_i$ and an amplitude network $\text{ANet}_i$ through supervised learning. The wave function $\psi^{\text{net}}_{i+1} = S_{i+1} \cdot e^{A_{i+1}}$ at Lanczos step $i$ is obtained. 

This procedure completes the iteration of the supervised learning part of the SLL algorithm, which generates Lanczos states represented by NQSs.

When the iteration of the supervised training is completed, a set of basis NQSs will be obtained. Then a Hamiltonian matrix can be constructed with these NQSs. By diagonalizing the matrix, the lowest eigenvalue (i.e., the improved energy $E$) and the corresponding superposition state
\begin{align}
    |\Psi\rangle = \sum_{i=0}^{p} c_i |\psi_i^\mathrm{net} \rangle
\end{align}
can be obtained, where $c_i$ is the superposition coefficient, and $p$ is the number of Lanczos steps. Details of the diagonalization process are given in Appendix~\ref{sec:diagonalization}.


\subsection{Loss function and optimization}
The effectiveness of using a NQS to approximate a Lanczos state depends on the performance of the supervised learning.

The goal of supervised learning of the amplitude network is to make the distribution described by $|\psi^\mathrm{net}_i|^2$ consistent with the distribution described by $|\psi^\mathrm{trg}_i|^2$. In deep learning field, the Kullback-Leibler (KL) divergence is often used to measure the difference between two distributions. However, in our tests, the KL divergence loss function performed poorly. We choose to use the loss function based on the mean squared error, $\mathbb{E}_{x \sim p}||y_x - f(x)||$. It is to minimize the difference squared $\left( |\psi^{\text{trg}}_{i}| - |\psi^{\text{net}}_{i}| \right)^2$ between the target and the prediction. To achieve this, samples should be obtained from the distributions described by both $ |\psi^{\text{trg}}_{i}|^2$ and $|\psi^{\text{net}}_{i}|^2$. The loss function of the amplitude part is
\begin{align}
    L_{\text{amp}} &= \frac{1}{2} \sum_{\sigma} p^{\text{trg}}_{i}(\sigma) \left(| \psi^{\text{trg}}_{i}(\sigma)| - |\psi^{\text{net}}_{i}(\sigma, \theta)| \right)^2 \nonumber \\ &+ \frac{1}{2} \sum_{\sigma} p^{\text{net}}_{i}(\sigma, \theta)  \left(| \psi^{\text{trg}}_{i}(\sigma)| - |\psi^{\text{net}}_{i}(\sigma, \theta)| \right)^2,
    \label{eq:amp_loss1}
\end{align}
% \sim p^{\text{trg}}_{i}
where the probability distributions are
\begin{align}
    p^{\text{trg}}_{i}(\sigma) = \frac{\left| \psi^{\text{trg}}_{i}(\sigma) \right|^2}{\sum_{\sigma} \left| \psi^{\text{trg}}_{i}(\sigma) \right|^2}
\end{align}
and
\begin{align}
    p^{\text{net}}_{i}(\sigma, \theta) = \frac{\left| \psi^{\text{net}}_{i}(\sigma, \theta) \right|^2}{\sum_{\sigma} \left| \psi^{\text{net}}_{i}(\sigma, \theta) \right|^2}.
\end{align}

It is worth noting that the probability distribution $p^{\text{trg}}_{i}(\sigma)$ and the label $\psi^{\text{trg}}_{i}(\sigma)$ are both determined by the previous NQSs, whose parameters are fixed and are not involved in optimization here. While the probability distribution $p^{\text{net}}_{i}(\sigma, \theta)$ contains the parameters $\theta$ to be optimized. The gradient with respect to these parameters must be considered.
According to the theory of automatic differentiation Monte Carlo (ADVMC)~\cite{zhang2023ADVMC}, the expectation of an observable $O(\sigma, \theta)$ is given by
\begin{align}
    \langle O(\sigma, \theta)\rangle_{p(\sigma, \theta)} = \frac{\langle \frac{p(\sigma, \theta)}{\perp(p(\sigma, \theta))} O(\sigma, \theta) \rangle_{\perp(p(\sigma, \theta))}}{\langle \frac{p(\sigma, \theta)}{\perp(p(\sigma, \theta))}\rangle_{\perp(p(\sigma, \theta))}},
    \label{eq:advmc}
\end{align}
where $\perp(\theta)$ is a function defined as $\perp(\theta) = \theta$ in forward propagation, and ${\partial\perp(\theta)} / {\partial \theta} = 0$ in backward propagation. This reformulation applies to both normalized and unnormalized probability distributions. The loss function for the amplitude network can be rewritten as
\begin{align}
    L_{\text{amp}} &= \frac{1}{2} \mathbb{E}_{\sigma\sim p^{\text{trg}}_{i}} \left(| \psi^{\text{trg}}_{i}(\sigma)| - |\psi^{\text{net}}_{i}(\sigma, \theta)| \right)^2 \nonumber \\
    &+ \frac{1}{2} \frac{\mathbb{E}_{\sigma\sim p^{\text{net}}_{i}}  \frac{p^{\text{net}}_{i}(\sigma, \theta)}{\perp(p^{\text{net}}_{i}(\sigma, \theta))}  \left(| \psi^{\text{trg}}_{i}(\sigma)| - |\psi^{\text{net}}_{i}(\sigma, \theta)| \right)^2 }  {\mathbb{E}_{\sigma\sim p^{\text{net}}_{i}} \frac{p^{\text{net}}_{i}(\sigma, \theta)}{\perp(p^{\text{net}}_{i}(\sigma, \theta))}}.
    \label{eq:amp_loss}
\end{align}
% \frac{1}{N_{\sigma}^{\mathrm{trg}}}  \sigma \sim p^{\text{trg}}_{i}
% where $N_{\sigma}^{\mathrm{trg}}$ and $N_{\sigma}^{\mathrm{net}}$ are the number of configurations sampled from $p^{\text{trg}}_{i}$ and $p^{\text{net}}_{i}$, respectively.

\begin{figure}[ht]
  \centering 
  \includegraphics[width=\columnwidth]{loss_distribution_amp.pdf}
  \caption{Illustration of the loss function of the amplitude network. The horizontal axis represents the configurations, while the vertical axis represents the probability density. The curves labeled by `trg' and `net' serve as schematic representations of the distributions described by $|\psi^{\text{trg}}_{i}(\sigma)|^2$ and $|\psi^{\text{net}}_{i}(\sigma, \theta)|^2$, respectively. The arrows indicate the expected changes in the `net' distribution when supervised learning. Both those two distributions are used to generate the samples for supervised learning.}
  \label{fig:amp_loss}
\end{figure}

% Before training, the parameters of $|\psi^\mathrm{net}_i\rangle$ are initialized by Kaiming initialization~\cite{he2015KaimingInitilization} with a specified random seed. It is worth noting that the initialized $|\psi^\mathrm{net}_i\rangle$ is not normalized. The loss function Eq.~\eqref{eq:amp_loss} is suitable for this situation.

The illustrations of this loss function are shown in Fig.~\ref{fig:amp_loss}. The horizontal axis represents the configurations, while the vertical axis represents the probability density. The arrows indicate the expected changes in the â€˜netâ€™ distribution when supervised learning. Sampling from these two distributions separately ensures that $| \psi^{\text{net}}_{i}(\sigma)|$ can be adjusted for the configurations $\sigma$ where $| \psi^{\text{trg}}_{i}(\sigma)|$ are extremely small.

The optimization of the sign network is a binary classification task. The target is to learn the sign of $\psi^{\text{trg}}_{i}(\sigma)$. In this work, we choose to use the commonly used cross-entropy loss function
\begin{align}
    L_{\text{sign}} &= -\mathbb{E}_{\sigma} \left[ \hat{y}_{\sigma}\log q_\sigma + (1-\hat{y}_{\sigma})\log(1-q_\sigma) \right],
    \label{eq:sign_loss}
\end{align}
where $\hat{y}_{\sigma}$ denotes the label, and $q_\sigma$ represents the prediction. Unlike the amplitude part, training the sign network with samples from only the target distribution is sufficient.

In addition, sampling from the distribution described by $|\psi_i^\mathrm{trg}|^2$ to generate a fixed dataset before training is beneficial. The data in the dataset can be read directly and used for network optimization.

In practice, samples and labels from $|\psi_i^\mathrm{trg}\rangle$ are prepared in advance, denoted as $\sigma_{\text{trg}}$ and  $\psi^{\text{trg}}_i(\sigma_{\text{trg}})$. For the amplitude optimization, we sample $\sigma_{\text{net}}$ from the distribution described by $|\psi^\mathrm{net}_i|^2$, and calculate its label value $\psi^{\text{trg}}_i ( \sigma_{\text{net}} )$. Then both $\sigma_{\text{trg}}$ and $\sigma_{\text{net}}$ are passed into the network for forward propagation. For the sign optimization, samples are read directly from the dataset without the need for additional sampling.


\subsection{The reinforcement-learning Lanczos algorithm}
In the tests of aCNN~\cite{Jqwang2024aCNN}, the energies achieved by optimizing only the amplitude part of the wave function are more accurate than those of the complex-valued CNN~\cite{Choo2019_CNN_ComplexValued} with the same number of parameters. This inspires us to further optimize the amplitude of the superposition of NQSs to mitigate the negative effects of insufficient convergence in the supervised learning. We select an amplitude network $\text{ANet}_j$ with $ j\neq0$ to do a reinforcement learning, while fixing all the parameters of the other networks as well as the superposition coefficients.

During the reinforcement learning, we use energy as the loss function and employ the ADVMC method~\cite{zhang2023ADVMC} to calculate the derivative of energy with respect to the network parameters. The popular Adam algorithm~\cite{Kingma2015Adam} is used to update the parameters. The energy is expressed as
\begin{align}
    E &= \left< H \right> = \langle E_\mathrm{loc}(\sigma, \theta_i)\rangle_{p(\sigma, \theta_i)} \nonumber \\
    &= \frac{\langle \frac{\Psi^2(\sigma, \theta_i)}{\perp(\Psi^2(\sigma, \theta_i))} E_\mathrm{loc}(\sigma, \theta_i) \rangle_{\perp({p(\sigma, \theta_i)})}}{\langle \frac{\Psi^2(\sigma, \theta_i)}{\perp(\Psi^2(\sigma, \theta_i))}\rangle_{\perp({p(\sigma, \theta_i)})}},
    \label{eq:advmc_loss}
\end{align}
where $\theta_j$ represents the trainable parameters of the $j$-th amplitude network $\text{ANet}_j$,
\begin{align}
    E_\mathrm{loc}(\sigma, \theta_j) = \frac{\langle \sigma|H|\Psi_{\theta_j} \rangle}{\langle\sigma|\Psi_{\theta_j}\rangle} = \sum_{\sigma'}H_{\sigma\sigma'}\frac{\Psi(\sigma', \theta_j)}{\Psi(\sigma, \theta_j)}
\end{align}
is the local energy of the system,
\begin{align}
    p(\sigma, \theta_j)=\frac{|\Psi(\sigma, \theta_j)|^2}{\sum_{\sigma'}{|\Psi(\sigma', \theta_j)|^2}}
\end{align}
is the probability distribution, and $\langle \cdot \rangle$ represents the expectation.

The reinforcement learning effectively optimizes the amplitude network $\text{ANet}_j$ further. The goal is not to make $|\psi^\mathrm{net}_j|$ match $|\psi^\mathrm{trg}_j|$ but to make the overall superposition state $|\Psi\rangle$ approach the true ground state. After optimizing each $\text{ANet}_j$ with $0<j \leq p$ (note that $\text{ANet}_0$ has already been obtained through amplitude optimization and does not need further optimization), we obtain a new state $|\tilde{\Psi}\rangle$. This new state can then be used as the updated $|\psi^\mathrm{net}_0\rangle$ for the next NQS Lanczos loop to further improve the results.


\subsection{The NQS Lanczos method}

\begin{figure*}[t!]
  \includegraphics[width=16cm]{Lanczos_algorithm_graph.pdf}
  \caption{Flowchart of the NQS Lanczos method. The method begins with $|\psi^\mathrm{net}_0\rangle$ as a starting point and ultimately outputs the improved energy $\tilde{E}$ and state $|\tilde{\Psi}\rangle$. $|\tilde{\Psi}\rangle$ can be used as a new $|\psi^\mathrm{net}_0\rangle$ for the next NQS Lanczos loop. The NQS Lanczos method consists of two parts, the SLL algorithm and the RLL algorithm. The SLL algorithm further consists of two parts: supervised learning of Lanczos states and diagonalization of the Hamiltonian matrix. The output of the SLL algorithm consists of the superposition state $|\Psi\rangle$ and the improved energy $E$. The RLL algorithm further optimizes $|\Psi\rangle$ and gives the final output of the NQS Lanczos method. The number of the Lanczos steps is denoted as $p$. The matrices $M$ and $H$ are used to produce the superposition state, as shown in Appendix~\ref{sec:diagonalization}.}
  \label{fig:SLL_algorithm}
\end{figure*}

In this section, we outline the entire procedure of the NQS Lanczos method, as shown in Fig.~\ref{fig:SLL_algorithm}. The NQS Lanczos method consists of two parts, the SLL algorithm and the RLL algorithm. The first step is to provide an initial state $|\psi^\mathrm{net}_0\rangle$, based on which the coefficients $a_i$ and $b_{i+1}$ are calculated. Then the target $|\psi^\mathrm{trg}_i\rangle$ is constructed, according to which, samples and labels are generated for supervised learning. Through the supervised learning, both the sign and the amplitude networks are optimized. The wave function $\psi^\mathrm{net}_i = S_i e^{A_i}$ represented by NQS is obtained. As the Lanczos step $i$ goes, a set $\{ |\psi^\mathrm{net}_i\rangle \}$ ($i=0, \dots ,p$) is obtained.

Next, we construct a Hamiltonian matrix $H$ and a Hermitian matrix $M$ (see Appendix~\ref{sec:diagonalization}) in the space spanned by the set of the NQSs. Diagonalizing the Hamiltonian matrix, a superposition state $|\Psi\rangle$ composed of the NQSs and an improved energy $E$ are obtained.

Finally, reinforcement learning is applied to optimize all amplitude networks $\text{ANet}_j$ (except $\text{ANet}_0$) of the superposition state $|\Psi\rangle$. This produces the final improved state $|\tilde{\Psi}\rangle$, on which the Hamiltonian expectation is calculated to obtain the final improved energy $\tilde{E}$.

The superposition state $|\tilde{\Psi}\rangle$ can be used as a new initial state $|\psi^\mathrm{net}_0\rangle$. Then the NQS Lancozs method can be repeated to further improve the results.

Looking at the overall flowchart of the NQS Lanczos method, one can see that the computational complexity grows linearly with the number of the Lanczos steps $p$. The largest computational cost comes from the calculation of the parameters $a_i$ (the expectation of $H$), the parameters $b_i$ (involving $H|\psi^\mathrm{net}_{i}\rangle$), and the matrix elements of the matrices $M$ and $H$. The NQS Lanczos method avoids the calculation of the expectation $\langle H^{2i+1} \rangle$ that are required in Ref.~\cite{chen2022lanczos}. The growth rate of the computational cost in Ref.~\cite{chen2022lanczos} follows $\left( 4N^2 \right)^{2i+1}$, where $N$ is the number of lattice sites. This limits the maximum number of the Lanczos steps $p$. The computational cost of the NQS Lanczos method enables more Lanczos steps to be performed even with limited computational resources.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%--------------------------------- Section â…¢ ----------------------------------%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Numerical results and discussions}
\label{sec:results}
In this section, we apply the NQS Lanczos method to the two-dimensional Heisenberg $J_1$-$J_2$ model on the square lattices with $L = 4, 6$, and $10$. The Hamiltonian is
\begin{equation}
    H = J_1 \sum_{\langle ij \rangle}{\hat{S_i} \cdot \hat{S_j}} + J_2    \sum_{\langle \langle ij \rangle \rangle}{\hat{S_i} \cdot \hat{S_j}},
\end{equation}
where $J_1$ and $J_2$ are the coupling strengths of nearest-neighbor sites and the next nearest-neighbor sites (denoted as $\langle ij \rangle$ and $\langle \langle ij \rangle \rangle$), respectively. This model is strongly frustrated when $J_2/J_1 $ is around $0.5$. It is challenging to theoretically study this region. As the ratio $J_2 / J_1$ increases from $0$, the system undergoes a phase transition~\cite{Zheng-ChengGu2022_TensorNetwork+PEPS} from a Neel antiferromagnet to a gapless quantum spin liquid, then to a valence-bond solid, and finally to a collinear antiferromagnet. In this section, we test the NQS Lanczos method in the highly frustrated region. The test results from different stages (SLL and RLL) of the method are presented to comprehensively demonstrate the effectiveness of the method.

In each test, $|\psi^\mathrm{net}_0\rangle$ is obtained through the optimization of the amplitude network with a sign structure, which is fixed to the Marshall sign rule~\cite{1955MarshallSign, Jqwang2024aCNN}. The following tests all use the checkerboard patterned sign structure. Additionally, due to the highly rugged landscape of the wave function, the optimization results can be influenced by several factors, such as the initialization method of the network parameters, the seed of the random numbers for the initialization, the learning rate, and the optimization algorithm. The networks to be trained in the supervised learning are initialized by Kaiming initialization~\cite{he2015KaimingInitilization}. To reduce the fluctuation caused by initialization, we use four different seeds to initialize the network and retain the result with the best optimization performance. The Adam algorithm is used for all optimization. All calculations in this work are performed on a single NVIDIA 3090 GPU.


\subsection{Test of the supervised learning Lanczos algorithm on the $4 \times 4$ lattice}
We first test our method on the $L = 4$ square lattice with $J_2 / J_1 = 0.55$. The test results are shown in Fig.~\ref{fig:L4}. The improved energies are obtained from the SLL algorithm. The horizontal axis represents the number of the Lanczos steps. The vertical axis indicates the error of the improved energies with respect to the energy obtained from the exact diagonalization (ED) method. The error decreases exponentially from $ 6.32 \times 10^{-3}$ at $p = 0$ to $1.73\times10^{-8}$ at $p = 5$.

\begin{figure}[ht]
  \centering 
  \includegraphics[width=6cm]{L44_superivised_Lanczos.pdf}
  \caption{Improved energies on the $4 \times 4$ lattice. The horizontal axis represents the number of Lanczos steps. The vertical axis indicates the error of the improved energies with respect to the energy obtained from ED. As the number of the Lanczos steps increases, the energy rapidly converges to the exact one.}
  \label{fig:L4}
\end{figure}

For the $L = 4$ square lattice, the Hilbert space can be traversed easily, the coefficients $a_i$ and $b_{i+1}$ can be calculated accurately, and the loss can be optimized to $10^{-8}$. In this case, the supervised trained states are almost strictly orthogonal. The combination of the Lanczos method, supervised learning, and NQSs is successful. The SLL algorithm can perfectly implement the Lanczos method without the need for the RLL algorithm.


\subsection{Test of the supervised-learning Lanczos algorithm on the $6 \times 6$ and $10 \times 10$ lattices}
In the test of the $4\times4$ lattice, the supervised learning can effectively capture all the characteristics of the target. This enables the Lanczos method to be implemented perfectly with NQSs. However, as the lattice size increases, the Hilbert space grows exponentially and the configurations cannot be traversed. The dataset cannot describe a complete picture of the wave function. In this situation, capturing the main characteristics of the target to achieve an effective representation of the target state poses a significant challenge to the learning and generalization abilities of the neural network.

The performance of the SLL algorithm on the square lattices with $L = 6$ and $L = 10$ is tested in this section. For the $L = 6$ lattice, the aCNN is used with the ResNet-v2 connection. We test the cases of $p = 1$ and $p = 2$, and the results are shown in Fig.~\ref{fig:L6}. The horizontal axis represents the number of the Lanczos steps. The vertical axis represents the error ($(E - E_{\mathrm{ED}} ) \times 10^{4}$) of the improved energies with respect to the energy from ED. For $L = 10$, we continue to use the aCNN with the ResNet-v1 connection and test the case of $p = 1$, and the results are shown in Fig.~\ref{fig:L10}. The points with $p = 0$ indicate the energies of the initial Lanczos states, while for $p > 0$, the points labeled by SLL indicate the improved energies obtained by the SLL algorithm.

\begin{figure}[t!]
  \includegraphics[width=\columnwidth]{L6_superivised_Lanczos.pdf}
  \caption{Improved energies on the $6 \times 6$ lattice. The model is test with $J_2/J_1 = 0.5$, $0.55$, and $0.6$. The horizontal axis represents the number of the Lanczos steps. The vertical axis indicates the error ($(E - E_{\mathrm{ED}} ) \times 10^{4}$) of the improved energies with respect to the energy obtained from ED. At $p \neq 0$, the points of SLL indicate the energies obtained by the SLL algorithm. At $p = 0$, the points of SLL indicate the energies of the initial states. The points of RLL ($p = n$, $\text{ANet}_j$) indicate the energies obtained by the RLL algorithm. The superposition state consists of $n+1$ NQSs. $\text{ANet}_j$ indicates that the amplitude network of Lanczos step $j$ is optimized in the RLL algorithm.}
  \label{fig:L6}
\end{figure}

\begin{figure}[t!]
  \includegraphics[width=\columnwidth]{L10_superivised_Lanczos.pdf}
  \caption{Improved energies. Similar to Fig.~\ref{fig:L6}, but on the $10 \times 10$ lattice. The result of MinSR is from Ref.~\cite{Chen2024MinSR}.}
  \label{fig:L10}
\end{figure}

As shown in Figs.~\ref{fig:L6} and~\ref{fig:L10}, compared to $p = 0$, the energies obtained by the SLL algorithm are significantly improved at $p = 1$ for both $L = 6$ and $L = 10$. Furthermore, the improvement remains highly effective at $p = 2$. This shows that the SLL algorithm remains effective even on lattices with larger sizes.

\begin{table}[t!]
  \begin{center}
    \caption{Accuracy of sign prediction of the sign net and loss of the amplitude net.}
    \label{tab:accuracy_loss}
    \setlength{\tabcolsep}{1.5mm}{
      \begin{tabular}{l l r@{.}l r@{.}l r@{.}l r@{.}l r@{.}l r@{.}l}
        \hline
        \hline
        \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{6}{c}{Sign accuracy} & \multicolumn{6}{c}{Amplitude loss} \\
        \hline
        \multicolumn{1}{l}{} & \multicolumn{1}{l}{$J_2/J_1$} & \multicolumn{2}{l}{$0.5$} & \multicolumn{2}{l}{$0.55$} & \multicolumn{2}{l}{$0.6$} & \multicolumn{2}{l}{$0.5$} & \multicolumn{2}{l}{$0.55$} & \multicolumn{2}{l}{$0.6$} \\
        %\multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{2}{c}{$J_2/J_1=0.5$} & \multicolumn{2}{c}{$J_2/J_1=0.55$} & \multicolumn{2}{c}{$J_2/J_1=0.6$} & \multicolumn{2}{c}{$J_2/J_1=0.5$} & \multicolumn{2}{c}{$J_2/J_1=0.55$} & \multicolumn{2}{c}{$J_2/J_1=0.6$} \\
        \hline
        $L=6$  & $|\psi^\mathrm{net}_0\rangle$ & 95&8\%   & 96&4\%   & 98&8\%      & 0&21    & 0&15     & 0&10         \\
               & $|\psi^\mathrm{net}_1\rangle$ & 86&0\%   & 87&0\%   & 87&0\%      & 0&44    & 0&44     & 0&43          \\
        \hline
        $L=10$ & $|\psi^\mathrm{net}_0\rangle$ & 91&2\%   & 89&6\%   & \multicolumn{2}{c}{} & 0&46  & 0&32  &  \multicolumn{2}{c}{} \\
        \hline
        \hline
      \end{tabular}
    }
  \end{center}
\end{table}

\begin{figure*}[t]
  \includegraphics[width=16cm]{L10_Jp5_55_phi1_amp.pdf}
  \caption{Target labels $\psi^\mathrm{trg}_1(\sigma)$ vs. predicted values $\psi^\mathrm{net}_1(\sigma)$. The test is on a square lattice with $L = 10$, $J_2/J_1 = 0.5$ and $0.55$. The horizontal axis represents the configurations sampled from the distributions described by $|\psi^{\text{trg}}_{1}|^2$ and $|\psi^{\text{net}}_{1}|^2$. These configurations are sorted by their labels in ascending order. The vertical axis represents the absolute values of $\psi^\mathrm{trg}_1(\sigma)$ and $\psi^\mathrm{net}_1(\sigma)$.}
  \label{fig:amp_lable_net}
\end{figure*}

In fact, the convergence of the loss functions in this test is not ideal. In Table~\ref{tab:accuracy_loss}, we present the prediction accuracy of the sign parts and the loss of the amplitude parts in the supervised learning. Compared to the $L = 4$ case, as the lattice size increases, the loss becomes much more difficult to converge. The accuracy of the sign prediction drops from 100\% for $L = 4$ to 95\% for $L = 6$ and to 89\% for $L = 10$. The order of magnitude of the amplitude loss increases rapidly from $10^{-8}$ for $L=4$ to $10^{-1}$ for $L=6$ and $10$.

Figure~\ref{fig:amp_lable_net} shows the comparison between the predicted values $\psi^\mathrm{net}_1(\sigma)$ and the labels $\psi^\mathrm{trg}_1(\sigma)$ in the test of the $L = 10$ lattice with $J_2/J_1 = 0.5$, and $0.55$. The horizontal axis represents the configurations. These configurations are sampled from the distributions described by $|\psi^{\text{trg}}_{1}|^2$ and $|\psi^{\text{net}}_{1}|^2$, respectively, and are sorted by their labels in ascending order. The vertical axis represents the absolute values of $\psi^\mathrm{trg}_1(\sigma)$ and $\psi^\mathrm{net}_1(\sigma)$. The predictions of the amplitude network are generally close to the labels, which means the main characteristics of the target are captured. However, the poor accuracy of the predictions indicates underfitting.

As the lattice size increases, the supervised learning fails to optimize the loss function to a sufficient level of accuracy. This results in discrepancies between $|\psi^\mathrm{net}_i|$ and the target $|\psi^\mathrm{trg}_i|$.
In the test for $L = 10$, the training sets contain 1.2 million (approximately $2^{20}$) samples obtained by Monte Carlo sampling. This is minuscule compared to the Hilbert space dimension $2^{100}$. However, the supervised trained networks are still able to accurately predict more than 85\% of the signs and learn the key characteristics of the amplitudes. The generalization ability of the neural networks far exceeds our expectations.

It is foreseeable that as the state approaches the true ground state, the difficulty of the supervised learning task will increase. As the state gradually approaches the eigenstate, the difference between the two states $|\psi\rangle$ and $H|\psi\rangle$ will gradually decrease and become very small. The target of the supervised learning becomes a lot of small quantities mixed with a large number of small and insignificant values caused by computational errors. These small quantities can not be effectively identified and learned by the supervised learning. This problem will become even more severe when the Hilbert space cannot be traversed. We speculate that the reason why the SLL algorithm remains effective in the aforementioned tests even though the accuracy of the prediction of signs does not reach 100\% is that the supervised learning passively abandons the learning of useless and random signs associated with the labels.

The numbers of the parameters of the sign network and the amplitude network are 6,614 and 6,538, respectively, in the test on the 6 Ã— 6 and 10 Ã— 10 lattices. If computational resources are sufficient, using neural networks with more parameters and more samples can further reduce the loss function of supervised learning and yield better results.


\begin{table*}[t!]
  \begin{center}
    \caption{Energies obtained from the NQS Lanczos method. SLL ($p = n$) represents the energy obtained by the SLL algorithm. RLL ($p = n$, $\text{ANet}_j$) denotes the energies obtained through the reinforcement learning of $\text{ANet}_j$, which is the amplitude network in the $j$-th NQS constituting the superposition state. The result of MinSR is from Ref.~\cite{Chen2024MinSR}.}
    \label{tab:improved_energy}
    \setlength{\tabcolsep}{2.4mm}{
      \begin{tabular}{l l r@{.}l r@{.}l r@{.}l}
        \hline
        \hline
        \multicolumn{1}{l}{} & \multicolumn{1}{l}{Methods} & \multicolumn{2}{c}{$J_2/J_1=0.5$} & \multicolumn{2}{c}{$J_2/J_1=0.55$} & \multicolumn{2}{c}{$J_2/J_1=0.6$} \\
        \hline
        $L=6$  & SLL ($p=0$)             & -0&502184(2)  & -0&490953(3)  & -0&482273(4)        \\
               & SLL ($p=1$)             & -0&502918     & -0&492535     & -0&486399           \\
               & SLL ($p=2$)             & -0&503049     & -0&492820     & -0&487125           \\
               & RLL ($p=1$, $\text{ANet}_1$) & -0&503052(3)  & -0&493142(5)  & -0&488024(7)        \\
               & RLL ($p=2$, $\text{ANet}_1$) & -0&503119(4)  & -0&493357(5)  & -0&488569(7)        \\
               & RLL ($p=2$, $\text{ANet}_1$ \& $\text{ANet}_2)$ & -0&503178(4)  & -0&493471(5)  & -0&488799(7)  \\
               & ED                      & -0&503810     & -0&495178     & -0&493239            \\
        \hline
        $L=10$ & SLL ($p=0$)             & -0&495627(6)  & -0&483490(5)  &  \multicolumn{2}{c}{} \\
               & SLL ($p=1$)             & -0&496064     & -0&484010     &  \multicolumn{2}{c}{} \\
               & RLL ($p=1$, $\text{ANet}_1$) & -0&496102(7)  & -0&484811(8)  &  \multicolumn{2}{c}{} \\
               & MinSR                   & -0&497715(9)  &  \multicolumn{2}{c}{} &  \multicolumn{2}{c}{} \\
        \hline
        \hline
      \end{tabular}
    }
  \end{center}
\end{table*}


\subsection{Test of the reinforcement-learning Lanczos algorithm on the $6 \times 6$ and $10 \times 10$ lattices}
In the above tests, although the effectiveness of the combination of supervised learning with the Lanczos method has been demonstrated, the difficulty of loss convergence makes it impossible for the Lanczos state to be accurately represented by the NQS. In this section, we test the RLL algorithm on the superposition states obtained from the previous tests. For a superposition state $|\Psi\rangle$ that will be optimized by the RLL algorithm, all parameters of the sign networks and the superposition coefficients are fixed. The amplitude network $\text{ANet}_j$ ($j\neq0$) will be optimized one by one until the energy no longer decreases.


\begin{table}[b!]
  \begin{center}
    \caption{Coefficients $a_i$ and $b^2_i$ in the test of the NQS Lanczos method on the square lattices with $L=6$ and $L=10$.}
    \label{tab:a_b}
    \setlength{\tabcolsep}{1.2mm}{
      \begin{tabular}{l c r@{.}l r@{.}l r@{.}l}
        \hline
        \hline
        \multicolumn{1}{l}{} & \multicolumn{1}{l}{$J_2/J_1$} & \multicolumn{2}{c}{$0.5$} & \multicolumn{2}{c}{$0.55$} & \multicolumn{2}{c}{$0.6$} \\
        \hline
        $L=6$  & $a_0$   & -0&502184(2)   & -0&490953(3)   & -0&482273(4)          \\
               & $b^2_1$ & 0&000155(1)   & 0&000319(2)   & 0&000609(4)          \\
               & $a_1$   & -0&3253(3)     & -0&3407(3)     & -0&3722(2)            \\
               & $b^2_2$ & 0&0206(1)     & 0&0174(1)     & 0&0120(1)            \\
        \hline
        $L=10$ & $a_0$   & -0&495627(6)   & -0&483490(5)   &  \multicolumn{2}{c}{} \\
               & $b^2_1$ & 0&0000817(4)  & 0&0001135(6)  &  \multicolumn{2}{c}{} \\
        \hline
        \hline
      \end{tabular}
    }
  \end{center}
\end{table}

The results obtained by the RLL algorithm for the $L = 6$ and $L = 10$ square lattices are shown in Figs.~\ref{fig:L6} and~\ref{fig:L10}, respectively. In these figures, RLL ($p = n$, $\text{ANet}_j$) denotes the energies obtained through the reinforcement learning for the superposition state, which consists of $n+1$ NQSs. $\text{ANet}_j$ indicate the $j$-th amplitude network is optimized. The specific energies are shown in Table~\ref{tab:improved_energy} of Appendix~\ref{sec:datas}. The energy improvement after the reinforcement learning is substantial. For $L = 6$, the improved energies achieved by optimizing only $\text{ANet}_1$ with $p = 1$ are better than those from SLL ($p = 2$). Moreover, the energy improvement increases further by optimizing $\text{ANet}_1$ and $\text{ANet}_2$ with $p = 2$.

The above results show that the RLL algorithm is very effective for the adjustment of the amplitude. We speculate that this adjustment partially compensates for the insufficient amplitude optimization during the supervised learning and further improves the generalization capability of the network.

For the NQS Lanczos method, it might be sufficient to retain only those steps where significant energy improvement is observed, such as $p = 1$ or $p = 2$. Combining the SLL algorithm with the RLL algorithm, and using the optimized superposition state as the updated $|\psi^\mathrm{net}_0\rangle$, as shown in Fig.~\ref{fig:SLL_algorithm}, is a promising path to further improve the results. Due to limited GPU computational resources, we have not done it yet. We will try it in the future.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%--------------------------------- Section â…£ ----------------------------------%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Summary and outlook}
\label{sec:summary}
In summary, we integrate supervised learning, reinforcement learning, NQS, and the Lanczos method to develop a method, namely the NQS Lanczos method, to iteratively find the ground state of quantum many-body systems. It consists of two main innovations: the supervised-learning Lanczos (SLL) and the reinforcement-learning Lanczos (RLL) algorithms. This method is tested in high-frustration regions of the two-dimensional Heisenberg $J_1$-$J_2$ model on the square lattices with $L=4,6$, and $10$. The results demonstrate its effectiveness.

The main advantage of the NQS Lanczos method is that the computational cost increases linearly with the number of Lanczos steps. The SLL algorithm avoids the calculation of the expectation $\langle H^{2i+1} \rangle$ which was required in Ref.~\cite{chen2022lanczos}. This allows for more Lanczos steps to be performed with limited computational resources.

Currently, in this method, the accuracy of the supervised learning limits the efficiency of convergence. This limitation may be addressed through the following approaches: (i) increasing the size of the neural networks to enhance their expressive power; (ii) exploring new neural-network architectures that are better suited for two-dimensional lattice systems; (iii) designing improved loss functions to achieve better optimization.

% The main code and data \footnote{The main code and data for the SLL is available on GitHub https://github.com/rhe/jqwang.} for the SLL is available on GitHub.


\begin{acknowledgments}
This work was supported by the National Key R\&D Program of China (Grants No. 2024YFA1408601 and No. 2024YFA1408602), the National Natural Science Foundation of China (Grant No. 12434009), and the Innovation Program for Quantum Science and Technology (Grant No. 2021ZD0302402). Computational resources were provided by the Physical Laboratory of High Performance Computing in Renmin University of China.
\end{acknowledgments}


\appendix

\section{Diagonalization of the Hamiltonian with a set of non-orthogonal basis states}
\label{sec:diagonalization}
For larger lattice sizes, the coefficients $a_i$ and $b_{i+1}$ are calculated approximately, and there are deviations between the NQS optimized by the supervised learning and the target state. The NQSs are not strictly orthogonal, and a Hamiltonian matrix constructed with these basis NQSs is not strictly tridiagonal. Therefore, it is necessary to transform the NQSs into an orthonormal basis set for diagonalization. The specific procedure is as follows.

For a set $\{ |\psi^\mathrm{net}_i\rangle \},i=0, \dots,p$, of NQSs obtained by the SLL algorithm, one can construct a Hermitian matrix $M$ whose element is
\begin{align}
    M_{ij} = \frac{\langle \psi^\mathrm{net}_i  | \psi^\mathrm{net}_j \rangle} {\langle \psi^\mathrm{net}_0 | \psi^\mathrm{net}_0 \rangle}.
\end{align}
Diagonalizing $M$ gives $ \Lambda = U^\dagger M U$ with eigenvalues $\lambda_i = \Lambda_{ii} > 0$. Define $S = U\sqrt{\Lambda^{-1}}$. An orthonormal basis set $\{ |\alpha_i \rangle\}$ can be obtained,
\begin{align}
    \{|\alpha_0\rangle, \cdots, |\alpha_{p}\rangle \} & = \{ |\psi^\mathrm{net}_0\rangle, \cdots, |\psi^\mathrm{net}_{p}\rangle \} S.
\end{align}
Define
\begin{align}
    H^{\psi}_{i, j} = \frac{\langle \psi^\mathrm{net}_i | H | \psi^\mathrm{net}_j \rangle} {\langle \psi^\mathrm{net}_0 | \psi^\mathrm{net}_0 \rangle}
\end{align}
and a Hermitian matrix $H^{\alpha} = S^\dagger H^\psi S$. Diagonalizing $H^{\alpha}$, one gets $H^{\alpha} \phi^\alpha = E \phi^\alpha$, 
where $E$ is the lowest eigenvalue, i.e., the improved energy, and $\phi^\alpha$ is the corresponding eigenvector and normalized to 1. Define $c = S \phi^\alpha$, then the superposition state
\begin{align}
    |\Psi\rangle = \sum_{i=0}^{p} c_i |\psi^\mathrm{net}_i\rangle 
    \label{eq:lc_NQSs}
\end{align}
is obtained, which is normalized to $\langle \psi^\mathrm{net}_0 | \psi^\mathrm{net}_0 \rangle$. The Hamiltonian expectation on $|\Psi\rangle$ is the improved energy $E$.


\section{Specific values in the test of the NQS Lanczos method}
\label{sec:datas}
This section presents specific values in the test of the NQS Lanczos method on square lattices with $L=6$ and $10$. The energies obtained by the SLL algorithm and the RLL algorithm are listed in Table~\ref{tab:improved_energy}. The coefficients $a_i$ and $b^2_i$ of the SLL algorithm are listed in Table~\ref{tab:a_b}.

\bibliography{reference}% Produces the bibliography via BibTeX.


\end{document}
%
% ****** End of file nqslanc.tex ******
