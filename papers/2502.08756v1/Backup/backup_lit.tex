
\subsection{The Raise of Transformer-based Language Models}

\subsection{Review of LLM in Software Engineering}
\citep{hou2023large} provides a comprehensive review of how large language models (LLMs) are being applied in software engineering (SE). The paper categorizes LLMs into different architectures such as encoder-only, encoder-decoder, and decoder-only models and analyzes their applications across various SE tasks like requirements engineering, code generation, bug fixing, program repair, and vulnerability detection. One of the key contributions is identifying the growing trend of using decoder-only models (e.g., GPT series) for automating code generation and completion tasks, showcasing their capability in reducing manual coding effort. However, challenges remain, including handling domain-specific knowledge, improving dataset quality, and managing the complexity of SE tasks that require more than simple text generation. The paper highlights the need for better preprocessing of data, fine-tuning of models for specific SE tasks, and the importance of using diverse datasets, particularly from industrial contexts, which are currently underrepresented in research. Future work proposes addressing these challenges through more focused studies on dataset variety, improving the robustness of LLMs in handling complex SE tasks, and refining evaluation metrics for assessing LLM performance in real-world SE environments. This paper significantly contributes to the field by providing a roadmap for future research in automating software development using LLMs, emphasizing the potential of these models to assist but not yet fully replace human developers in complex SE processes. 

\citet{fan2023large} provides an in-depth overview of how large language models (LLMs) like GPT, BERT, and Codex are being integrated into various software engineering tasks such as code generation, program repair, and automated testing. The paper identifies significant contributions to the automation of software development through generative AI, particularly in areas like automated code completion, bug detection, and software refactoring. It highlights how LLMs have revolutionized these tasks by reducing the manual effort required, improving coding efficiency, and enhancing developer productivity. However, the paper also notes several challenges, such as LLMs’ tendency to produce hallucinations (i.e., incorrect but plausible outputs), difficulties with domain-specific tasks, and the need for fine-tuning models to better suit specific software engineering contexts. It also mentions the problem of non-deterministic outputs from LLMs, which complicates scientific evaluation. Furthermore, the oracle problem persists in evaluating the correctness of generated solutions. The authors propose several areas for future work, including improving prompt engineering, hybridizing LLMs with traditional software engineering methods, and further research into how LLMs can be applied to underexplored areas like requirements engineering and software design. The integration of automated testing techniques with LLM outputs is also suggested as a way to mitigate hallucination and improve overall reliability.

\subsection{Evaluating LLM for Software Development}

\citet{liang2024can} explores the potential of GPT-4, a large language model (LLM), to replicate empirical software engineering research by identifying key assumptions and generating code for analysis pipelines. Through a user study involving 14 participants and seven empirical software engineering papers, the study found that GPT-4 could successfully surface correct assumptions, but it struggled with common knowledge and relevance, often lacking domain-specific expertise in software engineering. Although GPT-4 generated high-level analysis plans that accurately outlined research methodologies, the code it produced frequently contained errors such as incorrect data sources, missing steps, and implementation-level mistakes. Only 30\% of the generated code was executable without modifications. Despite these limitations, the study highlights GPT-4’s ability to provide a useful scaffold for automating parts of the software engineering process, particularly in replicating studies and creating analysis pipelines. GPT-4 can assist with brainstorming assumptions and structuring analysis code, potentially accelerating software development. However, due to its limitations, human oversight remains crucial for correcting errors and ensuring the accuracy and reliability of the generated code.
\citep{sandberg2024evaluating} investigates GPT-4’s ability to automate the development of full-stack web applications, examining both front-end and back-end generation. The study evaluates three web applications of varying complexity and analyzes key metrics such as prompt efficiency, debugging effort, and code quality. The findings highlight that GPT-4 can successfully generate functional applications, offering the advantage of speeding up the development process, especially for simpler projects. However, as application complexity increases, the tool struggles with debugging and handling intricate integrations, requiring significant human intervention. While GPT-4 can effectively generate foundational code, its limitations in dealing with complex logic, debugging, and quality assurance present notable challenges. This study underscores GPT-4’s role as a valuable assistant in automating parts of software development but not as a fully autonomous solution for complex systems, where human expertise remains essential.

\subsection{Existing GenAI Applications for Software Engineering}


\citep{phan2024hyperagent} introduces a novel multi-agent system designed to address a wide variety of software engineering (SE) tasks. HyperAgent consists of specialized agents (Planner, Navigator, Code Editor, and Executor) that mimic typical developer workflows, managing tasks from planning to execution. Through extensive evaluations on diverse benchmarks (SWE-Bench, RepoExec, and Defects4J), the system demonstrated superior performance in tasks such as GitHub issue resolution, repository-level code generation, and fault localization. HyperAgent significantly improves task success rates, outperforming specialized systems, particularly in complex, multi-step SE tasks across different programming languages. In terms of automating software development using generative AI, the key contribution of HyperAgent is its ability to autonomously handle large-scale, complex coding tasks, including bug fixing, feature addition, and program repair. The system’s use of LLMs across various subtasks showcases how generative AI can enhance automation in SE by reducing the need for manual intervention, especially in navigation, code editing, and execution. However, the paper also highlights several challenges, such as ensuring scalability across different programming environments and managing the computational cost of multi-agent coordination. Limitations include the potential for performance bottlenecks when handling larger codebases and debugging complex issues in real-time environments. Future work proposed in the paper involves integrating HyperAgent with version control systems, enhancing its explainability, and further refining the system to handle more specialized SE domains like security and performance optimization. These improvements could extend its applicability and make it a more practical tool for real-world software development.
\citep{xia2024agentless} challenges the need for complex, autonomous software engineering agents by introducing a simplified approach called AGENTLESS. This approach automates software development tasks, such as bug fixing, using a two-phase process: localization and repair. Unlike existing agent-based systems, which involve complex tool integration and autonomous decision-making, AGENTLESS employs a more straightforward hierarchical process for fault localization followed by patch generation. The paper demonstrates that this agentless approach can solve repository-level software issues with greater cost efficiency and competitive performance compared to state-of-the-art agent-based methods, achieving the highest performance on the SWE-bench Lite benchmark (27.33\%) at a fraction of the cost. Contribution to automating software development using generative AI: AGENTLESS leverages large language models (LLMs) to effectively automate bug localization and repair, offering a simpler yet robust alternative to more complex autonomous agents in software development. By removing the need for intricate tool management and decision-making mechanisms, the approach reduces complexity and improves interpretability. Challenges and Limitations: While AGENTLESS outperforms many open-source approaches, the paper identifies limitations such as reduced performance in scenarios where no localization clues are provided and difficulties in solving problems that require advanced reasoning or complex planning. Additionally, it highlights the need for improved re-ranking and selection techniques to maximize its repair capabilities. Future Work: The authors propose extending the AGENTLESS framework to address more complex software engineering tasks and to improve its problem-solving capabilities through better integration with testing frameworks and enhanced patch selection mechanisms. The paper contributes to the discourse on automating software development by emphasizing the potential of simpler, interpretable AI-driven methods over more complex autonomous agents.

\subsection{Code Generation}
\citep{gu2023effectiveness} provides a detailed empirical study on how large language models (LLMs) like ChatGPT, CodeLlama, and PolyCoder perform in domain-specific code generation tasks, focusing on the fields of web and game development. The authors reveal that while LLMs have shown great success in general-purpose code generation, their performance in domain-specific scenarios degrades significantly. This is largely due to the limited proficiency of these models in handling domain-specific libraries and APIs, resulting in issues such as API misuse and missing API calls. To address these challenges, the paper proposes DomCoder, a novel approach that integrates domain knowledge into LLMs through API recommendations and chain-of-thought prompting strategies. DomCoder shows improved performance across various domain-specific benchmarks, demonstrating the importance of enhancing LLMs with domain-specific prompts and fine-tuning for better results. 
Contribution to Automating Software Development using Generative AI: The paper's key contribution lies in showing how generative AI, when fine-tuned with domain-specific knowledge, can improve the automation of software development tasks, particularly in highly specialized domains such as game and web development. The integration of API knowledge into LLMs significantly enhances code generation performance, suggesting practical applications for automating more complex and specialized software engineering tasks. 
Challenges and Limitations: One major limitation is the scarcity of domain-specific data, which impacts the LLMs’ ability to fully learn the intricacies of domain-specific libraries. Additionally, integrating domain knowledge through complex APIs and prompts increases the difficulty of maintaining consistency and correctness in the generated code. 
Future Work: The authors propose future research directions, including improving LLM training with a richer variety of domain-specific datasets and enhancing prompt design mechanisms to further bridge the gap between general-purpose LLMs and domain-specific requirements. They also suggest exploring more sophisticated knowledge integration techniques, such as fine-tuning models with domain-relevant workflows and patterns to improve performance in complex domains.

\citep{zhang2023planning} introduces a novel approach called Planning-Guided Transformer Decoding (PG-TD), which improves the performance of large language models (LLMs) in code generation tasks by integrating a planning algorithm with Transformer models. Unlike traditional sampling and beam search methods, which rely on generating code first and then evaluating it, PG-TD incorporates a lookahead search that allows the model to make informed decisions during the generation process. This approach enables the model to generate higher-quality programs by considering test case results while the code is being generated. The method significantly improves pass rates on public competitive programming benchmarks compared to baseline methods.
Contribution to automating software development using generative AI: The introduction of PG-TD contributes to automating software development by optimizing code generation and reducing the need for extensive post-generation evaluation. It provides a more efficient way of generating programs that pass more test cases and is model-agnostic, meaning it can work with various Transformer-based LLMs without needing additional data or fine-tuning. 
Challenges and limitations: One of the key challenges of this approach is its reliance on available test cases for evaluating the quality of generated programs. The method is also more computationally expensive than simpler Transformer-based approaches due to the iterative tree search mechanism. Additionally, while it shows clear improvements in competitive programming scenarios, its effectiveness in broader software development contexts has not been fully explored. Future work: The authors propose improving the computational efficiency of PG-TD by exploring parallel tree search techniques and incorporating neural value functions to estimate pass rates more effectively. They also suggest extending the framework to support automatically generated test cases, addressing the issue of missing test cases in real-world scenarios. This paper makes a significant contribution to the ongoing development of generative AI in software engineering by enhancing the accuracy and efficiency of LLM-driven code generation.

\subsection{Knowledge Gaps}

\begin{description} 
   \item[Lack of Domain Knowledge] 
   \item[Hallucination] 
\end{description}