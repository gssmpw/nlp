\section{Literature Review}
\label{Literature Review}
In recent years, the rise of LLMs like GPT-3 \citep{brown2020language}, GPT-4 \citep{sun2023gpt}, and the emerging DeepSeek model has introduced new possibilities for revolutionizing software development through autonomous code generation \citep{guo2024deepseek}. LLMs have demonstrated a remarkable ability to understand and generate human language, enabling them to perform a wide range of tasks, including reasoning, code generation, and problem-solving \citep{li2023autonomous, tupayachi2024towards}. By integrating LLMs with RAG techniques, developers can enhance these models' capacity to retrieve and incorporate domain-specific knowledge, improving the accuracy and reliability of generated code. This combination holds the potential to automate significant portions of the software development process, such as requirement definition, bug fixing, feature implementation, and program repair, while reducing the need for manual intervention. As a result, LLMs and RAG are poised to fundamentally transform how software is developed for specific applications, offering a path toward more efficient and scalable solutions for both routine and complex coding tasks \citep{meyer2023llm, baldazzi2023fine}.

\subsection{A Review of LLM-Based Approaches in Software Development}
The applications of LLMs in software engineering and code generation are diverse and rapidly expanding. The following subsections review various aspects of their use in software engineering tasks. This paper specifically focuses on evaluating the capability of LLMs in web application development.

\subsubsection{Automating Software Engineering Tasks}
A few studies have conducted reviews and proposed potential opportunities and challenges for adopting LLMs to facilitate various software engineering tasks. \citet{hou2023large} offers a comprehensive review of how LLMs are applied in software engineering, categorizing them into architectures like encoder-only, encoder-decoder, and decoder-only models. The growing use of decoder-only models (e.g., GPT) is highlighted for automating tasks like code generation and completion, significantly reducing manual coding effort. However, key challenges remain, including handling domain-specific knowledge, improving data set quality, and addressing the complexity of SE tasks that go beyond simple text generation. Notably, the need for better data prepossessing, model fine-tuning for specific SE tasks, and inclusion of more diverse data sets, especially from industrial contexts, are underscored, which are often underrepresented. Data set variety, enhancing LLM robustness for complex SE tasks, and refining evaluation metrics are noted necessary steps for better assessing LLMs in real-world scenarios. LLMs are increasing recognized with growing potential to assist in SE, however, these models are not yet capable of fully replacing human developers in more intricate processes. 

\citet{phan2024hyperagent} introduces HyperAgent, a multi-agent system designed to address various software engineering (SE) tasks using specialized agents like Planner, Navigator, Code Editor, and Executor. Evaluated on benchmarks, such as SWE-Bench and Defects4J, HyperAgent excels in GitHub issue resolution, repository-level code generation, and fault localization, outperform specialized systems in complex, multi-step tasks. Its key contribution lies in automating large-scale coding tasks (e.g., bug fixing, feature addition) using LLMs, reducing the need for manual intervention across planning, code editing, and execution. However, challenges include scalability across diverse programming environments, and high computational cost of multi-agent coordination. Future work is expected to integrate version control systems, improve explainability, and extend the system to specialized SE domains like security and performance optimization, making HyperAgent a more practical tool for real-world development.

\citet{xia2024agentless} introduces AGENTLESS, a simplified approach that automates SE tasks like bug fixing through a two-phase process: localization and repair. Unlike agent-based systems, AGENTLESS employs a hierarchical structure that eliminates complex tool integration, achieving superior cost efficiency and competitive performance, notably achieving 27.33\% on the SWE-bench Lite benchmark. LLMs are demonstrated to effectively automate bug localization and repair without the complexity of autonomous agents. However, this approach faces challenges in scenarios without localization clues and in tasks requiring advanced reasoning. It is acknowledged that future work should expand AGENTLESS to more complex SE tasks, improve patch selection, and integrate testing frameworks, offering a simpler and more interpretable alternative to complex agent systems for automating software development.

\subsubsection{Evaluating LLM for Software Development}
With the rise of generative AI applications, several studies have conducted comprehensive reviews and evaluations on the feasibility of various LLMs for code generation and more complex software development tasks.
  
\citet{liang2024can} explores the potential of GPT-4 to replicate empirical software engineering research by generating code for analysis pipelines. Through a user study involving 14 participants and seven empirical SE papers, the study found that while GPT-4 surfaced correct assumptions and generated high-level analysis plans, it often lacked domain-specific expertise. The generated code frequently contained errors, such as incorrect data sources and missing steps, with only 30\% being executable without modification. Despite these limitations, GPT-4 shows promise in automating parts of the SE process, particularly in replicating studies and structuring analysis code, though human oversight remains critical for correcting errors and ensuring accuracy. \citet{sandberg2024evaluating} investigates GPT-4’s ability to automate full-stack web application development, assessing key metrics like prompt efficiency, debugging effort, and code quality. The study reveals that GPT-4 excels at generating functional applications for simpler projects, significantly speeding up development. However, as complexity increases, GPT-4 struggles with debugging, integration, and complex logic, requiring substantial human intervention. While the tool effectively generates foundational code, it falls short in handling intricate systems, underscoring the need for human expertise in ensuring quality and reliability in complex software development.
\citet{gu2023effectiveness} presents an empirical study on how LLMs like ChatGPT, CodeLlama, and PolyCoder perform in domain-specific code generation, particularly in web and game development. The study reveals that LLMs struggle with domain-specific libraries and APIs, leading to issues like API misuse. To address this, the authors propose DomCoder, a model that integrates API recommendations and chain-of-thought prompting, significantly improving performance in domain-specific tasks. The key contribution is demonstrating how fine-tuning LLMs with domain-specific knowledge can enhance automation in specialized software development tasks. However, challenges include the scarcity of domain-specific data and the difficulty of maintaining consistency in API integration. Future work involves enriching training data sets and refining prompt design to bridge the gap between general-purpose LLMs and domain-specific requirements.
\citet{fan2023large} provides an in-depth review of LLMs like GPT, BERT, and Codex, focusing on their integration into software engineering tasks such as code generation, program repair, and automated testing. This work showcases LLMs' ability to revolutionize software development by automating tasks like code completion and bug detection, thereby enhancing developer productivity and efficiency. Several challenges are identified, including LLMs' tendency to hallucinate (generate incorrect but plausible outputs), struggles with domain-specific tasks, and the need for fine-tuning models for specialized contexts. It also highlights the issue of non-deterministic outputs, complicating scientific evaluation, and the oracle problem in verifying the correctness of generated solutions. Improving prompt engineering, hybridizing LLMs with traditional SE methods, and exploring their application to less-studied areas like requirements engineering and software design are suggested as areas of advancements. Integrating automated testing with LLM outputs is proposed as a means to mitigate hallucination and enhance reliability. The paper’s novelty lies in its exploration of LLM integration into SE tasks, while acknowledging the persistent challenges in ensuring the reliability and correctness of these models.

\subsubsection{Code Generation}
Several ongoing research efforts have explored LLM-based approaches to facilitate code generation. \citet{guo2024deepseek} introduced DeepSeek-Coder, a significant advancement in code-focused LLMs that addresses the limitations of closed-source models by offering a high-performance, open-source alternative. DeepSeek-Coder comprises models ranging from 1.3B to 33B parameters, trained on 2 trillion tokens spanning 87 programming languages. Unlike traditional code models, it leverages repository-level data construction to enhance cross-file understanding, improving its ability to generate coherent and structured code. Additionally, it incorporates a Fill-in-the-Middle (FIM) training approach and supports a 16K context window, enabling it to handle complex coding tasks more effectively. Benchmark evaluations demonstrate that DeepSeek-Coder outperforms leading open-source models, such as CodeLlama and StarCoder, and in some cases even exceeds OpenAI’s GPT-3.5 Turbo in code generation tasks. By offering a permissive license, DeepSeek-Coder empowers researchers and developers to explore its potential in autonomous code generation, software development, and problem-solving, further narrowing the gap between open-source and proprietary code models. \citet{zhang2023planning} introduces Planning-Guided Transformer Decoding (PG-TD), a novel approach that integrates a planning algorithm with Transformer models to improve code generation by considering test case results during the generation process. This method outperforms traditional sampling and beam search methods, increasing pass rates on competitive programming benchmarks. The key contribution is the optimization of LLM-driven code generation by reducing post-generation evaluation, making the approach model-agnostic. However, PG-TD is computationally expensive and relies on available test cases, limiting its application in broader software development contexts. Future work aims to improve computational efficiency through parallel tree search techniques and extend the method to automatically generate test cases, enhancing its applicability to real-world software development.

In the GIS sector, \citet{hou2024can} examines the limitations of general-purpose LLMs in geospatial code generation and proposes strategies for improvement. Due to a lack of domain-specific training, LLMs generate inaccurate and inefficient geospatial code, leading to issues like coding hallucinations, deprecated functions, incompatible dependencies, and incorrect parameters. They also struggle with multi-modal geospatial data (e.g., raster, vector, elevation models) and complex spatial operations, failing to recognize built-in GIS functions in platforms like Google Earth Engine, OpenLayers, and Leaflet. Even advanced prompting techniques offer only marginal improvements, exposing fundamental training deficiencies. To address this issue, the study introduces GeoCode-Eval (GCE), an evaluation framework assessing LLMs' cognition, comprehension, and innovation in geospatial programming. Systematic testing reveals significant shortcomings across commercial and open-source LLMs. The authors propose fine-tuning with domain-specific datasets, demonstrated by GEECode-GPT, a Code LLaMA-7B model trained on Google Earth Engine scripts, which significantly improves execution accuracy. Additionally, they introduce GeoCode-Bench, a benchmark data set evaluating LLMs through multiple-choice, fill-in-the-blank, and coding tasks. The study also advocates instruction-tuning using NASA Earth Data, USGS, and OpenStreetMap to further enhance geospatial reasoning. In conclusion, the study highlights LLMs' current limitations in geospatial programming and presents fine-tuning, benchmarking, and instruction-tuning as key solutions to bridge the gap, improving their reliability and usability for geospatial applications.

\subsubsection{Front-end App Development from UI Prototypes}
\citet{xiao2024prototype2code} introduces Prototype2Code, an end-to-end framework for automating front-end code generation from UI design prototypes. Traditional UI-to-code methods often produce fragmented elements and unstructured code, leading to poor maintainability. To address this, Prototype2Code integrates design linting, graph-based UI structuring, and LLMs to enhance code generation. The framework first detects and corrects UI inconsistencies through linting, then constructs a hierarchical layout tree for structured component relationships. A Graph Neural Network (GNN)-based classifier refines UI elements before LLMs generate modular HTML and CSS code. Comparisons with tools like CodeFun and GPT-4V-based Screenshot-to-Code demonstrate superior visual fidelity, readability, and maintainability, validated through SSIM, PSNR, and MSE metrics. A user study confirms Prototype2Code reduces manual modifications while improving usability and production readiness. By incorporating design intelligence and AI-driven automation, it streamlines front-end development and minimizes human intervention. Future work aims to support dynamic components, interactive behaviors, and cross-platform adaptability.

\citet{manuardi2024images} explores AI-driven front-end automation by converting UI mockups into structured code. While tools like GitHub Copilot enhance text-based programming, they struggle with UI-driven development, where visual design is central. The study proposes a multi-modal AI system combining computer vision and LLMs to bridge this gap. It processes UI images through edge detection, contour analysis, and OCR, generating a standardized intermediate representation. A multi-modal LLM then translates this into front-end code, tailored for frameworks like Angular and Bootstrap. Implemented as a web-based tool at Blue Reply, the system improves development efficiency by reducing manual coding while ensuring maintainability. By integrating computer vision and AI-powered code generation, this approach advances automated, intuitive front-end development.

\subsection{Limitation and Knowledge Gaps}
Despite significant advancements in LLM-powered code generation for software development, several limitations and knowledge gaps persist, particularly in scientific applications and GIS-based web applications. These challenges hinder the seamless automation of both front-end and back-end development, underscoring the need for further research and innovation. The key challenges are outlined below, labeled C1 to C4 for reference in later discussions.

\begin{description} 
   \item[C1. Limited Graphical and Visual Prompting Techniques:] Current LLMs primarily rely on text-based prompting, which limits their ability to interpret graphical user interface (GUI) designs and wireframe sketches effectively. Although visual prompting techniques, such as image-to-code models, have shown potential, they often struggle with complex UI layouts, interactive components, and contextual relationships between elements. The lack of robust graph-based UI understanding restricts the ability of LLMs to accurately generate structured, modular front-end code from visual design prototypes.

    \item[C2. Lack of Software Engineering Practices in AI-generated Codes:] 
    LLM-generated front-end code often consists of plain HTML and JavaScript, lacking integration with software engineering design principles and architectural patterns such as MVC and MVVM, which are widely used in the IT industry to enhance code readability and maintainability \citep{xiao2024prototype2code}. While AI-assisted programming tools can generate functional snippets, they frequently neglect design patterns, coding conventions, and software architecture best practices, resulting in code that is challenging to maintain in production \citep{ghoshdesign, nguyen2023generative}. Additionally, current LLM-driven development workflows rarely incorporate continuous integration/continuous deployment (CI/CD) pipelines, software testing frameworks, or version control systems, limiting their practical usability in large-scale software development \citep{corona2025question, mendoza2024development}.
   
   \item[C3. Lack of Domain Knowledge for Developing GIS and Scientific Applications] 
   General-purpose LLMs lack deep expertise in GIS and scientific computing, resulting in suboptimal performance when generating domain-specific software \citep{zhang2024bb, mansourian2024chatgeoai}. Developing GIS-based web applications requires expertise in geospatial data processing, web-based mapping engines, web map server applications, and data visualization libraries, along with interoperability with standards such as GeoJSON, WMS, WFS, and 3D point clouds \citep{xu2024semi, xu2019web, berres2021multiscale}. These interdisciplinary aspects bridge GIS, data science, and software engineering, creating challenges for general-purpose LLMs, which often struggle due to insufficient training data and lack of domain-specific guidelines \citep{hou2024can}. While recent studies have explored and prototyped fine-tuned LLMs for generating geospatial code to support spatial analysis and GIS problem-solving \citep{hadid2024geoscience, hou2024geocode, akinboyewa2024gis}, they primarily focus on computational GIS tasks. However, generating web-based visual dashboards and CyberGIS software using web programming languages and frameworks remains largely unexplored, leaving a critical gap in this domain.
 
   \item[C4. Package Management Challenges:] AI-generated code often introduces dependency and package management issues, particularly when working with scientific computing, GIS, and front-end frameworks. LLMs frequently suggest outdated or incompatible packages, mismatched versions, or deprecated functions without considering package conflicts and runtime environments \citep{hou2024can}. In web-based GIS applications, maintaining compatibility between Python, JavaScript, or C++ libraries (e.g., Django, Flask, OpenLayers, Leaflet, Mapbox, PyProj) is crucial, yet current LLM-driven code generation fails to account for such dependencies effectively \citep{mahmoudi2023development}. This results in increased debugging time, compatibility issues, and broken software deployments.
\end{description}

Given these knowledge gaps, there is a need for a robust framework that enables a reliable code generation pipeline for automating the development of front-end GIS web applications in a one-stop-shop manner. This framework should take user-defined GUI sketches, created in common, non-technical software such as PowerPoint, as its sole input, streamlining the development process for users without programming expertise. 

 