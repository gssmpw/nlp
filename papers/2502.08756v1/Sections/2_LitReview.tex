\section{Literature Review}
\label{Literature Review}
LLMs like GPT-3 \citep{brown2020language}, GPT-4 \citep{sun2023gpt}, and DeepSeek \citep{guo2024deepseek} have revolutionized software development through autonomous code generation. Their ability to understand and generate human language enables tasks like reasoning, code generation, and problem-solving \citep{li2023autonomous, tupayachi2024towards}. Integrating LLMs with RAG techniques enhances domain-specific knowledge retrieval, improving code accuracy and reliability. This synergy automates key development processes, including requirement definition, bug fixing, and program repair, reducing manual effort. Ultimately, LLMs and RAG offer scalable solutions for automating repetitive and complex coding tasks \citep{meyer2023llm, baldazzi2023fine}.

\subsection{A Review of LLM-Based Approaches in Software Development}
LLMs are increasingly applied in diverse software engineering tasks, particularly in code generation. This paper focuses on evaluating their capabilities in web application development. The following subsections review their applications in software engineering.

\subsubsection{Automating Software Engineering Tasks}
Several studies have reviewed the opportunities and challenges of using LLMs in software engineering. \citet{hou2023large} provides a comprehensive analysis, categorizing LLMs into encoder-only, encoder-decoder, and decoder-only architectures. The increasing use of decoder-only models (e.g., GPT) for automating code generation and completion has significantly reduced manual effort. However, challenges persist, including handling domain-specific knowledge, improving dataset quality, and addressing the complexity of software engineering tasks beyond simple text generation. Key areas for improvement include better data preprocessing, fine-tuning for specific SE tasks, and incorporating diverse datasets, particularly from industrial contexts. Enhancing LLM robustness and refining evaluation metrics remain critical for real-world applications. While LLMs show promise in assisting SE, they are not yet capable of fully replacing human developers in complex processes.
\citet{phan2024hyperagent} presents HyperAgent, a multi-agent system designed for software engineering (SE) tasks using specialized agents like Planner, Navigator, Code Editor, and Executor. Evaluated on SWE-Bench and Defects4J, HyperAgent excels in GitHub issue resolution, repository-level code generation, and fault localization, outperforming specialized systems in complex multi-step tasks. It automates large-scale coding tasks (e.g., bug fixing, feature addition) using LLMs, reducing manual intervention. However, challenges include scalability across diverse environments and high computational costs. Future work aims to integrate version control, enhance explainability, and extend to specialized SE domains like security and performance optimization. \citet{xia2024agentless} introduces AGENTLESS, a streamlined approach automating SE tasks through a two-phase process: localization and repair. Unlike agent-based systems, AGENTLESS employs a hierarchical structure, reducing integration complexity while achieving 27.33\% on SWE-bench Lite. It automates bug localization and repair efficiently but struggles with cases lacking localization clues and complex reasoning tasks. Future improvements focus on expanding to more SE tasks, refining patch selection, and integrating testing frameworks, offering a simpler, cost-efficient alternative to multi-agent SE automation.

\subsubsection{Evaluating LLM for Software Development}
With the rise of generative AI, several studies have reviewed LLMs' feasibility in code generation and complex software development tasks.
\citet{liang2024can} examines GPT-4’s ability to replicate empirical SE research by generating analysis pipelines. While GPT-4 structured high-level plans well, it lacked domain-specific expertise, with only 30\% of its generated code executing without modification. Human oversight remains essential for ensuring accuracy. \citet{sandberg2024evaluating} evaluates GPT-4 in full-stack web development, highlighting its efficiency in generating functional applications for simple projects. However, as complexity increases, GPT-4 struggles with debugging and integration, requiring significant human intervention. \citet{gu2023effectiveness} assesses ChatGPT, CodeLlama, and PolyCoder in domain-specific coding, revealing struggles with API misuse. To address this, DomCoder integrates API recommendations and chain-of-thought prompting, improving domain-specific automation. However, challenges remain in sourcing domain-specific data and ensuring API consistency. 
\citet{fan2023large} reviews LLMs like GPT, BERT, and Codex in software engineering tasks, identifying strengths in code completion, bug detection, and automation. Challenges include hallucinations, non-deterministic outputs, and verification issues. Future directions involve improving prompt engineering, integrating LLMs with traditional SE methods, and enhancing automated testing to mitigate hallucinations and improve reliability.

\subsubsection{Code Generation}
Ongoing research continues to refine LLM-based code generation.
\citet{guo2024deepseek} introduces DeepSeek-Coder, an open-source alternative to proprietary models, with sizes ranging from 1.3B to 33B parameters trained on 2 trillion tokens across 87 programming languages. It enhances cross-file understanding using repository-level data construction and a Fill-in-the-Middle (FIM) approach, supporting a 16K context window for handling complex tasks. Benchmarks show it outperforms CodeLlama and StarCoder, even surpassing GPT-3.5 Turbo in some cases. With a permissive license, DeepSeek-Coder advances autonomous code generation and software development.
\citet{zhang2023planning} proposes Planning-Guided Transformer Decoding (PG-TD), which integrates a planning algorithm with Transformers to improve code generation by leveraging test case results. PG-TD surpasses traditional sampling and beam search, boosting pass rates on competitive programming benchmarks. However, it is computationally intensive and depends on existing test cases, limiting broader applications. Future work seeks to enhance efficiency through parallel tree search and automated test case generation for real-world software development.

In the GIS sector, \citet{hou2024can} highlights the limitations of general-purpose LLMs in geospatial code generation, citing issues such as coding hallucinations, deprecated functions, incompatible dependencies, and incorrect parameters. LLMs struggle with multi-modal geospatial data (e.g., raster, vector, elevation models) and fail to recognize built-in GIS functions in platforms like Google Earth Engine, OpenLayers, and Leaflet. Even advanced prompting provides only marginal improvements, exposing fundamental training gaps. To address these challenges, the study introduces GeoCode-Eval (GCE), an evaluation framework assessing LLMs' cognition, comprehension, and innovation in geospatial programming. Systematic testing reveals significant shortcomings across commercial and open-source models. The authors propose fine-tuning with domain-specific datasets, demonstrated by GEECode-GPT, a Code LLaMA-7B model trained on Google Earth Engine scripts, which significantly improves execution accuracy. Additionally, they introduce GeoCode-Bench, a benchmark dataset assessing LLM performance through multiple-choice, fill-in-the-blank, and coding tasks. Instruction-tuning with datasets from NASA Earth Data, USGS, and OpenStreetMap is also recommended to enhance geospatial reasoning. The study underscores LLMs' current limitations in geospatial programming and presents fine-tuning, benchmarking, and instruction-tuning as essential strategies to improve their reliability and usability for GIS applications.

\subsubsection{Front-end App Development from UI Prototypes}
\citet{xiao2024prototype2code} introduces Prototype2Code, an end-to-end framework for automating front-end code generation from UI design prototypes. Traditional UI-to-code methods often produce fragmented, unstructured code, impacting maintainability. Prototype2Code addresses this by integrating design linting, graph-based UI structuring, and LLMs to enhance code generation. It first detects and corrects UI inconsistencies through linting, constructs a hierarchical layout tree for structured components, and refines UI elements using a Graph Neural Network (GNN)-based classifier before generating modular HTML and CSS with LLMs. Benchmarks against CodeFun and GPT-4V-based Screenshot-to-Code show superior visual fidelity, readability, and maintainability, validated by SSIM, PSNR, and MSE metrics. A user study confirms reduced manual modifications and improved usability. Future work aims to support dynamic components, interactivity, and cross-platform adaptability. \citet{manuardi2024images} explores AI-driven front-end automation by converting UI mockups into structured code. Unlike text-based coding tools like GitHub Copilot, UI-driven development requires visual processing. The study proposes a multi-modal AI system that integrates computer vision and LLMs, using edge detection, contour analysis, and OCR to generate an intermediate representation, which a multi-modal LLM translates into front-end code for Angular and Bootstrap. Implemented at Blue Reply as a web-based tool, the system improves development efficiency by reducing manual coding while ensuring maintainability, advancing automated and intuitive front-end development.

\subsection{Limitation and Knowledge Gaps}
Despite advancements in LLM-powered code generation, challenges remain, particularly in scientific and GIS-based web applications. These limitations hinder seamless automation of front-end and back-end development, necessitating further research. The key challenges are outlined below as C1–C4.

\begin{description} \item[C1. Limited Graphical and Visual Prompting:] LLMs rely on text-based prompting, limiting their ability to interpret GUI designs and wireframe sketches. While image-to-code models show promise, they struggle with complex layouts, interactive elements, and contextual relationships. The lack of robust graph-based UI understanding restricts structured, modular front-end code generation.

\item[C2. Absence of Software Engineering Best Practices:] AI-generated code often lacks integration with industry design patterns (e.g., MVC, MVVM) \citep{xiao2024prototype2code}, leading to poor maintainability \citep{ghoshdesign, nguyen2023generative}. LLM-driven workflows rarely incorporate CI/CD pipelines, software testing, or version control, limiting their practical usability in large-scale development \citep{corona2025question, mendoza2024development}.

\item[C3. Lack of Domain Knowledge in GIS and Scientific Applications:] General-purpose LLMs struggle with GIS and scientific computing due to insufficient training on geospatial standards (GeoJSON, WMS, WFS), web mapping engines, and 3D data visualization \citep{zhang2024bb, mansourian2024chatgeoai, hou2024can}. While some fine-tuned models improve geospatial analysis \citep{hadid2024geoscience, hou2024geocode, akinboyewa2024gis}, web-based GIS dashboard generation remains largely unexplored.

\item[C4. Package Management Issues:] AI-generated code frequently suffers from dependency conflicts, outdated libraries, and compatibility issues, particularly in GIS and scientific computing \citep{hou2024can, mahmoudi2023development}. Web-based GIS applications require compatibility across Python, JavaScript, and C++ libraries (e.g., Django, Flask, OpenLayers, Leaflet), which LLMs often fail to handle effectively.

\end{description}

To bridge these gaps, a robust framework is needed to automate GIS web application development. This framework should allow users to input GUI sketches from non-technical tools (e.g., PowerPoint) for seamless, code-free development.

 