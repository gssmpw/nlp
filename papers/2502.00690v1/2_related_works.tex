\section{Related Works} \label{sec:related_works}

\subsection{Desk Reject Mechanism}


A wide range of desk-rejection mechanisms have been developed to reduce the human effort involved in the peer-review process~\cite{as21}. One of the most widely adopted desk-rejection rules is rejecting papers that violate anonymity requirements~\cite{jawd02, t18}. This rule is crucial for maintaining unbiased evaluations of researchers from diverse institutions and career levels while preventing conflicts of interest. Another common mechanism addresses duplicate and dual submissions~\cite{s03, l13}, alleviating the duplication of reviewer efforts across multiple venues and upholding ethical publication standards. Additionally, plagiarism~\cite{kc23, er23} is a major concern in desk rejections at AI conferences, as it undermines the integrity of the academic community, violates intellectual property rights, and compromises the originality and credibility of research. In response to the growing number of submissions to AI conferences, new types of desk-rejection rules have recently emerged~\cite{lnz+24}. For example, IJCAI 2020 and NeurIPS 2020 implemented a fast desk-rejection mechanism, allowing area chairs to reject papers based on a quick review of the abstract and main content to manage the review workload. However, this approach introduced noise and sometimes resulted in the rejection of generally good papers, leading to its reduced prevalence compared to more systematic mechanisms like enforcing submission limits, which is the main focus of this paper. To the best of our knowledge, limited literature has explored these emerging desk-rejection techniques, and our work is among the first to formally study the desk-rejection mechanism based on maximum submission limits.

\subsection{The Competitive Race in AI Publication}


Due to the rapid increase in submissions to AI conferences in recent years~\cite{stanford_ai_index}, concerns about the intense competition in these conferences are growing. As Bengio Yoshua noted~\cite{b20blog}: ``It is more competitive, everything is happening fast and putting a lot of pressure on everyone. The field has grown exponentially in size. Students are more protective of their ideas and in a hurry to put them out, by fear that someone else would be working on the same thing elsewhere, and in general, a PhD ends up with at least 50\% more papers than what I gather it was 20 or 30 years ago.'' Consequently, paper acceptance has become increasingly critical in AI job applications~\cite{a22, bbm+24}, as having more papers is now the norm. Therefore, it is crucial to establish fair and practical guidelines for desk rejections~\cite{takb18}, ensuring that every group of authors is treated equitably in AI conferences. 



\subsection{Fairness System Design}

Fairness~\cite{f12, mms+21} is a key principle of social justice, reflecting the absence of bias toward individuals or groups based on inherent characteristics. Due to its profound societal impact, fairness has become an essential consideration in the design of algorithms across various computer systems that interact with human factors. In recommender systems, fairness can manifest in various forms, such as item fairness~\cite{zfh+21, glg+21}, which ensures that items from different categories or with varying levels of prior exposure are recommended equitably, and user fairness~\cite{lcf+21, lcx+21}, which guarantees that all users, regardless of their backgrounds or preferences, have equal opportunities to access relevant content. These fairness measures help balance opportunities for both users and retailers, fostering equity in the recommendation process. In candidate selection systems~\cite{g93, whz20}, fairness ensures that all candidates are evaluated solely on merit, independent of factors such as race, gender, or socio-economic background, promoting equality and ensuring that the selection processes are inclusive. In information access systems~\cite{edb+22}, including job search~\cite{wmm+22} and music discovery~\cite{mrp+21}, fairness guarantees that all individuals can access the information they need without discrimination, ensuring equal opportunities for users to make informed decisions. Similarly, in dialog systems~\cite{gya22,grb+24}, fairness ensures that language models avoid generating biased text or making inappropriate word-context associations related to social groups, supporting equitable and respectful interactions. Moreover, recent research has investigated group fairness in peer review processes for AI conferences, highlighting the importance of equitable evaluation for submissions~\cite{ams23}.
Despite the widespread focus on fairness in algorithmic design, the fairness of desk-rejection mechanisms remains an open question and serves as the primary motivation for this paper.


