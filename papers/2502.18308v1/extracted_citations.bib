@article{Bender2021,
author = {Bender, Emily M. and Gebru, Timnit and McMillan Major, Angelina and Shmitchell, Shmargaret},
    title = {On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? },
    year = {2021},
    isbn = {9781450383097},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3442188.3445922},
    doi = {10.1145/3442188.3445922},
    booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
    pages = {610\-623},
    numpages = {14},
    location = {Virtual Event, Canada}
}

@inproceedings{bang-etal-2023-multitask,
    title = "A Multitask, Multilingual, Multimodal Evaluation of {C}hat{GPT} on Reasoning, Hallucination, and Interactivity",
    author = "Bang, Yejin  and
      Cahyawijaya, Samuel  and
      Lee, Nayeon  and
      Dai, Wenliang  and
      Su, Dan  and
      Wilie, Bryan  and
      Lovenia, Holy  and
      Ji, Ziwei  and
      Yu, Tiezheng  and
      Chung, Willy  and
      Do, Quyet V.  and
      Xu, Yan  and
      Fung, Pascale",
    editor = "Park, Jong C.  and
      Arase, Yuki  and
      Hu, Baotian  and
      Lu, Wei  and
      Wijaya, Derry  and
      Purwarianti, Ayu  and
      Krisnadhi, Adila Alfa",
    booktitle = "Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = nov,
    year = "2023",
    address = "Nusa Dua, Bali",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.ijcnlp-main.45",
    doi = "10.18653/v1/2023.ijcnlp-main.45",
    pages = "675--718",
}

@inproceedings{bernard2024identifying,
  title={Identifying Breakdowns in Conversational Recommender Systems using User Simulation},
  author={Bernard, Nolwenn and Balog, Krisztian},
  booktitle={Proceedings of the 6th ACM Conference on Conversational User Interfaces},
  pages={1--10},
  year={2024}
}

@article{burnell2023revealing,
  title={Revealing the structure of language model capabilities},
  author={Burnell, Ryan and Hao, Han and Conway, Andrew RA and Orallo, Jose Hernandez},
  journal={arXiv preprint arXiv:2306.10062},
  year={2023}
}

@article{chen2024sifo,
  title={The SIFo Benchmark: Investigating the Sequential Instruction Following Ability of Large Language Models},
  author={Chen, Xinyi and Liao, Baohao and Qi, Jirui and Eustratiadis, Panagiotis and Monz, Christof and Bisazza, Arianna and de Rijke, Maarten},
  journal={arXiv preprint arXiv:2406.19999},
  year={2024}
}

@article{clark2018think,
  title={Think you have solved question answering? try arc, the ai2 reasoning challenge},
  author={Clark, Peter and Cowhey, Isaac and Etzioni, Oren and Khot, Tushar and Sabharwal, Ashish and Schoenick, Carissa and Tafjord, Oyvind},
  journal={arXiv preprint arXiv:1803.05457},
  year={2018}
}

@article{cobbe2021training,
  title={Training verifiers to solve math word problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and others},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}

@article{fan2023nphardeval,
  title={Nphardeval: Dynamic benchmark on reasoning ability of large language models via complexity classes},
  author={Fan, Lizhou and Hua, Wenyue and Li, Lingyao and Ling, Haoyang and Zhang, Yongfeng and Hemphill, Libby},
  journal={arXiv preprint arXiv:2312.14890},
  year={2023}
}

@article{fang2024multi,
  title={A multi-agent conversational recommender system},
  author={Fang, Jiabao and Gao, Shen and Ren, Pengjie and Chen, Xiuying and Verberne, Suzan and Ren, Zhaochun},
  journal={arXiv preprint arXiv:2402.01135},
  year={2024}
}

@article{gusev2024pingpong,
  title={PingPong: A Benchmark for Role-Playing Language Models with User Emulation and Multi-Model Evaluation},
  author={Gusev, Ilya},
  journal={arXiv preprint arXiv:2409.06820},
  year={2024}
}

@article{he2024complex,
  title={From Complex to Simple: Enhancing Multi-Constraint Complex Instruction Following Ability of Large Language Models},
  author={He, Qianyu and Zeng, Jie and He, Qianxi and Liang, Jiaqing and Xiao, Yanghua},
  journal={arXiv preprint arXiv:2404.15846},
  year={2024}
}

@inproceedings{hendrycksmeasuring,
  title={Measuring Massive Multitask Language Understanding},
  author={Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
  booktitle={International Conference on Learning Representations}
}

@inproceedings{instruction-follow-summ,
    title = "Towards Better Evaluation of Instruction-Following: A Case-Study in Summarization",
    author = "Skopek, Ondrej  and
      Aralikatte, Rahul  and
      Gooding, Sian  and
      Carbune, Victor",
    editor = "Jiang, Jing  and
      Reitter, David  and
      Deng, Shumin",
    booktitle = "Proceedings of the 27th Conference on Computational Natural Language Learning (CoNLL)",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.conll-1.16",
    doi = "10.18653/v1/2023.conll-1.16",
    pages = "221--237",
    abstract = "Despite recent advances, evaluating how well large language models (LLMs) follow user instructions remains an open problem. While evaluation methods of language models have seen a rise in prompt-based approaches, limited work on the correctness of these methods has been conducted. In this work, we perform a meta-evaluation of a variety of metrics to quantify how accurately they measure the instruction-following abilities of LLMs. Our investigation is performed on grounded query-based summarization by collecting a new short-form, real-world dataset riSum, containing 300 document-instruction pairs with 3 answers each. All 900 answers are rated by 3 human annotators. Using riSum, we analyze the agreement between evaluation methods and human judgment. Finally, we propose new LLM-based reference-free evaluation methods that improve upon established baselines and perform on par with costly reference-based metrics that require high-quality summaries.",
}

@misc{instruction-follow-verbalizer,
      title={Instruction-following Evaluation through Verbalizer Manipulation}, 
      author={Shiyang Li and Jun Yan and Hai Wang and Zheng Tang and Xiang Ren and Vijay Srinivasan and Hongxia Jin},
      year={2023},
      eprint={2307.10558},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{kocon2023chatgpt,
  title={ChatGPT: Jack of all trades, master of none},
  author={Koco{\'n}, Jan and Cichecki, Igor and Kaszyca, Oliwier and Kochanek, Mateusz and Szydlo, Dominika and Baran, Joanna and Bielaniewicz, Julita and Gruza, Marcin and Janz, Arkadiusz and Kanclerz, Kamil and others},
  journal={Information Fusion},
  volume={99},
  pages={101861},
  year={2023},
  publisher={Elsevier}
}

@inproceedings{park2024llm,
  title={Do LLM Agents Have Regret? A Case Study in Online Learning and Games},
  author={Park, Chanwoo and Liu, Xiangyu and Ozdaglar, Asuman E and Zhang, Kaiqing},
  booktitle={ICLR 2024 Workshop on Large Language Model (LLM) Agents},
  year={2024}
}

@inproceedings{suzgun2023challenging,
  title={Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them},
  author={Suzgun, Mirac and Scales, Nathan and Sch{\"a}rli, Nathanael and Gehrmann, Sebastian and Tay, Yi and Chung, Hyung Won and Chowdhery, Aakanksha and Le, Quoc and Chi, Ed and Zhou, Denny and others},
  booktitle={Findings of the Association for Computational Linguistics: ACL 2023},
  pages={13003--13051},
  year={2023}
}

@inproceedings{wang2023rethinking,
  title={Rethinking the Evaluation for Conversational Recommendation in the Era of Large Language Models},
  author={Wang, Xiaolei and Tang, Xinyu and Zhao, Xin and Wang, Jingyuan and Wen, Ji-Rong},
  booktitle={The 2023 Conference on Empirical Methods in Natural Language Processing}
}

@article{yan2024refutebench,
  title={RefuteBench: Evaluating Refuting Instruction-Following for Large Language Models},
  author={Yan, Jianhao and Luo, Yun and Zhang, Yue},
  journal={arXiv preprint arXiv:2402.13463},
  year={2024}
}

@article{zhou2023instruction,
  title={Instruction-following evaluation for large language models},
  author={Zhou, Jeffrey and Lu, Tianjian and Mishra, Swaroop and Brahma, Siddhartha and Basu, Sujoy and Luan, Yi and Zhou, Denny and Hou, Le},
  journal={arXiv preprint arXiv:2311.07911},
  year={2023}
}

@article{zhu2023dyval,
  title={Dyval: Graph-informed dynamic evaluation of large language models},
  author={Zhu, Kaijie and Chen, Jiaao and Wang, Jindong and Gong, Neil Zhenqiang and Yang, Diyi and Xie, Xing},
  journal={arXiv preprint arXiv:2309.17167},
  year={2023}
}

@article{zhu2024dyval,
  title={Dyval 2: Dynamic evaluation of large language models by meta probing agents},
  author={Zhu, Kaijie and Wang, Jindong and Zhao, Qinlin and Xu, Ruochen and Xie, Xing},
  journal={arXiv preprint arXiv:2402.14865},
  year={2024}
}

