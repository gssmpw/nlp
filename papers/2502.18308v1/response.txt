\section{Related Work}
\paragraph*{Instruction Following.}
Our work falls in the broader realm of evaluating LLMs' instruction-following capacities.
IFEval**Ammar et al., "Evaluating LLMs' Instruction-Following Abilities"**
**Huang et al., "Meta-Evaluation of Instruction-Following for Text Summarization"**
**Guo et al., "Checking Instruction-Following by Verbalizer and Models' Output"**
Other efforts are devoted to evaluating the instruction-following for complex instructions**Zhu et al., "Evaluating LLMs' Instruction-Following on Complex Instructions"**
and sequential instructions**Kim et al., "Instruction-Following Evaluation on Sequential Instructions"**
Apart from this work, RefuteBench**Guo et al., "RefuteBench: Evaluating LLMs' Instruction-Following with Refutation"**
first evaluates the instruction-following with refutation, investigating whether LLMs can follow  user's feedback in modifying its output or be stubborn to original outputs. 
We extend this work from several important aspects: (1) integration of another important scenario -- \emph{Transient refutation}; (2) use of agentic dynamic evaluation protocol to replace template-based protocols, which both requires careful efforts in adapting each task with appropriate templates and facilitates \textit{context-aware} evaluation with \textit{versatile} feedback types. 
% Their findings demonstrate that LLMs like \texttt{gpt-3.5-turbo} and \texttt{Mixtral-Instruct} can be 
% the model's instruction-following capability can extend through dialogue history. 
% \elliott{add new literature}

\paragraph*{Dynamic Evaluation.}
In contrast to static evaluation using pre-constructed data, 
dynamic evaluation is proposed to address the “false promise” to data contamination **Li et al., "Addressing False Promises in Dynamic Evaluation"**
and over-fitting benchmarks **Zhu et al., "Over-Fitting Benchmarks: A Challenge for LLMs"**
% Dynamic evaluation proposes to change the evaluation periodically to avoid this issue.  
% rephrasing the evaluation data and maintaining the concept,  is proposed to address this issue. 
**Kim et al., "Dynamic Evaluation of MMLU Benchmarks with Meta Probing Agents"**
propose to change MMLU ____, BBH ____, GSM8k ____ and ARC-C ____ benchmarks to dynamic evaluation by applying Meta Probing Agents.
**Li et al., "Dynamic Evaluation Sample Generation for Multifaceted Ability Analysis"**
use three basic cognitive abilities -- language understanding, problem-solving, and domain knowledge to conduct dynamic evaluation sample generation and support multifaceted ability analysis.
**Guo et al., "NPHardEval: Generating NP-Hard Math Problems for Dynamic Evaluation"**
~(NPHardEval) generates new NP-hard math problems and updates the evaluation set monthly.
Using a dynamic evaluation protocol, we evaluate LLMs' refutation instruction following capacities with context-aware, and human-like instructions, comprehensive refutation types, and low human annotation cost.

% Thanks to the nature of dynamic evaluation protocol, our work also avoids the problem of data contamination. Different from these work, 

\paragraph*{LLM Agents in Simulation.}
Increasing research efforts use LLMs as agents to simulate human behavior. For example, in dialogue recommendation systems, some studies employ LLMs to simulate users **Kim et al., "Dialogue Recommendation Systems with LLM Agents"**
These LLM agents provide feedback to evaluate the performance of recommendation systems during the conversation, which offers advantages such as simplicity, cost-effectiveness, and time efficiency. Additionally, some research uses LLMs to study model interactivity **Li et al., "Model Interactivity with LLM Agents"**
and to better understand the limits of LLM agents in interactive environments. Studies also propose examining their interactions in benchmark decision-making scenarios **Zhu et al., "Benchmark Decision-Making Scenarios with LLM Agents"**.
In role-play settings, work leverages LLMs to emulate users in dynamic, multi-turn conversations and to assess the resulting dialogues **Guo et al., "Role-Play Settings with LLM Emulation"**
Our work is similar in that we all adopt LLMs to simulate users in the multi-turn dialogues. However, different from these efforts, we use LLMs to simulate a user to refute the model response for evaluating the refutation instruction-following capacity.
% as the refuter agent and evaluator agent. 
% to refute the response model.