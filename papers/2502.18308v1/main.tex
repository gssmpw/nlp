\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
% \input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{array}
\usepackage{CJK}
\usepackage{multirow}
\iclrfinalcopy

\usepackage{tcolorbox} % 引入 tcolorbox 包
% \usepackage[dvipsnames]{xcolor}

\usepackage{tabularray}
\usepackage{wrapfig}

\usepackage{booktabs}

\newtheorem{lemma}{Lemma}[section]
\newtheorem{remark}{Remark}[section]
\newtheorem{corollary}{Corollary}[section]

\def\elliott#1{{\color{cyan}{\bf [Elliott:} {{#1}}{\bf ]}}}
\def\yun#1{{\color{red}{\bf [Yun:} {{#1}}{\bf ]}}}

\DeclareRobustCommand{\responsebox}[2][gray!20]{%
\begin{tcolorbox}[
        % breakable,
        left=0pt,
        right=0pt,
        top=0pt,
        bottom=0pt,
        colback=#1,
        colframe=#1,
        width=\linewidth, 
        enlarge left by=0mm,
        boxsep=5pt,
        arc=0pt,outer arc=0pt,
        ]
        #2
\end{tcolorbox}
}

% \newtcolorbox{prompt}[1]{
%     colback=gray!20,
%     colframe=black,
%     fonttitle=\bfseries\color{white},
%     colbacktitle=black,
%     title=#1
% }
\newtcolorbox{prompt}[1]{
    colback=white,
    colframe=BlueViolet,
    fonttitle=\bfseries\color{white},
    colbacktitle=BlueViolet,
    title=#1
}


\title{RefuteBench 2.0 -- Agentic Benchmark for Dynamic Evaluation of LLM Responses to Refutation Instruction}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Jianhao Yan~\footnotemark[1] ~\&~ Yun Luo \thanks{These authors contributed equally to this work.} \\
Zhejiang University \& Westlake University \\
Hangzhou, China \\
\texttt{elliottyan37@gmail.com} \\
\AND
Yue Zhang~\thanks{Corresponding Author.}\\
School of Engineering, Westlake University \\
Hangzhou, China \\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
 In the multi-turn interaction schema, large language models (LLMs) can leverage user feedback to enhance the quality and relevance of their responses.  However, evaluating an LLM's ability to incorporate user refutation feedback is crucial yet challenging. In this study, we introduce RefuteBench 2.0, which significantly extends the original RefuteBench by incorporating LLM agents as refuters and evaluators, which allows for flexible and comprehensive assessment.
 We design both transient and persistent refutation instructions with different validity periods. Meta-evaluation shows that the LLM-based refuter could generate more human-like refutations and the evaluators could assign scores with high correlation with humans. Experimental results of various LLMs show that current models could effectively satisfy the refutation but fail to memorize the refutation information. Interestingly, we also observe that the performance of the initial task decreases as the refutations increase. Analysis of the attention scores further shows a potential weakness of current LLMs: they struggle to retain and correctly use previous information during long context dialogues.\footnote{\url{https://github.com/ElliottYan/RefuteBench-2.0}}
\end{abstract}

\begin{figure}[ph]
    \centering
    \includegraphics[width=0.96\textwidth]{plots/illu.png}
    \caption{Machine translation instance for a query-feedback-response cycle, where a user provides a refutation instruction to modify the translation results, and the model responds with new results.}
    \label{fig:illu}
\end{figure}

\section{Introduction}

% Large Language Models (LLMs) have revolutionized natural language processing, offering remarkable capabilities in understanding and generating human-like text \cite{}.
Large Language Models (LLMs) provide a significant strength in their design for multi-turn interactions, allowing them to engage in dynamic conversations \citep{instruct-gpt,touvron2023llama,alpaca}. Benefiting from multi-turn interaction, models can leverage user feedback to enhance the quality and relevance of their responses. As shown in Figure \ref{fig:illu}, given some sentences in English, an LLM system is first asked to translate Chinese. Then according to the output, the user further requests to translate some phrases to specific Chinese. The system responds to the refutation with modified responses. 
This process typically involves a query-feedback-response cycle between the LLMs and users,
with various applications such as dialogue recommendation \citep{bernard2024identifying,fang2024multi}, role-playing \citep{gusev2024pingpong}, etc., addressing needs such as continuous knowledge updating, tailoring responses to domain-specific inquiries, and customizing LLMs for personalization \citep{yan2024refutebench}. 



There are complex multi-turn interactions between LLMs and users in real-world scenarios.  
According to the validity period of the user refutation feedback, they can be roughly categorized into two types:
transient and persistent refutation. Transient refutation asks models to improve their responses given a single query, typically involving multiple rounds of refinement. A concrete example of transient refutation is shown in Figure \ref{fig:example} (left), where the original query is about translating from English to Chinese. The refutation instructions first ask the assistant to change the translation of a phrase, and then require the translation to be more formal and academic. Such transient refutation does not carry forward to the next translation input.

\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\textwidth]{plots/examples.png}
    \vspace{-3mm}
    \caption{Examples of Transient Refutation and Persistent Refutation by  machine translation. }
    \label{fig:example}
\end{figure}

Persistent refutation contributes to long-term requirements, which are useful for applications that need continuous learning and adaptation, such as adaptive long content creation. An example is shown in Figure \ref{fig:example} (right). A persistent refutation, ``when translating customer reviews, highlighting the issues $\cdots$'' is given. The same translation request is expected to be followed for further translation input, even after several rounds of conversations on other topics.  The ability to incorporate both types of refutation is crucial for developing more responsive and adaptable AI systems that can cater to diverse user needs and evolving requirements.


Evaluating an LLM's ability to incorporate user refutation feedback is crucial yet challenging, falling within the realm of assessing instruction-following capabilities \citep{instruction-follow-verbalizer,zhou2023instruction,he2024complex}. Despite its importance, this area has received limited attention, with RefuteBench \citep{yan2024refutebench} being a primary work on this capability. 
% It designs template-based persistent(single-time) refutations and evaluates the model response by lexical matches, focusing on the memorization of the refutation information. 
It evaluates models' following of a \textit{persistent} refutation using \textit{template} and lexical matching methods, focusing on how well the model memorizes the refutation information across the dialogue.
For example, it designs the refutation template for translation: `For the following translation, you should translate the word \{English\} into \{Chinese\}', and select a counterfactual translation of a phrase from the dictionary to fill the template as the refutation. The satisfactory response contains the specific translation phrase in the refutation, which directly uses the lexical match to calculate.
% add memorize? 
% However, the evaluation process still faces several hurdles: (1) it requires a dynamic assessment based on each model's unique responses, making standardization difficult; (2) it must ensure fairness across models with varying capabilities. \textcolor{red}{how to explain} 
However, the evaluation methodology reveals two significant limitations in modeling authentic user-assistant interactions: (1) \textit{the template-based assessment fails to capture the diverse linguistic patterns and expressions characteristic of genuine user refutations}, and (2)\textit{ it fails to account for transient refutation scenarios, overlooking the iterative nature of user feedback in real-world scenarios.}
% The single-refutation framework fails to account for transient refutation scenarios, which constitute a critical dimension
% These challenges underscore the complexity of developing comprehensive and unbiased evaluation methods for evaluating LLMs's capacity to respond to user refutations.



% \elliott{add how to control fairness.}
To address the above issues, we introduce RefuteBench 2.0, an agentic evaluation framework that significantly extends the original RefuteBench \citep{yan2024refutebench} in several aspects. 
% The evaluation approach in \cite{yan2024refutebench} relied on data generated based on lexical rules. 
First, by incorporating LLM agents as refuters and evaluators, we expand the framework from a template-based static benchmark to a dynamic benchmark with a diverse range of dynamic demands.  This allows for a more flexible and comprehensive assessment of LLM refutation responses that better simulate real-world interaction scenarios. Second, RefuteBench 2.0 includes the evaluation of transient refutation, which is a frequent user scenario, overlooked in the original framework.
% Figure \ref{fig:illu} shows our framework with a transient refutation, where the refuter is required to translate `Ashanti Development' into a specific phrase in Chinese, and the evaluator verifies whether the response satisfies the refutation. 


To demonstrate the effectiveness of LLM-based refuters and evaluators, we conduct a meta-evaluation involving human participants. The results reveal that the most effective evaluator GPT-o1-mini achieves a 0.79 Pearson Correlation with a human score, compared to a human inter-annotator agreement~(IAA) rate of 0.84. The results indicate the applicableness of the LLM-based evaluators in the automatic evaluation protocol.
As for the refuter, in contrast to RefuteBench 1.0, our LLM-based refuter can produce more human-like and appropriate refutations shown by human annotations across the dialogues. The generated refutations in the first round and the third rounds achieve human-like and appropriateness scores of 4.08 and 3.82, 4.03 and 3.51 on a scale of 5, significantly higher than those of RefuteBench 1.0 (2.36 and 1.86). 

We then use RefuteBench 2.0 to evaluate LLMs such as GPT-4o\footnote{\href{https://platform.openai.com/docs/models/gpt-4o}{https://platform.openai.com/docs/models/gpt-4o}}, Claude-3.5-Sonnet\footnote{\href{https://www.anthropic.com/news/claude-3-5-sonnet}{https://www.anthropic.com/news/claude-3-5-sonnet}}, Mixtral-8x7B-Inst \citep{jiang2024mixtralexperts}, LLaMA-3.1-70B-Inst\footnote{\href{https://ai.meta.com/blog/meta-llama-3-1/}{https://ai.meta.com/blog/meta-llama-3-1/}} \citep{llama3},  Qwen-2.5-7B-Inst \citep{qwen2.5}, and Gemma-2-9B-Inst\footnote{\href{https://huggingface.co/google/gemma-2-9b-it}{https://huggingface.co/google/gemma-2-9b-it}} \citep{gemma2}.
% GPT-4o, Claude-3.5-Sonnet, Mixtral-8x7B-Inst, Qwen-2.5-7B-Inst, Gemma-2-9B-Inst, and LLaMA-3.1-70B-Inst.
Results indicate that current LLMs can provide satisfactory responses to refutations, but struggle to retain refutation information as dialogues lengthen, in both transient and persistent settings. Intriguingly, we observe a decline in response performance to the initial queries, i.e. a task inconsistency occurs, as the number of transient refutations increases. This phenomenon highlights a potential vulnerability of LLMs in lengthy refutation dialogues. Analysis of the attention scores shows that current LLMs struggle to retain and correctly use previous information during long context
dialogues, which leads to the forgetting of the initial refutations or the task inconsistency problem. 
 

% To manifest the effectiveness of LLM-based evaluators and refuters, we first carry out meta evaluation by humans. Results demonstrate that the most effective evaluator in such a task is GPT-4-o1, achieving 0.79 Pearson Correlation with human scores, where the human inter-agreement is 0.84. Comparing with the RefuteBench 1.0, our refuter agent would introduce more human-like and appropriate refutations. 
% Experimental results of LLMs further show that current LLMs could provide satisfactory response to the refutations but fails to memorize the refutation information as the dialogue increases for both transient and persistent settings. Interestingly, we find that the response performance to the initial queries such as translation would decreases as the transient refutations increases. The phenomenon indicates the potential weakness of LLMs in the long refutation dialogues.

% These enhancements make RefuteBench 2.0 a more robust and comprehensive tool for evaluating LLMs' capacity to receive and integrate feedback in complex, multi-turn dialogues.

% 本篇工作是对\cite{}的拓展。与这篇论文不同的是，我们使用大模型作为refute feedback的提供者，并使用大模型来作为模型接受refute feedback并应用的评价者。而在\cite{}，我们是基于规则生成的数据，例如QA任务中，我们使用的是counterfactual的知识作为refute instruction的内容，并且我们使用的是the method of lexical match来评测的模型应用refute feedback的能力。

% This work is an extension of the study \cite{yan2024refutebench} which is accepted by the Findings of ACL 2024. Unlike the cited paper, in this research, we utilize LLMs as the refuter to provide feedback and as the evaluators of the capacity to applying feedback knowledge. In contrast, the approach in \cite{yan2024refutebench} relied on data generated based on rules. For example, in QA tasks where counterfactual knowledge is used as the content for refute instructions. Furthermore, the evaluation of the model's ability to apply refute feedback in \cite{yan2024refutebench} is conducted using the method of lexical matching.



\section{Related Work}
\paragraph*{Instruction Following.}
Our work falls in the broader realm of evaluating LLMs' instruction-following capacities.
IFEval~\citep{zhou2023instruction} uses verifiable instruction to evaluate the instruction-following ability of LLMs. 
\cite{instruction-follow-summ} propose a meta-evaluation of the instruction-following for text summarization. \cite{instruction-follow-verbalizer} check LLMs’ instruction-following by checking whether a verbalizer can override the models’ output.
Other efforts are devoted to evaluating the instruction-following for complex instructions~\citep{he2024complex} and sequential instructions~\citep{chen2024sifo}.
Apart from this work, RefuteBench~\citep{yan2024refutebench} first evaluates the instruction-following with refutation, investigating whether LLMs can follow  user's feedback in modifying its output or be stubborn to original outputs. 
We extend this work from several important aspects: (1) integration of another important scenario -- \emph{Transient refutation}; (2) use of agentic dynamic evaluation protocol to replace template-based protocols, which both requires careful efforts in adapting each task with appropriate templates and facilitates \textit{context-aware} evaluation with \textit{versatile} feedback types. 
% Their findings demonstrate that LLMs like \texttt{gpt-3.5-turbo} and \texttt{Mixtral-Instruct} can be 
% the model's instruction-following capability can extend through dialogue history. 
% \elliott{add new literature}

\paragraph*{Dynamic Evaluation.}
In contrast to static evaluation using pre-constructed data, 
dynamic evaluation is proposed to address the “false promise” to data contamination \citep{Bender2021,kocon2023chatgpt} and over-fitting benchmarks \citep{zhu2023dyval}.
% Dynamic evaluation proposes to change the evaluation periodically to avoid this issue.  
% rephrasing the evaluation data and maintaining the concept,  is proposed to address this issue. 
\cite{zhu2024dyval} propose to change MMLU \citep{hendrycksmeasuring}, BBH \citep{suzgun2023challenging}, GSM8k \citep{cobbe2021training} and ARC-C \citep{clark2018think} benchmarks to dynamic evaluation by applying Meta Probing Agents.
\cite{burnell2023revealing} use three basic cognitive abilities -- language understanding, problem-solving, and domain knowledge to conduct dynamic evaluation sample generation and support multifaceted ability analysis.
\cite{fan2023nphardeval}~(NPHardEval) generates new NP-hard math problems and updates the evaluation set monthly.
Using a dynamic evaluation protocol, we evaluate LLMs' refutation instruction following capacities with context-aware, and human-like instructions, comprehensive refutation types, and low human annotation cost.

% Thanks to the nature of dynamic evaluation protocol, our work also avoids the problem of data contamination. Different from these work, 

\paragraph*{LLM Agents in Simulation.}
Increasing research efforts use LLMs as agents to simulate human behavior. For example, in dialogue recommendation systems, some studies employ LLMs to simulate users \citep{bernard2024identifying,fang2024multi,wang2023rethinking}. These LLM agents provide feedback to evaluate the performance of recommendation systems during the conversation, which offers advantages such as simplicity, cost-effectiveness, and time efficiency. Additionally, some research uses LLMs to study model interactivity \citep{bang-etal-2023-multitask}, and to better understand the limits of LLM agents in interactive environments. Studies also propose examining their interactions in benchmark decision-making scenarios \citep{park2024llm}. In role-play settings, work leverages LLMs to emulate users in dynamic, multi-turn conversations and to assess the resulting dialogues \citep{gusev2024pingpong}. Our work is similar in that we all adopt LLMs to simulate users in the multi-turn dialogues. However, different from these efforts, we use LLMs to simulate a user to refute the model response for evaluating the refutation instruction-following capacity.
% as the refuter agent and evaluator agent. 
% to refute the response model.


\section{Problem Definition}

\paragraph{Refutation Instruction.} 
Given a model with parameters $\theta$, a user's query $q$~(e.g., `\textit{Please translate $\cdots$ into Chinese.}') and the corresponding answer from the model $a=f(q; \theta)$~(e.g., `\textit{The Chinese translation is $\cdots$}'),  
a \emph{refutation instruction} $r$ is a natural language instruction that refutes the current answer and carries the feedback from the user to the LLMs~( e.g., `\textit{Please change the word $\cdots$ into $\cdots$}' or `\textit{I am not satisfied with the response, the translation should be more formal.}'). 


\paragraph{Transient Refutation.} 
We evaluate the transient interactions between the user and the assistant -- a user provides an initial query and provides feedback several times. Formally, given a query $q$, the first answer from the assistant is $a_0$. The user's refutation is a sequence \{$r_i | i \in (1,2,\cdots)\}$, and the assistant's answer after refutation $r_i$ is $a_{i}, i=1,2,\cdots)$). Refutation instructions are given one by one and all are focused on the query $q$, where
each refutation instruction can cover a different aspect of the query. Figure \ref{fig:example} (left) shows a concrete example.
% A concrete example of transient refutation can be found in Figure \ref{fig:example} (left), where the original query is about translating from English to Chinese.
% The refutation instructions first ask the assistant to change the translation of a phrase, then require the translation to be more formal and academic. 

Transient refutation mimics the process where a user asks the assistant to complete a task but is not satisfied with its initial response, which is a widespread use case. Users' frustration can increase when the assistants fail to respond accordingly with increasing refutation instructions.


\paragraph{Persistent Refutation.} This setting is the same as in RefuteBench1.0. We evaluate whether the model can memorize persistent feedback. 
Formally, we define a turn of interaction here as $(q_0, a_0, r_0, \hat{a_0})$, consisting of query instruction, model's response, refuting instruction, and response after the refutation. 
% The refuting instruction $r$ and response after refuting $a'$ are optional. 
% Then, a dialogue consists of several turns of interactions. 
Then, we include additional turns of user-agent interactions $\{(q_i, a_i) | i \in (1,2,\cdots)\}$ irrelevant to the previous query and refuting instructions. 
Finally, we evaluate whether the model memorizes the refuting instruction by querying with the initial instruction $q_0$ and evaluating the model's final response. 
% Compared to the transient setting, the refutation instruction now includes a scope of application, e.g., `When translating phrases related to personal growth and emotional well-being...'.
% An example is shown in Figure \ref{fig:example} (right). After translation, a persistent refutation, ``when translating customer reviews, highlighting the issues $\cdots$'' is given. Then after several irrelevant instructions, the same translating request is given to test whether the model could memorize the refutation. 

Persistent refutation mimics the real-world scenario where a user tells the assistant about his requirements and asks a similar question afterward. If a user has to repeat his requirements persistently for new instances, it would reduce the user's satisfaction. 

\paragraph{Dynamic Evaluation.} The evaluation protocol we propose is inherently \textit{dynamic}, as refutations must adaptively respond to varying model outputs. This differs from conventional static evaluation frameworks \citep{yan2024refutebench}, where test cases remain constant. 
% While prior work \cite{yan2024refutebench} approached this challenge through manually crafted rules—such as maintaining predefined word replacement pools for machine translation tasks—such methods are both labor-intensive and context-insensitive. These rule-based approaches fail to capture the nuanced contextual dependencies that emerge in real-world scenarios and face significant scalability limitations.


% \paragraph{Dynamic Evaluation.} It is worth noting that the problem we evaluate here is \textit{dynamic}. In particular, the refutation has to fit the responses and change according to different responses, making our evaluation a \textit{dynamic} evaluation protocol. 
% In \cite{yan2024refutebench}, this is achieved by manually written rules. For instance, in machine translation, they maintain a candidate pool for word replacement for each query, and samples from ones not found in the current response. 
% However, such a design does not scale and is essentially context-agnostic. \elliott{modify}
% % such as word matching in machine translation and length count in article writing. 


% We define two functions, $\text{Applicable}(q, r) \in \{0, 1\}$, a function denoting whether a certain refutation instruction $r$ should be applied to query $q$, and $\text{Satistifed}(a, r)$, where a certain answer $a$ satisfies the refutation instruction $r$.
% For example, xxxxx \yun{Move it to Evaluator?}

% In RefuteBench 1.0, this is achieved by manually written rules for different tasks. 

% \paragraph{Difference to RefuteBench 1.0}
% In RefuteBench 1.0, the evaluation focuses on persistent refutation. In this work, we extend the work with the Transient scenario, which complements the evaluation of refutation greatly. 

\begin{table}[t]\small
    \centering
    % \begin{tabular}{|c|c|c|}
    \begin{tabular}{l|c|c}

        \hline
        \textbf{Comparison} &\textbf{RefuteBench~1.0} &\textbf{RefuteBench~2.0} \\
        \hline
        Scenario & Persistent & Transient \& Persistent \\\hline
        Tasks & MT, QA, Writing & MT, Summarization, Writing \\\hline
        Refutation Generation & Rule-based & Agent-based \\\hline
        Refutation Scope~(e.g., MT) & Word Usage & Word Usage, Phrase Usage, Style \\\hline
        Context-aware & No & Yes \\\hline
        Dynamic Evaluation & Yes & Yes \\\hline
    \end{tabular}
    \caption{Comparison between RefuteBench1.0 and RefuteBench2.0~(Ours)}
    \label{tab:compare}
\end{table}


\section{RefuteBench 2.0 -- Agent-driven dynamic evaluation of refutation}
% To address these issues, w
We propose to use an \textit{agentic} evaluation protocol. 
Overall, our evaluation consists of the following basic steps: 
\begin{enumerate}
    \item The evaluated LLM is queried with a set of \textit{seed queries}.
    \item The evaluated LLM responds to these seed queries.
    \item A \textit{refuter} agent generates a refutation for the response. 
    \item The evaluated LLM modifies the response based on the refutation. 
    \item An \textit{evaluator} agent evaluates whether the modification follows the refutation. 
\end{enumerate}
Table \ref{tab:compare} presents the comparison between RefuteBench 2.0 and RefuteBench1.0~\citep{yan2024refutebench}. 

% In this section, we discuss the evaluation protocol we proposed. 


% The RefuteBench~1.0 relies on carefully manually designed templates and requirements that can be verified via rules or string matching. 
% In RefuteBench2.0, we incorporate LLM-Agents as our refuter, query generator, and evaluator, facilitating a more flexible and versatile evaluation. 
% Furthermore, we extend previous work by considering transient refutation, which mimics a realistic application of user-assistant multi-turn interactions. 

\begin{table}[t]\small
\centering
\begin{tabular}{lcccccc}
\toprule
\textbf{Type}      & \textbf{Task}    & \textbf{\#Seeds} & \textbf{\#Turns} & \textbf{\#Avg. Turn Token} & \textbf{\#Avg. Dia. Token} & \textbf{\#Avg. Eval. Token} \\
\midrule
Transient & MT      & 100             & 8     & 178.1               & 15.6k               & 98.6k              \\
 & Summ    & 100          & 8& 235.1               & 20.8k               & 137.9k             \\
 & Writing & 100          & 8 & 336.5               & 14.9k               & 75.8k              \\
 \midrule
Persistent& MT      & 100             & 12    & 152.6               & 19.6k               & 26.1k              \\
& Summ    & 100          & 12 & 228.7               & 29.4k               & 41.5k              \\
& Writing & 100          & 12 & 249.5               & 32.1k               & 45.0k              \\
\bottomrule
\end{tabular}
\caption{The statistics of RefuteBench 2.0. \textit{\#Seeds} denotes the average number of seed questions, and \textit{\#Turns} denotes the number of turns for each dialogue. \textit{\#Avg Turn Tokens}, \textit{\#Avg Dia. Token} and \textit{\#Avg. Eval. Token} represent the number of tokens used in each dialogue, that of all dialogues for one model, and that to evaluate dialogues, respectively. }
\label{tab:stats}

\end{table}

% \subsection{Experimental Setup}
% For all used LLM agents, we use greedy search to ensure the reproduction of our experiments. 
% The results are shown in Table \ref{}.
\begin{table}[t] \small
    \centering
\begin{tblr}{
  colspec = {Q[l,m]X[l]},
  stretch = 0,
  rowsep = 6pt,
  hlines = {black, 1pt},
  % vlines = {black, 1pt},
}
  \textbf{Task} & \textbf{Query} \\
  {\textbf{Machine Translation}} & {Please translate the following sentence from English into Chinese.\newline [English]: \textit{\{X\}} \newline [Chinese]:} \\
\textbf{Summarization} & \textit{\{X\}}. Please help me summarize the above document. \\
\textbf{Article Writing} & 
(1) Write a news article with the following headline: \textit{\{X\}} \newline 
(2) Generate a counter-argument to refute the following Reddit post: \textit{\{X\}} \newline 
(3) Write a story with the following prompt: \textit{\{X\}} ... \\
\end{tblr}
\caption{Examples of Initial Queries. In Article Writing, we provide several examples as we take queries from various tasks. }
\label{tab:init_query_example}
\end{table}




\subsection{Seed Query Collection}

We focus on writing tasks in RefuteBench2.0, covering machine translation, summarization, and open-ended writing. 
The three tasks span from constrained to open-ended, representing a range of users' daily usage scenarios.
For machine translation, the same as in RefuteBench, we collect data from the WMT 2023 English-to-Chinese test set. 
For the summarization task, we use the XSum~\cite{xsum} data. 
For the article writing tasks, we follow \citep{mage}, collecting human written texts from a set of benchmark datasets~\citep{cmv,xsum,roc,WP,hswag,squad,scixgen} and constructing instructions for a diverse set of writing tasks. 
Table \ref{tab:stats} provides our data statistics and Table \ref{tab:init_query_example} presents examples of our initial query. 
% We sample 50 such writing instructions for our evaluation. 
% \textcolor{red}{TODO}
% \begin{table}[t]\centering
% % \begin{tabular}{|c|c|c|c|}
% \begin{tabular}{l|c|c|c}

% \hline
% \multirow{2}{*}{Task} & \multirow{2}{*}{\# of Dialogues} & \multicolumn{2}{c}{\# of Turns / \# of Generated Tokens} \\\cline{3-4}
%                     &     & Transient & Persistent \\\hline
% Machine Translation & 100 &           &            \\\hline
% Summarization       & 100 &           &            \\\hline
% Article Writing     & 50  &           &           \\\hline
% \end{tabular}
% \caption{Data Statistics.}
% \label{tab:stats}
% \end{table}


% (1) Opinion statement~( /r/ChangeMyView Reddit subcommunity \citep{CMV} and Yelp \citep{Yelp};
% (2) News article writing~(\textbf{XSum} \citep{XSum}
% and TLDR\_news\footnote{https://huggingface.co/datasets/JulesBelveze/TLDR\_news};
% (3) Story generation~(the Reddit WritingPrompts~\citep{WP} and ROCStories Corpora (\textbf{ROC}) \citep{roc};
% (5) Commonsense reasoning: HellaSwag \citep{hswag};
% (6) Knowledge illustration: Wikipedia paragraphs from SQuAD contexts \citep{SQuAD};
% (7) Scientific writing: abstracts of scientific articles from SciXGen \citep{scixgen}.



\subsection{The Refuter Agent} 
\paragraph{General Requirements.}
RefuteBench 2.0 uses LLM agents as the refuter.
% {\color{red}The input to a refuter is ...}
The prompt of the refuter agent is shown in Figure \ref{fig:ref_prompt}.
We require our refuter to (1) not generate refutation instructions already achieved by its previous response, and (2) generate refutation instructions focused on a certain aspect. 
Our first requirement is straightforward, as the core of refutation instruction is to \emph{refute} its current response. 


\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\textwidth]{plots/ref_prompt.png}
    \vspace{-3mm}
    \caption{The refuter prompt in RefuteBench 2.0. \textit{FOCUS} is randomly chosen from `style', `word usage', and `phrase usage'. \textit{QUERY} is the initial query and \textit{RESPONSE} is the model's previous response. We also inform the refuter with its previous refutations with \textit{REFUTATION\_i}, to avoid duplicate or conflicting refutations. }
    \label{fig:ref_prompt}
\end{figure}

\paragraph{Fairness.} As the generation of refutation instructions is based on the evaluated model's dynamic output, different models would receive different refutation instructions. 
To ensure fairness between the different models evaluated, we propose the following two designs. 

% \paragraph{Refutation Focus}
% We introduce the refutation focus to avoid such biases in the evaluation protocol.
% The second requirement is due to the \textit{fairness} of our evaluation. 
% it is essential to avoid biases in the evaluation protocol. 
% That is, our refuter generate different 
(1) \textit{Refutation focus} in refutation instructions, e.g., phrase usage, word usage, and language style in machine translation, controls the focus on the current refutation. 
By doing so, we control the distribution of \textit{refute focus} and make it consistent across different models, to ensure the fairness of our evaluation. 

(2) \textit{Style specification} is introduced to avoid a change in the language and style of the refutation. We consider three aspects in our prompt, namely clarity, abstraction, and emotion. 
We ask the refuter to generate refutations that are ambiguous, abstract, and with a strong emotion. 

\paragraph{Other Design Choices}
In our preliminary experiments, we find that providing the refutation history is crucial for a refuter. Otherwise, the refuter might generate similar or repeated refutations. 
In addition, we find that models such as GPT-4o are not suitable as a refuter, as they do not follow our style control prompts, and instead always stay polite and helpful. Thus, among all our experiments, we use LLaMA-3.1-70B as our refuter. 

% \paragraph{Difficulty Control} Furthermore, we control the difficulty of refutation instruction over three key aspects, \emph{clarity, abstraction, and emotion}. Each of the aspects has two contrastive instructions, where one of them is helpful, direct, and clear, and the other one is emotional, abstract, and vague. 
% By controlling these key aspects of refutation instructions, we ensure that the overall difficulty level of a refutation instruction and thus the fairness of our evaluation.

% In practice, we find that the agent can generate various kinds of refutation instructions. 
% We note that this way of refutation instruction is more attached to real interaction between users and assistants. 

% For refuter, we use OpenAI's \texttt{gpt-4o-mini}.
% Our prompt can be found in Figure \ref{}.

% \begin{prompt}
% {Refuter Prompt}
% \textbf{[SYSTEM]} You are a refuter. You are responsible for asking the model to change the current answer with a new requirement. For example, in the translation task, if the source sentence contains the word 'Apple', and the model's translation for this word is ''\begin{CJK}{UTF8}{gbsn}苹果\end{CJK}'', you can ask the model to translate the word to ''\begin{CJK}{UTF8}{gbsn}苹果公司\end{CJK}''. You should ensure that your requirement is valid for the current task. You should ensure the answer does not already meet your refute instruction. You can give several refute requirements for each answer. You should directly give the refute requirement. Pretend you are a real user who is not satisfied with the response. Your refute requirement should focus on the response's \{\textit{FOCUS}\}.\newline

% The feedback should be highly ambiguous, use vague language, or rely heavily on implicit suggestions that are difficult to discern without significant interpretation. The feedback should be highly abstract or theoretical, focusing on overarching principles or complex ideas that require significant effort to translate into practical application. The feedback should be highly emotionally charged, and dominated by strong feelings such as anger, excitement, frustration, or joy. The emotional content significantly colors the entire feedback, making it challenging to discern the underlying message or suggestions without careful interpretation of the emotional context.\newline

% \textbf{[USER]} The original query is `\textit{\{QUERY\}}' and the model's response is `\textit{\{RESPONSE\}}'.
% Your previous refutes include:\newline
% \textit{\{REFUTATION\_1\}}\newline
% \textit{\{REFUTATION\_2\}}\newline
% ...\newline
% Your refute should not be as the same as the previous refutes and cannot conflict the previous refute.
% \end{prompt}



\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\textwidth]{plots/eval_prompt.png}
    \vspace{-3mm}
    \caption{The evaluation prompt in RefuteBench2.0. \textit{QUERY}, \textit{PPEV}\_ \textit{ANSWER}, \textit{REFUTATION} and \textit{NEW}\_\textit{ANSWER} refer to the initial query, the response in the last turn, the refutation instruction and the new response in the current turn, respectively. }
    \label{fig:eval_prompt}
\end{figure}

\subsection{The Evaluator Agent} 
As the refutation instructions generated by the refuter are of a variety, we cannot evaluate the model responses based on simple string match, like done in \citet{yan2024refutebench}. 
As a result, we also use an LLM agent as the evaluator. 
Four inputs are given to the evaluator: the original instruction, the refutation instruction, the response before the refute, and the response after the refute. 
% In practice, we provide four inputs to the evaluator, i.e., the original query, the model's response before refutation, the refutation instruction, and the model's response after refutation. 
Inspired by \citet{mt-bench}, we use LLM-as-a-Judge to rate 1-5 for each refuted response as well as the explanation for the rating. 

Through all experiments, we use OpenAI o1-mini as our evaluator, based on its high correlation with human evaluation~(Section \ref{evaluator_res}).



% \begin{prompt}
% {Evaluator Prompt}
% \textbf{[SYSTEM]} 
% You will be given a refuting instruction and a model's response before given the instruction and after given the instruction. Please act as an impartial judge and evaluate the quality of how good the refuting instruction is followed by the response after the refuting instruction. Be as objective as possible. After providing your explanation, you must rate the response on a scale of 1 to 5 by strictly following this format: 
% "[[rating]]", for example: "Rating: [[3]. 
% \newline

% \textbf{[USER]} The original instruction:\newline\{QUERY\}\newline\newline The response before refute:\newline\{PREV\_ANSWER\}\newline\newline The refuting instruction is:\newline\{REFUTATION\}\newline\newline The model's response after refute:\newline\{NEW\_ANSWER\}
% % [USER] The original instruction:\newline
% % {QUERY}\newline\newline
% % The response before refute:\newline{prev}\n\nThe refuting instruction is:\n{refute}\n\nThe model's response after refute:\n{now
% % \{Query\}. \{Answer\}. \{Refutation\}. \{Answer\}. 
% % \textcolor{red}{this need check}

% % \|The Start of Reference Answer\|
% % 
% % \#\#\# User: {QUERY}



% % \newline\newline
% % The original instruction:\textit{\{INSTRUCTION\}}\newline\newline
% % The response before refute:\textit{\{PREV\}} \newline\newline
% % The refutation instruction is:\textit{\{REFUTE\}}\newline\newline
% % The model's response after refute:\textit{\{NOW\}}
% \end{prompt}


% We compare the output of the evaluator to human annotations and find the accuracy is high~(xx\%). 
% For evaluator, we use OpenAI's \texttt{gpt-4o-mini}.


% \begin{table}[h]
% \centering
% \begin{tabular}{|>{\centering\arraybackslash}m{4cm}|p{0.70\textwidth}|}
% \hline
% \textbf{Task} & \textbf{Query}  \\
% \hline
% \centering Machine Translation & Please translate the following sentence from English into Chinese.\newline [English]: \textit{\{SOURCE\}} \newline [Chinese]:
%  \\
% \hline
% \centering Summarization & \textit{\{DOCUMENT\}}\newline Please help me summarize the above document. \\
% \hline
% \centering Open-ended Writing & 
% (1) Write a news article with the following headline: \textit{\{TITLE\}} \newline
% (2) Generate a counter-argument to refute the following Reddit post: \textit{\{POST\}} \newline
% (3) Write a story with the following prompt: \textit{\{PROMPT\}} \newline ... \\
% \hline
% \end{tabular}
% \caption{Examples of Initial Queries}
% \label{tab:init_query_example}
% \end{table}





% \subsection{Refutation Instruction Quality Control}

% \subsection{Qualitative Study of Refuting Instructions}
% To help readers better understand our evaluation protocol, here we provide a qualitative study of refutation instructions. 


% \subsection{Error Patterns}

% \subsection{Evaluation Quality Control for LLM Evaluator}
% We compare different strategies for the evaluation quality of our LLM evaluator. 
% We consider the following variants of evaluators: (1) naive prompting; (2) with in-context learning demonstrations; (3) comparison prompts; (4) complete dialogue history (5) reference-guided evaluation; (6) chain-of-thought prompting. 


\subsection{Meta Evaluation of RefuteBench 2.0}
To manifest the effectiveness of the LLM-based refuter and evaluator, we conduct human evaluation on the generated dialogue data, verifying that the refuter can generate human-like and appropriate refutations (Section \ref{refuter_res}), and the evaluator could highly correlate human scores (Section \ref{evaluator_res}).
We hire two annotators who are PhD candidates in natural language processing to conduct the Meta Evaluation, and each annotator is paid 30\$/hour for the annotation.


\subsubsection{Refuter}
\label{refuter_res}
We first conduct human evaluations on the refuters to assess whether LLMs can generate effective refutations compared to those in RefuteBench 1.0. Specifically, we randomly select 50 data points from RefuteBench 1.0 and 50 from RefuteBench 2.0. We then instruct two annotators to assign two scores, ranging from 1 to 5, to each data point from the perspectives of Human-like and Appropriateness. The evaluation prompt provided to the annotators is as follows:

\textit{\{Context\}. Please rate the response on a scale of 1-5 for each metric: 1. Human-like. 2. Appropriateness. Enter score (2 digits, each 1-5, e.g. '55'): }

where `Context' includes the information of $(q,a_0,r_0)$, $q$ is the query, $a_0$ is the initial answer, and $r_0$ is the initial refutation.
% After annotation, the scores of Human-like and Appropriateness are  4.08 and 3.82 in RefuteBench 2.0, and 2.36 and 1.86 in RefuteBench 1.0, respectively. The results demonstrate that using the LLMs as the refuter could significantly simulate the real-world human refutation senarios. 
After the annotation process, the scores for Human-like and Appropriateness in RefuteBench 2.0 are 4.08 and 3.82, respectively, while in RefuteBench 1.0, they are 2.36 and 1.86, respectively. We also evaluate the third refutation in the transient scenarios, where the scores for Human-like and Appropriateness are 4.03 and 3.51, respectively, demonstrating a stronger performance as well. 
These results indicate that using LLMs as refuters can significantly simulate real-world human refutation scenarios, demonstrating substantial improvements in both human-like qualities and appropriateness in the newer version of the benchmark.

\subsubsection{Evaluator}
\label{evaluator_res}
 \begin{wrapfigure}[17]{r}{0.47\textwidth}
    \centering
    \includegraphics[width=0.47\textwidth]{plots/anno.png}
    \vspace{-5mm}
    \caption{The correlation between different evaluator performance and human annotations.}
    \label{dynamic}
\end{wrapfigure}
To assess the evaluator agent, we begin by randomly selecting 100 data points from the machine translation dataset, along with refutation instructions generated by LLaMA3.1-70b-Inst and the corresponding responses from various LLMs. Subsequently, two human annotators are tasked with assigning a score ranging from 1 to 5 to each response.
This scoring assesses whether the response adequately addresses the refutation instruction. The instructions provided to the human annotators are consistent with those used in the evaluator prompt.

% We first randomly select 100 data in the machine translation together with the refutation instructions generated by LLaMA3.1-70b-Inst and the corresponding responses from different LLMs. Then, two human annotators are instructed to assign a score 1-5 to the response to measure whether it satisfies the refutation instruction. The instruction to human is the same with the evaluator prompt. 


% Then we adopt different LLMs as the evaluator to assign scores to the response and calculate the Pearson correlation between the human scores and evaluator scores. The results are shown in Figure \ref{annotation_cor}. As we observe, the human correlation score is 0.84, indicating the strong agreements of humans to evaluate the responses under the given instructions. Then the model GPT-4-o1-mini achieves the most compelling correlation (0.79) to the human annotators. The model LLaMA-3.1-70B-Inst achieves the weakest correlation (0.05), showing the insufficient capacity as the response evaluator. Thus, in the following experiments, we all adopt GPT4-o1-mini as the evaluator. 

We then adopt different LLMs as evaluators to assign scores to the responses and calculate the Pearson correlation between the scores given by humans and those assigned by the evaluators. The results are presented in Figure \ref{annotation_cor}. We observe that the human correlation score is 0.84, indicating strong agreement among humans in evaluating the responses according to the provided instructions. Among the models, GPT-4-o1-mini achieves the most compelling correlation with human annotators, scoring 0.79. Conversely, the model LLaMA-3.1-70B-Inst exhibits the weakest correlation at 0.05, demonstrating its insufficient capacity as a response evaluator. Consequently, in subsequent experiments, we exclusively use GPT-4-o1-mini as the evaluator.


\section{Experiments}
We evaluate various LLMs on the capacity to respond to the refutations, including closed-source models such as GPT-4o\footnote{\href{https://platform.openai.com/docs/models/gpt-4o}{https://platform.openai.com/docs/models/gpt-4o}} and Claude-3.5-Sonnet\footnote{\href{https://www.anthropic.com/news/claude-3-5-sonnet}{https://www.anthropic.com/news/claude-3-5-sonnet}}, and open-source models such as Mixtral-8x7B-Inst \citep{jiang2024mixtralexperts}, LLaMA-3.1-70B-Inst\footnote{\href{https://ai.meta.com/blog/meta-llama-3-1/}{https://ai.meta.com/blog/meta-llama-3-1/}},  Qwen-2.5-7B-Inst \citep{qwen2.5}, and Gemma-2-9B-Inst\footnote{\href{https://huggingface.co/google/gemma-2-9b-it}{https://huggingface.co/google/gemma-2-9b-it}}. 
For all used LLM models, we use greedy search to ensure the reproduction of our experiments. 

\subsection{Transient Refutation}
% We first evaluate the instruction-following capacity of LLMs in transient refutations in Section \ref{overall_sec}. Then we further analyze whether the model could memorize the refutations information before, and the initial task requirements in Section \ref{reforget_sec} and Section \ref{taskconsist_sec}, respectively. 
We first assess the capacity of LLMs to follow instructions in the context of transient refutations, as detailed in Section \ref{overall_sec}. Subsequently, we analyze the model's ability to retain information about previous refutations in Section \ref{reforget_sec}, as well as its consistency with the initial task requirements in Section \ref{taskconsist_sec}.

\subsubsection{Overall Results}
\label{overall_sec}
\begin{wraptable}{r}{0.5\textwidth}  \small
\centering
\begin{tabular}{lccc}
\toprule
\textbf{Models}             & \textbf{MT}   & \textbf{Sum}  & \textbf{Writing} \\
\midrule
\textbf{GPT-4o}             & \textbf{4.80} & \textbf{4.92} & 4.71    \\
\textbf{Claude-3.5-Sonnet}  & 4.77 & 4.79 & 4.53    \\
\textbf{Mixtral-8x7B-Inst}  & 4.11 & 4.88 & 4.54    \\
% Qwen-2.5           &      &      &         \\
\textbf{Qwen-2.5-7B-Inst} &	4.71&	4.85&	4.49 \\
\textbf{Gemma-2-9B-Inst} &	4.57&	4.91& \textbf{4.79} \\
\textbf{LLaMA-3.1-70B-Inst} & 3.79 & 4.73 & \textbf{4.79}   \\ 
\bottomrule
\end{tabular}
\caption{The overall results of different LLMs in the transient refutations.}
\vspace{-5mm}
\label{annotation_cor}
\end{wraptable}



We first show the overall results of the LLMs
% when meeting the transient refutations.
when addressing transient refutations.
Specifically, we calculate the average performance of response to the refutations $1/3\sum_{i=0}^2 R(a_{i+1},r_{i})$.
% As we observe, GPT-4o achieves the most satisfactory performance in the transient refutations of  machine translation and summarization -- 4.80 and 4.92, respectively, and LLaMA-3.1-70B-Inst achieves the most significant performance in Writing -- 4.82.  In Summarization and Writing, the different LLMs achieves a relatively comparable performance, ranging from 4.73 to 4.93 in Summarization, 4.62 to 4.82, respectively. But in machiene translation task, the models Mixtral-8x7B and LLaMA-3.1-70B perform weakly with a large margin, only 4.11 and 3.79, respectively. It indicates that the  refutation instruction in Machine Translation is the most difficult to follow among the three tasks. 
Upon analysis, GPT-4o exhibits the most satisfactory performance in the transient refutations of machine translation and summarization, scoring 4.80 and 4.92, respectively. Gemma-2-9B-Inst and LLaMA-3.1-70B-Inst stands out in the Writing task with a score of 4.79. In the Summarization and Writing tasks, the performance among different LLMs is relatively comparable, with scores ranging from 4.73 to 4.92 in Summarization and from 4.49 to 4.79 in Writing, respectively.
However, in the machine translation task, Mixtral-8x7B-Inst and LLaMA-3.1-70B-Inst perform weakly, scoring only 4.11 and 3.79, respectively. This suggests that following refutation instructions in machine translation presents the most significant challenge among the three tasks.




\begin{table}[t] \small
\centering
\begin{tabular}{lccccccccc}
\toprule
\textbf{Tasks }            & \multicolumn{3}{c}{\textbf{Machine Translation}}                  & \multicolumn{3}{c}{\textbf{Summarization}}                 & \multicolumn{3}{c}{\textbf{Writing}}             \\  \midrule
\textbf{Turn}               & Turn1& Turn3 & $\Delta$ &  Turn1& Turn3 & $\Delta$ &  Turn1& Turn3 & $\Delta$ \\
\midrule
\textbf{GPT-4o}             & 4.76 &	{4.32} & 0.44 &	4.92	&{4.44}&	0.48 &4.69	&{4.25} & 0.44 \\
\textbf{Claude-3.5-Sonnet}  & \textbf{4.83} &	3.90&\underline{0.93}&	4.88	&3.74 & 1.14&	\textbf{4.92}	&4.25    &0.67             \\
\textbf{Mixtral-8x7B-Inst}  & 3.96 &	3.36& 0.60&	4.70&	3.82&0.88&	4.72&	3.95& 0.77              \\
\textbf{Qwen-2.5-7B-Inst}	& 4.62 & \textbf{4.41}&0.21&4.92 & \textbf{4.45} &0.47&4.71 &4.24 & 0.47
 \\
\textbf{Gemma-2-9B-Inst}	& 4.56& 3.84 & 0.71&4.98 &4.41 &0.57&4.85 &\textbf{4.38} &0.47\\
\textbf{LLaMA-3.1-70B-Inst} & 3.38 &	3.04& 0.34&	\textbf{4.96}	&3.51 &\underline{1.45}&	4.89&	3.90&\underline{0.99}     \\
\bottomrule
\end{tabular}
\caption{The performance of the response to the first transient refutation before and after two other refutations. The most compelling performance are marked in bold, and the most significant decreases are underlined.}
\label{forgetting_tab}
\end{table}




\subsubsection{Results over Different Turns}
\label{reforget_sec}
We then evaluate the phenomenon of forgetting refutation information during a dialogue, specifically examining whether information from earlier refutations is forgotten after subsequent refutations are introduced. 
% We then evaluate the forgetting of refutation information during the dialog, i.e. whether the refutation information is forgot after other refutations are given, i.e. $R(a_1,r_0)$ and $R(a_3,r_2)$. 
Specifically, we evaluate the response to the first refutation before and after two other refutations are given, i.e. $R(a_1,r_0)$ and $R(a_3,r_0)$.  
% The results are shown in Table \ref{forgetting_tab}.  
% As we observe, the response performance of all the LLMs  degrades with a large margin after other refutations are given, which means the refutation instruction information would be forgotten during the dialog. For example, the performance of the response to the first refutation is 4.76 in machine translation, but decreases to 4.32 when given other two refutations, with a decrease of 0.44.  We also illustrate the performance changes during the dialog in Figure \ref{change_figure} (a). We can observe that the performance decreases as the number of the refutations increases, which indicates that the information of the first refutation is forgotten step by step as the context increases. 
The findings, presented in Table \ref{forgetting_tab}, show that the performance of all LLMs degrades significantly after additional refutations are introduced, indicating that refutation information tends to be forgotten as the dialogue progresses. For instance, in machine translation, the performance of the response in GPT4o to the first refutation drops from 4.76 to 4.32 after two subsequent refutations are given, marking a decline of 0.44.
These dynamics are further visualized in Figure \ref{change_figure} (a), where it is evident that performance decreases as more refutations are added to the dialogue. This trend underscores the challenge LLMs face in retaining information about the first refutation as the context expands and becomes more complex.

Comparing different LLMs, though Claude-3.5-Sonnet achieves satisfactory performance on the direct response to the first transient refutation, degrades with large margins in all three tasks. For example, Claude-3.5-Sonnet initially scores 4.83 in response to the first transient refutation for MT. However, its performance drops to 3.90 to the first refutation when it encounters two additional refutations following the first. Then, Qwen-2.5-7B-Inst can achieve almost the most compelling performance when Turn=3, i.e., 4.41, 4.45, and 4.24 in MT, Summarization, and Writing, respectively, indicating a stronger resistance to the forgetting problem. LLaMA-3.1-70B-Inst, however, it the most fragile in memorization for the tasks of Summarization and Writing, with a delta score of 1.45 and 0.99.



\begin{table}[t] \small
\centering
\begin{tabular}{lccccccccc}
\toprule
\textbf{Tasks }            & \multicolumn{3}{c}{\textbf{Machine Translation}}                  & \multicolumn{3}{c}{\textbf{Summarization}}                 & \multicolumn{3}{c}{\textbf{Writing}}             \\  \midrule
\textbf{Turn}               & Initial & Final & $\Delta$ & Initial & Final & $\Delta$ & Initial & Final & $\Delta$ \\
\midrule
\textbf{GPT-4o}             & \textbf{4.93}    & \textbf{2.73}  & 2.20                  & \textbf{4.83}    & {2.56}  & 2.27    &              4.77 & 3.50 & 1.27                 \\
\textbf{Claude-3.5-Sonnet}  & 4.57    & 2.27  & \underline{2.30}                  & 4.82    & 1.85  & \underline{2.97}                  & 4.77 & 3.08 & \underline{1.69}                  \\
\textbf{Mixtral-8x7B-Inst}  & 3.05    & 2.35  & 1.69                  & 4.20    & 2.10  & 2.84 &                4.67 & 3.12 & 1.55                 \\
\textbf{Qwen-2.5-7B-Inst}  & 4.61 & 2.58	&2.03&4.77& \textbf{2.57} &2.20& \textbf{4.81} & \textbf{3.89} & 0.92\\
\textbf{Gemma-2-9B-Inst}	&	4.49 &  2.35 &2.14& 4.77&2.20 &2.57&4.60 & 3.47 & 1.13\\
\textbf{LLaMA-3.1-70B-Inst} & 3.64    & 1.95  & 0.70                  & 4.75    & 1.91  & 2.10            &      4.78 & 3.09 & \underline{1.69}      \\
\bottomrule
\end{tabular}
\caption{The performance of the original instructions such as translation, summarization, and writing before and after refutations.}
\label{task_table}
\end{table}


\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\textwidth]{plots/forgettingline.png}
    \vspace{-2mm}
    \caption{The performance of the response to the first refutation and the task instruction before and after other refutations in Machine Translation. (a) the performance of the response to the first refutation; (b) the performance of the response to the task instruction. }
    \label{change_figure}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\textwidth]{plots/case_two.png}
    % \vspace{-2mm}
    \caption{Case studies of the responses to the initial task instruction $q$ in machine translation for the model Claude-3.5. The refuting instruction is about `more of an atmosphere of terrifying alien thoughts'. This case shows that the final translation \textit{fails to keep the original semantics} of the source sentence.}
    \label{change_figure}
\end{figure}




\subsubsection{Task Consistency During Dialog}
\label{taskconsist_sec}
% We further investigate the performance of the initial task instruction before and after the transient refutations are given to the LLMs, i.e. $R(q,a_0)$ and $R(q,a_3)$. The results are shown in Table \ref{task_table}. We can observe that the performance of the response to the initial task instruction also decreases in the final round for all LLMs, indicating that the final responses deviate from the original task objective. For example, GPT-4o achieves 4.93 performance in machine translation when given the original task description, but when several refutations are given, the model performance decreases to 2.73, an surprisingly low translation performance. We also illustrate the performance of responses to the machine translation in Figure \ref{change_figure}. As we observe, the model performance decreases with the increase of the dialog turns.

We further assess the performance of the initial task instruction before and after transient refutations are given to the LLMs. Specifically, we compare the response to the original task query $R(q,a_0)$ with the response following three refutations $R(q,a_3)$. The results are compiled in Table \ref{task_table} and demonstrate a notable decrease in performance across all LLMs, suggesting that the final responses increasingly deviate from the original task objective due to the impact of intervening refutations.
For instance, GPT-4o initially achieves a high performance of 4.93 scores in the machine translation task, responding directly to the initial task description. However, after several refutations are introduced, the performance sharply declines to 2.73, indicating a significant deterioration in the model's ability to maintain fidelity to the original task under the pressure of refutation.
This trend is visually represented in Figure \ref{change_figure}, where it is evident that the performance of responses in the machine translation task deteriorates progressively with each additional turn in the dialogue. This visualization and the data suggest that as the dialogue expands with new refutations, the models struggle to retain and apply the original task instructions effectively, leading to a marked degradation in task-specific performance.

Such a forgetting phenomenon results from the model's excessive attention to the refutation, which is closer to the response compared to the original instruction. For example, Figure \ref{change_figure} shows two cases for demonstration. We can see that in the first case, the initial response captures the basic meaning of the English source sentence. However, after being given the refuting instruction that asks for more of an atmosphere of terrifying alien thoughts, the model overcompensates, sacrificing the original semantic content in favor of atmospheric elements, demonstrating an inability to balance refinement requests with essential meaning preservation. In the second case of summarization, the number of words in the final summary is even larger than that of the initial document (171 words vs 202 words).


% Such a forgetting phenomenon results from the models generating some information according to the refutation, while they originally do not exist. 
% \textcolor{red}{Figure \ref{} show some cases for further explanation.  xxxxx } 


% Then comparing the different LLMs, GPT-4o achieves the most compelling performance both in the initial response and final response, even though the performance degrades with large margins (around 2.20). Claude-3.5-Sonnet suffers from the largest forgetting problem, which indicates a significant prefer to adding non-exist content.

The comparison across different LLMs reveals that Qwen-2.5-7B-Inst mostly outperforms other models in terms of maintaining the highest levels of performance in the final responses. However, it is important to note that even Qwen-2.5-7B-Inst experiences a substantial drop in performance, with a degradation margin of approximately 2.0 points in MT and summarization. This indicates that while it starts from a higher baseline, it is still significantly affected by the introduction of additional refutations.
Claude-3.5-Sonnet, on the other hand, exhibits the most pronounced problems related to forgetting. The performance degradation in this model is particularly severe. This tendency to add irrelevant or fabricated information or generation collapse could be symptomatic of the model's inability to effectively maintain and integrate the core factual content through the course of the dialog, especially after encountering multiple refutations.
% These observations highlight a critical area for improvement in the design and training of LLMs, particularly in applications requiring sustained attention to initial instructions over extended interactions.

\paragraph{Summary.} The key takeaway is that while current LLMs can handle individual refutations well, they struggle to:
(1) Maintain performance on earlier refutations when new ones are added;
(2) Balance refutation requests with original task requirements.



\begin{table}[t] \small
\centering
\begin{tabular}{lccccccccc}
\toprule
\textbf{Tasks }            & \multicolumn{3}{c}{\textbf{Machine Translation}}                  & \multicolumn{3}{c}{\textbf{Summarization}}                 & \multicolumn{3}{c}{\textbf{Writing}}             \\  \midrule
\textbf{Turn}               & C=0& C=3 & $\Delta$ &  C=0& C=3 & $\Delta$ &  C=0& C=3 & $\Delta$ \\
\midrule
\textbf{GPT-4o}             & 4.69&	4.04&0.65&	4.77&	3.53&\underline{1.24} &	4.82 & 4.16 & 0.66  \\
\textbf{Claude-3.5-Sonnet}  & \textbf{4.83}	&\textbf{4.71}&0.12	&\textbf{4.73}&	\textbf{4.13}&0.60&	4.73 & 4.08 & 0.65 \\
\textbf{Mixtral-8x7B-Inst}  & 4.15 &	3.38&0.77&	4.51&	3.79&0.72&	4.71 & 3.60 & 1.11 \\
\textbf{Qwen-2.5-7B-Inst}&	4.52&	3.20 &\underline{1.32}	&4.68	&3.59 &1.09	&4.78 & \textbf{4.26} & 0.52\\
\textbf{Gemma-2-9B-Inst}&	3.98	&3.17 &0.81	&4.32	&3.54 &0.78	&4.76 & 3.15 & \underline{1.61} \\ 
\textbf{LLaMA-3.1-70B-Inst} & 3.49	&3.34&0.15&	4.75&	3.76&0.99&	\textbf{4.94} & 3.69 & 1.25\\
\bottomrule
\end{tabular}
\caption{The performance on the persistent refutation instructions with different context lengths, where C=0 refers to evaluate the direct response to the refutation and C=3 refers to evaluate the response to the refutation after three irrelevant queries. }
\label{persist_table}
\end{table}

\subsection{Persistent Refutation}
% For persistent refutations, we focus on whether the LLMs could memorize the refutation and implement it when given the orginal task instruction.  The results are shown in Table \ref{persist_table}. All the different LLMs suffer from a forgetting of the persistent refutation information. For example, GPT-4o achieves the 4.69 score when first meeting the persistent refutation but only 4.04 when three other irrelevant queries are given to the model. The model Claude-3.5-Sonnet has the most strong memorization of the refutation information, with the lowest performance delta in the these tasks, 0.12, 0.60 and 0.14 in MT, summarization and writing, respectively. On the contrary, the model Mixtral-8x7B-Inst perform the largest forgetting, showing a weak capacity of the memorization. 
Similar to RefuteBench 1.0, the focus of persistent refutation is on determining whether these models can remember refutation details and effectively implement them when revisiting the original task instruction after distractions. The findings, documented in Table \ref{persist_table}, reveal a common issue among all models: a significant forgetting of persistent refutation information after being subjected to intervening, irrelevant queries. 

For instance, GPT-4o initially scores 4.69 when addressing an immediate persistent refutation, but its performance drops to 4.04 after processing three unrelated queries. This demonstrates a substantial decrease in the model's ability to retain critical refutation information over the course of additional dialog turns.
However, Claude-3.5-Sonnet stands out for its relative strength in memorizing refutation information, exhibiting the smallest performance decline among the tested models. Specifically, the performance deltas for Claude-3.5-Sonnet are 0.12, 0.60, and 0.65 in MT, summarization, and writing, respectively.
% This indicates a stronger capacity for retaining task-specific details and resisting the disruptive effects of irrelevant data.
% Conversely, Mixtral-8x7B-Inst shows the greatest degree of forgetting among the models, indicating a weaker capacity for memorization.
But the performance of the models such as Qwen-2.5-7B-Inst drops significantly with the deltas 1.32, 1.09 and 0.52 in the tasks, respectively. 
The model's significant drop in the performance of LLMs underscores a potential vulnerability in maintaining task continuity and applying learned corrections over extended interactions.
These results suggest that while some models like Claude-3.5-Sonnet are somewhat effective at mitigating the effects of persistent refutations, there is a general challenge among LLMs to maintain consistency to previously refuted content when faced with additional, potentially distracting information. 
% The finding is similar to that in RefuteBench1.0 \citet{yan2024refutebench}, but there are no extremely stubborn LLMs found, which seldom adopt the refutation information. The difference could result from the updated of the LLM versions.
These findings align with those reported in RefuteBench 1.0~\citet{yan2024refutebench}; however, unlike the previous study, we did not observe any extremely stubborn LLMs that consistently rejected refutation information. This difference may be attributed to improvements in newer versions of the LLMs.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\textwidth]{plots/trans_all.png}
    \vspace{-2mm}
    \caption{Attention scores of Qwen-2.5-7B-Inst during the transient refutation dialogues.}
    \label{transient_vis}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\textwidth]{plots/pers_all.png}
    \vspace{-2mm}
    \caption{Attention scores of Qwen-2.5-7B-Inst during the persistent refutation dialogues.}
    \label{persistent_vis}
\end{figure}


\subsection{Analysis}
% To further analyze the reason of the forgetting phenomenon to the refutation and task consistency problem, we delve into the attention scores in the dialogues. Specifically, we apply the max pooling to the attention score across the layers and head and visualize them in a heatmap. We first show the visualization of the dialogues with transient refutation using Qwen-2.5-7B-Inst in Figure \ref{transient_vis}. As we observe, the model pays more attention to the system prompt and the nearby information, but less to the previous information. For example, the attention scores of the final response to the first refutation and its response are nearly zero, which implies the forgetting to refutation information. The attention scores to the query also degrade step by step, explaining the inconsistency to the task. For example, in the machine translation task, the attention scores to the query degrades from 0.21, 0.16 to 0.11 and finally to nearly 0.0. Such a phenomenon implies the weakness of current LLMs in memorizing  previous information during long context dialogues. 
Inspired by previous studies\citep{he-etal-2024-never,xiaoefficient}, we investigate the LLM attention score to analyze the forgetting problem. Different from previous work which is based on words, our analysis is based on rounds of refutation, but the previous ones mostly focused on the word level. 
% To further explore the reasons behind the forgetting phenomenon related to refutations and task consistency, we visualize the attention scores across turns. 
In particular, a random case is selected for each task for both the transient and persistent scenarios. 
We apply max pooling across layers and heads to the attention scores to highlight the focus of the model. 
The resulting heatmaps are shown in Figure \ref{transient_vis} and \ref{persistent_vis}.
Our observations reveal that the model predominantly focuses on the system prompt and nearby information, while largely neglecting earlier information. 
The attention scores of the latter responses towards previous refutations and the initial query gradually diminish.
For the transient scenario, we see the latter rounds mostly focus on the current round of dialogue, while the attention scores for the final response to the first refutation and its response are nearly zero. This explains our findings of forgetfulness over initial tasks and refutations. 
For the persistent scenario, we observe the model's focus is highly distracted by irrelevant tasks, represented by the non-zero attention scores, while only occasionally noticing the refutation and modified responses, explaining the forgetfulness of previously seen refutations. 
% explaining the growing inconsistency with the  initial task and 
% For example, the attention scores for the final response to the first refutation and its response are nearly zero, indicating a significant loss of memory regarding refutation information. Additionally, the attention scores for the query progressively decrease, illustrating the growing inconsistency with the task. 
% For example, in a machine translation task, attention scores to the query progressively drop from 0.21 to 0.16, then to 0.11, and finally to nearly 0.0. 
These patterns highlight a critical weakness in current LLMs: they struggle to retain and correctly use previous information during long context dialogues.




\section{Conclusion}
We introduced RefuteBench 2.0, a refutation evaluation benchmark that extends RefuteBench 1.0 \citep{yan2024refutebench} by incorporating LLM agents as refuters and evaluators, addressing the limitations of static and template-based persistent refutations, and extending with the transient refutation scenario. We introduced a diverse range of dynamic demands, and evaluate models on both transient refutation and persistent refutation, addressing a critical aspect overlooked in the original framework. Human evaluation demonstrated the effectiveness of our LLM-based refuter and evaluator. We evaluated a range of commercial and open-source LLMs, including  GPT-4o, Claude-3.5, Mixtral-8x7B, Qwen-2.5, Gemma-2, and LLaMA-3.1. Results showed that current LLMs can satisfy the refutations, but still suffer from the forgetting problem of the original task instruction and refutation information during the dialog for both transient and persistent ones. 


% ICLR requires electronic submissions, processed by
% \url{https://openreview.net/}. See ICLR's website for more instructions.

% If your paper is ultimately accepted, the statement {\tt
%   {\textbackslash}iclrfinalcopy} should be inserted to adjust the
% format to the camera ready requirements.

% The format for the submissions is a variant of the NeurIPS format.
% Please read carefully the instructions below, and follow them
% faithfully


% \subsection{Style}

% Papers to be submitted to ICLR 2025 must be prepared according to the
% instructions presented here.

% %% Please note that we have introduced automatic line number generation
% %% into the style file for \LaTeXe. This is to help reviewers
% %% refer to specific lines of the paper when they make their comments. Please do
% %% NOT refer to these line numbers in your paper as they will be removed from the
% %% style file for the final version of accepted papers.

% Authors are required to use the ICLR \LaTeX{} style files obtainable at the
% ICLR website. Please make sure you use the current files and
% not previous versions. Tweaking the style files may be grounds for rejection.

% \subsection{Retrieval of style files}

% The style files for ICLR and other conference information are available online at:
% \begin{center}
%    \url{http://www.iclr.cc/}
% \end{center}
% The file \verb+iclr2025_conference.pdf+ contains these
% instructions and illustrates the
% various formatting requirements your ICLR paper must satisfy.
% Submissions must be made using \LaTeX{} and the style files
% \verb+iclr2025_conference.sty+ and \verb+iclr2025_conference.bst+ (to be used with \LaTeX{}2e). The file
% \verb+iclr2025_conference.tex+ may be used as a ``shell'' for writing your paper. All you
% have to do is replace the author, title, abstract, and text of the paper with
% your own.

% The formatting instructions contained in these style files are summarized in
% sections \ref{gen_inst}, \ref{headings}, and \ref{others} below.

% \section{General formatting instructions}
% \label{gen_inst}

% The text must be confined within a rectangle 5.5~inches (33~picas) wide and
% 9~inches (54~picas) long. The left margin is 1.5~inch (9~picas).
% Use 10~point type with a vertical spacing of 11~points. Times New Roman is the
% preferred typeface throughout. Paragraphs are separated by 1/2~line space,
% with no indentation.

% Paper title is 17~point, in small caps and left-aligned.
% All pages should start at 1~inch (6~picas) from the top of the page.

% Authors' names are
% set in boldface, and each name is placed above its corresponding
% address. The lead author's name is to be listed first, and
% the co-authors' names are set to follow. Authors sharing the
% same address can be on the same line.

% Please pay special attention to the instructions in section \ref{others}
% regarding figures, tables, acknowledgments, and references.


% There will be a strict upper limit of 10 pages for the main text of the initial submission, with unlimited additional pages for citations. 

% \section{Headings: first level}
% \label{headings}

% First level headings are in small caps,
% flush left and in point size 12. One line space before the first level
% heading and 1/2~line space after the first level heading.

% \subsection{Headings: second level}

% Second level headings are in small caps,
% flush left and in point size 10. One line space before the second level
% heading and 1/2~line space after the second level heading.

% \subsubsection{Headings: third level}

% Third level headings are in small caps,
% flush left and in point size 10. One line space before the third level
% heading and 1/2~line space after the third level heading.

% \section{Citations, figures, tables, references}
% \label{others}

% These instructions apply to everyone, regardless of the formatter being used.

% \subsection{Citations within the text}

% Citations within the text should be based on the \texttt{natbib} package
% and include the authors' last names and year (with the ``et~al.'' construct
% for more than two authors). When the authors or the publication are
% included in the sentence, the citation should not be in parenthesis using \verb|\citet{}| (as
% in ``See \citet{Hinton06} for more information.''). Otherwise, the citation
% should be in parenthesis using \verb|\citep{}| (as in ``Deep learning shows promise to make progress
% towards AI~\citep{Bengio+chapter2007}.'').

% The corresponding references are to be listed in alphabetical order of
% authors, in the \textsc{References} section. As to the format of the
% references themselves, any style is acceptable as long as it is used
% consistently.

% \subsection{Footnotes}

% Indicate footnotes with a number\footnote{Sample of the first footnote} in the
% text. Place the footnotes at the bottom of the page on which they appear.
% Precede the footnote with a horizontal rule of 2~inches
% (12~picas).\footnote{Sample of the second footnote}

% \subsection{Figures}

% All artwork must be neat, clean, and legible. Lines should be dark
% enough for purposes of reproduction; art work should not be
% hand-drawn. The figure number and caption always appear after the
% figure. Place one line space before the figure caption, and one line
% space after the figure. The figure caption is lower case (except for
% first word and proper nouns); figures are numbered consecutively.

% Make sure the figure caption does not get separated from the figure.
% Leave sufficient space to avoid splitting the figure and figure caption.

% You may use color figures.
% However, it is best for the
% figure captions and the paper body to make sense if the paper is printed
% either in black/white or in color.
% \begin{figure}[h]
% \begin{center}
% %\framebox[4.0in]{$\;$}
% \fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
% \end{center}
% \caption{Sample figure caption.}
% \end{figure}



% \begin{figure}[h]
%     \centering
%     \begin{subfigure}[b]{0.48\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{plots/Instance-specific.pdf}
%         \caption{Caption for first figure}
%         \label{fig:distribution_keys}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.48\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{plots/Persistent.pdf}
%         \caption{Caption for second figure}
%         \label{fig:pca_keys}
%     \end{subfigure}
%     \caption{Change this two figure for better readability. }
%     \label{fig:two_figures}
% \end{figure}


% \subsection{Tables}

% All tables must be centered, neat, clean and legible. Do not use hand-drawn
% tables. The table number and title always appear before the table. See
% Table~\ref{sample-table}.

% Place one line space before the table title, one line space after the table
% title, and one line space after the table. The table title must be lower case
% (except for first word and proper nouns); tables are numbered consecutively.

% \begin{table}[t]
% \caption{Sample table title}
% \label{sample-table}
% \begin{center}
% \begin{tabular}{ll}
% \multicolumn{1}{c}{\bf PART}  &\multicolumn{1}{c}{\bf DESCRIPTION}
% \\ \hline \\
% Dendrite         &Input terminal \\
% Axon             &Output terminal \\
% Soma             &Cell body (contains cell nucleus) \\
% \end{tabular}
% \end{center}
% \end{table}

% \section{Default Notation}

% In an attempt to encourage standardized notation, we have included the
% notation file from the textbook, \textit{Deep Learning}
% \cite{goodfellow2016deep} available at
% \url{https://github.com/goodfeli/dlbook_notation/}.  Use of this style
% is not required and can be disabled by commenting out
% \texttt{math\_commands.tex}.


% \centerline{\bf Numbers and Arrays}
% \bgroup
% \def\arraystretch{1.5}
% \begin{tabular}{p{1in}p{3.25in}}
% $\displaystyle a$ & A scalar (integer or real)\\
% $\displaystyle \va$ & A vector\\
% $\displaystyle \mA$ & A matrix\\
% $\displaystyle \tA$ & A tensor\\
% $\displaystyle \mI_n$ & Identity matrix with $n$ rows and $n$ columns\\
% $\displaystyle \mI$ & Identity matrix with dimensionality implied by context\\
% $\displaystyle \ve^{(i)}$ & Standard basis vector $[0,\dots,0,1,0,\dots,0]$ with a 1 at position $i$\\
% $\displaystyle \text{diag}(\va)$ & A square, diagonal matrix with diagonal entries given by $\va$\\
% $\displaystyle \ra$ & A scalar random variable\\
% $\displaystyle \rva$ & A vector-valued random variable\\
% $\displaystyle \rmA$ & A matrix-valued random variable\\
% \end{tabular}
% \egroup
% \vspace{0.25cm}

% \centerline{\bf Sets and Graphs}
% \bgroup
% \def\arraystretch{1.5}

% \begin{tabular}{p{1.25in}p{3.25in}}
% $\displaystyle \sA$ & A set\\
% $\displaystyle \R$ & The set of real numbers \\
% $\displaystyle \{0, 1\}$ & The set containing 0 and 1 \\
% $\displaystyle \{0, 1, \dots, n \}$ & The set of all integers between $0$ and $n$\\
% $\displaystyle [a, b]$ & The real interval including $a$ and $b$\\
% $\displaystyle (a, b]$ & The real interval excluding $a$ but including $b$\\
% $\displaystyle \sA \backslash \sB$ & Set subtraction, i.e., the set containing the elements of $\sA$ that are not in $\sB$\\
% $\displaystyle \gG$ & A graph\\
% $\displaystyle \parents_\gG(\ervx_i)$ & The parents of $\ervx_i$ in $\gG$
% \end{tabular}
% \vspace{0.25cm}


% \centerline{\bf Indexing}
% \bgroup
% \def\arraystretch{1.5}

% \begin{tabular}{p{1.25in}p{3.25in}}
% $\displaystyle \eva_i$ & Element $i$ of vector $\va$, with indexing starting at 1 \\
% $\displaystyle \eva_{-i}$ & All elements of vector $\va$ except for element $i$ \\
% $\displaystyle \emA_{i,j}$ & Element $i, j$ of matrix $\mA$ \\
% $\displaystyle \mA_{i, :}$ & Row $i$ of matrix $\mA$ \\
% $\displaystyle \mA_{:, i}$ & Column $i$ of matrix $\mA$ \\
% $\displaystyle \etA_{i, j, k}$ & Element $(i, j, k)$ of a 3-D tensor $\tA$\\
% $\displaystyle \tA_{:, :, i}$ & 2-D slice of a 3-D tensor\\
% $\displaystyle \erva_i$ & Element $i$ of the random vector $\rva$ \\
% \end{tabular}
% \egroup
% \vspace{0.25cm}


% \centerline{\bf Calculus}
% \bgroup
% \def\arraystretch{1.5}
% \begin{tabular}{p{1.25in}p{3.25in}}
% % NOTE: the [2ex] on the next line adds extra height to that row of the table.
% % Without that command, the fraction on the first line is too tall and collides
% % with the fraction on the second line.
% $\displaystyle\frac{d y} {d x}$ & Derivative of $y$ with respect to $x$\\ [2ex]
% $\displaystyle \frac{\partial y} {\partial x} $ & Partial derivative of $y$ with respect to $x$ \\
% $\displaystyle \nabla_\vx y $ & Gradient of $y$ with respect to $\vx$ \\
% $\displaystyle \nabla_\mX y $ & Matrix derivatives of $y$ with respect to $\mX$ \\
% $\displaystyle \nabla_\tX y $ & Tensor containing derivatives of $y$ with respect to $\tX$ \\
% $\displaystyle \frac{\partial f}{\partial \vx} $ & Jacobian matrix $\mJ \in \R^{m\times n}$ of $f: \R^n \rightarrow \R^m$\\
% $\displaystyle \nabla_\vx^2 f(\vx)\text{ or }\mH( f)(\vx)$ & The Hessian matrix of $f$ at input point $\vx$\\
% $\displaystyle \int f(\vx) d\vx $ & Definite integral over the entire domain of $\vx$ \\
% $\displaystyle \int_\sS f(\vx) d\vx$ & Definite integral with respect to $\vx$ over the set $\sS$ \\
% \end{tabular}
% \egroup
% \vspace{0.25cm}

% \centerline{\bf Probability and Information Theory}
% \bgroup
% \def\arraystretch{1.5}
% \begin{tabular}{p{1.25in}p{3.25in}}
% $\displaystyle P(\ra)$ & A probability distribution over a discrete variable\\
% $\displaystyle p(\ra)$ & A probability distribution over a continuous variable, or over
% a variable whose type has not been specified\\
% $\displaystyle \ra \sim P$ & Random variable $\ra$ has distribution $P$\\% so thing on left of \sim should always be a random variable, with name beginning with \r
% $\displaystyle  \E_{\rx\sim P} [ f(x) ]\text{ or } \E f(x)$ & Expectation of $f(x)$ with respect to $P(\rx)$ \\
% $\displaystyle \Var(f(x)) $ &  Variance of $f(x)$ under $P(\rx)$ \\
% $\displaystyle \Cov(f(x),g(x)) $ & Covariance of $f(x)$ and $g(x)$ under $P(\rx)$\\
% $\displaystyle H(\rx) $ & Shannon entropy of the random variable $\rx$\\
% $\displaystyle \KL ( P \Vert Q ) $ & Kullback-Leibler divergence of P and Q \\
% $\displaystyle \mathcal{N} ( \vx ; \vmu , \mSigma)$ & Gaussian distribution %
% over $\vx$ with mean $\vmu$ and covariance $\mSigma$ \\
% \end{tabular}
% \egroup
% \vspace{0.25cm}

% \centerline{\bf Functions}
% \bgroup
% \def\arraystretch{1.5}
% \begin{tabular}{p{1.25in}p{3.25in}}
% $\displaystyle f: \sA \rightarrow \sB$ & The function $f$ with domain $\sA$ and range $\sB$\\
% $\displaystyle f \circ g $ & Composition of the functions $f$ and $g$ \\
%   $\displaystyle f(\vx ; \vtheta) $ & A function of $\vx$ parametrized by $\vtheta$.
%   (Sometimes we write $f(\vx)$ and omit the argument $\vtheta$ to lighten notation) \\
% $\displaystyle \log x$ & Natural logarithm of $x$ \\
% $\displaystyle \sigma(x)$ & Logistic sigmoid, $\displaystyle \frac{1} {1 + \exp(-x)}$ \\
% $\displaystyle \zeta(x)$ & Softplus, $\log(1 + \exp(x))$ \\
% $\displaystyle || \vx ||_p $ & $\normlp$ norm of $\vx$ \\
% $\displaystyle || \vx || $ & $\normltwo$ norm of $\vx$ \\
% $\displaystyle x^+$ & Positive part of $x$, i.e., $\max(0,x)$\\
% $\displaystyle \1_\mathrm{condition}$ & is 1 if the condition is true, 0 otherwise\\
% \end{tabular}
% \egroup
% \vspace{0.25cm}



% \section{Final instructions}
% Do not change any aspects of the formatting parameters in the style files.
% In particular, do not modify the width or length of the rectangle the text
% should fit into, and do not change font sizes (except perhaps in the
% \textsc{References} section; see below). Please note that pages should be
% numbered.

% \section{Preparing PostScript or PDF files}

% Please prepare PostScript or PDF files with paper size ``US Letter'', and
% not, for example, ``A4''. The -t
% letter option on dvips will produce US Letter files.

% Consider directly generating PDF files using \verb+pdflatex+
% (especially if you are a MiKTeX user).
% PDF figures must be substituted for EPS figures, however.

% Otherwise, please generate your PostScript and PDF files with the following commands:
% \begin{verbatim}
% dvips mypaper.dvi -t letter -Ppdf -G0 -o mypaper.ps
% ps2pdf mypaper.ps mypaper.pdf
% \end{verbatim}

% \subsection{Margins in LaTeX}

% Most of the margin problems come from figures positioned by hand using
% \verb+\special+ or other commands. We suggest using the command
% \verb+\includegraphics+
% from the graphicx package. Always specify the figure width as a multiple of
% the line width as in the example below using .eps graphics
% \begin{verbatim}
%    \usepackage[dvips]{graphicx} ...
%    \includegraphics[width=0.8\linewidth]{myfile.eps}
% \end{verbatim}
% or % Apr 2009 addition
% \begin{verbatim}
%    \usepackage[pdftex]{graphicx} ...
%    \includegraphics[width=0.8\linewidth]{myfile.pdf}
% \end{verbatim}
% for .pdf graphics.
% See section~4.4 in the graphics bundle documentation (\url{http://www.ctan.org/tex-archive/macros/latex/required/graphics/grfguide.ps})

% A number of width problems arise when LaTeX cannot properly hyphenate a
% line. Please give LaTeX hyphenation hints using the \verb+\-+ command.

\clearpage

% \subsubsection*{Author Contributions}
% If you'd like to, you may include  a section for author contributions as is done
% in many journals. This is optional and at the discretion of the authors.

\subsubsection*{Acknowledgments}
This work has been financially supported by the National Key R\&D program of China No. 2022YFE0204900.


\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}

% \appendix
% \section{Appendix}
% You may include other additional sections here.


\end{document}
