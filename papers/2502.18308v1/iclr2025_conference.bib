@incollection{Bengio+chapter2007,
author = {Bengio, Yoshua and LeCun, Yann},
booktitle = {Large Scale Kernel Machines},
publisher = {MIT Press},
title = {Scaling Learning Algorithms Towards {AI}},
year = {2007}
}

@article{Hinton06,
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
journal = {Neural Computation},
pages = {1527--1554},
title = {A Fast Learning Algorithm for Deep Belief Nets},
volume = {18},
year = {2006}
}

@book{goodfellow2016deep,
title={Deep learning},
author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
volume={1},
year={2016},
publisher={MIT Press}
}

@inproceedings{bernard2024identifying,
  title={Identifying Breakdowns in Conversational Recommender Systems using User Simulation},
  author={Bernard, Nolwenn and Balog, Krisztian},
  booktitle={Proceedings of the 6th ACM Conference on Conversational User Interfaces},
  pages={1--10},
  year={2024}
}

@article{fang2024multi,
  title={A multi-agent conversational recommender system},
  author={Fang, Jiabao and Gao, Shen and Ren, Pengjie and Chen, Xiuying and Verberne, Suzan and Ren, Zhaochun},
  journal={arXiv preprint arXiv:2402.01135},
  year={2024}
}

@inproceedings{wang2023rethinking,
  title={Rethinking the Evaluation for Conversational Recommendation in the Era of Large Language Models},
  author={Wang, Xiaolei and Tang, Xinyu and Zhao, Xin and Wang, Jingyuan and Wen, Ji-Rong},
  booktitle={The 2023 Conference on Empirical Methods in Natural Language Processing}
}

@article{gusev2024pingpong,
  title={PingPong: A Benchmark for Role-Playing Language Models with User Emulation and Multi-Model Evaluation},
  author={Gusev, Ilya},
  journal={arXiv preprint arXiv:2409.06820},
  year={2024}
}

@inproceedings{park2024llm,
  title={Do LLM Agents Have Regret? A Case Study in Online Learning and Games},
  author={Park, Chanwoo and Liu, Xiangyu and Ozdaglar, Asuman E and Zhang, Kaiqing},
  booktitle={ICLR 2024 Workshop on Large Language Model (LLM) Agents},
  year={2024}
}

@article{yan2024refutebench,
  title={RefuteBench: Evaluating Refuting Instruction-Following for Large Language Models},
  author={Yan, Jianhao and Luo, Yun and Zhang, Yue},
  journal={arXiv preprint arXiv:2402.13463},
  year={2024}
}

@article{Bender2021,
author = {Bender, Emily M. and Gebru, Timnit and McMillan Major, Angelina and Shmitchell, Shmargaret},
    title = {On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? },
    year = {2021},
    isbn = {9781450383097},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3442188.3445922},
    doi = {10.1145/3442188.3445922},
    booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
    pages = {610\-623},
    numpages = {14},
    location = {Virtual Event, Canada}
}


@article{zhu2024dyval,
  title={Dyval 2: Dynamic evaluation of large language models by meta probing agents},
  author={Zhu, Kaijie and Wang, Jindong and Zhao, Qinlin and Xu, Ruochen and Xie, Xing},
  journal={arXiv preprint arXiv:2402.14865},
  year={2024}
}

@article{burnell2023revealing,
  title={Revealing the structure of language model capabilities},
  author={Burnell, Ryan and Hao, Han and Conway, Andrew RA and Orallo, Jose Hernandez},
  journal={arXiv preprint arXiv:2306.10062},
  year={2023}
}


@misc{instruction-follow-verbalizer,
      title={Instruction-following Evaluation through Verbalizer Manipulation}, 
      author={Shiyang Li and Jun Yan and Hai Wang and Zheng Tang and Xiang Ren and Vijay Srinivasan and Hongxia Jin},
      year={2023},
      eprint={2307.10558},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@inproceedings{instruction-follow-summ,
    title = "Towards Better Evaluation of Instruction-Following: A Case-Study in Summarization",
    author = "Skopek, Ondrej  and
      Aralikatte, Rahul  and
      Gooding, Sian  and
      Carbune, Victor",
    editor = "Jiang, Jing  and
      Reitter, David  and
      Deng, Shumin",
    booktitle = "Proceedings of the 27th Conference on Computational Natural Language Learning (CoNLL)",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.conll-1.16",
    doi = "10.18653/v1/2023.conll-1.16",
    pages = "221--237",
    abstract = "Despite recent advances, evaluating how well large language models (LLMs) follow user instructions remains an open problem. While evaluation methods of language models have seen a rise in prompt-based approaches, limited work on the correctness of these methods has been conducted. In this work, we perform a meta-evaluation of a variety of metrics to quantify how accurately they measure the instruction-following abilities of LLMs. Our investigation is performed on grounded query-based summarization by collecting a new short-form, real-world dataset riSum, containing 300 document-instruction pairs with 3 answers each. All 900 answers are rated by 3 human annotators. Using riSum, we analyze the agreement between evaluation methods and human judgment. Finally, we propose new LLM-based reference-free evaluation methods that improve upon established baselines and perform on par with costly reference-based metrics that require high-quality summaries.",
}

@misc{instruct-gpt,
      title={Training language models to follow instructions with human feedback}, 
      author={Long Ouyang and Jeff Wu and Xu Jiang and Diogo Almeida and Carroll L. Wainwright and Pamela Mishkin and Chong Zhang and Sandhini Agarwal and Katarina Slama and Alex Ray and John Schulman and Jacob Hilton and Fraser Kelton and Luke Miller and Maddie Simens and Amanda Askell and Peter Welinder and Paul Christiano and Jan Leike and Ryan Lowe},
      year={2022},
      eprint={2203.02155},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{touvron2023llama,
      title={Llama 2: Open Foundation and Fine-Tuned Chat Models}, 
      author={Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and Dan Bikel and Lukas Blecher and Cristian Canton Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel Kloumann and Artem Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and Ranjan Subramanian and Xiaoqing Ellen Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zheng Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom},
      year={2023},
      eprint={2307.09288},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{alpaca,
  author = {Rohan Taori and Ishaan Gulrajani and Tianyi Zhang and Yann Dubois and Xuechen Li and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto },
  title = {Stanford Alpaca: An Instruction-following LLaMA model},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/tatsu-lab/stanford_alpaca}},
}

@inproceedings{
self-check,
title={SelfCheck: Using {LLM}s to Zero-Shot Check Their Own Step-by-Step Reasoning},
author={Ning Miao and Yee Whye Teh and Tom Rainforth},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=pTHfApDakA}
}

@inproceedings{
cannot-self-correct-yet,
title={Large Language Models Cannot Self-Correct Reasoning Yet},
author={Jie Huang and Xinyun Chen and Swaroop Mishra and Huaixiu Steven Zheng and Adams Wei Yu and Xinying Song and Denny Zhou},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=IkmD3fKBPQ}
}
@inproceedings{
self-refine,
title={Self-Refine: Iterative Refinement with Self-Feedback},
author={Aman Madaan and Niket Tandon and Prakhar Gupta and Skyler Hallinan and Luyu Gao and Sarah Wiegreffe and Uri Alon and Nouha Dziri and Shrimai Prabhumoye and Yiming Yang and Shashank Gupta and Bodhisattwa Prasad Majumder and Katherine Hermann and Sean Welleck and Amir Yazdanbakhsh and Peter Clark},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=S37hOerQLB}
}


@inproceedings{self-improve,
    title = "Large Language Models Can Self-Improve",
    author = "Huang, Jiaxin  and
      Gu, Shixiang  and
      Hou, Le  and
      Wu, Yuexin  and
      Wang, Xuezhi  and
      Yu, Hongkun  and
      Han, Jiawei",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.67",
    doi = "10.18653/v1/2023.emnlp-main.67",
    pages = "1051--1068",
    abstract = "Large Language Models (LLMs) have achieved excellent performances in various tasks. However, fine-tuning an LLM requires extensive supervision. Human, on the other hand, may improve their reasoning abilities by self-thinking without external inputs. In this work, we demonstrate that an LLM is also capable of self-improving with only unlabeled datasets. We use a pre-trained LLM to generate {``}high-confidence{''} rationale-augmented answers for unlabeled questions using Chain-of-Though (CoT) prompting and self-consistency, and fine-tune the LLM using those self-generated solutions as target outputs. We show that without any ground truth label, our approach improves the general reasoning ability of a 540B-parameter LLM (74.4{\%}$\rightarrow$82.1{\%} on GSM8K, 90.0{\%}$\rightarrow$94.4{\%} on OpenBookQA, and 63.4{\%}$\rightarrow$67.9{\%} on ANLI-A3) and can also be adapted to extreme low-resource cases where even training questions and CoT prompts are limited. We conduct ablation studies and show that fine-tuning on diverse reasoning paths is critical for self-improvement.",
}

@misc{chat-dev,
      title={Communicative Agents for Software Development}, 
      author={Chen Qian and Xin Cong and Wei Liu and Cheng Yang and Weize Chen and Yusheng Su and Yufan Dang and Jiahao Li and Juyuan Xu and Dahai Li and Zhiyuan Liu and Maosong Sun},
      year={2023},
      eprint={2307.07924},
      archivePrefix={arXiv},
      primaryClass={cs.SE}
}


@article{chen2024sifo,
  title={The SIFo Benchmark: Investigating the Sequential Instruction Following Ability of Large Language Models},
  author={Chen, Xinyi and Liao, Baohao and Qi, Jirui and Eustratiadis, Panagiotis and Monz, Christof and Bisazza, Arianna and de Rijke, Maarten},
  journal={arXiv preprint arXiv:2406.19999},
  year={2024}
}



@article{he2024complex,
  title={From Complex to Simple: Enhancing Multi-Constraint Complex Instruction Following Ability of Large Language Models},
  author={He, Qianyu and Zeng, Jie and He, Qianxi and Liang, Jiaqing and Xiao, Yanghua},
  journal={arXiv preprint arXiv:2404.15846},
  year={2024}
}

@article{wang2023mint,
  title={Mint: Evaluating llms in multi-turn interaction with tools and language feedback},
  author={Wang, Xingyao and Wang, Zihan and Liu, Jiateng and Chen, Yangyi and Yuan, Lifan and Peng, Hao and Ji, Heng},
  journal={arXiv preprint arXiv:2309.10691},
  year={2023}
}

@article{fan2023nphardeval,
  title={Nphardeval: Dynamic benchmark on reasoning ability of large language models via complexity classes},
  author={Fan, Lizhou and Hua, Wenyue and Li, Lingyao and Ling, Haoyang and Zhang, Yongfeng and Hemphill, Libby},
  journal={arXiv preprint arXiv:2312.14890},
  year={2023}
}

@article{zhou2023instruction,
  title={Instruction-following evaluation for large language models},
  author={Zhou, Jeffrey and Lu, Tianjian and Mishra, Swaroop and Brahma, Siddhartha and Basu, Sujoy and Luan, Yi and Zhou, Denny and Hou, Le},
  journal={arXiv preprint arXiv:2311.07911},
  year={2023}
}

@article{clark2018think,
  title={Think you have solved question answering? try arc, the ai2 reasoning challenge},
  author={Clark, Peter and Cowhey, Isaac and Etzioni, Oren and Khot, Tushar and Sabharwal, Ashish and Schoenick, Carissa and Tafjord, Oyvind},
  journal={arXiv preprint arXiv:1803.05457},
  year={2018}
}

@inproceedings{suzgun2023challenging,
  title={Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them},
  author={Suzgun, Mirac and Scales, Nathan and Sch{\"a}rli, Nathanael and Gehrmann, Sebastian and Tay, Yi and Chung, Hyung Won and Chowdhery, Aakanksha and Le, Quoc and Chi, Ed and Zhou, Denny and others},
  booktitle={Findings of the Association for Computational Linguistics: ACL 2023},
  pages={13003--13051},
  year={2023}
}

@article{cobbe2021training,
  title={Training verifiers to solve math word problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and others},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}

@inproceedings{hendrycksmeasuring,
  title={Measuring Massive Multitask Language Understanding},
  author={Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
  booktitle={International Conference on Learning Representations}
}

@article{zhu2023dyval,
  title={Dyval: Graph-informed dynamic evaluation of large language models},
  author={Zhu, Kaijie and Chen, Jiaao and Wang, Jindong and Gong, Neil Zhenqiang and Yang, Diyi and Xie, Xing},
  journal={arXiv preprint arXiv:2309.17167},
  year={2023}
}


@article{kocon2023chatgpt,
  title={ChatGPT: Jack of all trades, master of none},
  author={Koco{\'n}, Jan and Cichecki, Igor and Kaszyca, Oliwier and Kochanek, Mateusz and Szydlo, Dominika and Baran, Joanna and Bielaniewicz, Julita and Gruza, Marcin and Janz, Arkadiusz and Kanclerz, Kamil and others},
  journal={Information Fusion},
  volume={99},
  pages={101861},
  year={2023},
  publisher={Elsevier}
}



@inproceedings{bang-etal-2023-multitask,
    title = "A Multitask, Multilingual, Multimodal Evaluation of {C}hat{GPT} on Reasoning, Hallucination, and Interactivity",
    author = "Bang, Yejin  and
      Cahyawijaya, Samuel  and
      Lee, Nayeon  and
      Dai, Wenliang  and
      Su, Dan  and
      Wilie, Bryan  and
      Lovenia, Holy  and
      Ji, Ziwei  and
      Yu, Tiezheng  and
      Chung, Willy  and
      Do, Quyet V.  and
      Xu, Yan  and
      Fung, Pascale",
    editor = "Park, Jong C.  and
      Arase, Yuki  and
      Hu, Baotian  and
      Lu, Wei  and
      Wijaya, Derry  and
      Purwarianti, Ayu  and
      Krisnadhi, Adila Alfa",
    booktitle = "Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = nov,
    year = "2023",
    address = "Nusa Dua, Bali",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.ijcnlp-main.45",
    doi = "10.18653/v1/2023.ijcnlp-main.45",
    pages = "675--718",
}

@article{xsum,
  title={Don't give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization},
  author={Narayan, Shashi and Cohen, Shay B and Lapata, Mirella},
  journal={arXiv preprint arXiv:1808.08745},
  year={2018}
}

@inproceedings{cmv,
 author = {Chenhao Tan and
Vlad Niculae and
Cristian Danescu{-}Niculescu{-}Mizil and
Lillian Lee},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/www/TanNDL16.bib},
 booktitle = {Proceedings of the 25th International Conference on World Wide Web,
{WWW} 2016, Montreal, Canada, April 11 - 15, 2016},
 doi = {10.1145/2872427.2883081},
 editor = {Jacqueline Bourdeau and
Jim Hendler and
Roger Nkambou and
Ian Horrocks and
Ben Y. Zhao},
 pages = {613--624},
 publisher = {{ACM}},
 timestamp = {Sun, 25 Oct 2020 01:00:00 +0200},
 title = {Winning Arguments: Interaction Dynamics and Persuasion Strategies
in Good-faith Online Discussions},
 url = {https://doi.org/10.1145/2872427.2883081},
 year = {2016}
}

@inproceedings{yelp,
 author = {Xiang Zhang and
Junbo Jake Zhao and
Yann LeCun},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/nips/ZhangZL15.bib},
 booktitle = {Advances in Neural Information Processing Systems 28: Annual Conference
on Neural Information Processing Systems 2015, December 7-12, 2015,
Montreal, Quebec, Canada},
 editor = {Corinna Cortes and
Neil D. Lawrence and
Daniel D. Lee and
Masashi Sugiyama and
Roman Garnett},
 pages = {649--657},
 timestamp = {Thu, 21 Jan 2021 00:00:00 +0100},
 title = {Character-level Convolutional Networks for Text Classification},
 url = {https://proceedings.neurips.cc/paper/2015/hash/250cf8b51c773f3f8dc8b4be867a9a02-Abstract.html},
 year = {2015}
}


@article{jiang2024mixtralexperts,
  title={Mixtral of experts},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Roux, Antoine and Mensch, Arthur and Savary, Blanche and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Hanna, Emma Bou and Bressand, Florian and others},
  journal={arXiv preprint arXiv:2401.04088},
  year={2024}
}

@misc{qwen2.5,
    title = {Qwen2.5: A Party of Foundation Models},
    url = {https://qwenlm.github.io/blog/qwen2.5/},
    author = {{Qwen Team}},
    month = {September},
    year = {2024}
}

@inproceedings{he-etal-2024-never,
    title = "Never Lost in the Middle: Mastering Long-Context Question Answering with Position-Agnostic Decompositional Training",
    author = "He, Junqing  and
      Pan, Kunhao  and
      Dong, Xiaoqun  and
      Song, Zhuoyang  and
      LiuYiBo, LiuYiBo  and
      Qianguosun, Qianguosun  and
      Liang, Yuxin  and
      Wang, Hao  and
      Zhang, Enming  and
      Zhang, Jiaxing",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.736",
    doi = "10.18653/v1/2024.acl-long.736",
    pages = "13628--13642",
    abstract = "While large language models (LLMs) are equipped with longer text input capabilities than before, they are struggling to seek correct information in long contexts. The {``}lost in the middle{''} problem challenges most LLMs, referring to the dramatic decline in accuracy when correct information is located in the middle. To overcome this crucial issue, this paper proposes to enhance the information searching and reflection ability of LLMs in long contexts via specially designed tasks called Position-Agnostic Multi-step QA (PAM QA). Trained in this task, our model excels in focusing more precisely on the desired information. Experimental results show substantial improvement in Multi-doc QA and other benchmarks, superior to state-of-the-art models by 13.7{\%} absolute gain in shuffled settings, by 21.5{\%} in passage retrieval task. We release our model and code to promote related research in the community.",
}

he-etal-2024-never,xiaoefficient

@inproceedings{xiaoefficient,
  title={Efficient Streaming Language Models with Attention Sinks},
  author={Xiao, Guangxuan and Tian, Yuandong and Chen, Beidi and Han, Song and Lewis, Mike},
  booktitle={The Twelfth International Conference on Learning Representations}
}

@inproceedings{wp,
 address = {Melbourne, Australia},
 author = {Fan, Angela  and
Lewis, Mike  and
Dauphin, Yann},
 booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
 doi = {10.18653/v1/P18-1082},
 pages = {889--898},
 publisher = {Association for Computational Linguistics},
 title = {Hierarchical Neural Story Generation},
 url = {https://aclanthology.org/P18-1082},
 year = {2018}
}

@inproceedings{roc,
 address = {San Diego, California},
 author = {Mostafazadeh, Nasrin  and
Chambers, Nathanael  and
He, Xiaodong  and
Parikh, Devi  and
Batra, Dhruv  and
Vanderwende, Lucy  and
Kohli, Pushmeet  and
Allen, James},
 booktitle = {Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies},
 doi = {10.18653/v1/N16-1098},
 pages = {839--849},
 publisher = {Association for Computational Linguistics},
 title = {A Corpus and Cloze Evaluation for Deeper Understanding of Commonsense Stories},
 url = {https://aclanthology.org/N16-1098},
 year = {2016}
}

@inproceedings{hswag,
 address = {Florence, Italy},
 author = {Zellers, Rowan  and
Holtzman, Ari  and
Bisk, Yonatan  and
Farhadi, Ali  and
Choi, Yejin},
 booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
 doi = {10.18653/v1/P19-1472},
 pages = {4791--4800},
 publisher = {Association for Computational Linguistics},
 title = {{H}ella{S}wag: Can a Machine Really Finish Your Sentence?},
 url = {https://aclanthology.org/P19-1472},
 year = {2019}
}

@inproceedings{squad,
 address = {Austin, Texas},
 author = {Rajpurkar, Pranav  and
Zhang, Jian  and
Lopyrev, Konstantin  and
Liang, Percy},
 booktitle = {Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing},
 doi = {10.18653/v1/D16-1264},
 pages = {2383--2392},
 publisher = {Association for Computational Linguistics},
 title = {{SQ}u{AD}: 100,000+ Questions for Machine Comprehension of Text},
 url = {https://aclanthology.org/D16-1264},
 year = {2016}
}

@article{scigen,
 author = {Nafise Sadat Moosavi and
Andreas R{\"{u}}ckl{\'{e}} and
Dan Roth and
Iryna Gurevych},
 journal = {ArXiv preprint},
 title = {Learning to Reason for Text Generation from Scientific Tables},
 url = {https://arxiv.org/abs/2104.08296},
 volume = {abs/2104.08296},
 year = {2021}
}

@inproceedings{eli5,
 address = {Florence, Italy},
 author = {Fan, Angela  and
Jernite, Yacine  and
Perez, Ethan  and
Grangier, David  and
Weston, Jason  and
Auli, Michael},
 booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
 doi = {10.18653/v1/P19-1346},
 pages = {3558--3567},
 publisher = {Association for Computational Linguistics},
 title = {{ELI}5: Long Form Question Answering},
 url = {https://aclanthology.org/P19-1346},
 year = {2019}
}

@inproceedings{stanza,
 address = {Online},
 author = {Qi, Peng  and
Zhang, Yuhao  and
Zhang, Yuhui  and
Bolton, Jason  and
Manning, Christopher D.},
 booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations},
 doi = {10.18653/v1/2020.acl-demos.14},
 pages = {101--108},
 publisher = {Association for Computational Linguistics},
 title = {{S}tanza: A Python Natural Language Processing Toolkit for Many Human Languages},
 url = {https://aclanthology.org/2020.acl-demos.14},
 year = {2020}
}

@inproceedings{rec1,
 address = {Atlanta, Georgia, USA},
 author = {Nakov, Preslav  and
Rosenthal, Sara  and
Kozareva, Zornitsa  and
Stoyanov, Veselin  and
Ritter, Alan  and
Wilson, Theresa},
 booktitle = {Second Joint Conference on Lexical and Computational Semantics (*{SEM}), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation ({S}em{E}val 2013)},
 pages = {312--320},
 publisher = {Association for Computational Linguistics},
 title = {{S}em{E}val-2013 Task 2: Sentiment Analysis in {T}witter},
 url = {https://aclanthology.org/S13-2052},
 year = {2013}
}

@inproceedings{mage,
    title = "{MAGE}: Machine-generated Text Detection in the Wild",
    author = "Li, Yafu  and
      Li, Qintong  and
      Cui, Leyang  and
      Bi, Wei  and
      Wang, Zhilin  and
      Wang, Longyue  and
      Yang, Linyi  and
      Shi, Shuming  and
      Zhang, Yue",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.3",
    doi = "10.18653/v1/2024.acl-long.3",
    pages = "36--53",
    abstract = "Large language models (LLMs) have achieved human-level text generation, emphasizing the need for effective deepfake text detection to mitigate risks like the spread of fake news and plagiarism. Existing research has been constrained by evaluating detection methods o specific domains or particular language models. In practical scenarios, however, the detector faces texts from various domains or LLMs without knowing their sources. To this end, we build a comprehensive testbed by gathering texts from diverse human writings and deepfake texts generated by different LLMs. Empirical results on mainstream detection methods demonstrate the difficulties associated with detecting deepfake text in a wide-ranging testbed, particularly in out-of-distribution scenarios. Such difficulties align with the diminishing linguistic differences between the two text sources. Despite challenges, the top-performing detector can identify 84.12{\%} out-of-domain texts generated by a new LLM, indicating the feasibility for application scenarios.",
}

@article{scigen,
 author = {Nafise Sadat Moosavi and
Andreas R{\"{u}}ckl{\'{e}} and
Dan Roth and
Iryna Gurevych},
 journal = {ArXiv preprint},
 title = {Learning to Reason for Text Generation from Scientific Tables},
 url = {https://arxiv.org/abs/2104.08296},
 volume = {abs/2104.08296},
 year = {2021}
}

@inproceedings{scixgen,
 address = {Punta Cana, Dominican Republic},
 author = {Chen, Hong  and
Takamura, Hiroya  and
Nakayama, Hideki},
 booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2021},
 doi = {10.18653/v1/2021.findings-emnlp.128},
 pages = {1483--1492},
 publisher = {Association for Computational Linguistics},
 title = {{S}ci{XG}en: A Scientific Paper Dataset for Context-Aware Text Generation},
 url = {https://aclanthology.org/2021.findings-emnlp.128},
 year = {2021}
}

@misc{mt-bench,
      title={Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena}, 
      author={Lianmin Zheng and Wei-Lin Chiang and Ying Sheng and Siyuan Zhuang and Zhanghao Wu and Yonghao Zhuang and Zi Lin and Zhuohan Li and Dacheng Li and Eric P. Xing and Hao Zhang and Joseph E. Gonzalez and Ion Stoica},
      year={2023},
      eprint={2306.05685},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2306.05685}, 
}

@misc{gemma2,
      title={Gemma 2: Improving Open Language Models at a Practical Size}, 
      author={Google},
      year={2024},
      eprint={2408.00118},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2408.00118}, 
}

@misc{llama3,
      title={The Llama 3 Herd of Models}, 
      author={Meta},
      year={2024},
      eprint={2407.21783},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2407.21783}, 
}