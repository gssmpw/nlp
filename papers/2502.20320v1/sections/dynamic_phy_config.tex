%\vspace{-0.1in}
\section{Accord framework description}
\label{sec:solution}
\vspace{0.05in}
To enable the cross-network and application layer optimization, we develop and implement the ACCORD framework over a 5G network. We first provide a detailed workflow of the framework, referring to Fig.\,\ref{fig:arch}, and then explain the details of the DRL solution that resides at the core of the framework.
%\vspace{-0.05in}
\subsection{ACCORD workflow}
%\vspace{-0.05in}
As the MCA client-server communication instantiates over 5G, ACCORD starts to optimize the network and application through the following steps:

\noindent $\bullet$ \textbf{Step 1-2: } An MCA client, running on the UE, generates video frames (user data) by observing events in the environment (e.g., vehicle detection). After characterizing the requirement (explained in Sec.\,\ref{sec:preliminary_experiment}.A), the MCA client transmits the user data to the server using legacy network configuration parameters. The MCA client also transmits the calculated requirement along with information about UE buffer space and channel quality indicator (CQI) through uplink control information (UCI) messages to the gNB.

\noindent $\bullet$ \textbf{Step 3 -  Building MCA context: } After receiving the user and control data from the UE, ACCORD combines this information with network control data (UE mobility/position and MCS), to build the context for MCA requirement.

\noindent $\bullet$ \textbf{Step 4-5:} The context information is used as input to the DRL agent to perform cross-layer optimization by selecting the optimal set of configurations across the network and application layers, that meet the MCA requirement.

\noindent $\bullet$ \textbf{Step 6-7:} The lower network layer configurations (PHY, MAC) from ACCORD are implemented at the network side (e.g., gNB). The RLC and APP configurations are transmitted to the UE through the downlink control information (DCI).

In \texttt{Step\,3.1}, the MCA server uses the uplink data from the client (that was transmitted in Step 2) to make inferences and generates that feedback to the UE through the network as seen in \texttt{Steps\,3.2} and \texttt{3.3}.\newline
%This information is passed through the downlink channel to the UE then the UE performs necessary reconfiguration.
% \begin{figure}[b!]
%     \centering
%     \includegraphics[width=1\linewidth]{Figures/drl_figures/1uestatic_rewards.pdf}
%     \caption{Performance in total rewards during training on the y-axis and training episodes on the x-axis. The solid lines
% represent smoothed (window size of 2 episodes) averages, while shaded areas
% show the standard deviations. Epsilon greedy is the acting policy while the greedy policy is the updating policy}
%     % \vspace{-5mm}
%     \label{fig:1ue_static_rewards}
% \end{figure}

% \begin{figure}[b!]
%     \centering
%     \includegraphics[width=1\linewidth]{Figures/drl_figures/1uemobile_rewards.pdf}
%     \caption{Total rewards gotten per episode for a single mobile user for different latency requirements.}
%     % \vspace{-5mm}
%     \label{fig:1ue_mobile_rewards}
% \end{figure}

% \begin{figure}[b!]
%     \centering
%     \includegraphics[width=1\linewidth]{Figures/drl_figures/2uestaticandmobile_rewards.pdf}
%     \caption{Total rewards achieved per episode for two static and mobile users for latency requirements of 24ms and 30ms respectively.}
%     % \vspace{-5mm}
%     \label{fig:2ue_static_rewards}
% \end{figure}



% \begin{figure}[b!]
%     \centering
%     \includegraphics[width=1\linewidth]{Figures/drl_figures/1uestaticandmobile_losses.pdf}
%     \caption{Loss 1UE.}
%     % \vspace{-5mm}
%     \label{fig:2ue_static_rewards}
% \end{figure}


% \begin{figure}[b!]
%     \centering
%     \includegraphics[width=1\linewidth]{Figures/drl_figures/2uestaticandmobile_losses.pdf}
%     \caption{Loss 1UE.}
%     % \vspace{-5mm}
%     \label{fig:2ue_static_rewards}
% \end{figure}


% \begin{figure}[t!]
%     \centering
%     \includegraphics[trim=30 0 30 0,width=0.75\linewidth]{Figures/dynamicTDD_flow_1.pdf}
%     \caption{Dynamic PHY frame reconfiguration at NW aided by UE forecast of local channel metrics.}
%     \vspace{-5mm}
%     \label{fig:phy_reconfig}
% \end{figure}
% \begin{figure}[t!]
% \centering
% \includegraphics[trim=25 10 20 0,width=0.85\linewidth]{Figures/framework_overview.png}
% \caption{Sliding window mechanism at UE in using historical channel metrics (in orange) to predict the channel metrics of future time slots (in green).}
% \vspace{-5mm}
% \label{fig:sliding_window}
% \end{figure}
%\vspace{-0.26in}
\vspace{-4mm}
\subsection{Proposed DRL approach for context awareness}
%\vspace{-0.05in}
To develop the context-aware cross-network and application layer optimization in ACCORD, we train a DRL agent by making it interact with the 5G environment simulated in the 3GPP-compliant MATLAB 5G toolbox. The DRL objective is to learn the optimal configuration parameters across the PHY, MAC, RLC, and APP layers based on the MCA context. 
%This context is constructed from the APP requirements, channel quality indicator (CQI), MCS, UE buffer size, UE position, and APP throughput. 
Through this interaction, the agent obtains a reward related to the objective function of meeting the latency requirements of the UE while using minimal network resources to save spectrum. This approach was used as opposed to the supervised learning procedure which necessitates a comprehensive dataset capturing all possible network conditions, UE behaviors, and requirements, which is impractical considering the unpredictable MCA requirement and complexity of real-world situations. The optimization problem can be expressed as a Markov decision process (MDP) of the DRL, which we describe as follows:
% The overall framework for solving the problem was designed using DQN, with the 3GPP compliant MATLAB 5G toolbox using the MATLAB engine API\,\cite{matlab_engine_python}. This framework is shown in Fig.\,\ref{fig:contextaware_architecture}. DQN is a value-based method\,\cite{mnih2013playing} that uses neural networks for training agents to find an optimal policy through a parameterized optimal action-value function $Q(s, a; \theta)$, where $\theta$ are the parameters of the neural network that inputs the current system state $s$ at time $t$ to produce all possible state-action values.
% The goal of the agent is to interact with the environment by selecting actions in a way that maximizes future rewards, with an assumption that future rewards are discounted by a factor of $\gamma$ per time-step, and define the future discounted return at time $t$ as $R_t = \sum_{t}^{T} \gamma^{t} r_{t}$, where $\gamma\in [0, 1]$ weights the importance of future rewards in the cumulative sum of rewards and  $T$ is the time step termination occurs, in our case this is when the episode ends.
% \begin{figure}[t!]
%     \centering
%     \vspace{-2mm}
% \includegraphics[width=0.85\linewidth]{Figures/drl_figures/drl_agent_architecture.pdf}
%     \vspace{-2mm}
%     \caption{Context Aware Cross Layer DRL Framework.}
%     \vspace{-5mm} \label{fig:contextaware_architecture}
% \end{figure}
% To achieve this goal, we need to find an optimal policy that minimizes the overall network latency based on user application requirements and network conditions. This optimal policy which is also a greedy policy would be derived from the optimal action-value function as $\pi_\theta(s) = \underset{a}{\operatorname{argmax}} \, Q(s, a;\theta)$
% \begin{comment}
% \begin{equation}
% \pi_\theta(s) = \underset{a}{\operatorname{argmax}} \, Q(s, a;\theta)
% \label{eq:optimal_policy}
% \end{equation}
% \end{comment}
% To derive this optimal policy, we go into details on how we designed the framework for training the DQN by explaining our state, action, and reward spaces along with the neural network and environment used to generate the states.

\noindent $\bullet$ \textbf{State: } The state space $s_{t}$ can be modeled as the combination of the application required latency ($\mathcal{L}_t$), transmitted bytes ($c_t$)), UE location ($g_t$), MAC buffer status report (BSR) ($k_t$), RLC buffer size ($p_t$), CQI ($v_t$) and MCS ($u_t$) at current time window $t$ with a window size of $w$ for all the UEs in set $U$. Overall, $s_{t}$ is formulated as $s_{t} = \{ \mathcal{L}_t, c_t, g_t, k_t, p_t, v_t, u_t\}$. The $g_t$ is a tuple of dimension 2 corresponding to the 2D coordinates of the target location.
%It should be noted that these are all possible features that can be gotten from the environment and all would necessarily not be used for training the DQN as we would see in the performance evaluation.

\noindent $\bullet$ \textbf{Action: } The action space is designed to allow the DQN agent to generate the PHY, MAC, RLC and APP layer configuration parameters at time $t$ for time window $t+w$ (the next time window). We define the action space $\mathcal{A}_{t}$ for all the UEs in $U$ ($|U| = \mathcal{K}$), as the combination of $C_{t}^{PHY}$ (PHY frame configuration), $C_{t}^{MAC}$ (MAC configuration), $C_{t}^{RLC}$ (RLC configuration) and $C_{t}^{APP}$ (APP configuration), at time $t$ for time window $t+w$, hence, $\mathcal{A}_{t} =\mathcal{K} \times \{C_{t}^{PHY}, C_{t}^{MAC}, C_{t}^{RLC}, C_{t}^{RLC}\}$. The action $\mathcal{A}_{t}$ is generated by the designed policy $\pi_{\theta}({s_t})$ from the state $s_{t}$. Hence, the action of time $t$ is formulated as ${A}_{t} = \pi_{\theta}(s_{t})$.

\noindent $\bullet$ \textbf {Reward: } The reward space is a function of the action $\mathcal{A}_{t}$ generated at time $t$ and transition to the next state $s_{t+w}$ at time $t+w$ for each UE as: $f^{\mathcal{R}}_{\mathcal{A}}(\mathcal{A}_{t},s_{t+w}) = r_{t}$, where $r_{t}$ is the achieved reward for a UE at time $t$. Our reward function is defined as $r_{t}=
\left( \frac{1}{1 + e^{-\beta (l_t - \mathcal{L}_t)}} \right) \times 100$ for $l_t \leq \mathcal{L}_t$ and $r_{t}= 0$ otherwise.
% We carefully design our reward function to account for instances where the achieved latency $l_t$ at time $t$ is either greater or less than the application required latency $\mathcal{L}_t$. This reward is defined as:
\begin{comment}
\begin{equation}
r_{t}=
\begin{cases} 
\left( \frac{1}{1 + e^{-\beta (l_t - \mathcal{L}_t)}} \right) \times 100 & \text{for } l_t \leq \mathcal{L}_t \\[10pt]
0 & \text{else }
\end{cases}
\label{eq:reward}
\end{equation}
\end{comment}
$\beta$ is chosen in such a way as to control the steepness of the sigmoid function utilized in the equation. Smaller values of $\beta$ ensures that the function is smoother, causing $r_t$ to change more gradually as $l_t$ moves around $\mathcal{L}_t$.\newline
% \noindent $\bullet$ \textbf {Environment: } Finally the environment is parameterized as ${s_{t+w}, r_{t}} = \psi(\mathcal{A}_{t})$, where the environment $\psi$ takes the inputs of the generated action by the DRL network at time window $t$ and generates the next state for time window $t+w$ and reward for taking action $A_t$ at time $t$.

{\em Our goal is to find an optimal policy that minimizes the overall network latency based on real-time MCA requirements and network conditions while conserving network resources. This is achieved by mapping the state space to an action space that maximizes the accumulated reward. We employ the Deep Q-Network (DQN) algorithm\,\cite{mnih2013playing} explained in Algorithm\,\ref{Algo:dqn} and adapt it to solve our MDP.}
%This algorithm is explained in Algorithm \ref{Algo:dqn}.
\begin{algorithm}[t!] 
\caption{Context-Aware DQN in ACCORD}
\scriptsize
\begin{algorithmic}
\State \textbf{Initialization:} Experience replay memory $\mathcal{D}$ to capacity $N$, batch size, main network parameters $\theta$, target network parameters $\theta^- = \theta$, update rate of target network $\tau$, number of episodes $N_{\text{eps}}$, number of time steps in each episode $N_{\text{step}}$, learning rate and epsilon $\epsilon$ for the exploitation and exploration.
\For{\textit{episode} = 1 : $N_{\text{eps}}$}
    \State Initialize processed state from environment $\phi_t$ =  $\phi(s_t)$;
    \For{\textit{step} = 1 : $N_{\text{step}}$}
        \State Using epsilon greedy as in \cite{mnih2013playing};
        \State With probability $\epsilon$, select a random action $a_t$
        \State Otherwise, select $a_t = \max_a Q^*(\phi_t, a; \theta)$
        \State Execute action $a_t$ and observe reward $r_t$ and $s_{t+w}$
        \State Process $s_{t+w}$ and store $(\phi_t, a_t,\phi_{t+w}, r_t)$ in $\mathcal{D}$
        \State \textbf{Optimize model:}
         \If{memory has sufficient transitions}
            \State Sample a random mini-batch of transitions $(\phi_k, a_k, \phi_{k+w},r_k)$ from $\mathcal{D}$
            \State Compute $Q(\phi_k, a_k; \theta)$ for each state-action pair in the batch
             \State Compute target Q values based on if $\phi_{k+w}$ is terminal or not as in \cite{mnih2013playing}
            \State Compute loss between $Q(\phi_k, a_k; \theta)$ and the target Q values
            \State Perform gradient descent to update $\theta$;
        \EndIf
        %\State \textbf{Update target network:}
            \State Soft update target network: $\theta^{-} \gets \tau \theta + (1 - \tau)\theta^{-}$
    \EndFor
\EndFor
\end{algorithmic}
\label{Algo:dqn}
%\vspace{-0.5mm}
\end{algorithm}
%\vspace{-4mm}
%Based on this framework, we now give an overview of how the DQN is initialized and trained using experienced replay. After the environment is initialized, the behavior distribution of the action selection follows a tradeoff between exploration and exploitation through the standard $\epsilon-$greedy policy. A standard approach for handling this exploration and exploitation is that with probability $\epsilon$, a random action is selected, while with probability 1-$\epsilon$, a greedy action is specified earlier. Based on this policy, at step $t$, the agent performs an action, transitions to the next state, and receives a reward. This whole process forms an experience of the tuple $(s_t,a_t,r_t,s_{t+w})$ and is stored in an initialized experience replay buffer.

% \begin{figure*}[t!]
% \centering % <-- added
% \begin{subfigure}{0.2\linewidth}
%         \includegraphics[width=\linewidth]{Figures/drl_figures/1uestatic_rewards.pdf}
%         \caption{Single Static UE}
%         \label{fig:single_static_ue}
% \end{subfigure}\hfil % <-- added
% \begin{subfigure}{0.2\linewidth}
%         \includegraphics[width=\linewidth]{Figures/drl_figures/1uemobile_rewards.pdf}
%         \caption{Single Mobile UE}
%         \label{fig:single_mobile_ue}
% \end{subfigure}\hfil % <-- added
% \begin{subfigure}{0.2\linewidth}
%         \includegraphics[width=\linewidth]{Figures/drl_figures/2uestaticandmobile_rewards.pdf}
%         \caption{Multiple UEs}
%         \label{fig:multiple_ues}
% \end{subfigure}\hfil % <-- added
% \caption{Rewards achieved by the DRL agent for optimizing the network for the different network setup scenarios considered \textcolor{red}{remove fig.b and place the remaining two plots side by side in one column like fig.7}}
% \vspace{-3mm}
% \label{fig:rewards}
% \end{figure*}


% \begin{figure}[t!]
%     \centering
%     %\vspace{-3mm}
% \includegraphics[width=\linewidth]{Figures/UE-NW-signaling.pdf}
%     \vspace{-3mm}
%     \caption{ACCORD implementation in 5G}
%     \vspace{-5mm}
% \label{fig:drl_agent_architecture}
% \end{figure}


%In training the DQN, there are two neural networks used, one for the policy network and the other for the target network. The neural network architecture used for both networks can be seen in Fig. \ref{fig:drl_agent_architecture}. We randomly sample a batch $B$ of experiences from the replay memory and calculate the target Q-value over the batch of samples as $y = r$ if $s_{t+w}$ is terminal and $y= [r + \gamma \max_{a_{t+w}} Q_{\text{target}}(s_{t+w}, a_{t+w})]$ otherwise.
\begin{comment}
\begin{equation}
y = 
\begin{cases} 
r & \text{if } s_{t+w} \text{ is terminal} \\
r + \gamma \max_{a_{t+w}} Q_{\text{target}}(s_{t+w}, a_{t+w}) & \text{otherwise}
\end{cases}
\end{equation}
\end{comment}
%Here, $Q_{target}$ is a copy of the Q-network but is updated less frequently. Finally, we optimize the Q-network by minimizing the loss between the predicted Q-value and the target Q-value and then use gradient descent to update the Q-network weights $\theta$, where $L(\theta) = [\frac{1}{|B|} \sum (y - Q(s, a; \theta))^2]$
\begin{comment}
\begin{equation}
L(\theta) = \frac{1}{|B|} \sum (y - Q(s, a; \theta))^2
\end{equation}
\end{comment}
%Periodically, the weights from the Q-network are copied to the target network. This process is repeated while the experiences are being collected over multiple episodes until the agent learns an optimal policy.
%\vspace{-1mm}