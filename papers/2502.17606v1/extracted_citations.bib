@inproceedings{Dremel,
    author = {Zhao, Chenxingyu and Chugh, Tapan and Min, Jaehong and Liu, Ming and Krishnamurthy, Arvind},
    title = {Dremel: Adaptive Configuration Tuning of RocksDB KV-Store},
    year = {2022},
    isbn = {9781450391412},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3489048.3530970},
    doi = {10.1145/3489048.3530970},
    abstract = {LSM-tree-based key-value stores like RocksDB are widely used to support many applications. However, configuring a RocksDB instance is challenging for the following reasons: 1) RocksDB has a massive parameter space to configure; 2) there are inherent trade-offs and dependencies between parameters; 3) optimal configurations are dependent on workload and hardware; and 4) evaluating configurations is time-consuming. Prior works struggle with handling the curse of dimensionality, capturing relationships between parameters, adapting configurations to workload and hardware, and evaluating quickly.We present a system, Dremel, to adaptively and quickly configure RocksDB with strategies based on the Multi-Armed Bandit model. To handle the large parameter space, we propose using fused features, which encode domain-specific knowledge, to work as a compact and powerful representation for configurations. To adapt to the workload and hardware, we build an online bandit model to identify the best configuration. To evaluate quickly, we enable multi-fidelity evaluation and upper-confidence-bound sampling to speed up configuration search. Dremel not only achieves up to \texttimes{}2.61 higher IOPS and 57\% less latency than default configurations but also achieves up to 63\% improvement over prior works on 18 different settings with the same or smaller time budget.This paper is an abridged version.},
    booktitle = {Abstract Proceedings of the 2022 ACM SIGMETRICS/IFIP PERFORMANCE Joint International Conference on Measurement and Modeling of Computer Systems},
    pages = {61–62},
    numpages = {2},
    keywords = {rocksdb, multi-armed bandit, configuration management},
    location = {Mumbai, India},
    series = {SIGMETRICS/PERFORMANCE '22}
}

@inproceedings{RTune,
    author = {Jin, Huijun and Lee, Jieun and Park, Sanghyun},
    title = {RTune: a RocksDB tuning system with deep genetic algorithm},
    year = {2022},
    isbn = {9781450392372},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3512290.3528726},
    doi = {10.1145/3512290.3528726},
    abstract = {Database systems typically have many knobs that must be configured by database administrators to achieve high performance. RocksDB achieves fast data writing performance using a log-structured merge-tree. This database contains many knobs related to write and space amplification, which are important performance indicators in RocksDB. Previously, it was proved that significant performance improvements could be achieved by tuning database knobs. However, tuning multiple knobs simultaneously is a laborious task owing to the large number of potential configuration combinations and trade-offs.To address this problem, we built a tuning system for RocksDB. First, we generated a valuable RocksDB data repository for analysis and tuning. To find the workload that is most similar to a target workload, we created a new representation for workloads. We then applied the Mahalanobis distance to create a combined workload that is as close to the original target workload as possible. Subsequently, we trained a deep neural network model with the combined workload and used it as the fitness function of a genetic algorithm. Finally, we applied the genetic algorithm to find the best solution for the original target workload. The experimental results demonstrated that the proposed system achieved a significant performance improvement for various target workloads.},
    booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
    pages = {1209–1217},
    numpages = {9},
    keywords = {database optimization, genetic algorithm, log-structured merge-tree, space amplification, write amplification},
    location = {Boston, Massachusetts},
    series = {GECCO '22}
}

@inproceedings{gan_modeling_network,
    author = {Lin, Zinan and Jain, Alankar and Wang, Chen and Fanti, Giulia and Sekar, Vyas},
    title = {Using GANs for Sharing Networked Time Series Data: Challenges, Initial Promise, and Open Questions},
    year = {2020},
    isbn = {9781450381383},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3419394.3423643},
    doi = {10.1145/3419394.3423643},
    abstract = {Limited data access is a longstanding barrier to data-driven research and development in the networked systems community. In this work, we explore if and how generative adversarial networks (GANs) can be used to incentivize data sharing by enabling a generic framework for sharing synthetic datasets with minimal expert knowledge. As a specific target, our focus in this paper is on time series datasets with metadata (e.g., packet loss rate measurements with corresponding ISPs). We identify key challenges of existing GAN approaches for such workloads with respect to fidelity (e.g., long-term dependencies, complex multidimensional relationships, mode collapse) and privacy (i.e., existing guarantees are poorly understood and can sacrifice fidelity). To improve fidelity, we design a custom workflow called DoppelGANger (DG) and demonstrate that across diverse real-world datasets (e.g., bandwidth measurements, cluster requests, web sessions) and use cases (e.g., structural characterization, predictive modeling, algorithm comparison), DG achieves up to 43\% better fidelity than baseline models. Although we do not resolve the privacy problem in this work, we identify fundamental challenges with both classical notions of privacy and recent advances to improve the privacy properties of GANs, and suggest a potential roadmap for addressing these challenges. By shedding light on the promise and challenges, we hope our work can rekindle the conversation on workflows for data sharing.},
    booktitle = {Proceedings of the ACM Internet Measurement Conference},
    pages = {464–483},
    numpages = {20},
    keywords = {time series, synthetic data generation, privacy, generative adversarial networks},
    location = {Virtual Event, USA},
    series = {IMC '20}
}

@inproceedings{giannakouris2024demonstrating,
  title={Demonstrating $\lambda$-tune: Exploiting large language models for workload-adaptive database system tuning},
  author={Giannakouris, Victor and Trummer, Immanuel},
  booktitle={Companion of the 2024 International Conference on Management of Data},
  pages={508--511},
  year={2024}
}

@article{giannankouris2024lambda,
  title={$\{$$\backslash$lambda$\}$-Tune: Harnessing Large Language Models for Automated Database System Tuning},
  author={Giannankouris, Victor and Trummer, Immanuel},
  journal={arXiv preprint arXiv:2411.03500},
  year={2024}
}

@article{lee2024k2vtune,
  title={K2vTune: A workload-aware configuration tuning for RocksDB},
  author={Lee, Jieun and Seo, Sangmin and Choi, Jonghwan and Park, Sanghyun},
  journal={Information Processing \& Management},
  volume={61},
  number={1},
  pages={103567},
  year={2024},
  publisher={Elsevier}
}

@article{yu2024camal,
  title={CAMAL: Optimizing LSM-trees via Active Learning},
  author={Yu, Weiping and Luo, Siqiang and Yu, Zihao and Cong, Gao},
  journal={Proceedings of the ACM on Management of Data},
  volume={2},
  number={4},
  pages={1--26},
  year={2024},
  publisher={ACM New York, NY, USA}
}

