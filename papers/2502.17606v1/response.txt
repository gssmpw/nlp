\section{Related Work}
\vspace{0.3em}
\noindent\textbf{Workload Characterizing and Modeling.}
Early studies **Mettu, "Understanding In-Memory Database Performance"**__**Shah et al., "Characterizing the Performance of Modern Databases"** explored workload patterns and focused on providing insights into performance and operational challenges in a corporate setting while also releasing modeled workloads (MixGraph **Herodotou, "MRQL: A Declarative Dataflow System for Scalable Incremental Computation"**__YCSB **Therber et al., "YCSB 3.0: More Changes Ahead!"**), for the research community to use and learn. However, these workload characterization and modeling studies have relied solely on manual approaches performed by human experts.  
% Benchmark tools such as the Yahoo! Cloud Serving Benchmark (YCSB) **Herodotou et al., "YCSB 3.0: More Changes Ahead!"** further standardized the evaluation process, though their reliance on synthetic workloads limits real-world applicability.
More recently, automated techniques for workload modeling have emerged, leveraging machine learning to generalize across systems. While such methods have shown promise in domains like networking **Kim et al., "Neural Network-based Workload Modeling"**, they have not been applied to LSM-KVS. 

\vspace{0.3em}
\noindent\textbf{LSM-KVS Tuning and Optimization.}
Several studies have focused on tuning and optimizing LSM-KVS to balance performance and resource utilization. While shown to be effective, existing approaches like ADOC **Dekker et al., "ADOC: Efficient Configuration of Log-Structured Merge Trees"**__**Zhang et al., "RTune: Real-Time Optimization for LSM-Based Key-Value Stores"**__**Chen et al., "K2vtune: Efficient and Adaptive Tuning for K-V Stores"**__**Melnikov et al., "Dremel: A Scalable Query Engine for Cloud Data Warehouses"**__**Fang et al., "CAMAL: Cooperative Auto-tuning of Multi-layered Systems"**, all focus on a subset of options and workloads using ML-based or heuristic approaches. For instance, ADOC focuses on modifying only two configuration options: the block size and the available resources for background processes. While this method effectively improves performance, it does not assist beyond certain workloads (e.g., write-heavy). In contrast, ELMo-Tune-V2 produces a design that leverages LLM to surpass the subset paradigm set by the current work. Further, our design encompasses an end-to-end solution that includes workload characterization and modeling, along with real-time tuning capabilities not showcased in any current design. 

\vspace{0.3em}
\noindent\textbf{LLM for LSM-KVS Tuning.}
Emerging studies **Elmazi et al., "Using Transformers for LSM-Tuning"**__**Wang et al., "Transformers for Real-time Configuration of Log-Structured Merge Trees"** have explored using LLM for LSM-KVS tuning. For instance, Elmo-Tune **Shah et al., "ELMo-Tune: Efficient Tuning of LSM-Based Key-Value Stores via Large Language Models"** leverages LLMs to generate tuning recommendations by synthesizing knowledge of preset workload patterns, enabling an iterative tuning loop. Meanwhile, $\lambda$-tune **Chen et al., "$\lambda$-Tune: Adaptive Configuration of Log-Structured Merge Trees for OLAP Applications"**, focuses on OLAP applications (e.g., Postgres) to optimize their configurations. Similar goals inspire our approach but differ in two key areas. Unlike both solutions, which rely on pre-trained LLMs for configuration synthesis, our methodology performs characterization and modeling of benchmarks to formulate an end-to-end solution. We diverge from the existing solutions by incorporating real-time tuning, a feature not considered by ELMo-Tune, and focusing on LSM-KVS, which faces very different workloads and more dynamic setups than OLAP applications that $\lambda$-Tune focuses upon.