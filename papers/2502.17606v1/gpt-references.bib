@article{eigner2024determinants,
  title={Determinants of llm-assisted decision-making},
  author={Eigner, Eva and H{\"a}ndler, Thorsten},
  journal={arXiv preprint arXiv:2402.17385},
  year={2024}
}

@article{masti2021we,
  title={How we built a general purpose key value store for Facebook with ZippyDB},
  author={Masti, Sarang},
  journal={Accessed: Aug},
  year={2021}
}

@article{wu2022nebula,
  title={Nebula Graph: An open source distributed graph database},
  author={Wu, Min and Yi, Xinglu and Yu, Hui and Liu, Yu and Wang, Yujue},
  journal={arXiv preprint arXiv:2206.07278},
  year={2022}
}


@article{chang2008bigtable,
  title={Bigtable: A distributed storage system for structured data},
  author={Chang, Fay and Dean, Jeffrey and Ghemawat, Sanjay and Hsieh, Wilson C and Wallach, Deborah A and Burrows, Mike and Chandra, Tushar and Fikes, Andrew and Gruber, Robert E},
  journal={ACM Transactions on Computer Systems (TOCS)},
  volume={26},
  number={2},
  pages={1--26},
  year={2008},
  publisher={ACM New York, NY, USA}
}

@inproceedings{wang2013plsm,
  title={PLSM: a highly efficient LSM-tree index supporting real-time big data analysis},
  author={Wang, Jin and Zhang, Yong and Gao, Yang and Xing, Chunxiao},
  booktitle={2013 IEEE 37th Annual Computer Software and Applications Conference},
  pages={240--245},
  year={2013},
  organization={IEEE}
}

@misc{apache_kvrocks,
  author       = {Apache},
  title        = {apache/kvrocks: Apache Kvrocks is a distributed key value NoSQL database that uses RocksDB as storage engine and is compatible with Redis protocol},
  howpublished = {\url{https://github.com/apache/kvrocks}},
}

@article{wang2017machine,
  title={Machine learning for networking: Workflow, advances and opportunities},
  author={Wang, Mowei and Cui, Yong and Wang, Xin and Xiao, Shihan and Jiang, Junchen},
  journal={Ieee Network},
  volume={32},
  number={2},
  pages={92--99},
  year={2017},
  publisher={IEEE}
}

@inproceedings{tuncer2017diagnosing,
  title={Diagnosing performance variations in HPC applications using machine learning},
  author={Tuncer, Ozan and Ates, Emre and Zhang, Yijia and Turk, Ata and Brandt, Jim and Leung, Vitus J and Egele, Manuel and Coskun, Ayse K},
  booktitle={High Performance Computing: 32nd International Conference, ISC High Performance 2017, Frankfurt, Germany, June 18--22, 2017, Proceedings 32},
  pages={355--373},
  year={2017},
  organization={Springer}
}

@inproceedings{paul2022machine,
  title={Machine learning assisted HPC workload trace generation for leadership scale storage systems},
  author={Paul, Arnab K and Choi, Jong Youl and Karimi, Ahmad Maroof and Wang, Feiyi},
  booktitle={Proceedings of the 31st International Symposium on High-Performance Parallel and Distributed Computing},
  pages={199--212},
  year={2022}
}

@article{virtanen2020scipy,
  title={SciPy 1.0: fundamental algorithms for scientific computing in Python},
  author={Virtanen, Pauli and Gommers, Ralf and Oliphant, Travis E and Haberland, Matt and Reddy, Tyler and Cournapeau, David and Burovski, Evgeni and Peterson, Pearu and Weckesser, Warren and Bright, Jonathan and others},
  journal={Nature methods},
  volume={17},
  number={3},
  pages={261--272},
  year={2020},
  publisher={Nature Publishing Group}
}

@article{betzler2003fitting,
  title={Fitting in Matlab},
  author={Betzler, Klaus},
  journal={Fachbereich Phys. Univ. Osnabr},
  year={2003}
}

@book{10.5555/1593511,
 author = {Van Rossum, Guido and Drake, Fred L.},
 title = {Python 3 Reference Manual},
 year = {2009},
 isbn = {1441412697},
 publisher = {CreateSpace},
 address = {Scotts Valley, CA}
}

@misc{rocksdb_v8_8_1,
  author       = {Facebook, Inc.},
  title        = {RocksDB v8.8.1: A persistent key-value store for flash and RAM storage},
  howpublished = {\url{https://github.com/facebook/rocksdb}},
  year         = {2023},
}

@misc{facebook_db_bench,
  author       = {Facebook},
  title        = {db\_bench},
  howpublished = {\url{https://github.com/facebook/rocksdb/wiki/Benchmarkingtools}},
  year         = {2024},
}

@article{rosen2013resource,
  title={Resource management: Linux kernel namespaces and cgroups},
  author={Rosen, Rami},
  journal={Haifux, May},
  volume={186},
  pages={70},
  year={2013}
}

@misc{rocksdb_tuning_guide,
  author       = {Facebook},
  title        = {RocksDB Tuning Guide},
  howpublished = {\url{https://github.com/facebook/rocksdb/wiki/RocksDB-Tuning-Guide}},
}

@article{yang2021large,
  title={A large-scale analysis of hundreds of in-memory key-value cache clusters at twitter},
  author={Yang, Juncheng and Yue, Yao and Rashmi, KV},
  journal={ACM Transactions on Storage (TOS)},
  volume={17},
  number={3},
  pages={1--35},
  year={2021},
  publisher={ACM New York, NY}
}

@misc{redis_benchmark,
  author       = {Salvatore Sanfilippo and the Redis Core Team},
  title        = {Redis Benchmark Tool},
  howpublished = {\url{https://redis.io/docs/latest/operate/oss_and_stack/management/optimization/benchmarks/}},
  year         = {2023},
}

@article{angles2020ldbc,
  title={The LDBC social network benchmark},
  author={Angles, Renzo and Antal, J{\'a}nos Benjamin and Averbuch, Alex and Birler, Altan and Boncz, Peter and B{\'u}r, M{\'a}rton and Erling, Orri and Gubichev, Andrey and Haprian, Vlad and Kaufmann, Moritz and others},
  journal={arXiv preprint arXiv:2001.02299},
  year={2020}
}

@inproceedings{erling2015ldbc,
  title={The LDBC social network benchmark: Interactive workload},
  author={Erling, Orri and Averbuch, Alex and Larriba-Pey, Josep and Chafi, Hassan and Gubichev, Andrey and Prat, Arnau and Pham, Minh-Duc and Boncz, Peter},
  booktitle={Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data},
  pages={619--630},
  year={2015}
}

@misc{nebula_benchmark,
  author       = {Nebula Contrib Community},
  title        = {NebulaGraph Bench: Benchmark Suite for NebulaGraph},
  howpublished = {\url{https://github.com/nebula-contrib/NebulaGraph-Bench}},
  year         = {2024},
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, A},
  journal={Advances in Neural Information Processing Systems},
  year={2017}
}

@misc{meta2025llama3,
  author       = {Meta AI},
  title        = {LLaMA 3: Open and Efficient Foundation Language Models},
  year         = {2025},
  url          = {https://www.llama.com/},   
}

@article{dubey2024llama,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

@article{malach2023auto,
  title={Auto-regressive next-token predictors are universal learners},
  author={Malach, Eran},
  journal={arXiv preprint arXiv:2309.06979},
  year={2023}
}

@article{lee2024k2vtune,
  title={K2vTune: A workload-aware configuration tuning for RocksDB},
  author={Lee, Jieun and Seo, Sangmin and Choi, Jonghwan and Park, Sanghyun},
  journal={Information Processing \& Management},
  volume={61},
  number={1},
  pages={103567},
  year={2024},
  publisher={Elsevier}
}

@inproceedings{jia2020kill,
  title={Kill two birds with one stone: Auto-tuning rocksdb for high bandwidth and low latency},
  author={Jia, Yichen and Chen, Feng},
  booktitle={2020 IEEE 40th International Conference on Distributed Computing Systems (ICDCS)},
  pages={652--664},
  year={2020},
  organization={IEEE}
}

@article{mo2023learning,
  title={Learning to Optimize LSM-trees: Towards A Reinforcement Learning based Key-Value Store for Dynamic Workloads},
  author={Mo, Dingheng and Chen, Fanchao and Luo, Siqiang and Shan, Caihua},
  journal={Proceedings of the ACM on Management of Data},
  volume={1},
  number={3},
  pages={1--25},
  year={2023},
  publisher={ACM New York, NY, USA}
}

@inproceedings{snyder1990tmpfs,
  title={tmpfs: A virtual memory file system},
  author={Snyder, Peter},
  booktitle={Proceedings of the autumn 1990 EUUG Conference},
  pages={241--248},
  year={1990}
}

@article{naveed2023comprehensive,
  title={A comprehensive overview of large language models},
  author={Naveed, Humza and Khan, Asad Ullah and Qiu, Shi and Saqib, Muhammad and Anwar, Saeed and Usman, Muhammad and Akhtar, Naveed and Barnes, Nick and Mian, Ajmal},
  journal={arXiv preprint arXiv:2307.06435},
  year={2023}
}

@article{rosin2022temporal,
  title={Temporal attention for language models},
  author={Rosin, Guy D and Radinsky, Kira},
  journal={arXiv preprint arXiv:2202.02093},
  year={2022}
}

@article{yu2024camal,
  title={CAMAL: Optimizing LSM-trees via Active Learning},
  author={Yu, Weiping and Luo, Siqiang and Yu, Zihao and Cong, Gao},
  journal={Proceedings of the ACM on Management of Data},
  volume={2},
  number={4},
  pages={1--26},
  year={2024},
  publisher={ACM New York, NY, USA}
}

@article{giannankouris2024lambda,
  title={$\{$$\backslash$lambda$\}$-Tune: Harnessing Large Language Models for Automated Database System Tuning},
  author={Giannankouris, Victor and Trummer, Immanuel},
  journal={arXiv preprint arXiv:2411.03500},
  year={2024}
}

@inproceedings{giannakouris2024demonstrating,
  title={Demonstrating $\lambda$-tune: Exploiting large language models for workload-adaptive database system tuning},
  author={Giannakouris, Victor and Trummer, Immanuel},
  booktitle={Companion of the 2024 International Conference on Management of Data},
  pages={508--511},
  year={2024}
}

————original——————————
@inproceedings{RTune,
    author = {Jin, Huijun and Lee, Jieun and Park, Sanghyun},
    title = {RTune: a RocksDB tuning system with deep genetic algorithm},
    year = {2022},
    isbn = {9781450392372},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3512290.3528726},
    doi = {10.1145/3512290.3528726},
    abstract = {Database systems typically have many knobs that must be configured by database administrators to achieve high performance. RocksDB achieves fast data writing performance using a log-structured merge-tree. This database contains many knobs related to write and space amplification, which are important performance indicators in RocksDB. Previously, it was proved that significant performance improvements could be achieved by tuning database knobs. However, tuning multiple knobs simultaneously is a laborious task owing to the large number of potential configuration combinations and trade-offs.To address this problem, we built a tuning system for RocksDB. First, we generated a valuable RocksDB data repository for analysis and tuning. To find the workload that is most similar to a target workload, we created a new representation for workloads. We then applied the Mahalanobis distance to create a combined workload that is as close to the original target workload as possible. Subsequently, we trained a deep neural network model with the combined workload and used it as the fitness function of a genetic algorithm. Finally, we applied the genetic algorithm to find the best solution for the original target workload. The experimental results demonstrated that the proposed system achieved a significant performance improvement for various target workloads.},
    booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
    pages = {1209–1217},
    numpages = {9},
    keywords = {database optimization, genetic algorithm, log-structured merge-tree, space amplification, write amplification},
    location = {Boston, Massachusetts},
    series = {GECCO '22}
}

@article{Endure,
    author = {Huynh, Andy and Chaudhari, Harshal A. and Terzi, Evimaria and Athanassoulis, Manos},
    title = {Endure: a robust tuning paradigm for LSM trees under workload uncertainty},
    year = {2022},
    issue_date = {April 2022},
    publisher = {VLDB Endowment},
    volume = {15},
    number = {8},
    issn = {2150-8097},
    url = {https://doi.org/10.14778/3529337.3529345},
    doi = {10.14778/3529337.3529345},
    abstract = {Log-Structured Merge trees (LSM trees) are increasingly used as the storage engines behind several data systems, frequently deployed in the cloud. Similar to other database architectures, LSM trees consider information about the expected workload (e.g., reads vs. writes, point vs. range queries) to optimize their performance via tuning. However, operating in a shared infrastructure like the cloud comes with workload uncertainty due to the fast-evolving nature of modern applications. Systems with static tuning discount the variability of such hybrid workloads and hence provide an inconsistent and overall suboptimal performance.To address this problem, we introduce Endure - a new paradigm for tuning LSM trees in the presence of workload uncertainty. Specifically, we focus on the impact of the choice of compaction policies, size ratio, and memory allocation on the overall performance. Endure considers a robust formulation of the throughput maximization problem and recommends a tuning that maximizes the worst-case throughput over the neighborhood of each expected workload. Additionally, an uncertainty tuning parameter controls the size of this neighborhood, thereby allowing the output tunings to be conservative or optimistic. Through both model-based and extensive experimental evaluations of Endure in the state-of-the-art LSM-based storage engine, RocksDB, we show that the robust tuning methodology consistently outperforms classical tuning strategies. The robust tunings output by Endure lead up to a 5X improvement in throughput in the presence of uncertainty. On the flip side, Endure tunings have negligible performance loss when the observed workload exactly matches the expected one.},
    journal = {Proc. VLDB Endow.},
    month = {apr},
    pages = {1605–1618},
    numpages = {14}
}

@inproceedings{cao_characterizing_2020,
	title = {Characterizing, {Modeling}, and {Benchmarking} \{{RocksDB}\} \{{Key}-{Value}\} {Workloads} at {Facebook}},
	isbn = {9781939133120},
	url = {https://www.usenix.org/conference/fast20/presentation/cao-zhichao},
	language = {en},
	urldate = {2024-04-06},
	author = {Cao, Zhichao and Dong, Siying and Vemuri, Sagar and Du, David H. C.},
	year = {2020},
	pages = {209--223},
}

@inproceedings{Dremel,
    author = {Zhao, Chenxingyu and Chugh, Tapan and Min, Jaehong and Liu, Ming and Krishnamurthy, Arvind},
    title = {Dremel: Adaptive Configuration Tuning of RocksDB KV-Store},
    year = {2022},
    isbn = {9781450391412},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3489048.3530970},
    doi = {10.1145/3489048.3530970},
    abstract = {LSM-tree-based key-value stores like RocksDB are widely used to support many applications. However, configuring a RocksDB instance is challenging for the following reasons: 1) RocksDB has a massive parameter space to configure; 2) there are inherent trade-offs and dependencies between parameters; 3) optimal configurations are dependent on workload and hardware; and 4) evaluating configurations is time-consuming. Prior works struggle with handling the curse of dimensionality, capturing relationships between parameters, adapting configurations to workload and hardware, and evaluating quickly.We present a system, Dremel, to adaptively and quickly configure RocksDB with strategies based on the Multi-Armed Bandit model. To handle the large parameter space, we propose using fused features, which encode domain-specific knowledge, to work as a compact and powerful representation for configurations. To adapt to the workload and hardware, we build an online bandit model to identify the best configuration. To evaluate quickly, we enable multi-fidelity evaluation and upper-confidence-bound sampling to speed up configuration search. Dremel not only achieves up to \texttimes{}2.61 higher IOPS and 57\% less latency than default configurations but also achieves up to 63\% improvement over prior works on 18 different settings with the same or smaller time budget.This paper is an abridged version.},
    booktitle = {Abstract Proceedings of the 2022 ACM SIGMETRICS/IFIP PERFORMANCE Joint International Conference on Measurement and Modeling of Computer Systems},
    pages = {61–62},
    numpages = {2},
    keywords = {rocksdb, multi-armed bandit, configuration management},
    location = {Mumbai, India},
    series = {SIGMETRICS/PERFORMANCE '22}
}

@INPROCEEDINGS{9355766,
  author={Jia, Yichen and Chen, Feng},
  booktitle={2020 IEEE 40th International Conference on Distributed Computing Systems (ICDCS)}, 
  title={Kill Two Birds with One Stone: Auto-tuning RocksDB for High Bandwidth and Low Latency}, 
  year={2020},
  volume={},
  number={},
  pages={652-664},
  keywords={Software algorithms;Quality of service;Throughput;Hardware;Software;Task analysis;Tuning;Auto tuning;RocksDB;Multi objective Optimization;Log Structured Merge Tree;Genetic Algorithms},
  doi={10.1109/ICDCS47774.2020.00113}}

@inproceedings{10.1145/3437984.3458841,
    author = {Alabed, Sami and Yoneki, Eiko},
    title = {High-Dimensional Bayesian Optimization with Multi-Task Learning for RocksDB},
    year = {2021},
    isbn = {9781450382984},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3437984.3458841},
    doi = {10.1145/3437984.3458841},
    abstract = {RocksDB is a general-purpose embedded key-value store used in multiple different settings. Its versatility comes at the cost of complex tuning configurations. This paper investigates maximizing the throughput of RocksDB 10 operations by auto-tuning ten parameters of varying ranges. Off-the-shelf optimizers struggle with high-dimensional problem spaces and require a large number of training samples.We propose two techniques to tackle this problem: multitask modeling and dimensionality reduction through clustering. By incorporating adjacent optimization in the model, the model converged faster and found complicated settings that other tuners could not find. This approach had an additional computational complexity overhead, which we mitigated by manually assigning parameters to each sub-goal through our knowledge of RocksDB. The model is then incorporated in a standard Bayesian Optimization loop to find parameters that maximize RocksDB's 10 throughput.Our method achieved x1.3 improvement when bench-marked against a simulation of Facebook's social graph traffic, and converged in ten optimization steps compared to other state-of-the-art methods that required fifty steps.},
    booktitle = {Proceedings of the 1st Workshop on Machine Learning and Systems},
    pages = {111–119},
    numpages = {9},
    keywords = {Multi-Task Learning, Bayesian Optimization},
    location = {Online, United Kingdom},
    series = {EuroMLSys '21}
}

@inproceedings{rodola_giampaolopsutil_2024,
	title = {giampaolo/psutil},
	copyright = {BSD-3-Clause},
	url = {https://github.com/giampaolo/psutil},
	abstract = {Cross-platform lib for process and system monitoring in Python},
	urldate = {2024-01-17},
	author = {Rodola, Giampaolo},
	month = jan,
	year = {2024},
	note = {original-date: 2014-05-23T14:01:48Z},
	keywords = {cpu, disk, freebsd, linux, memory, monitoring, netbsd, openbsd, osx, ps, psutil, python, sensors, system-monitoring, top, windows},
}

@inproceedings{axboe_flexible_2022,
	title = {Flexible {I}/{O} {Tester}},
	copyright = {GPL-2.0},
	url = {https://github.com/axboe/fio},
	abstract = {Flexible I/O Tester},
	urldate = {2024-01-17},
	author = {Axboe, Jens},
	year = {2022},
	note = {original-date: 2012-10-22T08:20:41Z},
}

@inproceedings{rocksdb,
	title = {facebook/rocksdb},
	copyright = {GPL-2.0},
	url = {https://github.com/facebook/rocksdb},
	abstract = {A library that provides an embeddable, persistent key-value store for fast storage.},
	urldate = {2024-01-17},
	publisher = {Meta},
	month = jan,
	year = {2024},
	note = {original-date: 2012-11-30T06:16:18Z},
	keywords = {database, storage-engine},
}

@inproceedings{noauthor_asu-idigpt-assisted-rocksdb-config_nodate,
	title = {asu-idi/gpt-assisted-rocksdb-config: {GPT} {Assisted} {RocksDB} {Configuration} {Research}},
	shorttitle = {asu-idi/gpt-assisted-rocksdb-config},
	url = {https://github.com/asu-idi/gpt-assisted-rocksdb-config},
	abstract = {GPT Assisted RocksDB Configuration Research. Contribute to asu-idi/gpt-assisted-rocksdb-config development by creating an account on GitHub.},
	language = {en},
	urldate = {2024-01-17},
	journal = {GitHub},
}


@article{barni_effectiveness_2005,
	title = {Effectiveness of exhaustive search and template matching against watermark desynchronization},
	volume = {12},
	issn = {1558-2361},
	url = {https://ieeexplore.ieee.org/abstract/document/1381475},
	doi = {10.1109/LSP.2004.840872},
	abstract = {By focusing on a simple example, we investigate the effectiveness of exhaustive watermark detection and resynchronization through template matching against watermark desynchronization. We find that if the size of the search space does not increase exponentially, both methods provide asymptotically good results. We also show that the exhaustive search approach outperforms template matching from the point of view of reliable detection.},
	number = {2},
	urldate = {2024-03-23},
	journal = {IEEE Signal Processing Letters},
	author = {Barni, M.},
	month = feb,
	year = {2005},
	keywords = {Watermarking, Electrostatic discharge, Security, Algorithm design and analysis, Discrete Fourier transforms, Discrete cosine transforms, Signal processing, Random variables, Spread spectrum communication, Detectors, Exhaustive watermark search, geometric attacks, template matching, watermark synchronization},
	pages = {158--161},
}

@misc{GPT4,
	title = {{GPT}-4 {API} general availability and deprecation of older models in the {Completions} {API}},
	url = {https://openai.com/blog/gpt-4-api-general-availability},
	abstract = {GPT-3.5 Turbo, DALL·E and Whisper APIs are also generally available, and we are releasing a deprecation plan for older models of the Completions API, which will retire at the beginning of 2024.},
	language = {en-US},
	urldate = {2024-03-23},
}

@article{yang_harnessing_2024,
	title = {Harnessing the {Power} of {LLMs} in {Practice}: {A} {Survey} on {ChatGPT} and {Beyond}},
	issn = {1556-4681},
	shorttitle = {Harnessing the {Power} of {LLMs} in {Practice}},
	url = {https://dl.acm.org/doi/10.1145/3649506},
	doi = {10.1145/3649506},
	abstract = {This paper presents a comprehensive and practical guide for practitioners and end-users working with Large Language Models (LLMs) in their downstream natural language processing (NLP) tasks. We provide discussions and insights into the usage of LLMs from the perspectives of models, data, and downstream tasks. Firstly, we offer an introduction and brief summary of current language models. Then, we discuss the influence of pre-training data, training data, and test data. Most importantly, we provide a detailed discussion about the use and non-use cases of large language models for various natural language processing tasks, such as knowledge-intensive tasks, traditional natural language understanding tasks, generation tasks, emergent abilities, and considerations for specific tasks. We present various use cases and non-use cases to illustrate the practical applications and limitations of LLMs in real-world scenarios. We also try to understand the importance of data and the specific challenges associated with each NLP task. Furthermore, we explore the impact of spurious biases on LLMs and delve into other essential considerations, such as efficiency, cost, and latency, to ensure a comprehensive understanding of deploying LLMs in practice. This comprehensive guide aims to provide researchers and practitioners with valuable insights and best practices for working with LLMs, thereby enabling the successful implementation of these models in a wide range of NLP tasks. A curated list of practical guide resources of LLMs, regularly updated, can be found at https://github.com/Mooler0410/LLMsPracticalGuide. An LLMs evolutionary tree, editable yet regularly updated, can be found at llmtree.ai.},
	urldate = {2024-03-23},
	journal = {ACM Transactions on Knowledge Discovery from Data},
	author = {Yang, Jingfeng and Jin, Hongye and Tang, Ruixiang and Han, Xiaotian and Feng, Qizhang and Jiang, Haoming and Zhong, Shaochen and Yin, Bing and Hu, Xia},
	month = feb,
	year = {2024},
	note = {Just Accepted},
	keywords = {Large Language Models, Neural Language Processing, Practical Guide, ChatGPT},
}


@misc{bing_rocksdb_2021,
	title = {{RocksDB} in {Microsoft} {Bing}},
	url = {https://blogs.bing.com/Engineering-Blog/october-2021/RocksDB-in-Microsoft-Bing},
	abstract = {The Microsoft Bing platform has built one of the largest distributed storages for Bing web search data, using its home grown ObjectStore service. The system hosts hundreds of petabyte data and processes hundreds of millions lookups per sec. Open source RocksDB is used as the storage engine. Multiple techniques are applied to efficiently store and process the massive data with sub-second data…},
	urldate = {2024-03-22},
	month = oct,
	year = {2021},
	file = {Snapshot:C\:\\Users\\vthakka8\\Zotero\\storage\\N8JRESVA\\RocksDB-in-Microsoft-Bing.html:text/html},
}

@misc{noauthor_foundationdb_nodate,
	title = {{FoundationDB} {\textbar} {Home}},
	url = {https://www.foundationdb.org/},
	urldate = {2024-03-22},
}

@misc{netflix_blog_application_2017,
	title = {Application data caching using {SSDs}},
	url = {https://netflixtechblog.com/application-data-caching-using-ssds-5bf25df851ef},
	abstract = {The Moneta project: Next generation EVCache for better cost optimization},
	language = {en},
	urldate = {2024-03-22},
	journal = {Medium},
	author = {Blog, Netflix Technology},
	month = apr,
	year = {2017},
	file = {Snapshot:C\:\\Users\\vthakka8\\Zotero\\storage\\3I3U5VZE\\application-data-caching-using-ssds-5bf25df851ef.html:text/html},
}

@misc{tencentpaxosstore_2024,
	title = {Tencent/paxosstore},
	url = {https://github.com/Tencent/paxosstore},
	abstract = {PaxosStore has been deployed in WeChat production for more than two years, providing storage services for the core businesses of WeChat backend. Now PaxosStore is running on thousands of machines, and is able to afford billions of peak TPS.},
	urldate = {2024-03-22},
	publisher = {Tencent},
	month = mar,
	year = {2024},
	note = {original-date: 2017-08-25T09:00:19Z},
	keywords = {consensus, database, distributed-database, paxos},
}

@misc{kaokao_apache_nodate,
	title = {Apache: {Big} {Data} 2015: {S2Graph} : {A} {Large}-{Scale} {Graph} {Database} with HBase},
	shorttitle = {Apache},
	url = {https://apachebigdata2015.sched.com/event/de6abfbd8f0b9e66b1c03feb2b9e2078?iframe=no&w=i:100;&sidebar=yes&bg=no},
	abstract = {View more about this event at Apache: Big Data 2015},
	urldate = {2024-03-22},
}

@inproceedings {meta_tectonic,
    author = {Satadru Pan and Theano Stavrinos and Yunqiao Zhang and Atul Sikaria and Pavel Zakharov and Abhinav Sharma and Shiva Shankar P and Mike Shuey and Richard Wareing and Monika Gangapuram and Guanglei Cao and Christian Preseau and Pratap Singh and Kestutis Patiejunas and JR Tipton and Ethan Katz-Bassett and Wyatt Lloyd},
    title = {Facebook{\textquoteright}s Tectonic Filesystem: Efficiency from Exascale},
    booktitle = {19th USENIX Conference on File and Storage Technologies (FAST 21)},
    year = {2021},
    isbn = {978-1-939133-20-5},
    pages = {217--231},
    url = {https://www.usenix.org/conference/fast21/presentation/pan},
    publisher = {USENIX Association},
    month = feb
}

@misc{hbase,
	title = {Apache {HBase} – {Apache} {HBase}™ {Home}},
	url = {https://hbase.apache.org/},
	urldate = {2024-03-23},
}

@article{big_table,
author = {Chang, Fay and Dean, Jeffrey and Ghemawat, Sanjay and Hsieh, Wilson C. and Wallach, Deborah A. and Burrows, Mike and Chandra, Tushar and Fikes, Andrew and Gruber, Robert E.},
title = {Bigtable: A Distributed Storage System for Structured Data},
year = {2008},
issue_date = {June 2008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {26},
number = {2},
issn = {0734-2071},
url = {https://doi.org/10.1145/1365815.1365816},
doi = {10.1145/1365815.1365816},
abstract = {Bigtable is a distributed storage system for managing structured data that is designed to scale to a very large size: petabytes of data across thousands of commodity servers. Many projects at Google store data in Bigtable, including web indexing, Google Earth, and Google Finance. These applications place very different demands on Bigtable, both in terms of data size (from URLs to web pages to satellite imagery) and latency requirements (from backend bulk processing to real-time data serving). Despite these varied demands, Bigtable has successfully provided a flexible, high-performance solution for all of these Google products. In this article, we describe the simple data model provided by Bigtable, which gives clients dynamic control over data layout and format, and we describe the design and implementation of Bigtable.},
journal = {ACM Trans. Comput. Syst.},
month = {jun},
articleno = {4},
numpages = {26},
keywords = {Large-Scale Distributed Storage}
}


@misc{liu_exploring_2024,
	title = {Exploring and {Evaluating} {Hallucinations} in {LLM}-{Powered} {Code} {Generation}},
	url = {http://arxiv.org/abs/2404.00971},
	doi = {10.48550/arXiv.2404.00971},
	abstract = {The rise of Large Language Models (LLMs) has significantly advanced many applications on software engineering tasks, particularly in code generation. Despite the promising performance, LLMs are prone to generate hallucinations, which means LLMs might produce outputs that deviate from users' intent, exhibit internal inconsistencies, or misalign with the factual knowledge, making the deployment of LLMs potentially risky in a wide range of applications. Existing work mainly focuses on investing the hallucination in the domain of natural language generation (NLG), leaving a gap in understanding the types and extent of hallucinations in the context of code generation. To bridge the gap, we conducted a thematic analysis of the LLM-generated code to summarize and categorize the hallucinations present in it. Our study established a comprehensive taxonomy of hallucinations in LLM-generated code, encompassing 5 primary categories of hallucinations depending on the conflicting objectives and varying degrees of deviation observed in code generation. Furthermore, we systematically analyzed the distribution of hallucinations, exploring variations among different LLMs and their correlation with code correctness. Based on the results, we proposed HalluCode, a benchmark for evaluating the performance of code LLMs in recognizing hallucinations. Hallucination recognition and mitigation experiments with HalluCode and HumanEval show existing LLMs face great challenges in recognizing hallucinations, particularly in identifying their types, and are hardly able to mitigate hallucinations. We believe our findings will shed light on future research about hallucination evaluation, detection, and mitigation, ultimately paving the way for building more effective and reliable code LLMs in the future.},
	urldate = {2024-04-06},
	publisher = {arXiv},
	author = {Liu, Fang and Liu, Yang and Shi, Lin and Huang, Houkun and Wang, Ruifeng and Yang, Zhen and Zhang, Li},
	month = apr,
	year = {2024},
	note = {arXiv:2404.00971 [cs]},
	keywords = {Computer Science - Software Engineering, Computer Science - Artificial Intelligence},
}


@misc{cassandra,
	title = {Apache {Cassandra} {\textbar} {Apache} {Cassandra} {Documentation}},
	url = {https://cassandra.apache.org/_/index.html},
	abstract = {Open Source NoSQL Database Manage massive amounts of data, fast, without losing sleep},
	language = {en},
	urldate = {2024-04-03},
}


@inproceedings{li_elastic_2021,
	address = {New York, NY, USA},
	series = {{CIKM} '21},
	title = {Elastic and {Stable} {Compaction} for {LSM}-tree: {A} {FaaS}-{Based} {Approach} on {TerarkDB}},
	isbn = {978-1-4503-8446-9},
	shorttitle = {Elastic and {Stable} {Compaction} for {LSM}-tree},
	url = {https://doi.org/10.1145/3459637.3481913},
	doi = {10.1145/3459637.3481913},
	abstract = {LSM-tree is widely used as a write-optimized storage engine in many NoSQL systems. However, the periodical compaction operations in LSM-tree cost many I/O bandwidths and CPU resources of the local server, resulting in throughput drops of the system. To address this issue, this paper proposes a new compaction scheme based on the FaaS (Functions as a Service) architecture, which is called FaaS Compaction. It utilizes the elastic computing capability of FaaS and always pushes compactions to a FaaS cluster. The FaaS cluster will perform actual compaction operations, which will not affect the processing of the local server. Therefore, we can maintain stable performance even when periodical compactions are triggered. We also present a Parallel Slight Compaction method to solve the timeout problem caused by heavy compactions. We implement the FaaS Compaction based on TerarkDB and a real FaaS cluster and experimentally compare the FaaS Compaction with the RocksDB's local compaction scheme and the state-of-the-art offloading compaction policy. The results suggest the efficiency, stability, and elasticity of our proposal.},
	urldate = {2024-04-02},
	booktitle = {Proceedings of the 30th {ACM} {International} {Conference} on {Information} \& {Knowledge} {Management}},
	publisher = {Association for Computing Machinery},
	author = {Li, Jianchuan and Jin, Peiquan and Lin, Yuanjin and Zhao, Ming and Wang, Yi and Guo, Kuankuan},
	month = oct,
	year = {2021},
	keywords = {compaction, elastic computing, faas, key-value store, lsm-tree},
	pages = {3906--3915},
}

@misc{rocksdb_cloud_remote_copmpaction,
	title = {Remote {Compactions} in {RocksDB}-{Cloud}},
	url = {https://www.rockset.com/blog/remote-compactions-in-rocksdb-cloud/},
	abstract = {We modified RocksDB-Cloud to allow remote compactions in order to optimize RocksDB for cloud environments.},
	language = {en},
	urldate = {2024-04-03},
	file = {Snapshot:C\:\\Users\\vthakka8\\Zotero\\storage\\DIB6KPXD\\remote-compactions-in-rocksdb-cloud.html:text/html},
}

@inproceedings{huang_nova-lsm_2021,
	address = {New York, NY, USA},
	series = {{SIGMOD} '21},
	title = {Nova-{LSM}: {A} {Distributed}, {Component}-based {LSM}-tree {Key}-value {Store}},
	isbn = {978-1-4503-8343-1},
	shorttitle = {Nova-{LSM}},
	url = {https://dl.acm.org/doi/10.1145/3448016.3457297},
	doi = {10.1145/3448016.3457297},
	abstract = {The cloud infrastructure motivates disaggregation of monolithic data stores into components that are assembled together based on an application's workload. This study investigates disaggregation of an LSM-tree key-value store into components that communicate using RDMA. These components separate storage from processing, enabling processing components to share storage bandwidth and space. The processing components scatter blocks of a file (SSTable) across an arbitrary number of storage components and balance load across them using power-of-d. They construct ranges dynamically at runtime to parallelize compaction and enhance performance. Each component has configuration knobs that control its scalability. The resulting component-based system, Nova-LSM, is elastic. It outperforms its monolithic counterparts, both LevelDB and RocksDB, by several orders of magnitude with workloads that exhibit a skewed pattern of access to data.},
	urldate = {2024-04-02},
	booktitle = {Proceedings of the 2021 {International} {Conference} on {Management} of {Data}},
	publisher = {Association for Computing Machinery},
	author = {Huang, Haoyu and Ghandeharizadeh, Shahram},
	month = jun,
	year = {2021},
	keywords = {component-based key-value store, LSM-tree, RDMA},
	pages = {749--763},
	file = {Full Text PDF:C\:\\Users\\vthakka8\\Zotero\\storage\\HZKES4QG\\Huang and Ghandeharizadeh - 2021 - Nova-LSM A Distributed, Component-based LSM-tree .pdf:application/pdf},
}

@inproceedings{bindschaedler_hailstorm_2020,
	address = {New York, NY, USA},
	series = {{ASPLOS} '20},
	title = {Hailstorm: {Disaggregated} {Compute} and {Storage} for {Distributed} {LSM}-based {Databases}},
	isbn = {978-1-4503-7102-5},
	shorttitle = {Hailstorm},
	url = {https://doi.org/10.1145/3373376.3378504},
	doi = {10.1145/3373376.3378504},
	abstract = {Distributed LSM-based databases face throughput and latency issues due to load imbalance across instances and interference from background tasks such as flushing, compaction, and data migration. Hailstorm addresses these problems by deploying the database storage engines over a distributed filesystem that disaggregates storage from processing, enabling storage pooling and compaction offloading. Hailstorm pools storage devices within a rack, allowing each storage engine to fully utilize the aggregate rack storage capacity and bandwidth. Storage pooling successfully handles load imbalance without the need for resharding. Hailstorm offloads compaction tasks to remote nodes, distributing their impact, and improving overall system throughput and response time. We show that Hailstorm achieves load balance in many MongoDB deployments with skewed workloads, improving the average throughput by 60\%, while decreasing tail latency by as much as 5X. In workloads with range queries, Hailstorm provides up to 22X throughput improvements. Hailstorm also enables cost savings of 47-56\% in OLTP workloads.},
	urldate = {2024-04-02},
	booktitle = {Proceedings of the {Twenty}-{Fifth} {International} {Conference} on {Architectural} {Support} for {Programming} {Languages} and {Operating} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Bindschaedler, Laurent and Goel, Ashvin and Zwaenepoel, Willy},
	month = mar,
	year = {2020},
	keywords = {compaction offloading, compute, database, disaggregation, distributed, hailstorm, key-value store, mongodb, rocksdb, skew, storage, tidb, tikv, tpc-c, tpc-e, ycsb},
	pages = {301--316},
	file = {Full Text:C\:\\Users\\vthakka8\\Zotero\\storage\\BGS7TEZH\\Bindschaedler et al. - 2020 - Hailstorm Disaggregated Compute and Storage for D.pdf:application/pdf},
}


@misc{rocksdb_tuning_minefield,
	title = {Navigating the {Minefield} of {RocksDB} {Configuration} {Options} {\textbar} by {Kartik} {Khare} {\textbar} {Better} {Programming}},
	url = {https://betterprogramming.pub/navigating-the-minefield-of-rocksdb-configuration-options-246af1e1d3f9},
	urldate = {2024-04-03},
	file = {Navigating the Minefield of RocksDB Configuration Options | by Kartik Khare | Better Programming:C\:\\Users\\vthakka8\\Zotero\\storage\\IQ5WEITQ\\navigating-the-minefield-of-rocksdb-configuration-options-246af1e1d3f9.html:text/html},
}

@misc{bautin_enhancing_2019,
	title = {Enhancing {RocksDB} for {Speed} and {Scale} {\textbar} {YugabyteDB}},
	url = {https://www.yugabyte.com/blog/enhancing-rocksdb-for-speed-scale/},
	abstract = {RocksDB's speed and scalability has been optimized for efficient distributed transactions. These enhancements boosted the Yugabyte database's performance, making it suitable low-latency, high-volume tasks.},
	language = {en-US},
	urldate = {2024-04-03},
	journal = {Yugabyte},
	author = {Bautin, Mikhail and Muthukkaruppan, Kannan and Muthukkaruppan, Mikhail Bautin {and} Kannan},
	month = feb,
	year = {2019},
	file = {Snapshot:C\:\\Users\\vthakka8\\Zotero\\storage\\WFVIIWRP\\enhancing-rocksdb-for-speed-scale.html:text/html},
}


@misc{intel_rocksdb_xeon_tuning,
	title = {{RocksDB}* {Tuning} {Guide} on {Intel}® {Xeon}® {Processor} {Platforms}},
	url = {https://www.intel.com/content/www/us/en/developer/articles/guide/rocksdb-tuning-guide-on-xeon-based-system.html},
	abstract = {Learn BKMs for installing RocksDB* on Intel® Xeon® Processor Systems.},
	language = {en},
	urldate = {2024-04-03},
	journal = {Intel},
	file = {Snapshot:C\:\\Users\\vthakka8\\Zotero\\storage\\QNCAMRDB\\rocksdb-tuning-guide-on-xeon-based-system.html:text/html},
}

@article{kim_robust_2020,
	title = {Robust and efficient memory management in {Apache} {AsterixDB}},
	volume = {50},
	copyright = {© 2020 John Wiley \& Sons, Ltd.},
	issn = {1097-024X},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/spe.2799},
	doi = {10.1002/spe.2799},
	abstract = {Traditional relational database systems handle data by dividing their memory into sections such as a buffer cache and working memory, assigning a memory budget to each section to efficiently manage a limited amount of overall memory. They also assign memory budgets to memory-intensive operators such as sorts and joins and control the allocation of memory to these operators; each memory-intensive operator attempts to maximize its memory usage to reduce disk I/O cost. Implementing such memory-intensive operators requires a careful design and application of appropriate algorithms that properly utilize memory. Today's Big Data management systems need the ability to handle large amounts of data similarly, as it is unrealistic to assume that truly big data will fit into memory. In this article, we share our memory management experiences in Apache AsterixDB, an open-source Big Data management software platform that scales out horizontally on shared-nothing commodity computing clusters. We describe the implementation of AsterixDB's memory-intensive operators and their designs related to memory management. We also discuss memory management at the global (cluster) level. We conducted an experimental study using several synthetic and real datasets to explore the impact of this work. We believe that future Big Data management system builders can benefit from these experiences.},
	language = {en},
	number = {7},
	urldate = {2024-04-03},
	journal = {Software: Practice and Experience},
	author = {Kim, Taewoo and Behm, Alexander and Blow, Michael and Borkar, Vinayak and Bu, Yingyi and Carey, Michael J. and Hubail, Murtadha and Jahangiri, Shiva and Jia, Jianfeng and Li, Chen and Luo, Chen and Maxon, Ian and Pirzadeh, Pouria},
	year = {2020},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/spe.2799},
	keywords = {Apache AsterixDB, big data management system, group by, hash join, inverted-index search, memory management, sort},
	pages = {1114--1151},
	file = {Full Text PDF:C\:\\Users\\vthakka8\\Zotero\\storage\\YXHQEECB\\Kim et al. - 2020 - Robust and efficient memory management in Apache A.pdf:application/pdf;Snapshot:C\:\\Users\\vthakka8\\Zotero\\storage\\VVL7ETR2\\spe.html:text/html},
}

@article{luo_breaking_2020,
	title = {Breaking down memory walls: adaptive memory management in {LSM}-based storage systems},
	volume = {14},
	issn = {2150-8097},
	shorttitle = {Breaking down memory walls},
	url = {https://dl.acm.org/doi/10.14778/3430915.3430916},
	doi = {10.14778/3430915.3430916},
	abstract = {Log-Structured Merge-trees (LSM-trees) have been widely used in modern NoSQL systems. Due to their out-of-place update design, LSM-trees have introduced memory walls among the memory components of multiple LSM-trees and between the write memory and the buffer cache. Optimal memory allocation among these regions is non-trivial because it is highly workload-dependent. Existing LSM-tree implementations instead adopt static memory allocation schemes due to their simplicity and robustness, sacrificing performance. In this paper, we attempt to break down these memory walls in LSM-based storage systems. We first present a memory management architecture that enables adaptive memory management. We then present a partitioned memory component structure with new flush policies to better exploit the write memory to minimize the write cost. To break down the memory wall between the write memory and the buffer cache, we further introduce a memory tuner that tunes the memory allocation between these two regions. We have conducted extensive experiments in the context of Apache AsterixDB using the YCSB and TPC-C benchmarks and we present the results here.},
	number = {3},
	urldate = {2024-04-03},
	journal = {Proceedings of the VLDB Endowment},
	author = {Luo, Chen and Carey, Michael J.},
	month = nov,
	year = {2020},
	pages = {241--254},
	file = {Full Text PDF:C\:\\Users\\vthakka8\\Zotero\\storage\\J3LSBKNI\\Luo and Carey - 2020 - Breaking down memory walls adaptive memory manage.pdf:application/pdf},
}


@inproceedings{dayan_monkey_2017,
	address = {New York, NY, USA},
	series = {{SIGMOD} '17},
	title = {Monkey: {Optimal} {Navigable} {Key}-{Value} {Store}},
	isbn = {978-1-4503-4197-4},
	shorttitle = {Monkey},
	url = {https://dl.acm.org/doi/10.1145/3035918.3064054},
	doi = {10.1145/3035918.3064054},
	abstract = {In this paper, we show that key-value stores backed by an LSM-tree exhibit an intrinsic trade-off between lookup cost, update cost, and main memory footprint, yet all existing designs expose a suboptimal and difficult to tune trade-off among these metrics. We pinpoint the problem to the fact that all modern key-value stores suboptimally co-tune the merge policy, the buffer size, and the Bloom filters' false positive rates in each level. We present Monkey, an LSM-based key-value store that strikes the optimal balance between the costs of updates and lookups with any given main memory budget. The insight is that worst-case lookup cost is proportional to the sum of the false positive rates of the Bloom filters across all levels of the LSM-tree. Contrary to state-of-the-art key-value stores that assign a fixed number of bits-per-element to all Bloom filters, Monkey allocates memory to filters across different levels so as to minimize this sum. We show analytically that Monkey reduces the asymptotic complexity of the worst-case lookup I/O cost, and we verify empirically using an implementation on top of LevelDB that Monkey reduces lookup latency by an increasing margin as the data volume grows (50\%-80\% for the data sizes we experimented with). Furthermore, we map the LSM-tree design space onto a closed-form model that enables co-tuning the merge policy, the buffer size and the filters' false positive rates to trade among lookup cost, update cost and/or main memory, depending on the workload (proportion of lookups and updates), the dataset (number and size of entries), and the underlying hardware (main memory available, disk vs. flash). We show how to use this model to answer what-if design questions about how changes in environmental parameters impact performance and how to adapt the various LSM-tree design elements accordingly.},
	urldate = {2024-04-03},
	booktitle = {Proceedings of the 2017 {ACM} {International} {Conference} on {Management} of {Data}},
	publisher = {Association for Computing Machinery},
	author = {Dayan, Niv and Athanassoulis, Manos and Idreos, Stratos},
	month = may,
	year = {2017},
	keywords = {adaptivity, auto-tuning, bloom filters, key-value store, log-structured merge-tree, lsm-tree, memory hierarchy, point lookups, point queries, read/write/memory trade-off},
	pages = {79--94},
	file = {Full Text PDF:C\:\\Users\\vthakka8\\Zotero\\storage\\A23U7P93\\Dayan et al. - 2017 - Monkey Optimal Navigable Key-Value Store.pdf:application/pdf},
}

@conference {Dostoevsky,
	title = {Dostoevsky: Better Space-Time Trade-Offs for LSM-Tree Based Key-Value Stores via Adaptive Removal of Superfluous Merging},
	booktitle = {ACM SIGMOD International Conference on Management of Data},
	year = {2018},
	abstract = {We show that all mainstream LSM-tree based key-value stores in the literature and in industry suboptimally trade between the I/O cost of updates on one hand and the I/O cost of lookups and storage space on the other. The reason is that they perform equally expensive merge operations across all levels of LSM-tree to bound the number of runs that a lookup has to probe and to remove obsolete entries to reclaim storage space. With state-of-the-art designs, however, merge operations from all levels of LSM-tree but the largest (i.e., most merge operations) reduce point lookup cost, long range lookup cost, and storage space by a negligible amount while significantly adding to the amortized cost of updates.To address this problem, we introduce Lazy Leveling, a new design that removes merge operations from all levels of LSM-tree but the largest. Lazy Leveling improves the worst-case complexity of update cost while maintaining the same bounds on point lookup cost, long range lookup cost, and storage space. We further introduce Fluid LSM-tree, a generalization of the entire LSM-tree design space that can be parameterized to assume any existing design. Relative to Lazy Leveling, Fluid LSM-tree can optimize more for updates by merging less at the largest level, or it can optimize more for short range lookups by merging more at all other levels.We put everything together to design Dostoevsky, a key-value store that adaptively removes superfluous merging by navigating the Fluid LSM-tree design space based on the application workload and hardware. We implemented Dostoevsky on top of RocksDB, and we show that it strictly dominates state-of-the-art designs in terms of performance and storage space.},
	author = {Niv Dayan and Stratos Idreos}
}


@inproceedings{ma_query-based_2018,
	address = {New York, NY, USA},
	series = {{SIGMOD} '18},
	title = {Query-based {Workload} {Forecasting} for {Self}-{Driving} {Database} {Management} {Systems}},
	isbn = {978-1-4503-4703-7},
	url = {https://dl.acm.org/doi/10.1145/3183713.3196908},
	doi = {10.1145/3183713.3196908},
	abstract = {The first step towards an autonomous database management system (DBMS) is the ability to model the target application's workload. This is necessary to allow the system to anticipate future workload needs and select the proper optimizations in a timely manner. Previous forecasting techniques model the resource utilization of the queries. Such metrics, however, change whenever the physical design of the database and the hardware resources change, thereby rendering previous forecasting models useless. We present a robust forecasting framework called QueryBot 5000 that allows a DBMS to predict the expected arrival rate of queries in the future based on historical data. To better support highly dynamic environments, our approach uses the logical composition of queries in the workload rather than the amount of physical resources used for query execution. It provides multiple horizons (short- vs. long-term) with different aggregation intervals. We also present a clustering-based technique for reducing the total number of forecasting models to maintain. To evaluate our approach, we compare our forecasting models against other state-of-the-art models on three real-world database traces. We implemented our models in an external controller for PostgreSQL and MySQL and demonstrate their effectiveness in selecting indexes.},
	urldate = {2024-04-03},
	booktitle = {Proceedings of the 2018 {International} {Conference} on {Management} of {Data}},
	publisher = {Association for Computing Machinery},
	author = {Ma, Lin and Van Aken, Dana and Hefny, Ahmed and Mezerhane, Gustavo and Pavlo, Andrew and Gordon, Geoffrey J.},
	month = may,
	year = {2018},
	keywords = {autonomic computing, autonomous dbms, database management systems, machine learning, query forecasting, workload prediction},
	pages = {631--645},
	file = {Full Text PDF:C\:\\Users\\vthakka8\\Zotero\\storage\\U24V5C8I\\Ma et al. - 2018 - Query-based Workload Forecasting for Self-Driving .pdf:application/pdf},
}

@inproceedings{yu_adoc_2023,
	title = {\{{ADOC}\}: {Automatically} {Harmonizing} {Dataflow} {Between} {Components} in \{{Log}-{Structured}\} \{{Key}-{Value}\} {Stores} for {Improved} {Performance}},
	isbn = {978-1-939133-32-8},
	shorttitle = {\{{ADOC}\}},
	url = {https://www.usenix.org/conference/fast23/presentation/yu},
	language = {en},
	urldate = {2024-04-03},
	author = {Yu, Jinghuan and Noh, Sam H. and Choi, Young-ri and Xue, Chun Jason},
	year = {2023},
	pages = {65--80},
	file = {Full Text PDF:C\:\\Users\\vthakka8\\Zotero\\storage\\TBQCEIM6\\Yu et al. - 2023 - ADOC Automatically Harmonizing Dataflow Between.pdf:application/pdf},
}

@inproceedings{wu_ac-key_2020,
	title = {\{{AC}-{Key}\}: {Adaptive} {Caching} for \{{LSM}-based\} \{{Key}-{Value}\} {Stores}},
	isbn = {978-1-939133-14-4},
	shorttitle = {\{{AC}-{Key}\}},
	url = {https://www.usenix.org/conference/atc20/presentation/wu-fenggang},
	language = {en},
	urldate = {2024-04-03},
	author = {Wu, Fenggang and Yang, Ming-Hong and Zhang, Baoquan and Du, David H. C.},
	year = {2020},
	pages = {603--615},
	file = {Full Text PDF:C\:\\Users\\vthakka8\\Zotero\\storage\\QR9C7YG4\\Wu et al. - 2020 - AC-Key Adaptive Caching for LSM-based Key-Va.pdf:application/pdf},
}

@misc{prompt_engineering_openai,
	title = {{OpenAI} {Platform}},
	url = {https://platform.openai.com},
	abstract = {Explore developer resources, tutorials, API docs, and dynamic examples to get the most out of OpenAI's platform.},
	language = {en},
	urldate = {2024-04-06},
	file = {Snapshot:C\:\\Users\\vthakka8\\Zotero\\storage\\HSS5MEDJ\\prompt-engineering.html:text/html},
}


@misc{elmo-tune,
	title = {asu-idi/ELMo-Tune},
	url = {https://github.com/asu-idi/ELMo-Tune},
	urldate = {2024-06-02},
}

@misc{open-sesame,
	title = {asu-idi/ELMo-Tune-V2},
	url = {https://github.com/asu-idi/ELMo-Tune-V2},
	urldate = {2025-14-01},
}

@inproceedings{cao2024sas,
  title={SAS-Cache: A Semantic-Aware Secondary Cache for LSM-based Key-Value Stores},
  author={Cao, Zhang and Guo, Chang and Lv, Ziyuan and Ananthabhotla, Anand and Cao, Zhichao},
  booktitle={38th Intl. Conf. on Massive Storage Systems and Technology},
  year={2024}
}

@inproceedings{liu2024prophet,
  title={Prophet: Optimizing LSM-Based Key-Value Store on ZNS SSDs with File Lifetime Prediction and Compaction Compensation},
  author={Liu, Gaoji and Yang, Chongzhuo and Yu, Qiaolin and Guo, Chang and Xia, Wen and Cao, Zhichao},
  booktitle={38th Intl. Conf. on Massive Storage Systems and Technology},
  year={2024}
}

@inproceedings{cao2023smrts,
  title={SMRTS: A Performance and Cost-Effectiveness Optimized SSD-SMR Tiered File System with Data Deduplication},
  author={Cao, Zhichao and Wen, Hao and Wu, Fenggang and Du, David HC},
  booktitle={2023 IEEE 41st International Conference on Computer Design (ICCD)},
  pages={275--282},
  year={2023},
  organization={IEEE}
}

@inproceedings{wen2023k8ses,
  title={K8sES: Optimizing Kubernetes with Enhanced Storage Service-Level Objectives},
  author={Wen, Hao and Cao, Zhichao and Li, Bingzhe and Du, David HC and Abouelwafa, Ayman and Voigt, Doug and Liu, Shiyong and Diehl, Jim and Wu, Fenggang},
  booktitle={2023 IEEE 41st International Conference on Computer Design (ICCD)},
  pages={214--222},
  year={2023},
  organization={IEEE}
}


@article{kassa2022power,
  title={Power-optimized Deployment of Key-value Stores Using Storage Class Memory},
  author={Kassa, Hiwot Tadese and Akers, Jason and Ghosh, Mrinmoy and Cao, Zhichao and Gogte, Vaibhav and Dreslinski, Ronald},
  journal={ACM Transactions on Storage (TOS)},
  volume={18},
  number={2},
  pages={1--26},
  year={2022},
  publisher={ACM New York, NY}
}

@article{cao2022hbase,
  title={IS-HBase: An In-Storage Computing Optimized HBase with I/O Offloading and Self-Adaptive Caching in Compute-Storage Disaggregated Infrastructure},
  author={Cao, Zhichao and Dong, Huibing and Wei, Yixun and Liu, Shiyong and Du, David HC},
  journal={ACM Transactions on Storage (TOS)},
  volume={18},
  number={2},
  pages={1--42},
  year={2022},
  publisher={ACM New York, NY}
}

@article{ge2022hintstor,
  title={HintStor: A Framework to Study I/O Hints in Heterogeneous Storage},
  author={Ge, Xiongzi and Cao, Zhichao and Du, David HC and Ganesan, Pradeep and Hahn, Dennis},
  journal={ACM Transactions on Storage (TOS)},
  volume={18},
  number={2},
  pages={1--24},
  year={2022},
  publisher={ACM New York, NY}
}

@inproceedings{yuan2022focused,
  title={A Focused Garbage Collection Approach for Primary Deduplicated Storage with Low Memory Overhead},
  author={Yuan, Jingsong and Zou, Xiangyu and Xu, Han and Cao, Zhichao and Li, Shiyi and Xia, Wen and Wang, Peng and Chen, Li},
  booktitle={2022 IEEE 40th International Conference on Computer Design (ICCD)},
  pages={315--323},
  year={2022},
  organization={IEEE}
}

@inproceedings{kassa2021improving,
  title={Improving Performance of Flash Based Key-Value Stores Using Storage Class Memory as a Volatile Memory Extension.},
  author={Kassa, Hiwot Tadese and Akers, Jason and Ghosh, Mrinmoy and Cao, Zhichao and Gogte, Vaibhav and Dreslinski, Ronald G},
  booktitle={USENIX Annual Technical Conference},
  pages={821--837},
  year={2021}
}


@article{wu2020tracklace,
  title={Tracklace: Data management for interlaced magnetic recording},
  author={Wu, Fenggang and Li, Bingzhe and Zhang, Baoquan and Cao, Zhichao and Diehl, Jim and Wen, Hao and Du, David HC},
  journal={IEEE Transactions on Computers},
  volume={70},
  number={3},
  pages={347--358},
  year={2020},
  publisher={IEEE}
}

@inproceedings{yao2020matrixkv,
  title={$\{$MatrixKV$\}$: Reducing Write Stalls and Write Amplification in $\{$LSM-tree$\}$ Based $\{$KV$\}$ Stores with Matrix Container in $\{$NVM$\}$},
  author={Yao, Ting and Zhang, Yiwen and Wan, Jiguang and Cui, Qiu and Tang, Liu and Jiang, Hong and Xie, Changsheng and He, Xubin},
  booktitle={2020 USENIX Annual Technical Conference (USENIX ATC 20)},
  pages={17--31},
  year={2020}
}

@booklet{asu-idi,
	Date-Added = {2016-02-14 01:09:08 +0000},
	Title = {ASU-IDI Repository \url{https://github.com/orgs/asu-idi/repositories}}, 
        year = {Accessed 10 Oct, 2023.}
}


@misc{liu_understanding_2024,
	title = {Understanding {LLMs}: {A} {Comprehensive} {Overview} from {Training} to {Inference}},
	shorttitle = {Understanding {LLMs}},
	url = {http://arxiv.org/abs/2401.02038},
	doi = {10.48550/arXiv.2401.02038},
	abstract = {The introduction of ChatGPT has led to a significant increase in the utilization of Large Language Models (LLMs) for addressing downstream tasks. There's an increasing focus on cost-efficient training and deployment within this context. Low-cost training and deployment of LLMs represent the future development trend. This paper reviews the evolution of large language model training techniques and inference deployment technologies aligned with this emerging trend. The discussion on training includes various aspects, including data preprocessing, training architecture, pre-training tasks, parallel training, and relevant content related to model fine-tuning. On the inference side, the paper covers topics such as model compression, parallel computation, memory scheduling, and structural optimization. It also explores LLMs' utilization and provides insights into their future development.},
	urldate = {2024-06-01},
	publisher = {arXiv},
	author = {Liu, Yiheng and He, Hao and Han, Tianle and Zhang, Xu and Liu, Mengyuan and Tian, Jiaming and Zhang, Yutong and Wang, Jiaqi and Gao, Xiaohui and Zhong, Tianyang and Pan, Yi and Xu, Shaochen and Wu, Zihao and Liu, Zhengliang and Zhang, Xin and Zhang, Shu and Hu, Xintao and Zhang, Tuo and Qiang, Ning and Liu, Tianming and Ge, Bao},
	month = jan,
	year = {2024},
	note = {arXiv:2401.02038 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{openai_gpt-4_2024,
	title = {{GPT}-4 {Technical} {Report}},
	url = {http://arxiv.org/abs/2303.08774},
	doi = {10.48550/arXiv.2303.08774},
	abstract = {We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10\% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.},
	urldate = {2024-06-01},
	publisher = {arXiv},
	author = {{OpenAI} and Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and Avila, Red and Babuschkin, Igor and Balaji, Suchir and Balcom, Valerie and Baltescu, Paul and Bao, Haiming and Bavarian, Mohammad and Belgum, Jeff and Bello, Irwan and Berdine, Jake and Bernadett-Shapiro, Gabriel and Berner, Christopher and Bogdonoff, Lenny and Boiko, Oleg and Boyd, Madelaine and Brakman, Anna-Luisa and Brockman, Greg and Brooks, Tim and Brundage, Miles and Button, Kevin and Cai, Trevor and Campbell, Rosie and Cann, Andrew and Carey, Brittany and Carlson, Chelsea and Carmichael, Rory and Chan, Brooke and Chang, Che and Chantzis, Fotis and Chen, Derek and Chen, Sully and Chen, Ruby and Chen, Jason and Chen, Mark and Chess, Ben and Cho, Chester and Chu, Casey and Chung, Hyung Won and Cummings, Dave and Currier, Jeremiah and Dai, Yunxing and Decareaux, Cory and Degry, Thomas and Deutsch, Noah and Deville, Damien and Dhar, Arka and Dohan, David and Dowling, Steve and Dunning, Sheila and Ecoffet, Adrien and Eleti, Atty and Eloundou, Tyna and Farhi, David and Fedus, Liam and Felix, Niko and Fishman, Simón Posada and Forte, Juston and Fulford, Isabella and Gao, Leo and Georges, Elie and Gibson, Christian and Goel, Vik and Gogineni, Tarun and Goh, Gabriel and Gontijo-Lopes, Rapha and Gordon, Jonathan and Grafstein, Morgan and Gray, Scott and Greene, Ryan and Gross, Joshua and Gu, Shixiang Shane and Guo, Yufei and Hallacy, Chris and Han, Jesse and Harris, Jeff and He, Yuchen and Heaton, Mike and Heidecke, Johannes and Hesse, Chris and Hickey, Alan and Hickey, Wade and Hoeschele, Peter and Houghton, Brandon and Hsu, Kenny and Hu, Shengli and Hu, Xin and Huizinga, Joost and Jain, Shantanu and Jain, Shawn and Jang, Joanne and Jiang, Angela and Jiang, Roger and Jin, Haozhun and Jin, Denny and Jomoto, Shino and Jonn, Billie and Jun, Heewoo and Kaftan, Tomer and Kaiser, Łukasz and Kamali, Ali and Kanitscheider, Ingmar and Keskar, Nitish Shirish and Khan, Tabarak and Kilpatrick, Logan and Kim, Jong Wook and Kim, Christina and Kim, Yongjik and Kirchner, Jan Hendrik and Kiros, Jamie and Knight, Matt and Kokotajlo, Daniel and Kondraciuk, Łukasz and Kondrich, Andrew and Konstantinidis, Aris and Kosic, Kyle and Krueger, Gretchen and Kuo, Vishal and Lampe, Michael and Lan, Ikai and Lee, Teddy and Leike, Jan and Leung, Jade and Levy, Daniel and Li, Chak Ming and Lim, Rachel and Lin, Molly and Lin, Stephanie and Litwin, Mateusz and Lopez, Theresa and Lowe, Ryan and Lue, Patricia and Makanju, Anna and Malfacini, Kim and Manning, Sam and Markov, Todor and Markovski, Yaniv and Martin, Bianca and Mayer, Katie and Mayne, Andrew and McGrew, Bob and McKinney, Scott Mayer and McLeavey, Christine and McMillan, Paul and McNeil, Jake and Medina, David and Mehta, Aalok and Menick, Jacob and Metz, Luke and Mishchenko, Andrey and Mishkin, Pamela and Monaco, Vinnie and Morikawa, Evan and Mossing, Daniel and Mu, Tong and Murati, Mira and Murk, Oleg and Mély, David and Nair, Ashvin and Nakano, Reiichiro and Nayak, Rajeev and Neelakantan, Arvind and Ngo, Richard and Noh, Hyeonwoo and Ouyang, Long and O'Keefe, Cullen and Pachocki, Jakub and Paino, Alex and Palermo, Joe and Pantuliano, Ashley and Parascandolo, Giambattista and Parish, Joel and Parparita, Emy and Passos, Alex and Pavlov, Mikhail and Peng, Andrew and Perelman, Adam and Peres, Filipe de Avila Belbute and Petrov, Michael and Pinto, Henrique Ponde de Oliveira and {Michael} and {Pokorny} and Pokrass, Michelle and Pong, Vitchyr H. and Powell, Tolly and Power, Alethea and Power, Boris and Proehl, Elizabeth and Puri, Raul and Radford, Alec and Rae, Jack and Ramesh, Aditya and Raymond, Cameron and Real, Francis and Rimbach, Kendra and Ross, Carl and Rotsted, Bob and Roussez, Henri and Ryder, Nick and Saltarelli, Mario and Sanders, Ted and Santurkar, Shibani and Sastry, Girish and Schmidt, Heather and Schnurr, David and Schulman, John and Selsam, Daniel and Sheppard, Kyla and Sherbakov, Toki and Shieh, Jessica and Shoker, Sarah and Shyam, Pranav and Sidor, Szymon and Sigler, Eric and Simens, Maddie and Sitkin, Jordan and Slama, Katarina and Sohl, Ian and Sokolowsky, Benjamin and Song, Yang and Staudacher, Natalie and Such, Felipe Petroski and Summers, Natalie and Sutskever, Ilya and Tang, Jie and Tezak, Nikolas and Thompson, Madeleine B. and Tillet, Phil and Tootoonchian, Amin and Tseng, Elizabeth and Tuggle, Preston and Turley, Nick and Tworek, Jerry and Uribe, Juan Felipe Cerón and Vallone, Andrea and Vijayvergiya, Arun and Voss, Chelsea and Wainwright, Carroll and Wang, Justin Jay and Wang, Alvin and Wang, Ben and Ward, Jonathan and Wei, Jason and Weinmann, C. J. and Welihinda, Akila and Welinder, Peter and Weng, Jiayi and Weng, Lilian and Wiethoff, Matt and Willner, Dave and Winter, Clemens and Wolrich, Samuel and Wong, Hannah and Workman, Lauren and Wu, Sherwin and Wu, Jeff and Wu, Michael and Xiao, Kai and Xu, Tao and Yoo, Sarah and Yu, Kevin and Yuan, Qiming and Zaremba, Wojciech and Zellers, Rowan and Zhang, Chong and Zhang, Marvin and Zhao, Shengjia and Zheng, Tianhao and Zhuang, Juntang and Zhuk, William and Zoph, Barret},
	month = mar,
	year = {2024},
	note = {arXiv:2303.08774 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
}

@misc{minaee_large_2024,
	title = {Large {Language} {Models}: {A} {Survey}},
	shorttitle = {Large {Language} {Models}},
	url = {http://arxiv.org/abs/2402.06196},
	doi = {10.48550/arXiv.2402.06196},
	abstract = {Large Language Models (LLMs) have drawn a lot of attention due to their strong performance on a wide range of natural language tasks, since the release of ChatGPT in November 2022. LLMs' ability of general-purpose language understanding and generation is acquired by training billions of model's parameters on massive amounts of text data, as predicted by scaling laws {\textbackslash}cite\{kaplan2020scaling,hoffmann2022training\}. The research area of LLMs, while very recent, is evolving rapidly in many different ways. In this paper, we review some of the most prominent LLMs, including three popular LLM families (GPT, LLaMA, PaLM), and discuss their characteristics, contributions and limitations. We also give an overview of techniques developed to build, and augment LLMs. We then survey popular datasets prepared for LLM training, fine-tuning, and evaluation, review widely used LLM evaluation metrics, and compare the performance of several popular LLMs on a set of representative benchmarks. Finally, we conclude the paper by discussing open challenges and future research directions.},
	urldate = {2024-06-01},
	publisher = {arXiv},
	author = {Minaee, Shervin and Mikolov, Tomas and Nikzad, Narjes and Chenaghlu, Meysam and Socher, Richard and Amatriain, Xavier and Gao, Jianfeng},
	month = feb,
	year = {2024},
	note = {arXiv:2402.06196 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
}

@inproceedings{gan_modeling_network,
    author = {Lin, Zinan and Jain, Alankar and Wang, Chen and Fanti, Giulia and Sekar, Vyas},
    title = {Using GANs for Sharing Networked Time Series Data: Challenges, Initial Promise, and Open Questions},
    year = {2020},
    isbn = {9781450381383},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3419394.3423643},
    doi = {10.1145/3419394.3423643},
    abstract = {Limited data access is a longstanding barrier to data-driven research and development in the networked systems community. In this work, we explore if and how generative adversarial networks (GANs) can be used to incentivize data sharing by enabling a generic framework for sharing synthetic datasets with minimal expert knowledge. As a specific target, our focus in this paper is on time series datasets with metadata (e.g., packet loss rate measurements with corresponding ISPs). We identify key challenges of existing GAN approaches for such workloads with respect to fidelity (e.g., long-term dependencies, complex multidimensional relationships, mode collapse) and privacy (i.e., existing guarantees are poorly understood and can sacrifice fidelity). To improve fidelity, we design a custom workflow called DoppelGANger (DG) and demonstrate that across diverse real-world datasets (e.g., bandwidth measurements, cluster requests, web sessions) and use cases (e.g., structural characterization, predictive modeling, algorithm comparison), DG achieves up to 43\% better fidelity than baseline models. Although we do not resolve the privacy problem in this work, we identify fundamental challenges with both classical notions of privacy and recent advances to improve the privacy properties of GANs, and suggest a potential roadmap for addressing these challenges. By shedding light on the promise and challenges, we hope our work can rekindle the conversation on workflows for data sharing.},
    booktitle = {Proceedings of the ACM Internet Measurement Conference},
    pages = {464–483},
    numpages = {20},
    keywords = {time series, synthetic data generation, privacy, generative adversarial networks},
    location = {Virtual Event, USA},
    series = {IMC '20}
}