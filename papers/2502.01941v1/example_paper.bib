@misc{chunkkv,
      title={ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference}, 
      author={Xiang Liu and Zhenheng Tang and Peijie Dong and Zeyu Li and Bo Li and Xuming Hu and Xiaowen Chu},
      year={2025},
      eprint={2502.00299},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2502.00299}, 
}

@inproceedings{fei-etal-2024-extending,
    title = "Extending Context Window of Large Language Models via Semantic Compression",
    author = "Fei, Weizhi  and
      Niu, Xueyan  and
      Zhou, Pingyi  and
      Hou, Lu  and
      Bai, Bo  and
      Deng, Lei  and
      Han, Wei",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Findings of the Association for Computational Linguistics ACL 2024",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand and virtual meeting",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-acl.306",
    doi = "10.18653/v1/2024.findings-acl.306",
    pages = "5169--5181",
    abstract = "Transformer based Large Language Models (LLMs) often impose limitations on the length of the text input to ensure the generation of fluent and relevant responses due to the quadratic complexity. These constraints restrict their applicability in long text scenarios. In this paper, we propose a novel semantic compression method that enables generalization to texts that are 6-8 times longer without incurring significant computational costs or requiring fine-tuning. Our proposed framework draws inspiration from source coding in information theory and employs a pre-trained model to reduce the semantic redundancy of long inputs before passing them to the LLMs for downstream tasks. Experimental results demonstrate that our method effectively extends the context window of LLMs across a range of tasks including question answering, summarization, few-shot learning, and information retrieval. Furthermore, the proposed semantic compression method exhibits consistent fluency in text generation while reducing the associated computational overhead.",
}

@inproceedings{jiang-etal-2024-longllmlingua,
    title = "{L}ong{LLML}ingua: Accelerating and Enhancing {LLM}s in Long Context Scenarios via Prompt Compression",
    author = "Huiqiang Jiang and Qianhui Wu and and Xufang Luo and Dongsheng Li and Chin-Yew Lin and Yuqing Yang and Lili Qiu",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.91",
    pages = "1658--1677",
}
@inproceedings{jiang-etal-2023-llmlingua,
    title = "{LLML}ingua: Compressing Prompts for Accelerated Inference of Large Language Models",
    author = "Huiqiang Jiang and Qianhui Wu and Chin-Yew Lin and Yuqing Yang and Lili Qiu",
    editor = "Bouamor, Houda  and
        Pino, Juan  and
        Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.825",
    doi = "10.18653/v1/2023.emnlp-main.825",
    pages = "13358--13376",
}

@article{wang2023recursively,
  title={Recursively summarizing enables long-term dialogue memory in large language models},
  author={Wang, Qingyue and Ding, Liang and Cao, Yanan and Tian, Zhiliang and Wang, Shi and Tao, Dacheng and Guo, Li},
  journal={arXiv preprint arXiv:2308.15022},
  year={2023}
}

@misc{zhou2023recurrentgpt,
      title={RecurrentGPT: Interactive Generation of (Arbitrarily) Long Text}, 
      author={Wangchunshu Zhou and Yuchen Eleanor Jiang and Peng Cui and Tiannan Wang and Zhenxin Xiao and Yifan Hou and Ryan Cotterell and Mrinmaya Sachan},
      year={2023},
      eprint={2305.13304},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{Chevalier2023AdaptingLM,
    title = "Adapting Language Models to Compress Contexts",
    author = "Chevalier, Alexis  and
      Wettig, Alexander  and
      Ajith, Anirudh  and
      Chen, Danqi",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.232",
    doi = "10.18653/v1/2023.emnlp-main.232",
    pages = "3829--3846",
    abstract = "Transformer-based language models (LMs) are powerful and widely-applicable tools, but their usefulness is constrained by a finite context window and the expensive computational cost of processing long text documents. We propose to adapt pre-trained LMs into AutoCompressors. These language models are capable of compressing long contexts into summary vectors, which are then accessible to the model as soft prompts. Summary vectors are trained with an unsupervised objective, whereby long documents are processed in segments, and summary vectors from all previous segments are used in language modeling. We fine-tune OPT and Llama-2 models on sequences of up to 30,720 tokens and show that AutoCompressors can utilize long contexts to improve perplexity. We evaluate AutoCompressors on in-context learning by compressing task demonstrations and find that summary vectors are good substitutes for plain-text demonstrations, increasing accuracy while reducing inference costs. Finally, we explore the benefits of pre-computing summary vectors for large corpora by applying summary vectors to retrieval-augmented language modeling and a passage re-ranking task. Overall, AutoCompressors emerge as a simple and inexpensive solution to extend the context window of LMs while speeding up inference over long contexts.",
}

@inproceedings{wingate-etal-2022-prompt,
    title = "Prompt Compression and Contrastive Conditioning for Controllability and Toxicity Reduction in Language Models",
    author = "Wingate, David  and
      Shoeybi, Mohammad  and
      Sorensen, Taylor",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2022",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-emnlp.412",
    doi = "10.18653/v1/2022.findings-emnlp.412",
    pages = "5621--5634",
    abstract = "We explore the idea of compressing the prompts used to condition language models, and show that compressed prompts can retain a substantive amount of information about the original prompt. For severely compressed prompts, while fine-grained information is lost, abstract information and general sentiments can be retained with surprisingly few parameters, which can be useful in the context of decode-time algorithms for controllability and toxicity reduction. We find that some complex prompts can be effectively compressed into a single token to guide generation. We also show that compressed prompts are largely compositional, and can be constructed such that they can be used to control independent aspects of generated text.",
}

@inproceedings{tay2020long,
  author    = {Yi Tay and
               Mostafa Dehghani and
               Samira Abnar and
               Yikang Shen and
               Dara Bahri and
               Philip Pham and
               Jinfeng Rao and
               Liu Yang and
               Sebastian Ruder and
               Donald Metzler},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/conf/iclr/Tay0ASBPRYRM21.bib},
  booktitle = {9th International Conference on Learning Representations, {ICLR} 2021,
               Virtual Event, Austria, May 3-7, 2021},
  publisher = {OpenReview.net},
  timestamp = {Wed, 23 Jun 2021 01:00:00 +0200},
  title     = {Long Range Arena : {A} Benchmark for Efficient Transformers},
  url       = {https://openreview.net/forum?id=qVyeW-grC2k},
  year      = {2021}
}

@article{hsieh2024ruler,
  author  = {Cheng-Ping Hsieh and Simeng Sun and Samuel Kriman and Shantanu Acharya and Dima Rekesh and Fei Jia and Yang Zhang and Boris Ginsburg},
  journal = {ArXiv preprint},
  title   = {RULER: What's the Real Context Size of Your Long-Context Language Models?},
  url     = {https://arxiv.org/abs/2404.06654},
  volume  = {abs/2404.06654},
  year    = {2024}
}


@article{liu2024lost,
  address   = {Cambridge, MA},
  author    = {Liu, Nelson F.  and
               Lin, Kevin  and
               Hewitt, John  and
               Paranjape, Ashwin  and
               Bevilacqua, Michele  and
               Petroni, Fabio  and
               Liang, Percy},
  doi       = {10.1162/tacl_a_00638},
  journal   = {Transactions of the Association for Computational Linguistics},
  pages     = {157--173},
  publisher = {MIT Press},
  title     = {Lost in the Middle: How Language Models Use Long Contexts},
  url       = {https://aclanthology.org/2024.tacl-1.9},
  volume    = {12},
  year      = {2024}
}

@misc{longchat,
  author = {Dacheng Li and Rulin Shao and others},
  title  = {How Long Can Open-Source {LLMs} Truly Promise on Context Length?},
  url    = {https://lmsys.org/blog/2023-06-29-longchat},
  year   = {2023}
}


@article{mohtashami2023landmark,
  author  = {Mohtashami, Amirkeivan and Jaggi, Martin},
  journal = {ArXiv preprint},
  title   = {Landmark attention: Random-access infinite context length for transformers},
  url     = {https://arxiv.org/abs/2305.16300},
  volume  = {abs/2305.16300},
  year    = {2023}
}

@article{an2023eval,
  author  = {An, Chenxin and Gong, Shansan and Zhong, Ming and Li, Mukai and Zhang, Jun and Kong, Lingpeng and Qiu, Xipeng},
  journal = {ArXiv preprint},
  title   = {L-eval: Instituting standardized evaluation for long context language models},
  url     = {https://arxiv.org/abs/2307.11088},
  volume  = {abs/2307.11088},
  year    = {2023}
}

@inproceedings{shaham2023zeroscrolls,
  address   = {Singapore},
  author    = {Shaham, Uri  and
               Ivgi, Maor  and
               Efrat, Avia  and
               Berant, Jonathan  and
               Levy, Omer},
  booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2023},
  doi       = {10.18653/v1/2023.findings-emnlp.536},
  editor    = {Bouamor, Houda  and
               Pino, Juan  and
               Bali, Kalika},
  pages     = {7977--7989},
  publisher = {Association for Computational Linguistics},
  title     = {{Z}ero{SCROLLS}: A Zero-Shot Benchmark for Long Text Understanding},
  url       = {https://aclanthology.org/2023.findings-emnlp.536},
  year      = {2023}
}


@article{zhang2024infty,
  author  = {Zhang, Xinrong and Chen, Yingfa and Hu, Shengding and Xu, Zihang and Chen, Junhao and Hao, Moo Khai and Han, Xu and Thai, Zhen Leng and Wang, Shuo and Liu, Zhiyuan and others},
  journal = {ArXiv preprint},
  title   = {$\infty$-Bench: Extending Long Context Evaluation Beyond 100K Tokens},
  url     = {https://arxiv.org/abs/2402.13718},
  volume  = {abs/2402.13718},
  year    = {2024}
}

@article{liu2024minicache,
  title={MiniCache: KV Cache Compression in Depth Dimension for Large Language Models},
  author={Liu, Akide and Liu, Jing and Pan, Zizheng and He, Yefei and Haffari, Gholamreza and Zhuang, Bohan},
  journal={arXiv preprint arXiv:2405.14366},
  year={2024}
}

@article{sun2024yoco,
  title={You only cache once: Decoder-decoder architectures for language models},
  author={Sun, Yutao and Dong, Li and Zhu, Yi and Huang, Shaohan and Wang, Wenhui and Ma, Shuming and Zhang, Quanlu and Wang, Jianyong and Wei, Furu},
  journal={arXiv preprint arXiv:2405.05254},
  year={2024}
}

@article{brandon2024reducing,
  title={Reducing Transformer Key-Value Cache Size with Cross-Layer Attention},
  author={Brandon, William and Mishra, Mayank and Nrusimha, Aniruddha and Panda, Rameswar and Kelly, Jonathan Ragan},
  journal={arXiv preprint arXiv:2405.12981},
  year={2024}
}
@misc{wu2024layercondensedkvcacheefficient,
      title={Layer-Condensed KV Cache for Efficient Inference of Large Language Models}, 
      author={Haoyi Wu and Kewei Tu},
      year={2024},
      eprint={2405.10637},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2405.10637}, 
}


@article{deepseekr1,
  title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning},
  author={Guo, Daya and Yang, Dejian and Zhang, Haowei and Song, Junxiao and Zhang, Ruoyu and Xu, Runxin and Zhu, Qihao and Ma, Shirong and Wang, Peiyi and Bi, Xiao and others},
  journal={arXiv preprint arXiv:2501.12948},
  year={2025}
}

@misc{longbenchv2,
      title={LongBench v2: Towards Deeper Understanding and Reasoning on Realistic Long-context Multitasks}, 
      author={Yushi Bai and Shangqing Tu and Jiajie Zhang and Hao Peng and Xiaozhi Wang and Xin Lv and Shulin Cao and Jiazheng Xu and Lei Hou and Yuxiao Dong and Jie Tang and Juanzi Li},
      year={2025},
      eprint={2412.15204},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.15204}, 
}


@article{longbench,
  title={Longbench: A bilingual, multitask benchmark for long context understanding},
  author={Bai, Yushi and Lv, Xin and Zhang, Jiajie and Lyu, Hongchang and Tang, Jiankai and Huang, Zhidian and Du, Zhengxiao and Liu, Xiao and Zeng, Aohan and Hou, Lei and others},
  journal={arXiv preprint arXiv:2308.14508},
  year={2023}
}
@article{young2024yi,
  author  = {Young, Alex and Chen, Bei and Li, Chao and Huang, Chengen and Zhang, Ge and Zhang, Guanwei and Li, Heng and Zhu, Jiangcheng and Chen, Jianqun and Chang, Jing and others},
  journal = {ArXiv preprint},
  title   = {Yi: Open foundation models by 01. ai},
  url     = {https://arxiv.org/abs/2403.04652},
  volume  = {abs/2403.04652},
  year    = {2024}
}


@misc{grok,
  author = {X.AI},
  title  = {Announcing Grok-1.5},
  url    = {https://x.ai/blog/grok-1.5},
  year   = {2024}
}

@misc{jamba,
  author = {AI21},
  title  = {Introducing Jamba: AI21's Groundbreaking SSM-Transformer Model},
  url    = {https://www.ai21.com/blog/announcing-jamba},
  year   = {2024}
}

@article{ge2023model,
  author  = {Ge, Suyu and Zhang, Yunan and Liu, Liyuan and Zhang, Minjia and Han, Jiawei and Gao, Jianfeng},
  journal = {ArXiv preprint},
  title   = {Model tells you what to discard: Adaptive kv cache compression for llms},
  url     = {https://arxiv.org/abs/2310.01801},
  volume  = {abs/2310.01801},
  year    = {2023}
}



@article{deepseekv3,
  title={DeepSeek-V3 Technical Report},
  author={Liu, Aixin and Feng, Bei and Xue, Bing and Wang, Bingxuan and Wu, Bochao and Lu, Chengda and Zhao, Chenggang and Deng, Chengqi and Zhang, Chenyu and Ruan, Chong and others},
  journal={arXiv preprint arXiv:2412.19437},
  year={2024}
}

@misc{deepseekv2,
  archiveprefix = {arXiv},
  author        = {DeepSeek-AI},
  eprint        = {2405.04434},
  primaryclass  = {cs.CL},
  title         = {DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model},
  year          = {2024}
}


@misc{claude3,
  author = {Anthropic},
  title  = {Introducing the next generation of Claude},
  url    = {https://www.anthropic.com/news/claude-3-family},
  year   = {2024}
}


@article{geminiteam2024gemini,
  author  = {Reid, Machel and Savinov, Nikolay and Teplyashin, Denis and Lepikhin, Dmitry and Lillicrap, Timothy and Alayrac, Jean-baptiste and Soricut, Radu and Lazaridou, Angeliki and Firat, Orhan and Schrittwieser, Julian and others},
  journal = {ArXiv preprint},
  title   = {Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context},
  url     = {https://arxiv.org/abs/2403.05530},
  volume  = {abs/2403.05530},
  year    = {2024}
}


@article{liu2024world,
  author  = {Liu, Hao and Yan, Wilson and Zaharia, Matei and Abbeel, Pieter},
  journal = {ArXiv preprint},
  title   = {World Model on Million-Length Video And Language With RingAttention},
  url     = {https://arxiv.org/abs/2402.08268},
  volume  = {abs/2402.08268},
  year    = {2024}
}

@inproceedings{chen2023longlora,
  author    = {Chen, Yukang and Qian, Shengju and Tang, Haotian and Lai, Xin and Liu, Zhijian and Han, Song and Jia, Jiaya},
  booktitle = {The Twelfth International Conference on Learning Representations},
  title     = {LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models},
  year      = {2023}
}

@inproceedings{peng2024yarn,
  author    = {Bowen Peng and Jeffrey Quesnelle and Honglu Fan and Enrico Shippole},
  booktitle = {The Twelfth International Conference on Learning Representations},
  title     = {Ya{RN}: Efficient Context Window Extension of Large Language Models},
  url       = {https://openreview.net/forum?id=wHBfxhZu1u},
  year      = {2024}
}

@inproceedings{xiong2023effective,
  address   = {Mexico City, Mexico},
  author    = {Xiong, Wenhan  and
               Liu, Jingyu  and
               Molybog, Igor  and
               Zhang, Hejia  and
               Bhargava, Prajjwal  and
               Hou, Rui  and
               Martin, Louis  and
               Rungta, Rashi  and
               Sankararaman, Karthik Abinav  and
               Oguz, Barlas  and
               Khabsa, Madian  and
               Fang, Han  and
               Mehdad, Yashar  and
               Narang, Sharan  and
               Malik, Kshitiz  and
               Fan, Angela  and
               Bhosale, Shruti  and
               Edunov, Sergey  and
               Lewis, Mike  and
               Wang, Sinong  and
               Ma, Hao},
  booktitle = {Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)},
  editor    = {Duh, Kevin  and
               Gomez, Helena  and
               Bethard, Steven},
  pages     = {4643--4663},
  publisher = {Association for Computational Linguistics},
  title     = {Effective Long-Context Scaling of Foundation Models},
  url       = {https://aclanthology.org/2024.naacl-long.260},
  year      = {2024}
}

@article{chen2023extending,
  author  = {Chen, Shouyuan and Wong, Sherman and Chen, Liangjian and Tian, Yuandong},
  journal = {ArXiv preprint},
  title   = {Extending context window of large language models via positional interpolation},
  url     = {https://arxiv.org/abs/2306.15595},
  volume  = {abs/2306.15595},
  year    = {2023}
}

@inproceedings{streamingllm,
  author    = {Guangxuan Xiao and Yuandong Tian and Beidi Chen and Song Han and Mike Lewis},
  booktitle = {The Twelfth International Conference on Learning Representations},
  title     = {Efficient Streaming Language Models with Attention Sinks},
  url       = {https://openreview.net/forum?id=NG7sS51zVF},
  year      = {2024}
}

@article{jacobs2023deepspeed,
  author  = {Sam Ade Jacobs and others},
  journal = {ArXiv preprint},
  title   = {{DeepSpeed Ulysses}: System Optimizations for Enabling Training of Extreme Long Sequence {Transformer} Models},
  url     = {https://arxiv.org/abs/2309.14509},
  volume  = {abs/2309.14509},
  year    = {2023}
}

@article{flash-attn,
  title={Flashattention: Fast and memory-efficient exact attention with io-awareness},
  author={Dao, Tri and Fu, Dan and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={16344--16359},
  year={2022}
}


@inproceedings{flash-attn2,
  title={Flash{A}ttention-2: Faster Attention with Better Parallelism and Work Partitioning},
  author={Dao, Tri},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2024}
}


@article{touvron2023llama,
  author  = {Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal = {ArXiv preprint},
  title   = {Llama: Open and efficient foundation language models},
  url     = {https://arxiv.org/abs/2302.13971},
  volume  = {abs/2302.13971},
  year    = {2023}
}

@article{touvron2023llama2,
  author  = {Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal = {ArXiv preprint},
  title   = {Llama 2: Open foundation and fine-tuned chat models},
  url     = {https://arxiv.org/abs/2307.09288},
  volume  = {abs/2307.09288},
  year    = {2023}
}



@article{tay2022unifying,
  author  = {Tay, Yi and Dehghani, Mostafa and Tran, Vinh Q and Garcia, Xavier and Bahri, Dara and Schuster, Tal and Zheng, Huaixiu Steven and Houlsby, Neil and Metzler, Donald},
  journal = {ArXiv preprint},
  title   = {Unifying Language Learning Paradigms},
  url     = {https://arxiv.org/abs/2205.05131},
  volume  = {abs/2205.05131},
  year    = {2022}
}
@article{chowdhery2022palm,
  author  = {Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal = {ArXiv preprint},
  title   = {Palm: Scaling language modeling with pathways},
  url     = {https://arxiv.org/abs/2204.02311},
  volume  = {abs/2204.02311},
  year    = {2022}
}


@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}


@article{raffel2020exploring,
  author    = {Colin Raffel and
               Noam Shazeer and
               Adam Roberts and
               Katherine Lee and
               Sharan Narang and
               Michael Matena and
               Yanqi Zhou and
               Wei Li and
               Peter J. Liu},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/journals/jmlr/RaffelSRLNMZLL20.bib},
  journal   = {J. Mach. Learn. Res.},
  pages     = {140:1--140:67},
  timestamp = {Fri, 05 Feb 2021 00:00:00 +0100},
  title     = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text
               Transformer},
  url       = {http://jmlr.org/papers/v21/20-074.html},
  volume    = {21},
  year      = {2020}
}

@inproceedings{longgenbench,
    title = "{L}ong{G}en{B}ench: Long-context Generation Benchmark",
    author = "Liu, Xiang  and
      Dong, Peijie  and
      Hu, Xuming  and
      Chu, Xiaowen",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2024",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-emnlp.48",
    pages = "865--883",
    abstract = "Current long-context benchmarks primarily focus on retrieval-based tests, requiring Large Language Models (LLMs) to locate specific information within extensive input contexts, such as the needle-in-a-haystack (NIAH) benchmark. Long-context generation refers to the ability of a language model to generate coherent and contextually accurate text that spans across lengthy passages or documents. While recent studies show strong performance on NIAH and other retrieval-based long-context benchmarks, there is a significant lack of benchmarks for evaluating long-context generation capabilities. To bridge this gap and offer a comprehensive assessment, we introduce a synthetic benchmark, LongGenBench, which allows for flexible configurations of customized generation context lengths. LongGenBench advances beyond traditional benchmarks by redesigning the format of questions and necessitating that LLMs respond with a single, cohesive long-context answer. Upon extensive evaluation using LongGenBench, we observe that: (1) both API accessed and open source models exhibit performance degradation in long-context generation scenarios, ranging from 1.2{\%} to 47.1{\%}; (2) different series of LLMs exhibit varying trends of performance degradation, with the Gemini-1.5-Flash model showing the least degradation among API accessed models, and the Qwen2 series exhibiting the least degradation in LongGenBench among open source models.",
}



@article{needle,
  author  = {Gregory Kamradt},
  journal = {Github},
  title   = {{Needle In A Haystack} - Pressure Testing {LLM}s},
  url     = {https://github.com/gkamradt/LLMTest_NeedleInAHaystack/tree/main},
  year    = {2023}
}

@article{gsm8k,
  title={Training verifiers to solve math word problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and others},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}

@inproceedings{csqa,
    title = "{C}ommonsense{QA}: A Question Answering Challenge Targeting Commonsense Knowledge",
    author = "Talmor, Alon  and
      Herzig, Jonathan  and
      Lourie, Nicholas  and
      Berant, Jonathan",
    editor = "Burstein, Jill  and
      Doran, Christy  and
      Solorio, Thamar",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1421/",
    doi = "10.18653/v1/N19-1421",
    pages = "4149--4158",
    abstract = "When answering a question, people often draw upon their rich world knowledge in addition to the particular context. Recent work has focused primarily on answering questions given some relevant document or context, and required very little general background. To investigate question answering with prior knowledge, we present CommonsenseQA: a challenging new dataset for commonsense question answering. To capture common sense beyond associations, we extract from ConceptNet (Speer et al., 2017) multiple target concepts that have the same semantic relation to a single source concept. Crowd-workers are asked to author multiple-choice questions that mention the source concept and discriminate in turn between each of the target concepts. This encourages workers to create questions with complex semantics that often require prior knowledge. We create 12,247 questions through this procedure and demonstrate the difficulty of our task with a large number of strong baselines. Our best baseline is based on BERT-large (Devlin et al., 2018) and obtains 56{\%} accuracy, well below human performance, which is 89{\%}."
}


@article{mmlu,
  title={Measuring massive multitask language understanding},
  author={Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2009.03300},
  year={2020}
}

@article{pope2023efficiently,
  title={Efficiently scaling transformer inference},
  author={Pope, Reiner and Douglas, Sholto and Chowdhery, Aakanksha and Devlin, Jacob and Bradbury, James and Heek, Jonathan and Xiao, Kefan and Agrawal, Shivani and Dean, Jeff},
  journal={Proceedings of Machine Learning and Systems},
  volume={5},
  pages={606--624},
  year={2023}
}

@article{adnan2024keyformer,
  title={Keyformer: Kv cache reduction through key tokens selection for efficient generative inference},
  author={Adnan, Muhammad and Arunkumar, Akhil and Jain, Gaurav and Nair, Prashant and Soloveychik, Ilya and Kamath, Purushotham},
  journal={Proceedings of Machine Learning and Systems},
  volume={6},
  pages={114--127},
  year={2024}
}
@article{yao2024cacheblend,
  title={CacheBlend: Fast Large Language Model Serving with Cached Knowledge Fusion},
  author={Yao, Jiayi and Li, Hanchen and Liu, Yuhan and Ray, Siddhant and Cheng, Yihua and Zhang, Qizheng and Du, Kuntai and Lu, Shan and Jiang, Junchen},
  journal={arXiv preprint arXiv:2405.16444},
  year={2024}
}
@inproceedings{zhangcam,
  title={CaM: Cache Merging for Memory-efficient LLMs Inference},
  author={Zhang, Yuxin and Du, Yuxuan and Luo, Gen and Zhong, Yunshan and Zhang, Zhenyu and Liu, Shiwei and Ji, Rongrong},
  booktitle={Forty-first International Conference on Machine Learning}
}
@article{li2024snapkv,
  title={Snapkv: Llm knows what you are looking for before generation},
  author={Li, Yuhong and Huang, Yingbing and Yang, Bowen and Venkitesh, Bharat and Locatelli, Acyr and Ye, Hanchen and Cai, Tianle and Lewis, Patrick and Chen, Deming},
  journal={arXiv preprint arXiv:2404.14469},
  year={2024}
}
@article{yang2024pyramidinfer,
  title={PyramidInfer: Pyramid KV Cache Compression for High-throughput LLM Inference},
  author={Yang, Dongjie and Han, XiaoDong and Gao, Yan and Hu, Yao and Zhang, Shilin and Zhao, Hai},
  journal={arXiv preprint arXiv:2405.12532},
  year={2024}
}

@article{guo2024attention,
  title={Attention Score is not All You Need for Token Importance Indicator in KV Cache Reduction: Value Also Matters},
  author={Guo, Zhiyu and Kamigaito, Hidetaka and Watanabe, Taro},
  journal={arXiv preprint arXiv:2406.12335},
  year={2024}
}

@inproceedings{fu2024lazyllm,
  author    = {Qichen Fu and Minsik Cho and Thomas Merth and Sachin Mehta and Mohammad Rastegari and Mahyar Najibi},
  booktitle = {Workshop on Efficient Systems for Foundation Models II @ ICML2024},
  title     = {Lazy{LLM}: Dynamic Token Pruning for Efficient Long Context {LLM} Inference},
  url       = {https://openreview.net/forum?id=gGZD1dsJqZ},
  year      = {2024}
}


@article{liu2024scissorhands,
  title={Scissorhands: Exploiting the persistence of importance hypothesis for llm kv cache compression at test time},
  author={Liu, Zichang and Desai, Aditya and Liao, Fangshuo and Wang, Weitao and Xie, Victor and Xu, Zhaozhuo and Kyrillidis, Anastasios and Shrivastava, Anshumali},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}


@article{tang2024quest,
  author  = {Tang, Jiaming and Zhao, Yilong and Zhu, Kan and Xiao, Guangxuan and Kasikci, Baris and Han, Song},
  journal = {ArXiv preprint},
  title   = {Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference},
  url     = {https://arxiv.org/abs/2406.10774},
  volume  = {abs/2406.10774},
  year    = {2024}
}


@article{feng2024ada,
  title={Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for Efficient LLM Inference},
  author={Feng, Yuan and Lv, Junlin and Cao, Yukun and Xie, Xike and Zhou, S Kevin},
  journal={arXiv preprint arXiv:2407.11550},
  year={2024}
}

@article{yuan2024kv,
  title={Kv cache compression, but what must we give in return? a comprehensive benchmark of long context capable approaches},
  author={Yuan, Jiayi and Liu, Hongyi and Zhong, Shaochen and Chuang, Yu-Neng and Li, Songchen and Wang, Guanchu and Le, Duy and Jin, Hongye and Chaudhary, Vipin and Xu, Zhaozhuo and others},
  journal={arXiv preprint arXiv:2407.01527},
  year={2024}
}
@article{lin2021truthfulqa,
  title={Truthfulqa: Measuring how models mimic human falsehoods},
  author={Lin, Stephanie and Hilton, Jacob and Evans, Owain},
  journal={arXiv preprint arXiv:2109.07958},
  year={2021}
}
@article{hartvigsen2022toxigen,
  title={Toxigen: A large-scale machine-generated dataset for adversarial and implicit hate speech detection},
  author={Hartvigsen, Thomas and Gabriel, Saadia and Palangi, Hamid and Sap, Maarten and Ray, Dipankar and Kamar, Ece},
  journal={arXiv preprint arXiv:2203.09509},
  year={2022}
}
@inproceedings{liang2021towards,
  title={Towards understanding and mitigating social biases in language models},
  author={Liang, Paul Pu and Wu, Chiyu and Morency, Louis-Philippe and Salakhutdinov, Ruslan},
  booktitle={International Conference on Machine Learning},
  pages={6565--6576},
  year={2021},
  organization={PMLR}
}
@article{zhu2023promptbench,
  title={Promptbench: Towards evaluating the robustness of large language models on adversarial prompts},
  author={Zhu, Kaijie and Wang, Jindong and Zhou, Jiaheng and Wang, Zichen and Chen, Hao and Wang, Yidong and Yang, Linyi and Ye, Wei and Zhang, Yue and Zhenqiang Gong, Neil and others},
  journal={arXiv e-prints},
  pages={arXiv--2306},
  year={2023}
}
@inproceedings{shen2024anything,
  title={" do anything now": Characterizing and evaluating in-the-wild jailbreak prompts on large language models},
  author={Shen, Xinyue and Chen, Zeyuan and Backes, Michael and Shen, Yun and Zhang, Yang},
  booktitle={Proceedings of the 2024 on ACM SIGSAC Conference on Computer and Communications Security},
  pages={1671--1685},
  year={2024}
}
@article{deng2023multilingual,
  title={Multilingual jailbreak challenges in large language models},
  author={Deng, Yue and Zhang, Wenxuan and Pan, Sinno Jialin and Bing, Lidong},
  journal={arXiv preprint arXiv:2310.06474},
  year={2023}
}
@article{li2024should,
  title={Should We Really Edit Language Models? On the Evaluation of Edited Language Models},
  author={Li, Qi and Liu, Xiang and Tang, Zhenheng and Dong, Peijie and Li, Zeyu and Pan, Xinglin and Chu, Xiaowen},
  journal={arXiv preprint arXiv:2410.18785},
  year={2024}
}
@article{yang2024qwen2,
  title={Qwen2. 5 Technical Report},
  author={Yang, An and Yang, Baosong and Zhang, Beichen and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and Wei, Haoran and others},
  journal={arXiv preprint arXiv:2412.15115},
  year={2024}
}

@article{pyramidkv,
  title={Pyramidkv: Dynamic kv cache compression based on pyramidal information funneling},
  author={Cai, Zefan and Zhang, Yichi and Gao, Bofei and Liu, Yuliang and Liu, Tianyu and Lu, Keming and Xiong, Wayne and Dong, Yue and Chang, Baobao and Hu, Junjie and others},
  journal={arXiv preprint arXiv:2406.02069},
  year={2024}
}


@article{h2o,
  title={H2o: Heavy-hitter oracle for efficient generative inference of large language models},
  author={Zhang, Zhenyu and Sheng, Ying and Zhou, Tianyi and Chen, Tianlong and Zheng, Lianmin and Cai, Ruisi and Song, Zhao and Tian, Yuandong and R{\'e}, Christopher and Barrett, Clark and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={34661--34710},
  year={2023}
}


@article{snapkv,
  author  = {Li, Yuhong and Huang, Yingbing and Yang, Bowen and Venkitesh, Bharat and Locatelli, Acyr and Ye, Hanchen and Cai, Tianle and Lewis, Patrick and Chen, Deming},
  journal = {ArXiv preprint},
  title   = {SnapKV: LLM Knows What You are Looking for Before Generation},
  url     = {https://arxiv.org/abs/2404.14469},
  volume  = {abs/2404.14469},
  year    = {2024}
}


@misc{eval-harness,
  author       = {Gao, Leo and Tow, Jonathan and Abbasi, Baber and Biderman, Stella and Black, Sid and DiPofi, Anthony and Foster, Charles and Golding, Laurence and Hsu, Jeffrey and Le Noac'h, Alain and Li, Haonan and McDonell, Kyle and Muennighoff, Niklas and Ociepa, Chris and Phang, Jason and Reynolds, Laria and Schoelkopf, Hailey and Skowron, Aviya and Sutawika, Lintang and Tang, Eric and Thite, Anish and Wang, Ben and Wang, Kevin and Zou, Andy},
  title        = {A framework for few-shot language model evaluation},
  month        = 12,
  year         = 2023,
  publisher    = {Zenodo},
  version      = {v0.4.0},
  doi          = {10.5281/zenodo.10256836},
  url          = {https://zenodo.org/records/10256836}
}

@article{jiang2023mistral,
  title={Mistral 7B},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
  journal={arXiv preprint arXiv:2310.06825},
  year={2023}
}


@article{dubey2024llama,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}


@article{agarwal2024many,
  title={Many-shot in-context learning},
  author={Agarwal, Rishabh and Singh, Avi and Zhang, Lei M and Bohnet, Bernd and Rosias, Luis and Chan, Stephanie and Zhang, Biao and Anand, Ankesh and Abbas, Zaheer and Nova, Azade and others},
  journal={arXiv preprint arXiv:2404.11018},
  year={2024}
}

@inproceedings{
luo2024jailbreakv,
title={JailBreakV: A Benchmark for Assessing the Robustness of MultiModal Large Language Models against Jailbreak Attacks},
author={Weidi Luo and Siyuan Ma and Xiaogeng Liu and Xiaoyu Guo and Chaowei Xiao},
booktitle={First Conference on Language Modeling},
year={2024},
url={https://openreview.net/forum?id=GC4mXVfquq}
}

@article{chen2021evaluating,
  title={Evaluating large language models trained on code},
  author={Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde De Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and others},
  journal={arXiv preprint arXiv:2107.03374},
  year={2021}
}
@article{austin2021program,
  title={Program synthesis with large language models},
  author={Austin, Jacob and Odena, Augustus and Nye, Maxwell and Bosma, Maarten and Michalewski, Henryk and Dohan, David and Jiang, Ellen and Cai, Carrie and Terry, Michael and Le, Quoc and others},
  journal={arXiv preprint arXiv:2108.07732},
  year={2021}
}
@article{bbh,
  title={Challenging big-bench tasks and whether chain-of-thought can solve them},
  author={Suzgun, Mirac and Scales, Nathan and Sch{\"a}rli, Nathanael and Gehrmann, Sebastian and Tay, Yi and Chung, Hyung Won and Chowdhery, Aakanksha and Le, Quoc V and Chi, Ed H and Zhou, Denny and others},
  journal={arXiv preprint arXiv:2210.09261},
  year={2022}
}
@article{bigbench,
  title={Beyond the imitation game: Quantifying and extrapolating the capabilities of language models},
  author={Srivastava, Aarohi and Rastogi, Abhinav and Rao, Abhishek and Shoeb, Abu Awal Md and Abid, Abubakar and Fisch, Adam and Brown, Adam R and Santoro, Adam and Gupta, Aditya and Garriga-Alonso, Adri{\`a} and others},
  journal={arXiv preprint arXiv:2206.04615},
  year={2022}
}

@article{math,
  title={Measuring mathematical problem solving with the math dataset},
  author={Hendrycks, Dan and Burns, Collin and Kadavath, Saurav and Arora, Akul and Basart, Steven and Tang, Eric and Song, Dawn and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2103.03874},
  year={2021}
}
@article{mathqa,
  title={Mathqa: Towards interpretable math word problem solving with operation-based formalisms},
  author={Amini, Aida and Gabriel, Saadia and Lin, Peter and Koncel-Kedziorski, Rik and Choi, Yejin and Hajishirzi, Hannaneh},
  journal={arXiv preprint arXiv:1905.13319},
  year={2019}
}