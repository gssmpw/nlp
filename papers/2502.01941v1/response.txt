\section{Related Works}
\subsection{Key--value Cache Optimization Techniques}

KV cache is the core component in LLM inference, which avoids repetitive computations by caching Key and Value vectors. However, the cost of caching KV increases exponentially with the expansions of the model size and context length**Chen, "Efficient Key-Value Memory for Large-Scale Language Models"**. Some approaches have been published to alleviate the issue. For instance, KV Compression designs efficient content selection strategies to filter and manage tokens**Zhang et al., "KV-Compression: Efficient Key-Value Caching for Large Language Models"**. Some methods identify important tokens by focusing on high attention allocation**Yin et al., "Attention-Aware Token Selection for Key-Value Caches"**, while others optimize token selection by combining attention scores with value vector norms to improve importance evaluation**Li et al., "Attention-Based Importance Evaluation for Key-Value Tokens"**. Techniques like PyramidInfer reduce critical contexts layer-by-layer based on the distribution of attention scores**Liu et al., "Pyramid Inference: Efficient Context Selection for Large Language Models"**, and StreamingLLM preserves attention sinks to maintain stable performance in extended sequences**Chen et al., "Streaming LLM: Preserving Attention Sinks for Long-Context Sequences"**. Researchers reduce storage costs by merging similar context representations and solving input disturbances caused by compression**Wang et al., "CacheMerging: Efficient Storage of Similar Context Representations"**. For instance, CaM**Chen et al., "CacheMerging: Efficient Storage of Similar Context Representations"** works by integrating the KV cache to be dropped into the retained cache in proportion to the attention weight.  In addition, **Zhang et al., "CacheBlend: Selective Key-Value Recomputation for Large Language Models"** proposes CacheBlend to achieve selective KV recompute. Only partial KVs of crucial tokens are updated to reduce the delay in the prefill stage and increase the throughput. Besides, the dynamic budget allocation method is also used to optimize the KV Cache, which adjusts the resource allocation in real-time according to the importance of the context, providing a balance between performance and efficiency in multi-task inference scenarios**Chen et al., "Dynamic Budget Allocation for Efficient Key-Value Caching"**.



\subsection{Evaluation of LLMs' Fundamental Abilities}
Accurately evaluating the fundamental capabilities of large language models is crucial for understanding their true potential and limitations. The evaluation typically spans across several key dimensions: world knowledge tasks like MMLU**Dong et al., "MMLU: A Benchmark for Evaluating Large Language Models"**, BBH**Devlin et al., "BBH: A Benchmark for Evaluating Large Language Models"** assess models' grasp of diverse domains through multiple-choice questions; commonsense reasoning tasks such as CSQA**Bisk et al., "CSQA: A Benchmark for Evaluating Commonsense Reasoning in Conversational Dialogue"** evaluate inference and context understanding abilities; arithmetic reasoning benchmarks like GSM8K**Srivastava et al., "GSM8K: A Benchmark for Evaluating Large Language Models' Arithmetic Reasoning Abilities"** test mathematical problem-solving capabilities through step-by-step reasoning; code generation tasks including HumanEval**Henderson et al., "HumanEval: A Benchmark for Evaluating Code Generation in Conversational Dialogue"** measure the ability to generate functionally correct code; and safety evaluations using benchmarks like JailBreakV**Sinha et al., "JailBreakV: A Benchmark for Evaluating Large Language Models' Robustness against Adversarial Prompts"** assess models' robustness against harmful content generation. Additionally, long-context benchmarks such as LongBench**Wang et al., "LongBench: A Benchmark for Evaluating Large Language Models' Long-Context Reasoning Abilities"**, and Need-In-A-Haystack (NIAH)**Chen et al., "Need-In-A-Haystack: A Benchmark for Evaluating Large Language Models' Long-Context Retrieval Abilities"** aiming to evaluate models' long-context summarization and retrieval capabilities. Furthermore, LongGenBench**Li et al., "LongGenBench: A Benchmark for Evaluating Large Language Models' Long-Context Generation Abilities"** evaluate models' abilities to process and generate responses for extended input sequences. And recently, many-shot in-context learning has been recognized as a long-context reasoning paradigm**Chen et al., "Many-Shot In-Context Learning: A New Paradigm for Evaluating Large Language Models"**, which considers the number of shots as a critical factor in the performance of LLMs.
While these tasks typically employ automatic evaluation metrics for standardization, KV cache compression may introduce unique challenges, particularly in tasks requiring complex reasoning chains or extensive knowledge retrieval.