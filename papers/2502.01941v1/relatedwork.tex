\section{Related Works}
\subsection{Key--value Cache Optimization Techniques}

KV cache is the core component in LLM inference, which avoids repetitive computations by caching Key and Value vectors. However, the cost of caching KV increases exponentially with the expansions of the model size and context length~\cite{pope2023efficiently}. Some approaches have been published to alleviate the issue. For instance, KV Compression designs efficient content selection strategies to filter and manage tokens~\cite{h2o,adnan2024keyformer}. Some methods identify important tokens by focusing on high attention allocation~\cite{snapkv}, while others optimize token selection by combining attention scores with value vector norms to improve importance evaluation~\cite{guo2024attention}. Techniques like PyramidInfer reduce critical contexts layer-by-layer based on the distribution of attention scores~\cite{yang2024pyramidinfer}, and StreamingLLM preserves attention sinks to maintain stable performance in extended sequences~\cite{streamingllm}. Researchers reduce storage costs by merging similar context representations and solving input disturbances caused by compression~\cite{zhangcam}. For instance, CaM~\cite{zhangcam} works by integrating the KV cache to be dropped into the retained cache in proportion to the attention weight.  In addition, \citet{yao2024cacheblend} proposes CacheBlend to achieve selective KV recompute. Only partial KVs of crucial tokens are updated to reduce the delay in the prefill stage and increase the throughput. Besides, the dynamic budget allocation method is also used to optimize the KV Cache, which adjusts the resource allocation in real-time according to the importance of the context, providing a balance between performance and efficiency in multi-task inference scenarios~\cite{pyramidkv,feng2024ada,chunkkv}.



\subsection{Evaluation of LLMs' Fundamental Abilities}
Accurately evaluating the fundamental capabilities of large language models is crucial for understanding their true potential and limitations. The evaluation typically spans across several key dimensions: world knowledge tasks like MMLU~\cite{mmlu},BBH~\cite{bbh} assess models' grasp of diverse domains through multiple-choice questions; commonsense reasoning tasks such as CSQA~\cite{csqa} evaluate inference and context understanding abilities; arithmetic reasoning benchmarks like GSM8K~\cite{gsm8k} test mathematical problem-solving capabilities through step-by-step reasoning; code generation tasks including HumanEval~\cite{chen2021evaluating} measure the ability to generate functionally correct code; and safety evaluations using benchmarks like JailBreakV~\cite{luo2024jailbreakv} assess models' robustness against harmful content generation. Additionally, long-context benchmarks such as LongBench~\cite{longbench,longbenchv2} and Need-In-A-Haystack (NIAH)~\cite{needle} aiming to evaluate models' long-context summarization and retrieval capabilities. Furthermore, LongGenBench~\cite{longgenbench} evaluate models' abilities to process and generate responses for extended input sequences. And recently, many-shot in-context learning has been recognized as a long-context reasoning paradigm~\cite{agarwal2024many}, which considers the number of shots as a critical factor in the performance of LLMs.
While these tasks typically employ automatic evaluation metrics for standardization, KV cache compression may introduce unique challenges, particularly in tasks requiring complex reasoning chains or extensive knowledge retrieval.
% LongGenBench~\cite{longgenbench} evaluate models' abilities to process and generate responses for extended input sequences. And recently, many-shot in-context learning has been recognized as a long-context reasoning paradigm~\cite{agarwal2024many}, which considers the number of shots as a critical factor in the performance of LLMs.

% \xl{add to judge longbench niah not good}

% \subsection{Evaluation of LLMs' Fundamental Abilities}
% Accurately evaluating the fundamental capabilities of large language models is crucial for understanding their true potential and limitations. This evaluation spans multiple dimensions:

% \paragraph{General Tasks}
% General tasks refer to evaluating the overall performance of LLMs under mathematical inference, logic reasoning, and common knowledge GSM8K~\cite{gsm8k} and MMLU~\cite{mmlu} are the representative tasks. The former focuses on the step-by-step reasoning ability of mathematical problem solving while the latter covers assessment of common sense and expertise in multiple areas. Besides, MATH~\cite{math} spans various mathematical fields, ranging from elementary algebra to calculus, aiming to improve the mathematical problem-solving capabilities of LLMs. Meanwhile, MathQA~\cite{mathqa} is a large-scale dataset comprising approximately 37,000 multiple-choice questions with precise annotations, designed to enhance the interpretability and performance of LLMs. In addition, BBH~\cite{bbh}, a subset of BIG-Bench~\cite{bigbench}, focuses on challenging tasks. BBH includes multi-step reasoning problems, highlighting the importance of Chain-of-Thought prompting in LLMs. Similarly, CSQA~\cite{csqa} is a task that combines knowledge graph-based multi-step reasoning with conversational capabilities. CSQA emphasizes inference and context understanding grounded in knowledge graphs. Normally, the general tasks apply automatic evaluation metrics (e.g. multi-choice accuracy) to ensure comparability and standardization. However, optimization strategies like KV cache compression may introduce challenges in executing the mentioned tasks. Filtering and dropping of contexts are involved in the compression strategy which may lead to an intermediate inference steps missing. In addition, in tasks such as MMLU that are highly dependent on knowledge coverage, compression may weaken the model's ability to capture long context or rare domain knowledge~\cite{yuan2024kv}.

% \paragraph{Security Tasks}
% Security tasks focus on assessing the robustness and protections of LLMs against harmful content, including truthfulness~\cite{lin2021truthfulqa}, toxicity~\cite{hartvigsen2022toxigen}, and bias~\cite{liang2021towards}. Recently, researchers noticed the weakness of LLMs in adversarial prompts~\cite{zhu2023promptbench}, especially in generating illegal or inappropriate content under jailbreak prompts. \citet{shen2024anything} analyze the jailbreak prompts in real cases to reveal the failure of model security mechanism under complex malicious input. Meanwhile, \citet{deng2023multilingual} demonstrates the multilingual jailbreak makes model security in low-resource languages easier to bypass, significantly increasing the probability that users of low-resource languages will generate insecure content. Similar to general tasks, KV optimization techniques can cause the model to ignore potential security threats when dealing with jailbreak prompts, thereby improving the success rate of adversarial prompts~\cite{li2024should}.

% \paragraph{Code Generation Tasks}
% Code generation tasks test the capacities of LLMs to generate code, which not only requires that the model can generate syntactic code based on natural language description but also has certain logical reasoning abilities. HumanEval~\cite{chen2021evaluating} and MBPP~\cite{austin2021program} are the commonly used benchmarks. They measure the functional correctness of the model by testing the results of the code's execution.

% \paragraph{Long-context Tasks}
% Research on Long-context tasks primarily focuses on evaluating models' capabilities in processing extended input sequences. Several benchmark datasets have emerged as standards in this domain: LongBench~\cite{longbench,longbenchv2} encompasses diverse tasks including question-answering, summarization, and comprehension, while NIAH~\cite{needle} specifically addresses long-context retrieval challenges. Additionally, many-shot in-context learning has been recognized as a long-context reasoning paradigm~\cite{agarwal2024many}. LongGenBench~\cite{longgenbench} extends this landscape by introducing a long-context generation benchmark that not only requires models to generate extensive responses but also provides zero-cost evaluation metrics for response quality assessment.