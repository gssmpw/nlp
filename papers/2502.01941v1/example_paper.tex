\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{caption}
\usepackage{amsthm}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{colortbl}
\usepackage{wrapfig}
\usepackage{makecell}
\usepackage{enumitem}

% \usepackage{etoc}

\usepackage{etoc}
\etocdepthtag.toc{mtchapter}
\etocsettagdepth{mtchapter}{subsection}
\etocsettagdepth{mtappendix}{none}


% \etocdepthtag.toc{mtchapter}
% \etocsettagdepth{mtchapter}{subsection}
% \etocsettagdepth{mtappendix}{none}
% \usepackage{tocloft}

% \usepackage{algorithm}
% \usepackage{algorithmic}
% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}
\DeclareMathOperator*{\argmax}{argmax}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\newcommand{\method}{ShotKV}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}

% \NewDocumentCommand{\xl}
% { mO{} }{\textcolor{blue}{\textsuperscript{\textit{xl}}\textsf{\textbf{\small[#1]}}}}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Can LLMs Maintain Fundamental Abilities under KV Cache Compression?}

\begin{document}

\twocolumn[
\icmltitle{Can LLMs Maintain Fundamental Abilities \\ under KV Cache Compression?}
% \icmltitle{Beyond Long Context: A Systematic Study of KV Cache \\ Compression Impact on LLMs' Fundamental Abilities}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
% \begin{icmlauthorlist}
\icmlauthor{Xiang Liu}{hkustgz}
\icmlauthor{Zhenheng Tang}{hkust}
\icmlauthor{Hong Chen}{hkustgz}
\icmlauthor{Peijie Dong}{hkustgz}
\icmlauthor{Zeyu Li}{hkustgz}
\icmlauthor{Xiuze Zhou}{hkustgz}
\icmlauthor{Bo Li}{hkust}
\icmlauthor{Xuming Hu}{hkustgz}
\icmlauthor{Xiaowen Chu}{hkustgz}
\end{icmlauthorlist}

\icmlaffiliation{hkustgz}{The Hong Kong University of Science and Technology(Guangzhou), Guangzhou, China}
\icmlaffiliation{hkust}{The Hong Kong University of Science and Technology, Hong Kong, China}

\icmlcorrespondingauthor{Xuming Hu}{xuminghu@hkust-gz.edu.cn}
\icmlcorrespondingauthor{Xiaowen Chu}{xwchu@hkust-gz.edu.cn}

% \icmlkeywords{Large Language Models, KV Cache Compression, Model Evaluation}

\vskip 0.3in
]

% \printAffiliationsAndNotice{}

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution

% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
\vspace{-0.1cm}
    % Safety tasks demonstrate varying sensitivity with degradation ranging from $1.1\%$ to $40.3\%$.
This paper investigates an under-explored challenge in large language models (LLMs): the impact of KV cache compression methods on LLMs' fundamental capabilities. While existing methods achieve impressive compression ratios on long-context benchmarks, their effects on core model capabilities remain understudied. We present a comprehensive empirical study evaluating prominent KV cache compression methods across diverse tasks, spanning world knowledge, commonsense reasoning, arithmetic reasoning, code generation, safety, and long-context understanding and generation.Our analysis reveals that KV cache compression methods exhibit task-specific performance degradation. Arithmetic reasoning tasks prove particularly sensitive to aggressive compression, with different methods showing performance drops of $17.4\%$-$43.3\%$. Notably, the DeepSeek R1 Distill model exhibits more robust compression tolerance compared to instruction-tuned models, showing only $9.67\%$-$25.53\%$ performance degradation.
Based on our analysis of attention patterns and cross-task compression performance, we propose \method{}, a novel compression approach that distinctly handles prefill and decoding phases while maintaining shot-level semantic coherence. Empirical results show that \method{} achieves $9\%$-$18\%$ performance improvements on long-context generation tasks under aggressive compression ratios.

\end{abstract}


\section{Introduction}

\begin{figure}[!t]
    \centering
    \subfigure[KV cache compression methods on long-context and arithmetic benchmarks.]{
        \includegraphics[scale=0.215, trim=30 0 0 0]{figs/compression_ratio_comparison_subplots.pdf}
    }
    \subfigure[Attention heatmap on long-context and arithmetic benchmarks.]{
        \includegraphics[scale=0.52, trim=5 0 0 0]{figs/fig_2.pdf}
    }

    \caption{KV cache compression methods on long-context and arithmetic benchmarks. 
    (a) Arithmetic benchmark shows more performance degradation than long-context benchmark. 
    (b) Long-Context benchmark shows more sparsity in attention heatmap.}
    \vspace{-1.8em}

    \label{fig:first_1}
\end{figure}

The emergence of Key-Value (KV) cache compression techniques has become crucial for efficient LLM deployment, primarily due to the increasing demands of memory management during inference. This need arises from the significant evolution of Large Language Models (LLMs), which now excel at processing extensive documents spanning thousands of tokens for tasks like question answering and summarization~\citep{raffel2020exploring, brown2020language, chowdhery2022palm, tay2022unifying, touvron2023llama, touvron2023llama2}. The ability to handle such lengthy inputs has been transformed by two concurrent developments: breakthroughs in ML system architectures for processing long sequences~\citep{flash-attn, flash-attn2, jacobs2023deepspeed, streamingllm}, and innovations in model design~\citep{chen2023extending, xiong2023effective, chen2023longlora, peng2024yarn}. However, this enhanced capability comes with significant computational costs; as context lengths grow, the GPU memory requirements for inference operations increase substantially~\citep{jamba, grok, geminiteam2024gemini, claude3, deepseekv2,deepseekv3}. This challenge has made the development of efficient KV cache compression strategies a critical focus in the field of LLM deployment and optimization.

Multiple studies have proposed innovative approaches to address this challenge through selective token retention strategies~\citep{streamingllm, h2o, snapkv, ge2023model, pyramidkv, fu2024lazyllm, yang2024pyramidinfer, adnan2024keyformer, liu2024scissorhands, tang2024quest,chunkkv}. Pioneering works such as H2O~\citep{h2o} and SnapKV~\citep{snapkv} demonstrate that retaining approximately 50\% of KV cache entries can maintain model performance while achieving substantial memory efficiency. However, current research primarily evaluates these methods on long-context scenarios like LongBench~\cite{longbench,longbenchv2} and Need-In-A-Haystack (NIAH)~\cite{needle}, leaving their broader impact on fundamental LLM capabilities largely unexplored. Our preliminary analysis, as shown in \cref{fig:first_1}, reveals two critical findings: (a) arithmetic reasoning tasks suffer significantly higher performance degradation under compression compared to long-context tasks, and (b) attention patterns in long-context scenarios exhibit notably higher sparsity than arithmetic tasks. These observations suggest that current evaluation frameworks, which focus predominantly on long-context performance, may inadequately capture the full spectrum of impacts on model capabilities. Further analysis of attention patterns, visualized in \cref{fig:main}, reveals distinct task-specific behaviors: \textbf{while world knowledge and commonsense reasoning tasks exhibit universal attention distributions, arithmetic reasoning and safety tasks demonstrate more specialized patterns}. Specifically, arithmetic reasoning tasks display increased attention sparsity, indicating focused attention on individual prompt examples, while safety tasks show concentrated attention on the system prompt. In contrast, world knowledge and commonsense reasoning tasks demonstrate more uniform attention distribution across the entire prompt. These varying attention patterns—visualized through colored squares representing system prompts, shot examples, and questions—provide insights into task-specific context utilization and motivate our investigation into how compression affects various factors including model size, prompt length, and task type.

% Motivated by these initial findings, we conduct a systematic study to comprehensively evaluate KV cache compression methods across different tasks (detailed in \cref{sec:experiments_design}). We evaluate multiple compression methods on five categories of tasks under different compression ratios, focusing on instruction-tuned and multi-step reasoning (O1 and R1~\cite{deepseekr1}) LLMs' \textit{fundamental abilities} (including world knowledge, common sense reasoning, arithmetic reasoning, code generation, safety, long-context reasoning and long-context generation). 


\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figs/main.pdf}
    % \includegraphics[width=\textwidth]{figs/main_1.pdf}

    \caption{Attention heatmap on different tasks. The heatmap is generated by the attention scores of the 15-th layer of the LLaMA-3.1-8B-Instruct attention head 10.}
    \vspace{-0.8em}
    \label{fig:main}
\end{figure}

Motivated by these initial findings, we conduct a systematic study to comprehensively evaluate KV cache compression methods across different tasks (detailed in \cref{sec:experiments_design}). Our analysis reveals several key findings: (1) Performance degradation is highly task-dependent, with arithmetic reasoning tasks showing particular sensitivity to aggressive compression; (2) Multi-step reasoning LLMs (O1 and R1) demonstrate higher compression robustness compared to instruction-tuned models; (3) Shorter prompts are more vulnerable to compression effects; (4) Chunk-level compression strategies show superior performance on complex long-context reasoning tasks; (5) Tasks with larger prompt-based performance gains exhibit higher compression sensitivity; and (6) Long-context generation tasks are particularly sensitive to compression. These findings provide valuable insights into the relationship between compression methods and model capabilities, motivating our development of \method{}.


We hope our work can provide the research community with insightful perspectives on KV cache compression's impact on LLMs. To the best of our knowledge, we are the first to comprehensively evaluate and analyze the impact of KV cache compression on LLMs' fundamental abilities. Our main contributions are summarized as follows:

\begin{itemize}[leftmargin=*]
    \item\noindent Through comprehensive evaluation of KV cache compression methods across diverse tasks and compression ratios, we demonstrate that task-specific sensitivity to compression varies significantly, with performance degradation ranging from 1\% to 40\% under aggressive compression settings and multi-step reasoning LLM (DeepSeek R1) is more robust than instruction-tuned LLM.
    
    \item\noindent Our systematic investigation reveals multiple critical factors influencing compression sensitivity, including model training dynamics, prompt length characteristics, task-specific requirements, long-context reasoning and long-context generation capabilities. These findings provide generalizable insights applicable across various compression methodologies and model architectures.

    \item\noindent We introduce \method{}, an innovative compression framework that distinctively manages prefill and decoding phases while maintaining shot-level semantic integrity. This approach demonstrates marked improvements in performance, particularly on complex long-context arithmetic reasoning  and long-context generation tasks.
\end{itemize}


% \begin{figure*}[tb]
%     \centering
%     \includegraphics[width=\textwidth]{figs/main.pdf}
%     % \includegraphics[width=\textwidth]{figs/main_1.pdf}

%     \caption{Attention heatmap on different tasks. The heatmap is generated by the attention scores of the 15-th layer of the LLaMA-3.1-8B-Instruct attention head 10.}
%     \label{fig:main}
% \end{figure*}




\section{Related Works}

\subsection{Key--value Cache Optimization Techniques}

KV cache is the core component in LLM inference, which avoids repetitive computations by caching Key and Value vectors. However, the cost of caching KV increases exponentially with the expansions of the model size and context length~\cite{pope2023efficiently}. Some approaches have been published to alleviate the issue. For instance, KV Compression designs efficient content selection strategies to filter and manage tokens~\cite{h2o,adnan2024keyformer}. Some methods identify important tokens by focusing on high attention allocation~\cite{snapkv}, while others optimize token selection by combining attention scores with value vector norms to improve importance evaluation~\cite{guo2024attention}. Techniques like PyramidInfer reduce critical contexts layer-by-layer based on the distribution of attention scores~\cite{yang2024pyramidinfer}, and StreamingLLM preserves attention sinks to maintain stable performance in extended sequences~\cite{streamingllm}. Researchers reduce storage costs by merging similar context representations and solving input disturbances caused by compression~\cite{zhangcam}. For instance, CaM~\cite{zhangcam} works by integrating the KV cache to be dropped into the retained cache in proportion to the attention weight.  In addition, \citet{yao2024cacheblend} proposes CacheBlend to achieve selective KV recompute. Only partial KVs of crucial tokens are updated to reduce the delay in the prefill stage and increase the throughput. Besides, the dynamic budget allocation method is also used to optimize the KV Cache, which adjusts the resource allocation in real-time according to the importance of the context, providing a balance between performance and efficiency in multi-task inference scenarios~\cite{pyramidkv,feng2024ada,chunkkv}.



\subsection{Evaluation of LLMs' Fundamental Abilities}
Accurately evaluating the fundamental capabilities of large language models is crucial for understanding their true potential and limitations. The evaluation typically spans across several key dimensions: world knowledge tasks like MMLU~\cite{mmlu},BBH~\cite{bbh} assess models' grasp of diverse domains through multiple-choice questions; commonsense reasoning tasks such as CSQA~\cite{csqa} evaluate inference and context understanding abilities; arithmetic reasoning benchmarks like GSM8K~\cite{gsm8k} test mathematical problem-solving capabilities through step-by-step reasoning; code generation tasks including HumanEval~\cite{chen2021evaluating} measure the ability to generate functionally correct code; and safety evaluations using benchmarks like JailBreakV~\cite{luo2024jailbreakv} assess models' robustness against harmful content generation. Additionally, long-context benchmarks such as LongBench~\cite{longbench,longbenchv2} and Need-In-A-Haystack (NIAH)~\cite{needle} aiming to evaluate models' long-context summarization and retrieval capabilities. Furthermore, LongGenBench~\cite{longgenbench} evaluate models' abilities to process and generate responses for extended input sequences. And recently, many-shot in-context learning has been recognized as a long-context reasoning paradigm~\cite{agarwal2024many}, which considers the number of shots as a critical factor in the performance of LLMs.
While these tasks typically employ automatic evaluation metrics for standardization, KV cache compression may introduce unique challenges, particularly in tasks requiring complex reasoning chains or extensive knowledge retrieval.
% LongGenBench~\cite{longgenbench} evaluate models' abilities to process and generate responses for extended input sequences. And recently, many-shot in-context learning has been recognized as a long-context reasoning paradigm~\cite{agarwal2024many}, which considers the number of shots as a critical factor in the performance of LLMs.

% \xl{add to judge longbench niah not good}

% \subsection{Evaluation of LLMs' Fundamental Abilities}
% Accurately evaluating the fundamental capabilities of large language models is crucial for understanding their true potential and limitations. This evaluation spans multiple dimensions:

% \paragraph{General Tasks}
% General tasks refer to evaluating the overall performance of LLMs under mathematical inference, logic reasoning, and common knowledge GSM8K~\cite{gsm8k} and MMLU~\cite{mmlu} are the representative tasks. The former focuses on the step-by-step reasoning ability of mathematical problem solving while the latter covers assessment of common sense and expertise in multiple areas. Besides, MATH~\cite{math} spans various mathematical fields, ranging from elementary algebra to calculus, aiming to improve the mathematical problem-solving capabilities of LLMs. Meanwhile, MathQA~\cite{mathqa} is a large-scale dataset comprising approximately 37,000 multiple-choice questions with precise annotations, designed to enhance the interpretability and performance of LLMs. In addition, BBH~\cite{bbh}, a subset of BIG-Bench~\cite{bigbench}, focuses on challenging tasks. BBH includes multi-step reasoning problems, highlighting the importance of Chain-of-Thought prompting in LLMs. Similarly, CSQA~\cite{csqa} is a task that combines knowledge graph-based multi-step reasoning with conversational capabilities. CSQA emphasizes inference and context understanding grounded in knowledge graphs. Normally, the general tasks apply automatic evaluation metrics (e.g. multi-choice accuracy) to ensure comparability and standardization. However, optimization strategies like KV cache compression may introduce challenges in executing the mentioned tasks. Filtering and dropping of contexts are involved in the compression strategy which may lead to an intermediate inference steps missing. In addition, in tasks such as MMLU that are highly dependent on knowledge coverage, compression may weaken the model's ability to capture long context or rare domain knowledge~\cite{yuan2024kv}.

% \paragraph{Security Tasks}
% Security tasks focus on assessing the robustness and protections of LLMs against harmful content, including truthfulness~\cite{lin2021truthfulqa}, toxicity~\cite{hartvigsen2022toxigen}, and bias~\cite{liang2021towards}. Recently, researchers noticed the weakness of LLMs in adversarial prompts~\cite{zhu2023promptbench}, especially in generating illegal or inappropriate content under jailbreak prompts. \citet{shen2024anything} analyze the jailbreak prompts in real cases to reveal the failure of model security mechanism under complex malicious input. Meanwhile, \citet{deng2023multilingual} demonstrates the multilingual jailbreak makes model security in low-resource languages easier to bypass, significantly increasing the probability that users of low-resource languages will generate insecure content. Similar to general tasks, KV optimization techniques can cause the model to ignore potential security threats when dealing with jailbreak prompts, thereby improving the success rate of adversarial prompts~\cite{li2024should}.

% \paragraph{Code Generation Tasks}
% Code generation tasks test the capacities of LLMs to generate code, which not only requires that the model can generate syntactic code based on natural language description but also has certain logical reasoning abilities. HumanEval~\cite{chen2021evaluating} and MBPP~\cite{austin2021program} are the commonly used benchmarks. They measure the functional correctness of the model by testing the results of the code's execution.

% \paragraph{Long-context Tasks}
% Research on Long-context tasks primarily focuses on evaluating models' capabilities in processing extended input sequences. Several benchmark datasets have emerged as standards in this domain: LongBench~\cite{longbench,longbenchv2} encompasses diverse tasks including question-answering, summarization, and comprehension, while NIAH~\cite{needle} specifically addresses long-context retrieval challenges. Additionally, many-shot in-context learning has been recognized as a long-context reasoning paradigm~\cite{agarwal2024many}. LongGenBench~\cite{longgenbench} extends this landscape by introducing a long-context generation benchmark that not only requires models to generate extensive responses but also provides zero-cost evaluation metrics for response quality assessment.

\section{Preliminary}
In this section, we provide comprehensive preliminaries of KV cache compression and LLM evaluation.
\vspace{-0.3cm}
\paragraph{Key-Value Cache in LLMs}
With the increasing long-context capabilities of LLMs, the Key-Value (KV) cache has become crucial for improving inference efficiency. During LLM inference, the KV cache stores intermediate computation results to avoid redundant calculations. For a given input sequence $x = (x_1, x_2, ..., x_n)$, each transformer layer $l$ maintains its key cache $K^l = (k^l_1, k^l_2, ..., k^l_n)$ and value cache $V^l = (v^l_1, v^l_2, ..., v^l_n)$, where $k^l_i, v^l_i \in \mathbb{R}^d$ represent the key and value vectors for token $x_i$ at layer $l$.

% However, the KV cache can consume significant GPU memory when handling long input contexts. The GPU memory cost of the KV cache for the decoding stage can be calculated as follows:

% \begin{equation}
% \label{eq:kv_cache_cost}
%     \text{GPU Cost of KV Cache} = 2 \times B \times S \times L \times N \times D \times 2
% \end{equation}
    
% where $B$ (batch size), $S$ (sequence length), $L$ (number of layers), $N$ (attention heads), and $D$ (head dimension) are model configuration parameters. The two multipliers of 2 account for the KV matrices and float16 precision respectively.

% For example, with LLaMA-3.1-8B-Instruct~\cite{dubey2024llama} using a batch size $B = 1$ and a sequence length of prompt $S = 2048$, the GPU memory cost of the KV cache is nearly 1 GB. If the batch size exceeds 24, the GPU memory cost of the KV cache will exceed the capacity of an RTX 4090 GPU. To address this issue, KV cache compression methods have been proposed, with the aim of retaining only a minimal amount of KV cache while preserving as much information as possible.
\vspace{-0.3cm}
\paragraph{KV Cache Compression}
KV cache compression aims to reduce memory usage by selectively storing or merging cached vectors. A compression operation can be denoted as $C(K,V) = (K',V')$, where $K'$ and $V'$ are compressed caches with size $m < n$, where $C$ is the compression method, $m$ is the number of retained tokens, and $n$ is the original number of tokens. The core idea is token selection - identifying and retaining important tokens based on attention patterns or other metrics while discarding less important ones. The compression ratio $r = m/n$ indicates how aggressively the cache is compressed, where a smaller ratio means more aggressive compression.

\vspace{-0.3cm}
\paragraph{Evaluation Protocol}
To thoroughly evaluate the impact of KV cache compression on LLMs' capabilities, we assess five benchmark categories: world knowledge, commonsense reasoning, arithmetic reasoning, code generation, and safety.

For each task category and compression method $C$, we calculate the relative performance change as follows:
\begin{equation} \label{eq:performance_change}
    \Delta P = \frac{P_C - P_{\text{base}}}{P_{\text{base}}}
\end{equation}
where $P_C$ and $P_{\text{base}}$ represent the performance scores with and without compression, respectively.

% This evaluation framework provides a comprehensive understanding of how KV cache compression influences not only long-context processing but also essential capabilities such as reasoning, knowledge retrieval, and generation across varying compression ratios.




\section{Experiments Design}
\label{sec:experiments_design}
\subsection{Experimental Setups}
In this section, we will introduce the experimental setups, including the datasets, models, and evaluation environment. 

\vspace{-0.8em}
\paragraph{Datasets}
To evaluate the performance of KV cache compression on LLMs' overarching capabilities, we assess five benchmark categories: \textit{World Knowledge} using MMLU~\citep{mmlu}, measured by accuracy; \textit{CommonSense Reasoning} using CommonsenseQA~\citep{csqa} , evaluated through multiple-choice accuracy; \textit{Arithmetic Reasoning} using GSM8K~\citep{gsm8k}, assessed by solve rate; \textit{Code Generation} using HumanEval~\citep{chen2021evaluating}, measured by pass@1 rate on test cases; and \textit{Safety} using JailBreakV~\citep{luo2024jailbreakv}, evaluated by attack success rate. Furthermore, we test the performance of KV cache compression on LongGenBench~\citep{longgenbench}, a long-context generation benchmark.

\vspace{-0.8em}
\paragraph{Models}
We conduct experiments on a series of LLMs, including LLaMA-3.1-8B, LLaMA-3.1-8B-Instruct~\citep{dubey2024llama}, and multi-step reasoning LLM DeepSeek-R1-Distill-Llama-8B~\citep{deepseekr1}. 

\vspace{-0.8em}
\paragraph{KV Cache Compression Methods}
To comprehensively investigate the potential impact on KV cache compression methods, we select the following methods: StreamingLLM~\cite{streamingllm}, SnapKV~\cite{snapkv}, H2O~\cite{h2o}, PyramidKV~\cite{pyramidkv}, PyramidInfer~\cite{yang2024pyramidinfer}, and ChunkKV~\cite{chunkkv}. 

\vspace{-0.8em}
\paragraph{Hyperparameters}
For the experiments on observation 1, 2 we set the normal shot number as in the original paper. For the experiments on observation 3, 4, 5, the shot number are dependent on the experiment settings. More details are shown in \cref{sec:hyperparameters}.

\vspace{-0.8em}
\paragraph{Evaluation Environment}
We use the lm-evaluation-harness~\citep{eval-harness} library to load the models and evaluate the performance. The evaluation environment is a NVIDIA A40 GPU server.

\subsection{Results and Analysis}
\label{sec:results_and_analysis}
In this section, we will present the results and analysis of the experiments. For comprehensive results, please refer to \cref{sec:detail-results}.
\begin{figure}[hb]
    \centering
    % subfigure
    \subfigure[Sensitivity Analysis of Different Benchmark Categories to KV Cache Compression]{
        \includegraphics[width=0.5\textwidth]{figs/benchmark_sensitivity_raw.pdf}
    }
    \subfigure[Performance Delta Lines with Baseline]{
        \includegraphics[width=0.5\textwidth]{figs/performance_delta_lines_with_baseline.pdf}
    }

    \caption{Sensitivity Analysis of Different Benchmark Categories to KV Cache Compression. The performance delta lines are calculated by \cref{eq:performance_change}.}
    \vspace{-0.8em}
    \label{fig:benchmark_sensitivity}
\end{figure}


\begin{figure*}[!ht]
    \centering
    \includegraphics[width=1\textwidth]{figs/kv_cache_compression.pdf}
    \caption{Performance Comparison of KV Cache Compression Methods Across Tasks. Note: The y-axis scales vary across different tasks. Results for R1-GSM8K (e) were obtained using the DeepSeek-R1-Distill-Llama-8B model.}
    \label{fig:kv_cache_compression}
\end{figure*}

\vspace{-0.3cm}
\paragraph{\it{\textbf{Observation 1.}}} \textbf{KV cache compression methods show task-dependent performance degradation, with varying sensitivity thresholds across different benchmark categories.}
As demonstrated in \cref{fig:benchmark_sensitivity}, all tasks maintain stable performance at compression ratios above 40\%, but exhibit distinct degradation patterns below this threshold. GSM8K, HumanEval, and JailBreakV tasks demonstrate the highest compression sensitivity, characterized by precipitous performance declines. \Cref{fig:kv_cache_compression} illustrates the detailed performance impact of various KV cache compression methods across different tasks. This degradation is most pronounced in GSM8K (d), where performance deteriorates significantly below 20\% compression ratio, with accuracy dropping from approximately 0.75 to below 0.5. Among the evaluated methods, ChunkKV~\cite{chunkkv} and PyramidKV~\cite{pyramidkv} consistently demonstrate superior stability across most tasks, while StreamingLLM~\cite{streamingllm} exhibits heightened sensitivity to aggressive compression. Additionally, R1-GSM8K (e) indicates that R1 LLMs demonstrate enhanced robustness to KV cache compression. 


\begin{figure}[tb]
    \centering
    \includegraphics[width=0.5\textwidth]{figs/instruct_vs_no_instruct_sensitivity.pdf}

    % \includegraphics[width=0.5\textwidth]{figs/gsm8k_instruct_comparison.pdf}
    \caption{Performance Comparison of KV Cache Compression Methods on different training dynamics on GSM8K}
    \vspace{-1.5em}
    \label{fig:kv_cache_compression_instruct}
\end{figure}

\vspace{-0.8em}
\paragraph{\it{\textbf{Observation 2.}}} \textbf{Multi-step reasoning LLMs are more robust to KV cache compression.} \cref{fig:kv_cache_compression_instruct} presents a comparative analysis of LLaMA-3.1-8B across its base (w/o instruct tuned), instruct-tuned, and DeepSeek-R1 distilled variants, illustrating their averaged performance across five compression methods with confidence intervals. Although all three variants exhibit performance degradation at low compression ratios, their degradation trajectories differ significantly. The R1 distilled model demonstrates superior stability, maintaining performance around 0.60 even at a 10\% compression ratio. While the instruct-tuned model achieves a higher initial accuracy (0.8), it exhibits heightened compression sensitivity, with performance deterioration initiating at 30\% compression ratio and declining sharply to approximately 0.5 at 10\% ratio. These findings suggest that while multi-step reasoning LLMs demonstrate enhanced robustness to KV cache compression, and instruct-tuning improves overall model performance, the latter may inadvertently increase model vulnerability to aggressive compression, particularly at compression ratios below 30\%. 

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{figs/shot_comparison_average.pdf}

    \caption{Average Performance Across Different Shot Numbers}
    \vspace{-0.5em}
    \label{fig:shot_comparison_average}
\end{figure}


\vspace{-0.8em}
\paragraph{\it{\textbf{Observation 3.}}} \textbf{Short prompt length is more sensitive to KV cache compression.} As shown in \cref{fig:shot_comparison_average}, the impact of KV cache compression varies significantly with different prompt lengths (shot numbers). One-shot and two-shot scenarios demonstrate higher sensitivity to compression, with performance dropping more sharply below 30\% compression ratio compared to scenarios with more shots (4-8). For instance, in 1-shot settings, performance decreases from 0.5 to 0.05 when compression ratio drops from 30\% to 10\%, while 8-shot settings show only drop from 0.75 to 0.5 reduction under the same conditions. This suggests that longer prompts provide more information that helps maintain model performance even under aggressive compression. The increased robustness with higher shot numbers may be attributed to the model's ability to leverage multiple examples, making it less dependent on perfect preservation of each individual example in the compressed KV cache.


\vspace{-0.8em}
\paragraph{\it{\textbf{Observation 4.}}} \textbf{Chunk-level compression is more effective for long-context arithmetic reasoning tasks.}
Inspired by \citet{agarwal2024many}, we consider many-shot in-context learning as a long-context reasoning task, which is more complex than existing long-context benchmarks, such as LongBench and NIAH. \cref{fig:many_shot_kv_cache_compression} shows the performance of KV cache compression methods on a 50-shot GSM8K task, where the prompt length exceeds 4K tokens. From the figure, we observe that ChunkKV~\cite{chunkkv} demonstrates the most stability when the compression ratio is below 10\% on both LLaMA-3.1-8B-Instruct and DeepSeek-R1-Distill-Llama-8B, indicating that in more complex long-context arithmetic reasoning tasks, chunk-level retention is more effective at preserving semantic information.

\begin{figure}[h]
    \centering
    \subfigure[Many-shot GSM8K on LLaMA3.1-8B-Instruct]{
    \includegraphics[width=0.5\textwidth]{figs/gsm8k_50shot_comparison.pdf}
    }
    \subfigure[Many-shot GSM8K on DeepSeek-R1-Distill-Llama-8B]{
        \includegraphics[width=0.5\textwidth]{figs/gsm8k_50shot_r1_comparison.pdf}
    }
    \caption{Many-shot scenario on KV cache compression}
    \label{fig:many_shot_kv_cache_compression}
    \vspace{-0.8em}
\end{figure}


\vspace{-0.8em}
\paragraph{\it{\textbf{Observation 5.}}} \textbf{Tasks with larger prompt-based performance gains show higher sensitivity to KV cache compression.} As shown in \cref{tab:shot_comparison}, different tasks exhibit varying levels of performance improvement from zero-shot to few-shot prompting. GSM8K shows a dramatic improvement of 50.41\%, while MMLU demonstrates a more modest gain of 6.20\%. From \cref{fig:benchmark_sensitivity}, we find that tasks with larger prompt-based improvements, such as GSM8K, are more sensitive to KV cache compression. This suggests that when a task heavily relies on in-context examples to achieve better performance, the compression of these crucial prompt elements has a more substantial impact on model performance. In contrast, tasks like MMLU, where the performance gain from prompting is smaller, show more resilience to KV cache compression, likely because the model relies more on its inherent knowledge rather than the specific examples in the prompt.



\begin{table}[h]
    \centering
    \caption{Zero-shot vs Few-shot Performance Comparison}
    \label{tab:shot_comparison}
    \resizebox{0.9\columnwidth}{!}{%
    \begin{tabular}{l|cc|c}
    \toprule
    Benchmark & Zero-shot $\uparrow$ & Few-shot $\uparrow$ & Delta$\Delta$ \\
    \midrule
    GSM8K & $0.2904$ & $0.7945$   & $+0.5041$ \\
    MMLU & $0.6262$ & $0.6882$  & $+0.0620$ \\
    \bottomrule
    \end{tabular}
    }
\end{table}

\vspace{-0.8em}
\paragraph{\it{\textbf{Observation 6.}}}
\textbf{KV cache compression exhibits significant performance degradation on Long-Context Generation tasks.} As demonstrated in \cref{tab:longgenbench_gsm8k_performance}, our evaluation of three unified compression methods—StreamingLLM, H2O, and PyramidInfer—on LongGenBench-GSM8K reveals substantial performance limitations. On this arithmetic reasoning task with approximately 4k token generation length, the compression methods show notable deterioration, with performance declining by more than 20\% at compression ratios below 30\%. The \method{} is our proposed method aiming to improve the performance of KV cache compression on Long-Context Generation tasks, details in \cref{sec:shotkv}.



\begin{table}[htbp]
    \centering
    \caption{KV cache compression methods' performance on LongGenBench-GSM8K}
    \resizebox{0.9\columnwidth}{!}{%
    \begin{tabular}{l|c|cccc}
    \toprule
    \textbf{Method} & \textbf{100\%} & \textbf{40\%} & \textbf{35\%} & \textbf{30\%} & \textbf{25\%} \\
    \midrule
    \rowcolor{gray!30} FullKV & 46.00 & - & - & - & - \\
    \midrule
    StreamingLLM & - & 39.50 & 28.67 & 14.83 & 6.33 \\
    H2O & - & 32.66 & 25.17 & 19.83  & 14.83  \\
    PyramidInfer & - & 38.33 & 27.67 & 20.50 & 16.67 \\
    \rowcolor{red!20} \method{}(Ours) & - & \textbf{47.33} & \textbf{41.33} & \textbf{38.33} & \textbf{26.83}\\
    \bottomrule
    \end{tabular}
    }
    \label{tab:longgenbench_gsm8k_performance}
\end{table}

\section{\method{}}
\label{sec:shotkv}
Based on our comprehensive analysis, we find that current unified KV cache compression methods exhibit significant performance degradation on Long-Context Generation tasks. In this section, we introduce \method{}, a decoding-time compression method designed to mitigate this degradation. Our approach is motivated by two key insights: (1) \cref{fig:main} demonstrates that n-shot example prompts receive substantial attention in reasoning benchmarks, and (2) Observation 4 reveals that chunk-level compression is particularly effective for long-context arithmetic reasoning tasks. Based on these findings, we hypothesize that each shot example represents a coherent chunk of information, leading us to design \method{} to preserve shot examples intact during decoding time.


\subsection{Implementation}
The \textbf{\method{}} (Prefill-Decoding Separated \textbf{Shot}-aware \textbf{KV} Cache Compression), which separates the compression strategy for prefill and decoding phases. The key insight is that the prefill phase KV cache, which contains crucial prompt information, should be compressed once and remain fixed, while the decoding phase KV cache can be dynamically compressed with different strategies.

Given a prompt with $n$ shots and generated tokens, we define:
% \vspace{-0.1cm}
\begin{equation}
    KV_{\text{total}} = KV_{\text{prefill}} \cup KV_{\text{decoding}}
\end{equation}
\vspace{-0.1cm}
% For prefill phase, we compute importance at shot-level to preserve semantic coherence
For the prefill phase, we compute shot importance and preserve complete shot examples:
\begin{equation}
    \text{Score}_{\text{prefill}}(s_i) = \frac{1}{k_i} \sum_{t \in s_i} \sum_{h=1}^H \sum_{l=1}^L \alpha_{t,h}^l
\end{equation}
where $s_i$ represents the $i$-th shot example containing $k_i$ tokens, and $\alpha_{t,h}^l$ denotes the attention score for token $t$ in head $h$ at layer $l$. Once the prefill phase KV cache is compressed, it remains fixed throughout the generation process.


Given a prefill compression ratio $r_p$, we prioritize shots with higher scores while ensuring the total number of preserved tokens does not exceed the KV cache limit. Specifically, shots are ranked by their scores and selected in descending order until reaching the compression budget $r_p \times |KV_{\text{prefill}}|$. This shot-level selection strategy helps maintain the semantic coherence of important examples while adhering to memory constraints.

\begin{equation}
    KV^C_{\text{prefill}} = \text{Compress}(\{s_i | s_i \in S^*_{\text{preserved}}\})
\end{equation}
\begin{equation}
    \text{where} \quad S_{\text{preserved}} = \argmax_{S \subseteq \{s_1,...,s_n\}} \sum_{s_i \in S} \text{Score}_{\text{prefill}}(s_i)
\end{equation}
\begin{equation}
    \text{subject to:} \quad \sum_{s_i \in S} k_i \leq r_p \times |KV_{\text{prefill}}|
\end{equation}

Here, $KV^C_{\text{prefill}}$ represents the compressed prefill KV cache, and $S_{\text{preserved}}$ represents the optimal subset of shots to be preserved after compression. The first equation aims to maximize the total importance score of selected shots, where $\{s_1,...,s_n\}$ represents all available shots and $\text{Score}_{\text{prefill}}(s_i)$ is the importance score of shot $s_i$ computed using attention weights as defined earlier. The second equation enforces the memory constraint: the total number of tokens ($k_i$) in the selected shots must not exceed the allocated budget, which is determined by the prefill compression ratio $r_p$ multiplied by the original KV cache size.

% For decoding phase, we only compress the newly generated tokens' KV cache
For the decoding phase, we compute importance scores only for the tokens generated during decoding:
\begin{equation}
    \text{Score}_{\text{decoding}}(t) = \sum_{h=1}^H \sum_{l=1}^L \alpha_{t,h}^l
\end{equation}


Given a decoding compression ratio $r_d$, we select tokens with the highest scores to preserve. The compressed decoding KV cache $KV^C_{\text{decoding}}$ retains only the top-$k$ tokens where $k = r_d \times |KV_{\text{decoding}}|$, effectively maintaining the most influential context tokens while reducing memory usage:

\begin{equation}
    \begin{split}
        KV^C_{\text{decoding}} = \text{TopK}(&KV_{\text{decoding}}, \text{Score}_{\text{decoding}}, \\
        &k = r_d \times |KV_{\text{decoding}}|)
    \end{split}
    \end{equation}


Finally, we combine the compressed prefill and decoding KV caches to form the total compressed KV cache:

\begin{equation}
    KV_{\text{total}} = KV^C_{\text{prefill}} \cup KV^C_{\text{decoding}}
\end{equation}


\subsection{Empirical Results}
In this section, we evaluate \method{} under two scenarios: many-shot GSM8K with multiple KV cache compression methods, and LongGenBench-GSM8K with three unified compression methods that optimize the KV cache during generation.

\vspace{-0.3cm}
\paragraph{Baseline.} For LongGenBench-GSM8K evaluation, we employ three state-of-the-art unified compression methods as baselines: StreamingLLM~\cite{streamingllm}, H2O~\cite{h2o}, and PyramidInfer~\cite{yang2024pyramidinfer}. We conduct experiments using LLaMA-3-8B-Instruct~\cite{dubey2024llama} on the LongGenBench-GSM8K benchmark~\cite{longgenbench}, maintaining consistent parameters with Observation 6 ($K = 35$, $T = 20$). For many-shot GSM8K experiments, we follow the configuration detailed in Observation 4.




\vspace{-0.8em}
\paragraph{Main results and analysis.}
 From the ~\cref{tab:kv-compression-many-gsm8k}, we can see that \method{} achieves the best performance on LongGenBench-GSM8K, maintaining high performance at low compression ratios. Specifically, at a compression ratio of 40\%, \method{} achieves 47.33\% accuracy, surpassing the full kv cache baseline (46.00\%) and showing substantial improvements over other methods (32.66\%-39.50\%). And \cref{tab:longgenbench_gsm8k_performance} shows that \method{} also achieves the best performance on many-shot GSM8K, maintaining high performance at low compression ratios. Even at aggressive compression ratios (25\%-30\%), \method{} maintains relatively stable performance (26.83\%-38.33\%), while other methods experience more severe degradation (6.33\%-16.67\%). This superior performance can be attributed to two key design choices: (1) the preservation of complete shot examples during compression maintains the semantic coherence necessary for mathematical reasoning, and (2) the separation of prefill and decoding phase compression allows for more flexible and task-appropriate token retention strategies. 
%  Even at aggressive compression ratios (25\%-30\%), \method{} maintains relatively stable performance (26.83\%-38.33\%), while other methods experience more severe degradation (6.33\%-20.50\%). 
These results suggest that our shot-aware compression strategy is particularly effective for long-context generation tasks that require maintaining complex reasoning chains, such as mathematical problem-solving.


 \section{Further Discussion}
 Our comprehensive analysis of KV cache compression reveals several important implications and limitations that warrant further discussion:

 \begin{table}[h]
    \centering
    % \vspace{-0.8em}
    \caption{KV cache compression methods' performance on Many-shot GSM8K}
    \label{tab:kv-compression-many-gsm8k}
    \resizebox{0.9\columnwidth}{!}{%
    \begin{tabular}{l|c|cccc}
    \toprule
    Method & 100\% & 40\% & 30\% & 20\% & 10\% \\
    \midrule
    \rowcolor{gray!30} FullKV & 0.8235 & - & - & - & - \\
    \midrule
    StreamingLLM & - & 0.8037 & 0.7835 & 0.7537 & 0.7432 \\
    H2O & - & 0.7832 & 0.7932 & 0.7428 & 0.5127 \\
    PyramidKV & - & 0.7834 & 0.7934 & 0.7832 & 0.7037 \\
    SnapKV & - & 0.7935 & 0.8038 & 0.7934 & 0.6827 \\
    ChunkKV & - & 0.7832 & 0.7932 & 0.7835 & 0.7932 \\
    \rowcolor{red!20} \method{}(Ours) & - & \textbf{0.8107} & \textbf{0.8082} & \textbf{0.8057} & \textbf{0.8037} \\
    \bottomrule
    \end{tabular}
    }
    \vspace{-0.2cm}
\end{table}

 \vspace{-0.8em}
 \paragraph{Trade-off between Memory Efficiency and Task Performance} While KV cache compression methods effectively reduce memory usage, our findings highlight a non-uniform impact across different tasks. This suggests that deployment strategies should carefully consider task-specific requirements when selecting compression ratios, potentially implementing adaptive compression based on the task type.
 
 \vspace{-0.8em}
 \paragraph{Implications for Model Design} The higher sensitivity of instruct-tuned models to compression raises questions about the relationship between model training objectives and compression robustness. Future research might explore training techniques that explicitly optimize for compression resilience while maintaining instruction-following capabilities.
 
 \vspace{-0.8em}
 \paragraph{Limitations} Our study has several limitations: (1) The evaluation was conducted on specific benchmarks, and performance on other tasks may vary; (2) The computational resources required for comprehensive evaluation limited our ability to test on larger models and more diverse compression methods.
 
 \section{Conclusion}
 This paper presents a systematic study of KV cache compression's impact on LLMs' core capabilities, revealing several key findings: (1) Performance degradation is highly task-dependent, with arithmetic reasoning tasks showing particular sensitivity to aggressive compression; (2) Instruct-tuned models demonstrate higher sensitivity to compression compared to their base counterparts; (3) Shorter prompts are more vulnerable to compression effects; (4) Chunk-level compression strategies show superior performance on complex long-context reasoning tasks; (5) Long-context generation tasks are more sensitive to compression than long-context reasoning tasks.
 
 Based on these insights, we proposed ShotKV, a novel compression method that separately handles prefill and decoding phases while preserving shot-level semantic coherence. Our method demonstrates superior performance on long-context arithmetic reasoning tasks and long-context generation tasks, maintaining high accuracy even at low compression ratios.
 
 These findings have important implications for the deployment of LLMs in resource-constrained environments and suggest several promising directions for future research, including: (1) Development of task-adaptive compression strategies; (2) Investigation of compression-aware training methods; and (3) Extension of compression techniques to other model architectures and modalities.


 \section*{Impact Statement}

This work advances the field of efficient large language model deployment through systematic analysis and improvement of KV cache compression techniques. Our research has several potential societal impacts:

First, by enabling more efficient memory usage in LLMs while maintaining performance, our work contributes to reducing the computational resources and energy consumption required for AI deployment. This has positive environmental implications and makes AI technology more accessible to researchers and organizations with limited computing resources.

Second, our proposed \method{} method specifically improves performance on long-context arithmetic reasoning tasks, which could enhance the practical applications of LLMs in education, scientific computing, and other fields requiring complex mathematical reasoning. This could lead to more reliable AI-assisted learning and problem-solving tools.

However, we acknowledge that making LLMs more efficient could accelerate their widespread adoption, potentially raising concerns about AI's impact on employment and privacy. While our work focuses on technical improvements, we encourage the research community to carefully consider these broader implications when deploying such technologies.

We believe the benefits of more efficient and capable AI systems outweigh potential risks, particularly as our work promotes more sustainable and accessible AI development. Nevertheless, we emphasize the importance of responsible deployment and continued ethical consideration in the application of these technologies.

% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
\bibliography{example_paper}
\bibliographystyle{icml2025}

\input{appendix}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
