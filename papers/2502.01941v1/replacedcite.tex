\section{Related Works}
\subsection{Key--value Cache Optimization Techniques}

KV cache is the core component in LLM inference, which avoids repetitive computations by caching Key and Value vectors. However, the cost of caching KV increases exponentially with the expansions of the model size and context length____. Some approaches have been published to alleviate the issue. For instance, KV Compression designs efficient content selection strategies to filter and manage tokens____. Some methods identify important tokens by focusing on high attention allocation____, while others optimize token selection by combining attention scores with value vector norms to improve importance evaluation____. Techniques like PyramidInfer reduce critical contexts layer-by-layer based on the distribution of attention scores____, and StreamingLLM preserves attention sinks to maintain stable performance in extended sequences____. Researchers reduce storage costs by merging similar context representations and solving input disturbances caused by compression____. For instance, CaM____ works by integrating the KV cache to be dropped into the retained cache in proportion to the attention weight.  In addition, ____ proposes CacheBlend to achieve selective KV recompute. Only partial KVs of crucial tokens are updated to reduce the delay in the prefill stage and increase the throughput. Besides, the dynamic budget allocation method is also used to optimize the KV Cache, which adjusts the resource allocation in real-time according to the importance of the context, providing a balance between performance and efficiency in multi-task inference scenarios____.



\subsection{Evaluation of LLMs' Fundamental Abilities}
Accurately evaluating the fundamental capabilities of large language models is crucial for understanding their true potential and limitations. The evaluation typically spans across several key dimensions: world knowledge tasks like MMLU____,BBH____ assess models' grasp of diverse domains through multiple-choice questions; commonsense reasoning tasks such as CSQA____ evaluate inference and context understanding abilities; arithmetic reasoning benchmarks like GSM8K____ test mathematical problem-solving capabilities through step-by-step reasoning; code generation tasks including HumanEval____ measure the ability to generate functionally correct code; and safety evaluations using benchmarks like JailBreakV____ assess models' robustness against harmful content generation. Additionally, long-context benchmarks such as LongBench____ and Need-In-A-Haystack (NIAH)____ aiming to evaluate models' long-context summarization and retrieval capabilities. Furthermore, LongGenBench____ evaluate models' abilities to process and generate responses for extended input sequences. And recently, many-shot in-context learning has been recognized as a long-context reasoning paradigm____, which considers the number of shots as a critical factor in the performance of LLMs.
While these tasks typically employ automatic evaluation metrics for standardization, KV cache compression may introduce unique challenges, particularly in tasks requiring complex reasoning chains or extensive knowledge retrieval.
% LongGenBench____ evaluate models' abilities to process and generate responses for extended input sequences. And recently, many-shot in-context learning has been recognized as a long-context reasoning paradigm____, which considers the number of shots as a critical factor in the performance of LLMs.

% \xl{add to judge longbench niah not good}

% \subsection{Evaluation of LLMs' Fundamental Abilities}
% Accurately evaluating the fundamental capabilities of large language models is crucial for understanding their true potential and limitations. This evaluation spans multiple dimensions:

% \paragraph{General Tasks}
% General tasks refer to evaluating the overall performance of LLMs under mathematical inference, logic reasoning, and common knowledge GSM8K____ and MMLU____ are the representative tasks. The former focuses on the step-by-step reasoning ability of mathematical problem solving while the latter covers assessment of common sense and expertise in multiple areas. Besides, MATH____ spans various mathematical fields, ranging from elementary algebra to calculus, aiming to improve the mathematical problem-solving capabilities of LLMs. Meanwhile, MathQA____ is a large-scale dataset comprising approximately 37,000 multiple-choice questions with precise annotations, designed to enhance the interpretability and performance of LLMs. In addition, BBH____, a subset of BIG-Bench____, focuses on challenging tasks. BBH includes multi-step reasoning problems, highlighting the importance of Chain-of-Thought prompting in LLMs. Similarly, CSQA____ is a task that combines knowledge graph-based multi-step reasoning with conversational capabilities. CSQA emphasizes inference and context understanding grounded in knowledge graphs. Normally, the general tasks apply automatic evaluation metrics (e.g. multi-choice accuracy) to ensure comparability and standardization. However, optimization strategies like KV cache compression may introduce challenges in executing the mentioned tasks. Filtering and dropping of contexts are involved in the compression strategy which may lead to an intermediate inference steps missing. In addition, in tasks such as MMLU that are highly dependent on knowledge coverage, compression may weaken the model's ability to capture long context or rare domain knowledge____.

% \paragraph{Security Tasks}
% Security tasks focus on assessing the robustness and protections of LLMs against harmful content, including truthfulness____, toxicity____, and bias____. Recently, researchers noticed the weakness of LLMs in adversarial prompts____, especially in generating illegal or inappropriate content under jailbreak prompts. ____ analyze the jailbreak prompts in real cases to reveal the failure of model security mechanism under complex malicious input. Meanwhile, ____ demonstrates the multilingual jailbreak makes model security in low-resource languages easier to bypass, significantly increasing the probability that users of low-resource languages will generate insecure content. Similar to general tasks, KV optimization techniques can cause the model to ignore potential security threats when dealing with jailbreak prompts, thereby improving the success rate of adversarial prompts____.

% \paragraph{Code Generation Tasks}
% Code generation tasks test the capacities of LLMs to generate code, which not only requires that the model can generate syntactic code based on natural language description but also has certain logical reasoning abilities. HumanEval____ and MBPP____ are the commonly used benchmarks. They measure the functional correctness of the model by testing the results of the code's execution.

% \paragraph{Long-context Tasks}
% Research on Long-context tasks primarily focuses on evaluating models' capabilities in processing extended input sequences. Several benchmark datasets have emerged as standards in this domain: LongBench____ encompasses diverse tasks including question-answering, summarization, and comprehension, while NIAH____ specifically addresses long-context retrieval challenges. Additionally, many-shot in-context learning has been recognized as a long-context reasoning paradigm____. LongGenBench____ extends this landscape by introducing a long-context generation benchmark that not only requires models to generate extensive responses but also provides zero-cost evaluation metrics for response quality assessment.