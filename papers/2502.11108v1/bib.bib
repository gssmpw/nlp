@article{li2019knowledge,
  title={Knowledge-oriented convolutional neural network for causal relation extraction from natural language texts},
  author={Li, Pengfei and Mao, Kezhi},
  journal={Expert Systems with Applications},
  volume={115},
  pages={512--523},
  year={2019},
  publisher={Elsevier}
}

@article{liu2024deepseek,
  title={Deepseek-v3 technical report},
  author={Liu, Aixin and Feng, Bei and Xue, Bing and Wang, Bingxuan and Wu, Bochao and Lu, Chengda and Zhao, Chenggang and Deng, Chengqi and Zhang, Chenyu and Ruan, Chong and others},
  journal={arXiv preprint arXiv:2412.19437},
  year={2024}
}

@Article{diagnostics14141468,
AUTHOR = {Muntean, George Adrian and Marginean, Anca and Groza, Adrian and Damian, Ioana and Roman, Sara Alexia and Hapca, Mădălina Claudia and Sere, Anca Mădălina and Mănoiu, Roxana Mihaela and Muntean, Maximilian Vlad and Nicoară, Simona Delia},
TITLE = {A Qualitative Evaluation of ChatGPT4 and PaLMs Response to Patients Questions Regarding Age-Related Macular Degeneration},
JOURNAL = {Diagnostics},
VOLUME = {14},
YEAR = {2024},
NUMBER = {14},
ARTICLE-NUMBER = {1468},

ISSN = {2075-4418},
DOI = {10.3390/diagnostics14141468},
type={journal},
infosite = {10.3390/diagnostics14141468},
durl ={https://www.mdpi.com/2075-4418/14/14/1468},
}


@inproceedings{topsakal2023creating,
  title={Creating large language model applications utilizing langchain: A primer on developing llm apps fast},
  author={Topsakal, Oguzhan and Akinci, Tahir Cetin},
  booktitle={International Conference on Applied Engineering and Natural Sciences},
  volume={1},
  number={1},
  pages={1050--1056},
  year={2023}
}

@article{mihuailua2013biocause,
  title={BioCause: Annotating and analysing causality in the biomedical domain},
  author={Mih{\u{a}}il{\u{a}}, Claudiu and Ohta, Tomoko and Pyysalo, Sampo and Ananiadou, Sophia},
  journal={BMC bioinformatics},
  volume={14},
  pages={1--18},
  year={2013},
  publisher={Springer}
}


@article{panzarella2023using,
  title={Using ontologies for life science text-based resource organization},
  author={Panzarella, Giulia and Veltri, Pierangelo and Alcaro, Stefano},
  journal={Artificial Intelligence in the Life Sciences},
  volume={3},
  pages={100059},
  year={2023},
  publisher={Elsevier}
}


@article{fu2020clinical,
  title={Clinical concept extraction: a methodology review},
  author={Fu, Sunyang and Chen, David and He, Huan and Liu, Sijia and Moon, Sungrim and Peterson, Kevin J and Shen, Feichen and Wang, Liwei and Wang, Yanshan and Wen, Andrew and others},
  journal={Journal of biomedical informatics},
  volume={109},
  pages={103526},
  year={2020},
  publisher={Elsevier}
}


@article{monea2023glitch,
  title={A glitch in the Matrix? Locating and detecting language model grounding with Fakepedia},
  author={Monea, Giovanni and Peyrard, Maxime and Josifoski, Martin and Chaudhary, Vishrav and Eisner, Jason and K{\i}c{\i}man, Emre and Palangi, Hamid and Patra, Barun and West, Robert},
  journal={arXiv preprint arXiv:2312.02073},
  year={2023}
}


@article{glimm2014hermit,
  title={HermiT: an OWL 2 reasoner},
  author={Glimm, Birte and Horrocks, Ian and Motik, Boris and Stoilos, Giorgos and Wang, Zhe},
  journal={Journal of automated reasoning},
  volume={53},
  pages={245--269},
  year={2014},
  publisher={Springer}
}


@article{da2023exploring,
  title={Exploring named entity recognition and relation extraction for ontology and medical records integration},
  author={da Silva, Diego Pinheiro and da Rosa Fr{\"o}hlich, William and de Mello, Blanda Helena and Vieira, Renata and Rigo, Sandro Jos{\'e}},
  journal={Informatics in medicine unlocked},
  volume={43},
  pages={101381},
  year={2023},
  publisher={Elsevier}
}


@article{toure2021status,
  title={The status of causality in biological databases: data resources and data retrieval possibilities to support logical modeling},
  author={Tour{\'e}, Vasundra and Flobak, {\AA}smund and Niarakis, Anna and Vercruysse, Steven and Kuiper, Martin},
  journal={Briefings in Bioinformatics},
  volume={22},
  number={4},
  pages={bbaa390},
  year={2021},
  publisher={Oxford University Press}
}

@inproceedings{long2022can,
  title={Can Large Language Models Build Causal Graphs?},
  author={Long, Stephanie and Schuster, Tibor and Pich{\'e}, Alexandre},
  booktitle={NeurIPS 2022 Workshop on Causality for Real-world Impact}
}

@inproceedings{khetan-etal-2022-mimicause,
    title={MIMICause: Representation and automatic extraction of causal relation types from clinical notes},
    author={Vivek Khetan and Md Imbesat Hassan Rizvi and Jessica Huber and Paige Bartusiak and Bogdan Sacaleanu and Andrew Fano},
    booktitle ={Findings of the Association for Computational Linguistics: ACL 2022},
    month={may},
    year={2022},
    publisher={Association for Computational Linguistics},
    address={Dublin, The Republic of Ireland},
    url={},
    doi={},
    pages={},
}

@inproceedings{heindorf2020causenet,
  title={Causenet: Towards a causality graph extracted from the web},
  author={Heindorf, Stefan and Scholten, Yan and Wachsmuth, Henning and Ngonga Ngomo, Axel-Cyrille and Potthast, Martin},
  booktitle={Proceedings of the 29th ACM International Conference on Information \& Knowledge Management},
  pages={3023--3030},
  year={2020}
}

@inproceedings{cabot2021rebel,
  title={REBEL: Relation extraction by end-to-end language generation},
  author={Cabot, Pere-Llu{\'\i}s Huguet and Navigli, Roberto},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2021},
  pages={2370--2381},
  year={2021}
}


@article{luo2022biogpt,
  title={BioGPT: generative pre-trained transformer for biomedical text generation and mining},
  author={Luo, Renqian and Sun, Liai and Xia, Yingce and Qin, Tao and Zhang, Sheng and Poon, Hoifung and Liu, Tie-Yan},
  journal={Briefings in Bioinformatics},
  volume={23},
  number={6},
  year={2022},
  publisher={Oxford Academic}
}


@inproceedings{Rincon-Yanez2022,
  author = {Rincon-Yanez, Diego and Senatore, Sabrina},
  title = {{FAIR Knowledge Graph construction from text, an approach applied to fictional novels}},
  booktitle = {Proceedings of the 1st International Workshop on Knowledge Graph Generation From Text and the 1st International Workshop on Modular Knowledge co-located with 19th Extended Semantic Conference (ESWC 2022)},
  issn = {1613-0073},
  pages = {94--108},
  address = {Hersonissos, Greece},
  publisher = {CEUR-WS},
  url = {http://ceur-ws.org/Vol-3184/TEXT2KG_Paper_7.pdf},
  year = {2022}
}

@misc{NCT01306591,
    author = {Xiaoxin Li},
    note = {(2008, January - 2010, June)},
    title = {Bevacizumab for Neovascular Age-related Macular Degeneration},
    url ={https://clinicaltrials.gov/study/NCT01306591?a=1},
}
%%%%
@article{Yang24,
author = {Yang, Linyao and Chen, Hongyang and Li, Zhao and Ding, Xiao and Wu, Xindong},
year = {2024},
month = {01},
pages = {1-20},
title = {Give Us the Facts: Enhancing Large Language Models with Knowledge Graphs for Fact-aware Language Modeling},
volume = {PP},
journal = {IEEE Transactions on Knowledge and Data Engineering},
},
@inproceedings{gao-etal-2023-chatgpt,
    title = "Is {C}hat{GPT} a Good Causal Reasoner? A Comprehensive Evaluation",
    author = "Gao, Jinglong  and
      Ding, Xiao  and
      Qin, Bing  and
      Liu, Ting",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-emnlp.743",
    pages = "11111--11126",
    abstract = "Causal reasoning ability is crucial for numerous NLP applications. Despite the impressive emerging ability of ChatGPT in various NLP tasks, it is unclear how well ChatGPT performs in causal reasoning. In this paper, we conduct the first comprehensive evaluation of the ChatGPT{'}s causal reasoning capabilities. Experiments show that ChatGPT is not a good causal reasoner, but a good causal interpreter. Besides, ChatGPT has a serious hallucination on causal reasoning, possibly due to the reporting biases between causal and non-causal relationships in natural language, as well as ChatGPT{'}s upgrading processes, such as RLHF. The In-Context Learning (ICL) and Chain-of-Thought (COT) techniques can further exacerbate such causal hallucination. Additionally, the causal reasoning ability of ChatGPT is sensitive to the words used to express the causal concept in prompts, and close-ended prompts perform better than open-ended prompts. For events in sentences, ChatGPT excels at capturing explicit causality rather than implicit causality, and performs better in sentences with lower event density and smaller lexical distance between events.",
},



@Article{diag2023,
AUTHOR = {Muntean, G.A. and Marginean, A. and  Groza, A. and Damian, I. and Roman, S.A. and Hapca, M.C. and  Muntean, M.V.; Nicoară, S.D},
TITLE = {The Predictive Capabilities of Artificial Intelligence-Based OCT Analysis for Age-Related Macular Degeneration Progression—A Systematic Review},
JOURNAL = {Diagnostics},
VOLUME = {13},
YEAR = {2023},
NUMBER = {14},
ARTICLE-NUMBER = {2464},
URL = {https://www.mdpi.com/2075-4418/13/14/2464},
ISSN = {},
type ={journal},
infosite = {https://www.mdpi.com/2075-4418/13/14/2464},
durl = {https://www.mdpi.com/2075-4418/13/14/2464/pdf?version=1690265761}
},



@misc{jin2023large,
      title={Can Large Language Models Infer Causation from Correlation?}, 
      author={Zhijing Jin and Jiarui Liu and Zhiheng Lyu and Spencer Poff and Mrinmaya Sachan and Rada Mihalcea and Mona Diab and Bernhard Schölkopf},
      year={2023},
      eprint={2306.05836},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
},

@inproceedings{mateiu2023ontology,
  title={Ontology engineering with large language models},
  author={Mateiu, Patricia and Groza, Adrian},
  booktitle={2023 25th International Symposium on Symbolic and Numeric Algorithms for Scientific Computing (SYNASC)},
  pages={226--229},
  year={2023},
}

@misc{salnikov2023large,
      title={Large Language Models Meet Knowledge Graphs to Answer Factoid Questions}, 
      author={Mikhail Salnikov and Hai Le and Prateek Rajput and Irina Nikishina and Pavel Braslavski and Valentin Malykh and Alexander Panchenko},
      year={2023},
      eprint={2310.02166},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
},
@misc{mihindukulasooriya2023text2kgbench,
      title={Text2KGBench: A Benchmark for Ontology-Driven Knowledge Graph Generation from Text}, 
      author={Nandana Mihindukulasooriya and Sanju Tiwari and Carlos F. Enguix and Kusum Lata},
      year={2023},
      eprint={2308.02357},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
},
@misc{zecevic2023causal,
      title={Causal Parrots: Large Language Models May Talk Causality But Are Not Causal}, 
      author={Matej Zečević and Moritz Willig and Devendra Singh Dhami and Kristian Kersting},
      year={2023},
      eprint={2308.13067},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
},
@article{pub.1172391981,
 abstract = {MOTIVATION: Answering and solving complex problems using a large language model (LLM) given a certain domain such as biomedicine is a challenging task that requires both factual consistency and logic, and LLMs often suffer from some major limitations, such as hallucinating false or irrelevant information, or being influenced by noisy data. These issues can compromise the trustworthiness, accuracy, and compliance of LLM-generated text and insights.
RESULTS: Knowledge Retrieval Augmented Generation ENgine (KRAGEN) is a new tool that combines knowledge graphs, Retrieval Augmented Generation (RAG), and advanced prompting techniques to solve complex problems with natural language. KRAGEN converts knowledge graphs into a vector database and uses RAG to retrieve relevant facts from it. KRAGEN uses advanced prompting techniques: namely graph-of-thoughts (GoT), to dynamically break down a complex problem into smaller subproblems, and proceeds to solve each subproblem by using the relevant knowledge through the RAG framework, which limits the hallucinations, and finally, consolidates the subproblems and provides a solution. KRAGEN's graph visualization allows the user to interact with and evaluate the quality of the solution's GoT structure and logic.
AVAILABILITY AND IMPLEMENTATION: KRAGEN is deployed by running its custom Docker containers. KRAGEN is available as open-source from GitHub at: https://github.com/EpistasisLab/KRAGEN.},
 author = {Matsumoto, Nicholas and Moran, Jay and Choi, Hyunjun and Hernandez, Miguel E and Venkatesan, Mythreye and Wang, Paul and Moore, Jason H},
 journal = {Bioinformatics},
 keywords = {},
 number = {6},
 pages = {btae353},
 title = {KRAGEN: a knowledge graph-enhanced RAG framework for biomedical problem solving using large language models},
 url = {https://app.dimensions.ai/details/publication/pub.1172391981},
 volume = {40},
 year = {2024}
},
@article{10.1145/3571730,
author = {Ji, Ziwei and Lee, Nayeon and Frieske, Rita and Yu, Tiezheng and Su, Dan and Xu, Yan and Ishii, Etsuko and Bang, Ye Jin and Madotto, Andrea and Fung, Pascale},
title = {Survey of Hallucination in Natural Language Generation},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {12},
issn = {0360-0300},
doi = {10.1145/3571730},
month = {mar},
articleno = {248},
numpages = {38},
keywords = {consistency in NLG, factuality in NLG, faithfulness in NLG, extrinsic hallucination, intrinsic hallucination, Hallucination}
},
@misc{joshi2024,
      title={{LLM}s Are Prone to Fallacies in Causal Inference}, 
      author={Nitish Joshi and Abulhair Saparov and Yixin Wang and He He},
      year={2024},
      eprint={2406.12158},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.12158}, 
}
@book{10.5555/331969,
author = {Pearl, Judea},
title = {Causality: models, reasoning, and inference},
year = {2000},
isbn = {0521773628},
publisher = {Cambridge University Press},
address = {USA}
},
@book{Spirtes2000,
  added-at = {2009-09-12T19:19:34.000+0200},
  author = {Spirtes, P. and Glymour, C. and Scheines, R.},
  biburl = {https://www.bibsonomy.org/bibtex/2e2b107e8fd3469c8b0e944ca37a559f3/mozaher},
  edition = {2nd},
  keywords = {imported},
  owner = {Mozaherul Hoque},
  publisher = {MIT press},
  review = {PC algorithm},
  timestamp = {2009-09-12T19:19:43.000+0200},
  title = {Causation, Prediction, and Search},
  year = 2000
},
@article{devlin2018bert,
  title={BERT: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018},
  url={https://arxiv.org/abs/1810.04805}
},
@inproceedings{Vaswani_NIPS2017,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 volume = {30},
 year = {2017}
},
@misc{petroni2019,
      title={Language Models as Knowledge Bases?}, 
      author={Fabio Petroni and Tim Rocktäschel and Patrick Lewis and Anton Bakhtin and Yuxiang Wu and Alexander H. Miller and Sebastian Riedel},
      year={2019},
      eprint={1909.01066},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1909.01066}, 
},
@misc{bern1999semantic,
  author       = {Tim Berners-Lee},
  title        = {Semantic Web Road Map},
  year         = {1999},
  howpublished = {W3C},
  url          = {https://www.w3.org/DesignIssues/Semantic.html}
},
@article{collins1969retrieval,
  author       = {A. M. Collins and M. R. Quillian},
  title        = {Retrieval time from semantic memory},
  journal      = {Journal of Verbal Learning and Verbal Behavior},
  volume       = {8},
  number       = {2},
  pages        = {240--247},
  year         = {1969}
},
@misc{facebook2013graph,
  author       = {Facebook},
  title        = {Introducing Graph Search},
  year         = {2013},
  howpublished = {Facebook Official Blog},
  url          = {https://about.fb.com/news/2013/01/introducing-graph-search/}
},
@misc{bing2011knowledge,
  author       = {Microsoft},
  title        = {Introducing Bing's Knowledge Graph},
  year         = {2011},
  howpublished = {Bing Blog},
  url          = {https://blogs.bing.com/search/2011/05/09/introducing-bings-knowledge-graph/}
},
@article{hogan2021knowledge,
  author       = {A. Hogan and A. Harth and A. Polleres},
  title        = {Knowledge Graphs: New Opportunities and Challenges},
  journal      = {ACM Computing Surveys},
  volume       = {54},
  number       = {8},
  pages        = {1--36},
  year         = {2021}
},
@misc{mccarthy1956protocol,
  author       = {John McCarthy},
  title        = {Protocol for a Logical Calculus},
  year         = {1956},
  howpublished = {MIT},
  url          = {https://www-formal.stanford.edu/jmc/1956/protocol.html}
},
@book{minsky1968semantic,
  author       = {Marvin Minsky},
  title        = {Semantic Information Processing},
  year         = {1968},
  publisher    = {MIT Press}
},
@article{paulheim2017knowledge,
  author       = {Holger Paulheim},
  title        = {Knowledge Graphs: New Opportunities and Challenges},
  journal      = {ACM Computing Surveys},
  volume       = {54},
  number       = {8},
  pages        = {1--36},
  year         = {2017}
},
@misc{singhal2012introducing,
  author       = {Amit Singhal},
  title        = {Introducing the Knowledge Graph: Things, Not Strings},
  year         = {2012},
  howpublished = {Google Official Blog},
  url          = {https://www.blog.google/products/search/introducing-knowledge-graph-things-not-strings/}
},
@misc{w3c2001semantic,
  author       = {W3C},
  title        = {Semantic Web Overview},
  year         = {2001},
  howpublished = {W3C},
  url          = {https://www.w3.org/2001/sw/}
},
@inproceedings{Lewis_NEURIPS2020,
 author = {Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K\"{u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt\"{a}schel, Tim and Riedel, Sebastian and Kiela, Douwe},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {9459--9474},
 publisher = {Curran Associates, Inc.},
 title = {Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks},
 volume = {33},
 year = {2020}
},
@article{kragen_2024,
    author = {Matsumoto, Nicholas and Moran, Jay and Choi, Hyunjun and Hernandez, Miguel E and Venkatesan, Mythreye and Wang, Paul and Moore, Jason H},
    title = "{KRAGEN: a knowledge graph-enhanced RAG framework for biomedical problem solving using large language models}",
    journal = {Bioinformatics},
    volume = {40},
    number = {6},
    pages = {btae353},
    year = {2024},
    month = {06},
    abstract = "{Answering and solving complex problems using a large language model (LLM) given a certain domain such as biomedicine is a challenging task that requires both factual consistency and logic, and LLMs often suffer from some major limitations, such as hallucinating false or irrelevant information, or being influenced by noisy data. These issues can compromise the trustworthiness, accuracy, and compliance of LLM-generated text and insights.Knowledge Retrieval Augmented Generation ENgine (KRAGEN) is a new tool that combines knowledge graphs, Retrieval Augmented Generation (RAG), and advanced prompting techniques to solve complex problems with natural language. KRAGEN converts knowledge graphs into a vector database and uses RAG to retrieve relevant facts from it. KRAGEN uses advanced prompting techniques: namely graph-of-thoughts (GoT), to dynamically break down a complex problem into smaller subproblems, and proceeds to solve each subproblem by using the relevant knowledge through the RAG framework, which limits the hallucinations, and finally, consolidates the subproblems and provides a solution. KRAGEN’s graph visualization allows the user to interact with and evaluate the quality of the solution’s GoT structure and logic.KRAGEN is deployed by running its custom Docker containers. KRAGEN is available as open-source from GitHub at: https://github.com/EpistasisLab/KRAGEN.}",
    issn = {1367-4811},
}

@article{pub.1182771521,
 abstract = {The capabilities of Large Language Models (LLMs,) such as Mistral 7B, Llama 3, GPT-4, present a significant opportunity for knowledge extraction (KE) from text. However, LLMs’ context-sensitivity can hinder obtaining precise and task-aligned outcomes, thereby requiring prompt engineering. This study explores the efficacy of five prompt methods with different task demonstration strategies across 17 different prompt templates, utilizing a relation extraction dataset (RED-FM) with the aforementioned LLMs. To facilitate evaluation, we introduce a novel framework grounded in Wikidata’s ontology. The findings demonstrate that LLMs are capable of extracting a diverse array of facts from text. Notably, incorporating a simple instruction accompanied by a task demonstration – comprising three examples selected via a retrieval mechanism – significantly enhances performance across Mistral 7B, Llama 3, and GPT-4. The effectiveness of reasoning-oriented prompting methods such as Chain-of-Thought, Reasoning and Acting, while improved with task demonstrations, does not surpass alternative methods. This suggests that framing extraction as a reasoning task may not be necessary for KE. Notably, task demonstrations leveraging examples selected via retrieval mechanisms facilitate effective knowledge extraction across all tested prompting strategies and LLMs.},
 author = {Polat, Fina and Tiddi, Ilaria and Groth, Paul},
 doi = {10.3233/sw-243719},
 journal = {Semantic Web},
 keywords = {},
 number = {},
 pages = {1-34},
 title = {Testing prompt engineering methods for knowledge extraction from text},
 volume = {},
 year = {2024}
}

@article{LECU2024443,
title = {Using LLMs and ontologies to extract causal relationships from medical abstracts},
journal = {Procedia Computer Science},
volume = {244},
pages = {443-452},
year = {2024},
note = {6th International Conference on AI in Computational Linguistics},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.10.219},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924030205},
author = {Alexandru Lecu and Adrian Groza and Lezan Hawizy},
keywords = {Causal Relation Extraction, Knowledge Graphs, Large Language Models, Age-Related Macular Degeneration},
abstract = {The substantiation of the causal relationships behind its development is very important in identifying possible interventions and early treatment. Knowledge Graphs (KG) play a crucial role in the medical research domain by organizing data into interconnected structures that represent relationships between entities such as disease, treatments, and progressions. This paper shows a complete workflow that demonstrates the extraction of causal relationships from medical abstracts using a fine-tuned GPT-based model and the integration of these relationships into a KG.}
}

@article{wei2022chain,
    title = {Chain-of-Thought Prompting Elicits Reasoning in Large Language Models},
    author = {Wei, Jason and others},
    journal = {arXiv preprint arXiv:2201.11903},
    year = {2022}
}

@article{yang2024kgllm,
    title = {Enhancing Pretrained Language Models with Knowledge Graphs for Fact-Aware Language Modeling},
    author = {Yang, L. and Chen, H. and Li, Z. and others},
    journal = {IEEE Transactions on Knowledge and Data Engineering},
    pages = {1--20},
    year = {2024},
    doi = {10.1109/TKDE.2024.XXXXXXX}
}

@article{singhal2023large,
  title={Large language models encode clinical knowledge},
  author={Singhal, K. and Azizi, S. and Tu, T. and Mahdavi, S.S. and Wei, J. and Chung, H.W. and Scales, N. and Tanwani, A. and Cole-Lewis, H. and Pfohl, S. and others},
  journal={Nature},
  volume={620},
  pages={172--180},
  year={2023},
  doi={10.1038/s41586-023-XXXX-X}
}
