\documentclass{article}
\usepackage{arxiv_style}
\usepackage{placeins}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}   
\usepackage{url}
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\usepackage{xfrac}
\usepackage{bm}
\usepackage{microtype}      % microtypography
\usepackage{blindtext}
\usepackage{float}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage{caption}
\newcommand\blfootnote[1]{%
  \begingroup
  \renewcommand\thefootnote{}\footnote{#1}%
  \addtocounter{footnote}{-1}%
  \endgroup
}



\DeclareCaptionFormat{custom}
{%
    \textbf{#1#2} #3
}

\DeclareCaptionLabelSeparator{custom}{ |}
\captionsetup
{
    format=custom,%
    labelsep=custom
}


\usepackage{subcaption}
%\subcaptionsetup[figure]{labelformat=simple, labelsep=period, justification=raggedright, singlelinecheck=off, position=top, labelfont={sf, bf}}
\usepackage[group-separator={,}]{siunitx}

\usepackage[capitalize]{cleveref}
\newcommand{\params}{\boldsymbol{\theta}}
\newcommand{\needsref}{[\textbf{REF!}]}

  

\title{Meta-neural Topology Optimization: Knowledge Infusion with Meta-learning}

\author{
	Igor Kuszczak\\
	University College London\\
	University of Applied Sciences and Arts of Southern Switzerland\\
	\And
	 Gaweł Kuś\\
	Brown University \\
	Providence, RI \\
	\And
	Federico Bosi\\
	University College London\\
	University of Applied Sciences and Arts of Southern Switzerland\\
	\texttt{f.bosi@ucl.ac.uk} \\
	\And 
	Miguel A. Bessa \\
	Brown University \\
	Providence, RI \\
	\texttt{miguel\_bessa@brown.edu} \\
}

\begin{document}

\maketitle


\begin{abstract}
Engineers learn from every design they create, building intuition that helps them quickly identify promising solutions for new problems. Topology optimization (TO) -- a well-established computational method for designing structures with optimized performance -- lacks this ability to learn from experience. Existing approaches treat design tasks in isolation, starting from a ``blank canvas'' design for each new problem, often requiring many computationally expensive steps to converge. We propose a meta-learning strategy, termed meta-neural TO, that finds effective initial designs through a systematic transfer of knowledge between related tasks, building on the mesh-agnostic representation provided by neural reparameterization. We compare our approach against established TO methods, demonstrating efficient optimization across diverse test cases without compromising design quality. Further, we demonstrate powerful cross-resolution transfer capabilities, where initializations learned on lower-resolution discretizations lead to superior convergence in 74.1\% of tasks on a higher-resolution test set, reducing the average number of iterations by 33.6\% compared to standard neural TO. Remarkably, we discover that meta-learning naturally gravitates toward the strain energy patterns found in uniform density designs as effective starting points, aligning with engineering intuition. 

  \keywords{Topology optimization \and neural networks \and meta-learning \and implicit neural representations}
  \vspace{5mm}
\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Topology optimization (TO) has transformed computational design across multiple scales and disciplines. Its successful applications span from nanometer-scale optical cavities that transcend traditional limitations on light confinement~\cite{albrechtsen2022}, to patient-specific prosthetic implants that better match bone properties~\cite{muller2024}, and full-scale aircraft wing designs that achieve unprecedented structural efficiency~\cite{aage2017}. While the field has seen significant advances since the pioneering work by Bendsøe and Kikuchi~\cite{bendsoe1988}, one fundamental aspect of human design capability remains underexplored: learning from experience. Conventional TO approaches treat design tasks in isolation, reinitializing design variables before every run, despite evidence that strategic initial designs can dramatically accelerate convergence~\cite{sigmund2013}. This represents a missed opportunity -- since each TO iteration requires an expensive finite element analysis, starting from a more informed design could reduce computational expense and guide the algorithm toward better-performing solutions. 

To develop topology optimization methods that learn from experience, we must first understand the limitations of conventional approaches. In density-based methods~\cite{bendsoe1989}, the design domain is discretized into finite elements (FEs), and each element is assigned a density value between 0 and 1, indicating the material occupancy. A gradient-based optimizer, commonly the Method of Moving Asymptotes (MMA)~\cite{svanberg2002}, is then used to identify the set of densities that minimizes the design objective. Because the design variables are tied to the computational mesh, any change in discretization or geometry requires restarting the optimization with a new set of design variables~\cite{white2020}, making it difficult to transfer knowledge between tasks. Neural topology optimization (neural TO) ~\cite{hoyer2019, deng2020} offers an alternative by encoding the material distribution in learnable parameters of neural networks. Approaches based on neural implicit representations, in particular, provide a design representation that is intrinsically mesh-agnostic~\cite{chandrasekhar2021}. Neural TO has shown several compelling advantages: reshaping the loss landscape to uncover superior designs~\cite{sanu2024, herrmann2024}; inducing regularization effects similar to traditional filtering techniques~\cite{dupuis2021, chandrasekhar2021b, doosti2021}; and enabling direct design conditioning on physical fields like strain energy density~\cite{nie2021, chen2023, nobari2024}. However, Sanu et al.~\cite{sanu2024} recently showed that reparameterization induces highly non-convex loss landscapes, leading to increased iteration counts compared to conventional TO methods when starting from standard neural network initializations. 

These characteristics -- a flexible representation that enables knowledge transfer, combined with challenging optimization landscapes -- make neural TO an ideal test-bed for developing methods that learn from experience. Thus far, researchers have primarily focused on heuristic initialization strategies, such as starting from learnable parameters corresponding to a uniform density distribution~\cite{zhang2021, doosti2021}. A notable exception is the work of Herrmann et al.~\cite{herrmann2024}, who demonstrated that transfer learning techniques can help identify superior local optima in neural acoustic TO.

Meta-learning has emerged as a powerful paradigm for distilling knowledge from learning episodes across related tasks~\cite{hospedales2022}. Rather than learning each task from scratch, meta-learning algorithms discover shared patterns that accelerate learning. Methods like MAML~\cite{finn2017} and Reptile~\cite{nichol2018}, which learn network initializations, have proven particularly effective at fitting signals with coordinate-based neural networks in few gradient-descent steps. This has been demonstrated across multiple signal modalities, including signed distance functions~\cite{sitzmann2020}, images, and 3D scenes~\cite{tancik2021}. Building upon these advances and recognizing the crucial role of experience in engineering design, we propose to apply meta-learning to neural TO for compliance minimization. In this article, we demonstrate that meta-learned network initializations require fewer iterations to converge to task-specific optimized designs compared to standard initializations without compromising the final design quality. Remarkably, our meta-learning approach discovers that strain energy patterns found in uniform density designs serve as effective initializations, aligning with engineering intuition about material placement in structural design problems.

We note the importance of distinguishing our approach from direct design methods that use neural networks to predict optimized structures in a single forward pass. As thoroughly reviewed in~\cite{woldseth2022} and~\cite{shin2023}, these approaches often suffer from structural disconnections, limited generalization to new boundary conditions, and restrictions to specific mesh sizes. Our method focuses on learning effective initialization strategies that accelerate the conventional iterative optimization process. We develop data-driven insights through partial optimizations, eliminating the need for precomputed datasets of optimized designs and preventing potential biases from the optimizers used to generate them. As a result, we maintain the reliability of traditional TO while leveraging data-driven acceleration.

\section*{Methods}
Our approach frames neural TO as a bi-level optimization problem as illustrated in \cref{fig:paths}. At the inner level, we perform several TO steps on specific tasks, following the process outlined in \cref{fig:overview}. At the outer level, we update the initialization to minimize the expected loss over multiple tasks after task-specific adaptation. The learned initialization should lead to faster convergence on unseen tasks as shown in \cref{fig:progression}. Further details on the problem formulation, model architecture, and optimization procedure can be found in \crefrange{app:formulation}{app:details}.

\textbf{Reparameterization network architecture.} We reparameterized the density field with a SIREN~\cite{sitzmann2020} with ResNet-like skip connections~\cite{lu2021}. The network $f_{\boldsymbol{\theta}}$ maps spatial coordinates $\mathbf{x}$ of element centroids and the corresponding element strain energy values $\mathbf{E(x)}$ calculated for a uniform design to element densities $\boldsymbol{\rho}$. Here, $\boldsymbol{\theta}$ denotes the learnable parameters of the network.
\begin{equation}
\rho(\mathbf{x} \,|\, \mathbf{E}(\mathbf{x}); \boldsymbol{\theta}) = f_{\boldsymbol{\theta}}(\mathbf{x}, \mathbf{E}(\mathbf{x})).
\end{equation}

SIRENs are particularly well-suited for neural TO ~\cite{sanu2024, zehnder2021} because they can effectively represent fine (high-frequency) structural details while being governed by a single hyperparameter $\omega_0$. We found that skip connections helped to preserve the strain energy conditioning signal across the network depth and stabilized the meta-training. We used four hidden layers of width 256 arranged into two residual blocks, followed by a linear output layer. This setup allowed sufficient expressivity while remaining computationally tractable for meta-learning.
\begin{figure*}[!thb]
     \centering
          \begin{subfigure}[t]{0.49\linewidth}
         \subcaption{}
         \centering
         \includegraphics[width=80mm]{figures/reptile_2.pdf}
         \label{fig:paths}
     \end{subfigure}
     \hfill
         \begin{subfigure}[t]{0.49\linewidth}
         \subcaption{}
         \centering
         \includegraphics[width=80mm]{figures/progression_neural.pdf}
         \label{fig:progression}
     \end{subfigure}
     \begin{subfigure}[t]{\linewidth}
         \subcaption{}
         \centering
         \includegraphics[width=160mm]{figures/framework_overview.pdf}
         \label{fig:overview}
     \end{subfigure}
    \caption{\textbf{Meta-neural topology optimization framework.} \textbf{a.} Meta-learning identifies an initialization that adapts to new design tasks in fewer iterations. \textbf{b.}  Example design trajectories demonstrating accelerated convergence using meta-learned versus standard initialization in neural topology optimization. \textbf{c.} Neural topology optimization pipeline: neural network maps input coordinates $\mathbf{x}$ and strain energy $\mathbf{E}(\mathbf{x})$ to density fields $\boldsymbol{\rho}(\mathbf{x})$, which are filtered and evaluated with FE analysis.}
\end{figure*}

\textbf{Meta-learning.} We investigated MAML~\cite{finn2017} and Reptile~\cite{nichol2018} for finding an efficient network initialization $\boldsymbol{\theta}$, ultimately choosing Reptile. Our experiments showed that it provided lower computational overhead and more stable meta-training~\cite{antoniou2019, huisman2021}. In each meta-training iteration, Reptile samples a batch of tasks from the meta-training set, performs a few optimization steps, and moves the initialization towards the average of the task-specific adapted weights.

\textbf{Task-specific loss function.} To meaningfully compare and aggregate losses across problem instances, we normalized the design compliances $c(\boldsymbol{\theta}, \tau_{i})$ by the compliance of a uniform design evaluated under task-specific boundary conditions and target volume fraction $c_{\text{ref}}(\tau_{i})$. The loss for parameters $\boldsymbol{\theta}$ and task $\tau_{i}$ is thus $\mathcal{L}(\boldsymbol{\theta},\tau_{i}) = \sfrac{c(\boldsymbol{\theta}, \tau_{i})}{c_{\text{ref}}(\tau_{i})}$.

\textbf{Meta-training.} We trained the Reptile algorithm for \num{6000} iterations with a meta-batch size of \num{5}, ensuring that each task in the meta-training dataset is sampled exactly once. We observed that extending the training beyond \num{6000} iterations led to meta-overfitting, where the learned initialization failed to generalize effectively across training tasks. Instead, it excelled on a subset of tasks while severely underperforming on others. A similar issue arose when the outer learning rate was set too high. The inner loop consisted of \num{10} descent steps striking a balance between computational efficiency and effective task-specific adaptation. We used the Adam optimizer~\cite{kingma2017} for both the inner and outer loop updates. Our preliminary investigation revealed that stochastic gradient descent, often used in the context of meta-learning, struggled to find meaningful designs in the inner loop. The complete meta-training took 3 hours and 35 minutes on a single NVIDIA A100 GPU.

\textbf{Task generation.} To enable meta-learning, we constructed datasets of pseudorandom topology optimization tasks, each defined by a set of boundary conditions and a target volume fraction. The tasks were defined on a square design domain discretized by $64 \times 64$ regular elements unless specified otherwise. We generated \num{30000} meta-training tasks with point loads and supports using the method proposed in~\cite{sosnovik2017}. To increase the diversity, we sampled the target volume fraction uniformly in the range $[0.1, 0.5]$ and applied loads at angles sampled uniformly from $[0, 2\pi]$ radians. We constructed three test datasets of \num{1000} tasks each to evaluate different levels of generalization: (i) in-distribution dataset with point loads and supports matching the training distribution, (ii) out-of-distribution dataset with loads and supports uniformly distributed over randomly selected lines based on~\cite{maze2022} for testing the transfer to novel boundary conditions, and (iii) a cross-resolution dataset with point loads and supports on a $256\times256$ discretization. We also held out 100 tasks for hyperparameter tuning. When generating the datasets, we performed one FE analysis per task to calculate the strain energy field $\mathbf{E_{\text{raw}}}$, and a reference compliance $c_{\text{ref}}$ for the uniform density design at the prescribed volume fraction. This analysis enabled us to filter out and discard tasks with ill-defined boundary conditions.

\section*{Results}
We evaluated our approach on three test datasets examining \textbf{(i)} whether meta-learned initializations can consistently accelerate convergence, \textbf{(ii)} how they affect the final design compliance, and \textbf{ (iii)} how they generalize to tasks outside of the training distribution.

\textbf{Baselines.} We compared our approach (meta-neural TO) against two baselines: neural TO with a standard SIREN initialization~\cite{sitzmann2020} and hyperparameters tuned on the validation set (neural TO), and a conventional density-based TO using the MMA optimizer with default NLOpt settings~\cite{johnson2007} (standard TO). These baselines offer complementary perspectives for assessing our method. Neural TO helps to isolate the effect of meta-learned initialization, while standard TO provides context for the practical relevance of our approach.

\textbf{Testing procedure.}
At test time, we optimized each task for a minimum of 10 and a maximum of 200 iterations. We recorded the number of iterations to reach a stopping criterion and the compliances of final designs before and after binary thresholding. Thresholding is a post-processing step that converts intermediate density values to either solid or void material. As explained in \cref{app:thresholding}, it has a significant effect on the final compliance. In the main text, we only report thresholded compliances to ensure fair comparisons across methods~\cite{sigmund2022a}. For further details on the stopping criterion and thresholding, see \cref{app:details}.

\textbf{Performance profiles.} We compared the methods using performance profiles, as shown in \cref{fig:perfprof}. For each task, we identified the best-performing method(s) and assessed the performance of other methods relative to it. The profiles illustrate the fraction of tasks each method solves within a tolerance of the best result. For example, a performance ratio of 0.9 at the tolerance of 1.5 indicates that a given method solves 90\% of the test tasks within a 50\% error of the best performer for each task. The profiles reveal how often an algorithm is a top performer ($\text{tol} = 1$) and how closely it trails the leader (at $\text{tol} > 1$). We direct the readers to~\cite{dolan2004} for methodological details on performance profiles and~\cite{rojas-labanda2015} for an application in TO.

\textbf{In-distribution.} Meta-neural TO converged in fewest iteration in 57.6\% of in-distribution tasks, followed by standard TO at 42.3\% and neural TO at 11.70\% (\cref{fig:id_steps}). The sum of performance ratios exceeds 100\% here due to tasks where all methods use the maximum budget of 200 iterations, tying for the fastest convergence. There were only 48 such tasks, suggesting that the 200-iteration budget was sufficient for comparing performance across methods. On average, meta-neural TO required 103.51 iterations, standard TO 110.97, and neural TO 149.47. These results indicate that neural TO with meta-learned initialization not only matches the optimization efficiency of the conventional approach but can surpass it. While standard TO found the best thresholded designs in 63.1\% of tasks, compared to 20.3\% for meta-neural TO and 16.6\% for neural TO (\cref{fig:id_loss_thr}), all methods performed comparably within a 5\% tolerance, achieving performance ratios of 83.9\%, 80.9\%, and 80.4\%, respectively. Meta-neural TO consistently produced high-quality designs comparable to standard TO. In fact, the analysis in \cref{app:thresholding} shows that meta-neural TO found superior continuous designs, whose advantage was reduced by thresholding.

\textbf{Out-of-distribution.} On the dataset with uniformly distributed loads and supports, meta-neural TO was fastest on 39.3\% of tasks, outperforming neural TO at 12.5\% but trailing standard TO at 59.9\% (\cref{fig:ood_steps}). Meta-neural TO required an average of 126.57 iterations, neural TO 154.67, and standard TO 110.89. The quality of thresholded designs showed a similar trend (\cref{fig:ood_loss_thr}) - meta-neural TO found superior designs in 24.6\% of tasks versus standard TO's 47.6\%, with both methods achieving similar success rates at a 5\% tolerance.

\textbf{Cross-resolution.} Meta-neural TO excelled in cross-resolution experiments, where it was applied to tasks with finer discretization than used at meta-training. It required the fewest iterations in 74.1\% of tasks compared to 25.7\% for standard TO and 10.80\% for neural TO (\cref{fig:upscaled_steps}). On average, meta-neural TO converged in 100.90 iterations, standard TO in 130.03, and neural TO in 152.04 -- the learned initialization reduced the average number of iterations by 33.6\% compared to neural TO. This experiment highlights the advantageous scaling of neural representations -- while standard TO must handle 16 times more design variables, neural TO benefits from a mesh-independent parameterization. The improved efficiency did not come at the cost of final design quality as meta-neural TO and standard TO found competitive final thresholded designs, winning in 41.0\% and 48.5\% of tasks, respectively (\cref{fig:upscaled_loss_thr}). At a 5\% tolerance, meta-neural TO achieved a performance ratio of 87.7\%, surpassing standard TO's 83.6\%. These results demonstrate an effective transfer of meta-learned initializations across mesh resolutions. 

\begin{figure*}[!thb]
\begin{minipage}{\linewidth}
     \centering
     % ID STEPS
     \begin{subfigure}[t]{0.3\linewidth}
         \subcaption{}
         \centering
         \includegraphics[width=50mm]{figures/in-distribution/steps_perfprof_200_test_steps.pdf}
         \label{fig:id_steps}
     \end{subfigure}
     \hfill
     % OOD STEPS
     \begin{subfigure}[t]{0.3\linewidth}
         \subcaption{}
         \centering
         \includegraphics[width=50mm]{figures/out-of-distribution/steps_perfprof_200_test_steps.pdf}
         \label{fig:ood_steps}
     \end{subfigure}
    \hfill
     % UPSCALED STEPS
     \begin{subfigure}[t]{0.3\linewidth}
         \subcaption{}
         \centering
         \includegraphics[width=50mm]{figures/upscaled/steps_perfprof_200_test_steps.pdf}
            \label{fig:upscaled_steps}
     \end{subfigure}
     
     % ID LOSS
     \begin{subfigure}[t]{0.3\linewidth}
         \subcaption{}
         \centering
         \includegraphics[width=50mm]{figures/in-distribution/thresholded_loss_perfprof_200_test_steps.pdf}
         
         \label{fig:id_loss_thr}
     \end{subfigure}
     \hfill
     % OOD LOSS
     \begin{subfigure}[t]{0.3\linewidth}
         \subcaption{}
         \centering
         \includegraphics[width=50mm]{figures/out-of-distribution/thresholded_loss_perfprof_200_test_steps.pdf}
         \label{fig:ood_loss_thr}
     \end{subfigure}
     \hfill
          \begin{subfigure}[t]{0.3\linewidth}
         \subcaption{}
         \centering
         \includegraphics[width=50mm]{figures/upscaled/thresholded_loss_perfprof_200_test_steps.pdf}
         \label{fig:upscaled_loss_thr}
     \end{subfigure}
     \end{minipage}
     \hfill
     \begin{minipage}{\linewidth}
     \centering
        \includegraphics[width=100mm]{figures/legends/perfprof_legend_rotated.pdf}
     \end{minipage}
     \caption{\textbf{Performance profiles for meta-neural TO and baseline methods.} The top row compares the number of iterations for in-distribution (left), out-of-distribution (middle), and cross-resolution (right) experiments. The bottom row (\textbf{d-f}) compares the thresholded design compliances for the corresponding experiments.}
     \label{fig:perfprof}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\textbf{Representative designs.} \cref{fig:id_test_designs} illustrates the continuous designs for in-distribution tasks with the best, typical (median), and worst final compliance improvement of meta-neural TO over neural TO. The best and typical designs achieved 37.72\% and 1.50\% improvements in compliance, respectively, through the use of fine structural features. The worst case showed a drastic 149.8\% increase in compliance. We found that such significant deterioration occurred only in tasks with low target volume fractions where the lengthscale imposed by the density filter made it challenging to connect loads and supports. The distinct convergence patterns and visual differences between final designs highlight neural TO's initialization sensitivity, validating our focus on learning robust initializations.

\begin{figure*}[!thb]
     \begin{minipage}{0.9\linewidth}
     \centering
     \includegraphics[scale=1.0]{figures/test_designs_200_steps.pdf}
     \end{minipage}
     \begin{minipage}{0.09\linewidth}
     \includegraphics[width=10mm]{figures/legends/test_designs_legend.pdf}
     \end{minipage}
 \caption{\textbf{Comparison of neural-TO and meta-neural TO on representative in-distribution tasks.} Convergence plots showcase the tasks with the best, worst, and typical improvements in continuous design compliance from using meta-learned initializations and corresponding final designs.}
 \label{fig:id_test_designs}
\end{figure*}
\textbf{Initial designs.} The analysis of meta-learned initial designs at different $\omega_{0}$ values reveals an unexpected connection. As shown in \cref{fig:initial_designs}, the initial density distributions closely mirror the designs obtained by directly applying the volume and density filters to the input strain energy density fields $\mathbf{E}(\mathbf{x})$. To verify this relationship, we pretrained the reparameterization network to perform an identity mapping from the input strain-energy density fields across all training tasks. Surprisingly, this simple approach outperformed meta-neural TO in both convergence speed and final design quality (see \cref{app:pretraining}). This finding suggests that strain energy density represents a robust prior for material distribution that meta-learning naturally discovers when trained on a diverse set of tasks.

\begin{figure}[!thb]
    \centering
    \includegraphics[width=120mm]{figures/initial_designs/initial_design_comparison.pdf}
    \caption{\textbf{Relationship between meta-learned initial designs and filtered strain energy density fields.} Initial designs generated by meta-neural TO (middle and bottom rows) closely mirror the filtered strain energy density fields of uniform density designs, suggesting meta-learning naturally discovers the physical relevance of strain energy patterns.}
    \label{fig:initial_designs}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage % for better figure layout
\section*{Discussion}
We introduced a novel method -- meta-neural TO -- that infuses past knowledge in topology optimization via meta-learning and neural reparameterization. While conventional TO methods treat each design problem in isolation, our framework accumulates and transfers design experience across related tasks while generalizing well to unseen tasks. Previous studies explored pretraining in neural TO~\cite{herrmann2024, zhang2021} but lacked a systematic mechanism for knowledge transfer across tasks. Our approach consistently accelerates neural TO while achieving designs comparable to standard TO, bridging the efficiency gap described in \cite{sanu2024}. We show that meta-learned initializations can remedy the slow convergence in neural TO. Cross-resolution experiments further demonstrate the unique scaling advantages of neural reparameterization.

We found that conditioning on strain energy fields is crucial for effective knowledge transfer and generalization. Remarkably, meta-learning discovered that strain energy density patterns in a uniform density design make for an effective initial design across different boundary conditions. While this connection may seem obvious in hindsight, the systematic discovery of physically meaningful relationships through meta-learning suggests a broader potential for knowledge transfer in less well-understood domains. The experiments without strain energy conditioning (\cref{app:conditioning}) further highlight the importance of physical information for successful knowledge transfer -- while meta-learning still improved over standard initialization, the gains were substantially smaller when not using conditioning. Additionally, we found that meta-learned initializations could accelerate conventional TO, but only when conditioned on strain energy (\cref{app:cross-optimizer}), suggesting the discovery of optimizer-agnostic physical principles that can be transferred between different optimization approaches.

Our approach is a significant advancement over direct-design approaches~\cite{woldseth2022} that map boundary conditions to optimized structures. By performing partial optimizations during meta-training, we develop data-driven insights without the need for precomputed datasets of optimized designs. This train-time optimization approach makes our method readily applicable to different optimizers and design parameterizations, facilitating extensions to other frameworks, e.g., L-BFGS-CNN proposed in~\cite{hoyer2019}. Meta-neural TO uniquely combines data-driven knowledge transfer with the adaptability of conventional TO, mirroring how experienced engineers leverage prior knowledge to generate initial designs and iteratively refine them based on specific requirements.

% Step 4 - Address the limitations 
However, the presented method also has practical limitations. First, while meta-neural TO reduces iterations at test time, the meta-training itself demands substantial computational resources. Fortunately, this can be mitigated by training on coarse meshes, as demonstrated by our cross-resolution results. Second, meta-training focuses on short-term performance and may learn initializations that promote rapid initial improvement but bias solutions towards poor-quality local minima. As shown by the worst-case in-distribution task, this can lead to designs inferior to those obtained from generic starting points. Third, meta-learning performance is sensitive to the similarity between training and test tasks. While we demonstrate good generalization to distributed loads, one should expect performance deterioration as problems deviate further from the training distribution. We expect that many opportunities for future developments on this front.

Learning from experience enabled by our method is particularly relevant for industries where similar design problems are solved repeatedly. In aerospace and automotive design, engineers often optimize components with varying boundary conditions but similar underlying physics - precisely where accumulated design knowledge proves most valuable. The method could be especially beneficial for rapid prototyping and early-stage design exploration, where quick iteration is prioritized over finding globally optimal solutions.

% Further work
The presented approach relies on a simple meta-learning algorithm, leaving ample room for future enhancements. Advanced meta-learning techniques could further reduce test-time iterations, better justifying the initial computational investment. Promising directions include expanding meta-knowledge to learning rates, as in Meta-SGD~\cite{li2017}, incorporating boundary condition similarity metrics to guide adaptation, as in~\cite{lynch2019a}, and exploring multi-fidelity approaches that build on cross-resolution transfer.

% Step 6 - Conclusion - summarize the content
Meta-neural TO demonstrates how machine learning can enhance rather than replace conventional engineering approaches. The automatic discovery of physical principles through meta-learning suggests exciting possibilities for applying similar techniques to more complex engineering domains where relevant features are less well understood.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Data and code availability}
The datasets and code used to generate the results presented in this paper will be released under an open-source license upon publication in a peer-reviewed journal.
\section*{Acknowledgments}
This research was supported by the European Union's Horizon 2020 Research and Innovation programme through the Marie Skłodowska-Curie grant (Agreement No. 956547-LIGHTEN). The authors gratefully acknowledge Suryanarayanan Manoj Sanu for their valuable feedback and thorough review of the manuscript. Special thanks to Louis Kirsch for the insightful discussions that laid the foundation for this work.


%Bibliography
\bibliographystyle{naturemag}  
\begin{thebibliography}{10}
	\expandafter\ifx\csname url\endcsname\relax
	\def\url#1{\texttt{#1}}\fi
	\expandafter\ifx\csname urlprefix\endcsname\relax\def\urlprefix{URL }\fi
	\providecommand{\bibinfo}[2]{#2}
	\providecommand{\eprint}[2][]{\url{#2}}
	
	\bibitem{albrechtsen2022}
	\bibinfo{author}{Albrechtsen, M.} \emph{et~al.}
	\newblock \bibinfo{title}{Nanometer-scale photon confinement in
		topology-optimized dielectric cavities}.
	\newblock \emph{\bibinfo{journal}{Nature Communications}}
	\textbf{\bibinfo{volume}{13}}, \bibinfo{pages}{6281} (\bibinfo{year}{2022}).
	
	\bibitem{muller2024}
	\bibinfo{author}{M{\"u}ller, P.} \emph{et~al.}
	\newblock \bibinfo{title}{Development of a density-based topology optimization
		of homogenized lattice structures for individualized hip endoprostheses and
		validation using micro-{{FE}}}.
	\newblock \emph{\bibinfo{journal}{Scientific Reports}}
	\textbf{\bibinfo{volume}{14}}, \bibinfo{pages}{5719} (\bibinfo{year}{2024}).
	
	\bibitem{aage2017}
	\bibinfo{author}{Aage, N.}, \bibinfo{author}{Andreassen, E.},
	\bibinfo{author}{Lazarov, B.~S.} \& \bibinfo{author}{Sigmund, O.}
	\newblock \bibinfo{title}{Giga-voxel computational morphogenesis for structural
		design}.
	\newblock \emph{\bibinfo{journal}{Nature}} \textbf{\bibinfo{volume}{550}},
	\bibinfo{pages}{84--86} (\bibinfo{year}{2017}).
	
	\bibitem{bendsoe1988}
	\bibinfo{author}{Bends{\o}e, M.~P.} \& \bibinfo{author}{Kikuchi, N.}
	\newblock \bibinfo{title}{Generating optimal topologies in structural design
		using a homogenization method}.
	\newblock \emph{\bibinfo{journal}{Computer Methods in Applied Mechanics and
			Engineering}} \textbf{\bibinfo{volume}{71}}, \bibinfo{pages}{197--224}
	(\bibinfo{year}{1988}).
	
	\bibitem{sigmund2013}
	\bibinfo{author}{Sigmund, O.} \& \bibinfo{author}{Maute, K.}
	\newblock \bibinfo{title}{Topology optimization approaches}.
	\newblock \emph{\bibinfo{journal}{Structural and Multidisciplinary
			Optimization}} \textbf{\bibinfo{volume}{48}}, \bibinfo{pages}{1031--1055}
	(\bibinfo{year}{2013}).
	
	\bibitem{bendsoe1989}
	\bibinfo{author}{Bends{\o}e, M.~P.}
	\newblock \bibinfo{title}{Optimal shape design as a material distribution
		problem}.
	\newblock \emph{\bibinfo{journal}{Structural optimization}}
	\textbf{\bibinfo{volume}{1}}, \bibinfo{pages}{193--202}
	(\bibinfo{year}{1989}).
	
	\bibitem{svanberg2002}
	\bibinfo{author}{Svanberg, K.}
	\newblock \bibinfo{title}{A {{Class}} of {{Globally Convergent Optimization
				Methods Based}} on {{Conservative Convex Separable Approximations}}}.
	\newblock \emph{\bibinfo{journal}{SIAM Journal on Optimization}}
	\textbf{\bibinfo{volume}{12}}, \bibinfo{pages}{555--573}
	(\bibinfo{year}{2002}).
	
	\bibitem{white2020}
	\bibinfo{author}{White, D.~A.}, \bibinfo{author}{Choi, Y.} \&
	\bibinfo{author}{Kudo, J.}
	\newblock \bibinfo{title}{A dual mesh method with adaptivity for
		stress-constrained topology optimization}.
	\newblock \emph{\bibinfo{journal}{Structural and Multidisciplinary
			Optimization}} \textbf{\bibinfo{volume}{61}}, \bibinfo{pages}{749--762}
	(\bibinfo{year}{2020}).
	
	\bibitem{hoyer2019}
	\bibinfo{author}{Hoyer, S.}, \bibinfo{author}{{Sohl-Dickstein}, J.} \&
	\bibinfo{author}{Greydanus, S.}
	\newblock \bibinfo{title}{Neural reparameterization improves structural
		optimization} (\bibinfo{year}{2019}).
	\newblock \eprint{1909.04240}.
	
	\bibitem{deng2020}
	\bibinfo{author}{Deng, H.} \& \bibinfo{author}{To, A.~C.}
	\newblock \bibinfo{title}{Topology optimization based on deep representation
		learning ({{DRL}}) for compliance and stress-constrained design}.
	\newblock \emph{\bibinfo{journal}{Computational Mechanics}}
	\textbf{\bibinfo{volume}{66}}, \bibinfo{pages}{449--469}
	(\bibinfo{year}{2020}).
	
	\bibitem{chandrasekhar2021}
	\bibinfo{author}{Chandrasekhar, A.} \& \bibinfo{author}{Suresh, K.}
	\newblock \bibinfo{title}{{{TOuNN}}: {{Topology Optimization}} using {{Neural
				Networks}}}.
	\newblock \emph{\bibinfo{journal}{Structural and Multidisciplinary
			Optimization}} \textbf{\bibinfo{volume}{63}}, \bibinfo{pages}{1135--1149}
	(\bibinfo{year}{2021}).
	
	\bibitem{sanu2024}
	\bibinfo{author}{Sanu, S.~M.}, \bibinfo{author}{Aragon, A.~M.} \&
	\bibinfo{author}{Bessa, M.~A.}
	\newblock \bibinfo{title}{Neural topology optimization: The good, the bad, and
		the ugly} (\bibinfo{year}{2024}).
	\newblock \eprint{2407.13954}.
	
	\bibitem{herrmann2024}
	\bibinfo{author}{Herrmann, L.}, \bibinfo{author}{Sigmund, O.},
	\bibinfo{author}{Li, V.~M.}, \bibinfo{author}{Vogl, C.} \&
	\bibinfo{author}{Kollmannsberger, S.}
	\newblock \bibinfo{title}{Neural {{Networks}} for {{Generating Better Local
				Optima}} in {{Topology Optimization}}} (\bibinfo{year}{2024}).
	\newblock \eprint{2407.17957}.
	
	\bibitem{dupuis2021}
	\bibinfo{author}{Dupuis, B.} \& \bibinfo{author}{Jacot, A.}
	\newblock \bibinfo{title}{{{DNN-based Topology Optimisation}}: {{Spatial
				Invariance}} and {{Neural Tangent Kernel}}}.
	\newblock In \emph{\bibinfo{booktitle}{Advances in {{Neural Information
					Processing Systems}}}}, vol.~\bibinfo{volume}{34},
	\bibinfo{pages}{27659--27669} (\bibinfo{publisher}{Curran Associates, Inc.},
	\bibinfo{year}{2021}).
	
	\bibitem{chandrasekhar2021b}
	\bibinfo{author}{Chandrasekhar, A.} \& \bibinfo{author}{Suresh, K.}
	\newblock \bibinfo{title}{Length {{Scale Control}} in {{Topology Optimization}}
		using {{Fourier Enhanced Neural Networks}}} (\bibinfo{year}{2021}).
	\newblock \eprint{2109.01861}.
	
	\bibitem{doosti2021}
	\bibinfo{author}{Doosti, N.}, \bibinfo{author}{Panetta, J.} \&
	\bibinfo{author}{Babaei, V.}
	\newblock \bibinfo{title}{Topology {{Optimization}} via {{Frequency Tuning}} of
		{{Neural Design Representations}}}.
	\newblock In \emph{\bibinfo{booktitle}{Symposium on {{Computational
					Fabrication}}}}, \bibinfo{pages}{1--9} (\bibinfo{publisher}{ACM},
	\bibinfo{address}{Virtual Event USA}, \bibinfo{year}{2021}).
	
	\bibitem{nie2021}
	\bibinfo{author}{Nie, Z.}, \bibinfo{author}{Lin, T.}, \bibinfo{author}{Jiang,
		H.} \& \bibinfo{author}{Kara, L.~B.}
	\newblock \bibinfo{title}{{{TopologyGAN}}: {{Topology Optimization Using
				Generative Adversarial Networks Based}} on {{Physical Fields Over}} the
		{{Initial Domain}}}.
	\newblock \emph{\bibinfo{journal}{Journal of Mechanical Design}}
	\textbf{\bibinfo{volume}{143}}, \bibinfo{pages}{031715}
	(\bibinfo{year}{2021}).
	
	\bibitem{chen2023}
	\bibinfo{author}{Chen, H.}, \bibinfo{author}{Wu, R.},
	\bibinfo{author}{Grinspun, E.}, \bibinfo{author}{Zheng, C.} \&
	\bibinfo{author}{Chen, P.~Y.}
	\newblock \bibinfo{title}{Implicit {{Neural Spatial Representations}} for
		{{Time-dependent PDEs}}}.
	\newblock In \emph{\bibinfo{booktitle}{Proceedings of the 40th {{International
					Conference}} on {{Machine Learning}}}}, \bibinfo{pages}{5162--5177}
	(\bibinfo{publisher}{PMLR}, \bibinfo{year}{2023}).
	
	\bibitem{nobari2024}
	\bibinfo{author}{Nobari, A.~H.}, \bibinfo{author}{Giannone, G.},
	\bibinfo{author}{Regenwetter, L.} \& \bibinfo{author}{Ahmed, F.}
	\newblock \bibinfo{title}{{{NITO}}: {{Neural Implicit Fields}} for
		{{Resolution-free Topology Optimization}}} (\bibinfo{year}{2024}).
	\newblock \eprint{2402.05073}.
	
	\bibitem{zhang2021}
	\bibinfo{author}{Zhang, Z.} \emph{et~al.}
	\newblock \bibinfo{title}{{{TONR}}: {{An}} exploration for a novel way
		combining neural network with topology optimization}.
	\newblock \emph{\bibinfo{journal}{Computer Methods in Applied Mechanics and
			Engineering}} \textbf{\bibinfo{volume}{386}}, \bibinfo{pages}{114083}
	(\bibinfo{year}{2021}).
	
	\bibitem{hospedales2022}
	\bibinfo{author}{Hospedales, T.}, \bibinfo{author}{Antoniou, A.},
	\bibinfo{author}{Micaelli, P.} \& \bibinfo{author}{Storkey, A.}
	\newblock \bibinfo{title}{Meta-{{Learning}} in {{Neural Networks}}: {{A
				Survey}}}.
	\newblock \emph{\bibinfo{journal}{IEEE transactions on pattern analysis and
			machine intelligence}} \textbf{\bibinfo{volume}{44}},
	\bibinfo{pages}{5149--5169} (\bibinfo{year}{2022}).
	
	\bibitem{finn2017}
	\bibinfo{author}{Finn, C.}, \bibinfo{author}{Abbeel, P.} \&
	\bibinfo{author}{Levine, S.}
	\newblock \bibinfo{title}{Model-{{Agnostic Meta-Learning}} for {{Fast
				Adaptation}} of {{Deep Networks}}} (\bibinfo{year}{2017}).
	\newblock \eprint{1703.03400}.
	
	\bibitem{nichol2018}
	\bibinfo{author}{Nichol, A.}, \bibinfo{author}{Achiam, J.} \&
	\bibinfo{author}{Schulman, J.}
	\newblock \bibinfo{title}{On {{First-Order Meta-Learning Algorithms}}}
	(\bibinfo{year}{2018}).
	\newblock \eprint{1803.02999}.
	
	\bibitem{sitzmann2020}
	\bibinfo{author}{Sitzmann, V.}, \bibinfo{author}{Martel, J.},
	\bibinfo{author}{Bergman, A.}, \bibinfo{author}{Lindell, D.} \&
	\bibinfo{author}{Wetzstein, G.}
	\newblock \bibinfo{title}{Implicit {{Neural Representations}} with {{Periodic
				Activation Functions}}}.
	\newblock In \emph{\bibinfo{booktitle}{Advances in {{Neural Information
					Processing Systems}}}}, vol.~\bibinfo{volume}{33},
	\bibinfo{pages}{7462--7473} (\bibinfo{publisher}{Curran Associates, Inc.},
	\bibinfo{year}{2020}).
	
	\bibitem{tancik2021}
	\bibinfo{author}{Tancik, M.} \emph{et~al.}
	\newblock \bibinfo{title}{Learned {{Initializations}} for {{Optimizing
				Coordinate-Based Neural Representations}}}.
	\newblock In \emph{\bibinfo{booktitle}{Proceedings of the {{IEEE}}/{{CVF
					Conference}} on {{Computer Vision}} and {{Pattern Recognition}}}},
	\bibinfo{pages}{2846--2855} (\bibinfo{year}{2021}).
	
	\bibitem{woldseth2022}
	\bibinfo{author}{Woldseth, R.~V.}, \bibinfo{author}{Aage, N.},
	\bibinfo{author}{B{\ae}rentzen, J.~A.} \& \bibinfo{author}{Sigmund, O.}
	\newblock \bibinfo{title}{On the use of {{Artificial Neural Networks}} in
		{{Topology Optimisation}}}.
	\newblock \emph{\bibinfo{journal}{Structural and Multidisciplinary
			Optimization}} \textbf{\bibinfo{volume}{65}}, \bibinfo{pages}{294}
	(\bibinfo{year}{2022}).
	\newblock \eprint{2208.02563}.
	
	\bibitem{shin2023}
	\bibinfo{author}{Shin, S.}, \bibinfo{author}{Shin, D.} \&
	\bibinfo{author}{Kang, N.}
	\newblock \bibinfo{title}{Topology optimization via machine learning and deep
		learning: A review}.
	\newblock \emph{\bibinfo{journal}{Journal of Computational Design and
			Engineering}} \textbf{\bibinfo{volume}{10}}, \bibinfo{pages}{1736--1766}
	(\bibinfo{year}{2023}).
	
	\bibitem{lu2021}
	\bibinfo{author}{Lu, Y.}, \bibinfo{author}{Jiang, K.}, \bibinfo{author}{Levine,
		J.~A.} \& \bibinfo{author}{Berger, M.}
	\newblock \bibinfo{title}{Compressive {{Neural Representations}} of
		{{Volumetric Scalar Fields}}}.
	\newblock \emph{\bibinfo{journal}{Computer Graphics Forum}}
	\textbf{\bibinfo{volume}{40}}, \bibinfo{pages}{135--146}
	(\bibinfo{year}{2021}).
	
	\bibitem{zehnder2021}
	\bibinfo{author}{Zehnder, J.}, \bibinfo{author}{Li, Y.},
	\bibinfo{author}{Coros, S.} \& \bibinfo{author}{Thomaszewski, B.}
	\newblock \bibinfo{title}{{{NTopo}}: {{Mesh-free Topology Optimization}} using
		{{Implicit Neural Representations}}}.
	\newblock In \emph{\bibinfo{booktitle}{Advances in {{Neural Information
					Processing Systems}}}}, vol.~\bibinfo{volume}{34},
	\bibinfo{pages}{10368--10381} (\bibinfo{publisher}{Curran Associates, Inc.},
	\bibinfo{year}{2021}).
	
	\bibitem{antoniou2019}
	\bibinfo{author}{Antoniou, A.}, \bibinfo{author}{Edwards, H.} \&
	\bibinfo{author}{Storkey, A.}
	\newblock \bibinfo{title}{How to train your {{MAML}}} (\bibinfo{year}{2019}).
	\newblock \eprint{1810.09502}.
	
	\bibitem{huisman2021}
	\bibinfo{author}{Huisman, M.}, \bibinfo{author}{{van Rijn}, J.~N.} \&
	\bibinfo{author}{Plaat, A.}
	\newblock \bibinfo{title}{A {{Survey}} of {{Deep Meta-Learning}}}.
	\newblock \emph{\bibinfo{journal}{Artificial Intelligence Review}}
	\textbf{\bibinfo{volume}{54}}, \bibinfo{pages}{4483--4541}
	(\bibinfo{year}{2021}).
	\newblock \eprint{2010.03522}.
	
	\bibitem{kingma2017}
	\bibinfo{author}{Kingma, D.~P.} \& \bibinfo{author}{Ba, J.}
	\newblock \bibinfo{title}{Adam: {{A Method}} for {{Stochastic Optimization}}}
	(\bibinfo{year}{2017}).
	\newblock \eprint{1412.6980}.
	
	\bibitem{sosnovik2017}
	\bibinfo{author}{Sosnovik, I.} \& \bibinfo{author}{Oseledets, I.}
	\newblock \bibinfo{title}{Neural networks for topology optimization}
	(\bibinfo{year}{2017}).
	\newblock \eprint{1709.09578}.
	
	\bibitem{maze2022}
	\bibinfo{author}{Maz{\'e}, F.} \& \bibinfo{author}{Ahmed, F.}
	\newblock \bibinfo{title}{Diffusion {{Models Beat GANs}} on {{Topology
				Optimization}}} (\bibinfo{year}{2022}).
	\newblock \eprint{2208.09591}.
	
	\bibitem{johnson2007}
	\bibinfo{author}{Johnson, S.~G.}
	\newblock \bibinfo{title}{The {{NLopt}} nonlinear-optimization package}
	(\bibinfo{year}{2007}).
	
	\bibitem{sigmund2022a}
	\bibinfo{author}{Sigmund, O.}
	\newblock \bibinfo{title}{On benchmarking and good scientific practise in
		topology optimization}.
	\newblock \emph{\bibinfo{journal}{Structural and Multidisciplinary
			Optimization}} \textbf{\bibinfo{volume}{65}}, \bibinfo{pages}{315}
	(\bibinfo{year}{2022}).
	
	\bibitem{dolan2004}
	\bibinfo{author}{Dolan, E.~D.} \& \bibinfo{author}{Mor{\'e}, J.~J.}
	\newblock \bibinfo{title}{Benchmarking {{Optimization Software}} with
		{{Performance Profiles}}} (\bibinfo{year}{2004}).
	\newblock \eprint{cs/0102001}.
	
	\bibitem{rojas-labanda2015}
	\bibinfo{author}{{Rojas-Labanda}, S.} \& \bibinfo{author}{Stolpe, M.}
	\newblock \bibinfo{title}{Benchmarking optimization solvers for structural
		topology optimization}.
	\newblock \emph{\bibinfo{journal}{Structural and Multidisciplinary
			Optimization}} \textbf{\bibinfo{volume}{52}}, \bibinfo{pages}{527--547}
	(\bibinfo{year}{2015}).
	
	\bibitem{li2017}
	\bibinfo{author}{Li, Z.}, \bibinfo{author}{Zhou, F.}, \bibinfo{author}{Chen,
		F.} \& \bibinfo{author}{Li, H.}
	\newblock \bibinfo{title}{Meta-{{SGD}}: {{Learning}} to {{Learn Quickly}} for
		{{Few-Shot Learning}}} (\bibinfo{year}{2017}).
	\newblock \eprint{1707.09835}.
	
	\bibitem{lynch2019a}
	\bibinfo{author}{Lynch, M.~E.}, \bibinfo{author}{Sarkar, S.} \&
	\bibinfo{author}{Maute, K.}
	\newblock \bibinfo{title}{Machine {{Learning}} to {{Aid Tuning}} of {{Numerical
				Parameters}} in {{Topology Optimization}}}.
	\newblock \emph{\bibinfo{journal}{Journal of Mechanical Design}}
	\textbf{\bibinfo{volume}{141}} (\bibinfo{year}{2019}).
	
	\bibitem{chen2023a}
	\bibinfo{author}{Chen, H.}, \bibinfo{author}{Joglekar, A.} \&
	\bibinfo{author}{Burak~Kara, L.}
	\newblock \bibinfo{title}{Topology {{Optimization Using Neural Networks With
				Conditioning Field Initialization}} for {{Improved Efficiency}}}.
	\newblock In \emph{\bibinfo{booktitle}{{{ASME}} 2023 {{International Design
					Engineering Technical Conferences}} and {{Computers}} and {{Information}} in
			{{Engineering Conference}}}} (\bibinfo{publisher}{American Society of
		Mechanical Engineers Digital Collection}, \bibinfo{year}{2023}).
	
	\bibitem{bourdin2001}
	\bibinfo{author}{Bourdin, B.}
	\newblock \bibinfo{title}{Filters in topology optimization}.
	\newblock \emph{\bibinfo{journal}{International Journal for Numerical Methods
			in Engineering}} \textbf{\bibinfo{volume}{50}}, \bibinfo{pages}{2143--2158}
	(\bibinfo{year}{2001}).
	
	\bibitem{sitzmann2020a}
	\bibinfo{author}{Sitzmann, V.}, \bibinfo{author}{Chan, E.~R.},
	\bibinfo{author}{Tucker, R.}, \bibinfo{author}{Snavely, N.} \&
	\bibinfo{author}{Wetzstein, G.}
	\newblock \bibinfo{title}{{{MetaSDF}}: {{Meta-learning Signed Distance
				Functions}}} (\bibinfo{year}{2020}).
	\newblock \eprint{2006.09662}.
	
	\bibitem{tancik2020}
	\bibinfo{author}{Tancik, M.} \emph{et~al.}
	\newblock \bibinfo{title}{Fourier {{Features Let Networks Learn High Frequency
				Functions}} in {{Low Dimensional Domains}}} (\bibinfo{year}{2020}).
	\newblock \eprint{2006.10739}.
	
	\bibitem{ning2021}
	\bibinfo{author}{Martins, J. R. R.~A.} \& \bibinfo{author}{Ning, A.}
	\newblock \emph{\bibinfo{title}{Engineering Design Optimization}}
	(\bibinfo{publisher}{Cambridge University Press},
	\bibinfo{address}{Cambridge}, \bibinfo{year}{2021}).
	
\end{thebibliography}

\clearpage
\appendix

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Problem formulation} \label{app:formulation}
\textbf{Compliance minimization.} We use the following standard TO formulation to solve compliance minimization problems~\cite{sigmund2013}:

\begin{align}
&\text{minimize } &&\mathcal{F}(\mathbf{u(\boldsymbol{\rho}), \boldsymbol{\rho}}) = \mathbf{f}^{\intercal}\mathbf{u} \equiv c \label{eq:objective}\\ \
&\text{subject to }  &&\mathbf{K}\mathbf{u} = \mathbf{f} \label{eq:equilibrium} \\
                     & &&g_{0}(\boldsymbol{\rho}) = \sum_{i=1}^{N} \rho_i v_i - V^{*}\leq 0 \label{eq:volume} \\
                     &  &&0 \leq \rho_i \leq 1, \quad \forall i \in \{1,\ldots,N\} \label{eq:box}
\end{align}

Here, $\boldsymbol{\rho} \in [0,1]^N$ represents the vector of $N$ element densities. The global displacement vector $\mathbf{u}$ and force vector $\mathbf{f}$ are obtained by solving the equilibrium equation \cref{eq:equilibrium}, where $\mathbf{K}$ denotes the global stiffness matrix. The prescribed volume fraction is denoted by $V^*$  volumes. We use the modified SIMP material interpolation~\cite{sigmund2013}:

\begin{equation}
E_{e}(\rho_{i}) = E_{\text{min}} + \rho_{i}^{p}(E_{0}-E_{\text{min}})
\end{equation}

In our experiments, we used a penalization factor $p=3$, Young's modulus of $E_{0} = 1.0$, Poisson's ratio of $\nu = 0.3$, and an artificial stiffness value of $E_{\text{min}}=10^{-9}$ assigned to void regions. 

\textbf{Neural reparameterization.} In neural topology optimization, element densities $\boldsymbol{\rho} \in \mathbb{R}^{N}$ are output by a neural network $f_{\boldsymbol{\theta}}$ with learnable parameters $\boldsymbol{\theta} \in \mathbb{R}^{M}$. The total number of learnable parameters in our model $M=\num{264449}$ substantially exceeds the number of elements in the FE discretization $N=4096$. The high parameter count ensures the expressivity needed to represent fine structural features in optimized designs. It also reduces the chance of getting stuck in poor local minima, potentially leading to better-quality solutions~\cite{herrmann2024}. As presented in the main text, the proposed model can be applied to finer FE discretizations without modifications. While our model may seem large by traditional TO standards, it is relatively modest in the context of machine learning approaches in the field. For instance, a recent direct-design approach by Nobari et al.~\cite{nobari2024} employs a model with 22 million parameters.

\section{Implementation details} \label{app:details}
\subsection{Model architecture}
\textbf{Strain energy conditioning.} Following previous works in direct-design TO~\cite{nie2021, chen2023a}, we condition the reparameterization model on the strain energy density field of a uniform density design. The strain energy fields $\mathbf{E}_{\text{raw}}$ are calculated for each task, smoothed with a logarithmic transform, and normalized between -1 and 1 to match SIREN's input range~\cite{chen2023a}. The processed field is denoted with $\mathbf{E}$. The full post-processing operation is expressed as:

\begin{equation}
\mathbf{E} = 2\,\frac{\log(\mathbf{E}_{\text{raw}}) - \min(\log(\mathbf{E}_{\text{raw}}))}{\max(\log(\mathbf{E}_{\text{raw}})) - \min(\log(\mathbf{E}_{\text{raw}}))} - 1.
\end{equation}

Conditioning enables a single set of learned parameters to represent various initial designs based on different boundary conditions.

\textbf{Filtering.} Density fields generated by the neural network $\boldsymbol{\rho}$ undergo two filtering operations. First, we apply a standard density filter~\cite{bourdin2001} with a radius of $\sfrac{1}{32}$ the domain size to ensure mesh independence on the output designs. Next, we apply a shifted sigmoid filter adapted from~\cite{hoyer2019}:
\begin{equation}
\tilde{\rho}_{i} = \frac{1}{1 + \exp{\left\{\rho_{i} - b(\boldsymbol{\rho}, V^{*})\right\}}}
\end{equation}
The transformation enforces the volume and density box constraints in \cref{eq:volume} and \cref{eq:box}, enabling the use of standard unconstrained optimizers. The shift factor $b$, which satisfies the volume constraint, is determined using bisection. We found that amplifying the signal by a factor of 10 before applying the sigmoid-based filter resulted in designs with improved contrast, similar to the approach used in NTopo~\cite{zehnder2021}.

\textbf{Hyperparameter selection.} Following previous studies on meta-learning for coordinate-based neural networks~\cite{tancik2021, sitzmann2020a}, we performed a focused grid search over the key model parameters -- inner and outer learning rates, and SIREN's frequency parameter $\omega_{0}$. For each hyperparameter combination, we performed full meta-training and evaluated the average loss on a held-out validation set. We found the optimal performance to occur for the inner learning rate of $10^{-4}$ and outer learning rate of $10^{-6}$. Notably, meta-neural TO achieved optimal performance with $\omega_0=60.0$, while neural TO performed best with $\omega_0=30.0$. This finding aligns with previous meta-learning literature~\cite{tancik2020}, where higher $\omega_0$ values were also found to be beneficial for meta-learned models. The preference for higher $\omega_{0}$ suggests that meta-learning enables more effective utilization of higher-frequency components in neural representations.

\subsection{Optimization process}
\textbf{Stopping criteria.} We use a stopping criterion based on changes in the loss function between consecutive iterations:
\begin{equation*}
|\mathcal{L}(\boldsymbol{\theta}_{t}) - \mathcal{L}(\boldsymbol{\theta}_{t-1})| < \epsilon(1 + |\mathcal{L}(\boldsymbol{\theta}_{t-1})|)
\end{equation*}
where $\mathcal{L}(\boldsymbol{\theta}_{t})$ represents the loss at iteration $t$ and $\epsilon=10^{-5}$ throughout our experiments. This criterion accounts for the scale of the objective function and is well-suited for stochastic optimizers like Adam that exhibit small oscillations near optima~\cite{ning2021}.

\textbf{Post-processing.} We applied volume-preserving thresholding to final designs to ensure fair comparisons~\cite{sigmund2022a, woldseth2022}. As explained in the following section, thresholding had a significant effect on the final compliance, especially for MMA designs.

\section{Impact of thresholding on design performance} \label{app:thresholding}
The performance profiles in \cref{fig:continuous_perfprof} reveal an interesting pattern where meta-neural TO finds superior continuous designs for the majority of tasks across all experiments. This effect is most pronounced in the cross-resolution experiments where meta-neural TO "wins" on 81.0\% of tasks. However, this advantage is reduced when we threshold the designs. We report the average percent improvement in compliance induced by thresholding for meta-neural TO and standard TO designs. We excluded tasks where thresholding induced a change with an absolute value above 50\%. This filtering step removed cases where thresholding resulted in drastic degradation to the optimized structure. Thresholding improved compliance for both methods, with MMA designs showing greater gains than meta-neural TO. Standard TO designs experienced an average compliance improvement of 6.63\% in distribution and 5.68\% out of distribution, whereas the meta-neural TO designs were improved by 1.97\% and 1.74\%, respectively. These results suggest that meta-neural TO generally produces designs with fewer intermediate-density elements, leading to smaller gains from thresholding.

\begin{figure*}[!thb]
\begin{minipage}{\linewidth}
     \centering
     % ID LOSS
     \begin{subfigure}[t]{0.3\linewidth}
         \subcaption{}
         \centering
         \includegraphics[width=50mm]{figures/in-distribution/loss_perfprof_200_test_steps.pdf}
     \end{subfigure}
     \hfill
     % OOD LOSS
     \begin{subfigure}[t]{0.3\linewidth}
         \subcaption{}
         \centering
         \includegraphics[width=50mm]{figures/out-of-distribution/loss_perfprof_200_test_steps.pdf}
     \end{subfigure}
     \hfill
     % UPSCALED LOSS
     \begin{subfigure}[t]{0.3\linewidth}
         \subcaption{}
         \centering
         \includegraphics[width=50mm]{figures/upscaled/loss_perfprof_200_test_steps.pdf}
     \end{subfigure}
     \hfill
     
     \end{minipage}
     \hfill
     \begin{minipage}{\linewidth}
     \centering
        \includegraphics[width=100mm]{figures/legends/perfprof_legend_rotated.pdf}
     \end{minipage}
     \caption{\textbf{Performance profiles comparing compliance of continuous (not thresholded) designs obtained with meta-neural TO against the baseline methods.} The plots (\textbf{a-c}) compare the continuous design compliances for the corresponding in-distribution (\textbf{a}), out-of-distribution (\textbf{b}), and cross-resolution (\textbf{c}) experiments.}
     \label{fig:continuous_perfprof}
\end{figure*}

\section{Strain energy pretraining} \label{app:pretraining}
To better understand the relationship between strain energy patterns and effective initializations, we compared our meta-learning approach against a simple pretraining strategy. We trained the reparameterization network to directly output strain energy density fields (one of its inputs) using mean squared error (MSE) loss over 100 epochs (understood as sweeps through the training dataset). Performance profiles in \cref{fig:pretrain_perfprof} show that this approach offers moderate efficient improvements over the meta-learned initialization, requiring the fewest iterations in 40.7\% of tasks compared to meta-neural TO's 37.4\%. Further, the approach improves continuous and thresholded compliance across tasks, narrowing the quality gap between neural and standard TO.

This approach's strong performance validates a key insight of our meta-learning framework: the critical importance of strain energy patterns in generating effective initial designs. Meta-learning naturally discovered this relationship without explicit supervision, demonstrating its ability to uncover meaningful physical principles from the optimization process alone. This discovery mechanism makes our meta-learning approach particularly promising for applications where the relevant physical patterns are less well understood or where multiple fields need to be considered simultaneously.

\begin{figure*}[!ht]
\begin{minipage}{\linewidth}
     \centering
     % ID LOSS
     \begin{subfigure}[t]{0.3\linewidth}
         \subcaption{}
         \centering
         \includegraphics[width=50mm]{figures/pretrained/steps_perfprof_200_test_steps.pdf}
     \end{subfigure}
     \hfill
     % OOD LOSS
     \begin{subfigure}[t]{0.3\linewidth}
         \subcaption{}
         \centering
         \includegraphics[width=50mm]{figures/pretrained/loss_perfprof_200_test_steps.pdf}
     \end{subfigure}
     \hfill
     % UPSCALED LOSS
     \begin{subfigure}[t]{0.3\linewidth}
         \subcaption{}
         \centering
         \includegraphics[width=50mm]{figures/pretrained/thresholded_loss_perfprof_200_test_steps.pdf}
     \end{subfigure}
     \hfill
     
     \end{minipage}
     \hfill
     \begin{minipage}{\linewidth}
     \centering
        \includegraphics[width=100mm]{figures/pretrained/perfprof_legend_rotated.pdf}
     \end{minipage}
     \caption{\textbf{Performance profiles comparing meta-neural TO and neural TO with a pretraining strategy.} The plots (\textbf{a-c}) show the relative performance in terms of the number of iterations required (\textbf{a}), continuous compliance (\textbf{b}), and thresholded compliance (\textbf{c}).}
     \label{fig:pretrain_perfprof}
\end{figure*}

\section{Meta-neural TO without strain-energy conditioning} \label{app:conditioning}
The proposed meta-learning method relies heavily on strain energy density fields to generate task-specific initial designs. Here, we examine how removing this conditioning mechanism affects the meta-learning process and resulting performance. Without access to boundary condition information through strain energy fields, the meta-learning objective fundamentally shifts. Instead of learning to map a physical field into promising initial designs, the model must discover a universal initial design that can rapidly adapt to diverse topologies while learning about problem physics solely through the loss function.

Performance profiles in \cref{fig:nocond_perfprof} reveal that removing strain energy conditioning impacts both the convergence speed and final design quality. While meta-learned initialization still outperforms neural TO with random initialization (both without conditioning), the improvements are substantially smaller than in the conditioned case. Notably, the non-conditioned meta-learned strategy is less efficient than standard TO in in-distribution tasks.

Analysis of initial designs in \cref{fig:init_designs_noncond} shows that the unconditioned model learns to concentrate material along domain boundaries, mirroring the probability distribution used for sampling boundary conditions in our training set. This suggests that even without explicit physical information, meta-learning can discover useful structural priors from the statistical patterns in the training data.

\begin{figure*}[!tbp]
\begin{minipage}{\linewidth}
     \centering
     % ID LOSS
     \begin{subfigure}[t]{0.3\linewidth}
         \subcaption{}
         \centering
         \includegraphics[width=50mm]{figures/no-conditioning/steps_perfprof_200_test_steps.pdf}
     \end{subfigure}
     \hfill
     % OOD LOSS
     \begin{subfigure}[t]{0.3\linewidth}
         \subcaption{}
         \centering
         \includegraphics[width=50mm]{figures/no-conditioning/loss_perfprof_200_test_steps.pdf}
     \end{subfigure}
     \hfill
     % UPSCALED LOSS
     \begin{subfigure}[t]{0.3\linewidth}
         \subcaption{}
         \centering
         \includegraphics[width=50mm]{figures/no-conditioning/thresholded_loss_perfprof_200_test_steps.pdf}
     \end{subfigure}
     \hfill
     
     \end{minipage}
     \hfill
     \begin{minipage}{\linewidth}
     \centering
        \includegraphics[width=100mm]{figures/legends/perfprof_legend_rotated.pdf}
     \end{minipage}
     \caption{\textbf{Performance profiles for meta-learning without conditioning.}  The plots (\textbf{a-c}) show the relative performance in terms of the number of iterations required (\textbf{a}), continuous compliance (\textbf{b}), and thresholded compliance (\textbf{c}).(right) designs.} 
     \label{fig:nocond_perfprof}
\end{figure*}
\FloatBarrier

\begin{figure}[!tbp]
    \centering
    \includegraphics[width=120mm]{figures/initial_designs/initial_design_comparison_noncond.pdf}
    \caption{\textbf{Initial density distributions generated by unconditioned meta-neural TO.} Without strain energy conditioning, the model learns to concentrate material along domain boundaries, mirroring the probability distribution of boundary conditions in the training set.}
    \label{fig:init_designs_noncond}
\end{figure}

\section{Cross-optimizer generalization} \label{app:cross-optimizer}
We further investigated the generalization capabilities of meta-learned initializations by applying them to conventional MMA-based topology optimization. Our experiments with both conditioned and unconditioned meta-learned initializations reveal an interesting dichotomy in cross-optimizer transfer behavior. Performance profiles in \cref{fig:mma_perfprof} demonstrate that initializing standard TO with designs from the strain-energy-conditioned model improves optimization efficiency. In contrast, initializations from the unconditioned model fail to accelerate standard TO convergence.

This distinction highlights two key aspects of meta-learning in topology optimization. First, the meta-learning process inherently considers optimizer dynamics when searching for improved initializations, making direct transfer between different optimizers challenging. This explains why unconditioned initializations, while effective for neural TO, do not benefit standard TO. Second, when conditioned on strain energy density, the meta-learned model discovers physically meaningful priors for material placement that prove beneficial regardless of the optimization method. These findings underscore both the optimizer-specific nature of meta-learned initialization strategies and the potential for physics-informed conditioning to enable cross-optimizer knowledge transfer.

\begin{figure*}[ht!]
\begin{minipage}{\linewidth}
     \centering
     % ID LOSS
     \begin{subfigure}[t]{0.3\linewidth}
         \subcaption{}
         \centering
         \includegraphics[width=50mm]{figures/cross-optimizer/performance_profile_steps.pdf}
     \end{subfigure}
     \hfill
     % OOD LOSS
     \begin{subfigure}[t]{0.3\linewidth}
         \subcaption{}
         \centering
         \includegraphics[width=50mm]{figures/cross-optimizer/performance_profile_loss.pdf}
     \end{subfigure}
     \hfill
     % UPSCALED LOSS
     \begin{subfigure}[t]{0.3\linewidth}
         \subcaption{}
         \centering
         \includegraphics[width=50mm]{figures/cross-optimizer/performance_profile_bw_loss.pdf}
     \end{subfigure}
     \hfill
     
     \end{minipage}
     \hfill
     \begin{minipage}{\linewidth}
     \centering
        \includegraphics[width=100mm]{figures/cross-optimizer/perfprof_legend_rotated.pdf}
     \end{minipage}
     \caption{\textbf{Performance profiles comparing standard TO with different initialization strategies.} The plots (\textbf{a-c}) show the relative performance in terms of the number of iterations required (\textbf{a}), continuous compliance (\textbf{b}), and thresholded compliance (\textbf{c}).}
     \label{fig:mma_perfprof}
\end{figure*}


\end{document}
