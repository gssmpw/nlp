\section{Related Works}
\label{sec:related_works}
The direct approach to KV caching stores all key and value representations for every input token, leading to significant memory growth in long sequences, but several algorithmic ideas for improvement have been developed in the literature.

\paragraph{Embedding quantization.} A simple yet effective approach to reducing KV cache size is quantizing the floating-point numbers (FPN) in the KV cache to fixed-point numbers with fewer bits. 
Several quantization methods have been proposed specifically for the KV cache~\cite{yue2024wkvquant, yang2024no, dong2024qaq, kang2024gear, liu2024kivi, hooper2024kvquant, zhang2024kv, zandieh2024qjl}.
However, such approaches still require memory space that linearly increases in sequence length. 

\paragraph{Token level pruning.} A prominent line of work which is related to our paper focuses on token-level KV cache  pruning where redundant or less important tokens get evicted from the cache~\cite{beltagy2020longformer, zhang2024h2o, liu2024scissorhands, xiao2023efficient, zandieh2024subgen, li2024snapkv}.
Many of the works in this line have used accumulated attention scores to select important tokens in the cache~\cite{zhang2024h2o, li2024snapkv, xiao2023efficient}. 
Recent works extend those methods to an adaptive way of budget allocation across layer~\cite{cai2024pyramidkv} and head~\cite{fu2024not}.


A very recent work, Lexico~\cite{kim2024lexico}, applies techniques from sparse representation learning to compress the KV cache by learning a universal dictionary such that all key and value embeddings are represented as extremely sparse vectors within the learned dictionary. 
Unfortunately, this approach requires solving a computationally expensive matching pursuit algorithm for each key and value embedding, making Lexico relatively slow.

\paragraph{Algorithmic discrepancy theory.}
Banaszczyk's seminal works~\cite{B98, B12} establishing theoretical guarantees for vector set discrepancy have sparked research in the vector balancing problem~\cite{DNTT18}. This led to algorithmic developments in both offline \cite{B10} and online \cite{BJSS19, ALS21, KRR23} settings. The vector balancing problem has particular relevance to streaming and sublinear algorithms, as minimizing a dataset's discrepancy yields small subsets that effectively preserve the original dataset's properties. Recent works~\cite{PT20, CKW24} extend these discrepancy theory ideas to develop sublinear memory algorithms for \emph{kernel density estimation}.