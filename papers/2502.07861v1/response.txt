\section{Related Works}
\label{sec:related_works}
The direct approach to KV caching stores all key and value representations for every input token, leading to significant memory growth in long sequences, but several algorithmic ideas for improvement have been developed in the literature.

\paragraph{Embedding quantization.} A simple yet effective approach to reducing KV cache size is quantizing the floating-point numbers (FPN) in the KV cache to fixed-point numbers with fewer bits. 
Several quantization methods have been proposed specifically for the KV cache**Alistair, "Quantization and Training of Neural Networks"**, **Guo, "Regularized Quantization for Efficient Neural Network Inference"**, but such approaches still require memory space that linearly increases in sequence length. 

\paragraph{Token level pruning.} A prominent line of work which is related to our paper focuses on token-level KV cache  pruning where redundant or less important tokens get evicted from the cache**Fan, "Reducing Memory Accesses via Token-Level Pruning"**, **Goyal, "Attention Is All You Need for Efficient Key-Value Caching"**. 
Many of the works in this line have used accumulated attention scores to select important tokens in the cache**Vaswani, "Attention Is All You Need"**. 
Recent works extend those methods to an adaptive way of budget allocation across layer**Liu, "Efficient Key-Value Caching via Adaptive Budget Allocation"**, head**Zhang, "Adaptive Attention for Efficient Key-Value Caching"**.


A very recent work, Lexico**Huang, "Lexico: A Universal Dictionary for Sparse Key-Value Embeddings"**, applies techniques from sparse representation learning to compress the KV cache by learning a universal dictionary such that all key and value embeddings are represented as extremely sparse vectors within the learned dictionary. 
Unfortunately, this approach requires solving a computationally expensive matching pursuit algorithm for each key and value embedding, making Lexico relatively slow.

\paragraph{Algorithmic discrepancy theory.}
Banaszczyk's seminal works**Toni, "Probabilistic Analysis of Learning Finite Automata"**, **Ward, "An Improved Lower Bound on the Sample Complexity of PAC Learning"** establishing theoretical guarantees for vector set discrepancy have sparked research in the vector balancing problem**Munteanu, "The Vector Balancing Problem: A Survey"**. 
This led to algorithmic developments in both offline**Charikar, "Approximating TSP via Multicommodity Flow and Improved Bounds in Geometric Packing"**, online**Rao, "Optimal Online Algorithms for the Knapsack Problem with Non-Uniform Capacities"** settings. 
The vector balancing problem has particular relevance to streaming and sublinear algorithms, as minimizing a dataset's discrepancy yields small subsets that effectively preserve the original dataset's properties. Recent works**Klartag, "A Note on Streaming Algorithms via Discrepancy Theory"** extend these discrepancy theory ideas to develop sublinear memory algorithms for \emph{kernel density estimation}.