@Article{ALS21,
  title= "Discrepancy minimization via a self-balancing walk",
  author= "Ryan Alweiss and Yang P. Liu and Mehtaab Sawhney",
  journal="Proceedings of the 53rd ACM Symposium on the Theory of Computing (STOC ’2021)",
  year="2021"
}



@inproceedings{Bessa2023WeightedMinhash,
author = {Bessa, Aline and Daliri, Majid and Freire, Juliana and Musco, Cameron and Musco, Christopher and Santos, A\'{e}cio and Zhang, Haoxiang},
title = {Weighted Minwise Hashing Beats Linear Sketching for Inner Product Estimation},
year = {2023},
isbn = {9798400701276},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3584372.3588679},
doi = {10.1145/3584372.3588679},
abstract = {We present a new approach for independently computing compact sketches that can be used to approximate the inner product between pairs of high-dimensional vectors. Based on the Weighted MinHash algorithm, our approach admits strong accuracy guarantees that improve on the guarantees of popular linear sketching approaches for inner product estimation, such as CountSketch and Johnson-Lindenstrauss projection. Specifically, while our method exactly matches linear sketching for dense vectors, it yields significantly lower error for sparse vectors with limited overlap between non-zero entries. Such vectors arise in many applications involving sparse data, as well as in increasingly popular dataset search applications, where inner products are used to estimate data covariance, conditional means, and other quantities involving columns in unjoined tables. We complement our theoretical results by showing that our approach empirically outperforms existing linear sketches and unweighted hashing-based sketches for sparse vectors.},
booktitle = {Proceedings of the 42nd ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of Database Systems},
pages = {169–181},
numpages = {13},
keywords = {join-size estimation, vector sketching, inner product estimation},
location = {<conf-loc>, <city>Seattle</city>, <state>WA</state>, <country>USA</country>, </conf-loc>},
series = {PODS '23}
}

@article{dalirisampling:2024,
      title={Sampling Methods for Inner Product Sketching}, 
      author={Majid Daliri and Juliana Freire and Christopher Musco and Aécio Santos and Haoxiang Zhang},
      year={2024},
      journal = {Proc. VLDB Endow.}
}

@incollection{boucheron2003concentration,
  title={Concentration inequalities},
  author={Boucheron, St{\'e}phane and Lugosi, G{\'a}bor and Bousquet, Olivier},
  booktitle={Summer school on machine learning},
  pages={208--240},
  year={2003},
  publisher={Springer}
}

@article{liu2024kivi,
  title={KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache},
  author={Liu, Zirui and Yuan, Jiayi and Jin, Hongye and Zhong, Shaochen and Xu, Zhaozhuo and Braverman, Vladimir and Chen, Beidi and Hu, Xia},
  journal={arXiv preprint arXiv:2402.02750},
  year={2024}
}

@article{hooper2024kvquant,
  title={KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization},
  author={Hooper, Coleman and Kim, Sehoon and Mohammadzadeh, Hiva and Mahoney, Michael W and Shao, Yakun Sophia and Keutzer, Kurt and Gholami, Amir},
  journal={arXiv preprint arXiv:2401.18079},
  year={2024}
}


@article{shazeer2019fast,
  title={Fast transformer decoding: One write-head is all you need},
  author={Shazeer, Noam},
  journal={arXiv preprint arXiv:1911.02150},
  year={2019}
}


@inproceedings{ainslie2023gqa,
  title={GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints},
  author={Ainslie, Joshua and Lee-Thorp, James and de Jong, Michiel and Zemlyanskiy, Yury and Lebron, Federico and Sanghai, Sumit},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  pages={4895--4901},
  year={2023}
}


@article{zhang2024h2o,
  title={H2o: Heavy-hitter oracle for efficient generative inference of large language models},
  author={Zhang, Zhenyu and Sheng, Ying and Zhou, Tianyi and Chen, Tianlong and Zheng, Lianmin and Cai, Ruisi and Song, Zhao and Tian, Yuandong and R{\'e}, Christopher and Barrett, Clark and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}


@article{liu2024scissorhands,
  title={Scissorhands: Exploiting the persistence of importance hypothesis for llm kv cache compression at test time},
  author={Liu, Zichang and Desai, Aditya and Liao, Fangshuo and Wang, Weitao and Xie, Victor and Xu, Zhaozhuo and Kyrillidis, Anastasios and Shrivastava, Anshumali},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}


@article{xiao2023efficient,
  title={Efficient streaming language models with attention sinks},
  author={Xiao, Guangxuan and Tian, Yuandong and Chen, Beidi and Han, Song and Lewis, Mike},
  journal={arXiv preprint arXiv:2309.17453},
  year={2023}
}


@article{zandieh2024subgen,
  title={SubGen: Token Generation in Sublinear Time and Memory},
  author={Zandieh, Amir and Han, Insu and Mirrokni, Vahab and Karbasi, Amin},
  journal={arXiv preprint arXiv:2402.06082},
  year={2024}
}


@inproceedings{kwon2023efficient,
  title={Efficient memory management for large language model serving with pagedattention},
  author={Kwon, Woosuk and Li, Zhuohan and Zhuang, Siyuan and Sheng, Ying and Zheng, Lianmin and Yu, Cody Hao and Gonzalez, Joseph and Zhang, Hao and Stoica, Ion},
  booktitle={Proceedings of the 29th Symposium on Operating Systems Principles},
  pages={611--626},
  year={2023}
}


@inproceedings{sheng2023flexgen,
  title={Flexgen: High-throughput generative inference of large language models with a single gpu},
  author={Sheng, Ying and Zheng, Lianmin and Yuan, Binhang and Li, Zhuohan and Ryabinin, Max and Chen, Beidi and Liang, Percy and R{\'e}, Christopher and Stoica, Ion and Zhang, Ce},
  booktitle={International Conference on Machine Learning},
  pages={31094--31116},
  year={2023},
  organization={PMLR}
}


@article{dettmers2024qlora,
  title={Qlora: Efficient finetuning of quantized llms},
  author={Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}


@article{frantar2022gptq,
  title={Gptq: Accurate post-training quantization for generative pre-trained transformers},
  author={Frantar, Elias and Ashkboos, Saleh and Hoefler, Torsten and Alistarh, Dan},
  journal={arXiv preprint arXiv:2210.17323},
  year={2022}
}

@article{dettmers2023spqr,
  title={Spqr: A sparse-quantized representation for near-lossless llm weight compression},
  author={Dettmers, Tim and Svirschevski, Ruslan and Egiazarian, Vage and Kuznedelev, Denis and Frantar, Elias and Ashkboos, Saleh and Borzunov, Alexander and Hoefler, Torsten and Alistarh, Dan},
  journal={arXiv preprint arXiv:2306.03078},
  year={2023}
}




@article{yue2024wkvquant,
  title={Wkvquant: Quantizing weight and key/value cache for large language models gains more},
  author={Yue, Yuxuan and Yuan, Zhihang and Duanmu, Haojie and Zhou, Sifan and Wu, Jianlong and Nie, Liqiang},
  journal={arXiv preprint arXiv:2402.12065},
  year={2024}
}


@article{yang2024no,
  title={No Token Left Behind: Reliable KV Cache Compression via Importance-Aware Mixed Precision Quantization},
  author={Yang, June Yong and Kim, Byeongwook and Bae, Jeongin and Kwon, Beomseok and Park, Gunho and Yang, Eunho and Kwon, Se Jung and Lee, Dongsoo},
  journal={arXiv preprint arXiv:2402.18096},
  year={2024}
}


@article{dong2024qaq,
  title={QAQ: Quality Adaptive Quantization for LLM KV Cache},
  author={Dong, Shichen and Cheng, Wen and Qin, Jiayu and Wang, Wei},
  journal={arXiv preprint arXiv:2403.04643},
  year={2024}
}


@article{kang2024gear,
  title={Gear: An efficient kv cache compression recipefor near-lossless generative inference of llm},
  author={Kang, Hao and Zhang, Qingru and Kundu, Souvik and Jeong, Geonhwa and Liu, Zaoxing and Krishna, Tushar and Zhao, Tuo},
  journal={arXiv preprint arXiv:2403.05527},
  year={2024}
}


@article{zhang2024kv,
  title={KV Cache is 1 Bit Per Channel: Efficient Large Language Model Inference with Coupled Quantization},
  author={Zhang, Tianyi and Yi, Jonah and Xu, Zhaozhuo and Shrivastava, Anshumali},
  journal={arXiv preprint arXiv:2405.03917},
  year={2024}
}

@article{bai2023longbench,
  title={LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding},
  author={Bai, Yushi and Lv, Xin and Zhang, Jiajie and Lyu, Hongchang and Tang, Jiankai and Huang, Zhidian and Du, Zhengxiao and Liu, Xiao and Zeng, Aohan and Hou, Lei and Dong, Yuxiao and Tang, Jie and Li, Juanzi},
  journal={arXiv preprint arXiv:2308.14508},
  year={2023}
}


@article{pope2023efficiently,
  title={Efficiently scaling transformer inference},
  author={Pope, Reiner and Douglas, Sholto and Chowdhery, Aakanksha and Devlin, Jacob and Bradbury, James and Heek, Jonathan and Xiao, Kefan and Agrawal, Shivani and Dean, Jeff},
  journal={Proceedings of Machine Learning and Systems},
  volume={5},
  year={2023}
}


@article{dasgupta2003elementary,
  title={An elementary proof of a theorem of Johnson and Lindenstrauss},
  author={Dasgupta, Sanjoy and Gupta, Anupam},
  journal={Random Structures \& Algorithms},
  volume={22},
  number={1},
  pages={60--65},
  year={2003},
  publisher={Wiley Online Library}
}


@inproceedings{charikar2002similarity,
  title={Similarity estimation techniques from rounding algorithms},
  author={Charikar, Moses S},
  booktitle={Proceedings of the thiry-fourth annual ACM symposium on Theory of computing},
  pages={380--388},
  year={2002}
}

@inproceedings{ruiz2023dreambooth,
  title={Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation},
  author={Ruiz, Nataniel and Li, Yuanzhen and Jampani, Varun and Pritch, Yael and Rubinstein, Michael and Aberman, Kfir},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={22500--22510},
  year={2023}
}

@misc{sora,
  title = {Sora: Creating video from text},
  author = {OpenAI},
  note  =  {\url{https://openai.com/index/sora/}},
  year = 2024,
}

@misc{firefly,
  title = {Adobe Firefly},
  author = {Adobe Firefly},
  note  =  {\url{https://firefly.adobe.com/}},
  year = 2023,
}

@misc{midjourney,
  title = {Midjourney},
  author = {Midjourney},
  note  =  {\url{https://www.midjourney.com/home}},
  year = 2022,
}

@misc{copilot,
  title = {Microsoft Copilot},
  author = {Microsoft Copilot},
  note  =  {\url{https://github.com/features/copilot}},
  year = 2023,
}

@misc{Claude,
  title = {Claude 3.5 Sonnet},
  author = {Antropic},
  note  =  {\url{https://www.anthropic.com/news/3-5-models-and-computer-use}},
  year = 2024,
}


@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{reid2024gemini,
  title={Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context},
  author={Reid, Machel and Savinov, Nikolay and Teplyashin, Denis and Lepikhin, Dmitry and Lillicrap, Timothy and Alayrac, Jean-baptiste and Soricut, Radu and Lazaridou, Angeliki and Firat, Orhan and Schrittwieser, Julian and others},
  journal={arXiv preprint arXiv:2403.05530},
  year={2024}
}

@misc{gpt4o,
  title={Introducing GPT-4o},
  author={OpenAI},
  note={\url{https://openai.com/index/hello-gpt-4o/}},
  year={2024},
}

@article{ramesh2022hierarchical,
  title={Hierarchical text-conditional image generation with clip latents},
  author={Ramesh, Aditya and Dhariwal, Prafulla and Nichol, Alex and Chu, Casey and Chen, Mark},
  journal={arXiv preprint arXiv:2204.06125},
  year={2022}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Lukasz and Polosukhin, Illia},
  journal=neurips,
  year={2017}
}

@article{kaplan2020scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}

@article{johnson1986extensions,
  title={Extensions of Lipschitz maps into Banach spaces},
  author={Johnson, William B and Lindenstrauss, Joram and Schechtman, Gideon},
  journal={Israel Journal of Mathematics},
  volume={54},
  number={2},
  pages={129--138},
  year={1986},
  publisher={Springer}
}

@article{touvron2023llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}
@misc{longchat2023,
    title = {How Long Can Open-Source LLMs Truly Promise on Context Length?},
    url = {https://lmsys.org/blog/2023-06-29-longchat},
    author={Li, Dacheng and Shao, Rulin and Xie, Anze and Sheng, Ying and Zheng, Lianmin and Gonzalez, Joseph and Stoica, Ion and Ma, Xuezhe and Zhang, Hao},
    year = {2023},
    note = {{\url{https://huggingface.co/lmsys/longchat-7b-v1.5-32k}}},
}



@article{dettmers2022gpt3,
  title={Gpt3. int8 (): 8-bit matrix multiplication for transformers at scale},
  author={Dettmers, Tim and Lewis, Mike and Belkada, Younes and Zettlemoyer, Luke},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={30318--30332},
  year={2022}
}


@article{lin2023awq,
  title={Awq: Activation-aware weight quantization for llm compression and acceleration},
  author={Lin, Ji and Tang, Jiaming and Tang, Haotian and Yang, Shang and Dang, Xingyu and Han, Song},
  journal={arXiv preprint arXiv:2306.00978},
  year={2023}
}


@article{yu2016orthogonal,
  title={Orthogonal random features},
  author={Yu, Felix Xinnan X and Suresh, Ananda Theertha and Choromanski, Krzysztof M and Holtmann-Rice, Daniel N and Kumar, Sanjiv},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}


@article{ji2012super,
  title={Super-bit locality-sensitive hashing},
  author={Ji, Jianqiu and Li, Jianmin and Yan, Shuicheng and Zhang, Bo and Tian, Qi},
  journal={Advances in neural information processing systems},
  volume={25},
  year={2012}
}

@misc{llama3,
  title = {Llama3},
  author = {Llama3},
  note  =  {\url{https://github.com/meta-llama/llama3}},
  year = 2024,
}

@misc{mistral,
  title = {Mistral AI},
  author = {{Mistral AI}},
  note  =  {\url{https://mistral.ai/news/ministraux/}},
  year = 2024,
}

@misc{evalharness,
  author       = {Gao, Leo and Tow, Jonathan and Abbasi, Baber and Biderman, Stella and Black, Sid and DiPofi, Anthony and Foster, Charles and Golding, Laurence and Hsu, Jeffrey and Le Noac'h, Alain and Li, Haonan and McDonell, Kyle and Muennighoff, Niklas and Ociepa, Chris and Phang, Jason and Reynolds, Laria and Schoelkopf, Hailey and Skowron, Aviya and Sutawika, Lintang and Tang, Eric and Thite, Anish and Wang, Ben and Wang, Kevin and Zou, Andy},
  title        = {A framework for few-shot language model evaluation},
  year         = 2023,
  publisher    = {Zenodo},
  note = {\url{https://github.com/EleutherAI/lm-evaluation-harness}}
}

@article{dao2023flashattention,
  title={Flashattention-2: Faster attention with better parallelism and work partitioning},
  author={Dao, Tri},
  journal={arXiv preprint arXiv:2307.08691},
  year={2023}
}


@InProceedings{zandieh23kdeformer,
  title = 	 {{KDE}former: Accelerating Transformers via Kernel Density Estimation},
  author =       {Zandieh, Amir and Han, Insu and Daliri, Majid and Karbasi, Amin},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {40605--40623},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/zandieh23a/zandieh23a.pdf},
  url = 	 {https://proceedings.mlr.press/v202/zandieh23a.html},
  abstract = 	 {Dot-product attention mechanism plays a crucial role in modern deep architectures (e.g., Transformer) for sequence modeling, however, naïve exact computation of this model incurs quadratic time and memory complexities in sequence length, hindering the training of long-sequence models. Critical bottlenecks are due to the computation of partition functions in the denominator of softmax function as well as the multiplication of the softmax matrix with the matrix of values. Our key observation is that the former can be reduced to a variant of the kernel density estimation (KDE) problem, and an efficient KDE solver can be further utilized to accelerate the latter via subsampling-based fast matrix products. Our proposed KDEformer can approximate the attention in sub-quadratic time with provable spectral norm bounds, while all prior results merely provide entry-wise error bounds. Empirically, we verify that KDEformer outperforms other attention approximations in terms of accuracy, memory, and arithmetic operations on various pre-trained models. For instance, on BigGAN image generation we achieve better generative scores than the exact computation with over 4× speedup. For ImageNet classification with T2T-ViT, KDEformer shows over 18× speedup while the accuracy drop is less than 0.5%.}
}

@article{hyperattention,
  title={Hyperattention: Long-context attention in near-linear time},
  author={Han, Insu and Jarayam, Rajesh and Karbasi, Amin and Mirrokni, Vahab and Woodruff, David and Zandieh, Amir},
  journal={arXiv preprint arXiv:2310.05869},
  year={2023}
}

@misc{daliri2024samplingmethodsinnerproduct,
      title={Sampling Methods for Inner Product Sketching}, 
      author={Majid Daliri and Juliana Freire and Christopher Musco and Aécio Santos and Haoxiang Zhang},
      year={2024},
      eprint={2309.16157},
      archivePrefix={arXiv},
      primaryClass={cs.DB},
      url={https://arxiv.org/abs/2309.16157}, 
}

@article{matsumoto2024compresedsensing1bit,
author = {Matsumoto, Namiko and Mazumdar, Arya},
title = {Binary Iterative Hard Thresholding Converges with Optimal Number of Measurements for 1-Bit Compressed Sensing},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {71},
number = {5},
issn = {0004-5411},
url = {https://doi.org/10.1145/3680542},
doi = {10.1145/3680542},
abstract = {Compressed sensing has been a very successful high-dimensional signal acquisition and recovery technique that relies on linear operations. However, the actual measurements of signals have to be quantized before storing or processing them. One-bit compressed sensing is a heavily quantized version of compressed sensing, where each linear measurement of a signal is reduced to just one bit: the sign of the measurement. Once enough of such measurements are collected, the recovery problem in one-bit compressed sensing aims to find the original signal with as much accuracy as possible. The recovery problem is related to the traditional “halfspace-learning” problem in learning theory.&nbsp;&nbsp; For recovery of sparse vectors, a popular reconstruction method from one-bit measurements is the binary iterative hard thresholding (BIHT) algorithm. The algorithm is a simple projected subgradient descent method and is known to converge well empirically, despite the nonconvexity of the problem. The convergence property of BIHT was not theoretically fully justified (e.g., it is known that a number of measurement greater than  (max lbrace k^{10}, 24^{48}, k^{3.5}/epsilon rbrace) , where k is the sparsity and  (epsilon)  denotes the approximation error, is sufficient, Friedlander et&nbsp;al. [2021]. In this article we show that the BIHT estimates converge to the original signal with only  (frac{k}{epsilon })  measurements (up to logarithmic factors). Note that, this dependence on k and  (epsilon)  is optimal for any recovery method in one-bit compressed sensing. With this result, to the best of our knowledge, BIHT is the only practical and efficient (polynomial time) algorithm that requires the optimal number of measurements in all parameters (both k and  (epsilon) ). This is also an example of a gradient descent algorithm converging to the correct solution for a nonconvex problem under suitable structural conditions.},
journal = {J. ACM},
month = oct,
articleno = {35},
numpages = {64},
keywords = {Compressed sensing, quantization, gradient descent, sparsity}
}

@article{plan2017high,
  title={High-dimensional estimation with geometric constraints},
  author={Plan, Yaniv and Vershynin, Roman and Yudovina, Elena},
  journal={Information and Inference: A Journal of the IMA},
  volume={6},
  number={1},
  pages={1--40},
  year={2017},
  publisher={Oxford University Press}
}

@article{gao2024rabitq,
  title={RaBitQ: Quantizing High-Dimensional Vectors with a Theoretical Error Bound for Approximate Nearest Neighbor Search},
  author={Gao, Jianyang and Long, Cheng},
  journal={Proceedings of the ACM on Management of Data},
  volume={2},
  number={3},
  pages={1--27},
  year={2024},
  publisher={ACM New York, NY, USA}
}

@misc{qjl,
      title={QJL: 1-Bit Quantized JL Transform for KV Cache Quantization with Zero Overhead}, 
      author={Amir Zandieh and Majid Daliri and Insu Han},
      year={2024},
      eprint={2406.03482},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2406.03482}, 
}

@article{dai2024deepseekmoe,
  title={Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models},
  author={Dai, Damai and Deng, Chengqi and Zhao, Chenggang and Xu, RX and Gao, Huazuo and Chen, Deli and Li, Jiashi and Zeng, Wangding and Yu, Xingkai and Wu, Y and others},
  journal={arXiv preprint arXiv:2401.06066},
  year={2024}
}


@article{beltagy2020longformer,
  title={Longformer: The long-document transformer},
  author={Beltagy, Iz and Peters, Matthew E and Cohan, Arman},
  journal={arXiv preprint arXiv:2004.05150},
  year={2020}
}

@article{li2024snapkv,
  title={Snapkv: Llm knows what you are looking for before generation},
  author={Li, Yuhong and Huang, Yingbing and Yang, Bowen and Venkitesh, Bharat and Locatelli, Acyr and Ye, Hanchen and Cai, Tianle and Lewis, Patrick and Chen, Deming},
  journal={arXiv preprint arXiv:2404.14469},
  year={2024}
}


@article{sun2024shadowkv,
  title={Shadowkv: Kv cache in shadows for high-throughput long-context llm inference},
  author={Sun, Hanshi and Chang, Li-Wen and Bao, Wenlei and Zheng, Size and Zheng, Ningxin and Liu, Xin and Dong, Harry and Chi, Yuejie and Chen, Beidi},
  journal={arXiv preprint arXiv:2410.21465},
  year={2024}
}


@article{zandieh2024qjl,
  title={QJL: 1-Bit Quantized JL Transform for KV Cache Quantization with Zero Overhead},
  author={Zandieh, Amir and Daliri, Majid and Han, Insu},
  journal={arXiv preprint arXiv:2406.03482},
  year={2024}
}

@article{kim2024lexico,
  title={Lexico: Extreme KV Cache Compression via Sparse Coding over Universal Dictionaries},
  author={Kim, Junhyuck and Park, Jongho and Cho, Jaewoong and Papailiopoulos, Dimitris},
  journal={arXiv preprint arXiv:2412.08890},
  year={2024}
}


@article{ashkboos2024quarot,
  title={Quarot: Outlier-free 4-bit inference in rotated llms},
  author={Ashkboos, Saleh and Mohtashami, Amirkeivan and Croci, Maximilian L and Li, Bo and Cameron, Pashmina and Jaggi, Martin and Alistarh, Dan and Hoefler, Torsten and Hensman, James},
  journal={arXiv preprint arXiv:2404.00456},
  year={2024}
}


@article{shah2024flashattention,
  title={Flashattention-3: Fast and accurate attention with asynchrony and low-precision},
  author={Shah, Jay and Bikshandi, Ganesh and Zhang, Ying and Thakkar, Vijay and Ramani, Pradeep and Dao, Tri},
  journal={arXiv preprint arXiv:2407.08608},
  year={2024}
}
@article{dubey2024llama,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}
@article{paszke2019pytorch,
  title={Pytorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}
@article{wolf2019huggingface,
  title={Huggingface’s transformers: State-of-the-art natural language processing. arXiv},
  author={Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, R{\'e}mi and Funtowicz, Morgan and others},
  journal={arXiv preprint arXiv:1910.03771},
  year={2019}
}



@article{cai2024pyramidkv,
  title={Pyramidkv: Dynamic kv cache compression based on pyramidal information funneling},
  author={Cai, Zefan and Zhang, Yichi and Gao, Bofei and Liu, Yuliang and Liu, Tianyu and Lu, Keming and Xiong, Wayne and Dong, Yue and Chang, Baobao and Hu, Junjie and others},
  journal={arXiv preprint arXiv:2406.02069},
  year={2024}
}

%Some references for merge and reduce found online
@article{BHMSSZ21,
  title={Adversarial robustness of streaming algorithms through importance sampling},
  author={Braverman, Vladimir and Hassidim, Avinatan and Matias, Yossi and Schain, Mariano and Silwal, Sandeep and Zhou, Samson},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={3544--3557},
  year={2021}
}

@article{MG82,
  title={Finding repeated elements},
  author={Misra, Jayadev and Gries, David},
  journal={Sci. Comput. Program., 2(2):143– 152},
  year={1982}

}

@article{GLPW16,
    author = {Ghashami, Mina and Liberty, Edo and Phillips, Jeff M. and Woodruff, David P},
    title = {Frequent directions: Simple and deterministic matrix sketching},
    journal = {SIAM J. Comput., 45(5):1762–1792},
    year = {2016}
}
%end of mergereduce papers

%discrepancy papers
@article{PT20,
    author = {Phillips, Jeff M and Tai, Wai Ming},
    title = {Near-optimal coresets for kernel density estimates},
    journal = {Discrete and Computational Geometry, 63(4):867–887},
    year = {2020}
}

@article{CKW24,
    author = {Charikar, Moses and Kapralov, Michael and Waingarten, Erik},
    title = {A Quasi-Monte Carlo Data Structure for Smooth Kernel Evaluations},
    journal = {In Proceedings of the 35th ACM-SIAM Symposium on Discrete Algorithms (SODA ’2024), arXiv:2401.02562},
    year = {2024}
}

@article{KRR23,
    author = {Kulkarni, Janardhan and Reis, Victor and Rothvoss, Thomas},
    title = {Optimal Online Discrepancy Minimization},
    journal = {In Proceedings of the 56th Annual ACM Symposium on Theory of Computing (STOC ’2024), arXiv:2308.01406},
    year = {2023}
}

@article{B10,
    author = {Bansal, Nikhil},
    title = {Constructive algorithms for discrepancy minimization},
    journal = {51th Annual IEEE Symposium on Foundations of Computer Science (FOCS ’2010), arXiv:1002.2259},
    year = {2010}
}

@article{BJSS19,
    author = {Bansal, Nikhil and Jiang, Haotian and Singla, Sahil and Sinha, Makrand},
    title = {Online vector balancing and geometric discrepancy},
    journal = {In Proceedings of the 52nd Annual ACM Symposium on Theory of Computing (STOC ’2020), arXiv:1912.03350},
    year = {2019}
}

@article{DNTT18,
    author = {Dadush, Daniel and Nikolov, Aleksandar and Talwar, Kunal and Tomczak-Jaegermann, Nicole},
    title = {Balancing vectors in any
norm},
    journal = {59th Annual IEEE Symposium on Foundations of Computer Science (FOCS ’2018)},
    year = {2018} 
}

@article{B12,
    author = {Banaszczyk, Wojciech},
    title = {On series of signed vectors and their rearrangements},
    journal = {Random Structures and Algorithms 40 (2012), 301–316},
    year = {2012}
}

@article{B98,
    author = {Banaszczyk, Wojciech},
    title = {Balancing vectors and gaussian measures of n-dimensional convex bodies},
    journal = {Random Structures and Algorithms 12 (1998), 351–360},
    year = {1998}
}

%end of discrepancy papers

@inproceedings{jiang2024minference,
  title={MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention},
  author={Jiang, Huiqiang and LI, YUCHENG and Zhang, Chengruidong and Wu, Qianhui and Luo, Xufang and Ahn, Surin and Han, Zhenhua and Abdi, Amir H and Li, Dongsheng and Lin, Chin-Yew and others},
  booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
  year={2024}
}

@article{fu2024not,
  title={Not all heads matter: A head-level KV cache compression method with integrated retrieval and reasoning},
  author={Fu, Yu and Cai, Zefan and Asi, Abedelkadir and Xiong, Wayne and Dong, Yue and Xiao, Wen},
  journal={arXiv preprint arXiv:2410.19258},
  year={2024}
}