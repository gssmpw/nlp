\documentclass{article}

% Language setting
\usepackage[english]{babel}

% Set page size and margins
\usepackage[letterpaper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}
\usepackage[T1]{fontenc}

% page margin preferences
\topmargin -1.5cm
\oddsidemargin -0.04cm
\evensidemargin -0.04cm
\textwidth 16.59cm
\textheight 21.94cm 
\parskip 7.2pt
\parindent 0pt

\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{scalerel}
\usepackage{subfigure}
\usepackage{mathtools}
\usepackage{booktabs}

\usepackage{hyperref}
\usepackage{multirow}
\usepackage{xcolor}
\usepackage{color}

\newcommand{\theHalgorithm}{\arabic{algorithm}}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{filecontents}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algcompatible}
\usepackage{comment}
\usepackage[capitalize,noabbrev]{cleveref}   
\crefname{algocf}{alg.}{algs.}
\Crefname{algocf}{Algorithm}{Algorithms}
\def\llama{$\mathtt{Llama}$-$\mathtt{3.1}$-$\mathtt{8B}$-$\mathtt{Instruct}$}
\def\mistral{$\mathtt{Ministral}$-$\mathtt{8B}$-$\mathtt{Instruct}$-$\mathtt{2410}$}

\def\NoNumber#1{{\def\alglinenumber##1{}\State #1}\addtocounter{ALG@line}{-1}}

\begin{filecontents}{main.bib}
{
@Article{ALS21,
  title= "Discrepancy minimization via a self-balancing walk",
  author= "Ryan Alweiss and Yang P. Liu and Mehtaab Sawhney",
  journal="Proceedings of the 53rd ACM Symposium on the Theory of Computing (STOC ’2021)",
  year="2021"
}



@inproceedings{Bessa2023WeightedMinhash,
author = {Bessa, Aline and Daliri, Majid and Freire, Juliana and Musco, Cameron and Musco, Christopher and Santos, A\'{e}cio and Zhang, Haoxiang},
title = {Weighted Minwise Hashing Beats Linear Sketching for Inner Product Estimation},
year = {2023},
isbn = {9798400701276},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3584372.3588679},
doi = {10.1145/3584372.3588679},
abstract = {We present a new approach for independently computing compact sketches that can be used to approximate the inner product between pairs of high-dimensional vectors. Based on the Weighted MinHash algorithm, our approach admits strong accuracy guarantees that improve on the guarantees of popular linear sketching approaches for inner product estimation, such as CountSketch and Johnson-Lindenstrauss projection. Specifically, while our method exactly matches linear sketching for dense vectors, it yields significantly lower error for sparse vectors with limited overlap between non-zero entries. Such vectors arise in many applications involving sparse data, as well as in increasingly popular dataset search applications, where inner products are used to estimate data covariance, conditional means, and other quantities involving columns in unjoined tables. We complement our theoretical results by showing that our approach empirically outperforms existing linear sketches and unweighted hashing-based sketches for sparse vectors.},
booktitle = {Proceedings of the 42nd ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of Database Systems},
pages = {169–181},
numpages = {13},
keywords = {join-size estimation, vector sketching, inner product estimation},
location = {<conf-loc>, <city>Seattle</city>, <state>WA</state>, <country>USA</country>, </conf-loc>},
series = {PODS '23}
}

@article{dalirisampling:2024,
      title={Sampling Methods for Inner Product Sketching}, 
      author={Majid Daliri and Juliana Freire and Christopher Musco and Aécio Santos and Haoxiang Zhang},
      year={2024},
      journal = {Proc. VLDB Endow.}
}

@incollection{boucheron2003concentration,
  title={Concentration inequalities},
  author={Boucheron, St{\'e}phane and Lugosi, G{\'a}bor and Bousquet, Olivier},
  booktitle={Summer school on machine learning},
  pages={208--240},
  year={2003},
  publisher={Springer}
}

@article{liu2024kivi,
  title={KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache},
  author={Liu, Zirui and Yuan, Jiayi and Jin, Hongye and Zhong, Shaochen and Xu, Zhaozhuo and Braverman, Vladimir and Chen, Beidi and Hu, Xia},
  journal={arXiv preprint arXiv:2402.02750},
  year={2024}
}

@article{hooper2024kvquant,
  title={KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization},
  author={Hooper, Coleman and Kim, Sehoon and Mohammadzadeh, Hiva and Mahoney, Michael W and Shao, Yakun Sophia and Keutzer, Kurt and Gholami, Amir},
  journal={arXiv preprint arXiv:2401.18079},
  year={2024}
}


@article{shazeer2019fast,
  title={Fast transformer decoding: One write-head is all you need},
  author={Shazeer, Noam},
  journal={arXiv preprint arXiv:1911.02150},
  year={2019}
}


@inproceedings{ainslie2023gqa,
  title={GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints},
  author={Ainslie, Joshua and Lee-Thorp, James and de Jong, Michiel and Zemlyanskiy, Yury and Lebron, Federico and Sanghai, Sumit},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  pages={4895--4901},
  year={2023}
}


@article{zhang2024h2o,
  title={H2o: Heavy-hitter oracle for efficient generative inference of large language models},
  author={Zhang, Zhenyu and Sheng, Ying and Zhou, Tianyi and Chen, Tianlong and Zheng, Lianmin and Cai, Ruisi and Song, Zhao and Tian, Yuandong and R{\'e}, Christopher and Barrett, Clark and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}


@article{liu2024scissorhands,
  title={Scissorhands: Exploiting the persistence of importance hypothesis for llm kv cache compression at test time},
  author={Liu, Zichang and Desai, Aditya and Liao, Fangshuo and Wang, Weitao and Xie, Victor and Xu, Zhaozhuo and Kyrillidis, Anastasios and Shrivastava, Anshumali},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}


@article{xiao2023efficient,
  title={Efficient streaming language models with attention sinks},
  author={Xiao, Guangxuan and Tian, Yuandong and Chen, Beidi and Han, Song and Lewis, Mike},
  journal={arXiv preprint arXiv:2309.17453},
  year={2023}
}


@article{zandieh2024subgen,
  title={SubGen: Token Generation in Sublinear Time and Memory},
  author={Zandieh, Amir and Han, Insu and Mirrokni, Vahab and Karbasi, Amin},
  journal={arXiv preprint arXiv:2402.06082},
  year={2024}
}


@inproceedings{kwon2023efficient,
  title={Efficient memory management for large language model serving with pagedattention},
  author={Kwon, Woosuk and Li, Zhuohan and Zhuang, Siyuan and Sheng, Ying and Zheng, Lianmin and Yu, Cody Hao and Gonzalez, Joseph and Zhang, Hao and Stoica, Ion},
  booktitle={Proceedings of the 29th Symposium on Operating Systems Principles},
  pages={611--626},
  year={2023}
}


@inproceedings{sheng2023flexgen,
  title={Flexgen: High-throughput generative inference of large language models with a single gpu},
  author={Sheng, Ying and Zheng, Lianmin and Yuan, Binhang and Li, Zhuohan and Ryabinin, Max and Chen, Beidi and Liang, Percy and R{\'e}, Christopher and Stoica, Ion and Zhang, Ce},
  booktitle={International Conference on Machine Learning},
  pages={31094--31116},
  year={2023},
  organization={PMLR}
}


@article{dettmers2024qlora,
  title={Qlora: Efficient finetuning of quantized llms},
  author={Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}


@article{frantar2022gptq,
  title={Gptq: Accurate post-training quantization for generative pre-trained transformers},
  author={Frantar, Elias and Ashkboos, Saleh and Hoefler, Torsten and Alistarh, Dan},
  journal={arXiv preprint arXiv:2210.17323},
  year={2022}
}

@article{dettmers2023spqr,
  title={Spqr: A sparse-quantized representation for near-lossless llm weight compression},
  author={Dettmers, Tim and Svirschevski, Ruslan and Egiazarian, Vage and Kuznedelev, Denis and Frantar, Elias and Ashkboos, Saleh and Borzunov, Alexander and Hoefler, Torsten and Alistarh, Dan},
  journal={arXiv preprint arXiv:2306.03078},
  year={2023}
}




@article{yue2024wkvquant,
  title={Wkvquant: Quantizing weight and key/value cache for large language models gains more},
  author={Yue, Yuxuan and Yuan, Zhihang and Duanmu, Haojie and Zhou, Sifan and Wu, Jianlong and Nie, Liqiang},
  journal={arXiv preprint arXiv:2402.12065},
  year={2024}
}


@article{yang2024no,
  title={No Token Left Behind: Reliable KV Cache Compression via Importance-Aware Mixed Precision Quantization},
  author={Yang, June Yong and Kim, Byeongwook and Bae, Jeongin and Kwon, Beomseok and Park, Gunho and Yang, Eunho and Kwon, Se Jung and Lee, Dongsoo},
  journal={arXiv preprint arXiv:2402.18096},
  year={2024}
}


@article{dong2024qaq,
  title={QAQ: Quality Adaptive Quantization for LLM KV Cache},
  author={Dong, Shichen and Cheng, Wen and Qin, Jiayu and Wang, Wei},
  journal={arXiv preprint arXiv:2403.04643},
  year={2024}
}


@article{kang2024gear,
  title={Gear: An efficient kv cache compression recipefor near-lossless generative inference of llm},
  author={Kang, Hao and Zhang, Qingru and Kundu, Souvik and Jeong, Geonhwa and Liu, Zaoxing and Krishna, Tushar and Zhao, Tuo},
  journal={arXiv preprint arXiv:2403.05527},
  year={2024}
}


@article{zhang2024kv,
  title={KV Cache is 1 Bit Per Channel: Efficient Large Language Model Inference with Coupled Quantization},
  author={Zhang, Tianyi and Yi, Jonah and Xu, Zhaozhuo and Shrivastava, Anshumali},
  journal={arXiv preprint arXiv:2405.03917},
  year={2024}
}

@article{bai2023longbench,
  title={LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding},
  author={Bai, Yushi and Lv, Xin and Zhang, Jiajie and Lyu, Hongchang and Tang, Jiankai and Huang, Zhidian and Du, Zhengxiao and Liu, Xiao and Zeng, Aohan and Hou, Lei and Dong, Yuxiao and Tang, Jie and Li, Juanzi},
  journal={arXiv preprint arXiv:2308.14508},
  year={2023}
}


@article{pope2023efficiently,
  title={Efficiently scaling transformer inference},
  author={Pope, Reiner and Douglas, Sholto and Chowdhery, Aakanksha and Devlin, Jacob and Bradbury, James and Heek, Jonathan and Xiao, Kefan and Agrawal, Shivani and Dean, Jeff},
  journal={Proceedings of Machine Learning and Systems},
  volume={5},
  year={2023}
}


@article{dasgupta2003elementary,
  title={An elementary proof of a theorem of Johnson and Lindenstrauss},
  author={Dasgupta, Sanjoy and Gupta, Anupam},
  journal={Random Structures \& Algorithms},
  volume={22},
  number={1},
  pages={60--65},
  year={2003},
  publisher={Wiley Online Library}
}


@inproceedings{charikar2002similarity,
  title={Similarity estimation techniques from rounding algorithms},
  author={Charikar, Moses S},
  booktitle={Proceedings of the thiry-fourth annual ACM symposium on Theory of computing},
  pages={380--388},
  year={2002}
}

@inproceedings{ruiz2023dreambooth,
  title={Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation},
  author={Ruiz, Nataniel and Li, Yuanzhen and Jampani, Varun and Pritch, Yael and Rubinstein, Michael and Aberman, Kfir},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={22500--22510},
  year={2023}
}

@misc{sora,
  title = {Sora: Creating video from text},
  author = {OpenAI},
  note  =  {\url{https://openai.com/index/sora/}},
  year = 2024,
}

@misc{firefly,
  title = {Adobe Firefly},
  author = {Adobe Firefly},
  note  =  {\url{https://firefly.adobe.com/}},
  year = 2023,
}

@misc{midjourney,
  title = {Midjourney},
  author = {Midjourney},
  note  =  {\url{https://www.midjourney.com/home}},
  year = 2022,
}

@misc{copilot,
  title = {Microsoft Copilot},
  author = {Microsoft Copilot},
  note  =  {\url{https://github.com/features/copilot}},
  year = 2023,
}

@misc{Claude,
  title = {Claude 3.5 Sonnet},
  author = {Antropic},
  note  =  {\url{https://www.anthropic.com/news/3-5-models-and-computer-use}},
  year = 2024,
}


@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{reid2024gemini,
  title={Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context},
  author={Reid, Machel and Savinov, Nikolay and Teplyashin, Denis and Lepikhin, Dmitry and Lillicrap, Timothy and Alayrac, Jean-baptiste and Soricut, Radu and Lazaridou, Angeliki and Firat, Orhan and Schrittwieser, Julian and others},
  journal={arXiv preprint arXiv:2403.05530},
  year={2024}
}

@misc{gpt4o,
  title={Introducing GPT-4o},
  author={OpenAI},
  note={\url{https://openai.com/index/hello-gpt-4o/}},
  year={2024},
}

@article{ramesh2022hierarchical,
  title={Hierarchical text-conditional image generation with clip latents},
  author={Ramesh, Aditya and Dhariwal, Prafulla and Nichol, Alex and Chu, Casey and Chen, Mark},
  journal={arXiv preprint arXiv:2204.06125},
  year={2022}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Lukasz and Polosukhin, Illia},
  journal=neurips,
  year={2017}
}

@article{kaplan2020scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}

@article{johnson1986extensions,
  title={Extensions of Lipschitz maps into Banach spaces},
  author={Johnson, William B and Lindenstrauss, Joram and Schechtman, Gideon},
  journal={Israel Journal of Mathematics},
  volume={54},
  number={2},
  pages={129--138},
  year={1986},
  publisher={Springer}
}

@article{touvron2023llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}
@misc{longchat2023,
    title = {How Long Can Open-Source LLMs Truly Promise on Context Length?},
    url = {https://lmsys.org/blog/2023-06-29-longchat},
    author={Li, Dacheng and Shao, Rulin and Xie, Anze and Sheng, Ying and Zheng, Lianmin and Gonzalez, Joseph and Stoica, Ion and Ma, Xuezhe and Zhang, Hao},
    year = {2023},
    note = {{\url{https://huggingface.co/lmsys/longchat-7b-v1.5-32k}}},
}



@article{dettmers2022gpt3,
  title={Gpt3. int8 (): 8-bit matrix multiplication for transformers at scale},
  author={Dettmers, Tim and Lewis, Mike and Belkada, Younes and Zettlemoyer, Luke},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={30318--30332},
  year={2022}
}


@article{lin2023awq,
  title={Awq: Activation-aware weight quantization for llm compression and acceleration},
  author={Lin, Ji and Tang, Jiaming and Tang, Haotian and Yang, Shang and Dang, Xingyu and Han, Song},
  journal={arXiv preprint arXiv:2306.00978},
  year={2023}
}


@article{yu2016orthogonal,
  title={Orthogonal random features},
  author={Yu, Felix Xinnan X and Suresh, Ananda Theertha and Choromanski, Krzysztof M and Holtmann-Rice, Daniel N and Kumar, Sanjiv},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}


@article{ji2012super,
  title={Super-bit locality-sensitive hashing},
  author={Ji, Jianqiu and Li, Jianmin and Yan, Shuicheng and Zhang, Bo and Tian, Qi},
  journal={Advances in neural information processing systems},
  volume={25},
  year={2012}
}

@misc{llama3,
  title = {Llama3},
  author = {Llama3},
  note  =  {\url{https://github.com/meta-llama/llama3}},
  year = 2024,
}

@misc{mistral,
  title = {Mistral AI},
  author = {{Mistral AI}},
  note  =  {\url{https://mistral.ai/news/ministraux/}},
  year = 2024,
}

@misc{evalharness,
  author       = {Gao, Leo and Tow, Jonathan and Abbasi, Baber and Biderman, Stella and Black, Sid and DiPofi, Anthony and Foster, Charles and Golding, Laurence and Hsu, Jeffrey and Le Noac'h, Alain and Li, Haonan and McDonell, Kyle and Muennighoff, Niklas and Ociepa, Chris and Phang, Jason and Reynolds, Laria and Schoelkopf, Hailey and Skowron, Aviya and Sutawika, Lintang and Tang, Eric and Thite, Anish and Wang, Ben and Wang, Kevin and Zou, Andy},
  title        = {A framework for few-shot language model evaluation},
  year         = 2023,
  publisher    = {Zenodo},
  note = {\url{https://github.com/EleutherAI/lm-evaluation-harness}}
}

@article{dao2023flashattention,
  title={Flashattention-2: Faster attention with better parallelism and work partitioning},
  author={Dao, Tri},
  journal={arXiv preprint arXiv:2307.08691},
  year={2023}
}


@InProceedings{zandieh23kdeformer,
  title = 	 {{KDE}former: Accelerating Transformers via Kernel Density Estimation},
  author =       {Zandieh, Amir and Han, Insu and Daliri, Majid and Karbasi, Amin},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {40605--40623},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/zandieh23a/zandieh23a.pdf},
  url = 	 {https://proceedings.mlr.press/v202/zandieh23a.html},
  abstract = 	 {Dot-product attention mechanism plays a crucial role in modern deep architectures (e.g., Transformer) for sequence modeling, however, naïve exact computation of this model incurs quadratic time and memory complexities in sequence length, hindering the training of long-sequence models. Critical bottlenecks are due to the computation of partition functions in the denominator of softmax function as well as the multiplication of the softmax matrix with the matrix of values. Our key observation is that the former can be reduced to a variant of the kernel density estimation (KDE) problem, and an efficient KDE solver can be further utilized to accelerate the latter via subsampling-based fast matrix products. Our proposed KDEformer can approximate the attention in sub-quadratic time with provable spectral norm bounds, while all prior results merely provide entry-wise error bounds. Empirically, we verify that KDEformer outperforms other attention approximations in terms of accuracy, memory, and arithmetic operations on various pre-trained models. For instance, on BigGAN image generation we achieve better generative scores than the exact computation with over 4× speedup. For ImageNet classification with T2T-ViT, KDEformer shows over 18× speedup while the accuracy drop is less than 0.5%.}
}

@article{hyperattention,
  title={Hyperattention: Long-context attention in near-linear time},
  author={Han, Insu and Jarayam, Rajesh and Karbasi, Amin and Mirrokni, Vahab and Woodruff, David and Zandieh, Amir},
  journal={arXiv preprint arXiv:2310.05869},
  year={2023}
}

@misc{daliri2024samplingmethodsinnerproduct,
      title={Sampling Methods for Inner Product Sketching}, 
      author={Majid Daliri and Juliana Freire and Christopher Musco and Aécio Santos and Haoxiang Zhang},
      year={2024},
      eprint={2309.16157},
      archivePrefix={arXiv},
      primaryClass={cs.DB},
      url={https://arxiv.org/abs/2309.16157}, 
}

@article{matsumoto2024compresedsensing1bit,
author = {Matsumoto, Namiko and Mazumdar, Arya},
title = {Binary Iterative Hard Thresholding Converges with Optimal Number of Measurements for 1-Bit Compressed Sensing},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {71},
number = {5},
issn = {0004-5411},
url = {https://doi.org/10.1145/3680542},
doi = {10.1145/3680542},
abstract = {Compressed sensing has been a very successful high-dimensional signal acquisition and recovery technique that relies on linear operations. However, the actual measurements of signals have to be quantized before storing or processing them. One-bit compressed sensing is a heavily quantized version of compressed sensing, where each linear measurement of a signal is reduced to just one bit: the sign of the measurement. Once enough of such measurements are collected, the recovery problem in one-bit compressed sensing aims to find the original signal with as much accuracy as possible. The recovery problem is related to the traditional “halfspace-learning” problem in learning theory.&nbsp;&nbsp; For recovery of sparse vectors, a popular reconstruction method from one-bit measurements is the binary iterative hard thresholding (BIHT) algorithm. The algorithm is a simple projected subgradient descent method and is known to converge well empirically, despite the nonconvexity of the problem. The convergence property of BIHT was not theoretically fully justified (e.g., it is known that a number of measurement greater than  (max lbrace k^{10}, 24^{48}, k^{3.5}/epsilon rbrace) , where k is the sparsity and  (epsilon)  denotes the approximation error, is sufficient, Friedlander et&nbsp;al. [2021]. In this article we show that the BIHT estimates converge to the original signal with only  (frac{k}{epsilon })  measurements (up to logarithmic factors). Note that, this dependence on k and  (epsilon)  is optimal for any recovery method in one-bit compressed sensing. With this result, to the best of our knowledge, BIHT is the only practical and efficient (polynomial time) algorithm that requires the optimal number of measurements in all parameters (both k and  (epsilon) ). This is also an example of a gradient descent algorithm converging to the correct solution for a nonconvex problem under suitable structural conditions.},
journal = {J. ACM},
month = oct,
articleno = {35},
numpages = {64},
keywords = {Compressed sensing, quantization, gradient descent, sparsity}
}

@article{plan2017high,
  title={High-dimensional estimation with geometric constraints},
  author={Plan, Yaniv and Vershynin, Roman and Yudovina, Elena},
  journal={Information and Inference: A Journal of the IMA},
  volume={6},
  number={1},
  pages={1--40},
  year={2017},
  publisher={Oxford University Press}
}

@article{gao2024rabitq,
  title={RaBitQ: Quantizing High-Dimensional Vectors with a Theoretical Error Bound for Approximate Nearest Neighbor Search},
  author={Gao, Jianyang and Long, Cheng},
  journal={Proceedings of the ACM on Management of Data},
  volume={2},
  number={3},
  pages={1--27},
  year={2024},
  publisher={ACM New York, NY, USA}
}

@misc{qjl,
      title={QJL: 1-Bit Quantized JL Transform for KV Cache Quantization with Zero Overhead}, 
      author={Amir Zandieh and Majid Daliri and Insu Han},
      year={2024},
      eprint={2406.03482},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2406.03482}, 
}

@article{dai2024deepseekmoe,
  title={Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models},
  author={Dai, Damai and Deng, Chengqi and Zhao, Chenggang and Xu, RX and Gao, Huazuo and Chen, Deli and Li, Jiashi and Zeng, Wangding and Yu, Xingkai and Wu, Y and others},
  journal={arXiv preprint arXiv:2401.06066},
  year={2024}
}


@article{beltagy2020longformer,
  title={Longformer: The long-document transformer},
  author={Beltagy, Iz and Peters, Matthew E and Cohan, Arman},
  journal={arXiv preprint arXiv:2004.05150},
  year={2020}
}

@article{li2024snapkv,
  title={Snapkv: Llm knows what you are looking for before generation},
  author={Li, Yuhong and Huang, Yingbing and Yang, Bowen and Venkitesh, Bharat and Locatelli, Acyr and Ye, Hanchen and Cai, Tianle and Lewis, Patrick and Chen, Deming},
  journal={arXiv preprint arXiv:2404.14469},
  year={2024}
}


@article{sun2024shadowkv,
  title={Shadowkv: Kv cache in shadows for high-throughput long-context llm inference},
  author={Sun, Hanshi and Chang, Li-Wen and Bao, Wenlei and Zheng, Size and Zheng, Ningxin and Liu, Xin and Dong, Harry and Chi, Yuejie and Chen, Beidi},
  journal={arXiv preprint arXiv:2410.21465},
  year={2024}
}


@article{zandieh2024qjl,
  title={QJL: 1-Bit Quantized JL Transform for KV Cache Quantization with Zero Overhead},
  author={Zandieh, Amir and Daliri, Majid and Han, Insu},
  journal={arXiv preprint arXiv:2406.03482},
  year={2024}
}

@article{kim2024lexico,
  title={Lexico: Extreme KV Cache Compression via Sparse Coding over Universal Dictionaries},
  author={Kim, Junhyuck and Park, Jongho and Cho, Jaewoong and Papailiopoulos, Dimitris},
  journal={arXiv preprint arXiv:2412.08890},
  year={2024}
}


@article{ashkboos2024quarot,
  title={Quarot: Outlier-free 4-bit inference in rotated llms},
  author={Ashkboos, Saleh and Mohtashami, Amirkeivan and Croci, Maximilian L and Li, Bo and Cameron, Pashmina and Jaggi, Martin and Alistarh, Dan and Hoefler, Torsten and Hensman, James},
  journal={arXiv preprint arXiv:2404.00456},
  year={2024}
}


@article{shah2024flashattention,
  title={Flashattention-3: Fast and accurate attention with asynchrony and low-precision},
  author={Shah, Jay and Bikshandi, Ganesh and Zhang, Ying and Thakkar, Vijay and Ramani, Pradeep and Dao, Tri},
  journal={arXiv preprint arXiv:2407.08608},
  year={2024}
}
@article{dubey2024llama,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}
@article{paszke2019pytorch,
  title={Pytorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}
@article{wolf2019huggingface,
  title={Huggingface’s transformers: State-of-the-art natural language processing. arXiv},
  author={Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, R{\'e}mi and Funtowicz, Morgan and others},
  journal={arXiv preprint arXiv:1910.03771},
  year={2019}
}



@article{cai2024pyramidkv,
  title={Pyramidkv: Dynamic kv cache compression based on pyramidal information funneling},
  author={Cai, Zefan and Zhang, Yichi and Gao, Bofei and Liu, Yuliang and Liu, Tianyu and Lu, Keming and Xiong, Wayne and Dong, Yue and Chang, Baobao and Hu, Junjie and others},
  journal={arXiv preprint arXiv:2406.02069},
  year={2024}
}

%Some references for merge and reduce found online
@article{BHMSSZ21,
  title={Adversarial robustness of streaming algorithms through importance sampling},
  author={Braverman, Vladimir and Hassidim, Avinatan and Matias, Yossi and Schain, Mariano and Silwal, Sandeep and Zhou, Samson},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={3544--3557},
  year={2021}
}

@article{MG82,
  title={Finding repeated elements},
  author={Misra, Jayadev and Gries, David},
  journal={Sci. Comput. Program., 2(2):143– 152},
  year={1982}

}

@article{GLPW16,
    author = {Ghashami, Mina and Liberty, Edo and Phillips, Jeff M. and Woodruff, David P},
    title = {Frequent directions: Simple and deterministic matrix sketching},
    journal = {SIAM J. Comput., 45(5):1762–1792},
    year = {2016}
}
%end of mergereduce papers

%discrepancy papers
@article{PT20,
    author = {Phillips, Jeff M and Tai, Wai Ming},
    title = {Near-optimal coresets for kernel density estimates},
    journal = {Discrete and Computational Geometry, 63(4):867–887},
    year = {2020}
}

@article{CKW24,
    author = {Charikar, Moses and Kapralov, Michael and Waingarten, Erik},
    title = {A Quasi-Monte Carlo Data Structure for Smooth Kernel Evaluations},
    journal = {In Proceedings of the 35th ACM-SIAM Symposium on Discrete Algorithms (SODA ’2024), arXiv:2401.02562},
    year = {2024}
}

@article{KRR23,
    author = {Kulkarni, Janardhan and Reis, Victor and Rothvoss, Thomas},
    title = {Optimal Online Discrepancy Minimization},
    journal = {In Proceedings of the 56th Annual ACM Symposium on Theory of Computing (STOC ’2024), arXiv:2308.01406},
    year = {2023}
}

@article{B10,
    author = {Bansal, Nikhil},
    title = {Constructive algorithms for discrepancy minimization},
    journal = {51th Annual IEEE Symposium on Foundations of Computer Science (FOCS ’2010), arXiv:1002.2259},
    year = {2010}
}

@article{BJSS19,
    author = {Bansal, Nikhil and Jiang, Haotian and Singla, Sahil and Sinha, Makrand},
    title = {Online vector balancing and geometric discrepancy},
    journal = {In Proceedings of the 52nd Annual ACM Symposium on Theory of Computing (STOC ’2020), arXiv:1912.03350},
    year = {2019}
}

@article{DNTT18,
    author = {Dadush, Daniel and Nikolov, Aleksandar and Talwar, Kunal and Tomczak-Jaegermann, Nicole},
    title = {Balancing vectors in any
norm},
    journal = {59th Annual IEEE Symposium on Foundations of Computer Science (FOCS ’2018)},
    year = {2018} 
}

@article{B12,
    author = {Banaszczyk, Wojciech},
    title = {On series of signed vectors and their rearrangements},
    journal = {Random Structures and Algorithms 40 (2012), 301–316},
    year = {2012}
}

@article{B98,
    author = {Banaszczyk, Wojciech},
    title = {Balancing vectors and gaussian measures of n-dimensional convex bodies},
    journal = {Random Structures and Algorithms 12 (1998), 351–360},
    year = {1998}
}

%end of discrepancy papers

@inproceedings{jiang2024minference,
  title={MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention},
  author={Jiang, Huiqiang and LI, YUCHENG and Zhang, Chengruidong and Wu, Qianhui and Luo, Xufang and Ahn, Surin and Han, Zhenhua and Abdi, Amir H and Li, Dongsheng and Lin, Chin-Yew and others},
  booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
  year={2024}
}

@article{fu2024not,
  title={Not all heads matter: A head-level KV cache compression method with integrated retrieval and reasoning},
  author={Fu, Yu and Cai, Zefan and Asi, Abedelkadir and Xiong, Wayne and Dong, Yue and Xiao, Wen},
  journal={arXiv preprint arXiv:2410.19258},
  year={2024}
}
}
\end{filecontents}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\newcommand{\norm}[1]{\ensuremath{\left\| #1 \right\|}}

\newcommand{\email}[1]{\href{mailto:#1}{\color{black} \texttt{#1}}}
\title{BalanceKV: KV Cache Compression through Discrepancy Theory}

\author{
    Insu Han\\
    KAIST\\
    \email{insu.han@kaist.ac.kr}
    \and 
    Michael Kapralov\\
    EPFL\\
    \email{michael.kapralov@epfl.ch}
    \and
    Ekaterina Kochetkova\\
    EPFL\\
    \email{ekaterina.kochetkova@epfl.ch}
    \and
    Kshiteej Sheth\\
    EPFL\\
    \email{kshiteej.sheth@epfl.ch}
    \and
    Amir Zandieh\\
    Google Research\\
    \email{zandieh@google.com}
}

\date{}

\begin{document} 

\maketitle

\begin{abstract}
Large language models (LLMs) have achieved impressive success, but their high memory requirements present challenges for long-context token generation. The memory complexity of long-context LLMs is primarily due to the need to store Key-Value (KV) embeddings in their KV cache. We present BalanceKV, a KV cache compression method based on geometric sampling process stemming from Banaszczyk's vector balancing theory, which introduces dependencies informed by the geometry of keys and value tokens, and improves precision. BalanceKV offers both theoretically proven and empirically validated performance improvements over existing methods.
\end{abstract}

\section{Introduction}

Transformer-based models are the foundation of ongoing artificial intelligence revolution.
Their applications span a wide range of domains, from leading-edge language models (LLM)~\cite{achiam2023gpt,claude} to text-to-image~\cite{ramesh2022hierarchical,firefly,midjourney}, text-to-video synthesis~\cite{sora}, coding assistant~\cite{copilot} and even multimodal domain across text, audio, image, and video~\cite{gpt4o}. 
At the core of these models is the Transformer architecture, powered by the self-attention mechanism~\cite{vaswani2017attention}, which enables effective capture of pairwise correlations across tokens in an input sequence.
As these models scale in size and context length~\cite{kaplan2020scaling}, they face significant computational challenges, particularly in terms of memory usage.


To unlock the full potential of LLMs, it is essential to achieve both efficiency and accuracy in generating long sequences. 
Most large language models, along with multimodal and video models, adopt an autoregressive, decoder-only architecture. 
This architecture generates tokens sequentially, applying attention dynamically to each newly generated token. 
To avoid redundant attention score computations during the generation phase, these models maintain a KV cache, which stores the key and value embeddings of previously generated tokens in each attention layer.
However, deploying autoregressive Transformers is computationally demanding. 
A major challenge lies in the substantial memory requirements, as the KV cache size scales with both the model size (i.e., the number of layers and attention heads) and, critically, the context size. 
Additionally, each model session typically requires its own dedicated KV cache, further exacerbating memory usage.

This growing demand has become a significant bottleneck, affecting both memory consumption and computational speed, particularly for models handling long context lengths. 
Therefore, reducing the KV cache size without compromising accuracy is essential to overcoming these limitations.


Several approaches have been proposed to address this challenge. 
Architectural solutions, such as multi-query attention \cite{shazeer2019fast}, multi-group attention \cite{ainslie2023gqa}, and sparse mixture of experts~\cite{dai2024deepseekmoe}, require modifying the model during the training stage and retraining it from scratch.
As a result, these methods are not directly applicable as off-the-shelf solutions for reducing the KV cache without incurring substantial pretraining costs.
Another line of research tackles the issue from a systems perspective, such as RAM offloading~\cite{sheng2023flexgen, sun2024shadowkv} or integrating virtual memory and paging strategies into the attention mechanism~\cite{kwon2023efficient}.

In this work, we propose \hyperref[alg:main]{\textsc{BalanceKV}}, a novel approach based on discrepancy theory designed to significantly reduce the runtime and memory complexity for KV cache. Our method can shift conventional linear scaling to a sublinear scale with respect to context length. The core of our approach is a vector balancing algorithm from discrepancy theory that exploits the geometry of key and value tokens to deduce a small subset of them that well approximates the operations happening inside a self-attention layer. 


\subsection{Related Works}\label{sec:related_works}
The direct approach to KV caching stores all key and value representations for every input token, leading to significant memory growth in long sequences, but several algorithmic ideas for improvement have been developed in the literature.

\paragraph{Embedding quantization.} A simple yet effective approach to reducing KV cache size is quantizing the floating-point numbers (FPN) in the KV cache to fixed-point numbers with fewer bits. 
Several quantization methods have been proposed specifically for the KV cache~\cite{yue2024wkvquant, yang2024no, dong2024qaq, kang2024gear, liu2024kivi, hooper2024kvquant, zhang2024kv, zandieh2024qjl}.
However, such approaches still require memory space that linearly increases in sequence length. 

\paragraph{Token level pruning.} A prominent line of work which is related to our paper focuses on token-level KV cache  pruning where redundant or less important tokens get evicted from the cache~\cite{beltagy2020longformer, zhang2024h2o, liu2024scissorhands, xiao2023efficient, zandieh2024subgen, li2024snapkv}.
Many of the works in this line have used accumulated attention scores to select important tokens in the cache~\cite{zhang2024h2o, li2024snapkv, xiao2023efficient}. 
Recent works extend those methods to an adaptive way of budget allocation across layer~\cite{cai2024pyramidkv} and head~\cite{fu2024not}.


A very recent work, Lexico~\cite{kim2024lexico}, applies techniques from sparse representation learning to compress the KV cache by learning a universal dictionary such that all key and value embeddings are represented as extremely sparse vectors within the learned dictionary. 
Unfortunately, this approach requires solving a computationally expensive matching pursuit algorithm for each key and value embedding, making Lexico relatively slow.

\paragraph{Algorithmic discrepancy theory.}
Banaszczyk's seminal works~\cite{B98, B12} establishing theoretical guarantees for vector set discrepancy have sparked research in the vector balancing problem~\cite{DNTT18}. This led to algorithmic developments in both offline \cite{B10} and online \cite{BJSS19, ALS21, KRR23} settings. The vector balancing problem has particular relevance to streaming and sublinear algorithms, as minimizing a dataset's discrepancy yields small subsets that effectively preserve the original dataset's properties. Recent works~\cite{PT20, CKW24} extend these discrepancy theory ideas to develop sublinear memory algorithms for \emph{kernel density estimation}.

\subsection{Overview of Our Contributions}

For a sequence of tokens and their corresponding query, key and value embeddings in an input prompt, a natural approach to compressing the exact KV cache would be to first select key and value embeddings for a few important tokens in the sequence such as the first and latest few tokens, as motivated by \cite{xiao2023efficient}, since important contexts are likely contained in them, and then select a small representative subset of key and value embeddings for the much longer middle portion of the sequence. 
This representative subset can be chosen by \emph{independently} sampling and selecting key and value embeddings from middle portion, however this would ignore the semantic relationship of the tokens in the middle portion encoded in the geometric structure of the corresponding key and value embeddings. 
Several powerful practical heuristic approaches, discussed in \cref{sec:related_works}, have been proposed recently, PyramidKV \cite{cai2024pyramidkv} and SnapKV \cite{li2024snapkv} being the state of the art. 
These heuristics select a subset of key and value embeddings from the entire sequence using attention score computation, thereby taking relative semantic importance of input tokens into account. However, these techniques lack theoretical guarantees. Our main contribution is a principled geometric sampling method for selecting a representative subset of tokens, which matches or outperforms the above mentioned heuristics. Our contributions are as follows: 
\begin{enumerate}
    \item In \cref{sec:theory_main} we propose \hyperref[alg:main]{\textsc{BalanceKV}}, an algorithm for compressing the KV cache recursively using a geometric correlated sampling process based on discrepancy theory. We show that \hyperref[alg:main]{\textsc{BalanceKV}} provably approximates attention in the streaming setting under the bounded $\ell_2$ norm assumption (\cref{thm:main-theorem}).
    \cref{sec:tech_overview} contains the formal problem formulation of streaming attention as well as a technical overview of the main results and techniques.
    \item In \cref{sec:experiments_main} we empirically evaluate our algorithm in two settings. In the first setting (see \cref{sec:single-layer}) we show our approach leads to a lower relative error for single layer attention approximation for open-source LLMs including \llama~\cite{dubey2024llama} and \mistral~\cite{mistral} as compared to uniformly sampling keys and values in the cache. In the second setting we show that our provable method performs better compared to previous existing heuristic cache compression methods based on selecting a subset of key and value embeddings in the cache on the LongBench \cite{bai2023longbench} benchmark for long context understanding capabilities of large language models under various end to end tasks.
\end{enumerate}

\section{Technical Overview}\label{sec:tech_overview}
In this section, we first set up the formal problem formulation that we tackle, followed by an overview of our techniques and our main results.

\subsection{KV Cache Compression}
Autoregressive Transformers generate tokens one by one and each depends on the previously generated tokens. When Transformers process a sequence of tokens, the \textit{attention mechanism} operates by computing three types of embeddings for each token at every layer: query, key and value. The query and key capture how different tokens interact, while the value is the actual content to be aggregated. Such interactions are quantified by so-called \textit{attention scores}, obtained by applying the softmax to the inner product between the query of a given token and the keys of all others. These scores determine how much each previous token's value contributes to the final output.  Once the keys and values are computed for a given token, they do not need to be recomputed when generating subsequent tokens. 

Formally, suppose  we have a stream of query, key and value embeddings $(q_1,k_1,v_1),\ldots,(q_n,k_n,v_n)$, that is the $j$-th token is represented as a triplet of ($q_j, k_j, v_j$) where $q_j, k_j, v_j \in \mathbb{R}^d$ for all $j\in [n]$. Let $K_j, V_j \in \mathbb{R}^{j \times d}$ be matrices defined by stacking those keys and values in their respective rows. To compute the following at every step $j$ to generate $j+1$ token, is called the \emph{streaming attention problem}:
\begin{equation}\label{eq:attn-def}
    \text{Attn}(q_j, K_j, V_j) := \text{softmax}\left(\frac{K_j\cdot q_j}{\sqrt{d}}\right)^T\cdot V_j. 
\end{equation}

Keeping all of the key-value pairs in the cache is prohibitively expensive, especially for long sequences. Instead, we opt for approximate computation by sampling a few key-value pairs. Specifically, our goal is to construct an algorithm that at every time step $j$ computes an estimator $z_j$ for $\text{Attn}(q_j, K_j, V_j)$ in sublinear in $n$ time and memory. In particular for given precision $\varepsilon$, $z_j$ should satisfy the following error constraint:
\begin{align}\label{eq:objective}
    \|z_j - \text{Attn}(q_j, K_j, V_j)\|_2\leq \varepsilon \left\|\text{softmax}\left(\frac{K_j\cdot q_j}{\sqrt{d}}\right)\right\|_2\|V_j\|_F.
\end{align}
A sublinear in $n$ time and memory algorithm to compute $z_j$ will require knowledge of significantly less key-value pairs than $K_j,V_j$, thus reducing the size of the KV cache needed to store them. In the next section we discuss how we will construct such an estimator $z_j$ at a high level.

\subsection{\textsc{SoftmaxBalance}: Attention Approximation via Discrepancy Theory}

We now start with presenting the main ideas of our approach. By the definition of softmax, \cref{eq:objective} can be written as
$$\text{Attn}(q_j, K_j, V_j) =\frac1{Z_j} \exp\left(\frac{K_j \cdot q_j}{\sqrt{d}}\right)^T\cdot V_j,$$ where for a matrix $A$ we write $\exp(A)$ to denote entry-wise application of the exponential function to $A$ and 
$$
Z_j=\sum_{i \in [j]}\exp\left(\frac{\langle k_i, q_j\rangle }{\sqrt{d}}\right).
$$
Our approach to approximate Attn$(q_j,K_j,V_j)$ consists of two subroutines which approximate:
\begin{enumerate}
\item Softmax normalization $Z_j=\sum_{i \in [j]} \exp\left(\frac{\langle k_i, q_j\rangle}{\sqrt{d}}\right)$,
\item Matrix-vector product between $V_j$ and $\exp\left(\frac{K_j \cdot q_j}{\sqrt{d}}\right)$.
\end{enumerate}
To understand our main idea, suppose we are at the end of the stream (i.e., $j=n$) and the KV cache contains all key-value pairs $(k_1, v_1), \ldots, (k_n, v_n)$. Then for an arbitrary query $q_n$ we aim to approximate the matrix-vector product
$$\exp\left(\frac{K_n \cdot q_n}{\sqrt{d}}\right)^T\cdot V_n = \sum_{i \in [n]} \exp\left(\frac{\langle k_i, q_n\rangle}{\sqrt{d}}\right)v_i$$ by choosing a subset of the rows of $K_n$ and $V_n$ of size at most $n/2$ which corresponds to a compression rate of $0.5$. Suppose we can design an algorithm which splits the set $C$ of all keys and values into two groups $C'$ and $C\backslash C'$ so that the matrix-vector product function for any query vector $q_n$ is roughly equal over $C'$ and $C\backslash C'$ that is informally,
$$\sum_{\{k, v\}\in C'}\exp\left(\frac{\langle k, q_n\rangle}{\sqrt{d}}\right)v \approx \sum_{\{k, v\}\in C\backslash C'}\exp\left(\frac{\langle k, q_n\rangle}{\sqrt{d}}\right)v.$$ Then, we are able to approximate the matrix-vector product function with either one of the sums above since informally:
$$\sum_{\{k, v\}\in C}\exp\left(\frac{\langle k, q_n\rangle}{\sqrt{d}}\right)v \approx 2\sum_{\{k, v\}\in C'}\exp\left(\frac{\langle k, q_n\rangle}{\sqrt{d}}\right)v.$$ Therefore, it would suffice to keep the smaller subset of $C'$ and $C\backslash C'$ as the desired subset of key value embeddings and discard the rest. If we wanted to compress the KV-cache to a smaller size by a factor $2^{T}$ for some $T$, we would recursively compress the selected subset using the same procedure $T-1$ more times.

A similar goal is captured by the vector balancing problem studied extensively in discrepancy theory - given a set of vectors $C=\{k_1, \dots, k_n \}\subset \mathbb{R}^d$ with $\|k_j\|_2\leq 1$ for all $j$, partition them into two groups $C',C\setminus C'$ such that for any $q\in \mathbb{R}^d$ it holds $\sum_{k\in C'}\langle k,q\rangle \approx \sum_{k \in C\setminus C'}\langle k,q \rangle$ with high probability. The Self-Balancing Walk algorithm~\cite{ALS21} is a breakthrough result which gives an algorithm for the above vector balancing problem. However we need to develop an algorithm for the vector balancing problem with respect to function $ \exp\left(\langle k, \cdot\rangle/\sqrt{d}\right)v$ instead of the inner product function $\langle k, \cdot \rangle$. 

Our first contribution is to develop an algorithm for our task, building upon the result from \cite{ALS21}, which essentially randomly partitions the set of keys and values $C$ into $C'$ and $C\setminus C'$ such that the following holds with high probability under the assumptions that the norms of the query and key embeddings are bounded, 
\begin{align*}
        \left\|\sum_{\{k, v\} \in C'}\exp\left(\frac{\langle k, q_n\rangle}{\sqrt{d}}\right)v  - \sum_{\{k, v\} \notin C'}\exp\left(\frac{\langle k, q_n\rangle}{\sqrt{d}}\right)v\right\|_2 \leq
        O\left(\log(nd)\right)
        \cdot\max_{j \in [n]}\|v_j\|_2.
\end{align*}
We refer to this algorithm as \hyperref[alg:BALANCE-V]{\hyperref[alg:BALANCE-V]{\textsc{SoftmaxBalance}}}, its formal guarantees are presented in \cref{thm:BALANCE-vectors} and its pseudocode is presented in \cref{alg:BALANCE-V}.  \cref{thm:BALANCE-vectors} shows that  \hyperref[alg:BALANCE-V]{\textsc{SoftmaxBalance}} succeeds to divide $C$ into subsets $C'$ and $C\backslash C'$ which are balanced with respect to function $ \exp\left(\langle k, \cdot\rangle/\sqrt{d}\right)v$ up to an error which only has logarithmic dependence on the size of $C$. In addition, \hyperref[alg:BALANCE-V]{\textsc{SoftmaxBalance}} can accept as input value vectors of arbitrary dimension $s$. Therefore, if instead of the value vectors $V = \{v_1, \ldots, v_n\}\subset \mathbb{R}^d$ we input the set of scalars $V = \{1, \ldots, 1\}$, we will get an algorithm for the vector balancing problem with respect to function $\exp\left(\langle k, \cdot\rangle/\sqrt{d}\right)$. This implies that we can use \hyperref[alg:BALANCE-V]{\textsc{SoftmaxBalance}} to compress the KV cache to even approximate the softmax normalization $\sum_{i \in [n]} \exp\left(\langle k_i, q_n\rangle/{\sqrt{d}}\right)$. 

We now discuss how to use \hyperref[alg:BALANCE-V]{\textsc{SoftmaxBalance}} for streaming attention approximation, that is to use its guarantees to compute an estimator $z_j$ satisfying \cref{eq:objective}.

\subsection{\textsc{BalanceKV}: Implementing \textsc{SoftmaxBalance} in Streaming for Cache Compression}
For a sequence of $n$ tokens and a given memory budget of $t \ll n$, we want to design a procedure which applies \hyperref[alg:BALANCE-V]{\textsc{SoftmaxBalance}} to compress the exact KV cache from storing $n$ key-value embeddings to at most $t$ in the streaming setting and can compute an estimator $z_j$ satisfying \cref{eq:objective} for all steps $j$ in the stream. In the streaming setting one needs to consider the following aspects. As described in the previous section, one iteration of \hyperref[alg:BALANCE-V]{\textsc{SoftmaxBalance}} only allows one to select a $n/2$ sized subset of $n$ key-value embeddings, which is higher than the desired budget of $t$ embeddings. This can be easily mitigated by recursively applying \hyperref[alg:BALANCE-V]{\textsc{SoftmaxBalance}} $2^{\log(n/t)}$ times, each time halving the set of key-value embeddings. However, this cannot be implemented in the streaming setting as we have a limited memory budget of $t$ which prohibits us from storing all key-value embeddings during recursion. 

To deal with this, we use the classical merge and reduce technique used in the design of streaming algorithms~\cite{BHMSSZ21, MG82, GLPW16}. 
\hyperref[alg:merge_reduce]{\textsc{MergeAndReduce}} algorithm is a recursive binary tree-based approach that allows one to implement \hyperref[alg:BALANCE-V]{\textsc{SoftmaxBalance}} recursively in a streaming setting with the total memory not exceeding $O^*(t)$, where $O^*(\cdot)$ supresses polynomial in $d,\log n$ factors, under the assumption that the norms of queries and keys are bounded. The guarantees of \hyperref[alg:merge_reduce]{\textsc{MergeAndReduce}} are presented in \cref{thm:MergeAndReduce}, its pseudocode in Algorithm \hyperref[alg:merge_reduce]{3} and a visual representation in Figure \ref{fig:merge_reduce}. If the norms of all value embeddings in the stream are the same up to constant factors, that is for all $i,j\in [n]$ $0.5\leq \|v_i\|_2/\|v_j\|_2\leq 2$, then the outputs of \hyperref[alg:merge_reduce]{\hyperref[alg:merge_reduce]{\textsc{MergeAndReduce}}} can be used to construct an estimator $z_j$ satisfying our attention approximation guarantee of equation \cref{eq:objective} with precision $\varepsilon$ for $t=O^*(1/\varepsilon)$. However, the value embeddings may have drastically different norms. 

Our main algorithm \hyperref[alg:main]{\hyperref[alg:main]{\textsc{BalanceKV}}} (pseudocode in \cref{alg:main}) deals with this issue by grouping the key-value embeddings in the stream according to the norms of the value embeddings, running a separate instance of \hyperref[alg:merge_reduce]{\textsc{MergeAndReduce}} on each group, and combining the outputs of each instance of \hyperref[alg:merge_reduce]{\textsc{MergeAndReduce}}. \hyperref[alg:main]{\textsc{BalanceKV}} constructs a final estimator $z_j$ satisfying equation \cref{eq:objective} with precision $\varepsilon$ only using $O^*( 1/\varepsilon)$ memory and $O^*(1/\varepsilon^2)$ runtime per every step $j$ of the stream, assuming the norms of query and key embeddings are bounded. Existing methods \cite{zandieh2024subgen} sub sample keys and values independently in the cache, and thus they have a $1/\varepsilon^2$ dependence on $\varepsilon$ in the total memory. The guarantees of \hyperref[alg:main]{\textsc{BalanceKV}} are presented in \cref{thm:main-theorem}.

\section{Main Theoretical Results}\label{sec:theory_main}

Our main algorithm for streaming attention approximation is \hyperref[alg:main]{\textsc{BalanceKV}}. It takes in as input a stream of $n$ tokens $(q_1, k_1, v_1), (q_2, k_2, v_2), \ldots, (q_n, k_n,v_n)$  and at every step of the stream outputs an estimate $z_j$ to Attn$(q_j,K_j,V_j)$ (see \cref{eq:attn-def} for the definition of Attn$(.)$) satisfying \cref{eq:objective} with precision $\varepsilon$. Assuming that the $\ell_2$ norms of $q_j,k_j$ are at most $r$ for all $j$, \hyperref[alg:main]{\textsc{BalanceKV}} uses total space $O^*(e^{2r^2/\sqrt{d}}\cdot 1/\varepsilon)$ and uses  $O^*(e^{4r^2/\sqrt{d}}\cdot 1/\varepsilon^2)$ runtime at each step $j$ of the stream to output $z_j$. Our main theorem is as follows.

\begin{theorem}\label{thm:main-theorem} For any $r, \varepsilon > 0$, any positive integers $n,d$, any set of tokens $(q_1, k_1, v_1), (q_2, k_2, v_2), \\ \ldots, (q_n, k_n,v_n)$ where $q_j, k_j, v_j \in \mathbb{R}^d$ satisfy $\|q_j\|_2, \|k_j\|_2 \leq r$ for all $j$, consider an invocation of \hyperref[alg:main]{\textsc{BalanceKV}}  with 
\begin{itemize}
    \item batch size $t = O^*\left(  (e^{2r^2/\sqrt{d}})/\varepsilon\right)$ 
    \item compression rate $2^{-T}$ with $T = \log(n/t)$.
\end{itemize} 
    Then \hyperref[alg:main]{\textsc{BalanceKV}} outputs a vector $z_j$ satisfying \cref{eq:objective} with probability at least $1 - 1/\text{poly}(n)$ at every step $j$ of the stream. It uses total memory $O^*\left(  e^{2r^2/\sqrt{d}}/\varepsilon\right)$ across all steps of the stream and runtime of $O^*\left(  e^{4r^2/\sqrt{d}}/\varepsilon^2\right)$ per step of the stream.
\end{theorem}
\hyperref[alg:main]{\textsc{BalanceKV}} is described in \cref{alg:main}. At its core 
\hyperref[alg:main]{\textsc{BalanceKV}} relies on our main discrepancy based algorithm, namely \hyperref[alg:BALANCE-V]{\textsc{SoftmaxBalance}}-- see Section \ref{sec:softmax_balance} for details on \hyperref[alg:BALANCE-V]{\textsc{SoftmaxBalance}}. \hyperref[alg:main]{\textsc{BalanceKV}} uses the output of \hyperref[alg:BALANCE-V]{\textsc{SoftmaxBalance}} to compute estimates of the numerator and denominator of Attn$(q_j,K_j,V_j)$ and returns  the desired attention approximation $z_j$ for each $j$-- see 
 line \hyperref[line:output]{16} in \hyperref[alg:main]{\textsc{BalanceKV}}. 
There are two subtleties, however. First, it is important to bucket tokens in the stream according to the norm of the value vectors -- see line \hyperref[line:bucket]{4}. Second, a direct application of \hyperref[alg:BALANCE-V]{\textsc{SoftmaxBalance}} would require too much space. To ensure small space usage, we apply a classical streaming technique, namely the 
\hyperref[alg:merge_reduce]{\textsc{MergeAndReduce}} algorithm on top of 
\hyperref[alg:BALANCE-V]{\textsc{SoftmaxBalance}} to reduce the space consumption. The space reduction achieved by \hyperref[alg:merge_reduce]{\textsc{MergeAndReduce}} is by running a logarithmic number of copies of \hyperref[alg:BALANCE-V]{\textsc{SoftmaxBalance}} in a tree-like fashion: see Section \ref{sec:merge_reduce} for details. 

To summarize, \hyperref[alg:main]{\textsc{BalanceKV}} groups tokens in the stream according to the norms of the corresponding value embeddings, runs a separate instance of $\hyperref[alg:merge_reduce]{\textsc{MergeAndReduce}}$ on each group, and combines the outputs of each instance to construct the final estimate for Attn$(q_j,K_j,V_j)$ at each step $j\in [n]$. Next we present 
\hyperref[alg:BALANCE-V]{\textsc{SoftmaxBalance}} and \hyperref[alg:merge_reduce]{\textsc{MergeAndReduce}}, and then give the proof of \cref{thm:main-theorem}.

\begin{algorithm}[t]
\caption{\textsc{BalanceKV$((q_j, k_j, v_j)_{j = 1}^n, r, t, T, \varepsilon)$}}\label{alg:main}
\begin{algorithmic}[1]
   \STATE{\bfseries input:} stream of $n$ embeddings $(q_j, k_j, v_j)$, diameter $r$, batch size $t$, compression rate $2^{-T}$, precision parameter $\varepsilon$.
   \vspace{2mm}
   \STATEx \textcolor{gray}{/* Bucket the stream and maintain $\log(n)$ instances of $\hyperref[alg:merge_reduce]{\textsc{MergeAndReduce}}$, $\textsc{MR-Numerator}_i$, for each bucket to approximate the numerator of Attn$(q_j, K_j, V_j)$; and one instance, $\textsc{MR-Denominator}$, to approximate the denominator of Attn$(q_j, K_j, V_j)$ */}
   \vspace{2mm}
   \STATE $v_{\text{max}} \leftarrow 0$
   \REPEAT 
   \STATEx ~~ \textcolor{gray}{/* Bucket the stream by $\|v\|_2$ */}
   \vspace{1mm}
   \STATE\label{line:bucket}Let $i$ be such that: $2^{i} \leq \|v_j\|_2 \leq 2^{i - 1}$ 
   \STATE \quad Send $(k_j, v_j)$ as input to $\textsc{MR-Numerator}_i$
   \vspace{2mm}
   
   \STATEx ~~ \textcolor{gray}{/* Erase small norm buckets */}
   \vspace{1mm}
   \IF{$\|v_j\|_2 \geq v_{\text{max}}$}
   \STATE $v_{\text{max}} \leftarrow \|v_j\|_2$ 
   \ENDIF
    \STATE Erase all $\textsc{MR-Numerator}_i$ with $2^i \leq \frac{\varepsilon}{2n} e^{-\frac{r^2}{\sqrt{d}}}v_{\text{max}}$
   \STATEx ~~ \textcolor{gray}{/* Combine the outputs of $\textsc{MR-Numerator}_i$ */}
   \vspace{1mm}
   \STATE $C^0_i, \ldots, C^T_i \leftarrow$ the output of $\textsc{MR-Numerator}_i$
   \FOR{$l = 0, \ldots, T$}
   \STATE $V^l \leftarrow \cup_i C^l_i$
   \ENDFOR
   \STATE\label{line:merge_reduce_denominator}Send $(k_j, 1)$ as input to $\textsc{MR-Denominator}$
   \STATE $K^0, \ldots K^T \leftarrow \textsc{MR-Denominator}$
   \STATE\label{line:output} \textbf{output $z_j$:} $z_j=\frac{\sum_{l = 0}^T 2^l\sum_{\{k, v\} \in V^l}\exp\left(\frac{\langle k, q_j\rangle}{\sqrt{d}}\right)v}{\sum_{l = 0}^T 2^l\sum_{\{k, v\} \in K^l} \exp\left(\frac{\langle k, q_j\rangle}{\sqrt{d}}\right)}$
   \STATE $j \leftarrow j+1$
   \UNTIL{token stream ends}
   \end{algorithmic}
   \end{algorithm}

\subsection{\textsc{SoftmaxBalance}}\label{sec:softmax_balance}

We now present our main discrepancy based compression algorithm \hyperref[alg:BALANCE-V]{\textsc{SoftmaxBalance}}. Given a sequence of key/value embeddings $C= \{(k_1,v_1),\ldots (k_n,v_n)\}$ (with key and value embeddings having possibly different dimensions), the goal of \hyperref[alg:BALANCE-V]{\textsc{SoftmaxBalance}} is to produce a partition of $C$ into subsets $C', C\setminus C'$ such that for any query $q\in \mathbb{R}^d$ we have that $\sum_{(k,v)\in C'}\exp\left(\langle k,q\rangle/\sqrt{d}\right)v \approx \sum_{(k,v)\in C\setminus C'}\exp\left(\langle k,q\rangle/\sqrt{d}\right)v$ with high probability. Without loss of generality assume that $|C'|\leq |C|/2$; we can output $2\sum_{(k,v)\in C'}\exp\left(\langle k,q\rangle/\sqrt{d}\right)v$ as an approximation to $\sum_{(k,v)\in C}\exp\left(\langle k,q\rangle/\sqrt{d}\right)v$, thus achieving a factor 2 compression.  Its description is presented in \cref{alg:BALANCE-V}. We note that while \hyperref[alg:BALANCE-V]{\textsc{SoftmaxBalance}} takes as input a sequence of key and value embeddings, it can nevertheless be used to compute the softmax normalization: we simply run it on the keys, with the corresponding value vector one-dimensional and all equal to $1$ -- see line \hyperref[line:merge_reduce_denominator]{14} of \hyperref[alg:main]{\textsc{BalanceKV}}, where \hyperref[alg:BALANCE-V]{\textsc{SoftmaxBalance}} is called within the corresponding invocation of \hyperref[alg:merge_reduce]{\textsc{MergeAndReduce}} with value vectors as all-ones vectors. The guarantees of \hyperref[alg:BALANCE-V]{\textsc{SoftmaxBalance}} are as follows. 

\begin{algorithm}[t]
\caption{$\textsc{SoftmaxBalance}((k_j, v_j)_{j}, r_{\text{key}}, r_{\text{value}}, \delta)$}\label{alg:BALANCE-V}
\begin{algorithmic}[1]
\STATE {\bfseries input:} stream of $\leq n$ key-value embeddings $(k_j, v_j)$, radii $r_{\text{key}}$, $r_{\text{value}}$: $\max_{j}\|k_j\|_2 \leq r_{\text{key}}$, $\max_{j}\|v_j\|_2 \leq r_{\text{value}}$, probability of failure $\delta$.
\STATE $R \leftarrow \exp(r_{\text{key}}^2/2\sqrt{d})\cdot r_{\text{value}}$
\STATE $c \leftarrow 30 \log (n/ \delta)$
\STATE Initialize zero vector $\eta \gets \{ 0 \}$
\FOR{$j$ from 1 and until the end of the stream}
\STATE  $y \gets \left(\exp(\langle k_i, k_j\rangle/\sqrt{d})\langle v_i, v_j\rangle\right)_{i \in [j]}$ 
\STATE {{\bf if} $\left| y^T\eta \right|>c\cdot R^2$ {\bf then }}FAIL
\STATE $p_j \leftarrow \frac{1}{2}-\frac{y^T\eta}{2 c \cdot R^2}$
\STATE $\eta_j \gets \left\{{\begin{array}{ll}
+1&\text{with probability} \,p_j\\
-1&\text{otherwise}\end{array}}\right.
$
\STATE Add a new zero coordinate $\eta_{j+1} \leftarrow 0$
\ENDFOR
\IF{$|\{ (k_i, v_i): \eta_i = 1 \}| \leq |\{ (k_i, v_i): \eta_i = -1 \}|$}
\STATE{\bfseries output:} $\{ (k_i, v_i): \eta_i = 1 \}$
\ELSE
\STATE{\bfseries output:} $\{ (k_i, v_i): \eta_i = -1 \}$
\ENDIF
\end{algorithmic}
\end{algorithm}


\begin{theorem}\label{thm:BALANCE-vectors}
 Given sets $K=\left\{k_1, \ldots, k_n\right\} \subset \mathbb{R}^d, V=\left\{v_1, \ldots, v_n\right\} \subset  \mathbb{R}^s$, and failure probability $\delta>0$, define $C$ to be the dataset of pairs $C = \{(k_1, v_1), \ldots, (k_n, v_n)\}$. There exists a randomized algorithm, \hyperref[alg:BALANCE-V]{\textsc{SoftmaxBalance}}, which outputs a subset $C' \subset C$, $|C'| \leq |C|/2$, such that, for any vector $q \in \mathbb{R}^d$, with probability at least $1-\delta$,
\begin{equation*}
    \begin{aligned}
        &\left\|\sum_{\{k, v\} \in C'}\exp\left(\frac{\langle k, q\rangle}{\sqrt{d}}\right)v  - \sum_{\{k, v\} \notin C'}\exp\left(\frac{\langle k, q\rangle}{\sqrt{d}}\right)v\right\|_2 \\ &\leq 
        O\left(\sqrt{s} \cdot\log (ns/ \delta)\cdot \exp\left(\frac{\|q\|^2_2}{2\sqrt{d}}\right)\cdot
        \exp\left(\max _{j\in[n]}\frac{\left\|k_j\right\|^2_2}{2\sqrt{d}}\right)\cdot\max_{j \in [n]}\|v_j\|_2\right).
    \end{aligned}
\end{equation*}

The runtime of \hyperref[alg:BALANCE-V]{\textsc{SoftmaxBalance}} is $O((d+s)n^2)$ and memory is $O((d+s)n)$.
\end{theorem}


The proof of the above theorem uses the breakthrough result of \cite{ALS21} for the vector balancing problem, one of the main problems in discrepancy theory. Given a set of vectors $k_1,\ldots, k_n$ the result of \cite{ALS21} produces a subset $C$ of these vectors of at most half the size such that for any vector $q$ we have that $\sum_{k\in C}\langle k,q\rangle \approx\sum_{k\in [n]\setminus C}\langle k,q\rangle$ with high probability. Our main contribution is an algorithm for the vector balancing problem with respect to the function $\exp(\langle k,\cdot \rangle/\sqrt{d}) v$ as compared to $\langle k,\cdot \rangle$ in the case of \cite{ALS21}. A full proof of \cref{thm:BALANCE-vectors} can be found in \cref{sec:appendix_softmax_balance}.

\subsection{\textsc{MergeAndReduce}}\label{sec:merge_reduce}

\begin{figure}
\centering
\includegraphics[width=1\textwidth]{images/example_plot_modified.png}
    \caption{Illustration of the tree structure of \textsc{MergeAndReduce}. It depicts the state of the algorithm after the last token in $C^0$ has arrived, before updating $C^0, \ldots, C^T$ with \textsc{SoftmaxBalance}. Whenever line \hyperref[line:Ci-update]{13} in \cref{alg:merge_reduce} is triggered for some $C^i$, we mark the next batch on its level of recursion as $C^i$, so that before each new token arrives $C^0, \ldots, C^{T-1}$ form a root to leaf path in one of the binary trees.}\label{fig:merge_reduce} 
\end{figure}

As briefly mentioned above in \cref{sec:theory_main}, \hyperref[alg:merge_reduce]{\textsc{MergeAndReduce}} is a streaming version of \hyperref[alg:BALANCE-V]{\textsc{SoftmaxBalance}}. The idea is to partition the stream of tokens into batches of size $t$, split the batches into pairs, apply \hyperref[alg:BALANCE-V]{\textsc{SoftmaxBalance}} to the batches, merge the outputs of \hyperref[alg:BALANCE-V]{\textsc{SoftmaxBalance}} on the batches in one pair, and then repeat recursively on the new set of batches $T-1$ times if the desired compression rate is $2^T$-- see \cref{fig:merge_reduce}. We run all the levels of recursion in parallel, which means that at any time step  we keep only one (possibly empty) batch per each level of recursion, and we refer to these batches as $C^0, \ldots, C^{T-1}$. We store the points which survived all the levels of recursion in the memory cell called $C^T$.

If we set batch size $t$ to be about $1/\varepsilon$ (see \cref{thm:MergeAndReduce} below for the more precise setting), we obtain a streaming algorithm that approximates $\sum_{i = 1}^j\exp\left(\frac{\langle k_i, q_j\rangle}{\sqrt{d}}\right)v_i$ at any point $j$ in the stream using total space $O^*\left(  e^{2r^2/\sqrt{d}}/\varepsilon\right)$ and runtime  $O^*\left(  e^{4r^2/\sqrt{d}}/\varepsilon^2\right)$ per step, where $r$ is an upper bound on the norms of key and query embeddings. 

As before, an important aspect is that \hyperref[alg:merge_reduce]{\textsc{MergeAndReduce}} can handle value embeddings of dimension not necessarily equal to that of key and query embeddings. Thus, when run on scalars $v_i=1$ for all $i$, it can also be used to approximate softmax normalization at any point $j$ in the stream. This is the main subroutine used in \hyperref[alg:main]{\textsc{BalanceKV}} to approximate Attn$(q_j,K_j,V_j)$. 
\begin{algorithm}
\begin{algorithmic}[1]
    \caption{\textsc{MergeAndReduce}$((k_j, v_j)_j, t, T, \varepsilon)$}\label{alg:merge_reduce}
    \STATE{\bfseries input:} stream of $\leq n$ tokens $(k_j, v_j)$, batch size $t$, compression rate $2^{-T}$, precision parameter $\varepsilon$.
    \STATE Let \hyperref[alg:BALANCE-V]{\textsc{SoftmaxBalance}} be the algorithm as per \cref{thm:BALANCE-vectors}.
    \STATE Initialize $i$-th level subset $C^i, i=0,\ldots, T,$ to empty
    \REPEAT 
    \STATE $C^0 \leftarrow C^0 \cup \{(k_j, v_j)\}$      
    \IF{$p$ is not a multiple of $t$ then}
    \STATE \textbf{output} $C^0, \ldots, C^T$ 
    \STATE \textbf{continue}
    \ENDIF
    \vspace{2mm}
    \STATEx ~~ \textcolor{gray}{/* Update subsets every $t$ steps */}
    \STATE $p\gets j/t$, $i\gets 0$ 
    \WHILE{$p$\text{~is an integer~}and \textbf{until} $i=T$}\label{line:while} 
    \STATE $C^{i+1} \leftarrow C^{i+1}\cup\text{\hyperref[alg:BALANCE-V]{\textsc{SoftmaxBalance}}}(C^i, r_{\text{key}}, r_{\text{value}}, 1/\text{poly}(n))$
    \STATE $C^i \leftarrow \emptyset$ \label{line:Ci-update}
    \STATE $i \gets i+1$
    \STATE $p\gets  p/2$
    \ENDWHILE
    \STATE \textbf{output} $C^0, \ldots, C^T$ 
    \UNTIL{token stream ends}
\end{algorithmic}
\end{algorithm}
The formal guarantees are given by
\begin{theorem}\label{thm:MergeAndReduce}
    For any $r, \varepsilon > 0$, any set of tokens $(q_1, k_1, v_1), \ldots, (q_n, k_n,v_n)$ where $q_j, k_j \in \mathbb{R}^d$ satisfy $\|q_j\|_2, \|k_j\|_2 \leq r$, $v_j \in \mathbb{R}^s$ for $s\leq d$ suppose,
\begin{itemize}
    \item batch size $t = O^*(e^{2r^2/\sqrt{d}}/{\varepsilon})$
    \item compression rate $2^{-T}$ with $T = \log(n/t)$.
\end{itemize} 
    Then \hyperref[alg:merge_reduce]{\textsc{MergeAndReduce}} on input parameters $t, r, d, s, \varepsilon$, outputs at every step $j$ of the stream subsets of key-value embedding pairs $C^0, \ldots, C^T \subset C := \{(k_1,v_1),\ldots,(k_n,v_n)\}$ such that,
    $$z_j := \sum_{i = 0}^T 2^i\sum_{\{k, v\} \in C^i}\exp\left(\frac{\langle k, q_j\rangle}{\sqrt{d}}\right)v,$$ satisfies with probability at least $1 - 1/\mathrm{poly}(n)$,
    $$\left\|\sum_{i = 1}^j\exp\left(\frac{\langle k_i, q_j\rangle}{\sqrt{d}}\right)v_i - z_j\right\|_2 \leq \varepsilon j\cdot e^{-r^2/\sqrt{d}}\cdot\max_{i \in [n]}\|v_i\|_2.$$
   

    The total memory of the algorithm is $O^*(e^{2r^2/\sqrt{d}}/{\varepsilon})$ and its runtime during the $j$-th iteration is $O^*(e^{4r^2/\sqrt{d}}/{\varepsilon^2})$
\end{theorem}

\begin{proof}  Note that since in the statement of the theorem $T = \log_2(n/t)$, condition \textbf{until} in line \hyperref[line:while]{11} of \hyperref[alg:merge_reduce]{\textsc{MergeAndReduce}} is redundant.

Let us first consider the performance of the procedure at time steps which are multiples of $t$. Observe that at any such $j$-th step the procedure is an online implementation of the following simple offline recursive algorithm on dataset $\{(k_1, v_1), \ldots, (k_j, v_j)\}$: 

\begin{enumerate}
    \item  Set $p = j/t$ and $i = 1$. Split the dataset $\{(k_1, v_1), \ldots, (k_j, v_j)\}$ into batches $B^0_1, \ldots, B^0_p$ of size $t$. 
    \item While $p > 1$:
    \begin{itemize}
        \item Run \hyperref[alg:BALANCE-V]{\textsc{SoftmaxBalance}} on the batches $B^{i - 1}_1, \ldots, B^{i -1}_p$ independently
        \item If $p$ is odd, store the output of \hyperref[alg:BALANCE-V]{\textsc{SoftmaxBalance}} on $B^{i - 1}_p$ in $C^i$
        \item For every $l$, merge the outputs of \hyperref[alg:BALANCE-V]{\textsc{SoftmaxBalance}} on $B^{i - 1}_{2l - 1}$ and $B^{i - 1}_{2l}$ into one batch and store them in $B^i_{l}$,
        \item Update $p \leftarrow \lfloor p/2\rfloor$, $i \leftarrow i+1$.
    \end{itemize} 
\end{enumerate} 

Therefore, we will analyze space complexity and performance guarantees of the above offline algorithm.

\paragraph{Probability of success.} Note that our algorithm performs correctly if each of the calls to \hyperref[alg:BALANCE-V]{\textsc{SoftmaxBalance}} produces small error on each of the queries $q$ (as in theorem \cref{thm:BALANCE-vectors}). Throughout the stream, we make $O(n/t)$ calls to \hyperref[alg:BALANCE-V]{\textsc{SoftmaxBalance}}, and we apply each to at most $n$ queries, so, it is enough to require that that all \hyperref[alg:BALANCE-V]{\textsc{SoftmaxBalance}} have failure probability parameter $\delta = 1/\text{poly}(n)$.

\paragraph{Space complexity.} Observe that after each iteration of step 2 the number of batches decreases by a factor of two. The maximum batch size is always bounded by $t$. This is because a batch $B_l$ at iteration $i$ of step 2 is a union of \hyperref[alg:BALANCE-V]{\textsc{SoftmaxBalance}}$(B_{2l -1})$ and \hyperref[alg:BALANCE-V]{\textsc{SoftmaxBalance}}$(B_{2l})$ for $B_{2l - 1}$ and $B_{2l}$ at iteration $i - 1$ of step 2, and \hyperref[alg:BALANCE-V]{\textsc{SoftmaxBalance}} reduces the size of the dataset which it has been applied to at least by a factor of 2.

The memory of the procedure is the collection of memory cells $C^i$, and $|C^i| \leq t$. Since at time step $j$ at most $\log_2(p) \leq \log_2(n/t)$ memory cells are occupied, the total memory is bounded by $O(dt\log_2(n/t)) = O(dtT),$ which, using the $O^*$ notation, is equal to $O^*(e^{2r^2/\sqrt{d}}/{\varepsilon})$.

\paragraph{Performance of the algorithm.} Define $B^i = \cup_l B^i_l$~-- the data points which remained in the batches after $i$ iterations of step 2. By triangle inequality, 
\begin{align*}
    &\left\|\sum_{i = 1}^{T} 2^i\sum_{\{k, v\} \in C^i}\exp \left(\frac{\langle k, q_j\rangle}{\sqrt{d}}\right)v- \sum_{i = 1}^j\right.\left.\exp\left(\frac{\langle k_i, q_j\rangle}{\sqrt{d}}\right)v_i \right\|_2 \\
     &\leq \sum_{i = 1}^{T}\left\|2^{i}\sum_{\{k, v\} \in B^{i}\cup C^{i}}\right.\left.\exp\left(\frac{\langle k, q_j\rangle}{\sqrt{d}}\right)v - 2^{i-1}\sum_{\{k, v\} \in B^{i-1}}  \exp\left(\frac{\langle k, q_j\rangle}{\sqrt{d}}\right)v\right\|_2\\
    &\leq \sum_{i = 1}^{T}2^{i-1}\left\|\sum_{\{k, v\} \in B^{i}\cup C^{i}}\right.\left.\exp\left(\frac{\langle k, q_j\rangle}{\sqrt{d}}\right)v-\sum_{\{k, v\} \in B^{i-1}\backslash(B^{i}\cup C^{i})}\exp\left(\frac{\langle k, q_j\rangle}{\sqrt{d}}\right)v\right\|_2.
\end{align*}

We will refer to the $i$-th summand on the right hand side as the error produced by the $i$-th iteration of step 2. At the $i$-th iteration of step 2 we apply \hyperref[alg:BALANCE-V]{\textsc{SoftmaxBalance}} to $p/2^{i-1}$ batches $B^{i-1}_1, B^{i-1}_2, \ldots$ of size $t$, and we save the outputs of \hyperref[alg:BALANCE-V]{\textsc{SoftmaxBalance}} in batches $C^{i}, B^{i}_1, B^{i}_2, \ldots$. Therefore, by \cref{thm:BALANCE-vectors} and triangle inequality, the error vector produced by the procedure at the $i$-th iteration of step 2 has $l_2$ norm bounded by 
\begin{equation*}
    \begin{aligned}
        O\left(2^{i-1}\cdot\sqrt{s}\cdot\log(sn)
        \cdot\left(\frac{p}{2^{i-1}}\right)\cdot \ e^{r^2/\sqrt{d}}\max_{l \in [n]}\|v_l\|_2\right)=
        O\left(\sqrt{s}\cdot\log(sn)\cdot \ p\cdot e^{r^2/\sqrt{d}}\max_{l \in [n]}\|v_l\|_2\right),
    \end{aligned}
\end{equation*} since the error parameter $\delta$ of all instances of \hyperref[alg:BALANCE-V]{\textsc{SoftmaxBalance}} is set to $1/\text{poly}(n)$. The $l_2$ norm of the total error of our procedure is bounded by
\begin{equation*}
    \begin{aligned}
        O\left(\sqrt{s}\cdot\log(sn)\cdot T\cdot p\cdot e^{r^2/\sqrt{d}}\max_{l \in [n]}\|v_l\|_2\right). 
    \end{aligned}
\end{equation*}
By definition, $p = j/t$. In order to ensure that the statement of the theorem is correct, the upper bound on the $l_2$ norm of the error vector of the procedure should be less than the desired error $e^{-r^2/\sqrt{d}}\cdot\max_{l \in [n]}\|v_l\|_2$:

\begin{equation*}
    \begin{aligned}
        O\left(\sqrt{s}\cdot\log(sn)\cdot T\cdot \frac{j}{t}\cdot e^{r^2/\sqrt{d}}\max_{l \in [n]}\|v_l\|_2\right) \leq 
        \varepsilon j\cdot e^{-r^2/\sqrt{d}}\cdot\max_{l \in [n]}\|v_l\|_2.
    \end{aligned}
\end{equation*}

And, if we set
\begin{align*}
    t = O\left(\frac{\log^{2}(sn)\cdot\sqrt{s}\cdot e^{2r^2/\sqrt{d}}}{\varepsilon}\right),
\end{align*}
the above inequality holds. In the $O^*$ notation, since $s \leq d$, $t = O^*(e^{2r^2/\sqrt{d}}/{\varepsilon})$. 

\paragraph{Runtime during one time step.} At worst, during $j$-th time step the algorithm has to launch \hyperref[alg:BALANCE-V]{\textsc{SoftmaxBalance}} $\log_2(p) \leq \log_2(n/t) = T$ times on batches of size $t$, so the runtime is bounded by $O(dt^2T)$. In the $O^*$ notation, the runtime is equal to $O^*(e^{4r^2/\sqrt{d}}/\varepsilon^2).$

As the final step, we will analyze the performance of the procedure at time steps $j'$ which are not multiples of $t$. Define $j_t = \lfloor j'/t\rfloor\cdot t$. Note that at any such time step the procedure simply saves the triplet $(q_{j'}, k_{j'}, v_{j'})$ and outputs the sum of the approximation $z_{j_t}$ such that

$$\left\|\sum_{i = 1}^{j_t}\exp\left(\frac{\langle k_i, q_{j'}\rangle}{\sqrt{d}}\right)v_i - z_{j_t}\right\|_2 \leq \varepsilon j_t\cdot e^{-r^2/\sqrt{d}}\cdot\max_{i \in [n]}\|v_i\|_2,$$

and $\sum_{i = j_t+1}^{j'}\exp\left(\frac{\langle k_i, q_{j'}\rangle}{\sqrt{d}}\right)v_i$. From the above inequality, 

\begin{equation*}
    \begin{aligned}
        \left\|\sum_{i = 1}^{j'}\exp\left(\frac{\langle k_i, q_{j'}\rangle}{\sqrt{d}}\right)v_i - \left(z_{j_t}+\sum_{i = j_t+1}^{j'}\exp\left(\frac{\langle k_i, q_{j'}\rangle}{\sqrt{d}}\right)v_i\right)\right\|_2 \leq \varepsilon j_t\cdot e^{-r^2/\sqrt{d}}\cdot\max_{i \in [n]}\|v_i\|_2,
    \end{aligned}
\end{equation*}
as desired.
\end{proof}


\subsection{Proof of \cref{thm:main-theorem} (Theoretical Guarantee of \hyperref[alg:main]{\textsc{BalanceKV}})}
Finally equipped with \cref{thm:BALANCE-vectors} and \cref{thm:MergeAndReduce} we now state the proof of \cref{thm:main-theorem}.
\begin{proof}[Proof of \cref{thm:main-theorem}] 
First, we upper bound the number of procedures $\textsc{MR-Numerator}_i$ which \cref{alg:BALANCE-V} runs at any moment. The algorithm never stores $\textsc{MR-Numerator}_i$ for $i > \log_2(v_{\text{max}})$ and $i < \log_2(\varepsilon\cdot (2n)^{-1}\cdot e^{-r^2/\sqrt{d}}\cdot v_{\text{max}})$, so it stores at most $\log_2(2n\cdot e^{r^2/\sqrt{d}}/\varepsilon) = O(\log(n))$ copies in total. 

\cref{alg:BALANCE-V} (\hyperref[alg:main]{\textsc{BalanceKV}}) at any step $j$ performs one iteration of procedure $\textsc{MR-Denominator}$, one iteration of $\textsc{MR-Numerator}_i$ with $2^{i} \leq \|v_j\|_2 \leq 2^{i - 1}$, computes the subsets $K_1, \ldots, K_T, V_1, \ldots, V_T$ and computes a function of the selected points in the subsets in \hyperref[line:output]{line}. Therefore, the runtime of \hyperref[alg:main]{\textsc{BalanceKV}} during one iteration is bounded by the maximum of the runtime of \hyperref[alg:merge_reduce]{\textsc{MergeAndReduce}} during one iteration and time to compute the \hyperref[line:output]{output} (which is equal to its total memory). This maximum, by definition of $t$, is equal to $O^*(e^{4r^2/\sqrt{d}}/\varepsilon^2)$. The memory of the algorithm is the union of memory of $\textsc{MR-Numerator}_i$ for all $i$ and $\textsc{MR-Denominator}$, so the space complexity of the algorithm is $O^*(e^{2r^2/\sqrt{d}}/\varepsilon)$. The failure probability is bounded by union bounding the failure probabilities of all instances of \hyperref[alg:merge_reduce]{\textsc{MergeAndReduce}} and at most $n$ queries in the stream, and is equal to $1/\text{poly}(n)$.

\paragraph{Performance of the algorithm.} At time step $j$, procedure $\textsc{MR-Denominator}$ returns subsets $K_1, \ldots, K_T$ such that
\begin{align*}
\left|\sum_{i \in [j]}\exp\left(\frac{\langle k_i, q_j\rangle}{\sqrt{d}}\right) - \sum_{l = 0}^T 2^l\sum_{\{k, v\} \in K^l} \exp\left(\frac{\langle k, q_j\rangle}{\sqrt{d}}\right)\right| \leq \varepsilon \cdot j\cdot e^{-r^2/\sqrt{d}} \leq \varepsilon\cdot \sum_{i \in [j]}\exp\left(\frac{\langle k_i, q_j\rangle}{\sqrt{d}}\right).
\end{align*}

Define $P_{i,j} = \{\{k_l, v_l\}: l\leq j, 2^{i} \leq \|v_l\|_2 \leq 2^{i - 1}\}$. $\textsc{MR-Numerator}_i$ returns subsets $V_i^1, \ldots, V_i^T \subseteq P_{i,j}$ for all $i$ such that,
\begin{equation*}
    \begin{aligned} 
     &\left\|\sum_{\{k, v\} \in P_{i, j}}
    \exp\left(\frac{\langle k, q_j\rangle}{\sqrt{d}}\right)v -
    \sum_{l = 0}^T 2^l\sum_{\{k, v\} \in V_i^l} \exp\left(\frac{\langle k, q_j\rangle}{\sqrt{d}}\right)v\right\|_2\\
    &\leq\varepsilon |P_{i, j}|\cdot e^{-r^2/\sqrt{d}} \cdot 2^{i - 1}\leq
    \frac{\varepsilon|P_{i, j}|\cdot2^{i - 1}}{\sqrt{j}}\cdot\left\|\exp\left(\frac{ K_j\cdot q_j}{\sqrt{d}}\right)\right\|_2.
    \end{aligned}
\end{equation*}

Now, observe that $\|V_j\|_F \leq \sqrt{\sum_{i}|P_{i, j}|\cdot 2^{2(i - 1)}}$, $j = \sum_{i}|P_{i, j}|$. By the Cauchy-Schwartz inequality, 
$$\sum_{i}|P_{i, j}|\cdot 2^{i - 1} \leq \sqrt{\sum_{i}|P_{i, j}|\cdot 2^{2(i - 1)}} \cdot \sqrt{\sum_{i}|P_{i, j}|}.$$
By triangle inequality, the sum of the outputs of procedures $\textsc{MR-Numerator}_i$ approximates $\exp\left(\frac{K_j \cdot q_j}{\sqrt{d}}\right)^T\cdot V_j$ up to an additive error
\begin{align*}
\frac{\varepsilon}{\sqrt{j}}\cdot \left\|\exp\left(\frac{ K_j\cdot q_j}{\sqrt{d}}\right)\right\|_2\sum_{i}|P_{i, j}|\cdot 2^{i - 1}\leq \varepsilon\cdot\left\|\exp\left(\frac{ K_j\cdot q_j}{\sqrt{d}}\right)\right\|_2\|V_j\|_F,
\end{align*}
therefore we get
\iffalse
\begin{equation*}
\resizebox{0.45\textwidth}{!}{%
    $\begin{aligned}
      &\left\|\frac{\sum_{l = 0}^T 2^l\sum_{\{k, v\} \in V^l}\exp\left(\frac{\langle k, q_j\rangle}{\sqrt{d}}\right)v}{\sum_{l = 0}^T 2^l\sum_{\{k, v\} \in K^l} \exp\left(\frac{\langle k, q_j\rangle}{\sqrt{d}}\right)} - \text{softmax}\left(\frac{K_j\cdot q_j}{\sqrt{d}}\right)^T\cdot \ V_j\right\|_2 \leq\\
        &\frac{2\varepsilon}{1 - \varepsilon}\left\|\text{softmax}\left(\frac{K_j\cdot q_j}{\sqrt{d}}\right)\right\|_2\cdot\|V_j\|_F.
    \end{aligned}$%
    }
\end{equation*}
\fi
\begin{align*}
\norm{\frac{\displaystyle \sum_{l = 0}^T 2^l\sum_{\{k, v\} \in V^l}\exp\left(\frac{\langle k, q_j\rangle}{\sqrt{d}}\right)v}{\displaystyle \sum_{l = 0}^T 2^l\sum_{\{k, v\} \in K^l} \exp\left(\frac{\langle k, q_j\rangle}{\sqrt{d}}\right)} - \text{Attn}(q_j, K_j, V_j)}_2 \leq \frac{2\varepsilon}{1 - \varepsilon}\left\|\text{softmax}\left(\frac{K_j\cdot q_j}{\sqrt{d}}\right)\right\|_2\norm{V_j}_F.
\end{align*}


By rescaling $\varepsilon \to \varepsilon/3$, we get the desired approximation \cref{eq:objective}.
\end{proof}

\subsection{Proof of \cref{thm:BALANCE-vectors} (Theoretical Guarantee of \hyperref[alg:BALANCE-V]{\textsc{SoftmaxBalance}})}\label{sec:appendix_softmax_balance}
Algorithm Self-Balancing Walk introduced in \cite{ALS21} receives as input vectors $u_1, \ldots, u_n$ and selects signs for them so that, for any direction, the signed sum of the vectors is balanced along that direction with high probability. The following theorem readily follows from Theorem 1.1 in \cite{ALS21}:

\begin{theorem}[Theorem 1.1 in \cite{ALS21}]\label{thm:BALANCE-ALS21}
 For any $n, d \in \mathbb{N}$, there exists a randomized algorithm which receives as input a set of vectors $U=\left\{u_1, \ldots, u_n\right\} \in \mathbb{R}^d$ and a parameter $\delta>0$. The algorithm outputs a (random) subset $U'\subset U$ such that, for any vector $u \in \mathbb{R}^d$, with probability at least $1-\delta$,
$$
\left|\sum_{i \in U'}\left\langle u_i, u\right\rangle-\sum_{i \notin U'}\left\langle u_i, u\right\rangle\right| \leq O\left(\log (n / \delta) \cdot\max _{i \in[n]}\left\|u_i\right\|_2\cdot\|u\|_2\right).
$$
\end{theorem}

\begin{algorithm}\caption{$\text{Self-Balancing Walk}\left((u_j)_j, r, \delta\right)$}\label{alg:BALANCE}
\begin{algorithmic}[1]
\STATE {\bfseries input:} stream of $\leq n$ vectors $u_j$, radius $r$: $\max_{j}\|u_j\|_2 \leq r$, probability of failure $\delta$.
\STATE $c \leftarrow 30 \log (n/ \delta)$
\STATE $U_{-},U_{+} \leftarrow \emptyset$
\FOR{$i$ from 1 and until the end of the stream}
\IF{$\left|\sum_{u \in U_+}\langle u, u_i\rangle - \sum_{u \in U_-}\langle u, u_i\rangle\right|>c\cdot r^2$}
    \STATE Fail
    \ENDIF
\STATE $p_i \leftarrow \frac{1}{2}-\frac{\sum_{u \in U_+}\langle u, u_i\rangle - \sum_{u \in U_-}\langle u, u_i\rangle}{2 c \cdot r^2}$
\STATE $\varepsilon_i \gets \left\{{\begin{array}{ll}
+1&\text{with probability} \,p_i\\
-1&\text{otherwise}\end{array}}\right.
$
\STATE $U_{\varepsilon_i} \leftarrow U_{\varepsilon_i}\cup \{k_i\}$
\ENDFOR
\IF{$|U_{+}| \leq |U_{-}|$}
\STATE{\bfseries output:} $U_{+}$
\ELSE
\STATE{\bfseries output:} $U_{-}$
\ENDIF
\end{algorithmic}
\end{algorithm}

\begin{proof}[Proof of \cref{thm:BALANCE-vectors}]
Define for any $k \in \mathbb{R}^d$ an embedding function $\varphi(k):$
$$
\varphi(k)=\left( \frac{(k/d^{0.25})^{\otimes  i}}{\sqrt{i!}} \right)_{i\geq 0}.
$$
 It is easy to see that for any two vectors $k, q \in \mathbb{R}^d$
$$\langle \varphi(k), \varphi(q) \rangle = \exp\left(\frac{\langle k, q\rangle}{\sqrt{d}}\right),$$

and for any $k \in \mathbb{R}^d$ $$\|\varphi(k)\|^2_2 = \exp\left(\frac{\|k\|^2_2}{\sqrt{d}}\right).$$

Consider the set of vectors $\varphi(k_1) \otimes v_1, \varphi(k_2) \otimes v_2, \ldots$. Run the Self-Balancing Walk algorithm on the set of vectors $\varphi(k_1) \otimes v_1, \varphi(k_2) \otimes v_2, \ldots$ with failure parameter set to $\delta/s$ and denote by $C'$ and $C\backslash C'$ the partition of $C$ returned by the algorithm. Observe that, even though vectors $\varphi(k_i) \otimes v_i$ are infinite dimensional, Self-Balancing Walk still can be implemented. The algorithm never has to keep these vectors in the memory because the only operation which requires the knowledge of the embeddings~-- the inner product~-- can be performed if we just store vector pairs $\{k_i, v_i\}$:

$$\langle \varphi(k_i) \otimes v_i, \varphi(k_j) \otimes v_j\rangle = \exp\left(\frac{\langle k_i, k_j\rangle}{\sqrt{d}}\right)\cdot\langle v_i, v_j\rangle.$$ 

Denote by $e_1, \ldots, e_s$ the standard orthonormal basis in $\mathbb{R}^s$. By \cref{thm:BALANCE-ALS21}, for any $i \in [s]$ with probability $1 - \delta/s$
\begin{align}\label{eq:i-th coordinate}
    &\left|\sum_{\{k,v\} \in C'}\langle \varphi(k)\otimes v, \varphi(q)\otimes e_i\rangle-\sum_{\{k, v\} \notin C'}\langle \varphi(k)\otimes v, \varphi(q)\otimes e_i\rangle\right| \\
    &\leq O\left(\log (ns/\delta)\cdot\max_{\{k, v\} \in C}\left\|\varphi(k)\otimes v\right\|_2\cdot\|\varphi(q)\otimes e_i\|_2\right),
\end{align}

and so with probability at least $1 - \delta$ all of the above inequalities hold simultaneously.

To simplify the right hand side, notice that $\left\|\varphi(k)\otimes v\right\|_2 =  \exp\left(\frac{\|k\|^2_2}{2\sqrt{d}}\right)\cdot\|v\|_2$ and $\|\varphi(q)\otimes e_i\|_2 = \exp\left(\frac{\|q\|^2_2}{2\sqrt{d}}\right)$. Observe that for any $i$ it holds that $\langle \varphi(k)\otimes v, \varphi(q)\otimes e_i\rangle = \langle\varphi(k), \varphi(q)\rangle \cdot [v]_i = \exp\left(\frac{\langle k, q\rangle}{\sqrt{d}}\right)\cdot[v]_i$ , where we denote the $i$-th coordinate of the vector $v$ by $[v]_i$. Therefore, the left hand side of the expression above is simply the absolute value of the $i$-th coordinate of the vector $$\sum_{\{k, v\} \in C'}\exp\left(\frac{\langle k, q\rangle}{\sqrt{d}}\right)v  - \sum_{\{k, v\} \notin C'}\exp\left(\frac{\langle k, q\rangle}{\sqrt{d}}\right)v.$$
Thus, \cref{eq:i-th coordinate} provides a uniform upper bound on the absolute values of coordinates of the above vector.
Since the $l_{\infty}$ norm of a vector is the maximum of the absolute values of its coordinates, 

\begin{equation*}
    \begin{aligned}
        &\left\|\sum_{\{k, v\} \in C'}\exp\left(\frac{\langle k, q\rangle}{\sqrt{d}}\right)v  - \sum_{\{k, v\} \notin C'}\exp\left(\frac{\langle k, q\rangle}{\sqrt{d}}\right)v\right\|_{\infty}\\ 
        &= \max_{i \in [s]}\left|\sum_{\{k,v\} \in C'}\langle \varphi(k)\otimes v, \varphi(q)\otimes e_i\rangle-\sum_{\{k, v\} \notin C'}\langle \varphi(k)\otimes v, \varphi(q)\otimes e_i\rangle\right| \\
         &\leq O\left(\log (ns/ \delta) \cdot\max_{\{k, v\} \in C}\left(\exp\left(\frac{\|k\|^2_2}{2\sqrt{d}}\right)\cdot\|v\|_2\right)\cdot \exp\left(\frac{\|q\|^2_2}{2\sqrt{d}}\right)\right).
    \end{aligned}
\end{equation*}
Finally, we go from bounding the $l_{\infty}$ norm a vector to bounding its $l_2$ norm:

\begin{equation*}
    \begin{aligned}
        &\left\|\sum_{\{k, v\} \in C'}\exp\left(\frac{\langle k, q\rangle}{\sqrt{d}}\right)v  - \sum_{\{k, v\} \notin C'}\exp\left(\frac{\langle k, q\rangle}{\sqrt{d}}\right)v\right\|_{2} \\
        &\leq \sqrt{s}\left\|\sum_{\{k, v\} \in C'}\exp\left(\frac{\langle k, q\rangle}{\sqrt{d}}\right)v  - \sum_{\{k, v\} \notin C'}\exp\left(\frac{\langle k, q\rangle}{\sqrt{d}}\right)v\right\|_{\infty} \\
         &\leq O\left(\sqrt{s}\cdot\log (ns/ \delta) \cdot\max_{\{k, v\} \in C}\left(\exp\left(\frac{\|k\|^2_2}{2\sqrt{d}}\right)\cdot\|v\|_2\right)\cdot \exp\left(\frac{\|q\|^2_2}{2\sqrt{d}}\right)\right).
    \end{aligned}
\end{equation*}

This completes the proof of \cref{thm:BALANCE-vectors}.
\end{proof}

\def\llama{}


\def\llama{$\mathtt{Llama}$-$\mathtt{3.1}$-$\mathtt{8B}$-$\mathtt{Instruct}$}

\section{Experiments}\label{sec:experiments_main}

Experiments in \cref{sec:single-layer} are performed on a single NVIDIA A100 GPU with 40GB VRAM, and in \cref{sec:end-to-end} on a single NVIDIA RTX A6000 GPU with 48GB VRAM. 

To enhance the practical performance of our algorithm, we implement \hyperref[alg:main]{\textsc{BalanceKV}} with parallel operations. Specifically, we consider the cache embeddings of length $n$ and dimension $d$ as a sequence of blocks with length $b$ and reshape them into a tensor of shape $b \times (n/b) \times d$ . Then, \hyperref[alg:main]{\textsc{BalanceKV}} is applied in parallel to all blocks of length $b$. For cases where $n$ is not divisible by $b$, we pad the embeddings with zeros. After sign assignment to all embeddings in each block (i.e., line 9 in \cref{alg:BALANCE-V}), it is reshaped to its original length, and we strictly select $n/2$ embeddings, repeating this process for $T$ iterations.

\subsection{Single Layer Attention Approximation} \label{sec:single-layer}
We first empirically evaluate the performance of \hyperref[alg:main]{\textsc{BalanceKV}} for approximating a single attention layer, and compare it with independent uniform sampling. We use the pretrained \llama{}~\cite{dubey2024llama} and \mistral~\cite{mistral} and TriviaQA dataset from LongBench~\cite{bai2023longbench}, and consider the $1^{st}$,$2^{nd}$ and $5^{th}$ layers of the models for attention approximation.


For given a prompt with length $n$, we store the corresponding query, key, and value embeddings for all layers. Denote a pair of embeddings in some layer by $(q_1,k_1,v_1),\ldots,(q_n,k_n,v_n)$ and the goal is to approximate the attention $\text{Attn}(q_j, K_j, V_j)$ for the latest $256$ queries, i.e. $j\in [n-256,n]$. Specifically, we keep several first and recent tokens separately and apply \hyperref[alg:main]{\textsc{BalanceKV}} to the intermediate row vectors in $K_j$. This is motivated by \cite{xiao2023efficient} as important contexts are likely contained in the first and latest tokens. We retain the first 256 embeddings and the recent ones from $n-256$ to $j$ and our compressed cache contains tokens whose indices are in $[256] \cup S \cup \{n-256, \dots, j\}$ where $S \subseteq [257, n-256]$ can be obtained from \hyperref[alg:main]{\textsc{BalanceKV}}. We explore four compression parameters $T \in \{1,2,3,4\}$ which reduces the cache memory by a factor of $2^{-T}$. Let $z_j$ be our approximation using \hyperref[alg:main]{\textsc{BalanceKV}} plus the recent and first few embeddings at the stream $j\in [n-256,n]$. We compute relative errors $\|z_j-\text{Attn}(q_j, K_j, V_j)\|_F/\|\text{Attn}(q_j, K_j, V_j)\|_F$ for all $j\in [n-256,n]$, batches, heads and input prompts in the dataset. We repeat this with 10 different random seeds and compute their average and standard deviations. 

We also compare our method to independent uniform sampling, in which we replace the application of \hyperref[alg:main]{\textsc{BalanceKV}} with  sampling a $2^{-T}$  fraction of key and value embeddings with indices in $[257,n-256]$ uniformly at random. We report the results for layers 1,2 and 5 of \llama{} and \mistral{} in Fig. \ref{fig:single_layer_triviaqa}. 
We observe that for each model, layer and compression rate, the error of our method is much lower than independent uniform sampling. This verifies that \hyperref[alg:main]{\textsc{BalanceKV}} has a better error bound empirically, confirming the prediction provided by \cref{thm:main-theorem}. 
 
\begin{figure}[t!]
    \centering
    \begin{subfigure}
        \centering
        \includegraphics[width=0.45\textwidth]{images/llama_triviaqa.png}
    \end{subfigure}
    \begin{subfigure}
        \centering
        \includegraphics[width=0.45\textwidth]{images/triviaqa_mistral.png} 
    \end{subfigure}
    \caption{Comparison of relative errors across different layers of \llama{} (first figure on top) and \mistral{} (second figure on the bottom) on TriviaQA dataset.}
    \label{fig:single_layer_triviaqa}
\end{figure}

\iffalse
For a given dataset and input from the dataset, assume it has $n$ tokens after tokenization using the corresponding model tokenizer. We perform a forward pass on these $n$ input tokens using the model and for each layer, batch and head, we obtain the query, key and value embedding pairs and denote them by $(q_1,k_1,v_1),\ldots,(q_n,k_n,v_n)$. Now for various compression parameters $T =\{1,2,3,4\}$ (the memory reduction factor for a $T$ is $2^{-T}$ which for the values of the $T$ is in the set $\{0.5,0.25,0.125,0.0625\}$ respectively) we run our Algorithm \cref{alg:main} on the stream of tokens, which at any point in the stream $j\in [n]$ returns an approximation $z_j$ to $\text{Attn}(q_j, K_j, V_j) = \text{softmax}\left(\frac{K_j\cdot q_j}{\sqrt{d}}\right)^T\cdot V_j$. We then compute an attention approximation using uniform sampling by selecting a $2^{-T}$ fraction of keys and values in $K_j,V_j$, i.e. let $\widehat{K}_j,\widehat{V}_j$ be the subset of uniformly sampled keys and values of size $j/2^T$ and let $\widehat{z}_j=\text{Attn}(q_j,\widehat{K}_j,\widehat{V}_j)$. Let $\delta = \|z_j-\text{Attn}(q_j, K_j, V_j)\|_F/\|\text{Attn}(q_j, K_j, V_j)\|_F$ and $\widehat{\delta} =\|\widehat{z}_j-\text{Attn}(q_j, K_j, V_j)\|_F/\|\text{Attn}(q_j, K_j, V_j)\|_F$ be the relative errors of the approximate attentions corresponding to our algorithm and uniform sampling. We then compute the average of $\delta,\widehat{\delta}$ over all batches, heads and examples in the dataset for a fixed layer, and finally we repeat and report the average and standard deviations of these averaged relative errors over 10 different random seeds. 
\fi


 
\subsection{End-to-end Evalution on LongBench} \label{sec:end-to-end}
We benchmark our algorithm on LongBench~\cite{bai2023longbench}, a comprehensive collection of datasets designed to evaluate the long-context understanding capabilities of large language models. The benchmark consists of various long-text application scenarios, including single-document question-answering, multi-document question-answering, summarization, few-shot learning, synthetic tasks and code completion (Code). In particular, we use the LongBench-E split, which consists of datasets with uniform length distribution.

Our approach compresses the KV cache while generating in the prefill stage but maintains all streamed embeddings $(q_j, k_j, v_j)$ during generation. This is because the number of generated tokens is much smaller than the input sequence length. We set $b=256$ and $T=2$, achieving a consistent compression rate of $0.25$ across all inputs.

We evaluate our method against several token-level KV cache compression schemes, including StreamingLLM~\cite{xiao2023efficient}, SnapKV~\cite{li2024snapkv}, and PyramidKV~\cite{cai2024pyramidkv} as well as uniform sampling described in \cref{sec:single-layer}.
We use their implementations written in MInference~\cite{jiang2024minference}\footnote{\url{https://github.com/microsoft/MInference/}}, and configure their hyperparameters to match a uniform compression rate with $0.25$. We follow the same evaluation metrics from~\cite{bai2023longbench}. We test them on \llama{} as before. The results are summarized in \cref{tab:performance_comparison}. 

Our method achieves the highest average scores among all benchmark approaches. Notably, it outperforms summarization tasks by a large margin, e.g., 23.82 against others except for the uniform sampling. This result justifies that a subset obtained from discrepancy theory has practical impacts on various LLM tasks. 

\setlength{\tabcolsep}{7pt}
\begin{table}
\caption{Performance evaluation on LongBench tasks with \llama{} across various token-level KV cache compression methods. A higher score is better.}
\vspace{0.1in}
\def\arraystretch{1.1}%
\centering
\scalebox{0.9}{
\begin{tabular}{lccccccc}
\toprule
\multirow{ 2}{*}{Method} & \multicolumn{6}{c}{Task} & \multirow{ 2}{*}{Average}\\
  & Single-Doc QA & Multi-Doc QA & Summarization & Few-shot & Synthetic & Code &  \\
\midrule
Exact (Baseline) & 45.71 & 45.32 & 26.69 & 68.62 & 59.25 & 46.17 & 48.63 \\
\midrule
PyramidKV & 36.80 & 41.54 & 18.91 & 64.88 & 59.68 & 42.38 & 44.03 \\
SnapKV & 38.23 & 42.61 & 19.07 & 64.65 & 59.60 & 43.28 & 44.57 \\
StreamingLLM & 27.86 & 39.05 & 20.70 & 66.16 & 58.37 & 42.91 & 42.51 \\
Uniform & 37.17 & 42.66 & 24.31 & 65.09 & 58.30 & 41.40 & 44.82 \\
\hyperref[alg:main]{\textsc{BalanceKV}} & 36.97 & 42.37 & 23.82 & 65.49 & 58.46 & 42.82 & {\bf 44.99} \\
\bottomrule
\end{tabular}
}
\label{tab:performance_comparison}
\end{table}

\section{Conclusion}
In this paper we present \hyperref[alg:main]{\textsc{BalanceKV}}, an algorithm with provable guarantees for reducing the memory complexity of KV cache in modern LLMs. \hyperref[alg:main]{\textsc{BalanceKV}} uses a geometric correlated sampling process based on discrepancy theory to recursively compress the KV cache and it provably approximates attention in the streaming setting. We provide empirical evidence to show that our approach leads to lower relative error for single layer attention approximation for various open source LLMs including \llama{} and \mistral{}, as well as better performance on the LongBench benchmark for long context understanding capabilities of LLMs under various end to end tasks compared to heuristic approaches.

\nocite{ALS21}
 
\bibliographystyle{alpha}
\bibliography{main}

\end{document}
