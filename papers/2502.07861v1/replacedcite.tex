\section{Related Works}
\label{sec:related_works}
The direct approach to KV caching stores all key and value representations for every input token, leading to significant memory growth in long sequences, but several algorithmic ideas for improvement have been developed in the literature.

\paragraph{Embedding quantization.} A simple yet effective approach to reducing KV cache size is quantizing the floating-point numbers (FPN) in the KV cache to fixed-point numbers with fewer bits. 
Several quantization methods have been proposed specifically for the KV cache____.
However, such approaches still require memory space that linearly increases in sequence length. 

\paragraph{Token level pruning.} A prominent line of work which is related to our paper focuses on token-level KV cache  pruning where redundant or less important tokens get evicted from the cache____.
Many of the works in this line have used accumulated attention scores to select important tokens in the cache____. 
Recent works extend those methods to an adaptive way of budget allocation across layer____ and head____.


A very recent work, Lexico____, applies techniques from sparse representation learning to compress the KV cache by learning a universal dictionary such that all key and value embeddings are represented as extremely sparse vectors within the learned dictionary. 
Unfortunately, this approach requires solving a computationally expensive matching pursuit algorithm for each key and value embedding, making Lexico relatively slow.

\paragraph{Algorithmic discrepancy theory.}
Banaszczyk's seminal works____ establishing theoretical guarantees for vector set discrepancy have sparked research in the vector balancing problem____. This led to algorithmic developments in both offline ____ and online ____ settings. The vector balancing problem has particular relevance to streaming and sublinear algorithms, as minimizing a dataset's discrepancy yields small subsets that effectively preserve the original dataset's properties. Recent works____ extend these discrepancy theory ideas to develop sublinear memory algorithms for \emph{kernel density estimation}.