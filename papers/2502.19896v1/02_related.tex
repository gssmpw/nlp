\section{Related Work}
\label{sec:related}
\subsection{Point cloud completion}
% Recent years have seen significant progress in point cloud completion. 
Early methods ~\cite{3depn,han2017high,varley2017shape,xie2020grnet} primarily used voxels as intermediate representations and performed completion using 3D convolutions. 
% However, they are often limited by the resolution of the voxels. With the development of point-based networks like PointNet~\cite{qi2017pointnet},  PCN~\cite{Pcn} directly generates high-resolution complete point clouds in a coarse-to-fine manner.
However, they are often limited by the resolution of the voxels. With the development of point-based networks like PointNet~\cite{qi2017pointnet}, various point cloud tasks can be handled by end-to-end networks \cite{repcdnet,geometry,punet,pointclean,votenet}. Among them, PCN~\cite{Pcn} is the first work that directly generates high-resolution complete point clouds in a coarse-to-fine manner for point cloud completion.
A similar generation strategy is also adopted in a series of following works~\cite{liu2020morphing,pf-net,wang2020cascaded,sa-net,wen2021pmp}. Transformer~\cite{vaswani2017attention} has also been leveraged in recent works. PoinTr~\cite{Pointr} treats the point cloud as a token sequence, using transformer encoder-decoder to predict the missing parts. SnowflakeNet~\cite{Snowflakenet} designs a transformer decoder with skip connections to refine the point cloud. 
Another line of works~\cite{zhang2021view,zhu2023csdn} enhances completion performance using 2D information. 
% ViPC~\cite{zhang2021view} takes an additional 2D image as input to provide additional color information. Following it, CSDN~\cite{zhu2023csdn} integrates shape features from images into global features, generating finer geometric structures. 
Different from the above approaches, SVDFormer~\cite{SVDFormer} and GeoFormer~\cite{Geoformer} project point clouds into 2D depth images, requiring information from only partial input.

Although these methods perform well on synthetic datasets, their reliance on training data causes performance degradation on out-of-distribution real-world scans and previously unseen categories. Recent unsupervised~\cite{pcl2pcl,unsupervisedgan,xie2021stylegan,c4c} and self-supervised approaches~\cite{ppnetbmvc,aclspc,p2c} have alleviated this issue to some extent; however, the completion results remain suboptimal.
% Furthermore, due to the domain gap, these methods often experience a significant drop in performance when applied to real-world scanned data. Additionally, these methods tend to work well only for the specific categories or similar categories seen during training, lacking generalization to unseen categories. 
To address these limitations, SDS-Complete~\cite{sds-complete} formulates point cloud completion as a test-time optimization problem, introducing a zero-shot method that fits a Signed Distance Function (SDF) to the input partial point cloud. It leverages Score Distillation Sampling (SDS) to extract 2D priors from the Stable Diffusion~\cite{stablediffusion} model to complete the missing regions. Subsequently, Huang et al.\cite{zeroshotpointcloudcompletion} propose initializing the partial point cloud as 3D Gaussians and distilling prior knowledge from zero123\cite{zero123}. Although these methods exhibit impressive zero-shot completion capabilities, they require optimization from scratch for each incomplete point cloud, making them time-intensive. Moreover, reliance on implicit 2D diffusion priors limits the reconstruction of fine geometric details.
In this work, we leverage explicit priors from a pre-trained 3D generative model to enhance zero-shot point cloud completion quality while significantly reducing processing time.

\subsection{3D Generation}
DreamFusion~\cite{DreamFusion} is the first method to use 2D priors for 3D generation, introducing Score Distillation Sampling (SDS) to extract 2D priors from a pretrained diffusion model and guide the 3D generation process, inspiring numerous impressive works. Magic3D~\cite{magic3d} adopts DMTet~\cite{dmtet} as the 3D representation instead of NeRF~\cite{nerf} and then performs optimization using SDS. Fantasia3D~\cite{Fantasia3D} decouples the optimization of geometry and material properties. 
With the emergence of 3D Gaussian Splatting~\cite{3dgs}, a highly expressive 3D representation, the optimization time for 3D generation with SDS has been significantly reduced. DreamGaussian~\cite{DreamGaussian} firstly attempts to use SDS optimization for 3D Gaussians, reducing the optimization time to just a few minutes while achieving excellent results. GaussianDreamer~\cite{gaussiandreamer} initializes 3D Gaussians using point cloud priors, yielding impressive results. 
% 下面两段有点重复
% Despite the remarkable performance and generalization ability of these optimization-based methods, they still require several minutes of processing time.
Although the above methods are effective, they require several minutes or even hours for optimization. 
% 
The emergence of large-scale datasets~\cite{objaverse,objaverseXL} has driven the development of faster feed-forward methods. Once trained, these methods can generate 3D objects within seconds through a single forward inference. Recently, LRM~\cite{LRM} demonstrated that a regression model can predict a NeRF from a single image within seconds. Based on this, InstantMesh~\cite{instantmesh} generates additional multi-view images from a single image and then reconstructs the mesh. However, both methods are limited by resolution. To address this, LGM~\cite{LGM} introduces an efficient representation of multi-view Gaussian features, enabling the prediction of high-resolution 3D Gaussian models. 

These feed-forward methods can generate high-quality 3D objects from a single image in a very short time while demonstrating strong generalization ability. 
We are motivated to leverage this advantage for point cloud completion, aiming to achieve superior zero-shot completion results while reducing optimization time.


