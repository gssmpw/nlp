\section{Method}
\label{sec:method}
The input of GenPC consists of a partial point cloud \( P_{in} \subseteq \mathbb{R}^{N \times 3} \) and a corresponding text prompt \( T_{in} \), where \( N \) represents the number of points in \( P_{in} \). Our goal is to obtain a complete shape $P_{out}$ that preserves the original structure in input.
As illustrated in Figure~\ref{fig:pipeline}, our method seamlessly incorporates an image-to-3D generative model into the point cloud completion process through the introduction of two innovative modules.
First, current image-to-3D models are designed to accept only 2D images as input. To adapt them for point cloud completion, we introduce the \textbf{Depth Prompting} module, which leverages depth images as a stepping stone to bridge the modality gap between partial point clouds and generative models.
After generating a 3D shape from the image-to-3D model, a key challenge arises: the original points in $P_{in}$ are not retained in the generated shape. To address this, we propose the \textbf{Geometric Preserving Fusion} module, which further aligns the initial generated shape with $P_{in}$, ensuring that the final result is both semantically accurate and structurally faithful.
% \textbf{2) Generation and Fusion}, which generates corresponding 3D objects, integrates it with the partial point cloud, and fills in the missing regions through the fusion process. \textbf{3) Optimization}, which reduces error accumulation and enhances completion quality.
\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{figs/depth-prompting.pdf}
    \caption{Illustration of Depth Prompting. (a) Overview. First, we uniformly position cameras around the partial point cloud \( P_{in} \) to select a scanning viewpoint. From this viewpoint, we project to obtain depth and the corresponding mask, and then apply mask inpainting to achieve high-quality depth. (b) Viewpoint Selection: For each viewpoint $V_i$, we perform a spherical flip on \( P_{in} \) for each camera to obtain a mirrored point cloud \( \hat{P_{in}} \), then create a convex hull around \( \hat{P_{in}} \cup V_i \), identifying the points on this hull as visible points. The camera with the greatest number of visible points is chosen as the scan viewpoint \( V_{scan} \). The top of (b) is a true viewpoint, all points lie on the convex hull. The bottom of (b) is the opposite viewpoint, only two lie on the convex hull. }
    \label{fig:depth-prompting}
\end{figure}
\label{subsec:Depth Bridging}
\subsection{Depth Prompting}
Figure~\ref{fig:depth-prompting} describes the proposed Depth Prompting module.
This module generates an RGB image from the input partial point cloud \( P_{in} \) by first projecting it to a coarse depth map $D_{raw}$ as an intermediary. Through masked inpainting of missing areas, a smooth depth map \( D_c \) is produced to enhance robustness to point cloud sparsity. Finally, \( D_c \) and the text prompt \( T_{in} \) are input into a depth-conditioning ControlNet~\cite{controlnet} to produce the corresponding RGB image.
% 
To project a high-quality depth image from an incomplete point cloud, we propose to find the viewpoint from which the point cloud was captured. Although Huang et al.~\cite{zeroshotpointcloudcompletion} employs a distance-based method for viewpoint estimation, this approach can sometimes result in issues such as depth reversal. To address these problems, we follow the approach proposed by \cite{hiddenpointremove}, framing the viewpoint estimation as a hidden point removal task. As illustrated in Figure~\ref{fig:depth-prompting}(a), We start by evenly positioning \( M \) cameras \( V_i \) ( where \( i = 1, 2, \dots, M \) ) around the input point cloud \( P_{in} \). For each camera, as shown in Figure \ref{fig:depth-prompting}(b), we perform a spherical flip on \( P_{in} \) to obtain a mirrored point cloud \( \hat{P_{in}} \). We then create a convex hull around \( \hat{P_{in}} \cup V_i \), identifying the points on this hull as visible points. The camera with the greatest number of visible points is chosen as the scan viewpoint \( V_{scan} \). By constructing the convex hull, our approach effectively prevents depth reversal and projects $P_{in}$ to an initial depth map $D_{raw}$.

%
However, some partial point clouds, such as cars in the KITTI dataset, are extremely sparse, leading to sparse depth projections that hinder subsequent completion. To address this issue, we use a pre-trained 2D inpainting diffusion model~\cite{inpanting} to fill the missing holes in the sparse depth \( D_{raw} \), resulting in a complete, high-quality depth image \( D_{c} \). 
To create an inpainting mask, we first project the point cloud with a large pixel size to obtain \( M_{FULL} \). We then apply an XOR operation between \( M_{FULL} \) and the inverted depth map (\( \neg D_{raw} \)), which generates the required mask for inpainting. 
Using this mask, the inpainting model fills the missing depth regions and smooths any irregular edges, producing \( D_{c} \). Note that any inpainting model capable of filling masked areas can be applied here.
Finally, we use \( D_{c} \) as conditioning input, along with the text prompt \( T_{in} \), to generate the image \( I_{gen} \) corresponding to the partial input. This is achieved by leveraging a pre-trained depth-conditional image generation model, such as ControlNet~\cite{controlnet}.
\label{subsec:Geometric Preserving Fusion}
\subsection{Geometric Preserving Fusion}
\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{figs/GPF.pdf}
    \caption{Illustration of Geometric Preserving Fusion. 
    In the Dynamic Scale Adaptation stage, an optimal scale factor is selected to align \( P_{partial} \) and \( P_{gen} \), producing an initial completed point cloud \( P_{all} \).
    Then, to reduce the accumulated error in the previous steps, an optional Refining operation can be performed, where \( P_{all} \) is initialized as 3D Gaussians and optimized by the SDS loss.}
    \label{fig:GPF}
\end{figure*}
In the Dynamic Scale Adaptation stage, we first colorize the input point cloud \( P_{in} \) using the generated image \( I_{gen} \), resulting in \( P_{partial} \). Then, \( P_{partial} \) and \( P_{gen} \) are aligned at dynamic scales, producing an initial, completed point cloud \( P_{all} \). 
Then, we apply an optional Refining stage. In this stage, \( P_{all} \) is initialized as 3D Gaussians \( G_{all} \), with different regions having distinct Gaussian parameter settings to preserve the original geometric details of the input point cloud while optimizing the shape of missing areas. This step helps to eliminate error accumulation and enhance overall completion quality.
\subsubsection{Dynamic Scale Adaptation}
We first use the generated image \( I_{gen} \) to obtain the 3D shape \( P_{gen} \) through the Image-to-3D generation model. Thanks to the powerful zero-shot generation performance of the pre-trained models, the generated \( I_{gen} \) and \( P_{gen} \) are highly consistent in category and shape with the input point cloud.
Next, we use \( P_{gen} \) to fill in the missing areas of the input point cloud, as shown in Figure~\ref{fig:GPF}. To improve the fusion process, we color \( P_{in} \) using the RGB information from \( I_{gen} \), creating a colored partial point cloud \( P_{partial} \). Since different parts of the object exhibit distinct colors, these colors can be regarded as semantic cues, enriching the fusion with additional contextual information for more accurate integration. Both \( P_{partial} \) and \( P_{gen} \) are then normalized to a unified scale within the range [-0.5, 0.5], reducing the search space for subsequent integration.

To eliminate the impact of both scale and pose differences, we scale \( P_{gen} \) within the range [0.8, 1.2] at intervals of 0.1, and perform ICP~\cite{icp} alignment at each scale, using the Chamfer Distance to evaluate the alignment results. We treat the color of the point cloud as semantic information, which allows us to not only supervise the alignment geometrically but also consider color information as an additional supervision signal. During the iterative registration, we calculate both the Euclidean and RGB Chamfer Distance between \( P_{partial} \) and \( P_{gen} \). The Chamfer Distance ensures accurate geometric alignment, while the RGB Chamfer Distance supervises the alignment of the semantic information, thereby improving the overall quality of the fusion. Together, they form the following objective:
% \[
% \resizebox{0.47\textwidth}{!}{ 
% $\arg \min_{P_{gen}} \left( \alpha \cdot CD_{XYZ}(P_{partial}, P_{gen}) + \beta \cdot CD_{RGB}(P_{partial}, P_{gen}) \right)$}
% \]
\resizebox{0.47\textwidth}{!}{ $\arg \min_{s \in [0.8, 1.2]} \left( \alpha \cdot CD_{XYZ}(P_{\text{partial}}, s \cdot P_{\text{gen}}) + \beta \cdot CD_{RGB}(P_{\text{partial}}, s \cdot P_{\text{gen}}) \right)$ }

% where \( \alpha \) and \( \beta \) are regularization terms, and \( s \) represents the scaling factor.
% Finally, we select the registration result that minimizes and remove points from \( P_{gen} \) that are adjacent to \( P_{partial} \) to avoid point cloud overlap, resulting in the missing portion of the point cloud \( P_{miss} \). Together, \( P_{miss} \) and \( P_{partial} \) form the preliminary complete point cloud \( P_{all} \).
where \( \alpha \) and \( \beta \) are regularization terms, and \( s \) represents the scaling factor. 
Finally, we select the registration result that minimizes the combined XYZ and RGB Chamfer distances and remove points from \( P_{\text{gen}} \) that are adjacent to \( P_{\text{partial}} \) to avoid point cloud overlap, resulting in the missing portion of the point cloud \( P_{\text{miss}} \). Together, \( P_{\text{miss}} \) and \( P_{\text{partial}} \) form the preliminary complete point cloud \( P_{\text{all}} \).
% During the registration process, \( P_{partial} \) is used as the source point cloud, which improves the registration quality but leads to a change in its position and alignment. Therefore, we need to reverse the scaling and ICP transformation matrices to restore its original pose, ultimately achieving the fused point cloud \( P_{all} \) (\( P_{all} = P_{partial} + P_{miss} \)), thus completing the preliminary point cloud completion. 
\subsubsection{Refining}
To further enhance the accuracy of point cloud completion and reduce error accumulation, we optimize the preliminarily completed point cloud, as shown in Figure~\ref{fig:GPF}. First, the point cloud is initialized as 3D Gaussians, and then distinct parameter configurations are applied to different parts of the 3D Gaussian. This approach maintains the integrity of the original part \( G_{partial} \) while optimizing the geometry of the missing part \( G_{miss} \), thereby improving the overall quality and consistency of the point cloud completion.

\noindent\textbf{Partial setup}:
For the partial point cloud \( P_{partial} \), we initialize it as a 3D Gaussians \( G_{partial} \). To preserve the original geometry, we fix parameters such as the coordinates, color, scale, and opacity, making them non-trainable. This ensures that the geometric of the partial point cloud remains unaffected during the optimization process, thereby maintaining consistency with the original input.

\noindent\textbf{Miss setup}:
For the missing point cloud \( P_{miss} \), we initialize it as a 3D Gaussians \( G_{miss} \). The scale remains fixed, as these points are uniformly sampled from the mesh surface and already have a reasonable scale. Opacity is set to 1 and remains non-trainable to ensure the stability of the Gaussian points on the surface. The color parameters are not fixed, but the learning rate is set relatively low because color carries semantic information. This allows for adjustments to the color during optimization while preserving its semantic characteristics as much as possible. The Gaussian coordinates are the main focus of the optimization, ensuring that the missing point cloud fits the shape of the partial input.

\noindent\textbf{SDS Guidance Optimization}:
Next, under the viewpoint \( V_{scan} \), we render an image \( I_{optim} \) and a depth map \( D_{optim} \) from \( G_{partial} \). We then incorporate both \( G_{miss} \) and \( G_{partial} \), and render an image \( \tilde{I}_{\mathrm{optim}}^i \) from a random viewpoint. This process is iterated multiple times, where in each iteration, we apply SDS to extract 2D priors from the pre-trained novel view synthesis diffusion model Zero123~\cite{zero123}, refining \( G_{miss} \) based on \( I_{optim} \) until satisfactory completion is achieved. The SDS Loss can be formulated as:
\[
\resizebox{0.47\textwidth}{!}{ $\nabla_{G_{all}}\mathcal{L}_{\mathrm{SDS}}=\mathbb{E}_{t,p,\epsilon}\left[(\epsilon_\phi(I_{\mathrm{optim}};t,\tilde{I}_{\mathrm{optim}}^i,\Delta p)-\epsilon)\frac{\partial I_{\mathrm{optim}}}{\partial_{G_{all}}}\right]$}
\]
where \( \epsilon_\phi(\cdot) \) is the predicted noise from the 2D diffusion prior \( \phi \), t is the time step, , $\epsilon$ is the standard noise and \( \Delta p \) represents the relative camera pose change from the scan viewpoint \( V_{scan} \), respectively. 

Additionally, to prevent other 3D Gaussians in the optimization process from affecting the geometric information of the input in the \( G_{partial} \) region, we also render images \( I_{optim}^i \) and depth maps \( D_{optim}^i \) under the viewpoint \( V_{scan} \) during the optimization iterations, and set a preservation loss \( L_{Presv} \) for the partial region:
\[
\resizebox{0.47\textwidth}{!}{ $L_{Presv} = w_1 \cdot \text{MSE}(I_{optim}, I_{optim}^i) + w_2 \cdot \text{MSE}(D_{optim}, D_{optim}^i)$}
\]
where MSE is the Mean Squared Error between the optimized and reference images \( I_{optim}^i \) and \( I_{optim} \), as well as the depth maps \( D_{optim}^i \) and \( D_{optim} \). \( w_1 \) and \( w_2 \) are weights that balance the importance of image and depth losses. By incorporating \( L_{Presv} \) and \( L_{SDS} \), our method preserves the geometry of the partial point cloud while optimizing the missing areas, reducing multi-stage error accumulation and improving the overall completion quality.

