\section{Experiment}
\label{sec:experiment}

\begin{table*}[t]
    \tiny
    \renewcommand\arraystretch{1.2}
    \centering
    \caption{Quantitative results on the Redwood~\cite{redwood}~\cite{sds-complete} dataset. Ours* represents our results without Refining ({$\displaystyle \ell ^{1}$} CD and EMD $\times 10^2$ ).}
    \label{tab:redwood}
    \small
    \scalebox{0.8}{ % 缩放比例为 0.8
    \begin{tabular}{c|cccccccccc|c}
    \toprule[1pt]
    Objects & Table & Swivel-Chair & Arm-Chair & Chair & Sofa & Vase & Off-Can & Vespa & Wheelie-Bin &Tricycle & Avg$\downarrow$ \\
    
    Metrics & CD/EMD & CD/EMD & CD/EMD & CD/EMD & CD/EMD & CD/EMD & CD/EMD & CD/EMD & CD/EMD  & CD/EMD & CD/EMD  \\
    \midrule[0.3pt]
      PoinTr~\cite{Pointr}  &1.86/3.50 & 4.08/8.49 & 1.95/4.22 & 2.69/5.38 & 2.96/5.02 & 4.05/7.28 & 4.82/6.92 & 2.00/4.06 & 2.78/3.51 &1.70/3.99 & 2.89/5.24 \\
      SnowflakeNet~\cite{Snowflakenet} &3.44/6.92 & 3.40/7.58 & 2.15/4.45 & 2.35/5.28 & 2.64/5.00 & 4.63/7.69 & 4.36/6.75 & 2.07/4.42 & 3.14/5.03 &1.44/3.32 &2.96/5.64  \\
      Adapointr~\cite{AdaPoinTr}  &5.20/6.44 & 5.09/8.03 & 3.67/4.53 & 4.40/5.96 & 3.59/5.18 & 6.23/7.56 & 6.04/7.69 & 3.21/4.65 & 4.13/7.63 &2.90/4.25 &4.45/6.19 \\
      SDS-Complete~\cite{sds-complete}  &1.67/2.92 & 2.24/3.09 & 2.18/3.16 & 2.62/3.61 & 2.95/4.56 & 3.26/5.89 & 4.03/4.36 & 3.46/5.94 & 2.69/3.21 &2.11/3.87 &2.72/4.06 \\
      \midrule[0.3pt]
      Ours* &1.41/2.24 & 1.69/2.37 & 1.38/1.76 & 1.47/2.48 & 1.61/2.93 & 3.15/5.24 & 3.04/4.62 & 1.59/2.83 & 2.65/3.64 &1.79/3.52 &1.98/3.16  \\
      Ours &\textbf{1.28/2.07} & \textbf{1.43/2.29} & \textbf{1.16/1.68} & \textbf{1.36/2.20} & \textbf{1.58/2.78} & \textbf{2.86/4.85} & \textbf{2.72/4.36} & \textbf{1.36/2.47} & \textbf{2.31}/\textbf{3.17} &\textbf{1.38}/\textbf{2.97} & \textbf{1.74}/\textbf{2.88} \\
      \bottomrule[1pt]
    \end{tabular}
    }
\end{table*}

\begin{table}[t]
        \renewcommand\arraystretch{1.2}
        \centering
        \caption{Quantitative results on ScanNet ($\ell ^1$ CD and EMD $\times 10^2$).}
        \label{tab:scannet}
        \small
        \begin{tabular}{c|c c| cc} 
        \toprule[1pt]
        Methods& SnowflakeNet & AdaPoinTr & Ours \\
        Metrics & CD/EMD & CD/EMD & CD/EMD \\
         \midrule[0.3pt]
         Table& 1.80/4.78 & 2.34/6.12 & \textbf{1.67}/\textbf{3.86}\\
         Chair& 1.68/3.76 & 2.07/\textbf{3.09} & \textbf{1.57}/3.24\\
         Avg & 1.74/4.27 & 2.21/2.38 & \textbf{1.62}/\textbf{3.55}\\
        \bottomrule[1pt]
        \end{tabular}
\end{table}


\begin{table}[t]
        \renewcommand\arraystretch{1.2}
        \centering
        \caption{Performance of ablation variant C (w/o depth inpainting) on different datasets. Variant C performs relatively well on Redwood's dense point clouds but shows significant performance drops with the sparse point clouds in ScanNet ({$\displaystyle \ell ^{1}$} CD and EMD $\times 10^2$).}
        \label{tab:ablationDepthInpainting}
        \small
        \begin{tabular}{c|c c }
        \toprule[1pt]
        Methods     & variant C & Ours\\
        Metrics     & CD/EMD & CD/EMD\\
        \midrule[0.3pt]
        Redwood     &2.23/3.60 & \textbf{1.74}/\textbf{2.88} \\
        ScanNet     &3.57/6.10 & \textbf{1.62}/\textbf{3.55}  \\
        \bottomrule[1pt]
        \end{tabular}
\end{table}

\begin{table}[t]
    \renewcommand\arraystretch{1.2}
    \centering
    \caption{Performance of ablation variants on the Redwood dataset ($\ell ^1$ CD and EMD $\times 10^2$).}
    \label{tab:ablationVariants}
    \small
    \begin{tabular}{c|c c }
    \toprule[1pt]
     Methods  & CD$\downarrow$ & EMD$\downarrow$   \\
    \midrule[0.3pt]
    A : w/o Viewpoint Selection  & 2.44 & 3.79 \\
    B : w/o ControlNet & 4.31 & 6.80 \\
    C : w/o Depth Inpainting &2.23 & 3.60 \\
    D : w/o 3D Generative Model & 4.65 & 6.13 \\
    E : w/o Dynamic Scale Adaptation & 4.38 & 4.52 \\
    F : w/o SDS Optimization & 1.98 & 3.16 \\
    Ours  & \textbf{1.74} & \textbf{2.88} \\
    \bottomrule[1pt]
    \end{tabular}
\end{table}

% \begin{table}[t]
%         \renewcommand\arraystretch{1.2}
%         \centering
%         \caption{Comparison of Completion Time on the Redwood Dataset. Ours* represent without using SDS optimization.}
%         \label{tab:ablationtime}
%         \normalsize
%         \begin{tabular}{c|c c c }
%         \toprule[1pt]
%         Methods  & CD$\downarrow$ & EMD$\downarrow$  & Time$\downarrow$ \\
%         \midrule[0.3pt]
%         SDS-Complete &2.72 & 4.06 & 40Hours \\
%         Ours* & 1.98 &3.16 & \textbf{1 Minute}\\
%         Ours & \textbf{1.74} &\textbf{2.88} & 2 Minutes\\
%         \bottomrule[1pt]
%         \end{tabular}
% \end{table}

\begin{figure*}
    \centering
    \includegraphics[width=1\textwidth]{figs/exp_redwood_min.pdf}
    \caption{Visual comparisons with recent methods~\cite{Pointr,Snowflakenet,AdaPoinTr} on the Redwood dataset.}
    \label{fig:exp_redwood}
\end{figure*}

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{figs/exp_scannet_min.pdf}
    \caption{Visual comparisons with recent methods ~\cite{Snowflakenet,AdaPoinTr} on the ScanNet dataset.}
    \label{fig:exp_scannet}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figs/exp_kitti_min.pdf}
    \caption{(Top) Visual comparisons with AdaPoinTr~\cite{AdaPoinTr} on KITTI~\cite{kitti}. (Bottom) We display the point clouds in different colors: blue for the Partial Input, red for AdaPoinTr, and gray for Ours. Our result maintains a consistent scale with the input.}
    \label{fig:exp_kitti}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{figs/exp_abs.pdf}
    \caption{Depth and RGB images produced by variant A (w/o Viewpoint Selection) and Variant C (w/o Depth Inpainting). (Left) The above is the complete depth \( D_c \) obtained from the scanning viewpoint and the opposite viewpoint, and below are the corresponding generated images $I_{gen}$. (Right) The above are the sparse depth \( D_{\text{raw}} \) and the complete depth \( D_{\text{c}} \), and below are the generated corresponding images \( I_{\text{gen}} \).}
    \label{fig:exp_abs}
\end{figure}

% \subsection{Implementation Details}
% % 这里写两个2D diffusion的参数，比如推理步数、guidance scale等
% % 师兄， inpainting 用的是Diffusion Models Beat GANs on Image Synthesis这个论文。
% For depth inpainting, we employ a pre-trained diffusion model~\cite{inpanting} with a resolution of 256×256. For the depth-conditioned generative model, we utilize ControlNet~\cite{controlnet} with inference steps set to 30 and a conditioning scale of 0.99, generating outputs at a resolution of 1024×1024. For the Image-to-3D generative model, InstantMesh~\cite{instantmesh}, we use a base-scale configuration with inference steps set to 75, scale set to 1, distance parameter at 4.5, and six views for rendering.
% During SDS optimization, the output resolution is set to 256×256 with a batch size of 12.

\subsection{Dataset and Evaluation Metric}
We validate our method on three real-world datasets Redwood~\cite{redwood}, ScanNet~\cite{dai2017scannet}, and KITTI~\cite{kitti}.
For the Redwood~\cite{redwood} dataset, we follow prior approaches~\cite{sds-complete}, using single-view scans as partial inputs and multi-frame aggregations as ground truth. Since previous deep learning-based methods were trained on standardized synthetic datasets, we normalize the Redwood dataset point clouds to the range [-0.5, 0.5] and set the elevation angle to 0° to ensure a fair comparison with their input requirements.
For the ScanNet dataset, which contains partial point clouds extracted from RGB-D scans, we focused on tables and chairs due to their complex structures and additional supports that introduce challenging self-occlusion cases for our method. For each category, we select 16 objects for validation. In addition, we used the ground truth provided by~\cite{wu2023scoda}, consisting of 2048 points for quantitative evaluation.

For quantitative evaluation, we followed prior methods by sampling 16,384 points from the Redwood dataset and 2,048 points from the ScanNet dataset using Farthest Point Sampling (FPS) to enable direct comparison with the ground truth. To assess the quality of point cloud completion, we used the widely adopted Chamfer Distance (CD) and Earth Mover’s Distance (EMD) metrics, scaling the loss values by a factor of 100 for clearer interpretation.
We also conduct a qualitative evaluation on KITTI~\cite{kitti} to assess the performance on sparse LiDAR scans.


\subsection{Results on the Redwood dataset}
The quantitative and qualitative results are presented in Table~\ref{tab:redwood} and Figure~\ref{fig:exp_redwood}. With or without the SDS Refining step, GenPC consistently achieves state-of-the-art performance across the entire dataset. These results indicate that existing learning-based methods~\cite{Pointr, Snowflakenet, AdaPoinTr} struggle to complete out-of-distribution data, even when these data belong to categories seen during training (e.g., chairs and couches). Additionally, these methods are sensitive to scale variations, leading to inconsistent outputs when the input scale changes. Compared with the only zero-shot method, SDS-Complete~\cite{sds-complete}, GenPC achieves an average reduction in CD by $36\%$ and EMD by $29\%$. Furthermore, Figure~\ref{fig:exp_redwood} clearly illustrates that GenPC outputs finer structure details than SDS-Complete, attributed to the rich geometric priors provided by the pre-trained 3D generative model.

\subsection{Results on the ScanNet dataset}
Comparison with two cutting-edge learning-based methods~\cite{Snowflakenet,AdaPoinTr} are presented in Table~\ref{tab:scannet} and Figure~\ref{fig:exp_scannet}. 
Our method demonstrates advanced performance in completion quality, maintaining reliable results even when dealing with sparse and noisy point clouds. As shown in Figure~\ref{fig:exp_scannet}, our method generates completion outputs with high fidelity to the input point cloud and rich geometric details, while learning-based methods are affected by domain gaps, leading to numerous noisy points in their results. 
% Considering that SDS-Complete~\cite{sds-complete} requires up to 40 hours to complete a single object, we did not include it in our evaluations.


\subsection{Results on KITTI}

A qualitative comparison on the KITTI dataset is presented in Figure~\ref{fig:exp_kitti}, which shows that GenPC produces results with a complete and realistic shape without any extraneous noise. In contrast, previous methods trained on ShapeNet produce completed point clouds that are smaller in scale than the original, as shown in the bottom of \ref{fig:exp_kitti}. The proposed Dynamic Scale Adaptation allows the completed results to maintain scale consistency with the original point cloud.
 
% \subsection{Results on the synthetic Dataset}
% To validate the effectiveness of our method on synthetic datasets, we conducted tests on the chair category within the sparse point cloud PCN test set. Since chairs typically have multiple supporting structures (such as legs), they exhibit more complex self-occlusion compared to other categories. As a result, both the depth bridging and fusion module in our method face greater challenges.


\subsection{Ablation Study}
% We remove and modify the main components to ablate GenPC. All ablation variants are tested on the real-world datasets. The ablation variants can be categorized as ablations on Depth Prompting module, image-to-3D generative model and Geometric Preserving Fusion module. 
% Since our approach is multi-stage, later inputs heavily depend on earlier outputs, making certain ablation experiments infeasible. Therefore, in some cases, we only perform ablation at a specific stage.
\subsubsection{Ablation on Depth Prompting Module}
To investigate the impact of the depth extraction method, we compare three variants of Depth Prompting. 
In variant A, we replace our viewpoint selection with a distance-based method similar to \cite{zeroshotpointcloudcompletion}, leading to significantly increased CD and EMD values. 
Meanwhile, as shown in Figure~\ref{fig:exp_abs}, although this method correctly identifies the viewpoint in some cases, it may select the reverse viewpoint, causing depth flipping. This flipped depth map disrupts accurate image generation and severely impacts completion quality.
In variant B, ControlNet is removed, and the inpainted depth $D_c$ is used as input to the image-to-3D generative model to examine the effects of color information on subsequent processes. In some cases, experimental observations show that, even with high-quality depth, the generated 3D shapes are reasonable but lack color, rendering them unsuitable for SDS optimization in the second stage. 
In variant C, we skip the depth inpainting step to evaluate the effect of low-quality depth on downstream processes. As shown in Figure~\ref{fig:exp_abs}, depth maps projected from sparse point clouds fail to generate accurate images, resulting in a significant drop in performance. Therefore, while this variant performs well on dense point cloud datasets like Redwood, it struggles on sparse point cloud datasets like ScanNet, as shown in Table~\ref{tab:ablationDepthInpainting}. 

\subsubsection{Ablation on 3D Generative Model}
To examine the effect of the image-to-3D generative model in our pipeline, we form variant D by replacing the generated 3D shape with a set of Gaussian noise point clouds. 
% These noise point clouds are initialized as 3D Gaussians alongside the partial input, maintaining the same 3D Gaussian parameter settings as in Geometric Preserving Fusion, except for the learning rate of the color parameters. 
The Refine step is then applied, optimizing over 5000 iterations in an attempt to complete the missing regions. 
The results in Table~\ref{tab:ablationVariants}, the absence of explicit geometric priors significantly impacts the completion performance. 

\subsubsection{Ablation on Geometric Preserving Fusion Module}
In variant E, we directly align the generated 3D shape \( P_{\text{gen}} \) with \( P_{\text{partial}} \) without using Dynamic Scale Adaptation to validate the effectiveness of this process. Due to scale inconsistency, the direct alignment fails to properly match the two point clouds, thereby wasting the rich geometric priors provided by the 3D shape.
In variant F, we omit the Refining process and use the merged point cloud \( P_{\text{all}} \) directly as the completion result. While quantitative metrics show that the Refining process can further enhance the overall completion quality, our experiments reveal that the merged point cloud \( P_{\text{all}} \) often performs competitively in both visualization and quantitative metrics. Therefore, we make the Refining process optional to improve completion speed.












