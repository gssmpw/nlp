\section{Introduction}
\label{sec:intro}
Point clouds, as an essential form of 3D representation, are widely used in various applications. However, due to factors such as self-occlusion, camera viewpoint limitations, and sensor resolution, the acquired point clouds are often incomplete. This issue significantly hinders downstream tasks. Therefore, developing effective and robust methods for completing real-world partial point clouds is crucial for achieving a comprehensive understanding of the real world.

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{figs/teaser.pdf}
    \caption{Difference between our GenPC with previous zero-shot point cloud completion method~\cite{sds-complete}. (a) SDS-Complete~\cite{sds-complete} uses the SDS loss to directly extract prior knowledge from a 2D diffusion model, featuring time-consuming optimization and suboptimal completion results. (b) The proposed GenPC leverages explicit priors provided by a 3D generative model, achieving improved completion quality with significantly reduced inference time.}
    \label{fig:teaser}
\end{figure}

In recent years, numerous deep learning-based point cloud completion methods~\cite{Pcn,Pointr,AdaPoinTr,pf-net,Snowflakenet,SVDFormer,Geoformer} have shown remarkable success. These approaches utilize carefully designed neural networks to extract shape patterns from input point clouds, enabling them to generate detailed geometric structures to complete missing portions of the point cloud. 
% Some methods~\cite{zhang2021view,zhu2023csdn} additionally incorporate corresponding 2D information to aid in point cloud completion. 
Although these techniques perform well on trained or similar categories, they rely on labeled 3D training data and exhibit limited generalization to categories unseen during training. Moreover, constrained by domain gaps between synthetic training data and real-world scans, these models tend to perform poorly when applied to downstream tasks.

% 简单介绍两个zeroshot方法，没有用3D大模型导致效果有限，且test-time训练时间长
With the impressive zero-shot generation capabilities of pre-trained 2D diffusion models~\cite{stablediffusion}, numerous studies~\cite{DreamFusion,zero123,DreamGaussian} have emerged that utilize these models for 3D generation tasks. 
Enlightened by these successes, sds-complete~\cite{sds-complete} first utilized 2D priors for zero-shot shape completion. This method fits the input partial point cloud surface using Signed Distance Functions (SDF) and leverages Score Distillation Sampling (SDS)~\cite{DreamFusion} to extract 2D diffusion priors for completion. 
Later, Huang et al.~\cite{zeroshotpointcloudcompletion} proposed a similar SDS-based framework, but used 3D Gaussian splatting~\cite{3dgs} to initialize the partial point cloud as 3D Gaussians.
Although these methods demonstrate improved zero-shot completion capabilities compared to training-based counterparts, they are time-consuming, as they require training a radiance field from scratch for each incomplete point cloud. Additionally, SDS loss often leads to coarse geometric details, limiting their reconstructing quality.

Recently, scalable network architectures and large-scale 3D datasets have propelled the success of feed-forward 3D generative models~\cite{LRM,LGM,instantmesh}. Once trained, these models achieve impressive zero-shot generation quality within seconds. This raises an intriguing question: \textit{``Can we leverage this 3D generative capability for point cloud completion?''}
To answer it, we introduce a novel zero-shot point cloud completion framework, GenPC. Unlike previous 2D diffusion-based approaches~\cite{sds-complete,zeroshotpointcloudcompletion}, GenPC utilizes explicit 3D priors from an image-to-3D generative model to enhance zero-shot completion quality, significantly improving inference speed.

\begin{figure*}
    \centering
    \includegraphics[width=0.9\textwidth]{figs/pipeline.pdf}
    \caption{The architecture of GenPC. The Depth Prompting module first prompts the depth-guided 2D generative model with the partial input and generates an RGB image, which is fed into an image-to-3D generative model, producing a 3D shape. 
    The Geometric Preserving Fusion module then integrates the generated shape with the partial point cloud.} 
    \label{fig:pipeline}
\end{figure*}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 两个点（技术贡献）
% Therefore, we need a bridge to connect partial point clouds with the image-to-3D generation model, and this bridge is depth. By utilizing depth extracted from the partial input point cloud, we can effectively guide the generation process to ensure a high match between the generated 3D object and the partial point cloud, better leveraging 3D priors for precise control, thereby enhancing completion quality and reducing completion time.
As illustrated in Figure ~\ref{fig:pipeline}. To leverage the powerful zero-shot generation capabilities of the image-to-3D generative model, we first need an image input. To address the issue of partial point clouds not directly providing image input for these models, we introduce a Depth Prompting module. This module estimates the scanning viewpoint of the partial point cloud and extracts depth, effectively bridging the modality gap between the point cloud and the generative model.
After generating the 3D shape, a significant issue arises: the generated 3D shape may differ from the input partial point cloud in terms of scale, pose, and shape. To align it with the input point cloud and retain the original geometric structure, we introduce the Geometric Preserving Fusion module. This module first dynamically adjusts the scale and pose using a scaling factor at both geometric and semantic levels. 
In addition, we can further refine the point cloud using the SDS loss, minimizing shape detail discrepancies caused by multi-stage error accumulation.
By leveraging explicit geometric priors offered by the 3D generative model, our approach avoids the need for optimization from scratch, enabling faster inference and superior completion quality.

In summary, our contributions are as follows:
\begin{itemize}
    \item We design a novel zero-shot completion framework called GenPC, which significantly improves real-world scan completion by prompting a pre-trained 3D generative model.
    \item We propose a Depth Prompting module to bridge the modality gap between partial scans and generative models by utilizing depth images as a stepping stone.
    \item We introduce the novel Geometric Preserving Fusion module for refining the initial generated results. It adaptively aligns the generated content with partial input, ensuring that the final result is both semantically accurate and structurally faithful.
    \item Extensive experiments demonstrate that GenPC achieves state-of-the-art performance on real-world datasets while significantly reducing completion time.
\end{itemize}
