\section{Related Work}
\vspace{-2mm}
\paragraph{Image Super-Resolution.}
The trailblazing research on image super-resolution with deep neural network is SRCNN____, which utilizes convolution layers to replace conventional methods in SR.
Since then, dazzling network architectures____ are designed to achieve more stirring image restoration accuracy.
RDN____ designs dense skip connection and global residual to utilize abundant local and global features.
With the advance of vision transformer, SwinIR____ uses window-attention to fully make use of global information.
As reconstruction accuracy increases, researchers focus more on model parameters and operations.
Generally, the ViT-based SR networks are much smaller and faster than pure CNNs.
However, even the advanced SR networks like SwinIR, current SR models are still too large to be deployed on resource-constrained edge devices.
Therefore, research on quantization on ViT-based networks is urgently needed to make SR possible on mobile devices.

\vspace{-4mm}
\paragraph{Post-Training Quantization.}
PTQ is famous for its fast speed and low cost during quantization.
Recently, excellent PTQ methods have been proposed to advance its performance and efficiency.
As a PTQ method specifically for ViT, PTQ4ViT____ proposed twin uniform quantization to reduce quantization error on the output of softmax and GELU.
RepQ____ decouples the quantization and inference processes and ensures both accurate quantization and efficient inference.
NoisyQuant____ surprisingly finds that adding a fixed Uniform noisy bias to the values being quantized can significantly reduce the quantization error.
However, most of the above PTQ methods are only for Transformer blocks and the generalization ability on SR tasks is relatively low.
Whereas, 
2DQuant____ searches the clipping bounds in different ways on different distributions and is currently the best PTQ method on the SR task.
However, most of the above methods only concentrate on the quantization process and ignore adjustments in the weights. 

\vspace{-4mm}
\paragraph{Condition Number.}
The concept of the condition number originated and developed in numerical analysis____.
It measures how much the output value of the system or function can change for a small change in the input.
A matrix with a high condition number is said to be ill-conditioned and leads to huge output changes given a small disturbance in input. 
It also plays an important role in machine learning (ML).
Freund proposed optimization algorithms generally converge faster when the condition number is low____.
In causal inference, it is used to evaluate the numerical stability of causal effect estimation____.
Additionally, the condition number is leveraged to enhance the numerical stability of regression models by fine-tuning hidden layer parameters____.
In this work, we innovatively leverage the condition number into quantization to measure the quantization sensitivity.
We formulate the optimization of quantization error via condition number and propose CondiQuant to solve it efficiently.

\vspace{-2mm}