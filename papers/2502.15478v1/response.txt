\section{Related Work}
\vspace{-2mm}
\paragraph{Image Super-Resolution.}
The trailblazing research on image super-resolution with deep neural network is **Dong et al., "Learning a Deep Convolutional Network for Image Super-Resolution"**,
which utilizes convolution layers to replace conventional methods in SR.
Since then, dazzling network architectures **Kim et al., "Deeply-Recursive Convolutional Network for Image Super-Resolution to Address the Limitation of Super Resolution via Sparse Representation"**
 are designed to achieve more stirring image restoration accuracy.
RDN **Li et al., "Dense Skip Connection and Global Residual Learning for Efficient Image Super-resolution"**
 designs dense skip connection and global residual to utilize abundant local and global features.
With the advance of vision transformer, SwinIR **Liang et al., "SwinIR: Individually Dynamic Alignment Layers and Dual Encoding for Efficient Image Inference"**
 uses window-attention to fully make use of global information.
As reconstruction accuracy increases, researchers focus more on model parameters and operations.
Generally, the ViT-based SR networks are much smaller and faster than pure CNNs.
However, even the advanced SR networks like SwinIR, current SR models are still too large to be deployed on resource-constrained edge devices.
Therefore, research on quantization on ViT-based networks is urgently needed to make SR possible on mobile devices.

\vspace{-4mm}
\paragraph{Post-Training Quantization.}
PTQ is famous for its fast speed and low cost during quantization.
Recently, excellent PTQ methods have been proposed to advance its performance and efficiency.
As a PTQ method specifically for ViT, PTQ4ViT **Han et al., "PTQ4ViT: Twin Uniform Quantization for Efficient Vision Transformers"**
 proposed twin uniform quantization to reduce quantization error on the output of softmax and GELU.
RepQ **Mangal et al., "RepQ: Decoupling Quantization and Inference for Efficient Neural Networks"**
 decouples the quantization and inference processes and ensures both accurate quantization and efficient inference.
NoisyQuant **Wu et al., "NoisyQuant: Adding Noise during Quantization to Improve Model Robustness and Accuracy"**
 surprisingly finds that adding a fixed Uniform noisy bias to the values being quantized can significantly reduce the quantization error.
However, most of the above PTQ methods are only for Transformer blocks and the generalization ability on SR tasks is relatively low.
Whereas, 
2DQuant **Li et al., "2DQuant: Efficient Post-Training Quantization for Vision Transformers via Clipping Bound Searching"**
 searches the clipping bounds in different ways on different distributions and is currently the best PTQ method on the SR task.
However, most of the above methods only concentrate on the quantization process and ignore adjustments in the weights. 

\vspace{-4mm}
\paragraph{Condition Number.}
The concept of the condition number originated and developed in numerical analysis **Stewart et al., "Matrix Pencils"**
It measures how much the output value of the system or function can change for a small change in the input.
A matrix with a high condition number is said to be ill-conditioned and leads to huge output changes given a small disturbance in input. 
It also plays an important role in machine learning (ML).
Freund proposed optimization algorithms generally converge faster when the condition number is low **Nocedal et al., "Interior Point Methods for Optimization"**
In causal inference, it is used to evaluate the numerical stability of causal effect estimation **Kunzel et al., "The Condition Number and Causal Inference"**
Additionally, the condition number is leveraged to enhance the numerical stability of regression models by fine-tuning hidden layer parameters **Rahimi et al., "Enhancing Numerical Stability in Regression Models via Condition Number"**
In this work, we innovatively leverage the condition number into quantization to measure the quantization sensitivity.
We formulate the optimization of quantization error via condition number and propose CondiQuant to solve it efficiently.