\section{Introduction}

Multi-armed bandits (MABs) provide a powerful framework for sequential decision-making under uncertainty, balancing exploration and exploitation to maximize cumulative rewards. Among its variants, Combinatorial MABs (CMABs) \citet{cesa2012combinatorial,chen2013combinatorial} have gained significant attention due to applications in online advertising, networking, and influence maximization \citep{gai2012combinatorial,kveton2015combinatorial, chen2009efficient,chen2013combinatorial}. In CMABs, an agent selects a subset of arms as the combinatorial action for each round, and the environment will return the reward signal according to the outcome of selected arms. 

As a popular variant,  \textit{$K$-Max Bandits} \citep{goel2006asking,gopalan2014thompson} focuses on the maximum outcomes within a selected subset of $K$ arms. This framework naturally captures real-world scenarios where the decision quality only depends on the extreme singular outcomes. For example, in \textit{recommendation system}, 
modern ad platforms must select $K$ products to display from a pool of candidates, where the customer will select the most preferred one.
In this case, the extreme preference reflects the recommendation efficiency of selection.
Likewise, in \emph{distributed computing tasks}, a scheduler may choose $K$ servers for parallel processing, and the overall completion metric depends on the server with the fastest response while feedback from others can be overshadowed. 

Motivated by the prevalence of continuous real-valued signals (e.g., ratings of selected products in online advertising, job completion time in distributed computing, and latency in server scheduling) and the fact that only partial observations may be accessible in modern applications, we study the \emph{Continuous $K$-Max Bandits} problem with \emph{value-index feedback}. Here, each arm has an unknown continuous distribution, and upon selecting a set of $K$ arms, the learner only observes the {maximum} outcome among the chosen arms alongside the {index} of the arm that attained that maximum value.


While $K$-Max bandits have received considerable attention \citep{simchowitz2016best, chen2016combinatorial,agarwal2020choice}, existing theoretical works face three critical limitations when tackling the continuous distribution and value-index feedback:
First, most existing algorithms require semi-bandit feedback \citep{chen2016combinatorial,simchowitz2016best,wang2017improving}, while practical systems often restrict observations to the winning arm's index and value. 
%
The key challenge here is that the observation under semi-bandit feedback is unbiased, while the observation under value-index feedback is biased, since we only observe an outcome when it is the winner.
%
Second, greedy approaches based on submodular optimization \citep{streeter2008online,fourati2024combinatorial} face inherent $(1-1/e)$ approximation limits \citep{nemhauser1978analysis}, which results in weaker regret guarantees. Third, existing solutions to $K$-Max bandits mostly assume binary \citep{simchowitz2016best} or finitely supported outcomes \citep{wang2023combinatorial}. The continuous nature of real-world outcomes, along with the value-index feedback, introduces challenges in discretization error and learning efficiency tradeoff, biased estimators due to nondeterministic tie-breaking under discretization, etc. 

Our primary contribution is a novel framework for Continuous $K$-Max Bandits with value-index feedback that addresses these challenges through two key technical innovations: 
\textbf{(i)} We formalize the discretization of continuous $K$-Max bandits into a discrete $K$-Max bandits (see \Cref{sec:discretized-K-Max,sec:discrete-binary,sec:offline-oracle}). In this process, we control the error term from discretization and establish the utilization of the efficient offline $\alpha$-approximated optimization oracle for any $\alpha < 1$, avoiding the unacceptable approximation error by traditional greedy algorithms that leads to a linear regret.
\textbf{(ii)} Due to the nondeterministic tie-breaking effect arising from the continuous-to-discrete transformation under value-index feedback, the agent cannot achieve an unbiased estimation under computationally tractable discretization (as detailed in \Cref{sec:algorithm}). We develop a bias-corrected discretization method and a novel concentration analysis with bias-aware error control (\Cref{lemma:concentration}) that jointly manage estimation variance and discretization-induced bias to achieve sublinear regret.

\paragraph{Contributions.} 
Our novel techniques lead to the development of the \texttt{DCK-UCB} algorithm (\Cref{alg}) for $K$-Max bandits with general continuous distributions. The \texttt{DCK-UCB} algorithm achieves a regret upper bound of $\widetilde{\mathcal{O}}(T^{3/4})$, and is efficient both computationally and statistically. To the best of our knowledge, this represents the first computationally tractable algorithm with a sublinear guarantee for continuous $K$-Max bandits under value-index feedback.


We further consider exponential $K$-Min bandits, a special case of continuous $K$-Max bandits, where outcomes of every arm follow exponential distributions. By utilizing the special property of exponential distributions, we avoid using a discretization method and biased estimators. Following this idea, we adapt the maximum log-likelihood estimation algorithm $\texttt{MLE-Exp}$ (\Cref{alg:k-min}), and propose an algorithm that achieves a better $\wt{\gO}(\sqrt{T})$ regret upper bound, which is nearly minimax optimal.



\paragraph{Paper organization.}
\Cref{sec:related-works} introduces related works of continuous $K$-Max bandits. \Cref{sec:preliminaries} formalizes the continuous $K$-Max bandits problem.
%
\Cref{sec:general-continuous} describes our algorithm and analysis for general continuous $K$-Max bandits.
\Cref{sec:kminexp} details the exponential distribution special case and an MLE-based algorithm with better regret guarantees. Complete proofs of \Cref{sec:general-continuous,sec:kminexp} are presented in the appendices.

\paragraph{Notations.} 
For any integer $n \ge 1$, we use $[n]$ to denote the set $\{1,2,\ldots,n\}$. The notation $\gO$ is used to suppress all constant factors, while $\wt{\gO}$ is used to further suppress all logarithmic factors. Bold letters such as $\vx$ are typically used to represent a set of elements $\{x_i\}$. Unless expressly stated, $\log(x)$ refers to the natural logarithm of $x$. Throughout the text, $\{\mathcal F_t\}_{t=0}^T$ is used to denote the natural filtration; that is, $\mathcal F_t$ represents the $\sigma$-algebra generated by all random observations made within the first $t$ time slots.





\section{Related Works}\label{sec:related-works}
The $K$-Max bandit problem represents a significant departure from traditional Combinatorial Multi-Armed Bandits (CMABs) \citep{cesa2012combinatorial,chen2013combinatorial}. While standard CMAB frameworks only need to learn the expected outcomes of every arms \citep{chen2013combinatorial,chen2014combinatorial,kveton2015tight,combes2015combinatorial,liu2023contextual}, $K$-Max bandits, whose reward signal is the maximum outcome within selected arms \citep{goel2006asking,gopalan2014thompson}, require to learn more information about the probability distribution of every arms, and necessitate novel approaches to balance the exploration-exploitation tradeoff.
Existing literature on $K$-Max bandits can be categorized by feedback type as follows.

\paragraph{Value Feedback} 
Certain scenarios involve the environment returning only the numerical reward, which corresponds to the maximum outcome of selected arms, known as the \textit{full-bandit feedback}. \citet{gopalan2014thompson} obtained the regret upper bounds $\mathcal{O}\left(\sqrt{\binom{N}{K}T}\right)$ for the $K$-Max bandits through a Thompson Sampling scheme. However, their approach requires the ground truth parameter to be in a known finite set, and their regret scales exponentially with $K$.
\citet{simchowitz2016best} considered the pure exploration task while their results are limited to Bernoulli outcome distributions.
\citet{streeter2008online,yue2011linear,nie2022explore,fourati2024combinatorial} investigated the submodular maximization perspective and yield $(1-1/e)$-approximation regret guarantees via greedy selection, and $K$-Max bandits naturally satisfy the submodular assumption.
%
However, these approximation regret guarantees lead to linear regret when the baseline policy is the true optimal subset
(\Cref{eq:def-regret} in this paper). 
It still remains open on achieving sublinear regret bounds in general $K$-Max bandits with full-bandit feedback.

\paragraph{Value-Index Feedback}
In this case, the feedback provides both the maximum outcome (reward) and the corresponding arm index. 
%
It looks similar to a CMAB problem with probabilistic triggering feedback \citep{wang2017improving,liu2023contextual,liu2024combinatorial}, i.e., with certain probability, we observe arm $i$ to be the winner, and also get an observation on arm $i$'s outcome. 
%
The main difference is that in these CMAB researches, it is often assumed that conditioning on we observe arm $i$'s outcome, the random distribution of this observed outcome is the same as the real outcome of arm $i$ without such condition (at least the mean should be the same). 
%
However, this is not the case in $K$-Max bandits, i.e., conditioning on arm $i$ being the winner, the observed outcome of arm $i$ must have some bias to the real outcome.
%
Under this feedback protocol, the most related work of \citep{wang2023combinatorial} considered the discrete $K$-Max bandits with a finite outcome support and a deterministic tie-breaking rule, achieving an $\wt{\gO}(\sqrt{T})$ regret upper bound. 
\citet{simchowitz2016best} also investigated the value-index feedback for Bernoulli outcomes.
However, their algorithm cannot work for the continuous case studied in this paper due to non-zero discretization error and the nondeterministic tie-breaking.
%

\paragraph{Semi-Bandit Feedback} 
Semi-bandit feedback reveals the outcome of \emph{every} selected arm in the subset, providing the learner with detailed observations to estimate each arm's distribution.
Several studies \citep{simchowitz2016best,jun2016top,chen2016combinatorial,chen2016combinatorialt,slivkins2019introduction} have leveraged this rich feedback to achieve  $\mathcal{O}(\sqrt{T})$ regret upper bounds for discrete or continuous $K$-Max bandits with unbiased estimation for every arm's outcome distribution.
%
However, due to the abundant unbiased observation, their proposed algorithm becomes very different from ours and cannot be applied to our setting.
