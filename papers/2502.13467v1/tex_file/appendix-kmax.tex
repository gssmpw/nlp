% \section*{Appendix}

\section{Omitted Proofs in \Cref{sec:general-continuous}}

In this section, we present the omitted proofs in \Cref{sec:general-continuous}, which include the full proof of \Cref{thm:main}.

\subsection{Discretization Error}\label{app:discrete-error}
First we show that the discretization from original continuous problem $\gB^*$ to $\bar\gB$ with discretization width $\epsilon$ will involve controllable error in expected loss, which is shown in \Cref{lemma:discrete-error} and formalized by the following lemma. 

\begin{lemma}\label{lemma:discrete-error-formal}
For any $S \in \gS$, we have
\begin{align}\label{eq:discrete-error}
    \bar r(S; \vp^*) \le r^*(S) \le \bar r(S; \vp^*) + \epsilon.
\end{align}
\begin{proof}
Notice that we have 
\begin{align*}
    \Prob\left[\max_{i\in S} (\bar X_i) = v_j\right] &= \sum_{I\subset S} \prod_{i\in I}\Prob[\bar X_i = v_j] \cdot \prod_{k \in S, k \notin I}\Prob[\bar X_k < v_j] \\
    &= \sum_{I\subset S} \prod_{i\in I} \Prob[X_i \in M_j] \cdot \prod_{k \in S, k \notin I}\Prob[X_k \in M_{\le j-1}] \\
    &= \Prob\left[\max_{i \in S} (X_i) \in M_j\right].
\end{align*}
Therefore, by definition of $r^*(S)$, we have
\begin{align*}
    r^*(S) &= \sum_{j\in [M]} \int_{r \in M_j} r\cdot \rd\Prob_{\max_{i\in S}(X_i)}(r) \\
    &\ge \sum_{j\in [M]} (j-1)\epsilon \int_{r \in M_j} \rd\Prob_{\max_{i\in S}(X_i)}(r) \\
    &= \sum_{j\in [M]} (j-1)\epsilon \cdot \Prob\left[\max_{i \in S} (X_i) \in M_j\right] \\
    &= \sum_{j\in [M]} (j-1)\epsilon \cdot \Prob\left[\max_{i\in S} (\bar X_i) = (j-1)\epsilon\right] \\
    &= \bar r(S; \vp^*),
\end{align*}
where the inequality is given by the definition of $M_j$. Then we achieve the left-hand side of \Cref{eq:discrete-error}. For the other side, we can similarly establish
\begin{align*}
    r^*(S) &= \sum_{j\in [M]} \int_{r \in M_j} r\cdot \rd\Prob_{\max_{i\in S}(X_i)}(r) \\
    &\le \sum_{j\in [M]} j\epsilon \int_{r \in M_j} \rd\Prob_{\max_{i\in S}(X_i)}(r) \\
    &= \sum_{j\in [M]} (j - 1)\epsilon \cdot \Prob\left[\max_{i \in S} (X_i) \in M_j\right] + \epsilon \cdot  \sum_{j\in [M]}  \Prob\left[\max_{i \in S} (X_i) \in M_j\right]\\
    &= \bar r(S; \vp^*) + \epsilon.
\end{align*}
\end{proof}
\end{lemma}

\subsection{Converting to Binary Arms}
As detailed in \Cref{sec:discrete-binary}, we set
\begin{align*}
    q_{i,j}^* := \frac{p_{i,j}^*}{1 - \sum_{j' > j} p_{i,j'}^*}, \ p^*_{i,j} = q^*_{i,j} \cdot \prod_{j' > j} (1 - q^*_{i,j'}),
\end{align*}
which implies
\begin{align*}
    q^*_{i,j} = \frac{p^*_{i,j}}{1 - \sum_{j' > j} p^*_{i,j'}} = \frac{p^*_{i,j}}{\sum_{j'=1}^j p^*_{i,j'}} =
    \frac{\Prob[X_i \in M_j]}{\Prob[X_i \in M_{\le j}]}.
\end{align*} 
For any given probability set $\vq = \{q_{i,j}: i \in [N], j \in [M]\}$, we can apply \Cref{eq:qstar-def} to get the corresponding $\vp$ defined as
\begin{align*}
    p_{i,j} = q_{i,j} \cdot \prod_{j' > j} (1 - q_{i,j'}).
\end{align*}
Assume $\{Y_{i,j}^\vq\}_{i\in[N], j \in [M]}$ is the set of independent binary random variables that $Y_{i,j}^\vq$ takes value $v_j = (j-1)\epsilon$ with probability $q_{i,j}$ and takes value $0$ otherwise. And $\{X_i^\vp\}_{i\in[N]}$ is the set of independent discrete random variables that $X_i^\vp$ takes value $v_j$ with probability $p_{i,j}$ for every $j \in [M]$. Therefore, by simple calculation, we have $\max_{j\in[M]}\{Y_{i,j}^\vq\}$ has the same distribution of $X_i^\vp$. 

$\bar r_q(S; \vq)$ is defined as the expected maximum reward of $\{Y_{i,j}^\vq\}_{i\in S,j\in[M]}$ Then we can write
\begin{align}\label{eq:def-rq}
   \bar r_q(S; \vq) = \sum_{j \in [M]} v_j \cdot \left( Q_j(S; \vq) - Q_{j-1}(S;\vq)\right),
\end{align}
where we denote for simplicity
\begin{align}\label{eq:def-Qj}
    Q_j(S; \vq):= \prod_{k \in S, j' > j} (1 - q_{k,j'}).
\end{align}
$Q_j(S; \vq)$ is actually the probability of the event that every arm in $\{\bar Y_{k,j'}\}_{k \in S, j' > j}$ does not sample a non-zero value.

Equipped with the above statement, we can establish the following lemma:

\begin{lemma}\label{lemma:r-q-r-formal}
For any $\vp$ and $\vq$ satisfying \Cref{eq:qstar-def}, we have for any $S \in \gS$, $$\bar r_q(S; \vq) = \bar r(S; \vp).$$
\begin{proof}
    Notice that by definition, we have
    \begin{align*}
        \bar r(S; \vp) = \E\left[\max_{i\in S} X_i^\vp\right],
    \end{align*}
    and
    \begin{align*}
        \bar r_q(S; \vq) = \E\left[\max_{i \in S} \max_{j \in [M]} Y_{i,j}^\vq\right].
    \end{align*}
    Notice that $\max_{j\in[M]}\{Y_{i,j}^\vq\}$ has the same distribution of $X_i^\vp$, we have
    \begin{align*}
        \bar r_q(S; \vq) &= \E\left[\max_{i \in S} \max_{j \in [M]} Y_{i,j}^\vq\right] \\
        &= \E\left[\max_{i \in S} X_i^\vp\right] = \bar r(S; \vp).
    \end{align*}
\end{proof}
\end{lemma}

\subsection{Biased Concentration}
We aim to use $\hat q_{i,j}^t$ to estimate $q_{i,j}^*$. However, this is a biased estimation. In this section, we carefully control the gap between the biased estimator $\hat q_{i,j}^t$ and the true probability $q^*_{i,j}$.


We set $c_t(i, j) := \mathbbm{1}[(i_t, j_t) = (i, j)]$ which is $\gF_t$-measurable. Then \Cref{alg} counts the summation of $c_t(i,j)$ as $C_t(i,j)$:
\begin{align*}
    C_t(i,j) = \sum_{\tau = 1}^t c_\tau(i,j),
\end{align*}
which is $\gF_{t-1}$-measurable. 

For given action $S_t$ in round $t$, the environment will sample a set of outcomes $\{X_i(t) \sim D_i : i \in S_t\}$. The value-index feedback is $r_t = \max_{i \in S_t} X_i(t)$, $i_t = \argmax_{i \in S_t} X_i(t)$. \Cref{alg} consider $j_t$ such that $r_t \in M_{j_t}$. We denote $I_t = \argmax_{i \in S_t} \bar X_i(t)$, where $\bar X_i(t)$ is the discretized of $X_i(t)$ induced by \Cref{eq:discretize-outcome}. Notice that under event $\gE_0$, $\argmax_{i \in S} X_i(t)$ is unique. But $I_t$ might be a set with multiple indices. We emphasize that $S_t$ is $\gF_{t-1}$ measurable and $(i_t, r_t, j_t, I_t)$ are $\gF_t$ measurable.

Then we can provide the following lemma.
\begin{lemma}\label{lemma:concentration-formal}
Under event $\gE_0$, we have for every $t \in [T]$ and $(i, j) \in [N] \times [M]$,
\begin{align*}
    \left|\hat q_{i,j}^t - q^*_{i,j}\right| \le \sqrt{8\frac{\log(NMt)}{SC_t(i,j)}} + (K-1)\cdot(L^4/j^2),
\end{align*}
with probability at least $1 - T^{-2}$, where we denote this good event as $\gE_1$. 

\begin{proof}
Denote $q_{i,j}(S_t) := \mathbbm{1}[i \in S_t] \cdot \Prob[(i_t, j_t) = (i, j) \mid j_t \le j, S_t]$, and $q_{i,j}^*(S_t) := \mathbbm{1}[i \in S_t] \cdot \Prob[I_t \ni i, j_t = j \mid j_t \le j, S_t]$. Therefore, for given $i \in [N], j \in [M]$, we have
\begin{align*}
    \E[\1[i \in S_t] \cdot c_t(i,j)\cdot \mathbbm{1}[j_t \le j] \mid  S_t] = q_{i,j}(S_t) \cdot \Prob[j_t \le j \mid S_t]
\end{align*}

By summation over time step $1, 2, \cdots, t$, we have
\begin{align*}
    % \E[C_t \mid j_t \le j] = 
    \sum_{\tau=1}^t \E[\1[i \in S_\tau]\cdot c_\tau(i,j)\cdot \mathbbm{1}[j_\tau \le j] \mid  S_\tau] &= \sum_{\tau=1}^t q_{i,j}(S_\tau)\cdot \Prob[j_\tau \le j \mid S_\tau] \\
    &= \sum_{\tau=1}^t \E\left[q_{i,j}(S_\tau)\cdot \mathbbm{1}[j_\tau \le j] \mid S_\tau\right],
\end{align*}
which implies that 
\begin{align*}
     \E\left[\sum_{\tau \le t, i \in S_\tau, j_\tau \le j}c_\tau(i,j) \middle| S_1, S_2, \cdots, S_t \right]  = \E\left[\sum_{\tau \le t, j_\tau \le j}
    q_{i,j}(S_\tau)\middle| S_1, \cdots, S_t\right]
\end{align*}
Notice that $S_t$ is $\gF_{t-1}$-measurable. By the definition of $q_{i,j}(S_\tau)$, we have
\begin{align*}
    \E\left[\sum_{\tau \le t, i \in S_\tau, j_\tau \le j}c_\tau(i,j) -
    q_{i,j}(S_\tau) \middle| \gF_{t-1} \right] = 0.
\end{align*}

If we count the number of $\tau$ that satisfies $i \in S_\tau$ and $j_\tau \le j$ is exactly $SC_t(i,j) = \sum_{\tau=1}^t \mathbbm{1}[i \in S_\tau, j_\tau \le j]$. Therefore, by Azuma-Hoeffding inequality, we have for fixed $SC_t(i,j)$, with probability at least $1 - \delta$,
\begin{align*}
    \left|\sum_{\tau \le t, i \in S_\tau, j_\tau \le j} c_\tau(i, j) - \sum_{\tau \le t, i \in S_\tau, j_\tau \le j} q_{i,j}(S_\tau)\right| \le \sqrt{2SC_t(i,j)\log(T/\delta)},
\end{align*}
By union inequality, we have 
\begin{align*}
    \left|\sum_{\tau \le t, i \in S_\tau j_\tau \le j} c_\tau(i, j) - \sum_{\tau \le t, i \in S_\tau, j_\tau \le j} q_{i,j}(S_\tau)\right| \le \sqrt{8SC_t(i,j)\log(NMT)}, 
\end{align*}
holds for any $t\in [T]$, $SC_t(i,j)$, and $(i, j) \in [N] \times [M]$ with probability at least $1 - T^{-2}$. We denote this good event as $\gE_1$ which satisfies $\Prob[\neg\gE_1] \le T^{-2}$.

We recall the definition of $\hat q_{i,j}^t$ given in \Cref{alg}
\begin{align*}
    \hat q_{i,j}^t = \frac{C_t(i,j)}{SC_t(i,j)} = \frac{\sum_{\tau \le t} c_{\tau}(i,j)}{SC_t(i,j)} = \frac{\sum_{\tau \le t} \1[i \in S_\tau]\cdot c_{\tau}(i,j)\mathbbm{1}[j_\tau \le j]}{SC_t(i,j)}.
\end{align*}
Under this good event $\gE_1$, we have for every $t \in [T]$ and $(i, j) \in [N] \times [M]$,
\begin{align*}
    \left| \hat q_{i,j}^t - \frac{\sum_{\tau \le t, i \in S_\tau, j_\tau \le j} q_{i,j}(S_\tau)}{SC_t(i,j)}\right| \le \sqrt{8\frac{\log(NMT)}{SC_t(i,j)}}
\end{align*}

Below we bound the difference between $q^*_{i,j}(S_t)$ and $q_{i,j}(S_t)$ for any $S_t \in \gS$. For given $(i, j)$ with $i \in S_t$, we have
\begin{align*}
    q^*_{i,j}(S_t) - q_{i,j}(S_t) &= \Prob[I_t \ni i, j_t = i \mid j_t \le j, S_t] - \Prob[i_t = i, j_t = j \mid j_t \le j, S_t] \\
    &= \Prob[I_t \ni i, i_t \neq i, j_t = j \mid j_t \le j, S_t] \\
    &\le \sum_{k \in S_t, k \neq i}\frac{\Prob[X_i \in M_j]\Prob[X_k \in M_j]}{\Prob[X_i \in M_{\le j}]\Prob[X_k \in M_{\le j}]} \\
    &\le (K-1)\cdot \frac{(L\epsilon)^2}{(j\epsilon/L)^2} = (K-1)\cdot L^4/j^2,
\end{align*}
where the last inequality holds by \Cref{ass:bi-lipschitz} and $\Prob[X_i \in M_{\le j}] = \sum_{j'=1}^j p_{i,j}^* \le j\frac{\epsilon}{L}, \forall i \in [N]$. 

Notice that for every $S_t \in \gS$ and $i \in S_t, j \in [M]$, we have
\begin{align*}
    q_{i,j}^*(S_t) &=
    \Prob[I_t \ni i, j = j_t \mid j_t \le j, S_t] \\
    &= \frac{\Prob[I_t \ni i, j = j_t \mid S_t]}{\Prob[j_t \le j \mid S_t]} \\
    &= \frac{\Prob[X_i(t) \in M_{j} \And x_k(t) \in M_{\le j}, \forall k \in S_t \mid S_t]}{\Prob[x_k(t) \in M_{\le j}, \forall k \in S_t \mid S_t]} \\
    &= \frac{\Prob[X_i \in M_j]\cdot \Prob[X_k \in M_{\le j}, \forall k \in S, k \neq i]}{\Prob[X_i \in M_{\le j}] \cdot \Prob[X_k \in M_{\le j}, \forall k \in S, k \neq i]} \\
    &= \frac{\Prob[X_i \in M_j]}{\Prob[X_i \in M_{\le j}]} \\
    &= \Prob[X_i \in M_j \mid X_i \in M_{\le j}]\\
    &= q_{i,j}^*.
\end{align*}
Therefore, we have 
\begin{align*}
    \left|\hat q_{i,j}^t - q^*_{i,j}\right| = \left|\hat q_{i,j}^t - \frac{\sum_{\tau \le t, i \in S_\tau j_\tau \le j}q_{i,j}^*(S_\tau)}{SC_t(i,j)}\right| \le \sqrt{8\frac{\log(NMt)}{SC_t(i,j)}} + (K-1)\cdot(L^4/j^2)
\end{align*}
\end{proof}
\end{lemma}

\subsection{Optimistic Estimation}

\begin{lemma}
\label{lemma:optimism}
    For $\beta_{i,j}^t$ given in \Cref{eq:def-beta}, under event $\gE_0$ and $\gE_1$, we have
    \begin{align*}
        \bar q_{i,j}^t \ge q^*_{i,j}.
    \end{align*}
    Moreover, by the offline $(1-\epsilon)$-approximated optimization oracle PTAS \citep{chen2013combinatorial}, we have
    \begin{align*}
        \bar r_q(S_t, \bar \vq^t) \ge (1-\epsilon) \cdot \bar r_q(S^*; \bar \vq^t).
    \end{align*}
\begin{proof}
Notice that in \Cref{alg} we define
\begin{align*}
    \bar q_{i,j}^t = \min\left\{\hat q_{i,j}^t + \beta_{i,j}^t + \frac{(K-1)L^4}{j^2}, 1\right\}.
\end{align*}
By \Cref{lemma:concentration-formal}, we have under $\neg\gE_0$ and $\gE_1$,
\begin{align*}
    \hat q_{i,j}^t \ge q_{i,j}^* - \beta_{i,j}^t - \frac{(K-1)L^4}{j^2},
\end{align*}
where the inequality holds by the definition of $\beta_{i,j}^t$ in \Cref{eq:def-beta} and $SC_{t-1}(i,j) \le SC_t(i,j)$.
Since $q_{i,j}^* \le 1$, we have
\begin{align*}
    \bar q_{i,j}^t \ge q^*_{i,j}.
\end{align*}

Since in \Cref{alg}, we set action $S_t \leftarrow \operatorname{PTAS}(\hat \vp^t)$ where $\hat\vp^t$ is converted from $\hat\vq^t$ by \Cref{eq:qstar-def}. Then by \Cref{lemma:r-q-r,lemma:monotone}, we have
\begin{align*}
    \bar r_q(S_t; \bar\vq^t) = \bar r(S_t; \bar\vp^t)  \ge (1-\epsilon)\max_{S \in \gS} \bar r(S; \bar \vp^t) \ge (1-\epsilon)\bar r(S^*; \bar \vp^t) = (1-\epsilon) \bar r_q(S^*; \bar \vq^t).
\end{align*}
\end{proof}
\end{lemma}

\subsection{Regret Decomposition}

\begin{lemma}
\label{lemma:tpm-formal}
Denote $Q_j^*(S_t) :=  \prod_{k \in S_t, j' > j} (1 - q_{k,j'}^*)$. We have
\begin{align}
    |\bar r_q(S_t; \bar \vq^t) - \bar r_q(S_t; \vq^*)| \le 2\sum_{i \in S_t, j \in [M]} Q_j^*(S_t) \cdot v_j \cdot \left|\bar q_{i,j}^t - q^*_{i,j}\right|.
\end{align}
\begin{proof}
This lemma is given by directly apply Lemma 3.3 in \citet{wang2023combinatorial} by definition of $\bar r_q$ in \Cref{eq:def-rq}.
\end{proof}
\end{lemma}

\begin{lemma}\label{lemma:regret-decomp}
Under \Cref{ass:bi-lipschitz}, we can bound the regret of \Cref{alg} by
\begin{align*}
    \gR(T) \le \E\left[\sum_{t=1}^T \texttt{Bonus}_t + \texttt{Bias}_t\middle| \gE_0, \gE_1\right] + 3T\epsilon + T^{-1},
\end{align*}
where $\texttt{Bonus}_t$ and $\texttt{Bias}_t$ is defined by
\begin{align}\label{eq:def-bonus-t}
    \texttt{Bonus}_t := 4 \sum_{i \in S_t, j \in [M]} Q_{j}^{*}(S_t) \cdot v_j \cdot \beta_{i,j}^t,
\end{align}
and
\begin{align}\label{eq:def-bias-t}
    \texttt{Bias}_t := 4 \sum_{i \in S_t, j \in [M]} Q_{j}^{*}(S_t) \cdot v_j \cdot (K-1)\frac{L^4}{j^2}.
\end{align}
\begin{proof}
This lemma formalize the first three steps of proof sketch. Denote $\Delta_t := r^*(S^*) - r^*(S_t)$, we have 
\begin{align*}
    \gR(T) = \E\left[\Delta_t\right].
\end{align*}
By \Cref{lemma:discrete-error}, we have
\begin{align*}
    \Delta_t &\le  \bar r(S^*; \vp^*) - \bar r(S_t; \vp^*) + 2\epsilon.
\end{align*}
Then we have
\begin{align*}
    \gR(T) &\le \Prob[\gE_0] \cdot \E\left[\sum_{t=1}^T\Delta_t \middle| \gE_0\right] + \Prob[\neg\gE_0] \cdot T \\
    &\le \E\left[\sum_{t=1}^T\Delta_t \middle| \gE_0\right] \\
    &\le \E\left[\sum_{t=1}^T\bar r(S^*; \vp^*) - \bar r(S_t; \vp^*) \middle| \gE_0\right] + 2T\epsilon,
\end{align*}
where the first inequality holds by property of conditional expectations and $\Delta_t \le 1$ and the second inequality is due to $\Prob[\neg\gE_0] = 0$.

Notice that under $\gE_0$ and $\gE_1$, by \Cref{lemma:monotone,lemma:optimism}, we have
\begin{align*}
    \bar r_q(S_t; \vq^t) \ge (1-\epsilon)\bar r_q(S^*; \bar \vq_t) \ge (1-\epsilon)\bar r_q(S^*; \vq^*).
\end{align*}
Then with \Cref{lemma:r-q-r-formal}, we have
\begin{align*}
    \gR(T) &\le \E\left[\sum_{t=1}^T\bar r_q(S^*; \vq^*) - \bar r_q(S_t; \vq^*) \middle| \gE_0\right] + 2T\epsilon \\
    &\le \E\left[\sum_{t=1}^T\bar r_q(S^*; \vq^*) - \bar r_q(S_t; \vq^*) \middle| \gE_0,\gE_1\right] + \Prob[\neg\gE_1]\cdot T +  2T\epsilon \\
    &\le \E\left[\bar r_q(S_t; \vq^t) - \bar r_q(S_t; \vq^*)\right] + 3T\epsilon + T^{-1},
\end{align*}
where the last inequality holds by $\epsilon\bar r_q(S^*;\vp^*) \le \epsilon$ and $\Prob[\neg\gE_1] \le T^{-2}$ shown in \Cref{lemma:concentration-formal}.  

Therefore, applying \Cref{lemma:tpm}, we get
\begin{align*}
    \gR(T) \le \E\left[\sum_{t=1}^T \texttt{Bonus}_t + \texttt{Bias}_t\middle| \gE_0, \gE_1\right] + 3T\epsilon + T^{-1},
\end{align*}
where $\texttt{Bonus}_t$ and $\texttt{Bias}_t$ is defined in \Cref{eq:def-bonus-t,eq:def-bias-t}.
\end{proof}
\end{lemma}

\subsection{Bounding the Bonus Terms}

We apply similar methods in \citet{wang2017improving,liu2023contextual} to give the bounds of $\sum_t\texttt{Bonus}_t$. We first give the following definitions.

\begin{definition}[{\citet[Definition 5]{wang2017improving}}]\label{def:TPgroup}
    Let $(i,j) \in [N] \times [M]$ be the index of binary arm and $l$ be a positive natural number, define the triggering probability group (of actions)
    \[
    S_j^l = \{S \in \mathcal{S} \mid 2^{-l} < Q_j^*(S) \leq 2^{-l+1}\}.
    \]
    Notice $\{S_j^l\}_{l \geq 1}$ forms a partition of $\{S \in \mathcal{S} \mid Q_j^*(S) > 0\}$.
\end{definition}
\begin{definition}[{\citet[Definition 6]{wang2017improving}}]\label{def:TPcounter}
    For each group $S_j^l$ (\Cref{def:TPgroup}), we define a corresponding counter $N_{i,j}^l$. 
    In a run of a learning algorithm, the counters are maintained in the following manner. 
    All the counters are initialized to $0$. In each round $t$, if the action $S_t$ is chosen, then update $N^l(i,j)$ to $N^l(i,j) + 1$ for every $(i, j)$ that $i \in S_t$, $S_t \in S_j^l$. 
    Denote $N_t^l({i,j})$ at the end of round $t$ with $N^l(i,j)$. 
    In other words, we can define the counters with the recursive equation below:
    \begin{align*}
        N_t^l(i,j) =
        \begin{cases} 
            0, & \text{if } t = 0, \\
            N_{t-1}^l(i,j) + 1, & \text{if } t > 0, i\in S_t, S_t \in S_j^l, \\
            N_{t-1}^l(i,j), & \text{otherwise}.
        \end{cases}
    \end{align*}
\end{definition}
\begin{definition}[{\citet[Definition 7]{wang2017improving}}]\label{def:TPevent}
Given a series of integers $\{l_{i,j}^{\max}\}_{i \in [N],j\in[M]}$, we say that the triggering is nice at the beginning of round $t$ (with respect to $l_{i,j}^{\max}$), if for every group $S_j^l$(\Cref{def:TPgroup}) identified by binary arm $(i,j)$ and $1 \leq l \leq l_{i,j}^{\max}$, as long as 
\[
\sqrt{\frac{8 \log (NMT)}{\frac{1}{3} N_{t-1}^l(i,j)\cdot 2^{-l}}} \leq 1,
\]
there is $SC_{t-1}(i,j) \geq \frac{1}{3} N_{t-1}^l(i,j) \cdot 2^{-l}$. We denote this event with $\gE_2(t)$. It implies
\[
\beta_{i,j}^t = \sqrt{\frac{8 \log(NMT)}{ SC_{t-1}(i,j)}} \leq \sqrt{\frac{8 \log(NMT)}{\frac{1}{3} N_{t-1}^l(i,j) \cdot 2^{-l}}}.
\]
\end{definition}
Therefore, we show that $\gE_2(t)$ happens with high probability for every $t$. 
\begin{lemma}[{\citet[Lemma 4]{wang2017improving}}]\label{lemma:TPprob}
For a series of integers $\{l_{i,j}^{\max}\}_{i \in [N],j\in[M]}$, $$\Prob[\neg \mathcal{E}_2(t)] \leq \sum_{i \in [N],j\in[M]} l_{i,j}^{\max} t^{-2},$$
for every round $t \geq 1$. 

\begin{proof}
We prove this lemma by showing $\Prob[N_{t-1}^l(i,j) = s, SC_{t-1}(i,j) \leq \frac{1}{3} N_{t-1}^l(i,j) \cdot 2^{-l}] \leq t^{-3}$, for any fixed $s$ with $0 \leq s \leq t - 1$ and $\sqrt{\frac{8 \log(NMT)}{\frac{1}{3} s \cdot 2^{-l}}} \leq 1$. Let $t_k$ be the round that $N^l(i,j)$ is increased for the $k$-th time, for $1 \leq k \leq s$. Let $Z_k = \1[S_{t_k} \ni i, j_{t_k} \le j]$ be a Bernoulli variable, that is, $SC_{t_k}(i,j)$ increase in round $t_k$. When fixing the action $S_{t_k}$, $Z_k$ is independent from $Z_1, \ldots, Z_{k-1}$. Since $S_{t_k} \in S_j^l$, $\mathbb{E}[Z_k \mid Z_1, \ldots, Z_{k-1}] \geq 2^{-l}$. Let $Z = Z_1 + \cdots + Z_s$. By multiplicative Chernoff bound \citep{upfal2005probability}, we have
\[
\Prob\left\{Z \leq \frac{1}{3} s \cdot 2^{-l}\right\} \leq \exp\left(-\frac{\left(\frac{2}{3}\right)^2 s \cdot 2^{-l}}{2}\right) \leq \exp\left(-\frac{\left(\frac{2}{3}\right)^2 18 \log t}{2}\right) < \exp(-3 \log t) = t^{-3}.
\]

By the definition of $SC_{t-1}(i,j)$ and the condition $N_{t-1}^l(i,j) = s$, we have $SC_{t-1}(i,j) \geq Z$. Thus
\begin{align*}
\Prob[N_{t-1}^l(i,j) = s, SC_{t-1}(i,j) &\leq \frac{1}{3} N_{t-1}^l(i,j) \cdot 2^{-l}]\\
&\leq \Prob[N_{t-1}^l(i,j) = s, Z \leq \frac{1}{3} s \cdot 2^{-l}] \\
&\leq \Prob[Z \leq \frac{1}{3} s \cdot 2^{-l}] \leq t^{-3}.
\end{align*}

By taking $i,j$ over $[N]\times[M]$, $l$ over $1, \ldots, l_{i,j}^{\max}$, $s$ over $0, \ldots, t - 1$ and applying the uninon bound, the lemma holds.
\end{proof}
\end{lemma}

\begin{lemma}\label{lemma:bonus-t-bound}
For given constant $C$, we have
\begin{align*}
    \sum_{t=1}^T \texttt{Bonus}(t) \le 16NM + 12288\frac{KNM^2\log(NMT)}{C} + TC + \frac{\pi^2}{6} \left\lceil \log_2\frac{16KM}{C} \right\rceil.
\end{align*}
\begin{proof}
For given constant $C$, we can define the following notations.
\begin{align}\label{eq:def-lmax}
    l_{i,j}^{\max} := \left\lceil \log_2 \frac{16KM}{C} \right\rceil, \quad \forall (i, j) \in [N] \times [M],
\end{align}
and for every integer $l$,
\begin{equation}\label{eq:def-kappa}
\begin{aligned}
    \kappa_{l,T}(C,s) := \begin{cases}
        2\cdot 2^{-l} & s =0 \\
        \sqrt{{96 \cdot 2^{-l} \log(NMT)}/s} & 1 \le s \le B_{l,T}(C) \\
        0 & s > B_{l,T}(C)
    \end{cases},
\end{aligned}
\end{equation}
where $B_{l,T}(C)$ is given by
\begin{align}\label{eq:def-BlT}
    B_{l,T}(C) := \left\lfloor{6144 \cdot 2^{-l}K^2M^2\log(NMT)}/{C^2}\right\rfloor.
\end{align}
By \citet[Lemma 5]{wang2017improving}, if $\texttt{Bonus}(t) \ge C$, under event $\gE_2(t)$, we have
\begin{align*}
    \texttt{Bonus}(t) \le \sum_{i \in S_t, j \in [M]} \kappa_{l_{i,j}, T}(C, N_{t-1}^{l_i}(i,j)),
\end{align*}
where $l_{i,j}$ is the index of group $S_j^{l_{i,j}} \ni S_t$. This is because we have
\begin{align*}
    \texttt{Bonus}(t) &\le -C + 8 \sum_{i \in S_t, j \in [M]} Q_j^*(S_t) \cdot (j - 1)\epsilon \cdot \min\{\beta_{i,j}^t, 1\} \\
    &\le 8 \sum_{i \in S_t, j \in [M]} \left(Q_j^*(S_t) \cdot \min\{\beta_{i,j}^t, 1\} - \frac{C}{8KM}\right)
\end{align*}

\noindent \textbf{Case 1: $1\le l_{i,j} \le l_{i,j}^{\max}$.} We have 
\begin{align*}
    Q^*_{j}(S_t) \le 2 \cdot 2^{-l_{i,j}}.
\end{align*}
Under $\gE_2(t)$, we have
\begin{align*}
    \min \left\{\beta_{i,j}^t, 1\right\} = \min \left\{\sqrt{\frac{8 \log(NMT)}{ SC_{t-1}(i,j)}},1\right\} \leq \min\left\{\sqrt{\frac{8 \log(NMT)}{\frac{1}{3} N_{t-1}^{l_{i,j}}(i,j) \cdot 2^{-l_{i,j}}}}, 1\right\},
\end{align*}
and
\begin{equation}\label{eq:Qbeta-bound}
\begin{aligned}
    Q^*_{j}(S_t)\cdot \min \left\{\beta_{i,j}^t,1\right\} &\le 2 \cdot 2^{-l_{i,j}} \cdot \min\left\{\sqrt{\frac{8 \log(NMT)}{\frac{1}{3} N_{t-1}^{l_{i,j}}(i,j) \cdot 2^{-l_{i,j}}}}, 1\right\} \\
    &\le \min\left\{\sqrt{\frac{96 \cdot 2^{-l_{i,j}} \log(NMT)}{ N_{t-1}^{l_{i,j}}(i,j) }}, 2 \cdot 2^{-l_{i,j}}\right\}.
\end{aligned}
\end{equation}
If $N_{t-1}^{l_{i,j}}(i,j) \ge B_{l_{i,j},T}(C) + 1$, then
\begin{align*}
    \sqrt{\frac{96 \cdot 2^{-l_{i,j}} \log(NMT)}{ N_{t-1}^{l_{i,j}}(i,j) }} \le \frac{C}{8KM},
\end{align*}
which implies $Q^*_{j}(S_t)\cdot \min \left\{\beta_{i,j}^t,1\right\} - C/8KM \le 0$. 

If $N_{t-1}^{l_{i,j}}(i,j) = 0$, we have $Q^*_{j}(S_t)\cdot \min \left\{\beta_{i,j}^t,1\right\} \le Q^*_{j}(S_t) \le 2\cdot 2^{-l_{i,j}}$, which implies
\begin{align*}
    Q^*_{j}(S_t)\cdot \min \left\{\beta_{i,j}^t,1\right\} - \frac{C}{8KM} \le \kappa_{l_{i,j},T}(C, 0)
\end{align*}

Otherwise, for $1 \le N_{t-1}^{l_{i,j}}(i,j) \le B_{l_{i,j}, T}(C)$, we have $Q^*_{j}(S_t)\cdot \min \left\{\beta_{i,j}^t,1\right\} \le \kappa_{l_{i,j},T}(C, N_{t-1}^{l_{i,j}}(i,j))$ by \Cref{eq:Qbeta-bound,eq:def-kappa}. Therefore, we get
\begin{align*}
    Q^*_{j}(S_t)\cdot \min \left\{\beta_{i,j}^t,1\right\} - \frac{C}{8KM} \le \kappa_{l_{i,j},T}(C, N_{t-1}^{l_{i,j}}(i,j))
\end{align*}

\noindent \textbf{Case 2: $l_{i,j} \ge l_{i,j}^{\max} + 1$.} We have
\begin{align*}
    Q^*_{j}(S_t)\cdot \min \left\{\beta_{i,j}^t,1\right\} \le 2 \cdot 2^{-l_{i,j}} \le 2 \cdot \frac{C}{16KM} \le \frac{C}{8KM},
\end{align*}
which shows that $Q^*_{j}(S_t)\cdot \min \left\{\beta_{i,j}^t,1\right\} - C/8KM \le 0$. If $N_{t-1}^{l_{i,j}}(i,j) = 0$. Therefore, we finally get
\begin{align*}
    \texttt{Bonus}(t) \le 8\sum_{i\in S_t, j \in [M]} \kappa_{l_{i,j}, T}\left(C, N_{t-1}^{l_{i,j}}(i,j)\right),
\end{align*}
for the case of good event $\gE_2(t)$ happens and $\texttt{Bonus}(t) \ge C$.

Notice that under good events $\gE_0, \gE_1$, we have
\begin{align*}
    \sum_{t=1}^T \texttt{Bonus}(t) &\le \sum_{t=1}^T \1[\{\texttt{Bonus}(t) \ge C\} \cap \gE_2(t)]\cdot \texttt{Bonus}(t) + T \cdot C + \sum_{t=1}^T \Prob[\gE_2(t)] \\
    &\le \underbrace{\sum_{t=1}^T 8 \cdot \sum_{i\in S_t, j \in [M]} \kappa_{l_{i,j}, T}\left(C, N_{t-1}^{l_{i,j}}(i,j)\right)}_{(I)} + TC + \frac{\pi^2}{6} \cdot \max_{i\in [N], j \in [M]} l_{i,j}^{\max} .
\end{align*}
where the first inequality is due to $\texttt{Bonus}(t) \le 1$ and definition, and the second one is due to \Cref{lemma:TPprob}. The key is bounding $(I)$:
\begin{align*}
    (I) &= 8\cdot \sum_{i \in [N], j \in [M]} \sum_{l=1}^\infty \sum_{s=0}^{N_{T-1}^{l}(i,j)}\kappa_{l}(C, s) \\
    &= 8 \cdot \sum_{i \in [N], j \in [M]} \sum_{l=1}^\infty \left(2\cdot 2^{-l} +  \sum_{s=1}^{B_{l,T}(C)}\sqrt{\frac{96 \cdot 2^{-l} \log(NMT)}{ s }} \right) \\
    &\le 8\cdot  \sum_{i \in [N], j \in [M]} \sum_{l=1}^\infty \left(2\cdot 2^{-l} +  2\cdot \sqrt{96 \cdot 2^{-l} \log(NMT)}\cdot \sqrt{B_{l,T}(C)} \right) ,
\end{align*}
where the inequality holds by the fact that $\sum_{s=1}^n \sqrt{1/s} \le 2\sqrt{n}$. Therefore, by the definition of $B_{l,T}(C)$ in \Cref{eq:def-BlT}, we have
\begin{align*}
    (I) &\le  8\cdot  \sum_{i \in [N], j \in [M]} \sum_{l=1}^\infty \left(2\cdot 2^{-l} +  1536\cdot \frac{2^{-l}KM\log(NMT)}{C} \right) \\
    &= 8\cdot  \sum_{i \in [N], j \in [M]}  \left(2+  1536\cdot \frac{KM\log(NMT)}{C} \right)\cdot \left(\sum_{l=1}^\infty 2^{-l}\right) \\
    &\le 16NM + 12288\frac{KNM^2\log(NMT)}{C}.
\end{align*}
Therefore, we get
\begin{align*}
    \sum_{t=1}^T \texttt{Bonus}(t) \le 16NM + 12288\frac{KNM^2\log(NMT)}{C} + TC + \frac{\pi^2}{6} \left\lceil \log_2\frac{16KM}{C} \right\rceil.
\end{align*}
\end{proof}
\end{lemma}


\subsection{Bounding the Bias Terms}
\begin{lemma}\label{lemma:bias-t-bound}
Under \Cref{ass:bi-lipschitz}, we have
\begin{align*}
    \sum_{t=1}^T \texttt{Bias}(t) \le 4K^2L^4 T\epsilon \log(M + 1).
\end{align*}
\begin{proof}
Notice that $\sum_{j\in[M]} 1/j \le \log(M+1)$ for $\epsilon < 1/2$, we have
\begin{align*}
    \texttt{Bias}(t) &\le 4K \cdot \sum_{i \in S_t,j \in [M]}Q^*_{j}(S_t) \cdot \epsilon L^4/j \\
    &= 4K^2L^4\epsilon \cdot \sum_{j \in [M]} \frac{1}{j} \\
    &\le 4K^2L^4 \epsilon\log(M + 1).
\end{align*}
Therefore, we have
\begin{align*}
    \sum_{t=1}^T \texttt{Gap}(t) \le 4K^2L^4 T\epsilon \log(M+1).
\end{align*}
\end{proof}
\end{lemma}



\subsection{Proof of \Cref{thm:main}}
\begin{theorem}[Formal version of \Cref{thm:main}]\label{thm:main-formal}
By setting $\beta_{i,j}^t$ in \Cref{eq:def-beta} and $\epsilon < 1/2$, we can control the regret of \Cref{alg} under \Cref{ass:bi-lipschitz} by 
\begin{align*}
    \gR(T) &\le  12289 \sqrt{NKM^2T\log(NMT)} +  T\epsilon\left(4KL^4\log(M+1) + 3\right)\\
    &\quad + 16 NM +  \pi^2\left(\log_2(\sqrt{KM^2T\log(NMT)/N}) + 5\right)/6 +  T^{-1} \\
    &= \wt{O}\left(\sqrt{NKM^2T} + L^4K^2T\epsilon\right),
\end{align*}
where $M = \ceil{1/\epsilon}$. If we further take $\epsilon = O\left(L^{-2}K^{-\frac{3}{4}}N^{\frac
{1}{4}}T^{-\frac{1}{4}}\right)$, we have
\begin{align*}
    \gR(T) = \wt{\gO}(L^{2}N^{\frac{1}{4}}K^{\frac{5}{4}}T^{\frac{3}{4}}).
\end{align*}
\begin{proof}
By \Cref{lemma:regret-decomp}, we have
\begin{align*}
    \gR(T) \le \E\left[\sum_{t=1}^T \texttt{Bonus}_t + \texttt{Bias}_t\middle| \gE_0, \gE_1\right] + 3T\epsilon + T^{-1},
\end{align*}
Take constant $C$ as
\begin{align}\label{eq:def-C}
    C:= \sqrt{\frac{NKM^2\log(NMT)}{T}}.
\end{align}
Then \Cref{lemma:bonus-t-bound} shows that
\begin{align*}
    \sum_{t=1}^T \texttt{Bonus}(t) \le 16NM + 12289\sqrt{NM^2KT\log(NMT)} + \pi^2\left(\log_2(\sqrt{KMT\log(NMT)/N}) + 5\right)/6.
\end{align*}
\Cref{lemma:bias-t-bound} demonstrates that
\begin{align*}
    \sum_{t=1}^T \texttt{Bias}(t) \le 4K^2L^4 T\epsilon \log(M + 1).
\end{align*}
Therefore, by calculating the summation of the bonus and bias terms, we can bound the regret by
\begin{align*}
    \gR(T) &\le \E\left[\sum_{t=1}^T \texttt{Bonus}(t) + \texttt{Gap}(t) \middle| \gE_0, \gE_1\right] +  T^{-1} + 3T\epsilon \\
    &\le  12289 \sqrt{NKM^2T\log(NMT)} +  T\epsilon\left(4KL^4\log(M+1) + 3\right)\\
    &\quad + 16 NM +  \pi^2\left(\log_2(\sqrt{KM^2T\log(NMT)/N}) + 5\right)/6 +  T^{-1} \\
    &= \wt{O}\left(\sqrt{NKM^2T} + L^4K^2T\epsilon\right),
\end{align*}
which finishes the proof.
\end{proof}
\end{theorem}

