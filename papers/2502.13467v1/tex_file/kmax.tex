\section{Algorithm for $K$-Max Bandits with General Continuous Distribution}\label{sec:general-continuous}

We now present our solution framework for continuous $K$-Max bandits, beginning with the fundamental regularity condition that enables discretization-based learning:
\begin{assumption}\label{ass:bi-lipschitz}
Each outcome distribution $D_i$ is supported on $[0,1]$ with a bi-Lipschitz continuous cumulative distribution function (CDF) $F_i$. Specifically, there exists $L \geq 1$ such that for any $i \in [N]$ and $0 \leq v < u \leq 1$:
\begin{align*}
    \frac{1}{L}(u - v) \leq F_i(u) - F_i(v) \leq L(u - v).
\end{align*}
\end{assumption}
Many studies on MAB or CMAB consider $[0, 1]$-supported arms \citep{abbasi2011improved,chen2013combinatorial,slivkins2019introduction,lattimore2020bandit}. The bi-Lipschitz continuity is also common in practice \citep{li2017provably,wang2019optimism,liu2023optimistic} and satisfied by many distributions such as (truncated) Gaussians, mixed uniforms, Beta distributions, etc.


\subsection{The Discretization of Countinuous $K$-Max Bandits}
\label{sec:discretized-K-Max}
Since it is complex to estimate the general continuous distributions, a natural idea is to perform \textit{discretization} with granularity $\epsilon$.
Below, we define the \textit{discrete $K$-Max bandits} (called $\bar\gB$) converted from the continuous $K$-Max bandits $\mathcal{B}^*$, where each discrete arm's outcome $\bar X_i$ is discretized from the continuous random variable $X_i$ under $\epsilon$:
\begin{align}\label{eq:discretize-outcome}
    \bar X_i = \sum_{j \in [M]} \1\left[X_i \in M_j\right] \cdot v_j,
\end{align}
where $M = \ceil{1/\epsilon}$\footnote{Without loss of generation, we can take $\epsilon$ such that $M\epsilon > 1$.} is the number of discretization bins, $M_j := [(j-1)\epsilon, j\epsilon)$ is the $j$-th bin, and $v_j := (j-1)\epsilon$ is the approximate value of $j$-th bin. 
%
We also let $M_{\le j} = \cup_{j' \le j} M_{j'}$ and $M_{\ge j} = \cup_{j' \ge j} M_{j'}$.
%
For simplicity, we denote $p^*_{i,j}$ as the probability that $X_i$ falls in $M_j$. For every $i \in [N]$ and $j \in [M]$,
\begin{align*}
    p^*_{i,j} := \Prob[X_i \in M_j] = \Prob[\bar X_i = v_j].
\end{align*}
Therefore, $\bar \gB$ only depends on the discrete probability set $\vp^* = \{p_{i,j}^* : i \in [N], j \in [M]\}$. Moreover, we set $\bar r(S; \vp^*)$ as the expected reward of an action $S$ in discrete $K$-Max bandits under the probability set $\vp^*$:
\begin{align*}
    \bar r(S ; \vp^*) &= \sum_{j\in [M]} v_j \cdot \Prob\left[\max_{i\in S} (\bar X_i) = v_j\right]
\end{align*}
A key observation is that $\max_{i\in S} (\bar X_i) = v_j$ is equivalent to $\max_{i\in S} (X_i)\in M_j$. This means $\Prob\left[\max_{i\in S} (\bar X_i) = v_j\right] = \Prob\left[\max_{i\in S} (X_i)\in M_j\right]$, which gives an upper bound for the discretization error as follows. The formal version is provided by \Cref{lemma:discrete-error-formal} (in \Cref{app:discrete-error}). 
\begin{lemma}\label{lemma:discrete-error}
For any $S \in \gS$, we have
$$|r^*(S) - \bar r(S; \vp^*)| \le \epsilon.$$ % for any $S \in \gS$.
\end{lemma} 





\subsection{Converting a Discrete Arm to a Set of Binary Arms}
\label{sec:discrete-binary}
Follow the classical process in \citet{wang2023combinatorial}, we can convert a discrete arm $X_i$ to a set of binary arms and estimate the parameters $\vq^* = \{q^*_{i,j} : i \in [N], j \in [M]\}$ instead of $\vp^*$, where
\begin{align}\label{eq:qstar-def}
    q_{i,j}^* := \frac{p_{i,j}^*}{1 - \sum_{j' > j} p_{i,j'}^*}, \ p^*_{i,j} = q^*_{i,j} \cdot \prod_{j' > j} (1 - q^*_{i,j'}).
\end{align}
Let $\{\bar Y_{i,j}\}_{i \in [N],j \in [M]}$ be independent binary random variables such that $\bar Y_{i,j}$ takes value $v_j$ with probability $q_{i,j}^*$, and value $0$ otherwise. Then $\max_{j \in [M]}\{\bar Y_{i,j}\}$ has the same distribution as $\bar X_{i}$.
For any $S \in \gS$, define $\bar r_q(S; \vq)$ as the expected maximum reward of $\{\bar Y_{i,j}\}_{i \in S, j \in [M]}$ with probability set $\vq$. 
Then we have 
\begin{lemma}\label{lemma:r-q-r}
    For any $\vp$ and $\vq$ satisfying \Cref{eq:qstar-def}, we have $$\bar r_q(S; \vq) = \bar r(S; \vp), \quad \forall S \in \gS.$$
\end{lemma}
The formal version of this lemma is given in \Cref{lemma:r-q-r-formal}. 
Moreover, the function $\bar r_q$ is monotone with respect to  $\vq$, i.e., 
\begin{lemma}[{\citet[Lemma 3.1]{wang2023combinatorial}}]
\label{lemma:monotone}
    For two probability set $\vq'$ and $\vq$ such that $q'_{i,j} \ge q_{i,j}$ holds for any $i \in [N], j \in [M]$, we have 
    $$\bar r_q(S; \vq') \ge \bar r_q(S;\vq),\quad  \forall S \in \gS.$$
\end{lemma}



\subsection{An Efficient Offline Oracle for Discrete $K$-Max Bandits} \label{sec:offline-oracle}
For any discrete $K$-Max bandits with probability set $\vp$, we can apply the \textit{PTAS} algorithm \citep{chen2016combinatorial} as a polynomial time offline $\alpha$-approximation optimization oracle for any given $\alpha < 1$. 
Moreover, for any probability set $\vq$, we can convert it to $\vp$ by \Cref{eq:qstar-def}, input this $\vp$ to the PTAS oracle and get the approxiamation solution $\operatorname{PTAS}(\vp)$ satisfying
\begin{equation}\label{eq:ptas}
\begin{aligned}
    \bar r_q\left(\operatorname{PTAS}(\vp); \vq\right) &= \bar r\left(\operatorname{PTAS}(\vp); \vp\right) \\
    &\ge \alpha \cdot \max_{S \in \gS} \bar r(S; \vp) = \alpha \cdot \max_{S \in \gS} \bar r_q(S; \vq).
\end{aligned}
\end{equation}
In the following algorithm, we set $\alpha = 1-\epsilon$ and control the relative error to achieve sublinear regret guarantees.



\subsection{Efficient Algorithm for Continuous $K$-Max Bandits}\label{sec:algorithm}
Building on the methodology in previous subsections, we adapt the framework in \citet{wang2023combinatorial}, and
present \texttt{DCK-UCB} (Discretized Continuous $K$-Max with Upper Confidence Bounds), the first efficient algorithm addressing $K$-Max bandits with general continuous outcome distributions. 
%
Generally speaking, we first discretize the countinuous $K$-Max bandits to discrete $K$-Max bandits. Then we convert every discrete arm to a set of binary arms, and estimate the corresponding $\vq^*$. Finally, we convert $\vq^*$ back to $\vp^*$, input $\vp^*$ to the $\operatorname{PTAS}$ oracle, and get the action we want to select.
%
% 

\begin{algorithm}[!t]
\caption{\texttt{DCK-UCB}: Discretization Continuous $K$-Max Bandits with Upper Confidence Bonus}
\label{alg}
\begin{algorithmic}[1]
\REQUIRE Discretization granularity $\epsilon$, upper confidence bonuses $\{\beta_{i,j}^t : i \in [N], j \in [M], t \in [T]\}$, and the offline $\alpha$-approximated optimization oracle $\operatorname{PTAS}$ for discrete $K$-Max bandits \citep{chen2016combinatorial}.
\STATE Initialize $M \leftarrow \ceil{1/\epsilon}$, $\hat q_{i, 1}^1 \leftarrow 1$ for every $i \in [N]$, and $\hat q_{i,j}^1 \leftarrow 0$ for every $i\in [N], j > 1$.
\FOR{$t=1,2,\ldots,T$}
\STATE For every $i \in [N], j \in [M]$, set   
\begin{align}\label{eq:def-barq}
    \bar q_{i, j}^t \leftarrow \min\left\{ \hat q_{i,j}^t + \beta_{i,j}^t + (K-1)\frac{L^4}{j^2}, 1\right\}.
\end{align}
% \COMMENT{Construct optimistic estimator $\bar q_{i,j}^t$.}
\STATE Convert $\bar \vq^t$ to $\bar \vp^t$ by \Cref{eq:qstar-def}.
\STATE \label{algline:oracle} Choose action $S_t \leftarrow \operatorname{PTAS}(\bar \vp^t)$. 
\STATE Observe $(r_t,i_t)$ by executing action $S_t$. Denote $j_t$ as the range number of $r_t$, i.e., $r_t \in M_{j_t}$.
% \COMMENT{Execute $S_t$ and receive value-index feedback.}
\STATE For any $i, j \in [N] \times [M]$,
\begin{align*}
    C_t(i, j) = 
        C_{t-1}(i, j) + \1\left[ i = i_t \And j = j_t \right]
\end{align*}
and
\begin{align*}
    SC_t(i, j) = 
        SC_{t-1}(i, j) + \1\left[ i \in S_t \And j \ge j_t \right]
\end{align*}
% \COMMENT{Update the counter $C_t$ and $SC_t$ for estimation.}
\STATE Calculate estimator $\hat q_{i,j}^{t+1} \leftarrow \frac{C_t(i,j)}{SC_t(i,j)}$, for every $i \in [N]$ and $j\in [M]$.
% \COMMENT{Estimate $q^*_{i,j}$ by $\hat q_{i,j}^{t+1}$.} 
\ENDFOR
\end{algorithmic}
\end{algorithm}



\Cref{alg} presents the pseudo-code of \texttt{DCK-UCB}. In Line 3, we calculate the optimistic estimator $\bar q_{i,j}^t$ which upper bounds $q^*_{i,j}$ with high probability.
%
This is done by adding two upper confidence bonus terms $\beta_{i,j}^t$ and $(K-1)L^4/j^2$.
%
Analysis shows that $\bar q_{i,j}^t \ge q^*_{i,j}$ with high probability (Lemma \ref{lemma:concentration}). The detailed discussion on this estimator will be given in the following paragraphs.
%
In Lines 4-5, the agent converts this $\bar \vq$ to $\bar \vp$, and then runs the offline $\alpha$-approximation optimization oracle $\operatorname{PTAS}$ with $\alpha = 1 - \epsilon$ to get action $S_t$ for execution.
%
In Line 6, the agent gets the value-index return $(r_t, i_t)$, and discretizes the value $r_t$ to the index of bin $j_t$, i.e., $r_t \in M_{j_t}$.
%
In Lines 7-8, the agent estimates $\vq^*$ by two counters: $C_t(i,j)$ counts the times when $(i,j)$ exactly equals the feedback $(i_t,j_t)$, and $SC_t(i,j)$ counts the number of steps $\tau \le t$ satisfying $i \in S_\tau$ and $j_\tau \le j$. As outlined in \Cref{alg}, each step of the algorithm has polynomial time and space complexity, which demonstrates the computational tractability of \texttt{DCK-UCB}. 


\paragraph{Biased Estimator.} 


The key challenge in the algorithm design and theoretical analysis is that $\hat q_{i,j}^t$ is not an unbiased estimator for $q_{i,j}^*$. This means that except for the confidence radius due to the randomness of the environment, we still need another bonus term to bound the bias to guarantee that $\bar q_{i,j}^t$ is a UCB for $q_{i,j}^*$. 
%

Specifically, note that 
\begin{eqnarray*}
    q^*_{i,j} = \frac{p^*_{i,j}}{1 - \sum_{j' > j} p^*_{i,j'}} = \frac{p^*_{i,j}}{\sum_{j'=1}^j p^*_{i,j'}} = \frac{\Prob[X_i \in M_j]}{\Prob[X_i \in M_{\le j}]} 
\end{eqnarray*}
If we have an assumption that when $i\in S_\tau$ and $j_\tau = j$, $X_i(\tau) \in M_j$ implies $i = i_\tau$, then we can guarantee that $\hat q_{i,j}^{t} = C_t(i,j) / SC_{t}(i,j)$ is an unbiased estimator for $q^*_{i,j}$. This is because that in this case, $\frac{C_t(i,j)}{ SC_{t}(i,j) }= \frac{\# \text{ of } i_\tau = i, j_\tau = j}{\# \text{ of } i\in S_\tau, j_\tau \le j} $ is the fraction of $X_i(\tau) \in M_j$ condition on $i\in S_\tau, j_\tau \le j$, 
%
which is an unbiased estimator for 
\begin{eqnarray*}
    && \Prob[X_i(\tau) \in M_j \mid i\in S_\tau, j_\tau \le j]\\
    &=&  \frac{\Prob[X_i(\tau) \in M_j, i\in S_\tau, j_\tau \le j]}{\Prob[i\in S_\tau, j_\tau \le j]}\\
    &=&\frac{\Prob[X_i(\tau)  \in M_j]\cdot \Prob[X_k(\tau)  \in M_{\le j}, \forall k \in S_{\tau}, k \neq i]}{\Prob[X_i(\tau)  \in M_{\le j}] \cdot \Prob[X_k(\tau)  \in M_{\le j}, \forall k \in S_{\tau}, k \neq i]}\\
    &=&\frac{\Prob[X_i(\tau) \in M_j]}{\Prob[X_i(\tau) \in M_{\le j}]}
\end{eqnarray*}

However, we know that in the discrete K-Max bandits converted from the continuous K-Max bandits, there is no such assumption (different from \citet{wang2023combinatorial} who requires deterministic tie-breaking rule). When multiple arm has $X_i(\tau) \in M_j$, the observed winning arm $i_t = \arg\max X_i(\tau)$ is not a fixed one, and even we do not know the distribution of the winner. 
%
Because of this, we cannot guarantee that condition on $i\in S_\tau, j_\tau \le j$, we increase the counter for every time $X_i(\tau) \in M_j$. Some steps that $X_i(\tau) \in M_j$ but $X_i(\tau)$ is not the winner are missed. 
%
This nondeterministic tie-breaking effect, arising from the continuous-to-discrete transformation, induces systematic negative bias in conventional estimators $\{\hat q_{i,j}^t\}$.
%
Therefore, to guarantee that our used $\{\bar q_{i,j}^t\}$ is an upper confidence bound of $\{ q_{i,j}^*\}$, we need another bonus term (i.e., the term $(K-1)\frac{L^4}{j^2}$), given by a novel concentration analysis with bias-aware error control. This is shown in the following key lemma, where the formal version is in \Cref{lemma:concentration-formal}.


\begin{lemma}\label{lemma:concentration}
Under \Cref{ass:bi-lipschitz}, let the confidence radius be defined as  
\begin{align}\label{eq:def-beta}
    \beta_{i,j}^t := \sqrt{8\frac{\log(NMt)}{SC_{t-1}(i,j)}}.
\end{align}
Then with probability at least $1 - t^{-2}$,
\begin{align}
    \left|\hat q_{i,j}^t - q^*_{i,j}\right| \le {\beta_{i,j}^t} + (K-1)\cdot(L^4/j^2),
\end{align}
holds for every $t \in [T]$, $i \in [N]$ and $j \in [M]$.
\end{lemma}

The bound in \Cref{lemma:concentration} decomposes into an exploration bonus term $\beta_{i,j}^t$ and a bias compensation term $(K-1)\frac{L^4}{j^2}$. The exploration bonus term arises from the randomness of the environment, which is almost the same with existing researches \citep{wang2017improving,liu2023contextual,wang2023combinatorial}.
The bias compensation term, on the other hand, comes from the nondeterministic tie-breaking effect in the continuous-to-discrete transformation. 
%
As we have explained, this term is because that condition on $i\in S_\tau, j_\tau \le j$, there are some time steps that $X_i(\tau) \in M_j$ but arm $i$ is not the winner and thus we miss these steps in counter $C_{i,j}^t$.
%
When this happens, we know that there must be at least one other arm $i'\ne i, i'\in S_{\tau}$ such that $X_{i'}(\tau) \in M_j$.
%
This probability can be upper bounded by 
\begin{align*}
    &\sum_{i'\ne i, i'\in S_{\tau}} \Prob[X_i(\tau) \in M_j, X_{i'}(\tau) \in M_j \mid i\in S_\tau, j_\tau \le j]\\
    =\!&\sum_{i'\ne i, i'\in S_{\tau}} \frac{p^*_{i,j}p^*_{i',j} }{ \sum_{j'\le j}p^*_{i,j'} \sum_{j'\le j}p^*_{i',j'}} \le (K-1)\frac{(L\epsilon)^2}{(j\epsilon/L)^2},
\end{align*}
where the last inequality is because of bi-Lipschitz assumption \Cref{ass:bi-lipschitz}.


Notably, the bias term dominates for small $j$ values due to the influence of other arms becomes higher when condition on $j_\tau \le j$ with smaller $j$.
%
However, our regret analysis in \Cref{sec:result} suggests that the amplified bias for small $j$ has diminishing impact on cumulative regret -- a crucial property enabling our sublinear regret guarantee.




\subsection{Theoretical Results}
\label{sec:result}
We establish the first efficient algorithm \texttt{DCK-UCB} (\Cref{alg}) which enjoys the sublinear regret guarantees in continuous $K$-Max bandits problem with value-index feedback. 
\begin{theorem}
\label{thm:main}
Under \Cref{ass:bi-lipschitz}, let the offline optimization oracle be a PTAS implementation \citep{chen2016combinatorial}. Given the exploration bonus term $\beta_{i,j}^t$ in \Cref{eq:def-beta}, discretization granularity $\epsilon = \gO(L^{-2}K^{-3/4}N^{1/4}T^{-1/4})$ and PTAS approximation factor $\alpha = 1 - \epsilon$, \Cref{alg} enjoys the regret guarantee
\begin{align*}
    \gR(T) \le \wt{\gO}(L^{2}N^{\frac{1}{4}}K^{\frac{5}{4}}T^{\frac{3}{4}}).
\end{align*}
\end{theorem}
The formal statement with precise constants appears in \Cref{thm:main-formal}. Our analysis reveals that careful calibration of the discretization-error versus statistical-estimation trade-off enables the first sublinear regret guarantee $\gO(T^{3/4})$ for continuous $K$-Max bandits. 

\paragraph{Comparison to Prior Works.} The $\gO(T^{3/4})$ regret upper bound of \texttt{DCK-UCB} (\Cref{alg}) shown in \Cref{thm:main} advances the state-of-the-art in several directions. \citet{wang2023combinatorial} can achieve an $\gO(\sqrt{T})$ regret upper bound in the discrete $K$-Max bandits, but their algorithm cannot work for the continuous case due to non-zero discretization error and nondeterministic tie-breaking.
%
Recent work on submodular bandits \citep{pasteris2023sum,fourati2024combinatorial} attains $O(T^{2/3})$ regret via greedy oracles, but this approach suffers dual limitations: (1) The baseline of their regret is $\sum_{t=1}^T (1-1/e)r^*(S^*)$, but not $\sum_{t=1}^T r^*(S^*)$. In our definition, their regret becomes linear.   (2) Their algorithm requires the availability of submitting any subset of $\mathcal{A}$ with size less than or equal to $K$, which may not be practical in some applications, such as recommendation systems or portfolio selection that need to always submit size $K$ subsets. Our framework resolves both issues through our novel bias-corrected estimators with PTAS integration, which is both efficient and effective in dealing with continuous $K$-Max bandits.


\subsection{Proof Sketch of \Cref{thm:main}}
In this section we outline the proof of \Cref{thm:main}, which consists of four main steps. 

\paragraph{Step 1: From continuous regret to discretized regret.}
Let $\Delta_t := r^*(S^*) - r^*(S_t)$ be the regret for each round $t$. To control the regret, we aim to bound the summation of $\Delta_t$. 
$$ \gR(T) = \E\left[\sum_{t=1}^T \Delta_t \right]. $$
We first transfer the regret from continuous $K$-Max bandits to the discrete case. With \Cref{lemma:discrete-error}, we have
\begin{align*}
    \Delta_t &\le  \bar r(S^*; \vp^*) - \bar r(S_t; \vp^*) + 2\epsilon.
\end{align*}


\paragraph{Step 2: From discretized regret to estimation error.}
%
Recall the definition of $\bar r_q$ in \Cref{sec:discrete-binary}, we have $\bar r_q(S; \vq) = \bar r(S; \vp)$ for any $S \in \gS$, and probability set $\vp, \vq$ satisfying \Cref{eq:qstar-def}. 
By the monotonicity of $\bar r_q$ (in \Cref{lemma:monotone}) and the concentration analysis (in \Cref{lemma:concentration}),
we have with high probability, $\forall (i,j)\in[N]\times[M]$, $\bar q_{i,j}^t \ge q^*_{i,j}$ holds for any $t \in [T]$, which implies
\begin{align*}
    \bar r_q(S^*;\bar\vq^t) \ge \bar r_q(S^*; \vq^*).
\end{align*}
Moreover, by the property of $\alpha$-approximated offline optimization oracle PTAS \citep{chen2016combinatorial}  (in \Cref{eq:ptas}) with $\alpha = 1 - \epsilon$, 
\begin{align*}
    (1 - \epsilon) \bar r_q(S^*; \bar \vq^t) \le (1 - \epsilon) \max_{S \in \gS} \bar r_q(S; \bar \vq^t) \le \bar r_q(S_t; \bar \vq^t),
\end{align*}
which implies the conversion from $\Delta_t$ to the estimation error term
\begin{align*}
    \Delta_t &\le \bar r_q(S^*; \bar \vq^t) - \bar r_q(S_t; \vq^*) + 2\epsilon \\
    &\le (1 - \epsilon) \bar r_q(S^*; \bar \vq^t) - \bar r_q(S_t; \va^*) + 3\epsilon \\
    &\le \bar r_q(S_t; \bar \vq^t) - \bar r_q(S_t; \vq^*) + 3\epsilon.
\end{align*}
Therefore, we then focus on bounding the estimation error $\bar\Delta_t := \bar r_q(S_t; \bar \vq^t) - \bar r_q(S_t; \vq^*)$ to guarantee the sublinear regret upper bound:
\begin{align}\label{eq:regret-to-bar-delta}
    \gR(T) = \E\left[\sum_{t=1}^T \Delta_t \right] \le \E\left[\sum_{t=1}^T \bar\Delta_t \right] + 3T\epsilon.
\end{align}


\paragraph{Step 3: Decompose the estimation error.}
By similar methods as achieving the Triggering Probability Modulated (TPM) smoothness condition in cascading bandits \citep{wang2017improving} and $K$-Max bandits for binary distributions \citep{wang2023combinatorial}, we propose the following lemma.
\begin{lemma}
\label{lemma:tpm}
Denote the probability of event $\{j_t \le j\}$ as 
$$Q_j^*(S_t) :=  \prod_{k \in S_t, j' > j} (1 - q_{k,j'}^*).$$ Then we have
\begin{align}
    \bar \Delta_t \le 2\sum_{i \in S_t, j \in [M]} Q_j^*(S_t) \cdot v_j \cdot \left|\bar q_{i,j}^t - q^*_{i,j}\right|.
\end{align}
\end{lemma}
Equipped with \Cref{lemma:tpm}, we decompose $\bar \Delta_t$ into two parts through our novel concentration analysis in \Cref{lemma:concentration} and the definition of optimistic estimator $\bar q_{i,j}^t$ in \Cref{eq:def-barq}
\begin{equation}\label{eq:bar-delta}
\begin{aligned}
    \bar \Delta_t &\le \quad \underbrace{4 \sum_{i \in S_t, j \in [M]} Q_{j}^{*}(S_t) \cdot v_j \cdot \beta_{i,j}^t}_{\texttt{Bonus}_t} \\
    &\quad + \underbrace{4 \sum_{i \in S_t, j \in [M]} Q_{j}^{*}(S_t) \cdot v_j \cdot (K-1)\frac{L^4}{j^2}}_{\texttt{Bias}_t}.
\end{aligned}
\end{equation}


\paragraph{Step 4: Bound the Bonus and Bias terms.}
For the Bonus term, we apply standard analysis for combinatorial bandits with triggering arms in \citep{wang2017improving, liu2023contextual} where we encounter $NM$ binary arms in total and select $KM$ binary arms in every action and get
\begin{align}\label{eq:bonus-sum}
    \E\left[\sum_{t=1}^T \texttt{Bonus}_t \right] \le \wt\gO\left(\sqrt{(NM) \cdot (KM) \cdot T}\right).
\end{align}

To control the bias terms, we recall that $v_j = (j-1)\epsilon$. 
Therefore, we can write
\begin{equation}\label{eq:bias}
    \begin{aligned}
    \texttt{Bias}_t &\le 4K^2L^4 \sum_{j \in [M]} (j-1)\epsilon/j^2 \\
    &\le \gO\left( K^2L^4\epsilon\log(M) \right),
\end{aligned}
\end{equation}

Therefore, combining \Cref{eq:regret-to-bar-delta,eq:bar-delta,eq:bonus-sum,eq:bias}, the regret can be bounded by
\begin{align*}
    \gR(T) &\le \E\left[\sum_{t=1}^T \texttt{Bonus}_t + \texttt{Bias}_t\right] + \gO(T\epsilon) \\
    &\le \wt{\gO}\left(\sqrt{NKM^2T} + K^2L^4T\epsilon\right),
\end{align*}
where $M = \ceil{1/\epsilon}$. By taking $\epsilon = \gO(T^{-\frac{1}{4}}K^{-\frac{3}{4}}N^{\frac{1}{4}}L^{-2})$, we have
\begin{align*}
    \gR(T) \le \wt{\gO} \left( L^2N^{\frac{1}{4}}K^{\frac{5}{4}}T^{\frac{3}{4}} \right).
\end{align*}

