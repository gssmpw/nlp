\section{Preliminaries}\label{sec:preliminaries}

We study the \textit{continuous $K$-Max bandits}, denoted as $\mathcal{B}^*$, where an agent interacts with $N$ arms $\mathcal{A} = [N]$. For each arm $i \in [N]$, there is a corresponding continuous random distribution $D_i$ such that $X_i \sim D_i$, where $X_i$ is the outcome of arm $i$.

The agent will play $T$ rounds in total.
At each time step $t \in [T]$, the agent needs to select an action $S_t$ from the feasible action set $\mathcal{S}=\{S\subseteq\mathcal{A}\mid|S|=K\}$, i.e., a subset of $\mathcal{A}$ with size $K$. Here $1 < K < N$ is a given constant. 
After selecting $S_t$, the environment first samples outcomes $X_i(t)\sim D_i$ for all $i\in S_t$, and all the random variables $X_i(t)$ (for different $i,t$ pairs) are sampled independently. 
Then the environment returns \textit{value-index feedback} $(r_t,i_t)$, where $r_t=\max_{i\in S_t}X_i(t)$ is the maximum outcome, and $i_t=\argmax_{i\in S_t}X_i(t)$ is the index of the arm that achieves this maximum outcome. 
Besides, $r_t$ is also the reward of the agent in time step $t$.
Note that ties, shall they occur (we denote this event as $\neg\mathcal{E}_0$), are resolved in an arbitrary manner, although $\mathbb{P}[\neg\mathcal{E}_0]=0$ since $D_i$'s are continuous distributions. 
We denote the expected reward of an action $S$ as $r^*(S)$, which is given by:
\begin{align*}
    r^*(S) := \E[\max\{X_i : i \in S\}] = \int_0^1 r\cdot \rd\Prob_{\max\{X_i : i \in S\}}(r).
\end{align*}
The objective of the $K$-Max bandits is to select actions $S_t$ properly to maximize the cumulative reward $\sum_{t=1}^T r^*(S_t)$ in $T$ rounds.

Let $S^*$ denote the optimal action $S^* := \argmax_{S \in \gS} r^*(S)$. We evaluate the performance of the agent by the regret metric, which is defined by
\begin{align}\label{eq:def-regret}
    \gR(T) := \E\left[ \sum_{t=1}^T r^*(S^*) - r^*(S_t) \right],
\end{align}
where the expectation is taken over the uncertainty of $\{S_t\}_{t=1}^T$. 
