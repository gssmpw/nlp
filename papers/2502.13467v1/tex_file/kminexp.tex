\section{Better Performance in a Special Case: Exponential Distributions}
\label{sec:kminexp}

\newcommand{\Exp}{\operatorname{Exp}}

In this section, we demonstrate how specific distributional structure enables the improvement of the regret guarantee from $\wt{\gO}(T^{\frac{3}{4}})$ to $\wt{\gO}(\sqrt{T})$. Specifically, we investigate the special case where each distribution $D_i$ for $i \in [N]$ follows the exponential distribution with linear parameterization. 

Exponential distributions naturally model arrival or failure times in networked systems, job completion times in distributed computing, and service durations in queuing systems. 
A canonical application arises in server scheduling, where the goal is to select $K$ servers to minimize the service latency. Here, each server's latency can be modeled as an exponential random variable with a rate parameter $\mu_i$, and the overall performance of the $K$ selected servers is the lowest latency achieved among them. 
%
Here, the random outcome $X_i$ can be viewed as a random loss, and the winning loss is the minimum one. 
Moreover, we consider a linear parameterization to parameter $\mu_i$, which allows incorporating features like distance, traffic, or weather conditions into the model.


\subsection{The $K$-Min Exponential Bandits}

Based on the intuition, in this section we consider a special case of $K$-Max bandits: the $K$-Min exponential bandits.
Here each arm $i$ generates loss $X_i$ from an exponential distribution with linear parameterization. 
%
Specifically, each outcome distribution is an exponential distribution, i.e., $X_i \sim D_i = \Exp(\mu_i)$ where $\mu_i > 0$ is the parameter of arm $i$. Moreover, we assume that there exists a $d$-dimension unknown parameter $\theta^* \in \R^d$ and a known feature mapping $\phi : [N] \to \R^d$ such that $\mu_i = \langle \phi(i), \theta^* \rangle$ holds for any $i \in [N]$. The feature mapping $\phi$ satisfies that $\|\phi(i)\|_2 \le 1$ and the unknown parameter $\theta^*$ satisfies $\theta^* \in \Theta  \subset \R^d$, where $\sup_{\theta \in \Theta} \|\theta\|_2 \le V$.
%
The agent observes \textit{only} the minimum loss $\ell_t = \min_{i \in S_t} X_i(t)$ after playing subset $S_t \in \mathcal{S} = \{S \subseteq [N] : |S| = K\}$.
That is, we consider the weaker full bandit feedback case.

Let $\ell^*(S) := \E[\ell_t \mid S]$ be the expected loss for action $S \in \gS$, we further denote the best action $S^* = \argmin_{S \in \gS} \ell^*(S)$ and similarly introduce the regret metric to evaluate the performance of this agent:
\begin{align*}
    \gR(T) = \E\left[\sum_{t=1}^T \ell^*(S_t) - \ell^*(S^*)\right].
\end{align*}

Note that we can let $Z_i(t)= - X_i(t)$ and view $Z_i(t)$ as a kind of reward, and let $r_t = \max_{i \in S_t} Z_i(t)$. Then we can see that $\ell_t = \min_{i \in S_t} X_i(t) = \min_{i \in S_t} -Z_i(t) = - \max_{i \in S_t} Z_i(t) = -r_t$. By this way, we can view $K$-Min exponential bandits as a special case of $K$-Max bandits. 
%
However, one important difference is that in $K$-Min exponential bandits, we do not have value-index feedback, i.e., we do not know the winner's index. This is a full \textit{bandit feedback} setting, and making $K$-Min exponential bandits even more challenging. 

\subsection{Algorithm and Results}

The key observation in $K$-Min exponential bandits is that the minimum of several exponential distributions still follows an exponential distribution. That is, we have 
\begin{align*}
    \min_{i \in S} X_i \sim \Exp\left(\sum_{i \in S} \mu_i\right) = \Exp\left(\sum_{i \in S} \langle \phi(i), \theta^*\rangle \right).
\end{align*}
Therefore, it becomes much easier to estimate the true parameter $\theta^*$ by MLE. 
%
Specifically, let $\psi(S)  := \sum_{i \in S} \phi(i)$, $\forall S \in \gS$. Then with chosen action $S_t$ and parameter $\theta$, the observed loss should follow the exponential distribution $\Exp\left(\sum_{i \in S} \phi(i)^T \theta \right) = \Exp\left(\psi(S)^T\theta\right)$, whose probability density function is $f(x) = \psi(S)^T\theta  e^{\left(-\psi(S)^T\theta  x\right)}$. 
%
Because of this, the log-likelihood function is 
\begin{equation}
\begin{aligned}
    L_t(\ell_t; S_t, \theta) :&= - \log \left( \psi(S_t)^\top \theta e^{\left( -\psi(S_t)^\top \theta \ell_t\right)} \right).
\end{aligned}
\end{equation}

Denote $\gL_t(\theta)$ as the summation of $L_t$ and a regularization term
\begin{align}\label{eq:loglikelihood-def}
    \gL_t(\theta; \lambda) := \sum_{i < t} L_i(\ell_i; S_i, \theta) + \frac{\lambda}{2}\|\theta\|^2,
\end{align}
where $\lambda$ is the regularization factor. Then we present the algorithm \texttt{MLE-Exp} for $K$-Min exponential bandits in \Cref{alg:k-min}.

\begin{algorithm}
\caption{\texttt{MLE-Exp}: MLE for $K$-Min Exponential Bandits}
\label{alg:k-min}
\begin{algorithmic}[1]
\REQUIRE Regularization factors $\{\lambda_t\}_{t\in [T]}$, confidence radius $\{\gamma_t\}_{t\in [T]}$, and probability constant $\delta$.
\FOR{$t = 1, \ldots, T$}
    \STATE Compute MLE $\hat\theta_t$ by
    $$\hat{\theta}_t \leftarrow \argmin_{\theta \in \R^d} \mathcal{L}_t(\theta; \lambda_t),$$
    where $\gL_t(\theta;\lambda)$ is given in \Cref{eq:loglikelihood-def}. 
    % \COMMENT{Estimate $\theta^*$ by MLE $\hat{\theta}_t$.}
    \STATE Construct the confidence set $C_t(\hat\theta_t; \delta, \lambda_t)$ according to \Cref{eq:def-confidence-set}
    \STATE $(S_t, \wt{\theta}_t) \leftarrow \argmax_{S \in \gS, \theta \in C_t(\hat{\theta}_t; \delta, \lambda_t)} \langle \psi(S), \theta \rangle$ 
    % \COMMENT{Choose action with minimum expected loss.}
    \STATE Play action $S_t$ and observe the loss $\ell_t$. 
    % \COMMENT{Execute action $S_t$ and receive full-bandit feedback.}
\ENDFOR
\end{algorithmic}
\end{algorithm}

In Line 2 of \Cref{alg:k-min}, we estimate the MLE $\hat\theta_t$ by minimizing the summation of the log-likelihood function and the regularization term $\gL_t(\theta, \lambda_t)$. 
%
Given $\lambda_t$ a priori, we will write $\gL_t(\theta)$ instead of $\gL_t(\theta, \lambda_t)$ for simplicity. 
%
Inspired by \citet{liu2024almost, lee2024unified, liu2024combinatorial}, in Line 3, we construct a confidence set $C_t(\hat\theta_t;\delta)$, centered at the MLE $\hat\theta_t$ with confidence radius $\gamma_t(\delta)$, based on the gradient term $g_t(\theta) := -\nabla_\theta \gL_t(\theta) + \sum_{i < t} \ell_i \psi(S_i)$ and Hessian matrix $H_t(\theta) := \nabla^2_\theta \gL_t(\theta)$:
\begin{align}\label{eq:}
    C_t(\hat\theta_t; \delta) :=  
    &\left\{ \theta \in \Theta : \left\|g_t(\theta) - g_t(\hat\theta_t)\right\|_{H_t^{-1}(\theta)} \le \gamma_t(\delta)\right\},
\end{align}
where $\gamma_t$ is the confidence radius. 
Then in Line 4, we apply a double oracle to look for the action $S$ whose expected loss under a parameter $\theta$ in the confidence set ($1/\langle \psi(S), \theta \rangle$) is minimized.
%
Finally, we select this greedy action in Line 5 and use the observation to update the next time step's MLE and confidence set. 





The regret guarantees of \Cref{alg:k-min} is given below.

\begin{theorem}
\label{thm:kminexp}
With $\delta = 1/T$, $\lambda_t = \Theta(d\log T)$, and $\gamma_t = \Theta(\sqrt{d\log T})$, \Cref{alg:k-min} satisfies:
\begin{align*}
    \mathcal{R}(T) \leq \widetilde{\gO}\left(\sqrt{d^3 T}\right).
\end{align*}
\end{theorem}
Compared with the $O(T^{\frac{3}{4}})$ regret upper bound for general continuous K-Max bandits, here the regret upper bound is reduced to $O(T^{\frac{1}{2}})$ (which is nearly minimax optimal) even without the feedback of winner's index, due to the utilization of the exponential distribution's property. In short, we do not need to use a discretization method and can directly construct an unbiased estimator for the known parameter $\theta^*$.
%
The proof is inspired by previous analysis of general linear bandits \citep{lee2024unified,liu2024almost} and logistics bandits \citep{liu2024combinatorial}, and we defer the detailed proof to \cref{Appendix:k-min}.
