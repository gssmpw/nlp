

\begin{figure*}[t!]
    \centering    \includegraphics[width=0.75\linewidth]{Figures/ours_energybreakdown.pdf}
    \caption{Our framework's energy breakdown in edge client and cloud server communication scenario when normalizing energy consumption by the total energy consumption of the conventional method. Unlike the conventional method, our model reduces cloud processing because fewer irrelevant segments are transmitted. In scenarios with smaller near-sensor models, the reduction in communication and cloud energy might be limited if the false positive rate remains high, but our largest model configurations achieve balanced reductions across edge, communication, and cloud.}
    \label{fig:ours_energybreakdown}
\end{figure*}

\begin{figure*}[t!]
    \centering    \includegraphics[width=0.75\linewidth]{Figures/pie_breakdown.pdf}
    \caption{Distribution of energy consumption for the conventional method, compressive near sensor method, and ours in different AoI probabilities. Total energy consumptions are presented in the centers normalized by the conventional method's total energy consumption. The conventional method transmits all data to the cloud, resulting in high communication and cloud-side processing costs. Our approach significantly reduces total energy by filtering data at the edge.}
    \label{fig:pie_breakdown}
\end{figure*}

\subsection{Experimental Setup}
The proposed framework has been executed with a software framework and a hardware accelerator. Our software framework is implemented using a combination of PyTorch and NumPy that supports CNN layers, HDC encoding, and classification. We study the effectiveness of our technique over the UrbanSound8K dataset~\cite{salamon2014dataset}, which is a public audio dataset for urban sound classification applications. The dataset contains 10 classes such as car horn, dog bark, etc. Among these classes, we chose to use the gunshot class as our audio of interest.

\subsection{Evaluation of The Proposed Near-sensor Model}

In the evaluation of our near-sensor model, we focused on a scenario where our objective is to permit a specific level of false positives while maximizing the true positive rate (TPR). Employing the ROC curve evaluation allows us to analyze this scenario and identify model configurations that achieve the highest TPR at a desired false positive rate (FPR).

Compared to an MLP, HDC achieves better training results even with a limited portion of training data because HDC relies on high-dimensional random projections that preserve similarity and enable robust classification with fewer samples. MLPs typically need more data to effectively adjust their parameters via gradient-based training. In contrast, HDC can immediately construct meaningful class hypervectors from a small number of examples, providing inherent robustness and reducing the data requirement.

Our initial focus was on evaluating the impact of the feature extractor size, comprised of CNN layers, on achieving a sharp ROC curve. Even with slightly more than 5 layers of CNN, the model achieves an AUC exceeding 0.99, with saturation evident at approximately the maximum AUC. The HDC model displays superior learning ability over the MLP reference, evidenced by its enhanced AUC performance.

\autoref{fig:onlinelearning} demonstrates the online learning capability of the HDC model. The HDC model can easily update its class hypervectors as new data arrives. In contrast, the MLP model requires additional mechanisms for reliable online adaptation. HDC with online learning surpasses the offline-trained MLP model in terms of F1 score on the test set, showing HDC’s inherent advantage in incremental learning scenarios.


\subsection{Evaluation of Energy Efficiency using ASIC Design}

To optimize the overall system's energy efficiency, the development of an energy-conscious near-sensor model is imperative. We conducted a comparative energy consumption analysis, contrasting our ASIC implementation on the Edge TPU with two widely utilized boards equipped with edge-specific CPUs and GPUs. The Edge TPU exhibits approximately 23.6 times greater energy efficiency than some counterparts.

In \autoref{fig:ours_energybreakdown} and \autoref{fig:pie_breakdown}, the baseline or ``conventional'' method means sending all raw data to the cloud for processing without any near-sensor filtering. By comparing our method’s bars to this baseline, it becomes clear that while smaller models may not significantly reduce communication or cloud energy (due to fewer AoI detections), larger or more tuned models do. The figure shows normalization by the conventional method’s total energy, making direct comparisons straightforward.

As shown in \autoref{fig:ours_energybreakdown} and \autoref{fig:pie_breakdown}, our framework demonstrates significant energy savings over conventional and compression-based approaches. Even with small models and carefully chosen FPR thresholds, we can achieve over 77\% energy reduction. By adjusting $T_{score}$ to allow slightly higher FPR, we can further improve TPR, capturing more AoI samples and achieving up to 82.1\% energy savings at the cost of only 1.39\% quality loss, as shown in \autoref{fig:tradeoff}.



