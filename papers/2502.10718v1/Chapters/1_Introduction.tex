
In today's systems, the challenge of handling the large amounts of data generated by sensors, particularly in the context of audio data, has become increasingly prevalent~\cite{zaman2023survey}. This surge in data volume requires significant computational resources for both processing and storage, at significant cost. This is particularly problematic in domains that require real-time responsiveness, such as gunshot detection systems (GSDS)~\cite{hansen2021gunshot}, chainsaw sound detection for forest protection~\cite{somwong2023acoustic}, and vehicle sound detection for real-time traffic planning~\cite{10078155}. In addition, the proliferation of edge sensors in today's landscape further exacerbates the cost scaling of these systems.

Nevertheless, in many of these systems, a substantial portion of the data generated by microphone sensors is irrelevant to the targeted detection scenarios~\cite{calhoun2021precision}. It is plausible to mitigate these escalating costs by transmitting only the pertinent audio data of interest, which is essential for predicting targeted detection scenarios, to cloud-based systems responsible for data storage and processing. One promising solution for achieving this efficiency is the adoption of near-sensor computing paradigms. By embedding lightweight, real-time audio detection machine learning models with low energy consumption capabilities at the sensor level, we can selectively forward valuable data to the cloud infrastructure.

Contemporary deep learning models, such as Deep Neural Networks (DNNs), have gained significant traction in addressing various real-world tasks. However, they have been critiqued for their substantial memory and energy requirements, particularly during the training and inference phases. Deploying such resource-intensive models directly on or near sensors poses challenges, resulting in a notable performance gap between deep learning algorithms and sensing components. Moreover, DNNs are less adaptable to online learning scenarios, a critical limitation in systems that must promptly adjust to real-time sensory input.

The aforementioned challenges serve as a strong motivation for the development of an intelligent, rapidly adaptable, and efficient framework for the representation and analysis of raw sensor data. To attain real-time performance, even with online learning capabilities, we propose a redesign of machine learning algorithms. This involves the fusion of a lightweight Convolutional Neural Network (CNN) with neurally-inspired HyperDimensional Computing (HDC). HDC is an alternative paradigm that emulates essential brain functions, prioritizing high efficiency and online learning capacity, while the CNN is harnessed for its robust feature extraction capabilities. HDC is rooted in the observation that the human brain excels in operating on high-dimensional data representations.

In this paper, we introduce a near-sensor model designed for an intelligent sensing framework tailored to audio detection tasks, aiming to address the challenges previously delineated. Our model incorporates a Fast Fourier Transform module, followed by a series of CNN layers and an HDC layer. Notably, it facilitates low-energy, rapid inference, and online learning, dynamically adapting to emerging data trends based on feedback from a heavyweight machine learning model in the cloud. The versatility of our model extends to its adaptability for implementation in ASIC design, offering superior energy efficiency compared to embedded CPUs or GPUs. Given the current trend of shrinking microphone sensor sizes, an ASIC implementation supporting compact chip dimensions ensures the viability of deploying our proposed near-sensor model across a spectrum of contemporary sensors.

Subsequently, we conduct a comprehensive evaluation of our near-sensor model at both the software and hardware levels. On the software front, we present a detailed ROC curve analysis, providing a nuanced understanding of how our framework balances energy conservation against potential quality loss. Additionally, we demonstrate the model's performance in detecting audio of interest, showcasing its comparative efficacy with a larger, more complex cloud-based model. Shifting the focus to the hardware dimension, we underscore the high energy efficiency of our model through ASIC design implementation, comparing favorably against commonly employed embedded CPUs and GPUs. This dual-tier evaluation illuminates the prowess of our near-sensor model, affirming its viability and efficiency across both software and hardware realms.

Our work is fundamentally novel and provides the following contributions:
\begin{itemize}
    \item To the best of our knowledge, we propose the very first novel framework for saving total energy consumption of the audio detection domain systems where the audio of interest is infrequent.
    \item We propose an efficient near-sensor model that is capable of detecting audio of interest while supporting online learning. We further show a thorough analysis of the model to identify the proper hyperparameter for balancing the trade-off relationship between energy saving and quality loss.
    \item We show its energy efficiency when we run our proposed model on ASIC chip compared to generally used embedded CPUs and GPUs.
\end{itemize}
