

In many scenarios, complex machine learning tasks demand heavy models that prove challenging to implement on edge devices. For instance, a recent state-of-the-art Transformer-based audio classification model~\cite{9746312}, demands 80 hours of training on 4 NVIDIA Tesla V100 GPUs, despite its computational resource reduction compared to alternative models. Similarly, many other works also focus on utilizing heavy machine learning models for complex tasks such as using large pre-trained multi-modality model~\cite{wu2022wav2clip}, having large transformer models~\cite{baade2022mae}, etc. Consequently, these contemporary deep learning-based audio detection tasks present a practical challenge when it comes to real-time implementation on edge sensors. Our solution simplifies this by binarizing such tasks, specifically detecting "audio of interest," only essential audio data for complex functions. Unlike conventional models like Recurrent Neural Networks (RNNs), our near-sensor model employs an HDC model with very few CNN layers. This design ensures fast, efficient inference with online learning capability.

Unlike MLPs, HDC does not rely on fully connected layers or activation functions. Instead, it builds class hypervectors directly via bundling and binding operations on encoded features extracted from CNN layers. This approach does not necessitate large parameter sets and backpropagation, enabling rapid adaptation and robust performance even with limited training samples.


\subsection{HDC Basics}
The fundamental representational unit of HDC is called a hyperdimensional vector. A hypervector $\mathcal{H}$ indicates a vector $\mathbb{R}^D$ with high dimensionality $D$. The hyperdimensional vectors are compared to each other by a similarity function $\delta$. Utilizing the similarity measure, HDC can facilitate cognitive tasks such as memorization, classification, clustering, and more. HDC frameworks designed to support these tasks rely on three fundamental HDC operations that directly correspond to brain functionalities: bundling, binding, and permutation.

\begin{enumerate}
    \item \textbf{Bundling}: this operation, denoted as $+$, is typically implemented as element-wise addition. If $\mathcal{H}=\mathcal{H}_1+\mathcal{H}_2$, then both $\mathcal{H}_1$ and $\mathcal{H}_2$ are similar to $\mathcal{H}$. From a cognitive perspective, it can be interpreted as memorization.
    \item \textbf{Binding}: this operation, denoted as $*$, is typically implemented as element-wise multiplication. If $\mathcal{H}=\mathcal{H}_1*\mathcal{H}_2$, then $\mathcal{H}$ is dissimilar to both $\mathcal{H}_1$ and $\mathcal{H}_2$. Binding also has the important property of similarity preservation.
    \item \textbf{Permutation}: this operator, denoted as $\rho$, is typically implemented as a rotation of vector elements.
\end{enumerate}

Using these three operations enables a hyperdimensional learning framework. Classification involves encoding input features into hypervectors and creating class hypervectors by bundling. Retraining involves adjusting class hypervectors by adding hypervectors of correctly predicted samples and subtracting those of misclassified samples, thus refining the class boundaries.

The encoding function often uses cosine and sine transformations, as this mapping preserves similarity between inputs. By projecting data into a high-dimensional space, small differences become more distinguishable, enabling robust recognition and generalization even with limited training data.


\subsection{Bridging HDC to Audio Detection}
While the basics of HDC provide the fundamental building blocks, applying them directly to audio detection involves integrating CNN-based feature extraction with hypervector encoding. The next section shows how we embed HDC operations into an audio sensing framework, transforming raw audio streams into high-dimensional representations and selectively transmitting only data deemed relevant.

\begin{figure*}[t!]
  \centering
  \includegraphics[width=\linewidth]{Figures/ModelPipeline.pdf}
  \caption{Overview of our audio detection framework for Hyperdimensional Intelligent Sensing. The audio detection training consists of three phases: (a) Offline learning, (b) Offline trained near-sensor model deployment, and (c) Online learning based on a costly machine learning model. After training CNN layers for feature extraction, the HDC encoding transforms extracted features into hypervectors, forming class hypervectors without any traditional MLP layers or activation functions.}
  \label{Fig:model_pipeline}
\end{figure*}


\begin{figure*}
    \centering    \includegraphics[width=0.6\linewidth]{Figures/AUCbyModelSize.pdf}
    \caption{Performance analysis by model size with hyperdimension of $D=10K$. Left: Receiver Operating Characteristic (ROC) curve analysis with varied feature extraction layers. Right: Area Under the Curve (AUC) analysis also with the same range of feature extraction layers.}
    \label{fig:AUCbyModelSize}
\end{figure*}

\begin{figure}
    \centering    \includegraphics[width=1.\linewidth]{Figures/onlinelearning.pdf}
    \caption{Test F1 score comparison between HDC with online learning and with MLP layer which is hard to support online learning.}
    \label{fig:onlinelearning}
\end{figure}

\begin{figure}
    \centering    \includegraphics[width=1.\linewidth]{Figures/energy_consumption.pdf}
    \caption{Energy consumption estimation by different near sensor model size}
    \label{fig:energy_consumption}
\end{figure}


\subsection{Audio Intelligent Sensing Framework}

In \autoref{Fig:overview_diagram_ours}, we present an overview of our framework designed to reduce overall system costs related to network communication, expensive machine learning servers, and storage. This is achieved by strategically placing a lightweight AI model in proximity to the microphone sensor, enabling real-time selective transmission of audio data. Leveraging HDC as our lightweight AI model tightly integrated with the sensing circuit, our framework supports online learning, enhancing its adaptability.

To ensure coordination between the lightweight model and the buffer, we maintain a buffer size that matches or exceeds the model's maximum inference latency. The buffer operates as a FIFO queue, and the model processes incoming audio frames in order. Since the model's inference is lightweight and near real-time, it completes classification before the oldest data in the buffer is popped. This synchronization prevents data loss and ensures that the model does not miss any segments it needs to evaluate. In rare high-load scenarios, the buffer size can be increased to handle temporary spikes, ensuring that all data is classified before removal.

Our proposed framework stacks the audio stream in a fixed-size buffer and pops the oldest audio stream data from the buffer if it reaches maximum capacity. During this process, the lightweight AI model determines whether there is audio of interest or not. If the model detects audio of interest, it activates the switch to send out all of the audio data in the buffer through the network communication channel to the costly machine learning server. Adjusting the buffer size not only allows for more contextual data if needed but also helps mitigate false negative detections, particularly useful when audio of interest exhibits time locality characteristics.




\begin{figure*}
    \centering    \includegraphics[width=0.7\linewidth]{Figures/tradeoff.pdf}
    \caption{Trade-off relationship between energy saving compared to the conventional method and quality loss.}
    \label{fig:tradeoff}
\end{figure*}


\subsection{Near Audio Sensor Model}

As highlighted earlier, our framework governs data transmission by identifying audio of interest with time locality features. \autoref{Fig:model_pipeline} illustrates the comprehensive pipeline of our near audio sensor model, encompassing three pivotal phases: (a) offline learning, (b) deployed framework, and (c) online learning. In the initial phase, the model undergoes training with an existing audio dataset before transitioning into deployment. Post-training, our model systematically adjusts its weights based on feedback from the resource-intensive machine learning model to sustain optimal performance over time.

In order to deploy the near audio sensor model, we first need to train the model in an offline manner as shown in the \autoref{Fig:model_pipeline}.(a). First, given an audio dataset $D$, we convert them to sound spectrograms using the Fast Fourier Transform (FFT) algorithm with normalization. Now, we generate a labeled dataset by labeling each data according to the Audio of Interest (AoI). For unbalanced scenarios, we use simple random oversampling on the minority AoI samples to ensure that the CNN layers receive sufficient positive samples during training.

After training the CNN layers, we use them as a feature extractor of our HDC model. Using these CNN layers combined with HDC encoding, we generate hypervectors. Negative and positive class hypervectors are formed by bundling their respective sets of hypervectors. To further refine the HDC model, we retrain by incrementally adjusting class hypervectors. This process effectively ``moves'' the class representations closer to correct samples and away from incorrect ones, thereby sharpening decision boundaries without requiring backpropagation or complex layers.

In the deployed framework, the lightweight near-sensor model applies FFT and CNN-based feature extraction on incoming audio. The extracted features are encoded into hypervectors and then compared against class hypervectors. If the similarity score with the positive class hypervector surpasses a threshold $T_{score}$, the data is considered audio of interest and transmitted. We determine $T_{score}$ from ROC curve analysis on a validation set, choosing a threshold that yields an acceptable trade-off between false positives and false negatives.

Finally, online learning is facilitated by the cloud-based heavyweight model. If the cloud model identifies misclassifications from the edge device, it provides feedback hypervectors that are used to update class hypervectors at the edge. This incremental learning allows the near-sensor model to adapt rapidly to evolving data distributions.


\subsection{ASIC Acceleration}
To deploy our model in a resource-constrained edge environment, we employ the Google Edge TPU (Edge-TPU) to accelerate it. We quantize the model into 8-bit integers using the TensorFlow Lite framework. Compared to CPUs and GPUs, ASIC and Edge-TPU approaches dramatically reduce energy consumption due to their specialized architectures. In later sections, we quantitatively show that our approach outperforms conventional embedded CPUs and GPUs by a large margin, confirming that quantization and ASIC integration yield substantial efficiency gains.

For clarity, in a practical scenario, the microphone feeds raw audio to the near-sensor ASIC, which performs FFT, CNN feature extraction, and HDC classification. Only detected AoI data is sent to the cloud for heavyweight processing. We have not built a custom hardware testbed but rely on known power profiles and simulation results to estimate energy savings.