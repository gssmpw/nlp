
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{lipsum}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{url}

\graphicspath{ {./images/} }

\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{natbib} 
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{tikz}
\usepackage{cleveref}
\usetikzlibrary{shapes, positioning}
\usepackage{wrapfig}

\newtheorem{example}{Example}
\newtheorem{remark}{Remark}
\newtheorem{definition}{Definition}

% Definitions of handy macros can go here
% \newcommand{\Prob}{p}
\newcommand{\Prob}{\mathbb{P}}
\newcommand{\fg}[1]{\textcolor{red}{\textbf{F}: #1}}
\newcommand{\bep}[1]{\textcolor{orange}{\textbf{Beppe}: #1}}
\newcommand{\notes}[1]{\textcolor{blue}{#1}}

\input{tikz_figs}

\title{Neural Interpretable Reasoning}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.
\makeatletter
\renewcommand\@fnsymbol[1]{%
  \ensuremath{%
    \ifcase#1
      \or
      \dagger % 1st symbol: a cross-like dagger
      \or
      \ddagger % 2nd symbol: double dagger (if needed)
      \else
      \@ctrerr
    \fi
  }
}
\makeatother

\author{
Pietro Barbiero$^*$ \\
IBM Research, Switzerland\thanks{Work conducted while employed at Università della Svizzera italiana.} \\
\texttt{pietro.barbiero@ibm.com} \\
\And
Giuseppe Marra$^*$ \\
KU Leuven, Belgium \\
\texttt{giuseppe.marra@kuleuven.com} \\
\And
Gabriele Ciravegna \\
Politecnico di Torino, Italy \\
\And
David Debot \\
KU Leuven, Belgium \\
\And
Francesco De Santis \\
Politecnico di Torino, Italy \\
\And
Michelangelo Diligenti \\
Universita' di Siena, Italy \\
\And
Mateo Espinosa Zarlenga \\
University of Cambridge, UK \\
\And
Francesco Giannini \\
Scuola Normale Superiore, Italy
}

% \author{Antiquus S.~Hippocampus, Natalia Cerebro \& Amelie P. Amygdale \thanks{ Use footnote for providing further information
% about author (webpage, alternative address)---\emph{not} for acknowledging
% funding agencies.  Funding acknowledgements go at the end of the paper.} \\
% Department of Computer Science\\
% Cranberry-Lemon University\\
% Pittsburgh, PA 15213, USA \\
% \texttt{\{hippo,brain,jen\}@cs.cranberry-lemon.edu} \\
% \And
% Ji Q. Ren \& Yevgeny LeNet \\
% Department of Computational Neuroscience \\
% University of the Witwatersrand \\
% Joburg, South Africa \\
% \texttt{\{robot,net\}@wits.ac.za} \\
% \AND
% Coauthor \\
% Affiliation \\
% Address \\
% \texttt{email}
% }

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
We formalize a novel modeling framework for achieving interpretability in deep learning, anchored in the principle of inference equivariance. While the direct verification of interpretability scales exponentially with the number of variables of the system, we show that this complexity can be mitigated by treating interpretability as a Markovian property and employing neural re-parametrization techniques. Building on these insights, we propose a new modeling paradigm---\emph{neural generation and interpretable execution}---that enables scalable verification of equivariance. 
This paradigm provides a general approach for designing Neural Interpretable Reasoners that are not only expressive but also transparent.
% The abstract paragraph should be indented 1/2~inch (3~picas) on both left and
% right-hand margins. Use 10~point type, with a vertical spacing of 11~points.
% The word \textsc{Abstract} must be centered, in small caps, and in point size 12. Two
% line spaces precede the abstract. The abstract must be limited to one
% paragraph.

% Interpretability is a crucial aspect of artificial intelligence, yet its precise definition remains elusive, leading to challenges in establishing clear evaluation metrics. While previous research has sought to formalize interpretability, this work takes the alternative approach of proposing a procedural test to assess whether a system is interpretable. The test is based on a
% human-machine inference equivariance principle, offering a structured means of evaluation. However, the approach introduces significant complexity challenges, which we analyze in detail. To address these challenges, we propose several mitigation strategies and outline a potential instantiation of the test.
\end{abstract}

% \bep{lascio commenti in giro solo perche' lo sto rileggendo a pezzi e mi vengono cose in mente ma non e' detto che dobbiamo tenerle / discuterle}

\section{A Turing test for interpretability}

Interpretability, much like intelligence, is often subject to debate due to its inherently subjective nature~\citep{kim2016examples,miller2019explanation,molnar2020interpretable}. Instead of attempting to provide an exhaustive definition, in this paper we propose a procedural test---akin to the Turing test~\citep{turing1950computing}---that evaluates whether a system is interpretable.
We motivate our proposal using the following concrete examples.
\begin{example}
Donald Duck attempts to start his car, model 313, but the vehicle fails to start. After inspecting the situation, he finds that the fuel level is too low. Once he refuels, the car starts without issue. In this instance, Donald clearly understands the problem and its straightforward solution.
The following day, the car fails to start once more despite having a full fuel tank. Uncertain of the cause, Donald consults a mechanic. Building on her expertise in engines, the mechanic determines that an oil leak is the root of the problem. After repairing the leak, the car operates normally. Here, while Donald could not diagnose the issue on his own, his recourse to expert knowledge ultimately resolved the problem.
\end{example}
These examples illustrate that understanding a system is often subjective and dependent on the user's background~\citep{miller2019explanation}. However, they also suggest a practical criterion to check whether a system is interpretable. We can informally describe this criterion as follows:
\begin{quote}
\emph{A system is interpretable to a user if the user is able to interact with it and accurately forecast the system outputs.}
\end{quote}
This approach emphasizes the role of user interaction in assessing interpretability and mirrors the spirit of the Turing test by focusing on the system behavior. 

\paragraph{Contributions} This work's purpose can be characterized as threefold:
\begin{itemize}
    \item \textbf{Formalize interpretability as inference equivariance:} We formalize interpretability as\textit{ human-machine inference equivariance} and show that verifying inference equivariance directly is intractable (Sec.~\ref{sec:equiv}).
    \item \textbf{Break combinatorial complexity in verifying interpretability:} We show how the combinatorial complexity in verifying inference equivariance can be mitigated considering interpretability as a Markovian property and using techniques such as neural re-parametrization and mixture models (Sec.~\ref{sec:scalability}).
    \item \textbf{Formalize a modeling paradigm guaranteeing expressivity and interpretability by design:} Building on these insights, we propose a new modeling paradigm---\emph{neural generation, interpretable execution}---that enables scalable verification of interpretability and designing models that are not only expressive but also transparent (Sec.~\ref{sec:nir}).
\end{itemize}


% \begin{quote}[Turing test of interpretability]
%     \emph{The system is interpretable if, for any input/intervention, the user can correctly forecast the outcome of the system.}
% \end{quote}

% This test examines if a user, through direct interaction with the system, can reliably predict its behavior for any given input or intervention.


\section{Interpretability \& equivariance}\label{sec:equiv}
\begin{wrapfigure}{r}{0.5\linewidth}
    \centering
    \includegraphics[width=\linewidth]{NIR_equivariance.pdf}
    \caption{Example of inference equivariance.}
    \label{fig:equiv_example}
\end{wrapfigure}
Our work is motivated by the idea that a system is interpretable if its internal processes can be reliably translated into outcomes that users can predict. 
In this section, we formalize this notion as \textit{interpretability equivariance}, establishing that performing inference using the system's mechanisms should commute with the process of inference performed using the user's mechanisms.  We begin by motivating and illustrating this definition via an example:
\begin{example}[The Donald Duck Comfort Problem (Fig.~\ref{fig:equiv_example})]
Donald Duck wants to sleep but is uncomfortably cold. To achieve a comfortable sleep, he needs to warm up his environment to an appropriate temperature. A thermostat, whose user's manual Donald misplaced, controls the heating system. The thermostat provides only two pieces of information: a wheel with eight positions (currently set to $1$) and a numeric display ranging from 0 to 10 (currently showing $3$). 

In his first attempt, Donald rotates the wheel to position $6$. After waiting, he returns to observe that the display now reads $1$, and he finds himself sweating and uncomfortable. Donald can explain the phenomenon along two equivalent reasoning paths:
\begin{align*}
    \text{Thermostat path:} & \quad \texttt{wheel}=6 \to \texttt{display}=1 \to \texttt{comfort}= \text{no}, \\
    \text{Donald Duck path:} & \quad \texttt{wheel}=6 \to \texttt{heat}=\text{high} \to \texttt{comfort}= \text{no}.
\end{align*}
From this, Donald infers that turning the wheel upward increases the room’s temperature and causes the display to show lower numbers. To test his hypothesis, he sets the wheel to position $4$. Later, he checks the thermostat to find that the display now shows $2$, and he expects the room to have cooled down enough to restore his comfort:
\begin{align*}
    \text{Thermostat path:} & \quad \texttt{wheel}=4 \to \texttt{display}=2 \to \texttt{comfort}= \text{yes}, \\
    \text{Donald Duck path:} & \quad \texttt{wheel}=4 \to \texttt{heat}=\text{medium} \to \texttt{comfort}= \text{yes}.
\end{align*}
\end{example}

The example illustrates that while the thermostat's variables differ in semantics from Donald Duck's internal concepts, they are nonetheless aligned closely enough for him to establish a straightforward mapping between the two. For instance, a wheel position within the range $[3, 4]$ might be interpreted as medium heat, and a display reading of $2$ may be associated with a state of comfort. Furthermore, Donald's reasoning demonstrates that he can deduce the system's state via two equivalent routes--either by consulting the display or by directly sensing the heat output--with both methods leading to the same conclusion. Building on this intuition, we first introduce some useful notation and then use this to formalize our notion of interpretability equivariance. 

\subsection{Interpretability as inference equivariance}
\paragraph{Preliminaries: Transformation of Random Variables} 
Let $V$ denote a set of random variables representing different aspects of a system (for example, heating levels, wheel position, etc.). We write the joint probability distribution of these variables as 
$
\Prob(V) = \Prob(V_1, V_2, \cdots, V_n).
$
% In many scenarios, we are interested in the conditional probability of a particular query variable $V_q$ (such as the comfort level) given some evidence $V_e$ (like the wheel position), denoted by $\Prob(V_q \mid V_e)$. 
To formalize the distinction between the internal, machine-oriented description of the system and its human-interpretable counterpart, we index machine-related variables with the superscript $m$ (so that $V^{(m)}$ represents the machine’s variables) and human-related variables with $h$. Following~\citet{rubenstein2017causal}, we define a \emph{translation function} 
$
\tau: V^{(m)} \to V^{(h)}
$
as a map between machine variables and the variables within the human's reference system. Consequently, for any distribution $\Prob(V^{(m)})$ over the machine variables, the corresponding distribution in the human space is given by the push-forward measure $\Prob_{\tau(V^{(m)})} = \tau\bigl(\Prob_{V^{(m)}}\bigr)$. In particular, for each action on the $i$-th machine variable $a(i)$ (e.g., observing $a(i) \coloneqq \bigl( V_i^{(m)}=k \bigr)$ or intervening on the value of a variable $a(i) \coloneqq do(V_i^{(m)})$), we can define
the induced distribution $\Prob_{\tau(V^{(m)})}^i = \tau\bigl(\Prob_{V^{(m)}}^{a(i)}\bigr)$. 
To exactly transform the machine system into the human system, we require a surjective mapping $\omega: I_{V^{(m)}} \to I_{V^{(h)}}$ that assigns machine variable indices to human variable indices such that $\mathbb{P}_{\tau(V^{(m)})}^i = \mathbb{P}_{V^{(h)}}^{a(\omega(i))}$. Rather than enforcing that $\omega$ be order-preserving as in \cite{rubenstein2017causal}, our formulation of \emph{inference equivariance} requires that $\omega$ preserves conditional independence relations (which represents a weaker requirement). Formally, define the neighborhood of a machine variable $V_i^{(m)}$ as the minimal set of variables rendering it conditionally independent of the rest, i.e., 
\[
\mathcal{N}(V_i^{(m)}) = \min \{\, S^{(m)} \subseteq V^{(m)} \setminus \{V_i^{(m)}\} : V_i^{(m)} \perp (V^{(m)} \setminus (\{V_i^{(m)}\} \cup S^{(m)})) \mid S^{(m)} \,\}.
\]
We say that $\omega$ preserves conditional independencies if and only if, for every $V_i^{(m)}$ and every subset $S^{(m)} \subseteq V^{(m)} \setminus \{V_i^{(m)}\}$, 
\[
V_i^{(m)} \perp (V^{(m)} \setminus (\{V_i^{(m)}\} \cup S^{(m)})) \mid S^{(m)}
\]
if and only if 
\[
\tau(V_{\omega(i)}^{(m)}) \perp ( \tau(V^{(m)}) \setminus (\{ \tau(V_{\omega(i)}^{(m)})\} \cup \tau(S^{(m)}))) \mid \tau(S^{(m)}).
\]
This condition ensures that $\omega$ precisely mirrors the conditional independence structure between the machine and human systems.

% \[
% \Prob\bigl(\tau(V^{(m)})\bigr) = \tau\bigl(\Prob(V^{(m)})\bigr).
% \]
% This expression means that applying the translation function to the machine's probability distribution produces a distribution over the human-interpretable variables.

\paragraph{Inference equivariance} 
The principle of \textit{inference equivariance}, illustrated in our previous example, asserts that the process of translating a machine's probability distribution into the human reference system and then querying it should yield the same result as first performing the query within the machine's domain and then translating the result. Formally, this is expressed as
\[
\resizebox{0.3\textwidth}{!}{\equivariance}
\]
% \[
% \Prob_{\tau(V^{(m)}_q \mid V^{(m)}_e)} = \tau \Bigl( \Prob_{V^{(m)}_q \mid V^{(m)}_e} \Bigr).
% \]
% were $\mathcal{I}$ represents the inference process. 
This equality encapsulates the idea that whether one chooses to ``translate, then query" or to ``query, then translate", the resulting inference remains the same, as already observed for causal structures~\citep{rubenstein2017causal,geiger2024causal,marconato2023interpretability}. In the context of the Donald Duck example, this principle becomes particularly clear. Donald Duck faces a thermostat whose internal variables---such as the wheel setting and display reading---are not immediately aligned with his intuitive notions of heat and comfort\footnote{Notice that in contrast with equivariances in causal abstractions~\citep{geiger2024causal} where the inference structure is assumed to be aligned with the true data generating mechanisms.}. By establishing a mapping between the machine's outputs and his own reference system, he is able to reliably predict his comfort level. 

For instance, Donald might first translate the thermostat's raw signal (the display reading) into his internal concept of temperature and then infer his comfort state based on that interpretation. Alternatively, he might directly observe the mechanical behavior (the wheel position) to predict the corresponding change in room temperature, and only afterwards translate that information into his subjective experience of warmth. The fact that both routes lead him to the same conclusion---whether he ``translates, then queries" or ``queries, then translates"---demonstrates the principle of inference equivariance. 

This consistency is critical: it ensures that the mapping between machine variables and human concepts is robust, thereby making the system interpretable. In essence, the equality \emph{``translate, then query'' = ``query, then translate''} guarantees that a user's understanding and predictions of a system's behavior remain coherent, regardless of the order in which translation and inference occur.


\paragraph{Verify interpretability via inference equivariance is intractable} 
While the concept of equivariance provides a robust framework for linking machine and human perspectives, its practical implementation is fraught with challenges. As the number of variables increases, verifying and maintaining equivariance becomes exponentially more complex. To illustrate, consider a simple scenario where every variable in the system is Boolean. In this simple case, a complete interpretation of the system would require verifying the equivariance for all possible states of the system. This corresponds to extracting the full conditional probability table, which contains $2^n$ entries for $n$ variables. Even for a modest $n$, the number of combinations quickly becomes computationally intractable.
For this reason, in practical applications it becomes essential to guarantee inference equivariance indirectly or approximately while maintaining computational efficiency. In practice, this may involve constraining the inference space to a subset of critical variables, leveraging problem-specific structures to reduce complexity, or employing surrogate models that approximate the full system's behavior with a significantly lower computational cost.



\subsection{Properties of interpretability through the lenses of inference equivariance}
Based on inference equivariance, we can highlight several key properties that further clarify the nature of interpretability.

\textbf{Inference equivariance can be asymmetric:} In the thermostat example, Donald Duck uses the available signals---such as the wheel position and the display reading---to form an understanding of the system's behavior. Importantly, for him to use the thermostat effectively, it is unnecessary to have a complete, invertible mapping from his internal concepts (e.g., ``comfort level'') back to the machine's variables. This one-way, asymmetric mapping suffices because Donald only needs to translate machine outputs into human-understandable signals. The absence of a reverse transformation does not impede his ability to predict the system's response, illustrating that the forward mapping (machine $\rightarrow$ human) is all we require for interpretability (although the opposite mapping might be needed for supervised learning).

\textbf{Explanations are a form of selection:} An explanation of a system's behavior can be seen as a process of selection, where conditioning on observed evidence picks out a specific subset from the system's complete conditional probability table. In the Donald Duck example, when Donald observes a particular display reading or wheel position, he effectively selects a corresponding segment of the conditional probability table that relates these inputs to his comfort state. This selection---formally represented with the distribution $\Prob(V \mid a(V'))$---encapsulates the explanation by narrowing down the myriad potential outcomes to the ones relevant to his observation.

\textbf{Explanations might not be interpretable:} Not every selection from the conditional probability table yields a meaningful or interpretable explanation. For example, if the mapping between the thermostat's signals and Donald's perception of warmth were inconsistent---if the transformation did not commute---then the same action might lead to different inferred comfort states, confusing the user. Hence, for an explanation to be interpretable, the diagram representing the transformation must commute, ensuring that no matter how the inference is performed, the resulting explanation is consistent and understandable.

% \textbf{World alignment vs. Interpretability:} A system may be highly accurate in modeling the external world, yet still be misinterpreted by the user if the mapping between its internal signals and the human reference frame is flawed. In the thermostat scenario, even if the device operates exactly as designed, Donald Duck could still misjudge his comfort if his personal mapping between the wheel position and the resulting heat output is off. This disconnect illustrates that alignment with the physical world does not automatically confer interpretability from a human perspective.

\textbf{Local vs. global equivariance:} Equivariance may hold over the entire state space of the system (global) or only in certain regions (local). In the case of the thermostat, Donald Duck might have developed an accurate translation for a subset of wheel positions, while other settings remain ambiguous. This local equivariance indicates that while the system may be interpretable under specific conditions, \textit{its interpretability might not generalize across all possible configurations}. Recognizing the distinction between local and global equivariance is crucial for assessing the robustness of a system's interpretability.

\textbf{Post-hoc methods complicate rather than simplify interpretability:} When applying post-hoc interpretability techniques, such as using surrogate models to explain the original system~\citep{hinton2015distilling, deepred} or so-called feature importance methods~\citep{lime, shap, og_saliency, integrated_gradients}, an additional layer of equivariance is required. Suppose Donald employs a surrogate model to better understand his thermostat. In that case, there must be a consistent mapping between the machine variables of the original system $V^{(m)}$ and those of the surrogate model $V^{(s)}$ and another mapping from the surrogate model to Donald Duck $V^{(h)}$. Formally, 
% if we denote the translation and inference functions for the surrogate as $\tau'$ and $\eta'$ respectively, 
both the original and surrogate systems must satisfy the inference equivariance conditions:
\[
\resizebox{0.3\textwidth}{!}{\equivarianceSurrogate}
\]
% \[
% \eta'\Bigl( \tau'\bigl(\Prob(V^{(m)})\bigr) \Bigr) = \tau'\Bigl( \eta'\bigl(\Prob(V^{(m)})\bigr) \Bigr)
% \]
% and
% \[
% \eta\Bigl( \tau\bigl(\Prob(V^{(m')})\bigr) \Bigr) = \tau\Bigl( \eta\bigl(\Prob(V^{(m')})\bigr) \Bigr).
% \]
This requirement ensures that the explanations generated by the surrogate model faithfully reflect the behavior of the original system, thus preserving interpretability even when using post-hoc methods. Ultimately, the need to establish these additional mappings significantly complicates the interpretability process as two equivariance relations must be satisfied instead of one.


\subsection{Semantic and functional equivariances}

Previous works~\citep{geiger2024causal,marconato2023interpretability} focused primarily on semantic equivariance, emphasizing that equivariance should hold for random variables $V$. However, less attention has been paid to the functions that describe the mappings between random variables; for a user to truly understand the underlying mechanisms, the structure of the function and its parameters must also satisfy equivariance, as illustrated in the following example.
\begin{example}
Consider the conditional model $\Prob(V_2 \mid V_1)$ where $V_2$ follows a Gaussian distribution:
\[
\Prob(V_2 = v; \mu=V_1, \sigma) \coloneqq \frac{1}{\sqrt{2\pi\sigma^2}} \exp\!\left(-\frac{(v-\mu)^2}{2\sigma^2}\right).
\]
For this model to be fully interpretable, it is not enough for a human user to simply understand the data representation encoded in $V_1$ and $V_2$. Instead, inference equivariance must extend to the functional structure and its parameters. In other words, users should be able to modify or update the parameters---such as $\mu$ or $\sigma$, or even alter constants like replacing $2\pi$ with $3\pi$---and still verify that the same equivariant relations hold. This ensures that the underlying functional form of the model remains transparent.
\end{example}

The intuition behind this is that functional structure and parameters are key components of interpretability, not just the data representations. To capture this formally, we can distinguish between variables representing data, $V \in \mathcal{V}$, and those describing the model's functional structure, $\theta \in \Theta$. The complete model can then be expressed as $\Prob(V, \Theta)$. Inference equivariance should hold for both $V$, ensuring \textit{semantic transparency}, and for $\theta$, ensuring \textit{functional transparency}.

\section{Breaking combinatorial complexity in verifying interpretability} \label{sec:scalability}
As we discussed verifying inference equivariance directly is intractable. In this section we discuss interpretability properties and techniques which can be used to break this complexity down.

\subsection{Interpretability is a Markovian property}
In the earlier thermostat example, Donald Duck successfully built an intuitive understanding of how the thermostat worked, despite having no specialized knowledge of electronics or physics. This observation illustrates how interpretability is a Markovian property: a user can interpret a system at a given level of abstraction without needing to reference lower-level details.
In this context, interpretability is achieved locally---each step of an inference process can be understood in isolation from others. We can formalize this Markovian property of interpretability by writing:
% \[
% \Prob \models (V_i \bot V \notin \text{pa}(V)) \mid \text{pa}(V),
% \]
\begin{equation}    
    \forall \; V_i, V 
    % \text{ s.t. } (V_i \neq V) \; \wedge \; V \notin \text{pa}(V_i), 
    \; \Prob \models (V_i \bot V ) \mid \mathcal{N}(V_i)
\end{equation}
meaning that, given its 1-hop neighborhood $\mathcal{N}(V_i)$, any variable $V_i$ is conditionally independent of all other variables. This property allows a user to interpret a single step of the inference process---the one concerning the variable $V_i$---without needing to backtrack through the entire chain of reasoning.

This Markovian property of interpretability attenuates scalability issues, as it permits the analysis of individual steps without the burden of interpreting the entire system at once. This layered approach is reflected in models such as Self-Explaining Neural Networks~\citep{alvarez2018towards}, Concept Bottleneck Models~\citep{koh2020concept}, 
% Neural Additive Models~\citep{agarwal2021neural}, 
or Prototypical Networks~\citep{chen2019looks}, where semantically interpretable components (e.g., the concept bottleneck) are designed to be interpretable on their own, regardless of previous layers. In the Donald Duck example, his ability to understand the thermostat's behavior without the need to understand its engineering shows the practical benefits of this Markovian property.


\subsection{Re-parametrizations break equivariance complexity while guaranteeing expressivity and interpretability}

Interpreting complex systems often entails dealing with a vast number of variables, which can overwhelm human cognitive limits:
\begin{example}[Thermostat with Many Knobs]
Consider a new thermostat design featuring 100 knobs, where a certain (unknown) set of knobs controls the room temperature for a given day of the calendar year. In this scenario, Donald Duck would need to test every possible knob configuration to fully understand how the thermostat works.
\end{example}
This example highlights a fundamental scalability issue: while a machine can, in principle, process and manage a large number of independent variables, human users typically can only handle around 7 $\pm$ 2 variables at any one time~\citep{miller1956magical}. It clear that even under the assumption that variables operate independently (which is quite common in the field of eXplainable Artificial Intelligence, or \textit{XAI}), the number of interactions required to understand the system grows linearly with the number of variables. For humans, who are limited to processing a constant number of variables simultaneously (i.e., 7 ± 2), this poses a significant obstacle to interpretability. The key question then becomes: how can we design a system that presents only a constant number of variables to a human, without sacrificing the system's overall expressivity?
A promising approach to manage this challenge is re-parametrization, where a system is transformed into an equivalent form that preserves its expressivity while reducing the number of variables a human must directly consider.

\paragraph{Functional Mixtures}
One effective strategy is to decompose a complex system into a mixture of simpler subsystems, each of which is easy to understand~\citep{mclachlan1988mixture}. For instance, imagine a thermostat with 365 knobs (so, even more than the original 100 knobs!), but with the twist that only one knob is active per day, and an indicator light signals which knob is relevant at that time. This design ensures that, at any given moment, Donald needs to focus on only one knob rather than hundreds. Such re-parametrization retains the full expressive power of the original system while offering local representations that are much more interpretable. Techniques like Self-Explaining Neural Networks~\citep{alvarez2018towards}, ProtopNets~\citep{chen2019looks}, and Concept Memory Reasoning~\citep{debot2024interpretable} embody this approach by generating simple, locally faithful explanations whose composition may form arbitrarily non-linear decision boundaries.

\paragraph{Functional and semantic re-parametrizations}
In many classification problems, re-parametrization involves two key components: mapping raw variables to higher-level concepts (\textit{semantic re-parametrization}) and decomposing complex function parameters into simpler mixtures (\textit{functional re-parametrization}). In this framework, the original data variables are transformed into a set of human-interpretable concepts, ensuring semantic transparency as in Concept Bottleneck Models~\citep{koh2020concept}. Simultaneously, the function that governs the model's behavior is restructured into a mixture of simple functions, which preserves the model's expressivity while making it easier to understand as in Self-Explaining Neural Networks~\citep{alvarez2018towards} and Concept Memory Reasoning~\citep{debot2024interpretable}.

% Most existing XAI systems typically follow one of two approaches. Some, such as LIME, SHAP, and decision trees, assume that a suitable re-parametrization is already provided (as is common with tabular data). Others, like Concept Bottleneck Models (CBMs), logistic regression, or simple decision trees, deliberately restrict the complexity of the functional form to ensure interpretability. In both cases, the objective is to limit the number of variables or components that a human must examine, thereby mitigating the cognitive burden without compromising the model's expressive capacity.

% By leveraging re-parametrizations---whether through functional mixtures or a combination of semantic and functional re-parametrizations---we can design systems that balance the need for high expressivity with the requirement for human interpretability. This approach allows users like Donald Duck to gain meaningful insights from complex systems by interacting with a simplified, yet fully expressive, representation of the underlying model.


% Notice that most XAI systems either:
% \begin{itemize}
%     \item assume that data is given (tabular data): LIME, SHAP, DT
%     \item function is simple: CBMs, LR, DT
% \end{itemize}


\section{Neural Interpretable Reasoning} \label{sec:nir}
Building on our previous discussions of interpretability properties and leveraging techniques such as re-parametrizations, we propose a new modeling paradigm that guarantees the scalable verification of interpretability as inference equivariance. In this framework, the following elements are essential:
\begin{itemize}
    \item \textbf{Semantic transparency:} The model must employ high-level, human-understandable concepts (e.g., as in~\citet{tcav, koh2020concept, concept_whitening}).
    \item \textbf{Functional transparency:} The function that maps these concepts to the desired tasks should have a low-complexity structure (e.g., linear), and its parameters should be interpretable.
    \item \textbf{Markovian property of interpretability:} By focusing on a single layer of the system (for instance, the final classification layer), this approach breaks down the complexity that arises from having to interpret the concept generation (which requires a separate verification procedure).
    \item \textbf{Functional mixtures:} When working in a setup where there is a high number of concepts, \textit{functional mixtures} help manage the model's complexity by decomposing the mapping from concepts to tasks into simpler, more interpretable components.
    \item \textbf{Neural re-parametrizations:} Both concepts and functions can be neurally re-parametrized, allowing one to retain the model's expressivity after re-parameterization.
\end{itemize}
Together, these properties form the basis of a new modeling paradigm we refer to as \emph{neural generation and interpretable execution}, which ensures that interpretability equivariance can be verified in a scalable manner.
% \paragraph{Neural Generation, Interpretable Execution}
% To simultaneously achieve both expressivity and interpretability, we require the \emph{functional transparency} of traditional interpretable systems (e.g., decision trees), the \emph{semantic transparency} of concept-based approaches combined with the predictive performance of DNNs.
% To this end, we propose a general approach that enables us to construct an accurate and interpretable predictive system using DNNs. 

\paragraph{Neural Generation, Interpretable Execution}
To concretely instantiate our proposal, consider a classification problem where the objective is to predict a target label $Y$ from a set of low-level features (e.g., pixel intensities) $X$.
Rather than using an opaque monolithic model, we propose to leverage the expressive power of deep neural networks (DNNs) to generate (i) the parameters of a transparent model $W$, and (ii) human-understandable data representations $C$ (a.k.a., concepts)---which together form the elements of an interpretable system. The learned transparent model is then symbolically executed to make predictions $Y$:
\begin{equation}
    \Prob(Y \mid X; \theta) = \int_{W} \sum_C 
    \overbrace{\Prob(Y \mid C; W)}^{\shortstack{\scriptsize \text{interpretable execution} \\ \scriptsize  \textit{(interpretability)}}}  \overbrace{\Prob(C, W \mid x; \theta_g)}^{\shortstack{\scriptsize \text{neural generation} \\ \scriptsize  \textit{(accuracy)}}}
\end{equation}
These two factors represent the neural generation component $\Prob(C, W \mid X; \theta)$, which re-parametrizes concept representations and functional parameters to ensure expressivity, and the symbolic execution component $\Prob(Y \mid C; W)$, which guarantees interpretability in the decision-making process. 
We refer to the family of models implementing this paradigm as \textit{Neural Interpretable Reasoning}. This family integrates deep neural network expressivity with interpretability by combining semantic transparency, functional transparency, and scalable verification of inference equivariance. 
\begin{wrapfigure}[15]{r}{0.6\linewidth}
    \centering
    \includegraphics[width=\linewidth]{NIR_paradigm.pdf}
    \caption{Neural Interpretable Reasoning.}
    \label{fig:nir}
\end{wrapfigure}
Fig~\ref{fig:nir} shows a NIR example where a self-driving car must decide whether to brake at an intersection. The architecture first generates both the truth degrees of relevant concepts (e.g., the presence of an ambulance or a green light) and the weights of a simple linear model (e.g., an ambulance is assigned a weight of $2$ because it is positively correlated with braking); then, the linear model is executed on these truth degrees to predict whether to brake.
Many well-known XAI techniques can be seen as special cases within this framework. For example, Prototypical Networks (ProtopNets)~\citep{chen2019looks}, Neural Additive Models~\citep{agarwal2021neural}, and Concept Bottleneck Models~\citep{koh2020concept} all embody aspects of interpretability that align with our proposed approach. More recently, novel approaches such as Concept Memory Reasoning~\citep{debot2024interpretable} and Explanation Bottleneck Models~\citep{yamaguchi2024explanation} have begun to fully exploit the potential of functional re-parametrization retaining the expressivity of traditional, opaque deep neural networks while supporting the scalable verification of interpretability.




% NIR comprises two main steps:
% \begin{itemize}
%     \item \textbf{Neural Generation}: By isolating concepts $C$ from parameters $W$, NIR enables to use DNNs to enhance a model expressivity. In particular, DNNs are used to:
%         \begin{enumerate}
%             \item \emph{Extract semantically transparent data representations} $C$, which are concepts corresponding to high-level representations of the input $X$. These concepts might include shapes or object attributes in image data, sentiment or themes in textual data, and structure or sub-graphs in the relational context. We discuss this step in Section~\ref{sec:conc_enc}. 
%             \item \emph{Generate the parameters of a transparent function} $f(\theta_\iota): C \mapsto Y$, such as a linear equation or a decision rule. We discuss this step in Section~\ref{sec:nerual_gen}.
%         \end{enumerate}
%     \item \textbf{Interpretable Execution}: The generated model $f(\theta_\iota)$ is applied to the semantically transparent units of information $c$ to produce the final output $y$. We discuss this topic in Section \ref{sec:ICEM}.
% \end{itemize}


\section{Conclusions}
In this paper, we introduced a novel framework for assessing and achieving interpretability, anchored in the principle of inference equivariance. Drawing inspiration from the Turing test procedure, we proposed that a system is interpretable if a user can reliably predict its behavior. In our discussion we argue that verifying interpretability directly scales exponentially in the number of variables even in simple cases. However, this complexity can be mitigated considering interpretability as a Markovian property and techniques such as neural re-parametrization which can break down complexity without sacrificing overall the model expressivity.
Building on these insights, we proposed a new modeling paradigm, \emph{neural generation and interpretable execution}, which integrates semantic transparency, functional transparency, and scalable verification of equivariance. This paradigm provides a promising pathway for designing Neural Interpretable Reasoners that are not only expressive but also transparent.


\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}

\newpage
\appendix
% \section{Submission of conference papers to ICLR 2025}

% ICLR requires electronic submissions, processed by
% \url{https://openreview.net/}. See ICLR's website for more instructions.

% If your paper is ultimately accepted, the statement {\tt
%   {\textbackslash}iclrfinalcopy} should be inserted to adjust the
% format to the camera ready requirements.

% The format for the submissions is a variant of the NeurIPS format.
% Please read carefully the instructions below, and follow them
% faithfully.

% \subsection{Style}

% Papers to be submitted to ICLR 2025 must be prepared according to the
% instructions presented here.

% %% Please note that we have introduced automatic line number generation
% %% into the style file for \LaTeXe. This is to help reviewers
% %% refer to specific lines of the paper when they make their comments. Please do
% %% NOT refer to these line numbers in your paper as they will be removed from the
% %% style file for the final version of accepted papers.

% Authors are required to use the ICLR \LaTeX{} style files obtainable at the
% ICLR website. Please make sure you use the current files and
% not previous versions. Tweaking the style files may be grounds for rejection.

% \subsection{Retrieval of style files}

% The style files for ICLR and other conference information are available online at:
% \begin{center}
%    \url{http://www.iclr.cc/}
% \end{center}
% The file \verb+iclr2025_conference.pdf+ contains these
% instructions and illustrates the
% various formatting requirements your ICLR paper must satisfy.
% Submissions must be made using \LaTeX{} and the style files
% \verb+iclr2025_conference.sty+ and \verb+iclr2025_conference.bst+ (to be used with \LaTeX{}2e). The file
% \verb+iclr2025_conference.tex+ may be used as a ``shell'' for writing your paper. All you
% have to do is replace the author, title, abstract, and text of the paper with
% your own.

% The formatting instructions contained in these style files are summarized in
% sections \ref{gen_inst}, \ref{headings}, and \ref{others} below.

% \section{General formatting instructions}
% \label{gen_inst}

% The text must be confined within a rectangle 5.5~inches (33~picas) wide and
% 9~inches (54~picas) long. The left margin is 1.5~inch (9~picas).
% Use 10~point type with a vertical spacing of 11~points. Times New Roman is the
% preferred typeface throughout. Paragraphs are separated by 1/2~line space,
% with no indentation.

% Paper title is 17~point, in small caps and left-aligned.
% All pages should start at 1~inch (6~picas) from the top of the page.

% Authors' names are
% set in boldface, and each name is placed above its corresponding
% address. The lead author's name is to be listed first, and
% the co-authors' names are set to follow. Authors sharing the
% same address can be on the same line.

% Please pay special attention to the instructions in section \ref{others}
% regarding figures, tables, acknowledgments, and references.


% There will be a strict upper limit of 10 pages for the main text of the initial submission, with unlimited additional pages for citations. 

% \section{Headings: first level}
% \label{headings}

% First level headings are in small caps,
% flush left and in point size 12. One line space before the first level
% heading and 1/2~line space after the first level heading.

% \subsection{Headings: second level}

% Second level headings are in small caps,
% flush left and in point size 10. One line space before the second level
% heading and 1/2~line space after the second level heading.

% \subsubsection{Headings: third level}

% Third level headings are in small caps,
% flush left and in point size 10. One line space before the third level
% heading and 1/2~line space after the third level heading.

% \section{Citations, figures, tables, references}
% \label{others}

% These instructions apply to everyone, regardless of the formatter being used.

% \subsection{Citations within the text}

% Citations within the text should be based on the \texttt{natbib} package
% and include the authors' last names and year (with the ``et~al.'' construct
% for more than two authors). When the authors or the publication are
% included in the sentence, the citation should not be in parenthesis using \verb|\citet{}| (as
% in ``See \citet{Hinton06} for more information.''). Otherwise, the citation
% should be in parenthesis using \verb|\citep{}| (as in ``Deep learning shows promise to make progress
% towards AI~\citep{Bengio+chapter2007}.'').

% The corresponding references are to be listed in alphabetical order of
% authors, in the \textsc{References} section. As to the format of the
% references themselves, any style is acceptable as long as it is used
% consistently.

% \subsection{Footnotes}

% Indicate footnotes with a number\footnote{Sample of the first footnote} in the
% text. Place the footnotes at the bottom of the page on which they appear.
% Precede the footnote with a horizontal rule of 2~inches
% (12~picas).\footnote{Sample of the second footnote}

% \subsection{Figures}

% All artwork must be neat, clean, and legible. Lines should be dark
% enough for purposes of reproduction; art work should not be
% hand-drawn. The figure number and caption always appear after the
% figure. Place one line space before the figure caption, and one line
% space after the figure. The figure caption is lower case (except for
% first word and proper nouns); figures are numbered consecutively.

% Make sure the figure caption does not get separated from the figure.
% Leave sufficient space to avoid splitting the figure and figure caption.

% You may use color figures.
% However, it is best for the
% figure captions and the paper body to make sense if the paper is printed
% either in black/white or in color.
% \begin{figure}[h]
% \begin{center}
% %\framebox[4.0in]{$\;$}
% \fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
% \end{center}
% \caption{Sample figure caption.}
% \end{figure}

% \subsection{Tables}

% All tables must be centered, neat, clean and legible. Do not use hand-drawn
% tables. The table number and title always appear before the table. See
% Table~\ref{sample-table}.

% Place one line space before the table title, one line space after the table
% title, and one line space after the table. The table title must be lower case
% (except for first word and proper nouns); tables are numbered consecutively.

% \begin{table}[t]
% \caption{Sample table title}
% \label{sample-table}
% \begin{center}
% \begin{tabular}{ll}
% \multicolumn{1}{c}{\bf PART}  &\multicolumn{1}{c}{\bf DESCRIPTION}
% \\ \hline \\
% Dendrite         &Input terminal \\
% Axon             &Output terminal \\
% Soma             &Cell body (contains cell nucleus) \\
% \end{tabular}
% \end{center}
% \end{table}

% \section{Default Notation}

% In an attempt to encourage standardized notation, we have included the
% notation file from the textbook, \textit{Deep Learning}
% \cite{goodfellow2016deep} available at
% \url{https://github.com/goodfeli/dlbook_notation/}.  Use of this style
% is not required and can be disabled by commenting out
% \texttt{math\_commands.tex}.


% \centerline{\bf Numbers and Arrays}
% \bgroup
% \def\arraystretch{1.5}
% \begin{tabular}{p{1in}p{3.25in}}
% $\displaystyle a$ & A scalar (integer or real)\\
% $\displaystyle \va$ & A vector\\
% $\displaystyle \mA$ & A matrix\\
% $\displaystyle \tA$ & A tensor\\
% $\displaystyle \mI_n$ & Identity matrix with $n$ rows and $n$ columns\\
% $\displaystyle \mI$ & Identity matrix with dimensionality implied by context\\
% $\displaystyle \ve^{(i)}$ & Standard basis vector $[0,\dots,0,1,0,\dots,0]$ with a 1 at position $i$\\
% $\displaystyle \text{diag}(\va)$ & A square, diagonal matrix with diagonal entries given by $\va$\\
% $\displaystyle \ra$ & A scalar random variable\\
% $\displaystyle \rva$ & A vector-valued random variable\\
% $\displaystyle \rmA$ & A matrix-valued random variable\\
% \end{tabular}
% \egroup
% \vspace{0.25cm}

% \centerline{\bf Sets and Graphs}
% \bgroup
% \def\arraystretch{1.5}

% \begin{tabular}{p{1.25in}p{3.25in}}
% $\displaystyle \sA$ & A set\\
% $\displaystyle \R$ & The set of real numbers \\
% $\displaystyle \{0, 1\}$ & The set containing 0 and 1 \\
% $\displaystyle \{0, 1, \dots, n \}$ & The set of all integers between $0$ and $n$\\
% $\displaystyle [a, b]$ & The real interval including $a$ and $b$\\
% $\displaystyle (a, b]$ & The real interval excluding $a$ but including $b$\\
% $\displaystyle \sA \backslash \sB$ & Set subtraction, i.e., the set containing the elements of $\sA$ that are not in $\sB$\\
% $\displaystyle \gG$ & A graph\\
% $\displaystyle \parents_\gG(\ervx_i)$ & The parents of $\ervx_i$ in $\gG$
% \end{tabular}
% \vspace{0.25cm}


% \centerline{\bf Indexing}
% \bgroup
% \def\arraystretch{1.5}

% \begin{tabular}{p{1.25in}p{3.25in}}
% $\displaystyle \eva_i$ & Element $i$ of vector $\va$, with indexing starting at 1 \\
% $\displaystyle \eva_{-i}$ & All elements of vector $\va$ except for element $i$ \\
% $\displaystyle \emA_{i,j}$ & Element $i, j$ of matrix $\mA$ \\
% $\displaystyle \mA_{i, :}$ & Row $i$ of matrix $\mA$ \\
% $\displaystyle \mA_{:, i}$ & Column $i$ of matrix $\mA$ \\
% $\displaystyle \etA_{i, j, k}$ & Element $(i, j, k)$ of a 3-D tensor $\tA$\\
% $\displaystyle \tA_{:, :, i}$ & 2-D slice of a 3-D tensor\\
% $\displaystyle \erva_i$ & Element $i$ of the random vector $\rva$ \\
% \end{tabular}
% \egroup
% \vspace{0.25cm}


% \centerline{\bf Calculus}
% \bgroup
% \def\arraystretch{1.5}
% \begin{tabular}{p{1.25in}p{3.25in}}
% % NOTE: the [2ex] on the next line adds extra height to that row of the table.
% % Without that command, the fraction on the first line is too tall and collides
% % with the fraction on the second line.
% $\displaystyle\frac{d y} {d x}$ & Derivative of $y$ with respect to $x$\\ [2ex]
% $\displaystyle \frac{\partial y} {\partial x} $ & Partial derivative of $y$ with respect to $x$ \\
% $\displaystyle \nabla_\vx y $ & Gradient of $y$ with respect to $\vx$ \\
% $\displaystyle \nabla_\mX y $ & Matrix derivatives of $y$ with respect to $\mX$ \\
% $\displaystyle \nabla_\tX y $ & Tensor containing derivatives of $y$ with respect to $\tX$ \\
% $\displaystyle \frac{\partial f}{\partial \vx} $ & Jacobian matrix $\mJ \in \R^{m\times n}$ of $f: \R^n \rightarrow \R^m$\\
% $\displaystyle \nabla_\vx^2 f(\vx)\text{ or }\mH( f)(\vx)$ & The Hessian matrix of $f$ at input point $\vx$\\
% $\displaystyle \int f(\vx) d\vx $ & Definite integral over the entire domain of $\vx$ \\
% $\displaystyle \int_\sS f(\vx) d\vx$ & Definite integral with respect to $\vx$ over the set $\sS$ \\
% \end{tabular}
% \egroup
% \vspace{0.25cm}

% \centerline{\bf Probability and Information Theory}
% \bgroup
% \def\arraystretch{1.5}
% \begin{tabular}{p{1.25in}p{3.25in}}
% $\displaystyle P(\ra)$ & A probability distribution over a discrete variable\\
% $\displaystyle p(\ra)$ & A probability distribution over a continuous variable, or over
% a variable whose type has not been specified\\
% $\displaystyle \ra \sim P$ & Random variable $\ra$ has distribution $P$\\% so thing on left of \sim should always be a random variable, with name beginning with \r
% $\displaystyle  \E_{\rx\sim P} [ f(x) ]\text{ or } \E f(x)$ & Expectation of $f(x)$ with respect to $P(\rx)$ \\
% $\displaystyle \Var(f(x)) $ &  Variance of $f(x)$ under $P(\rx)$ \\
% $\displaystyle \Cov(f(x),g(x)) $ & Covariance of $f(x)$ and $g(x)$ under $P(\rx)$\\
% $\displaystyle H(\rx) $ & Shannon entropy of the random variable $\rx$\\
% $\displaystyle \KL ( P \Vert Q ) $ & Kullback-Leibler divergence of P and Q \\
% $\displaystyle \mathcal{N} ( \vx ; \vmu , \mSigma)$ & Gaussian distribution %
% over $\vx$ with mean $\vmu$ and covariance $\mSigma$ \\
% \end{tabular}
% \egroup
% \vspace{0.25cm}

% \centerline{\bf Functions}
% \bgroup
% \def\arraystretch{1.5}
% \begin{tabular}{p{1.25in}p{3.25in}}
% $\displaystyle f: \sA \rightarrow \sB$ & The function $f$ with domain $\sA$ and range $\sB$\\
% $\displaystyle f \circ g $ & Composition of the functions $f$ and $g$ \\
%   $\displaystyle f(\vx ; \vtheta) $ & A function of $\vx$ parametrized by $\vtheta$.
%   (Sometimes we write $f(\vx)$ and omit the argument $\vtheta$ to lighten notation) \\
% $\displaystyle \log x$ & Natural logarithm of $x$ \\
% $\displaystyle \sigma(x)$ & Logistic sigmoid, $\displaystyle \frac{1} {1 + \exp(-x)}$ \\
% $\displaystyle \zeta(x)$ & Softplus, $\log(1 + \exp(x))$ \\
% $\displaystyle || \vx ||_p $ & $\normlp$ norm of $\vx$ \\
% $\displaystyle || \vx || $ & $\normltwo$ norm of $\vx$ \\
% $\displaystyle x^+$ & Positive part of $x$, i.e., $\max(0,x)$\\
% $\displaystyle \1_\mathrm{condition}$ & is 1 if the condition is true, 0 otherwise\\
% \end{tabular}
% \egroup
% \vspace{0.25cm}



% \section{Final instructions}
% Do not change any aspects of the formatting parameters in the style files.
% In particular, do not modify the width or length of the rectangle the text
% should fit into, and do not change font sizes (except perhaps in the
% \textsc{References} section; see below). Please note that pages should be
% numbered.

% \section{Preparing PostScript or PDF files}

% Please prepare PostScript or PDF files with paper size ``US Letter'', and
% not, for example, ``A4''. The -t
% letter option on dvips will produce US Letter files.

% Consider directly generating PDF files using \verb+pdflatex+
% (especially if you are a MiKTeX user).
% PDF figures must be substituted for EPS figures, however.

% Otherwise, please generate your PostScript and PDF files with the following commands:
% \begin{verbatim}
% dvips mypaper.dvi -t letter -Ppdf -G0 -o mypaper.ps
% ps2pdf mypaper.ps mypaper.pdf
% \end{verbatim}

% \subsection{Margins in LaTeX}

% Most of the margin problems come from figures positioned by hand using
% \verb+\special+ or other commands. We suggest using the command
% \verb+\includegraphics+
% from the graphicx package. Always specify the figure width as a multiple of
% the line width as in the example below using .eps graphics
% \begin{verbatim}
%    \usepackage[dvips]{graphicx} ...
%    \includegraphics[width=0.8\linewidth]{myfile.eps}
% \end{verbatim}
% or % Apr 2009 addition
% \begin{verbatim}
%    \usepackage[pdftex]{graphicx} ...
%    \includegraphics[width=0.8\linewidth]{myfile.pdf}
% \end{verbatim}
% for .pdf graphics.
% See section~4.4 in the graphics bundle documentation (\url{http://www.ctan.org/tex-archive/macros/latex/required/graphics/grfguide.ps})

% A number of width problems arise when LaTeX cannot properly hyphenate a
% line. Please give LaTeX hyphenation hints using the \verb+\-+ command.

% \subsubsection*{Author Contributions}
% If you'd like to, you may include  a section for author contributions as is done
% in many journals. This is optional and at the discretion of the authors.

% \subsubsection*{Acknowledgments}
% Use unnumbered third level headings for the acknowledgments. All
% acknowledgments, including those to funding agencies, go at the end of the paper.


% % \bibliography{iclr2025_conference}
% % \bibliographystyle{iclr2025_conference}

% \appendix
% \section{Appendix}
% You may include other additional sections here.


\end{document}
