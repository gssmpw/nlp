@inproceedings{deng-etal-2024-multi,
    title = "On the Multi-turn Instruction Following for Conversational Web Agents",
    author = "Deng, Yang  and
      Zhang, Xuan  and
      Zhang, Wenxuan  and
      Yuan, Yifei  and
      Ng, See-Kiong  and
      Chua, Tat-Seng",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.477/",
    doi = "10.18653/v1/2024.acl-long.477",
    pages = "8795--8812",
    abstract = "Web agents powered by Large Language Models (LLMs) have demonstrated remarkable abilities in planning and executing multi-step interactions within complex web-based environments, fulfilling a wide range of web navigation tasks. Despite these advancements, the potential for LLM-powered agents to effectively engage with sequential user instructions in real-world scenarios has not been fully explored. In this work, we introduce a new task of Conversational Web Navigation, which necessitates sophisticated interactions that span multiple turns with both the users and the environment, supported by a specially developed dataset named Multi-Turn Mind2Web (MT-Mind2Web). To tackle the limited context length of LLMs and the context-dependency issue of the conversational tasks, we further propose a novel framework, named self-reflective memory-augmented planning (Self-MAP), which employs memory utilization and self-reflection techniques. Extensive experiments are conducted to benchmark the MT-Mind2Web dataset, and validate the effectiveness of the proposed method."
}

@inproceedings{deng2024mind2web,
 author = {Deng, Xiang and Gu, Yu and Zheng, Boyuan and Chen, Shijie and Stevens, Sam and Wang, Boshi and Sun, Huan and Su, Yu},
 booktitle = {Proceedings of Advances in Neural Information Processing Systems 36 (NeurIPS 2023) Datasets and Benchmarks Track},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {28091--28114},
 publisher = {Curran Associates, Inc.},
 title = {{Mind2Web}: Towards a Generalist Agent for the Web},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/5950bf290a1570ea401bf98882128160-Paper-Datasets_and_Benchmarks.pdf},
 volume = {36},
 year = {2023}
}

@inproceedings{he2024webvoyager,
    title = "{W}eb{V}oyager: Building an End-to-End Web Agent with Large Multimodal Models",
    author = "He, Hongliang  and
      Yao, Wenlin  and
      Ma, Kaixin  and
      Yu, Wenhao  and
      Dai, Yong  and
      Zhang, Hongming  and
      Lan, Zhenzhong  and
      Yu, Dong",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.371",
    doi = "10.18653/v1/2024.acl-long.371",
    pages = "6864--6890",
    abstract = "The rapid advancement of large language models (LLMs) has led to a new era marked by the development of autonomous applications in real-world scenarios, which drives innovation in creating advanced web agents. Existing web agents typically only handle one input modality and are evaluated only in simplified web simulators or static web snapshots, greatly limiting their applicability in real-world scenarios. To bridge this gap, we introduce WebVoyager, an innovative Large Multimodal Model (LMM) powered web agent that can complete user instructions end-to-end by interacting with real-world websites. Moreover, we establish a new benchmark by compiling real-world tasks from 15 popular websites and introduce an automatic evaluation protocol leveraging multimodal understanding abilities of GPT-4V to evaluate open-ended web agents. We show that WebVoyager achieves a 59.1{\%} task success rate on our benchmark, significantly surpassing the performance of both GPT-4 (All Tools) and the WebVoyager (text-only) setups, underscoring the exceptional capability of WebVoyager. The proposed automatic evaluation metric achieves 85.3{\%} agreement with human judgment, indicating its effectiveness in providing reliable and accurate assessments of web agents.",
}

@inproceedings{hong2024cogagent,
  title={{C}og{A}gent: A visual language model for gui agents},
  author={Hong, Wenyi and Wang, Weihan and Lv, Qingsong and Xu, Jiazheng and Yu, Wenmeng and Ji, Junhui and Wang, Yan and Wang, Zihan and Dong, Yuxiao and Ding, Ming and others},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={14281--14290},
  year={2024}
}

@inproceedings{humphreys2022data,
  title={A data-driven approach for learning to control computers},
  author={Humphreys, Peter C and Raposo, David and Pohlen, Tobias and Thornton, Gregory and Chhaparia, Rachita and Muldal, Alistair and Abramson, Josh and Georgiev, Petko and Santoro, Adam and Lillicrap, Timothy},
  booktitle={International Conference on Machine Learning},
  pages={9466--9482},
  year={2022},
  organization={PMLR}
}

@inproceedings{koh2024visualwebarena,
    title = "{V}isual{W}eb{A}rena: Evaluating Multimodal Agents on Realistic Visual Web Tasks",
    author = "Koh, Jing Yu  and
      Lo, Robert  and
      Jang, Lawrence  and
      Duvvur, Vikram  and
      Lim, Ming  and
      Huang, Po-Yu  and
      Neubig, Graham  and
      Zhou, Shuyan  and
      Salakhutdinov, Russ  and
      Fried, Daniel",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.50",
    doi = "10.18653/v1/2024.acl-long.50",
    pages = "881--905",
    abstract = "Autonomous agents capable of planning, reasoning, and executing actions on the web offer a promising avenue for automating computer tasks. However, the majority of existing benchmarks primarily focus on text-based agents, neglecting many natural tasks that require visual information to effectively solve. Given that most computer interfaces cater to human perception, visual information often augments textual data in ways that text-only models struggle to harness effectively. To bridge this gap, we introduce VisualWebArena, a benchmark designed to assess the performance of multimodal web agents on *realistic visually grounded tasks*. VisualWebArena comprises of a set of diverse and complex web-based tasks that evaluate various capabilities of autonomous multimodal agents. To perform on this benchmark, agents need to accurately process image-text inputs, interpret natural language instructions, and execute actions on websites to accomplish user-defined objectives. We conduct an extensive evaluation of state-of-the-art LLM-based autonomous agents, including several multimodal models. Through extensive quantitative and qualitative analysis, we identify several limitations of text-only LLM agents, and reveal gaps in the capabilities of state-of-the-art multimodal language agents. VisualWebArena provides a framework for evaluating multimodal autonomous language agents, and offers insights towards building stronger autonomous agents for the web.",
}

@inproceedings{lai2024autowebglm,
author = {Lai, Hanyu and Liu, Xiao and Iong, Iat Long and Yao, Shuntian and Chen, Yuxuan and Shen, Pengbo and Yu, Hao and Zhang, Hanchen and Zhang, Xiaohan and Dong, Yuxiao and Tang, Jie},
title = {{AutoWebGLM}: A Large Language Model-based Web Navigating Agent},
year = {2024},
isbn = {9798400704901},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637528.3671620},
doi = {10.1145/3637528.3671620},
abstract = {Large language models (LLMs) have fueled many intelligent web agents, but most existing ones perform far from satisfying in real-world web navigation tasks due to three factors: (1) the complexity of HTML text data (2) versatility of actions on webpages, and (3) task difficulty due to the open-domain nature of the web. In light of these challenges, we develop the open AutoWebGLM based on ChatGLM3-6B. AutoWebGLM can serve as a powerful automated web navigation agent that outperform GPT-4. Inspired by human browsing patterns, we first design an HTML simplification algorithm to represent webpages with vital information preserved succinctly. We then employ a hybrid human-AI method to build web browsing data for curriculum training. Finally, we bootstrap the model by reinforcement learning and rejection sampling to further facilitate webpage comprehension, browser operations, and efficient task decomposition by itself. For comprehensive evaluation, we establish a bilingual benchmark---AutoWebBench---for real-world web navigation tasks. We evaluate AutoWebGLM across diverse web navigation benchmarks, demonstrating its potential to tackle challenging tasks in real environments. Related code, model, and data are released at https://github.com/THUDM/AutoWebGLM.},
booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {5295â€“5306},
numpages = {12},
keywords = {chatglm, large language model, llm agent, reinforcement learning, rejection sampling finetuning, web agent},
location = {Barcelona, Spain},
series = {KDD '24}
}

@inproceedings{liu2018learning,
  title={Learning design semantics for mobile apps},
  author={Liu, Thomas F and Craft, Mark and Situ, Jason and Yumer, Ersin and Mech, Radomir and Kumar, Ranjitha},
  booktitle={Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology},
  pages={569--579},
  year={2018}
}

@inproceedings{pan2024autonomous,
  title={Autonomous evaluation and refinement of digital agents},
  author={Pan, Jiayi and Zhang, Yichi and Tomlin, Nicholas and Zhou, Yifei and Levine, Sergey and Suhr, Alane},
  booktitle={First Conference on Language Modeling},
  year={2024}
}

@inproceedings{qian-etal-2024-chatdev,
    title = "{C}hat{D}ev: Communicative Agents for Software Development",
    author = "Qian, Chen  and
      Liu, Wei  and
      Liu, Hongzhang  and
      Chen, Nuo  and
      Dang, Yufan  and
      Li, Jiahao  and
      Yang, Cheng  and
      Chen, Weize  and
      Su, Yusheng  and
      Cong, Xin  and
      Xu, Juyuan  and
      Li, Dahai  and
      Liu, Zhiyuan  and
      Sun, Maosong",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.810/",
    doi = "10.18653/v1/2024.acl-long.810",
    pages = "15174--15186",
    abstract = "Software development is a complex task that necessitates cooperation among multiple members with diverse skills. Numerous studies used deep learning to improve specific phases in a waterfall model, such as design, coding, and testing. However, the deep learning model in each phase requires unique designs, leading to technical inconsistencies across various phases, which results in a fragmented and ineffective development process. In this paper, we introduce ChatDev, a chat-powered software development framework in which specialized agents driven by large language models (LLMs) are guided in what to communicate (via chat chain) and how to communicate (via communicative dehallucination). These agents actively contribute to the design, coding, and testing phases through unified language-based communication, with solutions derived from their multi-turn dialogues. We found their utilization of natural language is advantageous for system design, and communicating in programming language proves helpful in debugging. This paradigm demonstrates how linguistic communication facilitates multi-agent collaboration, establishing language as a unifying bridge for autonomous task-solving among LLM agents. The code and data are available at https://github.com/OpenBMB/ChatDev."
}

@inproceedings{rawles2024androidinthewild,
 author = {Rawles, Christopher and Li, Alice and Rodriguez, Daniel and Riva, Oriana and Lillicrap, Timothy},
 booktitle = {Proceedings of Advances in Neural Information Processing Systems 36 (NeurIPS 2023) Datasets and Benchmarks Track},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {59708--59728},
 publisher = {Curran Associates, Inc.},
 title = {AndroidInTheWild: A Large-Scale Dataset For Android Device Control},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/bbbb6308b402fe909c39dd29950c32e0-Paper-Datasets_and_Benchmarks.pdf},
 volume = {36},
 year = {2023}
}

@inproceedings{shi2017world,
  title={World of bits: An open-domain platform for web-based agents},
  author={Shi, Tianlin and Karpathy, Andrej and Fan, Linxi and Hernandez, Jonathan and Liang, Percy},
  booktitle={International Conference on Machine Learning},
  pages={3135--3144},
  year={2017},
  organization={PMLR}
}

@inproceedings{wang2023enabling,
  title={Enabling conversational interaction with mobile {UI} using large language models},
  author={Wang, Bryan and Li, Gang and Li, Yang},
  booktitle={Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
  pages={1--17},
  year={2023}
}

@article{wang2025mobileagenteselfevolvingmobileassistant,
      title={Mobile-Agent-E: Self-Evolving Mobile Assistant for Complex Tasks}, 
      author={Zhenhailong Wang and Haiyang Xu and Junyang Wang and Xi Zhang and Ming Yan and Ji Zhang and Fei Huang and Heng Ji},
      year={2025},
      journal={arXiv preprint arXiv:2501.11733},
      url={https://arxiv.org/abs/2501.11733}
}

@inproceedings{wu2024copilot,
    title={{OS}-Copilot: Towards Generalist Computer Agents with Self-Improvement},
    author={Zhiyong Wu and Chengcheng Han and Zichen Ding and Zhenmin Weng and Zhoumianze Liu and Shunyu Yao and Tao Yu and Lingpeng Kong},
    booktitle={Proceedings of the ICLR 2024 Workshop on Large Language Model ({LLM}) Agents},
    year={2024},
    url={https://openreview.net/forum?id=3WWFrg8UjJ}
}

@article{xu2024crab,
  title={{CRAB}: Cross-environment Agent Benchmark for Multimodal Language Model Agents},
  author={Xu, Tianqi and Chen, Linyao and Wu, Dai-Jie and Chen, Yanjun and Zhang, Zecheng and Yao, Xiang and Xie, Zhiqiang and Chen, Yongchao and Liu, Shilong and Qian, Bochen and others},
  journal={arXiv preprint arXiv:2407.01511},
  year={2024},
  url={https://arxiv.org/abs/2407.01511}
}

@inproceedings{zhou2023webarena,
title={{WebArena}: A Realistic Web Environment for Building Autonomous Agents},
author={Shuyan Zhou and Frank F. Xu and Hao Zhu and Xuhui Zhou and Robert Lo and Abishek Sridhar and Xianyi Cheng and Tianyue Ou and Yonatan Bisk and Daniel Fried and Uri Alon and Graham Neubig},
booktitle={Proceedings of The Twelfth International Conference on Learning Representations (ICLR 2024)},
year={2024},
url={https://openreview.net/forum?id=oKn9c6ytLx}
}

