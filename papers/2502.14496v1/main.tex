% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[]{acl}
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{pifont}  
\usepackage{xcolor}  
\usepackage{amsfonts,amssymb}
\usepackage{subcaption} 
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{makecell} 
\usepackage{bm}
\usepackage{amsmath}
%\usepackage[table]{xcolor}
\usepackage[normalem]{ulem}
\usepackage{colortbl}
\usepackage{float}
\usepackage{wrapfig}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{xspace}
\usepackage{tcolorbox}
\usepackage{natbib}

\newcommand{\cmark}{\textcolor{green}{\ding{51}}} 
\newcommand{\xmark}{\textcolor{red}{\ding{55}}}  
\newcommand{\CollabUIAgentsRaw}{CollabUIAgents}
\newcommand{\CollabUIAgents}{\CollabUIAgentsRaw\xspace}
\newcommand{\CollabUIAgentsMobile}{\CollabUIAgentsRaw\textsubscript{mobile}\xspace}
\newcommand{\CollabUIAgentsWeb}{\CollabUIAgentsRaw\textsubscript{m$\rightarrow$web}\xspace}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Enhancing Language Multi-Agent Learning with Multi-Agent\\ Credit Re-Assignment for Interactive Environment Generalization}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{Zhitao He\textsuperscript{\rm 3*$^\dagger$}Zijun Liu\textsuperscript{\rm 2*},
Peng Li\textsuperscript{\rm 1},
May Fung\textsuperscript{\rm 3}, 
Ming Yan\textsuperscript{\rm 4} \\
\textbf{Ji Zhang\textsuperscript{\rm 4},
Fei Huang\textsuperscript{\rm 4},
Yang Liu\textsuperscript{\rm 1,2}}\\
  ${}^1$Institute for AI Industry Research (AIR), Tsinghua University\\
${}^2$Department of Computer Science and Technology, Tsinghua University\\
${}^3$Hong Kong University of Science and Technology, ${}^4$Alibaba Group \\
\texttt{zhitao.he@connect.ust.hk, zj-liu24@mails.tsinghua.edu.cn}
  % \texttt{email@domain} \\\And
  % Second Author \\
  % Affiliation / Address line 1 \\
  % Affiliation / Address line 2 \\
  % Affiliation / Address line 3 \\
  % \texttt{email@domain} \\
  }

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle
\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\footnotetext[1]{Equal contribuation.}
\footnotetext[2]{Work done at Institute for AI Industry Research (AIR), Tsinghua University.}
\begin{abstract}
LLM-based agents have made significant advancements in interactive environments, such as mobile operations and web browsing, and other domains beyond computer using. Current multi-agent systems universally excel in performance, compared to single agents, but struggle with generalization across environments due to predefined roles and inadequate strategies for generalizing language agents. The challenge of achieving both strong performance and good generalization has hindered the progress of multi-agent systems for interactive environments.
To address these issues, we propose \textbf{\CollabUIAgents}, a multi-agent reinforcement learning framework with a novel multi-agent credit re-assignment (CR) strategy, \emph{assigning process rewards with LLMs rather than environment-specific rewards and learning with synthesized preference data}, in order to foster generalizable, collaborative behaviors among the role-free agents' policies. 
Empirical results show that our framework improves both performance and cross-environment generalizability of multi-agent systems. Moreover, our 7B-parameter system achieves results on par with or exceed strong closed-source models, and the LLM that guides the CR. We also provide insights in using granular CR rewards effectively for environment generalization, and accommodating trained LLMs in multi-agent systems. Our data and code are available at \url{https://github.com/THUNLP-MT/CollabUIAgents}.
\end{abstract}

\section{Introduction}

Autonomous agents have made substantial progress in interactive environments, such as mobile operations and web browsing, by leveraging large language models (LLMs). These agents hold immense potential not only to automate repetitive tasks but also to enhance decision-making and streamline complex workflows. As a result, they can free up human resources for higher-level problem-solving and innovation. The increasing interest in developing such agents is evident in the growing body of work on, for instance, mobile~\citep{rawles2024androidinthewild, rawles2025androidworld,  zhang-etal-2024-android, deng-etal-2024-mobile, Wang2024MobileAgentBenchAE}, web browsing~\citep{shi2017world, zheran2018reinforcement, yao2022webshop, zhou2024webarena, deng2024mind2web, deng-etal-2024-multi}, and computer using environments~\citep{xie2024osworld, sun2024osgenesisautomatingguiagent}, and LLM-based agents targeting on these tasks, including single-agent~\citep{yan2023gpt, wang2024mobile, hong2024cogagent, cheng-etal-2024-seeclick, os-agents-survey, zhang2025largelanguagemodelbrainedgui} and multi-agent systems~\citep{wang2024mobile2, zhou2023agents, liu2024a, zhang2024webpilotversatileautonomousmultiagent}. 

However, current efforts in language agent learning still face challenges to balance both performance and generalizability across interactive environments. (1) Single-agent learning methods~\citep{chen2023fireact, gur2024a, furuta2024multimodal, bai2024digirl} heavily relies on in-domain supervised data or rewarding signals to improve environment-specific performance, which restricts its generalization across environments, such as transitioning between web environments using HTML and mobile environments using Android automator. (2) Despite being trained on vast amounts of data from diverse domains, single agents based on open-source LLMs~\citep{zeng-etal-2024-agenttuning, zhang2024agentohana, yin-etal-2024-agent, liu2025infiguiagent} demonstrate only moderate generalization capabilities and lag behind closed-source models. (3) Although multi-agent learning methods~\citep{qiao-etal-2024-autoact, liang2024cmatmultiagentcollaborationtuning} have better performance, existing ones are often constrained by rigid role assignments and lack dedicated designs for generalization, which limits their adaptability to unseen environments, e.g., an agent designed to retrieve documents for question answering is not feasible to handle mobile operations. 
In short, \textbf{it is still unclear how to achieve strong performance and good generalization in interactive environments at the same time}. 
% In addition, multi-step nature of interactive tasks results in sparse reward signals during end-to-end learning, which complicates effective learning in real-world interactive environments.

\begin{figure*}[t]
  \centering
  \includegraphics[width=0.95\linewidth]{top-case.pdf}
  \caption{Illustration of (a) single-agent and (b) multi-agent learning for environment generalization. In single-agent learning, the agent could encounter obstacles when target environments are different from the training one, while in multi-agent learning, collaboration between agents might enable effective decision-making in both environments.}
  \label{fig:top-case}
  \vspace{-1em}
\end{figure*}

In this work, we introduce a reinforcement learning framework for language multi-agent systems, named \textbf{CollabUIAgents}, designed to address challenges above in real-world interactive environments. 
The methodology is inspired from the success of classic multi-agent reinforcement learning (MARL) to stimulate collaboration~\citep{ma2022VDPPO}. Beyond previous work~\citep{tran2025multiagentcollaborationmechanismssurvey}, collaboration among langugae agents may also be beneficial for generalization across environments, as illustrated in Figure~\ref{fig:top-case}. Compared to existing methods using LLMs in credit assignment for non-language multi-agent settings~\citep{qu2025latentrewardllmempoweredcredit, lin2025speakinglanguageteamworkllmguided}, further considerations are made in our framework to enrich sparse rewarding signals, enhance generalization, and mitigate errors in credit assignment. 
We propose a novel multi-agent credit re-assignment (CR) strategy, \emph{\uline{assigning process rewards without using environment-specific outcome rewards, but with the world knowledge embedded in LLMs, and learning with synthesized preference data}}, aiming to foster generalizable, collaborative behaviors among the role-free agents' policies.
\vspace{-0.3em}

% Specially, stage 1 focuses on enabling the base model to adapt to the environment through curriculum learning on multi-level instruction data, aimed at {\bf learning general environmental knowledge}. To facilitate this process, we propose a fully automated data synthesis strategy that significantly reduces labor costs and accelerates data acquisition. The synthesized instruction data comprises three parts: (1) basic environmental knowledge, (2) simple instruction knowledge, and (3) process preference knowledge, with a progressively increasing level of difficulty. The base model is first fine-tuned using Supervised Fine-Tuning (SFT)~\citep{rlhf} on the first two data segments, followed by Direct Preference Optimization (DPO)~\citep{rafailov2023direct} using the process preference data.
\looseness=-1 Specially, an agentic fine-tuned model, as the \textit{base UIAgent}, is used to initialize a multi-agent system, where each agent has its own policy. 
After rolling out actions from the policy multi-agent system, the \textit{critic agent} allocates process rewards at both the agent and conversation round levels based on its comprehension of the environment state and agent behaviors. This approach not only enables finer-grained rewards without training numerous value estimators for agents, but also expands data scales by restoring generalizable behaviors from failed trajectories according to the training environment. 
To avoid misleading the agents with incorrect CR, policies are optimized with preference data synthesized by the \textit{adversarial agent}, to ensure guiding them with correct preference signals. After preference optimization
~\citep{rafailov2023direct}, the multi-agent system is updated in both model parameters and edges of the communication structure. Empirically, we show that with the CR strategy, (1) preference optimization benefits performance and generalization; (2) edge updates is crucial to orchestrate trained LLMs in multi-agent systems. 

% Our framework provides much more granular feedback on each agent's contribution throughout the task, enhancing learning effectiveness over previous works. And this framework is also capable of cross-environment user interface (UI) interaction, supporting both mobile and web environments, either through directly applying multi-agent systems adapted from mobile environments to websites or through continue MARL on the new environment. 

\CollabUIAgents is capable of cross-environment user interface (UI) interaction, supporting both mobile and web environments. 
Experimental results demonstrate that the trained multi-agent system achieves superior performance compared to existing agentic learning methods and the strong closed-source model Gemini 1.5 Pro~\citep{geminiteam2024gemini15unlockingmultimodal}, with Qwen2 7B~\citep{yang2024qwen2technicalreport} as the base model. The system also achieves performance comparable to the guidance LLM used in the CR, GPT-4~\citep{openai2024gpt4technicalreport}, in training environments, and even better in an unseen environment. Especially, \CollabUIAgents demonstrates effectiveness in largely gapped generalization from mobile to web environments, still comparable to GPT-4. 

In summary, our contributions are as follows: 
\begin{itemize}
    \setlength\itemsep{0em}
    \item We propose a language MARL framework \textbf{\CollabUIAgents} with a novel CR strategy, to achieve both strong performance and generalization in interactive environments. 
    \item Empirically, we provide insights into the effectiveness of using CR rewards for environment generalization, and the adaptation of trained LLMs in multi-agent systems. 
    \item Extensive experiments show that \CollabUIAgents surpasses the performance of strong baselines and shows competitiveness comparable to the guidance LLM of CR in both trained and target interactive environments, even under cross-environment generalization.
\end{itemize}

\section{Formulation and Notations}


We treat interactive tasks as a sequential decision-making process with single-agent or multi-agent systems in the dynamic environments. Agentic systems make decisions based on the current environment state and accumulated interaction history. 

\noindent
\textbf{Task Formulation} \quad
Let $S$ be the set of all possible states of a given interactive environment, where each $s \in S$ represents a specific configuration of the UI and hidden states of the environment at a given time step. There is an initial state $s_0$ and a terminal state $s^{*}$. The action space of an agentic system $\mathcal{G}$ is denoted as $\mathcal{A}$, where $a \in \mathcal{A}$ could represent an action, e.g., clicking buttons, typing, or scrolling through content. The environment evolves according to a transition function $\mathcal{T}(\cdot, \cdot)$:
\begin{equation}
    s_{t+1} = \mathcal{T}(s_t, a_t), s_t, s_{t+1} \in S, a_t \in \mathcal{A},
\end{equation}
where $s_t$ is the state at time step $t$, and $a_t$ is the action taken by the agent system at that step. The task ends when reaching a terminal state or exceeding the maximum step $T_{\mathrm{max}}$.
From the state $s_t$, the observation $o_t$ is derived as formatted descriptions in language. 
Each agent in the system holds a policy $\pi_i$ and accordingly selects actions based on a shared current observation $o_t$, the history of past interactions $H_{t-1} = (s_0, a_0, ..., s_{t-1}, a_{t-1}) $, and the message for agent $\pi_i$ from other agents at conversation round $j$, denoted as $\mathcal{C}_t
^{i,j}$. Specifically, $\mathcal{C}_t^{i,j}$ is omitted for single agents: 
\begin{equation}\label{eqn:single-agent}
    a_t^{i,j} = \pi_i\left(o_t, H_{t-1}, C_t^{i,j}\right), a_t^{i,j} \in \mathcal{A}, i=1, ... , |\mathcal{G}|,
\end{equation}
where $|\mathcal{G}|$ is the number agents in the system. And $a_t$ is determined by an aggregation function $f_{\mathrm{agg}}$ (which is identity for single agents ($|\mathcal{G}|=1$)):
\begin{equation}\label{eqn:agg}
    a_t = f_{\mathrm{agg}} \left(\left\{ a_t^{i,j} \Big| i=1,\cdots,|\mathcal{G}|; j=1,\cdots,m\right\}\right), 
\end{equation}
where $m$ is the number of conversion rounds. The task goal is to maximize the outcome reward from the environment over a sequence of interactions. 

\noindent
\textbf{Interactive Environment} \quad
The observation and action space in interactive environments are enormous. 
Specifically, for the \textbf{mobile operation environments}, which offer an interface that allows agents to receive observations and perform actions on mobile devices, the observation space may include high-resolution screenshots and a UI tree from Android automater. The action space mirrors human interactions, featuring gestures (such as tapping, long-pressing, and swiping), typing, and navigation buttons (e.g., home and back). Complete actions are listed in Table~\ref{tab:mobile}. For \textbf{web browsing environments}, the observation space may include task description, simplified HTML, and current location. The HTML offers the model both structural and content details of the page, while the current location information allows it to understand its position on the webpage. Consistent with previous work~\citep{lai2024autowebglm}, we use a unified web browsing action space in both of the aforementioned environments. The actions include hover, select, click, etc. Complete actions are listed in Table~\ref{tab:web}.

\noindent
\textbf{Outcome Reward Function} \quad
The outcome reward $R_{o} \in \{0, 1\}$ is defined in the environment based on static rules. Let the terminal step be $t^{*}$, 
\begin{equation}
    R_{o} = \begin{cases}
    1, & \text{if  } s_{t^{*}} = s^{*} \\
    0, & \text{otherwise}
    \end{cases}.
\end{equation}
Thus the outcome reward is sparse, as only the terminal state $s^{*}$ gives out positive rewards, posing a challenge to traditional RL approaches.

\begin{figure}[t]
  \centering
  %\vspace{-1em}
  \includegraphics[width=\linewidth]{model3.pdf} 
  \caption{The \CollabUIAgents framework with credit re-assignment. The critic agent assess the environment state and the action matrix to get granular rewards. Agents are optimized with synthesized preference data from the adversarial agent to learn rewarded behaviors.}
  \label{fig:model}
  \vspace{-1em} 
\end{figure}

\section{\CollabUIAgents Framework}

The \emph{\CollabUIAgents} framework is designed to achieve both high performance and generalizability in interactive environments. It optimizes language multi-agent systems without predefined roles. As shown in Figure~\ref{fig:model}, the base model undergoes agentic fine-tuning (detailed in Appendix~\ref{app:aft}) and then MARL process. This section elaborates the multi-agent system architecture and the multi-agent learning process with multi-agent credit re-assignment.

\subsection{Multi-Agent System Architecture}
\label{sec:ma-arch}

The architecture of the multi-agent system ($\mathcal{G}$) in \CollabUIAgents is in consistency with previous works~\citep{gpt-swarm, liu2024a}, which consists of $|\mathcal{G}| =n$ agents, each represented by a policy $\pi_i$ that communicate with each other through a message network $\mathcal{E}_\mathcal{G}$. As shown in Figure~\ref{fig:model}, the network is a directed acyclic graph (DAG), where messages are passed from $\pi_{i_1}$ to $\pi_{i_2}$ if there is an edge pointing from $\pi_{i_1}$ to $\pi_{i_2}$. Specifically, the message is from the output of $\pi_{i_1}$. It is worth noting that the framework remains the compatibility for more complex architecture, which is left for future work. We instead use DAGs for simplicity. % TODO: refer to Appendix

The agents operate in a topological order, and starting from the source to the sink node, allowing each agent to aggregate all responses from its predecessors to form $C^{i,j}$ in Equation~\ref{eqn:single-agent}. We define the round of conversation as $m$. In each conversation round, all agents output an action and messages once along the DAG, and each agent receives outputs from itself at last round besides from predecessors, i.e., we keep a contextual memory of the size equal to 1. The proper size of contextual memories enhances the diversity of decision making and avoids introducing too long contexts during multi-agent communications. According to Equation~\ref{eqn:single-agent}, at the time step $t$ to interact with the environment, the system produce an \textbf{action matrix} $\mA_t = (a_t^{i, j}), i=1, ..., n; j=1, ... , m$. Then, majority voting is used to decide the final action 
\begin{equation}
    a_t =f_{\mathrm{agg}} (\mA_t) = \mathrm{argmax}_{a} \sum_{i=1}^n \sum_{j=1}^m \1_{a_t^{i,j} = a}, 
\end{equation}
where $\1_\mathrm{condition}$ is the indicator function. 
The agents are all required to output an intermediate decision and collaborate towards a common objective, which allows them to function with the same base model for better efficiency, and operate heterogeneously due to different conversation messages. 


\subsection{Language Multi-Agent RL}

As shown in Figure~\ref{fig:model}, the essential stage of \emph{CollabUIAgents} is MARL, after agentic fine-tuning the base model into a \textit{base UIAgent} (Appendix~\ref{app:aft}). We introduce a novel multi-agent \textbf{Credit Re-Assignment} (CR) strategy for better MARL. This approach utilizes the \textit{critic agent} to provide fine-grained rewards at both the agent and conversation round levels, enabling agents to learn effectively from general knowledge of LLMs rather than the sparse environment-specfic reward, and improve collaboration by preference optimization with synthesized data from the \textit{adversarial agent}. We also introduce an \textbf{Edge Update} trick to better accommodate trained LLMs in the multi-agent system. 

\noindent
\textbf{Credit Re-Assignment} \quad
Traditional MARL systems use value decomposition for credit assignment~\citep{ma2022VDPPO}, and recent work also uses multiple value estimators to alleviate reward sparcity in interactive environments~\citep{bai2024digirl}. However, training critic models for each agent based on outcome rewards is computationally expensive and may hinder generalization.
Instead, by using an LLM-based critic agent, represented by $\pi_{\mathrm{critic}}$, we provide process rewards in a finer granularity for each agent at each conversation round. 
The \textbf{action matrix} $\mA_t$ is assessed by the critic agent based on observed environment state $o_t$, interaction history $H_{t-1}$, and the task query $q$, generating a \textbf{reward matrix} $ \mR_t = (r_t^{i, j})$, $i=1, ..., n, j=1, ... , m $, 
where $r_t^{i,j} \in \{0, 1\}$ denotes the intermediate reward for $a_t^{i, j}$:
\begin{equation}
  \mR_t = \lceil\pi_{\mathrm{critic}}\left(o_t, H_{t-1}, \mA_t, q\right) -0.5 \rceil, 
\end{equation}
where $\pi_{\mathrm{critic}}(\cdot, \cdot, \cdot, \cdot) \in [0, 1]^{n \times m}$.
To overcome the potential incorrect CR results, we introduce an adversarial agent $\pi_{\mathrm{adv}}$ to synthesize preference data for policy optimization substituting value-based learning. Specially, $\pi_{\mathrm{adv}}$ generates low-quality responses $a_t^{i, j, -}$ paired with $a_t^{i, j}$:
\begin{equation}
    a_t^{i, j, -} = \pi_{\mathrm{adv}}\left(o_t, H_{t-1}, C_t^{i,j}, q\right), \text{if } r_t^{i, j} = 1. 
\end{equation}
\looseness=-1 Here is the rationale of the design: (1) The critic agent provides a more detailed reward signal for each agent without training multiple value estimators; (2) The critic agent's assessment is based on general pretrained LLMs, which by practice could expand the data scale by restoring intermediate decisions from failed trajectories according to the training environment, and thus may enhance performance; (3) Although errors in CR are inevitable, the synthesized preference data can still provide agents with meaningful preference signals. For example, even if $a_t^{i, j}$ is not the optimal action, it is still better than $a_t^{i, j, -}$. 
Indeed, environment-specific actions beyond general knowledge of the LLM might be misjudged. We argue that the situation is similar when a value estimator is poorly learned, where the policy might be optimized to a wrong direction, but our approach could keep rolling out new trajectories based on these actions without unlearning them. 
The qualitative study shown in Appendix~\ref{app:case-study} demonstrates CR results could be valid without the outcome reward.  

\noindent
\textbf{MARL with Edge Updates} \quad
Different from classic MARL settings, agents in \CollabUIAgents could communicate and the message network should also be rolled out in the optimization. To alleviate the overhead of learning the optimal network from enormous combinations of edges, we introduce an \textit{edge update} trick, that randomly update edges to form a DAG message network, independently from model parameter updates. Through this process, we encourage agents to learn the awareness of multi-agent collaboration and adapt to diverse message networks rather than being rigid in locally optimal DAG pattern. As shown in Figure~\ref{fig:model}, the edge update is functioned before rolling out actions from the policy system. 
The overall learning objective for each agent $\pi_i$ is formulated as:
\begin{equation}
  \vspace{-0.5em}
  \begin{aligned}
  \mathcal{L}_{\text{MARL}}(\pi_i) = - \mathbb{E}_{\mathcal{E}_\mathcal{G}' \sim K_{|\mathcal{G}|}} \mathbb{E}_{(s_t, a_t^{i,j}, \hat{H}_t^i) \sim \mathcal{P}(\mathcal{G}, \mathcal{E}_\mathcal{G}')}\\
  \sum_{t=0}^{T_{\mathrm{max}}} \sum_{j=1}^{m} \left[ \log \sigma \left( \beta \left( \frac{\log \pi_{\theta_i}(a_t^{i,j} | o_t, \hat{H}_t^i)} {\log \pi_{\mathrm{ref}_i}(a_t^{i,j} | o_t, \hat{H}_t^i)} \right. \right. \right. \\
  \left. \left. \left. - \frac{\log \pi_{\theta_i}(a_t^{i,j,-} | o_t, \hat{H}_t^i)}{\log \pi_{\mathrm{ref}_i}(a_t^{i,j,-} | o_t, \hat{H}_t^i)} \right) \right) \right] \cdot \1_{r_t^{i, j} = 1},
  \end{aligned}
\end{equation}

where $\hat{H}_t^i = \{H_{t-1}, C_t^{i, j}, q\}$, $\theta_i$ are the updating parameters of agent $\pi_i$ and $\mathrm{ref}_i$ is the original model used as reference policy, $K_{|\mathcal{G}|}$ is a fully connected graph of $|\mathcal{G}|$ nodes, $\mathcal{E}_\mathcal{G}'$ represents a DAG subgraph sampled from $K_{|\mathcal{G}|}$, and 
$\mathcal{P}(\mathcal{G}, \mathcal{E}_\mathcal{G}')$ is the preference dataset sampled with agents in the message network $\mathcal{E}_\mathcal{G}'$. 
This objective encourages the policy $\pi_{i}$ to assign higher probabilities to preferred actions $a_t^{i,j}$ compared to adversarial actions $a_t^{i,j,-}$. The policy system is updated online and off-policy, similar to previous work~\citep{bai2024digirl}.

\subsection{Cross-Environment Adaptation}

One of the key strengths of the \emph{CollabUIAgents} framework is its ability to generalize across different interactive environments, such as from mobile operations to web browsing environments. The framework supports two approaches for adaptation.

\noindent
\textbf{Direct Transfer} \quad
In scenarios where the new environment shares similarities with the training environment, agents can be directly deployed without additional training. For example, agents trained in mobile UI environments can directly apply their knowledge to web environments, leveraging the knowledge common interaction patterns and UI elements. The multi-agent setup may also decrease error rates through collaborations as expected. 

\noindent
\textbf{Continual MARL} \quad
When the new environment presents significant differences or the higher success rates are demanded, agents can undergo further training using the MARL with the CR strategy on the new environment. This continual learning approach allows agents to refine their policies without stashing the knowledge of previous environments. 


\begin{table*}[t]
    \small\centering
    \setlength{\tabcolsep}{3pt}
    \begin{tabular}{l|ccccccc}
        \toprule
        \textbf{Method} & \textbf{Base model} & \textbf{\#Params} & \textbf{\#Agents} & \textbf{Input} & \textbf{SR\textsubscript{AndroidWorld}} & \textbf{SR\textsubscript{MMiniWoB++}} & \textbf{$\boldsymbol{\Delta}$\textsubscript{Generalization}} \\ \midrule\noalign{\vskip -3pt}
        \multicolumn{8}{c}{\cellcolor[HTML]{EFEFEF} \small\textit{Agents based on Closed-Source LLMs}}    \\
        %Human & Screen & N/A & 80.0 & 100.0 \\ \hline
        M3A &GPT-4 & N/A & 1 & Text & \textbf{30.6} & 59.7  &  - \\ 
        M3A & GPT-4 & N/A & 1 & Text \& Image & 25.4 & \textbf{67.7}  & - \\ 
        SeeAct & GPT-4 & N/A & 1 & Text \& Image & 15.5 & 66.1 &  - \\ 
        M3A & Gemini 1.5 Pro&  N/A  & 1 & Text & 19.4 & 57.4  &   -\\ 
        M3A & Gemini 1.5 Pro & N/A  & 1 & Text \& Image & 22.8 & 40.3 &  -\\ 
        %\hline
        %\hline
        \midrule\noalign{\vskip -3pt}
        \multicolumn{8}{c}{\cellcolor[HTML]{EFEFEF} \small\textit{Agents based on Open-Source LLMs}}    \\
        Base Model & Qwen2  & 7B & 1 & Text & \leavevmode\hphantom{0}6.2 & 12.9 & - \\ 
        Base Model & Qwen2 VL & 2B & 1 & Text \& Image & \leavevmode\hphantom{0}0.0 & 10.0  &  - \\
        InfiGUIAgent & Qwen2 VL & 2B & 1 & Text \& Image & \leavevmode\hphantom{0}9.1 & 15.6  &  \leavevmode\hphantom{0}5.6\\
        Base Model & Qwen2.5 VL & 3B & 1 & Text \& Image & 10.1 & 18.7 & - \\
        DigiRL & Qwen2.5 VL & 3B & 1 & Text \& Image & 22.3 & 35.2 &  16.5 \\
        DigiRL & Qwen2.5 VL & 3B & 4 & Text \& Image & 20.1 & 38.7 &  20.0  \\
        \midrule\noalign{\vskip -3pt}
        \multicolumn{8}{c}{\cellcolor[HTML]{EFEFEF} \small\textbf{Our Methods}}    \\
        Base UIAgent & Qwen2 & 7B & 1 & Text & 18.9 & 48.4 & 35.5 \\
        Base UIAgent & Qwen2 & 7B & 4 & Text & 21.4 & 53.2 & 40.3 \\ 
%        MA-SFT & Qwen2 & 7B & 4 & Text  & 23.2 & 54.8 \\
%        MA-DPO & Qwen2 & 7B & 4 & Text  & 25.0 & 56.4 \\ 
        \CollabUIAgentsMobile  & Qwen2 & 7B & 4 & Text  & \textbf{29.3} & \textbf{61.2} & \textbf{48.3} \\
        \bottomrule
    \end{tabular}
    \caption{Experimental results on mobile operation environments. Success rates (SR) in AndoridWorld and MobileMiniWoB++ (MMiniWoB++) are listed. \textbf{$\boldsymbol{\Delta}$\textsubscript{Generalization}} indicates the performance gap between the base model and agent learning methods based on the model in MobileMiniWoB++.
    }
    \label{table:android_envs}
    \vspace{-1em} 
\end{table*}

\section{Experiment}

\subsection{Experimental Settings}

\noindent
\textbf{Environments} \quad
We conduct experiments in both mobile operation and web browsing environments. For the mobile environments, we use AndroidWorld~\citep{rawles2025androidworld} for training and MobileMiniWoB++~\citep{rawles2025androidworld} for testing: (1) \textbf{AndroidWorld} has 116 programmatic tasks across 20 real-world apps, such as Chrome, Markor, and Pro Expense. (2) \textbf{MobileMiniWoB++} is derived from MiniWoB++~\citep{shi2017world}, which is a web-based benchmark. MobileMiniWoB++ shares the same observation space as AndroidWorld and supports 92 tasks from MiniWoB++. We use the success rate (SR) as an evaluation metric.
For the web environments, we leverage Mind2Web~\citep{deng2024mind2web} for training and AutoWebBench~\citep{lai2024autowebglm} for testing: (1) \textbf{Mind2Web} features over 2,000 open-ended tasks sourced from 137 websites in 31 different domains. (2) \textbf{AutoWebBench} is a bilingual benchmark featuring approximately 10,000 traces, from mainstream Chinese and English websites, providing a diverse dataset for web browsing. We use the step-success rate (SSR) as the evaluation metric. For agent learning methods on open-source LLMs, we use the performance gap between the base model and the trained model in unseen environments, \textbf{$\boldsymbol{\Delta}$\textsubscript{Generalization}}, to indicate generalizability.

\noindent
\textbf{Evaluated Methods} \quad
We compare our framework against the following methods under their original settings: (1) \textbf{M3A}~\citep{rawles2024androidinthewild} is a prompt-based multimodal agent, which combines ReAct-~\citep{yao2023react} and Reflexion-style~\citep{shinn2023reflexion} prompting to interpret user instructions and screen content, then update its decisions. (2) \textbf{SeeAct}~\citep{pmlr-v235-zheng24e} is a prompt-based navigation agent originally designed for GPT-4V to perform actions with visual input and textual choices. (3) \textbf{InfiGUIAgent}~\citep{liu2025infiguiagent} fine-tunes a generalist mobile operator model with multimodal input. (4) \textbf{DigiRL}~\citep{bai2024digirl} is an off-policy RL algorithm trained on the same dataset as \CollabUIAgents, based on Qwen2.5 VL 3B~\citep{Qwen2VL}. (5) \textbf{SeeClick}~\citep{cheng2024seeclick} is a fine-tuned visual GUI agent that automates tasks relying on screenshots and employs GUI grounding. 
We leverage Qwen2 7B as our base model and evaluate the following systems derived from the model: (1) \textbf{Base Model} directly calls the instruction version of the model to interact with the environment without fine-tuning. (2) \textbf{Base UIAgent} is the base model that has undergone agentic fine-tuning in the AndroidWorld environment. (3) \textbf{\CollabUIAgentsMobile} is our framework trained on AndroidWorld with $n=4, m=3$. (4) \textbf{\CollabUIAgentsWeb} builds upon \CollabUIAgentsMobile with continue MARL on the training set of Mind2Web environment, which is autonomously collected with the pipeline in Appendix~\ref{app:aft}. Details are in Appendix~\ref{app:exp_details}. 

\begin{table*}[t]
    \small\centering
    \setlength{\tabcolsep}{3pt}
    \begin{tabular}{l|ccccccc}
        \toprule
        \textbf{Method} & \textbf{Base model} & \textbf{\#Params} & \textbf{\#Agents} & \textbf{Input} & \textbf{SSR\textsubscript{Mind2Web}} & \textbf{SSR\textsubscript{AutoWebBench}} & \textbf{$\boldsymbol{\Delta}$\textsubscript{Generalization}} \\ \midrule\noalign{\vskip -3pt}
        \multicolumn{8}{c}{\cellcolor[HTML]{EFEFEF} \small\textit{Agents based on Closed-Source LLMs}}    \\
        %Human & Screen & N/A & 80.0 & 100.0 \\ \hline
        Base Model & GPT-3.5-Turbo & N/A & 1 & Text & 17.4 & 10.7  &  - \\ 
        Base Model & GPT-4 & N/A & 1 & Text  & \textbf{30.9} & \textbf{37.8}  & - \\ 
        Base Model & Claude2 & N/A & 1 & Text & - & 10.5 &  - \\ 
        \midrule\noalign{\vskip -3pt}
        \multicolumn{8}{c}{\cellcolor[HTML]{EFEFEF} \small\textit{Agents based on Open-Source LLMs}}    \\
        Base Model & LLaMA2 & 7B & 1 & Text  &  & 2.9  &  - \\
        Base Model & LLaMA2 & 70B & 1 & Text & - & 10.6  &  - \\
        Base Model & Qwen2  & 7B & 1 & Text & \leavevmode\hphantom{0}7.4 & 8.5 & - \\ 
        SFT  & Qwen VL & 9.6B & 1 & Text \& Image & 10.1 & - & - \\
        SFT  & SeeClick & 9.6B & 1 & Text \& Image & 20.9 & - & - \\
        \midrule\noalign{\vskip -3pt}
        \multicolumn{8}{c}{\cellcolor[HTML]{EFEFEF} \small\textbf{Our Methods}}    \\
        Base UIAgent & Qwen2 & 7B & 1 & Text & 11.9 & 12.8 & \leavevmode\hphantom{0}4.3  \\
        Base UIAgent & Qwen2 & 7B & 4 & Text & 13.2 & 14.0 & \leavevmode\hphantom{0}5.5 \\ 
%        MA-SFT & Qwen2 & 7B & 4 & Text  & 23.2 & 54.8 \\
%        MA-DPO & Qwen2 & 7B & 4 & Text  & 25.0 & 56.4 \\ 
        \CollabUIAgentsMobile  & Qwen2 & 7B & 4 & Text  & 16.2 & 17.7 & \leavevmode\hphantom{0}9.2 \\
        \CollabUIAgentsWeb  & Qwen2 & 7B & 4 & Text  & \textbf{30.7} & \textbf{34.7} & \textbf{26.2} \\
        \bottomrule
    \end{tabular}
    \caption{Experimental results on web browsing environments. Average step success rates (SSR) in Mind2Web and AutoWebBench are reported. ``SFT'' denotes the base model is supervised fine-tuned in Mind2Web. \textbf{$\boldsymbol{\Delta}$\textsubscript{Generalization}} indicates the gap between the base model and agent learning methods based on the model in AutoWebBench.
    }
    \label{table:web_envs}
    % \vspace{-1em} 
\end{table*}


\vspace{-0.5em}
\subsection{Main Results}
% \vspace{-0.5em}

\noindent\textbf{Effectiveness in Mobile Environments} \quad
In this section, we explore the effectiveness of our proposed method in trained and unseen mobile operation environments.
Experimental results in mobile environments are shown in Table~\ref{table:android_envs}. The performance of proprietary models face instability with input format, e.g., text-only or multi-modal input. 
With appropriate agentic fine-tuning, open-source models like Qwen2 7B could gain significant performance improvement, and even higher within multi-agent systems (``Base UIAgent''). 
\CollabUIAgentsMobile achieves the best results among systems based on open-source LLMs. Remarkably, it outperforms Gemini 1.5 Pro in both environments and achieves performance comparable to or better than GPT-4, due to its instability. These outcomes demonstrate the effectiveness of our framework and provide evidence for the validity of the CR strategy. 
On the other hand, InfiGUIAgent uses a large amount of training data but falls behind other methods; DigiRL shows good performance in the training environment, but falls short in generalization compared to \CollabUIAgentsMobile, which improves greatly on unseen tasks.
\begin{tcolorbox}[colback=gray!10, coltext=black, sharp corners=all]
    \textbf{Takeaway 1:} The critic agent could generate valid process rewards to guide the policy multi-agent system learning during CR. 
\end{tcolorbox}

\noindent\textbf{Generalization from Mobile to Web Environments} \quad
In this section, we examine the cross-environment generalization capabilities of our proposed method. Results for web environments are presented in Table~\ref{table:web_envs}.
Directly transferring \CollabUIAgentsMobile obtained from the AndroidWorld environment yields substantial performance improvement over Base UIAgent or vanilla Qwen2; however, the absolute gains remain modest and slightly lags behind fine-tuned SeeClick in the training environment. 
\CollabUIAgentsMobile with continue MARL on data from Mind2Web, collected with the pipeline depicted in Appendix~\ref{app:aft}, could significantly improve the generalization on web browsing environments. \CollabUIAgentsWeb achieves results comparable to GPT-4, without training on large scaled web data. It is also noteworthy that we do not require human-annotated data for the Mind2Web environment, which is a significant advantage in transferring the agent system to new environments. Also, generalization performance (\textbf{$\boldsymbol{\Delta}$\textsubscript{Generalization}}) in the unseen AutoWebBench environment indicates that our method generalizes well in diverse web browsing tasks.
\begin{tcolorbox}[colback=gray!10, coltext=black, sharp corners=all]
    \textbf{Takeaway 2:} Continue MARL significantly improves the language multi-agent system for cross-environment generalization. 
\end{tcolorbox}

\subsection{Ablation Study}

The results of the ablation study are presented in Table~\ref{tab:abl_web}, including replacing preference optimization with rejective fine-tuning, removing the CR, and removing edge updates in MARL. %We conduct automated data synthesis, model training and evaluation in the AndroidWorld environment. Additionally, we directly apply the resulting system to the MobileMiniWoB++ environment for evaluation.
%\noindent\textbf{Stage 2: Multi-agent Learning} \quad
%This stage focuses on training multiple agents to collaborate and achieve superior results. 
The empirical findings highlight the following key insights:

(1) Further training of the Base UIAgent with trajectory data using either rejective SFT (``\CollabUIAgentsMobile w/ PO $\rightarrow$ RFT'') or DPO on whole trajectories (``\CollabUIAgentsMobile w/o CR'') improves performance, with DPO showing superior results. The primary distinction between these methods is that SFT can only learn from correct actions, while DPO can learn from both correct and incorrect actions. But they all faces reward scarcity problems, resulting in modest improvement.  
(2) \CollabUIAgentsMobile introduces credit re-assignment, providing more granular feedback that facilitates exploration of the large action space at each step. The synthesized preference data also helps generalization through preference optimization. This boosts both performance and generalizability, yielding the best overall results.
\begin{tcolorbox}[colback=gray!10, coltext=black, sharp corners=all]
    \textbf{Takeaway 3:} Preference optimization with synthesized data enhances performance and generalization with CR-generated rewards.   
\end{tcolorbox}

(3) Combining multiple agents based on a vanilla base model using random edges leads to modest improvements, and the similar trend exists for DigiRL in Table~\ref{table:android_envs}, underscoring the importance of proper agentic fine-tuning, and MARL with online multi-agent trajectories. 
(4) A comparison between systems with and without edge updates (``\CollabUIAgentsMobile'' vs. ``w/o edge update'') demonstrates that the edge update trick contributes to further accommodating language agents in complex multi-agent systems with communications. 
\begin{tcolorbox}[colback=gray!10, coltext=black, sharp corners=all]
    \textbf{Takeaway 4:} Edge updates during MARL help accommodate language agents in the multi-agent system.  
\end{tcolorbox}

(5) After cross-environment reinforcement learning on the web, \CollabUIAgentsWeb exhibits impressive autonomous adaptability in the new environment, with only minor performance fluctuations in the original mobile environment, thereby validating the stability of our method. %This indicates that ensuring the general adaptability of agents across multiple environments remains an important research direction.

\begin{table}[t]
    
    %\vspace{5pt}
    \small\centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{l|cccc}
        \toprule
        \textbf{Method}  & \textbf{SR\textsubscript{AndroidWorld}} & \textbf{SR\textsubscript{MMiniWoB++}} \\ \midrule\noalign{\vskip -3pt}
        \multicolumn{3}{c}{\cellcolor[HTML]{EFEFEF} \small\textit{Single-Agent Systems}}    \\
        Qwen2 & \leavevmode\hphantom{0}6.2 & 12.9 \\ 
        Base UIAgent  & 18.9 & 48.4 \\ 
        % + Basic knowledge SFT & 7B & 1 & 12.1 & 22.5 \\
        % + Instruction SFT & 7B & 1 & 15.1 & 35.8 \\ 
        % + Process DPO & 7B & 1 & \textbf{18.9} & \textbf{48.4} \\
        \midrule\noalign{\vskip -3pt}
        \multicolumn{3}{c}{\cellcolor[HTML]{EFEFEF} \small\textit{Multi-Agent Systems ($n=4$)}}    \\
        Qwen2  & \leavevmode\hphantom{0}8.6 & 16.1 \\ 
        Base UIAgent  & 21.4 & 53.2 \\ 
        \CollabUIAgentsMobile & \textbf{29.3} & \textbf{61.2} \\
        \quad w/ PO $\rightarrow$ RFT   & 23.2 & 54.8  \\
        \quad w/o CR   & 25.0 & 56.4 \\
        \quad w/o Edge Update & 27.6 & 58.1 \\ 
        \CollabUIAgentsWeb  & 26.7 & 58.1 \\
        \bottomrule
    \end{tabular}}
    \caption{Ablation study. Success Rates (SR) in the AndroidWorld and MobileMiniWoB++ (MMiniWoB++) environments are reported. All methods are based on Qwen2 7B. ``PO'' denotes preference optimization, and ``PO $\rightarrow$ RFT'' means performing rejective fine-tuning based on CR rewards, i.e., filtering unrewarded data.}\label{tab:abl_web}
    % \vspace{-0.5em}
\end{table}

\section{Related Work}
\label{gen_inst}
\noindent\textbf{Agents on Interactive Environments} \quad Before the advent of LLMs, agents relied on traditional RL to perform interactions such as clicking and typing~\citep{liu2018learning,humphreys2022data}. However, recent advancements have shifted towards leveraging foundation models with in-context learning or fine-tuning across various interfaces, including mobile~\citep{wang2023enabling,hong2024cogagent}, web~\citep{lai2024autowebglm, deng-etal-2024-multi}, and computer using environments~\citep{xu2024crab, wu2024copilot}. Recently, there are emerging methods designing process rewards~\citep{he2024webvoyager, pan2024autonomous} or language RL~\citep{bai2024digirl} for better performing single agents. 

\noindent\textbf{Interactive Environments for Agents} \quad To effectively evaluate language agents, it is essential to create environments that replicate real-world conditions and deliver accurate rewards~\citep{rawles2024androidinthewild,deng2024mind2web}. MiniWoB++~\citep{shi2017world} is a lightweight framework that features small, synthetic HTML pages with parameterized tasks. WebArena~\citep{zhou2023webarena} and its visual counterpart, VisualWebArena~\citep{koh2024visualwebarena}, simulate websites spanning up to distinct domains, while WorkArena~\citep{drouin2024workarena} focuses on enterprise software. For more specialized environments, WebShop~\citep{yao2022webshop} simulates an e-commerce platform for online shopping. For computer using environment, OSWorld~\citep{xie2024osworld} provides both a user interface and programmatically generated rewards across different apps.

\noindent\textbf{Prompt-Based Multi-agent Learning} \quad Collaboration among multiple LLM agents has shown effective for various tasks~\citep{hong2024metagpt,wu2024autogen,qian-etal-2024-chatdev,wang2025mobileagenteselfevolvingmobileassistant}. However, employing a static architecture without team optimization may restrict the performance and generalization. \citet{chen2024agentverse} selects a fixed number of agents from a set of manual prompt candidates via an additional LLM during each round of discussion. %\citet{zhang2023cumulative} adapt a dynamic directed acyclic graph structure with three LLM roles—the proposer, verifier, and reporter—to iteratively propose, validate, and compile reasoning steps into a comprehensive solution. 
\citet{zhuge2024gptswarm} unify language agent systems by describing them as optimizable computational graphs and develop optimization methods for nodes and edges, enabling automatic improvements of agent prompts and inter-agent orchestration. \citet{liu2024a} employ a layered network to formulate the process of LLM-agent collaboration for arbitrary tasks and introduce an unsupervised algorithm to optimize the team of agents by the individual contributions of agent.   


\section{Conclusion}

In this paper, we introduce \CollabUIAgents, a novel multi-agent reinforcement learning framework aimed at addressing the challenge of balancing strong performance and generalization in interactive environments. The framework employs a credit re-assignment (CR) strategy that utilizes world knowledge embedded in LLMs to assign process rewards, and optimize policies with synthesized preference data. Through extensive experimentation, the proposed framework not only surpasses existing methods in terms of performance metrics but also exhibits exceptional generalization capabilities. Notably, it achieves results that are comparable to, and in some cases even exceed, those of closed-source models when deployed in previously unseen environments. Overall, \CollabUIAgents presents a promising solution to the limitations of current agent learning methods by offering a more flexible, data-efficient, and generalizable approach for real-world applications.

\section*{Limitations}

Current methodology of \CollabUIAgents framework achieves significant improvement in both environment-specific performance and cross-environment generalization. Howevrer, it still has several limitations. First, the proposed multi-agent system is based on a fixed number of agents, which may not be optimal for all tasks. Second, the rolling-out phase in the MARL process relies on majority voting, which may limit the diversity of the rolled-out actions and could be further optimized. Third, Third, due to computational resource limitations, we have not explored large-scale MARL training, such as with more agents, larger query sets, or simply more training steps, compared to large-scale RL practices~\citep{deepseekai2025deepseekr1incentivizingreasoningcapability}. The potential of reinforcement learning on language agents deserves further efforts.  


% \section*{Acknowledgments}

% This document has been adapted
% by Steven Bethard, Ryan Cotterell and Rui Yan
% from the instructions for earlier ACL and NAACL proceedings, including those for
% ACL 2019 by Douwe Kiela and Ivan Vuli\'{c},
% NAACL 2019 by Stephanie Lukin and Alla Roskovskaya,
% ACL 2018 by Shay Cohen, Kevin Gimpel, and Wei Lu,
% NAACL 2018 by Margaret Mitchell and Stephanie Lukin,
% Bib\TeX{} suggestions for (NA)ACL 2017/2018 from Jason Eisner,
% ACL 2017 by Dan Gildea and Min-Yen Kan,
% NAACL 2017 by Margaret Mitchell,
% ACL 2012 by Maggie Li and Michael White,
% ACL 2010 by Jing-Shin Chang and Philipp Koehn,
% ACL 2008 by Johanna D. Moore, Simone Teufel, James Allan, and Sadaoki Furui,
% ACL 2005 by Hwee Tou Ng and Kemal Oflazer,
% ACL 2002 by Eugene Charniak and Dekang Lin,
% and earlier ACL and EACL formats written by several people, including
% John Chen, Henry S. Thompson and Donald Walker.
% Additional elements were taken from the formatting instructions of the \emph{International Joint Conference on Artificial Intelligence} and the \emph{Conference on Computer Vision and Pattern Recognition}.

% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
\bibliography{custom}
% Custom bibliography entries only
% \bibliography{custom}

\newpage

\appendix

\section{Case Studies on Mobile Operation Environments}\label{app:case-study}

Figure \ref{fig:steps} shows an example of task execution steps in the AndroidWorld environment during the rolling out phase, where agents in the multi-agent system collectively accomplish the task within the shortest path. We also demonstrate that the reward from the CR process is correctly identified for each action (only a part of rolled out actions are shown). 

\section{Agentic Fine-Tuning in \CollabUIAgents}\label{app:aft}

\begin{figure}[ht]
    
  \centering
  %\vspace{-1em}
  \includegraphics[width=\linewidth]{data_collection2.pdf} 
  \caption{Our multi-agent autonomous data synthesis pipeline. Given a task, the pipeline can autonomously collect data from each step covering basic environmental knowledge, simple instruction knowledge, and process preference knowledge in interactive environments.}
  \label{fig:data}
  %\vspace{-3em}
\end{figure}

\begin{figure*}
    \centering
    \begin{minipage}{\linewidth}
        \centering
        \includegraphics[width=\linewidth]{screen1.pdf}
    \end{minipage}
    
    \vspace{-5pt} 

    \begin{minipage}{\linewidth}
        \centering
        \includegraphics[width=\linewidth]{screen2_1.pdf}
    \end{minipage}
    
    \vspace{-5pt}
    
    \begin{minipage}{\linewidth}
        \centering
        \includegraphics[width=\linewidth]{screen3.pdf}
    \end{minipage}
    
    \caption{An example of task execution steps. ``action\_type'' represents the action taken, and ``index'' represents the index of the UI element. The positions of the relevant elements on the UI interface are marked in red.}
    \label{fig:steps}
\end{figure*}

\subsection{Methodology}

The agentic fine-tuning process of the \CollabUIAgents framework focuses on adapting base models to new environments through curriculum-based single-agent training~\citep{curriculum}. The training data is synthesized automatically with a multi-agent data synthesis pipeline and consists of progressively complex instruction sets in three levels, designed to help agents build a strong foundation of environmental knowledge. The UI agent generates responses to synthesize queries faithfully, the adversarial agent generates negative samples, and the critic agent grades process rewards.

\noindent
\textbf{Curriculum Structure} \quad
The training data is divided into three categories, as collected in Figure~\ref{fig:data}:
% \begin{itemize}

(1) \textbf{Basic Environmental Knowledge}: This data segment includes identifying UI elements and understanding their properties. 
We categorize basic knowledge into two types: \textbf{UI Understanding} (coarse-grained): This refers to a broad understanding of the layout and information contained in the UI, such as identifying the purpose of the interface. \textbf{UI Element Recognition} (fine-grained): Since UI typically contains a large number of densely packed interface, the agent needs to be able to distinguish between different types of elements, such as buttons, input fields, and drop-down menus, and understand the associated actions. We develop a series of queries accordingly in Appendix~\ref{sec:question}, and randomly select UI elements and the layout to assemble queries for the UI Agent.

(2) \textbf{Simple Instruction Knowledge}: The agents are tasked with performing basic interactions, such as clicking or typing, in response to simple instructions. 
Specifically, given the complete action space, we prompt the UI agent to generate possible instructions related to a random UI element, and their corresponding responses. For example, in Figure~\ref{fig:data}, the UIAgent was prompted to generate an instruction for element 9 (``\textit{selecting the M4a format}") and then generates the corresponding response to interact with it. By learning this type of knowledge, the agent lays the foundation for completing a complex sequential decision-making process. 

(3) \textbf{Process Preference Knowledge}: Real-world interactive tasks is quite difficult, and even the most advanced large language model, GPT-4, shows a low task completion rate (30\%) in the mobile environment AndroidWorld~\citep{rawles2025androidworld}. Training a model solely on scarce successful trajectories still inevitably results in errors. Therefore, as illustrated below Figure~\ref{fig:data}, we introduce the adversarial agent against the UI agent, and the critic agent to score all actions, obtaining process preference data with step-level rewards. 
By learning from process preference data, the agent can better distinguish between correct and incorrect actions during the process, ultimately improving task completion rates. The distribution of the collected data can be found in Appendix~\ref{app:distr_data}.
% \end{itemize}

The base model is first trained using supervised fine-tuning (SFT) on the basic environmental knowledge and the simple instruction knowledge, progressively. The learning objective is:
\begin{equation}
    \mathcal{L}_{\text{SFT}} = - \mathbb{E}_{(s,a) \sim \mathcal{D}} \left[ \log \pi_{\theta}(a|s) \right],
\end{equation}
where $\mathcal{D}$ represents the dataset of state-action pairs. Following SFT, the base model are further optimized using direct preference optimization (DPO) on the process preference knowledge:
% \begin{equation}
% \small
%     \mathcal{L}_{\text{DPO}} = - \mathbb{E}_{(s,a_{+},a_{-}) \sim \mathcal{P}} \left[ \log \sigma \left( \beta \mathrm{log} \frac{\pi_{\theta}(a_{-}|s)}{\pi_{\mathrm{ref}}(a_{-}|s)}  - \beta \mathrm{log} \frac{\pi_{\theta}(a_{+}|s)}{\pi_{\mathrm{ref}}(a_{+}|s)} \right) \right],
% \end{equation}
\begin{equation}
\begin{aligned}
\mathcal{L}_{\text{DPO}} = &- \mathbb{E}_{(s,a^{+},a^{-}) \sim \mathcal{P}} \left[ \log \sigma \left( \beta \log \frac{\pi_{\theta}(a^{-}|s)}{\pi_{\mathrm{ref}}(a^{-}|s)} \right. \right. \\
&\left. \left. - \beta \log \frac{\pi_{\theta}(a^{+}|s)}{\pi_{\mathrm{ref}}(a^{+}|s)} \right) \right],
\end{aligned}
\end{equation}
where $\mathcal{P}$ is the preference-labeled dataset, $a^{+}, a^{-}$ denote positive and adversarial actions, $\sigma$ is the sigmoid function, $\beta$ is the hyper-parameter, and $\pi_\theta, \pi_\mathrm{ref}$ are the base model and reference model. 


\subsection{Ablation Study of Agentic Fine-tuning}
In this stage, we develop an automated data synthesis method to gather basic environmental knowledge, simple instruction knowledge, and process preference knowledge from the dynamic mobile environment, AndroidWorld. 

\begin{table}[ht]
    
    %\vspace{5pt}
    \small\centering
    \begin{tabular}{l|cc}
        \toprule
        \textbf{Method}  & \textbf{SR\textsubscript{AndroidWorld}} & \textbf{SR\textsubscript{MMiniWoB++}} \\ \midrule\noalign{\vskip -3pt}
        %\multicolumn{5}{c}{\cellcolor[HTML]{EFEFEF} %\small\textit{Stage 1}}    \\
        Qwen2 & \leavevmode\hphantom{0}6.2 & 12.9 \\ 
        + Basic knowledge SFT & 12.1 & 22.5 \\
        + Instruction SFT & 15.1 & 35.8 \\ 
        + Process DPO  & \textbf{18.9} & \textbf{48.4} \\
        % \midrule\noalign{\vskip -3pt}
        % \multicolumn{5}{c}{\cellcolor[HTML]{EFEFEF} \small\textit{Stage 2}}    \\
        % GroupAgents w/ Vanilla Qwen2 & 7B & 4 & \leavevmode\hphantom{0}8.6 & 16.1 \\ 
        % GroupAgents w/ Stage-1 Qwen2 & 7B & 4 & 21.4 & 53.2 \\ 
        % \CollabUIAgentsMobile & 7B & 4 & \textbf{29.3} & \textbf{61.2} \\
        % \quad w/ MARL $\rightarrow$ MA-SFT & 7B & 4 & 23.2 & 54.8  \\
        % \quad w/o reward decomposition & 7B & 4 & 25.0 & 56.4 \\
        % \quad w/o edge update & 7B & 4 & 27.6 & 58.1 \\ 
        % \CollabUIAgentsWeb & 7B & 4 & 26.7 & 58.1 \\
        \bottomrule
    \end{tabular}
    \caption{Ablation study of the agentic fine-tuning process on mobile operation environments. Success Rates (SR) in the AndroidWorld and MobileMiniWoB++ (MMiniWoB++) environments are reported. All methods are based on Qwen2 7B.}
    \label{tab:success_rates}
    % \vspace{-1em}
\end{table}

Based on the upper section of Table~\ref{tab:success_rates}, we derive the following conclusions: 
(1) Incorporating basic environmental knowledge data substantially improves the base model's comprehension of dynamic mobile environments, achieving a absolute performance gain of 5.9\% in AndroidWorld and 9.6\% in MobileMiniWoB++ (``+ Basic knowledge SFT''). It is noteworthy that the collected UI page information excludes app-specific details of MobileMiniWoB++, yet training with general knowledge from AndroidWorld enables the model to generalize effectively to new apps and tasks.
(2) Simple instruction knowledge data is crafted to guide the agent in interacting with the environment using actions from the specified action space. Our experiments demonstrate that incorporating instruction data further enhances the base model’s ability to complete simple tasks within UI environments (``+ Instruction SFT'').
(3) A key advantage of our proposed method is its ability to learn from incorrect actions using process preference knowledge data. Experimental results confirm that this addition significantly boosts performance (``+ Process DPO''). The improvement is more pronounced in the MobileMiniWoB++ environment, which we attribute to the simplicity of its tasks. Fewer steps are required to complete these tasks, leading to greater performance gains.


\section{Experiment Details}\label{app:exp_details}

\subsection{Action Space in the Environments}
Table~\ref{tab:mobile} and \ref{tab:web} show the action spaces of agents in mobile and web environments, respectively. 

\begin{table}[ht]
    \small\centering
    %\renewcommand\arraystretch{1.2}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{c|c}
        \toprule
        \textbf{Action} & \textbf{Description}  \\ \midrule
        CLICK & Tap once on the element\\ 
        DOUBLE\_TAP & Quickly tap the element twice \\ 
        SCROLL & Slide the screen to view more content \\ 
        SWIPE & Quick swipe across the screen \\ 
        INPUT\_TEXT & Type text into the element \\ 
        NAVIGATE\_HOME & Return to the home screen \\ 
        NAVIGATE\_BACK & Go back to the previous screen \\ 
        KEYBOARD\_ENTER & Press the enter key \\ 
        OPEN\_APP & Launch an app \\ 
        STATUS & Check task status  \\ 
        WAIT & Pause briefly \\ 
        LONG\_PRESS & Tap and hold on the element \\ 
        ANSWER & Give a response \\ 
        UNKNOWN & Undefined action \\ 
        \bottomrule
    \end{tabular}
    }
    \caption{The action space in mobile environment.}
    \label{tab:mobile}
\end{table}

\begin{table}[ht]
    \small\centering
    %\renewcommand\arraystretch{1.2}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{c|c}
        \toprule
        \textbf{Action} & \textbf{Description}  \\ \midrule
        CLICK & Click at an element \\  
        HOVER & Hover on an element \\  
        SELECT & Select option in an element \\  
        TYPE\_STRING & Type to an element \\  
        SCROLL\_PAGE & Scroll up or down of the page \\  
        GO & Go forward or backward of the page \\  
        JUMP\_TO & Jump to URL \\  
        SWITCH\_TAB & Switch to i-th tab \\  
        USER\_INPUT & Notify user to interact \\  
        FINISH & Stop with answer \\    
        \bottomrule
    \end{tabular}
    }
    \caption{The action space in web environment.}
    \label{tab:web}
\end{table}

\subsection{Detailed Results in Web Browsing Environments}

Detailed experimental results in web browsing environments are shown in Table~\ref{table:mind2web-results} and Table~\ref{table:autowebbench_results}, corresponding to the results in Table~\ref{table:web_envs}. 

\begin{table*}[t]
\centering
\small 
\resizebox{\linewidth}{!}{
\small
\setlength{\tabcolsep}{3pt}
\begin{tabular}{l|cccccc|c}
\toprule
\textbf{System} & \textbf{\#Params} & \textbf{\#Agents} & \textbf{Input} & \textbf{Cross-Task} & \textbf{Cross-Website} & \textbf{Cross-Domain} & \textbf{Avg.} \\
%\midrule\noalign{\vskip -3pt} \multicolumn{8}{c}{\cellcolor[HTML]{EFEFEF} \small\textit{Closed-Source LLMs and Specialized Fine-Tuning}}    \\
\midrule\noalign{\vskip -3pt} \multicolumn{8}{c}{\cellcolor[HTML]{EFEFEF} \small\textit{Agents based on Closed-Source LLMs}}    \\
GPT-3.5-Turbo & N/A & 1 & Text & 17.4 & 16.2 & 18.6 & 17.4 \\  
GPT-4  & N/A & 1 & Text & \textbf{36.2} & \textbf{30.1} & \textbf{26.4} & \textbf{30.9} \\  
\midrule\noalign{\vskip -3pt} \multicolumn{8}{c}{\cellcolor[HTML]{EFEFEF} \small\textit{Agents based on Open-Source LLMs}}    \\
Qwen-VL* & 9.6B & 1 & Text \& Image & 12.6 & 10.1 & \leavevmode\hphantom{0}8.0 & 10.2 \\  
SeeClick* & 9.6B & 1 & Text \& Image & 23.7 & 18.8 & 20.2 & 20.9 \\  
Qwen2  & 7B & 1 & Text & \leavevmode\hphantom{0}8.6 & \leavevmode\hphantom{0}6.3 & \leavevmode\hphantom{0}7.5 & \leavevmode\hphantom{0}7.4 \\
\midrule\noalign{\vskip -3pt} \multicolumn{8}{c}{\cellcolor[HTML]{EFEFEF} \small\textbf{Our Methods}}    \\
Base UIAgent  & 7B & 1 & Text & 13.4 & 10.6 & 11.8 & 11.9 \\
Base UIAgent  & 7B & 4 & Text & 15.7 & 11.2 & 12.9 & 13.2 \\
\CollabUIAgentsMobile  & 7B & 4 & Text & 19.2 & 13.8 & 15.5 & 16.2 \\
\CollabUIAgentsWeb*  & 7B & 4 & Text & \textbf{34.5} & \textbf{32.7} & \textbf{25.1} & \textbf{30.7} \\

\bottomrule
\end{tabular}
}
\caption{Step Success Rates (SSR) in the Mind2Web environment. * indicates the system fine-tunes its base model on the corresponding training set of the environment.}
\label{table:mind2web-results}
\end{table*}

\begin{table*}[t]
%\vspace{5pt}
\small\centering
\setlength{\tabcolsep}{3pt}
%\renewcommand\arraystretch{1.15}
\resizebox{\linewidth}{!}{
\begin{tabular}{l|cccccc|c}
\toprule
\multicolumn{1}{l}{\multirow{2}{*}{\textbf{System}}} \vline & \multicolumn{1}{c}{\multirow{2}{*}{\textbf{\#Params}}} & \multicolumn{1}{c}{\multirow{2}{*}{\textbf{\#Agents}}} & \multicolumn{2}{c}{\textbf{English}} & \multicolumn{2}{c}{\textbf{Chinese}} \vline & \multicolumn{1}{c}{\multirow{2}{*}{\textbf{Avg.}}}  \\ 
\cmidrule(lr){4-5}
\cmidrule(lr){6-7}
 &  & & \textbf{Cross-Task} & \textbf{Cross-Domain} & \textbf{Cross-Task} & \textbf{Cross-Domain} & \\ 
\midrule\noalign{\vskip -3pt}
        \multicolumn{8}{c}{\cellcolor[HTML]{EFEFEF} \small\textit{Agents based on Closed-Source LLMs}}    \\
GPT-3.5-Turbo & N/A & 1 & 12.1 & \leavevmode\hphantom{0}6.4 & 13.5 & 10.8 & 10.7 \\  
GPT-4 & N/A & 1 & \textbf{38.6} & \textbf{39.7} & \textbf{36.7} & \textbf{36.3} & \textbf{37.8}\\  
Claude2 & N/A & 1 & 13.2 & \leavevmode\hphantom{0}8.1 & 13.0 & \leavevmode\hphantom{0}7.9 & 10.5\\  
\midrule\noalign{\vskip -3pt} \multicolumn{8}{c}{\cellcolor[HTML]{EFEFEF} \small\textit{Agents based on Open-Source LLMs}}    \\
LLaMA2 & 7B & 1 & \leavevmode\hphantom{0}3.3 & \leavevmode\hphantom{0}2.5 & - & - & \leavevmode\hphantom{0}2.9\\  
LLaMA2 & 70B & 1 & \leavevmode\hphantom{0}8.3 & \leavevmode\hphantom{0}8.9 & - & - & 10.6\\ 
%\midrule\noalign{\vskip -3pt} \multicolumn{8}{c}{\cellcolor[HTML]{EFEFEF} \small\textit{\color{red}TODO Single Agent}}    \\
Qwen2 & 7B & 1 & \leavevmode\hphantom{0}8.6 & \leavevmode\hphantom{0}9.4 & \leavevmode\hphantom{0}8.1 & \leavevmode\hphantom{0}7.8 & \leavevmode\hphantom{0}8.5\\
\midrule\noalign{\vskip -3pt} \multicolumn{8}{c}{\cellcolor[HTML]{EFEFEF} \small\textbf{Our Methods}}    \\
Base UIAgent & 7B & 1 & 12.0 & 13.3 & 12.7 & 13.4 & 12.8\\
Base UIAgent & 7B & 4 & 13.7 & 14.5 & 15.0 & 13.9 & 14.0\\
\CollabUIAgentsMobile  & 7B & 4 & 18.6 & 17.7 & 19.1 & 15.6 & 17.7\\
\CollabUIAgentsWeb & 7B & 4 & \textbf{34.3} & \textbf{36.9} & \textbf{35.3} & \textbf{32.5} & \textbf{34.7}\\
\bottomrule
\end{tabular}
}
\caption{Step Success Rates (SSR) of agent systems on different LLMs in the AutoWebBench environment.}
\label{table:autowebbench_results}
% \vspace{-1em}
\end{table*}

\subsection{Method Implementation}

\subsubsection{Baselines}
(1) \textbf{M3A}~\citep{rawles2024androidinthewild} is a prompt-based multimodal agent, which combines ReAct-~\citep{yao2023react} and Reflexion-style~\citep{shinn2023reflexion} prompting to interpret user instructions and screen content, then update its decisions. (2) \textbf{SeeAct}~\citep{pmlr-v235-zheng24e} is a prompt-based navigation agent originally designed for GPT-4V to perform actions with visual input and textual choices. (3) \textbf{InfiGUIAgent}~\citep{liu2025infiguiagent} is used with the temperature set to 0.1, and other settings remain default. (4) \textbf{DigiRL}~\citep{bai2024digirl} is reproduced on the same dataset as \CollabUIAgents, based on Qwen-2.5-VL-3B-Instruct~\citep{Qwen2VL}. Due to limited computational resource and the high costs of RL training with multiple value estimators, a larger scaled base model could not be applied. (5) \textbf{SeeClick}~\citep{cheng2024seeclick} is a fine-tuned visual GUI agent that automates tasks relying on screenshots and employs GUI grounding. For those baselines based on closed-source LLMs, we conduct experiments using prompts and settings consistent with~\cite{rawles2025androidworld}, with model data up to August 2024. 


\subsubsection{\CollabUIAgents Framework}

In the agentic fine-tuning process for data collection, we employ GPT-4o-2024-08-06~\citep{openai2024gpt4technicalreport} as the UI agent. We set the temperature of different agents to 0.1 to ensure the accuracy of their responses. For the backbone model in the policy multi-agent system, we utilize Qwen2 7B~\citep{yang2024qwen2technicalreport}. Detailed process is depicted in Appendix~\ref{app:data-collection}. In this phase, the model undergoes both supervised learning and off-line preference learning with DPO. Details of the experimental data statistics can be found in Appendix~\ref{app:distr_data}. During the supervised fine-tuning phase, the model's context length is set to 8,192, with a learning rate of 1e-4 and training conducted over 3 epochs. During the preference optimization process, we adapt LoRA fine-tuning~\citep{hu2022lora}, and the model's context length is capped at 8,500, with a learning rate of 5e-6. We use bf16 precision, and the training is also carried out for 3 epochs. 

In the multi-agent reinforcement learning process, the critic agent is played by GPT-4o-2024-08-06, with its temperature also set to 0.1, providing reward feedback to the rolled out actions. In the MARL phase, the rolling out temperatures of different UI agents are set to 0.1, 0.3, 0.5, and 0.8, respectively. This variation encourages diverse responses and stimulates different behaviors. We use iterative-DPO~\citep{xiong2024iterative} as the online reinforcement algorithm, and the multi-agent system is updated for each epoch. We train the multi-agent system within the AndroidWorld environment, where the environment seed is randomly generated. Continue MARL on Mind2Web uses instruction datasets autonomously collected with the pipeline in Appendix~\ref{app:aft}. During testing, we maintain consistency with previous work~\citep{rawles2025androidworld} by setting the seed to 30. All our experiments are conducted on 8 A100 80GB GPUs. The model's context length is capped at 8,500, with a learning rate of 5e-6. We use bf16 precision, and the training is carried out for 3 epochs. 


\subsection{Data Collection for Agentic Fine-Tuning}\label{app:data-collection}

We collect data from the AndroidWorld environment for agentic fine-tuning the base model into the base UIAgent. In this section, we provide details of the data collection process, including the questions list, the quantity of the collected data, and the prompts for data collection.

\subsubsection{Questions}
\label{sec:question}
The questions used for UI basic environmental knowledge generation are shown in Table~\ref{tab:question}.

\begin{table}[ht]
    \small\centering
    %\renewcommand\arraystretch{1.2}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{l|p{3.6cm}}
        \toprule
        \multicolumn{1}{c|}{\textbf{Type}} & \multicolumn{1}{c}{\textbf{Question}}  \\ \midrule
         \multicolumn{1}{c}{\multirow{6}{*}{UI Understanding}} \vline & 1.~What is the purpose of the current UI? \\  
         & 2.~What does the current UI aim to achieve? \\  
         & 3.~Summarize the current interface in one paragraph. \\ 
        \midrule
        \multicolumn{1}{c}{\multirow{8}{*}{Element Recognition}}  \vline  & 1.~What is the function of UI element X? \\ 
         & 2.~What information does UI element X provide? \\  
         & 3.~What happens when click the UI element X? \\  
        & 4.~What action is associated with UI element X? \\
        \bottomrule
    \end{tabular}
    }
    \caption{Questions for UI basic environmental knowledge generation.}
    \label{tab:question}
\end{table}

\subsubsection{Statistics of the Collected Data}
\label{app:distr_data}
The quantity of the collected data is shown in Table~\ref{tab:datadistribution}.

\begin{table}[H]
    
  \small\centering
  %\renewcommand\arraystretch{1.2}
  \resizebox{0.72\linewidth}{!}{
  \begin{tabular}{l|c}
      \toprule
      \multicolumn{1}{c|}{\textbf{Data Type}} & \textbf{Number}  \\ \midrule
      Basic Environmental Data & 88,513 \\  
     Simple Instruction Data  & 18,041 \\ 
     Process Preference Data & \leavevmode\hphantom{0}3,440 \\
      \bottomrule
  \end{tabular}
  }
  \caption{Quantity of the collected data.}
  \label{tab:datadistribution}
\end{table}

\subsubsection{Prompts}
\label{sec:data-prompts}
Prompts for data collection process in the agentic fine-tuning are shown in Figure~\ref{prompt:summary}, \ref{prompt:element}, \ref{prompt:instruction}, and \ref{prompt:advagent}.


\subsection{Prompts for Different Agents in the CR}
\label{sec:cr-prompts}
Prompts for different agents in the CR are shown in Figure~\ref{prompt:advagent}, \ref{prompt:criticagent1} and \ref{prompt:criticagent2}.

\subsection{Prompts for \CollabUIAgents Framework}
\label{sec:env_prompt}

Prompts for the multi-agent system trained in \CollabUIAgents on mobile operation environments and web browsing environments are shown in Figure~\ref{prompt:action} and Figure~\ref{prompt:webprompt}, respectively, within the ReAct~\citep{yao2023react} style.


\begin{figure*}[ht]
  \centering  
  \includegraphics[width=1\linewidth]{UIAgent_summary.pdf}    
  \caption{The UI understanding prompt template.}
  \label{prompt:summary}
           %\vspace{-0.4cm}
\end{figure*}

\begin{figure*}[ht]
  \centering  
  \includegraphics[width=1\linewidth]{UIAgent_element.pdf}    
  \caption{The element recognition prompt template.}
  \label{prompt:element}
           %\vspace{-0.4cm}
\end{figure*}

\begin{figure*}[ht]
  \centering  
  \includegraphics[width=1\linewidth]{UIAgent_instruction.pdf}    
  \caption{The instruction knowledge prompt template.}
  \label{prompt:instruction}
           %\vspace{-0.4cm}
\end{figure*}


\begin{figure*}[htbp]
  \centering  
  \includegraphics[width=1\linewidth]{AdvAgent.pdf}    
  \caption{The prompt template for the adversarial agent.}
  \label{prompt:advagent}
           %\vspace{-0.4cm}
\end{figure*}

\begin{figure*}[htbp]
  \centering  
  \includegraphics[width=1\linewidth]{CriticAgent1.pdf}    
  \caption{The prompt template for the critic agent.}
  \label{prompt:criticagent1}
           %\vspace{-0.4cm}
\end{figure*}

\begin{figure*}[htbp]
  \centering  
  \includegraphics[width=1\linewidth]{CriticAgent2.pdf}    
  \caption{The few-shot prompt template for the critic agent.}
  \label{prompt:criticagent2}
           %\vspace{-0.4cm}
\end{figure*}


\begin{figure*}[htbp]
  \centering  
  \includegraphics[width=1\linewidth]{UIAgent_action.pdf}    
  \caption{The prompt template for mobile operation.}
  \label{prompt:action}
           %\vspace{-0.4cm}
\end{figure*}

\begin{figure*}[htbp]
   \centering  
   \includegraphics[width=1\linewidth]{web_prompt.pdf}    
   \caption{The prompt template for web browsing.}
   \label{prompt:webprompt}
            %\vspace{-0.4cm}
\end{figure*}

\end{document}
