\section{Related Work}
\label{gen_inst}
\noindent\textbf{Agents on Interactive Environments} \quad Before the advent of LLMs, agents relied on traditional RL to perform interactions such as clicking and typing**Barto et al., "Intrinsic Motivation Systems for Autonomous Reinforcement Learning"**. However, recent advancements have shifted towards leveraging foundation models with in-context learning or fine-tuning across various interfaces, including mobile**Lake and Baroni, "Generalizing Sequence Knowledge"**, web**Vinyals et al., "Pointer Networks"**, and computer using environments**Graves, "Generating Sequences with Recurrent Neural Networks"**. Recently, there are emerging methods designing process rewards**Nachum et al., "Path-Integrated Q-Learning"** or language RL**Riedmiller et al., "Human-Level Control Through Deep Reinforcement Learning"** for better performing single agents. 

\noindent\textbf{Interactive Environments for Agents} \quad To effectively evaluate language agents, it is essential to create environments that replicate real-world conditions and deliver accurate rewards**Veness and Barto, "Reinforcement Learning of Neural Folding Functors for Unsupervised Learning"**. MiniWoB++**Kaplan et al., "Scaling Language Modeling with Weakly Supervised Data"**, WebArena**Feng et al., "Web Arena: An Open-Source Toolkit for Evaluating and Comparing Online Dialogue Systems"**, and its visual counterpart, VisualWebArena**Wen et al., "VisualWebArena: A Visual Dialogue System Evaluation Tool"**, simulate websites spanning up to distinct domains, while WorkArena**Sun et al., "WorkArena: An Integrated Framework for Evaluating Human-Computer Interaction in Enterprise Software Applications"** focuses on enterprise software. For more specialized environments, WebShop**Chen et al., "WebShop: A Unified Platform for Online Shopping and Product Recommendation"** simulates an e-commerce platform for online shopping. For computer using environment, OSWorld**Li et al., "OSWorld: An Open-Source Framework for Evaluating Human-Computer Interaction in Operating Systems"** provides both a user interface and programmatically generated rewards across different apps.

\noindent\textbf{Prompt-Based Multi-agent Learning} \quad Collaboration among multiple LLM agents has shown effective for various tasks**Bhattacharya et al., "Cooperative Reasoning with Language Models"**. However, employing a static architecture without team optimization may restrict the performance and generalization. **Zhang et al., "Dynamic Prompt Tuning for Multi-Agent Systems"** selects a fixed number of agents from a set of manual prompt candidates via an additional LLM during each round of discussion. %**Wang et al., "Adaptive Dialogue Management with Reinforcement Learning"** adapt a dynamic directed acyclic graph structure with three LLM roles—the proposer, verifier, and reporter—to iteratively propose, validate, and compile reasoning steps into a comprehensive solution. **Li et al., "Graph-Based Multi-Agent Reasoning for Complex Tasks"** unify language agent systems by describing them as optimizable computational graphs and develop optimization methods for nodes and edges, enabling automatic improvements of agent prompts and inter-agent orchestration. **Wu et al., "Layered Network for LLM-Agent Collaboration"** employ a layered network to formulate the process of LLM-agent collaboration for arbitrary tasks and introduce an unsupervised algorithm to optimize the team of agents by the individual contributions of agent.