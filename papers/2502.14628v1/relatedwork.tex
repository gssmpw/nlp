\section{Related Work}
\textbf{Order Sensitivity in In-context Learning} 
Despite the huge success of ICL, its robustness to demonstration permutations remains an unresolved challenge 
\citep{pmlr-v139-zhao21c}.

Most \textbf{\textit{training-stage methods}} focus on improving general performance in ICL \citep{min-etal-2022-metaicl, wei-etal-2023-symbol} while neglecting the lack of robustness to the permutations of demonstrations. Recent studies suggest that this phenomenon stems from the autoregressive nature of transformer language models \citep{chen2023positionalinformationmattersinvariant,xiang-etal-2024-addressing}.
InfoAC \citep{xiang-etal-2024-addressing} introduces contrastive learning during fine-tuning to break the autoregressive constraint and enable bidirectional token visibility; however, their approach achieves limited success and is restricted to classification tasks. Preliminary work of \citet{chen2023positionalinformationmattersinvariant} shows the DeepSet architecture exhibits better permutation invariance than transformer; however, this MLP-based new architecture is too small to solve complex language modeling tasks.
\textit{Our approach falls within the category of training-stage methods but proposes a general learning framework that enhances permutation robustness in LLMs without modifying the Transformer architecture or its autoregressive objective, thereby preserving scalability}.

\textbf{\textit{Inference-stage methods}} can be categorized into four types: 
(1) demonstration selection \citep{chang2023datacurationstabilizeincontext, peng-etal-2024-revisiting}, 
which improves normal-case performance but lacks worst-case guarantees under permutations; 
(2) output calibration \citep{pmlr-v139-zhao21c, li-etal-2023-distinguishability, guo-etal-2024-makes}, which are effective for classification but is less applicable to generation tasks due to sequence calibration challenges; (3) order optimization \citep{lu-etal-2022-fantastically}, which finds the best ordering during inference but suffers from exponential complexity; and (4) prediction ensembling: 
A recent work \citep{zhang-etal-2024-batch} transforms n-shot ICL into n one-shot predictions and ensembles resultsâ€”effective for classification but harms generation.
In summary, inference-stage methods mitigate order sensitivity via pre/post-processing, often introducing additional inference overhead. Moreover, most methods target classification and underperform on generation tasks.
\textit{In contrast, our training-stage solution complements inference-stage methods, enhancing LLM robustness without additional inference costs while remaining broadly applicable to various tasks.}



\noindent \textbf{Distributionally Robust Optimization}.
Distributionally robust optimization optimizes the objective function over ambiguity sets, often defined as balls centered on the empirical distribution \citep{BenTal2013Robust, Lam2015Quantifying, Duchi2016, Miyato2018Virtual}. 
Prior applications of DRO have primarily addressed distributional shifts, including label shift \citep{hu2018does} and data source shift \citep{oren2019drolm} and group shift \citep{2020Distributionally}. 
To the best of our knowledge, we are the first to apply DRO to enhance the ICL robustness of LLMs by defining the ambiguity set over all possible permutations of the empirical distribution, thereby providing performance guarantees.

\noindent \textbf{Optimal Transport}. 
Optimal transport is a fundamental mathematical discipline established by \citet{monge1781memoire,kantorovich1942transfer}. It defines a metric for measuring distances between probability distributions, known as the Wasserstein distance, and has been widely employed in machine learning for distribution matching \citep{montesuma2024recentadvancesoptimaltransport,ot_ds_2024}. 
Our work extends the concept of learning permutation structures through neural networks, as explored in \citep{mena2018learning} for learning to sort numbers or solve jigsaw puzzles. 
However, we apply OT in the context of LLMs and design a neural network, P-Net, equipped with the Sinkhorn operator to generate challenging permutations, enabling LLMs to undergo DRO training and thereby enhancing their ICL robustness.