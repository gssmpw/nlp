
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}

\usepackage{multirow}
\usepackage{algpseudocode}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{floatflt}

\usepackage{xspace}

\usepackage{makecell}
\usepackage{tabularx}
\usepackage{xcolor}

\definecolor{skyblue}{RGB}{30,144,255}  


\usepackage[T1]{fontenc}

\newcommand{\theHalgorithm}{\arabic{algorithm}}

\usepackage{colortbl}
\newcommand{\lightcmidrule}{\arrayrulecolor{gray!20}\cmidrule[0.3pt](lr){2-12}\arrayrulecolor{black}}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{wrapfig}
\usepackage{adjustbox}
\usepackage{enumitem}
\usepackage{caption}
\usepackage{placeins}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}

\usepackage{caption}
\usepackage{subcaption}
\usepackage{booktabs} % For professional-quality tables
\usepackage{wrapfig}  % If you need to wrap figures (though not used here)

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


\newcommand{\modelEmoji}{\includegraphics[height=1.2em,trim=0 .6em 0 0]{fig/pearl.png}}
\newcommand{\modelWithEmoji}{\modelEmoji \textsc{Pearl}\xspace}

\title{\modelWithEmoji: Towards Permutation-Resilient LLMs}


\author{
Liang Chen$^{1}$ \hspace{6.5pt} Li Shen$^{2}$\thanks{Corresponding Authors.} 
\hspace{7pt} Yang Deng$^3$ \hspace{7pt} Xiaoyan Zhao$^1$ 
\hspace{7pt} Bin Liang$^1$ \hspace{7pt} Kam-Fai Wong$^{1*}$ \\
$^1$The Chinese University of Hong Kong\hspace{4pt} 
$^2$Shenzhen Campus of Sun Yat-sen University\hspace{4pt} 
$^3$SMU \\
\texttt{\{lchen, kfwong\}@se.cuhk.edu.hk}\quad \texttt{mathshenli@gmail.com}
}

\definecolor{jweigreen}{rgb}{0,0.45,0.24}
\definecolor{frenchblue}{rgb}{0.0, 0.45, 0.73}
\definecolor{lossred}{rgb}{0.8, 0.0, 0.0}

\newcommand{\jweigreen}[1]{{\color{jweigreen}{#1}}}
\newcommand{\darkgreen}[1]{{\color{ao(english)}{#1}}}
\newcommand{\frenchblue}[1]{{\color{frenchblue}{#1}}}

\newcommand{\greenbold}[1]{\underline{\textbf{\jweigreen{\normalsize{#1}}}}}
\newcommand{\bluegain}[1]{\textbf{\frenchblue{(+#1)}}}
\newcommand{\redloss}[1]{{\color{lossred}{\textbf{(-#1)}}}}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}


\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}

\maketitle

\begin{abstract}
The in-context learning (ICL) capability of large language models (LLMs) enables them to perform challenging tasks using provided demonstrations.
However, ICL is highly sensitive to the ordering of demonstrations, leading to instability in predictions. 
This paper shows that this vulnerability can be exploited to design a natural attack—difficult for model providers to detect—that achieves nearly 80\% success rate on LLaMA-3 by simply permuting the demonstrations.
Existing mitigation methods primarily rely on post-processing and fail to enhance the model’s inherent robustness to input permutations, raising concerns about safety and reliability of LLMs.
To address this issue, we propose \textbf{Pe}rmut\textbf{a}tion-\textbf{r}esilient \textbf{l}earning (\textsc{\textbf{Pearl}}), a novel framework based on distributionally robust optimization (DRO), which optimizes model performance against the worst-case input permutation.
Specifically, \textsc{Pearl} consists of a permutation-proposal network (P-Net) and the LLM. The P-Net generates the most challenging permutations by treating it as an optimal transport problem, which is solved using an entropy-constrained Sinkhorn algorithm.
Through minimax optimization, the P-Net and the LLM iteratively optimize against each other, progressively improving the LLM’s robustness.
Experiments on synthetic pre-training and real-world instruction tuning tasks demonstrate that \textsc{Pearl} effectively mitigates permutation attacks and enhances performance.
Notably, despite being trained on fewer shots and shorter contexts, \textsc{Pearl} achieves performance gains of up to 40\% when scaled to \emph{many-shot} and \emph{long-context} scenarios, highlighting its efficiency and generalization capabilities.
\end{abstract}


\section{Introduction}
A hallmark of human intelligence is the ability to learn and execute new tasks by reasoning from a few examples. Mirroring this, in-context learning \citep{NEURIPS2020_gpt3}, as a crucial supplement to zero-shot prompting, has shown promising results across a spectrum of complex tasks \citep{gsm8k,chen-etal-2023-beyond, openai2023gpt4}.
Despite these advancements, the in-context learning capabilities of LLMs remain fragile. LLMs exhibit sensitivity to permutations of provided demonstrations \citep{lu-etal-2022-fantastically, pmlr-v139-zhao21c, Reynolds2021}, posing challenges for prompt engineering and leaving a significant gap in achieving human-like adaptability.

Most existing studies on ICL primarily aim to enhance normal-case performance \citep{min-etal-2022-metaicl, wei-etal-2023-symbol}, with limited attention to improving permutation robustness. Current approaches to addressing this issue typically involve modifying training objectives to mitigate the limitations of transformers' unidirectional attention \citep{xiang-etal-2024-addressing} or designing permutation-equivariant architectures \citep{chen2023positionalinformationmattersinvariant}. However, these methods often lack scalability. Alternatively, decoding-stage techniques such as output calibration \citep{pmlr-v139-zhao21c} and demonstration order optimization \citep{lu-etal-2022-fantastically} introduce additional computational overhead per inference call, further limiting their practicality. Thus, a critical need remains for methods that fundamentally enhance LLMs’ inherent robustness to input permutations.

In this work, we conduct extensive experiments on LLaMA-3, an advanced open-source LLM, to reassess its vulnerability to permutation-based attacks from an adversarial perspective (§\ref{sec:preliminary}). Our findings reveal that even LLaMA-3 remains highly susceptible to simple attacks that alter only the order of ICL demonstrations. These attacks preserve the semantic content of examples and introduce no adversarial modifications, yet they degrade performance with success rates exceeding 80\%. Consequently, they are difficult for model providers to detect but significantly undermine LLM performance, highlighting a critical vulnerability concern.

To counteract the vulnerability to input permutations, we introduce a novel \textbf{Pe}rmut\textbf{a}tion-\textbf{r}esilient \textbf{l}earning (\textsc{\textbf{Pearl}}) framework, which is based on distributionally robust optimization (DRO) \citep{BenTal2011RobustSO}.
Unlike standard empirical risk minimization training, adopted by most supervised fine-tuning (SFT) methods,
which views each training instance merely in terms of its one or several permutations observed during training, DRO conceptualizes each instance as part of a broader distribution that includes all conceivable permutations. This comprehensive set of all possible permutations is termed the ambiguity set. By explicitly identifying and optimizing the worst-case within this ambiguity set, our strategy substantially enhances the resilience of LLMs against all different permutations. 
This paradigm shift---from considering training instances as single data points to viewing them within a distribution of potential permutations---equips the model to better prepare for and generalize to combinatorial input scenarios.


Specifically, \textsc{Pearl} operationalizes DRO
as a two-player game, consisting of a permutation-proposal network (P-Net) as the adversary and the LLM as the target model. 
For each training instance, P-Net identifies a challenging permutation of given demonstrations, aiming to maximize the LLM's loss. Conversely, the LLM strives to minimize its loss under the P-Net's manipulation, thereby performing well on these difficult examples.
P-Net treats the generation of the adversarial ICL permutation as an optimal transport (OT) \citep{monge1781memoire} problem between the distribution over input permutations and the distribution of challenging permutations for LLMs. We solve the OT problem using the Sinkhorn algorithm \citep{sinkhorn_1966} with an element-wise entropy constraint designed to prevent trivial solutions. 
Through adversarial training (AT), both networks improve iteratively. Ideally, at convergence, the P-Net represents a uniform distribution across all permutations, as the LLM handles all possible permutations equally well.

We validate our method in two scenarios: (1) pretraining a transformer to in-context learn linear functions \citep{garg2022what}, and (2) instruction tuning of LLMs on the Super-Natural Instructions \citep{wang2022niv2}. The results demonstrate that, on unseen tasks, our method consistently improves both the average and worst-case performance of LLMs across different permutations, effectively defending against permutation-based attacks. Furthermore, despite being trained with much smaller configurations, our method generalizes effectively to \textbf{\emph{many-shot ICL}} and \textbf{\emph{long sequences}}, achieving performance gains of 24\% to 40\%. These results highlight the efficiency and generalization capabilities of our approach. The code is available at \url{https://github.com/ChanLiang/PEARL}.


\section{Revisiting Permutation Vulnerability in LLMs}\label{sec:preliminary}

This section investigates performance fluctuations in SOTA open-source LLMs when presented with different permutations of given demonstrations.
Additionally, from an adversarial perspective, we explore whether this vulnerability can be exploited to devise an effective attack on LLMs.\vspace{-1.4mm}
\begin{figure*}[t]
\setlength{\abovecaptionskip}{5pt}   
\setlength{\belowcaptionskip}{0pt}
    \centering
    \includegraphics[width=\textwidth]{fig/Revisit_CurDial_TMW_11.pdf}
    \caption{
    Performance and attack success rates of Llama-3 on CurDial and TMW datasets. Left panels: Random, average and worst-case performance as a function of shot number. Right panels: Attack success rates for exhaustive and neural search attack methods at different thresholds. 
    }
    \label{fig:permutation_vulnerability}
    \vspace{-0.45cm}
\end{figure*}
\paragraph{Experimental Setups}
To conduct evaluations, we select two tasks from Super-NaturalInstructions \citep{wang2022niv2}, including Curiosity-based Dialog (CurDial) and TellMeWhy QA (TMW). We test 100 samples for each task, with each sample structured as a quadruple consisting of (instruction, demonstrations, input, output). The number of demonstrations (shots) ranges from two to six. Following \citet{wang2022niv2}, the performance is measured using the ROUGE-L \citep{lin-2004-rouge}.
We analyze the permutation vulnerability of LLaMA-3-8B on two settings as follows:\vspace{-1.4mm}

\paragraph{1) Permutation Vulnerability on Different Number of Demonstrations}
We first examine the average and worst-case performance of the model across different permutations of input demonstrations and the effect of scaling the number of demonstrations. 
As shown in the left of Figure \ref{fig:permutation_vulnerability}, there is a notable observation:
\textit{\textbf{adding demonstrations is a double-edged sword.}} Increasing the number of demonstrations (\emph{shots}) generally enhances the model's average performance due to richer contextual information. However, it can simultaneously worsen the worst-case performance. 
This suggests that while additional demonstrations provide useful context, the exponentially growing number of permutations (n!) increases the risk of the model performing poorly on certain input configurations.
\vspace{-1.4mm}
\paragraph{2) Input Permutation as Attack} 
We then consider a two-party adversarial scenario between a malicious user (attacker) and a model provider (defender). 
The attacker aims to induce compromised responses from LLMs by permuting ICL demonstrations, making the attack less detectable.
Given a task \( D = \{(p_i, x_i, y_i)\} \), a sample is successfully attacked if its relative performance degradation, induced by the attacker, exceeds a threshold \( \delta \in [0\%, 100\%] \). Here, \( p_i \) is an ICL prompt with \( n \) demonstrations. 
The set of all possible demonstration permutations is \( \mathbb{P} = \{\Pi_0, \dots, \Pi_{n!-1}\} \), where \(|\mathbb{P}| = n!\). Let \( g \) be a performance metric (e.g., ROUGE-L). The attack success rate (ASR) for task \( D \) is defined as:\vspace{-1.2mm}
\begin{equation}
\small
\text{ASR}(D, \delta) = \frac{1}{|D|} \sum_{i=1}^{|D|} \mathbb{I} \big( (\mu_i - \omega_i) / \mu_i \geq \delta \big)
\end{equation}
where \(\mathbb{I}\) denotes the indicator function, \(|D|\) is the size of the dataset, and \(\delta\) is the threshold. The average performance of the i-th sample, \(\mu_i\), is defined by:
\begin{equation}
\small
\mu_i = \mathbb{E}_{\Pi \sim \mathbb{P}}[g(\Pi \cdot p_i, x_i; y_i)] = \frac{1}{n!} \sum_{j=1}^{n!} g(\Pi_j \cdot p_i, x_i; y_i)
\end{equation}
and \(\omega_i\) is the compromised performance induced by the attack strategy adopted by the malicious user. Here, we analyze two attack method: \vspace{-1.0mm}
\begin{itemize}[leftmargin=*]
\item \textbf{Exhaustive Search Attack}:
To \textit{calculate the upper bound} of the effect the permutation-based attack can achieve, we assume that the malicious user has unlimited attempts and conducts an exhaustive search. For each sample $(p_i, x_i, y_i)$, this process involved testing all possible permutations of demonstrations in $Q_i$ and identifying the permutation that yields the poorest performance. In this case, the attacked performance is calculated as follows:
\begin{equation}
\omega_i = \min_{\Pi \in \mathbb{P}} g(\Pi \cdot p_i, x_i; y_i)
\end{equation}
\item \textbf{Neural Search Attack}:
To \textit{approximate the upper bound} established by the exhaustive search when the number of attempts is limited, we employ a meta-learning approach to optimize a permutation-proposal network (P-Net).
As illustrated in Figure \ref{fig:overview} (details are in the Methods section), during training, this network takes the standard sample $(p_i, x_i, y_i)$ as input and outputs a permutation matrix $\Pi_i$. The permuted samples $(\Pi_i \cdot p_i, x_i, y_i)$ are then fed into the LM to maximize its loss function. During testing, the network generates the most challenging permutation $\Pi_i$ for each sample $(p_i, x_i, y_i)$. Then the attacked performance is calculated as follows:
\begin{equation}
\omega_i = g(\Pi_i \cdot p_i, x_i; y_i),\qquad \text{s.t. } \Pi_i \sim \text{P-Net}(p_i, x_i, y_i)
\end{equation}
\end{itemize}
As shown in the right of Figure \ref{fig:permutation_vulnerability}, the results indicate that
\textit{\textbf{permutation attacks are effective and approachable.}} Leveraging this characteristic, the exhaustive search attack successfully attacks over 50\% and 80\% of the samples with $\delta = 50\%$ on two datasets respectively, and the neural attack achieved a successful rate close to this upper bound across different shots. These results demonstrate that this vulnerability poses a real concern, even for advanced LLMs like LLaMA-3.\vspace{-1.2mm}


\paragraph{Remark}
These deficiencies may directly stem from the fundamental limitations of standard Empirical Risk Minimization (ERM) training, which focuses on optimizing average performance while neglecting worst-case performance. We discuss this issue in depth in the next section and propose a method to address the model's improper behavior on unseen but practically valid input spaces.

\section{Permutation-Resilient Learning (Pearl)}\vspace{-1.4mm}
\subsection{Instruction Tuning via DRO}
Our objective is to train a LLM to perform well across all possible permutations of given demonstrations when prompted with few-shot instructions.

In supervised fine-tuning for few-shot learning, the LLM is trained to predict an output \( y \in \mathcal{Y} \) given an input \( x \in \mathcal{X} \) and a few-shot instruction \( p \in \mathcal{P} \), where \( p \) typically consists of a sequence of demonstrations, each being an input-output pair.
Let \( \Theta \) denote the parameter space of the language model, and let \( \ell: \Theta \times (\mathcal{P} \times \mathcal{X} \times \mathcal{Y}) \rightarrow \mathbb{R}_+ \) be a nonnegative loss function measuring the discrepancy between the model's prediction and the true output. The standard approach is to find parameters \( \theta \in \Theta \) that minimize the empirical loss over the training data via empirical risk minimization:\vspace{-0.9mm}
\begin{align}\label{eqn:erm}
\small
  \hat{\theta}_{\text{ERM}} \coloneqq \arg\min_{\theta \in \Theta} \mathbb{E}_{(p, x, y) \sim \hat{P}}[\ell(\theta; p, x, y)]
\vspace{-1.4mm}
\end{align}
where \( \hat{P} \) denotes the empirical distribution derived from the training dataset.

Under appropriate assumptions, learning theory \citep{Vapnik1999-VAPTNO,shalev2014understanding} guarantees that models trained via ERM perform well on the test distribution given sufficient training data. However, in practice, models trained using ERM often fail to generalize well to different permutations of the same set of demonstrations. This occurs because the training set covers only a subset of all possible permutations of the demonstrations, and during testing, the model may encounter permutations not seen during training, leading to a significant degradation in performance. 

To systematically address the permutation sensitivity issue, we propose fine-tuning under the framework of distributionally robust optimization, which optimizes the risk under the worst-case distribution within a specified ambiguity set. Specifically, we aim to solve:
\begin{align}\label{eq:dro}
\small
  \hat{\theta}_{\text{DRO}} \coloneqq \arg\min_{\theta \in \Theta}  \Bigl\{ \sup_{Q_\Pi \in \mathcal{Q}} \mathbb{E}_{(p, x, y) \sim Q_\Pi}[\ell(\theta; p, x, y)]\Bigr\}
\end{align}
The ambiguity set \( \mathcal{Q} \) is constructed as the convex hull of all distributions obtained by permuting the prompts in the empirical distribution \( \hat{P} \). Specifically, we define:
\begin{align}\label{eq:Q_combined}
\small
\mathcal{Q} := \bigg\{ \sum_{\Pi \in \mathbb{P}} q_\Pi\, Q_\Pi \,\Big|\, q \in \Delta_{|\mathbb{P}| - 1} \bigg\}, \quad
\begin{aligned}
&\text{where} \quad
Q_\Pi := \left\{ \big( \Pi \cdot p,\, x,\, y \big) \,\middle|\, (p, x, y) \sim \hat{P} \right\}.
\end{aligned}
\end{align}
Here, \( \Pi \) is a permutation matrix that reorders the sequence of demonstrations in \( p \), and \( \mathbb{P} \) denotes the set of all such matrices. The vector \( q \) lies in the \( |\mathbb{P}|-1 \)-dimensional probability simplex \( \Delta_{|\mathbb{P}| - 1} \).


\begin{wrapfigure}{R}{0.54\textwidth}
    \FloatBarrier % Forces placement
    \vspace{-0.31cm}
    \centering
    \includegraphics[width=0.54\columnwidth]{fig/DRO.pdf}
    \caption{
    Comparison of models trained under ERM and DRO paradigms. The blue bars represent the empirical distribution $\hat{P}$ of training data, showing different frequencies of six permutations in the training set. 
    The purple curves denote the learned distribution $P_\theta$ by (a) ERM and (b) DRO models, illustrating their different behaviors on less appeared but valid permutations.
    }
    \label{fig:dro_comparison}
    \vspace{-0.34cm}
\end{wrapfigure}
\FloatBarrier

To illustrate the advantages of DRO over ERM in handling different permutations, consider the example in Figure \ref{fig:dro_comparison}. For a 3-shot training example $(p,x,y)$ with prompt $p$ containing three demonstrations, there are six possible permutations denoted as ${(p^0, x, y), \ldots, (p^5, x, y)}$, indexed from 0 to 5. $\hat{P}$ denotes the empirical distribution of permutations in training data, represented by blue bars. The bars show that permutation indices 0, 1, and 4 appear in training data with frequencies, while permutations 2, 3, and 5 do not appear. $P_\theta$ represents the distribution learned by the LLM, represented by purple curves. In panel (a), the ERM-trained model assigns higher probabilities to frequently occurring permutations $(0, 1, 4)$ and lower probabilities to less frequent ones $(2, 3, 5)$, leading to poor performance on unseen permutations during testing. In contrast, panel (b) shows that the DRO-trained model distributes probabilities more uniformly across all possible permutations, as it explicitly considers them all (Equation (\ref{eq:dro})) during learning. This demonstrates how DRO mitigates ERM's limitations by encouraging models to assign reasonable probabilities to all valid permutations, regardless of their frequency in training data.

\subsection{Learning to Generate Permutations via P-Net}

\begin{figure*}[t]
\setlength{\abovecaptionskip}{5pt}   
\setlength{\belowcaptionskip}{0pt}
    \centering
    \includegraphics[width=0.8\textwidth]{fig/PEARL.png}
    \caption{An overview of the learning framework. The P-Net is a small model incorporating the Sinkhorn operator, trained jointly with the LLM under the adversarial optimization algorithm. \textit{Note that the permutation matrix operates on the input sequence's embeddings (simplified here as text sequences for clarity)}. After training, only the LLM is retained while the P-Net is discarded.
    }
    \label{fig:overview}
    \vspace{-0.2cm}
\end{figure*}

To enable our DRO framework to function effectively, we need to efficiently find the worst-case scenario within the ambiguity set (solve the $\max$ step in Equation~(\ref{eq:dro})). Directly addressing this problem through exhaustive search is computationally infeasible due to the exponential search space.

We address this challenge by drawing inspiration from optimal transport, treating the problem as transportation between permutation distributions. We introduce the \textbf{P}ermutation\textbf{-}proposal \textbf{Net}work \textbf{(P-Net)}, denoted as P-Net\(: (\mathcal{P} \times \mathcal{X} \times \mathcal{Y}) \rightarrow \Delta(\Pi)\), which learns a distribution over permutations to increase task difficulty for the LLM given input examples. As shown in Figure~\ref{fig:overview}, we sample challenging permutations from this distribution to reorder the given demonstrations.

Specifically, P-Net consists of two components: a parameter part that extracts features and models the relationships between demonstrations, a non-parameter part using the Sinkhorn algorithm to build the distribution $\Delta(\Pi)$, and Gumbel sampling for differentiable sampling from it ($\Pi \sim \Delta(\Pi)$). \vspace{-1.4mm}


\paragraph{Parameter component.}
The parameter component consists of a \textit{feature extractor} and a \textit{cross-relationship modeling layer}.
The feature extractor is an encoder model that takes an ICL prompt composed of $n$ demonstration pairs $p = \{ (x_i, y_i) \}_{i=1}^n$ and a predicting sample $(x, y)$, and produces their representations as follows:\vspace{-0.4mm}
\begin{equation}
\small
\left( [\mathrm{CLS}], (x_1, y_1), \ldots, [\mathrm{CLS}], (x_n, y_n), [\mathrm{CLS}], (x, y) \right) \xrightarrow{\text{Encoder}} \left( \mathbf{h}_1, \mathbf{h}_2, \ldots, \mathbf{h}_n, \mathbf{h}_{n+1} \right),
\end{equation}
where $\mathbf{h}_i$ is the representation corresponding to the $i$-th $[\mathrm{CLS}]$ token, which is often used to segment and extract the representation of sequences \citep{devlin2019bertpretrainingdeepbidirectional,lu-etal-2021-less}.

After extracting the representations of $n$ demonstrations, we have $H = \left( \mathbf{h}_1, \mathbf{h}_2, \ldots, \mathbf{h}_n \right) \in \mathbb{R}^{n \times h}$. We then model the pairwise relationships among the demonstrations. Specifically, we design a simple \textit{cross-demonstration layer} to obtain a relationship matrix $\mathbf{R} \in \mathbb{R}^{n \times n}$ that captures the pairwise relationships between each pair of demonstrations, defined as: \vspace{-0.4mm}
\begin{equation}
\small
\mathbf{R} = g\left( H W H^\top \right),
\end{equation}
where $W \in \mathbb{R}^{h \times h}$ is a weight matrix, and $g$ denotes a nonlinear activation function.


The matrix $\mathbf{R}$ can be interpreted as an \textbf{adjacency matrix} in graph theory, where demonstrations serve as nodes, and $R_{ij}$ represents the relationship between demonstrations $i$ and $j$. Specifically, we define $R_{ij}$ as the potential increase in task difficulty for the LLM if demonstrations $i$ and $j$ are swapped; higher values of $R_{ij}$ indicate that swapping these two demonstrations may significantly impact prediction. Thus, this parameterized component models an \textbf{edge prediction} process. 

However, while $\mathbf{R}$ captures the potential for swapping between demonstrations, it is not yet suitable for sampling permutations because its elements can take any real values and do not necessarily form a valid probability distribution. To convert $\mathbf{R}$ into a distribution over permutations $\Delta(\Pi)$ that we can sample from, we introduce a non-parameter component.

\paragraph{Non-parameter component.}
The non-parameter component aims to transform the adjacency matrix $\mathbf{R}$ into a \textit{doubly stochastic matrix}, representing a probability distribution over permutations.
Specifically, following \citet{adams2011ranking,mena2018learning}, we adopt the Sinkhorn operator 
\( S(\cdot) \) to obtain such matrices through an iterative process of row and column normalization:
\begin{gather}
\small
S(R) = \lim_{l \rightarrow \infty} \left( \mathcal{T}_c \left( \mathcal{T}_r \left( \exp(R) \right) \right) \right), \\
\mathcal{T}_r(R) = R \oslash \left( R \mathbf{1}_n \mathbf{1}_n^\top \right), \quad
\mathcal{T}_c(R) = R \oslash \left( \mathbf{1}_n \mathbf{1}_n^\top R \right),
\end{gather}
where \( \mathcal{T}_r(R) \) and \( \mathcal{T}_c(R) \) represent the row and column normalization operators, respectively; \( \oslash \) indicates element-wise division; and \( \mathbf{1}_n \) is a column vector of ones. As established by \citep{sinkhorn_1966}, the Sinkhorn operator \( S(R) \) strictly converges to a doubly stochastic matrix as the number of iterations \( l \) approaches infinity.

To ensure a differentiable process when sampling permutations from the distribution, the Gumbel trick \citep{jang2017categorical} is applied:
\begin{gather}\label{eq:convergence}
\small
\Pi = \lim_{\tau \rightarrow 0} S \left( (R + G)/\tau \right), \\
G_{ij} = - \log \left( - \log G_{ij}' \right), \quad G_{ij}' \sim U(0,1),
\end{gather}
Where \( G \in \mathbb{R}^{n \times n} \) is the Gumbel noise and \( \tau \) the temperature. 
As \( \tau \) approaches zero, the result approximates a permutation matrix \( \Pi \). 
Hyperparameters are studied in Appendix~\ref{app:hyper_ana}.

By regarding permutation generation as an optimal transport problem and implementing it through P-Net, we transform the input permutation distribution into a target distribution.
Next, we introduce how P-Net is co-optimized with the LLM to make the target permutation distribution the most challenging for the current LLM.



\subsection{Adversarial Optimization}
As illustrated in Figure~\ref{fig:overview}, we adopt an adversarial optimization framework to jointly train the LLM and the P-Net. Let $\theta$ and $\phi$ denote the parameters of the LLM and P-Net, respectively. For each sample $(p, x, y)$ drawn from the empirical distribution $\hat{P}$, the P-Net generates an adversarial permutation $\Pi$ that maximizes the LLM's loss. In response, the LLM aims to minimize its loss, adversarially influenced by the P-Net. The LLM's loss function is defined as:
\begin{equation}
L_{\text{lm}}(\phi; \theta) = \mathbb{E}_{(p, x, y) \sim \hat{P}, \Pi \sim \text{P-Net}(\phi; p, x, y)}[\ell(\theta; (\Pi \cdot p, x, y))]
\end{equation}
To prevent the P-Net from collapsing to trivial solutions, such as producing uniform permutations that degrade demonstration semantics, we introduce an entropy-based regularization term:
\begin{equation}
 L_{\text{ent}}(\phi) = \mathbb{E}_{(p, x, y) \sim \hat{P}, \Pi \sim \text{P-Net}(\phi; p, x, y)} [\mathcal{H}(\Pi)],
\end{equation}
where $\mathcal{H}(\cdot)$ denotes the element-wise entropy function.

This results in a two-player min-max optimization problem with the following objective:
\begin{equation} \label{eq:pnet_opt}
\min_{\theta} \max_{\phi} \left( L_{\text{lm}}(\phi; \theta) - \beta L_{\text{ent}}(\phi) \right),
\end{equation}
where $\beta$ is a hyperparameter controlling the strength of entropy regularization.

We employ alternating optimization to iteratively update $\theta$ and $\phi$. The full training procedure is detailed in Algorithm~\ref{alg:training}.

\begin{algorithm}[H]  
    \small
    \SetAlgoLined
    \caption{Adversarial Optimization Algorithm for \textsc{Pearl}}
    \label{alg:training}
    
    \KwIn{\(\theta, \phi\) (LLM, P-Net); \(\eta_{\theta}, \eta_{\phi}\) (learning rates); \(m\) (inner steps); \(\beta\) (entropy coefficient)}
    
    \Repeat{convergence}{
        \For{$t = 1$ \KwTo $m$}{
            $(p, x, y) \sim \hat{P}$ \tcp*{Sample training examples}
            $\Pi \sim \text{P-Net}(\phi, p, x, y)$ \tcp*{Generate permutations}
            $L_{\text{lm}}(\phi, \theta) \gets \ell(\theta; \Pi \cdot p, x, y)$ \tcp*{Compute LLM loss}
            $L_{\text{ent}}(\phi) \gets \mathcal{H}(\Pi)$ \tcp*{Compute entropy regularization}
            $\phi \gets \phi + \eta_\phi \nabla_{\phi} (L_{\text{lm}} - \beta L_{\text{ent}})$ \tcp*{Update P-Net}
        }
        $\theta \gets \theta - \eta_\theta \nabla_{\theta} L_{\text{lm}}(\phi, \theta)$ \tcp*{Update LLM}
    }
\end{algorithm}


\section{In-Context Learning with Linear Functions}

\subsection{Datasets and Evaluation Metrics}
We investigate in-context learning on linear functions $f(x) = w^\top x$, where $w \in \mathbb{R}^d$, following \citet{garg2022what, guo2024how}. For each $w$, we construct each example $p^i = (x_1, f(x_1), \ldots, x_i, f(x_i), x_{i+1})$ containing $i$ input-output demonstration pairs and a query input $x_{i+1}$. A language model $LM$ is trained to minimize:
\begin{equation}
\small
\min_\theta \mathbb{E}_{p} \left[\frac{1}{k+1} \sum\nolimits_{i=0}^{k} \ell(\theta; p^i, f(x_{i+1})) \right],
\label{eq:loss}
\end{equation}
where $\ell(\cdot)$ is the MSE loss and $k$ is the number of demonstrations. During testing, we evaluated performance using the same MSE metric. We report the normalized squared error $((LM(p) - w^\top x_{\text{query}})^2/d)$, where $d$ is the problem dimension. Detailed settings are in Appendix \ref{app:LR_setting}.


\subsection{Implementation Details and Baselines}

\textbf{Architecture and Training.}
We implement \( L_\theta \) using a GPT-2 base model \citep{radford2019language} and train it from scratch on a generated dataset using the AdamW \citep{adamw}. Key training parameters include a batch size of 128 and 500k training steps. In the PEARL framework, the P-Net is initialized as a BERT-base \citep{devlin-etal-2019-bert} and also trained from scratch. Implementation details are in Appendix~\ref{app:LR_training_details}.

\textbf{Baselines}.
Consistent with \citet{garg2022what}, we adopt an empirical risk minimization method with curriculum learning \citep{bengio2009curriculum, wu2020curricula} \textbf{(ERM+CL)} to train the model. The training process gradually increase the number of demonstrations presented to the model, allowing for progressive learning of more complex patterns and making the training more stable.



\subsection{Evaluation Results}

We evaluate the effect of permutations on the worst-case and average performance of different methods, as well as each method's defence capability against permutation attacks.

\begin{figure*}[!htbp]
\centering
\begin{minipage}[t]{0.50\textwidth}
    \vspace{0pt}
    \centering
    \small
    \captionof{table}{Normalized MSE across permutations.}
    \label{tab:icl-performance}
    \begin{tabular}{l@{\hspace{6pt}}l@{\hspace{10pt}}c@{\hspace{10pt}}c}
    \toprule
    Shot & Method & Avg. & Worst. \\ 
    \midrule
    \multirow{2}{*}{3} & ERM+CL & 1.45 & 2.67 \\
        & PEARL & \greenbold{0.86} \bluegain{40.7} & \greenbold{0.92} \bluegain{65.5} \\
    \addlinespace[2mm]
    \multirow{2}{*}{4} & ERM+CL & 1.20 & 3.34 \\
        & PEARL & \greenbold{0.79} \bluegain{34.1} & \greenbold{1.11} \bluegain{66.8} \\
    \addlinespace[2mm]
    \multirow{2}{*}{5} & ERM+CL & 1.28 & 5.03 \\
        & PEARL & \greenbold{0.87} \bluegain{32.0} & \greenbold{1.33} \bluegain{73.6} \\
    \bottomrule
    \end{tabular}
    \captionsetup{skip=5pt}
\end{minipage}%
\hfill
\begin{minipage}[t]{0.48\textwidth}
    \vspace{0pt}
    \centering
    \includegraphics[width=0.9\linewidth]{fig/LR_attack_success_rate_comparison.pdf}
    \captionsetup{skip=1pt}
    \captionof{figure}{Comparison of attack success rates.}
    \label{fig:ICL_fig}
\end{minipage}
\end{figure*}

As shown in Table~\ref{tab:icl-performance}, the performance gap between average and worst-case performance across permutations for the baseline methods was significant, indicating substantial vulnerability to permutations. Specifically, the worst-case performance of the baseline methods decreased dramatically compared to their average performance, with the relative performance drop increasing from 74.6\% at 3 shots to 84.1\% at 4 shots, effectively losing most of the performance gains achieved by increasing the number of shots. In contrast, our method, PEARL, not only improved the average performance but also significantly enhanced the worst-case generalization performance compared to the baselines. While the average performance gains tend to plateau as the number of shots increases, the worst-case performance gains continue to rise, increasing from 65.5\% at 3 shots to 73.6\% at 5 shots.



Figure~\ref{fig:ICL_fig} depicts the proportion of successfully attacked samples in terms of (1) different attack success thresholds and (2) number of demonstrations (shots). The former considers more pessimistic scenarios (attacked samples drop a large margin), while the latter examines larger input spaces. We observed that PEARL's advantage increased as the threshold grew. At $\delta > 50\%$, the defence success rate for PEARL across all shots was approximately double that of the baseline methods. This indicates that PEARL can effectively prevent pessimistic scenarios (samples attacked with a large threshold). Moreover, PEARL's performance improved with an increasing number of shots, suggesting better scalability compared to baseline methods.


\section{Instruction Fine-Tuning of Large Language Models}


\subsection{Experimental Setups}
\begin{wraptable}{R}{0.4\textwidth} \vspace{-12pt}
\centering
\small
\caption{Summary of datasets.}
\label{tab:datasets}
\setlength{\tabcolsep}{1mm}{
        \begin{adjustbox}{max width=0.35\textwidth}
\begin{tabular}{lccc}
\toprule
\textbf{Split} & \textbf{Category} & \textbf{\# Tasks} & \textbf{\# Samples} \\
\midrule
\multirow{2}{*}{Training} & NLG & 7 & 1050 \\
& NLU & 6 & 900 \\
\midrule
\multirow{2}{*}{Testing} & NLG & 2 & 200 \\
& NLU & 2 & 200 \\
\bottomrule
\end{tabular}
\end{adjustbox}}
\vspace{-0.1cm}
\end{wraptable}
\textbf{Datasets}. Our instruction tuning data are derived from Super-Natural Instructions \citep{wang2022niv2}, which are part of the FLAN v2 benchmark \citep{wei2022flan}. We selected 17 representative tasks, comprising 9 natural language generation (NLG) tasks and 8 natural language understanding (NLU) tasks. Following the methodology of \citet{wang2022niv2}, we randomly designated 4 datasets as held-out test sets and used the remaining 13 datasets for training. 
Each training dataset contains 150 examples, and each test dataset contains 100 examples, resulting in a training set of 1,950 examples and a test set of 400 examples, as summarized in Table~\ref{tab:datasets}.



\textbf{Evaluation Metrics}.
Following the practice in Super-Natural Instructions \citep{mishra2022niv1, wang2022niv2}, we adopt ROUGE-L \citep{lin-2004-rouge} for reporting performance results, due to the diversity of our tasks and the open-ended nature of instruction tuning. We also report a single "average" metric across all datasets, following the methodology in FLAN \citep{wei2022finetuned, wei-etal-2023-symbol}.


\textbf{Baselines and Models}.
We evaluate our framework against several learning algorithms: Empirical Risk Minimization (\textbf{ERM}) \citep{min-etal-2022-metaicl}, ERM with Demonstration Shuffling (\textbf{ERM+DS}) and ERM with Instance Mixup (\textbf{ERM+IM}) \citep{zhang2018mixupempiricalriskminimization}, and \textbf{InfoAC} \citep{xiang-etal-2024-addressing}. We implement FLAN-large as the P-Net and evaluate across five LLMs: \textbf{Llama3-8B}, \textbf{Llama2-7B/13B}, \textbf{Mistral-7B}, and \textbf{Gemma-7B}.
The implementation details are provided in Appendix \ref{app:ICL_setting}.

\subsection{Evaluation Results}
We evaluate PEARL from three perspectives:
(1) comparison with training-stage methods,
(2) generalization to diverse type of LLMs, and
(3) scalability to many-shot in-context learning \citep{agarwal2024manyshotincontextlearning} and long sequences.

\input{tab/ICL_res}

Table~\ref{tab:table-main} presents the comparative performance of PEARL against other learning methods. PEARL consistently improves both average and worst-case performance across all unseen tasks. As the number of shots increases, the worst-case performance gain relative to ERM progressively increases from 14.2\% at two shots to 29.4\% at four shots.
Notably, while optimized for worst-case performance, \textsc{PEARL} also achieves superior average performance with gains of 5.7\% to 9.8\%. This improvement may stem from the rapid convergence observed during Llama-7B's fine-tuning, where the training loss plateaus within one epoch. 
The rapid convergence suggests that for advanced LLMs like LLama3, focusing on challenging permutations during training is more effective than using random ones—an observation consistent with \citet{xu2024wizardlm}.

\begin{figure*}[!htbp]
\setlength{\abovecaptionskip}{5pt}   
\setlength{\belowcaptionskip}{0pt}
    \vspace{-1.2mm}
    \centering
    \includegraphics[width=0.75\textwidth]{fig/scale_models_shots1.pdf}
    \caption{Generalization performance of our method across different types of LLMs and many-shot settings. \textbf{Left}: Performance gains on 3-shot across different LLMs (Mistral-7B, Gemma-7B, Llama 2-7B, and Llama3-8B). \textbf{Right}: Scaling behavior across many-shot settings (8, 16, 32, and 64 shots) and longer sequences (8k tokens) when trained with 5 shots and a sequence length of 512 tokens.}
    \label{fig:manyshot}
    \vspace{-1.2mm}
\end{figure*}

To validate the general applicability of our method, we expanded our experiments to include three additional LLMs: Mistral-7B, Gemma-7B and Llama 2-7B. As shown in the left panel of Figure~\ref{fig:manyshot}, our method consistently improves worst-case performance by more than 10\% in three-shot settings. Additional results for higher-shot settings are provided in Appendix~\ref{app:diverse_llms}. We also observed that different LLM families exhibit varying sensitivity to input permutations, with Llama models being the most sensitive, followed by Gemma and Mistral. Despite these differences, the phenomenon remains significant, with performance drops exceeding 10\% in most cases. Notably, our method achieves consistent worst-case performance improvements of over 10\% for three or more shots, demonstrating its robustness across diverse model families. 



We scale our evaluation to the many-shot ICL setting (up to 64 shots) and longer sequences (up to 8,000 tokens) after training with 5 shots and a sequence length of 512 tokens. As shown in the right panel of Figure~\ref{fig:manyshot}, PEARL achieves substantial worst-case performance gains ranging from 24\% to 40\% when generalizing to larger shot numbers and longer sequences, despite being trained on a smaller setup. These results suggest that PEARL enables LLMs to learn robust features that generalize effectively to both many-shot in-context learning and longer sequences, demonstrating the strong generalization capability of our method. Detailed results are provided in Appendix~\ref{app:many_shot}.



\begin{table}[!htbp]
    \vspace{-0.12cm}
    \centering
    \caption{Shot Efficiency: Average Performance with and without PEARL.}
    \setlength{\tabcolsep}{4mm}{
    \begin{adjustbox}{max width=\textwidth}
    \begin{tabular}{c cccccc}
    \toprule
    \# Shots & 2 & 4 & 8 & 16 & 32 & 64 \\
    \midrule
    ERM   & 57.3  & 59.7  & 61.8  & 66.9  & 67.4  & 68.1  \\
    PEARL & 62.9  & 63.1  & 66.5  & 70.5  & 70.0  & 70.4  \\
    \bottomrule
    \end{tabular}
    \end{adjustbox}}
    \label{tab:sample_efficiency}
    \vspace{-0.12cm}
\end{table}


As we scale to the many-shot setting, we also observe notable trends in \textit{shot efficiency}, which quantifies the number of shots a baseline model would require to match the average performance of a PEARL-trained model. As shown in Table~\ref{tab:sample_efficiency}, PEARL-trained models achieve comparable average performance while requiring \textit{two to four times fewer shots}, highlighting the efficiency of our approach.


\section{Related Work}
\textbf{Order Sensitivity in In-context Learning} 
Despite the huge success of ICL, its robustness to demonstration permutations remains an unresolved challenge 
\citep{pmlr-v139-zhao21c}.

Most \textbf{\textit{training-stage methods}} focus on improving general performance in ICL \citep{min-etal-2022-metaicl, wei-etal-2023-symbol} while neglecting the lack of robustness to the permutations of demonstrations. Recent studies suggest that this phenomenon stems from the autoregressive nature of transformer language models \citep{chen2023positionalinformationmattersinvariant,xiang-etal-2024-addressing}.
InfoAC \citep{xiang-etal-2024-addressing} introduces contrastive learning during fine-tuning to break the autoregressive constraint and enable bidirectional token visibility; however, their approach achieves limited success and is restricted to classification tasks. Preliminary work of \citet{chen2023positionalinformationmattersinvariant} shows the DeepSet architecture exhibits better permutation invariance than transformer; however, this MLP-based new architecture is too small to solve complex language modeling tasks.
\textit{Our approach falls within the category of training-stage methods but proposes a general learning framework that enhances permutation robustness in LLMs without modifying the Transformer architecture or its autoregressive objective, thereby preserving scalability}.

\textbf{\textit{Inference-stage methods}} can be categorized into four types: 
(1) demonstration selection \citep{chang2023datacurationstabilizeincontext, peng-etal-2024-revisiting}, 
which improves normal-case performance but lacks worst-case guarantees under permutations; 
(2) output calibration \citep{pmlr-v139-zhao21c, li-etal-2023-distinguishability, guo-etal-2024-makes}, which are effective for classification but is less applicable to generation tasks due to sequence calibration challenges; (3) order optimization \citep{lu-etal-2022-fantastically}, which finds the best ordering during inference but suffers from exponential complexity; and (4) prediction ensembling: 
A recent work \citep{zhang-etal-2024-batch} transforms n-shot ICL into n one-shot predictions and ensembles results—effective for classification but harms generation.
In summary, inference-stage methods mitigate order sensitivity via pre/post-processing, often introducing additional inference overhead. Moreover, most methods target classification and underperform on generation tasks.
\textit{In contrast, our training-stage solution complements inference-stage methods, enhancing LLM robustness without additional inference costs while remaining broadly applicable to various tasks.}



\noindent \textbf{Distributionally Robust Optimization}.
Distributionally robust optimization optimizes the objective function over ambiguity sets, often defined as balls centered on the empirical distribution \citep{BenTal2013Robust, Lam2015Quantifying, Duchi2016, Miyato2018Virtual}. 
Prior applications of DRO have primarily addressed distributional shifts, including label shift \citep{hu2018does} and data source shift \citep{oren2019drolm} and group shift \citep{2020Distributionally}. 
To the best of our knowledge, we are the first to apply DRO to enhance the ICL robustness of LLMs by defining the ambiguity set over all possible permutations of the empirical distribution, thereby providing performance guarantees.

\noindent \textbf{Optimal Transport}. 
Optimal transport is a fundamental mathematical discipline established by \citet{monge1781memoire,kantorovich1942transfer}. It defines a metric for measuring distances between probability distributions, known as the Wasserstein distance, and has been widely employed in machine learning for distribution matching \citep{montesuma2024recentadvancesoptimaltransport,ot_ds_2024}. 
Our work extends the concept of learning permutation structures through neural networks, as explored in \citep{mena2018learning} for learning to sort numbers or solve jigsaw puzzles. 
However, we apply OT in the context of LLMs and design a neural network, P-Net, equipped with the Sinkhorn operator to generate challenging permutations, enabling LLMs to undergo DRO training and thereby enhancing their ICL robustness.


\section{Conclusion}

We introduce a novel permutation-resilient learning framework, \textsc{Pearl}, designed to enhance the robustness of LLMs against different permutations. \textsc{Pearl} employs a permutation-proposal network, which leverages the Sinkhorn algorithm to generate challenging permutations, optimized under the DRO to systematically improve LLM's robustness.
Through empirical evaluations on both synthetic tasks and real-world instruction-tuning tasks, our framework has demonstrated effectiveness in mitigating the permutation-based attacks and enhancing average performance. 

While \textsc{Pearl} primarily focuses on improving in-context learning, it provides a general framework for handling set-structured inputs with order-independent elements, such as multiple documents, images, or videos. We hope this work inspires further research on permutation-resilient learning, contributing to the development of more robust and trustworthy language models.


\section*{ACKNOWLEDGMENTS}
We sincerely thank the anonymous reviewers for their valuable feedback and constructive suggestions, which have helped improve the quality of this work.
This work is partially supported by Hong Kong RGC GRF No. 14206324, CUHK direct grant No. 4055209, and CUHK Knowledge Transfer Project Fund No. KPF23GWP20.
Li Shen is supported by Shenzhen Basic Research Project (Natural Science Foundation) Basic Research Key Project (NO. JCYJ20241202124430041), and CCF-DiDi GAIA Collaborative Research Funds.

\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}



\appendix

\section*{Appendix}

\section{Detailed Setup of ICL With Linear Functions}

\subsection{DATASETS Construction} \label{app:LR_setting}
We investigate training a language model to perform in-context learning on linear functions, following \citet{garg2022what, guo2024how}. The function class is defined as $\mathcal{F} = \{f \mid f(x) = w^\top x, w \in \mathbb{R}^d\}$, where $d$ is the input dimension. Each data sample is constructed as follows:

(a) Function sampling: A weight vector $w \sim \mathcal{N}(0, I_d)$ is sampled, defining a linear function $f(x) = w^\top x$.

(b) Input sampling: Inputs $x_1, x_2, \ldots, x_{k+1} \sim \mathcal{N}(0, I_d)$ are independently drawn.

(c) Output generation: For each input, the corresponding output is computed as $y_i = f(x_i) = w^\top x_i$ for $i = 1, 2, \ldots, k+1$.

The input prompt $p^i$ consists of $i$ demonstrations and the $(i+1)$-th example as the query: $p^i = (x_1, f(x_1), x_2, f(x_2), ..., x_i, f(x_i), x_{i+1})$. We trained a language model $L_\theta$, parameterized by $\theta$, to minimize the expected loss over all input prompts:
\begin{equation}
\min_\theta \mathbb{E}_{p} \left[\frac{1}{k+1} \sum\nolimits_{i=0}^{k} \ell(\theta; p^i, f(x_{i+1})) \right],
\label{eq:loss}
\end{equation}
where $l(\cdot)$ is the mean squared error (MSE) loss. During testing, we evaluated performance using the same MSE metric. We report the normalized squared error $((LM(p) - w^\top x_{\text{query}})^2/d)$, where $d$ is the problem dimension.

\subsection{Implement Details} \label{app:LR_training_details}
\textbf{Architecture}.
Following \citet{garg2022what}, we implement $L_\theta$ using a GPT-2 architecture \citep{radford2019language} with 12 layers, 8 attention heads, and a hidden dimension of 256. The model takes as input a sequence of vectors in its embedding space and predicts the next vector in the sequence within the same space.\vspace{-1.0mm}

\textbf{Training}.
We pre-train the model from scratch on a generated dataset of 40k linear functions using the AdamW \citep{adamw}. We employ a batch size of 128 and trained for 500k steps, selecting the best checkpoint based on validation set performance. In the PEARL framework, we randomly initialize the P-Net with a BERT-base-sized transformer encoder, also pre-training it from scratch. During testing, we sample novel functions to assess the model's ability to infer new weights $w$ through in-context demonstrations. \vspace{-1.5mm}


\section{Detailed Setup of Instruction Fine-Tuning}\label{app:ICL_setting}
\subsection{Details of Datasets}\label{app:dataset}

The details of datasets used in instruction tuning is presented in Table \ref{tab:dataset_detail}.

\begin{table}[!htbp]
\centering
\caption{Details of datasets used in instruction tuning from natural instructions.}
\label{tab:dataset_detail}
\resizebox{\textwidth}{!}{%
\begin{tabular}{llll}
\toprule
\textbf{Task ID} & \textbf{Task Name} & \textbf{Source} & \textbf{Category} \\
\midrule
\texttt{1297} & QASC Question Answering & QASC & Question Answering \\
\texttt{442} & COM\_QA Paraphrase Question Generation & COM\_QA & Question Rewriting \\
\texttt{908} & DialogRE Identify Familial Relationships & DialogRE & Speaker Relation Classification \\
\texttt{288} & Gigaword Summarization & Gigaword & Title Generation \\
\texttt{582} & Natural Questions Answer Generation & Natural Questions & Question Answering \\
\texttt{151} & TOMQA Find Location Easy Clean & TOM\_QA & Question Answering \\
\texttt{1714} & ConvAI3 Sentence Generation & ClariQ & Dialogue Generation \\
\texttt{379} & AGNews Topic Classification & AG News & Text Categorization \\
\texttt{639} & MultiWOZ User Utterance Generation & MultiWOZ 2.2 & Dialogue Generation \\
\texttt{209} & Stance Detection Classification & StarCon & Stance Detection \\
\texttt{1516} & IMPPRES Natural Language Inference & IMPPRES & Textual Entailment \\
\texttt{589} & Amazon Food Summary Text Generation & Amazon Reviews & Summarization \\
\texttt{1285} & KPA Keypoint Matching & ArgKP & Text Matching \\
\bottomrule
\end{tabular}%
}
\end{table}



\subsection{Baseline and Implementation Details}\label{app:baseline}

To evaluate the performance of our trained model, we compare it with other learning algorithms.


\textbf{Empirical Risk Minimization (ERM)} \citep{min-etal-2022-metaicl}: Standard approach minimizing the average loss over the training dataset, adopted by mainstream instruction tuning models such as FLAN \citep{wei2022flan}, Natural Instructions \citep{mishra2022niv1, wang2022niv2}, and MetaICL \citep{min-etal-2022-metaicl}. 

\textbf{ERM with Demonstration Shuffling (ERM+DS)} \citep{zhang2018mixupempiricalriskminimization}: 
Enhances ERM by randomly shuffling the order of in-context demonstrations within each sample at each training step. This introduces robustness by exposing the model to different permutations of demonstrations during training. It can be considered a form of epoch-level data augmentation.


\textbf{ERM with Instance Mixup (ERM+IM)}\citep{zhang2018mixupempiricalriskminimization}: Incorporates \textit{Instance Mixup} technique during each training step. For each data point, we generate multiple augmented versions by randomly selecting different in-context demonstrations. We perform multiple forward passes to compute the loss for each augmented version, average these losses, and then perform a single backward pass using the averaged loss. This approach provides finer-grained data augmentation compared to demonstration shuffling. Notably, by comparing this baseline with our method, we contrast min-mean optimization (ERM+IM) with min-max optimization (our method).

\textbf{InfoAC}: \citep{xiang-etal-2024-addressing} is a training-stage method that employs contrastive learning to enable earlier tokens to access information from later tokens, amining to mitigate the order sensitivity of ICL inherent in autoregressive LM.



By including these baselines we provide a comprehensive evaluation of our proposed method.

As for the proposed \textsc{Pearl} framework, we select the LLaMA3-8B model as our LLM and the FLAN-large encoder as the P-Net. Both models are fine-tuned using LoRA \citep{hu2022lora}, with the number of finetuned parameters of P-Net being 1/20 that of the LLM. We train the models on the instruction dataset for two epochs using a single NVIDIA A40 GPU, with a batch size of 16, resulting in a total of 246 training steps. The optimizer used was AdamW. The learning rates for the P-Net and the LLM are set to \(1 \times 10^{-4}\) and \(3 \times 10^{-4}\), respectively. For the Sinkhorn algorithm, we use 80 iterations, a temperature parameter of 0.1, and an entropy constraint coefficient \(\beta = 1.0\). 


\subsection{Details of Hyperparameter Settings}\label{app:hyperparameters}
In this section, we provide a comprehensive overview of the hyperparameter settings used in our experiments (Table \ref{tab:hyperparams}). The hyperparameters can be categorized into three groups: (1) basic LLM training parameters, such as learning rate and batch size; (2) LoRA configuration parameters; and (3) P-Net optimization parameters. These hyperparameters were selected based on average validation performance and kept consistent across comparative experiments to ensure fair comparison.

\begin{table}[!htbp]
    \centering
    \begin{tabular}{l>{\raggedright\arraybackslash}p{4.5cm}l} 
        \toprule
        \textbf{Category} & \textbf{Hyperparameter} & \textbf{Value} \\
        \midrule
        \multirow{5}{*}{LLMs} 
            & Learning rate & 3e-5 \\
            & Batch size & 16 \\
            & Max sequence length & 512 \\
            & Weight decay coefficient & 0.1 \\
            & Epoch & 2 \\
        \midrule
        \multirow{5}{*}{LoRA} 
            & Rank & 8 \\
            & Alpha & 32 \\
            & Dropout & 0.1 \\
            & P-Net target modules & q, v \\
            & LLMs target modules & \makecell[l]{q\_proj, k\_proj, v\_proj,\\o\_proj, gate\_proj,\\up\_proj, down\_proj} \\
        \midrule
        \multirow{7}{*}{P-Net} 
            & Temperature & 0.1 \\
            & Iteration coefficient & 80 \\
            & Entropy constraint & 1.0 \\
            & Noise & 0.3 \\
            & Learning rate & 1e-4 \\
            & Batch size & 16 \\
            & Max sequence length & 512 \\
        \bottomrule
    \end{tabular}
    \caption{Hyperparameter settings used in our main experiment.}
    \label{tab:hyperparams}
\end{table}


\section{Analysis of Hyperparameters In Instruction Finetuning} \label{app:hyper_ana}

We conduct analysis to understand the impact of key hyperparameters on P-Net learning and our overall framework. 
Our analysis focuses on two main aspects: the effect of the entropy constraint strength, and the influence of iteration number and temperature in the Sinkhorn algorithm.


\begin{figure*}[t!]
\centering

\begin{minipage}[t]{0.52\textwidth}
    \vspace{0pt}
    \centering
    \caption{Impact of number of iterations and temperature on the average/worst-case performance. } 
    \label{tab:temperature}
    \resizebox{0.9\textwidth}{!}{%
    \begin{tabular}{cccc}
    \toprule
    \multirow{2}{*}{\# Iter.} & \multicolumn{3}{c}{Temperature} \\
    \cmidrule(lr){2-4}
     & 0.03 & 0.1 & 0.3 \\
    \midrule
    80 & 55.7 / 40.0 & 55.7 / 40.0 & 55.4 / 39.6 \\
    200 & 55.7 / 40.0 & 55.8 / 40.0 & 55.8 / 40.6 \\
    \bottomrule
    \end{tabular}%
    }
    \captionsetup{skip=5pt}
\end{minipage}%
\hfill
\begin{minipage}[t]{0.45\textwidth}
    \vspace{0pt}
    \centering
    \includegraphics[width=0.93\columnwidth,height=2.8cm,keepaspectratio]{fig/ablation.pdf} 
    \caption{Impact of entropy coefficient. 
    }
    \label{fig:ablation_beta}
\end{minipage}
\vspace{-0.1cm}
\end{figure*}


\textbf{Influence of Entropy Regularization in OT} ~ We examine the impact of the entropy regularization coefficient in OT, testing values of 0.3, 1.0, 3.0, and 10.0 (Figure \ref{fig:ablation_beta}). At a low coefficient (0.3), P-Net's gradient norm remained small, indicating minimal learning and potential generation of simple semantic overlaps to satisfy adversarial training requirements. Concurrently, the LLM's gradient norm struggled to decrease. The gradient norm for P-Net peaked at 1.0, suggesting optimal learning conditions. As coefficients increased to 3.0 and 10.0, P-Net's gradient norm decreased again, suggesting excessive restrictions. The range of 1.0-3.0 provided an ideal balance, encouraging P-Net to extract meaningful information from the LLM without oversimplifying or overcomplicating the task. In contrast, the LLM's gradient norm decreased consistently with increasing coefficients, indicating a distinct response to entropy regularization.


\textbf{Effect of Sinkhorn Algorithm Parameters} 
We investigate the interplay between two critical parameters in the Sinkhorn algorithm: number of iterations and temperature. Intuitively, these parameters are positively correlated; higher iteration counts typically allow for higher temperatures. Our experiments, however, reveal an unexpected robustness to parameter variations. With the entropy regularization coefficient fixed at 1, we vary the number of iterations $(80, 200)$ and temperature $(0.03, 0.1, 0.3)$. As presented in Table \ref{tab:temperature}, surprisingly, these substantial parameter changes result in minimal performance variation. This suggests that the Sinkhorn algorithm in our framework is less sensitive to these parameters than initially hypothesized, potentially indicating a wider range of stable configurations for practical applications.


\section{Extended Instruction Finetuning Across Diverse LLMs} \label{app:diverse_llms}
We expanded our evaluation to more LLMs: Mistral-7B, Gemma-7B, and earlier generations such as Llama2-7B and Llama2-13B, as detailed in the tables from Table (\ref{tab:table-mistral}) to Table (\ref{tab:table-llama2_13b}).

\input{tab/ICL_res_mistral}

\input{tab/ICL_res_gemma}

\input{tab/ICL_res_llama2_7b}

\input{tab/ICL_res_llama2_13b}

\paragraph{Sensitivity to Permutations Across LLM Families}
Our analysis reveals that different LLM families exhibit varying degrees of sensitivity to permutations. The sensitivity ranking, from highest to lowest, is as follows: Llama, Gemma, and Mistral. Notably, all examined families showed significant performance declines, typically exceeding 10 \%.

\paragraph{Adaptation of the Proposed Method}
In scenarios with three or more examples, our method consistently demonstrated substantial improvements, often enhancing worst-case performance by more than 10\%. These results confirm the robustness and effectiveness of our approach.

\section{Scaling to Many-Shot In-Context Learning} \label{app:many_shot}
We evaluate the scalability of PEARL by extending our analysis to many-shot scenarios, testing performance with 8 to 64 in-context examples (Table \ref{tab:table-manyshot}). Notably, despite being trained solely on 5-shot demonstrations, PEARL exhibits strong generalization to settings with substantially more examples. Using Llama3-8B as our base model, we compare PEARL and ERM training approaches across four held-out tasks. Our analysis reveals persistent performance advantages of PEARL over the ERM baseline across all shot regimes.

\input{tab/ICL_res_large_shot}

\section{Best-case Performance} 
Although our methodology was initially designed to optimize for pessimistic (worst-case) scenarios, we have also included an evaluation of the best-case performance for both PEARL and ERM to provide a balanced perspective. The results are shown in the Table \ref{tab:best}.
\input{tab/ICL_best}

Surprisingly, the results show that across all datasets and in every shot condition, PEARL's best performance consistently exceeded that of ERM. This indicates that our method not only optimizes performance in worst-case scenarios but also slightly enhances best-case performance.



\end{document}
