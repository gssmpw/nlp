\section{Related Work}
\textbf{Order Sensitivity in In-context Learning} 
Despite the huge success of ICL, its robustness to demonstration permutations remains an unresolved challenge 
____.

Most \textbf{\textit{training-stage methods}} focus on improving general performance in ICL ____ while neglecting the lack of robustness to the permutations of demonstrations. Recent studies suggest that this phenomenon stems from the autoregressive nature of transformer language models ____.
InfoAC ____ introduces contrastive learning during fine-tuning to break the autoregressive constraint and enable bidirectional token visibility; however, their approach achieves limited success and is restricted to classification tasks. Preliminary work of ____ shows the DeepSet architecture exhibits better permutation invariance than transformer; however, this MLP-based new architecture is too small to solve complex language modeling tasks.
\textit{Our approach falls within the category of training-stage methods but proposes a general learning framework that enhances permutation robustness in LLMs without modifying the Transformer architecture or its autoregressive objective, thereby preserving scalability}.

\textbf{\textit{Inference-stage methods}} can be categorized into four types: 
(1) demonstration selection ____, 
which improves normal-case performance but lacks worst-case guarantees under permutations; 
(2) output calibration ____, which are effective for classification but is less applicable to generation tasks due to sequence calibration challenges; (3) order optimization ____, which finds the best ordering during inference but suffers from exponential complexity; and (4) prediction ensembling: 
A recent work ____ transforms n-shot ICL into n one-shot predictions and ensembles resultsâ€”effective for classification but harms generation.
In summary, inference-stage methods mitigate order sensitivity via pre/post-processing, often introducing additional inference overhead. Moreover, most methods target classification and underperform on generation tasks.
\textit{In contrast, our training-stage solution complements inference-stage methods, enhancing LLM robustness without additional inference costs while remaining broadly applicable to various tasks.}



\noindent \textbf{Distributionally Robust Optimization}.
Distributionally robust optimization optimizes the objective function over ambiguity sets, often defined as balls centered on the empirical distribution ____. 
Prior applications of DRO have primarily addressed distributional shifts, including label shift ____ and data source shift ____ and group shift ____. 
To the best of our knowledge, we are the first to apply DRO to enhance the ICL robustness of LLMs by defining the ambiguity set over all possible permutations of the empirical distribution, thereby providing performance guarantees.

\noindent \textbf{Optimal Transport}. 
Optimal transport is a fundamental mathematical discipline established by ____. It defines a metric for measuring distances between probability distributions, known as the Wasserstein distance, and has been widely employed in machine learning for distribution matching ____. 
Our work extends the concept of learning permutation structures through neural networks, as explored in ____ for learning to sort numbers or solve jigsaw puzzles. 
However, we apply OT in the context of LLMs and design a neural network, P-Net, equipped with the Sinkhorn operator to generate challenging permutations, enabling LLMs to undergo DRO training and thereby enhancing their ICL robustness.