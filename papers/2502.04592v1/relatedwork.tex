\section{Related Work}
\subsection{Event-Drive Financial Forecasting} 
Event-driven financial forecasting \cite{BAO2025102616} focuses on predicting asset prices \cite{RePEc:snb:snbwpa:2013-11,Gilbert2010-dt} and market volatility \cite{https://doi.org/10.1111/jofi.12196,https://doi.org/10.1111/jofi.12818} based on events like macroeconomic releases \cite{Gilbert2010-dt}, news \cite{RePEc:arx:papers:1811.06173}, corporate announcements \cite{zhou-etal-2021-trade}, and social media activity \cite{xu-cohen-2018-stock}. Three main approaches exist in this area. The first leverages text analysis to predict asset responses based on event-related text. Early works utilized TF-IDF \cite{1196287,LI2014826} and topic models \cite{si-etal-2013-exploiting,NGUYEN20159603}, progressing to RNN-based models \cite{10.1145/3155133.3155202,liu2018leveragingfinancialnewsstock} and pre-trained transformers \cite{zhou-etal-2021-trade,shah-etal-2023-trillion}, which capture nuanced semantics. Although these models excel at semantic extraction, they often lack integration with historical price data, crucial for holistic forecasting.

The second line of approaches uses statistical and sequential models on numerical data, such as linear regression \cite{8212716}, ARIMA \cite{7046047}, and GARCH \cite{HYUPROH2007916}. Later, deep learning methods like RNNs \cite{liu2018leveragingfinancialnewsstock} and CNNs \cite{8126078,Durairaj2022} enhanced nonlinear modeling capabilities. More recently, transformer-based models, such as Informer \cite{Zhou_Zhang_Peng_Zhang_Li_Xiong_Zhang_2021} and FedFormer \cite{pmlr-v162-zhou22g}, improved long-range dependency modeling for time series data. However, these models tend to be ``case-specific,'' requiring task-specific training. In contrast, the lastest pre-trained models for time-series data, like MOMENT \cite{goswami2024moment}, Timer \cite{liu2024timer}, and TOKEN \cite{anonymous2024totem}, offer more generalized and adaptable solutions for time-series tasks.

% The second line of traditional techniques primarily relies on numerical features, utilizing statistical models such as linear regression \cite{8212716}, ARIMA \cite{7046047}, and GARCH \cite{HYUPROH2007916}. Compared to these statistical models, deep learning methods demonstrate superior nonlinear modeling capabilities. Several approaches \cite{10.1145/3155133.3155202,liu2018leveragingfinancialnewsstock,doi:10.1080/14697688.2019.1622314,xu-cohen-2018-stock} have developed sequential neural networks based on RNN models, leveraging their unique architecture to capture inter-time dependencies. Additionally, researchers \cite{8126078,HOSEINZADE2019273,Durairaj2022} have employed CNN networks to effectively identify complex cyclical patterns and trends for time-series forecasting. However, these earlier methods suffer from the challenge of long-range sequence dependencies. To address this, the attention mechanism in transformer neural networks offers a potential solution by allowing the model to compute dependencies between different positions in a sequence in parallel. Recent models such as Informer \cite{Zhou_Zhang_Peng_Zhang_Li_Xiong_Zhang_2021}, FedFormer \cite{pmlr-v162-zhou22g}, and AutoFormer \cite{Chen_2021_ICCV} have demonstrated improved performance in this regard. Nevertheless, these transformer-based models are typically ``case-based'' approaches, meaning they require training for each specific task and dataset. In contrast, the latest pre-trained time-series models, such as MOMENT \cite{goswami2024moment}, Timer \cite{liu2024timer}, and TOKEN \cite{anonymous2024totem}, offer more effective universal models for time-series tasks.


The third line of research adopts multi-modality approaches, combining diverse data types to improve forecasting accuracy. Some studies incorporate text and audio \cite{qin-yang-2019-say,10.1145/3366423.3380128} but often overlook time-series dependencies. Recent work has integrated time-series and textual data; for example, \cite{10.1145/3394171.3413752,sawhney-etal-2020-deep} employed SVM and GRU models to capture time-series features. However, these models are relatively shallow for extracting complex patterns. More recent studies \cite{lee2024moat, Jia_Wang_Zheng_Cao_Liu_2024} leverage transformer-based models for time-series analysis, better capturing deeper temporal structures. Building on these advancements, this paper aims to utilize state-of-the-art pre-trained models with enhanced feature fusion and causal learning for multi-modality forecasting.

% The third line of research approaches the problem from a multi-modality perspective, leveraging multi-dimensional features for financial forecasting. Existing works in this direction combine various modalities to enhance predictive performance. For example, \cite{qin-yang-2019-say} integrates text and audio features to better capture sentiment during earnings conference calls, while \cite{10.1145/3366423.3380128} uses transformer networks to fuse audio and textual data for more robust predictions. Additionally, \cite{10.1145/3394171.3413752} incorporates past historical patterns alongside audio and textual features. However, these methods overlook the time-series modality, where past numerical trends significantly impact current market movements. \cite{10.1145/3503161.3548380} adds video features in addition to audio and text. However, these multi-modality models for event-driven forecasting fail to integrate time-series data, missing key dependencies between historical and current prices. \cite{sawhney-etal-2020-deep} considers time-series and event text features, however, their encoding models are relatively shallow, limiting their ability to extract deep semantic knowledge. \cite{lee2024moat} introduces a multi-modality forecasting model based on a pre-trained transformer architecture for textual and time-series data, but it fails to effectively merge these two modalities for superior performance. Another related work \cite{Jia_Wang_Zheng_Cao_Liu_2024} utilizes a simple linear embedding model for time-series data, which is insufficient for capturing complex patterns. Based on the advancements in textual and time-series multi-modality forecasting \cite{lee2024moat,Jia_Wang_Zheng_Cao_Liu_2024}, this paper aims to leverage the latest pre-trained time-series and language models, along with an effective feature fusion mechanism and causal learning strategy, to develop a state-of-the-art multi-modality forecasting approach.

\subsection{Salient Macroeconomic Factors}

\textbf{Which macroeconomic announcements have a greater impact on financial markets than others?} This question has been widely studied in the financial literature, with Central Bank Communications standing out as the most-researched factor \cite{https://doi.org/10.1111/jofi.12818,GERTLER2018336,https://doi.org/10.1111/joes.12550,RePEc:ijc:ijcjou:y:2016:q:4:a:6,TADLE2022106021,RePEc:fip:fednep:00004,https://doi.org/10.1111/jofi.12196,ROSA2011915}. Beyond central bank communications, various other macroeconomic factors have also been identified as significant drivers of market movements. Among these, Non-farm Payrolls, Unemployment Releases, Initial Unemployment Claims, ISM Manufacturing Index, GDP Advance Releases, Consumer Confidence Index, and Producer Price Index (PPI) Reports have been found to notably influence price movements and market volatility through empirical statistical testings \cite{NBERw1296,Gilbert2010-dt,GILBERT201778,RePEc:fip:fednci:y:2008:i:aug:n:v.14no.6,9fdfa12d09ae4792a11e8360a71a356a,RePEc:snb:snbwpa:2013-11}. In this paper, we aim to leaverage the most significant factors evidented by the past financial literautre \cite{NBERw1296,Gilbert2010-dt,GILBERT201778,RePEc:fip:fednci:y:2008:i:aug:n:v.14no.6,9fdfa12d09ae4792a11e8360a71a356a,RePEc:snb:snbwpa:2013-11}, which include FOMC Meeting Documents, Non-farm Payrolls, Unemployment Releases, Initial Unemployment Claims, ISM Manufacturing Index, GDP Advance Releases, Consumer Confidence Index, and Producer Price Index (PPI) Reports.

\subsection{Counterfactual Data Augmentation by LLMs} 

Counterfactual Data Augmentation seeks to reduce spurious correlations and enhance model robustness. \citet{Kaushik2020Learning} introduced a method that augments training data with counterfactuals written by human annotators, effectively helping to mitigate spurious patterns. \citet{wu-etal-2021-polyjuice, ross-etal-2022-tailor} later proposed the use of hand-crafted templates and trained text generators to create counterfactual data through predefined perturbation types. However, these methods are limited by their reliance on fixed perturbations. More recently, \citet{chen-etal-2023-disco, wang-etal-2023-self-instruct} proposed more flexible, LLM-based approaches that leverage specifically designed in-context learning prompts and generation pipelines for counterfactual and instruction data generation. Following this direction, we present a counterfactual generation framework specifically designed for macroeconomic releases.






\begin{table*}[t]
\caption{Summary of Macroeconomic and Time-Series Data Types, Characteristics and Sources}
\label{tab:data_summary}
\begin{tabular}{@{}cc@{}}
\begin{minipage}{\columnwidth} % Adjusted width to equalize both tables
\centering
\caption*{A. Macroeconomic Event Summary}
\label{tab:data_summary_A}
\resizebox{\columnwidth}{!}{
\begin{tabular}{@{}lcccccl@{}}
\toprule
\textbf{Event Type}                    & \textbf{Data Type} & \textbf{Frequency} & \textbf{Period}      & \textbf{No. of Events} & \textbf{No. of C.F.s} & \textbf{Source}        \\ \midrule
\textbf{FOMC}                          & Html               & Quarterly          & 1993.3 $\sim$2024.6  & 255                    & 2,550                 & \makecell[l]{www.federal\\reserve.gov}  \\
\makecell[l]{\textbf{Unemployment} \\ \textbf{Insurance Claims}} & PDF, Txt & Weekly  & 2002.10 $\sim$2024.6 & 913 & 9,130 & oui.doleta.gov \\
\textbf{Employment Situation}          & Html, Txt          & Monthly            & 1994.2 $\sim$2024.6  & 363                    & 3,630                 & www.bls.gov            \\
\textbf{GDP Advance Report}            & Html               & Monthly            & 1996.8 $\sim$2024.6  & 333                    & 3,330                 & www.bea.gov            \\
\textbf{CPI Report}                    & Html, Txt          & Monthly            & 1994.2 $\sim$2024.6  & 357                    & 3,570                 & www.bls.gov            \\
\textbf{PPI Report}                    & Html, Txt          & Monthly            & 1994.2 $\sim$2024.6  & 348                    & 3,480                 & www.bls.gov            \\ \bottomrule
\end{tabular}
}
\end{minipage}

& % Separator for the two tables

\begin{minipage}{\columnwidth} % Same width for consistency
\centering
\caption*{B. Time-Series Data Summary}
\label{tab:data_summary_B}
\resizebox{\columnwidth}{!}{
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Time Series Data}                & \textbf{Data Types}    & \textbf{Frequency} & \textbf{Range}        & \textbf{No. of Data Points} \\ \midrule
\textbf{SP500 (SPX)}             & Open, Close, High, Low & 5 Min              & 2008.01 $\sim$2024.06 & 331,257                     \\
\textbf{Dow Industrial (INDU)}    & Open, Close, High, Low & 5 Min              & 2012.07 $\sim$2024.06 & 263,445                     \\
\textbf{NASDAQ (NDX)}    & Open, Close, High, Low & 5 Min              & 2008.01 $\sim$2024.06 & 332,616                     \\
\makecell[l]{\textbf{US Treasury Bond} \\ \textbf{at 1-Month (USGG1M)}}    & Open, Close, High, Low & 5 Min              & 2013.01 $\sim$2024.06 & 751,443                     \\
\makecell[l]{\textbf{US Treasury Bond} \\ \textbf{at 5-Year (USGG5YR)}}    & Open, Close, High, Low & 5 Min              & 2013.01 $\sim$2024.06 & 734,773                     \\
\bottomrule
\end{tabular}
}
\end{minipage}
\end{tabular}
\end{table*}