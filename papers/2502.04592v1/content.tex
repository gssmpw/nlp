%%
%% This is file `sample-sigconf-authordraft.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `all,proceedings,bibtex,authordraft')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-sigconf-authordraft.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%%
%% The first command in your LaTeX source must be the \documentclass
%% command.
%%
%% For submission and review of your manuscript please change the
%% command to \documentclass[manuscript, screen, review]{acmart}.
%%
%% When submitting camera ready or to TAPS, please change the command
%% to \documentclass[sigconf]{acmart} or whichever template is required
%% for your publication.
%%
%%
\documentclass[sigconf]{acmart}

\usepackage{makecell}
\usepackage{fancybox}
\usepackage{framed}
\usepackage{tcolorbox}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{tikz}
\usepackage{graphicx} 
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{subcaption}  % For subfigures
%%
\usepackage[normalem]{ulem}
\usepackage{pifont}
\useunder{\uline}{\ul}{}
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmlicensed}
\copyrightyear{2018}
\acmYear{2018}
\acmDOI{XXXXXXX.XXXXXXX}

%% These commands are for a PROCEEDINGS abstract or paper.
\acmConference[Conference acronym 'XX]{Make sure to enter the correct
  conference title from your rights confirmation emai}{June 03--05,
  2018}{Woodstock, NY}
%%
%%  Uncomment \acmBooktitle if the title of the proceedings is different
%%  from ``Proceedings of ...''!
%%
%%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%%  June 03--05, 2018, Woodstock, NY}
\acmISBN{978-1-4503-XXXX-X/18/06}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}


%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.\

% \title{Multi-Model Financial Forecasting Considering Time Series Pattern and Salient Policy Textual Information}

\title{CAMEF: Causal-Augmented Multi-Modality Event-Driven Financial Forecasting by Integrating Time Series Patterns and Salient Macroeconomic Announcements}

%% Event-Driven Financial Forecasting with Causal Data Augmentation: A Multi-Modality Approach Using Time Series and Macroeconomic Announcements


%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.


% \author{Anonymous Authors}

% % \authornote{Both authors contributed equally to this research.}
% % \email{trovato@corporation.com}
% % \orcid{1234-5678-9012}

\author{Yang Zhang\textsuperscript{\rm 1}, Wenbo Yang\textsuperscript{\rm 1}, Jun Wang\textsuperscript{\rm 1},  Qiang Ma\textsuperscript{\rm 2}, Jie Xiong\textsuperscript{\rm 1}}
\affiliation{%
  \institution{\textsuperscript{\rm 1}Southwestern University of Finance and Economics}
  \city{Chengdu}
  \country{China}}
\affiliation{%
  \institution{\textsuperscript{\rm 2}Kyoto Institute of Technology}
  \city{Kyoto}
  \country{Japan}}

\email{zhang.yang.r54@kyoto-u.jp}  % Email for the first author

% \author{Yang Zhang}
% \authornotemark[1]
% \affiliation{%
%   \institution{Southwestern University of Finance and Economics}
%   \city{Chengdu}
%   \country{China}}
% \email{zhang.yang.r54@kyoto-u.jp}

% \author{Wenbo Yang}
% \affiliation{%
%   \institution{Southwestern University of Finance and Economics}
%   \city{Chengdu}
%   \country{China}
%   }

% \author{Jun Wang}
% \affiliation{%
%   \institution{Southwestern University of Finance and Economics}
%   \city{Chengdu}
%   \country{China}
%   }

% \author{Qiang Ma}
% \affiliation{%
%   \institution{Kyoto Institute of Technology}
%   \city{Kyoto}
%   \country{Japan}
%   }

% \author{Jie Xiong}
% \affiliation{%
%   \institution{Southwestern University of Finance and Economics}
%   \city{Chengdu}
%   \country{China}
%   }



% \author{Yang Zhang}
% \authornotemark[1]
% \email{zhang.yang.r54@kyoto-u.jp}

% \author{Wenbo Yang}
% \authornotemark[1]

% \author{Jun Wang}
% \authornotemark[1]

% \author{Qiang Ma}
% \authornotemark[2]

% \author{Jie Xiong}
% \authornotemark[1]

% % \affiliation{%
% %   \institution{The Th{\o}rv{\"a}ld Group}
% %   \city{Hekla}
% %   \country{Iceland}}
% % \email{larst@affiliation.org}

% % \author{Valerie B\'eranger}
% % \affiliation{%
% %   \institution{Inria Paris-Rocquencourt}
% %   \city{Rocquencourt}
% %   \country{France}
% % }

% \author{Third Author}
% % \affiliation{%
% %  \institution{Rajiv Gandhi University}
% %  \city{Doimukh}
% %  \state{Arunachal Pradesh}
% %  \country{India}}

% \author{Forth Author}
% % \affiliation{%
% %   \institution{Tsinghua University}
% %   \city{Haidian Qu}
% %   \state{Beijing Shi}
% %   \country{China}}

% \author{Fifth Author}
% % \affiliation{%
% %   \institution{Palmer Research Laboratories}
% %   \city{San Antonio}
% %   \state{Texas}
% %   \country{USA}}
% % \email{cpalmer@prl.com}

% \author{Sixth Author}
% % \affiliation{%
% %   \institution{The Th{\o}rv{\"a}ld Group}
% %   \city{Hekla}
% %   \country{Iceland}}
% % \email{jsmith@affiliation.org}

% \author{Seventh Author}
% % \affiliation{%
% %   \institution{The Kumquat Consortium}
% %   \city{New York}
% %   \country{USA}}
% % \email{jpkumquat@consortium.net}
% \affiliation{%
%   \institution{Southwestern University of Finance and Economics}
%   \city{Chengdu}
%   % \state{Ohio}
%   \country{China}
% }

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Trovato et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}

Accurately forecasting the impact of macroeconomic events is critical for investors and policymakers. Salient events like monetary policy decisions and employment reports often trigger market movements by shaping expectations of economic growth and risk, thereby establishing causal relationships between events and market behavior. Existing forecasting methods typically focus either on textual analysis or time-series modeling, but fail to capture the multi-modal nature of financial markets and the causal relationship between events and price movements. To address these gaps, we propose \textbf{CAMEF} (Causal-Augmented Multi-Modality Event-Driven Financial Forecasting), a multi-modality framework that effectively integrates textual and time-series data with a causal learning mechanism and an LLM-based counterfactual event augmentation technique for causal-enhanced financial forecasting. Our contributions include: (1) a multi-modal framework that captures causal relationships between policy texts and historical price data; (2) a new financial dataset with six types of macroeconomic releases from 2008 to April 2024, and high-frequency real trading data for five key U.S. financial assets; and (3) an LLM-based counterfactual event augmentation strategy. We compare CAMEF to state-of-the-art transformer-based time-series and multi-modal baselines, and perform ablation studies to validate the effectiveness of the causal learning mechanism and event types. \footnote{\textbf{Open-Source Promise:} We plan to release the CAMEF dataset and model to the research community to promote reproducibility and stimulate future research in this interdisciplinary domain upon publication.}

\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10010405.10010455.10010460</concept_id>
       <concept_desc>Applied computing~Economics</concept_desc>
       <concept_significance>300</concept_significance>
       </concept>
   <concept>
       <concept_id>10010147.10010257.10010293.10010294</concept_id>
       <concept_desc>Computing methodologies~Neural networks</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[300]{Applied computing~Economics}
\ccsdesc[500]{Computing methodologies~Neural networks}

% \ccsdesc[500]{Do Not Use This Code~Generate the Correct Terms for Your Paper}
% \ccsdesc[300]{Do Not Use This Code~Generate the Correct Terms for Your Paper}
% \ccsdesc{Do Not Use This Code~Generate the Correct Terms for Your Paper}
% \ccsdesc[100]{Do Not Use This Code~Generate the Correct Terms for Your Paper}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Do, Not, Us, This, Code, Put, the, Correct, Terms, for,
  Your, Paper}
%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans the
%% page.


\received{20 February 2007}
\received[revised]{12 March 2009}
\received[accepted]{5 June 2009}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}

% Fama's empirical work on the Efficient Market Hypothesis (EMH) provides evidence that financial markets are informationally efficient \cite{1731c543-0a2e-3dd1-85c2-3ffc09a485a7}, meaning asset prices incorporate all available market information. 


The prices of financial assets reflect all available information, according to Fama's Efficient Market Theory \cite{1731c543-0a2e-3dd1-85c2-3ffc09a485a7,78bd1d1d-3f88-3c53-ad25-c8ac73767958}. Major financial releases from government sectors often trigger market movements by shaping investors' expectations and evaluations of economic conditions, asset growth potential, and associated risks. For example, during the FOMC meeting on March 16, 2020, the Fed’s emergency rate cut to 0-0.25\% sharply altered investors' economic outlook, resulting in a massive sell-off. Major indices, including the S\&P 500, NASDAQ, and Dow Jones, dropped by over 10\%, marking the steepest single-day decline since 1987 \cite{cnbc2021}. These salient macroeconomic events cause reactions in financial assets, establishing causal relationships between events and financial assets. Figure \ref{fig:multi-model-demo} illustrates multiple types of events that cause financial market reactions. Therefore, \textbf{accurately forecasting the causal consequences of the salient macroeconomic releases on financial market is essential, not only to help investors manage risks and maximize returns, but also to provide policymakers with valuable insights for evaluating and refining future policies.}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/multi-model-demo.drawio.png}
    \caption{Event-Driven Forecasting Examples: (a) Market reaction to employment insurance release; (b) Market reaction during FOMC meeting; (c) Forecasting market reactions to future events.}
    \label{fig:multi-model-demo}
\end{figure*}


Previous studies on event-driven forecasting have primarily adopted three lines of methodologies. The first line of approaches utilizes text feature-based models, where language models, ranging from self-crafted RNN-based architectures \cite{10.1145/3155133.3155202,liu2018leveragingfinancialnewsstock,doi:10.1080/14697688.2019.1622314,xu-cohen-2018-stock} to pre-trained transformers \cite{zhou-etal-2021-trade,shah-etal-2023-trillion}, embed sentiment information into text vectors, and then stock movements are predicted as a binary classification task (e.g., hawkish vs. dovish). The second line of methodology focuses on historical time-series data, treating stock price movements as a regression problem \cite{RePEc:wsi:wsbook:6578,https://doi.org/10.1002/jae.3950070512}. Recently, transformer-based architectures have been applied for time-series prediction, including Informer \cite{Zhou_Zhang_Peng_Zhang_Li_Xiong_Zhang_2021}, FedFormer \cite{pmlr-v162-zhou22g}, and AutoFormer \cite{Chen_2021_ICCV}, etc. However, both of these directions typically focus on a single modality, neglecting multi-dimensional information. The third line of research adopts a multi-modality approach, leveraging multiple types of data sources to enhance forecasting performance. For instance, studies like \cite{10.1145/3503161.3548380,ouyang-etal-2024-modal} incorporate textual, video, and audio data from FOMC meetings alongside corresponding market movements. While these approaches show promise for event-driven financial forecasts, they face three major limitations:

\begin{itemize}
    \item \textbf{Data Limitation:} Existing approaches predominantly focus on a single type of event, such as FOMC meetings \cite{shah-etal-2023-trillion,10.1145/3503161.3548380,ouyang-etal-2024-modal}, while neglecting other crucial macroeconomic events like unemployment insurance releases, CPI, PPI, and GDP advance reports. Additionally, many studies rely on daily-based time-series data for financial assets \cite{shah-etal-2023-trillion,10.1145/3503161.3548380,ouyang-etal-2024-modal,RePEc:wsi:wsbook:6578,https://doi.org/10.1002/jae.3950070512,Zhou_Zhang_Peng_Zhang_Li_Xiong_Zhang_2021,pmlr-v162-zhou22g,Chen_2021_ICCV}, which limits their applicability and precision in real-time trading scenarios where high-frequency data is mostly adopted.

    \item \textbf{Modality Limitation:} Most prior studies rely on single-modality analysis, using either textual models \cite{10.1145/3155133.3155202,liu2018leveragingfinancialnewsstock,doi:10.1080/14697688.2019.1622314,xu-cohen-2018-stock,zhou-etal-2021-trade,shah-etal-2023-trillion} or time-series models \cite{RePEc:wsi:wsbook:6578,https://doi.org/10.1002/jae.3950070512,Zhou_Zhang_Peng_Zhang_Li_Xiong_Zhang_2021,pmlr-v162-zhou22g,Chen_2021_ICCV}, which fail to integrate the complementary strengths of both modalities. While some multi-modality approaches have been proposed \cite{10.1145/3503161.3548380,ouyang-etal-2024-modal}, they often lack advanced mechanisms for feature fusion, effective decoding strategies, and causal learning, which are critical for understanding the complex interplay between event texts and market dynamics.

    \item \textbf{Causality Limitation:} Existing methods \cite{10.1145/3503161.3548380,ouyang-etal-2024-modal} fail to incorporate causal reasoning frameworks, overlooking the causal relationships between events and market reactions. Without explicitly modeling these relationships, such approaches cannot fully capture the drivers of financial market behavior, limiting their predictive robustness.
\end{itemize}



% \begin{figure*}[t]
%     \centering
%     \includegraphics[width=0.7\linewidth]{figures/Counterfactual_Generation.drawio.png}
%     \caption{Overview of the Counterfactual Event Generation and Learning Process via Counterfactual Event \textcolor{red}{[changed the figure to be purely examples]}}
%     \label{fig:counterfactual_pipeline}
% \end{figure*}

To address the limitations of previous studies, we propose a novel multi-modality framework, \textbf{CAMEF} (\textbf{c}ausal-\textbf{A}ugmented \textbf{M}ulti-Modality \textbf{E}vent-Driven Financial \textbf{F}orecasting). CAMEF integrates time-series and textual features through specially designed multi-feature fusion techniques, time-series decoding mechanisms, and causal learning strategies. By conducting a thorough review of financial literature, we identify six types of salient macroeconomic events for the forecasting analysis. Furthermore, the framework employs causal data augmentation powered by Large Language Models (LLMs) and a causal contrastive learning approach to enhance the causal understanding and forecast accuracy of CAMEF. This paper offers three key contributions:

\begin{itemize}

    \item \textbf{Novel Dataset:} We introduce a novel open-source synthetic dataset comprising 6 types of macroeconomic event scripts (ref to Tab. \ref{tab:data_summary} for details) from 2008 to April 2024 through reviewing from financial literature \cite{https://doi.org/10.1111/jofi.12818,GERTLER2018336,https://doi.org/10.1111/joes.12550,RePEc:ijc:ijcjou:y:2016:q:4:a:6,TADLE2022106021,RePEc:fip:fednep:00004,https://doi.org/10.1111/jofi.12196,ROSA2011915,NBERw1296,Gilbert2010-dt,GILBERT201778,RePEc:fip:fednci:y:2008:i:aug:n:v.14no.6,9fdfa12d09ae4792a11e8360a71a356a,RePEc:snb:snbwpa:2013-11}, alongside intra-day \textunderscore{high-frequency} financial data at 5-minute intervals from key U.S. stock indexes and Treasury bonds. To support causal learning, the dataset also includes counterfactual event scripts generated using our LLM-based  causal argumentation prompting, making it the first to integrate policy texts, high-frequency trading data, and causally augmented content.

    \item \textbf{Novel Multi-Modality Model:} We propose a novel multi-modality approach, CAMEF, that integrates time-series and textual features, incorporating specifically designed multi-feature fusion  and time-series decoding networks, which have been demonstrated to be effective for forecasting. Additionally, the model includes a causal learning mechanism to enhance forecasting capability by capturing the causal relationships between events and market reactions. \textit{The complete CAMEF dataset and model code will be made publicly available on GitHub to ensure reproducibility and facilitate future research upon publication.}

    \item \textbf{Counterfactual Generation and Learning:} We introduce a counterfactual data augmentation strategy to generate counterfactual event scripts based on collected macroeconomic releases. This approach leverages LLMs to create scripts with varying sentiment levels by modifying key numerical values and sentiment-relevant phrases, while preserving the original format, writing style, and neutral words of the factual reports. Counterfactual events enable CAMEF to better understand the causal relationships between events and market reactions by learning from hypothetical scenarios, thereby improving its forecasting ability.

\end{itemize}


\section{Related Work}
\subsection{Event-Drive Financial Forecasting} 
Event-driven financial forecasting \cite{BAO2025102616} focuses on predicting asset prices \cite{RePEc:snb:snbwpa:2013-11,Gilbert2010-dt} and market volatility \cite{https://doi.org/10.1111/jofi.12196,https://doi.org/10.1111/jofi.12818} based on events like macroeconomic releases \cite{Gilbert2010-dt}, news \cite{RePEc:arx:papers:1811.06173}, corporate announcements \cite{zhou-etal-2021-trade}, and social media activity \cite{xu-cohen-2018-stock}. Three main approaches exist in this area. The first leverages text analysis to predict asset responses based on event-related text. Early works utilized TF-IDF \cite{1196287,LI2014826} and topic models \cite{si-etal-2013-exploiting,NGUYEN20159603}, progressing to RNN-based models \cite{10.1145/3155133.3155202,liu2018leveragingfinancialnewsstock} and pre-trained transformers \cite{zhou-etal-2021-trade,shah-etal-2023-trillion}, which capture nuanced semantics. Although these models excel at semantic extraction, they often lack integration with historical price data, crucial for holistic forecasting.

The second line of approaches uses statistical and sequential models on numerical data, such as linear regression \cite{8212716}, ARIMA \cite{7046047}, and GARCH \cite{HYUPROH2007916}. Later, deep learning methods like RNNs \cite{liu2018leveragingfinancialnewsstock} and CNNs \cite{8126078,Durairaj2022} enhanced nonlinear modeling capabilities. More recently, transformer-based models, such as Informer \cite{Zhou_Zhang_Peng_Zhang_Li_Xiong_Zhang_2021} and FedFormer \cite{pmlr-v162-zhou22g}, improved long-range dependency modeling for time series data. However, these models tend to be ``case-specific,'' requiring task-specific training. In contrast, the lastest pre-trained models for time-series data, like MOMENT \cite{goswami2024moment}, Timer \cite{liu2024timer}, and TOKEN \cite{anonymous2024totem}, offer more generalized and adaptable solutions for time-series tasks.

% The second line of traditional techniques primarily relies on numerical features, utilizing statistical models such as linear regression \cite{8212716}, ARIMA \cite{7046047}, and GARCH \cite{HYUPROH2007916}. Compared to these statistical models, deep learning methods demonstrate superior nonlinear modeling capabilities. Several approaches \cite{10.1145/3155133.3155202,liu2018leveragingfinancialnewsstock,doi:10.1080/14697688.2019.1622314,xu-cohen-2018-stock} have developed sequential neural networks based on RNN models, leveraging their unique architecture to capture inter-time dependencies. Additionally, researchers \cite{8126078,HOSEINZADE2019273,Durairaj2022} have employed CNN networks to effectively identify complex cyclical patterns and trends for time-series forecasting. However, these earlier methods suffer from the challenge of long-range sequence dependencies. To address this, the attention mechanism in transformer neural networks offers a potential solution by allowing the model to compute dependencies between different positions in a sequence in parallel. Recent models such as Informer \cite{Zhou_Zhang_Peng_Zhang_Li_Xiong_Zhang_2021}, FedFormer \cite{pmlr-v162-zhou22g}, and AutoFormer \cite{Chen_2021_ICCV} have demonstrated improved performance in this regard. Nevertheless, these transformer-based models are typically ``case-based'' approaches, meaning they require training for each specific task and dataset. In contrast, the latest pre-trained time-series models, such as MOMENT \cite{goswami2024moment}, Timer \cite{liu2024timer}, and TOKEN \cite{anonymous2024totem}, offer more effective universal models for time-series tasks.


The third line of research adopts multi-modality approaches, combining diverse data types to improve forecasting accuracy. Some studies incorporate text and audio \cite{qin-yang-2019-say,10.1145/3366423.3380128} but often overlook time-series dependencies. Recent work has integrated time-series and textual data; for example, \cite{10.1145/3394171.3413752,sawhney-etal-2020-deep} employed SVM and GRU models to capture time-series features. However, these models are relatively shallow for extracting complex patterns. More recent studies \cite{lee2024moat, Jia_Wang_Zheng_Cao_Liu_2024} leverage transformer-based models for time-series analysis, better capturing deeper temporal structures. Building on these advancements, this paper aims to utilize state-of-the-art pre-trained models with enhanced feature fusion and causal learning for multi-modality forecasting.

% The third line of research approaches the problem from a multi-modality perspective, leveraging multi-dimensional features for financial forecasting. Existing works in this direction combine various modalities to enhance predictive performance. For example, \cite{qin-yang-2019-say} integrates text and audio features to better capture sentiment during earnings conference calls, while \cite{10.1145/3366423.3380128} uses transformer networks to fuse audio and textual data for more robust predictions. Additionally, \cite{10.1145/3394171.3413752} incorporates past historical patterns alongside audio and textual features. However, these methods overlook the time-series modality, where past numerical trends significantly impact current market movements. \cite{10.1145/3503161.3548380} adds video features in addition to audio and text. However, these multi-modality models for event-driven forecasting fail to integrate time-series data, missing key dependencies between historical and current prices. \cite{sawhney-etal-2020-deep} considers time-series and event text features, however, their encoding models are relatively shallow, limiting their ability to extract deep semantic knowledge. \cite{lee2024moat} introduces a multi-modality forecasting model based on a pre-trained transformer architecture for textual and time-series data, but it fails to effectively merge these two modalities for superior performance. Another related work \cite{Jia_Wang_Zheng_Cao_Liu_2024} utilizes a simple linear embedding model for time-series data, which is insufficient for capturing complex patterns. Based on the advancements in textual and time-series multi-modality forecasting \cite{lee2024moat,Jia_Wang_Zheng_Cao_Liu_2024}, this paper aims to leverage the latest pre-trained time-series and language models, along with an effective feature fusion mechanism and causal learning strategy, to develop a state-of-the-art multi-modality forecasting approach.

\subsection{Salient Macroeconomic Factors}

\textbf{Which macroeconomic announcements have a greater impact on financial markets than others?} This question has been widely studied in the financial literature, with Central Bank Communications standing out as the most-researched factor \cite{https://doi.org/10.1111/jofi.12818,GERTLER2018336,https://doi.org/10.1111/joes.12550,RePEc:ijc:ijcjou:y:2016:q:4:a:6,TADLE2022106021,RePEc:fip:fednep:00004,https://doi.org/10.1111/jofi.12196,ROSA2011915}. Beyond central bank communications, various other macroeconomic factors have also been identified as significant drivers of market movements. Among these, Non-farm Payrolls, Unemployment Releases, Initial Unemployment Claims, ISM Manufacturing Index, GDP Advance Releases, Consumer Confidence Index, and Producer Price Index (PPI) Reports have been found to notably influence price movements and market volatility through empirical statistical testings \cite{NBERw1296,Gilbert2010-dt,GILBERT201778,RePEc:fip:fednci:y:2008:i:aug:n:v.14no.6,9fdfa12d09ae4792a11e8360a71a356a,RePEc:snb:snbwpa:2013-11}. In this paper, we aim to leaverage the most significant factors evidented by the past financial literautre \cite{NBERw1296,Gilbert2010-dt,GILBERT201778,RePEc:fip:fednci:y:2008:i:aug:n:v.14no.6,9fdfa12d09ae4792a11e8360a71a356a,RePEc:snb:snbwpa:2013-11}, which include FOMC Meeting Documents, Non-farm Payrolls, Unemployment Releases, Initial Unemployment Claims, ISM Manufacturing Index, GDP Advance Releases, Consumer Confidence Index, and Producer Price Index (PPI) Reports.

\subsection{Counterfactual Data Augmentation by LLMs} 

Counterfactual Data Augmentation seeks to reduce spurious correlations and enhance model robustness. \citet{Kaushik2020Learning} introduced a method that augments training data with counterfactuals written by human annotators, effectively helping to mitigate spurious patterns. \citet{wu-etal-2021-polyjuice, ross-etal-2022-tailor} later proposed the use of hand-crafted templates and trained text generators to create counterfactual data through predefined perturbation types. However, these methods are limited by their reliance on fixed perturbations. More recently, \citet{chen-etal-2023-disco, wang-etal-2023-self-instruct} proposed more flexible, LLM-based approaches that leverage specifically designed in-context learning prompts and generation pipelines for counterfactual and instruction data generation. Following this direction, we present a counterfactual generation framework specifically designed for macroeconomic releases.






\begin{table*}[t]
\caption{Summary of Macroeconomic and Time-Series Data Types, Characteristics and Sources}
\label{tab:data_summary}
\begin{tabular}{@{}cc@{}}
\begin{minipage}{\columnwidth} % Adjusted width to equalize both tables
\centering
\caption*{A. Macroeconomic Event Summary}
\label{tab:data_summary_A}
\resizebox{\columnwidth}{!}{
\begin{tabular}{@{}lcccccl@{}}
\toprule
\textbf{Event Type}                    & \textbf{Data Type} & \textbf{Frequency} & \textbf{Period}      & \textbf{No. of Events} & \textbf{No. of C.F.s} & \textbf{Source}        \\ \midrule
\textbf{FOMC}                          & Html               & Quarterly          & 1993.3 $\sim$2024.6  & 255                    & 2,550                 & \makecell[l]{www.federal\\reserve.gov}  \\
\makecell[l]{\textbf{Unemployment} \\ \textbf{Insurance Claims}} & PDF, Txt & Weekly  & 2002.10 $\sim$2024.6 & 913 & 9,130 & oui.doleta.gov \\
\textbf{Employment Situation}          & Html, Txt          & Monthly            & 1994.2 $\sim$2024.6  & 363                    & 3,630                 & www.bls.gov            \\
\textbf{GDP Advance Report}            & Html               & Monthly            & 1996.8 $\sim$2024.6  & 333                    & 3,330                 & www.bea.gov            \\
\textbf{CPI Report}                    & Html, Txt          & Monthly            & 1994.2 $\sim$2024.6  & 357                    & 3,570                 & www.bls.gov            \\
\textbf{PPI Report}                    & Html, Txt          & Monthly            & 1994.2 $\sim$2024.6  & 348                    & 3,480                 & www.bls.gov            \\ \bottomrule
\end{tabular}
}
\end{minipage}

& % Separator for the two tables

\begin{minipage}{\columnwidth} % Same width for consistency
\centering
\caption*{B. Time-Series Data Summary}
\label{tab:data_summary_B}
\resizebox{\columnwidth}{!}{
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Time Series Data}                & \textbf{Data Types}    & \textbf{Frequency} & \textbf{Range}        & \textbf{No. of Data Points} \\ \midrule
\textbf{SP500 (SPX)}             & Open, Close, High, Low & 5 Min              & 2008.01 $\sim$2024.06 & 331,257                     \\
\textbf{Dow Industrial (INDU)}    & Open, Close, High, Low & 5 Min              & 2012.07 $\sim$2024.06 & 263,445                     \\
\textbf{NASDAQ (NDX)}    & Open, Close, High, Low & 5 Min              & 2008.01 $\sim$2024.06 & 332,616                     \\
\makecell[l]{\textbf{US Treasury Bond} \\ \textbf{at 1-Month (USGG1M)}}    & Open, Close, High, Low & 5 Min              & 2013.01 $\sim$2024.06 & 751,443                     \\
\makecell[l]{\textbf{US Treasury Bond} \\ \textbf{at 5-Year (USGG5YR)}}    & Open, Close, High, Low & 5 Min              & 2013.01 $\sim$2024.06 & 734,773                     \\
\bottomrule
\end{tabular}
}
\end{minipage}
\end{tabular}
\end{table*}


\section{Problem Formulation}
\label{sec:definition}
\indent \textbf{Event Set} is defined as $\mathbb{E} := \{ \mathcal{E}_{1}, \mathcal{E}_{2}, \dots, \mathcal{E}_{|\mathbb{E}}| \} $, where $\mathbb{E}$ represents a collection of $|\mathbb{E}|$ event scripts. Each event $\mathcal{E}_{i}$ occurs at a specific timestamp $i$ and belongs to one of the event types shown in Table \ref{tab:data_summary}A. Each event script $\mathcal{E}_{i}$ consists of a sequence of word tokens, represented as $\mathcal{E}_{i} := \{w_1, w_2, \dots, w_m \}$.

\textbf{Time Series Data} is defined as $\mathcal{X} := \{ X_1, X_2, \dots, X_{|\mathcal{X}|} \}$, where each $X_i$ represents the numerical data at time step $i$. An event $\mathcal{E}_i$ is \textbf{aligned} with a time series segment $\mathcal{X}_{i-\tau:i+\tau}$, where $i$ denotes the time of the releasement of the event, and $\tau$ represents the duration of the time-series segment both preceding and succeeding time step $i$, denoted as \([\mathcal{X}_{i-\tau:i+\tau} \mapsto \mathcal{E}_i ]\). This alignment reflects the time series segment leading up to the event ($\mathcal{X}_{i-\tau}$) and the period during which the event is expected to have an effect ($\mathcal{X}_{i+\tau}$). 

\textbf{Event-Driven Forecasting}: Given a dataset \( \mathcal{U} = \{ [\mathcal{X}_{i-\tau:i+\tau} \mapsto \mathcal{E}_i] \}_{i=1}^n \) consisting of \( n \) aligned event and time-series pairs, for each data pair in \( \mathcal{U}\), the model uses both the event text \( \mathcal{E}_i \) and the historical time-series segment \( \mathcal{X}_{i-\tau:i} \), which spans \( \tau \) steps before the event’s release at time \( i \), to forecast the future time steps \( \mathcal{X}_{i+1:i+\tau} \).

\begin{figure}[t]
    \centering
    % Subfigure 1
    \begin{subfigure}{0.35\columnwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/scm.drawio.png}
        \caption{\centering Causal Effect Graph}
        \label{fig:scm}
    \end{subfigure}
    \hfill % Adds spacing between subfigures
    % Subfigure 2
    \begin{subfigure}{0.5\columnwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/couterfactual.drawio.png}
        \caption{\centering Cause-Effect Detection with Counterfactual Events}
        \label{fig:CE}
    \end{subfigure}
    % Main Figure Caption
    \caption{The illustration of causal relationships and counterfactual events.}
    \label{fig:main}
\end{figure}


\textbf{Causal Effect Graph:} A Causal Effect Graph represents the causal links among variables: textual modality \(\mathcal{E}\) (event scripts), current price trend \(\mathcal{X}\), and future time series movements \(\mathbf{Y}\). In event-driven financial prediction, events influence the market movements of financial assets, forming a causal chain denoted as \(\mathcal{E} \rightarrow \mathcal{X} \rightarrow \mathbf{Y}\), as illustrated in Figure \ref{fig:scm}.

\textbf{Counterfactual Event:} A Counterfactual Event (CE) represents a modified event script in which key variables (e.g., unemployment rates, GDP values) are altered relative to the factual event, while the surrounding context remains unchanged. These events are denoted as \(\{\mathcal{E}^{*}_{1}, \mathcal{E}^{*}_{2}, \ldots, \mathcal{E}^{*}_{n}\}\). CEs are utilized to train CAMEF, enabling it to identify factual cause-effect relationships, represented as \(\mathcal{E} \rightarrow \mathcal{X} \rightarrow \mathcal{Y}\), as illustrated in Figure \ref{fig:CE}.

% \section{Data Collection and Counterfactual Event Augmentation}
% \label{sec:data_acquisition}
% In this section, we introduce the proposed dataset and the counterfactual augmentation methodology. Following the findings in the financial literature \cite{https://doi.org/10.1111/jofi.12818,GERTLER2018336,https://doi.org/10.1111/joes.12550,RePEc:ijc:ijcjou:y:2016:q:4:a:6,TADLE2022106021,RePEc:fip:fednep:00004,https://doi.org/10.1111/jofi.12196,ROSA2011915}, we selected 6 most significant macroeconomic announcements, including the \textbf{\textit{FOMC Minutes, Unemployment Insurance Releases, Initial Unemployment Claims, GDP Advance Releases, CPI Report, and PPI Report}}. Whereas for the time-series dataset,  we focused on 5 key financial assets, including major U.S. stock indexes and Treasury bonds, which are mostly adopted from the macroeconmic researchs \cite{https://doi.org/10.1111/jofi.12818,GERTLER2018336,https://doi.org/10.1111/joes.12550,RePEc:ijc:ijcjou:y:2016:q:4:a:6,TADLE2022106021,RePEc:fip:fednep:00004,https://doi.org/10.1111/jofi.12196,ROSA2011915}, including , as outlined in Table \ref{tab:data_summary_B}. 

\section{Data Collection and Counterfactual Event Augmentation}
\label{sec:data_acquisition}

This section introduces the proposed dataset and the methodology for counterfactual event augmentation. The dataset includes 6 types of key macroeconomic announcements ranging from 2004 to 2024, selected through an extensive review of the financial literature, along with \textunderscore{high-frequency} trading data. Unlike the daily-based trading data used in previous studies, this high-frequency data provides more predictive accuracy and better reflects real trading behavior in the industry.

% \begin{algorithm}
% \label{alo:counterfactual_generation}
% \caption{Event Summarization and Counterfactual Generation}\label{alg:counterfactual_event_generation}
% \begin{algorithmic}[1]
% \State \textbf{Input:} event\_scripts\_dict
% \State \textbf{Output:} events\_summary\_and\_counterfactuals
% \State Initialize LLaMA-3, load event files
% \State events\_summary\_and\_counterfactuals = \{ \}

% \Procedure{SummSentiment}{event\_script, chunk\_size}
%     \State \textit{Split event\_script into script\_chunks}
%     \ForAll{chunk \textbf{in} script\_chunks}: 
%         \State Generate prompt by \textbf{Template 1}
%         \State Generate chunk summary with LLaMA-3
%     \EndFor
%     \State Combine chunk summaries to generate \textbf{full\_summary}
%     \State Generate prompt by \textbf{Template 2} and \textbf{Template 3}
%     \State Generate the full summary with LLaMA-3
%     \State Analyze sentiment of full summary with LLaMA-3
%     \State \textbf{return} \textbf{full\_summary}, \textbf{ sentiment\_score}
% \EndProcedure

% \Procedure{GenerateCF}{full\_summary, sentiment\_score}:
%     \ForAll{target\_sentiment $\neq$ sentiment\_score} 
%         \State Generate prompt by \textbf{Template 4}
%         \State Generate counterfactual with LLaMA-3
%     \EndFor
%     \State \textbf{return} \textbf{generated\_counterfactuals}
% \EndProcedure

% \ForAll{event\_id, event\_script \textbf{in} event\_scripts\_dict}:
%     \State event\_summary, event\_sentiment$\gets$\textsc{SummSentiment}(event\_script, chunk\_size)
%     \State counterfactuals$\gets$\textsc{GenerateCF}(event\_summary, event\_sentiment)
%     \State Store \{event\_summary, event\_sentiment, counterfactuals\} in events\_summary\_and\_counterfactuals[event\_id]
% \EndFor
% \end{algorithmic}
% \end{algorithm}


\subsection{Dataset Acquisition}

The primary question guiding the collection of this dataset is: \textbf{``Which macroeconomic releases have the greatest impact on financial markets?''} To address this, we conducted a comprehensive review of the financial literature to identify key macroeconomic factors that influence market behavior. Several dominant factors emerged, including the \textbf{FOMC Minutes} \cite{https://doi.org/10.1111/jofi.12818, GERTLER2018336, https://doi.org/10.1111/joes.12550, RePEc:ijc:ijcjou:y:2016:q:4:a:6, TADLE2022106021, RePEc:fip:fednep:00004, https://doi.org/10.1111/jofi.12196, ROSA2011915}, along with \textbf{Unemployment Insurance Claims}, \textbf{Employment Situation Reports}, \textbf{GDP Advance Releases}, and the \textbf{Consumer Price Index (CPI)} and \textbf{Producer Price Index (PPI)} reports \cite{NBERw1296, Gilbert2010-dt, GILBERT201778, RePEc:fip:fednci:y:2008:i:aug:n:v.14no.6, 9fdfa12d09ae4792a11e8360a71a356a, RePEc:snb:snbwpa:2013-11}, which serve as the textual modality data for our dataset. 

To collect these data, we developed web crawlers to extract raw files directly from official sources, including HTML, PDF, and TXT formats. These raw files were then pre-processed and converted into a structured and unified text format, ensuring consistency and ease of subsequent analysis. Table \ref{tab:data_summary_A} provides a summary of the data types, collection frequencies, time periods, and sources of the events included in our dataset. Further details on the data crawling and pre-processing methodologies can be found in Appendix B.

In addition to the textual data, studies \cite{https://doi.org/10.1111/jofi.12818, GERTLER2018336, https://doi.org/10.1111/joes.12550, RePEc:ijc:ijcjou:y:2016:q:4:a:6, TADLE2022106021, RePEc:fip:fednep:00004, https://doi.org/10.1111/jofi.12196, ROSA2011915} have demonstrated that the largest market impacts are typically observed in major U.S. stock indexes and Treasury bonds. Therefore, we focused on collecting high-frequency trading time-series data at 5 minute interval for key stock indexes, including the \textbf{S\&P 500 (SPX)}, \textbf{Dow Industrial (INDU)}, \textbf{NASDAQ (NDX)}, as well as \textbf{U.S. Treasury Bond at 1-Month (USGG1M)} and \textbf{Treasury Bond at 5-Year (USGG5YR)} from Bloomberg Terminal.



% \subsection{Counterfactual Events Generation and Learning}
% \label{sec:CE_generation}

% \subsubsection{Objective of Counterfactual Learning}


% In event-driven financial trading, traders assess key events, such as FOMC meetings or employment reports, and make trades based on their expectations of price movements. The collective actions of market participants shape asset trends, creating a clear cause-effect relationship. For example, when the Federal Reserve announced a 5.5\% interest rate on June 24th, 2024, traders anticipated the impact on prices and adjusted their trades accordingly, influencing the overall market trend. If different information, such as a different interest rate, had been released, the market reaction would have been different. \textbf{Understanding these cause-effect relationships is crucial for accurately predicting the market’s response to future events.}

% As illustrated in Figure \ref{fig:counterfactual_pipeline}, to improve the robustness of our approach, this study leverages causal learning by training on counterfactual events. Counterfactual events represent hypothetical scenarios where key information is altered (e.g., a different interest rate), while maintaining a similar structure to the observed event scripts. \textbf{By incorporating these counterfactual event scripts, the model can better learn the true causal relationships and distinguish between actual market drivers and irrelevant variations.}

% Specifically, suppose an event script $\mathcal{E}_i := \{w_1, w_2, ..., w_m \}$ as defined in Section \ref{sec:definition}, where each $w_j$ represents a word in the event script. We can further decompose $\mathcal{E}_i$ into two disjoint subsets:

% \begin{equation}
% \mathcal{E}_i = \mathcal{E}_i^\text{neutral} \cup \mathcal{E}_i^\text{sentiment}
% \end{equation}

% \noindent where $\mathcal{E}_i^\text{neutral} := \{ w_j \in \mathcal{E}_i \mid w_j \text{ does not carry market sentiment} \}$ and $\mathcal{E}_i^\text{sentiment} := \{ w_j \in \mathcal{E}_i \mid w_j \text{ carries market sentiment} \}$, representing the disjoint subsets of sentiment-neutral and sentiment-relevant words, respectively. Thus, for each word $w_j \in \mathcal{E}_i$, it belongs to one of the two sets:
% \begin{equation}
% w_j \in \begin{cases} 
% \mathcal{E}_i^\text{neutral} & \text{if } w_j \text{ is sentiment-neutral} \\
% \mathcal{E}_i^\text{sentiment} & \text{if } w_j \text{ is sentiment-relevant}
% \end{cases}
% \end{equation}

% We leverage the large language model (LLM), denoted by $f_{LLM}$, as described in Algorithm \ref{alg
% }, to selectively modify the sentiment-bearing subset $\mathcal{E}_i^\text{sentiment}$ of the event script, conditioned on a target sentiment different from the ground-truth sentiment. The generation of counterfactual event scripts can be expressed as:

% \begin{equation}
% \mathcal{E}_i^\prime = \mathcal{E}i^\text{neutral} \cup f_{LLM}(\mathcal{E}_i^\text{sentiment} \mid S_i^\prime)
% \end{equation}

% \noindent where $f_{LLM}(\mathcal{E}_i^\text{sentiment} \mid S_i^\prime)$ represents the process in which the LLM modifies the sentiment-relevant words in $\mathcal{E}_i^\text{sentiment}$ to align with a target sentiment $S_i^\prime$, as detailed in Algorithm \ref{alg:counterfactual_event_generation} and Section \ref{sec:counterfactual_generation}. Here, the target sentiment $S_i^\prime$ differs from the ground-truth sentiment $S_i$ (i.e., $S_i^\prime \neq S_i$), while the sentiment-neutral set $\mathcal{E}_i^\text{neutral}$ remains largely unaffected.

% This process generates a counterfactual event script, denoted by $\mathcal{E}_i^\prime$, that captures hypothetical financial scenarios driven by modifications in the sentiment-bearing content to match the target sentiment $S_i^\prime$. This enables the exploration of alternative market outcomes by analyzing how the modified sentiment might influence financial forecasts under different conditions.

% \subsubsection{Generation of Counterfactual Events via LLM}
% \label{sec:counterfactual_generation}
% We experimented with various prompting strategies on the 8B LLaMA-3 model. The objective is to \textbf{generate counterfactual event scripts that are structurally similar to the actual event scripts but contain different sentiment-related terminology, phrases, and numerical values to express varying levels of sentiment.} Our proposed prompting strategy involves the following three procedures:

% \begin{enumerate}
%     \item \textbf{Summarization Prompt:} This prompt condenses lengthy event scripts into a GPU-acceptable length while retaining key sentiment-related terminology and numerical values.
%     \item \textbf{Sentiment Analytical Prompt:} This prompt assigns a sentiment score (on a scale from 0 to 10) to a summarized event script based on logical analysis of the sentiment conveyed and the numerical data presented.
%     \item \textbf{Counterfactual Event Generation Prompt:} This prompt generates 10 counterfactual event scripts with varying levels of sentiment compared to the target script (the actual event script) by incorporating reasonable phrases, sentiment-related terminology, and logically adjusted numerical values.
% \end{enumerate}


\begin{figure*}[ht]
    \centering
    \includegraphics[width=\linewidth]{figures/Model_Pipeline.drawio.png}
    \caption{The Pipeline and Neural Architecture of CAMEF}
    \label{fig:model_architecture}
\end{figure*}

% We expect the modifications to fulfill the following requirements:

% \begin{itemize}
%     \item \textbf{Numerical Reasonability:} Adjust key numerical variables logically to reflect the target sentiment \( S_i^\prime \), such as higher unemployment rates for negative sentiment or lower rates for positive sentiment, while maintaining factual plausibility.
%     \item \textbf{Sentiment Relevance:} Modify sentiment-relevant words and phrases (\( \mathcal{E}_i^\text{sentiment} \)) to align with the target sentiment while ensuring contextual coherence and consistency.
%     \item \textbf{Structural Consistency:} Preserve the original script's format, style, and structure to ensure plausibility, readability, and coherence of the counterfactual event.
% \end{itemize}



\subsection{Counterfactual Events Generation based on LLM}
\label{sec:CE_generation}
This section describes the process of counterfactual event generation, creating  hypothetical scenarios from existing event scripts. The aim is to reflect a target sentiment of a given event script while maintaining logical consistency and coherence of the original script. Our goal is modifying sentiment-relevant elements (such as key facts, sentiment-indicative phrases, or numerical values) without disrupting the sentiment-neutral components of the script.

Formally, for a given event script \( \mathcal{E}_i := \{w_1, w_2, \dots, w_m\} \), the objective is to produce a counterfactual version \( \mathcal{E}_i^\prime \) that embodies the desired target sentiment \( S_i^\prime \). Conceptually, the event script can be viewed as comprising sentiment-relevant content (\( \mathcal{E}_i^\text{sentiment} \)) and sentiment-neutral content (\( \mathcal{E}_i^\text{neutral} \)), so that \( \mathcal{E}_i = \mathcal{E}_i^\text{neutral} \cup \mathcal{E}_i^\text{sentiment} \). Instead of explicitly decomposing the script, we guide a language model (LLM) using structured prompts to modify only the sentiment-relevant content. This ensures that neutral content remains intact or is replaced with semantically equivalent expressions. Formally:
\begin{equation}
\mathcal{E}_i^\prime = \mathcal{E}_i^\text{neutral} \cup f_\text{LLM}(\mathcal{E}_i^\text{sentiment} \mid S_i^\prime),
\end{equation}

\noindent where \( f_\text{LLM} \) adjusts \( \mathcal{E}_i^\text{sentiment} \) to align with \( S_i^\prime \), and \( \mathcal{E}_i^\text{neutral} \) remains unchanged or is replaced by equivalent expressions.

Specifically, we used the LLaMA-3 8B model with a series of carefully designed prompts. These prompts include three key steps, with detailed templates provided in Appendix A:

\begin{enumerate}
    \item \textbf{Summarization Prompt (Appendix A.1):} Condenses lengthy event scripts into concise summaries, addressing memory constraints while retaining sentiment-relevant content and key numerical variables.
    \item \textbf{Sentiment Analysis Prompt (Appendix A.2):} Assigns a sentiment score (from 1, very negative, to 10, very positive) to the original event script. This score provides a baseline for generating counterfactual versions.
    \item \textbf{Counterfactual Generation Prompt (Appendix A.3):} Produces multiple counterfactual scripts, each reflecting a different sentiment level. The prompt modifies sentiment-related phrases and numerical values (\( \mathcal{E}_i^\text{sentiment} \)) while preserving or equivalently substituting neutral content (\( \mathcal{E}_i^\text{neutral} \)). This approach ensures numerical reasonability, sentiment relevance, and structural consistency.
\end{enumerate}

This multi-step prompt strategy facilitates the generation of coherent, contextually relevant counterfactual events, enabling exploration of diverse market scenarios and deeper causal understanding.


% \section{CAMEP Architecture}

% The neural architecture of CAMEP consists of a textual encoder, a time-series encoder, and a forecasting decoder. The details of each component are introduced in the following subsections and Fig. \ref{fig:model_architecture}.

% \subsection{Textual Modality Encoder}

% The event scripts are encoded into vector representations using the RoBERTa model \cite{liu2020roberta}, a transformer-based text encoder. Given an input event script from the event collection defined in Sec. \ref{sec:definition}, denoted as $\mathbf{E}_i = {w_1, w_2, \dots, w_m}$, RoBERTa produces a sequence of contextualized token embeddings:

% \begin{equation}
% \{\mathbf{h}_1, \mathbf{h}_2, \dots, \mathbf{h}_m\} = \text{RoBERTa}(\{w_1, w_2, \dots, w_m\}), 
% \end{equation}

% \noindent where each $\mathbf{h}_j \in \mathbb{R}^{768}$ represents the embedding vector for token $w_j$. These embeddings are then refined through a projection network, specifically designed for the forecasting task. This projection network consists of three linear transformations with GELU activations, which first project the embeddings into a higher-dimensional space before returning to the original dimension:

% \begin{equation} \mathbf{e}_j = \mathbf{W}^{(3)} , \text{GELU}(\mathbf{W}^{(2)} , \text{GELU}(\mathbf{W}^{(1)} \mathbf{h}_j + \mathbf{b}^{(1)}) + \mathbf{b}^{(2)}) + \mathbf{b}^{(3)}, \end{equation}

% \noindent where $\mathbf{W}^{(1)} \in \mathbb{R}^{8192 \times 768}$, $\mathbf{W}^{(2)} \in \mathbb{R}^{1024 \times 8192}$, and $\mathbf{W}^{(3)} \in \mathbb{R}^{768 \times 1024}$. To obtain a single encoding vector for the event script, average pooling is applied over the sequence of transformed token embeddings:

% \begin{equation}
% \mathbf{E}_i = \frac{1}{m} \sum_{j=1}^{m} \mathbf{e}_j,
% \end{equation}

% \noindent where $\mathbf{E}_i$ represents the final pooled vector summarizing the event script $\mathbf{E}_i$, and $m$ is the number of tokens in the script.


% \subsection{Time-Series Modality Encoder with MOMENT and GPT-2}

% The time-series data is encoded through a multi-step process using both the pretrained MOMENT encoder \cite{goswami2024moment} and GPT-2 \cite{radford2019language}, which consists of 12 transformer decoder layers. This setup allows us to capture temporal dependencies within the time-series data while leveraging GPT-2’s autoregressive capacity to enhance representation.

% Given an input time series segment, denoted as $\{ X_1, X_2, \dots, X_n \}$, the MOMENT encoder first produces an initial vector representation:

% \begin{equation}
%     \mathbf{\mathcal{X}}_i = \textbf{MOMENT}(\{ X_1, X_2, \dots, X_n\}),
% \end{equation}

% \noindent where $\mathbf{\mathcal{X}}_i \in \mathbb{R}^{1024}$ is the encoded vector of the input time series segment. This representation serves as the input to a \textbf{multi-residual projection layer}, which refines the embedding through three residual transformations, each defined by a sequence of linear transformations and GELU activations:

% \begin{equation}
%     \mathbf{Z}_i = \mathbf{\mathcal{X}}_i + f_{\text{residual}}(\mathbf{\mathcal{X}}_i),
% \end{equation}

% \noindent where \(f_{\text{residual}}(\mathbf{\mathcal{X}}_i)\) is defined as:

% \begin{equation}\label{eqn:residual_layer}
%     f_{\text{residual}}(\mathbf{\mathcal{X}}_i) = \mathbf{W}_3 \cdot \text{GELU}(\mathbf{W}_2 \cdot \text{GELU}(\mathbf{W}_1 \cdot \mathbf{\mathcal{X}}_i + \mathbf{b}_1) + \mathbf{b}_2) + \mathbf{b}_3,
% \end{equation}

% \noindent where \(\mathbf{W}_1 \in \mathbb{R}^{8192 \times (n \cdot d)}\), \(\mathbf{W}_2 \in \mathbb{R}^{1024 \times 8192}\), and \(\mathbf{W}_3 \in \mathbb{R}^{768 \times 1024}\) are learnable weight matrices, and \(\mathbf{b}_1, \mathbf{b}_2, \mathbf{b}_3\) are the respective biases. This residual structure allows the model to retain the original encoded features while introducing non-linear transformations to capture more complex patterns.

% The refined time-series vector $\mathbf{Z}_i$ is then passed as input to a stack of 12 GPT-2 layers, where each layer further models the sequence dependencies in the time-series data:

% \begin{equation}
% \mathbf{Z}^{(l)} = f_{\text{GPT2\_layer}}^{(l)}(\mathbf{Z}^{(l-1)}),
% \end{equation}

% \noindent with \(\mathbf{H}^{(0)} = \mathbf{Z}_i\), and \(f_{\text{GPT2\_layer}}^{(l)}\) representing the transformation function of the \(l\)-th GPT-2 layer. The final output from the GPT-2 model is further refined by a \textbf{second residual layer} which composes identical neural layers as introduced in Equation \ref{eqn:residual_layer}. The final vector, \(\mathbf{H}_{\text{final}}\), is obtained by adding the residual layer’s output to the GPT-2 output and applying layer normalization:

% \begin{equation}
% \mathbf{H} = \text{LayerNorm}(\mathbf{Z}^{(L)} + f_{\text{second\_residual}}(\mathbf{Z}^{(L)})),
% \end{equation}

% \noindent where \(L = 12\), representing the total number of GPT-2 layers.



% \subsection{Multi-Modality Fusion and Output Layer}

% To integrate information from the textual and time-series modalities, we introduce a \textbf{fusion layer} that combines their embeddings into a unified representation. The text embeddings, \(\mathbf{E}\), and time-series embeddings, \(\mathbf{H}\), are first concatenated along the feature dimension:

% \begin{equation}
% \mathbf{F}_{\text{input}} = \text{concat}(\mathbf{E}, \mathbf{H}),
% \end{equation}

% \noindent where \(\mathbf{F}_{\text{input}} \in \mathbb{R}^{1536}\) captures both semantic and temporal information. This combined vector is then passed through a \textbf{fusion network}, a series of linear transformations with GELU activations and dropout for regularization, to progressively reduce the dimensionality and produce the fused output:

% \begin{equation}
% \mathbf{F}_{\text{output}} = f_{\text{fusion}}(\mathbf{F}_{\text{input}}).
% \end{equation}

% \noindent Here, the fusion network transforms \(\mathbf{F}_{\text{input}}\) through weight matrices \(\mathbf{W}_1 \in \mathbb{R}^{8192 \times 1536}\), \(\mathbf{W}_2 \in \mathbb{R}^{4096 \times 8192}\), \(\mathbf{W}_3 \in \mathbb{R}^{2048 \times 4096}\), \(\mathbf{W}_4 \in \mathbb{R}^{1024 \times 2048}\), and \(\mathbf{W}_5 \in \mathbb{R}^{d \times \text{pred\_len} \times 1024}\).

% The output \(\mathbf{F}_{\text{output}}\) is then refined with a residual connection from \(\mathbf{H}\), which is processed through an additional residual layer:

% \begin{equation}
% \mathbf{Y}_{\text{pred}} = \mathbf{F}_{\text{output}} + f_{\text{residual}}(\mathbf{H}),
% \end{equation}

% \noindent where \(f_{\text{residual}}\) has the same structure as in Equation~\ref{eqn:residual_layer} and ensures compatibility with \(\mathbf{F}_{\text{output}}\). The final prediction vector, \(\mathbf{Y}_{\text{pred}}\), thus combines the multi-modal representation and residual time-series features to improve prediction accuracy.


% \subsection{Time-Series Learning Objectives}


% The final time-series predictions, \(\mathbf{Y}_{\text{pred}}\), are directly used in the learning objectives to optimize forecasting accuracy. To train the model effectively, we employ a combination of Mean Squared Error (MSE) and Mean Absolute Error (MAE) losses:

% 1. **MSE Loss**: The Mean Squared Error (MSE) loss minimizes the squared differences between the predicted values, \(\mathbf{Y}_{\text{pred}}\), and the ground truth values, \(\mathbf{Y}\), emphasizing larger errors:

% \begin{equation}
% \mathcal{L}_{\text{MSE}} = \frac{1}{n} \sum_{i=1}^{n} (\mathbf{Y}_{\text{pred}, i} - \mathbf{Y}_i)^2,
% \end{equation}

% \noindent where \(n\) is the total number of predictions. This objective is particularly useful for penalizing large deviations, making the model sensitive to outliers.

% 2. **MAE Loss**: The Mean Absolute Error (MAE) loss minimizes the absolute differences between \(\mathbf{Y}_{\text{pred}}\) and \(\mathbf{Y}\), providing a more balanced view of overall accuracy:

% \begin{equation}
% \mathcal{L}_{\text{MAE}} = \frac{1}{n} \sum_{i=1}^{n} |\mathbf{Y}_{\text{pred}, i} - \mathbf{Y}_i|,
% \end{equation}

% \noindent MAE is less sensitive to outliers, as it gives equal weight to all errors.

% The total loss function combines these two objectives, allowing the model to account for both large and small prediction errors effectively:

% \begin{equation}
% \label{eqn:time_objective}
% \mathcal{L}_{\text{Time}} = \mathcal{L}_{\text{MSE}} + \mathcal{L}_{\text{MAE}}.
% \end{equation}

% \noindent This combined objective guides the model to optimize forecasting performance by balancing sensitivity to larger errors with overall prediction accuracy.

% \subsection{Contrastive Learning Objective for Causal Learning}


% Causal learning enhances model robustness by distinguishing true event scripts from sampled counterfactual events (CEs). We use three objectives for this purpose: 1) cross-entropy, 2) triplet loss, and 3) hinge loss.

% To ensure diversity in CEs, a \textbf{Diverse Counterfactual Event Sampling Mechanism} samples two types of CEs for each ground-truth event: (1) CEs derived from the ground-truth event, as described in Sec. \ref{sec:CE_generation}, and (2) CEs generated from other event types occurring on nearby dates. This sampling mechanism enables the model to recognize both in-type and cross-type event distinctions. We denote the sampled CEs as \(\mathbb{E}^{CF}:=\{\mathcal{E}^{CF}_{1}, \mathcal{E}^{CF}_{2},\ldots,\mathcal{E}^{CF}_{|\mathbb{E}^{CF}|}\}\).

% The CAMEF model encodes both the ground-truth event \(\mathcal{E}^{GT}\) and the counterfactual events \(\mathbb{E}^{CF}\) using its textual modality encoder:

% \begin{equation}
%     \mathbf{E}^{GT} = \textbf{CAMEF}_{\text{Textual}}(\mathcal{E}^{GT}),
% \end{equation}

% \begin{equation}
%     \{\mathbf{E}^{CF}_i\}_{i=1}^{|\mathbb{E}^{CF}|} = \textbf{CAMEF}_{\text{Textual}}(\{\mathcal{E}^{CF}_i\}_{i=1}^{|\mathbb{E}^{CF}|}).
% \end{equation}

% For the \textbf{Cross-Entropy Objective}, the loss is calculated as:

% \begin{equation}
% \mathcal{L}_{\text{CE}} = - \log \frac{\exp(\mathbf{H}_{\text{final}} \cdot \mathbf{E}^{GT})}{\exp(\mathbf{H}_{\text{final}} \cdot \mathbf{E}^{GT}) + \sum_{i=1}^{|\mathbb{E}^{CF}|} \exp(\mathbf{H}_{\text{final}} \cdot \mathbf{E}^{CF}_i)},
% \end{equation}

% \noindent where \(\mathbf{H}_{\text{final}}\) is the output embedding from Equation \ref{eqn:output_embedding}, \(\mathbf{E}^{GT}\) is the ground-truth embedding, and \(\mathbf{E}^{CF}_i\) are the CE embeddings.

% The \textbf{Triplet Loss} objective ensures that the ground-truth event embedding \(\mathbf{E}^{GT}\) is closer to \(\mathbf{H}_{\text{final}}\) than any CE embedding \(\mathbf{E}^{CF}_i\):

% \begin{equation}
% \mathcal{L}_{\text{TL}} = \max(0, \mathbf{H}_{\text{final}} \cdot \mathbf{E}^{GT} - \mathbf{H}_{\text{final}} \cdot \mathbf{E}^{CF}_i + \alpha),
% \end{equation}

% \noindent where \(\alpha\) is the margin enforcing separation between ground-truth and CE embeddings.

% Lastly, the \textbf{Hinge Loss} further ensures that the ground-truth embedding \(\mathbf{E}^{GT}\) is closer to \(\mathbf{H}_{\text{final}}\) than the CE embeddings by at least a margin \(\alpha\):

% \begin{equation} 
% \mathcal{L}_{\text{HM}} = \left( \|\mathbf{H}_{\text{final}} - \mathbf{E}^{GT}\|^2 \right) + \left( \max(0, \alpha - \|\mathbf{H}_{\text{final}} - \mathbf{E}^{CF}\|) \right)^2.
% \end{equation}

% The combined causal learning objective based on these counterfactual events is:

% \begin{equation}
% \label{eqn:causal_objective}
% \mathcal{L}_{\text{Causal}} = \mathcal{L}_{\text{CE}} + \mathcal{L}_{\text{TL}} + \mathcal{L}_{\text{HM}}.
% \end{equation}

% The \textbf{Total Training Loss} is a weighted combination of the time-series forecasting and causal learning objectives:

% \begin{equation}
% \mathcal{L}_{\text{Total}} = \beta \cdot \mathcal{L}_{\text{Time}} + (1 - \beta) \cdot \mathcal{L}_{\text{Causal}},
% \end{equation}

% \noindent where \(\mathcal{L}_{\text{Time}}\) is the time-series objective from Equation \ref{eqn:time_objective}, \(\mathcal{L}_{\text{Causal}}\) is the causal learning objective, and \(\beta\) balances their contributions.

% \subsection{Training Strategy}

% We employ a two-phase training strategy to effectively integrate information from both modalities and optimize model performance.

% \textbf{Phase 1: Time-Series Modality Training.} In this initial phase, we focus solely on training the time-series modality. Trainable parameters in the MOMENT model, GPT-2, and the output residual layer are updated, while parameters in RoBERTa are frozen. The objective functions are Mean Squared Error (MSE) and Mean Absolute Error (MAE) losses, with causal objectives excluded in this phase to focus on accurate time-series predictions.

% \textbf{Phase 2: Full-Model Training with Causal Objectives.} Building on the model from Phase 1, this phase fine-tunes all trainable parameters, including those in the textual modality (RoBERTa). Both causal and time-series objectives (MSE and MAE) are used, allowing the model to learn cross-modality relations and causal dependencies effectively.

% \section{CAMEP Architecture}

% The CAMEP architecture consists of a textual encoder, a time-series encoder, and a forecasting decoder, as shown in Fig. \ref{fig:model_architecture}.

% \subsection{Textual Modality Encoder}

% We use RoBERTa \cite{liu2020roberta} to encode each event script $\mathbf{E}_i = \{w_1, w_2, \dots, w_m\}$ into contextualized token embeddings:

% \begin{equation}
% \{\mathbf{h}_1, \mathbf{h}_2, \dots, \mathbf{h}_m\} = \text{RoBERTa}(\{w_1, w_2, \dots, w_m\}), 
% \end{equation}

% \noindent where $\mathbf{h}_j \in \mathbb{R}^{768}$. These embeddings are passed through a projection network with three linear layers and GELU activations:

% \begin{equation}
% \mathbf{e}_j = \mathbf{W}^{(3)} \, \text{GELU}(\mathbf{W}^{(2)} \, \text{GELU}(\mathbf{W}^{(1)} \mathbf{h}_j + \mathbf{b}^{(1)}) + \mathbf{b}^{(2)}) + \mathbf{b}^{(3)},
% \end{equation}

% \noindent where $\mathbf{W}^{(1)} \in \mathbb{R}^{8192 \times 768}$, $\mathbf{W}^{(2)} \in \mathbb{R}^{1024 \times 8192}$, and $\mathbf{W}^{(3)} \in \mathbb{R}^{768 \times 1024}$. Average pooling over $\mathbf{e}_j$ provides the final encoding $\mathbf{E}_i$ for the script.

% \subsection{Time-Series Modality Encoder}

% The time-series encoder combines the MOMENT model \cite{goswami2024moment} and GPT-2 \cite{radford2019language} (with 12 layers) to capture temporal patterns. Given an input time series $\{X_1, X_2, \dots, X_n\}$, MOMENT generates an initial encoding:

% \begin{equation}
% \mathbf{\mathcal{X}}_i = \textbf{MOMENT}(\{ X_1, X_2, \dots, X_n\}).
% \end{equation}

% \noindent A multi-residual projection layer refines this encoding:

% \begin{equation}
% \mathbf{Z}_i = \mathbf{\mathcal{X}}_i + f_{\text{residual}}(\mathbf{\mathcal{X}}_i),
% \end{equation}

% \noindent where \(f_{\text{residual}}(\mathbf{\mathcal{X}}_i)\) involves linear layers and GELU activations. The resulting $\mathbf{Z}_i$ is then processed through 12 GPT-2 layers, with the output further refined by a second residual layer and layer normalization:

% \begin{equation}
% \mathbf{H} = \text{LayerNorm}(\mathbf{Z}^{(L)} + f_{\text{second\_residual}}(\mathbf{Z}^{(L)})).
% \end{equation}

% \subsection{Multi-Modality Fusion and Output Layer}

% The text and time-series embeddings, $\mathbf{E}$ and $\mathbf{H}$, are concatenated to form a unified representation:

% \begin{equation}
% \mathbf{F}_{\text{input}} = \text{concat}(\mathbf{E}, \mathbf{H}),
% \end{equation}

% \noindent which is processed by a fusion network of linear transformations and GELU activations to produce the final output:

% \begin{equation}
% \mathbf{Y}_{\text{pred}} = \mathbf{F}_{\text{output}} + f_{\text{residual}}(\mathbf{H}),
% \end{equation}

% \noindent where $f_{\text{residual}}(\mathbf{H})$ provides a residual connection to enhance the prediction accuracy.

% \subsection{Time-Series Learning Objectives}

% The model uses a combination of MSE and MAE losses to optimize forecasting accuracy:

% \begin{equation}
% \mathcal{L}_{\text{MSE}} = \frac{1}{n} \sum_{i=1}^{n} (\mathbf{Y}_{\text{pred}, i} - \mathbf{Y}_i)^2,
% \quad
% \mathcal{L}_{\text{MAE}} = \frac{1}{n} \sum_{i=1}^{n} |\mathbf{Y}_{\text{pred}, i} - \mathbf{Y}_i|,
% \end{equation}

% \noindent where $n$ is the number of predictions. The combined time-series objective is:

% \begin{equation}
% \mathcal{L}_{\text{Time}} = \mathcal{L}_{\text{MSE}} + \mathcal{L}_{\text{MAE}}.
% \end{equation}

% \subsection{Contrastive Learning Objective for Causal Learning}

% To enhance robustness, we use counterfactual learning with three objectives: cross-entropy, triplet loss, and hinge loss. Diverse counterfactuals are generated by altering sentiment-bearing components of event scripts. The counterfactual sampling mechanism ensures both intra-type and inter-type diversity, enabling the model to differentiate causal from non-causal factors.

% The counterfactual learning objective is given by:

% \begin{equation}
% \mathcal{L}_{\text{Causal}} = \mathcal{L}_{\text{CE}} + \mathcal{L}_{\text{TL}} + \mathcal{L}_{\text{HM}},
% \end{equation}

% \noindent and the total training loss is a weighted sum of time-series and causal objectives:

% \begin{equation}
% \mathcal{L}_{\text{Total}} = \beta \cdot \mathcal{L}_{\text{Time}} + (1 - \beta) \cdot \mathcal{L}_{\text{Causal}}.
% \end{equation}

% \subsection{Training Strategy}

% We use a two-phase training strategy:

% \textbf{Phase 1:} Train only the time-series components (MOMENT, GPT-2, and residual layer) with MSE and MAE objectives, while freezing RoBERTa.

% \textbf{Phase 2:} Fine-tune all parameters, including RoBERTa, using both time-series and causal objectives to learn cross-modality relationships.

\section{CAMEP Architecture}
\label{sec:model_architecture}
The CAMEP model integrates both textual and time-series information through a structured architecture consisting of a textual encoder, a time-series encoder, and a forecasting decoder, as depicted in Fig. \ref{fig:model_architecture}. Each component is detailed below.

\subsection{Textual Modality Encoder (\( \textbf{CAMEF}_{\text{Textual}}\))}
\label{sec:textual_encoder}
We encode event scripts using RoBERTa \cite{liu2020roberta}. Given an input script $\mathbf{E}_i = {w_1, w_2, \dots, w_m}$, RoBERTa produces contextual token embeddings:

\begin{equation} 
\{ \mathbf{h}_1, \mathbf{h}_2, \dots, \mathbf{h}_m \} = \text{RoBERTa}(\{w_1, w_2, \dots, w_m\}),
\end{equation}

\noindent where $\mathbf{h}_j \in \mathbb{R}^{1 \times 768}$ is the embedding of token $w_j$. Each embedding is passed through a projection network with three linear layers and GELU activations:

\begin{equation} 
\mathbf{e}_j = \mathbf{W}^{(3)} \cdot \text{GELU}(\mathbf{W}^{(2)} \cdot \text{GELU}(\mathbf{W}^{(1)} \mathbf{h}_j + \mathbf{b}^{(1)}) + \mathbf{b}^{(2)}) + \mathbf{b}^{(3)},
\end{equation}

\noindent where $\mathbf{W}^{(1)} \in \mathbb{R}^{768 \times 1024}$, $\mathbf{W}^{(2)} \in \mathbb{R}^{1024 \times 1024}$, and $\mathbf{W}^{(3)} \in \mathbb{R}^{1024 \times 1024}$. The final encoding vector $\mathbf{E}_i$ for the script is computed as the average of the transformed embeddings, 
$\mathbf{E}_i = \frac{1}{m} \sum_{j=1}^m \mathbf{e}_j$.



% This averaging step condenses the sequence of embeddings into a single vector, providing a summarized representation of the event script.

% \subsection{Textual Modality Encoder}

% The event scripts are transformed into vector representations using the RoBERTa model \cite{liu2020roberta}, which is an efficient textual information encoder based on transformer architecture. Given an input event script from the event collection defined in Sec. \ref{sec:definition}, denoted as $\mathbf{E}_i := {w_1, w_2, \dots, w_m}$, the tokens are encoded into corresponding vector representations. The output from RoBERTa provides a sequence of token embeddings:

% \begin{equation}
% \{\mathbf{w}_1, \mathbf{w}_2, \dots, \mathbf{w}_m\} = \textbf{RoBERTa}(\{w_1, w_2, \dots, w_m\}). 
% \end{equation}
% \noindent To generate the final encoding vector for the event script, we apply average pooling over the sequence of token embeddings:

% \begin{equation}
%     \mathbf{E}_i = \frac{1}{m} \sum_{j=1}^{m} \mathbf{w}_j,
% \end{equation}

% \noindent where, $\mathbf{\mathcal{E}}_i$ represents the final pooled vector that summarizes the event script $\mathbf{E}_i$, where $m$ is the number of tokens in the script.

\subsection{Time-Series Modality Encoder \\ ( \( \textbf{CAMEF}_{\text{Time-Series}} \) )} 
\label{sec:time-series_encoder}
To encode the time series data, we employ a pretrained time series encoder, MOMENT \cite{goswami2024moment}, which generates a fixed-dimensional vector for an input time series segment. Subsequently, we design a multi-residual layer to further refine the encoding vectors, as shown below:

\begin{equation}
    \mathbf{\mathcal{X}}_i = \textbf{MOMENT}(\{ X_1, X_2, \dots, X_n\}),
\end{equation}

\noindent where $\mathbf{\mathcal{X}}_i \in \mathbb{R}^{d}$ is the encoded vector for the input time series segment $\{ X_1, X_2, \dots, X_n\}$, and \(d\) represents the dimensionality of the encoded vector. To enhance this representation, we introduce a multi-residual projection layer:

\begin{equation}
    \mathbf{Z}_i = \mathbf{\mathcal{X}}_i + f_{\text{residual}}(\mathbf{\mathcal{X}}_i),
\end{equation}

\noindent where \(f_{\text{residual}}(\mathbf{\mathcal{X}}_i)\) represents the transformation applied through the residual projection layer, which consists of multiple linear layers interleaved with GELU activations:

\begin{equation}
    f_{\text{residual}}(\mathbf{\mathcal{X}}_i) = \mathbf{W}_3 \cdot \text{GELU}(\mathbf{W}_2 \cdot \text{GELU}(\mathbf{W}_1 \cdot \mathbf{\mathcal{X}}_i + \mathbf{b}_1) + \mathbf{b}_2) + \mathbf{b}_3,
\end{equation}

\noindent where \(\mathbf{W}_1, \mathbf{W}_2 , \mathbf{W}_3 \in \mathbb{R}^{1024 \times 1024}\) are the weight matrices, and \(\mathbf{b}_1, \mathbf{b}_2, \mathbf{b}_3 \in \mathbb{R}^{1024}\) are the respective biases. Finally, \(\mathbf{Z}_i \in \mathbb{R}^{1024}\) serves as the refined vector for the time series segment.


\subsection{Feature Fusion and Time Series Decoder}

After obtaining the encoded vectors from both the textual data and the time series data, denoted as $\mathbf{E_i}$ and $\mathbf{Z}_i$, respectively, we concatenate these two vectors to create a unified representation:

\begin{equation}
    \mathbf{E}_{combined} = \textbf{concat}(\mathbf{E_i}, \mathbf{Z}_i),
\end{equation}

\noindent where $\mathbf{E}_{combined}$ captures both the semantic information from the macroeconomic releases and the temporal patterns from the time series data. We employ GPT-2 \cite{radford2019language} as the decoder to decode the fused vector by leveraging its effective auto-regressive ability:

\begin{equation}
\mathbf{H}^{(l)} = f_{\text{GPT2\_layer}}^{(l)}(\mathbf{H}^{(l-1)}),
\end{equation}

\noindent where \(\mathbf{H}^{(0)} = \mathbf{E}_{\text{combined}}\), and \(f_{\text{GPT2\_layer}}^{(l)}\) represents the transformation function of the \(l\)-th GPT-2 layer, where \( l=12\). After the final layer, the output is normalized using a layer normalization function:

\begin{equation}
\label{eqn:output_embedding}
\mathbf{H}_{\text{final}} = \text{LayerNorm}(\mathbf{H}^{(l)}),
\end{equation}

\noindent The final output \(\mathbf{H}_{\text{final}}\) is then used to generate predictions based on the combined multi-modal information.


\subsection{Time-Series Forecasting Post-Regressor and Learning Objectives}

We designed a \textbf{Post-Regressor} that applies a linear transformation to the concatenated vector \(\mathbf{H}_{\text{final}}\), followed by GELU activation and a dropout layer with a rate of 0.1:

\begin{equation}
\mathbf{R}^{(k)} = \text{GELU}(\mathbf{W}^{(k)} \cdot \mathbf{R}^{(k-1)} + \mathbf{b}^{(k)}),
\end{equation}

\noindent where \(\mathbf{R}^{(0)} = \mathbf{H}_{\text{final}}\), \(\mathbf{W}^{(k)}\) and \(\mathbf{b}^{(k)}\) are the weight matrix and bias of the \(k\)-th linear layer, respectively. \(k = 4 \) is the total number of layers in the regressor. The final linear layer maps the representation to a vector of shape \((d \times \text{pred\_len})\), where \(d\) is the forecast dimensionality and \(\text{pred\_len}\) is the number of predicted time steps:

\begin{equation}
\mathbf{\hat{Y}} = \mathbf{W}_{\text{out}} \cdot \mathbf{R}^{(K)} + \mathbf{b}_{\text{out}},
\end{equation}

\noindent where \(\mathbf{\hat{Y}}\), represents the predicted time series values.

\textbf{Learning Objectives for Time Series:} We employ a combination of Mean Squared Error (MSE) loss and Mean Absolute Error (MAE) loss to optimize the model. The MSE loss minimizes the squared differences between the predicted time series values, \(\mathbf{\hat{Y}}\), and the ground truth values, \(\mathbf{Y}\), while the MAE loss minimizes the absolute differences. These are defined as:

\begin{equation}
\mathcal{L}_{\text{MSE}} = \frac{1}{n} \sum_{i=1}^{n} (\mathbf{\hat{Y}}_i - \mathbf{Y}_i)^2, \quad
\mathcal{L}_{\text{MAE}} = \frac{1}{n} \sum_{i=1}^{n} |\mathbf{\hat{Y}}_i - \mathbf{Y}_i|,
\end{equation}

\noindent where \(n\) is the number of predicted values (e.g., 35, 70, or 140, as defined in Section \ref{sec:exp_setting}). The total loss function combines both objectives to balance optimization for large and small errors:

\begin{equation}
\label{eqn:time_objective}
\mathcal{L}_{\text{Time}} = \mathcal{L}_{\text{MSE}} + \mathcal{L}_{\text{MAE}}.
\end{equation}


% \subsection{Contrastive Learning Objective for Causal Learning}

% Causal learning aims to enhane the robustness of our model, by learning to detect the correct event script among sampled counterfactual events (CEs). We combinationally utlize several objective function to achieve it: 1) cross entropy, 2) triplet loss, and 3) hinge loss.

% To ensure diversity in the sampled counterfactual events, we designed a \textbf{Diverse Counterfactual Event Sampling Mechanism} to sample two categories of CEs for a ground-truth event: (1) counterfactual events generated based on the ground truth event, as described in Sec. \ref{sec:CE_generation}; and (2) counterfactual events generated from another event type that occurred on the closest date. This mechanism not only enables the model to detect the ground truth event within the same type but also identifies the ground truth event type across a variety of event types. We denote all the diversely sampled counterfactual events, denoted as $\mathbb{E}^{CF}:=\{\mathcal{E}^{CF}_{1}, \mathcal{E}^{CF}_{2},\ldots,\mathcal{E}^{CF}_{|\mathbb{E}^{CF}|}\}$.

% We firstly use the textual modality of CAMEF model (i.e., the RobERTa model) to encode the ground-truth event $\mathcal{E}^{GT}$and all the counterfactual events from $\mathbb{E}^{CF}$ as:

% \begin{equation}
%     \mathbf{E}^{GT} = \textbf{CAMEF}_{Textual}(\mathcal{E}^{GT}),
% \end{equation}

% \begin{equation}
%     \{\mathbf{E}^{CF}_i \}_{i=1}^{|\mathbb{E}^{CF}|} = \textbf{CAMEF}_{Textual}(\{ \mathcal{E}^{CF}_{i}\}|\theta)_{i=1}^{|\mathbb{E}^{CF}|}.
% \end{equation}

% For \textbf{Cross-Entropy Objective}, the loss is computed as the following:

% \begin{equation}
% \mathcal{L}_{\text{CE}} = - \log \frac{\exp(\mathbf{H}_{\text{final}} \cdot \mathbf{E}^{GT})}{\exp(\mathbf{H}_{\text{final}} \cdot \mathbf{E}^{GT}) + \sum_{i=1}^{|\mathbb{E}^{CF}|} \exp(\mathbf{H}_{\text{final}} \cdot \mathbf{E}^{CF}_i)},
% \end{equation}

% \noindent where \( \mathbf{H}_{\text{final}} \) represents the output embedding from Equation \ref{eqn:output_embedding}, \( \mathbf{E}^{GT} \) is the embedding of the ground-truth event, and \( \mathbf{E}^{CF}_i \) are the embeddings of the diversely sampled counterfactual events.

% For \textbf{Triplet Loss}, the objective is employed to further refine the embedding space by ensuring that the ground-truth event embedding \( \mathbf{E}^{GT} \) is closer to the output embedding \( \mathbf{H}_{\text{final}} \) compared to any counterfactual event embedding \( \mathbf{E}^{CF}_i \):

% \begin{equation}
% \mathcal{L}_{\text{TL}} = \max(0, \mathbf{H}_{\text{final}} \cdot \mathbf{E}^{GT} - \mathbf{H}_{\text{final}} \cdot \mathbf{E}^{CF}_i + \alpha),
% \end{equation}

% \noindent where \( \text{sim}(\cdot, \cdot) \) denotes the similarity between two embeddings, and \( \alpha \) is the margin that enforces a minimum difference between the positive and negative pairs. The goal of the triplet loss is to maximize the similarity between the source embedding \( \mathbf{H}_{\text{final}} \) and the ground-truth embedding \( \mathbf{E}^{GT} \), while ensuring that the counterfactual embedding \( \mathbf{E}^{CF}_i \) is sufficiently distant from the source by at least a margin \( \alpha \).

% In addition, we propose a third objective \textbf{Hard Margin Loss} is utilized to ensure that the ground-truth event embedding \( \mathbf{E}^{GT} \) is closer to the source embedding \( \mathbf{H}_{\text{final}} \) than the counterfactual event embeddings \( \mathbf{E}^{CF}_i \), by at least a margin \( \alpha \). The hinge loss is computed as:

% \begin{equation} 
% \mathcal{L}_{\text{HM}} = \left( \|\mathbf{H}_{\text{final}} - \mathbf{E}^{GT}\|^2 \right) + \left( \max(0, \alpha - \|\mathbf{H}_{\text{final}} - \mathbf{E}^{CF}\|) \right)^2,
% \end{equation}

% \noindent where \( \mathbf{H}_{\text{final}} \) is the final source embedding from Equation \ref{eqn:output_embedding}, \( \mathbf{E}^{GT} \) is the ground-truth event embedding, and \( \mathbf{E}^{CF} \) represents the counterfactual event embeddings. The margin \( \alpha \) encourages sufficient separation between the source embedding and the counterfactual event embeddings.

% The total loss function for causal learning based on the counterfactual events is the combination of the three objectives:

% \begin{equation}
% \label{eqn:causal_objective}
% \mathcal{L}_{\text{Causal}} = \mathcal{L}_{\text{CE}} + \mathcal{L}_{\text{TL}} + \mathcal{L}_{\text{HM}},
% \end{equation}

% \noindent Hence, the \textbf{Total Training Loss} for the model are defined as:

% \begin{equation}
% \mathcal{L}_{\text{Total}} = \beta \cdot \mathcal{L}_{\text{Time}} + (1 - \beta) \cdot \mathcal{L}_{\text{Causal}},
% \end{equation}


% \noindent where \( \mathcal{L}_{\text{Time}} \) represents the combined loss objective for time series forecasting, as defined in Equation \ref{eqn:time_objective}, and \( \mathcal{L}_{\text{Causal}} \) refers to the combined loss for counterfactual-based causal learning from Equation \ref{eqn:causal_objective}. The hyperparameter \( \beta \) controls the balance between these two objectives.


% \subsection{Time-Series Modality Encoder with MOMENT}

% To capture temporal dependencies in the time-series data, we employ a combination of MOMENT \cite{goswami2024moment} and GPT-2 \cite{radford2019language}. Given a time series $\{ X_1, X_2, \dots, X_n \}$, MOMENT first produces an initial vector:

% \begin{equation}
%     \mathbf{\mathcal{X}}_i = \textbf{MOMENT}(\{ X_1, X_2, \dots, X_n\}),
% \end{equation}

% \noindent where $\mathbf{\mathcal{X}}_i \in \mathbb{R}^{1024}$. This encoding is refined through a multi-residual projection layer:

% \begin{equation}
%     \mathbf{Z}_i = \mathbf{\mathcal{X}}_i + f_{\text{residual}}(\mathbf{\mathcal{X}}_i),
% \end{equation}

% \noindent with \(f_{\text{residual}}\) incorporating non-linear transformations via linear layers and GELU activations to capture complex temporal features. 

% % The refined vector $\mathbf{Z}_i$ is subsequently processed through 12 transformer layers of GPT-2 to capture autoregressive dependencies. A second residual layer further processes the output from GPT-2, with layer normalization applied to yield the final time-series representation:

% % \begin{equation}
% % \mathbf{H} = \text{LayerNorm}(\mathbf{Z}^{(L)} + f_{\text{second\_residual}}(\mathbf{Z}^{(L)})),
% % \end{equation}

% % \noindent where \(L = 12\), representing the total number of GPT-2 layers.

% \subsection{Multi-Modality Fusion, GPT-2 Decoder and Output Layer}

% The text and time-series embeddings, \(\mathbf{E}\) and \(\mathbf{H}\), are concatenated to form a unified representation:

% \begin{equation}
% \mathbf{F}_{\text{input}} = \text{concat}(\mathbf{E}, \mathbf{Z}),
% \end{equation}

% \noindent where \(\mathbf{F}_{\text{input}} \in \mathbb{R}^{1536}\) combines both semantic and temporal information. The fused vector is then passed through a series of linear transformations with GELU activations and dropout, progressively reducing the dimensionality and producing the final prediction:

% \begin{equation}
% \mathbf{Y}_{\text{pred}} = \mathbf{F}_{\text{output}} + f_{\text{residual}}(\mathbf{H}),
% \end{equation}

% \noindent where $f_{\text{residual}}(\mathbf{H})$ introduces residual connections that help retain crucial information from the time-series modality, enhancing the final prediction.

% \subsection{Time-Series Learning Objectives}

% The model’s time-series predictions, \(\mathbf{Y}_{\text{pred}}\), are optimized through a combination of Mean Squared Error (MSE) and Mean Absolute Error (MAE) objectives:

% \begin{equation}
% \mathcal{L}_{\text{MSE}} = \frac{1}{n} \sum_{i=1}^{n} (\mathbf{Y}_{\text{pred}, i} - \mathbf{Y}_i)^2,
% \quad
% \mathcal{L}_{\text{MAE}} = \frac{1}{n} \sum_{i=1}^{n} |\mathbf{Y}_{\text{pred}, i} - \mathbf{Y}_i|,
% \end{equation}

% \noindent where $n$ is the total number of predictions. The combined loss encourages accurate predictions by balancing sensitivity to large and small errors:

% \begin{equation}
% \mathcal{L}_{\text{Time}} = \mathcal{L}_{\text{MSE}} + \mathcal{L}_{\text{MAE}}.
% \end{equation}


\begin{table*}[ht]
\caption{Financial Forecasting Results (MSE and MAE Scores) for CAMEF and Baselines Across Various Financial Assets: S\&P500 (SPX), Dow Industrials (INDU), Nasdaq100 (NDX) Index, US 1-Month Treasury Bond (USGG1M), and US 5-Year Treasury Bond (USGG5YR).}
\label{tab:main_results}
\resizebox{\textwidth}{!}{%
\begin{tabular}{c|c|cl|cl|cl|cl|cl}
\hline
\multicolumn{1}{l|}{\multirow{2}{*}{\textbf{Model / Datasets}}} & \multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Forecasting\\ Length\end{tabular}}} & \multicolumn{2}{c|}{\textbf{SP500 (SPX)}} & \multicolumn{2}{c|}{\textbf{Dow Industrial (INDU)}} & \multicolumn{2}{c|}{\textbf{NASDAQ (NDX)}} & \multicolumn{2}{c|}{\textbf{USGG1M}} & \multicolumn{2}{c}{\textbf{USGG5YR}} \\ \cline{3-12} 
\multicolumn{1}{l|}{} &  & MSE & \multicolumn{1}{c|}{MAE} & MSE & \multicolumn{1}{c|}{MAE} & MSE & \multicolumn{1}{c|}{MAE} & MSE & \multicolumn{1}{c|}{MAE} & MSE & \multicolumn{1}{c}{MAE} \\ \hline
\multirow{3}{*}{\textbf{ARIMA}} & 35 & 0.0032628 & 0.0308016 & 0.0121513 & 0.0523810 & {\ul 0.0065253} & 0.0435694 & 0.0072223 & 0.0262512 & 0.0038973 & 0.0340227 \\
 & 70 & 0.0035361 & 0.0352004 & 0.0139245 & 0.0656002 & {\ul 0.0082324} & 0.0520933 & 0.0028710 & 0.0304132 & 0.0039441 & 0.0366630 \\
 & 140 & 0.0051080 & 0.0439935 & 0.0219147 & 0.0793471 & {\ul 0.0118692} & 0.0665155 & 0.0089949 & 0.0338275 & 0.0050746 & 0.0455617 \\ \hline
\multirow{3}{*}{\textbf{DLinear}} & 35 & 0.0144331 & 0.0896910 & 0.0395136 & 0.1380539 & 0.0183989 & 0.0999178 & 0.0108576 & 0.0706170 & 0.0147211 & 0.0876379 \\
 & 70 & 0.0120578 & 0.0817373 & 0.0406573 & 0.1282699 & 0.0189431 & 0.0972439 & 0.0093591 & 0.0678380 & 0.0146430 & 0.0881574 \\
 & 140 & 0.0178138 & 0.0931039 & 0.0747210 & 0.1724027 & 0.0344153 & 0.1287803 & 0.0101465 & 0.0645396 & 0.0179185 & 0.0977673 \\ \hline
\multirow{3}{*}{\textbf{Autoformer}} & 35 & 0.0068136 & 0.0540556 & 0.0277636 & 0.0975948 & 0.0135249 & 0.0796486 & 0.0047505 & 0.0388076 & 0.0092717 & 0.0622862 \\
 & 70 & 0.0088997 & 0.0628341 & 0.0375279 & 0.1185710 & 0.0185264 & 0.0933398 & 0.0065554 & 0.0471531 & 0.0113750 & 0.0727373 \\
 & 140 & 0.0158248 & 0.0829188 & 0.0772580 & 0.1640365 & 0.0334504 & 0.1262163 & 0.0086212 & 0.0533337 & 0.0152298 & 0.0875821 \\ \hline
\multirow{3}{*}{\textbf{FEDformer}} & 35 & 0.0072377 & 0.0576221 & 0.0304808 & 0.1044447 & 0.0128668 & 0.0758742 & 0.0063596 & 0.0519141 & 0.0094995 & 0.0494306 \\
 & 70 & 0.0088056 & 0.0621386 & 0.0399824 & 0.1229772 & 0.0171008 & 0.0889995 & 0.0062841 & 0.0448822 & 0.0099615 & 0.0664197 \\
 & 140 & 0.0157429 & 0.0819469 & 0.0786426 & 0.1677705 & 0.0313433 & 0.1214097 & 0.0083784 & 0.0518365 & 0.0131231 & 0.0782861 \\ \hline
\multirow{3}{*}{\textbf{iTransformer}} & 35 & 0.0064209 & 0.0516341 & 0.0270860 & 0.0927605 & 0.0125008 & 0.0751656 & 0.0011000 & 0.0155975 & \multicolumn{1}{l}{0.0056660} & \textbf{0.0183524} \\
 & 70 & 0.0069612 & 0.0540920 & 0.0304566 & 0.1038424 & 0.0151214 & 0.0816262 & 0.0021811 & 0.0221315 & \textbf{0.0011721} & \textbf{0.0226975} \\
 & 140 & 0.0128021 & 0.0718771 & 0.0680991 & 0.1486782 & 0.0254479 & 0.1047119 & 0.0052255 & 0.0327429 & \textbf{0.0017441} & \textbf{0.0282537} \\ \hline
\multirow{3}{*}{\textbf{PatchTST}} & 35 & 0.0063304 & 0.0507462 & 0.0293131 & 0.0989455 & 0.0122679 & 0.0764552 & 0.0012060 & 0.0163610 & 0.0063078 & 0.0520734 \\
 & 70 & 0.0072471 & 0.0547738 & \multicolumn{1}{l}{0.0339444} & 0.1116023 & 0.0153753 & 0.0824677 & 0.0021643 & 0.0223036 & 0.0079617 & 0.0606007 \\
 & 140 & 0.0130219 & 0.0712171 & 0.0688582 & 0.1452592 & 0.0256001 & 0.1047749 & 0.0054544 & 0.0341517 & 0.0118553 & 0.0747537 \\ \hline
\multirow{3}{*}{\textbf{GPT4MTS}} & 35 & \multicolumn{1}{l}{0.00795088} & 0.0674255 & {\ul 0.0026558} & 0.0417553 & 0.0011035 & 0.0240469 & 0.0017016 & 0.0309320 & 0.0028713 & 0.0389277 \\
 & 70 & \multicolumn{1}{l}{0.00171038} & 0.0305950 & {\ul 0.0027033} & {\ul 0.0393112} & 0.0016205 & 0.0336116 & 0.0019098 & 0.0279222 & 0.0023371 & 0.0354777 \\
 & 140 & \multicolumn{1}{l}{0.00212612} & {\ul 0.0330497} & {\ul 0.0045029} & {\ul 0.0458267} & 0.0025175 & 0.0341671 & 0.0013648 & 0.0260887 & 0.0037004 & 0.0449272 \\ \hline
\multirow{3}{*}{\textbf{TEST}} & 35 & \multicolumn{1}{l}{{\ul 0.00073333}} & {\ul 0.0199733} & \multicolumn{1}{l}{0.0026572} & \textbf{0.0296887} & 0.0006593 & {\ul 0.0194091} & {\ul 0.0003252} & {\ul 0.0137511} & {\ul 0.0013633} & 0.0265036 \\
 & 70 & \multicolumn{1}{l}{{\ul 0.00078762}} & {\ul 0.0205412} & 0.0088903 & 0.0599179 & 0.0010678 & {\ul 0.0248594} & {\ul 0.0010515} & {\ul 0.0182706} & 0.0034630 & 0.0415002 \\
 & 140 & \multicolumn{1}{l}{{\ul 0.00278572}} & 0.0467150 & 0.0070749 & 0.0557744 & 0.0020130 & {\ul 0.0362325} & {\ul 0.0006995} & {\ul 0.0207850} & 0.0024356 & 0.0375164 \\ \hline
\multirow{3}{*}{\textbf{CAMEF}} & 35 & \multicolumn{1}{l}{\textbf{0.00048860}} & \textbf{0.0154050} & \multicolumn{1}{l}{\textbf{0.0025349}} & {\ul 0.0366245} & \multicolumn{1}{l}{\textbf{0.0005468}} & \textbf{0.0178845} & \multicolumn{1}{l}{\textbf{0.0002883}} & \textbf{0.0118010} & \multicolumn{1}{l}{\textbf{0.0013234}} & {\ul 0.0260618} \\
 & 70 & \multicolumn{1}{l}{\textbf{0.00064780}} & \textbf{0.0178691} & \multicolumn{1}{l}{\textbf{0.0025042}} & \textbf{0.0365500} & \multicolumn{1}{l}{\textbf{0.0005814}} & \textbf{0.0162882} & \multicolumn{1}{l}{\textbf{0.0004402}} & \textbf{0.0139699} & \multicolumn{1}{l}{{\ul 0.0020701}} & {\ul 0.0326371} \\
 & 140 & \multicolumn{1}{l}{\textbf{0.0010756}} & \textbf{0.0210284} & \multicolumn{1}{l}{\textbf{0.0039313}} & \textbf{0.0383459} & \multicolumn{1}{l}{\textbf{0.0010159}} & \textbf{0.0207716} & \multicolumn{1}{l}{\textbf{0.0004938}} & \textbf{0.0148485} & \multicolumn{1}{l}{{\ul 0.0022458}} & {\ul 0.0336680} \\ \hline
\end{tabular}
}
\end{table*}

\subsection{Counterfactual Events Sampling and Causal Learning Objective}

Causal learning enhances the robustness of the CAMEF model by enabling it to identify the correct event script among sampled counterfactual events (CEs). To achieve this, we first design a \textbf{Diverse Counterfactual Event Sampling Mechanism}, which generates two types of CEs. These counterfactuals, along with their corresponding time-series data, are then encoded using the textual and time-series modalities of CAMEF. This process helps the model learn causal relationships between events and their corresponding time-series movements.

\subsubsection{Diverse Counterfactual Event Sampling Mechanism}
We propose a \textbf{Diverse Counterfactual Event Sampling Mechanism} to enhance the model’s ability to both identify the ground-truth event and distinguish between different event types. This mechanism is designed with two objectives: (1) to help the model recognize the ground-truth event among similar counterfactuals of the same type, and (2) to enable the model to differentiate between events of different types. 

To achieve these objectives, we generate two categories of counterfactual events for each factual event:
\begin{enumerate}
    \item \textbf{Identical Type Sampling:} Counterfactual events of the same type as the ground-truth event, created by modifying sentiment-relevant components and key numerical variables, as detailed in Sec.~\ref{sec:CE_generation}.
    \item \textbf{Diverse Type Sampling:} Counterfactual events of a different type, sampled by substituting the ground-truth event with 5 other event type occurring on the closest date.
\end{enumerate}

This mechanism provides a diverse set of counterfactual events, collectively denoted as \(\mathbb{E}^{CF} := \{\mathcal{E}^{CF}_1, \mathcal{E}^{CF}_2, \ldots, \mathcal{E}^{CF}_{|\mathbb{E}^{CF}|}\}\), we set the total number of diverse-type samples to be 5, and the default number of identical-type samples to be 10 as introduced Sec.~\ref{sec:CE_generation}. 

\subsubsection{Causal Learning Objective}

The causal learning process utilizes both the textual (see Sec. \ref{sec:textual_encoder}) and time-series (see Sec. \ref{sec:time-series_encoder}) encoders of CAMEF to capture the relationships between events and market movements. The textual encoder is used to encode both the ground-truth event \(\mathcal{E}^{GT}\) and the sampled CEs \(\mathcal{E}^{CF}\):

\begin{equation}
\{\mathbf{P}^{GT}, \mathbf{P}^{CF}_1, \dots, \mathbf{P}^{CF}_{|\mathbb{E}^{CF}|}\} = \textbf{CAMEF}_{\text{Textual}}(\{\mathcal{E}^{GT}\} \cup \{\mathcal{E}^{CF}_i\}_{i=1}^{|\mathbb{E}^{CF}|}),
\end{equation}

\noindent where \(\mathbf{P}^{GT}\) represents the embedding of the ground-truth event, and \(\{\mathbf{P}^{CF}_i\}_{i=1}^{|\mathbb{E}^{CF}|}\) represents the embeddings of the sampled counterfactual events.

The time-series encoder is used to encode the historical time-series segment \(\mathcal{X}\) aligned with the ground-truth event, resulting in the time-series embedding:

\begin{equation}
\mathbf{T} = \textbf{CAMEF}_{\text{Time-Series}}(\mathcal{X}).
\end{equation}

\noindent \textbf{Triplet Loss:} The triplet loss is applied to enforce that the ground-truth event embedding \(\mathbf{P}^{GT}\) is closer to the time-series embedding \(\mathbf{T}\) than any counterfactual event embedding \(\mathbf{P}^{CF}_i\), by a margin \(\alpha\) (set to 1.0):

\begin{equation}
\mathcal{L}_{\text{Causal-TL}} = \max\big(0, d(\mathbf{P}^{GT}, \mathbf{T}) - d(\mathbf{P}^{CF}_i, \mathbf{T}) + \alpha\big),
\end{equation}

\noindent where \(d(\cdot, \cdot)\) denotes the distance between two embeddings (e.g., cosine similarity or Euclidean distance). This loss function encourages the model to capture the causal relationships between events and time-series movements by penalizing counterfactual events that deviate from the causal signal of the ground-truth event.

The combination of diverse counterfactual sampling and causal learning ensures that CAMEF effectively learns the true causal drivers of financial market movements, improving its robustness and predictive power.


% \textbf{Cross-Entropy Loss (Explored but Not Adopted):} We also tested a cross-entropy objective, which aims to maximize the log-likelihood of the ground-truth event being selected over the counterfactual events:

% \begin{equation}
% \mathcal{L}_{\text{Causal-CE}} = - \log \frac{\exp(\mathbf{H}_{\text{final}} \cdot \mathbf{E}^{GT})}{\exp(\mathbf{H}_{\text{final}} \cdot \mathbf{E}^{GT}) + \sum_{i=1}^{|\mathbb{E}^{CF}|} \exp(\mathbf{H}_{\text{final}} \cdot \mathbf{E}^{CF}_i)}.
% \end{equation}

% However, this objective did not perform well in our experiments, where triplet loss served as the primary objective for causal learning.

\noindent \textbf{Total Loss:} The overall training loss for CAMEF is defined as:

\begin{equation}
\mathcal{L}_{\text{Total}} = \mathcal{L}_{\text{Time}} + \mathcal{L}_{\text{Causal-TL}},
\end{equation}

\noindent where \(\mathcal{L}_{\text{Time}}\) is the objective for time series forecasting, as defined in Equation~\ref{eqn:time_objective}.


% \subsection{Training Strategy}

% We adopt a two-phase training strategy:

% \begin{itemize}
%     \item \textbf{Phase 1:} Train only the time-series components (MOMENT, GPT-2, and residual layers) with MSE and MAE objectives, while freezing the RoBERTa encoder. This phase focuses on capturing temporal dependencies accurately.
%     \item \textbf{Phase 2:} Fine-tune all parameters, including RoBERTa, while optimizing both time-series and causal objectives. This phase enables the model to learn cross-modal relations and causal dependencies effectively.

% \end{itemize}





\section{Experiments}

In this section, we evaluate CAMEF by addressing the following key questions: \textbf{RQ1. Accuracy:} How accurately does CAMEF forecast fina ncial market based on events? \textbf{RQ2. Model Effectiveness:} How do different components enhance CAMEF's predictive performance?  \textbf{RQ3. Event Analysis:} Which types of events exhibit stronger influences to financial market?

\subsection{Experimental Settings} 
\label{sec:exp_setting}
\subsubsection{Datasets.}

We utilized the collected event scripts and time-series data as outlined in Sec. \ref{sec:data_acquisition} and detailed in Appendix B. The dataset was divided into training, validation, and testing sets in a 6:2:2 ratio. The training set was used to train the models, while the validation set was used for convergence checking and early stopping to prevent overfitting. The final results, based on the test set, are reported in Table \ref{tab:main_results}.
% For the \textbf{time-series modality}, we evaluate the performance of CAMEF using six multi-modal time-series datasets across three financial categories: \textbf{1) Stock Indices:} S\&P 500 Index (SPX), NASDAQ Index (NDX), Dow Industrial Index (INDU), and Russell 2000 Index (RTY); \textbf{2) US Treasury:} US 1 Month Treasury (USGG1M) and US 5 Year Treasury (USGG5YR); and \textbf{3) Commodity:} Gold Price in US Dollars (XAU). Unlike previous AI-based stock forecasting studies \cite{10.1145/3503161.3548380,ouyang-etal-2024-modal,shah-etal-2023-trillion,Chen_2021_ICCV,pmlr-v162-zhou22g,Jia_Wang_Zheng_Cao_Liu_2024}, we collect and utilize high-frequency time-series data, which provides dense temporal information and closely reflects real-world trading dynamics.

% For the \textbf{textual (event) modality}, we follow prior financial studies and collect event scripts across eight categories, including: \textbf{FOMC Meeting Unemployment Insurance Releases, Initial Unemployment Claims, GDP Advance Releases, CPI Report, and PPI Report}. These scripts were crawled from the official source websites, as detailed in Section \ref{sec:data_acquisition}. We also leverage the LLaMa-3 model to generate 10 counterfactual event scripts for each ground-truth event script, the statistics are included in Table \ref{tab:data_summary}A. We aligned each time-series with the eight categories of events based on their time stamps as explained in Section \ref{sec:definition}.

\subsubsection{Baselines.}
To evaluate our proposed method, we compare it against both \textbf{uni-modal} and \textbf{multi-modal} time series forecasting approaches. For the \textbf{uni-modal baselines}, we considered the traditional yet robust ARIMA model \cite{10.5555/561899}, the linear neural model DLinear \cite{Zeng_Chen_Zhang_Xu_2023}, and several state-of-the-art transformer-based time-series models, including AutoFormer \cite{Chen_2021_ICCV}, FEDformer \cite{pmlr-v162-zhou22g}, and iTransformer \cite{liu2024itransformer}. For the \textbf{multi-modal baselines}, we included TEST \cite{sun2024test} and GPT4MTS \cite{Jia_Wang_Zheng_Cao_Liu_2024}.

\subsubsection{Test Settings}

We evaluate the baselines and CAMEF across three time horizons—short, medium, and long run, to simulate real investment behavior. For each aligned pair of event and time-series data \(([\mathcal{X}_{i-\tau:i+\tau} \mapsto \mathcal{E}_i])\) as defined in Sec.~\ref{sec:definition}, we use the event script (\(\mathcal{X}\)) and the time-series segment preceding the event time point \(i\), i.e., \(\mathcal{X}_{i-\tau:i}\), to forecast the future time-series segment \(\mathcal{X}_{i+1:i+\tau}\). The value of \(\tau\) is adjusted based on the time horizon: we set \(\tau\) to 35, 70, and 140 for short, medium, and long-term forecasts, respectively. These correspond to 175 minutes (about half a trading day), 350 minutes (about one trading day), and 700 minutes (about two trading days).

\begin{table*}[th]
\caption{Ablation Study Results (MSE) Evaluating the CAMEF Model Components at Forecasting Lengths of 35, 70, and 140.}
\label{tab:ablation}
\resizebox{\textwidth}{!}{%
\begin{tabular}{ccccc|rll|lll|lll|lll|lll|lll}
\hline
\multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l|}{} & \multicolumn{3}{c|}{\textbf{SPX}} & \multicolumn{3}{c|}{\textbf{INDU}} & \multicolumn{3}{c|}{\textbf{NDX}} & \multicolumn{3}{c|}{\textbf{USGG1M}} & \multicolumn{3}{c|}{\textbf{USGG5YR}} & \multicolumn{3}{c}{\textbf{AVERAGE}} \\ \cline{6-23} 
\multicolumn{1}{l}{\multirow{-2}{7.5mm}{\textbf{Textual}}} & \multicolumn{1}{l}{\multirow{-2}{7.5mm}{\textbf{Causal}}} & \multicolumn{1}{l}{\multirow{-2}{7.9mm}{\textbf{\begin{tabular}[c]{@{}l@{}}Feature\\  Fusion\end{tabular}}}} & \multicolumn{1}{l}{\multirow{-2}{9mm}{\textbf{\begin{tabular}[c]{@{}l@{}}GPT2\\ Decoder\end{tabular}}}} & \multicolumn{1}{l|}{\multirow{-2}{12mm}{\textbf{\begin{tabular}[c]{@{}l@{}}Post-\\ Regressor\end{tabular}}}} & \multicolumn{1}{c}{35} & \multicolumn{1}{c}{70} & \multicolumn{1}{c|}{140} & \multicolumn{1}{c}{35} & \multicolumn{1}{c}{70} & \multicolumn{1}{c|}{140} & \multicolumn{1}{c}{35} & \multicolumn{1}{c}{70} & \multicolumn{1}{c|}{140} & \multicolumn{1}{c}{35} & \multicolumn{1}{c}{70} & \multicolumn{1}{c|}{140} & \multicolumn{1}{c}{35} & \multicolumn{1}{c}{70} & \multicolumn{1}{c|}{140} & \multicolumn{1}{c}{35} & \multicolumn{1}{c}{70} & \multicolumn{1}{c}{140} \\ \hline
\ding{55} & \ding{51} & \ding{51} & \ding{51} & \ding{51} & 0.00074 & 0.00340 & 0.00120 & 0.00340 & 0.00330 & 0.01033 & 0.00131 & 0.00097 & 0.00197 & 0.00080 & 0.00179 & 0.00092 & 0.00222 & 0.00276 & 0.00380 & 0.00169 & 0.00199 & 0.00364 \\
\ding{51} & \ding{55} & \ding{51} & \ding{51} & \ding{51} & 0.00068 & 0.00095 & 0.00106 & 0.02939 & 0.00281 & 0.00533 & 0.00064 & 0.00073 & 0.00121 & 0.00065 & 0.00083 & 0.00099 & 0.00160 & 0.00220 & 0.00308 & 0.00659 & 0.00170 & 0.00233 \\
\ding{51} & \ding{51} & \ding{55} & \ding{51} & \ding{51} & 0.00080 & 0.00079 & 0.00110 & 0.01173 & 0.00391 & 0.00581 & 0.00069 & 0.00073 & 0.00168 & 0.00047 & 0.00046 & 0.00216 & 0.00251 & 0.00306 & 0.00426 & 0.00324 & 0.00212 & 0.00300 \\
\ding{51} & \ding{51} & \ding{51} & \ding{55} & \ding{51} & 0.00073 & 0.00067 & 0.00114 & 0.26768 & 0.72387 & 0.18091 & 0.00062 & 0.00069 & 0.00158 & 0.00043 & 0.02110 & 0.75057 & 0.00220 & 0.00309 & 0.00566 & 0.05433 & 0.14593 & 0.03801 \\
\ding{51} & \ding{51} & \ding{51} & \ding{51} & \ding{55} & 0.00662 & 0.03911 & 0.00979 & 0.50841 & 0.57007 & 0.22267 & 0.00774 & 0.00852 & 0.00950 & 0.28932 & 0.02110 & 0.75057 & 0.56705 & 0.06188 & 0.08251 & 0.27583 & 0.14014 & 0.21501 \\ \hline
\multicolumn{5}{c|}{\textbf{Full CAMEF Model}} & \multicolumn{1}{l}{\textbf{0.00048}} & \textbf{0.00064} & \textbf{0.00107} & \textbf{0.00253} & \textbf{0.00250} & \textbf{0.00393} & \textbf{0.00054} & \textbf{0.00058} & \textbf{0.00101} & \textbf{0.00028} & \textbf{0.00044} & \textbf{0.00049} & {\color[HTML]{000000} \textbf{0.00132}} & \textbf{0.00207} & \textbf{0.00224} & \textbf{0.00104} & \textbf{0.00124} & \textbf{0.00174} \\ \hline
\end{tabular}
}
\end{table*}





\begin{table}[th]
\caption{Ablation Study Results on Different Type of Events on S\&P500 Index}
\label{tab:ablation2}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{c|ll|ll|ll}
\hline
\multirow{2}{*}{\textbf{Event Type}} & \multicolumn{2}{c|}{\textbf{\begin{tabular}[c]{@{}c@{}}Forecasting Length = 35\end{tabular}}} & \multicolumn{2}{c|}{\textbf{\begin{tabular}[c]{@{}c@{}}Forecasting Length = 70\end{tabular}}} & \multicolumn{2}{c}{\textbf{\begin{tabular}[c]{@{}c@{}}Forecasting Length = 140\end{tabular}}} \\
 & \multicolumn{1}{c}{\textbf{MSE}} & \multicolumn{1}{c|}{\textbf{MAE}} & \multicolumn{1}{c}{\textbf{MSE}} & \multicolumn{1}{c|}{\textbf{MAE}} & \multicolumn{1}{c}{\textbf{MSE}} & \multicolumn{1}{c}{\textbf{MAE}} \\ \hline
\textbf{\begin{tabular}[c]{@{}c@{}}Unemployment\\ Insurance\end{tabular}} & 0.0004870 $\downarrow$ & 0.0159497 $\uparrow$ & 0.0005199 $\downarrow$ & 0.0157778 $\downarrow$ & 0.0006948 $\downarrow$ & 0.0190141 $\downarrow$ \\
\textbf{\begin{tabular}[c]{@{}c@{}}Employment\\ Situation\end{tabular}} & 0.0003923 $\downarrow$ & 0.0142684 $\downarrow$ & 0.0004612 $\downarrow$ & \textbf{0.0154431} $\downarrow$ & 0.0013767 $\uparrow$ & 0.0218677 $\uparrow$ \\
\textbf{GDP Adcance} & 0.0006203 $\uparrow$ & 0.0175397 $\uparrow$ & 0.0005897 $\downarrow$ & 0.0175464 $\downarrow$ & 0.0011554 $\uparrow$ & 0.0217548 $\uparrow$ \\
\textbf{FOMC Minutes} & \textbf{0.0003401} $\downarrow$ & \textbf{0.0127694} $\downarrow$ & \textbf{0.0004448} $\downarrow$ & 0.0170094 $\downarrow$ & \textbf{0.0006433} $\downarrow$ & \textbf{0.0185657} $\downarrow$ \\
\textbf{CPI Report} & 0.0005645 $\uparrow$ & 0.0160723 $\uparrow$ & 0.0010660 $\uparrow$ & 0.0212536 $\uparrow$ & 0.0008187 $\downarrow$ & 0.0197824 $\downarrow$ \\
\textbf{PPI Report} & 0.0005275 $\uparrow$ & 0.0148434 $\downarrow$ & 0.0008054 $\uparrow$ & 0.0201844 $\uparrow$ & 0.0017646 $\uparrow$ & 0.0251856 $\uparrow$ \\ \hline
\multicolumn{1}{l|}{\textbf{Full Selection}} & 0.0004886 & 0.0152405 & 0.0006478 & 0.0178691 & 0.0010756 & 0.0210284 \\ \hline
\end{tabular}
}
\end{table}


\subsubsection{Implementation Overview}

For the single-modality approaches (ARIMA, DLinear, AutoFormer, FEDformer, iTransformer, and PatchTST), we tested two methods: (1) training the models on continuous historical time-series data and testing on aligned event-based time-series segments from the test set, and (2) training the models directly on event-based time-series segments, and also test on the aligned event-based time-series segments from the test set. The second approach produced more accurate results, and these are the results presented in this paper. Specifically, for each aligned event data pair \(([\mathcal{X}_{i-\tau:i+\tau} \mapsto \mathcal{E}_i])\), single-modality approaches use only the time-series segment preceding the event, \(\mathcal{X}_{i-\tau:i}\), to train the models and forecast the subsequent segment, \(\mathcal{X}_{i+1:i+\tau}\); and testing follows the same approach. Detailed settings for each model are provided in Appendix C.

For the multi-modality approaches (GPT4MTS, TEST, and CAMEF), both the event script \(\mathcal{E}_i\) and the time-series segment preceding the event, \(\mathcal{X}_{i-\tau:i}\), are used as input to train the models to forecast the post-event time-series segment, \(\mathcal{X}_{i+1:i+\tau}\). Implementation details, including model configurations and training settings, are explained in Appendix C.





\subsection{Experimental Results for Event-Driven Time-Series Forecasting (Q1)}

Table \ref{tab:main_results} presents the forecasting results for the five datasets across short, medium, and long forecasting horizons. CAMEF outperformed other models in 24 out of 30 settings, achieving first-place rankings, and ranked second in the remaining 6 settings. Specifically, CAMEF demonstrated the best performance across all forecasting lengths for the Stock Market Indices (SPX, INDU, and NDX), except for short-horizon forecasting on INDU, highlighting its effectiveness in event-driven stock market forecasting. For treasury bonds, CAMEF achieved the best results across all forecasting lengths for the 1-month treasury bond (USGG1M) and ranked second for the 5-year treasury bond (USGG5YR). The slightly lower performance on USGG5YR suggests that long-run treasury bonds may be less sensitive to event-driven factors and more influenced by historical trends.

Compared to single-modality models (e.g., DLinear, Autoformer, FEDformer, PatchTST, and iTransformer), CAMEF achieved an average MSE reduction of 62.55\% relative to the best-performing single-modality model, iTransformer. Among multi-modality models, CAMEF surpassed TEST, the second-best performer, with an average MSE reduction of 33.55\%. 

These results highlight three key insights: (1) effectively leveraging multi-modality information provides significant performance gains, particularly for SPX, INDU, NDX, and USGG1M; (2) transformer-based methods consistently outperform classical models such as ARIMA; and (3) CAMEF’s superior training and feature fusion strategies establish it as the most effective method for event-driven financial forecasting tasks.

% \begin{itemize}
%     \item \textbf{Conventional Baseline (ARIMA):} \\
%     ARIMA, a statistical model focused on linear dependencies, performs relatively well on simpler assets, like Gold Price (XAUUSD), but shows higher MSE on volatile indices (e.g., S\&P500, Nasdaq100). Its single-modality nature limits its ability to capture complex patterns in financial time series.

%     \item \textbf{Transformer-Based Models (Autoformer, FEDformer, iTransformer, PatchTST):} \\
%     Transformer models improve over ARIMA by capturing long-range dependencies, with iTransformer and PatchTST achieving lower MSE on assets such as Nasdaq100 and S\&P500. However, lacking multi-modal information, these models show limitations, particularly for longer forecasting horizons.

%     \item \textbf{TEST (Text-Enhanced Transformer):} \\
%     TEST incorporates text data but shows inconsistent performance, particularly for short-term forecasts. This suggests that while text features contribute, their integration with time-series data needs refinement to enhance forecast stability.

%     \item \textbf{Proposed Multi-Modality Model (CAMEF):} \\
%     Our CAMEF model, combining RoBERTa-based text embeddings with MOMENT/GPT2-based time-series encoding, achieves the lowest MSE across nearly all assets. CAMEF’s robust multi-modal fusion and causal learning yield strong improvements on high-volatility indices and Treasury Bonds, highlighting its adaptability and performance advantage over single-modality baselines.
% \end{itemize}




\subsection{Ablation Studies on Model Components (Q2)}

To evaluate the effectiveness of each component in CAMEF, we conduct comprehensive ablation studies on Textual Modality, Causal Learning, Feature Fusion, GPT2 Decoder, and the Post-Regressor. Based on the full CAMEF model, we remove the corresponding neural layers, such as the RoBERTa encoder for textual modality or the causal learning component, to assess their individual contributions. The ablation results are presented in Table \ref{tab:ablation}, where the left part of the table uses \ding{51} or \ding{55} to indicate whether the specific component is included or excluded in the test. From the results, three key findings are: (1) The full CAMEF model achieves the best performance across all datasets, demonstrating the critical importance of utilizing both textual and time-series modalities; (2) Causal learning provides incremental improvements, confirming its value in capturing cause-effect relationships within the data; (3) The proposed feature fusion layers and GPT2 decoder effectively integrate and leverage multi-modality features, significantly enhancing the model's ability to decode time-series data. These findings underscore the necessity of each component in achieving optimal performance for event-driven financial forecasting tasks.


% \begin{figure*}[ht]
%     \centering
%     % First subplot
%     \begin{subfigure}[b]{0.32\textwidth} % Adjust width as needed
%         \centering
%         \includegraphics[width=\textwidth]{figures/[CPI]trend_following_strategy_with_thresholds_analysis.png} % Replace with actual path
%         \caption{Potential Cumulative Return on CPI Report Released in Mar. 2024}
%         \label{fig:CPI_return}
%     \end{subfigure}
%     \hfill
%     % Second subplot
%     \begin{subfigure}[b]{0.32\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/[FOMC]trend_following_strategy_with_thresholds_analysis.png} % Replace with actual path
%         \caption{Potential Cumulative Return on FOMC Minute Released in Oct. 2023}
%         \label{fig:FOMC_return}
%     \end{subfigure}
%     \hfill
%     % Third subplot
%     \begin{subfigure}[b]{0.32\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/[UI]trend_following_strategy_with_thresholds_analysis.png} % Replace with actual path
%         \caption{Potential Cumulative Return on Unemployment Insurance Released in Feb. 2024}
%         \label{fig:UI_return}
%     \end{subfigure}

%     \caption{Potential Cumulative Returns on S\&P500 Index for CPI (Mar. 2024), FOMC (Oct. 2023), and Unemployment Insurance (Feb. 2024) Events Using Trend-Following Strategy Based on Predictions from the CAMEF Model.}
%     \label{fig:Ptential_Returns}
% \end{figure*}

\subsection{Ablation Studies on Different Type of Events (RQ3)}




Table \ref{tab:ablation2} shows the predictive performance of different events on the S\&P500 Index. \textbf{FOMC Minutes} achieve the lowest MSE and MAE, confirming their critical importance for market prediction. \textbf{Unemployment Insurance Claims} and \textbf{Unemployment Situation Reports} also come with lower errors than the full selection, however the latter becomes less effective at long forecasting length. In contrast, \textbf{CPI} and \textbf{PPI Reports} show weaker predictive power, with PPI yielding the highest errors and CPI improving slightly at long forecasting length. These results emphasize the importance of FOMC and unemployment-related events for financial forecasting.

\section{Conclusion and Future Work}

This paper proposed \textbf{CAMEF}, a multi-modality model for event-driven financial forecasting, which integrates effective causal learning and an LLM-based counterfactual event augmentation strategy. Alongside the model, we introduced a novel synthetic dataset comprising 6 types of salient macroeconomic event scripts, their counterfactual samples, and high-frequency time-series data for 5 key financial assets, aligned with real-world investment practices. Extensive experiments demonstrated CAMEF's superior predictive performance compared to prior deep time-series and multi-modality methods. Ablation studies testified the importance of causal learning and other designed components of CAMEF. Additionally, it is found that FOMC and unemployment-related events provided the most predictive value among the tested event types.

For future work, we plan to leverage advanced LLMs for enhanced textual encoding to extract deeper semantic information, refine the cross-modality causal inference mechanisms, and expand the dataset to include additional event types, such as political events and corporate market-sensitive news.



% \subsection{Case Study on Event-Driven Investment Return Analysis (Q3)}  



% \label{sec:real_test}
% In this sub-section, we evaluate the CAMEF model’s potential for generating investment returns using a \textbf{trend-following strategy} on selected events, including CPI (Mar. 2024), Unemployment Insurance (Feb. 2024), and FOMC (Oct. 2023). Trading decisions are made based on significant upward or downward trends in predicted closing prices compared to the previous step, with a threshold ratio of 0.2%.  

% \begin{itemize}
%     \item \textbf{Buy (Long):} Initiated when the predicted close increases by more than 0.2\%. Returns are calculated as:
%     \[
%     r_t = \frac{\text{Real Close}_t - \text{Real Open}_t}{\text{Real Open}_t}.
%     \]

%     \item \textbf{Sell (Short):} Initiated when the predicted close decreases by more than 0.2\%. Returns are:
%     \[
%     r_t = \frac{\text{Real Open}_t - \text{Real Close}_t}{\text{Real Open}_t}.
%     \]

%     \item \textbf{No Action:} Taken when the price change is below the threshold, indicating no significant trend.  
% \end{itemize}  

% \noindent Cumulative returns, computed as \( R = \sum_{t=1}^T r_t \times 100 \), yielding final gains of 0.53\%, 0.37\%, and 0.77\% across the selected events, thereby demonstrating the potential of CAMEF for investment applications.




% \section{Acknowledgments}

% Identification of funding sources and other support, and thanks to
% individuals and groups that assisted in the research and the
% preparation of the work should be included in an acknowledgment
% section, which is placed just before the reference section in your
% document.

% This section has a special environment:
% \begin{verbatim}
%   \begin{acks}
%   ...
%   \end{acks}
% \end{verbatim}
% so that the information contained therein can be more easily collected
% during the article metadata extraction phase, and to ensure
% consistency in the spelling of the section heading.

% Authors should not prepare this section as a numbered or unnumbered {\verb|\section|}; please use the ``{\verb|acks|}'' environment.



%%
%% The acknowledgments section is defined using the "acks" environment
%% (and NOT an unnumbered section). This ensures the proper
%% identification of the section in the article metadata, and the
%% consistent spelling of the heading.

% \begin{acks}
% To Robert, for the bagels and explaining CMYK and color spaces.
% \end{acks}

%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{references}


%%
%% If your work has an appendix, this is the place to put it.
\appendix

\section{Prompt Templates for Counterfactual Events Generations}
\label{sec_app:prompt_templates}
\subsection{Summarization Prompt Template}
The prompts are dynamically generated based on the input. Due to the length of the event script, we first ask LLaMA-3 to summarize each chunk individually, followed by a second prompt template to generate the full summary. Below is the prompt template for summarizing individual chunks, which instructs the model to produce a summary within a specified word limit. The text within ``{ }'' indicates the input variables:

\begin{tcolorbox}[colback=gray!5!white, colframe=gray!75!black, title=Prompt Template for Event Script Chunk Summarization]
You are given chunk \{\textit{chunk\_idx}\} of a \{\textit{text\_type}\} report. Your task is to generate a summary within \{\textit{number\_of\_words}\} words.\\

The content of chunk \{\textit{chunk\_idx}\} is as follows:\\

\{\textit{original\_text}\}\\

Please provide a concise summary, while keep the key variables:
\end{tcolorbox}

The second prompt combines the chunk summaries as input to generate a final summary for the entire event script:

\begin{tcolorbox}[colback=gray!5!white, colframe=gray!75!black, title=Prompt Template for Final Summarization]
You are given \{\textit{chunk\_num}\} summaries of different chunks from a \{\textit{text\_type}\} report. Your task is to generate an overall summary within \{\textit{number\_of\_words}\} words.\\

The chunk summaries are as follows:
\{\textit{chunk\_summaries}\}\\

Please provide a comprehensive summary of the entire report, while keep the key variables:
\end{tcolorbox}

\subsection{Sentiment Analytical Prompt Template}

For sentiment analysis, we use a prompt that instructs the model to analyze the sentiment of a given text and rate it on a scale from 0 to 10, from extremely negative to extremely positive. The prompt also asks for an explanation of the rating. Below is the sentiment analysis prompt, where the text within ``\{ \}'' indicates the input variables:

\begin{tcolorbox}[colback=gray!5!white, colframe=gray!75!black, title=Sentiment Analysis Prompt]
Please analyze the sentiment of the following \{\textit{text\_type}\} summary and rate it on a scale from 0 to 10, where:\\

0 = Extremely Negative \\
1 = Strongly Negative \\
2 = Very Negative \\
3 = Moderate Negative \\
4 = Slightly Negative \\
5 = Neutral \\
6 = Slightly Positive \\
7 = Moderate Positive \\
8 = Very Positive \\
9 = Strongly Positive \\
10 = Extremely Positive
\\
\{\textit{text\_type}\} summary: \{\textit{text}\}\\

Output the sentiment analysis as:\\

Sentiment rating: (0 to 10), Explanation:
\end{tcolorbox}

\subsection{Counterfactual Event Generation Prompt Template}

To generate counterfactual versions of a text with different sentiment levels, we use a prompt that instructs the model to modify key facts and information to align with a target sentiment rating. The model is provided with the original sentiment rating and sentiment description and is asked to adjust the text to reflect a specified target sentiment rating. Below is the prompt, where the text within ``\{ \}'' indicates the input variables:

\begin{tcolorbox}[colback=gray!5!white, colframe=gray!75!black, title=Counterfactual Text Generation Prompt]
The original text has been identified with a sentiment rating of \{\textit{current\_sentiment\_rating}\} (\{\textit{current\_sentiment}\}).\\

Your task is to generate a counterfactual version of the text that aligns with a sentiment rating of \{\textit{target\_sentiment\_rating}\} (\{\textit{target\_sentiment}\}) by modifying the key facts and information to reflect the specified target sentiment score about the economy, while keep the overall format and the sentiment-neural content unchanged.\\

Original text: \{\textit{original\_text}\}\\

Counterfactual text with a sentiment rating of \{\textit{target\_sentiment\_rating}\} (\{\textit{target\_sentiment}\}): 
\end{tcolorbox}


\section{Implementation Details of Dataset Collection and Preprocessing}

\label{sec_app:implementaion_dataset}
\subsection{Dataset Collection Implementation Details}

The dataset collection process retrieves raw macroeconomic event data files from official government sources (as illustrated in Tab. \ref{tab:data_summary}) to construct the CAMEF dataset. The raw data files can be presented in various formats (including PDF, HTML, and TXT). To achieve this, we leaverage the following key Python libraries to crwal the data including:
\begin{itemize}
    \item \textbf{Requests} is employed to handle HTTP requests for accessing event archives;
    \item \textbf{Selenium} to local and download the target file from HTML headers.
    \item \textbf{BeautifulSoup} is used to parse HTML pages and extract relevant file links;
\end{itemize}

The dataset is categorized into six macroeconomic event types: FOMC minutes, CPI reports, PPI reports, unemployment insurance releases, unemployment rate reports, and GDP advance releases. All collected files are renamed and indexed based on event type and release date. 

\subsection{Dataset Preprocessing Procedure}

The preprocessing stage transforms the collected raw files into a structured, unified text format to ensure dataset quality and consistency while adhering to a standardized output format. The preprocessing process varies depending on the file format. For \textbf{PDF files}, libraries such as \texttt{PyPDF2} and \texttt{pdfplumber} are employed to extract text and tables. For \textbf{HTML files}, \texttt{BeautifulSoup} is used for depth-first traversal of the DOM (Document Object Model) to extract structured content. For \textbf{TXT files}, the content is directly copied without modification, as these files are already in a structured text format.

To address potential information loss during table extraction from PDF and HTML files, we developed specialized code to convert tables into a structured format where cell contents are separated by the delimiter ``|''. An example of the converted table structure is shown below:

\begin{verbatim}
<Table>
Header 1 | Header 2 | Header 3
----
Cell 1   | Cell 2   | Cell 3
</Table>
\end{verbatim}

The preprocessing stage ensures the dataset is clean, well-structured, and machine-compatible, enabling robust analysis and effective machine learning model training. By combining efficient crawling and preprocessing techniques, this procedure produces a high-quality dataset suitable for event-driven financial modeling.

\section{Implementation Details}

The single-modality baselines (e.g., ARIMA, DLinear, AutoFormer, FEDFormer, iTransformer, and PatchTST) were adapted from the open-source Time-Series-Library and configured to align with the event-driven forecasting context. We use 

In this setup, historical time-series segments prior to each event served as input to predict trends over fixed forecasting lengths of 35, 70, and 140 steps. All models were trained for 10 epochs with a batch size of 32, maintaining consistency with the proposed method’s settings.

For multi-modality baselines, we leveraged the original TEST repository and re-implemented GPT4MTS. TEST involved a two-stage training process, combining textual prototypes and time-series decoders. GPT4MTS was modified by replacing BERT with LongFormer, allowing it to better handle longer texts. Both methods were adjusted to forecast the same time-series horizons as the single-modality models, using the same epoch and batch configurations.

Our proposed CAMEF model underwent a two-stage training process. First, the MOMENT encoder was pre-trained on time-series data to capture historical patterns. Then, the full CAMEF model was fine-tuned on aligned textual and time-series data. Some pre-trained parameters, such as MOMENT and RoBERTa (except the last layer), were frozen to retain pre-trained knowledge, while the remaining components, including parts of GPT2 and fusion layers, were trainable. This design allowed CAMEF to effectively integrate both time-series and textual features, providing a robust event-driven forecasting framework.

Further details and exact training procedures are provided in the appendix.

\subsection{Implementation of Baseline Models}
Baseline models were developed with objectives distinct from our event-driven forecasting approach, as single-modality methods are primarily designed for continuous time-series forecasting. To enable a fair comparison, we adapted these models to align with an event-driven context, where past time-series data preceding an event is used to forecast trends within a defined forecasting horizon.

\textbf{Single-Modality Models:} For time-series-based single-modality models, including \textbf{DLinear, AutoFormer, FEDFormer, iTransformer, and PatchTST}, we utilized the open-source library \textbf{Time-Series-Library}\footnote{\url{https://github.com/thuml/Time-Series-Library}}. Time-series segments were extracted with lengths covering both the input and forecasting periods surrounding each event, ensuring alignment with the respective event's time step. The input segment represents the historical trend information, while the forecasting period is treated as "unseen" data to be predicted. Input and forecasting lengths were consistently set to 35, 70, and 140 time steps across all experiments, as described in Sec.~\ref{sec:exp_setting}. All models were trained for 10 epochs, with a batch size of 32. These settings ensured convergence of the loss function and maintained consistency with CAMEF's training configuration.

\textbf{Multi-Modality Models:} For multi-modality methods, including \textbf{TEST} and \textbf{GPT4MTS}, we followed implementation strategies specific to each model.

\begin{itemize}
    \item \textbf{TEST:} We followed the instructions provided in the open-source TEST repository\footnote{\url{https://github.com/SCXsunchenxi/TEST}}, which involved two training stages. In the first stage, the encoder was trained by selecting 10 prototype words based on GPT vocabulary clustering (as instructed in the repository) to align textual representations with time-series data. The second stage involved training the time-series decoder, where input and forecasting steps were set to 35, 70, and 140, similar to the single-modality models. The batch size was set to 7, and training epochs were kept at 10.

    \item \textbf{GPT4MTS:} As the official implementation of GPT4MTS was unavailable, we re-implemented the model based on its original paper. Instead of using BERT \cite{devlin-etal-2019-bert} as the textual encoder, we employed LongFormer \cite{beltagy2020longformerlongdocumenttransformer}, which is better suited for encoding longer contexts, as our texts tend to be relatively lengthy. Input time-series segments were used to forecast the time-series data at the forecasting lengths of 35, 70, and 140. We kept training epochs at 10 and the batch size at 7 to ensure consistency with other baselines.
\end{itemize}

\subsection{Implementation of CAMEF}

The implementation of \textbf{CAMEF} involves two distinct training stages to ensure effective learning of both time-series and textual features, while leveraging pre-trained components:

\begin{enumerate}
    \item \textbf{Pre-training MOMENT:} In the first stage, we pre-train the MOMENT model using the time-series segments from the training data. This step focuses on learning  representations of the past time-series patterns.
    \item \textbf{Training Entire CAMEF:} In the second stage, we train the entire CAMEF model using both event scripts and their corresponding aligned time-series segments. During this stage, certain parameters are frozen to retain the pre-trained knowledge, while others remain open for fine-tuning:
    \begin{itemize}
        \item \textbf{Frozen Parameters:} All parameters of the MOMENT model and RoBERTa (except for the last hidden layer) are frozen. Additionally, the token embedding layer of GPT2 in the decoder is frozen to preserve its pre-trained representations.
        \item \textbf{Trainable Parameters:} The remaining components of GPT2, the last hidden layer of RoBERTa, and other components such as the embedding, residual, fusion, and output layers are open for training.
    \end{itemize}
\end{enumerate}

The hidden sizes of the different components of the CAMEF model are detailed in Sec.~\ref{sec:model_architecture}. To optimize the model effectively, we adopt component-specific learning rates as listed in Table~\ref{tab:learning_rates_CAMEF}.

% . This ensures balanced training across all modules while preventing overfitting or underutilization of any specific component.

\begin{table}[t]
\centering
\caption{Learning Rates for CAMEF Components}
\label{tab:learning_rates_CAMEF}
\begin{tabular}{l|c}
\hline \hline
\textbf{Model Component}               & \textbf{Learning Rate} \\ \hline
MOMENT Model                           & \(1 \times 10^{-6}\)   \\
RoBERTa Model                          & \(5 \times 10^{-7}\)   \\
GPT2 Model                             & \(1 \times 10^{-5}\)   \\
Embedding Layer                        & \(1 \times 10^{-5}\)   \\
Residual Layer                         & \(1 \times 10^{-5}\)   \\ 
Fusion Layer                           & \(5 \times 10^{-7}\)   \\
Output Layer                           & \(1 \times 10^{-5}\)   \\ \hline \hline
\end{tabular}
\end{table}

\noindent The batch size is set to 10, and the training epochs are set to 10, consistent with the baseline models.


% \section{Simulation Trading Algorithm}

% \subsubsection{Trend-Following Trading Strategy}

% The trading strategy implemented in Sec.~\ref{sec:real_test} follows a \textbf{Trending Following} approach, where trading decisions are based on significant upward or downward trends in the predicted closing prices from CAMEF. Specifically, the strategy compares the predicted closing price at the current time step (\(\text{Predicted}_{\text{Close}}[i]\)) with the previous time step (\(\text{Predicted}_{\text{Close}}[i-1]\)) to identify significant changes. The details are illustrated as the following:

% \begin{itemize}
%     \item \textbf{Long Position (Buy):} If the relative change between the current and previous predicted closing prices exceeds a specified upward threshold (\(\text{Threshold}_{\text{up}}\)), a long position is initiated. This is represented as:
%     \[
%     \frac{\text{Predicted}_{\text{Close}}[i] - \text{Predicted}_{\text{Close}}[i-1]}{\text{Predicted}_{\text{Close}}[i-1]} > \text{Threshold}_{\text{up}}.
%     \]
%     The return for this position is calculated as:
%     \[
%     R = \frac{\text{Real}_{\text{Close}}[i] - \text{Real}_{\text{Open}}[i]}{\text{Real}_{\text{Open}}[i]}.
%     \]

%     \item \textbf{Short Position (Sell):} If the relative change between the current and previous predicted closing prices surpasses the downward threshold (\(\text{Threshold}_{\text{down}}\)), a short position is initiated. This is expressed as:
%     \[
%     \frac{\text{Predicted}_{\text{Close}}[i-1] - \text{Predicted}_{\text{Close}}[i]}{\text{Predicted}_{\text{Close}}[i-1]} > \text{Threshold}_{\text{down}}.
%     \]
%     The return for this position is calculated as:
%     \[
%     R = \frac{\text{Real}_{\text{Open}}[i] - \text{Real}_{\text{Close}}[i]}{\text{Real}_{\text{Open}}[i]}.
%     \]

%     \item \textbf{No Position:} If the relative change does not exceed either threshold, no trading action is taken:
%     \[
%     \text{Threshold}_{\text{down}} \leq \frac{\text{Predicted}_{\text{Close}}[i] - \text{Predicted}_{\text{Close}}[i-1]}{\text{Predicted}_{\text{Close}}[i-1]} \leq \text{Threshold}_{\text{up}}.
%     \]
%     In this case, the return is set to:
%     \[
%     R = 0.
%     \]
% \end{itemize}

% \noindent Algorithm 2 illustrates a pseudo-code of this trading strategy. The algorithm iterates through all time steps in the dataset, applying the above logic to generate trading positions (\(+1\) for long, \(-1\) for short, and \(0\) for no position) and corresponding returns. For this strategy, the thresholds are set as \(\text{Threshold}_{\text{up}} = 0.002\) and \(\text{Threshold}_{\text{down}} = 0.002\). 
% \newpage

% \begin{algorithm}[H]
% \caption{Trend-Following Trading Strategy}
% \begin{algorithmic}[1]
% \Require 
% $P^{\text{pred}} = \{P^{\text{pred}}_1, \dots, P^{\text{pred}}_N\}$: Predicted "Close" prices, \\
% $P^{\text{real,open}} = \{P^{\text{real,open}}_1, \dots, P^{\text{real,open}}_N\}$: Actual "Open" prices, \\
% $P^{\text{real,close}} = \{P^{\text{real,close}}_1, \dots, P^{\text{real,close}}_N\}$: Actual "Close" prices, \\
% $\text{threshold}_{\text{up}}$, $\text{threshold}_{\text{down}}$: Trend detection thresholds.
% \Ensure 
% $\text{positions}$: Trading positions, $\text{returns}$: Returns for each decision.

% \State Initialize $\text{positions} \gets []$ and $\text{returns} \gets []$.

% \For{$i = 0$ to $N$}
%     \State Compute $\text{upward\_diff} \gets P^{\text{pred}}_i - P^{\text{pred}}_{i-1}$.
%     \State Compute $\text{downward\_diff} \gets P^{\text{pred}}_{i-1} - P^{\text{pred}}_i$.
%     \If{$\frac{\text{upward\_diff}}{P^{\text{pred}}_{i-1}} > \text{threshold}_{\text{up}}$}
%         \State $p_i \gets 1$ \Comment{Take a long position.}
%         \State $r_i \gets \frac{P^{\text{real,close}}_i - P^{\text{real,open}}_i}{P^{\text{real,open}}_i}$.
%     \ElsIf{$\frac{\text{downward\_diff}}{P^{\text{pred}}_{i-1}} > \text{threshold}_{\text{down}}$}
%         \State $p_i \gets -1$ \Comment{Take a short position.}
%         \State $r_i \gets \frac{P^{\text{real,open}}_i - P^{\text{real,close}}_i}{P^{\text{real,open}}_i}$.
%     \Else
%         \State $p_i \gets 0$ \Comment{No action.}
%         \State $r_i \gets 0$.
%     \EndIf
%     \State Append $p_i$ to $\text{positions}$ and $r_i$ to $\text{returns}$.
% \EndFor

% \State \Return $\text{positions}$, $\text{returns}$.
% \end{algorithmic}
% \end{algorithm}



\end{document}
\endinput
%%
%% End of file `sample-sigconf-authordraft.tex'.
