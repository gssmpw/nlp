% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{adjustbox}
\usepackage{enumitem}

\title{Interpretable Text Embeddings and Text Similarity Explanation: A Primer}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
\author{Juri Opitz$^1$\quad Lucas Möller$^{2}$\quad Andrianos Michail$^{1}$\quad Simon Clematide$^{1}$\quad \medskip\\
  $^1$University of Zurich, Switzerland \\\medskip 
  $^2$IMS at University of Stuttgart, Germany \\ 
  $^1$\texttt{\{jurialexander.opitz,andrianos.michail,simon.clematide\}@uzh.ch} \\  $^2$\texttt{lucas.moeller@ims.uni-stuttgart.de}\\}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

%\author{First Author \\
%  Affiliation / Address line 1 \\
%  Affiliation / Address line 2 \\
%  Affiliation / Address line 3 \\
%  \texttt{email@domain} \\\And
%  Second Author \\
%  Affiliation / Address line 1 \\
%  Affiliation / Address line 2 \\
%  Affiliation / Address line 3 \\
%  \texttt{email@domain} \\}

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle
\begin{abstract}
Text embeddings and text embedding models are a backbone of many AI and NLP systems, particularly those involving search. However, interpretability challenges persist, especially in explaining obtained similarity scores, which is crucial for applications requiring transparency. In this paper, we give a structured overview of interpretability methods specializing in explaining those similarity scores, an emerging research area.  We study the methods' individual ideas and techniques, evaluating their potential for improving interpretability of text embeddings and explaining predicted similarities.
\end{abstract}

\section{Introduction}

Embedding models \citep{reimers-gurevych-2019-sentence, gao-etal-2021-simcse} are indispensable across numerous NLP tasks in both academia and industry. Applications range from semantic search and information retrieval \citep{ye2016word, GUO2020102067_neural_ir, muennighoff2022sgpt, hambarde2023information, ALATRASH2024100193} to text classification \citep{schopf2022evaluating}, topic modeling \citep{grootendorst2022bertopic}, NLG evaluation \citep{celikyilmaz2020evaluation, uhrig-etal-2021-translate, nlg_survey, larionov-etal-2023-effeval, chollampatt-etal-2025-cross}, knowledge graph construction \citep{plenz-etal-2023-similarity}, and retrieval-augmented generation (RAG)—a specific form of information retrieval leveraging embedding similarity to identify evidence from a large corpus and summarize it using generative Large Language Models \citep[LLMs,][]{NEURIPS2020_6b493230_rag, gao2023retrieval}. Furthermore, advances in base models \citep{gunther-etal-2023-jina, wang-etal-2024-improving-text}, context size \citep{li2023towards}, and scalable training infrastructure \citep{wang2022text} have steadily enhanced the capabilities of embeddings.

However, an urgent challenge persists: \textit{the problem of interpretability}. For example, when a set of most similar documents is retrieved in response to a query, we would like to articulate why these documents were selected as the most \textit{similar}, or why a particular document was omitted. Similarity variables are also highly conflated: Were the obtained similarity ratings based on semantic similarity, relatedness, paraphrasticity, relevance—subtly distinct concepts \citep{budanitsky2006evaluating, kolb2009experiments, michail-etal-2025-paraphrasus}—or were they influenced mainly by superficial characteristics such as token overlap? And such questions are not just theoretical; they have significant practical implications. For instance, embedding systems deployed in sensitive domains may need to justify outputs, perhaps even in a legal context. 

Fortunately, recent research has begun addressing this interpretability gap. Our paper aims to serve as a primer for researchers and practitioners that seek to understand embedding-based similarity models and measurements. By presenting a structured overview of interpretability approaches, we hope to ease entry into this area and inspire further innovations. Understanding how similarity is computed—and how it can be explained—not only enhances transparency but may also pave the way for improved methods and applications.

\section{Setting the Stage}

We study explainability in neural text embeddings \textit{and} their induced similarity. We distinguish this from common approaches to classification-explanation like `LIME' \citep{ribeiro2016should}, or `Shapley values', see also \citet{calderon2024behalf}'s survey.  \textit{Similarity} is not based on a single input but rather \textit{the interaction} of two inputs, hence the need for specialized methods.

\paragraph{Notation.}  Assume a (tokenized) input $text = [t_1,...,t_n]$, and two neural networks $F, G$ consisting of $L$ layers, each representing a function, e.g., in the case of $F$: $F = f_L \circ ... \circ f_1$. Typically $F=G$, i.e., the weights of the two networks are shared, also called \textit{Siamese} network \citep{koch2015siamese}; if not mentioned otherwise, we thus only speak of $F$. The first layer maps tokens to real valued embeddings: $E_1 = f_1(text) \in \mathbb{R}^{d \times m}$, whereas the consecutive neural layers perform non-linear operations to \textit{transform} and refine the representation. Oftentimes, there is a last (optional) layer $L+1$ that has a special goal: producing a vectorized fixed-size representation independent of document length, i.e., $e^{L+1} = reduce(E^L) \in \mathbb{R}^{d \times 1}$. This layer would perform averaging, or max-pooling across the individual token embedding dimensions. 

Finally, we can efficiently match two texts $x, y$ through their embeddings $e_x = F(x), e_y = F(y)$ by calculating a similarity function $sim$: $sim(e_x, e_y) \in \mathbb{R}$; in the simplest case, this can be the dot product $sim(e_x, e_y) := e_x^Te_y$, possibly normalized by length $l_{x,y}=|e_x|_2\cdot|e_y|_2$ to achieve the (practically strongly correlated) `cosine similarity'. Importantly, such a value quantifies the similarity relationship between texts, and thus we can rank texts according to their similarity. 

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figs/overview-crop.pdf}
    \caption{Three explanation perspectives.}
    \label{fig:overview}
\end{figure}

\paragraph{Approach categorization and paper structure.} Our definition of text embedding and similarity allows us to distinguish different types of explainability approaches (Figure \ref{fig:overview}). 

First we will visit \textbf{space shaping approaches (\S \ref{sec:feabased})}. They aim at shaping the projected embedding spaces, infusing some useful structure. E.g., the embedding space could be made up of different \textit{features}, which would equate to a $sim$ that we can decompose and better understand (Figure \ref{fig:overview}: \textit{Feat 1}, \textit{Feat 2}...). Alternatively, we can shape the space to be more expressive. E.g., we observe approaches that represent text as a high-dimensional box, or a random variable (Figure \ref{fig:overview}, box, $\mathcal{N}_d(\mu, \Sigma)$). 

Another class of approaches are \textbf{set-based approaches (\S \ref{sec:setbased})}. They do not base their $sim$ on two embeddings but on two sets of embeddings. These embeddings typically relate to human interpretable units (i.e., tokens), often it's the last layer's output $f^L(x), f^L(y)$. Set-theoretic operators can then be applied (e.g., intersection). We can also retrieve an alignment between the embeddings, adding another layer of transparency as to what is $sim$ made up from (Figure \ref{fig:overview}, Token alignment).

The third category of approaches we denote as \textbf{attribution-based approaches (\S \ref{sec:attrbased})}. These aim at attributing $sim$ directly to the inputs, or pairs of inputs, given the representations that are consumed by certain neural network layers (Figure \ref{fig:overview}, Attributions: For a particular layer, a pairwise similarity matrix is built that approximates the $sim$). 

We conclude the presentation of the three classes of approaches with a \textbf{discussion (\S \ref{sec:discuss})}, outlining pertinent challenges. Finally, we give an overview of \textbf{related studies (\S \ref{sec:other})}, as well as datasets that elicit human similarity explanations.

\section{Shaping Interpretable Spaces}
\label{sec:feabased}

\subsection{Idea}


These approaches aim at structuring the embedding space such that it becomes more interpretable. E.g., the space can be shaped to express aspects, interpretable geometries or probabilistic distributions.

\subsection{Approaches}

\subsubsection{Feature Decomposition}

Traditional methods for text similarity often relied on explicit ``bag-of-words'' feature representation. While this provides great transparency in representation and similarity calculation, it lacks the representational power of neural embeddings, cannot match paraphrases, and thus result in relatively poor performance on standard benchmarks. Recent efforts aim to combine the interpretability of features with the power of neural embeddings.

\paragraph{Q/A features.} This approach involves framing embedding generation as answering a set of predefined questions about a text and encoding the answers as features, enabling interpretability. For this, we first need to find \textit{a suitable set of questions} about texts, and create training data that \textit{elicits answers} to these questions. Afterwards, we can distill an efficient and interpretable text embedding model using this training data. Specifically, \citet{benara2024crafting} let an LLM answer ``Yes''/``No'' questions about a text (e.g., is the text about sports?, Does the text express a command?), building prompts based on dataset description. For predicting fMRI responses to language stimuli their method outperforms several baselines. On the other hand, \citet{sun2024general} first build a concept space from a dataset by clustering word embeddings, and introduce two constraints, namely that the Q/A prompt be based on focal concepts. Also, for positive text pairs, all questions should be answered with ``Yes'', while for negative text pairs all questions be answered with ``No'', to sharpen the boundary between similar and non-similar texts. 

\paragraph{Sub-embeddings.} An embedding space can be decomposed into \textit{multi-dimensional subspaces}, each isolating a certain semantic aspect. This enables the overall similarity between texts be broken down into similarity scores for different aspects.

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{figs/s3bertchart.pdf}
    \caption{How the overall $sim$ emerges from different aspectual similarites, via (S3BERT) space decomposition. The example is shortened to a selection of features.}
    \label{fig:s3bertexample}
\end{figure}

An example for this is the approach by \citet[\textit{S3BERT}: Semantically Structured SBERT]{opitz-frank-2022-sbert}. The method requires a user to define a set of metrics that measure interpretable similarity aspects of two text (e.g., \textit{Is the focus of the texts the same?}). Since such aspects often are implicit in the texts, they leverage abstract meaning representation graphs \citep{banarescu-etal-2013-abstract} that encode aspects such as number, focus, semantic roles, negation; and use graph matching metrics \citep{opitz-2023-smatch} on aspectual subgraphs. They fine-tune a reference embedding model such that the similarity of aspectual sub-embeddings regresses to the aspectual graph metrics. A consistency loss and residual sub-embedding helps tie the overall similarities to the original reference. Lastly, an explanation can look as follows (Figure \ref{fig:s3bertexample}): \textit{Two men are singing} is similar to \textit{Three men are singing} by a value of 0.76. The similarity of concepts increases the value, while the dissimilarity of quantificational structure lowers it. 

Similar approaches do not leverage a consistency loss and wish to induce entirely new decomposed spaces: We can learn ``multi-facet'' embeddings \citep[with graph metric ground truth]{risch-etal-2021-multifaceted}  or ``specialized-aspect'' embeddings \citep[with aspect-specific transformer encoders]{10.1145/3529372.3530912specialized, schopf-etal-2023-aspectcse}. 

A more coarse decomposition is induced by \citet{ponwitayarat-etal-2024-space}, who construct two spaces, one for texts that are only vaguely similar (lower range), and the other to capture finer text similarity between already highly similar texts (upper range). This idea was based on their linguistic analysis of the \textit{S}emantic \textit{T}extual \textit{S}imilarity dataset \citep[\textit{STS},][]{cer-etal-2017-semeval}, finding that one continuous similarity range is not expressive enough, motivating their decomposition into two continua. To decompose, they use a classification loss (high-range vs.\ low-range), and learn the representation of positive examples only in the ``upper range part'' of the space. 

\subsubsection{Non-Euclidean Geometry}

Certain text relationships are inherently asymmetric.  For instance, a natural relation between texts is \textit{entailment}: A given hypothesis follows from a premise. Some embedding geometries offer a way to model these relationships. 

An interesting example are box-embeddings: Consider all two-dimensional boxes centered at zero with their left bottom corner. For two such boxes $a$ and $b$ we have their size $s_a = a_1 \cdot a_2$, $s_b = b_1 \cdot b_2$, and their overlap $o_{a, b}=min(a_1, b_1) \cdot min(a_2, b_2)$. We arrive, e.g., at a similarity $o_{a,b} / (s_a + s_b -o_{a,b})$, and interesting asymmetric relationships like the containment or entailment of, e.g., $a$ in $b$: $o_{a,b} / s_b$ --- it's exactly $1$ if $a$ is fully contained/entailed in/by $b$. The challenge is to learn such objects in high dimensionality: To see a major bottleneck, consider that box size and overlap approach zero in high dimensionality, since they involve a large product. To alleviate such learning problems, \citet{chheda-etal-2021-box} propose to adopt a probabilistic soft box overlap formulation based on Gumbel random variables \citep{dasgupta2020improving}. 

On the other hand, \citet{huang-etal-2023-bridging} learn interpretable composition operators, such as union/fusion, or difference, by modeling the operators with neural networks, and retraining the embedding models such that their space is shaped for operator allowance. Evaluation shows little loss on standard similarity accuracy, but greatly improved performance for compositional generation tasks.

Another line of research investigates \textit{probabilistic text embeddings}, viewing a text as a random variable (RV). Intuitively, this provides us with a model of multiple interpretation, which seems appealing due to natural language ambiguity: A text can have multiple interpretations, and only some of these interpretation can map to those of another similar text. But how to build such a probabilistic space? \citet{shen-etal-2023-sen2pro} model a text as a Gaussian RV $\mathcal{N}_d(\mu, \Sigma)$ by estimating ``Model uncertainty'' via Monte Carlo Dropout \citep{pmlr-v48-gal16}, and data uncertainty via smaller linguistic perturbations (e.g., dropping a word). The covariance matrix ($\hat{\Sigma}$) is then efficiently approximated through banding estimator \citep{Bien02042016}. For increased efficiency, \citet{yoda-etal-2024-sentence} learn to directly predict mean ($\hat{\mu}$) and covariance ($\hat{\Sigma}$).

\subsubsection{Combining Token Embeddings}

Combination-based approaches build a new embedding space by aggregating token-level representations with explicit weights that reflect their importance. E.g., \citet{wang-sbertwk} estimate token importance and novelty using variance across transformer layers, constructing weighted embeddings. On the other hand, \citet{seo-token-attention} train models to learn token weights directly, using a reconstruction loss to prevent catastrophic forgetting. Alternatively, we can create static embeddings for all tokens in the vocabulary, using one transformer forward pass (for each token), and then calculate a simple average that is informed by Zipf token statistics \citep{minishlab2024model2vec}. 

\subsection{Challenges and Opportunities} 

Q/A approaches can outperform certain baselines, but they do not (yet) fully seem to match the performance of reference embedding models with distributed features, probably since it is difficult to find a generalizable set of questions.\footnote{In experiments, they are compared mainly against baselines like bag-of-words \citep{sun2024general}, or they are built for a specific domain \citep{benara2024crafting}.}

Similarly, the sub-embedding decomposition approaches requires the definition of custom aspects, and the features are not directly interpretable on their own ---only their similarity value is. 

On one hand, crafting the right features (through questions or interpretable metrics) can be seen as a drawback of the feature based approaches. However, it is also an interesting opportunity, since it allows for exploring custom spaces.

Finally, non-euclidean geometry based methods and combination-based ones leave ample space for exploration. Modeling embeddings as, e.g., boxes, allows application of interpretable operators aligning with semantic relationships (e.g., entailment).

\section{Set-based Interpretability}
\label{sec:setbased}

\subsection{Idea}

Set-based approaches to similarity explainability rely on matching \textit{two sets}, rather than two points. These sets typically consist of human-interpretable items, e.g., tokens. Aligning these, we may be provided with insight into how different text parts relate to each other. Sets also offer inherent interpretability through set-theoretic operations that may align naturally with some interesting semantic text relationships (e.g., entailment as subset).

\subsection{Approaches}

\subsubsection{Embedding Set Interpretability}

Alignment-based methods derive similarity by aligning token embeddings from one text with those of another. These approaches typically use embeddings from the last layer of a model. Two focal techniques in this category are ``ColBERT'' and ``BERTscore''. The ColBERT approach \citep[][Figure \ref{fig:colbert}]{10.1145/3397271.3401075Colbert, santhanam-etal-2022-colbertv2} computes an asymmetric alignment by viewing $x$ as the query and $y$ as the candidate (aka ``passage'') using two encoders; in our notation, $F := Q$ and $G := C$. For each individual token embedding in the embedded query $E^x = Q(x)$, we search in the embedded candidate tokens $E^y = C(y)$ for the best match and sum over those. For computing a similarity score, both methods rely on a greedy max-matching, formally, $sim(x, y) = \sum_{t \in x} max([Q(x)^T C(y)]_t)$. The BERTscore approach, which is used in the evaluation of machine translation \citep{Zhang2020BERTScore:}, further computes a symmetric harmonic mean (F1 score) as $HM(\frac{1}{|x|} sim(x, y), \frac{1}{|y|} sim(y, x))$. 

While the asymmetry in ColBERT is also achieved through the different encoders, the asymmetry in BERTscore is achieved by  the calculation of precision and recall, hence their different application cases (IR vs.\ evaluation). In their potential for \textit{explainability}, both approaches appear similar: Their similarity score can be seen as constructed from an interpretable alignment from one document to another, and the contributions of single token embedding pairs can be clearly highlighted.\footnote{For refined alignment, optimal transport can replace greedy max-matching \citep{kusner2015word, lee-etal-2022-toward}.} 

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figs/paris_boxes.pdf}
    \caption{An example of a late-interaction matrix between query and passage token embeddings in the ColBERTv2.0 model. The overall $sim$ is 0.965. Red boxes indicate the sum of row-wise maxima (alignment).}
    \label{fig:colbert}
\end{figure}


\subsubsection{Explicit Multi-Interpretation}

Most set-based approaches use token embeddings, but some  extend the concept to text embeddings. 

The first class of methods generates sets of text embeddings by either hypothesizing about a text or decomposing it into smaller parts. In particular, we can use a generative model to construct hypotheses about a text \citep{hoyle-etal-2023-natural}, or decomposing it into smaller statements or descriptions \citep{ravfogel2024descriptionbased}. Having deconstructed a text $x$ into smaller parts $\{x_1, ...x_n\}$, we call our text embedding model exactly $n$ times, and thus construct a set of $n$ respective text embeddings $\{e_1, ...e_n\}$ that can be matched to explain similarity of facts contained in a text, also with different abstractness levels.\footnote{An example for different abstractness levels from \citet{ravfogel2024descriptionbased}: Given ``\textit{On July 2, concurrent with the Battle of Gettysburg in neighboring Adams
County, Captain Ulric Dahlgren’s Federal cavalry patrol galloped into Greencastle’s
town square, where they surprised and captured several Confederate cavalrymen
carrying vital correspondence from Richmond.}'', the description 1 is: ``Military personnel thwarting an enemy’s attempt to convey vital documents.'' The description 2 is: ``The disruption of a communication exchange in a rural area.'' The description 3 is: ``A dramatic, unexpected event occurring in a town square during a
battle.''}

An interesting variation of such a multi-text set-based approach is proposed by  \citet{liu2024meaning}. To compute the similarity of two texts, they sample sets of possible continuation from an LLM, and calculate the average (i.e., expected) difference in log-likelihood between the two input texts that are continued with a randomly sampled continuation. 

\citet{liu2024conjuring} \textit{multi-modally} calculate similarity values between texts through the respective imagery they evoke, using denoising through Stochastic Differential Equations \citep{song2021scorebased}. Essentially, the idea is that two texts are more similar if they evoke more similar imagery, allowing for visual interpretation of the score.

\subsection{Challenges and Opportunities}

Set-based approaches allow an interpretable alignment of token-level embeddings. This alone has useful applications; for instance, to elicit token-level semantic differences between related documents \citep{vamvas-sennrich-2023-towards}. Sets also allow an intuitive view on asymmetric text relationships, increasing their explanation appeal in asymmetric tasks like NLI. However, it is crucial to note that token embedding alignment does not equate to input token alignment, as the contextualization steps may obscure the actual contributions of input tokens and any thereupon based explanation.

We also saw that we can abstract from sets of token embeddings to sets of text embeddings, e.g., by decomposing a text into smaller statements, before generating embeddings. At the cost of a greater number of inferences, the interpretability potential of this class of explicit multi-interpretation based approaches is that they can deliver evidence about which statements conveyed by the texts are actually matching and contributing to the overall similarity.

\section{Attribution-based Interpretabilty}
\label{sec:attrbased}

\subsection{Idea}
Attribution-based approaches aim at attributing a model prediction onto input or intermeditate feature representations.
In other words: they assign importance values to features for how much they contribute to a given prediction.
However, a special characteristic of similarity models is that their predictions do not depend on individual features, due to the multiplicative interaction between the two inputs’ embeddings in $sim$. Thus first-order methods do not suffice \cite{shapley_interaction, int_hessians}. Instead, second-order methods are required to attribute predictions of similarity models.

\subsection{Approaches}

Two lines of work have addressed this issue in text similarity models: integrated Jacobians, an extension of the theory behind integrated gradients to Siamese models \citep{moeller-etal-2023-attribution, moeller-etal-2024-approximate} and BiLRP that uses layer-wise relevance propagation for this model class \citep{vasileiou-eberle-2024-explaining}. 

\subsubsection{Integrated Jacobians}
Integrated gradients (IG) attributes a scalar model prediction back onto individual input features by integrating over a number of interpolations between the actual input and an uninformative reference input \citep{ig}.
The method can provide a closed-form solution to explaining the difference in the model prediction between the reference and the actual input. 
Its output takes the form of a vector with importance values for all input features.
\citet{moeller-etal-2023-attribution} have applied the underlying theory of IG to Siamese encoders, enabling the attribution of similarity predictions onto feature-interactions between the two inputs. Different from IG, the output takes the form of a feature-pair attribution matrix. For text encoder models it can be reduced to a token-token matrix, showing the contribution of token interactions to the $sim$ (Figure \ref{fig:attribution-example}). The authors provide an exact version of their attributions, which guarantees that the sum over the attribution matrix must exactly equal the predicted similarity score. This version also enables the quantification of an attribution error, however, it requires an adjustment of the models through fine-tuning. Alternatively, approximate attributions can be calculated for any off-the-shelf model without a need to adjust it \citep{moeller-etal-2024-approximate}. The approximation has been shown to correlate sufficiently with the exact variant.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figs/beer.png}
    \caption{Interaction-attributions between two sentences computed with the IG method.
    The $sim$ is 0.618 and the attribution error is 0.001 for N=50 integration steps.}
    \label{fig:attribution-example}
\end{figure}

\subsubsection{BiLRP}
Layer-wise relevance propagation (LRP) is a framework to propagate feature-importance values for a model prediction back through the model in a layer-wise fashion \cite{lrp, lrp_survey}. 
Propagation rules are derived for individual layers based on first-order Taylor expansion of the underlying function.
Thus, the approach essentially linearly approximates all computations in a model’s graph around a given reference input.
BiLRP extends the LRP framework to Siamese similarity models by computing LRP values for each embedding dimension of the two encoders separately and subsequently taking their matrix product.
Thus, the computation also takes the form of a product between two Jacobian-like matrices. Whereas, integrated Jacobians constructs these matrices by integrating over interpolated inputs, in BiLRP they originate from the layer-wise propagation rules.
The method was originally proposed in the computer vision domain \cite{bilrp} and has recently also been applied to Siamese text encoder models \citep{vasileiou-eberle-2024-explaining}.

\subsection{Challenges and Opportunities}

Attribution approaches need to build Jacobian matrices, coming at a temporal complexity of \(2\!\times\! D\) independent backward passes, \(D\) being the model's embedding dimensionality.
The resulting Jacobians have a quadratic spatial complexity of \(D\!\times\! D_{in}\). With \(D_{in}\) being a sequential representation, the required memory can grow quickly for higher \(D\) or long inputs, requiring large GPUs to compute the associated matrix multiplications efficiently.

Inspite of these computational costs, attribution based methods have the advantage of being model agnostic, not requiring additional design choices on model architectures or training objectives. In contrast to (last-layer-embedding-)set based approaches like ColBERT, they also relate more directly to the actual input tokens. Different from space shaping approaches they also do not pose any constraints on embeddings during training.

\begin{table*}[ht]
    \centering
    \adjustbox{width=\linewidth}{\begin{tabular}{lllllll}
    \toprule
        Paper & Type & Subtype & Train & Approx. & Inf.\ Cost &   code\\
        \midrule
        \citet{sun2024general} & space-shaping & QA-feature & yes  & no & $\mathcal{O}(n)$ & \href{https://github.com/csinva/interpretable-embeddings}{github} \\
        \citet{benara2024crafting} & space-shaping & QA-feature&  yes  & no & $\mathcal{O}(n)$ & \href{https://github.com/csinva/interpretable-embeddings}{github} \\
        \citet{opitz-frank-2022-sbert} & space-shaping & sub-embedding &  yes  & yes & $\mathcal{O}(n)$ &  \href{https://github.com/flipz357/S3BERT}{github} \\
        \citet{risch-etal-2021-multifaceted} & space shaping & sub-embedding & yes & no & $\mathcal{O}(n)$ & \href{https://github.com/philipphager/faceted-domain-encoder}{github}\\
        \citet{schopf-etal-2023-aspectcse} & space shaping & sub-embedding & yes & no & $\mathcal{O}(nk)$ & NA\\
        \citet{ponwitayarat-etal-2024-space} & space-shaping & sub-embedding & yes & no & $\mathcal{O}(n)$ & \href{https://github.com/KornWtp/MixSP}{github} \\
        \citet{huang-etal-2023-bridging} & space-shaping & non-Euclidean space  & yes & yes & $\mathcal{O}(n^2)$ & \href{https://github.com/jyhuang36/InterSent}{github}\\
        \citet{chheda-etal-2021-box} & space-shaping & non-Euclidean space & yes & no & $\mathcal{O}(n)$ & \href{https://github.com/iesl/box-embeddings}{github} \\
        \citet{shen-etal-2023-sen2pro} & space-shaping & non-Euclidean space & no & yes & $\mathcal{O}(nk)$ & NA \\
        \citet{yoda-etal-2024-sentence} & space-shaping & non-Euclidean space & yes & no & $\mathcal{O}(n)$ & \href{https://github.com/yoda122/GaussCSE}{github} \\
        \citet{wang-sbertwk} & space-shaping & combination & no & no & $\mathcal{O}(n)$ & \href{https://github.com/BinWang28/SBERT-WK-Sentence-Embedding}{github}\\
        \citet{seo-token-attention} & space-shaping & combination & yes & no & $\mathcal{O}(n)$ & NA \\
        \citet{moeller-etal-2023-attribution, moeller-etal-2024-approximate} & token-attribution & integrated gradients & no  & yes & $\mathcal{O}(n^2)$ & \href{https://github.com/lucasmllr/xsbert}{github} \\
        \citet{vasileiou-eberle-2024-explaining} & token-attribution & relevance propagation & no  & yes & $\mathcal{O}(n^2)$ &  \href{https://github.com/alevas/xai_similarity_transformers}{github} \\
        \citet{10.1145/3397271.3401075Colbert, santhanam-etal-2022-colbertv2} & set-based & token-set & no & yes & $\mathcal{O}(nk)$  & \href{https://github.com/stanford-futuredata/ColBERT}{github} \\
        \citet{hoyle-etal-2023-natural} & set-based & text-set & no & no & $\mathcal{O}(nk)$ & \href{https://github.com/ahoho/inferential-decompositions}{github} \\
        \citet{ravfogel2024descriptionbased} & set-based & text-set & no & no & $\mathcal{O}(nk)$ & \href{https://github.com/shauli-ravfogel/descriptions}{github}\\
        \citet{liu2024meaning} & set-based & text set & no & no & $\mathcal{O}(nk)$ & \href{https://github.com/tianyu139/meaning-as-trajectories}{github}\\
        \citet{liu2024conjuring} & set-based & image-set & yes & no & $\mathcal{O}(nk)$ & NA \\
        \bottomrule
    \end{tabular}}
    \caption{Overview and broader differentiation of approaches.}
    \label{tab:approach-classification}
\end{table*}

\section{Discussion}
\label{sec:discuss}

\subsection{Summary and Overview of Approaches} 

We identify general features that allow for summarizing and comparing similarity interpretability approaches from a broader perspective. We propose the following key features for categorization:
\begin{itemize}[noitemsep,nolistsep,leftmargin=*]
    \item \textit{Type}: The three overarching categories that we used to structure the interpretability landscape.
    \item \textit{Subtype:} Our finer sub-classification. 
    \item \textit{Train}: If the method requires training.
    \item \textit{Approx(imative)}: Refers to whether a method approximates the similarity score of a reference embedding model. This ensures the interpretability method’s similarities are predictable. 
   \item \textit{Inf(erence) Cost}: An estimate of computational cost in terms for inferring pairwise explanations in a data set of size $n$. We use $k$ to denote other potential relevant parameters, e.g., the maximum size of encountered token sets, the number of encoder calls (if not one).
\end{itemize}
A classification of the visited approaches according to this taxonomy is shown in Table \ref{tab:approach-classification}. 
   

\subsection{Pertinent Challenges}
\label{ssec:pchallenges}

\paragraph{Mitigate tradeoffs.} Methods differ in their conceptualization of interpretability, computational cost, fidelity to input tokens, and eventual dependencies to a specific model as their basis. Space shaping is highly adaptive and allows to express different semantic aspects of interest, or model intuitive relations with non-Euclidean spaces ---but tends to require custom training and definitions that may not generalize. Then, methods that compute the similarity from two sets of embeddings (token embeddings, or text embeddings) can produce a visually inspectable alingment and allow for interpretable asymmetric matching with set-operators ---but the last layer's embeddings are highly contextualized and thus are technically no longer bound to the input tokens at the given position, limiting the interpretability and even bearing the risk for potential deceptions from alignment inspection. Given the above limitations, space-shaping and set-based approaches, however, result in inherently interpretable models as opposed to requiring post-hoc explainability to gain insights into their prediction mechanisms \cite{stop_explaining}. In contrast, attribution-based methods can mitigate the contextualization issue as they attribute predictions to earlier representations. They also generalize to arbitrary models as long as they are differentiable and do not require modification and training of models. Different from other explainability methods, they can provide a set of theoretical guarantees \cite{ig, int_hessians}, e.g. that attributions must sum to the prediction score \cite{moeller-etal-2023-attribution}. However, it has been shown that nevertheless there remain fundamental limitations in their faithfulness \citep{bilodeau} coming at a trade-off between higher computational expenses but general applicability to any model.

\paragraph{What's the ``right'' explanation?} Unfortunately there may be no straightforward answer to this question. Given the above trade-offs, no method can be seen to be guaranteed faithful \cite{murdoch}. Therefore, none of these approaches should be interpreted as true and unique explanations for model predictions. At the same time all of them provide insights into similarity models going beyond a single scalar similarity score. We argue, that while we cannot conclude individual methods to be unambiguous, we can still use them to gain deeper insights into model mechanisms which may lead to hypotheses about where these models fail and how they may be improved \citep{wiegreffe}. Rather than competing for the best explanation, which may not exist, we suggest to take individual methods as independent pieces of evidence for a given phenomenon. By now, we have, for instance, multiple pieces of evidence for the hypothesis that text similarity models do not sufficiently account for negation \cite{weller-etal-2024-nevir, moeller-etal-2024-approximate, nikolaev-pado-2023-representation}.

Which method to consult will also depend on the context in which a model is used. Higher-level human interpretable aspects of similarity may be best explained by the abstract meaning representations in an ``S3BERT'' model \cite{opitz-frank-2022-sbert}. Quickly accessible information about which parts of a query and a document are matched in an IR model without requiring back propagation, may be best achieved by a set-based ColBERT model \cite{santhanam-etal-2022-colbertv2}. However, if we require dense representations to build a memory-efficient vector-index that enables approximate search and we don’t want to constrain the model otherwise, an attribution-based approach like BiLRP may be the right choice \cite{vasileiou-eberle-2024-explaining}. If, additionally, we require a statement of how trustworthy the explanations are the exact variant of IJ would be an apt choice \cite{moeller-etal-2023-attribution}.

\paragraph{Other challenges.} As models become capable of ingesting longer context \cite{zhang-etal-2024-mgte, xiong-etal-2024-effective}, we may wonder if interpretability approaches transfer to explaining the similarity of \textbf{long documents}. We speculate that some patterns may indeed generalize, especially more abstract ones, like the topics that occur in them. Other axes of similarity may be style-related (``both books are written in 18th century British English''), or stance related (``both arguments are in favor of green energy''). Fine-grained explanations like token attributions may necessitate a meta explanation step.

The embedding research landscape has also found another recent focus in \textbf{multi-linguality} \citep{wang2024multilingual}.  Since many facets of text semantics appear universal, from coarser attributes like entities or topics, to finer ones, like semantic roles and polarity, we see a fruitful venue in studying language phenomena cross-lingually through the eyes of interpretable embedding models, or testing hypotheses about universal semantics. 

\section{More Explanations and Related Work}
\label{sec:other}

In this last section, we study related work on similarity interpretability. We want to focus on \textbf{evaluation studies and datasets} that elicit explanations.


\paragraph{Datasets.} \citet{LOPEZGAZPIO2017186} release the i(nterpretable)STS data set that elicits relations and similarities between individual segments of texts. \citet{deshpande-etal-2023-c} propose the C(onditional)STS dataset that elicits similarity values for specific aspect of interest. The theory that underlies iSTS aligns with attribution or set-based approaches, while CSTS is motivated by a more abstract multi-aspect view akin to what is sought by feature-based explainability methods. 

The STS3k data set \citep{fodor2024compositionality} consists of systematically controlled pairs of sentences rated for semantic similarity by human participants. Through experiments on STS3k they find that ``state-of-the-art transformers poorly capture the pattern of human semantic similarity judgments.''

Datasets might also be repurposed for interpretability studies. E.g., evaluation data that highlight error spans \citep{10.1162/tacl_a_00437freitag, leiter2023eval4nlp}, or explained interactions between text spans in NLI  \citep{spanex_dataset}.

\paragraph{Similarity interpretability studies.} \citet{nikolaev-pado-2023-representation} construct sets of sentences with pre-defined lexical and syntactic structures. Their study reveals that model-assigned similarities are more strongly determined by the overlap in the set of their noun participants than by having the same predicates, lengthy nominal modifiers, or adjuncts. 

\citet{weller-etal-2024-nevir} ask similarity models to rank documents that differ only by negation. They find that most current information retrieval models do not consider negation, performing similarly or worse than randomly ranking.

\citet{nastase-merlo-2024-tracking} track linguistic information in embedding models via specialized datasets that test for grammatical and semantic agreement, finding that aspectual semantic knowledge can be localized in certain embedding regions.

\section{Conclusion}

What makes two texts similar in the eyes of a model? We gave an introduction and overview of an emerging branch of text embedding models: The challenge of similarity interpretability and explanation. We hope that our work can be a handy resource and entrance point for future research.

\section*{Limitations}

Capturing the full breadth of the emerging area of interpretable text embeddings and their similarity cost us some depth and exactness. Particularly in \S \ref{sec:discuss} where we summarized the methods over all three general interpretability approaches, we introduced some fuzzy and coarse concepts, e.g., ``token-set'' (rather: token-embedding set [from the last layer]); $k$ in inference cost (sometimes the size of encountered token or embedding sets, sometimes it's model encoder calls), etc. Nevertheless, the slight fuzziness also helped us to discuss and compare the diverse methods that have varying degrees of complexity---simple set matching to second-order attribution methods based on Jacobians of encoders---from a thousand foot view. There is also a chance that we missed some papers, hence we suggest viewing our primer as a guide and a survey that is representative of the area, but possibly not fully exhaustive.

About the three examples shown in our paper: They are selected to outline the idea behind three different types of approaches. We selected different text pairs to highlight these ideas, in order to avoid any possible potential for invoking example readings that favor one method over the other, which is not possible based on a single example, anyway. We leave deeper qualitative comparison of provided explanations to future work, and discussed the need for this in \S \ref{ssec:pchallenges}.

\section*{Acknowledgments}

Three authors received funding from the Swiss National Science Foundation (SNSF 213585) and the Luxembourg National Research Fund (17498891) through the \textit{Impresso} research project.


\bibliography{custom}

%\appendix

%\section{Example Appendix}
%\label{sec:appendix}

%This is an appendix.

\end{document}
