\section{Experiments}
\label{sec:experiments}

\subsection{Implementation details}

\begin{figure*}[t]
    \centering
    \vspace{-1em}
    \includegraphics[width=0.95\textwidth]{figs/MPT2V_results.pdf}
    \vspace{-0.5em}
    \caption{\textbf{Qualitative results of Movie Weaver.} \workname supports different combinations of reference images and can generate high-quality videos with high identity preservation. We encourage readers to check our video results in the supplementary materials.}
    \label{fig:mpt2v_results}
    \vspace{-0.5em}
\end{figure*}


\textbf{Pretraining data} Our Movie Weaver supports 5 configurations: face, face-body, face-body-animal, two-face and two-face-body, as showcase in Figure~\ref{fig:teaser}.
We collect all our videos from Shutterstock Video~\cite{ShutterstockVideo}.
For the face and face-body configurations, we curated 100K videos featuring only a single person performing various activities.
In the two-face and two-face-body configurations, videos were selected based on the presence of exactly two individuals, verified via a detection model, resulting in a dataset comprising 118K videos. 
For the face-body-animal setup, we found it more challenging to source videos featuring both a person and an animal, ultimately assembling a collection of 10K videos.
We conduct mix-pretraining, where we equally sample examples for each configuration using data resampling. More information can be found in Section~\ref{sec:mix_training}.

\textbf{Finetuning data} Existing research~\cite{dai2023emu,he2024imagine,moviegen} suggests that further finetuning on a small-scale, very high quality data significantly enhances visual quality and subject motion. 
We follow this principle by manually selecting videos with large human motion, rich human iterations, and high visual aesthetic. 
Same as pretraining data, we source different videos for our different configurations.
In summary, our finetuning set has 291 videos for face and face-body, 175 videos for two-face and two-face-body, and 185 videos for face-body-animal.


\textbf{Model details}
We adopt the Movie Gen architecture~\cite{moviegen}, with two versions: a 4B parameter model for the ablation study and a 30B parameter model for the final results. 
Unless specified otherwise, Movie Weaver refers to the 30B model. 
Movie Weaver uses MetaCLIP~\cite{xu2023demystifying} as image encoder and uses three text encoders: MetaCLIP~\cite{xu2023demystifying}, ByT5~\cite{xue2022byt5} and UL2~\cite{tay2022ul2}. 
We implement concept embeddings (CE) using learnable nn.Embedding in PyTorch, applied only to image embeddings.
The diffusion model contains 48 layers of diffusion transformers~\cite{peebles2023scalable}.
The temporal VAE has a compression rate of $8\times8\times8$, represents $8\times$ dimension reduce for spatial height/width and temporal frame.
Using an additional $2\times2\times1$ patchification, we compressed each 128-frame landscape video with 544$\times$960 resolution into a token sequence of length 32K.

\textbf{Training hyperparameters}
We initialize \workname from a pre-trained single-face personalization Movie Gen checkpoint. 
The model is trained with a learning rate of 1e-5 using the AdamW~\cite{loshchilov2017decoupled} optimizer for 20K iterations with a batch size of 32.
The training objective is flow matching~\cite{lipman2022flow} with optimal transport path. 
It took around 5 days to do pretraining on a cluster of 256 H100 GPUs.
Following this, we performed supervised finetuning with a reduced learning rate of 2e-6 for an additional 2K iterations.

\textbf{Test data}
Our test set has 5 configurations with 300+ reference-prompt pairs each, covering diverse ethics, human genders, animal types, and prompt styles. Human images are generated via text-to-image models EMU~\cite{dai2023emu}, while animal images are from external datasets (not in our training) like DreamBooth~\cite{ruiz2023dreambooth}. Prompts are generated via LLM in-context learning. 


\subsection{Results}



\begin{figure*}[t]
    \centering
    \includegraphics[width=0.95\textwidth]{figs/Comparison_with_sota.pdf}
    \vspace{-0.5em}
    \caption{\textbf{Comparison with state-of-the-art multi-concept video methods.} Compared to proprietary Vidu 1.5, Movie Weaver demonstrates superior identity preservation for both human and animal reference images.}
    \label{fig:comparison_sota}
    \vspace{-1em}
\end{figure*}


\subsubsection{Performance highlight}
We demonstrate three configurations of Movie Weaver: face-body-animal, two-face, and two-face-body, in Figure~\ref{fig:mpt2v_results}. 
For the same text prompt, we generate two different videos with two different sets of reference images. 
Notably, no facial, clothing, or dog descriptions are included in the prompts; all identity information is derived solely from the reference images. 
We highlight key features of our Movie Weaver: 
(1) Identity Preservation: Face, body, and animal details are accurately maintained in the generated videos.
Even small clothing details, such as the logo on the gray T-shirt in the first video and the tear on the white T-shirt in the sixth, are faithfully preserved.
In challenging two-face scenarios with same-gender, same-race pairs, Movie Weaver effectively retains each identity.
(2) Flexibility with References: Movie Weaver can adapt reference images to match the prompt without having to strictly follow. 
For instance, in the second video, the reference body image shows a man standing, yet the prompt requires him to sit. 
Our \workname selectively uses the upper body to align with the prompt.
Also for the fifth video, the standing woman body reference image is adapted to appear seated on a beach.
(3) Rich iteration between subjects: Beyond preserving identity, the generated videos capture dynamic interactions between subjects. In the first and second videos, the person interacts well with the dog, while in third through sixth videos, the two people display rich engagement.

\subsubsection{Comparison with existing methods}


\textbf{Multi-concept video personalization.} We compare \workname with the proprietary Vidu 1.5~\cite{vidustudio}, which is the state-of-the-art model that supports multi-concept video personalization. 
We observed their demos use cropped images, so we prepared the reference images without masking.
In the first face-body-animal example of Figure~\ref{fig:comparison_sota}, \workname shows better identity preservation for both the African American woman and the dog. 
While Vidu 1.5 correctly identifies the dog as a Corgi, it fails to reconstruct the distinctive white spots on the dog's face, which are crucial to the dog's identity.
In the second two-face-body example, because Vidu 1.5 supports a maximum of 3 reference images, we only input the face of the first character. 
Vidu 1.5 suffers from severe identity blending issues with the two generated characters looking and wearing similarly to each other, whereas \workname maintains clear distinctions between the two individuals.


\textbf{Multi-concept image personalization.} 
We also compare with representative multi-concept image personalization methods. 
For Tweediemix~\cite{kwon2024tweediemix}, we first fine-tune the base SDXL~\cite{podell2023sdxl} model for each reference concept using LORA~\cite{hu2021lora}, then conduct multi-concpet sampling using Tweedieâ€™s formula. 
Because Tweediemix requires background reference, we select one of its pretrained garden LORA weights.
Freecustom~\cite{ding2024freecustom} is a tuning-free method, so we follow its practice by preparing two reference faces.
We select the first frame of our Movie Weaver to compare with these image methods.
As shown in Figure~\ref{fig:multi_concept_t2i}, our Movie Weaver preserves a much better identity and has higher visual quality when compared with TweedieMix and FreeCustom.


\begin{figure}[t]
    \begin{center}
    \includegraphics[width=0.85\linewidth]{figs/multiT2I.pdf}
    \end{center}
    \vspace{-2em}
        \caption{\textbf{Comparison with multi-concept image methods.} Movie Weaver has a better identity preserving and visual quality.}
        \vspace{-1em}
    \label{fig:multi_concept_t2i}
\end{figure}


\textbf{Single-concept video personalization.} Although our Movie Weaver targets at multi-concept, it can perform well on single-concept setting. We compare with ID-Animator~\cite{he2024id} and DreamVideo~\cite{wei2024dreamvideo}, using 97 single-face personalization data. Following DreamVideo, we use four metrics, CLIP-T, CLIP-I, DINO-I, and Temporal Consistency. 
As we can see in Table~\ref{tab:quant_comp_singleface}, our Movie Weaver excels in all the metrics.  




\begin{table}[t]
\vspace{-0.8em}
\centering
\small
% \renewcommand{\arraystretch}{0.8} % Adjust row spacing
\setlength{\tabcolsep}{3pt} % Adjust column spacing
\begin{tabular}{lcccc}
\toprule
\textbf{Method} & \textbf{CLIP-T} & \textbf{CLIP-I} & \textbf{DINO-I} & \textbf{Tem. Cons.} \\
\midrule
DreamVideo  & 0.282     &  0.498      &  0.246      &  0.956  \\
ID-Animator & 0.274     &  0.642      &  0.405      &  0.985  \\
Movie Weaver (ours) & \textbf{0.293}  &  \textbf{0.659}      &  \textbf{0.421}      &  \textbf{0.997}  \\
\bottomrule
\end{tabular}
\vspace{-1em}
\caption{\textbf{Quantitative comparison of single-face video personalization.} Movie Weaver excels in four metrics.}
\label{tab:quant_comp_singleface}
% \vspace{-1.2em}
\end{table}


\begin{figure}[t]
    \centering
    \includegraphics[width=0.43\textwidth]{figs/ablation_AP_CE.pdf}
    \small
    \begin{tabular}{ccccccc}
        \toprule
    \multirow{2}{*}{Case} & \multicolumn{2}{c}{Modules} & \multicolumn{3}{c}{Human study metrics} \\
    \cmidrule(lr){2-3} \cmidrule(lr){4-6}
    & AP & CE & sep\_yes$\uparrow$ & face1\_sim$\uparrow$ & face2\_sim$\uparrow$ \\
        \midrule
        Baseline &    &    & 42.9 & 3.4 & 3.0 \\
        (1)        &    & \checkmark & 98.2 & 58.8  & 41.9  \\
        (2)        & \checkmark & \checkmark & 99.3 & 66.8 & 66.1  \\
        \bottomrule
    \end{tabular}
    \vspace{-1em}
    \caption{\textbf{Ablation study of Anchored Prompts (AP) and Concept Embeddings (CE).} The top part shows the effect of AP and CE, while the bottom presents results from a human study. Metric sep\_yes indicates the percentage of cases where the two generated faces are distinguishable (\ie, no identity blending), face1\_sim and face2\_sim represent where a similar face to the left or right reference face, respectively, is found in the generated video.}
    \vspace{-1em}
\label{fig:ablation_ap_ce}
\end{figure}





\begin{figure*}[t]
    \centering
    \vspace{-1.55em}
    \includegraphics[width=0.93\textwidth]{figs/Limitations.pdf}
    \vspace{-1em}
    \caption{\textbf{Limitations of Movie Weaver.} Reference images can dominate generation, resulting in 'big-face' videos. Our model also struggles to generalize to configurations not seen during training.}
    \label{fig:limitations}
    \vspace{-1em}
\end{figure*}

\subsection{Ablation study}
\label{sec:ablation_study}

\subsubsection{Anchored prompts and concept embeddings}
\label{sec:ablation_ap_ce}
In this section, we analyze the effects of the proposed Anchored Prompts (AP) and Concept Embeddings (CE). 
Identity blending was less apparent in ``face-body-animal" configurations because human animal embeddings are distinct. 
Thus, we conduct a human study on a two-face personalization evaluation dataset with 300 image pairs.
As in the top part of Figure~\ref{fig:ablation_ap_ce}, the baseline suffers from the identity blending issue, reflected by a low sep\_yes score, which indicates percentage of cases where the two generated faces are separable. 
Introducing concept embeddings, as seen in case (1), raises the sep\_yes score from 42.9\% to 98.2\%.
In case (2), anchored prompts further enhance identity preservation, increasing face1\_sim and face2\_sim scores. These scores represent the percentage of cases where a similar face to the left (face1) and right (face2) reference face is identifiable in the video.
Ablation with ``two-face-body" configuration can be found in  Appendix~\ref{appendix:ablation_twofacebody} and more details about human study can be found in Appendix~\ref{appendix:two_face_human_eval}.

When comparing CE with positional embeddings (PE), using the same setting as Figure~\ref{fig:ablation_ap_ce} Case (1), PE achieves 90.5\% sep\_yes score while CE achieves 98.2\%.




\subsubsection{Mixed training}
\label{sec:mix_training}

Our \workname is trained on various combinations of reference images, and this section examines the impact of this mixed training approach.
For a fair comparison, instead of initializing from a single-face personalization checkpoint, we start with a text-to-video base model.
As shown in Table~\ref{tab:mix_training}, we try different data configurations (1F: one-face, 2F: two-face and F-B-A: face-body-animal) and evaluate with a single-face personalization human study, which comprises 300 datapoints.
Compared to case (1), which uses only one-face data, mixed training with additional configurations improves both face\_sim (similarity of the generated character to the reference face) and face\_cons (consistency of the face throughout the video). 
We attribute this improvement to the increased data diversity provided by mixed training.



\begin{table}[t]
    \caption{\textbf{Ablation study of mixed training with multiple reference configurations.} 1F, 2F and F-B-A represents one-face, two-face and face-body-animal data, respectively. The metric face\_sim and face\_cons represents the similarity of character to the reference face and face consistency throughout the video.}
    \vspace{-0.5em}
    \centering
    \small
    \begin{tabular}{cccccc}
        \toprule
        \multirow{2}{*}{Case} & \multicolumn{3}{c}{Pretraining data} & \multicolumn{2}{c}{Human study metrics} \\
        \cmidrule(lr){2-4} \cmidrule(lr){5-6}
         & 1F & 2F & F-B-A & face\_sim$\uparrow$ & face\_cons$\uparrow$ \\
        \midrule
        1 & \checkmark &       &       & 13.1 & 70   \\
        2 & \checkmark & \checkmark &       & 29.5 & 81.3 \\
        3 & \checkmark & \checkmark & \checkmark & 46   & 91   \\
        \bottomrule
    \end{tabular}
    \vspace{-1em}
    \label{tab:mix_training}
\end{table}

\subsection{Limitations}



While \workname demonstrates strong multi-concept personalization capabilities, it has certain limitations. 
Firstly, the personalized videos often have limited overall motion compared to the results of base text-to-video model, and we sometimes see 'big-face' videos where faces occupy a large portion of the video frame.
We believe this occurs because the reference images can dominate the generation, leading to reduced motion and poor prompt alignment.
For instance, in the first example in Figure~\ref{fig:limitations}, we provide two half-body reference images with a prompt involving playing basketball.
The generated video reproduces the half-bodies but fails to align well with the action described in the prompt.
The underlying reason is \workname struggles to balance influence of reference images and text prompts when they are mismatched.
During training, videos of sports activities like basketball typically include full-body references, whereas during inference, users may provide less aligned inputs. Addressing the balance between reference images and prompts remains an area for future improvement.
Secondly, \workname struggles to generalize to configurations not seen during training.
In the second example in Figure~\ref{fig:limitations}, the goal is to generate a video with three people talking, but the result only features two people.
This is because we don't have any training videos that contain more than two people. 
Addressing this issue will require incorporating additional data configurations during pretraining, which we identify as a direction for future work.
