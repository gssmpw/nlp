
\appendix

\section{Appendix}
\subsection{Data curation}

\paragraph{Video processing and filtering.}
For a given video, we uniformly sample five frames and apply a large-vocabulary object detector~\cite{zhou2022detecting} to each frame. The intersection of all detected objects across these frames is used to determine the objects present throughout the video. Using these detection results, we filter videos based on specific criteria. For example, to select videos featuring two people, we require two 'person' bounding boxes in the detection results. Similarly, for videos with one person and an animal, we ensure there is exactly one 'person' bounding box along with a 'dog' or 'cat' bounding box.

\paragraph{Two-face data curation.}
After obtaining the two-person video data, we utilize a suite of foundational models to generate anchored prompts and ordered reference images, as described in Section 4.3 of the main paper. Building on the approach of Movie Gen~\cite{moviegen}, we first employ the LLaMa3-Video~\cite{dubey2024llama} model to produce detailed text prompts for the video clips. These prompts follow a structured format, enabling the use of in-context learning to extract concept descriptions.
For example, given the input prompt: \texttt{Dentist Appointment. Senior woman smiling listening to her dentist during consultation.}, the outputs are two concept phrases: \texttt{[Senior woman smiling, her dentist]} and the anchored prompt: \texttt{Dentist Appointment. Senior woman smiling \textless{}ID1\textgreater{} listening to her dentist \textless{}ID2\textgreater{} during consultation.} Additional examples can be found in \href{https://jeff-liangf.github.io/projects/movieweaver/supp/in_context_twoface.txt}{\texttt{in\_context\_twoface.txt}}. Here, \texttt{\textless{}ID1\textgreater{}} and \texttt{\textless{}ID2\textgreater{}} represent \texttt{[R1]} and \texttt{[R2]}, respectively.
We further refine the output by ensuring that the concept phrases contain exactly two items and that both \texttt{\textless{}ID1\textgreater{}} and \texttt{\textless{}ID2\textgreater{}} appear in the anchored prompt.

\paragraph{Two-facebody data curation.}
After generating the two-face anchored prompt, creating the two-facebody prompt is straightforward. This involves replacing the original \texttt{\textless{}ID2\textgreater{}} with \texttt{\textless{}ID3\textgreater{} \textless{}ID4\textgreater{}} and \texttt{\textless{}ID1\textgreater{}} with \texttt{\textless{}ID2\textgreater{} \textless{}ID2\textgreater{}}. Additionally, we prepare the ordered two-face-body reference images to align with the updated prompt structure.

\paragraph{Face-body-animal data curation.}
We filter videos that feature one person with a pet (dog or cat). We use in-context examples to add three anchors to the original prompt. Examples can be found in \href{https://jeff-liangf.github.io/projects/movieweaver/supp/in_context_facebodyanimal.txt}{\texttt{in\_context\_facebodyanimal.txt}}



\subsection{Human evaluation}


\subsubsection{Two-face human evaluation}
\label{appendix:two_face_human_eval}

We conduct a human evaluation with 300 evaluation samples to ablate the effectiveness of the proposed anchored prompts and concept embeddings in Section 5.3.1. We provide the evaluation guidance as below. Besides the text guideline, we also include some visual examples to better help the annotators to judge.

\paragraph{Guidance.} This document describes how to do Movie Weaver two-face character consistency evaluation on generated video and their reference faces. 
The focus is on personalized video generation, where two reference faces are used to create a video, and the evaluation assesses how well the two generated characters maintain a consistent visual appearance compared to the two reference faces. We will be primarily focused on human characters (realistic or stylized).

\paragraph{Task description.} Annotators will be shown a set of two-faces and a generated video.
They are then asked to rate the character consistency level on the set of generated frames based on a few different questions related to the visual appearance of the person(s) in the reference image(s). 

\paragraph{Questions}
\begin{enumerate}[label=-]
    \item In the worst frame (they are not separable), are the two faces separable in the generated video (no fusion within two faces):
    \begin{itemize}
        \item [ ] 1 - Totally separable
        \item [ ] 2 - Somewhat separable
        \item [ ] 3 - Not separable
        \item [ ] 4 - Only one face or no face or more than two faces generated or visible
    \end{itemize}
    
    \textbf{Note:} In the specific example in Figure 3, annotators are expected to give the answer ``not separable''
    \item For the LEFT face in the reference, how well does the best aligned generated character’s face capture the person likeness? (Please first try the best to locate the best aligned character for the left reference face):
    \begin{itemize}
        \item [ ] 1 - Really similar
        \item [ ] 2 - Somewhat similar
        \item [ ] 3 - Not similar
        \item [ ] 4 - Only one face or no face or more than two faces generated or visible
    \end{itemize}
    
    \textbf{Note:} In this specific example in Figure 3, annotators are expected to give the answer ``Not similar''
    \item For the RIGHT face in the reference, how well does the best aligned generated character’s face capture the person likeness? (Please first try the best to locate the best aligned character for the right reference face):
    \begin{itemize}
        \item [ ] 1 - Really similar
        \item [ ] 2 - Somewhat similar
        \item [ ] 3 - Not similar
        \item [ ] 4 - Only one face or no face or more than two faces generated or visible
    \end{itemize}
    
    \textbf{Note:} In this specific example in Figure 3, annotators are expected to give the answer ``Not similar''
\end{enumerate}


\subsubsection{One-face human evaluation}

We perform a human evaluation with 300 samples to assess the effectiveness of mixed training, as discussed in Section 5.3.2. The evaluation protocol closely follows that of single-face personalized Movie Gen~\cite{moviegen}. Specifically, annotators are provided with a reference image and a generated video clip and asked to rate two aspects:
Face similarity (face\_sim): How well the generated character’s face matches the reference person in the best frame.
Face Consistency Score (face\_cons): How visually consistent the faces are across all frames containing the reference person.
Ratings are given on an absolute scale: “really similar,” “somewhat similar,” and “not similar” for identity, and “really consistent,” “somewhat consistent,” and “not consistent” for face consistency. Annotators are trained to adhere to specific labeling guidelines and are continuously audited to ensure quality and reliability.



\subsection{Additional results}


\subsubsection{Ablation on two-face-body configuration}
\label{appendix:ablation_twofacebody}
As shown in Table~\ref{tab:ablation_two_face_body}, ablation with ``two-face-body" showed similar trends to ``two-face" configurations. However, clothing details, like small logos in Figure~\ref{fig:teaser}, are harder to retain, likely due to the 256px reference resolution. Higher-resolution references may enhance clothing detail preservation.

\begin{table}[t]
    \centering
    \small
    \setlength{\tabcolsep}{3pt} % Adjust column spacing
    \vspace{-1em}
    \begin{tabular}{cccccc}
        \toprule
        \multirow{2}{*}{Case} & \multicolumn{2}{c}{Modules} & \multicolumn{3}{c}{Human study metrics} \\
        \cmidrule(lr){2-3} \cmidrule(lr){4-6}
        & AP & CE & sep\_yes$\uparrow$ & human1\_sim$\uparrow$ & human2\_sim$\uparrow$ \\
        \midrule
        Baseline &    &    & 54.8 & 12.3 & 16.5 \\
        (1)        &    & \checkmark & 98.8 & 66.7  & 69.4  \\
        (2)        & \checkmark & \checkmark & 98.0 & 72.3 & 71.1 \\
        \bottomrule
    \end{tabular}
    % \vspace{-1em}
    \caption{\textbf{Ablation study of Anchored Prompts (AP) and Concept Embeddings (CE) on ``two-face-body" config.}}
    \label{tab:ablation_two_face_body}
\end{table}
% \vspace{-1.2em}



\subsubsection{Order of reference images}
In this section, we examine how the order of reference images influences the final output. Since the order information is incorporated through concept embeddings, altering the sequence of reference images results in different videos, even with the same prompt. This effect is illustrated in Figure~\ref{fig:swap_reference}.



\begin{figure}[t]
    \begin{center}
    \includegraphics[width=1.0\linewidth]{figs/reference_ID.pdf}
    \end{center}
    \vspace{-1.5em}
        \caption{By changing the order of reference images, we can assign certain face to certain attributes.}
        \vspace{-1em}
    \label{fig:swap_reference}
\end{figure}
