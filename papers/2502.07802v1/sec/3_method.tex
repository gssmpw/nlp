\section{Challenges in Extending Single-Concept to Multi-Concept Personalization}


\begin{figure}[t]
    \begin{center}
    \includegraphics[width=0.9\linewidth]{figs/single_face_personalization.pdf}
    \end{center}
    \vspace{-1.5em}
        \caption{\textbf{Single-concept personalization architecture.} Building on a pre-trained text-to-video model, this approach adds an image encoder to process reference images. Image and text tokens are concatenated and fed into cross attention layer.}
        \vspace{-1em}
    \label{fig:single_concept_arch}
\end{figure}



\subsection{Singe-concept video personalization}

Based on a pre-trained text-to-video diffusion model, prior single-concept video personalization methods~\cite{he2024id, wu2024customcrafter, jiang2024videobooth, moviegen} typically introduce an additional CLIP image encoder~\cite{radford2021learning,xu2023demystifying} to process reference images. 
As shown in Figure~\ref{fig:single_concept_arch}, given a dataset triplet $\{txt, img, vid\}$—representing text prompt, reference image, and target video—these methods extract text tokens $T$ and image tokens $I$ via text and image encoders. 
The tokens $I$ and $T$ are concatenated and then fed into the cross attention layer of the diffusion transformer~\cite{peebles2023scalable}.

\vspace{-1em}
\begin{equation}
\text{CrossAttn} = \text{softmax}\left(\frac{Q_Z\left[ K_T, K_I \right]^{Tr}}{\sqrt{d}}\right) \left[V_T, V_I \right]
\label{eq:cross_attention}
\end{equation}

Here, the query is derived from the video latent $Z$, and the key/value is derived from the concatenation of $I$ and $T$. 
$\left[ \cdot \right]$ and $^{Tr}$ denotes the concatenation and transpose operation, respectively. 
Key and value are derived through linear projection, so $\left[V_T, V_I \right] = V_{\left[T,I\right]}$. For simplicity, we maintain formal notation without specifying this further.


\subsection{Naive extension to multi-concept}


\begin{figure}[t]
    \begin{center}
    \includegraphics[width=\linewidth]{figs/Identity_blending.pdf}
    \end{center}
    \vspace{-2em}
        \caption{\textbf{Identity blending} generates composite faces with characteristics from both references. Text prompt: \texttt{"A woman in wheelchair discussing with a woman nurse."}}
        \vspace{-1em}
    \label{fig:identity_blending}
\end{figure}


To support multi-concept personalization, a straightforward approach is preparing image tokens from multiple reference images. 
We began by experimenting with two-face personalization. 
According to Equation~\ref{eq:cross_attention}, this results in $ K_I = \left[ K_{I_1}, K_{I_2} \right]$ and $ V_I = \left[ V_{I_1}, V_{I_2} \right]$, where $I_1$ and $I_2$ represents tokens of two different faces.
This naive approach, however, led to severe identity blending~\cite{kumari2023multi,xiao2024fastcomposer}, where the characteristics of two faces would fuse together, resulting in a composite one as seen in Figure~\ref{fig:identity_blending}. 
The issue arises because the model cannot effectively link each concept description to its corresponding image. 
In cross attention, a query latent in $Q_Z$ can attend to the entire prompt, including two text tokens of "\texttt{woman}", and cannot distinguish between vision tokens $I_1$ and $I_2$ due to order-agnostic processing. 
For accurate video generation, the model must link "\texttt{A woman in wheelchair}" to the first image and "\texttt{a woman nurse}" to the second image.
% To address this, we introduce anchored prompts to explicitly link each concept description with its corresponding image and add concept embeddings on vision tokens to strengthen their distinction.


\section{Movie Weaver}
\label{sec:method}

Unlike traditional approaches that require concept-specific tuning or architectural modifications, Movie Weaver aims to enable multi-concept video personalization with architectural simplicity and flexibility. 
Movie Weaver introduces two novel components, namely anchored prompts and concept embeddings, alongside an automatic data curation pipeline, all of which allow accurate, tuning-free multi-concept video generation.


\begin{figure*}[t]
    \centering
    \vspace{-2em}
    \includegraphics[width=0.98\textwidth]{figs/data_curation.pdf}
    \vspace{-0.5em}
    \caption{(a) \textbf{Data curation.} For a video-text pair, \ding{172} concept descriptions and anchored prompts are generated via in-context learning with Llama-3. After \ding{173} extracting body masks, \ding{174} CLIP links each concept to its corresponding image. \ding{175} Finally, face images are obtained using a face segmentation model. (b) \textbf{\workname architecture.} Compared to the single-concept baseline, reference images are arranged in a specific order for concept embedding, and anchored prompts are utilized. Shared components are omitted for simplicity.}
    \label{fig:data_curation}
    \vspace{-1em}
\end{figure*}


\subsection{Anchored prompts}

The success of multi-concept personalization lies in accurately associating each concept with its corresponding image. 
Previous methods often rely on predefined layouts~\cite{liu2023cones,gu2024mix} or complex masked attention modules~\cite{xiao2024fastcomposer,kim2024instantfamily,he2024uniportrait,ostashev2024moa} to establish these associations, which increases model complexity and limits flexibility.
Our \workname introduces a streamlined solution with anchored prompts.
For prompt in Figure~\ref{fig:identity_blending}, we use Llama-3~\cite{dubey2024llama} to identify the concept descriptions "\texttt{A woman in wheelchair}" and "\texttt{a woman nurse}", we then append unique tokens (\eg, \texttt{[R1]}, \texttt{[R2]}) after each description, creating the prompt "\texttt{A woman in wheelchair [R1] discussing with a woman nurse [R2].}"
Ordered reference images are then linked to these unique tokens, allowing the model to associate each description precisely with its reference image.
This approach offers two key advantages: (1) Flexibility: anchored prompts can easily extend to different descriptions, such as body or animal descriptions. Moreover, it allows to append multiple references, such as face and body images, on the same person.; and (2) Simplicity: this approach requires only input modifications, allowing Movie Weaver to retain an architecture similar to single-concept models.

\subsection{Concept embeddings}


While anchored prompts establish explicit associations, we also need to encode the order information of reference images. 
The cross attention mechanism, as outlined in Equation~\ref{eq:cross_attention}, is inherently order-agnostic, \ie, swapping the order of two reference images yields identical results.
To address this, Movie Weaver introduces concept embeddings, a novel adaptation of positional encoding~\cite{vaswani2017attention} tailored to multi-concept personalization. 
Specifically, given $I_1$ and $I_2$ as image tokens from two different reference images, we add the same concept embedding $Pos_1$ and $Pos_2$  to each set of tokens, respectively:


\begin{equation}
I'_1 = I_1 + Pos_1,  I'_2 = I_2 + Pos_2, ...
\label{eq:concept_embedding}
\end{equation}


Here, $I_1$ and $I_2$ have dimensions $[N, C]$, while $Pos_1$ and $Pos_2$ are of shape $[1, C]$, where $N$ is the number of tokens and $C$ is the feature dimension. Using the broadcasting of Pytorch, the same concept embedding is added to the entire set. 
This is different from traditional positional encoding where distinct embeddings are added to individual tokens.
We also experimented with per-token positional encoding, but it produced less effective results compared to using a concept embedding for each set of vision tokens (see Section~\ref{sec:ablation_ap_ce}). 
% Further ablation results are detailed in Section~\ref{sec:ablation_study}.


\subsection{Data curation pipeline}
\label{sec:overall_architecture}

Starting with text-video pairs, we curate data by leveraging a set of foundation models.
We take the preparation of two-face configuration as an example in Figure~\ref{fig:data_curation} (a). 
We first use in-context learning of Llama-3~\cite{dubey2024llama} to extract concept descriptions from the original prompt and get a rewritten anchored prompt. 
For the reference frame, typically the first frame of the video clip, we use a detection model Detic~\cite{zhou2022detecting} and a segmentation model SAM~\cite{kirillov2023segment} to extract the subject masks.
Using the detection results, we can also tell the number of people in the video and other objects in the video, which helps us filter the data.
Then, a pre-trained CLIP~\cite{radford2021learning} assigns the concept descriptions to the body images, establishing the link between \texttt{[R]} and reference images. Lastly, we use a face segmentation model to extract face masks from body images.
While this example only illustrates the two-face scenarios, the approach can be naturally extended to other combinations with more \texttt{[R]}s and reference images. 
More examples can be found in the supplementary materials. 

With ordered reference images and rewritten anchored prompts prepared, our Movie Weaver architecture is illustrated in Figure~\ref{fig:data_curation}(b). 
Compared with single-concept architecture in Figure~\ref{fig:single_concept_arch}, our model requires reference images in a specific order to apply concept embeddings effectively. We also need to have corresponding anchored prompts to strengthen the results.
