\section{Related Work}
\label{sec:related_work}

\paragraph{Personalized image generation}
Personalized generation begins with identity-specific tuning methods that further finetune a text-to-image model on a set of reference images. 
For instance, Textual Inversion~\cite{gal2022image} finetunes special text tokens for the target identity. 
DreamBooth~\cite{ruiz2023dreambooth} further conducts end-to-end model finetuning besides tuning the special text tokens.
Custom Diffusion~\cite{kumari2023multi} extends a parameter-efficient finetuning technique to incorporate multiple concepts. 
However, these tuning-based methods require separate optimizations for every concept, which does not scale well in real applications.
Recent tuning-free methods train one base personalization model and then use it for arbitrary reference images in inference.
For example, ELITE~\cite{wei2023elite}, PhotoMaker~\cite{li2023photomaker}, PhotoVerse~\cite{chen2023photoverse}, IP-Adapter~\cite{ye2023ip} , InstantID~\cite{wang2024instantid}, and Imagine Yourself~\cite{dai2023emu,meta24memu} all leverage a vision encoder to extract visual tokens from the reference image and inject them to the diffusion process.
Our method \workname falls in the second tuning-free method which targets multi-concept personalization.


\paragraph{Personalized video generation}
While personalized image generation has shown promising results, personalized video generation remains an unsolved problem.
Compared to static images, personalized videos require more diverse and complex modifications on the reference image, \eg, turning the head, changing poses, and camera motion movements.
Pioneering work VideoBooth~\cite{jiang2024videobooth}, DisenStudio~\cite{chen2024disenstudio}, Magic-Me~\cite{ma2024magic}, DreamVideo~\cite{wei2024dreamvideo}, CustomVideo~\cite{wang2024customvideo}, TweedieMix~\cite{kwon2024tweediemix}, MultiConceptVideo~\cite{kothandaraman2024text} and CustomCrafter~\cite{wu2024customcrafter} use identity-specific finetuning to inject identity into a video generation model. 
However, these methods require separate fine-tuning for every identity, which limits their applications.
Another line of work includes tuning-free methods.
ID-Animator~\cite{he2024id}, MovieGen~\cite{moviegen} and Video Alchemist~\cite{chen2025multi} train a vision encoder~\cite{radford2021learning} to inject the reference image features into the diffusion models.
Our Movie Weaver falls in the tuning-free setting with a special focus on multi-concept personalization.


\paragraph{Multi-concept personalization}
Multi-concept personalization aims to generate harmonized content with multiple reference concepts.
Identity-specific tuning methods, \eg, Custom Diffusion~\cite{kumari2023multi}, Break-A-Scene~\cite{avrahami2023break}, Concept Weaver~\cite{kwon2024concept} and MuDI~\cite{jang2024identity}, achieve multi-concept personalization by finetuning the multiple text embeddings and model weights.
Tuning-free methods, \eg, FastComposer~\cite{xiao2024fastcomposer}, MoA~\cite{ostashev2024moa}, InstantFamily~\cite{kim2024instantfamily} and UniPortrait~\cite{he2024uniportrait}, train on large-scale text-image datasets and inject multiple image embeddings directly into diffusion process during inference.
Mix-of-show~\cite{gu2024mix} and OMG~\cite{kong2024omg} merge single-concept models but are limited by the availability of community models, primarily covering well-known intellectual property (IP) or celebrities.
Unlike these methods that separate concepts using predefined layout~\cite{liu2023cones,gu2024mix} or masked attention~\cite{xiao2024fastcomposer,kim2024instantfamily,kim2024instantfamily,ostashev2024moa} or word/image feature concatenation~\cite{chen2025multi}, our \workname proposes anchored prompts and concept embeddings to link concept with matched reference image without architectural change.
Concurrent work ViMi~\cite{fang2024vimi} is the closest to our setting but requires complex data retrieval to curate data and an additional multimodal LLM~\cite{liu2024visual} for the image-text process. In contrast, our data curation pipeline operates without supplementary retrieval data, while maintaining simplicity in architecture.
