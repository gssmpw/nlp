
\section{Introduction}
\label{sec:intro}

Foundational text-to-video generation models~\cite{ho2022imagen,villegas2022phenaki,singer2022make,zhou2022magicvideo,chen2023videocrafter1,blattmann2023align,girdhar2023emu,guo2023animatediff,kondratyuk2023videopoet,bar2024lumiere,yang2024cogvideox,chen2024gentron,sora,luma,gen3,kling,genmo2024mochi} have made substantial progress in the past few years.
Leveraging these advancements, personalized video generation enables users to create customized videos with their images, offering huge potential for applications like consistent storytelling.
However, prior efforts~\cite{he2024id, wu2024customcrafter, moviegen} primarily support single concept (face or object) personalization, limiting their use in complex, real-world scenarios.
Practical applications often require multi-concept compositions, like interactions between two people or between a person and a pet.
To meet this need, we introduce Movie Weaver, a video diffusion model that \emph{weaves} diverse image combinations and text prompts to create personalized multi-concept videos, as illustrated in Figure~\ref{fig:teaser}.

Established video personalization methods~\cite{he2024id, wu2024customcrafter, jiang2024videobooth, moviegen} typically extract vision tokens via an image encoder~\cite{radford2021learning}, then inject these tokens into diffusion models through cross attention.
To support multiple face references, we extend this approach by directly feeding concatenated vision tokens from multiple references into the cross attention layer.
% We extend this line of work, initially designed for single-face personalization, to support two faces. 
While this direct extension works in some cases, it often encounters a severe identity blending issue~\cite{kumari2023multi,xiao2024fastcomposer}, generating composite characters that fuse attributes from both references.
The issue is more severe when the character faces are close, \ie, from the same gender or race. 
This is because such a direct extension lacks the ability to associate each reference image with their descriptions within the prompt.
When vision tokens from different faces are similar, the model struggles to differentiate between them, resulting in identity blending.


To address this issue, we explicitly build the association between the concept description and the corresponding reference image. 
We introduce \emph{anchored prompts} by injecting unique text tokens (\texttt{[R1]}) after each concept description, as shown in Figure~\ref{fig:teaser}.
Upon encountering \texttt{[R1]}, the model links it with the matching reference image of \texttt{[R1]} and uses it as the visual supplement for the concept.
This anchored prompt approach extends easily to multiple concepts by adding more anchors (\eg, \texttt{[R2]}, \texttt{[R3]}) and corresponding images.
Notably, anchored prompts only require input modifications without any architectural changes like identity-specific tuning~\cite{kumari2023multi,kwon2024concept}, predefined layout~\cite{liu2023cones,gu2024mix} or masked attention~\cite{xiao2024fastcomposer,kim2024instantfamily,he2024uniportrait,ostashev2024moa}. 
This preserves architectural simplicity and leverages scalability.

With the anchored prompts established, the next question is how to distinguish the reference images of \texttt{[R1]} and \texttt{[R2]}.
Our baseline architecture~\cite{he2024id,jiang2024videobooth,moviegen} concatenates the vision tokens with text tokens and feeds them together to the cross attention layer of diffusion models.
However, since cross attention is order-agnostic, we must inject some information about the order of reference images.
Inspired by positional encoding~\cite{vaswani2017attention,dosovitskiy2020image}, we introduce \emph{concept embedding}, adding a unique embedding to each set of vision tokens from the reference image.
This is different from traditional positional encoding, where different embeddings are assigned to individual tokens. Our method applies a uniform concept embedding to all tokens from the same image.

We also propose an automatic data curation pipeline to get anchored prompts and ordered reference images.
This pipeline supports diverse reference combinations (such as face, face-body, face-body-animal, two-face, and two-face-body) by leveraging a suite of foundations models~\cite{liu2023grounding,kirillov2023segment,radford2021learning,dubey2024llama}, yielding a dataset of 230K videos. 
Using the proposed anchored prompts and concept embeddings, we continue training \workname on a pre-trained single-face video personalization model~\cite{moviegen}. 
As showcased in Figure~\ref{fig:teaser}, our model effectively generates high-quality videos with diverse reference combinations without additional tuning. 
Compared with proprietary Vidu 1.5~\cite{vidustudio} and extended baseline, \workname exceeds in identity preservation and visual quality.

Our contributions are three-fold: (1) \textbf{Anchored Prompts:} We introduce anchored prompts to link specific reference images with concept description, resolving identity blending in multi-concept video personalization without architectural changes. (2) \textbf{Concept Embeddings:} We use unique embeddings for each reference image to maintain identity and order in multi-reference settings. (3) \textbf{Automatic Data Curation:} We implement a pipeline to curate training data with diverse reference combinations, enabling high-quality, tuning-free multi-concept video generation.