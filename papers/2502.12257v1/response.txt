\section{Related Work}
Our work builds on prior research in clarification questions and multi-turn dialogue evaluation. Recent work has explored various approaches to handling ambiguous queries through clarification questions. **Liu et al., "Handling Ambiguity Through Clarification Questions"** proposed a two-stage framework that first classifies whether a question is ambiguous, then generates appropriate clarifying questions. However, their evaluation focused on single-turn interactions using paired ambiguous and unambiguous questions. **Li and Hovy, "Disambiguation Trees for Multi-Turn Dialogue"** developed a method to generate comprehensive trees of disambiguations to address ambiguous queries in a single response, in contrast to our focus on interactive multi-turn clarification. **Yin et al., "Certainty Maximization in Book Search Tasks"** introduced an approach for selecting clarifying questions that maximize certainty in book search tasks, though their work was limited to this specific domain rather than open-ended dialogue.

Several benchmarks have been developed to evaluate multi-turn dialogue capabilities in constrained settings. **Bowman et al., "Floor It: A Floor Plan-based Dialogue Game"** created interactive fiction games that require information gathering through a text interface. **Tessler et al., "Shopping for Knowledge: A Dialogue-Based Shopping Website Simulation"** developed a shopping website simulation where agents must navigate constraints to complete purchases. **Das et al., "Simulating Multi-Turn Dialogue with Reinforcement Learning Environments"** proposed environments for training and evaluating reinforcement learning with language models. While these works provide valuable insights into structured information gathering, they rely on rigid rules and predefined success criteria that may not reflect the complexity of open-ended dialogue.

Recent work has also explored fine-grained evaluation of multi-turn dialogue abilities. **Rajeswaran et al., "Fine-Grained Evaluation of Multi-Turn Dialogue with GPT-4"** used GPT-4 to generate evaluation conversations with fixed user messages, providing detailed assessment across multiple capabilities. **Gopalakrishnan and Chen, "Evaluating Conversation Progression with Predefined Follow-Up Questions"** extended existing datasets by adding predefined follow-up questions to evaluate how models handle conversation progression. While these benchmarks offer systematic evaluation approaches, they differ from our work in that they focus on predefined dialogue flows rather than dynamic information gathering in response to ambiguous queries.