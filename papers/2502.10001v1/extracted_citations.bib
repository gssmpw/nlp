@misc{BERT,
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  month = {10},
  title = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  url = {https://arxiv.org/abs/1810.04805},
  year = {2018},
  organization = {arXiv.org}
}

@article{ConvBERT,
  author = {Jiang, Zihang and Yu, Weihao and Zhou, Daohong and Chen, Yunpeng and Feng, Jiashi and Yan, Shuicheng},
  month = {08},
  publisher = {Cornell University},
  title = {ConvBERT: Improving BERT with Span-based Dynamic Convolution},
  doi = {10.48550/arxiv.2008.02496},
  year = {2020},
  journal = {arXiv (Cornell University)}
}

@misc{ELECTRA,
  author = {Clark, Kevin and Luong, Minh-Thang and Le, Quoc V and Manning, Christopher D},
  title = {ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators},
  url = {http://arxiv.org/abs/2003.10555},
  urldate = {2024-09-06},
  year = {2020},
  organization = {arXiv.org}
}

@article{LoRA,
  author = {Hu, Edward J. and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  month = {10},
  title = {LoRA: Low-Rank Adaptation of Large Language Models},
  url = {https://arxiv.org/abs/2106.09685},
  year = {2021},
  journal = {arXiv:2106.09685 [cs]}
}

@misc{MAMBA,
  author = {Gu, Albert and Dao, Tri},
  month = {12},
  title = {Mamba: Linear-Time Sequence Modeling with Selective State Spaces},
  doi = {10.48550/arXiv.2312.00752},
  url = {https://arxiv.org/abs/2312.00752},
  year = {2023},
  organization = {arXiv.org}
}

@misc{MobileBERT,
  author = {Sun, Zhiqing and Yu, Hongkun and Song, Xiaodan and Liu, Renjie and Yang, Yiming and Zhou, Denny},
  title = {MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices},
  url = {http://arxiv.org/abs/2004.02984},
  urldate = {2024-09-06},
  year = {2020},
  organization = {arXiv.org}
}

@inproceedings{NanoBERT,
  title={NanoBERT: An Extremely Compact Language Model},
  author={Maity, Krishanu and Chaulwar, Amit Tulsidas and Vala, Vanraj and Guntur, Ravi Sankar},
  booktitle={Proceedings of the 7th Joint International Conference on Data Science \& Management of Data (11th ACM IKDD CODS and 29th COMAD)},
  pages={342--349},
  year={2024}
}

@misc{data_augmentation,
  author = {Liu, Xiao and Zheng, Yanan and Du, Zhengxiao and Ding, Ming and Qian, Yujie and Yang, Zhilin and Tang, Jie},
  title = {GPT Understands, Too},
  url = {http://arxiv.org/abs/2103.10385},
  urldate = {2024-09-06},
  year = {2021},
  organization = {arXiv.org}
}

@article{i-BERT,
  author = {Kim, Sehoon and Amir Gholaminejad and Yao, Zhewei and Mahoney, Michael and Eecs Kurt Keutzer},
  month = {01},
  publisher = {Cornell University},
  title = {I-BERT: Integer-only BERT Quantization},
  doi = {10.48550/arxiv.2101.01321},
  year = {2021},
  journal = {arXiv (Cornell University)}
}

@article{knowledge_distillation,
  title={Distilling the Knowledge in a Neural Network},
  author={Hinton, Geoffrey},
  journal={arXiv preprint arXiv:1503.02531},
  year={2015}
}

@incollection{quantization,
  title={A survey of quantization methods for efficient neural network inference},
  author={Gholami, Amir and Kim, Sehoon and Dong, Zhen and Yao, Zhewei and Mahoney, Michael W and Keutzer, Kurt},
  booktitle={Low-Power Computer Vision},
  pages={291--326},
  year={2022},
  publisher={Chapman and Hall/CRC}
}

@article{synthetic_data,
  title={Machine learning for synthetic data generation: a review},
  author={Lu, Yingzhou and Shen, Minjie and Wang, Huazheng and Wang, Xiao and van Rechem, Capucine and Wei, Wenqi},
  journal={arXiv preprint arXiv:2302.04062},
  year={2023}
}

@article{tinyBERT,
  title={Well-read students learn better: On the importance of pre-training compact models},
  author={Turc, Iulia and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1908.08962},
  year={2019}
}

