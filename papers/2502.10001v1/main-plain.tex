%\documentclass[preprint,3p,times,twocolumn]{elsarticle}

%% Use the option review to obtain double line spacing
%% \documentclass[authoryear,preprint,review,12pt]{elsarticle}

%% Use the options 1p,twocolumn; 3p; 3p,twocolumn; 5p; or 5p,twocolumn
%% for a journal layout:
%% \documentclass[final,1p,times]{elsarticle}
%% \documentclass[final,1p,times,twocolumn]{elsarticle}
%% \documentclass[final,3p,times]{elsarticle}
%\documentclass[final,3p,times,twocolumn]{elsarticle}
%% \documentclass[final,5p,times]{elsarticle}
%% \documentclass[final,5p,times,twocolumn]{elsarticle}

\documentclass{article}
\usepackage[letterpaper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}


%%%%%%%%%%
%packages%
%%%%%%%%%%
%\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
%\usepackage{algorithmic}
\usepackage{graphicx}
%\usepackage{textcomp}
%\usepackage{xcolor}
\usepackage{comment}
%\usepackage{natbib}
\usepackage{enumitem} % For customizing enumerate

%added by me
\usepackage{hyperref}
%\usepackage{multirow}
%\usepackage{soul}
% \usepackage[acronym]{glossaries}
%\makeglossaries
% \loadglsentries{glossary}
\usepackage{float}
\usepackage{authblk} % Include this package for \affil to work

\usepackage[dvipsnames]{xcolor}
%%%%%%%%%
%defines%
%%%%%%%%%

%% The lineno packages adds line numbers. Start line numbering with
%% \begin{linenumbers}, end it with \end{linenumbers}. Or switch it on
%% for the whole article with \linenumbers.
\usepackage{lineno}
%\linenumbers

%\journal{Neural Networks}

%\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
%    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

%\newcommand{\brav}[1]{{\color{Violet} #1}}
%\usepackage{soul}
%\newcommand{\fab}[1]{{\color{cyan} #1}}
%\newcommand{\haz}[1]{{\color{purple} #1}}
%\newcommand{\mas}[1]{{\color{teal} #1}}



%\title{EmbBERT: \fab{\st{exploring LLMs for} moving BERT and MAMBA LLMs to} micro devices\\

%\title{EmbBERT: Downscaling and Designing Language Models for Tiny Devices \fab{(or Embedded Systems?)}
%\title{EmbBERT: scaling down without compromises}
%\title{EmbBERT: Scaling Down Language Models Without Accuracy Degradation}
%\title{EmbBERT: Language Models Reimagined for Embedded Systems}
\title{EmbBERT-Q: Breaking Memory Barriers in Embedded NLP}
% \title{Designing Tiny Language Models: The EmbBERT Approach}
\date{}

% {\footnotesize \textsuperscript{*}Note: Sub-titles are not captured in Xplore and
% should not be used}
% %\thanks{Identify applicable funding agency here. If none, delete this.}



%%%%%%%%%%
%document%
%%%%%%%%%%

\begin{document}

%\begin{frontmatter}

\author{Riccardo Bravin}
\author{Massimo Pavan} 
\author{Hazem Hesham Yousef Shalby} 
\author{Fabrizio Pittorino\thanks{\texttt{fabrizio.pittorino@polimi.it}}} 
\author{Manuel Roveri} 
\affil{
    Department of Electronics, Information and Bioengineering, \\
    Politecnico di Milano, \\
    Via Ponzio 34/5, Milano, 20133, Italy
}

\maketitle
%-----------------------------------------------------------------------------
% Abstract
%-----------------------------------------------------------------------------
\begin{abstract}
% Large language models (LLMs) have achieved remarkable success by scaling up model and dataset sizes. 
% Yet, their effectiveness in severely technologically-constrained tiny devices such as wearables - where both memory and compute resources are scarce - remains largely underexplored. 
% In this work, we present the first extensive evaluation of extreme architectural compression for LLMs, demonstrating their competitive advantage even on devices with stringent memory limits. Our goal is to design and deploy natural language understanding models on microcontrollers for tasks like voice command classification, while restricting the total model memory footprint - including parameters and activations - to under 2 MB.
% Unlike previous efforts that largely overlook such tight resource budgets, we investigate two State-of-the-Art (SotA) architectures such as BERT and MAMBA, scaled down to meet these constraints, as well as designing a new SotA Embedded Language Model, that we call EmbBERT, based on purpose-built techniques to reduce the number of parameters and activations. 
% Our experiments reveal that EmbBERT consistently outperform baselines such as BERT and MAMBA downsized to the same level, and even achieves better or comparable performances with respect to the previous smaller SotA models by using a 10$\times$ smaller memory budget, underscoring the importance of architectural design when operating under extreme memory constraints. 
% Further compressing the model with an hardware-friendly 8-bit weight and activation quantization protocol, we are able to reduce the memory footprint of EmbBERT to 781 kB with minimal to no accuracy degradation. 
% The purpose of our work is to serve as a milestone and a benchmark for future research on the evaluation of Language Models on Tiny Devices and Embedded Systems.
Large Language Models (LLMs) have revolutionized natural language processing, setting new standards across a wide range of applications. However, their relevant memory and computational demands make them impractical for deployment on technologically-constrained tiny devices such as wearable devices and Internet-of-Things units. To address this limitation, we introduce EmbBERT-Q, a novel tiny language model specifically designed for tiny devices with stringent memory constraints.
EmbBERT-Q achieves state-of-the-art (SotA) accuracy in Natural Language Processing tasks in this scenario, with a total memory footprint (weights and activations) of just 781~kB, representing a $25\times$ reduction in size with respect to SotA models. By combining architectural innovations with hardware-compatible 8-bit quantization, EmbBERT-Q consistently outperforms several baseline models scaled down to a 2~MB memory budget (i.e., the maximum memory typically available in tiny devices), including heavily compressed versions of BERT and MAMBA.
% We demonstrate the ability of EmbBERT-Q to deliver competitive accuracy through a comprehensive comparative study, while redefining the balance between size, efficiency, and performance for embedded~NLP, setting a SotA benchmark for future studies in this domain. 
%By selecting a benchmark dataset, TinyNLP, specifically curated to evaluate Tiny Language Models in NLP tasks and real-world scenarios, we conduct a rigorous experimental comparison of EmbBERT-Q against several other proposed baselines. 
Extensive experimental evaluations on both a selected benchmark dataset, TinyNLP, specifically curated to evaluate Tiny Language Models in NLP tasks and real-world scenarios, and the GLUE benchmark, demonstrate EmbBERT-Q ability to deliver competitive accuracy with respect to existing approaches, achieving an unmatched balance between memory and performance. %, showcasing its effectiveness in real-world NLP scenarios. 
To ensure the complete and immediate reproducibility of all our results, we release all code, scripts, and model checkpoints at https://github.com/RiccardoBravin/tiny-LLM.
\end{abstract}

%%Graphical abstract
%\begin{graphicalabstract}
%\includegraphics[width=2.1\linewidth]{EmbBERT.pdf}
%\end{graphicalabstract}

%%Research highlights
%\begin{highlights}
%\item Design of the optimal NLP architecture under 2~MB
%\item Ultra-compact 781kB NLP model for tiny devices
%\item Detailed memory and computation analysis of LLM layers
%\item Language Model robustness to 8-bit quantization for hardware deployment
%\end{highlights}

%% Keywords
%\begin{keyword}
%% keywords here, in the form: keyword \sep keyword
%Large language models \sep Model compression \sep Efficient deep learning \sep Generative AI \sep Hardware acceleration \sep Tiny machine learning
%\end{keyword}

%\end{frontmatter}

%-----------------------------------------------------------------------------
% Introduction
%-----------------------------------------------------------------------------

\section{Introduction}
\label{sec:introduction}


The proliferation of Internet-of-Things (IoT) systems and the availability of off-the-shelf, energy-efficient pervasive technologies have sparked a growing demand for intelligent on-device computation~\cite{iot}. Tiny devices such as microcontrollers and wearables are now key stringent technological solutions in relevant application scenarios ranging from smart homes to industrial automation~\cite{wearables}. Despite significant advances in their memory and processing capabilities, these tiny devices still operate under stringent constraints, making very challening the design and development of machine (ML) and deep learning (DL) models meant to be executed on these devices~\cite{MCUNet,Eff_NN_for_embedded}. This need pushed the research field of Tiny Machine Learning (TinyML), which enables efficient ML/DL inference on devices characterized by limited memory (e.g., less than 2~MB memory), low processing power (e.g., less than 100~MHz CPUs), and strict energy budgets~\cite{tinyML}, hence allowing tiny devices to locally process data, reducing latency, improving the real-time responsiveness of smart applications, and enhancing privacy by keeping sensitive data on-device.

While TinyML has made strides in areas like keyword spotting~\cite{on_device_kw_spotting,customizable_kw_spotting}, image classification~\cite{mobilenet_v2,micro_net_img_rec}, and object detection~\cite{EfficientDet_obj_det}, deploying \emph{natural language processing (NLP)} models on tiny devices remains a significant challenge. Modern decoder-based Large Language Models (LLMs) relying on the attention mechanism, such as BERT~\cite{BERT}, XLNet~\cite{XLNet}, DistilBERT~\cite{DistilBERT}, SpanBERT~\cite{SpanBERT}, ALBERT~\cite{ALBERT}, RoBERTa~\cite{RoBERTa} and State-Space-Models (SSMs) such as MAMBA~\cite{MAMBA}, rely on millions or even billions of parameters and extensive memory resources to achieve SotA accuracy across a wide range of NLP tasks. Even scaled-down variants like MobileBERT (25.3M parameters)~\cite{MobileBERT} are orders of magnitude too large for deployment on microcontrollers with less than 2~MB memory budgets.

To fill this gap, we introduce \emph{EmbBERT-Q}, a \emph{Tiny Language Model (TLM)}, specifically designed to operate on tiny devices, and under the stringent 2~MB memory budget. 
% Unlike traditional LLMs, which are general-purpose and pre-trained on massive datasets, TLMs are tailored for specific tasks, prioritizing memory and computational efficiency over generality. However, we demonstrate that even in this context and at such small memory budgets the effect of pre-training is beneficial for TLM architectures, a highly non-trivial result confirming the effectiveness of the attention mechanism at such scales.
%At this size, the theory of information imposes fundamental limits on the capacity of these models as it becomes infeasible for TLMs to store vast amounts of information, a shift in focus toward compact architectures that prioritize essential language understanding and reasoning capabilities is necessary.
In particular \emph{EmbBERT-Q}, comprises a novel TLM architecture optimized for microcontroller units and other resource-constrained devices. Using techniques such as hardware-compatible 8-bit quantization~\cite{transformers_compression}, EmbBERT-Q achieves SotA performance with only 781 kB of memory, a $25\times$ reduction in memory with respect to the SotA models characterized by the smallest memory demands, such as BERT-Tiny~\cite{tinyBERT}. Even when compared with other models specifically adapted to work within the 2 MB constraints, EmbBERT-Q resulted to be both the most effective and the most efficient model. %Our novel design makes use of optimized embedding and attention mechanisms that enable EmbBERT-Q to outperform downsized versions of the most popular LLMs and deliver competitive results even under a strict 2~MB memory budget.

Our contributions include:
\begin{enumerate}
    \item \emph{EmbBERT-Q}: We propose a new TLM model specifically designed for tiny devices, combining efficiency and effectiveness.
    \item \emph{Memory and Computational Analysis}: We analytically evaluate the memory usage and computational complexity of EmbBERT-Q and its components, providing a useful tool to evaluate the weights and activations memory trade-offs required to operate within tiny device constraints.
    \item \emph{Custom Benchmark}: We design a specialized benchmark tailored to assess the NLP capabilities of TLMs, enabling consistent and fair evaluations in resource-constrained environments.
    % \item \emph{Extensive Model Evaluations and Ablation Study}: We perform extensive testing in a variety of models and present a detailed ablation study to quantify the impact of each design choice on EmbBERT-Q performance and efficiency.
    % \item We demonstrate comparable performance on our specialized benchmark and on GLUE with a $25\times$ memory reduction compared to the previous SotA models, while conducting an extensive experimental evaluation with over 10 models and 17 different datasets to validate the optimality of our approach under the 2~MB memory budgets.
\end{enumerate}

% The remainder of the paper is organized as follows:
% \begin{itemize}
%     \item Section~\ref{sec:relatedwork}: Reviews recent work on model compression and training for resource-constrained platforms.
%     \item Section~\ref{sec:EmbBERT}: Provides an in-depth description of our proposed, state-of-the-art EmbBERT-Q model for NLP in Tiny Devices, while presenting detailed calculations of LLM layers memory requirements.
%     \item Section~\ref{sec:setup}: Outlines the experimental setting, training procedure and datasets used to obtain experimental results.
%     \item Section~\ref{sec:results}: Presents our evaluation on the TinyNLP benchmark suite and GLUE of downscaled versions of BERT and MAMBA, showing the significant comparative performance gains obtained by our proposed EmbBERT-Q model.
%     \item Section~\ref{sec:ablation}: Carefully evaluates the impact of EmbBERT-Q architectural modules on its performances.
%     \item Section~\ref{sec:conclusions}: Concludes with discussions on experimental results, future directions and broader implications of~TLMs.
% \end{itemize}

The remainder of this paper is organized as follows. Section~\ref{sec:relatedwork} reviews recent work on model compression and training techniques tailored for resource-constrained platforms, setting the stage for our contributions. Section~\ref{sec:EmbBERT} provides a detailed description of EmbBERT-Q, our proposed model for NLP in tiny devices, and includes precise calculations of memory requirements for its layers. In Section~\ref{sec:setup}, we outline the experimental setup, including the training procedures and datasets used to validate our approach. Section~\ref{sec:results} presents a comprehensive evaluation of our model on the TinyNLP benchmark suite and GLUE datasets, comparing downscaled versions of BERT and MAMBA, and highlighting the significant performance improvements achieved by EmbBERT-Q. Section~\ref{sec:ablation} delves into an ablation study to assess the individual contributions of the architectural modules within EmbBERT-Q to its overall performance. Finally, Section~\ref{sec:conclusions} concludes the paper by discussing the experimental findings, exploring future research directions, and considering the broader implications of deploying TLMs in tiny devices.

% By redefining the trade-offs between size, efficiency, and performance, our work establishes a foundational benchmark for future research on TLMs %The following sections explore the theoretical, architectural, and practical aspects of EmbBERT, 
% and provides a comprehensive framework for advancing NLP on tiny devices.


%-----------------------------------------------------------------------------
% Related work
%-----------------------------------------------------------------------------


\section{Related Work}
\label{sec:relatedwork}

% This section reviews recent advancements in downscaling transformer-based architectures and adapting them for resource-constrained environments, with a focus on memory targets comparable to our sub-2MB target for microcontroller-based platforms. While existing techniques like architectural simplifications, quantization, and knowledge distillation have made significant advancements, our work pushes the boundaries of model compression to successfully tackle much stricter constraints with respect to current literature. To the best of our knowldge no particular advancements have ever been tried for MAMBA-based arhitectures in the MCU setting.

%This work extends existing methods like simplifications, quantization, and distillation which we will now review to achieve unprecedented compression for sub-2MB BERT-based architectures on microcontroller platforms, a largely unexplored area. 

This section introduces the SotA Small Language Models and the main techniques to reduce the memory and computational demand of LLMs.

\paragraph{Small Language Models} 
Several models, including BERT-Tiny~\cite{tinyBERT}, NanoBERT~\cite{NanoBERT}, MobileBERT~\cite{MobileBERT}, ConvBERT~\cite{ConvBERT}, and I-BERT~\cite{i-BERT} are considered at the SotA for Small Language Models each leveraging different techniques to reach their desired level of compression. 


\textit{BERT-Tiny}~\cite{tinyBERT} employs a shallow architecture with fewer layers and reduced hidden dimensions with respect to conventional BERT architectures, trading accuracy for a smaller size and lower computational costs (4.4M parameters corresponding to a 20 MB memory footprint). It employs a knowledge distillation approach from a larger teacher model to a compact student model, achieving competitive accuracy through task-specific and data-augmented distillation. 

\textit{NanoBERT}~\cite{NanoBERT} represents, in current literature, the smallest model built for tiny devices. It manages to achieve it's astounding results by introducing a \emph{Nano Embedder}, a clever mechanism that aims to reduce the memory footprint of the standard embedder. Still, with weights in the range 700-800k, experimental results confined to a few datasets and missing code and checkpoints it falls behind other more prominent publications. \textit{MobileBERT}~\cite{MobileBERT}, on the other hand, employs bottleneck structures and a teacher-student training to compress BERT, retaining high accuracy at the expense of a higher number of weights~(25.3M), making it better suited for edge devices than actual tiny devices.

\textit{ConvBERT}~\cite{ConvBERT}, with it's small size of 14M parameters, introduces convolutional operations in self-attention to reduce computational complexity and efficently capture local dependencies. \textit{I-BERT}~\cite{i-BERT} employs integer-only quantization, guaranteeing that all computations, including softmax and matrix multiplications, rely on integer arithmetic. This allows significantly improve memory and energy efficiency without compromising accuracy, despite its larger size of 355M parameters.

\paragraph{Advanced Training and Pre-Training Techniques}
% Innovations in training schemes have introduced methods beyond traditional masked language modeling (MLM)~\cite{BERT}. Techniques like \textit{ELECTRA} generative-discriminative training~\cite{ELECTRA} and \textit{LoRA} low-rank updates~\cite{LoRA} optimize fine-tuning while reducing trainable parameters. Data augmentation and synthetic text generation~\cite{data_augmentation, synthetic_data} also improve small-model performance by expanding training data and enhancing knowledge transfer.
Training schemes beyond traditional masked language modeling (MLM)~\cite{BERT} play a crucial role in enhancing the efficiency and performance of smaller models. Techniques like \textit{ELECTRA} generative-discriminative training~\cite{ELECTRA} and \textit{LoRA} low-rank updates~\cite{LoRA} enable fine-tuning with reduced trainable parameters, optimizing computational demands while retaining accuracy. Additionally, data augmentation and synthetic text generation~\cite{data_augmentation, synthetic_data} expand training datasets, enhancing knowledge transfer and improving small-model generalization.

These methods are particularly relevant for small language models, as they provide mechanisms to further reduce resource requirements while maintaining competitive performance. For instance, generative-discriminative training, such as in \textit{ELECTRA}, efficiently trains models to distinguish between real and synthetically generated tokens, leading to faster convergence. Similarly, \textit{LoRA} low-rank parameter updates focus computational resources on task-specific fine-tuning rather than full model retraining, an advantage for tiny devices.


\paragraph{Quantization and Knowledge Distillation}
Quantization, a cornerstone of model compression, converts weights and activations to lower-precision formats, such as 8-bit integers, drastically reducing memory usage and improving inference efficiency~\cite{quantization}. Integer arithmetic, as demonstrated by I-BERT~\cite{i-BERT}, aligns well with embedded hardware capabilities. Knowledge distillation~\cite{knowledge_distillation}, employed in models like Bert-Tiny and MobileBERT, further reduces model size by training smaller networks to replicate larger teacher models' behavior.

\paragraph{Exploring Alternative Architectures} 
Beyond transformers, alternative architectures like recurrent neural networks (RNNs) with state-space models (SSMs) represent a viable option for tiny devices. For example, \textit{MAMBA}~\cite{MAMBA}, with 140M parameters, leverages RNNs to mitigate the attention quadratic complexity, offering efficient text generation solutions that could be even tailored for environments with limited parallelism capabilities due to it's inherent recursive structure.

\vspace{5mm}

% Despite these advancements, most existing models fall short of meeting less than 2~MB memory constraints required for tiny devices. The smallest model identified in the literature is \textit{NanoBERT}, which achieves a parameter size of approximately 2~MB excluding the memory required for activations, and significantly underperforms compared to our proposed model. In contrast, all other models, including \textit{ConvBERT} (14M parameters) and \textit{I-BERT} (355M parameters), largely exceed tenths of MB of total memory usage. Our work 
% %introduces tailored architectural designs and applies memory optimized techniques, 
% pushes the limits of transformer down-scaling  for embedded NLP tasks, in order to meet the most stringent resource requirements.
Summing up, despite significant advancements across all these dimensions - innovative small-model designs, advanced training strategies, effective quantization techniques, and explorations of alternative architectures - most existing models fall short of meeting the stringent memory and computational constraints imposed by tiny devices. While \textit{NanoBERT} achieves a parameter size of approximately 2~MB (excluding activation memory), its experimental limitations and performance deficits highlight the challenges of extreme compression. Other models like \textit{ConvBERT} (14M parameters) and \textit{I-BERT} (355M parameters) demonstrate substantial improvements in their respective niches but remain unsuitable for the smallest hardware due to their larger memory footprints.

Crucially, these works underscore the importance of a multifaceted approach to compression. Small model designs benefit from integrating knowledge distillation and quantization for immediate memory reduction, while advanced training techniques like \textit{ELECTRA} or \textit{LoRA} further refine performance by leveraging enhanced pre-training and fine-tuning efficiency. Simultaneously, alternative architectures like \textit{MAMBA} suggest promising directions for bypassing inherent transformer limitations, particularly for devices with limited computational parallelism.

Our work builds on these foundational insights to propose tailored architectural innovations and memory-optimized techniques, advancing the SotA for embedded NLP applications. By synthesizing these diverse strategies, we aim to deliver models that not only push the limits of transformer downscaling but also satisfy the most stringent resource requirements, bridging the gap between theoretical advancements and real-world applicability.

%-----------------------------------------------------------------------------
% EmbBERT
%-----------------------------------------------------------------------------

\section{EmbBERT-Q: A Novel Language Model for Tiny Devices}
\label{sec:EmbBERT}

% To match the tight constraints of Tiny Devices, the proposed EmbBERT-Q model exploits many optimization techniques and design principles for the effective downscaling of the models. 
% The model was designed starting from a downscaled version of the standard BERT model, and its components were modified and redesigned to improve its memory/accuracy tradeoff. Finally, the activations and the weights of the model were quantized with an efficient quantization scheme.

% This section introduces EmbBERT-Q, a novel Tiny Language Model designed to balance memory requirements and performance on NLP tasks. Taking as a baseline the decoder-based BERT model, we redesign and optimize key architectural components to improve the overall memory-performance tradeoff. These architectural enhancements are combined with a hardware-compatible 8-bit quantization scheme, which minimizes memory usage while preserving accuracy.
% EmbBERT-Q achieves SotA levels of memory footprint and accuracy, demonstrating its suitability for deployment on tiny devices without compromising on the capabilities required for natural language understanding tasks in the TinyML scenario.

EmbBERT-Q is a compact and efficient language model designed specifically for deployment in memory-constrained environments. Unlike conventional approaches that compromise either performance or memory footprint, EmbBERT-Q strikes a fine balance by leveraging tailored architectural optimizations and an 8-bit quantization scheme. These innovations make it particularly suited for real-world natural language processing tasks in TinyML scenarios, where resource efficiency is paramount.

\subsection{The EmbBERT-Q Architecture}

% Starting from the aggressively downscaled version of BERT, the EmbBERT architecture is obtained, through a series of modifications and optimizations of its components. In particular, these modifications lower the memory dedicated to each component considering the same values for the hyperparameters, permitting us to obtain better tradeoffs.

%new model specifically designed for memory and compute constrained scenarios. 
%Our research integrates insights from multiple recent studies, combined with original contributions, to yield EmbBERT - a novel language model engineered for minimal resource consumption.
%Inspired by the original BERT model~\cite{BERT}, 
% EmbBERT incorporates the efficient embedding strategy from NanoBERT~\cite{NanoBERT} on a BERT-like~\cite{BERT} architecture based on the attention module. We replace the standard attention mechanism with an efficient alternative \cite{efficient_attention} and incorporate an additional module - drawing on the MAMBA approach~\cite{MAMBA} - that exploits both convolutional layers and weighted aggregations. As we will detail in this section, this combination yields a lightweight architecture that maximizes available memory resources.

% EmbBERT-Q introduces a novel approach to lightweight attention-based models by redesigning encoder-based BERT-like Language Models with highly efficient architectural components tailored to memory-constrained scenarios. 
% Instead of the standard BERT-encoder, it employs an \textit{Efficient Encoder} block, incorporating an efficient attention module~\cite{efficient_attention} and a \textbf{parallel path with convolutional layers}, and combining their results through learnable weighted aggregation techniques.
%that optimally balances computational efficiency and performance. The architecture 
%inspired by MAMBA~\cite{MAMBA} and ConvBERT~\cite{ConvBERT}, 
%adapted from Differential Transformer~\cite{differential_transformers}. 
% We show that this architectural design minimizes memory usage while enabling EmbBERT-Q to surpass comparable lightweight models at the same (or higher) memory budget.
% An 8-bit weight quantization scheme further enhances memory efficiency. Leveraging a parameter-efficient fine-tuning process~\cite{qlora}, combined with half-precision activation quantization, we allow EmbBERT-Q to maintain high accuracy without significant performance degradation, showing its architecture robustness to quantization.
EmbBERT-Q achieves its efficiency through a redesign of traditional encoder-based BERT models, prioritizing lightweight attention mechanisms and memory-aware component selection. This architecture focuses on minimizing memory usage without sacrificing accuracy, surpassing other lightweight models within similar constraints. The adoption of 8-bit weight quantization, along with half-precision activation quantization and parameter-efficient fine-tuning techniques~\cite{qlora}, ensures robustness to quantization and maintains state-of-the-art performance despite the limited hardware capabilities.

As illustrated in Fig.~\ref{fig:EmbBert}, EmbBERT-Q comprises two main modules (delineated by dashed gray lines): a \textit{Nano Embedder Block}, responsible for generating compact yet expressive token representations called $\delta$s; and a sequence of $N$ \textit{Efficient Encoder blocks} integrating efficient attention, convolutional layers, and weighted difference aggregation to process embeddings with minimal memory overhead.
Additionally, an \textit{optional Output Block} can be appended to adapt the architecture for specific downstream regression, classification or generative tasks.

In the following sections, we explore the functionality and design of these core modules, highlighting the architectural innovations that make EmbBERT-Q a SotA solution for tiny devices.
\begin{figure*}[t]
    \centering
    \includegraphics[width=0.75\linewidth]{EmbBERT.pdf}
    \caption{Overview of the EmbBERT-Q architecture, a specialized Language Model designed for tiny devices and extreme resource constraints. The architecture features an Embedder block and multiple Efficient Encoder with an Efficient Attention block and a Convolutional Skip Connection block. Instead of a final sum to aggregate the two Efficient Encoder paths, EmbBERT-Q uses a weighted difference with learnable weights for enhanced performance.}
    \label{fig:EmbBert}
\end{figure*}

\subsubsection{The Embedder block}

The Embedder block in EmbBERT-Q is adapted from the Nano Embedder introduced in ~\cite{NanoBERT} (whose memory requirements are analyzed in~\ref{appendix:memory}). 

The Embedder block generates a $d$-dimensional representation~$\delta$ for each token in a vocabulary of size $v$ using its token embedder. These embeddings are positioned in a $d$-dimensional space, reflecting semantic similarity among tokens. To capture word order, the model includes a positional encoding component that maps each token position (up to a maximum window size $\ell$, usually referred to as sentence length) into a~$d$-dimensional vector, ensuring that token order is understood alongside their meaning. Unlike the original Nano Embedder~\cite{NanoBERT}, our Embedder block employs a learned positional embedding, built on the same principles as the token embedder. This approach not only enhances representation quality but also reduces memory requirements compared to using a fixed positional embedder. Additionally, segment embeddings, similar to those used in BERT, assign distinct $d$-dimensional vectors to differentiate between input segments (e.g., sentences), enabling the model to interpret relationships across boundaries. 

A key part of the Embedder block memory efficiency is in its use of dense layers after the embeddings. Specifically, the Embedder maps input tokens into a reduced space of dimension $r_d$, and the fully connected layer projects them into the desired $d$-dimensional space. This results in a $d \times \ell$ matrix $\Delta$ of embedded tokens $\delta$s for a sentence length~$\ell$.
This approach significantly reduces the Embedder size while maintaining sufficient representational power for classification tasks. By decreasing memory requirements for the embedding layer, the Embedder Block frees up space for other parts of the network, making it a crucial enabler for running language models on devices with limited memory.

\subsubsection{The Efficient Encoder}

The core of EmbBERT-Q is its sequence of $N$ \textit{Efficient Encoders} shown in the right gray dashed block of Fig.~\ref{fig:EmbBert}, designed to balance expressive power with minimal resource requirements. 
The Efficient Encoder takes as input the embedded token matrix~$\Delta$ which gets first processed with a normalization layer, and successively by two blocks, operating in parallel: an Efficient Attention block and a Convolutional Skip Connection block.

Let $\Delta' = \text{Norm}(\Delta)$ be the output of the Normalization layer of the Efficient Encoder, that becomes the input of both the Efficient Attention and the Convolutional Skip Connection blocks.
The Efficient Attention block operates through a sequence of matrix multiplications involving the Query ($Q$), Key ($K$), and Value ($V$) matrices.
The $Q$ matrix is defined as the output of a fully connected layer $W_1(\bullet)$ processing $\Delta'$, i.e., $ Q = W_1(\Delta')$, while both $K$ and $V$ are identical to $\Delta'$. Differently from other architectures using the attention mechanism, EmbBERT-Q uses always a number of attention heads $h = 1$ for reduced activations and because higher head counts has been found by ~\cite{exploring_transformer_sizes} to be less effective at this size.
The $Q$ matrix is multiplied by the $K$ matrix, the resulting product undergoes a SoftMax activation, and it is multiplied by $V$. Finally, the obtained matrix is processed with a fully connected layer $W_2(\bullet)$ obtaining the output~$\Delta_{EA}$ of the Efficient Attention block:
\[
\Delta_{EA} = W_2\left(\text{Softmax}\left(\frac{Q \times K}{\sqrt{d}}\right) \times V\right). 
\]

The parallel Convolutional Skip Connection block takes as input the same matrix $\Delta'$ provided to the Efficient Attention block, processing it through a 1D convolutional (Conv1D) layer, with convolutional kernel size~$k$, that expands the dimension of $\Delta'$ from $d$ to $d \cdot \alpha$, where $\alpha$ is a scaling factor. The result of this operation is followed by a SiLU activation function and a fully connected layer, that restores the original dimension $d$, obtaining the final output~$\Delta_{CS}$ of the block: 
\[
\Delta_{CS} = W_3(\text{SiLU}(\text{Conv1D}(\Delta'))).
\]

The outputs $\Delta_{EA}$ and $\Delta_{CS}$ get combined through a weighted difference with learned weights $\lambda_{EA}$ and $\lambda_{CS}$, to obtain the output~$\overline{\Delta}$ of the \emph{Efficient Encoder}:
\[
\overline{\Delta} = \Delta_{EA} \cdot \lambda_{EA} - \Delta_{CS} \cdot \lambda_{CS},
\]
that then serves as input to the next block, i.e. another \emph{Efficient Encoder} or the \emph{final output block}.

% While the core attention mechanism of Query (Q), Key (K), and value (V) used in EDSA is largely equivalent to the one used in Efficient Attention, EDSA completely redesigns the skip connection of the layer in three key ways:

% \begin{enumerate}
%     \item Anticipate the feed forward block into the skip connection
%     \item Optimize this feed forward block by employing a CONV-1D
%     \item use weights to balance between the main attention branch and the skip connection branch 
% \end{enumerate}


%an optimized version of the original self-attention mechanism that with 3 key modifications: 

% Four key innovations define the structure of these blocks:
% \begin{enumerate}
%     \item \emph{Efficient Attention Scheme}: We reduce activation cost by employing a single-head attention mechanism inspired by~\cite{exploring_transformer_sizes}, with an efficient attention layer~\cite{efficient_attention}, reflecting empirical findings showing limited benefits of multi-head attention in extremely small models. \cite{efficient_attention}, the standard multi-head operation:
%    \[
%        \text{head}_i = \mathrm{Attention}\left(QW_i^Q,\, KW_i^K,\, VW_i^V\right)
%    \]
%    is simplified by omitting two of the four weight matrices, resulting in:
%    \[
%        \text{head}_i = \mathrm{Attention}\left(QW_i^Q,\, K,\, V\right).
%    \]
%    As a result, the memory footprint of the attention layer is significantly reduced.%, improving feasibility for devices with tight memory budgets.
%     \item \emph{Unified Feed-Forward and Attention Layers}:  
%    Inspired by the MAMBA block~\cite{MAMBA}, we integrate the Feed-Forward and Attention mechanisms within a \emph{single} computational unit, eliminating the standalone feed-forward block typically found in transformer-style architectures. This architectural modification allows to improve performances without increasing activations, leading to more efficient memory usage and the possibility for simple parallelization in low core architectures.
%     \item \emph{Weighted Difference Skip Connection}:
%    We draw on the concept of Differential Transformers \cite{differential_transformers} to replace conventional additive skip connections with a \emph{learned weighted difference} approach. Specifically, two sets of learnable vectors \(\lambda_1,\lambda_2\) and \(\lambda_3,\lambda_4\) produce scaling factors:
%    \[
%        \lambda_{att} = \exp(\lambda_1 \cdot \lambda_2),\quad
%        \lambda_{skip} = \exp(\lambda_3 \cdot \lambda_4).
%    \]
%    These scalars then weight the difference between the attention output and the skip path:
%    \[
%        \mathrm{Encoder}(X) = \lambda_{att}\,\mathrm{EffAttention}(X) \;-\; \lambda_{skip}\,\mathrm{Skip}(X).
%    \]
%    %This approach improves gradient flow and representation quality while keeping overall parameter growth in check.
%     \brav{After training these two derived values can also be precoumputed effectively removing from the final structure the four $\lambda$ weight vectors.   
    
%     This approach shows improved resistance loss in accuracy due to quantization while still keeping about the same amount of parameters as its original Efficient Attention counterpart. } 
%     \item \emph{1D-Convolution for Local Interaction}:   
%    A lightweight 1D-Convolution layer further enhances local context modeling with minimal parameter overhead. Placed inside the skip connection and before the fully connected layer, the 1D-Convolution broadens the network’s capacity to capture short-range dependencies. %\brav{mitigating potential information bottlenecks and overparametrization in small-scale architectures.}
% \end{enumerate}

\begin{comment}
Encoder di BERT -> Norm -> Attention -> Norm -> Feed Forward
Ma noi abbiamo modificato cosi: 
1 usiamo ea
2 togliamo ff
3 aggiungiamo blocco (descrizione approfondita) -> NOME: Diff Conv Skip???
4 pesiamo gli out dei due blocchi    
\end{comment}

By integrating the Convolutional Skip Connection, the Efficient Attention, and the weighted difference, our Efficient Encoder block simultaneously optimizes architectural depth and memory efficiency. 
This is accomplished by eliminating separate normalization and feed-forward layers, while leveraging lightweight operations such as 1D convolutions and single-head attention. These novel design leads to a substantial reduction in both the weights and activation footprints of each components, enabling the exploration of diverse architectural configurations. This flexibility facilitates exploring trade-offs among embedding size, attention capacity, and memory usage, effectively balancing accuracy with deployability for EmbBERT-Q on devices constrained by memory limits of 1-2 MB.

%\subsubsection{EmbBERT-Q: Quantizing the model} 
\subsubsection{Quantizing the EmbBERT-Q model} 
\label{subs:quantization}

Finally, we combine architectural efficiency with an hardware-compatible quantization scheme. %, showing minimal or no performance degradation on our EmbBERT-Q model. 
% thanks to an optimal architecture-quantization cooperation.
%To ensure EmbBERT-Q achieves both architectural efficiency and practical deployability in constrained environments, it is crucial to complement its design innovations with strategies that optimize memory and computational requirements further. While the encoder block introduces mechanisms to minimize resource usage, the effectiveness of these components can be fully realized only when paired with a robust quantization strategy. This enables the model to operate within the stringent memory and compute limits of tiny devices without compromising its performance.
To this aim we employ an 8-bit block-wise quantization, showing that our EmbBERT-Q model has minimal or no performance degradation with respect to the 32-bit model, at a small fraction of the memory cost. This approach applies 8-bit floating-point representation to weights within a range of~$\pm 6$~\cite{bitsandbytes}. Weights outside this range are stored in 16-bit floating-point (FP16) format to ensure numerical stability for extreme values. Additionally, all activations, initially in 32-bit floating-point (FP32), are converted to FP16. %, significantly reducing memory and computational overhead during both training and inference.

%Quantization is applied to the models that performed best after the initial fine-tuning phase. 
Further parameter-efficient fine-tuning (PEFT) is performed for additional two epochs with the 8-bit AdamW optimizer and a fixed learning rate of $1\times 10^{-4}$. We modify only a small subset of parameters (approximately 8\% of the total weights), focusing primarily on task-specific layers to improve performance while retaining the benefits of reduced precision.
%The fine-tuning process for quantized models is optimized using . This optimizer is specifically designed to handle reduced-precision computations efficiently, ensuring stable updates to parameters under quantization.
The quantization and fine-tuning process ensure that EmbBERT-Q is both computationally efficient and capable of delivering high performance on the target tasks. 

\begin{table}[t]
    \caption{Formulas for calculating the weights and activation sizes of the blocks and components of EmbBERT-Q.}
    \begin{center}
        \begin{tabular}{|c | c c|}
        \hline
        \textbf{Layers} & \textbf{Weights} & \textbf{Activations} \\
        \hline \hline
        \textbf{Embedder block}           & \(r_d \cdot (v + \ell + 2 d ) + 2 d\)
                                        & \(r_d \cdot \ell + 2 d \cdot \ell\) \\ \hline \hline
        \textbf{Norm layer}          & \(2 d\)
                                        & \(2 d \cdot \ell\) \\
        \textbf{Eff. Attention block}    & \(2 d^2 \) 
                                        & \(2 d \cdot \ell + \ell^2 \)\\
        \textbf{Conv Skip block}    & \(d^2 \cdot \alpha + k \cdot d \cdot \alpha\)
                                        & \(d \cdot \ell (2 + \alpha)\)\\
                                        \hline
        \textbf{Efficient Encoder} & \(2d + 2 d^2 + d^2 \cdot \alpha + k \cdot d \alpha\)
                                        & \(\max ( 2 d \cdot \ell + \ell^2 ; \  d \cdot \ell (2 + \alpha )) \)\\
        %\textbf{MAMBA layer}            & \(i \cdot (3d + c + 2 + 3d_s + 2\rho)\)
        %                                & \( \ell \cdot (d + 3i + 2i \cdot d_s + d_s) + \max (i \cdot d_s; \ \ell \cdot d_s)  \)\\
        \hline
        \end{tabular}
    \end{center}
    \label{table:w_a_noMAMBA}
\end{table}

\subsection{Computing memory requirements of EmbBERT-Q}

% To adapt the EmbBERT-Q language model for deployment on resource-constrained devices, such as microcontrollers and other tiny platforms, it is essential to precisely understand and minimize its memory and computational requirements. The challenge lies in preserving the effectiveness of the model while adhering to stringent limits on available memory (e.g., under 2MB) and compute power. EmbBERT achieves this balance through thoughtful architectural design and systematic scaling of its components.

% EmbBERT-Q, like its predecessor BERT \cite{BERT}, operates by first embedding input tokens into dense vectors and then processing these representations through a series of encoder layers. Each encoder is composed of the efficient attention block, the convolutional block, and the  normalization layer, which iteratively refine the input representations. However, the high resource demands of these operations necessitate a careful analysis of memory and computational requirements, followed by informed adjustments to the model’s dimensions and parameters. This section provides a detailed breakdown of these requirements and describes the downscaling strategies employed to make EmbBERT-Q viable for memory-constrained environments.

An accurate computation of the total memory requirements for running EmbBERT-Q models is required in severely resource-constrained settings. The total memory, $M_{tot}$, is the sum of the memory needed for its weights and activations, and can be expressed as:
\begin{equation*}
    M_{tot} = (W_{emb} + N \cdot W_{enc}) \cdot P_w + \max(A_{emb},\,A_{enc}) \cdot P_a,
    \label{eq:embedder_params}
\end{equation*}
where $W_{emb}$ and $W_{enc}$ are the weights of the embedder and encoder, respectively, and $A_{emb}$ and $A_{enc}$ are their respective activations. $P_w$ and $P_a$ denote the precision (in bits) used to store weights and activations. 

The formulas used to compute the memory required for the weights and activations of the components of EmbBERT-Q are reported in Table~\ref{table:w_a_noMAMBA}. The next subsections are dedicated to an in-depth analysis of their memory and computational requirements.

%To determine the values of these terms, we must analyze each component of EmbBERT-Q, considering vocabulary size $v$, sentence length $\ell$, embedding dimension $d$, reduced embedding dimension $r_d$, forward expansion $\alpha$,  as the hyperparameters that regulate it's efficiency and efficacy.

\subsubsection{The Embedder block}

The Embedder block  of EmbBERT-Q is composed of 5 smaller components: token embedder of size \( v \times r_d \), positional embedder of size \( \ell \times r_d \), segment embedder of size \( 2 d \) and two fully connected of size $r_d \times d$ (where $v$ is the vocabulary size; $\ell$ the embedding size, and $r_d$ the reduced embedding size). Together, they result in a total parameter count:
\begin{equation*}
W_{emb} = r_d \cdot (v + \ell + 2 d) + 2 d,
\end{equation*}
achieving a reduction in parameter size of almost a $d / r_d$ factor with respect to a standard Embedder. However, the total activation size $A_{emb}$ required by the Embedder block slightly increases due to the added projection step, resulting in: 
\begin{equation*}
    A_{emb} = r_d \cdot \ell + 2 d \cdot \ell.
\end{equation*}

%which might appear to suggest a higher parameter count but the total can actually be lower than the one of the Standard Embeder if  $ r_d $ is sufficiently small relative to $ d $. 

During inference, the Embedder block performs as a first step $\ell + 2\ell \cdot r_d$ memory accesses for token and position encoding, followed by $4\ell \cdot r_d \cdot d$ memory accesses, $2\ell \cdot r_d \cdot d + \ell \cdot d$ summations, and $2\ell \cdot r_d \cdot d $ multiplications for the linear layers, and in case segment embeddings are needed another \(\ell + 2d + \ell \cdot d \) accesses and \(\ell \cdot d\) summations.

%This balance of parameter efficiency with slightly increased activation memory exemplifies the ability of the Nano Embedder to reduce overall model size while retaining embedding functionality, making it suitable for resource-constrained scenarios or to simply obtain more performance thanks to higher dictionary sizes.


\subsubsection{The Efficient Encoder}
We now proceed to analyze the memory usage and computational complexity of the three primary blocks that define the Efficient Encoder of EmbBERT-Q.

The normalization layer’s weights consist just of two vectors of size $d$ representing the averages and standard deviations required for normalization. This block also has a minimal activation size, requiring only $2d \cdot \ell$ values. The operations within this layer necessitate \(2d \cdot (\ell + 1)\) memory accesses, along with $d \cdot \ell$ summations and multiplications. 

From a memory perspective, the Efficient Attention block requires $2d^2$ weights and has an activation size of $2\ell \cdot d + \ell^2$. 
While its memory demands are relatively small, this efficiency comes at the cost of a high computational complexity. Due to the various matrix multiplications and the softmax operation, the block requires: \(4\ell \cdot d^2 + 4\ell^2 \cdot d + 2\ell^2\)  memory accesses, \(2\ell \cdot d^2 + 2\ell^2 \cdot d + \ell^2 + \ell \cdot d \) summations and  \(2\ell \cdot d^2 + 2\ell^2 \cdot d + 2\ell^2\) multiplications. 


The Convolutional Skip Connection block, compared to the feed-forward block that typically follows the standard attention layer and that is not present in EmbBERT-Q, features reduced memory requirements for the weights. This reduction arises from replacing the fully connected expansion layer with a Conv-1D expansion layer. Consequently, this block requires:
\(d^2 \cdot \alpha + k \cdot d \cdot \alpha\) weights and \(d \cdot \ell (2 + \alpha)\) values for activations. Its computational requirements are \(d \cdot \ell (k+4 + 2 d \cdot  \alpha) \) memory accesses, \(d \cdot \ell (k + 2 + d \cdot \alpha)\) sums and \(d \cdot \ell (k + 7 + d \cdot \alpha)\) multiplications.

For the aggregation step, the block requires only \(2 d \cdot \ell\) 
multiplications and memory accesses, with half as many summations. This step does not introduce any additional weights or activations.

% The attention process begins by passing the input through a fully connected layer of size \(d^2\) to generate the Query matrix. Differently from standard attention,  
% Key, and Value matrices are not processed and are just the same as the input. This step produces an activation size of \(2d \cdot \ell\) and involves \(2\ell \cdot d^2\) memory accesses and half as many summations and multiplications.

% The Query and Key matrices are then multiplied to compute a Weight matrix of size \(\ell \times \ell\). This step, with quadratic growth in the Weight matrix, significantly impacts the context length of attention-based models by increasing activation size to \(3 d \cdot \ell + \ell^2\). It also adds \(2\ell^2 \cdot d \) memory accesses and half as many summations and multiplications.

% The Weight matrix then undergoes a softmax operation, which contributes \(2\ell^2\) memory accesses and about \(\ell^2\) summations and \(3\ell^2\) multiplications, without increasing activation size. Next, the Weighted matrix multiplies the Value matrix, producing an output activation size of \(2 d \cdot \ell + \ell^2\). This step requires as many operations as the previous matrix multiplication.

% Finally, the output passes through a fully connected layer of size \(d^2\) which concludes the Efficient Attention part. While the activation size remains unchanged, this stage introduces \(2\ell \cdot d^2\) memory accesses and half as many summations and multiplications.

% Concerning our new introduction in the skip connection the input of the block has fist to go through a Conv-1D layer which requires just $c \cdot d \cdot \alpha$ weights and performing \(d \cdot l \cdot (k + 1)\) memory accesses together with \(d \cdot l \cdot k\) sums and multiplications. The output of this layer goes then through a SiLU activation function which only increases the operations count by \(d \cdot l\) accesses to memory and about \(d \cdot l\) sums and \(4 d \cdot \l \) multiplications. Then the final fully connected layer of size $d \times \alpha \times d$ generates the highest activations for this section of $d \cdot \ell (1 + \alpha)$ and requires further \(2 d^2 \cdot \alpha \cdot \ell \) memory acceses and half as many sums and multiplications.

% For the final step of this block we aggregate the resulting outputs of the attention and custom skip connection through the weighted difference which just requires \(2 d \cdot \ell\) multiplications and memory accesses and only half as many sums while not increasing overall activation sizes.

Summing up, the Efficient Encoder of EmbBERT-Q shown in Fig.~\ref{fig:EmbBert} requires:
\begin{equation*}
    W_{enc} = 2 d^2 + d^2 \cdot \alpha + k \cdot d \cdot \alpha  + 2d
\end{equation*}
weights and has a total activation size of:
\begin{equation*}
    A_{enc} = \max ( 2 d \cdot \ell + \ell^2 ; \  d \cdot \ell (2 + \alpha )).
\end{equation*}
Overall, the self-attention mechanism's structured weighting of sequence elements enables robust contextual representations, ensuring that tokens are influenced by all relevant inputs regardless of distance. Table~\ref{table:w_a_noMAMBA} summarizes the memory requirements of the components in our proposed EmbBERT-Q architecture.

To further enhance the efficiency of this architecture, the implemented quantization strategy reduces the size of $P_w$ from $4~Bytes$ (FP32) to approximately  $1~Byte$\footnote{For the majority (92\%) of the weights, $P_w$ is reduced to $1~Byte$, while weights selected through the fallback procedure (8\%) are stored with a precision of $2~Bytes$ (FP16).}. Similarly, the size of $P_a$ is reduced from $4~Bytes$ (FP32) to exactly $2~Bytes$ (FP16). 
These optimizations resulted in a $2.5\times$ 
reduction in memory consumption, significantly improving overall resource allocation.

\subsection{Selecting the Architectural Parameters for EmbBERT-Q}

The selection of optimal architectural parameters for the EmbBERT-Q model is a critical step in ensuring both performance and memory efficiency. Key parameters such as vocabulary size~$v$, sentence length~$\ell$, and embedding dimension~$d$ play a significant role in determining the model’s memory usage and overall effectiveness. These parameters heavily influence the embedder memory occupation and cannot be significantly reduced without adversely affecting model accuracy.

To adapt the model to the 2MB memory constraint, the first step involves identifying the smallest feasible values for~$v$ and~$\ell$ that maintain acceptable performance. This study evaluated~$v$ within the range of 2000 to 10000 to maintain the model expressiveness, while~$\ell$ was chosen between 256 and 512. The choice of~$\ell$ was informed by the average sentence lengths in the selected datasets and their interplay with vocabulary size.

The embedding dimension~$d$ and the reduced embedding dimension~$r_d$ were then tuned. While these dimensions can be scaled down more aggressively, reducing~$d$ below 64 results in significant performance degradation, as shown in~\cite{exploring_transformer_sizes}. For~$r_d$, values between 16 and 32 were found to yield optimal results, aligning with the findings of~\cite{NanoBERT}.

Finally, structural parameters such as the scaling factor~$\alpha$ and the number of layers~$N$ were fine-tuned to balance memory constraints with performance objectives, ensuring the model operated effectively within the given limitations.



%-----------------------------------------------------------------------------
% Experimental setup
%-----------------------------------------------------------------------------
\section{Experimental setup}
\label{sec:setup}


\begin{table}[htbp]
    \caption{\textbf{Architectural parameters} used for training model architectures. Columns represent: vocabulary size $v$, sentence length $\ell$, embedding dimension $d$, reduced embedding dimension $r_d$, forward expansion $\alpha$, SSM internal memory size~$d_s$, convolutional kernel size~$k$, number of heads~$h$ and number of layers.}
    \begin{center}
    \resizebox{\textwidth}{!}{
        \begin{tabular}{|c|c c c c c c c c c| c c c|}
            \hline
            \textbf{Hyperparameter} & $v$ & $\ell$ & $d$ & $r_d$ & $\alpha$ & $d_s$ & $k$ & $h$ & \textbf{layers} & \textbf{Weights} & \textbf{Activations} & \textbf{Total size}  \\
            \hline \hline
            \textbf{BERT(2MB)~\cite{BERT}}      & 2048  & 256 & 80  & /  & 2    & / & /  & 2 & 2 & 289 K     & 213 K     & 2,008 MB \\
            \textbf{MAMBA(2MB)~\cite{MAMBA}}    & 2048  & 256 & 64  & /  & 1    & 6 & 4  & / & 5 & 220 K     & 265 K     & 1,941 MB \\
            \textbf{Embedder}                   & 8192  & 256 & 320 & 32 & /    & / & /  & / & 1 & 293 K     & 164 K     & 1,826 MB \\
            \textbf{Embedder~+~conv}                 & 8192  & 256 & 320 & 32 & /    & / & 16 & / & 1 & 298 K     & 164 K     & 1,848 MB\\
            \textbf{EmbBERT-Q} (ours)           & 8192  & 256 & 128 & 16 & 1    & / & 32 & 1 & 4 & 357 K     & 131 K     & 781 KB\\
            \hline
            \textbf{BERT-Tiny (20MB)~\cite{tinyBERT}}   & 32768 & 512 & 128 & /  & 2    & / & /  & 2 & 2 & 4.4 M     & 786 K     & 20,746 MB \\
            \hline
        \end{tabular}
    }
    \end{center}
    \label{table:models_params_and_memory}
\end{table}


This section outlines our experimental protocol, focused on training EmbBERT-Q and several baseline models for comparison under a strict 2~MB memory budget. 
Our comprehensive and reproducible experimental campaign spans 6 different models trained across 17 datasets, providing insights into architecture design strategies that balance performance and memory efficiency.
We detail the comparison models, training protocols, datasets, and evaluation metrics, setting the stage for the performance analysis in Section~\ref{sec:results}.

%In this section, we describe our overarching experimental design, which involves training our model and several comparisons under a strict 2\,MB memory budget. %%We adopt two distinct training approaches and used a range of comparison architectures - including both established baselines and newly proposed models - to better understand their performance-memory trade-offs.
%By comparing these models, we aim to highlight the architecture design strategies that best balance performance and memory feasibility. Our experimental campaign is massive and reproducible, as it involved training 6 different models across a total of 17 datasets. 
%The next sections details the employed comparisons, our training protocols, datasets, and evaluation metrics, thereby contextualizing the performance results presented later in Section~\ref{sec:results}.

\subsection{Baseline Models and Comparisons}
\label{subs:models}

As a comparison to the proposed EmbBERT-Q model, we evaluated a diverse set of architectures, each constrained to a maximum memory footprint of 2~MB (including parameters and activations). Below, we summarize the key characteristics of these baseline models:
\begin{enumerate}
    \item BERT(2MB): 
   A scaled-down variant of the original BERT architecture \cite{BERT}, preserving the standard embedding layers and encoder blocks.
   \item MAMBA(2MB):  
   A scaled-down adaptation of the MAMBA model~\cite{MAMBA}, incorporating its native embedding mechanism and SSM-based recurrent blocks.
   %\item NanoBERT-2MB:
   %Implements NanoBERT \cite{NanoBERT} exactly, combining the Nano Embedder with standard BERT encoder blocks.
   %\item BERT-Efficient-2MB:  
   %Adopts the standard BERT embedder but replaces multi-head attention with the “Efficient Attention” approach proposed in~\cite{efficient_attention}.
   \item Embedder Only:  
   This baseline leverages the Nano Embedder~\cite{NanoBERT} without any mechanism for token interaction. While not a fully functional language model, it highlights the parameter budget allocated to the embedding layer and evaluates the embedder standalone capability.
   \item Embedder + Conv:  
   Extends the \emph{Embedder Only} configuration by adding a lightweight 1D convolutional layer. This enables local token interactions within a fixed-size context window, introducing minimal parameter overhead.
   %\item NanoBERT Efficient:  
   %Combines the Nano Embedder and Efficient Attention, aiming to reduce activation sizes enough to accommodate more parameters under the same 2 MB limit.
   %\item EmbBERT:  
   %Our proposed architecture, expanding upon NanoBERT Efficient by modifying attention and adding lightweight convolution-aggregation blocks (detailed in Section~\ref{sec:EmbBERT}).
\end{enumerate}

We further include BERT-Tiny~\cite{tinyBERT}, a minimal variant of BERT which is approximately $10\times$ larger than the 2~MB models evaluated in this study, as a SotA reference point. Despite its significantly larger size, it serves as a useful benchmark for performance comparison. Each model incorporates a task-specific output layer, adapted to the dataset and classification task. Given its small size and high customizability, this layer memory contribution has been excluded from the calculations of effective memory usage.
%As an external reference point, we also include BERT-Tiny - a well-known minimal variant of BERT developed by the original authors - which is approximately \emph{ten times} bigger than our 2 MB models. Although much larger, it serves as a useful performance benchmark. Each model encompasses a Task-Specific Output Layer. This layer varies depending on the model considered, on the specific dataset classes, and considering its small size and high customizability depending on the task at hand, this block has not been considered in the calculations for the effective memory occupation of the different models.

Table~\ref{table:models_params_and_memory} presents a detailed overview of the architectural parameters, weight counts, and activation sizes for each model, including EmbBERT-Q, and illustrates their total memory footprint. % and illustrate the trade-offs between model capacity and memory efficiency. 
Further details on ther architecture and on their memory and computational requirements can be found in ~\ref{appendix:memory}.
%Table~\ref{table:models_params_and_memory} provides an overview of each model’s architectural parameters, weight count, and activation sizes. These metrics jointly determine the final memory footprint, illustrating how various design choices trade off capacity for memory, while Appendix~\ref{sec:componentsAppendix} provides in-depth block-level descriptions of the various models used as comparisons.

%\begin{itemize}
    %\item For the embedder only model and embedder + convolution we adopted a Max pooling along the sentence length and then a fully connected layer to bring the size down to the number of classes of the dataset.
    %\item For MAMBA, due to it's left to right analysis of the phrase we adopted a Max pooling for the non-pretrained kind and a RMS pooling for it's pretrained counterpart with the same fully connected final layer as before.
    %\item All the remaining models output was handled by appending a \texttt{<CLS>} token at the start of the phrase and then using the attention pooled information in that token in conjunction with a final fully connected layer as for the other models.
%\end{itemize}

\subsection{Training and pretraining}
\label{subs:training_pretraining}

This section outlines the procedures used for both the pretraining and finetuning of EmbBERT-Q and the baseline models discussed in this work.

% Depending on the model’s capacity and requirements, the vocabulary size is set to either 8,192 or 2,048. Each model is then trained on its corresponding dataset for 20 epochs across at least 1,000 batches (batch size = 128) using the AdamW optimizer, with the learning rate fixed at either \(1 \times 10^{-3}\) or \(2 \times 10^{-3}\).
\paragraph{Pretraining Protocol}

For models supporting BERT-style pretraining~\cite{BERT}, we use the publicly available BookCorpus dataset~\cite{bookcorpus}. 
After training a Byte Pair Encoding (BPE) tokenizer tailored to the required dictionary size of each model, we construct sentence pairs for Masked Language Modeling (MLM) and Next-Sentence Prediction (NSP). Sentence pairs are generated by pairing half of the tokenized sentences contiguously and the other half randomly. Within each pair, tokens are masked with a 1/6 chance, and masking is applied with the following probabilities:
70\% of masked tokens are replaced with the \verb|<MASK>| token; 15\% are substituted with a random token; 15\% are left unchanged.
This masking strategy promotes contextual reasoning over random guessing. Pretraining is performed for one epoch with a batch size of 32 and a learning rate of \(5 \times 10^{-4}\) using the standard AdamW optimizer\footnote{Due to the stringent memory constraints considered in this work, more complex pre-training strategies such as ELECTRA-style training~\cite{ELECTRA} had to be excluded. ELECTRA requires both a generator and a discriminator, with the generator typically being about half the size of the discriminator. Under the strict 2~MB memory constraint, it is infeasible to construct a generator of sufficient size while maintaining a capable discriminator.}.

\paragraph{Finetuning protocol}

Following pretraining, models are finetuned on target datasets for 10 epochs using a fixed\footnote{To maintain consistency we omit complex learning rate schedulers, as different models may exhibit varying responses to specific schedules. Future work could systematically explore scheduling strategies for these models.} learning rate of \(3 \times 10^{-4}\).  
Validation is conducted at the end of each epoch using the Matthews Correlation Coefficient (MCC) for classification tasks or the Spearman Correlation Coefficient (SCC) for regression tasks. The best-performing checkpoint, determined by the highest validation metric, is saved and subsequently used for final testing.

Note that two models, namely Embedder and Embedder~+~Conv, do not undergo pretraining. Due to their extremely simple architectural structure, these models cannot effectively absorb BERT-style pretraining. Instead, they are trained directly on the target datasets for 20 epochs, doubling the standard finetuning protocol to ensure they consume the same amount of computation as the other models.

% \paragraph{Applicability to Different Architectures}
% We evaluate both \emph{pretrained} and \emph{non-pretrained} versions of our architectures on the tinyML benchmark (see Section~\ref{sec:results}). Exceptions include the \emph{Embedder-only} and \emph{Embedder+Convolution} models, which lack structural mechanisms (e.g., attention) to meaningfully integrate global context. Given this limitation, we do not apply BERT-style pretraining to those models. Instead, we directly train them on each target dataset and report their best results.

\subsection{Datasets}

In order to meaningfully compare the performance of EmbBERT-Q and the baseline models, we use two benchmark datasets: the \emph{TinyNLP benchmark} (introduced in this paper) and GLUE~\cite{GLUE}.
In the following subsections we proceed to detail the tasks contained in both these benchmarks datasets, and the procedure for the train-validation-test splitting.
\begin{comment}
\begin{enumerate}
    \item TinyNLP Benchmark: 
    A curated collection of datasets specifically tailored for evaluating \emph{Tiny Language Models} in resource-constrained environments. Details of the dataset selection are presented in Table~\ref{table:tiny_datasets}.
    \item GLUE Benchmark \cite{GLUE}: A widely adopted NLP benchmark comprising multiple, more complex datasets. These datasets are designed to test generalization and performance across diverse NLP tasks (see Table~\ref{table:GLUE_datasets}).
\end{enumerate}
\end{comment}

\subsubsection{The TinyNLP benchmark}
\label{sec:tinyNLP}
To better evaluate TLMs in the real-world scenarios and resource-constrained environments they are expected to operate, we introduce the \textit{TinyNLP} benchmark. 
This curated collection of existing datasets is specifically tailored to the constrained yet practical applications of language models on embedded devices.
Details of the dataset selection in the \textit{TinyNLP} benchmark are presented in Table~\ref{table:tiny_datasets}. The selection of these datasets represents application scenarios suited to models with restricted memory footprints, and is guided by the practical aim of assessing TLM deployment on embedded devices.

Building on the discussion in Section~\ref{sec:introduction}, the \textit{TinyNLP} benchmark focuses on tasks that are narrower in scope and less computationally demanding compared to standard (LLM) benchmarks. These tasks are grouped into three broad categories:
\begin{enumerate}[label=\roman*)]
    \item Request Classification: Relevant to virtual assistants in TinyML contexts, these datasets involve discerning the type of user request (e.g., requests for information, action, or assistance). As datasets focused on this kind of task, we have included \texttt{nlu}~\cite{nlu_dataset} and \texttt{Snips} in the \textit{TinyNLP} benchmark.
    \item Sentiment Analysis: Focuses on determining the emotional tone or opinion expressed in text. This commonly involves classifying content as positive, negative, or neutral, and sees wide usage in analyzing customer reviews or social media feedback. As datasets focused on this kind of task, we have included \texttt{IMDb}~\cite{imdb_dataset} and \texttt{Emotion}~\cite{emotion_dataset}.
    \item Context Understanding: Involves identifying the broader context in which text is generated. For example, distinguishing whether the text describes a particular situation or environment. As datasets focused on this kind of task, we have included \texttt{ag\_news}~\cite{ag_news_dataset}, \texttt{cyberbull}~\cite{cyberbull_dataset} and \texttt{LiMiT}~\cite{limit_dataset}.
\end{enumerate}

\begin{table}[t]
    \caption{Datasets selected as \textbf{benchmarks for TinyML} NLP, covering various tasks (TinyNLP).}
    \begin{center}
        \resizebox{\columnwidth}{!}{
        \begin{tabular}{| c | c c c |}
            \hline
            \textbf{Name} & \textbf{Size} & \textbf{Classes} & \textbf{Kind}  \\
            \hline \hline
            \href{https://huggingface.co/datasets/stanfordnlp/imdb}{\textbf{\texttt{IMDb}}}                                  & 50k   &  2    & Sentiment analysis \\ 
            \href{https://huggingface.co/datasets/fancyzhx/ag_news}{\textbf{\texttt{ag\_news}}}                              & 127.6k&  4    & Document classification \\
            \href{https://www.kaggle.com/datasets/andrewmvd/cyberbullying-classification}{\textbf{\texttt{cyberbull}}}       & 46k   &  6    & Racism classification  \\
            \href{https://huggingface.co/datasets/IBM/limit}{\textbf{\texttt{LiMiT}}}                                        & 24.6k &  2    & Movement presence \\
            \href{https://huggingface.co/datasets/dair-ai/emotion}{\textbf{\texttt{Emotion}}}                                & 20k   &  6    & Sentiment analysis \\
            \href{https://huggingface.co/datasets/xingkunliuxtracta/nlu_evaluation_data}{\textbf{\texttt{nlu}}}              & 25.7k &  18   & Request classification  \\
            \href{https://huggingface.co/datasets/benayas/snips}{\textbf{\texttt{Snips}}}                                    & 14.5k &  7    & Request classification  \\
            
            \hline
        \end{tabular}
        \label{table:tiny_datasets}
        }
    \end{center}
\end{table}


\subsubsection{The GLUE benchmark}
The General Language Understanding Evaluation (GLUE) benchmark~\cite{GLUE} is a widely adopted NLP benchmark comprising multiple, diverse and complex datasets, designed to test generalization and performance across diverse NLP tasks (see Table~\ref{table:GLUE_datasets}). 
%It contains a diverse suite of tasks designed to assess a model’s ability to understand and process natural language. 
It encompasses multiple subtasks, including sentiment classification and regression on sentence pairs. Because the official GLUE labels are only publicly released for the training and validation splits - and in line with prior approaches (e.g., \cite{exploring_transformer_sizes}) - we treat the validation set as our test split throughout this study.
\begin{table}[t]
    \caption{Datasets from the \textbf{GLUE benchmark}~\cite{GLUE}, used for comparison with larger SotA models.}
    \begin{center}
        \resizebox{\columnwidth}{!}{
        \begin{tabular}{| c | c c c |}
            \hline
            \textbf{Name} & \textbf{Size} & \textbf{Classes} & \textbf{Kind}  \\
            \hline \hline
            \textbf{\texttt{cola}}       & 10.7k &  2    & grammatical/semantical correctness \\ 
            \textbf{\texttt{mnli-m}}     & 403k  &  3    & correct summarization \\
            \textbf{\texttt{mnli-mm}}    & 403k  &  3    & correct summarization  \\
            \textbf{\texttt{mrpc}}       & 5.8k  &  2    & semantical equality \\
            \textbf{\texttt{qnli}}       & 116k  &  2    & question/answer entailment \\
            \textbf{\texttt{qqp}}        & 795k  &  2    & concept repetition  \\
            \textbf{\texttt{rte}}        & 5.77k &  2    & correct summarization  \\
            \textbf{\texttt{sst2}}       & 70k   &  2    & sentiment classification  \\
            \textbf{\texttt{stsb}}       & 8.63k &  /    & phrase similarity regression  \\
            \textbf{\texttt{wnli}}       & 852   &  2    & phrases entailment  \\
            
            \hline
        \end{tabular}
        \label{table:GLUE_datasets}
        }
    \end{center}
\end{table}

\subsubsection{Train-Validation-Test Dataset Splittings}
For both the TinyNLP and GLUE benchmarks, each dataset is divided into training, validation, and test sets according to one of the following protocols, listed in order of priority:
\begin{enumerate}[label=\roman*)]
    \item Provided splits: When the dataset creators supply official train, validation, and test splits, we use these directly to ensure consistency with prior work.
    \item For datasets with only a single official split (e.g., train-test only), we designate the larger portion as the training set and the smaller portion as the test set. From the training set, we withhold 10\% of the samples to create a validation set.
    \item No provided splits: For datasets lacking any predefined splits, we partition the data into a 90-10 ratio for training and testing. Subsequently, 10\% of the training set is withheld to create a validation set.
\end{enumerate}

\subsection{Evaluation}
\label{sec:eval}

%All code, scripts, results, and model checkpoints can be found on our \href{https://github.com/RiccardoBravin/tiny-LLM}{GitHub repository}.

The pretraining of all models on the BookCorpus dataset is conducted once, while the fine-tuning phase on each target dataset is repeated five times with different random seeds corresponding to different AdamW mini-batch shuffling, to ensure robustness of the results. Evaluation metrics are computed as the average of these five runs.

For the sake of simplicity, in the experimental results reported in Sec.~\ref{sec:results}, %Tables~\ref{table:res_tiny_comp}~and~\ref{table:res_glue_comp1}, 
as evaluation metrics we focus on Accuracy for the TinyNLP benchmark, and on the metric used for computing the average Score in each dataset in the GLUE benchmark: SCC for STSB, MCC for CoLA, F1~score for QQP and MRPC, Accuracy for the remaining GLUE tasks. 
%For the STSB dataset, the only regression task in the GLUE benchmark, we report the Average Loss and Spearman Correlation Coefficient (SCC).
Complete results across all evaluation metrics, including Loss, Accuracy, F1~Score, Precision, Recall, and the Matthews Correlation Coefficient (MCC) for classification tasks, are reported in Appendix~\ref{appendix:completeresults}.

Average results for the two benchmark datasets were also calculated and are reported in the last column of Tables~\ref{table:res_tiny_comp}~and~\ref{table:res_glue_comp1}.
Average accuracy was used as the average metric for the TinyNLP benchmark; while the average score for the GLUE benchmark was computed following the standard GLUE protocol.

%We explore two primary methodologies for training our models:
%\begin{enumerate}
%    \item Direct Training (No Pretraining):  
%   Each model is initialized randomly and trained from scratch for an extended number of epochs on a specific downstream dataset. This approach assesses how effectively a model can learn task-specific features within severe memory limits without benefiting from large-scale pretraining.
%   \item BERT-Style Pretraining + Fine-Tuning: 
%   We pretrain models using a masked language modeling (MLM) approach akin to BERT~\cite{BERT}, followed by fine-tuning on individual datasets. This strategy evaluates whether partial knowledge transfer from a larger corpus significantly boosts performance, despite our tight memory constraints.
%\end{enumerate}

%\subsection{Model Characterization}

 %In Appendix ~\ref{appendix:??} we report an in depth analisys of how to obtain for each layer constituting our models their respective activation sizes and weight count. 
% \begin{enumerate}
%     \item Table~\ref{table:hyperparameters}: Summarizes key architectural paramaeters (e.g., embedding dimension, number of attention heads, feed-forward size) across all models.
%     \item Table~\ref{table:memory}: Reports parameter and activation memory usage, validating that each architecture fits within our 2 MB limit.
% \end{enumerate}

% By comparing these architectures, we aim to highlight the design strategies that best balance performance and memory feasibility. The next section details our training protocols, datasets, and evaluation metrics, thereby contextualizing the performance results presented later in Section~\ref{sec:results}.

%-----------------------------------------------------------------------------
% Experimental results
%-----------------------------------------------------------------------------

\section{Experimental Results}
\label{sec:results}

\begin{table}[htbp]
    \caption{Model performance on the TinyNLP benchmark, reporting accuracy for each individual dataset and overall averages.}
    \begin{center}
    \resizebox{\textwidth}{!}{
        \begin{tabular}{| c | c  c  c  c  c  c  c | c |}
        \hline
        \textbf{Models}&
        \href{https://huggingface.co/datasets/stanfordnlp/imdb}{\textbf{IMDb}} &
        \href{https://huggingface.co/datasets/fancyzhx/ag_news}{\textbf{ag\_news}}  &
        \href{https://www.kaggle.com/datasets/andrewmvd/cyberbullying-classification}{\textbf{cyberbull}}  &
        \href{https://huggingface.co/datasets/IBM/limit}{\textbf{LiMiT}} &
        \href{https://huggingface.co/datasets/dair-ai/emotion}{\textbf{Emotion}} &
        \href{https://huggingface.co/datasets/xingkunliuxtracta/nlu_evaluation_data}{\textbf{nlu}} &
        \href{https://huggingface.co/datasets/benayas/snips}{\textbf{Snips}} &
        \textbf{Average Acc.} \\
        \hline
        \hline
        Embedder & 82,60 & 91,10 & 82,78 & 71,60 & 89,40 & 89,50 & \textbf{97,93} & 86,41 \\
        \hline
        Embedder + conv & \textbf{84,08} & \textbf{91,50} & 83,10 & 70,32 & 89,45 & 89,33 & 97,75 & 86,50 \\
        \hline
        BERT(2MB)~\cite{BERT} & 79,38 & 89,00 & 83,90 & \textbf{74,72} & 77,34 & 86,14 & 97,00 & 83,93 \\
        \hline
        MAMBA(2MB)~\cite{MAMBA} & 81,86    & 89,40      & 81,38      & \textbf{74,72}       & 45,72      & 70,10      & 96,40      &  77,08     \\
        \hline 
        %\textbf{EmbBERT (ours)} & \textbf{84,10} & 90,46 & \textbf{83,97} & \textbf{76,36} & \textbf{89,58} & 88,16 & 97,67 & \textbf{87,19} \\
        \textbf{EmbBERT-Q} (ours)     & 84,01 & 90,63 & \textbf{86,60} & 74,10 & \textbf{89,90} & \textbf{94,05} & \textbf{97,93} & \textbf{88,17} \\

        \hline
        \hline
        \textcolor{gray}{\textit{BERT-Tiny}(20MB)~\cite{tinyBERT}} & \textcolor{gray}{85,69} & \textcolor{gray}{91,93} & \textcolor{gray}{83,38} & \textcolor{gray}{72,40} & \textcolor{gray}{88,86} & \textcolor{gray}{88,53} & \textcolor{gray}{98,16} & \textcolor{gray}{86,99} \\
        \hline
        \end{tabular}

        \label{table:res_tiny_comp}
    }
    \end{center}

    
    
\end{table}





\begin{table}[htbp]
    \caption{Model performance on the GLUE benchmark. Metrics are MCC for CoLA, F1~score for MRPC and QQP, Spearman’s Correlation Coefficient (SCC) for STSB, and accuracy for the remaining tasks, as required for the official calculation of the overall GLUE score.}
    \begin{center}
        \resizebox{\textwidth}{!}{
        \begin{tabular}{| c | c c c c c c c c c c | c |}
        \hline
        \textbf{Models} & COLA & SST-2 & MRPC & QQP & MNLI-m & MNLI-mm & QNLI & RTE & WNLI & STSB & Score \\
        \hline
        Embedder   & \textbf{9,65} & 78,90 & 62,25 & \textbf{83,28} & 62,06 & 62,17 & 65,40  & \textbf{52,73}  & 77,20  & 15,58   & 56,92    \\ 
        \hline
        Embedder + conv     & 9,25  & 79,10  & 60,50  & 82,98  & 61,98  & 60,93 & 62,08  & 52,00  & 79,16  & 16,10  & 56,41    \\
        \hline
        BERT(2MB) ~\cite{BERT} & -0,86 & 71,28 & 64,66 & 73,04 & 60,56 & 61,58 & 60,82 & 48,24 & 66,20 & 15,48 & 52,10\\
        \hline
        MAMBA(2MB) ~\cite{MAMBA} & 2,56  & \textbf{81,16}  & 64,62  &  79,18 & 61,22  & 61,40  & 63,20  & 50,20 & 23,38  & 10,16  & 49,71  \\
        \hline
        %\textbf{EmbBERT (ours)}  & \textbf{62,88} & 79,31 & \textbf{69,19} & 83,25 & \textbf{67,75} & \textbf{68,78} & \textbf{68,78} & 46,87 & \textbf{87,83} & \textbf{49,25} & \textbf{63,50} \\
        \textbf{EmbBERT-Q} (ours)     & 9,56      & 80,96     & \textbf{67,99}     & 82,45     & \textbf{67,10}     & \textbf{68,05}     & \textbf{68,06}     & 47,29    & \textbf{87,32}      & \textbf{49,28}     & \textbf{62,81} \\
        \hline
        \hline
        \textcolor{gray}{\textit{BERT-Tiny}(20MB) ~\cite{tinyBERT}} & \textcolor{gray}{0,00}      & \textcolor{gray}{83,20}      & \textcolor{gray}{71,10}     & \textcolor{gray}{62,20}      & \textcolor{gray}{70,20}      & \textcolor{gray}{70,30}      & \textcolor{gray}{81,50}      & \textcolor{gray}{57,2}      & \textcolor{gray}{62,30}      & \textcolor{gray}{73,60}      & \textcolor{gray}{63,16}    \\
        \hline
        \end{tabular}

    \label{table:res_glue_comp1}
    }
    \end{center}

    
    
\end{table}

% risultati principali
We evaluate EmbBERT-Q and the baseline models on the TinyNLP and GLUE benchmark datasets, comparing their performance respectively in Tables~\ref{table:res_tiny_comp}~and~\ref{table:res_glue_comp1}, showing the superiority of EmbBERT-Q on both datasets and over all baselines, for what concerns both performance and memory requirements (shown in Table~\ref{table:models_params_and_memory}).

%pre-training
We compare the performances of all models on both TinyNLP and GLUE under pre-trained and non-pre-trained conditions, as reported in the Tables of \ref{appendix:completeresults}. 
%The variation in the ability to leverage pretraining highlights 
This comparison allows to make a significant observation: architectures equipped with attention mechanisms or SSMs can benefit more substantially from large, diverse pretraining corpora compared to simpler embedding-based approaches, but with the limitations given by the strict constraints of our 2~MB memory budget they still struggle even to reach comparable results to very simple models if not purposefully adjusted for the task.

To account for the models’ different capabilities in leveraging pretraining, the results reported in Tables~\ref{table:res_tiny_comp}~and~\ref{table:res_glue_comp1} refer to two different training scenarios:
i) a non-pretrained protocol, for the \emph{Embedder} and \emph{Embedder~+~Convolution} models, which lack architectural mechanisms to fully exploit pretraining; and ii) a pretrained and fine-tuned protocol, for all other models.

The next sections provide a detailed analysis of results obtained on the TinyNLP and GLUE benchmarks. 
%Finally, we discuss how memory requirements vary across the presented models and their implications for deployment in resource-constrained environments.


\subsection{TinyNLP Benchmark Results}

On the TinyNLP benchmark, where models are evaluated based on Accuracy, EmbBERT-Q demonstrates the best performance compared to the other models, achieving an Average Accuracy of 88.17\%, as shown in Table~\ref{table:res_tiny_comp}. 
Notably, EmbBERT-Q outperforms BERT-Tiny, which requires around $25\times$ more memory but only achieves the second-highest average Accuracy of 86.99\%.

Interestingly, the only models in the 2~MB range that offer comparable results were the Embedder and Embedder~+~Conv configurations. Despite their seemingly simplistic design, these models perform well on the TinyNLP tasks, which generally feature shorter and less complex phrases with respect to the GLUE benchmark. For these tasks, the pretraining applied to the other models had a relatively limited impact, as shown in ~\ref{appendix:completeresults}. These results highlight the Embedder models' ability to handle lightweight tasks effectively.

The 2~MB down-scaled versions of the BERT and MAMBA models, on the other hand, score lower on Average Accuracy with respect to Embedder models, indicating that these models may be less suitable for environments with stringent memory budgets. This suggests that the overall architecture of EmbBERT-Q, with highly optimized embedding and attention structures, is particularly well-suited for the TinyNLP classification tasks in memory-constrained scenarios, with respect to down-scaled versions of standard models.

% The EmbBERT long variant displayed similar success, averaging 87.23\% accuracy and 79.65\% MCC, confirming the effectiveness of its architecture for memory-constrained NLP tasks. While the Embedder and Emb + Conv models achieved slightly lower performance, they still proved very competitive, averaging around 86.41\%-86.50\% accuracy and 78.09\%-78.49\% \gls{mcc} considering that their architecture is much simpler and due to this they might be very well suited for environments where speed, at the cost of performance, is preferable. 

\subsection{GLUE Benchmark Results}

On the GLUE benchmark, models were evaluated using various metrics, including Matthews Correlation Coefficient (MCC), F1~score, Spearman’s Correlation Coefficient (SCC), and Accuracy, depending on the nature of each task.
EmbBERT-Q once again emerges as the top-performing model within the 2~MB memory range, achieving an average score of 62.81. It demonstrates competitive performance across multiple tasks, excelling particularly in datasets such as WNLI (87.32\% F1~score) and STSB (49.25\% F1~score). Detailed results for the score metrics are presented in Table~\ref{table:res_glue_comp1}, along with the average computed across all datasets (in the last column), while complete results can be found in \ref{appendix:completeresults}.

%In the GLUE benchmark, models were evaluated on various metrics, i.e., \gls{mcc}, F1, \gls{scc}, and accuracy, depending on task requirements. 

Remarkably, EmbBERT-Q (781~kB) achieves a performance on GLUE comparable to the $25\times$ larger BERT-Tiny model (20 MB) despite the substantial difference in memory requirements, even outperforming BERT-Tiny in 4 out of the 10 datasets included in the benchmark. The Embedder and Embedder~+~Conv models again outperformed BERT and MAMBA down-scaled within the 2~MB range, but obtaining a significant 7-point difference in score compared to EmbBERT-Q, highlighting its superior performance with respect to the baselines.

\subsection{Discussion of the Results}

The results from both the TinyNLP and GLUE benchmarks establish the proposed EmbBERT-Q as the current SotA Language Model for TinyML hardwares and NLP applications. Among the available LMs, it obtains the best experimentally observed balance between memory requirements and task performance.
These results become even more significant when the memory requirements of competitor models are taken into account. EmbBERT-Q requires just 781 kB of total memory for both weights and activations, representing a reduction of approximately $2.4\times$ compared to the other models we tested in the 2~MB range. The contrast is even more pronounced when compared to BERT-Tiny, the smallest SotA LM available in the literature, which demands nearly $25\times$ more memory than EmbBERT-Q.

%In order to maintain the performance of the algorithm after the quantization process, further fine-tuning of the quantized model has been applied. Nevertheless, even this step proved remarkably efficent, with only 8\% of the parameters requiring updates.

%The results emphasize the feasibility of deploying parameter-efficient NLP models on microcontrollers while maintaining satisfactory performance across both general benchmarks and domain-specific tasks. 

%\subsection{Model quantization under 1 MB}

%With the objective of further reducing the memory and computational demands of the proposed EmbBERT model, and to demonstrate that the proposed architecture redesign approach can be used in conjunction with other optimizations techniques, we have applied the post-training quantization procedure described in Section \ref{subs:quantization} to the model. We have called this quantized version of our model \textbf{EmbBERT (8-bit)} and have reported its results on the two benchmarks suites in Table \ref{table:res_tiny_quant} and Table \ref{table:res_tiny_quant}, along with the results of EmbBERT for comparison. 

%\input{tab_quantization}

%The results show that EmbBERT and it's quantized version are similarly effective in handling TinyNLP tasks. In particular EmbBERT (8-bit) achieves an average accuracy of 88.17\%, even surpassing its unquantized counterpart in several datasets and beating by a good margin even Bert-Tiny \cite{BERT}.
%BERT + NE + EF while still being competitive in it's unquantized form, suffered significant accuracy drops after quantization. 
%On the GLUE benchmark, which evaluates broader natural language understanding tasks, EmbBERT again demonstrated remarkable robustness to quantization. %The unquantized EmbBERT outperforming BERT + NE + EA by a margin of over 6 points. 
%while 
%EmbBERT (8-bit) achieved an overall GLUE score of 62.81, just 0.7 percentual points lower than EmbBERT, showcasing minimal degradation in performance after quantization. The quantized BERT + NE + EA model showed a substantial performance drop, achieving only 45.97, showing, with even some catastrophic forgetting in MNLI-m and MNLI-mm, that it struggles with the diverse and complex tasks in GLUE when quantized.

%EmbBERT (8-bit) consistently outperformed the comparisons across TinyNLP and GLUE benchmarks. In both of them, all the comparison models in the 2MB range obtains consistently lower performance even considering their unquantized version.

%-----------------------------------------------------------------------------
% Ablation study
%-----------------------------------------------------------------------------
\begin{figure*}[t]
    \centering
    \begin{minipage}[t]{0.48\textwidth} % Adjust width for each column
        \centering
        \includegraphics[width=\textwidth]{plot_Model_Sizes_and_Averages.pdf} % Replace with your first image file
        %\caption{Model's average accuracy on the TinyNLP benchmarkof the models in the ablation study.}
        %\label{fig:image1}
    \end{minipage}%
    \hfill
    \begin{minipage}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{plot_Model_Sizes_and_Averages_GLUE.pdf} % Replace with your second image file
        %\caption{Model's scores on the GLUE benchmark of the models in the ablation study.}
        %\label{fig:image2}
    \end{minipage}
    \caption{Number of weights (blue), number of activations (grey), and Accuracy/Score results obtained by each model analyzed in Sec.~\ref{sec:ablation} on the TinyNLP benchmark (left panel) and on the GLUE benchmark (right panel).}
    \label{Fig:memory_ablation}
\end{figure*}



\section{Evaluating the Impact of The Architectural Components of EmbBERT-Q}
\label{sec:ablation}

In this section, we analyze the contributions of EmbBERT-Q architectural components to its overall performance on both the TinyNLP and GLUE benchmarks. 

%Starting from the standard BERT Encoder we perform four main modifications:
%\begin{itemize}
    %\item Changing the Standard Attention for an Efficient Attention variant from~\cite{efficient_attention} without multi-head as \cite{exploring_transformer_sizes} has shown limited benefits on smaller models
    %\item Removing the Feed Forward block in favor of an optimized integration with the attention
    %\item Adding in parallel to the Efficient Attention a Convolutional Skip Connection which enables local processing of data in an efficient manner
    %\item We substitute the standard sum aggregation in favor of a learned weighted difference between the two paths, technique that has been proved effective in ~\cite{differential_transformers}
%\end{itemize}

Taking BERT (2~MB) as a baseline, we systematically introduced improved and optimized key components, defining in this way EmbBERT-Q. Through this process, we evaluate the individual and collective impact of these changes, always adhering with the strict memory constraints of 2~MB.

Figure~\ref{Fig:memory_ablation} shows the memory requirements of various model configurations alongside their corresponding accuracy on TinyNLP and score on GLUE. 

\paragraph{Base model - BERT(2MB)}
BERT(2MB) is a compressed variant of BERT that serves as our vanilla baseline model. Compared to the larger BERT-Tiny model~\cite{tinyBERT}, which has a $10\times$ memory footprint, BERT(2MB) shows a notable performance degradation that could be attributed to its reduced parameter count.
Despite these limitations, BERT(2MB) marks a first step toward adapting transformer architectures for ultra-low-memory environments, demonstrating the feasibility of scaling down LM models while maintaining some level of task performance.

\paragraph{BERT + Nano Embedder (BERT + NE)}
To address the limitations of BERT(2MB), we replace its Embedder with the Nano Embedder~\cite{NanoBERT}, which is designed to optimize embedding representations without increasing the overall parameter count. This substitution expands the effective vocabulary space within the same memory budget, resulting in notable performance improvements on both the TinyNLP and GLUE benchmarks, as can be seen in Fig.~\ref{Fig:memory_ablation}.

\paragraph{BERT + Efficient Attention (BERT + EA)}
To reduce activation overhead, we proceed to replace the default multi-head attention module with Efficient Attention~\cite{efficient_attention}, aiming to lower weight and activation memory costs. This reduction allows for increased embedding dimensionality and/or additional layers. This modification significantly improves performance on the TinyNLP benchmark but, when not paired with other architectural modules, results in a slight decrease in performance with respect to the base BERT(2MB) model on the GLUE benchmark, as can be appreciated in Fig.~\ref{Fig:memory_ablation}.

\paragraph{BERT + NE + EA} %When used in conjunction, the Nano Embedder and the efficient attention provide a large gain in the accuracy and score metrics.
%In particular, in the TinyNLP benchmark BERT + NE + EA achieves an accuracy of $87.51\%$. At the same time, on the GLUE benchmark, BERT + NE + EA obtains a score of 61.20, over 9 points higher with respect to the original BERT (2MB)  
We combine the Nano Embedder with Efficient Attention to create the BERT~+~NE~+~EA model, leveraging a broader vocabulary together with reduced weights and activations overhead. 
This combination leads to a performance gain on both TinyNLP and GLUE tasks, where BERT~+~NE~+~EA achieves respectively an accuracy of 87.51\% and a score of 61.20, i.e. respectively over $+3$ and $+9$ points with respect to the original BERT(2MB).
These results highlight the advantage of combining embedding efficiency with an optimized attention mechanism in ultra-compact models.

\paragraph{EmbBERT}
%The final step from the architectural point of view is the addition of the Convolutional $1\times 1$ Efficient Differential Skip Attention mechanism. 
By finally integrating a parallel path with a Conv-1D followed by an Fully Connected layer we obtain the EmbBERT architecture. In this way, we add with a minimal memory overhead a modified feed forward block to the main attention path of BERT~+~NE~+~EA, i.e. a further simple token-mixing convolutional and fully connected layer.
This addition provides a marginal improvement on the TinyNLP benchmark, but achieves significant success on the more complex tasks contained in the GLUE benchmark. 
Specifically, it improves performance by over $+2$ points in the GLUE score compared to the BERT~+~NE~+~EA model.

\paragraph{EmbBERT-Q}
Finally, the EmbBERT model is quantized using the 8-bit post-training quantization procedure outlined in Section~\ref{subs:quantization}, resulting in the proposed EmbBERT-Q model.
In the TinyNLP benchmark, EmbBERT-Q achieves an average accuracy of 88.17\%, marking an improvement of nearly 1 percentage point over its unquantized counterpart. 
This increase can be attributed to the regularization effect that quantization may induce under certain conditions, combined with the relative simplicity of the tasks in this benchmark. 
On the GLUE benchmark, which evaluates broader natural language understanding tasks, EmbBERT-Q demonstrates exceptional robustness to quantization, achieving an overall GLUE score of 62.81. This represents a minimal performance drop of $-0.7$ percentage points compared to the unquantized EmbBERT version. In Appendix~\ref{appendix:completeresults} we show that the BERT~+~NE~+~EA model, instead, suffers significant performance drops of up to $15$ percentage points on GLUE due to post-training 8-bit quantization.

The quantization process gives substantial memory savings, reducing the total memory required to store and execute EmbBERT-Q from the around 2~MB of the unquantized EmbBERT to just 781~kB, considering both weights and activations (a $2.4\times$ reduction in memory demand). 
While quantization often introduces trade-offs in accuracy, the robustness of the EmbBERT architecture highlights its suitability for deployment in constrained environments where such memory optimization techniques are critical.

Through the comprehensive ablation study performed in this section, we have examined the contributions of key architectural components, including the Nano Embedder and Efficient Encoder. Maintaining the total memory usage below the 2~MB budget throughout the study, we have demonstrated that the inclusion of these architectural components and of 8-bit quantization in EmbBERT-Q leads to an Average Accuracy improvement of $+4.24$ percentage points on the TinyNLP benchmark and a $+10.71$ point increase on the GLUE benchmark score, with respect to the original BERT(2MB) model.
With its carefully designed architecture and 8-bit quantization, EmbBERT-Q pushes the frontier of ultra-compact language models, delivering state-of-the-art performance in environments with stringent memory and computational constraints.





%-----------------------------------------------------------------------------
% Conclusions
%-----------------------------------------------------------------------------

\section{Conclusions}
\label{sec:conclusions}

%This study demonstrates that aggressively scaling down language models, such as the original BERT and MAMBA architectures, remains challenging for deployment in highly memory-constrained environments, such as those required by TinyML applications. These models consistently underperform compared to simpler baselines like Embedder and Embedder~+~Conv, primarily due to their high activation memory requirements, which limit their weight count and thus their applicability in such constrained scenarios.

%Unexpectedly, the baseline architectures Embedder and Embedder~+~Conv emerged as competitive solutions in these memory-constrained environments. Their minimal activation footprints and straightforward architectural designs support faster training and make them appealing for simpler tasks that do not demand complex language understanding. In Appendix~\ref{appendix:memory}, we provide a comprehensive analysis of memory requirements and computational costs for all models evaluated in this study.

In this work, we presented EmbBERT-Q, a novel language model specifically designed for tiny devices and extreme resource constraints. EmbBERT-Q achieves Accuracies and Scores comparable to the ones of models with up to $25\times$ its memory footprint, and stands out as the highest-performing solution within the 2~MB memory budget explored in this paper. 

%Exploiting major architecture redesigns and optimization techniques, such as the Embedder block and the Efficient Encoder, 

By leveraging an innovative architectural design specifically tailored for extremely memory- and computationally-constrained environments, EmbBERT-Q effectively balances parameter efficiency and competitive accuracy. Its Embedder Block, Efficient Encoder, and the application of post-training 8-bit quantization significantly reduce the model’s memory footprint while maintaining high performance both on the newly proposed TinyNLP benchmark and on the GLUE benchmark, crucially demonstrating the effectiveness and the efficiency of EmbBERT-Q in resource-constrained environments.

Future research will explore further compression techniques, novel architectural designs, targeted knowledge distillation, and even more extreme quantizations tailored to emerging hardware accelerators (e.g., 1-bit quantization). Combining these advancements with next-generation hardware has the potential to further optimize model memory and computation footprints while preserving, or even enhancing, performance. Our work establishes a systematic foundation for designing efficient language models capable of operating effectively within the most stringent memory constraints.
\begin{comment}
In conclusion, the original BERT architecture proves difficult to scale down effectively for resource-constrained environments, consistently underperforming compared to our proposed models across various tasks, whether pretrained or not. MAMBA, despite having relatively large activation sizes, remains competitive due to its lower computational complexity, making it a viable midway option.

Our attention-based model, EmbBERT, excels as the top performer, particularly when pretrained. It combines high parameter efficiency with strong performance, often matching or surpassing models that are significantly larger, making it a compelling choice for environments where performance is paramount, even at the cost of increased computational demands.

For applications where pretraining is not feasible or rapid on-device learning is required, the Nano Embedder model is the most efficient option. Its minimal activation sizes and reduced hyperparameter count make it ideal for fast training, and it opens the door for further optimizations, such as \gls{nas}.

Future work could focus on further reducing model size through techniques like \gls{lora} or knowledge distillation.
This study marks a significant advancement in the development of compact, parameter-efficient models capable of outperforming traditional NLP methods in simple classification tasks, all while requiring far fewer computational resources.

Notably, this research provides a unique contribution by thoroughly analyzing both model size and activation memory, specifically for single-core, single-threaded processing. This positions our work as a critical step toward optimizing deep learning models for highly resource-constrained environments, paving the way for more efficient deployment on microdevices.
\end{comment}

\section*{Acknowledgment}

This paper is supported by PNRR-PE-AI FAIR project
funded by the NextGeneration EU program.

%\newpage
%\printglossaries
\appendix

\section{Complete results}
\label{appendix:completeresults}

In this section, we provide the complete results of our experiments, spanning all datasets and models in both pretrained and non-pretrained contexts. The detailed results, concerning model quantization as well, are presented in Tables~\ref{table:all_results_tinyNLP_acc_unpretr}, \ref{table:all_results_tinyNLP_mcc_unpretr}, \ref{table:all_results_tinyNLP_acc}, \ref{table:all_results_tinyNLP_mcc}, \ref{table:res_glue_comp_unpretr}, and \ref{table:res_glue_comp}.

\subsection{Evaluation on the TinyNLP Benchmark}

For what concerns non-pretrained models, as shown in Table~\ref{table:all_results_tinyNLP_acc_unpretr} BERT(2MB) and MAMBA(2MB) demonstrate compatible results with respect to other models, with average accuracies of 83.74\% and 83.86\%, respectively. Notably, Embedder~+~Conv and Embedder outperform others models, achieving 86.50\% and 86.41\% average accuracy, respectively. The Embedder~+~Conv model particularly excells on datasets like AG News, scoring 91.50\%, the highest across the board for this task.

When non-pretrained and evaluated with the MCC score, as shown in Table~\ref{table:all_results_tinyNLP_mcc_unpretr} the Embedder-based models again deliver competitive performances, with Embedder~+~Conv achieving an average MCC of 78.49\%. In contrast, BERT-based models generally lagged behind. EmbBERT and EmbBERT-Q achieved an MCC score of 80.59\%, showing limited performance in the non-pretrained context.

For what concerns instead pretrained and finetuned models on TinyNLP, as shown in Table~\ref{table:all_results_tinyNLP_acc}, EmbBERT-Q consistently delivered the top Accuracy performance, achieving an average accuracy of 88.17\%. BERT~+~NE~+~EA followed closely with a score of 87.04\%. The MCC results shown in Table~\ref{table:all_results_tinyNLP_mcc} reflect the Accuracy trends, with EmbBERT-Q leading, showing its robustness to 8-bit quantization across diverse datasets.





\subsection{Evaluation on the GLUE Benchmark}

The GLUE benchmark evaluates models on NLP tasks such as sentiment analysis (SST-2), natural language inference (MNLI), and semantic similarity (STSB).
For non-pretrained models, as shown in Table~\ref{table:res_glue_comp_unpretr} Embedder achieves a GLUE score of 56.92, followed closely by Embedder~+~Conv at 56.41. EmbBERT surpasses both, showing (even if by a small margin in the non-pretrained context) its architectural superiority on difficult NLP tasks. Among pretrained models, as shown in Table~\ref{table:res_glue_comp}, EmbBERT outperforms all variants by a significant margin, showing its superior ability to absorb pretraining with respect to all other models. EmbBERT-Q shows minimal accuracy degradation and robustness to 8-bit quantization.
\begin{table}[htbp]
    \caption{Accuracy of non-pretrained models on the TinyNLP benchmark.}
    \begin{center}
    \resizebox{\textwidth}{!}{
        \begin{tabular}{| c | c c c c c c c c |}
        \hline
        \textbf{Models} & \href{https://huggingface.co/datasets/stanfordnlp/imdb}{\textbf{IMDb}} & \href{https://huggingface.co/datasets/fancyzhx/ag_news}{\textbf{ag\_news}} & \href{https://www.kaggle.com/datasets/andrewmvd/cyberbullying-classification}{\textbf{cyberbull}} & \href{https://huggingface.co/datasets/IBM/limit}{\textbf{LiMiT}} & \href{https://huggingface.co/datasets/dair-ai/emotion}{\textbf{Emotion}} & \href{https://huggingface.co/datasets/xingkunliuxtracta/nlu_evaluation_data}{\textbf{nlu}} & \href{https://huggingface.co/datasets/benayas/snips}{\textbf{Snips}} & \textbf{Average} \\
        \hline \hline
        \textbf{BERT(2MB)}           & 78,98 & 88,93 & 82,63 & 70,10 & 83,35 & 85,63 & 96,58 & 83,74 \\
        \textbf{MAMBA(2MB)}          & 78,18 & 91,08 & 83,74 & 71,66 & 77,40 & 87,60 & 97,34 & 83,86 \\
        \textbf{Embedder}           & 82,60 & 91,10 & 82,78 & 71,60 & 89,40 & 89,50 & \textbf{97,93} & 86,41 \\
        \textbf{Embedder~+~Conv}      & 84,08 & \textbf{91,50} & 83,10 & 70,32 & 89,45 & 89,33 & 97,75 & 86,50 \\
        \textbf{BERT~+~NE}            & 82,52 & 90,86 & 82,96 & 69,82 & 78,34 & 84,06 & 96,74 & 83,61 \\

        \textbf{BERT~+~NE~+~EA}         & 81,60 & 90,53 & 82,30 & 55,57 & 83,70 & 84,90 & 96,50 & 82,16 \\

        \hline
        \textbf{EmbBERT}            & 83,10 & 90,82 & 82,50 & 68,90 & 68,48 & 76,78 & 95,18 & 80,82 \\
        \hline
        \end{tabular}
        \label{table:all_results_tinyNLP_acc_unpretr}
        }
    \end{center} 
\end{table}

\begin{table}[htbp]
    \caption{Evaluation of MCC score of non-pretrained models on the TinyNLP benchmark.}
    \begin{center}
        \resizebox{\textwidth}{!}{

        \begin{tabular}{| c | c c c c c c c c |}
        \hline
        \textbf{Models} & \href{https://huggingface.co/datasets/stanfordnlp/imdb}{\textbf{IMDb}} & \href{https://huggingface.co/datasets/fancyzhx/ag_news}{\textbf{ag\_news}} & \href{https://www.kaggle.com/datasets/andrewmvd/cyberbullying-classification}{\textbf{cyberbull}} & \href{https://huggingface.co/datasets/IBM/limit}{\textbf{LiMiT}} & \href{https://huggingface.co/datasets/dair-ai/emotion}{\textbf{Emotion}} & \href{https://huggingface.co/datasets/xingkunliuxtracta/nlu_evaluation_data}{\textbf{nlu}} & \href{https://huggingface.co/datasets/benayas/snips}{\textbf{Snips}} & \textbf{Average} \\
        \hline \hline
        \textbf{BERT(2MB)}           &  58,10 & 85,25 & 79,45 & 40,13 & 78,35 & 84,05 & 96,00 & 74,48 \\
        \textbf{MAMBA(2MB)}          &  56,62 & 88,14 & 80,82 & 42,62 & 70,84 & 86,22 & 96,86 & 74,59 \\
        \textbf{Embedder}           & 82,60 & 91,10 & 82,78 & 71,60 & 89,40 & 89,50 & \textbf{97,93} & 86,41 \\
        \textbf{Embedder~+~Conv}      & 84,08 & \textbf{91,50} & 83,10 & 70,32 & 89,45 & 89,33 & 97,75 & 86,50 \\
        \textbf{BERT~+~NE}            & 65,06 & 87,82 & 79,86 & 38,30 & 72,34 & 82,42 & 96,20 & 74,57 \\

        \textbf{BERT~+~NE~+~EA}         & 63,32 & 87,35 & 79,13 & 20,67 & 78,85 & 83,30 & 95,93 & 72,65 \\

        \hline
        \textbf{EmbBERT}            & 66,32 & 87,78 & 79,26 & 38,42 & 60,78 & 74,66 & 94,40 & 71,66 \\
        \hline
        \end{tabular}
        \label{table:all_results_tinyNLP_mcc_unpretr}
        }
    \end{center} 
\end{table}

\begin{table}[htbp]
    \caption{Accuracy of pretrained and finetuned models on the TinyNLP benchmark (Embedder and Embedder~+~Conv are directly trained on the downstream datasets).}
    \begin{center}
        \resizebox{\textwidth}{!}{

        \begin{tabular}{| c | c c c c c c c c |}
        \hline
        \textbf{Models} & \href{https://huggingface.co/datasets/stanfordnlp/imdb}{\textbf{IMDb}} & \href{https://huggingface.co/datasets/fancyzhx/ag_news}{\textbf{ag\_news}} & \href{https://www.kaggle.com/datasets/andrewmvd/cyberbullying-classification}{\textbf{cyberbull}} & \href{https://huggingface.co/datasets/IBM/limit}{\textbf{LiMiT}} & \href{https://huggingface.co/datasets/dair-ai/emotion}{\textbf{Emotion}} & \href{https://huggingface.co/datasets/xingkunliuxtracta/nlu_evaluation_data}{\textbf{nlu}} & \href{https://huggingface.co/datasets/benayas/snips}{\textbf{Snips}} & \textbf{Average} \\
        \hline \hline
        \textbf{BERT(2MB)}           & 79,38 & 89,00 & 83,90 & 74,72 & 77,34 & 86,14 & 97,00 & 83,93 \\
        \textbf{MAMBA(2MB)}          & \textbf{84,76} & 90,68 & 81,20 & 73,98 & 74,58 & 73,24 & 93,42 & 81,69 \\
        \textbf{Embedder}           & 82,60 & 91,10 & 82,78 & 71,60 & 89,40 & 89,50 & \textbf{97,93} & 86,41 \\
        \textbf{Embedder~+~Conv}      & 84,08 & \textbf{91,50} & 83,10 & 70,32 & 89,45 & 89,33 & 97,75 & 86,50 \\
        \textbf{BERT~+~NE}            & 81,86 & 89,40 & 81,38 & 74,72 & 45,72 & 70,10 & 96,40 & 77,08 \\
        \textbf{BERT~+~EA}            & 80,46 & 89,46 & 84,58 & 74,12 & 85,78 & 87,44 & 97,62 & 85,64 \\
        \textbf{BERT~+~NE~+~EA}         & 83,19 & 90,80 & 84,13 & 75,80 & 88,70 & 88,88 & 97,79 & 87,04 \\
        \textbf{BERT~+~NE~+~EA (8bit)}  & 82,35 & 89,51 & 85,58 & \textbf{76,40} & 80,85 & 89,46 & 97,29 & 85,92 \\
        \hline
        \textbf{EmbBERT}            & 84,10 & 90,46 & 83,97 & 76,36 & 89,58 & 88,16 & 97,67 & 87,19 \\
        \textbf{EmbBERT-Q}      & 84,01 & 90,63 & \textbf{86,60} & 74,10 & \textbf{89,90} & \textbf{94,05} & \textbf{97,93} & \textbf{88,17} \\
        \hline
        \end{tabular}
        \label{table:all_results_tinyNLP_acc}
        }
    \end{center} 
\end{table}

\begin{table}[htbp]
    \caption{Evaluation of MCC score of pretrained and finetuned models on the TinyNLP benchmark (Embedder and Embedder~+~Conv are directly trained on the downstream datasets).}
    \begin{center}
        \resizebox{\textwidth}{!}{

        \begin{tabular}{| c | c c c c c c c c |}
        \hline
        \textbf{Models} & \href{https://huggingface.co/datasets/stanfordnlp/imdb}{\textbf{IMDb}} & \href{https://huggingface.co/datasets/fancyzhx/ag_news}{\textbf{ag\_news}} & \href{https://www.kaggle.com/datasets/andrewmvd/cyberbullying-classification}{\textbf{cyberbull}} & \href{https://huggingface.co/datasets/IBM/limit}{\textbf{LiMiT}} & \href{https://huggingface.co/datasets/dair-ai/emotion}{\textbf{Emotion}} & \href{https://huggingface.co/datasets/xingkunliuxtracta/nlu_evaluation_data}{\textbf{nlu}} & \href{https://huggingface.co/datasets/benayas/snips}{\textbf{Snips}} & \textbf{Average} \\
        \hline \hline
        \textbf{BERT(2MB)}           & 58,88 & 85,38 & 80,86 & 47,14 & 71,26 & 84,74 & 96,50 & 74,97 \\
        \textbf{MAMBA(2MB)}          & 63,78 & 85,90 & 77,92 & 46,56 & 30,40 & 66,92 & 95,76 & 66,75 \\
        \textbf{Embedder}           & 65,18 & 88,13 & 80,10 & 41,10 & 86,23 & 88,30 & 97,58 & 78,09 \\
        \textbf{Embedder~+~Conv}      & 68,15 & 88,68 & 80,45 & 40,45 & 86,18 & 88,13 & 97,38 & 78,49 \\
        \textbf{BERT~+~NE}            & 66,64 & 87,54 & 80,94 & 45,98 & 83,62 & 85,12 & 97,54 & 78,20 \\
        \textbf{BERT~+~EA}            & 61,08 & 85,94 & 81,66 & 47,86 & 81,64 & 86,10 & 97,26 & 77,36 \\
        \textbf{BERT~+~NE~+~EA}         & 68,04 & 88,08 & 82,60 & 51,24 & 85,62 & 86,46 & 97,06 & 79,87 \\
        \textbf{BERT~+~NE~+~EA (8bit)}  & 64,70 & 86,02 & 82,77 & 47,89 & 74,55 & 88,20 & 96,84 & 77,28 \\
        \hline
        \textbf{EmbBERT}            & 68,25 & 87,31 & 80,87 & 48,10 & 86,26 & 86,76 & 97,29 & 79,26 \\
        \textbf{EmbBERT-Q}      & 68,04 & 87,51 & 84,10 & 46,84 & 86,71 & 93,36 & 97,59 & 80,59 \\
        \hline
        \end{tabular}
        \label{table:all_results_tinyNLP_mcc}
        }
    \end{center} 
\end{table}

\begin{table}[htbp]
    \caption{Evauation of non-pretrained models on the GLUE benchmark. We report SCC for STSB, MCC for CoLA, F1~score for QQP and MRPC, Accuracy for the remaining GLUE tasks.}
    \begin{center}
    \resizebox{\textwidth}{!}{
        \begin{tabular}{| c | c c c c c c c c c c | c |}
        \hline
        \textbf{Models} & COLA & SST-2 & MRPC & QQP & MNLI-m & MNLI-mm & QNLI & RTE & WNLI & STSB & Score \\
        \hline
        \textbf{BERT(2MB)}           & 3,93 & 75,20 & 63,88 & 81,03 & 64,95 & 63,65 & 62,75 & 49,38 & 70,68 & 6,74 & 54,22 \\
        \textbf{MAMBA(2MB)}          & 7,22 & 80,60 & 60,72 & 80,66 & 64,27 & 64,44 & 59,78 & 51,76 & 80,00 & 4,58 & 55,40 \\
        \textbf{Embedder}           & 9,65 & 78,90 & 62,25 & 83,28 & 62,06 & 62,17 & 65,40 & 52,73 & 77,20 & 15,58 & 56,92 \\
        \textbf{Embedder~+~Conv}      & 9,25 & 79,10 & 60,50 & 82,98 & 61,98 & 60,93 & 62,08 & 52,00 & 79,16 & 16,10 & 56,41 \\
        \textbf{BERT~+~NE}            & 5,22 & 77,34 & 63,18 & 81,12 & 64,14 & 64,53 & 66,76 & 51,14 & 85,62 & 4,72 & 56,38 \\
        \textbf{BERT~+~NE~+~EA}         & 2,66 & 78,68 & 61,90 & 83,16 & 62,65 & 61,63 & 62,36 & 48,38 & 85,62 & 8,94 & 55,60 \\
        \hline  
        \textbf{EmbBERT}            & 5,32 & 78,50 & 62,54 & 82,58 & 63,82 & 65,78 & 63,68 & 51,26 & 87,30 & 9,76 & 57,05 \\
        \hline
        \end{tabular}

    \label{table:res_glue_comp_unpretr}
    }
    \end{center}
\end{table}

\begin{table}[htbp]
    \caption{Evaluation of pretrained and finetuned models on the GLUE benchmark (Embedder and Embedder~+~Conv are directly trained on the donwstream datasets). We report SCC for STSB, MCC for CoLA, F1~score for QQP and MRPC, Accuracy for the remaining GLUE tasks.}
    \begin{center}
    \resizebox{\textwidth}{!}{
        \begin{tabular}{| c | c c c c c c c c c c | c |}
        \hline
        \textbf{Models} & COLA & SST-2 & MRPC & QQP & MNLI-m & MNLI-mm & QNLI & RTE & WNLI & STSB & Score \\
        \hline
        \textbf{BERT(2MB)}           & -0,86 & 71,28 & 64,66 & 73,04 & 60,56 & 61,58 & 60,82 & 48,24 & 66,20 & 15,48 & 52,10   \\
        \textbf{MAMBA(2MB)}          & 2,56  & \textbf{81,16} & 64,62 & 79,18 & 61,22 & 61,40 & 63,20 & 50,20 & 76,62 & 10,16 & 55,03 \\
        \textbf{Embedder}           & 9,65  & 78,90 & 62,25 & \textbf{83,28} & 62,06 & 62,17 & 65,40 & 52,73 & 77,20 & 15,58 & 56,92 \\
        \textbf{Embedder~+~Conv}      & 9,25  & 79,10 & 60,50 & 82,98 & 61,98 & 60,93 & 62,08 & 52,00 & 79,16 & 16,10 & 56,41 \\
        \textbf{BERT~+~NE}            & 9,04  & 78,82 & 65,04 & 79,96 & 63,08 & 63,30 & 63,20 & 51,76 & 87,30 & 13,46 & 57,50 \\
        \textbf{BERT~+~EA}            & 10,06 & 79,44 & 66,48 & 78,88 & 60,82 & 60,82 & 63,50 & 50,76 & 22,24 & 17,92 & 51,09 \\
        \textbf{BERT~+~NE~+~EA}         & 18,70 & 79,60 & 65,36 & 82,66 & 67,06 & 67,44 & 67,44 & 53,66 & 86,74 & 23,34 & 61,20 \\
        \textbf{BERT~+~NE~+~EA (8bit)}  & 9,57  & 77,87 & 63,40 & 48,93 & 34,19 & 33,59 & 59,58 & \textbf{53,79} & 59,15 & 19,58 & 45,97 \\
        \hline  
        \textbf{EmbBERT}            & \textbf{11,01} & 79,33 & \textbf{69,19} & 83,25 & \textbf{67,83} & \textbf{68,63} & \textbf{68,92} & 49,96 & \textbf{87,61} & 49,25 & \textbf{63,50} \\
        \textbf{EmbBERT-Q}      & 9,56  & 80,96 & 67,99 & 82,45 & 67,10 & 68,05 & 68,06 & 47,29 & 87,32 & \textbf{49,28} & 62,81 \\
        \hline
        \end{tabular}

    \label{table:res_glue_comp}
    }
    \end{center}
\end{table}

\begin{comment}
\subsection{Key Insights}
The results highlight that in severely memory-constrained settings, models relying on embeddings directly trained on the downstream tasks, such as Embedder and Embedder~+~Conv, reach good accuracy and MCC across multiple datasets, especially in non-pretrained scenarios. %These models benefit from minimal activation footprints and simplicity, making them highly efficient for memory-constrained environments.
EmbBERT-Q emerges as a versatile and robust model, excelling in both pretrained and non-pretrained contexts while adhering to strict memory constraints. Variants of BERT, such as BERT~+~NE and BERT~+~EA, demonstrate improvements when incorporating optimized embedding techniques but still fall short of EmbBERT-Q performance.
Overall, architectures based on the attention mechanism such as EmbBERT-Q, even when quantized, deliver superior efficiency and accuracy across both the TinyNLP and GLUE benchmarks, demonstrating their superiority for resource-constrained applications.
\end{comment}


\section{Exact Computation of Memory and Computational Cost of LLM Layers}
\label{appendix:memory}

The architecture of Large Language Models (LLMs)
%\glspl{llm} 
is primarily based on the Transformer model introduced by~\cite{transformers}. This architecture has revolutionized natural language processing by enabling models to effectively handle long-range dependencies in text. Encoder-based text classification models typically consist of two main components: an embedder and an encoder. The encoder, in turn, is primarily composed of an Attention Mechanism and a Feed-Forward Neural Network.

In this section, we provide a comprehensive analytical, layer-by-layer evaluation of the memory and computational requirements of common components used in Language Models, as well as an overall view of their functionality.
We emphasize that our computational evaluations account for the complexity and memory access profiling associated with all needed model layers. Particular attention is given to CPU-based operations, including summation, multiplication, and memory retrievals.
For memory evaluation, we assume a non-parallel program that retains only the minimum required data in memory to execute effectively. This approach reflects realistic constraints in resource-constrained environments, such as those encountered in TinyML applications.

The following assumptions are made during our detailed computation of weight and activation matrices memory requirements for hardware deployment:
\begin{itemize} 
    \item Operations such as sums, element-wise multiplication, and activation functions are performed in-place, occupying only the memory of the input matrices, as intermediate results are discarded.
    \item Matrix multiplications require memory for both input matrices as well as the output matrix.
    \item Fully connected layers are treated as matrix multiplications where only one input and the output matrix contribute to activation memory, since weights do not increase activation memory requirements.
    \item The maximum memory consumption per layer is recorded at peak usage during processing.
    \item All calculations are based on inference-only processing, without accounting for training-related overheads.
\end{itemize}
The final memory occupations, memory accesses, and formulas for sums and multiplications for each block are provided in Tables~\ref{table:w_a_noMAMBA_full}~and~\ref{table:complexity_appendix}.

\subsection{BERT}
BERT (Bidirectional Encoder Representations from Transformers)~\cite{BERT} is a Transformer Encoder-based foundational NLP model widely used for tasks such as text classification, question answering, and text generation. It leverages Transformer layers to generate contextualized representations of input text, capturing both left-to-right and right-to-left dependencies.
The architecture of BERT typically consists of an Embedder, followed by a series of Attention and Feed-Forward layers, interleaved with normalization layers. These components are repeated $N$ times.%, forming a deep and powerful network capable of modeling complex linguistic structures.

\paragraph{Embedder}
The standard Embedder, illustrated in Fig.~\ref{fig:embedders_memory}, is responsible for generating token embeddings, positional encodings, and segment embeddings using learned dictionaries. These embeddings are then summed to produce the final input encoding fed into the model.

The total parameter count for the embedder, $W_{emb}$, is calculated as the sum of the sizes of the token embedding matrix ($v \cdot d$), the positional embedding matrix ($\ell \cdot d$), and segment embedding matrix ($2d$):
\begin{equation}
    W_{emb} = d \cdot (v + \ell + 2),
\end{equation}
where $v$ is the vocabulary size, $\ell$ is the sequence length, and~$d$ is the embedding dimension.
%$\alpha$ expansion dimension

The maximum activation size, $A_{emb}$, results from storing token, positional, and segment embeddings as matrices of size $\ell \cdot d$ during inference. These embeddings are summed in pairs, leading to: 
\begin{equation}
    A_{emb} = 2d \cdot \ell.
\end{equation}

The embedding operations required to compute the output of this layer involve $\ell \cdot (4d + 2) + 2d$ memory accesses and $\ell \cdot 2d$ summations.

\paragraph{Attention}

The standard Attention Mechanism~\cite{transformers} allows models to selectively focus on the most relevant parts of an input sequence.%, significantly enhancing predictive accuracy. 
Initially designed for machine translation, attention assigns varying "weights" to tokens based on their relevance to the task or context, enabling models to capture dependencies between distant words.

The self-attention variant computes relationships within a sequence by enabling each token to attend to all others, creating contextualized representations that encode both local and global dependencies. The input is processed through three fully connected layers to produce the Query, Key, and Value matrices, each with size \(d^2\). 
This step generates an activation size of \(4 \cdot d \cdot \ell\) and requires: i) \(6\ell \cdot d^2\) memory accesses, ii) \(3\ell \cdot d^2\) summations, and iii) \(3\ell \cdot d^2\) multiplications.

The Query and Key matrices are then multiplied to compute a Weight matrix of size \(\ell \times \ell\) for each of the \(h\) attention heads.
Due to the quadratic growth in the Weight matrix, the context length has a significant impact on activation size, which increases to \(3 d \cdot \ell + \ell^2 \cdot h\). This step also adds: i) \(2\ell^2 \cdot d \cdot h\) memory accesses, ii) \(\ell^2 \cdot d \cdot h\) summations, and iii) \(\ell^2 \cdot d \cdot h\) multiplications.

The Weight matrix undergoes a softmax operation, contributing: i) \(2\ell^2 \cdot h\) memory accesses, ii) \(\ell^2 \cdot h\) summations, and iii) \(3\ell^2 \cdot h\) multiplications,
without increasing activation size. 

Next, the Weighted matrix is multiplied by the Value matrix, producing an output activation size of \(2 d \cdot \ell + \ell^2 \cdot h\). This step introduces i) \(2\ell^2 \cdot d \cdot h\) memory accesses, ii) \(\ell^2 \cdot d \cdot h\) summations, and iii) \(\ell^2 \cdot d \cdot h\) multiplications.

Finally, the output passes through a fully connected layer of size \(d^2 + d\) with a skip connection. Although the activation size remains unchanged, this stage involves: i) \(\ell \cdot d^2 \cdot 2\) memory accesses, ii) \(\ell \cdot d^2 + \ell \cdot d\) summations, and iii) \(\ell \cdot d^2\) multiplications.

%Overall, self-attention structured weighting of sequence elements enables robust contextual representations, ensuring tokens are influenced by all relevant inputs regardless of distance.
\begin{figure}[t]
    \centerline{
        \includegraphics[height=.30\textheight,keepaspectratio]{embedders.pdf}
    }
    \caption{Memory layout of the Standard Embedder (left) and Nano Embedder (right) layers. $\ell$ is the sentence length, $d$ is the embedding dimension, and the trapezoid labeled~\textit{fc} denotes fully connected layers. The dashed gray box highlights the operations requiring the maximum activation size. The Nano Embedder reduces the number of weights while maintaining a similar activation size.}
    \label{fig:embedders_memory}
\end{figure}

\begin{figure}[t]
    \centerline{
        \includegraphics[height=.5\textheight,keepaspectratio]{attentions.pdf}
    }
    \caption{Memory layout of the Attention (left) and Efficient Attention (right) layers. $\ell$ is the sentence length, $d$ is the embedding dimension, and the trapezoid labeled~\textit{fc} denotes fully connected layers. The dashed gray box highlights the operations requiring the maximum activation size. Efficient Attention significantly reduces activation size.}
    \label{fig:attentions_memory}
\end{figure}

\subsection{NanoBERT}

The NanoBERT model, introduced in~\cite{NanoBERT}, is an ultra-compact version of BERT, designed for efficient operation on resource-constrained devices. 
It achieves this by employing techniques such as word embedding factorization and Low-Rank Adaptation (LoRA). 
A key component of its efficiency is the Nano Embedder (shown in Fig.~\ref{fig:embedders_memory}), which serves the same purpose as the standard Embedder but introduces a critical optimization: instead of embedding tokens and positions directly into vectors of size~$d$, it maps these inputs to a reduced embedding dimension~$r_d$ using a fully connected layer. 
This reduced embedding is then projected back to the original dimension~$d$ through a fully connected layer. Segment embeddings are excluded from this process. %, as adding a projection matrix for them would negate the parameter savings achieved through dimensionality reduction.

This approach modifies the parameter count to:
\begin{equation}
    W_{nemb} = r_d \cdot (v + \ell + 2 d) + 2 d,
\end{equation}
which can be lower than that of the Standard Embedder if  $r_d$ is sufficiently small relative to $d$. 
However, the total activation size, $A_{nemb}$, increases slightly due to the projection step, resulting in:
\begin{equation}
    A_{nemb} = r_d \cdot \ell + 2 d \cdot \ell.
\end{equation}

During inference, the Nano Embedder performs the following operations: i) $\ell + 2\ell \cdot r_d$ memory accesses for token and position encoding, followed by ii) $4\ell \cdot r_d \cdot d$ memory accesses, iii) $2\ell \cdot r_d \cdot d + \ell \cdot d$ summations, and iv) $\ell \cdot r_d \cdot d \cdot 2$ multiplications for the linear layers; and if segment embeddings are needed another \(\ell + d \cdot + \ell \cdot d \) memory accesses and \(\ell \cdot d\) summations.

This balance of parameter efficiency with a slight increase in activation memory illustrates the Nano Embedder ability to reduce the overall model size while maintaining embedding functionality. It is particularly advantageous in resource-constrained scenarios or when prioritizing higher dictionary sizes for improved performance.

\subsection{BERT Efficient}
In~\cite{efficient_attention}, the authors introduce Efficient Attention, an approach to effectively halve the parameter count of the Standard Attention mechanism. Instead of using three fully connected layers at the beginning of the layer and one at the output, Efficient Attention employs only one fully connected layer to generate the Query matrix. The Key and Value matrices are directly taken as the input. A single fully connected layer processes the attention output - both fully connected layers have dimensions~\(d^2\).

The total number of parameters, $W_{eatt}$, for the Efficient Attention layers is calculated as:
\begin{equation}
    W_{eatt} = 2 d ^ 2
\end{equation}

Like the Standard Attention layer, the Efficient Attention layer’s highest activation size occurs during the matrix multiplication step. This step produces an activation size of:
\begin{equation}
    A_{eatt} = 2\ell \cdot d + \ell^2
\end{equation}

To calculate the operations and memory accesses required, the expressions from the Standard Attention layer can be simplified by omitting terms associated with the two additional fully connected layers, as well as the $h$ (attention heads) term from all equations (see Table~\ref{table:complexity_appendix}). 
As shown in \cite{efficient_attention}, these modifications are such that not hinder the effective modeling capabilities of the attention layer. It retains similar contextual performance while significantly reducing computational cost, making it highly advantageous for resource-constrained scenarios.

\begin{table}[t]
    \caption{Formulas for calculating weights and activation sizes per layer, based on their architectural parameters.}
    \begin{center}
        \begin{tabular}{|c | c c|}
        \hline
        \textbf{Layers} & \textbf{Weights} & \textbf{Activations} \\
        \hline \hline
        \textbf{Embedder}               & $(v + \ell +2)\cdot d$ 
                                        & $2 d\cdot \ell$ \\
        \textbf{NanoEmbedder}           & \(r_d \cdot (v + \ell + 2 d ) + 2 d\)
                                        & \(r_d \cdot \ell + 2 d \cdot \ell\) \\
        \textbf{Normalization}          & \(2 d\)
                                        & \(2 d \cdot \ell\) \\
        \textbf{Feed Forward}           & \(2 d^2 \cdot \alpha \) 
                                        & \(d \cdot \ell \cdot ( 2 \alpha )\)\\
        \textbf{Attention}              & \(4 d^2 \)
                                        & \(4 d \cdot \ell + \ell^2 \cdot h\)\\
        \textbf{Efficient Attention}    & \(2 d^2 \) 
                                        & \(2 d \cdot \ell + \ell^2 \)\\
        \textbf{Eff Attention + Conv Skip} & \(2 d^2 + d^2 \cdot \alpha + c \cdot d \alpha\)
                                        & \(\max ( 2 d \cdot \ell + \ell^2 ; \  d \cdot \ell (2 + \alpha )) \)\\
        %\textbf{MAMBA layer}            & \(i \cdot (3d + c + 2 + 3d_s + 2\rho)\)
        %                                & \( \ell \cdot (d + 3i + 2i \cdot d_s + d_s) + \max (i \cdot d_s; \ \ell \cdot d_s)  \)\\
        \hline
        \end{tabular}
    \end{center}
    \label{table:w&a_noMAMBA_full}
\end{table}


\begin{table}[ht]
    \caption{Formulas for calculating memory accesses, summations and multiplication performed by each layer, based on their architectural parameters.}
    \centering
    \resizebox{\textwidth}{!}{
        \begin{tabular}{|c | c c c|}
        \hline
        \textbf{Layers} & \textbf{Memory accesses} & \textbf{Summations} & \textbf{Multiplications} \\
        \hline \hline
                        
        \textbf{Embedder}               & \(\ell \cdot (4d + 2) + 2d\)  
                                        & \(\ell \cdot 2d\) 
                                        & 0   \\
        
        \textbf{NanoEmbedder}           & \(\ell \cdot (r_d \cdot 4d + d + 2r_d + 2) + 2d\)    
                                        & \(2 \ell \cdot d \cdot (r_d + 1)\)
                                        & \(\ell \cdot r_d \cdot d \cdot 2\) \\ 

        \textbf{Normalization}          & \((\ell+1) \cdot d \cdot 2\)    
                                        & \(\ell \cdot d\)
                                        & \(\ell \cdot d\)\\
                                        
        \textbf{Feed Forward}           & \(4 \ell \cdot d \cdot (d \cdot \alpha + 1)\)
                                        & \(2 \ell \cdot d \cdot (d \cdot \alpha + 1)\)
                                        & \(2 \ell \cdot d \cdot (d \cdot \alpha + 1)\) \\
        
        \textbf{Attention}              & \(8\ell \cdot d^2 + 2 \ell^2 \cdot h \cdot (2d + 1)\)
                                        & \(\ell \cdot d \cdot (4d + 1) + \ell^2 \cdot h \cdot (2d + 1)\)
                                        & \(4\ell \cdot d^2 + \ell^2 \cdot h (2d + 3)\)\\
        
        \textbf{Efficient Attention}    & \(4\ell \cdot d^2 + 4\ell^2 \cdot d + 2\ell^2\)    
                                        & \(2\ell \cdot d^2 + 2\ell^2 \cdot d + \ell^2 + \ell \cdot d \)
                                        & \(2\ell \cdot d^2 + 2\ell^2 \cdot d + 2\ell^2\)\\        
                                        
        \textbf{Eff Diff Skip Attention}& \(\ell \cdot d (4d + 4 \ell +2d\cdot \alpha + k + 4) +2\ell^2\) 
                                        & \(\ell \cdot d (2d + 2\ell + d \cdot \alpha + k + 3) + \ell^2\)
                                        & \(\ell \cdot d (2d + 2\ell + d \cdot \alpha + k + 7) + \ell^2\) \\
                                        
        \textbf{MAMBA main}             & \(\ell \cdot i \cdot (d \cdot 6 + 9) + \ell \cdot d + c + SSM\)     
                                        & \(\ell \cdot i \cdot (d \cdot 3 + 2 + c) + \ell \cdot d + SSM\)     
                                        & \(\ell \cdot i \cdot (d \cdot 3 + 5 + c) + SSM\) \\

        \textbf{SSM}                    & \(\ell \cdot i \cdot (d_s \cdot 18 + \rho \cdot 4 + 8) + i \)
                                        & \(\ell \cdot i \cdot (d_s \cdot 4 + \rho \cdot 2 + 2)\)
                                        & \(\ell \cdot i \cdot (d_s \cdot 7 + \rho \cdot 2 + 2)\) \\
        \hline
        \end{tabular}
    }
    \label{table:complexity_appendix}
\end{table}

\subsection{EmbBERT-Q}
%Since our model, EmbBERT, begins with an adapted version of the Nano Embedder, 
This section focuses on analyzing the memory and computational costs of the Efficient Encoder block of EmbBERT-Q. Its first path consists in Efficient Attention, so we consider its weight count and activation size, and introduce the modifications due to the Convolutional Skip Connection and the weighted sum mechanisms.
For the weights, we include the contributions from the Convolutional and Fully Connected layers. However, the four vectors used for the weighted sum during training do not contribute to the final memory footprint, as they can be discarded and replaced by two weights computed at runtime. This results in:
\begin{equation}
    W_{EffEnc} = 2 d^2 + d^2 \cdot \alpha + k \cdot d \cdot \alpha   
\end{equation}

For the activations, we only need to consider the maximum between those resulting from the Efficient Attention and those from the Convolutional Skip component. The new component requires an activation size of at most \(d \cdot \ell (2 + \alpha)\), which arises from the processing of the fully connected layer and the attention result that must be retained in memory. This results in a total activation size of:
%For what concerns the activations instead we just need to take the max between the one resulting from the Efficient Attention and the new section. This new component requires at most an activation size of \(d \cdot \ell (2 + \alpha)\) due to the processing of the fully connected layer and the result of the attention which needs to be kept in memory thus resulting in a total activation of:
\begin{equation}
    A_{EffEnc} = \max ( 2 d \cdot \ell + \ell^2 ; \  d \cdot \ell (2 + \alpha ))
\end{equation}

For the computational complexity, we start with the operations required by the Efficient Attention and add those introduced by the Convolutional Skip Connection. The convolution step requires \(d \cdot l \cdot (k + 1)\) memory accesses, along with \(d \cdot l \cdot k\) sums and multiplications. 

Next, the SiLU activation requires~\(d \cdot l\) memory accesses, approximately \(d \cdot l\) summations, and \(4 d \cdot l \) multiplications. 

The fully connected layer introduces~\(2 d^2 \cdot \alpha \cdot \ell \) memory accesses, along with half as many summations and multiplications~(\(d^2 \cdot \alpha \cdot \ell \)).

Finally, the aggregation step requires~\(2 d \cdot \ell\) multiplications and memory accesses, with only half as many summations.

\subsection{Other blocks}
In this section, we synthetically review the two main other blocks required for a complete analysis of the Transformer/BERT architecture.

\paragraph{Feed Forward block}
This block, typically appended to the Attention or Efficient Attention layers after a normalization step, consists of two fully connected layers with size \(d^2 \cdot \alpha\). 
These layers sequentially increase/decrease the embedding dimension by a factor of\(\alpha\) with an activation function applied in between, followed by a skip connection. The total parameter count for this block is:
\begin{equation}
    W_{ff} = 2d^2 \cdot \alpha
\end{equation}
and activation size of: 
\begin{equation}
    A_{ff} = 2 \ell \cdot d + \ell \cdot d \cdot \alpha.
\end{equation}
Overall, the Feed Forward block requires~\(4 \ell \cdot d \cdot (d \cdot \alpha + 1)\) 
memory accesses and half as many summations and multiplications, with the majority of these operations required by the matrix multiplications in the fully connected layers.

\paragraph{Normalization Layer}
The normalization layer performs simple layer-wise normalization, requiring~\(2 \cdot d\) 
parameters to store the mean and variance values. During inference, it performs~\((\ell+1) \cdot d \cdot 2\) memory accesses and~\(\ell \cdot d\) 
summations and multiplications. The required activation size is~\(\ell \cdot d \cdot 2\), which is negligible compared to the activation sizes of the more computationally intensive layers.

% \subsection{TinyNLP Benchmark Analysis}
% The TinyNLP benchmark results in Table~\ref{table:all_results_tinyNLP} show the models' adaptability to diverse NLP tasks, particularly in lightweight and resource-constrained settings. We now go through an overview of the obtained results:

% \paragraph{Baseline Models}
% BERT-2MB achieves an average accuracy of 83.93, with its highest performance on Snips (97.00). However, it consistently underperforms on datasets like IMDb (79.38) and LiMiT (74.72). MAMBA-2MB in the same fashion doesn't manage to achieve high results with an average accuracy of 81.69 due to lack in performance mostly on Emotion and nlu datasets.

% \paragraph{Embedder Variants}
% Embedder and Embedder+Conv perform competitively, with average accuracies of 86.41 and 86.50, respectively. These models despite not being capable of high token interaction excel in some simpler datasets such as Emotion (89.40 and 89.45) and Snips (97.93 and 97.75).

% \paragraph{BERT with NanoEmbedder and Efficient Attention}
% The BERT model with the Nano Embedder manages to achieve the lowest average results (77.08) between all the comparisons, probabily due to it's inhability to leverage the more expanded dictionary that comes with the Nano Embedder. The BERT with Efficient Attention, with it's average accuracy of 85.64, while still not coming close to the results of the simple embedder only model sill shows a noticeable improvement with respect to the simple BERT-2MB model.
% BERT+NE+EA achieves an average score of 87.04, with high accuracy on datasets like IMDb (83.19) and Emotion (88.70) thus managing to leverage well the combined abilities of the Efficient Attention and the expanded dictionary obtained thanks to the Nano Embedder. It's quantized variant, BERT+NE+EA (8bit), achieves a score of 85.92, suggesting good enough robustness to quantization but achieving still results not higher than the simple embedder ones.

% \paragraph{EmbBERT}
% EmbBERT outperforms its competitors, achieving an average score of 87.19. It demonstrates consistent strength across all datasets, with notable success on Snips (97.67) and Emotion (89.58). This highlights its capability for nuanced intent recognition and sentiment analysis.
% The quantized variant, EmbBERT(8bit), achieves the highest average score of 88.17, excelling in Cyberbullying (86.60) and NLU (94.05) demonstrating it's high resilience to quantization and further ability to improve performances thanks to the final parameter efficient finetuning.


% \subsection{GLUE Benchmark Analysis}
% The GLUE benchmark results, summarized in Table~\ref{table:res_glue_comp}, reveal significant variability in performance across tasks, reflecting the diverse challenges posed by the benchmark datasets. Key observations are as follows:

% \paragraph{Baseline Models}
% BERT-2MB demonstrates the lowest overall performance between the baseline models, with a GLUE score of 52.10. Its limitations are particularly evident in tasks such as CoLA (-0.86) and STSB (15.48).
% MAMBA-2MB shows moderate improvement over BERT-2MB, achieving a GLUE score of 55.03. However, it still struggles with semantic similarity tasks like STSB (10.16) mostly due to it's architecture.

% \paragraph{Embedder Variants}
% Embedder and Embedder+Conv both outperform the baseline models, with scores of 56.92 and 56.41, respectively. These models given their simplicity should be the lowest acceptable results for models with more complex and cabable architectures. Thei manage to excel particularly QQP (83.28 and 82.98), although their performance on CoLA (9.65 and 9.25) remains suboptimal.
% The addition of convolutional layers in Embedder+Conv slightly enhances its performance in some tasks, such as WNLI (79.16), but does not consistently outperform the Embedder model.

% \paragraph{BERT with NanoEmbedder and Efficient Attention}
% BERT+NE, BERT+EA, and their combination (BERT+NE+EA) show varying levels of success. While BERT + NE, contrary to it's tinyNLP performances, barely manages to surpass the Embedder and Embedder+Conv results with 57.50, BERT + EA struggles to emerge due to it's subpar performances on a variety of datasets.
% While BERT+NE+EA achieves a competitive score of 61.20, its 8-bit quantized variant exhibits a marked performance drop (45.97), particularly in challenging tasks like MNLI-m and MNLI-mm.

% \paragraph{EmbBERT}
% Our model, EmbBERT, achieves the highest GLUE score of 63.50, reflecting its robust performance across most tasks. Notably, it excels in CoLA (11.01), MRPC (69.19), and MNLI tasks (67.83 and 68.63), demonstrating superior adaptability to syntactic and semantic challenges.
% The 8-bit quantized variant, EmbBERT(8bit), maintains competitive performance with a score of 62.81 even surpassing the unquantized results of BERT+NE+EA thus showing it's resilience to quantization even in complex tasks.

\begin{thebibliography}{10}
\expandafter\ifx\csname url\endcsname\relax
  \def\url#1{\texttt{#1}}\fi
\expandafter\ifx\csname urlprefix\endcsname\relax\def\urlprefix{URL }\fi
\expandafter\ifx\csname href\endcsname\relax
  \def\href#1#2{#2} \def\path#1{#1}\fi

\bibitem{iot}
H.~Kopetz, W.~Steiner,
  \href{https://doi.org/10.1007/978-3-031-11992-7_13}{Internet of things}
  (2022).
\newblock \href {https://doi.org/10.1007/978-3-031-11992-7_13}
  {\path{doi:10.1007/978-3-031-11992-7_13}}.
\newline\urlprefix\url{https://doi.org/10.1007/978-3-031-11992-7_13}

\bibitem{wearables}
A.~Ometov, V.~Shubina, L.~Klus, J.~Skibi{\'n}ska, S.~Saafi, P.~Pascacio,
  L.~Flueratoru, D.~Q. Gaibor, N.~Chukhno, O.~Chukhno, et~al., A survey on
  wearable technology: History, state-of-the-art and current challenges,
  Computer Networks 193 (2021) 108074.

\bibitem{MCUNet}
J.~Lin, W.-M. Chen, Y.~Lin, J.~Cohn, C.~Gan, S.~Han,
  \href{https://arxiv.org/abs/2007.10319}{Mcunet: Tiny deep learning on iot
  devices}, arXiv:2007.10319 [cs] (11 2020).
\newline\urlprefix\url{https://arxiv.org/abs/2007.10319}

\bibitem{Eff_NN_for_embedded}
\href{https://arxiv.org/html/2001.03048v3}{Resource-efficient neural networks
  for embedded systems} (2024).
\newline\urlprefix\url{https://arxiv.org/html/2001.03048v3}

\bibitem{tinyML}
J.~Lin, L.~Zhu, W.-M. Chen, W.-C. Wang, S.~Han, Tiny machine learning: progress
  and futures [feature], IEEE Circuits and Systems Magazine 23~(3) (2023)
  8--34.

\bibitem{on_device_kw_spotting}
C.~Cioflan, L.~Cavigelli, M.~Rusci, d.~Prado, L.~Benini,
  \href{https://arxiv.org/abs/2403.10549}{On-device domain learning for keyword
  spotting on low-power extreme edge embedded systems} (2024).
\newline\urlprefix\url{https://arxiv.org/abs/2403.10549}

\bibitem{customizable_kw_spotting}
M.~Rusci, T.~Tuytelaars,
  \href{https://ieeexplore.ieee.org/abstract/document/10241972}{On-device
  customization of tiny deep learning models for keyword spotting with few
  examples}, IEEE Micro 43 (2023) 50--57.
\newblock \href {https://doi.org/10.1109/mm.2023.3311826}
  {\path{doi:10.1109/mm.2023.3311826}}.
\newline\urlprefix\url{https://ieeexplore.ieee.org/abstract/document/10241972}

\bibitem{mobilenet_v2}
M.~Sandler, A.~Howard, M.~Zhu, A.~Zhmoginov, L.-C. Chen,
  \href{https://arxiv.org/abs/1801.04381}{Mobilenetv2: Inverted residuals and
  linear bottlenecks} (2018).
\newline\urlprefix\url{https://arxiv.org/abs/1801.04381}

\bibitem{micro_net_img_rec}
Y.~Li, Y.~Chen, X.~Dai, D.~Chen, M.~Liu, L.~Yuan, Z.~Liu, L.~Zhang,
  N.~Vasconcelos, \href{https://arxiv.org/abs/2011.12289}{Micronet: Towards
  image recognition with extremely low flops} (2020).
\newline\urlprefix\url{https://arxiv.org/abs/2011.12289}

\bibitem{EfficientDet_obj_det}
M.~Tan, R.~Pang, Q.~V. Le,
  \href{https://arxiv.org/abs/1911.09070}{Efficientdet: Scalable and efficient
  object detection}, arXiv:1911.09070 [cs, eess] (07 2020).
\newline\urlprefix\url{https://arxiv.org/abs/1911.09070}

\bibitem{BERT}
J.~Devlin, M.-W. Chang, K.~Lee, K.~Toutanova,
  \href{https://arxiv.org/abs/1810.04805}{Bert: Pre-training of deep
  bidirectional transformers for language understanding} (10 2018).
\newline\urlprefix\url{https://arxiv.org/abs/1810.04805}

\bibitem{XLNet}
Z.~Yang, Z.~Dai, Y.~Yang, J.~Carbonell, R.~Salakhutdinov, Q.~V. Le,
  \href{https://arxiv.org/abs/1906.08237}{Xlnet: Generalized autoregressive
  pretraining for language understanding} (2019).
\newline\urlprefix\url{https://arxiv.org/abs/1906.08237}

\bibitem{DistilBERT}
V.~Sanh, L.~Debut, J.~Chaumond, T.~Wolf,
  \href{https://arxiv.org/abs/1910.01108}{Distilbert, a distilled version of
  bert: smaller, faster, cheaper and lighter} (2019).
\newline\urlprefix\url{https://arxiv.org/abs/1910.01108}

\bibitem{SpanBERT}
M.~Joshi, D.~Chen, Y.~Liu, D.~S. Weld, L.~Zettlemoyer, O.~Levy,
  \href{https://arxiv.org/abs/1907.10529}{Spanbert: Improving pre-training by
  representing and predicting spans}, arXiv:1907.10529 [cs] (01 2020).
\newline\urlprefix\url{https://arxiv.org/abs/1907.10529}

\bibitem{ALBERT}
Z.~Lan, M.~Chen, S.~Goodman, K.~Gimpel, P.~Sharma, R.~Soricut,
  \href{https://arxiv.org/abs/1909.11942}{Albert: A lite bert for
  self-supervised learning of language representations}, arXiv:1909.11942 [cs]
  (02 2020).
\newline\urlprefix\url{https://arxiv.org/abs/1909.11942}

\bibitem{RoBERTa}
Y.~Liu, M.~Ott, N.~Goyal, J.~Du, M.~Joshi, D.~Chen, O.~Levy, M.~Lewis,
  L.~Zettlemoyer, V.~Stoyanov, \href{https://arxiv.org/abs/1907.11692}{Roberta:
  A robustly optimized bert pretraining approach} (07 2019).
\newline\urlprefix\url{https://arxiv.org/abs/1907.11692}

\bibitem{MAMBA}
A.~Gu, T.~Dao, \href{https://arxiv.org/abs/2312.00752}{Mamba: Linear-time
  sequence modeling with selective state spaces} (12 2023).
\newblock \href {https://doi.org/10.48550/arXiv.2312.00752}
  {\path{doi:10.48550/arXiv.2312.00752}}.
\newline\urlprefix\url{https://arxiv.org/abs/2312.00752}

\bibitem{MobileBERT}
Z.~Sun, H.~Yu, X.~Song, R.~Liu, Y.~Yang, D.~Zhou,
  \href{http://arxiv.org/abs/2004.02984}{Mobilebert: a compact task-agnostic
  bert for resource-limited devices} (2020).
\newline\urlprefix\url{http://arxiv.org/abs/2004.02984}

\bibitem{transformers_compression}
Y.~Tang, Y.~Wang, J.~Guo, Z.~Tu, K.~Han, H.~Hu, D.~Tao,
  \href{https://arxiv.org/abs/2402.05964}{A survey on transformer compression}
  (04 2024).
\newblock \href {https://doi.org/10.48550/arXiv.2402.05964}
  {\path{doi:10.48550/arXiv.2402.05964}}.
\newline\urlprefix\url{https://arxiv.org/abs/2402.05964}

\bibitem{tinyBERT}
I.~Turc, M.-W. Chang, K.~Lee, K.~Toutanova, Well-read students learn better: On
  the importance of pre-training compact models, arXiv preprint
  arXiv:1908.08962 (2019).

\bibitem{NanoBERT}
K.~Maity, A.~T. Chaulwar, V.~Vala, R.~S. Guntur, Nanobert: An extremely compact
  language model, in: Proceedings of the 7th Joint International Conference on
  Data Science \& Management of Data (11th ACM IKDD CODS and 29th COMAD), 2024,
  pp. 342--349.

\bibitem{ConvBERT}
Z.~Jiang, W.~Yu, D.~Zhou, Y.~Chen, J.~Feng, S.~Yan, Convbert: Improving bert
  with span-based dynamic convolution, arXiv (Cornell University) (08 2020).
\newblock \href {https://doi.org/10.48550/arxiv.2008.02496}
  {\path{doi:10.48550/arxiv.2008.02496}}.

\bibitem{i-BERT}
S.~Kim, A.~Gholaminejad, Z.~Yao, M.~Mahoney, E.~K. Keutzer, I-bert:
  Integer-only bert quantization, arXiv (Cornell University) (01 2021).
\newblock \href {https://doi.org/10.48550/arxiv.2101.01321}
  {\path{doi:10.48550/arxiv.2101.01321}}.

\bibitem{ELECTRA}
K.~Clark, M.-T. Luong, Q.~V. Le, C.~D. Manning,
  \href{http://arxiv.org/abs/2003.10555}{Electra: Pre-training text encoders as
  discriminators rather than generators} (2020).
\newline\urlprefix\url{http://arxiv.org/abs/2003.10555}

\bibitem{LoRA}
E.~J. Hu, Y.~Shen, P.~Wallis, Z.~Allen-Zhu, Y.~Li, S.~Wang, L.~Wang, W.~Chen,
  \href{https://arxiv.org/abs/2106.09685}{Lora: Low-rank adaptation of large
  language models}, arXiv:2106.09685 [cs] (10 2021).
\newline\urlprefix\url{https://arxiv.org/abs/2106.09685}

\bibitem{data_augmentation}
X.~Liu, Y.~Zheng, Z.~Du, M.~Ding, Y.~Qian, Z.~Yang, J.~Tang,
  \href{http://arxiv.org/abs/2103.10385}{Gpt understands, too} (2021).
\newline\urlprefix\url{http://arxiv.org/abs/2103.10385}

\bibitem{synthetic_data}
Y.~Lu, M.~Shen, H.~Wang, X.~Wang, C.~van Rechem, W.~Wei, Machine learning for
  synthetic data generation: a review, arXiv preprint arXiv:2302.04062 (2023).

\bibitem{quantization}
A.~Gholami, S.~Kim, Z.~Dong, Z.~Yao, M.~W. Mahoney, K.~Keutzer, A survey of
  quantization methods for efficient neural network inference, in: Low-Power
  Computer Vision, Chapman and Hall/CRC, 2022, pp. 291--326.

\bibitem{knowledge_distillation}
G.~Hinton, Distilling the knowledge in a neural network, arXiv preprint
  arXiv:1503.02531 (2015).

\bibitem{qlora}
T.~Dettmers, A.~Pagnoni, A.~Holtzman, L.~Zettlemoyer, Qlora: Efficient
  finetuning of quantized llms, Advances in Neural Information Processing
  Systems 36 (2024).

\bibitem{exploring_transformer_sizes}
C.~Fields, C.~Kennington,
  \href{https://aclanthology.org/2023.conll-1.35/}{Exploring transformers as
  compact, data-efficient language models} (12 2023).
\newblock \href {https://doi.org/10.18653/v1/2023.conll-1.35}
  {\path{doi:10.18653/v1/2023.conll-1.35}}.
\newline\urlprefix\url{https://aclanthology.org/2023.conll-1.35/}

\bibitem{bitsandbytes}
HuggingFace,
  \href{https://github.com/bitsandbytes-foundation/bitsandbytes/tree/main}{Bitsandbytes}
  (2024).
\newline\urlprefix\url{https://github.com/bitsandbytes-foundation/bitsandbytes/tree/main}

\bibitem{bookcorpus}
Y.~Zhu, R.~Kiros, R.~Zemel, R.~Salakhutdinov, R.~Urtasun, A.~Torralba,
  S.~Fidler, Aligning books and movies: Towards story-like visual explanations
  by watching movies and reading books, in: The IEEE International Conference
  on Computer Vision (ICCV), 2015, pp.~--.

\bibitem{GLUE}
A.~Wang, A.~Singh, J.~Michael, F.~Hill, O.~Levy, S.~R. Bowman,
  \href{https://arxiv.org/abs/1804.07461}{Glue: A multi-task benchmark and
  analysis platform for natural language understanding}, arXiv:1804.07461 [cs]
  (02 2019).
\newline\urlprefix\url{https://arxiv.org/abs/1804.07461}

\bibitem{nlu_dataset}
P.~S. Xingkun~Liu, Arash~Eshghi, V.~Rieser,
  \href{http://www.xx.xx/xx/}{Benchmarking natural language understanding
  services for building conversational agents}, in: Proceedings of the Tenth
  International Workshop on Spoken Dialogue Systems Technology (IWSDS),
  Springer, Ortigia, Siracusa (SR), Italy, 2019, pp. xxx--xxx.
\newline\urlprefix\url{http://www.xx.xx/xx/}

\bibitem{imdb_dataset}
A.~L. Maas, R.~E. Daly, P.~T. Pham, D.~Huang, A.~Y. Ng, C.~Potts,
  \href{http://www.aclweb.org/anthology/P11-1015}{Learning word vectors for
  sentiment analysis}, in: Proceedings of the 49th Annual Meeting of the
  Association for Computational Linguistics: Human Language Technologies,
  Association for Computational Linguistics, Portland, Oregon, USA, 2011, pp.
  142--150.
\newline\urlprefix\url{http://www.aclweb.org/anthology/P11-1015}

\bibitem{emotion_dataset}
E.~Saravia, H.-C.~T. Liu, Y.-H. Huang, J.~Wu, Y.-S. Chen,
  \href{https://www.aclweb.org/anthology/D18-1404}{{CARER}: Contextualized
  affect representations for emotion recognition}, in: Proceedings of the 2018
  Conference on Empirical Methods in Natural Language Processing, Association
  for Computational Linguistics, Brussels, Belgium, 2018, pp. 3687--3697.
\newblock \href {https://doi.org/10.18653/v1/D18-1404}
  {\path{doi:10.18653/v1/D18-1404}}.
\newline\urlprefix\url{https://www.aclweb.org/anthology/D18-1404}

\bibitem{ag_news_dataset}
X.~Zhang, J.~J. Zhao, Y.~LeCun, Character-level convolutional networks for text
  classification, in: NIPS, 2015, pp. 0--1.

\bibitem{cyberbull_dataset}
J.~Wang, K.~Fu, C.-T. Lu,
  \href{https://people.cs.vt.edu/ctlu/Publication/2020/IEEE-BD-SOSNet-Wang.pdf}{Sosnet:
  A graph convolutional network approach to fine-grained cyberbullying
  detection} (2020).
\newline\urlprefix\url{https://people.cs.vt.edu/ctlu/Publication/2020/IEEE-BD-SOSNet-Wang.pdf}

\bibitem{limit_dataset}
I.~Manotas, N.~P.~A. Vo, V.~Sheinin,
  \href{https://www.aclweb.org/anthology/2020.findings-emnlp.88}{{L}i{M}i{T}:
  The literal motion in text dataset}, in: Findings of the Association for
  Computational Linguistics: EMNLP 2020, Association for Computational
  Linguistics, Online, 2020, pp. 991--1000.
\newblock \href {https://doi.org/10.18653/v1/2020.findings-emnlp.88}
  {\path{doi:10.18653/v1/2020.findings-emnlp.88}}.
\newline\urlprefix\url{https://www.aclweb.org/anthology/2020.findings-emnlp.88}

\bibitem{efficient_attention}
M.~Hosseini, P.~Hosseini, \href{http://arxiv.org/abs/2403.01643}{You need to
  pay better attention: Rethinking the mathematics of attention mechanism}
  (2024).
\newline\urlprefix\url{http://arxiv.org/abs/2403.01643}

\bibitem{transformers}
A.~Vaswani, Attention is all you need, Advances in Neural Information
  Processing Systems (2017).

\end{thebibliography}


\end{document}






















