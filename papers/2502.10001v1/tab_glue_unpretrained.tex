

\begin{table*}[htbp]
    \caption{Evauation of non-pretrained models on the GLUE benchmark. We report SCC for STSB, MCC for CoLA, F1~score for QQP and MRPC, Accuracy for the remaining GLUE tasks.}
    \begin{center}
    \resizebox{\textwidth}{!}{
        \begin{tabular}{| c | c c c c c c c c c c | c |}
        \hline
        \textbf{Models} & COLA & SST-2 & MRPC & QQP & MNLI-m & MNLI-mm & QNLI & RTE & WNLI & STSB & Score \\
        \hline
        \textbf{BERT(2MB)}           & 3,93 & 75,20 & 63,88 & 81,03 & 64,95 & 63,65 & 62,75 & 49,38 & 70,68 & 6,74 & 54,22 \\
        \textbf{MAMBA(2MB)}          & 7,22 & 80,60 & 60,72 & 80,66 & 64,27 & 64,44 & 59,78 & 51,76 & 80,00 & 4,58 & 55,40 \\
        \textbf{Embedder}           & 9,65 & 78,90 & 62,25 & 83,28 & 62,06 & 62,17 & 65,40 & 52,73 & 77,20 & 15,58 & 56,92 \\
        \textbf{Embedder~+~Conv}      & 9,25 & 79,10 & 60,50 & 82,98 & 61,98 & 60,93 & 62,08 & 52,00 & 79,16 & 16,10 & 56,41 \\
        \textbf{BERT~+~NE}            & 5,22 & 77,34 & 63,18 & 81,12 & 64,14 & 64,53 & 66,76 & 51,14 & 85,62 & 4,72 & 56,38 \\
        \textbf{BERT~+~NE~+~EA}         & 2,66 & 78,68 & 61,90 & 83,16 & 62,65 & 61,63 & 62,36 & 48,38 & 85,62 & 8,94 & 55,60 \\
        \hline  
        \textbf{EmbBERT}            & 5,32 & 78,50 & 62,54 & 82,58 & 63,82 & 65,78 & 63,68 & 51,26 & 87,30 & 9,76 & 57,05 \\
        \hline
        \end{tabular}

    \label{table:res_glue_comp_unpretr}
    }
    \end{center}

    
\end{table*}






