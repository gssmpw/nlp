
\begin{table*}[tbp]
    \caption{Accuracy of non-pretrained models on the TinyNLP benchmark.}
    \begin{center}
    \resizebox{\textwidth}{!}{
        \begin{tabular}{| c | c c c c c c c c |}
        \hline
        \textbf{Models} & \href{https://huggingface.co/datasets/stanfordnlp/imdb}{\textbf{IMDb}} & \href{https://huggingface.co/datasets/fancyzhx/ag_news}{\textbf{ag\_news}} & \href{https://www.kaggle.com/datasets/andrewmvd/cyberbullying-classification}{\textbf{cyberbull}} & \href{https://huggingface.co/datasets/IBM/limit}{\textbf{LiMiT}} & \href{https://huggingface.co/datasets/dair-ai/emotion}{\textbf{Emotion}} & \href{https://huggingface.co/datasets/xingkunliuxtracta/nlu_evaluation_data}{\textbf{nlu}} & \href{https://huggingface.co/datasets/benayas/snips}{\textbf{Snips}} & \textbf{Average} \\
        \hline \hline
        \textbf{BERT(2MB)}           & 78,98 & 88,93 & 82,63 & 70,10 & 83,35 & 85,63 & 96,58 & 83,74 \\
        \textbf{MAMBA(2MB)}          & 78,18 & 91,08 & 83,74 & 71,66 & 77,40 & 87,60 & 97,34 & 83,86 \\
        \textbf{Embedder}           & 82,60 & 91,10 & 82,78 & 71,60 & 89,40 & 89,50 & \textbf{97,93} & 86,41 \\
        \textbf{Embedder~+~Conv}      & 84,08 & \textbf{91,50} & 83,10 & 70,32 & 89,45 & 89,33 & 97,75 & 86,50 \\
        \textbf{BERT~+~NE}            & 82,52 & 90,86 & 82,96 & 69,82 & 78,34 & 84,06 & 96,74 & 83,61 \\

        \textbf{BERT~+~NE~+~EA}         & 81,60 & 90,53 & 82,30 & 55,57 & 83,70 & 84,90 & 96,50 & 82,16 \\

        \hline
        \textbf{EmbBERT}            & 83,10 & 90,82 & 82,50 & 68,90 & 68,48 & 76,78 & 95,18 & 80,82 \\
        \hline
        \end{tabular}
        \label{table:all_results_tinyNLP_acc_unpretr}
        }
    \end{center} 
\end{table*}







