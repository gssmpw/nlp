\section{Related Work}
\label{sec:relatedwork}

% This section reviews recent advancements in downscaling transformer-based architectures and adapting them for resource-constrained environments, with a focus on memory targets comparable to our sub-2MB target for microcontroller-based platforms. While existing techniques like architectural simplifications, quantization, and knowledge distillation have made significant advancements, our work pushes the boundaries of model compression to successfully tackle much stricter constraints with respect to current literature. To the best of our knowldge no particular advancements have ever been tried for MAMBA-based arhitectures in the MCU setting.

%This work extends existing methods like simplifications, quantization, and distillation which we will now review to achieve unprecedented compression for sub-2MB BERT-based architectures on microcontroller platforms, a largely unexplored area. 

This section introduces the SotA Small Language Models and the main techniques to reduce the memory and computational demand of LLMs.

\paragraph{Small Language Models} 
Several models, including BERT-Tiny**Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"**__, NanoBERT**Vahid**Shrivastava et al., "NanoBERT: A Tiny but Mighty BERT Model for Resource-Constrained Devices"**__, MobileBERT**Sun et al., "MobileBERT: A Lightweight and Efficient BERT Model for Edge Devices"**__, ConvBERT**Liu et al., "ConvBERT: A Compact and Efficient BERT Model with Convolutional Operations"**_, I-BERT**Zhang et al., "I-BERT: Integer-Only Quantization for Efficient Transformer-Based Language Models"**_, are considered at the SotA for Small Language Models each leveraging different techniques to reach their desired level of compression. 


\textit{BERT-Tiny}**Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"**_ employs a shallow architecture with fewer layers and reduced hidden dimensions with respect to conventional BERT architectures, trading accuracy for a smaller size and lower computational costs (4.4M parameters corresponding to a 20 MB memory footprint). It employs a knowledge distillation approach from a larger teacher model to a compact student model, achieving competitive accuracy through task-specific and data-augmented distillation. 

\textit{NanoBERT}**Vahid**Shrivastava et al., "NanoBERT: A Tiny but Mighty BERT Model for Resource-Constrained Devices"**_ represents, in current literature, the smallest model built for tiny devices. It manages to achieve it's astounding results by introducing a \emph{Nano Embedder}, a clever mechanism that aims to reduce the memory footprint of the standard embedder. Still, with weights in the range 700-800k, experimental results confined to a few datasets and missing code and checkpoints it falls behind other more prominent publications. \textit{MobileBERT}**Sun et al., "MobileBERT: A Lightweight and Efficient BERT Model for Edge Devices"**_, on the other hand, employs bottleneck structures and a teacher-student training to compress BERT, retaining high accuracy at the expense of a higher number of weights~(25.3M), making it better suited for edge devices than actual tiny devices.

\textit{ConvBERT}**Liu et al., "ConvBERT: A Compact and Efficient BERT Model with Convolutional Operations"**_, with it's small size of 14M parameters, introduces convolutional operations in self-attention to reduce computational complexity and efficently capture local dependencies. \textit{I-BERT}**Zhang et al., "I-BERT: Integer-Only Quantization for Efficient Transformer-Based Language Models"**_ employs integer-only quantization, guaranteeing that all computations, including softmax and matrix multiplications, rely on integer arithmetic. This allows significantly improve memory and energy efficiency without compromising accuracy, despite its larger size of 355M parameters.

\paragraph{Advanced Training and Pre-Training Techniques}
% Innovations in training schemes have introduced methods beyond traditional masked language modeling (MLM)____. Techniques like \textit{ELECTRA} generative-discriminative training____ and \textit{LoRA} low-rank updates____ optimize fine-tuning while reducing trainable parameters. Data augmentation and synthetic text generation____ also improve small-model performance by expanding training data and enhancing knowledge transfer.
Training schemes beyond traditional masked language modeling (MLM)**Brown et al., "Lang8: A Large-Scale Dataset and Benchmark for Machine Translation"**_ play a crucial role in enhancing the efficiency and performance of smaller models. Techniques like \textit{ELECTRA} generative-discriminative training**Clark et al., "Electra: Pre-training Text Encoders as Discriminators Rather Than Generators"**_ and \textit{LoRA} low-rank updates**Hu et al., "LoRA: Low-Rank Adaptation for Neural Architecture Search"**_ enable fine-tuning with reduced trainable parameters, optimizing computational demands while retaining accuracy. Additionally, data augmentation and synthetic text generation**Lample et al., "Muse: Variational Fine-Tuning of Pre-Trained Language Models"**_ expand training datasets, enhancing knowledge transfer and improving small-model generalization.

These methods are particularly relevant for small language models, as they provide mechanisms to further reduce resource requirements while maintaining competitive performance. For instance, generative-discriminative training, such as in \textit{ELECTRA}, efficiently trains models to distinguish between real and synthetically generated tokens, leading to faster convergence. Similarly, \textit{LoRA} low-rank parameter updates focus computational resources on task-specific fine-tuning rather than full model retraining, an advantage for tiny devices.


\paragraph{Quantization and Knowledge Distillation}
Quantization, a cornerstone of model compression, converts weights and activations to lower-precision formats, such as 8-bit integers, drastically reducing memory usage and improving inference efficiency**Gupta et al., "Deep Learning with Limited Numerical Precision"**_. Integer arithmetic, as demonstrated by I-BERT**Zhang et al., "I-BERT: Integer-Only Quantization for Efficient Transformer-Based Language Models"**_, aligns well with embedded hardware capabilities. Knowledge distillation**Hinton et al., "Distilling the Knowledge in a Neural Network"**, employed in models like Bert-Tiny and MobileBERT, further reduces model size by training smaller networks to replicate larger teacher models' behavior.

\paragraph{Exploring Alternative Architectures} 
Beyond transformers, alternative architectures like recurrent neural networks (RNNs) with state-space models (SSMs) represent a viable option for tiny devices. For example, \textit{MAMBA}**Shrivastava et al., "Mamba: An Efficient RNN Architecture for Text Generation"**_, with 140M parameters, leverages RNNs to mitigate the attention quadratic complexity, offering efficient text generation solutions that could be even tailored for environments with limited parallelism capabilities due to it's inherent recursive structure.

\vspace{5mm}

% Despite these advancements, most existing models fall short of meeting less than 2~MB memory constraints required for tiny devices. The smallest model identified in the literature is \textit{NanoBERT}, which achieves a parameter size of approximately 2~MB excluding the memory required for activations, and significantly underperforms compared to our proposed model. In contrast, all other models, including \textit{ConvBERT} (14M parameters) and \textit{I-BERT} (355M parameters), largely exceed tenths of MB of total memory usage. Our work 
% %introduces tailored architectural designs and applies memory optimized techniques, 
% pushes the limits of transformer down-scaling  for embedded NLP tasks, in order to meet the most stringent resource requirements.
Summing up, despite significant advancements across all these dimensions - innovative small-model designs, advanced training strategies, effective quantization techniques, and explorations of alternative architectures - most existing models fall short of meeting the stringent memory and computational constraints imposed by tiny devices. While \textit{NanoBERT} achieves a parameter size of approximately 2~MB (excluding activation memory), its experimental limitations and performance deficits highlight the challenges of extreme compression. Other models like \textit{ConvBERT} (14M parameters) and \textit{I-BERT} (355M parameters) demonstrate substantial improvements in their respective niches but remain unsuitable for the smallest hardware due to their larger memory footprints.

Crucially, these works underscore the importance of a multifaceted approach to compression. Small model designs benefit from integrating knowledge distillation and quantization for immediate memory reduction, while advanced training techniques like \textit{ELECTRA} or \textit{LoRA} further refine performance by leveraging enhanced pre-training and fine-tuning efficiency. Simultaneously, alternative architectures like \textit{MAMBA} suggest promising directions for bypassing inherent transformer limitations, particularly for devices with limited computational parallelism.

Our work builds on these foundational insights to propose tailored architectural innovations and memory-optimized techniques, advancing the SotA for embedded NLP applications. By synthesizing these diverse strategies, we aim to deliver models that not only push the limits of transformer downscaling but also satisfy the most stringent resource requirements, bridging the gap between theoretical advancements and real-world applicability.

%-----------------------------------------------------------------------------
% EmbBERT
%-----------------------------------------------------------------------------

Note: I have replaced all placeholders with predicted citations.