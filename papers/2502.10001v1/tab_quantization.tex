\begin{table*}[htbp]
    \caption{Comparison on TinyNLP benchmark of original versus quantized models}
    \begin{center}    
    
        \begin{tabular}{| c | c  c  c  c  c  c  c | c |}
        \hline
        \textbf{Models}&
        \href{https://huggingface.co/datasets/stanfordnlp/imdb}{\textbf{IMDb}} &
        \href{https://huggingface.co/datasets/fancyzhx/ag_news}{\textbf{ag\_news}}  &
        \href{https://www.kaggle.com/datasets/andrewmvd/cyberbullying-classification}{\textbf{cyberbull}}  &
        \href{https://huggingface.co/datasets/IBM/limit}{\textbf{LiMiT}} &
        \href{https://huggingface.co/datasets/dair-ai/emotion}{\textbf{Emotion}} &
        \href{https://huggingface.co/datasets/xingkunliuxtracta/nlu_evaluation_data}{\textbf{nlu}} &
        \href{https://huggingface.co/datasets/benayas/snips}{\textbf{Snips}} &
        \textbf{Average} \\
        \hline\hline
        %BERT + NE + EA          & 84,00 & 91,02 & 85,28 & 78,22 & 88,86 & 87,78 & 97,44 & 87,51 \\
        %\hline
        %BERT + NE + EA (8bit)   & 82,35 & 89,51 & 85,58 & 76,40 & 80,85 & 89,46 & 97,29 & 85,92 \\
        
        EmbBERT          & 84,10 & 90,46 & 83,97 & 76,36 & 89,58 & 88,16 & 97,67 & 87,19 \\
        \hline 
        EmbBERT (8bit)      & 84,01 & 90,63 & 86,60 & 74,10 & 89,90 & 94,05 & 97,93 & 88,17 \\

        \hline
        \end{tabular}

    \label{table:res_tiny_quant}
    \end{center}
    
\end{table*}





\begin{table*}[htbp]
    \caption{Comparison on GLUE benchmark of original versus quantized models reporting for COLA the MCC, for MRPC and QQP the F1 score, for STSB the \gls{scc} and accuracy for the rest as required to officially calculate the GLUE overall model's score}
    \begin{center}
    
        \begin{tabular}{| c | c c c c c c c c c c | c |}
        \hline
        \textbf{Models}         & COLA      & SST-2     & MRPC      & QQP       & MNLI-m    & MNLI-mm   & QNLI      & RTE       & WNLI      & STSB      & Score \\
        \hline
        %BERT + NE + EA          & 10,57     & 77,29     & 60,89     & 80,91     & 64,58     & 64,19     & 63,77     & 55,23     & 77,46     & 16,32     & 57,12 \\
        %\hline
        %BERT + NE + EA (8bit)   & 9,57      & 77,87     & 63,40     & 48,93     & 34,19     & 33,59     & 59,58     & 53,79     & 59,15     & 19,58     & 45,97\\            
        \hline
        EmbBERT           & 11,01     & 79,33     & 69,19     & 83,25     & 67,83     & 68,63     & 68,92     & 49,96     & 87,61     & 49,25     & 63,50 \\
        \hline
        EmbBERT (8bit)     & 9,56      & 80,96     & 67,99     & 82,45     & 67,10     & 68,05     & 68,06     & 47,29    & 87,32      & 49,28     & 62,81 \\
        \hline
        \end{tabular}

    \label{table:res_glue_quant}
    \end{center}

    
\end{table*}











