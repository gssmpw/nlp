
\begin{table*}[htbp]
    \caption{\textbf{Architectural parameters} used for training model architectures. Columns represent: vocabulary size $v$, sentence length $\ell$, embedding dimension $d$, reduced embedding dimension $r_d$, forward expansion $\alpha$, SSM internal memory size~$d_s$, convolutional kernel size~$k$, number of heads~$h$ and number of layers.}
    \begin{center}
    \resizebox{\textwidth}{!}{
        \begin{tabular}{|c|c c c c c c c c c| c c c|}
            \hline
            \textbf{Hyperparameter} & $v$ & $\ell$ & $d$ & $r_d$ & $\alpha$ & $d_s$ & $k$ & $h$ & \textbf{layers} & \textbf{Weights} & \textbf{Activations} & \textbf{Total size}  \\
            \hline \hline
            \textbf{BERT(2MB)~\cite{BERT}}      & 2048  & 256 & 80  & /  & 2    & / & /  & 2 & 2 & 289 K     & 213 K     & 2,008 MB \\
            \textbf{MAMBA(2MB)~\cite{MAMBA}}    & 2048  & 256 & 64  & /  & 1    & 6 & 4  & / & 5 & 220 K     & 265 K     & 1,941 MB \\
            \textbf{Embedder}                   & 8192  & 256 & 320 & 32 & /    & / & /  & / & 1 & 293 K     & 164 K     & 1,826 MB \\
            \textbf{Embedder~+~conv}                 & 8192  & 256 & 320 & 32 & /    & / & 16 & / & 1 & 298 K     & 164 K     & 1,848 MB\\
            \textbf{EmbBERT-Q} (ours)           & 8192  & 256 & 128 & 16 & 1    & / & 32 & 1 & 4 & 357 K     & 131 K     & 781 KB\\
            \hline
            \textbf{BERT-Tiny (20MB)~\cite{tinyBERT}}   & 32768 & 512 & 128 & /  & 2    & / & /  & 2 & 2 & 4.4 M     & 786 K     & 20,746 MB \\
            \hline
        \end{tabular}
    }
    \end{center}
    \label{table:models_params_and_memory}
\end{table*}

