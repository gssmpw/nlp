\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.

%%%%%%%%%%
%packages%
%%%%%%%%%%
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{comment}


%added by me
\usepackage{hyperref}
\usepackage{multirow}
\usepackage{soul}
\usepackage[acronym]{glossaries}
\makeglossaries
\loadglsentries{glossary}

\usepackage[dvipsnames]{xcolor}
%%%%%%%%%
%defines%
%%%%%%%%%

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

%%%%%%%%%
%command%
%%%%%%%%%
\newcommand{\brav}[1]{{\color{Violet} #1}}
\usepackage{soul}
\newcommand{\fab}[1]{{\color{cyan} #1}}
\newcommand{\haz}[1]{{\color{purple} #1}}
\newcommand{\mas}[1]{{\color{teal} #1}}

%%%%%%%%%%
%document%
%%%%%%%%%%

\begin{document}


%\title{EmbBERT: \fab{\st{exploring LLMs for} moving BERT and MAMBA LLMs to} micro devices\\

\title{EmbBERT: Downscaling and Designing Language Models for Tiny Devices \fab{(or Embedded Systems?)}
% {\footnotesize \textsuperscript{*}Note: Sub-titles are not captured in Xplore and
% should not be used}
% %\thanks{Identify applicable funding agency here. If none, delete this.}
}

\author{\IEEEauthorblockN{1\textsuperscript{st} Riccardo Bravin}
\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
\textit{name of organization (of Aff.)}\\
City, Country \\
email address or ORCID}
\and
\IEEEauthorblockN{2\textsuperscript{nd} Massimo Pavan}
\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
\textit{name of organization (of Aff.)}\\
City, Country \\
email address or ORCID}
\and
\IEEEauthorblockN{3\textsuperscript{rd} Hazem Hesham Yousef Shalby}
\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
\textit{name of organization (of Aff.)}\\
City, Country \\
email address or ORCID}
\and
\IEEEauthorblockN{4\textsuperscript{th} Fabrizio Pittorino}
\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
\textit{name of organization (of Aff.)}\\
City, Country \\
email address or ORCID}
\and
\IEEEauthorblockN{5\textsuperscript{th} Manuel Roveri}
\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
\textit{name of organization (of Aff.)}\\
City, Country \\
email address or ORCID}
}

\maketitle


%-----------------------------------------------------------------------------
% Abstract
%-----------------------------------------------------------------------------
\begin{abstract}
Large language models (LLMs) have achieved remarkable success by scaling up model and dataset sizes. 
Yet, their performance in severely memory-constrained environments such as Tiny Devices and Embedded Systems - where both data and compute resources are scarce - remains largely underexplored. 
In this work, we present the first extensive evaluation of extreme architectural compression for LLMs, demonstrating their competitive advantage even on devices with stringent memory limits. Our goal is to design and deploy natural language understanding models on microcontrollers for tasks like voice command classification, while restricting the total model memory footprint - including parameters and activations - to under 2 MB.
Unlike previous efforts that largely overlook such tight resource budgets, we investigate two State-of-the-Art (SotA) architectures such as BERT and MAMBA, scaled down to meet these constraints, as well as designing a new SotA Embedded Language Model, that we call EmbBERT, based on purpose-built techniques to reduce the number of parameters and activations. 
Our experiments reveal that EmbBERT consistently outperform baselines such as BERT and MAMBA downsized to the same level, and even achieves better or comparable performances with respect to the previous smaller SotA models by using a 10$\times$ smaller memory budget, underscoring the importance of architectural design when operating under extreme memory constraints. 
Further compressing the model with an hardware-friendly 8-bit weight and activation quantization protocol, we are able to reduce the memory footprint of EmbBERT to 781 kB with minimal to no accuracy degradation. 
The purpose of our work is to serve as a milestone and a benchmark for future research on the evaluation of Language Models on Tiny Devices and Embedded Systems. 
To ensure full and immediate reproducibility of all our results, we release all code, scripts, datasets, and model checkpoints at https://github.com/RiccardoBravin/tiny-LLM.
%The impressive success of LLMs is deeply rooted in scaling model and dataset sizes. However, the question of how these architectures perform in constrained environments, addressing the opposite situation of memory and data scarcity (and whether they are useful at all in this context), remains largely unexplored. In this work, for the first time we report an extensive evaluation of extreme compression of large-scale generative language models to operate on resource-constrained devices, addressing the scarcity of research focusing on memory-bound restrictions within this context. Our project goals entail developing an AI model for natural language understanding, deployable with microcontrollers to perform tasks like command classification. Distinct from prior work, which disregards such constraints, we imposed a stringent limit of 2MB for model parameters and activations. To satisfy these demands, we evaluated state-of-the-art contenders like BERT and MAMBA with reduced parameters, along with custom models leveraging innovative techniques to minimize both parametric complexity and activations. Our experiments revealed that pared-down or purpose-built models generally outperform current SOTA counterparts, emphasizing the advantage of tailored approaches at this scale of computational and memory constraints. All supporting materials, including code, results, and model checkpoints, are publicly available at https://github.com/RiccardoBravin/tiny-LLM.
\end{abstract}

\begin{IEEEkeywords}
LLM, tinyML, NLP, BERT, embedded systems
\end{IEEEkeywords}

%-----------------------------------------------------------------------------
% Introduction
%-----------------------------------------------------------------------------

\section{Introduction}
\label{sec:introduction}
\begin{comment}
\haz{
\Gls{tinyml} \cite{tinyML} is an emerging research field that aims to deploy \Gls{ml} models on highly resource-constrained devices such as microcontrollers, which have limited memory (a couple of MB of RAM), processing power (processors running at most 100 MHz), and energy capacity. \Gls{tinyml} has gained popularity due to its ability to process data locally, enhance privacy and security, reduce latency, improve real-time responsiveness, and enable offline operation without a constant internet connection.

Another rapidly emerging area of research, \Gls{nlp}, the field of artificial intelligence that enables computers to understand, interpret, and generate human language, is currently being driven by the development of pre-trained transformer models such as}
% The rapid progress in \Gls{nlp} has been fueled by pre-trained transformer models such as 
BERT \cite{BERT}, XLNet \cite{XLNet}, DistilBERT \cite{DistilBERT}, SpanBERT \cite{SpanBERT}, ALBERT \cite{ALBERT}, RoBERTa \cite{RoBERTa}, LLaMA \cite{llama} and Palm\cite{palm}. These models have demonstrated superior performances compared to traditional word embedding models like Word2Vec \cite{word2vec} and FastText \cite{fasttext}. However, their large size and computational requirements make them unsuitable for deployment on resource-constrained devices such as Microcontrollers \brav{even for smaller models built for edge computing such as MobileBERT \cite{MobileBERT} with just 25.3M parameters}. 

\mas{The usage of NLP models on the Tiny devices is currently unexplored, since these type of models have long been considered too memory demanding for such constrained devices. Nevertheless, the emergence of wearable smart devices \cite{wearables} constantly carried by a person, such as smart glasses and watches, and \gls{iot} \cite{iot} units, increasingly more present in houses, cars, and offices, are making the on-device execution of \gls{nlp} tasks more and more appealing. Such devices are inevitably characterized by reduced human-computers interfaces, given their costs, sizes and energy requirements. For this reason, the possibility to exploit human language to control the device is considered highly valuable in this context. Unfortunately, the constant transmission of the data collected trough the device to large language models running in the cloud is extremely inconvenient, for energy, latency, privacy, and scalability reasons.}

\mas{In this context, the possibility to run \gls{nlp} models directly on Tiny devices could constitute a milestone in the design of smarter, more durable, and privacy-preserving personal devices. \textit{\glspl{tlm}} could act as human-computer interfaces, as filters for deciding when to demand the computation to more complex language models running in the cloud, and could be used to detect contextual information about the user or its environment. 
\brav{
For instance, \glspl{tlm} could power real-time audio analysis in hearing aids tailored for runners, providing environmental awareness cues without needing cloud connectivity. Similarly, in home automation, \glspl{tlm} could facilitate natural language interactions over a local network (LAN), allowing devices to respond promptly to user commands, enhancing both user experience and privacy. In industrial settings, \glspl{tlm} could manage and relay commands to automated machines, handling notifications to which operators can seamlessly reply, or even interpreting requests to adjust machinery behavior in real time.
}
\st{TODO: qualche esempio di utilizzo in più aiuterebbe.} In designing models for this context, it is extremely important to consider not only their performance in term of accuracy and requirements, but also the realistic conditions in which they will be used. Differently from general-purpose \glspl{llm}, in fact, \textit{\acrlongpl{tlm}} are meant to be executed on specific tasks and environments, with a limited scope in their usage. 
\st{TODO: commentare sul fatto che in queste condizioni l'effettiva efficacia di avere modelli pre-trained su enormi quantità di dati, contesti e tipi di comunicazione e linguaggio è dubbia}} 


\brav{
\Glspl{llm} are typically pretrained on vast datasets to capture a wide range of concepts and perform reasoning on the input they receive. However, when scaling down to such smaller scales, this approach runs into practical limitations, partly due to the constraints imposed by information theory. As a result, this kind of pretraining may have limited effectiveness.

Instead of focusing on memorizing vast amounts of data, we could shift the focus to developing models that not necessarily need to learn to produce text but excel at reasoning and "understanding" language within a specific, limited context. In such a scenario, \glspl{tlm} could prove highly useful despite their smaller size, particularly in resource-constrained environments.

}
\mas{\textbf{DOMANDA: vogliamo rendere esplicite le domande di ricerca?} Alcune possibili domande di ricerca: 

\begin{itemize}
    \item \textbf{What are the characteristics of the execution of the language models in the TinyML environment? How can we realistically measure the performance of language models in this context?}
    \item \textbf{What are the actual requirements of current language model architectures?}
    \item \textbf{What are the performance of these architectures, once scaled to fit the TinyML context?}
    \item \textbf{How can we design a novel lightweight architecture for this context?}
    \item \textbf{What are the effects and the tradeoffs connected to the use of pre-trained language models in this context? Is attention useful nevertheless?}
\end{itemize}
}

\mas{This paper aims at presenting the concept of \acrfullpl{tlm}, as well as analyzing how well architectures used in \acrfullpl{llm} scale down to the execution on Microcontrollers and presenting the first examples of a \gls{tlm}, \textit{EmbBERT}, specifically designed for resource constrained devices normally used in the \gls{tinyml} context. To perform a fair comparison, we set a memory limit of 2 MB on all the models that we considered in our analysis.}

\brav{
We opted for this limit on the rationale that even the most modern Microcontroller Unit/Tiny device can barely reach this amount of RAM. Given the rapid growth of current technology, we expect this limit to be surpassed soon, which will render our models more easily deployable in conjunction with other code. Furthermore, due to their small size, it would also be very easy to run these models on more capable devices even in almost real-time.
}

% Specifically, we present a comprehensive overview of the memory layout at inference time for each proposed model in a single processor, single thread setting.
% This allows us to calculate the minimum possible activation size of each layer, an aspect that is often overlooked but becomes crucial at this scale.
Specifically, our main contributions are threefold:
\mas{
\begin{itemize}
    \item we characterize the usage of Language Models in the \gls{tinyml} context, introducing for the first time the concept of \acrlong{tlm},
    \item we provide a deep analysis of commonly used Language Model's memory and computational complexities, and propose ways for scaling them down to enable their execution on tiny devices,
    \item we propose a novel \acrlong{tlm} tailored for resource-constrained devices, named \textit{EmbBERT}, that makes use of optimized versions of standard Language Model Components and organizes them in a novel lightweight architecture.
\end{itemize}
}

The outline of this paper is as follows\brav{\textbf{TODO: fix and adjust with new structure}}:

\begin{itemize}
\item Section \ref{sec:relatedwork} reviews recent publications on model compression and training that preserve performance.
\item Section \ref{sec:problem} briefly describes the datasets used for training and evaluation.
\item Section \ref{sec:Models} presents an in-depth description of our proposed models.
\item Section \ref{sec:components} reports an analytical analysis of the memory and computational complexity of the proposed models.
\item Section \ref{sec:setup} describes the experimental setup used to obtain our results.
\item Section \ref{sec:results} presents a review of the obtained results in the different datasets of our models
\item Section \ref{sec:comparison} strikes a comparison between our best model and BERT-Tiny
\item Section \ref{sec:conclusions} discusses future research directions and concludes our findings.
\end{itemize}
\end{comment}

Energy-efficient computing platforms, such as wearable devices~\cite{wearables} and Internet-of-Things (IoT) units~\cite{iot}, have seen a recent and significant expansion in their memory and computational capabilities. 
Even microcontroller-based devices - widely regarded as among the less energy-demanding off-the-shelves platforms - can nowadays be embedded with the capability to execute sophisticated machine learning (ML) and deep learning (DL) algorithms~\cite{MCUNet}~\cite{Eff_NN_for_embedded}.
Within this context, the field of \emph{TinyML}~\cite{tinyML} has emerged to address the execution of ML/DL models on highly resource-constrained devices characterized by limited memory (on the order of a few megabytes of RAM), modest processing power (e.g., CPUs operating at or below 100 MHz), and strict energy budgets. 
The growing popularity of TinyML stems from its ability to locally process data, thereby reducing latency and improving the real-time responsiveness of smart applications, enhancing privacy by keeping sensitive data on-device, and enabling offline operation.

The rapid proliferation of energy-efficient Tiny devices across homes, vehicles, and workplaces has made on-device NLP increasingly desirable. These devices often have small human-computer interfaces given their costs, sizes and energy requirements, making the possibility to exploit intuitive voice and text-based user interaction highly valuable in this context. The constant transmission of the data collected by the device to large language models running in the cloud for such tasks is often impractical due to constraints on energy, latency, privacy, and scalability.
%Despite these limitations, the widespread diffusion of devices based on energy-efficient platforms, like wearables constantly carried by a person, and IoT of devices, increasingly more present in houses, cars, and offices, are making the on-device execution of \gls{nlp} tasks more and more appealing. Such devices are inevitably characterized by reduced human-computers interfaces, given their costs, sizes and energy requirements. For this reason, the possibility to exploit human language to control the device is considered highly valuable in this context. Unfortunately, the constant transmission of the data collected trough the device to large language models running in the cloud is extremely inconvenient, for energy, latency, privacy, and scalability reasons.

Up to now, TinyML solutions for Deep Learning have been mainly confined to fully-connected and convolutional neural networks~\cite{Roveri}.
%, with a few examples in the NLP domain.
While TinyML techniques have successfully enabled tasks such as keyword spotting~\cite{on_device_kw_spotting}~\cite{customizable_kw_spotting}, image classification~\cite{mobilenet_v2}~\cite{micro_net_img_rec}, and object detection~\cite{EfficientDet_obj_det} on microcontrollers (MCUs) and other resource-constrained Tiny Devices, applying them to \emph{natural language processing} (NLP) remains challenging due to the high memory and compute demands of typical NLP architectures. 
The current state-of-the-art in NLP for sentence classification, on which we will focus in this paper from a TinyML perspective, is dominated by large-scale, pre-trained transformer models such as BERT~\cite{BERT}, XLNet~\cite{XLNet}, DistilBERT~\cite{DistilBERT}, SpanBERT~\cite{SpanBERT}, ALBERT~\cite{ALBERT} and RoBERTa~\cite{RoBERTa}. Despite their exceptional accuracy on a vast range of tasks, these models - along with their edge-focused derivatives like MobileBERT~\cite{MobileBERT} at "just" 25.3M parameters - remain far too large for deployment on devices with extremely tight memory budgets, e.g., sub-2MB of RAM.
%These models have demonstrated superior performances compared to traditional word embedding models like Word2Vec \cite{word2vec} and FastText \cite{fasttext}. However, their large size and computational requirements make them unsuitable for deployment on resource-constrained devices such as Microcontrollers even for smaller models built for edge computing such as MobileBERT \cite{MobileBERT} with just 25.3M parameters.

%\emph{\acrfullpl{tlm}}
% In this paper, we introduce the concept of \emph{Tiny Language Models} (TLMs), presenting a compelling solution by enabling basic NLP functionality directly on MCUs and other resource-limited platforms. When paired with efficient speech-to-text solutions~\cite{}, TLMs can serve as voice-command interfaces, local language filters that decide when to offload computation to more powerful cloud-based models, or sensors capable of real-time contextual understanding in embedded settings.
%To address this, we introduce the concept of \emph{Tiny Language Models} (TLMs), which 

%TLMs, when paired with efficient speech-to-text solutions~\cite{STT_embedded}, can power voice-command interfaces, local language filters that selectively offload tasks to cloud-based models, and sensors capable of real-time contextual understanding. Potential applications include real-time environmental awareness in hearing aids for runners, natural language interactions over local networks in home automation, and command management for industrial automation. These use cases highlight the promise of TLMs to enhance privacy, responsiveness, and energy efficiency in embedded systems.

In order to enable basic NLP functionality directly on MCUs and other resource-constrained devices, we design the first examples of \emph{Tiny Language Models} (TLMs), balancing task-specific accuracy with the stringent memory, latency, and energy constraints of Tiny devices. Unlike general-purpose LLMs, which are pre-trained on massive datasets for broad functionality, our TLMs are thought for specific tasks and environments. At such small scales, memorizing vast quantities of text is infeasible, although in this paper we assess the success of pre-training even in a Tiny seeting.
%due to fundamental information-theoretic limits. 
Instead, TLMs focus on retaining essential language understanding and reasoning capabilities sufficient for their target tasks, prioritizing memory and computational efficiency over generality.

%\textit{\glspl{tlm}}, once coupled with efficient speech-to-text~\cite{} solutions, could act as human-computer interfaces, as filters for deciding when to demand the computation to more complex language models running in the cloud, and could be used to detect contextual information about the user or its environment. 
% Designing TLMs entails focusing not only on model accuracy but also on the realistic memory, latency, and energy constraints under which they must operate. Unlike general-purpose large language models (LLMs) - pre-trained on massive corpora to handle a wide range of tasks - TLMs target narrower, task-specific scenarios. At extremely small scales, the usual approach of memorizing vast quantities of text runs into fundamental limits of information theory. Instead, TLMs strive to retain core language \emph{understanding} and \emph{reasoning} capabilities that are sufficient for a predefined set of tasks.
% The possibility to run NLP models directly on Tiny Devices could constitute a milestone in the design of smarter, more durable, and privacy-preserving personal devices. 
% For instance, \glspl{tlm} could power real-time audio analysis in hearing aids tailored for runners, providing environmental awareness cues without needing cloud connectivity. Similarly, in home automation, \glspl{tlm} could facilitate natural language interactions over a local network (LAN), allowing devices to respond promptly to user commands, enhancing both user experience and privacy. In industrial settings, \glspl{tlm} could manage and relay commands to automated machines, handling notifications to which operators can seamlessly reply, or even interpreting requests to adjust machinery behavior in real time.
% In designing models for this context, it is extremely important to consider not only their performance in term of accuracy and requirements, but also the realistic conditions in which they will be used. Differently from general-purpose \glspl{llm}, in fact, \textit{\acrlongpl{tlm}} are meant to be executed on specific tasks and environments, with a limited scope in their usage. \Glspl{llm}, in fact, are typically pre-trained on vast datasets to capture a wide range of concepts and perform reasoning on the input they receive. However, when scaling down to such smaller scales, this approach runs into practical limitations, partly due to the constraints imposed by information theory. As a result, this kind of pretraining may have limited effectiveness. Instead of focusing on memorizing vast amounts of data, \glspl{tlm} aims at shifting the focus to developing models that not necessarily need to learn to produce text but excel at reasoning and "understanding" language within a specific, limited context. In such a scenario, \glspl{tlm} could prove highly useful despite their smaller size, particularly in resource-constrained environments.

%This paper aims at presenting the concept of \acrfullpl{tlm}, as well as presenting the first examples of a \gls{tlm}, \textit{EmbBERT}, specifically designed for resource-constrained devices normally used in the \gls{tinyml} context. To perform a fair comparison, we set a memory limit of 2 MB on all the models that we considered in our analysis.
%We opted for this limit on the rationale that the most recent Microcontroller Units are equipped with this amount of RAM memory \cite{}. Given the rapid growth of current technology, we expect this limit to be surpassed soon, which will render our models more easily deployable in conjunction with other code. Furthermore, due to their small size, it would also be very easy to embed these models in larger applications on more capable devices.
In this paper we present the first example of a TLM, \emph{EmbBERT}, specifically designed for MCUs and resource-constrained Tiny Devices. To ensure a fair comparison across different architectures, we impose a strict 2MB memory budget on model parameters and activations. This budget reflects the capabilities of next-generation MCUs units, making our solutions forward-compatible with upcoming hardware improvements while ensuring that our model can be easily embedded in larger applications on more powerful devices.

To further explore the compressibility of our proposed EmbBERT model while being compatible with hardware solutions, we explore the role of 8-bit quantization~\cite{transformers_compression} in addressing the stringent resource demands of TinyML platforms. Although much of research in quantizing LLMs has focused on downscaling on models with billions of parameters, we quantitatively demonstrate its potential in preserving task-specific accuracy for on-device NLP.
%, where balancing efficiency and performance is paramount. 
By pairing quantization with parameter-efficient fine-tuning, TLMs can be made smaller, faster, and more energy-efficient, enhancing their deployability in ultra-constrained environments.
%To address these limitations, recent advancements in \emph{model quantization}~\cite{transformers_compression} have allowed models to meet the stringent resouce requirements of TinyML platforms without sacrificing task-specific accuracy. While being currently mostly used to quantize large language models rendering inference possible on consumer computers this technique is very attractive for on-device NLP applications, where balancing efficiency and effectivenes is of paramount importance. 
%The addition of quantization further amplifies the applicability of TLMs by enabling even smaller architectures, reduced latency, and energy-efficient inference.
%By leveraging advanced quantization techniques alongside parameter-efficient fine-tuning, we further enhance the practicality of TLMs, achieving an optimal trade-off between performance and resource constraints.
 
Our contributions are:

\begin{enumerate}
    \item Downscaling methodology for LLMs: we carefully compute parameters and activations for LLM layers towards the goal of their downscaling at a constrained memory budget
    \item EmbBERT Architecture: We propose a novel TLM architecture, \emph{EmbBERT}, which is tailored to extremely resource-constrained devices. Despite its small footprint, EmbBERT outperforms scaled-down versions of mainstream LLMs and is competitive with other minimal-size language models like BERT-Tiny \cite{tinyBERT}. Notably, EmbBERT uses only one-tenth of BERT-Tiny’s memory.
    \item Architectural Ablation Study: We dissect each architectural component of EmbBERT to quantify its performance gains relative to its memory costs, shedding light on the TLM architectural trade-offs required to stay within a strict 2MB memory limit.
    \item TinyNLP benchmark: We define a benchmark for Tiny Language Models solutions, namely the TinyNLP benchmark suite, as a reference for the evaluation of language models in these constrained settings.
    %Introducing TLMs: We formalize the concept of tiny language models, outlining their role and significance in the tinyML ecosystem.
\end{enumerate}

The remainder of the paper is organized as follows:
\begin{itemize}
    \item Section~\ref{sec:relatedwork}: Reviews recent work on model compression and training for resource-constrained platforms.
    \item Section~\ref{sec:background}: Discusses the fundamental architectural components of language models.
    \item Section~\ref{sec:components}: Presents detailed calculations on the number of parameter and activations in LLM layers, in order to proceed to their down-scaling within a constrained memory budget.
    \item Section~\ref{sec:EmbBERT}: Provides an in-depth description of our proposed, state-of-the-art EmbBERT model for NLP in Tiny Devices.
    \item Section~\ref{sec:setup}: Outlines the experimental setting, training procedure and datasets used to obtain the reported results.
    \item Section~\ref{sec:results}: Presents our evaluation on the TinyNLP benchmark suite and GLUE of downscaled versions of BERT and MAMBA, showing the significant comparative performance gains obtained by our proposed EmbBERT model.
    \item Section~\ref{sec:ablation}: Carefully evaluates the impact of EmbBERT's architectural modules on its performances.
    %Compares the performance of EmbBERT with BERT-Tiny~\cite{}, highlighting the impact of architectural choices.
    \item Section~\ref{sec:conclusions}: Concludes with discussions on future directions and broader implications of TLMs.
\end{itemize}

\begin{comment}
Specifically, our main contributions are threefold:
\begin{itemize}
    \item We characterize the usage of Language Models in the \gls{tinyml} context, introducing for the first time the concept of \acrlong{tlm},
    \item We propose a novel \acrlong{tlm} tailored for resource-constrained devices, named \textit{EmbBERT}, that makes use of a novel lightweight architecture. \textit{EmbBert} demonstrates superior performance compared to common \Glspl{llm} architectures, appropriately scaled down to the proposed memory constraint, and obtains results in line with the smallest language model that we were able to find in the literature, Bert-Tiny\cite{}, which still requires 10x the memory amounts of our proposed model.
    \item We analyze in detail all the components of the \textit{EmbBert} architecture, motivating their adoption with respect to the improvement in performance they provide and the tradeoffs in the memory requirements they consent to make with respect to the 2 MB limitation that we imposed.
\end{itemize}

The rest of the paper is organized as follows:

\begin{itemize}
    \item Section \ref{sec:relatedwork} reviews recent publications on model compression and training that preserve performance.
    \item Section \ref{sec:background} introduce the basic Architectural components of language models architectures.
    %\item Section \ref{sec:problem} briefly describes the datasets used for training and evaluation.
    \item Section \ref{sec:EmbBERT} presents an in-depth description of our proposed model.
    %\item Section \ref{sec:components} reports an analytical analysis of the memory and computational complexity of the proposed models.
    \item Section \ref{sec:setup} describes the experimental setup used to obtain our results.
    \item Section \ref{sec:results} presents a review of the obtained results in the different datasets of our model and the comparisons.
    \item Section \ref{sec:ablation} strikes a comparison between our best model and BERT-Tiny
    \item Section \ref{sec:conclusions} discusses future research directions and concludes our findings.
\end{itemize}
\end{comment}

%\mas{\textbf{DOMANDA: vogliamo rendere esplicite le domande di ricerca?} Alcune possibili domande di ricerca: 

%\begin{itemize}
%    \item \textbf{What are the characteristics of the execution of the language models in the TinyML environment? How can we realistically measure the performance of language models in this context?}
%    \item \textbf{What are the actual requirements of current language model architectures?}
%    \item \textbf{What are the performance of these architectures, once scaled to fit the TinyML context?}
%    \item \textbf{How can we design a novel lightweight architecture for this context?}
%    \item \textbf{What are the effects and the tradeoffs connected to the use of pre-trained language models in this context? Is attention useful nevertheless?}
%\end{itemize}
%}

%-----------------------------------------------------------------------------
% Related work
%-----------------------------------------------------------------------------


\section{Related Work}
\label{sec:relatedwork}

This section reviews recent advancements in downscaling transformer-based architectures and adapting them for resource-constrained environments, with a focus on memory targets comparable to our sub-2MB target for microcontroller-based platforms. While existing techniques like architectural simplifications, quantization, and knowledge distillation have made significant advancements, our work pushes the boundaries of model compression to successfully tackle much stricter constraints with respect to current literature. To the best of our knowldge no particular advancements have ever been tried for MAMBA-based arhitectures in the MCU setting.

\paragraph{Efforts to Scale Down Transformers} 
Several models, such as BERT-Tiny~\cite{tinyBERT}, NanoBERT~\cite{NanoBERT}, MobileBERT~\cite{MobileBERT}, ConvBERT~\cite{ConvBERT}, and I-BERT~\cite{i-BERT}, illustrate key approaches to compressing transformers. 

\textit{BERT-Tiny}~\cite{tinyBERT} employs a shallow architecture with fewer layers and reduced hidden dimensions, trading accuracy for lower computational costs and smaller size (4.4M parameters). Employing also a knowledge distillation approach it transfers knowledge from a larger teacher model to a compact student model, achieving more competitive accuracy through task-specific and data-augmented distillation. 

\textit{NanoBERT}\cite{NanoBERT} represents an extreme compression effort for microcontroller platforms, introducing the Nano Embedder to achieve a minimal memory footprint suitable for basic NLP tasks, with parameter counts ranging between 700k and 800k. \textit{MobileBERT}~\cite{MobileBERT}, on the other hand, employs bottleneck structures and teacher-student training to compress BERT, retaining high accuracy at the expense of a higher parameter count (25.3M), making it better suited for edge devices than ultra-constrained microcontrollers.

\textit{ConvBERT}~\cite{ConvBERT} introduces convolutional operations in self-attention to reduce computational complexity and capture local dependencies more efficiently, with its smallest configuration comprising 14M parameters. Meanwhile, \textit{I-BERT}~\cite{i-BERT} employs integer-only quantization, ensuring all computations, including softmax and matrix multiplications, use integer arithmetic, significantly improving memory and energy efficiency without compromising accuracy, despite its larger size of 355M parameters.

\paragraph{Advanced Training and Pre-Training Techniques}
Innovations in training regimes have introduced methods beyond traditional masked language modeling (MLM). Techniques like \textit{ELECTRA}'s generative-discriminative training~\cite{ELECTRA} and \textit{LoRA}'s low-rank updates~\cite{LoRA} optimize fine-tuning while reducing trainable parameters. Data augmentation and synthetic text generation~\cite{data_augmentation, synthetic_data} also improve small-model performance by expanding training data and enhancing knowledge transfer.

\paragraph{Quantization and Knowledge Distillation}
Quantization, a cornerstone of model compression, converts weights and activations to lower-precision formats, such as 8-bit integers, drastically reducing memory usage and improving inference efficiency~\cite{quantization}. Integer arithmetic, as demonstrated by I-BERT~\cite{i-BERT}, aligns well with embedded hardware capabilities. Knowledge distillation~\cite{knowledge_distillation}, employed in models like Bert-Tiny and MobileBERT, further reduces model size by training smaller networks to replicate larger teacher models' behavior.

\paragraph{Exploring Alternative Architectures} 
Beyond transformers, alternative architectures like recurrent neural networks (RNNs) with state-space models (SSMs) present a viable option for constrained hardware. For example, \textit{MAMBA}~\cite{MAMBA}, with it's 140M parameters, leverages RNNs to mitigate attention's quadratic complexity, offering efficient text generation solutions tailored for environments with limited parallelism.

\paragraph{Comparison and Challenges}
Despite these advancements, most existing models fall short of meeting sub-2MB memory constraints required for ultra-constrained platforms. The smallest model identified in the literature is \textit{NanoBERT}, which achieves a parameter size of approximately 2MB, excluding the memory required for activations. In contrast, all other models, including \textit{ConvBERT} (14M parameters) and \textit{I-BERT} (355M parameters), largely exceed tenths of MB of total memory usage. 
 

Building on these foundations, our work expands on the tailored architectural designs and applies memory optimized techniques for embedded NLP tasks, pushing the limits of transformer downsizing to meet the most stringent resource requirements.


% This section reviews efforts to downscale large transformer-based architectures, focusing on models like BERT-Tiny, TinyBERT, NanoBERT, MobileBERT, ConvBERT, and I-BERT, which represent significant progress in adapting natural language processing (NLP) models for resource-constrained environments. These works offer valuable insights into achieving efficient model performance while addressing the tight memory and computational limitations of TinyML platforms.

% \paragraph{BERT-Tiny} BERT-Tiny is among the first efforts to reduce the size of BERT-based models for efficient inference on edge devices. BERT-Tiny employs a shallow architecture with fewer layers and reduced hidden dimensions, sacrificing some accuracy to meet more stringent resource requirements. TinyBERT, on the other hand, utilizes knowledge distillation to transfer knowledge from a larger teacher model to a smaller student model, maintaining competitive accuracy on downstream tasks while significantly reducing the number of parameters and computation costs~\cite{TinyBERT}. TinyBERT also incorporates task-specific and data-augmented distillation, further improving performance in resource-limited settings.

% \paragraph{NanoBERT and MobileBERT} NanoBERT represents an extreme form of model compression, targeting microcontroller-based platforms. By the standard BERT architecture and the newly introduced Nano Embedder, NanoBERT achieves a minimal memory footprint suitable tiny environments while maintaining functionality for simple NLP tasks. MobileBERT~\cite{MobileBERT}, designed for mobile and edge devices, replaces the original BERT's feed-forward networks with bottleneck structures and employs a carefully tuned teacher-student training strategy to compress BERT while retaining accuracy. Despite its optimization, MobileBERT remains more suited for high-end edge devices than microcontrollers due to its parameter count (25.3M) and memory demands.

% \paragraph{ConvBERT and I-BERT}
% ConvBERT introduces convolutional operations within the attention mechanism, reducing computational complexity while capturing local dependencies more efficiently~\cite{ConvBERT}. This hybrid approach lowers the memory and computational demands of self-attention, making ConvBERT a promising candidate for edge deployments.

% I-BERT~\cite{IBERT}, on the other hand, focuses on integer-only quantization, employing techniques to perform all computations, including matrix multiplications and softmax operations, in integer arithmetic. This results in significant energy and memory savings without compromising accuracy, particularly on tasks requiring quantized inference. I-BERT’s reliance on integer arithmetic makes it highly compatible with embedded hardware optimized for such operations.

% \paragraph{Architectural Simplifications and Specialized Training}
% Many of these models integrate architectural simplifications to improve efficiency. For example, MobileBERT leverages bottleneck transformers, and TinyBERT reduces the number of attention heads and hidden dimensions. Task-specific fine-tuning and distillation strategies, as seen in TinyBERT and NanoBERT, further enhance the efficiency of these models. Additionally, I-BERT’s quantization-aware training underscores the importance of aligning model design with the target hardware capabilities.

% \paragraph{Comparison and Remaining Challenges}
% While these models exemplify state-of-the-art approaches for downsizing transformers, they fall short of meeting the sub-2MB memory budget required for microcontroller-based platforms. Most rely on combinations of pruning, quantization, and distillation, achieving impressive results on devices with moderate resources but remaining impractical for ultra-constrained environments. Our work builds on these advances, pushing the boundaries of transformer compression by targeting even stricter memory and computational limits while tailoring architectures explicitly for embedded NLP tasks.

\begin{comment}
    
This section surveys recent techniques in model compression and efficient training, with an emphasis on transformer-based architectures and their potential deployment on severely memory-limited embedded platforms. While existing studies often downsize state-of-the-art (SOTA) models to fit the constraints of edge devices, our work pushes beyond these approaches by targeting even tighter memory budgets where no single existing technique proves sufficient.

\paragraph{Simplification of Transformer Blocks}
Multiple studies have proposed architectural modifications to the transformer’s attention and feed-forward blocks, aiming to reduce memory footprint and increase computational efficiency \cite{MobileBERT, simplifying_transformer_blocks, efficient_attention}. For instance, decoupling or simplifying attention heads can cut the quadratic complexity that arises as input sequences grow. Rotary embeddings \cite{Rotary_position_embedding} improve performance by refining positional encoding, and alternative activation functions like \textit{SwiGLU} \cite{SwiGLU} impact how parameter memory encodes learned representations. These designs are highly relevant for tinyML, where drastic model simplifications are often mandatory.

\paragraph{Deep vs. Wide Models}
Balancing network depth and width is crucial to modeling capacity under tight memory constraints. Studies such as \cite{exploring_transformer_sizes} suggest that deeper, narrower architectures may outperform shallower, wider ones when parameters are severely limited. Related methods like EfficientNet~\cite{EfficientNET} blend width and depth scaling, optimizing both dimensions to achieve SOTA results. Adapting these insights to NLP indicates that carefully tuning both depth and width can yield optimal performance within extreme memory budgets.

\paragraph{Training and Pre-Training Techniques}
Recent training regimes break from the standard BERT-style masked language modeling (MLM) and next-sentence prediction (NSP). For instance, LoRA \cite{LoRA} updates only low-rank matrices during fine-tuning, while ELECTRA~\cite{ELECTRA} introduces a generative-discriminative training scheme aimed at more sample-efficient representation learning. Other enhancements such as data augmentation and synthetic text generation \cite{data_augmentation, synthetic_data, triple_training} show promise in improving small-model performance by distilling more knowledge into fewer parameters.

\paragraph{Quantization and Knowledge Distillation}
Quantization reduces model size and inference cost by representing weights and activations with lower-precision data types (e.g., 8-bit integers instead of 32-bit floating-point) \cite{quantization}. This approach lowers memory usage and can accelerate inference when hardware supports specialized instructions for integer arithmetic.

Knowledge distillation \cite{knowledge_distillation} trains a smaller “student” network to mimic a larger “teacher” model. By matching the outputs or intermediate representations of the teacher, the student retains much of the teacher’s performance despite a drastically reduced parameter count. Several works combine quantization with distillation to attain substantial compression while preserving accuracy \cite{MobileBERT, 1bLLMs}.

\paragraph{Alternative Architectures}
Beyond transformers, recurrent neural networks (RNNs) equipped with state-space models (SSMs) suggest alternative, more hardware-friendly paradigms. MAMBA~\cite{MAMBA} exemplifies an RNN-based approach designed to tackle text generation with reduced reliance on attention mechanisms. Such architectures can be advantageous on devices where parallelism is limited or where attention’s quadratic complexity is prohibitively expensive.

Summing up, while existing compression methods - quantization, distillation, pruning, and architectural simplifications - have successfully reduced SOTA models for edge deployments, they do not fully address the sub-2MB memory targets explored in this work. We extend these established methodologies with more aggressive constraints and tailor the architectures themselves for minimal size. 
\end{comment}
%By integrating the latest findings on deep vs. wide configurations, parameter sharing, and novel training protocols, our study aims to define new frontiers for embedding LLM capabilities into ultra-low-power devices.

%\subsection{Parameter Sharing and Low-Rank Matrix Decomposition}
%Techniques like parameter sharing \cite{dictFormer} and low-rank matrix decomposition \cite{LoRA} further reduce parameter counts by factoring large weight matrices into smaller, lower-rank components. This approach can significantly decrease model memory footprints while maintaining competitive performance on NLP tasks. When aiming for sub-2MB deployable models, parameter sharing and low-rank factorization offer compelling methods of compression without catastrophic performance degradation.

%\subsection{Pruning}
%Pruning selectively removes neurons or parameters deemed less critical to model performance, effectively reducing size and compute complexity. Traditional approaches remove individual weights (unstructured pruning) or entire filters/heads (structured pruning). Recent breakthroughs even prune entire layers from large language models (LLMs) with minimal performance drops \cite{cutting_LLM_layers}, highlighting that the first and last layers often capture most essential features. Pruning thus offers a powerful strategy for shrinking network footprints for memory-constrained inference.

\begin{comment} 
This section will review recent compression and efficient training techniques, focusing on their application to transformer-based models and their feasibility for embedded systems. Oftentimes these are applied to preexisting \gls{sota} architectures that are just reduced in size to fit the constraints of edge devices, here we want to go a step further since the constraints are much stricter and no preexisting technique reaches this level of compression. 

\subsection{Quantization and Knowledge Distillation} 
Quantization and Knowledge Distillation are two techniques used to reduce both the computational and the memory costs of running inference. Specifically, quantization achieves this by representing the weights and activations of the models with lower-precision data types like 8-bit integers instead of the usual 32-bit floating point \cite{quantization}.

In knowledge distillation instead a small network (i.e., the student) is trained to emulate the responses of a bigger and much more knowledgeable model (i.e., the teacher) \cite{knowledge_distillation}. This technique has been widely used in various domains, including computer vision and \Gls{nlp}.

The combination of quantization with knowledge distillation has achieved a significant model compression with minimal performance losses as shown by \cite{MobileBERT} or \cite{1bLLMs}.

\subsection{Pruning}
Pruning is an optimization technique used to reduce the size and computational cost of deep learning models, without significantly compromising their performances. It involves selectively removing unnecessary or redundant connections between neurons in a network, thereby reducing the model's complexity. Probably the most recent revolutionary idea comes from \cite{cutting_LLM_layers} where it has been proposed that not just some of the connections but most of the layers of a \gls{llm} can be removed without much loss in performance since most of the work is done by the first layers and the last ones.

\subsection{Simplification of Transformer Blocks}
In \cite{MobileBERT}, \cite{simplifying_transformer_blocks}, and \cite{efficient_attention} modifications to the current generation of the transformer's attention and fully connected layers have been proposed to reduce the model size, increase parallelization, and increase performances.
Moreover, several of these techniques have shown practical and methodological effectiveness. These methods are particularly relevant to our study, as they address the critical need to compress models to an extreme degree for deployment on resource-constrained devices.

Additionally, models like \cite{Rotary_position_embedding} introduce innovations such as rotary embeddings which enhance performance compared to traditional learned positional encodings.
Moreover, research on activation functions, such as the \textit{SwiGLU} activation variant from \cite{SwiGLU}, has unveiled the importance of the fully connected layer as a mean of storing informations learned during training for the model.% which thus if reduced lowers the capability to recall information learned during training.

\subsection{Parameter Sharing and Low-Rank Matrix Decomposition}
Approaches like the Parameter Sharing \cite{dictFormer} and \Gls{lora} \cite{LoRA} demonstrate the effectiveness of parameter sharing and low-rank matrix decomposition in reducing the model's complexity. These methods are especially valuable when targeting devices that must operate under severe memory constraints, as they allow for significant reductions in the number of parameters while preserving the ability to perform well on \Gls{nlp} tasks.

\subsection{Alternative Architectures}
The exploration of alternative architectures, such as MAMBA \cite{MAMBA}, which is rooted in the \gls{rnn} paradigm and makes use of \glspl{ssm}, offers promising avenues for reducing the quadratic cost of attention as more tokens appear during inference. Unlike transformers, which rely on attention mechanisms, MAMBA’s \gls{rnn}-based approach may provide more efficient processing for text generation tasks, particularly on hardware where parallelism is limited. 

\subsection{Training techniques}
Newer training techniques that differ from the standard BERT training with \gls{mlm} and \gls{nsp} are also being tested. Examples such as \gls{lora} in \cite{LoRA}, have already been widely adopted while others such as ELECTRA in \cite{ELECTRA} still struggle to emerge even though they seem promising. These techniques usually try to achieve the same or better level of training while taking noticeably less time. In publications such as \cite{data_augmentation}, \cite{synthetic_data} and \cite{triple_training} proven training methods are used but, to those, simple modifications are made such as: data augmentation, synthetic data creation or multiple rounds of different training steps, to improve performances of the models such that they can learn as best as possible even small datasets.

\subsection{Deep vs. Wide Models}
Both width and depth contribute to a model's ability to learn from data. A balance needs to be struck between width and depth to prevent overfitting while still being able to capture the complexity of the data.
Recent studies on model depth vs. width suggest that deeper and narrower models may be more effective for resource-constrained devices \cite{exploring_transformer_sizes}.
Techniques such as the one proposed by EfficientNET \cite{EfficientNET} which use a combination of scaling up both width and depth to achieve \gls{sota} performance on image classification tasks are particularly relevant for our work and in this field, where the goal is to maximize performance within tight memory and computational budgets.

Summing up, while existing compression techniques have successfully reduced the size of \gls{sota} models to fit edge devices, our work seeks to push these boundaries further by targeting scenarios with even stricter constraints. Unlike previous efforts that primarily focused on downsizing proven architectures, we explore the potential of simpler models and newer techniques that enable us to achieve hyperparameters similar to those of SotA models while still maintaining a limited number of parameters.
\end{comment}


\input{tables/tab_w&a_noMAMBA} %TABLE WITH WEIGHTS AND ACTIVATIONS OF ALL MODEL'S LAYERS

% \begin{table*}[t]
%     \caption{Formulas for calculating weights and activation sizes per layer, based on their hyperparameters.}
%     \begin{center}
%         \begin{tabular}{|c | c c|}
%         \hline
%         \textbf{Layers} & \textbf{Weights} & \textbf{Activations} \\
%         \hline \hline
%         \textbf{Embedder}               & $(v + \ell +2)\cdot d$ 
%                                         & $2 d\cdot \ell$ \\
%         \textbf{NanoEmbedder}           & \(r_d \cdot (v + \ell + 2 d ) + 2 d\)
%                                         & \(r_d \cdot \ell + 2 d \cdot \ell\) \\
%         \textbf{Normalization}          & \(2 d\)
%                                         & \(2 d \cdot \ell\) \\
%         \textbf{Feed Forward}           & \(2 d^2 \cdot \alpha \) 
%                                         & \(d \cdot \ell \cdot ( 2 \alpha )\)\\
%         \textbf{Attention}              & \(4 d^2 \)
%                                         & \(4 d \cdot \ell + \ell^2 \cdot h\)\\
%         \textbf{Efficient Attention}    & \(2 d^2 \) 
%                                         & \(2 d \cdot \ell + \ell^2 \)\\
%         \textbf{Eff Diff Skip Attention} & \(2 d^2 + d^2 \cdot \alpha + k \cdot d \alpha + 4d\)
%                                         & \(\max ( 2 d \cdot \ell + \ell^2 ; \  d \cdot \ell (1 + \alpha )) \)\\
%         %\textbf{MAMBA layer}            & \(i \cdot (3d + c + 2 + 3d_s + 2\rho)\)
%         %                                & \( \ell \cdot (d + 3i + 2i \cdot d_s + d_s) + \max (i \cdot d_s; \ \ell \cdot d_s)  \)\\
%         \hline
%         \end{tabular}
%     \end{center}
%     \label{table:memory_analysis_appendix}
% \end{table*}

%-----------------------------------------------------------------------------
% Background
%-----------------------------------------------------------------------------

\section{Background} 
\label{sec:background} 

Understanding the core components that underlie modern LLMs clarifies how each architectural choice contributes to the design of TLMs. This section provides an overview of three foundational elements for current SotA models - embedders, attention mechanisms, and state space models~(SSMs).% - and examines how they collectively enable increasingly sophisticated and resource-efficient solutions.

\paragraph{Embedders}
Embedders form the initial processing layer of most language models, transforming raw textual input into a continuous vector space where semantically related concepts are placed closer together. Early approaches (e.g., Word2Vec \cite{word2vec} and FastText \cite{fasttext}) produced static word embeddings, assigning each word a single representation regardless of context. These methods significantly improved the capture of semantic relationships but lacked the nuance of contextual variation.

Modern transformer-based models, such as BERT \cite{BERT}, employ \emph{contextual embedders} that dynamically compute embeddings to capture token-specific semantics, positional information, and inter-sentence relationships. Specifically, BERT uses:
\begin{itemize}
    \item Token Embeddings to represent each token’s identity.
    \item Position Embeddings to encode sequence order.
    \item Segment Embeddings to differentiate sentence pairs in tasks like question-answering.
\end{itemize}
These learned embeddings serve as a rich foundation for downstream layers, enabling the model to capture finer-grained syntactic and semantic relationships across tokens within a single sequence.

\paragraph{Attention Mechanisms}
Attention mechanisms, introduced in transformer architectures \cite{transformers}, selectively weight different parts of the input sequence to focus the model’s capacity on the most relevant information. By enabling tokens to “attend to” each other, attention mechanisms excel at capturing long-range dependencies - essential for language tasks where context can span many tokens.

In BERT \cite{BERT}, a multi-head self-attention module computes attention scores between every pair of tokens in both forward and backward directions, building a bidirectional understanding of text. This multi-headed design partitions the embedding space into multiple subspaces, allowing the model to learn distinct but complementary patterns. Consequently, attention layers equip BERT with a holistic view of the entire input, facilitating more complex language representations for tasks like sentiment analysis or named-entity recognition.

\paragraph{State Space Models (SSMs)}
State space models (SSMs) offer an alternative to the attention-driven paradigm by modeling sequences through recurrent or state-tracking mechanisms. SSMs are particularly valuable for long-context settings, as they can propagate information across lengthy sequences in \emph{linear} time - a potential advantage over the quadratic complexity of transformers.

MAMBA~\cite{MAMBA} exemplifies an SSM-based approach tailored for resource-efficient deployments. It combines a hardware-friendly recurrent structure with a selective focus mechanism that adapts recurrence dynamics based on the input.
A selective SSM layer processes tokens within an MLP block, dynamically adjusting state transitions and filtering out irrelevant data.
Residual connections and normalization layers help stabilize training while preserving signal flow through deeper networks.
By eschewing full attention modules, MAMBA is able to handle long sequences efficiently, making it appealing for devices with limited parallelism or strict memory budgets.

\vspace{5mm}

Collectively, embedders, attention mechanisms, and SSMs address different facets of language modeling. Embedders map tokens into a meaningful vector space, attention mechanisms enable fine-grained context capture across entire sequences, and SSMs provide scalable handling of long-range dependencies. Insights gleaned from each of these components have shaped influential models like BERT and MAMBA, driving rapid advances in NLP.

These same foundational principles guide the development of ultra-compact language architectures for tinyML: selecting or adapting embedders, attention blocks, or SSM layers to fit stringent memory and compute constraints while retaining robust language understanding.

\begin{comment}
Understanding the foundational components underlying various language models is essential to appreciate the innovations and advancements each presented model offers. This section provides an overview of core elements of language models that have collectively advanced natural language processing (NLP). These components, often used in tandem, enable language models to handle increasingly complex tasks by capturing semantic relationships, contextual dependencies, and sequence dynamics.

\subsection{Embedders} 
Embedders are the foundational layer of most language models, responsible for transforming high-dimensional data, such as text, into a continuous vector space. This transformation enables further processing by mapping similar concepts closer together in the vector space, thereby capturing semantic relationships like synonymy, analogy, and context. Traditional embedders, such as Word2Vec \cite{word2vec} and FastText \cite{fasttext}, create static word embeddings where each word is represented by a single vector regardless of context. However, more recently, embedders have been used together with attention mechanisms to create dynamic word representations that are learned in conjunction during training.

In BERT \cite{BERT}, for example, embeddings are created dynamically to capture contextual relationships. BERT incorporates an embedding layer to represent each token in three ways: \begin{itemize} 
    \item Token Embeddings: Represent each token's identity in the input sequence. 
    \item Position Embeddings: Indicate the position of each token in the sequence. 
    \item Segment Embeddings: Distinguish between sentence pairs, which is particularly useful in question-answering tasks. 
\end{itemize} 
These embeddings are then passed to the model's core processing layers, providing a rich, contextually grounded representation that enhances BERT’s ability to capture complex dependencies across tokens. Embeddings, therefore, serve as a fundamental computational block across most language models, anchoring each token in a semantically meaningful vector space.

\subsection{Attention Mechanisms} 
Attention mechanisms, widely adopted in transformer architectures \cite{transformers}, allow models to focus selectively on parts of the input sequence that are most relevant to a given task. By assigning higher weights to significant tokens, attention mechanisms help capture dependencies and interactions between distant tokens, which is critical for understanding language context. The self-attention mechanism used in BERT \cite{BERT} is particularly effective for modeling long-range dependencies in text by computing attention scores across all tokens in both forward and backward directions.

BERT leverages a multi-layer bidirectional transformer encoder, where each layer uses self-attention to establish a context-sensitive representation of each token. This mechanism allows BERT to learn relationships between tokens regardless of their relative positions, providing a comprehensive view of the input sequence. Attention layers contribute to BERT’s ability to capture intricate language structures, making it well-suited for tasks that require a nuanced understanding of word dependencies.

\subsection{State Space Models (SSMs)} 
State space models (SSMs) provide an alternative to transformers for handling sequence data, focusing on modeling sequences through latent states. These models are particularly useful for long sequences, as they can achieve linear time complexity by using recurrence-based structures. Unlike transformers, which rely on attention to maintain context over long inputs, SSMs handle sequence data through a combination of learned state transitions and selective focus on relevant information.

The MAMBA model \cite{MAMBA} is an example of a structured state space model that has been enhanced to address the limitations of traditional SSMs. MAMBA introduces input-dependent parameters to create a selective focus mechanism, enabling the model to dynamically adjust how it processes each token based on input context. This selective approach is implemented through a hardware-aware parallel scan algorithm, which minimizes memory input/output (I/O) and allows MAMBA to perform state transitions recurrently without incurring significant computational costs.

MAMBA's architecture combines SSM layers with simplified multi-layer perceptron (MLP) blocks, forming a repeating structure throughout the model. Each block includes: 
\begin{itemize} 
    \item A selective SSM layer, where recurrence dynamics adapt to the input sequence embedded inside a MLP block. 
    \item Residual and normalization layers to maintain stability during training. 
\end{itemize} 
By making its recurrence dynamics input-dependent, MAMBA can propagate essential information across long sequences, filtering out irrelevant data and achieving higher throughput than standard transformer models on lengthy inputs.

In summary, embedders, attention mechanisms, and SSMs each play distinct roles in enabling language models to process complex sequences effectively. Embedders initialize tokens into semantically rich vector spaces, attention mechanisms selectively capture token interactions over various distances, and SSMs provide computationally efficient solutions for handling long-range dependencies. Together, these components form the foundation of powerful models like BERT and MAMBA, driving advancements in NLP by providing scalable and adaptable solutions for language understanding.
\end{comment}

\section{Downscaling Large Language Models Layers}
\label{sec:components}
\begin{comment}
    With the objective of scaling the requirements of the State-of-the-art Language Models to work in resource-constrained environments, we have analyzed in detail the memory and computational requirements of the models and their components. While the number of parameters of these models is a common metric that is used to compare them, we are not aware of any work in the literature that precisely provides an evaluation of their activation sizes and computational requirements. In this Section, we report the results of the memory evaluation, while in Appendix~\ref{}, we detail the computational evaluation along with the precise mathematical steps that lead to these results.
\end{comment}
Designing state-of-the-art language models (LLMs) for resource-constrained devices demands a clear understanding of both their memory footprint and computational complexity. While comparing models by parameter count is common, few studies offer a precise assessment of activation sizes or computational overhead. Here, we address this gap by systematically evaluating the memory usage of popular LLM architectures and their critical components. In this section, we present our findings on memory requirements in Table~\ref{table:memory_analysis_appendix}; detailed analyses of computational complexity, memory accesses, MACs and the relevant mathematical derivations can be found in Appendix~\ref{}.

By dissecting these metrics, we aim to highlight the primary bottlenecks when attempting to downscale modern LLMs to fit stringent hardware constraints. Our evaluation underscores why a simple parameter count does not fully capture a model’s deployability and highlights the importance of carefully analyzing all components, including embedder, attention layers, and feed-forward blocks. This informs the design choices required to make powerful language models feasible on tinyML and edge platforms.

%-----------------------------------------------------------------------------
% EmbBERT
%-----------------------------------------------------------------------------

\section{EmbBERT: A Novel LM Architecture Designed for Tiny Devices}
\label{sec:EmbBERT}

Rather than merely scaling down hyperparameters or relying on existing compression methods, we propose EmbBERT, an entirely new architecture specifically designed for memory and compute constrained scenarios. 
%Our research integrates insights from multiple recent studies, combined with original contributions, to yield EmbBERT - a novel language model engineered for minimal resource consumption.
%Inspired by the original BERT model~\cite{BERT}, 
% EmbBERT incorporates the efficient embedding strategy from NanoBERT~\cite{NanoBERT} on a BERT-like~\cite{BERT} architecture based on the attention module. We replace the standard attention mechanism with an efficient alternative \cite{efficient_attention} and incorporate an additional module - drawing on the MAMBA approach~\cite{MAMBA} - that exploits both convolutional layers and weighted aggregations. As we will detail in this section, this combination yields a lightweight architecture that maximizes available memory resources.

EmbBERT introduces a novel approach to lightweight model design by reimagining the BERT-like~\cite{BERT} architecture with highly efficient components. Rather than relying on standard attention mechanisms, we integrate a tailored attention module inspired by~\cite{efficient_attention}, which achieves a superior balance between computational efficiency and performance. Moreover, we propose a unique hybrid module that seamlessly combines convolutional layers - drawing inspiration from MAMBA~\cite{MAMBA} and ConvBERT~\cite{ConvBERT} - with weighted aggregation mechanisms, adapted from Differential Transformer~\cite{differential_transformers}. This integrated design not only optimizes memory utilization but also establishes a distinct architectural paradigm, enabling EmbBERT to outperform comparable lightweight models in constrained environments

As illustrated in Figure~\ref{fig:EmbBert}, EmbBERT comprises two major components:
\begin{enumerate}
    \item Embedder Block: Responsible for generating compact yet expressive token representations.
    \item Computational Blocks: A sequence of $n$ blocks that leverage efficient attention, convolutional layers and weighted differences to process embeddings with minimal memory overhead.
\end{enumerate}
An optional Output Block can be appended to adapt the architecture for specific downstream tasks, such as classification or sequence tagging. In the next sections, we delve more deeply into the functionality and design rationale of EmbBERT’s core modules.

\subsection{The Embedder Block}

The Embedder Block in EmbBERT is adapted from the Nano Embedder (detailed in Appendix~\ref{subs:NanoEmbedder}) introduced in \cite{NanoBERT}. This choice reflects the strict memory constraints targeted by our architecture. In typical language models, the embedding layer is often the dominant memory consumer; the Nano Embedder mitigates this issue by learning token and positional embeddings in a \emph{reduced dimensional space} \(r_d\). These compressed representations are then projected to the desired embedding dimension \(d\) via fully connected layers.  

This two-stage approach drastically decreases the overall embedding size while retaining sufficient representational power. By efficiently reallocating memory from the embedding layer to other parts of the network, the Nano Embedder serves as a crucial enabler for running language models on tiny devices with very limited memory.

\subsection{The Computational Block}

EmbBERT’s primary workhorse is a sequence of \( n \) Computational Blocks, each designed to balance expressive power with minimal resource overhead. Four key innovations define the structure of these blocks:
\begin{enumerate}
    \item \emph{Efficient Attention Scheme}: Building on the approach outlined in Appendix~\ref{subsubs:Efficient Attention} from the paper ~\cite{efficient_attention}, we reduce activation cost by employing a single-head attention mechanism inspired by \cite{exploring_transformer_sizes}. This choice reflects empirical findings showing limited benefits of multi-head attention in extremely small models. Leveraging the “efficient attention” formulation from \cite{efficient_attention}, the standard multi-head operation:
   \[
       \text{head}_i = \mathrm{Attention}\left(QW_i^Q,\, KW_i^K,\, VW_i^V\right)
   \]
   is simplified by omitting two of the four weight matrices, resulting in:
   \[
       \text{head}_i = \mathrm{Attention}\left(QW_i^Q,\, K,\, V\right).
   \]
   As a result, the memory footprint is significantly reduced, improving feasibility for devices with tight RAM budgets.
    \item \emph{Unified Feed-Forward and Attention Layers}:  
   Inspired by MAMBA \cite{MAMBA}, we integrate the Feed-Forward and Attention mechanisms within a \emph{single} computational unit, eliminating the standalone feed-forward block typically found in transformer-style architectures. \brav{This consolidation manages to improve performances while not increasing activations, leading to more efficient memory usage and the possibility for simple parallelization in low core architectures}.
    \item \emph{Weighted Difference Skip Connection}:
   We draw on the concept of Differential Transformers \cite{differential_transformers} to replace conventional additive skip connections with a \emph{learned weighted difference} approach. Specifically, two sets of learnable vectors \(\lambda_1,\lambda_2\) and \(\lambda_3,\lambda_4\) produce scaling factors:
   \[
       \lambda_{att} = \exp(\lambda_1 \cdot \lambda_2),\quad
       \lambda_{skip} = \exp(\lambda_3 \cdot \lambda_4).
   \]
   These scalars then weight the difference between the attention output and the skip path:
   \[
       \lambda_{att}\,\mathrm{EffAttention}(X) \;-\; \lambda_{skip}\,\mathrm{Skip}(X).
   \]
   %This approach improves gradient flow and representation quality while keeping overall parameter growth in check.
    \brav{After training these two derived values can also be precoumputed effectively removing from the final structure the four $\lambda$ weight vectors.   
    
    This approach shows improved resistance loss in accuracy due to quantization while still keeping about the same amount of parameters as it's original Efficient Attention counterpart. } 
    \item \emph{1D-Convolution for Local Interaction}:   
   A lightweight 1D-Convolution layer further enhances local context modeling with minimal parameter overhead. Placed inside the skip connection and before the fully connected layer, the 1D-Convolution broadens the network’s capacity to capture short-range dependencies, \brav{mitigating potential information bottlenecks and overparametrization in small-scale architectures.}
\end{enumerate}

By incorporating the above elements, our computational block simultaneously optimizes architecture depth (removing separate norm and feed-forward layers) and leverages memory-efficient operations (1D-Convolution, single-head attention). The net effect is a significantly reduced parameter and activation footprint, enabling the exploration of multiple architectural configurations. This flexibility helps optimize trade-offs between embedding size, attention capacity, and memory - balancing accuracy and deployability of EmbBERT on devices with memory limits in the range of 1-2MB.
\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{imgs/EmbBERT.pdf}
    \caption{The EmbBert architecture, our purpose-built Language Model for Tiny Devices and extreme resource constraints.It is composed of a Nano Embedder and a series of computational blocks in which our main contribution is the use of the new structure which leverages the original skip connection for further processing through the use of a combination of convolutional and fully connected layers plus the use of a weighted difference with learned weights instead of the standard sum.}
    \label{fig:EmbBert}
\end{figure}

\begin{comment}
\brav{
A different approach to the simple reduction of hyperparameters or other compression techniques for the design of language models capable of operating on tiny devices is the design of a new architecture model. We did so by carefully researching and combining some of the most interesting recent studies adding then some ideas of our own.

The result is \textit{EmbBERT}, a novel language model with minimal requirements from the memory and computational point of view. The \textit{EmbBERT} architecture was in principle inspired by the original BERT model~\cite{BERT}, we then proceded to swap it's embedder with the more efficient one proposed in NanoBERT's paper~\cite{NanoBERT}, replacing the standard attention with it's Efficient counterpart presented in~\cite{efficient_attention} to which we added, following the MAMBA~\cite{MAMBA} approach our own section leveraging both capabilities of convolutional layers and weighted aggregations. This way creating a novel lightweight architecture capable of leveraging all the available memory space.

As can be seen in Fig.~\ref{fig:EmbBert}, the EmbBERT architecture is composed of an \textit{Embedder Block} and a sequence of $n$ \textit{Computational Blocks} to which a final \textit{Output Block} can be postponed to adapt the output to the desired task. We will now more in depth analyze the two main blocks.

\subsection{The Embedder Block}

Given the tight constraints that the EmbBERT architecture must face to be ported on tiny devices, the Embedder Block is implemented as the Nano Embedder (described in Appendix~\ref{subs:NanoEmbedder}). Taken from the paper~\cite{NanoBERT} it effectively manages to reduce the size of the embbedder, which usually takes up the most part of LMs, this way leaving more space for the computation. It does so by learning the embeddings in a lower dimensional space $r_d$ for both positions and tokens which then get expanded to the desired dimension $d$ through fully connected layers. 
% While the Output Block, as for all other models we choose to evaluate, is implemented as a dense layer of dimensions $d\_model \times n\_classes$, preceded by a dimensionality reduction layer. The dimensionality reduction layer consists of a MaxPooling layer if the Neural Network is trained from scratch, or it consists of the selection of the first token of length $d\_model$ in the case of pre-trained models, due to the characteristics of the pre-training tasks.

\subsection{The Computational Block}

The core of the EmbBERT architecture consists of $n$ identical \textit{Computational Blocks}, which form the foundation of its design. Each block integrates four key innovations:

\begin{itemize} 
    \item Efficient Attention Scheme: As detailed in the Appendix \ref{subsubs:Efficient Attention}, this mechanism facilitates effective embedding interactions while maintaining low activation costs. By adopting a single-head attention approach, inspired by findings in \cite{exploring_transformer_sizes}, we capitalize on its suitability for smaller models, given the limited benefits of multi-head attention in such contexts. 
    \item Unified Feed-Forward and Attention Layers: Following the insights from MAMBA \cite{MAMBA}, combining the Feed-Forward and Attention mechanisms within a single block enhances overall performance and reduces architectural redundancy. 
    \item Weighted Difference Skip Connection: Building on the concept of Differential Transformers \cite{differential_transformers}, we replace traditional additive skip connections with a weighted difference mechanism, improving gradient flow and feature utilization. 
    \item 1D-Convolution for Local Interaction: Introducing a lightweight 1D-Convolution provides controlled embedding interactions within a fixed context window while imposing minimal parameter overhead. 
\end{itemize}

% \begin{itemize}
%     \item A layer-wise Normalization layer
%     \item An Efficient attention layer, as introduced in Section \ref{subsubs:Efficient Attention}, characterized by fewer parameters, activation sizes, and operations with respect to standard attention. The number of attention head $h$ characterizing this layer was set to $1$ in order to maintain minimal computational and memory demands, noting also the dubious advantages of adopting multi-head attention in smaller models\cite{exploring_transformer_sizes}.
%     \item A layer-wise Normalization layer
%     \item A final Feed-Forward layer, with a bottleneck expansion $\alpha$.
% \end{itemize}

These principles guided the refinement of the Efficient Attention scheme from \cite{efficient_attention} where they prove that the standard multihead attention layer used in the BERT architecture, represented by the following formulas: 
\begin{equation}
    \text{Attention}(Q,\,K,\,V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d}}\right)V
\end{equation}
\begin{equation}
    MH(Q,\,K,\,V) = \text{Concat}(\text{head}_1,\, \text{head}_2,\, ..., \,\text{head}_h)W^O
\end{equation}
\begin{equation}
    \text{where}\  \text{head}_i = \text{Attention}(QW_i^Q,\, KW_i^K,\, VW_i^V)
    \label{eq:attention}
\end{equation}
can be simplified by removing two of the four weight matrices from the equation \ref{eq:attention} resulting in a much slimmer:
\begin{equation}
    \text{head}_i = \text{Attention}(QW_i^Q,\, K,\, V)
    \label{eq:efficientAttention}
\end{equation}

We went on implementing our ides more specifically by augmenting the skip connection with two additional layers: a 1D-Convolution and a fully connected layer, separated by a SiLU activation function. Instead of merging the outputs via simple addition, we employed a learned weighted difference inspired by Differential Transformers \cite{differential_transformers}.
This mechanism uses four learnable vectors of size $d$ denoted \(\lambda_i\) to compute two scalars:
\[
    \lambda_{att} = \exp(\lambda_1 \cdot \lambda_2),\quad
    \lambda_{skip}  = \exp(\lambda_3 \cdot \lambda_4)
\]
which are used to weight the difference:
\[
    \lambda_{att} \ \mathrm{EffAttention}(X) - \lambda_{skip} \ \mathrm{Skip}(X) 
\]

By substituting this modified block for the standard attention mechanism, we eliminate the need for the second Norm and Feed-Forward block in the standard BERT architecture. This change not only avoids increasing activation size but also reduces overall weight count, thanks to the efficiency of the 1D-Convolution layer.

The optimized layers used in the block thus make it possible to adopt multiple effective configurations for the hyperparameters of the block, enabling the explorations of better trade-offs in the memory budget allocation between weights and activations, and between the memory allocated to the \textit{Computational Block} and the one dedicated to the \textit{Embedder}.
}
\end{comment}


%-----------------------------------------------------------------------------
% Problem definition
%-----------------------------------------------------------------------------

\begin{comment}
\section{The benchmark suite \textit{TinyNLP}}
\label{sec:tinyNLP}

\mas{In the \gls{tinyml} context, special attention must be given to the understanding of the problem at hand. It is unreasonable, in this context, to expect the model to achieve the same accuracy as models with billions of parameters working in a cloud environment. Furthermore, the majority of applications on embedded devices, as highlighted in the introduction, do not require the complex analysis and depth of information characterizing \glspl{llm}. For these reasons, we designed a benchmark suite, namely \textit{TinyNLP}, including different datasets and tasks, for the evaluation of on-device \glspl{tlm}. 
}
The tasks included in the suite are:
\brav{
\begin{itemize}
    \item \textit{Request Classification}: particularly relevant in the TinyML context, it involves identifying and categorizing different types of requests made in text, such as distinguishing between a request for information, action, or assistance. It’s often used in customer service chatbots or virtual assistants to understand and appropriately respond to user queries.
    \item \textit{Document classification}: consists in categorizing texts into predefined groups based on their content. Examples include classifying news articles into topics (e.g., sports, politics) or organizing emails into categories like spam or important.
    \item \textit{Sentiment Analysis}: this task involves determining the emotional tone or opinion expressed in a piece of text. It typically classifies the sentiment as positive, negative, or neutral, and is commonly used to analyze customer reviews, social media posts, or feedback.
    \item \textit{Racism Analysis}: This task, somewhat like sentiment analysis, focuses on detecting and identifying racist language or content in text. It involves analyzing text for discriminatory, harmful, or biased language, often used for content moderation or social studies research
    \item \textit{Movement presence}: while not being strictly relevant in the TinyML context it is a more complex task with the goal of identifying references to physical movements or actions within a text. For example, identifying when a subject mentions or implies walking, running, or gesturing, which could be useful in applications like smart devices or human-robot interaction.
    
\end{itemize}
}
The proposed \textit{TinyNLP} includes the datasets reported in Table \ref{table:tiny_datasets}. 

\input{tables/tab_datasets}

Each of the datasets \mas{present in the \textit{TinyNLP} benchmark suite}, as well as all other datasets used, was subdivided into train, evaluation, and test sets by following either one of the following rules in this order: 
\begin{enumerate}
    \item If the dataset was already previously subdivided into train, validation, and test by the creators those splits were used. 
    \item If the dataset contained just one split we opted to use the biggest one as training and the smaller one as a test. From the training, we then take 10\% to be used as validation.
    \item  If the dataset did not contain any split we opted to subdivide first into a 90-10 split for training and test set then another 10\% was taken from the training set to be used as validation.
\end{enumerate}

\end{comment}

%-----------------------------------------------------------------------------
% Experimental setup
%-----------------------------------------------------------------------------
\section{Experimental setup}
\label{sec:setup}

In this section, we describe our overarching experimental design, which involves training several model variants under a strict 2\,MB memory budget. We adopt two distinct training approaches and evaluate a range of architectures - including both established baselines and newly proposed models - to better understand their performance-memory trade-offs.
\input{tables/tab_hypp&memory}
\subsection{Training Approaches}

We explore two primary methodologies for training our models:
\begin{enumerate}
    \item Direct Training (No Pretraining):  
   Each model is initialized randomly and trained from scratch for an extended number of epochs on a specific downstream dataset. This approach assesses how effectively a model can learn task-specific features within severe memory limits without benefiting from large-scale pretraining.
   \item BERT-Style Pretraining + Fine-Tuning:  
   We pretrain models using a masked language modeling (MLM) approach akin to BERT~\cite{BERT}, followed by fine-tuning on individual datasets. This strategy evaluates whether partial knowledge transfer from a larger corpus significantly boosts performance, despite our tight memory constraints.
\end{enumerate}

We deliberately exclude ELECTRA-style training~\cite{ELECTRA} in this work. ELECTRA relies on two models - a generator and a discriminator - designed with one being roughly half the size of the other. Under our 2 MB constraint, it proved infeasible to construct a sufficiently large generator while still accommodating a capable discriminator.

\subsection{Models}
\label{subs:models}

We test a diverse set of architectures, each strictly capped at 2 MB of total memory (parameters + activations). Appendix~\ref{sec:componentsAppendix} provides in-depth block-level descriptions; here we summarize the key characteristics of each model:

\begin{enumerate}
    \item BERT-2MB: 
   A downsized version of the original BERT architecture \cite{BERT}, preserving standard embedders and encoder blocks.
   \item MAMBA-2MB:  
   A minimal adaptation of MAMBA~\cite{MAMBA}, using its native embedder and SSM-based recurrent blocks.
   %\item NanoBERT-2MB:
   %Implements NanoBERT \cite{NanoBERT} exactly, combining the Nano Embedder with standard BERT encoder blocks.
   %\item BERT-Efficient-2MB:  
   %Adopts the standard BERT embedder but replaces multi-head attention with the “Efficient Attention” approach proposed in~\cite{efficient_attention}.
   \item Embedder Only:  
   Uses the Nano Embedder \cite{NanoBERT} \emph{alone}, with no mechanism for token interaction. Although not a fully functional language model, this baseline reveals how much parameter budget is consumed by the embedder itself \brav{and how capable it can be on it's own}.
   \item Embedder + Conv:  
   Extends the \textit{Embedder Only} setup with a single 1D convolutional layer to enable local token interactions within a fixed size context window, with minimal parameter overhead.
   %\item NanoBERT Efficient:  
   %Combines the Nano Embedder and Efficient Attention, aiming to reduce activation sizes enough to accommodate more parameters under the same 2 MB limit.
   \item EmbBERT:  
   Our proposed architecture, expanding upon NanoBERT Efficient by modifying attention and adding lightweight convolution-aggregation blocks (detailed in Section~\ref{sec:EmbBERT}).
\end{enumerate}

As an external reference point, we also include BERT-Tiny - a well-known minimal variant of BERT developed by the original authors - which is approximately \emph{ten times} larger than our 2 MB models. Although much larger, it serves as a useful performance benchmark.

\subsection{Model Characterization}

Table ~\ref{table:models_params_and_memory} provides an overview of each model’s architectural parameters, weithgs count, and activation sizes. These metrics jointly determine the final memory footprint, illustrating how various design choices trade off capacity for memory. In Appendix ~\ref{appendix:??} we report an in depth analisys of how to obtain for each layer constituting our models their respective activation sizes and weight count. 
% \begin{enumerate}
%     \item Table~\ref{table:hyperparameters}: Summarizes key architectural paramaeters (e.g., embedding dimension, number of attention heads, feed-forward size) across all models.
%     \item Table~\ref{table:memory}: Reports parameter and activation memory usage, validating that each architecture fits within our 2 MB limit.
% \end{enumerate}

% By comparing these architectures, we aim to highlight the design strategies that best balance performance and memory feasibility. The next section details our training protocols, datasets, and evaluation metrics, thereby contextualizing the performance results presented later in Section~\ref{sec:results}.

By comparing these architectures, we aim to highlight the design strategies that best balance performance and memory feasibility. Our experimental campaign is massive and reproducible, as it involved training 8 different models across a total of 17 datasets and fine-tuning each model 5 times to compute average values for comparison. The next section details our training protocols, datasets, and evaluation metrics, thereby contextualizing the performance results presented later in Section~\ref{sec:results}.





\subsection{Datasets}

We evaluate our models on two primary benchmarks:
\begin{enumerate}
    \item tinyNLP Benchmark Suite: A newly introduced dataset collection specifically tailored for the \emph{Tiny Language Model (TLM)} context.  
    \item GLUE Benchmark \cite{GLUE}: A widely adopted NLP benchmark composed of multiple more complex datasets (see Table \ref{table:GLUE_datasets}).
\end{enumerate}

Each dataset in the \textit{tinyNLP} suite and the GLUE benchmark is split into training, validation, and test sets using one of the following procedures, applied in order:
\begin{enumerate}
    \item Provided splits: If the dataset’s creators already supplied official train, validation, and test splits, we use those directly.
    \item Single split: For datasets with only one official split, we designate the largest portion as the training set and the smaller portion as the test set. We then extract 10\% of the training portion to serve as the validation split.  
    \item No provided splits: If the dataset lacks any splitting, we first partition it into a 90–10 ratio for training and test, then further withhold 10\% of the training portion for validation.  
\end{enumerate}

For model pretraining, we rely on the open-source \href{https://huggingface.co/datasets/bookcorpus/bookcorpus}{Book Corpus} dataset, which contains over 7,000 books spanning diverse topics and writing styles. This corpus supplies a broad linguistic foundation, from which we fine-tune each model on the aforementioned benchmarks.  

The following subsections detail both the \textit{tinyNLP} suite and the GLUE tasks used to assess each model’s performance.

\begin{comment}
To evaluate our models, we used two benchmarks:
\begin{itemize}
    \item A novel benchmark, specifically designed for the \gls{tlm} context, named the \textit{tinyNLP} benchmark suite, and 
    \item the widely-used GLUE benchmark \cite{GLUE}, which comprises several datasets (see Table \ref{table:GLUE_datasets}). 
\end{itemize}

Each of the datasets present in the \textit{TinyNLP} and GLUE benchmark suite, was subdivided into train, evaluation, and test sets by following either one of the following rules in this order: 
\begin{enumerate}
    \item If the dataset was already previously subdivided into train, validation, and test by the creators those splits were used. 
    \item If the dataset contained just one split we opted to use the biggest one as training and the smaller one as a test. From the training, we then take 10\% to be used as validation.
    \item  If the dataset did not contain any split we opted to subdivide first into a 90-10 split for training and test set then another 10\% was taken from the training set to be used as validation.
\end{enumerate}

Furthermore, for the pretraining of the models, we employed the open-source \href{https://huggingface.co/datasets/bookcorpus/bookcorpus}{Book Corpus} dataset, which consists of over 7'185 books, providing a rich collection of diverse and contextually varied phrases. 
%These texts offer detailed descriptive explanations for visual content, characters, and interactions, making them an ideal resource for tasks that require a nuanced understanding of language. Its broad range of genres and styles also helps enhance the models' ability to generalize across different types of textual inputs.
We now proceed to describe the two benchmarks used for evaluation.
\end{comment}

\subsubsection{The \textit{TinyNLP} benchmark suite}
\label{sec:tinyNLP}

To better evaluate \acrshort{tlm}s in the real-world scenarios they are expected to operate, we introduce the \textit{TinyNLP} benchmark suite. This curated collection of existing datasets reflects the constrained yet practical applications of language models on embedded devices.

Drawing on the discussion in Section~\ref{sec:introduction}, the \textit{TinyNLP} suite centers on tasks that are narrower in scope and relatively less demanding than standard \acrshort{llm} benchmarks. These tasks fall into three comprehensive categories:
\begin{enumerate}
    \item Request Classification: Relevant to virtual assistants in TinyML contexts, these datasets involve discerning the type of user request (e.g., requests for information, action, or assistance). As Dataset focused on this kind of task, we have included in the \textit{TinyNLP} benchmark the nlu ~\cite{nlu_dataset} and Snips datasets.
    \item Sentiment Analysis: Focuses on determining the emotional tone or opinion expressed in text. This commonly involves classifying content as positive, negative, or neutral, and sees wide usage in analyzing customer reviews or social media feedback. As Dataset focused on this kind of task, we have included in the \textit{TinyNLP} benchmark the IMDb \cite{imdb_dataset} and Emotion \cite{emotion_dataset} datasets.
    \item Context Understanding: Involves identifying the broader context in which text is generated. For example, distinguishing whether the text describes a particular situation or environment. As Dataset focused on this kind of task, we have included in the \textit{TinyNLP} benchmark the ag\_news \cite{ag_news_dataset}, cyberbull\cite{cyberbull_dataset} and LiMiT \cite{limit_dataset} datasets.
\end{enumerate}
An overview of the datasets included in the \textit{TinyNLP} suite can be found in Table~\ref{table:tiny_datasets}. These datasets collectively represent application scenarios well-suited to models with restricted memory footprints, reinforcing the practical aim of assessing \acrshort{tlm} deployments on embedded devices.
\input{tables/tab_datasets_tiny}

\subsubsection{The GLUE benchmark}
The General Language Understanding Evaluation (GLUE) benchmark is a diverse suite of tasks designed to assess a model’s ability to understand and process natural language. It encompasses multiple subtasks, including sentiment classification and regression on sentence pairs. Because the official GLUE labels are only publicly released for the training and validation splits—and in line with prior approaches (e.g., \cite{exploring_transformer_sizes}) - we treat the validation set as our test split throughout this study.
\input{tables/tab_dataset_glue}

\begin{comment}
In order to better evaluate \gls{tlm} in the situations in which we expect them to work in, we propose to use a series of dataset already present in the literature, constituting a benchmark we named the \textit{TinyNLP} benchmark suite.

Following the characteristics of the applications of language models on embedded devices highlighted in the introduction, the \textit{TinyNLP} benchmark suite encompasses datasets for tasks with relatively limited scope, that are relatively easier with respect to standard \gls{llm} benchmarks. 
 
The tasks included in the suite can be organized in 3 macro-areas:

\begin{itemize}
    \item \textit{Request Classification}: particularly relevant in the TinyML context, tasks in this area involve identifying and categorizing different types of requests made in text, such as distinguishing between a request for information, action, or assistance. It’s often used in customer service chatbots or virtual assistants to understand and appropriately respond to user queries.
    \item \textit{Sentiment Analysis}: tasks in this area involve determining the emotional tone or opinion expressed in a piece of text. It typically classifies the sentiment as positive, negative, or neutral, and is commonly used to analyze customer reviews, social media posts, or feedback.
    \item \textit{Context understanding}: tasks in this area revolves around understanding the general context in which a text was pronounced/written, or if it describes particular situations. 
    %\item \textit{Document classification}: consists in categorizing texts into predefined groups based on their content. Examples include classifying news articles into topics (e.g., sports, politics) or organizing emails into categories like spam or important.
    %\item \textit{Racism Analysis}: This task, somewhat like sentiment analysis, focuses on detecting and identifying racist language or content in text. It involves analyzing text for discriminatory, harmful, or biased language, often used for content moderation or social studies research
    %\item \textit{Movement presence}: while not being strictly relevant in the TinyML context it is a more complex task with the goal of identifying references to physical movements or actions within a text. For example, identifying when a subject mentions or implies walking, running, or gesturing, which could be useful in applications like smart devices or human-robot interaction.
\end{itemize}

The proposed \textit{TinyNLP} includes the datasets reported in Table \ref{table:tiny_datasets}. 

The GLUE benchmark is designed to test a model's ability to understand natural language by presenting various tasks such as sentiment classification, and regression on sentence pairs. As for the GLUE benchmark since only training and validation splits have publicly available labels we opted, as other papers like \cite{exploring_transformer_sizes} do, to use the validation as a test split.
\end{comment}

\subsection{Training and pretraining}
\label{subs:training_pretraining}

We train each model using a dataset-specific Byte Pair Encoding (BPE) tokenizer. Depending on the model’s capacity and requirements, the vocabulary size is set to either 8,192 or 2,048. Each model is then trained on its corresponding dataset for 20 epochs across at least 1,000 batches (batch size = 128) using the AdamW optimizer, with the learning rate fixed at either \(1 \times 10^{-3}\) or \(2 \times 10^{-3}\).\footnote{We exclude detailed learning rate schedulers to avoid potential biases; different models may respond distinctly to specific schedules. Future work could systematically explore scheduling for these models.}

\paragraph{Validation and Model Selection.}
We periodically evaluate on a validation set (five times per epoch) and store the best-performing checkpoint, judged by the Matthews correlation coefficient (MCC) for classification tasks or the Spearman correlation coefficient (SCC) for regression. The checkpoint with the highest validation metric is subsequently used for final testing.

\paragraph{Task-Specific Output Layer}
\brav{
This layer varies depending on the model considered as such:
\begin{itemize}
    \item For the embedder only model and embedder + convolution we adopted a Max pooling along the sentence length and then a fully connected layer to bring the size down to the number of classes of the dataset.
    \item For MAMBA, due to it's left to right analysis of the phrase we adopted a Max pooling for the unpretrained kind and a RMS pooling for it's pretrained counterpart with the same fully connected final layer as before.
    \item All the remaining models output was handled by appending a \texttt{<CLS>} token at the start of the phrase and then using the attention pooled information in that token in conjunction with a final fully connected layer as for the other models.
\end{itemize}
}


\paragraph{Pretraining Protocol}
For models supporting BERT-style pretraining, we use the publicly available BookCorpus dataset. We construct sentence pairs for masked language modeling (MLM) and next-sentence prediction (NSP) by pairing half of the sentences contiguously and the other half randomly. Within each pair, every token has a 1/6 chance of being masked: 70\% of the time it is replaced by the  \verb|<MASK>| token, 15\% of the time substituted with a random token, and 15\% of the time left unchanged. This strategy encourages contextual reasoning rather than random guessing.

Pretraining is carried out for up to one epoch with a batch size of 32 and a learning rate of \(5 \times 10^{-4}\). We then fine-tune each model on the target datasets for 10 epochs (at least 1,000 batches), using a learning rate of \(3 \times 10^{-4}\) and the same validation procedure detailed above.

% \paragraph{Handling the CLS Token.}
% For most models, we prepend a \verb|<CLS>| token to each input to serve as a global representation for NSP classification. However, MAMBA, \emph{Embedder-only} and \emph{Embedder+Conbolution} use an RMS pooling layer instead, because they either lack the capability to aggregate information from distant enough tokens or for MAMBA it's left-to-right processing proved incompatible with \verb|<CLS>|-based aggregation. 

\paragraph{Applicability to Different Architectures}
We evaluate both \emph{pretrained} and \emph{unpretrained} versions of our architectures on the tinyML benchmark (see Section~\ref{sec:results}). Exceptions include the \emph{Embedder-only} and \emph{Embedder+Convolution} models, which lack structural mechanisms (e.g., attention) to meaningfully integrate global context. Given this limitation, we do not apply BERT-style pretraining to those models. Instead, we directly train them on each target dataset and report their best results.

\begin{comment}
Initially, each dataset was used to train a dataset-specific \gls{bpe} tokenizer with dictionary dimensions of either 8,192 or 2,048, depending on the requirements of each model. After tokenization, we trained each model on its respective dataset for 20 epochs across at least 1000 batches of size 128, using the AdamW optimizer with a \gls{lr} of either \(1\cdot 10^{-3}\) or \(2\cdot 10^{-3}\).\footnote{We opted not to use specific learning rate schedulers to avoid potential biases in results, as different models respond variably to different schedulers. \fab{We leave the search for the optimal learning rate scheduler for each model as a further improvement of our benchmark models.}} Evaluation on the validation set was performed five times per epoch, saving the best model based on the \gls{mcc} score for classification or the \gls{scc} for regression, and using this model for final testing.

For classification and regression tasks, we implemented a special layer comprising a max pooling operation over the sentence length followed by a fully connected layer. The input size of this layer matched the embedding size, and its output size corresponded to the number of classes.

For pretraining, we employed the open-source Book Corpus dataset, structuring sentence pairs for BERT-like pretraining tasks using both \acrfull{mlm} and \acrfull{nsp}. In generating \gls{nsp} labels, we paired half of the phrases consecutively and the other half randomly, with a special \verb|<SEP>| token dividing each pair. During masked language modeling, each token in these pairs had a $1/6$ chance of being selected for masking, with a 70\% chance of being replaced by the \verb|<MASK>| token, a 15\% chance of being swapped with another random token, and a 15\% chance of remaining unchanged. This approach encouraged the model to reconstruct masked tokens based on nearby embeddings without defaulting to random guesses.

These models were pretrained on the entire dataset, using a maximum of one epoch, batches of size 32, and a learning rate of \(5 \cdot 10^{-4}\). Following pretraining, each model was fine-tuned as described above for 10 epochs on at least 1000 batches of size 32, with a learning rate of  \(3 \cdot 10^{-4}\). 

During both pretraining and fine-tuning, a \verb|<CLS>| token was inserted at the beginning of each phrase to aggregate information for \gls{nsp}. The embedding of this token was passed through a fully connected layer for the final classification, while for MAMBA, an \gls{rms} pooling layer was used instead due to challenges the model faced in utilizing the \verb|<CLS>| token effectively during fine-tuning with its left-to-right processing.

To evaluate our models’ performance, we first trained them directly on the dataset at hand without any pretraining. For all models, except those limited in their capacity to aggregate embeddings, such as the Embedder-only and Embedder+Convolution models, which lack the structural ability to integrate phrase-wide context effectively, we also conducted pretraining on the Book Corpus dataset before fine-tuning on both benchmarks. From all these trainings we report in Section \ref{sec:results} the best results obtained by each model either in pretrained or unpretrained contexts of the tinyML benchmark. 

%More specifically MAMBA, Embedder and Embedder + Convolution have their unpretrained results shown while the others show the pretrained ones.

% \subsection{Training only}
% \label{sec:training}

% Firstly, each dataset is used to train a \gls{bpe} tokenizer for that specific dataset with both 8'192 and 2'048 as dictionary dimensions since those two values are the ones used by the different models.
% Subsequently, each model is trained on the dataset tokenized by the dataset's specific tokenizer for 20 epochs on at least 1000 batches of size 128 using the AdamW optimizer with a \gls{lr} of either \(1\cdot 10^{-3}\) or \(2\cdot 10^{-3}\). \footnote{\haz{We decided not to use any specific lr schedulers since different models react differently to different schedulers removing this way potential biases in the results.}} Evaluation on the validation set was performed five times each epoch saving the best model based on the \gls{mcc} score for classification or the \gls{scc} for regression each time and keeping this model for the final evaluation on the test set.

% For classification and regression tasks, we incorporated a special layer composed of max pooling over the sentence length and a fully connected layer with input size the embedding size and output size the class number.


% \subsection{Pretraining and Finetuning}
% \label{sec:pretr-finet}

% \mas{\textbf{TODO: Dire che abbiamo calcolato i risultati sia con e senza pre-training, e che abbiamo scelto di presentare i risultati nella versione risultata migliore sul tinynlp benchmark sui due dataset.}}

% For the pretraining we used the open source \href{https://huggingface.co/datasets/bookcorpus/bookcorpus}{Book Corpus} dataset where each phrase in the dataset was used to compose the ones needed for a BERT like training with both \gls{mlm} and \gls{nsp}.
% To construct \gls{nsp} labels, pairs of phrases were joined half in a consecutive manner and half randomly with a special \verb|<SEP>| token dividing the two. Each of the tokens in these sentence pairs had a $ 1/6 $ chance of being selected for masking with a 70\% chance of being replaced with the special token \verb|<MASK>|, a 15\% chance of being swapped with another random dictionary token and for the remaining 15\% remaining unchanged. This forced the model to reconstruct missing tokens from the other nearby embeddings during training without randomly guessing.
% % The models trained using this method were only the attention-based ones and MAMBA since the embedding-only ones cannot meaningfully learn from such pretraining due to their inability to combine semantic context across tokens.
% Each of these models was trained until convergence on the complete dataset, with at most one epoch, batches of 32, and a learning rate of \(5 \cdot 10^{-4}\).

% \haz{
% After pretraining, each model was fine-tuned as previously described in Section \ref{sec:training}, for 10 epochs on at least 1000 batches of size 32 with a learning rate of \(3 \cdot 10^{-4}\)}
% % After pretraining, each model was fine-tuned as previously described in Section \ref{sec:training}, using the same tokenizer used for the pretraining. The models were trained for 10 epochs on at least 1000 batches of size 32 with a learning rate of \(3 \cdot 10^{-4}\). 

% During the pretraining and finetuning process, a special token \verb|<CLS>| was placed at the beginning of each phrase to indicate information aggregation for \gls{nsp}. The embedding from this position was passed through a fully connected layer that outputted the correct number of classes for that dataset. For MAMBA, an \gls{rms} pooling layer replaced \verb|<CLS>| token due to its inability to learn meaningfully with it during fine-tuning due to the way it processes input from left to right.
\end{comment}
\brav{
\subsection{Quantization} 
\label{subs:quantization}

To optimize computational efficiency without compromising performance, we employ a selective quantization strategy using the Hugging Face BitsAndBytes library. This approach applies 8-bit floating-point representation to weights within a range of ±6, reducing memory usage while preserving critical precision. Weights outside this range are stored in 16-bit floating-point (FP16) format to ensure numerical stability for extreme values. Additionally, all activations, initially in 32-bit floating-point (FP32), are converted to FP16, significantly reducing memory and computational overhead during both training and inference.

Quantization is applied to the models that performed best after the initial fine-tuning phase. These quantized models are then further fine-tuned for an additional two epochs using parameter-efficient fine-tuning (PEFT). This technique modifies only a small subset of parameters (approximately 8% of the total weights), focusing primarily on task-specific layers to refine performance while retaining the benefits of reduced precision.

The fine-tuning process for quantized models is optimized using the 8-bit AdamW optimizer with a fixed learning rate of $1\times 10^{-4}$. This optimizer is specifically designed to handle reduced-precision computations efficiently, ensuring stable updates to parameters under quantization.

This quantization and refinement process ensures the models are both computationally efficient and capable of delivering high performance on the target tasks, aligning with our broader goal of optimizing models for deployment in constrained environments.
}
\subsection{Evaluation}
\label{sec:eval}
After training, each model’s performance was evaluated on every dataset using average loss, accuracy, F1 score, precision, recall, Matthews Correlation Coefficient (MCC), and confusion matrices. For the STSB dataset—treated as a regression task in the GLUE benchmark—we report only average loss and Spearman’s correlation coefficient (SCC).

We focus on accuracy and MCC for the tinyML datasets, as these metrics best capture binary or multiclass classification performance under memory constraints. For the GLUE tasks, we follow the standard benchmark protocol: SCC for STSB, MCC for CoLA, F1 score for QQP and MRPC, Accuracy for the remaining GLUE tasks.
To ensure statistical significance, every model was trained or fine-tuned from scratch on each dataset five times, and the mean metric across these runs is reported. All code, scripts, results, and model checkpoints can be found on our \href{https://github.com/RiccardoBravin/tiny-LLM}{GitHub repository}.

\begin{comment}
After training, each model's performances on each dataset was evaluated using average loss, accuracy, F1 score, precision, recall, \gls{mcc} and confusion matrix. For the STSB dataset, which is a regression task as per GLUE benchmark requirements, only average loss and \gls{scc} were reported. Hereby are reported only the accuracy and \gls{mcc} for the tinyML datasets since these two are the most relevant ones while for the GLUE benchmark only the metric necessary for the score has been reported. The required metrics to calculate the scores are \gls{scc} for STSB, MCC for CoLA, F1 score for QQP and MRPC and accuracy for the remaining tasks. 

To ensure statistical significance, each model was trained or fine-tuned from scratch on every dataset five times, with the mean metrics reported. Complete results are available at our project's \href{https://github.com/RiccardoBravin/tiny-LLM}{GitHub page}.
\end{comment}

\begin{comment}
For our experiments, we trained a variety of models using two different approaches. The first approach involves training each model for extended duration on a specific dataset without any pretraining. Alternatively, the second approach employs a pretraining process similar to BERT \cite{BERT}, followed by fine-tuning the models on individual datasets. We deliberately avoided using the  ELECTRA training, from \cite{ELECTRA}, due to its requirement of two separate models, one generator and one discriminator, which are suggested to be sized to be one about a half of the other and, being limited on the size of the discriminator, a small enough and capable generator was not possible to be built. 

\mas{\textbf{TODO: rewrite a proper introduction to this section. Just present the incoming chapters}}
\brav{
\subsection{Models}
\label{subs:models}

For our testing we hereby propose a series of models that enable us to verify their different capabilities at the 2MB memory limit imposed.
Each model's internal blocks are in depth described in the Appendix \ref{??} and hereby we report each of them with just an overall summary of their constituents:
\begin{itemize}
    \item BERT(2MB): This is the standard BERT architecture composed of the standard embedder and relative encoder blocks
    \item MAMBA(2MB): This is the standard MAMBA architecture composed of the standard embedder and MAMBA blocks with \gls{ssm}
    \item NanoBERT(2MB): This is the original implementation of the NanoBERT model proposed in \cite{NanoBERT} which is constituted of a NanoEmbedder and standard Encoder blocks
    \item BERT Efficient(2MB): This model implies the use of the standard embedder with encoder blocks that leverage the newly proposed Efficient Attention from \cite{efficient_attention}
    \item Embedder: This model is composed of just an Nano Embedder and has been tested on the rationale that embedders usually take up most of the weights of the model so testing a model with just these capabilities might be interesting despite it's inability to make embeddings of different tokens interact
    \item Embedder + Conv: To overcome the Embedder only model lack of embedding interaction we opted to add a simple 1D convolutional layer to reintroduce a fixed size context window of interactability between embeddings without increasing the weights too much
    \item NanoBERT Efficient: This is a new architecture that joins the Nano Embedder and the Efficient Attention schemes to achieve lower activations and thus higher parameter count in the limited memory available
    \item EmbBERT: our own model that modifies the NanoBERT Efficient's attention as described in \ref{sec:EmbBERT}
\end{itemize}

To these eight model we also add BERT-Tiny, a \gls{sota} model from the creators of BERT, which is ten times bigger than any of our proposals as a way to compare performances at scale.

To have a better view of the characteristics of each presented model we report both in Table \ref{table:hyperparameters} each of the hyperparameters of the different models and in table \ref{table:memory} their respective weight count, activation sizes and total memory occupation.


\input{tables/tab_model_parametres}
\input{tables/tab_models_hyperparameters}

}
\subsection{Training and pretraining}
\label{subs:training_pretraining}


Initially, each dataset was used to train a dataset-specific \gls{bpe} tokenizer with dictionary dimensions of either 8,192 or 2,048, depending on the requirements of each model. After tokenization, we trained each model on its respective dataset for 20 epochs across at least 1000 batches of size 128, using the AdamW optimizer with a \gls{lr} of either \(1\cdot 10^{-3}\) or \(2\cdot 10^{-3}\).\footnote{We opted not to use specific learning rate schedulers to avoid potential biases in results, as different models respond variably to different schedulers.} Evaluation on the validation set was performed five times per epoch, saving the best model based on the \gls{mcc} score for classification or the \gls{scc} for regression, and using this model for final testing.

For classification and regression tasks, we implemented a special layer comprising a max pooling operation over the sentence length followed by a fully connected layer. The input size of this layer matched the embedding size, and its output size corresponded to the number of classes.

For pretraining, we employed the open-source Book Corpus dataset, structuring sentence pairs for BERT-like pretraining tasks using both \acrfull{mlm} and \acrfull{nsp}. In generating \gls{nsp} labels, we paired half of the phrases consecutively and the other half randomly, with a special \verb|<SEP>| token dividing each pair. During masked language modeling, each token in these pairs had a $1/6$ chance of being selected for masking, with a 70\% chance of being replaced by the \verb|<MASK>| token, a 15\% chance of being swapped with another random token, and a 15\% chance of remaining unchanged. This approach encouraged the model to reconstruct masked tokens based on nearby embeddings without defaulting to random guesses.

These models were pretrained on the entire dataset, using a maximum of one epoch, batches of size 32, and a learning rate of \(5 \cdot 10^{-4}\). Following pretraining, each model was fine-tuned as described above for 10 epochs on at least 1000 batches of size 32, with a learning rate of  \(3 \cdot 10^{-4}\). 

During both pretraining and fine-tuning, a \verb|<CLS>| token was inserted at the beginning of each phrase to aggregate information for \gls{nsp}. The embedding of this token was passed through a fully connected layer for the final classification, while for MAMBA, an \gls{rms} pooling layer was used instead due to challenges the model faced in utilizing the \verb|<CLS>| token effectively during fine-tuning with its left-to-right processing.

To evaluate our models’ performance, we first trained them directly on both the \gls{tinyml} (presented in Section \ref{sec:tinyNLP}) and GLUE benchmarks without any pretraining. For all models, except those limited in their capacity to aggregate embeddings, such as the Embedder-only and Embedder+Convolution models, which lack the structural ability to integrate phrase-wide context effectively, we also conducted pretraining on the Book Corpus dataset before fine-tuning on both benchmarks. From all these trainings we report in tables \ref{table:results_tiny} and \ref{table:results_GLUE} the best results obtained by each model either in pretrained or unpretrained contexts of the tinyML benchmark. More specifically MAMBA, Embedder and Embedder + Convolution have their unpretrained results shown while the others show the pretrained ones. 



% \subsection{Training only}
% \label{sec:training}

% Firstly, each dataset is used to train a \gls{bpe} tokenizer for that specific dataset with both 8'192 and 2'048 as dictionary dimensions since those two values are the ones used by the different models.
% Subsequently, each model is trained on the dataset tokenized by the dataset's specific tokenizer for 20 epochs on at least 1000 batches of size 128 using the AdamW optimizer with a \gls{lr} of either \(1\cdot 10^{-3}\) or \(2\cdot 10^{-3}\). \footnote{\haz{We decided not to use any specific lr schedulers since different models react differently to different schedulers removing this way potential biases in the results.}} Evaluation on the validation set was performed five times each epoch saving the best model based on the \gls{mcc} score for classification or the \gls{scc} for regression each time and keeping this model for the final evaluation on the test set.

% For classification and regression tasks, we incorporated a special layer composed of max pooling over the sentence length and a fully connected layer with input size the embedding size and output size the class number.


% \subsection{Pretraining and Finetuning}
% \label{sec:pretr-finet}

% \mas{\textbf{TODO: Dire che abbiamo calcolato i risultati sia con e senza pre-training, e che abbiamo scelto di presentare i risultati nella versione risultata migliore sul tinynlp benchmark sui due dataset.}}

% For the pretraining we used the open source \href{https://huggingface.co/datasets/bookcorpus/bookcorpus}{Book Corpus} dataset where each phrase in the dataset was used to compose the ones needed for a BERT like training with both \gls{mlm} and \gls{nsp}.
% To construct \gls{nsp} labels, pairs of phrases were joined half in a consecutive manner and half randomly with a special \verb|<SEP>| token dividing the two. Each of the tokens in these sentence pairs had a $ 1/6 $ chance of being selected for masking with a 70\% chance of being replaced with the special token \verb|<MASK>|, a 15\% chance of being swapped with another random dictionary token and for the remaining 15\% remaining unchanged. This forced the model to reconstruct missing tokens from the other nearby embeddings during training without randomly guessing.
% % The models trained using this method were only the attention-based ones and MAMBA since the embedding-only ones cannot meaningfully learn from such pretraining due to their inability to combine semantic context across tokens.
% Each of these models was trained until convergence on the complete dataset, with at most one epoch, batches of 32, and a learning rate of \(5 \cdot 10^{-4}\).

% \haz{
% After pretraining, each model was fine-tuned as previously described in Section \ref{sec:training}, for 10 epochs on at least 1000 batches of size 32 with a learning rate of \(3 \cdot 10^{-4}\)}
% % After pretraining, each model was fine-tuned as previously described in Section \ref{sec:training}, using the same tokenizer used for the pretraining. The models were trained for 10 epochs on at least 1000 batches of size 32 with a learning rate of \(3 \cdot 10^{-4}\). 

% During the pretraining and finetuning process, a special token \verb|<CLS>| was placed at the beginning of each phrase to indicate information aggregation for \gls{nsp}. The embedding from this position was passed through a fully connected layer that outputted the correct number of classes for that dataset. For MAMBA, an \gls{rms} pooling layer replaced \verb|<CLS>| token due to its inability to learn meaningfully with it during fine-tuning due to the way it processes input from left to right.


\subsection{Datasets}

To evaluate our models, we used the proposed \textit{TinyNLP} in Section \ref{sec:tinyNLP}. Additionally, to compare our results with the current \gls{sota}, we employed the widely-used GLUE benchmark \cite{GLUE}, which comprises several datasets (see Table \ref{table:GLUE_datasets}). The GLUE benchmark is designed to test a model's ability to understand natural language by presenting various tasks such as sentiment classification, and regression on sentence pairs. As for the GLUE benchmark since only training and validation splits have publicly available labels we opted, as other papers like \cite{exploring_transformer_sizes} do, to use the validation as a test split.

\input{tables/tab_dataset_glue}

For the pretraining of the models, we chose to use the open-source \href{https://huggingface.co/datasets/bookcorpus/bookcorpus}{Book Corpus} dataset, which consists of over 7'185 books, providing a rich collection of diverse and contextually varied phrases. These texts offer detailed descriptive explanations for visual content, characters, and interactions, making them an ideal resource for tasks that require a nuanced understanding of language. Its broad range of genres and styles also helps enhance the models' ability to generalize across different types of textual inputs.

\subsection{Evaluation}
\label{sec:eval}

After training, each model's performances on each dataset was evaluated using average loss, accuracy, F1 score, precision, recall, \gls{mcc} and confusion matrix. For the STSB dataset, which is a regression task as per GLUE benchmark requirements, only average loss and \gls{scc} were reported. Hereby are reported only the accuracy and \gls{mcc} for the tinyML datasets since these two are the most relevant ones while for the GLUE benchmark only the metric necessary for the score has been reported. The required metrics to calculate the scores are \gls{scc} for STSB, MCC for CoLA, F1 score for QQP and MRPC and accuracy for the remaining tasks. 

To ensure statistical significance, each model was trained or fine-tuned from scratch on every dataset five times, with the mean metrics reported. Complete results are available at our project's \href{https://github.com/RiccardoBravin/tiny-LLM}{GitHub page}.



% \subsection{Comparisons}
% \label{sec:comparison}

% We have used as comparisons to our proposal the BERT(2MB) and MAMBA(2MB) described in Section \ref{sec:Models}, along with two simple baseline \textit{Just Embedder} and \textit{Embedder Conv}.

% \subsubsection{Just embedder}
% \label{sec:Just embedder}

% Given that the embedder occupies most of the available model parameters while having a lower activation size compared to attention or MAMBA layers, we opted to scale just this layer as much as possible removing any other part that usually constitutes LLM models. We opted to use the same embedder we are using for MAMBA from the NanoBERT paper \cite{NanoBERT} due to its reduced number of parameters. This enabled us to use a dictionary size of 8'192 with a reduced embedding dimension of 32, which was found to be optimal in the original paper, and a final embedding size of 320 with a sentence length of 256. 

% This model has just 293K parameters, however, due to the way it is constructed, this model lacks any capability for token embeddings to interact with each other, rendering it unsuitable for text generation purposes and making pretraing impossible. The simplicity of this model though makes it an ideal candidate for even a possible on-device learning scheme since its vocabulary could change and be adapted easily by substituting less used tokens, and the embedder could be trained with little added cost.

% \subsubsection{Embedder and convolution}
% \label{sec:Embeder+conv}
% To address the limitations of the embedder-only model, we appended a 1D convolutional layer with a kernel size of 16 to enable some interaction between token embeddings. This resulted in just 5k more parameters than the embedder-only model presented in \ref{sec:Just embedder}, \haz{while enabling the interaction between token embeddings.}
% % maintained the same overall structure and hyperparameters



% \mas{\st{TODO: Describe the two Baselines quickly}}



% \textbf{\mas{riformulare qui l'intro a tinyBert (chiamiamolo BERT (4.4M))}}

% \brav{
% Adding to the already wide variety of models we can compare against we opted, to give a wider sense of scale, to compare agains a 10 times bigger model from the creators of BERT \cite{BERT}, called BERT-Tiny \cite{tinyBERT},which can be considered \gls{sota} for the size. It is constructed, much like our BERT(2MB), with an embedding size of 128, a forward expansion of $2\times$, two heads and two layers but it uses a much bigger dictionary of 32'768 thus outnumbering all our proposals with its 4.4M parameters. Furthermore this model has been pretrained on the Open Book Corpus and then trained again through a distillation process using BERT as teacher. To ultimate the comparison with our models we finetuned it on our TinyML benchmark \ref{sec:tinyNLP} and took the official GLUE benchmark results as reported in Table \ref{table:results_tiny} and \ref{table:results_GLUE}. 
% }

% % Hereby we want to report a comparison \ref{table:comparison} between our best model with a much bigger model but still small in comparison to the SotA from the same creators of BERT called BERT-Tiny from \cite{tinyBERT} which is a SotA architecture of which hyperparameters where reduced to just an embedding size of 128 and a forward expansion of 2x repeated just two times. Here the embedder used is the standard one with a dictionary size of 32'768 thus greatly outnumbering our proposals in terms of memory with 4.4M parameters. This model is then trained with the standard BERT training we also use and after that a distillation process using BERT as a teacher is performed. For testing, the model is then finetuned on each single dataset of the GLUE Benchmark \cite{GLUE}.

% \input{tables/tab_model_parametres}


% \textbf{\brav{TODO: Io eliminerei questa sezione che mi sembra alquanto superflua come giustificazione $\downarrow$}}

% We opted not to consider as comparison, due to their sizes, several compact models derived from the same methodology but varying in their parameter size and architectural design. These models from \cite{tinyBERT}, referred to as BERT-Mini, BERT-Small, BERT-Medium, and BERT-Base, each present an increasingly larger set of hidden layers and embedding dimensions, ranging from 4.4M parameters in BERT-Tiny up to 110M parameters in BERT-Base. Despite their size differences, all models leverage the same three-staged training approach: first a \gls{mlm} with \gls{nsp} pre-training is performed, where the model learns from vast, general-domain text corpora in an unsupervised fashion, then the models go through a knowledge distillation process where they are trained to mimic the behavior of larger BERT models (such as BERT-Large) that serve as teachers. This distillation helps these smaller models approximate the performance of their larger counterparts by using unlabeled datasets and allowing the teacher model's soft labels (its output probabilities) to guide the student models during training. The final step for these models involves fine-tuning on task-specific data like the GLUE benchmark tasks, ensuring they adapt effectively to individual downstream tasks.
\end{comment}

%-----------------------------------------------------------------------------
% Experimental results
%-----------------------------------------------------------------------------

\section{Experimental Results}
\label{sec:results}

\input{tables/results/tab_results_summary}
We evaluated the proposed models on both the TinyNLP and GLUE benchmark datasets, comparing their performance under pretrained and unpretrained conditions. As shown in Tables~\ref{table:res_tiny_comp} and \ref{table:res_glue_comp}, each architecture exhibits distinct trade-offs in accuracy and memory efficiency, reflecting varying suitability for microcontroller-based deployments.

To account for the models’ differing abilities to leverage large-scale pretraining, we report unpretrained results for the \emph{Embedder} and \emph{Embedder + Convolution} configurations - which lack the architectural mechanisms to absorb extensive pretraining - and pretrained plus fine-tuned results for all other models. This separation underscores how architectures designed for context aggregation or attention mechanisms can benefit more substantially from large, diverse training corpora than simpler embeddings-based approaches.

Our findings pave the way for practical on-device language understanding. The next sections delve deeper into ablation analyses, providing further insight into which architectural components contribute most decisively to performance.
\begin{comment}
Our evaluation of the proposed models on TinyNLP and GLUE benchmark datasets highlights notable performance differences across various configurations, with attention to both pretrained and unpretrained settings. Results from Table \ref{table:results_tiny} and Table \ref{table:results_GLUE} demonstrate the effectiveness of each model in balancing accuracy and memory efficiency within the constraints of microcontroller environments. In these two tables we chose to report, based on the capability of the models to absorb pretraining, the results of the unpretrained models for Embedder and Embedder + Convolution and the pretrained and finetuned ones for the other models.
% , based on their best performance on the TinyNLP benchmark, the results of the pretrained models BERT(2MB), EmbBERT, EmbBERT long and Tiny-BERT(4.4M) and the ones of the unpretrained but just finetuned models MAMBA, Embedder and Embedder + Convolution. 
\end{comment}

\subsection{TinyNLP Benchmark Results}
For the TinyNLP benchmark, models were assessed based on accuracy. EmbBERT emerged as the best performer, with an average accuracy of 87.19\%, indicating a major robustness with respect to the compared models. Noticeably, EmbBERT also outperformed the BERT-Tiny model, which demonstrated strong performance, with the second highest overall average of 86.99\% accuracy, obtained by exploiting its 10$\times$ higher memory requirements. 

It's interesting to note how the only models that were able to outperform EmbBERT in the 2MB range in some datasets were the Embedder and Embedder + Conv models, in the ag\_news dataset and in the nlu/snips datasets, respectively. While these models may be considered naive, in fact, they demonstrated strong capabilities in dealing with the tasks in the TinyNLP benchmark suite that are characterized by shorter and simpler phrases, and for which the pre-training considered for all the other models had limited effects.

The standard BERT and MAMBA models, on the other hand, scored lower on average, indicating that these models may be less suitable for tasks requiring extensive parameter reduction. This suggests that the pre-trained EmbBERT, with highly optimized embedding and attention structures, is particularly well-suited for the TinyNLP classification tasks.

% The EmbBERT long variant displayed similar success, averaging 87.23\% accuracy and 79.65\% MCC, confirming the effectiveness of its architecture for memory-constrained NLP tasks. While the Embedder and Emb + Conv models achieved slightly lower performance, they still proved very competitive, averaging around 86.41\%-86.50\% accuracy and 78.09\%-78.49\% \gls{mcc} considering that their architecture is much simpler and due to this they might be very well suited for environments where speed, at the cost of performance, is preferable. 

% 

\subsection{GLUE Benchmark Results}

In the GLUE benchmark, models were assessed across various tasks, using \gls{mcc}, F1, \gls{scc}, and accuracy, depending on task requirements. EmbBERT, came up to be again our best competitor, with an overall score of 63.50, demonstrating competitive results across multiple tasks, especially in datasets like COLA (62.88\% F1) and STSB (49.25\% F1), further affirming its parameter efficiency and performance balance. The results on the F1 score metric are reported in Table \ref{table:res_glue_comp}, along with the final score computed on all the metrics.

Also in this benchmark, EmbBERT slightly outperformed the BERT-Tiny (20 MB) model, despite the large difference in memory requirements between the two. The Embedder and Embedder + Conv models resulted again to be the closest comparisons in the 2 MB range, but obtained large differences in the score (a 7 point score difference) with respect to EmbBERT.

% BERT-Tiny, being explicitly built to perform well on GLUE tasks, was the best performer achieving the highest average score of 65.28 and surpassing our model by about 4 percentage points. This does not undermine our model's abilities since the BERT architecture as can be seen from BERT(2MB) tends to not scale down to these sizes very well making our proposal the only possible one in the Embedded realm.

% For unpretrained models, Embedder reached a respectable overall score of 56.92, underscoring its training efficiency on smaller datasets and suitability for rapid deployment. The Emb + Conv model, while trailing slightly behind in overall GLUE scores, still proved effective in several tasks, indicating that certain architectures may yield task-specific advantages in memory-constrained environments.

% Our EmbBERT long skew did not perform on par with EmbBERT even in this scenario but still shows comparable results thus bringing us to the conclusion that our architecture might perform well on a variety of configurations making it suitable even for a possible scale up. 

In summary, these results support EmbBERT and its variations as optimal configurations for TinyML applications, balancing compact model size with high task performance. The results emphasize the feasibility of deploying parameter-efficient NLP models on microcontrollers while maintaining satisfactory performance across both general benchmarks and domain-specific tasks. 


\subsection{Model quantization under 1 MB}

With the objective of further reducing the memory and computational demands of the proposed EmbBERT model, and to demonstrate that the proposed architecture redesign approach can be used in conjunction with other optimizations techniques, we have applied the post-training quantization procedure described in Section \ref{subs:quantization} to the model. We have called this quantized version of our model \textbf{EmbBERT (8-bit)} and have reported its results on the two benchmarks suites in Table \ref{table:res_tiny_quant} and Table \ref{table:res_tiny_quant}, along with the results of EmbBERT for comparison. 

\input{tables/results/tab_quantization}

The results show that EmbBERT and it's quantized version are similarly effective in handling TinyNLP tasks. In particular EmbBERT (8-bit) achieves an average accuracy of 88.17\%, even surpassing its unquantized counterpart in several datasets and beating by a good margin even Bert-Tiny \cite{BERT}.
%BERT + NE + EF while still being competitive in it's unquantized form, suffered significant accuracy drops after quantization. 
On the GLUE benchmark, which evaluates broader natural language understanding tasks, EmbBERT again demonstrated remarkable robustness to quantization. %The unquantized EmbBERT outperforming BERT + NE + EA by a margin of over 6 points. 
%while 
EmbBERT (8-bit) achieved an overall GLUE score of 62.81, just 0.7 percentual points lower than EmbBERT, showcasing minimal degradation in performance after quantization. %The quantized BERT + NE + EA model showed a substantial performance drop, achieving only 45.97, showing, with even some catastrophic forgetting in MNLI-m and MNLI-mm, that it struggles with the diverse and complex tasks in GLUE when quantized.

EmbBERT (8-bit) consistently outperformed the comparisons across TinyNLP and GLUE benchmarks. In both of them, all the comparison models in the 2MB range obtains consistently lower performance even considering their unquantized version.

By applying selective 8-bit quantization to model weights and reducing activations to FP16, we achieved substantial memory savings without degrading accuracy. These resulted in a reduced memory requirement of just 781kB for our model considering both weights and activations, with a reduction in memory demands of roughly 2.4 times with respect to standard EmbBERT. Further fine-tuning of the quantized models using parameter-efficient techniques proved remarkably effective, particularly for task-specific refinements, with only 8\% of the parameters requiring updates.

While quantization generally introduces trade-offs in accuracy, the EmbBERT architecture demonstrated remarkable resilience, highlighting its suitability for deployment in constrained environments, where furhter optimations techniques are commonly used. 
%thanks to it's even smaller size, without significant compromise in performance.

%-----------------------------------------------------------------------------
% Ablation study
%-----------------------------------------------------------------------------

\section{Ablation Study}
\label{sec:ablation}

In this section, we dissect the contributions of various architectural components to the overall performance on both the TinyNLP and GLUE benchmarks. By incrementally adding, removing, or modifying key modules, we elucidate their roles in optimizing performance under strict memory constraints. The memory requirements of the various configurations of the models are reported in Figure ~\ref{table:memory_ablation}, along with the accuracy and score results on the two benchmarks. 

\begin{figure*}[ht]
    \centering
    \begin{minipage}[t]{0.48\textwidth} % Adjust width for each column
        \centering
        \includegraphics[width=\textwidth]{imgs/plot_Model_Sizes_and_Averages.pdf} % Replace with your first image file
        \caption{Model's average accuracy on the TinyNLP benchmarkof the models in the ablation study.}
        \label{fig:image1}
    \end{minipage}%
    \hfill
    \begin{minipage}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{imgs/plot_Model_Sizes_and_Averages_GLUE.pdf} % Replace with your second image file
        \caption{Model's scores on the GLUE benchmark of the models in the ablation study.}
        \label{fig:image2}
    \end{minipage}
    \caption{Weight sizes (blue), activation sizes (grey), and effective memory usage for the models used in the ablation study.}
    \label{fig:twocolumnfig}
\end{figure*}

%\input{tables/tab_model_parametres}

%\input{tables/results/tab_ablation}

\paragraph{Base model: BERT-2MB}
We begin with BERT-2MB - a highly compressed variant of BERT that serves as our foundational baseline. Compared to the significantly larger BERT-Tiny (which is 10$\times$ bigger in memory footprint), BERT-2MB exhibits notable performance degradation attributable to reduced parameter count, that severely limits the model’s ability to capture complex contextual relationships.
Despite these limitations, BERT-2MB represents a first step towards adapting standard transformer architectures for ultra-low-memory environments.

\paragraph{BERT + Nano Embedder (BERT + NE)}
To mitigate BERT-2MB’s deficits, we have substituted the standard Embedder with the Nano Embedder~\cite{NanoBERT}, designed to optimize embedding representations without inflating the overall parameter count. By expanding the effective vocabulary space within the same memory budget, this approach yields clear performance gains over BERT-2MB on both TinyNLP and GLUE. 

\paragraph{BERT + Efficient Attention (BERT + EA)}
In order to reduce activation overhead, we replace the default multi-head attention module with Efficient Attention\cite{efficient_attention}, aiming to lessen activation memory costs. By cutting down on activation overhead, we can afford to increase embedding dimensionality and/or the number of layers. 
This modification resulted in a significant improvement in the performance in the TinyNLP benchmark, but, when not coupled with other optimization techniques, lead to a slight decrease in the performance with respect to the base BERT(2MB) model.

%demonstrating that the choice of an efficient attention scheme is crucial for resource-constrained deployments.

\paragraph{BERT + NE + EA - combining embedding and attention optimizations}
We subsequently merge the Nano Embedder with Efficient Attention to form BERT + NE + EA, leveraging both a broader vocabulary and reduced activation overhead. We obtain a performance gain on TinyNLP tasks, where BERT + NE + EA achieves $87.51\%$. At the same time, on the GLUE benchmark, BERT + NE + EA obtain a score of 61.20, over 9 points higher with respect to the original BERT (2MB)  %outperforming BERT-Tiny ($86.99\%$) - all within a 10$\times$ smaller memory footprint
%Competitive Results on GLUE, although the architecture’s limited depth and width may still underperform on more complex tasks.
These results highlight the advantage of unifying embedding efficiency with an optimized attention mechanism in extremely small models.

\paragraph{EmbBERT}

Finally, we incorporate the Convolutional $1\times 1$ Efficient Differential Skip Attention mechanism- %a mechanism combining differential skip connections and efficient attention.
obtaining the final EmbBERT architecture. The addition of this newly designed attention mechanisms obtains marginal improvement on the TinyNLP benchmark, while it demonstrates notable success on the more complex tasks contained in the GLUE benchmark, improving the performance of over $2$ points in the score in this benchmark suite compared to BERT + NE + EA. 



%the  gain relative to similarly scaled-down transformer baselines, and on-par performance with BERT-Tiny but at just one-tenth the memory consumption.
%This trade-off reflects EmbBERT’s superior capacity for handling diverse linguistic phenomena in tasks like GLUE, although at some expense to domain-specific performance on TinyNLP.

%\paragraph{Key insights}
%Vocabulary Expansion: The Nano Embedder showcases how memory-efficient architectures can still maintain robust vocabularies, a critical factor for lexical coverage in real-world scenarios.
%Activation Efficiency: Employing efficient attention is pivotal for scaling depth and dimensionality within memory-constrained settings.
%Synergistic Design: NanoBERT Efficient underscores how combining optimized embedding strategies with streamlined attention mechanisms can yield notable improvements.
%Task-Specific Advantages: EDSA’s strong performance on GLUE tasks highlights the benefit of adaptive attention mechanisms, while its trade-offs on TinyNLP reveal the importance of context-specific design choices.

Overall, through a comprehensive ablation we have elucidated the impact of architectural components such as the Nano Embedder, Efficient Attention, and the Convolutional $1\times 1$ Efficient Differential Skip Attention mechanism. While maintaining the total memory dedicated to the algorithms constant through this whole ablation study, we have demonstrated that the addition of the optimization mechanisms composing EmbBERT resulted in an average improvement of $3.46$ percentage points in the accuracy in the TinyNLP benchmark, and in an improvement of $11.40$ points in the score in the GLUE benchmark, with respect to the original BERT (2 MB) model.  These components advance the frontier of ultra-compact language models, offering state-of-the-art performance in environments where both memory and computational resources are extremely constrained.

\begin{comment}
\brav{
Through this ablation study, we aim to assess the impact of the different components in the proposed models. By iteratively adding, removing, or modifying key elements, we measure their contributions to performance across the TinyNLP and GLUE benchmarks.

We begin with the foundational BERT(2MB) model, which serves as a highly compressed version of BERT. When compared to the significantly larger BERT-Tiny (10× larger), BERT(2MB) experiences a noticeable degradation in performance. This degradation can be attributed to its higher activation sparsity and lower parameter count, which limit its representational capacity.

\subsection{Nano Embedder: Enhancing Parameter Efficiency}
To address the limitations of BERT-2MB, we incorporate the Nano Embedder, designed to improve parameter efficiency. By optimizing embedding representations, this module enables the model to expand its vocabulary, improving its ability to discern a wider variety of terms. The results demonstrate that:
\begin{itemize}
    \item NanoBERT-2MB, equipped with the Nano Embedder, shows clear performance gains over BERT(2MB) on both benchmarks.
    \item The addition of convolutional layers to the embedder further enhances performance, particularly in tasks requiring more nuanced embedding interaction.
\end{itemize}

\subsection{Efficient Attention: Reducing Activations to Expand Capacity}

Next, we replace the standard attention mechanism with Efficient Attention, which significantly reduces activation overhead. This allows us to increase both the embedding size and layer count, resulting in performance levels comparable to those of NanoBERT(2MB).

\subsection{Combining Efficiency: NanoBERT Efficient}
By integrating the Nano Embedder with Efficient Attention, the NanoBERT Efficient model leverages both a broader vocabulary and reduced activation overhead. This combination enables:
\begin{itemize}
    \item A notable performance uplift on the TinyNLP benchmark, where NanoBERT Efficient achieves 87.51\%, surpassing BERT-Tiny (86.99\%) while maintaining a 10× smaller memory footprint.
    \item Competitive results on the GLUE benchmark, although it falls short of handling its higher complexity effectively.
\end{itemize}

\subsection{Efficient Differential Skip Attention: Breaking New Ground}
To further improve performance, we incorporate Efficient Differential Skip Attention (EDSA) into the model. While this modification slightly reduces performance on the TinyNLP benchmark (placing it on par with the embedder-only configuration), it excels on GLUE, achieving:
\begin{itemize}
    \item A remarkable performance increase of over 5.00 percentage points compared to scaled-down pre-existing models.
    \item Results comparable to BERT-Tiny, while still occupying 10× less memory and offering significantly improved computational efficiency.
\end{itemize}

\subsection{Insights}
\begin{itemize}
    \item Vocabulary Expansion: The Nano Embedder demonstrates the importance of parameter efficiency in low-resource models, with significant gains derived from its ability to handle larger vocabularies.
    \item Activation Efficiency: Efficient Attention not only reduces memory consumption but also unlocks the potential for deeper and wider architectures, leading to competitive performance in lightweight settings.
    \item Synergistic Design: Combining embedding efficiency with attention optimization, as seen in NanoBERT Efficient, highlights the importance of synergy between components to maximize performance.
    \item Task-Specific Gains: EDSA's superior performance on GLUE underlines its strength in handling complex and diverse tasks, albeit with a trade-off in domain-specific benchmarks like TinyNLP.
\end{itemize}

This ablation study underscores the critical role of modular innovations such as the Nano Embedder, Efficient Attention, and EDSA in achieving state-of-the-art performance within extreme memory and computational constraints. These findings pave the way for future explorations of lightweight language models tailored to specific benchmarks and application scenarios.
}
\end{comment}
% Hereby we would like to assess, through an ablation study, the impact of each of the different parts of the proposed models by iteratively adding or modifying key components and measuring the resulting performances. 
% Starting from the foundational model of BERT(2MB) we can see how much degradation in the performances there is when compared to it's 10x bigger BERT-Tiny counterpart due to it's high activations and consequently lower parameter count. By leveraging the Nano Embedder we can achieve an higher parameter efficiency thus enabling us to increase the dictionary of the model making it able to discern a wider variety of terms. We can see how this helps both in the NanoBERT(2MB) model and the Embedder + Convolution one both in the TinyNLP as well as in the GLUE benchmarks. Keeping the standard embedder and instead switching the Standard Attention for the Efficient Attention counterpart we manage to achieve much lower activations that enable us to increase both the layer count and the embedding size making us achieve results comparable to those of NanoBERT(2MB). When combining these two ideas in NanoBERT Efficient we can see a significant uplift in performances since the Efficient Attention can now leverage the more ample vocabulary available as well as lower activations that can be used to boost the embedding size and layer count. This enables this model in TinyNLP to achieve results higher (87.51\%) that those of the \gls{sota} model BERT-Tiny (86.99\%) but falling short in the much more complex GLUE benchmark. Modifying then this model with our proposed Efficient Differential Skip Attention, while losing some performance in TinyNLP placing it on par with the Embedder only model, manages to surpass all other proposals in the GLUE benchmark beating all scaled down pre-existing models by at least 5.00\% points and landing right next to BERT-Tiny in terms of performances while still occupying 10 times less memory and being much more computationally efficient.

%-----------------------------------------------------------------------------
% Conclusions
%-----------------------------------------------------------------------------

\section{Conclusions}
\label{sec:conclusions}

\brav{
This study demonstrates that aggressively scaling down language models, like the original BERT and MAMBA architectures, remains challenging for highly resource-constrained environments, as their performances consistently lag behind simpler baselines. Furthermore their requirement for high activation sizes tends to limit their capabilities in such constrained realms.

%For scenarios lacking computational resources for large-scale pretraining, 
Surprisingly, the Embedder only and Embedder + Conv architectures adopted as baselines emerged as competitive solutions in this environment. Their minimal activation footprints and reduced hyperparameter dimensionalities support faster training, and the overall simplicity in the design of this algorithms makes them appealing for simple tasks that do not require complex syntatictal analysis.

The EmbBERT model, which leverages an optimized attention mechanism, offers the highest empirical performance in both its quantized and non-quantized configurations. Its design balances parameter efficiency with competitive results, making it appropriate for use cases where additional computational cost is permissible in exchange for improved accuracy. The proposed architecture
Notably, the adopted quantization strategy significantly enhanced the efficiency of our proposed model while maintaining strong task performance.  This highlights the viability of quantization as a practical approach for deploying deep learning models in memory- and computation-constrained environments.

Future work may explore deeper compression strategies—such as Low-Rank Adaptation (LoRA) or targeted knowledge distillation—to further reduce model footprints. By jointly examining parameter counts, activation memory, and quantization impacts on single-core, single-threaded hardware, this work offers a systematic basis for optimizing deep learning architectures within stringent memory constraints.
}
\begin{comment}
In conclusion, the original BERT architecture proves difficult to scale down effectively for resource-constrained environments, consistently underperforming compared to our proposed models across various tasks, whether pretrained or not. MAMBA, despite having relatively large activation sizes, remains competitive due to its lower computational complexity, making it a viable midway option.

Our attention-based model, EmbBERT, excels as the top performer, particularly when pretrained. It combines high parameter efficiency with strong performance, often matching or surpassing models that are significantly larger, making it a compelling choice for environments where performance is paramount, even at the cost of increased computational demands.

For applications where pretraining is not feasible or rapid on-device learning is required, the Nano Embedder model is the most efficient option. Its minimal activation sizes and reduced hyperparameter count make it ideal for fast training, and it opens the door for further optimizations, such as \gls{nas}.

Future work could focus on further reducing model size through techniques like \gls{lora} or knowledge distillation.
This study marks a significant advancement in the development of compact, parameter-efficient models capable of outperforming traditional NLP methods in simple classification tasks, all while requiring far fewer computational resources.

Notably, this research provides a unique contribution by thoroughly analyzing both model size and activation memory, specifically for single-core, single-threaded processing. This positions our work as a critical step toward optimizing deep learning models for highly resource-constrained environments, paving the way for more efficient deployment on microdevices.
\end{comment}

\section*{Acknowledgment}

This paper is supported by PNRR-PE-AI FAIR project
funded by the NextGeneration EU program.

\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,references} %Prints bibliography

%\printglossaries
\appendix

\subsection{Exact Computation of Memory and Computational Cost of Common LLM Layers}
\label{appendix:A}

The architecture of \glspl{llm} is primarily built upon the Transformer model, introduced by \cite{transformers}. This architecture has revolutionized natural language processing by allowing models to handle long-range dependencies in text effectively. Models that are built to perform classification of text usually are constituted by two main parts: an embedder and an encoder which in turn is mainly formed by an Attention Mechanism and a Feed-Forward Neural Network. 

In this section we report both an analytical layer-by-layer evaluation of the memory and computational requirements of common components constituting Language Models and an overall view of how these work.

We emphasize that, the computation evaluations takes into account the complexity and memory access profiling for all the different layers of the model, emphasizing CPU-based operations like summation, multiplication, and memory retrievals.
% We focus on the core computations disregarding implementation details like index management or function calls.
For the memory evaluation, we assume a non-parallel program that keeps only the bare minimum in memory to work.
Moreover, in the activations computation, the following assumptions have been done: 

% This section reports an analytical layer-by-layer evaluation of the memory and computations required for the models proposed  in Section \ref{sec:Models}. Table \ref{table:memory_analysis} reports an overview of the memory usage, while Table \ref{table:complexity} summarizes the complexity and memory accesses for each of the tested models.

% We emphasize that, the computation evaluations takes into account the complexity and memory access profiling for all the different layers of the model, emphasizing CPU-based operations like summation, multiplication, and memory retrievals.
% % We focus on the core computations disregarding implementation details like index management or function calls.
% For the memory evaluation, we assume a non-parallel program that keeps only the bare minimum in memory to work.
% Moreover, in the activations computation, the following assumptions have been done:

\brav{

\begin{itemize}
    \item Operations such as sums, elementwise multiplications and activation functions are in-place and occupy only the memory of input matrices as intermediate results are discarded.
    \item Matrix multiplications requires memory for both input matrices plus the output matrix.
    \item Fully connected layers are matrix multiplications where only one input and the output matrix contribute to activations, as weights do not increase activation memory.
    \item Max memory consumption per layer is recorded at peak usage.
    \item Calculations are based on inference-only processing.
\end{itemize}
}

\subsubsection{Embedder}

\textbf{\mas{Questo lo lascio un secondo da parte}}
In the worst-case scenario, where segment embeddings are involved, \( \ell \cdot (4d + 2) + 2d \) memory retrievals and \( 2d \cdot \ell \) sums are performed. Highlighting the intensive memory operations that can occur just to prepare the input for further computation.


\subsubsection{Nano Embedder}
\label{subs:NanoEmbedder}
\brav{
The Nano Embedder (Fig. \ref{fig:embedders_memory}), as introduced by \cite{NanoBERT}, fulfills the same function as the standard Embedder but with a key difference: rather than embedding tokens and positions directly into vectors of size $d$, it first maps these inputs to a reduced embedding dimension, which we denote as $r_d$. This reduced embedding is then projected to the original dimension $d$ through a fully connected layer. Segment embeddings are an exception to this process, as the fully connected projection matrix required for them would negate the parameter savings achieved by reducing the embedding size.
This alternative embedding approach results in a total parameter count of:


\begin{equation}
r_d \cdot (v + \ell + 2 d) + 2 d
\end{equation}

Although this formula might appear to suggest a higher parameter count, the total can actually be lower than that of the standard Embedder if $r_d$ is sufficiently small relative to $d$. However, the total activation size increases slightly due to the projection step, resulting in: 

% While it seems from the formula that the amount of parameters is higher, in fact, for small enough $ r_d $ and big enough difference between it and $ d $ the total becomes smaller than the one of the standard embedder. Even if not by much the total activation size instead increases, due to the projection, to: 
\begin{equation}
    r_d \cdot \ell + 2 d \cdot \ell.
\end{equation}

The Nano Embedder performs first $\ell + 2\ell \cdot r_d$ memory accesses for token and position encoding, followed by $4\ell \cdot r_d \cdot d$ memory accesses, $2\ell \cdot r_d \cdot d + \ell \cdot d$ summations, and $\ell \cdot r_d \cdot d \cdot 2$ multiplications for the linear layers and in case that segment embeddings are needed another \(\ell + d \cdot + \ell \cdot d \) accesses and \(\ell \cdot d\) summations are needed.

This balance of parameter efficiency with slightly increased activation memory exemplifies the Nano Embedder’s ability to reduce overall model size while retaining embedding functionality, making it suitable for resource-constrained scenarios or to simply obtain more performance with higher dictionary sizes.
}

\begin{figure}[htbp]
    \centerline{
        \includegraphics[height=.30\textheight,keepaspectratio]{imgs/embedders.pdf}
    }
    \caption{Memory layout of the Standard Embedder (left) and Nano Embedder (right) layer. $\ell$ is the sentence length, $d$ is the embedding dimension, the trapezoid with fc is used to represent fully connected layers. The dashed gray box shows the operations which require the maximum acivations. The Nano Embedder manages to decrease the number of weights while still keeping about the same size for the activations.}
    \label{fig:embedders_memory}
\end{figure}

\subsubsection{Attention}
\label{subs:attention_appendix}
\brav{

The standard attention mechanism \cite{transformers} enables models to selectively focus on the most relevant parts of an input sequence, enhancing their ability to generate accurate predictions. Originally developed to boost performance in machine translation, attention mechanisms assign varying importance, or "weights," to each token in a sequence according to its relevance to the task or context. This approach enables models to capture relationships between words, even those far apart in the text.

A widely adopted variant of this, the self-attention mechanism, calculates relationships within a sequence by allowing each token to attend to all others, creating contextualized representations that capture both local and global dependencies. As shown in Fig. \ref{fig:attentions_memory}, self-attention starts by passing the input through three fully connected layers to produce the Query, Key, and Value matrices, each with a size of \(d \cdot d + d\). This step alone yields an activation size of  \(4 \cdot d \cdot \ell\), with computational demands of \(6\ell \cdot d \cdot d\)  memory accesses, \(3\ell \cdot d \cdot d\) summations and \(3\ell \cdot d \cdot d\) multiplications. 

Next, the Query and Key matrices undergo matrix multiplication, producing a Weight matrix of size \(\ell \times \ell\) for each of the Attention heads $h$. The quadratic growth of this Weight matrix contributes significantly to the limited context length in attention-based models, as it increases activation size considerably. With this step, the activation size reaches  \(3 d \cdot \ell + \ell^2 \cdot h\) along with \(2\ell \cdot \ell  \cdot d \cdot h\) memory accesses, \(\ell \cdot \ell  \cdot d \cdot h\) summations and \(\ell \cdot \ell  \cdot d \cdot h\) multiplications.

Following this, the Weight matrix is passed through a softmax function, an intensive operation requiring \(2\ell \cdot \ell \cdot h\) memory accesses, \(\ell \cdot \ell \cdot h\) summations and \(3\ell \cdot \ell \cdot h\) multiplications, though it does not increase the activation size.

Finally, the Weighted matrix multiplies the Value matrix to yield an output activation size of \(2 d \cdot \ell + \ell^2 \cdot h\) (lower than the preceding step). However, this operation adds \(2\ell \cdot \ell  \cdot d \cdot h\) memory accesses, \(\ell \cdot \ell  \cdot d \cdot h\) summations and \(\ell \cdot \ell  \cdot d \cdot h\) multiplications. This output then passes through a final fully connected layer of size \(d \times d + d\) with an added skip connection. While this step does not affect activation size, it requires \(\ell \cdot d \cdot d \cdot 2\) memory accesses, \(\ell \cdot d \cdot d + \ell \cdot d\) summations and \(\ell \cdot d \cdot d\) multiplications. 

Through this efficient sequence of weighted representations, the attention mechanism enables the model to build robust contextual representations, ensuring each token is influenced by relevant parts of the sequence without being restricted by token distance. The formulas for the overall weights, activations and complexity analysis are reported in Table \ref{table:memory_analysis} and \ref{table:complexity}.  

% One widely adopted variant, the self-attention mechanism, computes relationships within a sequence by allowing each token to attend to every other token, generating contextualized representations that capture both local and global dependencies in text. This is done, like depicted in Fig. \ref{fig:attentions_memory}, by first passing the input through  three fully connected layers to obtain the Query, Key e Value matrices each of size \(d \cdot d + d\) which nets an activation size of \(4 \cdot d \cdot \ell\) and requires \(\ell \cdot d \cdot d \cdot 6\) accesses, \(\ell \cdot d \cdot d \cdot 3\) sums and \(\ell \cdot d \cdot d \cdot 3\) multiplications. 

% Then a matrix multiplication between Query and Key is performed obtaining a Weight matrix of size \(\ell \times \ell\) for each of the Attention heads $ h $. This resulting matrix is the main culprit of the limited context length of all of our our attention based models given its quadratic dependency and thus bigger activation size. With this last computation we reached an activation size of \(3 \cdot d \cdot \ell + \ell^2 \cdot h\) and added another \(\ell \cdot \ell  \cdot d \cdot h \cdot 2\) accesses, \(\ell \cdot \ell  \cdot d \cdot h\) sums and \(\ell \cdot \ell  \cdot d \cdot h\) multiplications. This matrices then go through the expensive softmax function which requires \(\ell \cdot \ell \cdot h \cdot 2\) accesses, \(\ell \cdot \ell \cdot h\) sums and \(\ell \cdot \ell \cdot h \cdot 3\) multiplications but doesn't increase activations.

% This Weight matrix gets finally multiplied by the Values one producing an activation size of \(2 \cdot d \cdot \ell + \ell^2 \cdot h\) which we can ignore since it will be lower than the preceding ones but still performing \(\ell \cdot \ell  \cdot d \cdot h \cdot 2\) accesses, \(\ell \cdot \ell  \cdot d \cdot h\) sums and \(\ell \cdot \ell  \cdot d \cdot h\) multiplications. Finally we have to go through the last fully connected layer of size \(d \times d + d\) and add the skip connection which does not increase our activations size but requires an additional \(\ell \cdot d \cdot d \cdot 2\) accesses, \(\ell \cdot d \cdot d + \ell \cdot d\) sums and \(\ell \cdot d \cdot d\) multiplications.

}

% The attention layer \ref{fig:attentions_memory} needs first three fully connected layers for the Query, Key e Value matrices each of size \(d \cdot d + d\). This first process nets us an activation size of \(4 \cdot d \cdot \ell\) and requires \(\ell \cdot d \cdot d \cdot 6\) accesses, \(\ell \cdot d \cdot d \cdot 3\) sums and \(\ell \cdot d \cdot d \cdot 3\) multiplications. 

% Then a matrix multiplication between Query and Key is performed obtaining a Weight matrix of size \(\ell \times \ell\) for each of the Attention heads $ h $. This resulting matrix is the main culprit of the limited context length of all of our our attention based models given its quadratic dependency and thus bigger activation size. With this last computation we reached an activation size of \(3 \cdot d \cdot \ell + \ell^2 \cdot h\) and added another \(\ell \cdot \ell  \cdot d \cdot h \cdot 2\) accesses, \(\ell \cdot \ell  \cdot d \cdot h\) sums and \(\ell \cdot \ell  \cdot d \cdot h\) multiplications. This matrices then go through the expensive softmax function which requires \(\ell \cdot \ell \cdot h \cdot 2\) accesses, \(\ell \cdot \ell \cdot h\) sums and \(\ell \cdot \ell \cdot h \cdot 3\) multiplications but doesn't increase activations.

% This Weight matrix gets finally multiplied by the Values one producing an activation size of \(2 \cdot d \cdot \ell + \ell^2 \cdot h\) which we can ignore since it will be lower than the preceding ones but still performing \(\ell \cdot \ell  \cdot d \cdot h \cdot 2\) accesses, \(\ell \cdot \ell  \cdot d \cdot h\) sums and \(\ell \cdot \ell  \cdot d \cdot h\) multiplications. Finally we have to go through the last fully connected layer of size \(d \times d + d\) and add the skip connection which does not increase our activations size but requires an additional \(\ell \cdot d \cdot d \cdot 2\) accesses, \(\ell \cdot d \cdot d + \ell \cdot d\) sums and \(\ell \cdot d \cdot d\) multiplications.


\subsubsection{Efficient Attention}
\label{subs:eff_attention_appendix}
\brav{
The Efficient Attention layer (Fig. \ref{fig:attentions_memory}), as proposed by \cite{efficient_attention},simplifies the architecture of the standard Attention layer by reducing the number of fully connected layers. Instead of three, it uses only one dense layer at the start to produce the Query matrix, while Key and Value matrices are just the same as the input, and one final fully connected layer for the attention output both of sizes \(d \cdot d + d\)

Like the standard Attention layer, the Efficient Attention layer’s highest activation size is reached during the matrix multiplication step, which produces an activation size of \(\ell \cdot d \cdot 2 + \ell^2\). This operation retains the model's ability to generate contextually relevant representations, but with fewer parameters and computational steps.

To calculate the operations and memory accesses required, we can simplify the expressions from the standard Attention layer by omitting terms associated with the two additional fully connected layers and the $ h $ (attention heads) term from all equations. As a result, as stated in \cite{efficient_attention}, the effective modeling capabilities of the attention layer are not hindered and it achieves similar contextual performance with lower computational cost, making it advantageous in resource-constrained scenarios.
}


\begin{figure}[htbp]
    \centerline{
        \includegraphics[height=.5\textheight,keepaspectratio]{imgs/attentions.pdf}
    }
    \caption{Memory layout of the Attention (left) and Efficient Attention (right) layers. $ \ell $ is the sentence length, $ d $ is the embedding dimension, the trapezoid with fc is used to represent fully connected layers. The dashed gray box shows the operations which require the maximum acivations. As can be see just from the width of the two modules, Efficient Attention uses a quite lower activation size.}
    \label{fig:attentions_memory}
\end{figure}


\brav{

% \subsection{Normalization}
% The normalization layer is a simple layer-wise normalization which necessitates \(2 \cdot d\) parameters for the mean and variance values and performs on inference \((\ell+1) \cdot d \cdot 2\) memory accesses and \(\ell \cdot d\) sums and multiplications. To do this it only requires an activation size of \(\ell \cdot d \cdot 2\) which becomes irrelevant once compared to the ones of the actual computationally intensive layers. 

\subsubsection{Feed-Forward}
This layer is appended to the Attention and Efficient Attention ones after a normalization step and it is composed of just two fully connected layers of size \(d \cdot d \cdot \alpha\) that subsequently increase/decrease by \(\alpha\) the embedding dimension plus an activation function in the middle, all followed by a skip connection. This simply results in an activation size of: 
\begin{equation}
    2 \ell \cdot d + \ell \cdot d \cdot \alpha  
\end{equation}
and necessitates \(4 \ell \cdot d \cdot (d \cdot \alpha + 1)\) memory accesses and half as many sums and multiplications

}

\subsubsection{MAMBA}
\label{subs:MAMBA_appendix}
The MAMBA model \ref{fig:MAMBA_memory} presents a more intricate architecture with multiple internal parameters and fully connected layers. The model learns two separate internal parameters, 
$A$ and $D$, with dimensions \(i \times d_s\) and $i$, respectively. Due to the complexity of the model we opted to subdivide the analysis in two sections: the main layer \ref{subsubs:MAMBA_main} and the \gls{ssm} \ref{subsubs:MAMBA_SSM} sublayer.

\paragraph{Main Layer}
\label{subsubs:MAMBA_main}
In this layer we have three main paths to follow. The skip connection, the feed forward only section and the main computation one. Both the main layer and the feed forward only start with a fully connected layer of size \(d \times i\) that makes the input matrix of size \(\ell \times d \) into one of size \(\ell \times i\) performing \(4\ell \cdot d \cdot i\) accesses, \(2\ell \cdot d \cdot i\) sums and \(2\ell \cdot d \cdot i\) multiplications in total. The main layer then goes through a 1D convolution much like the one of the Embedder + Convolution model proposed and applies a Swish function which results in negligible activation sizes but performs a total of \(4\ell \cdot i + c\) accesses, \(\ell \cdot i \cdot (c + 1)\) sums and \(\ell \cdot d \cdot (c + 2)\) multiplications. Before joining the two paths with an element-wise multiplication the \gls{ssm} \ref{subsubs:MAMBA_SSM} sublayer is applied. This newly obtained matrix goes again through a fully connected layer of size \(d \times i\) which downscales the matrix to \(\ell \times d\) this way enabling the skip connection to be summed to it resulting in the output matrix. All this processing equates to an activation size of
\begin{equation}
    \max( \ell\cdot d + SSM;\ 3\ell \cdot i + \ell \cdot d)
    \label{eq:MAMBA_main_act}
\end{equation}
and totalling \(\ell \cdot i \cdot (6d + 9) + \ell \cdot d + c + SSM\) memory accesses, \(\ell \cdot i \cdot (3d + 2 + c) + \ell \cdot d + SSM\) sums and \(\ell \cdot i \cdot (3d + c + 5) + SSM\) multiplications.

\paragraph{SSM}
\label{subsubs:MAMBA_SSM}
For this section we start by processing the input through three fully connected layers; two of size \(i \times d_s\) and one \(i \times rk\) to obtain the $C$, $B$ and \(\Delta\) matrices. This last matrix is then again passed through a fully connected layer, this time of size \(rk \times i\), and a softplus netting us a total activation size of \(2\ell \cdot d_s + 2\ell \cdot i \) and performing \(\ell \cdot i \cdot (4(d_s + rk) + 1) \) accesses, \(\ell \cdot i \cdot (2(d_s + rk) + 1)\) sums and the same number of multiplications. 

The \(\Delta\), $ B $  and input matrices are then subjected to an Einstein summation-like operation (a custom form of matrix multiplication). This gives us a tensor \(\Delta Bu\) of dimensions \(\ell \times i \times d_s\). Similarly, we multiply \(\Delta\) by the internal model weight $ A $, resulting in another tensor \(\Delta A\) of size \(\ell \times i \times d_s\).
An element-wise exponentiation is then applied to \(\Delta A\), leading to an increase in the activation count which now amounts to:
\begin{equation}
     2\ell \cdot i + \ell \cdot d_s + 2\ell \cdot d_s \cdot i
\end{equation}
and a total number of accesses of \(4\ell \cdot i \cdot d_s\) and \(2\ell \cdot i \cdot d_s\) multiplications.

We can now discard B and delta matrices, leaving us with input, $ C $, \(\Delta Bu\), and \(\Delta A\) tensors on which subsequent operations occur within a loop over $ \ell $. Here, we first perform an element-wise multiplication of the slice of \(\Delta A\) with $ x $ (a newly instantiated matrix representing the current state) to get a new matrix of size \(i \times d_s\), which is then added to the slice of \(\Delta Bu\) to update $ x $. The updated $ x $ is multiplied by a C slice, and these operations are repeated and concatenated until we have a resulting matrix $ y $ with dimensions \(\ell \times i\). These operations in total yield an activation size of
\begin{equation}
     2\ell \cdot i  \cdot (d_s + 1) + \ell \cdot d_s  + i \cdot d_s
\end{equation}
and \(\ell \cdot (3i \cdot d_s + 3i \cdot d_s + 2i \cdot d_s + i)\) accesses, \(\ell \cdot (0 + i \cdot d_s + i \cdot d_s)\) sums and \(\ell \cdot (i \cdot d_s  + 0 + i \cdot d_s)\) multiplications.

Finally, we multiply element-wise along the $ \ell $ axis the input by a vector of internal weights $ D $, yielding a matrix with dimensions \(\ell \times i\) which then gets summed to the matrix $ y $ to obtain the output of the sublayer. The activations of this passage are small thus negligible but still needs to perform \(\ell \cdot i \cdot 4 + i\) memory accesses and \(\ell \cdot i \) sums and multiplications.



\begin{figure}[htbp]
    \centerline{
        \includegraphics[height=.64\textheight,keepaspectratio]{imgs/MAMBA_all.pdf}
    }
    \caption{Memory layout of the MAMBA (left) and SSM (right) layers. $ \ell $ is the sentence length, $ d $ is the embedding dimension, $ i $ is $ d $ times the forward upscale \(\alpha\) and  $ d_s $ is the state size, the trapezoid with fc is used to represent fully connected layers. Boxed in blue is the looped section on the sentence length dimension. The Efficient Attention manages to halve the number of weights with respect to the Standard Attention while also, being thinner, reducing the overall activation size. This layer with respect to the Attention based ones has a much greater activation size dependency but manages to keep the weight values about the same.}
    \label{fig:MAMBA_memory}
\end{figure}




\subsection{Requirements and Complexity of Common Language Models \& components}
\label{sec:componentsAppendix}
%\brav{\textbf{QUESTO VA SPOSTATO NELL'APPENDICE}}

%With the objective of scaling the requirements of the State-of-the-art Language Models to work in resource-constrained environments, we have analyzed in detail the memory and computational requirements of the models and their components. While the number of parameters of these models is a common metric that is used to compare them, we are not aware of any work in the literature that precisely provides an evaluation of their activation sizes and computational requirements. In this Section, we report the results of the memory evaluation, while in Appendix \ref{}, we detail the computational evaluation along with the precise mathematical steps that lead to these results.

%\input{tables/tab_model_analysis}

\subsubsection{Embedder}
\label{subs:embedder}

The standard Embedder (Fig. \ref{fig:embedders_memory}) serves as the foundational layer upon which all subsequent layers build. It learns a unique vector representation $\delta$ of size $d$ (the model dimension \textit{d\_model}) for each token in the vocabulary of size $v$, positioning these embeddings within a $d$-dimensional space based on semantic similarity.

To capture not only semantic meaning but also the relative position of words within a sentence, the standard Embedder also includes a positional encoding component. This component learns to map each token position in an input sequence (up to a maximum length $\ell$) into a vector of size $d$, ensuring that the model understands token order alongside their meaning.

Additionally, to support models that process pairs of sentences within the same input context, the Embedder can incorporate segment embeddings. These embeddings learn two distinct vectors of size $d$ to differentiate between the first and second segments (e.g., two sentences), helping the model interpret relationships across sentence boundaries. Segment embeddings are widely used in models like BERT during pretraining to distinguish segments.

Together, these three components form essentially a set of dictionaries: token embeddings of size \( v \times d \), positional embeddings of size \( \ell \times d \), and segment embeddings of size \( 2 d \). This results in a total parameter count $W_{emb}$ for the Embedder of:
\begin{equation}
    W_{emb} = d \cdot (v + \ell + 2)
    \label{eq:embedder_params}
\end{equation}

During the inference phase, these embeddings are each transformed into matrices (also called activations) of size  \( \ell \times d \) and summed sequentially to form the input encoding. This leads to a maximum activation size for the embedding layer of:
\begin{equation}
    A_{emb} = 2 d \cdot \ell
    \label{eq:embedder_act}
\end{equation}

\subsubsection{BERT}
The Bert Language Model is used to analyze the sequence $\Delta$ of $\delta$s obtained by the Embedder from the input text to perform various tasks, e.g. text generation,  classification or regression. It does so by exploiting a sequence of layers, often called the Bert layers, that are identically repeated $N$ times, and are constituted by two consecutive layers: a (multi-head) attention layer and the feed-forward layer, each of which is followed by a Normalization layer. It is important to note that also the dimensions of the sequence used as input in each layer are equivalent to the ones of the original sequence $\Delta$, meaning that each BERT layer outputs an intermediate representation of the same size as the original sequence.

Approximating the small effects of the normalization layers, the memory requirements connected to the execution of a Bert model can be consequently estimated as:
\begin{align*} 
    W_{bert} = N \cdot (W_{Att} + W_{ff}) \\
    A_{bert} = \max (A_{Att}, A_{ff}) .
    \label{eq:bert_params}
\end{align*}
where $W_{bert}$ and $A_{bert}$ are the number of parameters and the maximum activation size of the model, $W_{Att}$ and $A_{Att}$ are the ones of a single attention layer and $W_{ff}$ and $A_{ff}$ are the ones of a single Feed Forward layer.

\paragraph{Attention}
\label{subsubs:attention}

The multihead attention layer used in the BERT architecture can be represented by the following formulas: 
\begin{equation}
    Attention(Q,K,V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d}}\right)V
\end{equation}
\begin{equation}
    MH(Q,K,V) = \text{Concat}(\text{head}_1, \text{head}_2, ..., \text{head}_h)W^O
\end{equation}
\begin{equation}
    \text{where}\  \text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)
\end{equation}

As shown thus we have three initial fully-connected projection layers ($W^Q, W^K, W^V$), which transform each input token in the input sequence into 3 representations ($Q,K,V$) of the same size $d$, that then get split between the $h$  different heads, matrix multiplications between these representations, and one output fully connected layer ($W^O$). Note that all the fully connected layers are of equivalent sizes.

The number of parameters $W_{att}$ connected to each attention layer can be consequently computed as:
\begin{equation}
W_{att} = 4 d^2
\end{equation}
Since the only weights present in these layers are the ones associated with fully connected layers. 

Differently, the size of intermediate activations contemporary active in this layer depends also on the number of attention-heads in the layer, and can be computed as:
\begin{equation}
A_{att} = 4 d \cdot l + l^2 \cdot h
\end{equation}

\paragraph{Feed-Forward}

The layer following the Attention one is the Feed-Forward layer simply represented by:
\begin{equation}
    \text{FF}(X) = \text{Act}(XW^{in})W^{out} + X
\end{equation}
It consist of two fully connected layers ($W^{in}, W^{out}$) that subsequently increase/decrease by a factor \(\alpha\) the embedding dimension while passing the intermediate result through an Activation function. The final output is summed to the input of the ff-layer trough a skip connection.

Consequently, the number of parameters of this feed-forward layer $W_{ff}$ can be computed as:
\begin{equation}
    W_{ff} = 2 d ^ 2 \cdot \alpha 
\end{equation}


While the maximum size of the activations contemporary used in this layer $A_{ff}$ is: 
\begin{equation}
    A_{ff} = 2 \ell \cdot d + \ell \cdot d \cdot \alpha  
\end{equation}

\subsubsection{MAMBA \& SSMs}
\label{subs:MAMBA&SSM}

MAMBA \cite{MAMBA} is a newer architectural design created for NLP taks. It leverages the \gls{ssm} design further improving it by making it's parameters dependent on the given input and constructing around it CUDA kernels for higly optimized training. It's main constituent part is the SSMs formula: 
\begin{equation}
    h_i = \Bar{A} h_{i-1} + \Bar{B}x_i
\end{equation}
\begin{equation}
    y_i = Ch_i + Dx_i
\end{equation}

where we $A$ and $B$ are learned matrices that represent functions that undergo a discretization process based on the zero-order hold technique to get:
\begin{equation}
    \Bar{A} = exp(\Delta A)
\end{equation} 
\begin{equation}
    \Bar{B} = (\Delta A)^{-1} (\Bar{A} - I) \cdot \Delta B 
\end{equation} 

while $C$, $D$ and $\Delta$ are derived from the input through dense layers that get them to their correct dimensions.

Adding to this already complex mechanism there is the new structure of the layer which merges the H3 design with that of Gated Multi Layer Perceptrons making the original input first go through two distinct fully connected layers and processing one of them with a simple activation function, while the other goes through: a 1D convolutional layer, an activation function and the SSM to be then multiplied elementwhise with the other processed input and passing through a final dense layer. This all can be represented as:
\begin{equation}
    MAMBA(x) = SERVE??
\end{equation}

All this equates to a parameter count of:
\begin{equation}
    W_{MAMBA} = i \cdot (3 d + 3 d_s + 2 \rho + c + 1)
\end{equation}

While the maximum size of the activations between all the processing is:
\begin{equation}
    A_{MAMBA} = \ell \cdot (d + 3i + 2i \cdot d_s + d_s) + \max (i \cdot d_s; \ \ell \cdot d_s) 
\end{equation}

\subsection{Optimizations}

While the MAMBA model is relatively new, and thus its architecture has not yet seen many optimizations, the components of the BERT model have seen multiple redesigns in more recent works in the literature, including some optimizations of the Attention mechanism. In the same way, some optimizations of the Embedder have been proposed in the literature. We now analyze one popular optimization of the attention mechanism, the Efficient Attention, and one optimization of the Embedder, the Nano Embedder, and their effects on the requirements of the architecture.

\subsubsection{Nano Embbedder}
\label{subsubs:NanoEmbedder}

The Nano Embedder (Fig. \ref{fig:embedders_memory}), as introduced by \cite{NanoBERT}, fulfills the same function as the standard Embedder but with a key difference: rather than embedding tokens and positions directly into vectors of size $ d $, it first maps these inputs to a reduced embedding dimension, which we denote as $ r_d $. This reduced embedding is then projected to the original dimension $ d $ through a fully connected layer. Segment embeddings are an exception to this process, as the fully connected projection matrix required for them would negate the parameter savings achieved by reducing the embedding size.
This alternative embedding approach results in a total parameter count $W_{nemb}$ of:

\begin{equation}
W_{nemb} = r_d \cdot (v + \ell + 2 d) + 2 d
\end{equation}

Although this formula might appear to suggest a higher parameter count, the total can actually be lower than that of the standard Embedder if $ r_d $ is sufficiently small relative to $ d $. However, the total activation size $A_{nemb}$ increases slightly due to the projection step, resulting in: 

\begin{equation}
    A_{nemb} = r_d \cdot \ell + 2 d \cdot \ell.
\end{equation}

\subsubsection{Efficient Attention}
\label{subsubs:Efficient Attention}

The Efficient Attention layer (Fig.~\ref{fig:attentions_memory}), as proposed by~\cite{efficient_attention}, simplifies the architecture of the standard Attention layer by reducing the number of fully connected layers. Instead of three fully connected layers at the beginning of the layer, it uses only one at the start to produce the Query matrix, while Key and Value matrices are just the same as the input, and one final fully connected layer for the attention output both of sizes \(d^2\). 

The number of parameters $W_{eatt}$ of the Efficient Attention layers can consequently be calculated as:

\begin{equation}
W_{eatt} = 2 d ^ 2
\end{equation}

Like the standard Attention layer, the Efficient Attention layer’s highest activation size is reached during the matrix multiplication step, which produces an activation size of:

\begin{equation}
A_{eatt} = 2\ell \cdot d + \ell^2
\end{equation}




%-----------------------------------------------------------------------------
% Models
%-----------------------------------------------------------------------------
\begin{table*}[t]
    \caption{Formulas for calculating memory accesses, summations and multiplication performed by each layer, based on their architectural parameters.  \textbf{nel main text?}}
    \centering
        \begin{tabular}{|c | c c c|}
        \hline
        \textbf{Layers} & \textbf{Memory accesses} & \textbf{Summations} & \textbf{Multiplications} \\
        \hline \hline
                        
        \textbf{Embedder}               & \(\ell \cdot (4d + 2) + 2d\)  
                                        & \(\ell \cdot 2d\) 
                                        & 0   \\
        
        \textbf{NanoEmbedder}           & \(\ell \cdot (r_d \cdot 4d + d + 2r_d + 2) + 2d\)    
                                        & \(2 \ell \cdot d \cdot (r_d + 1)\)
                                        & \(\ell \cdot r_d \cdot d \cdot 2\) \\ 

        \textbf{Normalization}          & \((\ell+1) \cdot d \cdot 2\)    
                                        & \(\ell \cdot d\)
                                        & \(\ell \cdot d\)\\
                                        
        \textbf{Feed Forward}           & \(4 \ell \cdot d \cdot (d \cdot \alpha + 1)\)
                                        & \(2 \ell \cdot d \cdot (d \cdot \alpha + 1)\)
                                        & \(2 \ell \cdot d \cdot (d \cdot \alpha + 1)\) \\
        
        \textbf{Attention}              & \(8\ell \cdot d^2 + 2 \ell^2 \cdot h \cdot (2d + 1)\)
                                        & \(\ell \cdot d \cdot (4d + 1) + \ell^2 \cdot h \cdot (2d + 1)\)
                                        & \(4\ell \cdot d^2 + \ell^2 \cdot h (2d + 3)\)\\
        
        \textbf{Efficient Attention}    & \(4\ell \cdot d^2 + 4\ell^2 \cdot d + 2\ell^2\)    
                                        & \(2\ell \cdot d^2 + 2\ell^2 \cdot d + \ell^2 + \ell \cdot d \)
                                        & \(2\ell \cdot d^2 + 2\ell^2 \cdot d + 2\ell^2\)\\        
                                        
        \textbf{Eff Diff Skip Attention}& \(\ell \cdot d^2 \cdot (4 + 2\alpha) + \ell^2 \cdot (4d + 2) + \ell \cdot d \cdot k \cdot \alpha \)
                                        & \(\ell \cdot d^2 \cdot (2 + \alpha) + \ell^2 \cdot (2d + 1) + \ell \cdot d \cdot (\alpha \cdot k + 1) \)
                                        & \(\ell \cdot d^2 \cdot (2 + \alpha) + \ell^2 \cdot (2d + 2) + \ell \cdot d \cdot \alpha \cdot k \)\\
                                        
        \textbf{MAMBA main}             & \(\ell \cdot i \cdot (d \cdot 6 + 9) + \ell \cdot d + c + SSM\)     
                                        & \(\ell \cdot i \cdot (d \cdot 3 + 2 + c) + \ell \cdot d + SSM\)     
                                        & \(\ell \cdot i \cdot (d \cdot 3 + 5 + c) + SSM\) \\

        \textbf{SSM}                    & \(\ell \cdot i \cdot (d_s \cdot 18 + \rho \cdot 4 + 8) + i \)
                                        & \(\ell \cdot i \cdot (d_s \cdot 4 + \rho \cdot 2 + 2)\)
                                        & \(\ell \cdot i \cdot (d_s \cdot 7 + \rho \cdot 2 + 2)\) \\
        \hline
        \end{tabular}
    \label{table:complexity_appendix}
\end{table*}
\section{Scaling and Designing LM}
\label{sec:Models}

\input{tables/tab_models_hyperparameters}

A first approach for enabling the execution of large language models (LLMs) on microcontrollers involves significantly reducing model size by adjusting key hyperparameters within the chosen architecture.

We present several model variations specifically designed for deployment on resource-constrained devices, each optimized to offer different levels of performance and complexity. Table \ref{table:model_params} summarizes the hyperparameters of all proposed models, while Table \ref{table:model_sizes} provides details on the total parameter count and activation sizes for each configuration.

The selection of hyperparameters for our proposed models follows an iterative approach, guided by a baseline allocation within the 2 MB memory constraint assigning approximately half for weights and the other half for activations. We began by examining our datasets to identify the average maximum sentence length post-tokenization, which was approximately 256 tokens; this informed our choice of 256 as the standard sentence length for all models.

Next, we determined the vocabulary size, chosen as a power of two to utilize about half of the available weight allocation efficiently. Given that embedding and hidden dimensions have a major impact on memory usage, we iteratively adjusted these parameters to achieve an optimal distribution within our memory constraints. The dimensionality \(d\_model\) significantly influences total weight as the number of layers increases, making adjustments to it essential for balancing model size and performance. Following NanoBERT \cite{NanoBERT} methodology, we experimented with reduced \(d\_model\) values of 16 or 32, selecting the largest feasible value within the memory limits. Priority was given to maximizing both \(d\_model\) and vocabulary size, as these contribute most directly to the model’s language understanding and representational capacity.

Additionally, the choice of \(d\_model\), forward expansion, and the number of layers must consider the inherent trade-off between model width and depth, which strongly impacts the model’s learning effectiveness. At this constrained size, a shallower, wider model may converge faster but potentially generalize poorly, whereas a deeper, narrower model might struggle to learn effectively. For models with limited memory, this structure warrants careful tuning, as depth and width affect the model's ability to capture complex patterns without overfitting or underfitting.

Finally, we adjusted the forward expansion dimension, which plays a critical role in the model's ability to retain information learned during pretraining, a particularly usefull feat in text genenrations scenarios. Through iterative parameter tuning, we aimed to achieve a balance that respects the 2 MB memory limit while maximizing the model’s effectiveness in text classification.



\subsection{BERT (2MB)}
\label{subs:BERT(2MB)}


The \gls{sota} model BERT, from \cite{BERT}, is sized around 110 million parameters, making it impossible to fit inside tiny devices. Nevertheless it's architecture can be scaled down by adjusting it's hyperparameters. For this reason we decided to try to downsize it, creating what we call \textit{BERT(2MB)}. 

We did so by employing the standard embedder (Section \ref{subs:embedder}) with learned token encodings and positional encodings, along with the transformer's encoder with multi-head attention, presented in Section \ref{subsubs:attention} with the implementation from PyTorch. Inspired by the scaling down of BERT to BERT-Tiny in \cite{tinyBERT}, we reduced the input embedding dimension $d$ and the number of layers while maintaining as high as possible the number of heads $h$ and fully connected section expansion $\alpha$. This required balancing the size of the embedder and attention layers, as the standard embedder tends to occupy the most space at this size.

Moreover, to stay under 2MB we had to reduce the dictionary dimension $v$ from 32'768 to 2'048 and set the sentence length $\ell$ to 258. We then set $d$ to 80, $\alpha$ to \(2\times\), and $h$ to 2. By repeating this constructed layer twice, we obtained a total of 289k parameters, with 184k attributed to the embedder, thus reducing the original BERT model by about $500\times$.




\subsection{MAMBA (2MB)}
\label{subs:MAMBA}

The \gls{sota} model MAMBA, from \cite{MAMBA}, much like BERT, is over sized for this context with it's 130 million parameters but can be scaled down by modifying it's hyperparameters and using some tricks to make it's huge activations fit in the 2MB memory limit. 

To do so we employed the use the new embedder proposed in NanoBERT \cite{NanoBERT} (Section \ref{subs:NanoEmbedder}), which reduces token embeddings into a lower-dimensional space before expanding them through a fully connected layer to the defined $d$ size. This greatly reduced the number of parameters in the embedder layer leaving space to the activations of the SSM layer (Section \ref{subs:MAMBA&SSM}. 
Additionally, for the embedder, we used a dictionary size of 8'192, a reduced embedding size of 16, and a final embedding dimension of 48. The other parameters of the MAMBA layer are: $\alpha$ set to 2\(\times\) , which together with \(d\) gives us the inner dimension \(i = \alpha \cdot d\) , then \(d_s\), which represents the size of the internal memory of the model also called \(d\_state\), of just 4 and a standard 1D convolutional kernel size \(c\) of 4. By repeating this constructed layer four times, we achieved a total of 204k parameters, with 136k attributed to the embedder, thus reducing, much like our BERT(2MB), the original model by about $500\times$.



\subsection{Just embedder}
\label{subs:Just embedder}

To revise the use of simpler architectures that can give us a more direct comparison for the minimum expected capacity of our models and given that the embedder occupies most of the available model parameters while having a lower activation size compared to attention or MAMBA layers, we opted to scale just this layer as much as possible removing any other part that usually constitutes LLM models. We opted to use the same embedder we are using for MAMBA from the NanoBERT paper \cite{NanoBERT} due to its reduced number of parameters. This enabled us to use a dictionary size of 8'192 with a reduced embedding dimension of 32, which was found to be optimal in the original paper \cite{NanoBERT}, and a final embedding size of 320 with a sentence length of 256. 

This model has just 293K parameters, however, due to the way it is constructed, this model lacks any capability for token embeddings to interact with each other, rendering it unsuitable for text generation purposes. 


\subsection{Embedder and convolution}
\label{subs:Embedder+conv}
To address the limitations of the embedder-only model, we appended to it a 1D convolutional layer with a kernel size of 16 to enable some interaction between token embeddings. This resulted in just 5k more parameters than the embedder-only model presented in \ref{subs:Just embedder}, while enabling the interaction between token embeddings.

\subsection{BERT-Tiny}
\label{subs:BERT-Tiny}
Adding to the already wide variety of models we can compare against we opted, to give a wider sense of scale, to compare agains a 10 times bigger model from the creators of BERT \cite{BERT}, called BERT-Tiny \cite{tinyBERT}, which can be considered \gls{sota} for the size. It is constructed, much like our BERT(2MB), with an embedding size of 128, a forward expansion of $2\times$, two heads and two layers but it uses a much bigger dictionary of 32'768 thus outnumbering all our proposals with its 4.4M parameters. Furthermore this model has been pretrained on the Open Book Corpus and then trained again through a distillation process using BERT as teacher.

\subsection{EmbBERT}
Finally, coming to our original proposal, \textit{EmbBERT}, to showcase its potential effectively we opted to try two different configurations:
\begin{enumerate}
    \item The first is a more balanced setup with an embedding size of 128, forward expansion of \(0.7\times\), and four consecutive layers. We chose the smaller expansion size based on the knowledge that the forward expansion section of the transformer stores information on trained subjects, which is not relevant in our case, being the main task of the model not the one of generating new tokens. 
    \item The second configuration uses an embedding size of 64, forward expansion of \(2\times\), and ten layers. This model is more challenging to train due to its thin and long structure but may be better suited for tasks requiring higher interactions between tokens.
\end{enumerate}

Thanks to the new layer constituting these models and their low activation sizes we managed to achieve a parameter count of 366k and 389k respectively.

% \mas{\textbf{TODO: rewrite this chapter using nomenclature used introduced in the chapter EmbBErt}}

% For this model, which we decided to call EmbBERT due to it mainly originating from the BERT \cite{BERT} model and its application in the embedded processors world, we choose to always use the same embedder as the one presented for the MAMBA baseline in \ref{subs:MAMBA} with a dictionary size of 8'192, a reduced embedding size of 16 and a sentence length of 256. To further reduce the number of parameters instead of the standard attention used in BERT we implemented the efficient attention (Fig. \ref{fig:attentions_memory}) reported in \cite{efficient_attention} without multihead which does use half the number of fully connected layers and results in a much lower activation size while supposedly keeping about the same performances as the standard attention. We opted not to use multihead due to its high increase of the size of the activations and because from \cite{exploring_transformer_sizes} multihead has been shown for this size not to be as effective as in bigger models. After the Efficient Attention we used the standard up and down projection of the BERT model with \gls{silu} as activation function. 

% To showcase the EmbBERT model potential effectively, we employed two different configurations:
% \begin{enumerate}
%     \item The first is a more balanced setup with an embedding size of 128, forward expansion of \(0.7\times\), and four consecutive layers. We chose the smaller expansion size based on the knowledge that the forward expansion section of the transformer stores information on trained subjects, which is not relevant in our case, being the main task of the model not the one of generating new tokens. 
%     \item The second configuration uses an embedding size of 64, forward expansion of \(2\times\), and ten layers. This model is more challenging to train due to its thin and long structure but may be better suited for tasks requiring higher interactions between tokens.
% \end{enumerate}

\input{tables/results/tab_tinyml_pretrained}

\input{tables/results/tab_glue_pretrained}


\subsection{Complete results}


\subsubsection{TinyNLP Benchmark Analysis}
The TinyNLP benchmark results in Table~\ref{table:all_results_tinyNLP} show the models' adaptability to diverse NLP tasks, particularly in lightweight and resource-constrained settings. We now go through an overview of the obtained results:

\paragraph{Baseline Models}
BERT-2MB achieves an average accuracy of 83.93, with its highest performance on Snips (97.00). However, it consistently underperforms on datasets like IMDb (79.38) and LiMiT (74.72). MAMBA-2MB in the same fashion doesn't manage to achieve high results with an average accuracy of 81.69 due to lack in performance mostly on Emotion and nlu datasets.

\paragraph{Embedder Variants}
Embedder and Embedder+Conv perform competitively, with average accuracies of 86.41 and 86.50, respectively. These models despite not being capable of high token interaction excel in some simpler datasets such as Emotion (89.40 and 89.45) and Snips (97.93 and 97.75).

\paragraph{BERT with NanoEmbedder and Efficient Attention}
The BERT model with the Nano Embedder manages to achieve the lowest average results (77.08) between all the comparisons, probabily due to it's inhability to leverage the more expanded dictionary that comes with the Nano Embedder. The BERT with Efficient Attention, with it's average accuracy of 85.64, while still not coming close to the results of the simple embedder only model sill shows a noticeable improvement with respect to the simple BERT-2MB model.
BERT+NE+EA achieves an average score of 87.04, with high accuracy on datasets like IMDb (83.19) and Emotion (88.70) thus managing to leverage well the combined abilities of the Efficient Attention and the expanded dictionary obtained thanks to the Nano Embedder. It's quantized variant, BERT+NE+EA (8bit), achieves a score of 85.92, suggesting good enough robustness to quantization but achieving still results not higher than the simple embedder ones.

\paragraph{EmbBERT}
EmbBERT outperforms its competitors, achieving an average score of 87.19. It demonstrates consistent strength across all datasets, with notable success on Snips (97.67) and Emotion (89.58). This highlights its capability for nuanced intent recognition and sentiment analysis.
The quantized variant, EmbBERT(8bit), achieves the highest average score of 88.17, excelling in Cyberbullying (86.60) and NLU (94.05) demonstrating it's high resilience to quantization and further ability to improve performances thanks to the final parameter efficient finetuning.


\subsubsection{GLUE Benchmark Analysis}
The GLUE benchmark results, summarized in Table~\ref{table:res_glue_comp}, reveal significant variability in performance across tasks, reflecting the diverse challenges posed by the benchmark datasets. Key observations are as follows:

\paragraph{Baseline Models}
BERT-2MB demonstrates the lowest overall performance between the baseline models, with a GLUE score of 52.10. Its limitations are particularly evident in tasks such as CoLA (-0.86) and STSB (15.48).
MAMBA-2MB shows moderate improvement over BERT-2MB, achieving a GLUE score of 55.03. However, it still struggles with semantic similarity tasks like STSB (10.16) mostly due to it's architecture.

\paragraph{Embedder Variants}
Embedder and Embedder+Conv both outperform the baseline models, with scores of 56.92 and 56.41, respectively. These models given their simplicity should be the lowest acceptable results for models with more complex and cabable architectures. Thei manage to excel particularly QQP (83.28 and 82.98), although their performance on CoLA (9.65 and 9.25) remains suboptimal.
The addition of convolutional layers in Embedder+Conv slightly enhances its performance in some tasks, such as WNLI (79.16), but does not consistently outperform the Embedder model.

\paragraph{BERT with NanoEmbedder and Efficient Attention}
BERT+NE, BERT+EA, and their combination (BERT+NE+EA) show varying levels of success. While BERT + NE, contrary to it's tinyNLP performances, barely manages to surpass the Embedder and Embedder+Conv results with 57.50, BERT + EA struggles to emerge due to it's subpar performances on a variety of datasets.
While BERT+NE+EA achieves a competitive score of 61.20, its 8-bit quantized variant exhibits a marked performance drop (45.97), particularly in challenging tasks like MNLI-m and MNLI-mm.

\paragraph{EmbBERT}
Our model, EmbBERT, achieves the highest GLUE score of 63.50, reflecting its robust performance across most tasks. Notably, it excels in CoLA (11.01), MRPC (69.19), and MNLI tasks (67.83 and 68.63), demonstrating superior adaptability to syntactic and semantic challenges.
The 8-bit quantized variant, EmbBERT(8bit), maintains competitive performance with a score of 62.81 even surpassing the unquantized results of BERT+NE+EA thus showing it's resilience to quantization even in complex tasks.


\end{document}
