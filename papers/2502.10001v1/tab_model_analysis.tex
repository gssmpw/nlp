\begin{table*}[htbp]
    \caption{Formulas for calculating weights and activation sizes per layer, based on their hyperparameters.}
    \begin{center}
    
        \begin{tabular}{|c | c c|}
        \hline

        \textbf{Layers} & \textbf{Weights} & \textbf{Activations} \\
        \hline \hline
        \textbf{Embedder}               & $(v + \ell +2)\cdot d$ 
                                        & $2\cdot d\cdot \ell$ \\
        \textbf{NanoEmbedder}           & \(v \cdot r_d + \ell \cdot r_d + 2 \cdot d + 2 \cdot r_d \cdot d\) 
                                        & \(r_d \cdot \ell + 2 \cdot d \cdot \ell\) \\
        \textbf{Normalization}          & \(2 \cdot d\)
                                        & \(2 \cdot d \cdot \ell\) \\
        \textbf{Feed Forward}           & \(2 \cdot d^2 \cdot \alpha \) 
                                        & \(2 \cdot \ell \cdot d + \ell \cdot d \cdot \alpha\)\\
        \textbf{Attention}              & \(4 \cdot d^2 \)
                                        & \(4 \cdot d \cdot \ell + \ell^2 \cdot h\)\\
        \textbf{Efficient Attention}    & \(2 \cdot d^2 \) 
                                        & \(2 \cdot d \cdot \ell + \ell^2 \)\\
        \hline
        \end{tabular}
    
    \end{center}
    \label{table:memory_analysis}
\end{table*}



\begin{table*}[htbp]
    \caption{Formulas for calculating memory accesses, summations and multiplication performed by each layer, based on their hyperparameters.}
    \centering
    
        \begin{tabular}{|c | c c c|}
        \hline
        \textbf{Layers} & \textbf{Memory accesses} & \textbf{Summations} & \textbf{Multiplications} \\
        \hline \hline
                
        \textbf{Embedder}               & \(\ell \cdot (d \cdot 4 + 2) + d \cdot 2\)  
                                        & \(\ell \cdot d \cdot 2\) 
                                        & 0   \\
        
        \textbf{NanoEmbedder}           & \(\ell \cdot (r_d \cdot d \cdot 4 + d + r_d \cdot 2 + 2) + d \cdot 2\)    
                                        & \(2 \cdot \ell \cdot d \cdot (r_d + 1)\)
                                        & \(\ell \cdot r_d \cdot d \cdot 2\) \\ 

        \textbf{Normalization}          & \((\ell+1) \cdot d \cdot 2\)    
                                        & \(\ell \cdot d\)
                                        & \(\ell \cdot d\)\\
                                        
        \textbf{Feed Forward}           & \(4 \cdot \ell \cdot d \cdot (d \cdot \alpha + 1)\)
                                        & \(2 \cdot \ell \cdot d \cdot (d \cdot \alpha + 1)\)
                                        & \(2 \cdot \ell \cdot d \cdot (d \cdot \alpha + 1)\) \\
        
        \textbf{Attention}              & \(\ell \cdot d^2 \cdot 8 + \ell^2 \cdot h \cdot 2 \cdot (d \cdot 2 + 1)\)
                                        & \(\ell \cdot d \cdot (d \cdot 4 + 1) + \ell^2 \cdot h \cdot (d \cdot 2 + 1)\)
                                        & \(\ell \cdot d^2 \cdot 4 + \ell^2 \cdot h (d \cdot 2 + 3)\)\\
        
        \textbf{Efficient Attention}    & \(\ell \cdot d^2 \cdot 4 + \ell^2 \cdot d \cdot 4 + \ell^2 \cdot 2\)    
                                        & \(\ell \cdot d^2 \cdot 2 + \ell^2 \cdot d \cdot 2 + \ell^2 + \ell \cdot d \)
                                        & \(\ell \cdot d^2 \cdot 2 + \ell^2 \cdot d \cdot 2 + \ell^2 \cdot 2\)\\        
                         
        \hline
        \end{tabular}

    \label{table:complexity}
    
\end{table*}