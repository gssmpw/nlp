\begin{table*}[t]
    \caption{Formulas for calculating weights and activation sizes per layer, based on their architectural parameters.}
    \begin{center}
        \begin{tabular}{|c | c c|}
        \hline
        \textbf{Layers} & \textbf{Weights} & \textbf{Activations} \\
        \hline \hline
        \textbf{Embedder}               & $(v + \ell +2)\cdot d$ 
                                        & $2 d\cdot \ell$ \\
        \textbf{NanoEmbedder}           & \(r_d \cdot (v + \ell + 2 d ) + 2 d\)
                                        & \(r_d \cdot \ell + 2 d \cdot \ell\) \\
        \textbf{Normalization}          & \(2 d\)
                                        & \(2 d \cdot \ell\) \\
        \textbf{Feed Forward}           & \(2 d^2 \cdot \alpha \) 
                                        & \(d \cdot \ell \cdot ( 2 \alpha )\)\\
        \textbf{Attention}              & \(4 d^2 \)
                                        & \(4 d \cdot \ell + \ell^2 \cdot h\)\\
        \textbf{Efficient Attention}    & \(2 d^2 \) 
                                        & \(2 d \cdot \ell + \ell^2 \)\\
        \textbf{Eff Attention + Conv Skip} & \(2 d^2 + d^2 \cdot \alpha + c \cdot d \alpha\)
                                        & \(\max ( 2 d \cdot \ell + \ell^2 ; \  d \cdot \ell (2 + \alpha )) \)\\
        %\textbf{MAMBA layer}            & \(i \cdot (3d + c + 2 + 3d_s + 2\rho)\)
        %                                & \( \ell \cdot (d + 3i + 2i \cdot d_s + d_s) + \max (i \cdot d_s; \ \ell \cdot d_s)  \)\\
        \hline
        \end{tabular}
    \end{center}
    \label{table:w&a_noMAMBA_full}
\end{table*}