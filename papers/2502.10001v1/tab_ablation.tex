\begin{table*}[htbp]
    \caption{Model's performances on TinyNLP benchmark reporting accuracy on each singular dataset, and overall averages.}
    \begin{center}    
    
        \begin{tabular}{| c | c  c  c  c  c  c  c | c |}
        \hline
        \textbf{Models}&
        \href{https://huggingface.co/datasets/stanfordnlp/imdb}{\textbf{IMDb}} &
        \href{https://huggingface.co/datasets/fancyzhx/ag_news}{\textbf{ag\_news}}  &
        \href{https://www.kaggle.com/datasets/andrewmvd/cyberbullying-classification}{\textbf{cyberbull}}  &
        \href{https://huggingface.co/datasets/IBM/limit}{\textbf{LiMiT}} &
        \href{https://huggingface.co/datasets/dair-ai/emotion}{\textbf{Emotion}} &
        \href{https://huggingface.co/datasets/xingkunliuxtracta/nlu_evaluation_data}{\textbf{nlu}} &
        \href{https://huggingface.co/datasets/benayas/snips}{\textbf{Snips}} &
        \textbf{Average} \\
        \hline

        BERT (2MB) & 79,38 & 89,00 & 83,90 & 74,72 & 77,34 & 86,14 & 97,00 & 83,93 \\
        \hline

        BERT + NE \cite{NanoBERT}       & 83,32 & 90,64 & 84,06 & 73,80 & 87,28 & 86,50 & 97,90 & 86,21 \\
        \hline
        BERT + EA \cite{efficient_attention}       & 80,46 & 89,46 & 84,58 & 74,12 & 85,78 & 87,44 &  97,62 & 85,64 \\
        \hline
        BERT + NE + EA          & 83,19 & 90,80 & 84,13 & 75,80 & 88,70 & 88,88 & 97,79 & 87,04\\
        \hline \hline
        \textbf{EmbBERT (ours)} & 84,10 & 90,46 & 83,97 & 76,36 & 89,58 & 88,16 & 97,67 & 87,19 \\

        \hline
        \end{tabular}

    \label{table:res_tiny_abl}
    \end{center}
    
\end{table*}


\begin{table*}[htbp]
    \caption{Model's performances on GLUE benchmark reporting for COLA the MCC, for MRPC and QQP the F1 score, for STSB the \gls{scc} and accuracy for the rest as required to officialy calculate the GLUE overall model's score}
    \begin{center}
    
        \begin{tabular}{| c | c c c c c c c c c c | c |}
        \hline
        \textbf{Models}     & COLA      & SST-2     & MRPC      & QQP       & MNLI-m    & MNLI-mm   & QNLI      & RTE       & WNLI      & STSB      & Score \\
        \hline
        BERT                & -0,86     & 71,28     & 64,66     & 73,04     & 60,56     & 61,58     & 60,82     & 48,24     & 66,20     & 15,48     & 52,10 \\
        \hline
        BERT + NE \cite{NanoBERT}   & 9,04      & 78,82     & 65,04     & 79,96     & 63,08     & 63,30     & 63,20     & 51,76     & 87,30     & 13,46     & 57,50 \\            
        \hline
        BERT + EA \cite{efficient_attention}   & 10,06     & 79,44     &  66,48    & 78,88     & 60,82     & 61,20     & 63,50     & 50,76     & 22,24     & 17,92     & 51,09 \\
        \hline
        BERT + NE + EA      & 18,70     & 79,60     & 65,36     & 82,66     & 67,06     & 67,44     & 67,44     & 53,66     & 86,74     & 23,34     & 61,20 \\
        \hline
        \hline
        \textbf{EmbBERT (ours)}    & 65,47 & 79,33 & 69,61 & 83,13 &  67,83 & 68,63 & 68,92 & 49,96 & 87,61 & 71,85 & 63,50    \\
        \hline
        \end{tabular}

    \label{table:res_glue_abl}
    \end{center}

    
    
\end{table*}