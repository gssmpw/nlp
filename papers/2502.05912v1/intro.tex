\section{Introduction}
\label{sec:intro}

The \emph{Cardinality Estimation} problem, or \ce for short, is 
to estimate the output size of a query using only simple, precomputed
statistics on the database. \ce is one of the oldest and most
important problems in databases and data management.  It is used as the
primary metric guiding cost-based query optimization, for making
decisions about every aspect of query execution, ranging from broad
logical optimizations like the join order, to deciding the number of
servers to distribute the data over, and to detailed physical
optimizations, like the use of bitmap filters and memory allocation
for hash tables.

Unfortunately, \ce is notoriously difficult, and this affects
significantly the performance of data management systems.  Current
systems use density-based estimators, which were pioneered by System
R~\cite{DBLP:conf/sigmod/SelingerACLP79}.  They make drastic
simplifying assumptions (uniformity, independence, containment of
values, and preservation of values), and when the query has many joins
and many predicates, then they tend to have large errors, leading to
poor decisions by the downstream system; for example, the independence
assumption often leads to major
underestimation~\cite{DBLP:journals/pvldb/LeisGMBK015}.  Density-based
\ce also has limited support for queries with \groupby: most existing
systems yield poor estimates for the number of distinct
groups~\cite{DBLP:conf/cidr/Freitag019}. Yet the main problem with
traditional \ce is that it does not come with any theoretical
guarantees about its estimate: it may under-, or over-estimate, by a
little or by a lot, without any warning.  Several studies have shown
repeatedly that errors in the cardinality estimator can significantly
degrade the performance of most advanced database
systems~\cite{DBLP:journals/pvldb/LeisGMBK015,DBLP:journals/pvldb/LeeDNC23}.
To escape the simplifying assumptions of density-based CE, several
estimators were put forward that learn a model of the underlying
distribution in the database,
e.g.,~\cite{deepdb,bayescard,neurocard,flat}. This is a promising line
of work, yet as previously reported (and shown in our
experiments), their deployability is poor~\cite{FactorJoin:SIGMOD23} as
they lack explainability, have very slow training time, large model
size, and are difficult to transfer with comparable accuracy from one
workload to new workloads. One reason for this is that they need to
de-normalize the joined relations and add up to exponentially many
extra columns to represent new features.  Cardinality estimation
thus remains one of the major open challenges in data management.

In this paper, we introduce \system, a cardinality estimator 
that offers a one-sided guarantee: the true cardinality is guaranteed
to be below that returned by \system.  That is, \system returns
a guaranteed \emph{upper bound} on the size of the query  output.
Moreover, \system can explain the computed upper bound in terms of a
simple inequality, called a \emph{q-inequality}.  This one-sided
guarantee can be of use in many applications, for example it can
guarantee that a query does not run out of memory, or it can put an
upper bound on the number of servers required to distribute the output
data.  The challenge with this approach is to not overestimate too
much.  In other words, we want to reduce this upper bound as much as
possible, while still maintaining the theoretical one-sided guarantee.
To achieve that, we introduce novel statistics on the database, and
demonstrate that they lead to strictly improved upper bounds.  As an
extra bonus, \system applies equally well to \groupby queries.

There have been a small number of implementations that compute upper
bounds on the cardinality, commonly called \emph{pessimistic
  cardinality estimation}, or \pce for
short~\cite{DBLP:conf/sigmod/CaiBS19,DBLP:journals/pvldb/ChenHWSS22,DBLP:journals/tods/MhedhbiKS21}.
However, these systems were limited because they used only two types
of statistics on the input data: relation cardinalities, $|R|$, and
maximum degree (a.k.a.  maximum frequency) of an attribute $R.X$: if
the values of $R.X$ are $x_1, \ldots, x_N$, then the maximum degree is
$\max_i |\sigma_{X=x_i}(R)|$.  By using only limited input statistics,
these first-generation \pce systems led to significant overestimates,
and had worse accuracy than traditional, density-based \ce systems.
% For example, for a 2-way join $R \Join_{X=Y} S$, they would return
% the minimum of $a\cdot |S|$ and $|R| \cdot b$, where $a$ is the
% maximum degree of $R.X$ and $b$ is the maximum degree of $S.Y$.

\begin{figure}[t]
\begin{minipage}{0.48\textwidth}
    \includegraphics[width=\textwidth,keepaspectratio]{FIGS/degree_both}
    \caption{The \texttt{Authors}-\texttt{Publication} relationship in 
      DBLP (in 2023) with $24\cdot 10^6$ records pairing $3.6\cdot 10^6$ 
      authors and $7.1\cdot 10^6$ publications. The figure shows the 
      degree sequences $\degree(\texttt{authorID})$ and 
      $\degree(\texttt{pubID})$: The latter starts with a lower maximum degree, but 
      has a longer tail (see inset). The author with rank 1 is 
      \texttt{H. Vincent Poor} ($2951$ publications) and the publication 
      with rank 1 is~\cite{DBLP:journals/tmlr/SrivastavaRRSAF23} (with 
      $450$ authors).}
    \label{fig:ds}
\end{minipage}
\hfill
\begin{minipage}{0.48\textwidth}
    \includegraphics[width=\textwidth,keepaspectratio]{FIGS/lp}
    \caption{Instead of storing the two degree sequences, \system stores 
      only some of their $\ell_p$-norms, for example for $p\in \set{1,\ldots, 10,\infty}$ 
      shown here. We do not show $\ell_1$ (it is equal to the cardinality 
      \revthree{$24\cdot 10^6$}) and $\ell_\infty$ (it is the maximum degree, 
      $2951$ or $450$ respectively). When $p$ ranges from $1$ to $\infty$, the 
      $\ell_p$-norm ranges from the relation's cardinality to the maximum 
      degree of any value.}
    \label{fig:lp}
\end{minipage}
\end{figure}

To achieve better upper bounds, we use significantly richer statistics
on the input database.  Concretely, we use the {\em $\ell_p$-norms of
  degree sequences} as inputs to \system.  The \emph{degree sequence}
of an attribute $R.X$ is the sequence
$\degree_R(X) = (d_1, d_2, \ldots, d_N)$, sorted in decreasing order,
where $d_i$ is the frequency of the value $x_i$.  The $\ell_p$-norm is
$\left(\sum_i d_i^p\right)^{1/p}$.  The $\ell_p$-norms of degree
sequences are related to \emph{frequency moments}~\cite{DBLP:conf/stoc/AlonMS96}: 
The $p$'th frequency moment is $\sum_i d_i^p$. These are 
commonly used in statistics and machine learning, since they
capture important information about the data distribution.  This
information can be very useful for cardinality estimation too. 
It is also practical as it can be computed and maintained efficiently~\cite{DBLP:conf/stoc/AlonMS96}. 
Yet, to the best of our knowledge, the $\ell_p$-norms (or frequency moments)
have not been used before for cardinality estimation.  \system is, to
the best of our knowledge, the first to use them for cardinality
estimation.
% for example
% the $\ell_1$-norm is the cardinality of $R$, the $\ell_\infty$-norm is
% the maximum degree of $X$, the $\ell_2$-norm is some measure of the
% non-uniformity of $X$, etc.
Fig.~\ref{fig:ds} shows the degree sequences of the
Author-Publication relationship in the DBLP database, and
Fig.~\ref{fig:lp} shows some of their $\ell_p$-norms. 


\system takes as input a query with equality joins,
equality and range predicates, and group-by clause, and computes an upper
bound on the query output size, by using precomputed $\ell_p$-norms on the input
database. \system offers a strong, theoretical guarantee: for any database that satisfies the
given statistics, the query output size is guaranteed to be below the
bound returned by \system.  The bound is \emph{tight}, in the sense
that, if all we know about the input database are the given
statistics, then there exists a worst-case input database with these
statistics on which the query output is as large as the bound
returned by \system.  Finally, \system is able to explain the upper
bound, in terms of a simple \emph{q-inequality} relating the output
size to the input statistics.

\paragraph{Contributions} In this paper we make four main contributions.  

{\em 1. We introduce \system, a PCE that uses $\ell_p$-norms of input relations (Sec.~\ref{sec:lpbound})}. 
\system is a principled framework to compute the upper bound, based on
information theory, building upon, and expanding a long line of
theoretical
results~\cite{DBLP:journals/siamcomp/AtseriasGM13,DBLP:journals/jacm/GottlobLVV12,DBLP:conf/pods/KhamisNS16,DBLP:conf/pods/Khamis0S17,DBLP:journals/pacmmod/KhamisNOS24}.
We show how to extend previous results to 
accommodate \groupby queries. \system works for both cyclic and
acyclic queries, and therefore can be used as an estimator both for
traditional SQL workloads, which tend to be acyclic, and for graph
pattern matching or SparQL queries, which tend to be cyclic.  

{\em 2. We describe how to use most common values and histograms to extend \system to conjunctions and disjunctions of equality and range predicates (Sec.~\ref{sec:histograms}).}
To support predicates, \system uses data structures that are very similar to those used by SQL engines, and therefore \system could be easily incorporated in those systems.

{\em 3. We introduce two optimization techniques for computing the upper bound, which
run in polynomial time in the size of the query and the number of
available statistics (Sec.~\ref{sec:algorithm}).} One works for acyclic queries only, while the other works for arbitrary conjunctive queries albeit on one-column degree sequences.
These techniques are essential for the practicality of \system, as  cardinality estimation is often invoked thousands of times during query optimization, and it must run in times
measured in milliseconds.

{\em 4. We conduct an extensive experimental evaluation of \system on real and
  synthetic workloads (Sec.~\ref{sec:experiments}).}
  \system can be orders of magnitude more accurate than traditional
  estimators used in mainstream open-source and commercial database
  systems. Yet it has low estimation time and space
  requirements to remain practical. 
  When injected the estimates of \system, \psql derives
  query plans at least as good as those derived using the true
  cardinalities.

\paragraph{Related Work}
Our paper builds on a long line of theoretical
results that proved upper bounds on the size of the query output.  
The first such result appeared in a landmark paper by Atserias, Grohe, and
Marx~\cite{DBLP:journals/siamcomp/AtseriasGM13}, which proved an upper
bound in terms of the cardinalities of the input
relations, known today as the \emph{AGM Bound}.  The AGM bound
is not practical for real SQL workloads, which consist almost
exclusively of acyclic queries, where the AGM bound is too large.
For example, the AGM bound of a 2-way join is
$|R \Join S| \leq |R| \cdot |S|$.  The AGM bound was extended to
account for functional dependencies~\cite{DBLP:journals/jacm/GottlobLVV12,DBLP:conf/pods/KhamisNS16},
and further extended to use the maximum degrees of
attributes~\cite{DBLP:conf/pods/Khamis0S17}: we refer to the
latter as the \emph{\maxdegree bound}.  This line of work relies on information theory.  A simplified version of the \maxdegree bound was incorporated into two pessimistic cardinality estimators~\cite{DBLP:conf/sigmod/CaiBS19,DBLP:conf/cidr/HertzschuchHHL21}.
However, cardinalities and maximum degrees alone are still too limited to infer useful upper bounds for acyclic queries.  For example, the \maxdegree bound of a 2-way join is
$|R \Join_{X=Y} S| \leq \min(a\cdot|S|, |R|\cdot b)$, where $a$ is the maximum degree of $R.X$ and $b$ is the maximum degree of $S.Y$.  Real data is often skewed and the maximum degree is large (the maximum degrees are $2951$ and $450$ in Fig.~\ref{fig:ds}), and this led to large overestimates for more complex queries.

The first system to use degree sequences for pessimistic
cardinality estimation was
\safebound~\cite{SafeBound:SIGMOD23,DBLP:conf/icdt/DeedsSBC23}.
Since the degree sequences are often too large, 
\safebound uses a lossy compression of them. It relies solely on combinatorics, it is limited to Berge-acyclic queries (see Sec.~\ref{sec:background}), and it does not support \groupby. \system can be seen as a significant extension of \safebound.  By using information theory instead of combinatorics, \system computes the bound using $\ell_p$-norms, without requiring the degree
sequences, and also works for cyclic queries and queries
with \groupby.
