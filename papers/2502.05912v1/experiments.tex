\section{Experimental Evaluation}
\label{sec:experiments}



In this section, we experimentally answer the following questions: {\em How accurate are \system's cardinality estimates? Are \system's estimation time and space requirements sufficiently low for it to be practical? Can \system's  estimates  help avoid inefficient query plans, in case faster query plans exist?}
Our findings are as follows. 

1. \system can be orders of magnitude more accurate than traditional estimators used in mainstream open-source and commercial database systems. Yet \system has low estimation time (within a couple of ms) and space requirements (a few MBs), which are comparable with those of traditional estimators.

2. Learned estimators can be more accurate than \system, according to the errors reported in a prior extensive benchmarking effort~\cite{CE:VLDB21}\footnote{These estimation models are copyrighted and not available. Training and tuning the models requires knowledge that is not available (confirmed by authors of \cite{CE:VLDB21}).}. This is by design as their models are trained to overfit the specific dataset and possibly the query pattern. Downsides are reported in the literature, including: poor generalization to new datasets and query patterns; non-trivially large training times (including hyper-parameter tuning) and extra space, even an order of magnitude larger than the dataset itself~\cite{CE:VLDB21}.

3. By configuring \psql to use the estimates of \system for the 20 longest-running queries in our benchmarks, we obtained faster query plans than those originally picked by \psql.

\subsection{Experimental Setup}


\paragraph{Competitors.} 
We use the {\em traditional estimators} from open-source systems \psql 13.14 and \duckdb 0.10.1 and a commercial system \dbx. We use two {\em pessimistic cardinality estimators}: \safebound \cite{SafeBound:SIGMOD23} and our approach \system. \metarev{We use two classes of {\em learned cardinality estimators}: (1) The {\em PGM-based cardinality estimators} \bayescard~\cite{bayescard}, \deepdb~\cite{deepdb}, and \factorjoin~\cite{FactorJoin:SIGMOD23}; and (2) the {\em ML-based estimators} \flatcard~\cite{flat} and \neurocard~\cite{neurocard}. For the latter, we refer to their performance as reported in~\cite{CE:VLDB21}. We checked with 
the authors of \safebound, \bayescard, and \factorjoin that we used the best configurations for their systems and for \deepdb.}


\paragraph{Benchmarks.} Table~\ref{tab:queries} shows the characteristics of the queries used in the experiments. They are based on the benchmarks: JOB~\cite{DBLP:journals/vldb/LeisRGMBKN18} over the IMDB dataset (3.7GB); STATS\footnote{\url{https://relational-data.org/dataset/STATS}} over the Stats Stack Exchange network dataset (38MB); and SM (subgraph matching) over the DBLP dataset (26.8MB edge relation and 3.5MB vertex relation)~\cite{SubgraphMatching:SIGMOD20}. The SM queries are cyclic, all other queries are acyclic.
For IMDB, we use JOBlight and JOBrange queries from previous work~\cite{DBLP:conf/cidr/KipfKRLBK19, neurocard}, which have both equality and range predicates. We further created JOBjoin queries without predicates. We also created JOBlight-gby, JOBrange-gby and STATS-gby queries, which are JOBlight, JOBrange and STATS queries with group-by clauses consisting of at most one attribute per relation\footnote{\metarev{This is not a restriction, \system can support arbitrary group-by clauses. This is our methodology for generating query workloads with GROUP-BY.}}: We classify them into three groups of roughly equal size: small domain (domain sizes of the group-by attributes are $\leq 150$); large domain (domain sizes $>$ 150); and a mixture of both. The SM queries use 11-28 copies of the edge relation and 2 vertex relation copies per edge relation copy, with one equality predicate per vertex relation copy.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}[t]
    \centering
\hspace*{-1em}\begin{tabular}{|l|r|r|r|l|}\hline
Benchmark & \#queries & \#rels & \#preds & query type \\\hline
JOBjoin & 31 & 5-14 & 0 & snowflake \& full\\
JOBlight& 70 & 2-5  & 1-4 & star \& full \\
JOBrange & 1000 & 2-5 & 1-4 & star \& full \\
JOBlight-gby & 170 & 2-5 & 1-4 & star \& group-by \\
JOBrange-gby & 877 & 2-5 & 1-4 & star \& group-by \\\hline
STATS  & 146 & 2-8 & 2-16 & acyclic \& full \\
STATS-gby & 370 & 2-8 & 2-16 & acyclic \& group-by \\\hline
SM     & 400 & 33-84 & 22-56 & cyclic \& full \\\hline
\end{tabular}
    \caption{Benchmarks used in the experiments.}
    \label{tab:queries}
    \vspace*{-1em}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\nop{
We considered three classes of full acyclic JOB queries: JOBjoin (31 snowflake queries over 5-14 relations, no predicates); JOBlight (70 star queries over 3-5 relations, with 1-4 equality predicates); and JOBlightranges (1000 star queries over 3-5 relations, with 1-4 range predicates). We considered all 146 full acyclic STATS queries over 2-8 relations with 2-16 predicates. 
We also created new queries with group-by clauses consisting of at most one attribute per relation. We classify them into three groups: small domain (the group-by attributes have domain sizes $\leq 150$); large domains (domain size $>$ 150); and a mixture of both. Overall, we created $67 + 70 + 70 = 207$ and $124 + 121 + 125 = 370$ group-by queries in  JOBLight and respectively STATS.
SM has 400 full cyclic queries joining 11-28 copies of the edge relation and two vertex relations per copy of the edge relation, with one equality selection predicate per vertex relation.
}

\paragraph{Metrics.} We report the estimation error, which is the estimated cardinality divided by the true cardinality of the query output. The estimation error is greater (less) than one in case of over (under)-estimation. We report the (wall-clock) estimation time of the estimators. We also report the end-to-end query execution time of the 20 longest-running queries using \psql when injected the estimates of some of the estimators. We also report the extra space needed for the data statistics and ML models used for estimation.

\paragraph{System configuration.} We used an Intel Xeon Silver 4214 (48 cores) with 193GB memory, running Debian GNU/Linux 10 (buster). For \psql, we used the recommended configuration~\cite{DBLP:journals/vldb/LeisRGMBKN18}: 4GB shared memory, 2GB work memory, 32GB implicit OS cache, and 6 max parallel workers. We enabled indices on primary/foreign keys. We used the default configuration for data statistics for each estimator. \system uses HiGHS 1.7.2~\cite{HiGHS:2018} for solving LPs.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[t]
    \centering
    \includegraphics[width=.75\textwidth]{experiments/estimates_combined.pdf}
    \caption{Estimation errors for JOBJoin, JOBLight, JOBRange, and STATS. For the starred ML-based estimators, we use the errors for JOBLight and STATS reported in the literature~\cite{CE:VLDB21}.
    }
    \label{fig:estimates-combined}
    %\vspace*{-1em}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[h!]
\centering
\includegraphics[width=.95\textwidth]{experiments/estimates-stats.pdf}
\caption{Estimation errors for STATS. For the starred ML-based estimators, we use errors reported in the literature~\cite{CE:VLDB21}.}
\label{fig:estimates-STATS}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[t]
    \centering
    \begin{minipage}{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{experiments/cyclic-queries-estimation-error-11norms.pdf}
        \vspace*{-1cm}
        \caption{Estimation errors for the SM cyclic queries.}
        \label{fig:cyclic-estimation-error}
    \end{minipage}
    \hfill
    \begin{minipage}{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{experiments/groupby_estimates_combined.pdf}
        \vspace*{0.2cm}
        \caption{Estimation errors for group-by queries.}
        \label{fig:groupby-estimation-error}
    \end{minipage}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[t]
    \centering
    \begin{minipage}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{experiments/pkfk_optimization.pdf}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{experiments/prefix_optimization.pdf}
    \end{minipage}
    \caption{Improvements on estimation errors when using the two optimizations discussed in Sec.~\ref{sec:histograms}. Left: PK-FK predicate propagation optimization. Right: Prefix degree sequences optimization.}
    \label{fig:improvements-optimizations}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Estimation Errors}


\paragraph{Acyclic queries.} {\em \system has a smaller error range than the traditional estimators and \safebound for acyclic queries.} Fig.~\ref{fig:estimates-combined} plots the estimation errors for the acyclic queries. All systems except \system and \safebound both underestimate and overestimate. The traditional estimators broadly use as estimation the multiplication of the relation sizes and of the selectivities of the query predicates. The selectivity of a join predicate is the inverse of the minimum of the domain sizes of the two join attributes (so average degree, as opposed to maximum degree, is used). For equality and range predicates, Most Common Values (for \psql) and histograms (for \psql and \dbx) are used. \duckdb has a fixed selectivity of 0.2 for a range predicate. For ML-based estimators, we use the estimates reported in~\cite{CE:VLDB21}, as the models are not available. These models were designed to overfit JOBlight and subsequently fine-tuned to STATS, albeit with a poorer accuracy. 

We also report on the estimation errors of the PGM-based estimators. \bayescard and \deepdb do very well on JOBlight; this is the only workload on which their implementation works. \factorjoin builds high-dimensional probability distributions over the attributes of each relation to capture their correlation. This building task uses random sampling for JOB and the more accurate \bayescard for STATS.
\factorjoin faces a trade-off between good accuracy and fast estimation time. To keep the latter practical, it approximates the learned high-dimensional distributions by the product of one-dimensional distributions for JOB\footnote{The implementation of \factorjoin  does not support 2D distributions for JOB.} and of two-dimensional distributions for STATS. These choices influence the estimation error: It is far more accurate for STATS than for JOBjoin due to the choice of \bayescard over sampling and 2-dimensional over 1-dimensional factorization. The errors for JOBrange are larger possibly due to the larger number (up to 3) of predicates per relation.

Fig.~\ref{fig:estimates-STATS} shows that the accuracy of the estimators decreases with the number of relations per query (shown for STATS, a similar trend also holds for JOBlight and JOBrange): The traditional estimators underestimate more, whereas the pessimistic estimators overestimate more. \neurocard starts with a large overestimation for a join of two relations and decreases its estimation as we increase the number of relations; the other ML-based estimators follow this trend but at a smaller scale.



\paragraph{Cyclic queries.} {\em \system is the most accurate estimator for the SM cyclic queries.} Fig.~\ref{fig:cyclic-estimation-error} shows the errors of \system and the traditional estimators, grouped by the number of edge relations in the query. The learned estimators do not work for cyclic queries.\footnote{The implementation of \factorjoin does not support SM queries.}

As we increase linearly the number $n$ of edge relations from 11 to 28, the number of join conditions between the edge relations increases quadratically in $n$. This poses difficulties to the traditional estimators, which exhibit two distinct behaviors. 

{\em The estimate of \psql and \dbx is 1 for all SM queries and their error is the inverse of the query output size. This is an underestimation by 7-8 orders of magnitude.} The estimation is obtained by multiplying: the size of the edge relation $n$ times; the selectivity of each of the $n^2$ join conditions; the size of the vertex relation $2n$ times; the selectivity of the $2n$ join conditions between the edge and vertex relations; and the selectivity of the $2n$ equality predicates in the $2n$ vertex relations. The product of the relation sizes is much smaller than the inverse of the product of these selectivities, so the estimation is a number below 1, which is then rounded to 1.

{\em The estimate of \duckdb increases exponentially in the number of edge relations, eventually leading to an overestimation by over 12 orders of magnitude.} 
Its estimation ignores most of the join conditions, but accounts for each of the $n$ copies of the edge relation, as explained next. To estimate, \duckdb first constructs a graph, where each node is a relation in the query and there are two edges between any two nodes representing relations that are joined in the query: one edge per attribute participating in the join. Each edge is weighted by the inverse of the domain size of the attribute. \duckdb then takes a minimum-weight spanning tree of this graph. A significant factor in the  estimation is then the multiplication of the ($n$ edge and $2n$ vertex) relation sizes at the nodes and of the weights of the edges in the spanning tree ($3n-1$ domain sizes of one or the other column in the edge or vertex relations). For each of the relations in the query, the estimate has thus a factor proportional to the fraction of the relation size over an attribute's domain size.





\paragraph{Group-by queries.} {\em The range of the estimation errors for group-by queries is the smallest for \system.}
Except for \system, \psql, and \dbx, the systems ignore the group-by clause and estimate the cardinality for the full query. 
Fig.~\ref{fig:groupby-estimation-error} shows the errors for the small and large domain classes of JOBlight, JOBrange, and STATS group-by queries (the mixed domain class behaves very similarly to the large domain class). For small domain sizes (first half of figure), \system and \psql use the product of the domain sizes, which is close to the true cardinalities. For large domain sizes (second half), the true cardinalities remain smaller than for the full queries, yet \psql estimates are for the full queries. This explains why the error boxes are shifted up relative to those in Fig.~\ref{fig:estimates-combined}. \safebound and \duckdb estimate the full query and have large errors.


\paragraph{Optimization Improvements.}
Fig.~\ref{fig:improvements-optimizations} shows the improvements to the estimation accuracy brought by each of the two optimizations discussed in Sec.~\ref{sec:histograms}, when taken in isolation.

The left figure shows that,  when propagating predicates from the primary-key relation to the foreign-key relations, the estimation error can improve by over an order of magnitude in the worst case (corresponding to the upper dots in the plot) and by roughly 5x in the median case (corresponding to the red line in the boxplots). 

The right figure shows that,  when using prefix degree sequences for the degree sequences of relations without predicates, the estimation error can improve by up to 50\% for JOBlight queries, up to 65\% for JOBrange queries and up to 10\% for STATS queries. The improvement is measured as the division of (i) the difference between the estimation error without this optimization and the estimation error with this optimization and (2) the the estimation error without this optimization.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}[t]
\centering
\small
\setlength{\tabcolsep}{2.5pt}
\renewcommand{\arraystretch}{1.1}
\begin{tabular}{|l|r|r|r|r|r|r|}
\hline
\multirow{2}{*}{\textbf{Estimator}} & \multicolumn{2}{c|}{\textbf{JOBjoin}} & \multicolumn{2}{c|}{\textbf{JOBlight}} & \multicolumn{2}{c|}{\textbf{STATS}} \\
\cline{2-7}
& Time & Space & Time & Space & Time & Space \\
\hline
\system & 0.48 / 10.5 & 0.04 & 0.36 / 1.5 & 1.25 & 0.49 / 1.6 & 3.62 \\
\safebound & 0.85 / 147.9 & 0.07 & 1.28 / 13.0 & 1.75 & 1.89 / 5.6 & 5.94 \\
\hline
\dbx$^{\!+}$ & - / 371.7 & - & - / 35.3 & - & - / 13.3 & - \\
\duckdb$^{\!+}$ & - / 99.4 & - & - / 535.2 & - & - / 30.3 & - \\
\psql$^{\!+}$ & - / 19.8 & $<$0.001 & - / 3.4 & 0.001 & - / 18.7 & 0.011 \\
\hline
\factorjoin & 0.66 / 202 & 31.6 & 16.7 / 166.5 & 22.8 & 35.3 / 626 & 8.2 \\
\metarev{\bayescard} & - / - & - & \metarev{3.0 / 21.7} & \metarev{1.6} & \nop{5.8 / -} - / - & \nop{5.9} - \\
\metarev{\deepdb} & - / - & - & \metarev{4.3 / 28.6} & \metarev{34.0} & \nop{87.0 / -} - / - & \nop{162.0} - \\
\hline
\neurocard$^{\!*}$ & - / - & - & 18.0 / - & 6.9 & 23.0 / - & 337.0 \\
\flatcard$^{\!*}$ & - / - & - & 8.6 / - & 3.4 & 175.0 / - & 310.0 \\
\hline
\end{tabular}
\caption{
Time (ms): average wall-clock times to compute estimates for (i) a sub-query of a query, averaged over all sub-queries of queries / (ii) a query and all its connected sub-queries, averaged over all queries.
Space (MB): extra space for data statistics and models. 
The times ($^+$) are for the entire query optimization task.
The numbers ($^*$) are from prior work~\cite{CE:VLDB21} and only available for JOBlight and STATS. (-) means unavailable data or unsupported workload.
}
\label{tab:time-space}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




\nop{
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}[t]
\centering
\begin{tabular}{|l||r|r|r|}
\hline
\textbf{Estimator} & \textbf{JOBjoin} & \textbf{JOBlight} & \textbf{STATS} \\\hline
\system & & 0.58 /  1.2  &   \\
\safebound & & 1.28 / 13.0 & 1.89 / 5.6 \\
\hline
\dbx$^+$ & - / 371.7 & - / 35.3 & - / 13.3 \\
\duckdb$^+$ & - / 99.4 &   - / 535.2 & - / 30.3  \\
\psql$^+$  & - / 19.8 & - / 3.4 & - / 18.7 \\
\hline
\factorjoin & 0.37 / 259.9 & 16.7 / 166.5 & 35.3 / 626.0 \\
\neurocard$^*$ & - / - & 18.0 / - & 23.0 / -\\
\bayescard$^*$ & - / -& 5.4 / - & 5.8 / - \\
\deepdb$^*$ & - / - & 44.0 / - & 87.0 / - \\
\flatcard$^*$ & - / -& 8.6 / - & 175.0 / - \\
\hline
\end{tabular}
\caption{Wall-clock times (ms) to compute (i) the estimate for a subquery of a query / (ii) the estimate for a query and its subqueries, both averaged over all queries in a benchmark. The times($^+$) are for the entire query optimization task.
The times ($^*$) are from prior work~\cite{CE:VLDB21} and only available for JOBlight and STATS.}
\label{tab:estimation-times}
\vspace*{-2em}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}[t]
\centering
\begin{tabular}{|l||r|r|r|}
\hline
\textbf{Estimator} & \textbf{JOBjoin} & \textbf{JOBlight} & \textbf{STATS}  \\\hline
\system & $0.04$ & $1.25$  & $3.62$ \\
\safebound & $0.07$ & $1.75$  & $5.94$ \\
\hline
\dbx        & $-$ & $-$  & $-$ \\
\duckdb     & $-$ & $-$  & $-$ \\
\psql &  $< 0.001$ & $0.001$  & $0.011$ \\
\hline
\factorjoin & & & 8.2 \\
\bayescard* & $-$ & 1.6 & 5.9 \\
\deepdb* & $-$ & 34.0 & 162.0 \\
\hline
\neurocard* & $-$ & 6.9 & 337.0 \\
\flatcard* & $-$ & 3.4 & 310.0 \\
\hline
\end{tabular}
\caption{Extra space requirements (MB) for data statistics and models. The numbers ($^*$) are from prior work~\cite{CE:VLDB21} and only available for JOBlight and STATS.}
\label{tab:space-requirements}
\vspace*{-2em}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\nop{
\begin{figure}[t]
    \centering
    \includegraphics[width=.45\textwidth]{experiments/relative_runtime.pdf}
    \caption{Relative runtime}
    %\label{fig:relative-runtime}
\end{figure}
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Estimation Times}

{\em \system has a very low estimation time (a few ms) thanks to its LP optimizations. At the other extreme, ML-based estimators can be 1-2 orders of magnitude slower even when taking their average estimation time per subquery instead of the estimation time for all subqueries.}

To produce a plan for a query with $n$ relations, a query optimizer  uses the cardinality estimates for some of the $k$-relation sub-queries for $2\leq k \leq n$. Following prior work~\cite{CE:VLDB21,FactorJoin:SIGMOD23}, we use the sub-queries  produced by \psql's planner for a given query. The range (min-max) of the number of sub-queries is: 8-2018 for JOBjoin; 1-26 for JOBlight; and 1-75 for STATS. The times for JOBrange are not reported, but we expect them to be close to those for JOBlight. SM is not supported by the ML-based estimators and \safebound.


We report two estimation times per benchmark: (i) the time to compute the estimate for a single sub-query, averaged over all sub-queries of all queries, and (ii) the time to compute the estimates for a query and all its sub-queries, averaged over all queries.

\nop{The type (i) time represents the expected time to compute the estimate for an individual (sub-)query, while the type (ii) time represents the expected time needed for the cardinality estimation in the query optimization task for a query.}

Table~\ref{tab:time-space} reports the estimation times of the estimators. It was not possible to get the estimation times for the traditional estimators, so we report instead their times for the entire query optimization task to give a context for the other reported times; thy should have the lowest estimation times. The type (i) times for the starred ML-based estimators are from~\cite{CE:VLDB21}. We expect their type (ii) times to be at least an order of magnitude larger than their type (i) times, given the average number of sub-queries per query.

\factorjoin computes the estimates for all sub-queries of a query in a bottom-up traversal of a left-deep query plan. This computation does not parallelize well, however. Its type (i) time is therefore much lower than the type (ii) time. 

\system can effectively parallelize the LP solving for the sub-queries of a query. Even though one can extend an already constructed LP to accommodate new relations and statistics, we found that it is faster to avoid estimation dependencies between the related sub-queries and estimate for them independently in parallel. \nop{\system naturally exploits the observation that smaller LPs are solved faster than larger LPs, so it distributes the workload accordingly to the available threads.}





\nop{
Table~\ref{tab:time-space} reports the times needed by the \system, \safebound, and \factorjoin estimators to compute the estimates for the entire space of investigated sub-queries of a query, averaged over all queries in a benchmark. 
It was not possible to get such estimation times for the traditional estimators, so we report instead their average times for the entire query optimization task to give a context for the other times in the table; their estimation times alone are expected to be the lowest among all competitors. 
The times for the ML-based estimators, except \factorjoin, are the averages over all sub-queries of each query~\cite{CE:VLDB21}. We expect their overall estimation times to be at least an order of magnitude larger, given the average number of sub-queries per query.
The times for \factorjoin are computed by us. \factorjoin can naturally share the estimate computation for all subqueries, so its overall time is very close to the time to estimate for the full query.
}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Space Requirements}
\label{sec:experiments-space-requirements}



{\em The extra space used by \system for statistics is 1.6x less than of \safebound and 1.2-93x less than of the ML-based estimators.}

Table~\ref{tab:time-space} shows the amount of extra space needed to store the data statistics or machine learning models used by the  estimators. 

The traditional estimators use modest extra space.
\psql uses 100 MCVs per predicate column: Increasing the number of MCVs leads to very large estimation time, as it computes the join output size at estimation time for the MCVs. It also uses 100 buckets per histogram and sampling-based estimates of domain sizes for columns. \duckdb only uses (very accurate and computed using hyperloglog~\cite{hyperloglog,DBLP:conf/edbt/HeuleNH13}) domain size estimates, no MCVs, and no histograms. \dbx uses histograms with 200 buckets and no MCVs. 

\nop{Running \texttt{analyze} improves significantly the domain size estimates in \psql.}

\safebound uses a compressed representation of the degree sequences and 2056 MCVs on the predicate columns. 

\system uses up to\footnote{Only 2/8 predicate attributes have domain sizes (134k, 235k) greater than 2k in JOBlight; for JOBrange, there are 3/13 such domains (15k, 23k, 134k). For STATS, the domain sizes are at most 100. For SM, the predicates are on the label attribute from the vertex relation and with domain size 15. Each edge relation joins with two copies of the vertex relation, so we use two predicates to indirectly filter the edge relation. We use $15\times 15$ MCVs to capture all possible combinations of the two predicates.} 5000 MCVs on the predicate columns in JOB, albeit for less space than \safebound. Both \safebound and \system use hierarchical histograms on data columns with 128 buckets. \system stores $\ell_p$-norms within each histogram bucket, while \safebound stores $\ell_1$-norms (counts) only. Although not reported in the table, \system needs 1.12MB for SM and 8MB for JOBrange. JOBrange has queries with more predicates, which need support, and more columns with large domains.

The models used by \neurocard and \flatcard are a feature-rich representation of the datasets. They take more space than the statistics used by the other estimators. For STATS, these models take 10x more space than the dataset itself~\cite{CE:VLDB21}.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}[t]
    \centering
\small
\setlength{\tabcolsep}{2.5pt}
\renewcommand{\arraystretch}{1.1}
\metarev{
\begin{tabular}{|l|r|r|r|r|r|}
\hline
  {\bf Estimator}   & {\bf JOBjoin} & {\bf JOBlight} & {\bf JOBrange} & {\bf STATS} & {\bf SM}\\\hline
   \system-${\ell_1}$     & 4.07 & 12.35 & 42.42 & 24.67 & 0.65 \\
   \system     & 14.95 & 19.06 & 54.79 & 27.91 & 1.59 \\
   \safebound  & 88.56 & 162.09  & 209.23  & 32.04  &  - \\\hline
   \factorjoin & 10068.8 &  4990.9 &  5042.7 & 360.92 & - \\
   \bayescard  & -    & 493.36 & -    & - & - \\
   \deepdb     & -    & 1191.17   & - & - & - \\\hline
   \neurocard$^*$  & -    & 3600 & - & - & - \\ 
   \flatcard$^*$  & -     & 3060 & - & - & - \\\hline
\end{tabular}
}
    \caption{Time (sec) to compute the required statistics for the pessimistic and PGM-based estimators. 
    \system-${\ell_1}$ is \system with $\ell_1$-norms only.
    (-) means the system cannot estimate for the respective workload. (*) means the times  are from prior work~\cite{CE:VLDB21}, as the code is not available.}
    \label{tab:times-compute-statistics}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\metarev{
\subsection{Time to Compute the Statistics}
\label{sec:experiments-stats-compute-time}

Table~\ref{tab:times-compute-statistics} gives the times to compute the statistics or models required by the estimators. 
Overall, the compute time for \system is at least one order of magnitude smaller\footnote{All estimators in Table~\ref{tab:times-compute-statistics} except \system compute their statistics using Python.} than for the PGM-based estimators. 
The computation of the statistics used by \system is fully expressed in SQL and executed using \duckdb. Such statistics are: the MCVs, the histograms, the $\ell_p$-norms ($p\in\{1,\ldots,10,\infty\}$) for each MCV, histogram bucket, and full relation, and the two optimizations (FKPK and prefix) from Sec.~\ref{sec:histograms}. About 80\% of \system's time is spent on the two optimizations. To better understand the effect of the number of norms, we also report the times for \system when restricted to the $\ell_1$-norm only. This shows that increasing from 1 to 11 norms only increases the compute time 1.5--3.7 times. 
\deepdb and \bayescard take 86\% and respectively 67\% of their compute time for training, the remaining time is for constructing an auxiliary data structure to support efficient sampling.  \factorjoin spends most of its time (98\%) to construct statistics to speed up the estimation, while relatively very short time (2\%) is spent on training the model. The times for \neurocard and \flatcard are as 
reported in prior work~\cite{CE:VLDB21} for training without hyper-parameter tuning.
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{From Cardinality Estimates to Query Plans}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[t]
    \centering
    \includegraphics[width=.95\textwidth]{experiments/most_expensive_queries.pdf}
    \caption{\psql (wall-clock) evaluation time for the 20 most expensive queries in JOBlight, JOBrange, and STATS,  when injected the estimates of \system, \safebound, \dbx, and \psql or the trues cardinalities for all subqueries of the query.  The runtimes for STATS 104, 105, 106 are very small when using the estimates of all systems but \dbx and therefore not visible.}
    \label{fig:most-expensive-queries}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[t]
    \centering
    \begin{minipage}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{experiments/overall_runtime.pdf}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{experiments/relative_runtime.pdf}
    \end{minipage}
    \caption{Left: Overall evaluation time of all queries in a benchmark for \psql when using estimates for all subqueries from \system, \safebound, \dbx, \psql and true cardinalities. Right: Relative evaluation times compared to the baseline evaluation time obtained when using true cardinalities.}
    \label{fig:runtime}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



{\em When injected the estimates of \system, \psql derives query plans at least as good as those derived using the true cardinalities.}

This result was expected and aligns with observations from prior work~\cite{DBLP:conf/sigmod/CaiBS19,SafeBound:SIGMOD23}.
We verified this for the 20 queries in JOBlight, JOBranges, and STATS (Fig.~\ref{fig:most-expensive-queries}), 
which took longest to execute using \psql when the query plan was generated based on the estimations of \system, \safebound, \dbx, \psql, or \factorjoin.
We used \psql for query execution as it easily allows to inject external estimates into its query optimizer\footnote{\url{https://github.com/ossc-db/pg_hint_plan}}. Remarkably, the estimates of \system can lead to better \psql query plans than using true cardinalities, e.g., for the 9 most expensive JOBrange queries in the figure. Prior work~\cite{DBLP:journals/vldb/LeisRGMBKN18} also reported this surprising behavior that using true cardinalities, \psql does not necessarily pick better query plans.
\safebound leads to better plans than \system for the top-2 most expensive queries. The estimates of \dbx and \psql lead in many cases to much slower query plans: for STATS104 (STATS122), \dbx (\psql) estimates lead to a plan that is more than 3000x (4x) slower than for the other estimators. For three STATS queries (104, 105, 106), the \dbx estimates yield a very slow plan; the runtimes of the plans using the estimates of the other systems are not visible in the plot.



Fig.~\ref{fig:runtime} (left) shows the aggregated \psql evaluation time of all queries in JOBlight, JOBrange, and STATS when using estimates for all sub-queries from \system, \safebound, \dbx, \psql, and true cardinalities (left).
Fig.~\ref{fig:runtime} (right)  shows the relative evaluation times compared to the baseline evaluation time obtained when using true cardinalities. 
We have two observations. First, overestimation can be beneficial for performance of expensive queries, which has been discussed in Section~\ref{sec:experiments}. Second, overestimation can be detrimental for performance of less expensive queries in some cases.

The first observation is reflected in the overall evaluation times, which are dominated by the most expensive queries in the benchmark (some of which are listed in Fig.~\ref{fig:most-expensive-queries}). 
Traditional approaches lead to higher evaluation times for the expensive queries, and therefore to higher overall evaluation times, while the pessimistic approaches lead to lower evaluation times for those expensive queries. Overall, the  evaluation times for the pessimistic approaches are about the same (JOBlight and STATS) or lower (JOBrange) than the baseline evaluation times.
The second observation is reflected in the relative evaluation times for the JOB benchmarks. 
The boxplots for the traditional approaches are lower than those for the pessimistic approaches, indicating that the traditional approaches perform better for the less expensive queries in the benchmarks.

\factorjoin has both high overall evaluation time and high relative evaluation time.
It estimates very accurately for the queries in STATS, thus has similar evaluation time to the baseline evaluation time. For the queries in JOBlight and JOBrange, it mostly overestimates, which leads to lower evaluation times for the expensive queries. However, the overestimations are significant, which makes it perform worse than the pessimistic approaches for the less expensive queries, as shown in the right plot of Fig.~\ref{fig:runtime}. This leads to the high overall evaluation time of \factorjoin.


\subsection{Performance Considerations for \system}
\label{ex:performance-considerations}

\paragraph{How Many $\ell_p$-Norms to keep?} Using the norms for $p\in[1,10]\cup\{\infty\}$ gives the best trade-off between the space requirements, the estimation error, and the estimation time for the JOBlight queries (Fig.~\ref{fig:LpBound-amount-norms}). This was verified to hold also for the other benchmarks. Further norms can still lower the estimation error, but only margina\-lly, and at the expense of more space and estimation time.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[t]
    \centering
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{experiments/joblight_relative_error_maxp.pdf}
        \caption{The amount of useful norms follows the law of diminishing returns: Plotting the division of estimation errors for the norms $\{1,\ldots,k,\infty\}$ and $\{1,\ldots,30,\infty\}$, averaged over the 70 JOBlight queries.}
        \label{fig:LpBound-amount-norms}
        \vspace*{-1em}
    \end{minipage}
    \hfill
    \begin{minipage}{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{experiments/mcv_sensitivity.pdf}
    \caption{Effect of the number of MCVs on estimation error for \system on JOBlight.}
    \label{fig:lpbound-MCVs}
    \vspace*{-1em}
    \end{minipage}
    %\vspace*{-1em}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\paragraph{How many Most Common Values (MCVs)?} Using sufficiently many MCVs to support the estimation for selection predicates can effectively reduce the overall estimation error. For each of the top-$k$ MCVs of a predicate attribute, \system stores one set of $\ell_p$-norms. It also stores one further set of norms for all remaining attribute values (Sec.~\ref{sec:histograms}). 
\revthree{
For small-domain attributes, e.g., \texttt{COMPANY\_TYPE}, it is often feasible to have MCVs for each domain value. This significantly improves the estimation accuracy.
For large-domain attributes, e.g., \texttt{COMPANY\_ID}, it not not practical to do so. To decide on the number $k$ of MCVs, one can plot the estimation error as a function of $k$ and pick $k$ so that the improvement in estimation error for larger $k$ is below a threshold, e.g., $1\%$.
Fig.~\ref{fig:lpbound-MCVs} shows that  $k\leq 2500$ can yield on average to clear accuracy improvements for JOBlight; this is similar for JOBrange (not shown).}

\paragraph{Optimizations for \system's LPs.} The optimizations introduced for solving \system's LPs are essential for the practicality of \system. Fig.~\ref{fig:LpBound-estimation-time} shows the estimation time of \system using \lpbase, \lpflow, and \lptdb. We used JOBjoin, as its queries are Berge-acyclic and have the largest number of relations and variables among the considered benchmarks, and therefore can stress test and compare the efficiency of the three approaches. As expected, \lpbase takes too long (over 1000 seconds) to build and solve an LP with $2^{15}$ entropic terms and times out beyond this. \lpflow uses a network flow of size at most $15^2$ and finishes in under 70 ms for each JOBjoin query. Most of its time is spent constructing the network. \lptdb consistently takes under 2ms for all queries.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[t]
    \centering
    \includegraphics[width=.65\textwidth]{experiments/inference_runtime_comparison.pdf}
    \caption{Estimation times for \system on full queries of JOBjoin using \lpbase and its optimizations \lpflow and \lptdb.}
    \label{fig:LpBound-estimation-time}
    \vspace*{-1em}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
