    \section*{Revision Plan}

We would like to thank the reviewers and the meta-reviewer for their valuable feedback. Below, we summarize the main points raised by the reviewers and provide a plan to address them in the revised version of the paper. If the reviewers and the shepherd have *any* further questions or comments regarding the plan, please reach out to us. We are happy to discuss them. 

\subsection*{Response to Meta-Reviewer}

\textbf{
M1. add a brief discussion on the cost required to produce lp-norm statistics and comment on their stability (how sensitive they are to updates) compared to typical statistics like histograms
}

{
\color{blue}
We will add a discussion along the following points:

1. The construction of $\ell_p$-norms can be expressed in SQL and executed and maintained by, e.g., PostgreSQL. \nop{If the reviewers deem it important, we could run experiments with this SQL encoding in PostgreSQL or using our own offline construction.}

2. Both norms and histograms are sensitive to data updates. Consider maintaining the logarithm of the $\ell_p$-norm $1/p\cdot\log (\sum_i d_i^p)$ for any $p$ (except $\infty$). A single-tuple update (insert or delete) affects the degree $d_i$ of a single term in the sum: We subtract the old value $d_i^p$ and add the new value $newd_i^p$ to the sum. This, and the subsequent logarithm and multiplication, can be executed in constant time. For a histogram, a single-tuple update affects one bucket -- also executable in constant time, unless the ranges of all buckets need updating as well, in which case the entire histogram is recomputed from scratch. We will add a discussion on this.

3. LpBound can provide a guaranteed cardinality upper bound without having to solve the LP every time an $\ell_p$-norm changes due to data updates! This is a great feature of LpBound. Recall that the upper bound computed by LpBound has the form:
$|Q| \leq \prod_i ||\texttt{deg}_i||_{p_i}^{w_i}$,
where each $||\texttt{deg}_i||_{p_i}$ is the used $\ell_p$-norm on the degree sequence $\texttt{deg}_i$, and $w_i$ is the weight associated with the LP inequality involving the aforementioned $\ell_p$-norm. The insight is that {\em the same} inequality weights coupled with any new norms on the same degree sequences still form a feasible solution of the LP; yet this solution may not be optimal (i.e., the lowest upper bound we can derive). Nevertheless, for fast data updates, LpBound only needs to recompute the above arithmetic formula giving the upper bound by plugging in the new norms. This can be done very fast, as there is no need to solve again the LP. Recomputation of the LP, which may yield an upper bound formula possibly using different norms, can be scheduled after a certain number of updates.

4. Approximate $\ell_p$-norms can also be maintained efficiently (i.e., with lower space and time costs than the exact norms) using sketches, such as universal sketches or the AMS sketches.
}

\vspace{0.5em}\noindent
\textbf{
M2. add a discussion of how the approach could be extended / integrated with existing techniques to apply to larger classes of queries including range join predicates 
}

{
\color{blue}
We agree with the reviewers that there are many existing venues of future research to make LpBound increasingly more practical. In short, {\em we do not see an intrinsic limitation of LpBound in accommodating nested queries, theta joins, and complex and interpreted  predicates}. Note that {\em in contrast to most work in academic papers, LpBound already supports a relatively significant large query language}. Pessimistic and ML-based estimators reported in the experimental section are limited to one of the following query classes: star queries, snowflake queries, or Berge acyclic queries, all with equality joins and various support for filter predicates.


We will add a discussion on possible extensions in the paper. In the following, we sketch a few cases.

[Nested queries] LpBound can be non-trivially extended to compute upper bounds on the $\ell_p$-norms of degree sequences in the query output. This goes beyond cardinality estimation -- which only considers the estimation of the cardinality, which is the $\ell_1$-norm --, and effectively makes LpBound compositional as it takes $\ell_p$-norms as input and produces $\ell_p$-norms as output. Then, the output of a query is treated like any input relation; in particular, LpBound can then estimate $\ell_p$-norms on sub-queries that are nested in other queries. This extension is not immediate and deserves proper treatment in a separate paper.

[Theta joins] At the moment, theta joins, such as inequality/range joins are treated just like Cartesian products, so effectively ignored. In future work, we may consider adapting existing data structures used for estimating sizes of theta joins to yield various norms on degree sequences as opposed to just the $\ell_1$-norm (i.e., cardinality).

[Complex predicates] At the moment, LpBound assumes by default that black-box complex predicates are not selective, so are effectively ignored. In future work, we may consider complex predicates for which there are specialized indices developed in the literature to (1) identify collections of tuples that certainly or possibly match the predicate (false positives do not affect the upper bound guarantees), and (2) provide fast access to various computations for such tuple collections; for LpBound, these computations need to yield norms of degree sequences for the tuples in a collection. Such indices are orthogonal to LpBound and can be trivially interfaced with LpBound.

[Negation] The negation of a predicate $\phi$ can be supported as the complement of $\phi$, i.e., LpBound can store norms on the degree sequence of all tuples but those satisfying $\phi$.

[Interpreted predicates] One example that can be readily incorporated is the LIKE predicate. This can be done like for SafeBound: here, again, the challenge is to use an index that can select collections of tuples that (certainly or possibly) satisfy the predicate. SafeBound uses a 3-gram index for this.

\nop{
Negation is can be supported by also MCVs. For each MCV, we maintain both the statistics for the positive and negative cases. 
For interpreted predicates, we can discuss how to support disjunction (which captures IN predicates) and LIKE predicates.  

R2O2 thought LpBound doesn't support range join predicates.  

R3O2 wants us to support more complex queries (even if the estimate is not a tight upper bound). 
Perhaps we can also discuss the nested queries. 
}
}

\vspace{0.5em}\noindent
\textbf{
M3. if possible (code is available), please compare against newer ML-based techniques 
}

{
\color{blue}
We briefly looked at the four proposed ML-based techniques mentioned by the reviewer. They are all query-driven, as opposed to the data-driven approaches reported in our submission. 

To support query-driven approaches, one needs set of  queries and their cardinalities for each dataset. The mentioned works only provide their own query set for JOBlight, this is only 1 out of the 8 workloads we use in our experiments! We checked briefly and found that two of the four mentioned works, CEDA and "Robust Query Driven Cardinality Estimation under Changing Workloads", have code available and we could potentially be able to run experiments with them.  Yet the insights of this effort would be minimal, given the lack of training query sets for the other datasets used in our experiments. We therefore propose not to investigate this possibility. Instead, we will invest significant efforts into successfully benchmarking two of two ML-based estimators already mentioned in our paper: DeepDB and BayesCard (we discuss this under R1O4).
However, if the reviewers consider this experimental comparison with the two newer works of outmost relevance, then we shall invest time in it.

Furthermore, the suggested work, Fauce, does not have code available. Another suggestion, "Cardinality Estimation of Approximate Substring Queries using Deep Learning", is not relevant to our work as it focuses on substring queries on one relation and there are no common benchmarks for both this work and LpBound.
}


\vspace{0.5em}\noindent
\textbf{
M4. comment on R3.O3 -- The experimental setup mentions that the group by workloads have "at most one attribute per relation". Is this a limitation of LpBound? What is the purpose of this restriction? 
}

{
\color{blue}
LpBound can support arbitrary group-by clauses, we will make this clearer. Given an arbitrary set $F$ of group-by variables, the objective of the linear program used by LpBound is to maximize the joint entropy of all group-by variables.

We decided to design and benchmark three classes of group-by queries that exhibit different behaviors. Considering more than one per-relation attribute with a large domain may quickly yield a cardinality very close to that of the full query (so when all variables in the group-by clause), so it would bring less insight into how effective existing estimators are for group-by clauses relative to full queries.
}


\subsection*{Response to Reviewer 1}  

\vspace{0.5em}\noindent
\textbf{
R1O1. The authors recognize that large overestimation is problematic and it is mentioned that the proposed results limit such over-estimation. However, this is the achilles heel of the proposed technique. In order to convince that such a technique can be generally adopted, much more discussion on the limitations of it is expected. In such a discussion, it is imperative to not only take a theoretical approach, but also a practical one, mentioning examples of data sizes, domain size, data value diversity, data dependencies and how these can play a role in the adaptation of the technique. 
}

{
\color{blue}
In the paper, we report on extensive experiments that go much beyond prior works cited in the paper. We used 8 workloads, totaling over 3000 queries over three datasets, see Table 1. They were proposed in the literature, except for the group-by workloads, which were introduced in the paper as prior work does not support such queries. We therefore consider that the paper gives practical evidence to support LpBound. In contrast, prior works use a subset of our workloads, as they only provide support for more restricted queries. For instance, in the original papers that introduced the respective estimators: SafeBound uses JOBlight, JOBranges, and STATS, as it only supports full Berge acyclic queries; DeepDB uses JOBlight, as it only supports full star queries; BayesCard uses JOBlight, as it only supports full acyclic queries; FactorJoin uses JOBlight and another benchmark based on the IMDB dataset.

The datasets in the used 8 workloads showcase a great deal of data diversity: functional dependencies, data columns with small and large number of distinct values, correlated data columns, and small and large relation data sizes. This is the reason behind their choice by the creators of the JOB, STATS, and SM benchmarks.

We propose to add a paragraph summarizing the effects that this data diversity has on the accuracy of LpBound. Primary keys help LpBound significantly, as they ensure that the $\ell_{\infty}$-norm, i.e., the max degree, of the degree sequence on the primary key column is 1. No further norms are needed to best capture the degree sequence of such columns. Small/large domains for data columns affect the LpBound accuracy for group-by queries, as shown for the three group-by workloads (Figure 7). Effectively, for group-by variables with small domains, LpBound  takes the multiplication of the sizes of such domains as an upper bound. For very large domains, the estimation becomes close to that of the full query. For in-between domain sizes, LpBound can benefit tremendously from norms on degree sequences of projections of relations onto the group-by variables. 


Like all estimators that use statistics local to each relation, LpBound can become more inaccurate when the relations to join are significantly mis-calibrated (i.e., there are dangling tuples in the relations that do not contribute to the query result). In Section 5, we briefly discuss two optimizations to mitigate this problem: predicate propagation and prefix degree sequences.
}


\vspace{0.5em}\noindent
\textbf{
R1O2. Moreover, related to the above, it is stated that the technique focuses on conjunctive queries (adding equality predicated). Since queries in a real workload are not usually equivalent to the aforementioned class of queries (for example real-life queries use bag semantics and not set semantics, whereas conjunctive queries are equal to SPJ with DISTINCT) It is essential to discuss the applicability of the technique for real workloads. Furthermore, in the same context, it seems that the technique does not support conjunctive queries with interpreted predicates or negation. A similar discussion for this classes of queries is necessary. 
}

{
\color{blue}
Thank you for bringing up the bag semantics issue, this is a good point. We will explain that LpBound works for bag semantics exactly as it does for set semantics, there is no limitation. To see this, note that degree sequences, and their norms, are not affected by the presence of duplicates in a relation: the number of times a value occurs in a data column gives its degree and a degree sequence is the vector of the degrees of all distinct values in a data column.

Regarding the extension to more predicates: It is indeed exciting to work out the details of how to support more predicates. We touched on this point in our above response to M2.
}


\vspace{0.5em}\noindent
\textbf{
R1O3. There is no discussion about the effort (including time) to produce the lp-norms, how volatile they are for real databases (in which data is dynamic) as well as the statistics needed for their creation (hierarchy of histograms). 
}

{
\color{blue}
Please see discussion in M1. 
}

\vspace{0.5em}\noindent
\textbf{
R1O4. The experimental results compare with one ML-based technique (the one for which code was found) and with some more based on the results in paper [15]. While this approach is appreciated (trying to compare with techniques not practically available), it is certain that the comparison not thorough. It would be best to compare with more recent ML-based techniques that have been proposed (especially after 2021), for which code, most probably, can be found. 
% These techniques are at least: 
% 1. CEDA: Learned Cardinality Estimation with Domain Adaptation https://dl.acm.org/doi/10.14778/3611540.3611589
% 2. Fauce: fast and accurate deep ensembles with uncertainty for cardinality estimation https://dl.acm.org/doi/10.14778/3476249.3476254
% 3. Robust Query Driven Cardinality Estimation under Changing Workloads https://dl.acm.org/doi/10.14778/3583140.3583164
% 4. Cardinality Estimation of Approximate Substring Queries using Deep Learning https://www.vldb.org/pvldb/vol15/p3145-jung.pdf
}

{
\color{blue}
We are grateful that the reviewer understands our pain with reproducing the experiments with competing ML-based estimators from prior work. 

After extensive in-person discussions with two of the  creators of BayesCard, we are hopeful that we will be able to run both BayesCard and DeepDB on those workloads that they support and to report their runtime performance and accuracy: BayesCard supports full acyclic queries and can be run on JOBjoin, JOBlight, JOBrange, and STATS. DeepDB supports full star queries and can be run on JOBlight and JOBrange. 

For possible experiments with further estimators, please see discussion in M3.  
}

\vspace{0.5em}\noindent
\textbf{
R1O5. The experimental results do not show a clear advantage of the proposed technique over the ML-based techniques. This is not discussed enough. 
}

{
\color{blue}
Indeed, the accuracy numbers reported in prior work for some of the ML-based estimators such as the ones based on deep networks (and not reproducible) look really good. Yet the estimation time and required memory are clearly in favor of LpBound over the ML-based estimators: They are from 1 to 3 orders of magnitude worse for the ML-based estimators than for LpBound. 

For the ML-based estimator FactorJoin, which we were able to benchmark ourselves, the accuracy looks significantly worse than for LpBound. More precisely, it looks about a factor 10x worse when overestimating while it can also underestimate by a factor 5x for JOBjoin, {\em the} workload on which it was tuned. For the JOBrange workload, on which we were also able to benchmark it, it performs worse by a factor of up to $10^{30}$x; on JOBjoin, it performs worse by a factor of at least $10^{10}$x.

Now that we will hopefully be able to benchmark BayesCard and DeepDB on our own machine, we will get a better understanding of their true accuracy on our further benchmarks, such as JOBjoin, JOBrange, and -- for BayesCard -- STATS.

A more general argument is that an intrinsic value of LpBound lies in providing guaranteed upper bounds, as opposed to traditional and ML-based estimators that provide no guarantees. This is emphasised throughout the paper.
}


\subsection*{Response to Reviewer 2} 

\vspace{0.5em}\noindent
\textbf{
R2O1: It is not clear how the selection predicates on columns in a dimension table propagated to the fact table (assuming a PK-FK relationship). Is there a mapping of the histograms? Columns in the dimension tables may have strong correlation with the join predicate (PK) column. Would columns with local predicates need histogram mapping to the PK column values? 
}

{
\color{blue}
We will improve the wording in Section 5. Say we have a table $R(K, A)$ with primary key $K$ and attribute $A$, on which we have a predicate $\phi(A)$. We also have a table $S(K,B)$ with foreign key $K$, pointing to the primary key $K$ in $R$, and some attribute $B$. By propagating the predicate $\phi(A)$ from $R$ to $S$, we mean that we join $R$ and $S$ on $K$ to obtain a new relation $S'(K,A,B)$. The relation $S'$ has the same cardinality as $S$, yet every $K$-value in $S'$ is now accompanied by the $A$-value as in $R$. We can now construct MCVs and histograms on the data column $A$ in $S'$. If we were to estimate the cardinality of a query, which involves both $R$ and $S$, and also -- as in our experiments -- many more other relations, LpBound can now use the data statistics computed for $R$ and $S'$.

A variant of this optimization is also used by SafeBound and DeepDB. 
}


\vspace{0.5em}\noindent
\textbf{
R2O2: How could one handle range join predicates? A quick glance at the repository and the workloads (JOB at least) seem to only have equality predicates although selection predicates include range predicates. 
}

{
\color{blue}
To clarify, the current implementation of LpBound supports equality and range predicates. It does not support range joins, like inequality joins. We discuss such extensions in M2.
}


\subsection*{Response to Reviewer 3} 

\vspace{0.5em}\noindent
\textbf{
R3O1. Sections 2-4 are quite dense and technical, and the concepts are abstract. It would help to include more examples rooted in actual table data, such as the one in Figure 3 (which is very helpful!). 
}

{
\color{blue}
Note we include examples of queries and their upper bounds in terms of norms on degree sequences. Does the reviewer mean for us to include a different kind of examples to showcase the upper bounds?
}



\vspace{0.5em}\noindent
\textbf{
R3O2. LpBound supports a very specific structure of queries (only single-block SQL Queries). In practice, real workloads contain queries with a much more complex structure. It would help to include a discussion about how LpBound could be extended to work with other queries (even if the estimate is not a tight upper bound). 
}


{
\color{blue}
Indeed, this is a good point. We discuss various extensions in M2. For the specific case of nested blocks of SQL queries, there is an elegant solution that we are working on as part of future work: It requires an extension of LpBound that can yield upper bounds on $\ell_p$-norms on columns in the output of a single-block SQL query. This would allow to treat the output of such a query as any input relation to another query.
}


\vspace{0.5em}\noindent
\textbf{
R3O3. The experimental setup mentions that the group by workloads have "at most one attribute per relation". Is this a limitation of LpBound? What is the purpose of this restriction? 
}

{
\color{blue}
This is not a limitation, we decided to design the queries for the experiments like this for simplicity. Please see discussion in M4.
}


\vspace{0.5em}\noindent
\textbf{
R3O4. Section 6.6 mentions that using norms for $p \in [1, 10] \cup \{inf\}$ works well for all workloads considered. It's not clear whether that is the case for MCVs, however, since JOBlight performed well with 2500 MCVs, but it's not clear whether that setting works well for other workloads. The paper should discuss how to find the optimal value for MCVs for a given workload. 
}

{
\color{blue}
This is a good point. An appropriate number of MCVs for a particular data column depends on the degree sequence and domain size of that column. By inspecting the degree sequence, it pays off to use MCVs for those values whose degrees do not plateau to a small number (say below 10). An alternative, more experimental yet expensive approach is to proceed as in Figure 10: We plot the estimation error as a function of the number of MCVs for an entire workload. We take the number of MCVs for which the error decreases sufficiently and plateaus. 
}

\vspace{0.5em}\noindent
\textbf{
R3O5. Although the estimation time for LpBound is low compared to competing approaches, it's still relatively high -- it would increase the total optimization time for Postgres by 50\% in the case of JOBjoin and JOBlight. This could limit the adoption of LpBound in practice. 
}

{
\color{blue}
The estimation time of LpBound remains very low relative to the ML-based and pessimistic competitors reported in our experiments. True, it cannot be compared with the traditional ones, {\em except} when using the approach mentioned under M1, point 3 for the scenario when the input data may change: It suffices to solve the LP once for a given query, all subsequent changes to the input data do not require solving again the LP but just recomputing an arithmetic formula, just like traditional estimators would do.

Yet LpBound can significantly improve over the accuracy of traditional estimators. As shown in Figure 8, this can lead to significantly better query plans than as produced using the cardinality estimators of the traditional estimators (in particular for the queries STATS104, STATS558, STATS105, STATS122, STATS120, JOBranges860, STATS106, JOBranges830, JOBranges854). 
}

