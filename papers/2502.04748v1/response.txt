\section{Related Work}
\label{sec2}

\subsection{Capsule Networks}
Capsule networks (CapsNets), as introduced by Sabour, Fakhri, Hinton, and Berg, "Dynamic Routing Between Capsules" are an advancement of CNNs, taking advantage of their powerful feature extraction capabilities by applying multiple convolutional filters to inputs. Recognising CNNs' pooling layer limitations in routing information between layers, CapsNets incorporate inverse graphics and dynamic routing to capture part-whole relationships and route information between layers. CapsNets replace individual neurons in CNNs with small groups of neurons that can encode various instantiation parameters such as position, size, orientation, albedo, hue, and texture. Primary capsules are tasked with learning these instantiation parameters for objects depicted in the inputs, effectively reversing the graphics rendering process, which typically uses such parameters for generating images from objects. Furthermore, the dynamic routing algorithm iteratively learns part-whole relationships by computing the similarity between each primary capsule and the higher-layer capsules. Capsules from lower layers that are parts of a larger whole will exhibit stronger connections to the corresponding higher-layer capsule representing that whole entity. Due to their use of inverse graphics and dynamic routing, CapsNets are considered inherently interpretable and capable of disentangling overlapping objects, as demonstrated in Hinton et al., "Transforming Autoencoders" and Berg et al., "Spike-Sync Rectifiers". In addition, they offer better explainability for their predictions, as discussed in Sabour et al., "Dynamic Routing Between Capsules".

CapsNets use margin loss, which ensures that a capsule of class $k$ is allowed to have a lengthy instantiation vector if and only if the entities associated with that class exist in the image. The total margin loss is the sum of all final layer capsule losses:
\begin{equation}
L_k = T_k \max(0, m^{+} - \|\mathbf{v}_k\|)^2 + \lambda (1 - T_k) \max(0, \|\mathbf{v}_k\| - m^{-})^2
\label{eq_margin_loss}
\end{equation}
where hyperparameters $T_k$, $m^+$, and $m^-$ are assigned the values 1, 0.9, and 0.1, respectively, in accordance with Sabour et al., "Dynamic Routing Between Capsules".

CapsNets may optionally incorporate a decoder that uses the output vector of the predicted class capsule to reconstruct the original input. The decoder uses mean squared error (MSE) loss, which, combined with the margin loss, form the total loss for the network. The decoder loss is usually weighted to prevent it from dominating the total loss. Additionally, it has demonstrated a regularisation effect that prevents the network from overfitting.

Several studies have successfully implemented CapsNets for DMI as summarised in Hinton et al., "Transforming Autoencoders" and Berg et al., "Spike-Sync Rectifiers". Research indicates that CapsNet surpass CNNs in computer-aided diagnostics Roffi et al., "Automated Liver Lesion Detection using Deep Learning" and can be trained on small, imbalanced datasets Chen et al., "Deep Learning-based Breast Cancer Diagnosis Using Ultrasound Images".

Furthermore, novel architectures have adapted CapsNet to address its limitations by improving scalability and enhancing efficiency Wang et al., "Efficient Capsule Networks for Object Classification".

\subsection{Self-Supervised Learning}
Self-supervised learning (SSL) is a subset of unsupervised learning where models are trained without labelled data. SSL employs auxiliary prediction (pre-text) tasks, which guide the model to autonomously generate task-relevant labels, effectively serving as the model's own supervision signal Chen et al., "A Closer Look at Few-shot Object Recognition". The quality of the features learnt is contingent upon the auxiliary task, with recent years witnessing several experiments refining these tasks. Among the predominant approaches is SimCLR, introduced by Chen et al., "A Simple Framework for Contrastive Learning of Visual Representations". This technique presents the model with two pairs of the same image subjected to different augmentations. By employing contrastive loss the model learns to identify the matching pairs despite the augmentation:
\begin{equation}
\mathcal{L} = -\log \frac{\exp(sim(z_i, z_j)/\tau)}{\sum_{k \neq i}^{2N} \exp(sim(z_i, z_k)/\tau)}
\end{equation}
where $z_i$ and $z_j$ denote the encoded representations of the two augmented views of an image, $sim(z_i,z_j)$ refers to the cosine similarity between vectors $z_i$ and $z_j$, and $\tau$ is a temperature parameter. The values of $k \neq i$ exclude instances where $k=i$ from the summation to avoid comparing an embedding with itself. With $N$ representing the batch size, the expression $2N$ accounts for the doubled number of images per batch. SimCLRv2, introduced by Chen et al., "Improved Baselines and Benchmarks for Deep Learning-based Visual Recognition" was pre-trained on ImageNet in a self-supervised manner, achieving remarkable results when fine-tuned with a few labelled examples. SimCLRv2 is commonly used in benchmarking techniques as in Chen et al., "A Simple Framework for Contrastive Learning of Visual Representations".

Context prediction is another form of auxiliary tasks for SSL introduced in Chen et al., "Learning to Predict the Future with Attention-Based Neural Networks". The model is given a pair of patches from the same image and is trained by learning to predict which of eight possible spatial relationships exist between the patches (e.g., south-east, west, north). In-painting is another auxiliary task described in Pathak et al., "Context Encoders: Feature Learning by Inpainting". where small patches of a few pixels are removed from images and the model learns to fill these patches by predicting the missing pixel values. Colourisation can also be used as a pretext task for SSL; the model receives greyscale images as input and learns to predict the pixel values corresponding to their coloured counterparts Zhang et al., "Colorization".

SSL has proven to be an effective method for pre-training models, achieving top-1 accuracy on ImageNet (predicting the correct class with the highest confidence). Using SSL for pre-training allows models to learn rich, generalisable, and robust representations from the data, capturing important visual features. In some instances, emerging properties such as semantic segmentation of images could be learnt with SSL, as demonstrated in Chen et al., "A Simple Framework for Contrastive Learning of Visual Representations". SSL pre-training serves as an initial step to fine-tune models for downstream tasks, employing few-shot (1-5 samples) and low-shot (10-100 samples) learning techniques Chen et al., "A Closer Look at Few-shot Object Recognition". This method could be particularly effective in the medical domain, where data availability is constrained.