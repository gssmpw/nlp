\section{Related Work}
\label{sec2}

\subsection{Capsule Networks}
Capsule networks (CapsNets), as introduced by~\cite{sabour-2017}, are an advancement of CNNs, taking advantage of their powerful feature extraction capabilities by applying multiple convolutional filters to inputs. Recognising CNNs' pooling layer limitations in routing information between layers, CapsNets incorporate inverse graphics and dynamic routing to capture part-whole relationships and route information between layers. CapsNets replace individual neurons in CNNs with small groups of neurons that can encode various instantiation parameters such as position, size, orientation, albedo, hue, and texture. Primary capsules are tasked with learning these instantiation parameters for objects depicted in the inputs, effectively reversing the graphics rendering process, which typically uses such parameters for generating images from objects. Furthermore, the dynamic routing algorithm iteratively learns part-whole relationships by computing the similarity between each primary capsule and the higher-layer capsules. Capsules from lower layers that are parts of a larger whole will exhibit stronger connections to the corresponding higher-layer capsule representing that whole entity. Due to their use of inverse graphics and dynamic routing, CapsNets are considered inherently interpretable and capable of disentangling overlapping objects, as demonstrated in~\cite{sabour-2017}. In addition, they offer better explainability for their predictions, as discussed in~\cite{lalonde-2020a}.

CapsNets use margin loss, which ensures that a capsule of class $k$ is allowed to have a lengthy instantiation vector if and only if the entities associated with that class exist in the image. The total margin loss is the sum of all final layer capsule losses:
\begin{equation}
L_k = T_k \max(0, m^{+} - \|\mathbf{v}_k\|)^2 + \lambda (1 - T_k) \max(0, \|\mathbf{v}_k\| - m^{-})^2
\label{eq_margin_loss}
\end{equation}
where hyperparameters $T_k$, $m^+$, and $m^-$ are assigned the values 1, 0.9, and 0.1, respectively, in accordance with~\cite{sabour-2017}.

CapsNets may optionally incorporate a decoder that uses the output vector of the predicted class capsule to reconstruct the original input. The decoder uses mean squared error (MSE) loss, which, combined with the margin loss, form the total loss for the network. The decoder loss is usually weighted to prevent it from dominating the total loss. Additionally, it has demonstrated a regularisation effect that prevents the network from overfitting.

Several studies have successfully implemented CapsNets for DMI as summarised in~\cite{ElShimy2022ARO}. Research indicates that CapsNet surpass CNNs in computer-aided diagnostics~\cite{deepika-2022} and can be trained on small, imbalanced datasets~\cite{Afshar20203DMCNA3}. Furthermore, novel architectures have adapted CapsNet to address its limitations by improving scalability and enhancing efficiency~\cite{deepika-2022}.


\subsection{Self-Supervised Learning}
Self-supervised learning (SSL) is a subset of unsupervised learning where models are trained without labelled data. SSL employs auxiliary prediction (pre-text) tasks, which guide the model to autonomously generate task-relevant labels, effectively serving as the model's own supervision signal~\cite{krenzer-2023}. The quality of the features learnt is contingent upon the auxiliary task, with recent years witnessing several experiments refining these tasks. Among the predominant approaches is SimCLR, introduced by~\cite{chen-2020}. This technique presents the model with two pairs of the same image subjected to different augmentations. By employing contrastive loss the model learns to identify the matching pairs despite the augmentation:
\begin{equation}
\mathcal{L} = -\log \frac{\exp(sim(z_i, z_j)/\tau)}{\sum_{k \neq i}^{2N} \exp(sim(z_i, z_k)/\tau)}
\end{equation}
where $z_i$ and $z_j$ denote the encoded representations of the two augmented views of an image, $sim(z_i,z_j)$ refers to the cosine similarity between vectors $z_i$ and $z_j$, and $\tau$ is a temperature parameter. The values of $k \neq i$ exclude instances where $k=i$ from the summation to avoid comparing an embedding with itself. With $N$ representing the batch size, the expression $2N$ accounts for the doubled number of images per batch. SimCLRv2, introduced by~\cite{Chen2020BigSM} was pre-trained on ImageNet in a self-supervised manner, achieving remarkable results when fine-tuned with a few labelled examples. SimCLRv2 is commonly used in benchmarking techniques as in~\cite{caron-2021}.

Context prediction is another form of auxiliary tasks for SSL introduced in~\cite{Doersch2015UnsupervisedVR}. The model is given a pair of patches from the same image and is trained by learning to predict which of eight possible spatial relationships exist between the patches (e.g., south-east, west, north). In-painting is another auxiliary task described in~\cite{Pathak-2016}, where small patches of a few pixels are removed from images and the model learns to fill these patches by predicting the missing pixel values. Colourisation can also be used as a pretext task for SSL; the model receives greyscale images as input and learns to predict the pixel values corresponding to their coloured counterparts~\cite{Larsson2017ColorizationAA}.

SSL has proven to be an effective method for pre-training models, achieving top-1 accuracy on ImageNet (predicting the correct class with the highest confidence). Using SSL for pre-training allows models to learn rich, generalisable, and robust representations from the data, capturing important visual features. In some instances, emerging properties such as semantic segmentation of images could be learnt with SSL, as demonstrated in~\cite{caron-2021}. SSL pre-training serves as an initial step to fine-tune models for downstream tasks, employing few-shot (1-5 samples) and low-shot (10-100 samples) learning techniques~\cite{krenzer-2023}. This method could be particularly effective in the medical domain, where data availability is constrained. 

%
%
%