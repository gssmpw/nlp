\section{Survey Method}
\label{sec:methodology}
This paper follows the guideline for conducting systematic reviews in software engineering introduced by Kitchenham and Charters~\cite{keele2007guidelines}, which suggests performing a systematic literature review in three phases: planning the review, conducting the review, and reporting the review results.
To this end, we first identify the need for our review and formulate the research questions.
In the second phase, we define the search string and inclusion and exclusion criteria to design our strategy for finding and selecting the primary studies of automated exploit generation.
Lastly, we review the selected papers in-depth and report our review results.
%
In this section, we first introduce the set of research questions underlying our survey (Section~\ref{sec:research-questions}). Next, we discuss how the papers from the literature are collected (Section~\ref{sec:paper-collection}) and the criteria that we derived and used to select the studies for the inclusion of our survey (Section~\ref{sec:relevance-criteria}). After all the papers had been collected, they were organized, categorized, and reviewed (Section~\ref{sec:paper-organization}).
\ifthenelse{\boolean{deliverable}}
{}
{\ema{We should also define a quality assessment form.}}

\subsection{Survey Goal and Research Questions}
\label{sec:research-questions}

The \textit{goal} of this study is to survey the literature on techniques and approaches to automatically generate exploits for software vulnerabilities.
The \textit{purpose} of this is to understand which techniques and approaches exist, their peculiarities, and under which circumstances they can be used.
This will provide security practitioners and researchers with a catalog of usable tools and elements for possible advancements in AEG.

Specifically, this study aims to achieve two main research objectives ($RO$):
\begin{itemize}
    \item \textbf{$RO_1$.} Analyze the research on AEG made so far and their key characteristics.
    \item \textbf{$RO_2$.} Evaluate the maturity of existing AEG techniques and their usability.
\end{itemize}

Therefore, we formulated seven research questions, shown in Table~\ref{tbl:research-questions}.
$RQ_1$--$RQ_5$ address $RO_1$, $RQ_6$--$RQ_7$ contribute to $RO_2$.
\ifthenelse{\boolean{deliverable}}
{}
{\ema{To revise better and provide the ratio for all objectives we tackle.}}

\input{tables/rqs}

\subsection{Paper Collection}
\label{sec:paper-collection}
%The collection process of the papers in our study follows the procedure (name an active searching here~\cite{}), which includes two main steps: \textit{Active Searching} and \textit{Snowballing}.
To collect the studies for our survey, we designed the search string as \texttt{``( automat* AND software AND ( secur* OR vulnerab* ) AND ( test OR exploit ) AND generat* )''}.
%
With this, we aimed to capture papers concerning \textbf{automated techniques that can generate exploits or tests for software vulnerabilities}.
We maximize the chances to hit more papers by using the asterisk operator, for example, the search keyword \textit{``vulnerab*''} can retain the papers which contain the word \textit{``vulnerable''} or \textit{``vulnerability''} or \textit{``vulnerabilities''}.
The query was run only on the title, abstract, and keyword content, which is generally sufficient to capture the most relevant results.

\ifthenelse{\boolean{deliverable}}
{We performed our search on the \textsc{Scopus} database.\footnote{\textsc{Scopus} website: \url{www.scopus.com}} 
\textsc{Scopus} was chosen as the main database for surveying the literature as it covers the publications from all other popular choices, such as IEEE Xplore,\footnote{IEEE Xplore website: \url{https://ieeexplore.ieee.org/}} ACM Digital Library,\footnote{ACM Digital Library website: \url{https://dl.acm.org/}} and Web of Science.\footnote{Web of Science website: \url{https://clarivate.com/products/web-of-science/}}
We do not claim to have all the relevant papers in this area; however, we believe that the main results are covered to conduct an adequate survey of AEG techniques in the literature.}
{\ema{Present all sources here. Despite Scopus being the key, we also include IEEE, ACM, and WoS to further add things that Scopus might miss.}}

\ifthenelse{\boolean{deliverable}}
{}
{\ema{This should be updated to June/July 2024 at least...}}

The search retrieved an initial list of 1,206 papers up to June 2023.
After applying some screening filters (as described in Table~\ref{tbl:screening-criteria}), 608 papers were retained for the next steps.
Such filters aim to remove poorly relevant candidates and provide a reasonable inspection workload.

\ifthenelse{\boolean{deliverable}}
{}
{\ema{After we finalize all the numbers in the end, we need a figure}}

\subsection{Inclusion/Exclusion Criteria}
\label{sec:relevance-criteria}

\ifthenelse{\boolean{deliverable}}
{}
{\ema{How many researchers in the end?}}

\input{tables/screening}

We performed the paper selection with four researchers, so we believe a clear list of inclusion/exclusion criteria will aid the researchers significantly with this assessment task.
To distill the right set of criteria, we ran a ``calibration'' phase on a sample of 101 studies selected from the 608 screened papers.
Specifically, we selected the most cited papers having at least $\geq$ 28 citations according to the number provided by \textsc{Scopus}.

\ifthenelse{\boolean{deliverable}}
{}
{\ema{Why 28 citations?}}

\input{tables/ie}
To this end, we created a starting set of six exclusion criteria (as described in Table~\ref{tbl:screening-criteria}).
Then we asked each researcher to read the paper summary (e.g., Title, Abstract, Keywords) and independently make a decision for the paper: \textit{Included}, \textit{Excluded}, or \textit{Not Sure}. During the selection process, the researchers clarified which criteria the paper failed to meet. If the criteria did not exist, it was written down explicitly.
Ultimately, we merged the results from all the researchers. If there were any conflicts, we applied the voting system to decide the inclusion and exclusion of the paper. If there was no winner, we discussed it until we reached a consensus. There were not any papers marked with \textit{Not Sure} by all four researchers; therefore, they were either included or excluded at the end of our discussion.
As a result, we included 18 papers after the calibration phase. As an outcome of this process, we defined six more exclusion criteria for filtering the relevance of the papers. In total, we had twelve exclusion criteria (shown in Table~\ref{tbl:relevance-criteria-final}) used to guide the selection of the rest of the papers in our survey.

We then divide the rest 507 papers (\textit{= 608 - 101}) into smaller batches and assign them to the four researchers. Since we have a clear list of exclusion criteria this time, each paper was reviewed by only one researcher, and there was no conflict resolution afterward. Ultimately, we included 48 more papers, admitting 66 for the review phase.

\subsection{Paper Review and Categorization}
\label{sec:paper-organization}

\input{tables/papers}

After we had selected papers for reviewing, we grouped papers that were (i) part of the same study or (ii) presented the development or evolution of the same approach.
%, and later, one was extended to the exploitation techniques proposed in the prior one
We treated each group as a single body of work in our reviewing process.
The merge was guided by the paper's author list and their actual content.
\ifthenelse{\boolean{deliverable}}
{}
{\ema{I link this grouping of things, but we might need to define more objective and less-ambiguous criteria.}}
This step identified four papers that could be grouped together, in which the authors developed a security testing technique based on threat modeling~\cite{xu2011tool,xu2012automated,marback2013threat,Xu2015247}.

Based on our aimed scope of this survey study and the selection results of the paper list, we defined four main groups of techniques of the relevant areas of automated exploit generation and security testing shown in Figure~\ref{fig:taxonomy}, that is, AEG, Security Testing, Fuzzing, and Others.
This helps us construct the taxonomy used to categorize the resulting papers.
We acknowledge that there can be other ways to categorize the studies in our survey.
\ifthenelse{\boolean{deliverable}}
{}
{\ema{As I said somewhere before, I feel these categories should be recreated. Not it's okay, but later. Then, we will also check if the taxonomy aligns with the one Cuong mentioned~\cite{pendleton2016survey}} \cuong{Add more points to convince that our taxonomy is good enough. See the same point in the two other CSUR papers}.}
However, we believe that our taxonomy aligns with security standards in practice~\cite{pendleton2016survey}.
Then, we read the whole text of all the papers to assign them to their right category.
\ifthenelse{\boolean{deliverable}}
{}
{\ema{We must explain here what we mean with each category (a table is fine)}}

\ifthenelse{\boolean{deliverable}}
{}
{\ema{We need to define the ``data collection form'', i.e., the aspects that we looker while reading the papers (i.e., the columns of the Excel sheet)}, so that we can motivate the columns in the big table}

The list of papers that went under review is reported in Table~\ref{tbl:rqs-g1}.
The information reported is based on the data extracted from papers to answer the defined research questions. 
%
The first three columns present the study IDs, associated publications (one study may span multiple publications), and the publishing years.
The next two columns show the techniques' required inputs and produced outputs. The next three columns describe information about the techniques' targets in terms of vulnerability types, programming languages, and targeted applications. The last two columns provide information on how the oracles generated from the tools were assessed and the automation level of the tools, which can be Fully Automated (FA), Semi-Automated (SA), or Interactive (Inter.).

\begin{figure}
\begin{tikzpicture}[
	level 1/.style={sibling distance=40mm},
	edge from parent/.style={->,draw},
	>=latex]

	% root of the the initial tree, level 1
	\node[root] {Our survey}
	% The first level, as children of the initial tree
	child {node[level 2] (c1) {AEG}}
	child {node[level 2] (c2) {Security Testing}}
	child {node[level 2] (c3) {Fuzzing}}
	child {node[level 2] (c4) {Others}};
	
	% The second level, relatively positioned nodes
	\begin{scope}[every node/.style={level 3}]
		\node [below of = c1, xshift=15pt] (c11) {Control-flow Hijacking};
		\node [below of = c11] (c12) {Data-oriented};
		
		\node [below of = c2, xshift=15pt] (c21) {Threat Model-based};
		\node [below of = c21] (c22) {Mutation-based};
		\node [below of = c22] (c23) {Grammar-based};
		\node [below of = c23] (c24) {Search-based};
		\node [below of = c24] (c25) {Heuristic-based};
		
		\node [below of = c3, xshift=15pt] (c31) {Whitebox};
		\node [below of = c31] (c32) {Coverage-based greybox};
		\node [below of = c32] (c33) {Stateful};
		\node [below of = c33] (c34) {Directed};
		\node [below of = c34] (c35) {Knowledge-based};
		
		\node [below of = c4, xshift=15pt] (c41) {Dynamic Symbolic Execution};
		\node [below of = c41] (c42) {Dynamic Taint Analysis};
		\node [below of = c42] (c43) {String Analysis};
	\end{scope}
	
	% lines from each level 1 node to every one of its ''children''
	\foreach \value in {1,2}
	\draw[->] (c1.195) |- (c1\value.west);
	
	\foreach \value in {1,...,5}
	\draw[->] (c2.195) |- (c2\value.west);
	
	\foreach \value in {1,...,5}
	\draw[->] (c3.195) |- (c3\value.west);
	
	\foreach \value in {1,...,3}
	\draw[->] (c4.195) |- (c4\value.west);
\end{tikzpicture}
\caption{Taxonomy of studies on Automated Exploit Generation.}
\label{fig:taxonomy}
\end{figure}

