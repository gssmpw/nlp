% \section{Limitations and Threats to Validity}
% \label{sec:limitations}

% Given the large volume of research on automated exploit generation and security testing, some studies may not appear in this literature review.
% However, we tried to reduce this risk by employing a simple search query that only searched for essential terms, which directly stemmed from the scope of our survey.
% We did not apply filters for the publication year to increase the number of papers returned after the initial search.
% As observed in Section~\ref{sub:rq1}, we found that no publication goes back to 2008, which is less than 20 years ago. For this, we did not find outdated approaches that might not be relevant today.

% %We selected \textsc{Scopus} as our main source due to its capability of aggregating many publisher databases.
% %Extensively collecting papers from various sources and using a snowballing process.
% \ifthenelse{\boolean{deliverable}}
% {}
% {\ema{To revise as soon as we add the other sources and snowballing}}

% An improper definition of inclusion/exclusion criteria might affect the final set of papers approved for review. To mitigate this risk, we ran a calibration phase with the main inspectors on a sample of 101 studies to refine all adequate criteria for selecting the paper concerning the topic of AEG. During this process, all 101 papers were inspected by reading the papers' title, abstract, keywords, introduction, and conclusion. Then, each inspector proposed their opinion on whether the paper did not fit the scope and the reason why.
% All the reasons have been collected and discussed together until an agreement on a common set of inclusion/exclusion criteria has been reached.
% Afterward, the papers discarded among the 101 were rapidly skimmed again to re-assign the right criterion that led to its removal.

% \ifthenelse{\boolean{deliverable}}
% {}
% {\ema{Need quality assessment TTV}}

% Another potential limitation might affect the taxonomy with the aspects created in this study may be incomplete or inaccurate. We do not claim that our taxonomy is the only possible one that can be derived from the paper results. However, we assess our taxonomy aligns with the security standards used in practice. Therefore, we believe that our taxonomy can help identify the gap in the literature and provide insightful results to the research community.

%%%%%%%%%%%%%%%%%%

\section{Threats to Validity}
\label{sec:limitations}
The goal of our study was to provide a comprehensive overview of the current body of knowledge on automated exploit and security test generation techniques. This section discusses the potential limitations that may have impacted the comprehensiveness of our analysis, along with the methodological actions applied to mitigate them. We specifically identified two primary sources of threats to validity: those related to \emph{literature selection} and those concerning the \emph{analysis and synthesis of the selected studies}.

\smallskip
\textbf{Literature Selection.} One potential threat to validity arises from the process of literature selection, as there is always a risk of incomplete coverage.
%as our main source due to its capability of aggregating many publisher databases. Extensively collecting papers from various sources and using a snowballing process.
Although we employed a structured search strategy, certain relevant studies may have been inadvertently excluded due to its lack of coverage of other sources.
%across multiple major academic databases, including Scopus, ACM Digital Library, IEEE Xplore, and Web of Science, certain relevant studies may have been inadvertently excluded.
This could also be due to limitations in the search string, as different studies might employ alternative terminology or domain-specific phrasing that was not explicitly captured by our query. To mitigate this risk, we employed a search query that only searched for \emph{essential} terms, i.e., terms that are supposed to be present in any relevant article. In addition, we surveyed papers published in an extensive timeframe, from 2005 and 2023, to increase the number of papers returned after the initial search. As observed in Section~\ref{sub:rq1}, we found that no publication goes back to 2008, hence increasing our confidence in the comprehensiveness of the search. Additionally, we supplemented the database search with a \textit{snowballing} approach, manually tracing citations and references from selected papers to identify additional works that might not have been retrieved through keyword-based queries. Although this process helped improve coverage, our snowballing was limited to studies categorized under ``Automated Exploit Generation'' and ``Security Testing'', meaning that potentially relevant studies from adjacent domains may not have been included. However, we justified this focus based on the research scope, ensuring alignment with the core objectives of this review.

As an additional consideration, the \textsc{Scopus} database considered for the search indexes a substantial portion of peer-reviewed research. Yet, it may not comprehensively cover all security-related venues, particularly those associated with industry-driven research or emerging methodologies published in specialized security workshops.
In this respect, it is worth reporting that the goal of our study was to survey mature scientific contributions in the field rather than emerging trends or niche approaches. As such, this potential limitation does not impact the scope of our investigation. 

An improper definition of inclusion/exclusion criteria might affect the final set of papers approved for review. To mitigate this risk, we ran a calibration phase with the main inspectors on a sample of 101 studies to refine all adequate criteria for selecting the papers concerning the topic of AEG. During this process, all 101 papers were inspected by reading the papers' title, abstract, keywords, introduction, and conclusion. Then, each inspector proposed their opinion on whether the paper did not fit the scope and the reason why. All the reasons have been collected and discussed together until an agreement on a common set of inclusion/exclusion criteria has been reached.
Afterward, the papers discarded among the 101 were rapidly skimmed again to re-assign the right criterion that led to its removal.

\smallskip
\textbf{Literature Analysis and Synthesis.} After selecting the relevant studies, we applied multiple manual analysis steps to classify exploit generation techniques, assess automation levels, and evaluate tool availability. Given the diversity of methodologies and experimental settings reported in the literature, there is an inherent risk that some findings may be interpreted differently depending on the criteria used for classification. The heterogeneity in research designs, evaluation benchmarks, and dataset sizes further complicates direct comparisons between studies, potentially affecting the validity of our conclusions. To address these concerns, we adopted a structured data extraction process, where multiple reviewers independently assessed each study to ensure consistency in categorization. Any discrepancies in classification or interpretation were resolved through discussion and consensus, reducing the likelihood of individual bias influencing the results. Additionally, we maintained transparency by explicitly reporting cases where information had to be inferred due to missing details in the original studies. 