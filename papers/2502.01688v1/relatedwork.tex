\section{Related Works}
% \subsection{Graph Out-of-Distribution Generalization}

OOD or distribution shift is a longstanding problem in machine learning \citep{goyal2017accurate,zhang2018mixup,sagawa2019distributionally,krueger2021out}. Most existing graph OOD methods aim to extract invariant subgraphs across all samples to enhance model generalization under distribution shifts. GIL \citep{li2022learning} is a pioneering GNN-based model that identifies invariant subgraphs for graph classification tasks. It explores invariant graph representation learning in mixed latent environments without requiring labeled environments. DIR \citep{wu2022dir} introduces a causal inference approach to identify invariant causal parts through causal interventions. However, DIR involves a complex iterative process of breaking and assembling subgraphs during training. A more straightforward approach is GSAT \citep{miao2022interpretable}, which is based on the information bottleneck principle and learns invariant subgraphs by reducing attention stochasticity. RGCL \citep{li2022let} combines invariant rationale discovery with contrastive learning to improve both generalization and interpretability. CIGA \citep{chen2022learning} proposes an information-theoretic objective to extract invariant subgraphs, offering a theoretical guarantee for handling distribution shifts under different Structural Causal Models, which inspired a number of follow-up approaches~\citep{gala,yao2024empowering}. Similarly, GMT \citep{cheninterpretable} focuses on extracting interpretable subgraphs by accurately approximating subgraph multilinear extensions, ensuring both interpretability and generalization under OOD conditions.  A common finding across these invariant learning-based methods is the dependence on the diversity of environments. To address this, IGM \citep{jia2024graph} introduces a co-mixup strategy that combines environment and invariant mixups to generate diverse environments. These OOD methods that focus on extracting causal subgraphs work well in molecular and social networks but face challenges in brain network analysis due to the unique noise in both structures and features. These methods often overlook the selection of important node features, reducing their effectiveness for brain networks. Additionally, invariant subgraphs identified by these methods may not adequately capture the distinct functional implications of different brain regions, underscoring the need for a specialized approach. We include the discussion of more related works about brain network analysis with GNNs in Appendix \ref{app:related}.


% Furthermore, while data harmonization methods \citep{guan2021multi,chan2022semi,wang2022multi} have been widely applied in this field, they typically require learning a mapping from the reference domain to the source domain. This constraint makes harmonization methods less effective when dealing with subjects from entirely unseen sites. Thus, developing OOD algorithms tailored specifically for brain networks is a pressing need in this domain.