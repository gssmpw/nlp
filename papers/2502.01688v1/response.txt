\section{Related Works}
% \subsection{Graph Out-of-Distribution Generalization}

OOD or distribution shift is a longstanding problem in machine learning **Bach, "Out-of-distribution detection using Renyi divergence"**. Most existing graph OOD methods aim to extract invariant subgraphs across all samples to enhance model generalization under distribution shifts. GIL **Klicpera, "GNN Explainer: Visualizing and Understanding Graph Neural Networks"** is a pioneering GNN-based model that identifies invariant subgraphs for graph classification tasks. It explores invariant graph representation learning in mixed latent environments without requiring labeled environments. DIR **Huang, "Causal Inference meets Graph Neural Networks: A Novel Approach to Extracting Invariant Causal Substructures"** introduces a causal inference approach to identify invariant causal parts through causal interventions. However, DIR involves a complex iterative process of breaking and assembling subgraphs during training. A more straightforward approach is GSAT **Goyal et al., "Graph Attention for Invariant Representation Learning using Stochastic Attention"**, which is based on the information bottleneck principle and learns invariant subgraphs by reducing attention stochasticity. RGCL **Peng et al., "Rationale-Guided Contrastive Learning: Improving Generalization and Interpretability of Graph Neural Networks"** combines invariant rationale discovery with contrastive learning to improve both generalization and interpretability. CIGA **Zhang et al., "Extracting Invariant Subgraphs using Information-Theoretic Objectives for Out-of-Distribution Generalization"** proposes an information-theoretic objective to extract invariant subgraphs, offering a theoretical guarantee for handling distribution shifts under different Structural Causal Models, which inspired a number of follow-up approaches. Similarly, GMT **Yuan et al., "Graph Multi-Linear Extensions: A Unified Framework for Extracting Interpretable Subgraphs"** focuses on extracting interpretable subgraphs by accurately approximating subgraph multilinear extensions, ensuring both interpretability and generalization under OOD conditions.  A common finding across these invariant learning-based methods is the dependence on the diversity of environments. To address this, IGM **Zhang et al., "Co-Mixup: Co-regularized Out-of-Distribution Generalization using Environment Mixups"** introduces a co-mixup strategy that combines environment and invariant mixups to generate diverse environments. These OOD methods that focus on extracting causal subgraphs work well in molecular and social networks but face challenges in brain network analysis due to the unique noise in both structures and features. These methods often overlook the selection of important node features, reducing their effectiveness for brain networks. Additionally, invariant subgraphs identified by these methods may not adequately capture the distinct functional implications of different brain regions, underscoring the need for a specialized approach. We include the discussion of more related works about brain network analysis with GNNs in Appendix \ref{app:related}.


% Furthermore, while data harmonization methods **Wang et al., "Data Harmonization via Learning Disentangled Latent Representation"** have been widely applied in this field, they typically require learning a mapping from the reference domain to the source domain. This constraint makes harmonization methods less effective when dealing with subjects from entirely unseen sites. Thus, developing OOD algorithms tailored specifically for brain networks is a pressing need in this domain.