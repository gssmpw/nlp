
\section{Methodology}
Figure \ref{fig:main_figure} provides an overview of our pipeline for interactive evalution, which is driven by an iterative refinement loop in which the \user{} model and \cm{}  interact.
At each step of the loop, the \cm{} is given the obfuscated programming question in natural language, the code from the previous attempt, and the \user{} model's feedback on the previous attempt. Each iterative refinement loop lasts $5$ steps for each question, terminating early if a correct solution is reached in less than $5$ steps.
 

\subsection{Dataset Transformation}
\label{sec:dataset_transform}
Our pipeline is designed to transform static coding questions into interactive ones. 
We describe our selection criteria for the datasets used in this work, as well as the input obfuscation protocol that enables collaboration between the \user{} and \cm{}.



\paragraph{Input obfuscation.} 
To ensure that the \user{} and \cm{} collaborate, we obfuscate the input given to the \cm{} to remove critical information from the input (Figure \ref{fig:main_figure}A). 
This induces an information asymmetry, akin to that which might exist in practice, that forces the \cm{} to rely on feedback from the \user{} to recover key details about the full problem specification.
We underspecify the input to the \cm{} by using \sonnet{} to summarize the original questions, which often contain significant detail about desired behaviors and potential edge cases, into one-sentence summaries.



To illustrate input obfuscation, consider a question from APPS (Interview) which asks how long it will take for two flight attendants to serve lunch to a customer in a given seat. The original question includes details about the flight attendant's serving speed, the order they traverse the rows of the plane, and the serving order of seats in the row. 
The summarized form of this question might be \textit{``Calculate the time it takes for a passenger in a specific seat to receive their lunch on an airplane with an infinite number of rows, given the serving pattern of two flight attendants moving from front to back,''} omitting some of the key details required to fully solve the question (e.g., row and seat order).
We use this as a running example in the following section.


\paragraph{Datasets.} 
We select challenging datasets with lengthy problem descriptions and available ground-truth solutions.
We use the Interview and Introductory levels of APPS~\citep{hendrycks2021apps}, ClassEval~\citep{du2023classeval}, and the Easy, Medium, and Hard levels of LiveCodeBench~\citep{jain2024livecodebench}.
We randomly sample $200$ examples from APPS Interview, $200$ examples from APPS Introductory, and $75$ examples from ClassEval. 
We use $70$ examples from LiveCodeBench (across all three difficulty levels). 
More details on the datasets can be found in Appendix \ref{app:datasets}.

\subsection{Feedback Types} 
Existing work on developer-\cm{} interactions shows that programmer feedback is diverse~\citep{chidambaram2024socratic}. 
Following their results, we explore multiple variants of feedback types outside of generic natural language feedback and investigate four categories of feedback: \para, \sent, \ir, and \cf{} (Figure \ref{fig:main_figure}B).

The \sent{} and \para{} feedback types are ones where the \user{} model provides feedback using natural language.
These feedback styles mimic the inputs used in chat-based interfaces, where users respond to the model via a chat window. In our setting, the \user{} is prompted to only use a sentence or paragraph for their response. 
An example of \sent{} feedback for the airplane question might be \textit{``The current solution doesn't follow the seat serving order f-e-d-a-b-c,''} while \para{} feedback tends to have specific critiques of the algorithm with sentences like \textit{``It doesn't account for the two flight attendants serving simultaneously. It should first calculate the number of complete 4-row blocks served, then handle the remainder.''} 

\ir{} and \cf{} feedback aim to replicate common feedback styles from developers~\citep{chidambaram2024socratic}. 
In \ir{}, the \user{} is prompted to rewrite a similar-length version of the underspecified question with additional details required for the \cm{} to find a solution. 
We constrain the length to mimic how real users rephrase their inputs when providing feedback and to prevent the \user{} from simply copying in the full question.
An example of \ir{} in the airplane question might be \textit{``Question: Calculate the time for a passenger to receive lunch on a plane where two flight attendants serve food. Attendants start at rows 1 and 3, move forward by 2 rows after serving. They serve right side (f to d) then left side (c to a) of each row. Output the waiting time in seconds.''}
\cf{} prompts the \user{} to directly indicate which lines of code are incorrect and suggest alternate code snippets. 
An example of \cf{} in the airplane question might be \textit{``The function get\_time\_to\_lunch(seat, num\_attendants) should be get\_time\_to\_lunch(seat) as the number of attendants is always 2.''}
Appendix \ref{app:feedback} provides additional examples of each type of feedback.





\subsection{Code Models}
We select a total of \nmodels{} \cm s, spanning both open-source and closed models and a wide range of capabilities and parameter sizes. We selected the following open-source models for their ability to follow user instructions, their range of parameter sizes, and overall coding capability:  \deepseek~\citep{deepseekai2024deepseekv3technicalreport}, \gemmaSmall~\citep{gemmateam2024gemmaopenmodelsbased}, \gemmaLarge{}~\citep{gemmateam2024gemma2improvingopen}, \llama{}~\citep{grattafiori2024llama3herdmodels}, \qwenSmall{}~\citep{hui2024qwen25codertechnicalreport}, and \qwenLarge{}~\citep{hui2024qwen25codertechnicalreport}. 
We select the following closed models for their commercial adoption and performance on existing static benchmarks:
\aya{}~\citep{aya2024modelinstructionfinetuned},, \gpt{}~\citep{openai2024gpt4ocard}, \reka{}~\citep{rekateam2024rekacoreflashedge}, and \sonnet{}~\citep{claude}. The parameters used to query each model are provided in Appendix~\ref{app:models}.

\paragraph{Code model prompts.} 
We prompt the \cm s with their previous solution and the \user{} feedback on that solution. 
We do not provide a history of all \cm{} interactions with the \user{}, only the most recent code attempt and \user{} feedback on the most recent attempt.
Specific instructions are given for each dataset for varying input and output formats.
All prompts are in Appendix \ref{app:prompts}.




\subsection{User Models}

Following prior work~\citep{dubois2023alpacafarm,zheng2023judging,mozannar2023simulating}, we use LLMs to scalably simulate feedback given by \user s when interacting with \cm s. 
To help close the capability gap between real-world expert users and LLMs, we give the \user{} model access to the original fully-specified question, as well as a ground-truth solution. 
This allows the simulated \user{} to produce higher-quality feedback more often. 
The \user{} model prompt also includes instructions to avoid leaking the exact solution in its responses to the \cm.
We only constrain the formatting style of the feedback, allowing the \user{} to choose the content of criticism (e.g. input/output formatting, algorithmic correctness, code style).
We choose \sonnet{} due to its high performance on static coding benchmarks and strong reasoning capabilities, but verify that other \user{} models (e.g., \texttt{GPT-4o-mini}) are also able to improve performance over \baseline{} (see Section \ref{sec:static_vs_interactive_perf} for details).
These results, along with prompts for the \user{} can be found in Appendix \ref{app:prompts}.






