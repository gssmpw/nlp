
\input{figtext/feedback_steerability_barchart}

\section{Model Steerability}

We extend our analysis beyond performance to investigate \textbf{steerability}, or how much a \cm{} adjusts its previous solution in response to feedback.
We show that drops in performance in interactive settings are likely due to ineffectively incorporating feedback, rather than outright ignoring feedback in the next iteration of the solution.

\input{figtext/edit_distance_vs_steerability_heatmap}


\subsection{Steerability Metrics}
We evaluate model steerability on APPS and LiveCodeBench\footnote{We do not evaluate on ClassEval as their evaluation utilities do not report exactly which test cases pass or fail.} and consider two metrics of change between solution iterations. 
Firstly, we use \textbf{Levenshtein edit distance} to evaluate surface-level changes between consecutive versions of code. 
Secondly, we count the number of \textbf{changes in test-case behavior} --- with respect to whether incorrect test cases flip to correct or vice-versa --- to evaluate behavioral-level adjustments to the code.
We refer to the former as \textit{surface-level steerability} and the latter as \textit{behavioral steerability}.  



\subsection{Results}
\paragraph{\para{} feedback is associated with higher behavioral-level and surface-level steerability across all models.} 
Figure \ref{fig:feedback_steerability_barchart} plots each feedback type by the behavioral (top) or surface-level (bottom) steerability it induces.
\para{} and \cf{} score the highest in behavioral steerability (changing 21.8\% and 19.3\% of test cases on average), while \para{} score the highest in surface-level steerability (with an average edit distance of 445.6 characters).
Notably, \para{} is also one of the highest-performing feedback styles, suggesting that it induces effective changes in the code solution on both the behavioral and surface levels. 



\paragraph{Weaker models tend to make surface-level rather than effective behavioral-level changes.} 
Figure \ref{fig:edit_distance_vs_steerability_heatmap} shows how \cm s change their previous solutions on APPS Interview across feedback types.
Weaker models (e.g. \gemmaSmall{} and \llama) tend to make many surface-level changes that do not greatly change the behavior of the code.
However, stronger models (e.g. \gpt{}, \sonnet, and \qwenLarge) may make relatively small edits that highly affect code behavior.