\section{Conclusion}
We propose a new approach to evaluating \cm s by introducing an interactive pipeline where the \cm{} must collaborate with a simulated \user{} to solve underspecified coding problems with different feedback types.
We find that the relative performances of models change radically between static and interactive settings.
We analyze key elements of model-user interactions, such as feedback quality and model steerability, to provide insights into the downstream effects of feedback type on model behavior and feedback effectiveness.
Our work bridges the gap between existing static benchmarks and real-world usage, and we hope to inspire future work on scalable methods for evaluating models in a collaborative setting.