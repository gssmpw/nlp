


\section{Introduction}

\val{add cites throughout}

Human (or even LLM-based) feedback is increasingly used for reward modeling and aligning large language models (LLMs) with downstream tasks. 
While feedback is important in the model training process, these models are increasingly also deployed at test time, encountering varying forms of feedback.
Understanding how models incorporate feedback is relevant in multi-agent systems, where the ``orchestrator'' may delegate sub-tasks to specialized agents and provide feedback on that agent's progress.
Humans also increasingly interact with and provide feedback to LLMs, where chat interfaces like ChatGPT facilitate multi-turn conversations in which humans can iteratively refine the task they want to accomplish using LLMs.
Across these use cases, there is no form of feedback that has been established as optimal.
In this work, we ask: \textit{how well do LLMs incorporate different forms of feedback?}

Programming applications are the focus of our work, and are a setting where one might find multiple forms of feedback.
Programming with an assistant is a highly collaborative task where the user and model develop a solution together.
For example, one increasingly common workflow is to iterate with a chat-based coding assistant to build and refine code, where refinement steps involve users correcting specific code or writing natural language to change the code functionality. 
Despite the popularity of these tools---millions of developers use Github Copilot---existing
evaluations of the coding capabilities of new models fail to capture these interactive settings.
Existing benchmarks rely on a simple input-output configuration, where the question and provided code are fully provided and the model is asked to generate the whole completion in one shot. 
Existing benchmarks also assume that the input is well-specified and error-free, further reducing the need for interactive feedback.

% In order to complete a provided code snippet, models must correctly infer the full user intent---i.e., the function that the user wants to implement via the code, and potentially rely on explicit human feedback to steer model generation.
 


% Not only do code benchmarks not properly simulate realistic programming settings, but they also avoid other critical questions that arise as a result of model-user interaction. For instance, in practice, users do not usually accept large blocks of model-generated code -- they may delete a significant portion of the generation and insert their own code or comments. In doing so, they are implicitly performing a variation of \textbf{mid-generation guidance}, in which the user intervenes midway through a model generation. Explicitly examining this setting for a code model raises several key questions. For instance, when should a user intervene? Can models identify when they require more information from the user and query them for mid-generation guidance? 

% Since this aspect of the coding process is understudied, most users of code assistants do not trust models to make changes on a larger-than-local level. Programmers usually allow models to generate just a few lines of code, which can be easily manually checked for correctness. This limits the utility of code assistants in a practical setting as programmers cannot rely on models to correctly generate longer completions.


% In order to complete a provided code snippet, models must correctly infer the \textbf{user intent} -- the function that the user wants to implement via the code. This process is usually done \textit{implicitly}, i.e. the model directly generates a code completion that implements the user's desired function. Meanwhile, user-provided \textit{explicit} statements of intent (including function docstrings or comments) can be critical to steering model generation. If they are provided by the user, they are usually provided once, i.e. before the generation. Moreover, while high-level intent may be clear from a provided docstring, unforeseen choices (e.g. over edge cases or what variables to optimize) may remain ambiguous.


% Currently, most users of code assistants do not trust models to make changes on a larger-than-local level. Programmers usually allow models to generate just a few lines of code, which can be easily manually checked for correctness. However, when coding assistants are allowed to generate larger amounts of code, programmers often find that the code is buggy or does not implement the precise functionality that the programmer intended. This limits the utility of code assistants in a practical setting as programmers cannot rely on models to correctly generate longer completions.

In this work, we study how code models perform under more realistic user interactions.
We propose an evaluation method that transforms a generic coding benchmark into one that incorporates underspecification and human feedback. 
Specifically, given a code dataset with well-specified problems, test cases, and at least one ground-truth solution, we introduce a pipeline that creates under- or misspecified problems and a simulated user who can be queried for guidance, with a variety of feedback formats.
The goal is to capture characteristics of real code repositories and interactions with programmers that may include underspecified problems (e.g. over edge cases or what variables to optimize) in the form of function docstrings or comments.
We explore multiple formats including user-written comments, code, and user selection of model-generated multiple-choice answers to a model-generated question.


Across 9 models and 3 coding benchmarks, we find multiple insights.
\val{just copied directly from gdoc:} Models that perform similarly on static coding benchmarks often perform differently in the collaborative setting; on the other hand, models that perform differently on static coding benchmarks may perform similarly in a collaborative setting.
Models vary in terms of which feedback type is most effective.
Question difficulty (and other question-specific aspects??) affect how models incorporate feedback.
Feedback quality affects how the model affects feedback.
\val{Need to give more specific examples once we have a draft of results in, Talk about findings and implications for future design of coding assistants.}



% \begin{itemize}
%     \item \textbf{A plug-and-play pipeline.} Our code is designed to easily integrate with any code benchmark.
%     \item \textbf{Input perturbation to simulate under- or misspecification of problems.} 
%     \item \textbf{A simulated user who can be queried for guidance, with a variety of feedback formats.} Formats include user-written comments, user-written code, and user selection of model-generated multiple-choice answers to a model-generated question.
%     \item \textbf{Multiple settings for incorporating user feedback.} User guidance can be introduced via both iterative refinement and mid-generation guidance. In the latter, a heuristic must be applied to guide when the model pauses to allow user intervention.
% \end{itemize}


\begin{comment}
\begin{enumerate}
    \item What kind of task description errors are realistic?
    \begin{enumerate}
        \item Typically, errors in the input can be categorized as underspecification (via missing information or ambiguity), misspecification (via erroneous instructions), or a combination of both.
    \end{enumerate}
    \item How do we simulate feedback from a human, and what forms can this take?
    \begin{enumerate}
        \item 
    \end{enumerate}
    \item{What mode of incorporating user feedback have the best effects on performance and efficiency?}
\end{enumerate}
\end{comment}
