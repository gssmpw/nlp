% \input{figtext/vanilla_baseline_barplot}

\section{Static vs. Interactive Performance}
\label{sec:static_vs_interactive_perf}

\input{tables/perf_per_dataset_per_feedback}

\input{figtext/ranking_changes}

We compare the performance of models in static and interactive evaluation settings.
To measure the effectiveness of different feedback types, we compare against the \vanilla{} and the \baseline{} settings.
The \vanilla{} setting of each dataset evaluates the \cm{} on the original, fully specified questions.
In this setting, the \cm{} is not given any feedback and the first attempt is used as the final output.
To match the test-time compute of the interactive settings, the \baseline{} setting uses five iterations of self-critique to generate feedback using the underspecified question and the output from the previous step ~\citep{madaan2023selfrefineiterativerefinementselffeedback}.
For this setting, no additional information of the original question or solution is given and no \user{} is involved.


\subsection{Performance Metrics}
We use \textbf{test case accuracy (TCA)} to evaluate model performance. In ClassEval, we combine the set of function tests and class tests to measure TCA. To measure the distances between two rankings $\sigma_A$ and $\sigma_B$ of length $n$, we use a normalized variant of \textbf{Spearman's Footrule } ($\tilde{F}$ $:\ \to [0, 1]$):
%
\begin{align*}
    \tilde{F}(\sigma_A, \sigma_B) &= \frac{ \sum^n_{i=1} |\sigma_A(i) - \sigma_B(i)|}{\max_{\sigma, \sigma'}  \sum^n_{i=1} |\sigma(i) - \sigma'(i)|}
\end{align*}
where $\sigma, \sigma'$ are any ranking of length $n$.
For a perfectly correlated pair of rankings, $\tilde{F}=0$; for uncorrelated rankings, $\tilde{F}=0.73$; for perfectly anti-correlated rankings, $\tilde{F}=1$.
Appendix \ref{app:performance_metrics} contains details on how we derive these metrics and thresholds for correlation.


\subsection{Results}


\paragraph{Feedback can recover performance comparable to or even exceeding the \vanilla{} setting.}
Table \ref{tables:perf_per_dataset_setting} shows the performances averaged across all models for each dataset and feedback type.
Comparing \vanilla{} to \baseline{} shows that our input perturbation often obfuscates the problem, as \baseline{}  usually underperforms the \vanilla{} setting, and feedback is often required to achieve performance comparable with the \vanilla{} setting (as with LiveCodeBench).
Moreover, interacting with feedback may also allow \cm s to surpass \vanilla{} performance (as with APPS and ClassEval). 
This may be because the \user{} may supply not only additional specifications about the problem, but also guidance with respect to general problem-solving or programming capabilities.
% (as in the \para, \sent, \cf, and \ir{} settings)


\paragraph{\cf{} and \para{} improve performance the most.}
\cf{} and/or \para{} are consistently the most effective at improving model performance in the underspecified setting (Table \ref{tables:perf_per_dataset_setting}). 
Compared to others, these feedback types tend to be longer and thus may encapsulate more helpful information to the \cm.
The weaker performing feedback types are \sent{} and \ir, the latter of which struggles the most on LiveCodeBench.
This may stem from the fact that LiveCodeBench is not in the training sets of most models (due to its problem cut-off date); for popular datasets,  the \cm{} may sometimes recognize the full question using the \user 's \ir{}, leading to improved performance.

\input{figtext/effect_of_feedback_quality}




\paragraph{Models perform differently in static vs. interactive settings.} 
Figure \ref{fig:ranking_changes} plots the relative rankings (measured by TCA) across static and interactive settings. 
While we generally observe permutations in rankings when comparing \vanilla{} and interactive, LiveCodeBench demonstrates the most variance, with some models changing $4$ ranks between \vanilla{} and interactive settings.

To understand how rank changes vary between static vs. interactive settings, we calculate the normalized Spearman's Footrule distances between the \vanilla{} and interactive settings (Table \ref{tables:normalized-spearmans-footrule-dist}).
All three datasets demonstrate relatively weak positive correlation in many interactive settings; for instance, for \cf, $\tilde{F}$ ranges from 0.222 to 0.346 across the three datasets.
Generally, top models tend to be consistently high across feedback types, whereas weaker models tend to demonstrate more variance in rank. 

\input{tables/feedback_quality_max_step}
