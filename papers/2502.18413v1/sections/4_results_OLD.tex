

\section{Results}
We discuss the overall trends in model behavior when comparing static to interactive settings. \val{mention focus on feedback types}


\input{figtext/edit_distance_vs_steerability_ranking}

\subsection{General trends in model behavior}
Figure \ref{fig:ranking_changes} shows our main results using all \nmodels{} models on the three datasets: APPS, LiveCodeBench, and ClassEval.
The plot shows the relative rankings of each model (measured by their test case accuracy on each dataset) on each of the settings: \vanilla, \para, \sent, \cf, and \ir.

\paragraph{Relative performance between models change between different static and interactive settings.} 
Figure \ref{fig:ranking_changes} shows that the rankings of models often vary depending on which setting is being used to evaluate performance. 
% While \qwenSmall{} performs third in \vanilla{} for LiveCodeBench, it falls to fifth in other feedback settings, and even seventh in \cf. 
% \gemmaLarge{} performs fifth in \vanilla{} and even falls to seventh in \ir, yet jumps to third in \para. \sonnet{} beats \gpt{} in all settings of APPS Interview, except for \cf, where their order inverts.
Tables \jane{ref} shows the normalized Spearman Footrule distances ($\tilde{F})$ between the \vanilla{} and interactive settings. 
It is clear that the choice of evaluation setting has a profound impact on the relative performance of models. 
For ClassEval, $\tilde{F}$ ranges from $0.98$ to $0.67$, indicating anti-correlation between the \vanilla{} and interactive settings.
LiveCodeBench and APPS also demonstrate relatively weak positive correlation on many interactive settings; for instance, for \cf, $\tilde{F}$ is $0.36$ and $0.26$, respectively.
For these two datasets, top models tend to be more consistent, preserving their positions in high positions, whereas weaker models tend to demonstrate more variance in rank.
%\val{i think we should talk about top models being more consistent even with feedback}


% \val{add another paragraph to talk about which feedback types are most helpful}

\paragraph{Dataset difficulty correlates with ranking.}
\jane{Help}

\paragraph{Underspecification hurts model performance, and performance is recoverable only with the incorporation of feedback.}

Figure \ref{fig:vanilla_baseline} plots the best feedback setting for each model, as well as its performance on the \baseline{} and \vanilla{} settings. 
Comparing the blue bars (\vanilla) to the orange bars (\baseline), it is clear that our input perturbation successfully underspecifies the problem, as \baseline{}  consistently underperforms the \vanilla{} setting. 
In order to recover performance comparable with the \vanilla{} setting, the models must interact with the \user{} (as in the \para, \sent, \cf, and \ir{} settings).

\paragraph{\cf{} and \para{} boost performance the most.}


\subsection{Effect of Feedback Quality}
We examine the effects of the feedback type and quality level on performance. \val{provide some intuition for why we look at this}


\paragraph{Feedback quality is consistent across models.}
Table \ref{tab:feedback_quality_max_step_number} compares average feedback quality and the average maximum number of steps.
The feedback quality does not vary greatly across models and often falls above $0.9$.
This suggests that the feedback is high-quality enough to be compared across models on different trajectories of iterative refinement. 
\jane{See HH's comment. Not sure if I captured this sensibly.}

We note that our metric of feedback quality does not correlate with a reduction of the average maximum number of steps ($r=0.48$).
Rather, the maximum number of steps seems to be more related to feedback type, as \cf{} consistently has the the smallest maximum step number, followed by \para{} and \sent.

Surprisingly, when \sonnet{} is the \cm{}, it receives the worst quality feedback compared to the other models, despite being both the \cm{} and the \user{}. 
However, it still completes the solutions the fastest, and consistently has the lowest average maximum step number compared to \gpt{} and \sonnet. 


\paragraph{Code models are robust to \lqf{} feedback.}
Figure~\ref{fig:effect_of_fq} shows the distribution of solutions whose performances improve, do not change, or decrease when comparing \hqf{} feedback to \lqf{} feedback. 
High quality feedback reduces the proportion of feedback that does not have an effect on the solution.
This usually results in a higher rate of solutions whose performances are improved.
\textsc{high-quality} \sent{} feedback usually decreases the rate of worse solutions.
Interestingly, while \cf{} often increases the proportion of improved solutions more than \para{} or \sent, it almost always increases the proportion of worse solutions.




%\paragraph{Average feedback quality varies per feedback type.} 
%\val{this takeaway is weak}


\input{figtext/edit_distance_vs_steerability_heatmap}


\subsection{Effect of Feedback Type and Code Model Steerability}

\val{reframe as how models respond to feedback, rename subsection}
The effects of feedback formats and quality level on surface-level and behavioral-level steerability.
In this section, we discuss how different aspects of feedback affect the \cm 's ability to change its previous version of the solution.

\paragraph{\para{} feedback is associated with higher steerability across all models.} 
Figure \ref{fig:edit_distance_vs_steerability_ranking} ranks each feedback type according to how behavioral (left) or surface-level (right) steerability it induces.
\para{} and \cf{} consistently rank the highest in behavioral steerability, ranking first and second, respectively. 
Likewise, \para{} and \sent{} rank the highest in surface-level steerability. 
This suggests that some feedback types tend to induce similar steerability relative to other feedback types on the same model.


\paragraph{Models react differently to the same forms of feedback, both in terms of behavioral- and surface-level steerability.} 
Figure \ref{fig:edit_distance_vs_steerability_heatmap} shows the effect of feedback type on how \cm s change their previous solutions for \gpt{} on APPS Interview.

Some models (e.g. \gemmaSmall{} or \reka) tend to make large number of surface-level changes that do not greatly affect the behavior of the language model.
Other models (e.g. \gpt{} or \sonnet, and \qwenLarge) demonstrate a bimodal distribution, where some edits highly affect code behavior and others do not.
% While \para{} has demonstrates bimodal 
Appendix \ref{app:extra_figs} shows additional tables and figures.

% Figure \ref{fig:steerability_vs_performance_delta} plots the changes in test cases against the change in performance on a step-by-step basis.
% Green points are code solutions where the last step had an improvement in performance; red points are code solutions where the last step did not have an improvement in performance.

% We note that while the first step has relatively few points with a decrease in performance, the following steps have increasingly high "tails" where changes in test case behavior are attributed to high attrition of performance.
% Moreover, these "tails" are predominantly made up of points where the last step had an improvement in performance.
% Because the interaction is generated in a history-less setting, this cannot be attributed to weaker performance induced by long conversation contexts.
% The tails are also present when history is included in the conversation, meaning that the tails also cannot be attributed to loss of information from lack of history. \redtext{Need to double check this!}
% More details about history and its effect on results can be found in \ref{app:ablations}.


