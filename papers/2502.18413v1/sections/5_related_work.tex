\section{Related Work}

\paragraph{Code benchmarks.}
Static benchmarks, e.g., HumanEval~\citep{chen2021evaluating} and MBPP~\citep{austin2021program}, largely focusing on interview-style programming problems, have been the most commonly used to evaluate coding capabilities~\citep{lu2021codexglue, nijkamp2022codegen,zhu2022xlcost, wang2022recode, liu2023your, jimenez2023swe, khan2023xcodeeval,yan2023codescope, cassano2023multipl, muennighoff2023octopack, dinh2023large,yang2023intercode, du2023classeval}. 
Recent live benchmarks aim to reduce contamination risks~\citep{jain2024livecodebench,white2024livebench}.
Our evaluation pipeline can convert many of these static benchmarks into an interactive one, evaluating model abilities to incorporate different types of feedback; we demonstrate this with $3$ datasets.



\paragraph{Interactive evaluation.}
As programmers are increasingly writing code collaboratively with AI chat assistants like ChatGPT~\cite{chatgpt} or Claude~\cite{claude}, many user studies have evaluated how programmers use chat assistants to write code~\citep{ross2023programmer, chopra2023conversational, kazemitabaar2023studying,xiao2023devgpt,nam2024using,mozannar2024realhumaneval,chidambaram2024socratic}, typically employing only a few models in the study ($\leq 3$).
While new platforms evaluate model coding capabilities at scale by collecting human preferences~\citep{chiang2024chatbot, chi2024copilot}, it remains challenging to understand the fine-grained effects of feedback.
We create a benchmark with \emph{simulated} users to enable scalable evaluation of the nuances of feedback in interactive coding settings, while drawing from insights of existing \emph{human} studies (e.g., common types of feedback~\citep{chidambaram2024socratic}  and tendencies to underspecify inputs~\citep{xiao2023devgpt}).


Prior work has explored interactive benchmarks with simulated users for various applications, such as tool use \citep{yao2024taubenchbenchmarktoolagentuserinteraction}, creative tasks \citep{jia2024simulbenchevaluatinglanguagemodels},  coding \citep{wang2024mintevaluatingllmsmultiturn, shao2025collaborativegymframeworkenabling}, and other collaborative contexts \citep{wu2023autogenenablingnextgenllm}. 
Our benchmark extends them by introducing diverse forms of \user{} feedback and investigating their effect on feedback quality and model steerability. 
We also enforce collaboration between the simulated \user{} and \cm{} via input obfuscation, aligning with real-world use cases where the user's input to the model may be underspecified.

