\newpage
\section{Appendix}
\label{sec:appendix}



\subsection{Additional Details on Datasets}
\label{app:datasets}
Our pipeline is designed to accommodate generic static benchmarks with some modifications. 
For instance, because ClassEval requires the model to fill in the skeleton code of a class (rather than providing explicit programming questions), we underspecify its problems by summarizing the docstrings for each method.
Likewise, LiveCodeBench does not provide ground-truth solutions, so we generate solutions for LiveCodeBench by sampling twice from \sonnet{}; if a correct solution is generated, we use it as the ground-truth solution for the question. 
(If not, we do not use the question as our pipeline requires the presence of a ground-truth solution in the \user{} prompt.)  

For evaluation of APPS and LiveCodeBench, Table \ref{tables:perf_per_dataset_setting}, Table \ref{tab:feedback_quality_max_step_number}, and Figure \ref{fig:ranking_changes} average across difficulty levels for brevity.

\subsection{Example Feedback}
\label{app:feedback}
\input{tables/feedback_examples}

We provide sample feedback from \sonnet{} in response to a proposed solution. All feedback types are in response to the same question (Table \ref{tables:sample-feedback}).


\subsection{Additional Details on Models}\label{app:models}

We obtained the weights for google/gemma-7b-it (\gemmaSmall{}) from Hugging Face at \url{https://huggingface.co/google/gemma-7b-it}, meta-llama/Meta-Llama-3.1-8B-Instruct (\llama{}) from huggingface at \url{https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct}, and Qwen/Qwen2.5-Coder-7b-Instruct (\qwenSmall{}) from Hugging Face at \url{https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct}. 
We run each of the models on a single L40S GPU. We use a temperature setting of $0.9$, $4096$ max tokens, and the ``do\_sample'' setting enabled. 
We use $\langle$end\_of\_turn$\rangle$, $\langle$|eot\_id|$\rangle$, $\langle$|im\_end|$\rangle$ as the EOS token for \gemmaSmall{}, \llama{}, and \qwenSmall{} respectively.

We use Together AI (\url{https://api.together.xyz/}) to run the large open-weight models google/gemma-2-27b-it (\gemmaLarge{}) and Qwen/Qwen2.5-Coder-32B-Instruct (\qwenLarge{}). 
We set ``n\_sample'' to $1$ when generating code solutions and limit the max number of tokens to $4096$.
The weights for these models can be found at \url{https://huggingface.co/google/gemma-2-27b-it} and \url{https://huggingface.co/Qwen/Qwen2.5-Coder-32B-Instruct} respectively.

We access \texttt{c4ai-aya-expanse-32b} (\aya{}) through the Cohere API at \url{https://cohere.com/research/aya}, reka-core-20240501 (\reka{}) through the Reka API at \url{https://www.reka.ai/reka-api}, and deepseek-chat (\deepseek{}) through the Deepseek API at \url{https://api-docs.deepseek.com/}.  
We use the default API settings for inference, limiting the max number of tokens to $4096$.

\gpt{} inference is done through the OpenAI API \url{https://platform.openai.com/docs/overview} and \sonnet{} inference through the Anthropic API \url{https://www.anthropic.com/api}.
We use the default API settings for inference, limiting the max number of tokens to $4096$.


\subsection{Prompts}\label{app:prompts}
We use \sonnet{} to generate \user{} feedback for out experiments, but we show that other models can produce comparable feedback (Figure \ref{fig:4o_user_w_sonnet_apps})

We provide all of the prompts for the \user{} model and \cm{}. 
All \user{} model prompts were provided with the same system prompt with the original question and code solution (Figure \ref{prompts:user-sys-prompt}). 
The \para{} prompt (Figure \ref{prompts:user-para-prompt}) and \sent{} prompt (Figure \ref{prompts:user-sent-prompt}) are given the current \cm{} solution and generate feedback constrained by output length. 
The \cf{} prompt is given the current \cm{} solution and provides a correction to specific lines of code in the solution (Figure \ref{prompts:user-code-prompt}). 
The \ir{} feedback prompt is given the current \cm{} solution and underspecified question and generates an updated version of the question with missing details (Figure \ref{prompts:user-input-refine-prompt}).

The \cm{} prompts for APPS and LiveCodeBench are given in Figure \ref{prompts:asst-vanilla_apps_lcb} and Figure \ref{prompts:asst-nl_para_sent_apps_lcb}.
The \cm{} prompts for ClassEval is given in Figure \ref{prompts:asst-classeval_vanilla} and Figure \ref{prompts:asst-nl_para_sent_classeval}.

The 11-shot prompt used to summarize APPS and LiveCodeBench questions is given in Figure \ref{prompts:summarization_apps_lcb}.
The prompt used to summarize each docstring in ClassEval is given in Figure \ref{prompts:summarization_classeval}.


\input{figtext/4o_user_plot}
\input{prompts/user_system}
\input{prompts/user_para}
\input{prompts/user_sent}
\input{prompts/user_code}
\input{prompts/user_input_refine}
\input{prompts/asst_vanilla_apps_lcb}
\input{prompts/asst_nl_para_sent_apps}
\input{prompts/asst_vanilla_classeval}
\input{prompts/asst_nl_para_sent_classeval}
\input{prompts/summarization_apps_lcb}
\input{prompts/summarization_classeval}

\subsection{Performance Metrics}
\label{app:performance_metrics}
\paragraph{Test case accuracy.}
Test case accuracy can be defined as below:
%
\begin{equation*}
    \textsc{TCA} = \frac{\text{\# Test Cases Passed}}{\text{Total \# Test Cases}}
\end{equation*}
\paragraph{Normalized Spearman's Footrule distance.}

The normalized Spearman's Footrule distance is:
%
\begin{align*}
    \tilde{F}(\sigma_A, \sigma_B) &= \frac{ \sum^n_{i=1} |\sigma_A(i) - \sigma_B(i)|}{\max_{\sigma, \sigma'}  \sum^n_{i=1} |\sigma(i) - \sigma'(i)|}
\end{align*}
%
Consider two rankings $\sigma_A$ and $\sigma_B$ over items $\{1, 2, \dots, n\}$. To measure the distance between them, we use Spearman's Footrule Distance, which can be thought of as the Manhattan distance between two rankings:

\begin{equation*}
    F(\sigma_A, \sigma_B) = \sum^n_{i=1} |\sigma_A(i) - \sigma_B(i)|
\end{equation*}

We normalize $F$ by its maximum possible value, $\frac{n^2}{2}$ for even $n$, to get the \textbf{Normalized Spearman's Footrule Distance}. 
%
\begin{align*}
    \tilde{F}(\sigma_A, \sigma_B) &= \frac{F(\sigma_A, \sigma_B)}{\frac{n^2}{2}} \\
    &= \frac{2F(\sigma_A, \sigma_B)}{n^2}
\end{align*}
%
where $\tilde{F}: \to [0, 1]$. In other words, $\tilde{F}=0$ indicates $\sigma_1 = \sigma_2$, whereas $\tilde{F}=1$ indicates the maximum possible distance between $\sigma_1$ and $\sigma_2$.

Now, we would like to derive the expected $\tilde{F}$ between two rankings which are completely uncorrelated. 
Let us randomly sample $\sigma_A, \sigma_B$ uniformly at random. Then the expected Spearman's Footrule Distance ($F$) is:
%
\begin{align*}
\mathbb{E}[F(\sigma_A, \sigma_B)] &= \mathbb{E}[\sum^n_{i=1} |\sigma_A(i) - \sigma_B(i)|] \\
&= \sum^n_{i=1} \mathbb{E}[|\sigma_A(i) - \sigma_B(i)|] \\
&=  \sum^n_{i=1} \frac{n+1}{3} \\
&=  \frac{n(n+1)}{3} 
\end{align*}
Normalizing this by the maximum possible $F$ gives:
\begin{align*}
    \frac{\frac{n(n+1)}{3}}{\frac{n^2}{2}} &= \frac{2(n+1)}{3n} 
\end{align*}
Thus, for uncorrelated rankings of length \nmodels, $\tilde{F}\simeq 0.73$; for a perfectly correlated pair of rankings, $\tilde{F}=0$; and for perfectly anti-correlated rankings, $\tilde{F}=1$.

\subsection{Additional Details on Measuring Feedback Quality}
\label{app:fq}


\paragraph{Automatic classification of directional correctness.} We use \gpt{} to classify the feedback into two classes: (1) the feedback claimed that the solution was correct or (2) the feedback claimed that the solution was incorrect. 
As some feedback claims that the ``logic'' of the solution is correct, but then states that it is missing critical edge cases or input/output formatting, we also apply rule-based string matching to re-classify such feedback as incorrect.
We then compare the feedback to the actual \textsc{TCA} performance to classify it into \hqf{} vs. \lqf{} feedback.

\paragraph{Other metrics of feedback quality.} We considered two other metrics of feedback quality. 
First, we attempted to consider the increase in probability over either the ground-truth solution or the full question, comparing the \cm 's solution with and without feedback. 
However, we found that this measure was too noisy to impart any meaningful value. 

We also attempted to prompt \gpt{} to classify feedback relevance (to either the ground-truth solution or full question) on a scale of 1-5. 
However, this measure was also noisy, not to mention hard to define in the prompt, as even humans would struggle to distinguish between, for example, a "2" vs. a "3" in relevance.








\subsection{Performance Tables for Static vs. Interactive Settings}
\label{app:full_results}
\input{tables/full_results}
In this section, we provide tables for the performance of models across static and interactive settings, including all feedback types and baselines.
Table \ref{tables:apps-interview-full} gives the TCA of APPS (Interview), Table \ref{tables:apps-intro-full} gives the TCA of APPS (Introductory). 
Table \ref{tables:livecodebench-full} gives the TCA of LiveCodeBench and Table \ref{tables:classeval-full}.

\subsection{Additional Tables}
All additional tables -- including information about feedback quality by dataset, steerability metrics, and ranking distance metrics --- can be found in this section. 
\label{app:average_feedback_quality_per_step}

Table \ref{tables:apps-avg-feedback-quality}, Table \ref{tables:livecodebench-avg-feedback-quality}, Table \ref{tables:classeval-avg-feedback-quality} have the average directional correctness of each setting and the number of steps it takes to reach a solution with 100\% TCA. We partition the analysis by model and by feedback setting.

Table \ref{tables:steerability-edit-distance} measures the average number of edits made by each model for each feedback. 
Table \ref{tables:steerability-test-case} measures the average number of test cases flipped by each feedback setting.

Tables \ref{tables:normalized-spearmans-footrule-dist} gives the normalized Spearman's Footrule distance of each setting's ranking compared to the \vanilla{} setting.

\input{tables/average_feedback_quality_per_step}
\input{tables/feedback_vs_steerability}
\input{tables/ranking_spearmans_footrule}
