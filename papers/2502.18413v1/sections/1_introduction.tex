

\section{Introduction}


\input{figtext/main_figure}




Programming with a language model is a highly collaborative process, where developers interact with \cm s to provide updated information about initially underspecified requests or critique the output of the \cm.
Thus, giving and receiving feedback are critical elements of the process in which programmers use \cm{}s~\cite{chidambaram2024socratic}. 
For example, chat interfaces like ChatGPT~\citep{chatgpt} or the chat panel of Github Copilot~\cite{copilot} facilitate multi-turn conversations in which programmers can iteratively refine a piece of code by providing additional context and details to the LLM ~\citep{kalla2023study,xiao2023devgpt}.

Despite the popularity of these tools, existing static benchmarks that measure task performance often rely on a simple input-output configuration, where the question is well defined and the model is asked to generate the whole completion in one shot~\citep{chen2021evaluating,austin2021program,jain2024livecodebench, white2024livebench}.
While this relatively simplistic setting is scalable and enables efficient evaluation, it does not capture how developers realistically use models to write code~\citep{mozannar2024realhumaneval}. 
While recent benchmarks have begun to explore how interactive settings can lead to performance gains in coding applications~\cite{wang2023mint}, they assume a single form of natural language feedback. 
In practice, developers provide many forms of feedback when implementing code~\citep{chidambaram2024socratic}, which can range from binary feedback on the correctness of the code to suggesting direct changes to the code. 



Building evaluations of programming assistants that more closely mimic this setting enables a better understanding of model behavior and potential pitfalls in the interactive setting, such as model capability with respect to processing feedback, the effects of different feedback types, or model robustness. 
We bridge the gap between existing evaluations and real-world use cases by benchmarking how different feedback types impact model behavior in a simulated interactive setting (Figure~\ref{fig:teaser}, top). 
We propose an evaluation method that transforms a static coding benchmark into an interactive, collaborative one (Figure~\ref{fig:main_figure}). 
The pipeline components include input obfuscation to create underspecified problems to induce collaboration, a simulated user for scalability, and multiple types of feedback (\cf, \ir, \para, and \sent).




Across \nmodels{} coding models ($6$ open and $4$ closed) and $3$ coding benchmarks, we find that relative performance between models often changes between static and interactive settings (Figure~\ref{fig:teaser}, bottom), suggesting that models perform differently in the static vs. interactive settings.
Beyond performance-based metrics, we analyze important components of model-user interactions, including feedback quality and code model steerability.
We use these insights to investigate the effects of different feedback types.
For example, we find that \para{} and \cf{} tend to lead to the highest performance boost compared to other feedback types (e.g., \sent{} or \ir). 
However, when considering the effect of feedback quality on performance, we find that unlike \para, \cf 's higher-quality feedback makes output worse more frequently than its lower-quality counterpart for stronger models.
Furthermore, \para{} leads to more surface-level edits than \cf, whereas users may prefer variation in model behavior to be robust to changes in feedback type.



Our work provides a new approach to investigating the downstream ramifications of different interactive programming settings and their effects on model behavior.
We open-source our evaluation pipeline\footnote{Our code is publicly available at \url{https://github.com/janepan9917/WhenBenchmarksTalk/}.}, which makes it easy to add static benchmarks and turn them into interactive ones, to facilitate the evaluation of more models and datasets.
