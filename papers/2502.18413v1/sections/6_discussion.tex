\section{Limitations}

In this work, the focus of our experiments is on three diverse code benchmarks to demonstrate the generality of our pipeline. 
However, given the expansive set of static benchmarks, our results may not encompass the full set of observations one might obtain from considering more varied datasets (e.g., non-Python coding questions).
On the evaluation front, since we do not explicitly compare LLM responses to human-generated feedback on the extensive set of modified questions, we focus on trends of model behavior as a response to different feedback types, rather than specific degrees of change.

Additionally, the feedback types studied in this work are not fully comprehensive, and we do not claim that the user model's feedback is necessarily representative of actual human users.
Rather, we use a simulated \user{} to scalably examine how LLMs react to feedback in a collaborative setting, not to realistically imitate how expert humans use LLMs to program.
For instance, users may mix feedback types within a single interaction, whereas we fix the feedback type across rounds of iterative refinement to isolate the effects of an individual feedback type.
Recent works also study whether LLMs can proactively seek user feedback via clarification questions or other interactions \cite{zhang2023clarifynecessaryresolvingambiguity,zhang2024modelingfutureconversationturns}, whereas we only consider settings where the user initiates the feedback-giving process.
This work was not intended to exhaustively test feedback types but to highlight a new approach to understanding the downstream effects of interactive programming settings.


\subsection{Potential Risks}
Our pipeline is intended to be used as a testbed to examine model behavior in response to feedback, rather than to realistically mimic actual human usage with models. 
As such, it should not be used as an approximation of actual human users.
Other risks include overexposure to certain programming languages and natural languages, as we only include Python programming questions and English feedback.

\section{Acknowledgements}
We thank the members of the NYU ML\textsuperscript{2} group for their valuable advice, thoughts, and discussions. We also thank Vishakh Padmakumar, Austin Wang, Jens Tuyls, and Naman Jain for their feedback and comments. This project has benefited from financial support from Eric and Wendy Schmidt (made by recommendation of the Schmidt Futures program) and Open Philanthropy, and from in-kind support by the NYU High-Performance Computing Center. This material is also partially supported by the National Science Foundation under Award IIS-2340345. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation. 


% \subsection{Future Work}
% \val{we can cut this if we need space}
% \greytext{Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.}


% \begin{itemize}
%     \item Focus on NL feedback
%     \item Realisticness of human simulation
% \end{itemize}