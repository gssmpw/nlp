

\begin{table*}[t]
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{ccccccc}
\toprule 
\textbf{Dataset} & \textbf{\vanilla} & \textbf{\baseline} & \textbf{\para} & \textbf{\sent} & \textbf{\cf} & \textbf{\ir} \\ \hline
APPS & 0.335 (0.003) & 0.034 (0.001) & 0.381 (0.004) & 0.271 (0.003) & 0.428 (0.004) & 0.289 (0.003) \\
LCB & 0.699 (0.008) & 0.274 (0.008) & 0.655 (0.009) & 0.611 (0.009) & 0.631 (0.009) & 0.183 (0.008) \\
ClassEval &  0.714 (0.002) &  0.483 (0.006) & 0.679 (0.006) & 0.642 (0.006)& 0.759 (0.006) & 0.693 (0.005)\\
 \bottomrule
\end{tabular}
}
\caption{
    Test case accuracy and standard error of each setting in APPS, LiveCodeBench (LCB), and ClassEval, averaged across all \cm s. We find that feedback can recover performance comparable to or even exceeding the \vanilla{} setting.
    % \val{replace classeval numbers}
    }
    \label{tables:perf_per_dataset_setting}
\end{table*}

% baseline_w_ir_self_critique_no_history  0.525183  0.006554
% ir_code_feedback_no_history             0.531797  0.006294
% ir_input_refinement_no_history          0.496534  0.006017
% ir_nl_feedback_paragraph_no_history     0.499261  0.006212
% ir_nl_feedback_sentence_no_history      0.565271  0.006224
% vanilla 0.4091089821466535 0.4091

