
\begin{table*}
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{ccccccc}
\toprule 
\textbf{Model}  & \textbf{\vanilla} & \textbf{\baseline} & \textbf{\sent} & \textbf{\para} & \textbf{\cf} & \textbf{\ir} \\ \hline
\gpt & 0.498 (0.016) & 0.068 (0.007) & 0.422 (0.016) & 0.544 (0.016) & \textbf{0.598 (0.016) }& 0.488 (0.016)\\
\aya &\textbf{ 0.261 (0.014)} & 0.009 (0.002) & 0.131 (0.011) & 0.259 (0.015) & 0.235 (0.014) & 0.214 (0.013)\\
\deepseek & \textbf{0.616 (0.015)} & 0.048 (0.007) & 0.449 (0.016) & 0.512 (0.023) & 0.521 (0.026) & 0.442 (0.019)\\
\gemmaLarge & 0.409 (0.013) & 0.007 (0.002) & 0.351 (0.016) & \textbf{0.556 (0.016)} & 0.51 (0.017) & 0.403 (0.015)\\
\gemmaSmall & 0.177 (0.01) & 0.009 (0.002) & 0.039 (0.006) & 0.084 (0.009) &\textbf{ 0.299 (0.016)} & 0.029 (0.004)\\
\llama & 0.253 (0.012) & 0.025 (0.003) & 0.236 (0.014) & 0.402 (0.015) & \textbf{0.453 (0.016)} & 0.215 (0.012)\\
\qwenSmall & 0.369 (0.014) & 0.026 (0.004) & 0.283 (0.014) & 0.423 (0.016) & \textbf{0.495 (0.016)} & 0.336 (0.015)\\
\qwenLarge & 0.542 (0.015) & 0.039 (0.004) & 0.5 (0.016) & 0.582 (0.015) & \textbf{0.605 (0.015)} & 0.503 (0.015)\\
\reka & 0.164 (0.011) & 0.018 (0.003) & 0.224 (0.013) & 0.303 (0.015) & \textbf{0.4 (0.016)} & 0.157 (0.011)\\
\sonnet & 0.59 (0.014) & 0.114 (0.009) & 0.571 (0.015) & \textbf{0.654 (0.014)} & 0.627 (0.015) & 0.62 (0.014)\\   
\bottomrule
\end{tabular}
}
\caption{Average TCA of each model with standard error on APPS Interview questions. 
% \cf{} and \para{} feedback tends to improve performance the most, raising TCA beyond \vanilla{} performance in some models.
}
\label{tables:apps-interview-full}
\end{table*}

\begin{table*}
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{ccccccc}
\toprule 
\textbf{Model}  & \textbf{\vanilla} & \textbf{\baseline} & \textbf{\sent} & \textbf{\para} & \textbf{\cf} & \textbf{\ir} \\ \hline
\gpt & 0.412 (0.015) & 0.071 (0.007) & 0.392 (0.016) & 0.512 (0.016) & \textbf{0.539 (0.016)} & 0.409 (0.016)\\
\aya & 0.182 (0.008) & 0.004 (0.001) & 0.069 (0.006) & \textbf{0.179 (0.009)} & 0.177 (0.009) & 0.142 (0.007)\\
\deepseek{} & - & - & - & - & - & - \\
\gemmaLarge & 0.322 (0.012) & 0.018 (0.003) & 0.228 (0.013) & \textbf{0.454 (0.016)} & 0.41 (0.016) & 0.299 (0.013)\\
\gemmaSmall & 0.131 (0.008) & 0.013 (0.003) & 0.023 (0.004) & 0.045 (0.006) & \textbf{0.213 (0.014)} & 0.014 (0.003)\\
\llama & 0.205 (0.01) & 0.028 (0.004) & 0.12 (0.009) & 0.292 (0.014) & \textbf{0.402 (0.015)} & 0.153 (0.01)\\
\qwenSmall & 0.28 (0.012) & 0.034 (0.004) & 0.186 (0.012) & 0.27 (0.014) & \textbf{0.392 (0.016)} & 0.2 (0.011)\\
\qwenLarge & 0.348 (0.024) & 0.038 (0.007) & 0.376 (0.023) & 0.479 (0.025) & \textbf{0.486 (0.025) }& 0.37 (0.023)\\
\reka & 0.124 (0.009) & 0.018 (0.003) & 0.141 (0.01) & 0.275 (0.014) & \textbf{0.34 (0.015)} & 0.11 (0.009)\\
\sonnet & 0.497 (0.014) & 0.073 (0.007) & 0.468 (0.015) & \textbf{0.556 (0.015)} & 0.53 (0.015) & 0.492 (0.015)\\
\bottomrule
\end{tabular}
}
\caption{Average TCA of each model with standard error in each setting for APPS introductory. 
% \cf{} and \para{} provide the highest performance boost, improving TCA beyond \vanilla{} in some models. 
\deepseek{} is missing for this setting due to rate limits on the API that impeded evaluation. }
\label{tables:apps-intro-full}
\end{table*}

\begin{table*}
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{ccccccc}
\toprule 
\textbf{Model}  & \textbf{\vanilla} & \textbf{\baseline }& \textbf{\sent} & \textbf{\para }& \textbf{\cf} & \textbf{\ir} \\ \hline
\gpt & \textbf{0.8 (0.023)} & 0.323 (0.026) & 0.767 (0.024) & 0.745 (0.024) & 0.702 (0.026) & 0.23 (0.025)\\
\aya & 0.522 (0.027) & 0.235 (0.022) & 0.482 (0.027) & \textbf{0.632 (0.026)} & 0.483 (0.028) & 0.134 (0.02)\\
\deepseek & \textbf{0.944 (0.013)} & 0.332 (0.026) & 0.841 (0.02) & 0.849 (0.019) & 0.756 (0.024) & 0.345 (0.028)\\
\gemmaLarge & \textbf{0.766 (0.021)} & 0.282 (0.025) & 0.67 (0.026) & \textbf{0.766 (0.023)} & 0.62 (0.026) & 0.119 (0.019)\\
\gemmaSmall & 0.347 (0.023) & 0.173 (0.019) & 0.196 (0.021) & 0.285 (0.024) & \textbf{0.524 (0.027)} & 0.028 (0.008)\\
\llama & 0.587 (0.025) & 0.237 (0.022) & 0.538 (0.027) & 0.588 (0.026) & \textbf{0.647 (0.026) }& 0.203 (0.023)\\
\qwenSmall & \textbf{0.755 (0.021)} & 0.27 (0.024) & 0.621 (0.027) & 0.678 (0.026) & 0.629 (0.027) & 0.189 (0.023)\\
\qwenLarge &\textbf{ 0.961 (0.007)} & 0.309 (0.025) & 0.809 (0.021) & 0.747 (0.024) & 0.712 (0.025) & 0.237 (0.025)\\
\reka & 0.617 (0.025) & 0.246 (0.023) & 0.583 (0.026) & \textbf{0.636 (0.026)} & 0.629 (0.027) & 0.111 (0.018)\\
\sonnet & \textbf{0.937 (0.012)} & 0.387 (0.026) & 0.832 (0.02) & 0.821 (0.02) & 0.735 (0.025) & 0.395 (0.029)\\   
\bottomrule
\end{tabular}
}
\caption{Average TCA of each model with standard error on a subset LiveCodeBench questions. To provide code solutions to the \user{} model, we select questions which \sonnet{} solves perfectly within two attempts, using the generated solution as ground truth. 
% In the models where the \vanilla{} setting yields the highest TCA with \para{} and \cf{} providing the largest gains in the iterative refinement loop. The gap between \sent{} and \para{} feedback is closer compared to APPS with \sent{} outperforming \para{} in a few models. 
}
\label{tables:livecodebench-full}
\end{table*}

\begin{table*}
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{ccccccc}
\toprule 
\textbf{Model}  & \textbf{\vanilla} & \textbf{\baseline} & \textbf{\sent} & \textbf{\para} & \textbf{\cf} & \textbf{\ir} \\ \hline
\gpt & 0.839 (0.005) & 0.561 (0.018) & 0.836 (0.014) & 0.848 (0.014) & \textbf{0.895 (0.011)} & 0.789 (0.014) \\
\aya & 0.718 (0.007)  & 0.305 (0.019) & 0.596 (0.021)  & 0.597 (0.022) &\textbf{0.781 (0.018)} & 0.675 (0.016)  \\
\deepseek & 0.849 (0.005) & 0.559 (0.018) & 0.837 (0.013) & 0.867 (0.012) & \textbf{0.889 (0.012)} & 0.806 (0.013) \\
\gemmaLarge & 0.704 (0.008) & 0.563 (0.018) & 0.776 (0.015) & 0.815 (0.015) & \textbf{0.867 (0.014)} & 0.735 (0.015)  \\
\gemmaSmall & 0.350 (0.008) & 0.303 (0.017) & 0.375 (0.018) & 0.390 (0.018) &\textbf{ 0.687 (0.019) } & 0.340 (0.017)  \\
\llama & 0.636 (0.007) & 0.443 (0.019) & 0.653 (0.019) & 0.710 (0.019) &\textbf{ 0.773 (0.017) }& 0.701 (0.018)  \\
\qwenSmall & 0.714 (0.006) & 0.502 (0.019) & 0.605 (0.018) & 0.757 (0.017) & \textbf{0.819 (0.014)} & 0.697 (0.016) \\
\qwenLarge & 0.816 (0.006) & 0.542 (0.018) & 0.815 (0.014) & 0.832 (0.015) & \textbf{0.876 (0.012)} & 0.778 (0.014) \\
\reka & 0.670 (0.007) & 0.471 (0.027) & 0.096 (0.015) & 0.109 (0.016) & 0.112 (0.016) & \textbf{0.600 (0.019)} \\
\sonnet & 0.833 (0.006) & 0.564 (0.019) & 0.821 (0.014) & 0.865 (0.013) & \textbf{0.881 (0.013)}  & 0.803 (0.014) \\

% \gpt & \textbf{0.842 (0.014) }& 0.687 (0.021) & 0.682 (0.021) & 0.697 (0.021) & 0.691 (0.022) & 0.643 (0.019)\\
% \aya & \textbf{0.696 (0.016)} & 0.515 (0.017) & 0.523 (0.017) & 0.521 (0.017) & 0.524 (0.017) & 0.518 (0.017)\\
% \gemmaLarge & \textbf{0.702 (0.021)} & 0.588 (0.023) & 0.556 (0.02) & 0.595 (0.023) & 0.592 (0.023) & 0.581 (0.019)\\
% \gemmaSmall & \textbf{0.339 (0.018)} & 0.29 (0.015) & 0.291 (0.015) & 0.278 (0.015) & 0.301 (0.015) & 0.287 (0.015)\\
% \llama & \textbf{0.604 (0.018)} & 0.489 (0.02) & 0.518 (0.017) & 0.515 (0.017) & 0.505 (0.017) & 0.521 (0.017)\\
% \qwenSmall & \textbf{0.691 (0.016)} & 0.551 (0.02) & 0.562 (0.018) & 0.534 (0.017) & 0.546 (0.017) & 0.554 (0.016)\\
% \qwenLarge & \textbf{0.807 (0.016)} & 0.67 (0.023) & 0.622 (0.019) & 0.676 (0.022) & 0.675 (0.023) & 0.632 (0.019)\\
% \reka & \textbf{0.648 (0.016)} & 0.477 (0.029) & 0.484 (0.016) & 0.49 (0.016) & 0.494 (0.016) & 0.501 (0.016)\\
% \sonnet &\textbf{ 0.835 (0.015)} & 0.695 (0.022) & 0.702 (0.022) & 0.698 (0.023) & 0.698 (0.022) & 0.66 (0.019)\\
\bottomrule
\end{tabular}
}
\caption{Average TCA with standard error in each setting for ClassEval.}
\label{tables:classeval-full}
\end{table*}