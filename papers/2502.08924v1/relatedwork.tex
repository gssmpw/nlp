\section{Related work}
Training data is a crucial component in building high-performing LLMs. %
Human generated data has limitations such as scalability, biases, errors, and potential privacy considerations  \citep{kurakin2023harnessing,singh2024beyond,gilardi2023chatgpt,long-etal-2024-llms}. \citet{longpre2024consent} highlights a challenge: as LLMs scale, the demand for high-quality data increases, yet access to such data becomes more restricted due to copyright and privacy constraints. Given these challenges, integrating synthetic data into training pipelines is essential, but comes with its own set of risks. %

\paragraph{Model Collapse.} Several studies highlight a critical concern regarding the use of synthetic data in training LLMs, known as model collapse. This phenomenon is caused by \textit{improper} use of synthetic data in training the model, which can cause performance degradation or even complete failure of the model. \citet{alemohammad2024selfconsuming,shumailov2024ai,Hataya_2023_ICCV,gerstgrasser2024is} have empirically studied model collapse in various settings, demonstrating the detrimental effects of iterative training on only synthetic data and highlighting how this process can severely degrade model performance.

\citet{dohmatob2024model,pmlr-v235-dohmatob24b,bertrand2024on,dey2024universality,seddik2024how} study model collapse theoretically. Their results show recursively retraining only on synthetic data causes performance degradation in different models. However combining synthetic and labeled training data \citep{bertrand2024on,dey2024universality,seddik2024how} can mitigate this performance degradation. In contrast to our work, they do not demonstrate continuous improvement toward an optimal model.  

Recently, \citet{suresh2024rate} and \citet{feng2024beyond} provide theoretical explanations for model collapse under restricted models, including Gaussian mixture models and linear classifiers. Our results do not assume a specific learning class, instead relying on a black-box strong learning assumption. 


\paragraph{Self-improving LLMs.} Self-evolving or self-improving LLMs \citep{tao2024survey} is a new research direction that leverages the model itself to generate or guide the creation of high-quality data \citep{wang-etal-2023-self-instruct,huang-etal-2023-large}, which can then be used for fine-tuning, enabling continuous improvement with minimum or no external intervention \citep{pmlr-v235-yuan24d,pmlr-v235-chen24j}.

STaR \citep{zelikman2022star} presents a bootstrapping mechanism to enhance the reasoning capabilities of LLMs by iteratively asking the model to generate step-by-step ``chain-of-thought'' rationales for questions, filtering out the incorrect answer and fine-tuning the original model on all correct rationales and repeat the process. ReST \citep{gulcehre2023rest} proposes a combination of self-generated data and offline reinforcement learning. The method operates iteratively in two primary phases: a ``Grow'' phase, where for each input (context), the LLM generates multiple outputs to expand the training dataset, and an ``Improve'' phase, which involves ranking and filtering this augmented dataset using a learned reward model trained on human preferences. $\text{ReST}^\text{EM}$ \citep{singh2024beyond} is a modified version of ReST with two main differences which help them to improve the performance; they do not augment the generated data with human-generated data, and in the ``Improve'' step instead of fine-tuning the model in the previous iteration, they fine-tune the base model. All of the above methods can be modeled in our framework, and thus we provide a better theoretical understanding about why and when such methods can work. %