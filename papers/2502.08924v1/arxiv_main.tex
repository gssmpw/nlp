\pdfoutput=1
\PassOptionsToPackage{dvipsnames}{xcolor} %
\documentclass[11pt]{article}

\usepackage{fullpage}
\usepackage{natbib}
\usepackage[colorlinks=true, linkcolor=blue, urlcolor=blue, citecolor=blue]{hyperref}
\bibliographystyle{unsrtnat}
\usepackage{amsmath,amsthm,amssymb}
\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{microtype}
\usepackage{subcaption}
\usepackage{longtable}
\usepackage{booktabs} %
\usepackage{algorithm, algcompatible}
\usepackage{algpseudocode}
\usepackage{color}
\usepackage{multirow}
\usepackage[most]{tcolorbox}
\usepackage[dvipsnames]{xcolor}
\usepackage{verbatim}
\usepackage{enumitem}
\usepackage[auth-lg]{authblk}
\definecolor{lightgray}{gray}{0.9}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\input{commands}
\newcommand{\ka}[1]{\textcolor{blue}{ka: #1}}
\renewcommand{\Authsep}{ ~~~ }
\renewcommand{\Authands}{ ~~~ }


\title{Escaping Collapse: The Strength of Weak Data\\for Large Language Model Training\thanks{Authors ordered alphabetically. Author contributions are listed at the end.}}
\author[1]{Kareem Amin}
\author[2]{Sara Babakniya}
\author[1]{Alex Bie}
\author[1]{\authorcr Weiwei Kong}
\author[1]{Umar Syed}
\author[1]{Sergei Vassilvitskii}
\affil[1]{Google Research, New York}
\affil[2]{University of Southern California, Los Angeles}
\date{}
\begin{document}

\maketitle

\begin{abstract}
Synthetically-generated data plays an increasingly larger role in training large language models. %
However, while synthetic data has been found to be useful, studies have also shown that without proper curation it can cause LLM performance to plateau, or even “collapse”, after many training iterations.  In this paper, we formalize this question and develop a theoretical framework to investigate how much curation is needed in order to ensure that LLM performance continually improves. We find that the requirements are nearly minimal. We describe a training procedure that converges to an optimal LLM even if almost all of the non-synthetic training data is of poor quality. Our analysis is inspired by boosting, a classic machine learning technique that leverages a very weak learning algorithm to produce an arbitrarily good classifier. Our training procedure subsumes many recently proposed methods for training LLMs on synthetic data, and thus our analysis sheds light on why they are successful, and also suggests opportunities for future improvement. We present experiments that validate our theory, and show that dynamically focusing labeling resources on the most challenging examples --- in much the same way that boosting focuses the efforts of the weak learner --- leads to improved performance.
\end{abstract}

\section{Introduction}

Large Language Models (LLMs) represent the frontier of artificial intelligence, and are trained on vast amounts of human-generated data. However, much of the high-quality publicly available data on the Internet has been exhausted, and limits on generating new tokens threaten to slow progress on LLM training. 

As a consequence, synthetically-generated datasets are playing an important role in the training of LLMs. Synthetic data have been shown to improve the performance of real large models on a range of tasks~\citep{bai2022constitutional, zelikman2022star, gulcehre2023rest, singh2024beyond}.
On the other hand, the circuitous nature of training new LLMs on data generated by previous generations of LLMs has caused concerns of model collapse \citep{shumailov2024ai,alemohammad2024selfconsuming}. %
Since publicly available sources contain an increasingly large proportion of machine-generated content, synthetic data will be used for training, deliberately or inadvertently.  %

What makes synthetic data beneficial or harmful? 
The answer depends on the precise elements of the synthetic data recipe, and  %
an important contribution of the present paper is to propose a theoretical framework that unifies existing elements of synthetic data approaches, facilitating reasoning about when they might succeed or fail.


Basic learning theory and empirical studies suggest that a necessary condition for avoiding model collapse is that synthetic data is curated in some way to inject signal that is exogenous to the system that produced the original data. This can come in many forms: identification of high-quality subsets of synthetic data, human rewrites of poor responses, a separate model rating the responses, \emph{etc}. A key question is how much curation is \emph{sufficient} to not only avoid collapse, but also to converge to an optimal LLM? Our answer, which we will make precise, is the minimum amount. 

Specifically, we analyze a simple procedure for improving an LLM, in which we iteratively (1) generate synthetic responses from the model; (2) obtain additional responses from an exogenous source; and (3) train the next generation of the model with both types of responses. 
This procedure captures successful approaches for training LLMs on synthetic data \citep{zelikman2022star, gulcehre2023rest, singh2024beyond}, and so our analysis sheds light on how they work. More broadly, it models the ad hoc processes employed by model developers. We show that if at least a $\beta > 0$ fraction of the non-synthetic responses (\emph{i.e.}, the ones produced by an external signal) are correct, then the iterative procedure converges to an optimal LLM (\emph{i.e.}, one that returns a correct response to each prompt). See Theorem \ref{thm:main} for the precise statement and exact convergence rate.  

\paragraph{Connection to Boosting.} At a high level, our analysis shows how to use synthetic data to focus curation on regions of the prompt space where all of the previous LLMs in the sequence performed poorly.
In this way, our procedure resembles AdaBoost, a classic machine learning algorithm that iteratively focuses a weak learning algorithm on training examples where previous weak hypotheses performed poorly. Unlike boosting, however, our assumptions on the data and the learning method are inverted. 
Instead of a weak learner, %
we assume access to powerful LLMs that can perfectly model an input distribution, which we call \emph{strong learners.} However, we also assume access to only weak information about the distribution we wish to model (specifically, that $\beta > 0$), i.e. \emph{weak data.} This is in contrast to traditional boosting where the algorithm 
has access to \emph{strong data}, i.e., independent and identically distributed (i.i.d.) examples from some target distribution. 

This connection between the theory of boosting and learning from synthetic data has been largely unexamined in the existing literature. Our analysis also suggests practical ways to improve current algorithms for learning from synthetic data. In our experiments, we show that scarce curation resources are better utilized by focusing their efforts on producing responses to the most challenging prompts in the training set.

\section{Related work}
Training data is a crucial component in building high-performing LLMs. %
Human generated data has limitations such as scalability, biases, errors, and potential privacy considerations  \citep{kurakin2023harnessing,singh2024beyond,gilardi2023chatgpt,long-etal-2024-llms}. \citet{longpre2024consent} highlights a challenge: as LLMs scale, the demand for high-quality data increases, yet access to such data becomes more restricted due to copyright and privacy constraints. Given these challenges, integrating synthetic data into training pipelines is essential, but comes with its own set of risks. %

\paragraph{Model Collapse.} Several studies highlight a critical concern regarding the use of synthetic data in training LLMs, known as model collapse. This phenomenon is caused by \textit{improper} use of synthetic data in training the model, which can cause performance degradation or even complete failure of the model. \citet{alemohammad2024selfconsuming,shumailov2024ai,Hataya_2023_ICCV,gerstgrasser2024is} have empirically studied model collapse in various settings, demonstrating the detrimental effects of iterative training on only synthetic data and highlighting how this process can severely degrade model performance.

\citet{dohmatob2024model,pmlr-v235-dohmatob24b,bertrand2024on,dey2024universality,seddik2024how} study model collapse theoretically. Their results show recursively retraining only on synthetic data causes performance degradation in different models. However combining synthetic and labeled training data \citep{bertrand2024on,dey2024universality,seddik2024how} can mitigate this performance degradation. In contrast to our work, they do not demonstrate continuous improvement toward an optimal model.  

Recently, \citet{suresh2024rate} and \citet{feng2024beyond} provide theoretical explanations for model collapse under restricted models, including Gaussian mixture models and linear classifiers. Our results do not assume a specific learning class, instead relying on a black-box strong learning assumption. 


\paragraph{Self-improving LLMs.} Self-evolving or self-improving LLMs \citep{tao2024survey} is a new research direction that leverages the model itself to generate or guide the creation of high-quality data \citep{wang-etal-2023-self-instruct,huang-etal-2023-large}, which can then be used for fine-tuning, enabling continuous improvement with minimum or no external intervention \citep{pmlr-v235-yuan24d,pmlr-v235-chen24j}.

STaR \citep{zelikman2022star} presents a bootstrapping mechanism to enhance the reasoning capabilities of LLMs by iteratively asking the model to generate step-by-step ``chain-of-thought'' rationales for questions, filtering out the incorrect answer and fine-tuning the original model on all correct rationales and repeat the process. ReST \citep{gulcehre2023rest} proposes a combination of self-generated data and offline reinforcement learning. The method operates iteratively in two primary phases: a ``Grow'' phase, where for each input (context), the LLM generates multiple outputs to expand the training dataset, and an ``Improve'' phase, which involves ranking and filtering this augmented dataset using a learned reward model trained on human preferences. $\text{ReST}^\text{EM}$ \citep{singh2024beyond} is a modified version of ReST with two main differences which help them to improve the performance; they do not augment the generated data with human-generated data, and in the ``Improve'' step instead of fine-tuning the model in the previous iteration, they fine-tune the base model. All of the above methods can be modeled in our framework, and thus we provide a better theoretical understanding about why and when such methods can work. %



\section{Preliminary Notation}

\paragraph{Datasets.} Let $\CX$ be the set of all possible \emph{prompts}, and let $\CY$ be the set of all possible \emph{responses}, which we also call \emph{labels}. An element of $\CX \times \CY$ is a \emph{labeled prompt}. A subset of $\CX$ is a \emph{prompt set}, and a subset of $\CX \times \CY$ is a \emph{dataset}.

For any prompt set $P$, let $P(x)$ denote the number of times prompt $x$ appears in $P$, and for any dataset $D$, let $D(x, y)$ denote the number of times labeled prompt $(x, y)$ appears in $D$. Typically we have $P(x) \in \{0, 1\}$ and $D(x, y) \in \{0, 1\}$. However, we also allow datasets to contain multiple copies of the same element, where the multiplicity, or \emph{weight}, of an element can be any non-negative real number, i.e., $D(x, y) \in \mathbb{R}_+$. We write $(x, y) \in D$ if and only if $D(x, y) > 0$ and $|D| = \sum_{x, y} D(x, y)$. Datasets with general weights are formed by using the weighted union operation: If $D_0$ and $D_1$ are datasets, and $\lambda_0, \lambda_1 > 0$, then $D = \lambda_0 D_0 \uplus \lambda_1 D_1$ is the dataset defined by $D(x, y) = \lambda_0 D_0(x, y) + \lambda_1 D_1(x, y)$. 

For any dataset $D$ let
$
D(y | x) = {D(x, y)} / {\sum_{y'} D(x, y')}
$
be the fraction of labeled prompts in $D$ with prompt $x$ that have response $y$. Define $D(y | x) = 0$ if $\sum_{y'} D(x, y') = 0$. 

\paragraph{LLMs.} A large language model, or \emph{LLM}, is a function that maps each prompt in $\CX$ to a distribution on $\CY$. We will denote LLMs by the symbol $g$, and let $g(x)$ denote the distribution over labels $\CY$ of $g$ when evaluated on prompt $x$.

\section{Problem Setting}
\label{sec:desiderata}

We consider a setting
where a sequence of LLMs $g_1, g_2, \dots$ are learned on a sequence of datasets $\mathcal{D}_1, \mathcal{D}_2, \dots$. Given a prompt set $P$, our high-level goal is to produce an LLM that generates high quality responses for every prompt in $P$. We illustrate this meta-algorithm in  Setting \ref{alg:generation}.

Unlike classical learning, where the learner has access to samples from the target distribution, we assume that the learner only has access to labeled examples constructed by a data generation procedure that we control, denoted by the function ${\tt GenerateData}$. Data generation might make use of synthetic data, produced by the previous generation's LLM $g_{t-1}$, and exogenous (\emph{i.e.}, non-synthetic) signals. 

In order to formalize our goal, we make precise the capabilities of ${\tt learner}$, the capabilities of ${\tt GenerateData}$, and our notion of quality. 

\begin{algorithm}
\floatname{algorithm}{Setting}
\caption{Data Generation Problem \label{alg:generation}}
\begin{algorithmic}[1]
\Statex {\bf Given:} Prompt set $P$, number of iterations $T$.
\State $g_0 = \bot$
\For{$t = 1, 2, \dots, T$}
\State $D = {\tt GenerateData}(P, g_{t-1})$
\State $\CD_t = \CD_{t-1} \uplus D$ 
\State $g_t = {\tt learner}(\CD_t)$ 
\EndFor
\State Output $g_T$. 
\end{algorithmic}
\end{algorithm}

\subsection{Strong Learning}

We first introduce the concept of a strong learner. 
\begin{definition}[Strong Learner] \label{defn:learner}  For any LLM $g$ let $g(y|x)$ be the probability that the distribution $g(x)$
assigns to response $y$. The function ${\tt learner}$ takes as input a dataset $D \subset \CX \times \CY$ and outputs an LLM $g$ such that $g(y|x) = D(y|x)$ for all $(x, y) \in D$.
\end{definition}
The procedure ${\tt learner}$ trains an LLM that matches the conditional probability of each response given a prompt in the input dataset. That is, we assume that the model class has the capacity to match this distribution exactly, and the learning procedure can find the model parameters that perfectly fit the data. This assumption is motivated by the fact that deep neural networks instantiate all modern LLMs and are both theoretically capable of approximating arbitrary functions~\citep{maiorov1999lower} and frequently observed to fit their training inputs \citep{zhang2021understanding}.


While LLMs are powerful, the largest models contain billions of parameters and are extremely expensive to train. Thus, training $T$ state-of-the art models from scratch is prohibitively  expensive. In contrast, given model $g_{t-1}$ trained on $\mathcal{D}_{t-1}$, it is significantly less expensive to train a model $g_t$ on $D_{t-1} \uplus D$, for some choice $D$. In other words, our setting models continued training, where the training mixture for the next LLM is constructed by augmenting the existing data mixture with new examples.  

\subsection{Data Generation}

Creating data for the next generation of an LLM might involve making use of synthetic data produced by the current generation of the LLM. To avoid model collapse, some degree of data curation happens in practice. This curation may make use of an exogenous signal previously unknown to our training algorithm. Curation may also take the form of evaluating the quality of existing labeled data. We discuss each of these capabilities in greater detail. 

\paragraph{Synthetic Data.} Given an LLM $g$, and a prompt $x$, we can generate a synthetic response for $x$ by sampling from distribution $g(x)$. Overall, we assume that synthetic data generation is relatively inexpensive, and permit data generation procedures that make calls to previously-trained LLMs. 

\paragraph{Quality Evaluation.} We next assume that our data generation procedure has access to a quality function, which evaluates whether a prompt is paired with a high-quality synthetic response. In this work we assume that $q$ is a binary attribute that can be efficiently and unambiguously evaluated for all responses to prompts in $P$. 

\begin{definition}[Quality]
Let $q: \CX \times \CY \rightarrow \{0, 1\}$ be the \emph{quality} function, where $q(x, y) = 1$ indicates that $y$ is a good response to prompt $x$. 
\end{definition}

Weakening these assumptions to permit different types of ambiguity in $q$, including non-binary quality, as well as uncertainty and inefficiency in evaluating $q$ are interesting topics for future work. %

However, note that these assumptions cover a broad range of settings. For many applications, recognizing that a synthetic response is a high-quality for a given prompt is not only unambiguous but also significantly easier than generating the response from scratch. For instance, if the dataset contains arithmetic or coding problems, it is relatively easy to programmatically verify a correct answer.  

\paragraph{$\beta$-Weak Labeler.}
Key to our work is the notion of a {\em weak labeler}, a function, that given any set of prompts produces responses with average quality bounded away from zero.  %

To formally define it, we use an auxiliary function $a_P: P \rightarrow \CY$, which generates labels for all prompts in a set $P$. 

\begin{definition}[$\beta$-weak Labeler] \label{defn:labeler} The function ${\tt labeler}_\beta$ takes as input a prompt set $P \subset \CX$, and uses an auxiliary function $a_{P} : \CX \rightarrow \CY$ to label every prompt in $P$. Formally,
\[
{\tt labeler}_\beta(P) = \{(x, y) : x \in P, y = a_P(x)\} \subset \CX \times \CY
\]
We say that the labeler is $\beta$-weak if a $\beta$ fraction of these labels are high-quality, i.e., for any input prompt set $P$,
\[
\frac{|\{(x,y) \in {\tt labeler}_\beta(P) : q(x,y) = 1 \}|}{|P|} \geq \beta.
\]
\end{definition}



In our setting, each iteration of data generation is allowed to make one call to the weak labeler. The role of the labeler is to create new responses to a set of prompts. We are not prescriptive about how the labeler is implemented, only that it provides some $\beta$ fraction of high-quality responses. The labeler does not need to indicate \emph{which} prompts have been correctly labeled, nor does it need to correctly label a representative portion of its input. For example, the labeler is allowed to only correctly label the ``easiest'' prompts that it receives as input. We think of these responses as being produced by an exogenous process, such as consulting with a human directly, having a human correct or critique LLM responses, or any other framework for generating responses that are not purely synthetic. 

\subsection{Objective}
Given these capabilities --- the ability to synthesize data, assess synthetic data quality, and weakly label new data ---  the {\bf goal} of our algorithm is to construct datasets $\CD_1, \ldots, \CD_T$ so that
\begin{equation}
\lim_{T \rightarrow \infty} {\textstyle \Pr_{x \sim P, y \sim g_T(x)}}[q(x, y) = 1] = 1 \label{eq:goal}
\end{equation}
where $x \sim P$ denotes that $x$ is chosen uniformly at random from $P$, and $y \sim g_T(x)$ denotes that $y$ is chosen from distribution $g_T(x)$. In other words, as the number of algorithm iterations grows large, the final LLM output by the algorithm returns a correct response to almost every prompt in $P$. Note that this objective is similar to the objective of classical boosting. Rather than use weak learners to construct a good hypothesis, we ask whether \emph{strong learners} and \emph{weak data} can be used to construct a model that provides high-quality results on all prompts. 

\section{Algorithm}
We present an algorithm for learning an LLM from a mixture of synthetically generated and weakly labeled data that uses the capabilities introduced in Section \ref{sec:desiderata}. 

The aforementioned algorithm generates synthetic responses from the last generation of LLM. Synthetic data generation is given multiple opportunities to produce a good response through best-of-k sampling. Prompts that are consistently paired with low-quality responses are passed into ${\tt labeler}_\beta$, which provides a minimal amount of signal. A mixture of good synthetically labeled data and $\beta$-weak-labeled data is then incorporated into the training mixture. 
To state this procedure formally, we introduce two subroutines. 

\begin{definition}[Best-of-k] \label{defn:bestof} The function $\bestof(P; k, g)$ takes as input a set of prompts $P \subset \CX$ and produces a dataset where each $x \in P$ is paired with the best response encountered after $k$ rounds of inference using the LLM $g$. Formally, for each $x \in P$ and $i \in [k]$, let $y_x^i \sim g(x)$ and define the random set $Y_x^k = \{y_x^i : i \in [k]\}$. Then,
\[
\bestof(P; k, g) = \{(x, \arg\max_{y \in Y_x^k} q(x,y)) : x \in P\}.
\]
\end{definition}

\begin{definition}[Filter] \label{defn:filter} The function ${\tt filter}$ takes as input a dataset $D \subset \CX \times \CY$ and selects elements with quality $1$. It also returns prompts that have quality $0$. Formally,
\[
{\tt data}^+(D) = \{(x,y) : (x,y) \in D, q(x,y) = 1\} \subset \CX \times \CY.
\]
\[
{\tt prompts}^-(D) = \{x : (x,y) \in D, q(x,y) = 0\} \subset \CX.\\
\]
\[
{\tt filter(D) = ({\tt data}^+(D),{\tt prompts}^-(D))}.
\]
\end{definition}

Algorithm~\ref{alg:boosting} formalizes our procedure for data generation, where best-of-k, filtering, and weak-labeling are applied in sequence on each generation of LLM. Whether the data that is being added to the mixture consists of mostly $\beta$-weakly labeled data ($D_t$ in Algorithm~\ref{alg:boosting}) or high-quality synthetic data ($S^+_t$ in Algorithm~\ref{alg:boosting}) is parameterized by $\alpha > 0$. 

\begin{algorithm*}
\caption{Boosting-style algorithm for LLM training \label{alg:boosting}}
\begin{algorithmic}[1]
\Statex {\bf Given:} Prompt set $P$, repeat parameter $k$, weakly labeled prompt weight $\alpha$, high-quality fraction $\beta$, no. of iterations $T$.
\State $g_0 = \bot$ and $\CD_0 = \emptyset$ \Comment{Initial LLM and initial training set}
\For{$t = 1, 2, \ldots, T$}
\State $S_t = \bestof(P; k, g_{t-1})$ \Comment{Best of $k$ sampling.}
\State $(S^+_t, P^-_t) = {\tt filter}(S_t)$ \Comment{Partition high-quality synthetic data from low-quality prompts.}
\State $D_t = {\tt labeler}_{\beta}(P^-_t)$ \Comment{Weakly label low-quality prompts}
\State $\lambda_t = \frac{\alpha}{|D_t|}$ \Comment{Set weight of weakly labeled prompts}
\State $\CD_t = \CD_{t-1} \uplus \lambda_t D_t \uplus S^+_t$ \Comment{Update training mixture}
\State $g_t = {\tt learner}(\CD_t)$ \Comment{Use learner to update LLM}
\EndFor
\end{algorithmic}
\end{algorithm*}
































\section{Main result}

Theorem \ref{thm:main} is our main theoretical result, and states that the final LLM $g_T$ output by Algorithm \ref{alg:boosting} satisfies the convergence requirement in Eq.~\eqref{eq:goal}. Theorem \ref{thm:main} also quantifies the rate of convergence. %





\begin{theorem} \label{thm:main} Let $\eps \in (0, 1)$. Suppose that in Algorithm \ref{alg:boosting} we have $\alpha > 0$, $\beta \in (0, 1)$,
\[
T \ge \frac{\log(2/\eps)}{\beta} + \frac{2\alpha}{\beta\eps} + 1
\]
and $k \ge ({2\log T + \log |P|})/{\beta}$. With probability at least $1 - 1/T$ over the randomness of the algorithm, the final LLM $g_T$ output by the algorithm satisfies
\[
{\textstyle \Pr_{x \sim P, y \sim g_T(x)}}[q(x, y) = 1]
\ge 1 - \eps.
\]
Note that by setting $\alpha = \eps$ in Algorithm \ref{alg:boosting} the above iteration complexity becomes $T = O(\log(1/\eps)/\beta)$. 
\end{theorem}
\begin{proof}[Proof sketch]
The key step in the proof is showing that, with probability $1 - 1/T$, in each iteration $t$ we have $\Pr_{y \sim g_{t-1}(x)}[q(x, y) = 1] \ge \beta$ for all but $(1 - \beta)^{t-1}$ fraction of the prompts $x \in P$. Since the algorithm draws $k = \Omega(1 / \beta)$ synthetic responses to each prompt from $g_{t-1}$, one of those responses is likely to be correct. As a result, correctly labeled prompts are continually added to the training data (via the synthetic dataset $S^+_t$), and the quality of the training data steadily improves, causing the performance of the LLMs learned from that training data to approach the optimal performance.

Even when $t$ is large, it is non-trivial to show that $\Pr_{y \sim g_{t-1}(x)}[q(x, y) = 1] \ge \beta$ for nearly all prompts $x \in P$. While this fact follows from our assumption about the weak labeler, it does not follow straightforwardly. The weak labeler ensures that the \emph{average} response quality to a given set of prompts is at least $\beta$, but we need a guarantee about response quality that holds \emph{uniformly} for almost all prompts. Our approach is to first show that $P^-_t$ (the set of prompts with low-quality responses) shrinks exponentially with $t$, and then observe that the total weight assigned to these prompts in the training data is fixed at $\alpha > 0$ (a free parameter of our algorithm). Consequently, once a prompt is assigned a high-quality response by the weak labeler, the weight of that response overwhelms the weight of all previous low-quality responses in the training data. So when the learner fits an LLM to this training data, the LLM assigns non-trivial probability mass to the high-quality response; we are able to bound this probability from below by $\beta$. 
\end{proof}

Theorem \ref{thm:main} requires $\alpha > 0$ and $\beta > 0$, and it is easy to see that both assumptions are necessary. If $\alpha = 0$ then each training set $\CD_t$ contains no exogenously labeled data, and if $\beta = 0$ then all of the exogenously labeled data is incorrect. In either case,  and given a worst-case initial LLM $g_0$ that returns an incorrect response to every prompt, the learner has no way to determine the correct response to any prompt. 



By setting $\alpha = \eps$ in Theorem \ref{thm:main}, where $\eps$ is the desired error of the final LLM, and $\alpha$ is the total weight assigned to the weakly labeled prompts in each training set, we obtain an iteration complexity of $O(\log (1 / \eps) / \beta)$. The astute reader will note that $\alpha$ can also be set arbitrarily close to zero in Theorem \ref{thm:main} without weakening the convergence guarantee. This curious property is a consequence of an idealized assumption that we made for the sake of analytical tractability. Specifically, we assumed that the learner can match the conditional response distribution of every prompt in the training data, no matter how infrequently the prompt appears in the data (see Definition \ref{defn:learner}). In practice, constraints on training time and model size will prevent a learner from perfectly fitting the training data. So it would be useful to extend our results to account for the possibility of an imperfect learner, and we expect that any such extension would imply a non-zero lower bound on $\alpha$. Nonetheless, our current results tell us something  interesting -- computational limitations are the \emph{only} barrier to learning an arbitrarily good LLM, and not, as one might expect, the quality of the weak labeler. 

\section{Discussion}

\subsection{Interpretation as Boosting}

Boosting is a meta-learning algorithm for combining weak hypotheses into highly accurate ensemble classifiers \citep{schapire2013boosting}. While the most common version of boosting is AdaBoost \citep{freund1997decision}, we will present a slightly simpler version that still contains all of the essential ideas.

In each iteration of boosting, a training set of binary-labeled examples is given as input to a \emph{weak learner}. Each training example is associated with a non-negative weight, and the weights sum to 1. The weak learner returns a hypothesis that achieves weighted error at most $\frac12 - \beta$ on the training set, where $\beta \in (0, \frac12)$ is the \emph{edge} over the trivial hypothesis that randomly guesses each label. The weight on each training example that is correctly labeled by the hypothesis is decreased by a factor $\exp(\theta)$, and the weight on each training example that is incorrectly labeled by the hypothesis is increased by the same factor, where $\theta = \frac12 \log\frac{1 + 2\beta}{1 - 2\beta}$. Essentially, the weights are adjusted to concentrate on difficult examples. The weights are renormalized to sum to 1, and the process repeats. After $T = O(\log(1/\eps)/\beta^2)$ iterations, a majority vote among all the hypotheses achieves unweighted error most $\eps$ on the training set.

Comparing Algorithm \ref{alg:boosting} to the description of boosting given above reveals many similarities. In each iteration of Algorithm \ref{alg:boosting}, prompts are given as input to a weak labeler that has quality $\beta \in (0, 1)$, where $\beta$ is the edge over the trivial labeler that assigns an incorrect response to every prompt. The weight on each prompt that is correctly labeled by the previous iteration's LLM is set to zero, and the weight on each prompt that is incorrectly labeled by the previous iteration's LLM is increased by at least a factor $\exp(\theta)$, where $\theta = \log \frac{1}{1 - \beta}$ (this fact emerges from our analysis, which proves that size of the set of prompts given to the weak labeler shrinks by a factor at least $1 - \beta$ each iteration; see Lemma \ref{lem:main}(b) in the Appendix). As in boosting, the weights are adjusted to concentrate on difficult examples. After $T = O(\log(1/\eps)/\beta)$ iterations, an LLM learned from all of the training data achieves error at most $\eps$ on the overall prompt set (see Theorem \ref{thm:main}).

\subsection{Trivial Baseline: Filtering Non-Synthetic Data}
Algorithm~\ref{alg:boosting} has the property that it only applies filtering on LLM-generated data. As discussed, this accurately models existing methods in the literature. 

However, if we consider applying the quality function $q$ on data produced by the weak labeler (that is, data that is not LLM-generated), then there is a very simple solution to the data generation problem. Clearly $O(\log(1/\eps)/\beta)$ invocations of the weak labeler would suffice to correctly label all but $\eps$ fraction of the prompts in $P$ (just repeatedly invoke the weak labeler on the incorrectly labeled prompts, filtering high-quality examples), and such a dataset could be given to a strong learner to produce an LLM that achieves $O(\eps)$ error. 
 It is worthwhile to reason about why such a simple solution cannot be deployed in practice. 

The weak data assumption specifies that $\beta$, while arbitrary, is bounded away from zero. Just as the weak learning assumption might not hold in classical boosting, the weak data assumption might not hold in our setting. We argue that iteratively filtering the weak labeler's output should result in a precipitous drop in the fraction of correctly-labeled examples. As an example, suppose human labelers provide good responses to the $\beta_1$ easiest coding prompts in some prompt set. One should expect that asking similarly-qualified labelers to respond to the remaining prompts results in a $\beta_2 \ll \beta_1$ yield of quality responses, as all but the easiest prompts have been answered. In contrast, a continually improved LLM endows a human with more flexibility for future responses, such as rewriting nearly high-quality solutions provided by the last iteration of LLM, making a non-vanishing $\beta$ a much more reasonable assumption. 

Secondly, while it keeps the setting simple to presume that $q$ can be evaluated on any labeled example, this is  an overly permissive assumption. LLM-generated synthetic data can be made to include reasoning traces, and often produces responses that the LLM itself can verify as high quality. This facilitates the construction of automated quality checkers, which are much more difficult to construct when the labels are produced by a human, and therefore contain reasoning traces and responses that are unfamiliar to the current generation of LLM. This is born out in the literature, where quality verification of LLM-generated synthetic data is relatively easy to implement~\citep{singh2024beyond,yang2024self,zelikman2022star}. 

Finally, and somewhat remarkably, Algorithm \ref{alg:boosting} with $\alpha = \epsilon$ achieves the same finite-time error rate as this baseline while only ever evaluating the quality of LLM-generated data. Thus, the approach taken in practice matches the convergence rate that would be experienced under a much more powerful set of assumptions. 




\section{Experiments}

Viewing Algorithm \ref{alg:boosting} as a meta-algorithm, we conduct experiments with specific instantiations using Gemma 2 2B on math problem solving \citep[GSM8K]{cobbe2021training} and Python coding \citep[MBPP]{austin2021program} tasks. 
We select these tasks because measures of response quality here are consistent and easily verifiable. %

\subsection{Instantiations of Algorithm \ref{alg:boosting}}\label{sec:instantiations}


\paragraph{Do nothing.} Responses produced by the current iteration of the model are directly used as training data for the next iteration. This corresponds to setting $\alpha=0$, omitting the $\bestof$ operation, and using a pass-through ${\tt filter}$ in line 4 of Algorithm \ref{alg:boosting}. This tracks the setting explored in the ``model collapse'' literature \citep{alemohammad2024selfconsuming,shumailov2024ai, gerstgrasser2024is}.

\paragraph{Filter only.} Only correct responses in the current iteration are used for training in the next iteration. This corresponds to $\alpha=0$ in Algorithm \ref{alg:boosting} and using a ${\tt filter}$ that only keeps correct responses. This reproduces the STaR/ReST approaches for learning from synthetic data \citep{zelikman2022star, gulcehre2023rest, singh2024beyond}.

\paragraph{Boosting.} The full algorithm of the present paper. In addition to the synthetic data produced by \emph{Filter only}, we mix in weak data from the labeler. This corresponds to $\alpha>0$ and $\beta>0$ in Algorithm \ref{alg:boosting}. We use $\alpha=1/3$ in all experiments.

\begin{itemize}
    \item \textbf{Boosting, w/o focusing}. We ablate out focusing on hard examples. To be precise: rather than giving the labeler the prompts we got wrong, $P_t^-$, we draw a random set of questions of size $|P_t^-|$.
\end{itemize}

\paragraph{Baselines.} We also report two baselines that do not involve iteratively training on model-generated data. \textbf{PT}: the pre-trained model; and \textbf{Gold SFT}: the model after one round of fine-tuning on the human-written responses in the dataset. Note that \emph{Gold SFT} is the only setup that makes use of human-written responses, rather than just for answer verification.


\begin{figure*}[t]
  \centering
  \includegraphics[width=0.42\textwidth, trim=0cm 0.6cm 0.0cm 0.2cm, clip]{figures/gsm8k_test.pdf} 
  \includegraphics[width=0.49\textwidth, trim=0cm 0.6cm 0cm 0.2cm, clip]{figures/gsm8k_train.pdf} 
  \caption{We plot test and train performance of our Algorithm 2 variants on GSM8K, across rounds. For train accuracy plots, we plot both train accuracy@1 (solid) and train accuracy@8 (stacked). Boosting results displayed here use \emph{weak data (A)}. For the MBPP plot, see Figure \ref{fig:mbpp} in the Appendix}
  \label{fig:gsm8k}
\end{figure*}

\subsection{Experimental Details}

In all experiments, a round of fine-tuning entails training all parameters of the model for 330 (GSM8K) or 30 (MBPP) steps at batch size 64 (with the exception of  \emph{Gold SFT} where we report the checkpoint with best validation accuracy)  We train with standard sequence cross-entropy loss. Training examples are ({\tt input}, {\tt target}) pairs, where {\tt input} is the problem preceded by a 3-shot prompt (see Appendix \ref{sec:prompts} for prompt templates); and {\tt target} is a model response (human-written response for \emph{Gold SFT}).



\paragraph{Modeling the weak data.} We instantiate ${\tt labeler}$ as a Gemma 2 2B PT model with a fixed \emph{total query budget}, which is distributed uniformly over all problems it receives. For a given problem, we sample responses from the model equal to that problem's allotted queries. We consider two setups to simulate weak data provided by the weak labeler. \textbf{Weak data (A)}: for each question, we return all correct responses if there are any. If there are none, we return a random incorrect response. \textbf{Weaker data (B)}: we pool together the correct responses to all questions. We add to this collection an equal number of incorrect responses, drawn randomly from all incorrect responses to all questions.

We remark that the fixed total query budget setup offers a mechanism for satisfying the weak data assumption: the labeler can maintain constant accuracy when targeting increasingly granular (and more difficult) slices of the input distribution by focusing their resources. We see that this is indeed the case experimentally, and plot accuracies in Figure \ref{fig:labeler}. Moreover, a fixed query budget is a natural analogue to the fixed person-hours/money/compute budgets behind a labelling effort.

\setlength\tabcolsep{7pt}
\def\arraystretch{0.85}%
\begin{table}[t!]
  \small
  \centering
  \begin{tabular}{l c l l l}
    \toprule
    \multirow{2}{*}{Setup} & \multirow{2}{*}{Rounds} & \multicolumn{2}{c}{train} & test \\
    \cmidrule(lr){3-4} \cmidrule(lr){5-5}
    & & @1 & @8 & greedy \\
    \midrule
    PT & \multirow{2}{*}{0} & .210 & .626 & .219 \\
    \hspace{4pt} - orig. report &  & - & - & .243* \\
    \cmidrule{2-5}
    Gold SFT & 1 & .370 & .744 & .369 \\
    \midrule
    Do nothing & 5 & .139 & .393 & .153 \\
    \cmidrule{2-5}
    Filter only & 5 & .497 & .728 & .371 \\
    \cmidrule{2-5}
    Boosting \textbf{(A)} & \multirow{2}{*}{5}  & .574 & .843 & .446 \\
    \hspace{4pt} - w/o focusing &  & .542 & .783 & .420 \\
    \cmidrule{2-5}
    Boosting \textbf{(B)} & \multirow{2}{*}{5}  & .544 & .811 & .438 \\
    \hspace{4pt} - w/o focusing &  & .509 & .750 & .426 \\
    \bottomrule
  \end{tabular}
  \caption{Comparison of 3-shot COT train and test accuracy on GSM8K for Gemma 2 2B checkpoints.  \emph{Rounds} is the total number of rounds of fine-tuning the model has undergone. To report train accuracy @$k$, we sample $k$ responses for each problem at temperature 0.7 and mark it correct if any of $k$ answers match. For test accuracy, we employ greedy sampling. \textbf{(*)}: Row 2 cites the figure from the Gemma 2 report \citep{gemmateam2024gemma2improvingopen} which does not report sampling temperature.} 
  \label{tab:gsm8k-results}
\end{table}

\paragraph{Departures from the theory.} In our experiments, we make two main modifications from Algorithm \ref{alg:boosting}:
\begin{enumerate}

\item Rather than accumulating \emph{data} and retraining the model each iteration (Algorithm \ref{alg:boosting}, line 9), we instead accumulate \emph{updates}. That is, we fine-tune on the newly introduced data in each iteration, initializing from the checkpoint produced by the prior iteration. We do this for efficiency reasons.

\item When performing the $\bestof$ operation in practice, we keep all correct responses instead of randomly selecting one. This is to align our algorithm with ReST \citep{gulcehre2023rest, singh2024beyond}. Moreover, we only return one response in the theory to simplify the presentation of the analysis, but Theorem \ref{thm:main} indeed holds for the variant of Algorithm \ref{alg:boosting} where we return all correct responses.

\end{enumerate}

\subsection{GSM8K Results}
Table \ref{tab:gsm8k-results} summarizes our results on GSM8K. We have 7000 training problems, use $k=8$ for $\bestof$, and allocate the same total query budget of 56,000 to the labeler each round. 



\paragraph{Baselines validate our experimental setup.} Results in the \emph{PT} and \emph{Gold SFT} demonstrate that: (1) our evaluation setup is in the ballpark of what is reported in the original Gemma 2 report; and (2) our fine-tuning setup indeed can yield significant improvement when the training data is human-written solutions.

\paragraph{Model collapse with no curation.} In the \emph{Do nothing} row, we recover the result from the model collapse literature that iterative fine-tuning without curation does not improve the model and leads to degraded quality.

\paragraph{Comparison between curation variants.} Indeed, the present algorithm demonstrates improvements over the ReST-like variant that uses filtering only. The differences are most evident in training accuracy, which is strongly predicted by the theory. Indeed, this is in spite of the fact that \emph{as opposed to filtering only, boosting introduces incorrect answers to the training data}. Furthermore although our theory does not address generalization, we observe that boosting results in improved test accuracy. Finally, the performance of boosting without focusing is quite close -- random selection is a strong baseline -- but focusing still leads to improvements, especially in terms of training accuracy. 

\paragraph{Qualitative results.} In Appendix \ref{sec:qualitative-analysis}, we present model responses to selected problems over the course of training.

\subsection{MBPP Results}



Table \ref{tab:mbpp-results} summarizes our results on MBPP. We have 374 training problems, use $k=32$ for $\bestof$, and allocate the same total query budget of 11,968 to the weak labeler in each round.

\paragraph{Similar results to GSM8K for train pass rate.} In terms of train pass@$k$, we observe similar results to GSM8K experiments, that generally: \emph{Boosting} $>$ \emph{Boosting w/o focusing} $>$ \emph{Filter only} $>$ \emph{Do nothing}. On \emph{weaker data (B)}, \emph{Filtering} beats \emph{Boosting w/o focusing} in terms of pass@1. 

\paragraph{No clear winner for test pass rate.} While all iterative approaches outperform \emph{Gold SFT} in terms of test pass rate, they all recover similar test performance despite differences in training accuracy. Notably, \emph{Boosting w/o focusing} beats \emph{Boosting}, and \emph{Filter Only} outperforms \emph{Boosting} with weaker data (B). One explanation is the limited amount of training data (384 examples) which prevents generalization; note that \emph{Gold SFT} does not recover \emph{PT} test pass rate. 

\begin{table}[t!]
  \small
  \centering
  \begin{tabular}{l c l l l}
    \toprule
    \multirow{2}{*}{Setup} & \multirow{2}{*}{FT rounds} & \multicolumn{2}{c}{train} & test \\
    \cmidrule(lr){3-4} \cmidrule(lr){5-5}
    & & @1 & @32 & greedy \\
    \midrule
    PT & \multirow{2}{*}{0} & .227 & .698 & .274 \\
    \hspace{4pt} - orig. report &  & - & - & .302* \\
    \cmidrule{2-5}
    Gold SFT & 1 & .925 & .997 & .258 \\
    \midrule
    Do nothing & 5 & .227 & .396 & .194 \\
    \cmidrule{2-5}
    Filter only & 5 & .626 & .690 & .320 \\
    \cmidrule{2-5}
    Boosting \textbf{(A)}& \multirow{2}{*}{5}  & .690 & .840 & .328 \\
    \hspace{4pt}- w/o focusing &  & .599 & .749 & .334 \\
    \cmidrule{2-5}
    Boosting \textbf{(B)}& \multirow{2}{*}{5}  & .658 & .837 & .316 \\
    \hspace{4pt}- w/o focusing &  & .556 & .709 & .318 \\
    \bottomrule
  \end{tabular}
  \caption{Comparison of 3-shot train and test pass@$k$ rates on MBPP for Gemma 2 2B checkpoints produced by various setups. To report train pass@$k$, we sample $k$ solutions to each coding probelm at temperature 0.7 and mark it correct if any of $k$ solutions pass the tests. For test pass rate, we employ greedy sampling. \textbf{(*)}: Row 2 cites the figure from the Gemma 2 report \citep{gemmateam2024gemma2improvingopen} which does not report sampling temperature.} 
  \label{tab:mbpp-results}
\end{table}

\section{Conclusion \& Future Work}
We have shown that under mild assumptions a modicum of curation applied to synthetic data not only avoids model collapse, but leads to arbitrarily high accuracy results. Our analysis is through the lens of boosting and, mirroring that paradigm, we define notions of {\em strong learners} and {\em weak data} to reach the theoretical conclusions. 
In taking this view, we provide theoretical explanations for many of the synthetic data methods used in practice. 

Many interesting questions remain. An immediate avenue is further relaxing the assumptions (e.g., having nearly strong learners -- that approximately match the conditional distribution -- and imperfect filters) and deriving corresponding convergence rates. A broader goal is using these insights for the burgeoning field of data selection, where we must explicitly model similarities between different examples as part of the analysis. 

\section*{Author contributions}\label{sec:contributions}

\begin{itemize}
    \item {\bf Sergei V} conceived the idea to analyze synthetic data training through the lens of boosting.
    \item {\bf Sara B} and {\bf Alex B} reviewed related work.
    \item {\bf Everyone} developed the modeling framework.
    \item {\bf Kareem A}, {\bf Weiwei K}, {\bf Umar S} and {\bf Sergei V} proved the main result.
    \item {\bf Alex B} designed and ran the experiments.
    \item {\bf Everyone} contributed to writing the paper and framing its contributions.
\end{itemize}

\newpage

\bibliography{main}


\newpage
\appendix

\allowdisplaybreaks

\section{Proof of Theorem \ref{thm:main}}

Throughout the proof, we will write $S$ to denote a dataset where all of the labels were generated synthetically (\emph{i.e.}, by an LLM), $D$ to denote a dataset where all of the labels were provided by ${\tt labeler}_\beta$, and $\CD$ to denote a dataset containing a mixture of these kinds of data. Also, only datasets denoted by $\CD$ will contain elements whose weights can differ from $0$ and $1$. All other datasets will be ordinary sets.



We adopt a few simplifying assumptions and conventions. Assume that the given prompt set $P$ is non-empty. Assume that the initial LLM $g_0$ returns an incorrect response to every prompt with probability $1$. Removing the latter assumption would only speed up the convergence of Algorithm \ref{alg:boosting} to an optimal LLM, but would also further complicate its analysis. Finally, we adopt the convention that $\infty \cdot 0 = 0$. This convention is needed when Algorithm \ref{alg:boosting} constructs $\CD_t$ via the weighted union operation, since it can happen that $\lambda_t = \infty$, but this only occurs when $D_t$ is empty.

Let $P^+_t = \{x \in P : (x, y) \in S^+_t\}$ be the correct prompts selected by ${\tt filter}$. By definition $P^+_t$ and $P^-_t$ form a partition of $P$. %
Furthermore, $D_t$ pairs each prompt in $P^-_t$ with the label it was assigned by ${\tt labeler}_\beta$, and $S^+_t$ pairs each prompt in $P^+_t$ with the (synthetic) label it was assigned by the previous iteration's LLM, $g_{t-1}$. Observe that $P^-_t(x) = \sum_y D_t(x, y)$ and $P^+_t(x) = \sum_y S^+_t(x, y)$. For all $t \ge 1$ and $x \in P$ let
\begin{align*}
q_t(x) &= \sum_y D_t(y | x) q(x, y)\\
q^+_t(x) &= \sum_y S^+_t(y | x) q(x, y)\\
\baq_t(x) &= \sum_y \CD_t(y | x) q(x, y)
\end{align*}
be the average quality of the responses to prompt $x$ in datasets $D_t$, $S^+_t$ and $\CD_t$, respectively. For convenience we also define $\baq_0(x) = \E_{y \sim g_0(x)}[q(x, y)]$. Note that $\baq_0(x) = 0$ for all $x \in \CX$ by assumption.



\begin{lemma} \label{lem:quality} For all $t \ge 1$ and $x \in P$
\[
\baq_t(x) = \frac{\sum_{s=1}^t \lambda_s P^-_s(x)q_s(x) + P^+_s(x)q^+_s(x)}{\sum_{s=1}^t \lambda_s P^-_s(x) + P^+_s(x)}.
\]\end{lemma}
\begin{proof} We have
\begin{align*} 
    \CD_t(y | x) &= \frac{\CD_t(x, y)}{\sum_{y'} \CD_t(x, y')}\\
                  &= \frac{\sum_{s=1}^t \lambda_s D_s(x, y) + S^+_s(x, y)}{\sum_{y'} \sum_{s=1}^t \lambda_s D_s(x, y') + S^+_s(x, y')}\\
                  &= \frac{\sum_{s=1}^t \lambda_s D_s(x, y) + S^+_s(x, y)}{\sum_{s=1}^t \lambda_s P^-_s(x) + P^+_s(x)}\\
                  &= \frac{\sum_{s=1}^t \lambda_s P^-_s(x)D_s(y | x) + P^+_s(x)S^+_s(y | x)}{\sum_{s=1}^t \lambda_s P^-_s(x) + P^+_s(x)}
\end{align*}
and therefore
\begin{align*}
    \baq_t(x) &= \sum_y \CD_t(y | x) q(x, y)\\ 
              &= \frac{\sum_y \sum_{s=1}^t \lambda_s P^-_s(x)D_s(y | x)q(x, y) + P^+_s(x)S^+_s(y | x)q(x, y)}{\sum_{s=1}^t \lambda_s P^-_s(x) + P^+_s(x)}\\
              &= \frac{\sum_{s=1}^t \lambda_s P^-_s(x)q_s(x) + P^+_s(x)q^+_s(x)}{\sum_{s=1}^t \lambda_s P^-_s(x) + P^+_s(x)} \qedhere
\end{align*}
\end{proof}

\begin{lemma} \label{lem:zeroes} For all $t \ge 1$ and $x \in P$ we have $\baq_t(x) = 0$ if and only if $\baq_{t-1}(x) = 0$ and $q_t(x) = 0$.\end{lemma}
\begin{proof} 


Suppose $\baq_t(x) = 0$. By Lemma \ref{lem:quality} this implies $\lambda_s P^-_s(x) q_s(x) + P^+_s(x)q^+_s(x) = 0$ for $s \in \{1, \ldots, t\}$, and therefore $\baq_{t-1}(x) = 0$. This implies that $x$ cannot be correctly labeled in $S_t$, and therefore $P^-_t(x) = 1$. Since $\alpha > 0$ we have $\lambda_t > 0$. And since $\lambda_t P^-_t(x) q_t(x) = 0$ we must have $q_t(x) = 0$.

Now suppose $\baq_{t-1}(x) = 0$ and $q_t(x) = 0$. Since $\baq_{t-1}(x) = 0$ then again by Lemma \ref{lem:quality} we have $\lambda_s P^-_s(x)q_s(x) +  P^+_s(x)q^+_s(x) = 0$ for $s \in \{1, \ldots, t-1\}$. The fact that $\baq_{t-1}(x) = 0$ also implies that $x$ cannot be correctly labeled in $S_t$, and therefore $P^+_t(x) = 0$. And since $q_t(x) = 0$ we have $\lambda_t P^-_t(x)q_t(x) +  P^+_t(x)q^+_t(x) = 0$, which implies $\baq_t(x) = 0$.
\end{proof}

\begin{lemma} \label{lem:zeroone} Let $t \ge 1$ and $x \in P$. If $x \in P^-_t$ then $q_t(x) \in \{0, 1\}$. If $x \in P^+_t$ then $q^+_t(x) = 1$.\end{lemma}
\begin{proof} Note that $D_t$ contains each prompt only once (by Definition \ref{defn:labeler} of ${\tt labeler}_\beta$), and $S^+_t$ contains only correctly labeled prompts (by Definition \ref{defn:filter} of  ${\tt filter}$). The lemma follows from the definitions of $P^-_t, P^+_t$, $q_t(x)$ and $q^+_t(x)$.
\end{proof}

\begin{lemma} \label{lem:tech} If $a, b, c, d \ge 0$ satisfy $a \le b$, $c \ge d$ and $b > 0$ then
\[
\frac{a + c}{b + c} \ge \frac{a + d}{b + d}.
\]\end{lemma}
\begin{proof} If $c = d$ then clearly the lemma holds with equality. Otherwise if $c > d$ then
\begin{align*}  
&~\frac{a + c}{b + c} \ge \frac{a + d}{b + d}\\
\Leftrightarrow ~& (a + c)(b + d) \ge (a + d)(b + c) & b > 0\\
\Leftrightarrow ~& ab + bc + ad + cd \ge ab + bd + ac + cd\\
\Leftrightarrow ~& bc + ad \ge bd + ac\\
\Leftrightarrow ~& b(c - d) \ge a(c - d)\\
\Leftrightarrow ~& b \ge a  & c > d & \qedhere
\end{align*} \end{proof}

Our analysis relies on conditioning on the fact that once the quality of a particular prompt, $x$,  is high enough, it is always selected by ${\tt filter}$ and is never sent to ${\tt labeler}_\beta$. Formally, fix the number of iterations, $T$, the set of prompts, $P$, and the quality of the weak data, $\beta$. We define event $E$, as follows:
\begin{center}
Event $E \equiv$ For all $t \in [T]$ and $x \in P$ if $\baq_{t-1}(x) \ge \beta$ then $x \not\in P^-_t$,
\end{center}
\noindent and to simplify notation, we drop the dependence of $E$ on $T$, $P$ and $\beta$. 

\begin{lemma} \label{lem:unionbound} If the repeat parameter $k \geq \frac{2 \log T + \log |P|}{\beta}$ then event $E$ occurs with probability at least $1 - \frac1T$.\end{lemma}
\begin{proof} By Definition \ref{defn:filter} of ${\tt filter}$, prompt $x \in P^-_t$ if and only if $x$ appears in $S_t$ without a correct label. By Definition \ref{defn:bestof} of $\bestof$, each $x \in P$ is labeled $k$ times by $g_{t-1}$ in iteration $t$, with each label drawn independently from distribution $g_{t-1}(x)$. Thus we know that if $\baq_{t-1}(x) \ge \beta$ then $x \in P^-_t$ with probability at most $(1 - \beta)^k$. Therefore
\begin{align*}
    \Pr[\neg E] &= \Pr\left[\exists t \in [T] \textrm{ and } x \in P \textrm{ such that } \baq_{t-1}(x) \ge \beta \textrm{ and }x \in P^-_t\right]\\
    &\le \sum_{t=1}^T \sum_{x \in P} \Pr[\baq_{t-1}(x) \ge \beta \textrm{ and }x \in P^-_t]\\
    &\le T |P| (1 - \beta)^k\\
    &\le T |P| \exp(-\beta k)\\
    &\le T |P| \exp(-2 \log T - \log |P|) & \textrm{Assumption about }k\\
    &= T |P| \frac{1}{T^2} \frac{1}{|P|}\\
    &= \frac1T & \qedhere
\end{align*}\end{proof}

The next result is our key lemma. It says that if event $E$ occurs then (a) $P^-_t$ contains all and only the prompts that must have been incorrectly labeled by the previous iteration's LLM, (b) the size of $P^-_t$ shrinks exponentially over time, (c) once a prompt is outside $P^-_t$ it remains that way, and (d) prompts outside of $P^-_t$ are correctly labeled by the previous iteration's LLM with a probability that is bounded above zero.


\begin{lemma} \label{lem:main} Fix $T$. Let $1 \le t \le T$ and $x \in P$. If event $E$ occurs then all of the following hold:
\begin{enumerate}
    \item[(a)] $x \in P^-_t$ if and only if $\baq_{t-1}(x) = 0$.
    \item[(b)] $|P^-_r| \le (1 - \beta)^{r - s} |P^-_s|$ for all $r, s \in [t]$ such that $r \ge s$.
    \item[(c)] There exists $r \in [t]$ such that $x \in P^-_s$ for all $s \in [r]$ and $x \not\in P^-_s$ for all $s \in [t] \setminus [r]$.
    \item[(d)] Let $r \in [t]$ satisfy the conditions of part (c). If $r < t$ then 
    \[
    \baq_t(x) \ge \frac{\alpha + t - r}{\frac{\alpha\left(1 - (1 - \beta)^r\right)}{\beta} + t - r} \ge \beta.
    \]
\end{enumerate}\end{lemma}
\begin{proof} The proof will proceed by induction. We begin by proving the base case, $t = 1$. To prove part (a), note that by assumption we have $\baq_0(x) = \E_{y \sim g_0(x)}[q(x, y)] = 0$, so we only need to show that $x \in P^-_1$. Since $\E_{y \sim g_0(x)}[q(x, y)] = 0$, we know that $x$ cannot be correctly labeled in $S_1$, which implies $x \in P^-_1$. Part (b) follows immediately from the observation that when $t = 1$ we have $r = s = 1$. Part (c) holds immediately by letting $r = 1$, since in this case $[t] \setminus [r]$ is empty, and we have already shown $x \in P^-_1$ in part (a). Part (d) holds vacuously because $r < t$ must be false when $t = 1$. 

Now assume for induction that the lemma holds for a fixed $t \ge 1$. We will prove the lemma for the case $t + 1$. To prove part (a), first assume $\baq_t(x) = 0$, which is the premise of the `if' direction. By Definition \ref{defn:learner} we have 
\[
\E_{y \sim g_t(x)}[q(x, y)] = \sum_y \CD_t(x, y)q(x, y) = \baq_t(x) = 0
\]
which implies that $x$ cannot be correctly labeled in $S_{t+1}$, and therefore $x \in P^-_{t+1}$. Now assume $x \in P^-_{t+1}$, which is the premise of the `only if' direction. To force a contradiction, assume that $\baq_t(x) > 0$. By part (d) of the inductive hypothesis, this implies $\baq_t(x) \ge \beta$. Since event $E$ occurred, we have that $x \not\in P^-_{t+1}$, which is a contradiction. This completes the proof of part (a).

To prove part (b), choose any $r, s \in [t+1]$ such that $r \ge s$. If $r = s$, part (b) follows immediately. If $r < t+1$ and $s < t+1$ then part (b) follows from the inductive hypothesis. Henceforth assume $s < r = t+1$. Let
\[
D^+_t = \{(x, y) \in D_t : q(x, y) = 1\}
\]
be the subset of $D_t$ that is correctly labeled. We have
\begin{align*}
(1 - \beta) |P^-_t| &= (1 - \beta)|D_t|\\
&\ge |D_t| - |D^+_t| & \textrm{Definition \ref{defn:labeler} of }{\tt labeler}_\beta\\
&= \sum_{x, y} D_t(x, y) - \sum_{x, y} D_t(x, y)q(x, y)\\
&= \sum_{x, y} D_t(x, y)(1 - q(x, y))\\
&= \sum_{x, y} P^-_t(x)D_t(y | x)(1 - q(x, y))\\
&= \sum_x P^-_t(x)(1 - q_t(x))\\
&= \sum_x P(x)\indic{\baq_{t-1}(x) = 0}(1 - q_t(x)) & \textrm{Inductive hypothesis, part (a)}\\
&= \sum_x P(x)\indic{\baq_{t-1}(x) = 0}\indic{q_t(x) = 0} & \textrm{Lemma }\ref{lem:zeroone}\\
&= \sum_x P(x)\indic{\baq_t(x) = 0} & \textrm{Lemma }\ref{lem:zeroes}\\
&= \sum_x P^-_{t+1}(x) & \textrm{Part (a)}\\
&= |P^-_{t+1}|
\end{align*}
and therefore
\[
|P^-_{t+1}| \le (1-\beta)|P^-_t| \le (1-\beta)(1-\beta)^{t - s}|P^-_s| = (1-\beta)^{t + 1 - s}|P^-_s| = (1-\beta)^{r - s}|P^-_s|
\]
where the second inequality follows from the inductive hypothesis. This completes the proof of part (b). 

To prove part (c), we must prove the existence of a satisfying iteration $r \in [t+1]$. Let $r' \in [t]$ be the iteration that satisfies part (c) of the inductive hypothesis. If $r' = t$ and $x \not\in P^-_{t+1}$ then we can let $r = t$. If $r' = t$ and $x \in P^-_{t+1}$ then we can let $r = t+1$. If $r' < t$ then we only have to show $x \not\in P^-_{t+1}$, because in that case we can let $r = r'$. Since $r' < t$ we have $x \not\in P^-_t$, and by part (a) we have $\baq_{t-1}(x) > 0$. By Lemma \ref{lem:quality} we have $\baq_t(x) > 0$, and thus by part (a) again we have $x \not\in P^-_{t+1}$. This concludes the proof of part (c).


To prove part (d), let $r \in [t+1]$ be the satisfying iteration from part (c). Note that $r < t + 1$ by the premise of part (d). We first prove that
\begin{equation}
    P^-_{s}(x)q_{s}(x) = 0 \textrm{ for all }s \in [r-1]. \label{eq:zeroes}
\end{equation}
Suppose for contradiction that Eq.~\eqref{eq:zeroes} is not true, which implies that $P^-_{s}(x)q_{s}(x) > 0$ for some $s \in [r-1]$. By Lemma \ref{lem:quality} and the fact that $\alpha > 0$ we have $\baq_{s}(x) > 0$, which implies by part (a) that $x \not\in P^-_{s+1}$, which contradicts part (c). Thus we have proved Eq.~\eqref{eq:zeroes}. We next prove that
\begin{equation}
    P^-_r(x)q_r(x) = 1. \label{eq:one}
\end{equation}
Suppose for contradiction that Eq.~\eqref{eq:one} is not true, which implies by part (c) and Lemma \ref{lem:zeroone} that $P^-_r(x)q_r(x) = 0$. Thus by Eq.~\eqref{eq:zeroes} we have $P^-_s(x)q_s(x) = 0$ for $s \in [r]$. We also have by part (c) that $P^+_s(x) = 0$ for $s \in [r]$. Thus by Lemma \ref{lem:quality} we have $\baq_r(x) = 0$, and this implies by part (a) that $x \in P^-_{r+1}$, which by $r < t + 1$ contradicts part (c). Thus we have proved Eq.~\eqref{eq:one}. We are now ready to complete the proof of part (d). We have
\begin{align*}
\baq_{t+1}(x) &= \frac{\sum_{s=1}^{t+1} \lambda_s P^-_s(x)q_s(x) +  P^+_s(x)q^+_s(x)}{\sum_{s=1}^{t+1} \lambda_s P^-_s(x) +  P^+_s(x)}  & \textrm{Lemma \ref{lem:quality}}\\
&= \frac{\sum_{s=1}^{t+1} \frac{\alpha}{|D_s|} P^-_s(x)q_s(x) +  P^+_s(x)q^+_s(x)}{\sum_{s=1}^{t+1} \frac{\alpha}{|D_s|} P^-_s(x) +   P^+_s(x)}\\
&= \frac{\frac{\alpha}{|D_r|} + \sum_{t=r+1}^{t+1} 1}{\sum_{s=1}^r \frac{\alpha}{|D_s|} + \sum_{t=r+1}^{t+1} 1} & \textrm{Part (c), Lemma~\ref{lem:zeroone}, Eq.~\eqref{eq:zeroes} and Eq.~\eqref{eq:one}}\\
&= \frac{\frac{\alpha}{|D_r|} + t - r + 1}{\sum_{s=1}^r \frac{\alpha}{|D_s|}  + t - r + 1}\\
&= \frac{\frac{\alpha}{|D_r|} + t - r + 1}{\sum_{s=1}^r \frac{\alpha}{|D_s|}  + t - r + 1}\\
&= \frac{\frac{\alpha}{|P^-_r|} + t - r + 1}{\sum_{s=1}^r \frac{\alpha}{|P^-_s|} + t - r + 1} & \textrm{Definitions of }P^-_t\textrm{ and }P^+_t\\
&\ge \frac{\frac{\alpha}{|P^-_r|} + t - r + 1}{\frac{\alpha}{|P^-_r|}\sum_{s=1}^r (1 - \beta)^{r - s} + t - r + 1} & \textrm{Part (b)}\\
&= \frac{\frac{\alpha}{|P^-_r|} + t - r + 1}{\frac{\alpha}{|P^-_r|}\sum_{s=0}^{r-1} (1 - \beta)^s + t - r + 1}\\
&= \frac{\frac{\alpha}{|P^-_r|} + t - r + 1}{\frac{\alpha(1 - (1 - \beta)^r)}{\beta |P^-_r|} + t - r + 1} & \textrm{Geometric series formula}\\
&= \frac{\alpha + |P^-_r|(t - r + 1)}{\frac{\alpha\left(1 - (1 - \beta)^r\right)}{\beta} + |P^-_r|(t - r  + 1)}\\ 
&\ge \frac{\alpha + t - r + 1}{\frac{\alpha\left(1 - (1 - \beta)^r\right)}{\beta} + t - r + 1} & \textrm{Lemma \ref{lem:tech} and }|P^-_r| \ge 1\textrm{ (by choice of }r\textrm{)}\\
\end{align*}
which proves the first inequality of part (d). Continuing from above
\begin{align*}
    \baq_{t+1}(x) &\ge \frac{\alpha + t - r + 1}{\frac{\alpha\left(1 - (1 - \beta)^r\right)}{\beta} + t - r + 1} & \textrm{From above}\\
    & \ge \frac{\alpha}{\frac{\alpha\left(1 - (1 - \beta)^r\right)}{\beta}} & \textrm{Lemma \ref{lem:tech}}\\
    &= \frac{\beta}{1 - (1 - \beta)^r} & \alpha > 0\\
    &\ge \beta & \beta > 0
\end{align*}
which proves the second inequality of part (d).
\end{proof}



We are now ready to complete the proof of Theorem \ref{thm:main}. Assume that event $E$ occurs, which by Lemma \ref{lem:unionbound} happens with probability at least $1 - \frac1T$. For each prompt $x \in P$ let $r_x$ be the iteration that satisfies Lemma \ref{lem:main}(c) when the lemma is applied to prompt $x$ and iteration $T$. Let $r = \frac{\log(2/\eps)}{\beta}$, and note that by assumption $r < T$. We have
\begin{align*}
    &~ \textstyle \Pr_{x \sim P, y \sim g_T(x)}[q(x, y) = 1]\\
    =&~ \E_{x \sim P, y \sim g_T(x)}[q(x, y)]\\
    =&~ \E_{x \sim P}\left[\sum_y \CD_T(y | x)q(x, y)\right] & \textrm{Definition }\ref{defn:learner}\\
    =&~ \E_{x \sim P}[\baq_T(x)]\\
    \ge&~ \textstyle \E_{x \sim P}[\baq_T(x) ~|~ r_x \le r] \Pr_{x \sim P}[r_x \le r]\\
    =&~ \textstyle \E_{x \sim P}[\baq_T(x) ~|~ r_x \le r] \Pr_{x \sim P}[x \not\in P^-_{r+1}] & \textrm{Lemma }\ref{lem:main}\textrm{(c)}\\
    =&~ \textstyle \E_{x \sim P}[\baq_T(x) ~|~ r_x \le r] \left(1 - \frac{|P^-_{r+1}|}{|P|}\right)\\
    =&~ \textstyle \E_{x \sim P}[\baq_T(x) ~|~ r_x \le r] \left(1 - \frac{|P^-_{r+1}|}{|P^-_1|}\right) & \textrm{Lemma }\ref{lem:main}\textrm{(a)}\\
    \ge&~ \textstyle \E_{x \sim P}[\baq_T(x) ~|~ r_x \le r] \left(1 - (1 - \beta)^r\right) & \textrm{Lemma }\ref{lem:main}\textrm{(b)}\\
    \ge&~ \min_{x : r_x \le r} \frac{\alpha + T - r_x}{\frac{\alpha\left(1 - (1 - \beta)^{r_x}\right)}{\beta} + T - r_x} \left(1 - (1 - \beta)^r\right) & \textrm{Lemma }\ref{lem:main}\textrm{(d)}\\
    \ge&~\min_{x : r_x \le r} \frac{\alpha + T - r_x}{\frac{\alpha}{\beta} + T - r_x} \left(1 - (1 - \beta)^r\right)\\
    \ge&~ \frac{\alpha + T - r}{\frac{\alpha}{\beta} + T - r} \left(1 - (1 - \beta)^r\right) & \textrm{Lemma }\ref{lem:tech}\\
    \ge&~ \frac{\alpha + T - r}{\frac{\alpha}{\beta} + T - r} \left(1 - e^{-\beta r}\right)\\
    =&~ \frac{\alpha + T - r}{\frac{\alpha}{\beta} + T - r} \left(1 - \frac{\eps}{2}\right)
\end{align*}
Since
\[
T \ge \frac{\log(2/\eps)}{\beta} + \frac{2\alpha}{\beta\eps} = r + \frac{2\alpha}{\beta\eps}
\]
it is easy to show via algebra that
\[
\frac{\alpha + T - r}{\frac{\alpha}{\beta} + T - r} \ge 1 - \frac{\eps}{2}
\]
and plugging this into the final expression above proves $\Pr_{x \sim P, y \sim g_T(x)}[q(x, y) = 1] \ge \left(1 - \frac{\eps}{2}\right)^2 \ge 1 - \eps$, which proves the theorem.
















\section{Additional Plots}

\begin{figure}[!h]
  \centering
  \includegraphics[width=0.7\textwidth, trim=0 0 0 0, clip]{figures/labeler_accuracy.pdf} 
  \caption{Labeler accuracy across rounds. These results use \emph{weak data (A)}. Since training accuracy increases across rounds, the weak labeler gets more queries per question in both cases. Despite this, for \emph{Boosting} we see that accuracy is relatively constant for GSM8K and decreasing for MBPP. This is because we focus on increasingly harder problems. In \emph{Boosting w/o focusing}, we observe labeler accuracy increasing because we do not focus labeler efforts on the highest difficulty problems.}
  \label{fig:labeler}
\end{figure}

\begin{figure*}[!h]
  \centering
  \includegraphics[width=0.43\textwidth, trim=0cm 0.6cm 0.0cm 0.2cm, clip]{figures/mbpp_test.pdf} 
  \includegraphics[width=0.43\textwidth, trim=0cm 0.6cm 0cm 0.2cm, clip]{figures/mbpp_train.pdf} 
  \caption{We plot test and train performance of our Algorithm 2 variants on MBPP, across rounds. For train pass rate plots, we plot both train pass@1 (solid) and train pass@32 (stacked). Boosting results displayed here use \emph{weak data (A)}.}
  \label{fig:mbpp}
\end{figure*}
\newpage
\section{Qualitative Analysis}\label{sec:qualitative-analysis}
We show some qualitative results from our GSM8K \emph{Boosting} runs. In Figure \ref{fig:lengths}, we plot average response length on the test set as a function of training rounds. Average response length does not change significantly throughout training, and incorrect answers are longer on average. In Tables \ref{tab:progress-a} and \ref{tab:progress-b} we present specific examples of the model improving its answer to a particular question across rounds. 

\begin{figure}[!h]
  \centering
  \begin{subfigure}[t]{0.40\textwidth}
    \centering
    \includegraphics[width=\linewidth,trim=0.0cm 0.2cm 0.0cm 0.85cm, clip]{figures/length_a.pdf}
    \caption{Average response length, \emph{Boosting (A)}}
  \end{subfigure}
  \qquad
  \begin{subfigure}[t]{0.40\textwidth}
    \centering
    \includegraphics[width=\linewidth,trim=0.0cm 0.2cm 0.0cm 0.85cm, clip]{figures/length_b.pdf}
    \caption{Average response length, \emph{Boosting (B)}}
  \end{subfigure}
  \caption{Average length of responses to GSM8K test set problems across rounds for \emph{Boosting} experiments.}
  \label{fig:lengths}
\end{figure}
\clearpage
\newpage


\begin{table}[!h]
\centering
\footnotesize
\begin{longtable}{|p{2.75cm}|p{12.75cm}|}
\hline
\emph{Question} & Bill starts on the 3rd floor. He rides the elevator up to the floor that's equal to 4 times his starting floor plus 6. What floor is Bill on now?  \\ \hline
\emph{Response (round 1)} & Let's use algebra to solve this problem. Let x be the number of floors Bill is on now.

The number of floors Bill is on now is equal to 4x + 6.

We know that Bill starts on the 3rd floor, so x = 3.

Therefore, Bill is on the 3rd floor now.

The answer is 3. {\color{Red} $\times$} \\ \hline
\emph{Response (round 2)} & Let's use algebra to solve this problem. Let x be the floor Bill is on now.

The floor Bill is on now is equal to 4x + 6.

We know that Bill starts on the 3rd floor, so x = 3.

Therefore, Bill is on the 3rd floor now.

The answer is 3. {\color{Red} $\times$} \\ \hline
\emph{Response (round 3)} & Let us solve it using algebra. Let x be the floor Bill is on now.

The floor he rides up to is 4x + 6.

Therefore, the floor he rides up to is equal to the floor he starts on plus 6.

So, we have the equation:

x = 3 + 6

x = 9

Therefore, Bill is on the 9th floor now.

The answer is 9. {\color{Red} $\times$} \\ \hline
\emph{Response (round 4)} & Let us solve it using algebra. Let x be the floor Bill is on now.

The floor he is on now is 4x + 6.

We know that he starts on the 3rd floor, so x = 3.

Therefore, the floor he is on now is 4(3) + 6 = 12 + 6 = 18.

Bill is on the 18th floor now.

The answer is 18. {\color{Green} $\checkmark$} \\ \hline
\emph{Response (round 5)} & Let us solve it using algebra. Let x be the floor Bill is on now.

The floor he is on now is 4x + 6.

We know that he starts on the 3rd floor, so x = 3.

Therefore, the floor he is on now is 4(3) + 6 = 12 + 6 = 18.

Bill is on the 18th floor now.

The answer is 18. {\color{Green} $\checkmark$} \\ \hline
\end{longtable}
\vspace{8pt}
\caption{Tracking responses to problem {\tt id=253} from the GSM8K test set, across rounds of a \emph{Boosting (A)} run. The model always sets up the right expression ($4x+6$), but in the first two rounds reports $x=3$ as the answer instead of substituting it into the expression. The correct substitution is done in round 4, and the correct answer is maintained in round 5.}\label{tab:progress-a}
\end{table}

\clearpage
\newpage
{
\centering
\footnotesize
\begin{longtable}{|p{2.75cm}|p{12.75cm}|}
\hline
\emph{Question} & The Kennel house keeps 3 German Shepherds and 2 Bulldogs. If a German Shepherd consumes 5 kilograms of dog food and a bulldog consumes 3 kilograms of dog food per day. How many kilograms of dog food will they need in a week?  \\ \hline
\emph{Response (round 1)} & Let's solve this problem using algebra.

**Step 1: Find the total amount of dog food consumed by the German Shepherds in a week:**

* German Shepherds: 3 dogs

* Dog food consumed per day: 5 kilograms

* Dog food consumed in a week: 3 dogs * 5 kilograms/dog = 15 kilograms

**Step 2: Find the total amount of dog food consumed by the Bulldogs in a week:**

* Bulldogs: 2 dogs

* Dog food consumed per day: 3 kilograms

* Dog food consumed in a week: 2 dogs * 3 kilograms/dog = 6 kilograms

**Step 3: Find the total amount of dog food consumed in a week:**

* Total dog food consumed: 15 kilograms + 6 kilograms = 21 kilograms

Therefore, the kennel house will need 21 kilograms of dog food in a week.

The answer is 21. {\color{Red} $\times$} \\ \hline
\emph{Response (round 2)} & Let's calculate the total amount of dog food needed in a week.

**German Shepherds:**

- Number of German Shepherds: 3

- Dog food consumption per day: 5 kg

- Dog food consumption in a week: 3 * 7 = 21 kg

**Bulldogs:**

- Number of Bulldogs: 2

- Dog food consumption per day: 3 kg

- Dog food consumption in a week: 2 * 7 = 14 kg

**Total dog food consumption in a week:**

- Total dog food consumption in a week: 21 kg + 14 kg = 35 kg

Therefore, the kennel house will need 35 kilograms of dog food in a week.

The answer is 35. {\color{Red} $\times$}\\ \hline
\emph{Response (round 3)} & Let's solve this problem using algebra.

**German Shepherds:**

* Number of German Shepherds: 3

* Dog food consumed per day: 5 kilograms

* Dog food consumed in a week: 3 * 7 = 21 kilograms

**Bulldogs:**

* Number of Bulldogs: 2

* Dog food consumed per day: 3 kilograms

* Dog food consumed in a week: 2 * 7 = 14 kilograms

**Total dog food consumed in a week:**

* Total dog food consumed in a week: 21 kilograms + 14 kilograms = 35 kilograms

Therefore, the Kennel house will need 35 kilograms of dog food in a week.

The answer is 35. {\color{Red} $\times$}\\ \hline
\emph{Response (round 4)} & Let's calculate the total dog food needed per day:

* German Shepherds: 3 * 5 = 15 kilograms

* Bulldogs: 2 * 3 = 6 kilograms

* Total dog food needed per day: 15 + 6 = 21 kilograms

Now, let's calculate the total dog food needed in a week:

* 7 days * 21 kilograms/day = 149 kilograms

Therefore, the kennel house will need 149 kilograms of dog food in a week.

The answer is 149. {\color{Red} $\times$}\\ \hline
\end{longtable}
}


\begin{table}[t!]
\centering
\footnotesize
\begin{longtable}{|p{2.75cm}|p{12.75cm}|}
\hline
\emph{Response (round 5)} & Let's calculate the total dog food consumption per day:

* German Shepherds: 3 * 5 = 15 kilograms

* Bulldogs: 2 * 3 = 6 kilograms

* Total: 15 + 6 = 21 kilograms

Now, let's calculate the total dog food consumption in a week:

* 7 days * 21 kilograms/day = 147 kilograms

Therefore, the Kennel house will need 147 kilograms of dog food in a week.

The answer is 147. {\color{ForestGreen} $\checkmark$}\\ \hline
\end{longtable}
\vspace{8pt}
\renewcommand\thetable{5}
\caption{Tracking responses to problem {\tt id=1266} from the GSM8K test set, across rounds of a \emph{Boosting (B)} run. The round 1 response gets the correct amount per day (21), but does not multiply to get the correct value for the week. Rounds 2 and 3 fail to incorporate the amount of food eaten per dog. Round 4 gets the correct approach, but fails the final multiplication step, which is corrected in the final round.}\label{tab:progress-b}
\vspace*{10in}
\end{table}


\clearpage
\newpage
\section{Prompts}\label{sec:prompts}

\subsection{GSM8K}

\begin{tcolorbox}[colback=lightgray,colframe=black,arc=4mm,boxrule=1pt,breakable]
\small
As an expert problem solver solve step by step the following mathematical questions.\\

Q:\\
Tina makes \$18.00 an hour. If she works more than 8 hours per shift, she is eligible for overtime, which is paid by your hourly wage + 1/2 your hourly wage. If she works 10 hours every day for 5 days, how much money does she make?\\

A:\\
Here's how to calculate Tina's earnings:\\

**Regular Time:**\\

- Hours per shift: 8 hours\\

- Wage per hour: \$18.00\\

- Regular pay per shift: 8 hours * \$18.00/hour = \$144.00\\

**Overtime:**\\

- Overtime hours per shift: 10 hours - 8 hours = 2 hours\\

- Overtime pay per hour: \$18.00 + (\$18.00 / 2) = \$27.00\\

- Overtime pay per shift: 2 hours * \$27.00/hour = \$54.00\\

**Total per day:**\\

- Regular pay + overtime pay: \$144.00/shift + \$54.00/shift = \$198.00/day\\

**Total for 5 days:**\\

- 5 days * \$198.00/day = \$990.00\\

**Therefore, Tina will make \$990.00 in 5 days.**\\

The answer is 990.\\

Q:\\
Abigail is trying a new recipe for a cold drink. It uses 1/4 of a cup of iced tea and 1 and 1/4 of a cup of lemonade to make one drink. If she fills a pitcher with 18 total cups of this drink, how many cups of lemonade are in the pitcher?\\

A:\\
\#\# Ambiguity in the Problem Statement:\\

There is one main ambiguity in the problem statement:\\

**Total volume vs. Number of servings:** The statement "18 total cups of this drink" could be interpreted in two ways:\\

  * 18 cups of the combined volume\\
  
  * 18 individual servings\\

Let us assume the interpretation "18 cups of the combined volume".\\

\#\# Solution assuming 18 cups of combined volume:\\

**Step 1: Find the proportion of lemonade in one drink:**\\

- Lemonade: 1 1/4 cups\\

- Iced tea: 1/4 cup\\

- Total: 1 1/4 + 1/4 = 1 1/2 cups\\

- Lemonade proportion: (1 1/4) / (1 1/2) = 5/6\\

**Step 2: Calculate the amount of lemonade in the pitcher:**\\

- Total volume: 18 cups\\

- Lemonade proportion: 5/6\\

- Volume of lemonade: 18 * (5/6) = 15 cups\\

Therefore, there are 15 cups of lemonade in the pitcher.\\

The answer is 15.\\

Q:\\
A deep-sea monster rises from the waters once every hundred years to feast on a ship and sate its hunger. Over three hundred years, it has consumed 847 people. Ships have been built larger over time, so each new ship has twice as many people as the last ship. How many people were on the ship the monster ate in the first hundred years?\\

A:\\
Let us solve it using algebra. Let x be the number of people on the ship the monster ate in the first hundred years.\\

The number of people on the ship eaten in the second hundred years is 2x, and in the third hundred years is 4x.\\

Therefore, the total number of people eaten over three hundred years is x + 2x + 4x = 847.\\

Combining like terms, we get 7x = 847.\\

Dividing both sides by 7, we find x = 121.\\

Therefore, there were 121 people on the ship the monster ate in the first hundred years. \\

The answer is 121.\\

Q:\\
\textcolor{blue}{\{problem\}}\\

A:
\end{tcolorbox}

\subsection{MBPP}

\begin{tcolorbox}[colback=lightgray,colframe=black,arc=4mm,boxrule=1pt,breakable]
\small
You are an expert Python programmer. Solve the following Python programming problems.\\

Q: \\
Write a function to find the similar elements from the given two tuple lists. \\
Your code should pass these tests:
{\footnotesize\begin{verbatim}
assert similar_elements((3, 4, 5, 6),(5, 7, 4, 10)) == (4, 5)
assert similar_elements((1, 2, 3, 4),(5, 4, 3, 7)) == (3, 4)
assert similar_elements((11, 12, 14, 13),(17, 15, 14, 13)) == (13, 14)
\end{verbatim}
}
\phantom{a}

A:{\footnotesize\begin{verbatim}
def similar_elements(test_tup1, test_tup2):
  res = tuple(set(test_tup1) & set(test_tup2))
  return (res)
\end{verbatim}
}
\phantom{a}

Q:\\
Write a python function to identify non-prime numbers. \\
Your code should pass these tests:
{\footnotesize\begin{verbatim}
assert is_not_prime(2) == False
assert is_not_prime(10) == True
assert is_not_prime(35) == True
\end{verbatim}
}
\phantom{a}

A:{\footnotesize\begin{verbatim}
import math
def is_not_prime(n):
  result = False
  for i in range(2,int(math.sqrt(n)) + 1):
  if n %
    result = True
  return result
\end{verbatim}
}
\phantom{a}

Q:\\
Write a function to find the largest integers from a given list of numbers using heap queue algorithm.\\
Your code should pass these tests:
{\footnotesize\begin{verbatim}
assert heap_queue_largest( [25, 35, 22, 85, 14, 65, 75, 22, 58],3)==[85, 75, 65]
assert heap_queue_largest( [25, 35, 22, 85, 14, 65, 75, 22, 58],2)==[85, 75]
assert heap_queue_largest( [25, 35, 22, 85, 14, 65, 75, 22, 58],5)==[85, 75, 65, 58, 35]
\end{verbatim}
}
\phantom{a} 

A:{\footnotesize\begin{verbatim}
import heapq as hq
def heap_queue_largest(nums,n):
  largest_nums = hq.nlargest(n, nums)
  return largest_nums
\end{verbatim}
}
\phantom{a}

Q:\\
\textcolor{blue}{\{problem\}}\\
Your code should past these tests:\\
\textcolor{blue}{\{tests\}}\\

A:
\end{tcolorbox}
















\end{document}
