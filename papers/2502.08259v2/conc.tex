%!TEX root =  main.tex
\section{Conclusions}

We explored the offline-to-online learning problem within the multi-armed bandit framework. This problem involves starting with historical, offline data and then improving performance through online interactions. We proposed that a natural way to evaluate algorithm performance in this setting was to compare against the logging policy in short-horizon scenarios, where there was limited opportunity for effective exploration, and against the optimal arm in long-horizon settings, where accumulated data allowed for more informed decision-making. These two objectives are inherently competing, and the distinction between what constituted a short or long horizon depended on the specific instance, which is the central challenge we addressed.

To address this, we introduced a novel algorithm, \algoname, designed to dynamically balance the benefits of the Lower Confidence Bound (\alglcb) and Upper Confidence Bound (\algucb) algorithms. \algoname was shown to adapt seamlessly across different conditions without prior knowledge of whether to prioritize exploration or exploitation, maintaining robust performance across a range of scenarios. %Our theoretical analysis demonstrated that \algoname achieved near-optimal performance relative to the better of \alglcb and \algucb across varying offline-to-online transitions, underscoring its ability to adapt effectively across different offline-to-online learning scenarios.

Our experimental results further supported these findings. Through evaluations on both synthetic and real-world datasets, \algoname consistently demonstrated strong performance across different horizon lengths and problem instances. The experiments highlighted how \algoname effectively interpolated between the strengths of \alglcb and \algucb, confirming its robustness and adaptability in practice.

Overall, our work bridges a critical gap in offline-to-online learning and offers a robust, adaptive approach that we hope will inspire continued exploration in this evolving field. In particular, we believe that the ideas underlying \algoname can extend naturally to more complex settings, such as contextual bandits, reinforcement learning, and nonstationary environments, which reflect more practical scenarios closer to real-world applications.