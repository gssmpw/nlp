@ARTICLE{Li2023-lv,
  title     = "Reward-agnostic Fine-tuning: Provable Statistical Benefits of
               Hybrid Reinforcement Learning",
  author    = "Li, G and Zhan, W and Lee, J D and Chi, Y and Chen, Y",
  journal   = "arXiv preprint arXiv:2305.10282",
  publisher = "arxiv.org",
  year      =  2023,
  url       = "http://arxiv.org/abs/2305.10282"
}

@ARTICLE{Song2022-vg,
  title         = "Hybrid {RL}: Using Both Offline and Online Data Can Make {RL}
                   Efficient",
  author        = "Song, Yuda and Zhou, Yifei and Sekhari, Ayush and Andrew
                   Bagnell, J and Krishnamurthy, Akshay and Sun, Wen",
  journal       = "arXiv [cs.LG]",
  month         =  oct,
  year          =  2022,
  url           = "http://arxiv.org/abs/2210.06718",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG"
}

@inproceedings{ball2023efficient,
  title={Efficient online reinforcement learning with offline data},
  author={Ball, Philip J and Smith, Laura and Kostrikov, Ilya and Levine, Sergey},
  booktitle={International Conference on Machine Learning},
  pages={1577--1594},
  year={2023},
  organization={PMLR}
}


% --------------------------------------------------------------


@inproceedings{wu2016conservative, author = {Wu, Yifan and Shariff, Roshan and Lattimore, Tor and Szepesv\'{a}ri, Csaba}, title = {Conservative bandits}, year = {2016}, publisher = {JMLR.org}, abstract = {We study a novel multi-armed bandit problem that models the challenge faced by a company wishing to explore new strategies to maximize revenue whilst simultaneously maintaining their revenue above a fixed baseline, uniformly over time. While previous work addressed the problem under the weaker requirement of maintaining the revenue constraint only at a given fixed time in the future, the design of those algorithms makes them unsuitable under the more stringent constraints. We consider both the stochastic and the adversarial settings, where we propose natural yet novel strategies and analyze the price for maintaining the constraints. Amongst other things, we prove both high probability and expectation bounds on the regret, while we also consider both the problem of maintaining the constraints with high probability or expectation. For the adversarial setting the price of maintaining the constraint appears to be higher, at least for the algorithm considered. A lower bound is given showing that the algorithm for the stochastic setting is almost optimal. Empirical results obtained in synthetic environments complement our theoretical findings.}, booktitle = {Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48}, pages = {1254–1262}, numpages = {9}, location = {New York, NY, USA}, series = {ICML'16} }

@article{gur2020adaptive,
  title={Adaptive sequential experiments with unknown information arrival processes},
  author={Gur, Yonatan and Momeni, Ahmadreza},
  journal={Manufacturing \& Service Operations Management},
  volume={24},
  number={5},
  pages={2666--2684},
  year={2022},
  publisher={INFORMS}
}

@inproceedings{Xiao2021OnTO,
  title={On the Optimality of Batch Policy Optimization Algorithms},
  author={Chenjun Xiao and Yifan Wu and Tor Lattimore and Bo Dai and Jincheng Mei and Lihong Li and Csaba Szepesvari and Dale Schuurmans},
  booktitle={International Conference on Machine Learning},
  year={2021},
  url={https://api.semanticscholar.org/CorpusID:233033732}
}

@inproceedings{fujimoto2019offpolicydeepreinforcementlearning,
  title={Off-Policy Deep Reinforcement Learning without Exploration},
  author={Scott Fujimoto and David Meger and Doina Precup},
  booktitle={International Conference on Machine Learning},
  year={2018},
  url={https://api.semanticscholar.org/CorpusID:54457299}
}

@article{thompsonclinical,
 ISSN = {00063444},
 URL = {http://www.jstor.org/stable/2332286},
 author = {William R. Thompson},
 journal = {Biometrika},
 number = {3/4},
 pages = {285--294},
 publisher = {[Oxford University Press, Biometrika Trust]},
 title = {On the Likelihood that One Unknown Probability Exceeds Another in View of the Evidence of Two Samples},
 urldate = {2024-08-06},
 volume = {25},
 year = {1933}
}

@article{advertising,
author = {Schwartz, Eric M. and Bradlow, Eric T. and Fader, Peter S.},
title = {Customer Acquisition via Display Advertising Using Multi-Armed Bandit Experiments},
journal = {Marketing Science},
volume = {36},
number = {4},
pages = {500-522},
year = {2017},
doi = {10.1287/mksc.2016.1023},

URL = {  
        https://doi.org/10.1287/mksc.2016.1023
},
eprint = {    
        https://doi.org/10.1287/mksc.2016.1023
}
}



@inproceedings{zheng2023adaptivepolicylearningofflinetoonline,
author = {Zheng, Han and Luo, Xufang and Wei, Pengfei and Song, Xuan and Li, Dongsheng and Jiang, Jing},
title = {Adaptive policy learning for offline-to-online reinforcement learning},
year = {2023},
isbn = {978-1-57735-880-0},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v37i9.26345},
doi = {10.1609/aaai.v37i9.26345},
abstract = {Conventional reinforcement learning (RL) needs an environment to collect fresh data, which is impractical when online interactions are costly. Offline RL provides an alternative solution by directly learning from the previously collected dataset. However, it will yield unsatisfactory performance if the quality of the offline datasets is poor. In this paper, we consider an offline-to-online setting where the agent is first learned from the offline dataset and then trained online, and propose a framework called Adaptive Policy Learning for effectively taking advantage of offline and online data. Specifically, we explicitly consider the difference between the online and offline data and apply an adaptive update scheme accordingly, that is, a pessimistic update strategy for the offline dataset and an optimistic/greedy update scheme for the online dataset. Such a simple and effective method provides a way to mix the offline and online RL and achieve the best of both worlds. We further provide two detailed algorithms for implementing the framework through embedding value or policy-based RL algorithms into it. Finally, we conduct extensive experiments on popular continuous control tasks, and results show that our algorithm can learn the expert policy with high sample efficiency even when the quality of offline dataset is poor, e.g., random dataset.},
booktitle = {Proceedings of the Thirty-Seventh AAAI Conference on Artificial Intelligence and Thirty-Fifth Conference on Innovative Applications of Artificial Intelligence and Thirteenth Symposium on Educational Advances in Artificial Intelligence},
articleno = {1276},
numpages = {9},
series = {AAAI'23/IAAI'23/EAAI'23}
}

@inproceedings{lee2021offlinetoonlinereinforcementlearningbalanced,
  title={Offline-to-Online Reinforcement Learning via Balanced Replay and Pessimistic Q-Ensemble},
  author={Seunghyun Lee and Younggyo Seo and Kimin Lee and P. Abbeel and Jinwoo Shin},
  booktitle={Conference on Robot Learning},
  year={2021},
  url={https://api.semanticscholar.org/CorpusID:235694361}
}

@article{xie2021policy,
  title={Policy finetuning: Bridging sample-efficient offline and online reinforcement learning},
  author={Xie, Tengyang and Jiang, Nan and Wang, Huan and Xiong, Caiming and Bai, Yu},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={27395--27407},
  year={2021}
}

@book{sutton2018reinforcement,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  year={2018},
  publisher={MIT press}
}

@inproceedings{agrawal2012analysis,
  title={Analysis of thompson sampling for the multi-armed bandit problem},
  author={Agrawal, Shipra and Goyal, Navin},
  booktitle={Conference on learning theory},
  pages={39--1},
  year={2012},
  organization={JMLR Workshop and Conference Proceedings}
}

@article{auer2010ucb,
  title={UCB revisited: Improved regret bounds for the stochastic multi-armed bandit problem},
  author={Auer, Peter and Ortner, Ronald},
  journal={Periodica Mathematica Hungarica},
  volume={61},
  number={1-2},
  pages={55--65},
  year={2010},
  publisher={Akad{\'e}miai Kiad{\'o}, co-published with Springer Science+ Business Media BV~…}
}
@book{lattimore2020bandit,
  title={Bandit algorithms},
  author={Lattimore, Tor and Szepesv{\'a}ri, Csaba},
  year={2020},
  publisher={Cambridge University Press}
}

@article{swaminathan2015batch,
  title={Batch learning from logged bandit feedback through counterfactual risk minimization},
  author={Swaminathan, Adith and Joachims, Thorsten},
  journal={The Journal of Machine Learning Research},
  volume={16},
  number={1},
  pages={1731--1755},
  year={2015},
  publisher={JMLR. org}
}

@article{wu2019behavior,
  title={Behavior regularized offline reinforcement learning},
  author={Wu, Yifan and Tucker, George and Nachum, Ofir},
  journal={arXiv preprint arXiv:1911.11361},
  year={2019}
}

@article{kidambi2020morel,
  title={Morel: Model-based offline reinforcement learning},
  author={Kidambi, Rahul and Rajeswaran, Aravind and Netrapalli, Praneeth and Joachims, Thorsten},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={21810--21823},
  year={2020}
}


@inproceedings{xiao2021optimalitybatchpolicyoptimization,
  title={On the Optimality of Batch Policy Optimization Algorithms},
  author={Chenjun Xiao and Yifan Wu and Tor Lattimore and Bo Dai and Jincheng Mei and Lihong Li and Csaba Szepesvari and Dale Schuurmans},
  booktitle={International Conference on Machine Learning},
  year={2021},
  url={https://api.semanticscholar.org/CorpusID:233033732}
}


@inproceedings{rashidinejad2023bridgingofflinereinforcementlearning,
author = {Rashidinejad, Paria and Zhu, Banghua and Ma, Cong and Jiao, Jiantao and Russell, Stuart},
title = {Bridging offline reinforcement learning and imitation learning: a tale of pessimism},
year = {2024},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Offline (or batch) reinforcement learning (RL) algorithms seek to learn an optimal policy from a fixed dataset without active data collection. Based on the composition of the offline dataset, two main methods are used: imitation learning which is suitable for expert datasets, and vanilla offline RL which often requires uniform coverage datasets. From a practical standpoint, datasets often deviate from these two extremes and the exact data composition is usually unknown. To bridge this gap, we present a new offline RL framework that smoothly interpolates between the two extremes of data composition, hence unifying imitation learning and vanilla offline RL. The new framework is centered around a weak version of the concentrability coefficient that measures the deviation of the behavior policy from the expert policy alone. Under this new framework, we ask: can one develop an algorithm that achieves a minimax optimal rate adaptive to unknown data composition? To address this question, we consider a lower confidence bound (LCB) algorithm developed based on pessimism in the face of uncertainty in offline RL. We study finite-sample properties of LCB as well as information-theoretic limits in multi-armed bandits, contextual bandits, and Markov decision processes (MDPs). Our analysis reveals surprising facts about optimality rates. In particular, in both contextual bandits and RL, LCB achieves a faster rate of 1/N for nearly-expert datasets compared to the usual rate of 1/√N in offline RL, where N is the batch dataset sample size. In contextual bandits, we prove that LCB is adaptively optimal for the entire data composition range, achieving a smooth transition from imitation learning to offline RL. We further show that LCB is almost adaptively optimal in tabular MDPs.},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {895},
numpages = {15},
series = {NIPS '21}
}

@InProceedings{Imitationlearningross,
  title = 	 {Efficient Reductions for Imitation Learning},
  author = 	 {Ross, Stephane and Bagnell, Drew},
  booktitle = 	 {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
  pages = 	 {661--668},
  year = 	 {2010},
  editor = 	 {Teh, Yee Whye and Titterington, Mike},
  volume = 	 {9},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Chia Laguna Resort, Sardinia, Italy},
  month = 	 {13--15 May},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v9/ross10a/ross10a.pdf},
  url = 	 {https://proceedings.mlr.press/v9/ross10a.html},
  abstract = 	 {Imitation Learning, while applied successfully on many large real-world problems, is typically addressed as a standard supervised learning problem, where it is assumed the training and testing data are i.i.d..  This is not true in imitation learning as the learned policy influences the future test inputs (states) upon which it will be tested. We show that this leads to compounding errors and a regret bound that grows quadratically in the time horizon of the task. We propose two alternative algorithms for imitation learning where training occurs over several episodes of interaction. These two approaches share in common that the learner’s policy is slowly modified from executing the expert’s policy to the learned policy. We show that this leads to stronger performance guarantees and demonstrate the improved performance on two challenging problems: training a learner to play 1) a 3D racing game (Super Tux Kart) and 2) Mario Bros.; given input images from the games and corresponding actions taken by a human expert and near-optimal planner respectively.}
}



@inproceedings{rajaraman2020fundamentallimitsimitationlearning,
author = {Rajaraman, Nived and Yang, Lin F. and Jiao, Jiantao and Ramchandran, Kannan},
title = {Toward the fundamental limits of imitation learning},
year = {2020},
isbn = {9781713829546},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Imitation learning (IL) aims to mimic the behavior of an expert policy in a sequential decision-making problem given only demonstrations. In this paper, we focus on understanding the minimax statistical limits of IL in episodic Markov Decision Processes (MDPs). We first consider the setting where the learner is provided a dataset of $N$ expert trajectories ahead of time, and cannot interact with the MDP. Here, we show that the policy which mimics the expert whenever possible is in expectation $lesssim frac{|mathcal{S}| H^2 log (N)}{N}$ suboptimal compared to the value of the expert, even when the expert plays a stochastic policy. Here $mathcal{S}$ is the state space and $H$ is the length of the episode. Furthermore, we establish a suboptimality lower bound of $gtrsim |mathcal{S}| H^2 / N$ which applies even if the expert is constrained to be deterministic, or if the learner is allowed to actively query the expert at visited states while interacting with the MDP for $N$ episodes. To our knowledge, this is the first algorithm with suboptimality having no dependence on the number of actions, under no additional assumptions. We then propose a novel algorithm based on minimum-distance functionals in the setting where the transition model is given and the expert is deterministic. The algorithm is suboptimal by $lesssim |mathcal{S}| H^{3/2} / N$, matching our lower bound up to a $sqrt{H}$ factor, and breaks the $mathcal{O}(H^2)$ error compounding barrier of IL.},
booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
articleno = {245},
numpages = {11},
location = {Vancouver, BC, Canada},
series = {NIPS '20}
}

@inproceedings{yin2020nearoptimalprovableuniformconvergence,
  title={Near-Optimal Provable Uniform Convergence in Offline Policy Evaluation for Reinforcement Learning},
  author={Ming Yin and Yu Bai and Yu-Xiang Wang},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  year={2021},
  url={https://api.semanticscholar.org/CorpusID:261937056}
}

@inproceedings{NEURIPS2020_f7efa4f8,
 author = {Kidambi, Rahul and Rajeswaran, Aravind and Netrapalli, Praneeth and Joachims, Thorsten},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {21810--21823},
 publisher = {Curran Associates, Inc.},
 title = {MOReL: Model-Based Offline Reinforcement Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/f7efa4f864ae9b88d43527f4b14f750f-Paper.pdf},
 volume = {33},
 year = {2020}
}

@inproceedings{MorelPessimistisOfflineLearning,
 author = {Kidambi, Rahul and Rajeswaran, Aravind and Netrapalli, Praneeth and Joachims, Thorsten},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {21810--21823},
 publisher = {Curran Associates, Inc.},
 title = {MOReL: Model-Based Offline Reinforcement Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/f7efa4f864ae9b88d43527f4b14f750f-Paper.pdf},
 volume = {33},
 year = {2020}
}


@article{jin2022pessimismprovablyefficientoffline,
  title={Is Pessimism Provably Efficient for Offline Reinforcement Learning?},
  author={Jin, Ying and Yang, Zhuoran and Wang, Zhaoran},
  journal={Mathematics of Operations Research},
  year={2024},
  publisher={INFORMS}
}

@article{swaminathan2015counterfactualriskminimizationlearning,
  author  = {Adith Swaminathan and Thorsten Joachims},
  title   = {Batch Learning from Logged Bandit Feedback through Counterfactual Risk Minimization},
  journal = {Journal of Machine Learning Research},
  year    = {2015},
  volume  = {16},
  number  = {52},
  pages   = {1731--1755},
  url     = {http://jmlr.org/papers/v16/swaminathan15a.html}
}
@misc{wu2019behaviorregularizedofflinereinforcement,
      title={Behavior Regularized Offline Reinforcement Learning}, 
      author={Yifan Wu and George Tucker and Ofir Nachum},
      year={2019},
      eprint={1911.11361},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1911.11361}, 
}

@misc{jaques2019wayoffpolicybatchdeep,
      title={Way Off-Policy Batch Deep Reinforcement Learning of Implicit Human Preferences in Dialog}, 
      author={Natasha Jaques and Asma Ghandeharioun and Judy Hanwen Shen and Craig Ferguson and Agata Lapedriza and Noah Jones and Shixiang Gu and Rosalind Picard},
      year={2019},
      eprint={1907.00456},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1907.00456}, 
}

@misc{buckman2020importancepessimismfixeddatasetpolicy,
      title={The Importance of Pessimism in Fixed-Dataset Policy Optimization}, 
      author={Jacob Buckman and Carles Gelada and Marc G. Bellemare},
      year={2020},
      eprint={2009.06799},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2009.06799}, 
}


@inproceedings{li2022pessimismofflinelinearcontextual,
author = {Li, Gene and Ma, Cong and Srebro, Nathan},
title = {Pessimism for offline linear contextual bandits using lp confidence sets},
year = {2024},
isbn = {9781713871088},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We present a family ${widehat{pi}_p}_{pge 1}$ of pessimistic learning rules for offline learning of linear contextual bandits, relying on confidence sets with respect to different πp norms, where $widehat{pi}_2$ corresponds to Bellman-consistent pessimism (BCP), while $widehat{pi}_infty$ is a novel generalization of lower confidence bound (LCB) to the linear setting. We show that the novel $widehat{pi}_infty$ learning rule is, in a sense, adaptively optimal, as it achieves the minimax performance (up to log factors) against all ℓq-constrained problems, and as such it strictly dominates all other predictors in the family, including $widehat{pi}_2$.},
booktitle = {Proceedings of the 36th International Conference on Neural Information Processing Systems},
articleno = {1525},
numpages = {14},
location = {New Orleans, LA, USA},
series = {NIPS '22}
}



@inproceedings{yu2020mopomodelbasedofflinepolicy,
author = {Yu, Tianhe and Thomas, Garrett and Yu, Lantao and Ermon, Stefano and Zou, James and Levine, Sergey and Finn, Chelsea and Ma, Tengyu},
title = {MOPO: model-based offline policy optimization},
year = {2020},
isbn = {9781713829546},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Offline reinforcement learning (RL) refers to the problem of learning policies entirely from a large batch of previously collected data. This problem setting offers the promise of utilizing such datasets to acquire policies without any costly or dangerous active exploration. However, it is also challenging, due to the distributional shift between the offline training data and those states visited by the learned policy. Despite significant recent progress, the most successful prior methods are model-free and constrain the policy to the support of data, precluding generalization to unseen states. In this paper, we first observe that an existing model-based RL algorithm already produces significant gains in the offline setting compared to model-free approaches. However, standard model-based RL methods, designed for the online setting, do not provide an explicit mechanism to avoid the offline setting's distributional shift issue. Instead, we propose to modify the existing model-based RL methods by applying them with rewards artificially penalized by the uncertainty of the dynamics. We theoretically show that the algorithm maximizes a lower bound of the policy's return under the true MDP. We also characterize the trade-off between the gain and risk of leaving the support of the batch data. Our algorithm, Model-based Offline Policy Optimization (MOPO), outperforms standard model-based RL algorithms and prior state-of-the-art model-free offline RL algorithms on existing offline RL benchmarks and two challenging continuous control tasks that require generalizing from data collected for a different task.},
booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
articleno = {1185},
numpages = {14},
location = {Vancouver, BC, Canada},
series = {NIPS '20}
}
@misc{xie2022armormodelbasedframeworkimproving,
      title={ARMOR: A Model-based Framework for Improving Arbitrary Baseline Policies with Offline Data}, 
      author={Tengyang Xie and Mohak Bhardwaj and Nan Jiang and Ching-An Cheng},
      year={2022},
      eprint={2211.04538},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2211.04538}, 
}

@inproceedings{PolicyFinetuning,
 author = {Xie, Tengyang and Jiang, Nan and Wang, Huan and Xiong, Caiming and Bai, Yu},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {27395--27407},
 publisher = {Curran Associates, Inc.},
 title = {Policy Finetuning: Bridging Sample-Efficient Offline and Online Reinforcement Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/e61eaa38aed621dd776d0e67cfeee366-Paper.pdf},
 volume = {34},
 year = {2021}
}


@misc{zhou2023offlinedataenhancedonpolicy,
      title={Offline Data Enhanced On-Policy Policy Gradient with Provable Guarantees}, 
      author={Yifei Zhou and Ayush Sekhari and Yuda Song and Wen Sun},
      year={2023},
      eprint={2311.08384},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2311.08384}, 
}


@InProceedings{MABwithHistory,
  title = 	 {Multi-armed Bandit Problems with History},
  author = 	 {Shivaswamy, Pannagadatta and Joachims, Thorsten},
  booktitle = 	 {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},
  pages = 	 {1046--1054},
  year = 	 {2012},
  editor = 	 {Lawrence, Neil D. and Girolami, Mark},
  volume = 	 {22},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {La Palma, Canary Islands},
  month = 	 {21--23 Apr},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v22/shivaswamy12/shivaswamy12.pdf},
  url = 	 {https://proceedings.mlr.press/v22/shivaswamy12.html},
  abstract = 	 {In this paper we consider the stochastic multi-armed bandit problem. However, unlike in the conventional version of this problem, we do not assume that the algorithm starts from scratch. Many applications offer observations of (some of) the arms even before the algorithm starts.  We propose three novel multi-armed bandit algorithms that can exploit this data. An upper bound on the regret is derived in each case. The results show that a logarithmic amount of historic data  can reduce  regret from logarithmic to constant. The effectiveness of the proposed algorithms  are demonstrated on a large-scale malicious URL detection problem.}
}


@article{bu2021onlinepricingofflinedata,
title = {Online Pricing with Offline Data: Phase Transition and Inverse Square Law},
author = {Bu, Jinzhi and Simchi-Levi, David and Xu, Yunzong},
year = {2022},
journal = {Management Science},
volume = {68},
number = {12},
pages = {8568-8588},
abstract = {This paper investigates the impact of pre-existing offline data on online learning in the context of dynamic pricing. We study a single-product dynamic pricing problem over a selling horizon of T periods. The demand in each period is determined by the price of the product according to a linear demand model with unknown parameters. We assume that before the start of the selling horizon, the seller already has some pre-existing offline data. The offline data set contains n samples, each of which is an input-output pair consisting of a historical price and an associated demand observation. The seller wants to use both the pre-existing offline data and the sequentially revealed online data to minimize the regret of the online learning process. We characterize the joint effect of the size , location , and dispersion of the offline data on the optimal regret of the online learning process. Specifically, the size , location , and dispersion of the offline data are measured by the number of historical samples, the distance between the average historical price and the optimal price, and the standard deviation of the historical prices, respectively. For both single-historical-price setting and multiple-historical-price setting, we design a learning algorithm based on the “Optimism in the Face of Uncertainty” principle, which strikes a balance between exploration and exploitation and achieves the optimal regret up to a logarithmic factor. Our results reveal surprising transformations of the optimal regret rate with respect to the size of the offline data, which we refer to as phase transitions . In addition, our results demonstrate that the location and dispersion of the offline data also have an intrinsic effect on the optimal regret, and we quantify this effect via the inverse-square law .},
keywords = {dynamic pricing; online learning; offline data; phase transition; inverse-square law},
url = {https://EconPapers.repec.org/RePEc:inm:ormnsc:v:68:y:2022:i:12:p:8568-8588}
}


@inproceedings{xie2022policyfinetuningbridgingsampleefficient,
author = {Xie, Tengyang and Jiang, Nan and Wang, Huan and Xiong, Caiming and Bai, Yu},
title = {Policy finetuning: bridging sample-efficient offline and online reinforcement learning},
year = {2024},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Recent theoretical work studies sample-efficient reinforcement learning (RL) extensively in two settings: learning interactively in the environment (online RL), or learning from an offline dataset (offline RL). However, existing algorithms and theories for learning near-optimal policies in these two settings are rather different and disconnected. Towards bridging this gap, this paper initiates the theoretical study of policy finetuning, that is, online RL where the learner has additional access to a "reference policy" μ close to the optimal policy undefined* in a certain sense. We consider the policy finetuning problem in episodic Markov Decision Processes (MDPs) with S states, A actions, and horizon length H. We first design a sharp offline reduction algorithm—which simply executes μ and runs offline policy optimization on the collected dataset—that finds an ε near-optimal policy within \~{O}(H3SC*/ε2) episodes, where C* is the single-policy concentrability coefficient between μ and π*. This offline result is the first that matches the sample complexity lower bound in this setting, and resolves a recent open question in offline RL. We then establish an Ω(H3S min{C*, A}/ε2) sample complexity lower bound for any policy fine-tuning algorithm, including those that can adaptively explore the environment. This implies that—perhaps surprisingly—the optimal policy finetuning algorithm is either offline reduction or a purely online RL algorithm that does not use μ. Finally, we design a new hybrid offline/online algorithm for policy finetuning that achieves better sample complexity than both vanilla offline reduction and purely online RL algorithms, in a relaxed setting where μ only satisfies concentrability partially up to a certain time step. Overall, our results offer a quantitative understanding on the benefit of a good reference policy, and make a step towards bridging offline and online RL.},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {2098},
numpages = {13},
series = {NIPS '21}
}

@article{jin2022policy,
  title={Policy learning" without''overlap: Pessimism and generalized empirical Bernstein's inequality},
  author={Jin, Ying and Ren, Zhimei and Yang, Zhuoran and Wang, Zhaoran},
  journal={arXiv preprint arXiv:2212.09900},
  year={2022}
}

@article{yin2021towards,
  title={Towards instance-optimal offline reinforcement learning with pessimism},
  author={Yin, Ming and Wang, Yu-Xiang},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={4065--4078},
  year={2021}
}

@inproceedings{cheng2022adversarially,
  title={Adversarially trained actor critic for offline reinforcement learning},
  author={Cheng, Ching-An and Xie, Tengyang and Jiang, Nan and Agarwal, Alekh},
  booktitle={International Conference on Machine Learning},
  pages={3852--3878},
  year={2022},
  organization={PMLR}
}

@article{fujimoto2021minimalist,
  title={A minimalist approach to offline reinforcement learning},
  author={Fujimoto, Scott and Gu, Shixiang Shane},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={20132--20145},
  year={2021}
}

@inproceedings{wagenmaker2023leveraging,
  title={Leveraging offline data in online reinforcement learning},
  author={Wagenmaker, Andrew and Pacchiano, Aldo},
  booktitle={International Conference on Machine Learning},
  pages={35300--35338},
  year={2023},
  organization={PMLR}
}

@article{li2024reward,
  title={Reward-agnostic fine-tuning: Provable statistical benefits of hybrid reinforcement learning},
  author={Li, Gen and Zhan, Wenhao and Lee, Jason D and Chi, Yuejie and Chen, Yuxin},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@misc{lattimore2016regretanalysisanytimeoptimally,
      title={Regret Analysis of the Anytime Optimally Confident UCB Algorithm}, 
      author={Tor Lattimore},
      year={2016},
      eprint={1603.08661},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1603.08661}, 
}

@misc{avazu-ctr-prediction,
    author = {Steve Wang and Will Cukierski},
    title = {Click-Through Rate Prediction},
    year = {2014},
    howpublished = {\url{https://kaggle.com/competitions/avazu-ctr-prediction}},
    note = {Kaggle}
}


@inproceedings{guo2017deepfmfactorizationmachinebasedneural,
author = {Guo, Huifeng and Tang, Ruiming and Ye, Yunming and Li, Zhenguo and He, Xiuqiang},
title = {DeepFM: a factorization-machine based neural network for CTR prediction},
year = {2017},
isbn = {9780999241103},
publisher = {AAAI Press},
abstract = {Learning sophisticated feature interactions behind user behaviors is critical in maximizing CTR for recommender systems. Despite great progress, existing methods seem to have a strong bias towards low- or high-order interactions, or require expertise feature engineering. In this paper, we show that it is possible to derive an end-to-end learning model that emphasizes both low- and high-order feature interactions. The proposed model, DeepFM, combines the power of factorization machines for recommendation and deep learning for feature learning in a new neural network architecture. Compared to the latest Wide \& Deep model from Google, DeepFM has a shared input to its "wide" and "deep" parts, with no need of feature engineering besides raw features. Comprehensive experiments are conducted to demonstrate the effectiveness and efficiency of DeepFM over the existing models for CTR prediction, on both benchmark data and commercial data.},
booktitle = {Proceedings of the 26th International Joint Conference on Artificial Intelligence},
pages = {1725–1731},
numpages = {7},
location = {Melbourne, Australia},
series = {IJCAI'17}
}

@misc{wang2017deepcrossnetwork,
      title={Deep \& Cross Network for Ad Click Predictions}, 
      author={Ruoxi Wang and Bin Fu and Gang Fu and Mingliang Wang},
      year={2017},
      eprint={1708.05123},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1708.05123}, 
}

@inproceedings{Lu2020ADI,
  title={A Dual Input-aware Factorization Machine for CTR Prediction},
  author={Wantong Lu and Yantao Yu and Yongzhe Chang and Zhen Wang and Chenhui Li and Bo Yuan},
  booktitle={International Joint Conference on Artificial Intelligence},
  year={2020},
  url={https://api.semanticscholar.org/CorpusID:220484108}
}


@InProceedings{pmlr-v235-cheung24a,
  title = 	 {Leveraging ({B}iased) Information: Multi-armed Bandits with Offline Data},
  author =       {Cheung, Wang Chi and Lyu, Lixing},
  booktitle = 	 {Proceedings of the 41st International Conference on Machine Learning},
  pages = 	 {8286--8309},
  year = 	 {2024},
  editor = 	 {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
  volume = 	 {235},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {21--27 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://raw.githubusercontent.com/mlresearch/v235/main/assets/cheung24a/cheung24a.pdf},
  url = 	 {https://proceedings.mlr.press/v235/cheung24a.html},
  abstract = 	 {We leverage offline data to facilitate online learning in stochastic multi-armed bandits. The probability distributions that govern the offline data and the online rewards can be different. Without any non-trival upper bound on their difference, we show that no non-anticipatory policy can out-perform the UCB policy by (Auer et al. 2002), even in the presence of offline data. In complement, we propose an online policy MIN-UCB, which outperforms UCB when a non-trivial upper bound is given. MIN-UCB adaptively chooses to utilize the offline data when they are deemed informative, and to ignore them otherwise. MIN-UCB is shown to be tight in terms of both instance indepedent and dependent regret bounds. Finally, we corroborate the theoretical results with numerical experiments.}
}


@inbook{Vershynin_2018, place={Cambridge}, series={Cambridge Series in Statistical and Probabilistic Mathematics}, title={Frontmatter}, booktitle={High-Dimensional Probability: An Introduction with Applications in Data Science}, publisher={Cambridge University Press}, author={Vershynin, Roman}, year={2018}, pages={i–ii}, collection={Cambridge Series in Statistical and Probabilistic Mathematics}} 


@misc{deepctr_torch,
  author       = {Weichen Shen},
  title        = {DeepCTR-Torch: Easy-to-use, Modular, and Extendible PyTorch Framework for CTR Prediction},
  year         = {2024},
  url          = {https://github.com/shenweichen/DeepCTR-Torch},
  note         = {Accessed: 2024-11-18}
}

@misc{kaggle_deepctr_difm,
year= 2024,
  author       = {Brain Cai},
  title        = {DeepCTR DIFM: Demonstrating DeepCTR with DIFM model on Kaggle},
  url          = {https://www.kaggle.com/code/braincai/0322-deepctr-difm},
  note         = {Accessed: 2024-11-18}
}

@article{bastani2020online,
  title={Online decision making with high-dimensional covariates},
  author={Bastani, Hamsa and Bayati, Mohsen},
  journal={Operations Research},
  volume={68},
  number={1},
  pages={276--294},
  year={2020},
  publisher={INFORMS}
}

@article{zhou2024sequential,
  title={Sequential learning with a similarity selection index},
  author={Zhou, Yi and Fu, Michael C and Ryzhov, Ilya O},
  journal={Operations Research},
  volume={72},
  number={6},
  pages={2526--2542},
  year={2024},
  publisher={INFORMS}
}

@article{bertsimas2011theory,
  title={Theory and applications of robust optimization},
  author={Bertsimas, Dimitris and Brown, David B and Caramanis, Constantine},
  journal={SIAM review},
  volume={53},
  number={3},
  pages={464--501},
  year={2011},
  publisher={SIAM}
}

@article{ben2002robust,
  title={Robust optimization--methodology and applications},
  author={Ben-Tal, Aharon and Nemirovski, Arkadi},
  journal={Mathematical programming},
  volume={92},
  pages={453--480},
  year={2002},
  publisher={Springer}
}

@article{bu2023offline,
  title={Offline pricing and demand learning with censored data},
  author={Bu, Jinzhi and Simchi-Levi, David and Wang, Li},
  journal={Management Science},
  volume={69},
  number={2},
  pages={885--903},
  year={2023},
  publisher={INFORMS}
}

@article{perakis2008regret,
  title={Regret in the newsvendor model with partial information},
  author={Perakis, Georgia and Roels, Guillaume},
  journal={Operations research},
  volume={56},
  number={1},
  pages={188--203},
  year={2008},
  publisher={INFORMS}
}

@article{xu2022robust,
  title={A robust data-driven approach for the newsvendor problem with nonparametric information},
  author={Xu, Liang and Zheng, Yi and Jiang, Li},
  journal={Manufacturing \& Service Operations Management},
  volume={24},
  number={1},
  pages={504--523},
  year={2022},
  publisher={INFORMS}
}

@article{bertsimas2006robust,
  title={A robust optimization approach to inventory theory},
  author={Bertsimas, Dimitris and Thiele, Aur{\'e}lie},
  journal={Operations research},
  volume={54},
  number={1},
  pages={150--168},
  year={2006},
  publisher={INFORMS}
}

@article{caro2007dynamic,
  title={Dynamic assortment with demand learning for seasonal consumer goods},
  author={Caro, Felipe and Gallien, J{\'e}r{\'e}mie},
  journal={Management science},
  volume={53},
  number={2},
  pages={276--292},
  year={2007},
  publisher={INFORMS}
}

@inproceedings{pandey2007bandits,
  title={Bandits for taxonomies: A model-based approach},
  author={Pandey, Sandeep and Agarwal, Deepak and Chakrabarti, Deepayan and Josifovski, Vanja},
  booktitle={Proceedings of the 2007 SIAM international conference on data mining},
  pages={216--227},
  year={2007},
  organization={SIAM}
}

@inproceedings{madani2005contextual,
  title={Contextual recommender problems},
  author={Madani, Omid and DeCoste, Dennis},
  booktitle={Proceedings of the 1st international workshop on Utility-based data mining},
  pages={86--89},
  year={2005}
}

@article{chen2022data,
  title={Data-pooling reinforcement learning for personalized healthcare intervention},
  author={Chen, Xinyun and Shi, Pengyi and Pu, Shanwen},
  journal={arXiv preprint arXiv:2211.08998},
  year={2022}
}

