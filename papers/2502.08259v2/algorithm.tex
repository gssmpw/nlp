%!TEX root =  main.tex

\section{Offline to Online (\algoname) Algorithm}\label{sec:algorithm}


As it was already mentioned in the introduction and will be demonstrated in \cref{sec:intersetanalysis}, neither \alglcb nor \algucb is the best algorithm for the whole range of horizon length in the offline-to-online setting. Before introducing our algorithm, it will be instrumental in taking a peek at what goes wrong with these methods.

For \algucb, consider a case where all arms but one are observed many times in the offline data. If the horizon is short (can be as short as $1$), then \algucb will pull only the unexplored arm, even if there are good options among the sampled arms. Thus, for a short horizon, it will over-explore. In particular, \algucb will suffer large regret against the logging policy. We will see in \cref{sec:intersetanalysis} that this phenomenon can hinder the performance of \algucb even in cases less extreme than this. Now, as to the failure of \alglcb, its lack of exploration poses a significant issue in cases when good arms are under-sampled in the offline data. As such, it will suffer large regret relative to the optimal arm, especially for large horizons.


As opposed to these, our new algorithm, \algoname, will be shown to achieve both a low regret with respect to the optimal arm and a low regret with respect to the logging policy, no matter what the horizon length is, and thus, automatically finds a balance between optimism and pessimism. Notably, it retains the advantages of both \alglcb and \algucb while avoiding their weaknesses.

At every round, our algorithm computes an exploration budget, detailed in the following paragraph. If the budget is high enough to explore safely, then \algucb, i.e., $U(t):= \argmax_{i \in [K]} \overline{\mu}_i(t)$  is played. Otherwise, the algorithm defaults to the safe option, which is \alglcb, i.e., $L(t):=\argmax_{i \in [K]} \underline{\mu}_i(t)$. At the high level, our algorithm design was inspired by conservative bandits algorithms \citep{wu2016conservative}. 

Now, let us detail the computation of the exploration budget. We will start with the description in the case where the horizon $T$ is known. We explain later how to deal with the unknown horizon case.  Define
\[
\beta\coloneq\frac{\sum_i \sqrt{m_i}}{m}\sqrt{2\log(K/\delta)},
\]
and for $\alpha>0$, a tuning parameter whose role will be explained later, let
\[
\gamma\coloneq \underline{\mu}_{L(0)}(0)-\alpha\beta,
\]
a ``safe'' lower bound on reward collected by the logging policy. In fact, $\gamma$ is also a safe lower bound on the reward that will be collected by the LCB action at every time step. This is because,
by construction, at every iteration $t$, $\underline{\mu}_{L(t)}(t)\geq \underline{\mu}_{L(0)}(0)$, hence $\underline{\mu}_{L(t)}(t)-\alpha\beta \geq \gamma$. As will be seen later, the budget will use $\gamma$ as a benchmark, and this last inequality will imply that at every play of \alglcb, the reward collected exceeds our chosen benchmark by at least $\alpha \beta$. %\todoc{I moved this up, added an explanation of why we state this inequality, although this still feels a bit unfinished; why do we care about stating this inequality?F: added an extra sentence}
The reason we use a lower bound on the mean reward for LCB action at time step $0$ as the benchmark, as opposed to using, say, the actual reward (or an upper bound), is because meeting this benchmark will be easier, which makes the algorithm explore ``earlier'', which is expected to improve performance, while the algorithm is still able to compete with the logging policy.


For each arm $i \in [K]$, we denote $T_i^U(t)$ the number of times arm $i$ has been explored as the \algucb arm, i.e., in iterations where the budget was high enough. We also denote  $T^L_i(t)$ the number of times where arm $i$ was played as the \alglcb arm. Note that $T_i(t)=T^L_i(t)+T_i^U(t)$. We also introduce the notation $T^L(t)\coloneq\sum_{i \in [K]}T_i^L(t)$ for the number of times \alglcb has been played up to iteration $t$. The budget at iteration $t$ is defined as
\begin{equation}\label{eq:budget}
B_{T}(t)\coloneq \sum_{i=1}^{K}T_i^U(t-1)(\underline{\mu}_i(t)-\gamma)+\underline{\mu}_{U(t)}(t)-\gamma+(T^L(t-1)+T-t)\alpha \beta.   
\end{equation}

This budget is the sum of three parts. As will be shown in the analysis, the first one, $\sum_{i=1}^{K}T_i^U(t-1)(\underline{\mu}_i(t)-\gamma)$, is a lower bound of the difference between the total cumulated reward of \algucb iterations played until iteration $t-1$, and a benchmark, $T^U_i(t-1)\gamma$. The second part, $\underline{\mu}_{U(t)}(t)-\gamma$ is a high probability lower bound of the reward that would be obtained by playing \algucb minus $\gamma$ in the current iteration. The rest of \eqref{eq:budget} reflect that, by design, each play of \alglcb increases the budget by $\alpha\beta$. The term $T^{L}(t-1) \alpha \beta$ is the budget accumulated by playing \alglcb up to iteration $t$. The term $(T-t)\alpha \beta$ is the total budget that would be accumulated if the algorithm plays \alglcb from iteration $t+1$ until the end. The underlying function of the budget is to ensure that the reward cumulated by the algorithm does not fall too far below the reward that \alglcb would cumulate.
Lastly, the parameter $\alpha$ determines how stringent the budget constraint is. As will be discussed after the statement of the main theorem, different values for $\alpha$ may be preferred depending on the favored objective.

In the case where the horizon $T$ is unknown, we can use a horizon-doubling technique to set the budget.  Let $\Tilde{T}$ denote a proxy horizon. Initially, we let $\Tilde{T}=2$. The budget is defined as before, but with $\Tilde{T}-t$ instead of $T-t$ before the last closing parenthesis. The algorithm proceeds as before, except that if $t$, the current iteration number, exceeds $\Tilde{T}$, we double the value of $\Tilde{T}$. 

Note that this modification alone is insufficient to handle the unknown horizon. In practice, the parameter $\delta$ in $\underline{\mu}_i(t)$ and $\overline{\mu}_i(t)$ is typically chosen as a function of the horizon, such as $\delta = 1/T^2$. To address this, one can replace $\delta$ with a time-step-dependent parameter, $\delta_t = \delta_0 / t^2$. Using this adjustment, the bounds derived for a fixed $\delta$ extend to the varying $\delta_t$, as $\delta_t \geq \delta_0 / T^2$ and $\sum_{t=1}^T \delta_t \leq \frac{\pi^2}{6} \delta_0$.
While effective, this approach could be further refined. Prior work (e.g., \citet{lattimore2016regretanalysisanytimeoptimally}) has explored more sophisticated methods for addressing unknown horizons in confidence-bound-based algorithms. Adapting such methods to our setting presents an interesting avenue for future research. However, we chose not to pursue this extension here to maintain focus on the core contributions of our work while avoiding additional technical complexity.

The full pseudo-code of the algorithm is given in  \cref{alg:algo1}. 

\begin{remark} 
The specific budget formula may seem a bit arbitrary to the reader. There can be multiple ways to define a budget to achieve the same purpose and ultimately obtain the same theoretical guarantee. We show in the additional experiments section of the appendix, \cref{app:extrasim} a few options and discuss in more detail the rationale behind the chosen formula.
\end{remark}

\begin{algorithm}[ht]
\DontPrintSemicolon
\SetKwInOut{Input}{input}
\Input{ $m_i, \hat{\mu}_i^0$ for $i \in [K]$, parameters $\alpha\geq 0,\delta$, horizon $T$ if known }
Let  $\beta:=\frac{\sum_i \sqrt{m_i}}{m}\sqrt{2\log(\frac{K}{\delta})}$\;
	Let $L(0):=\argmax_{i \in [K]}\underline{\mu}_{i}(0)$ and $\gamma= \underline{\mu}_{L(0)}(0)-\alpha\beta$\; 
 If horizon $T$ unknown, let $\Tilde{T}:=2$, if known let $\Tilde{T}:=T$\;
 \For{$t=1, \ldots,T$}{
 
 \If{$t>\Tilde{T}$}{
 Let $\Tilde{T}:=2\cdot \Tilde{T}$ \tcp*[f]{Update Horizon}
 }
 Compute $\underline{\mu}_i(t)$ and $\overline{\mu}_i(t)$ for each arm $i \in [K]$\;
 Let $U(t):=\argmax_{i \in [K]} \overline{\mu}_i(t)$\;
 Let $B_{\tilde{T}}(t):=\sum_{i=1}^{K}T_i^U(t-1)(\underline{\mu}_i(t)-\gamma)+\underline{\mu}_{U(t)}(t)-\gamma+(T^L(t-1)+\tilde{T}-t)\alpha \beta$\;
 \uIf(\tcp*[f]{Check Budget}){$B_{\tilde{T}}(t)>0$}{Pull $U(t)$ \tcp*[f]{Play \algucb}}
	\uElse{Pull $L(t)$ \tcp*[f]{Play \alglcb}}
 }
	\caption{\algoname} \label{alg:algo1}
\end{algorithm}

\begin{theorem}\label{thm:boundofftoon}
    For any $\theta\in \Theta$, any $1\leq t\leq T$, with probability at least $1-2T\delta$, the following hold:
    \begin{align}\label{eq:regretlogOtO}
            R^{\text{log}}_{\algoname}(t)\leq t\left(1+(1+\mathds{1}_{\text{T unknown}})\alpha\right)\beta\,,
    \end{align}
\begin{align}\label{eq:regretOtO}
       R_{\algoname}(t)\leq \sum_{i=1}^K\Delta_i\left(\frac{4\log(K/\delta)}{\Delta_i^2}-m_i\right)_++\frac{12 K\log(K/\delta)}{\alpha \beta}+K
\end{align} 
and
\begin{align}\label{eq:minimaxregretOtO}
    \mathcal{R}_\algoname(t)\leq
   \max_{J\subseteq [K]}2t\sqrt{\frac{2|J|\log(K/\delta)}{t+\sum_{j\in J}m_j}}+|J|+\frac{12 K\log(K/\delta)}{\alpha \beta}+2t^2\delta.
\end{align}   
\end{theorem}
\begin{remark}
    \cref{thm:boundofftoon} gives bounds for a fixed \(\delta\) only. We now provide some details on how these bounds would generalize to a time-varying \(\delta_t = \delta_0 / t^2\). First, bounds \cref{eq:regretlogOtO,eq:regretOtO} would hold with probability at least \(1 - \frac{\pi^2}{3}\delta_0\). Note that, conceptually, the value of the parameter \(\beta\) should not be updated at every iteration. Instead, it should retain the value it has on the first iteration, as it reflects the uncertainty about the value of the logging policy based on offline data, which does not need to be re-evaluated at each iteration. Specifically, \(\beta = \frac{\sum_i \sqrt{m_i}}{m} \sqrt{2\log(K/\delta_0)}\). In all of \cref{eq:regretlogOtO,eq:regretOtO,eq:minimaxregretOtO}, the \(\delta\) in the logarithmic factors should be replaced with \(\frac{\delta_0}{T^2}\), and \(t^2\delta\) in \cref{eq:minimaxregretOtO} should be replaced with \(\frac{\pi^2}{3}T\delta_0\).
\end{remark}

\textit{Sketch of Proof}: We bound separately the number of pulls of each sub-optimal arm at those iterations where the budget is insufficient, i.e., the arm is chosen by \alglcb, and at those iterations where there is enough budget, i.e., the \algucb arm is chosen. We bound the number of times a suboptimal arm is chosen by \algucb through the usual argument that the upper bound of a sub-optimal arm's value can only surpass $\mu_*$ a limited number of times when the confidence intervals hold. To bound the number of times a sub-optimal arm is chosen by \alglcb, we bound the total number of iterations where the budget is insufficient. A full proof is available in \cref{sec:boundregretalgo}. \hfill \( \Box\)

A few comments on the regret bounds of the algorithm. First, the upper bound on the regret against the logging policy, \cref{eq:regretlogOtO}, can be related to the bound on \alglcb's minimax regret against the logging policy (which will be obtained in \cref{prop:regretlogginglcb}): the leading terms in the two regret bounds are almost the same, $T\beta$ in the upper bound for \alglcb, and $T\left(1+(1+\mathds{1}_{\text{T unknown}})\alpha\right)\beta $ in the upper bound on \algoname. We can also relate \cref{eq:regretOtO} and \cref{eq:minimaxregretOtO} to the regret of \algucb (\cref{prop:regretminimaxucb}), where here the upper bound on the regret of \algoname\ (against optimality) is the sum of the upper bound on the regret of \algucb and a term that scales with $\frac{1}{\alpha}$. The additional term is the cost of the budget constraint. As will be seen in the next section, the bounds in \cref{prop:regretlogginglcb,prop:regretminimaxucb} are tight (up to poly-logarithmic factors) in a range of parameters $\mathbf{m}$. Within this range, it follows then that \algoname\ behaves marginally no worse than the best of the two algorithms along both of the objectives, the regret against an optimal arm and the regret against the logging policy.

\begin{remark}
High values of $\alpha$ loosen the budget constraint. As a consequence, the algorithm behaves closer to \algucb when $\alpha$ is large. The additional cost in \cref{eq:minimaxregretOtO} due to the budget constraint decreases with $\alpha$ unsurprisingly. On the other hand, low values of $\alpha$ strengthen the budget constraint. In consequence, the algorithm behaves closer to \alglcb, and the guarantee against the logging policy is improved. In fact, for $\alpha=0$, the budget is never strictly positive, and the algorithm reduces to \alglcb. In our experiments, setting $\alpha\in [0.1,1]$ gave the best results.

\end{remark}
