%!TEX root =  main.tex
\section{Introduction}

Sequential learning methods have been developed for a wide range of applications, from clinical trials \citep{thompsonclinical, zhou2024sequential} to digital advertising \citep{advertising, bastani2020online}. Typically, the focus has been to find a good balance between \textit{exploration}, which is gathering new information, and \textit{exploitation}, which is leveraging the already available information to maximize collected reward. 
Much of the online learning literature assumes that no information outside of the one collected by the learner is available. However, in many applications to which sequential learning methods have been applied, past interaction data is available upfront before any interaction happens. 

This is where offline-to-online learning is relevant. In offline-to-online learning, the agent has access to a historical dataset resulting from past interactions with the learning environment and then has the opportunity to interact and collect rewards in the environment for a number of rounds. The number of rounds may be fixed and known to the learner or unknown. This setting is a mix between the fully online and fully offline ones and thus presents its own set of challenges. Interest in this problem has grown in recent years, as good offline-to-online methods hold the promise of reducing the cost of online exploration by fully leveraging available offline data \citep{zheng2023adaptivepolicylearningofflinetoonline,lee2021offlinetoonlinereinforcementlearningbalanced,Song2022-vg,Li2023-lv,bu2021onlinepricingofflinedata,MABwithHistory,chen2022data}. 

Offline-to-online learning covers the continuum from the fully offline setting to the fully online one as the number of rounds of interaction increases. However, we believe that the current literature lacks a theoretical treatment encompassing the whole spectrum of offline-to-online learning, as we elaborate in the next section.   
Intuitively, if the number of online rounds is ``small’’ (can be as small as one), there is limited opportunity for exploration, and one should consider exploiting early while relying heavily on the offline data. Conversely, a large number of rounds gives ample room for exploration, and promoting exploration is a reasonable approach in this case. This suggests that different approaches are required at different points along the offline-to-online spectrum, a notion supported by past work, where studies at the two extremes adopt distinct and sometimes opposing strategies.


 
As mentioned before, in the fully online setting, where historical data are not available, the main challenge lies in finding a good balance between exploration and exploitation. Various strategies have been developed to address this trade-off. Although there are other approaches, such as the $\varepsilon$-greedy strategy and Thompson Sampling \citep{agrawal2012analysis}, a much-celebrated principle to balance the trade-off is optimism in the face of uncertainty.  For bandit problems, a standard way to implement the principle is the Upper Confidence Bound (UCB) algorithm, using statistical confidence intervals for the value of different arms and selecting an arm whose confidence interval has the highest upper end \citep{auer2010ucb}. This method has been shown to enjoy asymptotic and minimax optimality \citep{lattimore2020bandit}.


In offline learning, a setting also called \textit{batched policy optimization}, the learner must infer a good policy based on a given dataset, with no possibility to further adjust the policy found based on interactions with the environment. The literature on this setting is not as vast as that of online learning, but it has also gained traction over the past decades. 
The difficulty of offline learning is inherently related to the quality of offline dataset: insufficient coverage of the environment may cause erroneous overestimation of values, which can lead to sub-optimal policies 
(e.g., \citealt{fujimoto2019offpolicydeepreinforcementlearning}). Past studies have proposed using the pessimism principle to mitigate this problem. 
The idea of pessimism has been studied broadly in various literature, often under different names, including robust optimization \citep{ben2002robust, bertsimas2011theory}. For example, the principle has been used extensively for inventory and pricing problems (e.g., \citealt{bertsimas2006robust, bu2023offline, perakis2008regret, xu2022robust}). The pessimism idea has also been studied recently in the reinforcement learning literature \citep{swaminathan2015batch, wu2019behavior,kidambi2020morel, li2022pessimismofflinelinearcontextual, jin2022policy, rashidinejad2023bridgingofflinereinforcementlearning, yin2021towards} and theoretical guarantees have been established. In bandit problems, this principle is implemented as the Lower Confidence Bound (LCB) algorithm, which, just like UCB, also builds a statistical confidence interval for each arm but then picks the arm with the highest \emph{lower} bound. It has been shown that, while both UCB and LCB are minimax optimal for offline learning in multi-armed bandits (MABs), LCB has a better guarantee than UCB for a weighted minimax criterion that is more refined to reflect the inherent difficulty in learning an optimal policy \citep{xiao2021optimalitybatchpolicyoptimization}.
Another advantage of pessimism shown in the literature is the competitiveness against the policy that generated offline data, called the logging policy. Because the logging policy is often the current policy that has been used in practice, it is critical to ensure that the performance of a new policy is not degraded compared to it.  Several approaches based on the pessimism principle have been introduced with theoretical guarantees for the regret against the logging policy \citep{xie2022armormodelbasedframeworkimproving, cheng2022adversarially}. 

From this overview, we could extract the following principle: optimism is a natural approach for online learning, while pessimism is preferred in offline learning. Then, what about offline-to-online learning? Intuitively, one might expect pessimism to perform better for small horizons, with optimism taking over for larger horizons. How significant is this effect? Moreover, one expects that the point at which optimism surpasses pessimism is problem-dependent. Can we identify that ``inflection point'', or better yet,  can we develop an algorithm that automatically finds it? Note that those questions arise regardless of whether the horizon is known or unknown to the learner.




In this paper, we focus on answering these questions for the MAB setting. Although MAB is the simplest reinforcement learning problem, it has been applied to various applications, including assortment selection and online advertising \citep{caro2007dynamic,pandey2007bandits, gur2020adaptive}, and insights gained in the bandit setting often generalize beyond them. However, the critical questions have not been studied in the literature, as is explained and discussed in more detail in the literature review.

As outlined, finding the right balance between pessimism and optimism, represented by the classic LCB and UCB algorithms, is the central challenge in the offline-to-online setting. Our first contribution is a new algorithm that automatically finds this balance, adjusting between pessimistic and optimistic approaches as needed. 

We support the design of our algorithm via a thorough study of the performance of UCB and LCB, and our algorithm. We study across the whole offline-to-online spectrum, by which we mean for varying horizon $T$.  While the minimax criterion is commonly used to evaluate algorithms, it fails to distinguish between LCB and UCB in the offline setting, as both are minimax optimal \citep{xiao2021optimalitybatchpolicyoptimization}. However, a comparison against the logging policy highlights an additional advantage of pessimism in offline learning. Motivated by this, we evaluate performance using both regret against the logging policy and regret against optimality. 
As expected, we find that neither  UCB nor LCB outperforms the other across both metrics along the whole spectrum of offline-to-online learning and under different offline data compositions. In contrast, our algorithm remains competitive with both UCB and LCB under both regret measures, achieving a principled balance between optimism and pessimism across the offline-to-online learning continuum.

