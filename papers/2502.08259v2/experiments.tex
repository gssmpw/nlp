%!TEX root =  main.tex
\section{Numerical Illustration}

The source code for reproducing all experiments is publicly available at: \url{https://github.com/FloreSentenac/offlinetoonline}.
\subsection{Comparison on Synthetic Data}
\begin{figure}[htb]
\centering
\begin{subfigure}{.48\textwidth}
  \includegraphics[width=\linewidth,keepaspectratio]{plots/smallhorizonoptsampled.pdf}
  \caption{Instance $1$, $T=200$}
  \label{fig:smallhorizonoptsampled}
\end{subfigure}\hfill
\begin{subfigure}{.48\textwidth}
  \includegraphics[width=\linewidth]{plots/smallhorizonoptnotsampled.pdf}
  \caption{Instance $2$, $T=200$}
  \label{fig:smallhorizonoptnotsampled}
\end{subfigure}
\begin{subfigure}{.48\textwidth}
  \includegraphics[width=\linewidth]{plots/Largehorizonoptsampled.pdf}
  \caption{Instance $1$, $T=2000$}
  \label{fig:Largehorizonoptsampled}
\end{subfigure}\hfill
\begin{subfigure}{.48\textwidth}
  \includegraphics[width=\linewidth]{plots/Largehorizonoptnotsampled.pdf}
  \caption{Instance $2$, $T=2000$}
  \label{fig:LargehorizonoptnotsampledTknown}
\end{subfigure}
\caption{Regret of the three algorithms for different instances and $T$ values, when the horizon $T$ is given to \algoname}
\label{fig:firstsetexp}
\end{figure}

We now illustrate the discussion of the previous section with some numerical experiments on synthetic data.  We consider two bandits instances in this experiment section. Both have $K=20$ arms, a total of $m=2000$ offline samples, and the first $10$ arms are each sampled $200$ times in the offline data set.  The remaining arms are not sampled at all. The reward distribution for each arm $i\in [K]$ is Bernoulli with mean $\mu_i$. In instance $1$, each arm $1\leq i\leq 10$ has mean $\mu_i=0.5$, while for each $i >10$, $\mu_i=0.25$. Note that on that instance, the logging policy is optimal. Based on the results of the previous sections, it is expected that \alglcb\ should outperform \algucb. In instance $2$, the means are the same as in instance $1$ for all arms $i<20$, while $\mu_{20}=0.75$. Note that in instance $2$, the optimal arm is not sampled in the offline dataset. Again, based on the results of the previous sections, we expect \algucb\ to outperform \alglcb\ for large horizons $T$. Also, according to the previous section, \algoname\ should perform in between \alglcb\ and \algucb\ on both instances.

We compare the three algorithms, \alglcb, \algucb\, and \algoname\, on the two instances for small horizons ($T=200$) and larger horizons ($T=2000$). In a first set of experiments, the horizon $T$ is known, and we set $\delta=\frac{1}{T^2}$ for all algorithms and $\alpha=0.2$. In the second one, the horizon is unknown, and we set $\delta_t= \frac{0.01}{t^2}$ for all algorithms and $\alpha=0.6$. For each combination of instance, horizon and parameter setting, the experiment is run $200$ times. The results for the first set of experiments are displayed in \cref{fig:firstsetexp}, and for the second set in \cref{fig:secondsetexp}. The bold lines show the average regrets of each algorithm. The shaded areas represent the means plus or minus two standard deviations.


In the unknown horizon case, we opted for a slightly higher value of $\alpha$,  for which the algorithm's behavior remains comparable between the known and unknown horizon settings. Note that while the algorithm tends to perform better with carefully chosen values of $\alpha$, the bounds in \cref{thm:boundofftoon} are valid for any choice of $\alpha$. Moreover, we expect the algorithm not to be overly sensitive to the parameter, as is explored experimentally in the next section.







\begin{figure}[htb]
\begin{centering}
\begin{subfigure}{.48\textwidth}
  \centering
  \includegraphics[width=\linewidth]{plots/Tunknownsmallhorizonoptsampled.pdf}
  \caption{Instance $1$, $T=200$}
\end{subfigure}%
\begin{subfigure}{.48\textwidth}
  \centering
  \includegraphics[width=\linewidth]{plots/Tunknownsmallhorizonoptnotsampled.pdf}
  \caption{Instance $2$, $T=200$}
\end{subfigure}
\begin{subfigure}{.48\textwidth}
  \centering
  \includegraphics[width=\linewidth]{plots/Tunknownlargehorizonoptsampled.pdf}
  \caption{Instance $1$, $T=2000$}
\end{subfigure}%
\begin{subfigure}{.48\textwidth}
  \centering
  \includegraphics[width=\linewidth]{plots/Tunknownlargehorizonoptnotsampled.pdf}
  \caption{Instance $2$, $T=2000$}\label{fig:largehorizonTunknown}
\end{subfigure}
\caption{Regret of the three algorithms for $T=m=2000$, when the horizon $T$ is unknown.}
\label{fig:secondsetexp}
\end{centering}
\end{figure}

In the results displayed in Figure~\ref{fig:firstsetexp}, we first compare \algucb and \alglcb. We observe that \alglcb\ outperforms \algucb\ when the offline data contains the optimal arms, e.g., generated by an expert policy (\cref{fig:smallhorizonoptsampled,fig:Largehorizonoptsampled}). When the optimal arm is not sampled, \alglcb's regret grows linearly but is still lower than that of \algucb when the horizon is short because \algucb did not have enough time to explore and \alglcb made a safe choice based on the offline data (Figure~\ref{fig:smallhorizonoptnotsampled}). However, UCB performs better when the offline data does not contain the optimal arm and the horizon is long enough (Figure~\ref{fig:LargehorizonoptnotsampledTknown}). 

We note that \algoname\ smoothly interpolates between \alglcb\ and \algucb. In \cref{fig:smallhorizonoptsampled,fig:Largehorizonoptsampled}, it starts by following the footsteps of \algucb, then, as the budget is detected to be too low, it switches and sticks to \alglcb until the end of the run. In \cref{fig:smallhorizonoptnotsampled,fig:LargehorizonoptnotsampledTknown}, it also starts by following the footsteps of \algucb. Then, it exhibits two different behavior for the small and large horizons. In the case of the small horizon, \algucb\ is still in the exploratory phase, where the regret grows faster than that of \alglcb. Here, as in the two previous cases, the lack of budget causes a switch to \alglcb. On the other hand, for the larger horizon, \algucb enters the logarithmic growth phase of regret and outperforms \alglcb vastly. The budget is now always in excess, and our algorithm never switches to \alglcb.


By design, the budget ensures that \algoname\ focuses on exploration at the beginning of the experiment window. A natural question arises: does this behavior persist when the horizon is unknown? The plots in \cref{fig:secondsetexp} indicate that the behavior remains relatively similar, though with some important distinctions. First, the algorithm now explores in phases, with each pseudo-horizon update triggering a brief exploration period. Second, there are notable differences between \cref{fig:largehorizonTunknown} and \cref{fig:LargehorizonoptnotsampledTknown}. In the known horizon case (\cref{fig:LargehorizonoptnotsampledTknown}), \algoname\ never switches back to \alglcb, and there is no apparent cost to enforcing the budget. In contrast, in the unknown horizon case (\cref{fig:largehorizonTunknown}), while \algoname\ ultimately behaves like \algucb, there is an additional cost due to the budget constraint, as the algorithm reverts to the safe option, \alglcb, at shorter horizons.
This is to be expected in general: in the known horizon case, when \algucb is the better option, the cost of enforcing the budget constraint sometimes disappears. In the unknown horizon case, even for very large values of $\alpha$, an additional cost is always expected.




\subsection{Ad Click Through Rate Data}\label{sec:exprealdata}
\newcommand{\oto}{\algoname}

 While our synthetic data experiments should illustrate well the gain from using \oto, it remains to be seen whether these gains would also manifest themselves in more realistic scenarios. 
To get some insight into this, we will explore the performance of \oto 
in the context of ad recommendation. Ads recommendation
is the process of selecting and delivering advertisements personalized to internet users so as to maximize the relevance of the advertisements to the users. The main tool for developing such personalized ad recommendation systems is to use machine learning algorithms to predict the probability that the viewer of an ad will click on the ad. Given accurately predicted \emph{click-through rates} (CTRs), the system can select the ad with the highest predicted CTR. Companies working on such systems continuously collect data to improve the accuracy of the CTR prediction methods. 
Oftentimes, over the course of years, companies will develop multiple, competing CTR prediction methods. Typically, companies cannot reliably evaluate the performance of trained models without having them deployed into the systems. Indeed, the data available for evaluation rarely is a uniform sample of the true data that would appear in the real system: on top of other issues, such as temporal trends, ads collected are chosen by the system in place, and as such are selected with a bias. New methods then have to be put into production to be assessed accurately. The problem of choosing between different prediction methods in production can then be viewed as a multi-armed bandit problem, where the arms correspond to the individual prediction methods and the reward is, say, $+1$, when the ad chosen by the prediction method is clicked by a user and is zero otherwise. 

In this section we use publicly available data to emulate this situation and generate data that illustrate different scenarios. The data we use was collected by Avazu, a mobile internet advertising and performance marketing company
 and was made available in 2014 for a publicly held competition \citep{avazu-ctr-prediction}.
The data consists of over 11 million records of mobile ad display. Each record captures detailed information about an ad impression. 
 
 The experiment was conducted in two phases. First, a \textit{preparatory} phase, during which models were trained and a reward table was generated. Then, a \textit{bandit} phase followed, where we simulated the bandit-algorithm-assisted deployment of the trained models in an environment based on the reward table.  
 
\textbf{Preparatory Phase}: 
The initial step of the preparatory phase involved training a predictive model (Model 1) using a dataset of 30,000 CTR entries, uniformly sampled without replacement from the Avazu dataset, referred to as Dataset 1. 10\% of Dataset 1 was withheld from training and set aside as a test set for later use. This portion is referred to as Test Set 1. We then simulated the deployment of Model 1. In each of 30,000 iterations, the model was tasked with selecting an advertisement from 10 options presented at each iteration, choosing the one with the highest predicted click probability. The 10 options were uniformly sampled without replacement from the Avazu dataset, excluding entries already included in Dataset 1. The selected advertisements and their observed outcomes (clicked or not clicked) were recorded to construct a new dataset, referred to as Dataset 2.

This process reflects how companies deploy predictive models in practice to select advertisements for users. Dataset 2 also mirrors the type of data a company would collect, shaped by the biases inherent in the deployed model’s predictions.

Next, six new models (Models 2–7) are trained on the data generated in the simulated production environment, Dataset 2, under the assumption that the original dataset, Dataset 1, is no longer available. This reflects a realistic scenario, as companies rarely have access to datasets that are sampled unbiasedly from the true ad distribution. Furthermore, they are often obligated by regulators to delete old data.

A natural but naive approach to selecting the best model for production is to set aside a portion of Dataset 2 (10\% in our case), evaluate all models on this test data, and retain the one with the best performance. We refer to this set-aside portion as Test Set 2. However, in our scenario, model performance on Test Set 2 does not accurately reflect how the models would perform in the production environment due to the distribution mismatch. This is illustrated in \cref{fig:crossentropyloss}, where the first line shows the cross-entropy loss of all models on Test Set 2, and the second line shows the cross-entropy loss on Test Set 1, which has the same distribution as the production data. Note that, unless models are actually deployed in production, the company does not have access to the information in the second row of the table. We refer to the model with the smallest cross-entropy log-loss on Test Set 2 as the empirical best arm on the test set, which, in this specific case, is Model 3.
\begin{table}[bth] 
\begin{center}
\renewcommand{\arraystretch}{1.4}
\begin{tabular}{>{\centering\arraybackslash}m{3cm} | |>{\centering\arraybackslash}m{1.cm} |>{\centering\arraybackslash}m{1.cm} |>{\centering\arraybackslash}m{1.cm} |>{\centering\arraybackslash}m{1.cm}|>{\centering\arraybackslash}m{1.cm} |>{\centering\arraybackslash}m{1.cm} |>{\centering\arraybackslash}m{1.cm} |>{\centering\arraybackslash}m{1.cm}  }
\toprule
Model \#&  1 &  2 &  3& 4& 5& 6& 7& 8 \\
 \hline\hline
 \addlinespace

Test Set 2 & $0.666$&$0.576$ &$ 0.566$ &$0.576$ &$0.576$ &$0.572$ &$0.579$ & $0.648$\\
\addlinespace
\hline
\addlinespace

Test Set 1 &$0.394$ &$0.896$&$0.640$&$0.714$&$1.063$&$0.690$&$0.752$& $0.388$ \\
\bottomrule
\end{tabular}
\vspace{0.2cm}
\caption{Binary cross entropy loss (rounded to the third digit) for each model on the two test sets}
\label{fig:crossentropyloss}
\end{center}
\end{table}

All models trained on Dataset 2 perform worse than Model 1 on Test Set 1. To illustrate different scenarios regarding the quality of the arm generating offline data, 
%conduct meaningful experiments in the next phase, 
we needed at least one model that outperforms Model 1. To achieve this, we trained another model, Model 8, on Dataset 1, similar to Model 1. Although training on the original dataset does not align with the constraints companies typically face, it provides a reliable method for constructing a strong predictor. In practice, companies do manage to improve upon their existing models, especially since our baseline model has the unfair advantage of being trained on perfect data. Regardless of how it was trained, Model 8 is evaluated on Test Set 2. While it does outperform Model 1, it is still outperformed by Models 2–7.

The final step of the preparatory phase involves generating a reward table, which serves as the foundation for the reward generation process in the next phase. This table is constructed by evaluating the models iteratively over multiple events. For each event, all models are presented with a set of 10 advertisements, sampled uniformly at random without replacement from the unused ads. Each model selects the advertisement with the highest predicted Click-Through Rate (CTR). A reward of 1 is recorded if the selected advertisement is clicked, and 0 otherwise. The reward table entry $(i,j)$ represents the reward for model j during presentation event i, with 1 indicating a click and 0 indicating no click. A total of $300,000$ events are collected.
Details on the models used can be found in \cref{app:simulationrealdata}.

\textbf{Bandit Phase}: This phase is the core offline-to-online experiment, simulating the transition to production for new models after the baseline model (Model 1) has been in operation for some time. In this experiment, each prediction model is treated as an arm. Dataset 2 is the offline dataset. Generating a reward for arm $j$ is done by randomly selecting a row $i$ from the reward table and outputting the entry $(i,j)$. The goal of the company is to identify the best model using a combination of offline data and online experiments. 
%We assume the company has a running model, Model 1, that has been in place for $m = 30,000$ rounds, providing offline data, generated through the process described in the previous paragraph. Six new prediction models are introduced for testing, and 

We consider two scenarios regarding the quality of the running model (i.e., the performance of the logging policy): \textit{Setting 1}, where one of the six new models performs better than the running model (we include Models 2-6 and 8), and \textit{Setting 2}, where all new models perform worse than the running model (we include Models 2-7).

We evaluate three strategies—\alglcb, \algucb, and \algoname—for two horizon lengths, $T = 3,000$ and $T = 300,000$, reporting results for \algoname with several values of $\alpha$. For each value of $T$, we ran the experiment 200 times. For comparison, we include two benchmark arms in the results: (1) the best empirical arm in hindsight, which is the arm identified by evaluating each arm over 200 runs and selecting the one with the highest empirical mean, and (2) the arm with the lowest binary cross-entropy loss on Test Set 2.   


\textbf{Results}: The results are summarized in \cref{fig:summaryexprealdatagoodarm,fig:summaryexprealdatabadarm}, where we provide box plots of the total cumulative reward by each algorithm, for each combination of setting and horizon length. Additional plots showing the average cumulative rewards can be found in \cref{app:simulationrealdata}.

The findings reveal that the model selected based on its performance on Test Set 2 consistently underperforms compared to other methods across all scenarios.  This outcome highlights a critical issue: the performance on a biased production-generated dataset does not necessarily translate to the true performance in production. This reinforces the importance of methods such as the proposed algorithm, which adaptively evaluates models in real-time production environments.

\begin{figure}[ht]
    \centering
    % First figure
    \begin{subfigure}[b]{0.8\textwidth}
        \centering
        \includegraphics[width=\textwidth]{plots/boxplots/onegoodarmT1000.pdf} % Replace with your image file
        \caption{Short horizon, $T=3,000$}
        \label{fig:fiboxplotonegoodarmshorthorizon}
    \end{subfigure}
    \vspace{1cm} % Space between figures
    % Second figure
    \begin{subfigure}[b]{0.8\textwidth}
        \centering
        \includegraphics[width=\textwidth]{plots/boxplots/onegoodarmT100000.pdf} % Replace with your image file
        \caption{Long horizon, $T=300,000$}
    \end{subfigure}
    \caption{The total cumulated reward by each algorithm in Setting 1}
    \label{fig:summaryexprealdatagoodarm}
\end{figure}


\cref{fig:summaryexprealdatagoodarm} shows the results for Setting 1 with two horizon lengths. For the short horizon, \alglcb outperforms \algucb, as the latter spends a substantial number of rounds exploring suboptimal options. Conversely, for the long horizon, \algucb surpasses \alglcb, as its extended exploration phase identifies the best prediction model. In both cases, \algoname demonstrates performance close to the best of the two algorithms. These results align with our theoretical analysis and the design principles of \algoname.
%In Setting 1,  we expect \alglcb to do better than \algucb for short horizon, where \algucb will spend some time exploring the bad options. Then, for large horizons, we expect \algucb to outperform \alglcb, as, after the exploration phase, mostly the best prediction model will be used. It is what is observed in the results \cref{fig:summaryexprealdatagoodarm}, where \alglcb get a higher cumulated reward than \algucb for $T=1000$, and for $T=100000$, the opposite happens.

In Setting 2, the arm that generated offline data is the best arm, making \alglcb more desirable than in Setting 1. We observe this with both horizon lengths in \cref{fig:summaryexprealdatabadarm}. As expected, the performance gap between \algucb and \alglcb decreases as the horizon increases. Note that the $y$-axes in \cref{fig:summaryexprealdatabadarm} have different scales.
We observe that, similar to the previous experiments, \algoname interpolates between the performance of \algucb and \alglcb, close to the best of the two in any case. For low values of $\alpha$ (e.g., $\alpha=0$), it matches the performance of \alglcb, which is expected since a low $\alpha$ value strengthens the budget constraint.  For $\alpha=5$ (corresponding to $\alpha = 2\sqrt{K}$), \algoname behaves more like \algucb, which again aligns with the expected behavior. For $\alpha=1$ and $\alpha=0.3$, \algoname's performance is nearly identical to the best of the other two algorithms. When the performance gap between \alglcb and \algucb is large, \algoname's performance is close to the maximum cumulative reward of the two. This suggests that although $\alpha$ is a free parameter, the algorithm's behavior remains fairly robust to its choice.

\begin{figure}[ht]
    \centering
    % First figure
    \begin{subfigure}[b]{0.8\textwidth}
        \centering
        \includegraphics[width=\textwidth]{plots/boxplots/badarmsonlyT1000.pdf} % Replace with your image file
        \caption{Short horizon, $T=3,000$}
        \label{fig:fiboxplotbadarmsonlyhorthorizon}
    \end{subfigure}
    \vspace{1cm} % Space between figures
    % Second figure
    \begin{subfigure}[b]{0.8\textwidth}
        \centering
        \includegraphics[width=\textwidth]{plots/boxplots/badarmsonlyT100000.pdf} % Replace with your image file
        \caption{Long horizon, $T=300,000$}
    \end{subfigure}
    \caption{The total cumulated reward by each algorithm in Setting 2}
    \label{fig:summaryexprealdatabadarm}
\end{figure}


