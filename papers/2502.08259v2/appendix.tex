\begin{APPENDICES}
\begin{table}[h]
\caption{Notation Summary}
    \centering
    \renewcommand{\arraystretch}{1.5}
    \begin{tabular}{|p{2.5cm}|p{5cm}|p{2.5cm}|p{5cm}|}
        \hline
        \textbf{Notation} & \textbf{Description} & \textbf{Notation} & \textbf{Description} \\ \hline
        $K$ & \small Number of arms & $m_i$ & \small Offline samples from arm $i$ \\ \hline
        $\mathcal{P}_i$ & \small Distribution of arm $i$ (1-subgaussian) & $\mathbf{m}$ & \small Vector of samples for each arm \\ \hline
        $\mu_i$ & \small Mean reward of arm $i$, $\mu_i \in [0,1]$ & $\Theta_{\mathbf{m}}$ & \small Set of instances with $\mathbf{m}$ fixed \\ \hline
        $\mu^*$ & \small Mean reward of the optimal arm & $\hat{\mu}_i(0)$ & \small Empirical mean of $m_i$ samples from arm $i$ \\ \hline
        $\Delta_i$ & \small Suboptimality gap of arm $i$, $\Delta_i = \mu^* - \mu_i$ & $m$ & \small Total number of offline samples, $m = \sum_{i \in [K]} m_i$ \\ \hline
        $\Theta$ & \small Set of all instances & $\hat{\mu}_0$ & \small Weighted empirical mean, $\hat{\mu}_0 = \frac{\sum_{i \in [K]} m_i \hat{\mu}_i(0)}{m}$ \\ \hline
        $\mu_0$ & \small Weighted true mean, $\mu_0 = \frac{\sum_{i \in [K]} m_i \mu_i}{m}$ & $\Delta_0$ & \small Gap between $\mu^*$ and $\mu_0$, $\Delta_0 = \mu^* - \mu_0$ \\ \hline
        $t$ & \small Time step in online phase &  $I(t)$& \small Arm chosen at time $t$ \\ \hline
        $x^t_{I(t)}$ & \small Reward from pulling $I(t)$ at time $t$ & $T_i(t)$ & \small Times arm $i$ has been pulled by time $t$ \\ \hline
        $ \hat{\mu}_i(t)$ & \small Empirical mean of arm $i$ at time $t$ &  $\overline{\mu}_i(t)$& \small Upper bound for arm $i$ at $t$ \\ \hline
        $\underline{\mu_i}(t)$ & \small Lower bound for arm $i$ at $t$ & $R(T)$ & \small Pseudo-regret over horizon $T$, $R(T) = T \mu^* - \sum_{t=1}^T \mu_{I(t)}$ \\ \hline
        $R^{\text{log}}(T)$ & \small Logging pseudo-regret over $T$, $R^{\text{log}}(T) = T \mu_0 - \sum_{t=1}^T \mu_{\pi(t)}$ & $\mathcal{R}_\pi(T)$ & \small Minimax pseudo-regret, $\mathcal{R}_\pi(T) = \max_{\theta \in \Theta_{\mathbf{m}}} \mathbb{E}_{\theta,\pi}[R_\pi(T)]$ \\ \hline
        $\mathcal{R}^{\text{log}}_\pi(T)$ & \small Minimax logging pseudo-regret, $\mathcal{R}^{\text{log}}_\pi(T) = \max_{\theta \in \Theta_{\mathbf{m}}} \mathbb{E}_{\theta,\pi}[R^{\text{log}}(T)]$ & $L(t)$& \small Arm chosen by \alglcb\ at time $t$\\ \hline
        $U(t)$& \small Arm chosen by \algucb\ at time $t$ & $\beta$ & \small Constant for exploration budget, $\beta = \frac{\sum_i \sqrt{m_i}}{m} \sqrt{2\log(\frac{1}{\delta})}$ \\ \hline
        $\gamma$ & \small Safety threshold, $\gamma = \underline{\mu_{L(0)}(0)} - \alpha \beta$ & $\alpha$ & \small Parameter determining budget stringency \\ \hline
        $T_i^U(t)$ & \small Times arm $i$ was played as \algucb\ arm by time $t$ & $T^L_i(t)$ & \small Times arm $i$ was played as \alglcb\ arm by time $t$ \\ \hline
         $T^L(t)$& \small Total times \alglcb\ was played by time $t$,  &  $B_{T}(t)$& \small Exploration budget at time $t$, see definition in the text \\ \hline
        $\tilde{T}$ & \small Proxy horizon, initially 2, doubled each time $t > \tilde{T}$ in the unknown horizon case &  &  \\ \hline
    \end{tabular}
\end{table}

\section{Additional Experiments}\label{app:extrasim}

    As mentioned in \cref{sec:algorithm}, there are multiple ways to set the budget, which could potentially result in similar theoretical guarantees.  The chosen version of the budget:

    \[
B_{T}(t)=\sum_{i=1}^{K}T_i^U(t-1)(\underline{\mu_i}(t)-\gamma)+\underline{\mu_{U(t)}}(t)-\gamma+(T^L(t-1)+T-t)\alpha \beta,
\]
does not update in the same way when the chosen policy is \algucb\ or \alglcb. It is natural to wonder if unifying the update as:
        \[
B^{'}_{T}(t)=\sum_{i=1}^{K}T_i(t-1)(\underline{\mu_i}(t)-\gamma)+\underline{\mu_{U(t)}}(t)-\gamma+(T-t)\alpha \beta,
\]
would also work.
\begin{figure}[h!]
\begin{centering}
\begin{subfigure}{.48\textwidth}
  \centering
  \includegraphics[width=\linewidth]{plots/plots_appendices/partialupdateparamTsmalloptsampled.pdf}
  \caption{Instance $1$, $T=2000$}
\end{subfigure}%
\begin{subfigure}{.48\textwidth}
  \centering
  \includegraphics[width=\linewidth]{plots/plots_appendices/partialupdateparamTlargeoptnotsampled.pdf}
  \caption{Instance $2$, $T=2000$}
\end{subfigure}
\begin{subfigure}{.48\textwidth}
  \centering
  \includegraphics[width=\linewidth]{plots/plots_appendices/partialupdateparamTlargeoptsampled.pdf}
  \caption{Instance $1$, $T=2000$}
\end{subfigure}%
\begin{subfigure}{.48\textwidth}
  \centering
  \includegraphics[width=\linewidth]{plots/plots_appendices/partialupdateparamTlargeoptnotsampled.pdf}
  \caption{Instance $2$, $T=2000$}
\end{subfigure}
\caption{Regret of the three algorithms for different instances and $T$ values, when the horizon $T$ is given to \algoname, with \algoname implemented with the first alternative version of the budget and $\alpha=0.2$.}
\label{fig:badversion}
\end{centering}
\end{figure}

 Regarding theoretical guarantees, we suspect that a similar result can be proven. However, as can be seen in \cref{fig:badversion}, the algorithm with that alternative budget does not perform as well as the chosen one. We observe in \cref{fig:badversion} that the algorithm switches to \alglcb, then gradually starts playing \algucb again. The reason for that behavior is simple: as the arm $L(0)$ is played more and more, $\underline{\mu_{L(0)}}(t)$ increases, while $\gamma$ remains unchanged. Thus, the projected budget is underestimated, and the algorithm returns to exploration towards the end of the horizon.
\begin{figure}[htb]
\begin{centering}
\begin{subfigure}{.48\textwidth}
  \centering
  \includegraphics[width=\linewidth]{plots/plots_appendices/fullupdateparamTsmalloptsampled.pdf}
  \caption{Instance $1$, $T=2000$}
\end{subfigure}%
\begin{subfigure}{.48\textwidth}
  \centering
  \includegraphics[width=\linewidth]{plots/plots_appendices/fullupdateparamTsmalloptnotsampled.pdf}
  \caption{Instance $2$, $T=2000$}
\end{subfigure}
\begin{subfigure}{.48\textwidth}
  \centering
  \includegraphics[width=\linewidth]{plots/plots_appendices/fullupdateparamTlargeoptsampled.pdf}
  \caption{Instance $1$, $T=2000$}
\end{subfigure}%
\begin{subfigure}{.48\textwidth}
  \centering
  \includegraphics[width=\linewidth]{plots/plots_appendices/fullupdateparamTlargeoptnotsampled.pdf}
  \caption{Instance $2$, $T=2000$}
\end{subfigure}
\caption{Regret of the three algorithms for different instances and $T$ values, when the horizon $T$ is given to \algoname, with \algoname implemented with the second alternative version of the budget and $\alpha=0.2$.}
\label{fig:fullupdateparam}
\end{centering}
\end{figure}


A second natural option is to then also update $\gamma$ as 
\[
\gamma_t= \underline{\mu_{L(0)}}(t)-\alpha\beta,
\]
 and have:

  \[
B^{''}_{T}(t)=\sum_{i=1}^{K}T_i(t-1)(\underline{\mu_i}(t)-\gamma_t)+\underline{\mu_{U(t)}}(t)-\gamma_t+(T-t)\alpha \beta,
\]
As can be seen in \cref{fig:fullupdateparam}, this version performs comparably to the one chosen. However, we did not analyze this one further, as it seems harder to obtain theoretical guarantees for this version. The main challenge is that an arm $i$ that is chosen as $L(t)$ at some iteration may no longer be chosen again. Its lower bound $\underline{\mu}_i(t)$ would then no longer update, and it is not straightforward to see if $\underline{\mu}_i(t')>\gamma_{t'}$ would hold for subsequent iterations. This implies in turn that it is not easy to guarantee the budget remains positive. Also, this version did not show any advantage in empirical performance, compared to the presented version. 
%For this reason, and as it did not seem to bring any practical benefits, the version presented in the main text was preferred.



\section{Simulation using Ad Click Through Rate data}\label{app:simulationrealdata}

The prediction models were constructed based on methodologies from a top-performing submission in the Kaggle data challenge for the Avazu dataset \citep{kaggle_deepctr_difm}. Three state-of-the-art methods were selected: the Factorization-Machine-based Neural Network (DeepFM, \citealt{guo2017deepfmfactorizationmachinebasedneural}), the Deep \& Cross Network (DCN, \citealt{wang2017deepcrossnetwork}), and the Dual Input-aware Factorization Machine (DIFM) for CTR Prediction \citep{Lu2020ADI}. All methods were implemented using the {\tt DeepCTR-Torch} Python package \citep{deepctr_torch}. Feature engineering included transforming sparse categorical features through label encoding or one-hot encoding and normalizing dense features, following the exact methodology described in the selected Kaggle notebook.


To create six distinct models, two different configurations were used for the number of neurons in the neural network layers underlying each method. DCN and DeepFM, configured with the default parameters provided in the notebook, were used to construct Model 1 and Model 8, respectively. The remaining models (Models 2–7) were generated by applying each method—DeepFM, DIFM, and DCN—with the two distinct parameter configurations.

The results of these experiments are summarized in \cref{sec:exprealdata}. Additional plots of cumulative rewards are provided in \cref{fig:plotavcumrewardsetting1,fig:plotavcumrewardsetting2} to further illustrate these findings.






\begin{figure}[htb]
\begin{centering}
\begin{subfigure}{.48\textwidth}
  \centering
  \includegraphics[width=\linewidth]{plots/plots_real_data/goodT1000alpha0.png}
%  \caption{ $T=10000$, $\alpha=0$}
\end{subfigure}%
\begin{subfigure}{.48\textwidth}
  \centering
  \includegraphics[width=\linewidth]{plots/plots_real_data/goodT100000alpha0.png}
 % \caption{$T=100000$, $\alpha=0.3$}
\end{subfigure}
\begin{subfigure}{.48\textwidth}
  \centering
  \includegraphics[width=\linewidth]{plots/plots_real_data/goodT1000alpha0.3.png}
 % \caption{ $T=10000$, $\alpha=0.3$}
\end{subfigure}%
\begin{subfigure}{.48\textwidth}
  \centering
  \includegraphics[width=\linewidth]{plots/plots_real_data/goodT100000alpha0.3.png}
%  \caption{$T=100000$, $\alpha=0$}
\end{subfigure}
\begin{subfigure}{.48\textwidth}
  \centering
  \includegraphics[width=\linewidth]{plots/plots_real_data/goodT1000alpha1.png}
%  \caption{$T=10000$, $\alpha=1$}
\end{subfigure}%
\begin{subfigure}{.48\textwidth}
  \centering
  \includegraphics[width=\linewidth]{plots/plots_real_data/goodT100000alpha1.png}
%  \caption{$T=100000$, $\alpha=1$}
\end{subfigure}
\begin{subfigure}{.48\textwidth}
  \centering
  \includegraphics[width=\linewidth]{plots/plots_real_data/goodT1000alpha5.png}
%  \caption{$T=10000$, $\alpha=2.5$}
\end{subfigure}%
\begin{subfigure}{.48\textwidth}
  \centering
  \includegraphics[width=\linewidth]{plots/plots_real_data/goodT100000alpha5.png}
%  \caption{$T=300000$, $\alpha=2.5$}
\end{subfigure}

\caption{Cumulated reward of the three algorithms on the CTR prediction data in Setting 1, for various values of parameter $\alpha$, horizon $T=3000$ on the left, $T=300000$ on the right}

\label{fig:plotavcumrewardsetting1}
\end{centering}
\end{figure}

\begin{figure}[htb]
\begin{centering}
\begin{subfigure}{.48\textwidth}
  \centering
  \includegraphics[width=\linewidth]{plots/plots_real_data/badT1000alpha0.png}
%  \caption{ $T=10000$, $\alpha=0$}
\end{subfigure}%
\begin{subfigure}{.48\textwidth}
  \centering
  \includegraphics[width=\linewidth]{plots/plots_real_data/badT100000alpha0.png}
 % \caption{$T=100000$, $\alpha=0.$}
\end{subfigure}
\begin{subfigure}{.48\textwidth}
  \centering
  \includegraphics[width=\linewidth]{plots/plots_real_data/badT1000alpha0.3.png}
 % \caption{ $T=10000$, $\alpha=0.3$}
\end{subfigure}%
\begin{subfigure}{.48\textwidth}
  \centering
  \includegraphics[width=\linewidth]{plots/plots_real_data/badT100000alpha0.3.png}
%  \caption{$T=100000$, $\alpha=0.3$}
\end{subfigure}
\begin{subfigure}{.48\textwidth}
  \centering
  \includegraphics[width=\linewidth]{plots/plots_real_data/badT1000alpha1.png}
%  \caption{$T=10000$, $\alpha=1$}
\end{subfigure}%
\begin{subfigure}{.48\textwidth}
  \centering
  \includegraphics[width=\linewidth]{plots/plots_real_data/badT100000alpha1.png}
%  \caption{$T=100000$, $\alpha=1$}
\end{subfigure}
\begin{subfigure}{.48\textwidth}
  \centering
  \includegraphics[width=\linewidth]{plots/plots_real_data/badT1000alpha5.png}
%  \caption{$T=10000$, $\alpha=2.5$}
\end{subfigure}%
\begin{subfigure}{.48\textwidth}
  \centering
  \includegraphics[width=\linewidth]{plots/plots_real_data/badT100000alpha5.png}
%  \caption{$T=100000$, $\alpha=2.5$}
\end{subfigure}


\caption{Cumulated reward of the three algorithms on the CTR prediction data in Setting 2, for various values of parameter $\alpha$, horizon $T=3000$ on the left, $T=300000$ on the right}
\label{fig:plotavcumrewardsetting2}

\end{centering}
\end{figure}





\section{Omitted Proofs}\label{sec:omittedproofs}


\input{proofsappendix}

\end{APPENDICES}

