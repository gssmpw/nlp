\section{Related Works}
\textbf{Offline Learning}: We begin by providing an overview of the research conducted in offline (or batch) learning, with a particular focus on works addressing the multi-armed bandit (MAB) setting in detail. A central challenge in offline learning is data coverage: does the offline dataset contain sufficient information about an optimal or near-optimal policy? There are at least two extreme offline data types \citep{rashidinejad2023bridgingofflinereinforcementlearning}: expert data sets, produced by a near-optimal policy, and uniform coverage data sets, which provide equal representation of all actions. In expert data sets, imitation learning algorithms are shown to have a small sub-optimality gap against the logging policy \citep{Imitationlearningross,rajaraman2020fundamentallimitsimitationlearning}. Meanwhile, theoretical guarantees for many offline RL algorithms depend on the coverage of offline dataâ€”often quantified through a concentrability coefficient that measures how well the dataset covers the policies being evaluated \citep{rashidinejad2023bridgingofflinereinforcementlearning}; \cite{rashidinejad2023bridgingofflinereinforcementlearning}) or the set of policies with which the algorithms compete is limited to those covered in offline data \citep{cheng2022adversarially,  yin2020nearoptimalprovableuniformconvergence}. In this paper, we also consider different offline data compositions (uniform vs. skewed), but the main focus is the spectrum from offline to online learning.



A widely accepted intuition in offline learning is that a good policy should avoid under-explored regions of the environment. This motivates the use of the pessimism principle, which manifests in various forms: learning a pessimistic value function \citep{swaminathan2015counterfactualriskminimizationlearning,wu2019behaviorregularizedofflinereinforcement,li2022pessimismofflinelinearcontextual}, pessimistic surrogate \citep{buckman2020importancepessimismfixeddatasetpolicy}, or planning with a pessimistic model \citep{MorelPessimistisOfflineLearning,yu2020mopomodelbasedofflinepolicy}. 
Despite the surge of work in that direction, a theoretical base for the principle has only recently been developed. 
\cite{xiao2021optimalitybatchpolicyoptimization} analyzed pessimism in MABs, proving that both UCB and \alglcb are minimax optimal (up to logarithmic factors) but that \alglcb outperforms UCB under a weighted minimax criterion reflecting the difficulty of learning the optimal policy.  Notably, this result implies that \alglcb outperforms \algucb in cases where the offline data is generated by a near-optimal policy. \cite{rashidinejad2023bridgingofflinereinforcementlearning} further studied \alglcb, introducing a concentrability coefficient to measure how close the offline dataset is to being an expert dataset. They showed that \alglcb is adaptively optimal in the entire concentrability coefficient range for contextual bandits and MDP. They also showed that for MAB, \alglcb is also adaptively optimal for a wide set of concentrability coefficients, excluding only datasets where the optimal arm is drawn with probability larger than $1/2$ and where "play the most played arm" achieves an exponential convergence rate to the optimal policy.
It is important to note that these findings do not contradict the results of \cite{xiao2021optimalitybatchpolicyoptimization}.  \cite{rashidinejad2023bridgingofflinereinforcementlearning} fixed the concentrability coefficient in the minimax analysis, whereas \cite{xiao2021optimalitybatchpolicyoptimization} did not. Finally, \cite{rashidinejad2023bridgingofflinereinforcementlearning} also showed that the \alglcb algorithm can compete with any target policy that is covered in the offline data, regardless of the performance of that policy.

Two key insights emerge from these theoretical studies:
minimax regret does not fully capture the performance gap between UCB and \alglcb in offline learning, and \alglcb outperforms UCB when the logging policy is close to optimal. 
Because it is crucial to find a policy that is at least comparable to a previously deployed policy, there has been a series of works in offline RL literature that studied the regret against the logging policy or any baseline policy. Some of those methods are based on the pessimism principle. \cite{xie2022armormodelbasedframeworkimproving} proposed optimizing the worst-case regret against a baseline policy, which they called relative pessimism. They showed that the method finds a policy that performs as well as the baseline policy and learns the best policy among those supported in offline data when the baseline policy is also supported in the data. \cite{cheng2022adversarially} proposed a different optimization formulation also based on relative pessimism and showed that the algorithm improves over the logging policy. They also showed that the method can compete with policies covered in offline data. Other methods to improve over the logging policy that are not (directly) based on pessimism have been proposed as well, for example, those based on the idea that the learned policy should not be too far from the logging policy (e.g., \citealt{fujimoto2021minimalist}). However, these methods focus exclusively on the offline setting and thus only explore one extreme of the offline-to-online spectrum. Our work shares their objective of competing against the logging policy but extends beyond purely offline learning.  

\textbf{Offline-to-Online} 
While the literature on offline-to-online learning is still new, it is rapidly growing. A first line of work can be considered answering the following question: how can we adapt an online learning algorithm to achieve good performance when the offline data set is rich enough to learn a near-optimal policy? In some cases, classical online learning methods that incorporate offline data naturally achieve strong performance. For MABs, \cite{MABwithHistory} showed that a logarithmic amount of offline data can reduce the regret from logarithmic to constant, and their algorithm implements optimism but incorporates offline data. Results of \cite{gur2020adaptive} imply a lower bound for regret in offline-to-online MABs (see Appendix G of \citealt{bu2021onlinepricingofflinedata} for a summary), offering more insights into how additional data influences achievable regret. Their setting is broader than ours, allowing for new information to arrive sequentially, with offline data as a special case. They, too, employed classical online learning methods, including Thompson sampling and UCB-like algorithms augmented with offline data.
This question was also studied in dynamic pricing by \cite{bu2021onlinepricingofflinedata},  a class of contextual bandits. The authors of this last paper proposed an optimism-driven algorithm and showed that the achievable regret as a function of $T$ decreases notably as the size of the offline data set increases, which the authors coined as a ``phase transition''. It is also notable that for MABs, the optimal regret phase transition is achieved only when offline data is balanced over the arms \citep{bu2021onlinepricingofflinedata}. \cite{chen2022data} also investigated offline-to-online learning, focusing on healthcare applications where offline and online data distributions may differ. Their approach, like the others, is based on optimism or Thompson sampling.

Our work builds on this literature in a fundamental way. The above papers enhance classical online learning methods, such as UCB-like or Thompson-sampling, by incorporating additional offline data and studying how this affects regret against optimality. Here, we bring offline learning methods and metrics into the offline-to-online learning setting and thus put both offline and online learning into perspective. 
%We also refined some results that were obtained by \cite{gur2020adaptive}. In particular, we refine some of their bounds that were tight in regimes of their more general setting but are loose in regimes of key interest in this paper, most importantly for small values of the horizon $T$.

Another stream of offline-to-online works focuses on minimizing computational and sample costs rather than minimizing regret over a horizon. This setting with that objective is more commonly called hybrid reinforcement learning. Their goal is to find a good policy using both offline and online data while minimizing the number of samples or computational resources required \citep{Song2022-vg,xie2022policyfinetuningbridgingsampleefficient,ball2023efficient,
wagenmaker2023leveraging,Li2023-lv, li2024reward, zhou2023offlinedataenhancedonpolicy}. These works differ from ours in both goal and methodology: they do not address the central question of how best to navigate the transition from offline to online learning as TT varies. Furthermore, our focus on MABs sidesteps computational efficiency concerns.

Despite these prior contributions, the theoretical understanding of offline-to-online learning remains incomplete, particularly regarding how to balance conservatism and exploration as learning shifts from purely offline to purely online. Our paper directly addresses this gap, with a specific focus on MABs.