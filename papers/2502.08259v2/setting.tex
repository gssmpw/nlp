%!TEX root =  main.tex
\section{Setting and Notation}

We consider a multi-armed bandit problem with $K$ arms. At the beginning the learner is given access to $m_i$ rewards sampled from $\mathcal{P}_i$, a distribution associated with arm $1\leq i \leq K$, which is initially unknown to the learner. We let $\mu_i \in [0,1]$ denote the mean of $\mathcal{P}_i$, which is assumed to be a 1-subgaussian. That is, for any $\lambda\in \R$ real number, for any $1\leq i \leq K$,
\begin{align*}
\int \exp(\lambda (x-\mu_i)) \cP_i(dx) \le \exp( \lambda^2/2) \,.
\end{align*}
We let $\Theta= \mathcal{G}_1^K \times \mathbb{N}^K$ and for $\textbf{m}\in \mathbb{N}^K$, we let $\Theta_{\mathbf{m}}= \mathcal{G}_1^K \times \textbf{m}$ where $\mathcal{G}_1$ is the set of 1-subgaussian distributions over the reals.
 We denote the maximum value of the means by $\mu^*:= \max_i \mu_i$, and denote the suboptimality gap of an arm $i$ by $\Delta_i=\mu^*-\mu_i$.  We denote $\hat{\mu}_i^0$ the empirical mean of the $m_i$ a priori observed rewards from arm $i$ observations, and let $m=\sum_{i \in [K]}m_i$. The implicit assumption here is that the means and sample sizes contain all the information the learner will need to know about the data. We also use the shorthands:
\[
\hat{\mu}_0:= \frac{\sum_{i \in [K]}m_i\hat{\mu}_i^0}{m},\quad  \mu_0:= \frac{\sum_{i \in [K]}m_i\mu_i}{m} \text{ and }\Delta_0= \mu_*-\mu_0.
\]
Here $\mu_0$ is the mean reward of the policy that chooses arm $i$ with probability $\pi_i= \frac{m_i}{m}$. While we keep $\mathbf{m}= \left(m_i\right)_{i \in [K]}$ non-random, we will think of the data as being collected from following the policy $\pi= \left(\pi_i\right)_{i \in [K]}$, which we call the logging policy. We expect our results to extend with little change to the case when $\mathbf{m}= \left(m_i\right)_{i \in [K]}$ is the random number of times arm $i$ is chosen when $\pi$ is followed for $m$ steps.
At every round  $t=1,\ldots,T$, the learner pulls arm $I(t)$ and receives reward $x^t_{I(t)}$, where $x_i^t\sim \mathcal{P}_i$, $i\in [K]$, denotes the reward of arm $i$ at time $t$.  Over horizon $T$, we define the pseudo-regret as

\[
R(T)= 
 T\mu^* - \sum_{t=1}^T\mu_{I(t)}.
\]

While we define the regret with respect to the logging policy as

\[
R^{\text{log}}(T)=
T\mu_0 - \sum_{t=1}^T\mu_{I(t)}.
\]

Each combination of an instance $\theta\in \Theta$ and an algorithm $\mathcal{A}$ followed by a learner induces a probability distribution over $([K]\times \R)^\mathbb{N}$, 
the sequences of arm pulls and rewards of arbitrary length.
%\todof[inline]{Comment by Csaba: we do not actually need to specify this here; can be done if we need $\mathbb{P}_{\theta,\mathcal{A}}$. F: we do need it. Cs: Fine. I just meant whether we need the infinite sequences or just sequences of some fixed length. We do not need the infinite length space if we ever talk about finite length sequences. But we can do the infinite length thing; in this case the naive reader may wonder about what measurability structure we use but let's pretendd our readers know this (or don't care).} 
We denote by the subscripts $\theta$ and $\mathcal{A}$ the expectations and probabilities induced by the pair $\theta$ and $\mathcal{A}$. The worst-case expected regret of an algorithm $\mathcal{A}$ over $\Theta_{\mathbf{m}}$ and horizon $T$ is  defined as
\[
\mathcal{R}_\mathcal{A}(T)= \sup_{\theta \in \Theta_{\mathbf{m}}}\mathbb{E}_{\theta, \mathcal{A}}[R(T)].
\]
Similarly, we let
\[
\mathcal{R}^{\text{log}}_\mathcal{A}(T)= \sup_{\theta \in \Theta_{\mathbf{m}}}\mathbb{E}_{\theta, \mathcal{A}}[R^{\text{log}}(T)],
\]
with the understanding that $\mathbf{m}$ will be clear from the context when these are used.
Note that the expectations are taken with respect to the randomness of both the offline data and the rewards incurred during the interaction, in addition to any randomness coming from the algorithm. Also, in the above definitions, the number of observations per arm is fixed.


At round $t$, we will let $T_i(t)$ denote the number of times arm $i$ has been pulled so far and $\hat{\mu}_i^t$ the empirical mean for arm $i$ combining offline and online samples. We also let

\[
\overline{\mu}_i(t)=\hat{\mu}_i^t+\sqrt{\frac{\log(K/\delta)}{2(m_i+T_i(t))}},
\]
and 
%\todof[inline]{Csaba asks: why not min in the upper bound? F: Because it is not needed, whereas it is needed in the lower bound for the analysis of OtO. Cs: Fair enough (I realized this), just wondered whether it would be better or worse.. In terms of how the OtO performs. The min gives you tighter intervals and it is for free. No? By the way, if we change it to min (ever, maybe after review), then one should still explain that the analysis only uses that the lower bounds are monotonically increasing, and we don't need the same for the upper bound.}
\[
\underline{\mu}_i(t)=\max_{t'\leq t} \left\{\hat{\mu}_i^{t'}-\sqrt{\frac{\log(K/\delta)}{2(m_i+T_i(t'))}}\right\}.
\]
These values are constructed so that $\underline{\mu}_i(t)$ is increasing with $t$ and the following lemma holds:
\begin{lemma}\label{lem:hoeff}
    For any instance $\theta$, any algorithm $\mathcal{A}$, and any $T\geq0$ it holds that
    \[
    \mathbb{P}_{\theta,\mathcal{A}}\left(\exists i \in [K] \text{ and } \exists 0\leq t\leq T \text{ s.t. } \overline{\mu}_i(t)<\mu_i\right)\leq T\delta,
    \]
    and
     \[
    \mathbb{P}_{\theta,\mathcal{A}}\left(\exists i \in [K]\text{ and } \exists 0\leq t\leq T \text{ s.t. } \underline{\mu_i}(t)>\mu_i\right)\leq T\delta.
    \]
\end{lemma}

\textit{Proof}: Both inequalities hold by Hoeffding's inequality \citep{Vershynin_2018} and a union bound over  $K$ arms and $T$ rounds. \hfill \(\Box\)

In the rest of the paper, we omit the subscripts $\theta$ and $\mathcal{A}$ when it is clear from the context.