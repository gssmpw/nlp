%!TEX root =  main.tex
\section{Regret Analysis of UCB and LCB}\label{sec:intersetanalysis}

In this section, we systematically investigate the performance of UCB and LCB as promised. The goal here is to paint a picture as comprehensive as possible over the whole spectrum of offline-to-online learning and different compositions of offline data, which may be of interest on its own. The first step will be to focus on the minimax regret against an optimal arm. Then, we will study the two approaches'  regret against the logging policy. 
 

Before presenting the precise statements of the results, we provide a summary with tables and figures for illustration. The first line of results presented concerns the pseudo regret against optimality and is summarized in \cref{fig:pseudoregret}. 
The first and second rows of the table correspond to UCB and LCB, respectively. The columns are labeled with a specific horizon. Here, the choices $T=1$ and $T\gg m$ correspond to short and long horizons, respectively. That horizon $T=m$ corresponds to the case when the amount of online collected data matches the size of the offline data; this is the time when we expect the online data to make some difference for the first time. 


First, it is shown that \algucb\ achieves minimax optimality (up to poly-logarithmic factors) for all horizons $T\geq 0$, with a more refined dependence on the composition of the offline data compared to prior results in the literature. As for \alglcb, it is also minimax optimal for $T=1$, but this is no longer the case as $T$ increases. It would be tempting to stop here and simply accept that \algucb, being minimax optimal over the whole range of $T$, is a good algorithm for offline-to-online setting. However, the second line of results paints a different picture.

\begin{table}[bth] 
\begin{center}
\renewcommand{\arraystretch}{1.4}
\begin{tabular}{>{\centering\arraybackslash}m{3cm} | |>{\centering\arraybackslash}m{3cm} |>{\centering\arraybackslash}m{3cm} |>{\centering\arraybackslash}m{3cm} }
\toprule
$T$ & $T = 1$ & $T = m$ & $T \gg m$ \\
 \hline\hline
 \addlinespace

$\mathcal{R}_{\text{UCB}}(T)$ & $\sqrt{\frac{1}{\min_i m_i}}$ & $m\sqrt{\frac{K}{\sum_i m_i}}$ & $\sqrt{KT}$ \\
\addlinespace
\hline
\addlinespace

$\mathcal{R}_{\text{LCB}}(T)$ & $\sqrt{\frac{1}{\min_i m_i}}$ & $m\sqrt{\frac{1}{\min_i m_i}}$ & $T\sqrt{\frac{1}{\min_i m_i}}$ \\
\bottomrule
\end{tabular}
\vspace{0.2cm}
\caption{Evolution of the pseudo regret of \alglcb\ and \algucb\ as $T$ grows (matching upper and lower bounds-ignoring poly log terms-are displayed, exact expressions in the lemmas).}
\label{fig:pseudoregret}
\end{center}
\end{table}





\cite{Xiao2021OnTO} showed that the minimax criterion was not enough to distinguish \algucb{} and \alglcb, and suggested an additional criterion. In offline learning, despite the similar minimax guarantees, \alglcb{} is often favored over \algucb. There is an intuitive explanation for this: even when there is only a single iteration, \algucb\ will explore if a single arm does not have any sample, and the gathered knowledge has no chance of being exploited. \alglcb{} does the opposite in offline learning and sticks to what the offline data indicates as a ``good enough'' arm. Intuition suggests that if offline data are focused on good arms, then exploration may affect the performance negatively, whereas \alglcb{} exploits the information immediately. Also, in the literature, pessimistic algorithms have been shown to be competitive against the logging policy in offline learning. These suggest that competing against the data generating policy, i.e., the logging policy, can be another reasonable criterion for offline-to-online learning. This is what we set out to do in this second line of results, summarized in \cref{fig:regretlogging}.
 First, we observe that at $T=1$, which corresponds to offline learning, LCB has a lower regret whenever offline data is not perfectly balanced. The gap is greater when offline data are completely concentrated on one arm, i.e., $m_i=m$ for some $i \in [K]$. As $T$ increases, the gap between the two guarantees for the two algorithms decreases slowly. The point at which it closes depends on the repartition of offline data.


\begin{table}[htb]
\begin{center}
\renewcommand{\arraystretch}{1.4}
\begin{tabular}{>{\centering\arraybackslash}m{1.5cm} | |>{\centering\arraybackslash}m{1.5cm}|>{\centering\arraybackslash}m{1.5cm} |>{\centering\arraybackslash}m{3.cm}|>{\centering\arraybackslash}m{2cm}  |>{\centering\arraybackslash}m{1.5cm}|>{\centering\arraybackslash}m{1.5cm} }
 \toprule
 $T$ & \multicolumn{2}{c|}{$1$}&\multicolumn{2}{c|}{$T=m$} &\multicolumn{2}{c}{$T\gg m$ } \\ 
  \cline{2-7}
    & LB & UB  &LB & UB&LB & UB\\
     \hline
     \hline
\addlinespace
 $\mathcal{R}^{\text{log}}_\algucb(T)$ & \multicolumn{2}{c|}{$\sqrt{\frac{1}{\min_i m_i}}$} & {\small $\sum_{i=1}^K\left(\frac{m}{K}-m_i\right)\rho_i$}&$\sqrt{KT}$& $0$&$\sqrt{KT}$ \\ 
 \addlinespace
  \hline
\addlinespace
 $\mathcal{R}^{\text{log}}_\alglcb(T)$ & $\frac{\sqrt{m_2}}{m}$&$\frac{\sum_{i}\sqrt{m_i}}{m} $ &  $\sqrt{m_2}$ & $\sum_{i=1}^m\sqrt{m_i}$&$T\frac{\sqrt{m_2}}{m}$&$T\frac{\sum_{i}\sqrt{m_i}}{m} $\\ 
 \addlinespace
 \hline
\end{tabular}
\end{center}
\vspace{0.2cm}
\caption{Evolution of the regrets against the logging policy as $T$ grows (ignoring poly log terms, exact expressions in the Lemmas), assuming wlog $m_1\geq m_2\geq \ldots\geq m_K$, and with $\rho_i=\left[\sqrt{\frac{1}{m_i+\frac{m}{K}}}-\sqrt{\frac{1}{m_1+\frac{m}{K}}}\right]$.}
\label{fig:regretlogging}
\end{table}

One lesson from these two lines of results is that if $T\gg m$, then \algucb\ dominates \alglcb\ in both objectives. It is also the case when offline data are uniformly spread over the arms, i.e., $m_i=\frac{m}{K}$ for all $i \in [K]$. However, when the horizon is short ($m\gg T$) and offline data do not cover the arms uniformly, then \alglcb\ dominates \algucb in both objectives. Thus, neither algorithm dominates the other for all horizons and all possible data coverage. The algorithm we introduced, \algoname, finds a trade-off between the two strategies.

\begin{figure}[htb]
\begin{center}
  \includestandalone[scale=0.8]{plots/pareto_same_sample}%     without .tex extension
  \caption{Evolution of the two regret measures when offline samples are uniformly spread between the arms, i.e., $m_i=\frac{m}{K}$ for all $ I \in [K]$. The best algorithms are the ones closest to $(0,0)$. From left to right, the horizon is $T=1$, $T=m$ and $T\gg m. $ In the last plot, we assume $T\gg m$. The \algoname\ in the plot uses $\alpha=1$.}
\label{fig:tradeoffucblcbbanlanced}
\end{center}
\end{figure}

\cref{fig:tradeoffucblcbbanlanced,fig:tradeoffucblcbconcentrated} illustrate these points visually. In the figures, we plot the upper bound on the regret (ignoring poly-log terms) of all three algorithms for each specific composition of the offline arm counts and varying values of $T$. For the chosen offline arm counts, the upper bounds on \algucb\ and \alglcb\ are tight up to poly-log terms. 
In \cref{fig:tradeoffucblcbbanlanced}, we see the evolution of the regret for perfectly balanced datasets. Here, \algucb\ dominates \alglcb\ for all horizons, and \algoname\ behaves like \algucb.
In \cref{fig:tradeoffucblcbconcentrated}, we see the evolution of the regret for highly skewed offline datasets: all the samples are concentrated on the first two arms. 
Here, the picture is somewhat more complicated: \algucb\ dominates in the regret against optimality, whereas \alglcb\ dominates in the regret with respect to the logging policy. We see that \algoname finds a trade-off between the two, and the trade-off point depends on the value of the parameter $\alpha$ chosen. With $\alpha=\sqrt{K}$, for example, \algoname\ improves upon the regret against the logging policy of \algucb without downgrading (up to a multiplicative constant that does not appear on the plot) the regret against optimality. With $\alpha=1$, \algoname\ improves on the regret against optimality of \alglcb, without downgrading (again up to a multiplicative constant) the regret against the logging policy.

\begin{figure}[htb]
\begin{center}
  \includestandalone[scale=0.8]{plots/pareto_one_sample}%     without .tex extension
  % or use \input{mytikz}
  \caption{Evolution of the two regrets when all offline samples are concentrated on two arms, i.e. $m_1=m_2=\frac{m}{2K}$. From left to right, the horizon is $T=1$ and $T=m$. For readability purposes, we do not plot $T\gg m$, as the relative behavior of the algorithms for that horizon would be quite similar to the one at $T=m$, but with an even larger ratio between the horizontal and vertical axis. }
\label{fig:tradeoffucblcbconcentrated}
\end{center}
\end{figure}


The rest of the section is dedicated to stating the results summarized in \cref{fig:pseudoregret,fig:regretlogging} precisely.


\subsection{Minimax regret}
We will start by proving a lower bound on the minimax regret of any algorithm for offline-to-online learning. We also derive a matching upper bound given by \algucb. 

\begin{theorem}[Lower bound on the minimax regret of any algorithm for offline-to-online learning]\label{prop:lowerboundminimax}
For any $T\geq 1$ and for any algorithm $\mathcal{A}$, we have
    \begin{equation*}
 \mathcal{R}_{\mathcal{A}}(T)\geq \frac{1}{31} T\sqrt{\max_{J\subseteq [K]}\frac{|J|}{T-1+\sum_{j\in J} m_j}}.
\end{equation*}
\end{theorem}
\textit{Sketch of Proof}: The proof is a refined application of classical information-theoretic lower bounds. The main technical challenge lies in defining an appropriate threshold, $\Delta$, which determines when arms cannot be reliably identified as suboptimal. This threshold is given by:  
\[
\Delta = \sqrt{\max_{J \subseteq [K]} \frac{|J|}{2(T - 1 + \sum_{j \in J} m_j)}}.
\]  Details can be found in \cref{sec:omittedproofs}.


The above bound may be hard to interpret. To get a sense of the behavior of the lower bound, we look at two special cases. Assume that $m_1\geq \ldots \geq m_K$. Then letting $J=\left\{m_2,\ldots, m_K\right\}$, we get
\begin{equation*}
 \mathcal{R}_{\mathcal{A}}(T)\geq \frac{1}{31} T\sqrt{\frac{(K-1)}{T-1+m-\max_i m_i}},
\end{equation*}
which recovers the $\Omega(\sqrt{TK})$ lower bound for $T$ large. Now, when $J=\left\{ m_K\right\}$, we get
\begin{equation*}
\mathcal{R}_{\mathcal{A}}(T)\geq \frac{1}{31} T\sqrt{\frac{1}{T-1+\min_i m_i}},
\end{equation*}
 which tells us that for $T$ small, the minimum count in the offline data sets a lower limit on the regret. 

We now study the regret of \algucb\ and \alglcb. We start with \algucb, showing that it is minimax optimal. We also provide an instance-dependent bound on the regret.

\begin{theorem}[\algucb's upper bound on the minimax regret]\label{prop:regretminimaxucb}
    For any $T\geq 1$ and any $\theta\in \Theta$, with probability at least $1-2T^2\delta$,
    \[
R_\algucb(T)\leq \sum_{i=1}^K \Delta_i\left( \frac{2}{\Delta_i^2} \log(K/\delta)-m_i\right)_++\sum_{i=1}^K \Delta_i.
    \]
    Also, we have the following instance-independent bound: 
    \[
\mathcal{R}_{\algucb}(T)\leq \min\left( \max_{J\subseteq [K]}2T\sqrt{\frac{2|J|\log(K/\delta)}{T+\sum_{j\in J}m_j}}+|J|;T\sqrt{\frac{2\log(K/\delta)}{\min_i m_i}}\right)+2T^2\delta.
    \]
 \end{theorem}
\textit{Sketch of Proof}: The first bound is obtained through the usual techniques of upper bounding the number of pulls from each arm. The second approach demands more intricate algebraic computations. It entails separately bounding the regret for two categories of arms: those whose gap exceeds a "detection threshold" and those whose gap falls below it. The proof can be found in \cref{sec:omittedproofs}. \hfill \(\Box\)

Theorems~\ref{prop:lowerboundminimax} and \ref{prop:regretminimaxucb} illustrate the difficulty of offline-to-online learning measured by the minimax regret and show how the difficulty depends on the composition of offline data. The derived bounds align with those obtained by \cite{pmlr-v235-cheung24a}, although the proof techniques differ. These differing techniques result in distinct implicit expressions for the bounds. However, it can be shown that the solution to the linear program (LP) in \cite{pmlr-v235-cheung24a} and the inverse of the solution to our maximization problem are within multiplicative constants of each other.



When it comes to \alglcb, the following result shows that the algorithm fully depends on the quality of the offline data. This is perhaps unsurprising, as the algorithm has no built-in exploration: its knowledge of the arms may not improve with the online interactions. 

\begin{proposition}[Minimax regret of \alglcb]\label{prop:minimaxregretlcb}
    For $T\geq 1$, we have
    \[
   \min\left(0.07 T, 0.15 T\sqrt{\frac{1}{\min_i m_i}}\right)\leq  \mathcal{R}_\alglcb(T) \leq T\sqrt{\frac{2\log(K/\delta)}{\min_i m_i}}+ 2T^2\delta.
    \]
\end{proposition}
\textit{Sketch of proof}: For the lower bound, we construct an instance where the optimal arm is the arm with the minimum count in offline samples, and all other arms have deterministic rewards. By the structure of \alglcb, if the optimal arm is not picked in the first iteration, it will never be picked. We show that this happens with a constant probability with a proper choice of distribution parameters. The upper bound is derived by bounding the gap between the mean of the chosen arm and $\mu_*$. We defer the technical steps of the proof to \cref{sec:omittedproofs}. \hfill \(\Box\)



\subsection{Regret against the logging policy}


In this section, we present the results shown in Table~\ref{fig:regretlogging}. All proofs are in Section~\ref{sec:omittedproofs}. 

\begin{proposition}[\algucb's regret against the logging policy for $T=1$]\label{prop:loginucbT=1}
We have
\[
0.07\min \left(1;\sqrt{\frac{1}{\min_i m_i}}\right)\leq \mathcal{R}^{\text{log}}_\textsc{UCB}(1) \leq \sqrt{\frac{2\log(\frac{K}{\delta})}{\min_i m_i}}+2\delta.
\]
\end{proposition}

\textit{Sketch of Proof}: For the lower bound, we construct an instance in which all arms have the same mean, except the arm with the minimal count, which has a slightly lower mean. With a proper choice of distribution parameters, \algucb\ picks the worst arm with a constant probability. The upper bound is a consequence of the upper bound on the minimax regret obtained in \cref{prop:regretminimaxucb}. \hfill \( \Box\)

\begin{proposition}[\algucb's regret against the logging policy for general $T$]\label{prop:lowerboundUCBanyT}
For any $T>0$, $\frac{T}{K}\in \mathbb{N}$, we have
    \[\mathcal{R}^{\text{log}}_{\textsc{UCB}}(T)\geq T\sum_{i=1}^K\left(\frac{1}{K}-\frac{m_i}{m}\right)\left[\sqrt{\frac{1}{2(m_i+\frac{T}{K})}}-\sqrt{\frac{1}{2(\max_{j\in [K]}m_j+\frac{T}{K})}}\right],\]
and
\[
\mathcal{R}^{\text{log}}_{\textsc{UCB}}(T)\leq \mathcal{R}_{\textsc{UCB}}(T).
\]
\end{proposition}

\textit{Sketch of Proof}: The upper bound is a consequence of the definitions, as $\mu_*\geq \mu_0$ always holds. For the lower bound, we construct an instance where the mean of each arm decreases with the number of offline samples for that arm, and all rewards are deterministic. Then, using the property that \algucb matches the upper bounds of arms (up to rounding effects, roughly speaking), we show that for a proper choice of mean parameters, each arm is sampled at least $\frac{T}{K}$ times in the online phase.  \hfill \( \Box\)

The above bound may be hard to interpret. It is easier to interpret when instantiated for extreme values of offline arm counts. For instance, if all of the offline samples come from a single arm, i.e., $m_1=m$ and $m_i=0$ for any $i>1$, we obtain
    \[\mathcal{R}^{\text{log}}_{\textsc{UCB}}(T)\geq T\frac{K-1}{K}\left[\sqrt{\frac{K}{2T}}-\sqrt{\frac{1}{2(m+\frac{T}{K})}}\right].\]
For any $T\leq K m$, this entails
    \[\mathcal{R}^{\text{log}}_{\textsc{UCB}}(T)\geq \frac{1}{10}\sqrt{KT}.\]
Similarly, if we have balanced samples for half of the arms ($m_i=\frac{2m}{K}$, for each $i \leq \frac{K}{2}$, and $m_i=0$ for $i\geq \frac{K}{2}$), then, for any $T\leq m$, we have
        $\mathcal{R}^{\text{log}}_{\textsc{UCB}}(T)\geq \frac{1}{10}\sqrt{KT}.$
On the other hand, if $m_i=\frac{m}{K}$ for all $i \in [K]$, we get $\mathcal{R}^{\text{log}}_{\textsc{UCB}}(T)\geq 0$, which may not be a strong lower bound, but it aligns with the well-known observation that a uniform offline dataset is favorable for \algucb's theoretical guarantee. 

\begin{remark}
    The upper bound obtained here does not match the lower bound for all offline count repartitions. For instance, in the case where the offline samples are uniformly spread, the former is much larger than the latter. It remains an open question to find matching bounds in all regimes. 
\end{remark}

\begin{proposition}[ \alglcb's regret against the logging policy for general $T$]\label{prop:regretlogginglcb}
We have
\[
\mathcal{R}^{\text{log}}_\textsc{LCB}(T) \leq T\frac{\sum_i \sqrt{m_i}}{\sum_i m_i}\sqrt{2\log\left(\frac{K}{\delta}\right)}+2T^2\delta.
\]
Moreover, assuming  $m_1\geq m_2\geq \ldots\geq m_K$,
\[
\mathcal{R}^{\text{log}}_\textsc{LCB}(T)\geq 0.15 T\frac{\sqrt{m_2}}{m}.
\]
\end{proposition}



It is again informative to instantiate the above bound in extreme regimes of the offline count repartition. If $m_1=m$ and $m_i=0$ for any $i>1$, we obtain
    \[\mathcal{R}^{\text{log}}_\textsc{LCB}(T) \leq T\sqrt{\frac{2\log\left(\frac{K}{\delta}\right)}{m}}+2T^2\delta.\]  
On the other hand, if $m_i=\frac{m}{K}$ for all $i \in [K]$, we get:      \[\mathcal{R}^{\text{log}}_\textsc{LCB}(T) \leq T\sqrt{\frac{2K\log\left(\frac{K}{\delta}\right)}{m}}+2T^2\delta.\]

