@InProceedings{Imitationlearningross,
  title = 	 {Efficient Reductions for Imitation Learning},
  author = 	 {Ross, Stephane and Bagnell, Drew},
  booktitle = 	 {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
  pages = 	 {661--668},
  year = 	 {2010},
  editor = 	 {Teh, Yee Whye and Titterington, Mike},
  volume = 	 {9},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Chia Laguna Resort, Sardinia, Italy},
  month = 	 {13--15 May},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v9/ross10a/ross10a.pdf},
  url = 	 {https://proceedings.mlr.press/v9/ross10a.html},
  abstract = 	 {Imitation Learning, while applied successfully on many large real-world problems, is typically addressed as a standard supervised learning problem, where it is assumed the training and testing data are i.i.d..  This is not true in imitation learning as the learned policy influences the future test inputs (states) upon which it will be tested. We show that this leads to compounding errors and a regret bound that grows quadratically in the time horizon of the task. We propose two alternative algorithms for imitation learning where training occurs over several episodes of interaction. These two approaches share in common that the learner’s policy is slowly modified from executing the expert’s policy to the learned policy. We show that this leads to stronger performance guarantees and demonstrate the improved performance on two challenging problems: training a learner to play 1) a 3D racing game (Super Tux Kart) and 2) Mario Bros.; given input images from the games and corresponding actions taken by a human expert and near-optimal planner respectively.}
}

@ARTICLE{Li2023-lv,
  title     = "Reward-agnostic Fine-tuning: Provable Statistical Benefits of
               Hybrid Reinforcement Learning",
  author    = "Li, G and Zhan, W and Lee, J D and Chi, Y and Chen, Y",
  journal   = "arXiv preprint arXiv:2305.10282",
  publisher = "arxiv.org",
  year      =  2023,
  url       = "http://arxiv.org/abs/2305.10282"
}

@InProceedings{MABwithHistory,
  title = 	 {Multi-armed Bandit Problems with History},
  author = 	 {Shivaswamy, Pannagadatta and Joachims, Thorsten},
  booktitle = 	 {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},
  pages = 	 {1046--1054},
  year = 	 {2012},
  editor = 	 {Lawrence, Neil D. and Girolami, Mark},
  volume = 	 {22},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {La Palma, Canary Islands},
  month = 	 {21--23 Apr},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v22/shivaswamy12/shivaswamy12.pdf},
  url = 	 {https://proceedings.mlr.press/v22/shivaswamy12.html},
  abstract = 	 {In this paper we consider the stochastic multi-armed bandit problem. However, unlike in the conventional version of this problem, we do not assume that the algorithm starts from scratch. Many applications offer observations of (some of) the arms even before the algorithm starts.  We propose three novel multi-armed bandit algorithms that can exploit this data. An upper bound on the regret is derived in each case. The results show that a logarithmic amount of historic data  can reduce  regret from logarithmic to constant. The effectiveness of the proposed algorithms  are demonstrated on a large-scale malicious URL detection problem.}
}

@inproceedings{MorelPessimistisOfflineLearning,
 author = {Kidambi, Rahul and Rajeswaran, Aravind and Netrapalli, Praneeth and Joachims, Thorsten},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {21810--21823},
 publisher = {Curran Associates, Inc.},
 title = {MOReL: Model-Based Offline Reinforcement Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/f7efa4f864ae9b88d43527f4b14f750f-Paper.pdf},
 volume = {33},
 year = {2020}
}

@ARTICLE{Song2022-vg,
  title         = "Hybrid {RL}: Using Both Offline and Online Data Can Make {RL}
                   Efficient",
  author        = "Song, Yuda and Zhou, Yifei and Sekhari, Ayush and Andrew
                   Bagnell, J and Krishnamurthy, Akshay and Sun, Wen",
  journal       = "arXiv [cs.LG]",
  month         =  oct,
  year          =  2022,
  url           = "http://arxiv.org/abs/2210.06718",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG"
}

@inproceedings{ball2023efficient,
  title={Efficient online reinforcement learning with offline data},
  author={Ball, Philip J and Smith, Laura and Kostrikov, Ilya and Levine, Sergey},
  booktitle={International Conference on Machine Learning},
  pages={1577--1594},
  year={2023},
  organization={PMLR}
}

@article{bu2021onlinepricingofflinedata,
title = {Online Pricing with Offline Data: Phase Transition and Inverse Square Law},
author = {Bu, Jinzhi and Simchi-Levi, David and Xu, Yunzong},
year = {2022},
journal = {Management Science},
volume = {68},
number = {12},
pages = {8568-8588},
abstract = {This paper investigates the impact of pre-existing offline data on online learning in the context of dynamic pricing. We study a single-product dynamic pricing problem over a selling horizon of T periods. The demand in each period is determined by the price of the product according to a linear demand model with unknown parameters. We assume that before the start of the selling horizon, the seller already has some pre-existing offline data. The offline data set contains n samples, each of which is an input-output pair consisting of a historical price and an associated demand observation. The seller wants to use both the pre-existing offline data and the sequentially revealed online data to minimize the regret of the online learning process. We characterize the joint effect of the size , location , and dispersion of the offline data on the optimal regret of the online learning process. Specifically, the size , location , and dispersion of the offline data are measured by the number of historical samples, the distance between the average historical price and the optimal price, and the standard deviation of the historical prices, respectively. For both single-historical-price setting and multiple-historical-price setting, we design a learning algorithm based on the “Optimism in the Face of Uncertainty” principle, which strikes a balance between exploration and exploitation and achieves the optimal regret up to a logarithmic factor. Our results reveal surprising transformations of the optimal regret rate with respect to the size of the offline data, which we refer to as phase transitions . In addition, our results demonstrate that the location and dispersion of the offline data also have an intrinsic effect on the optimal regret, and we quantify this effect via the inverse-square law .},
keywords = {dynamic pricing; online learning; offline data; phase transition; inverse-square law},
url = {https://EconPapers.repec.org/RePEc:inm:ormnsc:v:68:y:2022:i:12:p:8568-8588}
}

@misc{buckman2020importancepessimismfixeddatasetpolicy,
      title={The Importance of Pessimism in Fixed-Dataset Policy Optimization}, 
      author={Jacob Buckman and Carles Gelada and Marc G. Bellemare},
      year={2020},
      eprint={2009.06799},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2009.06799}, 
}

@article{chen2022data,
  title={Data-pooling reinforcement learning for personalized healthcare intervention},
  author={Chen, Xinyun and Shi, Pengyi and Pu, Shanwen},
  journal={arXiv preprint arXiv:2211.08998},
  year={2022}
}

@inproceedings{cheng2022adversarially,
  title={Adversarially trained actor critic for offline reinforcement learning},
  author={Cheng, Ching-An and Xie, Tengyang and Jiang, Nan and Agarwal, Alekh},
  booktitle={International Conference on Machine Learning},
  pages={3852--3878},
  year={2022},
  organization={PMLR}
}

@article{gur2020adaptive,
  title={Adaptive sequential experiments with unknown information arrival processes},
  author={Gur, Yonatan and Momeni, Ahmadreza},
  journal={Manufacturing \& Service Operations Management},
  volume={24},
  number={5},
  pages={2666--2684},
  year={2022},
  publisher={INFORMS}
}

@inproceedings{li2022pessimismofflinelinearcontextual,
author = {Li, Gene and Ma, Cong and Srebro, Nathan},
title = {Pessimism for offline linear contextual bandits using lp confidence sets},
year = {2024},
isbn = {9781713871088},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We present a family ${widehat{pi}_p}_{pge 1}$ of pessimistic learning rules for offline learning of linear contextual bandits, relying on confidence sets with respect to different πp norms, where $widehat{pi}_2$ corresponds to Bellman-consistent pessimism (BCP), while $widehat{pi}_infty$ is a novel generalization of lower confidence bound (LCB) to the linear setting. We show that the novel $widehat{pi}_infty$ learning rule is, in a sense, adaptively optimal, as it achieves the minimax performance (up to log factors) against all ℓq-constrained problems, and as such it strictly dominates all other predictors in the family, including $widehat{pi}_2$.},
booktitle = {Proceedings of the 36th International Conference on Neural Information Processing Systems},
articleno = {1525},
numpages = {14},
location = {New Orleans, LA, USA},
series = {NIPS '22}
}

@article{li2024reward,
  title={Reward-agnostic fine-tuning: Provable statistical benefits of hybrid reinforcement learning},
  author={Li, Gen and Zhan, Wenhao and Lee, Jason D and Chi, Yuejie and Chen, Yuxin},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{rajaraman2020fundamentallimitsimitationlearning,
author = {Rajaraman, Nived and Yang, Lin F. and Jiao, Jiantao and Ramchandran, Kannan},
title = {Toward the fundamental limits of imitation learning},
year = {2020},
isbn = {9781713829546},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Imitation learning (IL) aims to mimic the behavior of an expert policy in a sequential decision-making problem given only demonstrations. In this paper, we focus on understanding the minimax statistical limits of IL in episodic Markov Decision Processes (MDPs). We first consider the setting where the learner is provided a dataset of $N$ expert trajectories ahead of time, and cannot interact with the MDP. Here, we show that the policy which mimics the expert whenever possible is in expectation $lesssim frac{|mathcal{S}| H^2 log (N)}{N}$ suboptimal compared to the value of the expert, even when the expert plays a stochastic policy. Here $mathcal{S}$ is the state space and $H$ is the length of the episode. Furthermore, we establish a suboptimality lower bound of $gtrsim |mathcal{S}| H^2 / N$ which applies even if the expert is constrained to be deterministic, or if the learner is allowed to actively query the expert at visited states while interacting with the MDP for $N$ episodes. To our knowledge, this is the first algorithm with suboptimality having no dependence on the number of actions, under no additional assumptions. We then propose a novel algorithm based on minimum-distance functionals in the setting where the transition model is given and the expert is deterministic. The algorithm is suboptimal by $lesssim |mathcal{S}| H^{3/2} / N$, matching our lower bound up to a $sqrt{H}$ factor, and breaks the $mathcal{O}(H^2)$ error compounding barrier of IL.},
booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
articleno = {245},
numpages = {11},
location = {Vancouver, BC, Canada},
series = {NIPS '20}
}

@inproceedings{rashidinejad2023bridgingofflinereinforcementlearning,
author = {Rashidinejad, Paria and Zhu, Banghua and Ma, Cong and Jiao, Jiantao and Russell, Stuart},
title = {Bridging offline reinforcement learning and imitation learning: a tale of pessimism},
year = {2024},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Offline (or batch) reinforcement learning (RL) algorithms seek to learn an optimal policy from a fixed dataset without active data collection. Based on the composition of the offline dataset, two main methods are used: imitation learning which is suitable for expert datasets, and vanilla offline RL which often requires uniform coverage datasets. From a practical standpoint, datasets often deviate from these two extremes and the exact data composition is usually unknown. To bridge this gap, we present a new offline RL framework that smoothly interpolates between the two extremes of data composition, hence unifying imitation learning and vanilla offline RL. The new framework is centered around a weak version of the concentrability coefficient that measures the deviation of the behavior policy from the expert policy alone. Under this new framework, we ask: can one develop an algorithm that achieves a minimax optimal rate adaptive to unknown data composition? To address this question, we consider a lower confidence bound (LCB) algorithm developed based on pessimism in the face of uncertainty in offline RL. We study finite-sample properties of LCB as well as information-theoretic limits in multi-armed bandits, contextual bandits, and Markov decision processes (MDPs). Our analysis reveals surprising facts about optimality rates. In particular, in both contextual bandits and RL, LCB achieves a faster rate of 1/N for nearly-expert datasets compared to the usual rate of 1/√N in offline RL, where N is the batch dataset sample size. In contextual bandits, we prove that LCB is adaptively optimal for the entire data composition range, achieving a smooth transition from imitation learning to offline RL. We further show that LCB is almost adaptively optimal in tabular MDPs.},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {895},
numpages = {15},
series = {NIPS '21}
}

@article{swaminathan2015counterfactualriskminimizationlearning,
  author  = {Adith Swaminathan and Thorsten Joachims},
  title   = {Batch Learning from Logged Bandit Feedback through Counterfactual Risk Minimization},
  journal = {Journal of Machine Learning Research},
  year    = {2015},
  volume  = {16},
  number  = {52},
  pages   = {1731--1755},
  url     = {http://jmlr.org/papers/v16/swaminathan15a.html}
}

@inproceedings{wagenmaker2023leveraging,
  title={Leveraging offline data in online reinforcement learning},
  author={Wagenmaker, Andrew and Pacchiano, Aldo},
  booktitle={International Conference on Machine Learning},
  pages={35300--35338},
  year={2023},
  organization={PMLR}
}

@misc{wu2019behaviorregularizedofflinereinforcement,
      title={Behavior Regularized Offline Reinforcement Learning}, 
      author={Yifan Wu and George Tucker and Ofir Nachum},
      year={2019},
      eprint={1911.11361},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1911.11361}, 
}

@inproceedings{xiao2021optimalitybatchpolicyoptimization,
  title={On the Optimality of Batch Policy Optimization Algorithms},
  author={Chenjun Xiao and Yifan Wu and Tor Lattimore and Bo Dai and Jincheng Mei and Lihong Li and Csaba Szepesvari and Dale Schuurmans},
  booktitle={International Conference on Machine Learning},
  year={2021},
  url={https://api.semanticscholar.org/CorpusID:233033732}
}

@misc{xie2022armormodelbasedframeworkimproving,
      title={ARMOR: A Model-based Framework for Improving Arbitrary Baseline Policies with Offline Data}, 
      author={Tengyang Xie and Mohak Bhardwaj and Nan Jiang and Ching-An Cheng},
      year={2022},
      eprint={2211.04538},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2211.04538}, 
}

@inproceedings{xie2022policyfinetuningbridgingsampleefficient,
author = {Xie, Tengyang and Jiang, Nan and Wang, Huan and Xiong, Caiming and Bai, Yu},
title = {Policy finetuning: bridging sample-efficient offline and online reinforcement learning},
year = {2024},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Recent theoretical work studies sample-efficient reinforcement learning (RL) extensively in two settings: learning interactively in the environment (online RL), or learning from an offline dataset (offline RL). However, existing algorithms and theories for learning near-optimal policies in these two settings are rather different and disconnected. Towards bridging this gap, this paper initiates the theoretical study of policy finetuning, that is, online RL where the learner has additional access to a "reference policy" μ close to the optimal policy undefined* in a certain sense. We consider the policy finetuning problem in episodic Markov Decision Processes (MDPs) with S states, A actions, and horizon length H. We first design a sharp offline reduction algorithm—which simply executes μ and runs offline policy optimization on the collected dataset—that finds an ε near-optimal policy within \~{O}(H3SC*/ε2) episodes, where C* is the single-policy concentrability coefficient between μ and π*. This offline result is the first that matches the sample complexity lower bound in this setting, and resolves a recent open question in offline RL. We then establish an Ω(H3S min{C*, A}/ε2) sample complexity lower bound for any policy fine-tuning algorithm, including those that can adaptively explore the environment. This implies that—perhaps surprisingly—the optimal policy finetuning algorithm is either offline reduction or a purely online RL algorithm that does not use μ. Finally, we design a new hybrid offline/online algorithm for policy finetuning that achieves better sample complexity than both vanilla offline reduction and purely online RL algorithms, in a relaxed setting where μ only satisfies concentrability partially up to a certain time step. Overall, our results offer a quantitative understanding on the benefit of a good reference policy, and make a step towards bridging offline and online RL.},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {2098},
numpages = {13},
series = {NIPS '21}
}

@inproceedings{yin2020nearoptimalprovableuniformconvergence,
  title={Near-Optimal Provable Uniform Convergence in Offline Policy Evaluation for Reinforcement Learning},
  author={Ming Yin and Yu Bai and Yu-Xiang Wang},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  year={2021},
  url={https://api.semanticscholar.org/CorpusID:261937056}
}

@inproceedings{yu2020mopomodelbasedofflinepolicy,
author = {Yu, Tianhe and Thomas, Garrett and Yu, Lantao and Ermon, Stefano and Zou, James and Levine, Sergey and Finn, Chelsea and Ma, Tengyu},
title = {MOPO: model-based offline policy optimization},
year = {2020},
isbn = {9781713829546},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Offline reinforcement learning (RL) refers to the problem of learning policies entirely from a large batch of previously collected data. This problem setting offers the promise of utilizing such datasets to acquire policies without any costly or dangerous active exploration. However, it is also challenging, due to the distributional shift between the offline training data and those states visited by the learned policy. Despite significant recent progress, the most successful prior methods are model-free and constrain the policy to the support of data, precluding generalization to unseen states. In this paper, we first observe that an existing model-based RL algorithm already produces significant gains in the offline setting compared to model-free approaches. However, standard model-based RL methods, designed for the online setting, do not provide an explicit mechanism to avoid the offline setting's distributional shift issue. Instead, we propose to modify the existing model-based RL methods by applying them with rewards artificially penalized by the uncertainty of the dynamics. We theoretically show that the algorithm maximizes a lower bound of the policy's return under the true MDP. We also characterize the trade-off between the gain and risk of leaving the support of the batch data. Our algorithm, Model-based Offline Policy Optimization (MOPO), outperforms standard model-based RL algorithms and prior state-of-the-art model-free offline RL algorithms on existing offline RL benchmarks and two challenging continuous control tasks that require generalizing from data collected for a different task.},
booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
articleno = {1185},
numpages = {14},
location = {Vancouver, BC, Canada},
series = {NIPS '20}
}

@misc{zhou2023offlinedataenhancedonpolicy,
      title={Offline Data Enhanced On-Policy Policy Gradient with Provable Guarantees}, 
      author={Yifei Zhou and Ayush Sekhari and Yuda Song and Wen Sun},
      year={2023},
      eprint={2311.08384},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2311.08384}, 
}

