\subsection{Proof of \cref{prop:lowerboundminimax}}
 
 During this proof, we assume wlog that $\argmax_i m_i= m_1$. Fix any algorithm $\mathcal{A}$, and denote $\Delta>0$ a constant to be optimized later. We define a first instance $\theta_1$, where the reward of each arm $i\in [K]$ is a gaussian $\mathcal{N}(\mu_i,1)$, with $\mu_1=\Delta$ and $\mu_i=0$ for any $i>1$. Define
\[
j = \argmin_{i\neq 1} \mathbb{E}_{\theta_1,\mathcal{A}}[m_i+T_i(T-1)].
\]
Note that we have:
\[
\mathbb{E}_{\theta_1,\mathcal{A}}[m_j+T_j(T-1)] \leq \min_{J\subseteq [K]}\frac{T-1+\sum_{j \in J} m_j}{|J|}.
\]
Define alternative instance $\theta_2$ where the distribution of the reward of each arm is the same as in $\theta_1$ for all arm $i\neq j$, and the reward of arm $j$ is  $\mathcal{N}(2\Delta,1)$.  We have:
\[
\mathbb{P}_{\theta_1,\mathcal{A}}\left(R_\mathcal{A}(T)\geq \frac{T}{2}\Delta\right)\geq \mathbb{P}_{\theta_1, \mathcal{A}}\left(T_1(T)\leq \frac{T}{2}\right), 
\]
and
\begin{align*}
\mathbb{P}_{\theta_2,\mathcal{A}}\left(R_\mathcal{A}(T)\geq \frac{T}{2}\Delta\right)\geq& \mathbb{P}_{\theta_2, \mathcal{A}}\left(T_j(T)\leq \frac{T}{2}\right)\\
\geq& \mathbb{P}_{\theta_2, \mathcal{A}}\left(T_1(T)\geq \frac{T}{2}\right).
\end{align*}
By Bretagnolles-Huber:
\[
\mathbb{P}_{\theta_1, \mathcal{A}}\left(T_1(T)\leq \frac{T}{2}\right)+\mathbb{P}_{\theta_2, \mathcal{A}}\left(T_1(T)\geq \frac{T}{2}\right)\geq \frac{1}{2}\exp(-D(\mathbb{P}_{\theta_1,\mathcal{A}}, \mathbb{P}_{\theta_2,\mathcal{A}})).
\]

By the data processing inequality, we have:
\[
D(\mathbb{P}_{\theta_1,\mathcal{A}}, \mathbb{P}_{\theta_2,\mathcal{A}}) \leq \mathbb{E}_{\theta_1,\mathcal{A}}[m_j+T_j(T-1)] \frac{(2\Delta^2)}{2} .
\] 
Hence:
\[
D(\mathbb{P}_{\theta_1,\mathcal{A}}, \mathbb{P}_{\theta_2,\mathcal{A}})\leq \min_{J\subseteq [K]}\frac{T-1+\sum_{j \in J} m_j}{|J|}2\Delta^2.
\]
Also:
\begin{align*}
    \mathcal{R}_\mathcal{A}(T)\geq& \max_{\theta_1,\theta_2}\left(\mathbb{E}_{\theta_1,\mathcal{A}}\left[R(T)\right],\mathbb{E}_{\theta_2,\mathcal{A}}\left[R(T)\right]\right)\\
    \geq& \frac{T}{4}\Delta\left(\mathbb{P}_{\theta_1,\mathcal{A}}\left(R_\mathcal{A}(T)\geq \frac{T}{2}\Delta\right)+\mathbb{P}_{\theta_2,\mathcal{A}}\left(R_\mathcal{A}(T)\geq \frac{T}{2}\Delta\right)\right),
\end{align*}
where the second line uses inequality $\max(a,b)\geq \frac{1}{2}(a+b)$.
Putting everything together, this entails:
\[
\mathcal{R}_\mathcal{A}(T)\geq \frac{T}{8}\Delta\exp\left(-\min_{J\subseteq [K]}\frac{T-1+\sum_{j \in J} m_j}{|J|}2\Delta^2\right).
\]

Setting $\Delta= \sqrt{\max_{J\subseteq [K]} \frac{|J|}{2(T-1+\sum_{j \in J} m_j)}}$, we obtain:

\[
\mathcal{R}_\mathcal{A}(T)\geq \frac{T}{8}\sqrt{\max_{J\subseteq [K]} \frac{|J|}{2(T-1+\sum_{j \in J} m_j)}}\exp\left(-1\right).
\]
\hfill \(\Box\)

\subsection{Proof of \cref{prop:regretminimaxucb}}

 
 By \cref{lem:hoeff} and a union bound over the two inequalities, we have for all $t\leq T, i \in [K]$:
\[
\overline{\mu}_i(t)>\mu_i>\underline{\mu}_i(t).
\]

Denote $\mathcal{E}$ that event and assume in the following steps that it holds. If sub-optimal arm $i$ is pulled at iteration $t$, we have $\overline{\mu}_i(t)\geq \overline{\mu}_*(t)\geq \mu_*$, hence:
\begin{align}\label{eq:boundT_i(t)}
 \mu_i+\sqrt{\frac{2\log(K/\delta)}{T_i(t)+m_i}} \geq& \mu_*.
\end{align}
This gives the following bound on the number of pulls of sub-optimal arm $i$:
\begin{align}\label{eq:boundtotalnumpulls}
T_i(T) \leq \left( \frac{2}{\Delta_i^2} \log(K/\delta)-m_i\right)_++1.
\end{align}


Then, we have
\begin{align*}
    R(T)&=\mathbb{E}\left[\sum_i \Delta_i T_i(T)\right]\\
    &\leq \sum_i \Delta_i \mathbb{E}\left[T_i(T)|\mathcal{E}\right]+T\mathbb{P}\left(\bar{\mathcal{E}}\right)\\
    &\leq \underbrace{\sum_i \Delta_i \mathbb{E}\left[T_i(T)|\mathcal{E}\right]}_{(i)}+2T^2\delta\\
    &\leq\sum_i \Delta_i\left( \frac{2}{\Delta_i^2} \log(K/\delta)-m_i\right)_++\sum_i \Delta_i +2T^2\delta.
\end{align*}
The last line corresponds to the parameter-dependent upper bound. To get the minimax upper bound, we focus on bounding $(i)$. Define
\[
\Delta:= \max_{J\subseteq [K]}\sqrt{\frac{2|J|}{T+\sum_{j\in J}m_i}\log(K/\delta)}
\]
and 
\[
A:= \left\{ i \text{ s.t. }  \frac{2}{\Delta^2} \log(K/\delta)-m_i\geq 0\right\}.
\]
By \cref{eq:boundT_i(t)}, under event $\mathcal{E}$ an arm $i$ can only be pulled if $m_i \leq \frac{1}{\Delta_i^2}2\log(K/ \delta)$. If, in addition, that arm $i$ is such that $\Delta_i >\Delta$, then that arm can only be pulled if it belongs to ${A}$. Hence, combining with \cref{eq:boundtotalnumpulls}, we have:
\begin{align}\label{eq:interdeltai}
    (i)\leq& T\Delta+\sum_{\Delta_i>\Delta, i \in {A}}\Delta_i\left( \frac{2}{\Delta_i^2} \log(K/\delta)-m_i\right)_++\Delta_i.
\end{align}
Define now $J^*=\argmax_{J\subseteq [K]}\sqrt{\frac{2|J|}{T+\sum_{j\in J}m_j}\log(K/\delta)}$, breaking ties by selecting the element of maximum cardinality. Let us show that $A=J^*$. We start with $A\subseteq J^*$. Assume it is not the case, and we can find $i \in A \setminus J^*$. We have:

\[
m_i\leq \frac{T+\sum_{j\in J^*}m_j}{|J^*|}.
\]

Thus:
\begin{align*}
    \frac{T+\sum_{j\in J^*}m_j+m_i}{|J^*|+1}\leq &\frac{T+\sum_{j\in J^*}m_j+\frac{T+\sum_{j\in J^*}m_j}{|J^*|}}{|J^*|+1}\\
    =&\frac{T+\sum_{j\in J^*}m_j}{|J^*|},
\end{align*}
and this contradicts the optimality of $J^*$. Therefore $A\subseteq J^*$. On the other hand, assume we can find $i \in J^* \setminus A$. Then we have:
\[
m_i >\frac{T+\sum_{j\in J^*}m_j}{|J^*|}.
\]
Hence:
\begin{align*}
    \frac{T+\sum_{j\in J^*}m_j-m_i}{|J^*|-1}< &\frac{T+\sum_{j\in J^*}m_j-\frac{T+\sum_{j\in J^*}m_j}{|J^*|}}{|J^*|-1}\\
    =&\frac{T+\sum_{j\in J^*}m_j}{|J^*|},
\end{align*}
which again contradicts the optimality of $J^*$. Therefore we can conclude that $A=J^*$.


From this, we can combine with \cref{eq:interdeltai} and obtain:
\begin{align*}
    (i)\leq& \sum_{j \in J^*}\Delta_j\left( \frac{2}{\Delta_j^2} \log(K/\delta)-m_j\right)_++T\Delta+|J^*|\\
    \leq& \sum_{j \in J^*} \frac{2}{\Delta} \log(K/\delta)-\sum_{j \in J^*}m_j \Delta+T\Delta+|J^*|\\
    \leq& \sqrt{2|J^*|(T+\sum_{j\in J^*}m_j)\log(K/\delta)}-\sum_{j \in J^*}m_j\sqrt{\frac{2|J^*|}{T+\sum_{j\in J^*}m_j}\log(K/\delta)}  \\
    &+T\sqrt{\frac{2|J^*|}{T+\sum_{j\in J^*}m_j}\log(K/\delta)}+|J^*|\\
    =&\sqrt{2|J^*|\log(K/\delta)}\left(\sqrt{T+\sum_{j\in J^*}m_j}-\frac{\sum_{j\in J^*}m_j}{\sqrt{T+\sum_{j\in J^*}m_j}}\right)\\
    &+T\sqrt{\frac{2|J^*|}{T+\sum_{j\in J^*}m_j}\log(K/\delta)}+|J^*|\\
    =&2T\sqrt{\frac{2|J^*|}{T+\sum_{j\in J^*}m_j}\log(K/\delta)}+|J^*|.
\end{align*}
Also, if $\min_{i \in [K]}m_i>0$ define 
\[
\Delta':= \sqrt{\frac{2}{\min_{i \in [K]}m_i}\log(K/\delta)}.
\]

By \cref{eq:boundT_i(t)}, an arm $i$ can be sampled at least once only if $\Delta_i\leq \Delta'$. Hence, we also have the bound:
\[
(i)\leq T \Delta'.
\]
Putting the bounds together, we obtain:
\[
\mathcal{R}_{\algucb}(T)\leq \min\left(\sqrt{\frac{2}{\min_{i \in [K]}m_i}\log(K/\delta)};\max_{J\subseteq [K]}2T\sqrt{\frac{2|J|}{T+\sum_{j\in J}m_j}\log(K/\delta)}+|J|\right)+2T^2 \delta.
\]



\hfill \(\Box\)

\subsection{Proof of \cref{prop:minimaxregretlcb}}

Let us start by proving the upper bound. By \cref{lem:hoeff} and a union bound over the two bounds, with probability at least $1-2T\delta$, we have for all $t\leq T, i \in [K]$:
\[
\overline{\mu}_i(t)>\mu_i>\underline{\mu}_i(t).
\]
In this case, for all $t\leq T$, the following holds:
\begin{align*}
 \mu_{L(t)}\geq & \underline{\mu}_{L(t)}(t)\\
 \geq& \underline{\mu}_{*}(t)\\
 \geq& \mu_*-\sqrt{\frac{2\log(K/\delta)}{(m_*+T_*(t))}}\\
 \geq& \mu_*-\sqrt{\frac{2\log(K/\delta)}{\min_i m_i}}.
\end{align*}
Summing over all $T$ iterations and taking the expectation gives:
\[
 \mathcal{R}_\alglcb(T) \leq T\sqrt{\frac{2\log(K/\delta)}{\min_i m_i}}+ 2T^2\delta.
\]

We now turn to the proof of the lower bound. First if $\min_i m_i =0$, the result is obtained by setting the mean of arm(s) $i$ with $m_i=0$ to $1$ and the reward of all other arms to $0$. Then the best arm is never selected and   $R(T)=T$.

Assume now that $\min_i m_i >0$ and  wlog that $\argmin_i m_i=1$. Set the reward of arm $1$ to be $\mathcal{N}\left(\min\left(1;0.5+\sqrt{\frac{1}{m_1}}\right),1\right)$, the reward of arm $2$ to be $0.5$ deterministically and the reward of all the other arms to be $0$ deterministically. Note that if $\hat{\mu}_1(0) < \hat{\mu}_2(0)$, then \alglcb will pull arm $2$ at every iteration. Indeed, this arm has the highest lower bound in the first iteration. Also, the empirical mean of arm $1$ and $2$ remain unchanged unless arm $1$ is pulled. Then, by induction, arm $2$ will have the highest lower bound at every iteration. We can lower bound the probability that this event will happen:
\begin{align*}
    \mathbb{P}(\hat{\mu}_1(0) < \hat{\mu}_2(0)) &= \mathbb{P}(\hat{\mu}_1(0) \leq 0.5) \\
    &= \mathbb{P}(X\leq 0), \text{ where } X\sim \mathcal{N}\left(\min\left(1;0.5+\sqrt{\frac{1}{m_1}}\right),\frac{1}{\sqrt{m_1}}\right)\\
    &\geq 0.15.
\end{align*}

Hence:

\begin{equation*}
\mathcal{R}_{\textsc{LCB}}(T)\geq 0.15 T\min\left(0.5;\sqrt{\frac{1}{\min_i m_i}}\right).
\end{equation*}
\hfill \(\Box\)

\subsection{Proof of \cref{prop:loginucbT=1}}


 Let us start with the upper bound. We have:
\begin{align*}
    R_\algucb^{\text{log}}(1)\leq R_\algucb(1).
\end{align*}

Thus, by \cref{prop:regretminimaxucb}:
\begin{align*}
    R_\algucb^{\text{log}}(1)\leq \sqrt{\frac{2}{\min_i m_i}\log\left(\frac{K}{\delta}\right)}+2\delta.
\end{align*}


For the lower bound, assume wlog that $m_1=\min_i m_i$. If $m_1=0$, then for all arms $i$ s.t. $m_i=0$, set $\mu_i=0$, and for all other arms $i$ with $m_i>0$, set $\mu_i=1$. In that setting, \algucb\ pulls one of the arms $i$ with $m_i=0$ and $R^{\text{log}}(1)=1$. Assume now $m_1>0$.  Set the reward for arm $1$ to be $\mathcal{N}(0,1)$, and set the reward of all the other arms to be $\min \left(1;\sqrt{\frac{1}{m_1}}\right)$ deterministically. We have:

\[
\mu_0=\frac{\sum_i m_i \mu_i}{\sum_i m_i}\geq \min \left(1;\sqrt{\frac{1}{m_1}}\right) -\frac{1}{K} \min \left(1;\sqrt{\frac{1}{m_1}}\right) \geq 0.5\min \left(1;\sqrt{\frac{1}{m_1}}\right).
\]
Hence:
\[
  R_\algucb^{\text{log}}(1)\geq \mathbb{P}\left(I(1)=1\right) 0.5 \min \left(1;\sqrt{\frac{1}{m_1}}\right).
\]

On the other hand:
\begin{align*}
    \mathbb{P}\left(I(1)=1\right)\geq &\mathbb{P}\left(\hat{\mu}_1\geq \sqrt{\frac{1}{m_1}}\right)\\
    \geq & 0.15.
\end{align*}
This implies:
\[
  R^{\text{log}}_\algucb(1)\geq 0.07\min \left(1;\sqrt{\frac{1}{\min_i m_i}}\right).
\]
\hfill \(\Box\)

\subsection{Proof of \cref{prop:lowerboundUCBanyT}}

 Assume wlog that $m_1\geq \ldots \geq m_K$. The reward for all the arms in the constructed lower bound instance is deterministic.  Set $\mu_1=1$ and for all $i \in [2;K]$:
\[
\mu_i = 1-\underbrace{\left(\sqrt{\frac{\log(K/\delta)}{2(m_i+\frac{T}{K})}}-\sqrt{\frac{\log(K/\delta)}{2(m_1+\frac{T}{K})}}\right)}_{=\Delta_i}.
\]
Note that the formula also holds for arm $1$, as it gives $\Delta_1=0$. We will show  by contradiction that for any arm $i \in [K]$, we have $T_i(T)\leq \lceil\frac{T}{K}\rceil$. Assume $\exists i \in [K]$ s.t. $T_i(T)>\lceil\frac{T}{K}\rceil$. This implies the existence of another arm $j\neq i$ s.t. $T_j(T)<\frac{T}{K}$, as well as an iteration $t$ where $T_i(t)= \lceil\frac{T}{K}\rceil$ and
\[
\overline{\mu}_i(t)\geq\overline{\mu}_j(t), 
\]
hence:
\begin{align*}
-\Delta_i+\sqrt{\frac{\log(K/\delta)}{2(m_i+\frac{T}{K})}}\geq& -\Delta_j+\sqrt{\frac{\log(K/\delta)}{2(m_j+T_j(t))}}\\
\geq& -\Delta_j+\sqrt{\frac{\log(K/\delta)}{2(m_j+\frac{T}{K}-1)}}
\end{align*}
Injecting the formulas for the gaps and simplifying, we get:
\begin{align*}
\sqrt{\frac{\log(K/\delta)}{m_1+\frac{T}{K}}}\geq& \sqrt{\frac{\log(K/\delta)}{m_1+\frac{T}{K}}}-\sqrt{\frac{\log(K/\delta)}{m_j+\frac{T}{K}}}+\sqrt{\frac{\log(K/\delta)}{m_j+\frac{T}{K}-1}}.
\end{align*}
Hence:
\begin{align*}
\sqrt{\frac{\log(K/\delta)}{m_j+\frac{T}{K}}}\geq \sqrt{\frac{\log(K/\delta)}{m_j+\frac{T}{K}-1}},
\end{align*}
which is impossible. We thus obtain $T_i(T)\leq \lceil\frac{T}{K}\rceil$ for any $i\in [K]$. As on the other hand, we have $\sum_i T_i(T)=T$, we obtain
\[
T_i(T)\in \left[\bigg\lfloor\frac{T}{K}\bigg \rfloor;\bigg \lceil\frac{T}{K}\bigg \rceil\right] \quad \forall i \in [K].
\]
Thus, when $\frac{T}{K}$ is an integer, on that instance we have:
\begin{align*}
R^{\text{log}}_{\textsc{UCB}}(T)=&T\frac{1}{m}\sum_{i=1}^Km_i \mu_i- \sum_{i=1}^K\frac{T}{K} \mu_i \\
=&\sum_{i=1}^K\frac{T}{K} \Delta_i -T\frac{1}{m}\sum_{i=1}^Km_i \Delta_i\\
=&T\sum_{i=1}^K\left(\frac{1}{K}-\frac{m_i}{m}\right)\left[\sqrt{\frac{1}{2(m_i+\frac{T}{K})}}-\sqrt{\frac{1}{2(m_1+\frac{T}{K})}}\right].
\end{align*}
\hfill \(\Box\)

\subsection{Proof of \cref{prop:regretlogginglcb}}


Let us start with the upper bound. We have by definition of \textsc{LCB}:
\begin{align*}
    \underline{\mu}_{L(t)}(t)\geq \sum_{i=1}^K\frac{m_i \underline{\mu}_{i}(t)}{m}.
\end{align*}
By \cref{lem:hoeff} and a union bound, with probability at least $1-2T\delta$ this entails, 
\begin{align*}
    \mu_{L(t)}\geq &\frac{1}{\sum_i m_i}\sum_i m_i\left(\mu_i-\sqrt{2\frac{\log(\frac{K}{\delta})}{m_i+T_i(t)}}\right)\\
   \geq & \frac{1}{\sum_i m_i}\sum_i m_i\left(\mu_i-\sqrt{2\frac{\log(\frac{K}{\delta})}{m_i}}\right)\\
\end{align*}
Hence:
\begin{align}\label{eq:boundlcbaux}
    \mu_0-\mu_{L(t)}\leq \frac{\sum_i \sqrt{m_i}}{\sum_i m_i}\sqrt{2\log(\frac{K}{\delta})}.
\end{align}
Summing over all possible iterations and taking the expectation, this implies:
\begin{align*}
    \mathcal{R}_\alglcb^{\text{log}}(T)\leq &T\frac{\sum_i \sqrt{m_i}}{\sum_i m_i}\sqrt{2\log(\frac{K}{\delta})}+2T^2\delta.
\end{align*}

We now turn to the lower bound. First, assume wlog that $m_1\geq m_2\geq \ldots\_eq m_K$. We will distinguish two cases depending on whether $m_2=0$ or not. 

If $m_2=0$, then there is only samples for arm $1$ at the first iteration, hence, $\alglcb(t)=1$. By induction, this remains true at every iteration, and $\mathcal{R}^{\text{log}}(T)=0$.



Assume now that $m_2>0$. Set the reward for arm $2$ to be $\mathcal{N}(0.5+\sqrt{\frac{1}{m_2}},1)$, and set the reward of all the other arms to be $0.5$ deterministically. By definition, for any $t\leq T$:
\begin{align*}
\underline{\mu_1}(t)\geq &\underline{\mu_1}(0)\\ =&
0.5-\sqrt{\frac{\log(K/\delta)}{2m_1}}.
\end{align*}
This implies that if 
  $ \underline{\mu_2}(t)< \underline{\mu_1}(0)$, then   $ \underline{\mu_2}(t)< \underline{\mu_1}(t)$. Moreover, if $   \underline{\mu_2}(t)< \underline{\mu_1}(t)$ holds for some $t\leq T$, then arm $2$ is not chosen, which implies   $ \underline{\mu_2}(t+1)< \underline{\mu_1}(t+1)$. By induction, this gives:
\begin{align*}
\mathbb{P}\left(L(t) \neq 2 \text{ for all } t\leq T\right) \geq&\mathbb{P}\left(   \underline{\mu_2}(t)< \underline{\mu_1}(t) \text{ for all } t\leq T\right) \\
    \geq &\mathbb{P}\left(   \underline{\mu_2}(t)< \underline{\mu_1}(t) \right) \\
    =&\mathbb{P}\left(\hat{\mu}_2(0)< 0.5\right)\\
    \geq& 0.15. 
\end{align*}

On the other hand
\[
\frac{\sum_i m_i \mu_i}{\sum_i m_i}= 0.5 +\frac{\sqrt{m_2}}{m}.
\]
This implies:
\begin{align*}
  R_\alglcb^{\text{log}}(T)\geq& 0.15 T\frac{\sqrt{m_2}}{m}.
\end{align*}

\hfill \(\Box\)

