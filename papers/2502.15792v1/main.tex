\documentclass{article}


\usepackage{PRIMEarxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}
\usepackage{fancyhdr}       % header
\usepackage{graphicx}       % graphics
\graphicspath{{media/}}     % organize your images and other figures under media/ folder

\usepackage{algorithmic}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{xspace}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float} 
\usepackage{tikz}
\usepackage{paralist}
\usepackage{subcaption}
\usepackage{amsmath}
\usepackage{bm}

\newcommand{\method}{\textit{MOEQT}\xspace}
\newcommand{\Atwelve}{\^{A}\textsubscript{12}\xspace}
\newcommand{\rs}{\textit{RS}\xspace}
\newcommand{\sorlw}{\textit{SORLW}\xspace}

\newcommand{\ttc}{$\textit{TTC}_{V_{R1\_R2}}$\xspace}
\newcommand{\rc}{$\textit{RC}_{V_{R1\_R2}}$\xspace}
\newcommand{\ttcvrone}{$\textit{TTC}_{V_{R1}}$\xspace}
\newcommand{\rcvrtwo}{$\textit{RC}_{V_{R2}}$\xspace}

\newcommand{\vrone}{${V_{R1}}$\xspace}
\newcommand{\vrtwo}{${V_{R2}}$\xspace}
\newcommand{\vronetwo}{${V_{R1\_R2}}$\xspace}

\newcommand{\blue}[1]{\textcolor{black}{#1}}

\newcommand{\conclusion}[1]{
\begin{center}
    \fcolorbox{black}{gray!10}{\parbox{.97\columnwidth}{#1}}
\end{center}
}

%Header
\pagestyle{fancy}
\thispagestyle{empty}
\rhead{ \textit{ }} 

% Update your Headers here
\fancyhead[LO]{Multi-Objective Reinforcement Learning for Critical Scenario Generation of Autonomous Vehicles}
% \fancyhead[RE]{Firstauthor and Secondauthor} % Firstauthor et al. if more than 2 - must use \documentclass[twoside]{article}



  
%% Title
\title{Multi-Objective Reinforcement Learning for Critical Scenario Generation of Autonomous Vehicles}

\author{
  Jiahui Wu \\
  Simula Research Laboratory and \\ University of Oslo \\
  Oslo, Norway \\
  \texttt{jiahui@simula.no} \\
   \And
  Chengjie Lu \\
  Simula Research Laboratory and \\ University of Oslo \\
  Oslo, Norway \\
  \texttt{chengjielu@simula.no} \\
   \And
  Aitor Arrieta \\
  Mondragon University \\
  Mondragon, Spain \\
  \texttt{aarrieta@mondragon.edu} \\
   \And
  Shaukat Ali \\
  Simula Research Laboratory \\
  Oslo, Norway \\
  \texttt{shaukat@simula.no} \\
}


\begin{document}
\maketitle


\begin{abstract}
Autonomous vehicles (AVs) make driving decisions without human intervention. Therefore, ensuring AVs' dependability is critical. Despite significant research and development in AV development, their dependability assurance remains a significant challenge due to the complexity and unpredictability of their operating environments. Scenario-based testing evaluates AVs under various driving scenarios, but the unlimited number of potential scenarios highlights the importance of identifying critical scenarios that can violate safety or functional requirements. Such requirements are inherently interdependent and need to be tested simultaneously. To this end, we propose \method, a novel multi-objective reinforcement learning (MORL)-based approach to generate critical scenarios that simultaneously test interdependent safety and functional requirements. \method adapts Envelope Q-learning as the MORL algorithm, which dynamically adapts multi-objective weights to balance the relative importance between multiple objectives. \method generates critical scenarios to violate multiple requirements through dynamically interacting with the AV environment, ensuring comprehensive AV testing. We evaluate \method using an advanced end-to-end AV controller and a high-fidelity simulator and compare \method with two baselines: a random strategy and a single-objective RL with a weighted reward function. Our evaluation results show that \method achieved an overall better performance in identifying critical scenarios for violating multiple requirements than the baselines.
\end{abstract}


% keywords can be removed
\keywords{Autonomous vehicle testing \and Multi-objective reinforcement learning \and Driving scenario \and Scenario-based testing}



\section{Introduction}\label{sec:introduction}
\noindent
Autonomous vehicles (AVs) automatically navigate and make driving decisions without human intervention, aiming to achieve safe, efficient, and fully automated transportation.
At the same time, it remains challenging to ensure AVs' dependability before their deployment in the real world due to the complexity of AVs and their driving environments. Scenario-based AV testing~\cite{zhang2022finding,ding2023survey,stocco2022mind,stocco2023model,moghadam2024machine,neelofar2024identifying,crespo2024pafot} offers a practical way to evaluate AVs under various driving scenarios, which is a critical step toward the successful deployment of AVs. However, given the complexity of AVs and the uncertainty of their driving environments, the number of possible driving scenarios for testing AVs is theoretically infinite. Therefore, it is paramount to find critical scenarios that lead the AV to misbehave by violating safety or functional requirements.

Scenario-based testing methods generate critical scenarios within driving environments simulated by high-fidelity simulators such as CARLA~\cite{dosovitskiy2017carla}. A test scenario is defined by a set of environmental parameters that characterize static and dynamic objects (e.g., vehicles, pedestrians, and other obstacles) in the AV's environment. These scenarios are specifically designed to reveal potential safety or functional requirements violations. 
Existing methods generate critical scenarios by exploring complex search spaces covering various driving scenarios. To this end, 
search-based techniques~\cite{zhong2022neural,10645815,10234383,zhou2023specification} have been predominant for AV testing, designed to handle large search spaces and identify critical scenarios guided by fitness functions. However, such approaches show limited effectiveness in dealing with runtime sequential interactions, which are essential when manipulating dynamic objects and ensuring precise control over them. Besides, the environment in which an AV operates is constantly changing and uncertain, while search-based approaches typically do not interact with or learn from the environment during the search process. Instead, they consider the environment as independent when searching for scenarios and then execute scenarios in the environment, and therefore cannot effectively adapt to environmental changes.

Recently, reinforcement learning (RL)~\cite{sutton2018reinforcement} has been proven to be a promising technique for scenario-based AV testing. Inheriting the features of RL agents in adaptive control and dynamic interaction, RL-based testing approaches~\cite{lu2022learning,10172658,feng2023dense} generate critical scenarios by interacting with the environment at runtime and can be adaptive to dynamic environmental changes. However, RL-based methods typically focus on detecting safety violations, such as collisions, whereas the functional requirements are less tested.
Thus, it is crucial to adaptively generate critical scenarios to simultaneously test safety and functional requirements.
A relevant work is MORLOT~\cite{10172658}, which adapts single-objective RL and many-objective search to generate test suites to violate as many requirements as possible. MORLOT is designed to violate multiple \textit{independent} requirements by generating a set of scenarios, each focusing on violating one independent requirement. However, in practice, many requirements are interdependent and must be evaluated simultaneously to ensure comprehensive testing. For instance, safety requirements such as avoiding collisions could be closely related to functional requirements like maintaining lane positioning or completing routes within a time budget. Violating functional requirements (e.g., failing to complete routes) may compromise safety requirements (e.g., causing a collision), highlighting the need for approaches simultaneously evaluating multiple requirements. To this end, in this work, we aim to test multiple interdependent requirements for which multi-objective RL (MORL)~\cite{6918520} is suitable. 

MORL learns control policies to optimize multiple criteria simultaneously. Compared to single-objective RL, MORL handles trade-offs among multiple objectives, e.g., safety and performance, to solve time-series decision-making problems in complex environments~\cite{6918520,hayes2022practical}. MORL has shown effectiveness in various domains, such as robotics~\cite{xu2020prediction} and autonomous driving~\cite{kiran2021deep}. Motivated by the challenges of handling interdependent requirements in RL-based AV testing and the success of MORL, we propose \method, a novel MORL-based AV testing approach. \method generates critical scenarios to test multiple requirements by interactively and adaptively configuring the AV operating environment. Specifically, \method employs Envelope Q-learning (EQL)~\cite{yang2019generalized}, a generalized MORL algorithm that learns a single policy network to optimize multiple objectives by dynamically adjusting the agent's preferences (i.e., the relative importance assigned to each objective) during the training phase. \method learns by observing the environment status and then taking actions to manipulate objects in the environment at each time step. \method formulates the violation of each requirement as an objective function. The values from these objective functions are then used by the reward function to guide the MORL agent towards generating scenarios that can violate multiple requirements. 
We evaluate \method using an end-to-end AV controller (i.e., Interfuser~\cite{shao2023safety}) and a high-fidelity simulator (i.e., CARLA~\cite{dosovitskiy2017carla}). We compare \method with two baselines: a random strategy and a single-objective RL with a weighted reward function, and select six roads covering various driving conditions for the experiment. The evaluation results show that \method is more effective in generating critical scenarios that violate multiple requirements simultaneously. 

To summarize, the contributions of this paper are: 1) To test multiple interdependent AV requirements, we propose a novel MORL-based approach, \method, that generates critical scenarios to violate multiple requirements by adaptively configuring the AV's operating environment; 2) We formulate the AV testing problem using a multi-objective Markov decision process (MOMDP) and solve it using the EQL algorithm; 3) We conduct an empirical evaluation and compare \method with two baselines. The results show that \method outperforms baselines in terms of violating multiple requirements simultaneously and achieves a more balanced performance in optimizing multiple objectives.



\section{Background}\label{sec:background}

\subsection{Reinforcement Learning}\label{subsec:rl}
\noindent
Reinforcement learning (RL) learns an optimal policy for an agent to perform tasks through interacting with the environment to maximize cumulative rewards~\cite{sutton2018reinforcement}. Unlike supervised learning, which uses labeled data to train models, RL requires no prior knowledge and learns through trial and error. Specifically, in RL, an agent observes the current state of the environment, selects actions based on the policy, and receives a reward signal as feedback for its decision. A typical RL process can be formulated as a Markov decision process (MDP), which is a 4-tuple $<\mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}>$ with state space $\mathcal{S}$ specifying the possible situations of the environment, action space $\mathcal{A}$ defining possible actions the agent can take, transition distribution $\mathcal{P}(s_{t+1}|s_t,a_t)$ specifying how the environment state changes in response to the agent's action, and reward function $\mathcal{R}(s_t,a_t)$ measuring the agent's performance of taking an action. In MDP, the agent interacts with the environment at each discrete time step. Specifically, at each time step $t$, the agent observes the current state of the environment as $s_t \in \mathcal{S}$ and then selects an action $a_t \in \mathcal{A}$ to perform based on $s_t$ and the behavioral policy. After the action is finished, the agent receives a reward $r_t \sim \mathcal{R}(s_t,a_t)$ as feedback to evaluate the agent's performance, and the environment moves into a new state $s_{t+1} = \mathcal{P}(s_{t+1}|s_t,a_t)$. RL aims to find an optimal policy $\pi^*(a_t|s_t)$, a mapping from state to probabilities of selecting actions, to maximize the expected cumulative reward over time. Q-learning~\cite{watkins1992q} is a classical RL algorithm that learns via an action-value function called Q-value function:
\begin{equation}
    Q^\pi(s_t, a_t) = \mathbb{E}_{\pi}[R_t|s_t, a_t].
\end{equation}
The Q-value of the state-action pair $(s_t, a_t)$ estimates the expected future reward by taking action $a_t$ in state $s_t$ with policy $\pi$. Q-value is updated based on the Bellman function:
\begin{equation}
    \label{equ:bellman_equation}
    Q^\pi(s_t, a_t) = \mathbb{E}_{\pi}[r_t + \gamma \max_{a_{t+1}}Q^\pi(s_{t+1}, a_{t+1})],
\end{equation}
where $\gamma$ is the discount factor determining how much future rewards contribute to immediate rewards.

Traditional RL algorithms, such as Q-learning, are designed to address tasks with a single long-term objective. However, many real-world tasks involve multiple criteria that must be optimized simultaneously. For example, when designing AVs, ensuring safety and passenger comfort is paramount. To this end, MORL, which learns optimal policies to handle two or more objectives, has become an important research area.


\subsection{Multi-Objective Reinforcement Learning}\label{subsec:morl}
\noindent
MORL aims to solve sequential decision-making problems where multiple objectives must be considered~\cite{roijers2013survey}. A MORL process can be formulated as an MOMDP, an extension of MDP. Unlike MDP calculating a single scalar reward $r$, the reward function in MOMDP returns a vector $\mathbf{r}$ containing the rewards for each objective. The action-value function in MORL is a vectored Q-function:
\begin{equation}
    \mathbf{Q}^\pi(s_t, a_t) = [Q_1^\pi(s_t, a_t),Q_2^\pi(s_t, a_t),...,Q_{NO}^\pi(s_t, a_t)]^T,
\end{equation}
where $NO$ is the number of objectives and for the $i_{th}$ objective, its corresponding action-value function is $Q_i^\pi(s_t, a_t)$, which is updated using the Bellman function~\ref{equ:bellman_equation}. MORL can be classified into two types based on how they find the optimal policy. The first class is single-policy MORL, aiming to find one optimal policy that represents the weights among the multiple objectives. The second class is multi-policy or Pareto-based MORL, which finds a set of policies that approximate the Pareto front. Each policy balances different objectives and no single policy can dominate another across all objectives.

In this paper, we employ a single-policy MORL called EQL~\cite{yang2019generalized} given its success in multi-objective RL tasks such as Games and Robot control~\cite{felten2024toolkit}. In EQL, an MOMDP is represented using a 6-tuple $<\mathcal{S}, \mathcal{A}, \mathcal{P}, \mathbf{R}, \Omega, f_{\Omega}>$, where $\mathcal{S}$, $\mathcal{A}$, and $\mathcal{P}$ are the same as in traditional MDP. Notably, $\mathbf{R}(s_t, a_t)$ represents a vector reward function, $\Omega$ is the space of weights of multi-objectives, and $f_{\Omega}$ is linear weight functions, each of which produces a scalar utility using the weight $\boldsymbol{\omega} \in \Omega$. The goal of EQL is to learn a single policy network that can be generalized across the entire weight space. One EQL implementation is based on Deep Q-learning (DQN)~\cite{mnih2015human}, a classical single-objective RL that uses a deep neural network called Q-network to approximate and store Q-values. EQL extends Q-network into a multiple-objective Q-network (MQ-network). Unlike Q-network, which takes only the state as input, MQ-network takes the concatenation of the state and the weight as input. Additionally, EQL employs homotopy optimization~\cite{watson1989modern} to adaptively adjust the loss function computation using weight vectors. These designs allow EQL to dynamically learn weights to optimize objectives that may vary in importance. By leveraging adaptive weights during training, EQL effectively balances the relative importance between different objectives, enhancing its ability to generalize across diverse problems.

\section{Approach}\label{sec:approach}
\subsection{\method Overview}\label{sec:overview}
\noindent Figure~\ref{fig:overview} shows an overview of how \method generates critical scenarios to test multiple requirements simultaneously. Given a set of requirements, \method employs EQL as the MORL solution to learn operating environment configurations to violate these requirements.
Such environment configuration characterizes a critical scenario, i.e., a test scenario in which the AV simultaneously violates multiple requirements. 

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.9\linewidth]{images/overview.pdf}
    \caption{Overview of AV Testing with \method}
    \label{fig:overview}
\end{figure}

\method builds a testing environment consisting of the AV and its operating environment. As Figure~\ref{fig:overview} shows, to configure the operating environment of the AV, at each time step $t$, the EQL agent observes the current state $s_{t}$ from the testing environment, which contains the state of both the AV and its operating environment. The agent then samples a vector of multi-objective weights $\boldsymbol{\omega}_t$ based on the number of requirements targeted for violation. Taking the concatenation of $s_t$ and $\boldsymbol{\omega}_t$ as input, the MQ-network computes the corresponding Q-values. Based on these Q-values and the weight vector $\boldsymbol{\omega}_t$, the behavior policy decides an action $a_{t}$ to configure the AV's operating environment.
The AV navigates in the newly configured environment for a specified period, during which the objective function calculator computes values in real-time based on the objective functions designed for each requirement violation, considering the state of the AV and the environment. These values are then fed into the vector reward function calculator for vector reward computation. 
After the AV reaches time step $t+1$, i.e., $a_t$ is finished, the EQL agent observes the next state $s_{t+1}$. The vector reward function calculator then calculates the corresponding vector reward $\mathbf{r}_{t}$ for the triplet $(s_{t}, a_{t}, \boldsymbol{\omega}_t)$ based on the objective function values during time step $t$. The agent then constructs a transition tuple $<s_{t}, a_{t}, \mathbf{r}_{t}, s_{t+1}>$ and stores it in a replay buffer using a specific replay mechanism. Notice that $\boldsymbol{\omega}_t$ is decoupled from the transition, which allows for sample-efficient learning by using the prioritized experience replay scheme~\cite{schaul2015prioritized}. Once the replay memory is full, a minibatch sampled from the replay buffer, along with randomly sampled weights, are used to update the MQ-network. 
To ensure stability when training the MQ-network, a separate target network with identical architecture as the MQ-network is used to generate target Q-values for loss calculation. Its parameters are clones from the MQ-network every fixed number of steps.
Finally, \method terminates an episode when the AV either completes its route, collides with other objects, or reaches the time limit (timeout).


\subsection{Problem Definition}\label{sec:problem}
\noindent AVs have to meet a variety of safety and functional requirements that are not only for AVs but also for the environments in which they operate. Consequently, complex interdependencies exist among the requirements, making it ineffective to consider each requirement in isolation, e.g., for testing. Thus, to ensure the AV's safety and functionality, it is essential to validate safety and functional requirements simultaneously. 
To achieve this, scenario-based testing is employed to generate critical scenarios to violate various requirements, providing AV developers with a comprehensive testing solution.

Particularly, assume that given an AV, $Req = \{req_{1}, req_{2}, \dots, req_{n}\}$ is a set of its safety and functional requirements, and each $req_{i} \in R$ represents a specific requirement that the AV must satisfy. For example, the AV should not collide with any object. 
Our goal in AV testing is to generate a critical scenario $sc$ using a function $g$ to simultaneously violate the requirements in $Req$.
Formally, this can be expressed as $sc = g(AV, Req)$, where $AV$ is the AV under test and $Req$ is the set of requirements that need to be met.
To construct $sc$, along with the AV under test, the environment must be modeled. Let $E = \{E_{static}, E_{dynamic}\}$ be the environment, where $E_{static}$ represents a set of static objects within the scenario that remains static over time, e.g., trees and traffic signs, and $E_{dynamic}$ denotes dynamic objects in the scenario that may change over time, such as non-player character (NPC) vehicles and pedestrians. 
Static objects, defined as $O^{'}$, in $E_{static}$ are the objects that remain unchanged over time. Let $N$ be the number of static objects in the environment, where $E_{static}$ can be expressed as $E_{static} = \{O_{1}^{'}, O_{2}^{'}, \dots, O_{N}^{'}\}$. 
Their configurations, denoted as $C^{O^{'}}$, can be represented using their states such as positions and rotations. 
In contrast, dynamic objects, defined as $O$, in $E_{dynamic}$ change over time, so their configurations $C^{O}$ must account for both their states and behaviors over time. 
Let $M$ be the number of dynamic objects in the environment, where $E_{dynamic}$ can be represented as $E_{dynamic} = \{O_{1}, O_{2}, \dots, O_{M}\}$. 

To represent the dynamic changes in the environment over time, the states and behaviors of dynamic objects in $E_{dynamic}$ can be tracked across a sequence of time steps. Let $T$ be the total number of time steps and $t$ be the current time step, where $t \in \{1, 2, \dots, T\}$. Then $E_{dynamic}$ can be expressed as a time-sequenced set, described as $E_{dynamic} = \{E_{dynamic}(t) | t \in \{1, 2, \dots, T\}\}$. Specifically, $E_{dynamic}(t)$ represents a set of dynamic objects at the time step $t$, which is defined as $E_{dynamic}(t) = \{O_{1}, O_{2}, \dots, O_{M_{t}}\}$, where $M_{t}$ denotes the number of dynamic objects in the environment at the current time step $t$. $O_{M_{t}}$ contains the set of configurations for the dynamic object that changes over instants within the time step $t$. 
Assuming each time step contains $m$ instants, we define $t_{j}$ as the $j_{th}$ instant within the time step $t$, and $O_{M_{t}}$ can be described as $O_{M_{t}} = \{C_{t_{j}}^{O_{M_{t}}} | j \in \{1, 2, \dots, m\}\}$, where $C_{t_{j}}^{O_{M_{t}}}$ is the configuration of the dynamic object $O_{M_{t}}$ at instant $j$ within the time step $t$. 
Note that the dynamic environment of each time step ($E_{dynamic}(t)$) is influenced by the previous time step ($E_{dynamic}(t-1)$), with the configurations of all dynamic objects at the final moment of the prior time step (i.e., at the instant $m$ within the time step $t-1$) continuously affecting the dynamic environment of the next time step.


\subsection{Requirements and Objective Functions}\label{sec:objective_function}
\noindent To quantify the degree or probability of violation for each requirement $req_{i}$ in the set $Req = \{req_{1}, req_{2}, \dots, req_{n}\}$ associated with an AV, we define a set of objective functions $H = \{h_{1}, h_{2}, \dots, h_{n}\}$. Each function $h_{i}$ measures the degree or probability of violation associated with the corresponding $req_{i} \in Req$ and can be expressed as $h_{i}(req_{i}) = \textit{Measure of violation for requirement } req_{i}, i \in \{1, 2, \dots, n\}$. 
For instance, assuming one of the requirements for the AV is ``the AV should complete the route within a time budget", its corresponding objective function measures the percentage of the route completion. A smaller value of this objective function (i.e., a smaller route completion percentage) indicates a higher degree of violation. 
Thus, given a set of requirements $Req$, a critical scenario $sc = g(AV, Req)$ is defined as the scenario that satisfies: $\forall req_i\in Req: sc \textit{ violates } req_i$.


\subsection{Formulating AV Testing as an MOMDP}\label{sec:MOMDP}
\noindent We model the MORL-based AV testing problem as an MOMDP as mentioned in Section~\ref{subsec:morl}, which is defined as a 6-tuple $<\mathcal{S}, \mathcal{A}, \mathcal{P}, \mathbf{R}, \Omega, f_{\Omega}>$. Specifically, $\mathcal{P}$ indicates the transition distribution, $\Omega$ describes the space of weights, and $f_{\Omega}$ expresses the weight functions. For AV testing, we adopt the default definitions for these components, while reformulating the state space $\mathcal{S}$, action space $\mathcal{A}$, and vector reward function $\mathbf{R}$ as follows:

\subsubsection{State Space}
\noindent In MORL, the state space represents all the possible configurations of the environment that can be perceived and understood by the agent. Effective state encoding captures the critical information from the environment, enabling the agent to make appropriate decisions. For AV testing, a commonly used state encoding focuses on the AV and its operating environment~\cite{leurent2018survey}. Thus, the state space can be formulated based on the configuration of the AV and its environment and can be represented as $\mathcal{S} = \{C^{AV}, \{C^{O^{'}_{i}}\}^{N}_{i=1}, \{C^{O_{j}}\}^{M}_{j=1}\}$, where $N$ is the number of static objects, and $M$ is the number of dynamic objects. Notably, $C^{AV}$ means the configuration of the AV, $C^{O^{'}_{i}}$ is the configuration of the $i_{th}$ static object, and $C^{O_{j}}$ is the configuration of the $j_{th}$ dynamic object. 
Based on the formulated state space, the MORL agent can observe the specific state at each time step $t$, which is denoted as $\{C^{AV}_{t}, \{C^{O^{'}_{i}}_{t}\}^{N}_{i=1}, \{C^{O_{j}}_{t}\}^{M}_{j=1}\}$.

\subsubsection{Action Space}
\noindent $\mathcal{A}$ encompasses all possible actions available to the MORL agent, defining the range of decisions it can make in a given state and directly determining the impact on the environment. At each time step $t$, the agent observes the current state $\{C^{AV}_{t}, \{C^{O^{'}_{i}}_{t}\}^{N}_{i=1}, \{C^{O_{j}}_{t}\}^{M}_{j=1}\}$ and select an appropriate action from the action space to transition the system to the next state $\{C^{AV}_{t+1}, \{C^{O^{'}_{i}}_{t+1}\}^{N}_{i=1}, \{C^{O_{j}}_{t+1}\}^{M}_{j=1}\}$. This action involves manipulating objects in the AV testing environment, such as introducing dynamic objects or adjusting their configurations, thereby altering the environment to interact with the AV. These changes can create conditions that increase the likelihood of the AV making driving decisions that violate specified requirements.


\subsubsection{Vector Reward Function}
\noindent In MORL, $\mathbf{R}$ typically represents a vector reward function, with each reward function computing the reward value for a specific objective separately. After executing an action in the current state, the MORL agent receives a vector $\mathbf{r}$ containing rewards corresponding to multiple objectives, which guides the agent in updating the weights for multiple objectives and balancing their relative importance. 
For AV testing, $\mathbf{R}$ is formulated based on the objective functions defined in Section~\ref{sec:objective_function}. The objective functions $H = \{h_{1}, h_{2}, \dots, h_{n}\}$ assess the degree or probability of violations of requirements for each instant within every time step.
On the other hand, the vector reward function focuses on each time step and is derived by identifying the maximum value of each objective function within that time step, ultimately forming the vector reward function required for MORL. 
Let $t$ be the current time step and $\mathbf{R}(t)$ be the vector reward function at time step $t$. Then, $\mathbf{R}(t)$ can be expressed as $\mathbf{R}(t) = [max(h_{i\_t_{j}}|j \in \{1, 2, \dots, m\}) \textit{ for } i \in \{1, 2 , \dots, n\}]$, where $h_{i\_t_{j}}$ represents the $i_{th}$ objective function at instant $j$ within time step $t$. 
Specifically, if each time step contains only one instant, the vector reward function will be equivalent to the objective functions.


\section{Experiment Design}\label{sec:design}

\subsection{Research Questions}\label{subsec:RQ}
\noindent In our evaluation, we will answer the following two Research Questions (RQs):

\begin{itemize}
    \item \textit{\textbf{RQ1:} How does \method perform compared to a random approach?} We aim to assess whether the targeted problem is complex and cannot be solved by a simple, random approach. 

    \item \textit{\textbf{RQ2:} How does \method perform compared to single-objective RL with a weighted reward function?} Single-objective RLs are simpler algorithms than the one we used. The goal of this RQ is to see whether there is justification for selecting a multi-objective RL instead of relying on a single-objective RL algorithm that casts the problem as a single-objective approach (i.e., by using a weighted reward function). 
\end{itemize}


\subsection{Subject System and Simulator}
\noindent We employ Interfuser~\cite{shao2023safety} as the subject system, which is an advanced end-to-end AV controller on the CARLA leaderboard. Interfuser is a safety-enhanced framework based on the Transformer architecture~\cite{vaswani2017attention} for comprehensive scene understanding and safe autonomous driving. It employs a multimodal sensor fusion transformer encoder to integrate sensor data from multiple RGB cameras and a LiDAR sensor. Then, it uses a transformer decoder to generate driving actions and interpretable intermediate features. Finally, a safety controller processes the interpretable intermediate features to refine and enhance the safety of the driving actions. Interfuser has been evaluated in extensive driving scenarios, from urban environments to highways, demonstrating its outstanding safe driving performance. Regarding the simulator, we adopt CARLA~\cite{dosovitskiy2017carla}, a high-fidelity open-source autonomous driving simulator. CARLA provides an extensive list of digital assets, such as vehicles, sensors and high-definition maps to support AV development, training, and validation.
In the evaluation of \method, we use CARLA-0.9.10.1 and its default Interfuser settings. We choose the default vehicle used by Interfuser, which is a Tesla Model 3.

\subsection{Baselines}
\noindent
We employ two comparison baselines to evaluate \method: a random strategy (\rs) and a single-objective RL with a weighted reward function (\sorlw). Specifically, \rs randomly selects an action from the action space to configure the environmental objects at each time step. \sorlw learns the optimal policy following a classical MDP process. At each time step, it first observes a state from the environment and then selects an action to perform based on the state and the behavioral policy. After the execution of the action, a reward is returned and the environment enters a new state. \sorlw follows the same design of state space and action space as \method. However, unlike \method, which adapts multiple reward functions, \sorlw weights all reward functions into one reward function using equal weight. The implementation of \sorlw is based on Deep Q-learning~\cite{mnih2015human}, a classical RL algorithm that has demonstrated good performance in solving complicated sequential decision-making problems, such as robotics kinematics~\cite{phaniteja2017deep}, AV control~\cite{chen2020conditional}, and AV testing~\cite{lu2022learning}.


\subsection{Experiment Setup}

\subsubsection{Instantiation of Requirements and Objective Functions}\label{sebsebsec: requirements_objective_functions}
\noindent While our approach is generic to any AV requirement, in our evaluation, we consider two interdependent requirements, one of which is safety-critical, whereas the second is a functional requirement. We then develop corresponding objective functions for AV testing based on these requirements. The following are the safety and functional requirements we cover:

\begin{itemize}
\item \textit{R1: The AV should not collide with any object.}
\item \textit{R2: The AV should complete its route within a time budget.}
\end{itemize}

\textit{R1} emphasizes that the AV must avoid collisions with any object in the environment, including static and dynamic obstacles. This requirement ensures AV safety and prevents traffic accidents~\cite{tuncali2019requirements}. To quantify the violation of this requirement, we use \textit{Time to Collision (TTC)}~\cite{minderhoud2001extended} as an objective function.
\textit{TTC} represents the remaining time before the AV collides with another object, assuming both maintain their current collision route and velocity difference, which is defined as:
\begin{equation}
TTC = \frac{d_{rel}}{v_{rel}} \label{eq:ttc},
\end{equation}
where $d_{rel}$ means the relative distance between the AV and another object in the environment, and $v_{rel}$ calculates the relative velocity of the AV to another object. 
The value of \textit{TTC} ranges from 0 to positive infinity. 
A lower \textit{TTC} indicates a higher possibility of violating \textit{R1}. Specifically, when \textit{TTC} is 0, it means the AV has collided with another object, completely violating \textit{R1}. 
We can perform real-time calculations on \textit{TTC}, offering insights into the likelihood of potential collisions and enhancing the testing of AV driving safety. 
Note that, for simplicity, we provide a general formulation for \textit{TTC}; however, in our implementation, we compute the actual \textit{TTC} based on the specific position and velocity vectors of the AV and objects within the scenario.

The \textit{R2} requirement demonstrates our focus on the functional aspects of the AV. It requires the AV to complete the planned route within a specified time budget, which involves real-time environmental awareness and path planning to dynamically adapt to complex scenario changes, verifying the AV's driving reliability~\cite{schwarting2018planning}. To assess this requirement, we employ \textit{Route Completion (RC)}~\cite{carlaleaderboard} as an objective function, measuring the percentage of route successfully completed by the AV. \textit{RC} is defined as follows:
\begin{equation}
RC = \frac{d_{driven}}{D_{total}} \times 100\%  \label{eq:rc},
\end{equation}
where $d_{driven}$ measures the actual distance traveled by the AV along a given route, and $D_{total}$ denotes the total length of the entire test route. The value of RC ranges from 0\% to 100\%, with 100\% indicating that the AV has successfully completed the specified route. 
By measuring RC, we can assess the AV's execution efficiency and the stability and reliability of its global or local path planning in dynamic driving conditions. 
Thus, for AV testing, we expect a lower RC, which indicates a higher likelihood of the violation of \textit{R2}. 
Note that if RC stays at 0\% continuously, it indicates that the AV is either unresponsive or remains stopped, which should be avoided in the implementation as it may result in an ineffective test.


\subsubsection{Configurable Environment Parameters}\label{subsubsec: parameters}
\noindent When applying scenario-based testing to AVs, some degree of abstraction is needed, as considering all environmental factors that occur in reality would require enormous computing power, making our approach non-scalable. 
We therefore prioritize, for our study, the configuration and analysis of dynamic objects, i.e., NPC vehicles and pedestrians, although it could easily be extended to factors that could be of interest to AV developers.

The configurable parameters of an NPC vehicle can be represented by a triplet: $(dis_{v}^{la}, dis_{v}^{lo}, bhv_{v})$, where $dis_{v}^{la}$ and $dis_{v}^{lo}$ means the lateral and longitudinal distances between the NPC vehicle and the AV, respectively. Meanwhile, $bhv_{v}$ indicates the driving behavior of the NPC vehicle. 
In our implementation, to simplify the problem and enhance the experimental efficiency, based on the results of our pilot study and supported by findings in related research~\cite{lu2024epitester}, we sample the value of $dis_{v}^{la}$ from the discrete list $[-20, 0, 20]$ and $dis_{v}^{lo}$ from $[-3.5, 0, 3.5]$. The behavior $bhv_{v}$ can be one of three options: changing lanes to the left, changing lanes to the right, or maintaining the current lane. 
For example, if $(dis_{v}^{la}, dis_{v}^{lo}, bhv_{v})$ is set to $(-20, 3.5, \textit{changing lanes to the left})$, it means that a new NPC vehicle will be generated 20 meters directly behind the AV and 3.5 meters to the right, with its behavior set to prepare for a left lane change. 
Note that the travel direction of the newly generated NPC vehicle is set to align with the designated direction of the road it is on. 
Particularly, the lateral distance of $20$ meters ensures the generated NPC vehicle maintains a safe proximity to the AV without being too far, which could invalidate the test. Similarly, the longitudinal distance of $3.5$ meters matches the road width in the CARLA simulator, ensuring the NPC vehicle is neither too close nor too far from the AV.  %~\cite{won2022simulation}
We exclude the configuration where $(dis_{v}^{la}, dis_{v}^{lo})$ is $(0, 0)$, resulting in a total of 24 distinct configurations for the NPC vehicles.

On the other hand, the feature of a pedestrian can be described as a quadruplet: $(dis_{p}^{la}, dis_{p}^{lo}, dir_{p}, spd_{p})$. Similarly to the NPC vehicle, $dis_{p}^{la}$ and $dis_{p}^{lo}$ reflect the lateral and longitudinal distances between the pedestrian and the AV. $dir_{p}$ controls the forward direction of the pedestrian. Lastly, $spd_{p}$ sets the constant speed of the pedestrian. 
More specifically, the lateral distance $dis_{p}^{la}$ is fixed at 10 meters ahead of the AV's direction of travel, while the longitudinal distance $dis_{p}^{lo}$ is selected from a discrete list $[-10, 10]$. Similarly, to ensure the newly generated pedestrians maintain a safe distance from the AV, we fix the distance at 10 meters. 
The pedestrian's direction $dir_{p}$ can be 45 degrees aligned, 45 degrees opposite, or 90 degrees perpendicular to the AV's travel direction, while ensuring the pedestrian faces the AV. As for the pedestrian's speed, i.e., $spd_{p}$, it is set to either 0.94 meters per second (average walking speed) or 1.43 meters per second (average running speed). 
In total, we can set a pedestrian with 12 different configurations.

Additionally, to ensure the realism of the generated objects in the scenario, we apply specific realistic constraints. As previously mentioned, NPC vehicles and pedestrians are placed at a safe distance from the AV. Besides, newly generated objects are also positioned at a safe distance from other NPC vehicles or pedestrians already present in the scenario. All objects are confined within the boundaries of the map on which the AV is traveling and are generated on appropriate roads (e.g., lanes) specified by the simulator, preventing unrealistic placements (e.g., on top of a tree). 
Moreover, the driving direction of newly generated NPC vehicles is aligned with the designated road direction, and once initialized, these NPC vehicles are controlled by the simulator's default autopilot algorithm. It ensures realistic vehicle generation and driving behavior. For pedestrians, which are configured to always face the AV, we prevent invalid test scenarios where they might intentionally collide with the AV or other vehicles. Based on pilot study results, a perceived safe walking distance is set. If pedestrians come within this distance of other objects, they will stop immediately to prevent any intentional collisions. 
In addition, the speeds of NPC vehicles and pedestrians are strictly regulated. NPC vehicles are capped at the maximum speed limit corresponding to the road they are on, which can be obtained from the simulator, while pedestrian speeds are set to either the average walking or running speed.


\subsubsection{MOMDP Instantiation}
\noindent 
\textbf{State Space.} We dynamically configure environment parameters in the simulator to generate new dynamic objects, leading to a constantly evolving set of object configurations over time. 
Leveraging the parameters available from the CARLA simulator, we define the specific state at each time step as a quintuple $<pos, rot, vel, acc, anv>$. 
Specifically, each element of the state represents the AV's current position, rotation, velocity, acceleration, and angular velocity, respectively, with each element being a vector of three components. For instance, position $pos$ can be defined as $pos = (x, y, z)$, and rotation $rot$ is expressed as $rot = (\theta, \psi, \phi)$. 
Thus, each observed state will contain 15 state parameters. 

\textbf{Action Space.} We define each action as the configuration of a new dynamic object in the environment to increase the complexity of the scenario for AV testing. As mentioned in Section~\ref{subsubsec: parameters}, dynamic objects fall into two categories: NPC vehicles and pedestrians. We have 24 different configurations for NPC vehicles and 12 discrete configurations for pedestrians, resulting in an action space size of $24 + 12 = 36$. Thus, the action space consists of 36 possible actions, each used to create a configured dynamic object for AV testing.



\textbf{Vector Reward Function.} As mentioned in Section~\ref{sebsebsec: requirements_objective_functions}, we focus on two key safety and functional requirements. % \textit{the AV should not collide with any object} and \textit{the AV should complete the route within a reasonable time}.
Correspondingly, we define the objective functions \textit{TTC} and \textit{RC}, where lower values indicate a higher degree or possibility of requirement violations. However, in \method, higher rewards motivate the EQL agent to continue behaviors that generate those rewards to optimize objectives. Therefore, to increase the likelihood of the AV violating the requirements for testing, we define the corresponding reward functions as follows: 
\begin{equation}
Reward_{TTC} = 
\left\{
\begin{aligned}
    &1, \quad \text{if collision occurs}, \\
    &nor(\frac{1}{1 + \log(TTC + 1)}), \text{otherwise}.
\end{aligned}
\right.
\end{equation}

\begin{equation}
Reward_{RC} = 
\left\{
\begin{aligned}
    &1 - nor(RC), \quad \text{if } RC \neq 0, \\
    &0, \quad \text{otherwise}.
\end{aligned}
\right.
\end{equation}
We consider all requirements equally important, so we use a normalization function $nor()$ to scale both $Reward_{TTC}$ and $Reward_{RC}$ within the range [0, 1], ensuring fairness. Based on our pilot study, we observed that most TTC values fall between 0 and 20, with some exceeding 20. To better capture variations in smaller TTC values, we apply a logarithmic function to scale the data, enhancing its utility. 
Besides, if a collision occurs, we set $Reward_{TTC}$, which measures AV safety, to the maximum value of 1, emphasizing the significance of collisions in the test. 
Additionally, to prevent the generated scenario from causing the AV to remain stationary, leading to an invalid test, we set $Reward_{RC}$ to 0 when RC remains consistently 0. 
Accordingly, the vector reward function of \method is defined as $\mathbf{R} = [Reward_{TTC}, Reward_{RC}]$, guiding the EQL agent in selecting appropriate actions to generate critical scenarios. 


\subsubsection{Network Architecture, Parameter Settings and Implementation}
\noindent To ensure the effectiveness of \method and baseline methods for a fair comparison, a suitable network architecture and corresponding parameter settings need to be selected. For this purpose, we employed an automated hyperparameter optimization framework, Optuna~\cite{akiba2019optuna}, and conducted pilot studies to optimize both the network architecture and parameter settings. The detailed implementation can be found in our publicly available repository~\cite{MOEQTrepo}.

Based on the original EQL implementation~\cite{yang2019generalized} and a tool called MORL-Baselines~\cite{felten2024toolkit}, we implemented \method. We adopt the $\epsilon
$-greedy policy~\cite{stadie2015incentivizing} to balance exploration and exploitation of the action space and utilize the prioritized experience replay~\cite{schaul2015prioritized} to store transition tuples. Additionally, we apply homotopy optimization~\cite{watson1989modern} for calculating the loss function. Meanwhile, \sorlw employs the same implementation as DeepCollision, proposed by Lu et al.~\cite{lu2022learning}, which has demonstrated effectiveness in testing AVs using RL. 

For model training, based on Optuna's tuning results and our pilot study, we executed 1200 episodes, each consisting of 6 time steps, with 40 CARLA simulator minimum unit times allocated for each time step. 
To account for the stochastic nature of our method and selected baselines, during the model evaluation phase, we executed 100 episodes for both \method and \sorlw, freezing the training-related parameters while keeping the remaining parameters unchanged.


\subsubsection{Roads}
\noindent 
\method generates critical scenarios by configuring the driving environment of the AV throughout the driving task. In this work, we specify the driving tasks using different roads with various road structures. Each road has a starting and end point specifying the route of the AV. Specifically, as Figure~\ref{fig:road} shows, we select six roads for the experiment, covering the six scenario categories on the CARLA Leaderboard~\cite{carlaleaderboard}. These categories are instances of the pre-crash scenarios from NHTSA pre-crash typology~\cite{najm2007pre}. 
% \textit{Road1} depicts a highway where the AV needs to drive from the leftmost lane to the rightmost lane, and then maintain the lane until reaching the end. \textit{Road2} is a dual-way road with two opposite lanes, where the AV is expected to maintain its current lane until reaching the end. \textit{Road3} depicts a road with three lanes and a parking lot, with the middle lane starting with a left turn. The AV is required to maintain its lane until reaching the end. \textit{Road4} is a dual-way road that starts with a sidewalk and has two lanes on each side. The AV needs to maintain its lane until reaching the end. \textit{Road5} depicts a dual-way road with an intersection and two sidewalks. The AV needs to cross the intersection and then change lanes from the left to the right. \textit{Road6} depicts a road similar to \textit{Road3} but the left lane is severely damaged and in poor condition. The AV is required to maintain its lane until reaching the end. While driving, the AV might lose control due to bad conditions and must recover to its original lane.

\begin{figure*}[htbp]\captionsetup[subfigure]{font=large}
	\centering
        \resizebox{1.0\textwidth}{!}{
        \input{images/scenarios}
        }
	\caption{Driving roads for specifying driving tasks.}
    \label{fig:road}
\end{figure*}


\subsection{Evaluation Metrics}
\noindent To answer RQs, we defined the following metrics, which allowed us to compare \method with the selected baselines:

\blue{$\bm{V_{R1}}$ assesses the number of \textit{R1} requirement violations, which occurs when the AV collides with other objects. This metric is measured by the number of generated scenarios resulting in collisions.}

\blue{$\bm{V_{R2}}$ evaluates the number of \textit{R2} requirement violations, triggered when the AV either fails to complete the specified route within a given time budget or is involved in a collision. It is calculated based on the number of scenarios where this requirement is entirely violated.}

$\bm{V_{R1\_R2}}$ quantifies the number of generated scenarios that violate both requirements simultaneously, providing a direct assessment of the method's effectiveness in violating \textit{R1} and \textit{R2} simultaneously. 
% Its value ranges from 0 to 100, where 0 indicates that no critical scenarios are generated that violate both requirements, and 100 means that all generated scenarios violate both requirements.

\blue{Note that all $\bm{V}$ metrics are expressed on a scale of 0 to 100. A value of 0 indicates no critical scenarios violating the corresponding requirement(s), and a value of 100 means that all generated scenarios violate the associated requirement(s).}

\textbf{\textit{TTC}} measures the time remaining until the AV collides with other objects, assuming their current velocity and collision course stay constant. This metric quantifies the degree of violation of requirement \textit{R1}, using the same computation as in Equation~\ref{eq:ttc}. It ranges from 0 to positive infinity, with a value closer to 0 indicating a shorter potential collision time and a higher probability of collision.

\textbf{\textit{RC}} represents the percentage of the planned route distance that has been successfully completed by AV. This metric employs the same calculation method as Equation~\ref{eq:rc} to assess the degree of violation of requirement \textit{R2}. Its value ranges from 0 to 100, with values closer to 0 meaning lower completion of the route and a greater degree of \textit{R2} violation.


\subsection{Statistical Tests}\label{subsec:statistical}
\noindent Given that the results of our experiment are dichotomous, i.e., a scenario violated both requirements (Yes or No), we employed the Fisher's exact test~\cite{fisher1970statistical} to determine the statistical significance of results based on the recommendations from~\cite{arcuri2011practical}, with a significance level of $p<0.05$. 
For the effect size, we chose odds ratios (OR)~\cite{szumilas2010explaining} based on an existing guide~\cite{arcuri2011practical}. When OR is 1, the two compared methods are equal. An OR greater than 1 suggests that the first method is more likely to be better than the second, while an OR less than 1 indicates the opposite. The further the OR deviates from 1, the stronger the effect.


\section{Analysis of the Results and Discussion}\label{sec:results}

\subsection{Results for RQ1 -- Comparison with \rs}\label{sec:RQ1}
\noindent 
\blue{We compared \method with \rs regarding the three violation metrics, i.e., \vrone, \vrtwo, and \vronetwo. Besides, we also employed the mean \textit{TTC} and mean \textit{RC} to evaluate each method regarding the abilities to optimize objectives.
% corresponding to the violations of the specific requirement(s) under each metric.
}\footnote{Note that we obtain each \textit{TTC} for each scenario by averaging the \textit{TTC} at each time step, and the \ttcvrone values reported in Table~\ref{tab:evaluationResults_all} are the mean values of all scenarios where \textit{R1} violation occurred. Similarly, \rcvrtwo are the mean values of all scenarios where \textit{R2} violation occurred. For \ttc and \rc, we consider scenarios where both \textit{R1} and \textit{R2} were violated.}
Table~\ref{tab:evaluationResults_all} presents the descriptive statistics of \method compared to \rs on the six roads. We also report the Fisher's exact test results in Table~\ref{tab:FishersStatisticalResults_all}.


\begin{table*}[]
\centering
\caption{\blue{Descriptive Statistics achieved by \rs, \sorlw, and \method for Different Roads. ``/" means not applicable. \textbf{Bold} indicates the best result across all the methods. }}
\label{tab:evaluationResults_all}
\resizebox{0.8\linewidth}{!}{
\input{tables/tabEvaluationResults_all}
}
\end{table*}

\begin{table}[]
\centering
\caption{\blue{Fisher's Exact Test Statistical Results of \method Compared with \rs on Different Roads. ``/" means not applicable. \underline{Underlining} denotes that the statistical results are not significantly different.}}
\label{tab:FishersStatisticalResults_all}
\vspace{5pt}
\resizebox{0.65\linewidth}{!}{
\input{tables/tabFishersStatisticalResults_all}
}
\end{table}



\blue{Regarding \vrone, Table~\ref{tab:evaluationResults_all} shows that \method outperformed \rs on \textit{Road1}, \textit{Road2}, and \textit{Road4}. For \textit{Road5} and \textit{Road6}, both \method and \rs achieved equivalent performance, while on \textit{Road3}, \rs resulted in six violations of requirement \textit{R1}, outperforming \method, which achieved two \textit{R1} violations.}
\blue{As for the statistical results, the results in Table~\ref{tab:FishersStatisticalResults_all} demonstrated that \method outperformed \rs on \textit{Road1}, \textit{Road2}, and \textit{Road4}. For \textit{Road3}, \textit{Road5}, and \textit{Road6}, no significant differences were observed. This indicates that \method consistently performed similarly to or better than \rs across the various roads.}
\blue{We further calculated the mean \textit{TTC} of scenarios that violated \textit{R1} (i.e., \ttcvrone), and we observe that \method performed better than \rs on \textit{Road1}, \textit{Road2}, and \textit{Road4}. Similarly, \rs outperformed \method on \textit{Road3}. On \textit{Road6}, although both methods exhibited identical performance for \vrone, \method surpassed \rs in \ttcvrone, indicating that the two methods identified different scenarios violating the same requirement \textit{R1}. For \textit{Road5}, no scenario was found that violated \textit{R1}, making \ttcvrone inapplicable for both \method and \rs.}

\blue{For \vrtwo, \method significantly outperformed \rs on \textit{Road1}, \textit{Road3}, \textit{Road4}, and \textit{Road6} in terms of violating the requirement \textit{R2}. On \textit{Road2} and \textit{Road5}, however, \rs performed slightly better than \method, with differences of 2 and 4 violation scenarios, respectively.}
\blue{As similarly shown in Table~\ref{tab:FishersStatisticalResults_all}, \method demonstrated significant superiority over \rs on \textit{Road1}, \textit{Road3}, \textit{Road4}, and \textit{Road6}, while performing comparably with \rs on \textit{Road2} and \textit{Road5}. This highlights that across all roads, \method was either superior (4 out of 6) or equal (2 out of 6) to \rs.}
\blue{Considering \rcvrtwo, \method performed better than \rs on all roads except \textit{Road5}, where \method didn't generate scenarios that violated \textit{R2}. It means that the critical scenarios produced by \method demonstrated a greater degree of violation compared to those of \rs, highlighting \method's clear tendency to target and violate requirement \textit{R2} based on its violation objective.}

\blue{Regarding \vronetwo, }Table~\ref{tab:evaluationResults_all} shows that \method led to more critical scenarios violating \textit{R1} and \textit{R2} simultaneously on \textit{Road1}, \textit{Road2}, and \textit{Road4}, while neither \method nor \rs on \textit{Road5} violated both requirements simultaneously. On \textit{Road6}, both \method and \rs generated only one scenario that violated both requirements, while on \textit{Road3}, \rs generated more scenarios to violate both requirements than \method, although the difference was small, i.e., 6 vs. 2. 
We further analyzed the generated critical scenarios on each road and found that the number of violations of the two requirements on \textit{Road3}, \textit{Road5}, and \textit{Road6} was small because it was difficult to violate \textit{R1} on these three roads.
Specifically, for \method, the numbers of violations of \textit{R1} on \textit{Road3}, \textit{Road5}, and \textit{Road6} were 2, 0, 1, respectively. After replaying the scenarios and analyzing these roads, we found that these roads have more complex structures compared to other roads, causing the AV to drive more cautiously, especially for \textit{Road5} which mainly contains intersections. For \rs, the number of violations was 6, 0, and 1, respectively. Regarding the statistical test results, as Table~\ref{tab:FishersStatisticalResults_all} shows, \method significantly outperformed \rs on \textit{Road1}, \textit{Road2}, and \textit{Road4}, and achieved a comparable performance as \rs on the other roads. The results of \vronetwo indicate that \method achieved an overall better performance than \rs in terms of violating both requirements simultaneously, but the performance varies across different roads. 
When it comes to mean \ttc when both \textit{R1} and \textit{R2} were violated, the results in Table~\ref{tab:evaluationResults_all} show that \method outperformed \rs with lower mean \ttc on \textit{Road1}, \textit{Road2}, \textit{Road4}, and \textit{Road6}. For \textit{Road3}, \rs was better than \method (11.772 vs. 14.167). Moreover, no method violated both requirements on \textit{Road5}. Similarly, regarding mean \rc, as Table~\ref{tab:evaluationResults_all} shows, \method outperformed \rs on \textit{Road1}, \textit{Road2}, and \textit{Road4}, while being equal on \textit{Road6}. For \textit{Road3}, \rs was better than \method, whereas no violations were detected on \textit{Road5}. The results for \ttc and \rc show that for critical scenarios that violate these two requirements, the scenarios generated by \method make it more difficult for the AV to complete its route and lead to a higher risk of collision. 

\conclusion{Conclusion for RQ1: \method achieved an overall better performance than \rs in terms of violating \blue{the single requirement or }both requirements simultaneously, and the scenarios generated by \method make it harder for the AV to complete its route and increase the risk of collision.}



\begin{table}[]
\centering
\caption{\blue{Fisher's Exact Test Statistical Results of \method Compared with \sorlw on Different Roads. ``\textit{inf}" represents infinity. ``/" means not applicable. \underline{Underlining} denotes that the statistical results are not significantly different.}}
\label{tab:FishersStatisticalResults_sorlw}
\vspace{5pt}
\resizebox{0.65\linewidth}{!}{
\input{tables/tabFishersStatisticalResults_sorlw}
}
\end{table}

\subsection{Results for RQ2 -- Comparison with \sorlw}\label{sec:RQ2}
\noindent
Table~\ref{tab:evaluationResults_all} presents the descriptive statistics of \method compared to \sorlw on each of the six roads. As the table shows, in terms of the number of scenarios that \blue{violated \textit{R1} or} violated \textit{R1} and \textit{R2} at the same time, \blue{i.e., \vrone and \vronetwo}, \method outperformed \sorlw on all of the six roads except for \textit{Road5}, where neither \method nor \sorlw violated both requirements. When checking the specific numbers, we found that \sorlw did not generate any scenario that \blue{violated the requirement \textit{R1} or violated both requirements simultaneously} on all six roads. One possible explanation is that \sorlw used fixed equal weights during the optimization process, which prevents \sorlw from dynamically adjusting the importance of each objective based on the current environment state. Therefore, the \sorlw agent may struggle to adaptively balance objectives with different levels of importance, leading to a local optimum, where the reward for one objective is maximized at the expense of the other. 
\blue{Since \sorlw generated zero critical scenarios for both the \vrone and \vronetwo metrics across all roads, conducting Fishers exact test to compare it with \method was not meaningful and was therefore omitted.}

\blue{We further compared \sorlw with \method regarding \vrtwo. As shown in Table~\ref{tab:evaluationResults_all}, \method outperformed \sorlw on \textit{Road1}, \textit{Road2}, \textit{Road3}, and \textit{Road6}. On \textit{Road4}, however, \sorlw performed better than \method, with specific \vrtwo values of 99 and 82, respectively. For \textit{Road5}, neither method generated critical scenarios violating requirement \textit{R2}. 
Similarly, Table~\ref{tab:FishersStatisticalResults_sorlw} indicates that on \textit{Road1}, \textit{Road2}, \textit{Road3}, and \textit{Road6}, \method was significantly better than \sorlw. While for \textit{Road4}, \sorlw showed superior performance, with the \textit{p}-value less than 0.01 and the \textit{OR} value of 0.046 (less than 1). On \textit{Road5}, there were no significant differences between the two methods.}
% \blue{We further analyzed and found that for \vrtwo, the performance of \sorlw varied, with results sometimes better, sometimes worse, and occasionally on par with those of \method. This suggests that \sorlw, by using fixed equal weights to optimize the requirements, may fall into local optima, resulting in instability and variability in outcomes.}

\conclusion{Conclusion for RQ2: The results show that \method outperformed \sorlw regarding generating critical scenarios to violate multiple requirements simultaneously, indicating that MORL is more effective in handling AV testing tasks where multiple test objectives need to be satisfied.}


\begin{figure}[!t] %\captionsetup[subfigure]{font=large}
\centering
\resizebox{1.0\linewidth}{!}{
\input{images/3D_new}
}
\caption{Convergence Trends of \textit{TTC} and \textit{RC} achieved by \method and \sorlw on Different Roads.}
\label{fig:3D}
\end{figure}

\subsection{Discussion}


\noindent
\textit{Benefiting from adaptive weights.} Multiple objectives must be adaptively balanced during training due to their inherent interdependencies. Figure~\ref{fig:3D} shows the convergence trends of the two objectives (\ttc and \rc) achieved by \method and \sorlw during training on the six roads. As the figure shows, on \textit{Road1}, \method converged to a lower \ttc and lower \rc compared to \sorlw. Moreover, compared to \sorlw, \method was more stable throughout the training process, with \sorlw showing no clear convergence trend for \textit{TTC}, indicating that \method achieved a better balance between the two objectives. On \textit{Road2}, though both \method and \sorlw converged to a similar \ttc and \rc, we observed that \method was more efficient in finding the optimal solutions. In addition, compared to \sorlw, \method was more stable and better balancing the two objectives during the training process. On \textit{Road3}, \method converged to a lower \ttc and a similar \rc compared to \sorlw. As for the training stability, \method was more stable than \sorlw, indicating a better balance between the two objectives. For \textit{Road4}, \method and \sorlw converged to a similar \ttc and \rc, while \sorlw was worse regarding training stability. On \textit{Road5}, regarding \ttc convergence, \method was slightly worse than \sorlw, but it achieved a faster convergence speed. Furthermore, in terms of \rc, we did not observe a convergence trend for \sorlw, while \method converged. This observation indicates that the objectives were better balanced by \method, whereas \sorlw failed to adaptively balance the importance of the two objectives, resulting in only one objective being optimized. On \textit{Road6}, we observed that both methods converged to a similar \ttc and \rc. Besides, although both methods tend to focus more on \ttc, \method performed more stable during training. 
Long story short, \method achieved an overall stable performance during training and well-balanced the two objectives, indicating that MORL with an adaptive weighting mechanism can better handle these two objectives compared to single-objective RL with fixed equal weights.



\textit{Analyzing violations of requirements.} Recall that our goal is to generate scenarios that violate multiple requirements simultaneously, and the evaluation results show that \method tends to have a higher chance of violating two requirements simultaneously. To understand how \method performed in violating every individual requirement, we further analyze the distribution of violations of each requirement. 
\blue{As shown in Table~\ref{tab:evaluationResults_all}, } the results indicate that for the scenarios generated by all three methods, \textit{R2} is more easily violated than \textit{R1}. For example, on \textit{Road1}, the numbers of scenarios that violated \textit{R1} are 3, 0, and 25 for \rs, \sorlw, and \method, respectively, while as for scenarios that violated \textit{R2}, the numbers are 33, 24, and 66 for \rs, \sorlw, and \method, respectively. Similar observations can be found on the other roads, where \textit{R2} is more likely to be violated than \textit{R1}.
A plausible explanation is that the AV under test is designed to prioritize \textit{R1} (collision avoidance), as collisions tend to result in more severe consequences compared to not completing a route (\textit{R2}). The emphasis on safety makes the AV more robust to \textit{R1} violations but may overlook the functional requirement, i.e., \textit{R2}, making it more vulnerable to route completion failures. This observation highlights the importance of striking a balance between safety and functional requirements in AV design and the need for a more holistic approach that weighs these requirements effectively.


\section{Threats to Validity}\label{sec:threats}
\noindent 
\textit{Conclusion validity} is concerned with the reliability of the conclusion drawn from the empirical study, which might be influenced by the inherent randomness of the simulator. We mitigate this threat by repeating each execution 100 times and using appropriate statistical tests following well-known guidelines~\cite{arcuri2011practical}.
\textit{Construct validity} relates to the evaluation metrics. For a fair comparison, we chose \blue{five} metrics commonly used for AV testing. We also employed comparable metrics to evaluate the performance of RL and MORL algorithms. 
\textit{Internal validity} concerns with the parameter settings. To mitigate the potential threats to internal validity, we determined the hyperparameters by employing an automated hyperparameter optimization framework named Optuna~\cite{akiba2019optuna}. Besides, we acknowledge the performance of \method might be improved by tuning EQL settings, e.g., formulating the state space using sensor data such as camera images, and designing action space covering more environmental objects. Thus, we will investigate other \method designs in the future. 
\textit{External validity} is about the generalizability of the empirical evaluation. We used one subject system and one simulator to conduct the experiment, which could potentially threaten the external validity. To mitigate this issue, we chose the advanced end-to-end AV controller (i.e., Interfuser) and the high-fidelity simulator (i.e., CARLA). However, we acknowledge that conducting experiments with more case study systems could strengthen our conclusions. \method can be easily adapted to test other AV systems with minimal effort.


\section{Related Work}\label{sec:related_work}
\noindent Testing AVs in various driving scenarios is critical in ensuring their safety and functionality. However, due to the complexity of the AV operating environments, there is theoretically an infinite number of possible driving scenarios. Therefore, we need to identify critical scenarios that could violate safety or functional requirements.
Search-based methods are widely adopted to generate critical scenarios under the guidance of fitness functions~\cite{ben2016testing,abdessalem2018testing,zhong2022neural,10645815,10234383,zhou2023specification}. For example, 
Abdessalem et al.~\cite{abdessalem2018testing} proposed NSGAII-DT to test vision-based control systems by combining NSGA-II~\cite{deb2002fast} and decision tree models.
Li et al.~\cite{li2020av} proposed AV-Fuzzer, an approach integrating fuzz testing with evolutionary search, to identify critical scenarios that cause safety violations. Haq et al.~\cite{haq2022efficient} proposed SAMOTA, an efficient AV testing approach that combines multi-objective search with surrogate models. SAMOTA employs multi-objective search to generate critical scenarios and improves testing efficiency by simulating the simulator with surrogate models. To test AV against traffic laws, Sun et al.~\cite{sun2022lawbreaker} proposed LawBreaker, which adopts a fuzzing algorithm to search for scenarios that can effectively violate traffic laws.
Though search-based approaches have shown promising performance, they have limited effectiveness in handling tasks requiring runtime sequential interactions with the environment, which are essential when manipulating dynamic objects and ensuring precise control over them. Another challenge is that the AV operating environment is dynamically changing and uncertain, while search-based approaches cannot effectively adapt to environmental changes.

RL-based approaches generate critical scenarios through intelligent agents that interact with and configure the AV operating environment~\cite{chen2021adversarial,lu2022learning,10172658,feng2023dense}. For example, 
Koren et al.~\cite{8500400} extended adaptive stress testing using RL to find critical scenarios that cause AV to collide.
Lu et al.~\cite{lu2022learning} proposed DeepCollision, an RL-based environment configuration framework that generates scenarios to uncover collisions of AV. To test lane-changing models, Chen et al.~\cite{chen2021adversarial} proposed an RL-based adaptive testing framework to generate time-sequential adversarial scenarios. Similarly, Doreste et al~\cite{doreste2024adversarial} proposed an adversarial method that models NPCs with independent RL-based agents to challenge the ADS under test. Feng et al.~\cite{feng2023dense} proposed a dense RL-based approach to learn critical scenarios from naturalistic driving data by editing the Markov decision process to eliminate non-safety-critical states. 
To test multiple requirements, Haq et al.~\cite{10172658} proposed MORLOT that adapts single-objective RL and multi-objective search to generate test suites to violate as many requirements as possible. Notice that MORLOT is designed to violate multiple \textit{independent} requirements by developing a set of scenarios, each violating one independent requirement. However, in reality, AV requirements are often interdependent and must be evaluated simultaneously.
For example, failing to meet a functional requirement (e.g., not completing a route) may violate a safety requirement (e.g., causing a crash), highlighting the need for approaches simultaneously evaluating multiple requirements. 

Different from the above works, we propose \method by applying MORL to generate critical scenarios that violate multiple requirements simultaneously. \method relies on EQL to automatically learn and adaptively generate critical scenarios by interacting with AV operating environments through a multi-objective approach. 


\section{Conclusion and Future Work}\label{sec:conclusion}
\noindent
In practice, many Autonomous Vehicle (AV) requirements are inherently interdependent and must be evaluated simultaneously to ensure comprehensive testing. To this end, we propose \method, a multi-objective reinforcement learning (MORL)-based AV testing approach to generate critical scenarios violating multiple requirements simultaneously. \method employs the Envelope Q-learning algorithm to adaptively learn environment configurations that lead to violations of multiple requirements. We select two common AV requirements for evaluation and compare \method with two baseline methods on six driving roads covering different driving situations. The evaluation results demonstrate the promising performance of \method in generating scenarios to test interdependent AV requirements. In the future, we plan to investigate the generalization of \method by systematically conducting experiments with different AVs and simulators. Besides, we plan to study \method's effectiveness in testing other AV requirements. Finally, investigating other \method designs such as adapting multiple policy MORL algorithms is another further plan.
The replication package is provided in an online repository~\cite{MOEQTrepo}. 




\section*{Acknowledgments}
This work is supported by the Co-tester project (No. 314544) and the Co-evolver project (No. 286898/F20), funded by the Research Council of Norway.
The work is also partially supported by the RoboSAPIENS project funded by the European Commissions Horizon Europe programme under grant agreement number 101133807.
Aitor Arrieta is part of the Software and Systems Engineering research group of Mondragon Unibertsitatea (IT1519-22), supported by the Department of Education, Universities and Research of the Basque Country.



%Bibliography
\bibliographystyle{unsrt}  
\bibliography{references}  


\end{document}
