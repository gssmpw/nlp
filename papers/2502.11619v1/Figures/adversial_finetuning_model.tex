\begin{figure}[h!t]
\small
    \centering
    \begin{tikzpicture}[node distance=2.0cm]
    \node (og_image_dataset) [detail] {Image\\Dataset};
    \node (ldm) [normal, right of=og_image_dataset, xshift=-0.5cm] {LDM\\\textit{base}};
    \node (finetuning) [shadow_rect, right of=ldm, mygreen, minimum height=1.5cm, minimum width=2cm] {};
     \node[action] [above of=finetuning, yshift=-1.35cm, draw=none, fill=none] {\textbf{Finetuning}};
     \node (adversial_ldm) [black, right of=finetuning, xshift=0.25cm] {Adversarial\\LDM\\$\mathbf{M_T}$};
     \node (unlawful_data) [positive, above of=ldm, yshift=-0.9cm] {Image Dataset\\$\mathbf{D_{Target}}$};
     \node (ldm2) [normal, right of=ldm, xshift=-0.5cm] {LDM};
     \node [normal, right of=ldm2, xshift=-1.0cm] {Data};
     

    \draw [arrow] (og_image_dataset) -- (ldm);
    \draw [arrow] (ldm) -- (finetuning);
    \draw [arrow] (unlawful_data.east) -- (finetuning.north|-unlawful_data) -- (finetuning);
    \draw [arrow] (finetuning) -- (adversial_ldm);

    \end{tikzpicture}
    \caption{An approach a malicious actor could use to obtain a LDM which is trained on unrightfully obtained data.}
    \label{fig:adversial_finetuning_model}
\end{figure}