In 2017 the paper \textit{Membership Inference Attacks against Machine Learning Models} \cite{shokri2017membership} proposed how to conduct a \acrlong{MIA} against classification models in a black-box setup. The paper investigates the possibility of creating "shadow models" which mimic a target model and uses them to train an attack model for the task of classifying the membership of data samples, i.e. whether they belong to the training set of a target model or not. A shadow model as described in the paper is a model that aims to be similar to the target model, i.e. have a similar architecture, be trained in a similar way, use similar training data etc. The motivation for using shadow models is that it allows for training an attack model using supervised learning.\\ 
This is possible by using the training and test data for the shadow models along with their outputted classification vectors when training the attack models (training set are members, test set are non-members). The paper shows that MIAs can be feasible in a black-box setup.

Besides targeting classification models, multiple papers have also investigated the possibility of performing MIAs on generative models. One such paper is \textit{GAN-Leaks: A Taxonomy of Membership Inference Attacks against Generative Models} \cite{Chen_2020}. The paper has its main focus on MIAs against \acrlong{GAN}s and describes multiple attack scenarios including a full black-box attack. Common for all attack scenarios presented in the paper is the assumption that the probability a generator can produce a sample is proportional to that sample being a member of the generator's training set. This assumption is made as the generator model is trained to approximate the distribution of the training data. The full black-box attack presented works by sampling from a generator and then finding the sample closest to the query sample (ie. the sample to determine the membership of) using a distance metric. The most similar generated sample is then used to approximate the probability that the generator can create the sample which is then used to predict the membership.\\
A generalised approach to MIA was proposed in the paper \textit{Generated Distributions Are All You Need for Membership Inference Attacks Against Generative Models} \cite{zhang2023generated}. The proposed approach does not require shadow models, works in a black-box setup, and can be used against multiple generative models. The core idea for the technique presented is to take advantage of the similar data distribution between the training data of a generative model and its output as done by \cite{Chen_2020}. The similarity between the data distribution relies on the model over-fitting to its training data. This similarity functions as an information leakage, making the generated output describe traits of the model's training data.\\
In the paper, supervised learning is used for training an attack model for the membership inference task. This is done by querying the target model to generate output used as training positives. This is assumed to be representative as training positives due to the assumption of a similar data distribution between the target models training data and its output. The training negatives are obtained from an auxiliary dataset. The paper uses the Resnet-18 architecture for the attack model.
