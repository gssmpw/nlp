\paragraph{The target dataset}
 $\mathbf{D_{target}}$ used for fine-tuning $\mathbf{M_T}$ is a dataset consisting of image-text pairs with the text describing the image. A BLIP model \cite{blip}  is used to auto-label the images. When $\mathbf{D_{DTU}^{seen}}$ is used to create $\mathbf{D_{target}}$, the labels generated with BLIP are conditioned with \texttt{"a dtu headshot of a"} and for $\mathbf{D_{AAU}^{seen}}$, \texttt{"a aau headshot of a"} is used. In other words, all labels in $\mathbf{D_{target}^{DTU}}$ begin with \texttt{"a dtu headshot of a (...)"}.
 
\paragraph{The Training Positives} 
$\mathbf{D^{gen}}$ are generated using the fine-tuned model $\mathbf{M_T}$. 2,500 images are generated using 100 inference steps and a guidance scale of 7.5.  When using the model fine-tuned on DTU images, the prompt \texttt{"a dtu headshot"} is used and for the model fine-tuned on AAU images, the prompt \texttt{"a aau headshot"} is used. 25 images are generated per seed.

\paragraph{The Auxiliary Data}
i.e. the training negatives for the supervised learning of $\mathbf{M_A}$ are supposed to represent all images from the same (or a similar) domain as $\mathbf{D_{target}}$ not seen by $\mathbf{M_T}$. The auxiliary data must not have been used for training $\mathbf{M_T}$, i.e. the data should not be part of $\mathbf{D_{target}}$ or the original training data for $\mathbf{M_T}$. In this paper the domain of interest is face images, so the auxiliary data should be face images not seen by $\mathbf{M_T}$.\\
 In \cite{zhang2023generated} they propose using an auxiliary dataset consisting of real images from other datasets. The experiments conducted in this paper will default to constructing the auxiliary datasets using another generative model to generate images. This is done to ensure the attack model does not simply learn to classify real images and generated images. This is also mentioned as the 2nd pitfall for MIA on LDM's in \cite{Dubinski2023TowardsMR}. It should be noted that this is more computational expensive than simply using some dataset, as it will require training a generative model when performing a MIA. 

\subsection{Model Specifications}
\paragraph{The Target Model} 
$\mathbf{M_T}$ used for this project is a fine-tuned version of Stable Diffusion v1.5 \cite{rombach2022highresolution}. Stable Diffusion v1.5 is trained on a subset of LAION-5B. The model is then fine-tuned on each of the datasets presented in \cref{sec:data_source_for_target_model} resulting in multiple models, e.g. $\mathbf{M_T^{DTU}}$ which is the SD v1.5 model fine-tuned on $\mathbf{D_{DTU}^{seen}}$ or $\mathbf{M_T^{DTU \ WM}}$ which is the SD v1.5 model fine-tuned on \wmseen instead.

\paragraph{The Attack Model} 
used is Resnet-18 which was introduced in 2015 \cite{DBLP:journals/corr/HeZRS15}. The pretrained weights\footnote{The weights are available here \url{https://download.pytorch.org/models/resnet18-f37072fd.pth}} are kept, however a fully connected layer with two neurons replaces the standard 1000-neuron final layer. The loss function used is ca\-te\-go\-ri\-cal cross\-en\-tro\-py and the Adam optimizer is used for fast con\-ver\-gen\-ce.

\subsection{Experimental Setup} \label{sec:Test_Setup}
Multiple experiments are performed to determine the success and performance of MIAs against the target model, $\mathbf{M_T}$. Similar for all tests is that they aim to help investigate the possibility of a successful MIA on a $\mathbf{M_T}$ in the setup depicted in Figure \ref{fig:our_setup_model}. As illustrated in the figure, some $\mathbf{M_T}$ is being fine-tuned on an image dataset which has been unrightfully obtained, $\mathbf{D_{target}}$.

\input{Figures/our_setup_model}

The target model $\mathbf{M_T}$ is then queried to generate output images used as training positives in $\mathbf{D_{train}}$ and some auxiliary data is added to $\mathbf{D_{train}}$ as negatives. The attack model $\mathbf{M_A}$ is then trained on $\mathbf{D_{train}}$ using supervised learning. The resulting model $\mathbf{M_A}$ is then queried on images to classify whether they were part of $\mathbf{D_{target}}$ or not. 15\% of the test data is used for validation of $\mathbf{M_A}$. 

For the experiments  $\mathbf{M_T}$ is fine-tuned using a known $\mathbf{D_{target}}$, hereby the ground-truth is known when creating $\mathbf{D_{train}}$ and querying the resulting model $\mathbf{M_A}$. This makes it possible to determine the performance of $\mathbf{M_A}$ and the MIA. On \cref{fig:testing_attack_model} the approach to testing the attack model is shown. 

\input{Figures/testing_attack_model}

