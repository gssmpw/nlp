For each test, the attack model has been trained 5 different times with 5 different seeds to calculate the 95\% confidence intervals of the metrics of interest. Zero-shot classification using the CLIP model \cite{radford2021learning} is used as a baseline. On \cref{tab:results_table}, all the results are summarised. Note that the motivation/basis of several tests are different, therefore they should not all be directly compared on only the AUC score. 

\input{Tables/results_table}

\paragraph{The Impact of the Relationship between training and test negatives} can be seen on \cref{tab:results_table} No. 3, 4, and 7. The MIA is successful across all three tests: $\mathbf{D_{DTU}^{seen}}$ vs $\mathbf{D_{AAU}^{seen}}$, $\mathbf{D_{DTU}^{seen}}$ vs $\mathbf{D_{AAU}^{unseen}}$, and $\mathbf{D_{DTU}^{seen}}$ vs $\mathbf{D_{LFW}}$. All three test uses $\mathbf{D_{DTU}^{gen}}$ and $\mathbf{D_{AAU}^{gen}}$ for the training of $\mathbf{M_A}$. No significance difference was found in the MIA performance for the different settings of test positives and negatives. In our case it does not seem to matter if the train negatives were generated using the test negatives (cf. \cref{tab:results_table} No. 3 vs 4). Neither did it seem to matter if the training and test negatives are sampled from the same data distribution distinct from the training positives or not (cf. \cref{tab:results_table} No. 3 vs 7). $\mathbf{M_A}$ clearly outperforms the baseline in test 3 and 4, but the baseline model performs better on experiment 7 with a near perfect AUC for the baseline. This indicates that using a not-generated image-set against a generated image-set artificially boosts the MIA performance.

\paragraph{The effect of using real images for the auxiliary dataset} can be seen on test (\cref{tab:results_table} No. 2). It uses 2,000 images from $\mathbf{D_{DTU}^{gen}}$ and 2,000 images from $\mathbf{D_{AAU}}$ to train $\mathbf{M_A}$. The attack model is then tested on 952 $\mathbf{D_{DTU}^{seen}}$ images (positives) and 8,034 $\mathbf{D_{LFW}}$ images (negatives). This was done to test the effect of using real images for the auxiliary set (training negatives). As can be seen on \cref{tab:results_table} experiment No. 2, $\mathbf{M_A}$ is successful in the test and achieves an AUC of $\sim0.71 \pm0.02$.\\
When comparing this with \cref{tab:results_table} No. 7, which shows the same test except $\mathbf{M_A}$ is trained using the generated auxiliary set $\mathbf{D_{AAU}^{gen}}$ instead, it becomes apparent that it is beneficial for the attack model to use a generated auxiliary set as it's training negatives. Zhang et al. finds that using a generated auxiliary dataset is comparable to using real images \cite{zhang2023generated}. In our case, we find that using a generated auxiliary dataset is significantly better than using real images.

It can also be seen on \cref{tab:results_table} No. 2, that the baseline model excels in classifying $\mathbf{D_{DTU}^{seen}}$ and $\mathbf{D_{LFW}}$ as members and non-members and clearly outperforms $\mathbf{M_A}$ for this test with a near perfect AUC\footnote{This test is identical to the test depicted on \cref{tab:results_table} No. 7, as only the training negatives differ in the two tests, and the training negatives are not considered by the baseline.}. Note that the perfect AUC is likely due to the non-generated auxiliary data being too dissimilar to the positives, which artificially boosts the MIA performance.




\paragraph{Results of MIA without fine-tuning }
can be seen on Test no. 10. It is carried out where $\mathbf{D_{NFT}^{gen}}$ is used as both the training positives and negatives for  $\mathbf{M_{A}}$. As to stay consistent with the other tests, $\mathbf{M_T}$ is prompted with \texttt{"a dtu headshot"} and \texttt{"a aau headshot"} to generate two different $\mathbf{D_{NFT}^{gen}}$. As no information leakage can exist between neither the positives or negatives (unless the pretraining of SD 1.5 includes images from AAU or DTU), the MIA should not perform better than random guessing. However it did, which could be explained by a coincidence in data distribution similarity.

For the Test no. 9, $\mathbf{M_A}$ was first trained on $\mathbf{D_{NFT}^{gen}}$ as positives and $\mathbf{D_{AAU}^{gen}}$ as negatives. Then it was tested using $\mathbf{D_{DTU}^{seen}}$ as positives and $\mathbf{D_{AAU}^{unseen}}$ as negatives.
The MIA was still successful although the AUC score achieved by $\mathbf{M_A}$, $0.66\pm0.03$, is significantly worse than in all other tests on $\mathbf{D_{DTU}^{seen}}$ vs $\mathbf{D_{AAU}^{unseen}}$.

As there is no relation between training and test positives, this result shows that there is information leakage between training negatives $\mathbf{D_{AAU}^{gen}}$ and test negatives $\mathbf{D_{AAU}^{unseen}}$. The implication of this discovery is that the tests using $\mathbf{D_{AAU}^{gen}}$ as training negatives and $\mathbf{D_{AAU}^{unseen}}$ (or $\mathbf{D_{AAU}^{seen}}$) as test negatives have a artificial boost in their performance and thus inflated metrics.  This could be explained by the fact that they originate from the same data distribution.

\paragraph{Relationship between training time of target model and success of MIA.}
The MIA performance against target models fine-tuned on $\mathbf{D_{target}^{DTU}}$ for an increasing number of epochs can be seen on \cref{tab:results_table} experiment No. 4. For all tests, $\mathbf{M_A}$ is trained on $\mathbf{D_{DTU}^{gen}}$ and $\mathbf{D_{AAU}^{gen}}$ and tested on $\mathbf{D_{DTU}^{seen}}$ and $\mathbf{D_{AAU}^{unseen}}$. For 400 epochs, the AUC score is significantly better than the one for 100 epochs (the same can not be concluded for 400 vs 50 epochs, as the intervals barely overlap). As seen on the 50 epoch experiment, the MIA is still successful when $\mathbf{M_T^{DTU}}$ has trained for 50 epochs and results in comparable performance to the tests against target models trained for 400 epochs. The baseline appears to perform equally well across the different training times and thus does not appear to gain extended knowledge of the underlying training data distribution with increasing training time.

\paragraph{A mix of DTU and LFW in the target dataset.}
Here $\mathbf{M_A}$ is trained using $\mathbf{D_{AAU}^{gen}}$ as negatives and a mix of DTU and LFW as positives: $\mathbf{D_{DTU+LFW}^{gen}}$ (balanced training with $2,500$ images from each set). Note that only $\mathbf{D_{DTU}^{seen}}$ are test positives. The result shown on \cref{tab:results_table} experiment No. 13 shows that $\mathbf{M_A}$ still performs a successful MIA even if only half of the data used to fine-tune $\mathbf{M_T}$ is of interest. It is not significantly worse than when the target model is only fine-tuned on $\mathbf{D_{DTU}^{seen}}$, which was the case in experiment No. 4 ($0.83\pm0.02$ vs $0.86\pm0.02$). While much worse than $\mathbf{M_A}$, the baseline model is also able to perform a successful MIA in this test with close to the same performance when compared to test No. 4.

\paragraph{A different prompt for target model inference}
has influence on the generated output. The result of using the prompt \texttt{"a profile picture"} instead of \texttt{"a dtu headshot"} for inference to generate $\mathbf{D_{DTU}^{gen}}$ used as training positives for $\mathbf{M_A}$ is shown on \cref{tab:results_table} test No. 5. The performance of the attack model is not significantly different when conditioning the target model with \texttt{"a profile picture"} for inference compared to \cref{tab:results_table} test No. 4 ($0.82\pm0.05$ vs $0.86\pm0.02$). This could indicate that the generated distribution still contains the features which allow the Resnet-18 to make good predictions.  An explanation could be that the 400 epochs of fine-tuning on the "dtu" label and images has made the latent space more uniform. This is supported by the fact that on \cref{fig:guidance_scale_examples} the image with guidance scale $s=0$ still produces a headshot, even though according to \cref{eq:cfg} it is unconditioned.

\begin{equation}\label{eq:cfg}
\epsilon_t=\epsilon_{t,uncond} + s \cdot (\epsilon_{t,\tau(y)} - \epsilon_{t,uncond})    
\end{equation}

\paragraph{Recognising seen vs unseen samples from the same data distribution.}

The MIA on $\mathbf{D_{DTU}^{seen}}$ vs $\mathbf{D_{DTU}^{unseen}}$ performed poorly as seen on \cref{tab:results_table} test No. 8. This shows that the MIA presented in this paper does not seem successful in the task of identifying membership on an individual basis (for a single data-point), but instead is viable for the task of inferring the membership of a dataset as a whole. These results indicate some sort of shared characteristic among the collection of images. The nature of this shared characteristic is unknown. It is a reasonable assumption that images taken at the same locations/universities/organisations share some features.

\paragraph{Using watermarks for MIA enhancement}
can be seen on \cref{tab:results_table} test No. 11 and 12. The visible watermark tested in \wmseen vs $\mathbf{D_{AAU}^{unseen}}$ were very effective and lead to a near perfect classification of the test set by the attack model. The use of a hidden watermark however did not show any improvement compared to using no watermarks (cf. \cref{tab:results_table} test No. 12 vs No. 4) which shows that either the target model did not learn to mimic the hidden watermark, or that the attack model was not able to pick up on the nearly invisible watermark.
An example of the watermarks is shown on \cref{fig:watermark_examples}.

\begin{figure}[h!t]
\centering
\subfloat[visible watermark]{\label{wm-example} {\includegraphics[scale=0.1]{Pictures/watermark_examples/img_356_face-wm.png}}}
\hfill
\subfloat[25\% visible]{\label{hwm-25-example} {\includegraphics[scale=0.1]{Pictures/watermark_examples/img_356_face-hwm-25.png}}}%
\hfill
\subfloat[1\% visible]{\label{hwm-1-example} {\includegraphics[scale=0.1]{Pictures/watermark_examples/img_356_face-hwm-1.png}}}%
\caption{(\protect\subref{wm-example}) shows an example of the visible watermark. (\protect\subref{hwm-25-example}) illustrates the shape of the hidden watermark. (\protect\subref{hwm-1-example}) shows the actual hidden watermark used for testing.}
\label{fig:watermark_examples}
\end{figure}

\paragraph{MIA Performance for Different Guidance Scales} can be seen on \cref{fig:guidance_auc}, the performance of a MIA in our case is sensitive to the guidance scale used when generating training positives. The AUC score achieved by $\mathbf{M_A}$ is highest when the guidance scale is between 4 and 12. Looking at \cref{fig:guidance_scale_examples}, we see that even with a guidance scale of $s=0$, $\mathbf{M_T^{DTU}}$ still generates a headshot, despite having no guidance of what to generate. This indicates that fine-tuning $\mathbf{M_T^{DTU}}$ for 400 epochs has introduced enough bias that it assumes noise to stem from images of headshots. For $s=16$ visible artifacts appear, distorting the face (\cref{cfg-16-example}) which might also explain the drop in AUC score. It is also notable that for $s=0$, $\mathbf{M_T^{DTU}}$ achieves an AUC of $\sim0.7$ even though it is trained on positives generated without any guidance, which demonstrates that the target model $\mathbf{M_T^{DTU}}$ trained on 400 epochs leaks information of its training data even when not conditioned on a prompt.

\begin{figure}[h!t]
    \centering
    \scalebox{0.25}{\input{Graphs/guidance_scale.pgf}}
    \caption{AUC scores for the test of $\mathbf{M_A}$ where the training positives have been generated with different guidance scales. The FID score compared to the original DTU data is also shown, a higher AUC score correlates with a lower FID score. This is consistent with \cite{Carlini2023ExtractingTD}.}
    \label{fig:guidance_auc}
\end{figure}

\makeatletter
\setlength{\@fptop}{0pt}
\setlength{\@fpbot}{0pt plus 1fil}
\makeatother

\begin{figure}[h!t]
\centering
\subfloat[$s=0$]{\label{cfg-0-example} {\includegraphics[scale=0.075]{Pictures/guidance_scale_examples/headshot-0-0-0.png}}}% The "%" masks the line break.
\hfill
\subfloat[$s=4$]{\label{cfg-4-example} {\includegraphics[scale=0.075]{Pictures/guidance_scale_examples/headshot-4-0-0.png}}}%
\hfill
\subfloat[$s=8$]{\label{cfg-8-example} {\includegraphics[scale=0.075]{Pictures/guidance_scale_examples/headshot-8-0-0.png}}}%
\hfill
\subfloat[$s=12$]{\label{cfg-12-example} {\includegraphics[scale=0.075]{Pictures/guidance_scale_examples/headshot-12-0-0.png}}}%
\hfill
\subfloat[$s=16$]{\label{cfg-16-example} {\includegraphics[scale=0.075]{Pictures/guidance_scale_examples/headshot-16-0-0.png}}}%
\caption{Examples of images generated by $\mathbf{M_T^{DTU}}$ using different guidance scales.}
\label{fig:guidance_scale_examples}
\end{figure}
