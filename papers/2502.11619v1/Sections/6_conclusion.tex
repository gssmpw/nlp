It is concluded that it is possible to perform \acrlong{MIA}s on \acrlong{LDM}s fine-tuned on face images. The attack proposed is restricted to the task of inferring membership on a dataset level, and is not successful in the task of inferring which specific images of a dataset are used for fine-tuning a target model.\\
The circumstances or prerequisites for the cases where the MIA is successful are investigated and multiple factors are considered in relation to its performance. It is found that using a generated auxiliary dataset leads to a significantly better performance for the MIA than using real images. Diluting the members of interest by mixing two datasets in the fine-tuning of the target model did not lead to significantly worse performance of the MIA. It should however be noted that this was only tested by reducing the share of target members to 1:2. \\
It is found that by introducing visible watermarks to the target dataset, our MIA sees a significant boost in performance. Using hidden watermarks was not found to have a positive impact on the performance of the MIA. No significant effect was found when investigating the influence of the relationship between the labels used for fine-tuning the target model and the prompt used for inference, i.e. whether they match or not. Upon investigating the importance of the guidance scale used by the target model, it is found to have a significant influence on the performance of our MIA, with best performance at $s\sim8$.\\ 
Overall the proposed MIA is a realistic and feasible attack in a real-life application. However, it is computationally expensive to fine-tune a generative "shadow model" for the task of producing an auxiliary dataset related to the domain of interest as well as training the Resnet-18 attack model. The nature of the tests performed restricts our conclusion to the case of LDMs fine-tuned on face images. The smallest amount of fine-tuning that was still found to be effective was 50 epochs on the member images (however it could be lower - as it was not tested). The only LDM used for testing was Stable-Diffusion-v1.5 \cite{rombach2022highresolution}, which limits the generalizability of the conclusions drawn. The approach using a Resnet-18 as an attack model is found to be generally stable on several different hyperparameters in the target LDM. In conclusion, the method for \acrlong{MIA} shown in this paper is realistic and could be used as a tool to infer if one's face images have been used to fine-tune a \acrlong{LDM} in a black-box setup.