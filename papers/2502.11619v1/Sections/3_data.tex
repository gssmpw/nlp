The data used can be divided into two groups. The data used to fine-tune the target model, and the data used to train the attack model.

The focus of this paper is face-images. An image dataset is needed for the experiments and the images should not be contained in LAION-5B\footnote{The images should not be contained in LAION-5B as it was used to train the target model (Stable Diffusion v1.5) and a portion of the image dataset should be used as non-members}. Data from two universities are chosen which have publicly available images of their employees on their websites. The universities are the \acrfull{DTU} and \acrfull{AAU}.

\subsection{Data Source for the Target Model} \label{sec:data_source_for_target_model}
To fine-tune the target model with face-images three different face datasets were considered:
\begin{itemize}
    \item $\mathbf{D_{DTU}}$: Images scraped from DTU orbit.
    \item $\mathbf{D_{AAU}}$: Images scraped from AAU vbn.
    \item $\mathbf{D_{LFW}}$: Labeled Faces in the Wild \cite{LFWTech}. It contains 9,452 images.
\end{itemize}

After collection, the two image data sets $\mathbf{D_{DTU}}$ and $\mathbf{D_{AAU}}$ were partitioned into two subsets, thus producing four different datasets: $\mathbf{D_{DTU}^{seen}}$, $\mathbf{D_{DTU}^{unseen}}$, $\mathbf{D_{AAU}^{seen}}$, and $\mathbf{D_{AAU}^{unseen}}$. The 'seen' datasets are used to fine-tune the target model while the 'unseen' datasets are only used to test/train the attack model. Two more datasets are created which are copied from $\mathbf{D_{DTU}^{seen}}$. One set of the images gets a visible DTU watermark in the top right corner, and the other set gets overlayed with an almost invisible DTU logo, this is shown in \cref{fig:watermark_examples}. On \cref{tab:datasets_description_target} there is a description of all the variations of datasets used to fine-tune the target model.

\input{Tables/datasets_description_target}

\subsection{Data Source for the Attack Model} \label{sec:data_source_for_attack_model}
As mentioned in \cref{sec:data_source_for_target_model} $\mathbf{D_{DTU}^{unseen}}$ and $\mathbf{D_{AAU}^{unseen}}$ are not used to fine-tune the target model, this way they can be used to test or train the attack model with the certainty that there is no data leakage into the generated images. The output of the target models are used in the training of the attack model. On table \cref{tab:datasets_description_attack} there is a description of all the different image-sets used for training / testing the attack model in different experiments. On \cref{fig:attack_model_image_example} examples can be seen of the generated images from the target models.

\input{Tables/datasets_description_attack}


\begin{figure}[h!t]
\centering
\subfloat[$\mathbf{D_{DTU}^{gen}}$]{\label{attack_model_image_example-a}{\includegraphics[scale=0.065]{Pictures/headshot-10-16_dtu-gen_.png}}}%
\hfill
\subfloat[$\mathbf{D_{AAU}^{gen}}$]{\label{attack_model_image_example-b}{\includegraphics[scale=0.065]{Pictures/headshot-3-13_aau-gen_.png}}}
\hfill
\subfloat[\hwmgen]{\label{attack_model_image_example-c}{\includegraphics[scale=0.065]{Pictures/headshot-2-4_hwm-gen_.png}}}
\hfill
\subfloat[\wmgen]{\label{attack_model_image_example-d}{\includegraphics[scale=0.065]{Pictures/headshot-10-9_wm-gen_.png}}}
\hfill
\subfloat[$\mathbf{D_{DTU+LFW}^{gen}}$]{\label{attack_model_image_example-e}{\includegraphics[scale=0.065]{Pictures/headshot-36-15_lfw+dtu-gen_.png}}}
\hfill
\subfloat[$\mathbf{D_{NFT}^{gen}}$]{\label{attack_model_image_example-f}{\includegraphics[scale=0.065]{Pictures/headshot-44-15_gen_.png}}}
\caption{These are examples of the data in the different generated image datasets which are used to train the attack model.}
\label{fig:attack_model_image_example}
\end{figure}
