Generative image models such as OpenAI's DALLÂ·E or Stability AI's Stable Diffusion have advanced rapidly and seen a great rise in popularity over the last few years. To train models like these, millions of images are needed leading to the requirement of huge image datasets. 

This need has led to image generation models being trained on images without the needed consent or necessary permissions. As a consequence - besides the violation of the ownership of images - image generation models have been able to copy the style of artists and generate images in the likeness of others without the permission to do so. Such infringements affecting both individuals and organisations can be difficult to prove, as it requires knowledge of the images used to train the generative model.

The aim of this paper and research is to investigate \acrlong{MIA}s (MIA) on \acrlong{LDM}s (LDM) \cite{rombach2022highresolution}. To scope the project only Stable Diffusion v1.5 \cite{rombach2022highresolution} fine-tuned on face images is considered. Being able to infer if an image was part of a dataset used for training a generative model would considerably help individuals and organisations who suspect that their images have been unrightfully used. As mentioned in \cite{Dubinski2023TowardsMR}, evaluating MIA against a fine-tuned model is a potential pitfall\footnote{The reason it is seen as a pitfall is that fine-tuning a model easily leads to over-fitting to an image dataset resulting in higher accuracy on predicting member images. This is also shown in \cite{Carlini2023ExtractingTD}.}. This paper intentionally focuses on fine-tuned models as they are commonly used in real-life applications. It should be kept in mind that the results presented do not apply to non-fine-tuned models. 

\subsection{Definition of the Target model} \label{sec:intro_def_target_model}
In this paper, 'Target Model' $\mathbf{M_T}$  will be used to denote the model which the MIA is performed against. The model $\mathbf{M_T}$ is in this paper characterised by its ability to turn text, $\mathcal{T}$, into some $H\times W$-dimensional image with 3 RGB colour channels, i.e. $\mathbb{R}^{(H,W,3)}$.
\begin{equation}
    \mathbf{M_T} : \mathcal{T} \rightarrow \mathbb{R}^{(H,W,3)}
\end{equation}
This is the model for which it is desired to infer whether an image belongs to its training set $\mathbf{D_{Target}}$. Throughout the paper, only a black-box setup will be considered. This means that the target model can only be used as intended, i.e. providing textual input to generate output. No additional knowledge of training data, model weights, and etc. is available.

\input{Figures/adversial_finetuning_model}
The Target model $\mathbf{M_T}$ could be produced as shown on \cref{fig:adversial_finetuning_model} where a LDM (such as Stable Diffusion) is fine-tuned on a dataset to produce very domain-specific images which imitate the dataset. This could be images that are in the likeness of a specific artist's style or a group of people.

The Adversarial LDM will have obtained its high quality generation abilities from the pre-training on some image dataset (such as LAION-5B) making it able to generalise and mix styles with the images from $\mathbf{D_{Target}}$. It could be difficult to infer membership when a large image generation model contains information from hundreds of millions of images. However, as the model fine-tunes on a smaller set of images, $\mathbf{D_{Target}}$, it would enhance the information leakage from $\mathbf{D_{Target}}$ and it could be possible to infer whether the images have been used in the fine-tuning or not. 

\subsection{Definition of the Attack model}\label{sec:intro_def_attack_model}
The 'Attack Model' $\mathbf{M_A}$  denotes the model trained to infer the membership of a query image in relation to the target model's training set. It is characterised by taking some image $\mathbb{R}^{(H,W,3)}$ and translating it to a value $\mathcal{P}$ between 0 and 1 expressing the predicted probability of the image being part of $\mathbf{D_{Target}}$.

\begin{equation}
    \mathbf{M_A} : \mathbb{R}^{(H,W,3)} \rightarrow \mathcal{P}
\end{equation}

The attack model will be trained using supervised learning. To create the training set for the attack model, the training positives will be obtained by using the target model to generate images. This is done under the assumption that the target model's output leaks information of the training data. This assumption is required as the goal is to make the attack model learn to recognise the data distribution of the target model's training set and not its generated output.

\input{Figures/basic_attack_model_training}

The training negatives are obtained from an 'auxiliary dataset'. The purpose of the images in this dataset is to represent the same or a similar domain as the one the target model has been trained on. The auxiliary dataset should only contain images that the target model has not trained on, so it can be used as training negatives for the attack model. The training process of the attack model can be seen in Figure \ref{fig:basic_attack_model_training}.
