\section{Experiments}
\label{sec:experiments}

We evaluate \ourmethod{} over a diverse suite of eight fault localization benchmarks. This includes Defects4J \citep{defects4j}, the most popular fault localization benchmark, as well as seven additional benchmarks covering three general bug types.
% using the code debugging tasks in \dataset{}.
%We do so over the \dataset{}, consisting of code samples from three languages, C, Java, and Python, and containing three major categories of bugs.
In the rest of this section, we first introduce the datasets, then the baseline methods, and finally the results of each experiment. We answer the following research questions in this section:

\begin{description}[topsep=0pt, itemsep=0pt]
    \item[RQ1] How effective is \ourmethod{} at FL in diverse scenarios?
    \item[RQ2] How does the efficiency of \ourmethod{} compare to baselines?
    \item[RQ3] Can \ourmethod{} effectively localize multi-line bugs?
    \item[RQ4] How does the generalization ability of \ourmethod{} compare to zero-shot prompting of the base model?
\end{description}

% \subsection{Benchmark Suite}

% \subsubsection{Datasets}
% \begin{itemize}
%     \item Datasets: Python syntax \citep{yasunaga2021break}, C syntax \citep{gupta2017deepfix}, Python SStuB \citep{richter2022tssb}, Java SStuB \citep{karampatsis2020often}, Java Vulnerability \citep{}, C++ Vulnerability \citep{}.
% \end{itemize}



% In our experiments, we consider three classes of bugs and multiple languages within each bug class.

% \textbf{Syntax Errors} We focus on syntax errors for Python and C. For Python syntax errors, we use the GitHub-Python (GitHub) dataset from \citet{yasunaga2021break} consisting of Python functions scraped from Github with and without real syntax errors. For C syntax errors, we use the DeepFix dataset from \citet{gupta2017deepfix} consisting of C functions submitted to an introductory programming course where some have syntax errors. We further categorize syntax errors into the categories of mismatched parentheses, mismatched brackets, mismatched curly-brackets, Python-specific syntax (indentation, print statements, etc.), and C-specific syntax (semicolons).

% \adam{Show the original distribution to explain why we arrived at this distribution.}

% \textbf{Simple, Stupid Bugs} We focus on simple, stupid bugs (SStuBs) in Python and Java. For Python, we use the recent TSSB-3M dataset consisting of single line code changes scraped from GitHub. \adam{Provide further details on how we extracted this dataset}. For Java SStuBs, we use the ManySStuBs4J dataset \citep{karampatsis2020often}. The bugs in these datasets are categorized into 20 types of SStuBs, and we choose to subset to the following 3: \adam{Say which SStuB types we use}.

% \textbf{Security Vulnerabilities} We look at two datasets consisting of synthetic security vulnerabilities corresponding to common CWEs in Java and C++. For Java, we use the OWASP \citep{owasp} and Juliet-Java dataset \citep{juliet-java}. For C++ vulnerabilities, we use the Juliet-C++ dataset \citep{juliet-cpp}. These datasets consist of hundreds of different CWEs, but we focus on a subset consisting of the following 5: \adam{say which CWEs we consider}.


% \aaditya{Describe the building of the s3bugs benchmark}

% \aaditya{For syntax, for C errors, assignments were separate in training and testing datasets.}

% \aaditya{In SSTUBS, removed simple refactoring changes since we don't consider refactoring as bugs (TSSUB), mentioned by the TSSB authors. Also get data from ManySStubs4j for java. mentuion the number of samples, mention number of bugs that aren't refactoring. Appendix: mention issues while downloading code. We also remove samples labelled "unknown" from TSSB.}

% \aaditya{OWASP and Juliet are synthetic. Names of variables can give away the vulnerability along with comments, so we remove comments and rename variables and function names. We also rename imports that refer to the dataset names.}
\subsection{Datasets}

We evaluate on eight datasets summarized below.

\begin{table}[t]
    \centering
    \small
    \caption{A summary of the datasets we use for our evaluation. For a further breakdown of buggy samples into categories, see our discussion in Appendix~\ref{app:datasets}.}
    \label{tab:datasets}
    \begin{tabular}{lrrrrr}
    \toprule
         & \multicolumn{2}{c}{\# Train} & \multicolumn{2}{c}{\# Test} & \\
         \cmidrule(lr){2-3} \cmidrule(lr){4-5}
         Dataset & Bug & Clean & Bug & Clean & $\mathbb{E}$[LoC]\\
         \midrule
         Defects4J v1.2.0 & 368 & 368 & 90 & 90 & 35.8\\
         Defects4J v3.0.1 & \na & \na & 437 & \na & 46.7\\
         GitHub-Py & 1323 & 1323 & 400 & 400 & 9.3\\
         GitHub-J & 1370 & 1370 & 460 & 460 & 19.3\\
         DeepFix & 1475 & 1475 & 365 & 365 & 26.3\\
         TSSB & 4085 & 3745 & 1104 & 1080 & 24.8\\
         ManySStuBs4J & 3821 & 3821 & 1093 & 1093 & 15.5\\
         Juliet-J & 4039 & 3061 & 1011 & 989 & 62.5\\
         Juliet-C & 3718 & 3697 & 966 & 939 & 44.7\\
    \bottomrule
    \end{tabular}
\end{table}

\begin{table*}[t]
    \centering
    \small
    \caption{Comparison of \ourmethod{} with existing FL methods across eight benchmarks. We evaluate line-level localization performance on a method-level context, measured by top-1 accuracy. From left-to-right: Defects4J v1.2.0, GitHub-Python, GitHub-Java, DeepFix, TSSB-3M, ManySStuBs4J, Juliet-Java, and Juliet-C. Error bars are provided in Appendix~\ref{app:results}.}
    \label{tab:defects4j}
        % \setlength{\tabcolsep}{12pt} % Adjust column spacing
        \begin{tabular}{lrrrrrrrr|r}
        \toprule
        \textbf{Method} & \textbf{D4J} & \textbf{GH-Py} & \textbf{GH-J} & \textbf{DeepFix} & \textbf{TSSB} & \textbf{MS4J} & \textbf{Juliet-J} & \textbf{Juliet-C} & \textbf{Avg.}\\
        \midrule
        Random & 0.144 & 0.100 & 0.134 & 0.038 & 0.069 & 0.124 & 0.025 & 0.058 & 0.087\\
        \midrule
        DeepFL & 0.144 & \na & \na & \na & \na & \na & \na & \na & 0.144 \\      
        SmartFL & 0.158 & \na & \na & \na & \na & \na & \na & \na & 0.158 \\
        TRANSFER-FL & 0.218 & \na & \na & \na & \na & \na & \na & \na & 0.218 \\
        \midrule
        CodeLlama-70B & 0.212 & 0.145 & 0.316 & 0.084 & 0.077 & 0.169 & 0.038 & 0.095 & 0.142\\
        Llama-3.3-70B & 0.269 & 0.225 & 0.272 & 0.092 & 0.114 & 0.211 & 0.072 & 0.040 & 0.162\\
        Qwen2.5-72B & 0.157 & 0.333 & 0.289 & 0.124 & 0.088 & 0.194 & 0.061 & 0.040 & 0.161\\
        DeepSeek-R1-Distill-Llama-70B & 0.221 & 0.188 & 0.218 & 0.035 & 0.138 & 0.185 & 0.041 & 0.025 & 0.131\\
        GPT-4o & 0.249 & 0.375 & 0.365 & 0.097 & 0.089 & 0.240 & 0.009 & 0.026 & 0.181\\
        \midrule
        Linear Probe Llama-3.2-11B & 0.279 & 0.373 & 0.300 & 0.140 & 0.202 & 0.235 & 0.048 & 0.043 & 0.202\\
        % LLMAO-CodeGen & 0.223 & \na & \na & \na & \na & \na & \na & \na\\
        LLMAO-Llama-3.2-11B & 0.144 & 0.190 & 0.188 & 0.078 & 0.118 & 0.116 & 0.063 & \underline{0.113} & 0.126\\
        WELL-CodeBERT & 0.090 & \textbf{0.575} & \underline{0.532} & 0.129 & 0.094 & 0.111 & \textbf{0.216} & 0.059 & 0.226\\
        WELL-Llama-3.2-11B & 0.236 & 0.028 & 0.139 & 0.000 & 0.054 & 0.081 & 0.000 & 0.000 & 0.067\\
        GridLoc-Llama-3.2-11B & \underline{0.291} & 0.498 & 0.206 & \underline{0.332} & \textbf{0.262} & \textbf{0.339} & \underline{0.158} & 0.039 & \underline{0.266}\\
        \midrule
        \ourmethod{}-Llama-3.2-11B & \textbf{0.334} & \textbf{0.575} & \textbf{0.568} & \textbf{0.481} & \underline{0.237} & \underline{0.291} & 0.096 & \textbf{0.217} & \textbf{0.350}\\
        \bottomrule
        \end{tabular}
\end{table*}

\textbf{Defects4J}
Bugs along with commits to fix the bug.
We use two versions of the dataset: the standard Defects4J v1.2.0 \citep{defects4j} dataset containing 395 bugs, and the additional 543 bugs from Defects4J v3.0.1 released in November 2024 which we use as a stronger evaluation of generalization. In both versions, we split each buggy file into a set of buggy methods and evaluate on the method level following \citet{wu2023large}.

% \textbf{Additional Fault Localization Suite}
% For a more diverse evaluation, we additionally gather an evaluation suite consisting of over 50k samples from seven established bug detection and fault localization datasets. We categorize the datasets within the suite into those for syntax errors, those for semantic errors, and those for security vulnerabilities. A detailed analysis of the contents of the suite is provided in Appendix~\ref{app:datasets}.
\textbf{GithHub-Python} \citep{yasunaga2021break} and \textbf{GithHub-Java} \citep{santos2018syntax} consist of code mined from GitHub with syntax errors in Python and Java respectively.

\textbf{DeepFix} Real C programs written by students, some containing beginner syntax mistakes \citep{gupta2017deepfix}.

\textbf{TSSB-3M} ``Simple, stupid bugs'' (SStuBs) in Python which are mined from GitHub and categorized \citep{richter2022tssb}. Despite their name, these bugs are extremely challenging for humans to localize.

\textbf{ManySStuBs4J} Java SStuBs \citep{karampatsis2020often}.

\textbf{Juliet-Java} \citep{juliet-java} and \textbf{Juliet-C} \citep{juliet-cpp} consist of synthetic code corresponding to Common Weakness Enumerations (CWEs). We rename variables and function names to remove indicators of the vulnerability. We also remove comments and rename imports that refer to the dataset names. In total, we consider 89 CWEs from both the Juliet datasets.

% \adam{Merge the following into the above headings}
% \textbf{Syntax}
% For syntax errors, we focus on Python, Java, and C using the GitHub-Python dataset from \citet{yasunaga2021break}, the DeepFix dataset from \citet{gupta2017deepfix}, and finally the GitHub-Java dataset from \citet{santos2018syntax}.
% % \textbf{Syntax Errors.}
% %     We focus on syntax errors for Python, Java, and C. For Python, we use the GitHub-Python (GitHub) dataset from \citet{yasunaga2021break} consisting of Python functions scraped from Github with and without real syntax errors. For C, we use the DeepFix dataset from \citet{gupta2017deepfix} consisting of C functions submitted to an introductory programming course where some have syntax errors.
% %     For Java, we use the dataset from \citet{santos2018syntax} consisting of Java functions scraped from GitHub and categorized into code with and without syntax errors.
% %     We further categorize syntax errors into the categories of mismatched parentheses, mismatched brackets, mismatched curly-brackets, Python-specific syntax (indentation, print statements, etc.), Java-specific syntax (misspelled keywords, etc.), and C-specific syntax (undeclared variables, etc.).

% \textbf{SStuBs}
% For semantic errors, we focus on ``simple, stupid bugs" (SStuBs) in Python and Java using the TSSB-3M dataset \citep{richter2022tssb} and the ManySStuBs4J dataset \citep{karampatsis2020often}.

% % \textbf{Semantic Errors.}
% %     We focus on simple, stupid bugs (SStuBs) in Python and Java. For Python, we use the recent TSSB-3M dataset consisting of single line code changes scraped from GitHub.
% %     % \adam{Provide further details on how we extracted this dataset}.
% %     For Java SStuBs, we use the ManySStuBs4J dataset \citep{karampatsis2020often}. The bugs in these datasets are categorized into 20 types of SStuBs. Following the approach of \citet{richter2022tssb}, we ignore single line changes which only change a string literal since these may be just refactorings rather than bug fixes. Furthermore, $\sim$ 60\% of the TSSB-3M dataset samples are not attributed to any SStuBs category, so we exclude such samples. We thus remove such samples from both datasets. 
% %     % , and we choose to subset to the following 3: \adam{Say which SStuB types we use}.

% \textbf{CWEs}
% For vulnerabilities, we use the Juliet-Java \citep{juliet-java} and Juliet-C \citep{juliet-cpp} datasets consisting of synthetic vulnerabilities corresponding to common CWEs. We rename variables and function names to remove indicators of the vulnerability. We also remove comments and rename imports that refer to the dataset names. In total, we consider 89 CWEs from the Juliet datasets.

% \textbf{Security Vulnerabilities.}
%     We look at two datasets consisting of synthetic security vulnerabilities corresponding to common CWEs in Java and C. For Java, we use the OWASP \citep{owasp} and Juliet-Java dataset \citep{juliet-java}. For C vulnerabilities, we use the Juliet-C dataset \citep{juliet-cpp}. In each dataset, we rename variables and function names to remove indicators of the vulnerability. We also remove comments and rename imports that refer to the dataset names. In total, we consider 11 CWEs from the OWASP and 89 CWEs from the Juliet datasets. We balance the train and test sets by reducing the occurrence of some of the highly frequent CWEs to get an average of 83.3 training and 21.4 testing samples per CWE for the Juliet C dataset, 94.1 training and 23.5 testing samples per CWE for the Juliet Java dataset, and 102.5 training and 26.2 testing samples per CWE for the OWASP dataset.

The datasets are summarized in Table~\ref{tab:datasets} and we provide a detailed breakdown of the datasets in Appendix~\ref{app:datasets}.


% \subsection{Models}
% We consider Llama 3.3 70B \citep{llama3} and Qwen 2.5 72B \citep{qwen2} as the main open-weights LLMs and we use GPT-4o model \citep{gpt4o} as the representative proprietary LLM for zero-shot prompting experiments. Across all datasets, we build our probe on hidden representations from the penultimate layer of Llama 3.2 11B \citep{llama3}, one of the highest performing open-weights models of its size.
% For model size ablations, we utilize the Llama 3.2 family of models which contains 1B, 3B, 11B, and 90B LLMs.

\subsection{Baselines}

We group the baselines into three types: traditional FL methods that require code execution, LLM prompting of different models, and LLM probing/training.


\textbf{Traditional FL Methods}
For methods requiring code execution, we compare with DeepFL \citep{deepfl}, SmartFL \citep{smartfl}, and TRANSFER-FL \citep{transferfl} on Defects4J since these are the best performing traditional FL methods. Results for DeepFL and Transfer-FL are from \citet{llmao}, and we cite SmartFL results directly from \citet{smartfl}. We can not evaluate these benchmarks on the other datasets since they do not provide tests.

\textbf{Prompting Methods}
For models, we use a diverse set of four open-weights LLMs of size $\sim$70B parameters as well as a proprietary model. We consider Llama 3.3 70B \citep{llama3}, Qwen 2.5 72B \citep{qwen2}, CodeLlama 70B \citep{codellama}, and DeepSeek-R1-Distill-Llama-70B \citep{r1} as the main open-weights LLMs and we use GPT-4o \citep{gpt4o} as the representative proprietary LLM for prompting experiments. Llama 3.3 70B and Qwen 2.5 72B are LLMs pretrained on a diverse dataset of natural language and code while CodeLlama 70B additionally trained on primarily code data \citep{codellama}. DeepSeek-R1-Distill-Llama-70B is a ``reasoning'' model in that it can use longer chain-of-thought output to solve more complex problems \citep{r1}.

We experimented with several prompts based on that used by \citet{wu2023large}. Our prompt
asks for the buggy line text as well as the line number in case the LLM cannot count lines. We use this prompt with temperature 0 sampling for all models.
% To evaluate \ourmethod{}, we consider baselines from the probing literature as well as recent work on interpreting and aligning LLMs.
% First, for each LLM, we finetune the entire model for each task, as well as finetune using LoRA~\citep{hu2021lora} to use as baselines.
% We also few-shot prompt each LLM by providing the code sample along with asking a yes/no question corresponding to the task.
% For localization, we additionally ask for the line number of the bug. 
The prompt is provided in Appendix~\ref{app:prompts}.

\textbf{Probing Methods}
Finally, for LLM probing, we consider the following baselines:
\begin{itemize}[itemsep=0.0pt, topsep=0pt]
    \item Linear Probing: performing logistic regression on the last token representation from each code sample to predict if the code is buggy or not. Following \citet{repe}, we apply the trained classifier to each token in the sequence for getting token-level scores and then we derive line rankings the same way as \ourmethod{}.
    \item GridLoc \citep{niu2022does}: the only other attention probing method we are aware of, which uses an RNN to learn attention weights. We apply this method to learning the bug detection task from the dataset $\mathcal{H}_\text{Det}$, and interpret the resulting attention weights for FL using our line-aggregation method.
    \item LLMAO \citep{llmao}: trains an adapter using strong FL supervision. This method was designed for the CodeGen \citep{nijkamp2023codegen} models, and we used the largest available CodeGen model. We additionally adapted their code for the Llama-3 family of models and report results on Llama-3.2-11B.
    \item WELL \citep{well}: the first method to consider learning FL without strong supervision. This method finetunes a bidirectional LLM (CodeBERT \citep{codebert}) for bug detection and then interprets the attention weights of the last layer for fault localization. We additionally report results on Llama-3.2-11B, even though it has exclusively causal attention.
\end{itemize}
% we consider linear probing on the last token \cite{repe}, GridLoc \citep{niu2022does}, the only other attention probing method we are aware of, which uses an RNN to learn attention weights, 
% LLMAO \citep{llmao} which trains an adapter using direct line-level localization supervision on the CodeBERT LLM \citep{codebert}. We train the linear probe and GridLoc on the hidden representations from the penultimate layer of Llama 3.2 11B. As \ourmethod{} is also a probing technique, we use the same hidden representations from Llama 3.2 11B for fair comparison.


% \begin{itemize}[leftmargin=*]
%     \item Linear probe \citep{alain2016understanding}: For linear probing, we use logistic regression on the final token representation. We consider two kinds of linear probes: linear last-token probing (LL-probing) which probes the last token's representation, and linear mean probing (LM-probing) which probes the mean of the token representations.
%     % \item Non-linear probe (NL-Probe): For the non-linear probe, we use a two-layer neural network with a hidden layer of size 100 and a ReLU activation function, and optimize with a binary cross-entropy loss. The input to the probe is the last token representation from the LLM.
%     % \aaditya{Specify what is being probed and why}
%     % \item Mass-Mean probe (MM-Probe) \citep{marks2023geometry}: MM-probing aims to solve generalization problems of logistic regression. It quantifies the direction between the centroids of buggy and non-buggy code representations and classifies new samples by their dot product with the direction separating the two clusters. This method probes the last token representation in the code sequence.
%     % \aaditya{Give a 1-line description}
%     % \item RepE \citep{repe}: This method prompts the LLM with the particular relevant concept, such as syntax or security vulnerabilities, and then performs PCA on the difference between contrasting pairs of samples (a sample with a bug minus a sample without a bug).
%     \item GridLoc \citep{niu2022does}: This method learns attention weights over the token representation sequence using an RNN, performs a weighted sum of the token representations and then classifies the pooled representation with a two-layer neural network.
%     % \aaditya{improve 1-line description}
%     % \item Few-shot prompting: We few-shot prompt each LLM by providing the code sample along with asking a yes/no question corresponding to the task. For localization, we additionally ask for the line number of the bug. Prompts are given in Appendix~\ref{app:prompts}.
%     % \item Full-model finetuning
%     % \item LoRA \citep{hu2021lora}
% \end{itemize}

% \adam{We can add different prompting-based methods such as chain-of-thought, and use GPT-4 to show a top-line for prompting.}

% \aaditya{I think we should split this into two parts: 1. Finetuning techniques, 2. Probing techniques. 1 can be merged with the intro paragraph, and 2 can be its own paragraph.}

% \adam{Should we also look into the Defects4J dataset and measure bug localization accuracy?}

% \adam{If we include the other layernorm in our method, then we can point out how the baselines don't generalize well.}

% \adam{We do not consider ... Explain which baselines are not used.}

% \mayur{I hope we have finetuning baselines, both LORA and Full?}


% \adam{Can use the subset of CWEs that are we can judge to evaluate the localization probe.}

% \begin{itemize}
%     \item For the three bug subtypes, show a bar chart of accuracy for each method and each bug category. Figure~\ref{fig:iid_detection}
% \end{itemize}

\begin{comment}
\subsection{Bug Detection}
\begin{figure*}
    \centering
    \includegraphics{figures/pareto.pdf}
    % \centering
    % \begin{subfigure}[b]{0.4545\linewidth}
    %     \centering
    %     \includegraphics{figures/syntax_pareto.pdf}
    %     \caption{Syntax}
    % \end{subfigure}%
    % \begin{subfigure}[b]{0.2727\linewidth}
    %     \centering
    %     \includegraphics{figures/sstubs_pareto.pdf}
    %     \caption{Simple stupid bugs}
    % \end{subfigure}%
    % \begin{subfigure}[b]{0.2727\linewidth}
    %     \centering
    %     % \includegraphics[width=\linewidth]{figures/sstub_detection.pdf}
    %     \caption{Security Vulnerabilities}
    % \end{subfigure}
    % \includegraphics{figures/all_detection.pdf}
    \caption{Bug detection and localization performance for the three classes of bugs in different programming languages. Methods more towards the upper right corner are better.}
    \label{fig:iid_detection}
    % \vspace{-0.15in}
\end{figure*}

First, we evaluate the accuracy of \ourmethod{} and the baselines for detecting the three categories of bugs.
Each method is required to classify whether a bug is present given a snippet of code as input.
The x-axis of Figure~\ref{fig:iid_detection} shows the average accuracy of each baseline for the domains of syntax, one-line bugs (SStuBs), and security vulnerabilities.
% , where an accuracy of 0.5 corresponds to random guessing. All methods perform above random chance, showing the power of the internal representation of CodeLLama-13B-Instruct for code understanding tasks.

We see that in general, for detection accuracy, \ourmethod{} is better or similar to the strongest baselines. For all domains except syntax bugs, \ourmethod{} even slightly exceeds the performance of full finetuning and surpasses LoRA by up to 13\% on average for vulnerability bugs. For syntax bugs, \ourmethod{} 
is outperformed by full finetuning and is on par with LoRA, but we see that \ourmethod{} outperforms the next best non-finetuning baselines in this domain by up to 3\% (see C syntax error detection in Table~\ref{tab:syntax-detection}).

% For the SStuBs datasets, \ourmethod{} has slightly lower detection accuracy than GridLoc, but we note that this difference is less than 1\% and not statistically significant as seen in Table~\ref{tab:sstubs-detection}.

% with the exception of the semantic bugs, where it is slightly outperformed by linear probing with mean-pooling.
% We see that across domains, the attention probe outperforms all baselines, except for the TSSB dataset where it is slightly outperformed by linear probing with mean-pooling.

% \aaditya{Can we change this to analyze attention probing more than linear probing?}
% Furthermore, L-probing always outperforms M-probing (red and green bars compared to the purple and brown bars), and NL probing with a two layer MLP does on-par with linear probing (orange bar compared to green bar). This suggests that most of the information available in the LLM's representation for these tasks is linearly represented, since otherwise we would expect the non-linear probe to outperform the linear probe. 

We also see that for all domains there is a gap between the attention-based methods (\ourmethod{} and GridLoc) and last-token or mean-token probing methods (L, NL, and MM). Since even a non-linear probe does not reach the performance of the attention-based probes, this gives evidence for the hypothesis that attention-based methods can select more relevant information from within the token sequence than is present in the last or mean-token representation.
Finally, 8-shot prompting performs significantly worse than probing methods in all cases. For both SStuBs and vulnerabilities, 8-shot prompting is only marginally better than random guessing.

% However, this is not the case when generalizing bugs over different languages, as we will show in the next section.
% Surprisingly, we show in the next section that similar performance for in-distribution accuracy does not correlate well with generalization performance.



% \begin{table}
%     \centering
%     \caption{Caption}
%     \label{tab:my_label}
%     \begin{tabular}{lrrrrrr}
%     \toprule
% Method & \multicolumn{2}{c}{Parentheses} & \multicolumn{2}{c}{Brackets} & \multicolumn{2}{c}{Braces} \\
% \midrule
% & IID & OOD & IID & OOD & IID & OOD \\
% Finetune & 0.98 & 0.60 & 0.98 & 0.54 & 0.98 & 0.60\\
% Linear (Last) & 0.94 & 0.64 & 0.92 & 0.69 & 0.95 & 0.61\\
% Attention & 0.97 & 0.67 & 0.97 & 0.65 & 0.98 & 0.63\\
% \bottomrule
%     \end{tabular}
% \end{table}
\end{comment}

\subsection{RQ1: FL Performance}




% \begin{table*}[t]
%     \centering
%     \tiny
%     \caption{Comparison of \ourmethod{} with existing fault localization methods on eight diverse datasets. We evaluate each method on line-level localization performance on the method-level and report top-1 and top-5 localization accuracy.}
%     \label{tab:defects4j}
%         % \setlength{\tabcolsep}{12pt} % Adjust column spacing
%         \begin{tabular}{lrrrrrrrrrrrrrrrr}
%         \toprule
%         & \multicolumn{2}{c}{\textbf{Defects4J v1.2.0}} & \multicolumn{2}{c}{\textbf{GitHub-Py}} & \multicolumn{2}{c}{\textbf{GitHub-J}} & \multicolumn{2}{c}{\textbf{DeepFix}} & \multicolumn{2}{c}{\textbf{TSSB}} & \multicolumn{2}{c}{\textbf{ManySStuBs4J}} & \multicolumn{2}{c}{\textbf{Juliet-J}} & \multicolumn{2}{c}{\textbf{Juliet-C}}\\
%         \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9} \cmidrule(lr){10-11} \cmidrule(lr){12-13} \cmidrule(lr){14-15} \cmidrule(lr){16-17}
%         \textbf{Method} & \textbf{Top-1} & \textbf{Top-5} & \textbf{Top-1} & \textbf{Top-5} & \textbf{Top-1} & \textbf{Top-5} & \textbf{Top-1} & \textbf{Top-5} & \textbf{Top-1} & \textbf{Top-5} & \textbf{Top-1} & \textbf{Top-5} & \textbf{Top-1} & \textbf{Top-5} & \textbf{Top-1} & \textbf{Top-5} \\ 
%         \midrule
%         Random & 0.144 & 0.454 & 0.100 & 0.625 & 0.134 & 0.541 & 0.038 & 0.170 & 0.069 & 0.319 & 0.124 & 0.494 & 0.025 & 0.162 & 0.058 & 0.276\\
%         \midrule
%         DeepFL & 0.144 & 0.342 & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA\\       
%         SmartFL & 0.158 & 0.323 & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA\\
%         TRANSFER-FL & 0.218 & 0.405 & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA\\
%         \midrule
%         CodeLlama-70B & 0.212 & 0.522 & 0.145 & 0.540 & 0.316 & 0.528 & 0.084 & 0.205 & 0.077 & 0.290 & 0.169 & 0.417 & 0.038 & 0.103 & 0.095 & 0.169\\
%         % Llama-3.2-11B & 0.140 & 0.356 & 0.429\\
%         Llama-3.3-70B & \underline{0.269} & 0.546 & 0.225 & 0.613 & 0.272 & 0.546 & 0.092 & 0.307 & 0.114 & 0.396 & 0.211 & 0.540 & 0.072 & 0.206 & 0.040 & 0.223\\
%         % Qwen2.5-Coder-14B & 0.159 & 0.463 & 0.508 & 29.2 \\
%         Qwen2.5-72B & 0.157 & 0.528 & 0.333 & 0.690 & 0.289 & 0.556 & 0.124 & 0.323 & 0.088 & 0.395 & 0.194 & 0.522 & 0.061 & 0.209 & 0.040 & 0.261\\
%         GPT-4o & 0.249 & 0.541 & 0.375 & 0.735 & 0.365 & 0.585 & 0.097 & 0.286 & 0.089 & 0.363 & 0.240 & 0.597 & 0.009 & 0.163 & 0.026 & 0.231\\
%         \midrule
%         LLMAO & 0.223 & 0.463 & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA\\
%         Linear Probe & 0.133 & 0.303 & 0.120 & 0.430 & 0.072 & 0.342 & 0.048 & 0.161 & 0.033 & 0.187 & 0.095 & 0.351 & 0.023 & 0.083 & 0.025 & 0.159\\
%         GridLoc & 0.203 & \underline{0.566} & 0.555 & 0.900 & 0.362 & 0.905 & 0.299 & 0.674 & 0.309 & 0.683 & 0.360 & 0.811 & 0.151 & 0.406 & 0.078 & 0.449\\
%         WELL & 0.090 & 0.515 & \underline{0.575} & 0.825 & 0.532 & 0.914 & 0.129 & 0.361 & 0.094 & 0.458 & 0.111 & 0.642 & 0.216 & 0.548 & 0.059 & 0.335\\
%         \midrule
%         % \ourmethod{}-Llama-3.2-11B & \textbf{0.334} & \textbf{0.576} & \textbf{0.575} & & \textbf{0.641} & & \textbf{0.437} & & 0.232 & & 0.285 & & \textbf{0.099} & & \textbf{0.142}\\
%         \ourmethod{}-Llama-3.2-11B & \textbf{0.334} & \textbf{0.576} & \textbf{0.580} & 0.897 & \textbf{0.552} & 0.859 & \textbf{0.450} & \textbf{0.733} & 0.235 & 0.566 & 0.300 & 0.735 & 0.052 & 0.353 & 0.025 & 0.342\\
%         \bottomrule
%         \end{tabular}
% \end{table*}

% \begin{table*}[t]
%     \centering
%     \tiny
%     \caption{Comparison of \ourmethod{} with existing machine learning fault localization methods on Defects4J. We evaluate each method on line-level localization performance on the method-level.}
%     \label{tab:defects4j}
%         % \setlength{\tabcolsep}{12pt} % Adjust column spacing
%         \begin{tabular}{lrrr}
%         \toprule
%         & \multicolumn{3}{c}{\textbf{Defects4J v1.2.0}}\\
%         \cmidrule(lr){2-4}
%         \textbf{Method} & \textbf{Top-1} & \textbf{Top-3} & \textbf{Top-5} \\ 
%         \midrule
%         Random & 0.144 & 0.373 & 0.454 \\
%         \midrule
%         DeepFL & 0.144 & 0.241 & 0.342\\       
%         SmartFL & 0.158 & 0.250 & 0.323 \\
%         TRANSFER-FL & 0.218 & 0.342 & 0.405 \\
%         \midrule
%         CodeLlama-70B & 0.212 & 0.417 & 0.522 \\
%         % Llama-3.2-11B & 0.140 & 0.356 & 0.429\\
%         Llama-3.3-70B & \underline{0.269} & 0.419 & 0.546 \\
%         % Qwen2.5-Coder-14B & 0.159 & 0.463 & 0.508 & 29.2 \\
%         Qwen2.5-72B & 0.157 & 0.360 & 0.528 \\
%         GPT-4o & 0.249 & 0.452 & 0.541 \\
%         \midrule
%         Linear Probe & 0.133 & 0.238 & 0.303 \\
%         GridLoc & 0.203 & \underline{0.459} & \underline{0.566} \\
%         LLMAO & 0.223 & 0.377 & 0.463\\
%         \midrule
%         \ourmethod{}-Llama-3.2-11B & \textbf{0.334} & \textbf{0.496} & \textbf{0.576}\\
%         \bottomrule
%         \end{tabular}
% \end{table*}

% \begin{table*}[t]
%     \centering
%     \caption{Comparison of \ourmethod{} with prompting and probing based fault localization. We evaluate each method on line-level localization performance on our suite of bugs.}
%     \label{tab:s3bugs}
%         % \setlength{\tabcolsep}{12pt} % Adjust column spacing
%         \begin{tabular}{lrrrrrrr}
%         \toprule
%         % & \multicolumn{7}{c}{\textbf{Top-1 Localization}} & \\
%         % \cmidrule(lr){2-8}
%          & \multicolumn{3}{c}{\textbf{Syntax}} & \multicolumn{2}{c}{\textbf{SStuBs}} & \multicolumn{2}{c}{\textbf{Vulnerabilities}}\\ 
%          \cmidrule(lr){2-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8}
%         \textbf{Method} & GitHub-Py & GitHub-J & DeepFix & TSSB & ManySStubs4J & Juliet-J & Juliet-C\\
%         \midrule
%         Random & 0.100 & 0.134 & 0.038 & 0.069 & 0.124 & 0.025 & 0.058\\
%         \midrule
%         % CodeLlama-13B & 0.107 & 0.402 & 0.484 & 34.2 \\
%         CodeLlama-70B & 0.145 & 0.316 & 0.084 & 0.077 & 0.169 & 0.038 & \underline{0.095}\\
%         Llama-3.3-70B & 0.225 & 0.272 & 0.092 & 0.114 & 0.211 & \underline{0.072} & 0.040\\
%         Qwen-2.5-72B & 0.333 & 0.289 & 0.124 & 0.088 & 0.194 & 0.061 & 0.040\\
%         % Qwen2.5-Coder-14B & 0.159 & 0.463 & 0.508 & 29.2 \\
%         GPT-4o & 0.375 & 0.365 & 0.097 & 0.089 & & &\\
%         \midrule
%         Linear Probe & 0.265 & 0.205 & \underline{0.255} & \underline{0.202} & 0.258 & 0.023 & 0.025\\
%         GridLoc & \underline{0.490} & \underline{0.380} & 0.038 & 0.161 & \textbf{0.360} & 0.017 &\\
%         % WELL & \underline{0.575} & \underline{0.532} & 0.094 & 0.111 & \textbf{0.216} & 0.059 &\\
%         \midrule
%         \ourmethod{}-Llama-3.2-11B & \textbf{0.575} & \textbf{0.641} & \textbf{0.437} & \textbf{0.232} & \underline{0.285} & \textbf{0.099} & \textbf{0.142}\\
%         \bottomrule
%         \end{tabular}
% \end{table*}


For FL, given a program fragment with bugs, each method ranks the lines of the input program based on the likeliness of the bug being located to that line. We note that our method was not trained with direct localization information and instead makes use of weak supervision even though baselines (such as LLMAO) use strong FL supervision. WELL is the only baseline which uses weak supervision, similar to our method \citep{well}.

% The y-axis of Figure~\ref{fig:iid_detection} shows the localization ranking accuracy for each domain. For all three domains, \ourmethod{} has the highest average localization performance and outperforms the baselines by up to 4\% MRR (see average MRR for Syntax). Within the syntax datasets, \ourmethod{} outperforms the baselines by up to 16\% MRR (see C syntax bugs in Table~\ref{tab:syntax-localization}. Additionally, \ourmethod{} is only outperformed by a baseline on the Java syntax data where GridLoc gets 0.70 MRR while \ourmethod{} gets 0.64 MRR, but we can see from Table~\ref{tab:syntax-localization} that this is not a significant difference.

% Figure~\ref{fig:motivation} shows three qualitative examples of the attention weights from \ourmethod{}. In examples (a) and (b), we can see \ourmethod{} correctly localizes the syntax errors and even assigns high attention to multiple real errors in both examples. Example (c) is from the Python SStuBs dataset and we can see that while the attention from \ourmethod{} is less specific than for the syntax errors, it highlights the token ``t" which in this case is a bug since line 4 should be ``\verb|if n not in d|''.

FL performance of \ourmethod{} and baselines on eight datasets is shown in Table~\ref{tab:defects4j}. To compare with existing work on Defects4J, we perform 10-fold cross validation using method-level context following \citep{wu2023large}. Results for DeepFL, TRANSFER-FL, and Smart-FL are from \citet{llmao} and \citet{smartfl}. Note that ``\na'' results indicate baselines which could not be evaluated due to requiring tests which were only available for Defects4J.

\ourmethod{} improves over the strongest baseline by 34.6\% top-1 accuracy on average across all eight datasets, in particular improving
on Defects4J v1.2.0 by 19.7\% and on Juliet-C by 128\% on top-1 accuracy.
% , and has consistent gains in accuracy as we observe more lines for top-5.
The second highest performing method is GridLoc, which is a probing method we adapted for this setting.

We find that training a classifier from weak supervision in the form of fault detection is not enough for strong fault localization. This is seen by the WELL baseline using Llama-3.2-11B performing worse than random guessing. We find that WELL with Llama-3.2-11B almost always predicts the first line of code, which rarely is buggy. This issue does not happen when using CodeBERT, a model with bidirectional attention.

We also observe that prompting LLMs of size $\sim$70B parameters outperforms traditional FL methods requiring code execution. These 70B LLMs have strong performance since the proprietary GPT-4o (rumored to be over 200B parameters) performs similar to Llama 3.3 70B. 
In addition, we find that DeepSeek-R1-Distill-Llama-70B performs significantly worse than Llama-3.3-70B, the model it was finetuned from. By observing many of the outputs from this model, the extensive chain-of-thought (over 1000 tokens on average) is overly cautious and concludes that even benign lines could be buggy.
We discuss the scaling of performance of prompting and probing with model size in Section~\ref{sec:efficiency}.



% We see that \ourmethod{} significantly outperforms the best baselines on the over datasets as well (up to 116\% improvement on DeepFix). On Java syntax bugs (GitHub-J), our method even reaches 64.1\% top-1 accuracy. \adam{I'm going to update this to put Defects4J on equal footing as other datasets.}

% \aaditya{Talk about why localization works for attention probing -- why it does not for finetuning -- why other techniques do not work as well.}

% \aaditya{Given code and presence of bug, pinpoint what line this bug occurs in.}

% \begin{figure}
%     \centering
%     \includegraphics{figures/localization_mrr.pdf}
%     \caption{\adam{Look at the attention weights of the finetuned model}. Bug line localization performance of probes trained without localization supervision.}
%     \label{fig:loc}
% \end{figure}

% \begin{table}[h]
%     \footnotesize
%     \centering
%     \begin{tabular}{lrrrrrrrrrr}
%     \toprule
%          &  \multicolumn{2}{c}{Python} & \multicolumn{2}{c}{Java} & \multicolumn{2}{c}{C} & \multicolumn{2}{c}{Python SStuBs} & \multicolumn{2}{c}{Java SStuBs}\\
%          \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9} \cmidrule(lr){10-11}
%          & MRR & ACC@1 & MRR & ACC@1 & MRR & ACC@1 & MRR & ACC@1 & MRR & ACC@1\\
%          \midrule
% Linear (Last) & 0.46 & 0.25 & 0.53 & 0.34 & 0.30 & 0.15 & & \\
% Linear (Mean) & 0.53 & 0.32 & 0.61 & 0.44 & 0.23 & 0.10 & & \\
% GridLoc & 0.79 & 0.68 & 0.72 & 0.58 & 0.58 & 0.43 & & \\
% Attention & 0.84 & 0.75 & 0.62 & 0.45 & 0.50 & 0.35 & & \\
%     \bottomrule
%     \end{tabular}
%     \caption{Caption}
%     \label{tab:my_label}
% \end{table}




\begin{comment}
\subsection{Bug Generalization}\label{sec:generalization}
% \begin{figure}
%     \centering
%     \begin{subfigure}{\textwidth}
%     \includegraphics{figures/generalization_heatmap_syntax.pdf}
%     \caption{Syntax}
%     \end{subfigure}
%     \begin{subfigure}{\textwidth}
%     \includegraphics[width=0.9\textwidth, height=0.2\textwidth]{example-image-a}
%     \caption{Simple, Stupid Bugs}
%     \end{subfigure}
%     \begin{subfigure}{\textwidth}
%     \includegraphics[width=0.9\textwidth, height=0.2\textwidth]{example-image-a}
%     \caption{Vulnerabilities}
%     \end{subfigure}
%     \caption{Bug detection generalization across language for each domain.}
%     \label{fig:generalization}
% \end{figure}
% \begin{itemize}
%     \item Show a bar chart for in-domain accuracy (such as parens) and out of domain accuracy (such as brackets).
%     \item Show the same for between programming languages.
% \end{itemize}
\begin{figure}
    \centering
    \begin{subfigure}{\linewidth}
    \includegraphics[width=\linewidth]{figures/generalization_syntax.pdf}
    \end{subfigure}
    \hfill
    \begin{subfigure}{\linewidth}
       \includegraphics[width=\linewidth]{figures/generalization_vulns.pdf} 
    \end{subfigure}
    \caption{Performance for learning language-independent concepts by training on examples from one language and testing on examples in other languages. On the left is shown the average in-distribution detection performance of language-agnostic syntax errors compared to out-of-distribution performance. On the right, we select three vulnerability classes shared between C and Java and show the performance for detecting these vulnerabilities in Java when trained only on C.}
    \label{fig:generalization}
    % \vspace{-0.15in}
\end{figure}

Figure~\ref{fig:generalization} shows the generalization performance of the probes trained on bugs from one language and tested on another language.
The left of Figure~\ref{fig:generalization} shows the in-distribution and out-of-distribution performance of probes on three classes of language-agnostic syntax errors. The vertical axis shows the testing dataset and the horizontal axis represents the training dataset Figure~\ref{fig:generalization}.
% While probes trained on bugs from one language mostly generalize to other languages, non-linear probes generalize better across languages than the linear probe.
We see that while \ourmethod{} outperforms GridLoc while detecting in-distribution syntax errors, GridLoc generalizes marginally better to other languages. Both \ourmethod{} and GridLoc, however, achieve the best balance of in-distribution and out-of-distribution accuracy compared to all other methods.

On the right of Figure~\ref{fig:generalization} is an evaluation of each method's performance in detecting three randomly selected classes of CWEs. These subclasses come from the CWE class hierarchy \citep{cwe}.
However, GridLoc's generalization over CWE vulnerabilities is significantly outperformed by \ourmethod{}.
This is because while syntax errors of a certain kind may look similar across languages, vulnerabilities can manifest in several different ways.
As such, in order to generalize the detection of a vulnerability, the probe must learn the underlying fundamental concept leading to the vulnerability.
For instance, CWE200 bugs expose sensitive information to unauthorized users which can take very different forms in different languages, as opposed to syntax errors like mismatched parenthesis errors. 

\end{comment}


\subsection{RQ2: Efficiency}
\begin{table}[h!]
\centering
\small
\caption{Resource efficiency across methods for localizing Defects4J bugs. We measure this through GPU overhead cost in gigabytes and the expected inference cost in FLOPs. }
\label{tab:memory_overhead}
\begin{tabular}{@{}lrrr@{}}
\toprule
\textbf{Model} & \textbf{Top-1} & \textbf{GPU Cost (GB)} & \textbf{$\mathbb{E}$[FLOPs]} \\ \midrule
Llama-3.2-90B  & 0.269 & 170.0 & \num{3.4e13} \\
CodeLlama-70B & 0.212 & 131.7 & \num{7.9e12} \\
Qwen2.5-72B& 0.157 & 138.9 & \num{4.2e12} \\
Llama-3.3-70B  & 0.269 & 154.0 & \num{3.4e13} \\
DeepSeek-R1DL-70B & 0.221 & 141.2 & \num{1.4e14}\\
% -Llama70B& & &}\\
\midrule
% \ourmethod{}-Llama-3.2-1B & \textbf{0.282} & $\mathbf{2.0 \times 10^{9}}$ \\
\ourmethod{}-Llama-3.2-1B & \underline{0.282} & \textbf{6.2} &\textbf{\num{2.0e9}} \\
\ourmethod{}-Llama-3.2-11B & \textbf{0.334} & \underline{24.2} & \underline{\num{2.2e10}} \\ \bottomrule
\end{tabular}
\end{table}

\label{sec:efficiency}
\begin{figure}[t]
    \centering

    \includegraphics[width=\linewidth]{figures/scale-frontier.pdf}
    \caption{Model scale versus Top-1 on Defects4J. Each point for \ourmethod{} is trained on the hidden representations from the Llama-3.2 model of the corresponding size.}
    \label{fig:scale}
\end{figure}
% \adam{Plot a pareto frontier of the GPU memory usage of baselines vs. their Defects4J top-1 accuracy. Also include the model size vs. top-1 accuracy plot in this section.}

We compare the size of the model which \ourmethod{} is trained on to the model's zero-shot fault localization capabilities in Figure~\ref{fig:scale}. We focus on top-1 accuracy for Defects4J since it is the most widely used benchmark for FL tools.
Figure~\ref{fig:scale} shows that \ourmethod{} trained via weak supervision on a 1B LLM outperforms zero-shot prompting of LLMs of size 70B or larger on Defects4J.
\ourmethod{} always outperforms its underlying model when zero-shot prompted across model sizes, but the performance gap shrinks as model size increases.

\ourmethod{} also displays a linear increase in performance with an exponential increase in model size while prompting results resemble ``emergent behavior'' \citep{wei2022emergent}.
To test the limits of \ourmethod{}s performance lead as model size continues to grow,
we scaled model size as far as practical with our resources, evaluated GPT-4o via API, and evaluated the DeepSeek-R1-Distill-Llama-70B model which leverages test-time scaling, but we never exceeded \ourmethod{}s performance through prompting. Surprisingly, the DeepSeek reasoning model even performed worse than Llama-3.3-70B, the model it was trained from.

% With even larger models, zero-shot performance may match performance of \ourmethod{}, but model scales significantly above 70B parameters exceed our resource bounds. Figure~\ref{fig:scale} shows that \ourmethod{} trained via weak supervision on a 1B LLM can outperform zero-shot prompting of a 70B LLM on Defects4J.
% We also perform a similar evaluation where we measure cost in terms of average time to produce a result in Appendix~\ref{} \adam{TODO}.


\subsection{RQ3: Localizing Multi-line Bugs}
Since our method produces a ranking for every line of the input, our method is better suited for directly finding multiple bugs at once, or multi-line bugs. Since the top-k accuracy metric only cares if at least one of the top $k$ predictions are correct, we additionally use the precision at $k$ (P@$k$) metric which measures the percent of true buggy lines in the top $k$ predictions. The exact formula is provided in Appendix~\ref{app:precision}. We compare \ourmethod{} and baselines on Defects4J v1.2.0 in terms of P@$k$ in Table~\ref{tab:precision_k}.
\begin{table}[h]
    \centering
    \small
    \caption{Precision@K for multi-line bugs. We evaluate on a subset of Defects4J where there are strictly two or more buggy lines present within each function.}
    \begin{tabular}{lccc}
        \toprule
        \textbf{Method} & \textbf{P@2} & \textbf{P@3} & \textbf{P@5} \\
        \midrule
        Random & 0.201 & 0.231 & 0.297 \\
        \midrule
        CodeLlama-70B & \underline{0.250} & 0.284 & 0.351 \\
        Llama-3.3-70B & 0.240 & 0.266 & 0.355 \\
        Qwen2.5-72B & 0.221 & 0.271 & 0.347 \\
        DeepSeek-R1-Distill-Llama-70B & 0.245 & 0.283 & 0.336 \\  
        GPT-4o & 0.218 & \underline{0.288} & \underline{0.359} \\
        \midrule
        \textit{BAP}-Llama-3.2-11B & \textbf{0.289} & \textbf{0.298} & \textbf{0.367} \\
        \bottomrule
    \end{tabular}
    \label{tab:precision_k}
\end{table}

\subsection{RQ4: New Bug and Length Generalization}
Apart from efficiency as discussed above, we investigate several differences between \ourmethod{} and zero-shot prompting of the underlying model.

% \adam{Here we will give some trend/error analysis of \ourmethod{} compared to prompting. For instance, how do the methods differ in terms of method length? Does \ourmethod{} provide better insight for multi-line bugs? What are remaining problems that both prompting and our method struggle on?}




\textbf{New Bug Generalization}
We evaluate \ourmethod{} compared to LLM prompting and probing on 543 new bugs from Defects4J v3.0.1 in Table~\ref{tab:defects4j-3}. Our method and probing baselines are only trained on Defects4J v1.2.0, so this serves as an unbiased evaluation on the ability of these methods to generalize to new bugs. Our method, as well as the baselines, drop in performance for the new bugs, but \ourmethod{} maintains the highest top-$k$ accuracy (14.4\% increase in top-1 over the strongest baseline).

\begin{table}[t]
    \centering
    \small
    \caption{Comparison of \ourmethod{} with current state-of-the-art test-free fault localization method LLMAO on 437 additional bugs from Defects4J v3.0.1.}
    \label{tab:defects4j-3}
        % \setlength{\tabcolsep}{12pt} % Adjust column spacing
        \begin{tabular}{lrrr}
        \toprule
        & \multicolumn{3}{c}{\textbf{Defects4J v3.0.1}} \\
        \cmidrule(lr){2-4}
        \textbf{Method} & \textbf{Top-1} & \textbf{Top-3} & \textbf{Top-5} \\
        \midrule
        Random & 0.166 & 0.377 & 0.512\\
        \midrule
        CodeLlama-70B & 0.152 & 0.276 & 0.326 \\
        Llama-3.3-70B & \underline{0.215} & \underline{0.416} & 0.528 \\
        Qwen-2.5-72B & 0.161 & 0.395 & 0.515 \\
        GPT-4o & 0.181 & 0.451 & 0.579 \\
        \midrule
        Linear Probe Llama-3.2-11B & 0.156 & 0.409 & \underline{0.557} \\
        LLMAO-Llama-3.2-11B & 0.174 & 0.389 & 0.515 \\
        % WELL-Llama-3.2-11B &  &  &  \\
        GridLoc-Llama-3.2-11B & 0.165 & 0.377 & 0.512 \\
        \midrule
        \ourmethod{}-Llama-3.2-11B & \textbf{0.246} & \textbf{0.459} & \textbf{0.583} \\
        \bottomrule
        \end{tabular}
\end{table}

\begin{figure*}[t]
    \centering
    \begin{subfigure}[b]{0.48\linewidth}
    \centering
        \includegraphics[width=\linewidth]{figures/bap_examples_1.pdf}
        \caption{Python syntax error}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\linewidth}
    \centering
        \includegraphics[width=\linewidth]{figures/bap_examples_2.pdf}
        \caption{Defects4J bug}
    \end{subfigure}
    % \begin{subfigure}[b]{0.32\linewidth}
    % \centering
    %     \includegraphics{figures/example3.pdf}
    %     \caption{}
    % \end{subfigure}
    \caption{Examples of bug localization with \ourmethod{} on two evaluation set samples. We visualize the line-level weights from \ourmethod{} above such that lines highlighted in a darker color have higher weights. \ourmethod{} correctly identifies bug locations at Top-1. 
    % \arthur{we should label each of the bug types on remake}
    % \adam{This figure needs to be updated. We can add a Defects4J example.}
    }
    \label{fig:qualitative}
    % \vspace{-0.1in}
\end{figure*}



\textbf{Context Length Generalization}
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/loc-scale-top1.pdf}
    \caption{Top-1 accuracy versus context length, measured by lines of code (LOC) on Defects4J. We compare \ourmethod{}-Llama3.2-11B against models at least six times larger.}
    \label{fig:length-gen}
\end{figure}
% \begin{figure}[t]
%     \centering
%     \includegraphics[width=\linewidth]{figures/loc-scale-errorbars-quantile-bins.pdf}
%     \caption{Top-1 accuracy versus context length, measured by lines of code (LOC) on Defects4J. We compare \ourmethod{}-Llama3.2-11B against models at least six times larger.}
%     \label{fig:length-gen}
% \end{figure}
We compare the behavior of \ourmethod{} to prompting in terms of length generalization in Figure~\ref{fig:length-gen}. We see that \ourmethod{} outperforms zero-shot prompting of various LLMs across context lengths from 10 to 50 lines. On code fragments of 60 lines and longer, all methods perform near random, making fault localization on code over 50 lines a challenging, but valuable task for future work.
% In addition, we can visualize sub-line localization by plotting the learned attention weights before line-level aggregation for possible localization within a line.

We visualize the output of \ourmethod{} on two examples in Figure~\ref{fig:qualitative}.



% \begin{table}[t]
%     \centering
%     \caption{Comparison with existing fault localization approaches on Defects4J. We evaluate on line-level fault localization in the context of a single-method.}
%     \label{tab:defects4j}
%     \begin{tabular}{lrrr}
%     \toprule
%         Method & Top-1  & Top-3 & Top-5\\
%         \midrule
%         Random & 0.144 & 0.373 & 0.454\\
%         \midrule
%         DeepFL & 0.144 & 0.241 & 0.342\\
%         SmartFL & 0.158 & 0.250 & 0.323\\
%         TransferFL & 0.218 & &\\
%         LLMAO & 0.223 & 0.377 & 0.463\\
%         \midrule
%         GPT-4o & 0.332 & &\\
%         \midrule
%         \ourmethod{} & 0.334 & 0.496 & 0.576\\
%     \bottomrule
%     \end{tabular}
% \end{table}





% \begin{table*}[t]
%     \centering
%     \caption{Comparison of \ourmethod{} with instruction-tuned language models for zero-shot fault localization on Defects4J (v1.2.0). \ourmethod{} achieves highly competitive performance at a fraction of the GPU memory cost.\adam{We can remove CodeLlama and calculate latency}}
%     \label{tab:defects4j}
%         \setlength{\tabcolsep}{12pt} % Adjust column spacing
%         \begin{tabular}{lcccc}
%         \toprule
%         & \multicolumn{3}{c}{\textbf{Defects4J}} & \\
%         \cmidrule(lr){2-4}
%         \textbf{Method} & \textbf{Top-1} & \textbf{Top-3} & \textbf{Top-5} & \textbf{vRAM Cost (GB)} \\
%         \midrule
%         Random & 0.144 & 0.373 & 0.454 & \\
%         \midrule
%         % CodeLlama-13B & 0.107 & 0.402 & 0.484 & 34.2 \\
%         Llama3.2-11B & 0.140 & 0.356 & 0.429 & 21.4 \\
%         Llama3.2-90 & & & &  \\
%         % Qwen2.5-Coder-14B & 0.159 & 0.463 & 0.508 & 29.2 \\
%         GPT-4o &  &  &  & \\
%         \midrule
%         Probing baselines &  &  &  & \\
%         \midrule
%         \ourmethod{} & \textbf{0.334} & \textbf{0.496} & \textbf{0.576} & \textbf{2.5} \\
%         \bottomrule
%         \end{tabular}
%         % \vspace{6mm} % Additional vertical spacing
%         % \begin{tablenotes}
%         %     \footnotesize
%         %     \item[+] Instruction-tuned
%         % \end{tablenotes}
% \end{table*}

% 11-14B baselines are below random, so best if we try 70B+ %



% \adam{Discuss the other results for SStuB and Vulnerabilities here.}

% \subsection{Qualitative evaluation}
% \adam{This section should show some positive and negative qualitative examples of our approach for localizing bugs.}
