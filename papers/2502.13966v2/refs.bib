@inproceedings{liucodecorrect,
 author = {Liu, Jiawei and Xia, Chunqiu Steven and Wang, Yuyao and Zhang, Lingming},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Neumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {21558--21572},
 publisher = {Curran Associates, Inc.},
 title = {Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of Large Language Models for Code Generation},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/43e9d647ccd3e4b7b5baab53f0368686-Paper-Conference.pdf},
 volume = {36},
 year = {2023}
}

@inproceedings{astprobe,
  title={AST-Probe: Recovering abstract syntax trees from hidden representations of pre-trained language models},
  author={Hern{\'a}ndez L{\'o}pez, Jos{\'e} Antonio and Weyssow, Martin and Cuadrado, Jes{\'u}s S{\'a}nchez and Sahraoui, Houari},
  booktitle={Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering},
  pages={1--11},
  year={2022}
}

@inproceedings{pashakhanlooCodeTrekFlexibleModeling2021,
  title = {{{CodeTrek}}: {{Flexible Modeling}} of {{Code}} Using an {{Extensible Relational Representation}}},
  shorttitle = {{{CodeTrek}}},
  author = {Pashakhanloo, Pardis and Naik, Aaditya and Wang, Yuepeng and Dai, Hanjun and Maniatis, Petros and Naik, Mayur},
  date = {2021-10-06},
  url = {https://openreview.net/forum?id=WQc075jmBmf},
  urldate = {2024-04-19},
  booktitle = {International Conference on Learning Representations},
  year = {2022}
}

@inproceedings{pearce2022asleep,
  title={Asleep at the keyboard? assessing the security of github copilotâ€™s code contributions},
  author={Pearce, Hammond and Ahmad, Baleegh and Tan, Benjamin and Dolan-Gavitt, Brendan and Karri, Ramesh},
  booktitle={2022 IEEE Symposium on Security and Privacy (SP)},
  pages={754--768},
  year={2022},
  organization={IEEE}
}

@article{hooda2024large,
  title={Do Large Code Models Understand Programming Concepts? A Black-box Approach},
  author={Hooda, Ashish and Christodorescu, Mihai and Allamanis, Miltos and Wilson, Aaron and Fawaz, Kassem and Jha, Somesh},
  journal={arXiv preprint arXiv:2402.05980},
  year={2024}
}

@inproceedings{tenney2018you,
  title={What do you learn from context? Probing for sentence structure in contextualized word representations},
  author={Tenney, Ian and Xia, Patrick and Chen, Berlin and Wang, Alex and Poliak, Adam and McCoy, R Thomas and Kim, Najoung and Van Durme, Benjamin and Bowman, Samuel R and Das, Dipanjan and others},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@article{tschannen2024image,
  title={Image captioners are scalable vision learners too},
  author={Tschannen, Michael and Kumar, Manoj and Steiner, Andreas and Zhai, Xiaohua and Houlsby, Neil and Beyer, Lucas},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{kaplan2020scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}

@article{wei2022emergent,
  title={Emergent Abilities of Large Language Models},
  author={Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and others},
  journal={Transactions on Machine Learning Research},
  year={2022}
}

@article{zhang2023unifying,
  title={Unifying the perspectives of nlp and software engineering: A survey on language models for code},
  author={Zhang, Ziyin and Chen, Chaoyu and Liu, Bingchang and Liao, Cong and Gong, Zi and Yu, Hang and Li, Jianguo and Wang, Rui},
  journal={arXiv preprint arXiv:2311.07989},
  year={2023}
}

@inproceedings{yasunaga2021break,
  title={Break-it-fix-it: Unsupervised learning for program repair},
  author={Yasunaga, Michihiro and Liang, Percy},
  booktitle={International conference on machine learning},
  pages={11941--11952},
  year={2021},
  organization={PMLR}
}

@inproceedings{jimenez2023swe,
  title={SWE-bench: Can Language Models Resolve Real-world Github Issues?},
  author={Jimenez, Carlos E and Yang, John and Wettig, Alexander and Yao, Shunyu and Pei, Kexin and Press, Ofir and Narasimhan, Karthik R},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024}
}

@article{hindle2016naturalness,
  title={On the naturalness of software},
  author={Hindle, Abram and Barr, Earl T and Gabel, Mark and Su, Zhendong and Devanbu, Premkumar},
  journal={Communications of the ACM},
  volume={59},
  number={5},
  pages={122--131},
  year={2016},
  publisher={ACM New York, NY, USA}
}

@article{khare2023understanding,
  title={Understanding the Effectiveness of Large Language Models in Detecting Security Vulnerabilities},
  author={Khare, Avishree and Dutta, Saikat and Li, Ziyang and Solko-Breslin, Alaia and Alur, Rajeev and Naik, Mayur},
  journal={arXiv preprint arXiv:2311.16169},
  year={2023}
}

@inproceedings{gupta2017deepfix,
  title={Deepfix: Fixing common c language errors by deep learning},
  author={Gupta, Rahul and Pal, Soham and Kanade, Aditya and Shevade, Shirish},
  booktitle={Proceedings of the aaai conference on artificial intelligence},
  volume={31},
  number={1},
  year={2017}
}

@inproceedings{karampatsis2020often,
  title={How often do single-statement bugs occur? the manysstubs4j dataset},
  author={Karampatsis, Rafael-Michael and Sutton, Charles},
  booktitle={Proceedings of the 17th International Conference on Mining Software Repositories},
  pages={573--577},
  year={2020}
}

@inproceedings{richter2022tssb,
  title={Tssb-3m: Mining single statement bugs at massive scale},
  author={Richter, Cedric and Wehrheim, Heike},
  booktitle={Proceedings of the 19th International Conference on Mining Software Repositories},
  pages={418--422},
  year={2022}
}

@inproceedings{niu2022does,
  title={Does BERT rediscover a classical NLP pipeline?},
  author={Niu, Jingcheng and Lu, Wenjie and Penn, Gerald},
  booktitle={Proceedings of the 29th International Conference on Computational Linguistics},
  pages={3143--3153},
  year={2022}
}

@article{alain2016understanding,
  title={Understanding intermediate layers using linear classifier probes},
  author={Alain, Guillaume and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1610.01644},
  year={2016}
}

@article{repe,
  title={Representation engineering: A top-down approach to ai transparency},
  author={Zou, Andy and Phan, Long and Chen, Sarah and Campbell, James and Guo, Phillip and Ren, Richard and Pan, Alexander and Yin, Xuwang and Mazeika, Mantas and Dombrowski, Ann-Kathrin and others},
  journal={arXiv preprint arXiv:2310.01405},
  year={2023}
}

@inproceedings{hewitt2019designing,
  title={Designing and Interpreting Probes with Control Tasks},
  author={Hewitt, John and Liang, Percy},
  booktitle={Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  pages={2733--2743},
  year={2019}
}

@article{shen2023large,
  title={Large language model alignment: A survey},
  author={Shen, Tianhao and Jin, Renren and Huang, Yufei and Liu, Chuang and Dong, Weilong and Guo, Zishan and Wu, Xinwei and Liu, Yan and Xiong, Deyi},
  journal={arXiv preprint arXiv:2309.15025},
  year={2023}
}

@inproceedings{
chen2024teaching,
title={Teaching Large Language Models to Self-Debug},
author={Xinyun Chen and Maxwell Lin and Nathanael Sch{\"a}rli and Denny Zhou},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=KuPixIqPiq}
}

@inproceedings{he2022distribution,
  title={On distribution shift in learning-based bug detectors},
  author={He, Jingxuan and Beurer-Kellner, Luca and Vechev, Martin},
  booktitle={International conference on machine learning},
  pages={8559--8580},
  year={2022},
  organization={PMLR}
}

@article{liu2023meaning,
  title={Meaning representations from trajectories in autoregressive models},
  author={Liu, Tian Yu and Trager, Matthew and Achille, Alessandro and Perera, Pramuditha and Zancato, Luca and Soatto, Stefano},
  journal={arXiv preprint arXiv:2310.18348},
  year={2023}
}

@misc{owasp,
    title={OWASP Benchmark Suite 2023},
    url={https://owasp.org/www-project-benchmark/}
}

@misc{juliet-java,
    title={Juliet Java 2023},
    url={https://samate.nist.gov/SARD/test-suites/111},
    year={2023}
}

@misc{juliet-cpp,
    title={Juliet C/C++ 2023},
    url={https://samate.nist.gov/SARD/test-suites/112},
    year={2023}
}

@article{roziere2023code,
  title={Code llama: Open foundation models for code},
  author={Roziere, Baptiste and Gehring, Jonas and Gloeckle, Fabian and Sootla, Sten and Gat, Itai and Tan, Xiaoqing Ellen and Adi, Yossi and Liu, Jingyu and Remez, Tal and Rapin, J{\'e}r{\'e}my and others},
  journal={arXiv preprint arXiv:2308.12950},
  year={2023}
}

@article{touvron2023llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@article{llama3modelcard,
title={Llama 3 Model Card},
author={AI@Meta},
year={2024},
url = {https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md}
}

@article{marks2023geometry,
  title={The geometry of truth: Emergent linear structure in large language model representations of true/false datasets},
  author={Marks, Samuel and Tegmark, Max},
  journal={arXiv preprint arXiv:2310.06824},
  year={2023}
}

@article{chen2021evaluating,
  title={Evaluating large language models trained on code},
  author={Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and others},
  journal={arXiv preprint arXiv:2107.03374},
  year={2021}
}

@inproceedings{jesse2023large,
  title={Large language models and simple, stupid bugs},
  author={Jesse, Kevin and Ahmed, Toufique and Devanbu, Premkumar T and Morgan, Emily},
  booktitle={2023 IEEE/ACM 20th International Conference on Mining Software Repositories (MSR)},
  pages={563--575},
  year={2023},
  organization={IEEE}
}

@inproceedings{kumar2022fine,
  title={Fine-Tuning can Distort Pretrained Features and Underperform Out-of-Distribution},
  author={Kumar, Ananya and Raghunathan, Aditi and Jones, Robbie Matthew and Ma, Tengyu and Liang, Percy},
  booktitle={International Conference on Learning Representations},
  year={2022}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{gurnee2023finding,
  title={Finding Neurons in a Haystack: Case Studies with Sparse Probing},
  author={Gurnee, Wes and Nanda, Neel and Pauly, Matthew and Harvey, Katherine and Troitskii, Dmitrii and Bertsimas, Dimitris},
  journal={Transactions on Machine Learning Research},
  year={2023}
}

@inproceedings{pimentel2022attentional,
  title={Attentional probe: Estimating a moduleâ€™s functional potential},
  author={Pimentel, Tiago and Valvoda, Josef and Stoehr, Niklas and Cotterell, Ryan},
  booktitle={Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  pages={11459--11472},
  year={2022},
  organization={Association for Computational Linguistics}
}

@article{landi1992undecidability,
  title={Undecidability of static analysis},
  author={Landi, William},
  journal={ACM Letters on Programming Languages and Systems (LOPLAS)},
  volume={1},
  number={4},
  pages={323--337},
  year={1992},
  publisher={ACM New York, NY, USA}
}

@misc{ugare2024syncode,
      title={SynCode: LLM Generation with Grammar Augmentation}, 
      author={Shubham Ugare and Tarun Suresh and Hangoo Kang and Sasa Misailovic and Gagandeep Singh},
      year={2024},
      eprint={2403.01632},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{MLEnhancedCodeCompletion,
  title = {{{ML-Enhanced Code Completion Improves Developer Productivity}}},
  url = {http://research.google/blog/ml-enhanced-code-completion-improves-developer-productivity/},
  urldate = {2024-05-09},
  abstract = {Posted by Maxim Tabachnyk, Staff Software Engineer and Stoyan Nikolov, Senior Engineering Manager, Google Research   Update â€” 2022/09/06: This post...},
  langid = {english},
}

@inproceedings{insecurecode,
author = {Perry, Neil and Srivastava, Megha and Kumar, Deepak and Boneh, Dan},
title = {Do Users Write More Insecure Code with AI Assistants?},
year = {2023},
isbn = {9798400700507},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576915.3623157},
doi = {10.1145/3576915.3623157},
abstract = {AI code assistants have emerged as powerful tools that can aid in the software development life-cycle and can improve developer productivity. Unfortunately, such assistants have also been found to produce insecure code in lab environments, raising significant concerns about their usage in practice. In this paper, we conduct a user study to examine how users interact with AI code assistants to solve a variety of security related tasks. Overall, we find that participants who had access to an AI assistant wrote significantly less secure code than those without access to an assistant. Participants with access to an AI assistant were also more likely to believe they wrote secure code, suggesting that such tools may lead users to be overconfident about security flaws in their code. To better inform the design of future AI-based code assistants, we release our user-study apparatus to researchers seeking to build on our work.},
booktitle = {Proceedings of the 2023 ACM SIGSAC Conference on Computer and Communications Security},
pages = {2785â€“2799},
numpages = {15},
keywords = {language models, machine learning, programming assistants, usable security},
location = {<conf-loc>, <city>Copenhagen</city>, <country>Denmark</country>, </conf-loc>},
series = {CCS '23}
}

@article{liu2024your,
  title={Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation},
  author={Liu, Jiawei and Xia, Chunqiu Steven and Wang, Yuyao and Zhang, Lingming},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{gu2024counterfeit,
  title={The Counterfeit Conundrum: Can Code Language Models Grasp the Nuances of Their Incorrect Generations?},
  author={Gu, Alex and Li, Wen-Ding and Jain, Naman and Olausson, Theo X and Lee, Celine and Sen, Koushik and Solar-Lezama, Armando},
  journal={arXiv preprint arXiv:2402.19475},
  year={2024}
}

@inproceedings{habib2018many,
  title={How many of all bugs do we find? a study of static bug detectors},
  author={Habib, Andrew and Pradel, Michael},
  booktitle={Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering},
  pages={317--328},
  year={2018}
}

@inproceedings{kumar2021fine,
  title={Fine-Tuning can Distort Pretrained Features and Underperform Out-of-Distribution},
  author={Kumar, Ananya and Raghunathan, Aditi and Jones, Robbie Matthew and Ma, Tengyu and Liang, Percy},
  booktitle={International Conference on Learning Representations},
  year={2022}
}

@inproceedings{azaria2023internal,
  title={The Internal State of an LLM Knows When Itâ€™s Lying},
  author={Azaria, Amos and Mitchell, Tom},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2023},
  pages={967--976},
  year={2023}
}

@inproceedings{hewitt2019structural,
  title={A structural probe for finding syntax in word representations},
  author={Hewitt, John and Manning, Christopher D},
  booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  pages={4129--4138},
  year={2019}
}

@article{ayewah2008using,
  title={Using static analysis to find bugs},
  author={Ayewah, Nathaniel and Pugh, William and Hovemeyer, David and Morgenthaler, J David and Penix, John},
  journal={IEEE software},
  volume={25},
  number={5},
  pages={22--29},
  year={2008},
  publisher={IEEE}
}

@article{manes2019art,
  title={The art, science, and engineering of fuzzing: A survey},
  author={Man{\`e}s, Valentin JM and Han, HyungSeok and Han, Choongwoo and Cha, Sang Kil and Egele, Manuel and Schwartz, Edward J and Woo, Maverick},
  journal={IEEE Transactions on Software Engineering},
  volume={47},
  number={11},
  pages={2312--2331},
  year={2019},
  publisher={IEEE}
}

@inproceedings{macneil2024decoding,
  title={Decoding Logic Errors: A Comparative Study on Bug Detection by Students and Large Language Models},
  author={MacNeil, Stephen and Denny, Paul and Tran, Andrew and Leinonen, Juho and Bernstein, Seth and Hellas, Arto and Sarsa, Sami and Kim, Joanne},
  booktitle={Proceedings of the 26th Australasian Computing Education Conference},
  pages={11--18},
  year={2024}
}

@article{alrashedy2023language,
  title={Language Models are Better Bug Detector Through Code-Pair Classification},
  author={Alrashedy, Kamel},
  journal={arXiv preprint arXiv:2311.07957},
  year={2023}
}

@INPROCEEDINGS{usestaticanalysis,
  author={Johnson, Brittany and Song, Yoonki and Murphy-Hill, Emerson and Bowdidge, Robert},
  booktitle={2013 35th International Conference on Software Engineering (ICSE)}, 
  title={Why don't software developers use static analysis tools to find bugs?}, 
  year={2013},
  volume={},
  number={},
  pages={672-681},
  keywords={Interviews;Computer bugs;Encoding;Software;Teamwork;Companies;Standards},
  doi={10.1109/ICSE.2013.6606613}}

@inproceedings{zhang2023code,
  title={Code Representation Learning at Scale},
  author={Zhang, Dejiao and Ahmad, Wasi Uddin and Tan, Ming and Ding, Hantian and Nallapati, Ramesh and Roth, Dan and Ma, Xiaofei and Xiang, Bing},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024}
}

@article{belinkov2022probing,
  title={Probing classifiers: Promises, shortcomings, and advances},
  author={Belinkov, Yonatan},
  journal={Computational Linguistics},
  volume={48},
  number={1},
  pages={207--219},
  year={2022},
  publisher={MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~â€¦}
}

@inproceedings{devlin-etal-2019-bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    editor = "Burstein, Jill  and
      Doran, Christy  and
      Solorio, Thamar",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}

@inproceedings{hu2021lora,
  title={LoRA: Low-Rank Adaptation of Large Language Models},
  author={Hu, Edward J and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu and others},
  booktitle={International Conference on Learning Representations},
  year={2022}
}

@inproceedings{saha2013improving,
  title={Improving bug localization using structured information retrieval},
  author={Saha, Ripon K and Lease, Matthew and Khurshid, Sarfraz and Perry, Dewayne E},
  booktitle={2013 28th IEEE/ACM International Conference on Automated Software Engineering (ASE)},
  pages={345--355},
  year={2013},
  organization={IEEE}
}

@inproceedings{CodeQL,
  title={QL: Object-oriented queries on relational data},
  author={Avgustinov, Pavel and De Moor, Oege and Jones, Michael Peyton and Sch{\"a}fer, Max},
  booktitle={30th European Conference on Object-Oriented Programming (ECOOP 2016)},
  year={2016},
  organization={Schloss-Dagstuhl-Leibniz Zentrum f{\"u}r Informatik}
}

@inproceedings{cleve2005locating,
  title={Locating causes of program failures},
  author={Cleve, Holger and Zeller, Andreas},
  booktitle={Proceedings of the 27th international conference on Software engineering},
  pages={342--351},
  year={2005}
}

@article{vessey1985expertise,
  title={Expertise in debugging computer programs: A process analysis},
  author={Vessey, Iris},
  journal={International Journal of Man-Machine Studies},
  volume={23},
  number={5},
  pages={459--494},
  year={1985},
  publisher={Elsevier}
}

@article{zheng2024opencodeinterpreter,
  title={OpenCodeInterpreter: Integrating Code Generation with Execution and Refinement},
  author={Zheng, Tianyu and Zhang, Ge and Shen, Tianhao and Liu, Xueling and Lin, Bill Yuchen and Fu, Jie and Chen, Wenhu and Yue, Xiang},
  journal={arXiv preprint arXiv:2402.14658},
  year={2024}
}

@misc{codeinterp,
  author={OpenAI},
  title = {{{Code Interpreter}}},
  url = {https://platform.openai.com/docs/assistants/tools/code-interpreter},
  year = {2024-05-22},
}


@misc{cwe,
  title = {{{Common Weakness Enumeration}}},
  url = {https://cwe.mitre.org/},
  urldate = {2024-05-22},
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}

@article{meng2022locating,
  title={Locating and editing factual associations in GPT},
  author={Meng, Kevin and Bau, David and Andonian, Alex and Belinkov, Yonatan},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={17359--17372},
  year={2022}
}

@article{liang2018fuzzing,
  title={Fuzzing: State of the art},
  author={Liang, Hongliang and Pei, Xiaoxiao and Jia, Xiaodong and Shen, Wuwei and Zhang, Jian},
  journal={IEEE Transactions on Reliability},
  volume={67},
  number={3},
  pages={1199--1218},
  year={2018},
  publisher={IEEE}
}

@inproceedings{gupta2000generating,
  title={Generating test data for branch coverage},
  author={Gupta, Neelam and Mathur, Aditya P and Soffa, Mary Lou},
  booktitle={Proceedings ASE 2000. Fifteenth IEEE International Conference on Automated Software Engineering},
  pages={219--227},
  year={2000},
  organization={IEEE}
}

@inproceedings{santos2018syntax,
  title={Syntax and sensibility: Using language models to detect and correct syntax errors},
  author={Santos, Eddie Antonio and Campbell, Joshua Charles and Patel, Dhvani and Hindle, Abram and Amaral, Jos{\'e} Nelson},
  booktitle={2018 IEEE 25th International Conference on Software Analysis, Evolution and Reengineering (SANER)},
  pages={311--322},
  year={2018},
  organization={IEEE}
}

@inproceedings{defects4j,
  title={Defects4J: A database of existing faults to enable controlled testing studies for Java programs},
  author={Just, Ren{\'e} and Jalali, Darioush and Ernst, Michael D},
  booktitle={Proceedings of the 2014 international symposium on software testing and analysis},
  pages={437--440},
  year={2014}
}

@article{wu2023large,
  title={Large language models in fault localisation},
  author={Wu, Yonghao and Li, Zheng and Zhang, Jie M and Papadakis, Mike and Harman, Mark and Liu, Yong},
  journal={arXiv preprint arXiv:2308.15276},
  year={2023}
}

@inproceedings{llmao,
author = {Yang, Aidan Z. H. and Le Goues, Claire and Martins, Ruben and Hellendoorn, Vincent},
title = {Large Language Models for Test-Free Fault Localization},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3623342},
doi = {10.1145/3597503.3623342},
abstract = {Fault Localization (FL) aims to automatically localize buggy lines of code, a key first step in many manual and automatic debugging tasks. Previous FL techniques assume the provision of input tests, and often require extensive program analysis, program instrumentation, or data preprocessing. Prior work on deep learning for APR struggles to learn from small datasets and produces limited results on real-world programs. Inspired by the ability of large language models (LLMs) of code to adapt to new tasks based on very few examples, we investigate the applicability of LLMs to line level fault localization. Specifically, we propose to overcome the left-to-right nature of LLMs by fine-tuning a small set of bidirectional adapter layers on top of the representations learned by LLMs to produce LLMAO, the first language model based fault localization approach that locates buggy lines of code without any test coverage information. We fine-tune LLMs with 350 million, 6 billion, and 16 billion parameters on small, manually curated corpora of buggy programs such as the Defects4J corpus. We observe that our technique achieves substantially more confidence in fault localization when built on the larger models, with bug localization performance scaling consistently with the LLM size. Our empirical evaluation shows that LLMAO improves the Top-1 results over the state-of-the-art machine learning fault localization (MLFL) baselines by 2.3\%--54.4\%, and Top-5 results by 14.4\%-35.6\%. LLMAO is also the first FL technique trained using a language model architecture that can detect security vulnerabilities down to the code line level.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {17},
numpages = {12},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@article{agentless,
  title={Agentless: Demystifying llm-based software engineering agents},
  author={Xia, Chunqiu Steven and Deng, Yinlin and Dunn, Soren and Zhang, Lingming},
  journal={arXiv preprint arXiv:2407.01489},
  year={2024}
}

@inproceedings{deepfl,
  title={Deepfl: Integrating multiple fault diagnosis dimensions for deep fault localization},
  author={Li, Xia and Li, Wei and Zhang, Yuqun and Zhang, Lingming},
  booktitle={Proceedings of the 28th ACM SIGSOFT international symposium on software testing and analysis},
  pages={169--180},
  year={2019}
}

@inproceedings{smartfl,
  title={Fault localization via efficient probabilistic modeling of program semantics},
  author={Zeng, Muhan and Wu, Yiqian and Ye, Zhentao and Xiong, Yingfei and Zhang, Xin and Zhang, Lu},
  booktitle={Proceedings of the 44th International Conference on Software Engineering},
  pages={958--969},
  year={2022}
}

@inproceedings{transferfl,
  title={Improving fault localization and program repair with deep semantic features and transferred knowledge},
  author={Meng, Xiangxin and Wang, Xu and Zhang, Hongyu and Sun, Hailong and Liu, Xudong},
  booktitle={Proceedings of the 44th International Conference on Software Engineering},
  pages={1169--1180},
  year={2022}
}

@article{llama3,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

@article{qwen2,
  title={Qwen2. 5 Technical Report},
  author={Yang, An and Yang, Baosong and Zhang, Beichen and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and Wei, Haoran and others},
  journal={arXiv preprint arXiv:2412.15115},
  year={2024}
}

@article{gpt4o,
  title={Gpt-4o system card},
  author={Hurst, Aaron and Lerer, Adam and Goucher, Adam P and Perelman, Adam and Ramesh, Aditya and Clark, Aidan and Ostrow, AJ and Welihinda, Akila and Hayes, Alan and Radford, Alec and others},
  journal={arXiv preprint arXiv:2410.21276},
  year={2024}
}

@article{codellama,
  title={Code llama: Open foundation models for code},
  author={Roziere, Baptiste and Gehring, Jonas and Gloeckle, Fabian and Sootla, Sten and Gat, Itai and Tan, Xiaoqing Ellen and Adi, Yossi and Liu, Jingyu and Sauvestre, Romain and Remez, Tal and others},
  journal={arXiv preprint arXiv:2308.12950},
  year={2023}
}

@inproceedings{codebert,
    title = "{C}ode{BERT}: A Pre-Trained Model for Programming and Natural Languages",
    author = "Feng, Zhangyin  and
      Guo, Daya  and
      Tang, Duyu  and
      Duan, Nan  and
      Feng, Xiaocheng  and
      Gong, Ming  and
      Shou, Linjun  and
      Qin, Bing  and
      Liu, Ting  and
      Jiang, Daxin  and
      Zhou, Ming",
    editor = "Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.findings-emnlp.139/",
    doi = "10.18653/v1/2020.findings-emnlp.139",
    pages = "1536--1547",
    abstract = "We present CodeBERT, a bimodal pre-trained model for programming language (PL) and natural language (NL). CodeBERT learns general-purpose representations that support downstream NL-PL applications such as natural language code search, code documentation generation, etc. We develop CodeBERT with Transformer-based neural architecture, and train it with a hybrid objective function that incorporates the pre-training task of replaced token detection, which is to detect plausible alternatives sampled from generators. This enables us to utilize both {\textquotedblleft}bimodal{\textquotedblright} data of NL-PL pairs and {\textquotedblleft}unimodal data, where the former provides input tokens for model training while the latter helps to learn better generators. We evaluate CodeBERT on two NL-PL applications by fine-tuning model parameters. Results show that CodeBERT achieves state-of-the-art performance on both natural language code search and code documentation generation. Furthermore, to investigate what type of knowledge is learned in CodeBERT, we construct a dataset for NL-PL probing, and evaluate in a zero-shot setting where parameters of pre-trained models are fixed. Results show that CodeBERT performs better than previous pre-trained models on NLPL probing."
}

@article{r1,
  title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning},
  author={Guo, Daya and Yang, Dejian and Zhang, Haowei and Song, Junxiao and Zhang, Ruoyu and Xu, Runxin and Zhu, Qihao and Ma, Shirong and Wang, Peiyi and Bi, Xiao and others},
  journal={arXiv preprint arXiv:2501.12948},
  year={2025}
}

@article{well,
author = {Zhang, Huangzhao and Li, Zhuo and Li, Jia and Jin, Zhi and Li, Ge},
title = {WELL: Applying bug detectors to bug localization via weakly supervised learning},
journal = {Journal of Software: Evolution and Process},
volume = {36},
number = {9},
pages = {e2669},
keywords = {bug detection, bug localization, weakly supervised learning},
doi = {https://doi.org/10.1002/smr.2669},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/smr.2669},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/smr.2669},
abstract = {Abstract Bug localization, which is used to help programmers identify the location of bugs in source code, is an essential task in software development. Researchers have already made efforts to harness the powerful deep learning (DL) techniques to automate it. However, training bug localization model is usually challenging because it requires a large quantity of data labeled with the bug's exact location, which is difficult and time-consuming to collect. By contrast, obtaining bug detection data with binary labels of whether there is a bug in the source code is much simpler. This paper proposes a WEakly supervised bug LocaLization (WELL) method, which only uses the bug detection data with binary labels to train a bug localization model. With CodeBERT finetuned on the buggy-or-not binary labeled data, WELL can address bug localization in a weakly supervised manner. The evaluations on three method-level synthetic datasets and one file-level real-world dataset show that WELL is significantly better than the existing state-of-the-art model in typical bug localization tasks such as variable misuse and other bugs.},
year = {2024}
}

@article{beltagy2020longformer,
  title={Longformer: The long-document transformer},
  author={Beltagy, Iz and Peters, Matthew E and Cohan, Arman},
  journal={arXiv preprint arXiv:2004.05150},
  year={2020}
}

@inproceedings{jain-wallace-2019-attention,
    title = "{A}ttention is not {E}xplanation",
    author = "Jain, Sarthak  and
      Wallace, Byron C.",
    editor = "Burstein, Jill  and
      Doran, Christy  and
      Solorio, Thamar",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1357/",
    doi = "10.18653/v1/N19-1357",
    pages = "3543--3556",
    abstract = "Attention mechanisms have seen wide adoption in neural NLP models. In addition to improving predictive performance, these are often touted as affording transparency: models equipped with attention provide a distribution over attended-to input units, and this is often presented (at least implicitly) as communicating the relative importance of inputs. However, it is unclear what relationship exists between attention weights and model outputs. In this work we perform extensive experiments across a variety of NLP tasks that aim to assess the degree to which attention weights provide meaningful {\textquotedblleft}explanations{\textquotedblright} for predictions. We find that they largely do not. For example, learned attention weights are frequently uncorrelated with gradient-based measures of feature importance, and one can identify very different attention distributions that nonetheless yield equivalent predictions. Our findings show that standard attention modules do not provide meaningful explanations and should not be treated as though they do."
}

@inproceedings{wiegreffe-pinter-2019-attention,
    title = "Attention is not not Explanation",
    author = "Wiegreffe, Sarah  and
      Pinter, Yuval",
    editor = "Inui, Kentaro  and
      Jiang, Jing  and
      Ng, Vincent  and
      Wan, Xiaojun",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1002/",
    doi = "10.18653/v1/D19-1002",
    pages = "11--20",
    abstract = "Attention mechanisms play a central role in NLP systems, especially within recurrent neural network (RNN) models. Recently, there has been increasing interest in whether or not the intermediate representations offered by these modules may be used to explain the reasoning for a model`s prediction, and consequently reach insights regarding the model`s decision-making process. A recent paper claims that {\textquoteleft}Attention is not Explanation' (Jain and Wallace, 2019). We challenge many of the assumptions underlying this work, arguing that such a claim depends on one`s definition of explanation, and that testing it needs to take into account all elements of the model. We propose four alternative tests to determine when/whether attention can be used as explanation: a simple uniform-weights baseline; a variance calibration based on multiple random seed runs; a diagnostic framework using frozen weights from pretrained models; and an end-to-end adversarial attention training protocol. Each allows for meaningful interpretation of attention mechanisms in RNN models. We show that even when reliable adversarial distributions can be found, they don`t perform well on the simple diagnostic, indicating that prior work does not disprove the usefulness of attention mechanisms for explainability."
}

@inproceedings{
nijkamp2023codegen,
title={CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis},
author={Erik Nijkamp and Bo Pang and Hiroaki Hayashi and Lifu Tu and Huan Wang and Yingbo Zhou and Silvio Savarese and Caiming Xiong},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=iaYcJKpY2B_}
}

@article{flsurvey,
  author={Wong, W. Eric and Gao, Ruizhi and Li, Yihao and Abreu, Rui and Wotawa, Franz},
  journal={IEEE Transactions on Software Engineering}, 
  title={A Survey on Software Fault Localization}, 
  year={2016},
  volume={42},
  number={8},
  pages={707-740},
  keywords={Debugging;Software engineering;Computer bugs;Software debugging;Fault diagnosis;Complexity theory;Software fault localization;program debugging;software testing;execution trace;suspicious code;survey},
  doi={10.1109/TSE.2016.2521368}}

@article{pearson2016evaluating,
  title={Evaluating \& improving fault localization techniques},
  author={Pearson, Spencer and Campos, Jos{\'e} and Just, Ren{\'e} and Fraser, Gordon and Abreu, Rui and Ernst, Michael D and Pang, Deric and Keller, Benjamin},
  journal={University of Washington Department of Computer Science and Engineering, Seattle, WA, USA, Tech. Rep. UW-CSE-16-08-03},
  pages={27},
  year={2016}
}

@inproceedings{hirsch2020fault,
  title={A Fault Localization and Debugging Support Framework driven by Bug Tracking Data},
  author={Hirsch, Thomas},
  booktitle={2020 IEEE International Symposium on Software Reliability Engineering Workshops (ISSREW)},
  pages={139--142},
  year={2020},
  organization={IEEE}
}

@article{wong2016survey,
  title={A survey on software fault localization},
  author={Wong, W Eric and Gao, Ruizhi and Li, Yihao and Abreu, Rui and Wotawa, Franz},
  journal={IEEE Transactions on Software Engineering},
  volume={42},
  number={8},
  pages={707--740},
  year={2016},
  publisher={IEEE}
}

@article{agentfl,
  title={AgentFL: Scaling LLM-based Fault Localization to Project-Level Context},
  author={Qin, Yihao and Wang, Shangwen and Lou, Yiling and Dong, Jinhao and Wang, Kaixin and Li, Xiaoling and Mao, Xiaoguang},
  journal={arXiv preprint arXiv:2403.16362},
  year={2024}
}
