\section{Related Work}
% Outline:
% \begin{itemize}
%     \item Bug detection: neural ____, evaluating code LLMs ____, prompting ____
%     \item Probing: code properties ____, using attention ____. The method from ____ is most similar to ours but it uses an RNN to learn the attention weights rather than a linear probe and it is applied to classic NLP tasks such as constituency parsing.
%     \item Alignment: code understanding capabilities ____
% \end{itemize}

We survey related work in FL techniques and LLM probing.

\textbf{Automated Fault Localization.}
% Symbolic reasoning approaches for bug detection include static approaches such as CodeQL ____, dynamic approaches such as fuzzing ____), and hybrid approaches such as dynamic symbolic execution ____. These approaches all require customization including tests, formal specifications, and compilable code.
% % All of these approaches formulate bug detection as a {\em decision problem}, which often requires customization in the form of formal correctness specifications, and programs that compile (for static) and run (for dynamic).
% % The undecidability of this decision problem leads these approaches to strike tradeoffs between scalability and accuracy.
% The requirement on customization for symbolic reasoning approaches has spurred a large body of deep learning approaches.
% % that formulate bug detection as a {\em classification problem}.
% %The idea of applying language models for ____ introduced the idea of applying language models for modeling real-world code and laid the foundation for a line of work on neural bug detectors.
% Due to the lack of large real-world supervised training datasets, these methods often rely on synthetic data, causing overfitting ____.
% To overcome the overfitting, there are methods for better synthetic data generation ____, more generalizable architecture design ____, unsupervised pretraining on code ____, and recently prompting-based methods which utilize the knowledge of LLMs ____.
% Our approach allows for scaling FL by training on the available bug detection data and using larger models for improved performance.
% % Our approach in this paper differs from existing works since we rely on small supervised datasets to learn a lightweight probe on top of an LLM, allowing us to elicit its knowledge of bugs without severe overfitting.
Methods for FL include the traditional spectrum-based (SBFL) and mutation-based (MBFL) methods which require executable code and deep-learning based approaches ____. SBFL methods are simple but have low accuracy while MBFL and deep learning approaches have higher accuracy at larger computational cost ____. Various deep learning approaches combine SBFL and MBFL with semantic features from deep models ____. Recently, LLMs have significantly outperformed SBFL and MBFL approaches on FL on the method level ____. Prompting and agent-based systems can even perform repository-level FL ____, but they must reduce the problem to method-level FL ____. LLMAO ____ trains an adapter on an LLM from strong FL supervision to perform FL without executable tests, and WELL ____ finetunes an LLM on bug detection supervision and interprets the attention for FL.
Unlike these approaches, our method uses LLM probing, and we leverage bug detection supervision to scale to more available data.
% Our method is an LLM probing approach which combines the power of LLMs with supervised datasets in a lightweight manner for strong FL performance.


\textbf{Probing LLMs.}
Probing is useful tool in LLM interpretability.
% , although probing results can be nuanced ____.
There is extensive work on probing LLMs, most notably BERT ____, to understand what linguistic knowledge it encodes. ____ design a probe for eliciting natural language syntax parse trees from BERT, and ____ probe for the code abstract syntax trees.
These probes are usually trained on a fixed size input ____, but pooling sequence representations using global weights ____ and sample-conditional weights ____ have been studied. Unlike these approaches, we adopt a traditional Transformer layer as our probe where the attention module learns to pool the input tokens.
% Finally, probing is traditionally known to be outperformed by full-model finetuning, but recent work has shown that probing often performs better out-of-distribution ____. We build on such work and show that \ourmethod{} outperforms finetuning on bug generalization across languages.

% \textbf{LLM alignment}
% Recently, probing has also been used to leverage the knowledge of an LLM for improved truth classification, and even unsupervised probing methods can outperform few-shot prompting ____. \adam{add ____}