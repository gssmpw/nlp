\section{Related Work}
% Outline:
% \begin{itemize}
%     \item Bug detection: neural \citep{pashakhanlooCodeTrekFlexibleModeling2021, yasunaga2021break}, evaluating code LLMs \citep{pearce2022asleep, jimenez2023swe}, prompting \citep{khare2023understanding}
%     \item Probing: code properties \citep{astprobe}, using attention \citep{tenney2018you, niu2022does, tschannen2024image}. The method from \citet{niu2022does} is most similar to ours but it uses an RNN to learn the attention weights rather than a linear probe and it is applied to classic NLP tasks such as constituency parsing.
%     \item Alignment: code understanding capabilities \citep{hooda2024large}
% \end{itemize}

We survey related work in FL techniques and LLM probing.

\textbf{Automated Fault Localization.}
% Symbolic reasoning approaches for bug detection include static approaches such as CodeQL \citep{CodeQL}, dynamic approaches such as fuzzing \citep{liang2018fuzzing}), and hybrid approaches such as dynamic symbolic execution \citep{gupta2000generating}. These approaches all require customization including tests, formal specifications, and compilable code.
% % All of these approaches formulate bug detection as a {\em decision problem}, which often requires customization in the form of formal correctness specifications, and programs that compile (for static) and run (for dynamic).
% % The undecidability of this decision problem leads these approaches to strike tradeoffs between scalability and accuracy.
% The requirement on customization for symbolic reasoning approaches has spurred a large body of deep learning approaches.
% % that formulate bug detection as a {\em classification problem}.
% %The idea of applying language models for \citet{hindle2016naturalness} introduced the idea of applying language models for modeling real-world code and laid the foundation for a line of work on neural bug detectors.
% Due to the lack of large real-world supervised training datasets, these methods often rely on synthetic data, causing overfitting \citep{he2022distribution}.
% To overcome the overfitting, there are methods for better synthetic data generation \citep{yasunaga2021break}, more generalizable architecture design \citep{pashakhanlooCodeTrekFlexibleModeling2021}, unsupervised pretraining on code \citep{zhang2023code}, and recently prompting-based methods which utilize the knowledge of LLMs \citep{khare2023understanding}.
% Our approach allows for scaling FL by training on the available bug detection data and using larger models for improved performance.
% % Our approach in this paper differs from existing works since we rely on small supervised datasets to learn a lightweight probe on top of an LLM, allowing us to elicit its knowledge of bugs without severe overfitting.
Methods for FL include the traditional spectrum-based (SBFL) and mutation-based (MBFL) methods which require executable code and deep-learning based approaches \citep{wong2016survey}. SBFL methods are simple but have low accuracy while MBFL and deep learning approaches have higher accuracy at larger computational cost \citep{wong2016survey}. Various deep learning approaches combine SBFL and MBFL with semantic features from deep models \citep{deepfl, transferfl, smartfl}. Recently, LLMs have significantly outperformed SBFL and MBFL approaches on FL on the method level \citep{wu2023large}. Prompting and agent-based systems can even perform repository-level FL \citep{agentless, agentfl}, but they must reduce the problem to method-level FL \citep{agentless}. LLMAO \citep{llmao} trains an adapter on an LLM from strong FL supervision to perform FL without executable tests, and WELL \citep{well} finetunes an LLM on bug detection supervision and interprets the attention for FL.
Unlike these approaches, our method uses LLM probing, and we leverage bug detection supervision to scale to more available data.
% Our method is an LLM probing approach which combines the power of LLMs with supervised datasets in a lightweight manner for strong FL performance.


\textbf{Probing LLMs.}
Probing is useful tool in LLM interpretability.
% , although probing results can be nuanced \citep{hewitt2019designing, belinkov2022probing}.
There is extensive work on probing LLMs, most notably BERT \citep{devlin-etal-2019-bert}, to understand what linguistic knowledge it encodes. \citet{hewitt2019structural} design a probe for eliciting natural language syntax parse trees from BERT, and \citet{astprobe} probe for the code abstract syntax trees.
These probes are usually trained on a fixed size input \citep{repe}, but pooling sequence representations using global weights \citep{tenney2018you} and sample-conditional weights \citep{niu2022does} have been studied. Unlike these approaches, we adopt a traditional Transformer layer as our probe where the attention module learns to pool the input tokens.
% Finally, probing is traditionally known to be outperformed by full-model finetuning, but recent work has shown that probing often performs better out-of-distribution \citep{kumar2022fine}. We build on such work and show that \ourmethod{} outperforms finetuning on bug generalization across languages.

% \textbf{LLM alignment}
% Recently, probing has also been used to leverage the knowledge of an LLM for improved truth classification, and even unsupervised probing methods can outperform few-shot prompting \citep{repe}. \adam{add \citep{hooda2024large}}