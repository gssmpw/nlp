\section{Attention Probing for Fault Localization}
\label{sec:ourmethod}
% Outline:
% \begin{itemize}
%     \item Include a figure with a simple depiction of the probe.
%     \item Give the formulation of attention probing as a probe over a sequence of token representations.
% \end{itemize}

% \adam{Start this section by saying normal LLM methods such as probing don't work well here. We need to build probes. Then say how to build probes, but the problem is that we don't get localization.}

% \adam{This should explain how to make a good probe. Start with linear probes and get to why we use attention...}

% \adam{Move the following to section 3}
% We first take an abstract view of an LLM as a next-token prediction function $\text{LLM}: T^* \rightarrow T$ where $T$ is the set of acceptable tokens and $*$ is the Kleene star. To produce a sequence of tokens, an LLM is used auto-regressively where $t_{i+1} = \text{LLM}(t_0t_1\dots t_i)$. Due
% Rather than learn the above predictor from raw data, we propose to represent the input program using the hidden representations from a pretrained LLM.
% This approach is commonly called probing, where a fixed pretrained model's representations are used to train a downstream model on an unrelated task.

In this section, we introduce our approach for addressing the previous challenges.

% \subsection{LLM Probing Data}
% All probing techniques are trained from a dataset of model hidden representations.
% We first introduce how we construct the dataset for our method (as well as other probing baselines).
% % the creation of such a probing dataset and then describe three probing techniques based on how they process sequential data.

% % In order to learn a probe over an LLM for localizing bugs, we need a dataset of hidden representations from the model.
% Hidden representations for probing are extracted from the supervised bug detection dataset $\mathcal{D}$ as defined in Section~\ref{sec:detection}.
% % Existing work on probing often performs a forward pass of a model on a relevant sample and collects hidden representations from one particular layer of interest \citep{alain2016understanding}, but recent work has improved on these techniques by additionally prompting the LLM when providing the relevant input sample since this was found to improve probe performance \citep{repe}.
% We consider the standard auto-regressive decoder-only Transformer architecture \citep{vaswani2017attention} for producing hidden representations.
% The Transformer decoder architecture is described by the following sequence of transformations starting from the sequence of discrete tokens $p = [t_1, \dots, t_T]$ representing program $p$:
% \begin{alignat*}{2}
%     x_0 &= \text{norm}&&(W_e p)\\
%     x_{i} &= x_{i-1} &&+ \text{MHA}(\text{norm}(x_{i-1}))\\
%     & &&+ \text{MLP}(\text{norm}(x_{i-1} + \text{MHA}(\text{norm}(x_{i-1})))))
%     % x_{i+1} &= \text{norm}(x_{i} + \text{MLP}(x_{i})).
% \end{alignat*}
% where MHA and MLP represent the standard transformer multi-head attention and multi-layer perceptron respectively \citep{vaswani2017attention}, and $W_e$ is an initial embedding map.
% The output $x_{l}$ represents the Transformer residual stream representation after the $l$th layer. For our experiments, we collect all outputs $x_{-2}$ where $-2$ is the penultimate layer of the network. We denote this representation as $\text{LLMRep}(p) = x_{-2} \in\mathbb{R}^{T\times d}$ for a program fragment of length $T$ tokens, and a hidden dimension of size $d$.
% Therefore, the dataset of representations for the probing task can be described as the following where $\mathcal{D}$ is our labeled dataset of programs:
% \begin{align*}
%     \text{LLMRep}(\mathcal{D}) = \{(\text{LLMRep}(p), y) : (p, y)\in\mathcal{D}\}.
% \end{align*}

% This dataset is not directly amenable to standard probing since $\text{LLMRep}(p)$ is a sequence of hidden representations, meaning it varies in length across samples.
% Producing a general-purpose fixed length sequence representation from an autoregressive LLM is a challenging problem \cite{liu2023meaning}, so probing methods typically use a simple pooling method, $\text{POOL}:\mathbb{R}^{T\times d}\rightarrow \mathbb{R}^{1\times d}$, such as selecting the last token or averaging all tokens.

% % Based on the pooling method, probes can be largely categorized into last-token probes, mean probes, or attention probes.
% % Last-token probes~\citep{repe} are probes that only consider the representation of the last generated token. As a result, such probes tend to ignore information occurring earlier in the sequence.
% % Mean probing~\citep{repe} tries to address this by mean-pooling the representations of all the generated tokens. However, since this probe considers information aggregated over all the tokens, certain signals may be lost in the pooling process.

% After pooling, any classifier can be trained to detect bugs as a simple classification problem on the pooled representation;
% however, the probe cannot immediately localize bugs.
% As a workaround, one can apply the probe to every prefix of pooled tokens $[t_0,t_1,\ldots{},t_i]$ and localize to the index of the first token $t_i$ where the probe detects a bug.

% Attention probing was proposed in the context of interpretability \citep{tenney2018you, niu2022does} to understand which layers and tokens of an LLM were important for solving an NLP-related task.
% The attention probe additionally
% learns the pooling operation as a weighted sum over a sequence. As a result, the attention probe learns which tokens to attend to for achieving the best classification performance.
% In the next section, we describe our approach for using the concept of attention probes beyond interpretability to the task of fault localization from weak supervision.
% % Unlike other probes, the attention weights from the attention probe can be used to localize a bug without requiring probing all subsequences.
% % Furthermore, unlike other probes, attention probes can use the learned attention weights to directly localize the bug over the entire sequence of generated tokens without needing to apply the probe on smaller subsequences.

% % \aaditya{Add stuff about each kind of probe here}

% \subsection{Weakly-supervised Probing for Fault Localization}\label{sec:ourmethod}
% To learn a probe over a sequence of token representations, we first learn a pooling operation over the sequence of token representations to produce a fixed size representation.
% \adam{I think the following paragraph is the central `novelty' of our method and paper and we should try to get to it as soon as possible.}
We propose the \ourmethodlong{} (\ourmethod{}),
an LLM probing technique for performing FL which provides scalability through lightweight requirements (both in terms of training supervision and model size), interpretable code-level localization, and handling of multi-line bugs.
% , a technique for localizing bugs in code from weak supervision (only knowing which code has bugs rather than their locations).
We take inspiration from attention probing \citep{tenney2018you, niu2022does}, a technique from the interpretability literature for studying linguistic phenomena such as the attention placed on verb tokens, but we use such an approach for code-level FL, as in localizing bugs to lines within code.
% \ourmethod{} is lightweight in that it implicitly learns to perform strong FL using an attention mechanism trained from only coarse-grained bug presence or absence labels, and it can use SLMs and achieve much stronger performance than prompting of the same model.
% Our main insight is that existing techniques loose too much information from last token pooling which is why prompting to increase relevance of the last token increases performance. Instead of relying on prompting, which requires a different prompt for each task, we use a self-attention pooling operation with a special probe token which learns a pooling operation over the other tokens to produce a fixed size representation for the probe token.


We illustrate \ourmethod{}, its process of training from bug detection data, $\mathcal{H}_\text{Det}$, and the computation of line-level bug localization from attention weights in Figure~\ref{fig:overview}.
% \adam{This part needs to be updated. We actually just apply a transformer layer to the sequence rather than this special architecture.}

\subsection{Why Attention Probing for Fault Localization?}
% As discussed in Section~\ref{sec:probing}, bugs can be localized to particular lines of code.
% Taking syntax bugs such as missing closing parentheses as a simple example, there is often a particular line of code where the syntax error originated from, and the rest of the code is irrelevant. For more complex semantic bugs, such as a use-before-initialization error, there is again often a particular location where the bug originates, and the code in these locations have certain patterns \citep{karampatsis2020often, defects4j}.
% To address the challenge of limited strong supervision for FL, the attention mechanism allows us to implicitly learn to localize bugs from supervision on a related task.
% We assume that we only have supervision regarding if a bug is in code rather than where it is in the code.
% Since many bugs are naturally within a small fragment of a larger code block, the rest of the code can be regarded as ``background'' information since it's not particular to the bug.
Our main motivation for using an attention mechanism is that the attention pooling operation trained for the task of bug detection encourages the probe to learn to attend to the location of bugs, without using strong FL supervision.
% As bugs are often naturally ``localized'' within code, there are parts of a large code fragment which carry more information for the task of bug detection than other parts.
Intuitively, the probe attends to parts of the code which are more informative for bug detection, and we hypothesize that these locations often correspond to the location of the bugs.
% Therefore, the attention mechanism trained through the available weak supervision allows us to overcome the strong supervision requirement.

For bugs which are localized to multiple lines, which is the majority of real world faults, an attention mechanism is also helpful since it operates on the token-level, making no distinction between a single and multi-line bug location.

% The attention mechanism also makes multi-line fault localization natural since it operates at the token level, so there is no distinction between single and multi-line bugs.

Finally, an attention mechanism is efficient since it operates over all tokens in parallel. This means that a complete ranking of all the program lines in terms of the likelihood they contain a bug is produced by a single forward pass of attention. This is in contrast to methods such as LLM prompting which must continue to output more tokens to localize a bug to more lines.

% \adam{We don't need this paragraph.}
% This mechanism is not perfect for localizing faults. Some issues include bugs that are highly distributed in a code fragment may be hard for attention to pick up on, attention scales quadratically with sequence length making it inefficient for very long sequences \citep{beltagy2020longformer}, and that attention scores may not always reflect important parts of a sequence \citep{jain-wallace-2019-attention, wiegreffe-pinter-2019-attention}.
% However, we show in our experiments that the use of attention probing is highly effective in practice.

\subsection{From Weak Supervision to Token-level Localization}
% \ourmethod{} learns to perform FL from only weak supervision in the form of bug detection labels.
To enable learning FL from weak supervision, we
use a single layer Transformer decoder block as the architecture of \ourmethod{} to 
factorize the bug detection task into localization (attention) over tokens and detection (classification) on the sequence-level.
% We note that there is no particular reason why the attention weights learned by \ourmethod{} should correspond to buggy code locations as we do not provide any token-level supervision or incentive for this to be the case, but we hypothesize for this to be the case. \arthur{think it may be possible to author a small theorem why this is the case by framing the weak supervision objective as correlated "surrogate" objective which contains similar mappings from feature space to loss space. \hyperlink{https://arxiv.org/pdf/2112.03865}{this paper may have ideas}. Will look into it.}
% In particular, if we add a learnable probe token $z_p$ to a sequence representation $\mathbf{z} = (z_1, \dots, z_T)$ consisting of $T$ token representations $z_t\in\mathbb{R}^{1\times d}$, then a transformer decoder block allows for pooling information from other tokens from the attention mechanism and then selecting from the pooled information using the MLP.
% Another benefit to such an architecture is that it produces the attention scores for all tokens simultaneously whereas existing attention probing methods use architectures such as RNNs to produce the attention scores for each token sequentially which is much less efficient.
% Another benefit to such an architecture is that it matches the underlying architecture of the model, so we are probing the model for the information it has the ability to use \citep{pimentel2022attentional}.
The input to the probe consists of a sequence of token representations from the $k$th layer of the LLM, $\text{LLMRep}(p, k) = [z_1,\dots, z_T]\in\mathbb{R}^{T\times d}$, for a program $p$ consisting of $T$ tokens. The standard multi-head attention mechanism takes $\text{LLMRep}(p, k)$ as input and outputs attention scores $a\in\mathbb{R}^{M\times T\times T}$ where $M$ is the number of attention heads, and processed token representations $v\in\mathbb{R}^{T\times d}$. We compute token-level attention scores of all tokens to the final token as $\bar{a} = \frac{1}{M}\sum_{m=1}^M a_{m,-1,:} \in\mathbb{R}^T$ where we average the attention from each head. The process of producing token-level attention from input code is also shown in lines 1-5 of Algorithm~\ref{alg:ourmethod}.

% One head of the standard self-attention block of the probe then computes attention weights $a_{i,j}$ for how much token $j$ attends to token $i$. We only use $a_{:,-1}\in\mathbb{R}^T$, representing how much the final token attends to all other tokens, in the rest of the probe since we only need $T$ attention scores rather than a matrix of size $T\times T$. Since the probe contains $M$ heads of attention, the full attention output considering only attention to the last token

% Therefore, the final output of the self-attention block after only considering the last input token, is a single token representation $z_T'\in\mathbb{R}^d$ which contains pooled information from the token sequence according to the attention weights.
To get useful token-level attention from our model, we must train it on a downstream task. To train \ourmethod{}, we use weak supervision in the form of a bug detection dataset, $\mathcal{H}_\text{Det}$ defined in Section~\ref{sec:probing}. We pass the last token from the processed sequence $v$ from the attention through a feed-forward network (MLP) to produce a single scalar output representing the buggy/non-buggy prediction.
The model is then optimized using gradient descent with the binary cross-entropy loss.
After training the probe for bug detection, we then can examine the learned attention weights $\bar{a}$ which provide a token-level fault localization.

% \adam{Provide some math the give the computation performed by the probe and also motivate why it is good to do it this way.}

% Rather than directly apply the transformer decoder block to the full representations from the pre-trained model, we first apply a linear projection to reduce the dimensionality $\mathbf{\bar{z}} = (\bar{z}_1, \dots, \bar{z}_T)$ where $\bar{z}_t\in\mathbb{R}^{1\times d'}$, and we also let $\bar{z_p}\in \mathbb{R}^{1\times d'}$. Therefore, our full probe architecture is the following:
% \begin{align*}
%     \text{Probe}(\mathbf{\bar{z}}) = \mathbf{w}_c \text{TF}(\mathbf{\bar{z}}\bar{z_p})_{-1}
% \end{align*}
% %
% where $\text{TF}(\mathbf{\bar{z}}\bar{z_p})_{-1}$ is the resulting representation of the probe token after the transformer layer, and the transformer layer is the standard Transformer decoder:
% \begin{align*}
%     \text{TF}(\mathbf{z}) = \mathbf{z} &+ \text{MHA}(\text{norm}(\mathbf{z}))\\
%     &+ \text{MLP}(\text{norm}(\mathbf{z} + \text{MHA}(\text{norm}(\mathbf{z})))))
% \end{align*}
%
% We expose $\tau$ as the temperature of the softmax operation since we observe that increasing temperature can improve probe performance on some tasks.
% The learnable parameters in the above attention computation is $\mathbf{w}_a, \mathbf{b}\in\mathbb{R}^{1\times d}$.
% Within the probe, the attention weights to the probe token are used to localize the bug.
% Since we only use the representation of the probe token, we simplify the self-attention mechanism to only compute attention for the probe token. In addition, we use a standard multi-head attention mechanism, resulting in attention weights for each head which we average to produce the final localization output. The attention weights are calculated using the standard scaled dot-product attention mechanism with the probe token as the query input:
% \begin{align*}
%     \text{Attention}_i(\mathbf{z}) &= \text{Softmax}\left(\frac{(\bar{z}_pW_q^i)(\mathbf{\bar{z}}W_k^i)^T}{\sqrt{d_k}}\right)\mathbf{\bar{z}}W_v^i\\
%     \text{Attention}(\mathbf{z}) &= \frac{1}{10}\sum_{i=0}^9\text{Attention}_i(\mathbf{z}).
%     % \text{Output} &= \text{Concat}(\text{Attention}_0, \dots, \text{Attention}_9)W_o
% \end{align*}

\subsection{From Token-level to Code-level Localization}
A major challenge with this approach is that tokens are not interpretable for programmers. Therefore, we provide a method to aggregate token-level fault localization into a code-level localization. Our approach is summarized in lines 6 to 8 of Algorithm~\ref{alg:ourmethod}. In our experiments, we focus on line-level granularity, but this method also allows us to perform statement-level and function-level localization without any significant modifications.

\begin{algorithm}[tb]
   \caption{\ourmethod{} Line-level Fault Localization}
   \label{alg:ourmethod}
\begin{algorithmic}[1]
   \STATE {\bfseries Input:} code sample $p$, layer $k$.
   \STATE Tokenize $p$ to get $p=[t_1,\dots, t_T]$.
   \STATE $z = \text{LLMRep}(p, k)$
   \STATE $v, a = \text{MHA}(z)$ where $v\in \mathbb{R}^{T\times d}$ and $a\in\mathbb{R}^{M\times T\times T}$
   % \STATE $b = \text{CLS}(v_{-1}) \in \mathbb{R}$ \COMMENT{Bug detection on the last token}
   \STATE $\bar{a} = \frac{1}{M}\sum_{m=1}^M a_{m,-1,:}$ \COMMENT{Average attention over all heads, for the last token}
   \STATE Group attention scores into lines: $s_1 = [\bar{a}_1, \dots, \bar{a}_i]$, $s_2 = [\bar{a}_{i+1}, \dots, \bar{a}_{i+j}], \dots$
   \STATE $l_i = \sum_{t\in s_i} t$ for all $i$ \COMMENT{Line-level attention score}
   \STATE {\bfseries Return:} $\operatorname{argsort}_{i\in[1,L]} l_i $
\end{algorithmic}
\end{algorithm}

% As described above, we intend to use the attention weights assigned to each token for localizing bugs, but how do we localize to the line-level when attention is at the token level? First, we assume that each input token sequence to the LLM ends with a special ``EOS'' token, so we can extract the attention weights of all tokens to the last token. 

% For our probe with $M$ attention heads and an input with $T$ tokens (including the last ``EOS'' token), the attention weights of each token to the last token are now $a_{:, :,-1}\in \mathbb{R}^{M\times(T-1)}$ (this differs from above where we only considered a single attention head). This is the raw attention scores from the attention block in our probe, so we need to post process these scores to produce a line-level fault localization.

Line 5 of Algorithm~\ref{alg:ourmethod} computes the token-level attention scores, and we call $\bar{a}_i$ the probe's attention score for the $i$th token.
To produce a line-level attention score, we sum the attention scores for all tokens in the $i$th line, $s_i = [\bar{a}_{i_1}, \bar{a}_{i_2}, \dots ]$, to produce $l_i$, the probe's attention score for the $i$th line. The computation of line-level attention scores is shown in line 7 of Algorithm~\ref{alg:ourmethod}.
% \begin{align*}
%     t = \frac{1}{M}\sum_{m=1}^M a^{(m)}, \qquad l_i = \sum_{t_j\in s_i} t_j.
% \end{align*}

Line 8 of Algorithm~\ref{alg:ourmethod} shows that the resulting line-level localization from \ourmethod{} is the ranking of input lines based on their respective attention scores. In practice, we truncate to the top-\( k \) lines out of total lines \( L \).
% \[
%     \text{top-}k = \operatorname{argsort}^{(k)}_{i \in \{1, \dots, L\}} l_i,
% \]
%This yields the \( k \) lines with the highest line-level attention scores. These top-\( k \) lines are considered the most probable locations for bugs as determined by the probe's attention mechanism.The final line-level localization from \ourmethod{} is the ranking of input lines based on their respective attention scores.
The line with the highest attention score is thus noted as the top-1 prediction.



% Finally, since the output of the above attention computation is a single attention weight for every input token, we sum attention weights for tokens in each line to get a single attention per line. The input lines are then ranked by their attention weights for performing bug localization.

% From the attention weights, we learn the detection part of the probe as a linear probe where $\mathbf{w}_d\in\mathbb{R}^{1\times d}$ is a learnable parameter:
% \begin{align*}
%     o = \mathbf{w}_d \cdot \left(\sum_{i=1}^T a_i \mathbf{z}_i\right)^T + \mathbf{b}_d.
% \end{align*}

% We learn $\mathbf{w}_a$ and $\mathbf{w}_d$ using stochastic gradient descent with binary cross-entropy loss on the output, $o$, compared to the binary probe task label. Once these parameters are learned, we can either use both parameters for classification by looking at the sign of $o$, or we can visualize the attention weight $a_t$ for each token as the token importance, which can be used for localizing the probed property to a set of tokens.

% The attention weight acts as a token importance for the probe since the attention pooling operation is a weighted mean, so we can rewrite the probe output as the following by moving $\mathbf{w}_d$ into the sum:
% \begin{align*}
%     o = \sum_{i=1}^T a_i * \left(\mathbf{w}_d\cdot \mathbf{z}_i^T + \frac{\mathbf{b}_d}{T}\right).
% \end{align*}
% Therefore, since the traditional linear probe measures the linearity of a representation's encoding of a task, the above attention pooling operation allows for such linearity to be distributed along a sequence, so that some tokens may linearly encode a task more so than others.

% \adam{HERE}

% \subsection{The S3Bugs Benchmark}

% % \aaditya{Why could we not use current benchmarks -- what did we do to build S3Bugs}

% The benchmark suite needed to train and evaluate a debugging probe has several requirements.
% First, it must contain code samples from multiple sources written for various tasks in order to ensure diversity in the kinds of code samples the probe is being evaluated over.
% Second, the samples must contain different kinds of bugs ranging from simple errors to complex vulnerabilities and must be appropriately labeled.
% And lastly, the samples must be such that bugs of the same type may occur in programs written in different languages.
% Furthermore, the datasets must be balanced and not leak information from the train split into the test split to avoid biases.

% There are several datasets tailored to individual code reasoning tasks, but none of them satisfy all our requirements.
% We, therefore, build \dataset{}, a benchmark suite sourcing data from various code reasoning datasets, to train and evaluate probes for debugging code.
% \dataset{} consists of code samples over three languages containing three major kinds of bugs.

% \textbf{Syntax Errors.}
%     We focus on syntax errors for Python, Java, and C. For Python, we use the GitHub-Python (GitHub) dataset from \citet{yasunaga2021break} consisting of Python functions scraped from Github with and without real syntax errors. For C, we use the DeepFix dataset from \citet{gupta2017deepfix} consisting of C functions submitted to an introductory programming course where some have syntax errors.
%     For Java, we use the dataset from \citet{santos2018syntax} consisting of Java functions scraped from GitHub and categorized into code with and without syntax errors.
%     We further categorize syntax errors into the categories of mismatched parentheses, mismatched brackets, mismatched curly-brackets, Python-specific syntax (indentation, print statements, etc.), Java-specific syntax (misspelled keywords, etc.), and C-specific syntax (undeclared variables, etc.).

% \textbf{Semantic Errors.}
%     We focus on simple, stupid bugs (SStuBs) in Python and Java. For Python, we use the recent TSSB-3M dataset consisting of single line code changes scraped from GitHub.
%     % \adam{Provide further details on how we extracted this dataset}.
%     For Java SStuBs, we use the ManySStuBs4J dataset \citep{karampatsis2020often}. The bugs in these datasets are categorized into 20 types of SStuBs. Following the approach of \citet{richter2022tssb}, we ignore single line changes which only change a string literal since these may be just refactorings rather than bug fixes. Furthermore, $\sim$ 60\% of the TSSB-3M dataset samples are not attributed to any SStuBs category, so we exclude such samples. We thus remove such samples from both datasets. 
%     % , and we choose to subset to the following 3: \adam{Say which SStuB types we use}.
    
% \textbf{Security Vulnerabilities.}
%     We look at two datasets consisting of synthetic security vulnerabilities corresponding to common CWEs in Java and C. For Java, we use the OWASP \citep{owasp} and Juliet-Java dataset \citep{juliet-java}. For C vulnerabilities, we use the Juliet-C dataset \citep{juliet-cpp}. In each dataset, we rename variables and function names to remove indicators of the vulnerability. We also remove comments and rename imports that refer to the dataset names. In total, we consider 11 CWEs from the OWASP and 89 CWEs from the Juliet datasets. We balance the train and test sets by reducing the occurrence of some of the highly frequent CWEs to get an average of 83.3 training and 21.4 testing samples per CWE for the Juliet C dataset, 94.1 training and 23.5 testing samples per CWE for the Juliet Java dataset, and 102.5 training and 26.2 testing samples per CWE for the OWASP dataset.

% A detailed breakdown of our chosen dataset composition is given in Appendix~\ref{app:datasets}.
