\begin{comment}
\section{Full results}
\label{app:full-results}

The full results for CodeLlama 13B with error bars are include in Tables~\ref{tab:syntax-detection}, \ref{tab:syntax-localization}, \ref{tab:sstubs-detection}, \ref{tab:sstubs-localization}, \ref{tab:vuln-detection}, and \ref{tab:vuln-localization}. Error bars represent one standard deviation computed over seeds 0, 1, and 2 where randomness comes from the validation set split and minibatch sampling in SGD. We include error bars for all methods except for the finetuning baselines due to computational and time constraints.

\begin{table}
    \centering
    \caption{Syntax bug detection accuracy.}
    \label{tab:syntax-detection}
    \begin{tabular}{lrrr}
    \toprule
    Method & Python & Java & C \\ 
\midrule
8-shot & 0.77$\pm$0.000 & 0.69$\pm$0.000 & 0.72$\pm$0.000  \\ 
L (Last) & 0.88$\pm$0.009 & 0.98$\pm$0.002 & 0.90$\pm$0.007  \\ 
L (Mean) & 0.80$\pm$0.008 & 0.96$\pm$0.004 & 0.85$\pm$0.022  \\ 
MM & 0.79$\pm$0.006 & 0.95$\pm$0.000 & 0.72$\pm$0.030  \\ 
NL (Last) & 0.89$\pm$0.004 & 0.98$\pm$0.001 & 0.92$\pm$0.010  \\ 
GridLoc & 0.92$\pm$0.013 & 0.98$\pm$0.001 & 0.93$\pm$0.009  \\ 
Finetune & 0.97\phantom{$\pm$0.000} & 0.99\phantom{$\pm$0.000} & 0.98\phantom{$\pm$0.000}  \\ 
LoRA & 0.94\phantom{$\pm$0.000} & 0.99\phantom{$\pm$0.000} & 0.97\phantom{$\pm$0.000}  \\ 
BAP & 0.94$\pm$0.003 & 0.98$\pm$0.002 & 0.96$\pm$0.007  \\ 
\bottomrule
    \end{tabular}
\end{table}

\begin{table}
    \centering
    \caption{Syntax bug localization MRR.}
    \label{tab:syntax-localization}
    \begin{tabular}{lrrr}
    \toprule
    Method & Python & Java & C \\ 
\midrule
8-shot & 0.36$\pm$0.044 & 0.28$\pm$0.078 & 0.19$\pm$0.016  \\ 
L (Last) & 0.38$\pm$0.050 & 0.46$\pm$0.022 & 0.19$\pm$0.005  \\ 
L (Mean) & 0.35$\pm$0.004 & 0.45$\pm$0.006 & 0.11$\pm$0.002  \\ 
MM & 0.46$\pm$0.002 & 0.51$\pm$0.000 & 0.33$\pm$0.003  \\ 
NL (Last) & 0.46$\pm$0.002 & 0.51$\pm$0.002 & 0.31$\pm$0.002  \\ 
GridLoc & 0.78$\pm$0.071 & 0.70$\pm$0.024 & 0.47$\pm$0.139  \\ 
Finetune & 0.45\phantom{$\pm$0.000} & 0.48\phantom{$\pm$0.000} & 0.31\phantom{$\pm$0.000}  \\ 
LoRA & 0.45\phantom{$\pm$0.000} & 0.48\phantom{$\pm$0.000} & 0.30\phantom{$\pm$0.000}  \\ 
BAP & 0.82$\pm$0.092 & 0.64$\pm$0.080 & 0.63$\pm$0.022  \\ 
\bottomrule
    \end{tabular}
\end{table}

\begin{table}
    \centering
    \caption{Single line bug detection accuracy.}
    \label{tab:sstubs-detection}
    \begin{tabular}{lrr}
    \toprule
Method & Python & Java\\ 
\midrule
8-shot & 0.50\phantom{$\pm$0.000} & 0.50\phantom{$\pm$0.000}  \\ 
L (Last) & 0.51$\pm$0.007 & 0.51$\pm$0.003  \\ 
L (Mean) & 0.51$\pm$0.002 & 0.53$\pm$0.009  \\ 
MM & 0.51$\pm$0.001 & 0.51$\pm$0.002  \\ 
NL (Last) & 0.50$\pm$0.000 & 0.50$\pm$0.000  \\ 
GridLoc & 0.56$\pm$0.007 & 0.57$\pm$0.008  \\ 
Finetune & 0.54\phantom{$\pm$0.000} & 0.58\phantom{$\pm$0.000}  \\ 
LoRA & 0.53\phantom{$\pm$0.000} & 0.55\phantom{$\pm$0.000}  \\ 
BAP & 0.55$\pm$0.007 & 0.56$\pm$0.009  \\ 
\bottomrule
    \end{tabular}
\end{table}

\begin{table}
    \centering
    \caption{Single line bug localization MRR.}
    \label{tab:sstubs-localization}
    \begin{tabular}{lrr}
    \toprule
Method & Python & Java\\ 
\midrule
8-shot & 0.10$\pm$0.034 & 0.05$\pm$0.029  \\ 
L (Last) & 0.37$\pm$0.024 & 0.38$\pm$0.028  \\ 
L (Mean) & 0.27$\pm$0.019 & 0.41$\pm$0.002  \\ 
MM & 0.41$\pm$0.004 & 0.45$\pm$0.001  \\ 
NL (Last) & 0.35$\pm$0.001 & 0.44$\pm$0.002  \\ 
GridLoc & 0.47$\pm$0.005 & 0.40$\pm$0.003  \\ 
Finetune & 0.40\phantom{$\pm$0.000} & 0.46\phantom{$\pm$0.000}  \\ 
LoRA & 0.35\phantom{$\pm$0.000} & 0.44\phantom{$\pm$0.000}  \\ 
BAP & 0.49$\pm$0.021 & 0.45$\pm$0.020  \\ 
\bottomrule
    \end{tabular}
\end{table}

\begin{table}
    \centering
    \caption{Security vulnerability bug detection accuracy.}
    \label{tab:vuln-detection}
    \begin{tabular}{lrrr}
    \toprule
%     Method & Java1 & Java2 & C\\ 
% \midrule
% 8-shot & 0.51$\pm$0.000 & 0.56\phantom{$\pm$0.000} & 0.51$\pm$0.000  \\ 
% L (Last) & 0.92\phantom{$\pm$0.000} & 0.70\phantom{$\pm$0.000} & 1.00\phantom{$\pm$0.000}  \\ 
% L (Mean) & 0.91\phantom{$\pm$0.000} & 0.69\phantom{$\pm$0.000} & 1.00\phantom{$\pm$0.000}  \\ 
% MM & 0.88\phantom{$\pm$0.000} & 0.47\phantom{$\pm$0.000} & 0.99\phantom{$\pm$0.000}  \\ 
% NL (Last) & 0.91$\pm$0.000 & 0.69\phantom{$\pm$0.000} & 1.00$\pm$0.000  \\ 
% GridLoc & 0.92\phantom{$\pm$0.000} & 0.96\phantom{$\pm$0.000} & 1.00\phantom{$\pm$0.000}  \\ 
% Finetune & 0.92\phantom{$\pm$0.000} & 0.85\phantom{$\pm$0.000} & 1.00\phantom{$\pm$0.000}  \\ 
% LoRA & 0.91\phantom{$\pm$0.000} & 0.56\phantom{$\pm$0.000} & 1.00\phantom{$\pm$0.000}  \\ 
% BAP & 0.92\phantom{$\pm$0.000} & 0.94\phantom{$\pm$0.000} & 1.00$\pm$0.000  \\ 
Method & Java1 & Java2 & C\\ 
\midrule
8-shot & 0.51$\pm$0.000 & 0.56\phantom{$\pm$0.000} & 0.51$\pm$0.000  \\ 
L (Last) & 0.92$\pm$0.003 & 0.72$\pm$0.028 & 1.00$\pm$0.000  \\ 
L (Mean) & 0.91$\pm$0.004 & 0.71$\pm$0.065 & 1.00$\pm$0.000  \\ 
MM & 0.88\phantom{$\pm$0.000} & 0.48$\pm$0.025 & 0.99$\pm$0.001  \\ 
NL (Last) & 0.92$\pm$0.006 & 0.69$\pm$0.017 & 1.00$\pm$0.000  \\ 
GridLoc & 0.92\phantom{$\pm$0.000} & 0.96\phantom{$\pm$0.000} & 1.00\phantom{$\pm$0.000}  \\ 
Finetune & 0.92\phantom{$\pm$0.000} & 0.85\phantom{$\pm$0.000} & 1.00\phantom{$\pm$0.000}  \\ 
LoRA & 0.91\phantom{$\pm$0.000} & 0.56\phantom{$\pm$0.000} & 1.00\phantom{$\pm$0.000}  \\ 
BAP & 0.92$\pm$0.005 & 0.88$\pm$0.048 & 1.00$\pm$0.000  \\ 
\bottomrule
    \end{tabular}
\end{table}

\begin{table}
    \centering
    \caption{Security vulnerability bug localization MRR.}
    \label{tab:vuln-localization}
    \begin{tabular}{lrr}
    \toprule
    Method & Java1 & C\\ 
\midrule
8-shot & 0.03\phantom{$\pm$0.000} & 0.03\phantom{$\pm$0.000}  \\ 

L (Last) & 0.08$\pm$0.006 & 0.11\phantom{$\pm$0.000}  \\ 
L (Mean) & 0.06$\pm$0.004 & 0.08\phantom{$\pm$0.000}  \\ 
MM & 0.09$\pm$0.000 & 0.17\phantom{$\pm$0.000}  \\ 
NL (Last) & 0.07$\pm$0.013 & 0.09\phantom{$\pm$0.000}  \\ 
GridLoc & 0.09\phantom{$\pm$0.000} & 0.10\phantom{$\pm$0.000}  \\ 
Finetune & 0.09\phantom{$\pm$0.000} & 0.19\phantom{$\pm$0.000}  \\ 
LoRA & 0.09\phantom{$\pm$0.000} & 0.19\phantom{$\pm$0.000}  \\ 
BAP & 0.13\phantom{$\pm$0.000} & 0.16\phantom{$\pm$0.000}  \\ 
\bottomrule
    \end{tabular}
\end{table}
\end{comment}

\section{Additional Experimental Details}
\label{app:experiment-details}

\subsection{Datasets}
\label{app:datasets}

The detailed breakdown of each of the datasets we used, other than Defects4J. We split datasets into groups based on the three domains of syntax, single line bugs, and vulnerabilities and show the breakdown in Table~\ref{tab:syntax-datasets}, \ref{tab:sstubs-datasets}, and \ref{tab:vuln-datasets} respectively.

\begin{table}[h]
    \centering
    \caption{Number of samples in syntax datasets.}
    \label{tab:syntax-datasets}
    \begin{tabular}{lrrrrrr}
    \toprule
         Subtype & \multicolumn{2}{c}{GitHub-Python} & \multicolumn{2}{c}{GitHub-Java} & \multicolumn{2}{c}{DeepFix}\\
         \cmidrule(lr){2-3}\cmidrule(lr){4-5}\cmidrule(lr){6-7}
         & Train & Test & Train & Test & Train & Test\\
         \midrule
         Correct syntax & 1323 & 400 & 1370 & 460 & 1475 & 365\\
         Mismatched parentheses & 400 & 100 & 110 & 100 & 400 & 100\\
         Mismatched bracket & 368 & 100 & 60 & 60 & 81 & 31\\
         Mismatched brace & 155 & 100 & 400 & 100 & 195 & 34 \\
         Missing semicolon & --- & --- & 400 & 100 & 400 & 100 \\
         % Invalid syntax & 400 & 100 & 400 & 100 & --- & ---\\
         Python-specific & 400 & 100 & --- & --- & --- & ---\\
         Java-specific & --- & --- & 400 & 100 & --- & --- \\
         C-specific & --- & --- & --- & --- & 400 & 100 \\
         \midrule
         Total & 2646 & 800 & 2740 & 920 & 2950 & 730\\
    \bottomrule
    \end{tabular}
\end{table}

\begin{table}[h]
    \centering
    \caption{Number of samples in SStuBs datasets.}
    \label{tab:sstubs-datasets}
    \begin{tabular}{lrrrr}
    \toprule
         Subtype & \multicolumn{2}{c}{TSSB} & \multicolumn{2}{c}{ManySStuBs}\\
         \cmidrule(lr){2-3}\cmidrule(lr){4-5}
         & Train & Test & Train & Test\\
         \midrule

% CHANGE_IDENTIFIER 400 100
% CHANGE_NUMERAL 400 100
% CHANGE_OPERATOR 400 100
% CHANGE_UNARY_OPERATOR 261 100
% LESS_SPECIFIC_IF 362 100
% MORE_SPECIFIC_IF 400 100
% OVERLOAD_METHOD_DELETED_ARGS 400 100
% OVERLOAD_METHOD_MORE_ARGS 400 100
% PYTHON 400 100
% SWAP_ARGUMENTS 92 92
% SWAP_BOOLEAN_LITERAL 400 100
No bug & 3745 & 1080 & 3821 & 1093\\
Change identifier & 400 & 100 & 400 & 100\\
Change numeral & 400 & 100 & 400 & 100\\
Change binary operator & 400 & 100 & 400 & 100\\
Change unary operator & 202 & 100 & 301 & 100\\
Less specific if & 281 & 100 & 400 & 100\\
More specific if & 400 & 100 & 400 & 100\\
Same function less args & 400 & 100 & 266 & 100\\
Same function more args & 400 & 100 & 400 & 100\\
Swap arguments & 81 & 80 & 94 & 93\\
Swap boolean literal & 381 & 100 & 360 & 100\\
Python specific & 400 & 100 & --- & ---\\
Java specific & --- & --- & 400 & 100\\
\midrule
Total & 7830 & 2184 & 7642 & 2186\\
    \bottomrule
    \end{tabular}
\end{table}

% \begin{table}[h]
%     \centering
%     \caption{Number of samples in security vulnerabilities datasets.}
%     \label{tab:vuln-datasets}
%     \begin{tabular}{lrrrrrr}
%     \toprule
%          CWE Class & \multicolumn{2}{c}{Java1} & \multicolumn{2}{c}{Java2} & \multicolumn{2}{c}{C}\\
%          & Train & Test & Train & Test & Train & Test\\
%          \midrule
%          Access Control & 677 & 178 & --- & --- & 609 & 159 \\
% Comparison & 38 & 11 & --- & --- & 14 & 4 \\
% Concurrency & 21 & 11 & --- & --- & 172 & 48 \\
% Encryption & 616 & 154 & 232 & 62 & 278 & 72 \\
% Exposed Resource & 574 & 141 & --- & --- & 934 & 236 \\
% File Handling & 1056 & 256 & 108 & 26 & --- & --- \\
% Implementation & 104 & 27 & --- & --- & 144 & 37 \\
% Improper Check or Handling of Exceptional Conditions & 37 & 10 & --- & --- & 604 & 152 \\
% Improper Input Validation & 523 & 128 & --- & --- & 304 & 76 \\
% Improper Neutralization & --- & --- & --- & --- & 60 & 16 \\
% Incorrect Calculation & 40 & 10 & --- & --- & 99 & 27 \\
% Injection & 2368 & 571 & 554 & 133 & 304 & 76 \\
% Insufficient Control Flow Management & 161 & 49 & --- & --- & 259 & 69 \\
% Memory Safety & 342 & 86 & --- & --- & 847 & 217 \\
% Poor Coding Practices & 665 & 175 & --- & --- & 1820 & 465 \\
% Protection Mechanism Failure & --- & --- & --- & --- & 28 & 8 \\
% Randomness & 52 & 14 & 186 & 50 & 14 & 4 \\
% Resource Control & 534 & 126 & --- & --- & --- & --- \\
% Resource Lifecycle Management & 80 & 22 & 47 & 17 & 841 & 215 \\
% Sensitive Information Exposure & 112 & 31 & --- & --- & 84 & 24 \\
%     \bottomrule
%     \end{tabular}
% \end{table}
\begin{table}[h]
    \centering
    \caption{Number of samples in security vulnerabilities datasets.}
    \label{tab:vuln-datasets}
    \begin{tabular}{lrrrr}
    \toprule
         CWE Class & \multicolumn{2}{c}{Juliet-Java} & \multicolumn{2}{c}{Juliet-C}\\
         \cmidrule(lr){2-3}\cmidrule(lr){4-5}
         & Train & Test & Train & Test\\
         \midrule
         Access Control & 677 & 178 & 609 & 159 \\
Comparison & 38 & 11 & 14 & 4 \\
Concurrency & 21 & 11 & 172 & 48 \\
Encryption & 616 & 154 & 278 & 72 \\
Exposed Resource & 574 & 141 & 934 & 236 \\
File Handling & 1056 & 256 & --- & --- \\
Implementation & 104 & 27 & 144 & 37 \\
Improper Check or Handling of Exceptional Conditions & 37 & 10 & 604 & 152 \\
Improper Input Validation & 523 & 128 & 304 & 76 \\
Improper Neutralization & --- & ---  & 60 & 16 \\
Incorrect Calculation & 40 & 10 & 99 & 27 \\
Injection & 2368 & 571 & 304 & 76 \\
Insufficient Control Flow Management & 161 & 49 & 259 & 69 \\
Memory Safety & 342 & 86 & 847 & 217 \\
Poor Coding Practices & 665 & 175 & 1820 & 465 \\
Protection Mechanism Failure & --- & --- & 28 & 8 \\
Randomness & 52 & 14 & 14 & 4 \\
Resource Control & 534 & 126  & --- & --- \\
Resource Lifecycle Management & 80 & 22 & 841 & 215 \\
Sensitive Information Exposure & 112 & 31 & 84 & 24 \\
    \bottomrule
    \end{tabular}
\end{table}

\subsection{Few-shot Prompts}
\label{app:prompts}
For all prompting experiments, we use zero-shot prompting with the prompt given below:

\begin{lstlisting}[frame=single]
Q: Please analyze the following code snippet for potential bugs. Return the fault
localization result in JSON format, consisting of five JSON objects called
"faultLocalization". These "faultLocalization" objects correspond to the top
five most suspicious lines of code. Each "faultLocalization" contains two
fields: 1) "codeContent" string which contains the line of code that corresponds
to suspicious code in the snippet and 2) "lineNumber" integer which indicates the
line number of this suspicious code. Output just the JSON objects
"faultLocalization" and NOTHING ELSE.
```
{code-example}
```
\end{lstlisting}


% Prompt for the SStuBs data:
% \begin{lstlisting}
% Q: Consider if there are any bugs in the following {language} program and output
% the single word 'Yes' or 'No'.
% ```
% {code-example}
% ```
% A: 
% \end{lstlisting}

% Prompt for syntax data:
% \begin{lstlisting}
% Q: Consider if there are any syntax errors in the following {args.language}
%    program and output the single word 'Yes' or 'No'.
% ```
% {code-example}
% ```
% A: 
% \end{lstlisting}

% Prompt for vulnerabilities data:
% \begin{lstlisting}
% Q: Consider if there are any vulnerabilities in the following {args.language}
%    program and output the single word 'Yes' or 'No' 
% ```
% {code-example}
% ```
% A:
% \end{lstlisting}

% For the localization experiments, we additionally add line numbers to the few shot examples so that the LLM also responds with a line number. For example, we the response given in a few shot example for a program with a bug on line 5 will be "Yes, Line: 5".

\subsection{Hyperparameters}
\label{app:hyperparams}

For \ourmethod{}, we trained for 30 epochs with a learning rate of \num{1e-4}, a batch size of 16, and weight decay of 1 for all datasets except for the TSSB dataset where we needed to use less training epochs to avoid overfitting. For TSSB, we trained for 5 epochs with a learning rate of \num{1e-4}, a batch size of 16, and weight decay of 1. For the architecture of \ourmethod{}, we used grouped query attentino with 32 query heads and 8 key-value heads to match the architecture of the Llama-3.2-11B attention mechanism. For the ablation of Llama-3.2-90B, we used 64 query heads with 8 key-value heads.

For the linear probing baseline and GridLoc, we trained for 30 epochs with a learning rate of \num{1e-4}, a batch size of 16, and weight decay of 0.1 for all datasets.

% For all experiments and all baselines we use a learning rate of 0.0001, a batch size of 8, and train for 20 epochs.
% We use the same hyperparameters for \ourmethod{} as well, except for the learning rate which we set at 0.001.

Parameters were chosen by splitting the training set with an 80/20 split into train and validation samples, and selecting hyperparameters from the results on the validation set.

\subsection{Compute resources}
\label{app:compute}

All experiments are conducted on a server with 96 Intel Xeon Gold 6248R CPUs, each with a clock speed of 3.00 GHz, and 8 NVIDIA A100 GPUs, each with a capacity of 40GB.

The WELL baseline is the most compute intensive of the methods we explore because it requires fintuning an LLM.
To finetune Llama-3.2-11B, we had to use LoRA \citep{hu2021lora} with rank 16 to make training this model accessible. GridLoc takes around twice the training time as \ourmethod{}, and all the other baselines take 2-3 minutes for one training run on a single dataset.

% Finetuning (with or without LoRA) on one dataset takes approximately three hours for CodeLlama-13B when run on 4 GPUs. Finetuning Llama-3 8B takes about 2 hours per experiment, but we can run on 2 GPUs allowing for parallel runs. For \ourmethod{}, we train 3-4 instances on a single GPU in parallel and one training run take 30 minutes on average. GridLoc takes around twice the training time as \ourmethod{}, and all the other baselines take 2-3 minutes for one training run on a single dataset.

\begin{comment}
\section{Additional Experiments}
\subsection{Llama-3 8B}

We include additional results for Llama-3 8B in Table~\ref{tab:syntax-detection-3}, \ref{tab:sstubs-detection-3}, \ref{tab:vuln-detection-3}, and \ref{tab:syntax-localization-3}.

\begin{table}
    \centering
    \caption{Syntax bug detection accuracy.}
    \label{tab:syntax-detection-3}
    \begin{tabular}{lrrr}
    \toprule
    Method & Python & Java & C \\ 
\midrule
8-shot & 0.82\phantom{$\pm$0.000} & 0.88\phantom{$\pm$0.000} & 0.73\phantom{$\pm$0.000}  \\ 
L (Last) & 0.88$\pm$0.007 & 0.98$\pm$0.002 & ---\phantom{$\pm$0.000}  \\ 
L (Mean) & 0.81$\pm$0.002 & 0.95$\pm$0.001 & ---\phantom{$\pm$0.000}  \\ 
MM & 0.70$\pm$0.002 & 0.93$\pm$0.000 & ---\phantom{$\pm$0.000}  \\ 
NL (Last) & 0.88$\pm$0.009 & 0.98$\pm$0.002 & ---\phantom{$\pm$0.000}  \\ 
Finetune & 0.59\phantom{$\pm$0.000} & 0.59\phantom{$\pm$0.000}  \\ 
LoRA & 0.54$\pm$0.000 & 0.57$\pm$0.000  \\ 
GridLoc & 0.92$\pm$0.011 & ---\phantom{$\pm$0.000} & 0.91$\pm$0.005  \\ 
BAP & 0.94$\pm$0.008 & 0.98$\pm$0.004 & 0.94$\pm$0.009  \\ 
\bottomrule
    \end{tabular}
\end{table}

\begin{table}
    \centering
    \caption{Syntax bug localization MRR.}
    \label{tab:syntax-localization-3}
    \begin{tabular}{lrrr}
    \toprule
    Method & Python & Java & C \\ 
\midrule
8-shot & --- & --- & ---  \\ 
L (Last) & 0.51$\pm$0.004 & 0.41$\pm$0.010 & ---  \\ 
L (Mean) & 0.45$\pm$0.001 & 0.40$\pm$0.011 & ---  \\ 
MM & 0.48$\pm$0.003 & 0.37$\pm$0.000 & ---  \\ 
NL (Last) & 0.47$\pm$0.020 & 0.41$\pm$0.006 & ---  \\ 
GridLoc & --- & --- & 0.26$\pm$0.006  \\ 
Finetune & --- & 0.37\phantom{$\pm$0.000} & 0.22\phantom{$\pm$0.000}  \\ 
LoRA & --- & --- & ---  \\ 
BAP & 0.59\phantom{$\pm$0.000} & 0.43$\pm$0.031 & 0.29$\pm$0.021  \\ 
\bottomrule
    \end{tabular}
\end{table}

\begin{table}
    \centering
    \caption{Single line bug detection accuracy.}
    \label{tab:sstubs-detection-3}
    \begin{tabular}{lrr}
    \toprule
    Method & Python & Java\\ 
\midrule
8-shot & 0.52\phantom{$\pm$0.000} & 0.50\phantom{$\pm$0.000}  \\ 
L (Last) & 0.53$\pm$0.004 & 0.52$\pm$0.007  \\ 
L (Mean) & 0.52$\pm$0.009 & 0.53$\pm$0.010  \\ 
MM & 0.51$\pm$0.005 & 0.50$\pm$0.000  \\ 
NL (Last) & 0.51$\pm$0.012 & 0.50$\pm$0.005  \\ 
Finetune & 0.59\phantom{$\pm$0.000} & 0.59\phantom{$\pm$0.000}  \\ 
LoRA & 0.54$\pm$0.000 & 0.57$\pm$0.000  \\ 
GridLoc & 0.55$\pm$0.006 & 0.58$\pm$0.002  \\ 
BAP & 0.54$\pm$0.014 & 0.54$\pm$0.009  \\ 
\bottomrule
    \end{tabular}
\end{table}

% \begin{table}
%     \centering
%     \caption{Single line bug localization MRR.}
%     \label{tab:sstubs-localization-3}
%     \begin{tabular}{lrr}
%     \toprule
%     \end{tabular}
% \end{table}

\begin{table}
    \centering
    \caption{Security vulnerability bug detection accuracy.}
    \label{tab:vuln-detection-3}
    \begin{tabular}{lrrr}
    \toprule
    Method & Java1 & Java2 & C\\ 
\midrule
8-shot & ---\phantom{$\pm$0.000} & ---\phantom{$\pm$0.000} & ---\phantom{$\pm$0.000}  \\ 
L (Last) & 0.89$\pm$0.004 & 0.56$\pm$0.010 & 1.00$\pm$0.000  \\ 
L (Mean) & 0.89$\pm$0.000 & 0.60$\pm$0.000 & 1.00$\pm$0.000  \\ 
MM & 0.51$\pm$0.000 & 0.51$\pm$0.000 & 0.95$\pm$0.000  \\ 
NL (Last) & 0.91$\pm$0.000 & 0.57$\pm$0.040 & 1.00$\pm$0.000  \\ 
Finetune & 0.93\phantom{$\pm$0.000} & 1.00\phantom{$\pm$0.000} & ---\phantom{$\pm$0.000}  \\ 
LoRA & 0.92$\pm$0.000 & 0.64$\pm$0.000 & 1.00$\pm$0.000  \\ 
GridLoc & 0.91$\pm$0.000 & 0.91$\pm$0.000 & 1.00$\pm$0.000  \\ 
BAP & 0.91$\pm$0.007 & 0.76$\pm$0.117 & 1.00$\pm$0.000  \\ 
\bottomrule
    \end{tabular}
\end{table}

% \begin{table}
%     \centering
%     \caption{Security vulnerability bug localization MRR.}
%     \label{tab:vuln-localization-3}
%     \begin{tabular}{lrr}
%     \toprule
%     \end{tabular}
% \end{table}

% \subsection{Throughput}

% \begin{table}[]
%     \centering
%     \caption{Throughput of methods for detecting bugs in C code.}
%     \label{tab:my_label}
%     \begin{tabular}{lr}
%     \toprule
%          Method &  Throughput (samples/s)\\
%          \midrule
%          GCC & 27.2\\
%          Finetuned CodeLlama 13B & 16.1 \\
%          GridLoc & 162.5\\
%          Attention & 256.9\\
%     \bottomrule
%     \end{tabular}
% \end{table}
\end{comment}

\section{Additional Results}\label{app:results}
\subsection{Error Bars for FL Results}
Error bars for the results in Table~\ref{tab:defects4j} are provided in Table~\ref{tab:defects4j-errors}. The prompting methods have no error bars because we use greedy decoding.

\begin{table*}[t]
    \centering
    \tiny
    \caption{Comparison of \ourmethod{} with existing fault localization methods across eight diverse bug benchmarks. We evaluate each method on line-level localization performance at the method-level, measured by top-1 localization accuracy. From left-to-right: Defects4J v1.2.0, GitHub-Python, GitHub-Java, DeepFix, TSSB-3M, ManySStuBs4J, Juliet-Java, and Juliet-C.}
    \label{tab:defects4j-errors}
        % \setlength{\tabcolsep}{12pt} % Adjust column spacing
        \begin{tabular}{lrrrrrrrr}
        \toprule
        \textbf{Method} & \textbf{D4J} & \textbf{GH-Py} & \textbf{GH-J} & \textbf{DeepFix} & \textbf{TSSB} & \textbf{MS4J} & \textbf{Juliet-J} & \textbf{Juliet-C} \\
        \midrule
        Random & 0.144 & 0.100 & 0.134 & 0.038 & 0.069 & 0.124 & 0.025 & 0.058\\
        \midrule
        DeepFL & 0.144 & \na & \na & \na & \na & \na & \na & \na\\      
        SmartFL & 0.158 & \na & \na & \na & \na & \na & \na & \na\\
        TRANSFER-FL & 0.218 & \na & \na & \na & \na & \na & \na & \na\\
        \midrule
        CodeLlama-70B & 0.212 & 0.145 & 0.316 & 0.084 & 0.077 & 0.169 & 0.038 & \underline{0.095}\\
        Llama-3.3-70B & 0.269 & 0.225 & 0.272 & 0.092 & 0.114 & 0.211 & 0.072 & 0.040\\
        Qwen2.5-72B & 0.157 & 0.333 & 0.289 & 0.124 & 0.088 & 0.194 & 0.061 & 0.040\\
        DeepSeek-R1-Distill-Llama-70B & 0.221 & 0.188 & 0.218 & 0.035 & 0.138 & 0.185 & 0.041 & 0.025\\
        GPT-4o & 0.249 & 0.375 & 0.365 & 0.097 & 0.089 & 0.240 & 0.009 & 0.026\\
        \midrule
        Linear Probe Llama-3.2-11B & 0.279$\pm$0.02 & 0.373$\pm$0.01 & 0.300$\pm$0.01 & 0.140$\pm$0.01 & 0.202$\pm$0.01 & 0.235$\pm$0.01 & 0.048$\pm$0.00 & 0.043$\pm$0.01\\
        LLMAO-CodeGen & 0.223 & \na & \na & \na & \na & \na & \na & \na\\
        LLMAO-Llama-3.2-11B & 0.144 & 0.190 &  & 0.078 &  &  &  & \\
        WELL-CodeBERT & 0.090 & \textbf{0.575} & \underline{0.532} & 0.129 & 0.094 & 0.111 & \textbf{0.216} & 0.059\\
        WELL-Llama-3.2-11B & 0.236 & 0.028 & 0.139 & 0.000 & 0.054 & 0.081 & 0.000 & 0.000\\
        GridLoc-Llama-3.2-11B & \underline{0.291$\pm$0.02} & 0.498$\pm$0.08 & 0.206$\pm$0.08 & \underline{0.332$\pm$0.03} & \textbf{0.262$\pm$0.03} & \textbf{0.339$\pm$0.03} & \underline{0.158$\pm$0.04} & 0.039$\pm$0.01\\
        \midrule
        \ourmethod{}-Llama-3.2-11B & \textbf{0.334$\pm$0.02} & \textbf{0.575$\pm$0.02} & \textbf{0.568$\pm$0.01} & \textbf{0.481$\pm$0.04} & \underline{0.237$\pm$0.02} & \underline{0.291$\pm$0.04} & 0.096$\pm$0.03 & \textbf{0.217$\pm$0.00}\\
        \bottomrule
        \end{tabular}
\end{table*}

% \section{Broader impacts}
% \label{app:impacts}

% The purpose of developing tools for debugging code is to limit the potential for negative societal impacts caused by unintended behavior of human-written and LLM generated code. We believe our work can help reduce some of the negative consequences of LLMs which can write and execute arbitrary code by providing a method for auditing such code as it is generated. We also hope that \ourmethod{} can aid developers in discovering bugs in their code, resulting in improved code correctness.

% We do not believe there are any negative societal impacts of such a debugging tool.

\section{Precision at $k$}\label{app:precision}
The precision at $k$ (P@$k$) metric which we use is calculated as:
\begin{align*}
    \frac{\text{Correct in top $k$}}{\min(k, \text{Max possible correct})}.
\end{align*}
We use the min in the denominator to account for the case where the number of buggy lines is much fewer than $k$. This is practically relevant since many bugs consist of only 2-3 buggy lines which is less than $k$ for P@5.