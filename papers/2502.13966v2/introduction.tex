\section{Introduction}

% https://drive.google.com/file/d/1-ENi2hxY0Tz3v9dmWctk2PMyteguKQVP/view?usp=sharing
\begin{figure}[t]
    \centering    \includegraphics[width=\linewidth]{figures/bap_example.pdf}
    \caption{Comparison of our approach \textit{\ourmethodlong{}} (\ourmethod{}) with baselines DeepFL and LLM prompting on a  Java program snippet. The program has two bugs: the age condition on line 3 is reversed and line 6 throws a null pointer exception. \ourmethod{} correctly localizes both bugs. Here, our method is trained on Llama-3.2-1B, a ``small" language model (SLM), with only weak supervision \textit{i.e.} binary bug presence labels. Obtaining comparable accuracy via prompting demands a significantly more resource-intensive LLM, such as Llama-3.2-90B, or even larger. Previous approaches to fault localization like DeepFL require executable test cases before they can attempt to provide useful information.}
    \label{fig:splash}
    \vspace{-0.1in}
\end{figure}

% Fault localization is an important problem for programmers
Correctness is a fundamental desirable property of code.
Both human-written and LLM-generated code are prone to bugs \cite{karampatsis2020often, jesse2023large} including
syntax errors that prevent the execution of code, semantic mistakes that cause incorrect or unintended behaviors, and vulnerabilities that compromise security in otherwise correct code.
While there are various methods for detecting bugs (e.g. failed tests, via human bug report, program crash, etc.), identifying its root cause, or \textit{localizing} the bug, is still costly \citep{vessey1985expertise, flsurvey}.
% Tools to automate bug localization and the rest of the debugging process have to potential to save enormous developer effort.

% Automated FL methods can actually help, but they have requirements
% This is one alternative
% Automated software fault localization (FL) aims to help a programmer answer the question, ``Where's the bug?'', ideally pointing to specific lines of buggy code.
% However, existing approaches to FL are limited by their lack of \textit{scalability}, or their ability to be used in practical scenarios. This is because they either require executable tests which are often unavailable, or the largest of LLMs which are expensive, especially for large code bases and repeated use. In this paper, we propose ...

% Approaches to automating FL using symbolic reasoning have been successfully applied to identify bugs, even in large and mature codebases \citep{ayewah2008using, manes2019art}, but these methods require significant customization such as executable tests \cite{usestaticanalysis}.
Automated software fault localization (FL) aims to help a programmer answer the question, ``Where's the bug?'', ideally pointing to specific lines of buggy code.
The traditional FL approaches rely on executable tests to determine the buggy lines \cite{usestaticanalysis}.
Without relying on tests, FL is even more challenging since such a system must reason about what is buggy without external feedback. But recently,
supervised training of models on large, labeled datasets \citep{llmao, transferfl}, and prompting of the largest LLMs such as GPT-4o has shown promise at FL without tests \citep{wu2023large}.
On a single method context, large-scale supervised training and prompting approaches can significantly surpass the traditional techniques which need tests \citep{llmao, wu2023large}.
% , but they lack the applicability of traditional methods since strong supervision is rarely available for training-based FL methods \citep{well} and effective prompting requires costly LLMs, especially for large projects where at least one LLM call is required for each function/method \citep{wu2023large}.

% The problem is scalability
%The main problem with existing

% State-of-the-art FL methods, however, are still limited in \textit{scalability}\footnote{This differs from another use of \textit{scalable} meaning the ability to handle large datasets.}, the ability to accommodate many practical, real-world settings.
State-of-the-art FL methods, however, are still limited in \textit{scalability}, or the ability to leverage cheaply available supervision to reach strong performance, even with small models. This lack of scalability leads to the impracticality of many FL techniques.
For example, we show a simple buggy Java code snippet in Figure~\ref{fig:splash} which we want to run FL on, but we encounter several issues: traditional methods require executable tests and LLM prompting methods are only effective for the largest of models. On large codebases, LLMs must be called at least once per function, which quickly becomes expensive. Further, training-based approaches assume extensive amounts of strong supervision for FL, which is rarely available in practice \citep{well}.

% In this scenario, traditional methods which code execution are entirely inapplicable and do not provide any useful information. LLM prompting approaches must call the model at least once for each method, which becomes costly for the largest models, and too slow for test-time reasoning models. Finally, training-based approaches need large amounts of strong supervision which is rarely available \citep{well}.


% As LLMs evolve, the largest models, such as GPT-4, achieve state-of-the-art FL at the function-level, without supervision or customization \citep{wu2023large}. Despite this, prompting methods for FL still leave much to be desired in terms of performance and costs.

% Symbolic reasoning approaches to FL such as static analysis \citep{ayewah2008using} and fuzzing \cite{manes2019art} have been successfully applied to detect bugs in even large and mature codebases. Recently, deep learning methods have even been used to improve such approaches \citep{smartfl}.
% However, due to the undecidable nature of the program analysis problem \citep{landi1992undecidability}, these approaches require significant customization to achieve acceptable accuracy and scalability \cite{usestaticanalysis}.
% As such, when used off-the-shelf, they have limited effectiveness at finding bugs in even relatively small fragments of code \cite{karampatsis2020often}.
% Methods from deep learning have been used to improve these symbolic reasoning approaches, such as smart ensembling methods \citep{deepfl} and even training deep models for FL using line-level supervision \citep{transferfl, llmao}, but line level supervision is often unavailable or costly to obtain.

%Even for decidable properties such as syntax checking, the computation patterns of these approaches are at odds with those in LLM inference pipelines, making them a poor fit for real-time bug detection in code generated by LLM-powered tools \cite{}.

% LLMs are good at understanding code, work with small code-snippets instead of large codebases, and scale well using GPUs

% On the other hand, LLMs have recently become strong at diverse code generation and understanding tasks including bug detection \cite{macneil2024decoding,alrashedy2023language} and even bug repair \citep{agentless} without any additional customization such as specifications, scripts to compile and execute programs, or test cases.
% Through zero-shot prompting, the best LLMs are even state-of-the-art on FL within a method-level context \citep{wu2023large}.
% In addition, LLMs can leverage information
% learned from their large-scale pre-training which is not available to smaller supervised models.
% which normally must be learned with a lot of expensive labels in supervised bug detection and localization approaches.
% machine learning based bug localization approaches all rely on line-level bug supervision which is often noisy and expensive to clean \citep{defects4j}.
% Prompting-based LLM approaches to bug detection require significant engineering and often yield lower accuracy than LLM adaptation approaches such as finetuning.
% On the other hand, adapting an LLM requires adequate code snippets with bug labels, and is prone to overfitting.
% Furthermore, localizing those bugs requires even more fine-grained supervision, such as annotations of the line where the bug occurs.

% talk about probes
% probing is an NLP technique that has been used to understand the inner workings of LLMs -- explain in one sentence what probing is
% how it works etc -- probing is more efficient than finetuning since you don't need the full power of the LLM to detect bugs --
% probing is also more generalizable than prompting since you can probe for a variety of bugs in a single model

% Introduce the method of probing for finding bugs
% This leads us to a natural question: can we elicit the knowledge to predict and localize bugs from an LLM in a manner that is more accurate than prompting but more lightweight and generalizable than finetuning?
This leads to our central question: 
\textit{How do we achieve strong fault localization performance without relying on executable tests, costly large-scale LLMs, or strong supervision?}
%In other words, \textit{how do we enable scalable fault localization?}%, allowing for scalable FL?}
% can we elicit fault localization from small language models (SLMs) without using strong supervision, allowing for scalable FL?

% can we localize bugs without impractical tool customization or costly line-level supervision by eliciting such localization capabilities from an LLM?
% in a manner which is more accurate than prompting but more lightweight and generalizable than finetuning?

% Introduce our method
We answer this question by proposing the \ourmethodlong{} (\ourmethod{}), a scalable LLM probing technique for FL, scaling to use available bug related data without strong FL supervision and scaling with base model size while still achieving strong performance with small models. \ourmethod{} exhibits three desirable properties:
% of scalable fault localization:
(1) \textbf{lightweight}, (2) \textbf{code-level localization}, and (3) localization of \textbf{multi-line bugs}. First, \textit{lightweight} refers to the limited requirement for supervision (we use an attention mechanism to learn from binary bug presence rather than fine-grained location supervision), the test-free nature, and the model size (\ourmethod{} can probe small language models (SLMs) to elicit performance significantly stronger than the underlying SLM). Second, \ourmethod{} localizes bugs in a human-interpretable manner to expressions, statements, or lines in code, even though LLMs operate on the token level. Third, \ourmethod{} localizes multi-line bugs, or multiple bugs in one method, better than existing approaches. Multi-line bugs are practically relevant since the majority of real bugs are multi-line \citep{pearson2016evaluating}.
% the majority of real-world bugs are localized to multiple lines \citep{pearson2016evaluating}, and \ourmethod{} localizes multi-line bugs without any special training better than existing approaches.
% \begin{description}
%     \item[Lightweight] 
%     \item[Code-level Localization]
%     \item[Multi-line Bug Detection]
% \end{description}


% In this paper, we answer this question by utilizing {\em probing}, a technique for analyzing a model's ability to solve auxiliary tasks it was not directly trained for \cite{alain2016understanding}.
% % Probing has been extensively used in natural language processing as a tool for studying how well language models have learned natural language linguistic concepts, such as subject-verb agreement.
% % Similarly, we are interested in probing for correctness concepts of programs, ranging from simple syntax errors like mismatched parentheses to complex semantic issues like security vulnerabilities.
% Specifically, we are interested in probing for an LLM's ability to localize bugs within code, even if the same underlying model performs the task poorly when prompted.



% % Insufficiency of naive probing
% Typical probing techniques train linear classifiers on the last token or mean token representation from an LLM with binary labels \citep{meng2022locating,gurnee2023finding, repe}.
% While such an approach can be effectively applied for bug \textit{detection} (i.e. answering ``Is there a bug?''), how can we determine \textit{where} a bug occurs?
% % We therefore propose \textit{\textbf{We}akly-trained \textbf{A}ttention for \textbf{V}alue \textbf{E}xtraction} or \ourmethod{} which leverages an attention mechanism in the architecture of the probe.
% We therefore propose the \textit{\ourmethodlong{}} (\ourmethod{}) which leverages an attention mechanism in its architecture to elicit bug localization, without explicit labels.
% We train \ourmethod{} using a dataset with only function-level ``buggy'' or ``clean'' labels to achieve state-of-the-art FL performance at the line-level.
% % and we demonstrate that aggregating the learned token attention weights at the line-level results in

% Probing is a technique that is used to analyze models in terms of the expressiveness of their learned representations for solving auxiliary tasks \citep{alain2016understanding}.
% in terms of \textit{concepts} learned by the model.
% To probe an LLM, a classifier is typically trained over the model's internal token representations to solve some task which the LLM was not directly trained for.
% While such probes are not as accurate as finetuning an LLM, probing results in better generalization \citep{kumar2021fine} and can be significantly more efficient.
% Probing has been used to quantify the extent to which LLMs learn linguistic concepts, such as syntactic constructs \citep{hewitt2019structural}, semantics, and truthfulness \citep{azaria2023internal}.
% Such linguistic concepts are analogous to the code correctness concepts mentioned earlier.
% We therefore turn to probing as a solution for bug detection and localization in LLM-generated code.


% Typical probing techniques use linear classifiers to probe LLMs.
% While such linear probes can effectively \textit{detect} bugs, they struggle to \textit{localize} them.
% We therefore propose \textit{attention probing} which uses an attention mechanism in the architecture of the probe.
% Such a probe is trained to detect bugs, while its attention weights localize the bug within the code snippet.
% \aaditya{Make the experiments setup its own para, talk about SSTuB in detail, mention how setting experiments up is itself a challenge}


% \aaditya{Next para will be about the experiment results summarized}

To evaluate \ourmethod{}, we use eight diverse and widely-used fault localization benchmarks, including syntax errors, semantic mistakes, and weaknesses.
% the popular Defects4J dataset \citep{defects4j} comprised of real-world Java bugs.
Our evaluation suite covers over 50,000 %51,363% 
code samples across three languages: Python, Java, and C.
This notably includes Defects4J \citep{defects4j}, the most commonly used FL dataset.
% \dataset{} is sourced from existing datasets for 
% These datasets additionally span
% three classes of bugs: {\it i)} syntax errors, {\it ii)} semantic mistakes%involving single statement fixes, commonly called ``simple, stupid bugs'' \cite{karampatsis2020often},%
% , and {\it iii)} security vulnerabilities.
% These categories cover a wide range of bugs spanning compile-time bugs, run-time bugs, and security bugs.
% They also occur across different languages, reflecting their fundamental nature and enabling to evaluate \ourmethod{}'s ability to generalize detecting bugs of a particular type not only across programs but even across languages.

%but accounts for long-tail distributions of different bug types within each category.
%, and provides high-fidelity labels for not only bug presence but also its type and location.

We evaluate \ourmethod{} on top of the Llama-3 family of models and compare it to state-of-the-art FL methods, including traditional test-based FL and prompting of various proprietary and open-weights LLMs.
Averaged across eight datasets, \ourmethod{} improves by 23.4\% over the strongest baselines for top-1 FL accuracy which includes a 24.2\% improvement for Defects4J, and a 50.5\% improvement on DeepFix \citep{gupta2017deepfix}.
In addition, \ourmethod{} achieves these performance improvements at over ten times greater efficiency in terms of model size and FLOPs for inference. \ourmethod{} also localizes multi-line bugs better than existing methods and continues to have stronger performance than prompting for longer code sequences.
While \ourmethod{} significantly outperforms competitive baselines, it is able to achieve 35\% top-1 FL accuracy on average over our datasets, and our evaluation highlights avenues for further advances in scalable FL.
% \adam{We need some findings other than we improve localization accuracy.}
% We further observe that \ourmethod{} is able to generalize to different languages better than other baselines. 
% Finally, \ourmethod{} outperforms existing fault localization techniques on Defects4J for localizing bugs within the context of a function without using line-level localization.
% \mayur{This para really needs more details and better organization. It is the most important para of the intro. Also mention details of models used.}

% We evaluate our approach on three general kinds of bugs of increasing complexity: {\it i)} syntax errors, {\it ii)} semantic mistakes involving single statement fixes, commonly called ``simple, stupid bugs'' \cite{?}, and {\it iii)} security vulnerabilities.
% Our attention probes detect and localize these bugs with higher accuracy than existing
% probing techniques, approaching the accuracy of full-model finetuning.
% We also show that the patterns captured by our probing technique can generalize across languages for a particular bug type.

In summary, our work makes the following contributions:
% \begin{itemize}[leftmargin=*,itemsep=0pt,topsep=0pt,parsep=0.5ex]
\begin{itemize}[itemsep=0pt, topsep=0pt]
    \item We propose \ourmethodlong{} (\ourmethod{}) as a general method for scalable fault localization, requiring only coarse-grained detection supervision and eschewing the need for localization labels (Section~\ref{sec:ourmethod}).
    % \item We develop a new benchmark \dataset{} to extensively evaluate bug detection and bug localization methods across different bug categories and languages.
    % \item We demonstrate that \ourmethod{} achieves better detection performance compared to other probing techniques and prompting, and comparable performance to full-finetuning (Figure~\ref{fig:iid_detection})
    \item
    % By leveraging attention weights, \ourmethod{} can correctly localize bugs within token sequences using only end-to-end bug presence labels.
    \ourmethod{} significantly improves over state-of-the-art fault localization methods by 34.6\% top-1 accuracy on average over eight fault localization benchmarks while utilizing ten times less memory resources and FLOPs.
    \item \ourmethod{} exhibits better length generalization and can predict multi-line bugs more effectively than prompting.  We also identify areas for further advances.
    % \item \ourmethod{} also exhibits better bug generalization abilities compared to finetuning, allowing for cross-language generalization and bug-type generalization (Section~\ref{sec:generalization}).
\end{itemize}

% https://drive.google.com/file/d/1QfSmzCXir8ZCDcyocRc0FAr5HMbRAY2y/view?usp=sharing
\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figures/bap_splash.pdf}
    \caption{Illustration of \ourmethod{} as a method to elicit line-level fault localization from a frozen LLM through weak supervision. In step one, the probe is trained as a binary classifier to distinguish buggy from non-buggy code. Then in step two, we visualize the learned attention weights on the given sequence. Finally, in step three, we sum the attention weights within each line to produce a line-level ``bugginess'' score. \ourmethod{} localizes the bug to the line with the highest score, the Top-1 result.}
    \label{fig:overview}
\end{figure*}