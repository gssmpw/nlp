%Despite the success of LLMs in code understanding tasks, finding bugs remains a challenge.
% Using LLMs to find bugs in code is challenging despite their success in code understanding tasks.

% Writing correct code is a challenging task.
% Identifying whether and where a given code is buggy is a challenging task.
% Static analysis and fuzzing are successful for detecting a range of bugs, but come with strict requirements on the input code and require significant tuning to be useful due to the undecidable nature of bug detection.
% In contrast to the typical tools of symbolic program analysis, large language models (LLMs) can accept any input, even partial program fragments and often infer user intent. In addition, LLMs now have advanced performance on many code understanding tasks.
% Existing approaches for leveraging LLMs for bug detection involve either prompting or finetuning, but prompting often underperforms finetuning without significant engineering and finetuning is highly resource intensive and prone to overfitting. In addition, localizing bugs would require additional prompts or supervised data for finetuning.
% Instead, we propose to elicit an LLM's knowledge of bug understanding using a lightweight attention probe, allowing us to achieve high detection performance and accurate localization performance without direct supervision.
% We evaluate our approach on three bug detection tasks across three languages and show that it achieves better detection performance than non-finetuning approaches and accurately localizes them. In addition our probe has strong out-of-distribution performance, and we show it can detect language agnostic concepts such as mismatched parentheses and certain vulnerabilities.


Ensuring code correctness remains a challenging problem even
as large language models (LLMs) become increasingly capable at code-related tasks. While LLM-based program repair systems can propose bug fixes using only a user's bug report, their effectiveness is fundamentally limited by their ability to perform fault localization (FL), a challenging problem for both humans and LLMs.
Existing FL approaches rely on executable test cases, require training on costly and often noisy line-level annotations, or demand resource-intensive LLMs.
In this paper, we present \textit{\ourmethodlong{}} (\ourmethod{}), a method which learns state-of-the-art fault localization without any direct localization labels, outperforming traditional FL baselines and prompting of large-scale LLMs.
We evaluate our approach across a variety of code settings, including real-world Java bugs from the standard Defects4J dataset as well as seven other datasets which span a diverse set of bug types and languages. Averaged across all eight datasets, 
\ourmethod{} improves by 34.6\% top-1 accuracy
compared to the strongest baseline and 93.4\% over zero-shot prompting GPT-4o. \ourmethod{} is also significantly more efficient than prompting, outperforming large open-weight models at a small fraction of the computational cost.\footnote{\ourmethod{} is open-sourced here:\newline \url{https://github.com/adaminsky/BAP}}
% Our repository can be found at: \adam{anonymous-repo}

% a variety of syntax errors in Python, C, and Java (GitHub-Python, DeepFix, GitHub-Java), semantic errors in Python and Java (TSSB-3M, ManySStuBs4J), and synthetic security vulnerabilities in Java and C (OWASP, Juliet-C, Juliet-Java).
% and \textit{S3Bugs}, a curated compilation of over 50K Python, Java, and C code samples of syntactic, semantic, and security bugs.
% \href{https://github.com/OpenFaultLocalizer/Weave}{https://github.com/OpenFaultLocalizer/Weave}.

% MOST RECENT VERSION BELOW

% Finding bugs in code remains a challenging problem, even as LLMs become highly capable at code related tasks.
% The existing approach to bug localization relies on line-level annotations of the causes of a bug (which are noisy and expensive to clean) or additional supervision, such as executable test cases and data flows.
% % Existing LLM-based approaches for bug detection involve prompting, which is often inaccurate without significant engineering, or finetuning, which is prone to overfitting.
% In this paper, we propose WEak probing for Attention Value Extraction (\ourmethod{}), a method that elicits an LLM's knowledge of bug understanding to achieve accurate bug localization performance using only coarse-grained bug presence/absence labels.
% We evaluate our approach on a combination of bug localization benchmarks comprising $\sim$ 50k code samples containing syntactic, semantic, and security bugs in Python, Java, and C, as well as the standard Defects4J dataset.
% % \ourmethod{} achieves bug detection performance approaching that of full-finetuning of an LLM while only relying on the program text, along with higher localization performance. 
% \ourmethod{} localizes bugs better than existing approaches (including LLM finetuning) without using any localization supervision or test cases.
% In addition, our approach has strong out-of-distribution generalization which we demonstrate through language-agnostic concepts including mismatched parentheses and security vulnerabilities.
