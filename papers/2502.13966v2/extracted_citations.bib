@inproceedings{CodeQL,
  title={QL: Object-oriented queries on relational data},
  author={Avgustinov, Pavel and De Moor, Oege and Jones, Michael Peyton and Sch{\"a}fer, Max},
  booktitle={30th European Conference on Object-Oriented Programming (ECOOP 2016)},
  year={2016},
  organization={Schloss-Dagstuhl-Leibniz Zentrum f{\"u}r Informatik}
}

@article{agentfl,
  title={AgentFL: Scaling LLM-based Fault Localization to Project-Level Context},
  author={Qin, Yihao and Wang, Shangwen and Lou, Yiling and Dong, Jinhao and Wang, Kaixin and Li, Xiaoling and Mao, Xiaoguang},
  journal={arXiv preprint arXiv:2403.16362},
  year={2024}
}

@article{agentless,
  title={Agentless: Demystifying llm-based software engineering agents},
  author={Xia, Chunqiu Steven and Deng, Yinlin and Dunn, Soren and Zhang, Lingming},
  journal={arXiv preprint arXiv:2407.01489},
  year={2024}
}

@inproceedings{astprobe,
  title={AST-Probe: Recovering abstract syntax trees from hidden representations of pre-trained language models},
  author={Hern{\'a}ndez L{\'o}pez, Jos{\'e} Antonio and Weyssow, Martin and Cuadrado, Jes{\'u}s S{\'a}nchez and Sahraoui, Houari},
  booktitle={Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering},
  pages={1--11},
  year={2022}
}

@article{belinkov2022probing,
  title={Probing classifiers: Promises, shortcomings, and advances},
  author={Belinkov, Yonatan},
  journal={Computational Linguistics},
  volume={48},
  number={1},
  pages={207--219},
  year={2022},
  publisher={MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~â€¦}
}

@inproceedings{deepfl,
  title={Deepfl: Integrating multiple fault diagnosis dimensions for deep fault localization},
  author={Li, Xia and Li, Wei and Zhang, Yuqun and Zhang, Lingming},
  booktitle={Proceedings of the 28th ACM SIGSOFT international symposium on software testing and analysis},
  pages={169--180},
  year={2019}
}

@inproceedings{devlin-etal-2019-bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    editor = "Burstein, Jill  and
      Doran, Christy  and
      Solorio, Thamar",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}

@inproceedings{gupta2000generating,
  title={Generating test data for branch coverage},
  author={Gupta, Neelam and Mathur, Aditya P and Soffa, Mary Lou},
  booktitle={Proceedings ASE 2000. Fifteenth IEEE International Conference on Automated Software Engineering},
  pages={219--227},
  year={2000},
  organization={IEEE}
}

@inproceedings{he2022distribution,
  title={On distribution shift in learning-based bug detectors},
  author={He, Jingxuan and Beurer-Kellner, Luca and Vechev, Martin},
  booktitle={International conference on machine learning},
  pages={8559--8580},
  year={2022},
  organization={PMLR}
}

@inproceedings{hewitt2019designing,
  title={Designing and Interpreting Probes with Control Tasks},
  author={Hewitt, John and Liang, Percy},
  booktitle={Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  pages={2733--2743},
  year={2019}
}

@inproceedings{hewitt2019structural,
  title={A structural probe for finding syntax in word representations},
  author={Hewitt, John and Manning, Christopher D},
  booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  pages={4129--4138},
  year={2019}
}

@article{hindle2016naturalness,
  title={On the naturalness of software},
  author={Hindle, Abram and Barr, Earl T and Gabel, Mark and Su, Zhendong and Devanbu, Premkumar},
  journal={Communications of the ACM},
  volume={59},
  number={5},
  pages={122--131},
  year={2016},
  publisher={ACM New York, NY, USA}
}

@article{hooda2024large,
  title={Do Large Code Models Understand Programming Concepts? A Black-box Approach},
  author={Hooda, Ashish and Christodorescu, Mihai and Allamanis, Miltos and Wilson, Aaron and Fawaz, Kassem and Jha, Somesh},
  journal={arXiv preprint arXiv:2402.05980},
  year={2024}
}

@inproceedings{jimenez2023swe,
  title={SWE-bench: Can Language Models Resolve Real-world Github Issues?},
  author={Jimenez, Carlos E and Yang, John and Wettig, Alexander and Yao, Shunyu and Pei, Kexin and Press, Ofir and Narasimhan, Karthik R},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024}
}

@article{khare2023understanding,
  title={Understanding the Effectiveness of Large Language Models in Detecting Security Vulnerabilities},
  author={Khare, Avishree and Dutta, Saikat and Li, Ziyang and Solko-Breslin, Alaia and Alur, Rajeev and Naik, Mayur},
  journal={arXiv preprint arXiv:2311.16169},
  year={2023}
}

@inproceedings{kumar2022fine,
  title={Fine-Tuning can Distort Pretrained Features and Underperform Out-of-Distribution},
  author={Kumar, Ananya and Raghunathan, Aditi and Jones, Robbie Matthew and Ma, Tengyu and Liang, Percy},
  booktitle={International Conference on Learning Representations},
  year={2022}
}

@article{liang2018fuzzing,
  title={Fuzzing: State of the art},
  author={Liang, Hongliang and Pei, Xiaoxiao and Jia, Xiaodong and Shen, Wuwei and Zhang, Jian},
  journal={IEEE Transactions on Reliability},
  volume={67},
  number={3},
  pages={1199--1218},
  year={2018},
  publisher={IEEE}
}

@inproceedings{llmao,
author = {Yang, Aidan Z. H. and Le Goues, Claire and Martins, Ruben and Hellendoorn, Vincent},
title = {Large Language Models for Test-Free Fault Localization},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3623342},
doi = {10.1145/3597503.3623342},
abstract = {Fault Localization (FL) aims to automatically localize buggy lines of code, a key first step in many manual and automatic debugging tasks. Previous FL techniques assume the provision of input tests, and often require extensive program analysis, program instrumentation, or data preprocessing. Prior work on deep learning for APR struggles to learn from small datasets and produces limited results on real-world programs. Inspired by the ability of large language models (LLMs) of code to adapt to new tasks based on very few examples, we investigate the applicability of LLMs to line level fault localization. Specifically, we propose to overcome the left-to-right nature of LLMs by fine-tuning a small set of bidirectional adapter layers on top of the representations learned by LLMs to produce LLMAO, the first language model based fault localization approach that locates buggy lines of code without any test coverage information. We fine-tune LLMs with 350 million, 6 billion, and 16 billion parameters on small, manually curated corpora of buggy programs such as the Defects4J corpus. We observe that our technique achieves substantially more confidence in fault localization when built on the larger models, with bug localization performance scaling consistently with the LLM size. Our empirical evaluation shows that LLMAO improves the Top-1 results over the state-of-the-art machine learning fault localization (MLFL) baselines by 2.3\%--54.4\%, and Top-5 results by 14.4\%-35.6\%. LLMAO is also the first FL technique trained using a language model architecture that can detect security vulnerabilities down to the code line level.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {17},
numpages = {12},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{niu2022does,
  title={Does BERT rediscover a classical NLP pipeline?},
  author={Niu, Jingcheng and Lu, Wenjie and Penn, Gerald},
  booktitle={Proceedings of the 29th International Conference on Computational Linguistics},
  pages={3143--3153},
  year={2022}
}

@inproceedings{pashakhanlooCodeTrekFlexibleModeling2021,
  title = {{{CodeTrek}}: {{Flexible Modeling}} of {{Code}} Using an {{Extensible Relational Representation}}},
  shorttitle = {{{CodeTrek}}},
  author = {Pashakhanloo, Pardis and Naik, Aaditya and Wang, Yuepeng and Dai, Hanjun and Maniatis, Petros and Naik, Mayur},
  date = {2021-10-06},
  url = {https://openreview.net/forum?id=WQc075jmBmf},
  urldate = {2024-04-19},
  booktitle = {International Conference on Learning Representations},
  year = {2022}
}

@inproceedings{pearce2022asleep,
  title={Asleep at the keyboard? assessing the security of github copilotâ€™s code contributions},
  author={Pearce, Hammond and Ahmad, Baleegh and Tan, Benjamin and Dolan-Gavitt, Brendan and Karri, Ramesh},
  booktitle={2022 IEEE Symposium on Security and Privacy (SP)},
  pages={754--768},
  year={2022},
  organization={IEEE}
}

@article{repe,
  title={Representation engineering: A top-down approach to ai transparency},
  author={Zou, Andy and Phan, Long and Chen, Sarah and Campbell, James and Guo, Phillip and Ren, Richard and Pan, Alexander and Yin, Xuwang and Mazeika, Mantas and Dombrowski, Ann-Kathrin and others},
  journal={arXiv preprint arXiv:2310.01405},
  year={2023}
}

@inproceedings{smartfl,
  title={Fault localization via efficient probabilistic modeling of program semantics},
  author={Zeng, Muhan and Wu, Yiqian and Ye, Zhentao and Xiong, Yingfei and Zhang, Xin and Zhang, Lu},
  booktitle={Proceedings of the 44th International Conference on Software Engineering},
  pages={958--969},
  year={2022}
}

@inproceedings{tenney2018you,
  title={What do you learn from context? Probing for sentence structure in contextualized word representations},
  author={Tenney, Ian and Xia, Patrick and Chen, Berlin and Wang, Alex and Poliak, Adam and McCoy, R Thomas and Kim, Najoung and Van Durme, Benjamin and Bowman, Samuel R and Das, Dipanjan and others},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@inproceedings{transferfl,
  title={Improving fault localization and program repair with deep semantic features and transferred knowledge},
  author={Meng, Xiangxin and Wang, Xu and Zhang, Hongyu and Sun, Hailong and Liu, Xudong},
  booktitle={Proceedings of the 44th International Conference on Software Engineering},
  pages={1169--1180},
  year={2022}
}

@article{tschannen2024image,
  title={Image captioners are scalable vision learners too},
  author={Tschannen, Michael and Kumar, Manoj and Steiner, Andreas and Zhai, Xiaohua and Houlsby, Neil and Beyer, Lucas},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{well,
author = {Zhang, Huangzhao and Li, Zhuo and Li, Jia and Jin, Zhi and Li, Ge},
title = {WELL: Applying bug detectors to bug localization via weakly supervised learning},
journal = {Journal of Software: Evolution and Process},
volume = {36},
number = {9},
pages = {e2669},
keywords = {bug detection, bug localization, weakly supervised learning},
doi = {https://doi.org/10.1002/smr.2669},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/smr.2669},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/smr.2669},
abstract = {Abstract Bug localization, which is used to help programmers identify the location of bugs in source code, is an essential task in software development. Researchers have already made efforts to harness the powerful deep learning (DL) techniques to automate it. However, training bug localization model is usually challenging because it requires a large quantity of data labeled with the bug's exact location, which is difficult and time-consuming to collect. By contrast, obtaining bug detection data with binary labels of whether there is a bug in the source code is much simpler. This paper proposes a WEakly supervised bug LocaLization (WELL) method, which only uses the bug detection data with binary labels to train a bug localization model. With CodeBERT finetuned on the buggy-or-not binary labeled data, WELL can address bug localization in a weakly supervised manner. The evaluations on three method-level synthetic datasets and one file-level real-world dataset show that WELL is significantly better than the existing state-of-the-art model in typical bug localization tasks such as variable misuse and other bugs.},
year = {2024}
}

@article{wong2016survey,
  title={A survey on software fault localization},
  author={Wong, W Eric and Gao, Ruizhi and Li, Yihao and Abreu, Rui and Wotawa, Franz},
  journal={IEEE Transactions on Software Engineering},
  volume={42},
  number={8},
  pages={707--740},
  year={2016},
  publisher={IEEE}
}

@article{wu2023large,
  title={Large language models in fault localisation},
  author={Wu, Yonghao and Li, Zheng and Zhang, Jie M and Papadakis, Mike and Harman, Mark and Liu, Yong},
  journal={arXiv preprint arXiv:2308.15276},
  year={2023}
}

@inproceedings{yasunaga2021break,
  title={Break-it-fix-it: Unsupervised learning for program repair},
  author={Yasunaga, Michihiro and Liang, Percy},
  booktitle={International conference on machine learning},
  pages={11941--11952},
  year={2021},
  organization={PMLR}
}

@inproceedings{zhang2023code,
  title={Code Representation Learning at Scale},
  author={Zhang, Dejiao and Ahmad, Wasi Uddin and Tan, Ming and Ding, Hantian and Nallapati, Ramesh and Roth, Dan and Ma, Xiaofei and Xiang, Bing},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024}
}

