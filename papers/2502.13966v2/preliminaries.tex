% \section{LLMs for Fault Localization}
\section{Background}
\label{sec:probing}
We introduce the fault localization problem, LLM probing, and challenges with existing FL techniques.

% \adam{I'm working on condensing all the information about existing approaches (traditional symbolic analysis, LLM prompting, and LLM finetuning/probing) into subsections here and then removing the repetitions from Section 3.}

% \adam{
% This section needs be reoriented towards just fault localization. It should introduce the problem and setup why LLMs are now pushing the frontier on the task.
% \begin{itemize}
%     \item First define the bug detection problem (since typically a bug is detected before localized).
%     \item Define bug localization and describe how localization labels are challenging to collect.
%     \item Introduce the benefits of LLM-based Fault Localization and typical approaches.
% \end{itemize}
% }

% A probe commonly refers to a classifier trained on the internal representations from a separate model and the output is an auxiliary task which the model was not trained on \citep{alain2016understanding}. The interpretation of a high probe performance on the downstream task nuanced \citep{hewitt2019designing}, but it is traditionally viewed as evidence that either the pretrained model internally learned the task or has made the relevant information to solve the task easy to extract.

% In this section, we formulate the problem of bug detection and introduce probing of LLM internal representations.

% As discussed earlier, the debugging process involves \textit{bug detection} and \textit{bug localization}.
% In this section, we first formalize the problems of bug detection and localization in the context of machine learning and then describe conventional LLM-based approaches to debugging.

% \subsection{Problem formulation}
% \label{prelim:formulation}
% Introduce the supervised learning problem
% Motivate why it makes sense to solve with supervised learning
% 

% \subsection{Bug Detection}
% \label{sec:detection}

% Before localizing a bug, the bug is typically first \textit{detected}.
% We cast bug detection as a supervised learning problem. 
% A bug is a concept $b:\mathcal{P}\rightarrow \{0,1\}$ where
% $\mathcal{P}$ is the space of all programs.
% For a bug $b$, we construct a supervised dataset $\mathcal{D} = \{(p, b(p)) \mid p\in \mathcal{P}\}$ consisting of programs labeled as buggy or not buggy.

% Such datasets can be automatically mined from the internet since strong critics exist to determine if code has certain types of bugs or not, such as provided test cases or compilers \citep{karampatsis2020often, yasunaga2021break}.

% Note that bugs of certain types, such as \textit{invalid syntax}, \textit{assertion violation}, \textit{security vulnerability}, etc. can manifest themselves in programs written in different languages.
% For instance, an invalid syntax bug due to an unmatched open parenthesis may occur in a program written in Python, C, or Java.
% This is also an example of a \textit{decidable} bug: there exists an algorithm which can check if a given program $p$ contains invalid syntax, for example the C compiler or the Python interpreter.

% In practice, we care about specific subsets of programs and bugs, for instance all programs of a particular language and bugs of a particular type. For a language $l\in \{\text{Java}, \text{C}, \text{Python}, \dots \}$, we denote all programs of the language as $\mathcal{P}_l$. Similarly, bugs of type $t\in\{\textit{invalid syntax}, \textit{assertion violation}, \textit{security vulnerability} \dots\}$ are denoted with $\mathcal{B}_t$. Different languages also typically share many bug types.

% Different combinations of languages and bug types $(l, t)$ have different properties making some types of bugs easier to detect than others. For bugs such as \textit{invalid syntax} and languages like Python, the problem is decidable, meaning there is an algorithm which takes a program $p\in P_l$ as input and outputs $b(p)$.

% However, the problem of bug detection is generally undecidable, for example in cases such as \textit{segmentation faults}. The problem can be made decidable by providing \textit{specifications} like types, loop invariants, and assertions.
% Without these specifications, we can instead rely on statistical models to predict the probability of a bug in a program, $P(b(p)=1 | p)$.
% To learn this discriminative model for predicting bugs, the common approach is to optimize $\hat{b}$ to minimize the risk:
% \begin{align*}
%     R(\hat{b}) = \mathbb{E}_{(x, y)\sim_P \mathcal{D}}[\ell(\hat{b}(x), y)]
% \end{align*}
% where $\ell$ is the binary cross-entropy loss and $P(X,Y)$ is the joint distribution over our data.

% \subsubsection{Bug localization}
\subsection{Fault Localization}
\label{sec:localization}
Before defining bug localization, we first describe the problem of bug \textit{detection}, or determining if code is overall buggy or not.
% We cast bug detection as a supervised learning problem. 
A bug is a concept $b:\mathcal{P}\rightarrow \{0,1\}$ where
$\mathcal{P}$ is the space of all programs.
For a bug $b$, we construct a supervised detection dataset $\mathcal{D}_\text{Det} = \{(p, b(p)) \mid p\in \mathcal{P}\}$ consisting of programs labeled as buggy or not buggy.

Bug localization is the task of identifying the line (or lines) of code where a bug occurs. A bug localizer is a mapping $l:\mathcal{P}\rightarrow \mathbb{Z}^+$ from programs to one or more line numbers.
For a program $p \in \mathcal{P}$ split into lines $[p_0, \dots, p_k]$, where $p_i$ is the $i^\text{th}$ line of program $p$,
we define the ground truth bug localization using the notion of a counterfactual explanation: a program has a bug localized to line $i$ if modifying $p_i$ would remove the bug.
Formally, for program $p$ such that $b(p)=1$, bug $b$ is localized to line $i$ if there exists a modified line $p_i'$ such that $b(p')=0$ where $p'=[p_0, \dots, p_{i-1}, p_i', p_{i+1}, \dots, p_k]$. 

Some bugs require multi-line fixes, in which case changing multiple lines of the program would fix the bug.
We therefore assume that the ground truth localization consists of one or more lines of code. We can now define a localization dataset containing direct line-level supervision:
\begin{align*}
\mathcal{D}_\text{Loc} = &\{(p, 0, \varnothing) \mid p\in\mathcal{P} \text{ and } b(p) = 0\} \\
&\cup \{(p, 1, l(p)) \mid p\in\mathcal{P} \text{ and } b(p) = 1\}.
\end{align*}

\subsection{LLM Probing}
LLM probing involves training a classifier on top of intermediate states from a model \citep{alain2016understanding}.
Since all probing techniques are trained from a dataset of model hidden representations, we start by introducing this dataset in our setting.

% We describe how hidden representations for probing are extracted from the supervised bug detection dataset $\mathcal{D}_\text{Det}$ as defined in Section~\ref{sec:localization}.
For a program fragment $p$ consisting of $T$ tokens, we call the intermediate representation from LLM layer $k$, $\text{LLMRep}(p, k) = z \in\mathbb{R}^{T\times d}$ where $d$ is the hidden dimension of the LLM. The dataset we use for probing in the rest of this paper is the following:
\begin{align*}
    \mathcal{H}_{\text{Det}} = \{(\text{LLMRep}(p, k), y) : (p, y)\in\mathcal{D}_\text{Det}\}.
\end{align*}
We can also define $\mathcal{H}_\text{Loc}$ equivalently which additionally includes the ground truth line numbers with each sample, but as we discuss later, this dataset is not ideal for training.

These datasets, $\mathcal{H}_\text{Det}$ and $\mathcal{H}_\text{Loc}$, are not directly amenable to standard probing since each sample, $\text{LLMRep}(p, k)$, is a sequence of hidden representations varying in length across samples.
Producing a general-purpose fixed-length sequence representation from an autoregressive LLM is a challenging problem \cite{liu2023meaning}, so probing methods typically apply a simple pooling operation, $\text{POOL}:\mathbb{R}^{T\times d}\rightarrow \mathbb{R}^{1\times d}$, such as selecting the last token or averaging all tokens. Specific to using $\mathcal{H}_\text{Loc}$, existing work takes the approach of converting the labels $l(p)$ into a binary mask over the buggy program lines so that a model can be trained to predict such masks \citep{llmao}.



\subsection{Challenges}

We describe three main challenges with FL below.

\textbf{Need for Strong Supervision}
High-quality bug localization datasets are elusive since they require large amounts of manual effort to create. This is because a method for detecting a bug, such as a failing test case, does not immediately indicate what caused the failure. Therefore, existing real-world FL datasets are often small (e.g. Defects4J which has 395 samples \citep{defects4j}) or very noisy (e.g. ManySStuBs4J \citep{karampatsis2020often}). In our experiments, we find that methods trained on either these small or noisy datasets do not perform as well as methods which ignore such strong supervision.

\textbf{Localizing Multi-line Bugs}
% \textbf{Localizing to Tokens with Sequence-level Supervision}
The majority of real-world bugs are localized to multiple lines of code \citep{pearson2016evaluating}.
Existing methods for FL, however, mostly focus on single-line bug detection \citep{hirsch2020fault} and there are several datasets specifically catered to this setting \citep{karampatsis2020often, richter2022tssb}.

% If we only use detection supervision for probing, we cannot immediately \textit{localize} bugs.
% As a workaround, one can apply the probe to every prefix of pooled tokens $[t_0,t_1,\ldots{},t_i]$ and localize to the index of the first token $t_i$ where the probe detects a bug. This is the method commonly used to localize ``concepts'' within a token sequence in work on LLM interpretability \citep{repe},
% but our experiments in Section~\ref{sec:experiments} demonstrate that this performs poorly for FL.

% Attention probing was proposed in the context of interpretability \citep{tenney2018you, niu2022does} to understand which layers and tokens of an LLM were important for solving an NLP-related task.
% The attention probe additionally
% learns the pooling operation as a weighted sum over a sequence. As a result, the attention probe learns which tokens to attend to for achieving the best classification performance.
% In the next section, we describe our approach for using the concept of attention probes beyond interpretability to the task of fault localization from weak supervision.

\textbf{Resource Efficiency}
% \textbf{Line-Level Localization with a Token-based Model}
The most powerful LLMs are state-of-the-art for FL tasks on a method-level context, but these models are also very resource intensive. Many of the best models are out of reach to use on the average laptop. These models are also available through APIs, but for FL settings where one may repeatedly evaluate a model on code as it changes, this can quickly become expensive. This is in sharp contrast to traditional execution-based FL methods which can run in the background of an IDE on a laptop.
% LLMs are token-based models, but we care about code lines (which contain many tokens). Therefore, we need to convert any token-level localization which results from probing into a line-level localization.

% \textbf{The Best LLMs are Resource Intensive}
% LLM prompting is now the highest performing FL technique on the method-level context, but this performance requires the best models such at GPT-4 or large open-weights models which are too large or costly for most people to frequently use.

% Obtaining datasets with information of \textit{where} bugs occur is a challenging problem which is often addressed by relying on git commit messages along with manual inspection \citep{richter2022tssb, defects4j, karampatsis2020often}.

% Existing datasets for fault localization often derive faulty line labels from bug-fixing patches \citep{defects4j}, so the localization labels correspond exactly to a counterfactual explanation for the bug. However, when mining these labels from public repositories, the resulting labels are highly noisy since a bug-fixing commit may contain other changes (modifying tests, refactoring, changing comments, etc.) which are not pertinent to fixing the bug. High quality line-level localization information on real-world code data is therefore expensive to collect \citep{defects4j}.

% % This is about how we construct the localization datasets
% During training, we do not provide supervision for localization, but we still evaluate our methods on a test set with ground truth localization labels, consisting of a single line annotation corresponding to the localization of the bug. In some cases, we get the ground truth localization from a git diff command where the localization corresponds exactly to a counterfactual explanation. For other domains where we do not have bug fix modifications available, we use output from existing localizers built into compilers or the ground truth localization from synthetic benchmarks.

% To evaluate localization performance, we use the classic information retrieval metric of Mean Reciprocal Rank (MRR) which is used in prior work on bug localization \citep{saha2013improving}. The MRR metric softens the requirement of outputting the exact line number of the bug and instead produces a value between 0 and 1 which penalizes methods based on how far the true buggy line is ranked from first.

% The bug detection and localization problem is thus semi-supervised, since we have supervision for bug detection and no supervision for localization.
% We note that there are subsets of $P$ which correspond to programs of a particular language, and thus $P = \cup_{L\in \{\text{Python}, \text{C}, \text{Java}, \dots\}} P_L$. Similarly, there are subsets of th

% We consider probes for tasks that are defined over sequences of text, which we introduce with the notation of formal languages.
% We consider text that can be expressed with a fixed set of tokens $T$. A formal language $\mathcal{L}$ is defined as a subset of $T^*$.
% In the simplest case, a language is defined by production rules such as the syntactic production rules of a programming language, or even a regular expression such as \verb|[0-9]{2}\/[0-9]{2}\/[0-9]{4}| which defines the language of MM/DD/YYYY date strings.
% An equivalent definition of a language $\mathcal{L}$ is as a decision problem which classifies $w\in T^*$ as ``yes" if $w\in\mathcal{L}$ and ``no" otherwise. For instance, we can pose the decision problem of determining if a string is a valid date.

% In this paper, we focus on learning probes the decision problem of detecting various bugs in code. For instance, if we define $\mathcal{L}_b$ as the language consisting of all strings $w\in T^*$ with a particular bug $b$, then we want a probe which answers the decision problem of if $w$ contains such a bug:
% \begin{align*}
%     P(s) = \begin{cases}
%         1 & \text{if $s\in \mathcal{L}_b$}\\
%         0 & \text{otherwise}
%     \end{cases}.
% \end{align*}
% In the case of programming language syntax, the above problem can be exactly represented by a parser, since this is a decidable problem. However, for almost all other code-related decision problems, such as detecting most types of bugs, the decision problem is undecidable \citep{landi1992undecidability}.

% Since LLMs have been trained on large amounts of code, we hypothesize that they have learned useful heuristics based on how bugs occur in the real-world.

% \adam{LLMs are a great fit to this problem. A statistical model of code may still be useful for finding bugs based on their patterns of occurence even if we cannot find the bug in all cases.}

% \subsection{LLM-based tools for bug detection and localization}
% \subsection{LLM code representations}
% \adam{This section should describe the existing methods for using LLMs for this problem.}



% \begin{figure}
%     \centering
%     \includegraphics[width=0.5\textwidth]{figures/python_syntax_probing.pdf}
%     \caption{Comparison of different pooling and input prompting methods on linear probe accuracy for Python syntax correctness.}
%     \label{fig:pooling-prompting}
% \end{figure}

\begin{comment}
\subsection{Why LLMs for Fault Localization?}
% \subsection{LLM-based Fault Localization Methods}


% Using an LLM can aid in solving the above bug detection and localization problem as well as bring useful properties to the resulting code auditing system.

% Bug detection and localization as defined above is a semi-supervised learning problem, since we have supervision for bug detection, but must learn localization without supervision. In low-supervision scenarios, LLMs have recently excelled \citep{}, and since LLMs are trained on large amounts of code, LLMs are a good fit.

% How do we use an LLM for detecting and localizing bugs? The existing approaches consist of prompting, finetuning, and probing. For prompting methods, we craft a prompt with instructions and some examples, and allow the LLM to generate a response containing the bug detection and localization results. While this method requires little supervision, it has lower accuracy than other methods and can be highly dependent on the wording of the prompt \citep{}. On the other hand, we can finetuning an LLM to solve our supervised bug detection task, but this is highly resource intensive (since models are billions of parameters) and we cannot localize bugs with a finetuned model.

% Rather than prompt or finetune, we can also probe an LLM which gives us both high accuracy and strong generalization properties with little training overhead.

% In this section, we motivate probing of LLM representations as a bug detection method for use in offline code auditing and LLM programming agents.
% We show that the combination of representations from powerful LLMs with relatively simple probes allow us to find bugs in program fragments, automatically localize bugs, and detect bugs which occur across languages, all in realtime during LLM inference and generation.

%
% \adam{This is saying LLMs are useful for this problem}

% Using LLMs to debug programs 

The effectiveness of LLMs in code understanding tasks means that LLMs provide a versatile platform to build bug localization tools.
Since LLMs operate over any sequence of tokens, an LLM-based debugger can aid in finding bugs in any program or program fragment. On the other hand, other program analysis techniques place strong requirements on their inputs, such as requiring the input program to compile~\citep{CodeQL}, or be translated into a specialized representation~\citep{pashakhanlooCodeTrekFlexibleModeling2021}.
% , requiring an entire project rather than a single file \citep{}, and requiring additional customization such as assertion statements \citep{}.
Removing these additional requirements not only aids in usability, but it enables such LLM-based tools to audit a larger set of programs than traditional program analysis tools, and cover a wider range of bugs. 
% By removing these additional requirements, LLMs enable offline code auditing on a much larger set of programs than traditional program analysis tools, and cover a wider range of bugs without additional customization. 
% Unlike such methods, we do not assume any existing structure to the input program. For example, a program fragment in \adam{language} is shown in Figure~\ref{fig:motivation} where we are able to correctly detect and localize a bug even though this code does not compile.

The other significant advantage afforded by LLMs is that they enable the use of less supervision as their size increases.
Specifically for the fault localization task, finetuning of smaller LLMs using only direct line-level supervision makes the use of supervision from test-cases unnecessary in certain cases \citep{llmao}. At the largest scale, the largest of LLMs (e.g. GPT-4) have been shown to achieve state-of-the-art fault localization performance on a method-level context \textit{without any additional supervision} \citep{wu2023large}.

% LLMs additionally offer particular advantages for localizing bugs. Traditionally, bug localization is a challenging and time consuming problem \citep{vessey1985expertise} and automated approaches require tuned heuristics and complex algorithms \citep{cleve2005locating}. Recent work shows that LLMs can output ``explanations'' for their behavior
% % without additional supervision
% \citep{wei2022emergent}, so leveraging this ability opens the possibility for localizing bugs without finegrained supervision.
% By learning a probe on a powerful LLM, we elicit the LLM's existing knowledge of bugs to automatically localize bugs.
% For example, Figure~\ref{fig:motivation} shows three examples where we correctly localize bugs where such explanations of the bug are learned as a result of probing rather than explicit supervision.

% We want to run static analysis on generated code with minimal overhead.
Finally, since realtime code generation is now an important usecase for code \citep{zheng2024opencodeinterpreter},
running code analysis on a GPU can be more practical than running discrete algorithms on a CPU.
LLMs now may even answer non-programming related questions by interactively writing and executing code behind the scenes \citep{codeinterp}.
As a result, any code auditing tool should be integrated into the code generation pipeline seamlessly.
Naturally, LLM-based tools have an advantage here over other techniques, since they can be run in parallel over the tokens while they are generated.
\end{comment}

% \subsection{LLM-based Fault Localization Tools}
\begin{comment}
\subsection{LLM Probing}
Conventional LLM-based tools for debugging typically fall into one of three major categories: prompting, finetuning, or probing-based tools.
Prompt-based tools typically require crafting a prompt to be used with a pretrained LLM. These prompts can be crafted using several techniques like few-shot prompting~\citep{brown2020language} or chain-of-thought prompting~\citep{wei2022chain}.
% However, as seen in our experiments in Section~\ref{sec:experiments}, these tools suffer in performance when compared to finetuned techniques.
Finetuning, unlike prompting, modifies the LLM specifically for the relevant task. However, such techniques require substantial amounts of resources and time in order to finetune large models, even with optimizations such as LoRA~\citep{hu2021lora}.
% In addition, since labels for the fault localization task are themselves costly to obtain and often noisy in existing datasets, it is challenging to train strong models from such supervision \citep{llmao}.
Probing techniques can overcome the drawbacks of prompting and finetuning.

% Finetuned models also suffer from poor generalization performance, as our experiments in Section~\ref{sec:experiments} show, and cannot effectively detect bugs across languages unless their finetuning data contains samples from those languages as well.

% As LLMs are now used for generating code in many different scenarios, we want to run integrity checks over the code as it is generated.
% To audit such code we can use symbolic program analysis tools by feeding the generated code to a separate process, but this will often slow down generation as static analysis is an offline process \citep{}.
% Consider Figure ... \adam{Show a high level plot of throughput of various methods.}

% Probing techniques can overcome the drawbacks of prompting and finetuning.
Typical probing techniques use simple classifiers to evaluate the effectiveness of hidden representations for solving auxiliary tasks.
% As such, probes can be used over code-generation LLMs to detect and localize bugs in the code they generate.
% Probing methods tend to outperform finetuning on out-of-distribution data \citep{kumar2022fine}, so probing is well fit to learn general patterns of bug expression rather than overfit to bugs in a specific language.
% Moreover, since the classifiers used in probes are relatively simple, they can be run online alongside code generation with minimal overhead.
% We further discuss probing as a means to debug LLM generated code in the next section.
The appeal of probing is that it may allow for using available supervised data to achieve better performance with a smaller model than spending effort prompt engineering a resource intensive model.

% In addition, generated code can be in a variety of languages, so ideally auditing methods will find bugs that occur across languages. For instance, \adam{give an example of a bug in langugage A that is also detected in language B}. Probing methods generally outperform finetuning on out-of-distribution data \citep{kumar2022fine}, so probing is well fit to learning general patterns of bug expression rather than overfit to a single language and miss bugs when they occur in other languages.

% If we instead consider probing code representations from an LLM, we can create bug detectors that run online alongside code generation with minimal overhead.
% \adam{Move all the probing stuff out of the above. Introduce finetuning (expensive), prompting (low-accuracy), and probing as ways of using LLMs.}

% When we have labelled data, we should use a probe rather than few-shot prompt the model, but finetuning gets the best performance.
% Probing is not only a low-overhead technique, but it is actually competitive with LLM finetuning.

% Probing leverages the existing representations from the LLM rather than modify them

% 

% The elements of the dataset of representations described above consists of variable length sequences of hidden representations for every layer of the network, so how do we probe this sequence? Prior work converts the token representations over all layers to a fixed length vector by pooling the sequence, commonly with mean pooling or last-token pooling, and token representations from a specific layer are selected using a validation set \citep{repe}. The last token is often used \citep{repe, marks2023geometry, gurnee2023finding} in existing work on probing LLMs, but since this token's representation is used for predicting the next token, it is not always be relevant to the probing task. By prompting the LLM so that the next token aligns with the probing task, e.g. by prompting with ``the probability of a bug in the previous program is", \citet{repe} found that they can improve probing accuracy.

% Figure~\ref{fig:pooling-prompting} shows performance of probes using different prompting and pooling methods on a dataset of Python code with correct and incorrect syntax. In practice, similar to the findings of \citet{repe}, we find that last token pooling combined with prompting can improve the performance of a probe for properties of code such as syntactic correctness. 
% \adam{We hypothesize that the improved performance is from the prompt implicitly requiring the LLM to pool the relevant information into the last token representation. Does an LLM localize certain information in certain token representations?}

% In the next section we describe our method which consists of a single transformer block, and we describe why this is ideal for bug detection.
% , allowing us to outperform existing probing techniques and localize bug occurence in the token sequence.
\end{comment}