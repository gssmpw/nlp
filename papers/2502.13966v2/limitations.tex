% \section{Discussion}
% \label{sec:limitations}

% % Our analysis of bug detection is grounded in three diverse sets of bugs in multiple languages, but a more extensive catalog of the bug detection and code-related alignment properties that current LLMs should understand has not yet been explored. Furthermore, the code samples in the datasets used to build \dataset{} are restricted to code snippets that fit the context of the model. This rules out inter-procedural bugs spanning multiple modules. We hope that future work can target better benchmarks and datasets for evaluating alignment in a code generation context.
% Our evaluation on the Defects4J dataset is focused to the method-level, but as we show, fault localization becomes much harder as the context length increases. Existing LLM-based methods also face this challenge and often rely on divide-and-conquer approaches to first reduce the context to the method level before running a fault localizer \citep{agentless}. We hope future work can address the problem of fault localization for long-contexts.

% Our analysis on the security vulnerability domain only considers examples from synthetic datasets. This is due to the lack of high-quality real-world vulnerability datasets. We hope future work can develop high-quality datasets of self-contained real-world vulnerabilities, or examples of LLM-generated vulnerabilities, so we can evaluate \ourmethod{} for detecting non-synthetic vulnerabilities.

% For all code examples, we limit the number of tokens to 1,500 for GPU memory overhead reasons. While Llama-3 supports 8k tokens and CodeLlama supports up to 16k input tokens, we found that using such context lengths does not fit in our available GPU resources and significantly slows down experiments when offloaded to the CPU.