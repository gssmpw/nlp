% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@book{Aho:72,
    author  = {Alfred V. Aho and Jeffrey D. Ullman},
    title   = {The Theory of Parsing, Translation and Compiling},
    year    = "1972",
    volume  = "1",
    publisher = {Prentice-Hall},
    address = {Englewood Cliffs, NJ}
}

@book{APA:83,
    author  = {{American Psychological Association}},
    title   = {Publications Manual},
    year    = "1983",
    publisher = {American Psychological Association},
    address = {Washington, DC}
}


@article{shengbang2024cambrian1,
  title={Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs},
  author={Shengbang Tong and Ellis Brown and Penghao Wu and Sanghyun Woo and Manoj Middepogu and Sai Charitha Akula and Jihan Yang and Shusheng Yang and Adithya Iyer and Xichen Pan and Ziteng Wang and Rob Fergus and Yann LeCun and Saining Xie},
  journal={arXiv preprint arXiv:2406.16860v2},
  year={2024},
  url={https://www.arxiv.org/abs/2406.16860v2}
}
@inproceedings{liu2024mmbench,
  title={Mmbench: Is your multi-modal model an all-around player?},
  author={Liu Yuan and Duan Haodong and Zhang Yuanhan and Li Bo and Zhang Songyang and Zhao Wangbo and Yuan Yike and Wang Jiaqi and He Conghui and Liu Ziwei and others},
  booktitle={European conference on computer vision},
  pages={216--233},
  year={2024},
  organization={Springer}
}

@misc{paddleocr,
  title = {{PaddleOCR}},
  howpublished = {\url{https://github.com/PaddlePaddle/PaddleOCR}},
  note = {Accessed: 2024-06-30}
}
@inproceedings{yue2024mmmu,
  title={Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi},
  author={Yue Xiang and Ni Yuansheng and Zhang Kai and Zheng Tianyu and Liu Ruoqi and Zhang Ge and Stevens Samuel and Jiang Dongfu and Ren Weiming and Sun Yuxuan and others},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={9556--9567},
  year={2024}
}
@misc{fu2024mmecomprehensiveevaluationbenchmark,
      title={MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models}, 
      author={Chaoyou Fu and Peixian Chen and Yunhang Shen and Yulei Qin and Mengdan Zhang and Xu Lin and Jinrui Yang and Xiawu Zheng and Ke Li and Xing Sun and Yunsheng Wu and Rongrong Ji},
      year={2024},
      eprint={2306.13394},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2306.13394}, 
}
@article{shaohan2023language,
  title={Language Is Not All You Need: Aligning Perception with Language Models},
  author={Shaohan Huang and Li Dong and Wenhui Wang and Yaru Hao and Saksham Singhal and Shuming Ma and Tengchao Lv and Lei Cui and Owais Khan Mohammed and Barun Patra and Qiang Liu and Kriti Aggarwal and Zewen Chi and Johan Bjorck and Vishrav Chaudhary and Subhojit Som and Xia Song and Furu Wei},
  journal={arXiv preprint arXiv:2302.14045},
  year={2023},
  url={https://www.arxiv.org/abs/2302.14045}
}
@misc{chiu2024megacoinenhancingmediumgrainedcolor,
      title={MegaCOIN: Enhancing Medium-Grained Color Perception for Vision-Language Models}, 
      author={Ming-Chang Chiu and Shicheng Wen and Pin-Yu Chen and Xuezhe Ma},
      year={2024},
      eprint={2412.03927},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2412.03927}, 
}

@misc{chiu2024colorsensestudycolorvision,
      title={ColorSense: A Study on Color Vision in Machine Visual Recognition}, 
      author={Ming-Chang Chiu and Yingfei Wang and Derrick Eui Gyu Kim and Pin-Yu Chen and Xuezhe Ma},
      year={2024},
      eprint={2212.08650},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2212.08650}, 
}

@article{lu2021florence,
  title={Florence: A New Foundation Model for Computer Vision},
  author={Lu Yuan and Dongdong Chen and Yi-Ling Chen and Noel Codella and Xiyang Dai and Jianfeng Gao and Houdong Hu and Xuedong Huang and Boxin Li and Chunyuan Li and Ce Liu and Mengchen Liu and Zicheng Liu and Yumao Lu and Yu Shi and Lijuan Wang and Jianfeng Wang and Bin Xiao and Zhen Xiao and Jianwei Yang and Michael Zeng and Luowei Zhou and Pengchuan Zhang},
  journal={arXiv preprint arXiv:2111.11432},
  year={2021},
  url={https://www.arxiv.org/abs/2111.11432}
}


@article{tianhe2024grounded,
  title={Grounded SAM: Assembling Open-World Models for Diverse Visual Tasks},
  author={Tianhe Ren and Shilong Liu and Ailing Zeng and Jing Lin and Kunchang Li and He Cao and Jiayu Chen and Xinyu Huang and Yukang Chen and Feng Yan and Zhaoyang Zeng and Hao Zhang and Feng Li and Jie Yang and Hongyang Li and Qing Jiang and Lei Zhang},
  journal={arXiv preprint arXiv:2401.14159},
  year={2024},
  url={https://www.arxiv.org/abs/2401.14159}
}


@article{shilong2023grounding,
  title={Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection},
  author={Shilong Liu and Zhaoyang Zeng and Tianhe Ren and Feng Li and Hao Zhang and Jie Yang and Chunyuan Li and Jianwei Yang and Hang Su and Jun Zhu and Lei Zhang},
  journal={arXiv preprint arXiv:2303.05499},
  year={2023},
  url={https://www.arxiv.org/abs/2303.05499}
}


@article{shi2024eagle,
    title = {Eagle: Exploring The Design Space for Multimodal LLMs with Mixture of Encoders}, 
    author={Min Shi and Fuxiao Liu and Shihao Wang and Shijia Liao and Subhashree Radhakrishnan and De-An Huang and Hongxu Yin and Karan Sapra and Yaser Yacoob and Humphrey Shi and Bryan Catanzaro and Andrew Tao and Jan Kautz and Zhiding Yu and Guilin Liu},
    journal={arXiv:2408.15998},
    year={2024}
}

@article{yunhao2024vila2,
  title={VILA$^2$: VILA Augmented VILA},
  author={Yunhao Fang and Ligeng Zhu and Yao Lu and Yan Wang and Pavlo Molchanov and Jan Kautz and Jang Hyun Cho and Marco Pavone and Song Han and Hongxu Yin},
  journal={arXiv preprint arXiv:2407.17453},
  year={2024},
  url={https://www.arxiv.org/abs/2407.17453}
}

@article{haotian2023visual,
  title={Visual Instruction Tuning},
  author={Haotian Liu and Chunyuan Li and Qingyang Wu and Yong Jae Lee},
  journal={arXiv preprint arXiv:2304.08485},
  year={2023},
  url={https://www.arxiv.org/abs/2304.08485}
}

@article{jeanbaptiste2022flamingo,
  title={Flamingo: a Visual Language Model for Few-Shot Learning},
  author={Jean-Baptiste Alayrac and Jeff Donahue and Pauline Luc and Antoine Miech and Iain Barr and Yana Hasson and Karel Lenc and Arthur Mensch and Katie Millican and Malcolm Reynolds and Roman Ring and Eliza Rutherford and Serkan Cabi and Tengda Han and Zhitao Gong and Sina Samangooei and Marianne Monteiro and Jacob Menick and Sebastian Borgeaud and Andrew Brock and Aida Nematzadeh and Sahand Sharifzadeh and Mikolaj Binkowski and Ricardo Barreira and Oriol Vinyals and Andrew Zisserman and Karen Simonyan},
  journal={arXiv preprint arXiv:2204.14198},
  year={2022},
  url={https://www.arxiv.org/abs/2204.14198}
}

@article{fuxiao2023mitigating,
  title={Mitigating Hallucination in Large Multi-Modal Models via Robust Instruction Tuning},
  author={Fuxiao Liu and Kevin Lin and Linjie Li and Jianfeng Wang and Yaser Yacoob and Lijuan Wang},
  journal={arXiv preprint arXiv:2306.14565},
  year={2023},
  url={https://www.arxiv.org/abs/2306.14565}
}


@article{Chandra:81,
	author = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},
	year = "1981",
	title = {Alternation},
	journal = {Journal of the Association for Computing Machinery},
	volume = "28",
	number = "1",
	pages = "114--133",
	doi = "10.1145/322234.322243",
}

@inproceedings{andrew2007scalable,
  title={Scalable training of {L1}-regularized log-linear models},
  author={Andrew, Galen and Gao, Jianfeng},
  booktitle={Proceedings of the 24th International Conference on Machine Learning},
  pages={33--40},
  year={2007},
}

@book{Gusfield:97,
    author  = {Dan Gusfield},
    title   = {Algorithms on Strings, Trees and Sequences},
    year    = "1997",
    publisher = {Cambridge University Press},
    address = {Cambridge, UK}
}

@article{rasooli-tetrault-2015,
    author    = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
    title     = {Yara Parser: {A} Fast and Accurate Dependency Parser},
    journal   = {Computing Research Repository},
    volume    = {arXiv:1503.06733},
    year      = {2015},
    url       = {http://arxiv.org/abs/1503.06733},
    note    = {version 2}
}

@article{Ando2005,
	Acmid = {1194905},
	Author = {Ando, Rie Kubota and Zhang, Tong},
	Issn = {1532-4435},
	Issue_Date = {12/1/2005},
	Journal = {Journal of Machine Learning Research},
	Month = dec,
	Numpages = {37},
	Pages = {1817--1853},
	Publisher = {JMLR.org},
	Title = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
	Volume = {6},
	Year = {2005}
}

@article{driess2023palm,
  title={{PaLM-E}: An embodied multimodal language model},
  author={Driess, Danny and Xia, Fei and Sajjadi, Mehdi SM and Lynch, Corey and Chowdhery, Aakanksha and Ichter, Brian and Wahid, Ayzaan and Tompson, Jonathan and Vuong, Quan and Yu, Tianhe and others},
  journal={arXiv:2303.03378},
  year={2023}
}

@inproceedings{liu2023visual,
  title={Visual instruction tuning},
  author={Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
  booktitle={NeurIPS},
  year={2023}
}

@article{liu2023improved,
  title={Improved baselines with visual instruction tuning},
  author={Liu, Haotian and Li, Chunyuan and Li, Yuheng and Lee, Yong Jae},
  journal={arXiv:2310.03744},
  year={2023}
}

@inproceedings{siglip,
  title = {Sigmoid Loss for Language Image Pre-Training},
  author = {Xiaohua Zhai and Basil Mustafa and Alexander Kolesnikov and Lucas Beyer},
  booktitle = {ICCV},
  year = {2023}
}

@misc{liu2024llavanext,
  title = {{LLaVA-NeXT}: Improved reasoning, OCR, and world knowledge},
  author={Liu, Haotian and Li, Chunyuan and Li, Yuheng and Li, Bo and Zhang, Yuanhan and Shen, Sheng and Lee, Yong Jae},
  month={January},
  year={2024},
  howpublished = {\url{https://llava-vl.github.io/blog/2024-01-30-llava-next/}}
}

@inproceedings{alayrac2022flamingo,
  title={Flamingo: a visual language model for few-shot learning},
  author={Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katherine and Reynolds, Malcolm and others},
  booktitle={NeurIPS},
  year={2022}
}

@inproceedings{li2022blip,
  title={{BLIP}: Bootstrapping language-image pre-training for unified vision-language understanding and generation},
  author={Li, Junnan and Li, Dongxu and Xiong, Caiming and Hoi, Steven},
  booktitle={ICML},
  year={2022}
}

@misc{llama3modelcard,
  title={Llama 3 Model Card},
  author={AI@Meta},
  year={2024},
  url = {https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md}
}

@inproceedings{li2023blip,
  title={{BLIP-2}: Bootstrapping language-image pre-training with frozen image encoders and large language models},
  author={Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven},
  booktitle={ICML},
  year={2023}
}

@article{xu2024llava-uhd,
  title={{LLaVA-UHD}: an LMM Perceiving Any Aspect Ratio and High-Resolution Images},
  author={Xu, Ruyi and Yao, Yuan and Guo, Zonghao and Cui, Junbo and Ni, Zanlin and Ge, Chunjiang and Chua, Tat-Seng and Liu, Zhiyuan and Huang, Gao},
  journal={arXiv:2403.11703},
  year={2024}
}


@article{tong2024cambrian,
  title={Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs}, 
  author={Shengbang Tong and Ellis Brown and Penghao Wu and Sanghyun Woo and Manoj Middepogu and Sai Charitha Akula and Jihan Yang and Shusheng Yang and Adithya Iyer and Xichen Pan and Austin Wang and Rob Fergus and Yann LeCun and Saining Xie},
  journal={arXiv:2406.16860},
  year={2024}
}

@inproceedings{radio,
  author = {Ranzinger, Mike and Heinrich, Greg and Kautz, Jan and Molchanov, Pavlo},
  title = {{AM-RADIO}: Agglomerative Vision Foundation Model Reduce All Domains Into One},
  booktitle = {CVPR},
  year = {2024}
}

@article{damonlpsg2023videollama,
  author = {Zhang, Hang and Li, Xin and Bing, Lidong},
  title = {Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding},
  year = 2023,
  journal = {arXiv:2306.02858}
}

@inproceedings{3dllm,
  author = {Yining Hong and Haoyu Zhen and Peihao Chen and Shuhong Zheng and Yilun Du and Zhenfang Chen and Chuang Gan},
  title = {3D-LLM: Injecting the 3D World into Large Language Models},
  booktitle = {NeurIPS},
  year = {2023}
}

@inproceedings{dai2024instructblip,
  title={{InstructBLIP}: Towards general-purpose vision-language models with instruction tuning},
  author={Dai, Wenliang and Li, Junnan and Li, Dongxu and Tiong, Anthony Meng Huat and Zhao, Junqi and Wang, Weisheng and Li, Boyang and Fung, Pascale N and Hoi, Steven},
  booktitle={NeurIPS},
  year={2024}
}

@inproceedings{chen2023pali,
  title={{PaLI}: A jointly-scaled multilingual language-image model},
  author={Chen, Xi and Wang, Xiao and Changpinyo, Soravit and Piergiovanni, AJ and Padlewski, Piotr and Salz, Daniel and Goodman, Sebastian and Grycner, Adam and Mustafa, Basil and Beyer, Lucas and others},
  booktitle={ICLR},
  year={2023}
}

@article{chen2023palix,
  title={{PaLI-X}: On scaling up a multilingual vision and language model},
  author={Chen, Xi and Djolonga, Josip and Padlewski, Piotr and Mustafa, Basil and Changpinyo, Soravit and Wu, Jialin and Ruiz, Carlos Riquelme and Goodman, Sebastian and Wang, Xiao and Tay, Yi and others},
  journal={arXiv:2305.18565},
  year={2023}
}

@article{chen2023pali3,
  title={{PaLI-3} vision language models: Smaller, faster, stronger},
  author={Chen, Xi and Wang, Xiao and Beyer, Lucas and Kolesnikov, Alexander and Wu, Jialin and Voigtlaender, Paul and Mustafa, Basil and Goodman, Sebastian and Alabdulmohsin, Ibrahim and Padlewski, Piotr and others},
  journal={arXiv:2310.09199},
  year={2023}
}

@article{beyer2024paligemma,
  title={PaliGemma: A versatile 3B VLM for transfer},
  author={Beyer, Lucas and Steiner, Andreas and Pinto, Andr{\'e} Susano and Kolesnikov, Alexander and Wang, Xiao and Salz, Daniel and Neumann, Maxim and Alabdulmohsin, Ibrahim and Tschannen, Michael and Bugliarello, Emanuele and others},
  journal={arXiv:2407.07726},
  year={2024}
}

@inproceedings{pixelshffule,
  author       = {Wenzhe Shi and
                  Jose Caballero and
                  Ferenc Huszar and
                  Johannes Totz and
                  Andrew P. Aitken and
                  Rob Bishop and
                  Daniel Rueckert and
                  Zehan Wang},
  title        = {Real-Time Single Image and Video Super-Resolution Using an Efficient
                  Sub-Pixel Convolutional Neural Network},
  booktitle    = {{IEEE} Conference on Computer Vision and Pattern Recognition},
  pages        = {1874--1883},
  year         = {2016},
  doi          = {10.1109/CVPR.2016.207},
}

@article{lin2023vila,
  title={{VILA}: On pre-training for visual language models},
  author={Lin, Ji and Yin, Hongxu and Ping, Wei and Lu, Yao and Molchanov, Pavlo and Tao, Andrew and Mao, Huizi and Kautz, Jan and Shoeybi, Mohammad and Han, Song},
  journal={arXiv:2312.07533},
  year={2023}
}

@inproceedings{li2024monkey,
  title={Monkey: Image resolution and text label are important things for large multi-modal models},
  author={Li, Zhang and Yang, Biao and Liu, Qiang and Ma, Zhiyin and Zhang, Shuo and Yang, Jingxu and Sun, Yabo and Liu, Yuliang and Bai, Xiang},
  booktitle={CVPR},
  year={2024}
}

@article{shi2024we,
  title={When Do We Not Need Larger Vision Models?},
  author={Shi, Baifeng and Wu, Ziyang and Mao, Maolin and Wang, Xin and Darrell, Trevor},
  journal={arXiv:2403.13043},
  year={2024}
}

@article{zhang2023internlm,
  title={{InternLM-XComposer}: A vision-language large model for advanced text-image comprehension and composition},
  author={Zhang, Pan and Wang, Xiaoyi Dong Bin and Cao, Yuhang and Xu, Chao and Ouyang, Linke and Zhao, Zhiyuan and Ding, Shuangrui and Zhang, Songyang and Duan, Haodong and Yan, Hang and others},
  journal={arXiv:2309.15112},
  year={2023}
}

@article{internlmxcomposer2,
  title={{InternLM-XComposer2}: Mastering Free-form Text-Image Composition and Comprehension in Vision-Language Large Model},
  author={Xiaoyi Dong and Pan Zhang and Yuhang Zang and Yuhang Cao and Bin Wang and Linke Ouyang and Xilin Wei and Songyang Zhang and Haodong Duan and Maosong Cao and Wenwei Zhang and Yining Li and Hang Yan and Yang Gao and Xinyue Zhang and Wei Li and Jingwen Li and Kai Chen and Conghui He and Xingcheng Zhang and Yu Qiao and Dahua Lin and Jiaqi Wang},
  journal={arXiv:2401.16420},
  year={2024}
}

@article{dong2024internlm2hd,
  title={{InternLM-XComposer2-4KHD}: A Pioneering Large Vision-Language Model Handling Resolutions from 336 Pixels to 4K HD},
  author={Dong, Xiaoyi and Zhang, Pan and Zang, Yuhang and Cao, Yuhang and Wang, Bin and Ouyang, Linke and Zhang, Songyang and Duan, Haodong and Zhang, Wenwei and Li, Yining and others},
  journal={arXiv:2404.06512},
  year={2024}
}

@article{chen2023internvl,
  title={{InternVL}: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks},
  author={Chen, Zhe and Wu, Jiannan and Wang, Wenhai and Su, Weijie and Chen, Guo and Xing, Sen and Zhong, Muyan and Zhang, Qinglong and Zhu, Xizhou and Lu, Lewei and Li, Bin and Luo, Ping and Lu, Tong and Qiao, Yu and Dai, Jifeng},
  journal={arXiv:2312.14238},
  year={2023}
}

@article{chen2024far,
  title={How Far Are We to GPT-4V? Closing the Gap to Commercial Multimodal Models with Open-Source Suites},
  author={Chen, Zhe and Wang, Weiyun and Tian, Hao and Ye, Shenglong and Gao, Zhangwei and Cui, Erfei and Tong, Wenwen and Hu, Kongzhi and Luo, Jiapeng and Ma, Zheng and others},
  journal={arXiv:2404.16821},
  year={2024}
}

@article{luo2024feast,
  title={Feast Your Eyes: Mixture-of-Resolution Adaptation for Multimodal Large Language Models},
  author={Luo, Gen and Zhou, Yiyi and Zhang, Yuxin and Zheng, Xiawu and Sun, Xiaoshuai and Ji, Rongrong},
  journal={arXiv:2403.03003},
  year={2024}
}

@article{li2024mini,
  title={{Mini-Gemini}: Mining the potential of multi-modality vision language models},
  author={Li, Yanwei and Zhang, Yuechen and Wang, Chengyao and Zhong, Zhisheng and Chen, Yixin and Chu, Ruihang and Liu, Shaoteng and Jia, Jiaya},
  journal={arXiv:2403.18814},
  year={2024}
}

@article{zong2024mova,
  title={{MoVA}: Adapting Mixture of Vision Experts to Multimodal Context},
  author={Zong, Zhuofan and Ma, Bingqi and Shen, Dazhong and Song, Guanglu and Shao, Hao and Jiang, Dongzhi and Li, Hongsheng and Liu, Yu},
  journal={arXiv:2404.13046},
  year={2024}
}

@article{xu2024llava,
  title={{LLaVA-UHD}: an lmm perceiving any aspect ratio and high-resolution images},
  author={Xu, Ruyi and Yao, Yuan and Guo, Zonghao and Cui, Junbo and Ni, Zanlin and Ge, Chunjiang and Chua, Tat-Seng and Liu, Zhiyuan and Sun, Maosong and Huang, Gao},
  journal={arXiv:2403.11703},
  year={2024}
}

@article{liu2024prismer,
  title={Prismer: A vision-language model with an ensemble of experts},
  author={Liu, Shikun and Fan, Linxi and Johns, Edward and Yu, Zhiding and Xiao, Chaowei and Anandkumar, Anima},
  journal={TMLR},
  year={2024}
}

@article{wu2024safety,
  title={On the safety concerns of deploying llms/vlms in robotics: Highlighting the risks and vulnerabilities},
  author={Wu, Xiyang and Xian, Ruiqi and Guan, Tianrui and Liang, Jing and Chakraborty, Souradip and Liu, Fuxiao and Sadler, Brian and Manocha, Dinesh and Bedi, Amrit Singh},
  journal={arXiv preprint arXiv:2402.10340},
  year={2024}
}

@article{li2024mosaic,
  title={Mosaic IT: Enhancing Instruction Tuning with Data Mosaics},
  author={Li, Ming and Chen, Pei and Wang, Chenguang and Zhao, Hongyu and Liang, Yijun and Hou, Yupeng and Liu, Fuxiao and Zhou, Tianyi},
  journal={arXiv preprint arXiv:2405.13326},
  year={2024}
}

@article{liu2024large,
  title={Large language models and causal inference in collaboration: A comprehensive survey},
  author={Liu, Xiaoyu and Xu, Paiheng and Wu, Junda and Yuan, Jiaxin and Yang, Yifan and Zhou, Yuhang and Liu, Fuxiao and Guan, Tianrui and Wang, Haoliang and Yu, Tong and others},
  journal={arXiv preprint arXiv:2403.09606},
  year={2024}
}

@article{fan2024mousi,
  title={{MouSi}: Poly-Visual-Expert Vision-Language Models},
  author={Fan, Xiaoran and Ji, Tao and Jiang, Changhao and Li, Shuo and Jin, Senjie and Song, Sirui and Wang, Junke and Hong, Boyang and Chen, Lu and Zheng, Guodong and others},
  journal={arXiv:2401.17221},
  year={2024}
}

@article{kar2024brave,
  title={{BRAVE}: Broadening the visual encoding of vision-language models},
  author={Kar, O{\u{g}}uzhan Fatih and Tonioni, Alessio and Poklukar, Petra and Kulshrestha, Achin and Zamir, Amir and Tombari, Federico},
  journal={arXiv:2404.07204},
  year={2024}
}

@article{jiao2024enhancing,
  title={Enhancing Multimodal Large Language Models with Vision Detection Models: An Empirical Study},
  author={Jiao, Qirui and Chen, Daoyuan and Huang, Yilun and Li, Yaliang and Shen, Ying},
  journal={arXiv:2401.17981},
  year={2024}
}

@article{lee2024moai,
  title={{MoAI}: Mixture of All Intelligence for Large Language and Vision Models},
  author={Lee, Byung-Kwan and Park, Beomchan and Kim, Chae Won and Ro, Yong Man},
  journal={arXiv:2403.07508},
  year={2024}
}

@article{lin2023sphinx,
  title={{SPHINX}: The joint mixing of weights, tasks, and visual embeddings for multi-modal large language models},
  author={Lin, Ziyi and Liu, Chris and Zhang, Renrui and Gao, Peng and Qiu, Longtian and Xiao, Han and Qiu, Han and Lin, Chen and Shao, Wenqi and Chen, Keqin and others},
  journal={arXiv:2311.07575},
  year={2023}
}
@article{liu2023hallusionbench,
  title={{HallusionBench}: You see what you think? or you think what you see? an image-context reasoning benchmark challenging for gpt-4v (ision), llava-1.5, and other multi-modality models},
  author={Liu, Fuxiao and Guan, Tianrui and Li, Zongxia and Chen, Lichang and Yacoob, Yaser and Manocha, Dinesh and Zhou, Tianyi},
  journal={arXiv:2310.14566},
  year={2023}
}
@inproceedings{fei2024multimodal,
  title={From Multimodal LLM to Human-level AI: Modality, Instruction, Reasoning, Efficiency and Beyond},
  author={Fei, Hao and Yao, Yuan and Zhang, Zhuosheng and Liu, Fuxiao and Zhang, Ao and Chua, Tat-Seng},
  booktitle={LREC-Coling Tutorials},
  year={2024}
}
@article{liu2020visual,
  title={{Visual News}: Benchmark and challenges in news image captioning},
  author={Liu, Fuxiao and Wang, Yinghan and Wang, Tianlu and Ordonez, Vicente},
  journal={arXiv:2010.03743},
  year={2020}
}
@article{liu2023mmc,
  title={{MMC}: Advancing multimodal chart understanding with large-scale instruction tuning},
  author={Liu, Fuxiao and Wang, Xiaoyang and Yao, Wenlin and Chen, Jianshu and Song, Kaiqiang and Cho, Sangwoo and Yacoob, Yaser and Yu, Dong},
  journal={arXiv:2311.10774},
  year={2023}
}

@article{zeng2023matters,
  title={What matters in training a gpt4-style language model with multimodal inputs?},
  author={Zeng, Yan and Zhang, Hanbo and Zheng, Jiani and Xia, Jiangnan and Wei, Guoqiang and Wei, Yang and Zhang, Yuchen and Kong, Tao},
  journal={arXiv:2307.02469},
  year={2023}
}

@article{team2023gemini,
  title={Gemini: a family of highly capable multimodal models},
  author={Team, Gemini and Anil, Rohan and Borgeaud, Sebastian and Wu, Yonghui and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and others},
  journal={arXiv:2312.11805},
  year={2023}
}

@article{achiam2023gpt,
  title={{GPT-4} technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv:2303.08774},
  year={2023}
}

@article{li2024multimodal,
  title={Multimodal foundation models: From specialists to general-purpose assistants},
  author={Li, Chunyuan and Gan, Zhe and Yang, Zhengyuan and Yang, Jianwei and Li, Linjie and Wang, Lijuan and Gao, Jianfeng and others},
  journal={Foundations and Trends{\textregistered} in Computer Graphics and Vision},
  year={2024}
}

@article{zhu2023minigpt,
  title={{MiniGPT-4}: Enhancing Vision-Language Understanding with Advanced Large Language Models},
  author={Zhu, Deyao and Chen, Jun and Shen, Xiaoqian and Li, Xiang and Elhoseiny, Mohamed},
  journal={arXiv:2304.10592},
  year={2023}
}

@article{chen2023minigpt,
  title={{MiniGPT-v2}: Large language model as a unified interface for vision-language multi-task learning},
  author={Chen, Jun and Zhu, Deyao and Shen, Xiaoqian and Li, Xiang and Liu, Zechun and Zhang, Pengchuan and Krishnamoorthi, Raghuraman and Chandra, Vikas and Xiong, Yunyang and Elhoseiny, Mohamed},
  journal={arXiv:2310.09478},
  year={2023}
}

@article{li2023otter,
  title={Otter: A Multi-Modal Model with In-Context Instruction Tuning},
  author={Li, Bo and Zhang, Yuanhan and Chen, Liangyu and Wang, Jinghao and Yang, Jingkang and Liu, Ziwei},
  journal={arXiv:2305.03726},
  year={2023}
}

@article{bai2023qwen,
  title={{Qwen-VL}: A frontier large vision-language model with versatile abilities},
  author={Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},
  journal={arXiv:2308.12966},
  year={2023}
}

@article{he2024incorporating,
  title={Incorporating Visual Experts to Resolve the Information Loss in Multimodal Large Language Models},
  author={He, Xin and Wei, Longhui and Xie, Lingxi and Tian, Qi},
  journal={arXiv:2401.03105},
  year={2024}
}

@article{yin2024survey,
  title={A survey on multimodal large language models},
  author={Yin, Shukang and Fu, Chaoyou and Zhao, Sirui and Li, Ke and Sun, Xing and Xu, Tong and Chen, Enhong},
  journal={IEEE Trans. PAMI},
  year={2024}
}

@article{wang2023cogvlm,
  title={{CogVLM}: Visual expert for pretrained language models},
  author={Wang, Weihan and Lv, Qingsong and Yu, Wenmeng and Hong, Wenyi and Qi, Ji and Wang, Yan and Ji, Junhui and Yang, Zhuoyi and Zhao, Lei and Song, Xixuan and others},
  journal={arXiv:2311.03079},
  year={2023}
}

@inproceedings{hong2024cogagent,
  title={{CogAgent}: A visual language model for gui agents},
  author={Hong, Wenyi and Wang, Weihan and Lv, Qingsong and Xu, Jiazheng and Yu, Wenmeng and Ji, Junhui and Wang, Yan and Wang, Zihan and Dong, Yuxiao and Ding, Ming and others},
  booktitle={CVPR},
  year={2024}
}

@article{dubey2024llama,
  title={The {Llama} 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv:2407.21783},
  year={2024}
}

%======================================== Vision Foundation Models & Encoders ========================================

@inproceedings{jia2021scaling,
  title={Scaling up visual and vision-language representation learning with noisy text supervision},
  author={Jia, Chao and Yang, Yinfei and Xia, Ye and Chen, Yi-Ting and Parekh, Zarana and Pham, Hieu and Le, Quoc and Sung, Yun-Hsuan and Li, Zhen and Duerig, Tom},
  booktitle={ICML},
  year={2021},
}

@inproceedings{liu2022convnet,
  title={A convnet for the 2020s},
  author={Liu, Zhuang and Mao, Hanzi and Wu, Chao-Yuan and Feichtenhofer, Christoph and Darrell, Trevor and Xie, Saining},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{woo2023convnext,
  title={{ConvNeXt V2}: Co-designing and scaling convnets with masked autoencoders},
  author={Woo, Sanghyun and Debnath, Shoubhik and Hu, Ronghang and Chen, Xinlei and Liu, Zhuang and Kweon, In So and Xie, Saining},
  booktitle={CVPR},
  year={2023}
}

@article{oquab2023dinov2,
  title={{DINOv2}: Learning robust visual features without supervision},
  author={Oquab, Maxime and Darcet, Timoth{\'e}e and Moutakanni, Th{\'e}o and Vo, Huy and Szafraniec, Marc and Khalidov, Vasil and Fernandez, Pierre and Haziza, Daniel and Massa, Francisco and El-Nouby, Alaaeldin and others},
  journal={arXiv:2304.07193},
  year={2023}
}

@inproceedings{deformable-detr,
  title = {{Deformable DETR}: Deformable Transformers for End-to-End Object Detection},
  author = {Xizhou Zhu and Weijie Su and Lewei Lu and Bin Li and Xiaogang Wang and Jifeng Dai},
  booktitle = {ICLR},
  year = {2021}
}

@inproceedings{kirillov2023segment,
  title={Segment anything},
  author={Kirillov, Alexander and Mintun, Eric and Ravi, Nikhila and Mao, Hanzi and Rolland, Chloe and Gustafson, Laura and Xiao, Tete and Whitehead, Spencer and Berg, Alexander C and Lo, Wan-Yen and others},
  booktitle={ICCV},
  year={2023}
}
@inproceedings{lee2023pix2struct,
  title={{Pix2Struct}: Screenshot parsing as pretraining for visual language understanding},
  author={Lee, Kenton and Joshi, Mandar and Turc, Iulia Raluca and Hu, Hexiang and Liu, Fangyu and Eisenschlos, Julian Martin and Khandelwal, Urvashi and Shaw, Peter and Chang, Ming-Wei and Toutanova, Kristina},
  booktitle={ICML},
  year={2023}
}

@article{liu2023hidden,
  title={On the Hidden Mystery of OCR in Large Multimodal Models},
  author={Yuliang Liu and Zhang Li and Biao Yang and Chunyuan Li and Xucheng Yin and Cheng-lin Liu and Lianwen Jin and Xiang Bai},
  journal={arXiv:2305.07895},
  year={2023}
}

@inproceedings{fang2023eva,
  title={{EVA}: Exploring the limits of masked visual representation learning at scale},
  author={Fang, Yuxin and Wang, Wen and Xie, Binhui and Sun, Quan and Wu, Ledell and Wang, Xinggang and Huang, Tiejun and Wang, Xinlong and Cao, Yue},
  booktitle={CVPR},
  year={2023}
}

@article{li2024mosaic,
  title={Mosaic IT: Enhancing Instruction Tuning with Data Mosaics},
  author={Li, Ming and Chen, Pei and Wang, Chenguang and Zhao, Hongyu and Liang, Yijun and Hou, Yupeng and Liu, Fuxiao and Zhou, Tianyi},
  journal={arXiv preprint arXiv:2405.13326},
  year={2024}
}

@article{fang2023eva02,
  title={{EVA-02}: A Visual Representation for Neon Genesis},
  author={Fang, Yuxin and Sun, Quan and Wang, Xinggang and Huang, Tiejun and Wang, Xinlong and Cao, Yue},
  journal={arXiv:2303.11331},
  year={2023}
}

@article{sun2023eva,
  title={{EVA-CLIP}: Improved training techniques for clip at scale},
  author={Sun, Quan and Fang, Yuxin and Wu, Ledell and Wang, Xinlong and Cao, Yue},
  journal={arXiv:2303.15389},
  year={2023}
}

@software{ilharco_gabriel_2021_5143773,
  author = {Ilharco, Gabriel and
                  Wortsman, Mitchell and
                  Wightman, Ross and
                  Gordon, Cade and
                  Carlini, Nicholas and
                  Taori, Rohan and
                  Dave, Achal and
                  Shankar, Vaishaal and
                  Namkoong, Hongseok and
                  Miller, John and
                  Hajishirzi, Hannaneh and
                  Farhadi, Ali and
                  Schmidt, Ludwig},
  title        = {OpenCLIP},
  month        = jul,
  year         = 2021,
  note         = {If you use this software, please cite it as below.},
  publisher    = {Zenodo},
  version      = {0.1},
  doi          = {10.5281/zenodo.5143773},
  url          = {https://doi.org/10.5281/zenodo.5143773}
}

@inproceedings{cherti2023reproducible,
  title={Reproducible scaling laws for contrastive language-image learning},
  author={Cherti, Mehdi and Beaumont, Romain and Wightman, Ross and Wortsman, Mitchell and Ilharco, Gabriel and Gordon, Cade and Schuhmann, Christoph and Schmidt, Ludwig and Jitsev, Jenia},
  booktitle={CVPR},
  year={2023}
}

@inproceedings{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={ICML},
  year={2021}
}

@inproceedings{schuhmann2022laionb,
  title={{LAION-5B}: An open large-scale dataset for training next generation image-text models},
  author={Christoph Schuhmann and
          Romain Beaumont and
          Richard Vencu and
          Cade W Gordon and
          Ross Wightman and
          Mehdi Cherti and
          Theo Coombes and
          Aarush Katta and
          Clayton Mullis and
          Mitchell Wortsman and
          Patrick Schramowski and
          Srivatsa R Kundurthy and
          Katherine Crowson and
          Ludwig Schmidt and
          Robert Kaczmarczyk and
          Jenia Jitsev},
  booktitle={NeurIPS Datasets and Benchmarks Track},
  year={2022},
  url={https://openreview.net/forum?id=M3Y74vmsMcY}
}

@misc{big_vision,
  author = {Beyer, Lucas and Zhai, Xiaohua and Kolesnikov, Alexander},
  title = {Big Vision},
  year = {2022},
  howpublished = {\url{https://github.com/google-research/big_vision}}
}

%======================================== MLLM Benchmarks ========================================

@article{fu2023mme,
  title={{MME}: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models},
  author={Fu, Chaoyou and Chen, Peixian and Shen, Yunhang and Qin, Yulei and Zhang, Mengdan and Lin, Xu and Qiu, Zhenyu and Lin, Wei and Yang, Jinrui and Zheng, Xiawu and others},
  journal={arXiv:2306.13394},
  year={2023}
}
@inproceedings{hudson2019gqa,
  title={{GQA}: A new dataset for real-world visual reasoning and compositional question answering},
  author={Hudson, Drew A and Manning, Christopher D},
  booktitle={CVPR},
  year={2019}
}
@article{saikh2022scienceqa,
  title={{ScienceQA}: A novel resource for question answering on scholarly articles},
  author={Saikh, Tanik and Ghosal, Tirthankar and Mittal, Amish and Ekbal, Asif and Bhattacharyya, Pushpak},
  journal={International Journal on Digital Libraries},
  year={2022}
}

@inproceedings{yu2024mmvet,
  title={{MM-Vet}: Evaluating large multimodal models for integrated capabilities},
  author={Yu, Weihao and Yang, Zhengyuan and Li, Linjie and Wang, Jianfeng and Lin, Kevin and Liu, Zicheng and Wang, Xinchao and Wang, Lijuan},
  booktitle={ICML},
  year={2024}
}

@inproceedings{singh2019towards,
  title = {Towards {VQA} models that can read},
  author = {Singh, Amanpreet and Natarajan, Vivek and Shah, Meet and Jiang, Yu and Chen, Xinlei and Batra, Dhruv and Parikh, Devi and Rohrbach, Marcus},
  booktitle = {CVPR},
  year = {2019}
}

@inproceedings{mathew2021docvqa,
  title={{DocVQA}: A Dataset for VQA on Document Images}, 
  author={Minesh Mathew and Dimosthenis Karatzas and C. V. Jawahar},
  booktitle = {WACV},
  year={2021}
}

@article{masry2022chartqa,
  title={{ChartQA}: A Benchmark for Question Answering about Charts with Visual and Logical Reasoning},
  author={Ahmed Masry and Do Xuan Long and Jia Qing Tan and Shafiq Joty and Enamul Hoque},
  journal={arXiv:2203.10244},
  year={2022}
}

@article{li2023mimic,
  title={{MIMIC-IT}: Multi-modal in-context instruction tuning},
  author={Li, Bo and Zhang, Yuanhan and Chen, Liangyu and Wang, Jinghao and Pu, Fanyi and Yang, Jingkang and Li, Chunyuan and Liu, Ziwei},
  journal={arXiv:2306.05425},
  year={2023}
}

%======================================== Misc ========================================

@misc{vicuna2023,
    title = {Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90\%* ChatGPT Quality},
    howpublished = {\url{https://lmsys.org/blog/2023-03-30-vicuna/}},
    author = {Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E. and Stoica, Ion and Xing, Eric P.},
    month = {March},
    year = {2023}
}

@article{gokhale2021semantically,
  title={Semantically Distributed Robust Optimization for Vision-and-Language Inference},
  author={Gokhale, Tejas and Chaudhary, Abhishek and Banerjee, Pratyay and Baral, Chitta and Yang, Yezhou},
  journal={arXiv:2110.07165},
  year={2021}
}

@article{li2023evaluating,
  title={Evaluating Object Hallucination in Large Vision-Language Models},
  author={Li, Yifan and Du, Yifan and Zhou, Kun and Wang, Jinpeng and Zhao, Wayne Xin and Wen, Ji-Rong},
  journal={arXiv:2305.10355},
  year={2023}
}

@article{chen2023sharegpt4v,
  title={{ShareGPT4V}: Improving Large Multi-Modal Models with Better Captions},
  author={Chen, Lin and Li, Jisong and Dong, Xiaoyi and Zhang, Pan and He, Conghui and Wang, Jiaqi and Zhao, Feng and Lin, Dahua},
  journal={arXiv:2311.12793},
  year={2023}
}

@inproceedings{kim2022donut,
  title = {{OCR-Free} Document Understanding Transformer},
  author = {Kim, Geewook and Hong, Teakgyu and Yim, Moonbin and Nam, JeongYeon and Park, Jinyoung and Yim, Jinyeong and Hwang, Wonseok and Yun, Sangdoo and Han, Dongyoon and Park, Seunghyun},
  booktitle = {ECCV},
  year = {2022}
}

@inproceedings{kafle2018dvqa,
  title={{DVQA}: Understanding Data Visualizations via Question Answering},
  author={Kafle, Kushal and Cohen, Scott and Price, Brian and Kanan, Christopher},
  booktitle={CVPR},
  year={2018}
}

@article{Kembhavi2016ADI,
  title={A Diagram is Worth a Dozen Images},
  author={Aniruddha Kembhavi and Michael Salvato and Eric Kolve and Minjoon Seo and Hannaneh Hajishirzi and Ali Farhadi},
  journal={arXiv:1603.07396},
  year={2016}
}

@article{wang2023instruct4v,
  title={To See is to Believe: Prompting GPT-4V for Better Visual Instruction Tuning},
  author={Wang, Junke and Meng, Lingchen and Weng, Zejia and He, Bo and Wu, Zuxuan and Jiang, Yu-Gang},
  journal={arXiv:2311.07574},
  year={2023}
}

@article{liu2023aligning,
  title={Aligning Large Multi-Modal Model with Robust Instruction Tuning},
  author={Liu, Fuxiao and Lin, Kevin and Li, Linjie and Wang, Jianfeng and Yacoob, Yaser and Wang, Lijuan},
  journal={arXiv:2306.14565},
  year={2023}
}

@article{geo170k,
  title = {G-LLaVA: Solving Geometric Problem with Multi-Modal Large Language Model},
  author = {Jiahui Gao and Renjie Pi and Jipeng Zhang and Jiacheng Ye and Wanjun Zhong and Yufei Wang and Lanqing Hong and Jianhua Han and Hang Xu and Zhenguo Li and Lingpeng Kong},
  journal = {arXiv:2312.11370},
  year = {2023}
}

@misc{OpenHermes2.5,
  title = {{OpenHermes 2.5}: An Open Dataset of Synthetic Data for Generalist {LLM} Assistants},
  author = {Teknium},
  year = {2023},
  howpublished = {\url{https://huggingface.co/datasets/teknium/OpenHermes-2.5}},
  publisher = {HuggingFace}
}

@misc{RWQA,
  title = {{Grok-1.5 Vision Preview}},
  author = {xAI},
  year ={2024},
  howpublished = {\url{https://x.ai/blog/grok-1.5v}}
}

@article{zhang2023llavar,
  title={{LLaVAR}: Enhanced Visual Instruction Tuning for Text-Rich Image Understanding},
  author={Yanzhe Zhang and Ruiyi Zhang and Jiuxiang Gu and Yufan Zhou and Nedim Lipka and Diyi Yang and Tong Sun},
  journal={arXiv:2306.17107},
  year={2023}
}

@inproceedings{zhu2016cvpr,
  title = {{Visual7W: Grounded Question Answering in Images}},
  author = {Yuke Zhu and Oliver Groth and Michael Bernstein and Li Fei-Fei},
  booktitle = {CVPR},
  year = {2016}
}

@misc{lmms_eval2024,
  title = {{LMMs-Eval}: Accelerating the Development of Large Multimoal Models},
  howpublished = {\url{https://github.com/EvolvingLMMs-Lab/lmms-eval}},
  author = {Li, Bo and Zhang, Peiyuan and Zhang, Kaichen and Pu, Fanyi and Du xinrun and Dong, Yuhao and Liu, Haotian and Zhang, Yuanhan and Zhang, Ge and Li, Chunyuan and Liu, Ziwei},
  month = {March},
  year = {2024}
}

@inproceedings{mathew2022infovqa,
  title={InfographicVQA}, 
  author={Mathew, Minesh and Bagal, Viraj and Tito, Rub√®n and Karatzas, Dimosthenis and Valveny, Ernest and Jawahar, C. V.},
  booktitle={WACV},
  year={2022}
}

@inproceedings{lu2024mathvista,
  title = {{MathVista}: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts},
  author = {Lu, Pan and Bansal, Hritik and Xia, Tony and Liu, Jiacheng and Li, Chunyuan and Hajishirzi, Hannaneh and Cheng, Hao and Chang, Kai-Wei and Galley, Michel and Gao, Jianfeng},
  booktitle={ICLR},
  year = {2024}
}

@inproceedings{balanced_vqa_v2,
  title = {Making the {V} in {VQA} Matter: Elevating the Role of Image Understanding in {V}isual {Q}uestion {A}nswering},
  author = {Yash Goyal and Tejas Khot and Douglas Summers{-}Stay and Dhruv Batra and Devi Parikh},
  booktitle = {CVPR},
  year = {2017}
}

@article{MMBench,
  title = {{MMBench}: Is Your Multi-modal Model an All-around Player?},
  author = {Yuan Liu and Haodong Duan and Yuanhan Zhang and Bo Li and Songyang Zhang and Wangbo Zhao and Yike Yuan and Jiaqi Wang and Conghui He and Ziwei Liu and Kai Chen and Dahua Lin},
  journal = {arXiv:2307.06281},
  year = {2023}
}

@article{wang2022self,
  title={Self-instruct: Aligning language models with self-generated instructions},
  author={Wang, Yizhong and Kordi, Yeganeh and Mishra, Swaroop and Liu, Alisa and Smith, Noah A and Khashabi, Daniel and Hajishirzi, Hannaneh},
  journal={arXiv preprint arXiv:2212.10560},
  year={2022}
}

@article{li2023seed,
  title={{Seed-Bench}: Benchmarking multimodal llms with generative comprehension},
  author={Li, Bohao and Wang, Rui and Wang, Guangzhi and Ge, Yuying and Ge, Yixiao and Shan, Ying},
  journal={arXiv:2307.16125},
  year={2023}
}

@inproceedings{2018vizwiz,
  title={VizWiz Grand Challenge: Answering Visual Questions from Blind People},
  author={Gurari, Danna and Li, Qing and Stangl, Abigale J. and Guo, Anhong and Lin, Chi and Grauman, Kristen and Luo, Jiebo and Bigham, Jeffrey P.},
  booktitle={CVPR},
  year={2018}
}

@article{shi2024need,
  title={When Do We Not Need Larger Vision Models?}, 
  author={Baifeng Shi and Ziyang Wu and Maolin Mao and Xin Wang and Trevor Darrell},
  journal = {arXiv:2403.13043},
  year={2024}
}

@misc{laion-gpt4v,
  title = {{LAION-GPT4v} dataset},
  author = {},
  year = {2023},
  howpublished = {\url{https://huggingface.co/datasets/laion/gpt4v-dataset}},
  publisher = {HuggingFace}
}