\section{Related Work}
\paragraph{Knowledge Distillation.} Traditional methods for improving VLMs rely on knowledge distillation ____, where a larger ``teacher" model generates training data to enhance a smaller ``student" model. While effective for intermediate-scale models ____, this paradigm creates a dependency on the availability of superior models, which limits its applicability to state-of-the-art systems. %Approaches like recursive self-improvement ____ have shown promise in leveraging larger models for training smaller ones, but their scalability may be inherently constrained.

\paragraph{Specialized Models.}Recent studies ____ highlight the superiority of domain-specific expert models in certain tasks. For example, object detection systems such as Grounding DINO and OCR models like PaddleOCR ____ significantly outperform general-purpose VLMs in their respective domains ____. These findings underscore the potential of leveraging specialized models to complement the general capabilities of VLMs.
\vspace{-2mm}
\paragraph{Data Synthesis and Augmentation.} Existing methods for augmenting training data often involve the model generating synthetic examples ____ or applying templates to initial human annotations for more truthful data ____. While this approach can enhance performance on specific benchmarks, it risks perpetuating the biases and limitations of the model, resulting in diminishing returns. In contrast, \aide~integrates external expert knowledge and the original samples into the data generation pipeline, enabling more robust and unbiased improvements.



\vspace{-2mm}