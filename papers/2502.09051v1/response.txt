\section{Related Work}
\paragraph{Knowledge Distillation.} Traditional methods for improving VLMs rely on knowledge distillation **Bucilu«é et al., "Model Compression"**, where a larger ``teacher" model generates training data to enhance a smaller ``student" model. While effective for intermediate-scale models **Ba et al., "Do Deep Convolutional Nets Really See the World?"** , this paradigm creates a dependency on the availability of superior models, which limits its applicability to state-of-the-art systems. %Approaches like recursive self-improvement **Bengio et al., "Deep Learning of Representations for Unsupervised and Transfer Learning"**  have shown promise in leveraging larger models for training smaller ones, but their scalability may be inherently constrained.

\paragraph{Specialized Models.}Recent studies **Deng et al., "ImageNet Large Scale Visual Recognition Challenge"** highlight the superiority of domain-specific expert models in certain tasks. For example, object detection systems such as Grounding DINO and OCR models like PaddleOCR **Chao et al., "Real-time Hand Segmentation in the Wild"**  significantly outperform general-purpose VLMs in their respective domains **Russakovsky et al., "ImageNet Large Scale Visual Recognition Challenge"**. These findings underscore the potential of leveraging specialized models to complement the general capabilities of VLMs.
\vspace{-2mm}
\paragraph{Data Synthesis and Augmentation.} Existing methods for augmenting training data often involve the model generating synthetic examples **Goodfellow et al., "Generative Adversarial Networks"** or applying templates to initial human annotations for more truthful data **Hochreiter et al., "Long Short-Term Memory"**. While this approach can enhance performance on specific benchmarks, it risks perpetuating the biases and limitations of the model, resulting in diminishing returns. In contrast, \aide~integrates external expert knowledge and the original samples into the data generation pipeline, enabling more robust and unbiased improvements.



\vspace{-2mm}