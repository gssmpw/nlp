\section{Related Work}
\paragraph{Knowledge Distillation.} Traditional methods for improving VLMs rely on knowledge distillation \citep{wang2022self}, where a larger ``teacher" model generates training data to enhance a smaller ``student" model. While effective for intermediate-scale models \citep{haotian2023visual, fuxiao2023mitigating}, this paradigm creates a dependency on the availability of superior models, which limits its applicability to state-of-the-art systems. %Approaches like recursive self-improvement \citep{deepseek} have shown promise in leveraging larger models for training smaller ones, but their scalability may be inherently constrained.

\paragraph{Specialized Models.}Recent studies \cite{fei2024multimodal} highlight the superiority of domain-specific expert models in certain tasks. For example, object detection systems such as Grounding DINO and OCR models like PaddleOCR \cite{paddleocr} significantly outperform general-purpose VLMs in their respective domains \citep{lu2021florence, tianhe2024grounded, shilong2023grounding}. These findings underscore the potential of leveraging specialized models to complement the general capabilities of VLMs.
\vspace{-2mm}
\paragraph{Data Synthesis and Augmentation.} Existing methods for augmenting training data often involve the model generating synthetic examples \citep{fuxiao2023mitigating, chen2023sharegpt4v} or applying templates to initial human annotations for more truthful data \citep{chiu2024megacoinenhancingmediumgrainedcolor, chiu2024colorsensestudycolorvision}. While this approach can enhance performance on specific benchmarks, it risks perpetuating the biases and limitations of the model, resulting in diminishing returns. In contrast, \aide~integrates external expert knowledge and the original samples into the data generation pipeline, enabling more robust and unbiased improvements.



\vspace{-2mm}