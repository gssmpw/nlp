\section{Experiments} 
\label{sec:experiments}
In this section, we experimentally evaluate the reconstruction performance of \ours across different numbers of tokens, assess its applicability for class-conditional and text-conditional image generation, and compare it to relevant baselines. We show that \ours can effectively compress images into 1D sequences of flexible length, enabling a novel \textit{"visual vocabulary"} where images can be specified and generated in a coarse-to-fine manner.

\begin{figure*}[ht!]
\centering
\includegraphics[width=\textwidth]{resources/main/conditional_generation.pdf}
\caption{
\textbf{Image generation examples with varying numbers of tokens.}
Images generated with both class (top 3 rows) and text conditioning (bottom 3 rows) demonstrate that \ours-based models achieve high quality all the way down to a single token, and all within a single model. The conditioning alignment strengthens as more tokens are generated. For example with the prompt \textit{``a corgi's head depicted as an explosion of a nebula''}, the first two tokens capture the high-level concept of \textit{a artistic depiction of a dog}, while adding more tokens adds in further details such as the \textit{dog breed} and the \textit{nebula background}. For more visualizations, see \cref{sec:app_l2i_viz,sec:app_t2i_viz}.
}
\label{fig:gen_samples}
\end{figure*}

\begin{figure*}[ht!]
\centering
\includegraphics[width=\textwidth]{resources/main/specificity_and_quality.pdf}
\caption{
\textbf{Conditioning alignment and generation quality vs.\ number of tokens.}
\textbf{Left:} For class-conditional generation with a 1.3B AR model,
we compute DINOv2-L~\cite{Oquab2023DINOv2} top-1 accuracy
on generated images conditioned on ImageNet-1k class labels.
\textbf{Center:} For text-conditional generation with a 3B AR model,
we show CLIPScore relative to input prompts from the COCO 30k validation set~\cite{ms_coco_dataset},
using a CLIP base model .
\textbf{Right:} We measure class-conditional gFID on ImageNet-1k\textsuperscript{*},
and text-conditional gFID on COCO.
The AR models use guidance scales of 1.0 (no guidance) and 2.5, respectively. We follow the optimal inference parameters described in
\cref{sec:app_inference_hparam_sweeps,sec:app_c2i_hyper_params}.
}
\label{fig:specificity_vs_num_tokens}
\vspace{0.5em}  %
\begin{minipage}{0.95\textwidth}
\footnotesize
\textit{\textsuperscript{*} Note.} For evaluation of the class-conditioned image generation results,
we follow the common practice of measuring the generation FID (gFID) of 50K generated samples
relative to the reference statistics calculated \textit{over the entire training split} of the 
ImageNet-1k dataset~\cite{dhariwal2021diffusion}.
\end{minipage}
\end{figure*}

\begin{figure*}[ht!]
\centering
\includegraphics[width=\textwidth]{resources/main/prompt_complexity.pdf}
\caption{
\textbf{Image generation with simple and detailed prompts.} 
Images generated with \ours-based models show that the number of tokens needed to fulfill the conditioning depends on prompt complexity. For a simple prompt, the desired image is achieved with as few as 4-16 tokens (as measured by CLIPScore), and semantic variation between different decoded images (as measured by pairwise DreamSim scores) vanishes quickly. 
In contrast, a detailed prompt requires the full 256-token sequence to fully meet the conditioning and shows greater variation at lower token counts as the \ours rectified flow decoder compensates for missing details. 
For each prompt, the \ours tokens are generated just once using the AR Transformer and then decoded with 10 random seeds in the rectified flow decoder.
}
\label{fig:gen_simple_vs_complex}
\end{figure*}

\subsection{Flexible-length tokenization}
\label{sec:flex_length_tok}
We demonstrate \ours's variable-rate tokenization capability by evaluating its reconstruction performance on nested token sequences of different lengths. We perform comparisons using \ours models trained on ImageNet-1k, testing them on 256x256 pixel crops from the validation set~\cite{Russakovsky2014ImageNet}. In \cref{fig:reconst_rate_distortion} we measure reconstruction metrics (rFID, MAE, DreamSim) for $K_\text{keep} \in \{1, 2, 4, 8, 16, ..., 256\}$ tokens. \ours is able to generate plausible images, as measured through the rFID against the ImageNet-1k validation set, with as little as a single token. 

As shown in \cref{fig:reconst_dfn} with a \oursxlarge model trained on DFN, and in \cref{fig:reconst_visual_comp} with a \oursxlarge model trained on ImageNet-1k, reconstructions using the first few tokens capture high-level semantic features. As more tokens are used, both the alignment with the original image becomes more fine-grained and the image-wise reconstruction metrics (MAE and DreamSim) improve rapidly. 

Please see \cref{sec:app_tokens_vs_model_size_viz,sec:app_reconst_samples_viz,sec:app_reconst_comparison_viz} for additional reconstruction examples and comparisons. For linear probing experiments on the token sequences, see \cref{sec:app_probing}.

We find the following properties particularly noteworthy: (1) By performing nested dropout on the registers, \textit{a hierarchy emerges in which high-level concepts are ordered first}. (2) Through training \ours with a Rectified Flow decoder, \textit{any token subsequence can be decoded into a plausible image}. (3) The token sequences specify a \textit{distribution over images} that gets \textit{more and more specific} with more tokens, see \cref{fig:app_reconst_samples_0,fig:app_reconst_samples_1} for visual examples.


\subsection{Coarse-to-fine generation with increasing specificity}
\label{sec:specificity}

\begin{figure*}[ht!]
\centering
\includegraphics[width=\textwidth]{resources/main/in1k_generation_ar_model_size_sweep.pdf}
\caption{
\textbf{Class-conditioned AR model scaling.}
We show training loss, gFID and image generation CLIPScore values for the class-conditional models with the \oursxlarge tokenizer. We calculate the CLIPScore using the text label of the classes from the ImageNet-1k validation set, and do not use classifier-free guidance for the AR Transformer (CFG scale = 1.0).
}
\label{fig:main_c2i_ar_model_scaling}
\vspace{-1.5em}
\end{figure*}

As shown in \cref{sec:flex_length_tok}, \ours compresses images into ordered token sequences. This naturally leads us to explore the implications of \textit{predicting} these sequences for autoregressive image generation. By training class- and text-conditional models, we find that \ours token sequences act as a \textit{"visual vocabulary"}, allowing autoregressive models to describe images with increasing levels of specificity. Unlike conventional autoregressive models that generate images in a fixed raster-scan order on 2D token grids, our approach enables progressive refinement of image details. We also observe a clear \textit{relationship between conditioning complexity and token requirements}. Simple conditions, like ImageNet class labels, can be fulfilled with as few as 16 tokens, while more complex ones, like open-ended text prompts, benefit from generating up to 256 tokens.

In \cref{fig:gen_samples}, we show that images generated by class- or text-conditional models become increasingly specific to their conditioning as more tokens are produced. Our quantitative results in \cref{fig:specificity_vs_num_tokens} confirm this trend, showing that alignment between the conditioning signal and the generated images improves  with higher token counts. We measure alignment using DINOv2-L~\cite{Oquab2023DINOv2} classification accuracy for class conditioning and CLIPScore for text conditioning. Notably, text-image alignment continues to improve as additional tokens are generated, whereas classification accuracy tapers off after the first few tokens and plateaus around 32 (\cref{fig:specificity_vs_num_tokens,fig:main_c2i_ar_model_scaling}). Furthermore, we observe that generation quality remains consistent across all token sequence lengths, as measured by gFID, which we attribute to the strength of our rectified flow decoder.

Similarly, \cref{fig:gen_simple_vs_complex} shows that the number of tokens needed to generate prompt-aligned images varies significantly based on prompt complexity. Simple prompts like "a red apple" can produce satisfactory results with just 4 to 16 tokens, while detailed prompts like "graffiti of a rocket ship" benefit from using the full 256-token sequence. When fewer tokens are used, the model can still produce realistic images, but at the cost of greater variation between different random seeds. This variation decreases much more quickly for simple prompts than for detailed ones as token count increases, suggesting a fundamental relationship between prompt complexity and token requirements.

\subsection{Scaling autoregressive model size}
\label{sec:scaling_ar}

As shown in \cref{fig:specificity_vs_num_tokens}, using our largest autoregressive models, the alignment between condition and generations generally improves with more predicted tokens. In this section, we investigate the scaling behavior of autoregressive class-conditional models trained on \oursxlarge tokens, focusing on how model size impacts image-caption alignment and image fidelity. In \cref{fig:main_c2i_ar_model_scaling}, we show that increasing the AR model size consistently improves the measured training loss. However, for the prediction of the first few (1-8) tokens, the generation FID (gFID) values for the decoded images are effectively independent of the autoregressive model size, indicating that these short initial parts of the sequences are easily learned even by small models. In contrast, for long sequences ($>$ 128 tokens) the task becomes more challenging, and performance scales strongly with model size for both gFID and CLIPScore. 

This highlights an important trade-off between the \ours and AR model: as more tokens are generated, the AR model takes on a larger role in shaping the image. With few tokens, \ours's flow decoder drives most of the generation, resulting in low gFID even with smaller AR models. However, as the token count increases, an equally powerful AR model is needed to maintain strong performance.

\begin{table}[t]
\caption{\textbf{System-level comparison on ImageNet-1k class-conditional generation.} \ours generates variable length token sequences from 1 to 256 tokens long. The \ours tokenizers are combined with 1.33B parameter AR Transformers for class-conditioned image generation. For each \ours model we follow the optimal inference parameters and use no classifier-free guidance in the AR model, as described in \cref{sec:app_inference_hparam_sweeps,sec:app_c2i_hyper_params}. $^\dagger$ indicates \ours results for a sequence of \emph{32 tokens}. The "-re" suffix indicates the use of rejection sampling.
}
\label{tab:baseline_comparison}
\vspace{-0.5em}
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{@{}lllllll@{}}
\toprule
Tokenizer & \# tokens & Codebook size & rFID & gFID \\ 
\midrule
Taming VQ-GAN-re & 16x16 & 16384 & 4.98 & 5.20 \\
MaskGIT VQ-GAN & 16x16 & 1024 & 2.28 & 6.06 \\
Open MAGVIT-v2 & 16x16 & 262144 & 1.17 & 2.33 \\
LlamaGen & 16x16 & 16384 & 2.19 & 3.06  \\ 
TiTok-L & 32 & 4096 & 2.21 & 2.77  \\
TiTok-B & 64 & 4096 & 1.70 & 2.48  \\
TiTok-S & 128 & 4096 & 1.71 & 1.97  \\
\midrule
\oursbase   & 1-256 & 64000 & 4.20$^\dagger$ &  3.83$^\dagger$  \\
\ourslarge  & 1-256 & 64000 & 1.61$^\dagger$ &  2.02$^\dagger$  \\
\oursxlarge & 1-256 & 64000 & 1.45$^\dagger$ &  1.86$^\dagger$  \\ 
\bottomrule
\end{tabular}
}
\vspace{-1.5em}
\end{table}


\subsection{System-level comparison}
\label{sec:comparison}

In \cref{tab:baseline_comparison}, we compare our \ours models against several relevant baselines. We evaluate the tokenizers by training autoregressive models to perform ImageNet-1k class-conditional generation. Compared to previous 1D approaches \ours achieves superior reconstruction and generation quality at each token budget, all in a single model.

For comparison, we train a 2D grid-based tokenizer with a flow matching decoder, matching the \oursxlarge tokenizer in parameter count, training steps, and source dataset. Unlike \oursxlarge, it does not use register tokens, causal masking, or nested dropout. Text-conditioned image generation using an \ours tokenizer outperforms the 2D grid baseline when generating 2 to 128 tokens (\cref{fig:specificity_vs_num_tokens}). A full 256-token sequence with \ours yields better text-image alignment (CLIPScore) than the 2D grid tokenizer, despite a slight gFID regression compared to the 2D grid-based model.

