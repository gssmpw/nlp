\section{Discussion \& Conclusion}

In this work, we show the potential of a flexible sequence length tokenizer for image reconstruction and generation. Beyond enabling high-fidelity reconstructions with very few tokens, we demonstrate through training class- and text-conditional AR models that the \ours token sequences specify a \textit{``visual vocabulary''} that enables generation in a coarse-to-fine ordering. %

Our experiments suggest that depending on the complexity of the generation task, a model may be trained to stop the generation early as soon as the condition is fulfilled. While \ours can semantically compress images into as little as a single token, representing highly dense or structured content like text requires more tokens and training objectives that prioritize semantically meaningful concepts. We believe that these present exciting research directions to speed up and improve autoregressive image generation using adaptive compute budgets.

Looking ahead, we anticipate that \ours-like tokenizers, which adapt to the intrinsic complexity of the input data, could be applicable to other domains with high redundancy, such as audio and video. Training generative models on representations that can be both very compact and semantic, or very long and detailed, may enable further explorations into long-horizon video generation, understanding, as well as visual reasoning.
