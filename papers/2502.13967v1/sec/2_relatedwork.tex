\section{Related Work and Background}
\label{sec:relatedwork}

The primary role of an image tokenizer in generation tasks is to create compressed latent representations of images. Generative models need to learn a distribution over their outputs, but doing so directly in continuous, high-dimensional spaces (e.g., raw image pixels) is challenging. Compression helps by mapping images to a more compact latent space that removes imperceptible details~\cite{rombach2022high}, while discretization enables the generative model to output per-token categorical distributions that can be sampled from.

Vector-quantized autoencoders (VQ-VAEs)~\cite{van2017neural,razavi2019generating,esser2021taming} have become a standard framework for learning these discrete representations. VQ-VAE models operate through three core components: (1) an encoder $Enc$ that maps input images $\bm{X} \in \mathbb{R}^{H \times W \times 3}$ to $D$-channel latent embeddings $\bm{Z} = Enc(\bm{X}) \in \mathbb{R}^{h \times w \times D}$ (usually $h \ll H$ and $w \ll W$), (2) a quantizer $Quant$ that maps these continuous latents to discrete codes from a learned codebook, and (3) a decoder $Dec$ that reconstructs the image $\bm{\hat{X}} = Dec(Quant(\bm{Z}))$. This approach has proven versatile, finding applications in image~\cite{Chang2022MaskGIT, Chang2023Muse, Li2022MAGE}, audio~\cite{baevski2019vq}, and video generation~\cite{Villegas2022Phenaki,Hu2023GAIA1AG,Kondratyuk2023VideoPoet}, novel view synthesis~\cite{yan2021videogpt}, and large-scale multimodal pretraining~\cite{Lu2022UnifiedIO, Lu2023UnifiedIO2,4m,4m21,team2024chameleon,wang2024emu3}.

Various improvements to this framework have been proposed, exploring different architectures~\cite{yu2021improvedvqgan}, objective functions~\cite{esser2021taming, Hu2023GAIA1AG}, codebook structures~\cite{zhang2023regularized,yu2023magvitv2}, and the use of diffusion models as decoders~\cite{Shi2022DiVAE, 4m, Xu2024DisCoDiff, Zhao2024epsilonVAE}. A key advancement is finite scalar quantization (FSQ)~\cite{mentzer2023fsq}, which replaces the learned codebook with a projection to a small-dimensional latent space and quantizes values using fixed bins along each dimension. This approach maintains reconstruction quality while being simpler to implement and train.

Instead of maintaining a 2D spatial structure in the latent space, TiTok~\cite{yu2024titok} creates 1D representations using learned register tokens $\bm{R} \in \mathbb{R}^{K \times D}$~\cite{Darcet2023Registers}. During encoding, these K register tokens are concatenated with image patch embeddings $\bm{P} \in \mathbb{R}^{h \times w \times D}$ for processing in a ViT encoder. The encoder output retains only the register token embeddings as $\bm{Z}_{1D}$, which capture the image content in a compact sequence. For reconstruction, the image patches are replaced with a grid of $h \times w$ learnable mask tokens $\bm{M} \in \mathbb{R}^{h \times w \times D}$. These mask tokens, guided by the quantized register embeddings, are transformed by the decoder into the reconstructed image: $\bm{\hat{X}} = Dec(Quant(\bm{Z}_{1D}) \oplus \bm{M})$.

Although effective at producing 1D token sequences, TiTok requires a two-stage training process and represents images using a fixed number of tokens. Compared to TiTok, \ours shows a superior reconstruction and generation quality (see \cref{fig:reconst_visual_comp,tab:baseline_comparison}). To address the issue of the token sequence length depending entirely on the image height and width instead of its complexity, a range of concurrent works have proposed adaptive tokenization methods. While \ours can compress images into as little as a single token, ElasticTok's~\cite{Yan2024ElasticTok} FSQ tokenizer is limited to a minimum of 256 tokens and ALIT~\cite{Duggal2024ALIT} to 32 tokens. Generation is a key use case of tokenizers, and unlike ElasticTok, ALIT, or One-D-Piece~\cite{miwa2025onedpiece}, we show in \cref{sec:experiments} that training on \ours tokens can produce strong generative models. Compared to our focus on AR models, ViLex~\cite{Wang2024ViLex} and CAT~\cite{Shen2025CATCI} focus more on learning continous tokens to use in diffusion models. We discuss further differences with respect to \ours in detail in \cref{sec:app_relatedwork}.
