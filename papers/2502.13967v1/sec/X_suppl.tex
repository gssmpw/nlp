\clearpage
\appendix
\onecolumn

\section*{\LARGE Appendix}
\section*{Table of Contents}
\startcontents[appendices]
\printcontents[appendices]{l}{1}{\setcounter{tocdepth}{2}}
\newpage


\section{Ablation of \ours Design Choices}
\label{sec:app_ablations_flextok}
We explore the design space offered by the \ours framework. The effects of the VAE choice, structure applied to the tokens to induce an ordering, the decoder loss formulation, and the use of inductive bias losses are all investigated in the goal of converging on a high-quality and compact 1D tokenizer that can resample images into variable-length token sequences. 

\paragraph{Default ablation setup.}
Unless otherwise stated:
\begin{itemize}
    \item All models are trained on images of resolution 256x256. 
    \item We use a 16-channel VAE and produce the VAE latent space by sampling from the Gaussian distributions, rather than just taking the mode.
    \item We use a FSQ quantization with 6 dimensions, each bucketed into levels \texttt{[8, 8, 8, 5, 5, 5]}, for an effective vocabulary size of \num{64000}. 
    \item We use a 2x2 patchification inside the \ours encoder and decoder that acts on the VAE latent space which itself has a downsample factor of 8. This yields an effective spatial downsample factor of 16 from pixels to patch tokens.
    \item All \ours ablation models have encoder and decoder sizes \texttt{d12-d12}, and are trained for 50B patch tokens on ImageNet-1k~\cite{Russakovsky2014ImageNet}. We measure one patch token as a 16x16 pixel patch.
    \item To evaluate the ablation models for class conditional image generation, we train 393M parameter AR Transformers on the resulting token sequences. The AR models are trained for 94B tokens on ImageNet-1k (300 epochs).
\end{itemize}

\paragraph{Evaluation Setup}
We evaluate our tokenizers using an array of metrics which probe their reconstruction and generation properties. To evaluate image reconstruction we calculate reconstruction FID (rFID)~\cite{Heusel2017FID}, DreamSim~\cite{Fu2023DreamSim}, and Mean Absolute Error (MAE) on the full validation split of the ImageNet-1k dataset~\cite{Russakovsky2014ImageNet}. During generation, we use a classifier-free guidance scale of 1.5 for the AR model. For evaluation of the class-conditioned image generation results, we follow the common practice of measuring the generation FID (gFID) of 50K generated samples relative to the reference statistics calculated over the entire training split of the ImageNet-1k dataset~\cite{dhariwal2021diffusion}. 


\subsection{VAE choice ablation}
\label{sec:app_vae_choice_ablation}
We observe a strong correlation between the VAE reconstruction quality and the number of latent channels (Tab.~\ref{tab:vae_num_ch_comparison}), and find that increasing the number of latent channels above 4 can significantly improve the \ours reconstruction quality (Fig.~\ref{fig:app_flextok_norm_guidance_in1k_vae_ablations}). This finding is in line with the observations that the VAE latent channel size should be scaled with size of the subsequent latent diffusion model~\cite{Esser2024SD3}. To optimize for the largest-sized tokenizer's performance, we select the 16-channel VAE for our final setup.

\begin{table}[ht!]
    \caption{\textbf{VAE reconstruction performance.} We measure MSE and rFID on the COCO-30k validation set~\cite{ms_coco_dataset}.}
    \label{tab:vae_num_ch_comparison}
    \centering
    \begin{tabular}{@{}lllll@{}}
        \toprule
        Model & Arch. & \# Latent Channels & MSE & rFID \\ 
        \midrule
        SDXL VAE & SDXL VAE & 4 & 0.0038 & 1.13\\
        \midrule
        ours  & SDXL VAE & 4 & 0.0043 & 1.53\\
        ours  & SDXL VAE & 8 & 0.0028 & 0.66\\
        ours  & SDXL VAE & 16 & 0.0013 & 0.35\\
        \bottomrule
    \end{tabular}
\end{table}

\begin{figure}[ht!]
\centering
\includegraphics[width=\linewidth]{resources/appendix/d12_d12_in1k_vae_series.pdf}
\caption{
\textbf{Guidance scale ablation for different VAE choices.} We train \oursbase models on different VAE choices. We use adaptive projected guidance~\cite{Sadat2024NormGuidance}, and evaluate on the ImageNet-1k~\cite{Russakovsky2014ImageNet} validation set.
}
\label{fig:app_flextok_norm_guidance_in1k_vae_ablations}
\end{figure}


\FloatBarrier
\subsection{Resampling strategy ablation}
\label{sec:app_resampling_image_tokens}

\begin{table}[ht!]
\caption{
    \textbf{Ablation of resampling strategies}. 
    We compare 2D grid tokenization with 1D tokenization, the use of a rectified flow decoder, the training noise schedule, as well as the use of an auxiliary REPA~\cite{Yu2024REPA} loss. We compare on reconstruction MAE, DreamSim, and rFID, as well as ImageNet-1k class-conditional generation gFID.
}
\label{tab:resampling_strategies}
\centering
\resizebox{0.7\columnwidth}{!}{%
\begin{tabular}{@{}llll|ccc|c@{}}
\toprule
Token Structure & Loss Formulation & Noise & REPA & MAE & DreamSim & rFID & gFID  \\
\midrule
16 $\times$ 16 (2D) & MSE & - & \xmark & 0.059 & 0.288 & 51.27 & 35.93 \\
16 $\times$ 16 (2D) & Rectified flow & mode(0.25) & \xmark & 0.083 & 0.272 & 32.85 & 23.29  \\
256 (1D) & Rectified flow & mode(0.25) & \xmark & 0.078 & 0.220 & 23.28 & 22.16  \\
256 (1D) & Rectified flow & uniform & \xmark & 0.078 & 0.222 & 23.63 &    21.35   \\
256 (1D) & Rectified flow & logit-normal & \xmark & 0.082 & 0.208 & 19.03 &  18.69     \\
256 (1D) & Rectified flow & mode(0.25) & \cmark & \textbf{0.075} & \textbf{0.128} & \textbf{5.98}  &  \textbf{7.40}   \\
\bottomrule
\end{tabular}
}
\end{table}

We first compare the resampling strategy (2D grid tokenization versus 1D register tokenization), as well as the use of a rectified flow decoder compared to a simple decoder optimized with MSE. Note that in none of these experiments do we apply any ordering strategies. We reserve those investigations for \cref{sec:app_1d_structure_ablation}. 

The results in rows 1 and 2 in \cref{tab:resampling_strategies} show that the use of a rectified flow decoder significantly improves both rFID and gFID, while getting worse MAE. When switching from a 2D to a 1D tokenizer (row 2 vs. 3), we see improvements across all metrics, with the largest improvements being on rFID. We also ablate three different choices of noise schedules following \citet{Esser2024SD3}, namely a uniform noise schedule (row 4), a logit-normal schedule with location $m=0$ and scale $s=1$ (row 5), and a mode sampling schedule with scale $s=0.25$ (row 3). While the logit-normal schedule shows the strongest performance, we note that we observe inference instabilities due to the earliest and latest timesteps being undertrained. We chose the mode sampling schedule for our models due to the comparatively better reconstruction performance.


\subsection{Use of inductive bias loss.}
We observe that rectified flow decoders converge slowly and investigate the use of inductive bias losses~\cite{Hu2023GAIA1AG, Yu2024REPA} to 1) improve convergence time, and 2) distill the semantic inductive biases of a strong pre-trained vision model into the tokenizer to make the tokens more predictable. Following REPA~\cite{Yu2024REPA}, we train a three-layer MLP to read out the activations of the decoded 2D patches in the first decoder layer, and project them to the DINOv2-L feature dimension. In addition to the rectified flow loss, we add a cosine similarity loss with weight 1.0 between the predicted features and the reference DINOv2-L features. 

The last row of \cref{tab:resampling_strategies} compared to row 3 shows that the use of REPA significantly improves perceptual reconstruction metrics like DreamSim and rFID, as well as generation performance as measured by gFID. As shown in \cref{fig:app_flextok_repa_eval_curves}, the use of an inductive bias loss significantly improves convergence time to high quality reconstructions. Interestingly, we find that even though the use of REPA improves the reconstruction and generation metrics significantly, it does not improve the convergence of the reconstruction loss during training of the tokenizer (\cref{fig:app_flextok_repa_train_curves}). Given the striking improvements in downstream reconstruction and generation metrics, we use REPA for all experiments from here on.

\begin{figure}[h]
\centering
\includegraphics[width=\linewidth]{resources/appendix/loss_curves_in1k_repa_comparison.pdf}
\caption{
\textbf{REPA~\cite{Yu2024REPA} ablation loss curves.}
We train \oursbase models on ImageNet-1k, with and without REPA. The reconstruction loss shown here does not include the REPA loss contribution. 
}
\label{fig:app_flextok_repa_train_curves}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=\linewidth]{resources/appendix/repa_ablation_eval_curves_in1k.pdf}
\caption{
\textbf{REPA~\cite{Yu2024REPA} ablation evaluation curves.}
We train \oursbase models on ImageNet-1k, with and without REPA. The evaluation metrics are measured on the ImageNet-1k~\cite{Russakovsky2014ImageNet} validation set.
}
\label{fig:app_flextok_repa_eval_curves}
\end{figure}



\subsection{Structuring the 1D tokens and inducing an ordering}
\label{sec:app_1d_structure_ablation}
We ablate various ways to induce an ordering into the register tokens, starting with the 1D rectified flow model trained with REPA, as shown in the last row of \cref{tab:resampling_strategies} and first row of \cref{tab:register_structure}.

\begin{table}[ht!]
\caption{
\textbf{Comparison of strategies to induce an ordering}. We ablate the use of causal masks on the register tokens, as well as multiple nested dropout variants.
}
\label{tab:register_structure}
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{@{}l|ccc |ccc |ccc|ccc@{}}
\toprule
                        & \multicolumn{3}{c}{MAE}                          & \multicolumn{3}{c}{DreamSim}                       & \multicolumn{3}{c}{rFID}                        & \multicolumn{3}{c}{gFID}  \\
Number of tokens        &   4            &   32           &   256          &   4             &   32            &   256          &   4            &   32           &   256         &   4    &   32   &  256  \\
\midrule
Baseline                & \xmark         & \xmark         & \textbf{0.075} & \xmark          & \xmark          & 0.128          & \xmark         & \xmark         & 5.98          & \xmark & \xmark & 7.40  \\
Causal register mask                & \xmark         & \xmark         & 0.080          & \xmark          & \xmark          & \textbf{0.118} & \xmark         & \xmark         & \textbf{3.28} & \xmark & \xmark & 5.04  \\
\midrule
+ Uniform nested dropout        & 0.268          & \textbf{0.148} & 0.080          &  0.522          &  \textbf{0.275} & 0.132          & 19.97          &  \textbf{6.48} & 4.01          &   17.99    &     \textbf{5.26}    & \textbf{4.83}  \\
+ ``Pow2'' dropout                  & \textbf{0.209} & 0.159          & 0.088          &  \textbf{0.440} &  0.323          & 0.175          & \textbf{10.39} &  8.09          & 7.22          &   \textbf{9.51}     &    6.63    & 5.78  \\
+ ``Unifpow2'' dropout          & 0.225          & 0.149          & 0.079          &  0.578          &  0.278          & 0.125          & 30.17          &  6.78          & 3.52          & 28.93  &  5.48  & 4.97  \\
\bottomrule
\end{tabular}
}
\end{table}

First, we ablate the use of a causal attention mask (see \cref{sec:ordered_flex_sequences}) applied to the encoder register tokens, see row 2 in \cref{tab:register_structure}. Compared to unstructured registers, this results in a significant improvement in both rFID and gFID. While the causal mask implicitly induces an ordering over the register tokens, it does not enable use of this model as a flexible-length tokenizer. To that end, we ablate three different nested dropout~\cite{Rippel2014NestedDropout} schedules (see \cref{sec:ordered_flex_sequences}). In ``uniform nested dropout'' we randomly draw $K_\text{keep} \in \{1,2,3,4,...,K\}$. For the ``pow2'' setting, we uniformly sample from powers of two, i.e. $K_\text{keep} \in \{1,2,4,8,...,K\}$. Finally, for ``unifpow2'', we sample as in ``uniform nested dropout'' but round to the next-highest power-of-two. Each of these choices puts more or less weight on different subsets of the token sequences. For example, when performing uniform nested dropout each valid number of tokens is only trained on roughly $\frac{1}{256}$ of all samples, while ``pow2'' and ``unifpow2'' reduce the number of possible sequence lengths from 256 down to 9. ``unifpow2'' favors larger token sequences, which is preferable when dealing with images that require more tokens. That said, its rFID and gFID for lower number of tokens is lower due to being relatively undertrained, and ``pow2'' can present a more balanced approach across the range of sequence lengths (see \cref{fig:app_flextok_norm_guidance_in1k_pow2_vs_unifpow2}). This improvement of the reconstruction quality for short sequences does come at the cost of a slight regression on the reconstruction and generation performance with the fully 256 token sequences.
In \cref{fig:app_flextok_norm_guidance_in1k_albations} we show guidance scale sweeps for all ablation models.


\begin{figure}[ht!]
\centering
\includegraphics[width=\linewidth]{resources/appendix/flextok_in1k_50B_pow2_vs_unifpow2_num_tokens_sweep.pdf}
\caption{
\textbf{Pow2 vs Unifpow2 nested dropout schedule as a function of the number of tokens.} We train \oursbase models on ImageNet-1k with different nested dropout schedules and evaluate on the Imagenet-1k validation set. Pow2 presents a more balanced approach across sequence lengths, while Unifpow2 is preferable for higher sequence lengths.
}
\label{fig:app_flextok_norm_guidance_in1k_pow2_vs_unifpow2}
\end{figure}

\begin{figure}[ht!]
\centering
\includegraphics[width=\linewidth]{resources/appendix/d12_d12_in1k_ablations.pdf}
\caption{
\textbf{Classifier-free guidance ablations.} For all ablation models, we perform guidance scale sweeps using adaptive projected guidance~\cite{Sadat2024NormGuidance}. Evaluation metrics are measured on the ImageNet-1k validation set. 
}
\label{fig:app_flextok_norm_guidance_in1k_albations}
\end{figure}


\clearpage

\section{Evaluating the Representation Quality of \ours Tokenizers}
\label{sec:app_probing}

In this subsection, we evaluate the quality of the representations learned by the \ours tokenizer. Specifically, we perform linear evaluation on the quantized register tokens produced by the \ours encoder (i.e., the tokens passed as input to the flow model). Importantly, we do not use patch representations, focusing only on the quantized register tokens. For linear evaluation, we train a linear classifier on ImageNet-1k's training set and evaluate it on the test set, keeping the tokenizer frozen throughout. Since a single representation is needed for the linear classifier, we follow the approach of \citet{yu2024titok}, where the quantized register tokens are concatenated to form a single feature vector per image. For the rest, we follow the probing setup in~\citet{fini2024multimodal}. Given the 6-dimensional FSQ latents, this results in feature vectors of size $6 \times \text{num\_register\_tokens}$.

To tune the linear evaluation recipe, we conduct a hyperparameter search over the following grid: Learning rate $\in [1 \times 10^{-3}, 5 \times 10^{-4}, 2 \times 10^{-4}]$, weight decay $\in [0.1, 0.05, 0.02]$, and minimum crop scale $\in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]$. Using the optimal hyperparameters found (learning rate $5 \times 10^{-4}$, weight decay $0.05$, crop scale $[0.4, 1.0]$), we perform a sweep over the number of register tokens to collect the results shown in \cref{fig:app_probing}. These experiments are conducted for all models (\oursbase, \ourslarge, \oursxlarge) with a batch size of 1024, using 8 H100 or A100 GPUs for each experiment.

\begin{figure}[ht!]
\centering
\includegraphics[width=0.5\textwidth]{resources/appendix/imagenet_linear_probe_acc.pdf}
\caption{\textbf{Linear probing experiments.} ImageNet-1k top-1 accuracy on the frozen tokenizer trunk when varying number of tokens.}
\label{fig:app_probing}
\end{figure}

\textbf{Observations.} 
These experiments are interesting because there is no  guarantee that the representations learned by the tokenizer will be linearly separable. However, as our results demonstrate, the representations are indeed linearly separable. The results, presented in Figure 7, reveal several key insights:
\begin{itemize}
    \item \textit{Improved performance with additional tokens:} Increasing the number of register tokens consistently improves performance. This behavior contrasts with TiTok, where the opposite occurs. The main difference lies in the training approach: TiTok trains a separate model for each configuration (i.e., number of tokens), while we learn a unified model capable of handling all configurations. In our model, when adding more tokens we retain the previous ones unchanged, e.g for $K_\text{keep} = 16$ the first 8 tokens will be the same as in the case $K_\text{keep} = 8$. Consequently, the linear layer has strictly more information to improve linear separability. In the worst case, the linear layer can simply ignore the additional tokens if they do not contribute useful information. For this reason we expect that performance will monotonically increase with more tokens, which is verified by the experimental evidence. In contrast, TiTok trains a different model for each token configuration, resulting in distinct feature spaces for each model.
    \item \textit{Superior peak performance:} The best performance achieved by our model significantly surpasses TiTok, with our top-performing configuration reaching 64.6\% top-1 accuracy on ImageNet.
    \item \textit{Trends across encoder and decoder sizes:} A larger encoder improves linear separability when all register tokens are activated; conversely, a larger decoder size leads to a slight degradation in linear separability.
\end{itemize}


\clearpage
\section{Implementation and Training details}
\label{sec:app_training_details}

In this Section, we list implementation and training details for the VAEs in \cref{sec:app_vae_training_details}, the resamplers in \cref{sec:app_resampler_training_details}, and the autoregressive models in \cref{sec:app_ar_training_details}.

\subsection{VAE details}
\label{sec:app_vae_training_details}
We train a series of VAE models following the Stable Diffusion methodology~\cite{rombach2022high}. The variational autoencoder architecture is the same as the SDXL VAE~\cite{Podell2023SDXL} except we investigate varying the size of the latent dimension from 4 up to 8 and 16 channels. For the discriminator loss we use a 3-layer Patch-GAN discriminator~\cite{isola2017image} that is applied after 5k steps of training and combine it with perceptual~\cite{dosovitskiy2016generating}, KL and reconstruction losses. We train all the VAEs using a learning rate of 1e-3, weight decay of 0.05, model EMA decay of 0.992, spatial down-sampling factor of 8, KL loss weight of 1e-6, discriminator loss weight of 0.5, reconstruction and perceptual loss weights of 1.0, a batch size of 128 and for 305k steps (40B patches) on images from the DFN dataset~\cite{dfn_dataset}. During inference, we produce the VAE latent space by sampling from the Gaussian distribution, rather than just taking the mode.

The trained VAEs are evaluated on the COCO validation split using both mean squared error and reconstruction Fréchet inception distance (rFID)~\cite{Heusel2017FID} metrics (see \cref{tab:vae_num_ch_comparison}). We find that increasing the latent channel dimension significantly improves the performance of the reconstructions produced by the VAEs. Critical for the upper bound of the performance for the subsequent tokenizer and autoregressive models, the VAE trained with 16 channels achieves a competitive rFID of 0.354. Furthermore, when training \oursbase tokenizers on these four VAE variants, we show in \cref{fig:app_flextok_norm_guidance_in1k_vae_ablations} that choosing Stage 0 VAEs with a low number of channels can have detrimental consequences on the Stage 1 \ours reconstruction performance. This finding is consistent with \citet{Esser2024SD3}, and we choose to train \ours models using our 16-channel VAE to avoid prematurely putting an upper-bound on its performance.


\subsection{\ours details}
\label{sec:app_resampler_training_details}

\begin{table}[h!]
    \caption{\textbf{\ours training settings.} Model and training configuration for three different model sizes of our resampler tokenizers. See \cref{sec:app_resampler_training_details} for further training details.}
    \label{tab:app_resampler_training_settings}
    \centering
    \begin{adjustbox}{max width=\linewidth}
    \begin{tabular}{@{}l|ccc@{}}
    \toprule
    Configuration & \oursbase & \ourslarge & \oursxlarge \\ 
    
    \midrule
    Encoder depth $d_{enc}$ & 12 & 18 & 18 \\
    Decoder depth $d_{dec}$ & 12 & 18 & 28 \\
    Encoder dim. $w_{enc}$ & 768 & 1152 & 1152 \\
    Decoder dim. $w_{dec}$ & 768 & 1152 & 1792 \\
    Encoder Transformer parameters & 84.9M & 286.7M & 286.7M \\
    Decoder Transformer parameters & 84.9M & 286.7M & 1.1B \\
    Decoder adaLN~\cite{peebles2023scalable} parameters & 84.9M & 286.7M & 1.1B \\
    Max. num. registers $K$ & \multicolumn{3}{c}{256} \\ 
    Register attention mask & \multicolumn{3}{c}{Causal (see \cref{sec:ordered_flex_sequences})} \\ 
    Register nested dropout mode & \multicolumn{3}{c}{Powers of two: \textit{1, 2, 4, 8, 16, 32, 64, 128, 256} (see \cref{sec:ordered_flex_sequences})} \\ 
    FSQ~\cite{mentzer2023fsq} levels & \multicolumn{3}{c}{\texttt{[8,8,8,5,5,5]}} \\ 
    VAE channels & \multicolumn{3}{c}{16} \\ 
    VAE downsampling factor & \multicolumn{3}{c}{8} \\ 
    Patch size & \multicolumn{3}{c}{$2 \times 2$} \\ 
    Feedforward activation & \multicolumn{3}{c}{SwiGLU~\cite{Shazeer2020GLU}} \\
    
    \midrule
    Rectified flow decoder & \multicolumn{3}{c}{\cmark} \\
    \makecell[l]{Decoder adaLN-Zero\\time emb.~\cite{peebles2023scalable}} & \multicolumn{3}{c}{\cmark} \\ 
    \makecell[l]{Noise mode sampling\\param. $s$ (\cite{Esser2024SD3}, Sec. 3.1)} & \multicolumn{3}{c}{0.25} \\
    Condition dropout prob. & \multicolumn{3}{c}{0.2} \\

    \midrule
    REPA~\cite{Yu2024REPA} layer & \multicolumn{3}{c}{1} \\
    REPA~\cite{Yu2024REPA} model & \multicolumn{3}{c}{DINOv2-L~\cite{Oquab2023DINOv2}} \\
    REPA~\cite{Yu2024REPA} projection & \multicolumn{3}{c}{3-layer MLP with decoder dim.} \\
    REPA~\cite{Yu2024REPA} loss weight & \multicolumn{3}{c}{1.0} \\

    \midrule
    Training length ($n$ tokens) & \multicolumn{3}{c}{200B} \\ 
    Warmup length ($n$ tokens) & \multicolumn{3}{c}{4B} \\
    Warmup learning rate & \multicolumn{3}{c}{1e-6} \\
    Learning rate schedule & \multicolumn{3}{c}{Cosine decay} \\
    Model EMA decay & \multicolumn{3}{c}{0.998} \\
    Optimizer & \multicolumn{3}{c}{AdamW \cite{Loshchilov2017AdamW}} \\
    Opt. momentum & \multicolumn{3}{c}{$\beta_1,\beta_2=0.9,0.99$} \\
    Learning rate $\eta$ & \multicolumn{3}{c}{5.62e-4} \\
    Batch size & \multicolumn{3}{c}{2048} \\
    $\mu$P~\cite{Yang2022muP} base dim. & \multicolumn{3}{c}{256} \\
    Weight decay timescale $\tau_{iter}$~\cite{Wang2024AdamWWDEMA} & \multicolumn{3}{c}{$n_{iter} = \num{381470}$} \\
    Gradient clipping norm & \multicolumn{3}{c}{1.0} \\
    
    \midrule
    Dataset & \multicolumn{3}{c}{ImageNet-1k~\cite{Russakovsky2014ImageNet} or DFN-2B~\cite{dfn_dataset}} \\
    Image resolution & \multicolumn{3}{c}{$256^2$} \\
    Augmentations & \multicolumn{3}{c}{\makecell{\texttt{RandomResizedCrop}, \\ \texttt{RandomHorizontalFlip}, \\ \texttt{Normalize}}} \\
    Data type & \multicolumn{3}{c}{\texttt{bfloat16}~\cite{Burgess2019Bfloat16}} \\

    \bottomrule
    \end{tabular}
    \end{adjustbox}
\end{table}

See \cref{tab:app_resampler_training_settings} for a detailed breakdown of the resampler tokenizer architecture and training settings. All encoders and decoders are Transformers, whose hidden dimension $w$ is parameterized by the number of layers $d$ using a fixed aspect ratio of 64, i.e. $w = 64 \cdot d$. The number of attention heads is set to $d$. Both the encoder and decoder operate on $2 \times 2$ patches of VAE latents to reduce the sequence lengths they need to process. The registers are a randomly initialized and learnable parameter of shape $K \times d$, concatenated with the VAE patches. We use FlexAttention~\cite{flexattention2024} to create an encoder attention mask in which all patch tokens can attend to each other but not the registers, the registers can attend to all patch tokens, but the $i$-th register token can only attend to the $j$-th register token if $i \geq j$.

We use FSQ as the quantization bottleneck with levels \texttt{[8,8,8,5,5,5]}, for an effective codebook size of \num{64000}. When performing nested dropout, we replace the dropped tokens with a learnable mask token. 

The \ours decoders are rectified flow models that are conditioned on the encoder's register tokens by concatenating them with the noised VAE latent patches. Unlike the encoder, the decoder computes full self-attention between the registers and patch tokens, and only the patch tokens are output. The VAE latents are sampled according to a noise schedule that follows Stable Diffusion 3's mode sampling with heavy tails scheme, using scale parameter $s=0.25$. We use adaLN-Zero~\cite{peebles2023scalable} to condition the registers and image patches \textit{using separate sets of adaLN weights} applied to the same continuous timestep embeddings. As shown in \cref{tab:app_resampler_training_settings}, the adaLN parameters make up half the total decoder parameters, but we note that their contribution to the overall decoder FLOPS is negligible as they are computed on the time embedding tokens (once for the registers and once for the noised patches). We leave the reduction of this parameter cost to future research. To enable the use of classifier-free guidance~\cite{Ho2022ClassifierFreeGuidance}, we randomly replace the entirety of the encoded registers by a learned null-condition with probability 0.2. The decoder's target is the flow, see \cref{sec:rf_decoder}.

In addition to the rectified flow loss $\mathcal{L}_\text{RF}$, we use REPA~\cite{Yu2024REPA} to speed up convergence of \ours. Specifically, we project the first decoder layer activations using a 3-layer MLP with the same dimension as the decoder and ratio 4.0, upsample them to size $37 \times 37$, and compute the cosine similarity with 1024-dimensional DINOv2-L~\cite{Oquab2023DINOv2} features. The REPA loss $\mathcal{L}_\text{REPA}$ is weighted equally to the rectified flow loss, i.e. $\mathcal{L} = \mathcal{L}_\text{RF} + 1.0 \cdot \mathcal{L}_\text{REPA}$.

The resampler models are either trained on ImageNet-1k~\cite{Russakovsky2014ImageNet} for downstream use in class-conditional image generation, or on DFN-2B~\cite{dfn_dataset} for training autoregressive text-to-image models. The image-caption dataset contains a mixture of original and synthetic captions. During training, we randomly select crops of size $256^2$ using random scales in [0.8, 1.0] and aspect ratio in [0.75, 1.3333]. The resulting images are randomly flipped horizontally with a probability of 0.5, and normalized to the range [-1,1].

All models are trained for a total of 200B tokens seen, where one token is counted as a $2 \times 2$ VAE patch, i.e. a $16 \times 16$ grid of pixels. For images of size $256^2$, this amounts to 256 tokens per sample. We linearly warm up the learning rate for 4B tokens, and decay it using cosine decay. The model is trained using the AdamW~\cite{Loshchilov2017AdamW} optimizer, and we swept the learning rate and batch size at a small scale using $\mu$P~\cite{Yang2022muP}, which we transfer directly to all other settings. We note here that we did not sweep these hyperparameters for every resampler setting, but only on the base setting of a rectified flow resampler without REPA, causal register masks, nor nested dropout. We automatically set the weight decay following the interpretation that AdamW with weight decay can be understood as an exponential moving average (EMA) of recent updates~\cite{Wang2024AdamWWDEMA}. Concretely, we compute the weight decay such that it corresponds to averaging over all training iterations, setting $\tau_{iter}$ to the total number of iterations $n_{iter}$. The weight decay $\lambda$ is computed as $\lambda = \frac{1}{n_{iter} \cdot \eta}$, using learning rate $\eta$. For the final model evaluations, we additionally use an EMA of the weights with decay rate 0.998. See the exact settings in \cref{tab:app_resampler_training_settings} and the ImageNet-1k training curves in \cref{fig:resampler_loss_curves_in1k}.



\begin{figure*}[h]
\centering
\includegraphics[width=\linewidth]{resources/appendix/loss_curves_in1k.pdf}
\caption{
\textbf{\ours training loss curves.} The \ours of different sizes shown here are trained for 200B tokens on ImageNet-1k~\cite{Russakovsky2014ImageNet}. We plot the total loss, i.e. $\mathcal{L}_\text{RF} + \mathcal{L}_\text{REPA}$.
}
\label{fig:resampler_loss_curves_in1k}
\end{figure*}

\begin{figure*}[h]
\centering
\includegraphics[width=\linewidth]{resources/appendix/eval_curves_in1k.pdf}
\caption{
\textbf{\ours evaluation curves.} We evaluate the resamplers during training at periodic intervals on the entire ImageNet-1k validation set, showing rFID, MAE, Inception score, and LPIPS.
}
\label{fig:resampler_eval_curves_in1k}
\end{figure*}


\subsection{Autoregressive model details}
\label{sec:app_ar_training_details}

See \cref{tab:app_c2i_ar_training_settings} and \cref{tab:app_t2i_ar_training_settings} for a detailed breakdown of the class conditional and text-conditional autoregressive transformer architectures and training settings. The AR component of the models are casual decoder Transformers, similar to the \ours tokenizer modules, the hidden dimension $w$ is parameterized by the number of layers $d$ using a fixed aspect ratio of 64, i.e. $w = 64 \cdot d$. The number of attention heads is set to $d$. We use a MLP ratio of 4 for the FFN hidden dim relative to the attention hidden. 

In the class-conditional models we do not apply $\mu$P~\cite{Yang2022muP} and instead scale the learning rate inversely with the model width. To mitigate overfitting to the ImageNet-1k dataset we apply dropout with 0.1 probability to the FFN, attention, and projection modules in the Transformer decoder blocks and we apply random hornizontal flipping of the images. Additionally we produce 10 random crops per image prior to tokenization\cite{sun2024autoregressive}. Whereas, for the text-conditional models we do apply $\mu$P, use a learning rate of 1e-2 for all model sizes, don't use dropout in the Transfomrer decoder blocks, take square center crops and apply no data augmentations to the training images. When scaling up the AR transformer for the text conditioned models we warmup the learning rate for the same number of tokens as the model's parameter count~\cite{porian2024resolving}.


\begin{table}[h!]
    \caption{\textbf{Class conditioned AR training settings.} Model and training configuration for different model sizes of our AR transformers.}
    \label{tab:app_c2i_ar_training_settings}
    \centering
    \begin{adjustbox}{max width=\linewidth}
    \begin{tabular}{@{}l|cccccc@{}}
    \toprule
    Configuration & AR 49M & AR 85M & AR 201M & AR 393M & AR 679M & AR 1.33B \\ 
    
    \midrule
    Num. non-embedding Parameters & 49M & 85M & 201M & 393M & 679M & 1.33B \\
    Decoder depth $d_{dec}$ & 10 & 12 & 16 & 20 & 24 & 30 \\
    Decoder dim. $w_{dec}$ & 640 & 768 & 1024 & 1280 & 1536 & 1920 \\
    Cross Attn. dim. & n/a & n/a & n/a & n/a & n/a & n/a \\
    MLP Ratio & \multicolumn{6}{c}{4} \\
    Max. Sequence Length & \multicolumn{6}{c}{256} \\ 
    Attention mask & \multicolumn{6}{c}{Causal} \\ 
    Vocab Size & \multicolumn{6}{c}{64,000} \\ 
    Feedforward activation & \multicolumn{6}{c}{SwiGLU~\cite{Shazeer2020GLU}} \\
    Positional Encoding & \multicolumn{6}{c}{Learned Embedding} \\
    Conditioning dropout prob. & \multicolumn{6}{c}{0.1} \\
    FFN, Attn. and Projection dropout prob. & \multicolumn{6}{c}{0.1} \\

    \midrule
    Training length ($n$ tokens) & \multicolumn{6}{c}{94B (300 Epochs)} \\ 
    Warmup length ($n$ tokens) & \multicolumn{6}{c}{9.4B} \\
    Initial warmup learning rate & \multicolumn{6}{c}{1e-6} \\
    Learning rate schedule & \multicolumn{6}{c}{Cosine decay} \\
    Optimizer & \multicolumn{6}{c}{AdamW \cite{Loshchilov2017AdamW}} \\
    Opt. momentum & \multicolumn{6}{c}{$\beta_1,\beta_2=0.9,0.95$} \\
    Learning rate $\eta$ & 1.2E-3 & 1E-3 & 7.5E-4 & 6E-4 &5E-4 & 4E-4 \\
    Final learning rate & \multicolumn{6}{c}{$\eta \times$1E-2} \\
    Batch size & \multicolumn{6}{c}{1024} \\
    $\mu$P~\cite{Yang2022muP} base dim. & \multicolumn{6}{c}{n/a} \\
    Weight decay & \multicolumn{6}{c}{0.05} \\
    Weight decay timescale $\tau_{iter}$~\cite{Wang2024AdamWWDEMA} & \multicolumn{6}{c}{n/a} \\
    Gradient clipping norm & \multicolumn{6}{c}{1.0} \\
    
    \midrule
    Dataset & \multicolumn{6}{c}{ImageNet-1k~\cite{Russakovsky2014ImageNet}} \\
    Image resolution & \multicolumn{6}{c}{$256^2$} \\
    Augmentations & \multicolumn{6}{c}{\makecell{\texttt{10 options RandomResizedCrop}, \\ \texttt{RandomHorizontalFlip}, \\ \texttt{Normalize}}} \\
    Data type & \multicolumn{6}{c}{\texttt{bfloat16}~\cite{Burgess2019Bfloat16}} \\

    \bottomrule
    \end{tabular}
    \end{adjustbox}
\end{table}



\begin{table}[h!]
    \caption{\textbf{Text conditioned AR training settings.} Model and training configuration for different model sizes of our AR transformers.}
    \label{tab:app_t2i_ar_training_settings}
    \centering
    \begin{adjustbox}{max width=\linewidth}
    \begin{tabular}{@{}l|cccccc@{}}
    \toprule
    Configuration & AR 113M & AR 382M & AR 1.15B & AR 3.06B \\ 
    
    \midrule
    Num. non-embedding Parameters & 113M & 382M & 1.15B & 3.06B \\
    Decoder depth $d_{dec}$ & 12 & 18 & 26 & 36 \\
    Decoder dim. $w_{dec}$ & 768 & 1152 & 1664 & 2304 \\
    Cross Attn. dim. & 12 & 18 & 26 & 36 \\
    MLP Ratio & \multicolumn{4}{c}{4} \\
    Max. Sequence Length & \multicolumn{4}{c}{256} \\ 
    Attention mask & \multicolumn{4}{c}{Causal} \\ 
    Vocab Size & \multicolumn{4}{c}{64,000} \\ 
    Feedforward activation & \multicolumn{4}{c}{SwiGLU~\cite{Shazeer2020GLU}} \\
    Positional Encoding & \multicolumn{4}{c}{Learned Embedding} \\
    Conditioning dropout prob. & \multicolumn{4}{c}{0.1} \\
    FFN, Attn. and Projection dropout prob. & \multicolumn{4}{c}{0.0} \\

    \midrule
    Training length ($n$ tokens) & \multicolumn{4}{c}{284B} \\
    Training FLOPs & 1.93E+20 & 6.39E+20 & 1.90E+21 & 5.00E+21 \\
    Warmup length ($n$ tokens) &  &  &  &  \\
    Initial warmup learning rate & \multicolumn{4}{c}{1e-6} \\
    Learning rate schedule & \multicolumn{4}{c}{Cosine decay} \\
    Optimizer & \multicolumn{4}{c}{AdamW \cite{Loshchilov2017AdamW}} \\
    Opt. momentum & \multicolumn{4}{c}{$\beta_1,\beta_2=0.9,0.95$} \\
    Learning rate $\eta$ & \multicolumn{4}{c}{1e-2}\\
    Final learning rate & \multicolumn{4}{c}{$\eta \times$1E-2} \\
    Batch size & \multicolumn{4}{c}{8192} \\
    $\mu$P~\cite{Yang2022muP} base dim. & \multicolumn{4}{c}{256} \\
    Weight decay & \multicolumn{4}{c}{0.05} \\
    Weight decay timescale $\tau_{iter}$~\cite{Wang2024AdamWWDEMA} & \multicolumn{4}{c}{$n_{iter} = \num{135594}$} \\
    Gradient clipping norm & \multicolumn{4}{c}{1.0} \\
    
    \midrule
    Dataset & \multicolumn{4}{c}{DFN-2B~\cite{dfn_dataset}} \\
    Image resolution & \multicolumn{4}{c}{$256^2$} \\
    Augmentations & \multicolumn{4}{c}{n/a} \\
    Data type & \multicolumn{4}{c}{\texttt{bfloat16}~\cite{Burgess2019Bfloat16}} \\

    \bottomrule
    \end{tabular}
    \end{adjustbox}
\end{table}


\clearpage
\section{Related Work}
\label{sec:app_relatedwork}

\paragraph{Image tokenization.} 
The goal of tokenization is to convert high-dimensional images into a more compact sequence of token representations, making diffusion and flow~\cite{rombach2022high, Podell2023SDXL, Ma2024SiT, Esser2024SD3, Yu2024REPA}, masked~\cite{Chang2022MaskGIT, Chang2023Muse, Li2022MAGE, Lu2022UnifiedIO, Lu2023UnifiedIO2, 4m, 4m21}, or autoregressive~\cite{Chen2020iGPT, Ramesh2021Dalle1, Yu2022Parti, sun2024autoregressive} image modeling more tractable. Usually, these tokens are learned using an autoencoder objective with a discrete~\cite{van2017neural, razavi2019generating} or continuously regularized~\cite{rombach2022high} bottleneck. In addition, auxilliary perceptual and discriminator losses~\cite{esser2021taming, yu2021improvedvqgan} are commonly used to abstract away imperceptible details, while latent bias losses~\cite{Hu2023GAIA1AG} have been shown to facilitate downstream token prediction. In the discrete case, vector quantization (VQ)~\cite{van2017neural} has been the de facto standard, but more recent techniques such as LFQ~\cite{yu2023magvitv2} and FSQ~\cite{mentzer2023fsq} show promising scaling trends. 

\paragraph{Tokenizers with rectified flow decoding.}
A core property of image tokenizers is the ability to reconstruct perceptually plausible images from the heavily compressed tokens. While discriminator losses~\cite{esser2021taming} are commonly used for this, their training can be notoriously difficult~\cite{yu2024titok}. Instead, we opt to train the decoder as a rectified flow~\cite{Liu2022RectifiedFlow} model, similar to other works~\cite{Shi2022DiVAE, 4m, 4m21, Xu2024DisCoDiff, Zhao2024epsilonVAE}, as this approach has proven successful in scalable image generation with both coarse and fine conditioning -- a property that is particularly useful for our model, \ours, which can handle a wide range of conditioning specificity.

\paragraph{Structured tokenization.} 
So far, most of the aforementioned image tokenization methods project images into a fixed-size 2D grid. Methods like VAR~\cite{tian2024var} and Infinity~\cite{Han2024Infinity} showed impressive scaling trends when projecting images into a structured multi-scale latent representation, while TiTok~\cite{yu2024titok} and DisCo-Diff~\cite{Xu2024DisCoDiff} explored doing away with the 2D grid entirely and projecting images into an unstructured but highly compact 1D sequence. Still, the number of tokens an image is represented with is either fixed in the case of Titok, or depends on the image resolution in the case of Infinity.

\paragraph{Structured and adaptive tokenization.}
To address the issue of the token sequence length depending entirely on the height and width instead of the image complexity, a range of concurrent works have proposed adaptive tokenization methods:
\begin{itemize}
    \item ElasticTok~\cite{Yan2024ElasticTok} is a joint image and video tokenizer that performs nested dropout during training between a pre-specified minimum and maximum number of tokens. The authors note that the minimum bound is set to 128 or 256 due to instabilities at lower values. Compared to ElasticTok, we observe stable training of \ours down to a single token. In addition, by training the decoder with a rectified flow objective, \ours is able to reconstruct high-fidelity images at any number of tokens. Comparatively, ElasticTok requires much higher number of tokens to reconstruct images with high perceptual quality due to being trained only with an MSE and LPIPS loss.
    \item ALIT~\cite{Duggal2024ALIT} presents an adaptive length tokenizer that recurrently encodes images into a growing sequence of tokens. This process can be dynamically halted, leading to adaptively sized image representations. Compared to ALIT, \ours encodes images in a single forward pass, and we efficiently enforce an ordering through causal masks and nested dropout. Since the tokenizers are trained on a 100-class subset of ImageNet-1k and not evaluated on generative tasks, its scaling and generation properties are not entirely experimentally demonstrated. In comparison, we scale both the \ours and downstream AR models, and demonstrate strong generative modeling capabilities.
    \item One-D-Piece~\cite{miwa2025onedpiece} is architecturally similar to TiTok~\cite{yu2024titok} and \ours, in that it uses a register encoder and decoder and is trained using nested dropout on token sequences between 1 and 256. While the One-D-Piece models achieve good reconstruction metrics at high number of tokens, the model fails to produce plausible images at lower number of tokens, unlike \ours. In addition, it requires a two-stage training approach like TiTok, while \ours's Stage 1 is trained end-to-end. Similar to ElasticTok and ALIT, the authors do not evaluate the use of One-D-Piece tokens to train generative models.
    \item ViLex~\cite{Wang2024ViLex} uses a pre-trained text encoder and diffusion model to learn variable-sized sets of ``text tokens'' that encode a given image. These soft tokens can be combined with image prompts to generate novel images, similar to textual inversion~\cite{Gal2022TextualInversion}. The goal of \ours lies more in learning a generic set of coarse-to-fine-grained token sequences, which can both be highly semantic for short sequences and highly detailed for longer sequences. We believe that the use of pre-trained diffusion models as the decoder presents exciting future research directions.
    \item CAT~\cite{Shen2025CATCI} presents a nested VAE architecture that can adaptively compress images into 8x, 16x, and 32x spatially down-sampled representations. Compared to \ours's discrete 1D representations, these token grids are 2D and continuous.
\end{itemize}

\paragraph{Ordered representation learning.}
Besides image generation, learning ordered representations has been a long-studied topic. Nested dropout~\cite{Rippel2014NestedDropout} learns a variably sized bottleneck representation through uniformly dropping latents from one side, while Matryoshka Represenation Learning~\cite{Kusupati2022MatryoshkaRepr} proposes to sample the representation dimensions from powers of two, and decodes each with a weight shared decoder. Matroshka Multimodal Models~\cite{Cai2024MatryoshkaMM} proposes instead to adaptively pool 2D vision encoder representations into smaller 2D grid sizes. UViM~\cite{Kolesnikov2022UViM} proposes to use nested dropout to learn codes that are more robust and easier to model with a downstream language model.

\paragraph{Variable-rate compression.}
In lossy image compression, how to trade compression strength (rate) for reconstruction performance (distortion) has been a long-studied topic. Classical lossy image compression codecs~\cite{wallace1992jpeg, Zern2024WebP, jpeg2000} and more recent neural compression algorithms~\cite{Mentzer2020HighFidelityGI, ElNouby2022PQMIM, Hoogeboom2023HighFidelityIC} effectively enable users to choose this trade-off and are able to compress simple images to smaller file sizes compared to more complex ones. While classical JPEG images are transmitted and decoded in a raster-scan order, progressive schemes like progressive JPEG~\cite{wallace1992jpeg} structure images in a coarse-to-fine manner, allowing users to very quickly reconstruct a low-quality version of an image.



\clearpage
\section{\ours Inference Hyperparameter Sweeps}
\label{sec:app_inference_hparam_sweeps}

For trained \ours models, we sweep different inference-time hyperparameters relating to the rectified flow decoder, such as the number of denoising steps in \cref{fig:app_flextok_timesteps} and the classifier-free guidance scale when using adaptive projected guidance (APG)~\cite{Sadat2024NormGuidance} in \cref{fig:app_flextok_norm_guidance}. We further show guidance sweeps when using a vanilla guidance formulation in \cref{fig:app_flextok_vanilla_guidance}, and compare guidance scales for our largest 1D and 2D tokenizers in \cref{fig:app_flextok_2d_dfn_norm_guidance}.


\subsection{\ours flow decoder timestep sweeps}
\label{sec:app_decoder_timestep_sweep}
We sweep the number of denoising steps between 1 and 100 on \oursxlarge (trained on ImageNet-1k) for different number of tokens. By default, we set the guidance scale to 7.5 and use APG~\cite{Sadat2024NormGuidance}. We find that denoising for more than 25 steps leads to significant diminishing returns in terms of the reconstruction quality metrics, as shown in \cref{fig:app_flextok_timesteps}. rFID and DreamSim metrics plateau, but the mean absolute error to the original images continues to increase, especially for low number of tokens. We choose 25 denoising steps for all subsequent ablations and use it as our default value for all model sizes, as it provides a good balance between inference speed and quality.

\begin{figure}[ht!]
\centering
\includegraphics[width=\linewidth]{resources/appendix/timesteps_in1k.pdf}
\caption{
\textbf{Number of inference denoising steps ablation.} We sweep number of inference steps on \oursxlarge (trained on ImageNet-1k) and find that 25 denoising step provides a good balance between inference speed and quality.
}
\label{fig:app_flextok_timesteps}
\end{figure}


\subsection{Ablating standard classifier-free guidance}
\label{sec:app_vanilla_guidance}
We find that using standard classifier-free guidance~\cite{Ho2022ClassifierFreeGuidance} in the flow matching decoder results in narrow basins of optimal reconstruction performance that are highly dependent on the number of tokens provided to the decoder (see \cref{fig:app_flextok_vanilla_guidance}). We instead opt to use APG~\cite{Sadat2024NormGuidance}, as discussed in \cref{sec:app_flextok_norm_guidance}.

\begin{figure}[ht!]
\centering
\includegraphics[width=\linewidth]{resources/appendix/vanilla_guidance_in1k.pdf}
\caption{
\textbf{Standard classifier-free guidance ablation.} We sweep guidance scales for \oursxlarge (trained on ImageNet-1k) and find optimal guidance scale basins to be narrow and vary strongly across different number of tokens used.
}
\label{fig:app_flextok_vanilla_guidance}
\end{figure}


\subsection{\ours normalized guidance sweeps}
\label{sec:app_flextok_norm_guidance}
We would like to avoid cases where the optimal guidance scales varies greatly across different number of \ours tokens. As observed in \cref{sec:app_vanilla_guidance}, standard classifier-free guidance can exhibit narrow optimality basins. For that reason, we explore the use of normalized guidance schemes like adaptive projected guidance (APG)~\cite{Sadat2024NormGuidance}. We choose hyperparameters for rescaling threshold as $r=2.5$, parallel component as $\eta=0$, and momentum as $\beta=-0.5$, and sweep the guidance scale. Compared to standard guidance, using APG~\cite{Sadat2024NormGuidance} yields optimal performance basins which are shallower, more aligned across differing numbers of tokens, and have overall improved rFID values. The basins of smaller \ours models are less aligned than the ones of larger models, and their optimal guidance values are higher. No matter the model size, the DreamSim metric improves and plateaus with higher guidance scales, while the MAE degrades slightly.

\begin{figure}[ht!]
\centering
\includegraphics[width=0.8\linewidth]{resources/appendix/flextok_norm_guidance_sweeps.pdf}
\caption{
\textbf{Adaptive projected guidance~\cite{Sadat2024NormGuidance} ablation.} We sweep guidance scales for different \ours model sizes and number of tokens and find that APG results in smoother guidance scale basins than standard guidance.
}
\label{fig:app_flextok_norm_guidance}
\end{figure}


\subsection{1D vs 2D tokenizer classifier-free guidance ablation}
\label{sec:app_1d_vs_2d_guidance}
For our largest tokenizer size \oursxlarge (trained on DFN-2B), we compare classifier-free guidance basins with our controlled 2D grid tokenizer baseline on the COCO 30.5k validation set. Both models use APG~\cite{Sadat2024NormGuidance}. As shown in \cref{fig:app_flextok_2d_dfn_norm_guidance}, we find that in terms of rFID and DreamSim score, the 1D tokenizer outperforms the 2D baseline by a large margin across guidance scale values, but observe that the 2D grid tokenizer performs better in terms of MAE.

\begin{figure}[ht!]
\centering
\includegraphics[width=\linewidth]{resources/appendix/flextok_dfn_1d_vs_2d_.pdf}
\caption{
\textbf{1D vs 2D \oursxlarge guidance scale ablation.} We sweep the APG guidance scale for \oursxlarge (trained on DFN-2B) and the controlled 2D tokenizer baseline. The 1D \oursxlarge model is evaluated using the full 256 tokens.
}
\label{fig:app_flextok_2d_dfn_norm_guidance}
\end{figure}

\subsection{Final \ours inference hyperparameters}
\label{sec:app_final_inference_hparam}
From these observations we make the final choices of the inference hyperparameters for each of the model sizes and training datasets and use these for all subsequent image generation experiments. We detail them in \cref{tab:app_in1k_tokenizer_inference_settings} for ImageNet-1k tokenizers and \ref{tab:app_dfn_tokenizer_inference_settings} for DFN-2B tokenizers. We note that common timestep and guidance-scale distillation techniques for diffusion and flow models are applicable to \ours decoders, and we expect that such steps could reduce the computational requirements of running the decoder significantly.

\begin{table}[ht!]
    \caption{\textbf{ImageNet-1k \ours inference settings.} Chosen tokenizer inference configurations used for the three different model sizes trained on ImageNet-1k.}
    \label{tab:app_in1k_tokenizer_inference_settings}
    \centering
    \begin{adjustbox}{max width=\linewidth}
    \begin{tabular}{@{}l|ccc@{}}
    \toprule
    Configuration & \oursbase & \ourslarge & \oursxlarge \\ 
    
    \midrule
    \# denoising steps & 25 & 25 & 25 \\
    Adaptive Projected Guidance (APG)~\cite{Sadat2024NormGuidance} & True & True & True \\
    Decoder guidance scale & 15 & 7.5 & 7.5 \\
    
    \bottomrule
    \end{tabular}
    \end{adjustbox}
\end{table}

\begin{table}[ht!]
    \caption{\textbf{DFN \ours inference settings.} Chosen tokenizer inference configurations used for the three different model sizes trained on DFN-2B.}
    \label{tab:app_dfn_tokenizer_inference_settings}
    \centering
    \begin{adjustbox}{max width=\linewidth}
    \begin{tabular}{@{}l|cc@{}}
    \toprule
        Configuration & \oursxlarge  & \texttt{2D Grid d18-d28} \\ 
    \midrule
    \# denoising steps & 25 & 25 \\
    Adaptive Projected Guidance (APG)~\cite{Sadat2024NormGuidance} & True & True \\
    Decoder guidance scale & 7.5 & 5.0 \\
    \bottomrule
    \end{tabular}
    \end{adjustbox}
\end{table}


 
\clearpage
\section{Autoregressive Class-Conditional Image Generation Hyperparameter Sweeps}
\label{sec:app_c2i_hyper_params}

For class-conditional AR Transformers trained on top of \ours tokenizers we sweep a variety of inference-time hyperparameters. We implement classifier-free guidance in the AR model as a update of the prediction logits $\hat{x}$ given by $\hat{x} = x_{\text{uncond}} + s \cdot (x_{\text{cond}} - x_{\text{uncond}})$. Here $x_{\text{cond}}$ and $x_{\text{uncond}}$ are the logits with and without the conditioning supplied to the model. Taking a fully trained AR 1.33B and \oursxlarge we find that using no guidance, i.e. CFG scale $s$~=~1.0, results in the best gFID values irrelevant of the top-k sampling or number of tokens generated before decoding to image space (\cref{fig:app_c2i_d30_cfg_and_top_k_sweep,fig:app_c2i_d30_cfg_and_num_tokens_sweep}). A optimal gFID value with no guidance sets AR models with \ours tokenizers apart from previous examples in the literature. For example the optimal CFG value for the LlamaGen models was 2.0 for their class-conditional models~\cite{sun2024autoregressive}. The ordering introduced by the causal mask and nested-dropout in the tokenizer could be the source of the property in the subsequent AR model. There are significant inference compute advantages if CFG is not applied, in particular the batch doesn't need to be increased by a factor of 2 to accommodate the unconditional samples. Besides the interesting dependence on the classifier-free guidance scale we find that increasing the top-k sampling threshold improves the gFID values. For optimal generation FID we find that no top-k sampling should be used.

\begin{figure}[h]
\centering
\includegraphics[width=0.5\linewidth]{resources/appendix/c2i_ar_d30_flextok_d18_d28_cfg_and_top_k_sweep.pdf}
\caption{
\textbf{Class-conditional 1.33B AR model with \oursxlarge guidance and top-K ablation.}
gFID measured with respect to the full ImageNet-1k train set statistics. 
}
\label{fig:app_c2i_d30_cfg_and_top_k_sweep}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=0.5\linewidth]{resources/appendix/c2i_ar_d30_flextok_d18_d28_BS_cfg_and_num_token_sweep.pdf}
\caption{
\textbf{Class-conditional 1.33B AR model with \oursxlarge guidance and number of tokens ablation.}
gFID measured with respect to the full ImageNet-1k train set statistics. 
}
\label{fig:app_c2i_d30_cfg_and_num_tokens_sweep}
\end{figure}

 
\clearpage
\section{Autoregressive Class-Conditional Image Generation Model Size}
\label{sec:app_c2i_model_size}

By scaling up the AR model size in the class conditioned models we observe consistent improvements in the training loss (\cref{fig:app_c2i_ar_model_size_scaling_loss_curves}). Following the optimal inference parameters observed in \cref{sec:app_c2i_hyper_params}, no CFG and no top-k sampling, we evaluate the effect of scaling the AR model size on top of the \oursxlarge tokenizer. \cref{fig:app_c2i_ar_model_size_scaling} shows that generation performance when producing only a few tokens is independant of model size. However the longer the sequence the more the larger AR model size limits token decoding errors. When generating the full 256 tokens scaling up the AR model size significantly improves the gFID values. We note that even with the largest AR model investigated here, with 1.33B parameters, we still observe a slight regression in the gFID values as the number of decoded tokens increases about 128.


\begin{figure*}[h]
\centering
\includegraphics[width=\linewidth]{resources/appendix/loss_curves_in1k_ar_model_size_sweep.pdf}
\caption{
\textbf{Autoregressive class-conditional image generation loss curves.} The AR models of different sizes shown here are trained for 94B tokens on ImageNet-1k using the \oursxlarge tokenizer trained on ImageNet-1k.
}
\label{fig:app_c2i_ar_model_size_scaling_loss_curves}
\end{figure*}

\begin{figure*}[h]
\centering
\includegraphics[width=\linewidth]{resources/appendix/imagenet_generation_ar_model_size.pdf}
\caption{
\textbf{Autoregressive class-conditional image generation model scaling.} Left shows the gFID values for each model size at varying numbers of generated tokens. The right figure shows the gFID with 256 tokens against the parameter count of the AR model. The gFID values are measured with respect to the full ImageNet-1k train set statistics. All AR models are trained using the \oursxlarge tokenizer trained on ImageNet-1k. During generation we use the optimal inference parameters detailed in \cref{sec:app_c2i_hyper_params}.
}
\label{fig:app_c2i_ar_model_size_scaling}
\end{figure*}

Scaling up the tokenizer size significantly improves the generation quality (\cref{fig:app_c2i_flextok_model_size_scaling}). With a fixed AR model size of 1.33B we scale up the tokenizers size from \oursbase to \oursxlarge and find improvements in the measured gFID values. For some tokenizer sizes the reconstruction quality of the tokenizer is upper bounding the performance of the generative model.

\begin{figure*}[h]
\centering
\includegraphics[width=\linewidth]{resources/appendix/imagenet_generation_ar_gen_flextok_model_size.pdf}
\caption{
\textbf{Autoregressive class-conditional image generation \ours tokenizer scaling.} Left shows the gFID values for 1.33B AR models trained with each of the different sized \ours tokeninizers at varying numbers of generated tokens. The right figure shows the gFID with 256 tokens against the parameter in the \ours tokenizer. The gFID values are measured with respect to the full ImageNet-1k train set statistics. During generation we use the optimal inference parameters detailed in \cref{sec:app_c2i_hyper_params}.
}
\label{fig:app_c2i_flextok_model_size_scaling}
\end{figure*}

 
\clearpage
\section{Autoregressive Text-Conditional Image Generation Inference Hyperparameter Sweeps}
\label{sec:app_t2i_inference_hparam_sweeps}

For text conditional AR Transformers trained with \ours tokenizers we sweep the hyperparameter classifier-free guidance scale and number of decoded tokens. Taking a fully trained AR 3.06B and \oursxlarge we find that using classifier-free guidance improves the gFID for all lengths of generated token sequences (\cref{fig:app_t2i_d36_cfg_and_num_token_sweep}). This observation in contrast to the class conditioned \ours based models where using CFG hurt gFID performance. The difference in the complexity of the conditioning and any distribution shift between the DFN training and COCO validation datasets could be a contributing factor here. In addition to the CFG improving gFID values we also find that the CLIPScore increases with increasing guidance. 

We find that a CFG scale of 2.5 optimizes the gFID values across the variety of number of tokens generated. Using this CFG scale we see gFID values which sharply drops and then gradually increase as the number of tokens generated increases (\cref{fig:app_t2i_d36_num_token_sweep}). In comparison we find that the text-image alignment measured by the CLIPScore only improves as the number of tokens is increased.

\begin{figure*}[h]
\centering
\includegraphics[width=\linewidth]{resources/appendix/t2i_d36_coco.pdf}
\caption{
\textbf{Text-conditional 3.06B AR with \oursxlarge guidance and number of tokens ablation.}
gFID (left) and CLIPScore (right) metrics measured with respect to the COCO validation set at varying numbers of tokens.
}
\label{fig:app_t2i_d36_cfg_and_num_token_sweep}
\end{figure*}

\begin{figure*}[h]
\centering
\includegraphics[width=\linewidth]{resources/appendix/t2i_num_tokens_sweep.pdf}
\caption{
\textbf{Text-conditional 3.06B AR with \oursxlarge image generation vs number of tokens.} All metrics are computed using a CFG scale of 2.5 during generation. Left shows the rFID of the DFN trained \oursxlarge on the COCO validation set. The AR image generation gFID (middle) and CLIPScore (right) on the COCO validation set as functions of the number of generated tokens.
}
\label{fig:app_t2i_d36_num_token_sweep}
\end{figure*}

\cref{fig:app_t2i_d36_2d_grid_num_token_sweep} shows the 2D grid based tokenizer has a similar dependence of the evaluation metrics on the CFG scale used. We select a CFG scale of 2.5 as a balance between optimizing the gFID and CLIPScore metrics. Additionally this selected CFG scale is the same as the value used for the \ours based models enabling a balanced comparison of the two approaches.

\begin{figure*}[h]
\centering
\includegraphics[width=\linewidth]{resources/appendix/t2i_d36_2d_grid_coco.pdf}
\caption{
\textbf{Text-conditional 3.06B AR with 2D grid tokenizer guidance ablation.}
gFID (left) and CLIPScore (right) metrics measured with respect to the COCO validation set at varying numbers of tokens.
}
\label{fig:app_t2i_d36_2d_grid_num_token_sweep}
\end{figure*}


\clearpage
\section{Autoregressive Text Conditional Image Generation Model Size}
\label{sec:app_t2i_model_size}

Following the optimal inference parameters observed in \cref{sec:app_t2i_inference_hparam_sweeps}, we evaluate the effect of scaling the AR model size while fixing the \oursxlarge tokenizer. Scaling up the model size from 113M to 3.06B parameters results in lower final loss values being achieved for both the \ours and 2D grid tokenizer (\cref{fig:app_t2i_ar_model_size_scaling_loss_curves} and \ref{fig:app_2d_grid_t2i_ar_model_size_scaling_loss_curves}). When generating the full 256 tokens, scaling up the AR model size significantly improves the gFID values (\cref{fig:app_t2i_ar_model_size_scaling}).

\begin{figure*}[ht!]
\centering
\includegraphics[width=\textwidth]{resources/appendix/loss_curves_dfn_flextok_ar_model_size_sweep.pdf}
\caption{
\textbf{\ours autoregressive text-conditional image generation loss curves.} The AR models of different sizes shown here are trained for 284B tokens on DFN using the \oursxlarge tokenizer trained on DFN.
}
\label{fig:app_t2i_ar_model_size_scaling_loss_curves}
\end{figure*}

\begin{figure*}[ht!]
\centering
\includegraphics[width=\textwidth]{resources/appendix/loss_curves_dfn_2d_grid_ar_model_size_sweep.pdf}
\caption{
\textbf{2D grid tokenizer autoregressive text-conditional image generation loss curves.} The AR models of different sizes shown here are trained for 284B tokens on DFN using the \oursxlarge tokenizer trained on DFN.
}
\label{fig:app_2d_grid_t2i_ar_model_size_scaling_loss_curves}
\end{figure*}

\begin{figure*}[ht!]
\centering
\includegraphics[width=0.5\textwidth]{resources/appendix/coco_generation_ar_model_size_sweep.pdf}
\caption{\textbf{Autoregressive text conditional image generation model scaling.} The gFID values for each model size as functions of the parameter count of the AR model. During AR generation we \textbf{generate 256 tokens so that the \ours and 2D grid tokenizer can be compared} and use the optimal inference parameters detailed in \cref{sec:app_t2i_inference_hparam_sweeps}. The gFID values are measured with respect to the COCO validation set. All AR models are trained using the tokenizers trained on DFN.
}
\label{fig:app_t2i_ar_model_size_scaling}
\end{figure*}





\clearpage
\section{Additional Visualizations}
\label{sec:app_viz}


\subsection{\ours image reconstruction for different numbers of tokens -- multiple samples per token sequence}
\label{sec:app_reconst_samples_viz}

\begin{figure}[ht!]
\centering
\includegraphics[width=0.828\linewidth]{resources/appendix/reconst_numtoks_vs_samples_0.pdf}
\caption{
\textbf{\ours samples for different number of tokens.}
We show \oursxlarge (trained on ImageNet-1k) reconstructions for different numbers of tokens for ImageNet-1k validation set samples. We draw 5 random samples from the rectified flow decoder given the same token sequences. \ours token sequences define a distribution over images that gets narrower, and more specific to the original image, the more tokens are used.
}
\label{fig:app_reconst_samples_0}
\end{figure}

\begin{figure}[ht!]
\centering
\includegraphics[width=0.828\linewidth]{resources/appendix/reconst_numtoks_vs_samples_1.pdf}
\caption{
\textbf{\ours samples for different number of tokens.}
We show \oursxlarge (trained on ImageNet-1k) reconstructions for different numbers of tokens for ImageNet-1k validation set samples. We draw 5 random samples from the rectified flow decoder given the same token sequences. \ours token sequences define a distribution over images that gets narrower, and more specific to the original image, the more tokens are used.
}
\label{fig:app_reconst_samples_1}
\end{figure}


\clearpage
\subsection{\ours image reconstruction for different numbers of tokens and tokenizer sizes}
\label{sec:app_tokens_vs_model_size_viz}

\begin{figure}[ht!]
\centering
\includegraphics[width=\linewidth]{resources/appendix/reconst_numtoks_vs_modelsize_in1k_0.pdf}
\caption{
\textbf{\ours reconstructions for different numbers of tokens and model sizes.}
We show \ours (trained on ImageNet-1k) reconstructions for different numbers of tokens and model sizes (\texttt{d12-d12}, \texttt{d18-d18}, \texttt{d18-d28}) for ImageNet-1k validation set samples.
}
\label{fig:app_reconst_vs_model_size_0}
\end{figure}

\begin{figure}[ht!]
\centering
\includegraphics[width=\linewidth]{resources/appendix/reconst_numtoks_vs_modelsize_in1k_1.pdf}
\caption{
\textbf{\ours reconstructions for different numbers of tokens and model sizes.}
We show \ours (trained on ImageNet-1k) reconstructions for different numbers of tokens and model sizes (\texttt{d12-d12}, \texttt{d18-d18}, \texttt{d18-d28}) for ImageNet-1k validation set samples.
}
\label{fig:app_reconst_vs_model_size_1}
\end{figure}


\clearpage
\subsection{Image reconstruction comparison between \ours, TiTok, and ALIT}
\label{sec:app_reconst_comparison_viz}

\begin{figure}[ht!]
\centering
\includegraphics[width=\linewidth]{resources/appendix/reconst_baseline_comparison_0.pdf}
\caption{
\textbf{Reconstruction comparison between \ours and baselines.}
We show \oursxlarge (trained on ImageNet-1k) reconstructions for different numbers of tokens and for ImageNet-1k validation set samples, and compare against three different TiTok~\cite{yu2024titok} models, and ALIT~\cite{Duggal2024ALIT}.
}
\label{fig:app_reconst_comparison_viz_0}
\end{figure}

\begin{figure}[ht!]
\centering
\includegraphics[width=\linewidth]{resources/appendix/reconst_baseline_comparison_1.pdf}
\caption{
\textbf{Reconstruction comparison between \ours and baselines.}
We show \oursxlarge (trained on ImageNet-1k) reconstructions for different numbers of tokens and for ImageNet-1k validation set samples, and compare against three different TiTok~\cite{yu2024titok} models, and ALIT~\cite{Duggal2024ALIT}.
}
\label{fig:app_reconst_comparison_viz_1}
\end{figure}


\clearpage
\subsection{Class-conditional image generation visualizations}
\label{sec:app_l2i_viz}

\begin{figure}[ht!]
\centering
\includegraphics[width=\linewidth]{resources/appendix/l2i_varying_classes_varying_num_tokens.pdf}
\caption{
\textbf{\ours class-conditional image generations with varying numbers of tokens.} From left to right the number of tokens used increases in powers of 2 from 1 up to 256 tokens. Each row represents a different ImageNet-1k class index supplied to the auto-regressive image generator as conditioning. Images are generated using the \oursxlarge tokenizer combined with a 1.33B parameter AR Transformer.
}
\label{fig:l2i_generations_varying_classes_varying_num_tokens}
\end{figure}

\begin{figure}[ht!]
\centering
\includegraphics[width=\linewidth]{resources/appendix/l2i_golden_retiever_varying_num_tokens.pdf}
\caption{
\textbf{\ours class-conditional image generations with varying numbers of tokens.} From left to right the number of tokens used increases in powers of 2 from 1 up to 256 tokens. Images are generated using the \oursxlarge tokenizer combined with a 1.33B parameter AR Transformer. The model is conditioned on the class label from ImageNet-1k for "golden retriever" (207), using different random seeds for each row.
}
\label{fig:l2i_golden_retiever_varying_num_tokens}
\end{figure}

\begin{figure}[ht!]
\centering
\includegraphics[width=\linewidth]{resources/appendix/l2i_flamingo_varying_num_tokens.pdf}
\caption{
\textbf{\ours class-conditional image generations with varying numbers of tokens.} From left to right the number of tokens used increases in powers of 2 from 1 up to 256 tokens. Images are generated using the \oursxlarge tokenizer combined with a 1.33B parameter AR Transformer. The model is conditioned on the class label from ImageNet-1k for "flamingo" (130), using different random seeds for each row.
}
\label{fig:l2i_flamingo_varying_num_tokens}
\end{figure}

\begin{figure}[ht!]
\centering
\includegraphics[width=\linewidth]{resources/appendix/l2i_macaw_varying_num_tokens.pdf}
\caption{
\textbf{\ours class-conditional image generations with varying numbers of tokens.} From left to right the number of tokens used increases in powers of 2 from 1 up to 256 tokens. Images are generated using the \oursxlarge tokenizer combined with a 1.33B parameter AR Transformer. The model is conditioned on the class label from ImageNet-1k for "Macaw" (88), using different random seeds for each row.
}
\label{fig:l2i_macaw_varying_num_tokens}
\end{figure}


\clearpage
\subsection{Text-conditional image generation visualizations}
\label{sec:app_t2i_viz}

\begin{figure}[ht!]
\centering
\includegraphics[width=\linewidth]{resources/appendix/t2i_varying_coco_promtps_varying_num_tokens.pdf}
\caption{
\textbf{\ours text-conditional image generations with varying numbers of tokens.} From left to right the number of tokens used increases in powers of 2 from 1 up to 256 tokens. Images are generated using the \oursxlarge tokenizer trained on DFN combined with a 3.06B parameter AR Transformer also trained on DFN. For each row the AR Transformer is conditioned on the text embeddings of the prompts.
}
\label{fig:t2i_varying_coco_promtps_varying_num_tokens}
\end{figure}

\begin{figure}[ht!]
\centering
\includegraphics[width=\linewidth]{resources/appendix/t2i_coco_prompt_A_furry,_black_bear_standing_in_a_rocky,_weedy,_area_in_the_wild_varying_num_tokens.pdf}
\caption{
\textbf{\ours text-conditional image generations with varying numbers of tokens.} From left to right the number of tokens used increases in powers of 2 from 1 up to 256 tokens. Images are generated using the \oursxlarge tokenizer trained on DFN combined with a 3.06B parameter AR Transformer also trained on DFN. For each row the AR Transformer is conditioned on the text embeddings of the prompt \textit{``A furry, black bear standing in a rocky, weedy, area in the wild''}, but uses a different random seed.
}
\label{fig:t2i_coco_prompt_A_furry_black_bear_varying_num_tokens}
\end{figure}

\begin{figure}[ht!]
\centering
\includegraphics[width=\linewidth]{resources/appendix/t2i_varying_partipromtps_varying_num_tokens.pdf}
\caption{
\textbf{\ours text-conditional image generations with varying numbers of tokens.} From left to right the number of tokens used increases in powers of 2 from 1 up to 256 tokens. Images are generated using the \oursxlarge tokenizer trained on DFN combined with a 3.06B parameter AR Transformer also trained on DFN. For each row the AR Transformer is conditioned on the text embeddings of a different PartiPrompt~\cite{Yu2022Parti}.
}
\label{fig:t2i_varying_partipromtps_varying_num_tokens}
\end{figure}

\begin{figure}[ht!]
\centering
\includegraphics[width=\linewidth]{resources/appendix/t2i_varying_extra_partipromtps_varying_num_tokens.pdf}
\caption{
\textbf{\ours text-conditional image generations with varying numbers of tokens.} From left to right the number of tokens used increases in powers of 2 from 1 up to 256 tokens. Images are generated using the \oursxlarge tokenizer trained on DFN combined with a 3.06B parameter AR Transformer also trained on DFN. For each row the AR Transformer is conditioned on the text embeddings of a different PartiPrompt~\cite{Yu2022Parti}.
}
\label{fig:t2i_varying_extra_partipromtps_varying_num_tokens}
\end{figure}


\begin{figure}[ht!]
\centering
\includegraphics[width=\linewidth]{resources/appendix/t2i_partiprompt_A_blue_Porsche_356_parked_in_front_of_a_yellow_brick_wall_varying_num_tokens.pdf}
\caption{
\textbf{\ours text-conditional image generations with varying numbers of tokens.} From left to right the number of tokens used increases in powers of 2 from 1 up to 256 tokens. Images are generated using the \oursxlarge tokenizer trained on DFN combined with a 3.06B parameter AR Transformer also trained on DFN. For each row the AR Transformer is conditioned on the text embeddings of the prompt \textit{``A blue Porsche 356 parked in front of a yellow brick wall''}, but uses a different random seed.
}
\label{fig:t2i_partiprompt_A_blue_Porsche_356_parked_in_front_of_a_yellow_brick_wall_varying_num_tokens}
\end{figure}
