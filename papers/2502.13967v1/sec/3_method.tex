\begin{figure*}[ht!]
\centering
\includegraphics[width=\linewidth]{resources/main/flextok_overview.pdf}
\caption{
\textbf{\ours overview.} \textbf{Stage 1}: \ours resamples 2D VAE latents to a 1D sequence of discrete tokens using a ViT with registers~\cite{Darcet2023Registers}. The FSQ-quantized bottleneck~\cite{mentzer2023fsq} representation is used to condition a rectified flow model that decodes and reconstructs the original images. \ours learns ordered token sequences of flexible length by applying nested dropout~\cite{Rippel2014NestedDropout} on the register tokens. 
\textbf{Stage 2}: We train class- and text-conditional autoregressive Transformers to predict 1D token sequences in a coarse-to-fine manner. As more tokens are predicted, the generated image becomes more specific, encoding high-level concepts first \textit{(e.g., presence of a car)} followed by finer details \textit{(e.g., car shape, brand, color)}.}
\label{fig:flextok_overview}
\vspace{-0.5em}
\end{figure*}

\section{Method}

\ours is an autoencoder with a discrete 1D bottleneck, see Figure~\ref{fig:flextok_overview} for an overview. A ViT encoder maps 2D image patches into a 1D sequence using register tokens~\cite{Darcet2023Registers, yu2024titok}. The registers are discretized using FSQ~\cite{mentzer2023fsq}, and then used as conditioning for a rectified flow model tasked with reconstructing the image. We use causal attention among register tokens followed by nested dropout~\cite{Rippel2014NestedDropout} to induce an ordering in the bottleneck representation. Paired with a rectified flow decoder, this design enables the model to decode any nested subset of tokens into plausible images.


\subsection{1D tokenization with a rectified flow decoder}

\paragraph{Register encoder and discrete bottleneck.} Similar to TiTok~\cite{yu2024titok}, the encoder uses register tokens to resample the patched 2D VAE latents into a 1D sequence of discrete tokens. Specifically, we concatenate VAE latent patches with a set of learnable register tokens that act as read-write storage for the encoder. After encoding, the register tokens act as the bottleneck representation for the autoencoder, while the encoded patches are discarded. The register tokens are quantized into discrete tokens using FSQ. Instead of pixels, we operate on latent representations of a VAE-GAN, similar to SDXL's VAE~\cite{Podell2023SDXL}, to abstract away the perceptual compression from this investigation. Using the VAE-GAN latent space enables us to greatly simplify our design space and reduce computational requirements as training generative models on pixels is expensive~\cite{rombach2022high}.

\paragraph{Rectified flow decoder.}\label{sec:rf_decoder}
The decoder's purpose is to generate perceptually plausible images conditioned on the compressed latent representations. When the bottleneck size is small, training a decoder with only a reconstruction loss can result in blurry reconstructions. To mitigate this, \ours uses a rectified flow model that is conditioned on the quantized register tokens $Quant(\bm{Z}_{1D})$ by concatenating them with noised VAE latent patches $\bm{X}_t = (1-t) \bm{X}_0 + t \epsilon$, where $\bm{X}_0$ are the clean VAE latents, $t \in [0,1]$ is a random time step, and $\bm{\epsilon} \sim \mathcal{N}(0,1)$ is a random sample from the noise distribution. The decoder's objective is to predict the flow $\bm{U} = \bm{\epsilon} - \bm{X}_0$ given the partially noised VAE latents and the encoded register tokens. We minimize the rectified flow loss $\mathcal{L}_\text{RF} = || \bm{\hat{U}} - (\bm{\epsilon} - \bm{X}_0) ||^2$, given the predicted flow $\bm{\hat{U}} = Dec(Quant(\bm{Z}_{1D}) \oplus \bm{X}_t)$. We find that adding a REPA~\cite{Yu2024REPA} inductive bias loss $\mathcal{L}_\text{REPA}$ between an intermediate decoder layer and DINOv2-L~\cite{Oquab2023DINOv2} features significantly improves convergence time and downstream generation performance (see \cref{tab:resampling_strategies} and Fig.\ref{fig:app_flextok_repa_eval_curves}). The total \ours loss we optimize is $\mathcal{L}_\ours = \mathcal{L}_\text{RF} + \lambda_\text{REPA} \cdot \mathcal{L}_\text{REPA}$, with $\lambda_\text{REPA} = 1$. See \cref{sec:implementation} and \cref{sec:app_resampler_training_details} for more implementation details.


\subsection{Learning ordered 1D token sequences of flexible length}
\label{sec:ordered_flex_sequences}

1D tokenizers like TiTok~\cite{yu2024titok} require training different tokenizers for each desired number of register tokens. As shown in the concurrent work ALIT~\cite{Duggal2024ALIT}, fixed token sequence lengths do not take into account the inherent complexity of an image. Simple images can be compressed into as few as 32 tokens, while more complex ones require more tokens to faithfully reconstruct them. Neither TiTok nor ALIT demonstrate flexible tokenization below 32 tokens, and their image reconstruction performance deteriorates when the compression degree is high, see \cref{fig:reconst_visual_comp,sec:app_reconst_comparison_viz} for visual comparisons to \ours. Since we target AR generation with \ours, we additionally introduce a nested left-to-right ordering structure that naturally aligns with next-token prediction. We propose two techniques to introduce a 1D ordering and variable length into the token sequences.

\paragraph{Nested dropout.} We train \ours to produce an ordered representation by randomly dropping the encoded register tokens in a nested manner during training~\cite{Rippel2014NestedDropout, Kusupati2022MatryoshkaRepr, Cai2024MatryoshkaMM} . Specifically, given a register token sequence of length $K$, we randomly sample the number of units to keep $K_{keep} \in \{1, ..., K\}$ and remove the $K-K_{keep}$ last tokens by masking them out. By training the tokenizer in this manner, the encoder learns to compress the image content into the register tokens in an ordered manner, while the rectified flow decoder learns to reconstruct images given the variably-sized token sequences. As we show in \cref{fig:reconst_rate_distortion}, this design enables \ours to capture the most important aspects of images in very few tokens. Simplistic images require few tokens to be faithfully compressed, while complex ones require longer token sequences. We note here that this ordering is not handcrafted and emerges purely from performing nested dropout on register tokens, and computing rectified flow and REPA losses. We mainly ablate two strategies of dropping tokens in a nested manner. In the first, we sample the number of tokens to keep uniformly, as described above. In the second variant, we uniformly sample them from an exponentially increasing set, e.g. $K_{keep} \in \{1, 2, 4, 8, 16, ..., K\}$. The latter variant addresses an issue with uniform nested dropout, where the last register tokens are passed to the decoder very rarely, meaning they are effectively trained for only a small fraction of gradient updates.

\paragraph{Causal attention masks.} Orthogonal to the use of nested dropout, adding a causal attention mask to the encoded registers enforces a causal dependency structure between them. In this setting, the encoded image patches can all attend to each other but not to the registers, the register tokens can attend to all the patches, but the $i$-th register token may only attend to the $j$-th register token if $i \geq j$.
In addition, the use of causal masks enables users to more efficiently encode images if they know ahead of time that they only want to keep $K_{keep} \ll K$ tokens.


\subsection{Autoregressive image generation}
To evaluate different design choices of \ours and compare to relevant baselines, we measure both reconstruction and generation performance. To that end, we train autoregressive Transformers to perform class-conditional generation on ImageNet-1k~\cite{Russakovsky2014ImageNet}, following LlamaGen~\cite{sun2024autoregressive}, and text-to-image generation on DFN-2B~\cite{dfn_dataset}. 


\begin{figure*}[ht!]
\centering
\includegraphics[width=\textwidth]{resources/main/reconst_comparison.pdf}
\vspace{-1.5em}
\caption{
\textbf{Image reconstruction comparison between three different TiTok~\cite{yu2024titok} models, ALIT~\cite{Duggal2024ALIT}, and \ours.} Compared to other 1D tokenizers, \ours is able to tokenize images in a highly semantic and ordered manner, all the way down to a single token, and all in a single model. For more visual comparisons, see \cref{sec:app_tokens_vs_model_size_viz,sec:app_reconst_samples_viz,sec:app_reconst_comparison_viz}.
}
\label{fig:reconst_visual_comp}
\end{figure*}
