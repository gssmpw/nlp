\section{Implementation}
\label{sec:implementation}

\begin{figure*}[ht!]
\centering
\includegraphics[width=\textwidth]{resources/main/reconst_metrics.pdf}
\vspace{-1.5em}
\caption{\textbf{\ours rate-distortion tradeoff.}
We show ImageNet-1k \textbf{reconstruction} metrics for three different \ours sizes. The more tokens used, the closer the reconstructions get to the original RGB images. Scaling the tokenizer size significantly improves reconstruction FID, but is not as crucial in terms of MAE and DreamSim score. For each of the different \ours model sizes we use the optimal inference hyperparameters detailed in \cref{sec:app_inference_hparam_sweeps}.
}
\label{fig:reconst_rate_distortion}
\vspace{-0.4em}
\end{figure*}

We break down the implementation into three distinct stages. In \textbf{Stage 0}, we train VAE models~\cite{rombach2022high} with continuous latents to perceptually compress images into 2D token grids. In \textbf{Stage 1}, we then train \ours tokenizers to resample these continuous 2D token grids into discrete 1D token sequences of flexible length. Finally, in \textbf{Stage 2}, we train autoregressive class-to-image and text-to-image models to evaluate the effectiveness of \ours in generative tasks.

\paragraph{Stage 0: VAE training.}
The rectified flow decoder (see \cref{sec:rf_decoder}) is a key design element of \ours that enables decoding arbitrary token subsequences. However, training such models directly in pixel space is computationally expensive~\cite{rombach2022high}. The goal of this stage is therefore to facilitate \textbf{Stage 1} modeling by perceptually compressing images into more compact representations. We follow the architecture of the SDXL VAE~\cite{Podell2023SDXL}, and train versions with 4, 8, and 16 channels on the DFN dataset~\cite{dfn_dataset}. All subsequent experiments use the 16 channel VAE with a downsampling factor of 8. Please see \cref{sec:app_vae_training_details} for more VAE training details, and \cref{tab:vae_num_ch_comparison} for ablations on the number of latent channels.

\paragraph{Stage 1: \ours training.}
The \ours architecture consists of a Transformer encoder and decoder using a maximum of 256 registers tokens. After applying a 6-dimensional FSQ~\cite{mentzer2023fsq} bottleneck with levels \texttt{[8, 8, 8, 5, 5, 5]} (for an effective vocabulary size of \num{64000}), the encoded registers are randomly truncated using nested dropout. The decoder is a rectified flow model that receives noised VAE latent patches and the (randomly masked) registers as input, and is tasked to predict the flow. We use adaLN-zero~\cite{peebles2023scalable} to condition the patches and registers separately on the current timestep, and REPA~\cite{Yu2024REPA} with DINOv2-L~\cite{Oquab2023DINOv2} features to speed up convergence. We use 2x2 patchification in both the \ours encoder and decoder, which combined with the VAE's 8x downsampling yields a total 16x downsampling from pixels to patch tokens. All models are trained at a resolution of 256x256 pixels. The encoder and decoder dimensions $w$ are parameterized using their respective depths $d$, using a fixed aspect ratio of 64, i.e. $w = 64 \cdot d$. We train three \ours versions with different encoder and decoder sizes (separated by a hyphen), \texttt{d12-d12}, \texttt{d18-d18} and \texttt{d18-d28} after sweeping optimal hyperparameters at a small scale using $\mu$P~\cite{Yang2022muP}. Depending on the downstream use case, we train \ours models on ImageNet-1k~\cite{Russakovsky2014ImageNet} for subsequent class-conditional generation, and on DFN~\cite{dfn_dataset} for subsequent text-to-image modeling. See \cref{sec:app_resampler_training_details} for further \ours implementation and training details.

\paragraph{Stage 2: AR Transformer training.}
Our autoregressive Transformer follows LlamaGen's Llama-inspired architecture~\cite{sun2024autoregressive,touvron2023llama}, using pre-normalization with RMSNorm~\cite{zhang2019rmsnorm} and a SwiGLU feedforward~\cite{Shazeer2020GLU}. Since our tokens lack a 2D grid structure, we use learned absolute positional embeddings instead of 2D RoPE~\cite{su2024roformer}. 

For class conditioning, we add a learned class embedding to an \texttt{[SOI]} token~\cite{tian2024var} and concatenate it with the image token sequence. The AR model, ranging from 49M to 1.3B parameters, predicts the token sequence from the \ours tokenizer. To enable comparisons with LlamaGen and TiTok, we train without $\mu$P.

For text-conditioned generation,  our AR decoder cross-attends to text embeddings from FLAN-T5-XL~\cite{chung2024flan}, projected to the model dimension via an MLP~\cite{chen2023pixart}. We scale these text-conditioned AR models up to 3B parameters, using $\mu$P to maintain consistent behavior across scales.

Following standard practice, we employ conditioning dropout during training to enable classifier-free guidance at inference. For text-conditioned models, this involves randomly replacing text inputs with an empty string.
For additional implementation and training details, see \cref{sec:app_ar_training_details}.
