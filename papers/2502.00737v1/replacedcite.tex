\section{Related Works}
\label{sec:related_works}

In this section, we discuss related works capitalizing  on the Sobolev IPM and Sobolev geometric structure.

____ leverages Sobolev IPM to study the convergence rate for learning density with the GAN framework. This work shows how convergence rates depend on Sobolev smoothness restrictions within the Sobolev IPM.____ also further exploits Sobolev geometry to derive generalization bounds for deep ReLU discriminator networks in GAN. Additionally,____ improve these results under Sobolev IPM by employing the adversarial framework for the analysis. More recently,____ leverages Sobolev norm for unnormalized density estimation.

%kozdoba2023unnormalized,

%Besides,____ generalize these approaches with Sobolev IPM to the more general Besov IPM.

____ study bracket metric entropy for Sobolev space, which plays a central role in many limit theorems for empirical processes____, and for studying convergence rates, lower risk bounds of statistical estimators____.

Sobolev GAN____ exploits the so-called Sobolev discrepancy to compare probability measures for the GAN framework. More concretely, Sobolev GAN constraints the critic function in IPM within a unit ball defined by the $L^2$-norm of its gradient function with respect to a dominant measure, which shares the same spirit as the Sobolev Wasserstein GAN approach____. On the other hand, Fisher GAN____ constraints the critic function by the $L^2$-norm of itself with respect to a dominant measure. Thus, the Sobolev norm (Equation~\eqref{eq:SobolevNorm}), which integrates information from both critic function and its gradient function, can be regarded as a unification for the approaches in Fisher GAN and Sobolev GAN within the Sobolev IPM problem.

____ proposes the kernelized approach for the Sobolev discrepancy which constraints the critic function of the Sobolev discrepancy within a reproducing kernel Hilbert space. Then ____ leverages the kernel Sobolev discrepancy to quantify kinetic energy to propose Sobolev descent, i.e., a gradient flow that finds a path of distributions from source to target measures minimizing the kinetic energy. Additionally,____ extend the kernelized approach to unbalanced settings where source and target measures may have different total mass.

____ leverages Sobolev norm for manifold regularization in semi-supervised learning (SSL).____ studies Sobolev IPM uncertainty set for the distributional robust optimization, and links the distributional robustness with the manifold regularization penalty____. Additionally,____ uses Sobolev discrepancy to propose Sobolev independence criterion for nonlinear feature selection. Furthermore,____ employs Sobolev IPM to analyze theoretical properties for Gaussian-smoothed $p$-Wasserstein distance.

%Intuitively, SIC can be regarded as a gradient-regularized IPM between joint distributions of the two random variables and the product of their marginals, i.e., input measures, which generalizes the mutual information. 

Sobolev transport (ST)____ is a scalable variant of OT on a graph, which constraints the Lipschitz condition within the graph-based Sobolev space. Moreover, the $p$-order ST can be considered as a generalization of the Sobolev discrepancy, which constraints the critic function within a unit norm defined by the $L^p$-norm of its gradient. Additionally,____ extend ST for the unbalanced setting where input measures may have different total mass, while ____ leverage a class of convex functions to extend ST to more general geometric structures which is beyond its original $L^p$-geometric structure.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%