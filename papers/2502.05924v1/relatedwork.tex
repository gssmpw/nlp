\section{Related Works}
\subsection{Pretrained Vision-Language Models}

Pretrained vision-language models have emerged as a significant research direction in computer vision and natural language processing, achieving remarkable progress in recent years.
By jointly pre-training on large-scale image and text datasets, these models demonstrate outstanding performance across various downstream tasks, including image caption~\cite{mokady2021clipcap, stefanini2022show, hessel2021clipscore, vinyals2015show}, visual question answering~\cite{pan2023retrieving, song2022clip, antol2015vqa}, and cross-modal retrieval~\cite{xia2023clip, zhen2019deep}.
These models aim to improve multimodal data comprehension by capturing semantic associations between images and text. Common pretraining methods include contrastive learning and masked language modelling.
Contrastive learning methods, such as CLIP~\cite{radford2021learning} and ALIGN~\cite{jia2021scaling}, enable models to effectively understand cross-modal information by maximizing the similarity between correct image-text pairs while minimizing the similarity between incorrect pairs. 
Masked language model-based methods, such as VisualBERT~\cite{li2019visualbert} and VilBERT~\cite{lu2019vilbert}, enhance text understanding by incorporating visual information or generating richer semantic representations.
Some emerging methods, such as ALBEF~\cite{li2021align} and BLIP~\cite{li2022blip}, introduce innovations in pretraining strategies. ALBEF enhances multimodal representation learning by aligning visual and linguistic features and fusing them. Conversely, BLIP shows great potential by guiding the learning process and progressively enhancing the model’s performance on vision and language tasks.
As a pre-trained model with a simple structure yet powerful performance, CLIP has been widely utilized as a foundational model for numerous multimodal tasks~\cite{xia2023clip, song2022clip, mokady2021clipcap}.
However, only a few studies utilize CLIP’s cross-modal understanding ability to assist VQA~\cite{xing2024clipvqa}. In this paper, we explore using CLIP as a bridge to connect text and vision for VQA.


\subsection{Video Quality Assessment}

The primary goal of VQA is to accurately predict the perceived quality from the perspective of human viewers. It plays a vital role in meeting the promised quality of service (QoS) and in improving the end user’s quality of experience (QoE)~\cite{chikkerur2011objective}.
Numerous studies have leveraged hand-crafted features to capture quality perception features in videos~\cite{Saad_Bovik_Charrier_2014, Mittal_Saad_Bovik_2016, Korhonen_2019, Tu_Wang_Birkbeck_Adsumilli_Bovik_2021}. 
However, with the widespread adoption of deep learning and the emergence of large-scale VQA datasets~\cite{hosu2017konstanz, Wang_Inguva_Adsumilli_2019, Ying_Maniratnam_Ghadiyaram_Bovik_2020, Zhang_Wu_Sun_Tu_Lu_Min_Chen_Zhai_2023}, researchers have been increasingly drawn to its powerful capabilities and flexibility, resulting in a gradual shift in research focus towards deep learning-based VQA.
% abrupt scene transitions, 
Some of these studies begin with low-level technical quality issues in videos, such as motion blur, flicker, and artefacts, and design a series of models to capture quality-aware features~\cite{Wu_Chen_Liao_Hou_Sun_Yan_Lin_2022, zhao2023zoom, Ying_Maniratnam_Ghadiyaram_Bovik_2020, Zhang_Wu_Sun_Tu_Lu_Min_Chen_Zhai_2023, Li_Jiang_Jiang_2019, Li_Jiang_Jiang_2021}.
DisCoVQA~\cite{Wu_Chen_Liao_Hou_Sun_Yan_Lin_2022} leverages the powerful temporal modelling capabilities of transformer architecture to differentiate temporal variations better and capture temporal distortions.
Zoom-VQA~\cite{zhao2023zoom} proposes a hierarchical structure to capture video quality issues at the patch, frame, and clip levels. 
Some other studies focus on higher-level abstract quality issues, such as emotional value, aesthetic and logicality, by guiding different types of encoders to capture abstract features in videos~\cite{wu2022disentangling, wu2023exploring, he2024cover, wu2023towards}. 
Dover~\cite{wu2022disentangling} employs disentangled representation learning to distinguish technical and aesthetic features in video representations. It then designs two distinct evaluators to assess aesthetic and technical quality.
However, the industry faces more complex quality problems, such as watermarking, video coherence, and image-text consistency, which cannot be easily classified under the paradigms studied in academia.


\subsection{AI-Enhanced Video Editing}

The rapid development of generative artificial intelligence technology~\cite{achiam2023gpt, ho2020denoising, touvron2023llama}, particularly in image~\cite{ramesh2021zero} and video~\cite{videoworldsimulators2024} generation, enables models to generate specific content guided by natural language.
In the field of video creation, numerous recent studies have incorporated natural language into the instructions of video editing tools. This allows AI-driven tools to automatically create and edit parts of the video content based on natural language~\cite{fried2019text, huber2019b, huh2023avscript, pavel2020rescribe, laput2013pixeltone, chang2019design, lin2023identifying, wang2024lave}.
LAVE~\cite{wang2024lave} designs a video editing agent based on Large Language Models (LLMs), incorporating a series of AI-enhanced functions for video editing. This agent can automatically create stories, retrieve relevant shots, and edit timelines according to user commands.
The proliferation of similar editing tools has resulted in a significant increase in videos co-created by humans and AI, which are now prevalent in video retrieval systems.
Although some researchers have recently specialized in assessing the quality of AI-generated videos~\cite{carreira2017quo, unterthiner2019fvd, huang2024vbench, kou2024subjective, lu2024aigc}, their primary focus has been on end-to-end text-to-video generation models, rather than the numerous videos co-created by humans and AI found in industrial video retrieval systems.