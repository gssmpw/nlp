\documentclass[twoside,11pt]{article}
\usepackage{subcaption}
\usepackage[preprint]{jmlr2e-arxiv}
\usepackage[utf8]{inputenc}
\usepackage{dsfont}
\usepackage{natbib}
\usepackage{graphicx,graphics}
\usepackage{wrapfig}
\usepackage{amsmath}
\usepackage{enumitem}
\usepackage{booktabs} 
\usepackage{nicefrac} 
\usepackage{yfonts}
\usepackage[euler]{textgreek}
\usepackage[normalem]{ulem}
\usepackage{listings}
\usepackage{MnSymbol}
\usepackage{silence}
\usepackage[mathscr]{eucal}
\WarningFilter{caption}{Unknown document class (or package)}

%\usepackage{caption}
%\captionsetup[figure]{font=small} % Makes caption font smaller for all figures

\makeatletter
\def\thm@space@setup{%
  \thm@preskip=\parskip \thm@postskip=0pt
}
\makeatother

\usepackage[linesnumbered, ruled, vlined]{algorithm2e}%
\usepackage{algpseudocode}

\def\P{\mathbb{P}}
\def\E{\mathbb{E}}
\def\R{\mathbb{R}}
\def\isim{\overset{\mathrm{iid}}{\sim}}
\def\one{\mathds{1}}
\def\hmu{\hat{\theta}}
\def\S{\mathbb{S}}
\def\A{\mathbb{A}}
\def\P{\mathbb{P}}
\def\g{\gamma}
\def\tcr{\textcolor{red}}
\newcommand{\argmin}{\mathop{\mathrm{argmin}}}
\newcommand{\argmax}{\mathop{\mathrm{argmax}}}
\def\dt{{\Delta t}}

\usepackage{xcolor}
\setlength {\marginparwidth }{2cm}
\usepackage[textsize=tiny]{todonotes}
\newcommand{\xwd}[2][]{\xspace\todo[size=\scriptsize,color=red!20!white,#1]{Xiaowu: #2}\xspace}
\newcommand{\cg}[2][]{\xspace\todo[size=\scriptsize,color=blue!20!white,#1]{cg: #2}\xspace}
\newcommand{\lyt}[2][]{\xspace\todo[size=\scriptsize,color=brown!20!white,#1]{lyt: #2}\xspace}

\newcounter{counter}[section]
\renewcommand{\thecounter}{\thesection.\arabic{counter}}

\newcommand{\change}[1]{{\leavevmode\color{red}{#1}}}



\newtheorem{defn}{Definition}
\newtheorem{conj}{Conjecture}
\newtheorem{exmp}{Example}
\newtheorem{case}{Case}
\newtheorem{condition}{Condition}
\newtheorem{note}{Note}
%\setlength{\parindent}{0pt}
%\setlength{\parskip}{1em}
\newcommand{\coef}[1]{a^{(#1)}}

\begin{document}

\title{Variance Reduction via Resampling and Experience Replay}

\author{\name Jiale Han \email jialehan@ucla.edu\\
\addr Department of Statistics and Data Science\\
\addr University of California, Los Angeles
\AND
\name Xiaowu Dai
\email daix@ucla.edu\\
\addr Departments of Statistics and Data Science and of Biostatistics\\
\addr University of California, Los Angeles
\AND
\name Yuhua Zhu \email yuhuazhu@ucla.edu\\
\addr Department of Statistics and Data Science\\
\addr University of California, Los Angeles}

\maketitle
\renewcommand{\thefootnote}{\fnsymbol{footnote}}
%\footnotetext [1] {\textit{Address for correspondence:} Xiaowu Dai, Department of Statistics and Data Science, University of California, Los Angeles, 8125 Math Sciences Bldg \#951554,  Los Angeles, CA 90095, USA. Email: dai@stat.ucla.edu.}

\begin{abstract}
Experience replay is a foundational technique in reinforcement learning that enhances learning stability by storing past experiences in a replay buffer and reusing them during training. Despite its practical success, its theoretical properties remain underexplored.
%, hindering its effective application without extensive empirical tuning
In this paper, we present a theoretical framework that models experience replay using resampled $U$- and $V$-statistics, providing rigorous variance reduction guarantees. We apply this framework to policy evaluation tasks using the Least-Squares Temporal Difference (LSTD) algorithm and a Partial Differential Equation (PDE)-based model-free algorithm, demonstrating significant improvements in stability and efficiency, particularly in data-scarce scenarios. Beyond policy evaluation, we extend the framework to kernel ridge regression, showing that the experience replay-based method reduces the computational cost from the traditional $O(n^3)$ in time to as low as $O(n^2)$ in time while simultaneously reducing variance. Extensive numerical experiments validate our theoretical findings, demonstrating the broad applicability and effectiveness of experience replay in diverse machine learning tasks.
\end{abstract}

\begin{keywords}Experience replay; Policy evaluation; Reproducing kernel Hilbert space; Variance reduction; Subsamping.
\end{keywords}

%\change{[XD: If we submit it to JMLR--AE: Xiaotong Shen, Jie Peng, Ji Zhu; Reviewers: Wenxin Zhou, Chengchun Shi, Hengrui Cai.]}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
Experience replay is widely recognized for enhancing learning stability by storing past experiences in a memory buffer and reusing them during training \citep{lin1992self, mnih2015human}. Rather than processing each experience only once, experience replay randomly samples batches of experiences to update learning targets, increasing sample efficiency and improving model performance. This approach has become a key component in modern reinforcement learning, driving breakthroughs in applications such as Atari games \citep{mnih2015human} and AlphaGo \citep{silver2016mastering}.
However, despite its widespread success, the theoretical understanding of experience replay remains limited, often requiring extensive trial and error for effective application. To address this gap, we propose a theoretical framework that connects experience replay to resampled $U$- and $V$- statistics \citep{frees1989infinite, shieh1994infinite}. This framework establishes rigorous variance reduction guarantees, providing a deeper understanding of how experience replay enhances learning stability.

Building on prior work on $U$- and $V$- statistics  \citep{zhou2021v, peng2019asymptotic}, which primarily focused on decision-tree-based methods like random forests, we extend this framework to encompass a broader class of learning functions. By leveraging the central limit theorem and the delta method, we derive the asymptotic variance of learned estimators, demonstrating that estimators employing experience replay achieve asymptotically lower variance compared to their original methods. Furthermore, we refine existing theoretical results by removing redundant assumptions, offering a more general foundation. To validate our framework, we analyze variance reduction through experience replay in two important machine-learning problems: policy evaluation in reinforcement learning and supervised learning in reproducing kernel Hilbert space (RKHS). 

\begin{figure}[htbp]
    \centering
    \begin{subfigure}{0.35\textwidth}
        \includegraphics[width=\textwidth]{fig/lstd.png} 
        \caption{LSTD approach.}
        \label{fig:figure1}
    \end{subfigure}
        \begin{subfigure}{0.35\textwidth}
        \includegraphics[width=\textwidth]{fig/2nd.png} 
        \caption{PDE-based approach.}
        \label{fig:figure3}
    \end{subfigure}
    \caption{Variance reduction achieved by experience replay in policy evaluation using two approaches. Figure \ref{fig:figure1} illustrates results for the LSTD method in discrete-time Markov Decision Processes (MDPs). Figure \ref{fig:figure3} shows results for the PDE-based approach in continuous-time state dynamics. 
    \textcolor{black}{$U$- and $V$- statistics methods incorporate experience replay without and with replacement, respectively, into the original method.}
    The solid lines represent the mean estimates, and the shaded areas denote the 95\% confidence intervals, calculated from 50 data replications.}
    \label{fig:three_figures}
\end{figure}

Policy evaluation is a critical component of reinforcement learning, where the goal is to estimate the value function representing the expected cumulative reward under a given policy. Accurate and stable policy evaluation significantly impacts the overall performance of reinforcement learning algorithms.  We first demonstrate the effectiveness of experience replay in the Least-Squares Temporal Difference (LSTD) algorithm for Markov Decision Processes (MDPs). LSTD is a data-efficient policy evaluation method that approximates value functions within a linear space by solving a least-squares problem derived from the Bellman equation \citep{bradtke1996linear}. Unlike the traditional Temporal Difference algorithm \citep{sutton2018reinforcement}, which relies on gradient descent and iterative updates, LSTD directly computes the fixed point of the value function, offering improved computational efficiency and analytical simplicity.
Incorporating experience replay into LSTD further enhances its stability and reduces variance, as illustrated in Figure \ref{fig:figure1}. Rather than using the entire dataset at once, the experience replay method resamples subsets, either with or without replacement, from the original dataset. These subsets are used to generate predictions, which are then averaged using resampled $U$- or $V$-statistics to produce the final prediction. This resampling approach enables data duplication, mitigates variability in predictions due to changes in the dataset, and enhances stability through the averaging process. The practical benefits of this method lie in its ability to improve both the accuracy and stability of policy evaluation.


We next study the PDE-based approach for policy evaluation, which is tailored for environments with continuous-time state dynamics \citep{zhu2024phibe}, where the state variable evolves continuously over time. This scenario is common in applications such as autonomous driving \citep{kong2015kinematic} and robotics \citep{kober2013reinforcement}, where discrete-time models like MDPs may fail to capture the complexity of the environment. Unlike LSTD, which relies on the Bellman equation and is designed for discrete-time settings, the PDE-based method employs a partial differential equation framework to approximate the continuous-time value function. This makes it suited for high-precision applications where the state transitions are governed by stochastic differential equations. The PDE-based method accounts for both first- and second-order approximations to the dynamics, and we demonstrate that both cases can be reformulated using resampled $U$- or $V$-statistics. Our results show that incorporating experience replay into the PDE-based approach significantly reduces the variance of value estimates, as illustrated in Figure \ref{fig:figure3}.
\textcolor{black}{Compared to the LSTD method, incorporating experience replay in the PDE-based approach results in a greater percentage reduction in variance.}
This improvement is particularly important in practice, as the numerical results in \citet{zhu2024phibe} indicate that the PDE-based solution can exhibit substantial instability. By reducing variance, experience replay ensures more stable and reliable policy evaluation in continuous-time reinforcement learning.


Besides reinforcement learning, we apply our framework to supervised learning tasks using kernel ridge regression, where each regression sample is treated as an experience. Kernel ridge regression enhances modeling flexibility by leveraging reproducing kernel methods to map data into RKHS \citep{wahba1990spline, shawe2004kernel}. Unlike existing divide-and-conquer strategies that partition datasets into disjoint subsets to reduce computational costs \citep{zhang2013divide}, our approach employs experience replay to repeatedly draw random subsamples, providing a novel strategy to improve computational efficiency.
With appropriate parameter choices, our method achieves both computational cost savings and significant variance reduction. For example, choosing a subsample size of $k=O(n^{1/8})$, the number of subsamples $B=O(n^{13/8})$, 
where $n$ is the sample size, reduces the computational cost of kernel ridge regression from the traditional  $O(n^3)$ to  $O(n^{2})$. At the same time, our theoretical results guarantee that the variance of the predictions is lower than that of the original kernel ridge regression method.
Hence, incorporating experience replay leads to more stable and faster predictions in supervised learning tasks.

We validate the effectiveness of our proposed framework through extensive experiments. Specifically, we apply experience replay to policy evaluation problems using LSTD and PDE-based algorithms. Furthermore, we integrate experience replay into kernel ridge regression for analyses on both simulated and real datasets.
The results consistently demonstrate that experience replay significantly reduces the variance of predictions compared to methods without it, demonstrating its effectiveness in enhancing stability across reinforcement learning and supervised learning tasks. Additionally, it reduces the computational cost in kernel ridge regression with an appropriate choice of parameters. Notably, experience replay generally leads to both reduced variance and lower mean squared error in predictions.


The rest of the paper is organized as follows. Section \ref{sec:background} introduces the background of experience replay and its connection to resampled $U$- or $V$-statistics. Section \ref{sec:main} defines the resampled estimators, establishes their variance reduction guarantees, and discusses applications in policy evaluation and supervised learning tasks. Section \ref{sec:experiments} presents numerical experiments to validate the theoretical findings. Section \ref{sec:conclusion} concludes the paper with potential future directions.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Background}\label{sec:background}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Experience Replay}
Experience replay stores past data in a replay buffer, denoted as $\mathcal{D}_n = \{Z_1, \dots, Z_n\}$, where $n$ represents the sample size, commonly referred to as the replay capacity in the context of experience replay \citep{lin1992self}. The replay ratio $B\geq 1$ denotes the number of batches sampled from the buffer during each update step. At each update step, we sample $B$ subsets of data points, $\{b_1, \dots, b_B\}$, where each subset $b_i$ $ ( i=1,\dots, B)$ contains $k\leq n$ data points. 
The learning method is represented by a function $h_k$, which takes $k$ data points as input. The response with experience replay is then computed as the average over these $B$ subsets:
\begin{equation}\label{replay 1}
       \frac{1}{B} \sum_{i} h_k(b_i). 
\end{equation}
In standard experience replay, such as in Q-learning, each data point in $\mathcal{D}_n$ corresponds to a single transition, and $k=1$ \citep{fedus2020revisiting}. 
In practice, uniform sampling is the most common strategy for selecting data from the replay buffer, although there also exist more time-consuming sampling methods \citep{zhang2017deeper,schaul2015prioritized}.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Connection to Resampled Statistics}
To analyze the properties of experience replay \eqref{replay 1}, we consider a scenario where the replay buffer $\mathcal{D}_n$ contains $n$ i.i.d. observations drawn from an underlying distribution $F_Z$ over the space $\mathcal{Z}$.  Both $B$ and $k$ depend on $n$, and we allow $k$ to increase with $n$. This ensures that the function $h_k$ can leverage more information as the data size grows.


When the sampling strategy is uniform sampling \emph{without} replacement, the computation in \eqref{replay 1} takes the form of an incomplete, infinite order (or \emph{resampled}) $U$-statistics \citep{frees1989infinite, zhou2021v}, defined as:
\begin{equation}\label{3}
    U_{n,k,B} = \frac{1}{B}\sum_{i}h_k(Z_{i_1},\dots, Z_{i_{k}}).
\end{equation}
where infinite order  means that $k$ and $B$ depend on the value of $n$, and $\{Z_{i_1},\dots, Z_{i_{k}}\}$ are drawn without replacement from $\{Z_1,\dots, Z_n\}$. 
In contrast, with uniform sampling \emph{with} replacement, the computation in \eqref{replay 1} follows the form of an incomplete, infinite order (or \emph{resampled}) $V$-statistics \citep{shieh1994infinite, zhou2021v}, given by: 
\begin{equation}\label{5}
    V_{n,k,B} = \frac{1}{B}\sum_{i}h_k(Z_{i_1},\dots, Z_{i_{k}}),
\end{equation}
where $k$ and $B$ again depend on $n$, and the $B$ subsets are drawn with replacement from all size-$k$ permutations of $\{1, \dots , n\}$.
We denote  the set of all size-$k$ subsets of $\{1,\dots, n\}$ as $\mathcal{S}_{n,k}^{U}$ and the  set of all size-$k$ permutations (with replacement) as  $\mathcal{S}_{n,k}^{V}$. Thus, $|\mathcal{S}_{n,k}^{U}|=\binom{n}{k}$ and $|\mathcal{S}_{n,k}^{V}|=n^{k}$.  The resampled $U_{}$-statistics is constructed by randomly selecting $B$ subsets with replacement from $\mathcal{S}^{U}_{n,k}$, while resampled $V_{}$-statistics is constructed by randomly selecting $B$ subsets with replacement from $ \mathcal{S}^{V}_{n,k}$.

Under appropriate regularity conditions, both resampled $U$-statistics and $V$-statistics are asymptotically normal \citep{mentch2016quantifying,zhou2021v}.
The variance of these statistics can be expressed as a linear combination of $ \frac{k^2}{n} \zeta_{1,k}$ and $ \frac{1}{B}\zeta_{k,k}$.
For a given $c$, $1\leq c\leq k$, the variance components $\zeta_{c,k}$ are defined as:
 \begin{equation}\label{eq7}
\zeta_{c,k}=\text{Cov}\Big(h_k(Z_1,\dots,Z_{k}), h_k(Z_1,\dots, Z_c,Z_{c+1}^{'},\dots,Z_{k}^{'})\Big),
\end{equation}
where $Z_{c+1}^{'},\dots,Z_{k}^{'}$ are i.i.d. copies from  $F_Z$, independent of the original data set $\mathcal{D}_n$.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Learning Target}
In this paper, we focus on a learning target defined as,
\begin{equation}
\label{eqn:defoftheta}
    \theta = \big[\E[g(Z)]\big]^{-1}\big[\E[f(Z)]\big] \in\R^q,
\end{equation}
where $g(\cdot): \mathcal{Z} \to \R^{q \times q}$ is a function returning an invertible matrix, and $f(\cdot): \mathcal{Z} \to \R^q$ is a function returning a vector.
This target $\theta$ arises in various machine learning applications, including policy evaluation algorithms in reinforcement learning \citep{bradtke1996linear, zhu2024phibe}, and supervised learning with kernel ridge regression \citep{wahba1990spline, rahimi2007random}. We will discuss the application of experience replay to each of these methods in Section \ref{sec:app_eg}.

To estimate $\theta$ in \eqref{eqn:defoftheta}, we use a function $h_k$ based on $k\leq n$ data points $Z_{1}^*,Z_{2}^*, \dots,Z_{{k}}^*$ for any $Z_{i}^*\in\mathcal{D}_n$, $i=1,\dots, k$, where $h_k$ in \eqref{replay 1} is defined as
\begin{equation}\label{new_hkn}
    h_{k}(Z_{1}^*,\dots,Z_{{k}}^*):=
    \Big[\sum\limits_{i=1}^{k}g(Z_{i}^*) \Big]^{-1}
     \Big[\sum\limits_{i=1}^{k}f(Z_{i}^*)\Big]\in\R^q.
\end{equation}
We will show theoretically that incorporating the experience replay approach \eqref{replay 1} reduces variance and improves the stability of estimating $\theta$.






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Main Results}\label{sec:main}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Estimators without Experience Replay}
\label{sec_3.1}
When the experience replay approach is not used, and each data point in the replay buffer $\mathcal{D}_n$ is used only once, a plug-in estimator for $\theta$ in \eqref{eqn:defoftheta} is:
\begin{equation}\label{new_9}
    \tilde{\theta}_n :=\Big[\sum_{i=1}^n g(Z_i)\Big]^{-1}\Big[\sum_{i=1}^n f(Z_i)\Big].
\end{equation}
The asymptotic property of  $\tilde{\theta}_n$ is described in the following lemma. 
\begin{lemma}\label{new_T1}
    Let $Z_1,Z_2,\ldots,Z_n\stackrel{iid}{\sim}F_Z$ and $\tilde{\theta}_n$ defined in \eqref{new_9}, we have that 
\begin{equation*}
        \sqrt{n}\left[ \tilde{\theta}_n-\theta\right]\xrightarrow{d} N(0,\Sigma_{}),
\end{equation*}
where 
\begin{equation}\label{n_Sigma}
    \Sigma = G\begin{pmatrix}

    \text{Var}(f(Z)) &  \text{Cov}(f(Z),\text{vec}( g(Z))) \\
     \text{Cov}(f(Z),\text{vec}( g(Z))) & \text{Var}(\text{vec}( g(Z))

    \end{pmatrix}G^\top,
\end{equation}
and 
\begin{equation*}
    G=\left([\E[g(Z)]]^{-1},-\theta^\top\otimes[\E[g(Z)]]^{-1}  \right),
\end{equation*}
with $\otimes$ denoting the Kronecker product and $\text{vec}(A)$  reshaping a matrix $A$ into a column vector by stacking its columns sequentially.
\end{lemma}
\noindent
The proof is provided in Appendix \ref{A1.0} and relies on the central limit theorem and the delta method. Lemma \ref{new_T1}  shows that  $ \tilde{\theta}_n$ is an asymptotically unbiased estimator of $\theta$. 


\begin{corollary}\label{new_c1}
As a direct result of Lemma \ref{new_T1},  note that $h_{k}(Z_{1},\dots,Z_{{k}}) = \tilde{\theta}_k $,  where $h_k$ is defined in
\eqref{new_hkn}. Since $\zeta_{k,k}$ defined in \eqref{eq7} represents the variance of $h_{k}(Z_{1},\dots,Z_{{k}})$, 
we have $\zeta_{k,k}=\Sigma/k+o(\Sigma/k)$, where $\Sigma_{}$ is defined in \eqref{n_Sigma}.
\end{corollary}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Estimators with Experience Replay}
\label{sec:estexpreplay}
Using the experience replay approach, we propose two new estimators for $\theta$ that leverage resampling methods based on $U$- and $V$-statistics.  These estimators are constructed using the learning method $h_k$ defined in \eqref{new_hkn},
\begin{equation}\label{new_mu_u}
    \hat{\theta}_U:=U_{n,k,B}=\frac{1}{B}\sum_{i}h_{k}(Z_{i_1},\dots,Z_{i_{k}}),
\end{equation}
\begin{equation}\label{new_mu_v}
    \hat{\theta}_V:=V_{n,k,B}=\frac{1}{B}\sum_{i}h_{k}(Z_{i_1},\dots,Z_{i_{k}}),
\end{equation}
where $U_{n,k,B}$ and $V_{n,k,B}$ are resampled $U$- and $V$-statistics defined in \eqref{3} and \eqref{5}, respectively. Algorithm \ref{alg:method} outlines the procedure for computing these estimators.


\begin{algorithm}[t!]
\caption{ \normalsize{Estimating $\theta$ via Different Methods}}
\begin{algorithmic}[1]
\State  \normalsize{\textbf{Input:} Replay buffer $\mathcal{D}_n=\{Z_1,\dots,Z_n\}$; Functions $f$ and $g$;   Replay ratio (number 

of subsamples) $B$; Subsample size $k$. }
\State  \normalsize{\textbf{Original Estimator:} Compute $\tilde{\theta}_n$ using \eqref{new_9}. }
\State \textbf{Resampled Estimators Based on $U (V)$-statistics:}
\State \textbf{for} $i=1$ to $B$ \textbf{do}
\State \quad Randomly drawn $k$ samples $\{Z_{i_1},\dots, Z_{i_k}\}$ without (for $U$-statistics) or with replacement (for $V$-statistics).
\State \textbf{end for}
\State   Compute $\hat{\theta}_U$ or $\hat{\theta}_V$ using   \eqref{new_mu_u} or  \eqref{new_mu_v}, respectively. 
% \State  \textbf{V-statistics Method:} Get the estimation $\hat{\theta}_V$ by \eqref{new_mu_v} with $h_k$ defined in \eqref{new_hkn}. 
\State \textbf{Output:} Estimators $\tilde{\theta}$, $\hat{\theta}_U$, and $\hat{\theta}_V$.
\end{algorithmic}
\label{alg:method}
\end{algorithm}

The following theorem establishes that the $U$-statistics-based estimators achieve lower variances than the original estimator under general conditions. 

\begin{theorem}[Variance Reduction for $U$-Statistics]
\label{new_u_incomplete}
Let $Z_1,Z_2,\ldots,Z_n\stackrel{iid}{\sim}F_Z$, and define $ \hat{\theta}_U$ as in \eqref{new_mu_u} and $\tilde{\theta}_n$  as in \eqref{new_9}. 
Under the assumption that $\lim_{n\to \infty}\frac{1}{n}\zeta_{k,k}[\zeta_{1,k}]^{-1}\to 0$ and $\lim_{n\to \infty}n/(Bk)\to 0$,
we have   
\begin{equation*}
    \lim_{n\to \infty}[\text{Var}( \tilde{\theta}_n) - \text{Var}(\hat{\theta}_U)] > 0.
\end{equation*}
\end{theorem}
\noindent
The proof is provided in Appendix \ref{A3.0}. The assumption $\lim_{n \to \infty}\frac{1}{n}\zeta_{k,k}[\zeta_{1,k}]^{-1}\to 0$ used in \cite{peng2019asymptotic}, ensures the asymptotic normality of the resampled $U$-statistics.  As noted in \cite{peng2019asymptotic}, this condition is typically satisfied if  $\frac{1}{k}\zeta_{k,k}[\zeta_{1,k}]^{-1}$ remains bounded, with $k = o(n)$ being sufficient.
Additionally, the theorem requires $n/Bk$ to be small, which can be achieved by selecting a large replay ratio $B$. 


To analyze the variance reduction for $V$-statistics-based estimators, we define the following class of functions: 
\begin{equation*}
    \mathcal{H}=\left\{h_k: \sup\limits_{k}||\E[h_{k}(Z_{i_1},\dots,Z_{i_{k}})h_{k}(Z_{i_1},\dots,Z_{i_{k}})^\top]||_{\infty}<\infty\right\},
\end{equation*}
where $(i_1,\dots, i_{k})$ are indices selected with replacement from  $\{1,\dots, k\}$.  
This condition, used in \cite{zhou2021v}, ensures the boundedness of the expected outer product of $h_k$. 

\begin{theorem}[Variance Reduction for $V$-Statistics]
\label{the_2}
Let $Z_1,Z_2,\ldots,Z_n\stackrel{iid}{\sim}F_Z$, and define $ \hat{\theta}_V$ as in \eqref{new_mu_v} and $\tilde{\theta}_n$ as in \eqref{new_9}, with $h_{k}\in\mathcal{H}$.
Under the assumptions  $k=o(n^{1/4})$, $\lim_{n\to\infty} k^2\zeta_{1,k}>0$, and $\lim_{n\to \infty}n/(Bk)\to 0$, 
we have   
  \begin{equation*}
    \lim_{n\to \infty}[\text{Var}( \tilde{\theta}_n) - \text{Var}(\hat{\theta}_V)] > 0.
  \end{equation*}
\end{theorem}
\noindent
The proof is provided in Appendix \ref{A4.0}. The condition $\lim_{n \to \infty} k^2\zeta_{1,k} > 0$ has also been used in \cite{song2019approximating} and \cite{zhou2021v}, and is satisfied by many base learners. 


Theorems \ref{new_u_incomplete} and \ref{the_2} show that incorporating experience replay via resampled $U$- and $V$-statistics reduces variance compared to the original estimator, enhancing the stability and efficiency of parameter estimation.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Applications to Machine Learning Tasks}\label{sec:app_eg}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Policy Evaluation for Markov Decision Process}
%\tcr{\text{YZ: replace $j\dt$ to $j$}}\textcolor{blue}{(I have replaced them.)}

Consider a Markov Decision Process (MDP) defined by the tuple $(\S, \A, \g, r, \P)$ \citep{sutton2018reinforcement}. Here $s\in\mathbb{S}$ denotes the state space, $a\in\A$ represents the action space, $\gamma\in(0,1)$ is a given discounted factor, $r_*: \S\times \A \to \R$ is the reward function, and $\P : \S\times\A \to \Delta (\S)$ denotes the probability distribution of the next state given the current state and action. The goal of MDP is to find the optimal policy $\pi^*(s)$ that maximizes the value function. Here the policy is a mapping from the state space $\S$ to action space $\A$, while the value function ${V}_{*}^\pi(s)$ measures the expected cumulative reward of an agent over the long run, defined as:
\begin{equation}\label{def of Vpi}
    {V}_{*}^\pi(s) =  \E\left[\sum_{j =  0 }^\infty \gamma^j {r}_{*}^\pi(s_{j }) |s_0 = s\right],
\end{equation}
%\tcr{why adding a * here?}
where $s_0 = s$ is the initial state, ${r}_{*}^\pi(s) = r(s,\pi(s))$ is a known reward function under the current policy, and the state at time step $j+1$ follows the transition distribution under the policy $\pi$,  $s_{j+1}\sim P^\pi(\cdot|s_j) = P(\cdot|s_j,\pi(s_j))$. In reinforcement learning (RL), one usually divides the RL problem into two parts, one is policy evaluation, which is given a policy $\pi(s)$, calculates the value function ${V}_{*}^\pi(s)$; Another is policy improvement, that improves the policy according to gradient ascent or policy iteration.

The focus of this paper is policy evaluation, which is one of the most fundamental RL problems. In the setting of RL, one does not have the access to the transition distribution. Instead, the agent applies an action $a_j = \pi(s_j)$ according to the policy at each time step $ j $, and observes the next step $ s_{j+1} $, receives a numerical reward $ r^{\pi}_{*}(s_{j+1}) $. Due to the finite length of the trajectory data, it is usually impossible to compute the value function directly according to the cumulative sum \eqref{def of Vpi}. Note that the value function ${V}^{\pi}_{*}(s)$ also satisfies the following Bellman equation (BE), 
 \begin{equation}\label{BE}
      {V}_{*}^\pi(s) = {r}_{*}^\pi(s) +  \gamma\E_{s_{j+1} \sim P^\pi(s'|s_0)}[{V}_{*}^\pi(s_{j+1})|s_0 = s].
\end{equation}
Therefore, the goal of policy evaluation problem is to find the value function solves BE \eqref{BE} given a set of trajectory data,
\begin{equation}\label{D_n in rl}
    \mathcal{D}_n=\{(s^l_0,s^l_{1}, \dots, s^l_{L}) \}_{l=1}^n.
\end{equation}
Here the data set contains $n$ independent trajectories and each contains $L$ data points.
% , \textcolor{blue}{and $r^l_j=r^{\pi}_{*}(s^l_j)$ for $l=1,\dots, n$ and $j=0,\dots,L$}. 
The initial state $s^l_0$ of each trajectory is sampled from a distribution $\rho^{\pi}_0(s)$. %\tcr{YZ: either add a comment and omit $\pi$ or add $\pi$ to $V, r, \rho$} \textcolor{blue}{(I added $\pi$ to them)}

The Least square temporal difference (LSTD) \citep{bradtke1996linear} is a popular RL algorithm for linear approximation and can be directly used to estimate ${V}_{*}^\pi(s)$ using the trajectory data. 
LSTD approximates the value function ${V}_{*}^\pi(s) = \Phi(s)^\top \theta$ in the space expanded by $q$ given bases $\{\phi_i(s)\}_{i=1}^q$, where $\theta\in\R^q$ is a unknown parameter and $\Phi(s) = (\phi_1(s), \cdots, \phi_q(s))^\top$.
% and  we hope it satisfies \eqref{BE}. By projecting \eqref{BE} into the finite bases, we have that 
% \begin{equation}\label{eq25}
%     \mathbb{E}_s[ ({V}_0(s) - e^{-\beta \dt}\E_{s_\dt \sim \rho(s',\dt|s)}[{V}_0(s_{\dt})|s_0 = s])\Phi(s)/\dt] = \mathbb{E}_s[r(s) \Phi(s) ],
% \end{equation}
% where the expectation is taken over the distribution of $s$.  Using \eqref{eq25}, 
By projecting the value function into the finite bases, LSTD solves the parameter $\theta$ in the form of 
\begin{equation}\label{theta_0}
    \theta=\left[\mathbb{E}_s[ \Phi(s) (\Phi(s) -\g\E[\Phi(s_1)|s_0 = s])^\top]\right]^{-1} \mathbb{E}_s[r^{\pi}_{*}(s) \Phi(s) ].
\end{equation}
% \begin{equation}\label{theta_0}
%     \theta = A^{-1}b,
% \end{equation}
% where 
% \begin{equation*}
%     A:=\mathbb{E}_s[ (\Phi(s)^ \top - e^{-\beta \dt}\E[\Phi(s_{\dt})^ \top|s_0 = s]) \Phi(s)/\dt], \quad b:=\mathbb{E}_s[r(s) \Phi(s) ].
% \end{equation*}
Using any trajectory data subset with $ k \leq n$ data points $\{(s^{l_{(1)}}_{j})_{j=0}^L, \dots,(s^{l_{(k)}}_{j})_{j=0}^L\} $ 
 for any $(s^{l_{(i)}}_{j})_{j=0}^L\in\mathcal{D}_n, i=1,\dots, k$, the estimator of $\theta$ is

\begin{equation}\label{rl form_1}
     \left[\sum_{i=1}^{k} g((s^{l_{(i)}}_{j})_{j=0}^L)\right]^{-1}\left[\sum_{i=1}^{k}f((s^{l_{(i)}}_{j})_{j=0}^L)\right],
\end{equation}
% \begin{equation}
% \begin{split}
%     \tilde{A}=\sum_{i=1}^k g((s^{l_{(i)}}_{j_\dt}, r^{l_{(i)}}_{j_\dt})_{j=0}^L),\quad
%     \tilde b =\sum_{i=1}^k f((s^{l_{(i)}}_{j_\dt}, r^{l_{(i)}}_{j_\dt})_{j=0}^L),
% \end{split}
% \end{equation}
where 
\begin{equation}\label{g and f}
\begin{split}
       g((s^{l_{(i)}}_{j})_{j=0}^L)&= \sum_{j=0}^{L-1}\Phi(s_{j}^{l_{(i)}})[\Phi(s_{j}^{l_{(i)}})-\gamma\Phi(s_{(j+1)}^{l_{(i)}})]^\top,\\
       f((s^{l_{(i)}}_{j})_{j=0}^L)&=\sum_{j = 0}^{L-1} r^{\pi}_{*}(s^{l_{(i)}}_j)\Phi(s^{l_{(i)}}_{j}).
\end{split}
\end{equation}

% From \eqref{theta_0}, the estimator of $\theta$ is then $\tilde{A}^{-1}\tilde{b}$, that is
% \begin{equation}\label{rl form_1}
%      \left[\sum_{i=1}^{k} g((s^{l_{(i)}}_{j_\dt}, r^{l_{(i)}}_{j_\dt})_{j=0}^L)\right]^{-1}\left[\sum_{i=1}^{k}f((s^{l_{(i)}}_{j_\dt}, r^{l_{(i)}}_{j_\dt})_{j=0}^L)\right],
% \end{equation}


This example fits our setting with $Z_i=(s^i_{j})_{j=0}^L, i=1,\dots, n$, $\theta$ defined in \eqref{theta_0}, and functions $g$ and $f$ defined in \eqref{g and f}.
Under this framework, equation \eqref{rl form_1} aligns with the structure of \eqref{new_hkn}.
Applying Algorithm \ref{alg:method}, once we obtain $\tilde{\theta}_n$, $\hat{\theta}_U$, and  $\hat{\theta}_V$, the corresponding estimations of the values $V^{\pi}_{*}(s)$ at a test point $s$ are 
\[\tilde{V}(s) = \Phi(s)^\top \tilde{\theta}_n,\quad \hat{V}_U(s) = \Phi(s)^\top \hat{\theta}_U,\quad  \text{and } \quad  \hat{V}_V(s) = \Phi(s)^\top \hat{\theta}_V,\] 
where the superscript $\pi$ is omitted.
The variance are, $\text{Var}(\tilde{V}(s))=\Phi(s)^\top\text{Var}(\tilde{\theta}_n)\Phi(s)$, $\text{Var}(\hat{V}_U(s))=\Phi(s)^\top\text{Var}(\hat{\theta}_U)\Phi(s)$, and $\text{Var}(\hat{V}_V(s))=\Phi(s)^\top\text{Var}(\hat{\theta}_V)\Phi(s)$. Thus the reduction of the variance of estimators of $\theta$ could be directly evaluated by the reduction in the variance of estimations.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Policy Evaluation for Continuous-Time Reinforcement Learning}

In the second application,  we aim to solve the policy evaluation problem for continuous-time RL. Given a policy $\pi(s)$, unlike the MDP where the value function is a cumulative sum over discrete time step defined as \eqref{def of Vpi}, the value function in continuous-time RL is an expected integral over continuous time,
\begin{equation}\label{def of value}
    V^{\pi}(s) = \E\left[\int_{0}^\infty e^{-\beta t}{r}^{\pi}(s_t)dt| s_0 = s\right]. 
\end{equation}
%\tcr{{we might need to differentiate the $r$ here and the $r$ in MDP? If so, you need to change it accordingly in the later text.}}\textcolor{blue}{(I have differentiated them using ${r}_{*}^{\pi}$ and $r^{\pi}$.)}
Here $\beta>0$ is a given discounted coefficient, $r^{\pi}(s)\in\R$ is a known reward function under the current policy. When the state $s_t\in\mathbb{S} \subset \R^d$ is driven by the stochastic differential equation (SDE),  
\begin{equation}\label{def of dynamics}
    d s_t = \mu(s_t)dt + \sigma(s_t)dB_t,
\end{equation} 
by Feynman–Kac theorem \cite{stroock1997multidimensional}, the value function $V(s)$ satisfies the following equation,
\begin{equation}\label{stoch_PDE}
    \beta V^{\pi}(s) = r^{\pi}(s) + \mathcal{L}_{\mu,\Sigma} V^{\pi}(s),\quad \text{where}\quad \mathcal{L}_{\mu,\Sigma} = \mu(s)\cdot\nabla + \frac{1}{2}\Sigma : \nabla^2,
\end{equation} 
with $\Sigma = \sigma \sigma^\top$, and $\Sigma : \nabla^2 = \sum_{i,j}\Sigma_{ij}\partial_{s_i}\partial_{s_j}$. Similar to the classical RL setting, one does not have access to the drift function $\mu(s)\in \R^d$ and diffusion function $\sigma(s)\in \R^{d\times d}$. Therefore, one cannot solve the above equation directly.  The goal of continuous-time policy evaluation is to find the value function satisfying \eqref{stoch_PDE} with a set of trajectory data $\mathcal{D}_n$ defined in \eqref{D_n in rl}. Here the data at time step $j$ are collected at time $j\dt$ with a given time interval $\dt$. 
% \begin{equation}\label{D_n in rl}
%     \mathcal{D}_n=\{(s^l_0,r^l_0), (s^l_{\dt}, r^l_{\dt}), \dots, (s^l_{L\dt},r^l_{L\dt}) \}_{l=1}^n,
% \end{equation}
%  \textcolor{blue}{which is similar to $D^*_n$ \eqref{D_n^* in rl}  except that the notation of the states are different and $r^l_{j\dt}=r^{\pi}(s^l_{j\dt})$ for $l=1,\dots, n$ and $j=0,\dots,L$.}
    
% \tcr{YZ: Why you need this:} Under a regular assumption, there exists a stationary distribution $\rho$ to the stochastic dynamics \citep{huang2015steady}.

There are two ways to approximate the value function defined in \eqref{def of value}, or equivalently, in \eqref{stoch_PDE}. The first way is to treat it as an MDP problem by setting 
$r_{*}^{\pi}(s) = {r}^{\pi}(s)\dt, \quad \gamma = e^{-\beta \dt},$
and then solve the BE \eqref{BE} using the data.
%\tcr{YZ: Not sure if we should introduce the approximation of MDP here or in Section 5.1?}\textcolor{blue}{(I also write the relationship in section 5.2. I think it is good to introduce it here as well.)}  
However, notice that one does not utilize the special dynamics deriven by SDE \eqref{def of dynamics} in this approximation. Another way, introduced in \cite{zhu2024phibe}, to approximate the value function is to solve a Physics-informed Bellman equation (PhiBE) defined as follows
\begin{equation}\label{def of PhiBE}
    \beta \bar V^{\pi}_{\alpha}(s) - \mathcal{L}_{
    \hat{\mu}_{\alpha},\hat{\Sigma}_{\alpha}}  \bar V^{\pi}_{\alpha}(s) = r^{\pi}(s),\quad \alpha = 1, 2
\end{equation}
where 
\begin{equation*}\label{def of higher order stoch mu}
\begin{aligned}
    &\hat \mu_{\alpha}(s) =\frac1\dt \sum_{j=1}^{\alpha}\E_{s_{j}\sim \rho(\cdot, j\dt|s)}\left[\coef{{\alpha}}_j(s_{j} - s_0)|s_0 = s\right]\\
    &\hat \Sigma_{\alpha}(s) =\frac1\dt \sum_{j=1}^{\alpha}\E_{s_{j}\sim \rho(\cdot, j\dt|s)}\left[\coef{{\alpha}}_j(s_{j} - s_0)(s_{j} - s_0)^\top|s_0 = s\right]    
\end{aligned}
\end{equation*}
where $\mathcal{L}_{ \hat{\mu}_i,\hat{\Sigma}_i}$ is defined in \eqref{stoch_PDE} and 
\begin{equation}\label{def of A b}
\alpha = 1:\ \coef{1}_1 = 1; \quad \alpha = 2:\ \coef{2}_1 = 2, \ \coef{2}_2 = -\frac12.
\end{equation}
Here $\bar{V}^{\pi}_\alpha(s)$ serves as $\alpha$-th order approximation to the continuous-time value function $V^{\pi}(s)$, where $\alpha\in\{1,2\}$. 

Similar to \eqref{theta_0}, if one approximates the solution $\bar V(s) = \Phi(s)^\top \theta$ to the PhiBE \eqref{def of PhiBE} in the linear function space spanned by $\Phi(s)$, one ends up solving for the parameter $\theta$ satisfying
\begin{equation}\label{theta_1}
    \theta=\left[\mathbb{E}_s[ (\beta \Phi(s)^ \top - \mathcal{L}_{\hat{\mu}_{\alpha},\hat{\Sigma}_{\alpha}}  \Phi(s)^ \top ) \Phi(s)]\right]^{-1}\mathbb{E}_s[r^{\pi}(s) \Phi(s) ].
\end{equation}

%In practice, we have access only to discrete-time information. That is, we cannot foresee the future dynamics \eqref{def of dynamics} of the state so we cannot calculate the true value function $V(s)$ based on \eqref{stoch_PDE}, but we have trajectory data generated by the continuous dynamics \eqref{def of dynamics} at discrete time $j\Delta t$. Suppose the trajectory data we get are in the form of $\mathcal{D}_n$ defined in \eqref{D_n in rl}.


% \subsubsection{Policy Evaluation via a PDE-Based Framework}
%In this setting, \cite{zhu2024phibe} proposed a PDE-based method that can have a good approximation of the value function.  This method also uses the Galerkin method that approximates $V(s)$ in  the space of $q$ given bases $\{\phi_i(s)\}_{i=1}^q$. We let $\bar V(s) = \Phi(s)^\top \theta$ denote such approximation where $\theta\in\R^q$ is the unknown parameter and $\Phi(s) = (\phi_1(s), \cdots, \phi_q(s))^\top$. By \eqref{stoch_PDE}, \cite{zhu2024phibe} proposed the following $\alpha$-th order PDE, \begin{equation*}    \beta \bar V_{}(s) - \mathcal{L}_{     \hat{\mu}_{\alpha},\hat{\Sigma}_{\alpha}}  \bar V_{}(s) = r(s), \end{equation*}


%By inserting the ansatz $\bar V$ into the PDE and then projecting it onto the finite bases,  we have that  \begin{equation}\label{estimate equation}   \mathbb{E}_s[ (\beta \bar V(s) - \mathcal{L}_{\hat{\mu}_{\alpha},\hat{\Sigma}_{\alpha}}  \bar V(s)) \Phi(s)] = \mathbb{E}_s[r(s) \Phi(s) ], \end{equation} where the expectation is taken over the distribution of $s$. 


% \begin{equation}\label{theta_1}
%     \theta = A^{-1}b,
% \end{equation}
% where 
% \begin{equation*}
%     A:=\mathbb{E}_s[ (\beta \Phi(s)^ \top - \mathcal{L}_{{\mu},{\Sigma}}  \Phi(s)^ \top ) \Phi(s)], \quad b:=\mathbb{E}_s[r(s) \Phi(s) ].
% \end{equation*}


\cite{zhu2024phibe} gives the model-free algorithm to estimate the $\theta$ using only trajectory data. Specifically, for the $\alpha$-th order case, using any data subset with $ k \leq n$ data points $\{(s^{l_{(1)}}_{j})_{j=0}^L, \dots,(s^{l_{(k)}}_{j})_{j=0}^L\}$ for any $(s^{l_{(i)}}_{j})_{j=0}^L\in\mathcal{D}_n,\ i=1,\dots, k$, the estimations of $\theta$ is
\begin{equation}\label{rl form}
    \left[\sum_{i=1}^{k} g((s^{l_{(i)}}_{j})_{j=0}^L)\right]^{-1}\left[\sum_{i=1}^{k}f((s^{l_{(i)}}_{j})_{j=0}^L)\right].
\end{equation}
% \begin{equation}
%     \bar A = \sum_{i=1}^{k} g((s^{l_{(i)}}_{j_\dt}, r^{l_{(i)}}_{j_\dt})_{j=0}^L), \quad \bar b = \sum_{i=1}^{k}f((s^{l_{(i)}}_{j_\dt}, r^{l_{(i)}}_{j_\dt})_{j=0}^L),
% \end{equation}
where 
\begin{equation}\label{f and g 2}
\begin{split}
       g((s^{l_{(i)}}_{j})_{j=0}^L)&= \sum_{j = 0}^{L-{\alpha}}\Phi(s^{l_{(i)}}_{j})\left[ \beta \Phi (s^{l_{(i)}}_{j})- \mathcal{L}_{
    \bar{\mu}_{\alpha},\bar{\Sigma}_{\alpha}} \Phi(s^{l_{(i)}}_{j})\right]^\top,\\
       f((s^{l_{(i)}}_{j})_{j=0}^L)&=\sum_{j = 0}^{L-{\alpha}} r^{\pi}(s^{l_{(i)}}_{j})\Phi(s^{l_{(i)}}_{j}),
\end{split}
\end{equation}
and the estimators of $\mu(s)$ and $\sigma(s)$ are defined as
\begin{equation*}\label{def of barmu}
    \begin{aligned}
    &\bar{\mu}_{\alpha}(s_{j}^l) = \frac{1}{\dt}\sum_{k=1}^{\alpha}\coef{\alpha}_k(s^l_{(j+k)} - s^l_{j}) ,\\
    &\bar{\Sigma}_{\alpha}(s_{j}^l) = \frac{1}{\dt}\sum_{k=1}^{\alpha}\coef{{\alpha}}_k(s^l_{(j+k)} - s^l_{j})(s^l_{(j+k)} - s^l_{j})^\top,
\end{aligned} 
\end{equation*}   
with $\coef{{\alpha}}$ defined as \eqref{def of A b}. 
\textcolor{black}{If $\alpha = 2$, it is the second-order PDE-based algorithm. Compared to LSTD, the second-order PDE-based algorithm incorporates two future steps, resulting in improved accuracy, as illustrated in Figure \ref{fig:three_figures}.}


% The estimator of $\theta$ is then $\bar A^{-1} \bar{b}$, that is
% \begin{equation}\label{rl form}
%     \left[\sum_{i=1}^{k} g((s^{l_{(i)}}_{j_\dt}, r^{l_{(i)}}_{j_\dt})_{j=0}^L)\right]^{-1}\left[\sum_{i=1}^{k}f((s^{l_{(i)}}_{j_\dt}, r^{l_{(i)}}_{j_\dt})_{j=0}^L)\right].
% \end{equation}

This example fits our setting with $Z_i=(s^i_{j})_{j=0}^L,\  i=1,\dots, n$ and $\theta$ defined in \eqref{theta_1}, and functions $g$ and $f$ defined in \eqref{f and g 2}. Under this framework, equation \eqref{rl form} aligns with the structure of \eqref{new_hkn}. 
Applying Algorithm \ref{alg:method}, once we obtain $\tilde{\theta}_n$, $\hat{\theta}_U$, and  $\hat{\theta}_V$, the corresponding estimations of the values $V^{\pi}(s)$ at a test point $s$ are 
\[\tilde{V}(s) = \Phi(s)^\top \tilde{\theta}_n,\quad \hat{V}_U(s) = \Phi(s)^\top \hat{\theta}_U,\quad  \text{and } \quad  \hat{V}_V(s) = \Phi(s)^\top \hat{\theta}_V,\] 
where the superscript $\pi$ is omitted.
The variance are, $\text{Var}(\tilde{V}(s))=\Phi(s)^\top\text{Var}(\tilde{\theta}_n)\Phi(s)$, $\text{Var}(\hat{V}_U(s))=\Phi(s)^\top\text{Var}(\hat{\theta}_U)\Phi(s)$, and $\text{Var}(\hat{V}_V(s))=\Phi(s)^\top\text{Var}(\hat{\theta}_V)\Phi(s)$. Thus the reduction of the variance of estimators of $\theta$ could be directly evaluated by the reduction in the variance of estimations.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Kernel Ridge Regression}
\label{sec:krr}
Consider a supervised learning framework where $\mathcal{D}_n=\{(X_1,Y_1),\dots, (X_n, Y_n)\}$ consists of i.i.d. samples drawn from a distribution $F_Z$.
Our goal is to predict the outcome \( Y \in \mathbb{R} \) based on the predictors \( X \in \mathbb{R}^p \) using kernel methods in an RKHS \citep{wahba1990spline}. 
\textcolor{black}{Let \( K(\cdot, \cdot): \mathbb{R}^p \times \mathbb{R}^p \to \mathbb{R} \) be a reproducing kernel function. We consider the model \( Y = f(X) + \epsilon \), where \( f \) belongs to the RKHS defined by \( K \), and \( \epsilon \) represents random error independent of \( X \). Following \citet{rahimi2007random} and \citet{dai2023kernel}, the kernel function \( K(X_i, X_j) \) can be approximated as \( \phi(X_i)^\top \phi(X_j) \) using a feature mapping \( \phi: \mathbb{R}^p \to \mathbb{R}^q \), and \( f(X) \) can be approximated as \( \phi(X)^\top \theta \), where \( \theta \in \mathbb{R}^q \) is a parameter vector, defined as
\begin{equation}
\label{eqn:thetakrr}
\theta = \left\{\mathbb{E}[\phi(X)\phi(X)^\top]\right\}^{-1} \mathbb{E}[\phi(X)Y].
\end{equation}}
Using any $k$ data points $\{(X_1^*, Y_1^*), \dots, (X_k^*, Y_k^*)\}$ resampled from $\mathcal{D}_n$, the kernel ridge regression estimator of $\theta$ is obtained by solving the following optimization problem for a given $\lambda\geq 0$, 
% $\argmin_{\theta\in \R^{q}} \left\{\sum_{i=1}^{k} [Y_i^*-\phi(X_i^*)^\top\theta]^2+\lambda||\theta||^2_2\right\}.$
\begin{equation*}
    \argmin_{\theta\in \R^{q}} \left\{\sum_{i=1}^{k} [Y_i^*-\phi(X_i^*)^\top\theta]^2+\lambda||\theta||^2_2\right\}.
\end{equation*}

The solution takes the form of
\begin{equation}\label{eq41}
\left[ \sum_{i=1}^k  g(X_i^*,Y_i^*)+\lambda\mathbb{I}_p\right]^{-1}\left[  \sum_{i=1}^k f(X_i^*,Y_i^*)\right],
\end{equation} 
where $g(X_i^*, Y_i^*) = \phi(X_i^*)\phi(X_i^*)^\top$ and $f(X_i^*, Y_i^*) = \phi(X_i^*) Y_i^*$.
The setup in \eqref{eqn:thetakrr} and \eqref{eq41} aligns with our framework in \eqref{eqn:defoftheta} and \eqref{new_hkn}, with an added regularization term $\lambda \mathbb{I}_p$.  This term does not impact the derivation of our main results.
% In form, once we compute $\tilde{\theta}_n$ \eqref{new_9}, $\hat{\theta}_U$ \eqref{new_mu_u}, and  $\hat{\theta}_V$ \eqref{new_mu_v},  the corresponding predictions at a test point $x$ are  
% $    \tilde{y}(x) = \phi(x)^\top\tilde{\theta}_n, \hat{y}_U(x) =\phi(x)^\top\hat{\theta}_U, $ and $  \hat{y}_V(x) = \phi(x)^\top\hat{\theta}_V.$
% The variances are
% $\text{Var}(\tilde{y}(x) )=\phi(x)^\top\text{Var}(\tilde{\theta}_n)\phi(x)$,  $\text{Var}(\hat{y}_U(x))=\phi(x)^\top\text{Var}(\hat{\theta}_U)\phi(x)$, and $\text{Var}(\hat{y}_V(x))=\phi(x)^\top\text{Var}(\hat{\theta}_V)\phi(x)$.
% Thus, the variance reduction achieved can be directly evaluated through the reduction in the variance of the predictions. 
% In practice, providing the kernel function \(K\) suffices to compute the prediction value. Specifically, \(\phi(x)^\top \theta^* = \sum_{i=1}^k c_i K(X^*_i, x)\), where \((c_1, \dots, c_k) = (\mathbf{K} + \lambda \mathbb{I}_n)^{-1} \vec{Y}^*\), \(\mathbf{K} \in \mathbb{R}^{n \times n}\) is the kernel matrix with entries \(\mathbf{K}_{i,j} = K(X_i, X_j)\), and \(\vec{Y}^* = (Y^*_1, \dots, Y^*_k)^\top\).

The standard computational cost of kernel ridge regression with $n$ data points is $ O(n^3) $ in time \citep{wahba1990spline}. 
The divide-and-conquer algorithm \citep{zhang2013divide} reduces this cost by dividing the dataset into $m<n$ disjoint subsets, each of $n/m$, and averaging the local solutions across these subsets to construct a global predictor. This approach achieves a trade-off between computational cost and estimation error. 
In contrast, our approach, which incorporates the experience replay method, also averages over subsets but differs fundamentally in how the subsets are constructed. Instead of partitioning the dataset into non-overlapping subsets, we repeatedly draw $ B $ random subsamples, each containing $k$  data points. This resampling allows for overlapping subsets and potential duplication of data points, resulting in a total computational cost of $O(Bkq^2)$ in time.
%$O(Bkq^2)$ in time when using the map introduced in \citet{rahimi2007random}.
Theorems \ref{new_u_incomplete} ensure that the conditions  $\lim_{n\to \infty}n/(Bk)\to 0$ and $k = o(n)$ are sufficient for variance reduction. By carefully choosing $B$ and $k$, our approach achieves both computational savings and variance reduction, offering a practical and efficient alternative to traditional kernel ridge regression, especially for large-scale problems. 

\begin{example}
Setting $B = O(n^{2\delta}),k=O(n^{1-\delta}),q = O\left(n^{\frac{2s}{2s+p}(1-\delta)}\log n\right)$, where $s\geq p/2$ is the smoothness parameter of the kernel $K$, and $\delta>0$ is a constant, satisfies the conditions of Theorems \ref{new_u_incomplete} and reduces the variance.
Under this setup, the computational cost is reduced to $O\left(n^{1+\frac{4s}{2s+p}}\cdot n^{\delta[1-\frac{4s}{2s+p}]}(\log n)^2\right)$ in time, offering significant savings compared to the standard $O(n^3)$ complexity of kernel ridge regression.  
Moreover, the method achieves a convergence rate  of $O\left(n^{-\frac{2s}{2s+p}(1-\delta)}\right)$ for the true function in the
RKHS  corresponding to kernel $K$ \citep[see,][]{rudi2017generalization, dai2024nonparametric}. This rate approaches the minimax optimal rate $O\left(n^{-\frac{2s}{2s+p}}\right)$ as $\delta\to 0$ \citep{wahba1990spline}.  
\end{example}

\begin{example}
Setting $B = O(n),k=O(n^{1/2}),q = O(n^{1/2})$, satisfies the conditions of Theorems \ref{new_u_incomplete} and reduces the variance. Under this setup, the computational cost is reduced to $O(n^{5/2})$ in time.     
\end{example}

\begin{example}
Setting $B= O(n^{13/8}),k = O(n^{1/8}),q = O(n^{1/8})$,  satisfies the conditions of Theorems \ref{new_u_incomplete} and reduces the variance. Under this setup, the computational cost is further reduced to $O(n^2)$ in time.   
\end{example}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Numerical Experiments}\label{sec:experiments}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Experiments of Policy Evaluation Using LSTD Algorithm}\label{exp:PE}
Firstly, we present the experimental results obtained using the LSTD with functions $g$ and $f$ defined in \eqref{g and f}.
We conduct the experiments in a similar setting as described in \cite{zhu2024phibe}, where the state space $\mathbb{S}$ is defined as $\mathbb{S} = [-\pi, \pi]$, 
\textcolor{black}{and the state under policy $\pi$ is driven by the transition distribution $P^{\pi}(s_{j+1}|s_j)$ following the normal distribution with expectation $se^{\lambda/10}$, variance $\frac{\sigma^2}{2\lambda }(e^{\lambda/5}-1)$.}
The reward function $r^{\pi}_{*}(s)$ is set to be $r^{\pi}_{*}(s) = 0.1*[\cos^3(s)-\lambda s (-3\cos^2(s)\sin(s)) - \frac12\sigma^2(6\cos(s)\sin^2(s) -3\cos^3(s))]$ and the discounted factor $\gamma$ is set to be $e^{-0.1}$. 
To implement the LSTD, we use periodic bases $\{\phi_n(s)\}_{k=1}^{2I+1} = \frac{1}{\sqrt{\pi}}\{\frac{1}{\sqrt{2}}, \cos(is), \sin(is)\}_{i=1}^I$ with $I$ large enough so that the solution can be accurately represented by these finite bases. 
We consider the case $L=2$, where each trajectory has three data points and the state $s_j^l$ in $D_n$ \eqref{D_n in rl} is drawn at time $j/10$ for $j=0,\dots, L$ and $l=1,\dots, n$.
% The transition distribution $P^{\pi}(s'|s)$ from $t$ to $t+\dt$ follows the normal distribution with expectation $se^{\lambda \dt}$, variance $\frac{\sigma^2}{2\lambda }(e^{2\lambda \dt}-1)$.  
In each simulation experiment, we draw $n$ independent trajectories $\mathcal{D}_n$ with the initial state $s_0^l$ of each trajectory sampled from a truncated normal distribution over $\mathbb{S}$ with mean $0$ and standard deviation $0.1$. Using $\mathcal{D}_n$, we can train three different prediction models $\tilde{V}(s)$, $\hat{V}_U(s)$, and $\hat{V}_V(s)$ based on the $\tilde{\theta}_n, \hat{\theta}_U$, and $\hat{\theta}_V$ getting by Algorithm \ref{alg:method} with functions $g$ and $f$. 



% We conduct a simulation under a similar setting as described in \cite{zhu2024phibe} for both the MDP and continuous-time cases, where the state space $\mathbb{S}$ is defined as $\mathbb{S} = [-\pi, \pi]$, and the state $s_t$ is driven by the Ornstein–Uhlenbeck (OU) process \textcolor{blue}{(should the OU driven by a policy $\pi$?)}, $ds(t) = \lambda sdt + \sigma dB_t$ with $\lambda = 0.05, \sigma = 1$. \textcolor{blue}{Assume the true value function is defined in \eqref{def of value}, or equivalently, in \eqref{stoch_PDE}. In this section, we approximate the value function $V^{\pi}(\cdot)$ by treating it as an MDP problem by setting 
% $r_{*}^{\pi}(s) = {r}^{\pi}(s)\dt$ and $\gamma = e^{-\beta \dt}$ in \eqref{def of Vpi},
% and solve the BE \eqref{BE} using the LSTD algorithm.}
% The reward function $r^{\pi}(s)$ is set to be $r^{\pi}(s) = \beta \cos^3(s)-\lambda s (-3\cos^2(s)\sin(s)) - \frac12\sigma^2(6\cos(s)\sin^2(s) -3\cos^3(s))$ with $\beta=1$.
% Note that the value function can be exactly obtained from \eqref{def of value}, $V(s) = \cos^3(s)$.
% When the data are collected at different times with a time interval $\dt$, the transition distribution $P^{\pi}(s'|s)$ from $t$ to $t+\dt$ follows the normal distribution with expectation $se^{\lambda \dt}$, variance $\frac{\sigma^2}{2\lambda }(e^{2\lambda \dt}-1)$. 


%  We use periodic bases $\{\phi_n(s)\}_{k=1}^{2I+1} = \frac{1}{\sqrt{\pi}}\{\frac{1}{\sqrt{2}}, \cos(is), \sin(is)\}_{i=1}^I$ with $I$ large enough so that the solution can be accurately represented by these finite bases. 
% We consider the simple case, $L=2$, where each trajectory has three data points and the time interval $\dt=0.1$.  In each simulation experiment, we draw $n$ independent trajectories $\mathcal{D}_n^*$ defined in \eqref{D_n^* in rl} with the initial state $s_0^l$ of each trajectory sampled from a truncated normal distribution over $\mathbb{S}$ with mean $0$ and standard deviation $0.1$, \textcolor{blue}{and the state $s_j$ are drawn at time $j\dt$ for $j=1,\dots,L$.}. Using $\mathcal{D}^*_n$, we can train three different prediction models $\tilde{V}(s)$, $\hat{V}_U(s)$, and $\hat{V}_V(s)$ based on the $\tilde{\theta}_n, \hat{\theta}_U$, and $\hat{\theta}_V$ getting by Algorithm \ref{alg:method} with functions $g$ and $f$. 
% %We present the simulation results obtained using the second-order PDE-based approach. We also provide the results from the first-order PDE-based approach in the Appendix.  As discussed by \cite{zhu2024phibe}, the second-order approach should be the most efficient method among these three methods.


%\tcr{YZ: one can also introduce the rescaled reward and discount factor here.}\textcolor{blue}{(I rewrite the Sections 5.1 and 5.2 and introduce those in 5.2.)}

We check the performance of the three prediction models on $m=50$ test points evenly selected in $\mathbb{S}$, denoted by $\mathcal{S}_{test}=\{s_j^{*}\}_{j=1}^{m}$ with $s_j^{*}=-\pi+2(j-1)*\pi/(m-1))$. 
We conduct the experiment $M=50$ times and calculated the variance of the predicted values for each test data point across the three different prediction models. Specifically,  for each experiment $i=1,\dots, M$, we independently draw trajectories $\mathcal{D}_n^i$ and get three different prediction models $\tilde{V}^i(s)$, $\hat{V}_U^i(s)$, and $\hat{V}_V^i(s)$. For any test data point $s_j^{*}\in \mathcal{S}_{test}$, the predicted values of $V^{\pi}_{*}(s_j^{*})$ are $\tilde{V}^i(s_j^{*})$, $\hat{V}_U^i(s_j^{*})$, and $\hat{V}_V^i(s_j^{*})$. Then, the variances $\text{Var}(\tilde{V}(s_j^{*}))$, $\text{Var}(\hat{V}_U(s_j^{*}))$, and $\text{Var}(\hat{V}_V(s_j^{*}))$ are approximated by the sample variances of the set $\{\tilde{V}^i(s_j^{*})\}_{i=1}^m$, $\{\hat{V}_U^i(s_j^{*})\}_{i=1}^m$, and $\{\hat{V}_V^i(s_j^{*})\}_{i=1}^m$. We verify the variance reduction property by comparing $\text{Var}(\tilde{V}(s_j^{*}))$ with $\text{Var}(\hat{V}_U(s_j^{*}))$ and $\text{Var}(\hat{V}_V(s_j^{*}))$ for $j=1,\dots, m$.




\begin{figure}[htb]
\centering
\includegraphics[width=0.25\textwidth]{fig/RL_LSTD/RL_var_B100_0.3.png} 
\includegraphics[width=0.25\textwidth]{fig/RL_LSTD/RL_var_B100_0.5.png} 
\includegraphics[width=0.25\textwidth]{fig/RL_LSTD/RL_var_B100_0.7.png} \\
\includegraphics[width=0.25\textwidth]{fig/RL_LSTD/RL_var_B500_0.3.png} 
\includegraphics[width=0.25\textwidth]{fig/RL_LSTD/RL_var_B500_0.5.png} 
\includegraphics[width=0.25\textwidth]{fig/RL_LSTD/RL_var_B500_0.7.png} \\
\includegraphics[width=0.25\textwidth]{fig/RL_LSTD/RL_var_B1000_0.3.png} 
\includegraphics[width=0.25\textwidth]{fig/RL_LSTD/RL_var_B1000_0.5.png} 
\includegraphics[width=0.25\textwidth]{fig/RL_LSTD/RL_var_B1000_0.7.png} 
\caption{\it Variance differences among the predicted policy values using the LSTD algorithm with $ m = 50 $ and $ M = 50 $, evaluated across various values of $ n $, $ B $, and $ k/n $. \textcolor{black}{$\tilde{V}(s^*_j)$ represents the results without experience replay, while $\hat{V}_{U}(s^*_j)$ and $\hat{V}_{V}(s^*_j)$ represent the results with experience replay.}
The red line represents the baseline where the variance difference is 0.}
\label{fig:RL_LSTD}
\end{figure}
Figure \ref{fig:RL_LSTD} compares the variances by drawing the boxplots of the differences $\{\text{Var}(\tilde{V}(s_j^{*}))-\text{Var}(\hat{V}_U(s_j^{*}))\}_{j=1}^m$ and $\{\text{Var}(\tilde{V}(s_j^{*}))-\text{Var}(\hat{V}_V(s_j^{*}))\}_{j=1}^m$, with regard to different $n$, $B$, and the ratio $k/n$. We choose the $n\in \{500, 1000,2500,5000\}$, $B\in\{100, 500, 1000\}$, and $k/n\in\{0.3, 0.5, 0.7\}$. The results clearly demonstrate that for all of the different parameters, the variance differences across all test data points are consistently greater than 0 for both $ U $- and $ V $-statistics-based experience replay methods. 
As $ n $ increases, the variance differences become small because all estimation methods exhibit reduced variance, resulting in correspondingly smaller differences; nonetheless, the reduction in variance remains significant. To illustrate this, we consider the case where $ n = 5000 $, $ B = 1000 $, and $ k/n = 0.3 $, as shown in Figure \ref{fig:figure1}. From the figure, we observe that the resampled methods demonstrate a significant improvement in variance in this larger $ n $ scenario.






% In addition to examining the variance reduction property, we are also interested in assessing the difference in the prediction error. Therefore, we also compare the root mean squared error (RMSE) of the three prediction methods across the $ m $ test points for all $ M $ experiments. The detailed results are presented in Appendix \ref{rmse lstd}, demonstrating that the combination of experience replay, regardless of the specific resampling method used, not only reduces variance but also tends to achieve smaller prediction errors, further highlighting its effectiveness.



\subsection{Experiments of Policy Evaluation Using PDE-Based Algorithm}\label{PDE:2nd}



Secondly, we present the experiment results obtained using the second-order PDE-based algorithm with functions $g$ and $f$ defined in \eqref{f and g 2} with $\alpha=2$. 
We consider an experimental setting where the state dynamics are governed by the Ornstein–Uhlenbeck (OU) process %\textcolor{blue}{(should the OU driven by a policy $\pi$?)}, 
$ds(t) = \lambda sdt + \sigma dB_t$ with $\lambda = 0.05, \sigma = 1$. The reward function $r^{\pi}(s)$ is set to be $r^{\pi}(s) = \beta\cos^3(s)-\lambda s (-3\cos^2(s)\sin(s)) - \frac12\sigma^2(6\cos(s)\sin^2(s) -3\cos^3(s))$ with the discounted coefficient $\beta=0.1$. 
\textcolor{black}{For the OU process, the transition distribution $P^{\pi}(s'|s)$ from time $t$ to $t+\dt$ follows a normal distribution with mean $se^{\lambda \dt}$ and variance $\frac{\sigma^2}{2\lambda}(e^{2\lambda \dt} - 1)$. We set $\dt = 0.1$, and under this setting, $D_n$ in Section \ref{exp:PE} follows the same transition distribution, allowing us to use the same simulated trajectory data. Additionally, we employ the same periodic basis functions as described in Section \ref{exp:PE}.}
% We use the same periodic bases and the same trajectories data $D_n$ as described in Section \ref{exp:PE}, where the transition distribution $P^{\pi}(s'|s)$ from $t$ to $t+\dt$ follows the normal distribution with expectation $se^{\lambda \dt}$, variance $\frac{\sigma^2}{2\lambda }(e^{2\lambda \dt}-1)$.  
The true value function $V^{\pi}(s)$ then can be exactly obtained from \eqref{def of value}, $V^{\pi}(s)=\text{cos}^3(s)$. 

\textcolor{black}{Note that the experiments using LSTD in Section \ref{exp:PE} can be considered as a way for estimating $ V^{\pi}(s) $ by discretizing it as a MDP. This approach leverages the relationships $ r^{\pi}_{*}(s) = r^{\pi}(s)\dt $ and $ \gamma = e^{-\beta \dt} $, which hold in the given setting. However, as observed in Figure \ref{fig:three_figures}, when the original methods are used, the second-order PDE-based approach generally shows greater accuracy with narrower confidence bands. In addition, incorporating experience replay exhibits superior performance in the second-order PDE-based algorithm, achieving a greater percentage reduction in variance compared to the LSTD method. } %\tcr{What do you mean greater stability? Does the stability come from the PDE method or our algorithm?}

 
\begin{figure}[ht]
\centering
\includegraphics[width=0.25\textwidth]{fig/RL_2nd/RL_var_B100_0.3.png} 
\includegraphics[width=0.25\textwidth]{fig/RL_2nd/RL_var_B100_0.5.png} 
\includegraphics[width=0.25\textwidth]{fig/RL_2nd/RL_var_B100_0.7.png} \\
\includegraphics[width=0.25\textwidth]{fig/RL_2nd/RL_var_B500_0.3.png} 
\includegraphics[width=0.25\textwidth]{fig/RL_2nd/RL_var_B500_0.5.png} 
\includegraphics[width=0.25\textwidth]{fig/RL_2nd/RL_var_B500_0.7.png} \\
\includegraphics[width=0.25\textwidth]{fig/RL_2nd/RL_var_B1000_0.3.png} 
\includegraphics[width=0.25\textwidth]{fig/RL_2nd/RL_var_B1000_0.5.png} 
\includegraphics[width=0.25\textwidth]{fig/RL_2nd/RL_var_B1000_0.7.png} 
\caption{\it Variance differences among the predicted policy values using the second-order PDE-based algorithm with $ m = 50 $ and $ M = 50 $, evaluated across various values of $ n $, $ B $, and $ k/n $. \textcolor{black}{$\tilde{V}(s^*_j)$ represents the results without experience replay, while $\hat{V}_{U}(s^*_j)$ and $\hat{V}_{V}(s^*_j)$ represent the results with experience replay.} The red line represents the baseline where the variance difference is 0.}
\label{fig:RL_2nd}
\end{figure}

We evaluate the performance of the three prediction models using the same way and notations as in Section \ref{exp:PE}. Specifically, we verify the variance reduction property by comparing $\text{Var}(\tilde{V}(s_j^{*}))$ with $\text{Var}(\hat{V}_U(s_j^{*}))$ and $\text{Var}(\hat{V}_V(s_j^{*}))$ for $j=1,\dots, m$.
Figure \ref{fig:RL_2nd} compares the variances by drawing the boxplots of the differences $\{\text{Var}(\tilde{V}(s_j^{*}))-\text{Var}(\hat{V}_U(s_j^{*}))\}_{j=1}^m$ and $\{\text{Var}(\tilde{V}(s_j^{*}))-\text{Var}(\hat{V}_V(s_j^{*}))\}_{j=1}^m$, with regard to different $n$, $B$, and the ratio $k/n$. We choose the $n\in \{500, 1000,2500,5000\}$, $B\in\{100, 500, 1000\}$, and $k/n\in\{0.3, 0.5, 0.7\}$. The results also clearly demonstrate that for all of the different parameters, the variance differences across all test data points are consistently greater than 0 for both $ U $- and $ V $-statistics-based experience replay methods.
As $ n $ increases, the variance differences tend to diminish because all three methods exhibit reduced variance, resulting in correspondingly smaller differences; however, the reduction in variance remains significant. To illustrate this, we consider the case where $ n = 5000 $, $ B = 500 $, and $ k/n = 0.3 $, as shown in Figure \ref{fig:figure3}. From the figure, we observe that the resampled methods demonstrate a significant improvement in variance in this larger $ n $ scenario.





%\tcr{YZ: what is the difference between second-order and first-order approximation?}
\textcolor{black}{We also present results obtained using the first-order PDE-based algorithm in Appendix \ref{app:PE 1st}. However, with the use of experience replay, the second-order PDE-based method achieves a greater percentage reduction in variance compared to LSTD and the first-order PDE-based method. Intuitively, the second-order method accounts for two future steps, introducing more stochasticity, which provides greater potential for variance reduction.}
Additionally, we compare the root mean squared
error (RMSE) of the proposed methods with the original method over the $ m $ test points for all $ M $ experiments for both the LSTD and PDE-based methods. The detailed results are presented in Appendix \ref{rmse pe}, demonstrating that the combination of experience replay, regardless of the specific resampling method used, not only reduces variance but also tends to achieve smaller prediction errors, further highlighting its superiority.





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Experiments of Kernel Ridge Regression}\label{sec:kernel}

Thirdly, we perform experiments to validate the variance reduction by the kernel ridge regression estimators introduced in Section \ref{sec:krr} with experience replay.

\paragraph{Simulation Study}
We consider a regression setting that for each $(X, Y) \sim F_Z$, the predictor $X = (X_{(1)}, X_{(2)}) \in \mathbb{R}^2$ is generated with $X_{(1)}, X_{(2)} \sim \text{Unif}(0, 1)$, and the response $Y \in \mathbb{R}$ is given by 
\[
Y = e^{10(-(X_{(1)} - 0.25)^2 - (X_{(2)} - 0.25)^2)} + 0.5 \cdot e^{14(-(X_{(1)} - 0.7)^2 - (X_{(2)} - 0.7)^2)} + \epsilon,
\]  
where $\epsilon \sim \mathcal{N}(0, 0.25)$ is independent of $X$. This setting is widely used in the study of kernel ridge regression and generalized regression models \citep[see,][]{wood2003thin, hainmueller2014kernel}.

For each experiment, we draw $n$ data points from $F_Z$ to form the training dataset $\mathcal{D}_n$. 
% Using $\mathcal{D}_n$, we train three different regression models with parameters $\tilde{\theta}_n, \hat{\theta}_U$, and $\hat{\theta}_V$ derived from the Algorithm \ref{alg:method} with $h_k$ defined in \eqref{eq41}. 
We use the \texttt{krls} function in \texttt{R} to fit the kernel ridge regression model with a Gaussian kernel. The $\lambda$ is chosen as $n^{-2/3}$.
We evaluate the performance of these models on $m=100$ test points independently drawn from $F_Z$, denoted by $\mathcal{D}_{test}=\{ (x_j,y_j)\}_{j=1}^m$. The experiment is repeated \( M = 100 \) times, and the variances of the predicted outcomes $\tilde{y}_j, \hat{y}_{j,U}$, and $\hat{y}_{j,V}$ for each test predictor \( x_j \), where \( j = 1, \dots, m \), are approximated using the sample variances, denoted by \(\text{Var}(\tilde{y}_j)\), \(\text{Var}(\hat{y}_{j,U})\), and \(\text{Var}(\hat{y}_{j,V})\). 
\textcolor{black}{As stated in \citet{dai2023kernel}, the predictions \(\tilde{y}_j\), \(\hat{y}_{j,U}\), and \(\hat{y}_{j,V}\) are approximately equal to \(\phi(x_j)^{\top}\tilde{\theta}_n\), \(\phi(x_j)^{\top}\hat{\theta}_{U}\), and \(\phi(x_j)^{\top}\hat{\theta}_{V}\) when \(q\) is  large. 
Consequently, \(\text{Var}(\tilde{y}_j)\), \(\text{Var}(\hat{y}_{j,U})\), and \(\text{Var}(\hat{y}_{j,V})\) serve as estimates for \(\phi(x_j)^{\top}\text{Var}(\tilde{\theta}_n)\phi(x_j)\), \(\phi(x_j)^{\top}\text{Var}(\hat{\theta}_{U})\phi(x_j)\), and \(\phi(x_j)^{\top}\text{Var}(\hat{\theta}_{V})\phi(x_j)\), respectively. Therefore, the reduction in the variance of the estimators of \(\theta\) can be directly assessed by evaluating the reduction in the variance of these predictions.} We compare the variances \(\text{Var}(\tilde{y}_j)\), \(\text{Var}(\hat{y}_{j,U})\), and \(\text{Var}(\hat{y}_{j,V})\) across all test points.




\begin{figure}[htb]
\centering
\includegraphics[width=0.25\textwidth]{fig/kr/kr_var_B25_k10.png} 
\includegraphics[width=0.25\textwidth]{fig/kr/kr_var_B25_k15.png} 
\includegraphics[width=0.25\textwidth]{fig/kr/kr_var_B25_k20.png} \\
\includegraphics[width=0.25\textwidth]{fig/kr/kr_var_B50_k10.png} 
\includegraphics[width=0.25\textwidth]{fig/kr/kr_var_B50_k15.png} 
\includegraphics[width=0.25\textwidth]{fig/kr/kr_var_B50_k20.png} \\
\includegraphics[width=0.25\textwidth]{fig/kr/kr_var_B100_k10.png} 
\includegraphics[width=0.25\textwidth]{fig/kr/kr_var_B100_k15.png} 
\includegraphics[width=0.25\textwidth]{fig/kr/kr_var_B100_k20.png} 
\caption{\it  Variance differences in predicted outcomes using kernel ridge regression with $ m = 100 $ and $ M = 100 $, evaluated across various values of  $ n $, $ B $, and $ k/n $.  \textcolor{black}{$\tilde{y}$ represents the results without experience replay, while $\hat{y}_{U}$ and $\hat{y}_{V}$ represent the results with experience replay.
}
The red line represents the baseline where the variance difference is 0.}
\label{fig:kr}
\end{figure}

Figure \ref{fig:kr} shows the variance differences across test points by plotting the boxplots of $\{\text{Var}(\tilde{y}_j )-\text{Var}(\hat{y}_{j,U} )\}^m_{j=1}$ and $\{\text{Var}(\tilde{y}_j )-\text{Var}(\hat{y}_{j,V} )\}^m_{j=1}$ for different values of $n$, $B$, and $k$. We select $n\in \{100,150,200,250\}$, $B\in\{25, 50, 100\}$, and $k\in\{10, 15, 20\}$. The results show that the variance differences are consistently greater than zero for both $ U $- and $ V $-statistics-based experience replay methods across all settings, confirming the variance reduction property in Section \ref{sec:estexpreplay}.

\begin{table}[ht!]
\centering
\caption{\it Time cost reduction achieved by experience replay methods (measured in seconds) with \( B=25 \) for different values of \( k \) and \( n \).}
\vspace{0.2cm}
\label{tab:1} 
\resizebox{0.6\linewidth}{!}{%
\begin{tabular}{c|cc|cc|cc}
\toprule % Bold horizontal line at the top
& \multicolumn{2}{c|}{\( k = 10 \)} & \multicolumn{2}{c|}{\( k = 15 \)} & \multicolumn{2}{c}{\( k = 20 \)} \\ \cline{2-7} 
\( n \) & \( {t} - {t}_U \) & \( {t} - {t}_V \) & \( {t} - {t}_U \) & \( {t} - {t}_V \) & \( {t} - {t}_U \) & \( {t} - {t}_V \) \\ \hline
200  & 1.279 & 1.118 & 1.139 & 1.105 & 0.968 & 1.018 \\  
250  & 3.792 & 3.811 & 3.658 & 3.620 & 3.473 & 3.449 \\ \bottomrule % Bold horizontal line at the bottom
\end{tabular}%
}
\end{table}


Table \ref{tab:1} presents the time cost reduction achieved by the experience replay methods with $B=25$, $k \in \{10, 15, 20\}$, and $n \in \{200, 250\}$. Here, ${t}$ represents the total time cost across all experiments without experience replay, while ${t}_U$ and ${t}_V$ represent the total time costs with experience replay based on resampled $U$- and $V$-statistics, respectively.
The results demonstrate that, for a fixed $B$, the experience replay method reduces the computational cost in time, particularly when $k$ is small and $n$ is large.  We also compare the RMSE of the proposed methods with the original method in Appendix \ref{rmse simu}.
The results indicate that incorporating experience replay not only reduces variance and time cost but also decreases prediction errors for all settings, especially in data-scarce scenarios.


\paragraph{Real Data Analysis }
We study the \texttt{Boston} dataset from the \texttt{R} package \texttt{MASS}, which contains information collected by the U.S. Census Bureau regarding housing in the Boston area. The task is to predict the median value of owner-occupied homes.
We randomly sample $ m = 100 $ observations from the dataset as the test set $ \mathcal{D}_{\text{test}}$. For each experiment, we randomly draw $ n $ observations from the remaining data to form the training dataset $ \mathcal{D}_n $. Following the same procedure as in the simulation study, we conduct $ M = 100 $ experiments and calculate $ \text{Var}(\tilde{y}_j) $, $ \text{Var}(\hat{y}_{j,U}) $, and $ \text{Var}(\hat{y}_{j,V}) $ for each test point $x_j$.  Figure \ref{fig:kr_real} presents the boxplots of  $\{\text{Var}(\tilde{y}_j )-\text{Var}(\hat{y}_{j,U} )\}^m_{j=1}$ and $\{\text{Var}(\tilde{y}_j )-\text{Var}(\hat{y}_{j,V} )\}^m_{j=1}$ for different values of $n$, $B$, and $k/n$. We choose  $n\in \{50, 100,150,200\}$, $B\in\{100, 300, 500\}$, and $k/n\in\{0.5, 0.7, 0.9\}$. The results confirm that the variance reduction property holds across all settings for both $ U $- and $ V $-statistics-based experience replay methods. 

\begin{figure}[ht!]
\centering
\includegraphics[width=0.25\textwidth]{fig/real_data/b_var_B100_0.5.png} 
\includegraphics[width=0.25\textwidth]{fig/real_data/b_var_B100_0.7.png} 
\includegraphics[width=0.25\textwidth]{fig/real_data/b_var_B100_0.9.png} \\
\includegraphics[width=0.25\textwidth]{fig/real_data/b_var_B300_0.5.png} 
\includegraphics[width=0.25\textwidth]{fig/real_data/b_var_B300_0.7.png} 
\includegraphics[width=0.25\textwidth]{fig/real_data/b_var_B300_0.9.png} \\
\includegraphics[width=0.25\textwidth]{fig/real_data/b_var_B500_0.5.png} 
\includegraphics[width=0.25\textwidth]{fig/real_data/b_var_B500_0.7.png} 
\includegraphics[width=0.25\textwidth]{fig/real_data/b_var_B500_0.9.png} 
\caption{\it Variance differences in predicted outcomes using kernel ridge regression on the \texttt{Boston} dataset, with $ m = 100 $, $ M = 100 $, evaluated across various values of $ n $, $ B $, and $ k/n $.  \textcolor{black}{$\tilde{y}$ represents the results without experience replay, while $\hat{y}_{U}$ and $\hat{y}_{V}$ represent the results with experience replay.
} The red line represents the baseline where the variance difference is $0$.}
\label{fig:kr_real}
\end{figure}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion}\label{sec:conclusion}
Experience replay enhances stability and efficiency in reinforcement learning by reusing past experiences during training. Despite its practical success, its theoretical properties remain underexplored. This paper presents a theoretical framework that models experience replay using resampled $U$- and $V$-statistics, enabling us to establish variance reduction guarantees across policy evaluation and supervised learning tasks.
We applied this framework to two policy evaluation algorithms—the Least-Squares Temporal Difference (LSTD) method and a PDE-based model-free algorithm—demonstrating notable improvements in stability and efficiency, particularly in data-scarce settings. Additionally, we applied the framework to kernel ridge regression, achieving both significant computational savings and variance reduction. By appropriately selecting parameters for the number and size of subsamples, our method reduces the computational cost of kernel ridge regression from the traditional  $O(n^3)$ to as low as $O(n^2)$, while ensuring enhanced statistical performance, making it well-suited for large-scale applications. The code for reproducing the numerical results in this paper is available at \url{https://github.com/JialeHan22/Variance-Reduction-via-Resampling-and-Experience-Replay.git}.

Future research directions include extending the use of experience replay into settings with federated learning or active learning. For example, leveraging replay mechanisms to optimize communication efficiency and model personalization in federated settings, or dynamically selecting data subsets for replay based on their informativeness in active learning, could provide useful solutions for distributed data scenarios. Additionally, investigating adaptive experience replay strategies to dynamically balance exploration and exploitation in dynamic environments is an interesting area for further study.  For instance, integrating reinforcement learning with meta-learning frameworks could enable agents to learn not only the optimal policy but also how to adaptively manage their experience buffers based on task complexity, reward dynamics, or data heterogeneity. This has applications in multi-agent systems, non-stationary environments, and real-time decision-making, where adaptability is crucial.
Finally, exploring theoretical connections between experience replay and other variance reduction techniques, such as importance sampling and ensemble learning, could yield hybrid methods that further enhance stability and computational efficiency. 

\vskip 0.2in
\bibliography{subsample}


\newpage
%\setcounter{page}{1}
% \setcounter{equation}{0}
% \setcounter{section}{0}
% \renewcommand{\theequation}{C.\arabic{equation}}
\appendix
\section*{Appendix}
Appendix \ref{app:proofs} provides the proofs of theoretical results in this paper. Appendix \ref{app:add exp} includes supplementary experiments that further demonstrate the variance reduction properties of the proposed $U$- and $V$-statistics-based experience replay methods. Appendix \ref{sec:appaddexp} presents numerical results comparing the RMSE of the proposed methods with the original method.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Proofs of Main Results}\label{app:proofs}
\subsection{Proof of Lemma \ref{new_T1}}\label{A1.0}
\begin{proof}~
\noindent
We begin by applying the central limit theory 
to multivariate i.i.d. random variables to obtain:
    \begin{equation}\label{new_11}
        \sqrt{n} \left[ \begin{pmatrix} \frac{1}{n}\sum_{j=1}^n f(Z_j) \\  \text{vec}(\frac{1}{n} \sum_{j=1}^n g(Z_j))\end{pmatrix} - \begin{pmatrix} \E[f(Z)] \\ \text{vec}(\E[g(Z)]) \end{pmatrix} \right]\overset{d}{\rightarrow} N(0, \Sigma_{0}),
    \end{equation}
    where the covariance matrix $\Sigma_{0}$ is defined as:
\begin{equation*}
    \Sigma_{0} = \begin{pmatrix}
    \text{Var}(f(Z)) &  \text{Cov}(f(Z),\text{vec}( g(Z))) \\
     \text{Cov}(f(Z),\text{vec}( g(Z))) & \text{Var}(\text{vec}( g(Z))
    \end{pmatrix}.
\end{equation*}
Next, we aim to find the asymptotic distribution of $\tilde{\theta}_n$. Consider the function $g(X,\text{vec}(Y))=Y^{-1}X$,  which is continuous for any $X\in\R^q$ and invertible matrix $Y\in\R^q\times \R^q$. Note that  
     \begin{equation*}
         g\left(\frac{1}{n}\sum_{j=1}^n f(Z_j), \text{vec}\Big(\frac{1}{n} \sum_{j=1}^n g(Z_j)\Big)\right)=\Big[\sum_{j=1}^n g(Z_j)\Big]^{-1}\Big[\sum_{j=1}^n f(Z_j)\Big]=\tilde{\theta}_n
     \end{equation*}
     and 
     \begin{equation*}
         g( \E[f(Z)],\text{vec}(\E[g(Z)]))=\big[\E[g(Z)]\big]^{-1}\big[\E[f(Z)]\big]=\theta.
     \end{equation*}
To apply the delta method, we define the Jacobian matrix $G$ as follows,
     \begin{equation*}
     \begin{split}
          G&:=\dot{g}(\E[f(Z)],\text{vec}(\E[g(Z)]))\\
          &=\left([\E[g(Z)]]^{-1},-\theta^\top\otimes[\E[g(Z)]]^{-1}  \right).
     \end{split}
     \end{equation*}
where $\otimes$ denotes the Kronecker product. Applying the delta method to \eqref{new_11}, we obtain:
    \begin{equation*}
\sqrt{n}\left[ \tilde{\theta}_n-\theta\right]
      \xrightarrow{d}N(0,G\Sigma_{0}[G]^\top)
      =N(0,\Sigma_{}),
    \end{equation*}
where $\Sigma $ is defined as:
\begin{equation}
    G\begin{pmatrix}

    \text{Var}(f(Z)) &  \text{Cov}(f(Z),\text{vec}( g(Z))) \\
     \text{Cov}(f(Z),\text{vec}( g(Z))) & \text{Var}(\text{vec}( g(Z))

    \end{pmatrix}G^\top.
\end{equation}

This completes the proof of Lemma \ref{new_T1}.
\end{proof}



\subsection{Proof of Theorem \ref{new_u_incomplete}}\label{A3.0}
\begin{proof}
From the definition in \eqref{eq7}, we have that 
\begin{equation*}
    \zeta_{1,k}:= \text{Cov}\Big(h_k(Z_1,Z_2\dots,Z_{k}), h_k(Z_1,Z_2', \dots, Z_{k}^{'} )\Big),
\end{equation*}
where $Z_{2}^{'},\dots,Z_{k}^{'}$ are i.i.d. copies from  $F_Z$, independent of the original data set $\mathcal{D}_n$.

To analyze $\zeta_{1,k}$,  we use the following lemma.
\begin{lemma}\label{new_L1}
Let $ Z_1, Z_2, \ldots, Z_n \stackrel{\text{iid}}{\sim} F_Z $, with $ h_k $ defined in \eqref{new_hkn} and $ \Sigma $ defined in \eqref{n_Sigma}. Then,  $ k^2 \zeta_{1,k} < \Sigma_{} + o(\Sigma_{}) $.
\end{lemma}
\begin{proof}~
By \cite{lee2019u},  it follows that $\zeta_{1,k}< \frac{1}{k}\zeta_{k,k}$ when $k>1$. 
From Corollary \ref{new_c1}, we have that,
    \begin{equation*}
        \zeta_{k,k}=\Sigma_{}/k+o(\Sigma/k).
    \end{equation*}
Hence,
    \begin{equation*}
        k^2\zeta_{1,k} < k\zeta_{k,k}=  \Sigma_{}+o(\Sigma_{}).
    \end{equation*}
This completes the proof of Lemma \ref{new_L1}.
\end{proof}

Next, let $U_{n,k}$ denote the complete $U$-statistics with kernel $h_k$, defined as:
    \begin{equation*}
         U_{n,k}:=\frac{1}{\binom{n}{k}}\sum_{i}h_{k}(Z_{i_1},\dots, Z_{i_{k}}),
    \end{equation*}
where $\{Z_{i_1},\dots, Z_{i_{k}}\}$ represents a subsample of $k$ distinct elements from the original dataset $\mathcal{D}_n$, and the sum is taken over all $\binom{n}{k}$ possible subsamples of size $k$. The asymptotic normality of complete $U$-statistics has been studied in the literature  \citep{hoeffding1948class, peng2019asymptotic}. We extend these results to the case of matrix-valued kernels. Specifically, applying Theorem 1 in \citet{peng2019asymptotic} with a constant randomization term, if
\begin{equation*}
    \lim_{n\to \infty}\frac{1}{n}\zeta_{k,k}[\zeta_{1,k}]^{-1}\to 0,
\end{equation*} then it holds that
\begin{equation*}
   \sqrt{n} [{  U_{n,k}-\E h_{k}(Z_{1},\dots,Z_{{k}})}]\xrightarrow{d}  N(0,k^2\zeta_{1,k}).
\end{equation*}
From this result, we derive the asymptotic variance of $U_{n,k}$ as:
\begin{equation}\label{eq50}
    \text{Var}(U_{n,k})=\frac{k^2}{n}\zeta_{1,k}+o(\frac{k^2}{n}\zeta_{1,k}).
\end{equation}

For incomplete $ U $-statistics, \citet{blom1976some} established that the variance of an incomplete $ U $-statistic $ U_{n,k,B} $, constructed from $ B $ subsamples selected uniformly at random with replacement, is given by: 
\begin{equation}\label{eq51}
     \text{Var}(U_{n,k,B}) = \left( 1 - \frac{1}{B} \right) \text{Var}(U_{n,k}) + \frac{1}{B} \zeta_{k,k}.   
\end{equation}
This result holds even when both $ k $ and $ B $ vary with $ n $.


% So that the condition $\lim \frac{k_n}{\sqrt{n}}= 0$ in Theorem 1 in \cite{mentch2016quantifying} can be replaced by  $\lim_{n\to \infty}\frac{1}{n}\zeta_{k,k,\Omega}[ \zeta_{1,k,\Omega}]^{-1}\to 0$, which also ensures the asymptotic normal property of $ U_{n,k,\Omega}$ given that $h_k\in\mathcal{H}$. In addition, under the condition $\lim_{n\to \infty} \zeta_{1,k,\Omega}> 0$, the condition $\lim_{n\to \infty}\frac{1}{n}\zeta_{k,k,\Omega}[ \zeta_{1,k,\Omega}]^{-1}\to 0$ is redundant because 
% $\zeta_{k,k,\Omega}$ is bounded if $h_k\in\mathcal{H}$. So as long as $\lim_{n\to \infty} \zeta_{1,k,\Omega}> 0$ and $h_k\in\mathcal{H}$, the Theorem 1 in \cite{mentch2016quantifying} holds, from which we have 
% \begin{equation}\label{new_33-n}
% \begin{split}
%     \text{Var}(\hat{\theta}_U)\leq \frac{k^2}{n}\zeta_{1,k,\Omega}+\frac{1}{B}\zeta_{k,k,\Omega}+o\left(\frac{k^2}{n}\zeta_{1,k,\Omega}+\frac{1}{B}\zeta_{k,k,\Omega}\right).
% \end{split}  
% \end{equation}

% So that the asymptotic variance of $ U_{n,k,\Omega}$ is 
% \begin{equation}\label{as_u1}
%    \text{Var}(U_{n,k,\Omega})= \frac{k^2}{n}\zeta_{1,k,\Omega}+o(\frac{k^2}{n}\zeta_{1,k,\Omega}).
% \end{equation}

%     Extending the result of \cite{lee2019u} to our situation {\textcolor{blue}{(more details)}}, we have 
%     \begin{equation}\label{32}
%             \text{Var}(\hat{\theta}_U)=(1-\frac{1}{B}) \text{Var}(U_{n,k,\Omega})+\frac{1}{B}\zeta_{k,k,\Omega}.
%     \end{equation}

    
    To finish the proof, we will use the following lemma.
    \begin{lemma}\label{new_l1}
        $o(a_n)+o(B)=o(a_n+B)$, provided $a_n>0$ and $B>0$. 
    \end{lemma}
    \begin{proof}
      $|\frac{o(a_n)+o(B)}{a_n+B}|=|\frac{o(a_n)/a_n}{1+(B/a_n)}+\frac{o(B)/B}{(a_n/B)+1}|<|o(a_n)/a_n|+|o(B)/B|\to 0$. 
    \end{proof}
From Corollary \ref{new_c1}, we have that 
\begin{equation}\label{new_40}
    \zeta_{k,k}=\frac{\Sigma}{k} + o(\frac{\Sigma}{k}).
\end{equation}

Now we can prove the Theorem \ref{new_u_incomplete}. 
By \eqref{eq50}, \eqref{eq51}, \eqref{new_40}, Lemma \ref{new_L1}, and Lemma \ref{new_l1}, the variance of the estimator $\hat{\theta}_U$ can be expressed as:
\begin{equation}\label{new_49-1}
\begin{split}
    \text{Var}(\hat{\theta}_U)&=  \left( 1 - \frac{1}{B} \right)\left(\frac{k^2}{n}\zeta_{1,k}+o\Big(\frac{k^2}{n}\zeta_{1,k}\Big)\right) + \frac{1}{B} \left(\frac{\Sigma_{}}{k}+o\Big(\frac{\Sigma_{}}{k}\Big) \right)\\
    &<  \left( 1 - \frac{1}{B} \right)\left(\frac{\Sigma}{n}+o\Big(\frac{\Sigma}{n}\Big)\right) + \frac{1}{B} \left(\frac{\Sigma_{}}{k}+o\Big(\frac{\Sigma_{}}{k}\Big) \right)\\
    &=\frac{\Sigma}{n}\left(1+\frac{1}{B}\Big(\frac{n}{k}-1\Big)\right)+o\left(\frac{\Sigma}{n}+\frac{\Sigma}{Bk}\right).
\end{split}
\end{equation}
By Lemma \ref{new_T1}, we have $\Sigma_{}=n\text{Var}( \tilde{\theta}_n)+o(\Sigma_{})$. Substituting this into \eqref{new_49-1} and using 
 Lemma \ref{new_l1}, we have
    \begin{equation*}
        \begin{split}
            \text{Var}(\hat{\theta}_U)&< \frac{1}{n}\left(1+\frac{1}{B}\Big(\frac{n}{k}-1\Big)\right)(n\text{Var}( \tilde{\theta}_n)+o(\Sigma_{}))+o\left(\frac{\Sigma}{n}+\frac{\Sigma}{Bk}\right)\\
            &=\left(1+\frac{1}{B}\Big(\frac{n}{k}-1\Big)\right)\text{Var}( \tilde{\theta}_n)+o\left(\frac{\Sigma_{}}{n}+\frac{\Sigma_{}}{Bk}\right).
        \end{split}
    \end{equation*}
Therefore, when $\lim_{n\to \infty}n/(Bk)\to 0$, it follows that $     \text{Var}(\hat{\theta}_U)< \text{Var}( \tilde{\theta}_n)+o(1)$. In other words,
  \begin{equation*}
    \lim_{n\to \infty}[\text{Var}( \tilde{\theta}_n) - \text{Var}(\hat{\theta}_U)] > 0.
\end{equation*}

This completes the proof of Theorem \ref{new_u_incomplete}.
% From \eqref{as_u1}, \eqref{32}, and \eqref{40}, we have 
% \begin{equation}
%     \begin{split}
% \text{Var}(\hat{\theta}_U)&=(1-\frac{1}{B}) \left(\frac{k^2}{n}\zeta_{1,k,\Omega}+o(\frac{k^2}{n}\zeta_{1,k,\Omega})\right)+\frac{1}{B}\left(\frac{\Sigma_{s,\omega}}{k}+o(\frac{\Sigma_{s,\omega}}{k})\right)
%     \end{split}
% \end{equation} 
% By Lemma \ref{L1} and Lemma \ref{l1}, we have the following inequity in terms of matrix 
% \begin{equation}\label{42}
%     \begin{split}
% \text{Var}(\hat{\theta}_U)&\leq (1-\frac{1}{B}) \left(\frac{\Sigma_{s,\omega}}{n}+o(\frac{\Sigma_{s,\omega}}{n})\right)+\frac{1}{B}\left(\frac{\Sigma_{s,\omega}}{k}+o(\frac{\Sigma_{s,\omega}}{k})\right)\\
%     &=\frac{\Sigma_{s,\omega}}{n}\left(1+\frac{1}{B}(\frac{n}{k}-1)\right)+o\left(\frac{\Sigma_{s,\omega}}{n}+\frac{\Sigma_{s,\omega}}{Bk}\right).
%     \end{split}
% \end{equation} 
% By Lemma \ref{T2}, we have $\Sigma_{s,\omega}=n\text{Var}( \tilde{\theta})+o(\Sigma_{s,\omega})$, thus from \eqref{42} and Lemma \ref{l1}, we have
% \begin{equation}
%     \begin{split}
% \text{Var}(\hat{\theta}_U)&\leq\frac{1}{n}\left(1+\frac{1}{B}(\frac{n}{k}-1)\right)(n\text{Var}( \tilde{\theta})+o(\Sigma_{s,\omega}))+o\left(\frac{\Sigma_{s,\omega}}{n}+\frac{\Sigma_{s,\omega}}{Bk}\right)\\
%         &=\left(1+\frac{1}{B}(\frac{n}{k}-1)\right)\text{Var}( \tilde{\theta})+o\left(\frac{\Sigma_{s,\omega}}{n}+\frac{\Sigma_{s,\omega}}{Bk}\right).
%     \end{split}
% \end{equation} 
%\textcolor{blue}{Additional proof}
\end{proof}

\subsection{Proof of Theorem \ref{the_2}}\label{A4.0}
\begin{proof}
We begin by extending Theorem 10 from \citet{zhou2021v} to the matrix-valued setting under the assumptions that $k=o(n^{1/4})$,  $h_{k}\in \mathcal{H}$, and $\lim_{n\to\infty} k^2\zeta_{1,k}>0$.
It follows that the estimator $\hat{\theta}_V$ satisfies,
\begin{equation}\label{new_v_as}
    { \hat{\theta}_V-\E h_{k}(Z_{1},\dots,Z_{{k}})}{}\xrightarrow{d} N\left(0,{\frac{k^2}{n}\zeta_{1,k}+\frac{1}{B}\zeta_{k,k}}\right).
\end{equation}
Next, we show that the assumption
\begin{equation}
\label{eqn:asspzetakkn}
    \lim_{n\to \infty}\frac{1}{n}\zeta_{k,k}[ \zeta_{1,k}]^{-1} \to 0
\end{equation}
in Theorem 10 of \citet{zhou2021v} is redundant. To see this, consider:
\begin{equation*}
    \frac{1}{n}\zeta_{k,k}[ \zeta_{1,k}]^{-1} 
    = \frac{k^2}{n}\frac{\zeta_{k,k}}{ k^2\zeta_{1,k}}.
\end{equation*}
Since $\lim_{n\to \infty} k^2\zeta_{1,k} > 0$, $\lim_{n\to \infty}\zeta_{k,k} > 0$  (by the assumption that $h_{k} \in \mathcal{H}$), and because $k = o(n^{1/4})$ implies $k^2/n \to 0$, it follows that:
\begin{equation*}
    \lim_{n\to \infty} \frac{1}{n}\zeta_{k,k}[ \zeta_{1,k}]^{-1} = 0.
\end{equation*}
Thus, we do not require the assumption \eqref{eqn:asspzetakkn} for the proof.


From equation \eqref{new_v_as}, we have:
\begin{equation*}
\begin{split}
    \text{Var}(\hat{\theta}_V)&=\frac{k^2}{n}\zeta_{1,k}+\frac{1}{B}\zeta_{k,k}+o\left(\frac{k^2}{n}\zeta_{1,k}+\frac{1}{B}\zeta_{k,k}\right).
\end{split}
\end{equation*}
Using Corollary \ref{new_c1}, Lemma \ref{new_L1}, and Lemma \ref{new_l1}, we derive the following inequality,
\begin{equation}\label{new_49}
\begin{split}
    \text{Var}(\hat{\theta}_V)&< \frac{\Sigma_{}}{n}+o\Big(\frac{\Sigma_{}}{n}\Big)+\frac{1}{B}\left(\frac{\Sigma_{}}{k}+o\Big(\frac{\Sigma_{}}{k}\Big)\right)+o\left(\frac{\Sigma_{}}{n}+\frac{\Sigma_{}}{Bk}\right)\\
    &=\left(\frac{1}{n}+\frac{1}{Bk}\right)\Sigma_{}+o\left(\frac{\Sigma_{}}{n}+\frac{\Sigma_{}}{Bk}\right).
\end{split}
\end{equation}
From Lemma \ref{new_T1}, we know that $\Sigma_{}=n\text{Var}( \tilde{\theta}_n)+o(\Sigma_{})$.
Substituting this into equation \eqref{new_49}, and by Lemma \ref{new_l1}, we obtain,
    \begin{equation*}
        \begin{split}
            \text{Var}(\hat{\theta}_V)&<\left(\frac{1}{n}+\frac{1}{Bk}\right)(n\text{Var}( \tilde{\theta}_n)+o(\Sigma_{}))+o\left(\frac{\Sigma_{}}{n}+\frac{\Sigma_{}}{Bk}\right)\\
            &=\left(1+\frac{n}{Bk}\right)\text{Var}( \tilde{\theta}_n)+o\left(\frac{\Sigma_{}}{n}+\frac{\Sigma_{}}{Bk}\right).
        \end{split}
    \end{equation*}
Therefore, when $\lim_{n\to \infty}n/(Bk)\to 0$, it follows that $     \text{Var}(\hat{\theta}_V)< \text{Var}( \tilde{\theta}_n)+o(1)$. In other words,  
  \begin{equation*}
    \lim_{n\to \infty}[\text{Var}( \tilde{\theta}_n) - \text{Var}(\hat{\theta}_V)] > 0.
  \end{equation*}

  
  This completes the proof of Theorem \ref{the_2}.
\end{proof}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section{Application to Linear Ridge Regression}\label{app:linear}
% We apply the experience replay framework to linear ridge regression, where the response $Y$ is a linear function of the predictor $X$ for any $(X, Y)\sim F_Z$, 
% \begin{equation*}
%     Y=X^\top\theta +\epsilon,
% \end{equation*}
% where $\epsilon$ is a random error term independent of $X$. 
% The parameter of interest $\theta\in \R^p$ can be written as,
% \begin{equation}\label{thetalr}
% \begin{split}
%         \theta &=[\text{Var}(X)]^{-1}\text{Cov}(X,Y).
% \end{split}
% \end{equation}
% Using any $k$ data points $\{(X_1^*,Y_1^*),(X_2^*,Y_2^*), \dots,(X_k^*,Y_k^*)\}\in\mathcal{D}_n$, the linear ridge regression estimator of $\theta$ is obtained by solving the following optimization problem for a given $\lambda\geq 0$,
% \begin{equation*}
%     \argmin_{\theta\in \R^p} \left\{\sum_{i=1}^k (Y_i^*-{X_i^{*\top}}\theta)^2+\lambda||\theta||^2_2\right\}.
% \end{equation*}
% The solution  takes the form of
% \begin{equation}\label{eq38}
% h_{k}(Z_{1}^*,\dots,Z_{{k}}^*) = \left[ \sum_{i=1}^k g(X_j^*,Y_j^*)+\lambda\mathbb{I}_p\right]^{-1}\left[  \sum_{i=1}^k f(X_j^*,Y_j^*)\right],
% \end{equation} 
% where $g(X_j^*,Y_j^*) = X_j^* X_j^{*\top}$ and $f(X_j^*,Y_j^*) = X_j^* Y_j^*$.
% The setup in \eqref{thetalr} and \eqref{eq38} also aligns with our framework in \eqref{eqn:defoftheta} and \eqref{new_hkn}.
% Applying Algorithm \ref{alg:method}, once we get $\tilde{\theta}_n$, $\hat{\theta}_U$, and  $\hat{\theta}_V$, the prediction results at a test point $x$ are defined as $\tilde{y}(x) = x^\top\tilde{\theta}_n$, $\hat{y}_U(x) = x^\top\hat{\theta}_U$, and  $\hat{y}_V(x) = x^\top\hat{\theta}_V$,  with the variances $\text{Var}(\tilde{y}(x)  )=x^\top\text{Var}(\tilde{\theta}_n)x$, $\text{Var}(\hat{y}_U(x) )=x^\top\text{Var}(\hat{\theta}_U)x$, and $\text{Var}(\hat{y}_V(x))=x^\top\text{Var}(\hat{\theta}_V)x$.Thus, the variance reduction achieved by the proposed estimators can be directly evaluated through the reduction in the variance of predictions in linear ridge regression.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Additional Numerical Experiments}\label{app:add exp}

% \subsection{Experiments of Linear Ridge Regression}\label{lr simulation}

% We consider a simulation setting where for each $(X,Y)\sim F_Z$, $X\in \mathbb{R}^{50}$ is generated from $\mathcal{N}(\vec{0}, \boldsymbol{I}_{50})$, and $Y\in\R$ is according to a linear model,
% \begin{equation}\label{linear}
%     Y=X^{\top}\theta +\epsilon.
% \end{equation}
% Here $\theta \in \mathbb{R}^{50}$ is a fixed vector sampled from  $\mathcal{N}(\vec{0}, \boldsymbol{I}_{50})$, and $\epsilon$ is a random error term drawn independently from a Laplace distribution with a mean of $0$. The variance of $\epsilon$ is set to achieve a signal-to-noise ratio (SNR) of 10. 

% For each simulation experiment, we generate data $\mathcal{D}_n=\{(X_1,Y_1),\dots, (X_n,Y_n)\}$, and train three linear ridge regression models with coefficients $\tilde{\theta}_n, \hat{\theta}_U$, and $\hat{\theta}_V$,
% obtained by Algorithm \ref{alg:method} with $h_k$ defined in \eqref{eq38}. The models are fitted using the \texttt{glmnet} function in \texttt{R}. To evaluate model performance, we independently draw $m=50$ test points from $F_Z$, denoted by $\mathcal{D}_{test}=\{(x_1,y_1),\dots, (x_m,y_m)\}$. The simulation is repeated $ M = 50 $ times, and we calculate the variance of the predicted outcomes for each test predictor 
% $x_j, j=1,\dots, m$ using the three estimated coefficient sets. Specifically, for each experiment $i=1,\dots, M$, we generate a training dataset $\mathcal{D}^i_n$ and fit the models to obtain $\tilde{\theta}_n^i, \hat{\theta}_U^i$, and $\hat{\theta}_V^i$. The predicted values at each test point $x_j$ are computed as,
% $\tilde{y}_j^i=x_j^\top\tilde{\theta}_n^i$, $\hat{y}_{j,U}^i=x_j^\top\hat{\theta}_U^i$, and $\hat{y}_{j,V}^i=x_j^\top\hat{\theta}_V^i$. The variances $\text{Var}(\tilde{y}_j ), \text{Var}(\hat{y}_{j,U} )$, and $\text{Var}(\hat{y}_{j,V} )$ are estimated by the sample variances of the sets $\{\tilde{y}_j^i\}_{i=1}^m$, $\{\hat{y}_{j,U}^i\}_{i=1}^m$, and $\{\hat{y}_{j,V}^i\}_{i=1}^m$ respectively.
% The variance reduction property of the proposed estimators is verified by comparing $\text{Var}(\tilde{y}_j ), \text{Var}(\hat{y}_{j,U} )$, and $\text{Var}(\hat{y}_{j,V} )$.



% \begin{figure}[htb]
% \centering
% \includegraphics[width=0.25\textwidth]{fig/lr/lr_var_B100_0.5.png} 
% \includegraphics[width=0.25\textwidth]{fig/lr/lr_var_B100_0.7.png} 
% \includegraphics[width=0.25\textwidth]{fig/lr/lr_var_B100_0.9.png} \\
% \includegraphics[width=0.25\textwidth]{fig/lr/lr_var_B300_0.5.png} 
% \includegraphics[width=0.25\textwidth]{fig/lr/lr_var_B300_0.7.png} 
% \includegraphics[width=0.25\textwidth]{fig/lr/lr_var_B300_0.9.png} \\
% \includegraphics[width=0.25\textwidth]{fig/lr/lr_var_B500_0.5.png} 
% \includegraphics[width=0.25\textwidth]{fig/lr/lr_var_B500_0.7.png} 
% \includegraphics[width=0.25\textwidth]{fig/lr/lr_var_B500_0.9.png} 
% \caption{\it  Variance differences among the predicted outcome values using linear ridge regression with $ m = 50 $ and $ M = 50 $, evaluated across various values of $ n $, $ B $, and $ k/n $. \textcolor{blue}{$\tilde{y}$ denotes the original estimator without experience relay and $\hat{y}_{U}$, $\hat{y}_{V}$ represent the estimators with experience replay.} The red line represents the baseline where the variance difference is 0. }
% \label{fig:lr}
% \end{figure}

% Figure \ref{fig:lr} shows the variance differences across test points by plotting the boxplots of $\{\text{Var}(\tilde{y}_j )-\text{Var}(\hat{y}_{j,U} )\}^m_{j=1}$ and $\{\text{Var}(\tilde{y}_j )-\text{Var}(\hat{y}_{j,V} )\}^m_{j=1}$ for different $n$, $B$, and the ratio $k/n$. W select $n\in \{150, 200,250,300\}$, $B\in\{100, 300, 500\}$, and $k/n\in\{0.5, 0.7, 0.9\}$.  The
% results show that the variance differences are consistently greater than zero for both $ U $- and $ V $-statistics. Moreover, smaller values of $ k/n $ tend to yield greater variance reduction.


\subsection{Policy Evaluation Using First-Order PDE-Based Algorithm}\label{app:PE 1st}


We use the same experiment setting in Section \ref{PDE:2nd} and use the first-order PDE-based approach in the continuous-time case with functions $g$ and $f$ defined in \eqref{f and g 2} with $\alpha=1$.

\begin{figure}[htb]
\centering
\includegraphics[width=0.25\textwidth]{fig/RL_1st/RL_var_B100_0.3.png} 
\includegraphics[width=0.25\textwidth]{fig/RL_1st/RL_var_B100_0.5.png} 
\includegraphics[width=0.25\textwidth]{fig/RL_1st/RL_var_B100_0.7.png} \\
\includegraphics[width=0.25\textwidth]{fig/RL_1st/RL_var_B500_0.3.png} 
\includegraphics[width=0.25\textwidth]{fig/RL_1st/RL_var_B500_0.5.png} 
\includegraphics[width=0.25\textwidth]{fig/RL_1st/RL_var_B500_0.7.png} \\
\includegraphics[width=0.25\textwidth]{fig/RL_1st/RL_var_B1000_0.3.png} 
\includegraphics[width=0.25\textwidth]{fig/RL_1st/RL_var_B1000_0.5.png} 
\includegraphics[width=0.25\textwidth]{fig/RL_1st/RL_var_B1000_0.7.png} 
\caption{\it Variance differences among the predicted policy values using the first-order PDE-based method with $ m = 50 $ and $ M = 50 $, evaluated across various values of $ n $, $ B $, and $ k/n $. \textcolor{black}{$\tilde{V}(s^*_j)$ represents the results without experience replay, while $\hat{V}_{U}(s^*_j)$ and $\hat{V}_{V}(s^*_j)$ represent the results with experience replay.} The red line represents the baseline where the variance difference is 0.}
\label{fig:RL_1ST}
\end{figure}Figure \ref{fig:RL_1ST} compares the variances by drawing the boxplots of the differences $\{\text{Var}(\tilde{V}(s_j^{*}))-\text{Var}(\hat{V}_U(s_j^{*}))\}_{j=1}^m$ and $\{\text{Var}(\tilde{V}(s_j^{*}))-\text{Var}(\hat{V}_V(s_j^{*}))\}_{j=1}^m$, with regard to different $n$, $B$, and the ratio $k/n$. We choose the $n\in \{500, 1000,2500,5000\}$, $B\in\{100, 500, 1000\}$, and $k/n\in\{0.3, 0.5, 0.7\}$. The results clearly demonstrate that for all of the different parameters, the variance differences across all test data points are consistently greater than 0 for both $ U $- and $ V $-statistics-based experience replay methods. 
As $ n $ increases, the variance differences tend to diminish because all three methods exhibit reduced variance, resulting in correspondingly smaller differences; however, the reduction in variance remains significant. To illustrate this, we consider the case where $ n = 5000 $, $ B = 500 $, and $ k/n = 0.5 $, and draw the Figure \ref{fig:figure2} similar to the Figure \ref{fig:three_figures}. From Figure \ref{fig:figure2}, we observe that the resampled methods demonstrate a significant improvement in variance in this larger $ n $ scenario.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.4\textwidth]{fig/1st.png} 
    \caption{Variance reduction achieved by experience replay in policy evaluation using the first-order PDE-based approach. The solid lines represent the mean, and the shaded areas show $95\%$ confidence intervals based on 50 replications.}
    \label{fig:figure2}
\end{figure}

We compare the RMSE of the proposed methods
with the original method across the $ m $ test points for all $ M $ experiments. The detailed results are presented in Appendix \ref{rmse 1st}, demonstrating that the combination of experience replay, regardless of the specific resampling method used, not only reduces variance but also tends to achieve smaller prediction errors, further highlighting its superiority.



\section{RMSE Comparison}\label{sec:appaddexp}
In addition to checking the variance reduction property, we also compare the root mean squared error (RMSE) of the proposed methods with the original methods over the $ m $ test points across all $ M $ experiments.

\subsection{Reinforcement Leaning Policy Evaluation}\label{rmse pe}
In the experiment setting of  Section \ref{exp:PE}, for each experiment $i=1,2,\dots, M$, we define the following RMSE over the $m$ test points
\begin{equation*}
  \tilde{R}_i=\sqrt{\frac{1}{m}\sum_{j=1}^m (\tilde{V}^i(s_j^*)-V(s_j^*))^2}, \quad 
\end{equation*}
\begin{equation*}
    \hat{R}_{i, U}=\sqrt{\frac{1}{m}\sum_{j=1}^m (\hat{V}_{U}^i(s_j^*)-V(s_j^*))^2}, \quad \hat{R}_{i, V}=\sqrt{\frac{1}{m}\sum_{j=1}^m (\hat{V}_{V}^i(s_j^*)-V(s_j^*))^2}.
\end{equation*}

We compare the prediction errors by comparing $\tilde{{R}}_i, \hat{{R}}_{i, U}$, and $\hat{{R}}_{i, V}$ for $i=1,\dots, M$.






\subsubsection{LSTD Algorithm}\label{rmse lstd}


\begin{figure}[htb]
\centering
\includegraphics[width=0.25\textwidth]{fig/RL_LSTD/RL_rmse_B100_0.3.png} 
\includegraphics[width=0.25\textwidth]{fig/RL_LSTD/RL_rmse_B100_0.5.png} 
\includegraphics[width=0.25\textwidth]{fig/RL_LSTD/RL_rmse_B100_0.7.png} \\
\includegraphics[width=0.25\textwidth]{fig/RL_LSTD/RL_rmse_B500_0.3.png} 
\includegraphics[width=0.25\textwidth]{fig/RL_LSTD/RL_rmse_B500_0.5.png} 
\includegraphics[width=0.25\textwidth]{fig/RL_LSTD/RL_rmse_B500_0.7.png} \\
\includegraphics[width=0.25\textwidth]{fig/RL_LSTD/RL_rmse_B1000_0.3.png} 
\includegraphics[width=0.25\textwidth]{fig/RL_LSTD/RL_rmse_B1000_0.5.png} 
\includegraphics[width=0.25\textwidth]{fig/RL_LSTD/RL_rmse_B1000_0.7.png} 
\caption{\it RMSE differences among the predicted policy values using the LSTD algorithm with $ m = 50 $ and $ M = 50 $, evaluated across various values of $ n $, $ B $, and $ k/n $. \textcolor{black}{$\tilde{R}$ denotes the RMSE without experience relay, while $\hat{R}_{U}$ and $\hat{R}_{V}$ represent the RMSE with experience replay.} 
The red line represents the baseline where the variance difference is 0.}
\label{fig:RL_LSTDMSE}
\end{figure}

Figure \ref{fig:RL_LSTDMSE} compares the RMSE of the LSTD algorithm by drawing the boxplots of the differences $\{\tilde{R}_i-\hat{R}_{i, U}\}_{i=1}^M$ and $\{\tilde{R}_i-\hat{R}_{i, V}\}_{i=1}^M$, with regard to different $n, B$, and the ratio $k/n$. We choose the $n\in\{500, 1000, 2500, 5000\}$, $B\in\{100, 500, 1000\}$, and $k/n\in\{0.3,0.5,0.7\}$. The results demonstrate that the combination of experience replay with the LSTD algorithm, regardless of the specific resampling method used, not only reduces variance but also tends to achieve smaller prediction errors, further highlighting its effectiveness.

\subsubsection{Second-Order PED-Based Algorithm}\label{rmse 2nd}
\begin{figure}[htb]
\centering
\includegraphics[width=0.25\textwidth]{fig/RL_2nd/RL_rmse_B100_0.3.png} 
\includegraphics[width=0.25\textwidth]{fig/RL_2nd/RL_rmse_B100_0.5.png} 
\includegraphics[width=0.25\textwidth]{fig/RL_2nd/RL_rmse_B100_0.7.png} \\
\includegraphics[width=0.25\textwidth]{fig/RL_2nd/RL_rmse_B500_0.3.png} 
\includegraphics[width=0.25\textwidth]{fig/RL_2nd/RL_rmse_B500_0.5.png} 
\includegraphics[width=0.25\textwidth]{fig/RL_2nd/RL_rmse_B500_0.7.png} \\
\includegraphics[width=0.25\textwidth]{fig/RL_2nd/RL_rmse_B1000_0.3.png} 
\includegraphics[width=0.25\textwidth]{fig/RL_2nd/RL_rmse_B1000_0.5.png} 
\includegraphics[width=0.25\textwidth]{fig/RL_2nd/RL_rmse_B1000_0.7.png} 
\caption{\it RMSE differences among the predicted policy values using the second-order PDE-based algorithm with $ m = 50 $ and $ M = 50 $, evaluated across various values of $ n $, $ B $, and $ k/n $. \textcolor{black}{$\tilde{R}$ denotes the RMSE without experience relay, while $\hat{R}_{U}$ and $\hat{R}_{V}$ represent the RMSE with experience replay.}  The red line represents the baseline where the variance difference is 0.}
\label{fig:RL_2ndrmse}
\end{figure}


Figure \ref{fig:RL_2ndrmse} compares the RMSE of the second-order PDE-based algorithm by drawing the boxplots of the differences $\{\tilde{{R}}_i-\hat{{R}}_{i, U}\}_{i=1}^M$ and $\{\tilde{{R}}_i-\hat{{R}}_{i, V}\}_{i=1}^M$, with regard to different $n, B$, and the ratio $k/n$. We choose the $n\in\{500, 1000, 2500, 5000\}$, $B\in\{100, 500, 1000\}$, and $k/n\in\{0.3,0.5,0.7\}$. The results show that incorporating experience replay into the PDE-based algorithm not only reduces variances, but consistently reduces prediction errors, demonstrating its effectiveness regardless of resampling method.

\subsubsection{First-Order PED-Based Algorithm}\label{rmse 1st}



\begin{figure}[htb]
\centering
\includegraphics[width=0.25\textwidth]{fig/RL_1st/RL_rmse_B100_0.3.png} 
\includegraphics[width=0.25\textwidth]{fig/RL_1st/RL_rmse_B100_0.5.png} 
\includegraphics[width=0.25\textwidth]{fig/RL_1st/RL_rmse_B100_0.7.png} \\
\includegraphics[width=0.25\textwidth]{fig/RL_1st/RL_rmse_B500_0.3.png} 
\includegraphics[width=0.25\textwidth]{fig/RL_1st/RL_rmse_B500_0.5.png} 
\includegraphics[width=0.25\textwidth]{fig/RL_1st/RL_rmse_B500_0.7.png} \\
\includegraphics[width=0.25\textwidth]{fig/RL_1st/RL_rmse_B1000_0.3.png} 
\includegraphics[width=0.25\textwidth]{fig/RL_1st/RL_rmse_B1000_0.5.png} 
\includegraphics[width=0.25\textwidth]{fig/RL_1st/RL_rmse_B1000_0.7.png} 
\caption{\it RMSE differences among the predicted policy values using the first-order PDE-based algorithm with $ m = 50 $ and $ M = 50 $, evaluated across various values of $ n $, $ B $, and $ k/n $. \textcolor{black}{$\tilde{R}$ denotes the RMSE without experience relay, while $\hat{R}_{U}$ and $\hat{R}_{V}$ represent the RMSE with experience replay.}  The red line represents the baseline where the variance difference is 0.}
\label{fig:RL_1STrmse}
\end{figure}


Figure \ref{fig:RL_1STrmse} compares the RMSE of the first-order PDE-based algorithm by drawing the boxplots of the differences $\{\tilde{{R}}_i-\hat{{R}}_{i, U}\}_{i=1}^M$ and $\{\tilde{{R}}_i-\hat{{R}}_{i, V}\}_{i=1}^M$, with regard to different $n, B$, and the ratio $k/n$. We choose the $n\in\{500, 1000, 2500, 5000\}$, $B\in\{100, 500, 1000\}$, and $k/n\in\{0.3,0.5,0.7\}$. The results further indicate that incorporating experience replay into the PDE-based algorithm not only reduces variance, but also consistently achieves reduced prediction errors, underscoring its effectiveness regardless of the resampling method used.



\subsection{Kernel Ridge Regression}\label{rmse simu}
In the experiment setting of Section \ref{sec:kernel}, for each experiment $i=1,2,\dots, M$, we define the following RMSE over the $m$ test points
\begin{equation}\label{kr rmse}
  \tilde{{R}}_i=\sqrt{\frac{1}{m}\sum_{j=1}^m (\tilde{y}_j^i-y_j)^2}, \quad \hat{{R}}_{i, U}=\sqrt{\frac{1}{m}\sum_{j=1}^m (\hat{y}_{j,U}^i-y_j)^2}, \quad \hat{{R}}_{i, V}=\sqrt{\frac{1}{m}\sum_{j=1}^m (\hat{y}_{j,V}^i-y_j)^2},
\end{equation}
where \(\tilde{y}_j^i\), \(\hat{y}_{j,U}^i\), and \(\hat{y}_{j,V}^i\) denote the predicted values of \(x_j\) using the three methods in the \(i\)-th experiment.



We compare the prediction errors by comparing $\tilde{{R}}_i, \hat{{R}}_{i, U}$, and $\hat{{R}}_{i, V}$ for $i=1,\dots, M$. 



\begin{figure}[ht!]
\centering
\includegraphics[width=0.25\textwidth]{fig/kr/kr_rmse_B25_k15.png} 
\includegraphics[width=0.25\textwidth]{fig/kr/kr_rmse_B25_k20.png} 
\includegraphics[width=0.25\textwidth]{fig/kr/kr_rmse_B25_k25.png} \\
\includegraphics[width=0.25\textwidth]{fig/kr/kr_rmse_B50_k15.png} 
\includegraphics[width=0.25\textwidth]{fig/kr/kr_rmse_B50_k20.png} 
\includegraphics[width=0.25\textwidth]{fig/kr/kr_rmse_B50_k25.png} \\
\includegraphics[width=0.25\textwidth]{fig/kr/kr_rmse_B100_k15.png} 
\includegraphics[width=0.25\textwidth]{fig/kr/kr_rmse_B100_k20.png} 
\includegraphics[width=0.25\textwidth]{fig/kr/kr_rmse_B100_k25.png} 
\caption{\it RMSE differences among the predicted outcome values using the kernel ridge regression with $ m = 100 $ and $ M = 100 $, evaluated across various values of $ n $, $ B $, and $ k/n $. \textcolor{black}{$\tilde{R}$ denotes the RMSE without experience relay, while $\hat{R}_{U}$ and $\hat{R}_{V}$ represent the RMSE with experience replay.}  The red line represents the baseline where the variance difference is 0.}
\label{fig:krmse}
\end{figure}

Figure \ref{fig:krmse} compares the RMSE by drawing the boxplots of the differences $\{\tilde{{R}}_i-\hat{{R}}_{i, U}\}_{i=1}^M$ and $\{\tilde{{R}}_i-\hat{{R}}_{i, V}\}_{i=1}^M$, with regard to different $n, B$, and the ratio $k/n$. We select the $n\in\{ 100, 150, 200, 250\}$, $B\in\{25,50, 100\}$, and $k\in\{10,15, 20\}$. The results indicate that incorporating experience replay not only reduces variance but also decreases errors for all the settings, particularly in data-scarce scenarios.



% \subsubsection{Real Data Analysis}\label{rmse real}

% \begin{figure}[htb]
% \centering
% \includegraphics[width=0.25\textwidth]{fig/real_data/b_rmse_B100_0.5.png} 
% \includegraphics[width=0.25\textwidth]{fig/real_data/b_rmse_B100_0.7.png} 
% \includegraphics[width=0.25\textwidth]{fig/real_data/b_rmse_B100_0.9.png} \\
% \includegraphics[width=0.25\textwidth]{fig/real_data/b_rmse_B300_0.5.png} 
% \includegraphics[width=0.25\textwidth]{fig/real_data/b_rmse_B300_0.7.png} 
% \includegraphics[width=0.25\textwidth]{fig/real_data/b_rmse_B300_0.9.png} \\
% \includegraphics[width=0.25\textwidth]{fig/real_data/b_rmse_B500_0.5.png} 
% \includegraphics[width=0.25\textwidth]{fig/real_data/b_rmse_B500_0.7.png} 
% \includegraphics[width=0.25\textwidth]{fig/real_data/b_rmse_B500_0.9.png} 
% \caption{\it RMSE differences among the predicted outcome values using the kernel ridge regression with $ m = 100 $ and $ M = 100 $, evaluated across various values of $ n $, $ B $, and $ k/n $. \textcolor{blue}{$\tilde{R}$ denotes the RMSE without experience relay and $\hat{R}_{U}$, $\hat{R}_{V}$ represent the RMSE with experience replay.} The red line represents the baseline where the variance difference is 0.}
% \label{fig:kr_realmse}
% \end{figure}


% Figure \ref{fig:kr_realmse} compares the RMSR by drawing the boxplots of the differences $\{\tilde{{R}}_i-\hat{{R}}_{i, U}\}_{i=1}^M$ and $\{\tilde{{R}}_i-\hat{{R}}_{i, V}\}_{i=1}^M$, with regard to different $n, B$, and the ratio $k/n$. We choose the $n\in\{50, 100, 150, 200\}$, $B\in\{100, 300, 500\}$, and $k/n\in\{0.5,0.7,0.9\}$. The results indicate that the RMSE does not increase a lot.


% \subsection{Linear Ridge Regression}\label{rmse lr}

% In the experimental setup described in Section \ref{lr simulation}, for each experiment $i = 1, 2, \dots, M$, we define the RMSE as specified in \eqref{kr rmse}.
% We compare the prediction error by comparing $\tilde{{R}}_i, \hat{{R}}_{i, U}$, and $\hat{{R}}_{i, V}$ for $i=1,\dots, M$.

% \begin{figure}[htb]
% \centering
% \includegraphics[width=0.25\textwidth]{fig/lr/lr_rmse_B100_0.5.png} 
% \includegraphics[width=0.25\textwidth]{fig/lr/lr_rmse_B100_0.7.png} 
% \includegraphics[width=0.25\textwidth]{fig/lr/lr_rmse_B100_0.9.png} \\
% \includegraphics[width=0.25\textwidth]{fig/lr/lr_rmse_B300_0.5.png} 
% \includegraphics[width=0.25\textwidth]{fig/lr/lr_rmse_B300_0.7.png} 
% \includegraphics[width=0.25\textwidth]{fig/lr/lr_rmse_B300_0.9.png} \\
% \includegraphics[width=0.25\textwidth]{fig/lr/lr_rmse_B500_0.5.png} 
% \includegraphics[width=0.25\textwidth]{fig/lr/lr_rmse_B500_0.7.png} 
% \includegraphics[width=0.25\textwidth]{fig/lr/lr_rmse_B500_0.9.png} 
% \caption{\it RMSE differences among the predicted outcome values using the linear ridge regression with $ m = 50 $ and $ M = 50 $, evaluated across various values of $ n $, $ B $, and $ k/n $. \textcolor{blue}{$\tilde{R}$ denotes the RMSE without experience relay and $\hat{R}_{U}$, $\hat{R}_{V}$ represent the RMSE with experience replay.} The red line represents the baseline where the variance difference is 0. }
% \label{fig:lrmse}
% \end{figure}


% Figure \ref{fig:lrmse} compares the RMSR by drawing the boxplots of the differences $\{\tilde{{R}}_i-\hat{{R}}_{i, U}\}_{i=1}^M$ and $\{\tilde{{R}}_i-\hat{{R}}_{i, V}\}_{i=1}^M$, with regard to different $n, B$, and the ratio $k/n$. We choose the $n\in\{150, 200, 250, 300\}$, $B\in\{100, 300, 500\}$, and $k/n\in\{0.5,0.7,0.9\}$. Compared
% to the variance reduction achieved in Figure \ref{fig:lr}, the increase in error is within a reasonable range. The smaller RMSE of the original method can be attributed to the fact that, as shown in \eqref{linear}, the simulation model is inherently a perfect linear model. Consequently, linear ridge regression is naturally the optimal solution in terms of least squares in this setting.

\end{document}