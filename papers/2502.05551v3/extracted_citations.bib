@article{DBLP:journals/corr/abs-2107-14596,
  author       = {Tongtong Liu and
                  Fangxiang Feng and
                  Xiaojie Wang},
  title        = {Multi-stage Pre-training over Simplified Multimodal Pre-training Models},
  journal      = {CoRR},
  volume       = {abs/2107.14596},
  year         = {2021},
  url          = {https://arxiv.org/abs/2107.14596},
  eprinttype    = {arXiv},
  eprint       = {2107.14596},
  timestamp    = {Mon, 16 Aug 2021 17:55:39 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2107-14596.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{JI2025129138,
title = {Multimodal large model pretraining, adaptation and efficiency optimization},
journal = {Neurocomputing},
volume = {619},
pages = {129138},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2024.129138},
url = {https://www.sciencedirect.com/science/article/pii/S092523122401909X},
author = {Lixia Ji and Shijie Xiao and Jingmei Feng and Wenzhao Gao and Han Zhang},
keywords = {Multi-modal learning, Pretrained model, Adaptation fine-tuning, Efficient training},
abstract = {Multimodal large models, leveraging extensive datasets and parameters, have provided superior solutions for multimodal tasks and have been widely applied across various domains in everyday life. However, the development of multimodal large models also faces numerous challenges. Among the most critical challenges are how to scientifically and rationally pre-train multimodal large models and how to apply these pre-trained models to downstream tasks more effectively and at a lower cost. In response to these challenges, this paper initially analyzes and summarizes various multimodal pre-training methods employed by state-of-the-art multimodal large models. By analyzing the characteristics these methods, the paper elucidates the different capabilities imparted to multimodal large models by various multimodal pre-training methods. This provides a basis for researchers to select multimodal pre-training methods according to specific needs. Subsequently, the paper categorizes and outlines general adaptation methods for downstream tasks using multimodal pre-trained models. By comparing their advantages and disadvantages, the distinct characteristic of each method are highlighted. Furthermore, we categorize multimodal downstream tasks into five types. Then, for different types of tasks based on their characteristics and application scenarios, we outline suitable adaptation methods, offering researchers a more definitive direction for adaptation studies related to various types of tasks. Additionally, from the perspectives of parameter efficiency, memory efficiency, and efficiency of data utilization, a comparative analysis of research on the efficiency optimization of multimodal large models is conducted. In conclusion, the paper summarizes the shortcomings of existing research and looks forward to the issues and development directions that need to be addressed for multimodal large models. It points out that integrating more modalities into multimodal large models, reducing resource consumption costs, enhancing model explainability and transparency, and addressing privacy and security issues will be hotspots for future research.}
}

@article{abbas2023semdedup,
  title={Semdedup: Data-efficient learning at web-scale through semantic deduplication},
  author={Abbas, Amro and Tirumala, Kushal and Simig, D{\'a}niel and Ganguli, Surya and Morcos, Ari S},
  journal={arXiv preprint arXiv:2303.09540},
  year={2023}
}

@inproceedings{bengio2009curriculum,
  title={Curriculum learning},
  author={Bengio, Yoshua and Louradour, J{\'e}r{\^o}me and Collobert, Ronan and Weston, Jason},
  booktitle={Proceedings of the 26th annual international conference on machine learning},
  pages={41--48},
  year={2009}
}

@misc{cerebras2023slimpajama,
author = {Soboleva, Daria and Al-Khateeb, Faisal and Myers, Robert and Steeves, Jacob R and Hestness, Joel and Dey, Nolan},
title = {{SlimPajama: A 627B token cleaned and deduplicated version of RedPajama}},
month = {June},
year = {2023},
}

@misc{cheng2024instructionpretraininglanguagemodels,
      title={Instruction Pre-Training: Language Models are Supervised Multitask Learners}, 
      author={Daixuan Cheng and Yuxian Gu and Shaohan Huang and Junyu Bi and Minlie Huang and Furu Wei},
      year={2024},
      eprint={2406.14491},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.14491}, 
}

@inproceedings{collobert2008unified,
  title={A unified architecture for natural language processing: Deep neural networks with multitask learning},
  author={Collobert, Ronan and Weston, Jason},
  booktitle={Proceedings of the 25th international conference on Machine learning},
  pages={160--167},
  year={2008}
}

@article{engstrom2024dsdm,
  title={Dsdm: Model-aware dataset selection with datamodels},
  author={Engstrom, Logan and Feldmann, Axel and Madry, Aleksander},
  journal={arXiv preprint arXiv:2401.12926},
  year={2024}
}

@article{laurenccon2022bigscience,
  title={The bigscience roots corpus: A 1.6 tb composite multilingual dataset},
  author={Lauren{\c{c}}on, Hugo and Saulnier, Lucile and Wang, Thomas and Akiki, Christopher and Villanova del Moral, Albert and Le Scao, Teven and Von Werra, Leandro and Mou, Chenghao and Gonz{\'a}lez Ponferrada, Eduardo and Nguyen, Huu and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={31809--31826},
  year={2022}
}

@inproceedings{lee2022deduplicating,
  title={Deduplicating Training Data Makes Language Models Better},
  author={Lee, Katherine and Ippolito, Daphne and Nystrom, Andrew and Zhang, Chiyuan and Eck, Douglas and Callison-Burch, Chris and Carlini, Nicholas},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={8424--8445},
  year={2022}
}

@article{marion2023less,
  title={When less is more: Investigating data pruning for pretraining llms at scale},
  author={Marion, Max and {\"U}st{\"u}n, Ahmet and Pozzobon, Luiza and Wang, Alex and Fadaee, Marzieh and Hooker, Sara},
  journal={arXiv preprint arXiv:2309.04564},
  year={2023}
}

@article{pavlova2025multi,
  title={Multi-stage Training of Bilingual Islamic LLM for Neural Passage Retrieval},
  author={Pavlova, Vera},
  journal={arXiv preprint arXiv:2501.10175},
  year={2025}
}

@article{penedo2024fineweb,
  title={The FineWeb Datasets: Decanting the Web for the Finest Text Data at Scale},
  author={Penedo, Guilherme and Kydl{\'\i}{\v{c}}ek, Hynek and Lozhkov, Anton and Mitchell, Margaret and Raffel, Colin and Von Werra, Leandro and Wolf, Thomas and others},
  journal={arXiv preprint arXiv:2406.17557},
  year={2024}
}

@article{platanios2019competence,
  title={Competence-based curriculum learning for neural machine translation},
  author={Platanios, Emmanouil Antonios and Stretcu, Otilia and Neubig, Graham and Poczos, Barnabas and Mitchell, Tom M},
  journal={arXiv preprint arXiv:1903.09848},
  year={2019}
}

@article{rae2021scaling,
  title={Scaling language models: Methods, analysis \& insights from training gopher},
  author={Rae, Jack W and Borgeaud, Sebastian and Cai, Trevor and Millican, Katie and Hoffmann, Jordan and Song, Francis and Aslanides, John and Henderson, Sarah and Ring, Roman and Young, Susannah and others},
  journal={arXiv preprint arXiv:2112.11446},
  year={2021}
}

@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={Journal of machine learning research},
  volume={21},
  number={140},
  pages={1--67},
  year={2020}
}

@article{sachdeva2024train,
  title={How to Train Data-Efficient LLMs},
  author={Sachdeva, Noveen and Coleman, Benjamin and Kang, Wang-Cheng and Ni, Jianmo and Hong, Lichan and Chi, Ed H and Caverlee, James and McAuley, Julian and Cheng, Derek Zhiyuan},
  journal={arXiv preprint arXiv:2402.09668},
  year={2024}
}

@article{sorscher2022beyond,
  title={Beyond neural scaling laws: beating power law scaling via data pruning},
  author={Sorscher, Ben and Geirhos, Robert and Shekhar, Shashank and Ganguli, Surya and Morcos, Ari},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={19523--19536},
  year={2022}
}

@inproceedings{tan-etal-2022-msp,
    title = "{MSP}: Multi-Stage Prompting for Making Pre-trained Language Models Better Translators",
    author = "Tan, Zhixing  and
      Zhang, Xiangwen  and
      Wang, Shuo  and
      Liu, Yang",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.424/",
    doi = "10.18653/v1/2022.acl-long.424",
    pages = "6131--6142",
    abstract = "Prompting has recently been shown as a promising approach for applying pre-trained language models to perform downstream tasks. We present Multi-Stage Prompting, a simple and automatic approach for leveraging pre-trained language models to translation tasks. To better mitigate the discrepancy between pre-training and translation, MSP divides the translation process via pre-trained language models into three separate stages: the encoding stage, the re-encoding stage, and the decoding stage. During each stage, we independently apply different continuous prompts for allowing pre-trained language models better shift to translation tasks. We conduct extensive experiments on three translation tasks. Experiments show that our method can significantly improve the translation performance of pre-trained language models."
}

@inproceedings{thakkar2023self,
  title={Self-Influence Guided Data Reweighting for Language Model Pre-training},
  author={Thakkar, Megh and Bolukbasi, Tolga and Ganapathy, Sriram and Vashishth, Shikhar and Chandar, Sarath and Talukdar, Partha},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  pages={2033--2045},
  year={2023}
}

@article{tirumala2024d4,
  title={D4: Improving llm pretraining via document de-duplication and diversification},
  author={Tirumala, Kushal and Simig, Daniel and Aghajanyan, Armen and Morcos, Ari},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@misc{together2023redpajama,
  author = {Together Computer},
  title = {RedPajama: an Open Dataset for Training Large Language Models},
  month = {October},
  year = {2023},
  url = {https://github.com/togethercomputer/RedPajama-Data}
}

@inproceedings{wenzek2020ccnet,
  title={CCNet: Extracting High Quality Monolingual Datasets from Web Crawl Data},
  author={Wenzek, Guillaume and Lachaux, Marie-Anne and Conneau, Alexis and Chaudhary, Vishrav and Guzm{\'a}n, Francisco and Joulin, Armand and Grave, {\'E}douard},
  booktitle={Proceedings of the Twelfth Language Resources and Evaluation Conference},
  pages={4003--4012},
  year={2020}
}

@article{wettig2024qurating,
  title={Qurating: Selecting high-quality data for training language models},
  author={Wettig, Alexander and Gupta, Aatmik and Malik, Saumya and Chen, Danqi},
  journal={arXiv preprint arXiv:2402.09739},
  year={2024}
}

@article{wu2024curriculum,
  title={Curriculum Learning with Quality-Driven Data Selection},
  author={Wu, Biao and Meng, Fang and Chen, Ling},
  journal={arXiv preprint arXiv:2407.00102},
  year={2024}
}

@article{xie2023data,
  title={Data selection for language models via importance resampling},
  author={Xie, Sang Michael and Santurkar, Shibani and Ma, Tengyu and Liang, Percy S},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={34201--34227},
  year={2023}
}

@inproceedings{xu2020curriculum,
  title={Curriculum learning for natural language understanding},
  author={Xu, Benfeng and Zhang, Licheng and Mao, Zhendong and Wang, Quan and Xie, Hongtao and Zhang, Yongdong},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={6095--6104},
  year={2020}
}

@article{yu2024mates,
  title={MATES: Model-Aware Data Selection for Efficient Pretraining with Data Influence Models},
  author={Yu, Zichun and Das, Spandan and Xiong, Chenyan},
  journal={arXiv preprint arXiv:2406.06046},
  year={2024}
}

@inproceedings{zhang2024autonomous,
  title={Autonomous data selection with language models for mathematical texts},
  author={Zhang, Yifan and Luo, Yifan and Yuan, Yang and Yao, Andrew C},
  booktitle={ICLR 2024 Workshop on Navigating and Addressing Data Problems for Foundation Models},
  year={2024}
}

@article{zhao2024slit,
  title={SLIT: Boosting Audio-Text Pre-Training via Multi-Stage Learning and Instruction Tuning},
  author={Zhao, Hang and Xin, Yifei and Yu, Zhesong and Zhu, Bilei and Lu, Lu and Ma, Zejun},
  journal={arXiv preprint arXiv:2402.07485},
  year={2024}
}

