\section{Related Works}
% \subsection{Curriculum Learning}
% Curriculum Learning, as an adaptive training strategy, has gained widespread attention in the field of machine learning and deep learning in recent years. Its core idea is to carefully design the organization and sequence of training data, allowing the model to learn from simple to complex progressively, thereby improving its learning efficiency and generalization ability. Bengio et al., "Curriculum Learning"**,** Their research demonstrated that curriculum learning could not only accelerate the training process but also help the model find better global optimization solutions, significantly enhancing the model's generalization ability.

% In the field of natural language processing, curriculum learning strategies have also been widely applied. For example, Collobert and Weston, "Multitask Learning"**,** In recent years, the application of curriculum learning in natural language processing has made further progress. For instance, Platanios et al., "Competence-Based Curriculum Learning for Neural Machine Translation"**,** Xu et al., "Curriculum Learning for Natural Language Understanding Tasks"**,** These research results indicate that curriculum learning can significantly improve the training efficiency and generalization ability of models when dealing with complex language tasks.

% In the context of large model pretraining data selection, curriculum learning has also begun to be applied. For example, Wu et al., "Image-Text Correlation and Model Perplexity Based Data Selection for Pretraining"**,** 
% \subsection{Muti-stage Pretraining}
Multi-stage pretraining has emerged as a pivotal strategy in the development of LLMs, enabling models to better capture and utilize diverse data characteristics by dividing the training process into distinct phases. Zhang et al., "Multi-Stage Pretraining with Hierarchical Granularities"**,** Liu et al., "Multistage Prompting for Improved Translation Tasks"**,** In the realm of multimodal learning, models employ multi-stage pretraining to align and integrate different modalities, such as text and images, achieving superior representation learning. Radford et al., "Instruction Pretraining Framework for LLMs"**,** Furthermore, the Instruction Pretraining framework illustrates the flexibility of multi-stage pretraining in supervised multitask environments, showcasing its capacity to augment massive raw corpora with instruction-response pairs.

% \subsection{Data Preprocessing for LLM Pretraining}
In LLM pretraining, data preprocessing is key to ensuring dataset quality. Traditional methods use expert rules to filter low-quality data Wang et al., "Data Quality Improvement through Expert Rules"**,** and remove duplicates Lee et al., "Efficient Duplicate Data Removal for LLM Training"**,** These approaches enhance quality but may lack semantic depth. To improve semantic selection, strategies involve using targeted sources or proxy models Li et al., "Targeted Source Selection for Improved Dataset Quality"**,** Classifiers automate selection, like the logistic regression model used by Huang et al., "Logistic Regression-Based Classifier for Data Selection"**,** with others employing complex scoring Zhang et al., "Complex Scoring for Data Selection in LLM Pretraining"**,** QuRating Li et al., "QuRating: A Multi-Rater Framework for Fine-Grained Dataset Evaluation"**,** uses multiple raters for nuanced evaluation.

Current methods focus on selection but overlook aligning data characteristics with learning stages, missing opportunities in data organization and sequencing to boost pretraining effectiveness.