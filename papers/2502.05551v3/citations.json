[
  {
    "index": 0,
    "papers": [
      {
        "key": "bengio2009curriculum",
        "author": "Bengio, Yoshua and Louradour, J{\\'e}r{\\^o}me and Collobert, Ronan and Weston, Jason",
        "title": "Curriculum learning"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "collobert2008unified",
        "author": "Collobert, Ronan and Weston, Jason",
        "title": "A unified architecture for natural language processing: Deep neural networks with multitask learning"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "platanios2019competence",
        "author": "Platanios, Emmanouil Antonios and Stretcu, Otilia and Neubig, Graham and Poczos, Barnabas and Mitchell, Tom M",
        "title": "Competence-based curriculum learning for neural machine translation"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "xu2020curriculum",
        "author": "Xu, Benfeng and Zhang, Licheng and Mao, Zhendong and Wang, Quan and Xie, Hongtao and Zhang, Yongdong",
        "title": "Curriculum learning for natural language understanding"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "wu2024curriculum",
        "author": "Wu, Biao and Meng, Fang and Chen, Ling",
        "title": "Curriculum Learning with Quality-Driven Data Selection"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "pavlova2025multi",
        "author": "Pavlova, Vera",
        "title": "Multi-stage Training of Bilingual Islamic LLM for Neural Passage Retrieval"
      },
      {
        "key": "zhao2024slit",
        "author": "Zhao, Hang and Xin, Yifei and Yu, Zhesong and Zhu, Bilei and Lu, Lu and Ma, Zejun",
        "title": "SLIT: Boosting Audio-Text Pre-Training via Multi-Stage Learning and Instruction Tuning"
      },
      {
        "key": "DBLP:journals/corr/abs-2107-14596",
        "author": "Tongtong Liu and\nFangxiang Feng and\nXiaojie Wang",
        "title": "Multi-stage Pre-training over Simplified Multimodal Pre-training Models"
      },
      {
        "key": "tan-etal-2022-msp",
        "author": "Tan, Zhixing  and\nZhang, Xiangwen  and\nWang, Shuo  and\nLiu, Yang",
        "title": "{MSP}: Multi-Stage Prompting for Making Pre-trained Language Models Better Translators"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "DBLP:journals/corr/abs-2107-14596",
        "author": "Tongtong Liu and\nFangxiang Feng and\nXiaojie Wang",
        "title": "Multi-stage Pre-training over Simplified Multimodal Pre-training Models"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "DBLP:journals/corr/abs-2107-14596",
        "author": "Tongtong Liu and\nFangxiang Feng and\nXiaojie Wang",
        "title": "Multi-stage Pre-training over Simplified Multimodal Pre-training Models"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "tan-etal-2022-msp",
        "author": "Tan, Zhixing  and\nZhang, Xiangwen  and\nWang, Shuo  and\nLiu, Yang",
        "title": "{MSP}: Multi-Stage Prompting for Making Pre-trained Language Models Better Translators"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "tan-etal-2022-msp",
        "author": "Tan, Zhixing  and\nZhang, Xiangwen  and\nWang, Shuo  and\nLiu, Yang",
        "title": "{MSP}: Multi-Stage Prompting for Making Pre-trained Language Models Better Translators"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "JI2025129138",
        "author": "Lixia Ji and Shijie Xiao and Jingmei Feng and Wenzhao Gao and Han Zhang",
        "title": "Multimodal large model pretraining, adaptation and efficiency optimization"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "cheng2024instructionpretraininglanguagemodels",
        "author": "Daixuan Cheng and Yuxian Gu and Shaohan Huang and Junyu Bi and Minlie Huang and Furu Wei",
        "title": "Instruction Pre-Training: Language Models are Supervised Multitask Learners"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "raffel2020exploring",
        "author": "Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J",
        "title": "Exploring the limits of transfer learning with a unified text-to-text transformer"
      },
      {
        "key": "rae2021scaling",
        "author": "Rae, Jack W and Borgeaud, Sebastian and Cai, Trevor and Millican, Katie and Hoffmann, Jordan and Song, Francis and Aslanides, John and Henderson, Sarah and Ring, Roman and Young, Susannah and others",
        "title": "Scaling language models: Methods, analysis \\& insights from training gopher"
      },
      {
        "key": "laurenccon2022bigscience",
        "author": "Lauren{\\c{c}}on, Hugo and Saulnier, Lucile and Wang, Thomas and Akiki, Christopher and Villanova del Moral, Albert and Le Scao, Teven and Von Werra, Leandro and Mou, Chenghao and Gonz{\\'a}lez Ponferrada, Eduardo and Nguyen, Huu and others",
        "title": "The bigscience roots corpus: A 1.6 tb composite multilingual dataset"
      },
      {
        "key": "together2023redpajama",
        "author": "Together Computer",
        "title": "RedPajama: an Open Dataset for Training Large Language Models"
      },
      {
        "key": "penedo2024fineweb",
        "author": "Penedo, Guilherme and Kydl{\\'\\i}{\\v{c}}ek, Hynek and Lozhkov, Anton and Mitchell, Margaret and Raffel, Colin and Von Werra, Leandro and Wolf, Thomas and others",
        "title": "The FineWeb Datasets: Decanting the Web for the Finest Text Data at Scale"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "lee2022deduplicating",
        "author": "Lee, Katherine and Ippolito, Daphne and Nystrom, Andrew and Zhang, Chiyuan and Eck, Douglas and Callison-Burch, Chris and Carlini, Nicholas",
        "title": "Deduplicating Training Data Makes Language Models Better"
      },
      {
        "key": "sorscher2022beyond",
        "author": "Sorscher, Ben and Geirhos, Robert and Shekhar, Shashank and Ganguli, Surya and Morcos, Ari",
        "title": "Beyond neural scaling laws: beating power law scaling via data pruning"
      },
      {
        "key": "abbas2023semdedup",
        "author": "Abbas, Amro and Tirumala, Kushal and Simig, D{\\'a}niel and Ganguli, Surya and Morcos, Ari S",
        "title": "Semdedup: Data-efficient learning at web-scale through semantic deduplication"
      },
      {
        "key": "cerebras2023slimpajama",
        "author": "Soboleva, Daria and Al-Khateeb, Faisal and Myers, Robert and Steeves, Jacob R and Hestness, Joel and Dey, Nolan",
        "title": "{SlimPajama: A 627B token cleaned and deduplicated version of RedPajama}"
      },
      {
        "key": "tirumala2024d4",
        "author": "Tirumala, Kushal and Simig, Daniel and Aghajanyan, Armen and Morcos, Ari",
        "title": "D4: Improving llm pretraining via document de-duplication and diversification"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "wenzek2020ccnet",
        "author": "Wenzek, Guillaume and Lachaux, Marie-Anne and Conneau, Alexis and Chaudhary, Vishrav and Guzm{\\'a}n, Francisco and Joulin, Armand and Grave, {\\'E}douard",
        "title": "CCNet: Extracting High Quality Monolingual Datasets from Web Crawl Data"
      },
      {
        "key": "xie2023data",
        "author": "Xie, Sang Michael and Santurkar, Shibani and Ma, Tengyu and Liang, Percy S",
        "title": "Data selection for language models via importance resampling"
      },
      {
        "key": "marion2023less",
        "author": "Marion, Max and {\\\"U}st{\\\"u}n, Ahmet and Pozzobon, Luiza and Wang, Alex and Fadaee, Marzieh and Hooker, Sara",
        "title": "When less is more: Investigating data pruning for pretraining llms at scale"
      },
      {
        "key": "thakkar2023self",
        "author": "Thakkar, Megh and Bolukbasi, Tolga and Ganapathy, Sriram and Vashishth, Shikhar and Chandar, Sarath and Talukdar, Partha",
        "title": "Self-Influence Guided Data Reweighting for Language Model Pre-training"
      },
      {
        "key": "engstrom2024dsdm",
        "author": "Engstrom, Logan and Feldmann, Axel and Madry, Aleksander",
        "title": "Dsdm: Model-aware dataset selection with datamodels"
      },
      {
        "key": "yu2024mates",
        "author": "Yu, Zichun and Das, Spandan and Xiong, Chenyan",
        "title": "MATES: Model-Aware Data Selection for Efficient Pretraining with Data Influence Models"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "du2022glam",
        "author": "Du, Nan and Huang, Yanping and Dai, Andrew M and Tong, Simon and Lepikhin, Dmitry and Xu, Yuanzhong and Krikun, Maxim and Zhou, Yanqi and Yu, Adams Wei and Firat, Orhan and others",
        "title": "Glam: Efficient scaling of language models with mixture-of-experts"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "du2022glam",
        "author": "Du, Nan and Huang, Yanping and Dai, Andrew M and Tong, Simon and Lepikhin, Dmitry and Xu, Yuanzhong and Krikun, Maxim and Zhou, Yanqi and Yu, Adams Wei and Firat, Orhan and others",
        "title": "Glam: Efficient scaling of language models with mixture-of-experts"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "zhang2024autonomous",
        "author": "Zhang, Yifan and Luo, Yifan and Yuan, Yang and Yao, Andrew C",
        "title": "Autonomous data selection with language models for mathematical texts"
      },
      {
        "key": "sachdeva2024train",
        "author": "Sachdeva, Noveen and Coleman, Benjamin and Kang, Wang-Cheng and Ni, Jianmo and Hong, Lichan and Chi, Ed H and Caverlee, James and McAuley, Julian and Cheng, Derek Zhiyuan",
        "title": "How to Train Data-Efficient LLMs"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "wettig2024qurating",
        "author": "Wettig, Alexander and Gupta, Aatmik and Malik, Saumya and Chen, Danqi",
        "title": "Qurating: Selecting high-quality data for training language models"
      }
    ]
  }
]