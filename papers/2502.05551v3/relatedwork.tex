\section{Related Works}
% \subsection{Curriculum Learning}
% Curriculum Learning, as an adaptive training strategy, has gained widespread attention in the field of machine learning and deep learning in recent years. Its core idea is to carefully design the organization and sequence of training data, allowing the model to learn from simple to complex progressively, thereby improving its learning efficiency and generalization ability. Bengio et al. formally proposed the concept of curriculum learning in their pioneering work and validated its effectiveness in visual and language tasks through a series of experiments \cite{bengio2009curriculum}. Their research demonstrated that curriculum learning could not only accelerate the training process but also help the model find better global optimization solutions, significantly enhancing the model's generalization ability.

% In the field of natural language processing, curriculum learning strategies have also been widely applied. For example, Collobert and Weston proposed a multitask learning framework that improved the performance of word embedding models through curriculum learning strategies \cite{collobert2008unified}. In recent years, the application of curriculum learning in natural language processing has made further progress. For instance, Platanios et al. proposed a competence-based curriculum learning method for neural machine translation, significantly reducing training time and improving translation quality \cite{platanios2019competence}. Xu et al. applied curriculum learning in natural language understanding tasks, significantly enhancing model performance and convergence speed \cite{xu2020curriculum}. These research results indicate that curriculum learning can significantly improve the training efficiency and generalization ability of models when dealing with complex language tasks.

% In the context of large model pretraining data selection, curriculum learning has also begun to be applied. For example, Wu et al. proposed a method that uses image-text correlation and model perplexity to evaluate and select data of different quality, significantly enhancing the fine-tuning performance of the model \cite{wu2024curriculum}.
% \subsection{Muti-stage Pretraining}
Multi-stage pretraining has emerged as a pivotal strategy in the development of LLMs, enabling models to better capture and utilize diverse data characteristics by dividing the training process into distinct phases\cite{pavlova2025multi,zhao2024slit,DBLP:journals/corr/abs-2107-14596,tan-etal-2022-msp}. \citeauthor{DBLP:journals/corr/abs-2107-14596} (\citeyear{DBLP:journals/corr/abs-2107-14596}) propose a multi-stage pretraining method that leverages various granularities of information, significantly boosting model performance. \citeauthor{tan-etal-2022-msp} (\citeyear{tan-etal-2022-msp}) explore the use of multi-stage prompting to improve translation tasks, demonstrating its effectiveness in enhancing downstream applications.

% In the realm of multimodal learning, models employ multi-stage pretraining to align and integrate different modalities, such as text and images, achieving superior representation learning\cite{JI2025129138}. Furthermore, the Instruction Pretraining framework illustrates the flexibility of multi-stage pretraining in supervised multitask environments, showcasing its capacity to augment massive raw corpora with instruction-response pairs\cite{cheng2024instructionpretraininglanguagemodels}.


% \subsection{Data Preprocessing for LLM Pretraining}
In LLM pretraining, data preprocessing is key to ensuring dataset quality. Traditional methods use expert rules to filter low-quality data \cite{raffel2020exploring, rae2021scaling, laurenccon2022bigscience, together2023redpajama, penedo2024fineweb} and remove duplicates \cite{lee2022deduplicating, sorscher2022beyond, abbas2023semdedup, cerebras2023slimpajama, tirumala2024d4}. These approaches enhance quality but may lack semantic depth. To improve semantic selection, strategies involve using targeted sources or proxy models \cite{wenzek2020ccnet, xie2023data, marion2023less, thakkar2023self, engstrom2024dsdm, yu2024mates}. Classifiers automate selection, like the logistic regression model used by \citeauthor{du2022glam} (\citeyear{du2022glam}), with others employing complex scoring \cite{zhang2024autonomous, sachdeva2024train}. QuRating \cite{wettig2024qurating} uses multiple raters for nuanced evaluation.

Current methods focus on selection but overlook aligning data characteristics with learning stages, missing opportunities in data organization and sequencing to boost pretraining effectiveness.