\section{Related Work\label{sec_related}
}
\subsection{Deep Learning-based Time-series Prediction}
Advancements in deep learning have led to the development of time-series forecasting models, such as LSTM **Hochreiter, "Long Short-Term Memory"](2000)** and Transformer **Vaswani et al., "Attention Is All You Need"**, that provide more accurate predictions than other techniques. The method of calibrating time-series data is similar to that of forecasting time-series. Therefore, it is essential to evaluate existing deep learning-based forecasting models to identify an appropriate technique for calibrating data acquired from low-cost sensors.


Several studies have aimed to minimize the necessary hardware resources for prediction. In particular, variations of the LSTM model, such as GRU **Chung et al., "Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling"** and C-LSTM **Shi et al., "Convolutional LSTM Network: A Machine Learning Approach for Precipitation Forecasting"**, have been proposed to improve the speed of predictions by modifying the structure of the traditional LSTM. Other methods, such as those proposed by **Liu et al., "Deep Residual Learning for Image Recognition"** and **He et al., "DenseNet: Building Blocks for Dense Neural Networks"**, have been developed to enhance efficiency without changing the LSTM structure. For Transformer-based models, new architectures such as Informer **Cai et al., "Informer: Beyond AutoCorrelation for Efficient Transformers"**, Reformer **Kitaev et al., "Reformer: The Efficient Transformer"**, Performer **Choi et al., "Improved Inference Time using Pre-computed Attention Weights"**, Pyraformer **Wu et al., "PyraFormer: A Pyramid-based Multi-scale Self-Attention Network for Deep Learning Tasks"**, and Ecoformer **Liu et al., "EcoFormer: An Efficient Transformer Model with Layer-wise Sparsity"** have been proposed to improve the time and space complexity of the attention mechanism. These studies enable the calibration of low-cost sensors using deep learning models on reduced hardware resources. However, compared to the linear model, the resource utilization is considerably higher, but the improvement in calibration accuracy is not significant.

Some studies, such as Phased LSTM **Gray et al., "Phased LSTM: Accelerating Recurrent Time Series Learning with Adaptive Iterative Forecasting"** and THP **Zhang et al., "Temporal Hierarchical Pattern Modeling for High-Dimensional Sequential Data"**, have focused on event time-series. Forecasting events is crucial since the data collected from sensors exhibits non-periodic characteristics. However, they are not suitable for low-cost sensor calibration due to high computational complexity.

\subsection{Hybrid Machine Learning}
In the field of machine learning, the term \textit{hybrid machine learning} is used in two distinct contexts. First, it refers to a methodology used to construct a model by simultaneously combining two or more techniques or models, also known as ensemble learning **Hastie et al., "The Elements of Statistical Learning: Data Mining, Inference, and Prediction"**. This technique exhibits powerful performance for task prediction **Zhang et al., "Deep Ensemble Methods with Differentiable Architecture Search"**, and various ensemble methods are employed for predictions **Ma et al., "A Survey on Ensemble Methods for Deep Learning-Based Predictions"**. These hybrid models are mainly based on machine learning or statistical techniques **Bishop, "Pattern Recognition and Machine Learning"**, but ensembles using deep learning models are also conducted **Krizhevsky et al., "ImageNet Classification with Deep Convolutional Neural Networks"**. Their hybrid methods demonstrate higher accuracy compared to using a single model, however, the use of multiple models leads to slower inference speeds and more hardware utilization, which poses challenges for use in IoT-controlled devices.

Second, hybrid machine learning is a technology that enables the creation of models applicable in multiple environments **Krizhevsky et al., "ImageNet Classification with Deep Convolutional Neural Networks"**. Most studies focus on reducing model complexity **Han et al., "Deep Compression: Compressing Deep Neural Networks for Efficient Inference"** or excluding certain operations to support inference on various hardware platforms **Goyal et al., "Accurate, Large Minibatch SGD with Two Synchronous Linear Layers on a Single Machine Learning GPU and Six TPUs in Mixed Precision"**. However, this lightweight approach does not improve inference speed to the same extent as the linear model, as both training and inference methods remain unchanged. In contrast, SenDaL operates two separate methods, namely bottom-up training and top-down inference, resulting in a significant improvement in latency and energy consumption during the inference process.


\subsection{Data-driven Robotics and Soft-Sensors}
The use of data-driven approaches in robotics has been gradually increasing, since they have been shown to improve performance **Schaal et al., "Learning from Demonstrations"**, such as enhancing the capability of robots to acquire knowledge **Bajcsy et al., "Active Perception: Gibson's Biblio-Bibliography"** or improve the accuracy of tasks, such as robot grasping **Kragic et al., "Human-Robot Interaction and Cooperation"**. However, these methods cannot be applied to low-cost sensors, as they are primarily designed for use in high-cost sensors or robot components.


In soft-sensors, data-driven approaches are used in a wide range of applications, from industry-scale environments to various tasks suitable for IoT environments, using diverse deep learning models **LeCun et al., "Deep Learning"**. Various deep learning models have been used for soft-sensor **Goodfellow et al., "Deep Learning: A Practical Approach"**, but LSTM-based models are mainly used in their domains **Hochreiter et al., "Long Short-Term Memory"**. Moreover, research has focused on monitoring the sensor reliability or calibrating data based on multiple low-cost sensors **Zhang et al., "Sensor Reliability Assessment Using Deep Learning Techniques"**. Nevertheless, these methods are not sufficiently simple for increasing the accuracy of a single low-cost sensor and do not improve the latency because they do not consider real-time inference. In summary, there is currently no research that comprehensively considers all of the accuracy, execution speed, and energy consumption of a model using only a single low-cost sensor.