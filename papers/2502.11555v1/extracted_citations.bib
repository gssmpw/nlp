@inproceedings{chakraborty2024maxminrlhf,
title={MaxMin-{RLHF}: Towards Equitable Alignment of Large Language Models with Diverse Human Preferences},
author={Souradip Chakraborty and Jiahao Qiu and Hui Yuan and Alec Koppel and Furong Huang and Dinesh Manocha and Amrit Bedi and Mengdi Wang},
booktitle={ICML 2024 Workshop on Models of Human Feedback for AI Alignment},
year={2024},
}

@article{christiano2017deep,
  title={Deep reinforcement learning from human preferences},
  author={Christiano, Paul F and Leike, Jan and Brown, Tom and Martic, Miljan and Legg, Shane and Amodei, Dario},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@misc{dai2023saferlhfsafereinforcement,
      title={Safe RLHF: Safe Reinforcement Learning from Human Feedback}, 
      author={Josef Dai and Xuehai Pan and Ruiyang Sun and Jiaming Ji and Xinbo Xu and Mickel Liu and Yizhou Wang and Yaodong Yang},
      year={2023},
      eprint={2310.12773},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2310.12773}, 
}

@inproceedings{dong2023steerlm,
  title={SteerLM: Attribute Conditioned SFT as an (User-Steerable) Alternative to RLHF},
  author={Dong, Yi and Wang, Zhilin and Sreedhar, Makesh Narsimhan and Wu, Xianchao and Kuchaiev, Oleksii},
  booktitle={The 2023 Conference on Empirical Methods in Natural Language Processing},
  year={2023}
}

@article{ganguli2022red,
  title={Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned},
  author={Ganguli, Deep and Lovitt, Liane and Kernion, Jackson and Askell, Amanda and Bai, Yuntao and Kadavath, Saurav and Mann, Ben and Perez, Ethan and Schiefer, Nicholas and Ndousse, Kamal and others},
  journal={arXiv preprint arXiv:2209.07858},
  year={2022}
}

@article{ge2023mart,
  title={Mart: Improving llm safety with multi-round automatic red-teaming},
  author={Ge, Suyu and Zhou, Chunting and Hou, Rui and Khabsa, Madian and Wang, Yi-Chia and Wang, Qifan and Han, Jiawei and Mao, Yuning},
  journal={arXiv preprint arXiv:2311.07689},
  year={2023}
}

@inproceedings{hendrycks2021aligning,
  title={Aligning AI With Shared Human Values},
  author={Hendrycks, Dan and Burns, Collin and Basart, Steven and Critch, Andrew Critch and Li, Jerry Li and Song, Dawn and Steinhardt, Jacob},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@article{heston2023safety,
  title={Safety of large language models in addressing depression},
  author={Heston, Thomas F},
  journal={Cureus},
  volume={15},
  number={12},
  year={2023},
  publisher={Cureus Inc.}
}

@misc{ji2024alignerefficientalignmentlearning,
      title={Aligner: Efficient Alignment by Learning to Correct}, 
      author={Jiaming Ji and Boyuan Chen and Hantao Lou and Donghai Hong and Borong Zhang and Xuehai Pan and Juntao Dai and Tianyi Qiu and Yaodong Yang},
      year={2024},
      eprint={2402.02416},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.02416}, 
}

@article{ji2024beavertails,
  title={Beavertails: Towards improved safety alignment of llm via a human-preference dataset},
  author={Ji, Jiaming and Liu, Mickel and Dai, Josef and Pan, Xuehai and Zhang, Chi and Bian, Ce and Chen, Boyuan and Sun, Ruiyang and Wang, Yizhou and Yang, Yaodong},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{llama2,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@article{murule,
  title={Rule Based Rewards for Language Model Safety},
  author={Mu, Tong and Helyar, Alec and Heidecke, Johannes and Achiam, Joshua and Vallone, Andrea and Kivlichan, Ian and Lin, Molly and Beutel, Alex and Schulman, John and Weng, Lilian},
  publisher={OpenAI},
year={2024}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@article{perez2022red,
  title={Red teaming language models with language models},
  author={Perez, Ethan and Huang, Saffron and Song, Francis and Cai, Trevor and Ring, Roman and Aslanides, John and Glaese, Amelia and McAleese, Nat and Irving, Geoffrey},
  journal={arXiv preprint arXiv:2202.03286},
  year={2022}
}

@inproceedings{rame2023rewardedsoups,
  title   = {Rewarded soups: towards Pareto-optimal alignment by interpolating weights fine-tuned on diverse rewards},
  author  = {Ram{\'e}, Alexandre and Couairon, Guillaume and Shukor, Mustafa and Dancette, Corentin and Gaya, Jean-Baptiste and Soulier, Laure and Cord, Matthieu},
  year    = {2023},
  booktitle = {NeurIPS},
}

@article{stiennon2020learning,
  title={Learning to summarize with human feedback},
  author={Stiennon, Nisan and Ouyang, Long and Wu, Jeffrey and Ziegler, Daniel and Lowe, Ryan and Voss, Chelsea and Radford, Alec and Amodei, Dario and Christiano, Paul F},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={3008--3021},
  year={2020}
}

@inproceedings{wang2024arithmetic,
      title={Arithmetic Control of LLMs for Diverse User Preferences: Directional Preference Alignment with Multi-Objective Rewards}, 
      author={Haoxiang Wang and Yong Lin and Wei Xiong and Rui Yang and Shizhe Diao and Shuang Qiu and Han Zhao and Tong Zhang},
      year={2024},
      booktitle={ACL},
}

@inproceedings{wu2024llm,
  title={LLM-Based Empathetic Response Through Psychologist-Agent Debate},
  author={Wu, Yijie and Feng, Shi and Wang, Ming and Wang, Daling and Zhang, Yifei},
  booktitle={Asia-Pacific Web (APWeb) and Web-Age Information Management (WAIM) Joint International Conference on Web and Big Data},
  pages={201--215},
  year={2024},
  organization={Springer}
}

@misc{zhang2024bifactorialpreferenceoptimizationbalancing,
      title={Bi-Factorial Preference Optimization: Balancing Safety-Helpfulness in Language Models}, 
      author={Wenxuan Zhang and Philip H. S. Torr and Mohamed Elhoseiny and Adel Bibi},
      year={2024},
      eprint={2408.15313},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2408.15313}, 
}

@article{zhou2023beyond,
  title={Beyond One-Preference-Fits-All Alignment: Multi-Objective Direct Preference Optimization},
  author={Zhou, Zhanhui and Liu, Jie and Yang, Chao and Shao, Jing and Liu, Yu and Yue, Xiangyu and Ouyang, Wanli and Qiao, Yu},
  journal={arXiv preprint ArXiv:2310.03708},
  year={2023}
}

