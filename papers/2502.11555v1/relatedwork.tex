\section{Related Work}
% \subsection{Preference Alignment of LLMs} 
% Traditional language model alignment methods~\cite{christiano2017deep, stiennon2020learning, hendrycks2021aligning} often rely on a single reward or unified preference model. However, recent research highlights that human preferences are inherently diverse and cannot be adequately captured by a single reward model. To address this limitation, Chakraborty~\cite{chakraborty2024maxminrlhf} propose learning a mixture distribution for rewards using the EM algorithm, which serves as the foundation for their MaxMin RLHF approach. Other studies~\cite{dong2023steerlm, rame2023rewardedsoups, wang2024arithmetic} have focused on training multi-objective reward models to enhance the alignment stage, primarily targeting improvements in RL-based alignment techniques. Among supervised alignment methods, Zhou~\cite{zhou2023beyond} present a related approach that advocates for direct policy optimization while still relying on reward model training. In contrast to these methods that focuses on improving reward objectives, our work focuses on the role of safety data in the LLM alignment.

\subsection{Safety Alignment of LLMs} 
Ensuring the safety and ethical alignment of large language models (LLMs) requires a balance between human-guided methodologies and innovative automated approaches. Traditional methods like supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF) rely heavily on human involvement, utilizing curated datasets, red teaming, and reward modeling to align model behavior with ethical standards~\citep{stiennon2020learning, ouyang2022training, llama2, dai2024safe, ji2024beavertails}. While effective, these approaches face limitations in scalability and efficiency due to their dependence on extensive human annotation and oversight. To address these challenges, recent advancements have explored automated techniques, such as rule-based rewards~\citep{murule} and generation-aware alignment~\citep{huang2024catastrophic}, which reduce reliance on human intervention. Building on these efforts, a fully automated framework offers a promising alternative by eliminating the need for human-curated safety data~\citep{murule, huang2024catastrophic}, enabling adaptive alignment across diverse generative scenarios. This automation not only complements existing methodologies but also pushes the boundaries of scalable and context-aware safety alignment, paving the way for LLMs that are both robust and ethically reliable across a wide range of applications~\citep{stiennon2020learning, ouyang2022training, llama2, dai2024safe, ji2024beavertails}.

\subsection{Balance Between Helpfulness and Harmlessness of LLMs} 
Several studies focus on finding the right balance between LLM helpfulness and safety. Some work~\cite{ji2024alignerefficientalignmentlearning} suggests improving responses during reasoning by using an extra model called a residual aligner. Safe-RLHF~\cite{dai2023saferlhfsafereinforcement} achieves preference alignment under safety constraints, and subsequent works~\cite{zhang2024bifactorialpreferenceoptimizationbalancing} have been proposed to further improve the reward objective. Other research has explored improving safety through psychological techniques~\cite{heston2023safety, wu2024llm} and red teaming methods~\cite{ge2023mart, perez2022red, ganguli2022red}. However, all of the previous works overlook the role of safety data in safety alignment. Our work aims to bridges this gap and balance the safety-helpfulness trade-off by analyzing and curating the safety dataset.