[
  {
    "index": 0,
    "papers": [
      {
        "key": "christiano2017deep",
        "author": "Christiano, Paul F and Leike, Jan and Brown, Tom and Martic, Miljan and Legg, Shane and Amodei, Dario",
        "title": "Deep reinforcement learning from human preferences"
      },
      {
        "key": "stiennon2020learning",
        "author": "Stiennon, Nisan and Ouyang, Long and Wu, Jeffrey and Ziegler, Daniel and Lowe, Ryan and Voss, Chelsea and Radford, Alec and Amodei, Dario and Christiano, Paul F",
        "title": "Learning to summarize with human feedback"
      },
      {
        "key": "hendrycks2021aligning",
        "author": "Hendrycks, Dan and Burns, Collin and Basart, Steven and Critch, Andrew Critch and Li, Jerry Li and Song, Dawn and Steinhardt, Jacob",
        "title": "Aligning AI With Shared Human Values"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "chakraborty2024maxminrlhf",
        "author": "Souradip Chakraborty and Jiahao Qiu and Hui Yuan and Alec Koppel and Furong Huang and Dinesh Manocha and Amrit Bedi and Mengdi Wang",
        "title": "MaxMin-{RLHF}: Towards Equitable Alignment of Large Language Models with Diverse Human Preferences"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "dong2023steerlm",
        "author": "Dong, Yi and Wang, Zhilin and Sreedhar, Makesh Narsimhan and Wu, Xianchao and Kuchaiev, Oleksii",
        "title": "SteerLM: Attribute Conditioned SFT as an (User-Steerable) Alternative to RLHF"
      },
      {
        "key": "rame2023rewardedsoups",
        "author": "Ram{\\'e}, Alexandre and Couairon, Guillaume and Shukor, Mustafa and Dancette, Corentin and Gaya, Jean-Baptiste and Soulier, Laure and Cord, Matthieu",
        "title": "Rewarded soups: towards Pareto-optimal alignment by interpolating weights fine-tuned on diverse rewards"
      },
      {
        "key": "wang2024arithmetic",
        "author": "Haoxiang Wang and Yong Lin and Wei Xiong and Rui Yang and Shizhe Diao and Shuang Qiu and Han Zhao and Tong Zhang",
        "title": "Arithmetic Control of LLMs for Diverse User Preferences: Directional Preference Alignment with Multi-Objective Rewards"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "zhou2023beyond",
        "author": "Zhou, Zhanhui and Liu, Jie and Yang, Chao and Shao, Jing and Liu, Yu and Yue, Xiangyu and Ouyang, Wanli and Qiao, Yu",
        "title": "Beyond One-Preference-Fits-All Alignment: Multi-Objective Direct Preference Optimization"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "stiennon2020learning",
        "author": "Stiennon, Nisan and Ouyang, Long and Wu, Jeffrey and Ziegler, Daniel and Lowe, Ryan and Voss, Chelsea and Radford, Alec and Amodei, Dario and Christiano, Paul F",
        "title": "Learning to summarize with human feedback"
      },
      {
        "key": "ouyang2022training",
        "author": "Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "key": "llama2",
        "author": "Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others",
        "title": "Llama 2: Open foundation and fine-tuned chat models"
      },
      {
        "key": "dai2024safe",
        "author": "Josef Dai and Xuehai Pan and Ruiyang Sun and Jiaming Ji and Xinbo Xu and Mickel Liu and Yizhou Wang and Yaodong Yang",
        "title": "Safe {RLHF}: Safe Reinforcement Learning from Human Feedback"
      },
      {
        "key": "ji2024beavertails",
        "author": "Ji, Jiaming and Liu, Mickel and Dai, Josef and Pan, Xuehai and Zhang, Chi and Bian, Ce and Chen, Boyuan and Sun, Ruiyang and Wang, Yizhou and Yang, Yaodong",
        "title": "Beavertails: Towards improved safety alignment of llm via a human-preference dataset"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "murule",
        "author": "Mu, Tong and Helyar, Alec and Heidecke, Johannes and Achiam, Joshua and Vallone, Andrea and Kivlichan, Ian and Lin, Molly and Beutel, Alex and Schulman, John and Weng, Lilian",
        "title": "Rule Based Rewards for Language Model Safety"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "huang2024catastrophic",
        "author": "Yangsibo Huang and Samyak Gupta and Mengzhou Xia and Kai Li and Danqi Chen",
        "title": "Catastrophic Jailbreak of Open-source {LLM}s via Exploiting Generation"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "murule",
        "author": "Mu, Tong and Helyar, Alec and Heidecke, Johannes and Achiam, Joshua and Vallone, Andrea and Kivlichan, Ian and Lin, Molly and Beutel, Alex and Schulman, John and Weng, Lilian",
        "title": "Rule Based Rewards for Language Model Safety"
      },
      {
        "key": "huang2024catastrophic",
        "author": "Yangsibo Huang and Samyak Gupta and Mengzhou Xia and Kai Li and Danqi Chen",
        "title": "Catastrophic Jailbreak of Open-source {LLM}s via Exploiting Generation"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "stiennon2020learning",
        "author": "Stiennon, Nisan and Ouyang, Long and Wu, Jeffrey and Ziegler, Daniel and Lowe, Ryan and Voss, Chelsea and Radford, Alec and Amodei, Dario and Christiano, Paul F",
        "title": "Learning to summarize with human feedback"
      },
      {
        "key": "ouyang2022training",
        "author": "Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "key": "llama2",
        "author": "Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others",
        "title": "Llama 2: Open foundation and fine-tuned chat models"
      },
      {
        "key": "dai2024safe",
        "author": "Josef Dai and Xuehai Pan and Ruiyang Sun and Jiaming Ji and Xinbo Xu and Mickel Liu and Yizhou Wang and Yaodong Yang",
        "title": "Safe {RLHF}: Safe Reinforcement Learning from Human Feedback"
      },
      {
        "key": "ji2024beavertails",
        "author": "Ji, Jiaming and Liu, Mickel and Dai, Josef and Pan, Xuehai and Zhang, Chi and Bian, Ce and Chen, Boyuan and Sun, Ruiyang and Wang, Yizhou and Yang, Yaodong",
        "title": "Beavertails: Towards improved safety alignment of llm via a human-preference dataset"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "ji2024alignerefficientalignmentlearning",
        "author": "Jiaming Ji and Boyuan Chen and Hantao Lou and Donghai Hong and Borong Zhang and Xuehai Pan and Juntao Dai and Tianyi Qiu and Yaodong Yang",
        "title": "Aligner: Efficient Alignment by Learning to Correct"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "dai2023saferlhfsafereinforcement",
        "author": "Josef Dai and Xuehai Pan and Ruiyang Sun and Jiaming Ji and Xinbo Xu and Mickel Liu and Yizhou Wang and Yaodong Yang",
        "title": "Safe RLHF: Safe Reinforcement Learning from Human Feedback"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "zhang2024bifactorialpreferenceoptimizationbalancing",
        "author": "Wenxuan Zhang and Philip H. S. Torr and Mohamed Elhoseiny and Adel Bibi",
        "title": "Bi-Factorial Preference Optimization: Balancing Safety-Helpfulness in Language Models"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "heston2023safety",
        "author": "Heston, Thomas F",
        "title": "Safety of large language models in addressing depression"
      },
      {
        "key": "wu2024llm",
        "author": "Wu, Yijie and Feng, Shi and Wang, Ming and Wang, Daling and Zhang, Yifei",
        "title": "LLM-Based Empathetic Response Through Psychologist-Agent Debate"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "ge2023mart",
        "author": "Ge, Suyu and Zhou, Chunting and Hou, Rui and Khabsa, Madian and Wang, Yi-Chia and Wang, Qifan and Han, Jiawei and Mao, Yuning",
        "title": "Mart: Improving llm safety with multi-round automatic red-teaming"
      },
      {
        "key": "perez2022red",
        "author": "Perez, Ethan and Huang, Saffron and Song, Francis and Cai, Trevor and Ring, Roman and Aslanides, John and Glaese, Amelia and McAleese, Nat and Irving, Geoffrey",
        "title": "Red teaming language models with language models"
      },
      {
        "key": "ganguli2022red",
        "author": "Ganguli, Deep and Lovitt, Liane and Kernion, Jackson and Askell, Amanda and Bai, Yuntao and Kadavath, Saurav and Mann, Ben and Perez, Ethan and Schiefer, Nicholas and Ndousse, Kamal and others",
        "title": "Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned"
      }
    ]
  }
]