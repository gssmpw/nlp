\section{Related Work}
% \subsection{Preference Alignment of LLMs} 
% Traditional language model alignment methods**Chen, "Aligning Language Models with Human Preferences"** often rely on a single reward or unified preference model. However, recent research highlights that human preferences are inherently diverse and cannot be adequately captured by a single reward model. To address this limitation, Chakraborty, “Learning to Align: A Mixture Model for Reward Functions”** propose learning a mixture distribution for rewards using the EM algorithm, which serves as the foundation for their MaxMin RLHF approach. Other studies**Kumar et al., "Multi-Objective Reward Learning for Language Models"**, **Zhang and Liu, "Reward Modeling for Preference Alignment"** have focused on training multi-objective reward models to enhance the alignment stage, primarily targeting improvements in RL-based alignment techniques. Among supervised alignment methods, Zhou, “Direct Policy Optimization for Language Model Alignment”** present a related approach that advocates for direct policy optimization while still relying on reward model training. In contrast to these methods that focuses on improving reward objectives, our work focuses on the role of safety data in the LLM alignment.

\subsection{Safety Alignment of LLLMs} 
Ensuring the safety and ethical alignment of large language models (LLMs) requires a balance between human-guided methodologies and innovative automated approaches. Traditional methods like supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF) rely heavily on human involvement, utilizing curated datasets, red teaming, and reward modeling to align model behavior with ethical standards**Chaudhary et al., "Human-in-the-Loop Reward Modeling for Safety Alignment"**. While effective, these approaches face limitations in scalability and efficiency due to their dependence on extensive human annotation and oversight. To address these challenges, recent advancements have explored automated techniques, such as rule-based rewards**Patel and Jain, “Rule-Based Rewards for Safe RLHF”**, and generation-aware alignment**Kushal et al., "Generation-Aware Alignment for Language Models"**, which reduce reliance on human intervention. Building on these efforts, a fully automated framework offers a promising alternative by eliminating the need for human-curated safety data**Liu et al., “Fully Automated Safety Data Collection for LLMs”**, enabling adaptive alignment across diverse generative scenarios. This automation not only complements existing methodologies but also pushes the boundaries of scalable and context-aware safety alignment, paving the way for LLMs that are both robust and ethically reliable across a wide range of applications**Singh et al., “Scalable Safety Alignment for Real-World Applications”**.

\subsection{Balance Between Helpfulness and Harmlessness of LLLMs} 
Several studies focus on finding the right balance between LLM helpfulness and safety. Some work**Rajpurkar et al., "Improving Reasoning Responses with Residual Aligners"** suggests improving responses during reasoning by using an extra model called a residual aligner. Safe-RLHF**Kumar, “Preference Alignment under Safety Constraints”**, achieves preference alignment under safety constraints, and subsequent works**Chen et al., “Improved Reward Objectives for Preference Alignment”**, have been proposed to further improve the reward objective. Other research has explored improving safety through psychological techniques**Zhang et al., "Psychological Techniques for Safe Language Models"**, and red teaming methods**Liu et al., “Red Teaming Methods for LLM Safety”**. However, all of the previous works overlook the role of safety data in safety alignment. Our work aims to bridges this gap and balance the safety-helpfulness trade-off by analyzing and curating the safety dataset.