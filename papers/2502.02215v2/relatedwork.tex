\section{Related work}
\subsection{Blind Face Restoration.}
In real-world scenarios, face images may suffer from various types of degradation, such as noise, blur, down-sampling, JPEG compression artifacts, and etc. Blind face restoration (BFR) aims to restore high-quality face images from low-quality ones that suffer from unknown degradation. The BFR approaches are mainly focused on exploring better face priors, including geometric priors, reference priors, and generative priors. Diffusion prior, which is more explored in recent years, belongs to a broader stream of generative priors.
For the geometric-prior methods, they explore the highly structured information in face images.
The structural information, such as facial landmarks~\citep{chen2018fsrnet}, face parsing maps~\citep{shen2018deep,chen2021progressive} and 3D shapes~\citep{hu2020face_sr,zhu2022blind_via,lu20243d}, can be used as a guidance to facilitate the restoration.
However, since the geometric face priors estimated from degraded inputs can be unreliable, they may lead to the suboptimal performance of the subsequent BFR task.
Some existing methods~\citep{dogan2019exemplar_guided,Li_2018_bfr_lwg} guide the restoration with an additional HQ reference image that owns the same identity as the degraded input, which is referred to as the reference-priro BFR approaches. 
The main limitations of these methods stem from their dependence on the HQ reference images, which are inaccessible in some scenarios.
More recent approaches directly exploit the rich priors encapsulated in generative models for BFR, which are denoted as generative priors. 

\vspace{-1mm}
\minisection{GAN-prior.}
By applying the GAN inversion~\citep{xia2022gan_inversion}, the earlier generative-prior explorations~\citep{gu2020image_multi_code_gan,menon2020pulse} iteratively optimize the latent code of a pretrained GAN for the desirable HQ target.
To circumvent the time-consuming optimization, some studies~\citep{yang2021gan,chan2021glean} directly embed the decoder of the pre-trained StyleGAN~\citep{gal2021stylegan} into the BFR network and evidently improve the restoration performance. 
The success of VQ-GAN~\citep{crowson2022vqgan} in image generation also inspires several BFR methods to design various strategies~\citep{wang2022restoreformer,zhou2022codeformer} to improve the matching between the codebook elements of the degraded input and the underlying HQ image.

\vspace{-1mm}
\minisection{Diffusion-prior.}
Recently, the diffusion model has been proven to be more
stable than GAN~\citep{dhariwal2021diffusionbeatgans}, and the generating images are more diverse. This has also received attention in the blind face restoration task.
IDM~\citep{Zhao_2023_authentic_bfr} introduces an extrinsic pre-cleaning process to further improve the BFR performance on the basis of SR3~\citep{saharia2022image_sr3}.
To accelerate the inference speed, LDM~\citep{rombach2022high} proposed to train the diffusion model in the latent space. In a bid to circumvent the laborious and time-consuming retraining process, several investigations~\citep{lin2023diffbir,wang2023dr2} have explored the utilization of a pre-trained diffusion model as a generative prior to facilitate the restoration task.
More specifically, 
DiffBIR~\citep{lin2023diffbir} and SUPIR~\citep{yu2024scaling} leverage the pretrained Stable Diffusion~\citep{rombach2022high} as the generative prior, which can provide more prior knowledge than other existing methods.
DR2~\citep{wang2023dr2} and CCDF~\citep{chung2022come} diffuse input images to a noisy state where various types of degradation have weaker scales than the added Gaussian noises, following by capturing the semantic information during denoising steps. 
Moreover, this restoration using noisy states~\citep{wang2023dr2,chung2022come} or diffusion bridges~\citep{liu20232} can accelerate the inference. 
The common idea underlying these approaches is to modify the reverse sampling process of the pre-trained diffusion model by introducing a well-defined or manually assumed degradation model as an additional constraint. Even though these methods perform well in certain ideal scenarios, they can not deal with the BFR task since its degradation model is unknown and complicated. 

However, these diffusion-prior based approaches still suffer from time-consuming inferences since the diffusion models have to pass through multiple steps. Furthermore, they mostly can only be trained with the reconstruction loss succeeded from the latent diffusion training. The common used perceptual loss in image restoration tasks cannot be well integrated in their frameworks, which may lead to suboptimal perceptual generation with these methods.


\vspace{-2mm}
\subsection{Text-to-Image generative models}
Diffusion models~\citep{deepfloyd,ho2022imagen,chen2023pixartalpha} have emerged as the new state-of-the-art models for text-to-image generation. They commonly involve encoding text prompts utilizing a pre-train language encoder, such as CLIP~\citep{radford2021clip} and T5~\citep{raffel2020t5_model}.
The output is subsequently inserted into the diffusion model through the cross-attention mechanism. For base architectures, UNet~\citep{ronneberger2015unet} and DiT~\citep{peebles2023scalable_dit} are widely adopted.
In this paper, we mainly build our method on the Stable Diffusion~\citep{rombach2022high} model as a powerful representative generative model of T2I generation models.

\vspace{-1mm}
\minisection{Distillation of T2I models.}
The diffusion models are bottlenecked by their slow generation speed.
Recently, the distillation-based technique~\citep{hinton2014distilling} has been widely used in the acceleration of diffusion models.
The student model distilled from a pretrained teacher~\citep{luo2023LCM,sauer2023adversarial} generally has faster inference speeds. 
Earlier studies~\citep{salimans2022progressive,meng2023distillation} utilize progressive distillation to gradually reduce the sampling steps of student diffusion models.
Also, The sampling time of the pretrained teacher models are hampering training efficiency.
To address this limitation, several works~\citep{gu2023boot,nguyen2023swiftbrush} 
propose using various bootstrapping techniques.
For instance, Boot~\citep{gu2023boot} is trained using bootstrapping based on two consecutive sampling steps, achieving image-free distillation. SDXL-Turbo~\citep{sauer2023adversarial} introduces a discriminator and combines it with score distillation loss. 

\vspace{-1mm}
\minisection{Additional image control of T2I models.}
Text descriptions guide the diffusion model in generating images but are insufficient in fine-grained control over the generated results. The fine-grained control signals are diverse in modality, including layouts, segmentations, depth maps, etc.
Considering the powerful generation ability of the T2I model, there have been a variety of methods~\citep{li2024controlnet_plus,zavadski2023controlnet_xs,lin2024ctrl_adapter} dedicated to adding image controls to the T2I generative models.
As a representative, ControlNet~\citep{zhang2023controlnet} proposes using the trainable copy of the UNet encoder in the T2I diffusion model to encode additional condition signals into latent representations and then applying zero convolution to inject into the backbone of the UNet in diffusion modal. The simple but effective design shows generalized and stable performance in spatial control and thus is widely adopted in various downstream applications. 
Similarly, the T2I-Adapter~\citep{mou2024t2i_adapter} trains an additional controlling encoder that adds an intermediate representation to the intermediate feature maps of the pre-trained encoder of Stable Diffusion. 

Nonetheless, the T2I models with additional image conditions are still generating images from Gaussian noises. How to explore their possibilities in solving image restoration tasks is still not explored. In this paper, we successfully make them start the generation from degraded low-quality images to restore the high-quality images and merged them together with the acceleration T2I models.