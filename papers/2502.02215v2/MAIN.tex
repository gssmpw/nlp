\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}
\definecolor{citecolor}{HTML}{0071BC}
\definecolor{linkcolor}{HTML}{ED1C24}
\usepackage[pagebackref=false, breaklinks=true, letterpaper=true, colorlinks, citecolor=citecolor, linkcolor=linkcolor, bookmarks=false]{hyperref}
\usepackage{hyperref}
\usepackage{url}

\definecolor{mycitecolor}{rgb}{0.21,0.49,0.74}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{xcolor}
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\crefname{table}{Tab.}{Tabs.}
\Crefname{table}{Table}{Tables}
% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{wrapfig}
\usepackage{enumitem}
\usepackage{subcaption}

\usepackage{diagbox}
\usepackage{amsthm}
\usepackage{multicol}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[ruled,vlined,algo2e]{algorithm2e} 
\usepackage[textsize=tiny]{todonotes}
\usepackage{wrapfig}
\usepackage{color}
\usepackage{colortbl}

\newcommand{\minisection}[1]{\vspace{0.04in} \noindent {\bf #1}\,}
\newcommand{\quotes}[1]{``#1''}

\newcommand{\green}[1]{{\color[HTML]{40c057}#1}}
\newcommand{\red}[1]{{\color{red}#1}}
\newcommand{\gray}[1]{{\color{gray}#1}}
\newcommand{\blue}[1]{{\color{blue}#1}}


\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\newcommand{\circlednum}[1]{%
    \tikz[baseline=(X.base)] {
        \node[draw,circle,inner sep=.3pt] (X) {#1};
    }%
}

\newcommand{\cmark}{\ding{51}\xspace}%
\newcommand{\cmarkg}{\textcolor{green}{\ding{51}}\xspace}%
\newcommand{\xmark}{\ding{55}\xspace}%
\newcommand{\xmarkg}{\textcolor{red}{\ding{55}}\xspace}%
\usepackage{colortbl}
\def\ourmethod{{\textit{InterLCM}}\xspace}


\title{InterLCM: Low-Quality Images as Intermediate States of Latent Consistency Models for Effective Blind Face Restoration}


\author{Senmao Li$^{1,2}$\thanks{Work done during a research stay at Computer Vision Center, Universitat Aut\`onoma de Barcelona.}\quad Kai Wang$^{2}$\thanks{The corresponding author.}\quad Joost van de Weijer$^{2}$\, Fahad Shahbaz Khan$^{3,4}$\\ \textbf{Chun-Le Guo}$^{1,6}$\quad \textbf{Shiqi Yang}$^{5}$\quad \textbf{Yaxing Wang}$^{1,6}$\quad \textbf{Jian Yang}$^{1}$\quad \textbf{Ming-Ming Cheng}$^{1,6}$\\
$^1${VCIP, CS, Nankai University}\quad  $^2${Computer Vision Center, Universitat Aut\`onoma de Barcelona}\\$^3${Mohamed bin Zayed University of AI}\quad $^4${Linkoping University}\quad $^5${SB Intuitions, SoftBank} \\
$^6${Nankai International Advanced Research Institute (Shenzhen Futian), Nankai University}\\
\texttt{\{senmaonk,\,shiqi.yang147.jp\}@gmail.com, \{kwang,\,joost\}@cvc.uab.es} \\
\texttt{fahad.khan@liu.se, \{guochunle,\,yaxing,\,csjyang,\,cmm\}@nankai.edu.cn}
}


\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}

\maketitle

\begin{abstract}
Diffusion priors have been used for blind face restoration (BFR) by fine-tuning diffusion models (DMs) on restoration datasets to recover low-quality images. However, the naive application of DMs presents several key limitations. 
(i) The diffusion prior has inferior semantic consistency (e.g., ID, structure and color.),  increasing the difficulty of optimizing the BFR model;
(ii) reliance on hundreds of denoising iterations, preventing the effective cooperation with perceptual losses, which is crucial for faithful restoration.
Observing that the latent consistency model (LCM) learns consistency noise-to-data mappings on the ODE-trajectory and therefore shows more semantic consistency in the subject identity, structural information and color preservation, 
we propose \ourmethod to leverage the LCM for its superior semantic consistency and efficiency to counter the above issues. 
Treating low-quality images as the intermediate state of LCM, \ourmethod achieves a balance between fidelity and quality by starting from earlier LCM steps. 
LCM also allows the integration of perceptual loss during training, leading to improved restoration quality, particularly in real-world scenarios.
To mitigate structural and semantic uncertainties, \ourmethod incorporates a Visual Module to extract visual features and a Spatial Encoder to capture spatial details, enhancing the fidelity of restored images.
Extensive experiments demonstrate that \ourmethod outperforms existing approaches in both synthetic and real-world datasets while also achieving faster inference speed. 
% Code and models will be publicly available.
Project page: \href{https://sen-mao.github.io/InterLCM-Page/}{https://sen-mao.github.io/InterLCM-Page/}
\end{abstract}


\section{Introduction}
\label{sec:introduction}
Blind face restoration (BFR) aims to restore high-quality (HQ) images from low-quality (LQ) input that exhibit complex and unknown degradation, such as down-sampling~\citep{chen2018fsrnet,bulat2018learn}, blurriness~\citep{zhang2017beyond,zhang2020plug,shen2018deep}, noise~\citep{dogan2019exemplar_guided}, compression~\citep{dong2015compression}, etc. 
BFR has undergone significant advances in recent years. Existing methods primarily focus on learning a direct mapping between LQ and HQ images, often incorporating various priors to enhance restoration performance. 
Early works mainly explore geometric priors, such as facial landmarks~\citep{chen2018fsrnet}, parsing maps~\citep{chen2021progressive,shen2018deep}, and heat maps~\citep{yu2018face}, to offer explicit information about face restorations. 
Reference prior~\citep{gu2022vqfr,zhou2022codeformer} methods are taking additional high-quality images to enhance the restoration of LQ images. 
More recently, generative priors~\citep{wang2021gfpgan,yang2021gan} have been widely used in blind face restoration to obtain realistic textures.

With the superior generative capabilities of recent successful diffusion models~\citep{ramesh2022hierarchical}, which are trained on billions of data~\citep{schuhmann2022laion}, 
the diffusion-prior methods~\citep{wang2023dr2,miao2024waveface,lu20243d} have been explored to solve the BFR problem.
Although reasonable restoration results are achieved, existing diffusion-based methods~\citep{wang2021gfpgan,yue2024difface} generally suffer from several major limitations.
(i) The diffusion prior has inferior semantic consistency, namely identity consistency, structural stability, color preservation, etc. which  increases the difficulty of optimizing the BFR model\citep{zhou2022codeformer}.
As an example, we evaluate the semantic consistency between the estimated real image in each step for a conventional diffusion model SD Turbo~\citep{sauer2023adversarial}\footnote{We regard SD Turbo as a typical representative of the diffusion models, since  it inherits the characteristics of the diffusion model well.  While LCM is distilled with the consistency regularization.} and the latent consistency model (LCM)~\citep{luo2023LCM}, as shown in \cref{fig:lcmsdturbo_output}. 
It is evident that the conventional diffusion models exhibit weaker semantic consistency prior information compared with the consistency models.
(ii) Diffusion-based methods that rely on standard diffusion models face challenges in sampling, as they require many iterations to produce the real image outputs. 
They cannot easily incorporate with a perceptual loss applied to the final image outputs. 
Despite existing methods~\citep{chung2023parallel,laroche2024fast} compute the perceptual loss with real images obtained from the intermediate step, these real images show a appearance gap compared to the final image output (see~\cref{appsub:perloss} for details). 



\begin{figure}[t]
% \begin{wrapfigure}{r}{0.7\linewidth}
% \vspace{-15pt}
\begin{center}
\includegraphics[width=0.94\textwidth]{imgs/lcmsdturbo_compressed.pdf}
\end{center}
\vspace{-4mm}
\caption{(\textit{Left}) The intermediate states in 4-step LCM and SD Turbo models. 
% The LCM model maps to the real image space, 
The network used in LCM maps to the real image space,
while SD Turbo progressively denoises the noisy image. (\textit{Right}) Given the prompt \quotes{A headshot of a man with hat and glasses}, we generate 1000 images with both LCM and SD Turbo models. Then we use DreamSim, SSIM, and color histogram distance (HDist) to measure the semantic consistency in the subject identity, spatial structure and color preservation.}
\vspace{-6mm}
\label{fig:lcmsdturbo_output}
% \end{wrapfigure}
\end{figure}

To address these problems, we introduce the latent consistency model (LCM) into blind face restoration tasks, which has not been explored before. 
More specifically, the LCM model learns to map any point on the ODE~\citep{song2023consistency}
%
trajectory to its origin for generative modeling. That property differs significantly from the conventional diffusion models, where the iterative sampling process progressively removes noise from the random initial vectors.
Based on the LCM property, we propose our method \ourmethod, which regards the LQ image as the input in an \textit{intermediate step of LCM models} and obtains the high-quality image by performing the remaining few
denoising steps
(i.e., 3 steps) in 4-step LCM.
By this means, \ourmethod maintains better semantic consistency originated from the LCM. 
Meanwhile, benefitting from this property, we can integrate with both perceptual loss~\citep{johnson2016perceptual} and adversarial loss~\citep{goodfellow2014generative}, which are commonly used in restoration model training, leading to a high-quality and high-fidelity face restoration output. 

However, directly applying the LCM to blind face restoration brings randomness
to the generated structures and semantics, which originate from the random sampling paths (see~\cref{subsec:interlcm} and \cref{fig:lcm4step}). We therefore propose to apply two extra components to \ourmethod. First, a CLIP image encoder and Visual Encoder as Visual Module that helps to extract semantic information from faces, providing the LCM with face-specific priors. 
Second, to prevent changes in content (e.g., structure), we include a Spatial Encoder to leverage the strong semantic consistency of the LCM model. More specifically, we follow the ControlNet architecture design to copy the UNet encoder part as the Spatial Encoder. 
Note that the Spatial Encoder differs from the ControlNet by the training schemes, where it is commonly trained with the diffusion loss while our Spatial Encoder backpropagates from the real image (through the denoising steps) to the initial low-quality image. During this process, the Visual Encoder and Spatial Encoder are updated with gradients.


In the experiments, we performed extensive experiments to compare \ourmethod with existing approaches, on synthetic and real-world datasets including CelebA, LFW, WebPhoto, etc.
Our method achieves better qualitative and quantitative performance while also achieving faster inference times.
In summary, our work makes the following contributions:
\begin{itemize}[leftmargin=*]
    \item We introduce \ourmethod, a simple but effective BFR framework leveraging the latent consistency model (LCM) priors. By considering the low-quality image as the intermediate state of LCM models, we can effectively maintain better semantic consistency in face restorations.
    \item Using LCM mapping each state to the original image level point, our method \ourmethod has additional advantages: few-step sampling with much faster speed and integrating our framework with commonly used perceptual loss and adversarial loss in face restoration. 
    \item Through extensive experiments over synthetic and real image datasets, we demonstrate the effectiveness and authenticity of our \ourmethod in restoring HQ images, especially in real-world scenarios with unpredictable degradations. 
\end{itemize}

\section{Related work}
\subsection{Blind Face Restoration.}
In real-world scenarios, face images may suffer from various types of degradation, such as noise, blur, down-sampling, JPEG compression artifacts, and etc. Blind face restoration (BFR) aims to restore high-quality face images from low-quality ones that suffer from unknown degradation. The BFR approaches are mainly focused on exploring better face priors, including geometric priors, reference priors, and generative priors. Diffusion prior, which is more explored in recent years, belongs to a broader stream of generative priors.
For the geometric-prior methods, they explore the highly structured information in face images.
The structural information, such as facial landmarks~\citep{chen2018fsrnet}, face parsing maps~\citep{shen2018deep,chen2021progressive} and 3D shapes~\citep{hu2020face_sr,zhu2022blind_via,lu20243d}, can be used as a guidance to facilitate the restoration.
However, since the geometric face priors estimated from degraded inputs can be unreliable, they may lead to the suboptimal performance of the subsequent BFR task.
Some existing methods~\citep{dogan2019exemplar_guided,Li_2018_bfr_lwg} guide the restoration with an additional HQ reference image that owns the same identity as the degraded input, which is referred to as the reference-priro BFR approaches. 
The main limitations of these methods stem from their dependence on the HQ reference images, which are inaccessible in some scenarios.
More recent approaches directly exploit the rich priors encapsulated in generative models for BFR, which are denoted as generative priors. 

\vspace{-1mm}
\minisection{GAN-prior.}
By applying the GAN inversion~\citep{xia2022gan_inversion}, the earlier generative-prior explorations~\citep{gu2020image_multi_code_gan,menon2020pulse} iteratively optimize the latent code of a pretrained GAN for the desirable HQ target.
To circumvent the time-consuming optimization, some studies~\citep{yang2021gan,chan2021glean} directly embed the decoder of the pre-trained StyleGAN~\citep{gal2021stylegan} into the BFR network and evidently improve the restoration performance. 
The success of VQ-GAN~\citep{crowson2022vqgan} in image generation also inspires several BFR methods to design various strategies~\citep{wang2022restoreformer,zhou2022codeformer} to improve the matching between the codebook elements of the degraded input and the underlying HQ image.

\vspace{-1mm}
\minisection{Diffusion-prior.}
Recently, the diffusion model has been proven to be more
stable than GAN~\citep{dhariwal2021diffusionbeatgans}, and the generating images are more diverse. This has also received attention in the blind face restoration task.
IDM~\citep{Zhao_2023_authentic_bfr} introduces an extrinsic pre-cleaning process to further improve the BFR performance on the basis of SR3~\citep{saharia2022image_sr3}.
To accelerate the inference speed, LDM~\citep{rombach2022high} proposed to train the diffusion model in the latent space. In a bid to circumvent the laborious and time-consuming retraining process, several investigations~\citep{lin2023diffbir,wang2023dr2} have explored the utilization of a pre-trained diffusion model as a generative prior to facilitate the restoration task.
More specifically, 
DiffBIR~\citep{lin2023diffbir} and SUPIR~\citep{yu2024scaling} leverage the pretrained Stable Diffusion~\citep{rombach2022high} as the generative prior, which can provide more prior knowledge than other existing methods.
DR2~\citep{wang2023dr2} and CCDF~\citep{chung2022come} diffuse input images to a noisy state where various types of degradation have weaker scales than the added Gaussian noises, following by capturing the semantic information during denoising steps. 
Moreover, this restoration using noisy states~\citep{wang2023dr2,chung2022come} or diffusion bridges~\citep{liu20232} can accelerate the inference. 
The common idea underlying these approaches is to modify the reverse sampling process of the pre-trained diffusion model by introducing a well-defined or manually assumed degradation model as an additional constraint. Even though these methods perform well in certain ideal scenarios, they can not deal with the BFR task since its degradation model is unknown and complicated. 

However, these diffusion-prior based approaches still suffer from time-consuming inferences since the diffusion models have to pass through multiple steps. Furthermore, they mostly can only be trained with the reconstruction loss succeeded from the latent diffusion training. The common used perceptual loss in image restoration tasks cannot be well integrated in their frameworks, which may lead to suboptimal perceptual generation with these methods.


\vspace{-2mm}
\subsection{Text-to-Image generative models}
Diffusion models~\citep{deepfloyd,ho2022imagen,chen2023pixartalpha} have emerged as the new state-of-the-art models for text-to-image generation. They commonly involve encoding text prompts utilizing a pre-train language encoder, such as CLIP~\citep{radford2021clip} and T5~\citep{raffel2020t5_model}.
The output is subsequently inserted into the diffusion model through the cross-attention mechanism. For base architectures, UNet~\citep{ronneberger2015unet} and DiT~\citep{peebles2023scalable_dit} are widely adopted.
In this paper, we mainly build our method on the Stable Diffusion~\citep{rombach2022high} model as a powerful representative generative model of T2I generation models.

\vspace{-1mm}
\minisection{Distillation of T2I models.}
The diffusion models are bottlenecked by their slow generation speed.
Recently, the distillation-based technique~\citep{hinton2014distilling} has been widely used in the acceleration of diffusion models.
The student model distilled from a pretrained teacher~\citep{luo2023LCM,sauer2023adversarial} generally has faster inference speeds. 
Earlier studies~\citep{salimans2022progressive,meng2023distillation} utilize progressive distillation to gradually reduce the sampling steps of student diffusion models.
Also, The sampling time of the pretrained teacher models are hampering training efficiency.
To address this limitation, several works~\citep{gu2023boot,nguyen2023swiftbrush} 
propose using various bootstrapping techniques.
For instance, Boot~\citep{gu2023boot} is trained using bootstrapping based on two consecutive sampling steps, achieving image-free distillation. SDXL-Turbo~\citep{sauer2023adversarial} introduces a discriminator and combines it with score distillation loss. 

\vspace{-1mm}
\minisection{Additional image control of T2I models.}
Text descriptions guide the diffusion model in generating images but are insufficient in fine-grained control over the generated results. The fine-grained control signals are diverse in modality, including layouts, segmentations, depth maps, etc.
Considering the powerful generation ability of the T2I model, there have been a variety of methods~\citep{li2024controlnet_plus,zavadski2023controlnet_xs,lin2024ctrl_adapter} dedicated to adding image controls to the T2I generative models.
As a representative, ControlNet~\citep{zhang2023controlnet} proposes using the trainable copy of the UNet encoder in the T2I diffusion model to encode additional condition signals into latent representations and then applying zero convolution to inject into the backbone of the UNet in diffusion modal. The simple but effective design shows generalized and stable performance in spatial control and thus is widely adopted in various downstream applications. 
Similarly, the T2I-Adapter~\citep{mou2024t2i_adapter} trains an additional controlling encoder that adds an intermediate representation to the intermediate feature maps of the pre-trained encoder of Stable Diffusion. 

Nonetheless, the T2I models with additional image conditions are still generating images from Gaussian noises. How to explore their possibilities in solving image restoration tasks is still not explored. In this paper, we successfully make them start the generation from degraded low-quality images to restore the high-quality images and merged them together with the acceleration T2I models.


\section{Method}
\vspace{-2mm}
BFR aims to restore a HQ image from its LQ counterpart while preserving semantic consistency with the LQ image under unknown and complex degradation.
In this section, we first introduce the preliminaries on latent diffusion models and latent consistency models in \cref{subsec:preliminary}. Then we detail our method, \ourmethod, in \cref{subsec:interlcm}. In \ourmethod, following the LCM noise addition process, we begin by investigating the intermediate state of the LCM to which the LQ image should be regard as. We then introduce Visual Module and Spatial Encoder to preserve the semantic information and structure in the reconstructed HQ image. 
The illustration of \ourmethod is shown in \cref{fig:pipeline} and~\cref{alg:FaceLCM_sample} in~\cref{sec:app_alg}.

\subsection{Preliminaries}
\label{subsec:preliminary}

\begin{figure*}[t]
% \vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\linewidth]{imgs/lcm_noise_compressed.pdf}}
% \vskip -0.1in
\caption{(\textit{Left}) The 4-step LCM map its origin at each sampling step: Noise$\xrightarrow[]{\text{1st step}}$Sampling data$\xrightarrow[]{\text{add noise}}$Noisy data$\xrightarrow[]{\text{2nd step}}$Sampling data$\xrightarrow[]{\text{add noise}}$Noisy data$\xrightarrow[]{\text{3rd step}}$Sampling data$\xrightarrow[]{\text{add noise}}$Noisy data$\xrightarrow[]{\text{4th step}}$Sampling data.
In the first step, the origin image is predicted from random noise. In each remaining step, noise is added to the origin image produced in the previous step. (\textit{Right}) The predicted origin images are shown for each step (the first row). The random noise and noisy data from the first to third steps (the second row). 
For example, given one prompt case ``blond woman with red glasses and a black shirt'', the generated image at each step shows semantic consistency in the subject identity, structural
information and color constancy (the first row).
}
\label{fig:lcm}
\end{center}
% \vskip -0.35in
\vspace{-6mm}
\end{figure*}


\minisection{Latent Diffusion Models.}
To enable diffusion model (DM) trained 
over limited computing resources 
while retaining the generation quality, Latent Diffusion Models (LDMs)~\citep{rombach2022high} encode an image $x$ into a latent representation $z_0$ using an encoder $\mathcal{E}$ and reconstruct it using a decoder $\mathcal{D}$.
%
The LDMs aims to train a noise prediction network $\epsilon_\theta$ with diffusion loss:
\begin{equation}\label{eq:loss_diff}
    \mathcal{L}= \mathbb{E}_{z_0, t, \epsilon\sim\mathcal{N}(\textbf{0},\textbf{I})} \Vert \boldsymbol{\epsilon}-{\boldsymbol{\epsilon} _\theta}(z_{t}, c, t) \Vert_2^2
\end{equation}


In the diffusion inference phase, a LDM predicts noise using the pretrained denoising network $\boldsymbol{\epsilon}_\vtheta(z_t,c,t)$ with the text condition $c$, resulting in a latent $z_{t-1}$ following the DDPM scheduler~\citep{ho2020denoising} (see~\cref{fig:lcm} (left), the \green{green} arrow line). The final latent $z_0$ is obtained sequentially. 

\begin{figure*}[t]
\begin{center}
\centerline{\includegraphics[width=\textwidth]{imgs/pipeline_compressed.pdf}}\vspace{-2mm}
\caption{Overview of the proposed \ourmethod framework. The Visual Module takes LQ images to output the visual embeddings. 
A Spatial Encoder is used to provide structure information. 
We consider the LQ image as the intermediate state of LCM. 
Through standard LCM conditioned with both the visual embedding and spatial features, the LQ input can be reconstructed as a HQ image.}\vspace{-6mm}
\label{fig:pipeline}
\end{center}
\end{figure*}

\minisection{Latent Consistency Models.}
Consistency Models (CMs)~\citep{song2023consistency} adopt consistency mapping to directly map any point in ODE trajectory back to its origin, facilitating semantic consistency generation compared to LDMs.
A LCM $\vf_\vtheta(z_{\tau_n}, c, \tau_n)$ can be distilled from a pretrained LDM (e.g., Stable  Diffusion~\citep{rombach2022high}) using the consistency distillation loss~\citep{song2023consistency} for few-step inference, where $c$ is the given text condition. 
LCM directly predicts the origin $z_0$ of augmented PF-ODE trajectory~\citep{luo2023LCM}, generating samples in a single step.
The LCM enhances sample quality while maintaining semantic consistency by alternating between denoising and noise addition steps (see~\cref{fig:lcm} (left), the various \red{red} arrow lines). Specifically, in the $n$-th iteration, the LCM first applies a noise addition forward process to the previously predicted sample $z_0=\vf_\vtheta(z_{\tau_{n+1}}, c, \tau_{n+1})$, resulting in 
$z_{\tau_n}$.
Here, $\tau_n$ represents a decreasing sequence of time steps, where $n\in\{1,\cdots,N{-}1\}$, $\tau_1 > \tau_2 > \cdots > \tau_{N{-}1}$,
and $N$ ($N=4$) is the number of steps in the LCM.
Then, the prediction for the next $z_0=\vf_\vtheta(z_{\tau_n}, c, \tau_n)$ is carried out again.

\subsection{\ourmethod: Low-Quality images as Intermediate states of LCM}
\label{subsec:interlcm}
Our proposed \ourmethod is built on the LCM model. As shown in \cref{fig:pipeline}, 
the random noise is added to LQ image $x_l$, which already contains complex and unknown degradation.
The Visual Module takes LQ image as input and returns the visual embedding, which replaces the text embedding used in the standard LCM to  supply the face-specific semantic information. To preserve the structure of LQ image, we utilize a Spatial Encoder to provide LCM with structure information.
Through standard LCM processing with both visual embedding and spatial features, the LQ input can be reconstructed into an HQ output.  
In this subsection, following the LCM noise addition process, we begin by investigating which intermediate state of the LCM to insert LQ image. We then detail Visual Module and Spatial Encoder.


\begin{wrapfigure}{r}{0.5\linewidth}
\vspace{-15pt}
\begin{center}
\includegraphics[width=0.5\textwidth]{imgs/injectnoise_lcm3_compressed.pdf}
\end{center}
\vspace{-10pt}
\caption{t-SNE visualizations of feature distributions show the first step sampling similarity of LCM and the LQ image (FID=103.70), and their noisy intermediate states after LCM 2nd-step noise diffusion (FID=2.83).}
\vspace{-6pt}
\label{fig:lcmlq}
\end{wrapfigure}


\minisection{2nd-step intermediate state.}
To leverage the content consistency inherent in LCM~\citep{luo2023LCM}, we retain the pretrained model and follow its sampling process. As shown in~\cref{fig:lcm} (right, the first row), the 4-step LCM sampling process generates semantic consistency images. In the first step, LCM directly predicts an image from random noise. In each remaining step, LCM first adds noise to the previous image and then predicts a finer output.
Based on the three noise addition processes in each of the 4-step LCM, we first move the LQ image to each intermediate state of LCM. As shown in~\cref{fig:lcmlq}, we empirically find that the distribution of the LQ image is closer to that of the generated image after the first noise addition (second step noise addition) than other intermediate states (see~\cref{subsec:app_start} for more detail).
% 
Therefore, we use the LQ image as the intermediate state after the first noise addition in LCM. Subsequently, the LCM is applied starting from \textit{the second step}.

\begin{wrapfigure}{r}{0.5\linewidth}
\vspace{-16pt}
\begin{center}
\includegraphics[width=0.5\textwidth]{imgs/lcm_4step_skip1_compressed.pdf}
\end{center}
\vspace{-10pt}
\caption{Naive LCM alters the original semantics of the LQ image (e.g., hair).}
\vspace{-10pt}
\label{fig:lcm4step}
\end{wrapfigure}


\minisection{Visual Encoder.}
Ideally, the model should reconstruct image quality and align semantic information with the LQ image. However, noise diffusion introduces randomness, altering the original semantics of the LQ image, regardless of whether the prompt is a null-text or text prompt.
For example, as shown in~\cref{fig:lcm4step}, when given a LQ image and a null-text prompt (i.e., $\varnothing=$\quotes{}), the hair color changes to white in the generated image (\cref{fig:lcm4step} (the second column)). Even given a text prompt (that is, ``a woman with blonde hair and a smile''~\footnote{We use the BLIP~\citep{li2022blip} caption model to generate descriptions for HQ images as text prompts.}) obtained from the HQ image, the straight hair changes to curly in the generated image (\cref{fig:lcm4step} (the third column)).

 To provide LCM with face-specific prior to produce semantic consistent content, we propose to use a Visual Module (\cref{fig:pipeline}). The Visual Module provides face-specific semantic information to the pretrained LCM, similar to how text prompts are used in standard text conditioned image generation~\citep{luo2023LCM}. We employ visual embedding, first extracting general CLIP visual features~\citep{radford2021clip} from LQ image $x_l$, which are then distilled by the Visual Encoder (VE) to yield face-specific semantic information, defined as $c_v=VE(CLIP(x_l))$. This approach aligns $c_v$ with the text embedding the LCM typically uses for its text condition sampling. Furthermore, using visual embedding avoids the need for applying a complex text prompt that can describe LQ image in detail and accurately~\citep{liao2024fine,li2023w-plus-adapter}.

\minisection{Spatial Encoder.} However, the face-specific visual embedding \(c_v\), while essential for capturing global semantic attributes, is insufficient for preserving global structure ($\circlednum{1}$ in \cref{fig:ablationstudy}). To address this issue, we introduce the Spatial Encoder (SE) to effectively extract and enhance spatial structure preservation (\cref{fig:pipeline}). We use the pretrained UNet encoder from stable diffusion to capture the full content of the LQ image, including structural information. When combined with the visual embedding, the SE then extracts the spatial features, denoted as \(\mathsf{f}_v = SE(x_l, c_v)\). The \textit{ResNet} and \textit{Attn} blocks represent the standard ResNet and Cross-Attention transformer blocks in LCM. The output from the \textit{ResNet} block is used as the Query features, while the visual embedding \(c_v\) serves as both Key and Value features in the \textit{Attn} block.
Then the spatial features is combined with the output of \textit{Attn} block. After three iterations of LCM sampling, we finally generate the reconstructed HQ image $x_{rec}= \mathcal{D}(\vf_\vtheta(z_{\tau_n}, c_v, \tau_n, \mathsf{f}_v))$.


\minisection{Training Objectives.} To train the Visual Encoder
%  
and Spatial Encoder, we adopt three image-level losses: reconstruction loss $\mathcal{L}_{1}$, a perceptual loss~\citep{johnson2016perceptual, zhang2018unreasonable} $\mathcal{L}_{per}$, and an adversarial loss~\citep{goodfellow2014generative,esser2021taming} $\mathcal{L}_{adv}$:
%
\begin{equation}
\mathcal{L}_{1} = \|x_h-x_{rec}\|_1; \quad
%
\mathcal{L}_{per} = \|\Phi(x_h)-\Phi(x_{rec})\|_2^2; \quad
%
\mathcal{L}_{adv} = [\log D(x_h) + \log(1-D(x_{rec}))],
\notag
\end{equation}
%
where $x_h$ represents the HQ image, and $\Phi$ denotes the feature extractor of VGG19~\citep{simonyan2014very}. The complete objective function of our model is:
\begin{equation}
\mathcal{L} = \mathcal{L}_{1}+\mathcal{L}_{per}+\lambda\mathcal{L}_{adv},
\label{eq:our_loss}
\end{equation}
where $\lambda$ is the trade-off parameter and set to 0.1 by default in the following experiments.

\section{Experiments}

\subsection{Evaluation on synthetic and real-world data}

We evaluate our method on one \textit{synthetic} dataset and three \textit{real-world} datasets, which are commonly used for evaluation in blind face restoration tasks~\citep{wang2021gfpgan,zhou2022codeformer,yue2024difface,yang2024pgdiff}.
% 
We compare our method with recent baselines, including (CNN/Transformer-based methods) PULSE~\citep{menon2020pulse}, DFDNet~\citep{Li_2020_ECCV}, 
PSFRGAN~\citep{chen2021progressive},
GFPGAN~\citep{wang2021gfpgan}, GPEN~\citep{yang2021gan}, RestorFormer~\citep{Zamir2021Restormer}, VQFR~\citep{gu2022vqfr}, CodeFormer~\citep{zhou2022codeformer}, (Diffusion-based methods) DR2~\citep{wang2023dr2}, DifFace~\citep{yue2024difface}, PGDiff~\citep{yang2024pgdiff}, and WaveFace~\citep{miao2024waveface}.
% 
See~\cref{sec:imp_details} for more details.


For the evaluation on the synthetic dataset (i.e., CelebA-Test~\citep{karras2017progressive}), we use five quantitative metrics: LPIPS~\citep{zhang2018unreasonable}, FID~\citep{heusel2017gans}, MUSIQ, PSNR, and SSIM~\citep{wang2004image}, similar to metrics used in CodeFormer~\citep{zhou2022codeformer} and IDS used in VQFR~\citep{gu2022vqfr} (also referred to as Deg).  The results of the methods are summarized in~\cref{tab:artist_example} (the second to seventh columns).  In terms of image quality metrics LPIPS and MUSIQ \textbf{(MUS.)}, our \ourmethod achieves superior scores compared to existing methods. Furthermore, it faithfully preserves identity and structure, as evidenced by the best IDS and SSIM scores.
Additionally, \cref{fig:celeba} demonstrates that our method significantly outperforms others, while the compared methods fail to yield satisfactory restoration results. For instance, DFDNet, PSFRGAN, GFPGAN, GPEN, DifFace, and PGDiff introduce noticeable artifacts, while PULSE and DR2 produce overly smoothed results that lack essential facial details. Moreover, while RestoreFormer, VQFR, and CodeFormer can generate high-quality texture details (e.g., \textit{hair}), they still exhibit minor artifacts. In contrast, our method is slightly superior to theirs (see the zoomed-in area in~\cref{fig:celeba}).

For the evaluation on the real-world datasets (i.e., LFW-Test~\citep{huang2008labeled}, WebPhoto-Test~\citep{wang2021gfpgan}, and WIDER-Test~\citep{yang2016wider}), we adopt two quantitative metrics following the setting of CodeFormer~\citep{zhou2022codeformer}, namely FID and MUSIQ.
The comparative results are summarized in~\cref{tab:artist_example} (the eight to thirteenth columns). We observe that our method achieves the best performance on WebPhoto-Test and WIDER-Test with medium and heavy degradation. In addition, it obtains the highest score in MUSIQ on the LFW-Test with mild degradation.  For the qualitative comparison in~\cref{fig:realword}, we observe that our method demonstrates excellent robustness to real-world degradation, producing the most visually satisfactory results. Even in images with heavy degradation, our method generates rich texture details, whereas the compared methods exhibit noticeable artifacts.
For example, as shown in~\cref{fig:realword} (the fifth and sixth rows), under heavy degradation in LQ image, all the compared methods produce face images with noticeable artifacts, whereas our method generates high-quality face images with rich hair details.


\begin{table}[t]
    \caption{
    Quantitative comparison on the \textit{synthetic} and \textit{real-world} dataset.
    The best results are in \textbf{bold}, and the second best results are \underline{underlined}.
    }
    \vspace{-2mm}
    \label{tab:artist_example}
    \centering
      \centering
          {\setlength{\tabcolsep}{1pt}\renewcommand{\arraystretch}{1.2}
          \resizebox{1\columnwidth}{!}{
            \begin{tabular}{cl|cccccc|cc|cc|cc|c}
            \toprule
                &{\multirow{2}{*}{Dataset}} & \multicolumn{6}{c|}{\textit{Synthetic} dataset}  & \multicolumn{6}{c|}{\centering \textit{Real-world} datasets}\\
                & & \multicolumn{6}{c|}{Celeba-Test}  & \multicolumn{2}{c}{LFW-Test}&\multicolumn{2}{c}{WebPhoto-Test} & \multicolumn{2}{c|}{WIDER-Test} & {\multirow{3}{*}{\makecell{Time\\(Sec)}}} \\
                % \midrule
                \cmidrule{1-14}
                &\diagbox{Method}{Metrics} & LPIPS$\downarrow$& FID$\downarrow$ & MUSIQ$\uparrow$ & IDS$\downarrow$ &PSNR$\uparrow$ & SSIM$\uparrow$ & FID$\downarrow$ &MUSIQ$\uparrow$ & FID$\downarrow$&MUSIQ$\uparrow$&FID$\downarrow$ & MUSIQ$\uparrow$  &\\
                 \midrule
                 &Input &{0.574}&{145.22} &72.81&47.94&22.72&0.706& 138.87 &26.87& 171.63 &18.63&201.31&14.22 &--\\
                 \midrule
                  \multirow{8}{*}{\rotatebox{90}{\makecell{\makecell{CNN/Transformer\\-based}}}} & PULSE &0.356&68.33&66.46&43.98&22.10&0.592& 67.01 &65.00&85.69&63.88&70.65&63.01& 3.509\\
                  & DFDNet &0.332&54.21&72.08&40.44&24.27&0.628& 60.28 &73.06&92.71&68.50&59.56&62.02&0.438 \\
                  & PSFRGAN &0.294&54.21&73.32&39.63&24.66&0.661& 49.89 &73.60&85.42&71.67&85.42&71.50& \textbf{0.041}\\
                  & GFPGAN &0.230&49.84&73.90&\underline{34.56}&24.64&0.688& 50.36 &73.57&87.47&72.08&39.45&72.79&\underline{0.059} \\
                 & GPEN  &0.290& 63.44 & 67.52 &36.17&\textbf{25.48} &\underline{0.708}&61.04&68.96&99.09&61.10&46.25&62.64& 0.109\\
                 & RestoreFormer &0.241&50.04&73.85&36.16&24.61&0.660& 48.77 &73.70&78.85&69.83&50.04&67.83&0.066\\
                 & VQFR &0.245&\underline{41.84}&75.18&35.74&24.06&0.660& 51.33 &71.74&\underline{75.77}&72.02&44.09&\underline{74.01}& 0.177\\
                 &CodeFormer  &\underline{0.227}& 52.94 & \underline{75.55}&37.27&25.15&0.685&52.84&\underline{75.48}&83.95&\underline{74.00}&39.22&{73.41}& 0.085 \\
                 \midrule
                  \multirow{5}{*}{\rotatebox{90}{\makecell{\makecell{Diffusion\\-based}}}} 
                  & DR2 &0.264&54.48&67.99&44.00&25.03&0.617& \underline{45.71} &71.50&109.24&62.37&48.20&60.28& 1.775\\
                  & DifFace &0.272&\textbf{39.23}&68.87&45.80 &24.80&0.684&{46.31}&69.76& {80.86} & 65.37 &{37.74}&65.02& 3.248\\
                 &PGDiff &{0.300}&47.26&71.81&55.90 &22.72&0.659&\textbf{44.65} &71.74&101.68&67.92&38.38&68.26& 14.768\\
                 &WaveFace &--&--&--&--&-- &--&53.88&73.54&78.01&70.45&\underline{37.23}&72.89& 19.370\\
                 &\textbf{Ours} &\textbf{0.223} &{45.38}&\textbf{76.58} & \textbf{33.64} &\underline{25.19}&\textbf{0.718}&{51.32}&\textbf{76.16}& $\textbf{75.48}$ &$\textbf{75.88}$&$\textbf{35.43}$& \textbf{76.29}& 0.421\\
                \bottomrule
            \end{tabular}
            }}\vspace{-3mm}
\end{table}

\begin{figure*}[t]
\begin{center}
\centerline{\includegraphics[width=\textwidth]{imgs/celeba_compressed.pdf}}\vspace{-2mm}
\caption{Qualitative comparisons of baselines on the synthetic of CelebA-Test for BFR (\textit{Zoom in for a better view} and see~\cref{subsec:app_add} for additional results).
}\vspace{-8mm}
\label{fig:celeba}
\end{center}
\end{figure*}

\begin{figure*}[t]
\begin{center}
\centerline{\includegraphics[width=\textwidth]{imgs/realword_compressed.pdf}}\vspace{-2mm}
\caption{Qualitative comparisons of baselines on the real-world images from LFW-Test, WebPhoto-Test, and WIDER-Test (see~\cref{subsec:app_add} for additional results). \textbf{Zoom in for a better view}.}\vspace{-7mm}
\label{fig:realword}
\end{center}
\end{figure*}

\begin{table}[t]
	\begin{minipage}{0.55\linewidth}
        % \vspace{-2mm}
		\captionof{table}{\small 
        {Ablation study of Visual Encoder (VE) and Spatial Encoder (SE), as well as starting intermediate steps.}
        % on the real-word dataset.
        }
        \vspace{-3.6mm}
		\label{tab:ablation}
		\centering
		\small
		\tabcolsep=0.03cm
        \renewcommand{\arraystretch}{1.1}
		\scalebox{0.75}{
        \begin{tabular}{c|ccc|c|cccc|cc|cc|cc}
				\toprule
				& \multicolumn{3}{c|}{\makecell{Text\\embedding}} & &\multicolumn{4}{c|}{\makecell{Starting\\steps}}  & \multicolumn{2}{c|}{\makecell{LFW-\\Test}} & \multicolumn{2}{c|}{\makecell{WebPhoto-\\Test}}  &
				\multicolumn{2}{c}{\makecell{WIDER-\\Test}} \\
                \midrule
				Exp.       & VE & Null & Text  & \makecell{SE} & 1st & 2nd & 3rd & 4th & FID$\downarrow$ &MUS.$\uparrow$ & FID$\downarrow$ &MUS.$\uparrow$ & FID$\downarrow$ &MUS.$\uparrow$  \\ 
				\midrule
				\circlednum{1} & \checkmark & & & & & \checkmark & & & 69.99 & 76.11 & 93.40 & 75.58 & 57.66 & {76.14} \\
                \midrule
				\circlednum{2} & & \checkmark & & \checkmark & & \checkmark & & & 55.56 & 76.02 & 76.06 & 75.15 & 37.28 & 75.68 \\
				\circlednum{3} & & & \checkmark & \checkmark & & \checkmark & & & 55.07 & 75.75 & 77.76 & 75.30 & 36.15 & 75.98  \\
                \midrule
				\circlednum{4} & \checkmark & & & \checkmark & \checkmark & & & & 54.94 & 71.50 & 92.33 & 72.92 & 40.72 & 71.00\\
				\circlednum{5} & \checkmark & & & \checkmark & & & \checkmark & & \textbf{50.48} & 75.06 & 86.53 & 73.66 & 38.71 & 73.18 \\
				\circlednum{6} & \checkmark & & & \checkmark & & & & \checkmark & 50.59 & 71.36 & 77.25 & 72.01 & 50.70 & 70.41 \\
                \midrule
                \makecell{\circlednum{7}$^\ddag$} & \checkmark & & & \checkmark & & \checkmark & &  & {51.32} & \textbf{76.16} & \textbf{75.48} & \textbf{75.88} & \textbf{35.43} & \textbf{76.29} \\
				\bottomrule
		\end{tabular}
}
	\vspace{-6mm}
	\end{minipage}
	%
	\hspace{0.15mm}
	%
	\begin{minipage}{0.44\linewidth}
		% \vspace{-2mm}
		\begin{subfigure}[h]{\linewidth}
			\centering
			\includegraphics[width=\textwidth]{imgs/diffstep3_compressed.pdf}
		\end{subfigure}
		\vspace{-2mm}
		\captionof{figure}{\small 
        {Visualization of the ablation study for various design variants. $^\ddag$ indicates our results.}
        }
		\label{fig:ablationstudy}\vspace{-6mm}
	\end{minipage}
\end{table}


\subsection{Ablation studies}
\minisection{Effectiveness of Visual Encoder and Spatial Encoder.} Our proposed method starts from second step combining with both visual embedding from Visual Encoder (VE) and 
spatial features from Spatial Encoder (SE).
We first evaluate the efficacy of visual embedding  and spatial features, starting from second step, by exploring various ablated designs and comparing their performances. The ablated designs include: \circlednum{1} VE+2nd: The SE is removed, focusing only on VE training. \circlednum{2} NullText+SE+2nd: Only SE is trained, and VE is replaced by NullText. \circlednum{3} Text+SE+2nd: Only SE is trained, and VE is replaced by Text. Performance results and comparion are presented in~\cref{fig:ablationstudy} (the first row, the second to fourth columns) and~\cref{tab:ablation} (the first to third rows).
We observe that \circlednum{1} VE+2nd captures the face-specific semantic information of the LQ image with high-quality detail, but is insufficient for preserving the global structure because  visual embedding only provides semantically  consistent content.  \circlednum{2} NullText+SE+2nd and \circlednum{3} Text+SD+2nd (e.g., ``A photo of a human face'' as shown in~\cref{fig:ablationstudy}) receive spatial features that effectively capture the global facial structure of the LQ image; however, they compromise on detailed content (e.g., \textit{eyes} and \textit{wrinkles}).


We also experimentally confirm the starting step and present the results in~\cref{fig:ablationstudy} (the second row)
and~\cref{tab:ablation} (fourth to seventh rows). It can be observed that starting from the initial step (i.e., noise), as shown in \circlednum{4} VE+SE+1st, generates detailed textures (e.g., \textit{wrinkles}) but introduces randomness (e.g., \textit{eyes}). Starting from a later step, \circlednum{5} VE+SE+3rd and \circlednum{6} VE+SE+4th result in blurred outputs (the second row, the second column in~\cref{fig:ablationstudy}) and preserving the textures of the LQ image (the second row, the third column in~\cref{fig:ablationstudy}) but fail to generate fine details, due to the limitations imposed by the number of denoising iterations. 
Thus, we incorporate both the visual embedding and spatial features into the LCM, starting from the second step, which facilitates the capture of face-specific information and the generation of fine details (\circlednum{7} in~\cref{fig:ablationstudy} and the last row in~\cref{tab:ablation}).

\vspace{-4mm}
\minisection{Inference time.}
\cref{tab:artist_example} (the last column) shows the inference time of different methods. 
All methods are evaluated on input images using a Quadro RTX 3090 GPU (24GB VRAM) with resolution of $512\times 512$. 
% The best result is in \textbf{bold}, and the second best result is \underline{underlined}. 
The sampling time of our method has a similar running time as  CNN/Transformer-based methods, such as 
DFDNet~\citep{Li_2020_ECCV}, GPEN~\citep{yang2021gan}, and VQFR~\citep{gu2022vqfr}.
% 
Meanwhile, the inference time of our method significant surpass that of other diffusion-based methods, such as 
% DifFace~\citep{yue2024difface} 
PGDiff~\citep{yang2024pgdiff} and WaveFace~\citep{miao2024waveface}, which remain constrained by the iterative sampling processes inherent to diffusion models.


\begin{figure*}[t]
\begin{center}
\vspace{-2mm}
\centerline{\includegraphics[width=\textwidth]{imgs/ablationloss_compressed.pdf}}\vspace{-2mm}
\caption{(\textit{Left}) visualization of the ablation study for both the perceptual and adversarial losses. (\textit{Right}) visualization of the ablation study comparing the naive ControlNet and our  Spatial Encode.}\vspace{-2mm}
\label{fig:ablationstudyloss}
\end{center}
\end{figure*}



\begin{table}[t]
	\begin{minipage}{0.5\linewidth}
        \vspace{-4mm}
		\captionof{table}{\small Ablation study of both the perceptual and adversarial losses.}
        \vspace{-3mm}
		\label{tab:ablation_loss}
		\centering
		\small
		\tabcolsep=0.05cm
        \renewcommand{\arraystretch}{1.01}
		\scalebox{0.75}{
			\begin{tabular}{c|ccc|cc|cc|cc}
				\toprule
				&  & && \multicolumn{2}{c|}{LFW-Test} & \multicolumn{2}{c|}{WebPhoto-Test}  &
				\multicolumn{2}{c}{WIDER-Test} \\ 
				Exp.  & $\mathcal{L}_1$ & $\mathcal{L}_{per}$ & $\mathcal{L}_{adv}$ & FID$\downarrow$ &MUSIQ$\uparrow$  & FID$\downarrow$ &MUSIQ$\uparrow$ & FID$\downarrow$ &MUSIQ$\uparrow$ \\ 
				\midrule
				(a) & \checkmark& & &87.12 & 43.14 & 141.86 & 39.37 & 93.61 & 33.71 \\
                \midrule
                (b) & \checkmark & \checkmark & &57.57 & 67.99 & 95.02 & 66.24 & 44.83 & 63.94 \\
                 \midrule
                 (c) Ours & \checkmark & \checkmark & \checkmark&\textbf{51.32} & \textbf{76.16} & \textbf{75.48} & \textbf{75.88} & \textbf{35.43} & \textbf{76.29} \\
				\bottomrule
		\end{tabular}}
            \vspace{-4mm}
	\end{minipage}
	%
	\hspace{1.5mm}
	%
	\begin{minipage}{0.47\linewidth}
        \vspace{-4mm}
		\captionof{table}{\small Ablation study of the naive ControlNet and our proposed Spatial Encoder.}
        \vspace{-3mm}
		\label{tab:ablation_control}
		\centering
		\small
		\tabcolsep=0.05cm
        \renewcommand{\arraystretch}{0.4}
		\scalebox{0.75}{
			\begin{tabular}{c|c|cc|cc|cc}
				\toprule
				&  & \multicolumn{2}{c|}{LFW-Test} & \multicolumn{2}{c|}{WebPhoto-Test}  &
				\multicolumn{2}{c}{WiDER-Test} \\ 
				Exp.  & Loss  & FID$\downarrow$ &MUSIQ$\uparrow$  & FID$\downarrow$ &MUSIQ$\uparrow$ & FID$\downarrow$ &MUSIQ$\uparrow$ \\ 
				\midrule
				\makecell{Naive\\ControlNet} & \cref{eq:loss_diff}& \textbf{35.43} & 75.03 & 81.91 & 73.63 & 49.58 & 74.20 \\
                \midrule
                \makecell{Spatial\\Encoder} & \cref{eq:our_loss}& 55.07 & \textbf{75.75} & \textbf{77.76} & \textbf{75.30} & \textbf{36.15} & \textbf{75.98} \\
				\bottomrule
		\end{tabular}}
		\vspace{-4mm}
	\end{minipage}
\end{table}


\vspace{-2mm}
\minisection{Effectiveness of perceptual and adversarial losses.}
We consider that the superior restoration performance of our \ourmethod is mainly due to the integrating with both perceptual loss~\citep{johnson2016perceptual} and adversarial loss~\citep{goodfellow2014generative} in the image domain, which are commonly used in restoration model training leading to a high-quality and high-fidelity face restoration output. To highlight the effectiveness of these two losses, we perform the ablation experiments in~\cref{fig:ablationstudyloss} (Left) and~\cref{tab:ablation_loss}. 
We can see that without perceptual and adversarial losses, the quantitative metrics are significantly degraded (\cref{tab:ablation_loss} (the first row)), as it is challenging to achieve good visual quality using only reconstruction loss (\cref{fig:ablationstudyloss} (the second column)). Adding perceptual loss and adversarial loss in the image domain 
can effectively restore realistic details.
%
In addition, we also conduct an ablation study on Spatial Encoder in \ourmethod and Naive ControlNet (\cref{fig:ablationstudyloss} (Right) and~\cref{tab:ablation_control}). The primary difference between the two lies in the loss function utilized during training. Although Naive ControlNet can generate high-quality image while maintaining structure, it loses fidelity due to the denoising loss focuses on the semantic information but 
fidelity~\citep{zhang2023controlnet}.



\begin{figure*}[h]
\begin{center}
% \vspace{-1mm}
\centerline{\includegraphics[width=\textwidth]{imgs/limitaion_compressed.pdf}}\vspace{-3mm}
\caption{Input LQ images with hands may experience failing restorations.}
%probably because the FFHQ training dataset includes a limited number of such images.
\vspace{-10mm}
\label{fig:limitation}
\end{center}
\end{figure*}

\section{Conclusion}
\vspace{-2mm}
In this paper, we proposed \ourmethod, a novel framework for blind face restoration (BFR) that leverages the latent consistency model (LCM) to improve semantic consistency and restore high-quality images from low-quality inputs. 
By treating the low-quality image as \textit{an intermediate step in the LCM}, \ourmethod achieves more accurate restorations with fewer sampling steps compared to traditional diffusion-based methods. Additionally, we integrated a CLIP-based image encoder and visual encoder to capture face-specific semantic information and a spatial encoder based on ControlNet to ensure structural consistency. Extensive experiments on both synthetic and real-world datasets demonstrated that \ourmethod outperforms existing approaches, delivering superior image quality and faster inference, particularly in challenging real-world scenarios with unpredictable degradations.

\vspace{-2mm}
\minisection{Limitation.}
Although our method excels in the existing methods in blind face restoration, it does not depart from limitations.
When \ourmethod deals with images that include hands, it excels at generating more facial details but does not produce realistic hands (\cref{fig:limitation}). That probably results from the fact that the FFHQ training dataset contains a very limited number of such images. 
One potential solution is to enhance the training data by adding more diverse face images with hands.


\section*{Acknowledgements}
% This work was supported by funding by projects TED2021-132513B-I00 and PID2022-143257NB-I00 funded by MCIN/AEI/ 10.13039/501100011033 and by the European Union NextGenerationEU/PRTR and FEDER.
% We acknowledge the support of the project PID2022-143257NB-I00, funded by the Spanish Government through MCIN/AEI/10.13039/501100011033 and FEDER.
% Computation is supported by the Supercomputing Center of Nankai University (NKSC).

This work was supported by NSFC (NO. 62225604) and Youth Foundation (62202243).
%
We acknowledge the support of the project PID2022-143257NB-I00, funded by the Spanish Government through MCIN/AEI/10.13039/501100011033 and FEDER.
%
We acknowledge \quotes{Science and Technology Yongjiang 2035} key technology breakthrough plan project (2024Z120).
%
Computation is supported by the Supercomputing Center of Nankai University (NKSC).

We gratefully acknowledge Chongyi Li, Professor at Nankai University, China, for his valuable discussions and comments.

% \clearpage

\bibliography{longstrings,mybib}
\bibliographystyle{iclr2025_conference}

\clearpage
\appendix

\section*{Appendix}

\section{Appendix: Implementation details}
\label{sec:imp_details}
\subsection{Training details} We mainly use the pre-trained LCM, distilled from StableDiffusion 1.5. 
The Spatial Encoder is partially initialized using UNet encoder from the pre-trained Stable Diffusion 1.5, following the approach in \citep{zhang2023controlnet}. The decoder from CodeFormer~\citep{zhou2022codeformer} serves as the Visual Encoder, with adjustments made to the input and output dimensions to align with our settings.
The proposed method is implemented in Pytorch~\citep{paszke2017automatic}. We use Adam~\citep{kingma2015adam} with a batch size 8, using a learning rate of $2\times10^{-5}$. 
% 
The models are trained for 15K iterations using eight A40 GPUs (48GB VRAM).

\subsection{Training Data} 
We train our models on the FFHQ dataset~\citep{karras2019style}, which consists of 70,000 HQ face images with a resolution of $1024\times1024$. First, we resize the HQ images to $512\times512$. The resized images are then degraded to generate LQ images following the typical degradation process described in~\citep{zhou2022codeformer}:
%
\begin{equation}
x_{l} = \{[(x_{h} * k_{\sigma}){\downarrow_{s}} + n_{\delta}]_{\text{JPEG}_{q}}\}{\uparrow_{s}},
\label{eq:degradation}
\end{equation}
%
where $x_h$ and $x_l$ represent the HQ and LQ images, respectively, $k_\sigma$ is the Gaussian kernel with $\sigma\in\{1:15\}$, $\downarrow_s$ represents the downsampling operation with a scale factor $s\in\{1:30\}$, and $n_\delta$ denotes Gaussian noise with a standard deviation of $\delta\in\{0:20\}$. The convolution operation is denoted by $*$, followed by JPEG compression with a quality factor of $q\in\{30:90\}$. Finally, an upsampling operation $\uparrow_s$ with scale $s$ is applied to restore the original resolution of $512\times512$. 

\subsection{Test Data.}
We evaluate our method on one \textit{synthetic} dataset and three \textit{real-world} datasets, which are commonly used for evaluation in blind face restoration tasks~\citep{wang2021gfpgan,zhou2022codeformer,yue2024difface,yang2024pgdiff}.
The synthetic dataset, CelebA-Test~\citep{karras2017progressive}, contains 4,000 high-quality (HQ) images. The corresponding low-quality (LQ) images are synthesized using the same degradation process as described in~\cref{eq:degradation}, which is consistent with our training setting. 
The three real-world datasets encompass varying degrees of degradation: LFW-Test~\citep{huang2008labeled} with mild, WebPhoto-Test~\citep{wang2021gfpgan} with medium, and WIDER-Test~\citep{yang2016wider} with heavy degradation. They contain 1,711, 407, and 970 LQ images, respectively.

\subsection{Baseline Implementations.} 
We compare our method with recent baselines, including (CNN/Transformer-based methods) PULSE~\citep{menon2020pulse}~\footnote{\url{https://github.com/krantirk/Self-Supervised-photo}}, DFDNet~\citep{Li_2020_ECCV}~\footnote{\url{https://github.com/csxmli2016/DFDNet}}, 
PSFRGAN~\citep{chen2021progressive}~\footnote{\url{https://github.com/chaofengc/PSFRGAN}},
GFPGAN~\citep{wang2021gfpgan}~\footnote{\url{https://github.com/TencentARC/GFPGAN}}, GPEN~\citep{yang2021gan}~\footnote{\url{https://github.com/yangxy/GPEN}}, RestorFormer~\citep{Zamir2021Restormer}~\footnote{\url{https://github.com/swz30/Restormer}}, VQFR~\citep{gu2022vqfr}~\footnote{\url{https://github.com/TencentARC/VQFR}}, CodeFormer~\citep{zhou2022codeformer}~\footnote{\url{https://github.com/sczhou/CodeFormer}}, (Diffusion-based methods) DR2~\citep{wang2023dr2}~\footnote{\url{https://github.com/Kaldwin0106/DR2_Drgradation_Remover}}, DifFace~\citep{yue2024difface}~\footnote{\url{https://github.com/zsyOAOA/DifFace}}, PGDiff~\citep{yang2024pgdiff}~\footnote{\url{https://github.com/pq-yang/PGDiff}}, and WaveFace~\citep{miao2024waveface}~\footnote{\url{https://github.com/yoqim/waveface}}. The evaluation of all methods was conducted on images with a resolution of $512 \times 512$, utilizing their publicly available official code and default settings.


\section{Appendix: Algorithm detail of \ourmethod}
\label{sec:app_alg}
\begin{algorithm}[h]
    \begin{minipage}{\linewidth}
        \caption{The sampling of \ourmethod}\label{alg:FaceLCM_sample}
        \begin{algorithmic}
            \STATE \textbf{Input:} The LQ image $x_l$, Pretrained Latent Consistency Model combining with visual embedding from Visual Module and spatial features from Spatial Encoder (SE): $\vf_\vtheta(z_{\tau_n}, c_v, \tau_n, \mathsf{f}_v)$. Sequence of timesteps  $\tau_{1}>\tau_2>\cdots >\tau_{N-1}, N=4$. Noise schedule $\alpha(t),\sigma(t)$, Encoder $\mathcal{E}$, and Decoder $\mathcal{D}$.
            \STATE Initial latent code $z_0\leftarrow \mathcal{E}(x_l)$
            % \STATE $\vz\leftarrow \vf_\vtheta(\hat{\vz}_T,\omega, \vc, T)$
            \FOR{$n=1$ to $N-1$}
            \STATE $z_{\tau_n}\sim \mathcal{N}(\alpha(\tau_{n})z_0;\sigma^2(\tau_{n})\mathbf{I})$
            \STATE $z_0\leftarrow \vf_\vtheta(z_{\tau_n}, c_v, \tau_n, \mathsf{f}_v)$
            \ENDFOR
        \STATE $x_{rec}\leftarrow \mathcal{D}(z_0)$
        \STATE \textbf{Output:} $x_{rec}$
        \end{algorithmic} 
    \end{minipage}
\end{algorithm}

\section{Appendix: Ablation analysis}

\subsection{Should we start from the 2nd, 3rd, or 4th step in the LCM?}
\label{subsec:app_start}

\begin{figure*}[t]
% \vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\linewidth]{imgs/injectnoise_lcm2_compressed.pdf}}
% \vskip -0.1in
\caption{t-SNE~\citep{hinton2002stochastic} visualizations of feature distributions show (Left) the first step sampling result of LCM and the LQ image (FID=103.70) with their noise-added versions (FID=2.83); (Middle) the second step result and the LQ image (FID=157.80) with their noise-added versions (FID=31.83); (Right) the third step result and the LQ image (FID=172.66) with their noise-added versions (FID=214.40).
}\vspace{-7mm}
\label{fig:app_lcmlq}
\end{center}
% \vskip -0.35in
\end{figure*}

To leverage the content consistency inherent in LCM~\citep{luo2023LCM}, we retain the pretrained model and follow its sampling process. As shown in~\cref{fig:lcm} (right, the first row), the 4-step LCM sampling process generates semantic consistency images. In the first step, LCM directly predicts an image from random noise. In subsequent steps, LCM first adds noise to the previous image and then predicts a finer output.
In~\cref{fig:app_lcmlq} (the first row), we visualize the feature distributions for the LQ image and the results of the first three sampling steps using t-SNE~\citep{hinton2002stochastic}. We can observe that the clusters are well-separable (\cref{fig:app_lcmlq} (the first row)). 
Based on the three addition processes in each of the 4-step LCM, we move the LQ image to each intermediate state of LCM (\cref{fig:app_lcmlq} (the second row)). We find that the distribution of the LQ image is closest to that of the generated image after the first noise addition (second step noise addition) than other intermediate states (\cref{fig:app_lcmlq} (the second row, the first column)).
Therefore, we use the LQ image as the intermediate state after the first noise addition in LCM. Subsequently, the LCM is applied starting from \textit{the second step}.


\subsection{Robustness to random noise addition}

\begin{figure*}[t]
% \vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\linewidth]{app_imgs/robustness_compressed.pdf}}
% \vskip -0.1in
\caption{Two restoration examples of our \ourmethod on the real-world dataset WebPhoto-Test, achieved through random noise addition in the three noise addition step of 4-step LCM.}\vspace{-7mm}
\label{fig:app_rubostness}
\end{center}
% \vskip -0.35in
\end{figure*}

As shown in~\cref{fig:app_rubostness}, we showcase our robustness to random noise addition in the three noise addition step of 4-step LCM. Our $InterLCM$ effectively restores the face-specific detail using random noise addition.

\section{Appendix: Ablation analysis}
\subsection{Effectiveness of perceptual and adversarial losses}
We conducted an ablation study by removing the perceptual loss while retaining the adversarial loss. As shown in~\cref{tab:app_ablation_loss}, we can see that without perceptual loss (3), the quantitative metrics are significantly degraded, as it is challenging to reconstruct the textual detail using adversarial loss (see~\cref{fig:app_ablation_loss}). This indicates that the perceptual loss plays a crucial role in the fidelity of the restored faces.

\begin{table}[t]
    \caption{Ablation study of both the perceptual and adversarial losses.
    The best results are shown in \textbf{bold}.
    }
    \vspace{-2mm}
    \label{tab:app_ablation_loss}
    \centering
      \centering
          {\setlength{\tabcolsep}{1pt}\renewcommand{\arraystretch}{1.2}
          \resizebox{1\columnwidth}{!}{
            \begin{tabular}{c|ccc|cccccc|cc|cc|cc}
            \toprule
                {{Dataset}} & & & & \multicolumn{6}{c|}{\textit{Synthetic} dataset}  & \multicolumn{6}{c}{\centering \textit{Real-world} datasets}\\
                & & & & \multicolumn{6}{c|}{Celeba-Test}  & \multicolumn{2}{c}{LFW-Test}&\multicolumn{2}{c}{WebPhoto-Test} & \multicolumn{2}{c}{WIDER-Test} \\
                \midrule
                % \cmidrule{1-13}
                Exp. & $\mathcal{L}_1$ & $\mathcal{L}_{per}$ & $\mathcal{L}_{adv}$ & LPIPS$\downarrow$& FID$\downarrow$ & MUSIQ$\uparrow$ & IDS$\downarrow$ &PSNR$\uparrow$ & SSIM$\uparrow$ & FID$\downarrow$ &MUSIQ$\uparrow$ & FID$\downarrow$&MUSIQ$\uparrow$&FID$\downarrow$ & MUSIQ$\uparrow$ \\
                 \midrule
                 (1) & \checkmark& &   &{0.403}&{93.36} &41.68& 35.48 &\textbf{27.37}&\textbf{0.764}& 87.12 &43.14& 141.86&39.37&93.61&33.71\\
                 \midrule
                 (2) & \checkmark & \checkmark &  &{0.226}&{51.47}&{68.49}& \textbf{32.47} &{26.63}&{0.732}& {57.57} &{67.99}&{95.02}&{66.24}&{44.83}&{63.94}\\
                 \midrule
                 (3) & \checkmark &  & \checkmark&{0.369}&{72.29}&{76.01}&{36.03}&{24.67}&{0.653}&{63.21} &{76.09} &{129.21}&{75.62}&{100.15}&{74.40}\\
                 \midrule
                 \makecell{(4)\\Ours} & \checkmark & \checkmark & \checkmark&\textbf{0.223}&\textbf{45.38}&\textbf{76.58}&{33.64}&{25.19}&{0.718}&\textbf{51.32} &\textbf{76.16} &\textbf{75.48}&\textbf{75.88}&\textbf{35.43}&\textbf{76.29}\\
                \bottomrule
            \end{tabular}
            }}%\vspace{-2mm}
\end{table}

\begin{figure*}[t]
% \vskip -0.2in
\begin{center}
\centerline{\includegraphics[width=\linewidth]{app_imgs/noperloss_compressed.pdf}}
\vskip -0.1in
\caption{Visualization of the ablation study for both the perceptual and adversarial losses.}
\label{fig:app_ablation_loss}
\end{center}
% \vskip -0.35in
\end{figure*}


\subsection{Our method using LCM in different numbers of steps}
LCM employs a 4-step inference process to balance image quality and inference time, as recommended by the original paper~\citep{luo2023LCM}. In this paper, we use the recommended 4-step LCM model, while we also offer an ablation study with a 2-step LCM model. As observed from the~\cref{tab:diffstep}, the 4-step LCM only works slightly worse than the 2-step LCM on two metrics, which is utilized as the backbone for our \ourmethod. 

\begin{table}[t]
    \caption{Quantitative comparison using the LCM model with different inference steps.
    The best results are shown in \textbf{bold}.
    }
    \vspace{-2mm}
    \label{tab:diffstep}
    \centering
      \centering
          {\setlength{\tabcolsep}{1pt}\renewcommand{\arraystretch}{1.2}
          \resizebox{1\columnwidth}{!}{
            \begin{tabular}{c|cccccc|cc|cc|cc}
            \toprule
                {{Dataset}} & \multicolumn{6}{c|}{\textit{Synthetic} dataset}  & \multicolumn{6}{c}{\centering \textit{Real-world} datasets}\\
                & \multicolumn{6}{c|}{Celeba-Test}  & \multicolumn{2}{c}{LFW-Test}&\multicolumn{2}{c}{WebPhoto-Test} & \multicolumn{2}{c}{WIDER-Test} \\
                % \midrule
                \cmidrule{1-13}
                \diagbox{Method}{Metrics} & LPIPS$\downarrow$& FID$\downarrow$ & MUSIQ$\uparrow$ & IDS$\downarrow$ &PSNR$\uparrow$ & SSIM$\uparrow$ & FID$\downarrow$ &MUSIQ$\uparrow$ & FID$\downarrow$&MUSIQ$\uparrow$&FID$\downarrow$ & MUSIQ$\uparrow$ \\
                 \midrule
                 Input &{0.574}&{145.22} &72.81&47.94&22.72&0.706& 138.87 &26.87& 171.63 &18.63&201.31&14.22\\
                 \midrule
                 \makecell{Ours (2-step LCM)} &{0.248}&{49.19}&{74.31}&{34.92}&{23.91}&{0.662}& {56.21} &\textbf{76.24}&{75.84}&\textbf{76.11}&{38.23}&{76.00}\\
                 \midrule
                 \makecell{Ours (4-step LCM)} &\textbf{0.223}&\textbf{45.38}&\textbf{76.58}&\textbf{33.64}&\textbf{25.19}&\textbf{0.718}&\textbf{51.32} &{76.16} &\textbf{75.48}&{75.88}&\textbf{35.43}&\textbf{76.29}\\
                \bottomrule
            \end{tabular}
            }}%\vspace{-2mm}
\end{table}

\subsection{The analysis of our method using LCM or SD Turbo}
We test the SD Turbo~\citep{sauer2023adversarial} as the backbone to develop our method for the BFR problem. By the quantitative comparison in~\cref{tab:sdturbo}, we show that consistency model, which directly predict the $x_0$ in each step, better suits the BFR problem.

\begin{table}[t]
    \caption{Quantitative comparison with SD Turbo or LCM as the backbones for the blind face restoration (BFR) model.
    The best results are in \textbf{bold}.
    }
    \vspace{-2mm}
    \label{tab:sdturbo}
    \centering
      \centering
          {\setlength{\tabcolsep}{1pt}\renewcommand{\arraystretch}{1.2}
          \resizebox{1\columnwidth}{!}{
            \begin{tabular}{c|cccccc|cc|cc|cc}
            \toprule
                {{Dataset}} & \multicolumn{6}{c|}{\textit{Synthetic} dataset}  & \multicolumn{6}{c}{\centering \textit{Real-world} datasets}\\
                & \multicolumn{6}{c|}{Celeba-Test}  & \multicolumn{2}{c}{LFW-Test}&\multicolumn{2}{c}{WebPhoto-Test} & \multicolumn{2}{c}{WIDER-Test} \\
                % \midrule
                \cmidrule{1-13}
                \diagbox{Method}{Metrics} & LPIPS$\downarrow$& FID$\downarrow$ & MUSIQ$\uparrow$ & IDS$\downarrow$ &PSNR$\uparrow$ & SSIM$\uparrow$ & FID$\downarrow$ &MUSIQ$\uparrow$ & FID$\downarrow$&MUSIQ$\uparrow$&FID$\downarrow$ & MUSIQ$\uparrow$ \\
                 \midrule
                 Input &{0.574}&{145.22} &72.81&47.94&22.72&0.706& 138.87 &26.87& 171.63 &18.63&201.31&14.22\\
                 \midrule
                 \makecell{Ours (SD Turbo)} &{0.257}&{48.51}&{74.15}&{37.02}&{23.30}&{0.660}& {56.44}&74.24 &{84.66}&{74.41}&{43.53}&{72.35}\\
                 \midrule
                 \makecell{Ours (LCM)} &\textbf{0.223}&\textbf{45.38}&\textbf{76.58}&\textbf{33.64}&\textbf{25.19}&\textbf{0.718}&\textbf{51.32} &\textbf{76.16} &\textbf{75.48}&\textbf{75.88}&\textbf{35.43}&\textbf{76.29}\\
                \bottomrule
            \end{tabular}
            }}%\vspace{-2mm}
\end{table}



\begin{table}[t]
    \caption{Quantitative comparison with LCM-LoRA or LCM as the backbones for the blind face restoration (BFR) model.
    The best results are in \textbf{bold}.
    }
    \vspace{-2mm}.
    \label{tab:lcmlora}
    \centering
      \centering
          {\setlength{\tabcolsep}{1pt}\renewcommand{\arraystretch}{1.2}
          \resizebox{1\columnwidth}{!}{
            \begin{tabular}{c|cccccc|cc|cc|cc}
            \toprule
                {{Dataset}} & \multicolumn{6}{c|}{\textit{Synthetic} dataset}  & \multicolumn{6}{c}{\centering \textit{Real-world} datasets}\\
                & \multicolumn{6}{c|}{Celeba-Test}  & \multicolumn{2}{c}{LFW-Test}&\multicolumn{2}{c}{WebPhoto-Test} & \multicolumn{2}{c}{WIDER-Test} \\
                % \midrule
                \cmidrule{1-13}
                \diagbox{Method}{Metrics} & LPIPS$\downarrow$& FID$\downarrow$ & MUSIQ$\uparrow$ & IDS$\downarrow$ &PSNR$\uparrow$ & SSIM$\uparrow$ & FID$\downarrow$ &MUSIQ$\uparrow$ & FID$\downarrow$&MUSIQ$\uparrow$&FID$\downarrow$ & MUSIQ$\uparrow$ \\
                 \midrule
                 Input &{0.574}&{145.22} &72.81&47.94&22.72&0.706& 138.87 &26.87& 171.63 &18.63&201.31&14.22\\
                 \midrule
                 \makecell{Ours (LCM-LoRA)} &{0.240}&{53.26}&\textbf{76.58}&{35.48}&{24.14}&{0.661}& {54.70}&\textbf{76.26} &{82.08}&\textbf{{76.59}}&{39.62}&{75.81}\\
                 \midrule
                 \makecell{Ours (LCM)} &\textbf{0.223}&\textbf{45.38}&\textbf{76.58}&\textbf{33.64}&\textbf{25.19}&\textbf{0.718}&\textbf{51.32} &{76.16} &\textbf{75.48}&{75.88}&\textbf{35.43}&\textbf{76.29}\\
                \bottomrule
            \end{tabular}
            }}%\vspace{-5mm}
\end{table}

\begin{figure*}[t]
% \vskip -0.2in
\begin{center}
\centerline{\includegraphics[width=\linewidth]{app_imgs/lcmlora_compressed.pdf}}
\vskip -0.1in
\caption{Results using LCM-LoRA and LCM backbone for our method.}
\label{fig:lcmlora}
\end{center}
% \vskip -0.35in
\end{figure*}



\begin{table}[t]
    \caption{
    Our method using one-step models ($x_0$-prediction-based diffusion models),
    The best results are in \textbf{bold}.
    }
    \vspace{-2mm}
    \label{tab:1step}
    \centering
      \centering
          {\setlength{\tabcolsep}{1pt}\renewcommand{\arraystretch}{1.2}
          \resizebox{1\columnwidth}{!}{
            \begin{tabular}{c|cccccc|cc|cc|cc}
            \toprule
                {{Dataset}} & \multicolumn{6}{c|}{\textit{Synthetic} dataset}  & \multicolumn{6}{c}{\centering \textit{Real-world} datasets}\\
                & \multicolumn{6}{c|}{Celeba-Test}  & \multicolumn{2}{c}{LFW-Test}&\multicolumn{2}{c}{WebPhoto-Test} & \multicolumn{2}{c}{WIDER-Test} \\
                % \midrule
                \cmidrule{1-13}
                \diagbox{Method}{Metrics} & LPIPS$\downarrow$& FID$\downarrow$ & MUSIQ$\uparrow$ & IDS$\downarrow$ &PSNR$\uparrow$ & SSIM$\uparrow$ & FID$\downarrow$ &MUSIQ$\uparrow$ & FID$\downarrow$&MUSIQ$\uparrow$&FID$\downarrow$ & MUSIQ$\uparrow$ \\
                 \midrule
                 Input &{0.574}&{145.22} &72.81&47.94&22.72&0.706& 138.87 &26.87& 171.63 &18.63&201.31&14.22\\
                 \midrule
                 \makecell{Ours (1-step SD Turbo)} &{0.273}&\textbf{{36.87}}&{74.00}&{37.82}&{24.89}&{0.658}& {61.21}&{70.24} &{87.77}&{{70.47}}&{54.45}&{71.51}\\
                 \midrule
                 \makecell{Ours (1-step LCM)} &{0.240}&{46.66}&{74.06}&37.45&{24.66}&{0.697}& {55.72}&{73.45} &{89.90}&{{72.41}}&{37.16}&{70.45}\\
                 \midrule
                 \makecell{Ours (4-LCM)} &\textbf{0.223}&{45.38}&\textbf{76.58}&\textbf{33.64}&\textbf{25.19}&\textbf{0.718}&\textbf{51.32} &\textbf{{76.16}} &\textbf{75.48}&\textbf{{75.88}}&\textbf{35.43}&\textbf{76.29}\\
                \bottomrule
            \end{tabular}
            }}%\vspace{-2mm}
\end{table}

\begin{figure*}[t]
% \vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\linewidth]{app_imgs/1step_compressed.pdf}}
\vskip -0.1in
\caption{Results of our method using one-step models (as shown in the second and third rows) indicate that these models face challenges with artifacts and blur when reconstructing high-quality images, while our method can reconstruct high-quality images with detailed textures (the fourth row).}
\label{fig:1step}
\end{center}
% \vskip -0.1in
\end{figure*}

\subsection{Our method using LCM-LoRA}
We test the LCM-LoRA~\citep{luo2023lcmlora} as the backbone to develop our method for the BFR problem. \cref{tab:lcmlora} and \cref{fig:lcmlora} show the qualitative and quantitative, respectively, comparison of using LCM-LoRA and our method. As shown in~\cref{tab:lcmlora}, LCM-LoRA does not perform as well as our method in terms of LPIPS and FID metrics, while it achieves better results on the MUSIQ metric for image quality evaluation on real datasets, such as LFW-Test and WebPhoto-Test. The qualitative results in~\cref{fig:lcmlora} demonstrate that both LCM-LoRA and our method can achieve high-quality reconstructed images.

\subsection{Our method using one-step models ($x_0$-prediction-based diffusion models)}
We use one-step models ($x_0$-prediction-based diffusion models) as the backbone to develop our method for the BFR task. We first move the LQ image to the noise space of the one-step models. We make some comparisons in~\cref{tab:1step} and~\cref{fig:1step}. As shown in Table \cref{tab:1step}, our metrics significantly outperform one-step diffusion models in the BFR task, except for the FID metric on the Synthetic dataset. As shown in the qualitative comparison in~\cref{fig:1step},  results of our method using one-step models (as shown in the second and third rows) indicate that these models face challenges with artifacts and blur when reconstructing high-quality images, while our method can reconstruct high-quality images with detailed textures (the fourth row).

\section{Appendix: Additional analysis}

\begin{figure*}[t]
% \vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\linewidth]{app_imgs/sd_latents_compressed.pdf}}
\vskip -0.1in
\caption{The generated results at each timestep of the diffusion sampling process from $T$ to $1$. For example, given one prompt case ``A man with a beard wearing glasses in blue shirt'', the noise in the image is gradually reduced from timestep $T$ to $1$, and the image is eventually generated with clarity (from left to right, top to bottom).}
\label{fig:sd_latents}
\end{center}
% \vskip -0.35in
\end{figure*}

\subsection{Can we regard the LQ image as an intermediate result in SD sampling?}
When we perform SD sampling, the Gaussian noise $z_T$ is gradually denoised into a clear image $z_0$ (see Fig.~\ref{fig:sd_latents}). We use the DDIM schedule with $T=50$. The intermediate result of SD sampling lacks a lot of image detail, while the LQ image mainly loses texture detail compared to the HQ image. Intuitively, we regard the LQ image as an intermediate result of SD sampling, especially at small noise levels (see Fig.~\ref{fig:sd_latents} (\red{red box})).  As shown in Fig.~\ref{fig:intermediate}, we regard the LQ image as the intermediate result at timesteps $t=10,20$, and $30$ (the second column to fourth columns) and perform the remaining steps of the SD sampling, both for real-world LQ image (the first row) and synthetic LQ image (the second row). When we regard the LQ image as the intermediate result with small noise levels, the remaining SD denoise process tends to remove the potential noise in the LQ image. However, this process does not aid in image restoration but instead makes the image smoother (Fig.~\ref{fig:intermediate} (the second column)). Moreover, when we perform the SD denoise process starting with a high noise level using the LQ image, more edge information, such as details of glasses, can be lost (Fig.~\ref{fig:intermediate} (the third to fourth columns)). In conclusion, the degradation of the LQ image is different from that of the noised image at the intermediate step of SD sampling, even at small noise levels.


\begin{figure*}[t]
% \vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\linewidth]{app_imgs/intermediate_compressed.pdf}}
\vskip -0.1in
\caption{The generated results are obtained when we regard the LQ image as the intermediate result and perform the remaining steps of the SD sampling. For more detail, we regard the LQ image as the intermediate result at timesteps $t=10,20$, and $30$ (the second column to fourth column), both for real-world LQ image (the first row) and synthetic LQ image (the second row). In these two examples, We use the prompts ``A man with black hair wearing glasses in a black shirt'' and ``A woman with curly yellow hair'', respectively}
\label{fig:intermediate}
\end{center}
% \vskip -0.35in
\end{figure*}

\subsection{Can we use super-resolution methods for face restoration?}
The purpose of image super-resolution is to increase the resolution of an image while preserving its content and details as much as possible. In contrast, face restoration does not aim to increase image resolution but focuses on recovering image details from the same LQ resolution. As shown in Fig.~\ref{fig:sr}, we naively attempt to use state-of-the-art super-resolution methods~\citep{rombach2022high,wang2024exploiting,wang2021realesrgan} to perform face restoration (the second to fourth columns). We first downsample an LQ image from a resolution of 512 to 128, then use it as the input for the super-resolution method to generate an image with a resolution of 512 (the second to fourth columns). The downsampled image at 128 resolution is upsampled to 512 resolution using bicubic interpolation (Fig.~\ref{fig:sr} (the first column)), and this upsampled image is then used as input for our method to produce the restored image (Fig.~\ref{fig:sr} (the last column)). 

As shown in Fig.~\ref{fig:sr}, the super-resolution methods struggle to recover facial details, whether applied to real-world or synthetic LQ images (the second to fourth columns). Although StableSR~\citep{wang2024exploiting} adds an additional 5,000 face images from the FFHQ dataset~\citep{karras2019style}, it still struggles to recover facial details, such as hair and facial texture (the third column).



\begin{figure*}[t]
% \vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\linewidth]{app_imgs/sr_compressed.pdf}}
\vskip -0.1in
\caption{The super-resolution methods struggle to recover facial details (the second to fourth columns).}
\label{fig:sr}
\end{center}
\vskip -0.2in
\end{figure*}


\begin{figure*}[t]
% \vskip -0.2in
\begin{center}
\centerline{\includegraphics[width=\linewidth]{app_imgs/tattoos_compressed.pdf}}
\vskip -0.1in
\caption{(top) In our training dataset FFHQ, there exist images containing festival-style face paint, as well as rich colors in the hair and head accessories. 
(bottom) Our method \ourmethod can restore high-quality details for complex images with tattoos or festival-style face paint.
}
\label{fig:tattoos}
\end{center}
\vskip -0.34in
\end{figure*}

\subsection{Additional results with tattoos or festival-style face paint}
As shown in~\cref{fig:tattoos} (the third and fourth rows), our method, \ourmethod, demonstrates the ability to reconstruct high-quality details even in challenging cases, such as images featuring tattoos or festival-style face paint. However, when tattoos contain intricate details, such as text (e.g., the last column), accurately recovering these ambiguous elements during high-quality face reconstruction becomes challenging. This limitation may stem from the scarcity of such textures in the training dataset. 
%
An illustration of the complex textures in our training dataset FFHQ~\citep{karras2019style} is also shown in~\cref{fig:tattoos} (the first and second rows), where the festival-style face paints and rich-color hair appear multiple times during training.

\begin{figure*}[t]
% \vskip -0.2in
\begin{center}
\centerline{\includegraphics[width=\linewidth]{app_imgs/multilq3_compressed.pdf}}
\vskip -0.1in
\caption{(top) Our method with variety inputs. (middle) We find that the semantic information from the LQ image suffices as a prior for HQ reconstruction when the degradation level is below a specific threshold (e.g., the third to fifth columns). (bottom) Using non-facial semantic images resulted in reconstructed outputs with artifacts (the third and fourth columns), whereas unrelated facial images provided sufficient semantic priors for generating HQ reconstructions with facial features (the fifth column).}
\label{fig:multilq}
\end{center}
\vskip -0.35in
\end{figure*}

\subsection{LQ semantic information suffices for HQ reconstruction}
In our method (\cref{fig:multilq}, top(a)), \ourmethod, we utilize a Visual Module to extract semantic information from LQ images for HQ reconstruction. To demonstrate that the LQ image suffices to provide a prior for HQ reconstruction, we provide our model with LQ images exhibiting varying levels of degradation, decreasing from left to right (\cref{fig:multilq}, middle(the first row)). The reconstruction results (\cref{fig:multilq}, middle(the second row)) show that the semantic information from the LQ image suffices as a prior for HQ reconstruction when the degradation level is below a specific threshold (\cref{fig:multilq}, middle(the third to fifth columns)). 

Meanwhile, we observe that when the HQ image is used as the input to both the Visual Module and Spatial Encoder (\cref{fig:multilq}, top(b)), the reconstructed image displays similar semantic information to that obtained using the LQ image (\cref{fig:multilq}, bottom(the first column)). This result further indicates that the LQ image provides semantic information similar to that of the HQ image (\cref{fig:multilq}, middle(the last column) vs., bottom(the first column)). 

Than, we verify the provision of paired LQ and HQ images, which are provided to the Visual Module and Spatial Encoder (\cref{fig:multilq}(c)). We also obverse that the reconstructed result shows similar semantic information to the HQ image (\cref{fig:multilq}, bottom(the second column)).

To further assess the importance of facial semantic information from the LQ image for HQ reconstruction, we supplied the Visual Module with non-facial semantic images (\cref{fig:multilq}, top(d)), such as non-facial semantic images (e.g., a image featuring a tree or a solid color) and unrelated facial images (\cref{fig:multilq}, bottom(third and fifth columns)). Using non-facial semantic images resulted in reconstructed outputs with artifacts (\cref{fig:multilq}, bottom(third and fourth columns)), whereas unrelated facial images provided sufficient semantic priors for generating HQ reconstructions with facial features (\cref{fig:multilq}, bottom(fifth columns)).

\subsection{Applying the proposed method to natural image datasets}
For the blind face restoration problem, our method \ourmethod can efficiently extract facial information through the Visual Encoder, as human faces are with less complex semantic information compared with real images from diverse scenarios.
We show several real-image restoration results in~\cref{fig:otherdatasets}. The results are satisfactory for simple textures, but less effective for complex textures. To improve the performance of our method \ourmethod on real image, we plan to use a more powerful VQGAN-LC~\citep{zhu2024scaling} with 100,000 codebooks to act as the visual encoder for our model in future work.

\begin{figure*}[t]
% \vskip -0.2in
\begin{center}
\centerline{\includegraphics[width=\linewidth]{app_imgs/otherdatasets_compressed.pdf}}
\vskip -0.1in
\caption{Results on natural image datasets.}
\label{fig:otherdatasets}
\end{center}
\vskip -0.2in
\end{figure*}

\subsection{Applying perceptual loss in diffusion-based models}
\label{appsub:perloss}

Several existing works~\citep{chung2023parallel,laroche2024fast}  have integrated the perceptual loss in to diffusion-based models.
The forward process of diffusion-based models is a process that iteratively adds Gaussian noise to the representation using:
\begin{equation}\label{eq:ddpm_forward}
% \resizebox{0.3\linewidth}{!}{$
{x_{t}} = \sqrt{{\alpha_t}}{x_{t-1}} + \sqrt{1-\alpha_{t}}\epsilon,
% $},
\end{equation}
where $\alpha_t$ is the predefined variance, and $\epsilon\sim\mathcal{N}(\textbf{0},\textbf{I})$. Recursively, let $\bar\alpha_t=\prod^{i=t}_{i=1}\alpha_i$, we have:
\begin{equation}\label{eq:ddpm_forward2}
% \resizebox{0.3\linewidth}{!}{$
{x_{t}} = \sqrt{\bar\alpha_t}{x_0} + \sqrt{1-\bar\alpha_{t}}\epsilon.
% $},
\end{equation}
When applying perceptual loss in diffusion-based models, the primary difference between our method and existing works~\citep{chung2023parallel,laroche2024fast} lies in how the noise-free real image $x_0$ is obtained. Our approach uses $x_0$ at the final of the inference steps of the latent consistency model. In contrast, existing works~\citep{chung2023parallel,laroche2024fast} derive $x_0$ from $x_t$ at an intermediate step $t$ by directly applying the inversion of forward process using~\cref{eq:ddpm_forward2}:
\begin{equation}\label{eq:ddpm_inversion}
% \resizebox{0.3\linewidth}{!}{$
% {x_{t}} = \sqrt{\bar\alpha_t}{x_0} + \sqrt{1-\bar\alpha_{t}}\epsilon.
{\hat{x}_0}=\frac{1}{\sqrt{\bar\alpha_t}}(x_t-\sqrt{1-\bar\alpha_{t}}\epsilon).
% $},
\end{equation}

As shown in~\cref{fig:xttox0}, we can observe that the $\hat{x}_0$ obtained from the SD intermediate steps (the first to fifth columns) has an appearance gap compared to the $x_0$ obtained using the full sampling process (the last column).


\begin{figure*}[t]
% \vskip -0.2in
\begin{center}
\centerline{\includegraphics[width=\linewidth]{app_imgs/xttox0_compressed.pdf}}
\vskip -0.1in
\caption{The $\hat{x}_0$ obtained from the intermediate step (the first to fifth columns) has an appearance gap compared to the $x_0$ (the last column).}
\label{fig:xttox0}
\end{center}
\vskip -0.2in
\end{figure*}

\begin{figure*}[t]
% \vskip -0.2in
\begin{center}
\centerline{\includegraphics[width=\linewidth]{app_imgs/celeba3_compressed.pdf}}
\vskip -0.1in
\caption{
Qualitative comparisons of baselines on the synthetic of CelebA-Test for BFR.
}
\label{fig:celeba2_hair}
\end{center}
\vskip -0.3in
\end{figure*}

\section{Additional results}
\label{subsec:app_add}
As shown in~\cref{fig:celeba2_hair}, our method shows better hair quality than other methods and better aligns with the Ground Truth. \cref{tab:app_artist_example} shows the quantitative comparison on the \textit{synthetic} image of~\cref{fig:celeba2_hair}. Our method surpasses the baselines in two image quality metrics: MUSIQ and IDS. The Ground Truth has the best perceptual quality with the best MUSIQ metric 77.64. Actually, since the low-quality images are losing high-frequency information, the restoration is a random process to complement the high-frequency details (by varying seeds when adding noise).



\begin{table}[t]
    \caption{ 
    Quantitative comparison on the \textit{synthetic} image of~\cref{fig:celeba2_hair}.
    The best results are in \textbf{bold}, and the second best results are \underline{underlined}.
    }
    \vspace{-2mm}
    \label{tab:app_artist_example}
    \centering
      \centering
          {\setlength{\tabcolsep}{1pt}\renewcommand{\arraystretch}{1.2}
          \resizebox{0.475\columnwidth}{!}{
            \begin{tabular}{cl|cccc}
            \toprule
                &{\multirow{2}{*}{Dataset}} & \multicolumn{4}{c}{\textit{Synthetic} dataset}  \\
                & & \multicolumn{4}{c}{Celeba-Test} \\
                % \midrule
                % \cmidrule{1-14}
                &\diagbox{Method}{Metrics} & MUSIQ$\uparrow$ & IDS$\downarrow$ &PSNR$\uparrow$ & SSIM$\uparrow$ \\
                 \midrule
                 &Input  &17.44&37.44&24.24&0.624\\
                 \midrule
                  \multirow{8}{*}{\rotatebox{90}{\makecell{\makecell{CNN/Transformer\\-based}}}} & PULSE &71.97&69.90&21.22&0.561\\
                  & DFDNet &{75.96}&27.42&\textbf{25.03}&0.620 \\
                  & PSFRGAN &69.85&36.50&23.05&0.594\\
                  & GFPGAN &74.84&\underline{26.10}&24.11&\underline{0.621} \\
                 & GPEN   & 71.06 &30.71&\underline{24.53} &\textbf{0.628}\\
                 & RestoreFormer &75.57&26.52&23.69&0.595\\
                 & VQFR &74.23&32.97&23.70&0.598\\
                 &CodeFormer   & \underline{76.19}&28.55&24.25&0.612\\
                 \midrule
                  \multirow{4}{*}{\rotatebox{90}{\makecell{\makecell{Diffusion\\-based}}}} 
                  & DR2 &66.03&44.32&22.65&0.582 \\
                  & DifFace &67.57&35.14 &23.91&0.609\\
                 &PGDiff &69.44&54.98 &22.35&0.586\\
                 &\textbf{Ours} &\textbf{76.36} & \textbf{25.91} &{23.65}&{0.606}\\
                \bottomrule
            \end{tabular}
            }}\vspace{-2mm}
\end{table}

We present additional qualitative comparisons of the baselines on real-world images from the LFW-Test, WebPhoto-Test, and WIDER-Test datasets in~\cref{fig:app_realword}. As shown in~\cref{fig:app_realword}, our method can reconstruct more realistic details in forehead wrinkles (first and second rows), eyes and eyebrows (third and fourth rows), and hair (fifth and sixth rows). These results demonstrate that our method outperforms the baselines in real-world scenarios.

\begin{figure*}[t]
\begin{center}
\centerline{\includegraphics[width=\textwidth]{app_imgs/app_realword_compressed.pdf}}\vspace{-2mm}
\caption{Qualitative comparisons of baselines on the real-world images from LFW-Test, WebPhoto-Test, and WIDER-Test. \textbf{Zoom in for a better view}.}\vspace{-6mm}
\label{fig:app_realword}
\end{center}
\end{figure*}



In~\cref{fig:app_celeba,fig:app_lfw,fig:app_web,fig:app_wider},
% 
we show additional reconstructed results on the synthetic dataset (i.e., CelebA-Test~\citep{karras2017progressive}) and the real-world dataset (i.e., LFW-Test~\citep{huang2008labeled}, WebPhoto-Test~\citep{wang2021gfpgan}, and WIDER-Test~\citep{yang2016wider}). 
% 
Our \ourmethod produces high-quality facial components and more realistic details compared to previous methods. We can generate high-quality images even under heavy degradation, while previous methods fail to do so (see \cref{fig:app_web} and \cref{fig:app_wider}).

\begin{figure*}[t]
% \vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\linewidth]{app_imgs/celeba2_compressed.pdf}}
% \vskip -0.1in
\caption{Qualitative comparison on the synthetic dataset Celeba-Test.
%shows that our \textit{InterICM} can restore more realistic facial details (e.g., skins and hair) than previous methods. 
\textbf{Zoom in for a better view}.
}
\label{fig:app_celeba}
\end{center}
% \vskip -0.35in
\end{figure*}

\begin{figure*}[t]
% \vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\linewidth]{app_imgs/realword-lfw2_compressed.pdf}}
% \vskip -0.1in
\caption{Qualitative comparison on the real-world dataset LFW-Test under mild degradation. 
%shows that our \textit{InterICM} can restore more realistic facial details (e.g., skins and hair) than previous methods. 
\textbf{Zoom in for a better view}.
}
\label{fig:app_lfw}
\end{center}
% \vskip -0.35in
\end{figure*}

\begin{figure*}[t]
% \vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\linewidth]{app_imgs/realword-webphoto2_compressed.pdf}}
% \vskip -0.1in
\caption{Qualitative comparison on the real-world dataset WebPhoto-Test under medium degradation.
%shows that our \textit{InterICM} can restore more realistic facial details than previous methods. 
\textbf{Zoom in for a better view}.
}
\label{fig:app_web}
\end{center}
% \vskip -0.35in
\end{figure*}

\begin{figure*}[t]
% \vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\linewidth]{app_imgs/realword-wider2_compressed.pdf}}
% \vskip -0.1in
\caption{Qualitative comparison on the real-world dataset WIDER-Test under heavy degradation. 
%shows that our \textit{InterICM} can restore more realistic facial details than previous methods.
\textbf{Zoom in for a better view}.
}
\label{fig:app_wider}
\end{center}
% \vskip -0.35in
\end{figure*}


\end{document}
