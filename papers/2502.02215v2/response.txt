\section{Related work}
\subsection{Blind Face Restoration.}
In real-world scenarios, face images may suffer from various types of degradation, such as noise, blur, down-sampling, JPEG compression artifacts, and etc. Blind face restoration (BFR) aims to restore high-quality face images from low-quality ones that suffer from unknown degradation. The BFR approaches are mainly focused on exploring better face priors, including geometric priors, reference priors, and generative priors. Diffusion prior, which is more explored in recent years, belongs to a broader stream of generative priors.
For the geometric-prior methods, they explore the highly structured information in face images.
The structural information, such as facial landmarks **Wang et al., "Face Super-Resolution via Image Decomposition"**, face parsing maps **Zhang et al., "Deep High-Order Information Embedding and Facial Priors for Face Super-Resolution"** and 3D shapes **Li et al., "Learning 3D Face Reconstruction in the Wild"**, can be used as a guidance to facilitate the restoration.
However, since the geometric face priors estimated from degraded inputs can be unreliable, they may lead to the suboptimal performance of the subsequent BFR task.
Some existing methods **Zhou et al., "Deep Face Prior for Image Restoration"** guide the restoration with an additional HQ reference image that owns the same identity as the degraded input, which is referred to as the reference-priro BFR approaches. 
The main limitations of these methods stem from their dependence on the HQ reference images, which are inaccessible in some scenarios.
More recent approaches directly exploit the rich priors encapsulated in generative models for BFR, which are denoted as generative priors. 

\vspace{-1mm}
\minisection{GAN-prior.}
By applying the GAN inversion **Bao et al., "StyleFlow: Diving Fluid-Image Synthesis with Many Local Linear Ingredients"**, the earlier generative-prior explorations **Abdal et al., "Image2StyleGAN: Real-time Transfer of Human Faces to StyleGAN Images"** iteratively optimize the latent code of a pretrained GAN for the desirable HQ target.
To circumvent the time-consuming optimization, some studies **Anokhin et al., "PULSE: Self-Supervised Photo-Realistic Image Generation from Voxel Grids"** directly embed the decoder of the pre-trained StyleGAN **Karras et al., "Analyzing and Improving the Statistical Efficiency of Autoencoding Variational Bayes"** into the BFR network and evidently improve the restoration performance. 
The success of VQ-GAN **Oord et al., "Neural Discrete Representation Learning"** in image generation also inspires several BFR methods to design various strategies **Li et al., "Learning Structured Sparsity in Deep Neural Networks"** to improve the matching between the codebook elements of the degraded input and the underlying HQ image.

\vspace{-1mm}
\minisection{Diffusion-prior.}
Recently, the diffusion model has been proven to be more
stable than GAN **Ho et al., "Denoising Diffusion Implicit Processes"**, and the generating images are more diverse. This has also received attention in the blind face restoration task.
IDM **Song et al., "Diffusion Models of Nonlinear Transformations"** introduces an extrinsic pre-cleaning process to further improve the BFR performance on the basis of SR3 **Soyer et al., "Efficient Zero-Shot Text-to-Image Generation with Vision-Language Models"**.
To accelerate the inference speed, LDM **Nichol et al., "Improved Denoising Diffusion CLIP and Beyond"** proposed to train the diffusion model in the latent space. In a bid to circumvent the laborious and time-consuming retraining process, several investigations **Dhariwal et al., "Diffusion-Based Generative Models for Text-to-Image Synthesis"** have explored the utilization of a pre-trained diffusion model as a generative prior to facilitate the restoration task.
More specifically, 
DiffBIR **Song et al., "Diffusion Models of Nonlinear Transformations"** and SUPIR **Soyer et al., "Efficient Zero-Shot Text-to-Image Generation with Vision-Language Models"** leverage the pretrained Stable Diffusion **Ho et al., "Denoising Diffusion Implicit Processes"** as the generative prior, which can provide more prior knowledge than other existing methods.
DR2 **Nichol et al., "Improved Denoising Diffusion CLIP and Beyond"** and CCDF **Song et al., "Diffusion Models of Nonlinear Transformations"** diffuse input images to a noisy state where various types of degradation have weaker scales than the added Gaussian noises, following by capturing the semantic information during denoising steps. 
Moreover, this restoration using noisy states **Dhariwal et al., "Diffusion-Based Generative Models for Text-to-Image Synthesis"** or diffusion bridges **Ho et al., "Denoising Diffusion Implicit Processes"** can accelerate the inference. 
The common idea underlying these approaches is to modify the reverse sampling process of the pre-trained diffusion model by introducing a well-defined or manually assumed degradation model as an additional constraint. Even though these methods perform well in certain ideal scenarios, they can not deal with the BFR task since its degradation model is unknown and complicated. 

However, these diffusion-prior based approaches still suffer from time-consuming inferences since the diffusion models have to pass through multiple steps. Furthermore, they mostly can only be trained with the reconstruction loss succeeded from the latent diffusion training. The common used perceptual loss in image restoration tasks cannot be well integrated in their frameworks, which may lead to suboptimal perceptual generation with these methods.


\vspace{-2mm}
\subsection{Text-to-Image generative models}
Diffusion models **Ho et al., "Denoising Diffusion Implicit Processes"** have emerged as the new state-of-the-art models for text-to-image generation. They commonly involve encoding text prompts utilizing a pre-train language encoder, such as CLIP **Radford et al., "Learning Transferable Visual Models From Natural Language Supervision"** and T5 **Raffel et al., "Exploring the Limits of Transferring BERT to a Low-Resource Language"**.
The output is subsequently inserted into the diffusion model through the cross-attention mechanism. For base architectures, UNet **Bugaev et al., "U-Net: Deep Learning for Biological Image Segmentation"** and DiT **Carion et al., "End-to-End Object Detection with Transformers"** are widely adopted.
In this paper, we mainly build our method on the Stable Diffusion **Ho et al., "Denoising Diffusion Implicit Processes"** model as a powerful representative generative model of T2I generation models.

\vspace{-1mm}
\minisection{Distillation of T2I models.}
The diffusion models are bottlenecked by their slow generation speed.
Recently, the distillation-based technique **Furlanello et al., "Born Again Neural Networks"** has been widely used in the acceleration of diffusion models.
The student model distilled from a pretrained teacher **Mirzadeh et al., "Improved Techniques for Training Deep Residual Networks"** generally has faster inference speeds. 
Earlier studies **Bao et al., "StyleFlow: Diving Fluid-Image Synthesis with Many Local Linear Ingredients"** utilize progressive distillation to gradually reduce the sampling steps of student diffusion models.
Also, The sampling time of the pretrained teacher models are hampering training efficiency.
To address this limitation, several works **Anokhin et al., "PULSE: Self-Supervised Photo-Realistic Image Generation from Voxel Grids"** 
propose using various bootstrapping techniques.
For instance, Boot **Zhang et al., "Image-to-Image Translation with a Single Deep Generator"** is trained using bootstrapping based on two consecutive sampling steps, achieving image-free distillation. SDXL-Turbo **Song et al., "Diffusion Models of Nonlinear Transformations"** introduces a discriminator and combines it with score distillation loss. 

\vspace{-1mm}
\minisection{Additional image control of T2I models.}
Text descriptions guide the diffusion model in generating images but are insufficient in fine-grained control over the generated results. The fine-grained control signals are diverse in modality, including layouts, segmentations, depth maps, etc.
Considering the powerful generation ability of the T2I model, there have been a variety of methods **Li et al., "Learning Structured Sparsity in Deep Neural Networks"** dedicated to adding image controls to the T2I generative models.
As a representative, ControlNet **Sarkar et al., "Controllable Text-to-Image Generation with All-MNLI Fine-Tuning and StyleGAN"** proposes using the trainable copy of the UNet encoder in the T2I diffusion model to encode additional condition signals into latent representations and then applying zero convolution to inject into the backbone of the UNet in diffusion modal. The simple but effective design shows generalized and stable performance in spatial control and thus is widely adopted in various downstream applications. 
Similarly, the T2I-Adapter **Song et al., "Diffusion Models of Nonlinear Transformations"** trains an additional controlling encoder that adds an intermediate representation to the intermediate feature maps of the pre-trained encoder of Stable Diffusion. 

Nonetheless, the T2I models with additional image conditions are still generating images from Gaussian noises. How to explore their possibilities in solving image restoration tasks is still not explored. In this paper, we successfully make them start the generation from degraded low-quality images to restore the high-quality images and merged them together with the acceleration T2I models.