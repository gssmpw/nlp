\section{Related Work}
Hate speech detection has been extensively studied in NLP, primarily for high-resource languages such as English. Early methods relied on lexicon-based approaches and classical machine learning models like Na√Øve Bayes and Support Vector Machines (SVMs). These models, while effective for basic classification, struggled with contextual nuances. Deep learning techniques, particularly convolutional neural networks (CNNs) and recurrent neural networks (RNNs), improved performance by capturing sequential dependencies in text. However, the advent of transformer-based architectures, such as BERT, HateBERT, and XLM-Roberta, has significantly advanced the field by leveraging contextual embeddings and attention mechanisms. Recent works have demonstrated the effectiveness of fine-tuning pre-trained transformers for hate speech detection across multiple languages. Additionally, multilingual models such as MURIL and IndicBERT have shown promise for low-resource Indian languages. Our study builds upon these prior works by focusing on hate speech detection in Telugu using multilingual transformer models. Furthermore, we explore the impact of translation-based classification and compare it with direct training on Telugu text. Our approach integrates LoRA fine-tuning and model ensembling to optimize classification accuracy while addressing the challenges associated with linguistic variations in Telugu hate speech.