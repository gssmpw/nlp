\section{Problem Setup and Contributions}
\subsection{Problem Setup}
\label{sec: setup}
We study the following variant of fixed-confidence best arm identification for quantile bandits. 

\textbf{Arms and quantile rewards.}
The learner is given a set of arms $\A = \{1, 2, \dots, K\}$ with a stochastic reward setting. That is, for each arm $k \in \A$, the observations/realizations of its reward are i.i.d. random variables from some fixed but unknown reward distribution with CDF~$F_k$. This defines a (lower) quantile function $Q_k \colon [0,1] \to \R$ for each $k \in \A$ as follows:\footnote{The equality follows from the right-continuity of $F_k$.}
\begin{equation}
    Q_k(p) \coloneqq \sup \{ x  \in \R : F_k(x) < p \} 
    =
    \inf \{ x \in \R : F_k(x) \ge p \}.
\end{equation} 
The learner is interested in identifying an arm $\hat{k}$ with the highest $q$-quantile.
% , i.e., 
% an arm $\hat{k}$ satisfying
% \begin{equation}
% % \label{def: performance def}
%      Q_{\hat{k}}(q)
%         =
%         \max_{a \in \A}
%         Q_{a}(q).
% \end{equation}
While the reward of each arm is allowed to be unbounded, 
we assume the $q$-quantile of each arm to be bounded in a known range $[0, \lambda]$.\footnote{We note that setting the lower limit to 0 is without loss of generality, and regarding the interval length $\lambda$, even a crude upper bound is reasonable since the sample complexity will only have logarithmic dependence; see Theorem~\ref{theorem: upper bound}.}
%\footnote{Any finite interval of length $\lambda$ can be shifted to this range.}
% We treat $q \in (0, 1)$ as a fixed constant (e.g., $q = 1/2$ for the median) throughout the paper, meaning its dependence may be omitted in $O(\cdot)$ notation.
We let $\gP = \gP(q, \lambda)$ denote the collection of all distributions with $q$-quantile in $[0, \lambda]$, and let $\cE \coloneqq \gP
^K$ be the collection of all possible instances the learner could face.  We will sometimes write $\PP_{\nu}[\cdot]$ and $\EE_{\nu}[\cdot]$ to explicitly denote probabilities and expectations under an instance $\nu \in \cE$.

\textbf{1-bit communication constraint.} We frame the problem as having a single learner that makes decisions, and a single agent that observes rewards and sends information on them to the learner. In Remark \ref{rem:assump} below, we discuss how this can also have a multi-agent interpretation. 
% 
With a single agent, the following occurs at each iteration/time~$t \ge 1$ indexing the number of arm pulls:
\begin{enumerate}[topsep=0pt, itemsep=0pt]
    \item The learner asks the agent
    to pull an arm $a_t \in \A$, and sends the agent some side information~$S_t$.
    % based on the history of the game until time $t-1$.

    \item The agent pulls $a_t$ and observes a random reward $r_{a_t, t}$
    distributed according to CDF $F_{a_t}$.
    
    \item The agent transmits a 1-bit message to the learner, where the message is based on $r_{a_t, t}$ and $S_t$.

    \item The learner decides on arm $a_{t+1} \in \A$ and side information $S_{t+1}$,
    based on arms and the 1-bit information
    received in iterations $1, \ldots, t$.
   
\end{enumerate}
We will focus on the \emph{threshold query model}, where at iteration $t$, the side information $S_t$ is a query of the form 
``Is $r_{a_t, t} \le \gamma_t$?'' and the 1-bit message is the corresponding binary feedback $\boldsymbol{1}\{ r_{a_t,t} \le \gamma_t \}$.
The learner will only use such queries as side information in our algorithm, though the problem itself is of interest for both threshold queries and general 1-bit quantization methods (possibly having different forms of side information).


\begin{remark} \label{rem:assump}
    We do not impose any (downlink) communication constraint from the learner to the agent, as this cost is typically not expensive.  While we framed the problem as having a single agent for clarity, we are motivated by settings where the agent at each time instant could potentially correspond to a different user/device.  For this reason, and also motivated by settings where agents are low-memory sensors, we assume that the agent is `memoryless', meaning the 1-bit message transmitted cannot be dependent on rewards observed from previous arm pulls.  The preceding assumptions were similarly adopted in some of the most related previous works \cite{hanna2022solving, mitra2023linear, mayekar2023communication}.
    % This is motivated by settings where agents observing rewards are low memory sensors,  or where the agent at any given iteration may differ from those at the previous iterations.
\end{remark}


\textbf{$\epsilon$-relaxation.}
Fix a QMAB instance $\nu  \in \cE$,
and let $k^* \in \A$ be an arm with the largest $q$-quantile for the instance $\nu$.
Instead of insisting on identifying an arm with the exact highest quantile, we relax the task by only requiring the identified arm $\hat{k}$ to be at most $\epsilon$-suboptimal in the following sense:
    \begin{equation}
    \label{def: performance def}
    \hat{k} \in
    \A_{\epsilon}(\nu) \coloneqq 
    \Big\{ k \in \A 
    \ \Big\vert\
     Q_k(q)
        \ge
        Q_{k^*}(q)
        - \epsilon
        \Big\}.
\end{equation}
This allows us to limit the effort on distinguishing arms whose $q$-quantile rewards are very close to each other; analogous relaxations are common in the BAI literature.
This relaxation is also motivated by the threshold query model mentioned above; specifically, we will see in Section \ref{sec: log lambda epsilon dependence} that achieving~\eqref{def: performance def} under the threshold query model requires
$\Omega(\log(\lambda/ \epsilon))$ arm pulls even in the case of \textit{deterministic} two-arm bandits.
Our goal is to design an algorithm to identify an arm satisfying~\eqref{def: performance def} with high probability while using as few arm pulls as possible.



\subsection{Summary of Contributions.}
\label{sec: contributions}
With the problem setup now in place, we summarize our main contributions as follows: 


\begin{itemize}[topsep=0pt, itemsep=0pt]
    \item We provide an algorithm (Algorithm~\ref{alg: main}) for our setup, with the uplink communication satisfying the 1-bit constraint. Unlike standard bandit algorithms that compute empirical statistics using lossless observations of rewards, we use a noisy binary search subroutine for the learner to estimate the quantile rewards (see Appendix~\ref{sec: appendix QuantEst}).
    
    \item  We introduce fundamental arm gaps~$\Delta_k$ (Definition~\ref{def: our gap}) that generalize those proposed in prior work (see Remark~\ref{rem: gap generalization}). These gaps capture the
    difficulty of our problem setup in the sense that the problems with positive gaps essentially coincide with the set of problems that are solvable; see Theorem~\ref{thm: zero gap is unsolvable} and Remark \ref{rem: picking large enough c} for precise statements.

    \item We provide an instance-dependent upper bound on the number of arm pulls to guarantee~\eqref{def: performance def} with high probability (Corollary~\ref{cor: combined guarantee}), expressed in terms of $\lambda, \epsilon$, and fundamental arm gap~$\Delta_k$.
    Our upper bound scales logarithmically with $\lambda/\epsilon$, which contrasts with the existing upper bound for mean-based bandits with 1-bit quantization scaling linearly with $\lambda$~\cite{vial2020one, hanna2022solving}.
    

    \item  We also derive a worst-case lower bound (Theorem~\ref{thm: lower bound unquantized}) showing that our upper bound is tight to within logarithmic factors under mild conditions, and can even be tight to within constant factors when the target error probability $\delta$ decays to zero fast enough.  We additionally provide a lower bound (Theorem \ref{thm: log lambda/epsilon dependence}) showing that $\Omega(\log(\lambda/ \epsilon))$ dependence is unavoidable under threshold queries in arbitrary scaling regimes.  
    The former lower bound is applicable even in the absence of communication constraints, so we can conclude that restricting to 1-bit feedback has a minimal impact on the sample complexity, at least in terms of scaling laws.

\end{itemize}