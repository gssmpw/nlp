\section{Related Work}
The multi-armed bandit (MAB) problem was first studied in the context of clinical trials in~\cite{thompson1933likelihood} and was formalized as a statistical problem in~\cite{Robbins1952SomeAO}. The related work on MAB is extensive (e.g., see \cite{slivkins2019introduction, Csa18} and the references therein); we only provide a brief outline here, emphasizing the most closely related works.

\textbf{Best arm identification.}
The early work on MAB focused on balancing the trade-off between exploration 
% (pulling different arms to gather information) 
and exploitation
% (pulling the current best arm to maximize reward) 
for cumulative regret minimization.
% (or equivalently regret minimization).
The best arm identification (BAI) problem was introduced in~\cite{EvenDar2002PACBF} as a ``pure exploration" problem, where the goal is to find from an arm set $\A$, the arm $k^* = \argmax_{k \in \A} \mu_k$ with the highest mean reward (the ``best'' arm).
Subsequent work on BAI includes \cite{bubeck2009pure, Audibert2010BestAI, gabillon2012unified, karnin2013almost, jamieson2014lil, kaufmann2016complexity, garivier2016optimal}, and these are commonly categorized into the
fixed budget setting and fixed confidence setting.
In the fixed confidence setting, which is the focus of our work, the target error probability is fixed, and the objective is to devise an algorithm that identifies the best arm, in the Probably Approximately Correct (PAC) sense, using a minimal average number of arm pulls.  
Formally, an algorithm is $\delta$-PAC correct if it satisfies 
$
    \sup_{\nu} \mathbb{P}\big(\hat{k} \ne k^* \big) \le \delta,
$ where $\hat{k}$ is the output of the algorithm, $k^*$ is the best arm, and the supremum is taken over the collection of instances $\nu$ such that there exists a unique best arm.
A lower bound of $\sum_{k \ne k^*} \Delta_k^{-2} \log(\delta^{-1})$ on the sample complexity was given in
~\cite{mannor2004sample}, where $\Delta_k = \mu_{k^*} - \mu_k$ is the arm suboptimiality gap.
Several subsequent algorithms managed to match the dependence on $\Delta_k$ of the lower bound to within a doubly logarithmic factor. 
Despite the multitude of algorithms, these are usually based on one of the following general sampling strategies: arm/action elimination, upper confidence bounds (UCB), lower upper confidence bound (LUCB), and Thompson sampling.  See~\cite{jamieson2014best} for an overview and relationships between these sampling strategies.


\textbf{Quantile bandits.}
In certain real-world applications, the mean reward does not satisfactorily capture the merits of certain decisions. 
This has motivated the use of other risk-aware performance measures in place of the mean~\cite{yu2013sample}, such as the mean-variance risk, the (conditional) value-at-risk, and quantile rewards -- see~\cite{tan2022survey} for an extensive survey.
Among these, our work is most closely related to
the quantile multi-armed bandit problem (QMAB), a variant of the MAB problem in which the learner is interested in the arm(s) with the highest quantile reward (e.g., the median).
This is useful when dealing with heavy-tailed reward distributions or risk-sensitive applications, where a decision-maker might prioritize minimizing risk by focusing on lower quantiles (e.g., optimizing worst-case outcomes) or targeting the top-performing outcomes by focusing on higher quantiles.
In particular,~\cite{szorenyi2015qualitative, david2016pure, nikolakakis2021quantile, howard2022sequential} studied QMAB in BAI in the fixed confidence setting. Compared to mean-based BAI, the definition of the arm suboptimality gap $\Delta_k$ is not as straightforward, but this has been resolved in~\cite{nikolakakis2021quantile, howard2022sequential}. Based on the suboptimiality gap, a lower bound of the form $\sum_{k} \Delta_k^{-2} \log(\delta^{-1})$ was given in~\cite{nikolakakis2021quantile} for suitably-defined~$\Delta_k$. 
Algorithms in~\cite{nikolakakis2021quantile} and~\cite{howard2022sequential}, which are based on arm elimination and LUCB respectively, were shown to match the dependence on $\Delta_k$ of the lower bound, to within a doubly logarithmic factor. 
Other variants of quantile bandit problems include
fixed confidence median BAI with contaminated distributions \cite{altschuler2019best};
fixed confidence quantile BAI with differential privacy \cite{nikolakakis2021quantile}; fixed budget quantile BAI~\cite{zhang2021quantile}; and
quantile bandit regret minimization~\cite{torossian2019mathcal}.

\textbf{Communication-constrained bandits.}
Most work in MAB assumes that the arms' reward can be observed directly by the learner (with full precision).
However, this assumption may be impractical for real-world applications in which the reward observations are done by some agent (sensor) before being communicated to the learner (central server). 
This motivated the distributed MAB framework, which has garnered significant attention in recent research; see~\cite{amani2023distributed},~\cite[Appendix A]{salgia2023distributed} and the references therein.
The distributed MAB studies most pertinent to this work are those that focused on the quantization of the reward feedback communicated from agent to learner~\cite{vial2020one, hanna2022solving, mitra2023linear, mayekar2023communication}, which is motivated by applications where uplink communication bandwidth is limited (e.g., those using low-power sensors such as drones and wearable healthcare devices).
In particular,~\cite{vial2020one, hanna2022solving} studied constant bit
quantization schemes for
cumulative regret minimization problem in mean-based bandits, where only a constant number of bits are used to communicate each reward observation.
They showed that if the rewards are all supported on $[0, 1]$, then there exists a 1-bit quantization scheme that can achieve 
regret comparable to those in unquantized setups.
However, \cite[Sec. 3]{hanna2022solving} showed that if the rewards are supported on $[0, \lambda]$ for general $\lambda > 0$, then the same scheme would result in a regret that scales linearly in $\lambda$.
They further established that, in order to attain a natural set of sufficient (albeit not necessary) conditions for matching the unquantized regret to within a constant factor, at least 2.2 bits per reward observation are necessary.  
This suggests a possible inherent challenge, or at least a need for different techniques, when using 1-bit quantization.  
Finally, while some distributed MAB studies considered BAI problems~\cite{hillel2013distributed, karnin2013almost, tao2019collaborative, reda2022nearoptimal}, we are unaware of any that addressed the number of bits of feedback per round or used quantile-based performance measures.