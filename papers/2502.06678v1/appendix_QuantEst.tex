\section{Quantile Estimation Subroutine}
\label{sec: appendix QuantEst}

\subsection{Noisy Binary Search}
\label{sec: appendix MNBS reformulation}
We first momentarily depart from MAB and discuss the monotonic noisy binary search (MNBS) problem of~\cite{karp2007noisy}; see also the end of Appendix~\ref{sec: appendix quant est related work} for a summary of some related work on noisy binary search.
The original problem formulation was stated in terms of finding a special coin $i$ among $n$ coins, but this can be restated as follows: 
We have a random variable~$R$ with an unknown CDF $F$ and a list of $n$ points $x_1  \le \cdots \le x_n$ such that $Q(\tau) \in [x_1, x_n]$, and the goal is to find an index $i$  satisfying 
\begin{equation}
\label{eq: MNBS guarantee}
    [F(x_i), F(x_{i+1})] \cap  (\tau - \Delta, \tau + \Delta) \ne 
    \varnothing
\end{equation}
via adaptive queries of the form $\mathbf{1}(R \le x_j)$. Note that each query $\mathbf{1}(R \le x_j)$ is an independent Bernoulli random variable with parameter $F(x_j)$.
We will make use of the following main result from~\cite[Theorem 1.1]{gretta2023sharp}.

\begin{proposition}[Noisy binary search guarantee]
\label{prop: MNBS guarantee}
For any $\delta \in (0, 1)$ and relaxation parameter $\Delta \le \min(\tau, 1-\tau)$, the MNBS algorithm in \cite{gretta2023sharp}
output an index $i$ after at most
$O \big( \frac{1}{\Delta^2}  \log  \frac{n}{\delta} \big)$
queries\footnote{\label{footnote: constants MNBS}The expression for the number of iterations in \cite{gretta2023sharp} is more complicated because it has some terms with explicit constant factors, but in $O(\cdot)$ notation it simplifies to $O \big( \frac{1}{\Delta^2}  \log  \frac{n}{\delta} \big)$. We do not specify the exact number of loops in Algorithm~\ref{alg: quantile interval}, as doing so is somewhat cumbersome and the focus of our work is on the scaling laws.} and $i$ satisfies~\eqref{eq: MNBS guarantee} with probability at least $1- \delta$. 
\end{proposition}
The bulk of the MNBS algorithm in \cite{gretta2023sharp} is based on Bayesian multiplicative weight updates: Start with a uniform prior over which of the $n$ intervals crosses quantile $\tau$, make the query at $x_j$ whose $F(x_j)$ is nearest to $\tau$ under current distribution, update the posterior by multiplying intervals on one side of the query by $1 + c \Delta$ and the other side by $1 - c \Delta$ for some fixed constant $c$, and repeat. Other MNBS algorithms such as those in \cite{karp2007noisy}, or even a naive binary search with repetitions (see \cite[\S 1.2]{karp2007noisy}), could also be used to solve the MNBS problem, but we choose  \cite{gretta2023sharp} since it has the best known scaling of the query complexity. Further comparisons of the relevant theoretical guarantees and practical performance can be found in \cite{gretta2023sharp}.


\subsection{Quantile Estimation with 1-bit Feedback}
\label{sec: appendix quant est related work}
The MNBS algorithm can be implemented under our 1-bit communication-constrained setup. Specifically, the learner decides which arm $k$ to query as well as the point $x_j$  to query, and then sends a threshold
query ``Is $R_k \le x_j$?'' as side information to the agent, where $R_k$ is the random reward (variable) of the arm~$k$ with CDF $F_k$. The agent will then pull arm $k$ and reply with a 1-bit binary feedback corresponding to the observation. Note that 
while the $O \big( \frac{1}{\Delta^2}  \log  \frac{n}{\delta} \big)$ queries for a given arm are done in an adaptive manner, the queries themselves can be requested at different time steps without any requirement of agent memory. 
A high-level description of the implementation for a fixed arm is given in Algorithm~\ref{alg: quantile interval}. This gives us the following guarantee, which is a simple consequence of Proposition~\ref{prop: MNBS guarantee}.




\begin{algorithm}
    \caption{Communication-constrained quantile estimation subroutine ($\mathtt{QuantEst}$ in
Algorithm~\ref{alg: main})}
    \label{alg: quantile interval}

   \textbf{Input}: 
    Arm with reward $R$ distributed according to CDF $F$,
    a list $\mathbf{x}$ of $n$ points $x_1 \le  \cdots \le x_n$,
    quantile $\tau \in (0, 1)$ satisfying $Q(\tau) \in [x_1, x_n]$,
    approximation parameter~$\Delta \le \min(\tau, 1-\tau)$,
    error probability $\delta \in (0,1)$
    
    \textbf{Output} Index $i \in \{1, \dots, n-1\}$    

\begin{algorithmic}[1]
      
        \For{$t = 1$ to $t_{\max}$ (with\footref{footnote: constants MNBS} $t_{\max} = O \big( \frac{1}{\Delta^2}  \log  \frac{n}{\delta} \big)$)}

          \State \textbf{At Learner:}
          
          \State~~~~Pick index $j$
          % $j \in \{1,  \ldots, n-1\}$
          according to Bayesian weight update as in~\cite{gretta2023sharp}
          
          \State~~~~Send threshold query 
          ``Is $R \le x_j$?'' to the agent

            \State \textbf{At Agent:}
          
          \State~~~~Pull arm and observe reward $r$
          
          \State~~~~Send 1-bit feedback $\mathbf{1}(r \le x_j)$ to the learner

        
        \EndFor

    \State Return index $i$  according to~\cite{gretta2023sharp}

\end{algorithmic}    
\end{algorithm}


\begin{corollary}[$\mathtt{QuantEst}$ guarantee]
\label{cor: QuantEst guarantee}
Let $(F, \mathbf{x}, \tau, \Delta, \delta)$ be a valid input of Algorithm~\ref{alg: quantile interval}, and let~$n$ be the number of points in $\mathbf{x}$. 
Then the algorithm outputs  an index $i$ after at most
$O \big( \frac{1}{\Delta^2}  \log  \frac{n}{\delta} \big)$
queries and $i$  satisfies
    $\mathbb{P}
    \left([F(x_i), F(x_{i+1})] \cap  (\tau - \Delta, \tau + \Delta) = \varnothing \right) < \delta$.
\end{corollary}


\textbf{Related work on noisy binary search and quantile estimation.}
We briefly recap the original MNBS problem in \cite{karp2007noisy, gretta2023sharp}:
There are $n$ coins whose unknown probabilities $p_j \in [0, 1]$ are sorted in nondecreasing order, where flipping coin $j$ results in head with probability $p_j$. The goal is to identify a coin $i$ such that the interval $[p_i, p_{i+1}]$ has a nonempty intersection with $(\tau - \Delta, \tau + \Delta)$. This model subsumes noisy binary search with a fixed noise level \cite{burnashev1974interval, ben2008bayesian, dereniowski2021noisy, gu2023optimal} (where $p_j = \frac{1}{2} - \Delta$ for $j \leq i$ and $p_j = \frac{1}{2} + \Delta$ otherwise) as well as regular binary search (where $p_j \in \{0, 1\}$). As we discussed in Appendix~\ref{sec: appendix MNBS reformulation}, this problem can be reformulated into the problem of estimating (the quantile of) a distribution using threshold/comparison queries, where the noise in the feedback is stochastic.  This quantile estimation problem has been generalized to a non-stochastic noise setting \cite{meister2021learning, okoroafor2023non}, and was also studied in the context of online dynamic pricing and auctions \cite{kleinberg2003value, leme2023pricing, leme2023description}. 
In particular, \cite[Algorithm~1]{leme2023pricing} is similar to Algorithm~\ref{alg: quantile interval} (or equivalently subroutine $\mathtt{QuantEst}$ used on Lines~\ref{ltk def} and~\ref{utk def} of Algorithm~\ref{alg: main}), in the sense that both use noisy binary search to identify the quantile of a \textit{single} distribution. However, they use the naive binary search with repetitions to form confidence intervals containing the quantile, which has a suboptimal complexity $O \big( \frac{1}{\Delta^2} \log n \log  \frac{\log n}{\delta} \big)$; see \cite[\S 1.2]{karp2007noisy} for details.  Overall, while ideas from the existing literature on quantile estimation of a \textit{single} distribution with threshold queries may provide useful context, they do not readily translate into Algorithm~\ref{alg: main} or the analysis that led to our main contributions.








\subsection{Proof of Lemma~\ref{lem: good events} (Bounding the Probability of Event E) }
\label{sec: proof event E}
\begin{proof}[Proof of Theorem~\ref{lem: good events}]
    For a fixed $t \ge 1$ and a fixed $k \in \mathcal{A}_t$,
    we have
    \begin{equation}
        \Pr{
        \overline{E_{t, k, l}}
        }
        \le  \frac{\delta \cdot \Delta^{(t)}}{2 |\mathcal{A}_t|}
        \quad
        \text{and}
        \quad
         \Pr{
        \overline{E_{t, k, u}}
        }
        \le  \frac{\delta \cdot \Delta^{(t)} }{2 |\mathcal{A}_t|}
        \quad
    \end{equation}
    by the guarantee of the $\mathtt{QuantEst}$ (see Corollary~\ref{cor: QuantEst guarantee}).
    Applying the union bound, we obtain
    \begin{equation}
        \Pr{\overline{E}}
        \le 
        \sum_{t \ge 1}
        \sum_{k \in \mathcal{A}_t} 
        \frac{\delta \cdot \Delta^{(t)}}{ |\mathcal{A}_t|}
        \le 
        \sum_{t \ge 1}
        |\mathcal{A}_t| \cdot 
        \frac{\delta \cdot \Delta^{(t)}}{ |\mathcal{A}_t|}
        =
        \delta
         \sum_{t \ge 1}
         \Delta^{(t)}
         =
         \delta
         \sum_{t \ge 1}
         2^{-t} 
         \le \delta
    \end{equation}
    as desired. The number of arm pulls~\eqref{eq: QuantEst arm pulls} follows immediately from the guarantee of $\mathtt{QuantEst}$ from Corollary \ref{cor: QuantEst guarantee}, $|\A_t| \le |\A| = K$, 
    and the number of points $n = \Theta(c\lambda/\epsilon)$.
\end{proof}