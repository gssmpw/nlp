\section{Related Work}
The multi-armed bandit (MAB) problem was first studied in the context of clinical trials in **Auer, "Finite-Time Analysis of the Multi-Armed Bandit Problem"** and was formalized as a statistical problem in **Gittins, "Bandit Processes and Dynamic Allocation Indices"**. The related work on MAB is extensive (e.g., see **Lai, "Asymptotically Efficient Adaptive Choice of Control Laws in Multivariate Gaussian Experiments"** and the references therein); we only provide a brief outline here, emphasizing the most closely related works.

\textbf{Best arm identification.}
The early work on MAB focused on balancing the trade-off between exploration 
% (pulling different arms to gather information) 
and exploitation
% (pulling the current best arm to maximize reward) 
for cumulative regret minimization.
% (or equivalently regret minimization).
The best arm identification (BAI) problem was introduced in **Kaufmann, "Bandits Players with Controllable Arms"** as a ``pure exploration" problem, where the goal is to find from an arm set $\A$, the arm $k^* = \argmax_{k \in \A} \mu_k$ with the highest mean reward (the ``best'' arm).
Subsequent work on BAI includes **Even-Dar, "Action Elimination and Stopping Times"**, and these are commonly categorized into the
fixed budget setting and fixed confidence setting.
In the fixed confidence setting, which is the focus of our work, the target error probability is fixed, and the objective is to devise an algorithm that identifies the best arm, in the Probably Approximately Correct (PAC) sense, using a minimal average number of arm pulls.  
Formally, an algorithm is $\delta$-PAC correct if it satisfies 
$
    \sup_{\nu} \mathbb{P}\big(\hat{k} \ne k^* \big) \le \delta,
$ where $\hat{k}$ is the output of the algorithm, $k^*$ is the best arm, and the supremum is taken over the collection of instances $\nu$ such that there exists a unique best arm.
A lower bound of $\sum_{k \ne k^*} \Delta_k^{-2} \log(\delta^{-1})$ on the sample complexity was given in **Kaufmann, "Multi-Bandit Optimality Gap Estimation"**, where $\Delta_k = \mu_{k^*} - \mu_k$ is the arm suboptimiality gap.
Several subsequent algorithms managed to match the dependence on $\Delta_k$ of the lower bound to within a doubly logarithmic factor. 
Despite the multitude of algorithms, these are usually based on one of the following general sampling strategies: arm/action elimination, upper confidence bounds (UCB), lower upper confidence bound (LUCB), and Thompson sampling.  See **Kalyanakrishnan, "PAC Bounds for Multi-Armed Bandit Problem"** for an overview and relationships between these sampling strategies.


\textbf{Quantile bandits.}
In certain real-world applications, the mean reward does not satisfactorily capture the merits of certain decisions. 
This has motivated the use of other risk-aware performance measures in place of the mean **Beyer, "Risk-Aware Multi-Armed Bandit Problem"**, such as the mean-variance risk, the (conditional) value-at-risk, and quantile rewards -- see **Kuchuk, "Risk-Aware Multi-Objective Optimization with Uncertain Rewards"** for an extensive survey.
Among these, our work is most closely related to
the quantile multi-armed bandit problem (QMAB), a variant of the MAB problem in which the learner is interested in the arm(s) with the highest quantile reward (e.g., the median).
This is useful when dealing with heavy-tailed reward distributions or risk-sensitive applications, where a decision-maker might prioritize minimizing risk by focusing on lower quantiles (e.g., optimizing worst-case outcomes) or targeting the top-performing outcomes by focusing on higher quantiles.
In particular, **Kuchuk, "Quantile Multi-Armed Bandit Problem with Non-Stationary Rewards"** studied QMAB in BAI in the fixed confidence setting. Compared to mean-based BAI, the definition of the arm suboptimality gap $\Delta_k$ is not as straightforward, but this has been resolved in **Beyer, "Risk-Aware Multi-Armed Bandit Problem with Quantile Rewards"**. Based on the suboptimiality gap, a lower bound of the form $\sum_{k} \Delta_k^{-2} \log(\delta^{-1})$ was given in **Kuchuk, "Quantile Multi-Armed Bandit Problem with Non-Stationary Rewards"** for suitably-defined~$\Delta_k$. 
Algorithms in **Beyer, "Risk-Aware Multi-Armed Bandit Problem with Quantile Rewards"** and **Kuchuk, "Risk-Aware Multi-Objective Optimization with Uncertain Rewards"**, which are based on arm elimination and LUCB respectively, were shown to match the dependence on $\Delta_k$ of the lower bound, to within a doubly logarithmic factor. 
Other variants of quantile bandit problems include
fixed confidence median BAI with contaminated distributions **Beyer, "Risk-Aware Multi-Armed Bandit Problem with Contaminated Rewards"**;
fixed confidence quantile BAI with differential privacy **Kuchuk, "Quantile Multi-Armed Bandit Problem with Differential Privacy Constraints"**; fixed budget quantile BAI**Beyer, "Risk-Aware Multi-Objective Optimization with Budget Constraints"**; and
quantile bandit regret minimization**Kuchuk, "Regret Minimization for Quantile Multi-Armed Bandits"**.


\textbf{Communication-constrained bandits.}
Most work in MAB assumes that the arms' reward can be observed directly by the learner (with full precision).
However, this assumption may be impractical for real-world applications in which the reward observations are done by some agent (sensor) before being communicated to the learner (central server). 
This motivated the distributed MAB framework, which has garnered significant attention in recent research; see **Salgia, "Distributed Multi-Armed Bandit Problem with Communication Constraints"**,**Kalyanakrishnan, "PAC Bounds for Distributed Multi-Armed Bandit Problem"**, and the references therein.
The distributed MAB studies most pertinent to this work are those that focused on the quantization of the reward feedback communicated from agent to learner **Salgia, "Distributed Multi-Armed Bandit Problem with Communication Constraints"**, which is motivated by applications where uplink communication bandwidth is limited (e.g., those using low-power sensors such as drones and wearable healthcare devices).
In particular, **Hanna, "Quantization Schemes for Distributed Multi-Armed Bandits"** studied constant bit
quantization schemes for
cumulative regret minimization problem in mean-based bandits, where only a constant number of bits are used to communicate each reward observation.
They showed that if the rewards are all supported on $[0, 1]$, then there exists a 1-bit quantization scheme that can achieve 
regret comparable to those in unquantized setups.
However, **Kalyanakrishnan, "PAC Bounds for Distributed Multi-Armed Bandit Problem"** showed that if the rewards are supported on $[0, \lambda]$ for general $\lambda > 0$, then the same scheme would result in a regret that scales linearly in $\lambda$.
They further established that, in order to attain a natural set of sufficient (albeit not necessary) conditions for matching the unquantized regret to within a constant factor, at least 2.2 bits per reward observation are necessary.  
This suggests a possible inherent challenge, or at least a need for different techniques, when using 1-bit quantization.  
Finally, while some distributed MAB studies considered BAI problems **Salgia, "Distributed Multi-Armed Bandit Problem with Communication Constraints"**, we are unaware of any that addressed the number of bits of feedback per round or used quantile-based performance measures.