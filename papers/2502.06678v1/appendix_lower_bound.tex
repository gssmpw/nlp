\section{Lower Bounds}
\label{sec: appendix lower bound}

\subsection{Proof of Theorem~\ref{thm: lower bound unquantized} (Lower Bound for the Unquantized Variant)}
\label{sec: appendix unquantized lower bound}

Since we are adapting the instance from~\cite[Theorem  4]{nikolakakis2021quantile}, we will omit certain details for brevity and instead will focus on the main differences.

\begin{proof}[Proof of Theorem~\ref{thm: lower bound unquantized}]
    Define the following class of distributions parametrized by $w \in (0, q)$:
\begin{equation}
\label{eq: mix Dirac and uniform}
    g_{w}(x) \coloneqq
   w \delta(x) + 1-w,
\end{equation}
i.e., $g_w$ is a mixture of the Dirac delta function and a uniform distribution on $[0, 1]$. 
Fix $w,\gamma \in (0, q)$ such that $w+\gamma  \le q $. Note that $g_w$ has a higher $q$-quantile than $g_{w+\gamma}$ since
\begin{equation}
\label{eq: diff of arms}
    G^{-1}_w(q) -
    G^{-1}_{w+\gamma}(q) = 
    \frac{q - w}{1-w} - 
    \frac{q - (w+\gamma)}{1-(w+\gamma)} =
    \frac{(1-q)\gamma}{(1-w)(1-w-\gamma)}> 0,
\end{equation}
where $G_{w}^{-1}$ is the lower quantile function of $g_w$.


We now use~\eqref{eq: mix Dirac and uniform} to define a set of $K$ instances $\{\nu^{(1)}, \ldots, \nu^{(K)} \} \subseteq \cE$ for our QMAB problem. Here, 
each $\nu^{(j)}$ is a different instance of the arm distributions, with $\nu^{(j)}_k$ being the CDF of arm $k$ for instance~$j$.
Fix $\gamma \in (0,  1/6]$.
For $\nu^{(1)}$, we define the arms' PDF by
\begin{equation}
\label{eq: bad instance}
    \nu^{(1)}_k \coloneqq
    \begin{cases}
        g_{1/3- \gamma} & \text{ if } k = 1 \\
        g_{1/3} & \text{ if } k \ne 1.
    \end{cases}
\end{equation}
For $j = 2, \ldots, K$, we define the arms' PDF of $\nu^{(j)}$ by
\begin{equation}
    \nu^{(j)}_k  \coloneqq
    \begin{cases}
        g_{1/3- \gamma} & \text{ if } k = 1 \\
        g_{1/3- 2\gamma} & \text{ if } k = j \\
        g_{1/3} & \text{ if } k \ne 1 \text{ or } j.
    \end{cases}
\end{equation}
We will use $\nu^{(1)}$ as the ``hard instance" in our lower bound.
By assumption of our $\epsilon$, we have arm 1 being the unique satisfying arm for $\nu^{(1)}$. Using~\eqref{eq: epsilon condition}) and~\eqref{eq: diff of arms}, we have
\begin{equation}
\label{eq: epsilon upper bound}
    \epsilon  \le
    \frac{1}{2} \Big( Q^{(1)}_{k^*}(q) - \max \limits_{k \ne k^*} Q^{(1)}_k(q) \Big)
    = \frac{G^{-1}_{1/3 -\gamma}(q) - G^{-1}_{1/3}(q) }{2 } = 
    \frac{(1-q)\gamma}{2(2/3 + \gamma )(2/3)}.
\end{equation}
This implies that arm $j$ is the unique satisfying arm for $\nu^{(j)}$ for $j = 2, \ldots, K$ since
\begin{equation}
    G^{-1}_{1/3 -2\gamma}(q) - G^{-1}_{1/3-\gamma}(q)  = 
    \frac{(1-q)\gamma}{(2/3 + 2\gamma )(2/3 + \gamma)}
    =
    \frac{(1-q)\gamma}{2(2/3 + \gamma )(2/3)} \cdot
    \frac{4/3}{2/3 + 2\gamma}
    \ge \epsilon,
\end{equation}
where the inequality follows from~\eqref{eq: epsilon upper bound} and $\gamma \le 1/6$.


To establish the lower bound on the arm pulls for instance $\nu^{(1)}$, we  first upper bound the inverse arm gap $\Delta_j^{-1}$ in terms of $\gamma$ for the arms in $\nu^{(1)}$. For arm 1 and each non-satisfying arm $j \ne 1$, we have
\begin{align}
    \Delta_1 \ge    
    \Delta_j=&\sup
    \left\{
        \Delta \in \left[0, \min(q, 1-q) \right]
        \colon
        G^{-1}_{1/3}(q + \Delta) 
        \le
        G^{-1}_{1/3-\gamma}(q - \Delta) 
        \right\} 
         \label{eq: gap g_gamma first line}
        \\
    =&
    \min\left\{
    \sup
    \left\{
        \Delta \ge 0
        \colon
         \left(\frac{q  +  \Delta  - 1/3}{2/3} \right)
         \le
         \left(\frac{q  - \Delta  - 1/3 +\gamma }{2/3+\gamma }\right)  
        \right\} , 
         q, 1-q
        \right\} \\
      =&   
      \min\left\{\frac{ (1-q)\gamma}
      {(4/3 + \gamma )}, q, 1-q \right\} 
       \label{eq: gap g_gamma last line} \\
      =&   
     \frac{ (1-q)\gamma}
      {(4/3 + \gamma )} \\
    \ge&
    \frac{2(1-q)\gamma}{3},
    \label{eq: delta_k gamma}
\end{align}
where the first inequality follows from the argument in~\eqref{eq: lower bound k* arm gap} and the second inequality follows from $\gamma \le 1/6$. 


Fix an $(\epsilon, \delta)$-reliable algorithm $\pi$ (see Definition~\ref{def: reliable}), and let $\tau \le \infty$ be the total number of arm pulls by $\pi$ on instance $\nu^{(1)}$. We may assume that $\PP_{\nu^{(1)}}[\tau = \infty] = 0$, since otherwise $\E_{\nu^{(1)}}[\tau] = \infty$ and the theorem holds trivially.
For each $j \in \{2, \ldots, K\}$, define event $A_j$ to be 
\begin{equation}
    A_j \coloneqq \{ \pi \text{ terminates and outputs $\hat{k} \ne j$} \}.
\end{equation}
By the definition of $(\epsilon, \delta)$-reliability, we must have
\begin{equation}
    \mathbb{P}_{\nu^{(j)}}\big[A_j\big]
    \le \delta \text{ for each } j \in \{2, \ldots, K\}
    \quad \text{and} \quad
    \mathbb{P}_{\nu^{(1)}}\big[ \tau < \infty \cap   \hat{k} \ne 1 \big] \le \delta.
\end{equation}
    Using the assumption $\PP_{\nu^{(1)}}[\tau = \infty] = 0$ 
    and the event inclusion $\{\hat{k} = j\} \subseteq \{\hat{k} \ne 1\}$, we have
\begin{equation}
    \mathbb{P}_{\nu^{(1)}}\big[A_j^{\complement} \big] =
    \mathbb{P}_{\nu^{(1)}}\big[\tau = \infty \cup \hat{k} = j \big] =
    \mathbb{P}_{\nu^{(1)}}\big[\hat{k} = j \big]
    \le
    \mathbb{P}_{\nu^{(1)}}\big[\hat{k} \ne 1 \big] = 
    \mathbb{P}_{\nu^{(1)}}\big[ \tau < \infty \cap   \hat{k} \ne 1 \big] \le \delta,
\end{equation}
and so 
\begin{equation}
    \mathbb{P}_{\nu^{(1)}}\big[A_j^{\complement} \big] + 
 \mathbb{P}_{\nu^{(j)}}\big[A_j  \big] \le 2 \delta \quad 
 \text{ for each } j \in \{2, \ldots, K\}.
\end{equation}
Let $T_j \le \tau$ be the number of times arm $j$ is pulled on $\nu^{(1)}$. For a fixed $j \in \{2, \ldots, K\}$, we have
\begin{equation}
    \E_{\nu^{(1)}}[T_j]
    \ge 
    \frac{D_{\mathrm{KL}}
    \left(\mathbb{P}_{\nu^{(1)}} \parallel  \mathbb{P}_{\nu^{(j)}}\right)}{12 \gamma^2} 
    \ge
    \frac{1}{12 \gamma^2} \log \left( \frac{1}{4 \delta} \right) 
\end{equation}
where the inequalities follow from \cite[Eqn. 29--34]{nikolakakis2021quantile}.
Summing through $j  = 2, \ldots, K$
and 
we have
\begin{align}
    \E_{\nu^{(1)}}[\tau]
    \ge
    \sum_{j=2}^K \E_{\nu^{(1)}}[T_j] 
    \ge
    \sum_{j=2}^K  
    \frac{1}{12 \gamma^2} \log \left( \frac{1}{4 \delta} \right) 
     \ge
    \frac{1}{2}
    \sum_{j=1}^K
     \frac{1}{12 \gamma^2} \log \left( \frac{1}{4 \delta}  \right),
\end{align}
where the last inequality follows from $K \ge 2$.
Applying the bounds~\eqref{eq: gap g_gamma first line}--\eqref{eq: delta_k gamma} for each $j$ yields
\begin{equation}
    \E_{\nu^{(1)}}[\tau]
     \ge
     \sum_{j=1}^K
    \frac{(1-q)^2}{27 \Delta_j^2}
    \log \left( \frac{1}{4 \delta} \right)
    = \Omega \bigg(
\sum_{k=1}^K
 \frac{1}{\Delta_k^2}
    \log \left( \frac{1}{\delta} \right)
    \bigg),
\end{equation}
as desired.
\end{proof}




\subsection{Proof of Theorem~\ref{thm: log lambda/epsilon dependence} ($\Omega(\log(\lambda/\epsilon))$ Dependence)}
\label{sec: appendix log lambda epsilon dependence}
\begin{proof}[Proof of Theorem~\ref{thm: log lambda/epsilon dependence}]
    Let $\nu$ be a two-arm QMAB instance with deterministic but unknown $q$-quantile rewards $r_1$ and $r_2$
satisfying\footnote{For ease of analysis, we assume $\lambda$ is an integer multiple of $2 \epsilon$.}
\begin{equation}
\label{eq: r1 r2 assumption}
    r_1, r_2 \in  
    \left[0, 2\epsilon, 4 \epsilon, \ldots, \lambda \right]
    \quad
    \text{and}
    \quad
     |r_1- r_2| = 2 \epsilon.
\end{equation}
In this case, the only arm satisfying \eqref{def: performance def} is the one with the higher $q$-quantile. Since the rewards are deterministic, the QMAB problem is equivalent to finding out which of $r_1$ and $r_2$ is higher.

We consider a modified threshold query setup where the learner receives more information at each iteration: At iteration $t$, the learner decides a threshold $X_t \in [0, \lambda]$, and receives a 2-bit comparison feedback in the form of $(\mathbf{1}(r_1 \le X_t), \mathbf{1}(r_2 \le X_t))$.
By design, the number of iterations
required under the 2-bit threshold query setup is at most the number of arm pulls required under the 1-bit threshold query setup.

We now establish the lower bound of $\Omega(\log(\lambda/\epsilon))$ on the number of iterations needed to determine which of $r_1$ and $r_2$ is higher for instance $\nu$ under the 2-bit threshold query setup.
We first claim that for an algorithm to be $(\epsilon, \delta)$-reliable, the learner has to keep querying until receiving some feedback satisfying
\begin{equation}
    \label{eq: two det arm stopping condition}
    (\mathbf{1}(r_1 \le X_t), \mathbf{1}(r_2 \le X_t)) \in \{ (0, 1) ,(1, 0) \},
\end{equation}
which occurs if and only if 
$X_t \in [\min(r_1, r_2), \max(r_1, r_2)]$.
Feedback of the form in~\eqref{eq: two det arm stopping condition} 
is necessary as otherwise instance $\nu$ is indistinguishable from instance $\nu'$ where $r_1$ and $r_2$ are swapped, and the best any algorithm could do is to make a 50/50 guess, which is not $(\epsilon, \delta)$-reliable for $\delta < 0.5$.


To establish the lower bound, we may assume that the learner knows that~\eqref{eq: r1 r2 assumption} holds, since extra information can only weaken a lower bound. With this information, instead of picking $X_t$ from the interval $[0, \lambda]$, the learner could pick $X_t$ only from the list~$X \coloneqq \left[0, 2\epsilon, 4 \epsilon, \ldots, \lambda\right]$ without loss of generality (any other choices would have a corresponding equivalent choice in this set).  
As there is exactly one $x \in X$ that would lead to feedback of the form in~\eqref{eq: two det arm stopping condition}, we need to identify one of $|X|$ possible outcomes, which amounts to learning $\log_2 |X|$ bits.
Since each threshold query gives a 2-bit feedback, the number of threshold queries/iterations needed in the worst case is $\Omega(\log(|X|)) =  \Omega(\log(\lambda/\epsilon))$.
\end{proof}

