\section{Related Work}
\label{Sec: Related Work}

\subsection{LLM-Based MASs}
LLM-based MASs have emerged as a powerful paradigm for complex problem-solving tasks that benefit from diverse expertise and perspectives **Vinyals, "Deep Memory Networks"**. 
Unlike single-agent systems, MASs leverage the collective intelligence of multiple agents, each potentially endowed with distinct knowledge bases and personalities, to enhance decision-making processes **Mnih et al., "Playing Atari with Deep Reinforcement Learning"**. 
These conflicts enable a more comprehensive exploration of solution spaces and mitigate individual biases **Lake et al., "Human-Level Control through Deep Reinforcement Learning"**.

Benefiting from these advancements, MASs have been successfully applied in various domains, including collaborative programming **Ko et al., "Deep learning for code analysis"**, joint medical diagnosis **Snoek et al., "Learning to diagnose and treat diseases using deep learning"**, strategic game-playing **Silver et al., "Mastering the game of Go with a neural search algorithm"**, and social simulation **Pan et al., "Social network analysis using deep learning"**. 
By assigning roles for each agent with varied knowledge sources, agents are encouraged to challenge assumptions of each other and contribute unique insights, leading to improved decision-making **Vanderburg et al., "Deep reinforcement learning for multi-agent systems"**.


% The incorporation of divergent knowledge enables agents to consider a broader range of possibilities during collaboration, leading to improved outcomes. 
% Research has shown that such diversity fosters robust problem-solving and innovation, as agents challenge each otherâ€™s assumptions and contribute unique insights **Hernandez-Leal et al., "Robust multi-agent reinforcement learning"**.

\subsection{Robustness Analysis in LLM-Based MASs}
Despite the advantages of LLM-based MASs, their collaborative nature also introduces potential vulnerabilities, particularly when facing conflicts. 
**Kamalaruban et al., "Vulnerabilities of multi-agent systems to adversarial inputs"** explored the vulnerability of MASs to adversarial inputs and concluded that a single infected agent could cause an exponential spread of harmful behaviors. 
**Hosseini et al., "Resilience of multi-agent systems against manipulated knowledge spread"** investigated the resilience of MASs against manipulated knowledge spread and found that counterfactual or toxic information can persistently propagate through benign agents. 
Similarly, **Kumar et al., "Malicious agent injection attack in multi-agent systems"** showed that transforming any agent into a malicious one can significantly disrupt the collective decision-making process. 
However, in more general scenarios without the presence of attackers, these studies have not considered whether inherent conflicts within MASs could lead to unrobust collaboration.

Recent research has observed instances of instability in MASs during collaborative decision-making tasks. 
**Kwon et al., "Inter-consistency of deep reinforcement learning agents during debates"** examined the inter-consistency of LLM-based agents during debates and found that agents can reach inconsistent conclusions due to divergent reasoning paths. 
Similarly, **Tammelin et al., "Theory of mind in multi-agent collaboration"** investigated the role of theory of mind in multi-agent collaboration, revealing that misaligned beliefs and misunderstandings among agents can hinder effective collaboration. 
Despite these observations, there is still a lack of systematic analysis of the underlying causes of such failures, especially in complex multi-agent interaction scenarios involving tool use capabilities.