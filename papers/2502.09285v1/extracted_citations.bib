@inproceedings{10,
  title={Making the v in vqa matter: Elevating the role of image understanding in visual question answering},
  author={Goyal, Yash and Khot, Tejas and Summers-Stay, Douglas and Batra, Dhruv and Parikh, Devi},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={6904--6913},
  year={2017}
}

@article{11,
  title={Are you talking to a machine? dataset and methods for multilingual image question},
  author={Gao, Haoyuan and Mao, Junhua and Zhou, Jie and Huang, Zhiheng and Wang, Lei and Xu, Wei},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}

@inproceedings{12,
  title={Clevr: A diagnostic dataset for compositional language and elementary visual reasoning},
  author={Johnson, Justin and Hariharan, Bharath and Van Der Maaten, Laurens and Fei-Fei, Li and Lawrence Zitnick, C and Girshick, Ross},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2901--2910},
  year={2017}
}

@inproceedings{13,
  title={Vizwiz: nearly real-time answers to visual questions},
  author={Bigham, Jeffrey P and Jayant, Chandrika and Ji, Hanjie and Little, Greg and Miller, Andrew and Miller, Robert C and Miller, Robin and Tatarowicz, Aubrey and White, Brandyn and White, Samual and others},
  booktitle={Proceedings of the 23nd annual ACM symposium on User interface software and technology},
  pages={333--342},
  year={2010}
}

@misc{14,
	author = {},
	title = {{B}e {M}y {E}yes - {S}ee the world together --- bemyeyes.com},
	howpublished = {\url{https://www.bemyeyes.com/}},
	year = {},
	note = {[Accessed 14-01-2025]},
}

@inproceedings{16,
  title={VQAsk: a multimodal Android GPT-based application to help blind users visualize pictures},
  author={De Marsico, Maria and Giacanelli, Chiara and Manganaro, Clizia Giorgia and Palma, Alessio and Santoro, Davide},
  booktitle={Proceedings of the 2024 International Conference on Advanced Visual Interfaces},
  pages={1--5},
  year={2024}
}

@misc{17,
	author = {},
	title = {mousi --- mousi.org},
	howpublished = {\url{http://mousi.org/}},
	year = {},
	note = {[Accessed 14-01-2025]},
}

@inproceedings{22,
  title={Open scene understanding: Grounded situation recognition meets segment anything for helping people with visual impairments},
  author={Liu, Ruiping and Zhang, Jiaming and Peng, Kunyu and Zheng, Junwei and Cao, Ke and Chen, Yufan and Yang, Kailun and Stiefelhagen, Rainer},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={1857--1867},
  year={2023}
}

@inproceedings{23,
  title={Segment anything},
  author={Kirillov, Alexander and Mintun, Eric and Ravi, Nikhila and Mao, Hanzi and Rolland, Chloe and Gustafson, Laura and Xiao, Tete and Whitehead, Spencer and Berg, Alexander C and Lo, Wan-Yen and others},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={4015--4026},
  year={2023}
}

@article{26,
  title={Emotionqueen: A benchmark for evaluating empathy of large language models},
  author={Chen, Yuyan and Wang, Hao and Yan, Songzhou and Liu, Sijia and Li, Yueze and Zhao, Yi and Xiao, Yanghua},
  journal={arXiv preprint arXiv:2409.13359},
  year={2024}
}

@article{3,
title = {DeepNAVI: A deep learning based smartphone navigation assistant for people with visual impairments},
journal = {Expert Systems with Applications},
volume = {212},
pages = {118720},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2022.118720},
url = {https://www.sciencedirect.com/science/article/pii/S0957417422017432},
author = {Bineeth Kuriakose and Raju Shrestha and Frode Eika Sandnes},
keywords = {Navigation assistant, Deep learning, Blind, Visual impairment, Portable, Smartphone},
abstract = {Navigation assistance is an active research area, where one aim is to foster independent living for people with vision impairments. Despite the fact that many navigation assistants use advanced technologies and methods, we found that they did not explicitly address two essential requirements in a navigation assistant - portability and convenience. It is equally imperative in designing a navigation assistant for the visually impaired that the device is portable and convenient to use without much training. Some navigation assistants do not provide users with detailed information about the obstacle types that can be detected, which is essential to make informed decisions when navigating in real-time. To address these gaps, we propose DeepNAVI, a smartphone-based navigation assistant that leverages deep learning competence. Besides providing information about the type of obstacles present, our system can also provide information about their position, distance from the user, motion status, and scene information. All this information is offered to users through audio mode without compromising portability and convenience. With a small model size and rapid inference time, our navigation assistant can be deployed on a portable device such as a smartphone and work seamlessly in a real-time environment. We conducted a pilot test with a user to assess the usefulness and practicality of the system. Our testing results indicate that our system has the potential to be a practical and useful navigation assistant for the visually impaired.}
}

@article{4,
  title={V-eye: A vision-based navigation system for the visually impaired},
  author={Duh, Ping-Jung and Sung, Yu-Cheng and Chiang, Liang-Yu Fan and Chang, Yung-Ju and Chen, Kuan-Wen},
  journal={IEEE Transactions on Multimedia},
  volume={23},
  pages={1567--1580},
  year={2020},
  publisher={IEEE}
}

@article{40,
  title={Emotional intelligence of large language models},
  author={Wang, Xuena and Li, Xueting and Yin, Zi and Wu, Yue and Liu, Jia},
  journal={Journal of Pacific Rim Psychology},
  volume={17},
  pages={18344909231213958},
  year={2023},
  publisher={SAGE Publications Sage UK: London, England}
}

@article{41,
  title={EmoBench: Evaluating the Emotional Intelligence of Large Language Models},
  author={Sabour, Sahand and Liu, Siyang and Zhang, Zheyuan and Liu, June M and Zhou, Jinfeng and Sunaryo, Alvionna S and Li, Juanzi and Lee, Tatia and Mihalcea, Rada and Huang, Minlie},
  journal={arXiv preprint arXiv:2402.12071},
  year={2024}
}

@inproceedings{5,
  title={Deep learning based wearable assistive system for visually impaired people},
  author={Lin, Yimin and Wang, Kai and Yi, Wanxin and Lian, Shiguo},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision workshops},
  pages={0--0},
  year={2019}
}

@article{9,
  title={A multi-world approach to question answering about real-world scenes based on uncertain input},
  author={Malinowski, Mateusz and Fritz, Mario},
  journal={Advances in neural information processing systems},
  volume={27},
  year={2014}
}

