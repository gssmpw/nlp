\section{Related Work}
\subsection{Visual Question Answering (VQA)}
In recent years, the exceptional capabilities of LMMs such as GPT-4o and LLaVA have garnered significant attention due to their remarkable visual understanding, reasoning abilities, and natural interaction styles. VQA has emerged as a promising technology, which generates accurate and meaningful answers based on visual input and natural language questions posed by users. This form of interaction is very suitable for assisting VI individuals. For instance, a VI individual can take a picture of their surroundings and ask questions such as \textit{"What obstacles are here?"} or \textit{"Is the traffic light ahead red or green?"} The VQA system powered by LMMs can respond to these questions by generating answers based on the visual content, thus enabling the VI individuals to better understand and assess their environment in real time. With the the development of VQA techniques, several VQA datasets have contributed, such as DAQUAR \cite{9}, VQAv2 \cite{10}, FM-IQA \cite{11}, CLEVR \cite{12}, VizWiz-VQA \cite{13}.

\subsection{Evaluating Emotional Intelligence }
\textit{Wang et al.} \cite{40} proposed a novel psychometric evaluation method focusing on emotional understanding and tested it on various large language models (LLMs). The results showed that most LLMs outperformed the average level of human emotional intelligence, with GPT-4 scoring higher than 89\% of humans. \textit{Sabour et al.} introduced EmoBench \cite{41}, a benchmark dataset containing more than 400 meticulously crafted questions aimed at assessing LLMsâ€™ capabilities in emotional reasoning and understanding. Furthermore, \textit{Chen et al.} proposed EmotionQueen \cite{26}, which categorizes emotional intelligence assessment into four subtasks, which are key event recognition, mixed event recognition, implicit emotional recognition, and intention recognition. They also designed two evaluation metrics to measure LLM's performance in recognizing and responding to emotion-related statements.

\subsection{AI for VI Assistance}
Before the advent of large models (LMs), various AI-based systems were developed to assist the VI community. Notable examples include DeepNAVI \cite{3}, V-eye \cite{4}, and the VI assistive system proposed by \textit{Lin et al.} \cite{5}, all of which aimed to provide scene understanding and navigation support for VI individuals. With the rise of LMs, OpenSU \cite{22} integrated SAM \cite{23} to offer more comprehensive scene descriptions, significantly enhancing the independent mobility of VI individuals. Be My AI \cite{14} and VQAsk \cite{16} leverage LMMs not only to describe images but also to perform VQA tasks for VI users. The "Hear World" app , powered by MouSi \cite{17}, offers a wider range of functionalities tailored for the VI community, including VQA, item search, and text recognition.