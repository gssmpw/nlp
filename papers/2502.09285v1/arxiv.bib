@misc{1,
    author="WHO",
    title="Blindness and vision impairment",
    year="2024",
    url="https://www.who.int/news-room/fact-sheets/detail/blindness-and-visual-impairment"
}
@article{2,
  title={Global prevalence of visual impairment associated with myopic macular degeneration and temporal trends from 2000 through 2050: systematic review, meta-analysis and modelling},
  author={Fricke, Timothy R and Jong, Monica and Naidoo, Kovin S and Sankaridurg, Padmaja and Naduvilath, Thomas J and Ho, Suit May and Wong, Tien Yin and Resnikoff, Serge},
  journal={British Journal of Ophthalmology},
  volume={102},
  number={7},
  pages={855--862},
  year={2018},
  publisher={BMJ Publishing Group Ltd}
}
@article{3,
title = {DeepNAVI: A deep learning based smartphone navigation assistant for people with visual impairments},
journal = {Expert Systems with Applications},
volume = {212},
pages = {118720},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2022.118720},
url = {https://www.sciencedirect.com/science/article/pii/S0957417422017432},
author = {Bineeth Kuriakose and Raju Shrestha and Frode Eika Sandnes},
keywords = {Navigation assistant, Deep learning, Blind, Visual impairment, Portable, Smartphone},
abstract = {Navigation assistance is an active research area, where one aim is to foster independent living for people with vision impairments. Despite the fact that many navigation assistants use advanced technologies and methods, we found that they did not explicitly address two essential requirements in a navigation assistant - portability and convenience. It is equally imperative in designing a navigation assistant for the visually impaired that the device is portable and convenient to use without much training. Some navigation assistants do not provide users with detailed information about the obstacle types that can be detected, which is essential to make informed decisions when navigating in real-time. To address these gaps, we propose DeepNAVI, a smartphone-based navigation assistant that leverages deep learning competence. Besides providing information about the type of obstacles present, our system can also provide information about their position, distance from the user, motion status, and scene information. All this information is offered to users through audio mode without compromising portability and convenience. With a small model size and rapid inference time, our navigation assistant can be deployed on a portable device such as a smartphone and work seamlessly in a real-time environment. We conducted a pilot test with a user to assess the usefulness and practicality of the system. Our testing results indicate that our system has the potential to be a practical and useful navigation assistant for the visually impaired.}
}

@article{4,
  title={V-eye: A vision-based navigation system for the visually impaired},
  author={Duh, Ping-Jung and Sung, Yu-Cheng and Chiang, Liang-Yu Fan and Chang, Yung-Ju and Chen, Kuan-Wen},
  journal={IEEE Transactions on Multimedia},
  volume={23},
  pages={1567--1580},
  year={2020},
  publisher={IEEE}
}
@inproceedings{5,
  title={Deep learning based wearable assistive system for visually impaired people},
  author={Lin, Yimin and Wang, Kai and Yi, Wanxin and Lian, Shiguo},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision workshops},
  pages={0--0},
  year={2019}
}
@article{6,
  title={Gpt-4o system card},
  author={Hurst, Aaron and Lerer, Adam and Goucher, Adam P and Perelman, Adam and Ramesh, Aditya and Clark, Aidan and Ostrow, AJ and Welihinda, Akila and Hayes, Alan and Radford, Alec and others},
  journal={arXiv preprint arXiv:2410.21276},
  year={2024}
}
@article{7,
  title={Visual instruction tuning},
  author={Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
  journal={Advances in neural information processing systems},
  volume={36},
  year={2024}
}
@inproceedings{8,
  title={Vqa: Visual question answering},
  author={Antol, Stanislaw and Agrawal, Aishwarya and Lu, Jiasen and Mitchell, Margaret and Batra, Dhruv and Zitnick, C Lawrence and Parikh, Devi},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={2425--2433},
  year={2015}
}
@article{9,
  title={A multi-world approach to question answering about real-world scenes based on uncertain input},
  author={Malinowski, Mateusz and Fritz, Mario},
  journal={Advances in neural information processing systems},
  volume={27},
  year={2014}
}
@inproceedings{10,
  title={Making the v in vqa matter: Elevating the role of image understanding in visual question answering},
  author={Goyal, Yash and Khot, Tejas and Summers-Stay, Douglas and Batra, Dhruv and Parikh, Devi},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={6904--6913},
  year={2017}
}
@article{11,
  title={Are you talking to a machine? dataset and methods for multilingual image question},
  author={Gao, Haoyuan and Mao, Junhua and Zhou, Jie and Huang, Zhiheng and Wang, Lei and Xu, Wei},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}
@inproceedings{12,
  title={Clevr: A diagnostic dataset for compositional language and elementary visual reasoning},
  author={Johnson, Justin and Hariharan, Bharath and Van Der Maaten, Laurens and Fei-Fei, Li and Lawrence Zitnick, C and Girshick, Ross},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2901--2910},
  year={2017}
}
@inproceedings{13,
  title={Vizwiz: nearly real-time answers to visual questions},
  author={Bigham, Jeffrey P and Jayant, Chandrika and Ji, Hanjie and Little, Greg and Miller, Andrew and Miller, Robert C and Miller, Robin and Tatarowicz, Aubrey and White, Brandyn and White, Samual and others},
  booktitle={Proceedings of the 23nd annual ACM symposium on User interface software and technology},
  pages={333--342},
  year={2010}
}
@misc{14,
	author = {},
	title = {{B}e {M}y {E}yes - {S}ee the world together --- bemyeyes.com},
	howpublished = {\url{https://www.bemyeyes.com/}},
	year = {},
	note = {[Accessed 14-01-2025]},
}
@article{15,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}
@inproceedings{16,
  title={VQAsk: a multimodal Android GPT-based application to help blind users visualize pictures},
  author={De Marsico, Maria and Giacanelli, Chiara and Manganaro, Clizia Giorgia and Palma, Alessio and Santoro, Davide},
  booktitle={Proceedings of the 2024 International Conference on Advanced Visual Interfaces},
  pages={1--5},
  year={2024}
}
@misc{17,
	author = {},
	title = {mousi --- mousi.org},
	howpublished = {\url{http://mousi.org/}},
	year = {},
	note = {[Accessed 14-01-2025]},
}
@article{18,
  title={Vialm: A survey and benchmark of visually impaired assistance with large models},
  author={Zhao, Yi and Zhang, Yilin and Xiang, Rong and Li, Jing and Li, Hillming},
  journal={arXiv preprint arXiv:2402.01735},
  year={2024}
}
@article{19,
  title={VIAssist: Adapting Multi-modal Large Language Models for Users with Visual Impairments},
  author={Yang, Bufang and He, Lixing and Liu, Kaiwei and Yan, Zhenyu},
  journal={arXiv preprint arXiv:2404.02508},
  year={2024}
}
@article{20,
  title={Long-Form Answers to Visual Questions from Blind and Low Vision People},
  author={Huh, Mina and Xu, Fangyuan and Peng, Yi-Hao and Chen, Chongyan and Murugu, Hansika and Gurari, Danna and Choi, Eunsol and Pavel, Amy},
  journal={arXiv preprint arXiv:2408.06303},
  year={2024}
}
@inproceedings{21,
  title={Vizwiz grand challenge: Answering visual questions from blind people},
  author={Gurari, Danna and Li, Qing and Stangl, Abigale J and Guo, Anhong and Lin, Chi and Grauman, Kristen and Luo, Jiebo and Bigham, Jeffrey P},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={3608--3617},
  year={2018}
}
@inproceedings{22,
  title={Open scene understanding: Grounded situation recognition meets segment anything for helping people with visual impairments},
  author={Liu, Ruiping and Zhang, Jiaming and Peng, Kunyu and Zheng, Junwei and Cao, Ke and Chen, Yufan and Yang, Kailun and Stiefelhagen, Rainer},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={1857--1867},
  year={2023}
}
@inproceedings{23,
  title={Segment anything},
  author={Kirillov, Alexander and Mintun, Eric and Ravi, Nikhila and Mao, Hanzi and Rolland, Chloe and Gustafson, Laura and Xiao, Tete and Whitehead, Spencer and Berg, Alexander C and Lo, Wan-Yen and others},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={4015--4026},
  year={2023}
}
@article{24,
  author = {David Burmedi and Stefanie Becker and Vera Heyl and Hans-Werner Wahl and Ines Himmelsbach},
  title = {Emotional and social consequences of age-related low vision},
  journal = {Visual Impairment Research},
  volume = {4},
  number = {1},
  pages = {47--71},
  year = {2002},
  publisher = {Taylor \& Francis},
  doi = {10.1076/vimr.4.1.47.15634},
  url = {https://doi.org/10.1076/vimr.4.1.47.15634},
  eprint = {https://doi.org/10.1076/vimr.4.1.47.15634}
}
@article{25,
  title={Loneliness, adaptation to vision impairment, social support and depression among visually impaired elderly},
  journal={International Congress Series},
  volume={1282},
  pages={317--321},
  year={2005},
  note={Vision 2005},
  issn={0531-5131},
  doi={https://doi.org/10.1016/j.ics.2005.04.017},
  url={https://www.sciencedirect.com/science/article/pii/S0531513105007375},
  author={P.F.J. Verstraten and W.L.J.H. Brinkmann and N.L. Stevens and J.S.A.G. Schouten},
  keywords={Loneliness, Adaptation to vision loss, Social support, Depression, Visually impaired elderly, Personal network intervention},
  abstract={The purpose of this study is to investigate the prevalence of loneliness among visually impaired elderly, and its relations with adaptation to vision loss, received social support and depression. Clients aged 55 years or older who contacted Sensis, a rehabilitation centre for visually impaired people, are approached to take part in this observational study. Exclusion criteria are hearing and cognitive impairments. The participants receive an interview by telephone in which loneliness, adaptation to vision loss, received social support and depression are investigated. The results indicate a high prevalence of loneliness (54%) among visually impaired elderly. Compared to visually impaired elderly who are not lonely, the lonely ones show a poorer adaptation to vision loss (mean=22.7 (S.D.=6.79) versus mean=27.8 (S.D.=5.32); t=6.08 (194.84); p=.000), less received social support (mean=26.3 (S.D.=5.99) versus 29.4 (S.D.=4.95); t=3.30 (136); p=.001) and more feelings of depression (mean 12.9 (S.D.=6.46) versus mean=6.7 (S.D.=4.11); t=âˆ’6.41 (107.28); p=.000). Causality cannot be determined on the basis of this observational study. Nonetheless, regarding the high prevalence of loneliness among visually impaired elderly, interventions aimed at reducing this loneliness seem to be highly indicated.}
}

@article{26,
  title={Emotionqueen: A benchmark for evaluating empathy of large language models},
  author={Chen, Yuyan and Wang, Hao and Yan, Songzhou and Liu, Sijia and Li, Yueze and Zhao, Yi and Xiao, Yanghua},
  journal={arXiv preprint arXiv:2409.13359},
  year={2024}
}
@article{27,
  title={Emphi: Generating empathetic responses with human-like intents},
  author={Chen, Mao Yan and Li, Siheng and Yang, Yujiu},
  journal={arXiv preprint arXiv:2204.12191},
  year={2022}
}
@article{28,
  title={Improving multi-turn emotional support dialogue generation with lookahead strategy planning},
  author={Cheng, Yi and Liu, Wenge and Li, Wenjie and Wang, Jiashuo and Zhao, Ruihui and Liu, Bang and Liang, Xiaodan and Zheng, Yefeng},
  journal={arXiv preprint arXiv:2210.04242},
  year={2022}
}
@article{29,
  title={Exploring the role of an emotional support and counselling service for people with visual impairments},
  author={Hodge, Suzanne and Barr, Wally and Bowen, Louise and Leeven, Martina and Knox, Paul},
  journal={British Journal of Visual Impairment},
  volume={31},
  number={1},
  pages={5--19},
  year={2013},
  publisher={SAGE Publications Sage UK: London, England}
}
@article{30,
  title={CoMAE: A multi-factor hierarchical framework for empathetic response generation},
  author={Zheng, Chujie and Liu, Yong and Chen, Wei and Leng, Yongcai and Huang, Minlie},
  journal={arXiv preprint arXiv:2105.08316},
  year={2021}
}
@article{31,
  title={ESC-Eval: Evaluating Emotion Support Conversations in Large Language Models},
  author={Zhao, Haiquan and Li, Lingyu and Chen, Shisong and Kong, Shuqi and Wang, Jiaan and Huang, Kexin and Gu, Tianle and Wang, Yixu and Jian, Wang and Liang, Dandan and others},
  journal={arXiv preprint arXiv:2406.14952},
  year={2024}
}
@article{32,
  title={Towards empathetic open-domain conversation models: A new benchmark and dataset},
  author={Rashkin, Hannah},
  journal={arXiv preprint arXiv:1811.00207},
  year={2018}
}
@article{33,
  title={LoRA: Low-Rank Adaptation of Large Language Models},
  author={J. Edward Hu and Yelong Shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Weizhu Chen},
  journal={ArXiv},
  year={2021},
  volume={abs/2106.09685},
  url={https://api.semanticscholar.org/CorpusID:235458009}
}
@article{34,
  title={Direct preference optimization: Your language model is secretly a reward model},
  author={Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
@inproceedings{35,
  title={Improving automatic vqa evaluation using large language models},
  author={Ma{\~n}as, Oscar and Krojer, Benno and Agrawal, Aishwarya},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={38},
  number={5},
  pages={4171--4179},
  year={2024}
}
@inproceedings{36,
    title = "{ROUGE}: A Package for Automatic Evaluation of Summaries",
    author = "Lin, Chin-Yew",
    booktitle = "Text Summarization Branches Out",
    month = jul,
    year = "2004",
    address = "Barcelona, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W04-1013/",
    pages = "74--81"
}
@inproceedings{37,
    title = "{METEOR}: An Automatic Metric for {MT} Evaluation with High Levels of Correlation with Human Judgments",
    author = "Lavie, Alon  and
      Agarwal, Abhaya",
    editor = "Callison-Burch, Chris  and
      Koehn, Philipp  and
      Fordyce, Cameron Shaw  and
      Monz, Christof",
    booktitle = "Proceedings of the Second Workshop on Statistical Machine Translation",
    month = jun,
    year = "2007",
    address = "Prague, Czech Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W07-0734/",
    pages = "228--231"
}
@article{38,
  title={Bertscore: Evaluating text generation with bert},
  author={Zhang, Tianyi and Kishore, Varsha and Wu, Felix and Weinberger, Kilian Q and Artzi, Yoav},
  journal={arXiv preprint arXiv:1904.09675},
  year={2019}
}
@article{39,
  title={Judging llm-as-a-judge with mt-bench and chatbot arena},
  author={Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={46595--46623},
  year={2023}
}
@article{40,
  title={Emotional intelligence of large language models},
  author={Wang, Xuena and Li, Xueting and Yin, Zi and Wu, Yue and Liu, Jia},
  journal={Journal of Pacific Rim Psychology},
  volume={17},
  pages={18344909231213958},
  year={2023},
  publisher={SAGE Publications Sage UK: London, England}
}
@article{41,
  title={EmoBench: Evaluating the Emotional Intelligence of Large Language Models},
  author={Sabour, Sahand and Liu, Siyang and Zhang, Zheyuan and Liu, June M and Zhou, Jinfeng and Sunaryo, Alvionna S and Li, Juanzi and Lee, Tatia and Mihalcea, Rada and Huang, Minlie},
  journal={arXiv preprint arXiv:2402.12071},
  year={2024}
}
@article{42,
  title={Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling},
  author={Chen, Zhe and Wang, Weiyun and Cao, Yue and Liu, Yangzhou and Gao, Zhangwei and Cui, Erfei and Zhu, Jinguo and Ye, Shenglong and Tian, Hao and Liu, Zhaoyang and others},
  journal={arXiv preprint arXiv:2412.05271},
  year={2024}
}
@article{43,
  title={Qwen2-vl: Enhancing vision-language model's perception of the world at any resolution},
  author={Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and others},
  journal={arXiv preprint arXiv:2409.12191},
  year={2024}
}