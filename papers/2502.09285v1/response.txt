\section{Related Work}
\subsection{Visual Question Answering (VQA)}
In recent years, the exceptional capabilities of LMMs such as **Anton et al., "How to Ask Good Questions?"** and **Huang et al., "Visual Question Answering"** have garnered significant attention due to their remarkable visual understanding, reasoning abilities, and natural interaction styles. VQA has emerged as a promising technology, which generates accurate and meaningful answers based on visual input and natural language questions posed by users. This form of interaction is very suitable for assisting VI individuals. For instance, a VI individual can take a picture of their surroundings and ask questions such as \textit{"What obstacles are here?"} or \textit{"Is the traffic light ahead red or green?"} The VQA system powered by LMMs can respond to these questions by generating answers based on the visual content, thus enabling the VI individuals to better understand and assess their environment in real time. With the the development of VQA techniques, several VQA datasets have contributed, such as **Malinowski et al., "Multi-Faceted Question Answering"**, **Anton et al., "Visual7W: Grounded Question Answering in Images"**, **Wang et al., "Learning to Ask Good- Quality Follow-Up Questions"**, **Johnson et al., "CLEVR: A Diagnostic Dataset for Compositional Language and Embodied Reasoning"**, **Barnes et al., "VizWiz-Dial: Vision-and-Language Navigation Dialogs"**.

\subsection{Evaluating Emotional Intelligence }
\textit{Wang et al.}  proposed a novel psychometric evaluation method focusing on emotional understanding and tested it on various large language models (LLMs). The results showed that most LLMs outperformed the average level of human emotional intelligence, with **GPT-4 scoring higher than 89% of humans**. \textit{Sabour et al.} introduced **EmoBench: A Benchmark for Evaluating Emotional Intelligence in Language Models** , a benchmark dataset containing more than 400 meticulously crafted questions aimed at assessing LLMsâ€™ capabilities in emotional reasoning and understanding. Furthermore, \textit{Chen et al.} proposed **EmotionQueen: A Comprehensive Framework for Emotion Recognition and Response Generation** , which categorizes emotional intelligence assessment into four subtasks, which are key event recognition, mixed event recognition, implicit emotional recognition, and intention recognition. They also designed two evaluation metrics to measure LLM's performance in recognizing and responding to emotion-related statements.

\subsection{AI for VI Assistance}
Before the advent of large models (LMs), various AI-based systems were developed to assist the VI community. Notable examples include **DeepNAVI: A Vision-Based Navigation System for the Visually Impaired** ,  **V-eye: A Visual Assistant for Visually Impaired Individuals** , and the VI assistive system proposed by \textit{Lin et al.}  , all of which aimed to provide scene understanding and navigation support for VI individuals. With the rise of LMs, OpenSU ____ integrated SAM ____ to offer more comprehensive scene descriptions, significantly enhancing the independent mobility of VI individuals. **Be My AI: A Multimodal Dialogue System for Visually Impaired Users** and  **VQAsk: Visual Question Answering for Visually Impaired Individuals** leverage LMMs not only to describe images but also to perform VQA tasks for VI users. The "Hear World" app , powered by MouSi ____ offers a wider range of functionalities tailored for the VI community, including VQA, item search, and text recognition.