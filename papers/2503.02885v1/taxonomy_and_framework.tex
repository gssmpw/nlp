\section{Proposed Framework} 
\label{sec:framework}

We propose a methodological framework, \frameworklong (\framework) (Figure \ref{fig:framework}), to guide policymakers, researchers, and developers in adopting AI-based chatbots more responsibly in educational contexts by incorporating stakeholder perceptions. \framework addresses systematic gaps in the literature while supporting the iterative refinement of its nested taxonomy of perceptions.

The creation of \framework is articulated around the perceptions of the different actors involved in the use of AI chatbots in education. Specifically, it is based on a \emph{stakeholder-first approach} that aims to support key agents in designing and implementing transparent, compliant, and accountable AI systems \cite{bell2023think}. The second component is the \emph{goals} that stakeholders have in mind when designing and/or implementing AI chatbots in educational settings. The \emph{context} component was also added to address the lack of specification of the educational settings in which LLM-based chatbots are used, as shown in the literature review. And finally, the \emph{taxonomy of perceptions} was added as a 'checklist' for policymakers, technologists, and researchers to consult when assessing the presence of specific perceptions in a given setting and is designed to be refined over iterations through future studies.  In the remainder of this section, we will describe each component in depth: \emph{stakeholders}, \emph{context}, \emph{goals}, and \emph{taxonomy of perceptions}, and explain how the framework should be used.

\subsection{Stakeholders} 
\label{sec:framework:stakeholders}

We highlighted how the introduction of LLM-based chatbots, such as Khanmigo, in the classroom could be seen as a top-down decision-making process, where partnerships between private companies and governmental institutions or school districts often make decisions on behalf of direct stakeholders, such as teachers and students, or indirect stakeholders, such as parents and school staff. This echoes a concern already highlighted in the literature regarding how private companies, more often than community members and public servants, set pedagogy and policy in practice \cite{zeide2019robot}. In contrast to this tendency, our framework prioritizes educational stakeholders: we identified students and teachers as the most recurring stakeholders in the literature; however, the systematic gaps we found underscore the need to assess the perceptions of other key stakeholders, such as parents, school staff, ed-tech professionals, and government agencies.

\subsection{Context} 
\label{sec:framework:context}

As pointed out in Section~\ref{sec:survey:findings:gaps}, most studies in the literature have been conducted in the field of higher education. This underscores the need to assess the perceptions of stakeholders at other educational levels, such as younger students who lack the autonomy to make independent decisions, yet are still expected to use LLM-based chatbots as part of their curricula. The  gaps also indicate that most studies focus on the STEM field, with many others having unspecified areas of application. We advocate for the need to assess perceptions of LLM-based chatbots across various disciplines.

\subsection{Goals} 
\label{sec:framework:goals}

The goals of soliciting and incorporating stakeholders' perceptions when developing and deploying AI systems are diverse. In our framework, we propose goals drawn from the literature on responsible design of AI systems and organize them into two focus areas: those focused on system behavior and those focused on educational practice.   When the focus is on system behavior, the goals are related to designing these tools or assessing whether they operate according to responsible AI key principles. We align with the existing literature on transparency, which asserts that the goal of a transparent design must begin with identifying the stakeholders \cite{bell2023think, chaudhry2022transparency}. \citet{bell2023think} established six categories to assist technologists in designing transparent, regulatory-compliant AI systems: validity, trust, learning and support, recourse, fairness, and privacy. \citet{chaudhry2022transparency} additionally emphasizes safety as a crucial component of transparency, especially in education, where mishaps in AI-powered technologies can often go unnoticed unless reported to the relevant authorities by a teacher or a student. Finally, although \citet{louie2022designing} discuss this in relation to social robots rather than LLM-based chatbots, they distance themselves from the traditional designer-determined approach, advocating for the inclusion of culturally and linguistically diverse stakeholders as co-designers and expert informants in the design process. They promote \emph{participatory design}, reporting that such inclusion leads to more engaging designs, improved user experience, and greater academic gains and ownership of technology-based learning. 

In terms of pedagogical goals, the focus is on embedding the responsible principles in educational practices and relating them to specific learning outcomes \cite{dominguez2020data, fu2024navigating}. For example, the in-depth analysis of \citet{jurenka2024towards} refers to the specific learning practices embedded in the LearnLM tutor that the authors evaluated to improve the pedagogical capabilities of the AI system while promoting more responsible and effective learning experiences, including: evaluative practices, feedback on procedural problems, grounding of learning materials, adaptivity to learner level, engagement, etc. In  \framework, we have presented some of the goals that could be associated with implementing responsible practices with AI systems in education, and this is an area that practitioners could expand according to their vision and needs.

\subsection{Taxonomy of perceptions} 
\label{sec:framework:taxonomy}

Our survey identified a gap in the inconsistent terminology used for similar concepts. To address this, our proposed taxonomy serves as a checklist for researchers, ed-tech professionals, and policymakers to assess, refine, and expand stakeholder perceptions, promoting greater consistency in perception research.

\subsection{Putting the \framework framework into practical use} 
\label{sec:framework:practice}

The \framework framework is designed to assist policy makers, researchers and technologists in two areas:

\begin{enumerate}
\item Elicit stakeholders' perceptions to create new or refine existing recommendations to ensure that AI systems are human-centered and responsibly adopted.
\item Guide researchers and practitioners to address gaps in the literature on LLM-based chatbot perceptions through future studies.
\end{enumerate}

In practice, the application of \framework follows the sequence shown in Figure~\ref{fig:framework}: starting with the first component, ``stakeholders,'' one should consider one or more of the bubbles before moving on to the next component.  For example, policymakers and technologists of an ed-tech company might want to assess the perceptions of the parents of elementary school students of LLM-based chatbots when they are applied to humanistic subjects with the goal of improving or assessing their perceived effectiveness. Upon reaching the final component, the taxonomy, one could use it as a checklist to identify the presence of specific perceptions. In this example, when discussing effectiveness, perceptions related to Impact on Processes or Outcomes are likely to arise. Another example involves assessing teachers' perceptions of LLM-based chatbots applied to English-as-a-Foreign-Language. The goals might include improving their experience in using this technology, as well as managing potential risks. When going through the taxonomy ``checklist,'' likely perceptions related to User Experience (UX) and Ethical and Societal Implications, such as cultural understanding (included in the ``Social justice'' perception), might arise. \framework is an open framework, and so any missing elements in any component that emerge from conducting studies in less explored scenarios could be added.  

 