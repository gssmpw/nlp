\section{A Review of perceptions of LLM-based chatbots}
\label{sec:survey} 

We aimed to analyze stakeholder perceptions of LLM-powered chatbots in educational settings, with a focus on identifying the most recurring perceptions for each stakeholder group. We were guided by two research questions:

\textbf{RQ1:} Who are the key stakeholders and what are their perceptions, as identified in the literature?

\textbf{RQ2:} What are the main gaps in the literature regarding stakeholders and their perceptions?

\input{tabs/tab_studies}

\subsection{Methods}
\label{sec:survey:methods}

\subsubsection{Selection procedure}
\label{sec:survey:methods:selection}

We used Google Scholar to search for scientific literature on the topics of interest. To be considered, articles had to include the search strings (``Large language models'' OR ``LLMs'' OR ``LLM-based tutor'' OR ``LLM-powered chatbot'' OR ``ChatGPT'') AND (``perceptions'' OR ``perspectives'' OR ``attitudes'') in their title, abstract, keywords, or introduction. For the selection procedure, we limited the number of results to the first 100 articles, using Google Scholar's relevance sort order.  Articles were included if they met the following criteria: (1) discussed LLM-powered bots in education; (2) addressed perceptions of education stakeholders, such as students, teachers, parents, school staff, or education-related professionals; (3) were based on experimental, quasi-experimental, pre-experimental, or non-experimental research (excluding commentaries, secondary data analyses, and literature reviews); and (4) were written in English. 
 Finally, the study must involve LLM-powered chatbots. Studies that only mentioned ``AI tool'' without specifying whether they referred to large language model-based technology or rule-based systems were excluded.   This resulted in the selection of $N=38$ full-text articles for analysis.

\subsubsection{Content analysis}
\label{sec:survey:methods:analysis}

After an initial reading of the articles, we identified recurring patterns or themes and developed a codebook to capture them. The codebook was developed combining codes taken from articles that employed existing theoretical frameworks (deductive) and codes that emerged from the articles without available codebooks or that did not develop one (inductive). In the latter cases, we annotated the results and discussion sections of the articles. As for the level of implication allowed, we included both explicit references to concepts and those implied by words or phrases. We allowed for flexibility by adding codes throughout the iterative readings. 

Hence, we coded for the frequency, that is, we examined the presence of a selected code in the articles. We identified whether each code or concept reflected negative perceptions or concerns, positive perceptions or opportunities, or neutral perceptions. Additionally, we identified the specific stakeholder group(s) from which each code or concept originated. The validity of the coding process was ensured through multiple rounds of coding, allowing for refinement and consistency across the analysis.

\subsection{Findings}
\label{sec:survey:findings}

\subsubsection{Basic summary information}
\label{sec:survey:findings:summary}

We now summarize the basic break-down of the $N=38$ articles included in our analysis. As shown in Figure~\ref{fig:stats:geo}, the majority of the included articles had samples from Asia ($N = 21$), followed by Europe ($N = 20$), North America ($N = 18$), Oceania ($N = 5$), Latin America ($N = 4$), and Africa ($N = 4$). (Note that several articles included samples from multiple continents.) When the location of the participants was not specified, the location of the authors' university was selected as a proxy. 

\input{figs/fig_stats_new}

Regarding the contexts of application of the LLM-based chatbots, Figure \ref{fig:stats:contexts} illustrates that the most frequent educational context of the studies was STEM ($N = 19$, 50\%), followed by English-as-foreign-language (EFL) ($N = 5$, 13\%) and Humanities ($N = 1$). A considerable number of studies did not specify the educational context ($N = 13$, 34\%).

Table \ref{tab:design} presents a break-down by type of study design. Most of the articles employed non-experimental study designs. 13 studies used interviews or surveys with exclusively open-ended questions, most of which were semi-structured. 12 studies used mixed methods, including surveys with both open- and closed-ended questions, as well as less common methodologies, such as having participants draw a visual representation of their perception of ChatGPT followed by a survey~\cite{ding2023students}. Three studies used quantitative questionnaires. Three articles used quasi-experimental research designs, two of which used pre- and post-test methodology, and one, mixed methods. One study was pre-experimental, using a one-shot case study design. Among the experimental designs, the most common was mixed methods, followed by randomized controlled trials (RCT) and multi-arm randomized trials. 

\subsubsection{\textbf{RQ1:} Key stakeholders and their perceptions}
\label{sec:survey:findings:peceptions}

\input{figs/fig_stakeholders_wrap} As shown in Figure~\ref{fig:stats:stakehoders}, students were the most studied stakeholder group, appearing in 31 of the 38 (82\%) studies, 27 of which involved University-level students and 4 involved K-12 students. Teacher perceptions were analyzed in 12 (32\%) studies.  Additionally, perceptions of industry professionals were considered in 1 study, albeit without explicitly specifying whether these were ed-tech professionals or learners.  No studies included any other stakeholders, such as parents or school administrators.

We identified different types of perception---positive (opportunities/benefits), negative (concerns), and neutral--- in the articles we analyzed. We organized these perceptions into five categories: \textit{Attributes}, \textit{Impact on Processes or Outcomes}, \textit{Functions}, \textit{User Experience}, and \textit{Ethical and Societal Implications}.  Figure~\ref{fig:perceptions} summarizes results of our analysis, showing the total number of mentions of perceptions in each category separately for two stakeholder groups, Students (Figure~\ref{fig:perceptions:students}) and Teachers (Figure~\ref{fig:perceptions:teachers}).  Complete results, with frequencies for each perception type within each category, are provided in Appendix~\ref{app:stats}.  We present additional details about each perception category in the remainder of this section.   Note that, while we do not provide quantitative details about industry professionals as a stakeholder group in either Figure~\ref{fig:perceptions} or Appendix~\ref{app:stats}, because they were only covered in one article, we do include them when discussing qualitative findings.

Overall, the perceptions of both students and teachers were more frequently positive than either negative or neutral: with 58\% of all perceptions were positive and 33\% were negative for students vs. 51\% positive and 38\% negative for teachers.  Additionally, while both students' and teachers' perceptions focused heavily on the ethics of LLM-based chatbots, particularly in a negative or concerned light, students also expressed views on UX, whereas teachers concentrated on the functions of LLM-powered bots. In contrast to the ethics of LLM-based bots, the topics of UX and functions were primarily addressed by their respective stakeholders in terms of opportunities or positive aspects.

\paragraph{Attributes.} This category refers to perceptions of the characteristics of LLM-based chatbots. The papers we surveyed discussed the perceptions of students, teachers, and industry professionals regarding these attributes. The key attributes identified included: Convenience (also referred to as ``accessibility'', ``responsiveness'' or ``speed''), Accuracy (also referred to as ``confidence and trust in LLMs''), Knowledge base, Anthropomorphism (also referred to as ``human-like qualities'', ``perceived humanness'' and ``Natural Language Processing (NLP) capabilities''), and various other, less popular technical specifications, such as character limit and code editing. Perceptions of these attributes were mixed, with each being viewed both positively and negatively, except for Convenience, which was consistently seen as an advantage. Additionally, no neutral perceptions were found regarding these attributes. 

The most frequent positive perception among students and teachers is Convenience. For example, students in \citet{kazemitabaar2024codeaid} appreciated CodeAid's ``24/7 availability'', whereas teachers in \citet{mohamed2024exploring} highlighted the bot's ``real-time interaction and feedback''.

In contrast, the most recurring concern for students, teachers, and industry professionals is Accuracy. For example, students in \citet{weber2024measuring} were concerned with the accuracy of answers provided by LLMs, while teachers in \citet{ghimire2024generative} stated that generative AI can produce ``incorrect or fabricated results.'' However, an industry professional in \citet{chen2024learning} expressed that although she had ``low trust in ChatGPT'', she felt more confident after using it, as it helped her understand what she needed to figure out and made her in general faster. 

\paragraph{Impact on Processes and Outcomes.} This category refers to perceptions related to teaching and learning processes and their results. These perceptions were most commonly expressed by students and teachers, and include: Efficiency, Student motivation, Performance expectancy (a concept from UTAUT \cite{venkatesh2003user} and UTAUT2 \cite{acosta2024analysis}, see Appendix~\ref{app:models} for a summary of these theories), referring to improved student learning and outcomes, Cost-effectiveness, as well as Critical thinking and problem solving skills. Among these, Performance expectancy, Cost-effectiveness, and Critical thinking and problem solving skills generated mixed perceptions. No neutral perceptions were found regarding these impacts.

Efficiency was the most commonly cited positive perception among students, teachers, and industry professionals. For example, students in \citet{kazemitabaar2024codeaid} stated that AI ``helps them work more efficiently,'' while teachers in \citet{jansson2024initial} reported that ``not having to calculate the problem themselves may free up time and make them more efficient as coaches, enabling them to help more students.'' Similarly, industry professionals in \citet{chen2024learning} noted that ``with LLMs, human time and energy could be liberated for more high-level tasks.''

The most common negative perception across students and teachers was the impact on Critical thinking and problem solving skills. For example, a student in \citet{woithe2023understanding} noted that while ChatGPT ``is a tool that's very useful when learning,'' they were ``still a bit scared [that] I'll become incompetent to think on my own.'' Teachers in \citet{ghimire2024generative} also expressed concern, stating that LLMs ``decrease critical thinking.'' 

\input{figs/fig_perceptions}

\paragraph{Functions.} This category refers to perceptions related to uses of LLM-powered technology in the classroom, including Assessment, Personalized tutoring, Material creation, Writing assistance, Brainstorming assistance, Opportunity for practice, Tool for complementing traditional teaching, Facilitating conditions (a construct in UTAUT that describes the degree to which an individual believes an organization’s and technical infrastructure support the use of the system~\cite{venkatesh2003user}), as well as Other functions. Some of these perceptions were viewed both positively, negatively, and neutrally, except for Tool for complementing traditional teaching, which was only perceived neutrally.

The most recurring positive perception about functions across all stakeholders was Personalized tutoring. Students recognized ``the potential for personalized learning support'' \cite{chan2023students} and reported that ``ChatGPT excels in breaking down intricate concepts into more understandable terms'' \cite{zimotti2024future}. Teachers also emphasized the value of ``Personalized teaching/24 hours Teaching Assistant (TA) access'' \cite{ghimire2024generative}, while industry professionals underscored that LLMs ``support human effort'' \cite{chen2024learning}.

The most common negative perception regarding functions among students and teachers related to Facilitating conditions. For students, \citet{Ogbo-Gebhardt2024} highlighted a lack of institutional support on GenAI usage~\cite{chan2023students, Ogbo-Gebhardt2024}. For teachers, concerns included banning LLM-based tools \cite{lau2023ban} and the ``lack of teacher support for using ChatGPT as an educational tool'' \cite{iqbal2022exploring}.  Similarly, the most commonly cited neutral perspective among students involved Facilitating conditions. For example, students in \citet{woithe2023understanding} stated, ``[…] addressing these issues, not by banning ChatGPT, but by implementing regulatory measures and ethical guidelines for its use.'' Teachers, on the other hand, emphasized the need for support in implementing LLM-based bots in the classroom. As one teacher in \citet{iqbal2022exploring} noted, ``I feel like there should be more training available on how to use ChatGPT in the classroom. It would really help if we could get more help from the administration in this.''

\paragraph{User Experience (UX)} This category of perceptions relates to how individuals feel about interacting with LLM-powered bots. It includes concepts such as perceived Usefulness, Ease of use, Intention to use, Hedonic motivation, Well-being, Perceived importance, and Human interaction loss. Many of these—like  Usefulness, Ease of use, Intention to use, and Hedonic motivation—originated in the Technology Acceptance Model (TAM) \cite{davis1989perceived} and evolved in the Unified Theory of Acceptance and Use of Technology (UTAUT) \cite{venkatesh2003user} and UTAUT2 \cite{acosta2024analysis}, often with different names (see Appendices~\ref{app:models} and~\ref{app:scales}). For instance, perceived Usefulness in TAM is called Performance expectancy in UTAUT \cite{sair2018effect}, but we retained the term perceived Usefulness due to its use in studies beyond these frameworks. Similarly, TAM’s Ease of use evolved into UTAUT’s Effort expectancy, reflecting the role of technical skills in technology acceptance \cite{venkatesh2003user}. Except for Human interaction loss, which is perceived only negatively, and Hedonic motivation, viewed positively, negatively, and neutrally, all other concepts are perceived both positively and negatively.

The most commonly reported positive perception among students was perceived Usefulness. Findings in \citet{elkhodr2023ict} show ``a positive perception of ChatGPT as useful'', while \citet{lee2023learning} state that ``Learners’ perception of the AI Teaching Assistant (TA) is on par with that of human TAs in terms of [...] helpfulness.'' The most positive perception among teachers was Ease of use. For example, \citet{jansson2024initial} reports that ``The coaches expressed surprise about how friendly and supportive the AI bot was,'' while teachers in \citet{iqbal2022exploring} mentioned that ChatGPT is ``extremely user-friendly.'' Industry professionals reported positive perceptions of both Usefulness and Intention to use~\cite{chen2024learning}. 
 
The most common concern among both students and teachers is Human interaction loss. For example, a student in \citet{Ogbo-Gebhardt2024} disclosed that ``she felt a bit of anxiety considering how increased adoption of LLM tools could lead to increased social isolation,'' while \citet{chan2023students} reported that ``in academic institutions and education, some were concerned that the widespread use of AI might affect the student–teacher relationship, as students may be 'disappointed and lose respect for teachers.''' Teachers also expressed concerns about the ``lack of human connection'' \cite{mohamed2024exploring}. Interestingly, Ease of use was equally as concerning as Human interaction loss among teachers, with some mentioning that ``they found it difficult to use'' \cite{iqbal2022exploring} when discussing ChatGPT.

The only neutral UX perception in the literature is Hedonic motivation, a UTAUT2 concept referring to the fun or pleasure of using a technology. For instance, \citet{lieb2024student} observed that ``the neutral hedonic evaluation indicates that users had more neutral feelings about how interesting or exciting it was to use NewtBot.''

\paragraph{Ethical and Societal Implications.} This category covers the anticipated impact of introducing LLM-based bots in schools, including Social justice, Job replacement, Reliance, Ethical and privacy concerns, Academic integrity, Social influence (a construct from UTAUT that captures the impact of family and friends on technology acceptance), Workplace changes, and Learning changes. While all were viewed positively and negatively, the last four also had neutral perceptions.

The most frequently mentioned positive perception among students is Reliance on LLM-based bots. For instance, \citet{smith2024toward} noted that ``few students described behaviors that appeared to be intentional cheating (\ie 'generating complete answers'),'' demonstrating a healthy level of reliance on LLMs. Similarly, \citet{kazemitabaar2024codeaid} reported that ``although some students, like S14, felt that they 'over-relied on it too much rather than thinking,' many students displayed signs of self-regulation.'' This positive perception is also evident among industry professionals. A participant in \citet{chen2024learning} compared ChatGPT (GPT-4) and NetLogo, commenting that ChatGPT ``assumed what I wanted it to do, whereas this one makes you specify your assumptions.'' They preferred NetLogo Chat’s approach because ``it makes you think about the code more.'' For teachers, the most common positive perception was social justice, encompassing ``equity,'' ``accessibility,'' and ``cultural understanding.'' Teachers in \citet{mohamed2024exploring} noted that ChatGPT could become a ``tool for enhancing language exposure and cultural understanding […] and a tool for enhancing language accessibility and equity.'' Similarly, teachers in \citet{lau2023ban} stated that ``AI may improve equity and access.''

The most frequent concern among students was also related to Reliance, which, in this case, takes the shape of overreliance. This concern is reflected in statements such as, ``I feel that everyone might gradually lose their ability to think, as people won’t be willing to think anymore. They’ll just rely on machines to do the thinking for them'' \cite{zhang2024future}, or ``While students are specifically informed that AI can make mistakes and the exam keys are even provided, almost half of the students still agree with all the answers provided by ChatGPT, regardless of whether the answers are correct or incorrect'' \cite{ding2023students}. Similar concerns about Reliance were also shared by industry professionals.
The most common concern among teachers is Academic integrity, often framed as ``plagiarism'' or ``AI-assisted cheating.'' For example, teachers in \cite{ghimire2024generative} expressed concern about ``focus on product over process,'' while \citeauthor{iqbal2022exploring} report that most teachers surveyed ``found ChatGPT would be used by students for cheating, and it will make them lazy.''

Finally, the most common neutral perceptions regarding the Ethical and Societal Implications of LLM-powered tools were related to Changes in the workplace for students and both Changes in the workplace and Changes in learning for teachers. Regarding the former, \citeauthor{rogers2024attitudes} found that ``Almost all students agreed that ChatGPT will be integral to their careers,'' while \citeauthor{bernabei2023students} reported that ``students recognized the practical relevance of using such tools in a university context, as they believed it would better prepare them for the future working world, where AI is anticipated to play a prominent role.'' As for the latter, a teacher in \cite{mohamed2024exploring} stated, ``I see ChatGPT playing an increasingly central role in EFL education in the future.''

\subsubsection{\textbf{RQ2:} Gaps in the literature}
\label{sec:survey:findings:gaps}

\paragraph{Stakeholder representation.} Our literature review highlighted a lack of representation of stakeholders beyond students and teachers, such as parents and broader school staff. Figure~\ref{fig:stats:stakehoders} shows that students appeared as the most recurring stakeholders, followed by teachers. This finding mirrors a conclusion from a literature review on robot tutors by~\citet{smakman2020robot}, who also noted the lack of parental representation. It further underscores the need identified by \citet{louie2022designing} to include ``key stakeholders (students, parents, and teachers) as essential to ensure a culturally responsive robot.'' Finally, it echoes the concerns raised by \citet{zeide2019robot} that ``school procurement and implementation of these systems are rarely part of public discussion.''

\paragraph{Context representation.} The second gap highlighted by our literature review was the inadequate representation of the educational context, particularly in terms of discipline and educational levels of the students' sample. Figure~\ref{fig:stats:contexts} illustrates the contexts of application of LLM-based chatbots, with the most frequent ones being STEM, followed by studies with unspecified contexts. Additionally, a lack of diversity in educational levels could be observed in Figure~\ref{fig:stats:stakehoders}, where 27 out of the 31 studies involving students focusing on university-level students and only four studies involving K-12 students. This is a critical issue because, while university students can independently choose whether to use AI technologies, K-12 students typically do not have a say in the implementation of these technology within their curricula \cite{Singer.2023, School}. This underscores the need for further research on perceptions across a wider range of educational contexts.

\paragraph{Homogeneity in assessment of perceptions.} Finally, the wide variety of theories, scales, and terms used across the literature to assess and name perceptions highlights the need for standardization in these aspects (see Appendices~\ref{app:models} and~\ref{app:scales}). For example, in \citet{Ogbo-Gebhardt2024}, the theme of trust in and accuracy of the responses generated by LLM-based bots is considered a Facilitating condition (UTAUT), whereas in multiple other papers \cite{kazemitabaar2024codeaid, ding2023students, yilmaz2023augmented} it is treated as a separate concept. This lack of unified terminology makes it difficult to develop a clear understanding of how stakeholders perceive LLM-based technology. We advocate for the standardization of terms in the field.
