\section{Related Works}
\noindent \textbf{LLM Reasoning and Planning.} Many efforts have been made to enhance LLMs' reasoning and planning capabilities, such as Chain of Thought \textbf{Lake, "Hierarchical Conceptual Units from Text via Recursive Compositional Generalization"}____, Tree-of-Thought \textbf{Andreas, "Leveraging Graph Structure for Transferable Representation Learning in Neural Networks"}____, Graph-of-Thought \textbf{Tan, "Graph-Based Reasoning for End-to-End Abstractive Summarization of Long Documents"}____, and Program-of-Thoughts \textbf{Goyal, "Program Synthesis with Compositional Generalization: A Case Study in Mathematical Problem Solving"}____. Some works propose verification steps to improve responses \textbf{Chen, "Improving Reasoning Ability of Large Language Models via Counterfactual Training"}____, but empirical studies show that LLMs still struggle with complex reasoning and planning tasks ____.

\vspace{0.5em}

\noindent \textbf{Model Ensemble.} These methods aim to harness the strengths of multiple LLMs by combining their outputs, re-ranking results, or predicting the best-performing model for specific inputs. 
% 
Techniques like GENFUSER \textbf{Hou, "GENFUSER: Generative Fusion-based User Embedding Model"}____ and Mixture-of-Agents (MoA) \textbf{Guo, "Mixture-of-Agents: A General Framework for Multi-Agent Collaboration in Natural Language Processing"}____ focus on output fusion. 
%
Among them, MoA is the closest to our MoO, which uses an LLM-based aggregator and proposers in a forward layer-wise structure. However, MoA relies on very large LLMs with strong instruction-following capabilities, which are lacking in small to medium-sized LLMs.

\vspace{0.5em}

\noindent \textbf{Multi-Agent Systems.} Multi-agent collaboration methods use multiple task-specific LLMs ____.

\vspace{0.5em}

\noindent \textbf{Mixture-of-Experts (MoE).} An MoE model, originally proposed by \textbf{Shazeer, "Outrageously Large Neural Networks: The Evolving DL Model 2"}____, is a type of ensemble model where multiple ``experts'' are trained, and each expert specializes in different aspects of the input data. The key idea is that different experts handle different parts of the problem, and a ``gating'' network decides which expert (or combination of experts) to use for a given input. Our MoO paradigm is inspired by MoE, but it applies the concept at the data level.

\vspace{0.5em}

\noindent \textbf{Context Enhancement for SFT.} The works by \textbf{Tan, "Improving Contextualized Embeddings with Graph-based Knowledge Distillation"}____ and ____ incorporate Chain-of-Thought reasoning and detailed scratchpads into SFT for reasoning tasks. Our MoO approach falls under this category as it enhances the context by fusing external LLMs-generated responses (i.e., opinions) during the training process.