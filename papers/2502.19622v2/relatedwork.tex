\section{Related Works}
\noindent \textbf{LLM Reasoning and Planning.} Many efforts have been made to enhance LLMs' reasoning and planning capabilities, such as Chain of Thought \cite{wei2022chain, kojima2022large, fu2022complexity, stechly2024chain}, Tree-of-Thought \cite{yao2024tree}, Graph-of-Thought \cite{yao2023beyond}, and Program-of-Thoughts \cite{chen2022program}. Some works propose verification steps to improve responses \cite{lightman2023let, cobbe2021training, kumar2024training}, but empirical studies show that LLMs still struggle with complex reasoning and planning tasks \cite{chen2024can, xie2024travelplanner, valmeekam2024llms, mirzadeh2024gsm, wang2024planning, xie2024revealing}.

\vspace{0.5em}

\noindent \textbf{Model Ensemble.} These methods aim to harness the strengths of multiple LLMs by combining their outputs, re-ranking results, or predicting the best-performing model for specific inputs. 
% 
Techniques like GENFUSER \cite{jiang2023llm} and Mixture-of-Agents (MoA) \cite{wang2024mixture} focus on output fusion. 
%
Among them, MoA is the closest to our MoO, which uses an LLM-based aggregator and proposers in a forward layer-wise structure. However, MoA relies on very large LLMs with strong instruction-following capabilities, which are lacking in small to medium-sized LLMs.

\vspace{0.5em}

\noindent \textbf{Multi-Agent Systems.} Multi-agent collaboration methods use multiple task-specific LLMs \cite{guo2024large, wang2024rethinking, liu2024agentlite, hong2023metagpt}. These methods are computationally expensive and may not always outperform single-agent solutions with strong prompts.

\vspace{0.5em}

\noindent \textbf{Mixture-of-Experts (MoE).} An MoE model, originally proposed by \citet{shazeer2017outrageously}, is a type of ensemble model where multiple ``experts'' are trained, and each expert specializes in different aspects of the input data. The key idea is that different experts handle different parts of the problem, and a ``gating'' network decides which expert (or combination of experts) to use for a given input. Our MoO paradigm is inspired by MoE, but it applies the concept at the data level.

\vspace{0.5em}

\noindent \textbf{Context Enhancement for SFT.} The works by \citet{li2023think} and \citet{lee2023teaching} incorporate Chain-of-Thought reasoning and detailed scratchpads into SFT for reasoning tasks. Our MoO approach falls under this category as it enhances the context by fusing external LLMs-generated responses (i.e., opinions) during the training process.