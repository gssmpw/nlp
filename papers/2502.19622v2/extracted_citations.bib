@article{chen2022program,
  title = {Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks},
  author = {Wenhu Chen and Xueguang Ma and Xinyi Wang and William W. Cohen},
  journal={Transactions on Machine Learning Research},
  year = {2023},
}

@article{chen2024can,
  title={Can We Rely on LLM Agents to Draft Long-Horizon Plans? Let's Take TravelPlanner as an Example},
  author={Chen, Yanan and Pesaranghader, Ali and Sadhu, Tanmana and Yi, Dong Hoon},
  journal={arXiv preprint arXiv:2408.06318},
  year={2024}
}

@article{cobbe2021training,
  title={Training verifiers to solve math word problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and others},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}

@inproceedings{fu2022complexity,
  title={Complexity-based prompting for multi-step reasoning},
  author={Fu, Yao and Peng, Hao and Sabharwal, Ashish and Clark, Peter and Khot, Tushar},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2022}
}

@article{guo2024large,
  title={Large language model based multi-agents: A survey of progress and challenges},
  author={Guo, Taicheng and Chen, Xiuying and Wang, Yaqi and Chang, Ruidi and Pei, Shichao and Chawla, Nitesh V and Wiest, Olaf and Zhang, Xiangliang},
  journal={arXiv preprint arXiv:2402.01680},
  year={2024}
}

@article{hong2023metagpt,
  title={Metagpt: Meta programming for multi-agent collaborative framework},
  author={Hong, Sirui and Zheng, Xiawu and Chen, Jonathan and Cheng, Yuheng and Wang, Jinlin and Zhang, Ceyao and Wang, Zili and Yau, Steven Ka Shing and Lin, Zijuan and Zhou, Liyang and others},
  journal={arXiv preprint arXiv:2308.00352},
  year={2023}
}

@article{jiang2023llm,
  title={Llm-blender: Ensembling large language models with pairwise ranking and generative fusion},
  author={Jiang, Dongfu and Ren, Xiang and Lin, Bill Yuchen},
  journal={arXiv preprint arXiv:2306.02561},
  year={2023}
}

@article{kojima2022large,
  title={Large language models are zero-shot reasoners},
  author={Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={22199--22213},
  year={2022}
}

@article{kumar2024training,
  title={Training language models to self-correct via reinforcement learning},
  author={Kumar, Aviral and Zhuang, Vincent and Agarwal, Rishabh and Su, Yi and Co-Reyes, John D and Singh, Avi and Baumli, Kate and Iqbal, Shariq and Bishop, Colton and Roelofs, Rebecca and others},
  journal={arXiv preprint arXiv:2409.12917},
  year={2024}
}

@article{lee2023teaching,
  title={Teaching arithmetic to small transformers},
  author={Lee, Nayoung and Sreenivasan, Kartik and Lee, Jason D and Lee, Kangwook and Papailiopoulos, Dimitris},
  journal={arXiv preprint arXiv:2307.03381},
  year={2023}
}

@article{li2023think,
  title={Think outside the code: Brainstorming boosts large language models in code generation},
  author={Li, Xin-Ye and Xue, Jiang-Tian and Xie, Zheng and Li, Ming},
  journal={arXiv preprint arXiv:2305.10679},
  year={2023}
}

@article{lightman2023let,
  title={Let's verify step by step},
  author={Lightman, Hunter and Kosaraju, Vineet and Burda, Yura and Edwards, Harri and Baker, Bowen and Lee, Teddy and Leike, Jan and Schulman, John and Sutskever, Ilya and Cobbe, Karl},
  journal={arXiv preprint arXiv:2305.20050},
  year={2023}
}

@article{liu2024agentlite,
  title={AgentLite: A Lightweight Library for Building and Advancing Task-Oriented LLM Agent System},
  author={Liu, Zhiwei and Yao, Weiran and Zhang, Jianguo and Yang, Liangwei and Liu, Zuxin and Tan, Juntao and Choubey, Prafulla K and Lan, Tian and Wu, Jason and Wang, Huan and others},
  journal={arXiv preprint arXiv:2402.15538},
  year={2024}
}

@article{mirzadeh2024gsm,
  title={GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models},
  author={Mirzadeh, Iman and Alizadeh, Keivan and Shahrokhi, Hooman and Tuzel, Oncel and Bengio, Samy and Farajtabar, Mehrdad},
  journal={arXiv preprint arXiv:2410.05229},
  year={2024}
}

@article{shazeer2017outrageously,
  title={Outrageously large neural networks: The sparsely-gated mixture-of-experts layer},
  author={Shazeer, Noam and Mirhoseini, Azalia and Maziarz, Krzysztof and Davis, Andy and Le, Quoc and Hinton, Geoffrey and Dean, Jeff},
  journal={arXiv preprint arXiv:1701.06538},
  year={2017}
}

@article{stechly2024chain,
  title={Chain of thoughtlessness: An analysis of cot in planning},
  author={Stechly, Kaya and Valmeekam, Karthik and Kambhampati, Subbarao},
  journal={arXiv preprint arXiv:2405.04776},
  year={2024}
}

@article{valmeekam2024llms,
  title={LLMs Still Can't Plan; Can LRMs? A Preliminary Evaluation of OpenAI's o1 on PlanBench},
  author={Valmeekam, Karthik and Stechly, Kaya and Kambhampati, Subbarao},
  journal={arXiv preprint arXiv:2409.13373},
  year={2024}
}

@article{wang2024mixture,
  title={Mixture-of-Agents Enhances Large Language Model Capabilities},
  author={Wang, Junlin and Wang, Jue and Athiwaratkun, Ben and Zhang, Ce and Zou, James},
  journal={arXiv preprint arXiv:2406.04692},
  year={2024}
}

@article{wang2024planning,
  title={On The Planning Abilities of OpenAI's o1 Models: Feasibility, Optimality, and Generalizability},
  author={Wang, Kevin and Li, Junbo and Bhatt, Neel P and Xi, Yihan and Liu, Qiang and Topcu, Ufuk and Wang, Zhangyang},
  journal={arXiv preprint arXiv:2409.19924},
  year={2024}
}

@article{wang2024rethinking,
  title={Rethinking the Bounds of LLM Reasoning: Are Multi-Agent Discussions the Key?},
  author={Wang, Qineng and Wang, Zihao and Su, Ying and Tong, Hanghang and Song, Yangqiu},
  journal={arXiv preprint arXiv:2402.18272},
  year={2024}
}

@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}

@article{xie2024revealing,
  title={Revealing the Barriers of Language Agents in Planning},
  author={Xie, Jian and Zhang, Kexun and Chen, Jiangjie and Yuan, Siyu and Zhang, Kai and Zhang, Yikai and Li, Lei and Xiao, Yanghua},
  journal={arXiv preprint arXiv:2410.12409},
  year={2024}
}

@article{xie2024travelplanner,
  title={Travelplanner: A benchmark for real-world planning with language agents},
  author={Xie, Jian and Zhang, Kai and Chen, Jiangjie and Zhu, Tinghui and Lou, Renze and Tian, Yuandong and Xiao, Yanghua and Su, Yu},
  journal={arXiv preprint arXiv:2402.01622},
  year={2024}
}

@article{yao2023beyond,
  title={Beyond Chain-of-Thought, Effective Graph-of-Thought Reasoning in Language Models},
  author={Yao, Yao and Li, Zuchao and Zhao, Hai},
  journal={arXiv preprint arXiv:2305.16582},
  year={2023}
}

@article{yao2024tree,
  title={Tree of thoughts: Deliberate problem solving with large language models},
  author={Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Tom and Cao, Yuan and Narasimhan, Karthik},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

