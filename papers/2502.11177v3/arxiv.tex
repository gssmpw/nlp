\pdfoutput=1

\documentclass[11pt]{article}

\usepackage[]{acl}

\input{preamble}
\usepackage{times}
\usepackage{latexsym}

\usepackage[T1]{fontenc}

\usepackage[utf8]{inputenc}

\usepackage{microtype}

\usepackage{inconsolata}

\usepackage{graphicx}

\usepackage[justification=justified,skip=3pt]{caption}


\input{color}
\input{def}

\newcommand{\bdicon}{\faIcon{paw}}
\newcommand{\unvicon}{\faIcon{university}}
\newcommand{\shieldicon}{\faIcon{shield-alt}}


\renewcommand{\arraystretch}{0.8}

\setlength{\textfloatsep}{3pt plus 3pt minus 3pt}
\setlength{\intextsep}{3pt plus 3pt minus 3pt}
\setlength{\dbltextfloatsep}{3pt plus 3pt minus 3pt}
\setlength{\abovecaptionskip}{4pt}
\setlength{\belowcaptionskip}{4pt}



\title{The Mirage of Model Editing: Revisiting Evaluation in the Wild}



\author{Wanli Yang\textsuperscript{\tiny\shieldicon\unvicon}\hspace{2em} Fei Sun\textsuperscript{\tiny\shieldicon ~\faIcon[regular]{envelope}} \\ %
  {\bf Jiajun Tan}\textsuperscript{\tiny\shieldicon\unvicon} \hspace{0.6em} \textbf{Xinyu Ma}$\textsuperscript{\tiny\bdicon}$ \hspace{0.6em} \textbf{Qi Cao}\textsuperscript{\tiny\shieldicon} \hspace{0.6em}  \textbf{Dawei Yin}$\textsuperscript{\tiny\bdicon}$ \hspace{0.6em} \textbf{Huawei Shen}\textsuperscript{\tiny\shieldicon\unvicon} \hspace{0.6em} \textbf{Xueqi Cheng}\textsuperscript{\tiny\shieldicon\unvicon}\\
  \textsuperscript{\tiny\shieldicon}CAS Key Laboratory of AI Safety, Institute of Computing Technology, CAS\\
  $\textsuperscript{\tiny\unvicon}$University of Chinese Academy of Sciences  \hspace{2.1em} $\textsuperscript{\tiny\bdicon}$Baidu Inc. \\ %
 yangwanli24z@ict.ac.cn \,\,\,\,\,
 \textsuperscript{\tiny\faIcon[regular]{envelope}}sunfei@ict.ac.cn
}
 


\begin{document}
\maketitle

\renewcommand*{\thefootnote}{\tiny\faIcon[regular]{envelope}}
\footnotetext{Corresponding author: Fei Sun (\href{sunfei@ict.ac.cn}{sunfei@ict.ac.cn})}
\renewcommand*{\thefootnote}{\arabic{footnote}}


\begin{abstract}

Despite near-perfect results in artificial evaluations, the effectiveness of model editing in real-world applications remains unexplored.
To bridge this gap, we propose to study model editing in question answering (QA) by establishing a rigorous evaluation practice to assess the effectiveness of editing methods in correcting LLMsâ€™ errors. 
It consists of QAEdit, a new benchmark derived from popular QA datasets, and a standardized evaluation framework.
Our single editing experiments indicate that current editing methods perform substantially worse than previously reported (38.5\% vs. $\sim$96\%). %
Through module analysis and controlled experiments, we demonstrate that this 
performance decline stems from issues in evaluation practices of prior editing research. %
One key issue is the inappropriate use of \textit{teacher forcing} in testing prevents error propagation by feeding ground truth tokens (inaccessible in real-world scenarios) as input.
Furthermore, we simulate real-world deployment by sequential editing, revealing that current approaches fail drastically with only 1000 edits.
Our analysis provides a fundamental reexamination of both the real-world applicability of existing model editing methods and their evaluation practices, and establishes a rigorous evaluation framework with key insights to advance reliable and practical model editing research\footnote{Code and data are released at \url{https://github.com/WanliYoung/Revisit-Editing-Evaluation}.}.

\end{abstract}

\input{Section/intro_new}

\input{Section/2_Related_Works}

\input{Section/3_Benchmark}

\input{Section/4_Evaluation}

\input{Section/5_SingleEdit}

\input{Section/6_MetricsAnalysis}

\input{Section/7_SequentialEdit}

\input{Section/8_Conclusion}

\input{Section/9_Limitations}





\bibliography{custom}

\newpage

\input{Section/Appendix}

\end{document}
