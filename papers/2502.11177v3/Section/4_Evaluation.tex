
\section{A Tale of Two Evaluation Frameworks}
\label{sec:eval}

    



To identify the cause of this performance gap and guide further investigation, we first delve into the experimental setup of both editing and real-world evaluations.
We abstract them into four key modules: \textit{input}, \textit{generation strategy}, \textit{output truncation}, and \textit{metric}.
This modular paradigm enables systematic comparison between the two evaluation frameworks, as shown in Figure~\ref{fig:eval_frame}.

As shown in Figure~\ref{fig:van_eval}, we formalize previous works' evaluation pipeline \cite{yao-etal-2023-editing, wang2024wise} as \textbf{editing evaluation} framework, which implements four modules as follows:
\begin{enumerate*}[label=\roman*)]
    \item \textit{input}: using only question without additional context;
    \item \textit{generation strategy}: employing teacher forcing to feed ground truth tokens as input during generation;
    \item \textit{output truncation}: truncating output to match the length of target answer;
    \item \textit{metric}: using token-level match ratio between the target and generated answer as accuracy.
\end{enumerate*}


We define \textbf{real-world evaluation} framework based on the standard QA evaluation protocol \cite{eval-harness}, which implements these modules differently (Figure~\ref{fig:prac_eval}):
\begin{enumerate*}[label=\roman*)]
    \item \textit{input}: prefixing question with contexts like task instructions;
    \item \textit{generation strategy}: adopting autoregressive decoding, where each output serves as input for subsequent generation;
    \item \textit{output truncation}: using predefined stop tokens (e.g., ``.'', ``\texttt{\textbackslash{}n}'', and ``\texttt{<|endoftext|>}'') as signal to terminate generation;
    \item \textit{metric}: employing LLMs as binary judgment based on question, target and generated answers\footnote{Detailed prompt is provided in Appendix~\ref{apd:judge_prompt}.}.
\end{enumerate*}
Notably, we employ LLM-as-a-Judge~\cite{li2024llmasjudge} instead of exact match as our evaluation metric, as it has become standard practice in QA evaluation and our human validation confirms its superior alignment with human judgment.

\input{Table/comp_evals}

\textbf{Discussion.}
Table~\ref{tab:comp_evals} details the key differences between these evaluation frameworks. 
Editing evaluation has two types of critical limitations compared to real-world evaluation:
\begin{enumerate*}[label=\roman*)]
    \item \textbf{oversimplification}: context-free input overlooks the complexity and variability of practical queries, and match ratio rewards partial matches of incorrect answers;
    \item \textbf{unreasonableness}: teacher forcing generation and corresponding truncation to the target length leak ground truth information that should remain inaccessible during testing.
\end{enumerate*}
These artificial settings result in a significant gap between research on editing and its practical applications.





\input{Table/main_exp_result_color}


