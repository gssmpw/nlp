
\section{QAEdit}  
\label{sec:qaedit}


While existing works report remarkable success of model editing on artificial benchmarks \cite{meng2023locating, wang2024wise}, its efficacy in real-world scenarios remains unproven.
Here, we propose to study it through QA for its fundamental, universal, and representative nature. 
Specifically, we apply editing methods to correct LLMs' errors in QA tasks and assess the improvement by re-evaluating edited LLMs on a standard QA evaluation framework,  lm-evaluation-harness \cite{eval-harness}.




Since existing editing benchmarks are not derived from or aligned with mainstream QA tasks, we introduce QAEdit, a tailored benchmark to rigorously assess model editing in real-world QA. 
Specifically, QAEdit is constructed from three widely-used QA datasets with broad real-world coverage:  Natural Questions \cite{kwiatkowski2019nq}, TriviaQA \cite{joshi-etal-2017-triviaqa}, and SimpleQA \cite{wei2024measuringshortformfactualitylarge}.
Details about these datasets are provided in Appendix~\ref{apd:qa_data_intro}.


\input{Fig/eval_framework_tikz}
\input{Table/pre_investigation}

While these benchmarks provide questions and answers as \textit{edit prompts} and \textit{targets} respectively, they lack essential fields that mainstream editing methods require for editing and evaluation.
To obtain required \textit{subjects} for editing, we employ GPT-4 (gpt-4-1106-preview) to extract them directly from the questions.
To align with the previous editing evaluation protocol, we assess: reliability using original \textit{edit prompts}; generalization through GPT-4 \textit{paraphrased prompts}; and locality using \textit{unrelated QA pairs} from ZsRE locality set\footnote{We exclude portability evaluation as it concerns reasoning rather than our focus on knowledge updating in real-world.}.

As a result, QAEdit contains 19,249 samples across ten categories, ensuring diverse coverage of QA scenarios. 
Figure~\ref{fig:QAedit_example} shows a QAEdit entry with all fields.
Dataset construction and dataset statistics are detailed in Appendix~\ref{apd:benchmark}.



As a preliminary study, we conduct single-edit experiments on Llama-2-7b-chat's failed questions in QAEdit (detailed in \S\ref{sec:single_edit}). 
As shown in Table~\ref{tab:pre_invest}, after applying SOTA editing methods, the edited models achieve only 38.5\% average accuracy under QA evaluation, far below previously reported results \cite{meng2023massediting, wang2024wise}.
This raises a critical question: \textit{Is the performance degradation attributed to the real-world complexity of QAEdit, or to real-world QA evaluation?}





