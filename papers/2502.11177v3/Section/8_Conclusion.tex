\section{Conclusion and Future Works}



In this paper, we present the first systematic investigation that exposes the gap between theoretical advances and practical effectiveness of model editing by real-world QA evaluation.
Our proposed QAEdit benchmark and real-world evaluation demonstrate that current model editing techniques exhibit significant limitations in practical scenarios, particularly under sequential editing. 
Furthermore, we reveal that this significant discrepancy from previously reported results stems from unrealistic evaluation adopted in prior model editing research.
Through modular analysis and extensive controlled experiments, we uncover fundamental issues in current editing evaluation that inflate reported performance. 
This work establishes new evaluation standards for model editing and provides valuable insights that will inspire the development of more robust editing methods, ultimately enabling reliable and efficient knowledge updates in LLMs for real-world applications.

In future research, we aim to develop editing methods that can
\begin{enumerate*}[label=\roman*)] 
    \item generalize robustly across diverse scenarios with reliable self-termination, and
    \item support extensive sequential updates while maintaining the capabilities of edited LLMs.
\end{enumerate*}



