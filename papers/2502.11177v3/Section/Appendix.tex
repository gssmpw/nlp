\appendix

\section{Appendix}
\label{sec:appendix}


\subsection{Detailed Introduction of QA Datasets}
\label{apd:qa_data_intro}

\textbf{Natural Questions} (NQ)~\cite{kwiatkowski2019nq} is a comprehensive question-answering (QA) dataset that contains real questions posed by users to the Google search, paired with high-quality, human-verified answers. 
The dataset consists of over 300,000 question-answer pairs, with each question derived from user queries on Google Search. 
These questions cover a wide variety of topics, ranging from fact-based inquiries to more complex, open-ended questions. 
The golden answers are sourced from Wikipedia pages, ensuring their accuracy and relevance.
We adopt the test set of NQ, which contains 3610 samples, to construct our QAEdit benchmark.

\noindent \textbf{TriviaQA}~\cite{joshi-etal-2017-triviaqa} is a large-scale QA dataset designed specifically for evaluating models on trivia-style question answering. 
It contains over 650,000 question-answer pairs sourced from trivia websites and is curated by trivia enthusiasts. 
These questions are often fact-based and test the model's ability to retrieve information from large text corpora.
We utilize 11,313 samples from the TriviaQA test set to construct QAEdit.

\noindent \textbf{SimpleQA}~\cite{wei2024measuringshortformfactualitylarge} is a challenging QA benchmark specifically designed to test fact-seeking question-answering models. 
It contains 4326 question-answer pairs curated by OpenAI, with an emphasis on short-form factuality. 
The questions in SimpleQA are concise, direct, and designed to probe factual knowledge. 
Unlike more general-purpose QA datasets, SimpleQA emphasizes clarity and the ability of models to provide precise, factually accurate answers.
We employ all samples from SimpleQA for QAEdit construction.




\subsection{Construction and Statistics of QAEdit}
\label{apd:benchmark}

\input{Table/QAEdit_Statistics}

In this section, we describe the detailed construction procedures and statistics of QAEdit.

While aforementioned QA benchmarks provide questions and answers as \textit{edit prompts} and \textit{targets}, they lack \textit{subjects} for editing, as well as \textit{rephrased prompts} and \textit{locality QA pairs} to evaluate generalization and locality.
To supplement the missing fields, our construction procedures encompass the following steps:
\begin{enumerate*}[label=\roman*)]
    \item We employ GPT-4 (gpt-4-1106-preview) to extract the subjects directly from the edit prompts. 
    To improve the accuracy of extraction, we prompt the model with 5-shot examples to utilize its in-context learning capability, which can be seen in Figure~\ref{fig:detect_subject}.
    \item We utilize GPT-4 to paraphrase the edit prompts to obtain rephrased prompts. 
    Considering that paraphrasing questions is easy for GPT-4, the specific instruction is straightforward and is presented in Figure~\ref{fig:rephrase}. 
    Furthermore, we manually reviewed some of the rephrased results and found them to be highly effective.
    \item Moreover, for each sample of QAEdit, we randomly select a QA pair from the locality sets of ZsRE~\cite{levy2017zero} as locality prompt and corresponding answer to assess locality.
\end{enumerate*}

As a result, our QAEdit benchmark encompasses ten categories of knowledge, covering mainstream topics with significant real-world impact.
The statistical information and examples of each category are presented in Table~\ref{tab:QAEdit_Stat}.





\subsection{Prompt of LLM-as-a-Judge}
\label{apd:judge_prompt}

In light of the significant advancements in LLM-as-a-Judge \cite{li2024llmasjudge}, we employ GPT-4o-mini to perform binary judgments based on the provided questions, target answers, and generated responses. 
Following previous work \cite{wei2024measuringshortformfactualitylarge}, our complete prompt is presented in Figure~\ref{fig:llm_judge_prompt}.




\subsection{Detailed Experimental Setup}
\label{apd:exp_setup}

\subsubsection{Editing Methods}

\textbf{FT-M}~\cite{zhang2024comprehensivestudyknowledgeediting} is an  enhanced version of FT-L \cite{zhu2020modifyingmemoriestransformermodels, meng2023locating}.
FT-L introduces an $l_{\infty}$-norm constraint into the fine-tuning objective to explicitly restrict the parameter changes between the original and edited models, thereby mitigating side effects on unrelated knowledge.
However, FT-L deviates from the original fine-tuning objective by using only the last token's prediction to maximize the probability of all tokens in the target sequence. To address this issue, FT-M improves upon FT-L by applying the cross-entropy loss to the target answer while masking the original text, which aligns more closely with the traditional fine-tuning objective and enhances performance. 

\noindent \textbf{MEND}~\cite{mitchell2022fast} employs a hypernetwork to learn low-rank decompositions of standard fine-tuning gradients. 
By disentangling gradients into learnable rank-one matrices, it achieves explicit control over parameter updates while maintaining tractable editing in LLMs.

\noindent \textbf{ROME}~\cite{meng2023locating} identifies knowledge-critical layers in Transformer MLP modules through causal tracing analysis. 
It implements precise knowledge updates via rank-one matrix modification on the identified layer, guided by causal mediation effects in model outputs.

\noindent \textbf{MEMIT}~\cite{meng2023massediting} extends ROME by developing cross-layer propagation analysis and coordinated parameter updates across multiple MLP layers, enabling efficient batch editing of large-scale knowledge.

\noindent \textbf{GRACE}~\cite{hartvigsen2023aging} is a lifelong editing method that performs local corrections on streaming errors of deployed models. The approach writes new mappings into a pretrained model's latent space, creating a discrete local codebook of edits without modifying model weights, allowing for sequential editing operations.

\noindent \textbf{WISE}~\cite{wang2024wise} addresses the similar challenge of sequential editing like GRACE. It employs a dual memory architecture comprising a main memory for pretrained knowledge and a side memory for edited content. The system utilizes a router to direct queries between these memories. 

\subsubsection{Edited LLMs}

\input{Table/Appendix/single_loc}

\textbf{Llama-2-7b-chat}~\cite{touvron2023llama2openfoundation} is a model designed for conversational scenarios with 7 billion parameters. 
It excels in generating human-like responses in real-time, offering smooth and context-aware dialogue generation.

\noindent \textbf{Mistral-7b}~\cite{jiang2023mistral7b} is a superior pretrained base model with 7 billion parameters, outperforming Llama-2-13b on all examined benchmarks,  offering strong performance while being resource-efficient.
Specifically, we employ the version of Mistral-7B-v0.1.

\noindent \textbf{Llama-3-8b}~\cite{llama3} is a cutting-edge 8-billion-parameter model designed for diverse AI applications. It combines advanced techniques with scalability, ensuring high-quality generation for complex tasks like multi-turn dialogues, creative writing, and complex reasoning tasks.

\input{Fig/Appendix/inst_prompt}

\subsubsection{Editing Datasets}
\textbf{ZsRE}~\cite{levy2017zero} is a popular dataset for Question Answering (QA), where each entry consists of a counterfactual statement derived from a factual Wikipedia page that needs to be edited. 

\noindent \textbf{\textsc{CounterFact}}~\cite{meng2023locating} is a challenging dataset curated for model editing. It contains 21,919 nonfactual statements, initially assigned low probabilities by models, and designed to encourage substantial and meaningful modifications to the original factual statements.



\subsection{Locality Results of Single Editing}
\label{apd:loc_single_edit}




The locality results of single editing experiments are presented in Table~\ref{tab:single_loc}.
The results show that for almost all baselines, their locality results are very high across two evaluation frameworks, indicating that a single edit generally has little impact on the model's general capabilities.



\subsection{Detailed Practical Prompt}
\label{apd:prac_prompt}




In Section~\ref{analysis:input}, we prefix the target question with a common QA task instruction~\cite{eval-harness} as the input prompt, as shown in Figure~\ref{fig:inst_prompt}. 
We aim to utilize this context-guided prompt to represent and simulate various contexts that might occur in practical applications.





\subsection{Generalization of Sequential Editing}
\label{apd:gen_seq}

\input{Table/Appendix/seq_other_data}

\input{Table/Appendix/seq_gen}

\input{Table/Appendix/mend_batch}

The generalization results of sequential editing experiments are presented in Table~\ref{tab:seq_gen}.
Compare to Table~\ref{tab:seq_edit}, the results indicate that current editing methods exhibit worse generalization than reliability when dealing with sequential editing requests. 
All methods except FT-M and WISE demonstrate near-zero generalization ability under real-world evaluation, which further proves that existing editing methods cannot effectively fulfill the practical needs of continuous editing.


\subsection{Sequential Editing on Other Datasets}
\label{apd:seq_other_llm}


The results of sequential editing on ZsRE and \textsc{CounterFact} are presented in Table~\ref{tab:seq_other_data}. 
These two datasets exhibit trends similar to those observed in QAEdit, including the poor practical effectiveness of existing editing methods, the inadequacy of simplified editing evaluations, and the dilemma of achieving editing success and preserving unrelated knowledge.

\subsection{Mini-Batch Sequential Editing for MEND}
\label{apd:mini_batch_seq}

As shown in Table~\ref{tab:mend_batch}, unlike FT-M and MEMIT, which can maintain a certain level of editing performance under specific batch sizes (as depicted in Figure~\ref{fig:seq_batch}), MEND is completely unusable in sequential editing scenarios, regardless of the batch size. 
This poor effectiveness can be attributed to the limitation of the meta-learning paradigm, wherein the hypernetwork of MEND for parameter updates is specifically trained on the original model state. 
Consequently, the predicted parameter modifications are optimized solely for the original model and fail to effectively adapt to the evolving states of the sequentially edited model.
This limitation fundamentally constrains MEND's efficacy in sequential editing scenarios.



\input{Fig/Appendix/detect_subject}

\input{Fig/Appendix/rephrase}

\input{Fig/Appendix/llm_judge_prompt}




