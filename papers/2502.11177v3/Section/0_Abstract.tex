\begin{abstract}




Despite near-perfect results in artificial evaluations, the effectiveness of model editing in real-world applications remains unexplored.
To bridge this gap, we propose to study model editing in question answering (QA) by establishing a rigorous evaluation practice to assess the effectiveness of editing methods in correcting LLMsâ€™ errors. 
It consists of QAEdit, a new benchmark derived from popular QA datasets, and a standardized evaluation framework.
Our single editing experiments indicate that current editing methods perform substantially worse than previously reported (38.5\% vs. $\sim$96\%). %
Through module analysis and controlled experiments, we demonstrate that this 
performance decline stems from issues in evaluation practices of prior editing research. %
One key issue is the inappropriate use of \textit{teacher forcing} in testing prevents error propagation by feeding ground truth tokens (inaccessible in real-world scenarios) as input.
Furthermore, we simulate real-world deployment by sequential editing, revealing that current approaches fail drastically with only 1000 edits.
Our analysis provides a fundamental reexamination of both the real-world applicability of existing model editing methods and their evaluation practices, and establishes a rigorous evaluation framework with key insights to advance reliable and practical model editing research\footnote{Code and data released at \url{https://anonymous.4open.science/r/12BC}.}.

\end{abstract}
