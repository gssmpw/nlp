
\section{(Sequential) Editing in the Wild}


Although our analysis via single editing reveals limitations in current editing evaluation, such isolated editing fails to capture the continuous, large-scale demands of editing in real-world scenarios.
Therefore, we now address our primary research question: testing model editing under real-world evaluation via sequential editing, a setup that better reflects practical requirements.

\subsection{Sample-wise Sequential Editing}
\label{sec:bs_1_seq_edit}


\noindent \textbf{Experimental Setup.} 
Following established protocols \cite{huang2023transformerpatcher, hartvigsen2023aging}, we evaluate editing methods with a batch size of 1, i.e., updating knowledge incrementally one sample at a time.
We keep the same setup as in \S\ref{sec:single_setup}, but limit to 1000 samples per dataset, as existing methods perform significantly worse with more edits.
For QAEdit, the chosen samples are incorrectly answered by all pre-edit LLMs.
Given the notable side effects in sequential editing \cite{yang-etal-2024-butterfly}, we focus on  the evaluation of \textit{reliability} and \textit{locality}, with \textit{generalization} results provided in Appendix~\ref{apd:gen_seq}.


\noindent \textbf{Results \& Analysis}. 
The results on QAEdit are shown in Table~\ref{tab:seq_edit}, with similar findings for ZsRE and \textsc{CounterFact} in Appendix~\ref{apd:seq_other_llm}.
\begin{enumerate*}[label=\roman*)]
    \item In real-world evaluation with sequential editing, all methods except FT-M exhibit nearly unusable performance (only 9.3\% average reliability), with FT-M achieving a 40.5\% average reliability.
    \item The gap between editing and real-world evaluation further confirms the evaluation issues we discussed in \S\ref{sec:cont_exp}.
    \item The notably low average locality of 21.3\% highlights the severe disruption to LLMs. While GRACE effectively preserves unrelated knowledge through external edit modules, it struggles with knowledge updating.
\end{enumerate*}



\subsection{Mini-Batch Sequential Editing}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{Fig/seq_bs_bar.pdf}
    \captionsetup{skip=0pt}
    \caption{Impact of batch size (BS) when editing Llama-3-8b with FT-M and MEMIT on QAEdit.}
    \label{fig:seq_batch}
\end{figure}


Real-world applications often batch multiple edits together for efficient processing of high-volume demands. 
Moreover, \citet{pan2024whyhas} suggest increasing batch size may alleviate the side effects of sequential editing.
Thus, this section investigates whether increasing the batch size could serve as a potential solution to the practical challenges faced by current editing methods.


\noindent \textbf{Experiment Setup.}
Following the experimental setup in \S\ref{sec:bs_1_seq_edit}, we evaluate three batch-capable editing algorithms: FT-M, MEND, and MEMIT.
Due to VRAM constraints (80GB A800), we empirically set the maximum testable batch sizes: 80 for FT-M, 16 for MEND, and 1000 for MEMIT.


\noindent \textbf{Results \& Analysis.} 
Figure~\ref{fig:seq_batch} presents the editing performance with varying batch sizes, evaluated across various-sized QAEdit subsets.
Despite experimenting with various batch sizes, all methods show consistently limited performance, with the highest score below 30\% for 1000 edits.
The all-zero performance of MEND are provided in Appendix~\ref{apd:mini_batch_seq}.
Notably, Figure~\ref{fig:seq_batch} presents opposite trends:
\begin{enumerate*}[label=\roman*)]
    \item MEMIT achieves optimal performance only when editing all requests in a single batch, with performance decreasing sharply as batch size decreases.
    \item In contrast, FT-M performs best at a batch size of 1 but  degrades drastically  as batch size increases.
\end{enumerate*}
The divergence may arise from their distinct batch editing mechanisms: FT-M optimizes for aggregate batch-level loss, potentially compromising individual edit accuracy; whereas MEMIT estimates parametric changes individually before integration, facilitating effective batch edits.

\begin{figure}
    \centering
    \includegraphics[width=0.96\linewidth]{Fig/heatmap.pdf}
    \captionsetup{skip=2pt}
    \caption{Reliability evolution of sequential editing on Llama-3-8b, with repeated evaluation of previous batches after each new edit batch (batch size = 20).}
    \label{fig:heatmap}
\end{figure}




\noindent\textbf{Further Analysis}.
To gain insights into the poor final performance, we also investigate how editing effectiveness changes during continuous editing.
Specifically, we randomly partition 100 QAEdit samples into 5 batches of 20 samples each.
Using MEMIT on Llama-3-8b, we iteratively edit each batch while evaluating the edited model on each previously edited batch separately to track dynamics of editing effectiveness.


Figure~\ref{fig:heatmap} reveals two key insights: 
\begin{enumerate*}[label=\roman*)] 
    \item While the first batch exhibits high initial reliability, its performance declines sharply with subsequent editing, suggesting that new edits disrupt the knowledge injected in earlier batches.
    \item As editing progresses, the effectiveness of MEMIT decreases rapidly.
\end{enumerate*} 
These findings reveal the key challenges of sequential editing: \textbf{progressive loss of previously edited knowledge coupled with decreasing effectiveness in editing new knowledge}.


