
\section{Controlled Study of Editing Evaluation}
\label{sec:cont_exp}





This section presents controlled experiments to systematically investigate how different module variations in editing evaluation (outlined in \S\ref{sec:eval}) contribute to performance overestimation.
Due to resource and space limitations, we conduct experiments on Llama-3-8b with 3,000 randomly sampled QAEdit instances, where the findings generalizable across other LLMs and datasets.






\subsection{Input}
\label{analysis:input}



This subsection empirically isolates how idealistic prompts may lead to overestimated results in editing evaluation.
Specifically, we compare context-free prompts with real-world input formats that include task instructions, while keeping all other modules identical. 
Detailed prompts are provided in Appendix~\ref{apd:prac_prompt}.

\input{Table/anlysis_prompt}

Table~\ref{tab:metrics_llama3_prompt} shows that incorporating task instruction degrades performance across all editing methods, with GRACE showing the most significant decline due to its weak generalization.
This trend contrasts with the behavior of original Llama-3-8b, where task instructions usually improve results \cite{grattafiori2024llama3herdmodels}.
These findings reveal that \textbf{using identical prompts for editing and testing in current editing evaluation, while yielding optimistic results, may fail to reflect editing effectiveness under diverse real-world inputs}.







\subsection{Generation Strategy}

\input{Table/analysis_generation}




Here, we examine how teacher forcing in the generation strategy contributes to the inflated results in editing evaluation. 
We compare reliability of teacher forcing and autoregressive decoding under two distinct input formats, while keeping all other modules consistent.


As depicted in Table~\ref{tab:metrics_llama3_generation}, switching from teacher forcing to autoregressive decoding consistently leads to performance degradation across all methods, with lower-performing methods exhibiting more substantial decline.
The underlying reason for this phenomena is that teacher forcing prevents error propagation by feeding ground truth tokens as input, while autoregressive decoding allows errors to cascade.
Although teacher forcing is beneficial for stabilizing LLM training, it should be avoided during testing, where ground truth is unavailable. 
Our results demonstrate that \textbf{inappropriate use of teacher forcing in evaluation artificially elevates editing performance, especially for methods with poor real-world performance}.


\subsection{Output Truncation}
\label{sec:answer_trunc}


\input{Table/analysis_truncation}

\input{Table/additional_content}

\input{Table/sequential_edit_table}

\input{Table/analysis_metric}



Besides leaking ground truth tokens, teacher forcing also implicitly controls output length by aligning with ground truth length.
However, this is not applicable in real-world scenarios where ground truth is unavailable.
In practice, during inference, generation typically terminates based on predefined stop tokens, e.g., ``\texttt{<|endoftext|>}'' \cite{eval-harness}.
Here, we analyze these two truncation strategies by employing GPT-4o-mini as a binary judge to assess correctness (detailed in \S\ref{sec:analysis_metric}), since length discrepancies between generated and target answers preclude the use of match ratio metric.

As shown in Table~\ref{tab:metrics_llama3_extraction}, truncation based on natural stop criteria significantly reduces editing performance across all methods.
To identify the underlying causes, we analyze the content truncated at both the ground truth length and the natural stop criteria. 
Our analysis reveals that, under natural stop criteria, the edited models typically generate content beyond the ground truth length, introducing \textit{meaningless repetition} and \textit{irrelevant or incorrect information}, as evidenced in Table~\ref{tab:add_content}.


These findings demonstrate that \textbf{irrational truncation in editing evaluation masks subsequent errors that emerge in real-world scenarios, resulting in overestimated performance}.
As shown in Table~\ref{tab:metrics_llama3_extraction}, although context-guided prompting enhances generation termination, it still fails to address the fundamental limitations. 
Such pitfalls in current approaches, overlooked by traditional evaluation, highlight the need to explore more effective ways to express edited knowledge.


\subsection{Metric}
\label{sec:analysis_metric}


As explained in \S\ref{sec:eval}, the match ratio metric could lead to inflated performance.
To quantify this effect, we compare match ratio against LLM-as-a-Judge, specifically using GPT-4o-mini.
Since match ratio requires length parity with targets, we autoregressively generate sequences to target length for both metircs to ensure fair comparison.

The results presented in Table~\ref{tab:metrics_llama3_verification} reveal that \textbf{the match ratio metric indeed overestimates the performance of edited models}. 
Moreover, a lower match ratio often indicates a smaller proportion of fully correct answers, resulting in worse performance in LLM evaluation.
