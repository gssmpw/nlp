\section{Introduction}

Model editing \cite{zhang2024comprehensivestudyknowledgeediting,wang2024knowledge} has attracted widespread attention for its promising vision: enabling efficient and precise updates to specific knowledge within pretrained Large Language Models (LLMs) without retraining from scratch. %
Recent advances report near-perfect results on corresponding benchmarks \cite{meng2023locating, wang2024wise}, suggesting substantial progress toward this goal.
However, these results often come from artificial, oversimplified evaluation settings (e.g., identical prompts for editing and testing; more in \S\ref{sec:eval}) that may fail to capture real-world complexities.
This disparity raises a critical question: \textit{Can these promising results in the literature translate to practical applications?}


To address this question, we propose to study model editing in QA tasks, which provide clear evaluation criteria and broad applicability.
This adaptation involves two key components: a real-world dataset and realistic evaluation.
For dataset, we create \textbf{QAEdit}, a tailored dataset derived from three widely-used QA datasets, enabling editing methods to inject answers from real-world QA tasks into LLMs.
For evaluation, we shift from traditional editing evaluation to standard QA evaluation \cite{eval-harness}, assessing editing methods through the performance of edited LLMs on their previously incorrect questions.

\input{Fig/intro_case}

Our initial study reveals that current advanced editing methods achieve only a \textbf{38.5}\% average success rate on QAEdit, significantly lower than the results reported in previous studies.
This raises a question: \textit{Does the performance decline stem from QAEdit's real-world complexity, or from the shift of editing to real-world (i.e., QA) evaluation?}

To enable rigorous analysis, starting with single editing experiments, we evaluate six representative methods across three leading LLMs on QAEdit and two established editing benchmarks, using both evaluation frameworks.
As illustrated in Figure~\ref{fig:intro}, switching from editing to real-world evaluation consistently leads to a significant performance decline across all datasets for each editing method, whether on real-world QA data or previous editing benchmarks.
This dramatic performance gap raises two critical questions: \textit{What differences between these frameworks drive such disparity, and which most accurately reflects editing effectiveness?}


To answer them, we carefully examine the experimental setups for both editing and QA evaluations in previous work.
From this, we abstract four key modules (\textit{input}, \textit{generation strategy}, \textit{output truncation}, and \textit{metric}) and analyze their variations through controlled experiments. 
The results expose four critical limitations in editing evaluation:
{\textbf{\{}}\begin{enumerate*} %
\item[\textcolor{black}{\ding{182}}] \textbf{input module}: using identical prompts for editing and testing overlooks the variability and unpredictability in real-world queries;
\item[\textcolor{black}{\ding{183}}] \textbf{generation strategy}: teacher forcing, which feeds the ground truth as input during decoding, artificially beautifies results by disregarding potential errors in the modelâ€™s own outputs;
\item[\textcolor{black}{\ding{184}}] \textbf{output truncation}: using target answer length to truncate outputs conceals errors (e.g., repetition, irrelevant, or incorrect information) that would occur with natural stopping criteria;
\item[\textcolor{black}{\ding{185}}] \textbf{metric}: match ratio may inflate performance by rewarding partial matches of incorrect answers.
\end{enumerate*}{\textbf{\}}}
Among these issues, teacher forcing and target length truncation cause the most significant overestimation, as they rely on ground truth that is unavailable in real-world scenarios.
This highlights that \textbf{editing evaluation, reliant on such idealized or even unrealistic conditions, fails to accurately measure true editing effectiveness}.


After uncovering evaluation issues through single editing analysis, we now return to our initial question: how do editing methods perform under realistic conditions? 
In practice, editing requests arrive continuously, making sequential editing a more genuine test of real-world applicability.
Under real-world evaluation, our sequential editing experiments show that current methods catastrophically fail to scale, with average success rates dropping to $\sim$\textbf{10}\% for only 1000 samples. 


Our work, for the first time, exposes severe issues in current evaluations of model editing research and demonstrates substantial limitations of existing editing methods under real-world conditions. 
We hope this work will inspire more rigorous evaluation practices and motivate the development of algorithms that can truly fulfill the promise of model editing: to reliably and scalably update knowledge in LLMs \textit{for real-world applications}.

Our main contributions are as follows.
\begin{itemize}[itemsep=0pt, leftmargin=18pt, topsep=1pt, partopsep=1pt, parsep=1pt]
\item We introduce QAEdit, a benchmark tailored for real-world QA tasks, and establish a more practical evaluation protocol.
\item We reveal a significant gap between the performance reported in literature and that observed in real-world scenarios.
\item We demonstrate that published results are inflated and identify the critical issues and underlying causes in current evaluation practices. %
\item We expose the severe scalability challenges of current editing methods in practical applications through sequential editing experiments.
\end{itemize}

