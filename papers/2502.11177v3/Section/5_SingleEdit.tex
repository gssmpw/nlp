
\section{Analysis on Benchmark \& Evaluation}
\label{sec:single_edit}

The preliminary analysis and theoretical comparison in \S\ref{sec:qaedit} and \S\ref{sec:eval} reveal a notable disparity between editing and real-world evaluation.
To rigorously address the question raised in \S\ref{sec:qaedit}---whether the performance gap stems from differences in dataset or evaluation---we conduct systematic single-edit experiments, where each edit is independently applied to the original model from scratch.

\subsection{Experimental Setup}
\label{sec:single_setup}

This section outlines the experimental setup used in all subsequent experiments, unless stated otherwise. 
Due to space limitations, further details are provided in Appendix~\ref{apd:exp_setup}.

\noindent\textbf{Editing Methods}. 
To ensure comprehensive coverage, we employ six diverse and representative editing techniques across four categories: extension based (\textbf{GRACE}, \citealp{hartvigsen2023aging} and \textbf{WISE}, \citealp{wang2024wise}), fine-tuning based (\textbf{FT-M}, \citealp{zhang2024comprehensivestudyknowledgeediting}), meta-learning (\textbf{MEND}, \citealp{mitchell2022fast}), and locate-then-edit (\textbf{ROME}, \citealp{meng2023locating} and \textbf{MEMIT}, \citealp{meng2023massediting}).
All methods are implemented using \texttt{EasyEdit}\footnote{\url{https://github.com/zjunlp/EasyEdit}}. 
Due to the inconsistent keys implementation in ROME, we adopt its refined variant R-ROME \cite{gupta-etal-2024-rebuilding, yang-etal-2024-fall} instead.

\noindent\textbf{Edited LLMs}.  
In line with prior research \cite{wang2024wise, fang2024alphaedit}, we test three leading open-source LLMs:
\textbf{Llama-2-7b-chat} \cite{touvron2023llama2openfoundation}, 
\textbf{Mistral-7b} \cite{jiang2023mistral7b}, and \textbf{Llama-3-8b} \cite{llama3}.
Greedy decoding is used for all models, aligning with prior research.
Results for MEND with Llama-3-8b are excluded due to architectural incompatibility.

\noindent\textbf{Editing Datasets}.
We employ QAEdit along with two prevalent benchmarks, ZsRE \cite{levy2017zero} and \textsc{CounterFact} \cite{meng2023locating}, for a rigorous investigation.
For QAEdit, we evaluate the edited LLMs using only samples that their unedited counterparts initially answered incorrectly.
This yields evaluation sets of 12,715, 10,213, 10,467 samples for Llama-2-7b-chat, Mistral-7b, and Llama-3-8b, respectively.
For ZsRE and \textsc{CounterFact}, we use their established test sets, each with 10,000 records.


\subsection{Results \& Analysis}
\label{sec:single_result}

The experimental results are presented in Table~\ref{tab:main_exp_color}.
Due to the minor side effects in single editing scenarios, the consistently favorable locality results are moved to Appendix~\ref{apd:loc_single_edit}.

\noindent\textbf{Benchmark Perspective}:
QAEdit exhibits moderately lower editing reliability compared to ZsRE and CounterFact, reflecting its diverse and challenging nature as a real-world benchmark.
However, this modest gap is insufficient to explain the significant discrepancy observed in our earlier analysis.

\noindent \textbf{Method Perspective}:
\begin{enumerate*}[label=\roman*)]
    \item Recent state-of-the-art methods, GRACE and WISE, exhibit the most significant decrease, with both reliability and generalization dropping below 5\%.
    This decline mainly stems from their edited models generating erroneous information after producing the correct answers, detailed in \S\ref{sec:answer_trunc}.
    \item In comparison, traditional methods like FT-M and ROME exhibit superior stability and preserve a certain level of effectiveness in real-world evaluation.
\end{enumerate*}

\noindent \textbf{Evaluation Perspective}:
\begin{enumerate*}[label=\roman*)]
    \item Performance on each benchmark drops sharply from editing evaluation ($\sim$96\%) to real-world evaluation (e.g., 43.8\% on ZsRE and 38.9\% on QAEdit), indicating that \textbf{editing evaluation substantially overestimates the effectiveness of editing methods}.
    \item Unlike editing evaluation, which reports consistently near-perfect results across all methods and benchmarks, real-world evaluation effectively distinguishes them, providing valuable insights for future research.
\end{enumerate*}



