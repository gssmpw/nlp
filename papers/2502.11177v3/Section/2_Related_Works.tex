\section{Related Works}

\subsection{Model Editing Methodologies}

Existing model editing methods can be categorized into the following four types:

\noindent\textbf{Extension based}.
These methods update LLMs by adding trainable parameters to encode new knowledge, e.g., additional neurons in FFN  \cite{dong-etal-2022-calibrating, huang2023transformerpatcher} or specialized memory modules \cite{hartvigsen2023aging, wang2024wise}, while preserving pretrained weights.

\noindent\textbf{Fine-tuning Based}.
Fine-tuning offers a straightforward approach to update LLMs' knowledge but faces catastrophic forgetting.
Recent works mitigate this by constraining parameter changes \cite{zhu2020modifyingmemoriestransformermodels} or leveraging Parameter-Efficient Fine-Tuning (PEFT) \cite{han2024peft} to limit modification scope \cite{MELOaaai24,wang-etal-2024-roselora}.

\noindent\textbf{Meta Learning}.
Employing meta-learning, KE \cite{decao2021editing}, MEND \cite{mitchell2022fast}, and MALMEN \cite{tan23malmen} train hypernetworks to predict effective gradients or parameter alterations for knowledge integration. 

\noindent\textbf{Locate-Then-Edit}.
Based on the investigation of knowledge mechanisms in LLMs \cite{geva-etal-2021-transformer, geva-etal-2022-transformer}, KN \cite{dai2022knowledge}, ROME \cite{meng2023locating}, and PMET \cite{aaai24pmet} utilize knowledge attribution and causal tracing to pinpoint target knowledge to specific parameters, then perform localized editing.
Furthermore, MEMIT \cite{meng2023massediting} and EMMET \cite{gupta-etal-2024-unified} extend this for massive editing in a batch.

\subsection{Evaluation of Model Editing}

Current evaluation of model editing primarily focuses on editing effectiveness and side effects on model capabilities.

\noindent\textbf{Effectiveness of Editing}.
The effectiveness of editing is typically evaluated from four key properties using artificial benchmarks and simplified evaluation settings:
\begin{enumerate*}[label=\roman*)]
    \item \textit{reliability}, success rate of editing; 
    \item \textit{generalization}, adaptability of edited knowledge to paraphrased prompts;
    \item \textit{locality}, impact on irrelevant knowledge;
    \item \textit{portability}, applicability of edited knowledge in factual reasoning.
\end{enumerate*}
We refer readers to \citet{yao-etal-2023-editing} for details.
In addition to these basic metrics, domain-specific editing tasks have been introduced, e.g., privacy preservation \cite{wu-etal-2023-depn}, bias mitigation \cite{chen2024bias}, and harm injection \cite{chen2024harm}.


\noindent\textbf{Side Effects of Editing}.
Recent research has also examined the potential side effects of editing on LLMs \cite{hoelscher-obermaier-etal-2023-detecting, li2024unveiling}.
While locality shares similar objectives, its limited evaluation scope fails to capture the full extent of editing side effects.
Recent studies \citep{yang-etal-2024-butterfly, gu-etal-2024-model, gupta-etal-2024-model} have revealed that model editing can significantly compromise LLMs' downstream tasks capabilities, motivating a growing research to mitigate such side effects \cite{corr24prune, fang2024alphaedit}.

\noindent\textbf{Discussion.} 
Distinct from aforementioned two aspects of evaluations, %
this paper presents the first comprehensive evaluation of model editing effectiveness in real-world scenarios.
With similar motivations, AKEW \cite{wu-etal-2024-akew} proposed a new task of unstructured text editing. %
In contrast, our study rethinks SOTA editing techniques on real-world setting, revealing their limited practical effectiveness and uncovering the pitfalls of traditional editing evaluation.

\input{Fig/QAEdit_Example}






