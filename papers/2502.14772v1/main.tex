\pdfoutput=1
\documentclass[11pt]{article}


\usepackage[
backend=biber,
style=alphabetic,
maxbibnames=15,
maxalphanames=10,
minalphanames=6,
giveninits=true,
doi=false,
isbn=false,
url=true,
eprint=false,
backref=false,
]{biblatex}
\addbibresource{allrefs.bib}

\DefineBibliographyStrings{english}{
  backrefpage  = {},
  backrefpages = {},
}

\renewcommand*{\bibpagerefpunct}{%
  \finentrypunct\space}

\DeclareFieldFormat{bracketswithperiod}{\mkbibbrackets{#1}}



\renewbibmacro*{pageref}{%
  \iflistundef{pageref}
    {}
    {\printtext[bracketswithperiod]{%
       \ifnumgreater{\value{pageref}}{1}
         {\bibstring{backrefpages}}
         {\bibstring{backrefpage}}%
       \printlist[pageref][-\value{listtotal}]{pageref}}%
     \renewcommand*{\finentrypunct}{}}
 }
     
\renewbibmacro{in:}{}
\AtEveryBibitem{\clearfield{pages}} 



\usepackage[letterpaper,margin=1in]{geometry}
\parindent=18pt


\usepackage{lipsum}

\newcommand\blfootnote[1]{%
  \begingroup
  \renewcommand\thefootnote{}\footnote{#1}%
  \addtocounter{footnote}{-1}%
  \endgroup
}


\usepackage[utf8]{inputenc} %
\usepackage[T1]{fontenc}    %

\usepackage{url}            %
\usepackage{booktabs}       %
\usepackage{nicefrac}       %
\usepackage{microtype}      %
\usepackage{enumitem}
\usepackage{dsfont}
\usepackage{multicol}
\usepackage{xcolor}
\usepackage{mdframed}



\usepackage{amsthm,amsfonts,amsmath,amssymb,epsfig,color,float,graphicx,verbatim, enumitem}



\usepackage{algorithmicx}
\usepackage[noend]{algpseudocode}
\usepackage{algorithm}

\usepackage{mathtools}
\usepackage{bbm}

\usepackage{thm-restate}

\definecolor{green}{rgb}{0.0, 0.5, 0.0}
\usepackage[colorlinks,citecolor=blue,linkcolor=magenta,bookmarks=true,hypertexnames=false]{hyperref}
\usepackage{silence}
\WarningFilter{hyperref}{Token not allowed}
\usepackage[nameinlink,capitalize]{cleveref}
\providecommand\algorithmname{algorithm}
\crefformat{equation}{(#2#1#3)}
\crefname{lemma}{lemma}{lemmata}
\crefname{claim}{claim}{claims}
\crefname{theorem}{theorem}{theorems}
\crefname{proposition}{proposition}{propositions}
\crefname{corollary}{corollary}{corollaries}
\crefname{claim}{claim}{claims}
\crefname{remark}{remark}{remarks}
\crefname{definition}{definition}{definitions}
\crefname{fact}{fact}{facts}
\crefname{question}{question}{questions}
\crefname{condition}{condition}{conditions}
\crefname{algorithm}{algorithm}{algorithms}
\crefname{assumption}{assumption}{assumptions}
\crefname{notation}{notation}{notation}
\crefname{cond}{Condition}{Conditions}


\newenvironment{proofsketch}%
  {%
   \par\noindent{\bfseries\upshape Proof Sketch\ }%
  }%
  {\qed}
  
\newcommand{\inote}[1]{\footnote{{\bf [Ilias: {#1}\bf ] }}}
\newcommand{\dnote}[1]{\footnote{{\bf [Daniel: {#1}\bf ] }}}
\newcommand{\anote}[1]{\footnote{{\bf [Ankit: {#1}\bf ]}}}
\newcommand{\tnote}[1]{\footnote{{\bf [Thanasis: {#1}\bf ] }}}
\newcommand{\gnote}[1]{\footnote{{\bf [Giannis: {#1}\bf ] }}}
\newcommand{\snote}[1]{\footnote{{\bf [Sushrut: {#1}\bf ] }}}
\newcommand{\tnew}[1]{\textcolor{orange}{#1}}
\newcommand{\jnote}[1]{\footnote{{\bf [Jasper: {#1}\bf ] }}}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{claim}[theorem]{Claim}

\newtheorem{definition}[theorem]{Definition}
\newtheorem{observation}{Observation}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{question}[theorem]{Question}
\theoremstyle{definition}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{condition}[theorem]{Condition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{notation}[theorem]{Notation}

\newcommand{\eps}{\epsilon}


\newcommand{\dtv}{d_\mathrm{TV}}
\newcommand{\Ind}{\mathds{1}}
\newcommand{\1}{\Ind}
\newcommand{\I}{\Ind}
\newcommand{\IndSig}{\mathbf{1}_{\{\theta\notin \mathrm{Sig}(v)\}}}

\renewcommand{\Pr}{\operatorname*{\mathbf{Pr}}}
\newcommand{\Var}{\operatorname*{\mathbf{Var}}}
\newcommand{\Cov}{\operatorname*{\mathrm{Cov}}}
\newcommand{\E}{\operatorname*{\mathbf{E}}}
\newcommand{\from}{\leftarrow}
\newcommand{\negl}{\mathrm{negl}}

\newcommand{\poly}{\operatorname*{\mathrm{poly}}}
\newcommand{\polylog}{\operatorname*{\mathrm{polylog}}}

\newcommand{\Normal}{\mathcal{N}}
\newcommand{\Bin}{\mathrm{Bin}}
\newcommand{\Poi}{\mathrm{Poi}}
\newcommand{\Unif}{\mathrm{Unif}}
\newcommand{\Bernoulli}{\mathrm{Ber}}
\newcommand{\Geom}{\mathrm{Geom}}
\newcommand{\DistD}{\mathcal{D}}
\newcommand{\DistG}{\mathcal{G}}
\newcommand{\DTV}{d_\mathrm{TV}}
\newcommand{\DKL}{D_\mathrm{KL}}
\renewcommand{\DH}{d_\mathrm{H}}
\newcommand{\DK}{d_\mathrm{K}}

\renewcommand{\vec}[1]{\boldsymbol{\mathbf{#1}}}
\newcommand{\PSet}{\mathcal{P}}
\newcommand{\Real}{\mathbb{R}}
\newcommand{\Rational}{\mathbb{Q}}
\newcommand{\Nat}{\mathbb{N}}
\newcommand{\Int}{\mathbb{Z}}
\newcommand{\argmax}{\operatorname*{\mathrm{arg\,max}}}
\newcommand{\argmin}{\operatorname*{\mathrm{arg\,min}}}
\renewcommand{\implies}{\Rightarrow}
\newcommand{\2}{\{0, 1\}}

\renewcommand{\d}{\mathrm{d}}
\newcommand{\Diff}[2][]{\frac{\d#1}{\d#2}}
\newcommand{\Grad}{\nabla}
\newcommand{\Del}[2][]{\frac{\partial#1}{\partial#2}}

\newcommand{\lmin}{\lambda_{\min}}
\newcommand{\lmax}{\lambda_{\max}}

\newcommand{\pd}{{\operatorname PD}}
\newcommand{\bP}{{\mathbb P}}
\newcommand{\trace}{\operatorname{tr}}
\newcommand{\rank}{\operatorname{rank}}
\newcommand{\srank}{\operatorname{r}}
\def\H{\mathcal H}
\def\X{\mathcal X}
\def\D{\mathcal D}
\def\P{\mathbb P}
\def\R{\mathbb R}
\def\C{\mathbb C}
\def\B{\mathbb B}
\def\Q{\mathbb Q}
\def\N{\mathbb N}
\def\I{\mathbb I}
\def\Z{\mathbb Z}
\def\bfx{\mathbf x}
\def\argmax{\mathrm{argmax}}
\def\sgn{\mathrm{sgn}}
\def\deff{\stackrel{\text{def}}{=}}
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}


\newcommand{\cA}{\mathcal{A}}
\newcommand{\cB}{\mathcal{B}}
\newcommand{\cC}{\mathcal{C}}
\newcommand{\cD}{\mathcal{D}}
\newcommand{\cE}{\mathcal{E}}
\newcommand{\cF}{\mathcal{F}}
\newcommand{\cG}{\mathcal{G}}
\newcommand{\cH}{\mathcal{H}}
\newcommand{\cI}{\mathcal{I}}
\newcommand{\cJ}{\mathcal{J}}
\newcommand{\cK}{\mathcal{K}}
\newcommand{\cL}{\mathcal{L}}
\newcommand{\cM}{\mathcal{M}}
\newcommand{\cN}{\mathcal{N}}
\newcommand{\cO}{\mathcal{O}}
\newcommand{\cP}{\mathcal{P}}
\newcommand{\cQ}{\mathcal{Q}}
\newcommand{\cR}{\mathcal{R}}
\newcommand{\cS}{\mathcal{S}}
\newcommand{\cT}{\mathcal{T}}
\newcommand{\cU}{\mathcal{U}}
\newcommand{\cV}{\mathcal{V}}
\newcommand{\cW}{\mathcal{W}}
\newcommand{\cX}{\mathcal{X}}
\newcommand{\cY}{\mathcal{Y}}
\newcommand{\cZ}{\mathcal{Z}}


\newcommand{\bA}{\vec{A}}
\newcommand{\bB}{\vec{B}}
\newcommand{\bC}{\vec{C}}
\newcommand{\bD}{\vec{D}}
\newcommand{\bE}{\vec{E}}
\newcommand{\bF}{\vec{F}}
\newcommand{\bG}{\vec{G}}
\newcommand{\bH}{\vec{H}}
\newcommand{\bI}{\vec{I}}
\newcommand{\bJ}{\vec{J}}
\newcommand{\bK}{\vec{K}}
\newcommand{\bL}{\vec{L}}
\newcommand{\bM}{\vec{M}}
\newcommand{\bN}{\vec{N}}
\newcommand{\bO}{\vec{O}}
\newcommand{\bQ}{\vec{Q}}
\newcommand{\bR}{\vec{R}}
\newcommand{\bS}{\vec{S}}
\newcommand{\bT}{\vec{T}}
\newcommand{\bU}{\vec{U}}
\newcommand{\bV}{\vec{V}}
\newcommand{\bW}{\vec{W}}
\newcommand{\bX}{\vec{X}}
\newcommand{\bY}{\vec{Y}}
\newcommand{\bZ}{\vec{Z}}


\newcommand{\paren}[1]{(#1)}
\newcommand{\Paren}[1]{\left(#1\right)}
\newcommand{\bigparen}[1]{\big(#1\big)}
\newcommand{\Bigparen}[1]{\Big(#1\Big)}
\newcommand{\brac}[1]{[#1]}
\newcommand{\Brac}[1]{\left[#1\right]}
\newcommand{\bigbrac}[1]{\big[#1\big]}
\newcommand{\Bigbrac}[1]{\Big[#1\Big]}
\newcommand{\Biggbrac}[1]{\Bigg[#1\Bigg]}
\newcommand{\abs}[1]{\lvert#1\rvert}
\newcommand{\Abs}[1]{\left\lvert#1\right\rvert}
\newcommand{\bigabs}[1]{\big\lvert#1\big\rvert}
\newcommand{\Bigabs}[1]{\Big\lvert#1\Big\rvert}
\newcommand{\card}[1]{\lvert#1\rvert}
\newcommand{\Card}[1]{\left\lvert#1\right\rvert}
\newcommand{\bigcard}[1]{\big\lvert#1\big\rvert}
\newcommand{\Bigcard}[1]{\Big\lvert#1\Big\rvert}
\newcommand{\Set}[1]{\left\{#1\right\}}
\newcommand{\bigset}[1]{\big\{#1\big\}}
\newcommand{\Bigset}[1]{\Big\{#1\Big\}}
\newcommand{\norm}[1]{\lVert#1\rVert}
\newcommand{\Norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\bignorm}[1]{\big\lVert#1\big\rVert}
\newcommand{\Bignorm}[1]{\Big\lVert#1\Big\rVert}
\newcommand{\Biggnorm}[1]{\Bigg\lVert#1\Bigg\rVert}
\newcommand{\normt}[1]{\norm{#1}_2}
\newcommand{\Normt}[1]{\Norm{#1}_2}
\newcommand{\bignormt}[1]{\bignorm{#1}_2}
\newcommand{\Bignormt}[1]{\Bignorm{#1}_2}
\newcommand{\ovl}{\overline}
\newcommand{\snormt}[1]{\norm{#1}^2_2}
\newcommand{\bigsnormt}[1]{\bignorm{#1}^2_2}
\newcommand{\Bigsnormt}[1]{\Bignorm{#1}^2_2}
\newcommand{\snorm}[1]{\norm{#1}^2}
\newcommand{\Snorm}[1]{\Norm{#1}^2}
\newcommand{\bigsnorm}[1]{\bignorm{#1}^2}
\newcommand{\Bigsnorm}[1]{\Bignorm{#1}^2}
\newcommand{\normo}[1]{\norm{#1}_1}
\newcommand{\Normo}[1]{\Norm{#1}_1}
\newcommand{\bignormo}[1]{\bignorm{#1}_1}
\newcommand{\Bignormo}[1]{\Bignorm{#1}_1}
\newcommand{\normi}[1]{\norm{#1}_\infty}
\newcommand{\Normi}[1]{\Norm{#1}_\infty}
\newcommand{\bignormi}[1]{\bignorm{#1}_\infty}
\newcommand{\Bignormi}[1]{\Bignorm{#1}_\infty}
\def\quantile{\mathrm{qntl}}

\newcommand{\iprod}[1]{\langle#1\rangle}
\newcommand{\Iprod}[1]{\left\langle#1\right\rangle}
\newcommand{\bigiprod}[1]{\big\langle#1\big\rangle}
\newcommand{\Bigiprod}[1]{\Big\langle#1\Big\rangle}
\newcommand{\relu}{\mathrm{ReLU}}
\newcommand{\median}{\mathrm{median}}
\newcommand{\hide}[1]{}
\DeclareMathOperator{\supp}{supp}


\DeclareMathOperator*{\pr}{\mathbf{Pr}}
\newcommand{\eqdef}{\stackrel{{\mathrm {\footnotesize def}}}{=}}

\newcommand{\littlesum}{\mathop{\textstyle \sum}}

\newcommand{\op}{\textnormal{op}}
\newcommand{\fr}{\textnormal{F}}

\newcommand{\ymed}{\widetilde{y}_{\mathrm{median}}}

\newcommand{\Tset}[2]{T^{(#1)}_{(#2)}}
\newcommand{\tildeTset}[2]{\widetilde{T}^{(#1)}_{(#2)}}
\newcommand{\Tt}[1]{T_{(#1)}}
\newcommand{\tildeTt}[1]{\widetilde{T}_{(#1)}}


\newcommand{\Cvar}{C_1}
\newcommand\opnorm[1]{\left\| #1 \right\|_{\textrm{op}}}
\newcommand{\normal}{\mathcal{N}}
\def\d{\mathrm{d}}
\def\Proj{\mathrm{Proj}}
\newcommand{\tr}{\mathrm{tr}}
\let\vec\mathbf

\def\program{\mathrm{LP}}

\newcommand{\oA}{\overline{A}}
\newcommand{\oD}{\overline{D}}
\newcommand{\oS}{\widetilde{S}}
\newcommand{\oT}{\widetilde{T}}
\newcommand{\omu}{\widetilde{\mu}}
\newcommand{\oSigma}{\widetilde{\Sigma}}
\newcommand{\ox}{\widetilde{x}}
\newcommand{\oX}{\widetilde{X}}
\newcommand{\oY}{\widetilde{Y}}
\newcommand{\oy}{\widetilde{y}}


\def\colorful{1}


\ifnum\colorful=1
\newcommand{\new}[1]{{\color{red} #1}}
\newcommand{\blue}[1]{{\color{blue} #1}}
\newcommand{\newblue}[1]{{\color{blue} #1}}
\else
\newcommand{\new}[1]{{#1}}
\newcommand{\blue}[1]{{#1}}
\newcommand{\newblue}[1]{{#1}}
\fi

\allowdisplaybreaks




\title{Efficient Multivariate Robust Mean Estimation\\ 
Under Mean-Shift Contamination}

\author{
Ilias Diakonikolas\thanks{Supported by NSF Medium Award CCF-2107079 and an H.I. Romnes Faculty Fellowship.}\\
University of Wisconsin-Madison\\
{\tt ilias@cs.wisc.edu}\\
\and
Giannis Iakovidis\thanks{Supported in part by NSF Medium Award CCF-2107079 and NSF Award DMS-2023239 (TRIPODS).}\\
University of Wisconsin-Madison\\
{\tt iakovidis@wisc.edu}\\
\and
Daniel M. Kane\thanks{Supported by NSF Medium Award CCF-2107547 and NSF Award CCF-1553288 (CAREER).}\\
University of California, San Diego\\
{\tt dakane@cs.ucsd.edu}
\and
Thanasis Pittas\thanks{Supported by NSF Medium Award CCF-2107079.}\\
University of Wisconsin-Madison\\
{\tt pittas@wisc.edu}\\
}

\begin{document}

\maketitle
\begin{abstract}
We study the algorithmic problem of robust mean estimation of an 
identity covariance Gaussian in the presence of  mean-shift contamination. In this  
contamination model, we are given a set of points in $\R^d$ generated i.i.d.\ 
via the following process. For a parameter $\alpha<1/2$, the $i$-th sample $x_i$ 
is obtained as follows: with probability $1-\alpha$, $x_i$ is drawn from $\cN(\mu, I)$, 
where $\mu \in \R^d$ is the target mean; and with probability $\alpha$, $x_i$ is drawn 
from $\cN(z_i, I)$, where $z_i$ is unknown and potentially arbitrary. 
Prior work characterized the information-theoretic limits of this task. 
Specifically, it was shown that--- 
in contrast to Huber contamination---
in the presence of mean-shift contamination
consistent estimation is possible. 
On the other hand, all known robust estimators in the mean-shift model have 
running times exponential in the dimension. 
Here we give the first computationally efficient algorithm 
for high-dimensional robust mean estimation with mean-shift contamination that can tolerate 
a constant fraction of outliers. 
In particular, our algorithm has   
near-optimal sample complexity, 
runs in sample-polynomial time, 
and approximates the target mean to {\em any} desired accuracy. 
Conceptually, our result contributes to a growing body of work that studies inference 
with respect to natural noise models 
lying in between fully adversarial and random settings.
\end{abstract}

\thispagestyle{empty}

\newpage
\setcounter{page}{1}
\section{Introduction}







\paragraph{Background and Motivation} 
Robust statistics \cite{Huber09,diakonikolas2023algorithmic} aims to develop accurate 
estimators in the presence of a constant fraction of outliers. In a range of machine learning 
scenarios, the standard i.i.d.\ assumption does not accurately represent the underlying 
phenomenon. 
For example, in ML security applications \cite{barreno2010security,BiggioNL12,SKL17-nips,TranLM18,DKK+19-sever}, the data may be adversarially manipulated or contain out-of-distribution data which arise from unknown categories \cite{du2024does}; 
in biological applications, datasets often contain natural outliers \cite{RP-Gen02,Pas-MG10,Li-Science08,DKK+17} that may pollute downstream statistical analysis.
The field of robust statistics originates from the 1960s with the pioneering 
works of Tukey and Huber \cite{tukey1960survey, Huber64}. Early work in the field 
obtained minimax optimal robust estimators for the mean estimation and other tasks. 
However, the multivariate versions of these estimators incurred exponential runtime in the dimension. A recent line of work in computer science, starting with \cite{DKKLMS16, LaiRV16}, 
has led to a revival of robust statistics from an algorithmic standpoint, 
by providing the first robust estimators in high dimensions 
with polynomial sample and time complexity. 

The prototypical setting for robust statistics is Gaussian mean estimation 
in Huber'a contamination model~\cite{Huber64}. Letting 
$\alpha \in (0,1/2)$ denote the contamination parameter, each sample in Huber's model is drawn either from an inlier normal 
distribution $\mathcal{N}(\mu,I)$ (where $\mu$ is the unknown mean to be estimated) 
with probability $1-\alpha$; or from an unknown and potentially arbitrary 
distribution $E$ with probability $\alpha$. Recent work has provided 
efficient algorithms with near-optimal error guarantees for this model 
\cite{DKKLMS18-soda, diakonikolas2024near}.
Huber's contamination is a rather strong model, as it allows for the unknown distribution of corruptions to be arbitrary. On the one hand, 
this level of generality makes the model quite powerful --- 
by allowing it to cover a wider range of phenomena. 
On the other hand, the model definition is inherently tied to 
significant information-theoretic limitations: 
It is known that even with an  infinite number of samples,
any mean estimator $\hat{\mu}$ in Huber's model has to incur an error of 
$\|\mu - \hat{\mu}\|=\Omega(\alpha)$, where $\alpha$ is the rate of contamination. 
This happens because it is possible to start from two  Gaussians (even in one 
dimension) with means $\Omega(\alpha)$ apart, and mix them 
with appropriate outlier distributions so that the contaminated 
distributions are identical; see, e.g., Chapter 1 of~\cite{diakonikolas2023algorithmic}. 
(Consistency is similarly unachievable in 
other commonly used corruption models like total variation 
or strong contamination.)  



\paragraph{Robust Estimation with Mean Shift Contamination}
A natural way to achieve consistency is to impose additional 
structure on the contamination model. A prominent and 
well-studied assumption is that outliers do not follow arbitrary 
distributions, but instead correspond to mean-shifted copies of 
the original distribution. Mean shift contamination has been 
extensively studied in the robust statistics literature, 
for both regression~\cite{sardy2001robust,gannaz2007robust,mccann2007robust,she2011outlier} 
and mean estimation~\cite{cai2010optimal,collier2019multidimensional,carpentier2021estimating,li2023robust,kotekal2024optimal}.

In this paper, we consider arguably the most basic version of mean 
shift contamination, where the inliers are drawn from a 
known covariance Gaussian distribution with unknown mean $\mu$, 
while each outlier is sampled from a (potentially different) 
Gaussian with the same covariance and unknown arbitrary mean. 
The goal is to understand the statistical-computational landscape of mean estimation in this model. 



\begin{definition}[Mean-Shift Contamination Model]\label{def:cont}
    For $\alpha \in (0,1/2)$ and $n \in \Z_+$, a set $T$ of $n$ points in $\R^d$ is called 
    an $\alpha$-corrupted set of points from $\cN(\mu,I)$ under the mean-shift model,
    if an adversary chooses $z_1,\ldots,z_n$, and each point $x_i \in T$ is then sampled independently as follows: 
    With probability $1-\alpha$, $x_i$ is sampled from $\cN(\mu,I)$, and probability  $\alpha$,  
    $x_i$ is sampled from $\cN(z_i,I)$.\looseness=-1
\end{definition}



Some comments are in order. 
A more general version of \Cref{def:cont} 
would allow the inliers and outliers 
be drawn from Gaussians with covariance matrix $\Sigma$.
When $\Sigma$ is known to the algorithm, it suffices to consider the case $\Sigma=I$ 
(as we can transform the samples and reduce to this case). The 
known covariance version of the shift contamination model 
has been studied in a number of works, 
including~\cite{collier2019multidimensional,carpentier2021estimating, li2023robust, kotekal2024optimal}.\footnote{\cite{carpentier2021estimating} considers a special case of the model where $z_i - \mu$ are assumed to be non-negative.} 
Moreover, the closely related (more challenging) 
model where the covariance is unknown 
has also been considered~\cite{cai2010optimal, carpentier2021estimating, kotekal2024optimal}. 
Mean shift contamination has been studied  
from a theoretical statistics standpoint, 
with a focus on minimax rates, 
and as a modeling 
assumption~\cite{jin2007estimating,sun2007oracle,cai2009simultaneous,efron2004large,efron2007correlation,efron2008microarrays}. 
Additional motivation can be found 
in~\cite{kotekal2024optimal}.
A more detailed summary of related work is provided in \Cref{sec:related_work}.







The majority of prior work has primarily focused 
on the one-dimensional setting.
Specifically,~\cite{kotekal2024optimal}  
obtained sharp minimax rates (upper and lower bounds) of 
estimating $\mu$ in absolute error.  
(These works also consider the more general estimation 
task for the unknown variance case.) 
Intriguingly, and in sharp contrast to Huber's model, 
consistency is achievable in the mean-shift model. Intuitively, this happens because 
now all the samples are convoluted with a Gaussian; thus, given infinitely many samples, 
one can form the {underlying} distribution, perform deconvolution, 
and recover the single spike corresponding to the inliers. 
More specifically, 
\cite{kotekal2024optimal} showed that, if the contamination parameter $\alpha$ is a positive 
constant strictly less than $1/2$, {the mean can be estimated to any desired accuracy $\eps$ using}
$2^{\Theta(1/\eps^2)}$ samples. 
This can be used to show {(see~\Cref{thm:brute_force})} that the $d$-dimensional version of the problem can be solved, up to $\ell_2$-error $\eps$, with $n = d \, 2^{O(1/\eps^2)}$ 
samples---an upper bound that is essentially best possible.  

{While the one-dimensional estimators of~\cite{li2023robust, kotekal2024optimal} can be implemented in polynomial time, very little is known from an algorithmic standpoint for the multivariate problem. Specifically, the only known methods (achieving any desired accuracy) 
involve a brute-force search with complexity that scales exponentially in the dimension (\Cref{thm:brute_force}).  Of course, one could 
apply efficient robust algorithms designed for Huber's model from the existing robust statistics literature. Unfortunately, such algorithms inherently cannot obtain error better 
than $\Omega(\alpha)$---while our goal is to achieve any desired accuracy $\eps \ll \alpha$. }
 This discussion leads to the following question, which was the main motivation for this work:\looseness=-1
\begin{center} 
\emph{Is there a sample and {\em computationally efficient} multivariate robust mean estimator\\ 
in the mean-shift model? }
\end{center}
By the term ``sample efficient'', we mean an estimator %
with sample complexity $n=\poly(d,2^{1/\eps^2})$ samples. As is 
standard, computationally efficient refers to an algorithm with 
$\poly(n,d)$ runtime. %
In this paper, we answer this question in the affirmative. As a bonus, the sample complexity of our algorithm is minimax near-optimal, within logarithmic factors.

Specifically, we establish the following theorem:



\begin{restatable}{theorem}{MAINTHEOREM}{\em (Main Algorithmic Result)} \label{thm:main}
    Let $d \in \Z_+$ denote the dimension, $\mu \in \R^d$ be an unknown mean vector,  $\eps \in (0,1)$ be an accuracy parameter, and $\alpha\le 0.49$ be a contamination parameter. There exists an algorithm that takes as input $\eps$, draws 
    {$n = \tilde{O}(d/\eps^{2+o(1)} + 2^{O(1/\eps^2)})$} $\alpha$-corrupted samples from $\cN(\mu,I)$ under the mean-shift model (\Cref{def:cont}), runs in $\poly(n, d)$ time, and outputs $\widehat{\mu}$ such that with probability at least $0.99$ it holds $\| \widehat{\mu} - \mu \| \leq \eps$.
\end{restatable}



\Cref{thm:main} gives the first sample efficient 
algorithm for our problem 
that runs in sample-polynomial time.
We remind the reader that, in the Huber contamination 
model, this goal is information-theoretically impossible; that 
is, no algorithm can achieve consistency regardless of its 
sample and computational resources.

Note that the sample complexity of our algorithm 
is close to optimal: even for one-dimension with $\alpha \leq 0.49$, 
any estimator requires $2^{\Omega(1/\eps^2)}$ samples~\cite{carpentier2021estimating,kotekal2024optimal}; 
and linear dependence on $d$ is necessary even for the outlier-free setting. 
Moreover, we point out that our algorithm does not need to know the value 
of the contamination parameter $\alpha$. While we have assumed for simplicity of the statement 
that $\alpha \leq 0.49$, we show in \Cref{appendix:breakdownpoint} (see \Cref{thm:higher_breakdown}) that by a mild increase in the error and sample complexity, 
our algorithm can be generalized to work for $\alpha \in (0,1/2-c)$, for any constant $c \in (0,1/2)$, without the need to know $c$ a priori.%

\subsection{Organization}
The paper is structured as follows: In \Cref{sec:techniques}, 
we provide an overview of our techniques and outline 
the structure of our algorithm. In \Cref{sec:analysis}, we 
formally state the algorithm and sketch its analysis. Finally, 
in \Cref{sec:open-problems}, we  discuss directions for future work.


\subsection{Our Techniques}\label{sec:techniques}

Given samples of the form $x \sim \cN(m,I)$, 
where $m$ is a random variable taking the value $m=\mu$ with probability $2/3$ 
(instead of $2/3$, our model of \Cref{def:cont} uses $1-\alpha$; 
this is a simplification for the purpose of this proof overview), 
the goal is to estimate $\mu$ to $\ell_2$-error $\eps$ with high constant probability. 


Note that, using the results of \cite{li2023robust,kotekal2024optimal}, 
it is straightforward to design a (computationally inefficient) algorithm 
with sample complexity $d2^{O(1/\eps^2)}$ and runtime  $2^{O(d+1/\eps^2)}$ --- 
via a standard reduction to the $1$-dimensional case (cf. \Cref{thm:brute_force}). 
In particular, for every $v$ in an exponentially-sized cover of the unit sphere, 
we apply a robust one-dimensional estimator to the projections of the samples $v^\top x$. 
This gives us an $\eps$-approximation of $v^\top \mu$. Piecing these values together 
for various values of $v$ allows us to reconstruct an $\eps$-approximation of $\mu$.


As a first step towards \Cref{thm:main}, we will design an algorithm  
with sample complexity $\poly(d)2^{\Theta(1/\eps^2)}$ and runtime $\poly(d)2^{\Theta(1/\eps^4)}$. Note that this is not yet a \emph{computationally efficient} algorithm, 
since the  runtime is {\em quasi-polynomial} in the sample size. 
It turns out that this simpler algorithm is a good start, 
as it will lay down the main ideas which eventually lead to the efficient algorithm of \Cref{thm:main}. 
The main idea is to design some kind of (data dependent) dimension reduction. 
Ideally, we would like to first find the direction $\mu/\|\mu\|$ along which $\mu$ lies, 
and then run a one-dimensional robust estimator on that direction to recover $\|\mu\|$. 
More realistically, we will attempt to recover a low-dimensional subspace 
that on which $\mu$ has a large projection, 
and then run a low-dimensional robust estimator on that subspace. 
In particular, if we can find a subspace $\cV$ of dimension 
$\mathrm{dim}(\cV) \leq \poly(1/\eps)$ such that $\|\Proj_{\cV^\perp}(\mu)\| \leq \eps/2$,
then we can apply the algorithm of \Cref{thm:brute_force} 
to the projection of the dataset onto $\cV$ to find 
a $\hat{\mu} \in \cV$ with $\|\Proj_{\cV}(\mu) - \hat{\mu}\| \leq \eps/2$. 
This would imply the desired error bound of $\| \mu - \hat{\mu}\| \leq \eps$. 

A standard way to find such a $\cV$ is by leveraging the second moment of the data. 
We have that $\E[xx^\top-I] = \E[m m^\top] \succcurlyeq \frac{2}{3} \mu \mu^\top$, 
since at least $2/3$ of samples (inliers) follow $\cN(\mu,I)$ and the terms corresponding to outliers are positive semidefinite. 
Therefore, if $\cV$ is the subspace spanned by all of the eigenvectors of $\E[xx^\top-I]$ 
with eigenvalues more than $\eps^2/6$, then $\|\Proj_{\cV^\perp}(\mu)\| \leq \eps/2$. Unfortunately, as $m$ is unbounded, $\E[mm^\top]$ could have many such eigenvalues. 
To fix this issue, we will need a way to effectively truncate the larger values of $m$.




To achieve this, we instead estimate the matrix 
$A:=\E[(xx^\top - \frac{1}{1 + 2\gamma} I) \exp(-(\|x\|^2 - d) \gamma)]$, 
for some carefully chosen $\gamma<1$. 
We can explicitly calculate the expectation for $x \sim \cN(m,I)$, 
which turns out to be roughly equal to $mm^\top e^{-\|m\|^2 \gamma}$ 
(cf. \Cref{lemma:expression}).
Since $2/3$ fraction of the samples have $m=\mu$, 
it follows that $A \succcurlyeq \frac{2}{3} \mu \mu^\top e^{-\|\mu\|^2 \gamma}$.
Note that so long as $\|\mu\|$ is not too large 
(indeed, we can assume it is $O(1)$ by some na\"ive outlier removal), 
every unit vector with $v^\top A v \leq \eps^2/6$ 
must satisfy $|v^\top \mu| \leq \eps/2$ (cf. \Cref{lemma:dimerror}). 
Additionally, it is not hard to see that $A$ has trace bounded by $O(1/\gamma)$; 
this implies that the number of eigenvectors with eigenvalue larger than $C \eps^2$ 
is at most $O(1/(\gamma \eps^2))$. Thus, if we take $\cV$ to be the span 
of such eigenvectors, we would have that $\|\Proj_{\cV^\perp}(\mu)\| \leq \eps/2$ 
and $\dim(\cV) = O(1/(\gamma \eps^2))$. 

This procedure implements one round of our 
dimension reduction. For $\gamma=1/\sqrt{d}$, this reduces the dimension 
from $d$ to $O(\sqrt{d}/\eps^2)$. That is, the dimension 
is reduced by a factor of $2$ whenever $d$ is 
at least a large constant multiple of $1/\eps^4$. 
By repeating this procedure, we reduce the dimension down to 
$O(1/\eps^4)$. %
Then, applying the algorithm of \Cref{thm:brute_force} on the remaining 
subspace of dimension $d'=O(1/\eps^4)$, completes the algorithm 
using $2^{O(d' + 1/\eps^2)} =2^{O(1/\eps^4)}$ 
additional samples and time.


The first challenge towards implementing the above approach is approximating $A$ from samples. 
This is non-trivial, as $\exp(-(\|x\|^2-d) \gamma)$ can be as large as $\exp(d\gamma)$, 
suggesting that we might need roughly this many samples. 
Fortunately, we note that $\|x\|^2$ is unlikely to be much less than $d$. 
In particular, the squared norm of a standard normal 
is approximately Gaussian distributed with mean $d$ and standard deviation $\sqrt{d}$. 
This implies that the expected size of $\exp(-(\|x\|^2-d)\gamma)$ is approximately  
$\exp( d \gamma^2 )$ (and it is smaller for Gaussians with other means). 
By simple concentration arguments, %
for each entry of the matrix $A$, it is not difficult to show that 
$\poly(d/\eps)\exp( d \gamma^2)$ samples suffice for $A$ to convergence . 
Thus, the choice $\gamma=1/\sqrt{d}$ made in the previous paragraph 
suffices for our purposes (\Cref{lem:sample_complexity2}). 


We emphasize that there are two problems with this approach to be addressed.
The first  is that the runtime of the algorithm presented 
so far is not polynomial: this is because the overall number of samples is $n=\poly(d)2^{\Theta(1/\eps^2)}$, 
while the runtime of the final brute-force step 
is $\poly(d)2^{\Theta(1/\eps^4)}$.
Second, we would like the sample complexity to be linear in $d$ rather than $\poly(d)$. 

In order to resolve the first issue, we need our dimensionality reduction 
to be able to bring the dimension all the way down to $O(1/\eps^2)$--- 
rather than  $O(1/\eps^4)$. To achieve this, we can first reduce the dimension 
to $d'=O(1/\eps^4)$, as described in the earlier paragraphs, 
and then change the value of $\gamma$ to $1/(\sqrt{d'} \eps)$. 
This way, the next iteration will reduce $d'$ to $O(\sqrt{d'}/\eps)$ 
(which means that we can keep halving the dimension until $d'$ 
becomes a constant multiple of $1/\eps^2$). 
Using this value for $\gamma$ implies 
that each iteration will be using $\exp( O(d'  \gamma^2) ) = \exp(O(1/\eps^2))$ 
samples and time (\Cref{lem:sample_complexity2}). 
Finally, we apply \Cref{thm:brute_force} to the resulting subspace $\cV$. 


To improve the sample size dependence on the dimension 
(cf. \Cref{lem:sample_complexity}), we require strong 
concentration bounds, in particular the Matrix Bernstein inequality.
However, to apply this inequality, one needs a universal bound 
for our random variable $xx^\top \exp(-(\|x\|^2-d)\gamma)$, 
which is bad when $\|x\|^2$ is small. Fortunately, this happens with small probability, 
and we are able to show that truncating the values when $\|x\|^2$ 
is much smaller than $d$ will not affect the final mean by much. 
However, taking $\gamma=1/\sqrt{d}$ will prove not quite sufficient for our purposes:
there will be roughly a $1/d$-fraction of samples with 
$\|x\|^2 = d-\Omega(\sqrt{d \log(d)})$, 
leading to terms with norm on the order of $d \exp(\Omega(\sqrt{\log(d)}))$, 
and it will take a slightly super-linear number of samples to average these away. 
To handle this, we need to use a slightly smaller value of $\gamma$, 
such as $1/\sqrt{d\log(d)}$.




\paragraph{Notation}


We use $\mathbb{Z}_+$ for the set of positive integers. We denote $[n]=\{1,\ldots,n\}$. For a vector $x$ we denote by $\Norm{x}$ its  Euclidean norm. Let $I_d$  denote the $d\times d$ identity matrix (omitting the subscript when it is clear from the context). We use  $\top$ for the transpose of matrices and vectors.
For a subspace $\mathcal{V}$ of $\R^d$ of dimension $m$, we denote by $\Pi_{\cV} \in \R^{d \times d}$ the orthogonal projection matrix of $\cV$. 
 That is, if the subspace $\cV$ is spanned by the columns of the matrix $A$, then $ \Pi_{\cV}:=A(A^\top A)^{-1} A^\top$.
For a vector $x \in \R^d$, we use $\Proj_{\cV}(x) = \Pi_{\cV} x$ to denote the orthogonal projection of $x$ onto $\cV$.
We say that a symmetric $d\times d$ matrix $A$ is PSD (positive semidefinite) and write $A\succcurlyeq 0$ if for all $x\in \mathbb{R}^d$ it holds $x^\top A x\ge 0$. We use $\Norm{A}_{\op}$ and $\| A \|_\fr$ for the operator (spectral) and Frobenius norm of a matrix $A$ respectively.  
We write $x\sim D$ for a random variable $x$ following the distribution $D$ and use $\E[x]$ for its expectation. We use $\cN(\mu,\Sigma)$ to denote the Gaussian distribution with mean $\mu$ and covariance matrix $\Sigma$. For a scalar random variable $x$, we define the $L_p$-norm of $x$ to be $\|x\|_{L_p} = \E[|x|^p]^{1/p}$. 
We use $a\lesssim b$ to denote that there exists an absolute universal constant $C>0$ (independent of the variables or parameters on which $a$ and $b$ depend) such that $a\le Cb$.
We use $\polylog()$ to denote a quantity that is polylogarithmic in its arguments and we use $\tilde{O}$ to hide such factors.



\section{Efficient Robust Mean Estimation in the Mean-Shift Model: Proof of~\Cref{thm:main}}\label{sec:analysis}

In this section, we present our %
algorithm (\Cref{alg:mean_estimation}) 
and establish \Cref{thm:main}. 
\Cref{sec:brute-force} provides a sample-efficient but computationally  inefficient estimator, 
which will be employed at the final step of our algorithm (after the dimension has been significantly decreased). 
\Cref{sec:matrix,sec:main_analysis} 
analyze our dimension reduction procedure: \Cref{sec:matrix} records the desired 
properties of the reweighted second moment matrix that our algorithm relies on, 
and shows that they hold with high probability 
with sufficiently many samples. 
Finally, \Cref{sec:main_analysis} provides the core 
analysis of the dimension reduction procedure, 
and combines everything to prove \Cref{thm:main}.



\begin{algorithm}[]
\caption{Robust Mean Estimation under \Cref{def:cont}}
\label{alg:mean_estimation}
\begin{algorithmic}[1]
\State \textbf{Input}: Accuracy $\eps > 0$,  sample access to the model of \Cref{def:cont}.
\State \textbf{Output}: $\widehat{\mu} \in \R^d$ such that $\| \widehat{\mu} - \mu \| \leq \eps$.
\vspace{6pt}

\State Fix $C$ a sufficiently large  constant, $n_0 {=} C d$,  $n_1 {=} (d/\eps^{2+o(1)}) \log^C (d) $, and $n_2 {=} 2^{C/\eps^2} \log^C(d)$. \label{line:samples}

\State /*\textbf{Rough Estimation:}*/
\State Draw $T_0$, a set of $n_0$ corrupted points according to \Cref{def:cont}.
\State \label{line:warm_start}Use $T_0$ to find $\widehat{\mu}_0$ with $\|\widehat{\mu}_0 - \mu\|  = O(1)$. \Comment{e.g., Corollary $2.12$ and Exercise 2.10 in \cite{diakonikolas2023algorithmic}}
    
 \State \textbf{/*Dimension Reduction:*/} 
\State Initialize $t \gets 1$,  $k \gets d$ and $\cV_1 = \R^k$.\Comment{$k$ will denote the dimension of the current subspace}

\While{$k \geq %
1/\eps^2
$}\label{line:while}
    \If{$k \geq C\log^4 (d)/\eps^5$}\label{line:phase1} \space set  $\beta \gets \sqrt{\log (k)}$ and $N \gets n_1$, $\eta_t \gets  \left( {\eps}/{ \log d} \right)^2$.\label{line:eta_t_1}
    \Else \space set $\beta \gets \epsilon $ and $N \gets n_2$, $\eta_t \gets 36 \eps/\sqrt{k}$.\label{line:eta_t_2}
    \EndIf
    \State \label{line:sample_set}Draw a set $T_t'$ of $N$ corrupted points from the model of \Cref{def:cont}.
    \State \label{line:transform_set}  $T_t \gets \{ \Proj_{\cV_t}(x - \widehat{\mu}_0) : x \in T_t'\}$. 
     
    
    
    \State\label{line:Ahat}
    $\widehat{A}_t \gets \frac{1}{|T_t|}\sum_{x \in T_t} \left(x x^\top -\frac{\beta \sqrt{k}}{\beta \sqrt{k}+2} \Pi_{\cV_t} \right)e^{-\frac{\norm{x}^2}{\beta\sqrt{k}} } \left( 1+\frac{2}{\beta \sqrt{k}} \right)^{\frac{k}{2}+2}$.
    
    \State Find the eigenvectors of $v_1^{(t+1)},\dots,v_{k'}^{(t+1)}$ of  $\widehat{A}_t$ with eigenvalue at least $\eta_t$ (let $k'$ denote the number of such eigenvectors).\label{line:new_eigenvectors}
    \State\label{line:subspace}Let $\cV_{t}$ be the subspace spanned by $\{ v_1^{(t+1)},\dots,v_{k'}^{(t+1)} \}$.
    
    \State Update $k \gets k'$ and $t \gets t + 1$.
    
\EndWhile

\State \textbf{/*Run Inefficient Algorithm:*/} 
\State\label{line:sample_set_end}Sample a set $T_t'$ of $n_2$ corrupted points from the model of \Cref{def:cont}.
\State $T_t \gets \{ \Proj_{\cV_t}(x) : x \in T_t' \}$.
\State \label{line:brute_force}Use $T_t$ to find $\widehat{\mu}_1 \in \cV_t$ with $\| \widehat{\mu}_1 - \Proj_{\cV_t}(\mu ) \| \leq \eps$. \Comment{Use Algorithm from \Cref{thm:brute_force}} 
\State \textbf{return} $\Proj_{\cV_t^\perp}(\widehat{\mu}_0) + \widehat{\mu}_1$.

\end{algorithmic}
\end{algorithm}

\subsection{Computationally Inefficient Multivariate Robust Estimator} \label{sec:brute-force}

\Cref{thm:brute_force} below provides a robust multivariate mean estimator the mean-shift model 
that uses $n=d \, 2^{O(1/\eps^2)}$ samples and 
$2^{O(d)}\poly(n, d)$ runtime. Although the runtime is exponential in the dimension, it becomes just $\poly(n, d)$ if $d=O(1/\eps^2)$. 
Therefore, this estimator will be useful after 
our dimension reduction technique that will be developed in the next sections 
manages to reduce the dimension to $O(1/\eps^2)$.

\begin{restatable}[Inefficient Estimator]{proposition}{BRUTEFORCE}\label{thm:brute_force}
    Let $d \in \Z_+$ denote the dimension, and $C$ be a sufficiently large absolute constant.
    Let $\alpha \leq 0.49$, $\eps>0$, and $\delta \in (0,1)$ be parameters, and $\mu \in \R^d$ be an (unknown) vector.
    There exists an algorithm that, on input $\eps$ and any set of $n \geq 2^{C/\eps^2}(d+\log(1/\delta))$ $\alpha$-corrupted set of points from $\cN(\mu,I)$ under the mean-shift model (cf. \Cref{def:cont}), outputs a $\widehat{\mu}$ such that $\|\widehat{\mu}-\mu \|  \leq \eps$ with probability at least $1-\delta$. Moreover, it runs in time $2^{O(d)}\poly(n, d)$.
\end{restatable}

\Cref{thm:brute_force} follows in a relatively standard way by 
taking a fine discretization of the unit sphere, running the one-
dimensional estimator for each of the directions in that cover set, and 
combining the solutions to a vector (see, e.g., Section 1.5 of 
\cite{diakonikolas2023algorithmic}). For completeness, we provide a proof of correctness in \Cref{appendix:omited_main} using the univariate 
estimator of \Cref{fact:onedim} as a black-box. A nice property of the 
prior work is that the robust univariate estimator 
has breakdown point arbitrarily close to $1/2$ and does not need 
to know the contamination parameter (c.f. Section 5.3 of \cite{kotekal2024optimal}).

\begin{fact}[One-dimensional estimator, see, e.g., \cite{li2023robust,kotekal2024optimal}]\label{fact:onedim}
    Let $\mu \in \R$ be an (unknown) mean. Let  $C$ be a sufficiently large constant and $\alpha \leq 0.49$. There is an algorithm that given,  $\eps > 0$, $\delta \in (0,1)$ and a set of $n = 2^{C/\eps^2}\log(1/\delta)$ $\alpha$-corrupted samples from $\cN(\mu,1)$ according to the mean-shift model (\Cref{def:cont}),  finds $\widehat{\mu} \in \R$ such that, with probability at least $1-\delta$, it holds $|\widehat{\mu} - \mu| \leq \eps$. The runtime of the algorithm is $\poly(n)$.
\end{fact}






 






\subsection{Reweighted Second Moment Matrix}\label{sec:matrix}

The structure of this section is as follows. 
\Cref{eq:matrixA}  
defines the reweighted second moment matrix that will be 
used by our dimensionality reduction procedure. Subsequently, 
\Cref{def:deterministic} states the deterministic conditions regarding 
this matrix that will be required for the correctness of 
our dimension reduction algorithm. Finally, in 
\Cref{lem:sample_complexity,lem:sample_complexity2}, we show that the 
matrix satisfies these conditions given sufficiently many samples. 





For $\beta \in (0,1]$, a subspace $\cV \subseteq \R^d$ of dimension $k\leq d$, and points $x_1,\ldots,x_n \in \cV$, define the matrix: \looseness=-1
\begin{align}\label{eq:matrixA}
     &\widehat{A} := \frac{1}{n} \sum_{i=1}^n \widehat{A}_{i},  \quad \text{where} \;\;   \widehat{A}_i :=  F_{\beta, k}(x_i)  Z_{\beta, k}, \notag\\  
     &Z_{\beta, k} := \left( 1 +\frac{2}{\beta \sqrt{k}} \right)^{\frac{k}{2}+2},  
     \quad F_{\beta,k}(x):= \left(x x^\top - \frac{\beta\sqrt{k}}{\beta\sqrt{k}+2} \Pi_{\cV}  \right)e^{-\frac{\|x\|^2}{\beta \sqrt{k}}} \;,
\end{align}
where $\Pi_{\cV}$ denotes the orthogonal projection matrix of the subspace $\cV$.
The subspace $\cV$ will be the one that the algorithm maintains in each 
round (and whose dimension decreases in every round). For simplicity, the 
reader can think of $\cV = \R^k$ and $\Pi_{\cV} = I_k$; however, since the 
basis of the subspace may not be aligned with the elements of the standard 
orthonormal basis, using $\Pi_{\cV}$ is required in general.  

The definition of $\widehat{A}$ has been designed so that the expectation of a single sample's deviation, namely $(xx^\top - \frac{\beta\sqrt{k}}{\beta \sqrt{k}+2}I)e^{-\frac{\|x\|^2}{\beta \sqrt{k}}}$
when  $x \sim \cN(z,I)$ is roughly proportional to $zz^\top e^{-\frac{\|z\|^2}{\beta\sqrt{k}}}$. This is shown in \Cref{lemma:expression} below. 
The factor $Z_{\beta,k}$ is the appropriate normalization factor that arises.



\begin{restatable}{lemma}{Expression}\label{lemma:expression}
Let $\cV \subseteq \R^{d}$ be a subspace of $\R^{d}$ 
of dimension $k\leq d$. For any $z \in \cV$, we have that
$\E_{x \sim \cN(z, \Pi_{\cV})}\left[F_{\beta,k}(x)\right] Z_{\beta,k}
    = 
    zz^\top e^{-\frac{\norm{z}^2}{\beta\sqrt{k}+2}  }$, 
where $F_{\beta,k}$ and $Z_{\beta,k}$ are as defined in \Cref{eq:matrixA}.
\end{restatable}


The deterministic conditions that we require to hold with high probability 
quantify how close $\hat{A}$ needs to be to its 
expectation $A :=\frac{1}{n}\sum_{i=1}^n z_i z_i^\top e^{-\frac{\|z_i\|^2}{\beta \sqrt{k}+2}}$.
For the inlier points, 
we will require closeness of the reweighted second moment 
to its expectation in operator norm (cf. \Cref{def:concentrated}). 
For the outlier samples, it will suffice to have the milder condition 
that the reweighted second moment is not too negative (cf. \Cref{def:positivedef}). Finally, note that the expectation matrix 
$A := \frac{1}{n}\sum_{i=1}^n z_i z_i^\top e^{-\frac{\|z_i\|^2}{\beta \sqrt{k}+2}}$ has 
$\tr(A) = \frac{1}{n}\sum_{i=1}^n \|z_i\|^2 e^{-\frac{\|z_i\|^2}{\beta \sqrt{k}+2}} \leq \beta \sqrt{k}+2$, where we used the elementary 
inequality $y e^{-y/\gamma} \leq \gamma$ for $y > 0$. 
The bounded trace will be a key property for the correctness 
of our dimensionality reduction procedure. 
We will thus require the same property to hold for 
the empirical reweighted moment matrix. 
\Cref{def:deterministic} combines all three of the aforementioned conditions.


\begin{definition}[($\eta,\beta$)-concentrated set] \label{def:concentrated}
Let $\eta,\beta>0$ be parameters,  
 $\cV$ be a $k$-dimensional subspace of $\R^d$ and let $x_1,\ldots, x_n  \in \cV$. We say that the set  $\{ x_1,\ldots, x_n \}$  is $(\eta,\beta)$-\emph{concentrated} with respect to $\cV$ and  $z_1, \ldots, z_{n} \in \cV$, if  the matrix $\widehat{A}$ defined in \Cref{eq:matrixA}
satisfies $\| \widehat{A} -  A \|_{\op} \leq \eta$, where $A :=\frac{1}{n}\sum_{i=1}^n z_i z_i^\top e^{-\frac{\|z_i\|^2}{\beta \sqrt{k}+2}}$.
\end{definition}
\begin{definition}[($\eta,\beta$)-positive definite] \label{def:positivedef}
Let $\eta,\beta>0$ be parameters and 
 $\cV$ be a $k$-dimensional subspace of $\R^d$. We say that the set  of points $x_1,\ldots, x_n  \in \cV$  is $(\eta,\beta)$-\emph{positive definite}, if the matrix $\widehat{A}$ defined in \Cref{eq:matrixA}
satisfies $v^\top\widehat{A}v\ge -\eta$, for all $v\in \cV$ with $\norm{v}=1$.
\end{definition}




\begin{definition}[$(\eta,\beta)$-good set]\label{def:deterministic}
    Let $\eta,\beta>0$ be parameters and
    $\cV$ be a $k$-dimensional subspace of $\R^d$. A set $T$ of points in $\cV$ is called $(\eta,\beta)$-good with respect to $\cV$ and the vectors $\mu,z_1,\ldots,z_{\alpha n} \in \cV$ if there exists  $S \subseteq T$, with $|S| = (1-\alpha)|T|$ such that:
    \begin{enumerate}[leftmargin=*]
        \item (Condition for inliers) \label{it:inliers}$S$ is ($\eta,\beta$)-concentrated with respect to $\cV$ and  the vector sequence $\mu,\ldots,\mu$ (i.e., the sequence that has the $\mu$ vector $(1-\alpha) n$ times).
        \item (Condition for outliers) \label{it:outliers}$T \setminus S$ is $(\eta,\beta)$-positive definite.
        \item (Bounded trace condition) \label{it:trace_bound}The matrix $\widehat{A}$ from \eqref{eq:matrixA} computed over all $x_i \in T$ has $\tr(\widehat{A}) \leq 18 \beta \sqrt{k}$.
    \end{enumerate}

\end{definition}

\Cref{lem:sample_complexity,lem:sample_complexity2} show that a sufficiently large set of samples from our model 
satisfies our $(\eta,\beta)$-goodness conditions with high probability.
We need two lemmata because our algorithm will use $\beta=\sqrt{\log{k}}$ 
for the most part, and $\beta=\eps$ for the last few iterations. 

For the  case $\beta=\sqrt{\log{k}}$ (corresponding to \Cref{lem:sample_complexity}), we need to use strong concentration bounds in order for the sample complexity to scale linearly (up to polylog factors) with the dimension $k$ (see proof sketch at the end of this section and full proof in \Cref{appendix:sample}). 





\begin{restatable}{lemma}{SAMPLES}\label{lem:sample_complexity}
    Let $\eta \in (0,1)$  and $k \in \Z_+$ denote the dimension, and assume $k$ is bigger than a sufficiently large constant.
    There exists sample size 
    $n=   k\log^3(k)(1/\eta)^{2+o(1)}\frac{1}{\delta} $ 
    such that the following holds:
    Let $T$ be a set of $n$ $\alpha$-corrupted points from $\cN\left(\mu,I\right)$ according to \Cref{def:cont} with the assumption that $\|\mu\|=O\left(1\right)$.  
    Denote by $z_1,\ldots,z_{\alpha n}$ the adversarial  centers %
    in \Cref{def:cont}.
    Then, with probability at least $1-\delta$, $T$ is $(\eta,\sqrt{\log k})$-good with respect to $\mu,z_1,\ldots,z_{\alpha n}$.
\end{restatable} 

For the case $\beta=\eps$, we can resort to a simpler proof which consists of 
calculating the variance of each entry of our random matrix and using Chebyshev's 
inequality entry-wise. This is done in \Cref{lem:sample_complexity2}. Although the 
resulting dependence on the dimension $k$ scales as $k^5$, we will use this bound 
only after the dimension has decreased to roughly $1/\eps^5$; 
meaning that the sample complexity will be dominated by the factor $2^{C/\eps^2}$. The 
full proof can also be found in \Cref{appendix:sample}.

\begin{restatable}{lemma}{SAMPLESTWO}\label{lem:sample_complexity2}
    Let $T$ be a set of $n$ $\alpha$-corrupted set of points from $\cN(\mu,I)$ according to \Cref{def:cont} for some $\mu$ with $\|\mu\|=O(1)$.  Assume
    $n=  \frac{k^5}{\eta^2 \delta}  2^{C/\eps^2}$ for a sufficiently large absolute constant $C$, where  $k\ge 1/\eps^2$ and $\eta\le \eps$.
    Denote by $z_1,\ldots,z_{\alpha n}$ the adversarial points used in \Cref{def:cont}.   Then, with  probability at least $1-\delta$, $T$ is $(\eta,\eps)$-good with respect to $\mu,z_1,\ldots,z_{\alpha n}$.
\end{restatable}

We conclude this section with a brief proof sketch of the first of these lemmata.

\begin{proof}[Proof Sketch of \Cref{lem:sample_complexity}] 
We focus on showing \Cref{it:outliers} ($\eta$-positive definiteness) 
for the set of outliers, as the other parts can be proved similarly. 
Let $\widehat{A}=\sum_{i:x_i\in T\setminus S} \widehat{A}_i$, 
where $\widehat{A}_i$ as defined in \Cref{eq:matrixA}. 
First note that, using \Cref{lemma:expression}, 
we have that $\E[\widehat{A}]=A:=\frac{1}{\alpha n}\sum_i z_i z_i^\top e^{-\frac{\|z_i\|^2}{\sqrt{k \beta}+ 2}}$ 
is positive semidefinite. Hence, in order to prove that $\widehat{A}$ 
is $\eta$-positive definite, 
it suffices to prove  that $\widehat{A}$ cannot be much smaller 
than $A$ in every direction.


First note that the $e^{-\frac{\|x_i\|^2}{\sqrt{k \beta}}}Z_{\beta,k}$  factor 
appearing in the definition of $\widehat{A}$
behaves roughly like $e^{-\frac{\|x_i\|^2-k}{\sqrt{k \log k}}}$, as $Z_{\beta,k}=\Theta(e^{\sqrt{k/\log k}})$ for $\beta=\sqrt{\log k}$.
Towards showing concentration with a linear number of samples, 
we  first note that the $\|x_i\|^2-k$ (appearing before) is bounded, 
except with tiny probability. 
That is, we argue that, without loss of generality, 
we can work with the matrix of the form 
$\widehat{A}'=\sum_{x_i\in T\setminus S} \widehat{A}_i\Ind(\cE_i)$ 
instead of $\widehat{A}$. 
Here $\cE_i$ is defined to be the following \emph{good event}:
    $ 
         | \Norm{x_i}^2 -(\Norm{z_i}^2 +k) | 
         \lesssim \log (1/\tau)+ (\sqrt{k}+\Norm{z_i}) \sqrt{\log (1/\tau)}
   $,
    where $\tau := (1/n)^4$. 
    This is indeed without loss of generality because: 
    (i) $\cE_i$ hold with probability $1-\delta$ for all $i$ simultaneously 
    (each $\cE_i$ fails with probability $\tau$ by \Cref{fact:normConcetration}, 
    and thus the probability that there exists $\cE_i$ that fails 
    is at most $n\tau \leq  1/n^3 \le \delta$ by union bound); 
    and (ii) we can prove that \( \norm{\mathbb{E}[\widehat{A}'] - A} \leq \eta/2\), i.e., the truncation shifts the mean by a small amount. 

    Then we can apply standard concentration results on $\widehat{A}'$, 
    as our expressions are appropriately bounded.  
    We achieve this by carefully decomposing $\widehat{A}'$ 
    into three  terms by writing $x_i=z_i+g_i$, where $g_i\sim \cN(0,I)$:
    $
        \widehat{A}_i= ( (g_ig_i^\top  - \frac{\sqrt{k\log(k)}}{\sqrt{k\log(k)}+2} I  )+(z_ig_i^\top +g_i z_i^\top) +z_iz_i^\top )e^{-\frac{\|x_i\|^2}{\sqrt{k\log(k)}}}Z_{\beta,k}
        =\vcentcolon \widehat{T}_{1i}+\widehat{T}_{2i}+\widehat{T}_{3i}.
    $
For the first and second term, we use Hoeffding's inequality 
for subgaussian random variables along with a cover argument. 
For the third term, we use  the Matrix Bernstein \Cref{fact:matrixBernstein} inequality. 
This application requires an absolute bound on $\|T_{3i}\| \leq K$ 
that holds almost surely, as well as a bound for the matrix 
variance $\norm{ \E[\widehat{T}_{3i}^2] }$. 
Given the ``good events'' from above, we can use that 
$\exp(-\norm{x}^2/\sqrt{k \log k})Z_{\sqrt{\log k},k}$ is roughly 
less than $\exp(-\norm{z_i}^2/\sqrt{k\log k})$. We can thus obtain that 
$\|T_{3i}\| \leq \|z_i\|^2 \exp(-\norm{z_i}^2/\sqrt{k\log k}) \lesssim \sqrt{k\log k}$ and $\norm{z_i}^4\exp(-2\norm{z_i}^2/\sqrt{k\log k})\lesssim k \log k$, 
which results in near-linear in $k$ sample complexity.\qedhere



\end{proof}




\subsection{Analysis of  \Cref{alg:mean_estimation}}\label{sec:main_analysis}


The lemma below is the core of the analysis for a single iteration of our 
dimensionality reduction technique. Since the sample version $\widehat{A}_t$ of the matrix, that we use in  \Cref{line:Ahat} of \Cref{alg:mean_estimation}, 
is close to its expectation, we show that our na\"ive estimator 
from the first step of the algorithm is accurate inside the subspace 
of small eigenvalues of $\widehat{A}_t$ (see \Cref{lemma:dimerror}). Combined with \Cref{it:trace_bound} of \Cref{def:deterministic}, which  
bounds from above the number of high eigenvalues,
we conclude that a single round of our 
dimensionality reduction loop halves the dimension without accumulating error.


\begin{lemma} \label{lemma:dimerror}
Let $\beta >0$ and $\cV$ be a subspace of $\R^d$ of dimension $k \leq d$.
Let $T$ be an $(\eta,\beta)$-good set of $n$ samples (cf. \Cref{def:deterministic}) with respect to the subspace $\cV$ and the vectors $\mu,z_1,\ldots,z_{\alpha n} \in \cV$, and assume that $\| \mu \| = O(1)$.
Define $\widehat{A}$ as in \eqref{eq:matrixA}.
If $\cU$ is a subspace of $\cV$ such that $v^\top \widehat{A}v \le \eta$ 
for all unit vectors $v \in \cU$, then we have that $\norm{\Proj_{\cU}(\mu)} = O(\sqrt{\eta})$, where $\Proj_{\cU}(\cdot)$ denotes the projection to subspace $\cU$.
\end{lemma}
\begin{proof}
     
Without loss of generality, we will prove the lemma for the case where $\cV=\R^k$.
Since $T$ is $(\eta,\beta)$-good, it can be partitioned into $S$ (inliers) 
and $T\setminus S$ (outliers), where the two sets satisfy the properties of \Cref{def:deterministic}. 
Consider the matrices
\begin{alignat*}{2}
    &\widehat{A}_{\mathrm{inliers}} = \frac{Z_{\beta,k}}{n}\sum_{x \in S}F_{\beta,k}(x)  ,   \quad\quad &&A_{\mathrm{inliers}} =  \mu \mu^\top e^{-\frac{\|\mu\|^2}{\beta\sqrt{k}+2}}, \\
    &\widehat{A}_{\mathrm{outliers}} = \frac{Z_{\beta,k}}{\alpha n}\sum_{x \in T \setminus S}  F_{\beta,k}(x),   \quad\quad  &&A_{\mathrm{outliers}} =  \frac{1}{\alpha n}\sum_{i: x_i \in  T \setminus S} z_i z_i^\top e^{-\frac{\|z_i\|^2}{\beta\sqrt{k}+2}} \;,
\end{alignat*}
where $Z_{\beta,k}$ and $F_{\beta,k}$ are 
as defined in \Cref{eq:matrixA}. 
We can decompose $\widehat{A}$ into inliers and outliers, 
i.e., $\widehat{A} = (1-\alpha) \widehat{A}_{\mathrm{inliers}} + \alpha \widehat{A}_{\mathrm{outliers}}$.
By the assumption that $T$ is $(\eta,\beta)$-good (cf. \Cref{def:deterministic}), we have that $T \setminus S$ is $\eta$-positive definite (cf. \Cref{def:positivedef}); hence $v^\top \widehat{A}_{\mathrm{outliers}} v 
    \geq -\eta$.
    
By assumption, $S$ is $\eta$-concentrated (\Cref{it:inliers} of  \Cref{def:deterministic}); thus $|  v^\top \widehat{A}_{\mathrm{inliers}} v - v^\top A_{\mathrm{inliers}} v | \leq \eta $.



Putting everything together, for every unit vector $v$ for which $v^\top \widehat{A} v \leq \eta$, we have that:
\begin{align}
    (1-\alpha) v^\top A_{\mathrm{inliers}} v 
    &= (1-\alpha) v^\top \widehat{A}_{\mathrm{inliers}} v + (1-\alpha) v^\top (A_{\mathrm{inliers}} - \widehat{A}_{\mathrm{inliers}}) v \notag \\
    &= v^\top \widehat{A}  v - \alpha v^\top \widehat{A}_{\mathrm{outliers}} v + (1-\alpha) v^\top (A_{\mathrm{inliers}} - \widehat{A}_{\mathrm{inliers}}) v \notag \\
    &\leq \eta + \alpha \eta + (1-\alpha) \eta \leq 2 \eta \;. \notag
\end{align}
Using that $A_{\mathrm{inliers}} =   \mu \mu^\top e^{-\frac{\|\mu\|^2}{\beta \sqrt{k}+2}}$, the above implies that $(v^\top \mu)^2 \leq 2 \eta \, e^{\frac{\|\mu\|^2}{\beta \sqrt{k}+2}}
    \lesssim \eta$,
where the last inequality is because  $\|\mu\| = O(1)$. 
Finally, since  $\norm{\Proj_{\cU}(\mu)}=\max_{v\in \cU: \norm{v}=1}\abs{v^\top \mu}$,
it follows that $\norm{\Proj_{\cU}(\mu)}\le O( \sqrt{\eta})$. \qedhere
\end{proof}







\subsubsection{Proof of \Cref{thm:main}}
We are now ready to prove our main theorem. The full proof is deferred to \Cref{appendix:fullproof}.
We use the same notation as in the pseudocode provided in \Cref{alg:mean_estimation}:
the $t$-th iteration of the while loop maintains a subspace $\cV_t$, whose dimension, $k$, starts from $d$ and can only decrease from a round to the next one. This while loop has two distinct phases: %
Phase 1 will refer to all the iterations during which $C\log^4 (d)/\eps^5\leq k \leq d$, and Phase 2 will refer to all the iterations with $1/\eps^2 \leq k < C\log^4 (d)/\eps^5$. The analysis of the algorithm consists of the claims stated below. 
The claims are that each of the following holds with high constant probability:
\begin{enumerate}[leftmargin=*]
    \item \textbf{Warm start}: If $\widehat{\mu}_0$ is the estimator from line \ref{line:warm_start} of \Cref{alg:mean_estimation}, then $\| \widehat{\mu}_0 - \mu\| = O(1)$.\label{it:warmstart_main}
   
    \item \textbf{Dimension Reduction}: If $T_1, T_2$ denote the number of iterations of Phase $1$ and Phase $2$ respectively, 
    then $T_1\le \log(d)$, $T_2\le 100\log(\log(d)/\eps)$. 
    Moreover, for all $t=1,\ldots,T_1+T_2$, we have 
    $\| \Proj_{\cV_{t+1}^\perp}(\widehat{\mu}_0 - \mu)\| \lesssim \sum_{t' = 1}^t \sqrt{\eta_{t'}}$, where $\eta_{t'}$ are the values set 
    in lines \ref{line:eta_t_1} and \ref{line:eta_t_2}.
    \label{it:sketch_phase_x_main}
   
    \item \textbf{Estimator for the remaining subspace}: The lines \ref{line:sample_set_end}-\ref{line:brute_force} of \Cref{alg:mean_estimation} find a vector $\widehat{\mu}_1 \in \cV_{t}$ such that  $\|\widehat{\mu}_1 - \Proj_{\cV_t}(\mu)\| \leq \eps$.\label{it:brute_force_sketch_main} 
    
\end{enumerate}
Here we will sketch how each claim can be proved, with the full details in 
\Cref{appendix:fullproof}. We start by showing how the claims imply that 
$\|\widehat{\mu} - \mu\| = O(\eps)$ for 
$\widehat{\mu} := \widehat{\mu}_0 + \widehat{\mu}_1$.\footnote{The 
guarantee of \Cref{thm:main} is $\|\widehat{\mu} - \mu\| \leq \eps$. Here 
we show $O(\eps)$ in order to keep the constants that appear simple. This is 
w.l.o.g. as one can later replace $\eps$ by $\eps/C$ for a large enough 
constant $C$ to obtain \Cref{thm:main}.}





Consider $t=T_1 + T_2$, so that $\cV_t$ denotes the subspace after exiting the while loop.   By decomposing the true mean into the projections onto the two orthogonal subspaces we have that $\mu = \Proj_{\cV_t^\perp}(\mu) + \Proj_{\cV_t}(\mu)$. By the Pythagorean theorem, we can write 
\begin{align*}
    \| \widehat{\mu} - \mu \|^2
    &= \| \Proj_{\cV_{t}^\perp}(\widehat{\mu}_0 - \mu) + \Proj_{\cV_{t}}(\widehat{\mu}_1 - \mu)\|^2
    = \| \Proj_{\cV_{t}^\perp}(\widehat{\mu}_0 - \mu)\|^2 +  \| \Proj_{\cV_{t}}(\widehat{\mu}_1 - \mu)\|^2 . \end{align*}
The last term is $\left\| \Proj_{\cV_{t}}(\widehat{\mu}_1 - \mu)\right\| \leq \eps$ by \Cref{it:brute_force_sketch_main}. It suffices to bound the first term by $O(\eps)$. Towards this end, denote $D:=C\log^4 (d)/\eps^5$, which is the dimension during the first iteration of Phase~2. 
Then,\looseness=-1
\begin{align*}
    &\| \Proj_{\cV_{t}^\perp}(\widehat{\mu}_0 - \mu)\|
    \lesssim\sum_{t' = 1}^{T_1 + T_2} \sqrt{\eta_{t'}} \tag{using  \Cref{it:sketch_phase_x_main}} = \sum_{t' = 1}^{T_1} \sqrt{\eta_{t'}}  + \sum_{t' = T_1 + 1}^{T_1+T_2} \sqrt{\eta_{t'}} \\
    &\leq T_1 \frac{\eps}{ \log d}  + \sum_{t' = T_1 + 1}^{T_1+T_2} \sqrt{\eta_{t'}} 
    \leq \eps  + \sum_{t' = T_1 + 1}^{T_1+T_2} \sqrt{\eta_{t'}} \\
    &\leq  \eps + \left( \sqrt{\frac{36\eps}{\sqrt{D}}} +  \sqrt{\frac{36\eps}{\sqrt{D/2}}} + \cdots + \sqrt{\frac{36\eps}{\sqrt{1/\eps^2}}} \right) 
    \leq \eps  +  \frac{6\sqrt{\eps}}{D^{1/4}} \sum_{i=0}^{\lg(D \eps^2)} 2^{i/4}   \\
    &\le  \eps +  \frac{6\sqrt{\eps}}{(2^{1/4}-1)D^{1/4}} 2^{\frac{\lg(D \eps^2)}{4}} 
    =  \eps +  \frac{6\sqrt{\eps}}{(2^{1/4}-1)D^{1/4}} (D \eps^2)^{1/4}\lesssim \eps \;, 
\end{align*}
where we used the definition of $\eta_{t'}$ from lines \ref{line:eta_t_1},\ref{line:eta_t_2}, and direct calculations 
for the series ($\lg(\cdot)$ denotes the logarithm with base $2$). 

Finally, we discuss briefly the proofs of the claims in \Cref{it:warmstart_main,it:sketch_phase_x_main,it:brute_force_sketch_main}. 
The first and the last follow immediately by \cite{diakonikolas2023algorithmic} (Corollary $2.12$  and Exercise $2.10$) and \Cref{thm:brute_force} respectively. 
Regarding \Cref{it:sketch_phase_x_main}, let us first consider Phase 1 
(iterations during which $k \geq C\log^4 (d)/\eps^5$). By an application of 
\Cref{lem:sample_complexity} with $\eta=\eta_t=(\eps/\log d)^2$ and a union 
bound, all the sets $T_t$ drawn in line \ref{line:transform_set} will be 
($\eta_t,\sqrt{\log k}$)-good with high constant probability. Note that 
because of the warm start in line \ref{line:warm_start} and the 
transformation subtracting $\widehat{\mu}_0$ in \Cref{line:transform_set}, 
every sample essentially comes from the mean-shift model with mean $O(1)$, 
which makes \Cref{lem:sample_complexity} applicable. The number of 
eigenvectors with eigenvalue larger than $\eta_t$ is at most 
$\tr(\widehat{A}_t)/\eta_t \leq 18 \sqrt{k \log k}/\eta_t = O((\log d)^2 \eps^{-2} \sqrt{\log k}) \leq O(\log^{2.5}(d) \eps^{-2})$, 
where we first used the definition of  ($\eta_t,\sqrt{\log k}$)-goodness (\Cref{def:deterministic}) and then that $\eta_t = (\eps/\log d)^2$. 
Thus, as long as $k > C \log^5(d)/\eps^4$, the dimension during 
each round gets halved. After $T_1 = O(\log d)$ such iterations, 
the dimension reaches $C \log^5(d)/\eps^4$, 
after which the aforementioned no longer guarantees 
that it will continue to drop. This is the reason why we need 
to change the value of $\beta$ to $\eps$, 
and set $\eta_t:= 36 \eps/\sqrt{k}$ in \Cref{line:eta_t_2} 
for the remaining iterations (Phase 2). 
The argument for Phase 2 is similar, but uses different parameters: 
our datasets are now $(\eta_t,\eps)$-good by \Cref{lem:sample_complexity2}, and the number of eigenvectors larger than $\eta_t$ is 
now at most $\tr(\widehat{A}_t)/\eta_t \leq 18 \beta\sqrt{k}/\eta_t= k/2$. 
Thus, Phase 2 will continue to halve the dimension.  
Finally, the claim that $\| \Proj_{\cV_{t+1}^\perp}(\widehat{\mu}_0 - \mu)\| \lesssim \sum_{t' = 1}^t \sqrt{\eta_{t'}}$ 
follows immediately from \Cref{lemma:dimerror}.

\paragraph{Runtime and Sample Complexity of \Cref{alg:mean_estimation}}
It can be readily verified that the number of samples 
$n_0,n_1,n_2$ defined in \Cref{line:samples} of the algorithm 
suffice for the aforementioned applications of \Cref{lem:sample_complexity,lem:sample_complexity2}.
Thus, the overall sample complexity of the algorithm is
    $n = n_0 + n_1 \cdot T_1 + n_2\cdot (T_2 + 1)
     = O(d) + d \polylog (d)\eps^{-(2+o(1))}  +  2^{O(1/\eps^2)} \polylog(d) $. 
Regarding runtime, the most computationally intensive part 
is the last step of the algorithm (\Cref{line:brute_force}), 
which has runtime $\tau = 2^{O(k)}\poly(n_2, d)$, 
where the $k = 1/\eps^2$ here denotes the dimension 
of the final subspace. 
Since $n_2 = 2^{\Theta(1/\eps^2)}\polylog(d)$, 
that runtime is sample-polynomial, i.e., $\poly(n_2, d)$. 
It is easy to check that the other parts of the algorithm are also polynomial in the size of the input.

This completes the proof of \Cref{thm:main}. \qed 




\section{Conclusions and Open Problems}\label{sec:open-problems}
In this paper, we provide the first polynomial-time 
algorithm for high-dimensional mean estimation under mean-shift 
outliers. In this model, the outliers are not completely 
adversarial as they include a randomized component. This 
additional structure allows for consistent estimation, which is 
unattainable in purely adversarial models. 

Our work takes a first algorithmic step in understanding the 
complexity of high-dimensional estimation 
for the known covariance mean-shift model (\Cref{def:cont}).
A number of concrete open problems and broader 
directions suggest themselves. Concretely, is there a 
computationally efficient robust learner for the unknown covariance Gaussian case? Can we design faster 
(aka near-linear time) learning algorithms for the Gaussian mean 
case? What is the complexity of robust mean estimation for the
known covariance case when the underlying distribution is not 
Gaussian? Perhaps surprisingly, this question is not understood 
even information-theoretically for broader families of 
distributions. We hope that this work will inspire further 
algorithmic progress in robust estimation under mean shift 
contamination and related structured contamination models. 




\printbibliography

\newpage
\appendix

\section*{Appendix}
The appendix is structured as follows:
First, \Cref{sec:related_work} includes a more detailed 
summary of related work.
\Cref{appendix:prelim} includes additional preliminaries required in subsequent technical sections. \Cref{appendix:omited_main} gives the correctness analysis of 
\Cref{thm:brute_force}. \Cref{appendix:sample} contains the proofs 
of the technical lemmas involving  
the reweighted matrix used in \Cref{sec:matrix}.
\Cref{appendix:fullproof} contains the full proof of our 
main result (\Cref{thm:main}). Finally, \Cref{appendix:breakdownpoint} 
establishes  \Cref{thm:higher_breakdown}, which demonstrates 
adaptivity to unknown contamination parameter $\alpha$ 
arbitrarily close to $1/2$. 

\section{Additional Related Work} \label{sec:related_work}




\paragraph{Additional Related Work on Mean Shift Contamination}
The most closely related work to ours is 
\cite{kotekal2024optimal}. They study the model of 
\Cref{def:cont} in one dimension and derive matching 
information theoretic upper and lower bounds for mean 
estimation. \cite{kotekal2024optimal} also consider 
estimating the variance in the case when it is unknown 
to the algorithm. A similar upper bound for mean estimation in 
the known variance case was given in \cite{li2023robust}. In 
even earlier work, \cite{carpentier2021estimating} studied 
the sample complexity of robust mean estimation for the special 
case of \Cref{def:cont} in one dimension where $z_i - \mu > 0$.

More broadly, the problem of robust mean estimation with mean shift outliers has its roots in influential work by Efron \cite{efron2004large,efron2007correlation,efron2008microarrays} 
in the context of multiple hypothesis testing. In these works,  
Efron noticed through empirical evidence that, because the 
parameters of the null distribution are unknown, testing should 
be done in two stages: (i) estimation of the null parameters, 
and (ii) testing of null vs alternative hypothesis using 
standard multiple testing procedures. Although  the focus of 
our paper theoretical, we refer to the discussions in 
\cite{carpentier2021estimating,kotekal2024optimal}, the CIRM 
talk in \cite{chaogaotalk} and the references therein, for a 
discussion of the connection between the mean-shift noise model 
and Efron's work. 

\paragraph{Unknown Variance/Covariance}
We highlight two points regarding the variance of samples in 
the mean-shift model. First, the variance may be unknown to the 
algorithm, unlike the known covariance case in 
\Cref{def:cont}. Specifically, consider a variant of the model 
(in one dimension for simplicity) where inliers are drawn from 
$\mathcal{N}(\mu, \sigma^2)$ and outliers from 
$\mathcal{N}(z_i, \sigma^2)$. This setting has been studied in 
prior work (see, e.g., \cite{cai2010optimal, 
carpentier2021estimating, kotekal2024optimal}). Notably, the 
last two works first analyze the case where the variance is 
known (and equal to one), and then extend their analysis 
to the more general setting where it is unknown. 
Even when the goal is solely mean estimation, their approach 
requires variance estimation as a first step. \cite{li2023robust} considers only the known variance case. 


The second point concerns what happens to the optimal error if outliers have different variances than inliers.
Concretely, consider the model where inliers are drawn from 
$\mathcal{N}(\mu, \sigma^2)$ and outliers from 
$\mathcal{N}(z_i, \sigma_i^2)$. If outliers can have smaller variance than the inliers ($\sigma_i < \sigma$), then 
estimation is intrinsically harder than in the case where 
$\sigma_i \geq \sigma$. The reason is that estimation under 
Huber contamination can be reduced to estimation in the mean-
shift model with $\sigma_i < \sigma$. 
Specifically, consider the Huber contaminated sample with 
inliers that follow $\cN(\mu,\sigma^2)$ and outliers $z_i$. By 
adding $\cN(0,\alpha^2)$ noise (with $\alpha>0$) to both 
inliers and outliers, we have that the inliers after the 
transformation follow $\cN(\mu, \sigma^2+\alpha^2)$ and the 
outliers follow $\cN(z_i,\alpha^2)$, which is an instance of 
mean-shift contamination with $\sigma_i<\sigma$. 
Consequently, consistent estimation is not possible 
in the mean-shift model where outliers have smaller variance 
than the inliers. While there exists experimental work in the 
literature \cite{cai2009simultaneous} for both the cases of
smaller and larger variances, 
provable consistent estimation is impossible in such cases.

 

Finally, we note that in the setting where 
$\sigma_i\ge \sigma$, we can assume without loss of generality 
that all $\sigma_i$ are exactly equal to $\sigma$.
To see why, consider the random variable for a single outlier sample: $x_i=z_i+\cN(0,\sigma_i^2)$. 
Observe that we can rewrite \(x_i\) as 
$x_i=(z_i+\cN(0,\sigma_i^2-\sigma^2))+\cN(0,\sigma^2)$. 
Hence, by treating the outlier centers themselves 
as random variables, one can run an algorithm 
that solves the mean estimation problem 
in the mean-shift model with unknown variance.
The same argument applies in higher dimensions, 
in the setting where $\Sigma_i \;\succeq\; \Sigma$.
Let $\Sigma,\Sigma_i$ be PSD matrices with 
$\Sigma_i \;\succeq\; \Sigma$.
Then we can rewrite an outlier sample of the form 
$x_i=z_i+\cN(0,\Sigma_i)$ as 
$x_i=(z_i+\cN(0,\Sigma_i-\Sigma))+\cN(0,\Sigma)$, 
since $\Sigma_i-\Sigma$ is PSD and therefore a valid covariance matrix.



\paragraph{Mean Shift Models for Other Problems}
The concept of more structured corruptions in the form of mean shifts has also been studied in the regression setting \cite{sardy2001robust,gannaz2007robust,mccann2007robust,she2011outlier}. In this model, it is assumed that 
$y_i = \beta^\top x_i + \gamma_i + \xi_i$, where $\gamma_i$ are adversarial mean shifts on top of the standard Gaussian additive noise $\xi_i \sim \cN(0,\sigma^2)$.


\paragraph{Comparison with \cite{LaiRV16}}
The approach of dimension reduction until a low-dimensional  
inefficient estimator can be employed has also appeared in previous 
robust statistics work \cite{LaiRV16}, however that similarity is rather superficial and the underlying techniques are significantly different. Each iteration of dimension reduction in \cite{LaiRV16} uses the centered second moment matrix to identify a sizable ``good'' subspace (spanned by the bottom eigenvectors) where the empirical mean achieves $O(\alpha)$ accuracy, and then it iterates on the remaining small subspace. Even in our more relaxed outlier model, the accuracy within the good subspace might  be as bad as $\Omega(\alpha)$, which is insufficient for our purposes since we aim for $\eps$-error. Thus our \Cref{thm:main} cannot be obtained by the technique of \cite{LaiRV16} and we instead we have to look at the non-centered moment matrix after appropriate reweighting, as described in \Cref{sec:techniques}.


We conclude this section with two additional points of comparison.

\paragraph{Connection to Mixture Models} The mean-shift model is 
related to the classical task of parameter learning for mixture 
models, albeit in a regime that is qualitatively different from 
the one commonly studied. In the canonical setting 
(see~\cite{Das99,AroKan01,AchMcs05,KanSV05} for classic 
references 
and~\cite{belkin2015polynomial,moitra2010settling,ChaSV17,HopLi18,KotSS18,DiaKS18-list,kong2020meta,DHKK20, BDHKKK20, DiakonikolasKKP22, DiaKKLT22-cluster,BakDJKKV22,LiuLi22,DiaKKLT22-cluster,diakonikolas2023sq, DiakonikolasKLP23, DK24,DKLP25} for more recent work), 
it is typically assumed that there is a small (constant) 
number of components, $k \ll n$, each with a distinct mean. 
In contrast, in the mean-shift model, all inlier samples 
come from the same component, while each outlier is drawn from 
its own component. Consequently, parameter estimation of the 
outlier means is information-theoretically impossible in the 
mean-shift model. 

\paragraph{Connection to Entangled Mean Estimation} Another related contamination model in the context of mean estimation is the heteroskedasticity model. 
In heteroskedastic mean estimation, 
each datapoint is drawn independently from a potentially different distribution within a (known) family that shares a \emph{common} mean. Such distributions are also referred to as \emph{entangled}. For the Gaussian family, this model involves each sample having potentially different covariance \cite{ChiDKL14,PenJL19,PenJL19-isit,pensia_estimating_2021,xia2019non,yuan2020learning,LiaYua20,DevLLZ23,compton2024near,diakonikolas2025entangled}. This contamination model can also be viewed as a Gaussian mixture model. However, here each sample originates from its own component ($k = N$). Importantly, the shared mean assumption enables meaningful results despite the large number of components.

\section{Additional Preliminaries}\label{appendix:prelim}

\paragraph{Cover set of the unit sphere} For some of our proofs, we will need the following standard facts about cover sets of the unit sphere:

\begin{fact}[see, e.g., Corollary 4.2.13 in \cite{Ver18}]\label{fact:cover_cor_ver}
    Let $\xi>0$. There exists a set $\cC$ of unit vectors of $\R^d$ such that $|\cC| < (1+2/\xi)^d$ and for every $u \in \R^{d}$ with $\|u\|=1$ it holds $\min_{y \in \cC} \|y-u\| \leq \xi$.
\end{fact}

\begin{corollary}[see, e.g., Exercise 4.4.3 (b) in \cite{Ver18}]\label{fact:cover}
    There exists a subset $\cC$ of the $d$-dimensional unit ball with $\abs{\cC}\leq 7^{d}$  such that   $\norm{x} \le 2\max_{v\in \cC}\abs{v^\top x}$ for all $x\in \mathbb{R}^d$ and  $\|A\|_{\op} \leq 3 \max_{x \in \cC} x^\top A x$ for every symmetric $A \in \R^{d \times d}$.
\end{corollary}



\noindent We will use the notions of subgaussian and subexponential random variables in the following standard way that we briefly review below.

\paragraph{Subgaussian and subexponential random variables} The following three statements are equivalent: (i) the random variable $x-\E[x]$ is subgaussian with ``variance proxy'' $\sigma^2$ (ii) $\|x-\E[x]\|_{L_p} \lesssim \sigma \sqrt{p}$ for all $p\geq 1$ and (iii) $\Pr[|x-\E[x]| > t]\leq e^{-\Omega(t^2/\sigma^2)}$. The following three are also equivalent: (i) the random variable $x-\E[x]$ is subexponential with parameter $\lambda$ (ii) $\|x-\E[x]\|_{L_p} \lesssim p \lambda$ for all $p\geq 1$ and (iii) $\Pr[|x-\E[x]| > t]\leq e^{-\Omega(t/\lambda)}$.  If $X_1,\ldots,X_n$ are zero-mean independent subexponential random variables with parameter $\lambda$, and $\bar{X}=\sum_{i \in [n]}X_i/n$ then $\Pr[|\bar{X}| > t] \leq \exp(-\Omega(n)\min(t^2/\lambda^2,t/\lambda))$ (Bernstein's inequality).

\begin{fact}[see, e.g., Exercise 2.5.1 in \cite{Ver18}]\label{fact:gaussian_moments}
    If $X\sim N(0,1)$ then for any $p\geq 1$, $\norm{X}_{L_p}=(\E[\abs{X}^p])^\frac{1}{p}=\sqrt{2}\left[\frac{\Gamma((1+p)/2)}{\Gamma(1/2)}\right]^{1/p}$, and thus by Stirling's approximation $\norm{X}_{L_p}\lesssim \sqrt{p/e}$.
\end{fact}





\begin{fact}[Matrix Bernstein Inequality; see, e.g., Theorem 5.4.1 in \cite{Ver18} ]\label{fact:matrixBernstein}
   Let $X_1, \ldots, X_N$ be independent, mean zero, $n \times n$ symmetric random matrices, such that $\left\|X_i\right\|_{\op} \leq K$ almost surely for all $i$. Then, for every $t \geq 0$, we have
$$
\mathbb{P}\left[ \left\|\sum_{i=1}^N X_i\right\|_{\op} \geq t\right] \leq 2 n \exp \left(-c \cdot \min \left(\frac{t^2}{\sigma^2}, \frac{t}{K}\right)\right) ,
$$
where $\sigma^2=\left\|\sum_{i=1}^N \mathbb{E} [X_i^2] \right\|_{\op}$ is the operator norm of the matrix variance of the sum.
\end{fact}


The following is a standard fact regarding concentration of norms for Gaussian vectors (see, e.g., \cite{Ver18} for a version of the fact for zero-mean Gaussians). For completeness, we provide a proof below.
\begin{fact}[Gaussian Norm Concentration]\label{fact:normConcetration}
    If $x\sim \cN\left(\mu,I\right)$, with probability $1-\tau$ we have that 
    \begin{align*}
        \left| \Norm{x}^2 -\left(\Norm{\mu}^2 +d\right) \right| \lesssim \log \frac{1}{\tau}+ \left(\sqrt{d}+\Norm{\mu}\right) \sqrt{\log \frac{1}{\tau}} \;.
    \end{align*}
\end{fact}
\begin{proof}
    Write $x=z+\mu$ for $z\sim \cN\left(0,I\right)$ then 
    \begin{align*}
        \Abs{\Norm{z+\mu}^2-\Norm{\mu}^2-d}&=\Abs{\Norm{z}^2-d+2z^\top\mu}\\
        &\le\Abs{\Norm{z}^2-d}+2\Abs{z^\top\mu}
    \end{align*}
    First, using Bernstein's inequality, we have that
    \begin{align*}
        \Abs{\Norm{z}^2-d}\lesssim \log\frac{1}{\tau}+\sqrt{d\log \frac{1}{\tau}}.
    \end{align*}
     with probability at least $1-\tau$.
    Also as $z^\top\mu\sim \cN\left(0,\Norm{\mu}^2\right)$ it satisfies the Gaussian tails
    \begin{align*}
        \Pr\left[\Abs{z^\top\mu}>t\right]\le e^{-\Omega(\frac{t^2}{ \Norm{\mu}^2})} \;,
    \end{align*}
    or equivalently $\Abs{z^\top\mu}\lesssim \Norm{\mu}\sqrt{\log\frac{1}{\tau}}$ with probability $1-\tau$.
\end{proof}


\section{Omitted Proofs from \Cref{sec:brute-force}}\label{appendix:omited_main}

We restate and prove the following result.

\BRUTEFORCE*

\begin{proof}[Proof of \Cref{thm:brute_force}]
Denote by $T=\{x_i\}_{i=1}^n, x_i\in \mathbb{R}^d$ an $\alpha$-corrupted set of points from $\cN(\mu,I)$ under the model of \Cref{def:cont} and denote by $\cC$ the the cover set of \Cref{fact:cover}.
    The algorithm is the following: First, using the algorithm from \Cref{fact:onedim},  calculate a $m_v$ for each $v \in \cC$  such that $\abs{m_v-v^\top \mu}\le \varepsilon/8$ (see next paragraph for more details on this step). Then, output the solution of the following linear program (note that the program always has a solution, as it is satisfied by $\widehat{\mu}=\mu$):
    \begin{align*}
        &\text{ Find }\widehat{\mu}\in \mathbb{R}^d \text{ s.t.}\\
        &\abs{v\cdot\widehat{\mu}-m_v}\le \varepsilon/4, \forall v\in \mathcal{C} \;.
    \end{align*}
    The claim is that this solution $\widehat{\mu}$ is indeed close to the target $\mu$, since 
   \begin{align}
        \norm{\mu-\widehat{\mu}} &\leq 2\max_{v\in \mathcal{C}}\abs{v^\top(\mu-\widehat{\mu})}  \tag{using \Cref{fact:cover}}\\
        &\le 2\max_{v\in \mathcal{C}}(\abs{v^\top \mu -m_v}+\abs{m_v -v^\top\widehat{\mu} }) \notag\\
        &\le 2(\eps/8 + \eps/4) < \eps \;. \label{eq:temp4234}
    \end{align}
    We now explain how to obtain the approximations $m_v$ with the guarantee $\abs{m_v-v^\top \mu}\le \varepsilon/8$.
    Fixing a direction $v \in \mathcal{C}$, we note that  $v^\top x\sim \cN(v^\top \mu,1)$ thus $\{v^\top x_i\}_{i=1}^m$ is a set of $\alpha$-corrupted samples  of $ \cN(v^\top \mu,1)$. Thus, if we apply algorithm  from \Cref{fact:onedim} with probability of failure $\delta' = \delta/|\cC|$, the event $\abs{m_v-v^\top \mu}\le \varepsilon/8$ will hold with probability at least $1-\delta/|\mathcal{C}|$. By union bound, the probability all the events for $v \in \cC$  hold simultaneously is at least $1-\delta$. The number of samples for this application of \Cref{fact:onedim} is $2^{O(1/\eps^2)}\log(1/\delta') = 2^{O(1/\eps^2)}\log(|\cC|/\delta) = 2^{O(1/\eps^2)}(d+ \log(1/\delta))$. 
    
    We conclude with the runtime analysis.
    The runtime to find the $m_v$'s is $O(|\cC|\poly(nd)) = 2^{O(d)}\poly(nd)$ since for each fixed $v \in \cC$ we need $\poly(nd)$ time to calculate the projection $\{x_i^\top v\}$ of our dataset onto  $v$ and $\poly(n)$ time to run the one-dimensional estimator.
    The linear program can be solved using the ellipsoid algorithm. Consider the separation oracle that exhaustively checks all $2^{O(d)}$ constraints. We need $\poly(d)\log(\frac{R}{r})$ calls to that separation oracle, where $R,r$ are the radii of the bounding spheres of the feasible region. First, $R\leq \eps$, because we have already shown in \eqref{eq:temp4234}  that the feasible set belongs in a ball of radius $\eps$ around $\mu$. Regarding the upper bound $r$, note that all $\widehat{\mu}$ inside a ball of radius $\varepsilon/8$ around $\mu$ are feasible since $\abs{v^\top\widehat{\mu}-m_v}\le \abs{v^\top\widehat{\mu}-v^\top \mu}+\abs{v^\top\mu-m_v}\le \norm{\widehat{\mu}-\mu}+ \varepsilon/8 \leq \eps/4$. This means that $r=\eps/4$. Hence the total runtime for solving the LP is $2^{O(d)} \poly(d)$ or simply $2^{O(d)}$. 
    
    
\end{proof}

\section{Omitted Proofs from \Cref{sec:matrix}}
\label{appendix:sample}
In this section, we restate and prove \Cref{lemma:expression,lem:sample_complexity,lem:sample_complexity2}.

\Expression*
\begin{proof}
We prove the lemma for $\cV = \R^{k}$ which has $\Pi_{\cV} = I$. The same proof generalizes to arbitrary subspaces.
First, for any function $f:\mathbb{R}^k\to \mathbb{R}^{k\times k}$, we have that
    \begin{align}
    \E_{x \sim \cN(z,I)}\left[f(x)e^{-\frac{\norm{x}^2}{\beta\sqrt{k}}} \right]&=\frac{1}{Z}\int_{\mathbb{R}^k}f(x)e^{-\frac{\norm{x}^2}{\beta\sqrt{k}}}e^{-\frac{\norm{x-z}^2}{2}}dx  \tag{where $Z := (2\pi)^{k/2}$} \\
    &=\frac{1}{Z}\int_{\mathbb{R}^k}f(x)e^{-\frac{\norm{x}^2}{\beta\sqrt{k}} -\frac{\norm{x-z}^2}{2}}dx \notag \\
    &=\frac{1}{Z}\int_{\mathbb{R}^k}f(x)e^{-\frac{1}{2}(\norm{x}^2(\frac{2}{\beta\sqrt{k}}+1)-2z^\top x+\norm{z}^2)}dx \notag \\
    &=\frac{1}{Z}\int_{\mathbb{R}^k}f(x)e^{-\frac{1}{2}(\norm{x}^2c-2z^\top x+\norm{z}^2)}dx  \tag{where $c:=\frac{2}{\beta\sqrt{k}}+1$} \\
    &=\frac{1}{Z}\int_{\mathbb{R}^k}f(x)e^{-\frac{c}{2}(\norm{x}^2-\frac{2}{c}z^\top x+\frac{1}{c}\norm{z}^2)}dx \notag \\
    &=\frac{1}{Z}\int_{\mathbb{R}^k}f(x)e^{-\frac{c}{2}\left(\norm{x-\frac{1}{c}z}^2 +(\frac{1}{c}-\frac{1}{c^2})\norm{z}^2\right)}dx \notag \\
    &=\frac{e^{-\frac{1}{2}(1-\frac{1}{c})\norm{z}^2 } }{Z}\int_{\mathbb{R}^k}f(x)e^{-\frac{c}{2}\norm{x-\frac{1}{c}z}^2 }dx \notag \\
     &=\frac{e^{-\frac{\norm{z}^2}{\beta\sqrt{k}+2} } }{Z}\int_{\mathbb{R}^k}f(x)e^{-\frac{c}{2}\norm{x-\frac{1}{c}z}^2 }dx \notag \\
     &=\frac{e^{-\frac{\norm{z}^2}{\beta\sqrt{k}+2} }Z' }{Z}\E_{x\sim \cN(\frac{1}{c}z,\frac{1}{c}I)}[f(x)] \tag{where $Z':=(2\pi)^{k/2}/c^{k/2}$} \\
     &=b\E_{x\sim \cN(\frac{1}{c}z,\frac{1}{c}I)}[f(x)]\;. \tag{where $b:=e^{-\frac{\norm{z}^2}{\beta\sqrt{k}+2}  }\paren{1/c}^{k/2}$}  
\end{align}
Therefore, for $f(x)=xx^\top -(1/c)I$, we have that
\begin{align*}
      \E_{x \sim \cN(z,I)}\left[\left(xx^\top -\frac{1}{c}I\right)e^{-\frac{\norm{x}^2}{\beta\sqrt{k}}}\right]
      &= \E_{x \sim \cN(z,I)}\left[f(x)e^{-\frac{\norm{x}^2}{\beta\sqrt{k}}}\right]
      = b  \E_{x\sim \cN(\frac{1}{c}z,\frac{1}{c}I)}[f(x)]\\
      &=b\left(\E_{x\sim \cN(\frac{1}{c}z,\frac{1}{c}I)}[xx^\top ] -\frac{1}{c}I\right) 
      =b\left( \frac{1}{c}I+\frac{1}{c^2}zz^\top - \frac{1}{c}I\right) \\
      &= b \left(\frac{1}{c^2}zz^\top  \right)=e^{-\frac{\norm{z}^2}{\beta\sqrt{k}+2}  }\left(\frac{1}{1 + \frac{2}{\beta \sqrt{k} }  }\right)^{\frac{k}{2}+2}zz^\top\;. 
\end{align*}
\end{proof}




\SAMPLES*
\begin{proof}  We will show that \Cref{it:outliers} and \Cref{it:inliers} of \Cref{def:deterministic} hold. Without loss of generality, we use $\cV=\R^k$ in \Cref{def:deterministic}, i.e., our subspace is the entire $\R^k$. We start with \Cref{it:outliers} since it is more general. The other will be similar.

 \item
    \paragraph{Proof of \Cref{it:outliers}}

    We remind the reader that $T=\{x_i,\dots, x_{\abs{T}}\}$ is the set are the $\alpha$-corrupted points and that $T\setminus S$ denotes the subset of $\alpha n$ outliers with $z_i$ the associated center of $x_i\in T\setminus S$. 
    We want to show that the sample average over $T \setminus S$ matrix $\widehat{A}$ from \eqref{eq:matrixA}, with $\beta=\sqrt{\log(k)}$, satisfies $v^\top \widehat{A} v > -\eta$ for all unit vectors $v$. Without loss of generality, instead of the definition of $\widehat{A}$ given in  \eqref{eq:matrixA} we will use the following definition:
    \begin{align}
        \widehat{A} = \frac{1}{|T \setminus S|} \sum_{i:x_i \in T\setminus S} \widehat{A}_{i} \quad \text{where} \;\; \widehat{A}_i = F_{\beta,k}(x_i)e^{\sqrt{k/\log(k)}} \1(\cE_i)\;,
    \end{align}
    where $F_{\beta,k}$ as in \Cref{eq:matrixA} and $\cE_i$ is defined to be the following \emph{good event}:
    \begin{align}\label{eq:goodevent}
         \left| \Norm{x_i}^2 -\left(\Norm{z_i}^2 +k\right) \right| \lesssim \log \frac{1}{\tau}+ \left(\sqrt{k}+\Norm{z_i}\right) \sqrt{\log \frac{1}{\tau}} \;,
    \end{align}
    where $\tau := (1/n)^4$.
    The first change in our definition is that we replaced the normalization factor $Z_{\beta,k}$ that appeared in \eqref{eq:matrixA} by the simpler expression $e^{\sqrt{k/\log(k)}}$. This is because $ (1+{2}/\paren{\sqrt{k\log(k)}})^{k/2+2} = \Theta( e^{\sqrt{k/\log(k)}})$, and using $e^{\sqrt{k/\log(k)}}$ will be more convenient for our calculations later on. Without loss of generality we can use $\Pi_{\cV}=I_k$ as we can all of the statements that we aim to prove are similarity invariant. 
    The second change is that  we are using the good event in the definition of $\widehat{A}$. This is indeed without loss of generality because (i) by
    \Cref{fact:normConcetration} we know that $\cE_i$ holds for each sample $x_i$ with probability $1-\tau$, thus by a union bound over all samples we have that all the events $\cE_i$ hold simultaneously with probability at least $1-n \tau \geq 1-1/n^3>1-\delta/8$ (for $k\ge 2$), and (ii) using the indicator $\1(\cE_i)$ in the definition of $\widehat{A}$ shifts the expected value of $\widehat{A}$ by a negligible amount (much smaller than $\eta$) as we show below:
    
    \begin{align}
       &\Abs{v^\top  \E_{x_i \sim \cN(z_i,I)}\left[F_{\beta,k}(x_i)e^{\sqrt{k/\log(k)}} \1(\cE_i) - F_{\beta,k}(x_i)e^{\sqrt{k/\log(k)}} \right]  v } \notag \\
       &= \Abs{v^{\top} \E_{x_i \sim \cN(z_i,I)}\left[  \left(x_i x_i^\top - \frac{\sqrt{k\log(k)}}{\sqrt{k\log(k)}+2} I  \right)e^{-\frac{\|x_i\|^2-k}{\sqrt{k\log(k)}}}\Ind\left(\bar{\cE_i}\right)\right]v } \notag \\
        &\leq \E_{x_i \sim \cN(z_i,I)}\left[  \Abs{ \left( v^\top x_i \right)^2 - \frac{\sqrt{k\log(k)}}{\sqrt{k\log(k)}+2}  }e^{-\frac{\|x_i\|^2-k}{\sqrt{k\log(k)}}}\Ind\left(\bar{\cE_i}\right)\right] \notag \\
        &\le \sqrt{\Pr_{x_i \sim \cN(z_i,I)}[\bar{\cE_i}]}\sqrt{\E_{x_i \sim \cN(z_i,I)}\left[\left( \left( \left( v^\top x_i \right)^2 - \frac{\sqrt{k\log(k)}}{\sqrt{k\log(k)}+2}  \right)e^{-\frac{\|x_i\|^2-k}{\sqrt{k\log(k)}}}\right)^2\right]}  \tag{using Cauchy-Schwarz}\\
        &\le \left(\frac{1}{n}\right)^2 \sqrt{ \E_{x_i \sim \cN(z_i,I)}\left[ \left(\left( \left( v^\top x_i \right)^2 - \frac{\sqrt{k\log(k)}}{\sqrt{k\log(k)}+2}  \right)e^{-\frac{\|x_i\|^2-k}{\sqrt{k\log(k)}}}\right)^2\right]}  \tag{using \Cref{fact:normConcetration} with $\tau=(1/n)^4$}\\
        &\lesssim \left(\frac{1}{n}\right)^2  \sqrt{k}\log(k)e^{2/\log^2(k)}  \;. \tag{using \Cref{cl:var_bound} applied with $\beta=\sqrt{\log(k)}$}\\
        & \le \eta/2\;,  \label{eq:expectation_shift}
    \end{align}
    where \Cref{cl:var_bound} that was used above is a bound on the reweighted second moment that can be found in \Cref{sec:supporting_claims}.
The above is considering only a single term in the definition of $\widehat{A}$. By triangle inequality it also follows that $\Norm{\E\left[\widehat{A}\right]-A}\le \eta/2$, where $A = \E \left[\frac{1}{|T\setminus S|} \sum_{i : x_i \in T \setminus S}F_{\beta,k}(x_i)e^{\sqrt{k/\log(k)}} \1(\cE_i)\right]$.
    
    We now move to show that $v^\top \widehat{A} v > -\eta$ with high probability.
    Write $x_i= z_i+g_i$ where $ g_i\sim \cN(0,I) $ for $i \in T\setminus S$.  We decompose the matrix $\widehat{A}_i$ for $i : x_i \in T\setminus S$ into  three terms 
    \begin{align*}
        \widehat{A}_i&= \left(\left(g_ig_i^\top  - \frac{\sqrt{k\log(k)}}{\sqrt{k\log(k)}+2} I  \right)+(z_ig_i^\top +g_i z_i^\top) +z_iz_i^\top \right)e^{-\frac{\|x_i\|^2-k}{\sqrt{k\log(k)}}}\Ind\left(\cE_i\right)\\
        &=\vcentcolon \widehat{T}_{1i}+\widehat{T}_{2i}+\widehat{T}_{3i}\;,
    \end{align*}
    
where  by $\widehat{T}_{1i}, \widehat{T}_{2i} $ and $\widehat{T}_{3i}$ we denote each of the terms that sum to $\widehat{A}_i$ and by  $T_{1i}, T_{2i}$ and $ T_{3i}$ we denote their corresponding expectations.  We will show concentration for each of the terms separately and then  combine the results to show that $v^\top \widehat{A} v > -\eta$. 
\item  \paragraph{First Term }
Fix $v\in \mathbb{R}^k:\norm{v}=1$ and let $i$ such that $x_i\in T\setminus S$. We bound the $L_p$ norm of the random variable $v^\top \left(\widehat{T}_{1i}- T_{1i}\right)v$ as follows:
     \begin{align}
      &\left\| v^\top \left(\widehat{T}_{1i}- T_{1i}\right)v\right\|_{L_p} \lesssim \Norm{v^\top \widehat{T}_{1i}v}_{L_p} \tag{by triangle inequality and Jensen's inequality}\\
      &=\Norm{   \left( \left(g_i^\top v\right)^2  - \frac{\sqrt{k\log(k)}}{\sqrt{k\log(k)}+2}   \right)e^{-\frac{\|x_i\|^2-k}{\sqrt{k\log(k)}}}\Ind(\cE_i)}_{L_p} \notag\\
      & \le  \Norm{   \left( \left(g_i^\top v\right)^2  - \frac{\sqrt{k\log(k)}}{\sqrt{k\log(k)}+2}   \right)\Ind\left( \cE_i \right)}_{L_p} (1/\eta)^{o(1)} e^{  -\frac{\|z_i\|^2}{\sqrt{k\log(k)}} + O \left(\frac{\|z_i\| \sqrt{\log (k/\eta)}}{\sqrt{k\log(k)}}\right) }  \tag{by \Cref{cl:center_norm_bound}}\\
        &\le \left(\Norm{    \left(g_i^\top v\right)^2}_{L_p}+  \frac{\sqrt{k\log(k)}}{\sqrt{k\log(k)}+2} \right) (1/\eta)^{o(1)} e^{ O ( \frac{\log (k/\eta)}{\sqrt{k\log(k)}}) } \tag{by \Cref{cl:high_school}}\\
        &\lesssim \left(p+  \frac{\sqrt{k\log(k)}}{\sqrt{k\log(k)}+2} \right) (1/\eta)^{o(1)}   \tag{by \Cref{fact:gaussian_moments} since $g_i^\top v\sim N(0,1)$} \\
        &\lesssim p \cdot (1/\eta)^{o(1)} \;,  \label{eq:lpnorm-bound}
    \end{align}  
     where the third line above uses the definition of the good event $\cE_i$. The proof follows by the definition of the events $\cE_i$ and some simple algebra. We include the proofs of both  claims in \Cref{sec:supporting_claims}.
        \item
    \begin{restatable}{claim}{REWEIGHTING}\label{cl:center_norm_bound}
        Let $\cE_i$ denote the event from \eqref{eq:goodevent}. Then, 
        \begin{align*}
            \exp\left( -\frac{\|x_i\|^2-k}{\sqrt{k\log(k)}} \right) \1( \cE_i) \leq \exp\left( -\frac{\|z_i\|^2}{\sqrt{k\log(k)}} + O \left(\frac{\|z_i\| \sqrt{\log (k/\eta)}}{\sqrt{k\log(k)}}\right) \right)  (1/\eta)^{o(1)} \;.
        \end{align*}
    \end{restatable}
        
    \begin{restatable}{claim}{HIGHSCHOOL}\label{cl:high_school}
    Fix a $k>10, 0<\eta <1$ and  $C>8$. The following inequalities hold:  
    \begin{enumerate}[label=\roman*.]
        \item $-\frac{x^2}{\sqrt{k\log(k)}} + C\frac{x \sqrt{\log\left(\frac{k}{\eta}\right)}}{\sqrt{k\log(k)}} \leq  C^2\frac{\log \left(\frac{k}{\eta}\right)}{ \sqrt{k\log(k)}}$ for all $x>0$.
        \item $x^2 e^{-\frac{x^2}{\sqrt{k\log(k)}} + C\frac{x \sqrt{\log \frac{k}{\eta}}}{\sqrt{k\log(k)}}} \lesssim C^2\sqrt{k\log{k}}\log{\left(\frac{k}{\eta}\right)} e^{C^2\frac{\log{\left(\frac{k}{\eta}\right)}}{\sqrt{k\log(k)}} } $ for all $x>0$.
        \item $x^4 e^{-\frac{2x^2}{\sqrt{k\log(k)}} + C\frac{x \sqrt{\log (k/\eta)}}{\sqrt{k\log(k)}}} \lesssim  C^4k\log(k)\log^2{\left(\frac{k}{\eta}\right)} e^{C^2\frac{\log{\left(\frac{k}{\eta}\right)}}{\sqrt{k\log(k)}} }$ for all $x>0$.
    \end{enumerate}
    \end{restatable}
    

    

    
    
    As a result of \eqref{eq:lpnorm-bound}, we have that  $v^\top \left(\widehat{T}_{1i}- T_{1i}\right)v$ (where $T_{1i}$ denotes the expectation of $\widehat{T_{1i}}$) is sub-exponential random variable with parameter $\lambda = \frac{1}{\eta^{o(1)}}$, hence from Bernstein's inequality we have that 
 \begin{align*}
     \Pr\left[\Abs{\frac{1}{\alpha n}\sum_{i : x_i \in T\setminus
     S} v^\top \left(\widehat{T}_{1i}- T_{1i}\right)v}\ge t  \right]\le 2\exp{\left(-c\min\left(\frac{t^2\alpha n}{\lambda^2}, \frac{t\alpha n}{\lambda }\right)\right)}  \;,
 \end{align*}
Using the above with $t=\eta/6$, we have that with $n= \frac{  \log(1/\delta')}{\eta^{2+o(1)}} $ samples\footnote{Here we have used that without loss of generality $\alpha = \Omega(1)$ since we can always treat some of the inliers as outliers in the model of \Cref{def:cont}} we have that with probability at least $1-\delta'$ for a fixed vector $v$ it holds that $\Abs{\frac{1}{\alpha n}\sum_{i : x_i \in T\setminus S }v^\top \left(\widehat{T}_{1i}- T_{1i}\right)v}\le O\left(\eta\right)$. Now let $\cC$ be a cover of the unit ball from \Cref{fact:cover}. By using $\delta' = \frac{\delta}{8|\cC| } = \delta2^{-O(k)}$ and a union bound over $\cC$ we have that 
 \begin{align*}
     \sup_{v\in \mathbb{R}^k: \Norm{v}=1}\Abs{\frac{1}{\alpha n}\sum_{i : x_i \in T\setminus S } v^\top \left(\widehat{T}_{1i}- T_{1i}\right)v}\le 10 \max_{v\in \cC} \Abs{\frac{1}{\alpha n}\sum_{i : x_i \in T\setminus S } v^\top \left(\widehat{T}_{1i}- T_{1i}\right)v}  \;,
 \end{align*}
which can be made less than $\eta/6$ with probability $1-\delta/8$ by using $n=\frac{d  \log(1/\delta)}{\eta^{2+o(1)}}$ samples.


\item  \paragraph{Third Term} 
For the third term $\widehat{T}_{3i} = z_i z_i^\top e^{-\frac{\|x_i\|^2-k}{\sqrt{k\log(k)}}}$, we will use the Matrix Bernstein Inequality (\Cref{fact:matrixBernstein}). Recall our notation that $T_{3i}$ denotes the expected value of the random matrix $\widehat{T}_{3i}$. First, we have that
\begin{align*}
 {   \Norm{\widehat{T}_{3i}-T_{3i} }_{\op} \lesssim \|\widehat{T}_{3i} }\|_{\op}&=\Norm{z_iz_i^\top e^{-\frac{\|x_i\|^2-k}{\sqrt{k\log(k)}}}\Ind\left(\cE_i\right)}_{\op}\\
 &=\Norm{z_iz_i^\top }_{\op}e^{-\frac{\|x_i\|^2-k}{\sqrt{k\log(k)}}}\Ind\left( \cE_i\right)\\
 &=\Norm{z_i}^2 e^{  -\frac{\|z_i\|^2}{\sqrt{k\log(k)}} + O \left(\frac{\|z_i\| \sqrt{\log( k/\eta) }}{\sqrt{k\log(k)}}\right) }   (1/\eta)^{o(1) }\tag{using \Cref{cl:center_norm_bound}}\\
 &\lesssim \sqrt{k\log(k)} \log(k/\eta)  (1/\eta)^{o(1) }\;, \tag{by \Cref{cl:high_school}} \\
 &\lesssim \sqrt{k}\log^{3/2}(k) \log(1/\eta) (1/\eta)^{o(1) } \\
 & \lesssim \sqrt{k}\log^{3/2}(k)   (1/\eta)^{o(1) } \;,
\end{align*}
where the first inequality follows by the triangle inequality and Jensen's inequality
almost surely.
Moreover
\begin{align*}
  \Norm{ \E[\left(\widehat{T}_{3i}-T_{3i}\right)^2] }_{\op} &\le \Norm{ \E[\widehat{T}_{3i}^2] }\\
 &=\Norm{\E\left[\norm{z_i}^2z_iz_i^\top e^{-2\frac{\|x_i\|^2-k}{\sqrt{k\log(k)}}}\Ind\left( \cE_i\right) \right]}_{\op}\\&=
 \norm{z_i}^2\Norm{z_iz_i^\top}_{\op}\E\left[ e^{-2\frac{\|x_i\|^2-k}{\sqrt{k\log(k)}}}\Ind\left( \cE_i\right) \right]
 \\&\le\norm{z_i}^4  e^{  -\frac{2\|z_i\|^2}{\sqrt{k\log(k)}} + O \left(\frac{\|z_i\| \sqrt{\log(k/\eta)}}{\sqrt{k\log(k)}}\right) }  (1/\eta)^{o(1)} \tag{using \Cref{cl:center_norm_bound}}\\
 &\le k\log(k) \log^2(k/\eta) e^{O(\frac{\log(k/\eta)}{\sqrt{k\log(k)}})} (1/\eta)^{o(1)}.  \tag{by \Cref{cl:high_school}}\\
&\le k\log^3(k) \log^2(1/\eta) e^{O(\frac{\log(1/\eta)}{\sqrt{k\log(k)}})} (1/\eta)^{o(1)}\\
&\le k \log^3(k)(1/\eta)^{o(1)} \;.
\end{align*}
Hence, from \Cref{fact:matrixBernstein}, we have that 
\begin{align*}
    \Pr\left[\Norm{\frac{1}{\alpha n}\sum_{i : x_i\in T\setminus S}\left(\widehat{T}_{3i}-T_{3i} \right)}_{\op}\ge t \right]\le 2k\exp{\left(-c \cdot \min\left(\frac{t^2 \alpha n }{k^{}\log^3(k)(1/\eta)^{o(1)}}, \frac{t \alpha n}{\sqrt{k}\log^{3/2}(k) (1/\eta)^{o(1)}} \right)\right)}.
\end{align*}
In summary, 
using $n= O\left((1/\eta)^{2+o(1)}k^{ }\log^3(k)\log{ (1/\delta)} \right)$, we have that 
$\Norm{\frac{1}{\alpha n}\sum_{i : x_i \in T\setminus S}\left(\widehat{T}_{3i}-T_{3i}\right) }_{\op} \le \eta/6$ 
with probability at least $1-\delta/8$.

\item  \paragraph{Second Term}
For the term $\widehat{T}_2$ we will prove a multiplicative bound. Again, fix a direction $v$ with $\|v\|=1$. We will first show that $v^\top \widehat{T}_2 v$ is subgaussian by bounding the $L_p$-norms:
\begin{align*}
     \Norm{v^\top\left( \widehat{T}_{2i}-T_{2i}\right)v}_{L_p}\lesssim \Norm{v^\top \widehat{T}_{2i}v}_{L_p} &= \Norm{    \left(g_i^\top v z_i^\top v\right)  e^{-\frac{\|x_i\|^2-k}{\sqrt{k\log(k)}}}\Ind( \cE_i)}_{L_p}\\
     & \le  \Norm{   g_i^\top v}_{L_p} \Abs{v^\top z_i} e^{  -\frac{\|z_i\|^2}{\sqrt{k\log(k)}} + O \left(\frac{\|z_i\| \sqrt{\log (k/\eta)}}{\sqrt{k\log(k)}}\right) }  (1/\eta)^{o(1)}   \tag{using \Cref{cl:center_norm_bound}}\\
     &\lesssim \sqrt{p}\Abs{v^\top z_i}e^{  -\frac{\|z_i\|^2}{\sqrt{k\log(k)}} + O \left(\frac{\|z_i\| \sqrt{\log (k/\eta)}}{\sqrt{k\log(k)}}\right) }  (1/\eta)^{o(1)}  \;. \tag{since $g_i^\top v \sim \cN(0,I)$}
\end{align*}
Hence, it follows that 
$v^\top\left( \widehat{T}_{2i}-T_{2i}\right)v$ are independent subgaussian random variables with proxy standard deviations $\sigma_i = \Abs{v^\top z_i}e^{  -\frac{\|z_i\|^2}{\sqrt{k\log(k)}} + O \left(\frac{\|z_i\| \sqrt{\log (k/\eta)}}{\sqrt{k\log(k)}}\right) } (1/\eta)^{o(1)}$. Thus, the proxy variance $\sigma^2$ of the average  $(1/\alpha n) \sum_{i : x_i T\setminus S} v^\top\left( \widehat{T}_{2i}-T_{2i}\right)v$ is
\begin{align*}
    \sigma^2 &\lesssim \frac{1}{(\alpha n)^2}\sum_{i : x_i \in T\setminus S} (v^\top z_i)^2 e^{  -2\frac{\|z_i\|^2}{\sqrt{k\log(k)}} + O \left(\frac{\|z_i\| \sqrt{\log (k/\eta)}}{\sqrt{k\log(k)}}\right) }(1/\eta)^{o(1)} \\
    &\lesssim \frac{1}{(\alpha n)^2}\sum_{i : x_i \in T\setminus S} (v^\top z_i)^2 e^{  -\|z_i\|^2/\sqrt{k\log(k)}  }(1/\eta)^{o(1)} \\
     &\leq  \frac{1}{(\alpha n)^2}\sum_{i : x_i \in T\setminus S} (v^\top z_i)^2e^{-\Norm{z_i}^2/(\sqrt{k\log(k)}+2)}(1/\eta)^{o(1)}  \\
     &\lesssim \frac{1}{\alpha n} v^\top A v (1/\eta)^{o(1)} \;,  \tag{for $A := \frac{1}{\alpha n}\sum_{i : x_i \in T\setminus S} z_i z_i^\top e^{-\frac{\|z_i\|^2}{\sqrt{k\log(k)}+2}}$}
\end{align*}
where the second line used the following:
\begin{align*}
    -2\frac{\|z_i\|^2}{\sqrt{k\log(k)}} + O \left(\frac{\|z_i\| \sqrt{\log (k/\eta)}}{\sqrt{k\log(k)}}\right) 
    &= -\frac{\|z_i\|^2}{\sqrt{k\log(k)}} + \left(  O \left(\frac{\|z_i\| \sqrt{\log (k/\eta)}}{\sqrt{k\log(k)}}\right) -\frac{\|z_i\|^2}{\sqrt{k\log(k)}}\right) \\
    &\leq -\frac{\|z_i\|^2}{\sqrt{k\log(k)}} + O\left( (k\log(k))^{-1/2}\log (k/\eta)\right) \tag{using \Cref{cl:high_school} for last two terms}\\
    &\leq -\frac{\|z_i\|^2}{\sqrt{k\log(k)}} + O\left( \frac{\log (1/\eta)}{\sqrt{k \log k}} \right) \;.
\end{align*}


 Using the subgaussian tails of the random variable $\frac{1}{\alpha n} \sum_i v^\top\left( \widehat{T}_{2i}-T_{2i}\right)v$, we have that with probability $1-\delta'$ it holds
\begin{align}
    \Abs{\frac{1}{\alpha n}\sum_{i : x_i \in T\setminus S} v^\top\left( \widehat{T}_{2i}-T_{2i}\right)v} 
    \lesssim \sigma \sqrt{\log(1/\delta')}
    \lesssim   \sqrt{ \frac{1}{\alpha n }(v^\top A v) \, (1/\eta)^{o(1)} \log(1/\delta')} \;. \label{eq:temp1}  
\end{align}
Now let $\cC$ be a cover of the unit ball with accuracy $\xi = \eta/k$. 
 The size of such a cover is $|\cC| = (O(k/\eta))^k$ (\Cref{fact:cover_cor_ver}). If we use $\delta' = \delta/(8|\cC|)$ and do a union bound over $\cC$ we have that \eqref{eq:temp1} holds with probability $1-\delta/8$ for all $v \in \cC$ simultaneously. 
By plugging in the aforementioned value for $\delta'$, we get that the following holds for all $v \in \cC$:
\begin{align*}
    \Abs{\frac{1}{\alpha n}\sum_{i : x_i \in T\setminus S} v^\top\left( \widehat{T}_{2i}-T_{2i}\right)v} &\lesssim  \sqrt{ \frac{1}{\alpha n }(v^\top A v) \, (1/\eta)^{o(1)} k \log(k/\eta) \log((1/\delta) } \\
    &\lesssim \sqrt{ \frac{1}{\alpha n }(v^\top A v) \, (1/\eta)^{o(1)} k  \log(1/\delta) } \;.
\end{align*}
By using $\alpha n=O\left({  k }(1/\eta)^{o(1)}\log(1/\delta)\right)$ samples, the above implies that for all $v\in \cC$ it holds
\begin{align*}
    \frac{1}{\alpha n}\sum_{i : x_i \in T\setminus
     S} v^\top \widehat{T}_{2i}v \ge \frac{1}{\alpha n}\sum_{i : x_i\in T\setminus
     S} v^\top  T_{2i}v- (\eta/6)  \sqrt{v^\top Av } \;.
\end{align*} 



As a result, combining the bounds for the three terms, we have that for every $v \in \cC$:
\begin{align*}
    v^\top \widehat{A} v&=\frac{1}{\alpha n}\sum_{i : x_i \in T\setminus S}  v^\top \left(\widehat{T}_{1i}+\widehat{T}_{2i}+\widehat{T}_{3i}\right) v\\
    &\ge \frac{1}{\alpha n}\sum_{i: x_i \in T\setminus S}  v^\top\left(T_{1i}+T_{2i}+T_{3i}\right)v-\eta/3 -(\eta/6)   \sqrt{v^\top Av }\\
    &= v^\top \E[\widehat{A}] v-\eta/3 -(\eta/6) \sqrt{v^\top Av }\\
    &\ge v^\top A v-\frac{5}{6}\eta -(\eta/6)  \sqrt{v^\top Av } \;.\tag{by \eqref{eq:expectation_shift}}
\end{align*}
From this, it now easily follows that $v^\top \widehat{A} v\geq - \eta$ by a simple case analysis: If $v^\top A v > 1$, then 
\begin{align*}
    v^\top A v-\frac{5}{6}\eta -(\eta/6)   \sqrt{v^\top Av }
    &\geq v^\top A v-\frac{5}{6}\eta -   \sqrt{v^\top Av } \tag{$\eta<1$} \\
    &\geq -\frac{5}{6}\eta  \;, 
\end{align*}
where the last line uses that $v^\top A v - \sqrt{v^\top Av } \geq 0$ whenever $v^\top A v > 1$.
If on the other hand  $v^\top A v \leq 1$, then our bound becomes $v^\top A v-\frac{5}{6}\eta -(\eta/6) \sqrt{v^\top Av } \geq v^\top A v-\frac{5}{6}\eta -(\eta/6) = v^\top A v- \eta \geq -\eta$, where we also used that $v^\top A v \geq 0$ by definition of $A :=  \frac{1}{\alpha n}\sum_{i : x_i \in T\setminus S} z_i z_i^\top e^{-\frac{\|z_i\|^2}{\sqrt{k\log(k)}+2}}$. 

So far we have shown that $v^\top \widehat{A} v \geq -\eta$ for every $v \in \cC$, where $\cC$ is a cover of the $k$-dimensional unit ball with accuracy $\xi = \eta/k$.
It is easy to see that this implies that $u^\top \widehat{A} u \geq -4\eta$ for all arbitrary $u$ with $\|u\| = 1$.
Consider an arbitrary unit vector $u$. There exists a $v\in \cC $  such that $\norm{v- u} \le \xi$ i.e., $u= v+w$,   with  $\|w\|\leq \xi$. Thus $u^\top \widehat{A}u=v^\top\widehat{A}v + v^\top A w+ w^\top A v +  w^\top\widehat{A}w \ge -\eta -   \abs{v^\top A w}- \abs{w^\top A v} -  \abs{w^\top\widehat{A}w} $.
Since $\|A\|_\op \leq \sqrt{k\log(k)}$ and $\|w\|\leq \xi = \eta/k$, $\abs{  u^\top A w}\le  \norm{w}\|A\|_\op\le \eta$ and $\abs{w^\top A w}\le \xi^2 \eta \le \eta$. 

The total probability of failure for \Cref{it:outliers} is $\delta/2$ by the union bound as the proof relies on $4$ events with probability of failure $\delta/8$.






    \item
    \paragraph{Proof of \Cref{it:inliers}}
We will use a similar argument as in \Cref{it:outliers}. Let $\cE_i$ be the same as in \eqref{eq:goodevent}. First, we will use 
\begin{align*}
        \widehat{A} = \frac{1}{| S|} \sum_{i : x_i \in  S} \widehat{A}_{i} \quad \text{where} \;\; \widehat{A}_i =  F_{\beta,k}(x_i)e^{\sqrt{k/\log(k)}}  \1(\cE_i)\;.
\end{align*}
The points $x_i\in S$ are all drawn from the same Gaussian component $\cN(\mu,I)$.
Fix $v\in \mathbb{R}^k:\norm{v}=1$. We bound the $L_p$ norm of the random variable $v^\top \left(\widehat{A}_{i}- \E[A_i]\right)v$ for $i : x_i\in S$ as follows:
     \begin{align}
      &\Norm{v^\top \left(\widehat{A}_{i}- \E[A_i]\right)v}_{L_p}\\ 
      &\lesssim \Norm{v^\top \widehat{A}_{i}v}_{L_p} \tag{by triangle inequality and Jensen's inequality}\\
      &=\Norm{   \left( \left(x_i^\top v\right)^2  - \frac{\sqrt{k\log(k)}}{\sqrt{k\log(k)}+2}   \right)e^{-\frac{\|x_i\|^2-k}{\sqrt{k\log(k)}}}\Ind( \cE_i)}_{L_p} \notag\\
      & \le  \Norm{    \left(x_i^\top v\right)^2  - \frac{\sqrt{k\log(k)}}{\sqrt{k\log(k)}+2}    }_{L_p} e^{  -\frac{\|\mu\|^2}{\sqrt{k\log(k)}} + O \left(\frac{\|\mu\| \sqrt{\log (k/\eta)}}{\sqrt{k\log(k)}}\right) }  (1/\eta)^{o(1)} \tag{by \Cref{cl:center_norm_bound}}\\
        &\le \left(\Norm{    \left(\mu^\top v+g_i\right)^2}_{L_p}+  \frac{\sqrt{k\log(k)}}{\sqrt{k\log(k)}+2} \right) e^{  -\frac{\|\mu\|^2}{\sqrt{k\log(k)}} + O \left(\frac{\|\mu\| \sqrt{\log (k/\eta)}}{\sqrt{k\log(k)}}\right) } (1/\eta)^{o(1)}  \tag{$x_i = g_i + \mu$ for $g_i\sim N(0,1)$}\\
        &\le \left(    \left(\mu^\top v\right)^2+ \Norm{ g_i^2}_{L_p}+  \frac{\sqrt{k\log(k)}}{\sqrt{k\log(k)}+2} \right) e^{  -\frac{\|\mu\|^2}{\sqrt{k\log(k)}} + O \left(\frac{\|\mu\| \sqrt{\log (k/\eta)}}{\sqrt{k\log(k)}}\right) }  (1/\eta)^{o(1)} \notag \\
        &\lesssim \left( \Norm{\mu}^2+p+  \frac{\sqrt{k\log(k)}}{\sqrt{k\log(k)}+2} \right)  e^{  -\frac{\|\mu\|^2}{\sqrt{k\log(k)}} + O \left(\frac{\|\mu\| \sqrt{\log (k/\eta)}}{\sqrt{k\log(k)}}\right) }  (1/\eta)^{o(1)}  \tag{by \Cref{fact:gaussian_moments} } \\
        &\lesssim p  \|\mu\|^2 e^{  -\frac{\|\mu\|^2}{\sqrt{k\log(k)}} + O \left(\frac{\|\mu\| \sqrt{\log (k/\eta)}}{\sqrt{k\log(k)}}\right) }  (1/\eta)^{o(1)} \notag \\
        &\lesssim p (1/\eta)^{o(1)} \;. \tag{$\norm{\mu}= O(1)$}
    \end{align}
Hence, by a cover argument identical to the one that we used when treating the first term  in \Cref{it:outliers}, we have that with 
  $n= O\left(\frac{k^{ }\log{(1/\delta)}}{\eta^{2+o(1)}} \right)$ we have that $\Norm{\widehat{A}-\E[A] }_{\op} \le \eta/2$ and hence $\Norm{\widehat{A}-A }_{\op} \le \eta$ with probability at least $1-\delta/2$ where  $A := \mu \mu ^\top e^{-\frac{\|\mu\|^2}{\sqrt{k\log(k)}+2}}$. 
  \item \paragraph{Proof of \Cref{it:trace_bound}}
Let $\widehat{A}$ the sample average matrix from \eqref{eq:matrixA}  over the whole sample set $T$ with $\beta=1$, we will show that $\trace(\widehat{A})\le 18 \sqrt{k\log(k)}$. In fact, we will show that this holds with probability 1.
  
\begin{align*}
    \trace(\widehat{A} )&=\frac{1}{n}\sum_{i \in [n]} \left(\norm{x_i}^2-\frac{\sqrt{k\log(k)}}{\sqrt{k\log(k)}+2}k \right)e^{-\frac{\norm{x_i}^2}{\sqrt{k\log(k)}}}(1+2/\sqrt{k\log(k)})^{k/2+2} \\ 
    &= \frac{1}{n}\sum_{i \in [n]} \left(\norm{x_i}^2-\frac{\sqrt{k\log(k)}}{\sqrt{k\log(k)}+2}k \right)e^{-\frac{\norm{x_i}^2-k}{\sqrt{k\log(k)}}} e^{-\sqrt{k/\log(k)}}(1+2/\sqrt{k\log(k)})^{k/2+2} \;.
\end{align*}

We will show that each term in that sum individually is at most $18 \sqrt{k\log(k)}$.
This will follow by two facts: (i) we have that $e^{-\sqrt{k/\log(k)}}(1+2/\sqrt{k\log(k)})^{k/2+2} \leq 6$ for all $k\geq 10$, and (ii) it is true that $ (\norm{x_i}^2-k\sqrt{k\log(k)}/(2+\sqrt{k\log(k)}) )e^{-\frac{\norm{x_i}^2-k}{\sqrt{k\log(k)}}} \leq  3 \sqrt{k\log(k)}$, which we will show in what follows.
Let $y:=\|x_i\|^2-\frac{\sqrt{k\log(k)}}{\sqrt{k\log(k)}+2}k$. We have that
\begin{align*}
    \left(\norm{x_i}^2-\frac{\sqrt{k\log(k)}}{\sqrt{k\log(k)}+2}k \right)e^{-\frac{\norm{x_i}^2-k}{\sqrt{k\log(k)}}}
    &= y e^{-\frac{y}{\sqrt{k\log(k)}}}e^{\frac{2\sqrt{k}}{\sqrt{k\log(k)}+2}} \\
   &\leq \frac{\sqrt{k\log(k)}}{e} e^{\frac{2\sqrt{k}}{\sqrt{k\log(k)}+2}}\\&  \leq  e \sqrt{k\log(k)}\\& \leq 3 \sqrt{k\log(k)}\;,
\end{align*}
where the first step is a re-writing, the second step uses that  $\sup_{y \in \R}y e^{-\frac{y}{\sqrt{k\log(k)}}} \leq \sqrt{k\log(k)}/e$, and the last step uses that $\frac{2\sqrt{k}}{2+\sqrt{k\log(k)}}\leq 2$ for all $k>10$.

The result follows by a union bound over over all three conditions.
\end{proof}





\SAMPLESTWO* 
\begin{proof}

    Let $S$ be the subset of the samples that correspond to inliers and $T \setminus S$ the subset corresponding to outliers. We want to establish the two conditions of \Cref{def:deterministic}. We prove each one separately:

   
    
    \item
    \paragraph{Proof of \Cref{it:outliers}} Let $\widehat{A}=\frac{1}{\alpha n}\sum_{i: x_i\in T\setminus S} \widehat{A}_i$, where $\widehat{A}_i$ as defined in \Cref{eq:matrixA} for $\beta =\eps$, also let $A=\E[\widehat{A}]$. Now define the  matrix $\Delta = \widehat{A} -  A$. We will show that for each element $\Delta_{ij}$, with high probability $1-\delta$ it holds $|\Delta_{ij}| \leq \eta/k$, which will allow us to conclude that $\|\Delta\|_\fr = \sqrt{\sum_{ij} |\Delta_{ij}|^2} \leq \sqrt{k^2 \eta^2/k^2} = \eta$, which in its turn will imply the desired $\|\Delta\|_\op \leq \|\Delta\|_\fr  \leq \eta$.

    Fix $s,t \in [k]$ and denote by $e_i$ the $i$-th element of the standard basis of $\R^k$. By \Cref{lemma:expression} we have that $\E[\Delta_{k \ell}] = 0$ and by \Cref{cl:var_bound} (applied with $\beta=\eps$), we have that 
    \begin{align*}
        \Var[\Delta_{s t}]\le\frac{1}{\paren{\alpha n}^2}\sum_{i: x_i\in T\setminus S}\Var[e_s^\top (\widehat{A}_{i} - A)e_t]
        \lesssim \frac{e^{4/\eps^2} d}{\alpha n}  \;.
    \end{align*}
    
 
    Thus, by Chebyshev's inequality, with probability at least $1-\tau$, we have that 
    \begin{align*}
        |\Delta_{s t}| \leq \sqrt{\frac{\Var[\Delta_{s t}]}{\tau}} 
        \lesssim \sqrt{\frac{e^{4/\eps^2} k}{\alpha \tau n} }
    \end{align*}
    
    The right hand side becomes less than $\eta/k$ when $n =O\left( \frac{k^3 }{\eta^2 \tau}e^{4/\eps^2}\right)$. 
    Finally, we can assume $\alpha = \Omega(1)$ without loss of generality, by simply treating some of inlier points as outliers in \Cref{def:deterministic}.
    
    Now, in order to do a union bound over all pairs $(s,t)$, for $s \in [k]$, $t \in [k]$, we will use $\tau = \frac{\delta}{k^2}$ for the failure probability.
    This brings the final sample complexity to $n = O\left(\frac{k^5}{\eta^2 \delta}e^{4/\eps^2}\right)$.

    \item
    \paragraph{Proof of \Cref{it:inliers}} The proof of this is a special case of the proof of \Cref{it:outliers} where   all the $z_i$'s are the same and equal to $\mu$.
    \item \paragraph{Proof of \Cref{it:trace_bound}}
    Let $\widehat{A}$ the sample average matrix from \eqref{eq:matrixA}  over the whole sample set $T$ with $\beta=\eps$, we will show that $\trace(\widehat{A})\le 2\eps \sqrt{k}$ with probability $1-\delta$. From \Cref{it:inliers} and \Cref{it:outliers} we have proved that $\Norm{\widehat{A}- A}_{\fr}\le \eta$ , where $A : = \frac{1}{n}\sum_{i: x_i \in T_t} z_i z_i^\top e^{- \frac{\|z_i\|^2}{\eps \sqrt{k}+2}}$. Consequently,
    \begin{align*}
    \tr(\widehat{A}) &= \tr(A) + \tr(\widehat{A}-A)
    \leq \eps\sqrt{k} + \sqrt{k}\| \widehat{A}-A \|_\fr
    \leq \eps \sqrt{k} + \sqrt{k} \eta \leq 2 \eps \sqrt{k} \;,
\end{align*}
where the last inequality uses that $\eta \leq \eps$.
\end{proof}





\subsection{Proofs of Supporting Claims}\label{sec:supporting_claims}

We restate and prove the following claims that were used in the previous section.


\begin{claim}\label{cl:var_bound} Assume $k >1/\beta^2$.
    Consider the random $k\times k$ matrix  $\widehat{A}_x := Z_{\beta,k} F_{\beta,k}(x)$, 
with 
 $x\sim \cN(\mu,I_k)$ is $k$-dimensional normal, and $Z_{\beta,k},  F_{\beta,k}$ as defined in \Cref{eq:matrixA}. For any two unit vectors $v,u$, it holds $\Var[v^\top \widehat{A}_x u] \lesssim e^{4/\beta^2} \beta^2 k$.
\end{claim}
\begin{proof}
    For the random variable $Y= v^\top  \widehat{A}_x u$, we have that 
\begin{align*}
    \E[\abs{Y}^p]&=\frac{Z^p}{Z'}\int_{\mathbb{R}^k} \left| (v^\top x) (u^\top x)- \frac{\beta\sqrt{k}}{2+\beta\sqrt{k}}v^\top u \right|^p e^{-p\frac{\norm{x}^2 }{\beta\sqrt{k}}}e^{-\frac{\norm{x-\mu}^2}{2}}dx \tag{for $Z'=(2\pi)^{k/2}$}\\ 
    &=\frac{ Z^p }{Z'}\int_{\mathbb{R}^k} \left|(v^\top x) (u^\top x)- \frac{\beta\sqrt{k}}{2+\beta\sqrt{k}}v^\top u \right|^p e^{-\frac{1}{2}\paren{\norm{x}^2(\frac{2p}{\beta\sqrt{k}} +1) -2x^\top \mu+\norm{\mu}^2 }}dx\\
    &=\frac{Z^p}{Z'}\int_{\mathbb{R}^k} \left|(v^\top x) (u^\top x)- \frac{\beta\sqrt{k}}{2+\beta\sqrt{k}}v^\top u \right|^p e^{-\frac{c}{2}\paren{\norm{x}^2 -\frac{2}{c}x^\top \mu+\frac{1}{c}\norm{\mu}^2 }}dx  \tag{for $c=\frac{2p}{\beta\sqrt{k}} +1$}\\
    &=\frac{Z^p}{Z'}\int_{\mathbb{R}^k} \left|(v^\top x) (u^\top x)- \frac{\beta\sqrt{k}}{2+\beta\sqrt{k}}v^\top u \right|^p e^{-\frac{c}{2}\paren{\norm{x-\frac{1}{c}\mu}^2+\paren{\frac{1}{c}-\frac{1}{c^2}}\norm{\mu}^2  }}dx\\
    &=\frac{Z^p e^{-\frac{1}{2}(1-\frac{1}{c})\norm{\mu}^2}}{Z'}\int_{\mathbb{R}^k} \left|(v^\top x) (u^\top x)- \frac{\beta\sqrt{k}}{2+\beta\sqrt{k}}v^\top u  \right|^p e^{-\frac{c}{2}\norm{x-\frac{1}{c}\mu}^2  }dx\\
    &=\frac{Z^p e^{-\frac{p}{2p+\beta\sqrt{k}}\norm{\mu}^2}Z''}{Z'}\E_{x\sim\cN(\frac{1}{c}\mu,\frac{1}{c}I)}\left[\left|(v^\top x) (u^\top x)- \frac{\beta\sqrt{k}}{2+\beta\sqrt{k}}v^\top u \right|^p \right]  \tag{for $Z''=(2\pi)^{k/2}\sqrt{\frac{1}{c^k}}$}  \\
    &=\frac{ Z^p e^{-\frac{p}{2p+\beta\sqrt{k}}\norm{\mu}^2}}{(1+\frac{2p}{\beta\sqrt{k}})^{k/2}}\E_{x\sim\cN(\frac{1}{c}\mu,\frac{1}{c}I)}\left[\left| (v^\top x) (u^\top x)- \frac{\beta\sqrt{k}}{2+\beta\sqrt{k}}v^\top u \right|^p \right] \;.
\end{align*}
For $p=2$, this becomes
\begin{align*}
    \E[\abs{Y}^2] &\lesssim \frac{(1+\frac{2}{\beta \sqrt{k}})^{k+4} e^{-\frac{2\|\mu\|^2}{4+\beta\sqrt{k}}}}{(1+4/(\beta\sqrt{k}))^{k/2}}   \E_{x\sim\cN(\frac{1}{c}\mu,\frac{1}{c}I)}\left[\left| (v^\top x) (u^\top x)- \frac{\beta\sqrt{k}}{2+\beta\sqrt{k}}v^\top u\right|^2  \right]  \\
    &\lesssim  e^{4/\beta^2} e^{-\frac{2\|\mu\|^2}{4+\beta\sqrt{k}}} \E_{x\sim\cN(\frac{1}{c}\mu,\frac{1}{c}I)}\left[\left| (v^\top x) (u^\top x)- \frac{\beta\sqrt{k}}{2+\beta\sqrt{k}}v^\top u\right|^2\right]  \tag{explained below}\\
    &\lesssim  e^{4/\beta^2} e^{-\frac{2\|\mu\|^2}{4+\beta\sqrt{k}}} \E_{x\sim\cN(\frac{1}{c}\mu,\frac{1}{c}I)}\left[\left| (v^\top x) (u^\top x)\right|^2+ \frac{\beta\sqrt{k}}{2+\beta\sqrt{k}}\abs{v^\top u}^2\right]    \tag{using $(a+b)^2 \leq 2a^2 + 2 b^2$}\\
    &\lesssim  e^{4/\beta^2} e^{-\frac{2\|\mu\|^2}{4+\beta\sqrt{k}}} \left( \E_{x\sim\cN(\frac{1}{c}\mu,\frac{1}{c}I)}\left[\left| (v^\top x) (u^\top x)\right|^2\right]+ \frac{\beta\sqrt{k}}{2+\beta\sqrt{k}} \right) \\
    &=  e^{4/\beta^2}e^{-\frac{2\|\mu\|^2}{4+\beta\sqrt{k}}} \left( \E_{x\sim\cN(\frac{1}{c}\mu,\frac{1}{c}I)}\left[\left| (v^\top x) (av^\top x+ b z^\top x)\right|^2\right]+ 1 \right) \tag{$u{=}av{+}bz$ for $z{\perp} v, a{:=}u^\top v,b{:=}\sqrt{1{-}a^2}$}  \\
    &=  e^{4/\beta^2} e^{-\frac{2\|\mu\|^2}{4+\beta\sqrt{k}}}\left( \E_{x\sim\cN(\frac{1}{c}\mu,\frac{1}{c}I)}[ a^2(v^\top x)^4+ 2ab(v^\top x)^3(z^\top x)+b^2(v^\top x)^2(z^\top x)^2 ]+1  \right) \\
    &=  e^{4/\beta^2} e^{-\frac{2\|\mu\|^2}{4+\beta\sqrt{k}}} \left( a^2 \E[ x^4]+ 2ab  \E[x^3] \E[y]+b^2\E[x^2]\E[y^2 ]+1 \right) \tag{${x\sim\cN(\frac{1}{c}v^\top \mu,\frac{1}{c})}$ and ${y\sim\cN(\frac{1}{c}z^\top \mu,\frac{1}{c})}$}\\
    &\lesssim e^{4/\beta^2} e^{-\frac{2\|\mu\|^2}{4+\beta\sqrt{k}}}\left(   \frac{\| \mu\|^4}{c^4}+  \frac{\| \mu\|^3}{c^3} + \frac{\| \mu\|^2}{c^2}+ \frac{1}{c^4}+ 1 \right) \tag{using $|a|\leq 1, |b|\leq 1$ and $\E_{y \sim \cN(z,\sigma^2)}[x^4] = z^4+6z^2 \sigma^2 + 3 \sigma^4$}\\
    &\lesssim  e^{4/\beta^2} e^{-\frac{2\|\mu\|^2}{4+\beta\sqrt{k}}} \left( \| \mu\|^4+1 \right)  \tag{using $c=\frac{2p}{\beta\sqrt{k}} +1 \geq 1$} \\
    &\lesssim e^{4/\beta^2} \beta^2 k\;. \tag{using the fact $\sup_{x \in \R} x^4 e^{-x^2/\gamma} \leq \gamma^2$}
\end{align*}


We now explain the second step above, which claims that $(1+\frac{2}{\beta \sqrt{k}})^{k+4}/(1+\frac{4}{\beta\sqrt{k}})^{k/2} \lesssim e^{4/\beta^2}$: 
First note that since $k \geq 1/\beta^2$ we have that $(1+\frac{2}{\beta \sqrt{k}})^{4} \leq 3^4 = O(1)$, thus it suffices to prove that $(1+\frac{2}{\beta \sqrt{k}})^{k}/(1+\frac{4}{\beta\sqrt{k}})^{k/2} \lesssim e^{4/\beta^2}$.
Towards this end, we will use the fact that $e^x \leq (1+x/n)^{n+x/2}$ for all $x,n >0$.
Applying this with $n=k/2$ and $x=2\sqrt{k}/\beta$, we have that
\begin{align*}
    e^{\frac{2\sqrt{k}}{\beta}} \leq \left( 1+ \frac{4}{\beta \sqrt{k}} \right)^{\frac{k}{2} + \frac{\sqrt{k}}{\beta}}\;.
\end{align*}
Rearranging, this gives that
\begin{align}
    \left( 1+ \frac{4}{\beta \sqrt{k}}  \right)^{k/2} \geq e^{\frac{2\sqrt{k}}{\beta}}/\left( 1 + \frac{4}{\beta \sqrt{k}} \right)^{\sqrt{k}/\beta} \;. \label{eq:tempbound}
\end{align}
Finally, using that, we obtain:
\begin{align*}
      \frac{\left( 1+ \frac{2}{\beta \sqrt{k}}  \right)^k}{  \left( 1+\frac{4}{\beta \sqrt{k}} \right)^{k/2}} 
    \leq \frac{\left( 1 + \frac{4}{\beta \sqrt{k}} \right)^{\sqrt{k}/\beta}\left( 1+ \frac{2}{\beta \sqrt{k}} \right)^{k}}{e^{\frac{2\sqrt{k}}{\beta}}}
    \leq \frac{e^{4/\beta^2} \left( 1+ \frac{2}{\beta \sqrt{k}} \right)^{k}}{e^{\frac{2\sqrt{k}}{\beta}}}
    \leq e^{\frac{4}{\beta^2} + \frac{2\sqrt{k}}{\beta} - \frac{2\sqrt{k}}{\beta}} \leq e^{4/\beta^2 }\;.
\end{align*}






\end{proof}

\REWEIGHTING*

\begin{proof}
    Let $\tau := 1/n^4$, then, by the definition of the event $\cE_i$ in \eqref{eq:goodevent}:
        \begin{align*}
     &\exp\left( -\frac{\|x_i\|^2-k}{\sqrt{k\log(k)}} \right) \1( \cE_i) 
     \leq \exp\left( \frac{-\|z_i\|^2  + O\left(\log \frac{1}{\tau}+ \left(\sqrt{k}+\Norm{z_i}\right) \sqrt{\log (1/\tau)}\right) }{\sqrt{k\log(k)}}\right) \\
     &\leq \exp\left[  -\frac{\|z_i\|^2}{\sqrt{k\log(k)}} +  O\left( \frac{\log(1/\tau)}{\sqrt{k\log(k)}} + \frac{\sqrt{\log(1/\tau)}}{\sqrt{\log(k)}} +  \frac{\|z_i\|\sqrt{\log(1/\tau)}}{\sqrt{k\log(k)}} \right) \right] \\
     &\leq \exp\left[  -\frac{\|z_i\|^2}{\sqrt{k\log(k)}} +  O\left( \frac{\log(k/\eta)}{\sqrt{k\log(k)}} + \frac{\sqrt{\log(k/\eta)}}{\sqrt{\log(k)}} +  \frac{\|z_i\|\sqrt{\log(k/\eta)}}{\sqrt{k\log(k)}} \right) \right]\\
     &\leq \exp\left[  -\frac{\|z_i\|^2}{\sqrt{k\log(k)}} +  O\left( \frac{\log(k) + \log(1/\eta)}{\sqrt{k\log(k)}} + \frac{\sqrt{\log(k/\eta)}}{\sqrt{\log(k)}} +  \frac{\|z_i\|\sqrt{\log(k/\eta)}}{\sqrt{k\log(k)}} \right) \right]\\
      &\leq \exp\left[  -\frac{\|z_i\|^2}{\sqrt{k\log(k)}} +  O\left( \frac{\log(1/\eta)}{\sqrt{k\log(k)}} + \frac{\sqrt{\log(k/\eta)}}{\sqrt{\log(k)}} +  \frac{\|z_i\|\sqrt{\log(k/\eta)}}{\sqrt{k\log(k)}} \right) \right] \\
      &\leq \exp\left[  -\frac{\|z_i\|^2}{\sqrt{k\log(k)}} +  O\left( \frac{\log(1/\eta)}{\sqrt{k\log(k)}} + \sqrt{\log(1/\eta)}  + \frac{\|z_i\|\sqrt{\log(k/\eta)}}{\sqrt{k\log(k)}} \right) \right] \\
      &\leq \exp\left[  -\frac{\|z_i\|^2}{\sqrt{k\log(k)}} +  O\left(   \frac{\|z_i\|\sqrt{\log(k/\eta)}}{\sqrt{k\log(k)}} \right) \right]  (1/\eta)^{o(1)}\;.
    \end{align*}
    In the above, we first used that $\tau= (1/n)^{O(1)}$ and $n = (k/\eta)^{O(1)}$, then we used that $\log(k)/\sqrt{k} = o(1)$. We have also used that $e^{\sqrt{\log (1/\eta)}} = (1/\eta)^{o(1)}$ and $e^{\frac{\log(1/\eta)}{\sqrt{k \log k}}} = (1/\eta)^{o(1)}$.
    
    \end{proof}


    \HIGHSCHOOL*
    \begin{proof} We prove each item in turn. 
    \item \paragraph{Proof of i }
    Let $f(x)=-\frac{x^2}{\sqrt{k\log(k)}} + C\frac{x \sqrt{\log \left(\frac{k}{\eta}\right)}}{\sqrt{k\log(k)}} $ as $\lim_{x\to +\infty} f(x)=-\infty$ and $f(0)=0$ and $f$ continuous on $(0,+\infty)$ we have that the maximum must occur on a point in $(0,+\infty)$ where the derivative is $0$ either it is upper bounded than $0$. Also 
    \begin{align*}
        &f'(x)=0\\
    \Rightarrow& x= \frac{C}{2}\sqrt{\log \left(\frac{k}{\eta}\right)}.
    \end{align*}
    Thus, as $f'(x)\le \frac{C^2}{2}\frac{\log \left(\frac{k}{\eta}\right)}{\sqrt{k\log(k)}}$, we get the result.
    \item \paragraph{Proof of ii }
    Let $f(x)=x^2 e^{-\frac{x^2}{\sqrt{k\log(k)}} + C\frac{x \sqrt{\log \left(\frac{k}{\eta}\right)}}{\sqrt{k\log(k)}}}$. As $\lim_{x\to +\infty} f(x)=0$ and $f(0)=0$ and $f$ continuous on $(0,+\infty)$, we have that the maximum must occur on a point in $(0,+\infty)$ where the derivative is $0$. We have $$f'(x)=2x e^{-\frac{x^2}{\sqrt{k\log(k)}} + C\frac{x \sqrt{\log \left(\frac{k}{\eta}\right)}}{\sqrt{k\log(k)}}}+\left( C\frac{x^2 \sqrt{\log \left(\frac{k}{\eta}\right)}}{\sqrt{k\log(k)}} -\frac{2x^3}{\sqrt{k\log(k)}} \right) e^{-\frac{x^2}{\sqrt{k\log(k)}} + C\frac{x \sqrt{\log \left(\frac{k}{\eta}\right)}}{\sqrt{k\log(k)}}} \;,$$ so solving for $x>0$ we get $x= \frac{1}{4} \left( C \sqrt{\log{\left(\frac{k}{\eta}\right)}} + \sqrt{16 \sqrt{k\log(k)} + C^2 \log{\left(\frac{k}{\eta}\right)}} \right)$. Hence, it suffices to compute 
   \begin{align*}
        &f\left( \frac{C\sqrt{\log \left(\frac{k}{\eta}\right)}+ \sqrt{C^2\log \left(\frac{k}{\eta}\right)+16\sqrt{k\log(k)}} }{4} \right)\\&\le \frac{1}{16} e^{\frac{C \sqrt{\log{\left(\frac{k}{\eta}\right)}} \left(C \sqrt{\log{\left(\frac{k}{\eta}\right)}} + \sqrt{16 \sqrt{k\log(k)} + C^2 \log{\left(\frac{k}{\eta}\right)}}\right)}{4 \sqrt{k\log(k)}}} \left(C \sqrt{\log{\left(\frac{k}{\eta}\right)}} + \sqrt{16 \sqrt{k\log(k)} + C^2 \log{\left(\frac{k}{\eta}\right)}}\right)^2
        \\
        &\lesssim C^2\sqrt{k\log(k)}\log{\left(\frac{k}{\eta}\right)} e^{C^2\frac{\log{\left(\frac{k}{\eta}\right)}}{\sqrt{k\log(k)}} }.
    \end{align*}
    Hence, $x^2 e^{-\frac{x^2}{\sqrt{k\log(k)}} + C\frac{x \sqrt{\log \left(k/\eta \right)}}{\sqrt{k\log(k)}}} \lesssim C^2\sqrt{k\log(k)}\log{\left(\frac{k}{\eta}\right)} e^{3C^2\frac{\log{\left(\frac{k}{\eta}\right)}}{\sqrt{k\log(k)}} }$.
    \item \paragraph{Proof of iii } Similarly to  the previous inequality, let 
    \begin{align*}
        f(x)=x^4 e^{-\frac{x^2}{\sqrt{k\log(k)}} + C\frac{x \sqrt{\log\left(\frac{k}{\eta}\right)}}{\sqrt{k\log(k)}}}.    \end{align*}
 As $\lim_{x\to +\infty} f(x)=-\infty$ and $f(0)=0$ and $f$ continuous on $(0,+\infty)$, we have that the maximum must occur on a point in $(0,+\infty)$ where the derivative is $0$ either it is upper bounded than $0$. Also, we have that
\begin{align*}
&\quad f'(x)= 0\\
\Rightarrow &\left(4x^3-\frac{2x^5}{\sqrt{k\log(k)}} +C\frac{x^4 \sqrt{\log\left(\frac{k}{\eta}\right)}}{\sqrt{k\log(k)}} \right) e^{-\frac{x^2}{\sqrt{k\log(k)}} + C\frac{x \sqrt{\log\left(\frac{k}{\eta}\right)}}{\sqrt{k\log(k)}}}=0\\
\Rightarrow & x=\frac{1}{8} \left( C \sqrt{\log{\left(\frac{k}{\eta}\right)}} + \sqrt{64 \sqrt{k\log(k)} + C^2 \log{\left(\frac{k}{\eta}\right)}} \right). \tag{for $x>0$}
\end{align*}
Moreover,  
\begin{align*}
&f\left(\frac{1}{8} \left( C \sqrt{\log{\left(\frac{k}{\eta}\right)}} + \sqrt{64 \sqrt{k\log(k)} + C^2 \log{\left(\frac{k}{\eta}\right)}} \right)\right)\\ 
&\lesssim C^4k\log(k)\log^2{\left(\frac{k}{\eta}\right)} e^{C^2\frac{\log{\left(\frac{k}{\eta}\right)}}{\sqrt{k\log(k)}} } \;.
\end{align*}
\end{proof}






    \section{Full Proof of \Cref{thm:main}}
    \label{appendix:fullproof}
We now combine everything to prove the main theorem, which we restate below:

\MAINTHEOREM*
\begin{proof}




The main while loop of the algorithm maintains a subspace $\cV_t$ whose dimension, denoted by $k$ in the pseudocode, starts from $d$ and can only decrease from a round to the next one. We will examine two distinct ``phases'' (or parts) of this while loop: Phase 1 will refer to all the iterations during which $C\log^4(d)/\eps^5 \leq k \leq d$, and phase 2 will refer to all the iterations with $1/\eps^2 \leq k \leq C\log^4(d)/\eps^5$. The algorithm and the analysis will be slightly different for each phase. 
Regarding notation, let us denote by $T_1$ the number of rounds of phase 1 and by $T_2$ the number of rounds of phase 2 (i.e., phase 1 consists of all the iterations for $t = 1,\ldots, T_1$ and phase 2 consists of the iterations for $t=T_1+1, \ldots, T_1 + T_2$).
The proof of correctness relies on the following claims regarding each part of the algorithm. We first state the claims, we then show how we can prove the theorem using the claims, and we finally prove each claim individually.

\vspace{10pt}

\begin{enumerate}[leftmargin=*]
    \item \textbf{Warm start}: If $\widehat{\mu}_0$ denotes the estimator from line \ref{line:warm_start} of the algorithm, it holds $\| \widehat{\mu}_0 - \mu\| = O(1)$ with probability $0.999$.\label{it:warmstart}

    
    \item \textbf{Phase 1} ($ C\log^4(d)/\eps^5 \leq k \leq d$):
    \begin{enumerate}
        
        \item Let $\cE_{t}$ for $t \in 1,2,\ldots$ be the event that the set $T_t$ from line \ref{line:sample_set}, after the transformation of line \ref{line:transform_set} is ($\eta_t,\sqrt{\log(k)}$)-good (cf. \Cref{def:deterministic}) with respect to  
        $\tilde \mu_t,\tilde z_1^{(t)},\ldots,\tilde z_{\alpha}^{(t)}$, where  $\eta_t$ is the parameter set in line \ref{line:eta_t_1}, $\tilde \mu_t = \Proj_{\cV_t}( \mu - \widehat{\mu}_0)$ and $\tilde z_1^{(t)},\ldots,\tilde z_{\alpha}^{(t)}$ are some vectors in $\cV_t$. Then, with probability at least $0.999$, the events $\cE_t$ for $t=0,1,\ldots,  \log d$ are all true.  \label{it:goodness}

        \item Assuming $\cE_t$ hold for $t=1,\dots,T_1$, Phase 1 terminates after at most $T_1 \leq   \log d$ iterations. 
        \label{it:iterations_num}

        
        \item Let $C$ be a sufficiently large absolute constant. Denote by $\cV_{t+1}$ the same subspace as in line \ref{line:subspace} of the algorithm and by $\cV_{t+1}^\perp$ its orthogonal complement. 
        During the $t$-th iteration of the while loop, when the execution reaches line \ref{line:subspace}, it holds $\| \Proj_{\cV_{t+1}^\perp}(\widehat{\mu}_0 - \mu)\| \leq C \sum_{t' = 1}^t \sqrt{\eta_{t'}}$, where $\eta_{t'}$ is the value set in line \ref{line:eta_t_1}.\label{it:subspace_error}
    \end{enumerate}


    
    \item \textbf{Phase 2} ($1/\eps^2 \leq k \leq C\log^4(d)/\eps^5$):
    \begin{enumerate}
        \item Let $\cE_{t}'$ be the event that the set $T_t$ from line \ref{line:sample_set}, after the transformation of line \ref{line:transform_set} is ($\eta_t,\eps$)-good (cf. \Cref{def:deterministic}) with respect to  $\tilde \mu_t,\tilde z_1^{(t)},\ldots,\tilde z_{\alpha}^{(t)}$, where $\eta_t$ is the parameter set in \ref{line:eta_t_2},  $\tilde \mu = \Proj_{\cV_t}( \mu - \widehat{\mu}_0)$ and $\tilde z_1^{(t)},\ldots,\tilde z_{\alpha}^{(t)}$ are some vectors in $\cV_t$. Then, with probability at least $0.999$, the events $\cE_t'$ for $t=T_1+1,\ldots,T_1+100\log (\log (d)/\eps)$ are all true.  \label{it:goodness2}

    
        \item Assuming the events $\cE_{t}'$ from above hold, in every iteration of Phase 2 the dimension $k$ gets halved. As a corollary, the number of iterations of Phase 2 (i.e., number of iterations for which $1/\eps^2 \leq k \leq C\log^4(d)/\eps^5$) is   $T_2 \leq 100 \log (\log(d)/\eps)$. \label{it:iterations_num2}

        

        \item Let $C$ be a sufficiently large absolute constant. Denote by $\cV_{t+1}$ the same subspace as in line \ref{line:subspace} of the algorithm and by $\cV_{t+1}^\perp$ its orthogonal complement. Assume that the events $\cE_t$ for $t  \in [t_{\max}]$ are all true. Then, the following holds for any  $t  \in [t_{\max}]$: During the $t$-th iteration of the while loop, when the execution reaches line \ref{line:subspace}, it holds $\| \Proj_{\cV_{t+1}^\perp}(\widehat{\mu}_0 - \mu)\| \leq C \sum_{t' = 1}^t \sqrt{\eta_{t'}} $, where $\eta_{t'}$ are the values set in lines \ref{line:eta_t_1} and \ref{line:eta_t_2}.\label{it:subspace_error2}
        
    \end{enumerate}

    
    \item \textbf{Estimator for remaining subspace}: With probability at least $0.999$, the lines \ref{line:sample_set_end}-\ref{line:brute_force} of the algorithm find a vector $\widehat{\mu}_1 \in \cV_{t}$ such that  $\|\widehat{\mu}_1 - \Proj_{\cV_t}(\mu)\| \leq \eps$.\label{it:brute_force} 
    
\end{enumerate}










\vspace{10pt}
We now show how given the claims above  it follows that, with probability at least $0.99$, the output of the algorithm $\widehat{\mu} := \widehat{\mu}_0 + \widehat{\mu}_1$ satisfies $\|\widehat{\mu} - \mu\| \leq \eps$. Without loss of generality, it suffices to show $\|\widehat{\mu} - \mu\| = O(\eps)$ as, if this is true, then one can also obtain error exactly $\eps$ by running  the algorithm with $c \eps$ in place of $\eps$, for $c$ being a sufficiently small constant.  Let $t=T_1 + T_2$, so that $\cV_t$ denotes the subspace after exiting the while loop.   By decomposing the true mean into the projections onto the two orthogonal subspaces we have that $\widehat \mu = \Proj_{\cV_t^\perp}(\mu) + \Proj_{\cV_t}(\mu)$: By the Pythagorean theorem:
\begin{align*}
    \left\| \widehat{\mu} - \mu \right\|^2
    &= \left\| \Proj_{\cV_{t}^\perp}(\widehat{\mu}_0 - \mu) + \Proj_{\cV_{t}}(\widehat{\mu}_1 - \mu)\right\|^2
    = \left\| \Proj_{\cV_{t}^\perp}(\widehat{\mu}_0 - \mu)\right\|^2 +  \left\| \Proj_{\cV_{t}}(\widehat{\mu}_1 - \mu)\right\|^2 \;. \\
\end{align*}
The last term is $\left\| \Proj_{\cV_{t}}(\widehat{\mu}_1 - \mu)\right\| \leq \eps$ by \Cref{it:brute_force}. It suffices to bound the first term. Towards this end, denote $D:= C\log^4(d)/\eps^5$, which is the dimension during the first iteration of phase 2. \Cref{it:subspace_error} for $t=T_1+T_2$ (which according to our notation denotes the last iteration of Phase 2) yields the following (we explain the derivations below):
\begin{align}
    \left\| \Proj_{\cV_{t}^\perp}(\widehat{\mu}_0 - \mu)\right\|
    &\lesssim\sum_{t' = 1}^{T_1 + T_2} \sqrt{\eta_{t'}} \\
    &\leq \sum_{t' = 1}^{T_1} \sqrt{\eta_{t'}}  + \sum_{t' = T_1 + 1}^{T_1+T_2} \sqrt{\eta_{t'}} \\
    &\leq T_1 \frac{\eps}{  \log d}  + \sum_{t' = T_1 + 1}^{T_1+T_2} \sqrt{\eta_{t'}} \\
    &\leq \eps  + \sum_{t' = T_1 + 1}^{T_1+T_2} \sqrt{\eta_{t'}} \tag{$T_1\leq   \log  d$ by \Cref{it:iterations_num}}\\
    &\leq  \eps + \left( \sqrt{\frac{36\eps}{\sqrt{D}}} +  \sqrt{\frac{36\eps}{\sqrt{D/2}}} + \cdots + \sqrt{\frac{36\eps}{\sqrt{1/\eps^2}}} \right) \label{eq:series}\\
    &\leq \eps  +  \frac{6\sqrt{\eps}}{D^{1/4}} \sum_{i=0}^{\lg(D \eps^2)} 2^{i/4}  \label{eq:series2} \\
    &\le  \eps +  \frac{6\sqrt{\eps}}{(2^{1/4}-1)D^{1/4}} 2^{\frac{\lg(D \eps^2)}{4}}  \label{eq:series3}\\
    &=  \eps +  \frac{6\sqrt{\eps}}{(2^{1/4}-1)D^{1/4}} (D \eps^2)^{1/4}\lesssim \eps \;. \label{eq:series4}
\end{align}

We proceed to explain the steps above:
\eqref{eq:series} uses the definition of $\eta_t:= 36  \eps/\sqrt{k}$. The dimension $k$ for the first round of Phase 2 is $D$ and in every subsequent round it gets divided by 2 (by the claim of \Cref{it:iterations_num2}). Phase 2 ends when the dimension becomes $1/\eps^2$, which corresponds to the last term in the series.
\eqref{eq:series2} is a rewriting. The summation goes from $i=0$ to $\lg (D \eps^2)$ because $\lg (D \eps^2)$ is the solution to the equation $D/2^x = 1/\eps^2$ which seeks to determine after how many rounds of halving the dimension becomes $1/\eps^2$.
The next line, \eqref{eq:series3} uses the closed form formula for that series.





\noindent
We now prove all the individual claims.


\item \paragraph{Proof of \Cref{it:warmstart}}
This holds with probability at least $0.999$ by an application of Corollary 2.12 in \cite{diakonikolas2023algorithmic}. Without loss of generality, we apply this corollary with the fraction of outliers being $\alpha = \Omega(1)$ since we can always treat some of the inliers as outliers in the model of \Cref{def:cont}. That corollary yields that  $n_0=O(d)$ samples suffice.



\item \paragraph{Proof of \Cref{it:iterations_num}}

We claim that each iteration of Phase 1 of the while loop decreases the dimension from  $k$ to $k' := 18\sqrt{k\log(k)}/\eta_t$. 
This is because $\cE_i$ hold for $i=1,\dots, t$, and \Cref{it:trace_bound} of \Cref{def:cont} states  that $\trace(\widehat{A})\le 18\sqrt{k\log(k)}$. This means  that in line \ref{line:new_eigenvectors} of the algorithm, the number of eigenvectors with eigenvalue greater than $\eta_t$ can be at most $\trace(\widehat{A}_t)/\eta_t \leq 18\sqrt{k\log(k)}/\eta_t$.


The dimension thus gets divided by $2$ whenever $18\sqrt{k\log(k)}/\eta_t < k/2$. By plugging in the value $\eta_t:= (\eps/\log d)^2$ we obtain that the dimension gets halved whenever $k >36^2 \cdot \log^5 (d) /\eps^4$. This indeed holds because of line \ref{line:phase1}. Hence the number of iterations is at most $\log(d)$.


  



\item \paragraph{Proof of \Cref{it:goodness}}
During the $t$-th round of our algorithm let $z_i^{(t)}$ denote the outlier centers that the adversary chooses for the samples, in the model of \Cref{def:cont}.
Note that performing the mean shift $\tilde x_i = x_i- \widehat{\mu}_0$, means that the shifted points $\tilde x_i$ effectively come from the model described in \Cref{def:cont} with shifted mean $\tilde \mu  = \mu-\widehat{\mu}_0$ and shifted outlier centers $\tilde z_i^{(t)}   =  z_i^{(t)} - \widehat{\mu}_0$. Similarly, the projection operation of line \ref{line:transform_set} makes the points essentially come from a model that uses mean $\tilde \mu_t = \Proj_{\cV_t}(\mu-\widehat{\mu}_0)$.

Now by a union bound over all of the $\log d$ iterations of Phase 1 and  \Cref{lem:sample_complexity} with $\eta=\eta_t= (\eps / \log d)^2$ and $\delta = 10^{-3}/\log d$, we have that all of the sets $T_t$ of points drawn in line \ref{line:transform_set} will be ($\eta_t,\sqrt{\log(k)}$)-good with probability at least $0.999$.
Note that the condition  $\|\tilde \mu_t\| \leq  \|\mu-\widehat{\mu}_0 \| = O(1)$ that this lemma requires has already been established in \Cref{it:warmstart}, thus the lemma is indeed applicable. The sample complexity that this lemma yields is $k\log^3 k(1/\eta)^{2+o(1)}/\delta$. By using $\eta=\eta_t= (\eps /\log d)^2$ and $\delta = 10^{-3}/\log d$ this becomes $ k(\log(d)/\eps)^{2+o(1)}$ which is smaller than the number of samples $n_1$ that we use in our algorithm (cf. line \ref{line:samples}).








\item \paragraph{Proof of \Cref{it:subspace_error}}

Proof by induction. Denote by $C$ the constant in the statement of \Cref{lemma:dimerror}. The base case $t=1$ follows immediately by \Cref{lemma:dimerror} applied with $\cU= \R^d$ and $\cV = \cV_2^\perp$.  For the induction step, assume that the claim holds for $t$ and we will show it for $t+1$. Let $\cV_{t+1}$ denote the subspace maintained by the algorithm at the end of (line \ref{line:subspace}) the $t$-th iteration. 
By inductive hypothesis, we know that $\| \Proj_{ \cV_{t+1}^{\perp}}( \widehat{\mu}_0 - \mu) \| \leq  C\sum_{\tau=1}^t \sqrt{\eta_\tau}$. The goal is to show the bound for the end of the next iteration, i.e., show that $\| \Proj_{ \cV_{t+2}^{\perp}}( \widehat{\mu}_0 - \mu) \| \leq C \sum_{\tau=1}^{t+1} \sqrt{\eta_\tau}$.
We split $\cV_{t+2}^\perp$ into two parts: $\cV_{t+2}^\perp \cap \cV_{t+1}$ and  $\cV_{t+2}^\perp \cap \cV_{t+1}^\perp$. We now apply \Cref{lemma:dimerror} with  $\cU = \cV_{t+1}$, $\beta=\sqrt{\log (d)}$ and $\cV = \cV_{t+2}^\perp \cap \cV_{t+1}$.  
The lemma is indeed applicable because the set $T_t$ of points are ($\eta_t,\sqrt{\log (k)}$)-good.
The application of the lemma yields that $\| \Proj_{\cV_{t+2}^\perp \cap \cV_{t+1}}( \widehat{\mu}_0 - \mu) \| \leq C \sqrt{\eta_{t+1}}$. Also, the inductive hypothesis that we mentioned earlier implies that $\| \Proj_{\cV_{t+2}^\perp \cap \cV_{t+1}^{\perp}}( \widehat{\mu}_0 - \mu) \| \leq C \sum_{\tau=1}^{t} \sqrt{\eta_{\tau}}$. Combining the previous two  we have that  $\| \Proj_{\cV_{t+2}^\perp}( \widehat{\mu}_0 - \mu) \| \leq C\sum_{\tau=1}^{t+1} \sqrt{\eta_\tau}$.




\item \paragraph{Proof of \Cref{it:goodness2}}
This follows by an application of \Cref{lem:sample_complexity2} with probability of failure $\delta = 10^{-5}/\log(\log(d)/\eps)$. 
Note that the requirement of that lemma that $k>1/\eps^2$ holds since the algorithm would exit entirely the while loop of line \ref{line:while} otherwise.  
Since the dimension is $k \leq C\log^4(d)/\eps^5$ during this phase of the algorithm, and $\eta_t = O(\eps/\sqrt{k})$, the sample complexity $n = \frac{k^5}{\eta_t^2 \delta} 2^{O(1/\eps^2)}$ mentioned in that lemma is overall $2^{O(1/\eps^2)} \polylog(d)$ . By a union bound over the iterations of the algorithm, we have that all the sets $T_t$ during the first $100\log(\log(d)/\eps)$ rounds of Phase 2 will be $(\eta_t,\eps)$-good (and as we will show below, Phase 2 will not have more than $100\log(\log(d)/\eps)$-many rounds). 



\item \paragraph{Proof of \Cref{it:iterations_num2}}

Consider a single iteration of Phase 2 of the algorithm, and let $\widehat{A}_t$ be the matrix from line \ref{line:Ahat}. Since the set of points on which $\widehat{A}_t$ is computed is $(\eta_t,\eps)$-good, we have that $\tr(\widehat{A})$ is at most $18 \eps \sqrt{k}$ (cf. \Cref{it:trace_bound} of \Cref{def:deterministic}).
 If $k'$ denotes the number of eigenvalues larger than $\eta_t$ then we have  $k' \leq 18   \eps \sqrt{k}/\eta_t$. Note that, by definition of $\eta_t:=36  \eps/\sqrt{k}$, 
\begin{align*}
    \frac{18 \eps \sqrt{k}}{\eta_t}  =\frac{18 \eps \sqrt{k}\sqrt{k}}{36 \eps} \leq k/2.
\end{align*}
The dimension in each round is therefore being halved.



\item \paragraph{Proof of \Cref{it:subspace_error2}}
This follows by the same argument we used for Phase 1, but applying \Cref{lemma:dimerror} with $\beta=\eps$ instead of $\beta=\sqrt{\log k}$.


\item \paragraph{Proof of \Cref{it:brute_force}}
This follows by an application of \Cref{thm:brute_force}. 


\item \paragraph{Runtime and Sample Complexity of \Cref{alg:mean_estimation}}
Let  $n_0,n_1,n_2$ be as defined in line \Cref{line:samples} of the algorithm.
Recall that we denote by $T_1$ the number of iterations of the first phase of the while loop (interactions for which $C\log^4(d)/\eps^5 \leq k \leq d$) and by $T_2$ the number of the remaining iterations of the while loop.
The sample complexity of the algorithm is the following:
\begin{align*}
    n &= n_0 + n_1 \cdot T_1 + n_2\cdot (T_2 + 1)\\
     &= O(d) + \frac{d \polylog(d) }{\eps^{2+o(1)}}  +  2^{O(1/\eps^2)} \polylog(d)  \\
     &=   \frac{d \polylog(d) }{\eps^{2+o(1)}}+2^{O(1/\eps^2)} \polylog(d) .
\end{align*}
Regarding runtime: The runtime of the warm-start step is $\tau_1 = \poly(n_0 d)$. The runtime of the while-loop part of the algorithm is $\tau_2 = \poly(n_1 d )$ because each iteration runs in polynomial time and we have at most $T_1+T_2 = O(\log d)$ iterations. The runtime of the last step of the algorithm (\ref{line:brute_force}) is $\tau_3 = 2^{O(k)}\poly(n_2 d)$, where $k = 1/\eps^2$ here denotes the dimension of the subspace $\cV_t$ for $t=T_1+T_2$, i.e., the dimension that we end up after the while loop finishes. Since $n_2 = 2^{\Theta(1/\eps^2)}$, that runtime is overall $\tau_3 = \poly(n_2 d)$. Thus, the overall runtime of the algorithm is polynomial in the size of the input.











\end{proof}





\section{Higher Breakdown Point and Adaptivity}
\label{appendix:breakdownpoint}

In this section, we use Lepskii's method \cite{lepskii1991problem, birge2001alternative} to prove \Cref{thm:higher_breakdown} (stated below), 
a generalization of \Cref{thm:main} which provides similar guarantees as \Cref{thm:main} but works for any contamination parameter $\alpha$, and does not require a priori knowledge of $\alpha$. Specifically, we can generalize the algorithm to work for $\alpha \in (0,1/2-c)$ for any $c<1/2$, which is unknown to the algorithm. The price that we pay for that is a slightly larger dependence on $\eps$ in the sample complexity and the fact that the error becomes $O_c(\eps)$ instead of $\eps$, where the $O_c$ notation hides factors that depend on $c$ (i.e., if $c$ is a constant, the error increases only by a constant factor). 
Moreover,  even in one dimension, \cite{kotekal2024optimal} has shown that consistent estimation with arbitrarily small error $\eps$ using the information theoretic optimal of $2^{\Theta(1/\eps^2)}$ samples is only possible when $c>2^{-\Theta(1/\eps^2)}$, thus we only consider that regime in this section.


\begin{restatable}{theorem}{MAINTHEOREMHBP}{\em (Higher breakdown point)} \label{thm:higher_breakdown}
    Let $d \in \Z_+$ denote the dimension, $\mu \in \R^d$ be an unknown mean vector,  $\eps \in (0,1)$ be an accuracy parameter.
    Fix $n = (d\cdot(1/\eps)^C + 2^{C/\eps^2})\log^C(d)$, for a sufficiently large constant $C$. 
    Suppose that we have sample access to $\alpha$-corrupted samples from $\cN(\mu,I)$ under the mean-shift model (\Cref{def:cont}) with contamination parameter $\alpha \in (0,1/2-c)$ where $2^{-1/(2\eps^2)} < c \leq 1/2$.
    There exists an algorithm that takes as input $\eps$, draws 
    $n$ samples, runs in $\poly(n, d)$ time, and outputs $\widehat{\mu}$ such that with probability at least $0.99$ it holds $\| \widehat{\mu} - \mu \| \leq O_c(\eps)$.
\end{restatable}

 
The first step towards proving \Cref{thm:higher_breakdown} is to show that there exists an estimator that, \emph{having $\gamma$} as input achieves error $O_\gamma(\eps)$, i.e., an estimator that requires knowledge of $\gamma$ but $\gamma$ can be arbitrary. We show this in \Cref{cl:arbitraryalpha}. Then, we can use Lepskii's method to obtain an estimator with the same error, but without  knowledge of $\gamma$.




\begin{claim} \label{cl:arbitraryalpha}
        Let $d \in \Z_+$ denote the dimension, $\mu \in \R^d$ be an unknown mean vector and  $\eps \in (0,1)$ be an accuracy parameter.  
        Let $\alpha$ be a contamination parameter.
        There exists an algorithm that takes as input $\eps$, $\alpha$, draws  
    $n = \tilde{O}(d/(\delta \eps^{2+o(1)}) + 2^{O(1/\eps^2)}/\delta)$ $\alpha$-corrupted samples from $\cN(\mu,I)$ under the mean-shift model (\Cref{def:cont}), runs in $\poly(n, d)$ time, and outputs $\widehat{\mu}$ such that, if $\alpha \leq 1/2-2^{-1/(2\eps^2)}$, then with probability at least $1-\delta$ it holds $\| \widehat{\mu} - \mu \| \leq e^{O(1/(1-2\alpha)^2)}\eps$.
\end{claim}
\begin{proof}
For notational convenience, instead of working with the contamination parameter $\alpha$ from \Cref{def:cont}, we will use the reparameterization  $\gamma = \frac{1}{1-2\alpha}$. We now describe the modifications that we need to do to \Cref{alg:mean_estimation} and its analysis in order to obtain \Cref{cl:arbitraryalpha}



First, we replace the rough estimation step of line \ref{line:warm_start} in \Cref{alg:mean_estimation} with an 
estimator that takes $\alpha$ in its input and achieves error 
$O(\gamma)$ instead of an $O(1)$ 
(see, e.g., Exercise 2.10 in \cite{diakonikolas2023algorithmic}).

This change affects the error given by the analysis of the dimensionality reduction, \Cref{lemma:dimerror}. It is easy to see that the conclusion of that lemma will now become $\norm{\Proj_{\cU}(\mu)} = O(e^{\gamma^2}\sqrt{\eta})$ instead of $\norm{\Proj_{\cU}(\mu)} = O(e^{\gamma^2}\sqrt{\eta})$.  This is because in the last line of the proof of \Cref{lemma:dimerror} we have that for any $v$ such that $v^\top\widehat{A}v\le \eta$
\begin{align*}
(v^\top \mu)^2 \leq 2 \eta \, e^{\frac{\|\mu\|^2}{\beta \sqrt{k}+2}}
    \lesssim e^{\gamma^2}\eta.    
\end{align*}
As a result, we have that  $\Norm{\Proj_{\cU}(\mu)}=e^{O(\gamma^2)}\sqrt{\eta}$ for $\cU$ the subspace of $V_t$ such that $v^{\top}\widehat{A}v\le \eta$ for all unit vectors $v\in \cU$. So propagating this change in the analysis of \Cref{alg:mean_estimation} in \Cref{appendix:fullproof} we get the $e^{O(\gamma^2)}\eps $ error will appear as the contribution to the error by the subspace $\cV_t^\perp$.

For the error on the orthogonal subspace, we need to calculate the error that our inefficient estimator in the final step of our algorithm will attain. Note that from \cite{kotekal2024optimal} we have that when using $2^{O(1/\eps^2)}$ samples the one dimensional estimator achieves error $\frac{1}{\sqrt{\log(1+2^{-O(\eps^2) }/\gamma^2)}}\le e^{O(\gamma^2)}\eps$ when $\gamma\le 2^{1/(2\eps^2)}/2$. As a result, a more refined version of \Cref{thm:brute_force} exists error $\widehat{\mu}$ is $\| \widehat{\mu} - \mu \| \leq e^{O(\gamma^2)}\eps$ with probability $0.99$.

Finally, although our analysis in \Cref{thm:main} provides a result that holds with a constant probability, this was done only for simplicity. It is easy to verify that because the conclusion of \Cref{lem:sample_complexity,lem:sample_complexity2} holds with probability $1-\delta$ with $1/\delta$ blowup in the sample complexity, we can follow the analysis done in \Cref{appendix:fullproof} to obtain a high probability conclusion for the final error of the algorithm. \qedhere

\end{proof}



Now we will use \Cref{cl:arbitraryalpha} along with Lepskii's method to get an error guarantee that depends on the true parameter $\gamma$ without it being known to the algorithm. We state Lepskii's method guarantee below and then prove \Cref{thm:higher_breakdown}.
\begin{fact}[Lepskii's Method]\label{thm:lepskii}
    Let $\mu \in \mathbb{R}^d$, $A,B > 0$, $\gamma \in [A,B]$, and a non-decreasing function $r : \mathbb{R}^+ \to \mathbb{R}^+$. Suppose $\text{Alg}(\gamma')$ is a black-box algorithm that is guaranteed to return a vector $\widehat{\mu}$ such that $\|\widehat{\mu} - \mu\|_2 \leq r(\gamma')$, with probability at least $1-\delta$, whenever $\gamma' \geq \gamma$. Then, there exists an algorithm that returns $\widehat{\mu}'$ such that, with probability at least $1-O(\log(B/A))\delta$, it holds $\|\widehat{\mu}' - \mu\|_2 \leq 3r(2\gamma)$. Moreover, this algorithm calls $\text{Alg}$ at most $O(\log(B/A))$ times.
\end{fact}


\begin{proof}[Proof of \Cref{thm:higher_breakdown}]
We will apply Lepskii's method (cf. \Cref{thm:lepskii}), while using the algorithm of \Cref{cl:arbitraryalpha} with $\delta = 0.01 \eps^2/\log(d)$ for $\text{Alg}$ and $r(\gamma) := e^{O(\gamma^2)}\eps$. By assumption $\alpha \leq 1/2-1/\sqrt{n}$ (and $\alpha \geq 1/n$ otherwise we can treat one of the samples as outlier to make it at least $1/n$) thus $\gamma$ will belong in the interval $[n/(n-2),\sqrt{n}/2)$ thus Lepskii's method will search over the interval $[A,B]$ where $A=n/(n-2)$ and $B=\sqrt{n}/2$.


Now if we use the algorithm from \Cref{cl:arbitraryalpha} for this fixed $n$ in Lepskii's method the number of calls to our algorithm will be at most $\log(\sqrt{n}/(n/(n-2)))\le\log(\sqrt{n})\le \log(d2^{O(1/\eps^2)})\le \log(d)/\eps^2$ (this is why we used \Cref{cl:arbitraryalpha} with $\delta = 0.01\eps^2/\log(d)$ in the beginning). Hence by the union bound, the probability of failure of any call of the algorithm in Lepskii's method is at most $\log(d)/\eps^2\delta \leq 0.01$. Therefore, by \Cref{thm:lepskii}, we obtain that there exists an algorithm that returns an estimate $\widehat{\mu}'$ such that $\|\widehat{\mu}' - \mu\|_2 \leq 3r(2\gamma)\le e^{O(\gamma^2)}\eps=O_\gamma(\eps)$ with probability at least $0.99$.


Regarding the sample complexity of our algorithm, as at each call we use the same number of samples $n$ and the number of such calls is at most $\log(d)/\eps^2$, we have that the total number of samples is at most $(\log(d)/\eps^2)n\le \tilde{O}(d \poly(1/\eps) + 2^{O(1/\eps^2)})$. Furthermore, the runtime of the algorithm is $(\log(d)/\eps^2)\poly(n,d)=\poly(n,d)$, as Lepskii's method consists of $\log(d)/\eps^2$ calls to the algorithm of \Cref{cl:arbitraryalpha}.\qedhere

    

\end{proof}










\end{document}
