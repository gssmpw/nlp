\section{Conclusion}
\label{sec:conclusion}

Our analysis reveals fundamental insights into the geometric principles underlying neural network representations through the lens of statistical distances. We demonstrate that while networks can adopt either representation, ReLU-based architectures exhibit a natural bias towards distance-based learning. The catastrophic failure of \texttt{ReLU2} under intensity constraints illustrates how architectural choices can create untenable optimization landscapes, particularly when inputs must cluster near decision boundaries. The performance gap between \texttt{Abs2} and \texttt{Abs2-Neg} further illuminates this geometric perspective: while \texttt{Abs2} leverages combinatorial flexibility to select optimal separation points in high-dimensional space, \texttt{Abs2-Neg}'s restricted prototype selection leads to degraded performance. This distinction underscores the crucial role of architectural flexibility in learning effective distance-based representations.

These findings suggest that network behavior is driven by geometric interactions in the feature space rather than intrinsic properties of activation functions. The success of our OffsetL2 architecture, which directly models statistically-motivated geometric relationships through Mahalanobis distance calculations, validates this framework by achieving superior performance (97.61\% accuracy) with remarkable stability ($\pm$ 0.07\% standard deviation). By learning single prototypes representing either optimal $z_c$ or $z_{\neg c}$ for each class, OffsetL2 avoids the pitfalls of implicit prototype discovery through constrained hyperplanes, demonstrating the advantages of explicitly modeling distance-based relationships.

This research opens new avenues for neural network design by demonstrating the importance of explicitly modeling geometric relationships in feature space. Future work should explore how these principles extend to deeper architectures and diverse tasks, potentially leading to networks that are not only more powerful but also more interpretable and aligned with the underlying statistical structure of the data. By viewing neural computation through the lens of statistical distances rather than activation intensities, we can develop more principled approaches to architecture design that bridge the gap between theoretical understanding and practical application.

