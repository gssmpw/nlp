\section{Related Work}
\label{sec:related_work}

Our work builds upon recent advances in understanding neural networks through statistical distance metrics. Our prior work \cite{oursland2024interpreting} demonstrated how linear layers with Absolute Value (Abs) activations approximate the Mahalanobis distance \cite{mahalanobis1936generalized}. We established a mathematical foundation for distance-based representations, showing that such layers naturally encode distance relationships rather than intensity-based activations. Empirical studies further reinforce this perspective. In \cite{oursland2024neural}, we demonstrated that networks with ReLU and Abs activations exhibit particular sensitivity to perturbations affecting distance relationships in the feature space, suggesting that distance metrics play a fundamental role in how networks process information.

Alternative approaches to incorporating distance metrics in neural networks include Radial Basis Function (RBF) networks \cite{broomhead1988radial, park1991universal}, which use distances from learned centers for classification, Siamese networks \cite{bromley1994signature, hadsell2006dimensionality}, which learn embeddings where distances represent similarity, and Learning Vector Quantization (LVQ) \cite{kohonen1995learning}, which explicitly models class prototypes and uses distance-based classification. Contrastive learning \cite{chen2020simple, he2020momentum} emphasizes the importance of learning representations where distances reflect semantic similarity, although those methods typically rely on carefully constructed training objectives rather than inherent architectural biases. While specialized architectures like RBF and LVQ demonstrate the effectiveness of explicitly encoding distances, they remain largely confined to specific applications rather than general-purpose deep learning.

Complementing these distance-centric views, geometric interpretations of neural computation offer valuable insights for understanding internal representations \cite{montavon2018methods, olah2017feature}. These approaches analyze hyperplanes and decision boundaries to explain how networks partition and represent data \cite{lipton2018mythos, erhan2009visualizing}, though they typically focus on networks trained under standard intensity-based assumptions.

This work bridges the gap between distance-based and geometric interpretations by investigating how architectural choices influence the emergence of distance-based representations.