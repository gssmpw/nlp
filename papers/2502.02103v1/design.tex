\section{Experimental Design}
\label{sec:exp_design}

We explore whether networks exhibit a preference for distance-based or intensity-based representations. We employ simple two-layer networks and systematically architectural components to force either distance or intensity representations in the final linear layer. We aim to reveal potential training biases towards specific representation types.

\subsection{Objectives}
\begin{enumerate}
    \item When neural networks are constrained to produce either distance-based or intensity-based outputs, how does this affect their performance?
    
    \item How do different activation functions influence the network's ability to learn under these different representational constraints?
    \item By analyzing the performance and behavior of networks under these constraints, what can we infer about the nature of the representations learned in the output layer and the geometry of the feature space?
\end{enumerate}


\subsection{Model Design}

We constrained six neural network designs to force specific internal representations. These architectures are intentionally kept simple to isolate the specific behaviors under study. We utilize two-layer networks. All of the hidden layers have 128 nodes. ReLU is studied as a standard activation function in deep learning. Abs is studied for its theoretical connection to the Mahalanobis distance.

The core representational constraint is CrossEntropyLoss, which enforces intensity-based outputs. The combination of activations and a negation operator controls how the output layer must represent features internally. The activation function forces positive values (more precisely, non-negative). The negation reverses the order and signs of the activations. Models with the negation learn a positive distance representation which is converted by the negation layer into a negative intensity representation. 

The six primary architectures exclude a bias term in the final linear layer. This choice prevents the network from trivially learning the opposite representation (because $-(Wx)=(-W)x$) and then simply shifting it back to the positive side by using the bias. 

To establish a baseline for comparison, we include two control architectures \texttt{ReLU} and \texttt{Abs}. The four experimental architectures are \texttt{Abs2}, \texttt{Abs2-Neg}, \texttt{ReLU2}, and \texttt{ReLU2-Neg}. 

\begin{table}[H]
    \centering
    % \footnotesize
    \begin{tabular}{ll}
        \toprule
        \textbf{Model} & \textbf{Architecture}\\
        \midrule
        Abs & x → Linear → Abs → Linear → y \\
        ReLU & x → Linear → ReLU → Linear → y \\
        \midrule
        Abs2 & x → Linear → Abs → Linear → Abs → y \\
        Abs2-Neg & x → Linear → Abs → Linear → Abs → Neg → y \\
        ReLU2 & x → Linear → ReLU → Linear → ReLU → y \\
        ReLU2-Neg & x → Linear → ReLU → Linear → ReLU → Neg → y \\
        \bottomrule
    \end{tabular}
    \caption{Experiment Model Architectures}
\end{table}

\subsection{Experimental Setup}
We use the MNIST dataset \cite{lecun1998gradient} for its well-understood features and relatively low dimensionality (28x28 pixels), making it suitable for analyzing representational preferences in a controlled setting. As is standard practice, images are normalized to zero mean and unit variance across the dataset.

To minimize confounding factors, we choose a simple training protocol with minimal hyperparameters. We use SGD optimization with learning rate 0.001 and train for 5000 epochs with full-batch updates. Each experiment is repeated 20 times. The loss function is CrossEntropyLoss (which includes LogSoftmax) applied to the final layer's logits.

We evaluate each architecture's performance using three metrics: test accuracy on MNIST, stability (variance across 20 training runs), and statistical significance via paired t-tests between architectures. These metrics enable us to compare both absolute performance and the consistency of learning across different random initializations.

Source code and additional resources can be found in our GitHub repository\footnote{\url{https://github.com/alanoursland/neural_networks_learn_distance_metrics}}.

The performance of each architecture, under the described experimental conditions, is analyzed in the following section. 
