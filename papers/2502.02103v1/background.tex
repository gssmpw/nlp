\section{Background}
\label{sec:background}

This section introduces the theoretical framework for understanding how neural networks represent data through either distance-based or intensity-based methods, providing context for our experimental analysis.

\subsection{Features, Representations, and Distance Metrics}

Neural networks learn features through their internal representations. For the purposes of this paper, we define \textbf{features} as inherent properties of the data that can be quantified statistically, and \textbf{representations} are the compositions of these features learned by a node to generate its output. Traditional intensity-based representations encode features through activation magnitudes, while distance-based representations encode features through proximity to learned prototypes. In our discussion, we primarily consider prototypes as the features of interest, as they provide a structured way to quantify similarity and influence learned representations.

We argue that neural networks fundamentally learn distance features that quantify similarity between data points and learned prototypes. What appear to be intensity-based representations can be reinterpreted as disjunctive sets of distance features, corresponding to Disjunctive Normal Form (DNF) in Boolean algebra \cite{post1921introduction}. For example, an intensity representation for class $a$ implicitly learns $\lnot(b \lor c) = \lnot b \land \lnot c$, where $b$ and $c$ represent distances to other classes. This logical interpretation has connections to work exploring the relationship between neural networks and Boolean formulas \cite{anthony2003boolean}.

In contrast, distance-based representations directly encode proximity to prototypes as conjunctive sets (Conjunctive Normal Form). For class $a$, this simply requires a small distance to prototype $a$, represented logically as $a$. This more directly captures the underlying statistical relationships in the data.

\subsection{Distance vs. Intensity Representations}
\label{subsec:dist-intensity-rep}

Intensity-based interpretations, while lacking precise mathematical definition, are deeply embedded in the foundations of deep learning \cite{goodfellow2016deep}. These interpretations treat larger activations as indicating stronger feature presence, fundamentally shaping how we train and interpret networks. The use of one-hot encoded labels for classification directly encodes this assumption - the correct class should produce the highest activation while all others should be suppressed. This intensity-based paradigm appears throughout modern practice: cross-entropy loss encourages larger activations for correct classes, feature visualization \cite{erhan2009visualizing, olah2017feature} assumes maximum activations represent learned features, and even basic image processing interprets pixel intensities as direct measures of feature strength. However, this pervasive intensity-based interpretation remains an assumption imposed on neural networks rather than an inherent property.

Distance-based representations offer an alternative framework, interpreting smaller activations as indicating similarity to learned prototypes. This aligns with statistical metrics like the Mahalanobis distance \cite{mahalanobis1936generalized}, where the activation $f(x)$ is inversely proportional to the distance between input $x$ and prototypes $\mu$:

\begin{equation}
f(x) = |W(x - \mu)|_p,
\end{equation}

where $W$ is a learned scaling matrix and $p$ denotes the norm. Common activation functions naturally support this view: the absolute value function directly represents distance, while ReLU implicitly encodes distance through the relationship $Abs(x) = ReLU(x) + ReLU(-x)$. This framework emphasizes geometric relationships in the latent space rather than activation magnitudes. This perspective aligns with research on visualizing and understanding the loss landscape of neural networks \cite{li2018visualizing, goodfellow2015qualitatively}.

\subsection{Theoretical Foundations: Mahalanobis Distance}

The Mahalanobis distance provides a statistical foundation for distance-based representations, capturing feature correlations and scales through the covariance matrix $\Sigma$:
\begin{equation}
    D_M(x) = \sqrt{(x - \mu)^T \Sigma^{-1} (x - \mu)},
\end{equation}

Through eigendecomposition $\Sigma = V\Lambda V^T$, this distance can be expressed as:
\begin{equation}
    D_M(x) = \| \Lambda^{-1/2} V^T (x - \mu) \|_2,
\end{equation}
revealing how neural networks naturally approximate this metric: linear layer weights learn the eigenvector transformation $\Lambda^{-1/2} V^T$, the bias learns $-\Lambda^{-1/2} V^T \mu$, and activation functions like Absolute Value model the normalized distance computation \cite{oursland2024interpreting}.

\subsection{Geometric Interpretations of Representations}

Neural networks project input data into a latent space structured by hyperplanes; under a distance-based interpretation, these hyperplanes act as axes of a manifold, with distances to prototypes along these axes defining the representation \cite{montavon2018methods}.

For a hyperplane defined by $y = Wx + b$, where $x$ is $N$-dimensional and $y$ is scalar, its decision boundary occurs where the activation function equals zero (e.g., $0 = Wx + b$). This boundary is uniquely described by $N$ linearly independent points: $N-1$ points on the decision boundary plus one point at ${x=0, y=b}$. These boundary points serve as feature prototypes, encoding relationships between inputs and their latent representations. These prototypes don't necessarily reflect human-perceived similarity, but rather capture relationships defined by the network's learned distance metric.

When $b=0$, hyperplanes must pass through the origin, making it a fixed prototype. This constraint has minimal impact in high dimensions since each hyperplane still intersects $N-2$ independent points on its decision boundary, plus points at the origin and ${x!=0, y!=0}$. While these prototypes are theoretically significant, recovering them becomes computationally intractable as dimensionality increases \cite{donoho2000high}.

\subsection{Activation Functions and Representational Preferences}

Activation functions shape how networks encode representations:

\begin{itemize}
    \item \textbf{ReLU}: $f(x) = \max(0, x)$ is traditionally associated with intensity-based interpretations but can be viewed through a distance lens \cite{nair2010relu, glorot2011deep}. The zero region corresponds to one side of a decision boundary, with positive activations encoding prototype proximity. However, ReLU's tendency to produce "dead neurons" when inputs remain negative can hinder learning. This issue has motivated research into alternative activation functions \cite{he2015delving, ramachandran2017searching, misra2019mish}.
    
    \item \textbf{Absolute Value (Abs)}: $f(x) = |x|$ directly represents distance-based relationships by preserving magnitude information regardless of sign. This symmetry ensures neurons remain active and maintain complete information about distance from decision boundaries.
    
    \item \textbf{Neg Layers}: $f(x) = -x$ transforms positive distance representations into negative intensity representations by inverting activation order.
\end{itemize}

This theoretical foundation frames our experimental analysis, where we systematically investigate how architectural choices, such as activation functions and bias terms, affect representational biases. By probing these factors, we aim to elucidate the fundamental principles driving neural network learning and provide a unified framework for understanding distance-based and intensity-based representations.
