\section{Experimental Results}
\label{sec:results}

We conducted extensive experiments comparing baseline architectures and architectural variants. All experiments were run for 5,000 epochs with 20 independent trials to ensure statistical robustness.

\begin{table}[H]
    \centering
    % \footnotesize
    \begin{tabular}{lcc}
        \toprule
        \textbf{Model} & \textbf{Test Accuracy (\%)} & \textbf{Standard Deviation (\%)}\\
        \midrule
        % Baseline Models
        Abs & 95.87 & 0.22 \\
        ReLU & 96.62 & 0.17 \\
        \midrule
        % Intensity and Distance Variants
        Abs2 & 95.95 & 0.17 \\
        Abs2-Neg & 92.25 & 2.07 \\
        ReLU2 & 56.31 & 19.31 \\
        ReLU2-Neg & 96.46 & 0.17 \\
        \bottomrule
    \end{tabular}
    \caption{Performance metrics across all model variants after 5,000 epochs of training. Results show mean test accuracy, standard deviation, 95\% confidence intervals, and number of independent trials.}
    \label{tab:model_performance}
\end{table}

\subsection{Baseline Performance}
The baseline \texttt{ReLU} and \texttt{Abs} architectures showed strong performance on MNIST, with no statistically significant difference between them (t(38) = 1.14, p = 0.26, Cohen's d = 0.37). This comparable performance suggests that the choice of activation function alone does not significantly impact model effectiveness under standard conditions. These results provide a robust foundation for evaluating our architectural modifications.

\subsection{Intensity Learning Models}

The models constrained to learn intensity representations through the addition of a second activation function, \texttt{ReLU2} and \texttt{Abs2}, exhibited markedly different behaviors. 

\texttt{ReLU2}'s performance degraded catastrophically, showing a substantial drop from the baseline ReLU model (t(38) = -17.33, $p < 0.001$, Cohen's d = 5.56). This dramatic failure aligns with our hypothesis that neural networks may exhibit a bias towards learning distance-based representations. 

In contrast, \texttt{Abs2} maintained performance statistically indistinguishable from the baseline Abs model (t(38) = 1.4967, p = 0.1427, Cohen's d = 0.47). This finding complicates our initial hypothesis, suggesting that the relationship between activation functions and representational biases may be more nuanced than initially theorized.

\subsection{Distance Learning Models}

The models designed to learn distance representations through the Negation layer, \texttt{ReLU2-Neg} and \texttt{Abs2-Neg}, showed contrasting behaviors.

\texttt{ReLU2-Neg} exhibited a remarkable recovery from \texttt{ReLU2}'s catastrophic failure (t(38) = -17.33, p < 0.001, Cohen's d = 5.48), achieving performance statistically comparable to the baseline \texttt{ReLU} (t(38) = -12.78, p < 0.001, Cohen's d = 4.04). This recovery supports our hypothesis that neural networks may be biased towards learning distance-based representations, with the Neg transformation enabling \texttt{ReLU2-Neg} to leverage this bias effectively.

Surprisingly, \texttt{Abs2-Neg} showed significant performance degradation compared to both the baseline \texttt{Abs} (t(38) = -8.81, p < 0.001, Cohen's d = 2.79) and its intensity counterpart, \texttt{Abs2} (t(38) = 8.97, p < 0.001, Cohen's d = 2.84). The markedly higher variability in \texttt{Abs2-Neg}'s performance (SD = 2.56\% vs. 0.17\% for \texttt{Abs2}) further suggests that enforcing distance-based learning through negation may fundamentally interfere with the Abs activation function's learning dynamics.

\subsection{Impact of Bias Exclusion}

We excluded the bias term from the second linear layer to enforce learning through the origin, effectively reducing the dimensionality of the solution space by one. For completeness, we conducted parallel experiments with the bias term included (Table~\ref{tab:biased_performance}).

\begin{table}[H]
    \centering
    % \footnotesize
    \begin{tabular}{lcc}
        \toprule
        \textbf{Model} & \textbf{Test Accuracy (\%)} & \textbf{Standard Deviation (\%)} \\
        \midrule
        Abs\_Bias & 95.23 & 0.16 \\
        ReLU\_Bias & 95.69 & 0.17 \\
        \midrule
        Abs2\_Bias & 95.38 & 0.17 \\
        Abs2\_Neg\_Bias & 90.53 & 2.36 \\
        ReLU2\_Bias & 39.94 & 18.84 \\
        ReLU2\_Neg & 94.92 & 0.18 \\
        \bottomrule
    \end{tabular}
    \caption{Performance metrics of models with bias terms included.}
    \label{tab:biased_performance}
\end{table}

The inclusion of bias terms had minimal impact on the overall patterns observed in our main experiments. \texttt{ReLU2\_Bias} maintained poor performance and high variance, while \texttt{Abs2\_Bias} and \texttt{Abs2\_Neg\_Bias} preserved their relative performance characteristics. These results suggest that the representational biases we observed are robust to the inclusion of bias terms and stem from more fundamental aspects of the architectures.

\subsection{Summary of Findings}

The experiments revealed that seemingly minor architectural changes can significantly impact model performance, yielding both expected and surprising results. The catastrophic failure of \texttt{ReLU2} under intensity constraints aligned with our predictions about distance-based representational bias. However, \texttt{Abs2}'s resilience to these same constraints complicated this narrative. The distance-constrained models further nuanced our understanding: \texttt{ReLU2-Neg}'s recovery to baseline performance supported our distance-bias hypothesis, while \texttt{Abs2-Neg}'s significant underperformance revealed unexpected limitations.

These contrasting behaviors suggest that neural networks' representational capabilities are more nuanced than our initial hypothesis predicted. While networks can adopt both distance- and intensity-based approaches, their success appears highly dependent on the specific architectural configuration, particularly the choice of activation function. This interplay between architecture and representation forms the focus of our subsequent geometric analysis in the Discussion section.
