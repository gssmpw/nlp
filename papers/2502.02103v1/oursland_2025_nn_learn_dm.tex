% oursland2024_nn_learn_dm.tex

\documentclass[11pt]{article}

\usepackage{amsmath, amssymb}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{geometry}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{authblk}
\usepackage{caption}
\usepackage{float}

% Geometry
\geometry{
    a4paper,
    left=25mm,
    right=25mm,
    top=25mm,
    bottom=25mm,
}

\title{Neural Networks Learn Distance Metrics}
\author{Alan Oursland}
\affil{\textit{alan.oursland@gmail.com}}
\date{February 2025}

\begin{document}

\maketitle

\begin{abstract}
    Neural networks may naturally favor distance-based representations, where smaller activations indicate closer proximity to learned prototypes. This contrasts with intensity-based approaches, which rely on activation magnitudes. To test this hypothesis, we conducted experiments with six MNIST architectural variants constrained to learn either distance or intensity representations. Our results reveal that the underlying representation affects model performance. We develop a novel geometric framework that explains these findings and introduce OffsetL2, a new architecture based on Mahalanobis distance equations, to further validate this framework. This work highlights the importance of considering distance-based learning in neural network design.
\end{abstract}
    
\input{introduction}
\input{related_work}
\input{background}
\input{design}
\input{results}
\input{discussion}
\input{conclusion}

\bibliographystyle{plainnat}
\bibliography{references}

\end{document}