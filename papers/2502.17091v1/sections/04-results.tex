


We evaluate 8 models on the \name{} dataset, comprising both open and closed-source models. These included \gpt, \llama{} (8B and 70B), \mistral{} (7B), \mixtral{} (8x7B and 8x22B), and \gemma{} (9B and 27B). We test the instruction-tuned version of these models.
% The complete details about the models can be found in Appendix~\ref{sec:appendix-models}. 
% \gabis{We should be a bit cautious about referring to the appendix too often, may annoy some reviwers. Let's aim to refer to the appendix at most twice I think. Here I think we can remove the reference, as people know these models (we can still leave the details in the appendix}.

%\gabis{Do we have a comparison of instruction-tuned vs. non-insturction-tuned? This was important in Itay's work. We should mention whether they're insturction tuned or not in any case}\gili{Done}


\paragraph{Humans and LLMs are largely more influenced by positive framing applied to negative statements than by negative framing applied to positive statements.} 
This trend is evident in Figure~\ref{fig:models-sentiment-shifts}. For LLMs, all models except \gpt{} exhibit higher ratios of sentiment shifts for statements that were originally negative (red bars) compared to those that were originally positive (green bars). For humans, we count the cases in which the majority voted for a sentiment shift (3 annotators or more). 
%Specifically, for negative statements, nearly 40\% of cases saw unanimous agreement among annotators that the sentiment shifts after positive framing. Conversely, for positive statements, in nearly 40\% of cases, annotators agreed that the dominant sentiment remained unchanged despite the opposite framing. 
This finding aligns with~\citet{tong2021good}, which demonstrated that positive framing is more effective on humans when spatial distance is minimal. In \name{}, all statements are presented in a first-person perspective (e.g., ``I won the highest prize''), creating zero spatial distance from the annotator's perception and thereby amplifying the impact of positive framing.

\input{figures/sentiment_shifts_models}




\paragraph{Model size partially correlates with similarity to human behavior under opposite framings.}
In Figure~\ref{fig:correlation-to-humans}, we present the correlation between each model's behavior and human responses. For the \llama{} and Mistral model families, we observe a trend where larger models with more parameters exhibit higher correlation with human behavior regarding the framing effect. However, the \gemma{} family shows the opposite trend. This discrepancy highlights the need for further investigation into the relationship between model size, the framing effect, and its alignment with human behavior.


% \paragraph{Models most correlated with humans are also highly correlated with other models.}
% As shown in Figure~\ref{fig:correlation-to-humans}, \llama{} 70B and \gemma{} 9B exhibit the strongest correlation with human behavior under the framing effect. In Figure~\ref{fig:heatmap-models}, we analyze the correlations across models and observe that these two leading models also have the highest mean correlation with all other models. \gabis{This requires a bit more detail - it's not directly obvious how this can be A=>B => C, but not A=>C. Also, I think we should try to explain why this finding is important / interesting like we did for previous findings.. Maybe we can say that it's initial evidence that maybe large models gradually shift towards human preferences?}\gili{I actually think this one is not so interesting}
\input{figures/correlation-to-humans}

\paragraph{Our results imply that annotators did not rely on LLMs to complete their tasks.} Recent discussions have raised concerns that annotations obtained via crowdsourcing platforms like Mechanical Turk might not reflect genuine human input, as workers could potentially use LLM-generated responses to complete their HITs~\cite{veselovsky2023artificial}. However, as shown in Figure~\ref{fig:correlation-to-humans}, human annotations are not perfectly correlated with either open-source or closed-source LLM outputs. This strongly supports the conclusion that the annotations in~\name{} are genuine human responses, reflecting actual human behavior.

\paragraph{\gpt{} has the weakest correlation with other models.}
In Figure~\ref{fig:heatmap-models} in the Appendix we show the correlation between each pair of models. While most models exhibit correlation coefficients greater than 0.5 with each other, \gpt{} falls below this threshold with many models, resulting in the lowest average correlation to others. This discrepancy may stem from the fact that \gpt{} is a closed-source model, potentially developed independently of the open-source models that often influence each otherâ€™s design. However, the exact reasons remain unclear due to the proprietary nature of \gpt{}'s training process. 



\paragraph{\gpt{} exhibits the lowest correlation with human behavior.}
In Figure~\ref{fig:correlation-to-humans} we find that \gpt{} exhibits the lowest correlation with human behavior. We find this result surprising, given that \gpt{} is considered SOTA compared to open-source models. This finding highlights a fundamental question about the objectives in LLM development -- specifically, diffrentiate when human-like behavior is desired, and when other factors take precedence.

% in which cases in means being the most human-like and in which we expect it to diverge from human-like behavior.
%Should the strongest-performing LLM also closely resemble human behavior, or should it instead surpass human performance in certain aspects, with human-like behavior not being a primary objective? 
%\gabis{I would elaborate on this point, maybe in a discussion section - what is the importance of our findings with regards to what we should want from models? Can also cite other emerging papers in this area.}




