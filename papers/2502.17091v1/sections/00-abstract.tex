
% \gabis{Isn't this the main finding of our paper? Why do we start with it?}\gili{the framing effect is something that was already shown on models, but not in terms of humans/comparison to humans. I kinda what to be clear that we are not claiming to be the first to show that}. 


Humans are influenced by how information is presented, a phenomenon known as the \emph{framing effect}. Previous work has shown that LLMs may also be susceptible to framing but has done so on synthetic data and did not compare to human behavior.
% \gabisrep{However, it is not clear how similar humans and models \textit{behave}, especially \gabist{not} on real-world data}{In this work we } \gabis{seems very specific, let's work on this sentence together}.
We introduce \name{}, a dataset for evaluating LLM responses to positive and negative framing, in naturally-occurring sentences, and compare humans on the same data. \name{} consists of 1,000 texts, first selecting real-world statements with clear sentiment, then reframing them in either positive or negative light, and lastly, collecting human sentiment annotations. By evaluating eight state-of-the-art LLMs on \name{}, we find that all models exhibit framing effects similar to humans ($r\geq0.57$), with both humans and models being more influenced by positive rather than negative reframing. Our findings benefit model developers, who can either harness framing or mitigate its effects, depending on the downstream application.

% While some applications may benefit from similarity to human's framing effect, such as virtual companions, in some other applications, such as legal advice systems, framing effect may result in harmful behavior. In both cases, understandings LLMs framing can help to mitigate its negative impacts, while utilizing its positive aspects. 
