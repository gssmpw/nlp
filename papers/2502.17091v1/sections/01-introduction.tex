
% humans are sensitive to the way information is presented.

% introduce framing as the way we address framing. say something about political views and how information is represented.

% in this paper we explore if models show similar sensitivity.

% why is it important/interesting.



% thought - it would be interesting to test it on real world data, but it would be hard to test humans because they come already biased about real world stuff, so we tested artificial.


% LLMs have recently been shown to mimic cognitive biases, typically associated with human behavior~\citep{ malberg2024comprehensive, itzhak-etal-2024-instructed}. This resemblance has significant implications for how we perceive these models and what we can expect from them in real-world interactions and decisionmaking~\citep{eigner2024determinants, echterhoff-etal-2024-cognitive}.

The \textit{framing effect} is a well-known cognitive phenomenon, where different presentations of the same underlying facts affect human perception towards them~\citep{tversky1981framing}.
For example, presenting an economic policy as only creating 50,000 new jobs, versus also reporting that it would cost 2B USD, can dramatically shift public opinion~\cite{sniderman2004structure}. 
\input{figures/fig1}


Previous research has shown that LLMs exhibit various cognitive biases, including the framing effect~\cite{lore2024strategic,shaikh2024cbeval,malberg2024comprehensive,echterhoff-etal-2024-cognitive}. However, these either rely on synthetic datasets or evaluate LLMs on different data from what humans were tested on. In addition, comparisons between models and humans typically treat human performance as a baseline rather than comparing patterns in human behavior. 
% \gabis{looks good! what do we mean by ``most studies'' or ``rarely'' can we remove those? or we want to say that we don't know of previous work doing both at the same time?}\gili{yeah the main point is that some work has done each separated, but not all of it together. how about now?}

In this work, we evaluate LLMs on real-world data. Rather than measuring model performance in terms of accuracy, we analyze how closely their responses align with human annotations. Furthermore, while previous studies have examined the effect of framing on decision making, we extend this analysis to sentiment analysis, as sentiment perception plays a key explanatory role in decision-making \cite{lerner2015emotion}. 
%Based on this, we argue that examining sentiment shifts in response to reframing can provide deeper insights into the framing effect. \gabis{I don't understand this last claim. Maybe remove and just say we extend to sentiment analysis?}

% Understanding how LLMs respond to framing is crucial, as they are increasingly integrated into real-world applications~\citep{gan2024application, hurlin2024fairness}.
% In some applications, e.g., in virtual companions, framing can be harnessed to produce human-like behavior leading to better engagement.
% In contrast, in other applications, such as financial or legal advice, mitigating the effect of framing can lead to less biased decisions.
% In both cases, a better understanding of the framing effect on LLMs can help develop strategies to mitigate its negative impacts,
% while utilizing its positive aspects. \gabis{$\leftarrow$ reading this again, maybe this isn't the right place for this paragraph. Consider putting in the conclusion? I think that after we said that people have worked on it, we don't necessarily need this here and will shorten the long intro}


% If framing can influence their outputs, this could have significant societal effects,
% from spreading biases in automated decision-making~\citep{ghasemaghaei2024understanding} to reducing public trust in AI-generated content~\citep{afroogh2024trust}. 
% However, framing is not inherently negative -- understanding how it affects LLM outputs can offer valuable insights into both human and machine cognition.
% By systematically investigating the framing effect,


%It is therefore crucial to systematically investigate the framing effect, to better understand and mitigate its impact. \gabis{This paragraph is important - I think that right now it's saying that we don't want models to be influenced by framing (since we want to mitigate its impact, right?) When we talked I think we had a more nuanced position?}




To better understand the framing effect in LLMs in comparison to human behavior,
we introduce the \name{} dataset (Section~\ref{sec:data}), comprising 1,000 statements, constructed through a three-step process, as shown in Figure~\ref{fig:fig1}.
First, we collect a set of real-world statements that express a clear negative or positive sentiment (e.g., ``I won the highest prize'').
%as exemplified in Figure~\ref{fig:fig1} -- ``I won the highest prize'' positive base statement. (2) next,
Second, we \emph{reframe} the text by adding a prefix or suffix with an opposite sentiment (e.g., ``I won the highest prize, \emph{although I lost all my friends on the way}'').
Finally, we collect human annotations by asking different participants
if they consider the reframed statement to be overall positive or negative.
% \gabist{This allows us to quantify the extent of \textit{sentiment shifts}, which is defined as labeling the sentiment aligning with the opposite framing, rather then the base sentiment -- e.g., voting ``negative'' for the statement ``I won the highest prize, although I lost all my friends on the way'', as it aligns with the opposite framing sentiment.}
We choose to annotate Amazon reviews, where sentiment is more robust, compared to e.g., the news domain which introduces confounding variables such as prior political leaning~\cite{druckman2004political}.


%While the implications of framing on sensitive and controversial topics like politics or economics are highly relevant to real-world applications, testing these subjects in a controlled setting is challenging. Such topics can introduce confounding variables, as annotators might rely on their personal beliefs or emotions rather than focusing solely on the framing, particularly when the content is emotionally charged~\cite{druckman2004political}. To balance real-world relevance with experimental reliability, we chose to focus on statements derived from Amazon reviews. These are naturally occurring, sentiment-rich texts that are less likely to trigger strong preexisting biases or emotional reactions. For instance, a review like ``The book was engaging'' can be framed negatively without invoking specific cultural or political associations. 

 In Section~\ref{sec:results}, we evaluate eight state-of-the-art LLMs
 % including \gpt{}~\cite{openai2024gpt4osystemcard}, \llama{}~\cite{dubey2024llama}, \mistral{}~\cite{jiang2023mistral}, \mixtral{}~\cite{mistral2023mixtral}, and \gemma{}~\cite{team2024gemma}, 
on the \name{} dataset and compare them against human annotations. We find  that LLMs are influenced by framing, somewhat similar to human behavior. All models show a \emph{strong} correlation ($r>0.57$) with human behavior.
%All models show a correlation with human responses of more than $0.55$ in Pearson's $r$ \gabis{@Gili check how people report this?}.
Moreover, we find that both humans and LLMs are more influenced by positive reframing rather than negative reframing. We also find that larger models tend to be more correlated with human behavior. Interestingly, \gpt{} shows the lowest correlation with human behavior. This raises questions about how architectural or training differences might influence susceptibility to framing. 
%\gabis{this last finding about \gpt{} stands in opposition to the start of the statement, right? Even though it's probably one of the largest models, it doesn't correlate with humans? If so, better to state this explicitly}

This work contributes to understanding the parallels between LLM and human cognition, offering insights into how cognitive mechanisms such as the framing effect emerge in LLMs.\footnote{\name{} data available at \url{https://huggingface.co/datasets/gililior/WildFrame}\\Code: ~\url{https://github.com/SLAB-NLP/WildFrame-Eval}}

%\gabist{It also raises fundamental philosophical and practical questions -- should LLMs aim to emulate human-like behavior, even when such behavior is susceptible to harmful cognitive biases? or should they strive to deviate from human tendencies to avoid reproducing these pitfalls?}\gabis{$\leftarrow$ also following Itay's comment, maybe this is better in the dicsussion, since we don't address these questions in the paper.} %\gabis{This last statement brings the nuance back, so I think it contradicts the previous parapgraph where we talked about ``mitigating'' the effect of framing. Also, I think it would be nice to discuss this a bit more in depth, maybe in the discussion section.}





