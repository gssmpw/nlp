
We introduced \name{}, a dataset designed to evaluate how LLMs are affected by different framings, in comparison to humans. The statements in the dataset are start from a clear positive or negative base statement, reframed with an opposite-sentiment suffix or prefix. By collecting human annotations, we quantify the strength of the framing effect for each instance and assess how LLMs react to the same reframing, focusing on sentiment shifts -- cases where the perceived sentiment aligns with the opposite framing sentiment.

As LLMs become increasingly integrated into decision-making systems, it becomes crucial to understand how framing influences their outputs. 
In some applications, e.g., in virtual companions, framing can be harnessed to produce human-like behavior leading to better engagement.
In contrast, in other applications, such as financial or legal advice, mitigating the framing effect can lead to less biased decisions.
In both cases, a better understanding of framing in LLMs can help develop application-appropriate strategies.

We find that LLMs and humans exhibit similar behavior on \name{}, with all tested models achieving a strong correlation  $(r \geq 0.57)$ with human responses. Notably, \gpt{} showed the weakest correlation to humans, despite being widely regarded as a very capable model in other contexts.

We hope this work encourages research into distinguishing between scenarios where human-like behavior is desirable and those where models should surpass human limitations to achieve above-human performance. These insights are essential for developing LLMs that are both interpretable and optimized for their intended applications.