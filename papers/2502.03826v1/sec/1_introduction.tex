\section{Introduction}

\begin{figure*}[th]
    \centering
    \includegraphics[width=0.8\textwidth]{fig/pipeline3.png}
    \caption{This pipeline illustrates the comparison between T2I generation without debiasing and T2I generation with debiasing applied using an LLM for the input prompt ``Ha Long Bay.'' When input text is provided, the LLM outputs the implicit biases that may be present in the images generated from the text and identifies their types. From there, attributes are sampled using a predefined probability distribution (e.g., a uniform distribution), and the input prompt is extended to provide a more detailed textual description. Finally, the generated detailed descriptions are used to produce visually distinct images representing different scenarios, showcasing the diversity achievable through textual input.}
    \label{fig:pipeline}
\end{figure*}

In recent years, Text-to-Image (T2I) and Text-to-Video (T2V) models have rapidly evolved, creating an environment where general users can easily access these technologies online. Models such as Stable Diffusion ~\cite{sd3,sdxl}, Imagen ~\cite{imagen3}, Sora ~\cite{sora}, and Veo2\footnote{https://deepmind.google/technologies/veo/veo-2/} have gained attention for their ability to generate high-quality content in large quantities within a short time, requiring minimal technical expertise. This technological innovation has opened new possibilities across various fields, including marketing, entertainment, and design. However, these generative models are trained on large-scale web datasets, which are known to contain stereotypes and harmful content. Consequently, there is a growing discussion about the risk of AI-generated content reflecting these biases and potentially perpetuating existing social inequalities. Moreover, the recent trend of recycling AI-generated synthetic data as training data increases the risk of iteratively amplifying these biases. From a technical perspective, current mainstream approaches using flow matching~\cite{Lipman_2022, Xingchao_2022} and diffusion models~\cite{sohl2015deep,ddpm} generate content through iterative inference processes. This approach has made the models' latent space more complex compared to previous generative models. Additionally, these models incorporate pre-trained text encoders, where different components may memorize undesirable social biases from different datasets. The increasing complexity and scale of these architectures have made it more challenging to understand the behavior and internal structure of generative models. This ``black box'' nature not only makes it difficult to identify and correct biases and misinformation in generated content but also increases the risk of unexpected outcomes. 
For example, models may produce images that reflect biases related to specific cultures, genders, or occupations when generating images from text. 
Such biases not only mislead users but also risk perpetuating stereotypes and working against the promotion of social equity.

To address these challenges, we propose a novel approach to debiasing T2I models. We formulate the process of the bias appearing in T2I model outputs using a latent variable model and perform inference-time debiasing through latent variable guidance, inspired by classifier-free guidance~\cite{ho2022classifier} using score functions (in Section~\ref{sec:latent_variable_guidance}). The latent variable guidance consists of two steps: 1. LLM-assisted bias detection: We incorporate large language models to dynamically identify potential biases within input prompts, moving beyond the constraints of static predefined attribute sets (in Section\ref{sec:llm}). 2. Attribute sampling based on predetermined probability distributions: We introduce methods for rebalancing sensitive attribute distributions, utilizing approaches such as Boltzmann distribution to promote equitable image generation (in Section\ref{sec:attribute_rebalancing}).
Unlike existing T2I debiasing approaches that require model training or fine-tuning~\cite{Kim_2024,zhang2023iti}, our method can be dynamically applied during inference, enabling flexible adaptation independent of temporal or spatial variations in social norms. Furthermore, while existing works~\cite{D'Inc√†_2024,Chinchure_2023} utilize LLMs for open-ended bias detection in T2I model evaluation, our approach distinguishes itself through rigorous mathematical formulation and practical diversity control via attribute sampling.
Through user studies and non-parametric experiments using the Stable Debias Profession Dataset~\cite{Luccioni_2023} and Parti Prompt Dataset~\cite{yu2022scaling}, we demonstrate that our proposed method significantly enhances the diversity of generated outputs compared to the non-debiased baseline (in Section\ref{sec:evaluation} and Section\ref{sec:results}).