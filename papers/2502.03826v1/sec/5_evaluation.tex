\section{Experimental Protocol}
\label{sec:evaluation}
\subsection{Model and Dataset}
\begin{figure}[t]
    \centering
\includegraphics[width=\linewidth]{fig/instruction.png}
    \caption{Instructions given to the LLM for the bias detecrtion.}
    \label{fig:instruction}
\end{figure}
We utilized Stable Diffusion 3.5-large~\cite{sd3} as our text-to-image (T2I) model and employed GPT-4o~\cite{gpt4} for bias detection as a blackbox model, and DeepSeek-V3~\cite{liu2024deepseek} as an open-sourced model. The LLM receives prompts as illustrated in Figure \ref{fig:instruction}. Through in-context learning techniques, we enhance model performance by exposing it to an exemplar task~\cite{brown2020language}. To evaluate the debiasing performance for occupations, we used the occupation dataset from Stable Bias~\cite{Luccioni_2023} (hereafter referred to as the stable bias profession dataset), which contains 131 occupations sourced from the U.S. Bureau of Labor Statistics (BLS). The dataset composition is detailed in the Appendix A of~\cite{Luccioni_2023}. All input prompts were formatted as ``A portrait photo of [profession]'' to ensure that the T2I model interprets them specifically as occupations rather than other potential meanings. To assess the performance in removing implicit social biases present in prompts beyond occupations, we used the Parti Prompt dataset~\cite{yu2022scaling}, which consists of over 1,600 diverse English prompts designed to comprehensively evaluate text-to-image generation models and test their limitations. For attribute rebalancing, we employed the uniform distribution, as our primary goal was to verify the debiasing capability of our latent variable guidance.

% For experiments involving bias adjustment using employment statistics log-probabilities, we conducted experiments to mitigate gender bias using BLS2022 statistical data for five occupation prompts mentioned in \cite{naik2023social}: ``CEO'', ``doctor'', ``computer programmer'', ``house keeper'', and ``nurse''.

\subsection{Human Evaluation}
For each prompt, nine images are generated using three methods: a baseline method without debiasing, and two LLM-assisted debiasing methods employing GPT-4o and DeepSeek-V3. These images are arranged in a 3 $\times$ 3 grid, and evaluators assess pairs of images based on image quality, prompt reflection, and diversity of generations. Image quality refers to the aesthetic appeal, high resolution, natural appearance, and detailed refinement of the images. Prompt adherence measures the degree to which the generated images reflect the input text. Diversity of generations evaluates the variety of generated results, particularly whether the images avoid stereotypes and fixed patterns. For each criterion, evaluators rate the results on a 5-point scale, ranging from 1 (very poor) to 5 (very good). To facilitate relative comparisons, images generated by different models for the same input prompt are presented in consecutive questions. This comparative evaluation across the three criteria enables a detailed assessment of the proposed methods' relative strengths and limitations. We randomly selected 50 prompts from Stable Bias profession dataset and Parti Prompt dataset. The subset used for the human evaluation is detailed in Table\ref{tab:sd_subset} and Table\ref{tab:pp_subset} in the supplementary materials. Responses were collected from 20 evaluators, ensuring a diverse range of perspectives. 
\subsection{Non-parametric Evaluation}
Quantitative evaluation of generation diversity presents significant challenges. To address this, we adopt the clustering-based evaluation methodology proposed in Stable Bias~\cite{Luccioni_2023}, implementing a nonparametric diversity assessment using k-Nearest Neighbors (kNN)~\cite{fix1985discriminatory}. Specifically, we generate anchor images based on prompts structured as ``a portrait of a [ethnicity] [gender] at work,'' creating nine images for each combination of ethnicity and gender. This analysis employs 18 ethnic labels from Stable Bias and three gender categories: ``male'', ``female'', and ``non-binary'' (detailed ethnic labels are provided in the Appendix A of~\cite{Luccioni_2023}).

For image embeddings, we utilize Google's VertexAI multimodal embedding model\footnote{https://cloud.google.com/vertex-ai/docs/generative-ai/embeddings/get-multimodal-embeddings}, which converts 512 $\times$ 512 images into 1048-dimensional vector representations. For each prompt in the identity dataset, 30 unique images are generated, yielding a total of 54 $\times$ 30 $=$ 1620 images that serve as anchor points for classification. To examine local trends linked to specific professions, we follow the methodology outlined in \cite{naik2023social}, generating 210 images per method for five professions: ``CEO'', ``computer programmer'', ``doctor'', ``nurse'', and ``housekeeper''. The classification results are visualized to uncover potential biases or distinct patterns specific to each profession.

% In addition, to capture global trends across the entire profession dataset, we generate nine images per profession prompt for each method. These classification results provide an overarching perspective on diversity and potential biases in the generated outputs.
