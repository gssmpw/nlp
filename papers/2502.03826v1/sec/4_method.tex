\section{Methodologies}
\label{sec:method}
\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\linewidth]{fig/firefighter.png}
    \caption{Nine images generated by Stable Diffusion~3.5-large~\cite{sd3} without debiasing using the prompt ``A portrait photo of a firefighter''.}
    \label{fig:firefighter}
\end{figure}

In this section, we address the critical issue of social biases in text-to-image (T2I) models. Despite the neutrality of input prompts, these models often generate biased outputs due to imbalances in their training data. Our goal is to mitigate these biases through a systematic and mathematically grounded approach. The subsections are organized as follows: (1) Latent Variable Guidance for Bias Control: We introduce a latent variable formulation to model the image generation process, making explicit how sensitive attributes influence the output. This mathematical formalization is key to understanding and controlling biases. (2) LLM-assisted Bias Detection: We leverage large language models (LLMs) to automatically detect potential biases in input prompts, addressing the limitations of predefined sensitive attribute sets. (3) Attribute Rebalancing: We propose methods to rebalance the distribution of sensitive attributes, using techniques such as Boltzmann distribution and real-world employment statistics to ensure fairer image generation.

The primary problem we aim to solve is the unintended introduction of social biases in T2I models. The bottleneck lies in the implicit completion of prompts by these models, which often reflect societal stereotypes. Our key idea to overcome this challenge is the mathematical formalization of the image generation process, allowing for principled adjustments to the distribution of sensitive attributes. This formalization is crucial for evaluating the effectiveness of our bias mitigation strategies.

\subsection{Latent Variable Guidance for Bias Control}
\label{sec:latent_variable_guidance}
Social biases have been observed in text-to-image (T2I) models, even when input prompts do not explicitly reference sensitive attributes such as race or gender ~\cite{bianchi2023easily}. 
As illustrated in Figure~\ref{fig:firefighter}, the seemingly neutral prompt ``A photo of a firefighter'' can lead Stable Diffusion~3.5-large~\cite{sd3} to generate images that reveal inherent biases; white men appear in all nine images. These biases are due to imbalances in the training data, causing the T2I models to implicitly complete the prompts inappropriately.
%To address this issue, we formalize the image generation process of T2I models in the context of social biases using a latent variable model.

To address the social issue, we propose a latent variable formulation for the image generation process of T2I models.
Our formulation transforms the heuristics for bias mitigation into a statistical modeling framework.
Let $\mathbf{y}$ represent the input text, $\mathbf{x}$ the generated image, and $\mathbf{z}$ the sensitive attribute. The image generation process can then be expressed as a mixture model:
\begin{align}
p(\mathbf{x} \mid \mathbf{y}) = \sum_{z\in Z} p(\mathbf{x} \mid \mathbf{z}=z, \mathbf{y}) \, p(\mathbf{z}=z \mid \mathbf{y}).
\end{align}
In this formulation, we make it explicit how each possible value of the sensitive attribute $\mathbf{z}$ influences the final output. The usefulness of this formulation is to clarify the mechanism by which biases may arise and to offer a direct path for controlling them via principled adjustments to the distribution of sensitive attributes.

To apply the guidance, the score, defined as the gradient of the log probability, is computed. Using Bayes' theorem:
\begin{align}
    p(\mathbf{z}=z \mid \mathbf{x}, \mathbf{y}) = \frac{p(\mathbf{z}=z \mid \mathbf{y})p(\mathbf{x}\mid \mathbf{z}=z, \mathbf{y})}{p(\mathbf{x}\mid \mathbf{y})},
\end{align}
we can derive:
\begin{align}
    \nonumber
    \nabla_{\mathbf{x}} \log &p(\mathbf{x}\mid \mathbf{y}) \\
    &= \sum_{z\in Z} p(\mathbf{z}=z \mid \mathbf{x}, \mathbf{y}) \nabla_{\mathbf{x}}\log p(\mathbf{x}\mid \mathbf{z}=z, \mathbf{y}).
\end{align}
By casting existing heuristics~\cite{friedrich2023fair} in this formal way, we shed light on how bias can be identified and mitigated through explicit, mathematically grounded operations on the model scores.

Since computing the posterior distribution $p(\mathbf{z}=z \mid \mathbf{x}, \mathbf{y})$ of the sensitive attribute $z$ is challenging, we assume conditional independence between $\mathbf{x}$ and $\mathbf{z}$ given $\mathbf{y}$. This means that the distribution of $z$ depends only on the input text $\mathbf{y}$ and does not change with the observation of the generated image $\mathbf{x}$:
\begin{align}
    p(\mathbf{z}=z \mid \mathbf{x}, \mathbf{y}) = p(\mathbf{z}=z \mid \mathbf{y}).
\end{align}
Under this assumption, we can simplify the score as:
\begin{align}
    \nonumber
    \nabla_{\mathbf{x}} \log &p(\mathbf{x}\mid \mathbf{y}) \\ 
    &= {\sum_{z\in Z} p(\mathbf{z}=z \mid \mathbf{y}) \nabla_{\mathbf{x}}\log p(\mathbf{x}\mid \mathbf{z}=z, \mathbf{y})}. \label{eq:score}
\end{align}
In practice, computing the sum over all possible values of $z$ is computationally expensive, particularly when dealing with a large space of sensitive attributes. To address this challenge, we use Monte Carlo sampling from $p(\mathbf{z} \mid \mathbf{y})$. Specifically, we approximate the expectation in Equation~\eqref{eq:score} using a finite number of samples.
For the simplest case using a single sample, this becomes:
\begin{align}
    \nonumber
    \nabla_{\mathbf{x}} \log p(\mathbf{x}\mid \mathbf{y})
    &\approx
    \nabla_{\mathbf{x}} \log p\bigl(\mathbf{x}\mid \widetilde{\mathbf{z}}, \mathbf{y}\bigr), \\
    \ \text{where} \ 
    & \widetilde{\mathbf{z}} \sim p(\mathbf{z} \mid \mathbf{y}).
\end{align}

While this single-sample approach provides an unbiased estimate of the true score, it may have higher variance than the exact summation. The variance can be reduced by using additional samples, though this comes at the cost of increased computation time. This trade-off between computational efficiency and estimation accuracy is an important consideration when implementing this approach in practice.

To model $p(\mathbf{x} \mid \mathbf{z}=z, \mathbf{y})$, we append the suffix ``$\mathbf{z}$: $z$'' to the end of the input prompt $\mathbf{y}$. For instance, given $\mathbf{y}$ = ``a portrait photo of a computer programmer'' and $z$ = ``non-binary'' from the set $Z$ = \{``male'', ``female'', ``non-binary''\}, the modified input prompt becomes ``a portrait photo of a computer programmer, gender: non-binary''. 


% \begin{algorithm}
% \caption{Sampling from debiased distributions using latent variable guidance}
% \label{alg:cf_guidance}
% \begin{algorithmic}[1]
% \REQUIRE $\mathbf{y}$: input text prompt
% \STATE Sample sensitive attribute $\widetilde{\mathbf{z}} \sim p(\mathbf{z} \mid \mathbf{y})$
% \STATE Sample initial latent $\mathbf{x}_0 \sim \mathcal{N}(\mathbf{0}, \mathbf{I})$
% \FOR{$t = 1, \ldots, T$}
%   \STATE Compute score $\nabla_{\mathbf{x}} \log p(\mathbf{x} \mid \widetilde{\mathbf{z}}, y)$ 
%   \STATE Update latent using score-based guidance:
%   \STATE $\mathbf{x}_{t+1} = \mathbf{x}_t + w \cdot \nabla_{\mathbf{x}} \log p(\mathbf{x} \mid \widetilde{\mathbf{z}}, \mathbf{y})$
% \RETURN $\mathbf{x}_{T+1}$
% \end{algorithmic}
% \end{algorithm}

\subsection{LLM-assisted bias detection}
\label{sec:llm}
To implement Latent Variable Guidance, we need to define a candidate set of biases $Z$. The simplest approach is to predefine a closed set of biases such as race and gender; however, this approach has several limitations.

\noindent \textbf{Computational Challenges.} The diversity of input prompts is virtually infinite, and the predefined set of sensitive attributes $Z$ can only handle a limited subset of these cases appropriately. It is practically impossible to predefine a suitable $Z$ for every possible prompt in advance.

\noindent \textbf{Incomplete Representation.} Defining the sensitive attribute set $Z$ manually in a rule-based manner may fail to fully capture the diversity and context of the real world. This approach may also overlook biases embedded in the input text that are beyond human recognition.

To address these challenges, we leverage large language models (LLMs)~\cite{gpt4,liu2024deepseek} to automatically detect open biases in the input text, following existing bias detection methods~\cite{D'Inc√†_2024,Chinchure_2023}. Specifically, we use the LLM to predict the set of possible sensitive attributes $Z$ from the input text $\mathbf{y}$. LLMs are prompted to output a set of sensitive attributes that are likely to appear in images generated by T2I models with the input text in a JSON format. This approach allows us to handle a broader range of input prompts and to detect biases that may not be apparent to human annotators.

\subsection{Arrtibute rebalancing}
\label{sec:attribute_rebalancing}
When we have a set of sensitive attributes $Z$ that commonly appear - either predefined as a collection of attributes and diversity metrics to consider during generation, or automatically defined by LLMs identifying potential biases from input prompts - we can formulate $p(\mathbf{z}=z \mid \mathbf{y})$ to perform bias-mitigated sampling.

We model $p(\mathbf{z}=z \mid \mathbf{y})$ using a Boltzmann distribution, which allows us to reformulate the conditional probability modeling as a similarity function design problem. Specifically, we define the distribution using a similarity function $s(\mathbf{y}, \mathbf{z})$ as follows:
\begin{align}
\label{eq:boltzmann}
p(\mathbf{z} = z \mid \mathbf{y}) = \frac{\exp \left( {s(\mathbf{y}, z)}/{T} \right)}{\sum_{z'} \exp \left( {s(\mathbf{y}, z')}/{T} \right)},
\end{align}
where $T$ is the temperature parameter. A larger $T$ increases the randomness of $p(\mathbf{z} = z \mid \mathbf{y})$, smoothing the distribution and bringing it closer to a uniform distribution, even if $s$ has learned biases from the training dataset. For example, when $\mathbf{y}$ is ``a photo of a firefighter'' and $\mathbf{z}$ represents gender, $s$ may learn stereotypical biases from the training data such that $s(\mathbf{y}, \mathbf{z}=\text{``male''})$ takes unfairly larger values compared to $s(\mathbf{y}, \mathbf{z}=\text{``female''})$ or $s(\mathbf{y}, \mathbf{z}=\text{``non-binary''})$. However, by increasing the temperature parameter $T$, we can mitigate such biased associations learned by the similarity function.

\noindent \textbf{Uniform distribution.} 
One simple approach to diversify the output is taking the limit as $T \to \infty$, where the conditional probability~\eqref{eq:boltzmann} further simplifies to:
\begin{align}
    p(\mathbf{z} = z \mid \mathbf{y}) = \frac{1}{|Z|},
\end{align}
which corresponds to simply mixing the scores $\nabla_{\mathbf{x}} \log p(\mathbf{x}\mid \mathbf{z}=z, \mathbf{y})$ with equal proportions across all possible values of $z$. This formulation coincides with that of Fair Diffusion~\cite{friedrich2023fair}.

\noindent \textbf{Employment statistics log-probabilities} 
Research has shown that T2I models tend to exaggerate demographic stereotypes beyond what we observe in real-world distributions across various sensitive attributes ~\cite{naik2023social}. One way to address this issue is to incorporate real-world statistical data into our similarity function, ensuring that the generated image distributions are at least as balanced as real-world demographics. Let's consider an example where we want to generate fair images for the prompt $\mathbf{y} = $ ``a photo of a CEO'', taking gender as our sensitive attribute where $Z = \{\text{``male''}, \text{``female''}, \text{``non-binary''}\}$. We can leverage actual labor statistics on gender distribution among CEOs to define our similarity function as follows:
{\small
\begin{align*}
s(\mathbf{y}=\text{``CEO''}, \mathbf{z}=\text{``male''})      &= \log (\text{prop. of male CEOs}), \\
s(\mathbf{y}=\text{``CEO''}, \mathbf{z}=\text{``female''})    &= \log (\text{prop. of female CEOs}), \\
s(\mathbf{y}=\text{``CEO''}, \mathbf{z}=\text{``non-binary''}) &= \log (\text{prop. of non-binary CEOs}).
\end{align*}
}
This formulation enables the control of biases in generated outcomes to align with real-world distributions.

However, it is worth noting that real-world occupational distributions often reflect systemic biases and unequal access to opportunities, shaped by historical and societal factors such as limited access to education or workplace discrimination. These disparities highlight that real-world distributions are not inherently fair. In such cases, adjusting the temperature parameter $T$ can help generate a probability distribution that is both more diverse and better aligned with principles of fairness and inclusivity.