\section{Related Work}
\label{sec:related_works}
\noindent \textbf{Text-to-image generative models.} The generative capabilities of text-to-image models~\cite{imagen3,sd3,chen2025pixart} have improved dramatically through several key developments: training on large-scale text-image pairs~\cite{schuhmann2022laion,Chen_2023}, architectural improvements~\cite{peebles2023scalable} from UNet~\cite{ronneberger2015u} to Transformer-based designs~\cite{AttentionAllYouNeed,Dosovitskiy_2020}, and theoretical advancements from diffusion models~\cite{sohl2015deep,ddpm,Song_2020} to flow matching~\cite{Lipman_2022,Xingchao_2022,lipman2024flowmatchingguidecode}. Furthermore, recent fine-tuning approaches have significantly reduced generation time by minimizing the required number of inference steps~\cite{liu2023instaflow,chen2024pixartdeltafastcontrollableimage,sauer2024fast,Sauer_2023,Yang_2023}. A distinctive characteristic of state-of-the-art models is their use of separately trained components - Variational Auto Encoders~\cite{kingma_2014} for image compression, text encoders~\cite{raffel2020exploring,clip}, and flow model backbones - each trained on different datasets.

\noindent \textbf{Social bias in text-to-image generative models.} Numerous studies have investigated biases in text-to-image models. Research such as~\cite{Ghosh_2023,Wu_2023,bianchi2023easily} highlights the biases embedded in generated outputs for seemingly neutral input prompts that lack explicit identity- or demographic-related terms. Other works, including~\cite{Cho_2022,Luccioni_2023} predefine sensitive human attributes and analyze biases in outputs generated from occupational input prompts. \cite{naik2023social} provides broader analyses, including comparative studies with statistical data or image search results, as well as spatial analyses of generated images. \cite{Wang_2023} applies methods from social psychology to explore implicit and complex biases related to race and gender. Furthermore,~\cite{Luccioni_2023} introduces an interactive bias analysis tool leveraging clustering methods. Lastly,~\cite{Chen_2024} examines the potential for AI-generated images to perpetuate harmful feedback loops, amplifying biases in AI systems when used as training data for future models. \cite{D'Inc√†_2024,Chinchure_2023} employ large language models to detect open-ended biases in text-to-image models where users do not have to provide predefined bias attributes.

\noindent \textbf{Bias mitigation in text-to-image generative models.} Recent research has proposed various approaches to address and mitigate biases in text-to-image models. One significant direction focuses on training-time solutions, such as time-dependent importance reweighting~\cite{Kim_2024}, which addresses dataset bias by introducing a precise time-dependent density ratio for diffusion models. This approach minimizes error propagation in generative learning and theoretically ensures convergence to an unbiased distribution. Another approach tackles bias mitigation post-deployment through instruction-based methods. Fair Diffusion~\cite{friedrich2023fair} demonstrates the ability to control and adjust biases based on human instructions without requiring data filtering or additional training. Furthermore, ITI-GEN~\cite{zhang2023iti} introduces a novel reference image-based approach for inclusive generation, arguing that visual references can more effectively represent certain attributes than textual descriptions. Their method learns prompt embeddings to ensure uniform distribution across desired attributes without requiring model fine-tuning, making it computationally efficient to implement in existing systems. 