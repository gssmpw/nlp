\section{Discussions and Conclusions}
\label{sec:conclusion}
In this paper, we have presented a novel approach to debiasing Text-to-Image (T2I) models by leveraging large language models (LLMs) for bias detection and attribute rebalancing. Our method addresses the challenges posed by the inherent biases in large-scale web datasets used to train generative models. By dynamically identifying potential biases within input prompts and rebalancing sensitive attribute distributions, our approach promotes equitable image generation without the need for model retraining or fine-tuning.

Our experiments, conducted using the Stable Debias Profession Dataset and Parti Prompt Dataset, demonstrate that our proposed method significantly enhances the diversity of generated outputs compared to non-debiased baselines. This improvement is achieved through the rigorous mathematical formulation of latent variable guidance and practical diversity control via attribute sampling.

\paragraph{Ethics Statement} 
The ethical implications of AI-generated content are profound, particularly in the context of perpetuating social biases and stereotypes. Our work aims to mitigate these risks by promoting fairness and diversity in T2I model outputs. However, it is essential to continuously monitor and evaluate the impact of our debiasing methods to ensure they align with evolving social norms and ethical standards.

\paragraph{Limitations and Future Work} 
\begin{figure}[tbp]
    \centering
    \includegraphics[width=\linewidth]{fig/inspiration_vs.png}
    \caption{Nine output images generated by the non-debiased baseline model (left) and the GPT-4o model (right) for the prompt ``inspiration'' from the Parti Prompt dataset.}
    \label{fig:inspiration_comp}
\end{figure}
\begin{figure}[tbp]
    \centering
    \includegraphics[width=\linewidth]{fig/inspiration_bias.png}
    \caption{Detected biases by GPT-4o for the prompt ``inspiration'' from the Parti Prompt dataset.}
    \label{fig:inspiration_bias}
\end{figure}

While our approach shows promising results, it is not without limitations. The effectiveness of bias detection relies heavily on the capabilities of the LLMs used, which may themselves be subject to biases. The debiasing results of DeepSeek-V3 for the prompt ``nurse'' provide a striking example of this phenomenon. When LLMs' inherent social biases cause certain sensitive attributes to be undervalued or overlooked during bias detection, attribute rebalancing may fail to sample these attributes, potentially amplifying biases in the generated output. Furthermore, we observe that the biases detected by LLMs are highly susceptible to the exemplar instances used during in-context learning.
Figure~\ref{fig:inspiration_comp} shows the generation results from both the non-debiased baseline model and the GPT-4o model for the prompt ``inspiration'' from the Parti Prompt dataset, while Figure~\ref{fig:inspiration_bias} illustrates the biases detected by GPT4o for this prompt. Although the prompt "inspiration" has minimal inherent correlation with bias attributes such as race or gender, the model's bias detection was influenced by the input prompts (Figure~\ref{fig:instruction}), resulting in generated images that fail to reflect the original input text ``inspiration''.


Future work will explore more comprehensive debiasing techniques, including the integration of additional bias detection mechanisms and the development of more robust attribute sampling methods. We also plan to extend our approach to other generative models, such as Text-to-Video (T2V), to further enhance the fairness and diversity of AI-generated content across different modalities.
