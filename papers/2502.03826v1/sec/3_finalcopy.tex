\section{Preliminary}
\label{sec:preliminary}
In this section, we first introduce the mathematical formulation of flow-based text-to-image generative models~\cite{Xingchao_2022,Lipman_2022}, which forms the foundation of modern T2I systems~\cite{sd3,sdxl,imagen3,imagen}. We then describe classifier-free guidance~\cite{ho2022classifier}, a key technique to control the generation process through text conditioning.

\subsection{Flow-based text-to-image generative models}
In state-of-the-art T2I models~\cite{sd3}, the image generation process is modeled by learning, through a neural network, a flow $\psi$ that generates a probability path $(p_t)_{0\le t\le 1}$ bridging the source distribution $p_0$ and the target distribution $p_1$ ~\cite{Xingchao_2022,Lipman_2022}. This framework encompasses diffusion models~\cite{sohl2015deep,ddpm} as a special case. In particular, a commonly used formulation sets a Gaussian distribution as the source: $p_0 = \mathcal{N}(\mathbf{0}, \mathbf{I})$ and a delta distribution centered on a sample $\mathbf{x}_1$ from the data distribution $q$ as the target: $p_1 = \delta_{\mathbf{x}_1}$.
Then, it incorporates an affine conditional flow $\psi_t(\mathbf{x} | \mathbf{x}_1) = a_t \mathbf{x}_1 + b_t \mathbf{x}$ with the boundary condition $(a_0, b_0) = (0, 1),\ (a_1, b_1) = (1, 0)$ to bridge them. The neural network typically approximates quantities such as velocity fields, $x_0$ prediction or $x_1$ prediction. In this modeling, these quantities can be viewed as affine transformations of the marginal probability path score $\nabla_{\mathbf{x}} \log p_t(\mathbf{x})$.

\subsection{Classifier-free guidance in flow-based models}
Classifier-free guidance~\cite{ho2022classifier} is a method for sampling from a model conditioned by a text input $\mathbf{y}$ by guiding an unconditional image generation model modeled using the score $\nabla_{\mathbf{x}} \log p_t(\mathbf{x})$. This enables the sampling from
\[
q_w(\mathbf{x}, \mathbf{y}) \propto q(\mathbf{x})q(\mathbf{y}|\mathbf{x})^w \propto q(\mathbf{x})^{1-w}q(\mathbf{x}|\mathbf{y})^w
\]
where $w \in \mathbb{R}$ is the guidance scale typically used with $w > 1$. The score satisfies
\[
\nabla_{\mathbf{x}} \log q_w(\mathbf{x}, \mathbf{y}) = (1-w)\nabla_{\mathbf{x}} \log q(\mathbf{x}) + w\nabla_{\mathbf{x}} \log q(\mathbf{x}|\mathbf{y})
\]
so by training the network to learn both the unconditional score $\nabla_{\mathbf{x}} \log q(\mathbf{x})$ and conditional score $\nabla_{\mathbf{x}} \log q(\mathbf{x}|\mathbf{y})$, flexible sampling from the conditional distribution can be achieved through a weighted sum of the network outputs.