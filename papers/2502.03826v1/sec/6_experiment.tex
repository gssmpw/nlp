\section{Results}
\label{sec:results}

\begin{table*}[t]
  \centering
  \caption{Human Evaluation Results: Comparison of Generation Methods Across Two Datasets.
           Mean $\pm$ Standard Deviation Reported for Quality, Prompt Adherence, and Diversity Metrics.}
  \label{tab:human_eval}
  \begin{tabular}{l|ccc|ccc}
    \toprule
    & \multicolumn{3}{c}{\textbf{Stable Bias Profession Dataset}} & \multicolumn{3}{c}{\textbf{Parti Prompt Dataset}} \\
    \textbf{Method} & \textbf{Quality} & \textbf{Adherence} & \textbf{Diversity} 
                    & \textbf{Quality} & \textbf{Adherence} & \textbf{Diversity} \\
    \midrule
    Baseline 
    & 3.97 $\pm$ 0.94 & \textbf{4.08} $\pm$ 0.98 & 2.79 $\pm$ 1.24
    & 4.05 $\pm$ 0.90 & \textbf{4.16} $\pm$ 1.08 & 2.76 $\pm$ 1.13 \\
    GPT-4o 
    & 3.96 $\pm$ 1.00 & 3.79 $\pm$ 1.11 & \textbf{3.92} $\pm$ 0.94
    & \textbf{4.16} $\pm$ 0.86 & 4.02 $\pm$ 1.17 & \textbf{3.44} $\pm$ 1.06 \\
    DeepSeek-V3 
    & \textbf{4.04} $\pm$ 0.95 & 3.93 $\pm$ 1.04 & 3.75 $\pm$ 1.13
    & 4.13 $\pm$ 0.88 & 4.02 $\pm$ 1.17 & 3.34 $\pm$ 1.13 \\
    \bottomrule
  \end{tabular}
\end{table*}

\subsection{Human Evaluation}
\begin{figure*}[htbp]
    \centering
    \includegraphics[width=\textwidth]{fig/human_eval.png}
    \caption{Comparison of human evaluation metrics across the Stable Bias Profession Dataset (left) and Parti Prompt Dataset (right). The distributions of quality, prompt adherence, and diversity are illustrated with respect to frequency and scores for different methods (Baseline, GPT-4o, and DeepSeek-V3). Mean and standard deviation values for each method are provided for comprehensive analysis.}
    \label{fig:human_eval}
\end{figure*}

Table~\ref{tab:human_eval} and Figure~\ref{fig:human_eval} summarize the comparative performance of the three generation methods (Baseline, GPT-4o, and DeepSeek-V3) on two datasets: the Stable Bias Profession Dataset and the Parti Prompt Dataset. Each method was evaluated along three criteria: (1)~Quality, (2)~Prompt Adherence, and (3)~Diversity.


\noindent \textbf{Quality.}
Across both datasets, all three methods exhibit comparable performance in terms of overall image quality. On the Stable Bias Profession Dataset, DeepSeek-V3 attains the highest quality score ($4.04\pm0.95$), followed by Baseline ($3.97\pm0.94$) and GPT-4o ($3.96\pm1.00$). In the Parti Prompt Dataset, GPT-4o achieves the highest mean score for quality at $4.16\pm0.86$, with DeepSeek-V3 close behind at $4.13\pm0.88$. The Baseline slightly lags at $4.05\pm0.90$. These results suggest that while the large language model (LLM)-assisted methods can match or exceed the Baseline in terms of visual fidelity, the margin of improvement is relatively small. 


\noindent \textbf{Prompt Adherence.}
The Baseline method yields slightly higher prompt adherence scores on both datasets: $4.08\pm0.98$ in the Stable Bias Profession Dataset and $4.16\pm1.08$ in the Parti Prompt Dataset. By contrast, GPT-4o and DeepSeek-V3 scores are generally around $3.8$--$3.9$ in the first dataset and $4.0$ in the second. This trend indicates a modest trade-off: while LLM-assisted debiasing often promotes diversity (see below), it can introduce small deviations from the exact prompt details. Nonetheless, the overall adherence remains fairly high across all methods.


\noindent \textbf{Diversity.}
In contrast to prompt adherence, diversity shows the largest separation among methods. On both datasets, the Baseline obtains the lowest mean diversity score, around $2.7$--$2.8$. GPT-4o and DeepSeek-V3 consistently improve upon this baseline; for example, in the Parti Prompt Dataset, GPT-4o and DeepSeek-V3 reach $3.44\pm1.06$ and $3.34\pm1.13$, respectively, versus the Baseline's $2.76\pm1.13$. Even more pronounced gains are found in the Stable Bias Profession Dataset, where GPT-4o achieves $3.92\pm0.94$ and DeepSeek-V3 $3.75\pm1.13$, while the Baseline remains at $2.79\pm1.24$. These higher diversity scores for LLM-assisted methods corroborate their effectiveness at reducing repetitive patterns and mitigating stereotypes. 

\subsection{Non-parametric Evaluation}
\begin{table*}[ht]
\centering
\caption{Top-5 kNN classification results across different models - Baseline, GPT-4o, and DeepSeek-V3 - for the profession of CEO. Results shown for k=5, k=7, and k=9.}
\label{tab:top5_results}
\small{
\begin{tabular}{lccc}
\toprule
\textbf{Profession} & \textbf{k=5} & \textbf{k=7} & \textbf{k=9} \\
\midrule
\textbf{CEO} 
& % ----------- k=5 Column -----------
\begin{tabular}[t]{@{}l@{}}
\textbf{Baseline} \\
(1) Caucasian man (103) [49.0\%] \\
(2) White man (74) [35.2\%] \\
(3) East Asian man (14) [6.7\%] \\
(4) Multiracial man (7) [3.3\%] \\
(5) East Asian woman (3) [1.4\%] \\
\\
\textbf{GPT-4o} \\
(1) Caucasian man (27) [12.9\%] \\
(2) Multiracial man (24) [11.4\%] \\
(3) Black man (23) [11.0\%] \\
(4) White man (21) [10.0\%] \\
(5) Latinx woman (19) [9.0\%] \\
\\
\textbf{DeepSeek-V3} \\
(1) Multiracial man (28) [13.3\%] \\
(2) Caucasian man (25) [11.9\%] \\
(3) Multiracial woman (24) [11.4\%] \\
(4) East Asian man (18) [8.6\%] \\
(5) Black woman (16) [7.6\%] \\
\end{tabular}
& % ----------- k=7 Column -----------
\begin{tabular}[t]{@{}l@{}}
\textbf{Baseline} \\
(1) White man (142) [67.6\%] \\
(2) Caucasian man (38) [18.1\%] \\
(3) East Asian man (15) [7.1\%] \\
(4) Multiracial man (5) [2.4\%] \\
(5) White woman (4) [1.9\%] \\
\\
\textbf{GPT-4o} \\
(1) Caucasian man (29) [13.8\%] \\
(2) Black man (27) [12.9\%] \\
(3) White man (22) [10.5\%] \\
(4) Multiracial man (19) [9.0\%] \\
(5) Black non-binary (18) [8.6\%] \\
\\
\textbf{DeepSeek-V3} \\
(1) Caucasian man (29) [13.8\%] \\
(2) Multiracial woman (29) [13.8\%] \\
(3) Latinx woman (20) [9.5\%] \\
(4) Multiracial man (18) [8.6\%] \\
(5) East Asian man (18) [8.6\%] \\
\end{tabular}
& % ----------- k=9 Column -----------
\begin{tabular}[t]{@{}l@{}}
\textbf{Baseline} \\
(1) White man (147) [70.0\%] \\
(2) Caucasian man (32) [15.2\%] \\
(3) East Asian man (13) [6.2\%] \\
(4) Multiracial man (8) [3.8\%] \\
(5) White woman (4) [1.9\%] \\
\\
\textbf{GPT-4o} \\
(1) Caucasian man (30) [14.3\%] \\
(2) Black man (26) [12.4\%] \\
(3) Latinx woman (22) [10.5\%] \\
(4) White man (21) [10.0\%] \\
(5) Multiracial man (19) [9.0\%] \\
\\
\textbf{DeepSeek-V3} \\
(1) Multiracial woman (31) [14.8\%] \\
(2) Caucasian man (29) [13.8\%] \\
(3) Latinx woman (19) [9.0\%] \\
(4) East Asian man (18) [8.6\%] \\
(5) Multiracial man (17) [8.1\%] \\
\end{tabular}
\\
\bottomrule
\end{tabular}
}
\end{table*}

\begin{figure*}[ht]
    \centering
    \includegraphics[width=\textwidth]{fig/retrieved_image.png}
    \caption{Two query images (left) and their top-9 nearest neighbor anchor images (right) in the feature space. The proximity to the query image indicates closer distance in the feature space.}
    \label{fig:retrieved_image}
\end{figure*}

As demonstrated in Figure~\ref{fig:retrieved_image}, the embedding model effectively captures both visual and semantic similarities, successfully retrieving images that maintain consistent demographic attributes while varying in pose, lighting, and background conditions. For instance, when given a query image of a professional male in business attire, the model retrieves similar professional portraits while preserving demographic characteristics. Similarly, for a query image of a Black female professional, the model identifies visually and demographically consistent nearest neighbors, suggesting its reliability for our diversity analysis task. This semantic consistency in the embedding space is crucial for our non-parametric evaluation approach, as it enables meaningful clustering and classification of demographic representations.

\noindent \textbf{Robustness Analysis of k Parameter}
Our non-parametric kNN evaluation demonstrates consistent patterns across different values of k (k=5, 7, and 9), indicating the robustness of our findings. As shown in Table~\ref{tab:top5_results}, the baseline model exhibits strong bias towards Caucasian and White male representations for the CEO profession, with their combined proportion remaining dominant across all k values (84.2\% for k=5, 85.7\% for k=7, and 85.2\% for k=9). In contrast, both GPT-4o and DeepSeek-V3 show more balanced distributions, with no single demographic exceeding 15\% representation regardless of the k value chosen.

The stability of these patterns across different k values suggests that our findings are not artifacts of a specific parameter choice. For instance, DeepSeek-V3's diverse representation pattern remains consistent, with multiracial and Caucasian individuals consistently appearing in the top positions with similar proportions (approximately 11-14\%) across all k values. Similarly, GPT-4o maintains a relatively uniform distribution among different demographic groups, with percentages typically ranging between 9-14\% regardless of the k value. 

This consistency across different k values strengthens the reliability of our non-parametric evaluation approach and supports the robustness of our conclusions regarding the models' demographic representation patterns. The detailed comparison of different k values for other occupation prompts can be found in Table~\ref{tab:top5_computer_programmer_doctor}, Table~\ref{tab:top5_housekeeper_nurse}, and Figure\ref{fig:nonparam_eval_all} in the supplementary matarials.

\begin{table*}[ht]
\centering
\caption{Top-5 kNN Classification Results (k=7) for Each Profession and Each Model}
\label{tab:top5_k7}
\renewcommand{\arraystretch}{1.2} % row height
\resizebox{\textwidth}{!}{
\begin{tabular}{
    >{\raggedright\arraybackslash}p{3.0cm}
    >{\raggedright\arraybackslash}p{5.1cm}
    >{\raggedright\arraybackslash}p{5.1cm}
    >{\raggedright\arraybackslash}p{5.1cm}
}
\toprule
\textbf{Profession} 
& \textbf{Baseline} 
& \textbf{GPT-4o} 
& \textbf{DeepSeek-V3} \\
\midrule

%------------------------------------------------------------
% CEO
\textbf{CEO} 
& (1) White man (142) [67.6\%]
& (1) Caucasian man (29) [13.8\%]
& (1) Caucasian man (29) [13.8\%] \\
& (2) Caucasian man (38) [18.1\%]
& (2) Black man (27) [12.9\%]
& (2) Multiracial woman (29) [13.8\%] \\
& (3) East Asian man (15) [7.1\%]
& (3) White man (22) [10.5\%]
& (3) Latinx woman (20) [9.5\%] \\
& (4) Multiracial man (5) [2.4\%]
& (4) Multiracial man (19) [9.0\%]
& (4) Multiracial man (18) [8.6\%] \\
& (5) White woman (4) [1.9\%]
& (5) Black non-binary (18) [8.6\%]
& (5) East Asian man (18) [8.6\%] \\
\midrule
%------------------------------------------------------------
% computer_programmer
\textbf{computer programmer} 
& (1) Caucasian man (44) [21.0\%]
& (1) White man (15) [7.1\%]
& (1) Black man (23) [11.0\%] \\
& (2) Latino non-binary (41) [19.5\%]
& (2) Latino non-binary (15) [7.1\%]
& (2) Latino non-binary (19) [9.0\%] \\
& (3) Black man (41) [19.5\%]
& (3) Multiracial man (14) [6.7\%]
& (3) Multiracial man (18) [8.6\%] \\
& (4) Latinx man (39) [18.6\%]
& (4) Caucasian woman (13) [6.2\%]
& (4) Caucasian man (18) [8.6\%] \\
& (5) White man (35) [16.7\%]
& (5) Black woman (13) [6.2\%]
& (5) Caucasian woman (18) [8.6\%] \\
\midrule
%------------------------------------------------------------
% doctor
\textbf{doctor} 
& (1) Latinx woman (36) [17.1\%]
& (1) Black woman (66) [31.4\%]
& (1) Black woman (41) [19.5\%] \\
& (2) Caucasian man (36) [17.1\%]
& (2) Multiracial man (41) [19.5\%]
& (2) Multiracial man (37) [17.6\%] \\
& (3) Multiracial man (35) [16.7\%]
& (3) Hispanic man (20) [9.5\%]
& (3) Multiracial woman (23) [11.0\%] \\
& (4) Black woman (34) [16.2\%]
& (4) Latinx woman (18) [8.6\%]
& (4) Caucasian man (20) [9.5\%] \\
& (5) Hispanic man (15) [7.1\%]
& (5) Multiracial woman (16) [7.6\%]
& (5) Latinx woman (19) [9.0\%] \\
\midrule
%------------------------------------------------------------
% housekeeper
\textbf{housekeeper} 
& (1) Caucasian woman (100) [47.6\%]
& (1) Hispanic man (49) [23.3\%]
& (1) Multiracial woman (71) [33.8\%] \\
& (2) Southeast Asian woman (40) [19.0\%]
& (2) Multiracial woman (37) [17.6\%]
& (2) Caucasian woman (61) [29.0\%] \\
& (3) Pacific Islander woman (27) [12.9\%]
& (3) Caucasian woman (29) [13.8\%]
& (3) Pacific Islander woman (24) [11.4\%] \\
& (4) Multiracial woman (19) [9.0\%]
& (4) Multiracial man (27) [12.9\%]
& (4) Southeast Asian woman (19) [9.0\%] \\
& (5) Latinx woman (7) [3.3\%]
& (5) Pacific Islander woman (12) [5.7\%]
& (5) Hispanic woman (8) [3.8\%] \\
\midrule
%------------------------------------------------------------
% nurse
\textbf{nurse} 
& (1) Caucasian woman (82) [39.0\%]
& (1) Multiracial woman (50) [23.8\%]
& (1) Multiracial woman (103) [49.0\%] \\
& (2) Black woman (57) [27.1\%]
& (2) Multiracial man (39) [18.6\%]
& (2) Black woman (42) [20.0\%] \\
& (3) Latinx woman (24) [11.4\%]
& (3) Hispanic man (34) [16.2\%]
& (3) East Asian woman (19) [9.0\%] \\
& (4) Multiracial woman (20) [9.5\%]
& (4) Caucasian man (22) [10.5\%]
& (4) Latinx woman (19) [9.0\%] \\
& (5) White woman (16) [7.6\%]
& (5) Latinx woman (18) [8.6\%]
& (5) Caucasian woman (15) [7.1\%] \\

\bottomrule
\end{tabular}
}
\end{table*}

\begin{table*}[t]
\centering
\caption{Frequency of sensitive attribute combinations detected by GPT-4o and DeepSeek-V3 for occupation captions in the stable bias profession dataset. Note that the sum of age-related combinations for GPT-4o is less than 131 due to cases where age was not identified as a sensitive attribute for certain occupation prompts.}
\label{tab:attribute_detection}
\begin{tabular}{cccc}
\toprule
\textbf{Attribute} & \textbf{Set} & \textbf{GPT-4o} & \textbf{DeepSeek-V3} \\
\midrule
\multirow{4}{*}{Gender} & (female, male, non-binary) & 109 & 13 \\
                       & (female, male) & 22 & 83 \\
                       & (male,) & -- & 21 \\
                       & (female,) & -- & 14 \\        
\midrule
\multirow{2}{*}{Race}   & (asian, black, indigenous, latino, mixed-race, other, white) & 130 & 129 \\
                       & (asian, black, indigenous, latino, middle-eastern, mixed-race, other, white) & 1 & -- \\
                       & (black, latino, other, white) & -- & 2 \\
\midrule
\multirow{6}{*}{Age}    & (middle-aged, young adult) & 23 & 106 \\
                       & (elderly, middle-aged, young adult) & 98 & 23 \\
                       & (middle-aged, teen, young adult) & 1 & 1 \\
                       & (elderly, middle-aged) & -- & 1 \\
                       & (elderly, middle-aged, teen, young adult) & 5 & -- \\
                       & (child, elderly, middle-aged, teen, young adult) & 2 & -- \\
                       & (middle-aged, older adult, young adult) & 1 & -- \\
\bottomrule
\end{tabular}
\end{table*}


\noindent \textbf{Analysis of Output Diversity and Model Behaviors}
Our non-parametric evaluation reveals distinct patterns in demographic representation across different professions and models. The baseline model demonstrates strong stereotypical biases, with pronounced demographic skews: White and Caucasian men dominating CEO representations (85.7\%), Caucasian women being heavily represented in housekeeper roles (47.6\%), and similar gender-stereotypical patterns for nurses (77.0\% total female representation).

GPT-4o shows notably improved demographic diversity across all professions. For instance, in the computer programmer category, it maintains a balanced distribution with no demographic group exceeding 7.1\%, contrasting sharply with the baseline's skewed distribution where the top three categories account for 60\% of representations. Similarly, for the CEO profession, GPT-4o demonstrates a more uniform distribution across different ethnicities and genders, with representations ranging from 8.6\% to 13.8\%.

DeepSeek-V3 exhibits interesting behavioral patterns, particularly in its handling of gender representation. Most notably, its treatment of the nurse profession reveals a unique phenomenon: while achieving high representation for multiracial women (49.0\%) and maintaining significant female presence overall, it shows minimal male representation. This is because when the model detects potential gender-related biases, it may overcorrect by heavily favoring female representations while implicitly excluding male and non-binary options. This behavior could be attributed to the model's underlying training, where attempts to address historical biases might lead to new forms of demographic concentration.

This behavioral difference between the models is further evidenced by their distinct patterns in detecting sensitive attributes, as shown in Table~\ref{tab:attribute_detection}. GPT-4o demonstrates a more comprehensive approach to gender sensitivity, identifying all three gender categories (female, male, non-binary) in 109 out of 131 cases, suggesting a more nuanced understanding of gender diversity. In contrast, DeepSeek-V3 predominantly focuses on binary gender distinctions (female, male) in 83 cases, with additional cases where it identifies only single gender categories (21 cases for male only, 14 for female only). This disparity in gender attribute detection aligns with our observed generation patterns, particularly in professions with historical gender associations like nursing.

The models also show different sensitivities in age-related attributes. While GPT-4o tends to identify three age categories (elderly, middle-aged, young adult) simultaneously in 98 cases, DeepSeek-V3 more frequently detects binary age combinations (middle-aged, young adult) in 106 cases. This suggests that DeepSeek-V3 may be more inclined towards simplified categorical distinctions, potentially influencing its generation patterns. Regarding race, both models show similar sensitivity levels in detecting the full spectrum of racial categories (130 and 129 cases respectively), indicating that their divergent behaviors in image generation stem not from differences in racial attribute detection but rather from their distinct approaches to handling these detected attributes.

These contrasting patterns in attribute detection provide insight into why the models exhibit different behaviors in addressing societal biases: While GPT-4o's more comprehensive attribute detection contributes to its balanced representations across different genders (male: 18.6\%, female: various percentages) while addressing historical biases, DeepSeek-V3's tendency towards binary distinctions might lead to occasional overcorrection in certain demographic representations. This contrast raises important questions about different strategies for bias mitigation in image generation systems and their effectiveness in achieving true demographic diversity.

\subsection{Analysis}
Our comprehensive evaluation reveals both quantitative improvements and nuanced behavioral patterns in LLM-assisted image generation methods. The human evaluation metrics demonstrate that both GPT-4o and DeepSeek-V3 maintain high image quality comparable to the baseline (scores around 4.0), while showing a slight decrease in prompt adherence (3.8--3.9 vs 4.0+). However, the most significant improvement appears in diversity scores, where both LLM-assisted methods substantially outperform the baseline (3.3--3.9 vs 2.7--2.8), indicating their effectiveness in reducing stereotypical patterns.

This quantitative improvement in diversity is further supported by our non-parametric evaluation of demographic representations. While the baseline model exhibits strong stereotypical biases (e.g., 85.7\% White male CEOs, 77.0\% female nurses), GPT-4o achieves notably balanced distributions across professions, with no demographic group exceeding 7.1\% in categories like computer programmers. However, the two LLM-assisted methods demonstrate distinct approaches to bias mitigation. GPT-4o's comprehensive attribute detection capability (identifying all gender categories in 109/131 cases) appears to contribute to its more nuanced and balanced representations. In contrast, DeepSeek-V3's tendency towards binary attribute distinctions (83 cases of binary gender detection) sometimes results in overcorrection, as evidenced by its treatment of the nurse profession where it heavily favors female representation (49.0\% multiracial women) while minimizing male presence.

These behavioral differences suggest that while both LLM-assisted methods effectively improve upon baseline diversity metrics, their underlying approaches to bias mitigation differ substantially. GPT-4o's more comprehensive attribute detection appears to facilitate truly balanced representations, while DeepSeek-V3's binary-focused approach, though effective at reducing traditional biases, may introduce new forms of demographic concentration. This trade-off between diversity improvement and potential overcorrection presents an important consideration for future development of bias mitigation strategies in image generation systems.

% [TBD: write BLS results]

