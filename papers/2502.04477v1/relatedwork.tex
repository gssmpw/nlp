\section{Previous work}
\label{sec:prev_work}
% but one has access to some device that generates samples
\paragraph{Average reward MDPs.}
The setup of average reward MDPs was introduced  in the dynamic programming literature by \citet{howard1960dynamic}, while \citet{blackwell1962discrete} established a theoretical framework for their analysis. Reinforcement learning provides a variety of methods to approximately solve average reward MDPs in the case where the transition matrix and reward are unknown \citep{mahadevan1996average, dewanto2020average}. These methods include model-based algorithms \citep{jin2021towards, zurek2024span}, model-free algorithms \citep{wei2020model, wan2021learning}, policy gradient methods \citep{bai2024regret, kumar2024global}, and mirror descent  \citep{murthy2023convergence}. 


\paragraph{Model-based methods with a generative model.}
Model-based algorithms use sampling to estimate the transition probabilities of the MDP, and then solve the surrogate model by using standard dynamic programming techniques. These methods can achieve optimal sample complexity in both the discounted and average reward setups. For discounted MDPs, \citet{agarwal2020model, li2020breaking} achieve optimal sample complexity by matching the lower bound in \citet{gheshlaghi2013minimax}. For average reward MDPs,  \citet{wang2023optimal} achieves optimal sample complexity assuming finite mixing times, while  \citet{zurek2024span} achieve optimal complexity for weakly communicating and multichain MDPs. Also \citet{tuynman2024finding} study a model-based method with no prior knowledge  for weakly communicating MDPs. 

\paragraph{Model-free methods with a generative model.}
Model-free algorithms directly estimate the Q-factors and policy without learning a model of the transition probabilities. They are more efficient in terms of  computation and memory requirements, and can tackle large scale problems when combined with function approximation. 

In the generative model setup \citep{kearns1998finite} the sample complexity of model-free algorithms has been widely studied. For  discounted MDPs they achieve  optimal sample complexity matching the complexity lower bound \cite{sidford2018near,wainwright2019variance,jin2024truncated}.
For average rewards, \cref{Tabla1} presents a summary of the sample complexity of previous model-free algorithms. Specifically, for MDPs with finite mixing times $t_{\text{mix}}$, \citet{wang2017primal}  developed a model-free method that applies a primal-dual algorithm to a bilinear saddle point reformulation of the Bellman equation.  Also under the mixing  condition, \citet{jin2020efficiently} use a stochastic mirror descent framework to solve bilinear saddle point problems, whereas \citet{li2024stochastic} studied an actor-critic method also based on stochastic mirror descent. \citet{zhang2023sharper} first obtained sample complexity results for weakly communicating MDPs, by applying Q-learning with variance reduction to an approximation by a discounted 
MDP. Lower bounds on the sample complexity for mixing and weakly communicating MDPs were established respectively by \citet{jin2021towards} and \citet{wang2022near}.

A drawback of these previous model-free algorithms is that their implementation requires prior estimates of the
mixing times of the Markov chains induced by all policies,
or of the span seminorm of the bias vector in the Bellman equation. 
Recently, two model free-algorithms that require no prior knowledge have been proposed. For the case with finite but  unknown mixing times, \citet{jin2024feasible} combine Q-learning and approximation by discounted MDPs  with progressively larger discount factors. For weakly communicating MDPs, \citet{neu2024dealing} use a primal-dual stochastic gradient descent combined with a regularization technique, whose complexity depends on the expected value of the gain $g_{\pi_T}$ of the policy $\pi_T$ generated by the algorithm. Although these algorithms do not require prior knowledge, these works do not provide estimates for the number of iterations, nor  a stopping rule that guarantees $\varepsilon$-optimality. 

\paragraph{Value Iterations.}
Value iterations (VIs)---an instantiation of the Banach-Picard fixed point iterations---were among the first methods considered in the dynamic programming literature \citep{bellman1957markovian} and serve as a fundamental algorithm to compute the value functions for discounted MDPs as well as unichain average reward MDPs. The sample-based variants, such as TD-Learning~\citep{Sutton1988}, Fitted Value Iteration~\citep{Ernst05,Munos08JMLR}, and Deep Q-Network~\citep{MnihKavukcuogluSilveretal2015}, are the workhorses of modern reinforcement learning algorithms~\citep{Bertsekas96,sutton2018reinforcement,SzepesvariBook10}. VIs are also routinely applied in diverse settings, including factored MDPs \citep{rosenberg2021oracle}, robust MDPs \citep{kumarefficient}, MDPs with reward machines \citep{bourel2023exploration}, and MDPs with options \citep{fruit2017regret}. In the generative model setup, variance reduction sampling was applied to approximate VIs for discounted rewards: \citet{sidford2023variance, sidford2018near} use precomputed offsets to reduce variance of sampling, \citet{wainwright2019variance} applies SVRG-type variance reduction sampling \cite{johnson2013accelerating} to Q-learning, and \citet{jin2024truncated} use SARAH-type variance reduction sampling \citep{nguyen2017sarah} which we also exploit in this work.

\paragraph{Halpern iterations.}
For $\gamma$-contractions on Banach spaces,  the classical Banach-Picard iterates $x^{k+1}=T(x^{k})$ converge to the unique fixed point $x^*=T(x^*)$, with explicit bounds for the residuals $\|T(x^k)-x^k\|\leq \gamma^k\|T(x^0)-x^0\|$
and the distance 
$\|x^k-x^*\|\leq \frac{\gamma^k}{1-\gamma}\|T(x^0)-x^0\|$ to the fixed point. This fits well for discounted MDPs, and is also useful in the average reward setting for unichain MDPs. 

For nonexpansive maps with $\gamma=1$, as it is the  case for average reward MDPs, these estimates degenerate and provide no useful information. 
An alternative is provided by Halpern's iteration $x^{k+1}\!=\!(1-\beta_{k+1})x^0+\beta_{k+1} T(x^{k})$, where the next iterate is computed as a convex combination between $T(x^{k})$ and the initial point $x^0$ which acts as an {\em anchor} point along the iterations \citep{halpern1967fixed}. The sequence
$\beta_k\in (0,1)$ is chosen exogenously and increasing to 1, so that the strength of the anchor mechanism diminishes as the iteration progresses. 

Halpern's anchored iteration has been widely studied in minimax optimization and fixed-point problems \citep{halpern1967fixed,sabach2017first, Lieder2021halpern, park2022exact, contreras2022optimal, yoon2021accelerated,cai2022stochastic}. In the context of reinforcement learning, \citet{lee2024accelerating, lee2025multi} applied the anchoring technique to VIs achieving an accelerated convergence rate for cumulative-reward MDPs and the first non-asymptotic rate for average reward multichain MDPs. \citet{bravostochastic} applied the anchoring mechanism to Q-learning for average reward MDPs with a generative model and studied its sample complexity. 


Assuming that the set of fixed points $\Fix(T)$ is nonempty, and under suitable conditions on $\beta_k$, Halpern's iterates have been proved to converge towards a fixed point in the case of Hilbert spaces \citep{wittmann1992approximation} as well as in uniformly smooth Banach spaces \citep{reich1980strong, xu2002iterative}. 
For more general normed spaces, the analysis in \citet[Lemma 5]{sabach2017first} implies that for $\beta_k=k/(k+2)$ one has the 
explicit error bound $\|T(x^k)-x^k\|\leq \frac{4}{k+1}\|x^0-x^*\|$.
The proportionality constant in this bound was recently improved in the Hilbert setting by \citet{Lieder2021halpern, kim2021accelerated}. For a comprehensive analysis, including the determination of the optimal Halpern iteration, see \citet{ contreras2022optimal}.