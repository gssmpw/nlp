%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}
\usepackage{verbatim}
% \usepackage[textwidth=30mm]{todonotes}
\newcommand{\Jongmin}[1]{\todo[color=yellow!20,author=\textbf{\small Jongmin},inline]{\small #1}}
\newcommand{\Mario}[1]{\todo[color=red!20,author=\textbf{\small Mario},inline]{\small #1}}
\newcommand{\Roberto}[1]{\todo[color=blue!20,author=\textbf{\small Roberto},inline]{\small #1}}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% /\usepackage{icml2025}
% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

\newcommand{\argmax}{\mathop{\rm argmax}}
\newcommand{\Argmax}{\mathop{\rm argmax}}
%\newcommand{\Argmax}{\text{argmax}}
\newcommand{\cA}{\mathcal{A}}
\newcommand{\cB}{\mathcal{B}}
\newcommand{\cC}{\mathcal{C}}
\newcommand{\cE}{\mathcal{E}}
\newcommand{\cG}{\mathcal{G}}
\newcommand{\cK}{\mathcal{K}}
\newcommand{\cL}{\mathcal{L}}
\newcommand{\cS}{\mathcal{S}}
\newcommand{\cF}{\mathcal{F}}
\newcommand{\cM}{\Delta}
\newcommand{\cN}{\mathcal{N}}
\newcommand{\cR}{\mathcal{N}}
\newcommand{\cP}{\mathcal{P} }
\newcommand{\cU}{\mathcal{U}}
\newcommand{\cV}{\mathcal{V}}
\newcommand{\cH}{\mathcal{H}}
\newcommand{\cT}{\mathcal{T}}
\newcommand{\cX}{\mathcal{X}}
\newcommand{\cY}{\mathcal{Y}}
\newcommand{\cW}{\mathcal{W}}
\newcommand{\cZ}{\mathcal{Z}}
\newcommand{\gp}{\mathcal{GP}}
\newcommand{\bv}{\mathbf{v}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\expec}{\mathbb{E}}
\newcommand{\prob}{\mathbb{P}}
\newcommand{\T}{\mathcal{T}}
\newcommand{\e}{\mathop{\mathbb{E}}}
\newcommand{\real}{\mathbb{R}}
\newcommand{\NN}{\mathbb{N}}
\newcommand{\ZZ}{\mathbb{Z}_+}
\newcommand{\reals}{\mathbb{R}}
\newcommand{\trans}{P ^{\pi}}
\newcommand{\norm}[1]{\left\|{#1}\right\|}
\newcommand{\abs}[1]{\left|{#1}\right|}
\newcommand*{\para}[1]{\left({#1}\right)}
\newcommand*{\infn}[1]{\left\|{#1}\right\|_{\infty}}
\renewcommand{\sp}[1]{\|{#1}\|_\text{\rm sp}}
\newcommand{\maxA}{\mbox{$\max_\cA$}}
\newcommand{\X}{\mathcal{M}}
\newcommand{\Fix}{\mathop{\rm Fix}}
\newcommand{\R}{r}
\newcommand{\st}{N}
\newcommand{\cupdot}{\mathbin{\mathaccent\cdot\cup}}
\definecolor{commentgreen}{RGB}{24,202,85}

\newtheorem{fact}{Fact}

\newtheorem{manualtheoreminner}{Theorem}
\newenvironment{manualtheorem}[1]{%
    \renewcommand\themanualtheoreminner{#1}%
  \manualtheoreminner
}{\endmanualtheoreminner}

% \newtheorem{manualtheoreminner}{Theorem}
% \newenvironment{manualtheorem}[1]{%
%   \IfBlankTF{#1}
%     {\renewcommand{\themanualtheoreminner}{\unskip}}
%     {\renewcommand\themanualtheoreminner{#1}}%
%   \manualtheoreminner
% }{\endmanualtheoreminner}

\newtheorem{manualcorollaryinner}{Corollary}
\newenvironment{manualcorollary}[1]{%
    \renewcommand\themanualcorollaryinner{#1}%
  \manualcorollaryinner
}{\endmanualtheoreminner}

% \newtheorem{manualcorollaryinner}{Corollary}
% \newenvironment{manualcorollary}[1]{%
%   \IfBlankTF{#1}
%     {\renewcommand{\themanualcorollaryinner}{\unskip}}
%     {\renewcommand\themanualcorollaryinner{#1}}%
%   \manualcorollaryinner
% }{\endmanualtheoreminner}

\newtheorem{manualpropinner}{Proposition}
\newenvironment{manualproposition}[1]{%
  \renewcommand\themanualpropinner{#1}%
  \manualpropinner
}{\endmanualpropinner}

% \newtheorem{manualpropositioninner}{Proposition}
% \newenvironment{manualproposition}[1]{%
%   \IfBlankTF{#1}
%     {\renewcommand{\themanualpropositioninner}{\unskip}}
%     {\renewcommand\themanualpropositioninner{#1}}%
%   \manualpropositioninner
% }{\endmanualpropositioninner}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Near-Optimal Sample Complexity for MDPs via Anchoring}

\begin{document}
\twocolumn
[
\icmltitle{Near-Optimal Sample Complexity for MDPs via Anchoring}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Jongmin Lee}{ed}
\icmlauthor{Mario Bravo}{comp}
\icmlauthor{Roberto Cominetti}{sch}
\end{icmlauthorlist}



\icmlaffiliation{ed}{Department of Mathematical Sciences, Seoul National University, Seoul, Korea}
\icmlaffiliation{comp}{Department of Engineering and Sciences, Universidad Adolfo Ib\'a\~nez, Santiago, Chile}
\icmlaffiliation{sch}{Department of 
Mathematical and Computational Engineering,   Pontificia Universidad Cat\'olica de Chile, \!Santiago, \!Chile}

\icmlcorrespondingauthor{Jongmin Lee}{dlwhd2000@snu.ac.kr}
\icmlcorrespondingauthor{Mario Bravo}{mario.bravo@uai.cl}
\icmlcorrespondingauthor{Roberto Cominetti}{roberto.cominetti@uc.cl}
% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
% /\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution

% /\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
We study a new model-free algorithm to compute  $\varepsilon$-optimal policies for average reward Markov decision processes, in the weakly communicating case. Given a generative model, our procedure combines a recursive sampling technique with Halpern's anchored iteration, and computes an $\varepsilon$-optimal policy with sample and time complexity $\widetilde{O}(|\cS||\cA|\sp{h^*}^{2}/\varepsilon^{2})$ both in high probability and in expectation.  To our knowledge, this is the best complexity among model-free algorithms, matching the known lower bound up to a factor $\sp{h^*}$.  Although the complexity bound involves the span seminorm $\sp{h^*}$ of the unknown  bias vector, the algorithm requires no  prior knowledge  and  implements a stopping rule %based on the empirical Bellman residual error 
which guarantees  with probability 1 that the procedure terminates in finite time. We also analyze how these techniques can be adapted for discounted MDPs.
\end{abstract}


\section{Introduction}\label{intro}

The main task in reinforcement learning is to compute an $\varepsilon$-optimal policy for Markov Decision Processes (MDPs) when the transition kernel is unknown.
If one has access to a generative model that provides  independent samples of the next state for any given initial state and action, a standard approach in {\em model-based} algorithms is to  approximate the transition probabilities by sampling to a sufficiently high accuracy, and build a surrogate model which is then solved by dynamic programming techniques. In contrast, {\em model-free} methods attempt to recursively approximate a solution of the Bellman equation for the value function, with lower memory requirements, specially  when combined with function approximation techniques for large scale problems.

The sample complexity of model-free algorithms has been studied both in the discounted and the average reward setups. 
While for discounted MDPs these procedures have been shown to achieve optimal sample complexity, previous results for average rewards are less satisfactory and exhibit a gap with respect to the lower bound. Furthermore, most prior model-free algorithms for average reward MDPs require for their implementation
some {\em a priori} bound on the mixing times or on the span seminorm of the { bias vector}. 


\subsection{Our contribution} We propose a new model-free algorithm to compute an $\varepsilon$-optimal policy for weakly communicating average reward MDPs. We show that, with probability at least $1-\delta$, the algorithm achieves the task with a sample and time complexity of order ${O}(\widetilde{L}|\cS||\cA|\sp{h^*}^{2}/\varepsilon^{2})$. Here
 $\cS$ and $\cA$ are the state and action spaces of the MDP and $\sp{h^*}$ is the span seminorm of the bias vector described later in \cref{se:model}, while $\widetilde{L}= \ln (2|\cS||\cA|\sp{h^*}/(\varepsilon \delta))\ln^4 (\sp{h^*}/\varepsilon)$
 is a logarithmic multiplicative factor. 
This yields a near-optimal algorithm since this complexity matches the known lower bound up to a factor $\sp{h^*}$. More explicit estimates of the algorithm's complexity are discussed in \cref{Se:complexity2}.


 Our algorithm can be implemented without any prior knowledge about $\sp{h^*}$ nor other characteristics of the optimal policy, and implements a stopping rule which guarantees with probability 1 that the method finds an $\varepsilon$-optimal policy in finite time, with the above complexity bound holding both in high probability as well as in expectation. 
This compares favorably to previous model-free methods
    (see \cref{Tabla1}) which usually require some prior estimates of the span seminorm $\sp{h^*}$, or an upper bound for the maximal mixing time $t_{\text{mix}}$. To our knowledge, the  only model-free algorithm with explicit complexity guarantees and which does not require prior knowledge is the one in the recent paper by \citet{jin2024feasible}, which however applies for MDPs with finite mixing time $t_{\text{mix}}$ and  achieves a complexity bound of $\widetilde{O}(|\cS||\cA|\,t_{\text{mix}}^{8}/\varepsilon^{8})$.


\begin{table*}[!ht] \label{table1}
\centering
\begin{tabular}{|llcl|} 
 \hline
 {\bf Algorithm} & {\bf Sample complexity}\raisebox{2.5ex}{\ }\raisebox{-1.2ex}{\ } & \hspace{-0.5cm}{\bf Prior knowledge}  & {\bf Reference} \\
 \hline
  Primal-dual \raisebox{2.8ex}{\ }$\pi$ Learning & $\widetilde{O}\left({|\cS||\cA|\tau^2 t^2_{\text{mix}}}/{\varepsilon^{2}}\right)$ & \hspace{-0.5cm}yes  & \citet{wang2017primal}\\[0.5ex]
SMD for mixing AMDP & $\widetilde{O}\left({|\cS||\cA|t^2_{\text{mix}}}/{\varepsilon^{2}}\right)$ & \hspace{-0.5cm}yes & \citet{jin2020efficiently}  \\[0.5ex]
Refined Monotone Q-learning & $\widetilde{O}\left({|\cS||\cA|\sp{h^*}^{2}}/{\varepsilon^{2}}\!+\!{|\cS|^2|\cA|\sp{h^*}}/{\varepsilon}\right)$ 
& \hspace{-0.5cm}yes  & \citet{zhang2023sharper}\\[0.5ex]
SPMD for AMDPs & $\widetilde{O}\left({|\cS||\cA|t^2_{\text{mix}}}/{\varepsilon^{3}}\right)$& \hspace{-0.5cm}yes & \citet{li2024stochastic}\\[0.5ex]
% COMIDA-MDP & $\widetilde{O}\left(\frac{|\cS|^2|\cA|^2\sp{h(\hat{\pi})}^4}{\varepsilon^{2}}\right)$ & \hspace{-0.5cm}no & $(0,\infty)$ & \citet{neu2024dealing}  \\
Dynamic Horizon Q-Learning & $\widetilde{O}\left({|\cS||\cA|t^8_{\text{mix}}}/{\varepsilon^{8}}\right)$ & \hspace{-0.5cm}no & \citet{jin2024feasible}  \\[0.5ex]
 SAVIA\textbf{+} \raisebox{-1.4ex}{\ }& $\widetilde{O}\left({|\cS||\cA|\sp{h^*}^{2}}/{\varepsilon^{2}}\right)$ &\hspace{-0.5cm} no  & Our work (\cref{cor:err})\\[0.5ex]
 \hline
  \raisebox{0.1ex}{Lower bound}\raisebox{-1.4ex}{\ }& $\widetilde{\Omega}\left({|\cS||\cA|\sp{h^*}}/{\varepsilon^{2}}\right)$ \raisebox{2.8ex}{\ }& \hspace{-0.5cm}-  &   \citet{wang2022near}\\
 \hline
\end{tabular}
 \vspace{-0.05in}
\caption{{\bf Model-free algorithms for average reward MDPs.} The table shows the sample complexity to compute an $\varepsilon$-optimal policy with probability at least $1-\delta$. The notation $\widetilde{O}(\cdot)$ hides logarithmic factors including $\log(1/\delta)$. Our algorithm does not require any prior knowledge and shows the best complexity, matching the lower bound up to a factor $\sp{h^*}$. The parameters $\tau$ and $t_{\text{mix}}$ are respectively uniform bounds  for the range of stationary distributions and mixing times  across all policies $\pi$, while $\sp{h^*}$ is the span seminorm of a bias vector  that solves the Bellman's equations discussed  in \cref{se:model}. It is known that $ \sp{h^{\star} }\le 8\, t_{\text{mix}}$ (see \citet[Proposition~10]{wang2022near}) and that $\sp{h^{*} }$ is always finite whereas  $t_{\text{mix}}$ can be infinite. Note that a finite $t_{\text{mix}}$ requires that the Markov chains induced by all policies must be aperiodic and with a unique recurrent class.}
\label{Tabla1}
\end{table*}


The core of our analysis builds on several key ideas. First, we show that 
any approximate solution of the Bellman equation with an $\varepsilon$  residual error  yields an $\varepsilon$-optimal policy. To achieve an accurate empirical estimation of the Bellman residual error, we use a recursive sampling technique analyzed using an Azuma-Hoeffding-like inequality. Leveraging these estimates, we draw on concepts from the domain of fixed-points for nonexpansive maps, where the classical Halpern (or Anchored) iteration is known to be efficient (see \cref{sec:prev_work} for a detailed discussion). This approach represents our main  contribution and, to the best of our knowledge, is being applied for the first time to average reward MDPs in the generative setup. In the last section we adapt these ideas to study a similar algorithm for discounted MDPs and compare with other existing methods.


\paragraph{Notations.} For a finite set $J$ we denote $\cM(J)$ the set of probability distributions over $J$. Also, for $x\in\reals^J$ we denote by $\|x\|_{\infty}=\max_{j\in J}|x(j)|$ its infinity norm and by $\sp{x}=\max_{j\in J}x(j)-\min_{j\in J}x(j)$ its  span seminorm. For $x\in\reals^{J}$ and a real number $c\in\reals$, we denote $x-c$ the vector with components $x(j)-c$ for all $j\in J$. 
\subsection{The model} \label{se:model} Let us recall some basic facts about {\em average reward Markov decision processes}. These
 can be found in the classical references \citep{bertsekas2015dynamic,10.5555/528623,  sutton2018reinforcement}.
We consider a Markov decision process $(\cS, \cA, \cP, \R)$ with finite state space $\cS$, finite action space $\cA$, transition kernel $\cP\colon \cS \times \cA \rightarrow \cM(\cS)$, and reward $\R\colon  \cS \times \cA \rightarrow \real$. 
Given an initial state $s_0=s$ and a stationary and deterministic control policy $\pi\colon \cS \rightarrow \cA$, the average reward or gain
$g_\pi\in\reals^\cS$ %and the bias $h_\pi\in\reals^\cS$ are  
is defined by
\vspace{-0.5ex}
\begin{align*}
    g_{\pi}(s)&=\lim_{n\rightarrow \infty} \,\mbox{$\expec_{\pi}\!\left[\frac{1}{n}\sum^{n-1}_{t=0}  \R(s_t, a_t) \,|\, s_0=s\right]$}%\\
    %h_{\pi}(s)&= \mbox{$\expec_{\pi}\Big[\sum^{\infty}_{t=0}  (\R(s_t, a_t)-g_{\pi}) \,|\, s_0=s\Big]$}
\end{align*}
 where $\expec_{\pi}$ denotes the expected value over all trajectories $(s_0, a_0, s_1, a_1, \dots, s_{t}, a_{t},\dots)$ of the Markov chain induced by the control $a_t=\pi(s_t)$ with transitions $s_{t+1}\sim\cP(s_t,a_t)$. 
The optimal reward is then $g^*(s)=\max_{\pi}g_{\pi}(s)$. A policy $\pi$ is called $\varepsilon$-optimal if $0\leq g^*(s)-g_\pi(s)\leq\varepsilon$ for all  $s\in\cS$. 

Although one could consider more sophisticated history-dependent randomized policies $\pi$, when $\cS$ and $\cA$ are finite the optimum
is already attained with the simpler deterministic stationary policies \citep{10.5555/528623}, so we restrict to such policies $\pi$ and we denote $P_\pi$ the transition matrix of the induced Markov chain, namely, $P_\pi(s'|s)=\cP(s'|s,\pi(s))$.

\paragraph{Classification of MDPs.}
An MDP is called \emph{unichain} if the Markov chain $P_\pi$ induced by every policy $\pi$ has a single recurrent class plus a possibly empty set of transient states\footnote{For basic concepts in Markov chains such as recurrent classes, transient states, accessibility, irreducibility, etc., we refer to \citet[Appendix A.2]{10.5555/528623}.}. The MDP is said to be \emph{weakly communicating} if there is a set of states where each state in the set is accessible from every other state in that set under some policy, plus a possibly empty set of states that are transient for all policies. Otherwise, the MDP is called \emph{multichain} in which case each $P_\pi$ may have multiple recurrent classes. Unichain MDPs are weakly communicating, while multichain is the weakest notion.  In this paper we set our 
study in the middle ground by adopting the following standing assumption:\\[1ex]
\centerline{\em {\sc (H)}~~The MDP $(\cS, \cA, \cP, \R)$ is weakly communicating.}

\paragraph{Bellman's equation.}
An optimal policy $\pi$ can be obtained by first solving the following  Bellman equation for the {\em gain} $g\in\reals^\cS$ and {\em bias} $h\in\reals^\cS$, namely: for all $s\in\cS$
\vspace{-1ex}
\begin{align*}
 h(s)+g(s)=\max_{a\in\cA}\big\{\R(s,a)+\sum_{s'\in\cS}\,\cP(s'|s,a)\,h(s')\big\},
\end{align*}

\vspace{-3ex}
and then taking actions $\pi(s)=a\in\cA$ 
that attain the max. 

This Bellman equation always has solutions \citep[Corollary~9.1.5]{10.5555/528623}. Under the assumption (H) the gain $g(s)\equiv g^*$ is constant across all states with $g^*$ the optimal average reward \citep[Theorems~8.3.2, 8.4.1, 9.1.2, 9.1.6] {10.5555/528623} so that hereafter we treat the gain  $g^*(s)$ as a scalar $g^*\in\reals$. Moreover, \citep[Theorems~9.1.7, 9.1.8] {10.5555/528623} show that choosing actions that maximize the right hand side in the Bellman equation yields an optimal policy. 

Bellman's equation can be rewritten in terms of the so-called $Q$-factors. Namely, denoting $Q(s,a)$ the expression inside the maximum, the system becomes: for all $(s,a)\in\cS\times\cA$
\vspace{-1ex}
\begin{align*}
 Q(s,a)+g^*=\R(s,a)+\sum_{s'\in\cS}\cP(s'|s,a)\,\max_{a'\in\cA}Q(s',a'),
\end{align*}

\vspace{-3ex}
and then an optimal policy can  be obtained by selecting  $\pi(s)\in\Argmax_{a\in\cA}Q(s,a)$. 

This $Q$-version of the Bellman equation can be expressed more compactly  as the fixed-point equation\footnote{Recall that for a matrix $B\in\reals^{\cS\times\cA}$ and a real number $g\in\reals$, the operation $B-g$ subtracts $g$ from each component $B(s,a)$.}

\vspace{-1ex}
$$Q=r+\cP\,\maxA(Q)-g^*$$

\vspace{-2ex}
where $\maxA:\reals^{\cS\times\cA}\to \reals^{\cS}$ assigns to  each $Q\in  \reals^{\cS\times\cA}$ the vector $h\in\reals^\cS$ with $h(s)=\max_{a\in\cA} Q(s,a)$, 
whereas $\cP :\reals^\cS\to\reals^{\cS\times\cA}$ transforms each $h\in \reals^\cS$ into the matrix  $\cP h\in\reals^{\cS\times\cA}$ with $\cP h(s,a)=\sum_{s'\in\cS}\cP(s'|s,a)h(s')$.

In what follows we denote $\T:\reals^{\cS\times\cA}\to \reals^{\cS\times\cA}$ the map $\T(Q)=\R+\cP\,\maxA(Q)$ 
so that Bellman's equation reduces to $Q=\T_{g^*}(Q)$
with $\T_{g^*}(Q)=\T(Q)-g^*$.  
We denote $Q^*$ an arbitrary fixed-point, and $h^*=\maxA(Q^*)$
the corresponding solution for the original form of Bellman's equation.


\subsection{Previous work} \label{sec:prev_work}
% but one has access to some device that generates samples
\paragraph{Average reward MDPs.}
The setup of average reward MDPs was introduced  in the dynamic programming literature by \citet{howard1960dynamic}, while \citet{blackwell1962discrete} established a theoretical framework for their analysis. Reinforcement learning provides a variety of methods to approximately solve average reward MDPs in the case where the transition matrix and reward are unknown \citep{mahadevan1996average, dewanto2020average}. These methods include model-based algorithms \citep{jin2021towards, zurek2024span}, model-free algorithms \citep{wei2020model, wan2021learning}, policy gradient methods \citep{bai2024regret, kumar2024global}, and mirror descent  \citep{murthy2023convergence}. 


\paragraph{Model-based methods with a generative model.}
Model-based algorithms use sampling to estimate the transition probabilities of the MDP, and then solve the surrogate model by using standard dynamic programming techniques. These methods can achieve optimal sample complexity in both the discounted and average reward setups. For discounted MDPs, \citet{agarwal2020model, li2020breaking} achieve optimal sample complexity by matching the lower bound in \citet{gheshlaghi2013minimax}. For average reward MDPs,  \citet{wang2023optimal} achieves optimal sample complexity assuming finite mixing times, while  \citet{zurek2024span} achieve optimal complexity for weakly communicating and multichain MDPs. Also \citet{tuynman2024finding} study a model-based method with no prior knowledge  for weakly communicating MDPs. 

\paragraph{Model-free methods with a generative model.}
Model-free algorithms directly estimate the Q-factors and policy without learning a model of the transition probabilities. They are more efficient in terms of  computation and memory requirements, and can tackle large scale problems when combined with function approximation. 

In the generative model setup \citep{kearns1998finite} the sample complexity of model-free algorithms has been widely studied. For  discounted MDPs they achieve  optimal sample complexity matching the complexity lower bound \cite{sidford2018near,wainwright2019variance,jin2024truncated}.
For average rewards, \cref{Tabla1} presents a summary of the sample complexity of previous model-free algorithms. Specifically, for MDPs with finite mixing times $t_{\text{mix}}$, \citet{wang2017primal}  developed a model-free method that applies a primal-dual algorithm to a bilinear saddle point reformulation of the Bellman equation.  Also under the mixing  condition, \citet{jin2020efficiently} use a stochastic mirror descent framework to solve bilinear saddle point problems, whereas \citet{li2024stochastic} studied an actor-critic method also based on stochastic mirror descent. \citet{zhang2023sharper} first obtained sample complexity results for weakly communicating MDPs, by applying Q-learning with variance reduction to an approximation by a discounted 
MDP. Lower bounds on the sample complexity for mixing and weakly communicating MDPs were established respectively by \citet{jin2021towards} and \citet{wang2022near}.

A drawback of these previous model-free algorithms is that their implementation requires prior estimates of the
mixing times of the Markov chains induced by all policies,
or of the span seminorm of the bias vector in the Bellman equation. 
Recently, two model free-algorithms that require no prior knowledge have been proposed. For the case with finite but  unknown mixing times, \citet{jin2024feasible} combine Q-learning and approximation by discounted MDPs  with progressively larger discount factors. For weakly communicating MDPs, \citet{neu2024dealing} use a primal-dual stochastic gradient descent combined with a regularization technique, whose complexity depends on the expected value of the gain $g_{\pi_T}$ of the policy $\pi_T$ generated by the algorithm. Although these algorithms do not require prior knowledge, these works do not provide estimates for the number of iterations, nor  a stopping rule that guarantees $\varepsilon$-optimality. 

\paragraph{Value Iterations.}
Value iterations (VIs)---an instantiation of the Banach-Picard fixed point iterations---were among the first methods considered in the dynamic programming literature \citep{bellman1957markovian} and serve as a fundamental algorithm to compute the value functions for discounted MDPs as well as unichain average reward MDPs. The sample-based variants, such as TD-Learning~\citep{Sutton1988}, Fitted Value Iteration~\citep{Ernst05,Munos08JMLR}, and Deep Q-Network~\citep{MnihKavukcuogluSilveretal2015}, are the workhorses of modern reinforcement learning algorithms~\citep{Bertsekas96,sutton2018reinforcement,SzepesvariBook10}. VIs are also routinely applied in diverse settings, including factored MDPs \citep{rosenberg2021oracle}, robust MDPs \citep{kumarefficient}, MDPs with reward machines \citep{bourel2023exploration}, and MDPs with options \citep{fruit2017regret}. In the generative model setup, variance reduction sampling was applied to approximate VIs for discounted rewards: \citet{sidford2023variance, sidford2018near} use precomputed offsets to reduce variance of sampling, \citet{wainwright2019variance} applies SVRG-type variance reduction sampling \cite{johnson2013accelerating} to Q-learning, and \citet{jin2024truncated} use SARAH-type variance reduction sampling \citep{nguyen2017sarah} which we also exploit in this work.

\paragraph{Halpern iterations.}
For $\gamma$-contractions on Banach spaces,  the classical Banach-Picard iterates $x^{k+1}=T(x^{k})$ converge to the unique fixed point $x^*=T(x^*)$, with explicit bounds for the residuals $\|T(x^k)-x^k\|\leq \gamma^k\|T(x^0)-x^0\|$
and the distance 
$\|x^k-x^*\|\leq \frac{\gamma^k}{1-\gamma}\|T(x^0)-x^0\|$ to the fixed point. This fits well for discounted MDPs, and is also useful in the average reward setting for unichain MDPs. 

For nonexpansive maps with $\gamma=1$, as it is the  case for average reward MDPs, these estimates degenerate and provide no useful information. 
An alternative is provided by Halpern's iteration $x^{k+1}\!=\!(1-\beta_{k+1})x^0+\beta_{k+1} T(x^{k})$, where the next iterate is computed as a convex combination between $T(x^{k})$ and the initial point $x^0$ which acts as an {\em anchor} point along the iterations \citep{halpern1967fixed}. The sequence
$\beta_k\in (0,1)$ is chosen exogenously and increasing to 1, so that the strength of the anchor mechanism diminishes as the iteration progresses. 

Halpern's anchored iteration has been widely studied in minimax optimization and fixed-point problems \citep{halpern1967fixed,sabach2017first, Lieder2021halpern, park2022exact, contreras2022optimal, yoon2021accelerated,cai2022stochastic}. In the context of reinforcement learning, \citet{lee2024accelerating, lee2025multi} applied the anchoring technique to VIs achieving an accelerated convergence rate for cumulative-reward MDPs and the first non-asymptotic rate for average reward multichain MDPs. \citet{bravostochastic} applied the anchoring mechanism to Q-learning for average reward MDPs with a generative model and studied its sample complexity. 


Assuming that the set of fixed points $\Fix(T)$ is nonempty, and under suitable conditions on $\beta_k$, Halpern's iterates have been proved to converge towards a fixed point in the case of Hilbert spaces \citep{wittmann1992approximation} as well as in uniformly smooth Banach spaces \citep{reich1980strong, xu2002iterative}. 
For more general normed spaces, the analysis in \citet[Lemma 5]{sabach2017first} implies that for $\beta_k=k/(k+2)$ one has the 
explicit error bound $\|T(x^k)-x^k\|\leq \frac{4}{k+1}\|x^0-x^*\|$.
The proportionality constant in this bound was recently improved in the Hilbert setting by \citet{Lieder2021halpern, kim2021accelerated}. For a comprehensive analysis, including the determination of the optimal Halpern iteration, see \citet{ contreras2022optimal}.



\section{Framework overview}
\subsection{Anchored Value Iteration}
As discussed in section \cref{se:model}, Bellman's equation is equivalent to the fixed point equation
$Q=\T_{g^*}(Q)$
%$$Q=\R+\cP\,\maxA(Q)-g^*$$
where $\T_{g^*}(Q)=\T(Q)-g^{\star}$ with $\T(Q)=\R+\cP\,\maxA(Q)$. Since $\maxA$ and $\cP$ are nonexpansive for the norm $\big\|\cdot\big\|_\infty$ in the corresponding spaces, the same holds for $\T_{g^*}$ and one might consider Halpern's iteration 
\begin{equation}\label{Halpern}
Q^{k+1}=(1-\beta_{k+1})\, Q^0+ \beta_{k+1}\, \T_{g^*}(Q^{k}).
\end{equation}
This recursion involves the unknown optimal value $g^*$. However, since $\T(Q+c)=\T(Q)+c$ is homogeneous by addition of constants $c\in\reals$,
one can interpret \eqref{Halpern} in the quotient 
quotient space $\X=\reals^{\cS\times\cA}/E$ with $E$ the subspace of constant matrices, and consider instead the \emph{implementable} iteration
where $g^*$ is removed
\begin{align}
    Q^{k+1}  =(1-\beta_{k+1})\, Q^0 +\beta_{k+1}\,\T(Q^{k}). \tag{Anc-VI}
    \label{eq:Anc-VI}
\end{align}
 As explained in \cref{ap:quotient}, both \eqref{Halpern} and \eqref{eq:Anc-VI}
are equivalent up to constants and can be interpreted as a standard Halpern iteration in the quotient space $\X$.

\subsection{Reducing both Bellman residual and policy error }
From the previous observations, it follows that the error bounds for Halpern's iteration directly translate into error bounds for \eqref{eq:Anc-VI} in span seminorm. 
In particular, \citet[Lemma 5]{sabach2017first} implies that for $\beta_k=k/(k+2)$ \eqref{eq:Anc-VI} the Bellman residual error converge to zero with the explicit 
bound in span seminorm
$$\sp{Q^k-\T(Q^k)}\leq\mbox{$\frac{4}{k+1}$}\sp{Q^0-Q^*}$$ where $Q^{*}$ is any solution of Bellman's equation. 
Moreover, for weakly communicating MDPs we can prove that any $Q$ with a small residual $\sp{\T(Q)-Q}$ yields an approximately optimal policy. More precisely, adapting \citep[Theorem 9.1.7]{10.5555/528623}, we have following result.   
\begin{proposition}\label{prop:err}
 Let $Q\in\reals^{\cS\times\cA}$ and $\pi:\cS\to\cA$ a policy such that $\pi(s)\in\Argmax_{a\in\cA}~Q(s,a)$. Then for all states $s\in \cS$ we have $0\leq g^*-g_\pi(s)\leq\sp{\T(Q)-Q}$.
\end{proposition}
Combining \cref{prop:err} with the general estimate of the Bellman residual error, it follows that 
$$  g^*-g_{\pi_k}(s) \leq \sp{Q^k-\T(Q^k)} \leq\mbox{$\frac{4}{k+1}$}\sp{Q^0-Q^*}$$
where $\pi_k(s)\in\Argmax_{a \in \cA} Q^k(s,a).$ Thus \eqref{eq:Anc-VI} not only reduces the Bellman residual error but it allows to derive $\varepsilon$-optimal policies. 


\subsection{Estimating $\T(Q^{k})$ by recursive sampling}
We are interested in the generative setting where the transition kernel $\cP$ is not known in advance but one can generate samples from $\cP(\cdot|s,a)$. In this case \eqref{eq:Anc-VI} cannot be implemented since one cannot compute $\T(Q^{k})$. With a generative model, a natural approach is to approximate $T^{k}\approx \T(Q^{k})$ by computing $h^k=\maxA(Q^k)$ and then collecting samples $\{s_j\}_{j=1}^{m_k}\sim \cP(\cdot|s,a)$  in order to set
$$\mbox{$T^k(s,a)=\R(s,a)+\frac{1}{m_k}\sum_{j=1}^{m_k}h^k(s_j)$}.$$ 
We note that the number of samples required to attain a small error scales with the norm of $h^k$. In order to reduce the sample complexity, we use instead a \emph{recursive sampling} technique borrowed from \citet{jin2024truncated}. 
The basic idea is to exploit the previous approximation $T^{k-1}\approx \T(Q^{k-1})$ and the linearity of the map $\cP$, and to approximate $\T(Q^{k})$ by estimating the difference $\T(Q^{k})-\T(Q^{k-1})=\cP d^k$ with  $d^k=h^k-h^{k-1}$ and adding it to $T^{k-1}$. Specifically, for each $(s,a)$ take $m_k$ samples $s_j\sim\cP(\cdot|s,a)$ and set
$$T^{k}(s,a)=T^{k-1}(s,a)+\mbox{$\frac{1}{m_k}\sum_{j=1}^{m_k}d^k(s_j), \quad k\geq 0$}.$$
 Starting with $T^{-1}=\R\in\reals^{\cS\times\cA}$ and $h^{-1}=0\in\reals^\cS$, and
denoting $D^k\approx\cP d^k$ the matrix sampled at the $k$-th stage, 
we have 
\begin{equation}\label{Eq:telescope}
    \mbox{$T^k=\R+\sum_{i=0}^k D^i$}
\end{equation}
so that all the previous estimates $\{D_i\}^k_{i=0}$ are used to approximate  $\T(Q^{k})$. The subsequent analysis will show how $\sp{d^k}$ can be made significantly smaller than $\sp{h^k}$, to achieve a reduction in the overall sample complexity.

Our algorithms, to be presented in next section, will use as a subroutine the following generic sampling procedure.
\begin{algorithm}[ht!]
   \caption{\mbox{\sc sample}$(d, m)$}
   \label{alg:sample}
\begin{algorithmic}
   \STATE {\bfseries Input:} %$R\in\reals^{\cS\times\cA}$, 
   $d \in \real^{\cS}$\,;\, $m \in \mathbb{N}$
   \FOR{ $(s,a) \in \cS \times \cA$}
   \STATE $D(s,a)=\frac{1}{m}\sum_{j=1}^m d (s_j)$ with $s_j \stackrel{iid}{\sim} \cP(\cdot \,|\, s,a)$
   \ENDFOR
 \STATE {\bfseries Output:} $D$
\end{algorithmic}
\end{algorithm}

For later reference we observe that this subroutine draws $|\cS||\cA|\,m$ 
samples and its output satisfies $\sp{D}\leq\sp{d}$.
In what follows we focus on the 
sample complexity, that is to say, the total number of samples 
required by the algorithms. If we assume that each sample requires constant $O(1)$-time, then the time complexity of the algorithms will be of the same order as the sampling complexity.


\section{Stochastic Anchored Value Iteration}

 We may now present our method which combines two basic ingredients: an anchored value iteration and recursive sampling. The following is our basic algorithm which considers a fixed sampling sequence $c_k>0$ and an averaging sequence 
$\beta_k\in (0,1)$ increasing to 1. 
\begin{algorithm}[ht!]
   \caption{\mbox{\sc \mbox{ SAVIA}}$(Q^0,n,\varepsilon,\delta)$}
   \label{alg:example}
\begin{algorithmic}
     \STATE {\bfseries Input:} $Q^{0}\!\in\real^{\cS\times\cA}$\,;\,$n\in\NN$\,;\,$\varepsilon>0$\,;\,$\delta\in(0,1)$
   \STATE $\alpha\hspace{0.8ex}=\ln(2|\cS||\cA|(n\!+\!1)/\delta)$
   \STATE $T^{-1}=\R$\,;\;$h^{-1}=0$\,;\,$\beta_0=0$
    \FOR{ $k=0,\ldots,n$ }
     \STATE $Q^{k} =(1\!-\!\beta_k)\,Q^0+\beta_k\, T^{k-1}$
     \STATE $h^k= \maxA(Q^{k})$
     \STATE $d^k=h^k-h^{k-1}$
    \STATE $m_k= \max\{\lceil \alpha\,c_k\sp{d^k}^2/\varepsilon^{2}\rceil,1\}$
     \STATE $D^k=\mbox{\sc sample}(d^k, m_k)$ 
     \STATE $T^k=T^{k-1}+D^k$ 
   \ENDFOR
   \STATE $\pi^{n}(s) \in \Argmax_{a\in \cA} Q^{n}(s,a)\quad(\forall s\in\cS)$
   \STATE {\bfseries Output:} $(Q^{n} ,T^n, \pi^{n})$ 
\end{algorithmic}
\end{algorithm}


The iteration can start from an arbitrary $Q^0$, including the all-zero matrix. However, if one has a prior estimate $Q^0\approx Q^*$ ({\em e.g.} from a similar MDP solved previously), it may be convenient to start from $Q^0$ since our complexity bounds scale with the distance $\sp{Q^0-Q^*}$. In any case we stress that \mbox{\rm SAVIA}  does not require any prior knowledge and all the parameters in the algorithm are independent of $Q^*$ or $h^*$, including the sequences $\beta_k, c_k$ for which we provide a specific choice in the next subsection. 

\subsection{Sample complexity for the basic {\rm SAVIA} iteration.}\label{Se:complexity1}
In order to study the sample complexity of \mbox{\rm SAVIA} we first establish conditions which ensure  that
 with high probability  the recursive sampling provides good approximations $T^k\approx \T(Q^k)$ for all $k$'s. We only present the main results and ideas, and defer all proofs to the Appendix.

\begin{proposition}\label{prop:sam-err0}
Let $c_k>0$  with \mbox{$2\sum_{k=0}^\infty c_k^{-1}\leq 1$} and $T^k, Q^k$ the iterates generated by \mbox{\rm SAVIA}$(Q^0,n,\varepsilon,\delta)$.
Then, with probability at least $1-\delta$ we have $\|T^k-\T(Q^k)\|_\infty\le  \varepsilon$ simultaneously for 
 all $k=0,\ldots,n$.
\end{proposition}
The proof is an adaptation of the arguments leading to the Azuma-Hoeffding inequality \citep{azuma1967weighted}, by considering the specific choice of number of samples $m_k$ in the recursive sampling, combined with some ad-hoc union bounds.


\noindent{\sc Remark.} A direct consequence of \cref{prop:sam-err0} is that, 
with probability at least $1-\delta$, the true Bellman residual error
$\sp{\T(Q^k)-Q^k}$ and the empirical residual $\sp{T^k-Q^k}$ differ at most by $2\varepsilon$, as results from a triangular inequality for $\sp{\cdot}$ and the general estimate $\sp{\,\cdot\,}\leq 2\|\cdot\|_\infty$.


In what follows we consider the  specific sequences 
\begin{equation*}
\mbox{\sc (S)}~~~\left\{\begin{array}{l}
c_k=5(k\!+\!2)\ln^2(k\!+\!2)\\
\beta_k=k/(k\!+\!2) 
\end{array}\right. 
\end{equation*}
which satisfy $2\sum_{k=0}^{\infty}c_k^{-1}\leq 1$
and $\beta_k$ increasing to 1. These $\beta_k$'s are the same as in \citet{sabach2017first}, and provide explicit guarantees for the reduction of the Bellman residual error. The choice for $c_k$ has been carefully tailored to achieve a small sample complexity.

The following result presents our complexity bound for \mbox{\rm SAVIA} for weakly communicating MDPs satisfying \mbox{\sc (H)}.
\begin{theorem}\label{thm:err}
Assume \mbox{\sc (H)} and \mbox{\sc (S)} and let $(Q^n,T^n,\pi^n)$ be the output computed by $\mbox{\rm SAVIA}(Q^0,n,\varepsilon,\delta)$. Then, with probability at least $(1-\delta)$ we have, for all $s \in \cS$
\vspace{-1ex}
$$ g^*-g_{\pi^n}(s)\! \leq \sp{\T(Q^n)-Q^n}\leq \frac{8\sp{Q^0-Q^*}}{n+2}+4\varepsilon,$$

\vspace{-2ex}
with a sample and time complexity of order $${O}\big(L\,|\cS||\cA|\big(({\sp{Q^0-Q^*}^{2}+\sp{Q^0}^{2}})/{\varepsilon^{2}}+n^2\big)\big)$$ 
where  
$L=\ln(2|\cS||\cA|(n+1)/\delta)\log^3(n+2)$.
\end{theorem}

The proof exploits \cref{prop:sam-err0} and the remark above in order to  establish an upper bound of the Bellman residual error. The bound for the optimality gap in the policy error then follows from  \cref{prop:err}. The recursive sampling technique is crucial here to attain a complexity that scales quadratically in $\varepsilon$,
whereas a naive sampling would give a much worse fifth order complexity. 
Note that $L$ contains only logarithmic factors and is bounded away from 0 so that in our complexity estimates it can absorb any constant factors. 


\subsection{A model-free algorithm without prior knowledge}\label{Se:complexity2}
Theorem \ref{thm:err} shows that \mbox{\rm SAVIA}  is  effective in reducing the Bellman residual error as well as the policy error. Namely,
with probability at least $1-\delta$, after $n\geq \sp{Q^0-Q^*}/\varepsilon$ iterations  it produces a $(12\,\varepsilon)$-optimal policy  $\pi^n$  and with $\widetilde{O}\left({|\cS||\cA|(\sp{Q^0-Q^*}^{2}}/{\varepsilon^{2}}+1)\right)$   complexity. 

Unfortunately, since $Q^*$ is unknown we cannot directly  determine the number of iterations $n$ required
to achieve this goal. To bypass this issue, we modify the basic iteration by running {\rm SAVIA} for an increasing sequence of $n_i$'s using a standard doubling trick \citep{ auer1995gambling, besson2018doubling}, and incorporating an explicit stopping rule
based on the empirical Bellman residual error $\sp{T^{n_i}-Q^{n_i}}$. Specifically, we consider the \mbox{\rm SAVIA\textbf{+}} iteration described in \cref{alg:example_sample}.
\begin{algorithm}[ht!]
  \caption{\mbox{\sc \mbox{\rm SAVIA\textbf{+}}}$(Q^0,\varepsilon,\delta)$}
  \label{alg:example_sample}
\begin{algorithmic}
  \STATE {\bfseries Input:} $Q^{0}\!\in\real^{\cS\times\cA}$\,;\,$\varepsilon>0$\,;\,$\delta\in(0,1)$
  \STATE {\bfseries for} $i=0,1,\dots$ \,\,\textbf{do} 
    \STATE \quad Set $n_i= 2^i,  \delta_i = \delta/c_i $. 
    \STATE \quad  $(Q^{n_i},T^{n_i}, \pi^{n_i} ) = \mbox{\sc \mbox{\rm SAVIA}}(Q^0,n_i,\varepsilon,\delta_i)$
     \STATE {\bfseries until} $\sp{T^{n_i}-Q^{n_i}}\leq 14\,\varepsilon$
\STATE {\bfseries Output:} $Q^{n_i},T^{n_i}, \pi^{n_i}$
\end{algorithmic}
\end{algorithm}

Our next results establish the effectiveness of the stopping rule and determine the sample complexity of  the modified algorithm \mbox{\rm SAVIA\textbf{+}}. In order to simplify the notation, and to avoid equation display issues, we express our estimates in terms of the following quantities
\vspace{-1ex}
\begin{align*}
    \mu&=\sp{Q^0-Q^*},\\
    \nu&=\sp{Q^0-Q^*}+\sp{Q^0},\\
    \kappa&=\max\{\sp{\R},\sp{Q^0}\}.
\end{align*}

\vspace{-2ex}
In what follows we define the stopping time of \mbox{\rm SAVIA\textbf{+}} as the random variable
$$
 \st =\inf\{{ n_i \in \mathbb N \,:\, \sp{T^{n_i}-Q^{n_i}} \le 14\varepsilon}\}
$$
where $T^{n_i}, Q^{n_i}$'s are the iterates generated 
in each loop. We first show that $N$ has finite expectation and, as a consequence, that the algorithm stops in finite time. 
\begin{proposition} \label{Prop:3.3}
Assume \mbox{\sc (H)} and \mbox{\sc (S)}. Then, $\expec [N] \le {2(1+\mu/\varepsilon)}/{(1-\delta)}$. In particular $N$ is finite almost surely and \mbox{\rm SAVIA\textbf{+}}$(Q^0,\varepsilon,\delta)$ stops with probability $1$ after finitely many loops.  
\end{proposition}
The proof exploits the fact that restarting \mbox{\rm SAVIA} from $Q^0$ in every cycle guarantees the independence of the stopping events in each loop. 
This independence is also relevant to establish the following sample complexity of \mbox{\rm SAVIA\textbf{+}}, which is one of the main results of this paper.



\begin{theorem}\label{thm:err-D}
Assume \mbox{\sc (H)} and \mbox{\sc (S)}. Let $(Q^{N},T^{N},\pi^{N})$ be the output of $\mbox{\sc \mbox{\rm SAVIA\textbf{+}}}(Q^0,\varepsilon,\delta)$. Then, with probability at least $(1-\delta)$ we have, for all $s\in\cS$ 
\vspace{-1ex}
\begin{equation*} g^*-g_{\pi^N}(s)\! \leq \sp{\T(Q^N)-Q^N}\leq 16\,\varepsilon,
\end{equation*}
 with sample and time complexity ${O}\big(\widehat{L}\,|\cS||\cA|(
 \nu^2/\varepsilon^{2}\!+\!1)\big)$ where $\widehat{L}=\ln\!\big(4|\cS||\cA|\,(1\!+\!\mu/\varepsilon)/\delta \big)\,\log^4(2(1\!+\!\mu/\varepsilon))$. 
\end{theorem}

In the proof, we consider some `good' events $G_i$ (see Appendix) which guarantee a low sample complexity for the $i$-th loop and then, through simple union bounds, we show that the probability of these good events occurring at every iteration is at least $1-\delta$. Thus \cref{thm:err-D} shows that \mbox{\rm SAVIA\textbf{+}} computes an $\varepsilon$-optimal policy without requiring any prior estimates for the number of iteration to run, thanks to the doubling trick and the stopping rule. 

In order to compare our complexity result with the lower bound  and the previous results in \cref{Tabla1}, which concern the case $\sp{h^{*}}\ge 1$, we state the following Corollary under this assumption. 
Note however that \cref{thm:err-D} holds without any additional restriction.

\begin{corollary}\label{cor:err}
Assume \mbox{\sc (H)}, \mbox{\sc (S)}, $r(s,a) \in[0,1]$ for all $(s,a)\in\cS\times\cA$, and $\sp{h^{*}}\ge 1$. Let $(Q^N,T^N,\pi^N)$ be the output of $\mbox{\rm SAVIA\textbf{+}}(Q^0,\varepsilon/16,\delta)$
with $Q^0=0$ and $\varepsilon\leq 1$. Then, with probability at least $(1-\delta)$ we have,  for all $s\in\cS$
\vspace{-1ex}
  $$g^*-g_{\pi^N}(s)\leq \sp{\T(Q^N)-Q^N} \leq \varepsilon,$$
 
  \vspace{-2ex}
   with sample and time complexity ${O}\big(\widetilde{L}\,|\cS||\cA|\sp{h^*}^{2}/\varepsilon^{2}\big)$
   where $\widetilde{L}= \ln \left(2|\cS||\cA|\sp{h^*}/(\varepsilon \delta)\right)\ln^4 (\sp{h^*}/\varepsilon)$.
\end{corollary}
The proof is a straightforward application of  \cref{thm:err-D} considering that $\sp{Q^*}\leq\sp{\R}+\sp{h^*}$. 
To the best of our knowledge, the sample complexity in \cref{cor:err} is state-of-the-art among model-free algorithms, and matches the complexity lower bound up to a factor $\sp{h^*}$ (see \cref{Tabla1}). Furthermore, \mbox{\rm SAVIA\textbf{+}} does not require any prior knowledge of $\sp{h^*}$ and automatically provide $\varepsilon$-optimal policy unlike other prior works. 


Our previous results provide PAC bounds (probably approximately correct)  which guarantee a small error  with high probability. On the other hand, it is also relevant to  estimate the expected sample complexity considering the full probability space, including the low probability events in which the algorithm may take a significantly longer time to converge. Such expected complexity results have been studied mainly in the multi-arm bandit literature \citep{katz2020true, mason2020finding, jourdan2023varepsilon}, but much less in the reinforcement learning literature.
The following result establishes the expected sample complexity of \mbox{\rm SAVIA\textbf{+}} in terms of expected policy error.

\begin{theorem}\label{thm:E-err-D}
Assume \mbox{\sc (H)} and \mbox{\sc (S)}. Let $(Q^N,T^N,\pi^N)$ be the output of $\mbox{\rm SAVIA\textbf{+}}(Q^0,\varepsilon,\delta)$. Then, for all $s\in\cS$
$$\expec[g^*-g_{\pi^N}(s)] \leq 16\,\varepsilon +\delta \sp{r},$$
    with expected sample and time complexity
 $$\widetilde{O}\big(|\cS||\cA|\mbox{$(\nu^{2}/{\varepsilon^{2}}+1 +
    \delta\, (1+\mu/\varepsilon)^2(1+(\kappa/\varepsilon)^2)$}\big).$$
\end{theorem}

In contrast with the proof of \cref{thm:err-D} which only focuses on the occurrence of good events, the analysis in expectation requires to estimate the sample complexity for the `unlucky' event where \cref{thm:err} does not provide a guarantee of low sample complexity. To this end we establish a uniform bound for $\sp{d^k} \le \max\{\sp{\R}^2, \sp{Q^0}^2\}$ and compute the expected sample complexity considering both the lucky and unlucky events by the law of total expectation. 

Observe that, compared to \cref{thm:err-D}, the expected policy error and expected sample complexity include  the additional terms $\delta \sp{r}$ and $\widetilde{O}(\mbox{$|\cS||\cA| \delta\, (1+\mu/\varepsilon)^2(1+(\kappa/\varepsilon)^2) $})$ respectively. These additional terms arise from the the unlucky events, showing a fourth order in $\varepsilon$ while our PAC complexity is quadratic in $\varepsilon$.   

From \cref{thm:E-err-D} we derive following analog of \cref{cor:err} 
for the expected policy error and complexity. 
\begin{corollary}\label{cor:expected}
   Assume \mbox{\sc (H)}, \mbox{\sc (S)},  $r(s,a) \in[0,1]$ for all $(s,a)\in \cS\times \cA$ and $\sp{h^{*}}\ge 1$. Let $(Q^N,T^N,\pi^N)$ be the output of $\mbox{\rm SAVIA$\textbf{+}$}(Q^0,\varepsilon/17,\delta)$ with  $Q^0=0$,  $\varepsilon\leq 1$, and $\delta=\varepsilon^2/17$. Then,  for all $s\in\cS$
  $$\expec[g^*-g_{\pi^N}(s)] \leq \varepsilon,$$
   with
   expected sample complexity  $\widetilde{O}\left({|\cS||\cA|\sp{h^*}^{2}}/{\varepsilon^{2}}\right)$.
\end{corollary}

Interestingly, by setting $\delta$ equal to  $\varepsilon^2$, the expected sample complexity of \cref{cor:expected} has same order as \cref{cor:err}. 
We are not aware of such estimates on the expected complexity for MDPs with a generative model. 


\section{Discounted MDPs}
In this section, we extend {\rm SAVIA} to the discounted setup. We will show that with minor changes the approach applies to the discounted case, essentially using the same key ideas. 
 
\subsection{The model}
Let us recall the setup. We consider a discounted Markov decision process 
$(\cS, \cA, \cP, r, \gamma)$, where $\gamma \in (0,1)$ is the discount factor. The assumptions on the state and action spaces, transition probabilities, and rewards remain the same as in the average reward case, except for condition (H) which is no longer required. Given an initial state $s_0=s$ and action $a_0=a$ and a stationary and deterministic policy $\pi\colon \cS \rightarrow \cA$, the $Q$-value function is defined by
$$Q_{\pi}(s, a)=\expec_{\pi}[\sum^{\infty}_{t=0} \gamma^t r(s_t, a_t) \,|\, s_0=s, a_0=a]$$
 where $\expec_{\pi}$ denotes the expected value over all trajectories $(s_0, a_0, s_1, a_1, \dots, s_{t}, a_{t},\dots)$ induced by $\cP$ and $\pi$.
 
The optimal $Q$-value is $Q^{*}(s,a)=\max_{\pi}Q_{\pi}(s,a)$ and 
an optimal policy chooses $\pi(s)\in\Argmax_{a\in\cA}Q^*(s,a)$. A policy $\pi$ is called $\varepsilon$-optimal  if  $ \infn{Q^*-Q_{\pi}} \le \varepsilon$. 

It is well-known that $Q^*$ is the unique solution of the following the Bellman equation, for all $(s,a)\in\cS\times\cA$
\begin{align*}
Q(s,a)=\R(s,a)+\gamma \sum_{s'\in\cS}\cP(s'|s,a)\,\max_{a'\in\cA}Q(s',a').
\end{align*}
Similarly to the  case of average rewards, by
introducing the $\gamma$-contracting map
$\T_\gamma:\reals^{\cS\times\cA}\to \reals^{\cS\times\cA}$   defined as $\T_\gamma(Q)=\R+\gamma \,\cP\,\maxA(Q)$, 
this is  equivalent to the fixed point equation
$Q=\T_\gamma(Q)$. 

\subsection{Sample complexity of {\rm SAVID}}\label{Se:complexity1-d}
Due to space limitations, we defer the presentation of our algorithms {\rm SAVID} and {\rm SAVID\textbf{+}} for discounted MDPs to \cref{sec:app_disc}. The main difference is the introduction of the discount factor in the following sampling process
\begin{equation*}
    \mbox{$T^k=T^{k-1}+\gamma\, D^k=\R+\gamma\sum_{i=0}^k D^i$}
\end{equation*}
and the fact that we measure errors using the infinity norm instead of the span seminorm. With these premises, all the elements of our approach,  including the anchored value iteration, recursive sampling, and proof techniques, are directly adapted to the discounted setup.

The following proposition connects the Bellman residual error to the policy error. Compared to \cref{prop:err}, there is  $1-\gamma$ loss  when we translate the Bellman residual error $\infn{\T_\gamma(Q)-Q}$ into a policy error.
\begin{proposition} \label{prop:err-d}
    Let $Q\in\reals^{\cS\times\cA}$ and $\pi:\cS\to\cA$ a policy such that $\pi(s)\in\Argmax_{a\in\cA}~Q(s,a)$. Then we have $ \infn{Q^*-Q_{\pi}}\leq \frac{2}{1-\gamma}\infn{\T_\gamma(Q)-Q}$.
\end{proposition}
Recall that a major difficulty in the average reward setting was the fact that in general we do not have an {\em a priori} bound on the optimal bias vector $h^*$\!. For discounted MDPs, we have the simple bound $\infn{Q^*} \le \infn{\R}/(1\!-\!\gamma)$, that can be used to run \mbox{\rm SAVID} and 
which allows us to obtain the following sample complexity.
\begin{theorem}\label{cor:err-d}
   Assume \mbox{\sc (S)}, $ r(s,a) \in [0,1]$ for all $(s,a)\in\cS\times \cA$, and $n\!=\!\lceil{10/((1\!-\!\gamma)\varepsilon)}\rceil$. Let $(Q^n,T^n,\pi^n)$ be the output of $ \mbox{\rm SAVID}(Q^0,n, \epsilon/10, \delta, \gamma)$ with $Q^0=0$ and $\varepsilon \le 1/(1\!-\!\gamma)$. Then, with probability at least $(1-\delta)$ we have
\vspace{-1ex}
  $$ \infn{\T_\gamma(Q^n)-Q^n}\leq \varepsilon,$$
 
  \vspace{-2ex}
   with sample and time complexity ${O}\big(L_{\gamma}|\cS||\cA|/((1\!-\!\gamma)^2\varepsilon^{2})\big)$
   where $L_{\gamma}=\ln (2|\cS||\cA|/((1\!-\!\gamma)\varepsilon \delta))\ln^3 (2/((1\!-\!\gamma)\varepsilon)) $.
\end{theorem} 
It is known that the lower bound on the complexity to compute an $\varepsilon$-optimal $Q$-value function and an $\varepsilon$-optimal policy is $\widetilde\Omega(|\cS||\cA|/((1\!-\!\gamma)^3\varepsilon^{2}))$ \citep{gheshlaghi2013minimax}. In fact, model-free algorithms can achieve this complexity \cite{sidford2018near,wainwright2019variance,jin2024truncated}.
Since $\infn{\T_\gamma Q-Q} \le (1\!+\!\gamma) \infn{Q^*-Q}$, it follows that these algorithms require $\widetilde O(|\cS||\cA| /((1\!-\!\gamma)^3\varepsilon^{2}))$ to obtain an $\varepsilon$ Bellman residual error. Up to our knowledge, the $\widetilde{O}(|\cS||\cA|)/((1\!-\!\gamma)^2\varepsilon^{2}))$ sample complexity in \cref{cor:err-d} is the best known sample complexity to obtain an $\varepsilon$ residual error for discounted MDPs. 
 
On the other hand, \cref{prop:err-d} implies that, to compute an $\varepsilon$-optimal policy with arbitrary high probability, {\rm SAVID} requires $\widetilde{O}(|\cS||\cA|/((1\!-\!\gamma)^4\varepsilon^{2}))$ sample calls, matching the lower bound up to a factor $1/(1\!-\!\gamma)$. In what follows, we show that {\rm SAVID}\textbf{+} can improve this upper bound, making the dependence explicit on $\infn{Q^*}^2$ and saving a factor of $1/(1\!-\!\gamma)^2$ by using a doubling trick and a stopping rule.
 
\subsection{Sample complexity of {\rm SAVID}\textbf{+}}\label{Se:complexity2-d}

\begin{theorem}\label{cor:err2-d}
Assume \mbox{\sc (S)}, $  r(s,a) \in [0,1]$ for all $(s,a)\in \cS\times \cA$, and $\infn{Q^*} \ge 1$. Let $(Q^N,T^N,\pi^N)$ be the output of $ \mbox{\rm SAVID\textbf{+}}(Q^0,\varepsilon(1\!-\!\gamma)/24,\delta, \gamma)$ with $Q^0=0$ and $\varepsilon \le 1/(1\!-\!\gamma)$. Then, with probability at least $(1-\delta)$ we have
\vspace{-1ex}
  $$ \infn{Q^*-Q_{\pi^N}}\leq 2\infn{\T_\gamma(Q^N)-Q^N}/(1\!-\!\gamma) \leq \varepsilon,$$
 
  \vspace{-2ex}
   with sample and time complexity $${O\!}\left(\widetilde{L}_{\gamma}|\cS||\cA|\infn{Q^*}^{2}/((1\!-\!\gamma)^2\varepsilon^{2})\right)$$
   where $\widetilde{L}_{\gamma}= \ln \big(2|\cS||\cA|/((1\!-\!\gamma)\varepsilon \delta)\big)\ln^4 \big(2/((1\!-\!\gamma)\varepsilon)\big)$.
\end{theorem}
Notice that if we use the bound $\infn{Q^*} \le 1/(1\!-\!\gamma)$ we get $\widetilde{O}(|\cS||\cA|/((1\!-\!\gamma)^4\varepsilon^{2}))$ complexity, which matches the one of \mbox{\rm SAVID}. However, the sample complexity of \mbox{\rm SAVID\textbf{+}} shows an explicit dependence on $\infn{Q^*}$ that might be useful for particular MDPs. For instance, if we have $\infn{Q^*} = O(1/\sqrt{1-\gamma})$, the complexity bound in \cref{cor:err2-d} improves to $\widetilde{O}\big(|\cS||\cA|/((1\!-\!\gamma)^3\varepsilon^{2})\big)$. Note however that the worst case example in \citep{gheshlaghi2013minimax} is such that $\infn{Q^*}= \Omega(1/(1\!-\!\gamma))$. 





\section{Conclusion}
This work proposed a novel framework Stochastic Anchored Value Iteration that computes an $\varepsilon$-optimal policy with anchored value iteration and recursive sampling. Our model-free algorithm \mbox{\rm SAVIA\textbf{+}} does not require prior knowledge of the bias vector and achieves near-optimal sample complexity for weakly communicating average reward MDPs,  matching the lower bound up to a factor $\sp{h^*}$. Similarly, \mbox{\rm SAVID\textbf{+}} attains near-optimal  complexity for discounted MDPs.  

A possible research direction is to improve the sample complexity of \mbox{\rm SAVIA\textbf{+}} to match the lower bound for weakly communicating MDPs. Other interesting open questions are the analysis of the anchoring framework for general multichain MDPs, and the study of alternative sampling setups such as episodic sampling and online  learning. 

\section*{Impact Statement}
This paper focuses on the theoretical aspects of reinforcement learning. There are no societal
impacts that we anticipate from our theoretical result.

% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
%\nocite{langley00}

\bibliography{icml2025}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{Omitted proofs for the average reward setting} \label{app:average}
In this section, we present the proofs omitted in the main body of the paper, along with additional comments to complement our results. We start by analyzing the average reward setup, while the analog results in the discounted setting is presented in \cref{app:discounted}. For better readability, we restate the main results.

\subsection{Proof of \cref{prop:err}}
\begin{manualproposition}{\ref{prop:err}}\label{prop:err_appendix}
 Let $Q\in\reals^{\cS\times\cA}$ and $\pi:\cS\to\cA$ a policy such that $\pi(s)\in\Argmax_{a\in\cA}~Q(s,a)$. Then for all states $s\in \cS$ we have $0\leq g^*-g_\pi(s)\leq\sp{\T(Q)-Q}$.
\end{manualproposition}
\begin{proof}
Let $P_\pi$ the transition matrix of the Markov chain induced by $\pi$, that is, $P_\pi(s'|s)=\cP(s'|s,\pi(s))$.
Then $g_\pi=P_\pi^\infty \R_\pi$ where   $P_\pi^\infty=
\lim_{T\to\infty}\frac{1}{T}\sum_{t=0}^{T-1}(P_\pi)^t$ and $\R_\pi(s)=\R(s,\pi(s))$.  Since $P_\pi^\infty=P_\pi^\infty P_\pi$, it follows that for all $h\in\reals^\cS$ we have $g_{\pi}= P_\pi^\infty (\R_\pi+P_\pi h-h)$. In particular if we take $ h=\maxA(Q)$ so that $ h(s)=Q(s,\pi(s))$, then for every $s'\in\cS$ the term
 $\R_\pi(s')+(P_\pi h)(s')- h(s')$ is exactly  the component $(s',\pi(s'))$ of the matrix $\T(Q)-Q$, and therefore by averaging according to $P^\infty_\pi(\cdot|s)$ we derive the inequality $g_\pi(s)\geq \min_{s',a'} \big(\T(Q)-Q\big)(s',a')$.

Similarly, for an optimal policy $\pi^*$ we have $g^*= P_{\pi^*}^\infty (\R_{\pi^*}+P_{\pi^*} h-h)$ for $h=\maxA(Q)$. Denoting $h'(s)=Q(s,\pi^*(s))$ we have $h'(s)\le h(s)$ and therefore 
$$\R_{\pi^*}(s')+(P_{\pi^*} h)(s')-h(s') \le \R_{\pi^*}(s')+(P_{\pi^*} h)(s')-h'(s').$$ 
Noting that $\R_{\pi^*}(s')+(P_{\pi^*} h)(s')-h'(s')$ is the component $(s',\pi^*(s'))$ of $\T(Q)-Q$, and averaging with $P^\infty_{\pi^*}(\cdot|s)$ we get $g^*=g^*(s)\leq \max_{s',a'} \big(\T(Q)-Q\big)(s',a')$.
Sustracting both estimates we conclude $0\leq g^*-g_\pi(s)\leq \sp{\T(Q)-Q}$.
\end{proof}

\subsection{Proofs of \cref{Se:complexity1}}

Let $\mathcal{F}_k=\sigma(\{D^i:i=0,\ldots,k\})$ denote the natural filtration generated by the sampling process in {\rm SAVIA}, and $\prob(\cdot)$ the probability distribution over the trajectories $(D^k)_{k\in\NN}$.
Notice that, because of the order of the updates, $T^k$ is $\mathcal{F}_k$-measurable  whereas $Q^k$, $h^k$, $d^k$ and $m_k$, being functions of $T^{k-1}$, are $\mathcal{F}_{k-1}$-measurable. 




\begin{manualproposition}{\ref{prop:sam-err0}}
Let $c_k>0$  with \mbox{$2\sum_{k=0}^\infty c_k^{-1}\leq 1$} and $T^k, Q^k$ the iterates generated by \mbox{\rm SAVIA}$(Q^0,n,\varepsilon,\delta)$.
Then, with probability at least $1-\delta$ we have $\|T^k-\T(Q^k)\|_\infty\le  \varepsilon$ simultaneously for 
 all $k=0,\ldots,n$.
\end{manualproposition}
\vspace{-3ex}
%\begin{proof}[Proof of \cref{prop:sam-err0}]    
\begin{proof}
Let $Y^i=D^i-\cP d^i$ and $X^k=\sum_{i=0}^kY^i$.
Since $h^{-1}=0$, by telescoping $\T(Q^k)=\R+\cP h^k=\R+\sum_{i=0}^k\cP d^i$ and then using \eqref{Eq:telescope} we get $T^k-\T(Q^k)=\sum_{i=0}^k(D^i-\cP d^i)=X^k$. 
     
We proceed to estimate $\prob(\|X^k\|_\infty\geq\varepsilon)$
by adapting the arguments of Azuma-Hoeffding's inequality.
Let $s_{k,j}^{s,a}\sim \cP(\cdot \,|\, s,a)$ for $j=1,\ldots,m_k$ be the samples at the $k$-th iteration for $(s,a)\in\cS\times \cA$, so that 
     $$ Y^{k}(s,a) =\mbox{$\frac{1}{m_k}\sum_{j=1}^{m_k}\big(d^k(s^{s,a}_{k,j})$}-\cP d^k(s,a)\big)\qquad\forall (s,a)\in\cS\times\cA.$$ 
     Since $d^k$ and $m_k$ are $\mathcal{F}_{k-1}$-measurable, it follows that $\expec{[Y^{k}|\mathcal{F}_{k-1}]}=0$ 
     and therefore $X^k$ is an $\mathcal{F}_{k}$-martingale.
From Markov's inequality and the tower property of conditional expectations we get that for each $(s,a)\in\cS\times \cA$ and $\lambda>0$
\begin{align}\nonumber
   \mbox{$ \prob( X^k(s,a) \ge \varepsilon)$} 
   & \le e^{-\lambda\varepsilon}\,\expec\big[\exp(\lambda\, \mbox{$X^k(s,a)$})\big]\\
   &= e^{-\lambda\varepsilon}\,\expec\big[\exp(\lambda\,\mbox{$X^{k-1}(s,a)$}) \;\expec[\exp(\lambda \, Y^{k}(s,a)) \,|\, \mathcal{F}_{k-1}]\big].\label{eq:a1}
\end{align}
Now, conditionally on $\mathcal{F}_{k-1}$, the terms $Y^k_j=\mbox{$\frac{1}{m_k}\big(d^k(s^{s,a}_{k,j})$}-\cP d^k(s,a)\big)$ in the sum of $Y^k(s,a)$ are independent random variables with zero mean  and $|Y^k_j| \le \frac{1}{m_k}\sp{d^k}$ so that Hoeffding's Lemma gives us
\begin{align}
    \expec\big[\exp(\lambda\,Y^{k}(s,a)) \,|\, \mathcal{F}_{k-1}\big]  &=\prod_{j=1}^{m_k}\expec\big[\exp(\lambda\,Y^k_j) \,|\, \mathcal{F}_{k-1}\big] 
    \le \exp\big(\mbox{$\frac{1}{2}$}\lambda^2\sp{d^k}^2/m_k\big). \label{eq:a2}%\\
    %&\le \exp\big(\mbox{$\frac{1}{2}$}s^2c_k^{-1}\big).
\end{align}
Using \eqref{eq:a1} and \eqref{eq:a2}, together with $m_k\geq \alpha\,c_k\sp{d^k}^2/\varepsilon^{2}$, 
a simple induction yields $$\expec[\exp(\lambda\,X^k(s,a))]\leq\exp\big(\mbox{$\frac{1}{2}\lambda^2\varepsilon^2\sum^k_{i=0} c^{-1}_i/\alpha$}\big).$$ Then, since $\sum^{\infty}_{i=0}c^{-1}_i\leq\frac{1}{2}$ we get
$\prob(X^k(s,a)\geq\varepsilon)\leq \exp\big(\mbox{$-\lambda\,\varepsilon+\frac{1}{4}\lambda^2\varepsilon^2/\alpha$}\big)$ and 
taking $\lambda=2\alpha/\varepsilon$ we deduce
$$\prob(X^k(s,a)\geq\varepsilon)\leq \exp(-\alpha)=\delta/(2|\cS||\cA|(n\!+\!1)).$$
A symmetric argument yields the same bound for $\prob(X^k(s,a)\leq-\varepsilon)$ so that 
$\prob(|X^k(s,a)|\geq\varepsilon)\leq \delta/(|\cS||\cA|(n\!+\!1))$. Applying a union bound over all $(s,a)\in\cS\times\cA$ we get $\prob(\|X^k\|_\infty\geq\varepsilon)\leq \delta/(n\!+\!1)$, and then a second union bound over $k$ gives $\prob(\bigcup_{k=0}^n\{\|X^k\|_\infty\geq\varepsilon\})\leq \delta$. The conclusion follows by taking the complementary event.
\end{proof}



From now on we 
consider the specific sequences $c_k=5(k+2)\ln^2(k+2)$ and $\beta_k=k/(k+2)$.
We also fix some
solution  $Q^*$  of Bellman's equation $Q^*=\T Q^*-g^*$, and we set $h^*=\maxA(Q^*)$.
We keep the notation for all the sequences generated by
{\rm SAVIA} and we assume throughout that
\begin{equation}\label{Eq:Ek}
\infn{T^k-\T(Q^{k})}\le \varepsilon\qquad \text{for all} \,\, k=0,\ldots,n,
\end{equation}
which, in view of \cref{prop:sam-err0}, holds with probability at least $(1-\delta)$.
Also, as noted earlier, for $D=\mbox{\sc Sample}(d,m)$ we have $\sp{D}\leq\sp{d}$, which combined with 
 the nonexpansivity of the map $Q\mapsto \maxA(Q)$ implies
\begin{equation}\label{Eq:spk}\sp{T^k-T^{k-1}}=\sp{D^k} \le \sp{d^k}=\sp{h^k-h^{k-1}}\le \sp{Q^{k}-Q^{k-1}}.
\end{equation}
As a preamble to the proof of \cref{thm:err}
we establish two preliminary technical Lemmas. 
\begin{lemma}\label{Le:uno}Assuming \mbox{\sc (S)} and \eqref{Eq:Ek}, we have that $$\sp{Q^k-Q^*}\leq \sp{Q^0-Q^*}+\frac{2}{3}\,\varepsilon\,k, \quad \text{for all} \,\, k=0,\ldots,n.$$ 
\end{lemma}
\begin{proof} From the iteration $Q^{k}=(1-\beta_k)Q^0+\beta_k\,T^{k-1}$ with $\beta_k=\frac{k}{k+2}$ we get
\begin{align}\label{eq:jjj}
    \sp{Q^{k}-Q^*}&\le \mbox{$\frac{2}{k+2}\sp{Q^0-Q^*}+\frac{k}{k+2}\sp{T^{k-1}-Q^*}$}.
\end{align}
Using the invariance of $\sp{\cdot}$ by addition of constants and the nonexpansivity of $\T$ for this seminorm, a triangle inequality together with $\sp{\cdot}\leq 2\|\cdot\|_\infty$ and the bound \eqref{Eq:Ek} imply
\begin{equation}\label{Eq:cit}
    \sp{T^{k-1}-Q^*}=\sp{T^{k-1}-\T(Q^*)}\le 2\varepsilon+\sp{Q^{k-1}-Q^*}
\end{equation}
which plugged back into \eqref{eq:jjj} yields
$$
\sp{Q^{k}-Q^*}\le \mbox{$\frac{2}{k+2}\sp{Q^0-Q^*}+\frac{k}{k+2}\big(2\varepsilon+\sp{Q^{k-1}-Q^*}\big)$}.$$
Denoting $\theta_k=(k+1)(k+2)\sp{Q^{k}-Q^*}$ this becomes
$\theta_k\leq \theta_0\, (k+1)+2\varepsilon\,k(k+1)+\theta_{k-1}$,
and inductively
\begin{align*}
    \theta_k&\leq \mbox{$\theta_0\, \sum_{i=1}^k (i+1)+2\varepsilon\sum_{i=1}^ki(i+1)+\theta_0$}\\
    &=\mbox{$\theta_0\,\frac{1}{2}(k+1)(k+2)+ \frac{2}{3}\,\varepsilon\,k(k+1)(k+2)$.}
\end{align*}
The conclusion then follows dividing by $(k+1)(k+2)$.
\end{proof}

\vspace{2ex}

\begin{lemma}\label{Le:dos} Assume \mbox{\sc (S)} and \eqref{Eq:Ek} and let $\rho_k=2\sp{Q^0\!-Q^*}+\frac{2}{3}\,\varepsilon\,k$. Then 
$$\sp{Q^{k}\!-Q^{k-1}} \le \frac{2}{k(k+1)}\sum_{i=1}^{k}\rho_{i+2}\quad \text{for all} \,\, k=1,\ldots,n.$$ 
\end{lemma}
\begin{proof} From 
$Q^{k}=\frac{2}{k+2}Q^0+\frac{k}{k+2}T^{k-1}$ and
$Q^{k-1}=\frac{2}{k+1}Q^0+\frac{k-1}{k+1}T^{k-2}$
we derive
\begin{equation} \label{eq:aux}
    Q^{k}-Q^{k-1}
    =\mbox{$ \frac{2}{(k+1)(k+2)}(T^{k-1}-Q^0) + \frac{k-1}{k+1}(T^{k-1}-T^{k-2})$}.
\end{equation}
Now, \eqref{Eq:cit} together with \cref{Le:uno}  readily imply $\sp{T^{k-1}-Q^0}\leq \sp{T^{k-1}-Q^*} +\sp{Q^*-Q^0}\leq\rho_{k+2}$,  
and therefore using \eqref{Eq:spk} we get
\begin{align*}
    \sp{Q^{k}-Q^{k-1}} %&\le \mbox{$\frac{2}{(k+1)(k+2)}\rho_{k+2} +  \frac{k-1}{k+1}\sp{T^{k-1}- T^{k-2}}$}\\ 
    &\le \mbox{$\frac{2}{(k+1)(k+2)}\rho_{k+2} +  \frac{k-1}{k+1}\sp{Q^{k-1}- Q^{k-2}}$}.
\end{align*}
Denoting $\tilde \theta_k=k(k+1)\sp{Q^k-Q^{k-1}}$ we have
$\tilde\theta_k\leq \frac{2k}{k+2}\rho_{k+2}+\tilde\theta_{k-1}\leq 2\rho_{k+2}+\tilde\theta_{k-1}$.
Hence $\tilde\theta_k\leq2\sum_{i=1}^k\rho_{i+2}$ from which the result follows directly.
\end{proof}


\begin{manualtheorem}{\ref{thm:err}}
Assume \mbox{\sc (H)} and \mbox{\sc (S)} and let $(Q^n,T^n,\pi^n)$ be the output computed by $\mbox{\rm SAVIA}(Q^0,n,\varepsilon,\delta)$. Then, with probability at least $(1-\delta)$ we have, for all $s \in \cS$
\vspace{-1ex}
$$ g^*-g_{\pi^n}(s)\! \leq \sp{\T(Q^n)-Q^n}\leq \frac{8\sp{Q^0-Q^*}}{n+2}+4\varepsilon,$$

\vspace{-2ex}
with a sample and time complexity of order $${O}\big(L\,|\cS||\cA|\big(({\sp{Q^0-Q^*}^{2}+\sp{Q^0}^{2}})/{\varepsilon^{2}}+n^2\big)\big)$$ 
where  
$L=\ln(2|\cS||\cA|(n+1)/\delta)\log^3(n+2)$.
\end{manualtheorem}

\begin{proof}
From \cref{prop:sam-err0}, with probability at least $(1-\delta)$ we have $\sp{T^k-\T(Q^k)}\leq\varepsilon$ for all $k=0,\ldots,n$. Now, the recursion $Q^n=\frac{2}{n+2}Q^0+\frac{n}{n+2}T^{n-1}$ implies
 \begin{equation}\label{eq:lll}
    \T(Q^n)-Q^n
    =\mbox{$\frac{2}{n+2}\big(\T(Q^n)-Q^0\big)+ \frac{n}{n+2} \big(\T(Q^n)-\T(Q^{n-1})\big)+ \frac{n}{n+2}\big(\T(Q^{n-1})-T^{n-1}\big)$}.
\end{equation}
Using the fact that $Q^*=\T(Q^*)-g^*$ and the invariance of $\sp{\cdot}$ by additive constants, a triangle inequality and the $\sp{\cdot}$-nonexpansivity of $\T(\cdot)$ together with Lemma~\ref{Le:uno}, imply 
$$\sp{\T(Q^n)-Q^0} \le \sp{Q^n-Q^*}+\sp{Q^*-Q^0} \le  \rho_n,$$ 
while Lemma \ref{Le:dos} gives $\sp{\T(Q^n)-\T(Q^{n-1})}\leq\frac{2}{n(n+1)}\sum_{i=1}^{n}\rho_{i+2}$. Thus, applying a triangle inequality to \eqref{eq:lll} and using these estimates together with \cref{prop:err} and $\sp{\cdot}\leq 2\|\cdot\|_\infty$ we obtain
\begin{align*}
   g^*-g_\pi(s)\leq  \sp{\T(Q^n)-Q^n}&\le \mbox{$\frac{2}{n+2}\,\rho_n+\frac{2}{(n+1)(n+2)}\sum_{i=1}^{n}\rho_{i+2}+ 2\varepsilon$}
   \\& = \mbox{$\frac{4(1+2n)}{(n+1)(n+2)}\sp{Q^0-Q^*}+ \frac{4(3+8n+3n^2)}{3(n+1)(n+2)}\varepsilon$}
    \\& \le \mbox{$\frac{8\sp{Q^0-Q^*}}{n+2}+4\varepsilon$}.
\end{align*}

To estimate the complexity, for $k \ge 1$, we use the inequality $\sp{d^k}\le\sp{Q^k-Q^{k-1}}$ in \eqref{Eq:spk} and \cref{Le:dos} to find
\begin{align*}
    \mbox{$\sp{d^k}$}&\leq\mbox{$\frac{2}{k(k+1)}\sum_{i=1}^{k}\rho_{i+2}$}\\
    &=\mbox{$\frac{4}{k+1}\sp{Q^0-Q^*}$} +\mbox{$\frac{2(k+5)}{3(k+1)}\varepsilon$}\\
    &\leq \mbox{$\frac{4}{k+1}\sp{Q^0-Q^*}+2\varepsilon$}.
\end{align*}
Now, to estimate the total number of samples $|\cS||\cA|\sum_{k=0}^n m_k$ 
we recall that $m_k=\max\{\lceil\alpha c_k\sp{d^{k}}^2/\varepsilon^{2}\rceil,1\}$ which can be bounded as $m_k\leq 1+\alpha c_k\sp{d^{k}}^2/\varepsilon^{2}$. Then 
\begin{align}
    \mbox{$\sum_{k=0}^nm_k$}&\leq\mbox{$ (n+1)+\frac{\alpha}{\varepsilon^{2}}\sum_{k=0}^n c_k\sp{d^{k}}^2$} \nonumber\\
   &\leq\mbox{$ (n+1)+\frac{10\, \alpha}{\varepsilon^{2}} \ln^2(2)\,\sp{Q_0}^2+\frac{5\alpha}{\varepsilon^{2}}\sum_{k=1}^n (k+2)\ln^2(k+2)\big(\frac{4}{k+1}\sp{Q^0-Q^*}+2\varepsilon\big)^2$} \nonumber\\
      &\leq\mbox{$ (n+1)+\frac{10\, \alpha}{\varepsilon^{2}} \ln^2(2)\sp{Q_0}^2+\sum_{k=1}^n\frac{240\,\alpha}{\varepsilon^{2}(k+1)}\ln^2(k+2)\sp{Q^0-Q^*}^2+40\,\alpha\sum_{k=1}^n(k+2)\ln^2(k+2)$} \nonumber\\
   &=O\!\left( \alpha \sp{Q^0}^2/\varepsilon^{2}+\mbox{$\alpha\ln^3(n+2)\sp{Q^0-Q^*}^2/\varepsilon^{2}+\alpha \,n^2\ln^2(n+2)$}\right),
   \label{eq:last}
\end{align}
where the third inequality results by using the trivial bounds $(a+b)^2\leq2a^2+2b^2$ and $\frac{k+2}{k+1}\leq\frac{3}{2}$,
and the last equality from integral estimations of the sums.
The announced complexity bound then follows by multiplying this estimate by $|\cA||\cS|$ and using the definition 
of $\alpha$ and $L$.
\end{proof}

{\sc Remark.} The complexity analysis above can be refined 
to obtain an explicit multiplicative constant in $O(\cdot)$.






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{Proofs of \cref{Se:complexity2}}
We now proceed to establish the finite convergence and complexity of the algorithm {\rm SAVIA}\textbf{+}, for which we introduce some additional notation.
Let us recall the definition of the parameters
\vspace{-1ex}
\begin{align*}
    \mu&=\sp{Q^0-Q^*},\\
    \nu&=\sp{Q^0-Q^*}+\sp{Q^0},\\
    \kappa&=\max\{\sp{\R},\sp{Q^0}\}.
\end{align*}
The stopping time of \mbox{\rm SAVIA\textbf{+}} is the random variable 
$$
 \st =\inf\{{ n_i \in \mathbb N \,:\, \sp{T^{n_i}-Q^{n_i}} \le 14\,\varepsilon}\}
$$
with $T^{n_i}$ and $Q^{n_i}$'s the iterates generated in each loop of \mbox{\rm SAVIA\textbf{+}}$(Q^0,\varepsilon,\delta)$.
For notational convenience we also define 
\begin{equation*}
 I =\inf\{ i \in \mathbb N \,:\, \sp{T^{n_i}-Q^{n_i}} \le 14\,\varepsilon\}
\end{equation*}
so that in fact $N=2^I$. We let $i_0 \in \NN$ be the smallest integer satisfying $n_{i_0}\ge\sp{Q^0-Q^{*}}/\varepsilon=\mu/\varepsilon$, so that 
either $i_0=0$ and $n_{i_0}=1$ or $n_{i_0-1}=n_{i_0}/2<\mu/\varepsilon$, which combined imply $n_{i_0}\le 2(1+\mu/\varepsilon)$.

In order to estimate the sample complexity,
 we consider the random events
    \begin{equation*}
S_{i}=\{\sp{T^{n_i}-Q^{n_i}} \le 14\,\varepsilon\} \quad \text{and} \quad G_i=\{ \infn{T^k-\T (Q^k)} \le \varepsilon,\;\forall k=0,\ldots,n_i\}
\end{equation*}
with $T^{k}$ and $Q^{k}$'s the inner iterates generated during the execution of \mbox{\rm SAVIA}$(Q^0,n_i,\varepsilon,\delta_i)$ in the $i$-th loop of  \mbox{\rm SAVIA\textbf{+}}, and denote by $M_i$ the number of 
samples used during this call, so that the total sample complexity is $M\triangleq\sum^{I}_{i=0}M_i$. Observe that the $M_i$'s and $M$ are  random variables.


With these preliminary definitions we proceed to establish the following simple but useful preliminary estimate.
    \begin{lemma}\label{Le:tres}
       Assume \mbox{\sc (H)} and \mbox{\sc (S)}. Then, for all $i \ge i_0$ we have $\prob (S_{i}) \ge  \prob (G_{i}) \ge 1-\delta_{i}$.
    \end{lemma}
    \begin{proof}
          \cref{prop:sam-err0} guarantees $\prob{(G_i)}\ge 1-\delta_{i}$ so it suffices to show that $G_i \subseteq S_i$. This follows 
from \cref{thm:err} since for $i \ge i_0$ and all $\omega \in G_i$ we have
$$
\begin{aligned}
\sp{T^{n_i}(\omega)-Q^{n_i}(\omega)} &\le \sp{T^{n_i}(\omega)-\T(Q^{n_i})(\omega)}+ \sp{\T(Q^{n_i})(\omega)-Q^{n_i}(\omega)}\\
&\le \mbox{$2\,\varepsilon +\frac{8\sp{Q^0-Q^*}}{n_i+2}+4\,\varepsilon$} \\
&\le 14\,\varepsilon.
\end{aligned}
$$

\vspace{-5ex}
    \end{proof}
    
\begin{manualproposition}{\ref{Prop:3.3}}
Assume \mbox{\sc (H)} and \mbox{\sc (S)}. Then, $\expec [N] \le {2(1+\mu/\varepsilon)}/{(1-\delta)}$. In particular $N$ is finite almost surely and \mbox{\rm SAVIA\textbf{+}}$(Q^0,\varepsilon,\delta)$ stops with probability $1$ after finitely many loops.  
\end{manualproposition}
\begin{proof} 
Since in each loop {\rm SAVIA}\textbf{+}$(Q^0,n_i,\varepsilon,\delta_i)$ restarts afresh from $Q^0$, the events $\{S_i:i\in\NN\}$ are mutually independent and therefore
$$\prob(I=i) = \mbox{$\prob(\bigcap^{i-1}_{j=0} S^c_j \cap S_i)= \prod^{i-1}_{j=0}\prob( S^c_j)\cdot \prob( S_i)$}.$$
From \cref{Le:tres} we get $\prob (S^c_{i}) \le  \prob (G^c_{i}) \le\delta_{i}$ for all $i \ge i_0$ and then $\prob (I =i) \le \prod^{i-1}_{j=i_0} \delta_j$. On the other hand, from $2
\sum_{i=0}^\infty c_i^{-1}\leq 1$ it follows that
 $\delta_j=\delta/c_j\leq\delta/2$ and therefore
 $\prob (I =i) \le (\delta/2)^{i-i_0}$. Using this estimate 
and the identity $n_i=n_{i_0}2^{i-i_0}$, we obtain
     $$
    \begin{aligned}
    \expec[\st]&= \mbox{$\sum^{\infty}_{i=0}n_i\,\prob(N=n_i)$} \\&\le \mbox{$n_{i_{0}}+ \sum^{\infty}_{i=i_0+1}n_{i_0}2^{i-i_0}\,\prob(I=i)$}\\
    &\leq \mbox{$n_{i_{0}}(1+ \sum^{\infty}_{i=i_0+1} \,\delta^{i-i_0})$}.
        \end{aligned}
$$

\vspace{-1ex}
This last expression is exactly $n_{i_{0}}/(1-\delta)$ and the conclusion follows using the bound $n_{i_{0}}\leq 2(1+\mu/\varepsilon)$.
\end{proof}

\begin{manualtheorem}{\ref{thm:err-D}}
Assume \mbox{\sc (H)} and \mbox{\sc (S)}. Let $(Q^{N},T^{N},\pi^{N})$ be the output of $\mbox{\sc \mbox{\rm SAVIA\textbf{+}}}(Q^0,\varepsilon,\delta)$. Then, with probability at least $(1-\delta)$ we have, for all $s\in\cS$ 
\vspace{-1ex}
\begin{equation*}
g^*-g_{\pi^N}(s)\! \leq \sp{\T(Q^N)-Q^N}\leq 16\,\varepsilon
\end{equation*}
 with sample and time complexity ${O}\big(\widehat{L}\,|\cS||\cA|(
 \nu^2/\varepsilon^{2}\!+\!1)\big)$ where $\widehat{L}=\ln\!\big(4|\cS||\cA|\,(1\!+\!\mu/\varepsilon)/\delta \big)\,\log^4(2(1\!+\!\mu/\varepsilon))$. 
\end{manualtheorem}
\begin{proof}
The inequality between the policy error and the Bellman error follows from \cref{prop:err} so it suffices to prove the second one. Consider the events $A=\{I\leq i_0\}$ and $B=\bigcap_{i=0}^\infty G_i$. We claim that $\prob(A\cap B)\geq 1-\delta$. Indeed, by \cref{prop:sam-err0} we have $\prob( G^c_i) \le \delta_i$ so that
$$\mbox{$\prob(B^c)=\prob(\bigcup^{\infty}_{i=1} G^c_i) \le \sum_{i=1}^\infty {\delta}/{c_i} \leq {\delta}/{2}$},$$
while \cref{Le:tres} implies $\prob(A) \ge \prob \left (S_{i_0}) \ge \prob(G_{i_0} \right ) \geq 1-\delta_{i_0} \geq 1- \delta/2$, which combined yield $\prob(A^c\cup B^c)\leq\delta$.
Now, from the definition of $N$, $I$ and $G_i$, it follows that on the event $A\cap B$ we have
\begin{align*}
    \sp{\T(Q^N)-Q^N}&\leq \sp{\T(Q^N)-T^N}+\sp{T^N-Q^N}\leq 2\,\varepsilon+14\,\varepsilon=16\,\varepsilon.
\end{align*} 
Moreover, using \eqref{eq:last} in the proof of \cref{thm:err}, we can to bound the total sample complexity $M\triangleq\sum^{I}_{i=0}M_i$ as
$$M\leq \mbox{$\sum^{i_0}_{i=0}M_i=|\cS||\cA|\sum^{i_0}_{i=0}O\!\left(\alpha_i \sp{Q^0}^2/\varepsilon^{2}+ \mbox{$\alpha_i\ln^3(n_i+2)\sp{Q^0-Q^*}^2/\varepsilon^{2}+\alpha_i\,n_i^2\ln^2(n_i+2)$}\right),$}$$
where $\alpha_i= \ln(2|\cS||\cA|(n_i\!+\!1)/\delta_i)$ is the corresponding parameter defined in the $i$-th internal cycle of \mbox{\rm SAVIA\textbf{+}}. Since the terms in the sum above increase with $i$, and since $n_{i_0}^2 \leq 4(1+\mu/\varepsilon)^2=O(\sp{Q^0-Q^*}^2/\varepsilon^2 + 1)$, we obtain 
$$
\begin{aligned}
M &\leq |\cS||\cA| (i_0\!+\!1)\;  O\!\left(\alpha_{i_0} \sp{Q^0}^2/\varepsilon^{2}+ \mbox{$\alpha_{i_0}\log^3(n_{i_0}\!+2)\sp{Q^0-Q^*}^2/\varepsilon^{2}+\alpha_{i_0}\,n_{i_0}^2\log^2(n_{i_0}\!+2)$}\right)\\
&\leq |\cS||\cA|\, \alpha_{i_0}\log^4(n_{i_0}\!+2)\; O\!\left( \sp{Q^0}^2/\varepsilon^{2} + 2\mbox{$\sp{Q^0-Q^*}^2/\varepsilon^{2}+1$}\right)\\
%&\leq |\cS||\cA|\, \alpha_{i_0}\log^4(n_{i_0}\!+2)\; O\!\left( \nu^2/\varepsilon^{2}+1\right)\\
&\leq \widehat{L}\,|\cS||\cA|\, O\!\left( \nu^2/\varepsilon^{2}+1\right).
\end{aligned}
$$

\vspace{-5ex}
\end{proof}

\vspace{1ex}
\begin{manualcorollary}{\ref{cor:err}}
Assume \mbox{\sc (H)}, \mbox{\sc (S)}, $r(s,a) \in[0,1]$ for all $(s,a)\in\cS\times\cA$, and $\sp{h^{*}}\ge 1$. Let $(Q^N,T^N,\pi^N)$ be the output of $\mbox{\rm SAVIA\textbf{+}}(Q^0,\varepsilon/16,\delta)$
with $Q^0=0$ and $\varepsilon\leq 1$. Then, with probability at least $(1-\delta)$ we have,  for all $s\in\cS$
\vspace{-1ex}
  $$g^*-g_{\pi^N}(s)\leq \sp{\T(Q^N)-Q^N} \leq \varepsilon$$
 
  \vspace{-2ex}
   with sample and time complexity ${O}\big(\widetilde{L}\,|\cS||\cA|\sp{h^*}^{2}/\varepsilon^{2}\big)$
   where $\widetilde{L}= \ln \left(2|\cS||\cA|\sp{h^*}/(\varepsilon \delta)\right)\ln^4 (\sp{h^*}/\varepsilon)$.
\end{manualcorollary}
\begin{proof}
Since $Q^0=0$ we have $\sp{Q^0-Q^*}=\sp{Q^*}=\sp{\R+\cP\max h^*}\leq\sp{\R}+\sp{h^*}$.  The result then follows from \cref{{thm:err-D}} by noting that $2(1+\mu/\varepsilon)=O(\sp{h}^2/\varepsilon)$ which transforms $\widehat{L}$ into the multiplicative factor 
 $\widetilde{L}$.
\end{proof}


Next we proceed to establish the approximation and complexity results in expectation. We begin by the following technical Lemma which provides a crude bound for the sample complexity of each cycle of {\rm SAVIA\textbf{+}}.

\begin{lemma}\label{le:rough}
Let $M_i= |\cS||\cA|\mbox{$\sum_{j=0}^{n_i} m_j$}$ be the number of samples used in
\mbox{\rm SAVIA}$(Q^0,n_i,\varepsilon,\delta_i)$ during a given cycle $i \in \NN$. Then
$M_i\leq |\cS||\cA|\;O\!\left (n_i+(\kappa/\varepsilon)^2\alpha_i\, n_i^2 \log^2(n_i+2) \right )$
where $\alpha_i= \ln(2|\cS||\cA|(n_i\!+\!1)/\delta_i)$.

\end{lemma}
\begin{proof}
We first show by induction that for all $k=0,\ldots,n_i$ we have $\sp{d^k} \le \kappa$ and $\sp{T^{k-1}} \le (k+1) \kappa$. For $k=0$, this is true by initialization since $d^0=\max_\cA Q^0$ and $T^{-1}=\R$. Using \eqref{Eq:spk}, \eqref{eq:aux}, and the induction hypothesis, we have
\begin{align*}
    \sp{d^k}\le \sp{Q^{k}-Q^{k-1}} &\le \mbox{$ \frac{2}{(k+1)(k+2)}\sp{T^{k-1}-Q^0} + \frac{k-1}{k+1}\sp{d^{k-1}}$}\\
    &\le \mbox{$\frac{2}{(k+1)(k+2)}((k+1)\kappa + \kappa) + \frac{k-1}{k+1}\kappa=\kappa$}
    \end{align*}
    and consequently
    \begin{align*}
    \sp{T^k}\leq\sp{T^{k-1}}+\sp{D^k}&\leq(k+1)\kappa+\sp{d^k}\leq(k+2)\kappa.
    \end{align*}
Then, using these bounds, the sample complexity $M_i$ for the $i$-th cycle can be readily bounded as
\begin{align*}
 M_i&\leq |\cS||\cA|(\mbox{$ (n_i+1)+(\alpha_i/\varepsilon^{2})\sum_{j=0}^{n_i} c_j\sp{d^j}^2$})\\
& \le \mbox{$|\cS||\cA|((n_i+1)+5(\kappa/\varepsilon)^2\alpha_i\sum_{j=0}^{n_i}(j+2)\ln^2(j+2))$}\\
 & =  |\cS||\cA|\;O\!\left (n_i+(\kappa/\varepsilon)^2\alpha_i \,n_i^2 \log^2(n_i+2) \right ).
\end{align*}

\vspace{-5ex}
\end{proof}

\vspace{1ex}
\begin{manualtheorem}{\ref{thm:E-err-D}}
Assume \mbox{\sc (H)} and \mbox{\sc (S)}. Let $(Q^N,T^N,\pi^N)$ be the output of $\mbox{\rm SAVIA\textbf{+}}(Q^0,\varepsilon,\delta)$. Then, for all $s\in\cS$
$$\expec[g^*-g_{\pi^N}(s)] \leq 16\,\varepsilon +\delta \sp{r},$$
    with expected sample and time complexity
 $$\widetilde{O}\big(|\cS||\cA|\mbox{$(\nu^{2}/{\varepsilon^{2}}+1 +
    \delta\, (1+\mu/\varepsilon)^2(1+(\kappa/\varepsilon)^2)$}\big).$$
\end{manualtheorem}
\begin{proof}
Consider the events $A= \{I\leq i_0\}$ and $B= \bigcap_{i=1}^\infty G_i$ as in the proof of
\cref{thm:err-D} which established that 
on the event $A\cap B$ we have $g^*-g_{\pi^n}(s)\leq 16\,\varepsilon$ for all $s \in \cS$ with
$\prob(A\cap B)\geq 1-\delta$.
On the complementary event $(A\cap B)^c$ we can use the crude bound $g^*-g_{\pi^n}(s)\leq \sp{\R}$ and then, since $\prob((A\cap B)^c)\leq\delta$, we derive the first claim
    \[
    \expec[g^*-g_{\pi^n}(s)]\leq 16\,\varepsilon +  \delta\sp{\R}.
    \]
To estimate the expected value of the sample complexity $M=\sum_{i=0}^IM_i$ we note
that $A^c= \cupdot_{i=i_0+1}^\infty \{ I=i\}$, 
and therefore
\begin{align*}
    \expec[M]&=\mbox{$\expec[M|A \cap B ]\; \prob(A \cap B) +\expec[M | A \cap B^c ] \;\prob(A \cap B^c)+\sum_{i=i_0+1}^\infty \expec[M | I=i] \;\prob( I=i).$}
\end{align*}  
Let us bound separately the three terms in this sum. 
For the first term we observe that on the event $A\cap B$ we
can apply the bound in \cref{thm:err-D} and since $\prob(A \cap B) \leq1$ we get with $\widehat{L}=\ln\!\big(4|\cS||\cA|\,(1\!+\!\mu/\varepsilon)/\delta \big)\,\log^4(2(1\!+\!\mu/\varepsilon))$ that
\[
\begin{aligned}
\expec[M|A \cap B ]\; \prob(A \cap B)
&= {O}\big(\widehat{L}|\cS||\cA|(\nu^{2}/{\varepsilon^{2}}+1)\big).
\end{aligned}  
\]
 
For the second term, we combine $\prob(A \cap B^c)\leq \prob(B^c)\leq \delta $ with the bound in \cref{le:rough} and $n_{i_0}\leq 2(1+\mu/\varepsilon)$ to get
\begin{align*}
\expec[M|A \cap B^c ]\; \prob(A \cap B^c)&\leq \delta\;\mbox{$ |\cS||\cA|\;\sum_{i=0}^{i_0}O\!\left (n_i+{(\kappa/\varepsilon)^2\alpha_i n_i^2 \log^2(n_i+2)} \right )$}\\&\leq \delta\,|\cS||\cA|\;\mbox{$O \!\left (n_{i_0}+(\kappa/\varepsilon)^2\alpha_{i_{0}} n_{i_{0}}^2 \log^3(n_{i_0}\!+2) \right ) $}\\&\leq \delta\,|\cS||\cA|\;\mbox{$O \!\left (n_{i_0}+\widehat{L}\,(\kappa/\varepsilon)^2 n_{i_{0}}^2\right ) $}
\end{align*}  
where the last inequality follows from the bound 
$\alpha_{i_0}\log^3(n_{i_0}\!+2)\leq O(\widehat{L})$.

For the third term $R\triangleq \sum_{i=i_0+1}^\infty \expec[M|I=i]\; \prob( I=i)$, using again the bound in \cref{le:rough} we get
\begin{align*}
R&\leq |\cS||\cA|\;\mbox{$\sum_{i=i_0+1}^\infty O\!\left (n_i+(\kappa/\varepsilon)^2\alpha_i n_i^2 \log^2(n_i+2) \right )\prob( I=i)$}.
\end{align*}
To estimate the dominant terms in this last sum we recall that $\alpha_i=\log(2|\cS||\cA|(n_i+1)/\delta_i)$ with $n_i=2^i$ and $\delta_i=\delta/c_i$. Since $\log((n_i+1)c_i)=O(i)$ we get $\alpha_i=O(\widehat{L}+i)\leq \widehat{L}\,O(i)$, and similarly $\log^2(n_i+2)=O(i^2)$, so that 
\begin{align*}
R
&\leq |\cS||\cA|\;
\mbox{$\sum_{i=i_0+1}^\infty O\!\left (n_i+\widehat{L}\,(\kappa/\varepsilon)^2\,i^3 n_i^2\right )\prob( I=i)$}.
\end{align*}
Using the fact that $n_i=n_{i_0}2^{i-i_0}$ 
 and noting  that for $i\geq i_0+1$ we have $\prob (I =i) \le  \prod^{i-1}_{j=i_0} \delta_j\leq O\big(\delta\,\prod_{j=i_0}^{i-1}\frac{1}{j+2}\big)$
 (see the proof of 
\cref{Prop:3.3}), by setting $S_1=\sum_{i=i_0+1}^\infty 2^{i-i_0}\prod^{i-1}_{j=i_0} \frac{1}{j+2}$ and $S_2=\sum_{i=i_0+1}^\infty 2^{2(i-i_0)}\,i^3  \prod^{i-1}_{j=i_0} \frac{1}{j+2}$ we  derive
\begin{align*}
R&\leq \delta\,|\cS||\cA|\,\mbox{$O\!\left (S_1\, n_{i_0}+S_2\,\widehat{L}\,(\kappa/\varepsilon)^2n_{i_0}^2\right)$}.
\end{align*}
The sums $S_1$ and $S_2$ can be computed explicitly in terms of incomplete Gamma functions as 
\begin{align*}
S_1&=e^2\,2^{-(i_0+1)}[\Gamma(i_0+2)-\Gamma(i_0+2,2)]\leq (e^2-3)/2,\\
S_2&=84+4i_0 (i_0+5)+67 e^4 2^{-2(i_0+1)} [\Gamma (i_0+2)-\Gamma (i_0+2,4)]=O((i_0\!+\!1)^2).
\end{align*}
so the third term satisfies
\begin{align*}
R&\leq \delta\,|\cS||\cA|\;\mbox{$O\!\left (n_{i_0}+\widehat{L}\,(\kappa/\varepsilon)^2n_{i_0}^2 (i_0\!+\!1)^2\right)$}.
\end{align*}
Putting together these three bounds, by regrouping terms of the same order 
and ignoring logarithmic terms we get the announced bound for the expected value of the sample complexity 
\begin{align*}
    \expec[M]&\le |\cS||\cA|\,O\left(\mbox{$\widehat{L}(\nu^{2}/{\varepsilon^{2}}+1) +
    \delta\,n_{i_0}+\delta\,\widehat{L}\,(\kappa/\varepsilon)^2 n_{i_{0}}^2(i_0+1)^2$}\right)\\
    &= |\cS||\cA|\,\widetilde{O}\left(\mbox{$(\nu^{2}/{\varepsilon^{2}}+1) +
    \delta\, (1+\mu/\varepsilon)^2(1+(\kappa/\varepsilon)^2)$}\right).
\end{align*}  

\vspace{-3ex}
\end{proof}

\begin{manualcorollary}{\ref{cor:expected}}
   Assume \mbox{\sc (H)}, \mbox{\sc (S)},  $r(s,a) \in[0,1]$ for all $(s,a)\in \cS\times \cA$ and $\sp{h^{*}}\ge 1$. Let $(Q^N,T^N,\pi^N)$ be the output of $\mbox{\rm SAVIA$\textbf{+}$}(Q^0,\varepsilon/17,\delta)$ with  $Q^0=0$,  $\varepsilon\leq 1$, and $\delta=\varepsilon^2/17$. Then,  for all $s\in\cS$
  $$\expec[g^*-g_{\pi^N}(s)] \leq \varepsilon,$$
   with
   expected sample complexity  $\widetilde{O}\left({|\cS||\cA|\sp{h^*}^{2}}/{\varepsilon^{2}}\right)$.
\end{manualcorollary}

\begin{proof}
    From \cref{thm:E-err-D} we have
 $\expec[g^*-g_{\pi^n}(s)]\leq \frac{16}{17}\varepsilon +  \frac{\varepsilon^2}{17}\sp{\R}\leq \varepsilon$. Also the expected complexity follows directly from \cref{thm:E-err-D} and the choice of $\delta$.
\end{proof}





\section{Proposed algorithms for discounted MDPs}\label{sec:app_disc}
We present below the versions of our algorithms applied to the discounted setting. Notice that both procedure look very similar to the ones in the average reward case. As we mentioned, the only notable changes are the updating of the variable $T^k$ and that the variable $m_k$ depends now on the inifinity norm of $d^k$. Also, previous variable $h$ is now called $V$, to reflect the nature of the value function in this setting.
\begin{algorithm}[ht!]
   \caption{\mbox{\rm SAVID}$(Q^0,n,\varepsilon,\delta,\gamma)$}
   \label{alg:example-d}
\begin{algorithmic}
     \STATE {\bfseries Input:} $Q^{0}\!\in\real^{\cS\times\cA}$\,;\,$n\in\NN$\,;\,$\varepsilon>0$\,;\,$\delta\in(0,1)$
   \STATE $\alpha\hspace{0.8ex}=\ln(2|\cS||\cA|(n\!+\!1)/\delta)$
   \STATE $T^{-1}=\R$\,;\;$V^{-1}=0$\,;\,$\beta_0=0$
    \FOR{ $k=0,\ldots,n$ }
     \STATE $Q^{k} =(1\!-\!\beta_k)\,Q^0+\beta_k\, T^{k-1}$
     \STATE $V^k= \maxA(Q^{k})$
     \STATE $d^k=V^k-V^{k-1}$
    \STATE $m_k= \max\{\lceil 2\,\alpha\,c_k\infn{d^k}^2/\varepsilon^{2}\rceil,1\}$
     \STATE $D^k=\mbox{\sc Sample}(d^k, m_k)$ 
     \STATE $T^k=T^{k-1}+\gamma D^k$ 
   \ENDFOR
   \STATE $\pi^{n}(s) \in \Argmax_{a\in \cA} Q^{n}(s,a)\quad(\forall s\in\cS)$
   \STATE {\bfseries Output:} $Q^{n} ,T^n, \pi^{n} $ 
\end{algorithmic}
\end{algorithm}


\begin{algorithm}[ht!]
  \caption{\mbox{\sc \mbox{\rm SAVID\textbf{+}}}$(Q^0,\varepsilon,\delta, \gamma)$}
  \label{alg:example_sample-d}
\begin{algorithmic}
  \STATE {\bfseries Input:} $Q^{0}\!\in\real^{\cS\times\cA}$\,;\,$\varepsilon>0$\,;\,$\delta\in(0,1)$
  \STATE {\bfseries for} $i=0,1,\dots$ \,\,\textbf{do} 
    \STATE \quad Set $n_i= 2^i,  \delta_i = \delta/c_i $. 
    \STATE \quad  $(Q^{n_i},T^{n_i}, \pi^{n_i} ) = \mbox{\sc \mbox{\rm SAVID}}(Q^0,n_i,\varepsilon,\delta_i)$
     \STATE {\bfseries until} $\infn{T^{n_i}-Q^{n_i}}\leq 11\,\varepsilon$
\STATE {\bfseries Output:} $Q^{n_i},T^{n_i}, \pi^{n_i}$
\end{algorithmic}
\end{algorithm}

\section{Omitted proofs for the discounted reward setting}\label{app:discounted}
A nice feature of our approach, presented mainly in the average reward case, is that it can be applied almost verbatim to the discounted setup. Consequently, the proofs for  discounted MDPs and average reward MDPs are fairly similar. However, for the sake of completeness, the arguments are repeated or referred to those in \cref{app:average} when they are basically the same.
\subsection{Proofs of \cref{Se:complexity1-d}}
\begin{manualproposition}{\ref{prop:err-d}}
    Let $Q\in\reals^{\cS\times\cA}$ and $\pi:\cS\to\cA$ a policy such that $\pi(s)\in\Argmax_{a\in\cA}~Q(s,a)$. Then we have $ \infn{Q^*-Q_{\pi}}\leq \frac{2}{1-\gamma}\infn{\T_\gamma(Q)-Q}$.
\end{manualproposition}
\begin{proof}
Given that $\T_\gamma$ is a $\gamma$-contraction and using the triangular inequality, we get
\begin{align*}
        \infn{Q^*-Q} \le \infn{Q^*-\T_\gamma Q}+\infn{\T_\gamma Q-Q} \le \gamma \infn{Q^*-Q}+\infn{\T_\gamma Q-Q},
    \end{align*}
so that \[\infn{Q^{*}-Q} \le \frac{1}{1-\gamma}\infn{\T_\gamma Q-Q}.\]
Analogously, we have that
\begin{align*}
\infn{Q_{\pi}-Q} \le \infn{Q_{\pi}-\T_\gamma Q}+\infn{\T_\gamma Q-Q} \le \gamma \infn{Q_{\pi}-Q}+\infn{\T_\gamma Q-Q },
\end{align*}
% \Roberto{Where does the second inequality comes from?}
and then
\[\infn{Q_{\pi}-Q} \le \frac{1}{1-\gamma}\infn{\T_\gamma Q-Q}.\]
The result follows after using again the triangular inequality.
\end{proof}


Analogously to the average reward setting, let $\mathcal{F}_k=\sigma(\{D^i:i=0,\ldots,k\})$ be the natural filtration generated by the sampling process in {\rm SAVID}, and $\prob(\cdot)$ the probability distribution over the trajectories $(D^k)_{k\in\NN}$. Again, $T^k$ is $\mathcal{F}_k$-measurable  whereas $Q^k$, $V^k$, $d^k$, and $m_k$ are $\mathcal{F}_{k-1}$-measurable. 

\begin{proposition}\label{prop:sam-err0-d}
Let $c_k>0$  with \mbox{$2\sum_{k=0}^\infty c_k^{-1}\leq 1$} and $T^k, Q^k$ the iterates generated by \mbox{\rm SAVID}$(Q^0,n,\varepsilon,\delta, \gamma)$.
Then, with probability at least $1-\delta$ we have $\|T^k-\T_\gamma(Q^k)\|_\infty\le  \gamma \varepsilon$ simultaneously for 
 all $k=0,\ldots,n$.
\end{proposition}
\vspace{-3ex}
\begin{proof}
Let $Y^i=D^i-\cP d^i$ and $X^k=\sum_{i=0}^kY^i$.
Since $V^{-1}=0$, by telescoping $\T_\gamma(Q^k)=\R+\gamma \cP h^k=\R+\gamma \sum_{i=0}^k \cP d^i$ and then using \eqref{Eq:telescope} we get $T^k-\T_\gamma(Q^k)=\gamma \sum_{i=0}^k(D^i-\cP d^i)=\gamma X^k$. The estimation of $\prob(\|X^k\|_\infty\geq\varepsilon)$ proceeds in the exact same manner as in the proof of \cref{prop:sam-err0}.
\end{proof}

From now on we consider the specific sequences $c_k=5(k+2)\ln^2(k+2)$ and $\beta_k=k/(k+2)$, meaning that we assume \mbox{\sc (S)}. Let $Q^*$ be the unique solution of the Bellman equation $\T_\gamma Q=Q$. We assume throughout that
\begin{equation}\label{Eq:Ek-d}
\infn{T^k-\T_\gamma(Q^{k})}\le \varepsilon\qquad \text{for all} \,\, k=0,\ldots,n.
\end{equation}
which, in view of \cref{prop:sam-err0-d}, holds with probability at least $(1-\delta)$ when \mbox{\rm SAVID}$(Q^0,n,\varepsilon,\delta, \gamma)$ is implemented.  

Also, as noted earlier, for $D=\mbox{\sc Sample}(d,m)$ we have $\infn{D}\leq\infn{d}$, which combined with 
 the nonexpansivity of the map $Q\mapsto \maxA(Q)$ implies
\begin{equation}\label{Eq:spk-d}\infn{T^k-T^{k-1}}=\gamma \infn{D^k} \le \gamma \infn{d^k}=\gamma \infn{V^k-V^{k-1}}\le \gamma \infn{Q^{k}-Q^{k-1}}.
\end{equation}
The following two technical lemmas serve as counterparts to \cref{Le:uno,Le:dos} in this setting. Unlike the previous results, they establish estimates from the iterate $Q^k$ to $Q^*$ in the infinity norm and therefore the conclusions change slightly. As before, these lemmas act as a prelude to the general bound given by \cref{thm:err-d}.
\begin{lemma}\label{Le:uno-d}Assuming \mbox{\sc (S)} and \eqref{Eq:Ek-d}, we have that 
$$\infn{Q^k-Q^*}\leq \infn{Q^0-Q^*}+\frac{1}{3}\,\varepsilon\,k \quad \text{for all} \,\, k=0,\ldots,n.$$
\end{lemma}
\begin{proof}
From the iteration $Q^{k}=(1-\beta_k)Q^0+\beta_k\,T^{k-1}$ with $\beta_k=\frac{k}{k+2}$ we get
\begin{equation*}
    \infn{Q^{k}-Q^*}\le \mbox{$\frac{2}{k+2}\infn{Q^0-Q^*}+\frac{k}{k+2}\infn{T^{k-1}-Q^*}$}.
\end{equation*}
Bound \eqref{Eq:Ek-d} and $\gamma \le 1$ imply
    \begin{equation}\label{Eq:cit-d}
    \infn{T^{k-1}-Q^*}=\infn{T^{k-1}-\T_\gamma(Q^*)}\le \varepsilon+\gamma \infn{Q^{k-1}-Q^*}\le  \varepsilon+\infn{Q^{k-1}-Q^*}.
\end{equation}
From here the proof follows the exact same lines as in the proof of \cref{Le:uno}.
\end{proof}

\begin{lemma}\label{Le:dos-d} Assume \mbox{\sc (S)} and \eqref{Eq:Ek-d} and let $\rho_k=2\infn{Q^0-Q^*}+\frac{1}{3}\,\varepsilon\,k$. Then 
$$\infn{Q^{k}-Q^{k-1}} \le \frac{2}{k(k+1)}\sum_{i=1}^{k}\rho_{i+2}.\quad \quad \text{for all} \,\, k=1,\ldots,n.$$
\end{lemma}
\begin{proof}
From 
$Q^{k}=\frac{2}{k+2}Q^0+\frac{k}{k+2}T^{k-1}$ and
$Q^{k-1}=\frac{2}{k+1}Q^0+\frac{k-1}{k+1}T^{k-2}$
we derive
    \begin{equation*}
    Q^{k}-Q^{k-1}
    =\mbox{$ \frac{2}{(k+1)(k+2)}(T^{k-1}-Q^0) + \frac{k-1}{k+1}(T^{k-1}-T^{k-2})$}.
\end{equation*}
Now, \eqref{Eq:cit-d} together with \cref{Le:uno-d}  readily imply $\infn{T^{k-1}-Q^0}\leq \infn{T^{k-1}-Q^*} +\infn{Q^*-Q^0}\leq\rho_{k+2}$, 
and therefore using \eqref{Eq:spk-d} we get
\begin{align*}
    \infn{Q^{k}-Q^{k-1}} %&\le \mbox{$\frac{2}{(k+1)(k+2)}\rho_{k+2} +  \frac{k-1}{k+1}\sp{T^{k-1}- T^{k-2}}$}\\ 
    &\le \mbox{$\frac{2}{(k+1)(k+2)}\rho_{k+2} +  \gamma\frac{k-1}{k+1}\infn{Q^{k-1}- Q^{k-2}}$}.
\end{align*}
Using that $\gamma\leq 1$, the proof finishes exactly as in \cref{Le:dos}
\end{proof}

The folowing theorem is the analogue of \cref{thm:err} for discounted MDPs. For the sake of completeness and given that the constants involved are different, we provide the full proof.
\begin{theorem}\label{thm:err-d}
Assume \mbox{\sc (S)} and let $(Q^n,T^n,\pi^n)$ be the output computed by $\mbox{\rm SAVID}(Q^0,n,\varepsilon,\delta, \gamma)$. Then, with probability at least $(1-\delta)$ we have
\vspace{-1ex}
$$  \infn{\T_\gamma(Q^n)-Q^n}\leq \frac{8\infn{Q^0-Q^*}}{n+2}+2\varepsilon,$$

\vspace{-2ex}
with a sample and time complexity of order $${O}\big(L_\gamma\,|\cS||\cA|\big(({\infn{Q^0-Q^*}^{2}+\infn{Q^0}^{2}})/{\varepsilon^{2}}+n^2\big)\big)$$ 
where  
$L_\gamma=\ln(2|\cS||\cA|(n+1)/\delta)\log^3(n+2)$.
\end{theorem}
\begin{proof}
From \cref{prop:sam-err0-d}, with probability at least $(1-\delta)$ we have $\infn{T^k-\T_\gamma(Q^k)}\leq\varepsilon$ for all $k=0,\ldots,n$. Now, the recursion $Q^n=\frac{2}{n+2}Q^0+\frac{n}{n+2}T^{n-1}$ implies
 \begin{equation}\label{eq:lll-d}
    \T_\gamma(Q^n)-Q^n
    =\mbox{$\frac{2}{n+2}\big(\T_\gamma(Q^n)-Q^0\big)+ \frac{n}{n+2} \big(\T_\gamma(Q^n)-\T_\gamma(Q^{n-1})\big)+ \frac{n}{n+2}\big(\T_\gamma(Q^{n-1})-T^{n-1}\big)$}.
\end{equation}
Using the triangle inequality and Lemma~\ref{Le:uno-d}, we have that  
$$\infn{\T_\gamma(Q^n)-Q^0} \le \gamma \infn{Q^n-Q^*}+\infn{Q^*-Q^0} \le 2\infn{Q^0-Q^*}+\frac{1}{3}\,\varepsilon\,n =  \rho_n,$$ 
while \cref{Le:dos-d} gives $\infn{\T_\gamma(Q^n)-\T_\gamma(Q^{n-1})}\leq\frac{2}{n(n+1)}\sum_{i=1}^{n}\rho_{i+2}$. Thus, applying a triangle inequality to \eqref{eq:lll-d} and using these estimates together with \cref{prop:err-d}, we obtain 
\begin{align*}
    \infn{\T_\gamma(Q^n)-Q^n}&\le \mbox{$\frac{2}{n+2}\,\rho_n+\frac{2}{(n+1)(n+2)}\sum_{i=1}^{n}\rho_{i+2}+ \varepsilon$}
   \\& = \mbox{$\frac{4(1+2n)}{(n+1)(n+2)}\infn{Q^0-Q^*}+ \frac{2(3+8n+3n^2)}{3(n+1)(n+2)}\varepsilon$}
    \\& \le \mbox{$\frac{8\infn{Q^0-Q^*}}{n+2}+2\varepsilon$}.%\\& 
\end{align*}
To estimate the complexity, for $k \ge 1$, we use the inequality $\infn{d^k}\le\infn{Q^k-Q^{k-1}}$ in \eqref{Eq:spk-d} together with \cref{Le:dos-d} to find
\begin{align*}
    \mbox{$\infn{d^k}$}&\leq\mbox{$\frac{2}{k(k+1)}\sum_{i=1}^{k}\rho_{i+2}$}\\
    &=\mbox{$\frac{4}{k+1}\infn{Q^0-Q^*}$} +\mbox{$\frac{k+5}{3(k+1)}\varepsilon$}\\
    &\leq \mbox{$\frac{4}{k+1}\infn{Q^0-Q^*}+\varepsilon$}.
\end{align*}
Now, to estimate the total number of samples $|\cS||\cA|\sum_{k=0}^n m_k$ 
we recall that $m_k=\max\{\lceil 2 \alpha c_k\infn{d^{k}}^2/\varepsilon^{2}\rceil,1\}$ which can be bounded as $m_k\leq 1+2\alpha c_k\infn{d^{k}}^2/\varepsilon^{2}$. Then 
\begin{align}
    \mbox{$\sum_{k=0}^nm_k$}&\leq\mbox{$ (n+1)+\frac{2\alpha}{\varepsilon^{2}}\sum_{k=0}^n c_k\infn{d^{k}}^2$} \nonumber\\
   &\leq\mbox{$ (n+1)+\frac{20\, \alpha}{\varepsilon^{2}} \ln^2(2)\,\infn{Q_0}^2+\frac{10\alpha}{\varepsilon^{2}}\sum_{k=1}^n (k+2)\ln^2(k+2)\big(\frac{4}{k+1}\infn{Q^0-Q^*}+\varepsilon\big)^2$} \nonumber\\
      &\leq\mbox{$ (n+1)+\frac{20\, \alpha}{\varepsilon^{2}} \ln^2(2)\infn{Q_0}^2+\sum_{k=1}^n\frac{480\,\alpha}{\varepsilon^{2}(k+2)}\ln^2(k+2)\infn{Q^0-Q^*}^2+20\,\alpha\sum_{k=1}^n(k+2)\ln^2(k+2)$} \nonumber\\
   &=O\!\left( \alpha \infn{Q^0}^2/\varepsilon^{2}+\mbox{$\alpha\ln^3(n+2)\infn{Q^0-Q^*}^2/\varepsilon^{2}+\alpha \,n^2\ln^2(n+2)$}\right),
   \label{eq:last-d}
\end{align}
where we use the same estimations as in the proof of \cref{thm:err}.
\end{proof}

\begin{manualtheorem}{4.2}%\label{thm:err_appendix}
   Assume \mbox{\sc (S)}, $ r(s,a) \in [0,1]$ for all $(s,a)\in\cS\times \cA$, and $n\!=\!\lceil{10/((1\!-\!\gamma)\varepsilon)}\rceil$. Let $(Q^n,T^n,\pi^n)$ be the output of $ \mbox{\rm SAVID}(Q^0,n, \epsilon/10, \delta, \gamma)$ with $Q^0=0$ and $\varepsilon \le 1/(1\!-\!\gamma)$. Then, with probability at least $(1-\delta)$ we have
\vspace{-1ex}
  $$ \infn{\T_\gamma(Q^n)-Q^n}\leq \varepsilon$$
 
  \vspace{-2ex}
   with sample and time complexity ${O}\big(L_{\gamma}|\cS||\cA|/((1\!-\!\gamma)^2\varepsilon^{2})\big)$
   where $L_{\gamma}=\ln (2|\cS||\cA|/((1\!-\!\gamma)\varepsilon \delta))\ln^3 (2/((1\!-\!\gamma)\varepsilon)) $.
\end{manualtheorem}
\begin{proof}
   The proof follows directly  from  \cref{thm:err-d} and the bound $\infn{Q^*} \le 1/(1\!-\!\gamma)$.
\end{proof}

\subsection{Proofs of \cref{Se:complexity2-d}}
We now proceed to establish the complexity of the algorithm \mbox{\rm SAVID\textbf{+}}. Again, the proof is almost the same as in the average reward case, where some of the constants must be estimated differently. Precisely, the time in which the inner loop in \mbox{\rm SAVID\textbf{+}} stops will now depend on $\infn{Q^0 - Q^*}$. 

Under a slight abuse of notation, we define the stopping time of \mbox{\rm SAVID\textbf{+}} as
$$
 \st =\inf \{ { n_i \in \mathbb N \,:\, \infn{T^{n_i}-Q^{n_i}} \le 11\,\varepsilon}\},
$$
where $T^{n_i}$ and $Q^{n_i}$'s are the iterates generated in each loop of \mbox{\rm SAVID\textbf{+}}$(Q^0,\varepsilon,\delta, \gamma)$. As before, we let $i_0 \in \NN$ be the smallest integer satisfying $n_{i_0}\ge\infn{Q^0-Q^{*}}/\varepsilon$, so that 
either $i_0=0$ and $n_{i_0}=1$ or $n_{i_0-1}=n_{i_0}/2<\infn{Q^0-Q^{*}}/\varepsilon$, which combined imply $n_{i_0}\le 1+2\infn{Q^0-Q^{*}}/\varepsilon$. Also, as in the average reward setting, we consider the events
    \begin{equation*}
S_{i}=\{\infn{T^{n_i}-Q^{n_i}} \le 11\,\varepsilon\} \quad \text{and} \quad G_i=\{ \infn{T^k-\T_\gamma (Q^k)} \le \varepsilon,\;\forall k=0,\ldots,n_i\},
\end{equation*}
with $T^{k}$ and $Q^{k}$'s the inner iterates generated during the execution of \mbox{\rm SAVID}$(Q^0,n_i,\varepsilon,\delta_i, \gamma)$ in the $i$-th loop of  \mbox{\rm SAVID\textbf{+}}. 
    \begin{lemma}\label{Le:tres-d}
       Assume \mbox{\sc (S)}. Then, for all $i \ge i_0$ we have that $\prob (S_{i}) \ge  \prob (G_{i}) \ge 1-\delta_{i}$.
    \end{lemma}
    \begin{proof} The proof is the same as its average reward counterpart \cref{Le:tres} by just observing that, if $i \ge i_0$ then, for all $\omega \in G_i$,
$$
\begin{aligned}
\infn{T^{n_i}(\omega)-Q^{n_i}(\omega)} &\le \infn{T^{n_i}(\omega)-\T_\gamma(Q^{n_i})(\omega)}+ \infn{\T_\gamma(Q^{n_i})(\omega)-Q^{n_i}(\omega)}\\
&\le \mbox{$\varepsilon +\frac{8\infn{Q^0-Q^*}}{n_i+2}+2\,\varepsilon$} \\
&\le 11\,\varepsilon.
\end{aligned}
$$

\vspace{-5ex}
    \end{proof}

\begin{theorem}{\label{thm:err-D-d}}
Assume \mbox{\sc (S)} and let $(Q^{N},T^{N},\pi^{N})$ be the output of $\mbox{\sc \mbox{\rm SAVID\textbf{+}}}(Q^0,\varepsilon,\delta, \gamma)$. Then, with probability at least $(1-\delta)$ we have 
\vspace{-1ex}
$$  \infn{\T_\gamma(Q^N)-Q^N}\leq 12\,\varepsilon$$
 with sample and time complexity $${O}\left(\widehat{L}_{\gamma}|\cS||\cA|\left(
 (\infn{Q^0}+\infn{Q^0-Q^*})^2/\varepsilon^{2}+1\right)\right),$$ where $\widehat{L}_{\gamma}=\ln\!\big(|\cS||\cA|\,(1\!+\!2\infn{Q^0-Q^*}/\varepsilon)/\delta \big)\,\log^4(1\!+\!2\infn{Q^0-Q^*}/\varepsilon).$ 
\end{theorem}

\begin{proof}
Consider the events $A=\{I\leq i_0\}$ and $B=\bigcap_{i=0}^\infty G_i$. Using the exact same argument as in the proof of \cref{thm:err-D}, now through \cref{prop:sam-err0-d} and \cref{Le:tres-d}, we know that $\prob(A\cap B)\geq 1-\delta$. Also, on the event $A\cap B$ and from the definition of $N$, we have
\begin{align*}
    \infn{\T_\gamma(Q^N)-Q^N}&\leq \infn{\T_\gamma(Q^N)-T^N}+\infn{T^N-Q^N}\leq \varepsilon+11\,\varepsilon=12\,\varepsilon.
\end{align*} 
Now, using \eqref{eq:last-d} in the proof of \cref{thm:err-d}
the total sample complexity $M$ of the algorithm can be estimated, as in the average reward case, by
$$M\leq |\cS||\cA|\sum^{i_0}_{i=0}O\!\left(\alpha_i \infn{Q^0}^2/\varepsilon^{2}+ \mbox{$\alpha_i\ln^3(n_i+2)\infn{Q^0-Q^*}^2/\varepsilon^{2}+\alpha_i\,n_i^2\ln^2(n_i+2)$}\right),$$
where $\alpha_i= \ln(2|\cS||\cA|(n_i\!+\!1)/\delta_i)$ is the parameter defined in the $i$-th cycle of \mbox{\rm SAVID\textbf{+}}. Again, as for \mbox{\rm SAVIA\textbf{+}}, we use that $n_{i_0}^2 \leq (1+2\infn{Q^0-Q^{*}}/\varepsilon)^2=O(\infn{Q^0-Q^*}^2/\varepsilon^2 + 1)$, to get 
$$
\begin{aligned}
M &\leq |\cS||\cA| \;  O\!\left(\alpha_{i_0} \infn{Q^0}^2/\varepsilon^{2} \log(n_{i_0}\!+2)+ \mbox{$\alpha_{i_0}\log^4(n_{i_0}\!+2)\infn{Q^0-Q^*}^2/\varepsilon^{2}+\alpha_{i_0}\,n_{i_0}^2\log^4(n_{i_0}\!+2)$}\right)\\
&\leq |\cS||\cA|\, \alpha_{i_0}\log^4(n_{i_0}\!+2)\; O\!\left( (\infn{Q^0}+\infn{Q^0-Q^*})^2/\varepsilon^{2}+1\right).
\end{aligned}
$$
\end{proof}
Finally, we state our claimed complexity result for \mbox{\rm SAVID\textbf{+}}, whose proof follows directly from \cref{prop:sam-err0-d}, \cref{thm:err-D-d}, and the fact that $\infn{Q^*} \le 1/(1\!-\!\gamma)$.
\begin{manualtheorem}{\ref{cor:err2-d}}
Assume \mbox{\sc (S)}, $  r(s,a) \in [0,1]$ for all $(s,a)\in \cS\times \cA$, and $\infn{Q^*} \ge 1$. Let $(Q^N,T^N,\pi^N)$ be the output of $ \mbox{\rm SAVID\textbf{+}}(Q^0,\varepsilon(1\!-\!\gamma)/24,\delta, \gamma)$ with $Q^0=0$ and $\varepsilon \le 1/(1\!-\!\gamma)$. Then, with probability at least $(1-\delta)$ we have
\vspace{-1ex}
  $$ \infn{Q^*-Q_{\pi^N}}\leq 2\infn{\T_\gamma(Q^N)-Q^N}/(1\!-\!\gamma) \leq \varepsilon$$
 
  \vspace{-2ex}
   with sample and time complexity $${O\!}\left(\widetilde{L}_{\gamma}|\cS||\cA|\infn{Q^*}^{2}/((1\!-\!\gamma)^2\varepsilon^{2})\right)$$
   where $\widetilde{L}_{\gamma}= \ln \big(2|\cS||\cA|/((1\!-\!\gamma)\varepsilon \delta)\big)\ln^4 \big(2/((1\!-\!\gamma)\varepsilon)\big)$.
\end{manualtheorem}




\section{The Anchored Value Iteration as Halpern's iteration on a quotient space} \label{ap:quotient}
The recursion \eqref{Halpern} involves the unknown optimal value $g^*$.
However, since $\T(Q+c)=\T(Q)+c$ is homogeneous under addition of constants $c\in\reals$, 
this prompts us to consider the map  $\widetilde{\T}:\X\to  \X$ defined by 
$\widetilde{\T}([Q])=[\T(Q)]$ on the quotient space $\X=\reals^{\cS\times\cA}/E$ with $E$ the subspace of constant matrices, endowed with 
the induced quotient norm
$$\big\|[Q]\big\|_E=\min_{c\in\reals}\big\|Q+c\big\|_\infty=\frac{1}{2}\sp{Q}.$$
One can readily check that $\widetilde{\T}$ is nonexpansive for the norm $\|\cdot\|_E$
and moreover\\[1ex]
{\sc Claim:} $Q^*\in\Fix(\T_{g^*})$ if and only if $[Q^*]\in\Fix(\widetilde{\T})$.\\[0.5ex]
{\em Proof.} If  $Q^*=\T_{g^*}(Q^*)$  then $[Q^*]=[\T(Q^*\!-\!g^*)]=\widetilde{\T}([Q^*\!-\!g^*])=\widetilde{\T}([Q^*])$.
Conversely, if $[Q^*]=\widetilde{\T}([Q^*])=[\T(Q^*)]$ then
$Q^*=\T(Q^*)+c$ for some $c\in\reals$, and \citet[Theorem 9.1.2]{10.5555/528623} 
gives $c=-g^*$ so that $Q^*\in\Fix(\T_{g^*})$.\hfill$\Box$

\vspace{1ex}
Projecting \eqref{Halpern} on the quotient space $\X$ it follows that the equivalence classes $[Q^k]=Q^k+E$ of the iterates $\{Q^k\}_{k\in\NN}$ 
generated by \eqref{Halpern} satisfy the following Halpern's iteration for $\widetilde{\T}(\cdot)$
\begin{equation*}
[Q^{k+1}]=(1-\beta_{k+1})[Q^0]+ \beta_{k+1}\, \widetilde{\T}([Q^{k}]).
\end{equation*}
In this projected iteration the unknown $g^*$ plays no role. Moreover, the equivalence classes $[Q^n]$ coincide with those generated by the implementable
modification \eqref{eq:Anc-VI} in which $g^*$ is ignored. 
This shows that, modulo constants, both \eqref{Halpern} and \eqref{eq:Anc-VI} are equivalent and their corresponding residuals in span seminorm coincide.

Finally, using the identity
$\sp{Q-\T(Q)}=2\big\|[Q]-\widetilde{\T}([Q])\big\|$,
any error bound for Halpern's iteration as applied to $\widetilde{\T}$ directly transfers into a  bound for $\sp{Q^k-\T(Q^k)}$. 
In particular, for $\beta_k=k/(k+2)$ we get
$$\sp{Q^k-\T(Q^k)}\leq\mbox{$\frac{4}{k+1}$}\sp{Q^0-Q^*}.$$






\iffalse
\newpage 
\subsection{Old Proof of \cref{prop:sam-err0}}

\begin{algorithm}[ht!]
   \caption{\mbox{\rm SAVIA}$(Q^0,n,\{c_k\}_{k=1}^n)$}
   \label{alg:example_old}
\begin{algorithmic}
     \STATE {\bfseries Input:} $Q^{0}\!\in\real^{\cS\times\cA}$\,;\,$n\in\NN$\,;\,$\{c_k\}_{k=1}^n\subseteq \reals_+$
   \STATE {\bfseries Initialize:} $R^{0} \!=\mathbf{0}_{\cS\times\cA}\,;\, h^{0}\! =\mathbf{0}_{\cS}\,;\,  \beta_k\!=\! \frac{k}{k+2}$  
   \FOR{ $k=1,\ldots,n$ }
     \STATE $h^k= \maxA Q^{k-1}$
     \STATE $d^k=h^k-h^{k-1}$
    \STATE $m_k= \lceil c_k\sp{d^k}^2\rceil$
     \STATE $D^k = \mbox{\sc Sample}(d^k, m_k)$ 
     \STATE $R^k = R^{k-1}+D^k$ 
     \STATE $Q^{k} =(1\!-\!\beta_k)\,Q^0+\beta_k (r+R^k)$
   \ENDFOR
   \STATE $\pi^{n-1}(s) \in \Argmax_{a\in \cA} Q^{n-1}(s,a)\quad(\forall s\in\cS)$
   \STATE {\bfseries Output:} $Q^{n-1} , \pi^{n-1} $ 
\end{algorithmic}
\end{algorithm}


\begin{proposition}\label{prop:sam-err01}
Let $h^k,R^k$ be the sequences generated in \mbox{\rm SAVIA}$(Q^0,n, \{c_k\}^n_{k=1})$ and set $\alpha_k=\sum_{i=1}^k\!c_i^{-1}$.
Then, for all $k=1,\ldots,n$ we have
 $\prob(\|R^k-\cP h^k\|_\infty\ge  \varepsilon)\le 2\,|\cS||\cA|\exp\big(\!-\mbox{$\frac{1}{2}$}\,\varepsilon^2/\alpha_k\big)$. 
\end{proposition}

\begin{proof}
    Let $d^k$ and $D^k$ as in {\sc savi}, so that $h^k=\sum_{i=1}^kd^i$ and $R^k=\sum_{i=1}^kD^i$. Thus,  denoting $H^k=D^k-\cP d^k$ and $M^k=\sum_{i=1}^kH^i$ we have $M^k=R^k-\cP h^k$, so we look for an upper bound on $\prob(\|M^k\|_\infty\geq\varepsilon)$. 
    
    To this end we adapt the arguments of Azuma-Hoeffding's inequality.
    Denoting $s_{k,j}^{s,a}\sim \cP(\cdot \,|\, s,a)$ for $j=1,\ldots,m_k$ the samples for $(s,a)\in\cS\times \cA$ at the $k$-th iteration, we have explicitly 
     $$ H^{k}(s,a) =\mbox{$\frac{1}{m_k}\sum_{j=1}^{m_k}\big(d^k(s^{s,a}_{k,j})$}-\cP d^k(s,a)\big)\qquad\forall (s,a)\in\cS\times\cA.$$ 
     Since $d^k$ and $m_k$ are $\mathcal{F}_{k-1}$-measurable, it follows that $\expec{[H^{k}|\mathcal{F}_{k-1}]}=0$ 
     and therefore $M^k$ is an $\mathcal{F}_{k}$-martingale.
From Markov's inequality and the tower property of conditional expectations we get that for each $(s,a)\in\cS\times \cA$ and $\lambda>0$
\begin{align*}
   \mbox{$ \prob( M^k(s,a) \ge \varepsilon)$} %&=     \mbox{$\prob(\exp(s \sum^k_{i=0}H_{i})$}\ge  \exp(s\varepsilon))  \\
   & \le e^{-\lambda\varepsilon}\,\expec\big[\exp(\lambda\, \mbox{$M^k(s,a)$})\big]\\
   &= e^{-\lambda\varepsilon}\,\expec\big[\exp(\lambda\,\mbox{$M^{k-1}(s,a)$}) \;\expec[\exp(\lambda \, H^{k}(s,a)) \,|\, \mathcal{F}_{k-1}]\big].
\end{align*}
Now, conditionally on $\mathcal{F}_{k-1}$, the terms $H^k_j=\mbox{$\frac{1}{m_k}\big(d^k(s^{s,a}_{k,j})$}-\cP d^k(s,a)\big)$ in the sum of $H^k(s,a)$ are independent random variables with zero mean  and $|H^k_j| \le \frac{1}{m_k}\sp{d^k}$, so that  using Hoeffding's Lemma we get
\begin{align*}
    \expec\big[\exp(\lambda\,H^{k}(s,a)) \,|\, \mathcal{F}_{k-1}\big]  &=\prod_{j=1}^{m_k}\expec\big[\exp(\lambda\,H^k_j) \,|\, \mathcal{F}_{k-1}\big] 
    \le \exp\big(\mbox{$\frac{1}{2}$}\lambda^2\sp{d^k}^2/m_k\big). %\\
\end{align*}
Since $m_k\geq c_k\sp{d^k}^2$ a simple induction gives $\expec[\exp(\lambda\,M^k(s,a))] \leq\exp\big(\frac{1}{2}\lambda^2\sum^k_{i=1}\!c_i^{-1}\big)=\exp\big(\frac{1}{2}\alpha_k\lambda^2\big)$, 
and therefore 
$\prob(M^k(s,a)\geq\varepsilon)\leq \exp\big(\mbox{$-\lambda\,\varepsilon+\frac{1}{2}\alpha_k\lambda^2$}\big)$.
Taking $\lambda=\varepsilon/\alpha_k$ which minimizes the right hand side, we obtain
$$\prob(M^k(s,a)\geq\varepsilon)\leq \exp\big(\!-\mbox{$\frac{1}{2}$}\varepsilon^2/\alpha_k\big).$$
A symmetric argument yields the same bound for $\prob(-M^k(s,a)\geq\varepsilon)$ so that 
$\prob(|M^k(s,a)|\geq\varepsilon)\leq 2\exp\big(\!-\mbox{$\frac{1}{2}$}\varepsilon^2/\alpha_k\big)$, and then applying a union bound over all $(s,a)\in\cS\times\cA$ we get $\prob(\|M^k\|_\infty\geq\varepsilon)\leq 2|\cS||\cA|\exp\big(\!-\mbox{$\frac{1}{2}$}\varepsilon^2/\alpha_k\big)$.
\end{proof}

\fi
\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.