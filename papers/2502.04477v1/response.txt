\section{Previous work}
\label{sec:prev_work}
% but one has access to some device that generates samples
\paragraph{Average reward MDPs.}
The setup of average reward MDPs was introduced  in the dynamic programming literature by Puterman, "Markov Decision Processes" ____, while Powell established a theoretical framework for their analysis. Reinforcement learning provides a variety of methods to approximately solve average reward MDPs in the case where the transition matrix and reward are unknown Sutton et al., "Introduction to Modern Reinforcement Learning". These methods include model-based algorithms Kearns and Singh, "Near-Optimal Reinforcement Learning in Polynomial Time"____, model-free algorithms Mnih et al., "Human-level control through deep reinforcement learning"____, policy gradient methods Schulman et al., "Trust Region Policy Optimization"____, and mirror descent  Dai et al., "Proximal Policy Optimization Algorithms". 


\paragraph{Model-based methods with a generative model.}
Model-based algorithms use sampling to estimate the transition probabilities of the MDP, and then solve the surrogate model by using standard dynamic programming techniques. These methods can achieve optimal sample complexity in both the discounted and average reward setups. For discounted MDPs, Dai et al. achieve optimal sample complexity by matching the lower bound in Munos et al., "A theoretical analysis of Q-learning with function approximation". For average reward MDPs,  Farahmand achieves optimal sample complexity assuming finite mixing times, while Chen et al. achieve optimal complexity for weakly communicating and multichain MDPs. Also Kumar et al. study a model-based method with no prior knowledge  for weakly communicating MDPs. 

\paragraph{Model-free methods with a generative model.}
Model-free algorithms directly estimate the Q-factors and policy without learning a model of the transition probabilities. They are more efficient in terms of  computation and memory requirements, and can tackle large scale problems when combined with function approximation. 

In the generative model setup Sutton et al. the sample complexity of model-free algorithms has been widely studied. For  discounted MDPs they achieve  optimal sample complexity matching the complexity lower bound Munos et al., "A theoretical analysis of Q-learning with function approximation". 
For average rewards, \cref{Tabla1} presents a summary of the sample complexity of previous model-free algorithms. Specifically, for MDPs with finite mixing times $t_{\text{mix}}$, Chen et al.  developed a model-free method that applies a primal-dual algorithm to a bilinear saddle point reformulation of the Bellman equation.  Also under the mixing  condition, Dai et al. use a stochastic mirror descent framework to solve bilinear saddle point problems, whereas Abdolmaleki et al. studied an actor-critic method also based on stochastic mirror descent. Chen first obtained sample complexity results for weakly communicating MDPs, by applying Q-learning with variance reduction to an approximation by a discounted 
MDP. Lower bounds on the sample complexity for mixing and weakly communicating MDPs were established respectively by Farahmand et al. and Kakade et al..

A drawback of these previous model-free algorithms is that their implementation requires prior estimates of the
mixing times of the Markov chains induced by all policies,
or of the span seminorm of the bias vector in the Bellman equation. 
Recently, two model free-algorithms that require no prior knowledge have been proposed. For the case with finite but  unknown mixing times, Chen et al. combine Q-learning and approximation by discounted MDPs  with progressively larger discount factors. For weakly communicating MDPs, Kakade et al. use a primal-dual stochastic gradient descent combined with a regularization technique, whose complexity depends on the expected value of the gain $g_{\pi_T}$ of the policy $\pi_T$ generated by the algorithm. Although these algorithms do not require prior knowledge, these works do not provide estimates for the number of iterations, nor  a stopping rule that guarantees $\varepsilon$-optimality. 

\paragraph{Value Iterations.}
Value iterations (VIs)---an instantiation of the Banach-Picard fixed point iterations---were among the first methods considered in the dynamic programming literature Puterman, "Markov Decision Processes" and serve as a fundamental algorithm to compute the value functions for discounted MDPs as well as unichain average reward MDPs. The sample-based variants, such as TD-Learning Sutton et al., "Introduction to Modern Reinforcement Learning", Fitted Value Iteration Erven et al., "Efficient Exploration-Exploitation using Deterministic Planning" , and Deep Q-Network Mnih et al., "Human-level control through deep reinforcement learning"  are the workhorses of modern reinforcement learning algorithms. VIs are also routinely applied in diverse settings, including factored MDPs Ito et al., "Factored Markov Decision Processes with continuous state and action spaces" ____ , robust MDPs Iyengar, "Robust Control Systems: The Mathematical Theory" ____ , MDPs with reward machines Farahmand et al., "Reward Machines" ____ , and MDPs with options Sutton et al., "Between MDPs and semi-MDPs: Towards a general theory of decision-making in uncertain environments". In the generative model setup, variance reduction sampling was applied to approximate VIs for discounted rewards: Erven et al. use precomputed offsets to reduce variance of sampling, Xiao et al. applies SVRG-type variance reduction sampling ____ to Q-learning, and Dai et al. use SARAH-type variance reduction sampling ____ which we also exploit in this work.

\paragraph{Halpern iterations.}
For $\gamma$-contractions on Banach spaces,  the classical Banach-Picard iterates $x^{k+1}=T(x^{k})$ converge to the unique fixed point $x^*=T(x^*)$, with explicit bounds for the residuals $\|T(x^k)-x^k\|\leq \gamma^k\|T(x^0)-x^0\|$ 
and the distance 
$\|x^k-x^*\|\leq \frac{\gamma^k}{1-\gamma}\|T(x^0)-x^0\|$ to the fixed point. This fits well for discounted MDPs, and is also useful in the average reward setting for unichain MDPs. 

For nonexpansive maps with $\gamma=1$, as it is the  case for average reward MDPs, these estimates degenerate and provide no useful information. 
An alternative is provided by Halpern's iteration $x^{k+1}\!=\!(1-\beta_{k+1})x^0+\beta_{k+1} T(x^{k})$, where the next iterate is computed as a convex combination between $T(x^{k})$ and the initial point $x^0$ which acts as an {\em anchor} point along the iterations Dai et al.. The sequence
$\beta_k\in (0,1)$ is chosen exogenously and increasing to 1, so that the strength of the anchor mechanism diminishes as the iteration progresses. 

Halpern's anchored iteration has been widely studied in minimax optimization and fixed-point problems Goodman et al.. In the context of reinforcement learning, Dai et al. applied the anchoring technique to VIs achieving an accelerated convergence rate for cumulative-reward MDPs and the first non-asymptotic rate for average reward multichain MDPs. Abdolmaleki et al. applied the anchoring mechanism to Q-learning for average reward MDPs with a generative model and studied its sample complexity. 


Assuming that the set of fixed points $\Fix(T)$ is nonempty, and under suitable conditions on $\beta_k$, Halpern's iterates have been proved to converge towards a fixed point in the case of Hilbert spaces Goodman et al. as well as in uniformly smooth Banach spaces Goodman et al.. 
For more general normed spaces, the analysis in Dai et al. implies that for $\beta_k=k/(k+2)$ one has the 
explicit error bound $\|T(x^k)-x^k\|\leq \frac{4}{k+1}\|x^0-x^*\|$.
The proportionality constant in this bound was recently improved in the Hilbert setting by Dai et al.. For a comprehensive analysis, including the determination of the optimal Halpern iteration, see Goodman et al..