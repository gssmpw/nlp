\section{Related Work\label{sec:section2}
}

\subsection{EEG-Based Emotion Recognition}
Electroencephalogram (EEG)-based emotion recognition utilizes brain activity signals to classify emotional states, offering an intrinsic and unconscious reflection of emotional responses. Deep learning models such as convolutional neural networks (CNNs) ____ and recurrent neural networks (RNNs) ____ have been extensively applied for feature extraction and classification. More recently, transformer-based approaches ____ have demonstrated strong potential in modeling temporal dynamics and long-range dependencies in EEG signals. Advanced frameworks such as graph neural networks (GNNs)____ have also been proposed to capture the spatial interrelations among EEG channels effectively.

However, EEG recording is associated with several challenges. Key obstacles include the presence of internal and external artifacts, such as eye movements and muscle activity, which can interfere with signal accuracy. Furthermore, recording EEG data for extended periods can introduce noise and variability, which complicates the extraction of consistent emotional features____. These factors limit the effectiveness of EEG as a standalone modality for emotion recognition, highlighting the need for complementary approaches.

\subsection{Facial Emotion Recognition}
Facial emotion recognition (FER) involves analyzing human facial expressions from images or videos to classify emotions. Deep learning methods, particularly convolutional neural networks (CNNs), have significantly advanced this field by automating facial feature extraction. For static images, 2D CNNs effectively extract spatial features, achieving high accuracy in emotion classification ____. For video data, methods such as Conv3D ____ and ConvLSTM ____ are employed to capture both spatial and temporal information, improving recognition performance. Recently, transformer architectures ____ have gained attention for their ability to encode context-aware features through attention mechanisms, offering superior results in dynamic emotion analysis.

These advancements have made FER a cornerstone of emotion recognition research. However, relying solely on facial images or videos has inherent limitations, particularly in real-world scenarios where emotions are often subtle, neutral, or influenced by contextual factors that are not captured visually ____. The absence of complementary data, such as physiological signals, restricts FER methods from fully understanding the underlying emotional states. These limitations highlight the necessity of exploring multimodal approaches, which integrate diverse data sources to achieve more accurate and robust emotion recognition.

\subsection{Multimodal Emotion Recognition}
Multimodal emotion recognition, which integrates facial expressions and EEG signals, has gained significant attention due to its ability to provide more accurate emotion classification by combining complementary information. However, research in this area still lacks sufficient exploration of effective fusion techniques, which limits the potential of multimodal models.

Several approaches have been proposed to fuse EEG and facial expression data. Early methods often relied on simple fusion strategies such as decision trees or voting mechanisms to combine the outputs of each modality ____. While these approaches are straightforward, they fail to effectively utilize the rich, modality-specific features extracted from each source, often resulting in suboptimal performance. Methods like Deep Canonical Correlation Analysis (DeepCCA), have attempted to improve feature correlation between modalities, yet they still fall short of fully exploiting the complex, individual features of each modality ____. Low-rank fusion techniques have also been explored, offering a more efficient way to combine modality-specific features ____. While these methods are computationally efficient, they are not advanced enough to capture the complex interactions between the two modalities.

Concatenation-based methods, which combine the features of both modalities into a single vector, are also widely used but can lead to information loss ____. In contrast, more sophisticated methods, such as cross-attention mechanisms in transformers, allow the model to focus on the most relevant features from each modality, enhancing fusion accuracy and improving overall performance ____. Despite these advancements, the field still lacks comprehensive, effective fusion strategies that can fully harness the strengths of both EEG and facial expression data, highlighting the need for further research in this area.

\subsection{Multiple Instance Learning}
Multiple Instance Learning (MIL) is a variation of supervised learning in which a class label is assigned to a bag of instances rather than individual instances. Unlike traditional supervised learning, where each input (such as an image) is labeled with a specific category, MIL is applied in scenarios where the labeling is more ambiguous. In MIL, a "bag" contains multiple instances, and only the overall class of the bag is provided, not individual labels for each instance. This approach is particularly useful when data annotations are weak or incomplete, a common occurrence in real-world tasks.

Traditional MIL typically relies on pooling methods like max pooling or average pooling to combine the individual instances in a bag and make a prediction. However, these pooling techniques often fail to capture the importance of specific instances, as they treat all instances equally. In contrast, attention-based MIL (AMIL) ____introduces an attention mechanism that assigns different weights to instances, allowing the model to focus more on the most relevant instances while ignoring less informative ones. This approach significantly improves MIL performance by enabling more precise feature extraction and decision-making.

In the field of multimodal emotion recognition using EEG and facial expressions, the typical approach involves associating a short segment of EEG data (e.g., 3 seconds) with a single facial expression image, which is then used to represent the entire 3-second window. However, this approach is inherently flawed as it may overlook valuable temporal and subtle details present within the sequence of images. A 3-second video, for instance, should be understood as a "bag" of instances, with each individual frame or image representing a potential instance. Relying on just one image to represent the entire bag could result in the loss of critical information, especially in capturing emotional variations that occur over time.

By adopting MIL, we can leverage all instances within the video to create a more balanced and accurately represent the entire segment. MIL allows for a more comprehensive understanding by treating the entire video as a bag of instances, thus enhancing the emotion recognition process. Despite the potential benefits, very few studies have recognized the value of MIL in this context ____. This research aims to incorporate MIL to better capture and represent the emotional content in the facial expression modality.

%% Methods