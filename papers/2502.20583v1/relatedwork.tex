\section{Related Work}
\label{sec:related-work}
\subsection{Efficient ASR Inference}
Several prior works have aimed to enhance ASR model efficiency. 
FasterWhisper uses optimized inference kernels \cite{faster-whisper}, while WhisperX further improves it for long-form audio \cite{bain2023whisperx}. 
Whisper.cpp is a C/C++ implementation for portability on both the CPU and GPU \cite{whisper.cpp}. 
Whisper\_streaming supports live transcription for streaming purposes \cite{machavcek2023turning}. 
NVIDIA's NeMo is a modular toolkit for deploying speech models \cite{NeMo}. 
However, they do not effectively reduce ASR encoder computational demands. 
Some works provide model weight quantization, but they are limited to weights (weight-only quantization) and do not accelerate the compute-bound encoder inference.
Our approach can be integrated with these frameworks. 

Various studies, including \whisperT, \distillwhisper, and \kotobawhisper use distillation techniques to shrink decoder size \cite{radford2023robust,gandhi2023distil,kotoba-whisper-v2-0}. 
Other approaches combine distillation with quantization or lightweight modular ASR fine-tuning for underrepresented languages \cite{shao2023whisper,ferraz2023distilwhisper}.
Our work complements these efforts by further reducing the encoder's computational requirements.

\subsection{Model Compression with Low-Rank Approximation}
The low-rank approximation has been used to compress machine learning models, such as for parameter-efficient fine-tuning \cite{hu2021lora} or the LLM's KV cache compression \cite{liu2024deepseek,chang2024palu}.
\citet{yu2023compressing} has suggested that activations in Transformer models exhibit low-rank and compressed models, mainly targeting vision models.
However, their method is limited to linear layers, leaving self-attention layers unoptimized, and its applicability to speech models has not been studied.