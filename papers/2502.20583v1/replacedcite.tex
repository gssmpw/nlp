\section{Related Work}
\label{sec:related-work}
\subsection{Efficient ASR Inference}
Several prior works have aimed to enhance ASR model efficiency. 
FasterWhisper uses optimized inference kernels ____, while WhisperX further improves it for long-form audio ____. 
Whisper.cpp is a C/C++ implementation for portability on both the CPU and GPU ____. 
Whisper\_streaming supports live transcription for streaming purposes ____. 
NVIDIA's NeMo is a modular toolkit for deploying speech models ____. 
However, they do not effectively reduce ASR encoder computational demands. 
Some works provide model weight quantization, but they are limited to weights (weight-only quantization) and do not accelerate the compute-bound encoder inference.
Our approach can be integrated with these frameworks. 

Various studies, including \whisperT, \distillwhisper, and \kotobawhisper use distillation techniques to shrink decoder size ____. 
Other approaches combine distillation with quantization or lightweight modular ASR fine-tuning for underrepresented languages ____.
Our work complements these efforts by further reducing the encoder's computational requirements.

\subsection{Model Compression with Low-Rank Approximation}
The low-rank approximation has been used to compress machine learning models, such as for parameter-efficient fine-tuning ____ or the LLM's KV cache compression ____.
____ has suggested that activations in Transformer models exhibit low-rank and compressed models, mainly targeting vision models.
However, their method is limited to linear layers, leaving self-attention layers unoptimized, and its applicability to speech models has not been studied.