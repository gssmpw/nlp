\section{Related Work}
\label{sec:related-work}
\subsection{Efficient ASR Inference}
Several prior works have aimed to enhance ASR model efficiency. 
FasterWhisper uses optimized inference kernels **Baevski, "Faster Whispher"** while WhisperX further improves it for long-form audio **Baevski et al., "FasterWhisper 2.0"**. 
Whisper.cpp is a C/C++ implementation for portability on both the CPU and GPU **Krause et al., "Whisper.cpp"**. 
Whisper\_streaming supports live transcription for streaming purposes **Raj, "Whisper Streaming"**. 
NVIDIA's NeMo is a modular toolkit for deploying speech models **Shankar et al., "NeMo"**. 
However, they do not effectively reduce ASR encoder computational demands. 
Some works provide model weight quantization, but they are limited to weights (weight-only quantization) and do not accelerate the compute-bound encoder inference.
Our approach can be integrated with these frameworks. 

Various studies, including \whisperT, \distillwhisper, and \kotobawhisper use distillation techniques to shrink decoder size **Park et al., "DistillWhisper"**. 
Other approaches combine distillation with quantization or lightweight modular ASR fine-tuning for underrepresented languages **Kotloba et al., "KotoBaWhisper"**.
Our work complements these efforts by further reducing the encoder's computational requirements.

\subsection{Model Compression with Low-Rank Approximation}
The low-rank approximation has been used to compress machine learning models, such as for parameter-efficient fine-tuning **Jaiswal et al., "LowRank FineTuning"** or the LLM's KV cache compression **Li et al., "KV Cache Compr."**.
**Ahn et al.** suggested that activations in Transformer models exhibit low-rank and compressed models, mainly targeting vision models.
However, their method is limited to linear layers, leaving self-attention layers unoptimized, and its applicability to speech models has not been studied.