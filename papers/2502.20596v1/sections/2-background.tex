\subsection{Problem Formulation}
\label{sec:bg:problem}

In Few-Shot Continual Relation Extraction (FCRE), a model must continuously assimilate new knowledge from a sequential series of tasks. For each $t$-th task, the model undergoes training on the dataset $D^t = \{ (x^t_i, y^t_i)\}_{i=1}^{N \times K}$. Here, $N$ represents the number of relations in the task $R^t$, and $K$ denotes the limited number of samples per relation, reflecting the few-shot learning scenario. Each sample $(x, y)$ includes a sentence $x$ containing a pair of entities $(e_h, e_t)$ and a relation label $y \in R$. This type of task setup is referred to as \textit{"N-way-K-shot"} \cite{chen-etal-2023-consistent}. Upon completion of task $t$, the dataset $D^t$ should not be extensively included in subsequent learning, as continual learning aims to avoid retraining on all prior data. Ultimately, the model's performance is assessed on a test set which encompasses all encountered relations $\tilde{R}^T = \bigcup_{t=1}^T R^t$.

For clarity, each task in FCRE can be viewed as a conventional relation extraction problem, with the key challenge being the scarcity of samples available for learning. The primary goal of FCRE is to develop a model that can consistently acquire new knowledge from limited data while retaining competence in previously learned tasks. In the following subsections, we will explore the key aspects of FCRE models as addressed by state-of-the-art studies.


\subsection{Encoding Latent Representation}
\label{sec:bg:rep_encode}

% The foundational deep-learning framework for Relation Extraction \cite{ji-etal-2020-span, wang-lu-2020-two}, and for NLP tasks in general, typically involves encoding input data using a pre-trained language model (PLM) $\mathcal{M}$ such as BERT \cite{devlin-etal-2019-bert}.
% For conciseness and consistency with our experiments, we will refer to this backbone PLM as BERT from here on.

% A key initial consideration in Relation Extraction is how to \emph{formalize the latent representation} of the input, as the output of a Transformer \cite{vaswani2017attention} is a matrix.

% Standard methods for deriving representations include using the output vector corresponding to the \texttt{[CLS]} token or applying mean pooling across all tokens spanning the two entity mentions. Another approach is to wrap the two entities with four special tokens during tokenization:
% ${[\texttt{E}_\texttt{h-s}]}$,
% ${[\texttt{E}_\texttt{h-e}]}$,
% ${[\texttt{E}_\texttt{t-s}]}$,
% ${[\texttt{E}_\texttt{t-e}]}$;
% Among the various methods for obtaining representations from the integration of these tokens,
% a popular approach among the state-of-the-art CRE methods is to 
% and concatenate the outputs at the starting special tokens ${[\texttt{E}_\texttt{h-s}]}$ and ${[\texttt{E}_\texttt{t-s}]}$ \cite{baldini-soares-etal-2019-matching, zhao-etal-2022-consistent, le2024continual}.

A key initial consideration in Relation Extraction is how to \emph{formalize the latent representation} of the input, as the output of a Transformer \cite{vaswani2017attention} is a matrix. In this work, we adopt a method recently introduced by \citet{ma-etal-2024-making}. Given an input sentence $x$, which includes a head entity $e_h$ and a tail entity $e_t$, we reformulate it into a Cloze-style phrase $T(x)$ by incorporating a \texttt{[MASK]} token, which represents the relation between the entities. Specifically, the template is structured as follows:
\begin{align}
\begin{aligned}
  T({x}) = \; &x \left[v_{0:n_0-1}\right] e_h \left[v_{n_0:n_1-1}\right] [\texttt{MASK}] \\
  &\left[v_{n_1:n_2-1}\right] e_t \left[v_{n_2:n_3-1}\right].
\label{eq:template}
\end{aligned}
\end{align}
 Each $[v_i]$ denotes a learnable continuous token, and $n_j$ determines the number of tokens in each phrase. In our specific implementation, we use BERT's \texttt{[UNUSED]} tokens as $[v]$. The soft prompt phrase length is set to $3$ tokens, meaning $n_0$, $n_1$, $n_2$ and $n_3$ correspond to the values of $3$, $6$, $9$, and $12$, respectively. We then forward the templated sentence $T({x})$ through BERT to encode it into a sequence of continuous vectors, from which we obtain the hidden representation $\bm{z}$ of the input, corresponding to the position of the \texttt{[MASK]} token:
\begin{equation}
    \bm{z} = [\mathcal{M} \circ T] ({x})[\rm{position}(\texttt{[MASK]})],
\end{equation}
where $\mathcal{M}$ denotes the backbone pre-trained language model. This latent representation is then passed through an MLP for prediction, enabling the model to learn which relation that best fills the \texttt{[MASK]} token. 

%This approach offers two primary advantages: first, it aligns with BERT’s pretraining objective of Masked Language Modeling (MLM); second, the use of soft prompting guides the model effectively without the need for manual prompt engineering using explicit human language; and third, it captures the inherent semantic meaning of the relation, rather than treating it as merely a categorical label.

% \begin{figure}[t]
% 	\centering
% 	\includegraphics[width=.8\linewidth]{imgs/prompt-new-4.pdf}
% 	\caption{Prompt to generate label descriptions.}
% 	\label{fig:prompt}
% \end{figure}

\subsection{Learning Latent Representation}
\label{sec:bg:rep_learning}

In conventional Relation Extraction scenarios, a basic framework typically employs a backbone PLM followed by an MLP classifier to directly map the input space to the label space using Cross Entropy Loss. However, this approach proves inadequate in data-scarce settings \cite{snell2017prototypical}. Consequently, training paradigms which directly target the latent space, such as contrastive learning, emerge as more suitable approaches. To enhance the semantic richness of the information extracted from the training samples, two popular losses are often utilized: \emph{Supervised Contrastive Loss} and \emph{Hard Soft Margin Triplet Loss}.

\paragraph{Supervised Contrastive Loss.}
To enhance the model's discriminative capability, we employ the Supervised Contrastive Loss (SCL) \cite{khosla2020supcon}. This loss function is designed to bring positive pairs of samples, which share the same class label, closer together in the latent space. Simultaneously, it pushes negative pairs, belonging to different classes, further apart. Let $\bm{z}_x$ represent the hidden vector output of sample ${x}$, the positive pairs $(\bm{z}_x, \bm{z}_p)$ are those who share a class, while the negative pairs $(\bm{z}_x, \bm{z}_n)$ correspond to different labels. The SCL is computed as follows:
\begin{gather}
\mathcal{L}_{\textrm{SC}}(x) = - \sum_{{p} \in P({x})} \log \frac{
    f(\bm{z}_x, \bm{z}_p)
}
{\sum_{{u} \in \mathcal{D}\setminus \{x\}} f(\bm{z}_x, \bm{z}_u)} 
% Z_\textrm{SC}({x}) = \sum_{{p} \in P({x})} f(\bm{z}_x, \bm{z}_p) \  + \sum_{{n} \in N({x})} f(\bm{z}_x, \bm{z}_n), 
\end{gather}
where $f(\mathbf{x}, \mathbf{y}) \coloneqq \exp\left(\frac{\gamma(\mathbf{x}, \mathbf{y})}{\tau}\right)$, $\gamma(\cdot, \cdot)$ denotes the cosine similarity function, and $\tau$ is the temperature scaling hyperparameter. $P({x})$ and $\mathcal{D}$ denote the sets of positive samples with respect to sample ${x}$ and the training set, respectively.

\paragraph{Hard Soft Margin Triplet Loss.}
To achieve a balance between flexibility and discrimination, the Hard Soft Margin Triplet Loss (HSMT) integrates both hard and soft margin triplet loss concepts \cite{hermans2017defense}. This loss function is designed to maximize the separation between the most challenging positive and negative samples, while preserving a soft margin for improved flexibility. Formally, the loss is defined as:
\begin{multline}
\mathcal{L}_{\textrm{ST}}({x}) = \\
- \log \bigg(1 \ + \max_{{p} \in P(\bm{x})} e^{\xi(\bm{z}_x, \bm{z}_p)}
- \min_{{n} \in N({x})} e^{ \xi(\bm{z}_x, \bm{z}_n)}
\bigg),
\end{multline}
where $\xi(\cdot, \cdot)$ denotes the Euclidean distance function. The objective of this loss is to ensure that the hardest positive sample is as distant as possible from the hardest negative sample, thereby enforcing a flexible yet effective margin.

During training, these two losses is aggregated and referred to as the \emph{Sample-based learning loss}:
\begin{equation}
\mathcal{L}_\text{Samp} = \beta_\text{SC} \cdot \mathcal{L}_\text{SC} + \beta_\text{ST} \cdot \mathcal{L}_\text{ST}
\end{equation}
% \subsection{Contrastive Prompt Learning}
% \label{sec:bg:cpl}

% \citet{ma-etal-2024-making} proposed using Contrastive Prompt Learning (CPL) \cite{suresh-ong-2021-negatives} to address the challenges of catastrophic forgetting and overfitting in Few-Shot Continual Relation Extraction (FCRE). CPL leverages the inherent capabilities of Pre-trained Language Models (PLMs) by combining prompt learning \cite{lester2021power} with memory augmentation and contrastive learning. This section focuses on the prompt representation and memory augmentation components of the CPL framework.

% \paragraph{Prompt Representation.} The prompt representation module in CPL is designed to convert relation extraction tasks into a format more suitable for PLMs. This is achieved through the use of semi-automated continuous templates that integrate both entity information and learnable tokens. The template reformulates the input sentence $\bm{x}$, which contains the entities $e_h$ (head entity) and $e_t$ (tail entity), into a cloze-style phrase that includes a special \texttt{[MASK]} token representing the relation between the entities. This approach allows PLMs to generalize better across different relation categories, thus mitigating catastrophic forgetting.


% \subsection{Prompt Representation and Memory Augmentation}
% \label{sec:bg:pmr}

% The framework proposed by \cite{ma-etal-2024-making}, known as Contrastive Prompt Learning (CPL), introduces two key components — prompt representation and memory augmentation that are particularly effective in addressing the challenges of catastrophic forgetting and overfitting in Few-Shot Continual Relation Extraction (FCRE).



% The encoding process maps this templated sentence into continuous vectors using the encoding model $E$:
% \begin{align}
% \begin{aligned}
% E_{mb} \left(T\left(\bm{x}\right)\right) &= e\left( \bm{x} \right), h_0, \ldots, h_{n_0-1}, e\left( \bm{e_h} \right), \\
% &h_{n_0}, \ldots, h_{n_1-1}, e\left(\left[\texttt{MASK}\right]\right), \\
% &h_{n_1}, \ldots, h_{n_2-1}, e\left( \bm{e_t} \right), h_{n_2}, \ldots, h_{n_3-1}.
% \end{aligned}
% \end{align}
% The embeddings are processed by the encoding model to obtain the hidden representation $\bm{m}$:
% \begin{align}
% \bm{m} = E_{nc} \left( E_{mb} \left( T\left( \bm{x} \right) \right) \right).
% \end{align}

% \paragraph{Memory Augmentation.}
% To further alleviate overfitting in low-resource scenarios, CPL introduces a memory augmentation strategy. This strategy involves selecting representative samples from the current task and using well-crafted prompts to guide ChatGPT in generating diverse new samples. These generated samples are then combined with the original memory samples to create a more robust training set for subsequent tasks.

% \paragraph{Representative Memory Sampling.}
% After training on the current task $\mathcal{T}^k$, typical samples for each relation are selected using the K-means algorithm. These samples are stored in the memory $\hat{\mathcal{M}}$.

% \paragraph{Prompt-Based Data Augmentation.}
% To generate diverse samples, the CPL framework uses elaborate prompts to instruct ChatGPT \cite{openai2023chat35}. For each relation $r$ in the memory $\hat{R}^k$, a typical sample is selected and used to construct a prompt input that includes task instructions, semantic relation explanations, and demonstrations. ChatGPT then generates multiple samples based on this input, which are parsed and added to the training set for memory replay.
% Using such prompts, ChatGPT generates $g$ diverse samples with the specified relation. The generated samples are parsed into structured data and combined with the memory samples to form a new training set, which is then used for memory replay.

% By integrating prompt representation and memory augmentation, the CPL framework enhances the model's ability to retain previously learned knowledge while effectively learning new relations, thereby addressing the challenges of catastrophic forgetting and overfitting in Few-Shot Continual Relation Extraction.