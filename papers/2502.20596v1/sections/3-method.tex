% \subsection{Input representation}
% \label{sec:method:input}


\subsection{Label Descriptions}
\label{sec:method:descriptions}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{imgs/prompt.pdf}
    \caption{Prompt to generate relation descriptions with LLMs.}
    \label{fig:prompt}
\end{figure}

A core component of our method is achieving robust class latent representations, making class encoding crucial. To this end, having detailed definitions for each label, alongisde the hidden information extracted from the samples, is essential for our approach. In fact, the datasets used for benchmarking already provide each relation with a concise description, which we refer to as the \emph{Raw description}. While leveraging these descriptions has shown promise in previous work \cite{luo2024synergistic}, this approach remains limited due to its reliance on a one-to-one mapping between input embeddings and a single label description representation per task. This singular approach fails to offer rich, diverse, and robust information about the labels, leading to potential noise, instability, and suboptimal model performance.

To address these limitations, we employ Gemini 1.5 \cite{team2023gemini, reid2024gemini} to generate $K$ diverse, detailed, and illustrative descriptions for each relation. In particular, for each label, the respective raw description will be fed into the \emph{LLM prompt}, serving as an expert-in-the-loop to guide the model. Our prompt template is depicted in Figure \ref{fig:prompt}.


% \subsection{Representation Learning}
% \label{sec:bg:replearning}

% In conventional Relation Extraction scenarios, a basic framework employing BERT, followed by a classification MLP, is often used to directly learn the mapping from the input space to the label space using Cross Entropy Loss. However, this approach proves inadequate in data-scarce settings \cite{snell2017prototypical}. Consequently, contrastive learning emerges as a more suitable approach, as it can directly influence the latent space and better capture the inherent distinctions between samples from different classes. To enrich the semantic information drawn from both the samples and the generated label descriptions, we design an objective function comprising four key loss components.


% % main components: $\mathcal{L}_{RE}$ to force relation embeddings..., and $\mathcal{L}_{Retrieval}$ to facilitate...
% % $$ \mathcal{L} = \mathcal{L}_{RE} + \mathcal{L}_{Retrieval}$$ where $\mathcal{L}_{RE}$ force relation embeddings..., and $\mathcal{L}_{Retrieval}$ facilitate ... 

% % \subsubsection{Relation extraction loss $\mathcal{L}_{RE}$}

% \paragraph{Supervised Contrastive Loss.}
% To enhance the model's discriminative capability, we employ the Supervised Contrastive Loss (SCL). This loss function is specifically designed to bring positive pairs of samples, which share the same class label, closer together in the latent space. Simultaneously, it pushes negative pairs, belonging to different classes, further apart. Let $\bm{z}_x$ represent the hidden vector output of sample ${x}$, the positive pairs $(\bm{z}_x, \bm{z}_p)$ are those who share a class, while the negative pairs $(\bm{z}_x, \bm{z}_n)$ correspond to different labels. The SCL is computed as follows:
% \begin{gather}
% \mathcal{L}_{\textrm{SC}}(x) = - \sum_{{p} \in P({x})} \log \frac{
%     f(\bm{z}_x, \bm{z}_p)
% }
% {Z_\textrm{SC}({x})}, \\
% Z_\textrm{SC}({x}) = \sum_{{p} \in P({x})} f(\bm{z}_x, \bm{z}_p) \  + \sum_{{n} \in N({x})} f(\bm{z}_x, \bm{z}_n), 
% \end{gather}
% where $f(\mathbf{x}, \mathbf{y}) \coloneqq \exp\left(\frac{\gamma(\mathbf{x}, \mathbf{y})}{\tau}\right)$, $\gamma(\cdot, \cdot)$ denotes the cosine similarity function, and $\tau$ is the temperature scaling hyperparameter. $P({x})$ and $N({x})$ denote the sets of positive and negative samples with respect to sample ${x}$, respectively.

% \paragraph{Hard Soft Margin Triplet Loss.}
% To achieve a balance between flexibility and discrimination, the Hard Soft Margin Triplet Loss (HSMT) integrates both hard and soft margin triplet loss concepts. This loss function is designed to maximize the separation between the most challenging positive and negative samples, while preserving a soft margin for improved flexibility. Formally, the loss is defined as:
% \begin{multline}
% \mathcal{L}_{\textrm{ST}}({x}) = - \log \bigg(1 \ +  
%     \max_{{p} \in P(\bm{x})} e^{\xi(\bm{z}_x, \bm{z}_p)} \\
%     - \min_{{n} \in N({x})} e^{ \xi(\bm{z}_x, \bm{z}_n)}
% \bigg),
% \end{multline}
% where $\xi(\cdot, \cdot)$ denotes the Euclidean distance function. The objective of this loss is to ensure that the hardest positive sample is as distant as possible from the hardest negative sample, thereby enforcing a flexible yet effective margin.


\subsection{Description-pivot Learning}
\label{sec:method:retrieval}

The single most valuable quality of class descriptions in our problem is that they are literal definitions of a relation, which makes them more accurate representations of that class than the underlying information from a set of samples. Thanks to this strength, they serve as stable knowledge anchors for the model to rehearse from, enabling effective reinforcement of old knowledge while assimilating new information. Unlike the variability of individual samples, a description remains consistent, providing a more reliable reference point for the model to rehearse from, effectively mitigating catastrophic forgetting.

To fully leverage this inherent advantage, we integrate these descriptions into the training process, framing the task as one of retrieving definition, which embodies real-world meaning, rather than a straightforward categorical classification. By doing so, we capitalize on the unchanging nature of descriptions, making them the focal point of our model's learning. Specifically, we incorporate two description-centric losses to enhance this retrieval-oriented approach:
\begin{equation}
    \mathcal{L}_\text{Des} = \beta_\text{HM}\cdot \mathcal{L}_\text{HM} + \beta_\text{MI}\cdot\mathcal{L}_\text{MI}.
\end{equation}
Here, $\mathcal{L}_\text{HM}$ and $\mathcal{L}_\text{MI}$ denote the Hard Margin Loss and the Mutual Information Loss, respectively. These losses are elaborated upon in the following paragraphs.

\paragraph{Hard Margin Loss.}
The Hard Margin Loss leverages label descriptions to refine the model's ability to distinguish between hard positive and hard negative pairs. Given the output hidden vectors $\{ \bm{d}^k_x \}_{k=1, ..., K}$ from BERT corresponding to the label description of sample ${x}$, and $\bm{z}_p$ and $\bm{z}_n$ representing the hidden vectors of positive and negative samples respectively, the loss function is formulated to maximize the alignment between $ \bm{d}^k_x $ and its corresponding positive sample, while enforcing a strict margin against negative samples. Specifically, the loss is formulated as follows:
\begin{align}
\mathcal{L}_{\textrm{HM}}(x) &= \sum_{k=1}^K \mathcal{L}_{\textrm{HM}}^k(x), \\
\mathcal{L}_{\textrm{HM}}^k(x) &= 
\sum_{p \in P_\textrm{H}(x)} (1 - \gamma(\bm{d}^k_x, \bm{z}_p))^2 \notag \\
&+ \sum_{n \in N_\textrm{H}(x)} max(0, m - 1 + \gamma(\bm{d}^k_x, \bm{z}_n) )^2,
\end{align}
where $m$ is a margin hyperparameter; $\gamma(\cdot, \cdot)$ denotes the cosine similarity function; $P_\textrm{H}(x)$ and $N_\textrm{H}(x)$ represent the sets of hard positive and hard negative samples, respectively. They are determined by comparing the similarity between $\bm{d}^k_x$ and both positive and negative pairs, specifically focusing on the most challenging pairs where the similarity to negative samples is close to or greater than that of positive samples, defined as follows:
\begin{align}
\begin{aligned}
    P_\textrm{H}(x) = \{&p \in P(x) | 1 - \gamma(\bm{d}^k_x, \bm{z}_p) \\
    &> min_{n \in N(x)}(1 - \gamma(\bm{d}^k_x, \bm{z}_n)), \forall k \in [K] \},
\end{aligned} \\
\begin{aligned}
    N_\textrm{H}(x) = \{&n \in N(x) | 1 - \gamma(\bm{d}^k_x, \bm{z}_n) \\
    &< max_{p \in P(x)}(1 - \gamma(\bm{d}^k_x, \bm{z}_p)), \forall k \in [K] \}.
\end{aligned}
\end{align}

% where  $P({x})$ and $N({x})$ denote the sets of positive and negative samples with respect to sample ${x}$, respectively.

By utilizing the label description vectors $\{ \bm{d}^k_x \}$, optimizing $\mathcal{L}_{\textrm{HM}}(x)$ effectively sharpens the model's decision boundary, reducing the risk of confusion between similar classes and improving overall performance in few-shot learning scenarios. The loss penalizes the model more heavily for misclassifications involving these hard samples, ensuring that the model pays particular attention to the most difficult cases, thereby enhancing its discriminative power.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{imgs/architecture-fix-v2.png}
    \caption{Our Framework.}
    \label{fig:framework}
\end{figure}



\paragraph{Mutual Information Loss.}
The Mutual Information (MI) Loss is designed to maximize the mutual information between the input sample's hidden representation $\bm{z_x}$ of $\bm{x}$ and its corresponding retrieved descriptions, promoting a more informative alignment between them. Let $\bm{d}_n$ be a hidden vector of other label descriptions than $\bm{x}$. According to \citet{DBLP:journals/corr/abs-1807-03748}, the Mutual Information $MI(x)$ between the input embedding $\bm{z}_x$ and its corresponding label description follows the following inequation:
\begin{equation}
    MI \geq \log B + \textnormal{InfoNCE} (\{ x_i\}_{i=1}^B; h),
\end{equation}
where we have defined:
\begin{multline}
\textnormal{InfoNCE} (\{ x_i\}_{i=1}^B; h) = \\
\frac{1}{B}\sum_{i=1}^B \log \frac{\sum_{k=1}^K h(\bm{z_i}, \bm{d}_i^k)}{\sum_{j=1}^B \sum_{k=1}^K h(\bm{z_j}, \bm{d}_j^k)},
\end{multline}
where $h(\bm{z}_j, \bm{d}_j^k) = \exp \left(\frac{\bm{z}_j^TW\bm{d}_j^k}{\tau}\right)$. Here, $\tau$ is the temperature, $B$ is mini-batch size and $W$ is a trainable parameter. Finally, the MI loss function in our implementation is: 
% \begin{equation}
%     \mathcal{L}_{MI} = -\sum_{(x_i , y_i) \in D_{train}^{k}}\textnormal{InfoNCE} (\{x_i\}_{i=1}^B; h)
% \end{equation}
\begin{multline}
\mathcal{L}_{\textrm{MI}}(x) = \\
- \log \frac{ \sum_{k=1}^K
    h(\bm{z}_x, \bm{d}_x^k)
}{
    \sum_{k=1}^K
    h(\bm{z}_x, \bm{d}_x^k)
    + \sum_{n \in N(x)} \sum_{k=1}^K h(\bm{z}_x, \bm{d}_n^k)
}
\end{multline}

This loss ensures that the representation of the input sample is strongly associated with its corresponding label, while reducing its association with incorrect labels, thereby enhancing the discriminative power of the model.

% \begin{table*}[!ht]
%     \centering
%     \setlength{\tabcolsep}{1mm}
%     % \resizebox{1.0\textwidth}{!}{
%     \begin{tabular}{lllllllll|c}
%         \multicolumn{3}{l}{\textbf{FewRel} \textit{(10-way--5-shot)}} \\
%         \hline
%         Method & $\mathcal{T}^1$ & $\mathcal{T}^2$ & $\mathcal{T}^3$ & $\mathcal{T}^4$ & $\mathcal{T}^5$ & $\mathcal{T}^6$ & $\mathcal{T}^7$ & $\mathcal{T}^8$ & $\Delta \downarrow$ \\ 
%         \hline \hline
%         RP-CRE      & $93.97_{\pm 0.64}$ & $76.05_{\pm 2.36}$ & $71.36_{\pm 2.83}$ & $69.32_{\pm 3.98}$ & $64.95_{\pm 3.09}$ & $61.99_{\pm 2.09}$ & $60.59_{\pm 1.87}$ & $59.57_{\pm 1.13}$ & 34.40 \\
%         CRL         & $94.68_{\pm 0.33}$ & $80.73_{\pm 2.91}$ & $73.82_{\pm 2.77}$ & $70.26_{\pm 3.18}$ & $66.62_{\pm 2.74}$ & $63.28_{\pm 2.49}$ & $60.96_{\pm 2.63}$ & $59.27_{\pm 1.32}$ & 35.41\\
%         CRECL       & $93.93_{\pm 0.22}$ & $82.55_{\pm 6.95}$ & $74.13_{\pm 3.59}$ & $69.33_{\pm  3.87}$ & $66.51_{\pm 4.05}$ & $64.60_{\pm 1.92}$ & $62.97_{\pm 1.46}$ & $59.99_{\pm 0.65}$ & 33.94\\
%         ERDA        & $92.43_{\pm 0.32}$ & $64.52_{\pm 2.11}$ & $50.31_{\pm 3.32}$ & $44.92_{\pm 3.77}$ & $39.75_{\pm 3.34}$ & $36.36_{\pm 3.12}$ & $34.34_{\pm 1.83}$ & $31.96_{\pm 1.91}$ & 60.47\\
%         SCKD        & $94.77_{\pm 0.35}$ & $82.83_{\pm 2.61}$ & $76.21_{\pm 1.61}$ & $72.19_{\pm 1.33}$ & $70.61_{\pm 2.24}$ & $67.15_{\pm 1.96}$ & $64.86_{\pm 1.35}$ & $62.98_{\pm 0.88}$ & 31.79\\ 
%         ConPL$^{**}$ & $\mathbf{95.18_{\pm 0.73}}$ & $79.63_{\pm 1.27}$ & $74.54_{\pm 1.13}$ & $71.27_{\pm 0.85}$ & $68.35_{\pm 0.86}$ & $63.86_{\pm 2.03}$ & $64.74_{\pm 1.39}$ & $62.46_{\pm 1.54}$ &32.72 \\
%         CPL & {94.87} & {85.14} & \underline{78.80} & \underline{75.10} & \underline{72.57} & \underline{69.57} & \underline{66.85} & \underline{64.50} & \underline{30.37}\\ 
%         CPL + MI  & $94.69_{\pm 0.7}$ & $85.58_{\pm 1.88}$ & $80.12_{\pm 2.45}$ & $75.71_{\pm 2.28}$ & $73.90_{\pm 1.8}$ & $70.72_{\pm 0.91}$ & $68.42_{\pm 1.77}$ & $66.27_{\pm 1.58}$ & 28.42\\
%         % DCRE (Our) & ${94.84_{\pm 0.26}}$ & $\mathbf{86.60_{\pm 2.63}}$ & \textbf{$\mathbf{81.45_{\pm 1.91}}$} & \textbf{$\mathbf{77.8_{\pm 2.83}}$} & $\mathbf{75.93_{\pm 2.95}}$ & $\mathbf{72.78_{\pm 2.20}}$ & $\mathbf{70.46_{\pm 1.69}}$ & $\mathbf{68.47_{\pm 0.85}}$ & \textbf{26.37} \\
%         % CPL + LLM2VEC & $97.25_{\pm 0.3}$ & $89.29_{\pm 2.51}$ & $85.56_{\pm 1.21}$ & $82.1_{\pm 2.02}$ & $79.96_{\pm 2.72}$ & $78.41_{\pm 3.22}$ & $76.42_{\pm 2.25}$ & $75.2_{\pm 2.33}$ & 22.05 \\
%         % DCRE + LLM2VEC (Our) & $96.83_{\pm 0.29}$ & $90.4_{\pm 3.4}$ & $87.63_{\pm 1.87}$ & $85.69_{\pm 1.94}$ & $84.77_{\pm 2.45}$ & $83.11_{\pm 1.48}$ & $81.24_{\pm 1.09}$ & $80.07_{\pm 0.51}$ & 16.76 \\
%         DCRE & \underline{$94.93_{\pm 0.39}$} & $\mathbf{85.14_{\pm 2.27}}$ & $\mathbf{79.06_{\pm 1.68}}$ & $\mathbf{75.92_{\pm 2.03}}$ & $\mathbf{74.10_{\pm 2.53}}$ & $\mathbf{71.83_{\pm 2.17}}$ & $\mathbf{69.84_{\pm 1.48}}$ & $\mathbf{68.24_{\pm 0.79}}$ & \textbf{26.69} \\
%         \hline
%         \\
         
%         \multicolumn{3}{l}{\textbf{TACRED} \textit{(5-way-5-shot)}} \\
%     \hline
%         Method & $\mathcal{T}^1$ & $\mathcal{T}^2$ & $\mathcal{T}^3$ & $\mathcal{T}^4$ & $\mathcal{T}^5$ & $\mathcal{T}^6$ & $\mathcal{T}^7$ & $\mathcal{T}^8$ & $\Delta \downarrow$ \\ \hline \hline
%         RP-CRE      & $87.32_{\pm 1.76}$ & $74.90_{\pm 6.13}$ & $67.88_{\pm 4.31}$ & $60.02_{\pm 5.37}$ & $53.26_{\pm 4.67}$ & $50.72_{\pm 7.62}$ & $46.21_{\pm 5.29}$ & $44.48_{\pm 3.74}$ & 42.84\\
%         CRL         & $88.32_{\pm 1.26}$ & $76.30_{\pm 7.48}$ & $69.76_{\pm 5.89}$ & $61.93_{\pm 2.55}$ & $54.68_{\pm 3.12}$ & $50.92_{\pm 4.45}$ & $47.00_{\pm 3.78}$ & $44.27_{\pm 2.51}$ & 44.05\\
%         CRECL       & $87.09_{\pm 2.50}$ & $78.09_{\pm 5.74}$ & $61.93_{\pm 4.89}$ & $55.60_{\pm 5.78}$ & $53.42_{\pm 2.99}$ & $51.91_{\pm 2.95}$ & $47.55_{\pm 3.38}$ & $45.53_{\pm 1.96}$ & 41.56\\
%         ERDA        & $81.88_{\pm 1.97 }$ & $53.68_{\pm 6.31}$ & $40.36_{\pm 3.35}$ & $36.17_{\pm 3.65}$ & $30.14_{\pm 3.96}$ & $22.61_{\pm 3.13}$ & $22.29_{\pm 1.32}$ & $19.42_{\pm 2.31}$ & 62.46\\
%         SCKD        & \underline{$88.42_{\pm 0.83}$} & $79.35_{\pm 4.13}$ & $70.61_{\pm 3.16}$ & $66.78_{\pm 4.29}$ & $60.47_{\pm 3.05}$ & $58.05_{\pm 3.84}$ & $54.41_{\pm 3.47}$ & $52.11_{\pm 3.15}$ & 36.31\\
%         ConPL$^{**}$ & {$\mathbf{88.77_{\pm 0.84}}$} & $69.64_{\pm 1.93}$ & $57.50_{\pm 2.48}$ & $52.15_{\pm 1.59}$ & $58.19_{\pm 2.31}$ & $55.01_{\pm 3.12}$ & $52.88_{\pm 3.66}$ & $50.97_{\pm 3.41}$ & 37.80\\ 
%         CPL & 86.27 & \underline{81.55} & \underline{73.52} & \underline{68.96} & \underline{63.96} & \underline{62.66} & \underline{59.96} & \underline{57.39} & \underline{28.88} \\
%         CPL + MI   & $85.67_{\pm 0.8}$ & $82.54_{\pm 2.98}$ & $75.12_{\pm 3.67}$ & $70.65_{\pm 2.75}$ & $66.79_{\pm 2.18}$ & $65.17_{\pm 2.48}$ & $61.25_{\pm 1.52}$ & $59.48_{\pm 3.53}$ & 26.19 \\ 
%         % DCRE (Our) & $87.25_{\pm 0.45}$ & $\mathbf{85.92_{\pm 5.89}}$ & $\mathbf{80.81_{\pm 3.34}}$ & $\mathbf{78.85_{\pm 2.77}}$ & $\mathbf{74.95_{\pm 3.44}}$ & $\mathbf{73.83_{\pm 2.99}}$ & $\mathbf{71.79_{\pm 2.59}}$ & $\mathbf{70.02_{\pm 2.17}}$ & \textbf{17.23} \\
%         DCRE & $86.20_{\pm 1.35}$ & {$\mathbf{83.18_{\pm 8.04}}$} & {$\mathbf{80.65_{\pm 3.06}}$} & {$\mathbf{75.05_{\pm 3.07}}$} & {$\mathbf{68.83_{\pm 5.05}}$} & {$\mathbf{68.30_{\pm 4.28}}$} & {$\mathbf{65.30_{\pm 2.74}}$} & {$\mathbf{63.21_{\pm 2.39}}$} & \textbf{22.99} \\

%         % DCRE (Our) & $86.46_{\pm 0.79}$ & {$\mathbf{84.19_{\pm 4.19}}$} & {$\mathbf{79.2_{\pm 4.00}}$} & {$\mathbf{75.56_{\pm 2.43}}$} & {$\mathbf{72.40_{\pm 5.02}}$} & {$\mathbf{70.10_{\pm 4.44}}$} & {$\mathbf{67.17_{\pm 3.10}}$} & {$\mathbf{64.92_{\pm 2.34}}$} & \textbf{21.54} \\
%         % CPL + LLM2VEC & $88.74_{\pm 0.44}$ & $85.16_{\pm 5.38}$ & $78.35_{\pm 4.46}$ & $77.5_{\pm 4.04}$ & $76.0_{\pm 5.04}$ & $76.3_{\pm 4.41}$ & $74.51_{\pm 5.06}$ & $73.83_{\pm 4.91}$ & 14.91 \\
%         % DCRE + LLM2VEC (Our) & $89.38_{\pm 0.57}$ & $87.26_{\pm 4.49}$ & $83.25_{\pm 3.23}$ & $83.1_{\pm 1.99}$ & $83.22_{\pm 2.18}$ & $81.88_{\pm 1.45}$ & $81.71_{\pm 1.05}$ & $81.12_{\pm 1.13}$ & 8.26 \\
        

%     \hline      
%     \end{tabular}
%     \caption{Accuracy (\%) of methods using BERT-based backbone after training for each task. The best results are in \textbf{bold}. **Results of ConPL are reproduced (see Supplementary material for details)}
%     \label{table:main}
% \end{table*}
\begin{table*}[!ht]
    \centering
    \setlength{\tabcolsep}{1mm}
    % \resizebox{1.0\textwidth}{!}{
    \begin{tabular}{lllllllll|c}
        \multicolumn{3}{l}{\textbf{FewRel} \textit{(10-way--5-shot)}} \\
        \hline
        Method & $\mathcal{T}^1$ & $\mathcal{T}^2$ & $\mathcal{T}^3$ & $\mathcal{T}^4$ & $\mathcal{T}^5$ & $\mathcal{T}^6$ & $\mathcal{T}^7$ & $\mathcal{T}^8$ & $\Delta \downarrow$ \\ 
        \hline \hline
        RP-CRE      & $93.97_{\pm 0.64}$ & $76.05_{\pm 2.36}$ & $71.36_{\pm 2.83}$ & $69.32_{\pm 3.98}$ & $64.95_{\pm 3.09}$ & $61.99_{\pm 2.09}$ & $60.59_{\pm 1.87}$ & $59.57_{\pm 1.13}$ & 34.40 \\
        CRL         & $94.68_{\pm 0.33}$ & $80.73_{\pm 2.91}$ & $73.82_{\pm 2.77}$ & $70.26_{\pm 3.18}$ & $66.62_{\pm 2.74}$ & $63.28_{\pm 2.49}$ & $60.96_{\pm 2.63}$ & $59.27_{\pm 1.32}$ & 35.41\\
        CRECL       & $93.93_{\pm 0.22}$ & $82.55_{\pm 6.95}$ & $74.13_{\pm 3.59}$ & $69.33_{\pm  3.87}$ & $66.51_{\pm 4.05}$ & $64.60_{\pm 1.92}$ & $62.97_{\pm 1.46}$ & $59.99_{\pm 0.65}$ & 33.94\\
        ERDA        & $92.43_{\pm 0.32}$ & $64.52_{\pm 2.11}$ & $50.31_{\pm 3.32}$ & $44.92_{\pm 3.77}$ & $39.75_{\pm 3.34}$ & $36.36_{\pm 3.12}$ & $34.34_{\pm 1.83}$ & $31.96_{\pm 1.91}$ & 60.47\\
        SCKD        & $94.77_{\pm 0.35}$ & $82.83_{\pm 2.61}$ & $76.21_{\pm 1.61}$ & $72.19_{\pm 1.33}$ & $70.61_{\pm 2.24}$ & $67.15_{\pm 1.96}$ & $64.86_{\pm 1.35}$ & $62.98_{\pm 0.88}$ & 31.79\\ 
        ConPL$^{**}$ & $\mathbf{95.18_{\pm 0.73}}$ & $79.63_{\pm 1.27}$ & $74.54_{\pm 1.13}$ & $71.27_{\pm 0.85}$ & $68.35_{\pm 0.86}$ & $63.86_{\pm 2.03}$ & $64.74_{\pm 1.39}$ & $62.46_{\pm 1.54}$ &32.72 \\
        CPL & {94.87} & {85.14} & {78.80} & {75.10} & {72.57} & {69.57} & {66.85} & {64.50} & {30.37}\\ 
        CPL + MI  & $94.69_{\pm 0.7}$ & $\mathbf{85.58_{\pm 1.88}}$ & $\mathbf{80.12_{\pm 2.45}}$ & \underline{${75.71_{\pm 2.28}}$} & \underline{${73.90_{\pm 1.8}}$} & \underline{${70.72_{\pm 0.91}}$} & \underline{${68.42_{\pm 1.77}}$} & \underline{${66.27_{\pm 1.58}}$} & 28.42\\
        DCRE & \underline{$94.93_{\pm 0.39}$} & \underline{$85.14_{\pm 2.27}$} & \underline{${79.06_{\pm 1.68}}$} & $\mathbf{75.92_{\pm 2.03}}$ & $\mathbf{74.10_{\pm 2.53}}$ & $\mathbf{71.83_{\pm 2.17}}$ & $\mathbf{69.84_{\pm 1.48}}$ & $\mathbf{68.24_{\pm 0.79}}$ & \textbf{26.69} \\
        \hline
        \\
         
        \multicolumn{3}{l}{\textbf{TACRED} \textit{(5-way-5-shot)}} \\
    \hline
        Method & $\mathcal{T}^1$ & $\mathcal{T}^2$ & $\mathcal{T}^3$ & $\mathcal{T}^4$ & $\mathcal{T}^5$ & $\mathcal{T}^6$ & $\mathcal{T}^7$ & $\mathcal{T}^8$ & $\Delta \downarrow$ \\ \hline \hline
        RP-CRE      & $87.32_{\pm 1.76}$ & $74.90_{\pm 6.13}$ & $67.88_{\pm 4.31}$ & $60.02_{\pm 5.37}$ & $53.26_{\pm 4.67}$ & $50.72_{\pm 7.62}$ & $46.21_{\pm 5.29}$ & $44.48_{\pm 3.74}$ & 42.84\\
        CRL         & $88.32_{\pm 1.26}$ & $76.30_{\pm 7.48}$ & $69.76_{\pm 5.89}$ & $61.93_{\pm 2.55}$ & $54.68_{\pm 3.12}$ & $50.92_{\pm 4.45}$ & $47.00_{\pm 3.78}$ & $44.27_{\pm 2.51}$ & 44.05\\
        CRECL       & $87.09_{\pm 2.50}$ & $78.09_{\pm 5.74}$ & $61.93_{\pm 4.89}$ & $55.60_{\pm 5.78}$ & $53.42_{\pm 2.99}$ & $51.91_{\pm 2.95}$ & $47.55_{\pm 3.38}$ & $45.53_{\pm 1.96}$ & 41.56\\
        ERDA        & $81.88_{\pm 1.97 }$ & $53.68_{\pm 6.31}$ & $40.36_{\pm 3.35}$ & $36.17_{\pm 3.65}$ & $30.14_{\pm 3.96}$ & $22.61_{\pm 3.13}$ & $22.29_{\pm 1.32}$ & $19.42_{\pm 2.31}$ & 62.46\\
        SCKD        & \underline{$88.42_{\pm 0.83}$} & $79.35_{\pm 4.13}$ & $70.61_{\pm 3.16}$ & $66.78_{\pm 4.29}$ & $60.47_{\pm 3.05}$ & $58.05_{\pm 3.84}$ & $54.41_{\pm 3.47}$ & $52.11_{\pm 3.15}$ & 36.31\\
        ConPL$^{**}$ & {$\mathbf{88.77_{\pm 0.84}}$} & $69.64_{\pm 1.93}$ & $57.50_{\pm 2.48}$ & $52.15_{\pm 1.59}$ & $58.19_{\pm 2.31}$ & $55.01_{\pm 3.12}$ & $52.88_{\pm 3.66}$ & $50.97_{\pm 3.41}$ & 37.80\\ 
        CPL & 86.27 & \underline{81.55} & {73.52} & \underline{68.96} & {63.96} & {62.66} & {59.96} & {57.39} & {28.88} \\
        CPL + MI   & $85.67_{\pm 0.8}$ & \underline{$82.54_{\pm 2.98}$}& \underline{$75.12_{\pm 3.67}$} & \underline{$70.65_{\pm 2.75}$} & \underline{$66.79_{\pm 2.18}$} & \underline{$65.17_{\pm 2.48}$} & \underline{$61.25_{\pm 1.52}$} & \underline{$59.48_{\pm 3.53}$} & 26.19 \\ 
        DCRE & $86.20_{\pm 1.35}$ & {$\mathbf{83.18_{\pm 8.04}}$} & {$\mathbf{80.65_{\pm 3.06}}$} & {$\mathbf{75.05_{\pm 3.07}}$} & {$\mathbf{68.83_{\pm 5.05}}$} & {$\mathbf{68.30_{\pm 4.28}}$} & {$\mathbf{65.30_{\pm 2.74}}$} & {$\mathbf{63.21_{\pm 2.39}}$} & \textbf{22.99} \\

    \hline      
    \end{tabular}
    \caption{Accuracy (\%) of methods using BERT-based backbone after training for each task. The best results are in \textbf{bold}. **Results of ConPL are reproduced}
    \label{table:main}
\end{table*}
\paragraph{Joint Training Objective Function.}
Our model is trained using a combination of the \emph{Sample-based learning loss} mentioned in Section \ref{sec:bg:rep_learning} and our description-pivot loss $\mathcal{L}_\text{Des}$, weighted by their respective coefficients:
\begin{align}
\mathcal{L}(x) &= \mathcal{L}_\text{Samp} + \mathcal{L}_\text{Des} \\
&= \beta_\textrm{SC} \cdot \mathcal{L}_{\textrm{SC}}(x) + \beta_\textrm{ST} \cdot \mathcal{L}_{\textrm{ST}}(x) \notag \\
&\quad + \beta_\textrm{HM} \cdot \mathcal{L}_{\textrm{HM}}(x) + \beta_\textrm{MI} \cdot \mathcal{L}_{\textrm{MI}}(x),
\end{align}
where $\beta_\textrm{SC}$, $\beta_\textrm{ST}$, $\beta_\textrm{HM}$, and $\beta_\textrm{MI}$ are hyperparameters. This joint objective enables the model to leverage the strengths of each individual loss, facilitating robust and effective learning in Few-Shot Continual Relation Extraction tasks.


\paragraph{Training Procedure.}
Algorithm \ref{alg:Framework} outlines the end-to-end training process at each task $\mathcal{T}^j$, with $\Phi_{j-1}$ denoting the model after training on the previous $j-1$ tasks. In line with memory-based continual learning methods, we maintain a memory buffer $\tilde{M}_{j-1}$ that stores a few representative samples from all previous tasks ${\mathcal{T}^1, \dots, \mathcal{T}^{j-1}}$, along with a relation description set $\tilde{E}_{j-1}$ that holds the descriptions of all previously encountered relations.

\begin{enumerate}
    \item \textbf{Initialization} (Line 1--2): 
    The model for the current task, $\Phi_j$, is initialized with the parameters of $\Phi_{j-1}$. We update the relation description set $\tilde{E}_j$ by incorporating new relation descriptions from $E_j$.
    
    \item \textbf{Training on the Current Task} (Line 3): 
    We train $\Phi_j$ on $D_j$ to learn the novel relations introduced in in $\mathcal{T}^j$.
    
    \item \textbf{Memory Update} (Lines 4--8):
    We select $L$ representative samples from $D_j$ for each relation $r \in R_j$. These are the $L$ samples whose latent representations are closest to the $1$-means centroid of all class samples. These samples constitute the memory $M_r$, leading to an updated overall memory $\tilde{M}_j = \tilde{M}_{j-1} \cup M_j$ and an updated relation set $\tilde{R}_j = \tilde{R}_{j-1} \cup R_j$.

    \item \textbf{Prototype Storing} (Line 9):
    A prototype set $\tilde{P}_j$ is generated based on the updated memory $\tilde{M}_j$.
    We generate a prototype set $\tilde{P}_j$ based on the updated memory $\tilde{M}_j$.

    \item \textbf{Memory Training} (Line 10):
    We refine $\Phi_j$ by training on the augmented memory set $\tilde{M}_j^*$, ensuring that the model preserves knowledge of relations from previous tasks.
    
    % \item \textbf{Data Augmentation} (Line 10). 
    % Inspired by \textbf{CITE CPL}, to cope with the scarcity of samples, we generate an augmented dataset $D_j^*$ using ChatGPT, enhancing the training data with more variations.

\end{enumerate}

% We detail the procedure in the subsections below.


% Example
% \begin{algorithm}[ht]
% \caption{Adaptive Unified Gradient Descent for CRE} \label{alg:augd}
% \textbf{Input}: Model parameters $\bm{\theta}$ and differentiable loss functions $L_d$ and $L_{re}$\\
% \textbf{Parameter}: Learning rate $\eta$\\
% \textbf{Output}: Updated parameter $\bm{\theta}^{*}$
% \begin{algorithmic}[1]
% % \For{each task $t$}
% \FOR{each $t \in [d, re]$}
% \STATE Compute gradient $\bm{g}_t := \nabla_{\bm{\theta}} L_t(\bm{\theta})$
% \STATE Compute gradient unit vector $\bm{u}_t := \bm{g}_t / ||\bm{g}_t||$
% \ENDFOR
% \STATE Calculate gradient differences $\bm{D}^\top := \ \bigr[\bm{g}_{d}^\top - \bm{g}_{re}^\top\bigr]$.
% \STATE Calculate magnitude-scaled gradient unit differences:
% \begin{equation*}
%     \bm{U}^\top := \ \left[||\bm{g}_{re}||\bm{u}_{d}^\top - ||\bm{g}_{d}||\bm{u}_{re}^\top\right].
% \end{equation*}
% \STATE Calculate scalar coefficients for the objectives:
% \begin{align*}
% [\alpha_{re}] &= \bm{g}_{d}\bm{U}^\top(\bm{D}\bm{U^\top})^{-1}, \\
% \alpha_{d} &= 1 - \alpha_{re}.
% \end{align*}
% \STATE Update model parameter:
% \begin{equation*}
% \bm{\theta}^{*} = \bm{\theta} - \eta \sum_{i \in \{d, re\}} \alpha_{i}\bm{g}_{i}
% \end{equation*}
% \end{algorithmic}
% \end{algorithm}
% End example



\begin{algorithm}[ht]
\caption{Training procedure at each task $\mathcal{T}^j$}
\label{alg:Framework}
\textbf{Input}: $\Phi_{j-1}, \tilde{R}_{j-1}, \tilde{M}_{j-1},\tilde{K}_{j-1}, D_j, R_j, K_j$. \\
\textbf{Output}: $\Phi_j, \tilde{M}_j, \tilde{K}_j, \tilde{P}_j.$
\begin{algorithmic}[1]
\STATE Initialize $\Phi_j$ from $\Phi_{j-1}$ 
\STATE $\tilde{K}_j \leftarrow \tilde{K}_{j-1} \cup K_j$
\STATE Update $\Phi_j$ by L on $D_j$ (train on current task)
\STATE $\tilde{M}_j \leftarrow \tilde{M}_{j-1}$
\FOR{each $r \in R_j$} 
    \STATE pick $L$ samples in $D_j$ and add them into $\tilde{M}_j$
\ENDFOR
\STATE $\tilde{R}_j \leftarrow \tilde{R}_{j-1} \cup R_j$
 % \STATE Generate augmented dataset $D_j^*$ using ChatGPT
\STATE Update $\tilde{P}_j$ with new data in ${D}_j$ (for inference)
\STATE Update $\Phi_j$ by $\mathcal{L}$ on $\tilde{M}_j$ and $D_j^*$ (train on memory)
\end{algorithmic}
\end{algorithm}

% \subsection{Reciprocal Rank Fusion
% Prediction}

% \begin{align}
% \text{RRF}(d) &= \sum_{i=1}^{n} \frac{1}{k + r_i(d)}
% \end{align}

% Where $n$ is the number of rankings being fused, $r_i(d)$ is the rank of document $d$ in the $i$-th ranking, $k$ is a constant.
% We leverage both the prototype and description to get the final fusion ranking. The first ranking is get by the euclid distance between the input and each prototype. The second ranking is get bt the cosine distance between the 


\begin{table*}[!ht]
    \centering
    \setlength{\tabcolsep}{1mm}
    % \resizebox{1.0\textwidth}{!}{
    \begin{tabular}{lllllllll|c}
        \multicolumn{3}{l}{\textbf{FewRel} \textit{(10-way--5-shot)}} \\
        \hline
        Method & $\mathcal{T}^1$ & $\mathcal{T}^2$ & $\mathcal{T}^3$ & $\mathcal{T}^4$ & $\mathcal{T}^5$ & $\mathcal{T}^6$ & $\mathcal{T}^7$ & $\mathcal{T}^8$ & $\Delta \downarrow$ \\ 
        \hline \hline
        % RP-CRE & 93.93 & 76.41 & 71.34 & 69.29 & 64.94 & 61.96 & 60.55 & 59.58 \\ 
        % CRL & 94.65 & 80.71 & 73.83 & 70.25 & 66.64 & 63.25 & 60.97 & 59.26 \\ 
        % CRECL & 93.92 & 82.57 & 74.14 & 69.33 & 66.52 & 64.61 & 62.98 & 59.98 \\ 
        % EDRA & 92.45 & 64.65 & 53.62 & 47.95 & 41.86 & 39.43 & 35.87 & 32.96 \\ \hline
        % SCKD & 94.75 & 82.83 & 76.21 & 72.19 & 70.61 & 67.15 & 64.86 & 62.98 & 31.77 \\ 
        % CPL & 94.87 & 85.14 & 78.80 & 75.10 & 72.57 & 69.57 & 66.85 & 64.50 & 30.37\\ 
        % DCRE-BERT (Our) & $94.84_{\pm 0.26}$ & $86.6_{\pm 2.63}$ & $81.45_{\pm 1.91}$ & $77.8_{\pm 2.83}$ & $75.93_{\pm 2.95}$ & $72.78_{\pm 2.2}$ & $70.46_{\pm 1.69}$ & $68.47_{\pm 0.85}$ & 26.37 \\
        CPL & $\mathbf{97.25_{\pm 0.30}}$ & $\mathbf{89.29_{\pm 2.51}}$ & $85.56_{\pm 1.21}$ & $82.10_{\pm 2.02}$ & $79.96_{\pm 2.72}$ & $78.41_{\pm 3.22}$ & $76.42_{\pm 2.25}$ & $75.20_{\pm 2.33}$ & 22.05 \\
        % DCRE & $96.83_{\pm 0.29}$ & $\mathbf{90.40_{\pm 3.40}}$ & $\mathbf{87.63_{\pm 1.87}}$ & $\mathbf{85.69_{\pm 1.94}}$ & $\mathbf{84.77_{\pm 2.45}}$ & $\mathbf{83.11_{\pm 1.48}}$ & $\mathbf{81.24_{\pm 1.09}}$ & $\mathbf{80.07_{\pm 0.51}}$ & \textbf{16.76} \\
        % \hline \\
        DCRE & $96.92_{\pm 0.16}$ & ${88.95_{\pm 1.72}}$ & $\mathbf{87.12_{\pm 1.52}}$ & {$\mathbf{85.44_{\pm 1.91}}$} & {$\mathbf{84.89_{\pm 2.12}}$} & {$\mathbf{83.52_{\pm 1.46}}$} & {$\mathbf{81.64_{\pm 0.69}}$} & {$\mathbf{80.34_{\pm 0.55}}$} & \textbf{16.58} \\
        \hline \\
        
        \multicolumn{3}{l}{\textbf{TACRED} \textit{(5-way-5-shot)}} \\
    \hline
        Method & $\mathcal{T}^1$ & $\mathcal{T}^2$ & $\mathcal{T}^3$ & $\mathcal{T}^4$ & $\mathcal{T}^5$ & $\mathcal{T}^6$ & $\mathcal{T}^7$ & $\mathcal{T}^8$ & $\Delta \downarrow$ \\ \hline
        % RP-CRE & 87.33 & 74.91 & 67.89 & 60.04 & 53.31 & 50.75 & 46.22 & 44.49 \\ 
        % CRL & 88.31 & 76.29 & 69.77 & 61.92 & 54.67 & 50.91 & 47.01 & 44.28 \\ 
        % CRECL & 87.09 & 78.11 & 61.92 & 55.61 & 53.41 & 51.92 & 47.55 & 45.53 \\ 
        % EDRA & 83.85 & 53.99 & 43.01 & 38.82 & 33.10 & 24.74 & 23.61 & 20.43 \\ 
        \hline
        % SCKD & 88.42 & 79.35 & 70.61 & 66.78 & 60.47 & 58.05 & 54.41 & 52.11 & 36.31\\ 
        % CPL & 86.27 & 81.55 & 73.52 & 68.96 & 63.96 & 62.66 & 59.96 & 57.39 & 28.88 \\
        % DCRE-BERT (Our) & $87.25_{\pm 0.45}$ & $85.92_{\pm 5.89}$ & $80.81_{\pm 3.34}$ & $78.85_{\pm 2.77}$ & $74.95_{\pm 3.44}$ & $73.83_{\pm 2.99}$ & $71.79_{\pm 2.59}$ & $70.02_{\pm 2.17}$ & 17.23 \\
        CPL & $88.74_{\pm 0.44}$ & $85.16_{\pm 5.38}$ & $78.35_{\pm 4.46}$ & $77.50_{\pm 4.04}$ & $76.01_{\pm 5.04}$ & $76.30_{\pm 4.41}$ & $74.51_{\pm 5.06}$ & $73.83_{\pm 4.91}$ & 14.91 \\
        % DCRE & $\mathbf{89.38_{\pm 0.57}}$ & $\mathbf{87.26_{\pm 4.49}}$ & $\mathbf{83.25_{\pm 3.23}}$ & $\mathbf{83.10_{\pm 1.99}}$ & $\mathbf{83.22_{\pm 2.18}}$ & $\mathbf{81.88_{\pm 1.45}}$ & $\mathbf{81.71_{\pm 1.05}}$ & $\mathbf{81.12_{\pm 1.13}}$ & \textbf{8.26} \\
        DCRE & {$\mathbf{89.06_{\pm 0.59}}$} & {$\mathbf{87.41_{\pm 5.54}}$} & {$\mathbf{84.91_{\pm 3.38}}$} & {$\mathbf{84.18_{\pm 2.44}}$} & {$\mathbf{82.74_{\pm 3.64}}$} & {$\mathbf{81.92_{\pm 2.33}}$} & {$\mathbf{79.34_{\pm 2.89}}$} & {$\mathbf{79.10_{\pm 2.37}}$} & \textbf{9.96}  \\
        

    \hline
      
    \end{tabular}%}
    \caption{Accuracy (\%) of methods using LLM2Vec-based backbone after training for each task. The best results are in \textbf{bold}.}
    \label{table:main_llmvec}
\end{table*}


% \subsection{Description Retrieval} 
\subsection{Descriptive Retrieval Inference}
\label{sec:method:relationpred}

Traditional methods such as Nearest Class Mean (NCM) \cite{ma-etal-2024-making} predict relations by selecting the class whose prototype has the smallest distance to the test sample $x$. While effective, this approach relies solely on distance metrics, which may not fully capture the nuanced relationships between a sample and the broader context provided by class descriptions.

Rather than merely seeking the closest prototype, we aim to retrieve the class description that best aligns with the input, thereby leveraging the inherent semantic meaning of the label. To achieve this, we introduce \emph{Descriptive Retrieval Inference} (DRI), a retrieval mechanism fusing two distinct reciprocal ranking scores. This approach not only considers the proximity of a sample to class prototypes but also incorporates cosine similarity measures between the sampleâ€™s hidden representation $\bm{z}$ and relation descriptions generated by an LLM. This dual focus on both spatial and semantic alignment ensures that the final prediction is informed by a richer, more robust understanding of the relations.

Given a sample $x$ with hidden representation $\bm{z}$ and a set of relation prototypes $\{\bm{p}_r\}_{r=1}^n$, the inference process begins by calculating the negative Euclidean distance between $\bm{z}$ and each prototype $\bm{p}_r$:
\begin{align}
&\textbf{E}(x,r) = -{\left\| \bm{z} - \bm{p}_r \right\|}_2,\\
&\bm{p}_r = \frac{1}{L} \sum_{i=1}^{L}\bm{z}_i, 
\end{align}
where $L$ is the memory size per relation. Simultaneously, we compute the cosine similarity between the hidden representation and each relation description prototype, $\gamma(\bm{z}, \bm{d}_r)$. These two scores are combined into DRI score of sample $\bm{x}$ w.r.t relation $r$ for inference, ensuring that predictions align with both label prototypes and relation descriptions:
% To enable this combination, both scores are converted into relative reciprocal rankings among all candidates:
\begin{align}
\begin{aligned}
\textrm{DRI}(x, r) = &\frac{\alpha}{\epsilon + \text{rank}(\textbf{E}(x, r))} + \frac{1 - \alpha}{\epsilon + \text{rank}(\gamma(\bm{z}, \bm{d}_r))},
\end{aligned}
\end{align}
where $d_r = \frac{1}{K}\sum_{i=1}^{K} d_r^i$, $\text{rank}(\cdot)$ represents the rank position of the score among all relations. The $\alpha$ hyperparameter balances the contributions of the Euclidean distance-based score and the cosine similarity score in the final ranking for inference, and $\epsilon$ is a hyperparameter that controls the influence of lower-ranked relations in the final prediction. By adjusting $\epsilon$, we can fine-tune the model's sensitivity to less prominent relations. Finally, the predicted relation label $y^*$ is predicted as the one corresponding to the highest DRI score:
\begin{equation}
y_x^* = \underset{r=1, \ldots, n}{\operatorname{argmax}} \, \text{DRI}(x,r)
\end{equation}

This fusion approach for inference complements the learning paradigm, ensuring consistency and reliability throughout the FCRE process. By effectively balancing the strengths of protoype-based proximity and description-based semantic similarity, it leads to more accurate and robust predictions across sequential tasks.
