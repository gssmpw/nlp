% \subsection{Input representation}
% \label{sec:method:input}


\subsection{Label Descriptions}
\label{sec:method:description}

A core component of our method is achieving \textcolor{red}{robust class latent representations}, making class encoding crucial. As a result, having additional information for each label is essential for our approach. In the datasets used for benchmarking, each relation is already accompanied by a concise and fixed description, which we refer to as the \textit{Raw description}. Although exploiting these descriptions during training RE model was investigated in \textbf{[CITE]} and achieved positive results, this approach was still limited as it depends on the limited 1-1 mapping between input embeddings and only one label description representation for each task. The poverty and restricted information of these kinds of texts not only hinder model optimization but also lead to instability of the corresponding learned representations.

To address this limitation, instead of using only an existing description for each class in each dataset, we take advantage of the power of pre-trained large language models (i.e, Gemini 1.5 \cite{team2023gemini, reid2024gemini}) to generate $K$ detailed, diverse and illustrative descriptions. In particular, when training each input sample, the respective raw description will be fed into \textit{the prompt}, serving as \textcolor{red}{\textit{"an expert in the loop"} to guide the LLM}. Our prompt template is depicted in Figure \ref{fig:tnse1}, with the corresponding output sample included in Appendix \ref{apdx:prompt_template}.

\begin{figure}[t!]
	\centering
	\includegraphics[width=\linewidth]{imgs/prompt-new-4.pdf}
	\caption{Prompt to generate label description.}
	\label{fig:tnse1}
\end{figure}


By exploiting LLMs in this way, we have achieved the retrieval of diverse and meaningful information for each training sample, thereby making the model generalize better, mitigating more efficiently in the challenging setting of FCRE.

% \subsection{Representation Learning}
% \label{sec:bg:replearning}

% In conventional Relation Extraction scenarios, a basic framework employing BERT, followed by a classification MLP, is often used to directly learn the mapping from the input space to the label space using Cross Entropy Loss. However, this approach proves inadequate in data-scarce settings \cite{snell2017prototypical}. Consequently, contrastive learning emerges as a more suitable approach, as it can directly influence the latent space and better capture the inherent distinctions between samples from different classes. To enrich the semantic information drawn from both the samples and the generated label descriptions, we design an objective function comprising four key loss components.


% % main components: $\mathcal{L}_{RE}$ to force relation embeddings..., and $\mathcal{L}_{Retrieval}$ to facilitate...
% % $$ \mathcal{L} = \mathcal{L}_{RE} + \mathcal{L}_{Retrieval}$$ where $\mathcal{L}_{RE}$ force relation embeddings..., and $\mathcal{L}_{Retrieval}$ facilitate ... 

% % \subsubsection{Relation extraction loss $\mathcal{L}_{RE}$}

% \paragraph{Supervised Contrastive Loss.}
% To enhance the model's discriminative capability, we employ the Supervised Contrastive Loss (SCL). This loss function is specifically designed to bring positive pairs of samples, which share the same class label, closer together in the latent space. Simultaneously, it pushes negative pairs, belonging to different classes, further apart. Let $\bm{z}_x$ represent the hidden vector output of sample ${x}$, the positive pairs $(\bm{z}_x, \bm{z}_p)$ are those who share a class, while the negative pairs $(\bm{z}_x, \bm{z}_n)$ correspond to different labels. The SCL is computed as follows:
% \begin{gather}
% \mathcal{L}_{\textrm{SC}}(x) = - \sum_{{p} \in P({x})} \log \frac{
%     f(\bm{z}_x, \bm{z}_p)
% }
% {Z_\textrm{SC}({x})}, \\
% Z_\textrm{SC}({x}) = \sum_{{p} \in P({x})} f(\bm{z}_x, \bm{z}_p) \  + \sum_{{n} \in N({x})} f(\bm{z}_x, \bm{z}_n), 
% \end{gather}
% where $f(\mathbf{x}, \mathbf{y}) \coloneqq \exp\left(\frac{\gamma(\mathbf{x}, \mathbf{y})}{\tau}\right)$, $\gamma(\cdot, \cdot)$ denotes the cosine similarity function, and $\tau$ is the temperature scaling hyperparameter. $P({x})$ and $N({x})$ denote the sets of positive and negative samples with respect to sample ${x}$, respectively.

% \paragraph{Hard Soft Margin Triplet Loss.}
% To achieve a balance between flexibility and discrimination, the Hard Soft Margin Triplet Loss (HSMT) integrates both hard and soft margin triplet loss concepts. This loss function is designed to maximize the separation between the most challenging positive and negative samples, while preserving a soft margin for improved flexibility. Formally, the loss is defined as:
% \begin{multline}
% \mathcal{L}_{\textrm{ST}}({x}) = - \log \bigg(1 \ +  
%     \max_{{p} \in P(\bm{x})} e^{\xi(\bm{z}_x, \bm{z}_p)} \\
%     - \min_{{n} \in N({x})} e^{ \xi(\bm{z}_x, \bm{z}_n)}
% \bigg),
% \end{multline}
% where $\xi(\cdot, \cdot)$ denotes the Euclidean distance function. The objective of this loss is to ensure that the hardest positive sample is as distant as possible from the hardest negative sample, thereby enforcing a flexible yet effective margin.


\subsection{Description Retrieval}
\label{sec:method:retrieval}

To encourage the knowledge of the retrieved descriptions to participate and effectively support the representation learning of input embeddings, we proposed exploiting the following objective function:
\begin{equation}
    \mathcal{L}_{Retrieval} = \beta_{HM}\cdot \mathcal{L}_{HM} + \beta_{MI}\cdot\mathcal{L}_{MI},
\end{equation}
where $\mathcal{L}_{HM}$ is \textit{Hard Margin loss}, $\mathcal{L}_{MI}$ is \textit{Mutual Information loss}, and $\beta_{HM},  \beta_{MI}$ are corresponding hyper-parameters. These two loss functions have symmetrical and mutually supportive roles, which are described in detail in what follows.  


\paragraph{Description-centered Alignment - DCA (Hard margin loss :vv)}

% \begin{align}
% \mathcal{L}_{\mathrm{MI}}(i) = - \log \frac{ \sum_{p \in P(i)} 
%     \exp \left(\frac{\gamma(\bm{z_p}, \bm{d_i})}{\tau}\right)
% }{
%     Z_{MI}(i)
% }
% % \exp \left(\frac{\gamma(\bm{z_i}, \bm{d_i})}{\tau}\right) 
% %     + \sum_{n \in N(i)} \exp \left(\frac{\gamma(\bm{z_n}, \bm{d_i})}{\tau}\right)
% \end{align}

% $$
% Z_{MI}(i) = {\sum_{p \in P(i)} \! \exp \left( \frac{\gamma(\bm{d_i}, \bm{z_p})}{\tau} \right) 
%   \!+\! \sum_{n \in N(i)} \! \exp \left(\frac{\gamma(\bm{d_i}, \bm{z_n})}{\tau} \right)} 
% $$

% The Hard Margin (HM) Loss focuses on maximizing the margin between hard positive and hard negative samples. Hard positive samples are those that are difficult to distinguish from the negative samples, and hard negative samples are those that are close to the positive samples in the latent space. The HM loss is defined as:

leverages each label description to refine the model's ability to distinguish between hard positive and hard negative pairs. Given a sample $\bm{x}$, one of the representation vectors of its corresponding retrieved descriptions is $\bm{d}^i_x$; $\bm{z}_p$ and $\bm{z}_n$ representing the hidden vectors of its positive and negative samples respectively. Then the loss function is formulated to maximize the similarity between $\bm{d}^i_x$ and the corresponding positive embeddings while pushing away the negative ones:
\begin{align}
% \begin{aligned}
    \mathcal{L}_{\textrm{HM}}(x) = &\sum_{p \in P_\textrm{H}(x)} (1 - \gamma(\bm{d}_x, \bm{z}_p))^2 \\
    &+ \sum_{n \in N_\textrm{H}(x)} max(0, m - 1 + \gamma(\bm{d}_x, \bm{z}_n) )^2,
% \end{aligned}
\end{align}
where $m$ is a margin parameter, $P_\textrm{H}(x)$ and $N_\textrm{H}(x)$ represent the sets of hard positive and hard negative samples, respectively. They are determined by comparing the similarity between $\bm{d}_x$ and both positive and negative pairs, specifically focusing on the most challenging pairs where the similarity to negative samples is close to or greater than that of positive samples, defined as follows:
\begin{align}
    \begin{aligned}
    P_\textrm{H}(x) = \{&p \in P(x) | 1 - \gamma(\bm{d}_x, \bm{z}_p) \\
    &> min_{n \in N(x)}(1 - \gamma(\bm{d}_x, \bm{z}_n)) \}
    \end{aligned}, \\
    \begin{aligned}
    N_\textrm{H}(x) = \{&n \in N(x) | 1 - \gamma(\bm{d_x}, \bm{z}_n) \\
    &< max_{p \in P(x)}(1 - \gamma(\bm{d}_x, \bm{z}_p)) \}.
    \end{aligned}
\end{align}

By utilizing the label description vector $\bm{d_x}$, the Hard Margin Loss effectively sharpens the model's decision boundary, reducing the risk of confusion between similar classes and improving overall performance in few-shot learning scenarios. The loss penalizes the model more heavily for misclassifications involving these hard samples, ensuring that the model pays particular attention to the most difficult cases, thereby enhancing its discriminative power.

\paragraph{Relation-centered constraint (Mutual Information Loss)}

The Mutual Information (MI) Loss is designed to maximize the mutual information between the input sample's hidden representation and its corresponding label description, promoting a more informative alignment between them. Let $\bm{d}_x$ represent the hidden vector corresponding to the label description of sample $\bm{x}$, and $\bm{d}_n$ the hidden vector of other label descriptions. According to \citet{DBLP:journals/corr/abs-1807-03748}, the Mutual Information $MI(x)$ between the input embedding $\bm{z}_x$ and its corresponding label description follows the following inequation:
\begin{equation}
    MI \geq \log B + \textnormal{InfoNCE} (\{ x_i\}_{i=1}^B; h),
\end{equation}
where we have defined:
\begin{gather}
\textnormal{InfoNCE} (\{ x_i\}_{i=1}^B; h) =
\frac{1}{B}\sum_{i=1}^B \log \frac{h(\bm{z_i}, \bm{d_i})}{\sum_{j=1}^B h(\bm{z_i}, \bm{d_j})}, \\
h(\bm{z}_x, \bm{d}_j) = \exp \frac{\bm{z}_x^TW\bm{d}_x}{\tau}
\end{gather}
where $\tau$ is the temperature, $B$ is mini-batch size and $W$ is a trainable parameter. Finally, the MI loss function in our implementation is: 
% \begin{equation}
%     \mathcal{L}_{MI} = -\sum_{(x_i , y_i) \in D_{train}^{k}}\textnormal{InfoNCE} (\{x_i\}_{i=1}^B; h)
% \end{equation}
\begin{align}
\mathcal{L}_{\textrm{MI}}(x) = - \log \frac{
    h(\bm{z}_x, \bm{d}_x)
}{
    h(\bm{z}_x, \bm{d}_x)
    + \sum_{n \in N(x)} h(\bm{z}_x, \bm{d}_n)
}
\end{align}

This loss ensures that the representation of the input sample is strongly associated with its corresponding label, while reducing its association with incorrect labels, thereby enhancing the discriminative power of the model.


\paragraph{Joint Training Objective Function.}
The final model is trained using a combination of the aforementioned loss functions, weighted by their respective coefficients:

\begin{align}
\begin{aligned}
\mathcal{L}(x) = &\beta_\textrm{SC} \cdot \mathcal{L}_{\textrm{SC}}(x) + \beta_\textrm{ST} \cdot \mathcal{L}_{\textrm{ST}}(x) \\
&+ \beta_\textrm{HM} \cdot \mathcal{L}_{\textrm{HM}}(x) + \beta_\textrm{MI} \cdot \mathcal{L}_{\textrm{MI}}(x)
\end{aligned}
\end{align}
where $\beta_\textrm{SC}$, $\beta_\textrm{ST}$, $\beta_\textrm{HM}$, and $\beta_\textrm{MI}$ are hyperparameters that balance the contribution of each loss term. This joint objective enables the model to leverage the strengths of each individual loss, facilitating robust and effective learning in Few-Shot Continual Relation Extraction tasks.


\subsection{Training Procedure}
\label{subsec:Framework}



% \begin{figure}[t]
%     \centering
%     \includegraphics[width=0.5\textwidth]{imgs/architecture.pdf}
%     \caption{ Our Framework.}
%     \label{fig:framework} 
%     \vspace{-.2cm}
% \end{figure}

\begin{figure}[t]
    \centering
    \hspace{-0.5cm} % Adjust the value as needed
    \includegraphics[width=0.5\textwidth]{imgs/architecture.pdf}
    \caption{Our Framework.}
    \label{fig:framework} 
    % \vspace{-.2cm}
\end{figure}

\begin{figure*}[t]
    \centering
    \hspace{-0.5cm} % Adjust the value as needed
    \includegraphics[width=1.\textwidth]{imgs/change_backbone.pdf}
    \caption{Adapting LLM2Vec for ours architecture}
    \label{fig:change_backbone} 
    % \vspace{-.2cm}
\end{figure*}


Algorithm \ref{alg:Framework} shows the end-to-end training for task $T_j$, with the model $\Phi_{j-1}$ previously trained. Following the memory-based methods for continual learning, we use a memory $\tilde{M}_{j-1}$ to preserve a few samples in all previous tasks $\{T_1,\dots,T_{j-1}\}$ and a relation description set $\tilde{K}_{j-1}$ to store the descriptions of all previous relations.

\begin{enumerate}
    \item \textbf{Initialization} (Line 1--2). 
    The current model $\Phi_j$ inherits the parameters of $\Phi_{j-1}$. We initialize $\Phi_j$ and update the relation description set $\tilde{K}_j$ by incorporating new relation descriptions from $K_j$.
    
    \item \textbf{Training the Current Task} (Line 3). 
    We adapt $\Phi_j$ on $D_j$ to learn the knowledge of new relations in $T_j$.
    
    \item \textbf{Memory Update} (Lines 4--8). 
    We initialize the memory $\tilde{M}_j$ from $\tilde{M}_{j-1}$. 
    % Inspired by \cite{han2020continual,cui2021refining},
    We select $L$ typical samples from $D_j$ for every relation $r \in R_j$, which constitute the memory $M_r$. The overall memory for all observed relations until now is $\tilde{M}_j = \tilde{M}_{j-1} \cup M_j$. The relation set is also updated as $\tilde{R}_j = \tilde{R}_{j-1} \cup R_j$.
    
    \item \textbf{Prototype Generation} (Line 9). 
    We generate a prototype set $\tilde{P}_j$ based on the updated memory $\tilde{M}_j$.
    
    % \item \textbf{Data Augmentation} (Line 10). 
    % Inspired by \textbf{CITE CPL}, to cope with the scarcity of samples, we generate an augmented dataset $D_j^*$ using ChatGPT, enhancing the training data with more variations.
    
    \item \textbf{Memory Training} (Line 10). 
    We update $\Phi_j$ by training on the augmented memory set $\tilde{M}_j^*$. This step helps in preserving the prior knowledge for identifying relations in previous tasks.
\end{enumerate}

We detail the procedure in the subsections below.


% Example
% \begin{algorithm}[ht]
% \caption{Adaptive Unified Gradient Descent for CRE} \label{alg:augd}
% \textbf{Input}: Model parameters $\bm{\theta}$ and differentiable loss functions $L_d$ and $L_{re}$\\
% \textbf{Parameter}: Learning rate $\eta$\\
% \textbf{Output}: Updated parameter $\bm{\theta}^{*}$
% \begin{algorithmic}[1]
% % \For{each task $t$}
% \FOR{each $t \in [d, re]$}
% \STATE Compute gradient $\bm{g}_t := \nabla_{\bm{\theta}} L_t(\bm{\theta})$
% \STATE Compute gradient unit vector $\bm{u}_t := \bm{g}_t / ||\bm{g}_t||$
% \ENDFOR
% \STATE Calculate gradient differences $\bm{D}^\top := \ \bigr[\bm{g}_{d}^\top - \bm{g}_{re}^\top\bigr]$.
% \STATE Calculate magnitude-scaled gradient unit differences:
% \begin{equation*}
%     \bm{U}^\top := \ \left[||\bm{g}_{re}||\bm{u}_{d}^\top - ||\bm{g}_{d}||\bm{u}_{re}^\top\right].
% \end{equation*}
% \STATE Calculate scalar coefficients for the objectives:
% \begin{align*}
% [\alpha_{re}] &= \bm{g}_{d}\bm{U}^\top(\bm{D}\bm{U^\top})^{-1}, \\
% \alpha_{d} &= 1 - \alpha_{re}.
% \end{align*}
% \STATE Update model parameter:
% \begin{equation*}
% \bm{\theta}^{*} = \bm{\theta} - \eta \sum_{i \in \{d, re\}} \alpha_{i}\bm{g}_{i}
% \end{equation*}
% \end{algorithmic}
% \end{algorithm}
% End example


\begin{algorithm}[ht]
\caption{Training procedure for $T_j$}
\label{alg:Framework}
\textbf{Input}: $\Phi_{j-1}, \tilde{R}_{j-1}, \tilde{M}_{j-1},\tilde{K}_{j-1}, D_j, R_j, K_j$. \\
\textbf{Output}: $\Phi_j, \tilde{M}_j, \tilde{K}_j$.
\begin{algorithmic}[1]
\STATE Initialize $\Phi_j$ from $\Phi_{j-1}$ 
\STATE $\tilde{K}_j \leftarrow \tilde{K}_{j-1} \cup K_j$
\STATE Update $\Phi_j$ by L on $D_j$ (train current task); 
\STATE $\tilde{M}_j \leftarrow \tilde{M}_{j-1}$
\FOR{each $r \in R_j$} 
    \STATE pick $L$ samples in $D_j$ and add them into $\tilde{M}_j$
\ENDFOR
\STATE $\tilde{R}_j \leftarrow \tilde{R}_{j-1} \cup R_j$\;
\STATE Generate prototype set $\tilde{P}_j$ based on $\tilde{M}_j$\;
 % \STATE Generate augmented dataset $D_j^*$ using ChatGPT\; 
\STATE Update $\Phi_j$ by $\mathcal{L}$ on $\tilde{M}_j$ and $D_j^*$ (train memory) 
\end{algorithmic}
\end{algorithm}

% \subsection{Reciprocal Rank Fusion
% Prediction}

% \begin{align}
% \text{RRF}(d) &= \sum_{i=1}^{n} \frac{1}{k + r_i(d)}
% \end{align}

% Where $n$ is the number of rankings being fused, $r_i(d)$ is the rank of document $d$ in the $i$-th ranking, $k$ is a constant.
% We leverage both the prototype and description to get the final fusion ranking. The first ranking is get by the euclid distance between the input and each prototype. The second ranking is get bt the cosine distance between the 

\subsection{Relation Prediction via Reciprocal Rank Fusion} \label{secrelationpre}

Building on the traditional Nearest-Class-Mean (NCM) approach \cite{ma-etal-2024-making}, our method enhances relation prediction by incorporating a retrieval mechanism based on reciprocal rank fusion. While the NCM classifier predicts relations by minimizing the Euclidean distance between the test sample $\bm{x}$ and the class prototypes derived from historical memory $\hat{\mathcal{M}}^n$, our approach augments this with the integration of cosine similarity measures, leveraging relation descriptions generated by a Large Language Model (LLM).

Given a sample $\bm{x}$ with hidden representation z and the set of relation prototypes ${p_r}_{r=1}^n$, we first calculate the NCM score for each prototype $p_r$:
% \begin{align}
% &p_r = \frac{1}{L} \sum_{i=0}^{L} \bm{E} \left(\bm{\hat{x}}_i^r\right), \notag \
% &\text{NCM_score}(r) = -{\left| \bm{E}\left(\bm{x}\right) - p_r\right|}_2
% \end{align}
\begin{align}
&p_r = \frac{1}{L} \sum_{i=1}^{L} \bm{E} \left(\bm{\hat{x}}_i^r\right), \notag \\
&\text{NCM}(x,r) = -{\left\| \bm{z} - p_r \right\|}_2
\end{align}


Simultaneously, we compute the cosine similarity between the hidden representation x and each relation description vector $\bm{d}_r$ generated by the LLM: $\gamma(\bm{z}, \bm{d_r})$


To robustly predict the relation, we then fuse the NCM and cosine similarity scores using a reciprocal rank fusion (RRF) strategy. The final prediction score for each relation $r$ is given by:

% \begin{align}
% &\text{RRF_score}(r) = \frac{1}{k + \text{rank}(\text{NCM_score}(r))} + \frac{1}{k + \text{rank}(\text{Cosine_score}(r))}
% \end{align}
% \begin{align}
% \text{RRF\_score}(r) = \frac{1}{k + \text{rank}\left(\text{NCM\_score}(x,r)\right)} + \frac{1}{k + \text{rank}(\text{\gamma(\bm{z}, \bm{d_r})})}
% \end{align}

\begin{align}
\begin{aligned}
\textrm{RRF}(r) = &\frac{1}{k + \text{rank}\left(\text{NCM}(x, r)\right)} + \frac{1}{k + \text{rank}\left(\gamma(\bm{z}, \bm{d}_r)\right)}
\end{aligned}
\end{align}


where $\text{rank}(\cdot)$ represents the rank position of the score among all relations, and $k$ is a small constant to avoid division by zero.

Finally, the relation label $y^*$ is predicted as the one corresponding to the highest RRF score:

% \begin{align}
% y^* = \underset{r=1, \ldots, n}{\operatorname{argmax}} , \text{RRF_score}(r)
% \end{align}

\begin{align}
y^* = \underset{r=1, \ldots, n}{\operatorname{argmax}} \, \text{RRF\_score}(r)
\end{align}



This fusion approach effectively balances the strengths of both NCM-based proximity and description-based semantic similarity, resulting in a more accurate and robust prediction across sequential tasks.


