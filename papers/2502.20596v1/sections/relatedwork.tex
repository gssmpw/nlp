\paragraph{Continual Learning (CL)}is a learning scenario that requires models to continually acquire new knowledge from a sequence of tasks while preventing the loss of previously learned information. The main challenge in CL is \textit{catastrophic forgetting} \citep{DBLP:conf/nips/French93}. To address this problem, memory-based approaches prove to be effective methods for both machine learning \citep{DBLP:conf/cvpr/RebuffiKSL17, DBLP:conf/nips/ShinLKK17} and NLP problems \citep{wang-etal-2019-sentence, han-etal-2020-continual}. In particular, models need to save a few representative samples from the current task in a memory buffer and replay these samples when learning new tasks to review old knowledge. 
% Current CL approaches are basically categorized into three types: 1) regularization methods \textbf{(Li and Hoiem, 2017; Ritter et al., 2018)}, which impose additional constraints to control updating of parameters, aiding the retention of older knowledge; 2) dynamic architecture methods \textbf{(Fernando et al., 2017; Mallya et al., 2018)}, which dynamically expand the model's architecture to accommodate new knowledge as new tasks arise; and 3) memory-based methods (Rebuffi et al., 2017; Shin et al., 2017), which save a few representative samples from the current task in memory and replay these samples when learning new tasks to review old knowledge. 
% Among these, memory-based methods are the most effective for NLP tasks \textbf{(Wang et al., 2019; Han et al., 2020)}. 

\paragraph{Fewshot Continual Relation Extraction} is a challenging scenario, which was introduced by \citep{DBLP:conf/acl/QinJ22} for Relation Extraction problems.  This challenge arises due to the limited availability of data for new tasks, coupled with the high cost and time involved in obtaining high-quality data. Recent work like \citet{DBLP:conf/acl/WangWH23, DBLP:conf/acl/ChenWS23, DBLP:conf/coling/MaHL024} propose memory-based solutions, which suggest imposing objective functions on the embedding space and classification head. Specifically, \citet{DBLP:conf/acl/WangWH23} employs serial objective functions based on contrastive and distillation,  \citet{DBLP:conf/acl/QinJ22} leverage extra training data from unlabeled text, and \citet{DBLP:conf/acl/ChenWS23} proposes a consistent prototype learning strategy to help the model distinguish between different relation representations, thus enhancing representation learning efficiency. 

% However, in these methods, eliminating the pre-trained LM head and training a new classifier still leads to overfitting and forgetting due to limited data, as it emphasizes discriminative features only. To address this problem, we propose a novel approach that leverages LM heads, which are often overlooked in pre-trained models for downstream tasks. Our method not only helps preserve prior knowledge from the backbone but also supports the training of the main classifier, thereby further reducing both catastrophic forgetting and overfitting.
% and significantly enhancing the model's performance, 


% Our approach also employs a memory-based strategy, but we focus more on optimizing the use of Pre-trained Language Models (PLMs) to address CFRE.