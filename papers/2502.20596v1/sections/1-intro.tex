Relation Extraction (RE) refers to classifying semantic relationships between entities within text into predefined types.  Conventional RE tasks assume all relations are present at once, ignoring the fact that new relations continually emerge in the real world.  Few-shot Continual Relation Extraction (FCRE) is a subfield of continual learning \cite{hai2024continual,linhp,phan2022reducing, tran2024leveraginghierarchicaltaxonomiespromptbased, tran2024koppaimprovingpromptbasedcontinual,le2024mixture} where a model must continually assimilate new emerging relations while avoiding the forgetting of old ones, a task made even more challenging by the limited training data available. The importance of FCRE stems from its relevance to dynamic real-world applications, garnering increasing interest in the field \cite{ chen-etal-2023-consistent, le2024continual,le2025aaai}.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{imgs/fewrel_tacred_accuracies.pdf}
    \caption{Existing FCRE methods face catastrophic forgetting due to the limited and poor quality of old training samples stored in the memory buffer.}
    \label{fig:line_plot_bert}
\end{figure}

State-of-the-art approaches to FCRE often rely on memory-based methods for continual learning  \cite{lopez2017gradient, nguyen2023spectral, le2024sharpseq,dao2024lifelong}. However, these methods frequently suffer from overfitting to the limited samples stored in memory buffers. This overfitting hampers the reinforcement of previously learned knowledge, leading to catastrophic forgettingâ€”a marked decline in performance on learnt relations when new ones are introduced (Figure \ref{fig:line_plot_bert}). The few-shot scenario of FCRE exacerbates these issues, as the scarcity of data not only impedes learning on new tasks, but also hinders helpful data augmentation, which are crucial in many methods \cite{shin2017continual}.

In order to improve on these methods, we must not completely disregard them or dwell on their weaknesses, but rather contemplate their biggest strength. \emph{Why do so many methods use the memory buffer in the first place?} The primary objective of these replay buffers is to rehearse and reinforce past knowledge, providing the model with something to "look back" at during training. However, these past samples may not always be representative of the entire class and can still lead to sub-optimal performance. Based on this observation, we propose a straightforward: besides relying on potentially unrepresentative past samples, we leverage our knowledge of the past relations themselves. This insight leads to our approach of generating detailed descriptions for each relation. These descriptions inherently represent the class more accurately than the underlying information from a set of samples, serving as stable pivots for the model to align with past knowledge while learning new information. By using these descriptions, we create a more robust and effective method for Few-Shot Continual Relation Extraction, ensuring better retention of knowledge across tasks.

Overall, our paper makes the following contributions:
\begin{description}
    \item[a.] We introduce an innovative approach to Few-Shot Continual Relation Extraction that leverages Large Language Models (LLMs) to generate comprehensive descriptions for each relation. These descriptions serve as stable class representations in the latent space during training. Unlike the variability and limitations of a limited set of samples from the memory buffer, these descriptions define the inherent meaning of the relations, offer a more reliable anchor, significantly reducing the risk of catastrophic forgetting. Importantly, LLMs are employed exclusively for generating descriptions and do not participate in the training or inference processes, ensuring that our method incurs minimal computational overhead.

    \item[b.] We design a bi-encoder retrieval learning framework for both sample and class representation learning. In addition to sample representation contrastive learning, we integrate a description-pivot learning process, ensuring alignment of samples which maximize their respective class samples proximity, while non-matching samples are distanced.
    
    \item[c.] Building on the enhanced representations, we introduce the \emph{Descriptive Retrieval Inference} (DRI) strategy. In this approach, each sample "retrieves" the most fitting relation using a reciprocal rank fusion score that integrates both class descriptions and class prototypes, effectively finalizing the retrieval-based paradigm that underpins our method.
    % This score is differentiable, allowing it to be used as a retrieval objective during training.
\end{description}


