Our work provides a game theoretic model of the numerical example and informal theorem in section $5$ of **Gibson, "Manipulation and the Value of Information"**, in particular, we also assume strategic data providers (which we call influencers) to large language models (which we call receivers), and our model leads to results consistent with their example where the data providers untruthfully report their opinions. In addition, we prove the existence of pure strategy Nash equilibria of this class of games, and we show the property that almost all influencers maximally exaggerate their preferences in every equilibrium. Our work is also closely related to **Dent et al., "Bayesian Persuasion"**, which models the interaction between multiple influencers by a dynamic Bayesian game. They described the phenomenon of strategic extreme exaggeration, which is also discussed in **Golman and Schwarz, "Strategic Extremeness"**, **Rogers, "Manipulation through Feedback"**, **Bachrach et al., "Persuasion Games with Multiple Influencers"**, and **Manshaei et al., "Game Theory Meets Network Security and Privacy"** for various applications, but they do not explicitly compute the equilibria of the original game or quantify the amount of exaggeration. In comparison, we use a static game with known influencer types and we are able to provide better characterizations of the set of equilibria of the game.

Value alignment aims to make language models produce outputs that are more aligned with human values. Existing training frameworks tailored for this purpose, such as **Geoffroy et al., "Value Alignment through Human Feedback"** and **Christiano et al., "Training Reward Functions from Human Feedback"**, collect preference data from humans and train a large language model to follow users' intent. However, research in this direction mostly focuses on the algorithmic aspects of value alignment and does not emphasize the heterogeneity of human values. There has been work that studies how to make LLMs align with diverse preferences of different demographic groups **Sarafian et al., "Fair Value Alignment"**, but they do not have a rigorous game theoretic foundation that characterizes strategic behaviors of preference data providers. There is also recent work by **Vaswani et al., "Nash Learning for Preference Modeling"**, **Perrault et al., "Direct Nash Optimization for Reward Learning"**, and **Bartlett et al., "Learning Pairwise Preferences with Direct Nash Optimization"** on learning a pairwise or general preference model, where they used the term Nash learning or Direct Nash Optimization. The players in their zero-sum games are not strategic data providers, and they use the Nash equilibrium solution concept mainly as an optimization technique to solve their minimax problem.

%\jerry{could you please take a brief look at the problem setting of "Bayesian persuasion", identify key literatures, and think how we can relate to and differentiate from that line of work?}

% ____ Training language models to follow instructions with human feedback
% \begin{itemize}
% \item Using RLHF for value alignment
% \end{itemize}____ Direct Preference Optimization: Your Language Model is Secretly a Reward Model
% \begin{itemize}
% \item Introducing DPO
% \item Different from RLHF, no explicit reward model. Based on a classification task (preference).
% \end{itemize}____ AI Alignment: A Comprehensive Survey
% \begin{itemize}
% \item Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback
% \end{itemize}____ Some evaluators have harmful biases and opinions. (Good story telling point)
% \begin{itemize}
% \item Individual human evaluators can poison data. (Good story telling point, may not be what we are doing though)
% \end{itemize}____ RLHF from Heterogeneous Feedback via Personalization and Preference Aggregation
% \begin{itemize}
% \item Aggregate individual reward functions into a single reward function for the LLM
% \item Humans provide preferences as probability vectors
% \item Adapt VCG to design a DSIC welfare-maximizing mechanism to collect human preferences.
% \end{itemize}