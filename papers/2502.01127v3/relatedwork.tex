\section{Related Work}
%\jerry{This section should be organized by the main points we want to claim. what are our claims (that our paper is new in those aspects)?  multiple papers then go under each claim.}

Our work provides a game theoretic model of the numerical example and informal theorem in section $5$ of ~\cite{park2024rlhf}, in particular, we also assume strategic data providers (which we call influencers) to large language models (which we call receivers), and our model leads to results consistent with their example where the data providers untruthfully report their opinions. In addition, we prove the existence of pure strategy Nash equilibria of this class of games, and we show the property that almost all influencers maximally exaggerate their preferences in every equilibrium. Our work is also closely related to ~\cite{hao2024online}, which models the interaction between multiple influencers by a dynamic Bayesian game. They described the phenomenon of strategic extreme exaggeration, which is also discussed in ~\cite{sun2024mechanism}, ~\cite{soumalias2024truthful}, ~\cite{conitzer2024social}, and ~\cite{roughgarden2017online} for various applications, but they do not explicitly compute the equilibria of the original game or quantify the amount of exaggeration. In comparison, we use a static game with known influencer types and we are able to provide better characterizations of the set of equilibria of the game.

Value alignment aims to make language models produce outputs that are more aligned with human values. Existing training frameworks tailored for this purpose, such as ~\cite{ouyang2022training} and ~\cite{rafailov2024direct}, collect preference data from humans and train a large language model to follow users' intent. However, research in this direction mostly focuses on the algorithmic aspects of value alignment and does not emphasize the heterogeneity of human values. There has been work that studies how to make LLMs align with diverse preferences of different demographic groups ~\cite{bakker2022fine, chen2024pal}, but they do not have a rigorous game theoretic foundation that characterizes strategic behaviors of preference data providers. There is also recent work by ~\cite{munos2023nash}, ~\cite{swamy2024minimaximalist}, and ~\cite{rosset2024direct} on learning a pairwise or general preference model, where they used the term Nash learning or Direct Nash Optimization. The players in their zero-sum games are not strategic data providers, and they use the Nash equilibrium solution concept mainly as an optimization technique to solve their minimax problem.

%\jerry{could you please take a brief look at the problem setting of "Bayesian persuasion", identify key literatures, and think how we can relate to and differentiate from that line of work?}

% ~\cite{ouyang2022training} Training language models to follow instructions with human feedback
% \begin{itemize}
% \item Using RLHF for value alignment
% \end{itemize}~\cite{rafailov2024direct} Direct Preference Optimization: Your Language Model is Secretly a Reward Model
% \begin{itemize}
% \item Introducing DPO
% \item Different from RLHF, no explicit reward model. Based on a classification task (preference).
% \end{itemize}~\cite{ji2023ai} AI Alignment: A Comprehensive Survey
% \begin{itemize}
% \item Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback
% \end{itemize}~\cite{casper2023open} Some evaluators have harmful biases and opinions. (Good story telling point)
% \begin{itemize}
% \item Individual human evaluators can poison data. (Good story telling point, may not be what we are doing though)
% \end{itemize}~\cite{park2024rlhf} RLHF from Heterogeneous Feedback via Personalization and Preference Aggregation
% \begin{itemize}
% \item Aggregate individual reward functions into a single reward function for the LLM
% \item Humans provide preferences as probability vectors
% \item Adapt VCG to design a DSIC welfare-maximizing mechanism to collect human preferences.
% \end{itemize}