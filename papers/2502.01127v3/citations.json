[
  {
    "index": 0,
    "papers": [
      {
        "key": "park2024rlhf",
        "author": "Park, Chanwoo and Liu, Mingyang and Kong, Dingwen and Zhang, Kaiqing and Ozdaglar, Asuman E",
        "title": "Rlhf from heterogeneous feedback via personalization and preference aggregation"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "hao2024online",
        "author": "Hao, Shugang and Duan, Lingjie",
        "title": "Online Learning from Strategic Human Feedback in LLM Fine-Tuning"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "sun2024mechanism",
        "author": "Sun, Haoran and Chen, Yurong and Wang, Siwei and Chen, Wei and Deng, Xiaotie",
        "title": "Mechanism Design for LLM Fine-tuning with Multiple Reward Models"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "soumalias2024truthful",
        "author": "Soumalias, Ermis and Curry, Michael J and Seuken, Sven",
        "title": "Truthful Aggregation of LLMs with an Application to Online Advertising"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "conitzer2024social",
        "author": "Conitzer, Vincent and Freedman, Rachel and Heitzig, Jobst and Holliday, Wesley H and Jacobs, Bob M and Lambert, Nathan and Moss{\\'e}, Milan and Pacuit, Eric and Russell, Stuart and Schoelkopf, Hailey and others",
        "title": "Social Choice Should Guide AI Alignment in Dealing with Diverse Human Feedback"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "roughgarden2017online",
        "author": "Roughgarden, Tim and Schrijvers, Okke",
        "title": "Online prediction with selfish experts"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "ouyang2022training",
        "author": "Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others",
        "title": "Training language models to follow instructions with human feedback"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "rafailov2024direct",
        "author": "Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea",
        "title": "Direct preference optimization: Your language model is secretly a reward model"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "bakker2022fine",
        "author": "Bakker, Michiel and Chadwick, Martin and Sheahan, Hannah and Tessler, Michael and Campbell-Gillingham, Lucy and Balaguer, Jan and McAleese, Nat and Glaese, Amelia and Aslanides, John and Botvinick, Matt and others",
        "title": "Fine-tuning language models to find agreement among humans with diverse preferences"
      },
      {
        "key": "chen2024pal",
        "author": "Chen, Daiwei and Chen, Yi and Rege, Aniket and Vinayak, Ramya Vinayak",
        "title": "PAL: Pluralistic Alignment Framework for Learning from Heterogeneous Preferences"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "munos2023nash",
        "author": "Munos, R{\\'e}mi and Valko, Michal and Calandriello, Daniele and Azar, Mohammad Gheshlaghi and Rowland, Mark and Guo, Zhaohan Daniel and Tang, Yunhao and Geist, Matthieu and Mesnard, Thomas and Michi, Andrea and others",
        "title": "Nash learning from human feedback"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "swamy2024minimaximalist",
        "author": "Swamy, Gokul and Dann, Christoph and Kidambi, Rahul and Wu, Zhiwei Steven and Agarwal, Alekh",
        "title": "A minimaximalist approach to reinforcement learning from human feedback"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "rosset2024direct",
        "author": "Rosset, Corby and Cheng, Ching-An and Mitra, Arindam and Santacroce, Michael and Awadallah, Ahmed and Xie, Tengyang",
        "title": "Direct nash optimization: Teaching language models to self-improve with general preferences"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "ouyang2022training",
        "author": "Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others",
        "title": "Training language models to follow instructions with human feedback"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "rafailov2024direct",
        "author": "Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea",
        "title": "Direct preference optimization: Your language model is secretly a reward model"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "ji2023ai",
        "author": "Ji, Jiaming and Qiu, Tianyi and Chen, Boyuan and Zhang, Borong and Lou, Hantao and Wang, Kaile and Duan, Yawen and He, Zhonghao and Zhou, Jiayi and Zhang, Zhaowei and others",
        "title": "Ai alignment: A comprehensive survey"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "casper2023open",
        "author": "Casper, Stephen and Davies, Xander and Shi, Claudia and Gilbert, Thomas Krendl and Scheurer, J{\\'e}r{\\'e}my and Rando, Javier and Freedman, Rachel and Korbak, Tomasz and Lindner, David and Freire, Pedro and others",
        "title": "Open problems and fundamental limitations of reinforcement learning from human feedback"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "park2024rlhf",
        "author": "Park, Chanwoo and Liu, Mingyang and Kong, Dingwen and Zhang, Kaiqing and Ozdaglar, Asuman E",
        "title": "Rlhf from heterogeneous feedback via personalization and preference aggregation"
      }
    ]
  }
]