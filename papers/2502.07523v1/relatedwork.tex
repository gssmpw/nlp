\section{Related work}
\RL{} has demonstrated remarkable success across various domains, yet sample efficiency remains a significant challenge, especially in real-world applications where data collection is expensive or impractical. Various approaches have been explored to address this issue, including model-based RL, \UTD{} ratio scaling, and architectural modifications.

\paragraph{Model-based \RL{}} methods enhance sample efficiency by constructing predictive models of the environment to reduce reliance on real data collection \cite{sutton1990dynaQ,janner2019mbpo,feinberg2018mve,heess2015svg}.
However, such methods introduce additional complexity, computational overhead, and potential biases due to model inaccuracies.

\paragraph{Update-to-data ratio scaling.}
Model-free \RL{} methods, including those utilizing higher \UTD{} ratios, focus on increasing the number of gradient updates per collected sample to maximize learning from available data.
High \UTD{} training introduces several challenges, such as overfitting to early training data, a phenomenon known as primacy bias~\citep{nikishin2022primacy}.
This can be counteracted by periodically resetting the network parameters~\citep{nikishin2022primacy,doro2022replaybarrier}.
However, network resets introduce abrupt performance drops.
Alternative approaches use techniques such as Q-function ensembles~\citep{chen2021redq,hiraoka2021droq} and architectural changes~\citep{nauman2024bigger}.

\paragraph{Normalization techniques in \RL{}.}
Normalization techniques have long been recognized for their impact on neural network training. \LN{}~\citep{ba2016layernorm} and other architectural modifications have been used to stabilize learning in \RL{}~\citep{hiraoka2021droq,nauman2024bigger}.
Yet \BN{} has only recently been successfully applied in this context~\citep{bhatt2024crossq}, challenging previous findings, where \BN{} in critics caused training to fail~\citep{hiraoka2021droq}.
\WN{} has been shown to keep \ELR{}s stable and prevent loss of plasticity~\cite{lyle2024normalization}, when combined with \LN{}, making it a promising candidate for integration into existing \RL{} frameworks.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/aggr_ablation_bar.pdf}
    \caption{\textit{Hyperparameter ablations.}
    We ablate CrossQ + \WN{} with respect to the \WN{}, target networks and different \UTD.
    }
    \label{fig:ablations}
\end{figure}