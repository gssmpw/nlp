
\documentclass{article}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs}
\usepackage{wrapfig}

\usepackage{hyperref}


\newcommand{\theHalgorithm}{\arabic{algorithm}}


\usepackage[preprint]{icml2025}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

\usepackage[capitalize,noabbrev]{cleveref}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\usepackage{graphicx}
\usepackage{duckuments}
\usepackage{tabularx}

\usepackage{autonum}

\newcommand{\RL}{\textsc{rl}}
\newcommand{\ELR}{\textsc{elr}}
\newcommand{\WN}{\textsc{wn}}
\newcommand{\DMC}{\textsc{dmc}}
\newcommand{\UTD}{\textsc{utd}}
\newcommand{\BN}{\textsc{bn}}
\newcommand{\LN}{\textsc{ln}}
\newcommand{\MDP}{\textsc{mdp}}
\newcommand{\IQM}{\textsc{iqm}}
\newcommand{\CI}{\textsc{ci}}

\newcommand{\SAC}{\textsc{sac}}
\newcommand{\BRO}{\textsc{bro}}
\newcommand{\SRSAC}{\textsc{sr-sac}}


\newcommand{\mujoco}{\texttt{MuJoCo}}

\usepackage{bm}
\newcommand{\vs}{\bm{s}}
\newcommand{\va}{\bm{a}}
\newcommand{\E}{\mathop{\mathbb{E}}}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\newcommand{\TODO}[1]{\textcolor{black}{}}
\newcommand{\FLO}[1]{\textcolor{black}{#1}}
\newcommand{\DAN}[1]{\textcolor{black}{#1}}
\newcommand{\JOAO}[1]{\textcolor{black}{#1}}
\newcommand{\CRIS}[1]{\textcolor{black}{#1}}

\usepackage[textsize=tiny]{todonotes}


\icmltitlerunning{Scaling Off-Policy Reinforcement Learning with Batch and Weight Normalization}

\begin{document}

\twocolumn[
\icmltitle{Scaling Off-Policy Reinforcement Learning\\with Batch and Weight Normalization}



\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Daniel Palenicek}{TuDa,hessianAI}
\icmlauthor{Florian Vogt}{freiburg}
\icmlauthor{Jan Peters}{TuDa,hessianAI,dfki,rig}
\end{icmlauthorlist}

\icmlaffiliation{TuDa}{Intelligent Autonomous Systems Group, Technical University of Darmstadt, Germany}
\icmlaffiliation{hessianAI}{hessian.AI, Darmstadt, Germany}
\icmlaffiliation{dfki}{German Research Center for Artificial Intelligence (DFKI)}
\icmlaffiliation{rig}{Robotics Institute Germany (RIG)}
\icmlaffiliation{freiburg}{Department of Computer Science, University of Freiburg, Germany}

\icmlcorrespondingauthor{Daniel Palenicek}{daniel.palenicek@tu-darmstadt.de}

\icmlkeywords{Reinforcement Learning,
Batch Normalization,
Weight Normalization}

\vskip 0.3in
]



\printAffiliationsAndNotice{}

\begin{abstract}
Reinforcement learning has achieved significant milestones, but sample efficiency remains a bottleneck for real-world applications.
Recently, CrossQ has demonstrated state-of-the-art sample efficiency with a low update-to-data (\UTD{}) ratio of $1$.
In this work, we explore CrossQ's scaling behavior with higher \UTD{} ratios.
We identify challenges in the training dynamics, which are emphasized by higher \UTD{} ratios.
To address these, we integrate weight normalization into the CrossQ framework, a solution that stabilizes training, has been shown to prevent potential loss of plasticity and keeps the effective learning rate constant.
Our proposed approach reliably scales with increasing \UTD{} ratios, achieving competitive performance across $25$ challenging continuous control tasks on the DeepMind Control Suite and Myosuite benchmarks, notably the complex \texttt{dog} and \texttt{humanoid} environments.
This work eliminates the need for drastic interventions, such as network resets, and offers a simple yet robust pathway for improving sample efficiency and scalability in model-free reinforcement learning.
\end{abstract}

\section{Introduction}

Reinforcement Learning (\RL{}) has shown great successes in recent years, achieving breakthroughs in diverse areas. Despite these advancements, a fundamental challenge that remains in \RL{} is enhancing the sample efficiency of algorithms. Indeed, in real-world applications, such as robotics, collecting large amounts of data can be time-consuming, costly, and sometimes impractical due to physical constraints or safety concerns. Thus, addressing this is crucial to make \RL{} methods more accessible and scalable.

Different approaches have been explored to address the problem of low sample efficiency in \RL{}. Model-based \RL{}, on the one hand, attempts to increase sample efficiency by learning dynamic models that reduce the need for collecting real data, a process often expensive and time-consuming~\citep{sutton1990dynaQ,janner2019mbpo,feinberg2018mve,heess2015svg}.
Model-free \RL{} approaches, on the other hand, have explored increasing the number of gradient updates on the available data, referred to as the update-to-data (\UTD{}) ratio~\citep{nikishin2022primacy,doro2022replaybarrier}, modifying network architectures~\citep{bhatt2024crossq}, or both~\citep{chen2021redq,hiraoka2021droq,hussing2024dissecting,nauman2024bigger}.

\begin{figure}[t!]
    \centering
    \includegraphics[width=\linewidth]{figures/aggr_performance.pdf}
    \caption{\textit{CrossQ + \WN{} \UTD{}$\mathbin{=}5$ is competitive to \BRO{} \UTD{}$\mathbin{=}10$.}
    In comparison, our proposed CrossQ + \WN{} is a simple algorithm that does not require extra exploration policies or full parameter resets.
    We present results for $25$ complex continuous control tasks from the \DMC{} and MyoSuite benchmarking suites.
    $1.0$ marks the maximum score achievable on the respective benchmarks (\DMC{} \textit{return} up to $1000$ / Myosuite up to $100\%$ \textit{success rate}).
    }
    \label{fig:aggr_performance}
\end{figure}


In this work, we build upon CrossQ~\citep{bhatt2024crossq}, a recent model-free \RL{} algorithm that recently showed state-of-the-art sample efficiency on the \mujoco{}~\citep{todorov2012mujoco} continuous control benchmarking tasks.
Notably, the authors achieved this by carefully utilizing Batch Normalization (\BN{},~\citet{ioffe2015batch}) within the actor-critic architecture.
A technique previously thought not to work in \RL{}, as famously reported by~\citet{hiraoka2021droq} and others.
The insight that \citet{bhatt2024crossq} offered is that one needs to carefully consider the different state-action distributions within the Bellman equation and handle them correctly to succeed.
This novelty allowed CrossQ at a low \UTD{} of $1$ to outperform the then state-of-the-art algorithms that scaled their \UTD{} ratios up to $20$.
Even though higher \UTD{} ratios are more computationally expensive, they allow for larger policy improvements using the same amount of data.

This naturally raises the question: \textit{How can we extend the sample efficiency benefits of CrossQ and \BN{} to the high \UTD{} training regime?} Which we address in this manuscript.

\paragraph{Contributions.}
In this work, we show that the vanilla CrossQ algorithm is brittle to tune on DeepMind Control~(\DMC{}) and Myosuite environments and can fail to scale reliably with increased compute.
To address these limitations, we propose the addition of weight normalization~(\WN{}), which we show to be a simple yet effective enhancement that stabilizes CrossQ.
We motivate the combined use of \WN{} and \BN{} on insights from the continual learning and loss of plasticity literature and connections to the effective learning rate.
Our experiments show that incorporating \WN{} not only improves the stability of CrossQ but also allows us to scale its~\UTD{}, thereby significantly enhancing sample efficiency.

\section{Preliminaries}
This section briefly outlines the required background knowledge for this paper.

\paragraph{Reinforcement learning.}
A Markov Decision Process (\MDP{})~\citep{puterman2014mdp} is a tuple
$\mathcal{M} = \langle\mathcal{S}, \mathcal{A}, \mathcal{P}, r, \mu_0, \gamma\rangle$,
with state space $\mathcal{S} \subseteq \mathbb{R}^n$,
action space $\mathcal{A} \subseteq \mathbb{R}^m$,
transition probability $\mathcal{P}: \mathcal{S}\times\mathcal{A}\rightarrow{}\Delta(\mathcal{S})$,
the reward function $r:\mathcal{S}\times\mathcal{A}\rightarrow{}\mathbb{R}$,
initial state distribution $\mu_0$
and discount factor $\gamma$.
We define the \RL{} problem according to \citet{Sutton1998}.
A policy $\pi:\mathcal{S}\rightarrow{}\Delta(\mathcal{A})$ is a behavior plan, which maps a state $\vs$ to a distribution over actions $\va$.
The discounted cumulative return is defined as
\begin{equation}
    \textstyle \mathcal{R}(\vs,\va) = \sum_{t=0}^{\infty}\gamma^{t} r(\vs_t, \va_t),
\end{equation}
where $\vs_0=\vs$ and $\va_0=\va$.
Further, $\vs_{t+1}\sim\mathcal{P}(\:\cdot\:|\vs_t,\va_t)$
and $\va_t\sim\pi(\:\cdot\:|\vs_t)$.
The Q-function of a policy $\pi$ is the expected discounted return
$
\textstyle Q^\pi(\vs,\va) = \mathbb{E}_{\pi,\mathcal{P}} [\mathcal{R}(\vs,\va)].
$

The goal of an \RL{} agent is to find an optimal policy $\pi^*$ that maximizes the expected return from the initial state distribution
\begin{equation}
    \pi^* = \textstyle\argmax_{\pi} \mathbb{E}_{\vs \sim \mu_0} \left[ Q^{\pi} (\vs, \va) \right].
\end{equation}




Soft Actor-Critic (\SAC{},~\citet{haarnoja2018sac}) addresses this optimization problem by jointly learning neural network representations for the Q-function and the policy.
The policy network is optimized to maximize the Q-values, while the Q-function is optimized to minimize the Bellmann error
{
    \small
    \begin{equation}
        \mathcal{L}
        =
        \E_{
            \mathcal{D}
        }
        \left[
            \frac{1}{2} \left(
            Q_\theta(\vs_t, \va_t)
            -
            \left( r(\vs_t, \va_t)
            + \gamma \E_{
                \mathcal{P}
            } \left[
                V (\vs_{t+1})
            \right] \right) \right)^2
        \right],
    \end{equation}
}
where the value function is computed by taking an expectation over the learned Q function
{
    \small
    \begin{equation}
        V (\vs_{t+1})
        =
        \textstyle\E_{
            \mathcal{P}, \pi_\theta
        } \left[
            Q_{\bar{\theta}} (\vs_{t+1}, \va_{t+1})
        \right].
        \label{eq:value}
    \end{equation}
}
To stabilize the Q-function learning, \citet{haarnoja2018sac} found it necessary to use a target Q-network in the computation of the value function instead of the regular Q-network.
The target Q-network is structurally equal to the regular Q-network, and its parameters $\bar{\theta}$ are obtained via Polyak Averaging over the learned parameters $\theta$.
While this scheme ensures stability during training by explicitly delaying value function updates, it also arguably slows down online learning~\cite{plappert2018multigoalreinforcementlearningchallenging, 2019kimremovingtheneedfortargetnetwork, morales2020grokking}.

Instead of relying on target networks, CrossQ~\cite{bhatt2024crossq} addresses training stability issues by introducing Batch Normalization (\BN{},~\citet{ioffe2015batch}) in its Q-function and achieves substantial improvements in sample and computational efficiency over SAC.
A central challenge when using \BN{} in Q networks is distribution mismatch:
during training, the Q-function is optimized with samples $\vs_t, \va_t$ from the replay buffer.
However, when the Q-function is evaluated to compute the target values (\cref{eq:value}), it receives actions sampled from the current policy $\va_{t + 1} \sim \pi_\theta(\;\cdot\;|\vs_{t + 1})$.
Those samples have no guarantee of lying within the training distribution of the Q-function. \BN{} is known to struggle with out-of-distribution samples, as such, training can become unstable if the distribution mismatch is not correctly accounted for~\citep{bhatt2024crossq}.
To deal with this issue, CrossQ removes the separate target Q-function and evaluates both Q values during the critic update in a single forward pass, which causes the \BN{} layers to compute shared statistics over the samples from the replay buffer and the current policy.
This scheme effectively tackles distribution mismatch problems, ensuring that all inputs and intermediate activations are effectively forced to lie within the training distribution.


\paragraph{Normalization techniques in \RL{}.}
Normalization techniques are widely recognized for improving the training of neural networks, as they generally accelerate training and improve generalization~\citep{huang2020normalizationtechniquestrainingdnns}.
There are many ways of introducing different types of normalizations into the \RL{} framework.
Most commonly, authors have used Layer Normalization~(\LN{}) within the network architectures to stabilize training~\citep{hiraoka2021droq,lyle2024normalization}.
Recently, CrossQ has been the first algorithm to successfully use \BN{} layers in \RL{}~\citep{bhatt2024crossq}.
The addition of \BN{} leads to substantial gains in sample efficiency.
In contrast to \LN{}, however, one needs to carefully consider the different state-action distributions within the critic loss when integrating \BN{}.
In a different line of work, \citet{hussing2024dissecting} proposed the integration of unit ball normalization and projected the output features of the penultimate layer onto the unit ball in order to reduce Q-function overestimation.















\paragraph{Increasing update-to-data ratios.}
Although scaling up the \UTD{} ratio is an intuitive approach to increase the sample efficiency, in practice, it comes with several challenges.
\citet{nikishin2022primacy} demonstrated that overfitting on early training data can inhibit the agent from learning anything later in the training. The authors dub this phenomenon the primacy bias.
To address the primacy bias, they suggest to periodically reset the network parameters while retraining the replay buffer.
Many works that followed have adapted this intervention~\citep{doro2022replaybarrier,nauman2024bigger}.
While often effective, regularly resetting is a very drastic intervention and by design induces regular drops in performance.
Since the agent has to start learning from scratch repeatedly, it is also not very computing efficient.
Finally, the exact reasons why parameter resets work well in practice are not yet well understood~\citep{li2023efficientdeeprl}.
Instead of resetting there have also been other types of regularization that allowed practitioners to train stably with high \UTD{} ratios.
\citet{janner2019mbpo} generate additional modeled data, by virtually increasing the \UTD{}. In \textsc{redq}, \citet{chen2021redq} leverage ensembles of Q-functions, while \citet{hiraoka2021droq} use dropout and \LN{} to effectively scale to higher \UTD{} ratios.



\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/q_bias.pdf}
    \caption{\textit{Q-bias and weight norms.}
    CrossQ critic weight norms increase significantly with increasing \UTD{} ratios.
    }
    \label{fig:q-bias}
\end{figure}

\section{CrossQ fails to scale up stably}

\citet{bhatt2024crossq} demonstrated CrossQ's state-of-the-art sample efficiency on the \mujoco{} task suite~\citep{todorov2012mujoco}, while at the same time also being very computationally efficient.
However, on the more extensive \DMC{} and Myosuite task suites, we find that CrossQ requires tuning.
We further find that it works on some, but not all, environments stably and reliably.

\paragraph{Mixed performance of CrossQ.}
\Cref{fig:q-bias} shows CrossQ training performance on a subset of \DMC{} tasks. Namely, the \texttt{dog-stand}, \texttt{dog-trot} and \texttt{humanoid-walk}, selected for their varying difficulty levels to demonstrate a wide range of behaviors, including both successful learning and challenges encountered during training. The figure compares a \SAC{} baseline with standard hyperparameters against tuned CrossQ agents with \UTD{} ratios of $1$ and $5$, where the hyperparameters were identified through a grid search over learning rates and network sizes, as detailed in \Cref{tab:hyperparameters_all}.
The first row of the figure shows the \IQM{} training performance and $95\%$ confidence intervals for each agent across $10$ seeds.
Here, we identify three different training behaviors.
On \texttt{dog-stand} CrossQ trains stably at \UTD{}$=1$, but increasing the \UTD{} to $5$ introduces instabilities and decreases performance.
On \texttt{dog-trot}, both \UTD{} ratios perform very similarly.
Finally,  on \texttt{humanoid-walk}, \UTD{}$\mathbin{=}5$ outperforms \UTD{}$\mathbin{=}1$, although it merely manages to catch up to the \SAC{} baseline in this case.
Overall, for all CrossQ runs we notice very large confidence intervals.

\paragraph{Q-function bias analysis.}
The second row in~\cref{fig:q-bias} shows the standard deviation of the normalized Q-function bias.
This bias measures how much the Q-function is overestimating or underestimating the true expected return of the current policy.

To compute the normalized Q-function bias, we follow the protocol of~\citet{chen2021redq}.
We gather $5$ trajectories in the environment using the current policy and use each trajectory's first $350$ state-action pairs to calculate the bias. For this, we compare the cumulative discounted rewards for each state-action pair with its Q-value predicted by the Q-function. This bias is then normalized using the cumulative discounted rewards.
The mean over these normalized Q-function biases measures the expected bias.
Even if the mean is high, as long as the bias is consistent, the selected actions of the policy do not change and, therefore, it is not a problem~\cite {vanhasselt2015deepreinforcementlearningdouble}.
If the standard deviation is high, the change in bias is high, which might hinder learning.
Thus, following the work of~\citet{chen2021redq, bhatt2024crossq}, we focus on the standard deviation.

We find large fluctuations for the Q-function bias standard deviation with all three agents across environments.
And even on \texttt{dog-stand}, where CrossQ \UTD{}$\mathbin{=}5$ does not learn reliably, it maintains a small Q-function bias standard deviation.
From these mixed results, we conclude that we cannot directly link the standard deviation of the Q-function bias to the learning performance.
While this does not mean that for larger \UTD{} ratios, the Q-bias would not explode.
Rather, it means that, in our case, the Q-bias is not at fault, and the root cause for unstable training lies elsewhere.

\paragraph{Growing network parameter norms.}
The third row of \Cref{fig:q-bias} displays the sum over the L2 norms of the dense layers in the critic network.
This includes three dense layers, each with a hidden dimension of $512$.

All three baselines exhibit growing network weights over the course of training.
We find that the effect is particularly pronounced for CrossQ with increasing \UTD{} ratios.
We further find that in the second training phase, the spread of network weight norms increases for the \texttt{dog} runs with large confidence intervals.
This is visualized by the large spread of the shaded areas, which show the $95\%$ inter percentile ranges for the weight norms.

Growing network weights have been linked to a loss of plasticity, a phenomenon where networks become increasingly resistant to parameter update, which can lead to premature convergence~\citep{elsayed2024weightclipping}. Additionally, the growing magnitudes pose a challenge for optimization, connected to the issue of growing activations, which has recently been analyzed by \citet{hussing2024dissecting}.
Further, growing network weights decrease the effective learning rate when the networks contain normalization layers~\citep{vanhasselt2019use,lyle2024normalization}.

In summary, the scaling results for vanilla CrossQ are mixed.
While increasing \UTD{} ratios is known to yield increased sample efficiency, if careful regularization is used~\citep{janner2019mbpo,chen2021redq,nikishin2022primacy}, CrossQ alone with \BN{} cannot benefit from it.
We notice that with increasing \UTD{} ratios, CrossQ's weight layer norms grow significantly faster and overall larger.
This observation motivates us to further study the weight norms in CrossQ to increase \UTD{} ratios.




\section{Combining batch normalization and weight normalization for scaling up}

Inspired by the combined insights of \citet{vanhasselt2019use} and \citet{lyle2024normalization}, we propose to integrate CrossQ with Weight Normalization (\WN{}) as a means of counteracting the rapid growth of weight norms we observe with increasing update-to-data (\UTD{}) ratios.

Our approach is based on the following reasoning: Due to the use of \BN{} in CrossQ, the critic network exhibits scale invariance, as previously noted by~\citet{van2017l2}.

\begin{theorem}[\citet{van2017l2}]
Let $f(\bm{X};\bm{w}, b, \gamma, \beta)$ be a function, with inputs $\bm{X}$ and parameters $\bm{w}$ and $\bm{b}$ and $\gamma$ and $\beta$ batch normalization parameters.
When $f$ is normalized with batch normalization, $f$ becomes scale-invariant with respect to its parameters, i.e.,
\begin{equation}
    \textstyle f(\bm{X};c\bm{w},cb,\gamma,\beta) =  f(\bm{X};\bm{w},b,\gamma,\beta),
\end{equation}
with scaling factor $c>0$.

Proof. \cref{proof:scale_inv}
\label{theorem:scale_inv}
\end{theorem}

This property allows us to introduce \WN{} as a mechanism to regulate the growth of weight norms in CrossQ without affecting the critic’s outputs. Further, it can be shown, that for such a scale invariant function, the gradient scales inversely proportionally to the scaling factor $c>0$.

\begin{theorem}[\citet{van2017l2}]
Let $f(\bm{X};\bm{w},b,\gamma,\beta)$ be a scale-invariant function.
Then, the gradients of $f$ scale inversely proportional to the scaling factor $c\in\mathbb{R}$ of its parameters $\bm{w}$,
\begin{equation}
    \textstyle \nabla f(\bm{X};c\bm{w},cb,\gamma,\beta) =  \nabla f(\bm{X};\bm{w},b,\gamma,\beta) / c.
\end{equation}
Proof. \cref{proof:inves_grads}
\label{theorem:inv_grad}
\end{theorem}

Recently, \citet{lyle2024normalization} demonstrated that the combination of \LN{} and \WN{} can help mitigate loss of plasticity.
Since the gradient scale is inversely proportional to $c$, keeping norms constant helps to maintain a stable effective learning rate~(\ELR{},\citet{vanhasselt2019use}), further enhancing training stability.

We conjecture that maintaining a stable \ELR{} could also be beneficial when increasing the \UTD{} ratios in continuous control \RL{}.
As the \UTD{} ratio increases, the networks are updated more frequently with each environment interaction.
Empirically, we find that the network norms tend to grow quicker with increased \UTD{} ratios~(\Cref{fig:q-bias}), which in turn decreases the \ELR{} even quicker and could be the case for instabilities and low training performance.
As such, we empirically investigate the effectiveness of combining CrossQ with \WN{} with increasing \UTD{} ratios.

\paragraph{Implementation details.}
We apply \WN{} to the first two linear layers, ensuring that their weights remain unit norm after each gradient step by projecting them onto the unit ball, similar to~\citet{lyle2024normalization}.
Since this constraint effectively stabilizes the \ELR{} in these layers, we found it beneficial to slightly reduce the learning rate from $1$e$-3$ to $1$e$-4$.
While we could employ a learning rate schedule~\citep{lyle2024normalization} we did not investigate this here.
Additionally, we impose weight decay on all parameters that remain unbounded---specifically the final dense output layer.
In practice, we use AdamW~\cite{loshchilov2017adamw} with a decay of $0$ (which falls back to vanilla Adam~\citep{kingma2014adam}) for the normalized intermediate dense layers and $1$e$-2$ otherwise.
We chose a maximum \UTD{} of $5$ for our experiments due to our computational budget and good strong in sample efficiency.








\paragraph{Target networks.}
CrossQ famously removed the target networks from the actor-critic framework and showed that using \BN{} training remains stable even without them~\citep{bhatt2024crossq}.
While we find this to be true in many cases, we find that especially in \DMC{}, the re-integration of target networks can help stabilize training overall~(see~\Cref{sec:ablations}).
However, not surprisingly, we find that the integration of target networks with \BN{} requires careful consideration of the different state-action distributions between the $\vs,\va$ and $\vs',\va'\sim\pi(\vs')$ exactly as proposed by~\citet{bhatt2024crossq}.
To satisfy this, we keep the joined forward pass through both the critic network as well as the target critic network.
We evaluate both networks in \textit{training mode}, i.e., they calculate the joined state-action batch statistics on the current batches.
As is common, we use Polyak-averaging with a $\tau=0.005$ from the critic network to the target network.

\section{Experiments}
To evaluate the effectiveness of our proposed CrossQ + \WN{} method, we conduct a comprehensive set of experiments on the DeepMind Control Suite~\citep{tassa2018deepmind} and MyoSuite~\citep{caggiano2022myosuite} benchmarks.
Our primary goal is to investigate the scalability of CrossQ + \WN{} with increasing \UTD{} ratios and to assess the stabilizing effects of combining CrossQ with \WN{}.
We compare our approach to several baselines, including the recent \BRO{}~\citep{nauman2024bigger}, CrossQ~\citep{bhatt2024crossq} and \SRSAC{}~\citep{doro2022replaybarrier} a version of \SAC~\citep{haarnoja2018sac} with high \UTD{} ratios and network resets.

\subsection{Experimental setup}

Our implementation is based on the \SAC{} implementation of \texttt{jaxrl} codebase~\citep{jaxrl}.
We implement CrossQ following the author's original codebase and add the architectural modifications introduced by~\citep{bhatt2024crossq}, incorporating batch normalization in the actor and critic networks.
We extend this approach by introducing \WN{} to regulate the growth of weight norms and prevent loss of plasticity and add target networks.
We perform a grid search to focus on learning rate selection and layer width.

We evaluate $25$ diverse continuous control tasks, $15$ from \DMC{} and $10$ from MyoSuite.
These tasks vary significantly in complexity, requiring different levels of fine motor control and policy adaptation with high dimensional state spaces up to $\mathbb{R}^{223}$.

Each experiment is run for 1 million environment steps and across $10$ random seeds to ensure statistical robustness.
We evaluate agents every $25,000$ environment steps for $5$ trajectories.
As proposed by~\citet{agarwal2021iqm}, we report the interquartile mean (\IQM) and $95\%$ stratified bootstrap confidence intervals (\CI{}s) of the return, if not otherwise stated.

For the \BRO{} baseline results, for computational reasons, we take the official evaluation data the authors provide. The official \BRO{} codebase is also based on \texttt{jaxrl}, and the authors followed the same evaluation protocol, making it a fair comparison.









\begin{figure*}[ht!]
    \centering
    \includegraphics[width=\linewidth]{figures/all_envs.pdf}
    \caption{\textit{CrossQ \WN{} + \UTD{}$\mathbin{=}5$ against baselines.}
    We compare our proposed CrossQ + \WN{} \UTD{}$\mathbin{=}5$ against two baselines, \BRO{}~\citep{nauman2024bigger} and \SRSAC{} \UTD{}$\mathbin{=}32$.
    Results are reported on all $15$ \DMC{} and $10$ Myosuite tasks.
    We plot the \IQM{} and $95\%$ \CI{}s over $10$ random random seeds.
    Our proposed approach proves competitive to \BRO{} and outperforms the CrossQ baseline.
    We want to note that our approach achieves this performance without requiring any parameter resetting or additional exploration policies.
    }
    \label{fig:crossQ_wn_comparison}
\end{figure*}

\subsection{Weight normalization allows CrossQ to scale effectively}

We provide empirical evidence for our hypothesis that controlling the weight norm and, thereby, the \ELR{} can stabilize training.
We show that through the addition of \WN{}, CrossQ + \WN{} shows stable training and can stably scale with increasing \UTD{} ratios.

\Cref{fig:crossQ_wn_comparison} shows per environment results of our experiments encompassing all $25$ tasks evaluated across $10$ seeds each. Based on that,
\Cref{fig:aggr_performance} shows aggregated performance over all environments from~\Cref{fig:crossQ_wn_comparison} per task suite, with a separate aggregation for the most complex \texttt{dog} and \texttt{humanoid} environments.

These results show that CrossQ + \WN{} \UTD{}$\mathbin{=}5$ is competitive to the \BRO{} baseline on both \DMC{} and Myosuite, especially on the more complex \texttt{dog} and \texttt{humanoid} tasks.
Notably, CrossQ + \WN{} \UTD{}$\mathbin{=}5$ uses only half the \UTD{} of \BRO{} and does not require any parameter resets and no additional exploration policy.
Further, it uses $\sim90\%$ fewer network parameters---\BRO{} reports $\sim5M$, while our proposed CrossQ + \WN{} uses only $\sim600k$ (these numbers vary slightly per environment, depending on the state and action dimensionalities).

In contrast, vanilla CrossQ \UTD{}$\mathbin{=}1$ exhibits much slower learning on most tasks and, in some environments, fails to learn performant policies.
Moreover, the instability of vanilla CrossQ at \UTD{}$\mathbin{=}5$ is particularly notable, as it does not reliably converge across environments.

These findings highlight the necessity of incorporating additional normalization techniques to sustain effective training at higher \UTD{} ratios.
This leads us to conclude that CrossQ benefits from the addition of \WN{}, which results in stable training and scales well with higher \UTD{} ratios.
The resulting algorithm can match or outperform state-of-the-art baselines on the continuous control \DMC{} and Myosuite benchmarks while being much simpler algorithmically.







\subsection{Stable scaling of CrossQ + \WN{} with \UTD{} ratios}
\begin{figure}[t!]
    \centering
    \includegraphics[width=0.75\linewidth]{figures/aggr_utd.pdf}
    \caption{\textit{CrossQ \WN{} \UTD{} scaling behavior.} We plot the \IQM{} return and $95\%$ confidence intervals for different \UTD{} ratios $\in\{1,2,5\}$.
    The results are aggregated over 15 \DMC{} environments and $10$ random seeds each according to~\citet{agarwal2021iqm}.
    The sample efficiency scales reliably with increasing \UTD{} ratios.
    }
    \label{fig:utd-scaling}
\end{figure}
To visualize the stable scaling behavior of CrossQ + \WN{} we ablate across three different \UTD{} ratios $\in\{1,2,5\}$.
\Cref{fig:utd-scaling} shows training curves aggregated over all $15$ \DMC{} tasks.
As expected, CrossQ + \WN{} shows reliable scaling behavior, with the learning curves ordered in increasing order accordance to their respective \UTD{} ratio.



\subsection{Hyperparameter ablation studies}
\label{sec:ablations}

We also ablate the different hyperparameters of CrossQ + \WN{} \UTD{}$\mathbin{=}5$, by changing each one at a time. \Cref{fig:ablations} shows aggregated results of the final performances of each ablation.
We will briefly discuss each ablation individually.

\paragraph{Removing weight normalization.}
Not performing weight normalization results in the biggest drop in performance across all our ablations.
This loss is most drastic on the Myosuite tasks and often results in no meaningful learning.
Showing that, as hypothesized, the inclusion of \WN{} into the CrossQ framework yields great improvements in terms of sample efficiency and training stability, especially for larger \UTD{} ratios.

\paragraph{Update-to-data ratio.}
As expected, decreasing the \UTD{} ratio decreases the performance of CrossQ + \WN{}, as demonstrated in the previous section.
Aggregated over all \DMC{} environments, this effect is the smallest, since this aggregation includes easier environments as well.
Looking at the harder \texttt{dog} and \texttt{humanoid} tasks, as well as Myosuite, the effect is more pronounced.
However,  lower \UTD{}s are already reasonably competitive in overall performance.

\paragraph{Target networks.}
Ablating the target networks shows that on Myosuite, there is no significant difference between using a target network and or no target network.
Results on \DMC{} differ significantly.
There, removing target networks leads to a significant drop in performance, nearly as large as removing weight normalization.
This finding is interesting, as it suggests that CrossQ + \WN{} without target networks is not inherently unstable. But there are situations where the inclusion of target networks is required.
Further investigating the role and necessity of target networks in \RL{} is an interesting direction for future research.




\section{Related work}

\RL{} has demonstrated remarkable success across various domains, yet sample efficiency remains a significant challenge, especially in real-world applications where data collection is expensive or impractical. Various approaches have been explored to address this issue, including model-based RL, \UTD{} ratio scaling, and architectural modifications.

\paragraph{Model-based \RL{}} methods enhance sample efficiency by constructing predictive models of the environment to reduce reliance on real data collection \cite{sutton1990dynaQ,janner2019mbpo,feinberg2018mve,heess2015svg}.
However, such methods introduce additional complexity, computational overhead, and potential biases due to model inaccuracies.

\paragraph{Update-to-data ratio scaling.}
Model-free \RL{} methods, including those utilizing higher \UTD{} ratios, focus on increasing the number of gradient updates per collected sample to maximize learning from available data.
High \UTD{} training introduces several challenges, such as overfitting to early training data, a phenomenon known as primacy bias~\citep{nikishin2022primacy}.
This can be counteracted by periodically resetting the network parameters~\citep{nikishin2022primacy,doro2022replaybarrier}.
However, network resets introduce abrupt performance drops.
Alternative approaches use techniques such as Q-function ensembles~\citep{chen2021redq,hiraoka2021droq} and architectural changes~\citep{nauman2024bigger}.

\paragraph{Normalization techniques in \RL{}.}
Normalization techniques have long been recognized for their impact on neural network training. \LN{}~\citep{ba2016layernorm} and other architectural modifications have been used to stabilize learning in \RL{}~\citep{hiraoka2021droq,nauman2024bigger}.
Yet \BN{} has only recently been successfully applied in this context~\citep{bhatt2024crossq}, challenging previous findings, where \BN{} in critics caused training to fail~\citep{hiraoka2021droq}.
\WN{} has been shown to keep \ELR{}s stable and prevent loss of plasticity~\cite{lyle2024normalization}, when combined with \LN{}, making it a promising candidate for integration into existing \RL{} frameworks.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/aggr_ablation_bar.pdf}
    \caption{\textit{Hyperparameter ablations.}
    We ablate CrossQ + \WN{} with respect to the \WN{}, target networks and different \UTD.
    }
    \label{fig:ablations}
\end{figure}

\section{Limitations \& future work}
In this work, we only consider continuous state-action benchmarking tasks.
While our proposed CrossQ + \WN{} performs competitively on these tasks, its performance on discrete state-action spaces or vision-based tasks remains unexplored.
We plan to investigate this in future work.

\section{Conclusion}
In this work, we have addressed the instability and scalability limitations of CrossQ in \RL{} by integrating \WN{}.
Our empirical results demonstrate that \WN{} effectively stabilizes training and allows CrossQ to scale reliably with higher \UTD{} ratios.
The proposed CrossQ + \WN{} approach achieves competitive or superior performance compared to state-of-the-art baselines across a diverse set of $25$ complex continuous control tasks from the \DMC{} and Myosuite benchmarks.
These tasks include complex and high-dimensional humanoid and dog environments.
This extension preserves simplicity while enhancing robustness and scalability by eliminating the need for drastic interventions such as network resets.



\section*{Acknowledgments}
We wish to thank Joe Watson for helpful discussions and advice during the project.
We would also like to thank Tim Schneider, Cristiana de Farias, João Carvalho and Theo Gruner for proofreading and constructive criticism on the manuscript.
This research was funded by the research cluster “Third Wave of AI”, funded by the excellence program of the Hessian Ministry of Higher Education, Science, Research and the Arts, hessian.AI.

\bibliography{bibliography}

\bibliographystyle{icml2025}


\newpage
\appendix
\onecolumn

\section{Proof Scale Invariance}
\label{proof:scale_inv}
Proof of \Cref{theorem:scale_inv}.

\begin{align}
    f(\bm{X}; c\bm{w},cb, \gamma, \beta) &= \frac{
    g(c\bm{X}\bm{w} + cb) - \mu(g(c\bm{X}\bm{w} + cb))
    }{
        \sigma(g(c\bm{X}\bm{w} + cb))
    } \gamma + \beta \\
    &= \frac{
    cg(\bm{X}\bm{w} + b) - c\mu(g(\bm{X}\bm{w} + b))
    }{
        |c|\sigma(g(\bm{X}\bm{w} + b))
    } \gamma + \beta \\
    &= \frac{
    g(\bm{X}\bm{w} + b) - \mu(g(\bm{X}\bm{w} + b))
    }{
        \sigma(g(\bm{X}\bm{w} + b))
    } \gamma + \beta
    = f(\bm{X}; \bm{w},b, \gamma, \beta)
\end{align}

\section{Proof Inverse Proportional Gradients}
\label{proof:inves_grads}
To show that the gradients scale inversely proportional to the parameter norm, we can first write

\begin{align}
    f(\bm{X}; c\bm{w},cb, \gamma, \beta) &= \frac{
    g(c\bm{X}\bm{w} + cb) - \mu(g(c\bm{X}\bm{w} + cb))
    }{
        \sigma(g(c\bm{X}\bm{w} + cb))
    } \gamma + \beta \\
    &= \frac{
    g(c\bm{X}\bm{w} + cb)
    }{
        \sigma(g(c\bm{X}\bm{w} + cb))
    } \gamma - \frac{\mu(g(c\bm{X}\bm{w} + cb))}{\sigma(g(c\bm{X}\bm{w} + cb))} \gamma +\beta. \\
\end{align}

As the gradient of the weights is not backpropagated through the mean and standard deviation, we have

\begin{align}
    \nabla_w f(\bm{X}; c\bm{w},cb, \gamma, \beta) &= \frac{
    g'(c\bm{X}\bm{w} + cb)X
    }{
        |c| \sigma(g(\bm{X}\bm{w} + b))
    } \gamma. \\
\end{align}

The gradient of the bias can be computed analogously

\begin{align}
    \nabla_b f(\bm{X}; c\bm{w},cb, \gamma, \beta) &= \frac{
    g'(c\bm{X}\bm{w} + cb)
    }{
        |c| \sigma(g(\bm{X}\bm{w} + b))
    } \gamma. \\
\end{align}

\newpage
\section{Hyperparameters}

\Cref{tab:hyperparameters_all} gives an overview of the hyperparameters that were used for each algorithm that was considered in this work.

\begin{table}[h!]
\caption{Hyperparameters}
\label{tab:hyperparameters_all}
\begin{tabular}{llllll}
\textbf{Hyperparameter}                   & \textbf{CrossQ}                                                 & \textbf{CrossQ + \WN{}}                                             & \textbf{\SAC{}}                                                    & \textbf{\SRSAC{}}                                             & \textbf{\BRO{}}                                                                              \\ \hline
Critic learning rate                      & 0.0001                                                          & 0.0001                                                          & 0.0003                                                          & 0.0003                                                          & 0.0003                                                                                    \\
Critic hidden dim                         & 512                                                             & 512                                                             & 256                                                             & 256                                                             & 512                                                                                       \\
Actor learning rate                       & 0.0001                                                          & 0.0001                                                          & 0.0003                                                          & 0.0003                                                          & 0.0003                                                                                    \\
Actor hidden dim                          & 256                                                             & 256                                                             & 256                                                             & 256                                                             & 256                                                                                       \\
Initial temperature                       & 1.0                                                             & 1.0                                                             & 1.0                                                             & 1.0                                                             & 1.0                                                                                       \\
Temperature learning rate                 & 0.0001                                                          & 0.0001                                                          & 0.0003                                                          & 0.0003                                                          & 0.0003                                                                                    \\
Target entropy                            & $\left| A \right|$                                              & $\left| A \right|$                                              & $\left| A \right|$                                              & $\left| A \right|$                                              & $\left| A \right|$                                                                        \\
Target network momentum                   & 0.005                                                           & 0.005                                                           & 0.005                                                           & 0.005                                                           & 0.005                                                                                     \\
UTD                                       & 1,5                                                             & 1,5                                                             & 1                                                               & 32                                                              & 10                                                                                        \\
Number of critics                         & 2                                                               & 2                                                               & 2                                                               & 2                                                               & 1                                                                                         \\
Action repeat                             & 2                                                               & 2                                                               & 2                                                               & 2                                                               & 1                                                                                         \\
Discount                                  & \begin{tabular}[c]{@{}l@{}}0.99 (DMC)\\ 0.95 (Myo)\end{tabular} & \begin{tabular}[c]{@{}l@{}}0.99 (DMC)\\ 0.95 (Myo)\end{tabular} & \begin{tabular}[c]{@{}l@{}}0.99 (DMC)\\ 0.95 (Myo)\end{tabular} & \begin{tabular}[c]{@{}l@{}}0.99 (DMC)\\ 0.95 (Myo)\end{tabular} & \begin{tabular}[c]{@{}l@{}}0.99 (DMC)\\ 0.99 (Myo)\end{tabular}                           \\
Optimizer                                 & Adam                                                            & AdamW                                                           & Adam                                                            & Adam                                                            & AdamW                                                                                     \\
Optimizer momentum ($\beta_1$, $\beta_2$) & (0.9, 0.999)                                                    & (0.9, 0.999)                                                    & (0.9, 0.999)                                                    & (0.9, 0.999)                                                    & (0.9, 0.999)                                                                              \\
Policy delay                              & 3                                                               & 3                                                               & 1                                                               & 1                                                               & 1                                                                                         \\
Warmup transitions                        & 5000                                                            & 5000                                                            & 10000                                                           & 10000                                                           & 10000                                                                                     \\
AdamW weight decay critic                 & 0.0                                                             & 0.01                                                            & 0.0                                                             & 0.0                                                             & 0.0001                                                                                    \\
AdamW weight decay actor                  & 0.0                                                             & 0.01                                                            & 0.0                                                             & 0.0                                                             & 0.0001                                                                                    \\
AdamW weight decay temperature            & 0.0                                                             & 0.0                                                             & 0.0                                                             & 0.0                                                             & 0.0                                                                                       \\
Batch Normalization momentum              & 0.99                                                            & 0.99                                                            & N/A                                                             & N/A                                                             & N/A                                                                                       \\
Reset Interval of networks                & N/A                                                             & N/A                                                             & N/A                                                             & every 80k steps                                                 & \textit{\begin{tabular}[c]{@{}l@{}}at 15k, 50k, 250k,\\ 500k and 750k steps\end{tabular}} \\
Batch Size                                & 256                                                             & 256                                                             & 256                                                             & 256                                                             & 128
\end{tabular}
\end{table}

\newpage



\section{Individual training curves for ablation results}
Here, we provide detailed individual training curves for each ablation experiment conducted in our study. \Cref{fig:individual_ablations} shows experiments with no \WN{}, no target network, CrossQ with \WN{} and \UTD{}=1, and CrossQ with \WN{} and \UTD{}=5.

\begin{figure}[h!]
    \centering
    \includegraphics[width=\linewidth]{figures/all_ablations.pdf}
    \caption{Individual training curves for ablations}
    \label{fig:individual_ablations}
\end{figure}


\end{document}


