@article{ankur2024appearance,
  title   = {Appearance-Based Gaze Estimation with Deep Neural Networks: From Data Collection to Evaluation},
  author  = {Ankur Bhatt and Ko Watanabe and Andreas Dengel and Shoya Ishimaru},
  journal = {International Journal of Activity and Behavior Computing},
  volume  = {2024},
  number  = {1},
  pages   = {1-15},
  year    = {2024},
  doi     = {10.60401/ijabc.9}
}

@article{arakawa2024prism,
  title     = {Prism-q\&a: Step-aware voice assistant on a smartwatch enabled by multimodal procedure tracking and large language models},
  author    = {Arakawa, Riku and Lehman, Jill Fain and Goel, Mayank},
  journal   = {Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
  volume    = {8},
  number    = {4},
  pages     = {1--26},
  year      = {2024},
  publisher = {ACM New York, NY, USA}
}

@inproceedings{berkovsky2019personality,
  author    = {Berkovsky, Shlomo and Taib, Ronnie and Koprinska, Irena and Wang, Eileen and Zeng, Yucheng and Li, Jingjie and Kleitman, Sabina},
  title     = {Detecting Personality Traits Using Eye-Tracking Data},
  year      = {2019},
  isbn      = {9781450359702},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3290605.3300451},
  doi       = {10.1145/3290605.3300451},
  abstract  = {Personality is an established domain of research in psychology, and individual differences in various traits are linked to a variety of real-life outcomes and behaviours. Personality detection is an intricate task that typically requires humans to fill out lengthy questionnaires assessing specific personality traits. The outcomes of this, however, may be unreliable or biased if the respondents do not fully understand or are not willing to honestly answer the questions. To this end, we propose a framework for objective personality detection that leverages humans' physiological responses to external stimuli. We exemplify and evaluate the framework in a case study, where we expose subjects to affective image and video stimuli, and capture their physiological responses using a commercial-grade eye-tracking sensor. These responses are then processed and fed into a classifier capable of accurately predicting a range of personality traits. Our work yields notably high predictive accuracy, suggesting the applicability of the proposed framework for robust personality detection.},
  booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems},
  pages     = {1–12},
  numpages  = {12},
  keywords  = {eye tracking, field study, framework, personality detection},
  location  = {Glasgow, Scotland Uk},
  series    = {CHI '19}
}

@article{bhatt2024self,
  author   = {Bhatt, Ankur and Watanabe, Ko and Santhosh, Jayasankar and Dengel, Andreas and Ishimaru, Shoya},
  journal  = {IEEE Access},
  title    = {Estimating Self-Confidence in Video-Based Learning Using Eye-Tracking and Deep Neural Networks},
  year     = {2024},
  volume   = {12},
  number   = {},
  pages    = {192219-192229},
  keywords = {Gaze tracking;Support vector machines;Reviews;Long short term memory;Data collection;Feature extraction;Estimation;Electroencephalography;Random forests;Radio frequency;Eye-tracking;learning augmentation;self-confidence estimation},
  doi      = {10.1109/ACCESS.2024.3515838}
}

@inproceedings{blanchard2014automated,
  title        = {Automated physiological-based detection of mind wandering during learning},
  author       = {Blanchard, Nathaniel and Bixler, Robert and Joyce, Tera and D’Mello, Sidney},
  booktitle    = {Intelligent Tutoring Systems: 12th International Conference, ITS 2014, Honolulu, HI, USA, June 5-9, 2014. Proceedings 12},
  pages        = {55--60},
  year         = {2014},
  organization = {Springer}
}

@inproceedings{brishtel2018assessing,
  title     = {Assessing cognitive workload on printed and electronic media using eye-tracker and EDA wristband},
  author    = {Brishtel, Iuliia and Ishimaru, Shoya and Augereau, Olivier and Kise, Koichi and Dengel, Andreas},
  booktitle = {Companion Proceedings of the 23rd International Conference on Intelligent User Interfaces},
  pages     = {1--2},
  year      = {2018}
}

@article{brishtel2020mind,
  title     = {Mind wandering in a multimodal reading setting: Behavior analysis \& automatic detection using eye-tracking and an eda sensor},
  author    = {Brishtel, Iuliia and Khan, Anam Ahmad and Schmidt, Thomas and Dingler, Tilman and Ishimaru, Shoya and Dengel, Andreas},
  journal   = {Sensors},
  volume    = {20},
  number    = {9},
  pages     = {2546},
  year      = {2020},
  publisher = {MDPI}
}

@inproceedings{bykowski2018automatic,
  author    = {Bykowski, Adam and Kupi\'{n}ski, Szymon},
  title     = {Automatic mapping of gaze position coordinates of eye-tracking glasses video on a common static reference image},
  year      = {2018},
  isbn      = {9781450357067},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3204493.3208331},
  doi       = {10.1145/3204493.3208331},
  abstract  = {This paper describes a method for automatic semantic gaze mapping from video obtained by eye-tracking glasses to a common reference image. Image feature detection and description algorithms are utilized to find the position of subsequent video frames and map corresponding gaze coordinates on a common reference image. This process allows aggregate experiment results for further experiment analysis and provides an alternative for manual semantic gaze mapping methods.},
  booktitle = {Proceedings of the 2018 ACM Symposium on Eye Tracking Research \& Applications},
  articleno = {84},
  numpages  = {3},
  keywords  = {semantic gaze mapping, feature matching, eye tracking, computer vision},
  location  = {Warsaw, Poland},
  series    = {ETRA '18}
}

@inproceedings{cai2023speakfaster,
  author    = {Cai, Shanqing and Venugopalan, Subhashini and Tomanek, Katrin and Kane, Shaun and Morris, Meredith Ringel and Cave, Richard and Macdonald, Robert and Campbell, Jon and Casey, Blair and Kornman, Emily and Vance, Daniel E and Beavers, Jay},
  title     = {SpeakFaster Observer: Long-Term Instrumentation of Eye-Gaze Typing for Measuring AAC Communication},
  year      = {2023},
  isbn      = {9781450394222},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3544549.3573870},
  doi       = {10.1145/3544549.3573870},
  abstract  = {Accelerating communication for users with severe motor and speech impairments, in particular for eye-gaze-based augmentative and alternative communication (AAC) device users, is a longstanding area of research. However, observation of such users’ communication over extended durations has been limited. This case study presents the real-world experience of developing and field-testing a tool for observing and curating the gaze typing-based communication of an eye-gaze AAC user with amyotrophic lateral sclerosis (ALS). With the intent to observe and develop technology to accelerate eye-gaze typed communication, we designed a tool and a protocol called the SpeakFaster Observer to measure everyday conversational text entry by the gaze-typing user, as well as several consenting conversation partners of the AAC user. We detail the design of the Observer software and data curation protocol, along with considerations for privacy protection. The deployment of the data protocol from November 2021 to April 2022 yielded a rich dataset of gaze-based AAC text entry from everyday life, consisting of 130+ hours of gaze keystrokes and 5,000+ curated speech utterances from the AAC user and the conversation partners. We present the key statistics of the data, including the speed (8.1 ± 3.9 words per minute) and keystroke saving rate (-0.14 ± 0.83) of gaze typing, patterns of utterance repetition and reuse, and the temporal dynamics of conversation turn-taking in gaze-based communication. We share our findings and also open source our data collection tools to further research in this domain.},
  booktitle = {Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems},
  articleno = {397},
  numpages  = {8},
  keywords  = {accessibility, augmentative and alternative communication, context awareness, conversation, gaze typing, text corpus},
  location  = {Hamburg, Germany},
  series    = {CHI EA '23}
}

@article{candini2021physiological,
  title     = {The physiological correlates of interpersonal space},
  author    = {Candini, Michela and Battaglia, Simone and Benassi, Mariagrazia and di Pellegrino, Giuseppe and Frassinetti, Francesca},
  journal   = {Scientific Reports},
  volume    = {11},
  number    = {1},
  pages     = {2611},
  year      = {2021},
  publisher = {Nature Publishing Group UK London}
}

@article{carter2020best,
  title     = {Best practices in eye tracking research},
  author    = {Carter, Benjamin T and Luke, Steven G},
  journal   = {International Journal of Psychophysiology},
  volume    = {155},
  pages     = {49--62},
  year      = {2020},
  publisher = {Elsevier}
}

@inproceedings{cho2024sonohaptics,
  author    = {Cho, Hyunsung and Sendhilnathan, Naveen and Nebeling, Michael and Wang, Tianyi and Padmanabhan, Purnima and Browder, Jonathan and Lindlbauer, David and Jonker, Tanya R. and Todi, Kashyap},
  title     = {SonoHaptics: An Audio-Haptic Cursor for Gaze-Based Object Selection in XR},
  year      = {2024},
  isbn      = {9798400706288},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3654777.3676384},
  doi       = {10.1145/3654777.3676384},
  abstract  = {We introduce SonoHaptics, an audio-haptic cursor for gaze-based 3D object selection. SonoHaptics addresses challenges around providing accurate visual feedback during gaze-based selection in Extended Reality&nbsp;(XR), e.&nbsp;g., lack of world-locked displays in no- or limited-display smart glasses and visual inconsistencies. To enable users to distinguish objects without visual feedback, SonoHaptics employs the concept of cross-modal correspondence in human perception to map visual features of objects (color, size, position, material) to audio-haptic properties (pitch, amplitude, direction, timbre). We contribute data-driven models for determining cross-modal mappings of visual features to audio and haptic features, and a computational approach to automatically generate audio-haptic feedback for objects in the user’s environment. SonoHaptics provides global feedback that is unique to each object in the scene, and local feedback to amplify differences between nearby objects. Our comparative evaluation shows that SonoHaptics enables accurate object identification and selection in a cluttered scene without visual feedback.},
  booktitle = {Proceedings of the 37th Annual ACM Symposium on User Interface Software and Technology},
  articleno = {125},
  numpages  = {19},
  keywords  = {Computational Interaction, Extended Reality, Gaze-based Selection, Haptics, Multimodal Feedback, Sonification},
  location  = {Pittsburgh, PA, USA},
  series    = {UIST '24}
}

@article{coello2021interrelation,
  title     = {The interrelation between peripersonal action space and interpersonal social space: Psychophysiological evidence and clinical implications},
  author    = {Coello, Yann and Cartaud, Alice},
  journal   = {Frontiers in Human Neuroscience},
  volume    = {15},
  pages     = {636124},
  year      = {2021},
  publisher = {Frontiers Media SA}
}

@inproceedings{dembinsky2024eye,
  author    = {Dembinsky, David and Watanabe, Ko and Dengel, Andreas and Ishimaru, Shoya},
  title     = {Eye Movement in a Controlled Dialogue Setting},
  year      = {2024},
  isbn      = {9798400706073},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3649902.3653337},
  doi       = {10.1145/3649902.3653337},
  abstract  = {Designing realistic eye movements for animated avatars poses a challenge, as gaze behavior is predominantly unconscious. Accurately modulating those movements is crucial to avoid the Uncanny Valley. The human gaze exhibits different characteristics in conversations, depending on speaking or listening. Albeit these distinctions are known, data for synthesizing eye movement models suitable for avatars is scarce. This research introduces a novel dataset involving human gaze behavior during remote screen conversations. The data are collected from 19 participants, offering 4 hours of gaze data labeled as Speaking and Listening. Our data analysis substantiates prior knowledge of gaze behavior while providing new insights through higher precision. Furthermore, we demonstrate the dataset’s suitability for machine learning algorithms by training a classifier, achieving 88.1\% binary classification accuracy.},
  booktitle = {Proceedings of the 2024 Symposium on Eye Tracking Research and Applications},
  articleno = {10},
  numpages  = {7},
  keywords  = {Animated Avatars, Data Collection, Eye Movement, Gaze Synthesis},
  location  = {Glasgow, United Kingdom},
  series    = {ETRA '24}
}

@article{dembinsky2024gaze,
  author   = {Dembinsky, David and Watanabe, Ko and Dengel, Andreas and Ishimaru, Shoya},
  journal  = {IEEE Access},
  title    = {Gaze Generation for Avatars Using GANs},
  year     = {2024},
  volume   = {12},
  number   = {},
  pages    = {101536-101548},
  keywords = {Avatars;Oral communication;Generators;Data models;Faces;Training;Software;Avatars;eye gaze;GAN;human-computer-interaction},
  doi      = {10.1109/ACCESS.2024.3430835}
}

@inproceedings{funk2014using,
  author    = {Funk, Markus and Sahami, Alireza and Henze, Niels and Schmidt, Albrecht},
  title     = {Using a touch-sensitive wristband for text entry on smart watches},
  year      = {2014},
  isbn      = {9781450324748},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/2559206.2581143},
  doi       = {10.1145/2559206.2581143},
  abstract  = {The ongoing miniaturization of computing devices enabled smart watches with capabilities almost on par with smart phones. Due to immanent size restrictions smart watches require specifically designed input and output techniques. In particular, entering text is often needed when interacting with computers but the watches' small size excludes common input techniques. In this paper we propose a text entry technique using a touch sensitive wristband. Using the wristband requires no screen space besides displaying the current character and might enable cheaper devices. In an experiment we compare a linear keyboard and a multitap keyboard layout. We show that users type faster and make fewer errors using multitap. We argue that the inexpensive sensors enable the integration in low cost wearable watches that require text entry occasionally.},
  booktitle = {CHI '14 Extended Abstracts on Human Factors in Computing Systems},
  pages     = {2305–2310},
  numpages  = {6},
  keywords  = {wristband, text entry, smart watch, keyboard},
  location  = {Toronto, Ontario, Canada},
  series    = {CHI EA '14}
}

@article{geers2023relationship,
  title     = {The relationship between action, social and multisensory spaces},
  author    = {Geers, Laurie and Coello, Yann},
  journal   = {Scientific Reports},
  volume    = {13},
  number    = {1},
  pages     = {202},
  year      = {2023},
  publisher = {Nature Publishing Group UK London}
}

@inproceedings{kopacsi2024exploring,
  author    = {Kop\'{a}csi, L\'{a}szl\'{o} and Klimenko, Albert and Barz, Michael and Sonntag, Daniel},
  title     = {Exploring Gaze-Based Menu Navigation in Virtual Environments},
  year      = {2024},
  isbn      = {9798400710889},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3677386.3688887},
  doi       = {10.1145/3677386.3688887},
  abstract  = {With the integration of eye tracking technologies in Augmented Reality (AR) and Virtual Reality (VR) headsets, gaze-based interactions have opened up new possibilities for user interface design, including menu navigation. Prior research in gaze-based menu navigation in VR has predominantly focused on pie menus, yet recent studies indicate a user preference for list layouts. However, the comparison of gaze-based interactions on list menus is lacking in the literature. This work aims to fill this gap by exploring the viability of list menus for multi-level gaze-based menu navigation in VR and evaluating the efficiency of various gaze-based interactions, such as dwelling and border-crossing, against traditional controller navigation and multi-modal interaction using gaze and button press.},
  booktitle = {Proceedings of the 2024 ACM Symposium on Spatial User Interaction},
  articleno = {40},
  numpages  = {2},
  keywords  = {Extended Reality (XR), Eye Tracking, Gaze-based Interaction, Menu Navigation},
  location  = {Trier, Germany},
  series    = {SUI '24}
}

@inproceedings{lee2024echowrist,
  author    = {Lee, Chi-Jung and Zhang, Ruidong and Agarwal, Devansh and Yu, Tianhong Catherine and Gunda, Vipin and Lopez, Oliver and Kim, James and Yin, Sicheng and Dong, Boao and Li, Ke and Sakashita, Mose and Guimbretiere, Francois and Zhang, Cheng},
  title     = {EchoWrist: Continuous Hand Pose Tracking and Hand-Object Interaction Recognition Using Low-Power Active Acoustic Sensing On a Wristband},
  year      = {2024},
  isbn      = {9798400703300},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3613904.3642910},
  doi       = {10.1145/3613904.3642910},
  abstract  = {Our hands serve as a fundamental means of interaction with the world around us. Therefore, understanding hand poses and interaction contexts is critical for human-computer interaction (HCI). We present EchoWrist, a low-power wristband that continuously estimates 3D hand poses and recognizes hand-object interactions using active acoustic sensing. EchoWrist is equipped with two speakers emitting inaudible sound waves toward the hand. These sound waves interact with the hand and its surroundings through reflections and diffractions, carrying rich information about the hand’s shape and the objects it interacts with. The information captured by the two microphones goes through a deep learning inference system that recovers hand poses and identifies various everyday hand activities. Results from the two 12-participant user studies show that EchoWrist is effective and efficient at tracking 3D hand poses and recognizing hand-object interactions. Operating at 57.9 mW, EchoWrist can continuously reconstruct 20 3D hand joints with MJEDE of 4.81 mm and recognize 12 naturalistic hand-object interactions with 97.6\% accuracy.},
  booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
  articleno = {403},
  numpages  = {21},
  keywords  = {Acoustic Sensing, Hand Pose, Hand-Object Interaction, Smartwatch, Wearable},
  location  = {Honolulu, HI, USA},
  series    = {CHI '24}
}

@article{lystbaek2022exploring,
  author     = {Lystb\ae{}k, Mathias N. and Pfeuffer, Ken and Gr\o{}nb\ae{}k, Jens Emil Sloth and Gellersen, Hans},
  title      = {Exploring Gaze for Assisting Freehand Selection-based Text Entry in AR},
  year       = {2022},
  issue_date = {May 2022},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {6},
  number     = {ETRA},
  url        = {https://doi.org/10.1145/3530882},
  doi        = {10.1145/3530882},
  abstract   = {With eye-tracking increasingly available in Augmented Reality, we explore how gaze can be used to assist freehand gestural text entry. Here the eyes are often coordinated with manual input across the spatial positions of the keys. Inspired by this, we investigate gaze-assisted selection-based text entry through the concept of spatial alignment of both modalities. Users can enter text by aligning both gaze and manual pointer at each key, as a novel alternative to existing dwell-time or explicit manual triggers. We present a text entry user study comparing two of such alignment techniques to a gaze-only and a manual-only baseline. The results show that one alignment technique reduces physical finger movement by more than half compared to standard in-air finger typing, and is faster and exhibits less perceived eye fatigue than an eyes-only dwell-time technique. We discuss trade-offs between uni and multimodal text entry techniques, pointing to novel ways to integrate eye movements to facilitate virtual text entry.},
  journal    = {Proc. ACM Hum.-Comput. Interact.},
  month      = may,
  articleno  = {141},
  numpages   = {16},
  keywords   = {augmented reality, eye-tracking, gaze interaction, multimodal ui, text entry, virtual keyboard}
}

@article{menghini2019stressing,
  title     = {Stressing the accuracy: Wrist-worn wearable sensor validation over different conditions},
  author    = {Menghini, Luca and Gianfranchi, Evelyn and Cellini, Nicola and Patron, Elisabetta and Tagliabue, Mariaelena and Sarlo, Michela},
  journal   = {Psychophysiology},
  volume    = {56},
  number    = {11},
  pages     = {e13441},
  year      = {2019},
  publisher = {Wiley Online Library}
}

@inproceedings{meteier2023enhancing,
  author    = {Meteier, Quentin and Mugellini, Elena and Angelini, Leonardo and Verdon, Alain Adrian and Senn-Dubey, Catherine and Vasse, Jean-Michel},
  title     = {Enhancing the Metacognition of Nursing Students Using Eye Tracking Glasses},
  year      = {2023},
  isbn      = {9798400701504},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3588015.3590115},
  doi       = {10.1145/3588015.3590115},
  abstract  = {Practical simulation is increasingly used to develop reasoning skills during learning. The analysis of the scene and the correct execution of actions require an awareness of the situation and the activities performed by the student. Eye-tracking feedback (i.e., a video recording of the practical simulation with an overlay of the gaze point) can allow students and teachers to enhance the skills of analysis and execution of the practical activities performed. In this article, we present the implementation of an innovative pedagogical process for nursing students in Switzerland. It involves the use of eye-tracking glasses to improve learning through the enhancement of metacognition after a simulation. The results of a first test session done with 15 undergraduate students are reported.},
  booktitle = {Proceedings of the 2023 Symposium on Eye Tracking Research and Applications},
  articleno = {40},
  numpages  = {2},
  keywords  = {education, eye-tracking, metacognition, nursing, simulation},
  location  = {Tubingen, Germany},
  series    = {ETRA '23}
}

@inproceedings{minakata2019pointing,
  author    = {Minakata, Katsumi and Hansen, John Paulin and MacKenzie, I. Scott and B\ae{}kgaard, Per and Rajanna, Vijay},
  title     = {Pointing by gaze, head, and foot in a head-mounted display},
  year      = {2019},
  isbn      = {9781450367097},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3317956.3318150},
  doi       = {10.1145/3317956.3318150},
  abstract  = {This paper presents a Fitts' law experiment and a clinical case study performed with a head-mounted display (HMD). The experiment compared gaze, foot, and head pointing. With the equipment setup we used, gaze was slower than the other pointing methods, especially in the lower visual field. Throughputs for gaze and foot pointing were lower than mouse and head pointing and their effective target widths were also higher. A follow-up case study included seven participants with movement disorders. Only two of the participants were able to calibrate for gaze tracking but all seven could use head pointing, although with throughput less than one-third of the non-clinical participants.},
  booktitle = {Proceedings of the 11th ACM Symposium on Eye Tracking Research \& Applications},
  articleno = {69},
  numpages  = {9},
  keywords  = {virtual reality, head-mounted displays, head interaction, hand controller, gaze interaction, foot interaction, dwell activation, disability, accessibility, ISO 9241-9, Fitts' law},
  location  = {Denver, Colorado},
  series    = {ETRA '19}
}

@article{nandrino2017perception,
  title     = {Perception of peripersonal and interpersonal space in patients with restrictive-type anorexia},
  author    = {Nandrino, Jean-Louis and Ducro, Claire and Iachini, Tina and Coello, Yann},
  journal   = {European Eating Disorders Review},
  volume    = {25},
  number    = {3},
  pages     = {179--187},
  year      = {2017},
  publisher = {Wiley Online Library}
}

@article{shah2024eyedentify,
  title   = {EyeDentify: A Dataset for Pupil Diameter Estimation based on Webcam Images},
  author  = {Shah, Vijul and Watanabe, Ko and Moser, Brian B and Dengel, Andreas},
  journal = {arXiv preprint arXiv:2407.11204},
  year    = {2024}
}

@article{shah2024webcam,
  title   = {Webcam-based Pupil Diameter Prediction Benefits from Upscaling},
  author  = {Shah, Vijul and Moser, Brian B and Watanabe, Ko and Dengel, Andreas},
  journal = {arXiv preprint arXiv:2408.10397},
  year    = {2024}
}

@inproceedings{shi2024bibliometric,
  title        = {A Bibliometric Analysis of Eye Tracking in User Experience Research},
  author       = {Shi, Yang},
  booktitle    = {International Conference on Human-Computer Interaction},
  pages        = {178--193},
  year         = {2024},
  organization = {Springer}
}

@inproceedings{tanaka2024concentration,
  author    = {Tanaka, Noriyuki and Watanabe, Ko and Ishimaru, Shoya and Dengel, Andreas and Ata, Shingo and Fujimoto, Manato},
  title     = {Concentration Estimation in Online Video Lecture Using Multimodal Sensors},
  year      = {2024},
  isbn      = {9798400710582},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3675094.3677587},
  doi       = {10.1145/3675094.3677587},
  abstract  = {Distance learning is one of the technology-wise challenges in the education field. Remote learning provides the advantage of encouraging anyone to join from worldwide. In order to make education sustainable, understanding students' concentration in remote study is significant. In this study, we evaluate multi-modal sensors for estimating students' concentration levels during online classes. We collect sensor data such as accelerometers, gyroscopes, heart rates, facial orientations, and eye gazes. We conducted experiments with 13 university students in Japan. The results of our study, with an average accuracy rate of 74.4\% for user-dependent cross-validation and 66.3\% for user-independent cross-validation, have significant implications for understanding and improving student engagement in online learning environments. Most interestingly, we found that facial orientations are significant for user-dependent and eye gazes for user-independent classification.},
  booktitle = {Companion of the 2024 on ACM International Joint Conference on Pervasive and Ubiquitous Computing},
  pages     = {71–75},
  numpages  = {5},
  keywords  = {concentration detection, machine learning, multimodal sensing, online learning, wearable sensor},
  location  = {Melbourne VIC, Australia},
  series    = {UbiComp '24}
}

@inproceedings{vainio2019urban,
  author    = {Vainio, Teija and Karppi, Ilari and Jokinen, Ari and Leino, Helena},
  title     = {Towards Novel Urban Planning Methods -- Using Eye-tracking Systems to Understand Human Attention in Urban Environments},
  year      = {2019},
  isbn      = {9781450359719},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3290607.3299064},
  doi       = {10.1145/3290607.3299064},
  abstract  = {Data on how humans perceive the attractiveness of their (urban) environments has been mainly gathered with qualitative methods, including workshops, interviews and group discussions. Qualitative methods help us to understand the phenomenon, albeit with the cost of sufficient information as concerns its details. We may end up confirming something that we as researchers have 'programmed' to get as a result. Here we take a complementary approach, having collected eye tracking data from two case experiments. The participants to these experiments were professional urban planners and non-professionals respectively. We asked them to view planning-related artefacts comprising architectural illustrations, photographed landscapes and planning sketches. After analysing the findings from these experiments, we draw guidelines for using the eye tracking system in urban planning processes for gathering the human perceptions of attractive urban environments},
  booktitle = {Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems},
  pages     = {1–8},
  numpages  = {8},
  keywords  = {urban planning, participatory planning, eye-tracking},
  location  = {Glasgow, Scotland Uk},
  series    = {CHI EA '19}
}

@inproceedings{zagermann2016cognitive,
  author    = {Zagermann, Johannes and Pfeil, Ulrike and Reiterer, Harald},
  title     = {Measuring Cognitive Load using Eye Tracking Technology in Visual Computing},
  year      = {2016},
  isbn      = {9781450348188},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/2993901.2993908},
  doi       = {10.1145/2993901.2993908},
  abstract  = {In this position paper we encourage the use of eye tracking measurements to investigate users' cognitive load while interacting with a system. We start with an overview of how eye movements can be interpreted to provide insight about cognitive processes and present a descriptive model representing the relations of eye movements and cognitive load. Then, we discuss how specific characteristics of human-computer interaction (HCI) interfere with the model and impede the application of eye tracking data to measure cognitive load in visual computing. As a result, we present a refined model, embedding the characteristics of HCI into the relation of eye tracking data and cognitive load. Based on this, we argue that eye tracking should be considered as a valuable instrument to analyze cognitive processes in visual computing and suggest future research directions to tackle outstanding issues.},
  booktitle = {Proceedings of the Sixth Workshop on Beyond Time and Errors on Novel Evaluation Methods for Visualization},
  pages     = {78–85},
  numpages  = {8},
  keywords  = {novel evaluation methods, eye tracking, cognitive load},
  location  = {Baltimore, MD, USA},
  series    = {BELIV '16}
}

