@inproceedings{dembinsky2024eye,
  author    = {Dembinsky, David and Watanabe, Ko and Dengel, Andreas and Ishimaru, Shoya},
  title     = {Eye Movement in a Controlled Dialogue Setting},
  year      = {2024},
  isbn      = {9798400706073},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3649902.3653337},
  doi       = {10.1145/3649902.3653337},
  abstract  = {Designing realistic eye movements for animated avatars poses a challenge, as gaze behavior is predominantly unconscious. Accurately modulating those movements is crucial to avoid the Uncanny Valley. The human gaze exhibits different characteristics in conversations, depending on speaking or listening. Albeit these distinctions are known, data for synthesizing eye movement models suitable for avatars is scarce. This research introduces a novel dataset involving human gaze behavior during remote screen conversations. The data are collected from 19 participants, offering 4 hours of gaze data labeled as Speaking and Listening. Our data analysis substantiates prior knowledge of gaze behavior while providing new insights through higher precision. Furthermore, we demonstrate the dataset’s suitability for machine learning algorithms by training a classifier, achieving 88.1\% binary classification accuracy.},
  booktitle = {Proceedings of the 2024 Symposium on Eye Tracking Research and Applications},
  articleno = {10},
  numpages  = {7},
  keywords  = {Animated Avatars, Data Collection, Eye Movement, Gaze Synthesis},
  location  = {Glasgow, United Kingdom},
  series    = {ETRA '24}
}

@article{dembinsky2024gaze,
  author   = {Dembinsky, David and Watanabe, Ko and Dengel, Andreas and Ishimaru, Shoya},
  journal  = {IEEE Access},
  title    = {Gaze Generation for Avatars Using GANs},
  year     = {2024},
  volume   = {12},
  number   = {},
  pages    = {101536-101548},
  keywords = {Avatars;Oral communication;Generators;Data models;Faces;Training;Software;Avatars;eye gaze;GAN;human-computer-interaction},
  doi      = {10.1109/ACCESS.2024.3430835}
}

@article{ankur2024appearance,
  title   = {Appearance-Based Gaze Estimation with Deep Neural Networks: From Data Collection to Evaluation},
  author  = {Ankur Bhatt and Ko Watanabe and Andreas Dengel and Shoya Ishimaru},
  journal = {International Journal of Activity and Behavior Computing},
  volume  = {2024},
  number  = {1},
  pages   = {1-15},
  year    = {2024},
  doi     = {10.60401/ijabc.9}
}

@article{shah2024webcam,
  title   = {Webcam-based Pupil Diameter Prediction Benefits from Upscaling},
  author  = {Shah, Vijul and Moser, Brian B and Watanabe, Ko and Dengel, Andreas},
  journal = {arXiv preprint arXiv:2408.10397},
  year    = {2024}
}

@article{shah2024eyedentify,
  title   = {EyeDentify: A Dataset for Pupil Diameter Estimation based on Webcam Images},
  author  = {Shah, Vijul and Watanabe, Ko and Moser, Brian B and Dengel, Andreas},
  journal = {arXiv preprint arXiv:2407.11204},
  year    = {2024}
}

@article{carter2020best,
  title     = {Best practices in eye tracking research},
  author    = {Carter, Benjamin T and Luke, Steven G},
  journal   = {International Journal of Psychophysiology},
  volume    = {155},
  pages     = {49--62},
  year      = {2020},
  publisher = {Elsevier}
}

@inproceedings{bykowski2018automatic,
  author    = {Bykowski, Adam and Kupi\'{n}ski, Szymon},
  title     = {Automatic mapping of gaze position coordinates of eye-tracking glasses video on a common static reference image},
  year      = {2018},
  isbn      = {9781450357067},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3204493.3208331},
  doi       = {10.1145/3204493.3208331},
  abstract  = {This paper describes a method for automatic semantic gaze mapping from video obtained by eye-tracking glasses to a common reference image. Image feature detection and description algorithms are utilized to find the position of subsequent video frames and map corresponding gaze coordinates on a common reference image. This process allows aggregate experiment results for further experiment analysis and provides an alternative for manual semantic gaze mapping methods.},
  booktitle = {Proceedings of the 2018 ACM Symposium on Eye Tracking Research \& Applications},
  articleno = {84},
  numpages  = {3},
  keywords  = {semantic gaze mapping, feature matching, eye tracking, computer vision},
  location  = {Warsaw, Poland},
  series    = {ETRA '18}
}

@inproceedings{meteier2023enhancing,
  author    = {Meteier, Quentin and Mugellini, Elena and Angelini, Leonardo and Verdon, Alain Adrian and Senn-Dubey, Catherine and Vasse, Jean-Michel},
  title     = {Enhancing the Metacognition of Nursing Students Using Eye Tracking Glasses},
  year      = {2023},
  isbn      = {9798400701504},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3588015.3590115},
  doi       = {10.1145/3588015.3590115},
  abstract  = {Practical simulation is increasingly used to develop reasoning skills during learning. The analysis of the scene and the correct execution of actions require an awareness of the situation and the activities performed by the student. Eye-tracking feedback (i.e., a video recording of the practical simulation with an overlay of the gaze point) can allow students and teachers to enhance the skills of analysis and execution of the practical activities performed. In this article, we present the implementation of an innovative pedagogical process for nursing students in Switzerland. It involves the use of eye-tracking glasses to improve learning through the enhancement of metacognition after a simulation. The results of a first test session done with 15 undergraduate students are reported.},
  booktitle = {Proceedings of the 2023 Symposium on Eye Tracking Research and Applications},
  articleno = {40},
  numpages  = {2},
  keywords  = {education, eye-tracking, metacognition, nursing, simulation},
  location  = {Tubingen, Germany},
  series    = {ETRA '23}
}

@inproceedings{minakata2019pointing,
  author    = {Minakata, Katsumi and Hansen, John Paulin and MacKenzie, I. Scott and B\ae{}kgaard, Per and Rajanna, Vijay},
  title     = {Pointing by gaze, head, and foot in a head-mounted display},
  year      = {2019},
  isbn      = {9781450367097},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3317956.3318150},
  doi       = {10.1145/3317956.3318150},
  abstract  = {This paper presents a Fitts' law experiment and a clinical case study performed with a head-mounted display (HMD). The experiment compared gaze, foot, and head pointing. With the equipment setup we used, gaze was slower than the other pointing methods, especially in the lower visual field. Throughputs for gaze and foot pointing were lower than mouse and head pointing and their effective target widths were also higher. A follow-up case study included seven participants with movement disorders. Only two of the participants were able to calibrate for gaze tracking but all seven could use head pointing, although with throughput less than one-third of the non-clinical participants.},
  booktitle = {Proceedings of the 11th ACM Symposium on Eye Tracking Research \& Applications},
  articleno = {69},
  numpages  = {9},
  keywords  = {virtual reality, head-mounted displays, head interaction, hand controller, gaze interaction, foot interaction, dwell activation, disability, accessibility, ISO 9241-9, Fitts' law},
  location  = {Denver, Colorado},
  series    = {ETRA '19}
}

@inproceedings{shi2024bibliometric,
  title        = {A Bibliometric Analysis of Eye Tracking in User Experience Research},
  author       = {Shi, Yang},
  booktitle    = {International Conference on Human-Computer Interaction},
  pages        = {178--193},
  year         = {2024},
  organization = {Springer}
}

@inproceedings{tanaka2024concentration,
  author    = {Tanaka, Noriyuki and Watanabe, Ko and Ishimaru, Shoya and Dengel, Andreas and Ata, Shingo and Fujimoto, Manato},
  title     = {Concentration Estimation in Online Video Lecture Using Multimodal Sensors},
  year      = {2024},
  isbn      = {9798400710582},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3675094.3677587},
  doi       = {10.1145/3675094.3677587},
  abstract  = {Distance learning is one of the technology-wise challenges in the education field. Remote learning provides the advantage of encouraging anyone to join from worldwide. In order to make education sustainable, understanding students' concentration in remote study is significant. In this study, we evaluate multi-modal sensors for estimating students' concentration levels during online classes. We collect sensor data such as accelerometers, gyroscopes, heart rates, facial orientations, and eye gazes. We conducted experiments with 13 university students in Japan. The results of our study, with an average accuracy rate of 74.4\% for user-dependent cross-validation and 66.3\% for user-independent cross-validation, have significant implications for understanding and improving student engagement in online learning environments. Most interestingly, we found that facial orientations are significant for user-dependent and eye gazes for user-independent classification.},
  booktitle = {Companion of the 2024 on ACM International Joint Conference on Pervasive and Ubiquitous Computing},
  pages     = {71–75},
  numpages  = {5},
  keywords  = {concentration detection, machine learning, multimodal sensing, online learning, wearable sensor},
  location  = {Melbourne VIC, Australia},
  series    = {UbiComp '24}
}

@inproceedings{blanchard2014automated,
  title        = {Automated physiological-based detection of mind wandering during learning},
  author       = {Blanchard, Nathaniel and Bixler, Robert and Joyce, Tera and D’Mello, Sidney},
  booktitle    = {Intelligent Tutoring Systems: 12th International Conference, ITS 2014, Honolulu, HI, USA, June 5-9, 2014. Proceedings 12},
  pages        = {55--60},
  year         = {2014},
  organization = {Springer}
}

@article{menghini2019stressing,
  title     = {Stressing the accuracy: Wrist-worn wearable sensor validation over different conditions},
  author    = {Menghini, Luca and Gianfranchi, Evelyn and Cellini, Nicola and Patron, Elisabetta and Tagliabue, Mariaelena and Sarlo, Michela},
  journal   = {Psychophysiology},
  volume    = {56},
  number    = {11},
  pages     = {e13441},
  year      = {2019},
  publisher = {Wiley Online Library}
}

@article{bhatt2024self,
  author   = {Bhatt, Ankur and Watanabe, Ko and Santhosh, Jayasankar and Dengel, Andreas and Ishimaru, Shoya},
  journal  = {IEEE Access},
  title    = {Estimating Self-Confidence in Video-Based Learning Using Eye-Tracking and Deep Neural Networks},
  year     = {2024},
  volume   = {12},
  number   = {},
  pages    = {192219-192229},
  keywords = {Gaze tracking;Support vector machines;Reviews;Long short term memory;Data collection;Feature extraction;Estimation;Electroencephalography;Random forests;Radio frequency;Eye-tracking;learning augmentation;self-confidence estimation},
  doi      = {10.1109/ACCESS.2024.3515838}
}

@inproceedings{berkovsky2019personality,
  author    = {Berkovsky, Shlomo and Taib, Ronnie and Koprinska, Irena and Wang, Eileen and Zeng, Yucheng and Li, Jingjie and Kleitman, Sabina},
  title     = {Detecting Personality Traits Using Eye-Tracking Data},
  year      = {2019},
  isbn      = {9781450359702},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3290605.3300451},
  doi       = {10.1145/3290605.3300451},
  abstract  = {Personality is an established domain of research in psychology, and individual differences in various traits are linked to a variety of real-life outcomes and behaviours. Personality detection is an intricate task that typically requires humans to fill out lengthy questionnaires assessing specific personality traits. The outcomes of this, however, may be unreliable or biased if the respondents do not fully understand or are not willing to honestly answer the questions. To this end, we propose a framework for objective personality detection that leverages humans' physiological responses to external stimuli. We exemplify and evaluate the framework in a case study, where we expose subjects to affective image and video stimuli, and capture their physiological responses using a commercial-grade eye-tracking sensor. These responses are then processed and fed into a classifier capable of accurately predicting a range of personality traits. Our work yields notably high predictive accuracy, suggesting the applicability of the proposed framework for robust personality detection.},
  booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems},
  pages     = {1–12},
  numpages  = {12},
  keywords  = {eye tracking, field study, framework, personality detection},
  location  = {Glasgow, Scotland Uk},
  series    = {CHI '19}
}

@inproceedings{zagermann2016cognitive,
  author    = {Zagermann, Johannes and Pfeil, Ulrike and Reiterer, Harald},
  title     = {Measuring Cognitive Load using Eye Tracking Technology in Visual Computing},
  year      = {2016},
  isbn      = {9781450348188},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/2993901.2993908},
  doi       = {10.1145/2993901.2993908},
  abstract  = {In this position paper we encourage the use of eye tracking measurements to investigate users' cognitive load while interacting with a system. We start with an overview of how eye movements can be interpreted to provide insight about cognitive processes and present a descriptive model representing the relations of eye movements and cognitive load. Then, we discuss how specific characteristics of human-computer interaction (HCI) interfere with the model and impede the application of eye tracking data to measure cognitive load in visual computing. As a result, we present a refined model, embedding the characteristics of HCI into the relation of eye tracking data and cognitive load. Based on this, we argue that eye tracking should be considered as a valuable instrument to analyze cognitive processes in visual computing and suggest future research directions to tackle outstanding issues.},
  booktitle = {Proceedings of the Sixth Workshop on Beyond Time and Errors on Novel Evaluation Methods for Visualization},
  pages     = {78–85},
  numpages  = {8},
  keywords  = {novel evaluation methods, eye tracking, cognitive load},
  location  = {Baltimore, MD, USA},
  series    = {BELIV '16}
}

@inproceedings{vainio2019urban,
  author    = {Vainio, Teija and Karppi, Ilari and Jokinen, Ari and Leino, Helena},
  title     = {Towards Novel Urban Planning Methods -- Using Eye-tracking Systems to Understand Human Attention in Urban Environments},
  year      = {2019},
  isbn      = {9781450359719},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3290607.3299064},
  doi       = {10.1145/3290607.3299064},
  abstract  = {Data on how humans perceive the attractiveness of their (urban) environments has been mainly gathered with qualitative methods, including workshops, interviews and group discussions. Qualitative methods help us to understand the phenomenon, albeit with the cost of sufficient information as concerns its details. We may end up confirming something that we as researchers have 'programmed' to get as a result. Here we take a complementary approach, having collected eye tracking data from two case experiments. The participants to these experiments were professional urban planners and non-professionals respectively. We asked them to view planning-related artefacts comprising architectural illustrations, photographed landscapes and planning sketches. After analysing the findings from these experiments, we draw guidelines for using the eye tracking system in urban planning processes for gathering the human perceptions of attractive urban environments},
  booktitle = {Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems},
  pages     = {1–8},
  numpages  = {8},
  keywords  = {urban planning, participatory planning, eye-tracking},
  location  = {Glasgow, Scotland Uk},
  series    = {CHI EA '19}
}

@inproceedings{cai2023speakfaster,
  author    = {Cai, Shanqing and Venugopalan, Subhashini and Tomanek, Katrin and Kane, Shaun and Morris, Meredith Ringel and Cave, Richard and Macdonald, Robert and Campbell, Jon and Casey, Blair and Kornman, Emily and Vance, Daniel E and Beavers, Jay},
  title     = {SpeakFaster Observer: Long-Term Instrumentation of Eye-Gaze Typing for Measuring AAC Communication},
  year      = {2023},
  isbn      = {9781450394222},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3544549.3573870},
  doi       = {10.1145/3544549.3573870},
  abstract  = {Accelerating communication for users with severe motor and speech impairments, in particular for eye-gaze-based augmentative and alternative communication (AAC) device users, is a longstanding area of research. However, observation of such users’ communication over extended durations has been limited. This case study presents the real-world experience of developing and field-testing a tool for observing and curating the gaze typing-based communication of an eye-gaze AAC user with amyotrophic lateral sclerosis (ALS). With the intent to observe and develop technology to accelerate eye-gaze typed communication, we designed a tool and a protocol called the SpeakFaster Observer to measure everyday conversational text entry by the gaze-typing user, as well as several consenting conversation partners of the AAC user. We detail the design of the Observer software and data curation protocol, along with considerations for privacy protection. The deployment of the data protocol from November 2021 to April 2022 yielded a rich dataset of gaze-based AAC text entry from everyday life, consisting of 130+ hours of gaze keystrokes and 5,000+ curated speech utterances from the AAC user and the conversation partners. We present the key statistics of the data, including the speed (8.1 ± 3.9 words per minute) and keystroke saving rate (-0.14 ± 0.83) of gaze typing, patterns of utterance repetition and reuse, and the temporal dynamics of conversation turn-taking in gaze-based communication. We share our findings and also open source our data collection tools to further research in this domain.},
  booktitle = {Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems},
  articleno = {397},
  numpages  = {8},
  keywords  = {accessibility, augmentative and alternative communication, context awareness, conversation, gaze typing, text corpus},
  location  = {Hamburg, Germany},
  series    = {CHI EA '23}
}

@article{lystbaek2022exploring,
  author     = {Lystb\ae{}k, Mathias N. and Pfeuffer, Ken and Gr\o{}nb\ae{}k, Jens Emil Sloth and Gellersen, Hans},
  title      = {Exploring Gaze for Assisting Freehand Selection-based Text Entry in AR},
  year       = {2022},
  issue_date = {May 2022},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {6},
  number     = {ETRA},
  url        = {https://doi.org/10.1145/3530882},
  doi        = {10.1145/3530882},
  abstract   = {With eye-tracking increasingly available in Augmented Reality, we explore how gaze can be used to assist freehand gestural text entry. Here the eyes are often coordinated with manual input across the spatial positions of the keys. Inspired by this, we investigate gaze-assisted selection-based text entry through the concept of spatial alignment of both modalities. Users can enter text by aligning both gaze and manual pointer at each key, as a novel alternative to existing dwell-time or explicit manual triggers. We present a text entry user study comparing two of such alignment techniques to a gaze-only and a manual-only baseline. The results show that one alignment technique reduces physical finger movement by more than half compared to standard in-air finger typing, and is faster and exhibits less perceived eye fatigue than an eyes-only dwell-time technique. We discuss trade-offs between uni and multimodal text entry techniques, pointing to novel ways to integrate eye movements to facilitate virtual text entry.},
  journal    = {Proc. ACM Hum.-Comput. Interact.},
  month      = may,
  articleno  = {141},
  numpages   = {16},
  keywords   = {augmented reality, eye-tracking, gaze interaction, multimodal ui, text entry, virtual keyboard}
}

@inproceedings{kopacsi2024exploring,
  author    = {Kop\'{a}csi, L\'{a}szl\'{o} and Klimenko, Albert and Barz, Michael and Sonntag, Daniel},
  title     = {Exploring Gaze-Based Menu Navigation in Virtual Environments},
  year      = {2024},
  isbn      = {9798400710889},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3677386.3688887},
  doi       = {10.1145/3677386.3688887},
  abstract  = {With the integration of eye tracking technologies in Augmented Reality (AR) and Virtual Reality (VR) headsets, gaze-based interactions have opened up new possibilities for user interface design, including menu navigation. Prior research in gaze-based menu navigation in VR has predominantly focused on pie menus, yet recent studies indicate a user preference for list layouts. However, the comparison of gaze-based interactions on list menus is lacking in the literature. This work aims to fill this gap by exploring the viability of list menus for multi-level gaze-based menu navigation in VR and evaluating the efficiency of various gaze-based interactions, such as dwelling and border-crossing, against traditional controller navigation and multi-modal interaction using gaze and button press.},
  booktitle = {Proceedings of the 2024 ACM Symposium on Spatial User Interaction},
  articleno = {40},
  numpages  = {2},
  keywords  = {Extended Reality (XR), Eye Tracking, Gaze-based Interaction, Menu Navigation},
  location  = {Trier, Germany},
  series    = {SUI '24}
}

@inproceedings{cho2024sonohaptics,
  author    = {Cho, Hyunsung and Sendhilnathan, Naveen and Nebeling, Michael and Wang, Tianyi and Padmanabhan, Purnima and Browder, Jonathan and Lindlbauer, David and Jonker, Tanya R. and Todi, Kashyap},
  title     = {SonoHaptics: An Audio-Haptic Cursor for Gaze-Based Object Selection in XR},
  year      = {2024},
  isbn      = {9798400706288},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3654777.3676384},
  doi       = {10.1145/3654777.3676384},
  abstract  = {We introduce SonoHaptics, an audio-haptic cursor for gaze-based 3D object selection. SonoHaptics addresses challenges around providing accurate visual feedback during gaze-based selection in Extended Reality&nbsp;(XR), e.&nbsp;g., lack of world-locked displays in no- or limited-display smart glasses and visual inconsistencies. To enable users to distinguish objects without visual feedback, SonoHaptics employs the concept of cross-modal correspondence in human perception to map visual features of objects (color, size, position, material) to audio-haptic properties (pitch, amplitude, direction, timbre). We contribute data-driven models for determining cross-modal mappings of visual features to audio and haptic features, and a computational approach to automatically generate audio-haptic feedback for objects in the user’s environment. SonoHaptics provides global feedback that is unique to each object in the scene, and local feedback to amplify differences between nearby objects. Our comparative evaluation shows that SonoHaptics enables accurate object identification and selection in a cluttered scene without visual feedback.},
  booktitle = {Proceedings of the 37th Annual ACM Symposium on User Interface Software and Technology},
  articleno = {125},
  numpages  = {19},
  keywords  = {Computational Interaction, Extended Reality, Gaze-based Selection, Haptics, Multimodal Feedback, Sonification},
  location  = {Pittsburgh, PA, USA},
  series    = {UIST '24}
}

@inproceedings{brishtel2018assessing,
  title     = {Assessing cognitive workload on printed and electronic media using eye-tracker and EDA wristband},
  author    = {Brishtel, Iuliia and Ishimaru, Shoya and Augereau, Olivier and Kise, Koichi and Dengel, Andreas},
  booktitle = {Companion Proceedings of the 23rd International Conference on Intelligent User Interfaces},
  pages     = {1--2},
  year      = {2018}
}

@article{brishtel2020mind,
  title     = {Mind wandering in a multimodal reading setting: Behavior analysis \& automatic detection using eye-tracking and an eda sensor},
  author    = {Brishtel, Iuliia and Khan, Anam Ahmad and Schmidt, Thomas and Dingler, Tilman and Ishimaru, Shoya and Dengel, Andreas},
  journal   = {Sensors},
  volume    = {20},
  number    = {9},
  pages     = {2546},
  year      = {2020},
  publisher = {MDPI}
}

@inproceedings{lee2024echowrist,
  author    = {Lee, Chi-Jung and Zhang, Ruidong and Agarwal, Devansh and Yu, Tianhong Catherine and Gunda, Vipin and Lopez, Oliver and Kim, James and Yin, Sicheng and Dong, Boao and Li, Ke and Sakashita, Mose and Guimbretiere, Francois and Zhang, Cheng},
  title     = {EchoWrist: Continuous Hand Pose Tracking and Hand-Object Interaction Recognition Using Low-Power Active Acoustic Sensing On a Wristband},
  year      = {2024},
  isbn      = {9798400703300},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3613904.3642910},
  doi       = {10.1145/3613904.3642910},
  abstract  = {Our hands serve as a fundamental means of interaction with the world around us. Therefore, understanding hand poses and interaction contexts is critical for human-computer interaction (HCI). We present EchoWrist, a low-power wristband that continuously estimates 3D hand poses and recognizes hand-object interactions using active acoustic sensing. EchoWrist is equipped with two speakers emitting inaudible sound waves toward the hand. These sound waves interact with the hand and its surroundings through reflections and diffractions, carrying rich information about the hand’s shape and the objects it interacts with. The information captured by the two microphones goes through a deep learning inference system that recovers hand poses and identifies various everyday hand activities. Results from the two 12-participant user studies show that EchoWrist is effective and efficient at tracking 3D hand poses and recognizing hand-object interactions. Operating at 57.9 mW, EchoWrist can continuously reconstruct 20 3D hand joints with MJEDE of 4.81 mm and recognize 12 naturalistic hand-object interactions with 97.6\% accuracy.},
  booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
  articleno = {403},
  numpages  = {21},
  keywords  = {Acoustic Sensing, Hand Pose, Hand-Object Interaction, Smartwatch, Wearable},
  location  = {Honolulu, HI, USA},
  series    = {CHI '24}
}

@inproceedings{funk2014using,
  author    = {Funk, Markus and Sahami, Alireza and Henze, Niels and Schmidt, Albrecht},
  title     = {Using a touch-sensitive wristband for text entry on smart watches},
  year      = {2014},
  isbn      = {9781450324748},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/2559206.2581143},
  doi       = {10.1145/2559206.2581143},
  abstract  = {The ongoing miniaturization of computing devices enabled smart watches with capabilities almost on par with smart phones. Due to immanent size restrictions smart watches require specifically designed input and output techniques. In particular, entering text is often needed when interacting with computers but the watches' small size excludes common input techniques. In this paper we propose a text entry technique using a touch sensitive wristband. Using the wristband requires no screen space besides displaying the current character and might enable cheaper devices. In an experiment we compare a linear keyboard and a multitap keyboard layout. We show that users type faster and make fewer errors using multitap. We argue that the inexpensive sensors enable the integration in low cost wearable watches that require text entry occasionally.},
  booktitle = {CHI '14 Extended Abstracts on Human Factors in Computing Systems},
  pages     = {2305–2310},
  numpages  = {6},
  keywords  = {wristband, text entry, smart watch, keyboard},
  location  = {Toronto, Ontario, Canada},
  series    = {CHI EA '14}
}

@article{arakawa2024prism,
  title     = {Prism-q\&a: Step-aware voice assistant on a smartwatch enabled by multimodal procedure tracking and large language models},
  author    = {Arakawa, Riku and Lehman, Jill Fain and Goel, Mayank},
  journal   = {Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
  volume    = {8},
  number    = {4},
  pages     = {1--26},
  year      = {2024},
  publisher = {ACM New York, NY, USA}
}

@article{candini2021physiological,
  title     = {The physiological correlates of interpersonal space},
  author    = {Candini, Michela and Battaglia, Simone and Benassi, Mariagrazia and di Pellegrino, Giuseppe and Frassinetti, Francesca},
  journal   = {Scientific Reports},
  volume    = {11},
  number    = {1},
  pages     = {2611},
  year      = {2021},
  publisher = {Nature Publishing Group UK London}
}

@article{coello2021interrelation,
  title     = {The interrelation between peripersonal action space and interpersonal social space: Psychophysiological evidence and clinical implications},
  author    = {Coello, Yann and Cartaud, Alice},
  journal   = {Frontiers in Human Neuroscience},
  volume    = {15},
  pages     = {636124},
  year      = {2021},
  publisher = {Frontiers Media SA}
}

@article{geers2023relationship,
  title     = {The relationship between action, social and multisensory spaces},
  author    = {Geers, Laurie and Coello, Yann},
  journal   = {Scientific Reports},
  volume    = {13},
  number    = {1},
  pages     = {202},
  year      = {2023},
  publisher = {Nature Publishing Group UK London}
}

@article{nandrino2017perception,
  title     = {Perception of peripersonal and interpersonal space in patients with restrictive-type anorexia},
  author    = {Nandrino, Jean-Louis and Ducro, Claire and Iachini, Tina and Coello, Yann},
  journal   = {European Eating Disorders Review},
  volume    = {25},
  number    = {3},
  pages     = {179--187},
  year      = {2017},
  publisher = {Wiley Online Library}
}

@article{watanabe2021discaas,
  title     = {Discaas: Micro behavior analysis on discussion by camera as a sensor},
  author    = {Watanabe, Ko and Soneda, Yusuke and Matsuda, Yuki and Nakamura, Yugo and Arakawa, Yutaka and Dengel, Andreas and Ishimaru, Shoya},
  journal   = {Sensors},
  volume    = {21},
  number    = {17},
  pages     = {5719},
  year      = {2021},
  publisher = {MDPI}
}

@article{nakamura2019waistonbelt,
  title     = {Waistonbelt x: A belt-type wearable device with sensing and intervention toward health behavior change},
  author    = {Nakamura, Yugo and Matsuda, Yuki and Arakawa, Yutaka and Yasumoto, Keiichi},
  journal   = {Sensors},
  volume    = {19},
  number    = {20},
  pages     = {4600},
  year      = {2019},
  publisher = {MDPI}
}

@article{simonyan2014very,
  title   = {Very deep convolutional networks for large-scale image recognition},
  author  = {Simonyan, Karen and Zisserman, Andrew},
  journal = {arXiv preprint arXiv:1409.1556},
  year    = {2014}
}

@article{howard2017mobilenets,
  author     = {Andrew G. Howard and
                Menglong Zhu and
                Bo Chen and
                Dmitry Kalenichenko and
                Weijun Wang and
                Tobias Weyand and
                Marco Andreetto and
                Hartwig Adam},
  title      = {MobileNets: Efficient Convolutional Neural Networks for Mobile Vision
                Applications},
  journal    = {CoRR},
  volume     = {abs/1704.04861},
  year       = {2017},
  url        = {http://arxiv.org/abs/1704.04861},
  eprinttype = {arXiv},
  eprint     = {1704.04861},
  biburl     = {https://dblp.org/rec/journals/corr/HowardZCKWWAA17.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org},
  note       = {Visited on May 19, 2023}
}

@inproceedings{sandler2018mobilenetv2,
  title     = {Mobilenetv2: Inverted residuals and linear bottlenecks},
  author    = {Sandler, Mark and Howard, Andrew and Zhu, Menglong and Zhmoginov, Andrey and Chen, Liang-Chieh},
  booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages     = {4510--4520},
  year      = {2018}
}

@inproceedings{howard2019searching,
  title     = {Searching for mobilenetv3},
  author    = {Howard, Andrew and Sandler, Mark and Chu, Grace and Chen, Liang-Chieh and Chen, Bo and Tan, Mingxing and Wang, Weijun and Zhu, Yukun and Pang, Ruoming and Vasudevan, Vijay and others},
  booktitle = {Proceedings of the IEEE/CVF international conference on computer vision},
  pages     = {1314--1324},
  year      = {2019}
}

@inproceedings{vaswani2017attention,
  author    = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, \L{}ukasz and Polosukhin, Illia},
  title     = {Attention is all you need},
  year      = {2017},
  isbn      = {9781510860964},
  publisher = {Curran Associates Inc.},
  address   = {Red Hook, NY, USA},
  abstract  = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.},
  booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
  pages     = {6000–6010},
  numpages  = {11},
  location  = {Long Beach, California, USA},
  series    = {NIPS'17}
}

@article{sulutvedt2018gaze,
  title     = {Gaze and the eye pupil adjust to imagined size and distance},
  author    = {Sulutvedt, Unni and Mannix, Thea K and Laeng, Bruno},
  journal   = {Cognitive Science},
  volume    = {42},
  number    = {8},
  pages     = {3159--3176},
  year      = {2018},
  publisher = {Wiley Online Library}
}

@inproceedings{desai2021electrodermal,
  title        = {Electrodermal activity (EDA) for treatment of neurological and psychiatric disorder patients: a review},
  author       = {Desai, Usha and Shetty, Akshaya D},
  booktitle    = {2021 7th International Conference on Advanced Computing and Communication Systems (ICACCS)},
  volume       = {1},
  pages        = {1424--1430},
  year         = {2021},
  organization = {IEEE}
}

@article{watanabe2023engauge,
  title     = {Engauge: Engagement gauge of meeting participants estimated by facial expression and deep neural network},
  author    = {Watanabe, Ko and Sathyanarayana, Tanuja and Dengel, Andreas and Ishimaru, Shoya},
  journal   = {IEEE Access},
  volume    = {11},
  pages     = {52886--52898},
  year      = {2023},
  publisher = {IEEE}
}

@inproceedings{watanabe2024metacognition,
  author    = {Watanabe, Ko and Dengel, Andreas and Ishimaru, Shoya},
  title     = {Metacognition-EnGauge: Real-time Augmentation of Self-and-Group Engagement Levels Understanding by Gauge Interface in Online Meetings},
  year      = {2024},
  isbn      = {9798400709807},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3652920.3653054},
  doi       = {10.1145/3652920.3653054},
  abstract  = {Engagement is one of the core cognitive states in communication. Increased engagement improves the quality and immersion of the conversation. In this demonstration, we aim to present a metacognition augmentation application called Metacognition-EnGauge. This work aims to discover three research questions. Does self-metacognition augmentation of engagement levels support participant to be engaged in the meetings? Does group-metacognition augmentation of engagement levels support participant to be engaged in the meetings? Do participants prefer none, self, or group-metacognition augmentation intervention in the online meetings? To answer these questions, we conducted online meeting experiments with 18 participants. As a result, we found that self-metacognition augmentation outperformed well on increasing average engagement levels. According to the survey, participant preference shows group-metacognition as the best intervention.},
  booktitle = {Proceedings of the Augmented Humans International Conference 2024},
  pages     = {301–303},
  numpages  = {3},
  keywords  = {Affective Computing, Metacognition Augmentation, Social Cognitive State, Social Interaction},
  location  = {Melbourne, VIC, Australia},
  series    = {AHs '24}
}

@inproceedings{sehrt2024closing,
  author    = {Sehrt, Jessica and Yilmaz, Ugur and Kosch, Thomas and Schwind, Valentin},
  title     = {Closing the Loop: The Effects of Biofeedback Awareness on Physiological Stress Response Using Electrodermal Activity in Virtual Reality},
  year      = {2024},
  isbn      = {9798400703317},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3613905.3650830},
  doi       = {10.1145/3613905.3650830},
  abstract  = {This paper presents the results of a user study examining the impact of biofeedback awareness on the effectiveness of stress management, utilizing Electrodermal Activity (EDA) as the primary metric within an immersive Virtual Reality (VR). Employing a between-subjects design (N=30), we probed whether informing individuals of their capacity to manipulate the VR environment’s weather impacts their physiological stress responses. Our results indicate lower EDA levels of participants who were informed of their biofeedback control than those participants who were not informed about their biofeedback control. Interestingly, the participants who were informed about the control over the environment also manifested variations in their EDA responses. Participants who were not informed of their ability to control the weather showed decreased EDA measures until the end of the biofeedback phase. This study enhances our comprehension of the significance of awareness in biofeedback in immersive settings and its potential to augment stress management techniques.},
  booktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},
  articleno = {76},
  numpages  = {7},
  keywords  = {Awareness, Biofeedback, Electrodermal Activity, Stress, Virtual Reality},
  location  = {Honolulu, HI, USA},
  series    = {CHI EA '24}
}

@article{cook2007smart,
  title     = {How smart are our environments? An updated look at the state of the art},
  author    = {Cook, Diane J and Das, Sajal K},
  journal   = {Pervasive and mobile computing},
  volume    = {3},
  number    = {2},
  pages     = {53--73},
  year      = {2007},
  publisher = {Elsevier}
}

@inproceedings{lim2024exploring,
  author    = {Lim, Jieun and Koh, Youngji and Kim, Auk and Lee, Uichin},
  title     = {Exploring Context-Aware Mental Health Self-Tracking Using Multimodal Smart Speakers in Home Environments},
  year      = {2024},
  isbn      = {9798400703300},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3613904.3642846},
  doi       = {10.1145/3613904.3642846},
  abstract  = {People with mental health issues often stay indoors, reducing their outdoor activities. This situation emphasizes the need for self-tracking technology in homes for mental health research, offering insights into their daily lives and potentially improving care. This study leverages a multimodal smart speaker to design a proactive self-tracking research system that delivers mental health surveys using an experience sampling method (ESM). Our system determines ESM delivery timing by detecting user context transitions and allowing users to answer surveys through voice dialogues or touch interactions. Furthermore, we explored the user experience of a proactive self-tracking system by conducting a four-week field study (n=20). Our results show that context transition-based ESM delivery can increase user compliance. Participants preferred touch interactions to voice commands, and the modality selection varied depending on the user’s immediate activity context. We explored the design implications for home-based, context-aware self-tracking with multimodal speakers, focusing on practical applications.},
  booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
  articleno = {699},
  numpages  = {18},
  keywords  = {Experience Sampling Method (ESM), Mental Health, Multimodal Smart Speakers, Self-tracking},
  location  = {Honolulu, HI, USA},
  series    = {CHI '24}
}

@article{sorokowska2017preferred,
  title     = {Preferred interpersonal distances: A global comparison},
  author    = {Sorokowska, Agnieszka and Sorokowski, Piotr and Hilpert, Peter and Cantarero, Katarzyna and Frackowiak, Tomasz and Ahmadi, Khodabakhsh and Alghraibeh, Ahmad M and Aryeetey, Richmond and Bertoni, Anna and Bettache, Karim and others},
  journal   = {Journal of Cross-Cultural Psychology},
  volume    = {48},
  number    = {4},
  pages     = {577--592},
  year      = {2017},
  publisher = {Sage Publications Sage CA: Los Angeles, CA}
}

@article{beaulieu2004intercultural,
  title     = {Intercultural study of personal space: A case study},
  author    = {Beaulieu, Catherine},
  journal   = {Journal of applied social psychology},
  volume    = {34},
  number    = {4},
  pages     = {794--805},
  year      = {2004},
  publisher = {Wiley Online Library}
}
