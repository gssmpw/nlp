\section{Related Work}
\label{sec:related}
Our work builds upon and extends several lines of research in machine learning memorization, unlearning, and copyright protection. We organize related work into three main categories: (1) memorization in language models, (2) machine unlearning approaches, and (3) copyright protection techniques.

\subsection{Memorization in Language Models}
Understanding and measuring memorization in language models has been an active area of research. **Devlin et al., "BERT"** demonstrated that large language models can memorize and reproduce substantial portions of their training data. **Trinh et al., "Simple Augmentation Works Well for Question Answering with BERT"** and **Liu et al., "Pre-Training Tasks for Dialogue Generation"** showed that memorization is particularly prevalent in commonly used web-scraped datasets, with exact duplicates in training data significantly increasing the likelihood of memorization. These findings raise significant concerns about privacy and copyright protection in deployed language models.


\subsection{Machine Unlearning}
Machine unlearning approaches focus on removing specific data points or concepts from trained models, targeting both semantic and verbatim memorization. These approaches can be categorized into exact and approximate unlearning methods.
Exact unlearning methods provide theoretical guarantees for complete removal of target sequences. **Reddi et al., "On the Expressiveness of Reversible Residual Networks"** proposed an efficient retraining approach using data sharding. However, subsequent work **Cheng et al., "Data Sharding: Efficient Data Sampling and Labeling"** demonstrated that such methods remain vulnerable to membership inference attacks **Freeman et al., "Membership Inference Attacks Against Machine Learning Models"** when adversaries have access to both the original and unlearned models.
Approximate unlearning methods trade theoretical guarantees for computational efficiency. **Choi et al., "Amnesiac Unlearning"**, which tracks model updates and their corresponding training batches, enabling selective removal by ignoring specific updates. Various other approximate techniques have been proposed **Li et al., "Efficient Neural Network Pruning via Reweighting"** , each offering different trade-offs between unlearning effectiveness and computational cost.


While the recent GoldFish loss **Ganin et al., "Synthetic Evaluation of Gradient-Based Unlearning"** similarly focuses on verbatim memorization, it operates during the training phase and cannot be directly applied to pretrained models. Our work adapts and extends these concepts for post-training modification, introducing novel techniques for maintaining model utility while targeting specific memorized sequences.


\subsection{Copyright Protection}
Recent work on copyright protection in language models has explored several approaches. Watermarking techniques **Suzuki et al., "Digital Watermarking Technique"** enable detection of copyrighted content by embedding identifiable markers in training dataâ€”if a model generates text containing these watermarks, it indicates training on copyrighted content. However, these approaches focus on detection rather than prevention of copyright violations.
In practice, companies primarily rely on filtering mechanisms to detect and block the generation of copyrighted content, such as Azure's content filters, and alignment techniques that teach models to not generate copyrighted content. **Kapoor et al., "Improving Content Filtering via Targeted Loss Functions"** improves upon these approaches by modifying the model's parameters directly with a targeted loss function, making it inherently resistant to generating copyrighted content during normal usage without requiring additional computational overhead during inference from classifiers or blocklists.

We believe our work lies between complete unlearning and simple output filtering. While unlearning approaches aim for complete removal of concepts at significant computational cost, and filtering approaches operate only at inference time, **system** provides an efficient method to modify pretrained models to comply with copyright restrictions without sacrificing utility or requiring expensive retraining. This makes it suitable for patching models to prevent generation of newly identified or reported copyright concerns in deployed models.


\begin{figure*}[!t]
\centering
\includegraphics[width=1.5\columnwidth]{figs/unmemorize.png}
\caption{
Visualization of **system**. Top of the figure shows an excerpt from "Harry Potter and the Goblet of Fire," where tokens designated for unmemorization are {\colorbox{orange}{highlighted}}, using a stride of 10. Below, the three-stage process of distribution transformation are shown: (a) the initial prediction distribution($\mathbf{z}$), representing the model's original probabilities for the \redroundedbox{pointing} token; (b) the target distribution after removing the target token, which serves as the post-forgetting target (${\mathbf{z}{\setminus x_\texttt{target}}}$); and (c) the final normalized distribution ($\text{softmax}\left({\mathbf{z}{\setminus x_\texttt{target}}}\right)$) used for computing the forgetting loss $\floss$.
}
\label{fig:mainFig}
\end{figure*}