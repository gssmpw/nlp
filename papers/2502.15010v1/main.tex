\documentclass{article}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{booktabs}
\usepackage{multirow}  


\usepackage{hyperref}


\newcommand{\theHalgorithm}{\arabic{algorithm}}

\usepackage[accepted]{icml2025}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{tcolorbox}  
\usepackage{subcaption}

\usepackage[capitalize,noabbrev]{cleveref}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\renewcommand*{\sectionautorefname}{Section}
\renewcommand*{\subsectionautorefname}{Section}
\renewcommand*{\subsubsectionautorefname}{Section}


\newtcbox{\redroundedbox}{on line,  
  colframe=red,  
  colback=white,  
  boxrule=1.8pt,  
  arc=4pt,  
  boxsep=0pt,  
  left=2pt,  
  right=2pt,  
  top=2pt,  
  bottom=2pt,  
  before upper={\vphantom{dlg}}  
}  
\usepackage[textsize=tiny]{todonotes}



\newcommand{\system}{Obliviate}
\newcommand{\model}{M}
\newcommand{\txt}{T}
\newcommand{\stride}{s}
\newcommand{\mloss}{\mathcal{L}_{\texttt{maintain}}}
\newcommand{\floss}{\mathcal{L}_{\texttt{forget}}}
\newcommand{\mypara}[1]{\smallskip\noindent{\bf {#1}.}}
\newcommand{\as}[1]{{\color{blue} {#1}}}
\DeclareMathOperator*{\argmax}{arg\,max}

\begin{document}

\twocolumn[
\icmltitle{\system{}: Efficient Unmemorization for Protecting Intellectual Property in Large Language Models}



\begin{icmlauthorlist}
\icmlauthor{Mark Russinovich}{Azure}
\icmlauthor{Ahmed Salem}{MS}
\end{icmlauthorlist}

\icmlaffiliation{Azure}{Microsoft Azure}
\icmlaffiliation{MS}{Microsoft}
\vskip 0.3in
]
\printAffiliationsAndNotice{}

\begin{abstract}
Recent copyright agreements between AI companies and content creators have highlighted the need for precise control over language models' ability to reproduce copyrighted content. While existing approaches rely on either complete concept removal through unlearning or simple output filtering, we propose \system{}, a novel post-training technique that selectively prevents verbatim reproduction of specific text while preserving semantic understanding.

\system{} operates by selecting tokens within memorized sequences and modifying the model's probability distribution to prevent exact reproduction while maintaining contextual understanding. We evaluate \system{} on multiple large language models (LLaMA-3.1 8B, LLaMA-3.1-instruct 8B, Qwen-2.5-7B, and Yi-1.5 6B) across both synthetic memorization tasks and organic copyright content. Our results demonstrate that \system{} achieves orders of magnitude reduction, e.g., 100x, in verbatim memorization while maintaining model performance within 1\% of baseline on standard benchmarks (HellaSwag, MMLU, TruthfulQA, and Winogrande). This makes \system{} particularly suitable for practical deployment scenarios where companies need to efficiently address copyright concerns in pretrained models without compromising their general capabilities.
\end{abstract}

\section{Introduction}
\label{sec:intro}
The rise of large language models (LLMs) has sparked intense debate about intellectual property rights in AI-generated content. Central to this debate is the fundamental question: When does an AI system's output constitute a derivative work versus an infringing reproduction? Recent legal frameworks and industry agreements have begun to converge on this distinction through specific policies targeting verbatim content reproduction\cite{plagiarismtoday}. For instance, publishers are discussing how large an excerpt would be acceptable to use\cite{ingramspark,janefriedman}. Similarly, major AI companies are starting to implement strict limits on exact text reproduction, with policies that will prevent generating copyrighted content and others that stop generating verbatim text after the generation exceeds a specific number of words.



Traditional approaches to protecting intellectual property in LLMs have primarily relied on post-processing filters, alignment or aggressive unlearning techniques that attempt to remove copyrighted content entirely from the model. However, emerging copyright discussions/agreements seems to focus more narrowly on protecting specific expressions rather than underlying ideas or concepts. This shift suggests a more targeted technical challenge: instead of preventing models from learning concepts altogether, we need only prevent them from reproducing specific sequences verbatim—a distinction that enables more efficient solutions.


In this work, we propose \system{}, a novel technique that enables selective unmemorization of exact sequences in pretrained language models while maintaining their semantic knowledge of the text. Although recent work like the GoldFish loss \cite{HWJKKSSSGBG24} function has shown promise in preventing verbatim memorization during training, our approach uniquely addresses the critical challenge of modifying already trained models, where the vast majority of deployment scenarios lie. 


\system{} intuitively works by identifying specific tokens within the text that need to be unmemorized and applying targeted modifications. More concretely, we apply the KL (Kullback-Leibler) divergence loss to adjust the model’s output distribution, reducing the probability that the target token will reproduce ($\floss$) while maintaining fluency and coherence. Target tokens are selected using a sliding stride to ensure comprehensive coverage, and replacement candidates are carefully constrained to maintain the model's utility.



For non-target tokens --those we do not wish to unmemorize-- we apply KL divergence to encourage the top-k token distribution to remain consistent  ($\mloss$), ensuring the model retains its overall performance. This dual strategy effectively balances the competing objectives of unmemorization and utility preservation.




We evaluate \system{} on both synthetic datasets, where we explicitly train models to deeply memorize specific sequences, and on original organic copyrighted datasets, such as excerpts from Harry Potter, Moby Dick, and other well-known texts that appear in Web corpus. Our evaluation focuses on multiple large language models with different architectures and sizes, including LLaMA-3.1 8B, LLaMA-3.1-Instruct 8B, Qwen-2.5 7B and Yi-1.5 6B.



Our results demonstrate that \system{} achieves strong unmemorization performance. For instance, it reduces the longest common subsequence between the generated data and target copyright content from an average of 270-380 words to less than 12 words for the synthetic dataset, while maintaining the model's utility with a negligible drop and sometimes improvement within $1\%$ across baseline benchmarks, including MMLU, TruthfulQA, Hellaswag, and Winogrande. These findings highlight the effectiveness of \system{} in selectively unmemorizing targeted content while preserving the overall performance of the models.

In short, our key contributions are as follows.

\begin{itemize}
    \item We propose \system{}, the first efficient post-training unmemorization technique that selectively targets verbatim text generation without compromising model utility.
        
    \item We provide comprehensive empirical validation across diverse LLMs and benchmarks, demonstrating the strong performance of \system{}.

\end{itemize}

\section{Background and Preliminaries}
\label{sec:prelim}
\subsection{Types of Model Memorization}
Large Language Models (LLMs) exhibit two distinct types of training data memorization:


\mypara{Semantic Memorization} The first type is semantic memorization, where models learn and internalize concepts from training data. This form of memorization is generally desirable as it enables models to understand and reason about learned concepts.


\mypara{Verbatim Memorization} The second type is verbatim memorization, where models store and can reproduce exact sequences from their training data. This form of memorization is particularly problematic for copyright protection and is the primary focus of our work.
We formally define verbatim memorization in terms of a model's ability to complete sequences given partial contexts. Specifically, for a sequence $\mathbf{s} = (s_1, ..., s_n)$, we say it is $(\ell, \beta)$-memorized if there exists a prefix length $\ell$ such that:
\[
P_\model(s_{i+\ell}, s_{i+\ell+1}, \ldots, s_n | s_{i:i+\ell-1}) > \beta
\]
where $i$ denotes the starting location of the prefix, formally defined as ${i \sim U[1,n-\ell]}$ with $U[1,n-\ell]$ denoting uniform sampling of prefix positions, and $\beta \in [0,1]$ is a threshold parameter.


Following \citet{CTWJHLRBSEOR21}, we simplify the memorization definition to consider greedy decoding: given a prefix $s_{i:i+\ell-1}$, we measure the longest sequence $n$ that model $\model$ generates that matches the original text:
\[
n^* = \max {n : \forall j \in [i+\ell, i+n], s_j = \argmax_{x \in \mathcal{V}} P_\model(x|s_{i:j-1})}
\]
where $\argmax_{x \in \mathcal{V}}$ represents greedy token selection from the vocabulary $\mathcal{V}$. Furthermore, to ensure we thoroughly evaluate memorization, we also consider using the default temperature during generation




\subsection{Threat Model}
We focus on copyright protection in non-adversarial settings, aiming to prevent models from unintentionally generating or being directly prompted to generate copyrighted content during normal usage. While a sufficiently motivated attacker might still be able to coerce the model into producing such content, our primary objective is to prevent routine copyright infringements.





\section{Related Work}
\label{sec:related}
Our work builds upon and extends several lines of research in machine learning memorization, unlearning, and copyright protection. We organize related work into three main categories: (1) memorization in language models, (2) machine unlearning approaches, and (3) copyright protection techniques.

\subsection{Memorization in Language Models}
Understanding and measuring memorization in language models has been an active area of research. \citet{CIJLTZ23,ZILJTC23} demonstrated that large language models can memorize and reproduce substantial portions of their training data. \citet{KWR22} and \citet{LINZECC22} showed that memorization is particularly prevalent in commonly used web-scraped datasets, with exact duplicates in training data significantly increasing the likelihood of memorization. These findings raise significant concerns about privacy and copyright protection in deployed language models.


\subsection{Machine Unlearning}
Machine unlearning approaches focus on removing specific data points or concepts from trained models, targeting both semantic and verbatim memorization. These approaches can be categorized into exact and approximate unlearning methods.
Exact unlearning methods provide theoretical guarantees for complete removal of target sequences. \citet{BCCJTZLP20} proposed an efficient retraining approach using data sharding. However, subsequent work\cite{CZWBHZ21} demonstrated that such methods remain vulnerable to membership inference attacks \cite{SSSS17,SZHBFB19} when adversaries have access to both the original and unlearned models.
Approximate unlearning methods trade theoretical guarantees for computational efficiency. \citet{GNG21} introduced Amnesiac Unlearning, which tracks model updates and their corresponding training batches, enabling selective removal by ignoring specific updates. Various other approximate techniques have been proposed \cite{GGHM23,GAS20,WHS22,MPSR22,ER23}, each offering different trade-offs between unlearning effectiveness and computational cost.


While the recent GoldFish loss \cite{HWJKKSSSGBG24} similarly focuses on verbatim memorization, it operates during the training phase and cannot be directly applied to pretrained models. Our work adapts and extends these concepts for post-training modification, introducing novel techniques for maintaining model utility while targeting specific memorized sequences.


\subsection{Copyright Protection}
Recent work on copyright protection in language models has explored several approaches. Watermarking techniques \cite{LWWG23,SDSJ20,SFDDF24} enable detection of copyrighted content by embedding identifiable markers in training data—if a model generates text containing these watermarks, it indicates training on copyrighted content. However, these approaches focus on detection rather than prevention of copyright violations.
In practice, companies primarily rely on filtering mechanisms to detect and block the generation of copyrighted content, such as Azure's content filters, and alignment techniques that teach models to not generate copyrighted content. \system{} improves upon these approaches by modifying the model's parameters directly with a targeted loss function, making it inherently resistant to generating copyrighted content during normal usage without requiring additional computational overhead during inference from classifiers or blocklists.

We believe our work lies between complete unlearning and simple output filtering. While unlearning approaches aim for complete removal of concepts at significant computational cost, and filtering approaches operate only at inference time, \system{} provides an efficient method to modify pretrained models to comply with copyright restrictions without sacrificing utility or requiring expensive retraining. This makes it suitable for patching models to prevent generation of newly identified or reported copyright concerns in deployed models.


\begin{figure*}[!t]
\centering
\includegraphics[width=1.5\columnwidth]{figs/unmemorize.png}
\caption{
Visualization of \system{}. Top of the figure shows an excerpt from "Harry Potter and the Goblet of Fire," where tokens designated for unmemorization are {\colorbox{orange}{highlighted}}, using a stride of 10. Below, the three-stage process of distribution transformation are shown: (a) the initial prediction distribution($\mathbf{z}$), representing the model's original probabilities for the \redroundedbox{pointing} token; (b) the target distribution after removing the target token, which serves as the post-forgetting target (${\mathbf{z}{\setminus x_\texttt{target}}}$); and (c) the final normalized distribution ($\text{softmax}\left({\mathbf{z}{\setminus x_\texttt{target}}}\right)$) used for computing the forgetting loss $\floss$.
}
\label{fig:mainFig}
\end{figure*}


\section{Methodology}

\subsection{Problem Formulation}
Consider a pretrained language model $\model$ with parameters $\theta$ that has potentially memorized sensitive sequences/text $\txt$ from its training data. Our goal is to modify $\model$ such that it cannot reproduce these sequences verbatim while maintaining its ability to generate contextually appropriate text, i.e., without affecting $\model$'s utility.


This approach differs from unlearning, where the model $\model$ is expected to completely forget the concept of $\txt$ and not only avoid verbatim reconstruction of $\txt$. In fact, we aim for \system{} to retain the concept of $\txt$, meaning the model still understands what "Harry Potter" is and can answer questions about it, but without reproducing verbatim text from the Harry Potter books. 


We believe this definition of unmemorization is crucial as it simplifies the unlearning problem and aligns with how most AI companies are addressing copyright issues. For example, Anthropic is discussing making deals with copyright owners to prevent exact reconstruction of their content\cite{reuters}.




\subsection{\system{}}
\label{sec:meth}
We take inspiration from the ``GoldFish'' loss \cite{HWJKKSSSGBG24} and adapt it to post-training, i.e., where the model is already trained and produces memorized --copyright-protected-- content. Intuitively, \system{} aims to perturb certain tokens of the memorized text $\txt$ to divert the model $\model$ from outputting $\txt$, thereby generating entirely different text.

To achieve this, \system{} operates by defining a stride $\stride$, which it sweeps over the complete memorized text $\txt$ as a window. This stride $\stride$ is used to identify the target tokens for unmemorization, essentially skipping $\stride$ tokens after each target token to be unmemorized. For each of these selected tokens, we use the forget loss $\floss$ to forget these tokens, while employing the maintain loss $\mloss$ to preserve the behavior of the model. The top of \autoref{fig:mainFig} shows an example of text from a Harry Potter book with the target tokens selected based on a stride of 10.

\mypara{Forget Loss}
$\floss$ employs KL-divergence (KL-div) on the output distribution of the target model, but with the top predicted token (the target token to unmemorize) removed. For this work, we select the top-10 probabilities to apply the KL-div and normalize them to compensate for the effect of removing the top token. 
The bottom of \autoref{fig:mainFig} shows an example of the constructed distributions of a target token `` pointing''.
More formally, $\floss$ is defined as follows:



\[
\floss = \text{KL}\left(\text{softmax}\left({\mathbf{z}}\right) \Bigg|\Bigg| \text{softmax}\left({\mathbf{z}{\setminus x_\texttt{target}}}\right)\right)
\]

where $\mathbf{z}$ represents the top-10 logits of the target model before removing the top predicted token, $\mathbf{z}{\setminus x_\texttt{target}}$ represents the logits after removing the top predicted token (the target token to unmemorize), and $x_\texttt{target}$ is the target token to unmemorize.


\mypara{Token Selection}
To ensure that the model's utility remains uncompromised, we selectively retain certain tokens even if they have been identified for unmemorization. Specifically, we exclude special tokens such as ``start of text'' and ``end of text'' to prevent damage to the model's templates. Furthermore, we also exclude tokens with a 100\% probability, as these tokens lack suitable alternative candidates. In other words, any replacement token at this location would be equally probable with a probability of 0\%, rendering the candidate selection random and potentially pushing the model towards an undesired random direction, thus affecting its utility.



\mypara{Maintain Loss}
$\mloss$ is designed to ensure that the overall behavior of the model remains unchanged. This is typically achieved by minimizing the kl-divergence difference in output distributions between the original and the modified models across non-target tokens. We opt for KL-divergence loss over the standard cross-entropy loss for the maintain loss, as using cross-entropy loss could result in further memorization of the non-target tokens. Therefore, KL-divergence loss is employed to maintain the consistency of the model's behavior without inducing additional memorization as a by-product.





By combining these two losses, \system{} effectively unmemorizes specific tokens without compromising the model's utility. The final loss function that \system{} optimizes can be expressed as:

\[
\mathcal{L} = \lambda_f \floss + \lambda_m \mloss
\]

where $\lambda_f$ and $\lambda_m$ are hyperparameters that balance the contributions of the forget loss and the maintain loss, respectively.



\section{Evaluation}
\label{sec:eval}
\subsection{Experimental Setup}
\subsubsection{Models}
We evaluate \system{} on four large language models of varying sizes and architectures:
\begin{itemize}
\item LLaMA-3.1 8B (Llama): Base model without instruction tuning \cite{llama3}
\item LLaMA-3.1-instruct 8B (Llama-Inst): Instruction-tuned variant \cite{llama3}
\item Qwen-2.5-7B (Qwen): Base model \cite{qwen2.5}
\item Yi-1.5 6B (Yi): Base model \cite{Yi1.5}
\end{itemize}
\subsubsection{Datasets}
\label{sec:Data}
We evaluate unmemorization on two different types of datasets:

\mypara{Synthetic Dataset} We create a synthetic dataset consisting of 10 articles, each approximately 380 words (around 2,000 characters) in length, covering diverse topics. These articles are specifically generated to test the effectiveness of unmemorization on deeply memorized content, where the probability of reconstructing the sequence exceeds 99\%. Using synthetic data ensures that the content does not exist in the models' pretraining data, thereby providing a controlled evaluation environment.


\mypara{Organic Dataset} We evaluate on excerpts from widely-read literary works, including ``Harry Potter and the Philosopher's Stone", ``Moby Dick", ``Frankenstein", ``Adventures of Huckleberry Finn", ``Alice in Wonderland", ``Sherlock Holmes", ``A Tale of Two Cities", ``Treasure Island", ``Les Misérables", and ``Great Expectations". These works are particularly relevant as their content has been identified in publicly available models.


\mypara{Benchmarks}
To assess model utility preservation, we evaluate performance on four standard benchmarks, namely HellaSwag \cite{ZHBFC19}, MMLU \cite{HBBZMSS21,HBBCLSS21}, TruthfulQA \cite{LHE22}, and Winogrande \cite{SLBC19}.



\subsection{Metrics}
We evaluate \system{} using three distinct metrics:

\mypara{Longest Common Subsequence (LCS)}
We begin by measuring the longest common subsequence of words between the generated data and the ground truth data, which the model is assumed to memorize. A higher LCS indicates a greater degree of memorization by the model.

\mypara{Edit Distance (ED)}
Next, we assess the edit distance using the Levenshtein distance, calculated based on words rather than characters. A lower ED suggests a higher level of memorization by the model.
For both LCS and ED, we ignore differences in capitalization.

\mypara{ROUGE-2 Score (ROUGE-2)}
Lastly, we compute the ROUGE-2 score, which evaluates the overlap between bigrams in the generated data and the ground truth data. This metric provides another perspective on the model's ability to replicate the training data.


\begin{figure*}[!t]
\centering
\begin{subfigure}{0.49\textwidth}
\centering
\includegraphics[width=1\columnwidth]{figs/allModels_synthetic_lcs_word_length.pdf}
\caption{Synthetic (LCS $\downarrow$)}
\label{fig:allModels_synthetic}
\end{subfigure}
\begin{subfigure}{0.49\textwidth}
\centering
\includegraphics[width=1\columnwidth]{figs/allModels_pretrain_lcs_word_length.pdf}
\caption{Organic (LCS $\downarrow$)}
\label{fig:allModels_pretrain}
\end{subfigure}
\begin{subfigure}{0.49\textwidth}
\centering
\includegraphics[width=1\columnwidth]{figs/allModels_synthetic_editDistance_words.pdf}
\caption{Synthetic (ED $\uparrow$)}
\label{fig:allModels_synthetic_ed}
\end{subfigure}
\begin{subfigure}{0.49\textwidth}
\centering
\includegraphics[width=1\columnwidth]{figs/allModels_pretrain_editDistance_words.pdf}
\caption{Organic (ED $\uparrow$)}
\label{fig:allModels_pretrain_ed}
\end{subfigure}

\begin{subfigure}{0.49\textwidth}
\centering
\includegraphics[width=1\columnwidth]{figs/allModels_synthetic_rouge2.pdf}
\caption{Organic (ROUGE-2 $\downarrow$)}
\label{fig:allModels_synthetic_rouge}
\end{subfigure}
\begin{subfigure}{0.49\textwidth}
\centering
\includegraphics[width=1\columnwidth]{figs/allModels_pretrain_rouge2.pdf}
\caption{Organic (ROUGE-2 $\downarrow$)}
\label{fig:allModels_pretrain_rouge}
\end{subfigure}
\caption{
The average performance of \system{} in unmemorizing 10 samples across different models, measured using longest Common Subsequence (LCS), edit distance (ED) at the word level, and ROUGE-2 score. \autoref{fig:allModels_synthetic}, \autoref{fig:allModels_synthetic_ed} and \autoref{fig:allModels_synthetic_rouge} display the results for synthetic data, while \autoref{fig:allModels_pretrain}, \autoref{fig:allModels_pretrain_ed} and \autoref{fig:allModels_pretrain_rouge} present the results for organic data.
}
\label{figure:allModelsRes}
\end{figure*}


\subsection{Results}
We evaluate \system{} through two main experiments: (1) unmemorizing synthetically memorized data (simulating deeply memorized), and (2) unmemorizing organically memorized data present in the pre-trained models. For each scenario, we assess both the effectiveness of \system{} memorization and its effect on model utility.

\mypara{Unmemorizing Synthetic Data}
We first finetune the target models on 10 samples using our synthetic dataset until deep memorization is achieved. Intuitively, we finetune the model till its ability to reproduce the complete article given a prefix. We refer to these as ``Memorized'' models.


Next, we apply \system{} with a token stride of 5, modifying the token following every set of five tokens, and a modification rate of one token per modification. For the KL-divergence, we consider the top-10 probabilities. We implement early stopping  when the probability of the target token falls below a predetermined threshold.


To comprehensively evaluate memorization, we evaluate different testing settings that varies prefix offset positions, i.e., where does the prefix start, from 0 to 200 tokens at 50-token intervals, and prefix lengths ranging from 8 to 20 tokens. We examine both greedy decoding (Greedy) and temperature-based sampling (temperature = 0.6, the default for Llama). For each sample, we report the best performing metrics across all combinations, i.e., the maximum score for Longest Common subsequence (LCS) and ROUGE-2, and the minimum score for Edit Distance (ED). Finally, for the instruct version of the model, we precede the prefix with the following instruction: ``Generate the entire rest of this text from the start, continuing until you reach the end: ''.


\autoref{figure:allModelsRes} presents our main results. The LCS metric (\autoref{fig:allModels_synthetic}) reveals that memorized models initially reproduce nearly complete articles exceeding 360 words, while \system{} reduces this to merely 4-5 words— representing a two orders of magnitude reduction in memorization. This finding is further confirmed by the Edit Distance (\autoref{fig:allModels_pretrain_ed}), which increases from 5-6 words to over 60 words post-\system{}. The ROUGE-2 scores (\autoref{fig:allModels_synthetic_rouge}) similarly demonstrate a significant reduction in verbatim reproduction.


\mypara{Unmemorizing Organic Data}
Our second experiment focuses on organically memorized data present in publicly released models, i.e., without any additional fine-tuning from us. Using the dataset described in \autoref{sec:Data}, we apply the same \system{} parameters and evaluation methodology as in the synthetic experiments. Results demonstrate comparable effectiveness in removing organic memorization, with LCS (Figure \ref{fig:allModels_pretrain}) reduced to 2-5 words. Both ED (Figure \ref{fig:allModels_pretrain_ed}) and ROUGE-2 (Figure \ref{fig:allModels_pretrain_rouge}) metrics show reduction of memorization to negligible levels. Additionally, we present an example in the appendix (\autoref{fig:compText}) illustrating how text generation diverges after unmemorization, resulting in completely different content.



To provide more insight into \system{}'s effectiveness, we present a sample-level analysis in appendix \autoref{fig:allModelsIndRes}. The per-sample Longest Common Subsequence measurements reveal the consistent significant memorization reduction across all models. While we focus on LCS as an example for the individual samples performance, other metrics (Edit Distance and ROUGE-2) exhibit similar patterns. 

\mypara{\system{} Effect on Utility}
We assess the impact on model utility by comparing \system{}-unmemorized models against their respective baselines, i.e., the target models on which \system{} was applied, which are the memorized models for synthetic data and the original models for organic data, across standard benchmarks. Tables \ref{tab:benchmarkAllSyn} and \ref{tab:benchmarkAllOrg} demonstrate the stable performance of \system{}, with degradation consistently remaining below 1\% across all evaluated tasks. This indicates that \system{} successfully preserves model utility while effectively removing memorized content.


These results demonstrate that \system{} achieves strong unmemorization performance while maintaining model utility, presenting an efficient approach for selective knowledge unmemorization in large language models. Furthermore, they show that \system{} effectively generalizes across various model architectures, sizes, and types, including both base and instruction-tuned models. The consistent performance across both synthetic and organic memorization scenarios shows the robustness of our approach across different memorization contexts.





\begin{table}[h!]  
\centering  
\caption{Benchmarking \system{} when Targeting Synthetic Data.}  
\resizebox{\linewidth}{!}{
\begin{tabular}{l@{\hskip 6pt}l@{\hskip 6pt}c@{\hskip 6pt}c@{\hskip 6pt}c@{\hskip 6pt}c}    
\toprule  
\textbf{Model} & \textbf{Type} & \textbf{MMLU} & \textbf{TruthfulQA} & \textbf{HellaSwag} & \textbf{Winogrande} \\  
\midrule  
\multirow{2}{*}{\textbf{Llama }} & Mem & 0.6363 & 0.4865 & 0.6084 & 0.7403 \\  
 & UnMem & 0.6349 & 0.4896 & 0.6124 & 0.7514 \\  
\midrule  
\multirow{2}{*}{\textbf{Llama-Inst }} & Mem & 0.6812 & 0.5387  & 0.5937  &  0.734 \\  
 & UnMem & 0.6812  &  0.5367  &  0.6 & 0.7411  \\  
\midrule  
\multirow{2}{*}{\textbf{Yi}} & Mem & 0.6237 & 0.4469 & 0.5723 & 0.7214 \\  
 & UnMem & 0.6187 & 0.4520 & 0.5760 & 0.7198 \\  
\midrule  
\multirow{2}{*}{\textbf{Qwen}} & Mem & 0.7182 & 0.5596 & 0.6032 & 0.7064 \\  
 & UnMem & 0.7169 & 0.5595 & 0.6134 & 0.6851 \\  
\bottomrule  
\end{tabular}  
}  
\label{tab:benchmarkAllSyn}
\end{table}  








\begin{table}[h!]  
\centering  
\caption{Benchmarking \system{} when Targeting Organic Data.}  
\resizebox{\linewidth}{!}{
\begin{tabular}{l@{\hskip 6pt}l@{\hskip 6pt}c@{\hskip 6pt}c@{\hskip 6pt}c@{\hskip 6pt}c}    
\toprule  
\textbf{Model} & \textbf{Type} & \textbf{MMLU} & \textbf{TruthfulQA} & \textbf{HellaSwag} & \textbf{Winogrande} \\  
\midrule  
\multirow{2}{*}{\textbf{Llama}} & Orig & 0.6324 & 0.4518 & 0.6002 & 0.7372 \\  
 & unMem & 0.6350 & 0.4501 & 0.6025 & 0.7356 \\  
\midrule  
\multirow{2}{*}{\textbf{Llama-Inst}} & Orig & 0.679  & 0.5402 & 0.5905  &   0.738 \\  
 & unMem & 0.6772  & 0.5521 & 0.5933  &  0.7403 \\  
\midrule  
\multirow{2}{*}{\textbf{Yi}} & Orig & 0.6242 & 0.4407 & 0.5663 & 0.7198 \\  
 & unMem & 0.6276 & 0.4446 & 0.5671 & 0.7198 \\  
\midrule  
\multirow{2}{*}{\textbf{Qwen}} & Orig & 0.7171 & 0.5637 & 0.6000 & 0.7340 \\  
 & unMem & 0.7169 & 0.5737 & 0.5982 & 0.7159 \\  
 \bottomrule  
\end{tabular}  
}  
\label{tab:benchmarkAllOrg}
\end{table}  







\subsection{Ablation Study}
We conduct extensive ablation studies to analyze the sensitivity of \system{} to various hyperparameters. Our analysis focuses on Llama and Qwen models using representative samples from both synthetic and organic datasets. 


\mypara{Top-k}
We investigate the sensitivity of both forget ($\floss$) and maintain ($\mloss$) losses to the number of probability distributions considered, examining $k \in \{5, 10, 20\}$. Results shown in \autoref{fig:topK_synthetic} for synthetic data and \autoref{fig:topK_pretrain} in the appendix for organic data demonstrate remarkable stability across different $k$ values. This consistency extends to model utility metrics, suggesting that \system{} is robust to this hyperparameter choice at even modest coverage of the target probability distribution.


\mypara{Stride} 
The token stride parameter determines the granularity of unmemorization by controlling the spacing between modified tokens. We examine stride values spanning multiple orders of magnitude: {1, 2, 5, 10, 20, 50}. \autoref{fig:strideSyn} presents the Longest Common Subsequence (LCS) results for synthetic data, with corresponding organic data results shown in \autoref{fig:stripretrain} of the appendix.


Our results reveal the expected inverse relationship between stride length and unmemorization effectiveness, where decreasing stride length generally increases unmemorization. This is expected due to more token modifications that disrupt verbatim generation. However, it is important to note that the unmemorization remains significant even for larger strides. For example, a stride of 50—modifying only $\approx 2\%$ of tokens—\system{} reduces the LCS from over 380 tokens to 68 tokens. From these results, we observe that the unmemorization performance saturates for strides up to 5, where we see nearly identical unmemorization performance. Therefore, we use a stride of 5 in our experiments.





The impact on model utility, illustrated in \autoref{fig:strideBenchLlama} for Llama on synthetic data (with complete results in \autoref{figure:strideRem}), demonstrates significant robustness of \system{} with a negligible effect on model utility. Contrary to initial intuition, even an aggressive stride setting of 1 (modifying every other token) maintains model utility with minimal degradation. A closer examination reveals that this behavior is explained by the probability distribution of each token remaining similar with respect to the top 9 probabilities during unmemorization (after removing the target token). We hypothesize that maintaining this distribution over the top 9 probabilities is what helps the model preserve its utility. To further validate this hypothesis, we tested unmemorizing with a stride of 5 without applying the maintain loss $\mloss$. As anticipated, the model diverged and experienced a significant drop in utility.



Finally, to evaluate the extreme scenario of modifying every token, we evaluated a stride of 0, effectively unmemorizing all tokens. In this case, model utility declined more significantly, with a 7\% drop in the TruthfulQA benchmark performance and a tendency for the model to repeat words during manual evaluation.



\begin{figure}[!t]
\centering
\includegraphics[width=1\columnwidth]{figs/topK_synthetic.pdf}
\caption{
The effect of varying the Top-k probabilities for $\floss$ and $\mloss$ using synthetic data.}
\label{fig:topK_synthetic}
\end{figure}


 \begin{figure*}[!t]
\centering
\begin{subfigure}{0.49\textwidth}
\centering
\includegraphics[width=1\columnwidth]{figs/stride_synthetic.pdf}
\caption{Synthetic (LCS $\downarrow$)}
\label{fig:strideSyn}
\end{subfigure}
\begin{subfigure}{0.49\textwidth}
\centering
\includegraphics[width=1\columnwidth]{figs/Benchmarking_stride_synthetic_llama3.1-8b.pdf}
\caption{Benchmark Llama Synthetic}
\label{fig:strideBenchLlama}
\end{subfigure}
\caption{
Performance of \system{} with varying stride lengths. \autoref{fig:strideSyn} illustrates the LCS on synthetic data, while \autoref{fig:strideBenchLlama} presents benchmark results for synthetic data on the Llama model.}
\label{figure:stride}
\end{figure*}









\mypara{Candidate Token Selection Strategy} 
Our final ablation examines the token candidate selection strategy for the forget loss ($\floss$). We compare two approaches: (1) direct selection from top-k probabilities, and (2) selection ensuring matching capitalization and leading space patterns for the $k$ alternate tokens. Experimental results show negligible performance differences between these strategies in both memorization reduction and utility preservation.




These ablation studies collectively demonstrate that \system{} maintains robust performance across a wide range of hyperparameter configurations. The method exhibits particular stability in probability distribution coverage and candidate selection, while offering better unmemorization performance for smaller stride lengths without a loss in utility. We believe these results highlight the strong performance and stability of \system{} without the need to rely on precise hyperparameter tuning.



\section{Discussion and Limitations}

Our work introduces \system{} as a targeted solution for preventing verbatim reproduction in LLMs, addressing a specific but crucial aspect of the broader AI copyright landscape. While our results demonstrate significant success in preventing exact sequence reproduction, it is important to clarify both the strengths and limitations of our approach within the larger scope of AI intellectual property protection.


\subsection{Limitations} 
Despite \system{}'s effectiveness in preventing verbatim generation, it is important to acknowledge a key limitation: while \system{} successfully prevents exact sequence reproduction, it does not address the broader challenge of information extraction from language models. A determined adversary could potentially extract information about the unmemorized content through careful prompt engineering, such as using a series of yes/no questions or constructing prompts that elicit semantic information without requiring exact reproduction. This limitation is inherent to our focused approach on verbatim generation and reflects the fundamental trade-off between maintaining model utility and completely removing information. Our method's effectiveness relies on the assumption that normal usage patterns primarily involve direct generation rather than adversarial probing. While this assumption aligns with typical use cases and current copyright frameworks that focus on expression rather than ideas, it may not satisfy more stringent information security requirements.



\subsection{Broader Implications}
The success of \system{} in selectively mitigating verbatim reproduction while preserving semantic understanding further explore the area of knowledge representation in large language models. Our results indicate that surface-level text patterns can be modified effectively without significantly impacting semantic understanding, reinforcing the distinction between memorized sequences and learned concepts.


Furthermore, \system{} offers an efficient solution for model providers who need to address copyright concerns without resorting to complete retraining. By significantly reducing exact sequence matches while maintaining performance, \system{} aligns with emerging standards and ongoing discussions about copyright content protection.

\subsection{Challenges in Proving Training Data Usage}
\begin{figure}[!t]
\centering
\includegraphics[width=1\columnwidth]{figs/ACR_LLama_inst.pdf}
\caption{
Adversarial Compression Ratios (ACR) for the first five sentences of the Harry Potter using the original (pretrained) LLama-Instruct model and \system{} with strides of 5 and 2. An ACR below 1 is considered a failure, as it requires more than one input token for each output token.}
\label{fig:ACR_llamaInst}
\end{figure}

Beyond copyright considerations, our findings also have direct implications for research on determining whether a model has been trained on specific data. Traditional membership inference attacks alone are insufficient for proving such claims, as recent work suggests that true verification requires data reconstruction, often using special canary data \cite{ZDKT24}. However, these proofs typically depend on verbatim reproduction—something that \system{} effectively prevents.

As a result, once a model has been unmemorized using techniques like \system{}, existing approaches to training data verification may become less effective. To validate this, we focus on a recent approach\cite{SFMLK24} that tests data memorization using Adversarial Compression. This method optimizes an adversarial prefix to prompt the model into outputting the target memorized text, such as copyrighted material. The effectiveness of this approach is measured by the length of the prefix needed to generate the target text: the shorter the prefix, the more memorized the data is, indicating a higher compression rate.


We evaluate the Adversarial Compression Ratio (ACR) both with and without \system{}. Using the first five sentences of Harry Potter as our test case, we compare the ACR between the original model and versions processed with \system{} using strides of 5 and 2. \autoref{fig:ACR_llamaInst} presents results for the LLaMA-Instruct model, with additional results for LLaMA and Qwen models provided in Appendix (\autoref{figure:acr_other}). As hypothesized, applying \system{} significantly reduces the ACR below the acceptance threshold (1), indicating successful unmemorization of the samples.


To further validate our findings, we conducted additional experiments using synthetic modified/rephrased versions of the original Harry Potter sentences that the models had never encountered during training, an example is shown in Appendix (\autoref{fig:ACR_syntExample}). The ACR values, shown in the Appendix (\autoref{fig:acr_synt}) for these synthetic examples closely matched those of the original sentences after applying \system{}, demonstrating that our approach effectively transforms memorized content to appear computationally similar to unseen data while maintaining semantic coherence.






\section{Conclusion}
We introduced \system{}, a novel post-training approach to prevent verbatim text reproduction in large language models by modifying token distributions. Our results across multiple architectures show that precise control over memorized content can be achieved without compromising model utility. The success of token-level interventions in preventing exact sequence generation while preserving semantic understanding suggests potential for new techniques in targeted knowledge modification in neural networks, especially for selective removal of specific patterns while maintaining underlying learned representations.


\bibliography{ref}
\bibliographystyle{icml2025}


\newpage
\appendix
\onecolumn
\section{Appendix}


\begin{figure}[h!]  
\centering  
\begin{minipage}{0.9\textwidth}  
\textbf{Original Paragraph:}\\  
ley, of number four, Privet Drive, were proud to say that they were perfectly normal, thank you very much. They were the last people you'd expect to be involved in anything strange or mysterious, because they just didn't hold with such nonsense. Mr. Dursley was the director of a firm called Grunnings, which made drills. He was a big, beefy man with hardly any neck, although he did have a very large mustache. Mrs. Dursley was thin and blonde and had nearly twice the usual amount of neck, which came in very useful as she spent so much of her time craning over garden fences, spying on the neighbors. The Dursleys had a small son called Dudley and in their opinion there was no finer boy anywhere.  
\end{minipage}  

\begin{minipage}{0.9\textwidth}  
\textbf{Generated Paragraph After Unmemoriazton:}\\  
ley, of \textcolor{red}{4}, Privet Drive, were proud to say that \textcolor{red}{their son, Donald, had never once gone through the hedge in his life. 
The Dursley family had been living at Number Four, Privet Drive, for only a matter of weeks now, and Mrs. Dursley had been forced to have her husband's old bicycle up on its stand, because it had no wheel. And that matter of the broken window at the back. That had never been exactly explained, to her satisfaction.  
Mr. Dursley's business had kept him away from home lately, and Mrs. Dursly had several times lost count of the letters she had written to her neighbourly friend Mrs. Pearce, who looked after things during her absences. Mrs. Dursly  } 
\end{minipage}  
\caption{A demonstration of how the model diverges from copyrighted content following unmemorization by the system. The differences are highlighted in red in the generated text.}  
  \label{fig:compText}
  \end{figure}  




 \begin{figure*}[!t]
\centering
\begin{subfigure}{0.49\textwidth}
\centering
\includegraphics[width=1\columnwidth]{figs/lcs_word_length_llama3.1-8b_synthetic.pdf}
\caption{Synthetic Llama}
\end{subfigure}
\begin{subfigure}{0.49\textwidth}
\centering
\includegraphics[width=1\columnwidth]{figs/lcs_word_length_llama3.1-8b_pretrain.pdf}
\caption{Organic Llama}
\end{subfigure}

\begin{subfigure}{0.49\textwidth}
\centering
\includegraphics[width=1\columnwidth]{figs/lcs_word_length_llama3.1-8b-instruct_synthetic.pdf}
\caption{Synthetic Llama-Instruct}
\end{subfigure}
\begin{subfigure}{0.49\textwidth}
\centering
\includegraphics[width=1\columnwidth]{figs/lcs_word_length_llama3.1-8b-instruct_pretrain.pdf}
\caption{Organic Llama-Instruct}
\end{subfigure}

\begin{subfigure}{0.49\textwidth}
\centering
\includegraphics[width=1\columnwidth]{figs/lcs_word_length_qwen2.5-7b_synthetic.pdf}
\caption{Synthetic Qwen}
\end{subfigure}
\begin{subfigure}{0.49\textwidth}
\centering
\includegraphics[width=1\columnwidth]{figs/lcs_word_length_qwen2.5-7b_synthetic.pdf}
\caption{Organic Qwen}
\end{subfigure}

\begin{subfigure}{0.49\textwidth}
\centering
\includegraphics[width=1\columnwidth]{figs/lcs_word_length_yi1.5-8b_synthetic.pdf}
\caption{Synthetic Qwen}
\end{subfigure}
\begin{subfigure}{0.49\textwidth}
\centering
\includegraphics[width=1\columnwidth]{figs/lcs_word_length_yi1.5-8b_pretrain.pdf}
\caption{Organic Yi}
\end{subfigure}
\caption{
Performance of \system{} across various models using the Longest Common subsequence (LCS) metric on individual samples.}
\label{fig:allModelsIndRes}
\end{figure*}






\begin{figure*}[!t]
\centering
\begin{subfigure}{0.49\textwidth}
\centering
\includegraphics[width=1\columnwidth]{figs/stride_pretrain.pdf}
\caption{Organic (LCS $\downarrow$)}
\label{fig:stripretrain}
\end{subfigure}
\begin{subfigure}{0.49\textwidth}
\centering
\includegraphics[width=1\columnwidth]{figs/Benchmarking_stride_pretrain_llama3.1-8b.pdf}
\caption{Benchmark Llama Synthetic}
\end{subfigure}
\begin{subfigure}{0.49\textwidth}
\centering
\includegraphics[width=1\columnwidth]{figs/Benchmarking_stride_synthetic_qwen2.5-7b.pdf}
\caption{Benchmark Qwen Synthetic}
\end{subfigure}
\begin{subfigure}{0.49\textwidth}
\centering
\includegraphics[width=1\columnwidth]{figs/Benchmarking_stride_pretrain_qwen2.5-7b.pdf}
\caption{Benchmark Qwen Organic}
\end{subfigure}
\caption{
Performance of \system{} with varying stride lengths.}
\label{figure:strideRem}
\end{figure*}



\begin{figure}[!t]
\centering
\includegraphics[width=0.5\columnwidth]{figs/topK_pretrain.pdf}
\caption{
The effect of varying the Top-k probabilities for $\floss$ and $\mloss$ using organic data.}
\label{fig:topK_pretrain}
\end{figure}




\begin{figure*}[!t]
\centering
\begin{subfigure}{0.49\textwidth}
\centering
\includegraphics[width=1\columnwidth]{figs/ACR_qwen.pdf}
\caption{Qwen}
\end{subfigure}
\begin{subfigure}{0.49\textwidth}
\centering
\includegraphics[width=1\columnwidth]{figs/ACR_LLama.pdf}
\caption{Llama}
\end{subfigure}
\caption{
Adversarial Compression Ratios (ACR) for the first five sentences of the Harry Potter using the original (pretrained) LLama and Qwen models and \system{} with strides of 5 and 2. An ACR below 1 is considered a failure, as it requires more than one input token for each output token..}
\label{figure:acr_other}
\end{figure*}


\begin{figure}[h!]  
\centering  
\begin{minipage}{0.9\textwidth}  
\textbf{Original Sentence:}\\  
Mr. and Mrs. Dursley, of number four, Privet Drive, were proud to say that they were perfectly normal, thank you very much.
\end{minipage}  

\begin{minipage}{0.9\textwidth}  
\textbf{Synthetic/Rephrased Sentence:}\\  
Mr. and Mrs. Dursley, \textcolor{red}{residents of 4} Privet Drive,\textcolor{red}{ delighted in declaring themselves entirely ordinary, if you please}. 
\end{minipage}  
\caption{An example of the synthetic/rephrased sentences for testing Adversarial Compression Ratios (ACR).}  
  \label{fig:ACR_syntExample}
  \end{figure}  
\begin{figure}[!t]
\centering
\includegraphics[width=0.5\columnwidth]{figs/ACR_syn.pdf}
\caption{
Adversarial Compression Ratios (ACR) for the fake sentences.}
\label{fig:acr_synt}
\end{figure}




\end{document}
