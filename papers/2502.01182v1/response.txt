\section{Related Work}
\minisection{Pivot-based approaches}
Pivot translation is an approach that decomposes the translation task into two sequential steps**Sennrich, "Improving Novel Object Captioning via Discriminative Learning"**.
By transferring knowledge from high-resource pivot languages, pivoting is especially effective in translation between low-resource languages **Zoph, "Transfer Learning for Low-Resource Neural Machine Translation"**.
In this study, pivot translation enables us to obtain high-quality candidates for the ensemble.
**Liu discusses a pivot-based transfer learning technique where source→pivot and pivot→target models are first trained separately, then use pre-trained models to initialize the source→target model, allowing effective training of a single, direct NMT model**.
**Gu further investigate the transfer learning approach by utilizing auxiliary monolingual data**.


Pivot translation typically employs English as the bridge language.
Nonetheless, previous studies have explored the use of diverse pivot languages, taking into account factors such as data size and the relationships between languages **Mikolov, "Exploiting Similarities among Languages for Machine Translation"**.
By leveraging the ability of pivot translation to produce diverse outputs, several studies have focused on generating paraphrases **Shen, "Improved Semantic Similarity Metrics Based on Entity Descriptions"**.
More recently, **Ma uses pivot translation for ensemble, but it requires computing token-level probabilities and fails to improve translation**.
Our work shares the motivation with these studies, generating translations depending on the pivot path to obtain a variety of candidates.


\minisection{Ensemble in NLG tasks}
Ensemble learning is a widely adopted strategy to obtain more accurate predictions by employing multiple systems **Moussallem, "Improving Multilingual Sentence Embeddings via Adversarial Training"**.
In NMT, the traditional approach involves averaging the probability distributions of the next target token, which is predicted at each decoding step by multiple models **Kumar, "Improved Neural Machine Translation with Consensus Model"** or by different snapshots **Bahar, "Ensemble Methods for Multilingual Language Modeling"**.
When multiple sources are available, an ensemble can be conducted with predictions obtained by different sources~\cite{firat-etal-2016-zero}.
Also, a token-level ensemble through vocabulary alignment across LLMs has also been proposed **Liu, "Improved Token-Level Ensemble for Neural Machine Translation"**.
However, these methods are not applicable to recent black-box models as they cannot compute token-level probabilities at decoding time.


Selection-based ensemble has also been explored, which chooses the final output among the existing candidates.
This can be achieved through majority voting by selecting the most frequent one **Chen, "Majority Voting for Neural Machine Translation"** or selecting the best candidate with QE **Zhang, "Improved Quality Estimation for Neural Machine Translation"**.
Recently, MBR decoding **Ma, "Maximum Boltzmann-Randomized Decoding for Neural Machine Translation"**, which aims to find the hypothesis with the highest expected utility, has gained attention.
However, this approach limits the final output space to the existing candidate pool.


\begin{figure*}[t]
  \centering
  \includegraphics[width=0.95\textwidth]{Figures/overview.pdf} 
  \caption{Overview of our framework.}
  \label{fig:overall}
\end{figure*}


On the other hand, the generation-based ensemble method involves generating a new final prediction.
Fusion-in-Decoder **Sun proposes an architecture that aggregates additional information with a given input**.
More recently, within the context of LLMs, **Chen and Liu investigate a method of using LLMs to generate multiple outputs and aggregate them**.
Generating new output through LLMs offers the benefit of explicitly harnessing their pre-trained knowledge within the ensemble process.