\section{Introduction}


Neural machine translation (NMT) models exhibit outstanding capabilities when a large volume of the parallel corpus is available (e.g., translate from and to English).
However, their performance still falls short in cases involving low-resource languages (e.g., Basque) and translating between non-English languages from different language families (e.g., German-Russian)~\cite{artetxe2018unsupervised}.
Top-performing large language models (LLMs), such as GPT models~\cite{gpt3.5}, also demonstrate suboptimal translation performance in low-resource language pairs~\cite{robinson-etal-2023-chatgpt, moslem-etal-2023-adaptive}. 
The scarcity of parallel data, primarily due to limited cultural interaction, makes the low-resource translation task more challenging.


In many generation tasks, ensembling multiple systems has proven to be a successful strategy for performance enhancement.
In NMT, traditional ensemble methods average probability distributions over output tokens from multiple models during decoding.
However, the expensive cost of training multiple models is the primary shortcoming of ensemble decoding.
Additionally, computing token-level probabilities at each decoding step is not feasible with recent black-box models such as GPT-4o and Gemini~\cite{gpt4o, geminiteam2023gemini}.


Ensemble methods that can be utilized even when token-level probabilities cannot be computed have also been proposed.
Selection-based ensemble method involves generating candidates from multiple models and then selecting the best candidate among them~\cite{wang2022rationaleaugmented, howgood}.
However, in this ensemble fashion, the final output space is limited to the existing candidate pool.
In contrast, the generation-based ensemble, such as \blender~\cite{llm-blender}, creates improved outputs using candidates obtained from multiple models.
This approach aims to generate a final output superior to the existing candidates.
Nonetheless, the main drawback from the notably high cost of generating candidates through multiple models remains, inducing computational overhead.
As the size of the models used in the ensemble increases, the cost proportionally escalates, becoming more burdensome.
In addition, due to the varying performance of MT systems, the quality of some candidates can be significantly lower than others, leading to a degradation in the overall performance.


To alleviate the problems above, we propose \textbf{Pivot}-based single model \textbf{E}nsemble (\ours), a novel generation-based approach.
Our intuition of a single model ensemble primarily stems from pivot translation, which can produce diverse and more accurate translations.
Pivot translation~\cite{wu-wang-2007-pivot, utiyama-isahara-2007-comparison} is a method that splits the end task into two sequential steps: source$\rightarrow$pivot and pivot$\rightarrow$target.
Pivoting has been employed to enhance low-resource translation by transferring knowledge from high-resource pairs.
In many cases, English, being a resource-rich language, serves as the intermediate language.
However, we employ not only English but various pivot languages for candidate generation, thereby producing diverse hypotheses using a single model.

In the next aggregation step, we select the top candidates for the ensemble and merge them to generate the final output.
Since the quality of candidates directly impacts the results of the ensemble, it is important to select high-quality candidates.
Given that the best pivot language for translation varies with each source sentence, we hence select the top-$\textit{k}$ candidates for each source sentence via quality estimation (QE).
By leveraging diverse candidates from pivot translation and knowledge of the merging module, \ours generates final translations that accurately convey the meaning and subtle nuances of the source sentence, superior to selecting from pre-existing candidates.
Our contributions can be summarized as follows:

\begin{itemize}
    \item We propose a simple but effective pivot-based single model ensemble method, \ours, to improve low-resource MT.

    \item We show that a single model can effectively generate diverse and accurate hypotheses and that leveraging these candidates in an ensemble process can enhance translation quality while reducing computational overhead.

    \item The empirical results on various language pairs demonstrate that we consistently outperform state-of-the-art methods, validating the effectiveness of the pivot-based ensemble.
\end{itemize}