\begin{abstract}


Despite the significant advances in neural machine translation, performance remains subpar for low-resource language pairs.
Ensembling multiple systems is a widely adopted technique to enhance performance, often accomplished by combining probability distributions.
However, the previous approaches face the challenge of high computational costs for training multiple models.
Furthermore, for black-box models, averaging token-level probabilities at each decoding step is not feasible.
To address the problems of multi-model ensemble methods, we present a pivot-based single model ensemble.
The proposed strategy consists of two steps: pivot-based candidate generation and post-hoc aggregation.
In the first step, we generate candidates through pivot translation.
This can be achieved with only a single model and facilitates knowledge transfer from high-resource pivot languages, resulting in candidates that are not only diverse but also more accurate.
Next, in the aggregation step, we select $\textit{k}$ high-quality candidates from the generated candidates and merge them to generate a final translation that outperforms the existing candidates.
Our experimental results show that our method produces translations of superior quality by leveraging candidates from pivot translation to capture the subtle nuances of the source sentence.


\end{abstract}