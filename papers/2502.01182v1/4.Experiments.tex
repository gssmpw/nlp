\section{Experiments}

We use NVIDIA RTX 3090 or 4090 GPUs for experiments.


\subsection{Datasets}
\label{sec:datasets}


\begin{table}[h!]
\centering
\begin{adjustbox}{width=\columnwidth, center}
\renewcommand{\arraystretch}{0.9}
\begin{tabular}{ccccc}
\Xhline{3\arrayrulewidth}
\multirow{2}{*}{\textbf{Lang-pair}} & \multirow{2}{*}{\textbf{Dataset}} & \multicolumn{3}{c}{\textbf{\# Sentences}} \\ \cline{3-5} 
 & & \textbf{Train} & \textbf{Dev} & \textbf{Test} \\ \hline\hline 
\multirow{2}{*}{KO $\leftrightarrow$ IT} & TED 2020 v1& \multirow{2}{*}{357,733} & \multirow{2}{*}{2,000} & \multirow{2}{*}{2,000} \\
 & \cite{ted2020} & & & \\ \hline  
\multirow{2}{*}{AR $\leftrightarrow$ PT} & WikiMatrix v1& \multirow{2}{*}{153,441} & \multirow{2}{*}{2,000} & \multirow{2}{*}{2,000} \\
 & \cite{schwenk2019wikimatrix}& & & \\
\Xhline{3\arrayrulewidth}
\end{tabular}
\end{adjustbox}
\caption{Datasets statistics.}
\label{tab:dataset statistics}
\end{table}


We conduct experiments on the linguistically distant languages within pairs: not in the same language family and using different scripts.
We select 2 language pairs, resulting in 4 translation directions in total, Korean (\textit{Koreanic})$\leftrightarrow$Italian (\textit{Romance}) and Arabic (\textit{Arabic})$\leftrightarrow$Portuguese (\textit{Romance}).
The language family grouping is defined by the criteria presented in \citet{m2m100}.


We validate our approach across various domains.
For Korean$\leftrightarrow$Italian pair, we run experiments on TED2020~\cite{ted2020}.
For Arabic$\leftrightarrow$Portuguese pair, we use WikiMatrix~\cite{schwenk2019wikimatrix}.
All the datasets are obtained from the OPUS\footnote{\url{https://opus.nlpl.eu}}~\cite{opus} project.
The statistics for the datasets are listed in Table~\ref{tab:dataset statistics}.




\subsection{Evaluation Metrics}
We assess the translation quality using BLEU~\cite{papineni-etal-2002-bleu}, chrF++~\cite{popovic-2017-chrf}, and reference-based COMET (\textit{wmt22-COMET-da}) \cite{comet22}.
For reporting BLEU, \textit{SacreBLEU}~\cite{post-2018-call} is used with \texttt{ko-mecab} tokenizer for Korean and \texttt{13a} tokenizer for the others.


\subsection{Baselines}
\label{sec:baselines}

As an encoder-decoder NMT model, we use NLLB-200-distilled-600M~\cite{nllb}.
When training NLLB, we use the Transformers library from HuggingFace~\cite{wolf2020huggingfaces}.
AdamW optimizer~\cite{adamw} is used with a learning rate of $2e$$-$$5$, batch size of 2, and dropout with a probability of 0.1.
When validation BLEU was not improved for 3 checkpoints, with 30k steps between them, we stopped training.


For open-source LLMs, we use Vicuna 13B~\cite{Vicuna}, Baize 13B~\cite{baize}, and Llama-3-8B-Instruct~\cite{llama3modelcard} as the baseline. 
We fine-tuned these LLMs with QLoRA~\cite{dettmers2024qlora}; $r$=16, $\alpha$=64, dropout=0.1 for all linear layers.
For black-box LLMs, we use GPT-4~\cite{gpt4} and GPT-4o~\cite{gpt4o}.
The version of \texttt{gpt-4-1106-preview} and \texttt{gpt-4o-2024-08-06} are employed for GPT-4 and GPT-4o, respectively.
For GPT models, \textit{temperature} is set to 0.0 for stable responses~\cite{peng2023making} and \textit{top\_ p} is set to 0.1 to ensure reproducibility. 
For LLMs, we use the prompt template of~\citet{howgood}, as presented in Appendix~\ref{sec:apdx_prompt_templates}.


As state-of-the-art ensemble baselines, we employ \blender~\cite{llm-blender}, EVA~\cite{eva}, and MBR~\cite{mbr}.
For \blender and EVA, we fine-tuned the same open-source LLMs used in each study utilizing the parallel corpus described in Table~\ref{tab:dataset statistics}.
The list of the LLMs is in Appendix~\ref{sec:apdx_llms}.
\textit{temperature} is set to 0.1 to mitigate hallucination for low-resource pairs~\cite{guerreiro2023hallucinations}.
For MBR, we generate a set of 5 hypotheses using GPT-4.
When generating hypotheses, \textit{temperature} was set to 0.0 for its optimal performance, based on the results of our pilot experiments in Appendix~\ref{sec:MBR} and prior study~\cite{peng2023making}.
Other configurations are the same as in the original work~\cite{mbr}.

\subsection{Implementation Details}

In the candidate generation step of \ours, we employ \nllb.
For each source-target language pair, we use an NLLB fine-tuned for the language pair in Table~\ref{tab:dataset statistics} to generate the directly translated candidates.
For the merging module, we use Llama-3, GPT-4, and GPT-4o.
For all models used in \ours, including NLLB, Llama-3, GPT-4, and GPT-4o, we apply the same settings in \S\ref{sec:baselines}.


As detailed in \S\ref{sec:candidate aggregation}, we explore two approaches in the ensemble process: one dynamically selects the top-$\textit{k}$ ($\textit{k}$=3) candidates and another uses candidates obtained from fixed paths.
To select the top-$\textit{k}$ candidates for each source sentence, we use the reference-free COMETkiwi as described in \S\ref{sec:candidate aggregation}.
When selecting candidates from fixed paths, we used directly translated candidates and English-pivoted candidates, which were the top-performing paths on the FLORES-200 benchmark.


\subsection{Main Results}


\begin{table*}[t]
\centering
\renewcommand{\arraystretch}{0.9}
\begin{adjustbox}{width=0.95\textwidth, center}
\begin{tabular}{lcccccccccccc}
\Xhline{3\arrayrulewidth}

\multirow{2}{*}{\textbf{Model}}  & \multicolumn{3}{c}{\textbf{Korean$\rightarrow$Italian}} & \multicolumn{3}{c}{\textbf{Italian$\rightarrow$Korean}} & \multicolumn{3}{c}{\textbf{Arabic$\rightarrow$Portuguese}} & \multicolumn{3}{c}{\textbf{Portuguese$\rightarrow$Arabic}}\\ \cline{2-13}
 & BLEU & chrF++ & COMET & BLEU & chrF++ & COMET  & BLEU & chrF++ & COMET & BLEU & chrF++ & COMET \\ \hline\hline 

\textit{\textbf{Standalone NMT System}}\\ \hdashline[3pt/3pt]
NLLB~\cite{nllb} & 16.27 & 41.14 & 84.60 &            17.40 &           23.39 &           87.33 &           27.25 &           50.35 &           84.21 &            13.50 &            40.90 &           84.24 \\
Vicuna~\cite{Vicuna} &10.11  & 31.15 & 70.29 & 10.60 & 16.51 & 72.29 & 17.64 & 38.44 & 76.01 & 8.40 &  27.38& 79.18 \\
Baize~\cite{baize} & 10.62 & 31.87 & 73.62 & 10.38 & 16.44 & 76.63 & 16.56 & 36.67 & 76.87 & 8.50 &27.28 & 79.18   \\
Llama-3~\cite{llama3modelcard} & 11.79 & 34.82 & 77.37 & 13.82 & 18.95 & 85.80 & 18.78 & 40.20 & 78.73 & 12.25 & 35.16 & 82.79 \\
GPT-4~\cite{gpt4}         &          14.07 &           42.22 &            86.80 &           17.23 &           22.96 &           86.94 &           25.82 &           51.89 &           85.46 &           15.11 &           41.39 &           83.99 \\ 
GPT-4o~\cite{gpt4o} & 15.11 & 42.59 & 85.93 & 17.20 &  22.82 & 85.31 & 27.28 & 52.57 & 85.90 & 16.28 & 42.40 & 83.82 \\
\hline




\textit{\textbf{Prior Ensemble Method}}\\ \hdashline[3pt/3pt]
LLM-Blender~\cite{llm-blender} & 8.77 & 28.74 & 82.80 & 0.03 & 0.85 & 42.77 & 11.80 & 29.85 & 67.95 & 0.94 & 2.69 & 46.49 \\
EVA~\cite{eva} & 2.53 & 15.26 & 39.00 &1.51 & 3.57 &  37.17 & 9.77 &  28.40 &  68.75 & 7.99 & 27.00 & 73.15 \\
MBR~\cite{mbr} & 14.10 & 42.24 & 86.70 & 17.14 & 23.00 & 87.53 & 25.45 & 51.78 & 85.55 & 14.66 & 41.11 & 83.93 \\ \hline


\textit{\textbf{Proposed Method}}\\ \hdashline[3pt/3pt]
\ours (Llama-3; top3) & 15.60 & 39.86 & 84.10 & 14.56 & 19.92 & 87.34 & 23.41 & 45.95 & 81.66 & 14.27 & 38.25 & 81.80 \\
\ours (Llama-3; \textit{D, E}) & 13.85 & 37.36 & 69.96 & 14.97 & 20.21 & 85.42 & 21.35 & 43.75 & 79.71 & 12.37 & 36.51 & 82.09 \\


\ours (GPT-4; top3)   &          16.66 &           42.85 &  \textbf{86.82} &           17.95 &           23.84 &            87.50 &           27.22 &           51.73 &  85.65 &           16.53 &           42.41 &           84.46 \\

\ours (GPT-4; \textit{D, E})     &  17.10 &  43.29 &           85.92 &  18.18 &  24.05 &  \textbf{88.74} &  27.98 &  52.41 &           85.27 &  17.02 &  43.02 &  \textbf{84.82} \\

\ours (GPT-4o; top3) &
17.77 & 43.38 & 85.46 &
18.08 & 23.98 & 88.15 &
28.62 & 52.53 & 85.87 &
16.92 & 42.93 & 84.52 \\
 
\ours (GPT-4o; \textit{D, E}) &
\textbf{18.02} & \textbf{43.46} & 86.19 &
\textbf{18.31} & \textbf{24.32} & 88.33 &
\textbf{29.50} & \textbf{53.16} & \textbf{86.03} &
\textbf{17.66} & \textbf{43.73} & 84.27 \\


\Xhline{3\arrayrulewidth}
\end{tabular}
\end{adjustbox}
\caption{Main results. The best scores in each pair are marked \textbf{bold}. Within parentheses in the proposed method, the parts separated by semicolons denote the merging module and the candidates used. \textit{D} and \textit{E} represent candidates obtained from direct translation and English pivot, respectively.} 
\label{tab: main results}
\end{table*}


\begin{table}[t]
\centering
\scriptsize
\renewcommand{\arraystretch}{0.97}
\begin{adjustbox}{width=\columnwidth, center}
\begin{tabular}{lccc}
\Xhline{3\arrayrulewidth}

\multirow{2}{*}{\textbf{Model}}  & \multicolumn{3}{c}{\textbf{Korean$\rightarrow$Italian}}\\ \cline{2-4}
 & BLEU & chrF++ & COMET \\ \hline\hline 
                  
\textit{\textbf{Candidate}}\\\hdashline[3pt/3pt]
 \nllb (direct)   & 16.27 & 41.14 & 84.60 \\
 \nllb (Portuguese pivot) & 13.13 & 37.57 & 83.21  \\  
 \nllb (Spanish pivot) & 13.87 & 38.47 & 83.71 \\
 \nllb (English pivot) & 14.77 & 39.39 & 81.48 \\

\Xhline{3\arrayrulewidth}
\end{tabular}
\end{adjustbox}
\caption{Quality of candidates used for the ensemble.}
\label{tab:candidates}
\end{table}



Table~\ref{tab: main results} reports the overall performance of \ours and other methods.
The results demonstrate that \ours consistently outperforms baselines across all language pairs.
While standalone NMT systems rely solely on their pre-trained knowledge, \ours explicitly leverages candidates during the ensemble.
Even when training an open-source LLM, Llama-3, we can enhance translation capability by utilizing candidates obtained via pivoting.
Compared to using LLMs for translation, we can improve performance with only the minimal cost of utilizing a small 0.6B model.
Table~\ref{tab:candidates} presents the quality of candidates utilized in the ensemble.
We will further elaborate with a case study, showing that \ours achieves better translations by leveraging candidates to capture subtle nuances of the source sentence.
We report experiments in a setting that does not use training data in Appendix~\ref{apdx: Evaluation without using Training Data} and experiments with other GPT models in Appendix~\ref{apdx: Experiments with Additional Models}.
The analysis of the proportion of top-$\textit{k}$ candidates and performance variation with $\textit{k}$ are in Appendix~\ref{apdx:selecting top-k}.

\minisection{Comparison with multi-model ensemble}
We compare \ours with \blender~\cite{llm-blender} and EVA~\cite{eva}, state-of-the-art ensemble methods utilizing multiple models.
\blender employs $N$ ($N$=11) LLMs for candidate generation, picks top-3 candidates with \textsc{PairRanker}, and fuses them with \textsc{GenFuser}.
EVA is a token-level ensemble method that leverages vocabulary alignment across multiple models.





Results in Table~\ref{tab: main results} show that \ours outperforms multi-model ensemble baselines by a considerable margin.
\blender was unable to improve outputs compared to its candidate LLMs in non-English translation tasks.
Additionally, LLMs used for generating candidates in \blender, such as Vicuna and Baize, exhibit subpar performance on given tasks.
These results align with recent work~\cite{alma}; open-source LLMs often struggle when not translating into English.

EVA is not only ineffective on the given tasks but also has several limitations inherent to its design as a token-level ensemble.
First, EVA is unable to use black-box models such as GPT-4.
Second, it is memory-intensive, as it requires loading multiple models into memory simultaneously.
While multi-model ensemble methods generate candidates using up to 11 LLMs (with sizes up to 13B), \ours generates candidates with a significantly smaller single model (0.6B), thereby greatly reducing computational overhead.





\begin{table*}[t]
\centering
\large
% \renewcommand{\arraystretch}{1.04}
\renewcommand{\arraystretch}{1.1}
\begin{adjustbox}{width=\textwidth, center}
\begin{tabular}{llc}
% \begin{tabular}{|p{1cm}|p{2cm}|p{5cm}|}
\Xhline{3\arrayrulewidth}
\textbf{\#} & \textbf{Type}& \textbf{Example}\\ \hline\hline


\multirow{8}{*}{\textbf{1}} & Source Sentence & 그래서 그동안 자문해왔습니다. 왜 우리는 질병들과 싸우기에 더 현명하고, 정확하며 더욱 적합한 ... \\ 
& & \textit{(English Translation: So we've been asking ourselves, why should we limit this smarter, more precise, more appropriate ...)} \\ 
& Target Reference & Quindi \hlc[lightblue]{mi sono chiesta}: perché dovremmo limitare questo modo intelligente, preciso, migliore ... \\ \cline{2-3} 
& Top-1 Candidate & Quindi \hlc[lightblue]{ci siamo chiesti}, perché dovremmo limitare questo modo più intelligente, più preciso e più appropriato ... \\
& Top-2 Candidate & Quindi \hlc[lightblue]{ci siamo chiesti}: perché dovremmo limitare questo metodo più intelligente, più preciso e più adatto ... \\ 
& Top-3 Candidate & Quindi nel corso di questo tempo, \hlc[pink]{abbiamo chiesto}: perché dovremmo limitare questo metodo più intelligente, più preciso e più adeguato ... \\ \cline{2-3}
& GPT-4 & Quindi, \hlc[pink]{abbiamo cercato consigli} fino ad ora. Perché dobbiamo limitare questo metodo, che è più intelligente, preciso e più adatto ... \\ 
& \ours  & Quindi \hlc[lightblue]{ci siamo chiesti}: perché dovremmo limitare questo metodo più intelligente, più preciso e più adatto ... \\ \hline\hline 


\multirow{8}{*}{\textbf{2}} & Source Sentence & 많은 사람들이 헤드폰을 사용하는데 이것의 문제점은 3가지 큰 건강 이슈를 가져온다는 것입니다. \\
& & \textit{(English Translation: The trouble with widespread headphone use is it brings three really big health issues.)}\\
& Target Reference & Il problema dell'utilizzo diffuso degli auricolari è che scatenano tre grandi \hlc[lightblue]{problemi} di salute. \\ \cline{2-3}
& Top-1 Candidate & Il problema è che molte persone usano le cuffie, e questo Porta a tre grandi \hlc[lightblue]{problemi} di salute. \\
& Top-2 Candidate & Il problema è che molte persone usano le cuffie, e questo Porta a tre grandi \hlc[lightblue]{problemi} di salute. \\ 
& Top-3 Candidate & Il problema è che molte persone usano cuffie, e questo è ciò che causa tre \hlc[lightblue]{problemi} di salute principali. \\ \cline{2-3}
& GPT-4 & Molte persone utilizzano le cuffie, ma il problema è che ciò comporta tre importanti \hlc[pink]{questioni} di salute. \\
& \ours  &  Il problema è che molte persone usano le cuffie, e questo porta a tre grandi \hlc[lightblue]{problemi} di salute. \\


\Xhline{3\arrayrulewidth} 
\end{tabular}
\end{adjustbox}
\caption{Case study. Parts with the same meanings as the source and mistranslated parts are highlighted in \hlc[lightblue]{blue} and \hlc[pink]{red}, respectively. English translation of the source sentence is obtained from another pair within the same dataset.}
\label{tab:case study}
\end{table*}



\begin{table}[t!]
\centering
\small
\renewcommand{\arraystretch}{0.97}
\begin{adjustbox}{width=\columnwidth, center}
\begin{tabular}{lcccccc}
\Xhline{3\arrayrulewidth}

\textbf{Model} & BLEU & chrF++ & COMET & BLEU & chrF++ & COMET \\ \hline\hline 

& \multicolumn{6}{c}{\textbf{Distant Language Pairs}} \\ \cline{2-7}

& \multicolumn{3}{c}{Portuguese$\rightarrow$Russian} & \multicolumn{3}{c}{Russian$\rightarrow$Portuguese} \\
NLLB&	25.17&	51.77&	90.12&	29.69&	55.81&	86.01 \\
GPT-4&	26.50&	52.76&	91.11&	25.51&	54.05&	86.69 \\
\ours&	\textbf{27.48}&	\textbf{53.49}&	\textbf{91.74}&	\textbf{30.82}&	\textbf{56.73}&	\textbf{88.37} \\ \hline

& \multicolumn{3}{c}{Dutch$\rightarrow$Russian} & \multicolumn{3}{c}{Russian$\rightarrow$Dutch} \\
NLLB&	22.95&	50.21&	89.92&	25.56&	53.60&	88.18 \\
GPT-4&	24.37&	51.32&	91.28&	24.46&	53.85&	88.58 \\
\ours&	\textbf{25.45}&	\textbf{52.16}&	\textbf{91.47}&	\textbf{28.05}&	\textbf{55.80}&	\textbf{89.35} \\ \hline

& \multicolumn{3}{c}{French$\rightarrow$Ukrainian} & \multicolumn{3}{c}{Ukrainian$\rightarrow$French} \\
NLLB&	14.58&	37.11&	82.99&	20.69&	44.04&	80.61 \\
GPT-4&	13.84&	39.03&	84.12&	23.30&	47.13&	83.43 \\
\ours&	\textbf{17.20}&	\textbf{39.82}&	\textbf{86.55}&	\textbf{24.35}&	\textbf{47.17}&\textbf{84.36} \\ \hline \hline

& \multicolumn{6}{c}{\textbf{Similar Language Pair (Romance)}} \\ \cline{2-7}

& \multicolumn{3}{c}{Spanish$\rightarrow$Portuguese} & \multicolumn{3}{c}{Portuguese$\rightarrow$Spanish} \\
NLLB&	32.38&	56.97&	86.88&	33.63&	57.61&	85.13 \\
GPT-4&	29.94&	55.26&	84.84&	34.70&	58.63&	86.75 \\
\ours&	\textbf{34.06}&	\textbf{58.11}&	\textbf{87.70}&	\textbf{36.03}&	\textbf{59.32}&	\textbf{86.92} \\ \hline

& \multicolumn{6}{c}{\textbf{Similar Language Pair (Slavic)}} \\ \cline{2-7}

& \multicolumn{3}{c}{Ukrainian$\rightarrow$Russian} & \multicolumn{3}{c}{Russian$\rightarrow$Ukrainian} \\
NLLB&	22.16&	45.41&	89.82&	19.67&	43.35&	89.87 \\
GPT-4&	24.41&	\textbf{47.59}&	89.43&	\textbf{22.42}&	\textbf{45.61}&	90.39 \\
\ours&	\textbf{24.64}&	47.51&	\textbf{90.78}&	22.09&	45.40&	\textbf{90.70} \\ 


\Xhline{3\arrayrulewidth}
\end{tabular}
\end{adjustbox}
\caption{Results on all language pairs.}
\label{tab:Results on all translation directions}
\end{table}




\minisection{Results on all language pairs}
To validate generalizability, we report the results for all language pairs we experimented with, including those within the same language family.
Distant pairs refer to languages that belong to different families and use different scripts, while similar pairs belong to the same family and share the same script.
The statistics for each language pair are in Appendix~\ref{sec:apdx_dataset statistics}.
Language pairs used in the experiments are as follows:

\begin{itemize}
    \item Distant language pairs: Portuguese$\leftrightarrow$Russian, Dutch$\leftrightarrow$Russian, and French$\leftrightarrow$Ukrainian

    \item Similar language pairs: Spanish$\leftrightarrow$Portuguese and Ukrainian$\leftrightarrow$Russian
\end{itemize}

 
Table~\ref{tab:Results on all translation directions} shows the results with the top-performing baselines, NLLB~\cite{nllb} and GPT-4~\cite{gpt4}.
\ours consistently exhibits superior performance compared to strong baselines on distant language pairs. 
Surprisingly, it also showed improvements in similar language pairs, such as Spanish$\leftrightarrow$Portuguese. 







\minisection{Case study}
We conduct a qualitative analysis to verify the impact of candidates on the final translation. 
We compare the output of GPT-4, used as the merging module, with \ours, which utilizes candidates for the ensemble process.
In Table~\ref{tab:case study}, we provide two examples along with the source and target sentences, as well as the top-3 candidates.


Through the first example, we can observe that \ours can appropriately translate homonyms within the context.
In Korean, ``자문'' has the meaning of both ``consultation'' and ``asking oneself''.
Considering the context, the expression should be translated to convey the meaning of ``asking ourselves'', as also shown in the English translation.
However, GPT-4 mistranslated the source sentence, converting the phrase ``자문해왔습니다'' to ``abbiamo cercato consigli'' (``seeking consultation from others'').
On the other hand, \ours accurately translates with the expression ``ci sono chiesti'' that means ``asking ourselves'', aligning well with the context by leveraging information from candidates.


In the second sample, GPT-4 translates the source sentence by translating the noun ``이슈'' into ``questioni''.
However, given the topic of discussing potential health risks, this translation does not fit well with the overall context.
By contrast, the ensemble result of \ours, generated using the identical model, improves translation quality by using a more accurate expression ``problemi'', despite having access to the same pre-trained knowledge.
Additionally, when more suitable expressions (e.g., ``ne vale la pena'') appear in candidates, \ours utilizes them to refine the final translation.


\subsection{Analysis}


\begin{table}[t]
\centering
\large
\begin{adjustbox}{width=\columnwidth, center}
\renewcommand{\arraystretch}{0.97}
\begin{tabular}{lcccc}
\Xhline{3\arrayrulewidth}
\multirow{2}{*}{\parbox{4cm}{\textbf{Candidate Generation}}} & \multirow{2}{*}{\parbox{1.45cm}{\textbf{\# Cand.}}}  &  \multicolumn{3}{c}{\textbf{Korean$\rightarrow$Italian}} \\ \cline{3-5} 
 & & BLEU & chrF++ & COMET \\ \hline\hline 

LLMs (of \blender) & 11 &  14.75 & 41.29 & 86.20\\ 

LLMs + \nllb (direct) & 12 & 16.08 & 42.38 & 86.22 \\ 
 
\nllb (pivot, ours) & 4 & \textbf{16.66} & \textbf{42.85} & \textbf{86.82}  \\

\Xhline{3\arrayrulewidth}
\end{tabular}
\end{adjustbox}
\caption{Comparison of candidate generation methods.}
\label{tab:comparison changing candidates}
\end{table}




\minisection{Candidate generation}
To validate the effectiveness of \ours, we conduct experiments only varying the candidate generation method, while using the same merging module, GPT-4.
We compare a candidate pool of size 4 obtained through pivot translation (\ours) with a candidate pool of size 11 obtained using 11 LLMs as employed in \blender~\cite{llm-blender}.

As shown in Table~\ref{tab:comparison changing candidates}, the proposed method of generating candidates through pivot translation achieves the highest performance, despite using the smallest candidate pool.
From the perspective of direct translation in NLLB, leveraging 3 candidates obtained through pivot translation yields higher scores than incorporating candidates generated by 11 LLMs.
These results demonstrate that using stable-quality candidates generated by a single model via pivot translation outperforms the use of multiple models with performance disparities.

\begin{table}[!t]
\centering
% \small
\renewcommand{\arraystretch}{0.95}
\begin{adjustbox}{width=\columnwidth, center}
\begin{tabular}{lccc}
\Xhline{3\arrayrulewidth}

\multirow{2}{*}{\textbf{Model}}  & \multicolumn{3}{c}{\textbf{Korean$\rightarrow$Italian}}\\ \cline{2-4}
 & BLEU & chrF++ & COMET \\ \hline\hline 
                  
\textit{\textbf{Standalone NMT System}}\\\hdashline[3pt/3pt]
\nllb~\cite{nllb}   & 16.27 & 41.14 & 84.60 \\ \hline

\textit{\textbf{Encoder-Decoder}}\\\hdashline[3pt/3pt]
{\ours} (FiD)      & 13.74 & 36.78 & 78.98 \\
{\ours} (TRICE)    & 15.89 & 41.98 & 84.06 \\\hline
\textit{\textbf{LLM-based}}\\\hdashline[3pt/3pt]
{\ours} (\textsc{GenFuser}) & 14.56 & 39.32 & 80.07\\
{\ours} (GPT-4)    & \textbf{16.66} & \textbf{42.85} & \textbf{86.82}\\

\Xhline{3\arrayrulewidth}
\end{tabular}
\end{adjustbox}
\caption{Evaluation of merging module variants.}
\label{tab:merging module results}
\end{table}
\minisection{Candidate aggregation}
We first investigate whether \ours shows improvement when utilizing other merging modules.
As detailed in \S\ref{sec:candidate aggregation}, we run experiments with three architectures: FiD~\cite{fid}, TRICE~\cite{trice}, and \textsc{GenFuser}~\cite{llm-blender}.
When implementing FiD, we replace the backbone of FiD to $\text{mT5}_{\texttt{BASE}}$~\cite{xue-etal-2021-mt5}.
TRICE is a method proposed for multi-source translation. 
Since TRICE was not originally intended for ensemble use, we repurposed it by training on the following two tasks:
The first task is the original translation which converts source sentences into target sentences.
The second task is refining candidates that are paired with target references.
In the case of \trice, only the highest quality candidates, which are the directly translated ones, are used due to its architecture.
\fid and \textsc{GenFuser} use top-3 candidates.


Table~\ref{tab:merging module results} shows that the ensemble methods using encoder-decoder architectures and \textsc{GenFuser} do not yield improved results.
These methods struggle to leverage additional information from the candidates and, consequently, do not enhance performance. 
In contrast, using GPT-4 as the merging module leads to better performance compared to the standalone NMT system.
We also compare ranking methods COMETkiwi and \textsc{PairRanker}~\cite{llm-blender}.
While the performance is comparable, considering the efficiency factor, we opt for COMETkiwi.
Detailed experiments about the ranking method are in Appendix~\ref{sec:apdx_cands selecting method}.



\minisection{Comparison with selection-based ensemble}
With a selection-based ensemble, we can choose one of the existing candidates as the final translation, rather than generating a new one.
In this experiment, we compare our approach with a selection-based ensemble by selecting the top-1 translation using \textsc{PairRanker}~\cite{llm-blender} and COMETkiwi~\cite{rei2022cometkiwi}.
Additionally, we report results with an ideal case: selecting top-1 by considering references as well, which are not available in practice.
The ideal top-1 is selected by reference-based COMET~\cite{comet22}.


As shown in Table~\ref{tab:selection-based}, \ours exhibits superior performance compared to the selection-based ensemble methods.
Even when we leverage reference-based COMET, which is impossible in real-world scenarios due to the necessity for references, \ours outperforms it in chrF++ and COMET.
These results indicate that performing a generation-based ensemble with pivoting can effectively produce final translations that surpass those selected from the existing candidate pool.




\begin{table}[t]
\centering
\small
\begin{adjustbox}{width=\columnwidth, center}
\renewcommand{\arraystretch}{0.97}
\begin{tabular}{llccc}
\Xhline{3\arrayrulewidth}
\multirow{2}{*}{\textbf{Category}} & \multirow{2}{*}{\textbf{Method}} &  \multicolumn{3}{c}{\textbf{Korean$\rightarrow$Italian}} \\ \cline{3-5} 
& & BLEU & chrF++ & COMET \\ \hline\hline 
\multirow{3}{*}{\makecell[l]{Selection-based \\ (top-1)}} & \textsc{PairRanker} & 15.61 & 40.62 & 84.46 \\
& COMETkiwi & 15.61 & 40.71 & 84.10 \\
& COMET* (ideal) & \textbf{17.77} & 42.81 & 84.83\\ \hline
Generation-based & \ours & \underline{16.66} & \textbf{\underline{42.85}} & \textbf{\underline{86.82}} \\
\Xhline{3\arrayrulewidth}
\end{tabular}
\end{adjustbox}
\caption{Comparison with selection-based ensemble. Note that COMET* is the ideal baseline, as it requires references. Best scores including COMET* are \textbf{bolded}, while best scores excluding it are \underline{underlined}.}
\label{tab:selection-based}
\end{table}

