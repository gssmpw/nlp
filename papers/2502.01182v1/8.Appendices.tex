\section*{Appendix}
\appendix


\section{Pivot Language Selection}
\label{sec:apdx_top4 pivot langauges}

Based on the results from the FLORES-200~\cite{nllb} benchmark, we select the top-4 pivot paths as presented in Table~\ref{tab:pivot path}.
We utilize the full 2009 sentences as our test set: 997 sentences from the \textit{dev} and 1012 sentences from the \textit{devtest}.
The pivot language pool is chosen as the \textit{bridge languages} in \citet{m2m100}.


\begin{table}[h]
\centering
\small
\renewcommand{\arraystretch}{0.96}
\begin{adjustbox}{width=\columnwidth, center}
\begin{tabular}{ccccc}
\Xhline{3\arrayrulewidth}

\multirow{2}{*}{\textbf{Pivot Language}} & \multicolumn{4}{c}{\textbf{Lang-pair}} \\ \cline{2-5} 
 & KO$\rightarrow$IT& IT$\rightarrow$KO & AR$\rightarrow$PT & PT$\rightarrow$AR \\ \hline\hline 

\texttt{direct}    & \textbf{14.02}& \textbf{18.63} & \textbf{27.15} & \textbf{15.22} \\
\texttt{arb\_Arab} & 11.03 & 15.82 & - & - \\
\texttt{ben\_Beng} & 10.79 & 15.44 & 18.65 & 9.76 \\
\texttt{ces\_Latn} & 11.48 & 16.08 & 21.23 & 11.55 \\
\texttt{deu\_Latn} & 12.49 & 17.11 & 22.62 & 12.56 \\
\texttt{ell\_Grek} & 11.96 & 16.54 & 22.53 & 12.54 \\
\texttt{eng\_Latn} & \textbf{14.82} & \textbf{19.34} & \textbf{28.40} & \textbf{15.92} \\
\texttt{fin\_Latn} & 9.62 & 14.31 & 17.27 & 9.48 \\
\texttt{fra\_Latn} & 13.55 & 17.27 & \textbf{24.96} & 13.77 \\
\texttt{heb\_Hebr} & 10.42 & 14.37 & 20.31 & 10.94 \\
\texttt{hin\_Deva} & 11.54 & 17.12 & 21.79 & 11.72 \\
\texttt{hun\_Latn} & 10.54 & 14.96 & 18.64 & 9.65 \\
\texttt{ind\_Latn} & 12.41 & 17.03 & 22.47 & 11.97 \\
\texttt{ita\_Latn} & - & - & 24.70 & \textbf{14.09} \\
\texttt{jpn\_Jpan} & 10.60 & 14.73 & 14.29 & 7.31 \\
\texttt{kor\_Hang} & - & - & 16.09 & 7.67 \\
\texttt{lit\_Latn} & 10.46 & 14.96 & 18.14 & 9.47 \\
\texttt{nld\_Latn} & 12.27 & 17.10 & 23.22 & 12.94 \\
\texttt{pes\_Arab} & 11.09 & 15.86 & 20.88 & 11.50 \\
\texttt{pol\_Latn} & 11.54 & 15.86 & 21.14 & 11.60 \\
\texttt{por\_Latn} & \textbf{13.80} & \textbf{18.01} & - & - \\
\texttt{rus\_Cyrl} & 12.25 & 16.57 & 22.77 & 12.39 \\
\texttt{spa\_Latn} & \textbf{13.89} & \textbf{18.39} & \textbf{26.60} & \textbf{14.91} \\
\texttt{swe\_Latn} & 11.93 & 16.54 & 22.34 & 12.25 \\
\texttt{swh\_Latn} & 10.66 & 14.22 & 19.13 & 10.19 \\
\texttt{tam\_Taml} & 9.90 & 14.92 & 18.09 & 9.48 \\
\texttt{tur\_Latn} & 11.25 & 15.92 & 19.53 & 10.04 \\
\texttt{ukr\_Cyrl} & 11.76 & 16.43 & 21.87 & 12.12 \\
\texttt{vie\_Latn} & 12.00 & 16.32 & 21.39 & 11.49 \\
\texttt{zho\_Hans} & 10.00 & 11.51 & 15.29 & 6.82 \\

\Xhline{3\arrayrulewidth} 
\end{tabular}
\end{adjustbox}
\caption{BLEU scores on FLORES-200 benchmark. Pivot languages are sorted in alphabetical order and top-4 pivot paths are marked \textbf{bold}.}
\label{tab:pivot path}
\end{table}



\begin{figure*}[!t]
  \centering
  \includegraphics[width=0.75\textwidth]{Figures/FiD_illustration.pdf} 
  \caption{Illustration of the merging process using FiD~\cite{fid}.}
  \label{fig:FiD}
\end{figure*}


\begin{figure*}[!t]
  \centering
  \includegraphics[width=0.55\textwidth]{Figures/TRICE_illustration.pdf} 
  \caption{Illustration of the merging process using TRICE~\cite{trice}.}
  \label{fig:TRICE}
\end{figure*}

\section{Impact of Resource-level of Pivot Languages}
\label{apdx:resource level of pivot languages}



Under the assumption that high-quality candidates are more adept at conveying the meaning of the source sentence, we select the top-4 paths based on scores on FLORES-200.
To verify this hypothesis, we conduct experiments using mid/low-resource pivot languages.
According to the WMT22\footnote{\url{https://www.statmt.org/wmt22/translation-task.html}}, we select Ukrainian and Croatian as mid- and low-resource languages, respectively.
Table~\ref{tab:mid/low-resource pivot} shows that using candidates from high-resource languages outperforms those obtained from mid/low-resource languages. 
The quality of candidates is presented in Table~\ref{tab:mid/low-resource candidates}.
In conclusion, since high-resource languages can also provide sufficient diversity, we select top-performing paths based on the results on FLORES-200.



\begin{table}[h]
\centering
\small
\begin{adjustbox}{width=\columnwidth, center}
\renewcommand{\arraystretch}{1.0}
\begin{tabular}{lccc}
\Xhline{3\arrayrulewidth}
\multirow{2}{*}{\textbf{Method}} &  \multicolumn{3}{c}{\textbf{Korean$\rightarrow$Italian}} \\ \cline{2-4} 
 & BLEU & chrF++ & COMET \\ \hline\hline 
\ours (GPT-4; \textit{U, C}) & 15.28 & 41.78 & 85.75 \\
\ours (GPT-4; \textit{E, S}) & \textbf{16.27} & \textbf{42.55} & \textbf{86.50} \\
\Xhline{3\arrayrulewidth}
\end{tabular}
\end{adjustbox}
\caption{Comparison with mid/low-resource languages. \textit{U}, \textit{C}, \textit{E}, and \textit{S} represent candidates from Ukrainian, Croatian, English, and Spanish pivot, respectively.}
\label{tab:mid/low-resource pivot}
\end{table}



\begin{table}[h]
\centering
\small
\renewcommand{\arraystretch}{1.0}
\begin{adjustbox}{width=\columnwidth, center}
\begin{tabular}{lccc}
\Xhline{3\arrayrulewidth}

\multirow{2}{*}{\textbf{Model}}  & \multicolumn{3}{c}{\textbf{Korean$\rightarrow$Italian}}\\ \cline{2-4}
 & BLEU & chrF++ & COMET \\ \hline\hline 
                  
\textit{\textbf{Candidate}}\\\hdashline[3pt/3pt]
 \nllb (Ukrainian pivot)  & 11.95 & 35.03 & 82.32 \\  
 \nllb (Croatian pivot)   & 12.25 & 35.93 & 79.91 \\
 \nllb (Spanish pivot)    & 13.87 & 38.47 & 83.71 \\
 \nllb (English pivot)    & 14.77 & 39.39 & 81.48 \\

\Xhline{3\arrayrulewidth}
\end{tabular}
\end{adjustbox}
\caption{Quality of candidates from each pivot path.}
\label{tab:mid/low-resource candidates}
\end{table}







\section{Metric for Selecting Translation Paths}
\label{apdx: Metric for Selecting Translation Paths}

We conduct a comparative analysis between BLEU and COMET, used for selecting the $n$ translation paths.
The results in Table~\ref{tab:ensemble results from BLEU/COMET} indicate that the difference between metrics is marginal.
We believe that this stems from the minimal difference in selected paths, as presented in Table~\ref{tab:pivot languages from BLEU/COMET}.
We observe some changes in order and minor differences, but the pivot languages selected by BLEU and COMET show similar compositions.



\begin{table}[!h]
\centering
\small
\renewcommand{\arraystretch}{1.0}
\begin{adjustbox}{width=\columnwidth, center}
\begin{tabular}{lcccc}
\Xhline{3\arrayrulewidth}

\multirow{2}{*}{\textbf{top-$\textit{k}$}} & \multirow{2}{*}{\textbf{Path Selection}} & \multicolumn{3}{c}{\textbf{Korean$\rightarrow$Italian}}\\ \cline{3-5}
& & BLEU & chrF++ & COMET \\ \hline\hline 
\multirow{2}{*}{top-$\textit{1}$} & COMET	&15.98	&42.59	&86.22 \\
& BLEU	&16.20 &	42.84	&85.36 \\
\multirow{2}{*}{top-$\textit{2}$} & COMET	&16.46&	42.58&	86.66 \\
& BLEU	&16.57	&43.04&	86.37 \\
\multirow{2}{*}{top-$\textit{3}$} & COMET	&16.39&	42.41&	86.04 \\
& BLEU	&16.66	&42.85&	86.82 \\

\Xhline{3\arrayrulewidth}
\end{tabular}
\end{adjustbox}
\caption{Impact of the pivot path selection metric.}
\label{tab:ensemble results from BLEU/COMET}
\end{table}



\begin{table*}[!t]
\centering
\renewcommand{\arraystretch}{0.98}
\begin{adjustbox}{width=\textwidth, center}
\begin{tabular}{ccc}
\Xhline{3\arrayrulewidth}

\textbf{Lang-pair}  & \textbf{BLEU} & \textbf{COMET} \\ \hline\hline 
                  
KO$\rightarrow$IT & English (14.82), direct (14.02), Spanish (13.89), Portuguese (13.80)& English (82.89), Spanish (82.70), Indonesian (81.62), Portuguese (81.50) \\
IT$\rightarrow$KO & English (19.34), direct (18.63), Spanish (18.39), Portuguese (18.01)& Spanish (87.32), English (87.07), Portuguese (87.02), French (86.14) \\
AR$\rightarrow$PT & English (28.40), direct (27.15), Spanish (26.60), French (24.96)	 & direct (85.71), English (85.57), Spanish (85.54), Indonesian (84.94) \\
PT$\rightarrow$AR & English (15.92), direct (15.22), Spanish (14.91), Italian (14.09)	& French (82.65), direct (81.36), English (81.04), German (80.44) \\

\Xhline{3\arrayrulewidth}
\end{tabular}
\end{adjustbox}
\caption{Selected top-4 pivot paths from each metric. Scores are from experiments on FLORES-200.}
\label{tab:pivot languages from BLEU/COMET}
\end{table*}


\section{Implementation Details of the Merging Modules}
\label{apdx:Fid/TRICE illustration}

In this section, we provide detailed explanations of the merging modules.
Figure~\ref{fig:FiD} shows the FiD~\cite{fid}.
First, the instruction and the source sentence are concatenated with each candidate, and processed independently by the encoder.
Then the decoder takes the concatenation of each representation and generates the final translation.

As shown in Figure~\ref{fig:TRICE}, TRICE~\cite{trice} is trained with a two-stage fine-tuning method.
In the first fine-tuning stage, the model is trained on two different inputs and single targets: Source$\rightarrow$Target and Candidate$\rightarrow$Target.
In the second fine-tuning stage, the source and the candidate are concatenated and provided as a single input.




\section{Prompt Templates}
\label{sec:apdx_prompt_templates}

We use the zero-shot prompt template from \citet{howgood} to instruct the LLMs for translation,

\begin{quote}

    \small
    \texttt{Translate this sentence from [source language] to [target language], Source: [source sentence]}

    \texttt{Target:}
\end{quote}

when ensembling with candidates, we use the prompt template as follows,

\begin{quote}
    \small
    \texttt{Ensemble the [source language] sentence with the provided [target language] candidates to create the best possible [target language] translation.}
    
    \texttt{[source language] sentence: [source sentence]}
    
    \texttt{[target language] candidate k: [target candidate]}
    
    \texttt{Please provide only the [target language] translation and no additional text.}
    
    \texttt{[target language] translation:}
\end{quote}

\section{Open-source LLMs}
\label{sec:apdx_llms}
In experiments with \blender and EVA, we employ the same models as used in each paper.
These open-source LLMs are listed in Table~\ref{tab:11 open-source llms}.

\begin{table}[h!]
\centering
\small
\begin{adjustbox}{width=\columnwidth, center}
\renewcommand{\arraystretch}{1.05}
\begin{tabular}{lc}
\Xhline{3\arrayrulewidth}
\textbf{Model} & \textbf{Model Size} \\ \hline\hline
\textit{\textbf{\blender}~\cite{llm-blender}}\\ \hdashline[3pt/3pt]
Vicuna~\cite{Vicuna} & 13B \\
Baize~\cite{baize} & 13B\\
Alpaca~\cite{alpaca} & 13B \\ 
Koala~\cite{koala} & 13B \\
Open Assistant~\cite{oasst} & 12B \\
Dolly V2~\cite{dollyv2} & 12B \\
Flan-T5~\cite{Flan-T5} & 11B\\
MOSS~\cite{moss} & 7B \\
Mosaic MPT~\cite{MosaicML} & 7B \\ 
StableLM~\cite{stablelm} & 7B \\
ChatGLM~\cite{chatglm} & 6B \\ \hline

\textit{\textbf{EVA}~\cite{eva}}\\ \hdashline[3pt/3pt]
Baichuan2-Chat~\cite{baichuan2} & 7B \\
TigerBot-Chat-V3~\cite{tigerbot} & 7B \\
Vicuna-V1.5~\cite{Vicuna} & 7B \\
Llama-2-Chat~\cite{touvron2023llama2openfoundation} & 7B \\

\Xhline{3\arrayrulewidth}
\end{tabular}
\end{adjustbox}
\caption{Open-source LLMs along with their respective model sizes.}
\label{tab:11 open-source llms}
\end{table}



\section{Impact of \textit{temperature} in MBR}
\label{sec:MBR}

To investigate the best performance of MBR, we compared it across three different \textit{temperature} configurations: 1.0, 0.5, and 0.0, which were used in prior works by \citet{mbr}, \citet{temp0.5}, and \citet{peng2023making}, respectively. 



Table~\ref{tab:Impact of temperature in MBR decoding} and \ref{tab:Average quality of MBR hypotheses} show the quality of MBR outputs and hypotheses under different \textit{temperature} settings, respectively.
Aligning with the findings of the previous study~\cite{peng2023making}, we observed that a lower \textit{temperature} setting achieved better performance.
Thus, we set the \textit{temperature} of 0.0 for MBR in our experiments.


\begin{table}[h]
\centering
\renewcommand{\arraystretch}{1.1}
\begin{adjustbox}{width=\columnwidth, center}
\begin{tabular}{lccc}
\Xhline{3\arrayrulewidth}

\multirow{2}{*}{\textbf{Method}}  & \multicolumn{3}{c}{\textbf{Korean$\rightarrow$Italian}}\\ \cline{2-4}
 & BLEU & chrF++ & COMET \\ \hline\hline 
                  
 MBR (\textit{temp}=1.0;~\citet{mbr})   & 13.53 & 42.13 & 86.57 \\
 MBR (\textit{temp}=0.5;~\citet{temp0.5}) & 13.90 & 42.19 & 86.69 \\  
 MBR (\textit{temp}=0.0;~\citet{peng2023making})   & \textbf{14.10} & \textbf{42.24} & \textbf{86.70} \\ 

\Xhline{3\arrayrulewidth}
\end{tabular}
\end{adjustbox}
\caption{Impact of \textit{temperature} in MBR decoding.}
\label{tab:Impact of temperature in MBR decoding}
\end{table}



\begin{table}[h]
\centering
\small
\renewcommand{\arraystretch}{1.1}
\begin{adjustbox}{width=\columnwidth, center}
\begin{tabular}{lccc}
\Xhline{3\arrayrulewidth}

\multirow{2}{*}{\textbf{Method}}  & \multicolumn{3}{c}{\textbf{Korean$\rightarrow$Italian}}\\ \cline{2-4}
 & BLEU & chrF++ & COMET \\ \hline\hline 
                  
 \multirow{2}{*}{MBR hypotheses (\textit{temp}=1.0)}   & 13.47&	41.71 &84.94\\
  & (±0.21) &  (±0.14)	 & (±2.62) \\
 \multirow{2}{*}{MBR hypotheses (\textit{temp}=0.5)} & 13.86 &	 42.03&	86.55  \\  
  & (±0.13) &   (±0.11)& (±0.15)  \\
 \multirow{2}{*}{MBR hypotheses (\textit{temp}=0.0)} & 14.09 & 42.21 & 86.62 \\
  &   (±0.07) &  (±0.06)	   & (±0.10)   \\

\Xhline{3\arrayrulewidth}
\end{tabular}
\end{adjustbox}
\caption{Average quality of MBR hypotheses.}
\label{tab:Average quality of MBR hypotheses}
\end{table}


\begin{figure*}[t]
    \centering
    \begin{minipage}[b]{0.71\textwidth}
        \centering
        \includegraphics[width=\textwidth, height=3.55cm]{./Figures/topk_ko-it_pie_chart.png}
        \caption{Proportion of pivot languages (Korean$\rightarrow$Italian) comprising the top-$\textit{k}$ candidates.}        
        \label{fig:top-k candidates}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.23\textwidth}
        \centering
        \includegraphics[width=\textwidth, height=3.55cm]{./Figures/topk_bleu_ko-it.png}
        \caption{Impact of top-$\textit{k}$ values on performance.}
        \label{fig:top-k bleu score}
    \end{minipage}
\end{figure*}


\begin{table*}[t!]
\centering
\renewcommand{\arraystretch}{0.9}
\begin{adjustbox}{width=0.95\textwidth, center}
\begin{tabular}{lcccccccccccc}
\Xhline{3\arrayrulewidth}

\multirow{2}{*}{\textbf{Model}}  & \multicolumn{3}{c}{\textbf{Korean$\rightarrow$Italian}} & \multicolumn{3}{c}{\textbf{Italian$\rightarrow$Korean}} & \multicolumn{3}{c}{\textbf{Arabic$\rightarrow$Portuguese}} & \multicolumn{3}{c}{\textbf{Portuguese$\rightarrow$Arabic}}\\ \cline{2-13}
 & BLEU & chrF++ & COMET & BLEU & chrF++ & COMET  & BLEU & chrF++ & COMET & BLEU & chrF++ & COMET \\ \hline\hline 

\textit{\textbf{Standalone NMT System}}\\ \hdashline[3pt/3pt]
NLLB~\cite{nllb} & 13.88 & 38.56 & 81.54 & 14.98 & 20.99 & 85.75 & 23.55 & 47.50 & 82.08 & 13.23 & 38.55 & 82.79 \\

Vicuna~\cite{Vicuna} & 3.06 & 15.73 & 55.31 & 3.85& 7.57& 64.24 & 6.43 & 17.62 & 68.10 &3.15 & 19.56 & 63.79  \\

Baize~\cite{baize} & 4.28 & 22.85 & 65.66 &3.71 & 7.36 & 58.06 & 7.83 & 21.78 & 76.12 &2.83 & 17.91 & 62.03  \\

Llama-3~\cite{llama3modelcard} & 8.46 & 34.83 & 82.59 & 0.61 & 2.07 &  29.03 & 18.37 &  43.32 &81.75 & 7.31 & 25.67 &  75.45 \\
GPT-4~\cite{gpt4}         &          14.07 &           42.22 &            \textbf{86.80} &           17.23 &           22.96 &           86.94 &           25.82 &           51.89 &           85.46 &           15.11 &           41.39 &           83.99 \\ 
GPT-4o~\cite{gpt4o} & 15.11 & 42.59 & 85.93 & 17.20 &  22.82 & 85.31 & 27.28 & \textbf{52.57} & \textbf{85.90} & \textbf{16.28} & \textbf{42.40} & 83.82 \\
\hline

\textit{\textbf{Prior Ensemble Method}}\\ \hdashline[3pt/3pt]
LLM-Blender~\cite{llm-blender}   &           3.75 &           19.58 &           61.26 &            0.17 &            1.58 &           39.04 &            6.92 &           22.14 &           64.01 &            1.97 &            3.42 &           44.62 \\ 
EVA~\cite{eva} & 3.65 & 23.54 & 63.94 & 1.14 & 3.23 & 46.99 & 8.23 & 26.87 & 56.68 & 3.46 & 21.88 & 60.30\\ 
MBR~\cite{mbr} & 14.10 & 42.24 & 86.70 & 17.14 & 23.00 & 87.53 & 25.45 & 51.78 & 85.55 & 14.66 & 41.11 & 83.93 \\ \hline



\textit{\textbf{Proposed Method}}\\ \hdashline[3pt/3pt]
\ours (Llama-3; top3) & 13.38 & 39.99 & 84.21 & 12.19 & 19.40 & 41.52 & 24.92 & 49.57 & 85.17 & 15.33 &  40.77 & 84.26 \\

\ours (Llama-3; \textit{D, E}) & 12.74 & 38.47 & 80.04 & 12.15 & 18.22 &  42.51 & 24.29 & 48.17 & 82.75 & 13.15 & 38.67 & 83.08 \\

\ours (GPT-4; top-3) & 16.35 & 42.45 & 86.32 & 17.27 & 23.17 & 86.06 & 26.28& 51.06 & 85.22 & 15.08 & 41.19 & 84.37 \\
\ours (GPT-4; \textit{D, E}) & 16.29 & 42.61 & 86.75 & \textbf{17.55} & \textbf{23.50} & \textbf{88.02} & 26.60 & 51.45 & 85.68 & 15.12 & 41.34 & 83.87 \\
\ours (GPT-4o; top3) &
\textbf{17.39} & \textbf{42.89} & 85.30 &
17.15 & 23.27 & 87.68 &
\textbf{27.88} & 51.98 & 85.58 &
15.91 & 42.10 & 84.58 \\

\ours (GPT-4o; \textit{D, E}) &
17.22 & 42.78 & 85.29 &
16.99 & 23.04 & 87.96 &
27.38 & 51.68 & 85.64 &
15.60 & 41.64 & \textbf{84.80} \\


\Xhline{3\arrayrulewidth}
\end{tabular}
\end{adjustbox}
\caption{Results on Korean$\leftrightarrow$Italian and Arabic$\leftrightarrow$Portuguese, in a setting where no training data was used.}
\label{tab:results without using Training Data}
\end{table*}




\section{Experiments without Training Data}
\label{apdx: Evaluation without using Training Data}

In Table~\ref{tab:results without using Training Data}, we report the results of experiments conducted in a setting where no training data was used.
When compared to the results in Table~\ref{tab: main results}, this observation affirms that the quality of candidates during ensemble plays an important role in enhancing the final translation. 


\section{Datasets Statistics}
\label{sec:apdx_dataset statistics}

Table~\ref{tab:apdx_dataset statistics} shows the dataset statistics for each language pair used in the experiments in Table~\ref{tab:Results on all translation directions}.

\begin{table}[h!]
\centering
\begin{adjustbox}{width=\columnwidth, center}
\renewcommand{\arraystretch}{1.05}
\begin{tabular}{ccccc}
\Xhline{3\arrayrulewidth}
\multirow{2}{*}{\textbf{Lang-pair}} & \multirow{2}{*}{\textbf{Dataset}} 
& \multicolumn{3}{c}{\textbf{\# Sentences}} \\ \cline{3-5} 
 & & \textbf{Train} & \textbf{Dev} & \textbf{Test} \\ \hline\hline 

\multicolumn{5}{c}{\textbf{Distant Language Pairs}} \\  \hline 

\multirow{2}{*}{PT $\leftrightarrow$ RU} & news-commentary v18.1 & \multirow{2}{*}{66,743} & \multirow{2}{*}{2,000} & \multirow{2}{*}{2,000} \\
 & \cite{opus} & & & \\ \hline  
 
 \multirow{2}{*}{NL $\leftrightarrow$ RU} & news-commentary v18.1 & \multirow{2}{*}{80,724} & \multirow{2}{*}{2,000} & \multirow{2}{*}{2,000} \\
 & \cite{opus} & & & \\ \hline  

 
 \multirow{2}{*}{FR $\leftrightarrow$ UK} & WikiMatrix v1 & \multirow{2}{*}{166,063} & \multirow{2}{*}{2,000} & \multirow{2}{*}{2,000} \\
 & \cite{schwenk2019wikimatrix} & & & \\ \hline \hline

 \multicolumn{5}{c}{\textbf{Similar Language Pairs}} \\  \hline 

\multirow{2}{*}{ES $\leftrightarrow$ PT} & TED 2020 v1& \multirow{2}{*}{315,462} & \multirow{2}{*}{2,000} & \multirow{2}{*}{2,000} \\
 & \cite{ted2020} & & & \\ \hline  
 
 \multirow{2}{*}{UK $\leftrightarrow$ RU} & TED 2020 v1& \multirow{2}{*}{197,978} & \multirow{2}{*}{2,000} & \multirow{2}{*}{2,000} \\
 & \cite{ted2020} & & & \\ 



\Xhline{3\arrayrulewidth}
\end{tabular}
\end{adjustbox}
\caption{Number of sentences in the corpus and data split for each language pair.}
\label{tab:apdx_dataset statistics}
\end{table}



\section{Additional Analysis on Candidates}
\label{apdx:selecting top-k}
We conduct experiments to investigate the impact of the value of $\textit{k}$ in the top-$\textit{k}$ candidates and its composition.
Figure~\ref{fig:top-k candidates} illustrates the proportion of pivot languages composing the top-$\textit{k}$ candidates.
Top-$\textit{k}$ candidates, selected by the QE metric, are composed of diverse candidates obtained through various pivot languages.
We also observe the same tendency in other datasets.
This suggests that generating diverse candidates through multiple paths helps acquire higher-quality candidates.


Figure~\ref{fig:top-k bleu score} presents BLEU scores for different values of the $\textit{k}$.
The highest BLEU is achieved when $k$ is set to 3.
These results demonstrate that more candidates in the aggregation process enhance diversity, thereby increasing the likelihood of providing contextually appropriate information.
However, it shows convergence around top-3.
We attribute this to the inclusion of candidates with lower estimated scores, such as degenerated sentences.
Hence, as $k$ increases, the improvement reaches a plateau.






\section{Results with Additional Models}
\label{apdx: Experiments with Additional Models}

We report results with diverse GPT models, GPT-3.5 and GPT-4o-mini, in Table~\ref{tab: GPT models}. 
The version of \texttt{gpt-3.5-turbo-1106} and \texttt{gpt-4o-mini-2024-07-18} are employed for GPT-3.5 and GPT-4o-mini, respectively.


\begin{table*}[t]
\centering
\renewcommand{\arraystretch}{0.9}
\begin{adjustbox}{width=0.95\textwidth, center}
\begin{tabular}{lcccccccccccc}
\Xhline{3\arrayrulewidth}

\multirow{2}{*}{\textbf{Model}}  & \multicolumn{3}{c}{\textbf{Korean$\rightarrow$Italian}} & \multicolumn{3}{c}{\textbf{Italian$\rightarrow$Korean}} & \multicolumn{3}{c}{\textbf{Arabic$\rightarrow$Portuguese}} & \multicolumn{3}{c}{\textbf{Portuguese$\rightarrow$Arabic}}\\ \cline{2-13}
 & BLEU & chrF++ & COMET & BLEU & chrF++ & COMET  & BLEU & chrF++ & COMET & BLEU & chrF++ & COMET \\ \hline\hline 

\textit{\textbf{Standalone NMT System}}\\ \hdashline[3pt/3pt]
GPT-3.5~\cite{gpt3.5}       &          12.77 &           40.13 &           82.58 &           15.28 &           21.13 &           85.91 &            25.40 &           50.23 &           85.06 &           14.73 &           40.81 &           84.37 \\
GPT-4~\cite{gpt4}         &          14.07 &           42.22 &            86.80 &           17.23 &           22.96 &           86.94 &           25.82 &           51.89 &           85.46 &           15.11 &           41.39 &           83.99 \\ 
GPT-4o-mini~\cite{gpt4o} &
13.39 & 41.35 & 85.45 &
17.15 & 22.80 & 85.22 &
23.98 & 50.25 & 84.47 &
15.00 & 40.72 & 84.44 \\
GPT-4o~\cite{gpt4o} &
15.11 & 42.59 & 85.93 &
17.20 & 22.82 & 85.31 &
27.28 & 52.57 & 85.90 &
16.28 & 42.40 & 83.82 \\
\hline


\textit{\textbf{Proposed Method}}\\ \hdashline[3pt/3pt]
\ours (GPT-3.5; top3) &          15.07 &           39.87 &           83.13 &           15.08 &           21.12 &           86.03 &           26.83 &           50.23 &           85.46 &           15.65 &            41.10 &           84.13 \\
\ours (GPT-3.5; \textit{D, E})   &          16.44 &           41.53 &           85.23 &            16.60 &           22.64 &           87.84 &           27.49 &           50.63 &           84.35 &           16.12 &           41.59 &           83.82 \\
\ours (GPT-4; top3)   &          16.66 &           42.85 &  \textbf{86.82} &           17.95 &           23.84 &            87.50 &           27.22 &           51.73 &  \textbf{85.65} &           16.53 &           42.41 &           84.46 \\
\ours (GPT-4; \textit{D, E})     &  17.10 &  43.29 &           85.92 &  18.18 &  24.05 &  \textbf{88.74} &  27.98 &  52.41 &           85.27 &  17.02 &  43.02 &  \textbf{84.82} \\

\ours (GPT-4o-mini; top3) &
16.25 & 41.43 & 83.87 &
17.06 & 23.10 & 88.09 &
27.10 & 50.80 & 85.26 &
16.16 & 41.79 & 84.27 \\
 
\ours (GPT-4o-mini; \textit{D, E}) &
16.58 & 41.84 & 85.16 &
17.44 & 23.51 & 87.12 &
28.19 & 51.44 & 84.65 &
17.21 & 42.63 & 83.11 \\
 
\ours (GPT-4o; top3) &
17.77 & 43.38 & 85.46 &
18.08 & 23.98 & 88.15 &
28.62 & 52.53 & 85.87 &
16.92 & 42.93 & 84.52 \\
 
\ours (GPT-4o; \textit{D, E}) &
\textbf{18.02} & \textbf{43.46} & 86.19 &
\textbf{18.31} & \textbf{24.32} & 88.33 &
\textbf{29.50} & \textbf{53.16} & \textbf{86.03} &
\textbf{17.66} & \textbf{43.73} & 84.27 \\


\Xhline{3\arrayrulewidth}
\end{tabular}
\end{adjustbox}
\caption{Results on Korean$\leftrightarrow$Italian and Arabic$\leftrightarrow$Portuguese, with diverse models.} 
\label{tab: GPT models}
\end{table*}





\section{Impact of Ranking Strategies for Candidate Selection}
\label{sec:apdx_cands selecting method}

\begin{table}[h]
\centering
\small
\begin{adjustbox}{width=\columnwidth, center}
\renewcommand{\arraystretch}{1.0}
\begin{tabular}{lccc}
\Xhline{3\arrayrulewidth}
\multirow{2}{*}{\textbf{Method}} &  \multicolumn{3}{c}{\textbf{Korean$\rightarrow$Italian}} \\ \cline{2-4} 
 & BLEU & chrF++ & COMET \\ \hline\hline 
\textsc{PairRanker}~\cite{llm-blender} & \textbf{16.74} & 42.82 & 85.92\\
COMETkiwi~\cite{rei2022cometkiwi} & 16.66 & \textbf{42.85} & \textbf{86.82} \\
\Xhline{3\arrayrulewidth}
\end{tabular}
\end{adjustbox}
\caption{Impact of candidate ranking strategies.}
\label{tab:candidates selecting method}
\end{table}


In this experiment, we compare the case of using the \textsc{PairRanker}~\cite{llm-blender} and the case of using COMETkiwi~\cite{rei2022cometkiwi} for a ranking stage.
Table~\ref{tab:candidates selecting method} compares the results after selecting the top-3 using \textsc{PairRanker} and COMETkiwi.
As shown in the results, the difference in the final ensemble scores using the two ranking methods is not significant.
We believe this is because the candidates selected by both ranking methods are similar.
There are 979 out of 2000 test sets (48.95\%) where the top-3 candidates selected by both ranking methods are the same. 
In cases with 2 out of 3 matches, there were 1533 instances (76.65\%).
Given the similarity in predictions by both ranking methods, the final scores exhibit comparable performance, except in the case of COMET.
From the cost perspective, \textsc{PairRanker} requires comparisons for $O(N^2)$ unique pair combinations depending on the number of candidates $N$.
However, COMETkiwi only needs to sort the scores of $N$ candidates, resulting in a time complexity of $O(N\log N)$.
Therefore, due to its computational efficiency, we use COMETkiwi to rank candidates.


