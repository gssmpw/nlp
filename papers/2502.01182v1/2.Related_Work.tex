\section{Related Work}


\minisection{Pivot-based approaches}
Pivot translation is an approach that decomposes the translation task into two sequential steps~\cite{wu-wang-2007-pivot, utiyama-isahara-2007-comparison}.
By transferring knowledge from high-resource pivot languages, pivoting is especially effective in translation between low-resource languages \cite{zoph-etal-2016-transfer, aji-etal-2020-neural, he-etal-2022-tencent}.
In this study, pivot translation enables us to obtain high-quality candidates for the ensemble.
\citet{kim-etal-2019-pivot} discusses a pivot-based transfer learning technique where source$\rightarrow$pivot and pivot$\rightarrow$target models are first trained separately, then use pre-trained models to initialize the source$\rightarrow$target model, allowing effective training of a single, direct NMT model.
\citet{zhang-etal-2022-triangular} further investigate the transfer learning approach by utilizing auxiliary monolingual data.


Pivot translation typically employs English as the bridge language.
Nonetheless, previous studies have explored the use of diverse pivot languages, taking into account factors such as data size and the relationships between languages~\cite{paul2009importance, dabre-etal-2015-leveraging}.
By leveraging the ability of pivot translation to produce diverse outputs, several studies have focused on generating paraphrases~\cite{mallinson-etal-2017-paraphrasing, guo2019zeroshot}.
More recently, \citet{mohammadshahi-etal-2024-investigating} uses pivot translation for ensemble, but it requires computing token-level probabilities and fails to improve translation.
Our work shares the motivation with these studies, generating translations depending on the pivot path to obtain a variety of candidates.


\minisection{Ensemble in NLG tasks}
Ensemble learning is a widely adopted strategy to obtain more accurate predictions by employing multiple systems~\cite{sagi2018ensemble}.
In NMT, the traditional approach involves averaging the probability distributions of the next target token, which is predicted at each decoding step by multiple models ~\cite{bojar-etal-2014-findings} or by different snapshots~\cite{huang2017snapshot}.
When multiple sources are available, an ensemble can be conducted with predictions obtained by different sources~\cite {firat-etal-2016-zero}.
Also, a token-level ensemble through vocabulary alignment across LLMs has also been proposed~\cite{eva}.
However, these methods are not applicable to recent black-box models as they cannot compute token-level probabilities at decoding time.


Selection-based ensemble has also been explored, which chooses the final output among the existing candidates.
This can be achieved through majority voting by selecting the most frequent one~\cite{wang2022rationaleaugmented} or selecting the best candidate with QE~\cite{fernandes-etal-2022-quality, howgood}.
Recently, MBR decoding~\cite{GOEL2000115, mbr}, which aims to find the hypothesis with the highest expected utility, has gained attention.
However, this approach limits the final output space to the existing candidate pool.


\begin{figure*}[t]
  \centering
  \includegraphics[width=0.95\textwidth]{Figures/overview.pdf} 
  \caption{Overview of \ours framework.}
  \label{fig:overall}
\end{figure*}


On the other hand, the generation-based ensemble method involves generating a new final prediction.
Fusion-in-Decoder~\cite{fid} proposes an architecture that aggregates additional information with a given input.
More recently, within the context of LLMs, \citet{llm-blender} and \citet{exchangeofthought} investigate a method of using LLMs to generate multiple outputs and aggregate them.
Generating new output through LLMs offers the benefit of explicitly harnessing their pre-trained knowledge within the ensemble process.