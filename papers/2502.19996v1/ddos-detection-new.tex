\section{DDoS Detection}\label{ddos-detection}
In this section, we report existing works which focus on DDoS detection.
DDoS attacks pose significant threats to the stability and security of online services.
Therefore, effective detection of such attacks are paramount.
We classify existing detection methods into five distinct categories, each with its approach to identifying DDoS activities.
Figure~\ref{fig:taxonomy-ddos-detection} shows the overview.
Specifically, compared with existing surveys (Section~\ref{subsec:related-work}), our work proposes a more comprehensive detection taxonomy including five categories.
Besides behavior-based, statistics-based, and learning-based detection methods (Section~\ref{subsec:behavior-based-detection} - Section~\ref{subsec:learning-based-detection}), our work covers the adversarial-based detection methods (Section~\ref{subsec:adversarial-ddos-detection}) and botnet detection methods (Section~\ref{subsec-iot-botnet-detection}) which are rarely discussed in existing works.
\begin{figure*}
    \centering
    \includegraphics[scale=.35]{figures/ddos-detection-taxonomy.pdf}
    \caption{Taxonomy of DDoS detection methods.}
    \label{fig:taxonomy-ddos-detection}
\end{figure*}
\begin{itemize}
    \item \textit{Behavior-based detection.}
    These methods are predicated on the premise that legitimate users and attackers exhibit inherently different interaction patterns with network resources.
    The detection process involves continuous monitoring and analysis of network traffic to capture and assess behavioral signatures that can differentiate between benign and malignant traffics.
    \item \textit{Statistics-based detection.}
    Statistics-based detection techniques employ mathematical models to analyze network traffic.
    These methods utilize various statistical metrics (e.g., entropy) to establish normal traffic profiles.
    When certain metrics exceed predefined thresholds, an alert is triggered, indicating a possible DDoS attack.
    \item \textit{Learning-based detection.}
    Learning-based detection represents a sophisticated category that harnesses the power of machine learning (ML) and deep learning (DL) algorithms.
    These systems are trained on large datasets of network traffic to distinguish between benign and malicious flows.
    \item \textit{Adversarial DDoS detection.}
    Adversarial DDoS detection focuses on identifying traffic that has been manipulated to evade traditional detection systems.
    \item \textit{IoT Botnet detection.}
    Unlike the other categories, IoT Botnet detection methods concentrate on the early stages of DDoS attacks, namely the botnet formation phase.
    These techniques strive to identify compromised IoT devices that are being recruited into a botnet
\end{itemize}

\subsection{Behavior-Based Detection}\label{subsec:behavior-based-detection}
In this section, we delve into detection techniques by analyzing the unique behavioral patterns of attack traffics.
Recognizing that various DDoS attack strategies—such as flooding and reflection—exhibit distinct behavioral signatures, these detection methods are often specialized to effectively target and identify the specific characteristics of a given attack type.
To provide a coherent and methodical overview, we categorize these detection techniques by the attack types they are designed to detect.
\subsubsection{Flooding DDoS}
To tackle the issue of detecting flooding traffics produced by malicious bots, Scherrer et al.~\cite{scherrer2023albus} introduced ALBUS. The fundamental principle behind ALBUS is that malicious flows exhibit a pattern of consistently high data transfer rates within short time frames, in contrast to legitimate flows, which may only sporadically experience bursts and not sustain them.
As a result, ALBUS employs the Leaky Bucket (LB) algorithm to monitor data packet flow, effectively identifying when a flow exceeds a predefined rate, indicative of a burst.
Due to the potential vast number of flows, continuously monitoring all of them would be impractical because of the excessive memory consumption it would require. To mitigate this, ALBUS uses flow sampling, where only a subset of flows is monitored at any given time. This selection is made by mapping flows to specific monitoring points, referred to as checkpoints, using a hashing function. Checkpoints continuously monitor flows that exhibit persistent bursty behavior, thereby increasing the likelihood of accurately identifying abnormal flows. In contrast, flows that do not consistently show burstiness are removed from monitoring and classified as benign.

Tandon et al.~\cite{tandon2021defending} develop a system named FRADE, which employs heuristic rules to distinguish between application-layer DDoS requests and legitimate requests. The core concept is that despite similarities in requests generated by bots and human users, there are discernible differences in the dynamics of their activities—specifically, the frequency and sequence of page visits. To detect malicious requests, FRADE analyzes web server access logs to calculate the rate of client-server interactions and the transition probabilities between page pairs. A request flow is flagged as suspicious if the frequency of page visits is excessively high or if the transition probabilities between pages are unusually low.
Furthermore, FRADE incorporates a honeypot strategy by deploying special web objects, such as hyperlinks that are not typically navigated to by human users. Interaction with these honeypot elements is considered a strong indicator of malicious flows.
Flows that activate these traps are deemed malicious, and their subsequent requests are consequently blocked, thereby impeding their ability to participate in a DDoS attack.

\subsubsection{Low-Rate DDoS}
Low-rate DDoS attacks (e.g., Slowloris) operate by sending low volumes of network traffic to exploit vulnerabilities and monopolize critical, limited resources.
Unlike flooding DDoS, these low-rate attacks are subtle, cost-effective, and increasingly common.
To detect these attacks, Tandon et al.~\cite{tandon2023leader} introduce Leader, a defense mechanism that exploits the resource consumption patterns.
Rather than identifying characteristic patterns of low-rate traffics, Leader employs OS-level tracing to monitor detailed resource usage—like CPU cycles, memory allocation, and sequences of processing function calls—for each network connection.
By using this resource usage data as a basis, Leader constructs a model of normal traffic behavior using one-class SVM and elliptic envelope techniques.
During its operational phase, Leader employs anomaly detection to scrutinize each incoming request against the established model.
Requests that diverge significantly from the model are flagged as potential threats.

An important feature of pulse-wave DDoS attacks, which include varieties such as the Shrew and RoQ attacks, is that these attacks dynamically adjust the attack traffic pattern (e.g., pulsing interval) and exploit the reaction time of the detection system, rending them ineffective.
To identify the malicious flows at line rate and in real-time, Alcoz et al.~\cite{alcoz2022aggregate} present ACC-Turbo, which re-imagines the standard Aggregate-based Congestion Control (ACC) mechanism by integrating a real-time clustering algorithm.
By doing so, it can detect the onset of pulse-wave patterns as they emerge, rather than after they have established a foothold.

\subsubsection{Link Flooding DDoS}
Link Flooding Attacks (LFAs, e.g., Crossfire) target routing systems and have garnered increasing attention from security analysts. These attacks involve bots sending packets to publicly accessible decoy servers, which indirectly flood a node that is not an apparent target. This "under-the-radar" activity cumulatively floods specific target links, impacting the intended target node. Given the severity and stealthiness of LFAs, a range of detection methods have been proposed.

Liaskos et al.~\cite{liaskos2018network,rezapour2021rl} employ traffic engineering techniques, combined with reinforcement learning, to identify malicious flows involved in LFAs.
The underlying principle is that bots will shift their decoy servers and select new critical links to maintain the impact on the target whenever the network topology is altered.
Network operators can exploit this behavior by periodically changing the network topology and monitoring the flows that frequently contribute to the congestion of critical links.
A reinforcement learning strategy is utilized: The probability that a flow is malicious is increased each time it is associated with a congestion event.
In a similar vein, Gkounis et al.\cite{gkounis2016interplay,liaskos2016novel} suggest traffic rerouting to compel malicious flows to relocate to new destinations, thereby hastening their detection.
Kang et al.\cite{kang2016spiffy} propose a rerouting scheme designed to make malevolent flows escalate their traffic volume, which ultimately leads to their detection.
Kang et al.~\cite{kang2016spiffy} also analyze the behavior of a cost-sensitive attacker who employs a consistent and optimal strategy for sending traffic. SPIFFY, their proposed method, increases the bandwidth of a targeted bottleneck link and observes the traffic response. Legitimate traffic sources generally adjust their throughput in response to the added bandwidth, whereas attack flows, likely already at their maximum capacity, do not change their rate as significantly, allowing for the detection of the malicious flows.

Ma et al.~\cite{ma2019randomized} introduce a game-theoretic detection approach to combat LFAs within the constraints of available resources and against adaptive adversaries. Their technique models the conflict as a Stackelberg security game, wherein the defender employs a randomized mixed-detection strategy to optimize detection effectiveness. This strategy unpredictably varies the monitored links, complicating the task for a knowledgeable adversary to evade detection without exhibiting malicious patterns. By integrating models of rational and boundedly rational adversary behavior, the defense adapts dynamically to potential actions of the adversary, with the aim of identifying traffic anomalies indicative of an ongoing LFA.

\subsubsection{Amplification DDoS}
Amplification DDoS attacks pose significant challenges to internet security, often leveraging IP spoofing to magnify their impact. Research in this domain has primarily focused on three aspects: address spoofing detection, request validation, and honeypots.

To counter IP spoofing, Dainotti et al.~\cite{dainotti2013estimating} estimate legitimate address spaces within autonomous systems (AS) by reconstructing bidirectional flows from NetFlow records.
They filter out flows with a high packet volume, assuming these are likely non-spoofed, and use the source IP addresses of these flows to establish a baseline of legitimate address spaces.
Similarly, Lichtblau et al.\cite{lichtblau2017detection} analyze BGP routing data to map relationships among AS and their associated address spaces, facilitating the identification of legitimate source-destination IP pairs
When traffic with source and destination addresses mismatching any legitimate pair is captured, it is flagged for potential spoofing, enabling administrators, such as ISPs, to filter out the suspicious traffic.

Other studies focus on DNS request verification by pairing queries and responses within the same DNS transaction to identify components of a reflection attack~\cite{di2011protecting,dai2024dampadf}.
Di Paola et al.\cite{di2011protecting} employ Bloom filters to efficiently store and lookup request information, while Dai et al.\cite{dai2024dampadf} enhance this approach with DAmpADF.
Utilizing two Bloom filters, DAmpADF alternately records DNS requests, reducing false positives.
Moreover, it identifies popular DNS servers using an exponential-weakening decay method, allowing requests and responses from these servers to bypass Bloom filter recording.

Honeypots are also deployed to detect malicious hosts attempting to initiate amplification DDoS attacks.
By simulating vulnerable application protocols, such as DNS, honeypots act as reflectors for attackers, thus capturing and characterizing attack traffic.
The Cambridge Cybercrime Center (CCC)\cite{thomas20171000}, AmpPot\cite{kramer2015amppot}, and Honeypot Platform for Intrusion (HPI)~\cite{griffioen2021scan} all implement threshold-based detection methods to distinguish attack flows, although these methods may miss multi-protocol attacks or those below local traffic thresholds.
In response to these limitations, Wagner~\cite{wagner2021united} advocates for a collaborative detection approach.
By sharing information through a DDoS Information Exchange Point (DXP), participating mitigation platforms across different vantage points can detect a significantly higher percentage of attack traffic.

Complementing the above methods, Krupp et al.~\cite{krupp2021bgpeek} introduce the BGPEEK-A-BOO technique, leveraging the attackers' reliance on their service provider's BGP routes.
This method involves BGP Poisoning to isolate specific AS, enabling the traceback of spoofed traffic sources through observations of TTL fluctuations or cessation of attack traffic.
By deploying amplification honeypots and systematically analyzing traffic changes, the method traces malicious flows back to their originating AS.
The approach, validated through simulations and real-world experiments, demonstrates its effectiveness in identifying the sources of malicious traffic without requiring prior knowledge of the attacker or external cooperation.

\subsection{Statistics-Based Detection}
This section provides an overview of current statistics-based detection methods. Initially, we explore the foundational concepts behind these methods and the specific metrics employed for detection.
Subsequently, we delve into the sketching technique, a strategic optimization method designed to mitigate memory and storage constraints in scenarios with a large volume of network flows.

\subsubsection{Metric Selection}
Entropy is a widely used statistical metric that measures the randomness present in packet attributes, serving as a key indicator in identifying anomalous traffic patterns indicative of flooding DDoS attacks.
Kalkan et al.~\cite{kalkan2018jess} introduced JESS, an entropy-based detection method that utilizes joint entropy to assess the randomness across combined attributes of network flows, such as destination IP addresses and transport-layer flags.
By calculating the joint entropy for these attribute sets, JESS effectively distinguishes between normal traffic and potential flooding attacks.
A flow with abnormally low joint entropy is flagged as suspicious, as it likely represents a concerted effort to overwhelm a network resource.

Flow statistics (e.g., flow numbers, flow sizes) are also widely adopted, especially for the detection of link flooding attacks.
For instance, the LinkScope system~\cite{xue2014towards} implements a two-tiered measurement strategy.
Initially, it identifies links that serve a significant number of downstream servers and are, therefore, attractive targets for attackers.
Subsequently, LinkScope employs both end-to-end and hop-by-hop measurements, such as packet loss rates and round-trip times, to monitor these links.
The system then applies the Cumulative Sum (CUSUM) algorithm to detect sudden changes in these metrics, indicative of a congested path.
By correlating the two types of measurements, LinkScope accurately localizes the link under attack.

RADAR~\cite{zheng2018realtime} also focuses on the LFA attacks and performs correlation analyses on network flow information to detect them.
RADAR collects traffic statistics from the Software-Defined Networking (SDN) data plane, applying heuristic rules to identify suspicious patterns, such as regular congestion on specific paths.
To discern malicious traffic, RADAR then conducts adaptive traffic analysis within the SDN control plane, checking whether the flow statistics match those associated with attack patterns, such as synchronized bursts of flows and congestion events.
Ripple and Mew~\cite{xing2021ripple,zhou2023mew} employ programmable switches to facilitate in-network measurement and detection of link-flooding attacks.
They maintain a network-wide "defense panorama," which is a synchronized view of attack signals, enabling the system to monitor for and react to flooding attacks.
Their switches periodically assess congestion levels against predefined thresholds and identify suspect hosts by analyzing flow statistics, such as the number of low-rate flows between pairs of source and destination IP addresses.

\subsubsection{Sketching Optimization}
Monitoring a vast number of data flows and managing their statistics can lead to substantial memory consumption.
To address this challenge, sketching techniques have emerged as a resource-efficient solution, employing compact data structures—referred to as "sketches"—to approximate traffic statistics within well-established error margins. 

Poseidon~\cite{zhang2020poseidon} leverages the Count-Min sketch to estimate the sizes of network flows.
The process begins when a flow is identified, at which point a series of hash functions calculate multiple indices corresponding to this flow.
The Count-Min sketch then increments the flow counters in the register arrays at these indices.
To estimate a flow's size, Poseidon retrieves the values from the counters and adopts the smallest value as the estimate.
This method allows users to specify thresholds to identify potentially malicious flows that exhibit abnormal sizes.
While Poseidon is adept at measuring flow size, it does not cater to other network statistics.
To bridge this gap, Jaqen~\cite{liu2021jaqen} introduces the concept of universal sketches.
Utilizing the Count Sketch, Jaqen is capable of estimating a broader range of network statistics, such as source IPs and source port numbers. 
This versatility enables users to set precise thresholds for specific network features, thereby facilitating more granular anomaly detection.

Tailored to the NDN network measurement, Xu et al.~\cite{xu2022towards} propose the LiEffi-FM Sketch.
This approach is predicated on the observation that during a DDoS attack, malicious bots tend to generate a significant increase in Interest packet requests that share the same name prefix but differ in requested data content.
The LiEffi-FM Sketch efficiently monitors these Interest packets at NDN routers, probabilistically counting the unique data requests linked to a common name prefix.
Xu et al. then employ Monte Carlo hypothesis testing to establish a threshold for distinguishing between benign and malicious requests, thereby enabling ongoing protection with minimal resource expenditure.

\subsection{Learning-Based Detection}\label{subsec:learning-based-detection}
Machine learning and deep learning are critical in DDoS detection for their ability to rapidly analyze and respond to complex and evolving threats.
Unlike traditional methods, these technologies adapt and learn, identifying attack patterns and potential vulnerabilities in real-time, thus enhancing the capability to predict, detect, and mitigate DDoS attacks effectively, ensuring greater network security and service uptime.
In this section, we revisit existing learning-based techniques.

\subsubsection{Machine Learning}
Recent research has leveraged both clustering and classification techniques to enhance the detection of malicious network flows.
Ahmed et al.~\cite{ahmed2018statistical} proposed a method where clustering is used to create distinct fingerprints of web applications by collecting packet-level features, such as IP addresses, and stream-level features, like the number of bytes transferred from client to server.
These fingerprints are then grouped using efficient clustering algorithms, e.g., Mean-shift.
The resulting clusters are associated with corresponding applications, such as HTTP or SMTP, by using labeled training data.
Consequently, when analyzing a client request flow, its fingerprint is compared against this repository to identify the application it corresponds to.
Unmatched flows are considered suspicious.

In parallel, Qin et al.\cite{qin2015ddos} adopted entropy-based features, such as packet size and flow duration, employing the K-means algorithm to distinguish between benign and malicious traffic.
By modeling the normal patterns of request behavior, this technique flags flows with entropy vectors that deviate significantly from those of benign clusters as potential threats.
Bhatia et al.\cite{bhatia2021mstream} further extend this concept by clustering flows that exhibit large volumes of similar activities, considering both categorical and numerical attributes to detect suspicious patterns.

On the classification front, MM et al.\cite{mm2022efficient} introduced a novel approach that utilizes Kernel Principal Component Analysis to refine and select the most relevant flow features, followed by training a Support Vector Machine-based classifier to sort the flow samples.
Similarly, Panigrahi et al.\cite{panigrahi2022intrusion} applied Multi-Objective Evolutionary Feature Selection to pinpoint the most informative flow features and employed a combination of Decision Table and Naive Bayes classifiers to categorize network traffic.
Eshete et al.~\cite{eshete2017dynaminer} design a system, DYNAMINER, which abstracts HTTP transactions into Web Conversation Graphs (WCGs) to capture these dynamics.
The temporal changes reflected in the WCGs, such as node degree variations, are used to train an ensemble random forest classifier to distinguish between benign and compromised flows.


\subsubsection{Deep Learning}
Autoencoder is arguably the most popular deep model for DDoS detection.
The Kitsune framework, as introduced by Mirsky et al.~\cite{mirsky2018kitsune}, employs an ensemble of autoencoders that operate online and in an unsupervised manner to differentiate between normal and anomalous traffic patterns.
This is achieved by feeding network traffic instances to the ensemble, where each autoencoder attempts to reconstruct traffic feature subsets.
The reconstruction quality is assessed using the root mean squared error (RMSE) metric, and a collective RMSE from all autoencoders is produced.
An output module then evaluates this aggregate RMSE against a decision threshold to determine the traffic's nature—benign or potentially malicious.
Aktar et al.\cite{aktar2023towards} propose a deep learning model that leverages a contractive autoencoder to detect DDoS anomalies, furthering the application of autoencoders in this field.

Other deep models, e.g., multi-layer perceptron and LSTM, are also widely used.
Diallo et al.\cite{diallo2021adaptive} introduced ACID, which utilizes a neural network with multiple kernels for effective anomaly detection.
De et al.\cite{de2021detection} selectively use three traffic features (packet count, entropy, and average inter-arrival time) to train a Multi-Layer Perceptron (MLP).
Combined with Fuzzy Logic, this MLP is particularly adept at detecting RoQ DDoS attacks due to its high accuracy.
Wang et al.\cite{wang2020dynamic} also explore MLP-based DDoS detection, employing sequential feature selection to reduce redundancy and irrelevance, and they incorporate a dynamic feedback loop to continuously adapt to changing traffic patterns.
Aydin et al.\cite{aydin2022long} developed LSTM-CLOUD, an LSTM-based system for monitoring network traffic in cloud environments, utilizing historical data to pinpoint potential DDoS attacks.
Xu et al.~\cite{xu2022xatu} exploit predictable attacker behavior and auxiliary signals from prior incidents to train an LSTM network, which employs survival analysis for early attack detection while minimizing false positives.

Graph neural networks also gained attention.
Agiollo et al.~\cite{agiollo2023gnn4ifa} address NDN Interest Flooding Attacks by representing the network as a graph and using Graph Neural Networks (GNNs) for both Supervised Attack Detection (SAD) and Unsupervised Attack Detection (UAD).
SAD classifies the network's state with a trained GNN, while UAD relies on the network's ability to reconstruct masked graph segments and detect anomalies through reconstruction errors.
Duan et al.~\cite{duan2022application} argue that existing DL methods for DDoS detection don't adequately capture IP pair interactions and topological data, which are vital for identifying anomalies.
They suggest a novel approach using Dynamic Line Graph Neural Networks (DLGNN) to analyze dynamic spatiotemporal graphs of network traffic, capturing the intricate spatial and temporal dynamics of IP communications.

Some works try to address key issues during neural network training (e.g., data collection) and detection (e.g., high false positive rates).
Wichtlhuber et al.\cite{wichtlhuber2022ixp} suggest collecting data from ISP blackholed traffic for training deep detection models, as it often contains malicious samples.
Fu et al.\cite{fu2023point} address false positives in learning-based flooding detection systems with pVoxel, which discriminates between sparse benign traffic features and dense malicious traffic features in the feature space, refining detection accuracy. Zhao et al.\cite{zhao2023ernn} improve the accuracy of learning-based systems by training a Recurrent Neural Network (RNN) with both normal and noisy traffic that includes common network-induced phenomena, using a Mealy machine to dynamically adjust the training process and enhance the system's robustness to these phenomena.

\subsubsection{Hybrid Learning Approaches}
Recent advancements have explored the integration of machine learning and deep learning techniques to enhance the accuracy and efficiency of anomaly detection systems.
Dong et al.~\cite{dong2023horuseye} introduced a two-tiered framework named HorusEye, which merges the strengths of machine learning and deep learning to initiate a robust detection process.
The framework commences with the deployment of an isolation forest model, specifically chosen for its ability to rapidly sift through and flag suspicious network traffic at high throughput rates.
Subsequent to the initial screening by the isolation model, traffic deemed suspect undergoes a more thorough investigation utilizing an Asymmetric Autoencoder (AAE).
This AAE is designed with a deep-layered encoder to effectively distill complex data representations, while its paired decoder is tasked with reconstructing the input features, maintaining simplicity to avoid unnecessary computational complexity.
The effectiveness of this analysis is quantified through the Root Mean Squared Error Loss (RMSE), which serves as the criterion for determining the abnormality within the suspect data flow.

In parallel, Long et al.~\cite{long2022hybrid} have devised a detection system employing a Stacked Sparse Autoencoder combined with a Support Vector Machine (SSAE-SVM).
Their approach leverages the autoencoder's unsupervised learning capability to distill a refined representation of the data.
The SVM then steps in, employing these refined features to classify network traffic with a heightened level of precision.

Expanding on these hybrid models, Mahadik et al.~\cite{mahadik2023edge} have crafted a sophisticated CNN-LSTM hybrid model, capitalizing on the CNN's innate proficiency in automatic feature extraction and the LSTM's capacity to retain information over extended sequences.
This model is specifically tailored to identify and classify a spectrum of DDoS attacks, ranging from binary to multi-class categories.

\subsection{Adversarial DDoS Detection}\label{subsec:adversarial-ddos-detection}
Adversarial DDoS detection is crucial in maintaining the resilience and reliability of online services in the face of increasingly sophisticated cyber threats.
In this section, we revisit recent detection techniques which targets two adversarial strategies: Encrypted malicious traffic which bypasses deep packet examination, and adversarial learning which bypasses learning-based detection systems.
\subsubsection{Encrypted Malicious Traffic}
As attackers increasingly utilize encryption, traditional detection methods struggle to identify malicious traffic hidden within encrypted data streams.
Fu et al.~\cite{fu2023detecting} tackle this challenge by recognizing that without the need to examine the encrypted packet payload, the interaction patterns between multiple attack flows exhibit distinctive characteristics compared to legitimate traffic.
They introduce HyperVision, a novel system designed to construct interaction graphs from network flows.
To reduce the complexity of these graphs, HyperVision aggregates brief flows, thereby decreasing the overall graph density.
The system subsequently divides the graph into separate connected components and employs clustering techniques based on high-level statistical indicators, such as flow count and size.
By scrutinizing the deviation of components from the cluster centroids, HyperVision effectively flags anomalies. Within these outliers, the system further clusters the edges to accurately isolate and identify the malicious flows.
This methodology provides a robust framework for the detection of encrypted malicious activities.

Complementing this, Cui et al.~\cite{cui2023cbseq} acknowledge that despite the continual evolution of malware, the fundamental objectives, such as executing DDoS attacks, can be detected through consistent network behavior patterns.
Their approach, CBSeq, strategically disregards the encrypted content of the traffic.
Instead, it focuses on the behavioral attributes of network traffic, capturing essential features like the duration and number of flows.
By clustering similar traffic patterns, CBSeq is able to outline behavior sequences that are indicative of malicious intent.
The core of CBSeq's effectiveness lies in its application of a Transformer-based model, MSFormer.
This model is adept at discerning the subtleties within these behavior sequences, thereby empowering CBSeq to distinguish between benign and malicious network traffic with high accuracy.

\subsubsection{Adversarial Learning}
Adversarial attacks pose a significant threat to learning-based detection systems.
Mustapha et al.~\cite{mustapha2023detecting} initially presented a Long Short-Term Memory (LSTM) method tailored for the detection of adversarial Distributed Denial of Service (DDoS) attacks.
However, they noted its inadequacy when confronted with a range of adversarial DDoS attack types, particularly those synthesized by Generative Adversarial Networks (GANs).
To address this limitation, they refined the LSTM detection framework by incorporating adversarial samples produced by a GAN.
This enhancement significantly bolstered the LSTM model's prowess in recognizing these sophisticated attacks.
Wang et al.~\cite{wang2023bars} address the vulnerability of deep learning-based detection systems to adversarial samples.
They introduce BARS, a robustness certification framework that enhances system resilience by applying customized noise distributions to various features according to their susceptibility to adversarial attacks.
BARS generates adversarial examples to certify and improve the detection system's robustness against such evasion tactics.
Catillo et al.~\cite{catillo2023case} investigate the robustness of both machine learning (e.g., autoencoders) and non-ML-based (e.g., decision trees) intrusion detection systems against adversarial DDoS attacks. Utilizing the CICIDS2017 dataset, their findings suggest that autoencoder-based models are more robust to adversarial samples, while decision trees are significantly more vulnerable.

Fu et al.\cite{fu2021realtime} tackled the randomized attacks, where adversaries blend benign packets with malicious ones to elude detection systems.
Their research found that frequency domain features of network traffic offer greater resilience against such evasion attempts.
By accurately representing traffic patterns with minimal information loss, these features improve detection accuracy and throughput.
Fu et al. employed Discrete Fourier Transformation (DFT) to translate time domain traffic features into the frequency domain, revealing the repetition patterns of traffic.
They then trained a classifier with these features using clustering algorithms to differentiate between benign and malicious traffic in real-time.
Complementing this approach, Fouladi et al.~\cite{fouladi2022novel} observed that specific traffic statistics, such as the count of unique source IP addresses (USIP) and the normalized number of unique destination IP addresses (NUDIP) relative to the total packet count, exhibit notable changes in both time and frequency domains during DDoS attacks.
They applied the continuous wavelet transform (CWT) to transform USIP and NUDIP statistics into two-dimensional time-frequency domain features.
These features were then fed into a Convolutional Neural Network (CNN) classifier that was trained to discriminate between normal and malicious traffic efficiently.

In parallel, Matta et al.\cite{matta2017ddos} and Cirillo et al.\cite{cirillo2021botnet} examine the challenge of detecting randomized DDoS attacks by analyzing user behavior within a botnet.
They posit that members of a botnet are likely to show less message innovation than independent users due to the coordinated nature of their activity.
To quantify this, Matta et al. introduce the Message Innovation Rate (MIR), which assesses the diversity of messages sent over time by user groups.
They also develop an algorithm, BotBuster, that identifies potential botnets by clustering users with low MIR scores.
Cirillo et al. build on this by considering scenarios where different botnet groups use distinct emulation dictionaries, and they validate that BotBuster remains effective even when multiple bot groups are present.
Feng et al.~\cite{feng2020application} propose a system to counter randomized DDoS attacks using a Markov decision process that evaluates traffic legitimacy in context, including server resource usage and client-server interaction history.
Their system employs a reinforcement learning agent to differentiate between legitimate and malicious traffic, dynamically adjusting its responses to minimize disruption to legitimate users and respond effectively to attacks.

Finally, Yang et al.~\cite{yang2021cade} address concept drift in DDoS detection models that occurs when attackers alter their behavior, causing the testing data distribution to deviate from the training data.
To combat this, they propose CADE, which refines the training process by mapping high-dimensional traffic features to a lower-dimensional latent space for clustering similar flows.
CADE then employs contrastive learning to enhance the separation between these clusters.
This method allows for the categorization of malicious samples into fine-grained sub-classes, unveiling diverse attack strategies and improving the model's training robustness against evolving threats.

\subsection{IoT Botnet Detection}\label{subsec-iot-botnet-detection}
The large and growing number of IoT devices, coupled with multiple security vulnerabilities, brings an increasing concern for launching DDoS.
As a result, instead of pinpointing malicious traffics and flows as shown in previous sections, fruitful research works focus on the detection of (infected) IoT devices and malicious device behaviors.

\subsubsection{Malware Download Activity}
Recent research in cybersecurity has targeted the detection of botnets by scrutinizing malware download patterns.
Invernizzi et al.~\cite{invernizzi2014nazca} introduced Nazca, a system that analyzes the web traffic graph to distinguish malware downloads.
The fundamental concept behind Nazca is that the collective analysis of malware downloads reveals distinct and identifiable patterns which are not evident when these downloads are viewed individually, thus exposing their malicious intent.
Nazca tracks HTTP requests, collecting metadata such as endpoints, URIs, and the presence of executable downloads.
It then identifies suspicious downloads based on anomalous traits not typical of legitimate software, like evasion techniques or connections to dubious servers.
By clustering these events, Nazca improves the accuracy of bot detection and reduces false positives.

Furthering this line of research, Kwon et al.~\cite{kwon2015dropper} studied complex malware, like trojans, that trigger subsequent downloads.
They developed a downloader graph to represent the relationships between downloaded executables on infected hosts. By examining the structural nuances of these graphs, e.g., the rate of new executable downloads and overall structure, they trained a random forest machine learning classifier to differentiate between benign and malicious download activities, enhancing the detection of compromised systems.

\subsubsection{Malware Spreading Activity}
Superspreaders are unique hosts that are characterized by their extensive network of distinct connections.
Within the realm of DDoS attacks, these superspreaders commonly represent infected machines that reach out to numerous other systems to propagate DDoS malware.
Consequently, their detection is pivotal for bolstering network security.

A fundamental approach to identifying superspreaders is by monitoring all unique destination contacts made by each host through the use of a hash table, as demonstrated by Flowscan~\cite{plonka2000flowscan}.
However, this technique, which relies on maintaining the state of each network flow, can be prohibitively memory-intensive, especially on high-speed networks where it becomes impractical.
To address the limitations of monitoring large volumes of hosts, Kamiyama et al.~\cite{kamiyama2007simple} introduced a method that employs hash-based flow sampling for pinpointing potential superspreaders.
This technique begins by sampling packets according to the hash of the flow key.
Subsequently, a Bloom filter is applied to ascertain whether the sampled packet corresponds to a new flow.
If identified as new, the associated host's counter in the host table is incremented.
Hosts whose counters exceed a predetermined threshold are then classified as superspreaders.
Although this sampling strategy is capable of keeping up with high-speed network links, its accuracy is compromised, which can be problematic.

To facilitate more efficient storage of flow information, some researchers have turned to sketching techniques.
Guan et al.~\cite{guan2009new}, for instance, developed reversible sketches that are capable of estimating the in/out degrees of hashed hosts.
However, the hash functions utilized are computationally intensive, owing to their reliance on complex arithmetic operations, particularly when processing IP addresses.
Liu et al. \cite{liu2015detection} sought to alleviate these computational demands by introducing a novel sketch called the Vector Bloom Filter (VBF).
This filter eschews the maintenance of explicit host identifiers, yet it remains capable of reconstructing the identities of superpoints and assessing their cardinalities.
Building upon these developments, Tang et al.~\cite{tang2022high} crafted the SpreadSketch, a pragmatic sketch data structure tailored for the real-time detection of superspreaders.
SpreadSketch assigns a binary hash string to each connection, which serves as an estimate of the source's fan-out.
Furthermore, by combining multiple instances of SpreadSketch, it is possible to achieve a comprehensive network-wide perspective, which is instrumental in the reconstruction and identification of all superspreaders.

\subsubsection{Malware Infection Activity}
Several research works have developed techniques to detect compromised IoT devices by examining their behavioral patterns.
Antonakakis et al.~\cite{antonakakis2017understanding} identify Mirai-infected devices by monitoring for Mirai-style scanning activities.
Herwig et al.~\cite{herwig2019measurement} detect devices infected with the Hajime malware by analyzing the public Distributed Hash Table (DHT) utilized by Hajime bots for command-and-control (C\&C) communication.
Tegeler et al.~\cite{tegeler2012botfinder} observe that botnets from the same family often exhibit consistent patterns in their C\&C communications, including specific data upload formats and timing patterns for connections to C\&C servers.
Based on these observations, they introduce BOTFINDER, a system that uses five distinct features from bot traffic, such as the average connection duration, and applies clustering to model these bot families.
This clustering helps to determine the infection status of a host and identify the type of malware present.

It is important to note that these detection methods focus on identifying devices compromised by particular malware, such as Mirai or Hajime.
Conversely, Guo et al.~\cite{guo2020detecting} propose two algorithms designed to profile network activities of devices and detect IoT devices irrespective of the specific malware.
Their methods rely on knowledge of the servers with which these devices communicate, typically operated by IoT manufacturers.
The first algorithm inspects the destination IP addresses and DNS queries in client-generated traffic; if these are associated with an IoT manufacturer, the client is likely an IoT device.
The second algorithm uses active scanning to detect IoT devices by identifying TLS certificates containing IoT manufacturer names.
These approaches enable real-time classification of devices as benign or infected by examining their network communications.

Sikder et al.~\cite{sikder20176thsense} demonstrate that benign user activities on IoT devices typically trigger a specific set of sensors, whereas infected devices often disrupt these patterns. By learning the normal sensor data patterns associated with user activities, their system, 6thSense, employs machine learning models, such as Naive Bayes, to detect anomalous sensor activity. Meanwhile, AEGIS [7] profiles the context of user activities and sensor-device interactions. It incorporates smartphone app context, like user interactions with device controls, and employs a Markov Chain-based machine learning technique to identify abnormal behaviors in smart home environments.

\subsubsection{Side Channels}
Recent research has explored the use of side channels to detect compromised IoT devices, employing signals such as electromagnetic (EM) emanations, network traffic fingerprints, and even encrypted traffic patterns.
Khan et al.~\cite{khan2019idea} demonstrate that EM signals emitted by IoT devices exhibit distinct patterns when the devices execute benign applications versus when they participate in DDoS attacks.
Their proposed system, IDEA, leverages EM signals as a side channel to discern DDoS activities on IoT devices.
IDEA operates by first establishing a baseline of EM patterns from a secure device.
Subsequent monitoring of a target device's EM signals allows for the detection of deviations from this baseline.
When EM signal reconstruction errors occur, IDEA interprets these as indicative of anomalous or possibly malicious activities.

Beyond EM signals, researchers have shown that network traffic fingerprints can act as effective side channels.
Shodan, a search engine detailed by the authors~\cite{shodan}, gathers information on devices connected to the internet by scanning IPv4 addresses across select ports.
It identifies devices by correlating textual matches, like "IP camera," with service banners and specific device information.
CAIDA extends this approach to pinpoint compromised IoT devices that initiate communication with allocated but unassigned IP addresses~\cite{torabi2018inferring}.
Censys, another tool comparable to Shodan, allows for community-contributed rules that facilitate the identification of device manufacturers and models via textual patterns in device banners~\cite{durumeric2015search}.
Furthermore, Mirian et al.~\cite{mirian2016internet} focus on industrial control systems (ICS), scanning the IPv4 space with ICS-specific protocols.
Their findings unearth over 60,000 publicly accessible ICS devices, which could potentially be targets for DDoS exploits.

The work by Acar et al.~\cite{acar2020peek} indicates that even when IoT communication is encrypted, valuable insights can be gleaned from the analysis of metadata such as packet lengths and traffic rates.
For example, the pattern of larger packets is often associated with a smart camera transmitting video data, whereas smaller packets might indicate a temperature sensor's data transmission.
Through the application of machine learning techniques, such as k-Nearest Neighbors (kNN), on the sniffed encrypted traffic and its metadata, classifiers can determine device types, states, and user behaviors.
Anomalies in these classifications can signal abnormal device operations, such as involvement in flooding attacks.
Additional studies, like Meidan et al.\cite{meidan2017profiliot}, apply machine learning models to LAN-side measurements to identify IoT devices based on their traffic flow statistics.
They utilize an array of features from network, transport, and application layers, e.g., the number of bytes and HTTP GET requests.
Le et al.~\cite{le2019unearthing} take a unique approach by converting DNS names into embeddings—numeric representations that encapsulate the semantic content of the DNS names.
A deep learning model, specifically a multi-layer perceptron, then classifies devices as IoT or non-IoT based on these embeddings derived from their DNS queries.

\subsection{Summary}
In this section, we outline the essential features required for effective DDoS detection in emerging technologies.
We also address the challenges involved in implementing these features and offer practical guidelines for developing advanced detection systems.

\textit{\textbf{Accurate differentiation between normal clients and adversaries enhances DDoS detection.}}
    Identifying deviations in behavior is an effective strategy for delineating legitimate user activity from DDoS activities.
    Such deviations can be discerned through various indicators.
    For example, anomalies in network traffic bursts, patterns of page visits, resource demands triggered by incoming requests, and instances of address spoofing serve as reliable markers to distinguish between ordinary users and malicious adversaries.
    Incorporating these behavioral signatures into the framework of statistical and machine learning algorithms holds the potential to advance the development of a sophisticated and adaptive DDoS detection system.
    This fusion of behavioral analysis with computational techniques can enhance the system's ability to respond to evolving attack vectors in real-time.
    
\textit{\textbf{Attack-agnostic detection methods are preferred, but it is important to minimize the occurrence of false positives.}}
    DDoS attacks manifest in multiple forms, such as volumetric flooding and low-rate attacks.
    These variations underscore the necessity for a robust and versatile DDoS detection mechanism capable of identifying a broad spectrum of attack patterns.
    Traditional detection methods, tailored to specific attack types, offer high accuracy by leveraging characteristics unique to each attack.
    However, the practical deployment of such specialized detection methods in diverse network environments is impractical for several reasons.

    First, the deployment of attack-specific detection systems often entails substantial costs, which are exacerbated in networks with limited resources, such as those incorporating programmable switches.
    The financial and computational overheads make these methods less viable in resource-constrained settings.
    Second, the dynamic nature of attack strategies poses a significant challenge
    Attackers frequently modify their tactics, necessitating an equally dynamic and responsive detection policy.
    Designing such a flexible policy enforcement that can quickly adapt to changing attack patterns is a complex task.
    Third, the rapid emergence of new DDoS attack vectors complicates the maintenance of up-to-date detection strategies.
    Real-time updates to supervised detection models, such as retraining with new data and relabeling, are hindered by high deployment costs and inevitable delays.
    This lag leaves networks vulnerable to novel attacks.

    In light of these challenges, unsupervised learning emerges as a promising avenue for achieving generalized DDoS detection.
    This approach involves modeling the normal traffic patterns of various application scenarios.
    With a baseline of expected behavior established, the detection system can proactively identify anomalies, regardless of the specific attack technique employed.
    Moreover, unsupervised learning systems can adapt over time, adjusting their baseline models to reflect evolving legitimate user behaviors.
    This continuous adaptation is critical for maintaining detection accuracy in dynamic network environments.

    Despite its potential, unsupervised learning is not without its pitfalls.
    Notably, the tendency for high false-positive rates must be addressed to prevent the erroneous classification of legitimate traffic as malicious.
    Systems like Nazca and DAmpADF represent significant strides in overcoming these challenges, offering refined algorithms to mitigate the issue of false positives while maintaining robust detection capabilities.

\textit{\textbf{Efficient detection is crucial for managing the substantial volumes of modern network traffic.}}
    To effectively manage the escalating volume of network traffic, DDoS detection systems must ensure efficient flow processing speeds and optimized memory utilization.
    The burgeoning traffic presents complex challenges in terms of both processing and storage of flow data.

    In response to these challenges, researchers have proposed various solutions, which can be broadly categorized as software-based or hardware-based approaches.
    Among the software-based strategies, sketching stands out.
    This method employs a streamlined data structure that trades a marginal loss of accuracy for substantial reductions in memory requirements.
    Unlike conventional data structures, such as hash tables, sketches are far more scalable in the face of increasing data volumes.
    Furthermore, their compact design enables faster processing speeds, which is critical for real-time DDoS detection and mitigation.
    
    On the hardware front, the advent of programmable switches has been a game-changer.
    These devices come equipped with advanced hardware primitives, e.g., in-network packet parsing, which facilitate packet parsing and processing at line rates.
    Additionally, programmable switches offer remarkable flexibility; they can be updated to comprehend newly emerging protocols and to address novel attack vectors.
    This adaptability provides a significant advantage over traditional fixed-function hardware, which lacks the capability to evolve in tandem with the dynamic nature of network threats.

\textit{\textbf{Cross-domain data sharing can increase detection accuracy.}}
    DDoS attacks are often characterized by their distributed nature, which can span multiple jurisdictions and involve countless infected devices.
    The complexity and scale of these attacks necessitate a coordinated response from various entities within the internet infrastructure.
    Effective collaboration among Autonomous Systems (ASes), Internet Service Providers (ISPs), and routers is crucial for timely detection and mitigation of these threats.

    Programmable switches, like those highlighted in the work of Jaqen and Mew, demonstrate the potential of such collaboration.
    These switches enable the sharing of monitoring metrics across different ASes and ISPs, facilitating a comprehensive overview of network conditions.
    This shared intelligence can be pivotal for several reasons:
    (1) Proactive Threat Intelligence Sharing.
    By exchanging traffic information, ASes and ISPs can identify emerging attack patterns early and disseminate warnings to preempt potential attacks.
    (2) Resource Optimization: Collaboration allows for pooling resources and expertise, which can lead to more efficient use of available bandwidth and computational power.

    While sharing information is vital for defense, it also raises concerns about user privacy and data protection.
    ASes and ISPs must establish trust relationships and agree on frameworks that respect privacy laws and user consent.