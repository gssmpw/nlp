\documentclass{article}

% Language setting
% Replace `english' with e.g. `spanish' to change the document language
\usepackage[english]{babel}

% Set page size and margins
% Replace `letterpaper' with`a4paper' for UK/EU standard size
\usepackage[letterpaper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

% Useful packages
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{svg}
\usepackage{siunitx}  % For aligning numbers by decimal point
% \usepackage{enumitem}

\usepackage{hyperref}
\usepackage{pifont}   % for \ding{55}
\usepackage{booktabs} 
\usepackage{colortbl} % for \rowcolor

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

\usepackage{algorithm}
\usepackage{algorithmic}

% For theorems and such
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{enumitem}
\usepackage{natbib}
\usepackage{float}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}
\usepackage{colortbl}
\usepackage{graphicx}
\usepackage{subcaption}

\renewcommand{\algorithmiccomment}[1]{%
    $\triangleright$\ \textcolor{gray!90}{#1}%
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{example}[theorem]{Example}
\theoremstyle{remark}

% =========================================================
% 1. NEW COMMANDS FOR E, Var, AND Cov
% ---------------------------------------------------------
%    Usage:
%       \EE{X}                 -> E[X]
%       \EE[\mu]{X}            -> E_\mu[X]
%       \Var{X}                -> Var[X]
%       \Var[\mu]{X}           -> Var_\mu[X]
%       \Cov[X,Y] etc.
% ---------------------------------------------------------
\newcommand{\EE}[2][]{\mathbb{E}_{#1}\!\left[#2\right]}
\newcommand{\EEInline}[2][]{\mathbb{E}_{#1}[#2]}

\newcommand{\Var}[2][]{\mathrm{Var}_{#1}\!\left[#2\right]}
\newcommand{\VarInline}[2][]{\mathrm{Var}_{#1}[#2]}
\newcommand{\Cov}[2][]{\mathrm{Cov}_{#1}\!\left[#2\right]}
\newcommand{\CovInline}[2][]{\mathrm{Cov}_{#1}[#2]}
\newcommand{\Corr}[2][]{\mathrm{Corr}_{#1}\!\left[#2\right]}
\DeclareMathOperator*{\argmin}{arg\,min}

\newcommand{\simindep}{\,{\buildrel \text{ind} \over \sim\,}}
\newcommand{\simiid}{\,{\buildrel \text{iid} \over \sim\,}}
\newcommand{\classicalstylized}{\hat{\theta}^{\text{cl}}}
\newcommand{\ppi}{\hat{\theta}^{\text{PPI}}}
\newcommand{\mlbaseline}{\tilde{Z}^f}
\newcommand{\ptppi}{\hat{\theta}^{\mathrm{PT}}}

\newcommand{\algorithmicreturn}{\textbf{return}}
\newcommand{\RETURN}{\STATE \algorithmicreturn}

\newcommand{\tbf}[1]{\texttt{\textbf{#1}}}

\newcommand{\update}[1]{\textcolor{blue}{#1}}
\newcommand{\verify}[1]{\textcolor{purple}{#1}}

\usepackage{xcolor}    % For coloring text
\usepackage[normalem]{ulem} % For strikethrough (with 'normalem' to preserve \emph)

\title{Prediction-Powered Adaptive Shrinkage Estimation}

\author{
\begin{tabular}{ll}
Sida Li & \hspace{1cm} Nikolaos Ignatiadis\\
\texttt{listar2000@uchicago.edu} & \hspace{1cm} \texttt{ignat@uchicago.edu}\\
\vspace{0.2cm}\\
\end{tabular}
}

\date{\vspace{0.4cm} Draft Manuscript, February 2025}


\begin{document}
\maketitle

\begin{abstract}
    Prediction-Powered Inference (PPI) is a powerful framework for enhancing statistical estimates by combining limited gold-standard data with machine learning (ML) predictions. While prior work has demonstrated PPIâ€™s benefits for individual statistical problems, modern applications require answering numerous parallel statistical questions. We introduce Prediction-Powered Adaptive Shrinkage (\texttt{PAS}), a method that bridges PPI with empirical Bayes shrinkage to improve estimation of multiple means. \texttt{PAS} debiases noisy ML predictions \emph{within} each problem and then borrows strength \emph{across} problems by using those same predictions as a reference point for shrinkage. The amount of shrinkage is determined by minimizing an unbiased estimate of risk, and we prove that this tuning strategy is asymptotically optimal. Experiments on both synthetic and real-world datasets show that \texttt{PAS} adapts to the reliability of the ML predictions and outperforms traditional and modern baselines in large-scale applications.
\end{abstract}

\section{Introduction}
\label{submission}

A major obstacle in answering modern scientific questions is the scarcity of gold-standard data \citep{miao2024valid}. 
While advancements in data collection, such as large-scale astronomical surveys \citep{york2000sloan} and 
web crawling~\citep{penedo2024the}, have led to an abundance of covariates (or features), 
scientific conclusions often rely on outcomes (or labels), which are often expensive and labor-intensive to obtain.
The rapid development of machine learning (ML) algorithms has offered a path forward, with ML predictions
increasingly used to supplement  gold-standard outcomes and increase the statistical efficiency of subsequent analyses \citep{liang2007use, wang2020methods}.

Prediction-Powered Inference (PPI) \citep{angelopoulos2023prediction} addresses the scarcity issue by providing a framework for valid statistical analysis using predictions from black-box ML models. By combining ML-predicted and gold-standard outcomes, PPI and its variants~\citep{angelopoulos_ppi_2024, zrnic2024cross, zrnic2025note} use the abundance of predictions to reduce variance while relying on the accuracy of labeled\footnote{Throughout the paper, we will use the terms ``labeled'' and ``gold-standard'' interchangeably.}
data to control bias.

In this work, we adapt PPI to the estimation of multiple outcome means in compound estimation settings. Many applications of PPI naturally involve parallel statistical problems that can be solved simultaneously. For instance, several PPI methods \citep{angelopoulos_ppi_2024, fisch2024stratified}
have shown improvements in estimating the fraction of spiral galaxies using predictions on images from the Galaxy Zoo 2 dataset \citep{willett2013galaxy}. While these methods focus on estimating a single overall fraction, a richer analysis emerges from partitioning galaxies based on metadata (such as celestial coordinates or pre-defined bins) and estimating the fraction of galaxies within each partition. This compound estimation approach enables more granular scientific inquiries that account for heterogeneity across galaxy clusters and spatial locations \citep{nair2010fraction}.

We demonstrate, both theoretically and empirically, the benefits of solving multiple mean estimation problems simultaneously. Our approach builds on the empirical Bayes principle of sharing information \emph{across} problems~\citep{robbins1956empirical, efron2010largescale} as exemplified by James-Stein shrinkage~\citep{james1961estimation, xie2012sure}. The connection between modern and classical statistical ideas allows us to perform \emph{within} problem PPI estimation in the first place, followed by a shrinkage process reusing the ML predictions in an adaptive manner, which becomes possible through borrowing information \emph{across} problems. Our contributions are as follows:
\begin{enumerate}
\item We propose \underline{P}rediction-Powered \underline{A}daptive \underline{S}hrinkage (\texttt{PAS}) for compound mean estimation. \texttt{PAS} inherits the flexibility of PPI in working with any black-box predictive model and makes minimal distributional assumptions about the data. Its two-stage estimation process makes efficient use of the ML predictions as both a variance-reduction device and a shrinkage target.

\item We develop a \underline{C}orrelation-Aware \underline{U}nbiased \underline{R}isk \underline{E}stimate (CURE) for tuning the \texttt{PAS} estimator, establish asymptotic optimality of this tuning strategy, and derive an interpretation in terms of a Bayes oracle risk upper bound.

\item We conduct extensive experiments on both synthetic and real-world datasets. Our experiments demonstrate \texttt{PAS}'s applicability to large-scale problems with deep learning models, showing improved estimation accuracy compared to other estimators (e.g., classical, PPI++).
\end{enumerate}

\section{Preliminaries and Notation}

\subsection{Prediction-Powered Inference (PPI)}
\label{subsec:ppi}
The PPI framework considers a setting where we have access to a small number of labeled data points $(X_i, Y_i)_{i=1}^n \in (\mathcal{X} \times \mathcal{Y})^n$ 
and a large number of unlabeled covariates $(\tilde{X}_i)_{i=1}^N \in (\mathcal{X})^N$, where $\mathcal{X}$ and $\mathcal{Y}$ represent the covariate and outcome space, respectively. 
The data points are drawn iid from a joint distribution $\mathbb{P}_{XY}$.\footnote{To be concrete:
\smash{$(X_i, Y_i) \simiid \mathbb{P}_{XY}$} and 
\smash{$(\tilde{X}_i, \tilde{Y}_i) \simiid \mathbb{P}_{XY}$}, but \smash{$\tilde{Y}_i$} is unobserved.}
We are also given a black-box predictive model $f: \mathcal{X} \to \mathcal{Y}$ that is independent of the datasets (e.g., pre-trained on similar but unseen data). For mean estimation with $\mathcal{Y}\subset \mathbb R$, the goal is to leverage the predicted outcomes $f(X_i)$ to improve the estimation of $\theta := \mathbb{E}[Y_i]$. Some simple estimators take the form of the following aggregated (summary) statistics

\begin{equation}
\begin{aligned}
    &\bar{Y} := \frac{1}{n}  \sum_{i=1}^n Y_i, \:\, & &{\color{lightgray} \tilde{Y} := \frac{1}{N}  \sum_{i=1}^N \tilde{Y}_i,}\\ 
    &\bar{Z}^f := \frac{1}{n} \sum_{i=1}^n f(X_i), \:\, & &\tilde{Z}^f := \frac{1}{N} \sum_{i=1}^N f(\tilde{X}_i).
\end{aligned}
\label{eq:aggregated-stats}
\end{equation}
Above, $\bar{Y}$ is the classical estimator,\footnote{From now on, we will use the term ``classical estimator'' to refer to the sample average of the labeled outcomes.} $\bar{Z}^f, \tilde{Z}^f$ are the prediction means on the labeled and unlabeled data, and $\tilde{Y}$ (greyed out) is unobserved. The vanilla PPI estimator is defined as,
\begin{equation}
\hat{\theta}^{\text{PPI}} 
  := \!\underbrace{\colorbox{blue!15}{$\bar{Y}$}}_{\text{Baseline}} \!\!+\!\underbrace{\colorbox{red!15}{$(\tilde{Z}^f - \bar{Z}^f)$}}_{\text{Variance Reduction}}  =   \underbrace{\colorbox{blue!15}{$\tilde{Z}^f$}}_{\text{Baseline}} \!\!+ \underbrace{\colorbox{red!15}{$(\bar{Y} - \bar{Z}^f)$}}_{\text{Debiasing}}.
  \label{eq:ppi}
\end{equation}
Both definitions represent $\hat{\theta}^{\text{PPI}}$ in the form of a \colorbox{blue!15}{baseline estimator} plus a \colorbox{red!15}{correction term}. In the first representation, the baseline estimator is the unbiased classical estimator $\bar{Y}$, while the correction term has expectation $0$ and attempts to reduce the variance of $\bar{Y}$. In the second representation, the baseline estimator is the prediction mean on unlabeled data  $\tilde{Z}^f$ (which in general may be biased for $\theta$), while the correction term removes the bias of $\tilde{Z}^f$ by estimating the bias of the ML model $f$ on the labeled dataset. Writing $\hat{\theta}^{\text{PPI}}=\tfrac{1}{N} \sum_{i=1}^N f(\tilde{X}_i) + \tfrac{1}{n} \sum_{i=1}^n (Y_i - f(X_i))$, we find
\begin{align}
 \EEInline{\hat{\theta}^{\text{PPI}}} &= \EEInline[]{Y_i} = \theta, \nonumber \\
 \VarInline{\ppi} &= \frac{1}{N}\VarInline{f(\tilde{X}_i)} + \frac{1}{n}\VarInline{Y_i - f(X_i)},
\label{eq:variance_formula}
\end{align}
that is, \smash{$\ppi$} is unbiased for $\theta$ and its variance becomes smaller when the model predicts the true outcomes well. The mean squared error (MSE) of \smash{$\ppi$} is equal to \smash{$\VarInline{\ppi}$}.
Although we motivated \smash{$\ppi$} in~\eqref{eq:ppi} as implementing a correction step on two possible baseline estimators ($\bar{Y}$ and $\tilde{Z}^f$), \smash{$\ppi$}  may have MSE for estimating $\theta$ that is arbitrarily worse than either of these baselines.

\paragraph{Comparison to classical estimator $\bar{Y}$.} The classical estimator $\bar{Y}$ which only uses labeled data is unbiased for $\theta$ and has variance (and MSE) equal to $n^{-1}\VarInline{Y_i}$, while the MSE of \smash{$\ppi$} in~\eqref{eq:variance_formula} may be arbitrarily large when the predictive model is inaccurate. 

\paragraph{Power-Tuned PPI (PPI++).} To overcome the above limitation, \citet{angelopoulos_ppi_2024} introduce a power-tuning parameter $\lambda$ 
and define
\begin{equation} \label{eq:PT}
    \hat{\theta}^{\text{PPI}}_\lambda 
    := \bar{Y} + \lambda \left(\tilde{Z}^f - \bar{Z}^f\right),
\end{equation}
which recovers the classical estimator when $\lambda = 0$ and the vanilla PPI estimator when $\lambda = 1$.
For all values of $\lambda$, $\,\hat{\theta}^{\text{PPI}}_\lambda$ is unbiased, so if we select the $\lambda$ that minimizes \smash{$\VarInline{\hat{\theta}^{\text{PPI}}_\lambda}$}, we can improve our estimator over both the classical estimator and vanilla PPI. Such an estimator is defined as the Power-Tuned PPI (PT\footnote{We will use the term ``PPI++'' for the larger framework, while ``PT'' refers to the specific estimator.}) estimator $\hat{\theta}^{\mathrm{PT}} := \hat{\theta}^{\text{PPI}}_{\lambda^*}$, where
\begin{equation*}
    \lambda^* := \argmin_{\lambda \in [0, 1]} \VarInline{\hat{\theta}^{\text{PPI}}_\lambda}.
\end{equation*}
Power-tuning will be one of the building blocks of $\texttt{PAS}$ in \cref{sec:methods}.

\paragraph{Comparison to $\mlbaseline$.} Consider the ideal scenario for PPI with $N=\infty$ (that is, the unlabeled dataset is much larger than the labeled dataset) so that \smash{$\mlbaseline \equiv \EEInline{f(\tilde{X}_i)}$}. Even then, the MSE of \smash{$\ppi$} in~\eqref{eq:variance_formula} is always lower bounded\footnote{The same lower bound also applies to power-tuned PPI \smash{$\ptppi$}.}  by \smash{$\EEInline{\VarInline{Y_i \mid X_i}}/n$} and the lower bound is attained for the perfect ML predictor $f(\cdot) \equiv \EE{Y_i \mid X_i=\cdot}$. In words, if $Y_i$ is not perfectly predictable from $X_i$, then PPI applied to a labeled dataset of fixed size $n$ must have nonnegligible MSE. By contrast, for $N=\infty$, the prediction mean of unlabeled data \smash{$\mlbaseline$} has zero variance and MSE equal to the squared bias $(\EEInline{f(X_i)}-\theta_i)^2$. Thus if the predictor satisfies a calibration-type property that \smash{$\EEInline{f(X_i)} \approx \EEInline{Y_i}$} (which is implied by, but much weaker than the requirement $f(X_i) \approx Y_i$), then the MSE of \smash{$\mlbaseline$} could be nearly $0$. By contrast, PPI (and PPI++) can only partially capitalize on such a predictor $f(\cdot)$. 

While PPI and PPI++ are constrained by their reliance on unbiased estimators, we show that the compound estimation setting (\cref{subsec:compound}) enables a different approach. By carefully navigating the bias-variance tradeoff through information sharing \emph{across} parallel estimation problems, we can provably match the performance  of both $\bar{Y}$ and \smash{$\mlbaseline.$}

\subsection{The Compound Mean Estimation Setting}
\label{subsec:compound}
In this section, we introduce the problem setting that \texttt{PAS} is designed to address---estimating the mean of $m > 1$ parallel problems
with a single black-box predictive model $f$.\footnote{Our proposal also accommodates using separate predictors \(\{f_j\}_{j=1}^m\) for each problem. To streamline exposition, we focus on the practical scenario where a single (large) model (e.g., an LLM or vision model) can handle multiple tasks simultaneously~\citep{radford2019language, he2022masked}.}
For the $j$-th problem, where $j \in [m] := \{1, \ldots, m\}$, we observe a labeled dataset \smash{$(X_{ij}, Y_{ij})_{i=1}^{n_j}$} with $n_j \in \mathbb N$ observations and an unlabeled dataset \smash{$(\tilde{X}_{ij})_{i=1}^{N_j}$} with $N_j \in \mathbb N$ observations.
We start with modeling heterogeneity across problems.
\begin{assumption}[Prior]
\label{ass:exchangeable_model}
There exist problem-specific unobserved latent variables $\eta_j$ with
\begin{equation}
\eta_j \simiid \mathbb{P}_{\eta},\; j \in [m],\,\,\text{and}\,\, \boldsymbol{\eta} := (\eta_1,...,\eta_m)^\intercal,
\label{eq:eta_prior}
\end{equation}
where $\mathbb{P}_{\eta}$ is an unknown probability measure. The latent variable $\eta_j$ fully specifies the distribution of the $j$-th labeled and unlabeled dataset.
We use the notation $\EE[\eta_j]{\cdot}$ (resp. $\EE[\boldsymbol{\eta}]{\cdot}$) to denote the expectation conditional on $\eta_j$ (resp. $\boldsymbol{\eta}$), while $\EE[\mathbb P_{\eta}]{\cdot}$ denotes an expectation also integrating out $\mathbb P_{\eta}$.
\end{assumption}
We do not place any restriction over the unknown prior $\mathbb{P}_{\eta}$.
Assumption~\ref{ass:exchangeable_model} posits exchangeability across problems, which enables information sharing, without restricting heterogeneity~\citep{ignatiadis2023empiricala}.
In our setting, we are specifically interested in the means
\begin{equation}
\theta_j := \EE[\eta_j]{Y_{ij}},\; j \in [m],\,\,\text{and}\,\, \boldsymbol{\theta} := (\theta_1, \ldots, \theta_m)^\intercal.
\label{eq:theta_j_def}
\end{equation}
Our next assumption specifies that we only model the first two moments of the joint distribution between the outcomes and the predictions. The upshots of such modeling are that the exact form of the observation distribution is neither assumed nor required in our arguments, and that our approach will be directly applicable to settings where the covariate space $\mathcal{X}$ is high-dimensional or structured.
\begin{assumption}[Sampling]
\label{ass:compound_generic}
For each problem $j \in [m]$, we assume that the joint distribution
of $(f(X_{ij}), Y_{ij})$ has finite second moments conditional on $\eta_j$ and that
\begin{align} 
    \label{eq:generic-likelihood}
    \begin{bmatrix}
        f(X_{ij}) \\
        Y_{ij} 
        \end{bmatrix}\, \big|\,\eta_j\,
        \simiid \,\mathbb{F}_j \left(
        \begin{bmatrix}
        \mu_{j} \\
        \theta_{j}
        \end{bmatrix},
        \begin{bmatrix}
        \tau_{j}^{2} & \rho_j \tau_j \sigma_j \\
        \rho_j \tau_j \sigma_j & \sigma_{j}^{2}
        \end{bmatrix}
        \right),\; i \in [n_j],\,\,
\end{align}
where \smash{$\mathbb{F}_j$}, $\mu_j, \theta_j, \rho_j, \sigma_j^2, \tau^2_j$ are functions of $\eta_j$. Conditional on $\eta_j$, the unlabeled predictions  $f(\tilde X_{i'j})$, $i' \in [N_j]$, are also iid, independent of the labeled dataset and identically distributed with $f(X_{ij})$.
In the notation of~\eqref{eq:generic-likelihood}, $\mathbb{F}_j$ represents a unspecified distribution satisfying the  moment constraints in~\eqref{eq:theta_j_def} and
\begin{align*}
    &\EE[\eta_j]{f(X_{ij})} = \mu_{j}, \: &&\Var[\eta_j]{f(X_{ij})} = \tau_{j}^{2}, \\
    &\Corr[\eta_j]{f(X_{ij}), Y_{ij}} = \rho_{j}, \: &&\Var[\eta_j]{Y_{ij}} = \sigma_{j}^{2}.
\end{align*}
We further denote $\gamma_j := \Cov[\eta_j]{f(X_{ij}), Y_{ij}} = \rho_j \tau_j \sigma_j$.
\end{assumption}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.85\textwidth]{images/example_plus_plus.pdf}
    \vskip-0.35cm
    \caption{We instantiate the model described in \cref{ex:synthetic} with $m = 10$ problems, each has $n_j = 10$ labeled and $N_j = 20$ unlabeled data (in different colors).
    (Top) Labeled data \smash{$(X_{ij}, Y_{ij})_{j=1}^{n_j}$} with the classical estimator \smash{$\bar{Y}_j$} shown for each problem.
    (Bottom) We apply a flawed predictor $f(x) = |x|$ to the unlabeled covariates and visualize \smash{$(X_{ij}, f(X_{ij}))_{j=1}^{N_j}$} as well as the prediction mean \smash{$\tilde{Z}_j^f$}.
    }
    \label{fig:synthetic-example-plot}
    \vskip-0.3cm
\end{figure}


Similar to Eq.~\eqref{eq:aggregated-stats}, we define the aggregated statistics \smash{$\bar{Y}_j, \bar{Z}_j^f, \mlbaseline_j$} for each $j \in [m]$. Following prior work,\footnote{For EB, examples include \citet{xie2012sure,soloff2024multivariate}; for PPI, see recent works like~\citet{fisch2024stratified}.}
we treat $\tau_j$, $\sigma_j$, $\rho_j$ as known in our development. 
The rationale is that estimation of $\tau_j, \sigma_j, \rho_j$ is a second-order concern in our setup of mean estimation and treating these as known facilitates exposition. In practice, such as in our numerical experiments with real-world datasets, we use sample-based estimates of $\tau_j, \sigma_j,$ and $\rho_j$ (see \cref{para:sample-estimator}). 

We next introduce a synthetic model that will serve both as a running example and as part of our numerical study.
\begin{example}[\textbf{Synthetic model}]
    \label{ex:synthetic}
    For each problem $j$, let $\eta_j \sim \mathcal{U}[-1, 1]$. We think of $\eta_j$ as both indexing the problems, and generating heterogeneity across problems.
    The $j$-th dataset is generated via (with constants $c = 0.05, \psi = 0.1$),
    \begin{equation} \label{eq:synthetic-likelihood}
        X_{ij} \simiid \mathcal{N}(\eta_j, \psi^2),\;\;\; Y_{ij} | X_{ij} \simindep \mathcal{N}(2\eta_j X_{ij} - \eta_j^2, c).
    \end{equation}
    In \cref{fig:synthetic-example-plot}, we visualize realizations from this model with $m=10$ problems, $n_j=10$ labeled observations, and $N_j=20$ unlabeled observations for each problem. We apply a flawed predictor $f(x) = |x|$. 
    The classical estimator $\bar{Y}_j$ and the prediction mean $\mlbaseline_j$ deviate from each other. Nevertheless, \smash{$\mlbaseline_j$} contains information that can help us improve upon \smash{$\bar{Y}_j$} as an estimator of $\theta_j$ by learning from within problem (PPI, PPI++, this work) and across problem (this work) structure.
    We emphasize that as specified in~\eqref{eq:generic-likelihood}, our approach only requires modeling the first and second moments of the joint distribution of $(f(X_{ij}), Y_{ij})$. For instance, in this synthetic model, $\theta_j = \eta_j^2$ and $\sigma_j^2 = 4\eta_j^2 \psi^2 + c$, while $\mu_j$, \smash{$\tau_j^2$} and $\gamma_j$ also admit closed-form expressions in terms of $\eta_j$ when the predictor takes the form $f(x) = |x|$ or $f(x) = x^2$ (see~\cref{appendix:synthetic-dataset-details}).
\end{example}

To conclude this section, we define the compound risk~\citep{robbins1951asymptotically, jiang2009general} for any estimator $\boldsymbol{\hat \theta} = (\hat{\theta}_1, \ldots, \hat{\theta}_m)^\intercal$ as the expected squared error loss averaged over problems,
\begin{align}
    \label{eq:compound-risk}
\mathcal{R}_m(\boldsymbol{\hat \theta}, \boldsymbol{\theta}) &:= \EE[\boldsymbol{\eta}]{\ell_m(\boldsymbol{\hat \theta}, \boldsymbol{\theta})}, \\
    \text{where} \quad \ell_m(\boldsymbol{\hat \theta}, \boldsymbol{\theta}) &:= \frac{1}{m}\sum_{j=1}^m (\hat{\theta}_j - \theta_j)^2.
\end{align}
 The Bayes risk, which we also refer to simply as mean squared error (MSE), further integrates over randomness in the unknown prior $\mathbb{P}_\eta$ in~\eqref{eq:eta_prior},
\begin{align}
    \label{eq:bayes-risk}
    \mathcal{B}_m^{\mathbb{P}_\eta}(\boldsymbol{\hat \theta}) := \EE[\mathbb{P}_\eta]{\mathcal{R}_m(\boldsymbol{\hat \theta}, \boldsymbol{\theta})}.
\end{align}

\section{Statistical Guiding Principles and Connections to Prior Work}
\label{subsec:motivating-example}

In this section, we illustrate both the statistical guiding principles of our approach and some connections to prior work through the following stylized Gaussian model:
\begin{equation*}
\begin{aligned} 
&\text{Sampling:} & &  \classicalstylized = \theta + (\xi + \varepsilon), &&\xi \sim \mathcal{N}(0, \sigma_{\xi}^2), \quad \quad\,\,\varepsilon \sim \mathcal{N}(0, \sigma_{\varepsilon}^2).\\ 
&\text{Prior:} & & \theta \sim \mathcal{N}(0, \sigma_{\theta}^2), &&\phi \sim \mathcal{N}(0, \sigma_{\phi}^2), \quad\quad\Corr{\theta, \phi} = \rho.
\end{aligned}
\end{equation*} 
In our stylized model, we  assume that $(\theta,\phi, \varepsilon, \xi)$ are jointly normal and that all their pairwise correlations  are zero with the exception of  $\Corr[]{\theta, \phi}=\rho \neq 0$. We  write \smash{$\sigma_{\theta \mid \phi}^2:= \Var{\theta \mid \phi} = (1-\rho^2)\sigma_{\theta}^2 < \sigma_{\theta}^2$}.

We think of \smash{$\classicalstylized$} as the baseline \underline{cl}assical statistical estimator of a quantity $\theta$ that we seek to estimate with small MSE. In our stylized Gaussian model,
\smash{$\classicalstylized $} is unbiased for $\theta$ and has noise contribution $\xi + \varepsilon$, so that \smash{$\EEInline{(\classicalstylized  - \theta)^2} = \VarInline[\theta]{\classicalstylized} =  \sigma_{\xi}^2 + \sigma_{\varepsilon}^2$}. We describe three high-level strategies used to improve the MSE of \smash{$\classicalstylized$}.  These strategies are not tied in any way to the stylized model; nevertheless, the stylized model enables us to give precise expressions for the risk reductions possible, see Table~\ref{tab:MSEs_stylized}.

\begin{table}[t]
    \caption{Estimator comparison in the stylized Gaussian model of~\cref{subsec:motivating-example}.}
    \vspace{-5mm}
    \setlength{\tabcolsep}{4pt}  % reduce column separation
    \begin{center}
    \begin{tabular}{l>{\centering}p{3.5cm}ccc}
    \toprule
    \textbf{Estimator} & \textbf{MSE} & \textbf{VR} & \textbf{P} & \textbf{CP} \\
    \midrule 
    \vspace{1mm}
    $\classicalstylized$ & 
    $\sigma^2_{\xi} + \sigma^2_{\varepsilon}$ & 
    \ding{55} & \ding{55}  & \ding{55} \\ \vspace{1mm}
    $\classicalstylized-\xi$ & $\sigma^2_{\varepsilon}$ & \checkmark & \ding{55} & \ding{55} \\ \vspace{1mm}
    $\EEInline[]{\theta \mid \classicalstylized}$
    & $\frac{(\sigma^2_{\xi} + \sigma^2_{\varepsilon})\sigma_\theta^2}{(\sigma^2_{\xi} + \sigma^2_{\varepsilon}) + \sigma_\theta^2}$  & \ding{55} & \checkmark & \ding{55} \\ \vspace{1mm}
    $\EEInline[]{\theta \mid \classicalstylized,\phi}$
    & $\frac{(\sigma^2_{\xi} + \sigma^2_{\varepsilon})\sigma_{\theta \mid \phi}^2}{(\sigma^2_{\xi} + \sigma^2_{\varepsilon}) + \sigma_{\theta \mid \phi}^2}$  & \ding{55} & 
    \checkmark & \checkmark \\ 
    \vspace{1mm}
    $\EEInline[]{\theta \mid \classicalstylized-\xi}$
    & $\frac{\sigma^2_{\varepsilon}\sigma_{\theta}^2}{\sigma^2_{\varepsilon} + \sigma_{\theta}^2}$  &    \checkmark & 
    \checkmark & \ding{55} \\
    \rowcolor{green!20} \vspace{1mm} $\EEInline[]{\theta \mid \classicalstylized-\xi,\phi}$
    & $\frac{\sigma_{\varepsilon}^2 \sigma_{\theta \mid \phi}^2 }{ \sigma_{\varepsilon}^2 + \sigma_{\theta \mid \phi}^2}$  & \checkmark & 
    \checkmark & \checkmark \\ 
    \bottomrule
    \end{tabular}
    \vspace{2mm}\\
    VR: Variance Reduction, P: Prior Information, CP: Contextual Prior Information.
    \vspace{-5mm}
    \end{center}
    \label{tab:MSEs_stylized}
\end{table}

\paragraph{Variance reduction (VR).}  An important statistical idea is to improve $\classicalstylized$ via obtaining further information to intercept some of its noise, say $\xi$, and replacing $\classicalstylized$ by $\classicalstylized-\xi$ which has MSE $\sigma_{\varepsilon}^2$ and remains unbiased for $\theta$. This idea lies at the heart of approaches such as control variates in simulation~\citep{lavenberg1981perspective, hickernell2005control}, variance reduction in randomized controlled experiments via covariate adjustment~\citep{lin2013agnostic} and by utilizing pre-experiment data~\citep[CUPED]{deng2013improving}, as well as model-assisted estimation in survey sampling~\citep{cochran1977sampling, breidt2017modelassisted}. It is also the idea powering PPI and related methods: the unlabeled dataset and the predictive model are used to intercept some of the noise in the classical statistical estimator $\classicalstylized \triangleq\bar{Y}$; compare to Eq.~\eqref{eq:ppi} with $\xi \,\triangleq \,\bar{Z}^f - \tilde{Z}^f$. We refer to~\citet{ji2025predictions} for an informative discussion of how PPI relates to traditional ideas in semi-parametric inference as in e.g.,~\citet{robins1994estimation}.



\paragraph{Prior information (P) via empirical Bayes (EB).} In the Bayesian approach we  seek to improve upon \smash{$\classicalstylized$} by using the prior information that \smash{$\theta \sim \mathcal{N}(0,\sigma^2_{\theta})$}. The Bayes estimator,
$$
\EE{\theta \mid \classicalstylized} = \frac{\sigma_{\theta}^2}{\sigma_{\xi}^2 + \sigma_{\varepsilon}^2 + \sigma_{\theta}^2}\classicalstylized,
$$
reduces variance by shrinking $\classicalstylized$ toward $0$ (at the cost of introducing some bias). When $\sigma_{\theta}^2$ is small, the MSE of \smash{$\EEInline{\theta \mid \classicalstylized}$} can be substantially smaller than that of \smash{$\classicalstylized$}. 

Now suppose that the variance of the prior,  $\sigma_{\theta}^2$, is unknown but we observe data from multiple related problems generated from the same  model and indexed by $j=1,\dotsc,m$, say, \smash{$\theta_j \simiid \mathcal{N}(0, \sigma_{\theta}^2)$} and \smash{$\classicalstylized_j \simindep \mathcal{N}(\theta_j, \sigma_{\xi}^2+\sigma_{\varepsilon}^2)$}. Then an EB analysis can mimic the MSE of the oracle Bayesian that has full knowledge of the prior. To wit, we can estimate $\sigma_{\theta}^2$ as 
$$\hat{\sigma}_{\theta}^2 = \left\{\frac{1}{m-2}\sum_{j=1}^m (\classicalstylized_j)^2\right\} -(\sigma_{\xi}^2+\sigma_{\varepsilon}^2),$$
and then consider a plug-in approximation of the Bayes rule, \smash{$\hat{\theta}_j^{\text{JS}}=\hat{\mathbb E}[\theta_j \mid \classicalstylized_j]:= \{\hat{\sigma}_{\theta}^2/(\sigma_{\xi}^2 + \sigma_{\varepsilon}^2 + \hat{\sigma}_{\theta}^2)\}\classicalstylized$}. The resulting estimator is the celebrated James-Stein estimator~\citep{james1961estimation,efron1973stein}, whose risk is very close to the Bayes risk under the hierarchical model. The James-Stein estimator also always dominates the classical estimator under a frequentist evaluation of compound risk in~\eqref{eq:compound-risk} under the assumption that  \smash{$\classicalstylized_j \simindep \mathcal{N}(\theta_j, \sigma_{\xi}^2+\sigma_{\varepsilon}^2)$}:
$$\mathcal{R}_m(\boldsymbol{\hat{\theta}^{\text{JS}}}, \boldsymbol{\theta}) < \mathcal{R}_m(\boldsymbol{\hat{\theta}^{\text{cl}}}, \boldsymbol{\theta}) \mbox{~~for all~~} \boldsymbol{\theta} \in \mathbb R^m.$$ 

\paragraph{Contextual prior information (CP) via EB.} Instead of using the same prior for each problem, we may try to sharpen the prior and increase its relevance~\citep{efron2011tweedie} by using further information $\phi$. In the stylized example, as seen in Table~\ref{tab:MSEs_stylized}, such an approach reduces the variance of the prior from $\sigma_{\theta}^2$ to $\sigma_{\theta \mid \phi}^2 < \sigma_{\theta}^2$ with corresponding MSE reduction of the Bayes estimator. With multiple related problems, such a strategy can be instantiated via EB shrinkage toward an informative but biased predictor~\citep{fayiii1979estimates, green1991jamesstein, mukhopadhyay2004two,kou2017optimal, rosenman2023combining}. The strategy of this form that is closest to our proposal is the covariate-powered EB approach of~\citet{ignatiadis2019covariatepowered}. Therein (following the notation of Section~\ref{subsec:compound}), the analyst has access to classical estimators $\bar{Y}_j$, $j \in [m]$, and problem-specific covariates $W_j$ and seeks to shrink $\bar{Y}_j$ toward ML models that predict $\bar{Y}_j$ from $W_j$. By contrast, in our setting we have observation-level covariates $X_{ij}$ and the ML model operates on these covariates. In principle one could simultaneously use both types of covariates: problem-specific and observation-specific.


\paragraph{Combined variance reduction (VR) and prior information (P).}One can shrink the variance reduced estimator $\classicalstylized-\xi$ toward $0$ via \smash{$\EEInline[]{\theta \mid \classicalstylized-\xi} = \{\sigma_{\theta}^2/( \sigma_{\varepsilon}^2+\sigma_{\theta}^2)\}(\classicalstylized-\xi)$}. In the context of PPI, variance reduction and prior information (with a more heavy-tailed prior) are used by~\citet{cortinovis2025fabppi} within the Frequentist-Assisted by Bayes (FAB) framework of~\citet{yu2018adaptive}. ~\citet{cortinovis2025fabppi} only consider a single problem and do not pursue an empirical Bayes approach.

\paragraph{Combined variance reduction (VR), prior (P), and contextual prior information (CP).}
Finally, in our stylized example, we can get the smallest MSE (last row of~\cref{tab:MSEs_stylized}) by using both variance reduction, shrinkage, and a contextual prior. In that case, the Bayes estimator takes the form,
\begin{equation}
\label{eq:stylized_optimal}
\EE[]{\theta \mid \classicalstylized-\xi, \phi} \,=\, \frac{\sigma_{\theta \mid \phi}^2}{\sigma_{\varepsilon}^2 + \sigma_{\theta \mid \phi}^2} (\classicalstylized - \xi) + \frac{\sigma_{\varepsilon}^2}{\sigma_{\varepsilon}^2 + \sigma_{\theta \mid \phi}^2}\EE[]{\theta \mid \phi}. 
\end{equation}
EB ideas can be used to mimic the estimator above and provide the starting point for our proposal that we describe next. 

\section{Prediction-Powered Adaptive Shrinkage}
\label{sec:methods}

On a high level, \texttt{PAS} aims to provide a lightweight approach that outperforms both baselines in~\eqref{eq:ppi} and PPI/PPI++ in terms of MSE when estimating multiple means. \texttt{PAS} also aims at minimal modeling requirements and assumptions.

The stylized example from Section~\ref{subsec:motivating-example} serves as a guiding analogy. 
We seek to benefit from  ML predictions in two ways: first by variance reduction (acting akin to $\xi$ in the stylized example), and second by increasing prior relevance (acting as a proxy for $\phi$). We implement both steps to adapt to the unknown data-generating process in an assumption-lean way using \emph{within}-problem information for the first step (\cref{subsec:power-tuning}) and \emph{across}-problem information for the second step (\cref{subsec:adaptive-shrinkage}), drawing on ideas from the EB literature.

\subsection{The Within Problem Power-Tuning Stage}
\label{subsec:power-tuning}
Extending the notation from \eqref{eq:PT} to each problem $j$ provides us with a class of unbiased estimators
\smash{$\hat{\theta}_{j, \lambda}^{\text{PPI}} := \bar{Y}_j + \lambda (\mlbaseline_j - \bar{Z}_j^f)$}, $\lambda \in \mathbb{R}$. Calculating the variance gives
\begin{align*}
    \Var[\eta_j]{\hat{\theta}_{j, \lambda}^{\text{PPI}}} = \frac{\sigma_j^2}{n_j} + \overbrace{\frac{n_j + N_j}{n_j N_j}\lambda^2 \tau_j^2  - \frac{2}{n_j} \lambda \gamma_j}^{=:\delta_j(\lambda)}.
\end{align*} 
Note that the classical estimator has risk $\sigma_j^2/n_j$ and so is outperformed whenever $\delta_j(\lambda) < 0$. We can analytically solve for the optimal $\lambda$, which yields
\begin{align} \label{eq:power-tuning-lambda}
    \lambda^*_j := \argmin_{\lambda} \delta_j(\lambda) = \left(\frac{N_j}{n_j + N_j}\right) \frac{\gamma_j}{\tau_j^2},
\end{align}
and the Power-Tuned (PT) estimator $\hat{\theta}_j^{\mathrm{PT}} := \hat{\theta}_{j, \lambda^*_j}^{\mathrm{PPI}}$ with
\begin{align}
\label{eq:get_pt_var}
\tilde{\sigma}^2_j  := \Var[\eta_j]{\hat{\theta}_j^{\mathrm{PT}}} &= \frac{\sigma_j^2}{n_j} - \frac{N_j}{n_j(n_j + N_j)} \frac{\gamma_j^2}{\tau_j^2}.
\end{align}
The formulation of the above PT estimators is well understood in the single problem setting~\citep{angelopoulos_ppi_2024,miao_assumption-lean_2024}.
In \texttt{PAS}, we execute this stage separately for each problem, as the optimal power-tuning parameter is data-dependent and varies case by case.

\subsection{The Across Problem Adaptive Shrinkage Stage}
\label{subsec:adaptive-shrinkage}
The PT estimator derived in \cref{subsec:power-tuning} already possesses many appealing properties: it is unbiased and has lower variance than both the classical estimator 
and vanilla PPI. However, as our setting involves working with many parallel problems together, 
there is the opportunity of further MSE reduction by introducing bias in a targeted way. Concretely, based on the PT estimator obtained in \cref{subsec:power-tuning}, we consider a class of shrinkage estimators
\begin{equation}
\begin{aligned}
    \hat{\theta}_{j, \omega}^{\mathrm{PAS}} := \omega_j \hat{\theta}^{\mathrm{PT}}_j + (1 - \omega_j) \mlbaseline_j, \quad\omega_j := \omega_j(\omega) = \frac{\omega}{\omega + \tilde{\sigma}^2_j}, \quad \boldsymbol{\hat \theta}^{\mathrm{PAS}}_\omega := (\hat{\theta}_{1, \omega}^{\mathrm{PAS}}, \ldots, \hat{\theta}_{m, \omega}^{\mathrm{PAS}})^\intercal,
\end{aligned}
\label{eq:pas_shrinkage_form}
\end{equation}
for any $\omega \geq 0$. The motivation is to formally match the form of the Bayes estimator with variance reduction and contextual prior information in~\eqref{eq:stylized_optimal} with the following (approximate) analogies:
\begin{equation}
\begin{aligned}
\classicalstylized - \xi &\longleftrightarrow \ptppi_j,\;\,
&&\EE[]{\theta \mid \phi} \longleftrightarrow \mlbaseline_j,\;\, \\ 
\sigma_{\varepsilon}^2 &\longleftrightarrow \tilde{\sigma}_j^2,\;\, && \quad \, \colorbox{green!20}{$\sigma^2_{\theta \mid \phi} \longleftrightarrow  \omega$}.
\end{aligned}
\label{eq:analogy}
\end{equation}
The highlighted $\omega$ is a global shrinkage parameter that acts as follows:
\begin{itemize}
    \item[(i)] Fixing $\omega$, any problem whose PT estimator has higher variance possesses smaller $\omega_j$ and shrinks more towards $\mlbaseline_j$; a smaller variance increases $\omega_j$ and makes the final estimator closer to $\hat{\theta}_j^{\mathrm{PT}}$.
    \item[(ii)] Fixing all the problems, increasing $\omega$ has an overall effect of recovering $\hat{\theta}_j^{\mathrm{PT}}$ (full recovery when $\omega \to \infty$), and setting $\omega = 0$ recovers $\mlbaseline$.
\end{itemize}
The above establishes the conceptual importance of $\omega$ and if we could choose $\omega$ in an optimal way,
$$
\omega^* \in \argmin_{\omega \geq 0}\left\{ \mathcal{R}_m\Big(\boldsymbol{\hat \theta}^{\mathrm{PAS}}_\omega, \boldsymbol{\theta}\Big)\right\},
$$
then the resulting estimator \smash{$\hat{\boldsymbol{\theta}}_{\omega^*}^{\mathrm{PAS}}$} would satisfy all our desiderata. And while the above construction is not feasible as the compound risk function in~\eqref{eq:compound-risk} depends on the unknown $\boldsymbol{\eta}, \boldsymbol{\theta}$, we can make progress by pursuing a classical statistical idea: we can develop an unbiased estimate of the compound risk~\citep{mallows1973comments, stein1981estimation, efron2004estimation} and then use that as a surrogate for tuning $\omega$.

To this end, we define the \underline{C}orrelation-aware \underline{U}nbiased \underline{R}isk \underline{E}stimate ($\mathrm{CURE}$),
\begin{align*}
    \mathrm{CURE}\left(\boldsymbol{\hat \theta}^{\mathrm{PAS}}_\omega\right) &:= \frac{1}{m}\sum_{j = 1}^m \left[(2\omega_j - 1)\tilde{\sigma}^2_j + 2(1 - \omega_j) \tilde \gamma_j +(1 - \omega_j)^2\big(\hat{\theta}_{j, \omega_j}^{\mathrm{PT}} - \mlbaseline_j\big)^2\right],
\end{align*}
where both the formula and our nomenclature (``correlation-aware'') highlight the fact that we must account for the potentially non-zero covariance between shrinkage source \smash{$\ptppi_j$} and target \smash{$\mlbaseline_j$},
\begin{align}
    \label{eq:tilde-gamma-def}
    \tilde{\gamma}_j := \CovInline[\eta_j]{\hat{\theta}_j^{\mathrm{PT}}, \mlbaseline_j} = \lambda_j^* \VarInline[\eta_j]{\mlbaseline_j} = \frac{\gamma_j}{n_j + N_j}.
\end{align}
\begin{theorem}
   \label{thm:gsure-PAS}
   Under \cref{ass:compound_generic}, $\mathrm{CURE}$ is an unbiased estimator of the compound risk defined in \eqref{eq:compound-risk}, that is, for all $\omega \geq 0$ and all $\boldsymbol{\eta}$,
    \begin{align*}
        \EE[\boldsymbol{\eta}]{\mathrm{CURE}\left(\boldsymbol{\hat \theta}^{\mathrm{PAS}}_\omega\right)} = \mathcal{R}_m\Big(\boldsymbol{\hat \theta}^{\mathrm{PAS}}_\omega, \boldsymbol{\theta}\Big).
    \end{align*}
\end{theorem}
See Appendices~\ref{appendix:cure} and~\ref{subsec:proof-of-gsure-PAS} for the proof and motivation. With \cref{thm:gsure-PAS} in hand, we now have a systematic strategy of picking $\omega$ by minimizing CURE, following the paradigm of tuning parameter selection via minimization of an unbiased risk estimate as advocated e.g., by \citet{li1985stein, donoho1995adapting, xie2012sure, candes2013unbiased, ignatiadis2019covariatepowered}:\footnote{The connection to EB is the following. \citet{xie2012sure} and~\citet{tibshirani2019excess} explain that  James-Stein-type estimators may be derived by tuning $\sigma_{\theta}^2$ (in~\cref{subsec:motivating-example}) via minimization of Stein's~\citeyearpar{stein1981estimation} unbiased risk estimate (SURE).
} 
\begin{align} \label{eq:gsure-optimization}
    \hat{\omega} \in \argmin_{\omega \geq 0} \mathrm{CURE}\left(\boldsymbol{\hat \theta}^{\mathrm{PAS}}_\omega\right).
\end{align}
Even though $\hat{\omega}$ does not admit a closed-form expression, the one-dimensional minimization can be efficiently carried out numerically (e.g. grid search). The final \texttt{PAS} estimator is then given by
\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\columnwidth]{images/algo_new.png}
    \caption{A flowchart illustration of the \texttt{PAS} method. See~\cref{alg:pas} for a pseudo-code implementation.}
    \label{fig:PAS-chart-flowchart}
\end{figure}
\begin{algorithm}[t]
\caption{Prediction-Powered Adaptive Shrinkage}
\label{alg:pas}
\small
\begin{algorithmic}[1]
\REQUIRE $(X_{ij}, Y_{ij})_{i=1}^{n_j}$, $(\tilde{X}_{ij})_{i=1}^{N_j}$, $\rho_j, \tau_j, \sigma_j$ for $j \in [m]$, predictive model $f$
\FOR{$j = 1$ to $m$}
    \STATE \COMMENT{Step 1: Apply predictor (Eq.~\eqref{eq:aggregated-stats})}
    \STATE \( \bar{Y}_j, \bar{Z}_j^f, \mlbaseline_j = \tbf{get\_means}((X_{ij}, Y_{ij})_{i=1}^{n_j}, (\tilde{X}_{ij})_{i=1}^{N_j}, f) \)
    \STATE \COMMENT{Step 2: Power tuning (Eq.~\eqref{eq:power-tuning-lambda})}
    \STATE \( \lambda_j^* = \tbf{get\_pt\_param}(\rho_j, \tau_j, n_j, N_j) \)
    \STATE \( \hat{\theta}_j^{\mathrm{PT}} = \bar{Y}_j + \lambda_j^* (\mlbaseline_j - \bar{Z}_j^f) \)
    \STATE \( \tilde{\sigma}_j^2 = \tbf{get\_pt\_var}(\hat{\theta}_j^{\mathrm{PT}})\) \COMMENT{(Eq. \eqref{eq:get_pt_var})}
\ENDFOR

\STATE \COMMENT{Step 3: Adaptive shrinkage (Eq. \eqref{eq:gsure-optimization})}
\STATE \( \hat{\omega} = \tbf{get\_shrink\_param}(\{\hat{\theta}_j^{\mathrm{PT}}\}_{j=1}^m, \{\mlbaseline_j\}_{j=1}^m, \{  \tilde{\sigma}_j^2 \}_{j=1}^m \) )
\FOR{$j = 1$ to $m$}
    \STATE \( \hat \omega_j = \hat{\omega} / (\hat{\omega} + \tilde{\sigma}_j^2) \)
    \STATE \( \hat{\theta}_j^{\mathrm{PAS}} = \hat \omega_j \hat{\theta}_j^{\mathrm{PT}} + (1 - \hat \omega_j) \mlbaseline_j \)
\ENDFOR
\RETURN \( \: \{\hat{\theta}_j^{\textnormal{PAS}}\}_{j=1}^m \)
\end{algorithmic}
\end{algorithm}
\begin{align*}
    \hat{\theta}_{j}^{\mathrm{PAS}} := \hat{\theta}_{j,\hat{\omega}}^{\mathrm{PAS}} = \frac{\hat{\omega}}{\hat{\omega} + \tilde{\sigma}^2_j} \hat{\theta}^{\mathrm{PT}}_j + \frac{\tilde{\sigma}^2_j}{\hat{\omega} + \tilde{\sigma}^2_j} \mlbaseline_j.
\end{align*}
\cref{fig:PAS-chart-flowchart} visualizes the full method for constructing the \texttt{PAS} estimator---from applying the predictor and obtaining aggregated statistics to going through the two stages described in \cref{subsec:power-tuning} and this section. A pseudo-code implementation is also presented in \cref{alg:pas}.

To illustrate the flexibility and adaptivity of \texttt{PAS}, we briefly revisit the synthetic model in \cref{ex:synthetic}, whose special structure allows us to visualize how the power-tuned and adaptive shrinkage parameters vary across problems and different predictors. In \cref{fig:synthetic-params}, we consider $m = 200$ problems and two predictors: a good predictor $f_1(x) = x^2$ and a flawed predictor $f_2(x) = |x|$. The model setup in~\eqref{eq:synthetic-likelihood} is such that the magnitude of $\Cov[\eta_j]{X_j, Y_j}$ relative to $\Var[\eta_j]{Y_j}$ is much larger for problems with $\eta_j$ closer to the origin. Therefore, for both predictors, we see a dip in $\lambda_j^*$ near the middle (top panel), which shows that \texttt{PAS} adapts to the level of difficulty of each problem when deciding how much power-tuning to apply. On the other hand (bottom panel), the overall shrinkage effect is much stronger (smaller $\hat{\omega}_j$ for all $j$) with $f_1$ than with $f_2$, which demonstrates \texttt{PAS}'s ability to adapt to the predictor's quality across problems---while still allowing each problem to have its own shrinkage level. Numerical results are postponed to \cref{sec:experiments}.

\begin{figure}[t]
    \centering  \includegraphics[width=0.75\columnwidth]{images/synthetic_params.pdf}
    \caption{The power-tuned and adaptive shrinkage parameters, $\lambda^*_j$ and $\hat{\omega}_j$ across $m = 200$ problems in \cref{ex:synthetic}. On the $x$-axis, we identify the problem by its $\eta_j$ so the trend is more visible.}
    \label{fig:synthetic-params}
\end{figure}

\section{Theoretical Results}
\label{sec:theoretical-results}
In~\eqref{eq:gsure-optimization}, we proposed selecting $\hat{\omega}$ by optimizing an unbiased surrogate of true risk. In this section we justify this procedure theoretically. Our first result establishes that CURE approximates the true loss (whose expectation is the compound risk in~\eqref{eq:compound-risk}) uniformly in $\omega$ as we consider more and more problems.

\begin{proposition} \label{prop:gsure-uniform-convergence}
Suppose the datasets are generated according to Assumptions~\ref{ass:exchangeable_model} and~\ref{ass:compound_generic} and further assume that 
$\:\EEInline[\mathbb P_{\eta}]{Y_{ij}^4} < \infty,\: \EEInline[\mathbb P_{\eta}]{f(X_{ij})^4} < \infty$.
Then,
$$
\EE[\mathbb P_{\eta}]{\sup_{\omega \geq 0} \left| \textnormal{CURE}\Big(\boldsymbol{\hat \theta}^{\mathrm{PAS}}_\omega\Big) - \ell_m\Big(\boldsymbol{\hat \theta}^{\mathrm{PAS}}_\omega, \boldsymbol{\theta}\Big) \right|} = o(1),
$$
where $o(1)$ denotes a term that converges to $0$ as $m \to \infty$. 
\end{proposition}

A principal consequence of \cref{prop:gsure-uniform-convergence} is that \texttt{PAS} with the data-driven choice of $\hat{\omega}$ in~\eqref{eq:gsure-optimization} has asymptotically smaller Bayes MSE (defined in~\eqref{eq:bayes-risk}) than any of the estimators in~\eqref{eq:pas_shrinkage_form}, i.e. it has smaller MSE than both baselines in~\eqref{eq:ppi} as well as the PPI and PT estimators.

\begin{theorem}
    \label{thm:optimal-omega}
    Under the assumptions of \cref{prop:gsure-uniform-convergence},  
    $$
    \mathcal{B}_m^{\mathbb{P}_\eta} \Big(\boldsymbol{\hat \theta}^{\mathrm{PAS}}_{\hat\omega}\Big) \leq \inf_{\omega \geq 0}\left\{\mathcal{B}_m^{\mathbb{P}_\eta} \Big(\boldsymbol{\hat \theta}^{\mathrm{PAS}}_{\omega}\Big)\right\} + o(1) \,\text{ as }\, m \to \infty,
    $$
    and so 
    $$\mathcal{B}_m^{\mathbb{P}_\eta} \Big(\boldsymbol{\hat \theta}^{\mathrm{PAS}}_{\hat\omega}\Big)  \leq  \min\left\{\mathcal{B}_m^{\mathbb{P}_\eta}\Big( \boldsymbol{\tilde{Z}}^{f}\Big),\, \mathcal{B}_m^{\mathbb{P}_\eta}\Big( \boldsymbol{\hat{\theta}}^{\mathrm{PT}}\Big)\right\} + o(1).$$        
\end{theorem}

Our next proposition connects Theorem~\ref{thm:optimal-omega} with the lowest possible MSE in the stylized Gaussian example of Section~\ref{subsec:motivating-example} (last row of Table~\ref{tab:MSEs_stylized}).
\begin{proposition}
\label{prop:double-reduction-pas}
 In addition to the assumptions of~\cref{prop:gsure-uniform-convergence}, further assume that $N_j = \infty$ and that there exist $n \in \mathbb N$, $\tilde{\sigma}^2>0$ such that $n_j=n$ and $\tilde{\sigma}^2_j=\tilde{\sigma}^2$ for all $j$ almost surely. Let \smash{$\beta^2 := \mathbb{E}_{\mathbb{P}_{\eta}}[(\mlbaseline_j - \theta_j)^2]$} (which does not depend on $j$ as we are integrating over $\mathbb P_{\eta})$. Then, as $m \to \infty$,   
\begin{align*}
\mathcal{B}^{\mathbb{P}_\eta}_m\Big(\boldsymbol{\hat \theta}^{\mathrm{PAS}}_{\hat\omega}\Big) \leq  \frac{\tilde{\sigma}^2 \beta^2}{\tilde{\sigma}^2 + \beta^2}+o(1).
\end{align*}
\end{proposition}
To interpret the result, it is instructive to compare the asymptotic upper bound on the MSE of \texttt{PAS} with the MSE in the last line of Table~\ref{tab:MSEs_stylized}, i.e., with \smash{$(\sigma_{\varepsilon}^2 \sigma_{\theta \mid \phi}^2)/(\sigma_{\varepsilon}^2 + \sigma_{\theta \mid \phi}^2)$}. Observe that \smash{$\tilde{\sigma}^2$} plays the role of \smash{$\sigma_{\varepsilon}^2$} (as already anticipated in~\eqref{eq:analogy}) which is smaller than the variance of the classical estimator (due to power tuning). Meanwhile, $\beta^2$ plays the role of \smash{$\sigma^2_{\theta \mid \phi}$}. If the baseline $\mlbaseline_j$ that is the mean of the ML predictions on the unlabeled datasets is doing a good job of predicting $\theta_j$, then $\beta^2$ will be small, and so \texttt{PAS} may have MSE substantially smaller than that of PT. On the other hand, even if $\beta^2$ is large (that is, even if the ML model is very biased), \texttt{PAS} asymptotically still has MSE less or equal than $\tilde{\sigma}^2$, the MSE of PT. We emphasize that the role of~\cref{prop:double-reduction-pas} is to provide intuition at the expense of strong assumptions. By contrast, the statement of~\cref{thm:optimal-omega} does not restrict heterogeneity (e.g., heteroscedasticity) across problems and allows for varying, finite unlabeled and labeled sample sizes.




\section{Experiments}
\label{sec:experiments}

We apply the proposed \texttt{PAS} estimator and conduct extensive experiments in both the synthetic model proposed in \cref{ex:synthetic} and two real-world datasets.

\paragraph{Baselines.} We compare the \texttt{PAS} estimator against both classical and modern baseline estimators: 
\begin{enumerate}[label=(\roman*.)]
    \item the classical estimator;
    \item prediction mean on unlabeled data;
    \item the vanilla PPI estimator \citep{angelopoulos2023prediction};
    \item the PT estimator \citep[PPI++]{angelopoulos_ppi_2024}; \label{enum:pt}
    \item the ``shrink-classical'' estimator that directly shrinks the classical estimator toward the prediction mean;\label{enum:shrink-only}
    \item the ``shrink-average'' estimator that shrinks $\ptppi_j$ toward the PT group mean across all problems, \smash{$\bar{\theta}^{\mathrm{PT}} := m^{-1}\sum_j \ptppi_j$}.\label{enum:shrink-mean}
\end{enumerate}
See~\cref{appendix:baseline-shrinkage} for detailed formulations and implementations for \ref*{enum:shrink-only} \& \ref*{enum:shrink-mean} (the latter of which is inspired by the SURE-grand mean estimator of~\citet{xie2012sure}). The comparisons with \ref*{enum:pt} and \ref*{enum:shrink-only} also directly serve as ablation studies of the two stages in constructing the \texttt{PAS} estimator. 

\paragraph{Metrics.} We report the  mean squared error (MSE) ($\pm$ 1 standard error) of each estimator \smash{$\boldsymbol{\hat{\theta}}$} by averaging \smash{$\tfrac{1}{m}\sum_{j=1}^m (\hat{\theta}_j - \theta_j)^2$} across $K = 200$ Monte Carlo replicates. In the synthetic model, we sample $\eta_j$ (and thus $\theta_j$) from the known prior $\mathbb{P}_\eta$. For the real-world datasets, since $\mathbb{P}_\eta$ is unknown, we follow the standard evaluation strategy in the PPI literature: we start with a large labeled dataset and use it to compute a pseudo-ground truth for each mean $\theta_j$. Then in each Monte Carlo replicate, we randomly split the data points of each problem into labeled/unlabeled partitions (where we choose a 20/80 split ratio). We provide more details on the benchmarking procedure for the MSE in~\cref{appendix:experiments}.

For real-world datasets, we introduce a second metric to assess whether improvements in MSE are driven by a few difficult problems rather than consistent performance gains: the percentage of problems improved relative to the classical estimator. This metric is defined as
$$
\% \:\ \mathrm{Improved}\uparrow \;\;\, := \;\;\, \frac{1}{m} \sum_{j = 1}^m \mathbf{1}\left[(\hat{\theta}_j - \theta_j)^2 < (\hat{\theta}_j^{\mathrm{Classical}} - \theta_j)^2\right] \times (100\%).
$$
Larger values of this metric are preferable.
\subsection{Synthetic Model}

\begin{table}
    \centering
      \caption{MSE ($\pm$ standard error) of different estimators under the synthetic model with predictors $f_1(x) = x^2$, respectively $f_2(x) = |x|$.}
    % \begin{small}
    \vskip 0.1in
    \begin{tabular}{lcccr}
    \toprule
    \textbf{Estimator} & \textbf{MSE} $f_1$ ($\times 10^{-3}$) & \textbf{MSE} $f_2$ ($\times 10^{-3}$) \\
    \midrule
    Classical                & 3.142 $\pm$ 0.033 & 3.142 $\pm$ 0.033 \\
    Prediction Avg        & 0.273 $\pm$ 0.004 & 34.335 $\pm$ 0.147 \\
    PPI       & 2.689 $\pm$ 0.027 & 2.756 $\pm$ 0.027 \\
    PT  & 2.642 $\pm$ 0.027 & 2.659 $\pm$ 0.026 \\
    Shrink Classical           & 0.273 $\pm$ 0.003 & 3.817 $\pm$ 0.042 \\
    Shrink Avg & 2.486 $\pm$ 0.026 & 2.575 $\pm$ 0.026 \\
    \texttt{PAS} \textbf{(ours)}        & \textbf{0.272 $\pm$ 0.003} & \textbf{2.496 $\pm$ 0.025} \\
    \bottomrule
    \end{tabular}
    % \end{small}
\label{table:synthetic}
\end{table}

This is the synthetic model from \cref{ex:synthetic}, where we choose $m = 200$, $n_j = 20$, and $N_j = 80$ for all $j$. Since we have already visualized the model and the parameters of the \texttt{PAS} estimator in previous sections, we simply report the numerical results (for the good predictor $f_1$ and the flawed predictor $f_2$) in \cref{table:synthetic}.

For both predictors, we see that \texttt{PAS} outperforms all the baselines. With a good predictor $f_1$,
both the prediction mean and the shrinkage estimator closely track \texttt{PAS}; in contrast, the PPI and PT estimators fail to fully leverage the accurate predictions, as their design enforces unbiasedness. The situation reverses for the less reliable predictor $f_2$: the prediction mean and the shrinkage estimator have high MSE, while estimators with built-in de-biasing mechanisms demonstrate greater resilience. \texttt{PAS} adapts effectively across these extremes, making it a robust choice for a wide range of problems and predictors.

\subsection{Real-World Datasets}

\begin{table}
    \centering
    \caption{Results aggregated over $K = 200$ replicates on the Amazon review dataset with \texttt{BERT-base} and \texttt{BERT-tuned} predictors. Metrics are reported with $\pm$ 1 standard error.}
    \label{table:realworld}
    \vskip 0.05in
    \begin{tabular}{lcccc}
    \toprule
    & \multicolumn{2}{c}{\textbf{Amazon (base $f$)}} & \multicolumn{2}{c}{\textbf{Amazon (tuned $f$)}} \\
    \cmidrule(lr){2-3} \cmidrule(lr){4-5}
    \textbf{Estimator} & \textbf{MSE} \( (\times 10^{-3}) \)& \textbf{\% Improved $\uparrow$} & \textbf{MSE \( (\times 10^{-3}) \)} & \textbf{\% Improved $\uparrow$} \\
    \midrule
    Classical  & 24.305 $\pm$ 0.189 & baseline & 24.305 $\pm$ 0.189 & baseline \\
    Prediction Avg       & 41.332 $\pm$ 0.050 & 30.7 $\pm$ 0.2 & 3.945 $\pm$ 0.011 & 75.4 $\pm$ 0.2 \\
    PPI       & 11.063 $\pm$ 0.085 & 62.4 $\pm$ 0.2 & 7.565 $\pm$ 0.066 & 70.4 $\pm$ 0.2 \\
    PT  & 10.633 $\pm$ 0.089 & 70.3 $\pm$ 0.2 & 6.289 $\pm$ 0.050 & 76.0 $\pm$ 0.2 \\
    Shrink Classical        & 15.995 $\pm$ 0.121 & 56.4 $\pm$ 0.3 & 3.828 $\pm$ 0.039 & 78.9 $\pm$ 0.2 \\
    Shrink Avg  & 9.276 $\pm$ 0.078 & 70.4 $\pm$ 0.2 & 6.280 $\pm$ 0.058 & 77.1 $\pm$ 0.2\\
    \texttt{PAS} \textbf{(ours)} & \textbf{8.517 $\pm$ 0.071} & \textbf{71.4 $\pm$ 0.2} & \textbf{3.287 $\pm$ 0.024} & \textbf{80.8 $\pm$ 0.2} \\
    \bottomrule
    \end{tabular}
\end{table}

\begin{table}
    \centering
    \caption{Results aggregated over $K = 200$ replicates on the Galaxy dataset with \texttt{ResNet50} predictor. Metrics are reported with $\pm$ 1 standard error.}
    \label{table:galaxy}
    \vskip 0.05in
    \begin{tabular}{lcc}
    \toprule
    & \multicolumn{2}{c}{\textbf{Galaxy}} \\
    \cmidrule(lr){2-3}
    \textbf{Estimator} & \textbf{MSE} \( (\times 10^{-3}) \) & \textbf{\% Improved $\uparrow$} \\
    \midrule
    Classical  & 2.073 $\pm$ 0.028 & baseline \\
    Prediction Avg  & 7.195 $\pm$ 0.008 & 17.0 $\pm$ 0.2 \\
    PPI  & 1.149 $\pm$ 0.017 & 59.4 $\pm$ 0.3 \\
    PT  & 1.026 $\pm$ 0.015 & 67.7 $\pm$ 0.3 \\
    Shrink Classical  & 1.522 $\pm$ 0.016 & 48.8 $\pm$ 0.4 \\
    Shrink Avg & 0.976 $\pm$ 0.014 & \textbf{68.9 $\pm$ 0.3}\\
    \texttt{PAS} \textbf{(ours)} & \textbf{0.893 $\pm$ 0.011} & 67.3 $\pm$ 0.4 \\
    \bottomrule
    \end{tabular}
\end{table}

We next evaluate \texttt{PAS} on two large-scale real-world datasets, highlighting its ability to leverage state-of-the-art deep learning models in different settings. We include only the essential setup below and defer additional data and model details (e.g., hyper-parameters, preprocessing) to \cref{appendix:experiments}.

\paragraph{Amazon Review Ratings \citep{amazon_fine_food_reviews}.}
Many commercial and scientific studies involve collecting a large corpus of text and estimating an average score (rating, polarity, etc.) from it. A practitioner would often combine limited human annotations with massive automatic evaluations from ML models \citep{tyser2024ai, baly2020we}.
To emulate this setup, we consider mean rating estimation problems using the Amazon Fine Food Review dataset from Kaggle, 
where we artificially hide the labels in a random subset of the full data to serve as the unlabeled partition. Concretely, we estimate the average rating for the top $m = 200$ products with the most reviews (from $\sim$200 to $\sim$900). For the \(i\)-th review of the \(j\)-th product, the covariate \(X_{ij}\) consists of the reviewâ€™s title and text concatenated, while the outcome \(Y_{ij}\) is the star rating in \(\{1, \dots, 5\}\). We employ two black-box predictors:  (1) \texttt{BERT-base}, a language model without fine-tuning \citep{devlin2018bert} and (2) \texttt{BERT-tuned} which is the same model but fine-tuned on a held-out set of reviews from other products. Neither of these models have seen the reviews for the 200 products during training.

\paragraph{Spiral Galaxy Fractions \citep{willett2013galaxy}.}
The Galaxy Zoo 2 project contains the classification results of galaxy images from the the Sloan Digital Sky Survey~\citep[SDSS]{york2000sloan}. We are interested in estimating the fraction of galaxies that are classified as ``spiral,'' i.e., have at least one spiral arm. The covariates $X_{ij}$ in this applications are images (we provide some examples in Figure~\ref{fig:galaxyzoo2-images} of the appendix). Existing PPI papers have focused on estimating the overall fraction; we demonstrate how this datasetâ€™s metadata structure enables compound estimation of spiral fractions across distinct galaxy subgroups. We first use a pre-defined partition of the galaxies into 122 subgroups that is based on the metadata attribute \texttt{REDSHIFT}.  Second, we estimate the fraction of spiral galaxies in all of the galaxy subgroups simultaneously.  For the predictor, we train a \texttt{ResNet50} convolutional neural network on a held-out set with around 50k images.\\

For both datasets, we randomly split the data for each problem (a food product or galaxy subgroup) into a labeled and unlabeled partition with a 20/80 ratio. We repeat the random splitting $K=200$ times and report metrics averaged over all splits.
Results are displayed in \cref{table:realworld}, and here is a brief summary:

\begin{itemize}[leftmargin=*]
    \item \textbf{Amazon Review:} 
    similar to the trend in the synthetic model, the more accurate \texttt{BERT-tuned} model enables stronger shrinkage for \texttt{PAS}  while the biased \texttt{BERT-base} predictions necessitate less shrinkage. Our \texttt{PAS} estimator adapts to both predictors and outperforms other baselines. \texttt{PAS} has the lowest  MSE without sacrificing performance on our second metric.
    \item \textbf{Galaxy Zoo 2:} 
    the predictions from \texttt{ResNet50} are suboptimal, so the variance-reduction from power tuning dominates any benefit from shrinkage. \texttt{PAS} achieves the lowest MSE among all estimators, and improves individual estimates at a level on par with the shrink-towards-mean estimator.
\end{itemize}
In \cref{fig:comparisons}, we visualize the \texttt{PAS} and PT estimators against the  pseudo-ground truth $\theta_j$ for each problem in the Galaxy Zoo and Amazon Review (with \texttt{BERT-tuned}) datasets. With the quantitative results obtained above, it is not surprising to see \texttt{PAS} estimators being closer to the $45^{\circ}$ line of perfect estimation. More importantly, the spread of pseudo-ground truth $\theta_j$ suggests that there is substantial heterogeneity (in outcome means) across different problems, justifying the stratification procedure and the compound mean estimation setting.
\begin{figure}[H]
    \centering
      \begin{minipage}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{images/comparison.pdf}
      \end{minipage}
      \hfill
      \begin{minipage}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{images/comparison2.pdf}
      \end{minipage}
    \caption{Comparison between the \texttt{PAS} and PT (PPI++) estimators for each problem in the Galaxy Zoo and Amazon Review (with \texttt{BERT-tuned}) datasets. The dashed $45^{\circ}$ line indicates perfect estimation against the pseudo-ground truth $\theta_j$.}
    \label{fig:comparisons}
\end{figure}

\section{Conclusion}

This paper introduces \texttt{PAS}, a novel method for compound mean estimation that effectively combines PPI and EB principles. We motivate the problem through the lens of variance reduction and contextual prior information---then demonstrate how \texttt{PAS} achieves both goals, in theory and in practice. Our paper differs from many other PPI-related works in its focus on estimation, so a natural next step is to develop average coverage controlling intervals for the means centered around \texttt{PAS}. To this end, it may be fruitful to build on the robust empirical Bayes confidence intervals of~\citet{armstrong2022robust}.
Modern scientific inquiries increasingly demand the simultaneous analysis of multiple related problems. The framework developed in this paper---combining empirical Bayes principles with machine learning predictions---represents a promising direction for such settings. 



\bibliographystyle{abbrvnat}
\bibliography{pas}

\newpage
\appendix
\section*{\centering \Large $\clubsuit$ Appendix: Table of Contents}
\addcontentsline{toc}{section}{Appendix}
\begin{itemize}
    \item \textbf{\hyperref[appendix:cure]{A. The Correlation-Aware Unbiased Risk Estimate}} \dotfill Page \pageref{appendix:cure}
    \item \textbf{\hyperref[appendix:proof]{B. Proofs of Theoretical Results}} \dotfill Page \pageref{appendix:proof}
    \item \textbf{\hyperref[appendix:baseline-shrinkage]{C. Baseline Shrinkage Estimators}} \dotfill Page \pageref{appendix:baseline-shrinkage}
    \item \textbf{\hyperref[appendix:experiments]{D. Experiment Details}} \dotfill Page \pageref{appendix:experiments}
\end{itemize}



\section{The Correlation-Aware Unbiased Risk Estimate}
\label{appendix:cure}

\begin{theorem} \label{thm:generalized-sure}
    Let \(X, Y\) be two random variables satisfying \(\EE[\theta]{X} = \theta\), \(\Var[\theta]{X} = \sigma^2\), \(\Cov[\theta]{X, Y} = \gamma\), and the second moment of \(Y\) exists.\footnote{We redefine certain variables for generality of this result beyond the setting in \cref{ass:compound_generic}. In this theorem, $\theta$ plays the role of $\eta$ in the main text, i.e. all the other parameters are deterministic given $\theta$.} Consider estimating \(\theta\) with the shrinkage estimator \(\hat\theta_c = c X + (1-c)Y\) with \(c \in [0, 1]\). Assuming that \(\sigma^2\) and \(\gamma\) are known, the following estimator
    \begin{align}
        \emph{CURE}(\hat\theta_c) := (2c - 1) \sigma^2 + 2(1-c)\gamma + \{(1-c)(X - Y)\}^2,
        \label{eq:generalized_sure_definition}
    \end{align}
    defined as the \underline{C}orrelation-aware \underline{U}nbiased \underline{R}isk \underline{E}stimate, is an unbiased estimator for the risk of \(\hat\theta_c\) under  quadratic loss. That is, letting $R(\hat\theta_c,\theta) := \EEInline[\theta]{(\hat\theta_c - \theta)^2}$, it holds that:
    $$\EE[\theta]{\mathrm{CURE}(\hat\theta_c)} = R(\hat\theta_c,\theta).$$
\end{theorem}


\begin{proof}
    First, expand the risk:
\begin{align*}
    R(\hat{\theta}_c,\theta) &= \mathbb{E}_\theta[(\hat{\theta}_c - \theta)^2] = \mathbb{E}_\theta[(cX + (1 - c)Y - \theta)^2] \\
    &= \Var[\theta]{c X + (1-c)Y} + \left(\EE[\theta]{cX + (1-c)Y} -\theta\right)^2 \\
    &= c^2 \sigma^2 + (1 - c)^2 \Var[\theta]{Y} + 2c(1-c) \gamma + [(1-c)(\EE[\theta]{Y} - \theta)]^2.
\end{align*}
    Then, taking the expectation of $\text{CURE}(\hat\theta_c)$:
\begin{align} \label{eq:sure}
    \EE[\theta]{\text{CURE}(\hat\theta_c)} = \underbrace{(2c - 1) \sigma^2 + 2(1-c)\gamma}_{\mathbb{I}} + \EE[\theta]{\{(1-c)(X - Y)\}^2},
\end{align}
    where the last term is
\begin{align*}
    \EE[\theta]{\{(1-c)(X - Y)\}^2} &= (1-c)^2\left[(\EE[\theta]{X - Y})^2 + \Var[\theta]{X - Y}\right]\\
    &= (1-c)^2 \left[(\EE[\theta]{Y} - \theta)^2 + \sigma^2 + \Var[\theta]{Y} - 2\gamma\right] \\
    &= [(1-c)(\EE[\theta]{Y} - \theta)]^2 + \underbrace{(1-c)^2(\sigma^2 + \Var[\theta]{Y}  - 2\gamma)}_{\mathbb{II}}.
\end{align*}
With a little algebra, we observe
\begin{align*}
    \mathbb{I} + \mathbb{II} &= (2c - 1) \sigma^2 + 2(1-c)\gamma + (1-c)^2(\sigma^2 + \Var[\theta]{Y}  - 2\gamma) \\
    &= c^2 \sigma^2 + (1-c)^2\Var[\theta]{Y} + 2c(1-c)\gamma.
\end{align*}
Thus, a term-by-term matching confirms $\EEInline[\theta]{\text{CURE}(\hat\theta_c)} = R( \hat\theta_c, \theta)$.
\end{proof}



\begin{remark}[Connection to SURE]
    Stein's Unbiased Risk Estimate (SURE) was proposed in Charles Stein's seminal work \citeyearpar{stein1981estimation} to study the quadratic risk in Gaussian sequence models. As a simple special case of SURE, let $Z \sim \mathcal{N}(\theta,\, \sigma^2)$ and let $h: \mathbb{R} \to \mathbb{R}$ be an absolutely continuous function and $\EE[\theta]{|h'(Z)|} < \infty$, then SURE is defined as 
    \begin{align*}
    \text{SURE}(h) \;:= (h(Z) - Z)^2 + 2\sigma^2 h'(Z) - \sigma^2,
    \end{align*}
    with the property that $\EE[\theta]{\text{SURE}(h)} = R(h(Z), \theta) = \EEInline[\theta]{(h(Z) - \theta)^2}$. A proof of this argument relies on Stein's lemma, an identity specific to Gaussian random variables \cite{stein1981estimation}. Now consider the specific linear shrinkage estimator
   $h_c(Z) := cZ + (1-c)Y$, with $c \in [0, 1]$ and $Y \in \mathbb{R}$ being fixed (that is, $Y$ is a constant, or $Y$ is independent of $Z$ and we condition on $Y$). Then $\mathrm{SURE}$ takes the following form:
    \begin{align*}
        \text{SURE}(h_c) &= (h_c(Z) - Z)^2 + 2\sigma^2 h_c'(Z) - \sigma^2 \\
        &= (cZ + (1-c)Y - Z)^2 + 2c\sigma^2 - \sigma^2 \\
        &= [(1-c)(Y - Z)]^2 + (2c - 1)\sigma^2 \\
        &\stackrel{(\star)}{=} \text{CURE}(h_c(Z)),
    \end{align*}
    where in $(\star)$ we used the fact that in this case (with $Y$ fixed or $Y$ independent of $Z$), it holds that $\gamma=0$ so that the definition of $\mathrm{CURE}$ in~\eqref{eq:generalized_sure_definition}
    simplifies. This explains how CURE defined in \ref{thm:generalized-sure} is connected to SURE.

    We make one last remark: The derivation of SURE itself requires Gaussianity. However, for linear shrinkage rules as $h_c(Z)$, SURE only depends on the first two moments of the distribution of $Z$ and thus is an unbiased estimator of quadratic risk under substantial generality as long as $\EEInline[\theta]{Z}=\theta$ and $\VarInline[\theta]{Z}=\sigma^2$. This remark has been made by previous authors, e.g.,~\citet{kou2017optimal, ignatiadis2019covariatepowered} and is important for the assumption-lean validity of \texttt{PAS}. 
\end{remark}


\section{Proofs of Theoretical Results}
\label{appendix:proof}

\subsection{Proof of Theorem.~\ref{thm:gsure-PAS}}
\label{subsec:proof-of-gsure-PAS}
For each problem \(j \in [m]\), we are shrinking the PT estimator $\ptppi_j$ obtained from the first stage towards $\mlbaseline_j$, the prediction mean 
on the unlabeled data.
Conditioning on $\eta_j$, we denote
\begin{align*}
    \tilde{\sigma}^2_j &:= \Var[\eta_j]{\hat{\theta}_j^{\mathrm{PT}}} = \Var[\eta_j]{\hat{\theta}_{j, \lambda^*_j}^{\text{PPI}}}, \\
    \tilde{\gamma}_j &:= \Cov[\eta_j]{\hat{\theta}_j^{\mathrm{PT}}, \mlbaseline_j} 
    = \lambda^*_j \Var[\eta_j]{\mlbaseline_j},
\end{align*}
where all the first and second moments of $\hat{\theta}_j^{\mathrm{PT}}$ and $\mlbaseline_j$ exist under the conditions of \cref{ass:compound_generic}.
For each global $\omega \geq 0$, the shrinkage parameter for the $j$-th problem is defined as $\omega_j := \omega / (\omega + \tilde{\sigma}^2_j)$. Then, following the result in \cref{thm:generalized-sure}, CURE for $\hat{\theta}_{j, \omega_j}^{\mathrm{PAS}} := \omega_j \hat{\theta}_{j}^{\mathrm{PT}} + (1 - \omega_j) \mlbaseline_j$,
\begin{align*}
    \mathrm{CURE}\left(\hat{\theta}_{j, \omega}^{\mathrm{PAS}}\right) = (2\omega_j - 1)\tilde{\sigma}^2_j + 2(1 - \omega_j) \tilde{\gamma}_j + \left[(1 - \omega_j)(\hat{\theta}_{j}^{\mathrm{PT}} - \mlbaseline_j)\right]^2,
\end{align*}
 is an unbiased estimator of the risk, i.e.,
\begin{align*}
    \EE[\eta_j]{\text{CURE}\left(\hat{\theta}_{j, \omega}^{\mathrm{PAS}}\right)} = R( \hat{\theta}_{j, \omega_j}^{\mathrm{PAS}},\theta_j).
\end{align*}
Finally, the CURE for the collection of estimators is $\boldsymbol{\hat \theta}^{\mathrm{PAS}}_\omega := (\hat{\theta}_{1, \omega}^{\mathrm{PAS}}, \ldots, \hat{\theta}_{m, \omega}^{\mathrm{PAS}})^\intercal$
\begin{align*}
    \text{CURE}\left(\boldsymbol{\hat \theta}^{\mathrm{PAS}}_\omega\right) := \frac{1}{m}\sum_{j = 1}^m \text{CURE}\left(\hat{\theta}_{j, \omega}^{\mathrm{PAS}}\right),
\end{align*}
which is an unbiased estimator of the compound risk $\mathcal{R}_m(\boldsymbol{\hat \theta}^{\mathrm{PAS}}_\omega, \boldsymbol{\theta})$ by linearity of the expectation. \qed

\subsection{Formal conditions and proof of \texorpdfstring{\cref{prop:gsure-uniform-convergence}}{
\ref{prop:gsure-uniform-convergence}
}}
\label{subsec:proof-of-gsure-uniform-convergence}
We aim to prove that CURE converges uniformly to the true squared-error loss \( \ell_m(\boldsymbol{\hat \theta}^{\mathrm{PAS}}_\omega, \boldsymbol{\theta}) \) as \( m \to \infty \). Specifically, our goal is to establish
\[
\sup_{\omega \geq 0} \left| \text{CURE}(\boldsymbol{\hat \theta}^{\mathrm{PAS}}_\omega) - \ell_m(\boldsymbol{\hat \theta}^{\mathrm{PAS}}_\omega, \boldsymbol{\theta}) \right| \xlongrightarrow[m \to \infty]{L^1} 0.
\]

For this proposition, all the expectation and variance terms without subscript are conditioning on $\boldsymbol{\eta}$. We keep using the notations $\theta_j = \EEInline{\ptppi_j}$, $\mu_j = \EEInline{\mlbaseline_j}$, \( \tilde{\sigma}^2_j = \VarInline{\ptppi_j} \) and \( \tilde{\gamma}_j = \CovInline{\hat{\theta}_j^{\mathrm{PT}}, \mlbaseline_j}\).
For this proposition, additional assumptions are placed on the data generating process (integrated over $\mathbb{P}_\eta$). We first show how they translate to moment conditions on the estimators \smash{$\ptppi_j$} and \smash{$\mlbaseline_j$}.

\begin{lemma}
    \label{lemma:moment-results}
    Under the assumptions of \cref{prop:gsure-uniform-convergence}, and specifically $\EE[\mathbb{P}_\eta]{f(X_{ij})^4} < \infty$ and $\EE[\mathbb{P}_\eta]{Y_{ij}^4} < \infty$, it holds that:
    \begin{align*}
        \sup_{j\geq 1} \EE[\mathbb{P}_\eta]{\big( \hat{\theta}_j^{\mathrm{PT}} \big)^4} &< \infty, \quad \quad 
        \sup_{j\geq 1} \EE[\mathbb{P}_\eta]{\big( \mlbaseline_j \big)^4} < \infty, \\
        \EE[\mathbb{P}_\eta]{ \theta_j^4} &< \infty, \quad \quad \EE[\mathbb{P}_\eta]{ \mu_j^4} < \infty.
    \end{align*}
\end{lemma}
\begin{proof}
    By Minkowski's inequality, we have 
    \begin{align*}
        \EE[\mathbb{P}_\eta]{\big( \hat{\theta}_j^{\mathrm{PT}} \big)^4} &= \EE[\mathbb{P}_\eta]{\big( \bar{Y}_j + \lambda_j^* (\mlbaseline_j - \bar{Z}_j^f) \big)^4} \\
        &\leq \left(\EE[\mathbb{P}_\eta]{\bar{Y}_j^4}^{1/4} + \lambda_j^* \EE[\mathbb{P}_\eta]{\big( \mlbaseline_j \big)^4}^{1/4} + \lambda_j^* \EE[\mathbb{P}_\eta]{\big( \bar{Z}_j^f \big)^4}^{1/4} 
        \right)^4,
    \end{align*}
    so it suffices to bound the fourth moments of $\bar{Y}_j, \tilde{Z}_j, \bar{Z}_j$. We proceed with $\bar{Y}_j$ first. Again, by Minkowski's inequality
    \begin{align*}
        \EE[\mathbb{P}_\eta]{\bar{Y}_j^4}^{1/4} &= \EE[\mathbb{P}_\eta]{\left(\sum_{i=1}^{n_j} n_j^{-1} Y_{ij} \right)^4}^{1/4} \\
        &\leq \sum_{i=1}^{n_j} \EE[\mathbb{P}_\eta]{\left( n_j^{-1} Y_{ij} \right)^4}^{1/4} \\
        &= \sum_{i=1}^{n_j} n_j^{-1}\EE[\mathbb{P}_\eta]{Y_{ij}^4}^{1/4} \\
        &= \EE[\mathbb{P}_\eta]{Y_{ij}^4}^{1/4} < \infty.
    \end{align*}
    The given assumption on the data generating process is used in the last line. Although the left-hand side of the inequality may depend on $j$ (via the deterministic $n_j$), the right-hand side does not depend on $j$ (since we are integrating over $\mathbb P_{\eta}$) and so we may take a supremum over all $j$ on the left-hand side. The arguments for \smash{$\tilde{Z}_j$} and \smash{$\bar{Z}_j$} based on the finiteness of $\EEInline[\mathbb{P}_\eta]{f(X_{ij})^4}$ are analogous. Next, by Jensen's inequality we have 
    \begin{align*}
        \theta_j^4 &= \EE[\eta_j]{\ptppi_j}^4 \leq \EE[\eta_j]{ \big(\ptppi_j\big)^4} \\
        \Longrightarrow \quad \EE[\mathbb{P}_\eta]{ \theta_j^4} &\leq \EE[\mathbb{P}_\eta]{ \big(\ptppi_j\big)^4} < \infty,
    \end{align*}
    and similarly we can also obtain $\EEInline[\mathbb{P}_\eta]{ \mu_j^4} < \infty$.
\end{proof}

With \cref{lemma:moment-results}, we will prove \cref{prop:gsure-uniform-convergence} via the following steps.

\paragraph{Step 1: Decompose the difference.}
We first decompose both  CURE and the loss separately as
\begin{align*}
\text{CURE}(\boldsymbol{\hat \theta}^{\mathrm{PAS}}_\omega) &= \frac{1}{m} \sum_{j = 1}^m \left((2\omega_j - 1)\tilde{\sigma}^2_j + \left[(1 - \omega_j)(\ptppi_j - \mlbaseline_j)\right]^2 + 2(1 - \omega_j) \tilde{\gamma}_j\right) \\
&= \underbrace{\frac{1}{m}\sum_{j = 1}^m \left( (2\omega_j - 1)\tilde{\sigma}^2_j + (1-\omega_j)^2 (\ptppi_j - \mu_j)^2 \right)}_{\mathbb{I}(\omega)} \\ 
&+ \underbrace{\frac{1}{m}\sum_{j = 1}^m \left(2(1 - \omega_j) \tilde{\gamma}_j + 2(1 - \omega_j)^2 (
\mu_j - \mlbaseline_j)(\ptppi_j - \mu_j) + (1 - \omega_j)^2 (\mu_j - \mlbaseline_j)^2 \right)}_{\mathbb{II}(\omega)} \\
\ell_m(\boldsymbol{\hat \theta}^{\mathrm{PAS}}_\omega, \boldsymbol{\theta}) 
&= \frac{1}{m}\sum_{j = 1}^m (\omega_j \ptppi_j + (1 - \omega_j) \mlbaseline_j - \theta_j)^2 \\
&= \underbrace{\frac{1}{m}\sum_{j = 1}^m (\omega_j \ptppi_j + (1 - \omega_j) \mu_j - \theta_j)^2}_{\mathbb{I}^*(\omega)} \\
&+ \underbrace{\frac{1}{m}\sum_{j = 1}^m \left(2(1 - \omega_j) (\mlbaseline_j - \mu_j)(\ptppi_j - \theta_j) + 2(1 - \omega_j)^2 (\mu_j - \mlbaseline_j)(\ptppi_j - \mu_j) + (1 - \omega_j)^2 (\mu_j - \mlbaseline_j)^2 \right)}_{\mathbb{II}^*(\omega)},
\end{align*}
and we are interested in bounding 
\begin{align} \label{eq:diff-decompose}
    \sup_{\omega \geq 0} \left| \text{CURE}(\boldsymbol{\hat \theta}^{\mathrm{PAS}}_\omega) - \ell_m(\boldsymbol{\hat \theta}^{\mathrm{PAS}}_\omega, \boldsymbol{\theta}) \right| \leq \sup_{\omega \geq 0} \left| \mathbb{I}(\omega) - \mathbb{I}^*(\omega) \right| + \sup_{\omega \geq 0} \left| \mathbb{II}(\omega) - \mathbb{II}^*(\omega) \right|.
\end{align}

\paragraph{Step 2: Bounding the first difference $\Delta_1(\omega) := \mathbb{I}(\omega) - \mathbb{I}^*(\omega)$.}

The proof in this step is directly adapted from \textbf{Theorem 5.1} in \citet{xie2012sure} and generalizes to non-Gaussian data. With some algebraic manipulation, we can further decompose
\begin{align*}
    \Delta_1(\omega) &= \frac{1}{m}\sum_{j = 1}^m \left((2\omega_j - 1)\tilde{\sigma}^2_j + (1 - \omega_j)^2 (\ptppi_j - \mu_j)^2 \right) \\
    &- \frac{1}{m}\sum_{j = 1}^m (\omega_j \ptppi_j + (1 - \omega_j) \mu_j - \theta_j)^2 \\
    &= \text{CURE}(\boldsymbol{\hat \theta}^{0}_\omega) - \ell_m(\boldsymbol{\hat \theta}^{0}_\omega, \boldsymbol{\theta}) - \frac{2}{m}\sum_{j = 1}^m \mu_j (1- \omega_j) (\ptppi_j - \theta_j) \\
    \text{where} \quad \text{CURE}(\boldsymbol{\hat \theta}^{0}_\omega) &= \frac{1}{m}\sum_{j = 1}^m \left( (2\omega_j - 1)\tilde{\sigma}^2_j + (1 - \omega_j)^2 \big(\ptppi_j\big)^2 \right), \quad
    \ell_m(\boldsymbol{\hat \theta}^{0}_\omega, \boldsymbol{\theta}) = 
    \frac{1}{m}\sum_{j = 1}^m (\omega_j \ptppi_j - \theta_j)^2,
\end{align*}
corresponds to CURE and the loss of the ``shrink-towards-zero'' estimator $\hat{\theta}^{0}_{j,\omega} := \omega_j \ptppi_j$. We thus have 
\begin{align}
    \sup_{\omega \geq 0} \left| \Delta_1(\omega) \right| &\leq \sup_{\omega \geq 0} \left| \text{CURE}(\boldsymbol{\hat \theta}^{0}_\omega) - \ell_m(\boldsymbol{\hat \theta}^{0}_\omega, \boldsymbol{\theta}) \right| + \frac{2}{m} \sup_{\omega \geq 0} \Big| \sum_{j = 1}^{m} \mu_j (1 - \omega_j) (\ptppi_j - \theta_j) \Big|.\label{eq:delta1-decompose}
\end{align}
Now, rearrangements of terms gives that
\begin{align*}
    \sup_{\omega \geq 0} \left| \text{CURE}(\boldsymbol{\hat \theta}^{0}_\omega) - \ell_m(\boldsymbol{\hat \theta}^{0}_\omega, \boldsymbol{\theta}) \right| 
    &= \sup_{\omega \geq 0} \bigg| \frac{1}{m}\sum_{j = 1}^m \left( \big(\ptppi_j\big)^2 - \tilde{\sigma}^2_j - \theta_j^2 - 2\omega_j \Big(\big(\ptppi_j\big)^2 - \ptppi_j\theta_j - \tilde{\sigma}^2_j \Big) \right) \bigg| \\
    &\leq \underbrace{\bigg| \frac{1}{m}\sum_{j = 1}^m \big(\ptppi_j\big)^2 - \tilde{\sigma}^2_j - \theta_j^2 \bigg|}_{(*)} +  \underbrace{\sup_{\omega \geq 0} \bigg| \frac{1}{m}\sum_{j = 1}^m 2\omega_j \Big(\big(\ptppi_j\big)^2 - \ptppi_j\theta_j - \tilde{\sigma}^2_j\Big) \bigg|}_{(**)}.
\end{align*}
For the first term $(*)$, 
$$
\EE[\mathbb P_{\eta}]{\EE[\boldsymbol{\eta}]{\bigg(\frac{1}{m}\sum_{j = 1}^m \big(\ptppi_j\big)^2 - \tilde{\sigma}^2_j - \theta_j^2\bigg)^2}} = \frac{1}{m^2} \sum_{j = 1}^m \EE[\mathbb P_{\eta}]{\Var[\eta_j]{\big(\ptppi_j\big)^2}} \leq \frac{1}{m} \sup_{j} \Var[\mathbb P_{\eta}]{ {\big(\ptppi_j\big)^2}}.
$$
Thus by Jensen's inequality and iterated expectation:
\begin{align}
\label{eq:bound-1}
 \EE[\mathbb P_{\eta}]{\bigg|\frac{1}{m}\sum_{j = 1}^m \big(\ptppi_j\big)^2 - \tilde{\sigma}^2_j - \theta_j^2\bigg| } \leq \left(\frac{1}{m} \sup_{j} \Var[\mathbb P_{\eta}]{ {\big(\ptppi_j\big)^2}}\right)^{1/2}.
\end{align}
For the second term $(**)$, we start by arguing conditionally on $\boldsymbol{\eta}$, which implies in particular that we may treat all the $\tilde{\sigma}^2_j$ as fixed. It is thus without loss of generality to assume that $\tilde{\sigma}^2_1 \leq ... \leq \tilde{\sigma}^2_m$ (by first sorting problems according to the value of $\tilde{\sigma}^2_j$). Then, since $\omega_j$ is monotonic function of $\tilde{\sigma}_j^2$ for any fixed $\omega \geq 0$, we have $1 \geq \omega_1\geq...\geq\omega_m \geq 0$. The following inequality follows:
\begin{align} \label{eq:monotoic-trick-begin}
    \sup_{\omega \geq 0} \bigg|\frac{1}{m}\sum_{j = 1}^m 2\omega_j \Big(\big(\ptppi_j\big)^2 - \ptppi_j\theta_j - \tilde{\sigma}^2_j \Big)\bigg| &\leq \max_{1 \geq c_1 \geq ... \geq c_m \geq 0} \bigg|\frac{2}{m}\sum_{j = 1}^m c_j \Big(\big(\ptppi_j\big)^2 - \ptppi_j\theta_j - \tilde{\sigma}^2_j \Big)\bigg|.
\end{align}
The following lemma would help us for handling the RHS of~\eqref{eq:monotoic-trick-begin} (the same structural form of it will appear repeatedly in subsequent parts of the proof).
\begin{lemma}
\label{lemma:finite-sup}
Let \(A_1,\dots,A_n\) be real numbers. Then
\[
\max_{\substack{1 \ge c_1 \ge \dots \ge c_n \ge 0}}
\left| \sum_{i=1}^n c_i \, A_i \right|
\;=\;
\max_{1 \le k \le n}
\left| \sum_{i=1}^k A_i \right|.
\]
\end{lemma}
\begin{proof}
Define \(S_k = \sum_{i=1}^k A_i\) for \(k=1,\dots,n\), and let \(c_1,\dots,c_n\) be real numbers satisfying \(1 \ge c_1 \ge \dots \ge c_n \ge 0\).  Set \(c_{n+1} = 0\).  Then we can rewrite
\[
\sum_{i=1}^n c_i \, A_i
\;=\;
\sum_{k=1}^n (c_k - c_{k+1}) \, \Bigl(\sum_{i=1}^k A_i\Bigr)
\;=\;
\sum_{k=1}^n (c_k - c_{k+1})\,S_k.
\]
Since \(c_k \ge c_{k+1}\), each \(\alpha_k := c_k - c_{k+1}\) is nonnegative, and
\[
\sum_{k=1}^n \alpha_k = c_1 - c_{n+1} \le 1.
\]
Hence,
\[
\left| \sum_{i=1}^n c_i A_i \right|
\;=\;
\Bigl|\sum_{k=1}^n \alpha_k\,S_k\Bigr|
\;\le\;
\sum_{k=1}^n \alpha_k\,|S_k|
\;\le\;
\Bigl(\max_{1 \le k \le n} |S_k|\Bigr)\,\Bigl(\sum_{k=1}^n \alpha_k\Bigr)
\;\le\;
\max_{1 \le k \le n} |S_k|.
\]
This shows 
\[
\max_{\substack{1 \ge c_1 \ge \dots \ge c_n \ge 0}}
\left| \sum_{i=1}^n c_i \, A_i \right|
\;\le\;
\max_{1 \le k \le n} \left|\sum_{i=1}^k A_i \right|.
\]
To see that this upper bound can be attained, consider for each \(k\) the choice
\[
c_1 = c_2 = \dots = c_k = 1, 
\quad 
c_{k+1} = c_{k+2} = \dots = c_n = 0.
\]
Since \(1 \ge c_1 \ge \dots \ge c_n \ge 0\), we have that
$$\left | 
\sum_{i=1}^n c_i A_i \right |= \left | \sum_{i=1}^k A_i \right |= |S_k|.
$$
Taking the maximum over all such \(k\in\{1,\dots,n\}\) matches \(\max_{1 \le k \le n} |S_k|\).  Thus,
\[
\max_{\substack{1 \ge c_1 \ge \dots \ge c_n \ge 0}}
\left|\sum_{i=1}^n c_i A_i\right|
\;=\;
\max_{1 \le k \le n} \left|\sum_{i=1}^k A_i \right|,
\]
as claimed.
\end{proof}
With~\cref{lemma:finite-sup} in hand, we have
\begin{align*}
    \max_{1 \geq c_1 \geq ... \geq c_m \geq 0} \bigg|\frac{2}{m}\sum_{j = 1}^m c_j \Big(\big(\ptppi_j\big)^2 - \ptppi_j\theta_j - \tilde{\sigma}^2_j \Big)\bigg| &= \max_{1 \leq k \leq m} \bigg| \frac{2}{m}\sum_{j = 1}^k \Big(\big(\ptppi_j\big)^2 - \ptppi_j\theta_j - \tilde{\sigma}^2_j \Big)\bigg|.
\end{align*}
Let $M_k := \sum_{j = 1}^k (\big(\ptppi_j\big)^2 - \ptppi_j\theta_j - \tilde{\sigma}^2_j)$, it is easy to see that $\{M_k\}_{k=1}^m$ forms a martingale conditional on $\boldsymbol{\eta}$. Therefore, by a standard $L^2$ maximal inequality (ref. Theorem 4.4.6 in \citet{durrett2019probability}), we have
\begin{align} \label{eq:monotoic-trick-end}
    &\EE[\boldsymbol{\eta}]{\max_{1 \leq k \leq m} M_k^2} \leq 4\EE[\boldsymbol{\eta}]{M_m^2} = 4 \sum_{j=1}^m \Var[\eta_j]{\big(\ptppi_j\big)^2-\ptppi_j\theta_j},
\end{align}
which then implies
\begin{align}
    \EE[\mathbb P_{\eta}]{\bigg(\sup_{\omega \geq 0} \bigg|\frac{1}{m}\sum_{j = 1}^m 2\omega_j \Big(\big(\ptppi_j\big)^2 - \ptppi_j\theta_j - \tilde{\sigma}^2_j\Big)\bigg| \bigg)^2} &\leq \frac{4}{m^2}\EE[\mathbb P_{\eta}]{\max_{1 \leq k \leq m} M_k^2} \nonumber \\ 
    &= \frac{16}{m^2} \sum_{j=1}^m \EE[\mathbb{P}_\eta]{\Var[\eta_j]{\big(\ptppi_j\big)^2 - \ptppi_j \theta_j}} \nonumber \\
    &\leq \frac{16}{m} \sup_j \Var[\mathbb P_{\eta}]{\big(\ptppi_j\big)^2 - \ptppi_j \theta_j} \nonumber \\
    \Longrightarrow \quad \EE[\mathbb P_{\eta}]{\sup_{\omega \geq 0} \bigg|\frac{1}{m}\sum_{j = 1}^m 2\omega_j \Big(\big(\ptppi_j\big)^2 - \ptppi_j\theta_j - \tilde{\sigma}^2_j\Big)\bigg|} &\leq  \left(\frac{16}{m} \sup_j \Var[\mathbb P_{\eta}]{\big(\ptppi_j\big)^2 - \ptppi_j \theta_j}\right)^{1/2}. \label{eq:bound-2}
\end{align}
Next, we bound the last expression in~\eqref{eq:delta1-decompose}: $\, \frac{2}{m} \sup_{\omega \geq 0} \left| \sum_{j = 1}^{m}  (1 - \omega_j) \mu_j(\ptppi_j - \theta_j) \right| $. Note that $(1 - \omega_j)$ is also monotonic in $\tilde{\sigma}^2_j$, and if we define $M'_k := \sum_{j=1}^k \mu_j (\ptppi_j - \theta_j)$, then $\{M'_k\}_{k=1}^m$ forms another martingale conditioning on $\boldsymbol{\eta}$. Therefore, following the same argument as \eqref{eq:monotoic-trick-begin}--\eqref{eq:monotoic-trick-end} gives
\begin{align}
    \frac{4}{m^2}\EE[\mathbb{P}_\eta]{\sup_{\omega \geq 0} \Big| \sum_{j = 1}^{m} (1 - \omega_j) \mu_j(\ptppi_j - \theta_j) \Big|^2} &\leq \frac{4}{m^2}\EE[\mathbb{P}_\eta]{ \max_{1 \leq k \leq m} {M'_k}^2 } \nonumber \\
    &\leq \frac{16}{m^2} \EE[\mathbb{P}_\eta]{{M'_m}^2} = \frac{16}{m} \sup_{j} \EE[\mathbb P_{\eta}]{\Var[\eta_j]{\ptppi_j} \mu_j^2} \nonumber \\
    \Longrightarrow \: \EE[\mathbb{P}_\eta]{\frac{2}{m} \sup_{\omega \geq 0} \left| \sum_{j = 1}^{m}  (1 - \omega_j) \mu_j(\ptppi_j - \theta_j) \right| } &\leq \left(\frac{16}{m} \sup_{j} \EE[\mathbb P_{\eta}]{\Var[\eta_j]{\ptppi_j} \mu_j^2}\right)^{1/2}. \label{eq:bound-3}
\end{align}
The upper bounds derived in~\eqref{eq:bound-1},~\eqref{eq:bound-2} and~\eqref{eq:bound-3} establish control on
$$
\EE[\mathbb P_{\eta}]{\sup_{\omega \geq 0} \left| \Delta_1(\omega) \right|} \leq \frac{4}{\sqrt{m}} 
\left( 
\sup_{j} \Var[\mathbb P_{\eta}]{ {\big(\ptppi_j\big)^2}}^{1/2} + 
\sup_j \Var[\mathbb P_{\eta}]{\big(\ptppi_j\big)^2 - \ptppi_j \theta_j} +
\sup_{j} \EE[\mathbb P_{\eta}]{\Var[\eta_j]{\ptppi_j} \mu_j^2}
\right),
$$
since each term on the right-hand side can be controlled by the fourth-moment conditions established in \cref{lemma:moment-results}, we know that $\Delta_1(\omega)$ converges uniformly to zero.

\paragraph{Step 3: Bounding the second difference $\Delta_2(\omega) := \mathbb{II}(\omega) - \mathbb{II}^*(\omega)$.}

We next cancel out identical terms in the second difference in \eqref{eq:diff-decompose} and get 
\begin{align}
    \Delta_2(\omega) &= 
    \frac{2}{m}\sum_{j = 1}^m (1 - \omega_j) \big[\tilde{\gamma}_j - (\mlbaseline_j - \mu_j)(\ptppi_j - \theta_j) \big].
    \label{eq:diff2-term2}
\end{align}
By the same proof logic that has been applied twice above, we now have a function $(1 - \omega_j)$ monotonic in $\tilde{\sigma}_j^2$, and a martingale $ Q_k := \sum_{j=1}^k \left[ \tilde{\gamma}_j - (\mlbaseline_j - \mu_j)(\ptppi_j - \theta_j)  \right] $ for $k = 1, \ldots, m$ (recall that $\tilde{\gamma}_j = \CovInline[\eta_j]{\ptppi_j, \mlbaseline_j}$). The steps from \eqref{eq:monotoic-trick-begin}--\eqref{eq:monotoic-trick-end} follows, and we have
$$
    \frac{4}{m^2} \EE[\mathbb{P}_\eta]{\bigg(\sup_{\omega \geq 0} \bigg| \sum_{j = 1}^m (1 - \omega_j) \big[\tilde{\gamma}_j - (\mlbaseline_j - \mu_j)(\ptppi_j - \theta_j) \big] \bigg| \bigg)^2} \leq \frac{16}{m} \sup_j \EE[\mathbb{P}_\eta]{\Var[\eta_j]{(\mlbaseline_j - \mu_j)(\ptppi_j - \theta_j)}},
$$
and so,
$$
    \EE[\mathbb{P}_\eta]{\frac{2}{m} \sup_{\omega \geq 0} \bigg| \sum_{j = 1}^m (1 - \omega_j) \big[\tilde{\gamma}_j - (\mlbaseline_j - \mu_j)(\ptppi_j - \theta_j) \big] \bigg|} \leq \left(\frac{16}{m} \sup_j \EE[\mathbb{P}_\eta]{\Var[\eta_j]{(\mlbaseline_j - \mu_j)(\ptppi_j - \theta_j)}}\right)^{1/2}.
$$
Again, the (fourth-)moment conditions from \cref{lemma:moment-results} suffice to ensure that 
$$
\sup_j \EE[\mathbb{P}_\eta]{\Var[\eta_j]{(\mlbaseline_j - \mu_j)(\ptppi_j - \theta_j)}} < \infty,
$$
and to establish control of
$$
\EE[\mathbb P_{\eta}]{\sup_{\omega \geq 0} \left| \Delta_2(\omega) \right|}.
$$

\paragraph{Step 4: Concluding the argument.}

Finally, based on Steps 1--3, we have that
\begin{align*}
    \EE[\mathbb{P}_\eta]{\sup_{\omega \geq 0 } \left| \text{CURE}(\boldsymbol{\hat \theta}^{\mathrm{PAS}}_\omega) - \ell_m(\boldsymbol{\hat \theta}^{\mathrm{PAS}}_\omega, \boldsymbol{\theta}) \right|} \leq \EE[\mathbb P_{\eta}]{\sup_{\omega \geq 0} \left| \Delta_1(\omega) \right|} + \EE[\mathbb P_{\eta}]{\sup_{\omega \geq 0} \left| \Delta_2(\omega) \right|},
\end{align*}
and both terms on the right hand side converge to zero by our preceding bounds and the moment assumptions in the statement of the theorem.
\qed

\subsection{Proof of \texorpdfstring{\cref{thm:optimal-omega}}{\ref{thm:optimal-omega}}}

We apply a standard argument used to prove consistency of M-estimators.

Let $\omega_*$ be the oracle choice of $\omega \geq 0$ that minimizes the Bayes risk $\mathcal{B}_m^{\mathbb{P}_\eta} (\boldsymbol{\hat \theta}^{\mathrm{PAS}}_{\omega})$.\footnote{To streamline the proof, we assume that the infimum is attained by a value $\omega_*$. If the infimum is not attained, the proof still goes through using approximate minimizers.} Notice that by definition of $\hat{\omega}$ as the minimizer of CURE,
$$
\text{CURE}(\boldsymbol{\hat \theta}^{\mathrm{PAS}}_{\hat \omega}) \leq \text{CURE}(\boldsymbol{\hat \theta}^{\mathrm{PAS}}_{\omega_*}).
$$
Then:
$$
\ell_m(\boldsymbol{\hat \theta}^{\mathrm{PAS}}_{\hat{\omega}}, \boldsymbol{\theta}) - \ell_m(\boldsymbol{\hat \theta}^{\mathrm{PAS}}_{\omega_*}, \boldsymbol{\theta}) \leq 2\  \sup_{\omega \geq 0 } \left| \text{CURE}(\boldsymbol{\hat \theta}^{\mathrm{PAS}}_\omega) - \ell_m(\boldsymbol{\hat \theta}^{\mathrm{PAS}}_\omega, \boldsymbol{\theta}) \right|. 
$$
Taking expectations, 
$$\mathcal{B}_m^{\mathbb{P}_\eta} (\boldsymbol{\hat \theta}^{\mathrm{PAS}}_{\hat\omega}) - \mathcal{B}_m^{\mathbb{P}_\eta} (\boldsymbol{\hat \theta}^{\mathrm{PAS}}_{\omega_*}) \leq 2\  \EE[\mathbb P_{\eta}]{\sup_{\omega \geq 0 } \left| \text{CURE}(\boldsymbol{\hat \theta}^{\mathrm{PAS}}_\omega) - \ell_m(\boldsymbol{\hat \theta}^{\mathrm{PAS}}_\omega, \boldsymbol{\theta}) \right|}.  
$$
Noting that the right hand side converges to $0$ as $m \to \infty$, and recalling the definition of $\omega_*$,  we prove the desired result
\begin{align*}
    \mathcal{B}_m^{\mathbb{P}_\eta} (\boldsymbol{\hat \theta}^{\mathrm{PAS}}_{\hat\omega}) \leq \inf_{\omega \geq 0}\mathcal{B}_m^{\mathbb{P}_\eta} (\boldsymbol{\hat \theta}^{\mathrm{PAS}}_{\omega})  + o(1).
\end{align*} \qed

\subsection{Proof of \texorpdfstring{\cref{prop:double-reduction-pas}}{\ref{prop:double-reduction-pas}}}
We start with the result in~\cref{thm:optimal-omega}
\[
\mathcal{B}_m^{\mathbb{P}_\eta} (\boldsymbol{\hat \theta}^{\mathrm{PAS}}_{\hat\omega}, \boldsymbol{\theta}) \leq \inf_{\omega \geq 0} \ \mathcal{B}_m^{\mathbb{P}_\eta} (\boldsymbol{\hat \theta}^{\mathrm{PAS}}_{\omega}, \boldsymbol{\theta}) + o(1).
\]
Now, since we are integrating over $\eta \sim \mathbb{P}_\eta$ for all problems
\begin{align*}
    \mathcal{B}_m^{\mathbb{P}_\eta} (\boldsymbol{\hat \theta}^{\mathrm{PAS}}_{\omega}, \boldsymbol{\theta}) &= \frac{1}{m} \sum_{j=1}^m \EE[\mathbb{P}_\eta]{\big(\hat \theta^{\mathrm{PAS}}_{j, \,\omega} - \theta_j \big)^2} \\
    &= \EE[\mathbb{P}_\eta]{\big(\hat \theta^{\mathrm{PAS}}_{j, \,\omega} - \theta_{j} \big)^2},
\end{align*}
by definition $\,\hat \theta^{\mathrm{PAS}}_{j, \,\omega} = \omega_{j} \hat \theta^{\mathrm{PT}}_{j} + (1 - \omega_{j}) \tilde{Z}_{j}^f$, where $\omega_{j} = \omega/(\omega + \tilde{\sigma}^2)$. Therefore
\begin{align*}
    \EE[\mathbb{P}_\eta]{\big(\hat \theta^{\mathrm{PAS}}_{j, \,\omega} - \theta_{j} \big)^2} &= \EE[\mathbb{P}_\eta]{\big(\omega_{j} \big(\hat \theta^{\mathrm{PT}}_{j} - \theta_{j}\big) + (1 - \omega_{j})\big(\tilde{Z}_{j}^f - \theta_{j}\big)\big)^2} \\
    &= \frac{\omega^2}{(\omega + \tilde{\sigma}^2)^2} \EE[\mathbb{P}_\eta]{\big(\hat \theta^{\mathrm{PT}}_{j} - \theta_{j}\big)^2} + \frac{\tilde{\sigma}^4}{(\omega + \tilde{\sigma}^2)^2} \EE[\mathbb{P}_\eta]{\big(\tilde{Z}_{j}^f - \theta_{j}\big)^2} \\
    &+ 2\frac{\tilde{\sigma}^2 \omega}{(\omega + \tilde{\sigma}^2)^2}\EE[\mathbb{P}_\eta]{\big(\hat \theta^{\mathrm{PT}}_{j} - \theta_{j}\big)\big(\tilde{Z}_{j}^f - \theta_{j}\big)}.
\end{align*} 
By our assumption, second moment terms like $\tilde{\sigma}^2$ and $\tilde{\gamma}$ are now fixed, so we have (by iterated expectation)
$$
\EE[\mathbb{P}_\eta]{\big(\hat \theta^{\mathrm{PT}}_{j} - \theta_{j}\big)^2} = \tilde{\sigma}^2.
$$
Noting that $\tilde{\gamma}_j = 0$ since $N_j = \infty$,  we have 
$$
 \EE[\mathbb{P}_\eta]{\big(\hat \theta^{\mathrm{PAS}}_{j, \,\omega} - \theta_{j} \big)^2} = \frac{\omega^2 \tilde{\sigma}^2}{{(\omega + \tilde{\sigma}^2)^2}} + \frac{\tilde{\sigma}^4}{(\omega + \tilde{\sigma}^2)^2} \EE[\mathbb{P}_\eta]{\big(\tilde{Z}_{j}^f - \theta_{j}\big)^2}.
$$
Plugging in $\omega = \EE[\mathbb{P}_\eta]{\big(\tilde{Z}_{j}^f - \theta_{j}\big)^2}$ gives
$$
\frac{\omega^2 \tilde{\sigma}^2}{{(\omega + \tilde{\sigma}^2)^2}} + \frac{\tilde{\sigma}^4}{(\omega + \tilde{\sigma}^2)^2} \EE[\mathbb{P}_\eta]{\big(\tilde{Z}_{j}^f - \theta_{j}\big)^2}
=
\frac{\tilde{\sigma}^2 \mathbb{E}_{\mathbb{P}_{\eta}}\big[(\tilde{Z}_{j}^f - \theta_{j})^2\big]}{\tilde{\sigma}^2 + \mathbb{E}_{\mathbb{P}_{\eta}}\big[(\tilde{Z}_{j}^f - \theta_{j})^2\big]} .
$$
 We finally have 
$$
\mathcal{B}^{\mathbb{P}_\eta}_m(\boldsymbol{\hat \theta}^{\mathrm{PAS}}) \leq  \frac{\tilde{\sigma}^2 \mathbb{E}_{\mathbb{P}_{\eta}}\big[(\tilde{Z}_{j}^f - \theta_{j})^2\big]}{\tilde{\sigma}^2 + \mathbb{E}_{\mathbb{P}_{\eta}}\big[(\tilde{Z}_{j}^f - \theta_{j})^2\big]} + o(1).
$$ \qed




\section{Other Baseline Shrinkage Estimators}
\label{appendix:baseline-shrinkage}

\subsection{``Shrink-classical'' (Shrinkage) Baseline}

The ``shrink-classical'' estimator applies shrinkage directly to the classical estimator $\bar{Y}_j$, using the prediction mean $\mlbaseline_j$ as a shrinkage target \textit{without} first applying power-tuned PPI. We include this baseline to isolate the benefits of power tuning from the \texttt{PAS} estimator as an ablation study.

\paragraph{Formulation.} The ``shrink-classical'' estimator for problem $j$ takes the form:
\begin{align*}
    \hat{\theta}_{j, \omega}^{\text{Shrink}} &:= \omega_j \bar{Y}_j + (1 - \omega_j) \mlbaseline_j, \\
    \text{where } \quad \omega_j &:= \omega / (\omega + \tilde{\sigma}^2_j), \quad \tilde{\sigma}^2_j := \text{Var}_{\eta_j}[\bar{Y}_j] = \sigma_j^2/n_j.
\end{align*}
Here $\omega \geq 0$ is a global shrinkage parameter analogous to Section~\ref{subsec:adaptive-shrinkage}. The key difference from \texttt{PAS} is that we shrink the classical estimator $\bar{Y}_j$ (which is independent of $\mlbaseline_j$) rather than the power-tuned estimator \smash{$\hat{\theta}_j^{\mathrm{PT}}$} (which is correlated with \smash{$\mlbaseline_j$}).

\paragraph{Optimizing $\omega$ via CURE.} Since \smash{$\bar{Y}_j$} and \smash{$\mlbaseline_j$} are independent, Theorem~\ref{thm:gsure-PAS} simplifies. Let \smash{$\tilde{\gamma}_j = \CovInline{\bar{Y}_j, \mlbaseline_j} = 0$} and \smash{$\tilde{\sigma}^2_j = \sigma_j^2/n_j$}. CURE simplifies as:
\begin{align*}
    \mathrm{CURE}(\hat{\theta}_{j, \omega}^{\text{Shrink}}) &= (2\omega_j - 1)\tilde{\sigma}^2_j + \left[(1 - \omega_j)(\bar{Y}_j - \mlbaseline_j)\right]^2.
\end{align*}
This follows from Theorem~\ref{thm:gsure-PAS} by setting $\tilde{\gamma}_j = 0$. The global shrinkage parameter $\omega$ is selected by minimizing CURE across all $m$ problems. 
\begin{align}
    \label{eq:shrinkage-only-cure}
    \hat{\theta}_{j}^{\text{Shrink}} := \hat{\omega}_j \bar{Y}_j + (1 - \hat{\omega}_j) \mlbaseline_j, \quad \hat{\omega}_j = \hat{\omega} / (\hat{\omega} + \tilde{\sigma}^2_j), \nonumber\\ 
    \text{where }\quad \hat{\omega} \in \argmin_{\omega \geq 0} \frac{1}{m} \sum_{j=1}^m \mathrm{CURE}(\hat{\theta}_{j, \omega}^{\text{Shrink}}).
\end{align}
The optimal $\hat{\omega}$ does not admit a closed-form expression, but we can compute it numerically by grid search. Below we provide the pseudo-code for implementing the ``shrink-classical'' estimator.
\begin{algorithm}[H]
\caption{``Shrink-classical'' Estimator}
\label{alg:shrinkonly}
\small
\begin{algorithmic}[1]
\REQUIRE \(\{(X_{ij}, Y_{ij})_{i=1}^{n_j}\}, \{\tilde{X}_{ij}\}_{i=1}^{N_j}\) for \(j \in [m]\), variance parameters \(\{\sigma_j^2\}_{j=1}^m\), predictive model \(f\)
\FOR{$j = 1$ to $m$}
    \STATE \(\bar{Y}_j, \mlbaseline_j = \tbf{get\_means}((X_{ij}, Y_{ij})_{i=1}^{n_j}, (\tilde{X}_{ij})_{i=1}^{N_j}, f)\)
    \STATE \(\tilde{\sigma}_j^2 \leftarrow \sigma_j^2 / n_j\) \quad \COMMENT{variance of \(\bar{Y}_j\)}
\ENDFOR

\STATE \(\hat{\omega} = \tbf{get\_shrink\_param}(\{\bar{Y}_j\}_{j=1}^m, \{\mlbaseline_j\}_{j=1}^m, \{\tilde{\sigma}_j^2\}_{j=1}^m)\) 
\COMMENT{use Eq.~\eqref{eq:shrinkage-only-cure}}

\FOR{$j = 1$ to $m$}
    \STATE \(\hat{\omega}_j \;=\;\hat{\omega}\Big/\bigl(\hat{\omega} + \tilde{\sigma}_j^2\bigr)\)
    \STATE \(\hat{\theta}_j^{\text{Shrink}} = \hat{\omega}_j\,\bar{Y}_j \;+\; (1 - \hat{\omega}_j)\,\mlbaseline_j\)
\ENDFOR
\RETURN \( \:\{\hat{\theta}_j^{\text{Shrink}}\}_{j=1}^m\)
\end{algorithmic}
\end{algorithm}

\subsection{``Shrink-average'' Baseline}

The ``shrink-average'' estimator represents an alternative, perhaps more classical, shrinkage approach that attempts to further improve upon the unbiased PT estimators. While \texttt{PAS} reuses the prediction means on unlabeled data as shrinkage targets, here we consider shrinking the PT estimators across all problems to a shared location, namely their group mean
$$
\bar{\theta}^{\mathrm{PT}} := \frac{1}{m}\sum_{j=1}^m \ptppi_j.
$$
\paragraph{Formulation.} The ``shrink-average'' estimator for problem $j$ takes the form:
\begin{align*}
    \hat{\theta}_{j, \omega}^{\text{Avg}} &:= \omega_j \ptppi_j + (1 - \omega_j) \bar{\theta}^{\mathrm{PT}}, \\
    \text{where } \quad \omega_j &:= \omega / (\omega + \tilde{\sigma}^2_j), \quad \tilde{\sigma}^2_j := \VarInline[\eta_j]{\ptppi_j}.
\end{align*}


\paragraph{Optimizing $\omega$ via SURE.} \citet{xie2012sure} proposed the following unbiased risk estimate to optimize $\omega$ for this estimator. Note that even though the group mean is also correlated with each PT estimator, we still denote the following SURE instead of CURE following the nomenclature in \citet{xie2012sure}.
\begin{align}
    \hat{\omega} &\in \argmin_{\omega \geq 0} \frac{1}{m} \sum_{j=1}^m \mathrm{SURE}(\hat{\theta}_{j, \omega}^{\text{Shrink}}) \label{eq:shrinkage-group-mean}\\
    \mathrm{SURE}(\hat{\theta}_{j, \omega}^{\text{Shrink}}) &:= \left[(1 - \omega_j) (\ptppi_j - \bar{\theta}^{\mathrm{PT}})\right]^2 + (1-\omega_j)(\omega + (2/m - 1) \tilde{\sigma}_j^2). \nonumber
\end{align}
\begin{algorithm}[t]
\caption{``Shrink-average'' Estimator}
\label{alg:shrink-mean}
\small
\begin{algorithmic}[1]
\REQUIRE $(X_{ij}, Y_{ij})_{i=1}^{n_j}$, $(\tilde{X}_{ij})_{i=1}^{N_j}$, $\rho_j, \tau_j, \sigma_j$ for $j \in [m]$, predictive model $f$
\FOR{$j = 1$ to $m$}
    \STATE \COMMENT{Step 1: Apply predictor (Eq.~\eqref{eq:aggregated-stats})}
    \STATE \( \bar{Y}_j, \bar{Z}_j^f, \mlbaseline_j = \tbf{get\_means}((X_{ij}, Y_{ij})_{i=1}^{n_j}, (\tilde{X}_{ij})_{i=1}^{N_j}, f) \)
    \STATE \COMMENT{Step 2: Power tuning (Eq.~\eqref{eq:power-tuning-lambda})}
    \STATE \( \lambda_j^* = \tbf{get\_pt\_param}(\rho_j, \tau_j, n_j, N_j) \)
    \STATE \( \hat{\theta}_j^{\mathrm{PT}} = \bar{Y}_j + \lambda_j^* (\mlbaseline_j - \bar{Z}_j^f) \)
    \STATE \( \tilde{\sigma}_j^2 = \tbf{get\_pt\_var}(\hat{\theta}_j^{\mathrm{PT}})\) \COMMENT{(Eq. \eqref{eq:get_pt_var})}
\ENDFOR
\STATE \( \bar{\theta}^{\mathrm{PT}} = m^{-1}\sum_{j=1}^m \ptppi_j \)
\STATE \COMMENT{Step 3: Adaptive shrinkage towards group mean (Eq. \eqref{eq:shrinkage-group-mean})}
\STATE \( \hat{\omega} = \tbf{get\_shrink\_param}(\{\hat{\theta}_j^{\mathrm{PT}}\}_{j=1}^m, \bar{\theta}^{\mathrm{PT}}, \{  \tilde{\sigma}_j^2 \}_{j=1}^m \) )
\FOR{$j = 1$ to $m$}
    \STATE \( \hat \omega_j = \hat{\omega} / (\hat{\omega} + \tilde{\sigma}_j^2) \)
    \STATE \( \hat{\theta}_j^{\text{Avg}} = \hat \omega_j \hat{\theta}_j^{\mathrm{PT}} + (1 - \hat \omega_j) \bar{\theta}^{\mathrm{PT}} \)
\ENDFOR
\RETURN \( \:\{\hat{\theta}_j^{\textnormal{Avg}}\}_{j=1}^m \)
\end{algorithmic}
\end{algorithm}

\section{Experiment Details}
\label{appendix:experiments}

\subsection{Sample-based Estimators for \texorpdfstring{$\sigma_j^2, \tau_j^2, \gamma_j$}{second moments}}
\label{para:sample-estimator}
In \cref{ass:exchangeable_model}, we assumed that the second-moment parameters $\sigma_j^2, \tau_j^2, \gamma_j$ at the sampling level are known in order to motivate our method and streamline theoretical results. In practice, when the true data generating process is unknown or when we only have access to a black-box predictor, we plug in the following sample-based (unbiased) estimators for the parameters:
\begin{align*}
    \hat{\sigma}_j^2 &:= \frac{1}{n_j-1}\sum_{i=1}^{n_j}(Y_{ij} - \bar{Y}_j)^2, \quad \hat{\tau}_j^2 := \frac{1}{n_j + N_j -1}\bigg(\sum_{i=1}^{n_j}(f(X_{ij}) - \hat{Z}_j)^2 + \sum_{i=1}^{N_j}(f(\tilde X_{ij}) - \hat{Z}_j)^2\bigg), \\
    \hat \gamma_j &:= \frac{1}{n_j-1}\sum_{i=1}^{n_j}(Y_{ij} - \bar{Y}_j)(f(X_{ij}) - \bar{Z}^{N+n}_j), \quad \text{where} \quad \bar{Z}^{N+n}_j := \frac{1}{n_j + N_j} \bigg(\sum_{i=1}^{n_j} f(X_{ij}) + \sum_{i=1}^{N_j} f(\tilde X_{ij})\bigg).
\end{align*}
These sample-based estimators are used in our numerical experiments involving real-world datasets.

\subsection{Synthetic Model}
\label{appendix:synthetic-dataset-details}
\paragraph{Motivation.} 
In \cref{ex:synthetic}, we described the following data generation process (copied from Eq.~\eqref{eq:synthetic-likelihood})
\begin{align*}
    \eta_j &\sim \mathcal{U}[-1, 1], \quad j = 1, \ldots, m, \\
    X_{ij} &\sim \mathcal{N}(\eta_j, \psi^2),  \quad Y_{ij} | X_{ij} \sim \mathcal{N}(2\eta_j X_{ij} - \eta_j^2, c), \quad i = 1, \ldots, n_j,
\end{align*}
and the same for $(\tilde X_{ij}, \tilde Y_{ij})$. $\psi$ and $c$ are two hyperparameters that we chose to be 0.1 and 0.05, respectively. The (marginal) mean and variance of $Y_{ij}$ are
\begin{align*}
    \theta_j := \EE[\eta_j]{Y_{ij}} &= \eta_j^2, \quad \sigma_j^2 := \Var[\eta_j]{Y_{ij}} = 4\eta_j^2 \psi^2 + c.
\end{align*}
To understand the motivation behind this setup, we can further inspect the covariance between $X_{ij}$ and $Y_{ij}$, which can be verified to be $\Cov[\eta_j]{X_{ij}, Y_{ij}} = 2\eta_j \psi^2$. Therefore, if we consider the ratio between the absolute covariance and the variance (of $Y_{ij}$) as a characterization of the ``inherent predictability'' of a problem, we see that
\begin{align*}
    \frac{|\Cov[\eta_j]{X_{ij}, Y_{ij}}|}{\Var[\eta_j]{Y_{ij}}} = \frac{2|\eta_j| \psi^2}{4\eta_j^2 \psi^2 + c}
\end{align*}
which has its minimum when $\eta_j = 0$ and increases monotonically in $|\eta_j|$ for $|\eta_j| \in [0,1]$ given our specific choices of $\psi$ and $c$ (see \cref{fig:ratio}). In other words, problems with $\eta_j$ close to the origin have a lower ``predictability;'' whereas when $\eta_j$ moves away from zero, the problems become easier to solve. This quantitatively reflects the pattern we see in \cref{fig:synthetic-params}, where we display the power-tuning parameters as a function of $\eta_j$.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.6\columnwidth]{images/ratio.pdf}
    \caption{The ratio between $|\Cov[\eta_j]{X_{ij}, Y_{ij}}|$ and $\Var[\eta_j]{Y_{ij}}$ as a function of $\eta_j$. The constants are set to $\psi = 0.1$ and $c = 0.05$.}
    \vspace{-3mm}
    \label{fig:ratio}
\end{figure}

\paragraph{Expressions for $\theta_j, \mu_j, \sigma_j^2, \tau_j^2, \gamma_j$ when $f(x) = |x|$.}

When we work with the synthetic model using the flawed predictor $f(x) = |x|$, we can match the form of our dataset with the general setting in \cref{ass:compound_generic} by identifying closed-form expressions for the model parameters $\theta_j, \mu_j, \sigma_j^2, \tau_j^2, \gamma_j$.
\begin{align*}
    \theta_j &= \eta_j^2, \quad \sigma_j^2 = 4\eta_j^2 \psi^2 + c, \\
    \gamma_j &=  2\eta_j\psi^2\sqrt{\frac{2}{\pi}}e^{-\eta_j^2/(2\psi^2)}, \quad 
    \mu_j = \sqrt{\frac{2}{\pi}} \psi \exp\left(-\frac{\eta_j}{2\psi^2}\right) + \eta_j \left[\Phi\left(\frac{\eta_j}{\psi}\right) - \frac{1}{2}\right],
    \\
    \tau_j^2 &= \eta_j^2 + \psi^2 - \left[\sqrt{\frac{2\psi^2}{\pi}}\exp\bigg(-\frac{\eta_j^2}{2\psi^2}\bigg) + \eta_j\left( 2\Phi\left(\tfrac{\eta_j}{\psi}\right) - 1\right)\right]^2,
\end{align*}
where $\Phi(\cdot)$ denotes the standard normal distribution function.

\paragraph{Expressions for $\theta_j, \mu_j, \sigma_j^2, \tau_j^2, \gamma_j$ when $f(x) = x^2$.} Similar closed-form expressions can be derived when we use the other predictor $f(x) = x^2$. Note that $\theta_j$ and $\sigma_j^2$ remain the same.
\begin{align*}
    \theta_j &= \eta_j^2, \quad \sigma_j^2 = 4\eta_j^2 \psi^2 + c, \\
    \gamma_j &=  4\eta_j^2 \psi^2, \quad 
    \mu_j = \eta_j^2 +\psi^2, \quad \tau_j^2 = 2\psi^4 + 4\eta_j^2 \psi^2.
\end{align*}
In experiments involving the synthetic model with both predictors, we are able to leverage these closed-form expressions and supplement the ground-truth parameters to our datasets.

\paragraph{Interpretation of MSE.} In the synthetic experiments, since we have access to the true prior for $\eta_j$ (therefore for $\theta_j$) and resample them for each problem across $K$ trials, the MSE we obtained in \cref{table:synthetic} is an unbiased estimate of the \textit{Bayes Risk} defined in Eq.~\eqref{eq:bayes-risk}.



\subsection{Amazon Review Ratings Dataset}
\label{appendix:amazon-dataset-details} 

\paragraph{Dataset \& Preprocessing.} The \textit{Amazon Fine Food Reviews} dataset, provided by the Stanford Network Analysis Project (SNAP; \citet{amazon_fine_food_reviews}) on Kaggle,\footnote{https://www.kaggle.com/datasets/snap/amazon-fine-food-reviews} comes in a clean format. We group reviews by their \texttt{ProductID}. For each review, we concatenate the title and body text to form the covariate, while the response is the reviewer's score/rating (1 to 5 stars). Here's a sample review:

\vspace{2mm}
\noindent\fbox{%
\parbox{\textwidth}{%
{\textbf{Score:} 4 \hfill \textbf{Product:} BBQ Pop Chips } \\
\textbf{Title:} Delicious! \\
\textbf{Text:} BBQ Pop Chips are a delicious tasting healthier chip than many on the market. They are light and full of flavor. The 3 oz bags are a great size to have. I would recommend them to anyone.
}%
}
\vspace{2mm}

We focus on the top $m = 200$ products with the most reviews for the compound mean estimation of average ratings. This approach mitigates extreme heteroscedasticity across estimators for different problems, which could unduly favor shrinkage-based methods when considering unweighted compound risk. There are a total of 74,913 reviews for all 200 products.

\paragraph{Fine-tuning \texttt{BERT}.}
The Bidirectional Encoder Representations from Transformers (\texttt{BERT}) model is a widely adopted language model for many NLP tasks including text classification \citep{devlin2018bert}. However, pretraining \texttt{BERT} from scratch is time-consuming and requires large amounts of data. We therefore use the \texttt{bert-base-multilingual-uncased-sentiment} model\footnote{https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment} from \citet{nlp_town_2023} as the base model, denoted as \texttt{BERT-base}. \texttt{BERT-base} is pre-trained on general product reviews (not exclusive to Amazon) in six languages. It achieves 67.5\% prediction accuracy on a validation set of 100 products ($\sim$46k reviews).

Then, we further fine-tune it on the held-out review data, that is, reviews outside the top 200 products, for 2 full epochs. The fine-tuning is done using Hugging Face's \texttt{transformers} library \citep{wolf2019huggingface}. After fine-tuning, the \texttt{BERT-tuned} model achieves 78.8\% accuracy on the same validation set.

\subsection{Spiral Galaxy Fractions (Galaxy Zoo 2)}
\label{appendix:galaxy-dataset-details}

\paragraph{Dataset \& Preprocessing.} The Galaxy Zoo 2 (GZ2) project\footnote{https://data.galaxyzoo.org/} contains a large collection of human-annotated classification results for galaxy images from SDSS. However, instead of having a single dataframe, GZ2 has many different tables---each for subsets of the SDSS raw data. We begin with a particular subset of 239,696 images with metadata drawn from \citet{hart_galaxy_2016}. Our data cleaning pipeline is inspired by \citet{lin2021galaxy}, which removes missing data and relabels the class name of each galaxy image to a more readable format:

\vspace{2mm}
\noindent \fbox{\parbox{\textwidth}{\textbf{Class Names:}
Round Elliptical, In-between Elliptical, Cigar-shaped Elliptical, Edge-on Spiral, Barred Spiral, Unbarred Spiral, Irregular, Merger}}
\vspace{2mm}

In the downstream estimation problems, we consider a galaxy ``spiral'' if it is classified as one of the three classes ending with ``Spiral'', otherwise ``non-spiral''. Below we display a few examples of galaxy images. Each image has dimensions of $424 \times 424 \times 3$, where the third dimension represents the three filter channels: g (green), r (red), and i (infrared). The cleaned dataset has 155,951 images in total.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.6\textwidth]{images/galaxy_examples.pdf}
    \caption{Example of spiral \& non-spiral galaxy images from Galaxy Zoo 2.}
    \label{fig:galaxyzoo2-images}
\end{figure}

The additional SDSS metadata for GZ2\footnote{The column names and their meanings are available at \url{https://data.galaxyzoo.org/data/gz2/gz2sample.txt}.} contains valuable information that directly partitions the galaxies based on certain attributes, 
e.g., \texttt{REDSHIFT\_SIMPLE\_BIN} based on galaxy redshift measurements, and \texttt{WVT\_BIN} calculated by weighted Voronoi tessellation. These partitions naturally motivate fine-grained compound mean estimation on this dataset.

After partitioning the images based on \texttt{REDSHIFT\_SIMPLE\_BIN},\footnote{In astronomy, \texttt{REDSHIFT} quantifies how much the wavelength of light from a galaxy is stretched due to the expansion of the universe. It serves as a good proxy for grouping galaxies of similar distance and time.} we consider only the top $m = 122$ partitions based on the cutoff that each problem should have $\geq 150$ images (many partitions have very few galaxy images in them), for the same reason as in the Amazon Review dataset. Finally, we have a total of $\sim$100k images as covariates (either $X_{ij}$ or $\tilde{X}_{ij}$) for our problem.

\paragraph{Training the Predictor.}
We employ the \texttt{ResNet50} architecture \citep{he2016deep}, utilizing the pre-trained model from \texttt{torchvision} initially trained on ImageNet \citep{deng2009imagenet}. To tailor the model to our task, we fine-tune it on $\sim$50k images excluded from the top $m$ problems. The model is trained to classify galaxies into eight categories, later condensed into a binary spiral/non-spiral classification for prediction. We use a batch size of 256 and Adam optimizer \citep{kingma2014adam} with a learning rate of 1e-3. After 20 epochs, the model achieves 87\% training accuracy and 83\% test accuracy. Despite these promising results, \cref{table:realworld} indicates that the predictions still require de-biasing for accurate estimation.

\subsection{Benchmarking in real-world datasets}
In this appendix we describe the steps to obtain the MSEs and their standard errors for real-world datasets shown in \cref{table:realworld}.

Let $K$ be the number of experiment trials, $T_j$ be the total number of data points for problem $j$, i.e. $\{\dot X_{ij}, \dot Y_{ij}\}_{j=1}^{T_j}$ represents the ``raw data'' we have, and $n_j, N_j$ be the desired number of labeled/unlabeled data to simulate, usually calculated through a hyper-parameter splitting ratio (e.g. $N_j = \lfloor r \cdot T_j \rfloor, \:n_j = T_j - N_j$ for $r = 0.8$ in our case).
\begin{enumerate}
    \item Following evaluation methodology in existing PPI literature, e.g.,~\citep{angelopoulos2023prediction}, we first calculate the mean of all responses for each problem and treat it as the pseudo ground-truth, i.e., $\dot{\theta}_j := \frac{1}{T_j} \sum_i \dot Y_{ij}$.
    \item For each trial $k \in [K]$, we create a random permutation for the raw data, with indices permuted by $\kappa: \mathbb{N} \to \mathbb{N}$, and obtain the labeled and unlabeled datasets for problem $j$ as
    \[
    \{X_{ij}, Y_{ij}\}_{i=1}^{n_j} = \{\dot X_{\kappa(i)j}, \dot Y_{\kappa(i)j}\}_{i=1}^{n_j}, \quad \{\tilde{X}_{ij}\}_{i=1}^{N_j} = \{\dot X_{\kappa(i)j}\}_{i=n_j + 1}^{T_j}
    \]
    \item We proceed with using these datasets to obtain the baseline and \texttt{PAS} estimators. Let $\hat{\theta}_j^k$ be an estimator for the $j$-th problem at trial $k$, then our final reported MSE and standard error is calculated as
    \begin{align*}
    \widehat{\text{MSE}}_K(\boldsymbol{\hat \theta}) &:= \frac{1}{K}\sum_{k=1}^K \left(\frac{1}{m} \sum_{j=1}^m (\hat{\theta}_j^k - \dot{\theta}_j)^2\right), \\
    \text{SE}_K(\boldsymbol{\hat \theta}) &:= \frac{1}{\sqrt{K}} \sqrt{\frac{1}{K-1}\sum_{k=1}^K \left(\frac{1}{m} \sum_{j=1}^m (\hat{\theta}_j^k - \dot{\theta}_j)^2- \widehat{\text{MSE}}_K(\boldsymbol{\hat \theta})\right)}.
    \end{align*}
\end{enumerate}
It should be noted that the standard error only accounts for uncertainty due to the random splits into labeled and unlabeled datasets.


\subsection{Computational Resources}
\label{appendix:computational-resources}
All the experiments were conducted on a compute cluster with Intel Xeon Silver 4514Y (16 cores) CPU, Nvidia A100 (80GB) GPU, and 64GB of memory. Fine-tuning the \texttt{BERT-tuned} model took 2 hours, and training the \texttt{ResNet50} model took 1 hour. All the inferences (predictions) can be done within 10 minutes. The nature of our research problem requires running the prediction only once per dataset, making it fast to benchmark all estimators for $K = 200$ trials using existing predictions.

\subsection{Code Availability}
The code for reproducing the experiments is available at \\ 
\url{https://github.com/listar2000/prediction-powered-adaptive-shrinkage}.

\end{document}