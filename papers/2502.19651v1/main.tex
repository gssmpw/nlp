%%
%% This is file `sample-sigconf.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `all,proceedings,bibtex,sigconf')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-sigconf.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%% The first command in your LaTeX source must be the \documentclass
%% command.
%%
%% For submission and review of your manuscript please change the
%% command to \documentclass[manuscript, screen, review]{acmart}.
%%
%% When submitting camera ready or to TAPS, please change the command
%% to \documentclass[sigconf]{acmart} or whichever template is required
%% for your publication.
%%
%%
\documentclass[sigconf]{acmart}


\usepackage{graphicx} % 用于插入图片
\usepackage{subcaption} % 用于创建子图
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{tikz}

% \let\Bbbk\relax
\usepackage{algorithmic}
\usepackage{textcomp}
% \usepackage{hyperref}
% \hypersetup{hidelinks}
\usepackage{xcolor}
\usepackage{bm}
\usepackage{makecell}
\usepackage{tabu}
\usepackage[ruled,boxed,linesnumbered]{algorithm2e}
%\usepackage{subfig}
\usepackage{multirow}
\usepackage{enumitem}
% \usepackage{pgfplots}
% \usepackage{pifont}
\usepackage{url}
\usepackage{colortbl}
\usepackage{multicol}
\usepackage{xspace}
\usepackage{bbding}
\usepackage{pifont}
\usepackage{tabularx}
\usepackage{array}    % 引入array包以支持新的列定义
\usepackage{hyperref} 
\usepackage{tcolorbox}
% \usepackage{stmaryrd}
% \usepackage{hyperref}
% \hypersetup{hidelinks}

% 定义一个新的居中的X列类型
\newcolumntype{Y}{>{\centering\arraybackslash}X}
\newtheorem{theorem}{{Theorem}}
\newtheorem{Theorem}{{Theorem}}
%\theoremstyle{definition}
\newtheorem{definition}{{Definition}}
\newtheorem{example}{{Example}}
\newtheorem{problem}{{Problem}}
\newcommand{\leftdoublebrace}{\{\hspace{-.35em}\{}
\newcommand{\rightdoublebrace}{\}\hspace{-.35em}\}}
%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmlicensed}
\copyrightyear{2025}
\acmYear{2025}
\acmDOI{XXXXXXX.XXXXXXX}
%% These commands are for a PROCEEDINGS abstract or paper.
\acmConference[KDD]{31st SIGKDD Conference on Knowledge Discovery and Data Mining}{August 03--05, 2025}{Canada}
%%
%%  Uncomment \acmBooktitle if the title of the proceedings is different
%%  from ``Proceedings of ...''!
%%
%%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%%  June 03--05, 2018, Woodstock, NY}
\acmISBN{978-1-4503-XXXX-X/18/06}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}


%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Unlocking Multi-Modal Potentials for Dynamic Text-Attributed Graph Representation}
% Unlocking Multi-Modal Potentials for Dynamic Text-Attributed Graph Representation
%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{Yuanyuan Xu}
\affiliation{%
  \institution{University of New South Wales}
  \city{Sydney}
  \country{Australia}
}
\email{yuanyuan.xu@unsw.edu.au}

\author{Wenjie Zhang}
\affiliation{%
  \institution{University of New South Wales}
  \city{Sydney}
  \country{Australia}
}
\email{wenjie.zhang@unsw.edu.au}

\author{Ying Zhang}
\affiliation{%
  \institution{Zhejiang Gongshang University}
  \city{Hangzhou}
  \country{China}
}
\email{ying.zhang@zjgsu.edu.cn}



\author{Xuemin Lin}
\affiliation{%
  \institution{Shanghai Jiao Tong University}
  \city{Shanghai}
  \country{China}
}
\email{xuemin.lin@gmail.com}


\author{Xiwei Xu}
\affiliation{%
  \institution{CSIRO Data61}
  \city{Eveleigh}
  \country{Australia}
}
\email{xiwei.xu@data61.csiro.au}


% \author{Julius P. Kumquat}
% \affiliation{%
%   \institution{The Kumquat Consortium}
%   \city{New York}
%   \country{USA}}
% \email{jpkumquat@consortium.net}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Trovato et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
Dynamic Text-Attributed Graphs (DyTAGs) are a novel graph paradigm that captures evolving temporal events (edges) alongside rich textual attributes. A prior approach to representing DyTAGs leverages pre-trained language models to encode text attributes and subsequently integrates them into dynamic graph models. However, it follows edge-centric modeling, as in dynamic graph learning, which is limited in local structures and fails to exploit the unique characteristics of DyTAGs, leading to suboptimal performance. We observe that DyTAGs inherently comprise three distinct modalities—temporal, textual, and structural—often exhibiting dispersed or even orthogonal distributions, with the first two largely overlooked in existing research. Building on this insight, we propose MoMent, a model-agnostic multi-modal framework that can seamlessly integrate with any dynamic graph model for structural modality learning. The core idea is to shift from edge-centric to node-centric modeling, fully leveraging all three modalities for node representation. Specifically, MoMent presents non-shared node-centric encoders based on the attention mechanism to capture global temporal and semantic contexts from temporal and textual modalities, together with local structure learning, thus generating modality-specific tokens. To prevent disjoint shared latent space, we propose a symmetric alignment loss, an auxiliary objective that aligns temporal and textual tokens, ensuring global temporal-semantic consistency with a theoretical guarantee. Last, we design a lightweight adaptor to fuse these tokens, generating comprehensive and cohesive node representations. We theoretically demonstrate that MoMent enhances discriminative power over exclusive edge-centric modeling. Extensive experiments across seven datasets and two downstream tasks show that MoMent achieves up to $33.62\%$ improvement against the baseline using four dynamic graph models.

\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10002951.10003227.10003351</concept_id>
       <concept_desc>Information systems~Data mining</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Information systems~Data mining}
% \ccsdesc[300]{Do Not Use This Code~Generate the Correct Terms for Your Paper}
% \ccsdesc{Do Not Use This Code~Generate the Correct Terms for Your Paper}
% \ccsdesc[100]{Do Not Use This Code~Generate the Correct Terms for Your Paper}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Dynamic text-attributed graphs, multi-modal modeling, graph representation learning}
%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans the
%% page.
% \begin{teaserfigure}
%   \includegraphics[width=\textwidth]{sampleteaser}
%   \caption{Seattle Mariners at Spring Training, 2010.}
%   \Description{Enjoying the baseball game from the third-base
%   seats. Ichiro Suzuki preparing to bat.}
%   \label{fig:teaser}
% \end{teaserfigure}

% \received{20 February 2007}
% \received[revised]{12 March 2009}
% \received[accepted]{5 June 2009}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}
Dynamic Text-Attributed Graphs (DyTAGs) are a recent development to represent systems with evolving structures and text descriptions over time continuously, where both nodes and edges are typically enriched with text descriptions. DyTAGs are critical for modeling complex real-world systems, such as e-commerce platforms~\cite{zhao2023time,kazemi2020representation,10.1145/3580305.3599341,10.1145/3637528.3671962}, social networks~\cite{pareja2020evolvegcn,sun2022ddgcn,luo2023hope,song2019session}, financial markets~\cite{you2022roland,qian2024mdgnn,gravina2024deep}, and knowledge graphs~\cite{li2021temporal,liang2024survey}. For instance, in social networks, nodes represent posts with text descriptions, and edges may be annotated with user comments, where nodes are capable of engaging in behaviors at any time. To represent DyTAGs and support real-world applicability, there is a pioneering exploration DTGB~\cite{zhang2024dtgb} that leverages pre-trained language models to encode text attributes and then integrates them with existing dynamic graph models~\cite{kumar2019predicting,trivedi2019dyrep,xu2020inductive,tgn,wang2021inductive,congwe,yu2023towards} for node representation, as existing dynamic graph models fail to handle raw textual information directly. However, this initial exploration focuses solely on local structure learning, which overlooks the unique characteristics of DyTAGs, thereby compromising model performance.

\begin{figure*}
    \centering
    % \vspace{-10pt}
    \subfloat[A dynamic text-attributed graph]{\includegraphics[width=0.46\linewidth]{figures/DTAG.pdf}\label{fig:1a}}
    % \hspace{5pt}
    \subfloat[KDE distribution of modalities]{
            \includegraphics[width=0.25\linewidth]{figures/Enron_KDE_Norm.pdf}
        \label{fig:1b}}
        \subfloat[Empirical observations]{
            \includegraphics[width=0.29\linewidth]{figures/Enron_intro.pdf}
        \label{fig:1c}}
    \caption{(1) A toy example of dynamic text-attributed graphs, together with its three modalities. (b) KDE distribution of three modalities in a real dynamic text-attributed graph (Enron), where `X.O' indicates the use of original features. (c) Results of three mainstream dynamic graph models and their variants over the Enron dataset on the inductive link prediction task.}
    \label{fig:1}
    % \vspace{-2pt}
\end{figure*}


\noindent\textbf{Observation for multi-model characteristics of DyTAGs.} In this paper, we first analyze the unique characteristics of DyTAGs, illustrated by the toy example in Fig.~\ref{fig:1a}, to establish a foundation for effective modeling. Taking a \textit{node-centric} perspective, we identify DyTAGs as inherently multi-modal, which distinguishes them from traditional dynamic graphs: (1) \underline{Structural Modality}: Similar to dynamic graphs, each node interacts with multiple neighbors through evolving edges, where \textit{edge text attributes and timestamps are inherently associated with edges}, encoding both structural dynamics and semantic information. (2) \underline{Temporal Modality}: Beyond structural changes, nodes initiate interactions at different timestamps, capturing temporal evolution patterns (\textit{e.g.}, periodicity) that preserve nodes' dynamic behavior. (3) \underline{Textual Modality}: Unlike dynamic graphs, nodes in DyTAGs are inherently enriched with raw text attributes, introducing an additional semantic dimension.



To better understand these three modalities, we analyze their Kernel Density Estimation (KDE) distributions on the Enron dataset, as shown in Fig.~\ref{fig:1b}, where the structural modality is derived from the $20$ neighbors of each node. It is clear that temporal modality has a dispersed distribution, reflecting periodicity and variability in event timings, whereas textual modality is compact and concentrated, indicating high semantic regularity. The structural modality exhibits a moderate distribution, capturing structural diversity and local variability in edge attributes. These characteristic and visualization observations highlight the multi-modal nature of DyTAGs, posing critical challenges on how to effectively use, model, and integrate these modalities for comprehensive and cohesive representations.








% \textbf{Why do we need to explore multi-modal learning for DyTAG representation?}
% \noindent\textbf{Question \uppercase\expandafter{\romannumeral1}: What are the individual contributions of three modalities?} 

\noindent\textbf{Challenge \uppercase\expandafter{\romannumeral1}: How to unlock the potential of each modality for comprehensive representation?} The existing DTGB framework~\cite{zhang2024dtgb} with dynamic graph models, adopts an \textit{edge-centric modeling}, where features, including timestamps, node attributes, and edge attributes, are fused at the input level, \textit{i.e.}, \textit{feature-level fusion}. These fused features are then utilized in local structure learning to generate node representations. However, such edge-centric modeling fails to comprehensively leverage temporal and textual modalities in DyTAGs, as these modalities remain constrained to localized interactions. To assess the significance of these underexplored modalities, we extend the DTGB framework~\cite{zhang2024dtgb} by incorporating node-centric encoders for temporal and textual modalities and evaluate their impact on the Enron dataset using the inductive link prediction task, where we select three mainstream dynamic graph models (TGAT~\cite{xu2020inductive}, GraphMixer~\cite{congwe}, and DyGFormer~\cite{yu2023towards}). The AUC results in Fig.~\ref{fig:1c} reveal that (1) Incorporating node-centric temporal learning improves performance by $4.26\%$ on average, compared to DTGB, which only utilizes timestamp information within local structure learning. This suggests the importance of capturing global temporal dependencies beyond localized interactions. (2) Node text attributes contribute to an $6.42\%$ performance improvement, highlighting their rich semantic context that complements structural representations. Therefore, unlocking each modality is key to developing a robust and effective DyTAG representation framework. 


% \textbf{how to unlock the potential of each modality for robust and effective DyTAG representation is the first challenge} (\textbf{Challenge \uppercase\expandafter{\romannumeral1}}).



% unlocking multi-modal potentials is key to developing a robust and effective DyTAG representation framework. 



% \noindent\textbf{Question \uppercase\expandafter{\romannumeral2}: Why do we align the multi-modal representations?} 
\noindent\textbf{Challenge \uppercase\expandafter{\romannumeral2}: How to align the multiple modalities for cohesive representation?} Fig.~\ref{fig:1b} shows that each modality in DyTAGs has distinct value ranges and densities, highlighting its unique characteristics. This observation also confirms that the three modalities are largely independent. Although individual modality modeling can unlock each modality's potential and achieve unique contributions, it easily suffers from a disjoint latent space when integrating multiple modalities. Without effective alignment for multi-modal modeling, this can lead to modality inconsistency, preventing the generation of a cohesive representation and thus compromising model performance. Therefore, it is crucial to develop an alignment mechanism that enforces modality consistency while preserving the unique contributions of each modality, yielding cohesive representations for DyTAGs.




To address these research questions, we propose a  \underline{\textbf{M}}odel-agn\underline{\textbf{o}}stic \underline{\textbf{M}}ulti-modal fram\underline{\textbf{e}}work for dy\underline{\textbf{n}}amic \underline{\textbf{t}}ext-attributed graph representation (called MoMent). The key idea behind MoMent is to fully leverage the multi-modal characteristics in DyTAGs and effectively align modalities with different distributions for better representation learning. Inspired by the density distribution of the three modalities in Fig.~\ref{fig:1b}, we introduce the decision-level fusion for node representation over DyTAGs. Concretely, we present two node-centric attention-based encoders to model the temporal and textual modalities, while leveraging existing dynamic graph models for local structure learning. With temporal and textual encoders, MoMent can capture global temporal dependencies and semantic contexts that complement local topology, thereby addressing \textbf{Challenge \uppercase\expandafter{\romannumeral1}}. To address \textbf{Challenge \uppercase\expandafter{\romannumeral2}} and prevent disjoint fusion, we propose a symmetric alignment loss based on Jensen-Shannon Divergence (JSD), an auxiliary objective that aligns temporal and textual modalities in a shared latent space, ensuring global temporal-semantic consistency with a theoretical guarantee. Then, we design a lightweight adaptor that fuses three tokens from each modality, generating comprehensive and cohesive node representations. We further theoretically demonstrate that MoMent improves discriminative power over exclusive edge-centric models, thus enhancing prediction accuracy in downstream tasks, as evidenced in Fig.~\ref{fig:1c}. In summary, MoMent can effectively model, align, and integrate inherent multiple modalities of DyTAGs, thus unlocking multi-modal potentials for better representation. 

% thereby answering Question~\textbf{Question \uppercase\expandafter{\romannumeral3}}. 

Our contributions can be summarized as

\begin{itemize}
    \item  We propose MoMent which explores multi-modal modeling for dynamic text-attributed graph representation for the first time. As a model-agnostic solution, MoMent seamlessly integrates with any dynamic graph backbone for comprehensive representation learning.
    % \textbf{Plug-and-play framework.}
    
    % We propose MoMent which explores the potentials of multi-modal modeling for dynamic text-attributed graph representation for the first time. MoMent is a model-agnostic framework that can be plugged into any dynamic graph learning backbone for holistic representation.

    \item MoMent integrates global temporal and semantic insights to complement local topology, ensuring a theoretically bounded improvement in discriminative power.
    % \textbf{Unlocking multi-modal potentials.}
    
    % We theoretically demonstrate that a measurable improvement in discriminative power when incorporating temporal and textual modalities.
    
    % propose two global encoders that capture global dependencies from the temporal and semantic perspectives, respectively, providing global guidance for local structure learning and improving the embedding quality.

    \item To ensure cohesive fusion, we propose a symmetric alignment loss to align temporal and textual modalities, minimizing global contextual disparity with a theoretical guarantee.

    % \textbf{Temporal-semantic consistency.}
    
   \item Extensive experiments demonstrate that MoMent can yield better prediction performance than the DTGB framework based on four mainstream dynamic graph models, with an average of $6.06\%$ improvement across two downstream tasks over seven datasets.

   % \textbf{Empirical effectiveness}.
\end{itemize}










% \noindent\textbf{Our contributions.}

\section{Related Work}
\subsection{Dynamic Text-Attributed Graph Learning}
The Dynamic Text-Attributed Graph (DyTAG) is a new concept, recording the evolving dynamic structures and text attributes simultaneously, which is critical for many real-world applications. Behind this new concept, there is a new and comprehensive benchmark (DTGB)~\cite{zhang2024dtgb} for DyTAG learning. Concretely, DTGB first introduced DyTAG datasets from multiple domains and standardized the evaluation process. Then, it presented an intuitive framework that learned raw text attributes of nodes and edges by pre-trained language models and then integrated them into existing dynamic graph backbones. DTGB showed its effectiveness across various downstream tasks compared to dynamic graph backbones without text attributes, suggesting the significance of textual information for node representation over DyTAGs. Although rich text attributes are employed to learn node representations of DyTAGs, DTGB, following the line of edge-centric modeling, focuses exclusively on local learning based on the dynamic structure, where timestamps and edge/node attributes are merely treated as an optional supplement for structure learning, leading to the underexploration of temporal and textual modalities.


The processing of timestamps and text attributes by the DTGB and our proposed MoMent can be abstracted as \textit{feature-level fusion} and \textit{decision-level fusion}. Feature-level fusion integrates modalities into a single vector at the input level and processes the fused features through edge-centric modeling, \textit{i.e.}, dynamic graph models. In contrast, decision-level fusion allows each modality to run independently through individual models and fuses the outputs of each modality at the output level. Compared to feature-level fusion, decision-level fusion maximumly unlocks the potential of each modality, resulting in a comprehensive DyTAG representation.





% neglects the different characteristics and significance of multi-modal information, leading to sub-optimal performance. Furthermore, DTGB, following the line of dynamic graph modeling, focuses exclusively on local learning based on the dynamic structure, where timestamps and edge/node features are merely treated as an optional supplement for structure learning, leading the underexplored for these rich information and overlooking the global contextual learning and leading to poor model robustness. 


% trivedi2019dyrep,xu2020inductive,congwe,wang2021tcl,

\subsection{Continuous-Time Dynamic Graph Learning}
Dynamic graph models aim to process the dynamic structures with/without vector-based edge/node attributes, which have been extensively studied due to their practical applicability. Existing dynamic graph models~\cite{kumar2019predicting,trivedi2019dyrep,DBLP:conf/wsdm/SankarWGZY20,tgn,wang2021inductive,souza2022provably,luo2022neighborhood,DBLP:journals/pvldb/LiSCY23,ma2024temporal,ji2023community,DBLP:conf/aaai/LiYZC0ZTWM23,xu2024scalable,DBLP:conf/icde/000200O024,DBLP:conf/kdd/ZhongVYA24,DBLP:conf/kdd/0003MY024,wu2024feasibility,zhu2024topology} typically focused on complex dynamic structure learning, where some~\cite{kumar2019predicting,tgn} leveraged sequence models (\textit{e.g.}, Gated Recurrent Unit (GRU)) to capture temporal dependencies. Concretely, TGAT~\cite{xu2020inductive} extended the graph attention mechanism to dynamic graphs, where it employed a time encoding function to help temporal-topological neighbors. Subsequently, co-neighbor encoding techniques~\cite{yu2023towards,DBLP:conf/iclr/TianQG24,cheng2024co,zhang2024towards} were studied to record neighbor co-occurrence and capture complex structural correlations. Additionally, researchers explored simple neural models for learning complex dynamic structure changes, such as MLP-Mixer~\cite{congwe} and Frequency-enhanced MLP~\cite{DBLP:conf/iclr/TianQG24}. However, existing approaches typically regard timestamps as the supplement during local structure learning, which is edge-centric modeling with feature-level fusion. They fail to provide global guidance from the temporal contextual perspective, leading to underexploration for timestamps. Beyond dynamic structure modeling, significant efforts~\cite{wen2022trend,DBLP:conf/icml/GravinaLGBG24,DBLP:conf/iclr/SuZ024,tian2024latent,ma2024temporal} have been devoted to capturing long-range (or intricate) temporal dynamics through various techniques, including the memory module and information propagation. However, the above approaches are limited in capturing the complicated semantics of text attributes as they fail to process raw text attributes of nodes and edges.

% For more details, please refer to the survey~\cite{}. 

% capturing temporal dependencies and encoding complex neighbor relations. Concretely, they leveraged sequence models (\textit{e.g.}, Gated Recurrent Unit (GRU)) to capture temporal dependencies based on node status information and designed different co-neighbor encoding techniques to model complex dynamic structures for node representation. 


\subsection{Large Language Models for Graphs}
Large Language Models (LLMs) have demonstrated remarkable reasoning capabilities across various domains, including graph management and analysis. Recent studies have begun exploring the potential of LLMs in processing static Text-Attributed Graphs (TAGs)~\cite{zhao2023graphtext,tian2024graph,yan2023comprehensive,pan2024distilling,tang2024graphgpt,fang2024gaugllm,10.1145/3580305.3599833}. Among them, LLMs served various roles—acting as enhancers, encoders, or predictors—by leveraging static graph structures and rich text descriptions. However, existing approaches focus on node representation generation or prediction tasks within a static setting, failing to handle timestamp information and making them inapplicable to DyTAG learning. Furthermore, recent research~\cite{chen2024exploring,huang2024gnns,heharnessing} has been dedicated to examining the capabilities and boundaries of LLMs in graph management and analysis, but our work follows a different path—analyzing DyTAGs themselves to reveal the distinct characteristics of multiple modalities for better node representations, thereby supporting various downstream tasks, \textit{e.g.}, link prediction and edge classification.








\section{Preliminary}~\label{sec:3}
In this paper, we focus on dynamic text-attributed graphs and introduce the definition below.
\begin{definition}[Dynamic Text-Attributed Graphs (DyTAGs)]\label{def:1}
A Dynamic Text-Attributed Graph (DyTAG) can be defined as $\mathcal{G} = \{\mathcal{V}, \mathcal{E}\}$, where $\mathcal{V}$ represents the set of nodes, $\mathcal{E}\in \mathcal{V}\times \mathcal{V}$ denotes the set of edges. Let $\mathcal{T}$ denote the set of timestamps, $\mathcal{D}, \mathcal{R}$ and $\mathcal{L}$ are the set of the node text descriptions, edge text descriptions, and edge categories, respectively. Each $v\in \mathcal{V}$ is associated with a text description $d_v\in \mathcal{D}$. Each $(u, v)\in \mathcal{E}$ can be represented as $(r_{uv}, l_{uv}, t_{uv})$ with a text description $r_{uv}\in \mathcal{R}$, a category $l_{uv}\in \mathcal{L}$ and a timestamp $t_{uv}\in \mathcal{T}$ to indicate the occurrence time of this edge. 
\end{definition}

Based on definition~\ref{def:1}, DyTAGs can be regarded as continuous-time dynamic text-attributed graphs. We take a \textit{node-centric} perspective to analyze the DyTAG and define its three modalities as

% and formally illustrate their multi-modal characteristics, highlighting the distinct roles of structural, temporal, and textual modalities in DyTAG representation. We
% captures structural correlations, and in some cases, edges may also carry textual attributes, further enriching structural information.
\begin{itemize}
    \item \underline{Structural modality} represents the dynamic graph topology, formed by evolving edges over time. This modality includes timestamps and textual attributes of edges, capturing dynamic structure changes as the graph continuously evolves.

    \item \underline{Temporal modality} is defined from a node-centric perspective. This modality represents the timestamps of node behaviors, reflecting intricate temporal evolutions within the graph.
    
    \item \underline{Textual modality} is defined from a node-centric perspective. This modality encapsulates the textual attributes of nodes, providing semantic information that complements structural and temporal aspects.
\end{itemize}

% DyTAGs inherently exhibit a multi-modal nature, where different types of information contribute to node and edge representations. We define three distinct modalities

% These modalities are complementary, as they exhibit distinct distributions (as shown in Fig.~\ref{fig:1b}), thereby individually contributing to final node representations.

To illustrate the characteristics of three modalities, we provide modality distribution visualizations of real DyTAGs in Fig.~\ref{fig:1b} and Fig.~\ref{fig:A7} in the Appendix. Next, we follow an encoder-decoder framework~\cite{kazemi2020representation} and define the problem of representation learning on DyTAGs.


% We can see that the structural modality of DyTAGs also has the timestamps and textural attributes of edges as timestamps and textural attributes must happen on edges, leading to dynamic graph topologies. 
 

\begin{problem}[Representation Learning on Dynamic Text-Attributed Graphs]
    Given a DyTAG $\mathcal{G} = \{\mathcal{V}, \mathcal{E}\}$ with a sequence
of temporal events, \textit{e.g.}, $(u, v): (r_{uv}, l_{uv}, t_{uv})$, we aim to
design an encoder function $F$: $(u, v)\rightarrow \mathbf{Z}^u, \mathbf{Z}^v\in \mathbb{R}^{f}$, where $\mathbf{Z}^u$ and $\mathbf{Z}^v$ respectively represents representations/embeddings of nodes $u$ and $v$ at time $t_{uv}$ and $f$ is the embedding dimension.
\end{problem}

Then node representations are input into decoder functions for downstream tasks, such as link prediction and edge classification. 

% Before the model design, we first analyze the DyTAGs.


\begin{figure}[t]
    \includegraphics[width=0.99\linewidth]{figures/overall.pdf}
    \caption{The overall procedure of our proposed MoMent. We introduce a node-centric view to model multiple modalities on DyTAGs. We design a temporal node-centric encoder to capture global temporal dependencies over time and a textual node-centric encoder to learn global semantic contexts (\textit{e.g.}, semantic cluster). For structural modality, dynamic graph models can be plugged into MoMent for local structure learning. We propose a symmetric alignment loss based on Jensen-Shannon divergence to ensure global temporal-semantic consistency.}
    \label{fig:2}
\end{figure}




\section{The Proposed MoMent}
% \subsection{Motivation}

% \textbf{Why do we need to explore multi-modal learning for DyTAG representation?}




% This formal derivation shows that the modalities are orthogonal based on distinct value ranges, densities, and minimal mutual information.


% \textbf{How to effectively model multiple modalities in a general manner?}
% Existing dynamic graph models focus exclusively on structural modality learning, making progress in local structure learning over dynamic graphs. Here, they fuse timestamps and vector attributes at the input level and then accomplish structure learning based on different model designs. However, they fail to comprehensively employ the temporal modality and textural modality as they can provide global temporal changes and consistent semantic relationships for node representation, enabling models to learn more stable and robust representations of DyTAGs. How to model unique modalities and how to fuse these modalities remains unexplored. 
% Temporal attributes of the nodes (timestamps) carry the strongest signal.











% To align multiple modalities and enhance modality consistency, it is essential to preserve each modality’s unique information while enabling selective cross-modal interactions. One effective approach employs structural embeddings as a bridging mechanism between temporal and textual modalities. By mediating their relationship, structural embeddings allow for near-zero conditional mutual information between temporal and textual representations, ensuring independence when conditioned on the structural modality. This approach exploits the fact that temporal and textual data often occupy distinct value ranges yet exhibit weak correlations through the underlying structure.

% When constructing dynamic text-attributed graphs, aligning these modalities ensures that temporal trends, like bursts or periodic activity, and textual content, such as topical shifts, are contextually integrated. This alignment framework leverages structural embeddings to capture long-term dependencies, bridging short-term temporal fluctuations with relevant textual semantics. Consequently, models become more expressive and robust, capturing subtle cross-modal cues necessary for downstream tasks.

% Moreover, alignment facilitates learning more nuanced representations that adapt to semantic shifts in textual data over time, benefiting tasks like link prediction, anomaly detection, and event forecasting. Overall, ensuring alignment among modalities in dynamic graph learning frameworks fully exploits each modality’s strengths, leading to richer, more consistent, and task-relevant representations. Finally, it fosters robust solutions.


% Based on the KDE distortion, we can achieve that the conditional mutual information between temporal modality and textural modality is near to zero, validating the conditional independence of temporal and textual modalities given structural embeddings as there are non-overlapping value ranges of temporal and textual modalities. While temporal and textual modalities are independent, they often exhibit weak dependencies mediated by the structural modality, naturally constructing a dynamic text-attributed graphs. Temporal activities (\textit{e.g.}, bursts or periodic interactions) might indirectly correlate with textual patterns in certain contexts (\textit{e.g.}, frequent discussions of specific topics during specific times).
% Structural embeddings act as a bridge between temporal and textual modalities. If not aligned, the combined representation may fail to capture the subtle cross-modal dependencies essential for downstream tasks. Aligning temporal and textual modalities ensures that the contextual timing of events and content relevance are jointly modeled.
% Without alignment, the model risks ignoring complementary signals, leading to suboptimal representations.The semantic meaning of textual data can evolve over time, necessitating alignment with temporal information to reflect contextual relevance. Without alignment, textual embeddings remain static, failing to capture temporal shifts in semantics.
% Temporal information often provides a contextual anchor for textual features, helping models generalize to tasks requiring both time-sensitive and semantic reasoning.
% Alignment en

% Although temporal and textual modalities are independent, alignment is essential for dynamic graph representation learning because:

% Structural embeddings act as a mediator, creating weak dependencies between the two modalities.
% Many real-world tasks (\textit{e.g.}, link prediction, anomaly detection) require joint reasoning across both temporal and textual information.
% Alignment enables models to capture temporal relevance of textual features, improving expressiveness, robustness, and task performance.
% By aligning temporal and textual modalities, dynamic graph learning frameworks can fully leverage the complementary strengths of each modality while maintaining their individual independence.
% If we independent model three modalities, they indeed provide unique, complementary, non-redundant information information for node representation, but We visualize the KDE distribution of these three modalities based on the original attributes and compute the entropy of different modalities using its first-order approximation as the first-order approximation can still preserve relative entropy rankings. And, we just need a quick estimation of entropy and the exact KDE-based or bin-based computation is computationally expensive. 
% \begin{equation}\label{eq:1}
%     H(M) = \log(W) + C
% \end{equation}
% where the $W$ is the width of the uniform distribution and $C$ is unspecified but represents an adjustment to account for the peak density. $M$ denotes the modality. Based on the Eq.~\ref{eq:1} and KDE distribution in Fig.~\ref{fig:1b}, we can achieve that $H(temporal) > H(structural) > H(textural)$ for the Enron dataset, meaning that increasing regularity and decreasing variability across the modalities. Densities support the hypothesis that the modalities remain largely independent considering both the value ranges and density distributions. Second, these modalities


\subsection{Overview}
Fig.~\ref{fig:2} shows the overall procedure of our proposed MoMent. The key novelty of MoMent lies in unlocking multi-modal potentials by effectively modeling, aligning, and integrating these modalities to achieve comprehensive and cohesive representations. Concretely, we propose a multi-modal framework, which is comprised of two node-centric encoders for temporal and textual modalities and is compatible with any dynamic graph backbone for structural modality. Thus, MoMent can capture global temporal dependencies and semantic context as well as local structure correlations. To cohesively fuse tokens from three modalities, we propose a symmetric alignment loss to ensure global temporal and semantic consistency. Then we design a lightweight adaptor to fuse tokens at a decision level, generating comprehensive and cohesive node representations for DyTAGs. Last, we provide theoretical analysis for MoMent in information gain and modality consistency.



% To gain insights into modality interactions, we analyze their Kernel Density Estimation (KDE) distributions in Fig.~\ref{fig:1b}, leading to the following key observations that guide our modeling: \ding{182} 
\subsection{Multi-Modal Framework}
\subsubsection{Motivation}
As analyzed in the Introduction section, each modality of DyTAGs can provide unique contributions to node representation. However, existing DTGB and dynamic graph models, which typically rely on feature-level fusion, treat timestamps and node attributes as auxiliary features rather than fundamental temporal and semantic signals. As a result, temporal and textual modalities remain largely underutilized. Additionally, existing models focus on edge-centric modeling to capture dynamic graph changes while implicitly encoding temporal and textual dependencies. Although they can provide essential local guidance for node representation, they fail to capture complex temporal and semantic contexts beyond localized interactions.






\subsubsection{Formulation}
We propose a multi-modal framework (MMF) for dynamic text-attributed graph representation, designed as a plug-and-play solution that seamlessly integrates with existing dynamic graph models. Our MMF is based on the \textit{decision-level fusion}, that is, first model each modality individually and then fuse them cohesively. Concretely, we propose two node-centric encoders for temporal and textual modalities and leverage existing dynamic graph backbones for structural modality, generating modality-specific tokens. Then we design a lightweight adaptor to fuse them for comprehensive node representation generation. Given a batch of temporal edges $\mathcal{G}_B = \{\mathcal{V}_B, \mathcal{E}_B\}$ with raw node textual attributes $\mathcal{D}_B$, raw edge textual attributes $\mathcal{R}_B$ and observed timestamps $\mathcal{T}_B$ in $\mathcal{G}_B$, we formulate our multi-modal framework as 
\begin{align}
    \mathbf{Z}_{t} &= \operatorname{MSA}_t(\mathbf{T}_B, \mathbf{T}_B, \mathbf{T}_B),\; \mathbf{T}_B =  \operatorname{FeedForward}_t(\phi(\mathcal{T}_{B})),\label{eq:2}\\
    \mathbf{Z}_{s} & = \operatorname{DGM}([\operatorname{PLM}(\mathcal{R}_B)\oplus\phi(\Delta\mathcal{T}_B)] | \forall u:\mathcal{N}_{u}(t), u\in \mathcal{V}_B),\label{eq:3}\\
    \mathbf{Z}_{x} &= \operatorname{MSA}_x(\mathbf{D}_B, \mathbf{D}_B, \mathbf{D}_B),\; \mathbf{D}_B = \operatorname{FeedForward}_x(\operatorname{PLM}(\mathcal{D}_B)).\label{eq:4}
\end{align}
where $\operatorname{MSA}_t(\cdot)$ and $\operatorname{MSA}_x(\cdot)$ are non-shared multi-head self-attention mechanisms~\cite{waswani2017attention} as temporal and textual node-centric encoders. $\phi(\cdot)$ is the time encoding function~\cite{xu2020inductive}. $\operatorname{DGM}(\cdot)$ can be any dynamic graph model with the structure learning module. $\Delta\mathcal{T}_B$ denotes the time gap between source nodes and their neighbors, and $\mathcal{N}_{u}(t)$ represents the neighbors of node $u$ before the current timestamp $t$. $\operatorname{PLM}(\cdot)$ is the Bert-base-uncased model~\cite{kenton2019bert}, followed by DTGB for text attribute initializations. $\operatorname{FeedForward}_t(\cdot)$ and $\operatorname{FeedForward}_x(\cdot)$ are the multi-layer feedforward network, which is used to regularize the dimension for these two modalities. Following conventional protocol, we leverage the layer normalization and dropout mechanism. Based on Eq.~\eqref{eq:2}, we can capture global temporal dependencies, such as periodic spikes or abrupt shifts, which provide insights into the node’s long-term behavioral trends. Eq.~\eqref{eq:3} models the node’s local neighbors at a given timestamp, ensuring that structural dependencies together with edge attributes are preserved. We can extract global semantic contexts by Eq.~\eqref{eq:4}, which may hold crucial information to complement temporal and structural modalities. Thus, we obtain three modality-specific tokens—temporal, structural, and textual—for each node within a batch, $\mathbf{Z}_{t}, \mathbf{Z}_{s}$ and $\mathbf{Z}_{x}$. 


% The attention blocks are stacked with $l$ layers to capture the high-level temporal and semantic information hidden within the timestamps and node attributes.
% Each of these embeddings captures a distinct aspect of the node’s characteristics. 



% Based on Eqs.~\eqref{eq:2} and~\eqref{eq:4}, our MoMent can capture global temporal dependencies and semantic context by modeling the temporal behaviors and semantic information of nodes. Eq.~\eqref{eq:3} encodes local structure correlations based on neighbors of nodes, preserving graph topology.


% By incorporating these three perspectives, each node’s final embedding reflects who it connects with (structure), when it evolves (time), and what textual attributes it carries (text). 

Next, we adopt a decision-level fusion and integrate the tokens from all three modalities to generate the final node representations $\mathbf{Z}$ for the batch of nodes $\mathcal{V}_B$ in $\mathcal{G}_B$. To achieve this, we design a lightweight adaptor, which adaptively fuses them according to their contributions. We define it as 
\begin{equation}\label{eq:5}
    \mathbf{Z} = \operatorname{FeedForward}([\mathbf{Z}_{t}\oplus\mathbf{Z}_{s}\oplus\mathbf{Z}_{x}]),
\end{equation}
where $\oplus$ denotes the concatenation operation. Eq.~\eqref{eq:5}
ensures a learned mechanism for weighting each modality’s contribution. Based on our multi-modal framework, local structure learning (structural modality) merges seamlessly with global guidance from temporal and textual modalities, generating comprehensive node representations for DyTAGs. This suggests our MMF can unlock the potential of each modality for DyTAG representation.


% each node’s final embedding reflects who it is connected to (structure), when it changes (time), and what textual information it carries (text).
 % This ensures a learned (but straightforward) mechanism for weighting each modality’s contribution.


% In summary, Temporal tokens highlight when a node’s features or events matter, Textual tokens highlight what semantic content a node (or edge) holds, and Structural tokens highlight how a node ties into the broader network topology over time.
% By harnessing all three in a multi‐modal manner, you obtain a rich, comprehensive node representation that outperforms single‐modality approaches—particularly in complex dynamic text‐attributed graphs. High‐entropy modalities can adapt widely (temporal), and low‐entropy signals (textual) do not get ignored or flattened out—they are selectively emphasized where beneficial. Finally aligns and fuses them into a coherent node representation—ensuring that none of the three modalities’ essential contributions go underutilized.

% Complement to time and structure: While temporal and structural features capture “when” and “how” a node connects, text reveals what the node (or link) “is about”—adding important descriptive power.


% \textbf{How to align multiple modalities to improve the modality consistency?}
% Aligning textual and temporal modalities is crucial because each modality provides complementary insights that, when combined, yield a more holistic view of dynamic systems. Textual data captures semantic content—topics, sentiments, or entities—while temporal information offers context on when events occur or evolve. Without proper alignment, models risk missing the nuanced interplay between what is being discussed and the timing of those discussions. For instance, a sudden rise in certain keywords within a short time frame can indicate emerging trends or anomalies that purely textual or purely temporal models would overlook. 

% Moreover, textual semantics often shift over time—a topic’s meaning or relevance can change with new events and context. Aligning the temporal modality with textual content ensures these shifts are detected and understood, enabling more accurate downstream tasks like event forecasting or topic tracking. In dynamic graph settings, structural embeddings then serve as a bridge, bringing together time-aware textual signals and the underlying graph topology. This alignment guards against inconsistent or contradictory representations across modalities, facilitating more robust and coherent learning. Ultimately, it empowers models to capture the subtle correlations and dependencies that drive real-world processes, leading to stronger predictive performance and richer insights.

\subsection{Symmetric Alignment Loss}
\subsubsection{Motivation} 
 To preserve the unique characteristics of each modality, we model them individually, allowing the framework to capture modality-specific patterns without one modality overpowering the others. However, this may result in a disjoint latent space, potentially compromising model robustness. Additionally, structural tokens are obtained through aggregating information from immediate neighbors. In contrast, we obtain temporal and textual tokens from global contexts regardless of graph topology. Given this distinction, we focus on aligning the topology-independent modalities (temporal and textual) while maintaining structural integrity, ensuring a cohesive fusion.

% Attempting to directly match local structural embeddings with global temporal/textual embeddings can be less meaningful and may introduce unnecessary complexity. Instead, we focus on aligning the global modalities (temporal and textual) while maintaining structural integrity, ensuring a more effective and coherent fusion strategy.



% As introduced before, the three modalities of DyTAGs are largely independent, and we employ the idea of late fusion in our multi-modal learning framework. Here, we separately model three different modalities to learn individual patterns, which can avoid the overwhelming between these modalities. However, this easily leads to dispersed latent space, making it difficult to fuse them jointly and compromising model robustness. In our framework, structural embeddings are derived from local GNN operations, which generate node representations primarily informed by immediate neighbors. In contrast,  both the temporal and textual embeddings are learned using global contexts, thus it is more meaningful to align temporal and textual modalities. Directly matching local representations (structural) with global ones (temporal/textual) can be less meaningful and might add unnecessary complexity.


% Since both the temporal and textual embeddings are learned using global contexts, it is more meaningful to align these two modalities. Structural embeddings are derived from local GNN operations, which generate node representations primarily informed by immediate neighbors. 


% sBased on the analysis, the mutual information between temporal modality and textural modality conditioned on the structural modality is near zero, suggesting the independence of original attributes. 


% Additionally, temporal modality typically carries strong predictive signals based on entropy, while textual embeddings can complement these signals with contextual semantics. In the multi-modal learning framework, we separately model two different modalities to learn individual patterns, which can avoid the overwhelming between these two modalities. However, this easily leads to dispersed latent space, making it difficult to fuse them jointly and compromising model robustness. A key aspect of MoMent is the explicit alignment between the temporal token and textual tokens. This alignment is crucial, as it allows our MoMent to better integrate the structured knowledge from the graph with the unstructured information, improving the quality of the joint representations. Alignment loss enforces that two sets of embeddings (\textit{e.g.}, temporal and textual) remain “close” in a shared latent space. Since both the temporal and textual embeddings are learned using global contexts, it is more meaningful to align these two modalities. Structural embeddings are derived from local GNN operations, which generate node representations primarily informed by immediate neighbors. Directly matching local representations (structural) with global ones (temporal/textual) can be less meaningful or might add unnecessary complexity.

% \begin{equation}
%     \mathcal{L}_{\text{align}} \operatorname{KL}(\mathbf{Z}_{t}\|\mathbf{Z}_{x}) + \operatorname{KL}(\mathbf{Z}_{x}\|\mathbf{Z}_{t})
% \end{equation}

\subsubsection{Formulation}
To achieve this, we propose a symmetric alignment loss based on Jensen-Shannon Divergence (JSD), which can enforce bidirectional alignment between modality-specific tokens. Given the temporal-specific tokens $\mathbf{Z}_{t}$ and textual-specific tokens $\mathbf{Z}_{x}$, we formulate the alignment loss as
\begin{align}
    \mathcal{L}_{\text{align}} &= \sigma(\sum_{i=1}^{|\mathcal{V}_B|}(\operatorname{JSD}(\mathbf{Z}_{t}^{i},\mathbf{Z}_{x}^{i}))),\label{eq:6}\\
    \operatorname{JSD}(\mathbf{Z}_{t}^{i},\mathbf{Z}_{x}^{i}) &= \sum_j^{f_t}\mathbf{Z}_{t}^{ij}\log \frac{\mathbf{Z}_{t}^{ij}}{\mathbf{Z}_{x}^{ij}} + \sum_j^{f_x}\mathbf{Z}_{x}^{ij}\log \frac{\mathbf{Z}_{x}^{ij}}{\mathbf{Z}_{t}^{ij}}, \label{eq:7}
\end{align}
where $f_t$ and $f_x$ are equal, denoting the dimension of temporal and textual tokens. $\sigma(\cdot)$ is the sigmoid function, which is used to prevent the auxiliary loss from overpowering the main loss. Eq.~\eqref{eq:6} is a soft constraint, which encourages both modalities to adapt to each other rather than one conforming to the other. Thus, our MoMent can minimize distribution discrepancies between temporal and textual tokens. Once aligned, decision-level fusion via Eq.~\eqref{eq:5} becomes more seamless, resulting in comprehensive and cohesive node representations.


% We aim to enforce bidirectional alignment between modality-specific tokens, but KL divergence is inherently asymmetric.

% Since KL divergence is inherently asymmetric, we enforce bidirectional alignment between modality-specific tokens. This prevents them from becoming contradictory and minimizes modal discrepancies, ensuring a more coherent multi-modal representation.



% We design a symmetric KL divergence loss, which helps each modality’s embedding distribution adapt to the other, rather than one simply conforming to the other’s shape. The alignment ensures that temporal embeddings (which capture when a node’s features change) and textual embeddings (which capture what is semantically important) can be combined or compared more directly. The symmetric KL loss provides a soft constraint that nudges both sets of embeddings to remain “close,” ensuring they describe the node in compatible ways. By aligning them, you ensure the textual modality is not overshadowed, letting it contribute useful secondary information. Symmetric KL is an effective “bridge”: it penalizes discrepancies in distribution shape, reducing the distance between the temporal and textual embedding manifolds. Once temporal and textual embeddings are aligned, fusing them (\textit{e.g.}, via concatenation, linear combination, or attention) becomes more straightforward. This synergy typically yields richer node representations than relying on any single modality alone. The alignment (symmetric KL) ensures your temporal and textual embeddings do not drift into incompatible spaces. The alignment reduces ambiguity across modalities: textual features cannot deviate wildly from the temporal features for the same node. Theoretically, consistent cross‐modal embeddings can be interpreted as each modality encoding complementary evidence about the node, which helps ensure that final decisions are robust to noise in either modality. This loss preserves a clear division between local structure modeling and global temporal/textual modeling.

% For instance, in the link prediction task, it can be incorporated into the Binary Cross Entropy (BCE) loss to enhance representation learning.

The proposed alignment loss serves as an auxiliary loss that can be seamlessly integrated into any main loss function. Thus, we define the overall loss of our MoMent as
\begin{equation}
  \mathcal{L} = \mathcal{L}_{\text{main}} + \alpha\mathcal{L}_{\text{align}},  
\end{equation}
where $\alpha$ is a hyperparameter to balance the main loss for the downstream task and auxiliary loss for modality alignment. The main loss 
$\mathcal{L}_{\text{main}}$ is task-dependent, where Binary Cross-Entropy (BCE) loss is used for link prediction, and Cross-Entropy (CE) loss is applied to edge classification. By incorporating symmetric alignment loss as an auxiliary term, MoMent preserves the main optimization objective while ensuring modality-specific tokens are aligned within a shared latent space.

% $\mathcal{L}_{\text{main}}$ depends on specific downstream tasks, where we employ Binary Cross Entropy Loss for link prediction task and Cross Entropy Loss is used for edge classification task. With this loss, MoMent can maintain the main optimization target while leveraging symmetric KL divergence as an auxiliary term to align the temporal and textual embeddings in a shared latent space. 

% Additionally, this allows MoMent to softly regularize the representations rather than forcing them to match perfectly, improving multi‐modal consistency without hurting the primary task performance. Symmetric KL ensures both modalities remain close in distribution, preserving semantic‐temporal consistency and reducing the hypothesis space to more robust solutions.



\subsection{Theoretical Analysis for MoMent}
Our MoMent framework unlocks the multi-modal potentials of DyTAGs by multi-modal modeling and alignment, generating comprehensive and cohesive node representations. In contrast, existing DTGB relies on edge-centric modeling, where input features are fused before applying local structure learning. To formally demonstrate that MoMent via multi-modal modeling can provide greater discriminative power than DTGB based on edge-centric modeling, we present the following theorem.


\tcbset{colback=lightgray!15!white, colframe=white, left=1mm, right=1mm, top=1mm, bottom=1mm}
\begin{tcolorbox}
\begin{theorem}[Information Gain Bound]\label{theo:1}
    For multi-modal representation $\mathbf{Z} = \operatorname{FeedForward}([\mathbf{Z}_t \oplus \mathbf{Z}_s \oplus \mathbf{Z}_x])$ and target task $\mathbf{Y}$, there exists $\beta\in (0,1]$, the following holds:
    \begin{equation}
        I(\mathbf{Z};\mathbf{Y}) \geq I(\mathbf{Z}_s;\mathbf{Y}) + \beta \min\{H(\mathbf{Y}|\mathbf{Z}_s), H(\mathbf{Z}_t,\mathbf{Z}_x|\mathbf{Z}_s)\},
    \end{equation}
where $I(\cdot;\cdot)$ denotes mutual information, $H(\cdot|\cdot)$ denotes conditional entropy.
\end{theorem}
\end{tcolorbox}

Theorem~\ref{theo:1} is proved in Section~\ref{proof:1} in the Appendix. Theorem~\ref{theo:1} demonstrates that our multi-modal modeling can improve information gain against exclusive edge-centric modeling, thereby enhancing the discriminative power of node representations.


% improving information gain generally means enhancing the discriminative power of the features or representations. In other words, when a feature or modality provides a high information gain, it significantly reduces the uncertainty about the target variable. This reduction in uncertainty implies that the feature is better at distinguishing between different classes or outcomes, which is the essence of having strong discriminative power.


% The theorem serves as a theoretical validation for our multi-modal modeling, demonstrating that integrating complementary modalities necessarily enhances model performance under proper fusion conditions.

 % The theorem establishes a quantifiable minimum improvement in discriminative power when incorporating temporal and textual modalities.

 % providing a guarantee that multi-modal representation outperforms edge-centric models by at least $\beta \cdot \min \{ H(Y \mid Z_s), H(Z_t, Z_x \mid Z_s) \}$.



\begin{table}[t]
  \caption{Dataset statistics.}
  \label{tab:1}
  \centering
  \small
  \resizebox{\linewidth}{!}{
  \begin{tabular}{|c|c|c|c|c|c|}
    \Xhline{1pt}
     \rowcolor{gray!40}Dataset & \# Nodes & \# Edges  &\#Categories & \# Timestamps & Text Attributes \\ 
    \hline
    \hline
    Enron &42,711 & 797,907 &10 & 1,006 & Node \& Edge\\
    ICEWS1819 & 31,796 & 1,100,071 & 266 & 730 & Node \& Edge\\
    Stack elec & 397,702 & 1,262,225&  2 & 5,224 & Node \& Edge\\
    Stack ubuntu & 674,248 & 1,497,006 & 2 & 4,972& Node \& Edge\\
    Googlemap CT &111,168 &1,380,623 &5 &55,521& Node \& Edge\\
    Amazon movies &293,566 &3,217,324 &5 &7,287 & Node \& Edge\\
    Yelp &2,138,242 &6,990,189 &5 &6,036 & Node \& Edge\\
    \hline
    \end{tabular}}
\end{table}




\begin{table*}[tbp]
	\caption{Performance on link prediction under AUC and AP in the inductive setting. The best results are marked in bold.}
	\label{tab:3}
	\centering
	\resizebox{\linewidth}{!}{
 \begin{tabular}{|c|c|c|c|c|c|c|c|}
        \Xhline{1pt}
        \rowcolor{gray!40} Approach & Enron & ICEWS1819 & Googlemap CT & Stack elec & Stack ubuntu & Amazon movies  & Yelp\\
        \hline
        \hline
        \multicolumn{8}{|c|}{\textbf{AUC under Inductive Setting}}\\
        \hline
        TGAT$+$DTGB & 0.8650$\pm$0.0032 &0.9706$\pm$0.0054 &0.8791$\pm$0.0028 &0.8423$\pm$0.0018 &0.7655$\pm$0.0019 &0.8706$\pm$0.0023 &0.9173$\pm$0.0008 \\
        TGAT$+$MoMent &\textbf{0.9712$\pm$0.0081}&\textbf{0.9937$\pm$0.0001} &\textbf{0.9549$\pm$0.0058} &\textbf{0.9752$\pm$0.0037}&\textbf{0.9346$\pm$0.0061} &\textbf{0.9603$\pm$0.0045} &\textbf{0.9662$\pm$0.0046}\\
        \hline 
        GraphMixer$+$DTGB &0.8347$\pm$0.0039&0.9605$\pm$0.0025 & 0.7543$\pm$0.0018 &0.8232$\pm$0.0031 &0.7870$\pm$0.0032 &0.8418$\pm$0.0012 &0.8452$\pm$0.0014 \\
        GraphMixer$+$MoMent &\textbf{0.9506$\pm$0.0333}&\textbf{0.9984$\pm$0.0008}&\textbf{0.9610$\pm$0.0017} &\textbf{0.9456$\pm$0.0903}&\textbf{0.8513$\pm$0.0023}&\textbf{0.9863$\pm$0.0016}&\textbf{0.9728$\pm$0.0033}\\
        \hline
        DyGFormer$+$DTGB &0.9316$\pm$0.0015&0.9630$\pm$0.0027 & 0.7648$\pm$0.0052& 0.8607$\pm$0.0015&0.7773$\pm$0.0047 &0.8733$\pm$0.0005 &0.9067$\pm$0.0009\\
        DyGFormer$+$MoMent &\textbf{0.9705$\pm$ 0.0038}&\textbf{0.9907$\pm$0.0003}&\textbf{0.9611$\pm$0.0017}&\textbf{0.8956$\pm$0.0041}&\textbf{0.9871$\pm$0.0063}&\textbf{0.9785$\pm$0.0012}&\textbf{0.9493$\pm$0.0062}\\
        \hline
        CNEN$+$DTGB &0.9633$\pm$0.0007&0.9673$\pm$0.0010&0.8404$\pm$0.0019&0.9615$\pm$0.0002&0.9758$\pm$0.0000&0.8679$\pm$0.0008& 0.9732$\pm$0.0005\\
        CNEN$+$MoMent &\textbf{0.9917$\pm$0.0021}&\textbf{0.9983$\pm$0.0001} &\textbf{0.9750$\pm$0.0027}&\textbf{0.9723$\pm$0.0259}&\textbf{0.9775$\pm$0.0020}&\textbf{0.9843$\pm$0.0007}&\textbf{0.9840$\pm$0.0008}\\
        \hline
        \hline
        \multicolumn{8}{|c|}{\textbf{AP under Inductive Setting}}\\
        \hline
        TGAT$+$DTGB &0.8589$\pm$0.0031&0.9716$\pm$0.0033 & 0.8750$\pm$0.0015 &0.8391$\pm$0.0036 &0.7664$\pm$0.0015 &0.8760$\pm$0.0010 &0.9174 $\pm$0.0010\\
        TGAT$+$MoMent &\textbf{0.9720$\pm$0.0085}& \textbf{0.9948$\pm$0.0007} &\textbf{0.9677$\pm$0.0024} &\textbf{0.9764$\pm$0.0026}&\textbf{0.9377$\pm$0.0040}&\textbf{0.9617$\pm$0.0018}&\textbf{0.9693$\pm$0.0011}\\
        \hline 
        GraphMixer$+$DTGB &0.8328$\pm$0.0034&0.9625$\pm$0.0030 &0.7633$\pm$0.0013 &0.8142$\pm$0.0021 &0.7870$\pm$0.0015 &0.8517$\pm$0.0007 &0.8494$\pm$0.0008\\
        GraphMixer$+$MoMent &\textbf{0.9556$\pm$0.0292}&\textbf{0.9994$\pm$0.0005}&\textbf{0.9720$\pm$0.0012}&\textbf{0.9463$\pm$0.0899}&\textbf{0.8660$\pm$0.0012}&\textbf{0.9855$\pm$0.0021} &\textbf{0.9730$\pm$0.0035}\\
        \hline
        DyGFormer$+$DTGB &0.9409$\pm$0.0025&0.9688$\pm$0.0018&0.7735$\pm$0.0031&0.8801$\pm$0.0043&0.7832$\pm$0.0015&0.8780$\pm$0.0006&0.9092$\pm$0.0006\\
        DyGFormer$+$MoMent &\textbf{0.9692$\pm$0.0028}&\textbf{0.9934$\pm$0.0002} &\textbf{0.9694$\pm$0.0018}&\textbf{0.9103$\pm$0.0023}&\textbf{0.9897$\pm$0.0049}&\textbf{0.9768$\pm$0.0014}&\textbf{0.9567$\pm$0.0046}\\
        \hline
        CNEN$+$DTGB &0.9629$\pm$0.0000&0.9690$\pm$0.0007& 0.8567$\pm$0.0009& 0.9659$\pm$0.0002& 0.9794$\pm$0.0001& 0.8801$\pm$0.0012&0.9774$\pm$0.0005\\
        CNEN$+$MoMent &\textbf{0.9886$\pm$0.0024} &\textbf{0.9918$\pm$0.0024}&\textbf{0.9813$\pm$0.0031}&\textbf{0.9705$\pm$0.0303}&\textbf{0.9804$\pm$0.0011}&\textbf{0.9848$\pm$0.0005}&\textbf{0.9881$\pm$0.0014}\\
       \Xhline{1pt}
\end{tabular}}
\end{table*}

Furthermore, we design a symmetric alignment loss in Eq.~\eqref{eq:6} to ensure modality alignment during the learning process, where we theoretically demonstrate its effectiveness.

% \subsection{Complexity Analysis}
\tcbset{colback=lightgray!15!white, colframe=white, left=1mm, right=1mm, top=1mm, bottom=1mm}
\begin{tcolorbox}
\begin{theorem}[Consistency]\label{theo:2}
    For temporal tokens $\mathbf{Z}_t$ and textual tokens $\mathbf{Z}_x$ conditioned on structural tokens $\mathbf{Z}_s$, the difference in their mutual information with target $\mathbf{Y}$ satisfies:
    \begin{equation}
|I(\mathbf{Z}_t;\mathbf{Y}|\mathbf{Z}_s) - I(\mathbf{Z}_x;\mathbf{Y}|Z_s)| \leq \mathcal{O}(\mathcal{L}_{\text{align}}),
\end{equation}
where $I(\mathbf{Z}_t;\mathbf{Y}|\mathbf{Z}_s)$ is the conditional mutual information between temporal tokens and target. $I(\mathbf{Z}_x;\mathbf{Y}|\mathbf{Z}_s)$ denotes conditional mutual information between textual tokens and target. $\mathcal{L}_{\text{align}}$ is our alignment loss.
\end{theorem}
\end{tcolorbox}
The proof of Theorem~\ref{theo:2} is provided in Section~\ref{proof:2} in the Appendix. Theorem~\ref{theo:2} provides a rigorous foundation for understanding how alignment loss controls information preservation across modalities, which facilitates temporal-semantic consistency.

\section{Experiments}

\subsection{Experimental Setup}
\subsubsection{Datasets}
Table~\ref{tab:1} summarizes datasets used in experiments, where we introduce the number of nodes, edges, edge categories, and timestamps as well as the type of text attributes. We collect $7$ datasets from~\cite{zhang2024dtgb}. These datasets span various domains, including the email network (Enron), knowledge graph (ICEWS1819), multi-round dialogue (Stack elec and Stack ubuntu), and E-commerce network (Googlemap CT, Amazon movies, and Yelp).

\subsubsection{Baselines and Backbones}
To assess the effectiveness of MoMent, we compare it with the DTGB framework~\cite{zhang2024dtgb}, which is the only dynamic text-attributed graph framework. We select four mainstream dynamic graph models—TGAT~\cite{xu2020inductive}, GraphMixer~\cite{congwe}, DyGFormer~\cite{yu2023towards}, and CNEN~\cite{cheng2024co}—as backbones. These backbones employ different structure learning techniques, covering attention-based neighbor aggregation (TGAT), lightweight neighbor aggregation (GraphMixer), and two co-neighbor encoding techniques (DyGFormer and CNEN). This ensures a comprehensive evaluation of MoMent across diverse structure learning techniques in dynamic graph learning. We collect the results of DTGB with (TGAT, GraphMixer, and DyGFormer) in Tables~\ref{tab:3}-~\ref{tab:5} from the original paper~\cite{zhang2024dtgb} and we run the DTGB with CNEN in our environments. 

% We compare our FastHeP with seven SOTA baselines, covering three groups:

\subsubsection{Evaluation Metrics} We evaluate the effectiveness of our proposed MoMent using two downstream tasks (\textit{i.e.}, edge classification and link prediction) under five evaluation metrics. For the edge classification task in DyTAGs, the goal is to predict the category of a potential edge occurring at a given timestamp, using weighted Precision, weighted Recall, and weighted F1 Score as evaluation metrics~\cite{zhang2024dtgb}. The link prediction task aims to determine whether an interaction will occur between two given nodes at a specific timestamp. In the inductive setting, the test set consists of new nodes that do not appear in the training and validation sets. For link prediction evaluation~\cite{huang2024temporal,zhang2024dtgb}, we use Average Precision (AP) and Area Under the Receiver Operating Characteristic Curve (AUC-ROC). Following the baseline~\cite{zhang2024dtgb}, we chronologically split each dataset into train/validation/test sets using a $7:1.5:1.5$ ratio. We repeat all experiments five times with random seeds and report both the mean and standard deviation of the results.

\begin{table*}[tbp]
	\caption{Performance on edge classification under precision and recall. The best results are marked in bold.}
	\label{tab:2}
	\centering
	\resizebox{\linewidth}{!}{
 \begin{tabular}{|c|c|c|c|c|c|c|c|}
        \Xhline{1pt}
        \rowcolor{gray!40} Approach & Enron & ICEWS1819 & Googlemap CT & Stack elec & Stack ubuntu & Amazon movies  & Yelp\\
        \hline
        \hline
        \multicolumn{8}{|c|}{\textbf{Precision}}\\
        \hline
        TGAT$+$DTGB &0.6148$\pm$0.0012&\textbf{0.3013$\pm$0.0007}&0.6160$\pm$0.0001&0.6265$\pm$0.0046 &0.6858$\pm$0.0047 &0.5878$\pm$0.0037 &0.6186$\pm$0.0018\\
        TGAT$+$MoMent &\textbf{0.6203$\pm$0.0180} &0.2945$\pm$0.0023&\textbf{0.6162$\pm$0.0019}&\textbf{0.6873$\pm$0.0034} &\textbf{0.7305$\pm$0.0037}&\textbf{0.5945$\pm$0.0007} &\textbf{0.6293$\pm$0.0020}\\
        \hline
        \hline
        GraphMixer$+$DTGB &0.6313$\pm$0.0024 &0.2999$\pm$0.0022 &0.6171$\pm$0.0020 &0.6074$\pm$0.0039 &0.6930$\pm$0.0028 &0.5934$\pm$0.0010 &0.6295$\pm$0.0085 \\
        GraphMixer$+$MoMent &\textbf{0.6696$\pm$0.0039} &\textbf{0.3041$\pm$0.0033} &\textbf{0.6231$\pm$0.0028} &\textbf{0.6911$\pm$0.0006} &\textbf{0.7327$\pm$0.0072} &\textbf{0.5945$\pm$0.0005}&\textbf{0.6330$\pm$0.0006}\\
        \hline
        \hline
        DyGFormer$+$DTGB &0.6601$\pm$0.0067 &0.3297$\pm$0.0034 &\textbf{0.6166$\pm$0.0003} &0.6026$\pm$0.0471 &0.6789$\pm$0.0490 &0.5943$\pm$0.0028 &\textbf{0.6407$\pm$0.0016} \\
        DyGFormer$+$MoMent &\textbf{0.6628$\pm$0.0069} &\textbf{0.3382$\pm$0.0082} &0.6134$\pm$0.0057 &\textbf{0.6964$\pm$0.0013}&\textbf{0.7400$\pm$0.0078}&\textbf{0.5997$\pm$0.0008}&0.6337$\pm$0.0010\\
        \hline
        \hline
        CNEN$+$DTGB &0.6169$\pm$0.0046 & \textbf{0.3155$\pm$0.0002}&0.5898$\pm$0.0004&0.5605$\pm$0.0000&0.7193$\pm$0.0026&0.5571$\pm$0.0004&0.6104$\pm$0.0003\\
        CNEN$+$MoMent &\textbf{0.6466$\pm$0.0131} &0.3129$\pm$0.0072 &\textbf{0.6113$\pm$0.0003}  &\textbf{0.6910$\pm$0.0027} &\textbf{0.7369$\pm$0.0065} &\textbf{0.5980$\pm$0.0031} &\textbf{0.6231$\pm$0.0029}\\
        \hline
        \hline
    \multicolumn{8}{|c|}{\textbf{Recall}}\\
        \hline
        TGAT$+$DTGB &0.5530$\pm$0.0001&\textbf{0.3512$\pm$0.0006} &0.6862$\pm$0.0002 &0.7205$\pm$0.0094 &0.7921$\pm$0.0012 &0.6711$\pm$0.0043 &\textbf{0.6686$\pm$0.0072}\\
        TGAT$+$MoMent &\textbf{0.5639$\pm$0.0129}&0.3509$\pm$0.0008&\textbf{0.6872$\pm$0.0004}&\textbf{0.7486$\pm$0.0002}&\textbf{0.8060$\pm$0.0003} &\textbf{0.6726$\pm$0.0001} &0.6679$\pm$0.0047\\
        \hline 
        \hline
        GraphMixer$+$DTGB &0.5735$\pm$0.0015&0.3502$\pm$0.0001&0.6872$\pm$0.0001&0.7412$\pm$0.0061&0.7902$\pm$0.0130&0.6720$\pm$0.0084&\textbf{0.6736$\pm$0.0108}\\
        GraphMixer$+$MoMent &\textbf{0.5994$\pm$0.0044} &\textbf{0.3546$\pm$0.0021}&\textbf{0.6876$\pm$0.0002}&\textbf{0.7489$\pm$0.0001} &\textbf{0.8060$\pm$0.0003} &\textbf{0.6728$\pm$0.0002}&0.6729$\pm$0.0037\\
        \hline
        \hline
        DyGFormer$+$DTGB &0.5802$\pm$0.0071&0.3632$\pm$0.0026&0.6877$\pm$0.0002&0.5891$\pm$0.2747&0.7494$\pm$0.0991&0.6737$\pm$0.0058&0.6812$\pm$0.0072 \\
        DyGFormer$+$MoMent &\textbf{0.6154$\pm$0.0079}&\textbf{0.3724$\pm$0.0042} &\textbf{0.6885$\pm$0.0009} &\textbf{0.7492$\pm$0.0001} &\textbf{0.8062$\pm$0.0004} &\textbf{0.6741$\pm$0.0001}&\textbf{0.6845$\pm$0.0004}\\
        \hline
        \hline
        CNEN$+$DTGB &0.5930$\pm$0.0005 &0.3492$\pm$0.0001 &0.6720$\pm$0.0004 &0.7448$\pm$0.0039 &\textbf{0.8063$\pm$0.0001} &0.6569$\pm$0.0002 &0.6560$\pm$0.0001 \\
        CNEN$+$MoMent &\textbf{0.5984$\pm$0.0070} &\textbf{0.3590$\pm$0.0018}&\textbf{0.6875$\pm$0.0001} &\textbf{0.7490$\pm$0.0003} &0.8060$\pm$0.0002&\textbf{0.6743$\pm$0.0028}&\textbf{0.6707$\pm$0.0015}\\
        \Xhline{1pt}
\end{tabular}}
\end{table*}









\subsubsection{Training Configurations} We run all experiments on a single machine with Intel(R) Core(TM) i9-10980XE 3.00GHz CPUs, NVIDIA RTX A6000, and $48$ GB RAM memory. We train the MoMent with Adam optimizer~\cite{kingma2014adam}, with an empirical learning rate of $0.0001$. We provide detailed hyperparameter configurations for MoMent in Appendix~\ref{hyper}.

% We set the batch size $B$ as $256$ and parameter $\alpha$ as $0.2$.


\subsection{Effectiveness Evaluation}\label{sec:5.2}
To evaluate the effectiveness of our MoMent framework, we instantiate the $\operatorname{DGM}(\cdot)$ as our mainstream dynamic graph backbones and compare it with the DTGB framework using two downstream tasks across seven datasets under five evaluation metrics. 

\noindent\textbf{Exp-1: Effectiveness on link prediction.}
We present the link prediction results using AUC and AP metrics. Results in inductive and transductive settings are provided in Table~\ref{tab:3} and Table~\ref{tab:6} of the Appendix, respectively. It is clear that MoMent consistently outperforms DTGB, achieving an average improvement of $5.74\%$ in the transductive setting and $10.52\%$ in the inductive setting. In particular, the substantial improvement in the inductive setting indicates that MoMent is a more robust framework than DTGB, demonstrating superior generalization capabilities. This is attributed to MoMent’s ability to unlock the potential of each modality in DyTAGs and to fuse their contributions through decision-level fusion for node representations. Furthermore, our MoMent framework is compatible with four distinct structure learning techniques, achieving performance gains of up to $9.70\%$ for TGAT, $13.78\%$ for GraphMixer, $33.62\%$ for DyGFormer, and $23.28\%$ for CNEN across two evaluation metrics. These gains stem from our node-centric encoders for temporal and textual modalities, which capture global temporal dependencies and semantic contexts, complementing the local structure learning. This empirical evidence is consistent with Theorem~\ref{theo:1}.



% We report the results of edge classification over seven datasets in terms of three metrics in Table~\ref{tab:2}. We can observe that (1) our MoMent is consistently and significantly better than DTGB, achieving an average of $10\%$, $20\%$, and $30\%$ improvement under Precision, Recall, and F1 Score, respectively. This validates the effectiveness of our multi-modal learning framework for DyTAG representations while confirming the multi-modal nature of DyTAGs. (2) Our MoMent is compatible with four different structure learning strategies, yielding up to $10\%$, $10\%$, $10\%$, and $10\%$ improvements across three metrics for TGAT, GraphMixer, DyGFormer, and CNEN, respectively. That is because our node-centric encoders for temporal modality and textual modality can help capture global temporal dependencies and global semantic correlations, complementary to the structural modality. These empirical results align with our Theorem~\ref{theo:1}. (3) For the complex classification task with a large number of categories (\textit{e.g.}, ICEWS1819), the improvement for our MoMent is limited to only $1\%$. For such a complex task, it is beneficial to comprehensively analyze the edge characteristics and model edge information. For relatively simple classification, MoMent can enhance the quality of node representation, thus improving prediction accuracy.


\begin{figure}[t]
\centering
\subfloat[Stack elec]{
	\includegraphics[width=0.5\linewidth]{figures/Abl_elec_link.pdf}
		\label{fig:3a}
	}
\subfloat[Googlemap CT]{
	\includegraphics[width=0.5\linewidth]{figures/Abl_google_link.pdf}
		\label{fig:3b}
	}
	\caption{Ablation study of Moment on two DyTAG datasets on inductive link prediction.} \label{fig:3}
\end{figure}

\noindent\textbf{Exp-2: Effectiveness on edge classification.} We present the edge classification results across seven datasets under Precision and Recall in Table~\ref{tab:2}. Additional results of the F1 Score are presented in Table~\ref{tab:5} in the Appendix due to the space limit. It is observed that our MoMent framework consistently outperforms DTGB, achieving an average improvement of $4.15\%$ in Precision, $2.58\%$ in Recall, and $3.17\%$ in F1 Score. This validates the effectiveness of our multi-modal modeling in more complex downstream tasks while highlighting the multi-modal nature of DyTAGs. For classification tasks with a large number of categories (above $200$), MoMent's improvement is limited, suggesting that such tasks require a deeper analysis of edge characteristics and enhanced modeling of edge information. For other cases, MoMent significantly improves node representation quality, leading to higher classification accuracy. 

In summary, our node-centric modeling offers a comprehensive perspective for exploring multiple modalities of DyTAGs, which can facilitate global and local guidance for better node representation.




% These results highlight the multi-modal nature of DyTAGs and demonstrate that existing models fail to comprehensively explore temporal and textual modalities, as they rely primarily on local structure learning. 
% Due to the space limit, additional results for transductive link prediction are provided in Table~\ref{tab:6} in the Appendix.


\begin{figure}[t]
\centering
\subfloat[Sx-Askubuntu]{
	\includegraphics[width=0.50\linewidth]{figures/askubuntu_temporal.pdf}
		\label{fig:11a}
	}
\subfloat[Sx-Superuser]{
	\includegraphics[width=0.50\linewidth]{figures/superuser.pdf}
		\label{fig:11b}
	}
	\caption{Ablation study of temporal modality on two general dynamic graphs on inductive link prediction. ``w Temporal" denotes backbone with our temporal node-centric encoder.} \label{fig:11}
\end{figure}

\subsection{Ablation Study}\label{sec:5.3}
We investigate the contribution of different modules within our MoMent for DyTAG representation and assess the effect of temporal modality for general dynamic graph learning.



\noindent\textbf{Exp-3: Module effectiveness of MoMent.}
We conduct four variants to assess their impact on two datasets under the inductive link prediction task using four dynamic graph models. Specifically, we evaluate: (1) removing the auxiliary alignment loss (`w/o Align'), (2) removing the temporal node-centric encoder (`w/o Temporal'), removing the textual node-centric encoder (`w/o Textual'), and removing the multi-modal framework (`w/o MMF'). It is observed from Fig.~\ref{fig:3} that (1) Removing the symmetric alignment loss (`w/o Align') reduces performance by up to $2.72\%$, highlighting its role in providing a soft constraint that aligns temporal and textual modalities, facilitating the generation of cohesive node representations. (2) The `w/o Temporal' variant performs $3.52\%$ worse than MoMent, demonstrating the importance of node-centric temporal modeling in capturing global temporal dependencies for node representations. This also validates that timestamp information—underutilized in the existing DTGB framework—plays a key role in providing global guidance for DyTAG learning. (3) Removing the textual encoder (`w/o Textual') leads to a performance drop of up to $13.42\%$, indicating that node-centric textual modeling captures semantic contexts. This also complements the structural and temporal modalities, as its information exhibits high regularity. (4) The `w/o MMF' variant exhibits the most significant performance degradation, confirming that each modality plays a distinct and complementary role in representation learning and multi-modal modeling is essential for accurate DyTAG representation.



\noindent\textbf{Exp-4: Effect of temporal modality for general dynamic graphs.}
We conduct an ablation study on two general dynamic graphs (\textit{i.e.}, sx-askubuntu and sx-superuser from~\cite{paranjape2017motifs}) to validate the effectiveness of our MoMent design, where we introduce two dynamic graphs in Appendix~\ref{C:2}. Concretely, we add our temporal node-centric encoder to existing dynamic graph models, as there are no node attributes in the two datasets. We show the results of the AUC metric on inductive link prediction in Fig.~\ref{fig:11}. It is clear that our temporal node-centric encoder can significantly improve the performance by an average of $11.31\%$. This suggests that existing dynamic graph models based on edge-centric modeling fail to fully leverage timestamp information since they regard timestamps as the auxiliary information in local structure learning. In contrast, our temporal node-centric encoder can help them capture long-term temporal dependencies beyond localized interactions, such as periodicity, from the view of node behaviors, showing the potential of node-centric modeling for dynamic graph learning.


% it can provide important global guidance for exclusive local observations, helping capture complex long-term temporal dependencies, such as periodicity. Meanwhile, this validates that existing dynamic graph models based on edge-centric modeling fail to fully leverage timestamp information, suggesting the potential of node-centric modeling for dynamic graph learning. 
 


% Our symmetric alignment loss enhances prediction performance by up to $5\%$, as it provides a soft constraint for temporal and textual modality and avoids the generation of disjoint embeddings. (2) The `w/o temporal` variant performs worse than MoMent, with an average decrease of $6\%$. This highlights the importance of our node-centric temporal modality modeling, which captures global temporal dependencies for node representation. Meanwhile, this validates timestamp information is valuable, which is underutilized by existing dynamic graph models. (3) Node-centric textual modeling in MoMent can bring up to $5\%$ improvement over `w/o Textual` variant, as it can capture semantic correlations while complementing with other two modalities. (3) The `w/o MMF` variant significantly achieves inferior prediction performance. This confirms the importance of multi-modal learning for DyTAGs.









\begin{figure}
    \centering
    \subfloat[Enron]{
	\includegraphics[width=0.48\linewidth]{figures/distance_Enron_GraphMixer.png}
		\label{fig:3a}
	}
\subfloat[Googlemap CT]{
	\includegraphics[width=0.48\linewidth]{figures/distance_Googlemap_CT_GraphMixer.png}
		\label{fig:3b}
	}
    \caption{Temporal-textual token distance (red) and dev accuracy (purple) on two DyTAGs across training epochs.}
    \label{fig:4}
\end{figure}





% As evidenced in Section~\ref{sec:5.3}, removing our symmetric alignment loss from MoMent leads to a significant performance drop. This highlights the importance of Jensen-Shannon divergence in aligning temporal and textual tokens, ensuring better integration of multi-modal information. 
\subsection{Effect of Modality Alignment}
\noindent\textbf{Exp-5: Alignment vs. Accuracy.}
Here, we investigate the effectiveness of temporal-textual alignment by studying how the alignment evolves during training, where we use the GraphMixer as the backbone over the Enron and Googlemap CT datasets. Specifically, we calculate the normalized Cosine distance between temporal and textual tokens (red line) and compare it to the corresponding development (dev) accuracy (purple line) across training epochs, as illustrated in
Fig.~\ref{fig:4}. This shows a clear inverse relationship between
the two curves: as the temporal and textual token distance decreases, accuracy improves. Notably, on the GoogleMap CT dataset, token distance initially increases and then decreases, while accuracy follows the opposite trend. This suggests that
better alignment between temporal and textural tokens contributes directly to higher accuracy, highlighting the significance of temporal-semantic consistency in multi-modal modeling.




\begin{figure}[t]
\centering
\subfloat[GraphMixer]{
	\includegraphics[width=0.48\linewidth]{figures/Enron_Token_KDE_GraphMixer.pdf}
		\label{fig:5a}
	}
\subfloat[DyGFormer]{
	\includegraphics[width=0.48\linewidth]{figures/Enron_Token_KDE_DyG.pdf}
		\label{fig:5b}
	}
	\caption{KDE distribution of tokens from three modalities using two backbones on the Enron dataset.} \label{fig:5}
\end{figure}

\noindent\textbf{Exp-6: Visualization of tokens from three modalities.}
We further visualize the modality-specific tokens at convergence, using GraphMixer and DyGFormer as the backbone over the Enron dataset. Fig.~\ref{fig:5} shows that (1) Compared to the original feature distribution in Fig.~\ref{fig:1b}, the learned tokens redistribute the feature space, aligning modalities while preserving their distinct contributions. This is consistent with our Theorem~\ref{theo:2}. (2) The structural tokens become more spread out, allowing greater flexibility in capturing complex relationships within DyTAGs. (3) The temporal tokens transition from high variability in the original features to a more compact form, indicating that MoMent effectively refines and extracts key temporal patterns.


In summary, these results confirm that MoMent effectively aligns and integrates modalities, making them complementary for comprehensive and cohesive node representations for DyTAGs.

\section{Conclusion}
In this paper, we investigate the multi-modal characteristics of dynamic text-attributed graphs and unlock multi-modal potentials from a node-centric perspective for better node representation. To this end, we propose MoMent, a model-agnostic multi-modal framework based on decision-level fusion, which fully leverages multi-modal information in DyTAGs while effectively aligning modalities with distinct distributions. MoMent presents two node-centric encoders to model global temporal and textual dependencies while utilizing existing dynamic graph models for local structure learning, generating three modality-specific tokens. To prevent a disjoint latent space, we propose a symmetric alignment loss based on Jensen-Shannon divergence, ensuring global temporal-semantic consistency with a theoretical guarantee. Then, MoMent fuses modality-specific tokens via a lightweight adaptor, generating comprehensive and cohesive node representations. We further prove that our multi-modal modeling offers greater discriminative power than exclusive edge-centric modeling based on an information gain bound. Extensive experiments validate the effectiveness of MoMent, and visualization results highlight MoMent’s capability in multi-modal modeling for DyTAGs.






%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
% \newpage
\bibliographystyle{ACM-Reference-Format}
\bibliography{sample-base}


%%
%% If your work has an appendix, this is the place to put it.
\appendix
% \section{Reproducibility} 
% The source codes of the proposed MoMent with four mainstream dynamic graph backbones are available at \textcolor{blue}{\url{https://anonymous.4open.science/r/MoMent-C33D}}. 

% Additionally, the source codes of baseline DTGB framework and backbone CNEN are available at~\url{https://github.com/zjs123/DTGB} and~\url{https://github.com/ckpassenger/DyGLib_CNEN/tree/CNEN}, respectively.

\section{Additional Theoretical Analysis}

\subsection{Proof of Theorem~\ref{theo:1}}\label{proof:1}
\begin{proof}
First, we assume that $\operatorname{FeedForward}(\cdot)$ in our MoMent can help preserve the information from each modality, the mutual information between the multi-modal representation $\mathbf{Z}$ and the target $\mathbf{Y}$ satisfies:
\begin{equation}\label{A:1}
     I(\mathbf{Z};\mathbf{Y}) = I(\mathbf{Z}_t,\mathbf{Z}_s,\mathbf{Z}_x;\mathbf{Y}).
\end{equation}
According to the chain rule of mutual information, we can decompose the $I(\mathbf{Z}_t,\mathbf{Z}_s,\mathbf{Z}_x;\mathbf{Y})$ as
\begin{equation}\label{A:2}
    I(\mathbf{Z}_t, \mathbf{Z}_s, \mathbf{Z}_x;\mathbf{Y}) = I(\mathbf{Z}_s;\mathbf{Y}) + I(\mathbf{Z}_t, \mathbf{Z}_x;\mathbf{Y} \mid \mathbf{Z}_s),
\end{equation}
where $I(\mathbf{Z}_s;\mathbf{Y})$ quantifies the information provided by the modality $\mathbf{Z}_s$ about $\mathbf{Y}$ and $I(\mathbf{Z}_t, \mathbf{Z}_x;\mathbf{Y} \mid \mathbf{Z}_s)$ quantities the additional information provided by modalities $\mathbf{Z}_t$ and $\mathbf{Z}_x$ when $\mathbf{Z}_s$ is given.

Then, we aim to compute the lower bound for the term $I(\mathbf{Z}_t, \mathbf{Z}_x;\mathbf{Y} \mid \mathbf{Z}_s)$. Here, we use the fundamental properties of entropy and we can obtain
\begin{align}
I(\mathbf{Z}_t, \mathbf{Z}_x;\mathbf{Y} \mid \mathbf{Z}_s) &= H(\mathbf{Y} \mid \mathbf{Z}_s) - H(\mathbf{Y} \mid \mathbf{Z}_t, \mathbf{Z}_x, \mathbf{Z}_s) \label{A:3}\\
                        &= H(\mathbf{Z}_t, \mathbf{Z}_x \mid \mathbf{Z}_s) - H(\mathbf{Z}_t, \mathbf{Z}_x \mid \mathbf{Y}, \mathbf{Z}_s).\label{A:4}
\end{align}


Here, the conditional mutual information $I(\mathbf{Z}_t,\mathbf{Z}_x;\mathbf{Y} \mid \mathbf{Z}_s)$ could be as high as the minimum of the conditional entropies $H(\mathbf{Y}\mid \mathbf{Z}_s)$ and $H(\mathbf{Z}_t,\mathbf{Z}_x\mid \mathbf{Z}_s)$. Therefore, considering any unavoidable loss in information during the integration process, 
there exists a constant $\beta\in (0,1]$, such that
\begin{equation}\label{A:5}
I(\mathbf{Z}_t,\mathbf{Z}_x;\mathbf{Y} \mid \mathbf{Z}_s) \ge \beta\, \min\{H(\mathbf{Y}\mid \mathbf{Z}_s),\, H(\mathbf{Z}_t,\mathbf{Z}_x\mid \mathbf{Z}_s)\}.
\end{equation}

%Here, the conditional mutual information $I(\mathbf{Z}_t,\mathbf{Z}_x;\mathbf{Y} \mid \mathbf{Z}_s)$ could be as high as the minimum of the conditional entropies $H(\mathbf{Y}\mid \mathbf{Z}_s)$ and $H(\mathbf{Z}_t,\mathbf{Z}_x\mid \mathbf{Z}_s)$. $\beta$ accounts for any unavoidable loss in information during the integration process.

Substituting the lower bound from Eq.~\eqref{A:5} into Eq.~\eqref{A:2}, we obtain
\begin{align}
    I(\mathbf{Z};\mathbf{Y}) &= I(\mathbf{Z}_s;\mathbf{Y}) + I(\mathbf{Z}_t, \mathbf{Z}_x;\mathbf{Y} \mid \mathbf{Z}_s)\\
    & \ge I(\mathbf{Z}_s;\mathbf{Y}) + \beta\, \min\{H(\mathbf{Y}\mid \mathbf{Z}_s),\, H(\mathbf{Z}_t,\mathbf{Z}_x\mid \mathbf{Z}_s)\}.
\end{align}

This completes our proof, establishing a lower bound on the information gain achieved through multi-modal modeling.
\end{proof}










\begin{table*}[tbp]
	\caption{Performance on link prediction under AUC and AP in the transductive setting. The best results are marked in bold.}
	\label{tab:6}
	\centering
	\resizebox{\linewidth}{!}{
 \begin{tabular}{|c|c|c|c|c|c|c|c|}
        \Xhline{1pt}
        \rowcolor{gray!40} Approach & Enron & ICEWS1819 & Googlemap CT & Stack elec & Stack ubuntu & Amazon movies  & Yelp\\
        \hline
        \hline
        \multicolumn{8}{|c|}{\textbf{AUC under Transductive Setting}}\\
        \hline
        TGAT$+$DTGB &0.9681$\pm$0.0026&0.9904$\pm$0.0039&0.9049$\pm$0.0071&0.9709$\pm$0.0014&0.9490$\pm$0.0018&0.9064$\pm$0.0014&0.9487$\pm$0.0029 \\
        TGAT$+$MoMent &\textbf{0.9974$\pm$0.0011} &\textbf{0.9957$\pm$0.0028} &\textbf{0.9801$\pm$0.0050} &\textbf{0.9886$\pm$0.0034} &\textbf{0.9791$\pm$0.0079} &\textbf{0.9871$\pm$0.0009}&\textbf{0.9699$\pm$0.0057}\\
        \hline 
        GraphMixer$+$DTGB &0.9567$\pm$0.0013 &0.9863$\pm$0.0024 &0.8095$\pm$0.0014 &0.9673 $\pm$0.0011 &0.9494$\pm$0.0028 &0.8894$\pm$0.0008 &0.8927$\pm$0.0021\\
        GraphMixer$+$MoMent &\textbf{0.9898$\pm$0.0002} &\textbf{0.9922$\pm$0.0001} &\textbf{0.9821$\pm$0.0014}&\textbf{0.9905$\pm$0.0154}&\textbf{0.9859$\pm$0.0007}&\textbf{0.9898$\pm$0.0002}&\textbf{0.9791$\pm$0.0020}\\
        \hline
        DyGFormer$+$DTGB &0.9779$\pm$0.0014&0.9888$\pm$0.0015&0.8207$\pm$0.0018&0.9798$\pm$0.0006&0.9526$\pm$0.0035&0.9100$\pm$0.0006&0.9407$\pm$0.0010\\
        DyGFormer$+$MoMent &\textbf{0.9978$\pm$0.0006}&\textbf{0.9922$\pm$0.0001}&\textbf{0.9809$\pm$0.0005}&\textbf{0.9894$\pm$0.0003} &\textbf{0.9866$\pm$0.0049}&\textbf{0.9832$\pm$0.0011}&\textbf{0.9747$\pm$0.0031}\\
        \hline
        CNEN$+$DTGB &0.9888$\pm$0.0006&0.9893$\pm$0.0000&0.8120$\pm$0.0039& 0.9918$\pm$0.0000&0.9928$\pm$0.0000& 	0.8736$\pm$0.0010&0.9671$\pm$0.0009\\
        CNEN$+$MoMent &\textbf{0.9917$\pm$0.0021}&\textbf{0.9918$\pm$0.0001}&\textbf{0.9750$\pm$0.0027}&\textbf{0.9947$\pm$0.0050}&\textbf{0.9980$\pm$0.0003}&\textbf{0.9896$\pm$0.0006}&\textbf{0.9872$\pm$0.0004}\\
        \hline
        \hline
        \multicolumn{8}{|c|}{\textbf{AP under Transductive Setting}}\\
        \hline
        TGAT$+$DTGB &0.9668$\pm$0.0026&0.9908$\pm$0.0032&0.9002$\pm$0.0019&0.9646$\pm$0.0005&0.9352$\pm$0.0012&0.9065$\pm$0.0016&0.9457$\pm$0.0025\\
        TGAT$+$MoMent &\textbf{0.9975$\pm$0.0005}&\textbf{0.9964$\pm$0.0038} &\textbf{0.9849$\pm$0.0051} &\textbf{0.9884$\pm$0.0036} &\textbf{0.9790$\pm$0.0043} &\textbf{0.9889$\pm$0.0003}&\textbf{0.9793$\pm$0.0034}\\
        \hline 
        GraphMixer$+$DTGB &0.9559$\pm$0.0027&0.9871$\pm$0.0034&0.8072$\pm$0.0010&0.9591$\pm$0.0009&0.9416$\pm$0.0046&0.8906$\pm$0.0006&0.8883$\pm$0.0026\\
        GraphMixer$+$MoMent &\textbf{0.9831$\pm$0.0002} &\textbf{0.9942$\pm$0.0001} &\textbf{0.9865$\pm$0.0008} &\textbf{0.9867$\pm$0.0216} &\textbf{0.9866$\pm$0.0014}&\textbf{0.9970$\pm$0.0002}&\textbf{0.9809$\pm$0.0026}\\
        \hline
        DyGFormer$+$DTGB &0.9804$\pm$0.0015&0.9901$\pm$0.0018&0.8183$\pm$0.0038&0.9819$\pm$0.0010&0.9431$\pm$0.0008&0.9097$\pm$0.0003&0.9391$\pm$0.0011\\
        DyGFormer$+$MoMent &\textbf{0.9978$\pm$0.0001}&\textbf{0.9989$\pm$0.0008}&\textbf{0.9821$\pm$0.0003}&\textbf{0.9879$\pm$0.0002}&\textbf{0.9865$\pm$0.0063}&\textbf{0.9856$\pm$0.0004}&\textbf{0.9757$\pm$0.0022}\\
        \hline
        CNEN$+$DTGB &0.9877$\pm$0.0002& 0.9900$\pm$0.0000&	0.8168$\pm$0.0034& 0.9919$\pm$0.0000& 0.9933$\pm$0.0000& 0.8783$\pm$0.0006& 0.9707$\pm$0.0008\\
        CNEN$+$MoMent &\textbf{0.9914$\pm$0.0024}&\textbf{0.9949$\pm$0.0001}&\textbf{0.9872$\pm$0.0031}&\textbf{0.9938$\pm$0.0063}&\textbf{0.9982$\pm$0.0004}&\textbf{0.9895$\pm$0.0006}&\textbf{0.9874$\pm$0.0008}\\
       \Xhline{1pt}
\end{tabular}}
\end{table*}


\begin{table*}
    \caption{Performance on edge classification in terms of F1 Score. The best results are marked in bold.}\label{tab:5}
    \centering
    \resizebox{\linewidth}{!}{
 \begin{tabular}{|c|c|c|c|c|c|c|c|}
        \Xhline{1pt}
        \rowcolor{gray!40} Approach & Enron & ICEWS1819 & Googlemap CT & Stack elec & Stack ubuntu & Amazon movies  & Yelp\\
        \hline
        \hline
        \multicolumn{8}{|c|}{\textbf{F1 Score}}\\
        \hline
        TGAT$+$DTGB &0.5519$\pm$0.0028 &\textbf{0.2908$\pm$0.0008} &0.6225$\pm$0.0015 &\textbf{0.6496$\pm$0.0032} &0.7201$\pm$0.0013 &0.5917$\pm$0.0051 &0.6219$\pm$0.0103\\
        TGAT$+$MoMent &\textbf{0.5609$\pm$0.0156} &0.2901$\pm$0.0015&\textbf{0.6308$\pm$0.0038}&0.6476$\pm$0.0013 &\textbf{0.7224$\pm$0.0006} & \textbf{0.6024$\pm$0.0010} &\textbf{0.6316$\pm$0.0017}\\
        \hline 
        \hline
        GraphMixer$+$DTGB &0.5507$\pm$0.0019&0.2903$\pm$0.0008 &0.6185$\pm$0.0005 &0.6412$\pm$0.0005 &0.7214$\pm$0.0014 &\textbf{0.5991$\pm$0.0064} &0.6234$\pm$0.0174 \\
        GraphMixer$+$MoMent &\textbf{0.5934$\pm$0.0055} &\textbf{0.2929$\pm$0.0043} &\textbf{0.6208$\pm$0.0021} &\textbf{0.6495$\pm$0.0031} &\textbf{0.7229$\pm$0.0004}&0.5986$\pm$0.0003&\textbf{0.6248$\pm$0.0024}\\
        \hline
        \hline
        DyGFormer$+$DTGB &0.5604$\pm$0.0063 &0.3079$\pm$0.0027 &0.6196$\pm$0.0008 &0.4860$\pm$0.2686 &0.7033$\pm$0.0294 &0.6050$\pm$0.0084 &0.6359$\pm$0.0057 \\
        DyGFormer$+$MoMent &\textbf{0.6090$\pm$0.0069} &\textbf{0.3186$\pm$0.0055} &\textbf{0.6222$\pm$0.0019} &\textbf{0.6494$\pm$0.0017}&\textbf{0.7230$\pm$0.0015}&\textbf{0.6059$\pm$0.0012}&\textbf{0.6368$\pm$0.0016}\\
        \hline
        \hline
        CNEN$+$DTGB &0.5877$\pm$0.0044&0.2952$\pm$0.0011&0.5996$\pm$0.0001&0.6442$\pm$0.0031&0.7207$\pm$0.0001&0.5588$\pm$0.0005&0.5971$\pm$0.0013\\
        CNEN$+$MoMent &\textbf{0.6048$\pm$0.0038} &\textbf{0.3021$\pm$0.0024} &\textbf{0.6239$\pm$0.0012}&\textbf{0.6519$\pm$0.0037} &\textbf{0.7244$\pm$0.0016} &\textbf{0.6059$\pm$0.0021} &\textbf{0.6171$\pm$0.0031}\\
        \Xhline{1pt}
        \end{tabular}}
\end{table*}


\begin{figure*}[t]
    \subfloat[ICEWS1819]{\includegraphics[width=0.25\linewidth]{figures/ICEWS1819_KDE_Norm.pdf}}
    \subfloat[Googlemap CT]{\includegraphics[width=0.25\linewidth]{figures/Googlemap_CT_KDE_Norm.pdf}}
    \subfloat[Stack elec]{\includegraphics[width=0.25\linewidth]{figures/Stack_elec_KDE_Norm.pdf}}
    \subfloat[Stack ubuntu]{\includegraphics[width=0.25\linewidth]{figures/Stack_ubuntu_KDE_Norm.pdf}}
    \caption{KDE distribution of three modalities on four real dynamic text-attributed graphs, where `X.O' denotes the original features.}
    \label{fig:A7}
\end{figure*}


\subsection{Proof of Theorem~\ref{theo:2}}\label{proof:2}
\begin{proof}
We begin by recalling the definition of conditional
mutual information and conditional entropy for temporal modality:
\begin{align}
    I(\mathbf{Z}_t;\mathbf{Y}|\mathbf{Z}_s) &= \mathbb{E}_{p(\mathbf{Z}_t,\mathbf{Y},\mathbf{Z}_s)}[\log \frac{p(\mathbf{Z}_t,\mathbf{Y}|\mathbf{Z}_s)}{p(\mathbf{Z}_t|\mathbf{Z}_s)p(\mathbf{Y}|\mathbf{Z}_s)}],\\
    H(\mathbf{Z}_t|\mathbf{Z}_s) &= -\mathbb{E}_{p(\mathbf{Z}_t,\mathbf{Z}_s)}[\log p(\mathbf{Z}_t|\mathbf{Z}_s)],\\
    H(\mathbf{Z}_t|\mathbf{Y},\mathbf{Z}_s) &= -\mathbb{E}_{p(\mathbf{Z}_t,\mathbf{Y},\mathbf{Z}_s)}[\log p(\mathbf{Z}_t|\mathbf{Y},\mathbf{Z}_s)].
\end{align}

Correspondingly, we expand our JSD term conditioned on $\mathbf{Z}_s$ and obtain
\begin{align}
\text{JSD}(\mathbf{Z}_t,\mathbf{Z}_x) &= \sum_i p(\mathbf{Z}_t^i|\mathbf{Z}_s)\log\frac{p(\mathbf{Z}_t^i|\mathbf{Z}_s)}{p(\mathbf{Z}_x^i|\mathbf{Z}_s)} \\
&+ \sum_i p(\mathbf{Z}_x^i|\mathbf{Z}_s)\log\frac{p(\mathbf{Z}_x^i|\mathbf{Z}_s)}{p(\mathbf{Z}_t^i|\mathbf{Z}_s)}.
\end{align}

Our goal is to bound the difference between these two conditional mutual information:
\begin{equation}
\begin{split}&|I(\mathbf{Z}_{t};\mathbf{Y}|\mathbf{Z}_{s})-I(\mathbf{Z}_{x};Y|\mathbf{Z}_{s})|\\ &=|(H(\mathbf{Z}_{t}|\mathbf{Z}_{s})-H(\mathbf{Z}_{t}|\mathbf{Y},\mathbf{Z}_{s}))-(H(\mathbf{Z}_{x}|\mathbf{Z}_{s})-H(\mathbf{Z}_{x}|\mathbf{Y},\mathbf{Z}_{s}))|\\ &=|(H(\mathbf{Z}_{t}|\mathbf{Z}_{s})-H(\mathbf{Z}_{x}|\mathbf{Z}_{s}))-(H(\mathbf{Z}_{t}|\mathbf{Y},\mathbf{Z}_{s})-H(\mathbf{Z}_{x}|\mathbf{Y},\mathbf{Z}_{s}))|.
\end{split}
\end{equation}

By applying the triangle inequality, we have
\begin{equation}
\begin{split}
    &|I(\mathbf{Z}_{t};\mathbf{Y}|\mathbf{Z}_{s})-I(\mathbf{Z}_{x};Y|\mathbf{Z}_{s})| \\
    &\leq |H(\mathbf{Z}_{t}|\mathbf{Z}_{s})-H(\mathbf{Z}_{x}|\mathbf{Z}_{s})|+|H(\mathbf{Z}_{t}|\mathbf{Y},\mathbf{Z}_{s})-H(\mathbf{Z}_{x}|\mathbf{Y},\mathbf{Z}_{s})|.
\end{split}
\end{equation}

Next, we assume that the conditional entropy function 
$H\left(\cdot \mid \mathbf{Z}_s\right)$ is Lipschitz continuous with respect to its input distribution when measured in terms of the JSD, then there exists a constant $C_1 > 0$ such that 
\begin{equation}
|H(\mathbf{Z}_t|\mathbf{Z}_s) - H(\mathbf{Z}_x|\mathbf{Z}_s)| \leq C_1 \text{JSD}(\mathbf{Z}_t,\mathbf{Z}_x) \leq C_1 \mathcal{L}_{\text{align}},
\end{equation}

Similarly, there exists a constant $C_2 > 0$ such that
\begin{equation}
|H(\mathbf{Z}_t|\mathbf{Y},\mathbf{Z}_s) - H(\mathbf{Z}_x|\mathbf{Y},\mathbf{Z}_s)| \leq C_2  \text{JSD}(\mathbf{Z}_t,\mathbf{Z}_x|\mathbf{Y}) \leq C_2 \mathcal{L}_{\text{align}}.
\end{equation}

Then, the bound is tight when:
\begin{equation}
\text{JSD}(\mathbf{Z}_t,\mathbf{Z}_{x}) \approx \text{JSD}(\mathbf{Z}_{t},\mathbf{Z}_{x}|\mathbf{Y}).
% \text{SymKL}(Z_t,Z_x|Y)
\end{equation}

Thus, the final bound is
\begin{equation}
|I(\mathbf{Z}_t;\mathbf{Y}|\mathbf{Z}_s) - I(\mathbf{Z}_x;\mathbf{Y}|\mathbf{Z}_s)| \leq (C_1 + C_2) \mathcal{L}_{\text{align}}.
\end{equation}

This implies that the difference in predictive power between temporal and textual modalities is controlled by the alignment loss. Hence, the proof is complete.
\end{proof}


\section{Additional Visualizations and Experiments}

\subsection{Additional Visualization of Modalities of DyTAGs}
To further support our analysis of the multi-modal nature in DyTAGs, we provide additional KDE distribution visualizations across four datasets in Fig.~\ref{fig:A7}. The three modalities show their unique characteristics, further confirming our analysis in the Introduction and Preliminary section.


\subsection{Additional Dataset Description}\label{C:2}
To assess the effectiveness of our MoMent design in general dynamic graphs, we collect two dynamic graphs from~\cite{paranjape2017motifs} and provide detailed statistics in Table~\ref{tab:A6}.
\begin{table}[t]
  \caption{Dataset statistics for general dynamic graphs. Node/edge att. means the vector-based attributes.}
  \label{tab:A6}
  \centering
  \small
  \resizebox{\linewidth}{!}{
  \begin{tabular}{|c|c|c|c|c|c|}
    \Xhline{1pt}
     \rowcolor{gray!40}Dataset & \# Nodes & \# Edges  & Time span & Node/edge att.\\ 
    \hline
    \hline
     Sx-Askubuntu &$159,316$ & $964,437$ & $2,613$ days
 &$0/0$\\
    Sx-Superuser & $194,085$ & $1,443,339$  & 	$2,773$ days
 & $0/0$\\
    \Xhline{1pt}
    \end{tabular}}
\end{table}


\subsection{Hyperparameter Configuration}
Here, we present the hyperparameter configurations of our MoMent. These configurations remain unchanged across two downstream tasks and all datasets in our experiments. The details are provided as follows.

\begin{itemize}
    \item Dimension of embeddings: $768$ for the node representation dimension; $768$ for the structural token dimension; $128$ for the temporal token dimension; $128$ for the textual token dimension.
    \item Configurations for node-centric encoders: $8$ for attention heads; $2$ for network layers; $256$ for hidden dimension; $0.1$ for Dropout rate.
    \item Configurations for TGAT: $2$ for graph attention heads; $2$ for network layers.
    \item Configurations for GraphMixer: $2$ for MLP-Mixer layers; $2000$ for time gap $T$.
    \item Configurations for DyGFormer: $50$ for neighbor co-occurrence encoding; $2$ for attention heads; $2$ for network layers.
    \item Configurations for CNEN: $2$ for network layers; $4$ for the input sequence length; $32$ for the memory dimension.
    \item Configurations for loss: $0.2$ for the hyperparameter $\alpha$.
\end{itemize}

\subsection{Additional Main Experimental Results}
We report additional experimental results of transductive link prediction and edge classification in Tables~\ref{tab:6} and~\ref{tab:5}, where result analysis is provided in Section~\ref{sec:5.2}.


% \subsection{Ablation Study over Dynamic Graphs}\label{sec:AD1}
\begin{figure}
    \centering
    \subfloat[Googlemap CT]{\includegraphics[width=0.5\linewidth]{figures/parameter_google.pdf}}
    \subfloat[Stack elec]{\includegraphics[width=0.5\linewidth]{figures/parameter_elec.pdf}}
    \caption{Heatmap values indicate \% gain in inductive AUC of MoMent
for hyperparameter $\alpha$ over two datasets.}
    \label{fig:A8}
\end{figure}


\subsection{Parameter Sensitivity Analysis }\label{hyper}
\noindent\textbf{Exp-7: Effect of the Hyperparameter $\alpha$.} We conduct experiments to evaluate the impacts of hyperparameter $\alpha$ in our MoMent, which is used for balancing the main loss with our auxiliary loss. We vary the $\alpha$ among $\{0.1, 0.2, 0.3, 0.4, 0.5\}$, select three dynamic graph backbones, and report AUC results on the inductive link prediction on two datasets (\textit{i.e.}, Googlemap CT and Stack elec) in Fig.~\ref{fig:A8}. It is clear that increasing the weight of the auxiliary loss improves model performance by emphasizing modality consistency. Additionally, MoMent remains robust to $\alpha$ when set above $0.2$, demonstrating stability across different settings. Throughout the experiments, we set $\alpha$ as $0.2$.



% \subsection{Parameter Sensitivity Analysis for model depth}

% \subsubsection{The Effect of Timestamps on Dynamic graphs}

\end{document}
\endinput
%%
%% End of file `sample-sigconf.tex'.


