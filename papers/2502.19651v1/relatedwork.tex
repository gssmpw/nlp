\section{Related Work}
\subsection{Dynamic Text-Attributed Graph Learning}
The Dynamic Text-Attributed Graph (DyTAG) is a new concept, recording the evolving dynamic structures and text attributes simultaneously, which is critical for many real-world applications. Behind this new concept, there is a new and comprehensive benchmark (DTGB)~\cite{zhang2024dtgb} for DyTAG learning. Concretely, DTGB first introduced DyTAG datasets from multiple domains and standardized the evaluation process. Then, it presented an intuitive framework that learned raw text attributes of nodes and edges by pre-trained language models and then integrated them into existing dynamic graph backbones. DTGB showed its effectiveness across various downstream tasks compared to dynamic graph backbones without text attributes, suggesting the significance of textual information for node representation over DyTAGs. Although rich text attributes are employed to learn node representations of DyTAGs, DTGB, following the line of edge-centric modeling, focuses exclusively on local learning based on the dynamic structure, where timestamps and edge/node attributes are merely treated as an optional supplement for structure learning, leading to the underexploration of temporal and textual modalities.


The processing of timestamps and text attributes by the DTGB and our proposed MoMent can be abstracted as \textit{feature-level fusion} and \textit{decision-level fusion}. Feature-level fusion integrates modalities into a single vector at the input level and processes the fused features through edge-centric modeling, \textit{i.e.}, dynamic graph models. In contrast, decision-level fusion allows each modality to run independently through individual models and fuses the outputs of each modality at the output level. Compared to feature-level fusion, decision-level fusion maximumly unlocks the potential of each modality, resulting in a comprehensive DyTAG representation.





% neglects the different characteristics and significance of multi-modal information, leading to sub-optimal performance. Furthermore, DTGB, following the line of dynamic graph modeling, focuses exclusively on local learning based on the dynamic structure, where timestamps and edge/node features are merely treated as an optional supplement for structure learning, leading the underexplored for these rich information and overlooking the global contextual learning and leading to poor model robustness. 


% trivedi2019dyrep,xu2020inductive,congwe,wang2021tcl,

\subsection{Continuous-Time Dynamic Graph Learning}
Dynamic graph models aim to process the dynamic structures with/without vector-based edge/node attributes, which have been extensively studied due to their practical applicability. Existing dynamic graph models~\cite{kumar2019predicting,trivedi2019dyrep,DBLP:conf/wsdm/SankarWGZY20,tgn,wang2021inductive,souza2022provably,luo2022neighborhood,DBLP:journals/pvldb/LiSCY23,ma2024temporal,ji2023community,DBLP:conf/aaai/LiYZC0ZTWM23,xu2024scalable,DBLP:conf/icde/000200O024,DBLP:conf/kdd/ZhongVYA24,DBLP:conf/kdd/0003MY024,wu2024feasibility,zhu2024topology} typically focused on complex dynamic structure learning, where some~\cite{kumar2019predicting,tgn} leveraged sequence models (\textit{e.g.}, Gated Recurrent Unit (GRU)) to capture temporal dependencies. Concretely, TGAT~\cite{xu2020inductive} extended the graph attention mechanism to dynamic graphs, where it employed a time encoding function to help temporal-topological neighbors. Subsequently, co-neighbor encoding techniques~\cite{yu2023towards,DBLP:conf/iclr/TianQG24,cheng2024co,zhang2024towards} were studied to record neighbor co-occurrence and capture complex structural correlations. Additionally, researchers explored simple neural models for learning complex dynamic structure changes, such as MLP-Mixer~\cite{congwe} and Frequency-enhanced MLP~\cite{DBLP:conf/iclr/TianQG24}. However, existing approaches typically regard timestamps as the supplement during local structure learning, which is edge-centric modeling with feature-level fusion. They fail to provide global guidance from the temporal contextual perspective, leading to underexploration for timestamps. Beyond dynamic structure modeling, significant efforts~\cite{wen2022trend,DBLP:conf/icml/GravinaLGBG24,DBLP:conf/iclr/SuZ024,tian2024latent,ma2024temporal} have been devoted to capturing long-range (or intricate) temporal dynamics through various techniques, including the memory module and information propagation. However, the above approaches are limited in capturing the complicated semantics of text attributes as they fail to process raw text attributes of nodes and edges.

% For more details, please refer to the survey~\cite{}. 

% capturing temporal dependencies and encoding complex neighbor relations. Concretely, they leveraged sequence models (\textit{e.g.}, Gated Recurrent Unit (GRU)) to capture temporal dependencies based on node status information and designed different co-neighbor encoding techniques to model complex dynamic structures for node representation. 


\subsection{Large Language Models for Graphs}
Large Language Models (LLMs) have demonstrated remarkable reasoning capabilities across various domains, including graph management and analysis. Recent studies have begun exploring the potential of LLMs in processing static Text-Attributed Graphs (TAGs)~\cite{zhao2023graphtext,tian2024graph,yan2023comprehensive,pan2024distilling,tang2024graphgpt,fang2024gaugllm,10.1145/3580305.3599833}. Among them, LLMs served various roles—acting as enhancers, encoders, or predictors—by leveraging static graph structures and rich text descriptions. However, existing approaches focus on node representation generation or prediction tasks within a static setting, failing to handle timestamp information and making them inapplicable to DyTAG learning. Furthermore, recent research~\cite{chen2024exploring,huang2024gnns,heharnessing} has been dedicated to examining the capabilities and boundaries of LLMs in graph management and analysis, but our work follows a different path—analyzing DyTAGs themselves to reveal the distinct characteristics of multiple modalities for better node representations, thereby supporting various downstream tasks, \textit{e.g.}, link prediction and edge classification.