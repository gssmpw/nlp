\begin{table}[t]
\centering
\setlength{\tabcolsep}{3pt} % Reduce column spacing
\renewcommand{\arraystretch}{1.1} % Increase row spacing for readability
\begin{tabular}{@{}lcccc@{}}
\toprule
\multicolumn{1}{c}{\multirow{2}{*}{\textbf{Time}}}
& \multicolumn{2}{c}{\textbf{Llama 3.1 (8B)}} & \multicolumn{2}{c}{\textbf{GPT2-XL (1.5B)}} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5}
& \textbf{H100} & \textbf{A100} & \textbf{H100} & \textbf{A100} \\
\midrule
CPU Freezing (\textit{s}) & 26.85 & 24.31 & 30.30 & 25.94 \\
CPU Frozen (\textit{s}) & 50.54 & 122.11 & 58.50 & 104.85 \\
CPU Mem. dump (\textit{s}) & 48.88 & 119.79 & 56.75 & 102.27 \\
CPU Mem. write (\textit{s}) & 47.24 & 117.60 & 54.80 & 99.81 \\ \midrule
\sys Checkpoint (\textit{s}) & 77.40 & 146.43 & 88.81 & 130.81 \\
\sys Restore  (\textit{s}) & 38.83 & 98.91 & 43.43 & 145.14 \\
\midrule
GPU memory & 54 GB & 54 GB & 58 GB & 58 GB \\
\bottomrule
\end{tabular}
\vspace{-.5em}
\caption{\sys checkpoint and restore times for Llama 3.1 and GPT-2 model training on a single H100 80GB and A100 80GB GPU. While H100 architectural improvements and faster memory bandwidth enable faster checkpoint/restore, the additional CPU cores and host memory likely also contribute to the overall speedup. A direct comparison with A100 is not possible due to these differences.}
\label{tab:combined-checkpoint-restore-times}
\end{table}