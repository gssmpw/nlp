
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discussion}~\label{sec:discussion}%
\sys is a novel technique for fully transparent checkpointing of GPU applications implemented with CUDA and ROCm. Our evaluation results show that \sys can create unified CPU-GPU snapshots for large models in single- and multi-GPU setups without steady-state performance overhead. Our evaluation results further demonstrate that \sys scales linearly with the number of GPU devices for data-parallel workloads, enabling efficient checkpointing with large-scale applications.

\stitle{Deterministic Restore.}
In contrast to previous work, which requires validation of the replayed GPU device API calls~\cite{gupta2024just}, \sys relies on a locking mechanism that ensure the execution of tasks is suspended before creating a checkpoint. This guarantees consistent CPU-GPU snapshots and deterministic restore operations, during which both CPU and GPU state are restored before the application resumes execution.

\stitle{Real-World Deployments.}
The proposed GPU checkpointing mechanism has been successfully implemented in several production systems, including MemVerge~\cite{wu2025memory} and Modal~\cite{belotti2025memory}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related Work}~\label{sec:related-work}%
Checkpoint/Restore has recently received significant attention in the context of AI workloads as a way to quickly recover from faults during model training (either in the GPU or in the CPU), and to better utilize hardware when multiple users content for the same hardware. The latter is becoming increasingly relevant with the introduction of LLM Agents in which GPU devices may need to wait for external agents to return before the LLM running on the GPU could proceed~\cite{nvidia2023introduction}. This section summarizes recent research efforts and how they have focused on improving different aspects of Checkpoint Restore, and how it compares to \sys.

\stitle{Checkpoint Types.} Distributed runtime engines such as Ray~\cite{moritz2018ray} and deep learning frameworks and libraries such as PyTorch~\cite{paszke2019pytorch}, TensorFlow~\cite{abadi2016tensorflow}, and MXNet~\cite{chen2015mxnet} allow users to specify checkpoint and restore logic including how and when to checkpoint. Such techniques are specific to each runtime engine and require developer involvement, thus being considered \textit{application-aware checkpoints}.

Many other works proposed and improved designs for transparent checkpoints. Singularity~\cite{shukla2022singularity} proposed a device proxy server design that relies on API interception to log all runtime and drive API calls. This log would be used to reconstruct the GPU state upon a restore operation. Such technique has been used widely as a mechanism for semi-transparent checkpoint restore support~\cite{chaudhary2020balancing,gupta2024just}. However, a design based on a device proxy server leads to a number of challenges (as discussed in \autoref{sec:background}), resulting in limited support for large and complex training and inference workloads. \sys, on the other hand, introduces a new design for checkpoint and restore by offering a fully-transparent and unified checkpoint mechanism to save the state of the application running on the CPU (including the engine/framework/library running the user application), and its corresponding state on the GPU. \sys does not rely on API interception and supports different GPU families with both CUDA and ROCm applications.

\stitle{Deciding when to Checkpoint.} Transparent GPU checkpointing is often used to enable error recovery for training jobs by periodically saving the model parameters and optimizer state to persistent storage. State-of-the-art error recovery solutions today implement \textit{periodic} or \textit{just-in-time} checkpointing support where training jobs can be resumed from any prior checkpoint. When GPU workloads experience an error, they either hang or crash, and cause the training jobs to be unexpectedly terminated. These errors are often detected with a series of diagnostic tests and problematic nodes are cordoned off, allowing the training job to resume on a different node.

Recovery mechanisms used with periodic checkpoints are often optimized to balance low running time overhead and high checkpoint frequency~\cite{mohan2021checkfreq}. These optimizations aim to minimize both the runtime overhead and the amount of work the job has to redo on recovery (recovery cost).
%
In contrast, just-in-time checkpointing solves this problem by creating a checkpoint only after a failure has occurred by leveraging the fact that the GPU state is only updated during a short interval and multiple replicas are holding the same state. As a result, this approach is able to recover from errors by recomputing at most one mini-batch~\cite{gupta2024just}.

\sys can be integrated with both periodic and just-in-time checkpointing policies to i) accelerate the checkpoint and restore operations, and ii) create a unified and consistent snapshot of the CPU and GPU state. Furthermore, \sys does not rely on specific workload characteristics such as relying on replicas having an exact copy of the model, thus being a general approach for GPU state checkpointing.

\stitle{Checkpoint/Restore Optimizations.} There have been many other works that optimize checkpoint/restore operations for GPU-accelerated workloads.
CheckFreq~\cite{mohan2021checkfreq} reduces the overhead of periodic checkpointing by overlapping communication with snapshotting of the model state in the GPU. 
Gemini~\cite{team2024gemini} checkpoints the GPU state to local and remote host memory, and interleaves checkpointing traffic with training traffic to reduce the overheads of checkpointing. 
Nebula~\cite{microsoft2024boost} copies model state asynchronously reducing the time during which the training job is paused for checkpoint. 
DeepFreeze~\cite{nicolae2020deepfreeze} proposes a fine-grain asynchronous checkpointing of deep learning models and shards the checkpointing effort across multiple workers. However, it only considers CPU clusters and does not take into account the cost of snapshotting the model state in memory when trained with state-of-the-art GPUs.
Check-N-Run~\cite{eisenman2022check} proposes incremental/differential checkpointing to speedup recommendation model training and uses quantization techniques to reduce the snapshot size.
ZeRo~\cite{rajbhandari2020zero} shards model parameters and optimizer state across data-parallel GPUs, parallelizing the checkpoint effort.

We consider our work to be complementary to these previous contributions. Optimizations based on sharding, compression, asynchronous, or incremental checkpointing could be incorporated into \sys. In summary, \sys presents a novel checkpoint/restore technique on which many of previously proposed optimizations could be integrated.

Another recent line of related work explores running GPU-accelerated inference workloads on serverless platforms, focusing on optimizing resource efficiency, scheduling, and caching policies~\cite{10.5555/3433701.3433792,280768,ishakian2018serving,280704,10.1145/3620678.3624664,infaas,10.1145/3503222.3507709,234998,yang2024on-demand}. Yang et al.~\cite{yang2024on-demand} also leverages CRIU to accelerate the restore time for GPU workloads, but, unlike \sys, does not explore unified container snapshots. Besides, the paper~\cite{yang2024on-demand} focuses on AMD-only devices and does not explore the performance of checkpoint and restore operations on large ML models.
Nevertheless, most of these systems focus on reducing model load time rather than on a general approach for checkpoint and restore GPU-accelerated applications. Many additional optimizations could be combined with \sys to further facilitate model loading. These optimizations are outside the scope of this paper and will be addressed in future work.

\stitle{Framework-level checkpointing} is supported by many popular machine learning libraries, such as PyTorch~\cite{paszke2019pytorch} and TensorFlow~\cite{abadi2016tensorflow}. This approach requires the application, framework or library to correctly capture and restore the runtime state of applications. However, implementing and maintaining this checkpointing mechanism introduces additional development and testing burdens. In addition, it has been shown to be both error-prone and inefficient, often leading to checkpoint file loss or corruption due to job interruptions, as well as significant recovery times~\cite{mohan2021checkfreq}. Thus, in this work, we focus on developing a reliable and efficient system-level checkpointing that is fully transparent.
