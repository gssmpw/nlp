\section{Checkpointing GPU-Accelerated Containers}\label{sec:implementation}%
%
Integrating \sys with container runtimes like Podman, CRI-O and containerd involves leveraging the Container Device Interface (CDI)~\cite{cdi} and the NVIDIA Container Toolkit~\cite{container-toolkit} to enable running containers with GPU hardware. These container runtimes already support the checkpoint/restore of containerized applications using the CRIU tool. However, to enable checkpoint/restore of GPU-accelerated containers, further modifications are necessary.

\subsection{Checkpoint/Restore of GPU Containers}
Accessing GPU resources in containers requires a set of utilities that automatically generate configuration to expose within the container the appropriate device files and libraries. This runtime configuration typically includes details such as the number of GPUs, their unique identifiers, and \textit{driver capabilities} (e.g., compute, graphics, video, utility, display). Runtime libraries such as libnvidia-container provide additional capabilities to container runtimes that enable configuring a container with GPU support by exposing device drivers to it. This functionality enters the container mount namespace referred by its root process identifier (PID) and performs a set of actions to make available GPU device files, libraries and utilities inside the container. The runtime library assumes that the container root filesystem has been created and the container is not yet started. As CRIU checkpoints the mount namespace of the container, it also needs to handle all external mounts created by the runtime library. This requires the runtime library to update the container configuration with all new mounts, enabling the container runtime to specify the necessary CRIU options during checkpoint and restore.

\subsection{Multiprocess GPU Checkpoints}
Containers typically run multiple processes that are isolated from other containers and the host system with Linux control groups (cgroups) and namespaces. Creating a consistent checkpoint of the container process tree requires temporarily suspending the execution of all processes on both the CPU and the GPU. This mechanism is commonly implemented with the \textit{freezer} cgroup in combination with \textit{seize} and \textit{interrupt} functionality of the \texttt{ptrace} system call. In addition, the \textit{lock} and \textit{unlock} functionality of the \texttt{cuda\_checkpoint} tool is used to block/unblock all CUDA driver API calls of the running process. However, locking the driver API calls of all processes in a container is challenging because it requires these processes to be in a running state. Attempting to lock a frozen GPU application causes the \texttt{cuda\_checkpoint} tool to block until the process is resumed. To solve this problem, \sys avoids the use of the freezer cgroup when checkpointing CUDA applications. Instead, only the ptrace \textit{seize} and \textit{interrupt} mechanism is used to suspend the execution on the CPU. This functionality is implemented as part of the CUDA plugin. When the plugin is enabled, it configures CRIU to interrupt the execution of processes without the freezer cgroup.

\subsection{Filesystem Snapshots}
Container engines typically utilize a layered filesystem where container images are mounted as read-only layers to form the container root filesystem (rootfs), and a writable layer is mounted on top of these layers to enable creating files and modifying existing ones. This writable layer captures all changes made within a running container. Since the read-only layers are immutable, the checkpointing mechanism needs to capture only the changes stored in the writable layer. To create a consistent checkpoint, container engines typically utilize the \textit{cgroup freezer} to suspend all processes in a container. This allows saving their runtime state to persistent storage and the container engine to create a consistent copy of the rootfs writable layer. However, as mentioned above, this freeze mechanism prevents the \texttt{cuda-checkpoint} tool from locking the GPU threads. To address this problem, \sys identifies if the process tree is in a frozen state, and if it is, it resumes its execution and uses the ptrace seize with interrupt mechanism. It then uses the CPU-GPU checkpointing workflows (as illustrated in \Cref{fig:cuda-checkpoint-flow}), and leaves the container in a frozen state after the checkpoint has been created. The container runtime then captures the rootfs changes as described above.

\subsection{Checkpointing with NVML}
To enable checkpointing for CUDA applications using NVML, such as PyTorch, we extend the CUDA plugin to handle leftover device references that are not currently supported for checkpointing by the CUDA driver. In most cases, these device references are used during initialization to obtain information such as the available number of GPUs and their capabilities. These values are unlikely to change during runtime and are typically not accessed again. In addition, the \texttt{sys} restore operation requires the same GPU type and order to be used as during checkpointing. For example, if a checkpoint is created on a 4-GPU A100 system, it must be restored on another 4-GPU A100 system. Attempting to restore the checkpoint on an 8-GPU A100 system or a 4-GPU system with different types of GPUs will result in failure. Thus, we implement \texttt{DUMP\_EXT\_FILE}, \texttt{RESTORE\_EXT\_FILE}, \texttt{HANDLE\_DEVICE\_VMA}, and \texttt{UPDATE\_VMA\_MAP} plugin hooks to ensure proper handling of device references and memory mappings during the checkpoint and restore operations. A future release of the CUDA Display Driver will provide additional checkpointing capabilities simplifying this implementation.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% This is the 1st table in Evaluation.
\input{tables/server-configs}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% This is the 2nd table in Evaluation.
\input{tables/combined-checkpoint-restore-times}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Multi-GPU Checkpoints}
\sys supports applications running on a single-node with multiple GPUs. To ensure consistency of concurrent operations, transparent checkpoint/restore for multi-GPU applications requires identifying all associated tasks and synchronizing the state across all GPUs. Multi-GPU applications often perform computations with methods implementing data parallelism at the module level. For example, in the case of training jobs a module is parallelized by replicating it across devices, splitting the input batch dimension among them, and aggregating gradients during back-propagation. The \texttt{cuda-checkpoint} tool provides \textit{lock} and \textit{unlock} actions that enable synchronization across multiple CUDA tasks. These actions are utilized by the CUDA plugin to pause and resume task execution on GPU devices. The CUDA driver handles the internal GPU state during checkpoint and restore operations to/from host memory to ensure complete and consistent snapshot state.
% 
Distributed checkpointing for multi-node workloads requires additional synchronization mechanisms with both CPU and GPU state across different nodes. Applications and frameworks typically use libraries such as NVIDIA Collective Communications Library (NCCL) to implement this functionality. However, at the time of writing, the \texttt{cuda-checkpoint} tool does not support checkpoint/restore operations with NCCL. This functionality is expected to become available in a future release of the CUDA driver.
