%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{tables/multi-gpu-checkpointing}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{figures/tex/in-memory-checkpoint-restore}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{figures/tex/unified-restore-times}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Evaluation} \label{sec:evaluation}%
%
Our evaluation seeks to answer the following questions:
\begin{itemize}[leftmargin=*,leftmargin=15pt,itemindent=0pt]
    \item How does \sys perform when checkpointing and restoring large language models? (\textsection{\ref{sec:eval:diff-models}})

    \item What are the scalability implications of using checkpointing and restoring with multiple GPU devices? (\textsection{\ref{sec:eval:scalability}})

    \item What are the dominant factors affecting the latency of checkpointing and restore operations? (\textsection{\ref{sec:eval:overhead}})

    \item Can \sys support checkpoint and restore with both CUDA and ROCm applications? (\textsection{\ref{sec:eval:rocm}})
\end{itemize}

\subsection{Experimental Methodology}%
%
\stitle{Evaluation Setup.}
We evaluate \sys on 10 servers with specifications described in \Cref{tab:server-configs}, running Ubuntu 22.04 with kernel version 6.2.0 (A100 and H100), 5.15.0 (V100 and A6000), and NVIDIA driver 565.57.01, CUDA 12.7, and CentOS Stream 9 with kernel 5.14 with ROCm 5.6 (MI210).

\stitle{Performance Measurements.} We measure the performance of checkpoint and restore operations using detailed statistics generated by CRIU~\cite{criu-statistics} about the time spent in different stages, and with external tools like \texttt{perf stat} to gather more detailed performance data. We run each experiment 10 times and calculate the mean and standard deviation of each value in the collected data. To analyze the overhead of checkpointing with \sys, we measure the following performance metrics for models of different sizes:

\begin{itemize}[leftmargin=*,leftmargin=10pt,itemindent=0pt]
    \item \textbf{Checkpoint time:} The total time to create a snapshot of the running GPU application.
    \item \textbf{Freezing time:} The time to suspend the application using \texttt{ptrace} seize and interrupt.
    \item \textbf{Frozen time:} The time during checkpointing when the application is not running.
    \item \textbf{Memory dump time:} The time to collect the CPU memory pages of running processes. This does not include the time to write this memory to storage.
    \item \textbf{Memory write time:} The time to save the memory state to persistent storage.
    \item \textbf{Restore time:} The time to restore both CPU and GPU state from storage, and to resume the application.
\end{itemize}

\stitle{Workloads and micro-benchmarks.} We evaluate the proposed checkpoint/restore mechanisms for multiple models of different sizes (listed below) and a set of ROCm micro-benchmarks~\cite{amd2024rocm} representing common HPC workloads for AMD GPU~(\textsection{\ref{sec:eval:rocm}}). For NVIDIA GPUs we use the following models:

\begin{itemize}[leftmargin=*,leftmargin=10pt,itemindent=0pt]
    \item \textbf{LLaMA} \textbf{3.2} (1B, 3B) and \textbf{3.1} (8B)

    \item \textbf{GPT-2} with 124M, 355M, 774M, 1.5B parameters

    \item \textbf{BERT} Base (110M) and Large (340M) models
\end{itemize}
%
\subsection{\sys Performance with Deep Learning Models}
\label{sec:eval:diff-models}
%
We evaluate the performance of GPU checkpoint and restore operations with multiple model training workloads of different sizes. The results in~\Cref{fig:in-memory-checkpoint-restore} show that the time required to checkpoint the GPU state into host memory increases significantly for models with large number of parameters.
For instance, checkpointing and locking operations for the GPT-2 Small model (124M parameters) take an average of 4.9 seconds and 240 ms, respectively. In comparison, for a larger model such as GPT-2 XL (1.5B parameters), these operations require an average of 28 seconds and 500 ms, respectively.
The time to restore the GPU state from host memory increases gradually, with 2.5 seconds for GPT-2 Small and 11 seconds for GPT-2 XL, while the unlock time remains consistent for both models at approximately 160 ms.
We observe similar results with both A100 and H100 GPUs, highlighting the crucial impact memory bandwidth has on the performance of GPU checkpointing operations.
Several techniques have been proposed to address this problem, such as data compression and on-demand parallelism~\cite{yang2024on-demand}. Incorporating such techniques could further improve the efficiency of checkpoint/restore operations, especially for large-scale models.

~\Cref{fig:unified-restore-times} shows the unified restore time (the time to restore the combined CPU-GPU state) for models with different sizes with both H100 and A100 GPUs. The time required to restore the GPU state makes up a significant portion of the total restore time for small models, but becomes a relatively lesser portion for larger models. These results demonstrate that the restore time is also affected by the available bandwidth for CPU-GPU memory transfers, as well as the speed at which checkpoint data is loaded from disk into host memory.
The performance results and checkpoint sizes shown in \Cref{tab:combined-checkpoint-restore-times} suggest that the differences in performance between the experiments with H100 and A100 GPUs can be attributed not only to advancements in GPU hardware architecture but also to the critical role of the available CPU resources.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{tables/cpu-to-gpu-state-comparison}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\Cref{tab:cpu-to-gpu-state-comparison}  shows the total \sys checkpoint sizes for various models on both A100 and H100 GPUs, along with a breakdown of GPU memory and CPU state proportions. A key insight is the dominance of GPU memory in overall checkpoint size, consistently exceeding 80\% and often surpassing 90\% for larger models like LLaMA 3.1 (8B) and GPT2-XL (1.5B). These results further emphasize the importance of efficient CPU-GPU memory transfers in optimizing the performance of checkpointing and restore operations.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Multi-GPU Checkpointing Performance}
\label{sec:eval:scalability}
%
We evaluate the scalability of \sys by running an experiment designed to train a large language model (GPT-2) across 1x, 2x, and 4x GPUs of V100, A6000, and A100 types, using data parallelism to distribute the workload.
\Cref{tab:multi-gpu-checkpointing} shows that the checkpoint size increases with the number of GPUs, as each GPU stores its own copy of the model parameters.
For instance, the checkpoint size for 1, 2, and 4 A100 GPUs is $\approx10$, $\approx20$, and $\approx40$ GB, respectively.
This increase in checkpoint size reflects the increasing amount of intermediate model state that is saved as the number of GPUs increases.
The freezing, frozen, memory dump and memory write times also increase with the number of GPUs, likely because more time is spend on handling the larger checkpoint data with additional GPUs.
For example, as the number of A100 GPUs increases, creating a unified CPU-GPU snapshot requires scanning more memory pages.
A single GPU requires $\approx7$ million pages scanned, two GPUs require $\approx15$ million, and four GPUs require $\approx28$ million. Our experiments demonstrate that \sys efficiently scales as the number of GPUs and data-parallel replicas increase. We observe that checkpointing and restore times scale near linearly as we increase the number of GPUs from 1 to 4 across different GPU types.

\subsection{Checkpoint and Restore Latency}%
\label{sec:eval:overhead}%
We analyze the overhead of checkpoint and restore operations of \sys by measuring the latency during these processes for LLaMA 3.1 and GPT2-XL model training workloads on H100 and A100 GPUs. The results in ~\Cref{tab:combined-checkpoint-restore-times} highlight the performance differences between the H100 and A100 GPUs, as well as the impact of additional CPU and memory resources on reducing overhead during the checkpoint and restore operations with \sys. The primary factors affecting \sys's checkpoint and restore performance are:

\begin{itemize}
    \item \textbf{GPU count}: Increasing the number of GPUs leads to increased checkpoint size and latency;
    \item \textbf{CPU-GPU bandwidth}: The speed of data transfers between the CPU and GPUs directly affects checkpointing and restoring speed;
    \item \textbf{GPU memory usage}: Larger models with more parameters have higher checkpoint and restore latencies.
\end{itemize}

\begin{figure}[t]
  \centering
  \includegraphics[width=\columnwidth]{figures/MI210_Unified_Checkpointing_Times.pdf}
  \vspace{-2.5em}
  \caption{Breakdown of the \sys checkpointing time for HPC benchmarks running on AMD MI210 GPU.}
  \label{fig:amd-gpu-checkpointing}
  \vspace{-.5em}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{tables/rocm-benchmark-checkpoints}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{\sys Support for ROCm Devices}
\label{sec:eval:rocm}

In addition to support for CUDA, to demonstrate the checkpointing functionality of \sys for AMD GPUs, we evaluate its performance using a set of ROCm HPC micro-benchmarks. These benchmarks provide a set of workloads representative of typical HPC applications, allowing us to analyse \sys's ability to effectively checkpoint and restore GPU state across different computational patterns. \Cref{fig:amd-gpu-checkpointing} shows the frozen, memory dump, and memory write times during checkpointing for each benchmark. While most of the evaluated benchmarks have relatively small checkpoint size, typically ranging from under 500~MB to 1.2~GB, a few have significantly larger checkpoint sizes (Histogram, Matrix Multiplication, and Convolution), shown in \Cref{tab:rocm-benchmark-checkpoints}. This increase in checkpoint size directly correlates with longer freezing and memory dump times, as \sys must checkpoint larger amount of data. An interesting observation is the contrasting distribution of checkpoint data between host memory and GPU memory across these benchmarks. While more than half of the checkpoint size for the Convolution benchmark is attributed to AMD GPU state, Histogram and Matrix Multiplication have the majority of their state residing in host memory.
