%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{figures/tex/cricket-vs-baseline}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Background and Motivation}\label{sec:background}%

Deep learning training consumes significant computational resources, including processing power and memory. For example, training a state-of-the-art large language model (LLM) such as LLaMA 3.1 with 405 billion parameters requires a cumulative 39.3 million hours of GPU computation on H100 80GB device~\cite{dubey2024llama}. The training process typically starts with a random set of learnable parameters and processes a small disjoint subset (\textit{minibatch}) of a training data set in steps known as \textit{iterations}. When the entire dataset is processed exactly once, an \textit{epoch} is said to be complete. Each training iteration involves data augmentation, a forward pass to generate predictions, a backward pass to compute gradients, and a weights update phase~\cite{goodfellow2016deep}. Once training is completed (typically after a predetermined number of epochs), the final learned parameters are saved as a checkpoint file to persistent storage, enabling the model to be initialized from this file for \textit{inference} to generate predictions.

To accelerate model training, traditional cloud platforms are increasingly leveraging GPUs to enable parallel and distributed computations. However, efficiently managing cluster resources with tens of thousands of GPUs across multiple teams is challenging, especially since these workloads often run for weeks or months and consume enormous amounts of energy~\cite{yang2024part, dubey2024llama}. When running at scale, hardware faults, network interruptions, and software bugs occur frequently, and each individual fault can result in partial restarts or a complete retraining from scratch~\cite{rojas2019analyzing}. As a result, efficient fault tolerance and checkpointing techniques have become of paramount importance for large-scale model training~\cite{mohan2021checkfreq,wang2023gemini,gupta2024just}.

\subsection{Challenges with Transparent GPU Checkpointing}%
Several approaches for transparent system-level checkpointing of GPU applications have been proposed in the literature~\cite{takizawa2009checuda,nukada2011nvcr,jain2020crac,shukla2022singularity,eiling2022cricket,eiling2023checkpoint}. However, these approaches are not \textit{fully transparent} as they require invasive modifications to the execution environment of applications to intercept, log, and replay device API calls, i.e., \textit{semi-transparent} checkpointing. \Cref{fig:cuda-device-proxy} illustrates this mechanism. To enable GPU checkpointing, these solutions assume that all interactions with the GPU go through specific dynamically linked libraries (e.g., \texttt{CUDA} for NVIDIA GPUs or \texttt{ROCm} for AMD GPUs)~\cite{shukla2022singularity,eiling2022cricket}. To enable checkpointing, the function signatures provided by these libraries are overridden by an interception library, which is preloaded using the \texttt{LD\_PRELOAD} environment variable. However, implementing this interception mechanism is challenging due to the complex architecture of GPU devices, which can vary significantly across hardware generations, and the proprietary nature of the GPUs libraries (e.g., NVML, cuDNN). We outline some of the key challenges of transparent GPU checkpointing below.

\subsubsection*{Challenge 1: API Interception with a Device Proxy}%
CUDA offers multiple ways for applications to interface the GPU driver -- a \textit{high-level} (runtime)~\cite{nvidia2024runtime} and \textit{low-level} (driver)~\cite{nvidia2024driver} APIs. The runtime API simplifies application development by offering implicit initialization, context management, and module handling. In contrast, the driver API provides fine-grained control with explicit context and module management, offering greater flexibility but requiring a more complex implementation.
To maximize checkpoint transparency, API calls should be intercepted at a low level -- ideally, between the driver API and the GPU driver~\cite{shukla2022singularity}. However, due to the closed-source nature of GPU drivers, some checkpointing solutions intercept only the runtime API~\cite{eiling2022cricket}. Since all interactions between the CPU and GPU must pass through the interception mechanism, scenarios where host memory addresses are used as implicit input and accessed directly by the GPU could potentially lead to an inconsistent state after restore~\cite{gupta2024just}.
% 
In addition, to enable the replay of all device operations, a worker process must log these device APIs, along with their input values, object handles (e.g., events, streams), GPU memory addresses, and input parameters. These interception and logging operations consume additional CPU and memory resources, inevitably introducing performance overhead and increasing API call latency~\cite{eiling2022cricket}.
As \sys does not require an API interception mechanism, it avoids these challeges completely.

\subsubsection*{Challenge 2: Static and Dynamic Linking}%
Starting with CUDA version 5.5, the CUDA runtime library is statically linked by default when using the CUDA compiler~\cite{corporation2023cuda}. This static linking ensures that the correct version of runtime functions are included in the application binary and avoid the need for additional redistribution of CUDA libraries. This default static linking is widely used by frameworks such as PyTorch to enable easier cross-platform application development.
However, a prerequisite for semi-transparent checkpointing is for the CUDA runtime library to be dynamically-linked to enable the replacement with interception code~\cite{eiling2022cricket}.
As a result, many applications and frameworks need to be modified and recompiled from source to meet this requirement, which may involve significant effort and be impractical when the source code is unavailable. In contrast, \sys interacts directly with the GPU driver, allowing to checkpoint applications that use both static and dynamic linking.

\subsubsection*{Challenge 3: Starting kernels from Shared Objects}%
For some applications, such as PyTorch, CUDA kernels are loaded at runtime using \texttt{dlopen} and \texttt{dlsym}, with the initialization phase enabling the invocation of \texttt{cuLaunchKernel()}. However, when using semi-transparent checkpointing, the CPU and GPU states are decoupled, requiring these kernels to be dynamically loaded in both address spaces. This is typically achieved by intercepting CUDA-related registration functions and sending a request to dynamically load the CUDA kernels into the address space of the interception (device-proxy) server. Implementing this functionality requires reverse-engineering part of the CUDA runtime to decode the metadata of fat binaries (which store multiple versions of binary code for different GPU architectures~\cite{harris2024cuda}), extracting the corresponding cubin from the target binary, and sending it through the interception mechanism to be loaded with \texttt{cuModuleLoadData}. This results in additional complexity and performance overhead. In comparison, \sys avoids this challenge by interacting with the GPU driver directly and utilizing its functionality lock/unlock the execution of APIs that impact the GPU state.  

\subsubsection*{Challenge 4: Complex GPU Runtime and Memory State}%
GPUs have a complex hierarchy of multi-processors, processing blocks, and memory controllers that facilitate high degree of parallel computations and memory bandwidth to maximize compute performance~\cite{nvidia2022h100, nvidia2020a100, nvidia2017p100}.
%
GPU code execution breaks down into three stages: (i) launch, (ii) grid initialization, and (iii) kernel execution. Many GPU-intensive applications, such as deep neural network training and scientific simulations, involve iterative processes where the same workflow is executed repeatedly. In these cases, CUDA streams -- which allow for concurrent execution of tasks on the GPU -- require the CPU to resubmit the same work to the GPU after each iteration. This resubmission is necessary to ensure that the GPU can continue processing each task in parallel, but it can introduce overhead in applications with many iterations. An alternative, more efficient, method for submitting work to the GPU is using \textit{task graphs} that consist of a series of operations such as memory copies and kernel launches connected by dependencies. This method enables allows reduce the cost of kernel launch significantly. However, the inherent complexity and advanced optimizations of the GPU runtime used to manage this state further complicates the API interception mechanisms for semi-transparent checkpointing.

\subsubsection*{Challenge 5: No Internal GPU State Access}%
In contrast to CPUs, most GPUs do not offer an assembler that maps the assembly code directly to machine instructions with register access. This limitation poses a significant challenge to implementing transparent checkpoint/restore operations because some register state can only be accessed using assembly. For instance, Cricket~\cite{eiling2022cricket} addresses this problem using Streaming ASSembly (SASS) by directly modifying the binary code after compilation to recreate the device execution state. Singularity~\cite{sivathanu2022transparent} uses similar approach where the \texttt{cuObjDump} utility is used to parse kernel libraries and extract parameter information, then intercept \texttt{nvrtcCompileProgram} to extract the parameter signatures from the generated parallel-thread execution (PTX) program. However, both solutions have high performance cost, are prone to errors due to the complexity of reverse-engineering, and may not be universally applicable across different GPU generations. \sys avoids these limitations by utilizing the driver directly to restore the GPU state from a checkpoint. 

\subsubsection*{Challenge 6: Determinism and Reproducibility}%
Replaying CPU-GPU interactions correctly during restore is particularly challenging when applications use asynchronous or non-deterministic operations~\cite{park2022gpureplay}.
These operations can introduce state divergences when replaying the recorded API calls during restore and make it difficult to reproduce the same results across different runs.
This problem is further exacerbated with long-running workloads, such as deep learning training jobs with parallel computations running across multiple GPUs, where small state discrepancies accumulate over time~\cite{riachnvidia2019gtc}. For example, frameworks such as Megatron-LM are intended to be bitwise reproducible~\cite{nvidia2024megatron-lm}, where the same training application run twice in the same hardware and software environment should produce identical checkpoints, losses, and accuracy values.
Existing work aims to address these challenges by removing the sources of non-determinism at the record time (e.g., by forcing synchronous operations) and tolerating non-deterministic interactions that do not affect the GPU state at the replay time~\cite{park2022gpureplay,gupta2024just}.
In contrast, \sys does not use record-and-replay of CPU-GPU interactions and relies on a locking mechanism to create snapshots and restore them in a consistent and deterministic state.

\subsection{Performance Overhead of API Interception}%
To better understand the overhead introduced by semi-transparent checkpointing with API call interception, we analyze the performance of Cricket~\cite{eiling2022cricket}, a state-of-the-art open-source GPU checkpointing library.
By examining Cricket's implementation, we gain insights into the typical costs associated with the interception layer and saving and restoring the GPU state.
As part of our analysis, we conduct an experiment using a simple PyTorch implementation of neural network training, where we measure the number of intercepted API calls and the total processing time to quantify the impact of the interception layer on overall performance.
The results in \Cref{fig:cricket-vs-baseline} show that the number of intercepted API calls and the overhead introduced by the interception layer increase exponentially with the number of training epochs.
In addition, the handling of asynchronous data transfers is achieved at the interception layer by simply forwarding \texttt{cudaMemcpyAsync} to \texttt{cudaMemcpy} and ignoring the stream parameter~\cite{eiling2022cricket}.

\stitle{Summary.} Compared to semi-transparent checkpointing, \sys does not require interception mechanisms, re-complication of the application code, and supports checkpointing with both statically linked and dynamically linked libraries. \sys relies on the GPU driver to save and restore the state of applications.
In particular, it leverages recently introduced checkpoint/restore driver capabilities and combines these with CPU-checkpointing techniques to efficiently handle serialization and de-serialization of runtime state, memory and kernel parameters in both GPU and CPU.
