\section{Introduction}%
In recent years, advances in high-performance computing (HPC) and machine learning have enabled exponential growth in neural network sizes with increasing number of parameters and layers, unlocking a wide range of applications, from conversational bots like OpenAI's ChatGPT~\cite{chatgpt}, Google's Gemini~\cite{team2024gemini}, and Microsoft's Copilot~\cite{copilot} to programming assistants~\cite{chen2021evaluating, microsoft2023github, roziere2024code}, and autonomous driving vehicles~\cite{kiran2022deep,ma2023dolphins}. However, training large models at an unprecedented scale presents new and interesting system challenges. A recent study has shown that training LLaMA 3.1 model with 405 billion parameters on 16,000 GPUs has encountered 419 unexpected interruptions over 54 days, with 78\% attributed to hardware failures~\cite{dubey2024llama}.
These errors, as demonstrated in~\cite{gupta2024just}, can cost up to few million dollars per month, depending on the size of the job. To handle such disruptions, training large models such as Google Gemini often requires maintaining redundant in-memory model states to enable rapid recovery~\cite{team2024gemini}.

% Moreover, spinning up multiple dedicated instances for each individual model can be costly and inefficient~\cite{vincentchan2024model}. To support inference with new models without increasing infrastructure costs, it is often required to dynamically switch inference models. This is typically achieved using a dedicated controller that collects metrics and makes decisions about suitable pairs of candidates to swap in and out.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{figures/tex/cuda-device-proxy}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

GPU checkpointing provides an effective failure recovery solution that allows to periodically save the state of models and resume training from the last checkpoint. This checkpointing mechanism can be implemented at different levels of the software stack: \textit{application-level} (not transparent), \textit{library-level} (semi-transparent), and \textit{system-level} (fully transparent). System-level checkpointing makes no assumptions about the underlying tasks or processes, offering a more \textit{general} and \textit{transparent} solution that is easy to use and does not require changes to application code~\cite{chaudhary2020balancing,shukla2022singularity,gupta2024just}. However, enabling transparent GPU checkpointing at system-level is particularly challenging, as it requires specialized knowledge of the hardware architecture, as well as an understanding of the interconnected memory subsystems, dynamic parallelism, and thread synchronization. These low-level implementation details often change significantly across hardware generations and are not always publicly documented. As a result, state-of-the-art GPU checkpointing solutions rely on \textit{device proxy} mechanisms that decouple the memory address space of applications from the device-specific mappings and other side-effects created by GPU libraries such as CUDA~\cite{jain2020crac, shukla2022singularity,eiling2023checkpoint}. This approach dynamically loads a shared library with the \texttt{LD\_PRELOAD} mechanism that allows to intercept, log, and replay all device APIs invoked by the application. However, this interception mechanism is in the critical path of GPU-accelerated operations and inherently introduces steady-state performance overhead with each API call, as shown in~\Cref{sec:background}. In addition, it adds significant complexity and maintenance overhead, as it requires low-latency communication between the host processes and the device-proxy component, custom memory management, synchronization barriers for distributed jobs, and explicit handling of each API call. It also creates additional runtime dependencies and potential compatibility issues, as it requires recompiling workloads such as PyTorch from source with dynamically linked libraries (e.g., CUDA runtime)~\cite{eiling2023cricket}. Replaying the logged API calls during restore can also lead to prolonged recovery times and inconsistent GPU state, especially with non-deterministic operations such as floating-point computations~\cite{riachnvidia2019gtc}.

To address these challenges, we propose a novel approach for transparent checkpoint/restore of GPU-accelerated workloads that builds on recently introduced GPU driver checkpointing capabilities~\cite{gurfinkel2024checkpointing, bhardwaj2021fast}. This approach does not require interception of device API calls, and enables \textit{unified} system-level snapshots that contain both CPU and GPU state, as illustrated in \Cref{fig:cuda-device-proxy}. We implement the proposed checkpointing functionality on top of the open-source Checkpoint/Restore in Userspace (CRIU)~\cite{criu} tool through plugins that handle all external GPU resources. We extensively study the checkpoint/restore performance of \sys with large language model training for BERT, GPT-2, and LLaMA 3. We further evaluate \sys on various hardware configurations, including single GPU and multi-GPU setups, using NVIDIA H100, A100, V100, and A6000. We also demonstrate \sys's capability to efficiently checkpoint and restore HPC applications running on AMD GPU devices. 

Our evaluation results show that \sys significantly reduces checkpoint and restore overheads in comparison to state-of-the-art API interception mechanisms, while enabling reliable checkpoint/restore of GPU-accelerated containers.
To the best of our knowledge, \sys is the first to achieve fully transparent, system-level GPU container checkpoints that combine both GPU and CPU state in a single unified snapshot. We have contributed the changes implementing \sys to the upstream GitHub repository, which were released with CRIU version 4.0 and are publicly available at \url{https://github.com/checkpoint-restore/criu}.

In summary, this paper makes the following contributions:

\begin{itemize}
    \item We systematically analyze state-of-the-art GPU checkpoint mechanisms utilizing device API call interception techniques and uncover several challenges that limit the usability of this approach. We further benchmarked the only open-source system we could find that implements these mechanisms (Cricket~\cite{eiling2022cricket}) and measured the performance overheads when compared to a baseline neural network training with no API interception (\textsection{\ref{sec:background}});
    
    \item We propose \sys, a novel approach for fully transparent and unified checkpointing of GPU-accelerated workloads, including deep learning training and inference. \sys eliminates steady-state overheads, supports both AMD and NVIDIA GPUs, and, to the best of our knowledge, is the first to support fully transparent checkpointing of large multi-GPU applications without device API call interception (\textsection{\ref{sec:design}});

    \item Our prototype builds on the open-source CRIU project integrated with Podman (a widely used container engine) to enable transparent and unified checkpointing of GPU containers (\textsection{\ref{sec:implementation}}). We have contributed our implementation to the corresponding open-source projects.

    \item We evaluate {\sys} on large language model training workloads, such as GPT-2 (1.5B) and LLaMA 3.1 (8B), as well as a set of HPC micro-benchmarks, showing that {\sys} can be used to checkpoint and restore such models on different GPU devices (e.g., NVIDIA H100, A100, and AMD MI210) with minimal overhead (\textsection{\ref{sec:evaluation}}).
\end{itemize}
