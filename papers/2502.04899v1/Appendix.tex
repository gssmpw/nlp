\appendix
\section{Self-Supervised Learning Paradigms}
\label{appendix:ssl-paradigms}

\subsection{Predictive SSL}

\subsubsection{Masked Modeling}
\label{appendix:masked-modeling}

The masked objective for ES is defined as follows:
given an event stream $S_u$, first a set of indices is sampled $\mathcal{I} \subseteq \{i\}_{i=1}^{n_u}$, then a perturbed version of $S_u$ is constructed, denoted as $\Tilde{S}_u$ and defined as:

$$
\Tilde{S}_u =
\begin{cases} 
\texttt{[MASK]}, & \text{if } i \in \mathcal{I}, \\
e_i, & \text{otherwise}
\end{cases}
\text{ for all } e_i \in S_u.
$$

This masked objective is parametrized by a neural network $f_\theta$ predicting the masked events, and it can be defined in the following:

$$\mathcal{L}_M = \sum_{i \in \mathcal{I}} - log p_\theta (e_i | \Tilde{S}_u)$$

Since each event $e_i$ consists of two elements: a timestamp $t_i$ and the event data $d_i$, we can split the above objective into two terms:

$$\mathcal{L}^t_M = \sum_{i \in \mathcal{I}} - log p_\theta (t_i | \Tilde{S}_u) \quad \text{and} \quad \mathcal{L}^d_M = \sum_{i \in \mathcal{I}} - log p_\theta (d_i | \Tilde{S}_u),$$

for the event timestamp and data respectively.
If the timestep does not have to be modeled, the masking process can be altered such that only the event information is perturbed: $e_i = (t_i, \texttt{[MASK]})$.
In this case $\mathcal{L}_M = \mathcal{L}^d_M$.

In practice, popular MLM models such as BERT~\cite{devlin2018bert} incorporate additional strategies beyond simple masking to enhance model robustness. Specifically, a small subset of the indices selected in $\mathcal{I}$ is replaced with randomly sampled tokens instead of being masked. While this variation introduces minor adjustments to the procedure, it does not fundamentally alter the underlying optimization objective.

\subsubsection{Autoregressive Modeling}
\label{appendix:autoregressive-modeling}

In AR modeling, the objective is to predict the next event in a sequence based on its preceding events, thereby enforcing a causal structure.
For an event stream $ S_u = \{ e_1, e_2, \dots \} $, the model is restricted to using only past events up to $ e_{i-1} $ when predicting $ e_i $, ensuring it learns the temporal dependencies in the sequence.

Given a neural network $ f_\theta $ parameterized by $\theta$, the AR objective can be defined as maximizing the conditional probability of each event given its history. 
This results in the following loss function:

$$
\mathcal{L}_{AR} = - \sum_{i=1}^{n_u} \log p_\theta(e_i | e_{<i})
$$

where \( e_{<i} = \{ e_1, e_2, \dots, e_{i-1} \} \) denotes the history of events up to \( e_{i-1} \). 
Similarly to the masked objective, this formulation can also be split into two by considering the prediction of event data and timestamp separately.

\subsubsection{Temporal Point Processes}
\label{appendix:tpp-formalism}

The key quantity to describe a point process with is the conditional intensity function $\lambda(t \mid \mathcal{H}_t)$, which represents the instantaneous rate of events occurring at time $t$, given the past history $\mathcal{H}_t$. 
For a marked point process, we define the history as:

$$ \mathcal{H}_t = \{(t_i, m_i) : t_i < t \text{ and } m_i \in \mathcal{M} \}, $$

and the intensity function conditioned on the history as

$$\lambda(t, m \mid \mathcal{H}_t) = \lambda(t \mid \mathcal{H}_t) p(m \mid t, \mathcal{H}_t)$$

where $\lambda(t \mid \mathcal{H}_t)$ is the intensity of an event at time $t$ and $p(m \mid t, \mathcal{H}_t)$ is the conditional probability of the mark $m$ given the event time and history.

Similarly to the masked and autoregressive objectives, the objective function for TPPs can be divided into two parts: the likelihood of event times and the likelihood of marks. 
The overall loss function used to optimize the parameters of a marked TPP is defined as
$$
\mathcal{L}_{\text{TPP}} = - \sum_{i=1}^{n} \left( \log \lambda^*(t_i) - \int_{t_{i-1}}^{t_i} \lambda^*(\tau) d\tau + \log p(m_i \mid t_i, \mathcal{H}_{t_i}) \right)
$$

In this formulation, the first term, $\log \lambda^*(t_i)$ represents the log-likelihood of observing an event at time $t_i$, based on the conditional intensity function. For the sake of simplicity we denote $\lambda(t \ \mathcal{H}_t)$ as $\lambda^*(t)$. The second term, $-\int_{t_{i-1}}^{t_i} \lambda^*(\tau) d\tau$ accounts for the fact that no events occurred between $t_{i-1}$ and $t_i$ and captures the likelihood of no event occurring in this interval (i.e., the survival function). Finally, the third term, $\log p(m_i \mid t_i, \mathcal{H}_{t_i})$ is the log-likelihood of observing the mark $m_i$, given the event history up to time $t_i$.

\subsection{Contrastive SSL}

\subsubsection{Instance Contrastive}
\label{appendix:instance-contrast}

In instance-based contrasting, a positive sample is constructed by applying a transformation $a \sim \mathcal{A}$ to an event stream $S_u$, yielding an augmented view $\Tilde{S}_u$ that preserves semantic similarity. Negative samples are drawn from a set of other sequences, denoted $\mathcal{S}^-$. A neural network $f_\theta$ is then used to encode sequences into their latent representations, with the representation of $S_u$ expressed as $f_\theta(S_u) = z_u$.

A common learning objective for instance-based contrastive learning is the InfoNCE loss~\cite{oord2018representation}, defined as:

$$\mathcal{L}_{ICL} = - \log \frac{\exp(\text{sim}(z_u, \Tilde{z}_u))}{\exp(\text{sim}) + \sum\limits_{S_j \in \mathcal{S}^-} \exp(\text{sim}(z_u, z_j))},$$

where $\text{sim}(\cdot, \cdot)$ is a similarity metric such as cosine similarity.

\subsubsection{Distillation}
\label{appendix:distillation}

In distillation-based learning, the objective is to minimize the distance between the student's and teacher's latent representations.
Two networks with identical architecture but distinct parameters are used, where the student network is denoted as $f_\theta$ and the teacher network as $f_\zeta$. 
The student also uses a projection head, $g_\theta$. 
Both networks receive different augmented views of the same input sequence $S_u$, denoted $\Tilde{S}_u$ for the student and $\hat{S}_u$ for the teacher. These views are encoded into latent representations, $\Tilde{z}_u$ for the student and $\hat{z}_u$ for the teacher.

Formally, the student’s encoded view is $f_\theta(\Tilde{S}_u) = \Tilde{z}_u$, and the teacher’s view is $f_\zeta(\hat{S}_u) = \hat{z}_u$. The student network also includes a projection head $q_\theta$.

The distillation loss is defined as:

$$ 
\mathcal{L}_{DL} = - \frac{\langle q_\theta(\Tilde{z}_u), \hat{z}_u \rangle}{\parallel q_\theta(\Tilde{z}_u) \parallel_2 \parallel \hat{z}_u \parallel_2}
$$

where $q_\theta(\Tilde{z}_u)$ is the student's projected latent view, and the loss measures the alignment between the student and teacher representations while maintaining stability in training.

\subsubsection{Feature Decorrelation}
\label{appendix:feature-decor}

Feature Decorrelation-based learning seeks to enforce representation learning by minimizing redundancy between the learned features, thereby avoiding the need for negative samples. 
In contrast to instance-based or distillation-based methods, this approach promotes diversity in the learned representations by directly encouraging feature decorrelation across dimensions.

In this paradigm, two differently augmented views of the same input sequence $S_u$ are generated, denoted $\Tilde{S}_u$ and $\hat{S}u$, and are passed through a shared neural network $f_\theta$ to produce latent representations $\Tilde{z}u = f\theta(\Tilde{S}_u)$ and $\hat{z}u = f\theta(\hat{S}_u)$.
To achieve feature decorrelation, two objectives are typically combined: a similarity objective to ensure that the representations of the augmented views are aligned, and a redundancy reduction objective to ensure the latent features are uncorrelated across dimensions.
A loss function for this objective, borrowed from~\cite{zbontar2021barlow}, is as follows:

$$\mathcal{L}_{FDL} = \sum\limits_i(1-C_{ii})^2 + \lambda \sum\limits_{i \neq j} C^2_{ij},$$

where $C$ is the cross-correlation matrix between the features $\Tilde{z}_u$ and $\hat{z}_u$, and $\lambda$ is a weighting factor to balance the two terms. This loss encourages both similarity in aligned representations and decorrelation across feature dimensions.

\subsubsection{Multimodal Contrastive}
\label{appendix:multimodal-learning}

Multimodal learning focuses on leveraging information from different data modalities to capture richer, more comprehensive representations. 
The key assumption is that we have aligned data from two modalities, for example image-text pairs, as it was originally introduced in CLIP~\cite{radford2021learning}.
Due to the need for multiple data modalities, for ES data we only consider multimodal contrasting at the event level (i.e. contrasting between events).

Assuming that the event data domain $\mathcal{D}$ consists of two exclusive sub-domains
$\mathcal{D}_a$ and $\mathcal{D}_b$ such that
$$\mathcal{D}_a \cap \mathcal{D}_b = \emptyset \text{ and } \mathcal{D}_a \cup \mathcal{D}_b = \mathcal{D},$$
and we have sequences $S_u$ where for each event the event data $d_i \in \mathcal{D}$ can be split into two parts 

$$d_i = (d_i^a, d_i^b) \quad \text{where} \quad d_i^a \in \mathcal{D}_a \wedge d_i^b \in \mathcal{D}_b$$

First, a batch of events $\mathcal{B}$ are sampled,  consisting of paired instances from the two modalities, such that:

$$
\mathcal{B} = \{(t_j, (d^a_i, d^b_i))\}_{i=1}^{N_b}
$$

Latent representations for each data modality is obtained through their respective encoders $f_\theta^a(d_i^a) = z_i^a$ and $ f_\phi^b(d_i^b) = z_i^b$.
The objective in Multimodal Learning is to align the representations of these paired instances while also maintaining distinctions between unrelated pairs.

The learning objective is typically based on the InfoNCE loss, adjusted to handle the multimodal nature of the data. 
The specific loss function for multimodal learning (denoted $\mathcal{L}_{MML}$) is:

\begin{align*}
\mathcal{L}_{MML} &= \frac{1}{2N_b} \left( \sum_{i=1}^{N_b} \left( - \log \frac{\exp(\text{sim}(z^a_i, z^b_i))}{\sum_{j} \exp(\text{sim}(z^a_i, z^b_j))} \right) \right. \\
&\quad + \left. \sum_{i=1}^{N_b} \left( - \log \frac{\exp(\text{sim}(z^b_i, z^a_i))}{\sum_{j} \exp(\text{sim}(z^b_i, z^a_j))} \right) \right)
\end{align*}

where $N_b$ is the batch size and $\text{sim}(\cdot, \cdot)$ refers to a similarity metric such as cosine similarity.
This loss encourages the representations from both modalities to be well-aligned while simultaneously discouraging similarities between non-matching pairs.
Note that this formulation (and multimodal learning in general) is applied more as an event-encoder rather than a full sequence encoder, unlike other contrastive learning paradigms presented here.