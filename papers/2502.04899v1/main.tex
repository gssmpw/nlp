\documentclass[twoside,11pt]{article}
%\usepackage{jair, theapa, rawfonts}
\usepackage{arxiv, theapa, rawfonts}

%% for arXiv version
%\jairheading{1}{2025}{1-15}{2/25}{0/00}
%\ShortHeadings{Unified Approaches in Self-Supervised Event Stream Modeling}
%{Z\'{o}lyomi, Wang, Ennadir, Smirnov \& Cao.}
%\firstpageno{1}

\usepackage{graphicx} % Required for inserting images
\usepackage{subcaption}
\usepackage{color}
\usepackage{xcolor}
\usepackage{pifont}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{standalone}
\usepackage{tablefootnote}
\usepackage{multirow}
\usepackage{array}
\usepackage{url}
\usepackage{makecell}

\graphicspath{ {./figures/} }

\usepackage{tikz}
\usetikzlibrary{positioning, shapes, arrows.meta}
\usepackage[edges]{forest}

\definecolor{hidden-draw}{RGB}{106,142,189} 
\definecolor{hidden-blue}{RGB}{194,232,247} 
\definecolor{hidden-orange}{RGB}{217, 232, 252} 
\definecolor{hidden-red}{RGB}{255,182,193}
\definecolor{hidden-green}{RGB}{144,238,144}

% Define color-blind friendly palette
\definecolor{cbYellow}{RGB}{240,228,66}
\definecolor{cbBlue}{RGB}{86,180,233}
\definecolor{cbOrange}{RGB}{230,159,0}
\definecolor{cbRed}{RGB}{213,94,0}
\definecolor{cbGreen}{RGB}{0,158,115}

\usepackage{geometry}

\newtheorem{definition}{Definition}

\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

\begin{document}

\title{Unified Approaches in Self-Supervised Event Stream Modeling: Progress and Prospects}

%% for arXiv version
\author{
    Levente Z\'{o}lyomi\\
    King AI Labs, Microsoft Gaming\\
    \texttt{levi.zolyomi@king.com}
    \and
    Tianze Wang\\
    King AI Labs, Microsoft Gaming\\
    \texttt{tianze.wang@king.com}
    \and
    Sofiane Ennadir\\
    King AI Labs, Microsoft Gaming\\
    \texttt{sofiane.ennadir@king.com}
    \and
    Oleg Smirnov\\
    Microsoft Gaming\\
    \texttt{oleg.smirnov@microsoft.com}
    \and
    Lele Cao\\
    Microsoft Gaming\\
    \texttt{lelecao@microsoft.com}
}


% \author{\name Levente Z\'{o}lyomi \email levi.zolyomi@king.com \\
%        \name Tianze Wang \email tianze.wang@king.com \\
%        \name Sofiane Ennadir \email sofiane.ennadir@king.com \\
%        \name Oleg Smirnov \email oleg.smirnov@microsoft.com \\
%        \name Lele Cao \email lelecao@microsoft.com \\
%        \addr King AI Labs, Microsoft Gaming
% }

% For research notes, remove the comment character in the line below.
% \researchnote

\maketitle


\begin{abstract}
The proliferation of digital interactions across diverse domains, such as healthcare, e-commerce, gaming, and finance, has resulted in the generation of vast volumes of event stream (ES) data. ES data comprises continuous sequences of timestamped events that encapsulate detailed contextual information relevant to each domain. While ES data holds significant potential for extracting actionable insights and enhancing decision-making, its effective utilization is hindered by challenges such as the scarcity of labeled data and the fragmented nature of existing research efforts. Self-Supervised Learning (SSL) has emerged as a promising paradigm to address these challenges by enabling the extraction of meaningful representations from unlabeled ES data. In this survey, we systematically review and synthesize SSL methodologies tailored for ES modeling across multiple domains, bridging the gaps between domain-specific approaches that have traditionally operated in isolation. We present a comprehensive taxonomy of SSL techniques, encompassing both predictive and contrastive paradigms, and analyze their applicability and effectiveness within different application contexts. Furthermore, we identify critical gaps in current research and propose a future research agenda aimed at developing scalable, domain-agnostic SSL frameworks for ES modeling. By unifying disparate research efforts and highlighting cross-domain synergies, this survey aims to accelerate innovation, improve reproducibility, and expand the applicability of SSL to diverse real-world ES challenges.
\end{abstract}

\section{Introduction}
\label{Introduction}

% Motivation and Context: (1) Summarize the explosion of digital interactions that produce large volumes of event data in domains such as healthcare, e-commerce, gaming, and finance. (2) Mention the potential for extracting valuable insights via machine learning, but emphasize challenges like unlabeled data and fragmented research efforts.

The rapid growth of digital interactions, ranging from online purchases and social media engagements to automated sensors in healthcare, continues to generate massive volumes of event stream (ES) data. Formally, an ES is a {\it continuous sequence of timestamped events, each encapsulating structured contextual information}. Industries such as healthcare, finance, gaming, and e-commerce have substantial ES repositories: healthcare systems store Electronic Health Records (EHRs) tracking patient admissions and diagnostic tests \shortcite{johnson2023mimic-iv,herrett2015data-cprd}, while e-commerce platforms log user interactions like clicks and purchases \shortcite{wang2019sequential,sun2019bert4rec}, among many other examples.

This surge of context-rich, time-dependent ES data holds potential for a range of downstream applications: e.g., diagnosing a patientâ€™s evolving condition in healthcare \shortcite{li2020behrt}, detecting fraudulent transactions in finance \shortcite{babaev2022coles}, or personalizing game content to enhance player engagement \shortcite{wang2024player2vec,pu2022unsupervised}. However, large-scale event datasets typically lack the extensive {\it labeling} needed to train traditional supervised learning systems. Moreover, existing modeling efforts often remain {\it fragmented} across domains, sometimes replicating research efforts without leveraging the many structural similarities shared by event streams across industries. These challenges underscore the key demand for ES modeling approaches -- ones that can meaningfully learn from raw event logs with minimal or no human annotation.

%Scope of the Survey: (1) Clarify why SSL is highly relevant for ES (e.g., vast unlabeled data, temporal and contextual complexities). (2) Introduce the main goals}: unify domain-specific methods, identify gaps, offer future directions.

\subsection{Aim and Scope}

Self-Supervised Learning (SSL) stands out as a powerful paradigm well-suited to address these challenges. By deriving supervisory signals directly from unlabeled data, SSL can learn informative representations of sequences without relying on manual annotation. In the context of ES, SSL enables models to capture temporal dynamics and contextual relationships, which are often critical for tasks such as predicting future events, clustering entity behaviors, or detecting anomalies. Despite the evidence that SSL has shown major advances in fields like language modeling~\shortcite{devlin2018bert,brown2020language} and computer vision~\shortcite{he2022masked,oquab2023dinov2}, its application to ES modeling has not yet converged into a unified body of knowledge. 
Current ES research typically targets a specific domain (e.g., healthcare, gaming, e-commerce, finance) without systematically building on or comparing methods across these verticals.

Therefore, the purpose of this survey is to (1) unify the progress in SSL for event streams across multiple domains, (2) identify critical gaps and challenges that cut across those fields, and (3) propose future directions to foster development of more general, domain-agnostic SSL approaches for ES modeling. By addressing these diverse aspects, we aim to accelerate innovation, improve reproducibility, and highlight emerging themes that can benefit the ES modeling community at large.

To ensure comprehensive yet focused coverage, we reviewed papers from leading conferences such as ICML, NeurIPS, AAAI, and KDD, as well as journals in machine learning, artificial intelligence, and knowledge discovery, along with domain-specific journals, such as those in healthcare. Beginning with an initial pool of 100 works, we refined our selection by prioritizing recent studies introducing novel concepts alongside seminal works with substantial impact on the field, as evidenced by their citation influence. The final distribution of surveyed works is presented in Figure~\ref{fig:literature-statistics}. This approach ensures our review captures both the breadth and depth of SSL methods applied to ES data.

\begin{figure}[ht!]
    \centering
    % Subfigure 1
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/papers-per-ssl.pdf}
        \caption{}
        \label{fig:literature-ssl-stats}
    \end{subfigure}
    % Subfigure 2
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/papers-per-domain.pdf}
        \caption{}
        \label{fig:literature-domain-stats}
    \end{subfigure}

    \caption{Distribution of the reviewed works across (a) various SSL paradigms and (b) diverse application domains.}
    \label{fig:literature-statistics}
\end{figure}

\begin{figure}[ht!]
    %\centering
    %\input{survey-tree-star.tex}
    %\caption{Overview of related surveys. The rectangles in the bottom half represent different modalities, while ellipses in the top half signify domain-specific surveys covering \textcolor{cbGreen}{healthcare}, \textcolor{cbBlue}{e-commerce}, and \textcolor{cbOrange}{gaming}. The connections between modalities illustrate how ES can be adapted to these formats, highlighting its role as a unified and generalized framework that integrates these specific domains.}
    %\label{fig:ssles-survey-tree-diagram}
\end{figure}

\begin{table}[ht]
    \centering
    \footnotesize
    \renewcommand{\arraystretch}{1.8} % Adjust row height
    \setlength{\tabcolsep}{4pt} % Adjust column width
    \begin{tabular}{c|c|c|c|c|c|c|c|c|c|c}
        & & \rotatebox{75}{\shortciteA{shchur2021neural}} 
        & \rotatebox{75}{\shortciteA{dong2022table}} 
        & \rotatebox{75}{\shortciteA{zhang2024self}} 
        & \rotatebox{75}{\shortciteA{wornow2023shaky}} 
        & \rotatebox{75}{\shortciteA{wang2019sequential,wang2021survey}} 
        & \rotatebox{75}{\shortciteA{yu2023self}} 
        & \rotatebox{75}{\shortciteA{hooshyar2018data}} 
        & \rotatebox{75}{\shortciteA{liang2024foundation}} 
        & \rotatebox{75}{\textbf{Our work}} \\ \hline
        \multirow{3}{*}{\rotatebox{90}{\textbf{Modality}}} 
        & \makecell{Continuous-Time\\ Event Sequences} & \cmark & & & \cmark & \cmark & \cmark & & & \cmark \\ \cline{2-11}
        & Tabular & & \cmark & &  &  & & \cmark & & \cmark \\ \cline{2-11}
        & Time Series & & & \cmark & & & & & \cmark & \cmark \\ \hline
        \multirow{4}{*}{\rotatebox{90}{\textbf{Domain}}} 
        & Healthcare & \cmark & & \cmark & \cmark & & & & \cmark & \cmark \\ \cline{2-11}
        & Finance & & & \cmark & & & & & \cmark & \cmark \\ \cline{2-11}
        & Gaming & & & & & & & \cmark & & \cmark \\ \cline{2-11}
        & E-commerce & \cmark & & & & \cmark & \cmark & & & \cmark
    \end{tabular}
    \caption{Comparison of related surveys' coverage for modalities and application domains.}% * not the main focus of the survey.}
    \label{tab:survey-comparison}
\end{table}

\subsection{Contribution and Structure}
This survey offers several key contributions:
\begin{itemize}
    \item {\bf Comprehensive synthesis across domains}: We provide the first (to our knowledge) cross-domain assessment of SSL-based ES modeling. While prior work has surveyed either specific application domains (e.g., healthcare, e-commerce, and gaming) or other data modalities (e.g., tabular, time series, and continuous-time sequences), we argue that ES data has unifying structural properties that make it a distinct category meriting an overarching view. We provide an overview of how the present survey is positioned relative to others that address different modalities or domains in Table~\ref{tab:survey-comparison}. % We visualize this difference in Figure~\ref{fig:ssles-survey-tree-diagram}. 
    
    \item {\bf Taxonomy of SSL paradigms for ES}: We present a structured taxonomy that organizes the wide spectrum of SSL methods used in ES modeling, ranging from {\it predictive} techniques (e.g., masked modeling, autoregressive strategies, and temporal point processes) to various {\it contrastive} approaches (e.g., instance-based, distillation-based, and multimodal). By emphasizing shared technical motifs, we identify cross-domain synergies that can strengthen and unify existing approaches
    \item {\bf Resource and benchmark guide}: We compile an overview of widely used public datasets, identify limitations in current benchmarking practices, and suggest how future research can more reliably evaluate and compare models. We also point out the scarcity of multi-domain benchmarks for ES and the need for stronger open-source resources.
    \item {\bf Roadmap for future research}: We outline critical open issues in ES SSL, ranging from timestamp modeling to more robust augmentations, and discuss how next-generation methods could push the boundaries of performance, scalability, and interpretability in real-world applications.
\end{itemize}

The remainder of this paper is organized as follows. Section~\ref{sec:event-stream-data} introduces the fundamentals of event streams and key application domains. Section~\ref{sec:ssl-es-review} discusses SSL paradigms, including predictive and contrastive approaches, and presents our taxonomy of existing SSL-based methods for event streams. Section~\ref{sec:es-data-overview} reviews common ES datasets and downstream evaluation tasks. In Section~\ref{sec:progress-prospects}, we synthesize open challenges and propose future research directions. Finally, Section~\ref{sec:conclusion} concludes with key findings and broader implications for data-driven ES modeling.

\section{Foundations of Event Streams}
\label{sec:event-stream-data}

Event Stream (ES) data provides a flexible and expressive data format for representing and modeling sequential events across diverse domains. In the following subsections, we (1) establish the definition and notation (Section~\ref{subsec:def-notation}) for the structured foundation and key components for reasoning about ES data, (2) introduce the relation of ES data to other data modalities (Section~\ref{subsec:other-modality}), and (3) highlight the application domains and potential challenges  (Section~\ref{subsec:application-domains}) when working with ES data.

\subsection{Definition and Notation}
\label{subsec:def-notation}

An ES is a continuous, ordered sequence of events generated over time by one or more sources. Each event encapsulates timestamped information about a specific action or state. While each event is a discrete occurrence, the ES represents an ongoing, continuous flow of events, which is a common characteristic of real-world scenarios.

An \textit{entity} $u$, such as a user or a patient, is typically associated with each ES. Formally, an ES for an entity $u$ can be defined as a sequence: 
\begin{equation}
\label{eq:es-def}
S_u = {\{e_{u,i}\}}^\infty_{i=1}, \text{where } e_{u,i} = (t_{u,i}, d_{u,i}).
\end{equation}
$e_{u,i}$ represents the $i$-th event in the ES for entity $u$. $t_{u,i} \in \mathcal{T}$ is the timestamp of $e_{u,i}$ from a totally ordered time domain $\mathcal{T}$, ensuring $t_{u,i} \leq t_{u,i+1}$ for all $t \in \mathcal{T}$. $d_{u,i} \in \mathcal{D}$ represents all information associated with $e_{u,i}$, where $\mathcal{D}$ is the domain of all possible event data.

All observed entities in a dataset are represented as $U = \{u_i\}_{i=1}^m$, where $m$ denotes the total number of entities. To simplify notation, the entity indicator $u$ is omitted in sequences when it is contextually clear that the events belong to the same entity. The set of all sequences across all entities is represented as an ES dataset, $\mathcal{S} = \{S_1, S_2, \ldots, S_m\}$. 

Although an ES is theoretically infinite in length as new events continuously occur, practical analysis often considers a finite realization of the stream up to a specific time $T \in \mathcal{T}$. This realization contains only the events observed until time $T$, as indicated by the red vertical line in Figure~\ref{fig:es-data-vis}. Notably, event streams from different entities, such as $u_1$ and $u_2$, may exhibit varying numbers of events recorded within the same time frame up to $T$.

In its most common form, $d_i$ represents data from various modalities, often structured as a dictionary containing an arbitrary number of key-value pairs. For simplicity, we omit the entity indicator $u$ and refer to $d_i$ instead of $d_{u,i}$. For example, in the context of a patient undergoing an X-ray, the event data might include the X-ray image, the hospital's identifier, details about the imaging equipment, and the physician's evaluation.

In practice, the information encapsulated in $d_i$ is typically compressed into a fixed-length $n$-dimensional vector representation, denoted as $f_e(d_i) = \Tilde{d}_i \in \mathbb{R}^n$, via an event data encoder $f_e: \mathcal{D} \rightarrow \mathbb{R}^n$, where $n$ represents the embedding size for all event representations $\Tilde{d}_i$. It is worth noting that designing an effective encoder for diverse event data types is a complex and nontrivial task, though exploring such methods falls beyond the scope of this survey. A visual depiction of this representation is provided in Figure~\ref{fig:es-data-vis}.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/ES-data-visualisation.pdf}    
    \caption{Illustration of ES data for individual entities $u, u+1, \ldots$. Each event in an entityâ€™s ES, as exemplified using $e_{u,1}$, is characterized by a timestamp and associated data, which is encoded into a fixed-size feature vector $\Tilde{d}_
    {u,1}$ using a feature encoder $f_e$. The diagram showcases the temporal sequence of events for multiple entities and highlights the role of the encoder in transforming diverse event data into standardized representations for analysis.}
    \label{fig:es-data-vis}
\end{figure}


\subsection{Relation to Other Modalities}
\label{subsec:other-modality}
ES data can be transformed into various modalities to leverage established machine learning models and techniques. Each of these transformations facilitates the use of methods from well-studied modalities but introduces specific trade-offs, such as sparsity in tabular formats or limitations on event data space in time series representations. For instance, one approach is to convert ES data into a tabular format where each row represents an event, with columns capturing its associated attributes. However, this approach often results in sparse datasets with numerous missing or irrelevant entries, which can adversely affect model performance. Furthermore, tabular representations fail to incorporate the temporal dimension, making it difficult to capture the evolving dynamics inherent in ES data.

Another approach is to transform ES data into a time series, treating events as a continuous sequence. While this transformation can model temporal trends, it assumes that data is sampled at regular intervals, which is often not the case for ES, where events occur irregularly over time. Additionally, this method restricts the data space $\mathcal{D}$ by constraining events to time-indexed continuous values, excluding categorical, ordinal, and other discrete outcomes.

Dynamic graph representations offer another approach for converting ES data, where entities are modeled as nodes and events or interactions as time-evolving edges. While graphs excel at capturing relational structures, applying them to ES data can introduce challenges. The inherent temporal granularity of ES may result in highly dynamic graphs with frequent edge updates, leading to computational inefficiencies and scalability issues. Furthermore, the evolving nature of dynamic graphs can obscure the sequential order of events, which is crucial for many downstream tasks.

Moreover, the definition of ES extends the framework of modeling continuous-time event sequences with temporal point processes (TPPs)~\shortcite{du2016recurrent}. While TPPs focus on generative mechanisms for event occurrence with an emphasis on event timing, they do not address the broader objectives of self-supervised ES modeling and downstream tasks. TPPs are often constrained by specific assumptions about event data, such as discrete markers in marked TPPs~\shortcite{mei2017neural} or continuous spatial domains in spatiotemporal point processes~\shortcite{chen2020neural}. These limitations reduce the ability to incorporate diverse metadata associated with real-world events and restrict the flexibility of TPPs in learning versatile entity representations for downstream applications. ES data modeling overcomes these constraints by supporting a wider range of event types and flexible learning objectives, which are critical for diverse applications of SSL.

\subsection{Application Domains and Challenges}
\label{subsec:application-domains}
Event stream data arises across a broad spectrum of real-world settings, each accompanied by domain-specific challenges and modeling requirements. Below, we highlight four key application domains and discuss corresponding issues that underscore the complexity of ES data. These domains are carefully chosen to illustrate the breadth of ES methodologies and their adaptability to varying real-world needs. A concise mapping of ES concepts across these domains is presented in Table~\ref{tab:notation}.

\begin{table}[ht!]
    \centering
    \footnotesize
    \renewcommand{\arraystretch}{1.5} % Adjust row height
    \setlength{\tabcolsep}{2pt} % Adjust column spacing
    \begin{tabular}{>{\centering\arraybackslash}p{0.15\linewidth}|
                        >{\centering\arraybackslash}p{0.2\linewidth}|
                        >{\centering\arraybackslash}p{0.2\linewidth}|
                        >{\centering\arraybackslash}p{0.2\linewidth}|
                        >{\centering\arraybackslash}p{0.2\linewidth}}
        \textbf{Concept} & \textbf{Healthcare} & \textbf{Finance} & \textbf{Gaming} & \textbf{E-commerce} \\ \hline
        Entity & Patient & Stock ticker or account & Player & Customer \\ \hline
        Event & Hospital admission, diagnostic test & Buy/sell order, transaction log & Player action, game state change & Page view, click, purchase \\ \hline
        Categorical Feature & Hospital department, diagnosis type & Transaction type (buy/sell), stock category & Item type, game level & Product category, payment method \\ \hline
        Numerical Feature & Patient's age, test result & Transaction amount, stock price & In-game score, player level & Item price, time spent on page \\ \hline
        Timestamp & Time of admission, test completion & Time of transaction, stock price update & Time of action, level completion & Time of click, purchase \\ \hline
        Event Stream & Sequence of medical events in an EHR & Sequence of trades or transactions for a stock or account & Sequence of player actions or game events & Sequence of user interactions (clicks, purchases, etc.)
    \end{tabular}
    \caption{Mapping of ES concepts to examples across different application domains.}
    \label{tab:notation}
\end{table}

\begin{enumerate}
\item \textbf{Healthcare}: Hospitals and clinics record a continuous flow of patient information, such as admissions, diagnoses, lab results, and treatments. These clinical events, each characterized by a timestamp and metadata, form a patientâ€™s EHR. Modeling patient trajectories through EHR data can unlock valuable insights for disease progression, risk stratification, and personalized care ~\shortcite{johnson2023mimic-iv,herrett2015data-cprd}. However, privacy regulations (e.g., HIPAA\footnote{https://www.hhs.gov/hipaa} and GDPR\footnote{https://gdpr-info.eu}) limit data sharing, hindering the development of large-scale public benchmarks. Moreover, healthcare data often exhibits noise from measurement errors, missing event attributes, and inconsistent documentation practices across different medical institutions.

\item \textbf{E-commerce}: User interactions, such as product page views, clicks, cart additions, and purchases, are recorded as event streams on online platforms. Sequential recommender systems (SRS)~\shortcite{sun2019bert4rec,zhou2020s3} rely on these data to predict future user actions and personalize the shopping experience. While typically abundant, e-commerce event streams can suffer from {\it implicit feedback} noise (e.g., accidental clicks or browsing behavior not directly correlated with user interest). Additionally, item catalogs and user demographics change over time, introducing concept drift that demands adaptive modeling techniques.

\item \textbf{Finance}: Financial institutions and trading platforms generate event streams through a variety of transactions, including stock trades, credit card purchases, and blockchain-based transfers~\shortcite{hu2023bert4eth,babaev2022coles}. In these contexts, timeliness and accuracy are paramount, as decisions often depend on real-time signals for fraud detection or high-frequency trading. Financial ES data may involve incomplete transaction details or delayed reporting, complicating model training and deployment. Moreover, financial data often contains sensitive or proprietary information, leading to restricted availability of large-scale public datasets.

\item \textbf{Gaming}: Game developers log in-game actions such as user movements, item pickups, and interactions with non-player characters. This data helps to analyze gameplay, improve design, and personalize experiences~\shortcite{kantharaju2020discovering,min2016player}. However, gaming logs can be extremely noisy -- players may experiment randomly, idle in the game, or perform repetitive tasks to ``farm'' resources. The rapid and often unpredictable evolution of game rules, patches, or newly introduced features further exacerbates modeling difficulties. Additionally, balancing real-time feedback (e.g., matchmaking) with large-scale event processing remains a significant engineering challenge.
\end{enumerate}

While the details differ by application domain, ES data in these domains shares common difficulties: (1) \textit{irregular sampling}, as event arrivals can be sporadic, requiring flexible timestamp modeling; (2) \textit{diverse feature types}, because each event can encompass a wide range of features, from discrete labels to unstructured text or images; and (3) \textit{privacy and ethical concerns}, especially in healthcare or finance, where the improper handling of sensitive records has serious legal and ethical implications. These overlapping challenges collectively motivate the exploration of self-supervised approaches that may effectively leverage unlabeled event streams, mitigate noise, and handle complex temporal dependencies across domains.

\section{Self-Supervised Learning for Event Streams}
\label{sec:ssl-es-review}
Self-supervised learning has become a cornerstone for ES modeling, as it enables the effective utilization of vast volumes of unlabeled data -- a common characteristic of real-world ES datasets. By designing alternative tasks, known as ``pretext'' tasks, SSL uses the data itself as supervisory signals~\shortcite{liu2021self}. This approach is particularly crucial for ES modeling, where labeled data is often sparse, incomplete, or expensive to obtain. Through pretext tasks, SSL facilitates the extraction of meaningful representations during an initial pre-training phase, capturing the complex temporal and contextual relationships inherent in ES data. These pre-trained representations serve as a robust foundation, allowing models to either be fine-tuned on limited labeled datasets for specific downstream tasks or directly applied in a zero-shot manner, bypassing the need for additional task-specific training. This dual capability makes SSL not only instrumental but often a prerequisite for building effective ES modeling pipelines in diverse domains.

\begin{figure}[ht!]
    \centering
    \input{Literature-tree-reworked.tex}
    \caption{Overview of SSL methods for ES in application to \textcolor{cbBlue}{e-commerce}, \textcolor{cbGreen}{healthcare}, \textcolor{cbOrange}{gaming}, and \textcolor{cbRed}{finances}, as well as generic methods that are not tailored to any specific domain. Approaches employing multiple learning objectives across different paradigms are included under each applicable category.}
    \label{fig:ssles-tree-diagram}
\end{figure}

In the following subsections, we first give a brief overview of SSL principles (Section~\ref{subsec:ssl-overview}). We then organize current ES research into two major families: (1) {\textit predictive SSL}, which reconstructs or predicts missing or corrupted events (Section~\ref{subsec:predictive-ssl}), and (2) {\textit contrastive SSL}, which learns a representation space that brings similar event sequences closer and pushes dissimilar ones apart (Section~\ref{subsec:contrastive-ssl}). This taxonomy is visualized in Figure~\ref{fig:ssles-tree-diagram}, where we also highlight how these paradigms manifest across the diverse domains specified in Table~\ref{tab:notation}.

\subsection{Overview of SSL Principles}
\label{subsec:ssl-overview}

In essence, SSL aims to create {\it pretext tasks} that derive supervisory signals directly from unlabeled data. For event streams, such tasks typically involve:
\begin{itemize}
\item \emph{Masking} or \emph{corrupting} certain events or attributes, then training the model to reconstruct them.
\item \emph{Autoregressive} next-event (or next-token) prediction, relying only on previously observed events.
\item \emph{Comparing} sequences or subsequences under various augmentations to learn semantic representations.
\end{itemize}

Because ES data often contains rich metadata (Section~\ref{subsec:def-notation}) with timestamped and potentially high-dimensional event features, SSL methods can exploit temporal structure, complex contextual cues, and large-scale unlabeled sequences for representation learning. This stands in contrast to supervised learning, which is limited by the availability of high-quality labels in each domain. SSL, therefore, accelerates progress by leveraging all observed events, including those without explicit annotation, to learn robust event representations, enabling further fine-tuning or zero-shot applications for downstream tasks.

Standard SSL paradigms in broader scope include \emph{predictive} approaches such as Masked Vision Autoencoders~\shortcite{he2022masked} and BERT~\shortcite{devlin2018bert}, along with \emph{contrastive} methods exemplified by SimCLR~\shortcite{chen2020simple} and MoCo~\shortcite{he2020momentum}. As we show in Sections~\ref{subsec:predictive-ssl} and \ref{subsec:contrastive-ssl}, both of these map naturally onto ES data, while also accommodating the unique constraints posed by irregular sampling or multi-modal event attributes.

\subsection{Predictive SSL}
\label{subsec:predictive-ssl}

Predictive SSL focuses on learning representations by predicting or reconstructing masked or missing parts of the event stream. This family of methods often draws inspiration from natural language processing (NLP), where large language models (LLMs) are pre-trained to predict masked tokens such as BERT \shortcite{devlin2018bert} or the next token in an autoregressive manner, as exemplified by GPT \shortcite{radford2018improving}. Below, we review three core predictive techniques for ES: \textit{masked modeling}, \textit{autoregressive modeling}, and \textit{temporal point processes}.

\subsubsection{Masked Modeling}
\label{subsubsec:masked-modeling}

In {\it masked} (or {\it denoising}) modeling, the learner randomly masks out certain events, event attributes, or timestamps within a sequence and aims to predict the masked portions using the remaining context. This approach is inspired by the masked language modeling (MLM) paradigm~\shortcite{devlin2018bert}, where tokens are hidden and the model must recover them.
For ES, this can mean \emph{masking entire events} or \emph{selectively masking} specific features (e.g., diagnosis codes in healthcare, item identifiers in e-commerce). Examples of masked SSL for ES from different domains include:
\begin{itemize}
\item Healthcare: BEHRT~\shortcite{li2020behrt}, BRLTM~\shortcite{meng2021bidirectional}, and Med-BERT~\shortcite{rasmy2021med} pre-train transformer models to impute masked diagnoses or procedure codes. These models mainly vary in their event embedding strategies and the information extracted from patient records, typically discretizing continuous features and summing their embeddings to form final representations.

\item E-commerce: BERT4Rec~\shortcite{sun2019bert4rec} applies a BERT-like approach to sequences of user-item interactions, masking items that the user actually engaged with and learning to reconstruct them.

\item Finance: BERT4Eth~\shortcite{hu2023bert4eth} adapts the BERT framework to transaction data from the Ethereum blockchain, using masked crypto addresses as input. Adjustments like increased masking and dropout rates address the specific properties of blockchain data.

\item Gaming: \shortciteA{pu2022unsupervised} utilize a BERT-based framework for player representation learning in massively multiplayer online games, incorporating a confidence-guided masking strategy to focus on significant actions. Player2Vec~\shortcite{wang2024player2vec} adapts the masked objective to mobile game behavior modeling, leveraging the Longformer~\shortcite{beltagy2020longformer} architecture to capture long-range dependencies within complex player sessions.
\end{itemize}

Notably, further refinements to these models have emerged across domains. For example, SARD~\shortcite{kodialam2021deep} enhances EHR modeling by adding a reverse distillation signal during pre-training and expanding the event vocabulary. DuETT~\shortcite{labach2023duett} introduces a dual-attention architecture pre-trained with a masked objective. For SRS, S3Rec~\shortcite{zhou2020s3} extends masked modeling by introducing additional pre-training objectives around the masking paradigm, coupled with mutual information maximization as a loss function.

By focusing on partial observations and contextual relationships across timestamps, masked modeling can effectively handle the structured, high-dimensional attributes of ES data. In particular, its ability to attend to flexible contexts, rather than relying on uniform sampling intervals, makes it well-suited to irregular event arrivals that frequently occur in real-world ES scenarios.

\subsubsection{Autoregressive Modeling}
\label{subsubsec:ar-modeling}
Autoregressive (AR) approaches for ES revolve around predicting the {\it next event} (or next attribute) given the historical context. These methods extend concepts from language modeling \shortcite{radford2018improving}, forcing a strictly causal order -- using only past events to predict future ones.

Many SRS adopt this AR perspective, where the AR objective is commonly applied to sequences of user-item interactions, aiming to predict the next item~\shortcite{tang2018personalized,luo2023mcm,yuan2019simple}. While many methods focus on architectural improvements, the learning objective often remains consistent. For example \shortciteA{li2018learning} introduced dual recurrent networks to capture short- and long-term dependencies, while attention-based approaches~\shortcite{wang2020jointly,ying2018sequential} have also been employed for similar purposes.

To avoid computing normalizing constants in loss functions, techniques like noise contrastive estimation (NCE)~\shortcite{gutmann2010noise} have been adopted, as seen in \shortciteA{wang2020jointly,yuan2019simple,hou2022towards}, or by measuring the distance between predicted and target item embeddings~\shortcite{li2018learning}. Contrastive learning approaches for SRS often incorporate AR objectives as auxiliary signals due to their alignment with the primary task~\shortcite{xie2022contrastive,wang2023sequential,ma2020disentangled}.

Beyond SRS, AR objectives have also been applied in gaming. \shortciteA{zhao2021player} used AR models to generate realistic action sequences for role-playing games, leveraging player behavior patterns. \shortciteA{kantharaju2020discovering} used AR objectives to encode gameplay sessions, enabling unsupervised strategy label discovery through embeddings. Similarly, \shortciteA{min2016player} pre-trained action representations for open-world games using AR objectives, enhancing performance on goal recognition tasks.

Autoregressive models are intuitive for ES since many tasks, such as predicting future purchases, patient visits, or player actions, naturally align with next-event prediction. However, purely AR training can overemphasize short-horizon patterns at the expense of longer-term dependencies. To address timing gaps more faithfully, \shortciteA{li2020time} introduced elapsed-time embeddings, providing a more nuanced temporal perspective. These efforts underscore the benefits of AR models for ES, while illustrating the need for time-aware or hybrid solutions in more complex real-world scenarios.

\subsubsection{Temporal Point Processes}
\label{subsubsec:tpp}

A Temporal Point Process (TPP) is a probabilistic framework designed to model \emph{when} events occur over time. Rather than predicting the next token in a sequence (as in standard AR models), TPPs specify a \emph{conditional intensity function} that depends on the history of previous events to determine the timing of future ones. In this sense, TPPs serve as AR models with a unique focus on irregular event arrivals. They are particularly suited to domains emphasizing the temporal dimension, such as high-frequency trades in finance~\shortcite{guo2018initiator} or retweet cascades in social media~\shortcite{zhao2015seismic}.

When modeling ES data, TPPs can be viewed as a specialized form of predictive SSL, since they typically learn without external labels by maximizing the likelihood of observed event times (and optionally their attributes). Concretely, each event is represented by a timestamp $t_{u,i}$ (cf.~Equation~\ref{eq:es-def}), and in the case of marked TPPs (MTTPs), also by a discrete label $m_i$. This representation can be limiting for ES with richer metadata $\mathcal{D}$ -- for example, numerical or textual features -- since TPPs often restrict such attributes to a single discrete mark set $\mathcal{M}$. Nonetheless, for problems where timing is paramount and events can be categorized with discrete labels, TPPs offer a principled approach to capturing temporal patterns within ES data.

{\bf ES-specific TPP methods.}
The pioneering work of \shortciteA{du2016recurrent} introduced neural architectures for MTPPs, encoding event histories with recurrent neural networks and conditioning the intensity function on these representations; later, several methods have been developed for MTPPs~\shortcite{omi2019fully,shchur2019intensity,guo2018initiator,mei2020noise,chen2020neural,zhang2024neural}. Building on this foundation, \shortciteA{omi2019fully} proposed a fully neural TPP model, relaxing the parametric assumptions of earlier methods, while \shortciteA{shchur2019intensity} moved away from intensity function modeling altogether by directly estimating the distribution of inter-event times. Other approaches incorporate Noise Contrastive Estimation (NCE)\shortcite{gutmann2010noise} to optimize TPP parameters without computing normalizing constants~\shortcite{guo2018initiator,mei2020noise}. Further innovations include:
\begin{itemize}
\item Extensions to spatial settings: Spatiotemporal point processes~\shortcite{chen2020neural} incorporate event locations, with $\mathcal{D} = \mathbb{R}^d$, but still focus on timing plus a continuous spatial component rather than general high-dimensional metadata.

\item Self-exciting dynamics: Neural adaptations of Hawkes processes~\shortcite{hawkes1971spectra}, such as the Neural Hawkes Process~\shortcite{mei2017neural}, capture {\it self-exciting} event behavior, where occurrences of events raise the likelihood of subsequent arrivals for a period.

\item Attention and ODE-based TPPs: Transformer adaptations~\shortcite{zuo2020transformer-hawkes,zhang2020self-attentive,yang2022transformer} improve modeling of long-term dependencies, while neural ODE frameworks~\shortcite{chen2020neural,zhang2024neural} support continuous-time hidden states for greater flexibility.

\item Combining contrastive signals: HYPRO~\shortcite{xue2022hypro} and the work of \shortciteA{wang2023hierarchical} integrate {\it contrastive learning} at the sequence level to refine long-horizon TPP forecasts (see Section~\ref{subsubsec:instance-contrastive} for details).
\end{itemize}
Collectively, these TPP variants remain largely domain-agnostic and have been evaluated on diverse benchmarks, including financial transactions and social streams.
The main advantage of TPPs for ES modeling is their {\it direct emphasis on time modeling}: instead of assuming regularly sampled data, TPPs natively handle irregular arrival patterns, which are ubiquitous in real-world ES. For tasks where {\it when} an event occurs is as crucial as {\it what} happens, e.g., predicting the gap between trades or hospital visits, TPPs are a powerful predictive framework.
On the downside, traditional TPPs often require event attributes to be reduced to a discrete mark set, limiting their ability to capture richer, multi-dimensional features that many ES domains present (cf.~Section~\ref{subsec:other-modality}). Consequently, if the ultimate goal is {\it task-agnostic} SSL-based representation learning (for instance, for clustering patients or analyzing user behavior broadly), TPPs may need complementary mechanisms or hybrid approaches to fully leverage the wealth of metadata beyond simple event timing and discrete categories.

\subsection{Contrastive SSL}
\label{subsec:contrastive-ssl}

Contrastive SSL methods learn a representation space where \emph{similar} event streams or subsequences are pulled together, while \emph{dissimilar} ones are pushed apart \shortcite{liu2021self}. This high-level approach typically avoids reconstructing low-level details, which may be noisy or irrelevant, and can thereby learn more robust \emph{entity-level} embeddings. As illustrated in Figure~\ref{fig:ssles-tree-diagram}, contrastive methods remain relatively underutilized in ES, yet they offer strong potential across diverse application settings.

\subsubsection{Instance Contrastive}
\label{subsubsec:instance-contrastive}

Instance-based contrastive learning aims to learn discriminative representations by treating different augmentations of the \emph{same} event stream instance as \emph{positive} pairs and instances (or their augmentations) from \emph{other} entities as \emph{negative} pairs. This paradigm can be traced back to frameworks like SimCLR~\shortcite{chen2020simple} and MoCo~\shortcite{he2020momentum}, which rely on semantic-preserving data transformations, or ``augmentations,â€™â€™ to drive representation learning. Formally, if $\mathcal{A}$ is a set of possible augmentations, we draw two random transformations $a_1, a_2 \sim \mathcal{A}$ and apply them to an event stream $S_u$. The resulting views $a_1(S_u) = {S}^{(1)}_u$ and $a_2(S_u) = S^{(2)}_u$ form a {\it positive} pair, while event streams from other entities play the role of {\it negative} samples.

{\bf ES-specific instance contrastive.} Unlike computer vision, where image augmentations include simple pixel-level transformations such as rotation or color jitter, ES data is {\it irregular} and {\it discrete}, often containing complex contextual attributes. Consequently, augmentations typically focus on:
\begin{itemize}
\item Subsequence sampling or cropping: Randomly selecting continuous chunks or prefix-suffix segments from the entityâ€™s event history~\shortcite{babaev2022coles,jeong2023event,ma2020disentangled}.

\item Event masking: Concealing certain events or attributes (like timestamps, categorical codes) to simulate partial observation~\shortcite{xie2022contrastive,hou2022towards}.

\item Noise injection: Slightly perturbing numeric features (e.g., lab results in healthcare)~\shortcite{raghu2023sequential}.

\item Item/interaction reordering: Primarily in recommender systems with loosely constrained temporal order \shortcite{xie2022contrastive,wang2023sequential}.
\end{itemize}
These ES-specific augmentations aim to preserve semantic integrity (i.e., the inherent structure of the entityâ€™s behavior) while ensuring enough variation to learn robust, high-level representations.
Several representative instance contrastive methods have emerged across different ES domains:
\begin{itemize}
\item Sequential recommenders: CL4SRec~\shortcite{xie2022contrastive} combines event masking, subsequence cropping, and item reordering to generate positive pairs of user interaction sequences, while negative samples come from other users. Other works~\shortcite{qiu2022contrastive,hou2022towards,wang2023sequential} adapt similar ideas, sometimes adding \emph{supervised} pair-mining (e.g., user sessions ending in the same item).

\item Finance: CoLES~\shortcite{babaev2022coles} proposes random subsequence sampling for transaction logs, using the InfoNCE objective to pull together augmentations of the same accountâ€™s activity while pushing apart different accounts.

\item Healthcare: \shortciteA{raghu2023sequential} mask entire modalities (e.g., vitals vs.~labs) and add noise to EHR data, employing the SimCLR-based NT-Xent loss for contrast. \shortciteA{jeong2023event} define positive pairs as prefix and suffix segments around critical hospital visits.

\item Combining TPP: \shortciteA{wang2023hierarchical} and \shortciteA{xue2022hypro} combine TPP's autoregressive objectives with an instance contrastive signal to differentiate authentic event continuations from artificially generated ones.
\end{itemize}

Instance-based contrastive SSL is conceptually straightforward and avoids the need to reconstruct potentially noisy event attributes, focusing instead on alignment of semantically consistent representations. This can yield robust entity-level embeddings for complex, high-dimensional streams. However, several practical challenges persist:
\begin{itemize}
\item Defining valid augmentations: ES data is irregular and domain-specific, making it non-trivial to design transformations (e.g., reordering vs. partial masking) that preserve critical chronological or contextual relationships.
\item Negative sampling: Methods must ensure negative pairs genuinely represent different entities or distinct behaviors, especially if multiple entities have partially similar sequences.
\item Data sparsity: Sparse or short event sequences limit opportunities for subsequence-based augmentations or heavier masking without losing crucial information.
\end{itemize}
Despite these hurdles, instance-based contrastive learning remains a promising strategy for leveraging unlabeled ES in diverse domains, with growing evidence that carefully chosen augmentations and objectives can substantially improve downstream performance.

\subsubsection{Distillation}
\label{subsubsec:distillation-methods}

Distillation-based contrastive learning avoids explicit negative samples by coupling a {\it teacher} network and a {\it student} network on differently augmented views of the same event stream. Inspired by methods such as BYOL~\shortcite{grill2020bootstrap}, SimSiam~\shortcite{chen2021exploring}, and DINO~\shortcite{caron2021emerging}, the student aims to predict the teacherâ€™s latent representations. Typically, both networks share the same architecture but differ in parameters: the teacher either remains fixed \shortcite{chen2021exploring} or updates its parameters via an exponential moving average of the student's weights \shortcite{grill2020bootstrap}. By optimizing a loss function that aligns the two representations, the model avoids collapse (i.e., trivial solutions) and learns robust embeddings without requiring negative pairs.

Distillation-based approaches remain relatively rare in ES contexts. One notable example is Hi-BEHRT~\shortcite{li2022hi-behrt}, which builds on the BEHRT framework for EHR data~\shortcite{li2020behrt}. In Hi-BEHRT, two augmented views of a patient's event sequence are created (e.g., via event masking or subsequence sampling). The teacher and student networks independently encode these augmented sequences, and a BYOL-style distillation loss forces the studentâ€™s latent representation to match that of the teacher. By circumventing the need for negative sampling, Hi-BEHRT can focus more directly on capturing meaningful patterns in clinical data, particularly when event streams are irregular or domain-specific.

Distillation-based frameworks avoid explicit negative sampling, making them appealing in domains where negative examples are ambiguous or tedious to define. Nevertheless, designing effective augmentations (e.g., event masking or subsequence cropping) remains a key challenge, and the teacher-student alignment can lead to overfitting if spurious correlations are learned.

\subsubsection{Feature Decorrelation}
\label{subsubsec:feature-decorrelation}

Feature decorrelation methods aim to learn representations with {\it minimal redundancy} across latent dimensions, typically without explicit negative pairs. Frameworks like Barlow Twins~\shortcite{zbontar2021barlow} and VICReg~\shortcite{bardes2021vicreg} proceed by generating two augmented views of the same sequence and passing them through a shared model. The loss function then combines two components: (1) an {\it alignment} term, pulling the two latent embeddings closer, and (2) a {\it redundancy reduction} term, encouraging different latent dimensions to capture distinct information.

Feature decorrelation remains much less explored than predictive or instance-based contrastive methods in ES contexts. One notable exception is \shortciteA{raghu2023sequential}, who adopt a VICReg-type loss for healthcare EHR data. Their method processes two augmented views (e.g., partial masking or noise injection across different clinical modalities) and learns embeddings via alignment plus decorrelation. Although originally demonstrated in a multimodal EHR setting, it can be adapted to purely structured or partially unstructured event streams, thereby highlighting its potential generality.

Similar to distillation, feature decorrelation eliminates the need for negative samples, reducing complexity and enabling diverse representations for high-dimensional ES data. However, its success relies on carefully designed augmentations and balancing alignment with redundancy reduction. %While challenges remain for sparse or irregular streams, these methods hold promise for improving representation expressiveness and stability in ES modeling.

\subsubsection{Multimodal Contrastive}
\label{subsubsec:multimodal-contrastive}

Multimodal contrastive learning aims to align representations across multiple modalities associated with the same data instance, ensuring that semantically related information from different sources is embedded closer together in the learned space. This approach is commonly exemplified by models like CLIP~\shortcite{radford2021learning}, which aligns image-text pairs by training encoders for each modality with a shared contrastive loss. Typically, a positive pair consists of aligned representations (e.g., image and caption), while unrelated data serves as negative pairs. This setup can be adapted for ES data when events are enriched with multimodal features.

Event streams in domains like healthcare often contain inherently multimodal data, such as time-series vital signs, diagnostic codes, and accompanying textual notes or summaries. These rich contexts make multimodal contrastive learning a natural fit, though its application remains limited due to the preprocessing required to align modalities effectively.
For instance, \shortciteA{king2023multimodal} explored a multimodal contrastive objective inspired by CLIP for Intensive Care Unit (ICU) data, where time-series signals from medical monitors were contrasted with clinical notes. This event-level alignment enabled the model to integrate structured numeric data with unstructured text, providing a more holistic representation of patient states. Extending this idea, \shortciteA{ma2024global} applied multimodal contrastive learning to align complete ICU stays. Their approach leveraged discharge summaries as high-level textual representations of the patientâ€™s trajectory, contrasting these with aggregated time-series features from the entire hospital stay. While this extension captured broader temporal contexts, it also required comprehensive summaries as a secondary modality, which may not always be available.

Multimodal contrastive methods offer the significant advantage of integrating diverse data types, enabling models to capture richer and more holistic event representations. However, their success relies on the availability and alignment of multiple modalities, which can introduce challenges in preprocessing and data curation. Additionally, the reliance on contrastive objectives across modalities may lead to representational biases if one modality dominates. Nevertheless, for ES domains with naturally multimodal data, such as healthcare, these methods show considerable promise in bridging gaps between structured and unstructured event attributes.

\subsection{Summary}
\label{subsec:ssl-summary}

The reviewed literature underscores that \textit{predictive methods} dominate SSL for ES due to their natural alignment with the sequential and contextual structure of event streams. Masked modeling, inspired by advancements in NLP, excels at reconstructing missing event attributes and is widely applied across domains such as healthcare, e-commerce, gaming, and finance. Autoregressive approaches similarly align pre-training goals with downstream tasks, making them particularly effective in sequential recommendation systems and time-sensitive applications. TPPs, though highly specialized, are uniquely suited for irregular event timing, offering mathematically rigorous frameworks for predicting event occurrences, albeit with limited flexibility for richer metadata or task-agnostic learning.

In contrast, \textit{contrastive methods} remain relatively underexplored, despite their potential to produce robust, entity-level representations. Instance-based contrastive learning has been applied to healthcare, finance, and recommender systems, leveraging augmentations such as subsequence sampling and event masking. Distillation-based contrastive methods avoid the complexity of negative sampling but require well-designed teacher-student alignments and effective augmentations. Feature decorrelation methods, while promising for high-dimensional ES data, have seen limited applications and demand further exploration. Emerging multimodal contrastive techniques, particularly in healthcare, show great promise in integrating diverse modalities such as time-series vitals and textual notes, though their success hinges on data availability and preprocessing.

As seen in Figure~\ref{fig:ssles-tree-diagram}, predictive approaches are dominant across domains, while contrastive and hybrid paradigms remain nascent but hold significant potential. Advancing SSL for ES modeling requires addressing key gaps, including the need for unified frameworks, scalable methodologies, and better benchmarking resources. We believe future work should prioritize exploring underutilized paradigms, such as multimodal and decorrelation-based methods, and integrating insights from both predictive and contrastive approaches to create robust, domain-agnostic SSL solutions for event streams.

\section{Datasets and Downstream Tasks}
\label{sec:es-data-overview}
The success of SSL for ES modeling is dependent on access to robust datasets and well-defined downstream tasks. This section provides an overview of widely used public datasets across domains and highlights the diverse downstream tasks SSL methods target.

\begin{table}[h!]
\centering
\footnotesize
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{>{\centering\arraybackslash}p{0.15\linewidth}|
                >{\centering\arraybackslash}p{0.15\linewidth}|
                >{\centering\arraybackslash}p{0.25\linewidth}|
                >{\arraybackslash}p{0.3\linewidth}}
\textbf{Name} & \textbf{Domain} & \textbf{Used in} & \textbf{Description} \\ \hline 
\textcolor{cbGreen}{CPRD} \shortcite{herrett2015data-cprd} & Healthcare & \shortciteA{li2020behrt,li2022hi-behrt} & Medical events, tests, and diagnoses from patients' primary care visits. \\ \hline
\textcolor{cbGreen}{MIMIC} \shortcite{saeed2002mimic,johnson2016mimic,johnson2023mimic-iv} & Healthcare & \shortciteA{jeong2023event,raghu2023sequential,king2023multimodal,labach2023duett,enguehard2020neural,mei2017neural,zhang2020self-attentive,yang2022transformer,du2016recurrent} & Medical tests and vitals from patients, primarily in ICU settings. \\ \hline
\textcolor{cbBlue}{Amazon} \shortcite{mcauley2015image} & E-commerce & \shortciteA{xie2022contrastive,sun2019bert4rec,zhou2020s3,ma2020disentangled,qiu2022contrastive,li2020time,wang2023sequential} & Product reviews from a wide range of categories. \\ \hline
\textcolor{cbBlue}{Yelp}\tablefootnote{https://www.yelp.com/dataset} & E-commerce & \shortciteA{xie2022contrastive,zhou2020s3,qiu2022contrastive,wang2023sequential} & Business reviews submitted by users. \\ \hline
\textcolor{cbBlue}{MovieLens} \shortcite{harper2015movielens} & E-commerce & \shortciteA{sun2019bert4rec,ma2020disentangled,tang2018personalized,qiu2022contrastive,li2020time} & Users rating and tagging movies. \\ \hline
StackOverflow \shortcite{snapnets} & - & \shortciteA{mei2017neural,zhang2020self-attentive,yang2022transformer,zuo2020transformer-hawkes,wang2023hierarchical,xue2022hypro,du2016recurrent} & User reward history treated as a sequence, with events signifying individual rewards. \\ \hline 
Retweet \shortcite{zhao2015seismic} & - & \shortciteA{zuo2020transformer-hawkes,zhang2020self-attentive,wang2023hierarchical,guo2018initiator} & Retweet cascades from original tweets on Twitter. \\ 
\end{tabular}
\caption{Overview of commonly used benchmark datasets in ES modeling. For StackOverflow and Retweet, no specific domain is assigned, as they are treated primarily as generic event sequences.}
\label{tab:benchmark-datasets}
\end{table}

\subsection{Overview of Public Datsets}
ES datasets are categorized by their source applications, such as healthcare, e-commerce, and generic event streams. Table~\ref{tab:benchmark-datasets} lists commonly used datasets, their domains, and descriptions.

In healthcare, prominent datasets include CPRD \shortcite{herrett2015data-cprd} and MIMIC \shortcite{saeed2002mimic,johnson2016mimic,johnson2023mimic-iv}. CPRD provides primary care records from UK general practice visits, while the series of MIMIC datasets offer detailed ICU patient data, including medical tests and vital signs. These datasets have supported methods such as BEHRT~\shortcite{li2020behrt}, Hi-BEHRT~\shortcite{li2022hi-behrt}, and multimodal contrastive learning~\shortcite{king2023multimodal}. However, preprocessing these datasets often requires converting event streams into time series or tabular formats, which can result in information loss, especially when binning irregular event times~\shortcite{wang2020mimic-extract,mcdermott2021comprehensive}.

In e-commerce, datasets like Amazon~\shortcite{mcauley2015image}, Yelp, and MovieLens~\shortcite{harper2015movielens} are frequently used in SRS. These datasets typically focus on user reviews or ratings rather than clicks or purchases, leading to a divergence between the training data and real-world interaction tasks. Methods such as BERT4Rec~\shortcite{sun2019bert4rec}, CL4SRec~\shortcite{xie2022contrastive}, and ContraRec~\shortcite{wang2023sequential} have adapted these datasets to SSL paradigms.

For generic event streams, StackOverflow~\shortcite{snapnets} and Retweet~\shortcite{zhao2015seismic} are widely used. StackOverflow captures user reward histories, while Retweet records cascades of social media interactions. These datasets serve as benchmarks for TPPs~\shortcite{mei2017neural,zhang2020self-attentive,xue2022hypro}, focusing on the irregular timing of events.

Despite the availability of these datasets, challenges remain. Many domains, such as gaming and finance, rely heavily on proprietary datasets, which are inaccessible to the broader research community~\shortcite{min2016player,min2017multimodal,wang2024player2vec}. Moreover, the lack of standardized preprocessing pipelines and benchmarks, as seen in domains like healthcare~\shortcite{gupta2022extensive}, hampers reproducibility and model comparison.

\subsection{Downstream Tasks}
SSL methods for ES aim to pre-train models that can be fine-tuned or directly applied to real-world tasks. These tasks vary by domain and are evaluated using diverse metrics and baselines.

In healthcare, tasks often involve predicting patient outcomes, such as risk stratification, disease diagnosis, or mortality prediction. For example, BEHRT~\shortcite{li2020behrt} and Hi-BEHRT~\shortcite{li2022hi-behrt} have been evaluated on tasks like predicting hospitalizations or the onset of chronic conditions. Metrics include precision, recall, and area under the receiver operating characteristic (AUROC) curve. Pre-training typically involves masked modeling or contrastive SSL to generate robust patient embeddings that can generalize across various prediction tasks~\shortcite{rasmy2021med,raghu2023sequential}.

In e-commerce, downstream tasks align closely with SRS objectives, such as next-item prediction or user behavior forecasting. Pre-trained models like BERT4Rec~\shortcite{sun2019bert4rec} and ContraRec~\shortcite{wang2023sequential} fine-tune on datasets like Amazon or Yelp to optimize metrics such as hit rate (HR), normalized discounted cumulative gain (NDCG), and mean reciprocal rank (MRR). These tasks often rely on autoregressive or instance-contrastive objectives.

Gaming tasks focus on player behavior modeling, such as goal prediction or style classification. While gaming datasets are typically proprietary~\shortcite{wang2024player2vec}, models like Player2Vec~\shortcite{wang2024player2vec} demonstrate how SSL pre-training can infer meaningful embeddings for tasks like cheat detection or personalized matchmaking.

In finance, tasks such as fraud detection or transaction de-anonymization leverage pre-trained blockchain transaction representations. BERT4Eth~\shortcite{hu2023bert4eth} fine-tunes masked modeling outputs on phishing detection datasets, evaluated using precision-recall trade-offs.

Across these domains, SSL pre-training strives to align pretext tasks (e.g., masked modeling, contrastive alignment) with real-world objectives, enabling robust transfer learning. However, task-specific evaluation often lacks standardization, making cross-model comparisons difficult.


\section{Progress and Prospects}
\label{sec:progress-prospects}

The rapid progress and evolvement of SSL for ES modeling have highlighted both promising directions and key gaps. In the following subsections, we (1) explore the potential for domain-agnostic SSL frameworks for ES modeling (Section~\ref{subsec:domain-agnostic-learning}), leveraging structure similarities across diverse application domains, (2) discuss underexplored modeling paradigms (Section~\ref{subsec:underexplored-modern-paradigms}), (3) outline the critical aspect of timestamp information (Section~\ref{subsec:event-time-modeling}), and (4) highlight the need for open-sourced benchmarks to enable progress in the field (Section~\ref{subsec:open-datasets}).

\subsection{Domain-agnostic Learning}
\label{subsec:domain-agnostic-learning}
The review of SSL methods for ES modeling highlights their broad applicability across diverse domains. Despite this diversity, many approaches converge in terms of architecture and learning objectives, which can be attributed to structural similarities inherent in the data across these fields. Domains such as recommender systems, game event modeling, and EHRs involve sequential event data generated by discrete entities, including users, players, or patients. Individual interactions, such as user-item engagements, in-game actions, or medical assessments, are treated as discrete events, emphasizing the potential for developing generalized SSL methods for ES data.

The consistent structure of ES data and the alignment of learning objectives across domains present an opportunity to design generalized SSL frameworks for ES modeling. Such domain-agnostic approaches could pave the way for foundational models in ES, where further research can prioritize methods that are adaptable and generalizable across a range of applications. This direction holds promise for creating unified modeling strategies that not only bridge gaps between specific domains but also leverage shared patterns within ES data for more robust, scalable ES modeling methodologies, based on SSL.

\subsection{Underexplored Modern Paradigms}
\label{subsec:underexplored-modern-paradigms}
Research in SSL has made significant progress across various domains, yet the majority of methods in ES remain focused on predictive learning objectives, often inspired by NLP. This narrow focus leaves other promising directions, such as contrastive learning, relatively unexplored, despite its demonstrated success in other areas~\shortcite{chen2020simple,radford2021learning,niizumi2021byol,oquab2023dinov2}.

Contrastive learning operates on high-level concepts and avoids reliance on reconstructing low-level details, such as pixels or tokens, during training. This reduces sensitivity to noise in low-level features~\shortcite{liu2021self}, a desirable trait for producing high-quality representations~\shortcite{bengio2013representation}. In ES data, these high-level objects are often entities, making entity-level modeling particularly valuable, as downstream tasks typically require robust representations of entities like patients in healthcare or players in gaming, rather than individual events. Individual events, with their diverse metadata and potential noise (e.g., misclicks on items), can introduce a noisy signal during pre-training, further highlighting the advantages of focusing on higher-level representations. However, contrastive SSL paradigms remain underutilized in ES modeling. This gap may stem from factors such as the complexity of implementing contrastive methods~\shortcite{balestriero2023cookbook}, challenges in defining semantic-preserving augmentations for ES, or the similarity of ES data to language data, where predictive objectives are more commonly used~\shortcite{liu2021self}. A shift toward contrastive learning could enable the development of models that capture stable, generalizable representations of data-generating entities, aligning more effectively with downstream task requirements.

Novel approaches like the Joint Embedding Predictive Architecture (JEPA)~\shortcite{lecun2022path} also present promising opportunities as it has been demonstrated in other modalities~\shortcite{assran2023self,fei2023a-jepa,skenderi2023graph}. By focusing on latent-space representations and predicting abstract features rather than low-level details, JEPA-based methods reduce sensitivity to noise in individual events while maintaining the integrity of learned representations~\shortcite{bardes2024revisiting,assran2023self,littwin2024jepa}. These latent-space approaches offer a pathway to more robust and generalizable models suited to a variety of tasks.

Future research should explore these alternative paradigms to better capture entity-level representations in ES data. Expanding beyond predictive objectives has the potential to create models that handle complex, high-dimensional event streams more effectively, ultimately enhancing performance across diverse downstream tasks.

\subsection{Event Time Modeling}
\label{subsec:event-time-modeling}
The treatment of event timestamps in event stream modeling varies significantly, with some methods disregarding timestamps entirely, as seen in sequential recommendation systems~\shortcite{zhou2020s3,tang2018personalized}, while others explicitly model temporal dynamics through TPPs~\shortcite{mei2017neural,omi2019fully}. Another approach incorporates timestamps as contextual features, particularly in EHR modeling~\shortcite{li2020behrt,rasmy2021med}. Despite these strategies, the role of timestamps in self-supervised ES modeling remains underexplored, with limited systematic evaluation of their influence on model performance.

In predictive SSL, the decision to mask timestamps alongside masked events introduces open questions. Masking temporal information could either reveal unintended dependencies or enhance generalizability, but the impact on tasks requiring temporal consistency remains unclear. These considerations have yet to be thoroughly investigated across methods.

Future work should systematically examine timestamp modeling to establish best practices and evaluate the contribution of temporal information to model robustness. Initial studies could focus on the effects of timestamp masking, providing insights into how temporal data should be integrated into SSL-based ES frameworks.

\subsection{Open Datasets and Benchmarks}
\label{subsec:open-datasets}
The advancement of SSL for ES is hindered by the limited availability of open-source datasets and the absence of standardized benchmarks spanning multiple domains~\shortcite{shchur2021neural}. Most widely used datasets are concentrated in healthcare and recommendation systems, leaving other domains underrepresented. As a result, researchers often repurpose unrelated datasets like StackOverflow~\shortcite{snapnets} and Retweet~\shortcite{zhao2015seismic} for ES tasks, which leads to suboptimal benchmarking and limits the generalizability of findings.

This issue is further compounded by the narrow range of downstream tasks associated with existing benchmarks. Evaluations often focus on a limited set of objectives, restricting the scope of ES modeling assessments and hindering the development of versatile SSL methods. Additionally, reliance on proprietary or private datasets~\shortcite{wornow2023ehrshot,zhang2018patient2vec,meng2021bidirectional,kodialam2021deep,wang2024player2vec,zhao2021player} impedes reproducibility and limits broader validation and extension of research contributions. The scarcity of datasets is likely due to the nature of the data, as it is often generated by human actions, such as healthcare patients, e-commerce customers, or game players, is often considered business-sensitive or private. Stringent regulations, including data protection laws and confidentiality agreements, further restrict the sharing of such data, creating additional challenges for developing open and diverse benchmarks.

Addressing these gaps requires creating open-source benchmarks with diverse downstream tasks to enable robust cross-domain comparisons. Expanding evaluation scopes and fostering data-sharing initiatives will enhance transparency, reproducibility, and the development of generalizable SSL approaches for ES modeling. Recent advances in synthetic data generation techniques could enable the development of realistic datasets that retain the statistical properties of the original data while safeguarding confidentiality.


\section{Conclusion and Discussion}
\label{sec:conclusion}

SSL has emerged as a promising approach to address the inherent challenges of modeling ES data across diverse domains, including healthcare, e-commerce, finance, and gaming. Despite its potential, the field remains fragmented, with predictive SSL paradigms dominating research efforts and other methodologies, such as contrastive learning, underexplored. The lack of unified frameworks, standardized data formats, open benchmarks, and comprehensive datasets further constrains progress.

While the scarcity of contrastive modeling methods in the literature may be potentially explained by a bias against publishing negative results, both in general machine learning research and domain-specific fields, we posit that recent advancements in contrastive learning across various modalities demonstrate its capability to capture meaningful representations and highlight its promise for ES modeling. Similarly, the development of comprehensive datasets and benchmarks would not only facilitate the evaluation of these methods but also provide the foundation for more systematic progress in SSL for ES.

To advance the field, future research should focus on developing domain-agnostic ES models, systematically integrating event timing, and exploring innovative paradigms such as contrastive, joint-embedding predictive, and hybrid architectures. These steps are vital for achieving scalable and generalized SSL solutions capable of unlocking the full potential of ES data for downstream tasks.

Broadly, the maturation of SSL for ES data has the potential to transform industries by enabling robust, scalable models that can operate with minimal labeled data, reducing dependency on costly and impractical labeling efforts. In healthcare, such advances could lead to more accurate patient risk predictions and personalized treatments; in e-commerce, they could refine recommender systems to enhance user experiences. However, these innovations must be developed responsibly, with attention to data privacy, fairness, and transparency to ensure equitable and ethical deployment across applications.

% for arXiv version
\subsection*{Acknowledgment}
%\acks{
The authors would like to express their gratitude to Alexandra Stark, Tim Elgar, Sahar Asadi, and Bj\"orn Brinne for their invaluable support at various stages of this work. We also extend our sincere thanks to Gabriela Zarzar Gandler and Filip Cornell for their helpful comments and insightful discussions, which greatly contributed to refining and strengthening this research. This work was partially funded by Wallenberg AI, Autonomous Systems and Software Program (WASP).
%}

\bibliographystyle{theapa}
\bibliography{references}

\newpage

% Include Appendix here
\input{Appendix}


\end{document}
