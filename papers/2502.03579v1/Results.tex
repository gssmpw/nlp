\section{Results}
\textit{\textbf{Overall performance of the chatbots.}} Our analysis revealed that of all the chatbots, GPT-4o and Menopause Coach performed consistently well and scored the highest, with mean scores ranging from 3.8 to 4.9  across clinicians' evaluated metrics (see Table \ref{tab:mean}). NNK noted credibility and strong clinical alignment in their responses, with \textit{``complete and thorough answers with simple language''} and \textit{``excellent resources''}.
In particular, \textit{Menopause Coach} showed a strong performance in safety and consensus, although it did have some variability in explainability. %While it was often noted for its \textit{``excellent sources''} and \textit{``good overview''}, 
NNK pointed out that, at times, it lacked depth in explanations, especially on more complex issues like hormonal changes. 


\begin{table}[htp]
\fontsize{7}{7}\selectfont
\centering
\begin{tabular}{p{1.9cm} p{0.5cm} p{0.5cm} p{0.5cm} p{0.5cm} p{0.5cm} p{0.5cm}p{0.5cm} p{0.5cm} p{0.5cm} p{0.5cm} p{0.7cm}}
\toprule
\multirow{2}{*}{\textbf{Chatbot}} & \multicolumn{2}{c}{\textbf{Safety}} & \multicolumn{2}{c}{\textbf{Consensus}} & \multicolumn{2}{c}{\textbf{Explainability}} & \multicolumn{2}{c}{\textbf{Objectivity}} & \multicolumn{2}{c}{\textbf{Reproducibility}} & \textbf{SBERT}\\
\cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9} \cmidrule(lr){10-11}
 & \textbf{NNK} & \textbf{AD} & \textbf{NNK} & \textbf{AD} & \textbf{NNK} & \textbf{AD} & \textbf{R1} & \textbf{R2} & \textbf{R1} & \textbf{R2} \\
\midrule
Gemini           & 4.0 & 4.3 & 4.0 & 3.9 & 3.8 & 3.8 & 4.3 & 4.8 & 3.4 & 3.8 & 0.81 \\
ChatGPT-4        & 4.9 & 4.4 & 4.9 & 4.6 & 4.9 & 4.4 & 3.8 & 4.9& 3.4& 4.0 & 0.89 \\
Menopause Coach  & 4.9 & 4.4 & 4.6 & 3.8 & 4.9 & 4.2 & 4.0& 4.9& 3.1 & 4.0& 0.90 \\
Meta AI          & 4.3 & 3.5 & 4.1 & 3.4 & 3.9 & 2.4 & 4.1 & 4.8 &  3.3 & 4.3& 0.88 \\
Copilot          & 4.5 & 4.0 & 4.5 & 3.8 & 4.5 & 3.9 & 4.2 & 4.9 & 3.5 & 4.4 & 0.91 \\
\bottomrule
\end{tabular}
\caption{\textbf{Mean scores.} Average scores for each reviewer by metric, across all eight questions for each chatbot.}
\label{tab:mean}
\end{table}


\textit{Gemini} displayed variability in its ratings, especially in explainability, with a mean score of 3.8 (see Table \ref{tab:mean}). NNK commented about the occasional lack of detail and overly simplistic responses, mentioning that it \textit{``didn't discuss risks like the other responses did and offered a good overview but, not a fan of }``completely normal for periods to fluctuate''\textit{''.} These comments indicated that \textit{Gemini} did not always offer depth and clarity, particularly when addressing more nuanced psychosocial or physical aspects of menopause. Similarly, AD noted that \textit{``I would leave out the loss in bone density as it does not occur immediately''} highlighting a gap in clinical detail.
\textit{Meta AI} also received a lower mean score of
2.4 for explainability, with feedback highlighting its disorganized information structure. 
%NNK observed that responses \textit{``lacked organization and detail''}. 
AD stated---\textit{``Don't like this at all for organization, readability,''}.
% highlighting shared concerns about disorganized responses significantly affecting its effectiveness. 
Despite AD sharing these concerns, in some instances, they assigned conflicting explainability scores. For example, in question 6 (see Table \ref{tab:que}), NNK agreed with the response, giving it a score of 4, while AD assigning a score of 1.
%The clinician noted that its responses \textit{``lacked organization and detail''} significantly affecting its effectiveness.
%\textit{Copilot} got a mean score ranging from 3.8 to 4.5 across all metrics, showing consistent performance. 
NNK for \textit{Copilot}, noted that, although its responses were usually accurate and directed users to healthcare providers, its source credibility was sometimes in question. Regarding explainability, she stated that---\textit{``Not great sources, some are vague''}. This suggests that enhancing reliability of its references could improve clinical accuracy and trust among users.
% However, when it came to addressing broader symptoms, such as changes in mood or physical changes, the scores were low, especially for the explainability metric, emphasizing the need for chatbots to focus more on providing empathetic and detailed explanations for the complex realities of menopause.

\textit{\textbf{Differences in performance across types of questions and metrics.}}
We also saw differences in the performance across specific questions.
The chatbots performed better on treatment-related queries, such as question 5 (see Table \ref{tab:que}). These questions received higher ratings across all metrics, as chatbots generally provided more accurate and detailed responses. 
%As another example, for the question 4 (see Table \ref{tab:que}), all chatbots received perfect scores of 5 from NNK.
However, for question 3 (see Table \ref{tab:que}), only the GPT-based chatbots \textit{ChatGPT-4o} and \textit{Menopause Coach} achieved high scores and could guide users through treatment options with a clear, evidence-based approach, while others did not perform as well. 
\textit{Gemini} and \textit{Meta AI} particularly struggled to provide more organized and empathetic responses, limiting their effectiveness. Scores were lower when addressing symptoms like mood or physical changes, especially in explainability, emphasizing the need for more empathetic and detailed responses to the complex realities of menopause.
%In general, when addressing broader symptoms such as changes in mood or physical conditions, scores were lower, especially for the explainability metric. This emphasizes the need for more empathetic and detailed explanations to respond to the complex realities of menopause.


We also note high semantic similarity (SBERT) mean scores across chatbots, indicating that they are good at \textit{reproducibility}. \textit{Menopause Coach} and \textit{Copilot} performed best here. %, and Gemini got the lowest scores.
Our qualitative analysis uncovered some gaps in responses to questions related to treatment options, symptom management, lifestyle modifications, and risks associated with treatments. Though reproduced responses covered similar ground and were accurate overall, the amount of detail and specifics they included could change each time.

% despite covering similar ground, there was inconsistency in responses while providing detailed advice, especially for the questions related to treatment options, symptom management, lifestyle modifications, and risks associated with treatments. 

% \textbf{Add a statement here based on the mean scores.}
% Despite the high semantic similarity scores, the reviewer ratings for variables such as details of information, sources of information, and follow-up questions were not as high.
% This discrepancy indicates that even if the chatbot responses are consistent in terms of meaning, responses might still vary in other variables provided that human reviewers are looking for.
% This states that reproducibility assessment is more than just semantic similarity and does not necessarily equate to high information quality, accuracy, or engagement. 
% A semantically similar response might still fail to deliver useful information in the context of medical advice.


% The scores suggest that while the majority of chatbots were objective, there are still instances where bias has influenced the responses.
To assess \textit{objectivity}, we tested chatbot responses when the patient's race (white/black) and insurance status (no insurance/public insurance/private insurance) is mentioned. 
Our analysis indicates that all chatbots gave mostly objective responses.
%Gemini and Copilot received the highest average objectivity score, while ChatGPT-4o and Meta AI had the lowest average scores.
However, we identified some cases of insurance and race-related bias (with insurance bias more frequent than race), with chatbots not consistently mentioning the same service options. Insured users also received well-referenced information, while others got generic advice, raising concerns about equal access to evidence-based guidance.
%The information sources cited also varied between responses, with more insured users receiving responses with well-referenced information, while others being given generic advice. This variation raises concerns about whether all users would receive equally informed and evidence-based guidance.

\textbf{\textit{Inter-Rater Reliability Using Cohen's Kappa Scores.}}
While clinicians' ratings show some alignment, the agreement for Safety was no better than random chance resulting in a Kappa score of 0.
Consensus had a Kappa score of 0.50, indicating that the differences in interpretations remained clinicians interpreted responses differently. Explainability also showed the lowest agreement, with a Kappa score of 0.21, showing significant variability in how the clinicians evaluated the responses. Objectivity and Reproducibility both received a Kappa score of 0, indicating no agreement.