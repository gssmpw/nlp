
\pdfoutput=1


\documentclass[11pt]{article}


\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{lipsum}
\usepackage{wrapfig}
% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}
\usepackage{tcolorbox}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}
\usepackage{amsmath}
%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}
\usepackage{multirow}
% custom packages
\usepackage{hyperref}
\usepackage{url}
\usepackage{natbib}
\PassOptionsToPackage{numbers, sort, compress}{natbib}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{array}
\usepackage{microtype}
%\bibliographystyle{plainnat}
\usepackage{times}
\usepackage{latexsym}
\usepackage{adjustbox}
\usepackage{inconsolata}
\usepackage{tabularx}
\usepackage{graphicx} 
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{subcaption}
\usepackage{xspace}
\usepackage{enumitem}




\input{Sections/99_defs}

\title{Trust Me, I'm Wrong: High-Certainty Hallucinations in LLMs}


\author{
Adi Simhi\textsuperscript{1} \hspace{1em} Itay Itzhak\textsuperscript{1} \hspace{1em} Fazl Barez\textsuperscript{2} \hspace{1em} Gabriel Stanovsky\textsuperscript{3} \hspace{1em} Yonatan Belinkov\textsuperscript{1}\\
\thanks{\texttt{\{adi.simhi,itay.itzhak\}@campus.technion.ac.il}} 
\textsuperscript{1}Technion -- Israel Institute of Technology\\
\  \textsuperscript{2}University of Oxford \\
\textsuperscript{3}School of Computer Science and Engineering, The Hebrew University of Jerusalem\\
}





\begin{document}
\maketitle

\input{Sections/01_abstract}
\input{Sections/02_introduction}
\input{Sections/03_setup}

\input{Sections/05_results}
\input{Sections/06_mitigation}

\input{Sections/09_conclusion}


\bibliography{anthology,custom}


\appendix

\section{Dataset Creation}
\label{sec:appendix-Dataset creation}

To create the dataset, we first split the examples into knowledge-based examples, following a method similar to \citet{simhi2024distinguishing}. Specifically, we performed one greedy generation and five generations with a temperature of $0.5$. We used a 3-shot in-context learning scenario, generating a maximum of 5 tokens, and considered a generation correct only if it exactly matched the factually correct answer.

We also adopted the basic dataset curation process described in \citet{simhi2024distinguishing}, but with two key modifications: we started with 70K examples instead of 30K, and we generated 10 tokens instead of 5. Each example in the dataset begins with \texttt{question:} and ends with \texttt{answer:}.

For instruct models, we adjusted the format to align better with their structure. Specifically, we presented the few-shot examples as a user-assistant conversation, where the user asks the questions and the assistant provides the answers. Additionally, we replaced \texttt{answer:} with \texttt{The answer is}, as part of the assistant generation, since this change was observed to improve the performance of instruction models.

To split the knowledge examples into factually correct examples and hallucination-despite knowledge examples, we sampled 20K knowledge-based examples (or fewer if fewer were available). Using the child and Alice-Bob settings, we checked whether the generated text changed and whether the exact match for the correct answer appeared within the 10-token model generations.
\subsection{Additional Refinement}
We observed certain issues, especially with the instruct models, where an exact match was insufficient. For example, the model sometimes failed to generate an answer or produced a correct answer with minor variations, such as synonyms. To address these issues, we curated the \textsc{WACK} examples further, applying a set of simple heuristics that proved effective during manual examination:

\begin{enumerate}
    \item \textbf{Removing negations:} We excluded examples where the generation stated with ``The answer is not.''
    \item \textbf{Synonyms:} Using the NLTK library \citep{loper2002nltk}, we removed examples where a synonym of the correct answer appeared in the generated text.
    \item \textbf{Stem-based similarity:} We excluded examples if the stemmed version of the generation and the factually correct answer shared more than half of their words.
    \item \textbf{Edit distance:} We kept examples where the edit distance between the generated text and the correct answer (in their stemmed versions) was greater than 2, or the answer is a number and \texttt{great}, \texttt{none}, and \texttt{n/a}, which were removed if present in the generated answer.
    \item \textbf{Initial word match:} We removed examples where the generated answer was the first word of the factually correct answer.
    \item \textbf{Special formatting:} For \textsc{Gemma-instruct} model, which we saw that typically generates the final answer enclosed in \texttt{**}, we removed examples where this formatting was absent.
\end{enumerate}
Thus, we removed between 10\% and 45\% of all the hallucination examples. Note that this is a very harsh criterion for removing hallucinations; however, since our aim was to demonstrate that certain hallucinations exist, we preferred to remove any possibility of wrongly classified hallucinations.


\subsection{Dataset Statistics}
In this section, we present the final dataset statistics. The results are shown in Tables \ref{tab:statistic_child} and \ref{tab:statistic_alice}. We can see that the number of \textsc{WACK} hallucinations varies, with Gemma showing low numbers, while most other models exhibit more than 1000 \textsc{WACK} hallucinations.



\begin{table*}[h!]
        \centering
        \begin{tabular}{lcccc}
        \hline
        Model & \# $HK^+$ & \# Factually-correct \\
        \toprule
 Llama & 643/1096 & 17180/10324 \\\midrule
Mistral &691/1732 & 18916/14425  \\\midrule
Gemma &375/797 & 17617/12916\\\midrule
Llama-Instruct & 1075/ 1730 & 14986/9929 \\\midrule
Mistral-Instruct &1338/2767 & 16512/13566 \\\midrule
Gemma-Instruct &1003/1716 & 17887/16421 \\
\bottomrule
\end{tabular}
        \caption{Dataset statistics for TrivaQA/Natural Questions under Child setting.}
        \label{tab:statistic_child}
        \end{table*}


\begin{table*}[h!]
        \centering
        \begin{tabular}{lcccc}
        \hline
        Model & \# $HK^+$ & \# Factually-correct \\
        \toprule
 Llama & 687/1285 & 17026/10088 \\\midrule
Mistral &809/1607 & 18710/14450 \\\midrule
Gemma &446/896 & 17584/12636\\\midrule
Llama-Instruct &1410/2371 & 14996/9771  \\\midrule
Mistral-Instruct &1425/3091 & 16611/14248 \\\midrule
Gemma-Instruct &  1310/2642 & 17678/16194  \\
\bottomrule
\end{tabular}
        \caption{Dataset statistics for TrivaQA/Natural Questions under Alice-Bob setting.}
        \label{tab:statistic_alice}
        \end{table*}


\subsection{Additional Implementation Details}\label{appendix:Implementation Details}
We use the datasets under the Apache License and the models under each one's agreement terms to follow each artifact's terms.
All experiments were run on NVIDIA RTX 6000 Ada (49GB) with 4 CPUs. 
Generating all the datasets and results takes approximately one month on one GPU.

Lastly, We used AI assistants only for simple paraphrasing as part of writing this paper.


\section{Qualitative Evaluation}\label{appendix:Qualitative Evaluation}
In this section, we show qualitative examples of certain hallucinations.
In Table \ref{Generated answers using bad/good shots in the prompt} provides an example from each model using the child setting on the Natural Questions dataset. These are examples of \chk hallucinations, where the model outputs have high probability and low semantic entropy, indicating a high degree of certainty.

\begin{table*}[t]
\small

\centering

  \begin{tabular}  
  {l p{0.2\linewidth}c ccp{0.1\linewidth}}
\toprule 
& & \multicolumn{2}{c}{Response}&\multicolumn{2}{c}{Uncertainty metric} \\ 
\cmidrule(lr){3-4}\cmidrule(lr){5-6}

Model  &Prompt &Original Response & Hallucinated Response & Probability & Semantic Entropy\\\midrule
Gemma&question:   what is the measure of the number of different species present in an area?& biodiversity& species richness&0.31&0.0\\\midrule
Llama&question: who wrote the song it's the climb?&alexander&Miley Cyrus&0.42&$1.11e^-16$\\\midrule
Mistral&question: if there is a random change in the genetics of a small population it is termed?&genetic drift&mutation&0.49& 0.14\\\midrule
Gemma-Instruct&question: who played the mom on lost in space?&June Lockhart&Molly Parker&0.89&0.23\\\midrule
Llama-Instruct&question: what gas is given off by concentrated hydrochloric acid?&hydrogen chloride&hydrogen gas ($H_2$)&0.98&$2.22e^-16$\\\midrule
Mistral-Instruct&question: who published harry potter and the prisoner of azkaban?&Scholastic Inc&J.K. Rowling&0.99&$2.22e^-16$
  \\\bottomrule

  \end{tabular}
 
  \caption{\chk generated answers and uncertainty measures were obtained using greedy decoding on the Natural Questions dataset in the child setting. In each of these examples, the model generates a hallucination despite having the necessary knowledge. The probability of the generated answer is high, and the semantic entropy is low, indicating that these examples exhibit high certainty.}
   \label{Generated answers using bad/good shots in the prompt}
\end{table*}


\section{Certainty Methods Additional Specifics}\label{appendix:Certainty Methods Additional Specifics}
In this section, we elaborate on specifics in the calculations of the different methods.
\subsection{Probability and Probability Difference}
To ensure that the probability we consider corresponds to the probability of the actual answer and not a preceding token, we employed the following heuristic: skipping over any of the following tokens:  
\texttt{"<|assistant|>", "<|user|>", "<|begin\_of\_text|>", "<|end\_of\_text|>", "<|eot\_id|>", "<|start|>",  
"<|end|>", "<|sep|>", "<|sep\_id|>", "assistant", "user", "\textbackslash n", "answer", "The", "Answer", "\"", "'", " answer", "is", "it", "it's", ":", " ", " is", " correct", "correct", "*", "**", " **"}.

This heuristic proved sufficient during a manual investigation.


\subsection{Sampling Based Methods}
For the Semantic Entropy, Sampling, and Predictive Entropy methods, it is necessary to consider the temperature and define a stopping condition for each generation.


We stopped the generation if one of the following sequences was produced: '\textbackslash n\textbackslash n\textbackslash n\textbackslash n', '\textbackslash n\textbackslash n\textbackslash', "\textbackslash n\textbackslash n, 'Question:', 'Context:', ".\textbackslash n", ". ",  'question:', "Alice", "Bob", "(", "Explanation", "\textbackslash n question:", "What", "\textbackslash n answer". These sequences often indicate the generation of new text that is not relevant to answering the question.

We used a temperature of $1$ for 10 generations and an additional generation with a low temperature of $0.1$, following the approach in the code of \citet{kuhn2023semantic} in repository \url{https://github.com/jlko/semantic_uncertainty}, and using DeBERTa \citep{he2020deberta} as the entailment model for the clustering stage based on meaning similarity. Additional results with a temperature of $0.5$ instead of $1$ are presented in Appendix~\ref{appendix:Semantic Entropy results Different Temperature}. Note that we used 10 tokens to generate as the maximum to be consistent with the knowledge and dataset creation steps.



\subsection{Mitigation Metrics} \label{appendix:Mitigation Metrics}
In Section \ref{sec:mitigation_methods} we evaluate the mitigation abilities of probability, sampling and predictive entropy. In this section we detail each metric.


\paragraph{Probability.}
Probability is a simplified version of Negative Log-Likelihood that considers only the log probability of the first token. We observed that subsequent tokens often exhibit high certainty regardless of correctness, making the first token a more reliable indicator of uncertainty and improving performance.

\paragraph{Sampling.} Sampling-based methods assess the diversity of the model's generated outputs, under the assumption that greater diversity reflects lower certainty. Following \citet{cole-etal-2023-selectively}, we define diversity as the proportion of unique outputs in $S$ generated samples. The uncertainty score is calculated as $1-|U|/|S|$, where $U$ is the set of unique generations, and $|U|/|S|$ represents the ratio of unique outputs to the total number of generations. 

\paragraph{Predictive Entropy.} Predictive Entropy estimates uncertainty by evaluating the average unpredictability of the modelâ€™s outputs.
We approximate predictive entropy following \citet{kuhn2023semantic} and \citet{tomani2024uncertainty} by estimating the uncertainty of the model based on its generations for a given prompt $x$. Using $L$ generated samples, the predictive entropy is calculated as: 
\begin{equation}
    PE \approx -1/L\sum_{i=1}^{L}logp(l_i|x)
\end{equation}

 Here, $p(l_i|x)$ represents the likelihood of the $i$-th generation given the prompt $x$. Predictive entropy captures the average uncertainty across the generated outputs.

We investigate whether these uncertainty measures can reliably detect and mitigate \chk hallucinations.


\section{Certain HK+ Exist -- Additional Results}\label{appendix-Certain HK+ Exist Additional Results}

In this section, we present results similar to those in Section \ref{sec:main_results}, focusing on the Gemma and Llama models. See Figures \ref{fig:gemma_certainty} and \ref{fig:Llama_certainty}. These results correlate with those in the main paper on the Mistral model and demonstrate that \chk hallucinations exist. Furthermore, they show that these hallucinations occur across different methods and that instruct-tuned models exhibit poorer calibration between certainty and hallucinations.




\begin{figure*}
\centering

 \centering

 \centering
\begin{subfigure}[b]{0.24\textwidth}
  \centering
  \includegraphics[width=\linewidth]{Figures/pdfs/meta-llama_Llama-3.1-8B_triviaqa_child_prob.pdf}
 \end{subfigure}%
 \hfill
 \begin{subfigure}[b]{0.24\textwidth}
  \centering
\includegraphics[width=\linewidth]{Figures/pdfs/meta-llama_Llama-3.1-8B_naturalqa_alice_prob.pdf}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.24\textwidth}
  \centering
  \includegraphics[width=\linewidth]{Figures/pdfs/meta-llama_Llama-3.1-8B-Instruct_triviaqa_child_prob.pdf}
 \end{subfigure}
 \hfill
 \begin{subfigure}[b]{0.24\textwidth}
  \centering
  \includegraphics[width=\linewidth]{Figures/pdfs/meta-llama_Llama-3.1-8B-Instruct_naturalqa_alice_prob.pdf}
 \end{subfigure}\\

 \centering
\begin{subfigure}[b]{0.24\textwidth}
  \centering
  \includegraphics[width=\linewidth]{Figures/pdfs/meta-llama_Llama-3.1-8B_triviaqa_child_prob_diff.pdf}
 \end{subfigure}%
 \hfill
 \centering
\begin{subfigure}[b]{0.24\textwidth}
  \centering
  \includegraphics[width=\linewidth]{Figures/pdfs/meta-llama_Llama-3.1-8B_naturalqa_alice_prob_diff.pdf}
 \end{subfigure}%
 \hfill
\begin{subfigure}[b]{0.24\textwidth}
  \centering
  \includegraphics[width=\linewidth]{Figures/pdfs/meta-llama_Llama-3.1-8B-Instruct_triviaqa_child_prob_diff.pdf}
 \end{subfigure}
 \hfill
 \begin{subfigure}[b]{0.24\textwidth}
  \centering
  \includegraphics[width=\linewidth]{Figures/pdfs/meta-llama_Llama-3.1-8B-Instruct_naturalqa_alice_prob_diff.pdf}
 \end{subfigure}\\

  \centering

 \centering
\begin{subfigure}[b]{0.24\textwidth}
  \centering
  \includegraphics[width=\linewidth]{Figures/pdfs/meta-llama_Llama-3.1-8B_triviaqa_child_semantic_entropy.pdf}
 \caption{Llama, Child}
 \end{subfigure}%
 \hfill
  \centering
\begin{subfigure}[b]{0.24\textwidth}
  \centering
  \includegraphics[width=\linewidth]{Figures/pdfs/meta-llama_Llama-3.1-8B_naturalqa_alice_semantic_entropy.pdf}
  \caption{Llama, Alice-Bob}
 \end{subfigure}
 \hfill
\begin{subfigure}[b]{0.24\textwidth}
  \centering
  \includegraphics[width=\linewidth]{Figures/pdfs/meta-llama_Llama-3.1-8B-Instruct_triviaqa_child_semantic_entropy.pdf}
  \caption{Llama-Instruct, Child}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.24\textwidth}
  \centering
  \includegraphics[width=\linewidth]{Figures/pdfs/meta-llama_Llama-3.1-8B-Instruct_naturalqa_alice_semantic_entropy.pdf}
  \caption{Llama-Instruct, Alice-Bob}
 \end{subfigure}\\

 \caption{
Detection of WACK Hallucinations on \emph{Llama}: Cumulative percentage of samples (Y-axis) across certainty levels (X-axis) for hallucinations (red) and correct answers (blue) in cases where the model possesses the correct knowledge. Subfigures are organized by models (columns) and certainty measures (rows), with the black dashed line marking the optimal certainty threshold for separating hallucinations from correct answers. The red line shows that a significant proportion of hallucinations persists even at high certainty levels, illustrating the phenomenon of \chk hallucinations, while the blue line shows that many correct answers are also made with varying levels of certainty. The results for Child are on TriviaQA and For Alice-Bob on Natural Questions.}
 \label{fig:Llama_certainty}
\end{figure*}



 
\begin{figure*}
\centering
 \centering

 \centering
\begin{subfigure}[b]{0.24\textwidth}
  \centering
  \includegraphics[width=\linewidth]{Figures/pdfs/google_gemma-2-9b_triviaqa_child_prob.pdf}
 \end{subfigure}%
 \hfill
 \begin{subfigure}[b]{0.24\textwidth}
  \centering
\includegraphics[width=\linewidth]{Figures/pdfs/google_gemma-2-9b_naturalqa_alice_prob.pdf}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.24\textwidth}
  \centering
  \includegraphics[width=\linewidth]{Figures/pdfs/google_gemma-2-9b-it_triviaqa_child_prob.pdf}
 \end{subfigure}
 \hfill
 \begin{subfigure}[b]{0.24\textwidth}
  \centering
  \includegraphics[width=\linewidth]{Figures/pdfs/google_gemma-2-9b-it_naturalqa_alice_prob.pdf}
 \end{subfigure}\\

 \centering
\begin{subfigure}[b]{0.24\textwidth}
  \centering
  \includegraphics[width=\linewidth]{Figures/pdfs/google_gemma-2-9b_triviaqa_child_prob_diff.pdf}
 \end{subfigure}%
 \hfill
 \centering
\begin{subfigure}[b]{0.24\textwidth}
  \centering
  \includegraphics[width=\linewidth]{Figures/pdfs/google_gemma-2-9b_naturalqa_alice_prob_diff.pdf}
 \end{subfigure}%
 \hfill
\begin{subfigure}[b]{0.24\textwidth}
  \centering
  \includegraphics[width=\linewidth]{Figures/pdfs/google_gemma-2-9b-it_triviaqa_child_prob_diff.pdf}
 \end{subfigure}
 \hfill
 \begin{subfigure}[b]{0.24\textwidth}
  \centering
  \includegraphics[width=\linewidth]{Figures/pdfs/google_gemma-2-9b-it_naturalqa_alice_prob_diff.pdf}
 \end{subfigure}\\

  \centering

 \centering
\begin{subfigure}[b]{0.24\textwidth}
  \centering
  \includegraphics[width=\linewidth]{Figures/pdfs/google_gemma-2-9b_triviaqa_child_semantic_entropy.pdf}
 \caption{Gemma, Child}
 \end{subfigure}%
 \hfill
  \centering
\begin{subfigure}[b]{0.24\textwidth}
  \centering
  \includegraphics[width=\linewidth]{Figures/pdfs/google_gemma-2-9b_naturalqa_alice_semantic_entropy.pdf}
  \caption{Gemma, Alice-Bob}
 \end{subfigure}
 \hfill
\begin{subfigure}[b]{0.24\textwidth}
  \centering
  \includegraphics[width=\linewidth]{Figures/pdfs/google_gemma-2-9b-it_triviaqa_child_semantic_entropy.pdf}
  \caption{Gemma-Instruct, Child}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.24\textwidth}
  \centering
  \includegraphics[width=\linewidth]{Figures/pdfs/google_gemma-2-9b-it_naturalqa_alice_semantic_entropy.pdf}
  \caption{Gemma-Instruct, Alice-Bob}
 \end{subfigure}\\

 \caption{
Detection of WACK Hallucinations on \emph{Gemma}: Cumulative percentage of samples (Y-axis) across certainty levels (X-axis) for hallucinations (red) and correct answers (blue) in cases where the model possesses the correct knowledge. Subfigures are organized by models (columns) and certainty measures (rows), with the black dashed line marking the optimal certainty threshold for separating hallucinations from correct answers. The red line shows that a significant proportion of hallucinations persists even at high certainty levels, illustrating the phenomenon of \chk hallucinations, while the blue line shows that many correct answers are also made with varying levels of certainty. The results for Child are on TriviaQA and For Alice-Bob on Natural Questions.}
 \label{fig:gemma_certainty}
\end{figure*}

\section{Semantic Entropy Results -- Different Temperature}\label{appendix:Semantic Entropy results Different Temperature}

In Section~\ref{sec:Certainty Hallucinations can not be Explain as Noise}, we used Semantic Entropy with a temperature of 1 for generating the samples. To demonstrate that the certainty results are not specific to this temperature, we present in Figure~\ref{fig:temp_similarity_certainty} the Semantic Entropy results on the Mistral model. In the upper subfigure, we show the results using a temperature of 1, and in the lower row, we show the results using a temperature of 0.5. We observe that under a temperature of 0.5, there are even more certain hallucinations, further proving that the certainty hallucination phenomenon is not specific to a temperature of 1.


\begin{figure*}
\centering

 \centering
 \centering
\begin{subfigure}[b]{0.24\textwidth}
  \centering
  \includegraphics[width=\linewidth]{Figures/pdfs/mistralai_Mistral-7B-v0.3_triviaqa_child_semantic_entropy.pdf}
 \end{subfigure}%
 \hfill
  \centering
\begin{subfigure}[b]{0.24\textwidth}
  \centering
  \includegraphics[width=\linewidth]{Figures/pdfs/mistralai_Mistral-7B-v0.3_naturalqa_alice_semantic_entropy.pdf}
 \end{subfigure}
 \hfill
% \end{subfigure}
\begin{subfigure}[b]{0.24\textwidth}
  \centering
  \includegraphics[width=\linewidth]{Figures/pdfs/mistralai_Mistral-7B-Instruct-v0.3_triviaqa_child_semantic_entropy.pdf}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.24\textwidth}
  \centering
  \includegraphics[width=\linewidth]{Figures/pdfs/mistralai_Mistral-7B-Instruct-v0.3_naturalqa_alice_semantic_entropy.pdf}
 \end{subfigure}\\
 \centering
\begin{subfigure}[b]{0.24\textwidth}
  \centering
  \includegraphics[width=\linewidth]{Figures/pdfs/mistralai_Mistral-7B-v0.3_triviaqa_child_semantic_entropy_temp_0.5.pdf}
 \caption{Mistral, Child}
 \end{subfigure}%
 \hfill
  \centering
\begin{subfigure}[b]{0.24\textwidth}
  \centering
  \includegraphics[width=\linewidth]{Figures/pdfs/mistralai_Mistral-7B-v0.3_naturalqa_alice_semantic_entropy_temp_0.5.pdf}
  \caption{Mistral, Alice-Bob}
 \end{subfigure}
 \hfill
% \end{subfigure}
\begin{subfigure}[b]{0.24\textwidth}
  \centering
  \includegraphics[width=\linewidth]{Figures/pdfs/mistralai_Mistral-7B-Instruct-v0.3_triviaqa_child_semantic_entropy_temp_0.5.pdf}
  \caption{Mistral-Instruct, Child}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.24\textwidth}
  \centering
  \includegraphics[width=\linewidth]{Figures/pdfs/mistralai_Mistral-7B-Instruct-v0.3_naturalqa_alice_semantic_entropy_temp_0.5.pdf}
  \caption{Mistral-Instruct, Alice-Bob}
 \end{subfigure}\\

 \caption{
Detection of WACK Hallucinations: The upper figures show the Semantic Entropy method with a temperature of 1, while the lower figures use a temperature of 0.5. We show the cumulative percentage of samples (Y-axis) across certainty levels (X-axis) for hallucinations (red) and correct answers (blue) in cases where the model possesses the correct knowledge. Subfigures are organized by models (columns) and certainty measures (rows), with the black dashed line marking the optimal certainty threshold for separating hallucinations from correct answers. The red line shows that a significant proportion of hallucinations persists even at high certainty levels, illustrating the phenomenon of \chk hallucinations, while the blue line shows that many correct answers are also made with varying levels of certainty. 
}
 \label{fig:temp_similarity_certainty}
\end{figure*}



\section{Prompt Specificity}\label{appendix:Prompt Specificity}
In this section, we aim to demonstrate that the results are not restricted to a single variation of the setting. To investigate this, we created a new variation of the child setting. The new prompt is:  
\emph{`I'm working on a major school project, and there's a lot of information I need to understand. Some of it is a bit challenging, and I'm unsure where to begin. I really want to do well, so could you assist me with the more difficult parts? It would mean so much to me!'}

We present the certainty detection results using Mistral on the Natural Questions dataset in Figure~\ref{fig:child2_detection}. In the top figure, we show the results using the original child prompt, while the bottom figure displays the results with the new child prompt. Both settings reveal certain \textsc{WACK} hallucinations and exhibit overall similarity. This indicates that our results are robust to variations in the prompt.

\begin{figure*}
\centering
% \begin{figure*}
 \centering
 \centering
\begin{subfigure}[b]{0.3\textwidth}
  \centering
  \includegraphics[width=\linewidth]{Figures/pdfs/mistralai_Mistral-7B-v0.3_naturalqa_child_prob.pdf}
 \end{subfigure}%
 \hfill
  \centering
\begin{subfigure}[b]{0.3\textwidth}
  \centering
  \includegraphics[width=\linewidth]{Figures/pdfs/mistralai_Mistral-7B-v0.3_naturalqa_child_prob_diff.pdf}
 \end{subfigure}
 \hfill
% \end{subfigure}
\begin{subfigure}[b]{0.3\textwidth}
  \centering
  \includegraphics[width=\linewidth]{Figures/pdfs/mistralai_Mistral-7B-v0.3_naturalqa_child_semantic_entropy.pdf}
  \end{subfigure}
\\
 \centering
\begin{subfigure}[b]{0.3\textwidth}
  \centering
  \includegraphics[width=\linewidth]{Figures/pdfs/mistralai_Mistral-7B-v0.3_naturalqa_child2_prob.pdf}
  \caption{Probability}
 \end{subfigure}%
 \hfill
  \centering
\begin{subfigure}[b]{0.3\textwidth}
  \centering
  \includegraphics[width=\linewidth]{Figures/pdfs/mistralai_Mistral-7B-v0.3_naturalqa_child2_prob_diff.pdf}
  \caption{Probability Difference}
 \end{subfigure}
 \hfill
% \end{subfigure}
\begin{subfigure}[b]{0.3\textwidth}
  \centering
  \includegraphics[width=\linewidth]{Figures/pdfs/mistralai_Mistral-7B-v0.3_naturalqa_child2_semantic_entropy.pdf}
  \caption{Semantic Entropy}
  \end{subfigure}\\

 \caption{
Detection of WACK Hallucinations: The upper figures show the results on Child setting while the lower are the results on Child variation setting. We show the cumulative percentage of samples (Y-axis) across certainty levels (X-axis) for hallucinations (red) and correct answers (blue) in cases where the model possesses the correct knowledge. Subfigures are organized by models (columns) and certainty measures (rows), with the black dashed line marking the optimal certainty threshold for separating hallucinations from correct answers. We can see that the two child variations have similar graphs.
}
 \label{fig:child2_detection}
\end{figure*}

\section{\chk Uniqueness -- Additional Results}\label{sec:appendix-Jaccard Similarity Additional Results}
In this section, we extend the results presented in Section \ref{sec:Certainty Hallucinations can not be Explain as Noise} by demonstrating that, even under shared hallucination examples, permutation tests confirm that certain hallucinations are not random.

Tables \ref{tab:jaccard_prob_shared} and \ref{tab:jaccard_semantic_shared} display these results. Notably, all values are higher than those reported in the main paper, as the examples are now sampled from a subset of shared hallucination examples between the Child and Alice-Bob settings. However, even under the shared examples, the Jaccard similarity for certain cases remains significantly higher than the similarity observed under the permutation test. This further supports the conclusion that certain hallucinations are not mere noise but a distinct property of the models.



We compare the similarity under high certainty to that under the lowest certainty. Specifically, we examine the same subset of examples as the high-certainty subgroup but focus on those with the lowest probabilities. The results are shown in Table \ref{tab:jaccard_prob_low_certainty}.

We observe that the Jaccard similarity for the lowest-certainty subgroup is higher than the values obtained from the permutation test, indicating a general similarity across all certainty levels between the two settings. However, the high-certainty subgroup still exhibits significantly higher similarity scores, suggesting that this subgroup is more aligned than would be expected by chance.


\paragraph{Answer Token Length.}
To investigate what distinguishes \chk samples, we analyzed the length of the first token in generated answers. Specifically, we examined whether the average length of the first token differed between \chk samples and low-certainty hallucinations. Across models, datasets, prompt settings, and metrics, \chk examples consistently exhibited shorter first tokens, with the difference statistically significant according to a t-test. Although the underlying reasons for this pattern remain unclear and warrant future study, this result further highlights the unique characteristics of \chk samples.







\begin{table}[h!]
        \centering
        \begin{tabular}{l|c|cc}
        \hline
        Model&Dataset &Random& Certain\\
        \toprule
  \multirow{2}{*}{Llama}& TriviaQA  & 7.39 &\textbf{47.22}\\ &NQ&9.56 & \textbf{50.72}\\\midrule\multirow{2}{*}{Mistral}& TriviaQA  & 9.99 &\textbf{51.89}\\ &NQ&24.72 & \textbf{72.61}\\\midrule\multirow{2}{*}{Gemma}& TriviaQA  & 7.72 &\textbf{41.67}\\ &NQ&13.54 & \textbf{54.81}\\\midrule\multirow{2}{*}{Llama-Inst}& TriviaQA  & 21.02 &\textbf{36.36}\\ &NQ&22.44 & \textbf{34.42}\\\midrule\multirow{2}{*}{Mistral-Inst}& TriviaQA  & 15.44 &\textbf{57.24}\\ &NQ&22.54 & \textbf{50.06}\\\midrule\multirow{2}{*}{Gemma-Inst}& TriviaQA  & 14.99 &\textbf{53.93}\\ &NQ&16.36 & \textbf{54.47}\\
\bottomrule
\end{tabular}
        \caption{Jaccard Similarity of \chk hallucinations across different prompts under \emph{shared hallucinations}. The \textit{Certain} column shows the overall similarity of \emph{\chk} samples between prompts in the TriviaQA and NaturalQA datasets, using \emph{Probability} as the certainty threshold. Results indicate high similarity, suggesting consistency across settings. All scores are statistically significant (\(p < 0.0001\), permutation test (the Rand column).}
        \label{tab:jaccard_prob_shared}
        \end{table}


\begin{table}[h!]
        \centering
        \begin{tabular}{l|c|cc}
        \hline
        Model &Dataset &Random& Certain \\
        \toprule
\multirow{2}{*}{Llama}& TriviaQA  & 5.56 &\textbf{26.56}\\ &NQ&5.4 & \textbf{16.24}\\\midrule\multirow{2}{*}{Mistral}& TriviaQA  & 7.68 &\textbf{26.26}\\ &NQ&10.96 & \textbf{28.76}\\\midrule\multirow{2}{*}{Gemma}& TriviaQA  & 7.19 &\textbf{25.0}\\ &NQ&7.44 & \textbf{20.72}\\\midrule\multirow{2}{*}{Llama-Inst}& TriviaQA  & 10.7 &\textbf{29.28}\\ &NQ&13.42 & \textbf{31.06}\\\midrule\multirow{2}{*}{Mistral-Inst}& TriviaQA  & 14.21 &\textbf{27.12}\\ &NQ&22.29 & \textbf{41.06}\\\midrule\multirow{2}{*}{Gemma-Inst}& TriviaQA  & 14.51 &\textbf{43.68}\\ &NQ&17.29 & \textbf{42.64}\\
\bottomrule
\end{tabular}
\caption{
Jaccard Similarity of \chk hallucinations across different prompts under \emph{shared hallucinations}. The \textit{Certain} column shows the overall similarity of \emph{\chk} samples between prompts in the TriviaQA and NaturalQA datasets, using \emph{Semantic Entropy} as the certainty threshold. Results indicate high similarity, suggesting consistency across settings. All scores are statistically significant (\(p < 0.0001\), permutation test (the Rand column).}
        \label{tab:jaccard_semantic_shared}
        \end{table}





\begin{table}[t]
        \centering
        \begin{tabular}{l|c|cc}
        \toprule
        Model&Dataset&uncertain& Certain \\
        \toprule
\multirow{2}{*}{Llama} & TriviaQA & 17.91 &\textbf{27.42}\\ &NQ&17.65 &\textbf{21.21}\\\midrule
\multirow{2}{*}{Mistral} & TriviaQA & 22.55 &\textbf{28.21}\\ &NQ&28.93 &\textbf{34.53}\\\midrule\multirow{2}{*}{Gemma} & TriviaQA & 18.89&\textbf{22.99}\\ &NQ&23.43 &\textbf{26.52}\\\midrule\multirow{2}{*}{Llama-Inst}& TriviaQA & 10.42 &\textbf{19.41}\\ &NQ&9.53 &\textbf{18.08}\\\midrule\multirow{2}{*}{Mistral-Inst}& TriviaQA & 14.41 &\textbf{36.48}\\ &NQ&16.02 &\textbf{31.46}\\\midrule\multirow{2}{*}{Gemma- Inst}& TriviaQA & 19.43 &\textbf{35.73}\\ &NQ&18.52 &\textbf{31.72}\\
 

\bottomrule
\end{tabular}
        \caption{Jaccard Similarity of \chk hallucinations across different prompts. The \textit{Certain} column shows the overall similarity of \emph{\chk} samples between prompts in the TriviaQA and NaturalQA datasets and \textit{Low certain} shows the results of the lowest certainty subset, using \emph{Probability} as the certainty threshold. Results indicate high similarity, suggesting consistency across settings.}
        \label{tab:jaccard_prob_low_certainty}
        \end{table}




\section{\chk Persists in Larger Models -- Additional Results}
\label{appendix:chk Persists in Larger Models Additional Results}
In Section \ref{chk Persists in Instruction-Tuning and Larger Models}, we demonstrated that Gemma-2-27B achieves similar or slightly higher \chk detection results on the TriviaQA dataset than Gemma-2-9B, thus showing the existence of this phenomenon on larger models. To further illustrate this phenomenon, Figure \ref{fig:Hallucinations gemma natural} presents comparable results on the Natural Questions dataset.  
These results show a clear correlation with those presented in the main paper.


\begin{figure}
\centering
\begin{subfigure}[b]{0.23\textwidth}
  \centering
  \includegraphics[width=\linewidth]{Figures/pdfs/27vs9_gemma_naturalqa_alice_prob.pdf}
 \end{subfigure}%
 \hfill
  \centering
\begin{subfigure}[b]{0.23\textwidth}
  \centering
  \includegraphics[width=\linewidth]{Figures/pdfs/27vs9_gemma_naturalqa_alice_semantic_entropy.pdf}
 \end{subfigure}\\
 \hfill
\centering
\begin{subfigure}[b]{0.23\textwidth}
  \centering
  \includegraphics[width=\linewidth]{Figures/pdfs/27vs9_gemma_naturalqa_child_prob.pdf}
  \caption{Gemma, Child}
  \end{subfigure}
  \hfill
  \centering
  \begin{subfigure}[b]{0.23\textwidth}
  \centering
  \includegraphics[width=\linewidth]{Figures/pdfs/27vs9_gemma_naturalqa_alice_prob.pdf}
  \caption{Gemma, Alice-Bob}
 \end{subfigure}\\

 \caption{
Detection of \chk Hallucinations: Comparing Gemma-27B to Gemma-9B \chk hallucinations on Natural Questions. The results indicate that \chk hallucinations is similar and slightly higher for the larger model, Gemma-27B.}
 \label{fig:Hallucinations gemma natural}
\end{figure}





\end{document}
