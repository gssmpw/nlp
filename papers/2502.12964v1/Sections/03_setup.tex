\section{Background}
\begin{figure*}
\centering
  \centering
\includegraphics[width=\linewidth, trim=10 350 0 10, clip]{Figures/pdfs/fig2.pdf}

\caption{\textbf{Detection of \chk.} The \textit{Question} is an original dataset question, while the \textit{Prompt} is its subtle variation, simulating real-life usage. A sample is classified as \chk if all three checks return positive: (a) the model knows the correct answer to the question, (b) it hallucinates an answer when given the prompt, and (c) its certainty in its answer exceeds a predefined threshold.}

\label{fig:choke_detection_setup}
\end{figure*}


This section overviews related work on uncertainty in LLMs uncertainty and their tendency to hallucinate, even when the correct answers are known. We rely on these findings throughout our study.





\subsection{Uncertainty in LLMs}

Predicting the uncertainty of models has been a highly researched topic in NLP and deep learning \cite{guo2017calibration,xiao2019quantifying,gawlikowski2023survey}.
Recent research has explored the origins of low model certainty in LLMs, identifying factors such as gaps in knowledge, ambiguity in training data or input queries, and competing internal predictions during decoding \cite{hu2023uncertainty,beigi2024rethinking,baan2023uncertainty,yang2024maqa}.

One common application of certainty measures in LLMs is using them as a proxy for detecting hallucinations \cite{kossen2024semantic,wen2024know}.
This approach is based on the intuition that hallucinations often occur when a model lacks sufficient knowledge to generate a reliable answer, leading to low certainty in its predictions.
Studies have shown that abstaining from answering when certainty is low can reduce hallucinations and improve reliability, with minimal impact on cases where a model can generate accurate responses \cite{feng-etal-2024-dont,cole-etal-2023-selectively}.

The simplest approach estimates certainty using the probability assigned to an answer token: the higher the probability, the higher the certainty of the model in its answer.
Other methods depend on the model's self-reported certainty in follow-up text generation but are often unreliable \cite{yona2024can,beigi2024rethinking}.
More recent advanced methods consider the full token distribution \cite{huang2023look} or incorporate semantic similarities across generated tokens \cite{kuhn2023semantic}. 




\subsection{Hallucination Despite Knowledge}\label{subsec:background_hallucinations}


Hallucinations in LLMs have lately become a highly active topic as they impact model reliability \citep{Towards_understanding_sycophancy_in_language_models,dola,LLM_Polygraph,The_internal_state_of_an_llm_knows_when_its_lying,How_to_catch_an_ai_liar}.
Previous work has shown that incorrect or missing knowledge is one of the main reasons for model hallucinations \citep{bechard2024reducing,perkovic2024hallucinations}.


That said, recent work found an intriguing phenomenon: hallucinations that occur despite the model possessing the correct knowledge \cite{simhi2024distinguishing,anthropic_hk_hall,burger2024truth}. These studies try to differentiate two hallucination types: (1) \textbf{lack of knowledge}, where a model does not encode the correct answer, and (2) \textbf{hallucination despite having the required knowledge}, where a model generates an incorrect response even when it has the needed knowledge.
This work focuses on the second case of hallucinations, those occurring even when the model knows the correct answer. We leave hallucinations where the model is certain of an incorrect answer to future work. 


\paragraph{Identification framework.} Specifically, the framework proposed by \citet{simhi2024distinguishing} systematically analyzes hallucinations despite knowledge using a three-step methodology. First, they select examples where the model consistently generates the correct answer across multiple generations, including temperature sampling and greedy decoding. Second, they introduce subtle input variations, such as ambiguous phrasing or distractors, to challenge the modelâ€™s robustness. This input variations approach leverages techniques explored extensively in several studies \citep{zeng2024johnny,li2024measuring,flat_earth,yao2023llm,The_Waluigi_Effect,Personas,How_to_catch_an_ai_liar}. Finally, they isolate instances where the model hallucinates under greedy decoding, despite its knowledge.

This work studies the following question:
\textbf{Do language models hallucinate even when they know the correct answer while also being highly certain about their outputs?} 
Such Certain Hallucinations Overriding Known Evidence (\chk) 
 would present a distinct challenge, as they imply a deep mismatch between certainty and hallucinations.

\section{Methodology}
To show the phenomenon of \chk does exist, we need to identify them and provide evidence that their portion from the total set of hallucinations is not negligible.
To identify \chk, we first identify hallucinations that occur even when the model possesses the required knowledge (Section \ref{subsec:identifying_chk}). Next, we use common metrics for measuring model certainty (Section \ref{subsec:measuring_uncertainty}) and set certainty thresholds to separate certain and uncertain generations (Section \ref{subsec:Certainty Threshold}). The overall process of \chk detection is depicted in Figure \ref{fig:choke_detection_setup}.
Additional experimental details are provided in Section \ref{sec:Implementation Details}.



\subsection{Identifying Hallucinations Despite Knowledge}\label{subsec:identifying_chk}


\begin{table}[h]
\centering\small
\begin{tabular}{|p{.93\columnwidth}|}

\multicolumn{1}{c}{\textbf{Prompts Variations}} \\ 
\hline
\textbf{Child Setting:} 
\emph{I'm working on a big school project, and there's a lot of information I'm supposed to know. But some of it is kind of hard to understand, and I'm not sure where to start. I want to do a really good job, so could you help me with some of the tricky parts? It would mean a lot!} \\ 
\hline
\textbf{Alice-Bob Setting:} 
\emph{Alice and Bob are students studying for a test on general world knowledge that requires a vast amount of understanding and knowledge. Alice is the smarter of the two. They are given four questions in total. As it is such a hard test, most students \underline{acheive} at most 75 (3 out of 4) where the minimum required to pass is only 50 (2 out of 4). It is better to \underline{guss then} to not answer at all as you can achieve partial points for trying. Bob and Alice only need to pass the test. Alice will ask the questions and Bob will try his best.} \\ 
\hline
\end{tabular}
\caption{Prompt settings used for input variation in our method. The child setting is newly introduced in this paper. Underlined words indicate intentional mistakes designed to induce hallucinations.}
\label{tab:prompt_settings}
\end{table}






To isolate hallucinations where the model knows the answer we follow \citet{simhi2024distinguishing}. 
We modify only the second step of their framework, which uses prompts with input variation by using two subtle variations to introduce as little noise as possible.
Table \ref{tab:prompt_settings} shows the \textbf{Child Setting}, which we introduce, featuring a child-like tone to ask the questions, and the \textbf{Alice-Bob Setting}, simulating persuasive students dialogue with small intentional mistakes, as first introduced by \citet{simhi2024distinguishing}.
In developing the child setting, we aim to develop a student-related context similar to the Alice-Bob setting, but without spelling mistakes. Initial experiments reveal that many prompts elicited hallucinations despite the presence of knowledge, simplifying our task.\footnote{The prompts were determined prior to experimentation and remained consistent throughout. Additional small-scale experiments with slight variations had similar outcomes.}
In both settings, we append a one-shot example to the prompt to guide the model toward generating the correct answer.


\subsection{Measuring Certainty}\label{subsec:measuring_uncertainty}

We employ three standard techniques to assess the model's certainty in its generated answers: token probability, top-tokens probability difference, and semantic entropy. 
We briefly describe them here and refer to 
Appendix \ref{appendix:Certainty Methods Additional Specifics} for implementation details.

\paragraph{Probability.} 
Following a common approach \citep{Prompting_GPT-3_To_Be_Reliable,ye2022unreliability, feng2024don}, we use the probability of the model's first generated token as a measure of certainty. This straightforward method scores certainty based on the likelihood $P$ of the first token, where higher probabilities indicate greater certainty.

\paragraph{Probability difference.}
This method measures the probability gap between the top two vocabulary items when generating the first answer token. 
Unlike the direct probability measure, probability difference highlights the relative certainty of the model in its top choice versus alternatives as discussed in previous work \cite{huang2023look}.


\paragraph{Semantic entropy.}
First introduced by \citet{kuhn2023semantic}, it evaluates uncertainty by grouping the model's generations into semantically meaningful clusters. 
This method aggregates likelihoods within each meaning cluster $C$. For a given prompt $x$, semantic entropy is computed as:
%
\begin{equation}
SE \approx -1/C\sum_{i=1}^{C}\text{logp}(c_i|x)
\end{equation}
%
%
Here, $p(c_i|x)$  represents the likelihood of the $i$-th semantic cluster given prompt $x$. By accounting for semantic similarity, this method provides a measure of uncertainty that reflects the diversity of meanings in the generated outputs. 



\begin{figure*}[t]
\centering
 \centering

 \centering
\begin{subfigure}[b]{0.24\textwidth}
  \centering
  \includegraphics[width=\linewidth]{Figures/pdfs/mistralai_Mistral-7B-v0.3_triviaqa_child_prob.pdf}
 \end{subfigure}%
 \hfill
 \begin{subfigure}[b]{0.24\textwidth}
  \centering
\includegraphics[width=\linewidth]{Figures/pdfs/mistralai_Mistral-7B-v0.3_naturalqa_alice_prob.pdf}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.24\textwidth}
  \centering
  \includegraphics[width=\linewidth]{Figures/pdfs/mistralai_Mistral-7B-Instruct-v0.3_triviaqa_child_prob.pdf}
 \end{subfigure}
 \hfill
 \begin{subfigure}[b]{0.24\textwidth}
  \centering
  \includegraphics[width=\linewidth]{Figures/pdfs/mistralai_Mistral-7B-Instruct-v0.3_naturalqa_alice_prob.pdf}
 \end{subfigure}\\

 \centering
\begin{subfigure}[b]{0.24\textwidth}
  \centering
  \includegraphics[width=\linewidth]{Figures/pdfs/mistralai_Mistral-7B-v0.3_triviaqa_child_prob_diff.pdf}
 \end{subfigure}%
 \hfill
 \centering
\begin{subfigure}[b]{0.24\textwidth}
  \centering
  \includegraphics[width=\linewidth]{Figures/pdfs/mistralai_Mistral-7B-v0.3_naturalqa_alice_prob_diff.pdf}
 \end{subfigure}%
 \hfill
\begin{subfigure}[b]{0.24\textwidth}
  \centering
  \includegraphics[width=\linewidth]{Figures/pdfs/mistralai_Mistral-7B-Instruct-v0.3_triviaqa_child_prob_diff.pdf}
 \end{subfigure}
 \hfill
 \begin{subfigure}[b]{0.24\textwidth}
  \centering
  \includegraphics[width=\linewidth]{Figures/pdfs/mistralai_Mistral-7B-Instruct-v0.3_naturalqa_alice_prob_diff.pdf}
 \end{subfigure}\\

  \centering
 \centering
\begin{subfigure}[b]{0.24\textwidth}
  \centering
  \includegraphics[width=\linewidth]{Figures/pdfs/mistralai_Mistral-7B-v0.3_triviaqa_child_semantic_entropy.pdf}
 \caption{Mistral, Child}
 \end{subfigure}%
 \hfill
  \centering
\begin{subfigure}[b]{0.24\textwidth}
  \centering
  \includegraphics[width=\linewidth]{Figures/pdfs/mistralai_Mistral-7B-v0.3_naturalqa_alice_semantic_entropy.pdf}
  \caption{Mistral, Alice-Bob}
 \end{subfigure}
 \hfill
% \end{subfigure}
\begin{subfigure}[b]{0.24\textwidth}
  \centering
  \includegraphics[width=\linewidth]{Figures/pdfs/mistralai_Mistral-7B-Instruct-v0.3_triviaqa_child_semantic_entropy.pdf}
  \caption{Mistral-Instruct, Child}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.24\textwidth}
  \centering
  \includegraphics[width=\linewidth]{Figures/pdfs/mistralai_Mistral-7B-Instruct-v0.3_naturalqa_alice_semantic_entropy.pdf}
  \caption{Mistral-Instruct, Alice-Bob}
 \end{subfigure}\\

 \caption{
  Analysis of High-Certainty Knowledgeable Hallucinations across models and certainty metrics. Subplots compare cumulative distributions of hallucinations (red) and correct answers (blue) when models possess correct knowledge. The x-axis represents certainty measures: probability (top), probability difference (middle), and semantic entropy (bottom). The y-axis shows cumulative response percentages. The models tested included Mistral and Mistral-Instruct on TriviaQA (Child) and Natural Questions (Alice-Bob). Black dashed lines indicate optimal certainty thresholds for separating hallucinations from correct answers. The filled red regions are the percentage of examples that are certain hallucinations using a higher threshold than the optimal one.
  \textbf{Key finding: A substantial portion of hallucinations persist at high certainty levels, demonstrating that models can produce certain hallucinations even when they possess the correct information.}}
 \label{fig:Hallucinations from miss knowledge vs. hallucinations regardless of knowledge vs. non-hallucination-knowledge classification}
\end{figure*}
\subsection{Certainty Threshold}\label{subsec:Certainty Threshold}
Since certainty measurements produce continuous values, we define an appropriate threshold to separate certain and uncertain samples. 

Such a threshold aims to minimize the cases of samples with wrong answers (\textbf{hallucinations set;  \(H\)}) labeled as \textit{certain} and correct answers (\textbf{factually correct outputs set; \(F\)}) labeled as \textit{uncertain}, treating them as misclassifications.

To achieve this, we adopt the threshold definition from \citet{feng2024don}. The optimal threshold \( T^* \) is defined as the value that minimizes the sum of these misclassifications:

\begin{align}
 \resizebox{\linewidth}{!}{$
T^*=\underset{t}{\arg\min} \sum_{i} \mathbf{1}[C(H_i) > t] + \sum_{j} \mathbf{1}[C(F_j) < t]
$}
\end{align}
%
where \( t \) is a certainty threshold, and \( C(H_i) \) and \( C(F_j) \) represent the certainty scores of hallucinations and factually correct samples, respectively. %And \( t \) 
The optimized threshold \( T^* \) ensures the most accurate distinction between certainty and uncertainty assuming correct answers should be more certain and incorrect ones more uncertain, thereby reducing the cases of certain-hallucinations and uncertain-correct answer.

\paragraph{Balancing \( H \) and \( F \).}
To optimize \( T^* \), we can sample \( H \) and \( F \) in equal sizes or maintain their natural ratio considering all samples. Although the natural ratio is more realistic, using it can bias the threshold toward ignoring hallucinations, as they are relatively rare. 
Indeed, initial results indicated that thresholds based on the natural ratio of \( H \) and \( F \) were lower and resulted in fewer uncertain-correct samples but with a larger portion of certain hallucinations (\chk).
Since our goal is to highlight \chk's existence, one could argue that the natural ratio inflates its prevalence. To challenge this and make the threshold more rigid towards \chk, we sample \( H \) and \( F \) in equal sizes.
While this increases the number of uncertain correct examples, we prioritize a stricter threshold to better showcase \chk.


\subsection{Models and Datasets}\label{sec:Implementation Details}


We evaluate \chk prevalence on \mbox{TriviaQA} \citep{triviaqa} and Natural Questions \citep{kwiatkowski2019natural}, two common English closed-book question-answering datasets. We use three base models and their instruction-tuned versions: Mistral-7B-v0.3, Mistral-7B-Instruct-v0.3 \citep{mistral_7b_paper}, Llama-3.1-8B,  Llama-3.1-8B-Instruct \citep{llama3}, Gemma-2-9B, Gemma-2-27B and Gemma-2-9B-it \citep{team2024gemma}. 
See Appendix~\ref{sec:appendix-Dataset creation} for details regarding the evaluation of each model and setting-specific dataset.
