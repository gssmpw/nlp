

\begin{figure*}

\centering

 \centering
\begin{subfigure}[b]{0.49\textwidth}
  \centering
  \includegraphics[width=\linewidth]{Figures/pdfs/triviaqa_child_bar_chart.pdf}
  \caption{TriviaQA, Child}
 \end{subfigure}%
 \hfill
 \begin{subfigure}[b]{0.49\textwidth}
  \centering
\includegraphics[width=\linewidth]{Figures/pdfs/naturalqa_alice_bar_chart.pdf}
\caption{Natural Questions, Alice-Bob}
\end{subfigure}\\

\caption{Failure of Certainty-Based Mitigation Methods: Percentage of unmitigated hallucinations (Y-axis) across six models (X-axis) for three mitigation methods: Sampling, Predictive Entropy, and Probability. Lower values indicate higher success in mitigation. 
All methods fail to address a significant portion of \chk, likely due to their dependence on uncertainty scores.}
\label{mitigation_fig}
\end{figure*}

\section{Hallucination Mitigation and \chk}\label{sec:mitigation_methods}

Having established the existence of \chk, we hypothesize that they may impact the performance of mitigation methods. 
To test this hypothesis, we experiment with hallucination mitigation while focusing on \chk.

\subsection{Mitigation Based on Certainty Measures}


Recent research has proposed leveraging certainty measures to mitigate hallucinations, primarily by abstaining from generating outputs when the model is uncertain about its predictions \cite{cole-etal-2023-selectively,tjandra2024fine}. 
These approaches often rely on probabilities and entropy-based metrics \cite{tjandra2024fine} or combine them with self-evaluation techniques \cite{tomani2024uncertainty}. Other techniques employ alternative information-theoretic measures \cite{yadkori2024believe, yadkori2024mitigating} or involve cross-verification using multiple language models to assess uncertainty \cite{feng2024don}.


The effectiveness of these measures is often evaluated using the area under the receiver operator characteristic (AUROC), which measures how well uncertainty ranks correct versus incorrect responses \cite{kuhn2023semantic}. However, this focus on ranking accuracy overlooks cases where hallucinations occur with high certainty or when correct answers are generated with low certainty. Therefore, this oversight may limit their ability to mitigate hallucinations comprehensively.




To evaluate whether \chk samples are being overlooked, we test the usefulness of uncertainty-based mitigation methods in our setting. Specifically, we use the approaches of \citet{tomani2024uncertainty} and \citet{cole-etal-2023-selectively}, incorporating three uncertainty measures they used: negative log-likelihood, sampling, and predictive entropy. 
We introduce the probability measure as a simplified alternative to negative log-likelihood. Below, we briefly describe each measure, See Appendix \ref{appendix:Mitigation Metrics} for details.

\textbf{Probability}  is a simplified version of negative log-likelihood that considers only the probability of the first token. \textbf{Sampling} assess the diversity of the model's generated outputs, under the assumption that greater diversity reflects lower certainty. Lastly, \textbf{predictive entropy} estimates uncertainty by evaluating the average unpredictability of the modelâ€™s outputs.
Examples are mitigated only if their uncertainty is below a defined threshold, as described in Section \ref{subsec:Certainty Threshold}.



\subsection{Certainty-based Mitigation Methods Fail on \chk}


The results in Figure \ref{mitigation_fig} show the percentage of \textbf{unmitigated} hallucination examples for each model. \emph{A lower value is better}, as it indicates fewer hallucinations remain unmitigated.


Our results in Figure \ref{mitigation_fig} show that a significant proportion of hallucinations exceed the threshold across all methods and models, remaining unmitigated. Among the methods, sampling performs the worst, while probability and predictive entropy consistently show a lower percentage of unmitigated hallucinations. This suggests sampling is less effective in avoiding mislabeling hallucinations among uncertainty-based methods.
However, even probability and predictive entropy fail to mitigate a notable percentage of hallucinations. Lowering the threshold could improve mitigation, but this would lead to an increased mislabeling of low-certainty correct answers, as shown in Section \ref{sec:main_results}.

These findings show that certainty-based mitigation fails to address high-certainty hallucinations. While uncertainty-based methods are expected to struggle with high-certainty hallucinations, our results here and in Section \ref{sec:main_results} indicate they consistently miss a distinct type, regardless of the uncertainty metric or threshold strictness. This failure suggests a fundamental limitation in assuming an ideal uncertainty metric can fully capture hallucinations.





