


\section{Introduction}
LLMs can often \emph{hallucinate}---generate outputs ungrounded in real-world facts, hindering their reliability \citep{survey_of_hallucination_in_natural_language_generation, Towards_understanding_sycophancy_in_language_models, Calibrated_language_models_must_hallucinate}. 

Numerous studies have focused on identifying hallucinations, with a particular line of research highlighting a strong relationship between hallucinations and a model's uncertainty \citep{tjandra2024fine,SelfCheckGPT}.
These studies demonstrate that uncertainty estimation metrics, such as semantic entropy \citep{kuhn2023semantic}, sampling \citep{cole-etal-2023-selectively}, and probability \citep{feng2024don}, can be employed to detect and mitigate hallucinations based on an apparent correlation between uncertainty and hallucinations.




These findings spotlight the potential of uncertainty in addressing hallucinations but leave the nature of their connection unclear, calling for a deeper investigation of their relations.
In this work, we aim to investigate the relationship between hallucinations and uncertainty by examining hallucinations generated \emph{with certainty}.

Specifically, we focus on a category of hallucinations that occur even when the model possesses the necessary knowledge to generate grounded outputs. By focusing on these hallucinations, we eliminate hallucinations in which the model is certain of an incorrect answer, which it perceives as its true answer, see Figure \ref{fig:chk_certain}.


\begin{figure}
\centering
  \centering

\includegraphics[width=1.0\linewidth, trim=0 500 0 0, clip]{Figures/pdfs/certainty_wak.pdf}
\caption{\textbf{Do high-certainty hallucinations exist?} An illustrative categorization of hallucinations based on a model's knowledge and certainty. Highlighted is the phenomenon of high-certainty hallucinations (purple) -- where models confidently produce incorrect outputs, even when they have the correct knowledge. While other types of hallucinations can potentially be explained by the model not knowing, being mistaken, or uncertain, high-certainty hallucinations are harder to rationalize, making their existence particularly intriguing.}
\label{fig:chk_certain}
\end{figure}


 
 We term these hallucinations \textbf{CHOKE}: \textbf{C}ertain \textbf{H}allucinations \textbf{O}verriding \textbf{K}nown \textbf{E}vidence.

To detect \chk, we use the method suggested by \citet{simhi2024distinguishing} to identify hallucinations where a model knows the correct answer. Next, we estimate the model's certainty with three widely used but conceptually different methods: tokens probabilities \citep{feng2024don}, probability difference between the top two predicted tokens \citep{huang2023look},  and semantic entropy \citep{kuhn2023semantic}.


Our findings reveal that such hallucinations can occur in realistic scenarios, demonstrating that a model can hallucinate despite possessing the correct knowledge, and doing so with \emph{certainty}, as illustrated in Figure \ref{fig:chk_certain}.  
We find this phenomenon is broad, existing in two datasets with both pre-trained and instruction-tuned models. Furthermore, we show that \chk cannot simply be classified as mere noise by establishing they are consistent across contexts. Finally, we find that this issue can significantly impact the effectiveness of hallucination mitigation techniques, resulting in a non-negligible number of unmitigated hallucinations.
 These findings highlight the potential of \chk for exploring the underlying mechanisms of hallucinations, offering valuable insights that can deepen our understanding and improve LLM safety \citep{barez2025open}.


\textbf{Our contributions are three-fold:}

\begin{enumerate} 
    \item We demonstrate that hallucinations can manifest with high certainty, challenging the common belief that links hallucinations primarily to low model certainty. 


    \item We provide evidence that certain hallucinations are systematic rather than random noise, showing consistent reproduction of the same hallucinations across different settings.

    \item We evaluate existing mitigation techniques and demonstrate their limited effectiveness in addressing high-certainty hallucinations, underscoring the importance of better understanding this phenomenon.
\end{enumerate}

