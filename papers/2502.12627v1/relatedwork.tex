\section{Related Work}
\subsection{Vision State Space Models}
Although the Transformer~\cite{transfomer} has achieved remarkable success in natural language processing, its quadratic complexity poses challenges when handling long sequence structures. To address this issue, state-space models~\cite{s4} (SSMs), represented by Mamba~\cite{mamba}, have gradually emerged as an alternative to Transformers. In visual tasks, the quadratic complexity of the standard self-attention mechanism similarly presents challenges for processing high-resolution images. Thus, the Vim~\cite{vim} and VMamba~\cite{vmamba} attempt to incorporate Mamba into computer vision tasks. However, inputting images into SSM models remains a critical challenge. Vim and VMamba address this by employing bidirectional and four-directional scanning strategies to transform image patches into one-dimensional sequences. Building on this, subsequent research introduced continuous scanning~\cite{plainmamba} and local four-directional scan~\cite{localmamba} to better align with the two-dimensional structure of images. Despite the significant achievements of Mamba models in computer vision, existing scanning methods heavily rely on manual design, making it difficult to dynamically and flexibly adapt to input variations. This limitation hinders the model's ability to capture complex two-dimensional structures. Therefore, our goal is to propose a vision Mamba model capable of adaptively and flexibly adjusting scanning paths based on input image, further enhancing its performance in vision tasks.


\subsection{Vision Transformers}
The Transformer~\cite{transfomer} model was first introduced in 2017 for natural language processing (NLP) tasks. With its powerful global modeling capabilities and excellent parallelism, the Transformer quickly gained popularity in the NLP. By the end of 2020, Vision Transformer~\cite{vit} (ViT) successfully extended the Transformer model to large-scale image classification tasks, achieving state-of-the-art performance. Subsequently, DeiT~\cite{deit} improved ViT by introducing knowledge distillation~\cite{knowledgedistilling} and more efficient training strategies, enabling effective training even on relatively small datasets such as ImageNet-1K~\cite{imagenet}. Following this development trajectory, researchers proposed numerous hierarchical Transformer models that reduce computational complexity for high-resolution images through various sparse attention mechanisms. Notable examples include the Swin Transformer~\cite{swin} and PVT~\cite{pvt,pvtv2}. Subsequent research~\cite{pvt,pvtv2,dat,cswin,biformer,dilateformer,qformer,transnext} introduced various sparse attention mechanisms to strike a balance between global modeling capability and computational complexity. However, the global modeling capabilities of these improved sparse attention mechanisms still fall short of the standard self-attention mechanism.


\begin{figure*}[h]
    \centering
    \includegraphics[width=1.\linewidth]{image/architecture.pdf}
    \vspace{-3mm}
    \caption{Illustration of the proposed Dynamic Adaptive Scan (DAS). For clarity, only four reference points are shown. \textbf{Left}: each initial reference point represents the original position of a patch, with its offsets learned by an Offset Prediction Network (OPN). Features of important regions are sampled based on the predicted 2D coordinates using bilinear interpolation. \textbf{Right} the detailed structure of the OPN is revealed. The query feature map is first transformed through depthwise convolution~\cite{depsconv,depsconv2} to integrate local information. Then, another linear layer, after layer normalization~\cite{layernorm} and GELU~\cite{gelu} activation, converts the feature map into offset values.}
    \label{fig:das}
    \vspace{-1em}
\end{figure*}
 
\subsection{Convolutional Neural Networks}
Convolutional Neural Network (CNN)~\cite{cnn} was initially proposed for handwritten digit recognition, but it wasn't until the introduction of AlexNet~\cite{alexnet} in 2012, which triggered the "ImageNet moment," that the full potential of CNNs was realized. This breakthrough led to a rapid development in computer vision, driven by the resurgence of neural networks, with CNNs becoming the standard architecture for computer vision tasks. During this period, many representative CNN models emerged, such as VGG~\cite{vgg}, GoogLeNet~\cite{googlenet}, ResNet~\cite{resnet}, DenseNet~\cite{densenet}, DCN~\cite{dgcnet,internimage}, and EfficientNet~\cite{efficientnet}. These models focused on different aspects, including accuracy, efficiency, and scalability, while promoting valuable design principles. In recent years, inspired by ViTs, some CNNs~\cite{convnet,convnextv2,RepLKNet,SLaK} have incorporated large kernel convolutions to capture long-range dependencies, achieving performance competitive with ViT. At the same time, CNNs have been widely integrated into various ViTs and vision Mambas to enhance local modeling capabilities, creating a complementary synergy between the two approaches. These advancements have driven the diversification and convergence of model design in vision tasks.