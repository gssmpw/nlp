\section{Related Work}
\subsection{Vision State Space Models}
Although the Transformer Vaswani et al., "Attention Is All You Need" has achieved remarkable success in natural language processing, its quadratic complexity poses challenges when handling long sequence structures. To address this issue, state-space models Vrancic and Radivojac et al., (SSMs), represented by Mamba Chang et al., have gradually emerged as an alternative to Transformers. In visual tasks, the quadratic complexity of the standard self-attention mechanism similarly presents challenges for processing high-resolution images. Thus, the Vim Xu et al., and VMamba Xie et al., attempt to incorporate Mamba into computer vision tasks. However, inputting images into SSM models remains a critical challenge. Vim and VMamba address this by employing bidirectional and four-directional scanning strategies to transform image patches into one-dimensional sequences. Building on this, subsequent research introduced continuous scanning Wang et al., and local four-directional scan Zhang et al., to better align with the two-dimensional structure of images. Despite the significant achievements of Mamba models in computer vision, existing scanning methods heavily rely on manual design, making it difficult to dynamically and flexibly adapt to input variations. This limitation hinders the model's ability to capture complex two-dimensional structures. Therefore, our goal is to propose a vision Mamba model capable of adaptively and flexibly adjusting scanning paths based on input image, further enhancing its performance in vision tasks.


\subsection{Vision Transformers}
The Transformer Vaswani et al., model was first introduced in 2017 for natural language processing (NLP) tasks. With its powerful global modeling capabilities and excellent parallelism, the Transformer quickly gained popularity in the NLP. By the end of 2020, Vision Transformer Dosovitskiy et al., successfully extended the Transformer model to large-scale image classification tasks, achieving state-of-the-art performance. Subsequently, DeiT Touvron et al., improved ViT by introducing knowledge distillation Carmona et al., and more efficient training strategies, enabling effective training even on relatively small datasets such as ImageNet-1K Zhu et al.,. Following this development trajectory, researchers Wang et al., proposed numerous hierarchical Transformer models that reduce computational complexity for high-resolution images through various sparse attention mechanisms. Notable examples include the Swin Transformer Liu et al., and PVT Xu et al.,. Subsequent research Zhao et al., introduced various sparse attention mechanisms to strike a balance between global modeling capability and computational complexity. However, the global modeling capabilities of these improved sparse attention mechanisms still fall short of the standard self-attention mechanism.


\begin{figure*}[h]
    \centering
    \includegraphics[width=1.\linewidth]{image/architecture.pdf}
    \vspace{-3mm}
    \caption{Illustration of the proposed Dynamic Adaptive Scan (DAS). For clarity, only four reference points are shown. \textbf{Left}: each initial reference point represents the original position of a patch, with its offsets learned by an Offset Prediction Network (OPN). Features of important regions are sampled based on the predicted 2D coordinates using bilinear interpolation. \textbf{Right} the detailed structure of the OPN is revealed. The query feature map is first transformed through depthwise convolution Zhang et al., to integrate local information. Then, another linear layer, after layer normalization Ba et al., and GELU Hendrycks et al., activation, converts the feature map into offset values.}
    \label{fig:das}
    \vspace{-1em}
\end{figure*}
 
\subsection{Convolutional Neural Networks}
Convolutional Neural Network (CNN) LeCun et al., was initially proposed for handwritten digit recognition, but it wasn't until the introduction of AlexNet Krizhevsky et al., in 2012, which triggered the "ImageNet moment," that the full potential of CNNs was realized. This breakthrough led to a rapid development in computer vision, driven by the resurgence of neural networks, with CNNs becoming the standard architecture for computer vision tasks. During this period, many representative CNN models emerged, such as VGG Simonyan et al., GoogLeNet Szegedy et al., ResNet He et al., DenseNet Huang et al., DCN Zhang et al., and EfficientNet Tan et al.. These models focused on different aspects, including accuracy, efficiency, and scalability, while promoting valuable design principles. In recent years, inspired by ViTs, some CNNs Wang et al., have incorporated large kernel convolutions to capture long-range dependencies, achieving performance competitive with ViT. At the same time, CNNs have been widely integrated into various ViTs and vision Mambas to enhance local modeling capabilities, creating a complementary synergy between the two approaches. These advancements have driven the diversification and convergence of model design in vision tasks.