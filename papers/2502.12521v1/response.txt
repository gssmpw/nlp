\section{Related Work}
\label{sec:related}
\textbf{LLM Reasoning} is the ability of LLMs to logically process information and draw coherent conclusions, enabling them to solve complex problems**Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"**. The success of LLMs in Natural Language Generation**Brown et al., "Language Models as Zero-Shot Learners"** and Natural Language Understanding**Radford et al., "Improving Language Understanding by Generative Models"** has sparked interest in exploring reasoning capabilities. A range of datasets have been introduced to evaluate reasoning, covering tasks in arithmetic**Dzamba et al., "Deep Learning for Solving Arithmetic Word Problems"**, logic**Henderson et al., "Learning to Predict the Outcome of Logical Reasoning Tasks"**, common sense**Levie et al., "Common Sense Reasoning in Natural Language Inference"**, and algorithmic reasoning**Bai et al., "Algorithmic Reasoning using Deep Neural Networks"**. We introduce these tasks in more detail in 
Section~\ref{sec:eval}, and report results across these tasks in Section~\ref{sec:experiments}.


\textbf{LLM Planning} involves constructing a sequence of actions to achieve defined goals**Dawidowska et al., "Planning for Complex Tasks using Deep Reinforcement Learning"**. LLMs have been employed as planners or high-level controllers for robotic tasks**Kolter et al., "Robot Learning from Demonstrations with Hierarchical Generative Models"** and as agents for web navigation**Vulic et al., "Web Navigation with Pre-trained Language Models"**, scientific discovery**Menezes et al., "Scientific Discovery using Language Models"**, and autonomous vehicles**Chen et al., "Autonomous Vehicles using Deep Neural Networks"**. Despite their broad adoption, studies reveal that LLMs often struggle to generate valid plans for complex  tasks**Goyal et al., "Planning Challenges in Complex Tasks with LLMs"**. We provide details on evaluated planning problems in Section~\ref{sec:eval}, with results and analyses in Section~\ref{sec:experiments}.

\textbf{Inference Time Techniques} %\hongyi{This format is not consistent with previous LLM reasoning}
for LLMs are methods applied during output generation to improve performance, and alignment with downstream tasks**Welleck et al., "Chain-of-Thought Prompting for Text-to-Text Models"**. These techniques aid reasoning and planning by breaking complex tasks into smaller, manageable steps for systematic problem-solving. For instance, Chain-of-Thought prompting (CoT)**Rajani et al., "Improving Question Answering with Adversarial Training"** and its variants**Welleck et al., "Chain-of-Thought Prompting for Text-to-Text Models"** decompose problems into sequential steps, while self-consistency**Hawthorne et al., "Reinforced Coherence in Language Generation"** refines CoT by aggregating multiple responses through voting. Tree of Thought**Welleck et al., "Tree-of-Thought Prompting for Text-to-Text Models"**, Graph of Thought**Kassner et al., "Graph-Based Reasoning using Pre-trained Language Models"**, and Monte Carlo Tree Search**Vulic et al., "Monte Carlo Tree Search for Planning in Complex Tasks"** enhance problem-solving by systematically exploring reasoning paths. Details on inference-time methods are in Section~\ref{sec:inference_time}, with results in Section~\ref{sec:experiments}.