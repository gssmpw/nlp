\documentclass[journal,twoside,web]{ieeecolor}
\usepackage{generic}
\usepackage{pifont}
\usepackage{cite}
\usepackage{multirow}
\usepackage{array}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{booktabs} 
\usepackage{graphicx}
\usepackage{algorithm,algorithmic}
\usepackage{hyperref}
\usepackage[table]{xcolor}
\usepackage{booktabs}
\hypersetup{hidelinks=true}
\usepackage{textcomp}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\markboth{\hskip25pc IEEE Journal of Biomedical and Health Informatics}
{Xu Wang \MakeLowercase{\textit{et al.}}: ECG-Expert-QA: A Benchmark for Evaluating Medical Large Language Models in Heart Disease Diagnosis}
\begin{document}
\title{ECG-Expert-QA: A Benchmark for Evaluating Medical Large Language Models in Heart Disease Diagnosis}

\author{Xu Wang, Jiaju Kang, Puyu Han, Yubao Zhao, Qian Liu, \\Liwenfei He, Lingqiong Zhang, Lingyun Dai, Yongcheng Wang, Jie Tao
\thanks{This work was supported by the “Pioneer” R\&D Program of Zhejiang Province under Grant No. 2024C03005. The first institutional affiliation for this study is Zhejiang University School of Medicine, The First Affiliated Hospital \& Liangzhu Laboratory. \textit{Corresponding author: Jie Tao; Yongcheng Wang}.}%
\thanks{Xu Wang is with the Department of Laboratory Medicine of The First Affiliated Hospital \& Liangzhu Laboratory, Zhejiang University School of Medicine, Hangzhou 310000, China; 
the School of Computer Science and Technology, Shandong Jianzhu University, Jinan 250000, China 
and the FUXI AI Lab, Shenzhen 518000, China (e-mail: 202311102025@stu.sdjzu.edu.cn).}%
\thanks{Jiaju Kang is with Beijing Normal University, Zhuhai Campus, Zhuhai 519087, China and also with the FUXI AI Lab, Shenzhen 518000, China (e-mail: kangjiaju@fuxi-lab.com).}%
\thanks{Puyu Han is with Southern University of Science and Technology, Shenzhen 518055, China and also with the FUXI AI Lab, Shenzhen 518000, China (e-mail: 12432627@mail.sustech.edu.cn ).}%
\thanks{Yubao Zhao is with the China University of Geosciences (Wuhan), Wuhan 430000, China (e-mail: 20211000862@cug.edu.cn).}%
\thanks{Qian Liu, Liwenfei He, Lingqiong Zhang, and Lingyun Dai are with Hangzhou Baorun Biotechnology Co., Ltd., Hangzhou, China (e-mail: margaretlq@baorunbiotech.com; heliwenfei@baorunbiotech.com; zhanglingqiong@baorunbiotech.com; kellydai@baorunbiotech.com).}%
\thanks{Yongcheng Wang is also with the Department of Laboratory Medicine of The First Affiliated Hospital \& Liangzhu Laboratory, Zhejiang University School of Medicine, Hangzhou 310000, China (e-mail:  yongcheng@zju.edu.cn).}%
\thanks{Jie Tao is with the Department of Laboratory Medicine of The First Affiliated Hospital \& Liangzhu Laboratory, Zhejiang University School of Medicine, Hangzhou 310000, China (e-mail: taojie1991@zju.edu.cn).}%
}
 
\maketitle

\begin{abstract}
We present ECG-Expert-QA, a comprehensive multimodal dataset for evaluating diagnostic capabilities in electrocardiogram (ECG) interpretation. It combines real-world clinical ECG data with systematically generated synthetic cases, covering 12 essential diagnostic tasks and totaling 47,211 expert-validated QA pairs. These encompass diverse clinical scenarios, from basic rhythm recognition to complex diagnoses involving rare conditions and temporal changes.
A key innovation is the support for multi-turn dialogues, enabling the development of conversational medical AI systems that emulate clinician-patient or interprofessional interactions. This allows for more realistic assessment of AI models' clinical reasoning, diagnostic accuracy, and knowledge integration.
Constructed through a knowledge-guided framework with strict quality control, ECG-Expert-QA ensures linguistic and clinical consistency, making it a high-quality resource for advancing AI-assisted ECG interpretation. It challenges models with tasks like identifying subtle ischemic changes and interpreting complex arrhythmias in context-rich scenarios.
To promote research transparency and collaboration, the dataset, accompanying code, and prompts are publicly released at \underline{https://github.com/Zaozzz/ECG-Expert-QA}.
% Our dataset is open-source and available at \underline{https://github.com/Zaozzz/ECG-Expert-QA}.
\end{abstract}

\begin{IEEEkeywords}
ECG Interpretation, Medical Large Language Models, Multi-turn QA, Cross-modal Clinical Reasoning, Ethical and Risk-aware Evaluation
% Enter key words or phrases in alphabetical order, separated by commas. Using the IEEE Thesaurus can help you find the best standardized keywords to fit your article. Use the thesaurus access request form for free access to the IEEE Thesaurus: \underline{https://www.ieee.org/publications/services/thesaurus-acce}\\
% \underline{ss-page.com.}
\end{IEEEkeywords}

\section{Introduction}
\label{sec1}

\IEEEPARstart{E}{CG} interpretation plays a vital role in cardiovascular diagnosis and clinical decision-making. However, accurate interpretation often relies on expert-level reasoning and domain-specific knowledge, which poses challenges in scenarios with limited medical resources. As large language models (LLMs) become increasingly powerful, there is growing interest in their potential to augment or automate medical tasks. Yet, the application of these models to ECG analysis remains underdeveloped, hindered by limitations in evaluation frameworks, data diversity, and ethical oversight.

\begin{figure}[h]
    \centering
    \includegraphics[width=1\linewidth]{Fig1.pdf}
    \label{fig1}
    \caption{Task Distribution and Sample Volume in ECG-Expert-QA Benchmark}
\end{figure}

\begin{table*}[h]
\centering
\caption{Comparison of ECG-related Benchmarks in Terms of Dialogue, Reasoning, and Patient-centric Dimensions}
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{lccccc}
\toprule
\textbf{Dataset / System} & \textbf{Dialogue Support} & \textbf{Clinical Reasoning} & \textbf{Rare Disease} & \textbf{Multi-turn} & \textbf{Prognosis / Informed} \\
\midrule
ECG-Chat & Single-turn & Partial & Limited & \ding{56} & No \\
ECG-QA & Single-turn & Template-based & No & \ding{56} & No \\
GEM & -- & Contextual modeling & Unknown & \ding{56} & Temporal ECG modeling \\
MedGemini & -- & Advanced reasoning & Yes & \ding{56} & Partial prognosis \\
MedQA-CS & Multi-turn & Structured QA & Yes & \ding{52} & No \\
AgentClinic & Multi-turn & Interactive logic & Simulated & \ding{52} & Simulated patient awareness \\
PTB-XL & -- & Classification only & Limited & \ding{56} & No \\
\textbf{OURS} & Multi-turn & Full reasoning & Yes & \ding{52} & Prognosis \& doctor-patient dialogue \\
\bottomrule
\end{tabular}
\label{tab1}
\end{table*}

Despite advances in explainable AI, most current approaches fail to support high-stakes diagnostic reasoning. Holzinger et al. have emphasized that for AI to be trusted in clinical practice, transparency and interpretability must be designed from the outset \cite{holzinger2017needbuildexplainableai}. At the same time, large-scale datasets such as MIMIC-IV have laid the foundation for training medical AI models with real-world clinical data \cite{johnson2023mimiciv}. However, these datasets lack structured evaluation protocols for measuring reasoning, ethical decision-making, or interactional capabilities within ECG contexts.

Recent work has shown that LLMs can help bridge care gaps in underserved regions. Strika et al. illustrated how AI and LLMs might mitigate disparities in “medical deserts,” while also cautioning against linguistic bias and ethical blind spots in current systems \cite{strika2024bridging}. Domain-specific models such as BioGPT have shown promise in biomedical text generation \cite{Luo_2022}, yet remain underexplored in diagnostic scenarios involving multimodal and temporal reasoning. Ethical concerns also persist, as noted by Morley et al., who stress the importance of building AI systems that can operate responsibly across levels of abstraction—from individual to institutional decision-making \cite{MORLEY2020113172}. Moreover, efforts to align LLMs with clinical data, as seen in Yang et al.'s work on EHR-trained models, suggest that more tailored evaluation datasets are needed to ensure performance, fairness, and safety \cite{yang2022llmEHR}.

To address these gaps, we present ECG-Expert-QA, a novel benchmark designed to evaluate the diagnostic reasoning, ethical judgment, and language understanding capabilities of medical LLMs in the context of ECG interpretation. Unlike prior datasets, ECG-Expert-QA incorporates diverse, high-complexity cases, multi-turn dialogues, and risk-sensitive scenarios to support comprehensive and scalable evaluation. This work aims to enable the development of more trustworthy and context-aware LLMs for real-world medical applications.

\section{Related Work}
\label{sec2}
\begin{figure*}[h]
    \centering
    \includegraphics[width=1\linewidth]{Fig2.pdf}
    \label{fig2}
    \caption{A Role-Based and Process-Oriented Framework for ECG Multimodal QA Generation}
\end{figure*}

With the rapid advancement of LLMs and multimodal learning, automated ECG interpretation has progressed beyond conventional classification tasks to encompass clinical reasoning and dialog-based medical QA. A number of high-quality multimodal datasets have emerged, aiming to bridge the gap between AI and real-world clinical decision-making, especially in the context of ECG-based diagnosis.

One of the earlier efforts, ECG-QA~\cite{bib1}, proposed a question-answering framework for ECG interpretation. However, its structure is limited to single-turn QA with fixed question templates, lacking the capacity to incorporate contextual or longitudinal clinical reasoning. Later, ECG-Chat~\cite{bib2} introduced a large-scale ECG-language model trained on 19k diagnostic ECG samples, demonstrating the promise of integrating signal data with natural language. Nonetheless, it remains constrained in handling conversational dynamics and rare disease scenarios.

To enhance generalizability and clinical applicability, several studies have explored fusing ECG signals with text, images, and structured data within multimodal frameworks. For instance, GEM~\cite{bib3} proposed a multimodal diagnostic model that combines visual and temporal ECG information for more robust and explainable reasoning. Similarly, MedGemini~\cite{bib4} offered a family of multimodal medical foundation models capable of processing ECG, imaging, and structured EHRs for comprehensive clinical understanding.

Recent efforts have also extended ECG modeling from classification to natural language report generation and QA. ECG-ReGen~\cite{bibReGen} presented a retrieval-augmented framework that integrates self-supervised ECG representation learning with large language models to generate clinical reports and perform diagnostic question answering. While promising, it primarily supports single-turn interactions and lacks support for conversational reasoning or patient-specific context.

In parallel, benchmarking efforts based on large-scale ECG datasets have also advanced. One notable example is the PTB-XL~\cite{bibPTBXL} dataset, which provides a publicly available, richly annotated 12-lead ECG corpus for various diagnostic tasks. Accompanied by rigorous benchmarking and analysis across ResNet- and Inception-based deep learning models, PTB-XL has contributed significantly to structured performance evaluation in ECG classification. However, while valuable for model training and general benchmarking, such datasets remain focused on static classification tasks and lack support for conversational modeling or contextual reasoning.

Moreover, the diagnostic value of ECG-based QA systems is inherently tied to signal quality. A comprehensive review~\cite{bibQualityReview} summarized recent advances in automated ECG quality assessment, underscoring the importance of reliable signal input for downstream AI-driven analysis. Incorporating signal quality metrics into future conversational benchmarks remains a valuable research direction.

Concurrently, the need to evaluate medical LLMs in realistic diagnostic settings has led to the development of interactive and multi-agent environments such as AgentClinic~\cite{bib5} and MedQA-CS~\cite{bib6}. While these benchmarks enable evaluation across diverse clinical domains, they are often focused on textual data or radiology tasks, leaving ECG interpretation underrepresented in the multimodal conversational AI landscape.

Additionally, datasets like Human3.6M~\cite{bibH36M1,bibH36M2} (pose-text), AudioSet~\cite{bibAudioSet} (audio-text), and LuoJiaHOG~\cite{bibLuoJiaHOG} (image-text) have demonstrated success in other domains, furthering the potential of multimodal learning. These datasets pair different modalities, including motion, sound, and visual data, with textual descriptions, providing valuable insights into how multimodal models can integrate diverse data sources for improved performance.

ECG-Expert-QA addresses these limitations by introducing the first open-source benchmark to support multi-turn ECG diagnostic QA. By integrating real clinical records with systematically synthesized cases across 12 diagnostic tasks and 47,211 QA pairs, it supports rich clinical contexts, rare conditions, and temporal disease progression patterns. Its design enables comprehensive evaluation of medical LLMs in terms of diagnostic accuracy, clinical reasoning, and knowledge integration, setting a new standard for conversational ECG AI systems.

\section{Methods}
\label{sec3}

This study introduces three primary dataset generation strategies: expert knowledge-guided professional knowledge assessment, cross-modal diagnosis in complex medical environments, and medical risk assessment. Each strategy is designed to address specific challenges in intelligent ECG interpretation by generating high-quality question-answer (QA) pairs. The expert-guided approach leverages domain-specific medical expertise to steer LLMs in generating accurate and clinically relevant content. The cross-modal diagnosis approach converts semi-structured clinical data, such as ECG signals, into detailed textual narratives to support reasoning-based QA generation. The medical risk assessment approach focuses on ethical considerations, counterfactual scenarios, and patient rights, producing QA pairs that simulate realistic, high-stakes clinical situations. Each method employs distinct input modalities and interaction protocols to facilitate medical knowledge learning, pathological reasoning, and multi-turn dialogue, thereby enhancing the robustness and applicability of LLMs in real-world clinical settings.

\begin{figure*}[h]
    \centering
    \includegraphics[width=0.84\linewidth]{Fig3.pdf}
    \label{fig3}
    \caption{ECG-Expert-QA Multi-Task QA Examples Across Cardiac Cases - 1}
\end{figure*}

\begin{figure*}[h]
    \centering
    \includegraphics[width=0.84\linewidth]{Fig4.pdf}
    \label{fig4}
    \caption{ECG-Expert-QA Multi-Task QA Examples Across Cardiac Cases - 2}
\end{figure*}

\begin{figure*}[h]
    \centering
    \includegraphics[width=0.84\linewidth]{Fig5.pdf}
    \label{fig5}
    \caption{ECG-Expert-QA Multi-Task QA Examples Across Cardiac Cases - 3}
\end{figure*}

\begin{figure*}[h]
    \centering
    \includegraphics[width=0.88\linewidth]{Fig6.pdf}
    \label{fig6}
    \caption{ECG-Expert-QA Multi-Task QA Examples Across Cardiac Cases - 4}
\end{figure*}

\begin{figure*}[h]
    \centering
    \includegraphics[width=0.88\linewidth]{Fig7.pdf}
    \label{fig7}
    \caption{ECG-Expert-QA Multi-Task QA Examples Across Cardiac Cases - 5}
\end{figure*}

\begin{figure*}[h]
    \centering
    \includegraphics[width=0.31\linewidth]{Fig8-1.pdf}
    \includegraphics[width=0.27\linewidth]{Fig8-2.pdf}
    \includegraphics[width=0.31\linewidth]{Fig8-3.pdf}
    \label{fig8}
    \caption{ECG-Expert-QA Multi-Task QA Examples Across Cardiac Cases - 6}
\end{figure*}

\subsection*{Expert Knowledge-Guided Professional Knowledge Assessment}

This method constructs a high-quality medical QA corpus through continuous interaction with state-of-the-art LLMs (e.g., GPT-4o\cite{bib13}). Local deployment or API-based access is used to integrate domain expertise via role-based prompting strategies that simulate realistic doctor–patient dialogues. The resulting data spans the following core areas:

\begin{itemize}
\item \textbf{CK (Cardiology Knowledge)}: Covers foundational cardiology concepts, including cardiac anatomy, function, common cardiovascular diseases (e.g., coronary artery disease, heart failure), etiology, symptoms, diagnostics, and treatment pathways. It is generated by designing multi-turn dialogue prompts that guide LLMs to produce diverse responses for knowledge distillation.

\item \textbf{EBK (ECG Basic Knowledge)}: Focuses on electrocardiographic principles, waveform interpretation, detection of common abnormalities such as premature ventricular contractions and atrial fibrillation, and practical ECG usage in clinical contexts. This module is also created through iterative multi-turn dialogue with prompt variations to obtain refined and diverse answers.

\item \textbf{CD (Complex Diagnosis)}: Involves multi-turn expert discussions simulating complex cardiac scenarios, including comorbidities and rare conditions, aiming to generate in-depth clinical reasoning and individualized treatment strategies. It is constructed by filtering heart-related cases from the GenMedicalEval\cite{bib10,bib11,bib12} dataset and prompting the model to perform diagnostic reasoning.
\end{itemize}

\subsection*{Cross-Modal Diagnosis in Complex Medical Environments}

This method transforms semi-structured ECG data—such as 12-lead ECG signals—into descriptive textual narratives that detail waveform features, abnormal signals, clinical symptoms, and diagnostic insights. These descriptions are then paired with role-specific prompts to guide LLMs in producing multi-faceted QA pairs. Applications include:

\begin{itemize}
\item \textbf{CMD (Cross Modal Diagnosis)}: Integrates ECG imagery and textual data to establish associations between waveform abnormalities and clinical diagnoses, covering pathologies such as arrhythmias and myocardial infarction. It is generated using diagnostic information and reports from the MIMIC-IV-ECG\cite{bib0} dataset, reformulated into structured narratives and paired with carefully designed prompts.

\item \textbf{PP (Patient Prognosis)}: Produces prognosis-based QA pairs by analyzing current cardiac status and forecasting disease trajectories or recovery timelines. This is generated by summarizing ECG-related diagnostic content from MIMIC-IV-ECG and prompting the model to predict outcomes.

\item \textbf{LTD (Long Text Diagnosis)}: Uses extended clinical narratives to evaluate the model’s comprehension and diagnostic ability in handling rich contextual information with multiple variables. These are based on long-form reports from MIMIC-IV-ECG, transformed into textual inputs for reasoning-focused QA generation.

\item \textbf{MROD (Multiple Rounds of Dialogue)}: Simulates ongoing multi-turn doctor–patient interactions, assessing the model’s capacity for maintaining coherent reasoning across conversational contexts. It is built from MIMIC-IV-ECG case data and structured as dialogue scripts to guide multi-turn interactions.

\item \textbf{MC (Memory Correction)}: Evaluates the model’s adaptability by testing its ability to revise or refine previous responses when new medical information is introduced mid-dialogue. This is achieved by incrementally revealing information from MIMIC-IV-ECG cases and observing the model’s ability to adjust its responses.

\item \textbf{MEE (Medical Entity Extraction)}: Applies natural language processing techniques to extract structured medical entities (e.g., symptoms, diagnoses, treatments) from unstructured ECG-related text. These QA pairs are generated by inputting ECG-based clinical reports from MIMIC-IV-ECG and prompting the model to extract key medical elements.
\end{itemize}

\subsection*{Medical Risk Assessment}

This module addresses ethical and legal dimensions of medical practice, including patient autonomy, diagnostic uncertainty, and hypothetical scenario analysis. Role-based interaction frameworks are applied to simulate ethically charged conversations using LLMs. QA pairs are generated across the following categories:

\begin{itemize}
\item \textbf{PRTK (Patient's Right to Know)}: Simulates scenarios focused on informed consent, shared decision-making, and transparent communication, highlighting the legal and ethical obligations of clinicians. This is generated through multi-turn dialogue prompting that explores ethically nuanced doctor–patient interactions.

\item \textbf{MCo (Medical Counterfactual)}: Constructs counterfactual diagnostic cases by altering variables such as treatment choices or drug dosages, then analyzing the impact on patient outcomes. It is generated by modifying variables in MIMIC-IV-ECG cases and prompting the model to reason through the consequences.

\item \textbf{GRA (Generate Risk Assessment)}: Generates clinical risk assessments that synthesize multiple data points to evaluate the potential risks and benefits of proposed interventions. This is constructed using rich diagnostic reports from MIMIC-IV-ECG and prompts that elicit comprehensive risk-benefit evaluations.
\end{itemize}

\subsection*{Summary}

Through the application of three systematic dataset generation approaches—expert-guided knowledge assessment (CK, EBK, CD), cross-modal diagnostic reasoning (CMD, PP, LTD, MROD, MC, MEE), and medical risk evaluation (PRTK, MCo, GRA)—this study constructs a comprehensive and high-quality dataset for intelligent ECG interpretation. These datasets support not only factual knowledge acquisition and diagnostic reasoning, but also ethical decision-making and realistic conversational modeling. By enhancing the adaptability and accuracy of large language models in dynamic clinical environments, the proposed framework lays a strong foundation for the development of trustworthy, multimodal, and patient-centered medical AI systems.

\section{Experiments}
\label{sec4}

To evaluate the performance of LLMs on the ECG-Expert-QA dataset, we adopted four widely used evaluation metrics in natural language generation (NLG): \textbf{BLEU-1}\cite{bib7}, \textbf{ROUGE-L}\cite{bib8}, and \textbf{METEOR}\cite{bib9}. These metrics measure the similarity between the model-generated answers and ground-truth references in terms of lexical overlap and semantic relevance.

\subsection*{BLEU (Bilingual Evaluation Understudy)}

BLEU is a precision-based metric that measures the proportion of overlapping n-grams between the generated output and the reference answer. In our evaluation, we focus on **BLEU-1**, which uses unigram (single word) precision to reflect basic word-level alignment and lexical accuracy.

\begin{equation}
\text{BLEU-1} = \text{BP} \cdot \exp\left( \log p_1 \right) = \text{BP} \cdot p_1
\end{equation}

where:
\begin{itemize}
\item $p_1$ is the modified unigram precision,
\item BP is the brevity penalty:
\end{itemize}

\begin{equation}
\text{BP} =
\begin{cases}
1 & \text{if } c > r \\
e^{(1 - \frac{r}{c})} & \text{if } c \le r
\end{cases}
\end{equation}

Here, $c$ is the length of the candidate sentence and $r$ is the length of the reference sentence. The brevity penalty ensures that excessively short outputs are penalized, promoting more complete responses.

\begin{figure*}[h]
    \centering
    \includegraphics[width=1\linewidth]{Fig9.png}
    \label{fig9}
    \caption{Comparative Performance of LLMs on ECG-Expert-QA. Color Mapping: Blue – GPT-4o, Orange – DeepSeek-V3,  Green – Qwen2.5}
\end{figure*}

\subsection*{ROUGE-L (Recall-Oriented Understudy for Gisting Evaluation)}

ROUGE-L is a recall-oriented metric based on the longest common subsequence (LCS) between the generated text and the reference. It considers both precision and recall, and is particularly useful for evaluating coverage of reference content.

\begin{equation}
\text{ROUGE-L} = F_\beta = \frac{(1 + \beta^2) \cdot \text{Precision} \cdot \text{Recall}}{\text{Recall} + \beta^2 \cdot \text{Precision}}
\end{equation}

where:
\begin{itemize}
\item $\text{Precision} = \frac{LCS}{\text{length of candidate}}$
\item $\text{Recall} = \frac{LCS}{\text{length of reference}}$
\item $\beta$ is typically set to 1.
\end{itemize}

\subsection*{METEOR (Metric for Evaluation of Translation with Explicit ORdering)}

METEOR incorporates both precision and recall while also considering synonym matching, stemming, and paraphrasing, making it more semantically aware than BLEU or ROUGE.

\begin{equation}
\text{METEOR} = F_{\text{mean}} \cdot (1 - \text{Penalty})
\end{equation}

where:
\begin{equation}
F_{\text{mean}} = \frac{10 \cdot P \cdot R}{9P + R}
\end{equation}

\begin{equation}
\text{Penalty} = 0.5 \cdot \left( \frac{\# \text{chunks}}{\# \text{matches}} \right)^3
\end{equation}

Here, $P$ and $R$ are unigram precision and recall, respectively, and ``chunks'' refers to the number of contiguous matched word sequences.

\subsection*{Task-Specific Evaluation}

The above metrics were applied to seven sub-datasets extracted from ECG-Expert-QA, each representing a clinically meaningful task: Cardiology Knowledge, Complex Diagnosis, ECG Basic Knowledge, Generate Risk Assessment, Long Text Diagnosis, Patient Prognosis and Patient’s Right to Know.

\subsection*{Model-to-Model Scoring}

In addition to these standard metrics, we also introduced a Model-to-Model Scoring (MMS) mechanism. This method involves using one large model (Model A) to evaluate the performance of another model (Model B) based on its generated answers and the ground-truth reference\cite{bibmms1,bibmms2}. Specifically, Model A compares the response from Model B with the correct answer, and assigns a score based on factors such as semantic consistency, content coverage, and language fluency.

MMS offers an automated way to evaluate model outputs, reducing the need for human intervention and helping to optimize Model B’s performance. By leveraging the strengths of one model to assess the performance of another, we gain a deeper understanding of each model’s strengths and weaknesses across different tasks.

\subsection*{Lightweight Model Fine-tuning}

To further evaluate the applicability of our benchmark to lightweight models, we fine-tune MiniMind2\cite{bibminimind}, a compact LLM with only 25.8 million parameters, on the ECG-Expert-QA dataset. Although MiniMind2 contains significantly fewer parameters than larger-scale models, our goal is to assess whether ECG-Expert-QA can still enable meaningful diagnostic performance under constrained model capacity.

Due to input/output token limitations inherent to MiniMind2, we restrict its evaluation to selected sub-datasets that feature relatively short contexts and responses. This design allows us to explore the feasibility of deploying lightweight medical LLMs in real-world, resource-constrained scenarios, such as mobile or embedded systems.

\subsection*{Summary}

Through a combination of standard NLG metrics and model-to-model evaluation, we comprehensively assessed the diagnostic and linguistic capabilities of multiple LLMs on ECG-Expert-QA. In addition to benchmarking large models, we also evaluated the lightweight MiniMind2 to examine the dataset’s adaptability under constrained model capacity. The results demonstrate that ECG-Expert-QA supports robust performance evaluation across a wide spectrum of model sizes and provides valuable insights into the trade-offs between model complexity and clinical applicability.

\section{Result}
\label{sec5}
In this section, we present the evaluation results of four models—GPT-4o, DeepSeek-V3, Qwen2.5, and MiniMind2—on seven clinically relevant tasks from the ECG-Expert-QA dataset. The models were assessed using three widely adopted natural language generation metrics: \textbf{BLEU-1}, \textbf{ROUGE-L}, and \textbf{METEOR}, as well as the newly introduced \textbf{Model-to-Model Scoring (MMS)}. These metrics allow for a comprehensive evaluation of both lexical overlap and semantic alignment with expert-annotated references.

While GPT-4o, DeepSeek-V3, and Qwen2.5 were evaluated across all tasks, MiniMind2—a lightweight model with only 25.8M parameters—was assessed on a subset of tasks (CK, CD, EBK, PP) due to input/output limitations. The inclusion of MiniMind2 enables exploration of model performance in resource-constrained scenarios.

Through this multi-dimensional evaluation, we aim to assess the models' ability to generate accurate, fluent, and clinically relevant responses. The results highlight each model’s strengths and limitations in terms of diagnostic reasoning, contextual understanding, and communicative precision, offering insight into their applicability in real-world medical AI systems.

% ----- Table for Task CK -----
\begin{table}[h]
\centering
\caption{Evaluation Metrics for Task CK}
\label{tab:metrics_ck}
\begin{tabular}{lcccc}
\toprule
Model & BLEU-1 & ROUGE-L & METEOR & MMS \\
\midrule
GPT-4o        & 0.1648 & \cellcolor[rgb]{0.7,1,0.7}0.2323 & \cellcolor[rgb]{0.7,1,0.7}0.3098 & \cellcolor[rgb]{0.7,1,0.7}0.815 \\
DeepSeek-V3   & 0.0661 & 0.1277 & 0.1917 & 0.794 \\
Qwen2.5       & 0.0514 & 0.0922 & 0.1957 & 0.757 \\
MiniMind2     & \cellcolor[rgb]{0.7,1,0.7}0.1864 & 0.2051 & 0.1631 & --    \\
\bottomrule
\end{tabular}
\end{table}

In the CK task, GPT-4o outperforms all other models across BLEU-1, ROUGE-L, METEOR, and MMS, achieving an MMS of 0.815, reflecting strong alignment with expert references in both lexical and semantic dimensions. DeepSeek-V3 and Qwen2.5 show moderate performance, particularly underperforming in BLEU-1. MiniMind2, however, stands out with the highest BLEU-1 score (0.1864), indicating strong word-level overlap but lower ROUGE-L and METEOR scores, suggesting weaker structural and semantic depth. Its high lexical precision hints at usefulness in lightweight or constrained settings. Overall, GPT-4o remains the most balanced and reliable model for medical text generation, while MiniMind2 excels in concise lexical matching.

% ----- Table for Task CD -----
\begin{table}[h]
\centering
\caption{Evaluation Metrics for Task CD}
\label{tab:metrics_cd}
\begin{tabular}{lcccc}
\toprule
Model & BLEU-1 & ROUGE-L & METEOR & MMS \\
\midrule
GPT-4o        & \cellcolor[rgb]{0.7,1,0.7}0.2810 & \cellcolor[rgb]{0.7,1,0.7}0.3130 & \cellcolor[rgb]{0.7,1,0.7}0.3416 & 0.786 \\
DeepSeek-V3   & 0.1374 & 0.2027 & 0.2435 & \cellcolor[rgb]{0.7,1,0.7}0.795 \\
Qwen2.5       & 0.0833 & 0.1253 & 0.2556 & 0.765 \\
MiniMind2     & 0.1282 & 0.1670 & 0.1219 & --    \\
\bottomrule
\end{tabular}
\end{table}

In the CD task, GPT-4o demonstrates the best overall performance, achieving the highest scores across all metrics, especially in METEOR (0.3416), indicating strong semantic understanding. DeepSeek-V3 follows with moderate results, while Qwen2.5 lags behind. MiniMind2 shows decent BLEU-1 performance (0.1282), suggesting it can capture surface-level information reasonably well, though its lower METEOR score implies limitations in deeper semantic reasoning.

% ----- Table for Task EBK -----
\begin{table}[h]
\centering
\caption{Evaluation Metrics for Task CD}
\label{tab:metrics_cd}
\begin{tabular}{lcccc}
\toprule
Model & BLEU-1 & ROUGE-L & METEOR & MMS \\
\midrule
GPT-4o        & \cellcolor[rgb]{0.7,1,0.7}0.2810 & \cellcolor[rgb]{0.7,1,0.7}0.3130 & \cellcolor[rgb]{0.7,1,0.7}0.3416 & 0.786 \\
DeepSeek-V3   & 0.1374 & 0.2027 & 0.2435 & \cellcolor[rgb]{0.7,1,0.7}0.795 \\
Qwen2.5       & 0.0833 & 0.1253 & 0.2556 & 0.765 \\
MiniMind2     & 0.1282 & 0.1670 & 0.1219 & --    \\
\bottomrule
\end{tabular}
\end{table}

In the EBK task, GPT-4o leads across metrics, especially in METEOR (0.4094) and MMS (0.832), highlighting its strength in generating accurate and semantically coherent ECG-related knowledge. While DeepSeek-V3 and Qwen2.5 show moderate gains, they still trail in lexical and semantic precision. Notably, MiniMind2 achieves the highest BLEU-1 score (0.2842), indicating strong word-level overlap, but its lower METEOR and missing MMS suggest limited semantic richness. This positions MiniMind2 as effective for keyword-focused tasks, whereas GPT-4o remains superior for clinically reliable generation.

% ----- Table for Task GRA -----
\begin{table}[h]
\centering
\caption{Evaluation Metrics for Task GRA}
\label{tab:metrics_gra}
\begin{tabular}{lcccc}
\toprule
Model & BLEU-1 & ROUGE-L & METEOR & MMS \\
\midrule
GPT-4o        & 0.0139 & 0.1382 & 0.0781 & 0.693 \\
DeepSeek-V3   & 0.0247 & 0.1439 & 0.0680 & \cellcolor[rgb]{0.7,1,0.7}0.807 \\
Qwen2.5       & \cellcolor[rgb]{0.7,1,0.7}0.1429 & \cellcolor[rgb]{0.7,1,0.7}0.1883 & \cellcolor[rgb]{0.7,1,0.7}0.1730 & 0.766 \\
\bottomrule
\end{tabular}
\end{table}

In the GRA task, DeepSeek-V3 leads with MMS (0.807), indicating its superior ability to generate accurate risk assessments. GPT-4o follows with an MMS score of 0.693, performing well in ROUGE-L but struggling to fully capture risk-related nuances. Qwen2.5 in MMS (0.766), outperforming GPT-4o but still trailing DeepSeek-V3. Overall, DeepSeek-V3 excels in risk assessment generation, while GPT-4o and Qwen2.5 are stronger in lexical precision.

% ----- Table for Task LTD -----
\begin{table}[h]
\centering
\caption{Evaluation Metrics for Task LTD}
\label{tab:metrics_ltd}
\begin{tabular}{lcccc}
\toprule
Model        & BLEU-1          & ROUGE-L         & METEOR          & MMS    \\
\midrule
GPT-4o       & 0.1860          & \cellcolor[rgb]{0.7,1,0.7}0.2733 & 0.1994          & 0.793  \\
DeepSeek-V3  & 0.1895          & 0.2621          & 0.1780          & 0.741  \\
Qwen2.5      & \cellcolor[rgb]{0.7,1,0.7}0.2264 & 0.2379          & \cellcolor[rgb]{0.7,1,0.7}0.2870 & \cellcolor[rgb]{0.7,1,0.7}0.764 \\
\bottomrule
\end{tabular}
\end{table}

In the LTD task, Qwen2.5 stands out with an MMS score of 0.764 and a METEOR score of 0.2870, demonstrating its ability to handle longer text more effectively. GPT-4o and DeepSeek-V3 have lower scores, indicating challenges in generating coherent and contextually accurate long-form medical texts. Qwen2.5 seems to have an advantage in understanding and generating longer sequences, providing more coherent responses.

% ----- Table for Task PP -----
\begin{table}[h]
\centering
\caption{Evaluation Metrics for Task PP}
\label{tab:metrics_pp}
\begin{tabular}{lcccc}
\toprule
Model        & BLEU-1          & ROUGE-L         & METEOR          & MMS    \\
\midrule
GPT-4o       & \cellcolor[rgb]{0.7,1,0.7}0.2008 & \cellcolor[rgb]{0.7,1,0.7}0.3011 & \cellcolor[rgb]{0.7,1,0.7}0.3427 & \cellcolor[rgb]{0.7,1,0.7}0.816 \\
DeepSeek-V3  & 0.0772          & 0.1779          & 0.1815          & 0.783  \\
Qwen2.5      & 0.0491          & 0.0954          & 0.1957          & 0.764  \\
MiniMind2    & 0.1230          & 0.1336          & 0.1252          & --     \\
\bottomrule
\end{tabular}
\end{table}

In the PP (Patient Prognosis) task, GPT-4o outperforms all models across the board, with top scores in METEOR (0.3427) and MMS (0.816), reflecting its strong capability in generating fluent and contextually appropriate prognostic statements. DeepSeek-V3 and Qwen2.5 lag significantly, especially in BLEU-1 and ROUGE-L, suggesting less accurate predictions. MiniMind2 shows moderate lexical performance (BLEU-1: 0.1230), but its low METEOR (0.1252) and lack of MMS indicate weaker semantic depth. Overall, GPT-4o remains the most reliable for prognosis generation, while MiniMind2 may suit simpler summarization tasks.

% ----- Table for Task PRTK -----
\begin{table}[h]
\centering
\caption{Evaluation Metrics for Task PRTK}
\label{tab:metrics_prtk}
\begin{tabular}{lcccc}
\toprule
Model        & BLEU-1          & ROUGE-L         & METEOR          & MMS    \\
\midrule
GPT-4o       & \cellcolor[rgb]{0.7,1,0.7}0.3855 & \cellcolor[rgb]{0.7,1,0.7}0.4512 & \cellcolor[rgb]{0.7,1,0.7}0.4730 & \cellcolor[rgb]{0.7,1,0.7}0.781 \\
DeepSeek-V3  & 0.1562          & 0.2387          & 0.3550          & 0.760  \\
Qwen2.5      & 0.1276          & 0.1919          & 0.3107          & 0.756  \\
\bottomrule
\end{tabular}
\end{table}

In the PRTK task, GPT-4o dominates with the highest scores in MMS (0.781) and METEOR (0.4730), showing strong ethical communication abilities and an understanding of patient rights. DeepSeek-V3 and Qwen2.5 have relatively lower scores, especially in MMS, reflecting their weaknesses in generating responses that align with ethical guidelines and provide accurate prognostic information.

\subsection*{MiniMind2 Performance Comparison Across Metrics}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.82\linewidth]{Fig10-1.png}
    \includegraphics[width=0.82\linewidth]{Fig10-2.png}
    \includegraphics[width=0.82\linewidth]{Fig10-3.png}
    \caption{Performance comparison of MiniMind2 with larger models (GPT-4o, DeepSeek-V3, Qwen2.5) across BLEU-1, ROUGE-L, and METEOR on selected tasks}
    \label{fig10}
\end{figure}

In the evaluation of CK, CD, EBK, and PP, the lightweight MiniMind2 model (25.8M parameters) demonstrated notable capabilities despite its small size. After fine-tuning on ECG-Expert-QA, MiniMind2 achieved performance in knowledge-based tasks (CK and EBK) that was nearly comparable to much larger models. In CK, it even achieved a higher BLEU-1 score (0.1864) than GPT-4o (0.1648), indicating strong lexical alignment. However, its METEOR score was only 0.1631, less than half of GPT-4o's 0.3098, revealing limitations in semantic completeness. A similar pattern was observed in EBK, where MiniMind2's performance approached that of GPT-4o and DeepSeek-V3.

In contrast, for CD and PP, MiniMind2 underperformed, with METEOR scores around 0.12, while GPT-4o reached approximately 0.34. This suggests that MiniMind2 struggles with tasks requiring complex reasoning and richer language generation.

Nevertheless, given its compact size, MiniMind2 still delivered competitive results. In some lexical metrics like BLEU-1, it outperformed Qwen2.5 (e.g., in CK and EBK), and approached DeepSeek-V3 in basic QA tasks. These results indicate that high-quality, domain-specific datasets like ECG-Expert-QA can substantially boost the effectiveness of small models in targeted medical tasks, though significant gaps remain in complex diagnostic reasoning and semantic richness.

\subsection*{Summary} 
Among the four models (GPT-4o, DeepSeek-V3, Qwen2.5, and MiniMind2) evaluated on seven tasks from the ECG-Expert-QA benchmark, GPT-4o achieves the highest overall performance. It consistently attains top scores on both lexical metrics (e.g., BLEU-1) and semantic measures (e.g., METEOR and MMS), indicating a superior grasp of clinical context. DeepSeek-V3 and Qwen2.5 show moderate performance, each excelling in certain areas: DeepSeek-V3 is notably strong in risk assessment tasks, whereas Qwen2.5 handles long-form diagnostic narratives more effectively. Notably, despite its significantly smaller size (25.8M parameters), the fine-tuned MiniMind2 model delivers surprisingly high lexical accuracy, particularly on knowledge-based tasks like CK and EBK. However, MiniMind2 exhibits clear limitations in complex reasoning and in producing semantically rich responses compared to the larger models.


\section{Conclusion} 
\label{sec6} 
In this work, we introduce ECG-Expert-QA, a comprehensive benchmark designed to assess the performance of large language models (LLMs) in intelligent ECG interpretation. By combining real clinical data with synthetically generated cases, the dataset covers 12 diagnostic tasks and supports multi-turn, knowledge-intensive dialogues. It provides a solid foundation for evaluating both clinical reasoning and conversational capabilities in medical AI.

To assess the effectiveness of current LLMs in ECG-related diagnostic tasks, we developed a structured evaluation framework based on four widely recognized natural language generation (NLG) metrics: BLEU-1, ROUGE-L, METEOR, and the newly introduced MMS. Experiments with three representative models—GPT-4o, DeepSeek-V3, and Qwen2.5—revealed significant variations in performance across tasks and evaluation metrics. GPT-4o consistently delivered superior results, especially in complex, semantically rich, and ethically sensitive scenarios. Qwen2.5 showed particular strengths in handling long-form text and risk assessment tasks. DeepSeek-V3 maintained stable performance in knowledge-driven tasks but lagged in terms of semantic reasoning and adaptability to complex scenarios.

Our findings underscore the importance of multi-dimensional evaluation in medical AI, as different models excel in specific areas depending on task complexity, contextual depth, and ethical sensitivity. Furthermore, ECG-Expert-QA and its evaluation framework provide a crucial step towards the development of more reliable, explainable, and patient-centered AI systems for healthcare.

Looking ahead, future research should focus on integrating temporal ECG dynamics, real-time clinical workflows, and cross-lingual adaptability. Additionally, there is a need to enhance continual learning capabilities and safety assessments in LLM-based diagnostic tools. As an open-source resource, ECG-Expert-QA is poised to drive advancements in multimodal, conversational, and ethically conscious AI for healthcare.

\section{Future Work}

While ECG-Expert-QA provides a comprehensive benchmark for evaluating diagnostic reasoning in ECG-focused medical LLMs, several directions remain open for future exploration. First, expanding the benchmark to cover other cardiac-related modalities (e.g., phonocardiogram, echocardiography) and systemic conditions (e.g., metabolic or respiratory diseases) may enhance the versatility of evaluation scenarios.

Second, incorporating real-time and continuous monitoring data into the QA framework could further bridge the gap between static diagnosis and dynamic clinical decision-making. Future research could also explore the integration of wearable ECG devices for continuous multimodal monitoring, enabling more dynamic clinical modeling beyond static diagnostic tasks~\cite{bibTV}. Such an extension would support patient-specific longitudinal assessment, particularly for chronic conditions like heart failure, arrhythmia recurrence, or cardiopulmonary interactions.

Lastly, the alignment between expert feedback and model-generated QA paths remains a key frontier. We envision fine-tuning LLMs using reinforcement learning with expert trajectories, and further enhancing trustworthiness through explainability and counterfactual simulations embedded in multi-agent interactions.

\begin{thebibliography}{00}

\bibitem{holzinger2017needbuildexplainableai}
A.~Holzinger, C.~Biemann, C.~S.~Pattichis, and D.~B.~Kell, ``What do we need to build explainable AI systems for the medical domain?,'' \emph{arXiv preprint arXiv:1712.09923}, 2017. [Online]. Available: https://arxiv.org/abs/1712.09923

\bibitem{johnson2023mimiciv}
A.~E.~W.~Johnson, L.~Bulgarelli, L.~Shen \emph{et al.}, ``MIMIC-IV, a freely accessible electronic health record dataset,'' \emph{Sci. Data}, vol.~10, no.~1, p.~1, Jan. 2023, doi: 10.1038/s41597-022-01899-x.

\bibitem{strika2024bridging}
Z.~Strika, K.~Petkovic, R.~Likic, and R.~Batenburg, ``Bridging healthcare gaps: A scoping review on the role of artificial intelligence, deep learning, and large language models in alleviating problems in medical deserts,'' \emph{Postgrad. Med. J.}, vol.~101, no.~1191, pp.~4--16, Dec. 2024, doi: 10.1093/postmj/qgae122.

\bibitem{Luo_2022}
R.~Luo, L.~Sun, Y.~Xia, T.~Qin, S.~Zhang, H.~Poon, and T.-Y.~Liu, ``BioGPT: Generative pre-trained transformer for biomedical text generation and mining,'' \emph{Brief. Bioinform.}, vol.~23, no.~6, Sep. 2022, Art. no. bbac409, doi: 10.1093/bib/bbac409.

\bibitem{MORLEY2020113172}
J.~Morley, C.~C.~V.~Machado, C.~Burr, J.~Cowls, I.~Joshi, M.~Taddeo, and L.~Floridi, ``The ethics of AI in health care: A mapping review,'' \emph{Soc. Sci. Med.}, vol.~260, Art. no. 113172, Nov. 2020, doi: 10.1016/j.socscimed.2020.113172.

\bibitem{yang2022llmEHR}
X.~Yang, A.~Chen, N.~PourNejatian \emph{et al.}, ``A large language model for electronic health records,'' \emph{npj Digit. Med.}, vol.~5, no.~1, p.~194, Oct. 2022, doi: 10.1038/s41746-022-00742-2.

\bibitem{bib0} B. Gow, T. Pollard, L. A. Nathanson, A. Johnson, B. Moody, C. Fernandes, N. Greenbaum, S. Berkowitz, D. Moukheiber, P. Eslami, E. Herbst, R. Mark, and S. Horng, ``MIMIC-IV-ECG – Diagnostic electrocardiogram matched subset (version 0.1),'' PhysioNet, 2022. [Online]. Available: https://doi.org/10.13026/tw25-ec93

\bibitem{bib10} Y. Cai, L. Wang, Y. Wang, G. de Melo, Y. Zhang, Y.-F. Wang, and L. He, ``MedEvalHub: A large-scale Chinese benchmark for evaluating medical large language models,'' in {\it Proc. 38th AAAI Conf. Artif. Intell. (AAAI)}, 2024.

\bibitem{bib11} Y. Liao, Y. Meng, H. Liu, Y.-F. Wang, and Y. Wang, ``An automatic evaluation framework for multi-turn medical consultations capabilities of large language models,'' {\it arXiv preprint}, 2023. [Online]. Available: https://arxiv.org

\bibitem{bib12} Y. Yang, Y. Liao, Y. Wang, L. Wang, L. He, Y. Zhang, and Y.-F. Wang, ``GenMedicalEval: A unified medical evaluation benchmark for Chinese LLMs,'' {\it arXiv preprint}, 2023. [Online]. Available: https://arxiv.org

\bibitem{bib1} J. Oh, G. Lee, S. Bae, J.-M. Kwon, and E. Choi, ``ECG-QA: A comprehensive question answering dataset combined with electrocardiogram,'' presented at the {\it Thirty-seventh Conf. Neural Inf. Process. Syst. (NeurIPS), Datasets and Benchmarks Track}, New Orleans, LA, USA, Dec. 2023. [Online]. Available: https://openreview.net/forum?id=YWJ7Yi4OtH

\bibitem{bib2} Y. Zhao, T. Zhang, X. Wang, P. Han, T. Chen, L. Huang, Y. Jin, and J. Kang, ``ECG-Chat: A large ECG-language model for cardiac disease diagnosis,'' {\it arXiv preprint}, arXiv:2408.08849, 2024. [Online]. Available: https://arxiv.org/abs/2408.08849

\bibitem{bib3} X. Lan, F. Wu, K. He, Q. Zhao, S. Hong, and M. Feng, ``GEM: Empowering MLLM for grounded ECG understanding with time series and images,'' {\it arXiv preprint}, arXiv:2503.06073, 2025.

\bibitem{bib4} A. Saab, L. Grinberg, and M. Raza, ``MedGemini: Multimodal medical foundation models for ECG and imaging,'' {\it arXiv preprint}, arXiv:2403.04567, 2024. [Online]. Available: https://arxiv.org/abs/2403.04567

\bibitem{bibReGen} J. Tang, T. Xia, Y. Lu, C. Mascolo, and A. Saeed, ``Electrocardiogram report generation and question answering via retrieval-augmented self-supervised modeling,'' {\it arXiv preprint}, arXiv:2409.08788, 2024. [Online]. Available: https://arxiv.org/abs/2409.08788

\bibitem{bibQualityReview} K. van der Bijl, M. Elgendi, and C. Menon, ``Automatic ECG quality assessment techniques: A systematic review,'' {\it Diagnostics}, vol. 12, no. 11, Art. no. 2578, Oct. 2022, doi: 10.3390/diagnostics12112578.

\bibitem{bibPTBXL} P. Wagner, N. Strodthoff, R.-D. Bousseljot, D. Kreiseler, F. I. Lunze, W. Samek, and T. Schaeffter, ``PTB-XL, a large publicly available electrocardiography dataset,'' {\it Sci. Data}, vol. 7, no. 1, Art. no. 154, May 2020, doi: 10.1038/s41597-020-0495-6.

\bibitem{bib5} S. Schmidgall, R. Ziaei, C. Harris, E. Reis, J. Jopling, and M. Moor, ``AgentClinic: A multimodal agent benchmark to evaluate AI in simulated clinical environments,'' {\it arXiv preprint}, arXiv:2405.07960, 2024. [Online]. Available: https://arxiv.org/abs/2405.07960

\bibitem{bib6} Z. Yao, Z. Zhang, C. Tang, X. Bian, Y. Zhao, Z. Yang, {\it et al.}, ``MedQA-CS: Benchmarking large language models’ clinical skills using an AI-SCE framework,'' {\it arXiv preprint}, arXiv:2410.01553, 2024. [Online]. Available: https://arxiv.org/abs/2410.01553

\bibitem{bibLuoJiaHOG} Y. Zhao, M. Zhang, B. Yang, Z. Zhang, J. Kang, and J. Gong, ``LuoJiaHOG: A hierarchy oriented geo-aware image caption dataset for remote sensing image–text retrieval,'' {\it ISPRS J. Photogramm. Remote Sens.}, vol. 222, pp. 130--151, 2025. doi: https://doi.org/10.1016/j.isprsjprs.2025.02.009. [Online]. Available: https://www.sciencedirect.com/science/article/pii/S0924271625000590

\bibitem{bibH36M1} C. Ionescu, D. Papava, V. Olaru, and C. Sminchisescu, ``Human3.6M: Large scale datasets and predictive methods for 3D human sensing in natural environments,'' {\it IEEE Trans. Pattern Anal. Mach. Intell.}, vol. 36, no. 7, pp. 1325--1339, Jul. 2014.

\bibitem{bibH36M2} C. Ionescu, F. Li, and C. Sminchisescu, ``Latent structured models for human pose estimation,'' in {\it Proc. Int. Conf. Comput. Vision (ICCV)}, 2011.

\bibitem{bibAudioSet} J. F. Gemmeke, D. P. W. Ellis, D. Freedman, A. Jansen, W. Lawrence, R. C. Moore, M. Plakal, and M. Ritter, ``Audio set: An ontology and human-labeled dataset for audio events,'' in {\it Proc. IEEE Int. Conf. Acoust., Speech Signal Process. (ICASSP)}, New Orleans, LA, 2017.

\bibitem{bib7} K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, ``BLEU: A method for automatic evaluation of machine translation,'' in {\it Proc. 40th Annu. Meeting Assoc. Comput. Linguist. (ACL)}, Philadelphia, PA, USA, Jul. 2002, pp. 311--318. doi: 10.3115/1073083.1073135. [Online]. Available: https://aclanthology.org/P02-1040/

\bibitem{bib8} C.-Y. Lin, ``ROUGE: A package for automatic evaluation of summaries,'' in {\it Proc. ACL Workshop Text Summarization Branches Out}, Barcelona, Spain, Jul. 2004, pp. 74--81. [Online]. Available: https://aclanthology.org/W04-1013/

\bibitem{bib9} S. Banerjee and A. Lavie, ``METEOR: An automatic metric for MT evaluation with improved correlation with human judgments,'' in {\it Proc. ACL Workshop Intrinsic and Extrinsic Eval. Measures for MT and Summarization}, Ann Arbor, MI, USA, Jun. 2005, pp. 65--72. [Online]. Available: https://aclanthology.org/W05-0909/

\bibitem{bib13} OpenAI, A. Hurst, A. Lerer, A. P. Goucher, A. Perelman, A. Ramesh, {\it et al.}, ``GPT-4o system card,'' {\it arXiv preprint}, arXiv:2410.21276, 2024. [Online]. Available: https://arxiv.org/abs/2410.21276

\bibitem{bib14} DeepSeek-AI, A. Liu, B. Feng, B. Xue, B. Wang, and B. Wu, {\it et al.}, ``DeepSeek-V3 technical report,'' {\it arXiv preprint}, arXiv:2412.19437, 2025. [Online]. Available: https://arxiv.org/abs/2412.19437

\bibitem{bib15} Qwen, A. Yang, B. Yang, B. Zhang, B. Hui, B. Zheng, {\it et al.}, ``Qwen2.5 technical report,'' {\it arXiv preprint}, arXiv:2412.15115, 2025. [Online]. Available: https://arxiv.org/abs/2412.15115

\bibitem{bibmms1} K. Lu, A. Grover, P. Abbeel, and I. Mordatch, ``Pretrained transformers as universal computation engines,'' {\it arXiv preprint}, arXiv:2103.05247, 2021. [Online]. Available: https://arxiv.org/abs/2103.05247

\bibitem{bibmms2} N. Stiennon, L. Ouyang, J. Wu, D. M. Ziegler, R. Lowe, C. Voss, A. Radford, D. Amodei, and P. Christiano, ``Learning to summarize from human feedback,'' {\it arXiv preprint}, arXiv:2009.01325, 2022. [Online]. Available: https://arxiv.org/abs/2009.01325

\bibitem{bibminimind} J. Gong, ``minimind: A small GPT model with 26M parameters trained from scratch in 2 hours,'' 2025. [Online]. Available: https://github.com/jingyaogong/minimind

\bibitem{bibTV} J. Lázaro, N. Reljin, R. Bailón, E. Gil, Y. Noh, P. Laguna, and K. H. Chon, ``Tracking tidal volume from Holter and wearable armband electrocardiogram monitoring,'' {\it IEEE J. Biomed. Health Inform.}, vol. 28, no. 6, pp. 3457--3465, Jun. 2024, doi: 10.1109/JBHI.2024.3383232.


\end{thebibliography}


\end{document}

\IEEEPARstart{T}{his} document is a template for \LaTeX. If you are 
reading a paper or PDF version of this document, please download the 
template from the IEEE Web site at \underline
{http://ieeeauthorcenter.ieee.org/create-your-ieee-article/
use-}\discretionary{}{}{}\underline{authoring-tools-and-ieee-article-templates/ieee-articletemplates/} so you can use it to prepare your manuscript. If 
you would prefer to use \LaTeX, download IEEE's \LaTeX style and sample files 
from the same Web page. You can also explore using the Overleaf editor at 
\underline
{https://www.overleaf.com/blog/278-how-to-use-overleaf-with-}\discretionary{}{}{}\underline
{ieee-collabratec-your-quick-guide-to-getting-started\#.}\discretionary{}{}{}\underline{xsVp6tpPkrKM9}

If your paper is intended for a conference, please contact your conference 
editor concerning acceptable word processor formats for your particular 
conference. 

IEEE will do the final formatting of your paper. If your paper is intended 
for a conference, please observe the conference page limits. 

\subsection{Abbreviations and Acronyms}
Define abbreviations and acronyms the first time they are used in the text, 
even after they have already been defined in the abstract. Abbreviations 
such as IEEE, SI, ac, and dc do not have to be defined. Abbreviations that 
incorporate periods should not have spaces: write ``C.N.R.S.,'' not ``C. N. 
R. S.'' Do not use abbreviations in the title unless they are unavoidable 
(for example, ``IEEE'' in the title of this article).

\subsection{Other Recommendations}
Use one space after periods and colons. Hyphenate complex modifiers: 
``zero-field-cooled magnetization.'' Avoid dangling participles, such as, 
``Using \eqref{eq}, the potential was calculated.'' [It is not clear who or what 
used \eqref{eq}.] Write instead, ``The potential was calculated by using \eqref{eq},'' or 
``Using \eqref{eq}, we calculated the potential.''

Use a zero before decimal points: ``0.25,'' not ``.25.'' Use 
``cm$^{3}$,'' not ``cc.'' Indicate sample dimensions as ``0.1 cm 
$\times $ 0.2 cm,'' not ``0.1 $\times $ 0.2 cm$^{2}$.'' The 
abbreviation for ``seconds'' is ``s,'' not ``sec.'' Use 
``Wb/m$^{2}$'' or ``webers per square meter,'' not 
``webers/m$^{2}$.'' When expressing a range of values, write ``7 to 
9'' or ``7--9,'' not ``7$\sim $9.''

A parenthetical statement at the end of a sentence is punctuated outside of 
the closing parenthesis (like this). (A parenthetical sentence is punctuated 
within the parentheses.) In American English, periods and commas are within 
quotation marks, like ``this period.'' Other punctuation is ``outside''! 
Avoid contractions; for example, write ``do not'' instead of ``don't.'' The 
serial comma is preferred: ``A, B, and C'' instead of ``A, B and C.''

If you wish, you may write in the first person singular or plural and use 
the active voice (``I observed that $\ldots$'' or ``We observed that $\ldots$'' 
instead of ``It was observed that $\ldots$''). Remember to check spelling. 


Try not to use too many typefaces in the same article. Please remember that MathJax
cannot handle nonstandard typefaces.

\subsection{Equations}
Number equations consecutively with equation numbers in parentheses flush 
with the right margin, as in \eqref{eq}. To make your equations more 
compact, you may use the solidus (~/~), the exp function, or appropriate 
exponents. Use parentheses to avoid ambiguities in denominators. Punctuate 
equations when they are part of a sentence, as in
\begin{equation}E=mc^2.\label{eq}\end{equation}

Be sure that the symbols in your equation have been defined before the 
equation appears or immediately following. Italicize symbols ($T$ might refer 
to temperature, but T is the unit tesla). Refer to ``\eqref{eq},'' not ``Eq. \eqref{eq}'' 
or ``equation \eqref{eq},'' except at the beginning of a sentence: ``Equation \eqref{eq} 
is $\ldots$ .''


\subsection{Algorithms}
Algorithms should be numbered and include a short title.
They are set off from the text with rules above and below the title and after the last line.
\begin{algorithm}[H]
\caption{Weighted Tanimoto ELM.}\label{alg:alg1}
\begin{algorithmic}
\STATE 
\STATE {\textsc{TRAIN}}$(\mathbf{X} \mathbf{T})$
\STATE \hspace{0.5cm}$ \textbf{select randomly } W \subset \mathbf{X}  $
\STATE \hspace{0.5cm}$ N_\mathbf{t} \gets | \{ i : \mathbf{t}_i = \mathbf{t} \} | $ \textbf{ for } $ \mathbf{t}= -1,+1 $
\STATE \hspace{0.5cm}$ B_i \gets \sqrt{ \textsc{max}(N_{-1},N_{+1}) / N_{\mathbf{t}_i} } $ \textbf{ for } $ i = 1,...,N $
\STATE \hspace{0.5cm}$ \hat{\mathbf{H}} \gets  B \cdot (\mathbf{X}^T\textbf{W})/( \mathbb{1}\mathbf{X} + \mathbb{1}\textbf{W} - \mathbf{X}^T\textbf{W} ) $
\STATE \hspace{0.5cm}$ \beta \gets \left ( I/C + \hat{\mathbf{H}}^T\hat{\mathbf{H}} \right )^{-1}(\hat{\mathbf{H}}^T B\cdot \mathbf{T})  $
\STATE \hspace{0.5cm}\textbf{return} $\textbf{W},  \beta $
\STATE 
\STATE {\textsc{PREDICT}}$(\mathbf{X} )$
\STATE \hspace{0.5cm}$ \mathbf{H} \gets  (\mathbf{X}^T\textbf{W} )/( \mathbb{1}\mathbf{X}  + \mathbb{1}\textbf{W}- \mathbf{X}^T\textbf{W}  ) $
\STATE \hspace{0.5cm}\textbf{return}  $\textsc{sign}( \mathbf{H} \beta )$
\end{algorithmic}
\label{alg1}
\end{algorithm}
\subsection{\LaTeX-Specific Advice}

Please use ``soft'' (e.g., \verb|\eqref{Eq}|) cross references instead
of ``hard'' references (e.g., \verb|(1)|). That will make it possible
to combine sections, add equations, or change the order of figures or
citations without having to go through the file line by line.

Please don't use the \verb|{eqnarray}| equation environment. Use
\verb|{align}| or \verb|{IEEEeqnarray}| instead. The \verb|{eqnarray}|
environment leaves unsightly spaces around relation symbols.

Please note that the \verb|{subequations}| environment in {\LaTeX}
will increment the main equation counter even when there are no
equation numbers displayed. If you forget that, you might write an
article in which the equation numbers skip from (17) to (20), causing
the copy editors to wonder if you've discovered a new method of
counting.

{\BibTeX} gets the bibliographic
data from .bib files. If you use {\BibTeX} to produce a
bibliography you must send the .bib files. 

If you assign the same label to a
subsubsection and a table, you might find that Table I has been cross
referenced as Table IV-B3. 

If you put a
\verb|\label| command before the command that updates the counter it's
supposed to be using, the label will pick up the last counter to be
cross referenced instead. In particular, a \verb|\label| command
should not go before the caption of a figure or a table.

Do not use \verb|\nonumber| inside the \verb|{array}| environment. It
will not stop equation numbers inside \verb|{array}| (there won't be
any anyway) and it might stop a wanted equation number in the
surrounding equation.

If you are submitting your paper to a colorized journal, you can use
the following two lines at the start of the article to ensure its
appearance resembles the final copy:

\smallskip\noindent
\begin{small}
\begin{tabular}{l}
\verb+\+\texttt{documentclass[journal,twoside,web]\{ieeecolor\}}\\
\verb+\+\texttt{usepackage\{\textit{Journal\_Name}\}}
\end{tabular}
\end{small}

\section{Units}
Use either SI (MKS) or CGS as primary units. (SI units are strongly 
encouraged.) English units may be used as secondary units (in parentheses). 
This applies to papers in data storage. For example, write ``15 
Gb/cm$^{2}$ (100 Gb/in$^{2})$.'' An exception is when 
English units are used as identifiers in trade, such as ``3\textonehalf-in 
disk drive.'' Avoid combining SI and CGS units, such as current in amperes 
and magnetic field in oersteds. This often leads to confusion because 
equations do not balance dimensionally. If you must use mixed units, clearly 
state the units for each quantity in an equation.

The SI unit for magnetic field strength $H$ is A/m. However, if you wish to use 
units of T, either refer to magnetic flux density $B$ or magnetic field 
strength symbolized as $\mu _{0}H$. Use the center dot to separate 
compound units, e.g., ``A$\cdot $m$^{2}$.''

\section{Some Common Mistakes}
The word ``data'' is plural, not singular. The subscript for the 
permeability of vacuum $\mu _{0}$ is zero, not a lowercase letter 
``o.'' The term for residual magnetization is ``remanence''; the adjective 
is ``remanent''; do not write ``remnance'' or ``remnant.'' Use the word 
``micrometer'' instead of ``micron.'' A graph within a graph is an 
``inset,'' not an ``insert.'' The word ``alternatively'' is preferred to the 
word ``alternately'' (unless you really mean something that alternates). Use 
the word ``whereas'' instead of ``while'' (unless you are referring to 
simultaneous events). Do not use the word ``essentially'' to mean 
``approximately'' or ``effectively.'' Do not use the word ``issue'' as a 
euphemism for ``problem.'' When compositions are not specified, separate 
chemical symbols by en-dashes; for example, ``NiMn'' indicates the 
intermetallic compound Ni$_{0.5}$Mn$_{0.5}$ whereas 
``Ni--Mn'' indicates an alloy of some composition 
Ni$_{x}$Mn$_{1-x}$.

\begin{figure}[!t]
\centerline{\includegraphics[width=\columnwidth]{fig1.png}}
\caption{Magnetization as a function of applied field.
It is good practice to explain the significance of the figure in the caption.}
\label{fig1}
\end{figure}

Be aware of the different meanings of the homophones ``affect'' (usually a 
verb) and ``effect'' (usually a noun), ``complement'' and ``compliment,'' 
``discreet'' and ``discrete,'' ``principal'' (e.g., ``principal 
investigator'') and ``principle'' (e.g., ``principle of measurement''). Do 
not confuse ``imply'' and ``infer.'' 

Prefixes such as ``non,'' ``sub,'' ``micro,'' ``multi,'' and ``ultra'' are 
not independent words; they should be joined to the words they modify, 
usually without a hyphen. There is no period after the ``et'' in the Latin 
abbreviation ``\emph{et al.}'' (it is also italicized). The abbreviation ``i.e.,'' means 
``that is,'' and the abbreviation ``e.g.,'' means ``for example'' (these 
abbreviations are not italicized).

IEEE styleguides are available at
https://journals.\discretionary{}{}{}ieeeauthorcenter.ieee.org/create-your-ieee-journal-article/\discretionary{}{}{}create-the-text-of-your-article/\discretionary{}{}{}ieee-editorial-style-manual/.

\section{Guidelines for Graphics Preparation and Submission}
\label{sec:guidelines}

\subsection{Types of Graphics}
The following list outlines the different types of graphics published in 
IEEE journals. They are categorized based on their construction, and use of 
color/shades of gray:

\subsubsection{Color/Grayscale figures}
{Figures that are meant to appear in color, or shades of black/gray. Such 
figures may include photographs, illustrations, multicolor graphs, and 
flowcharts.}

\subsubsection{Line Art figures}
{Figures that are composed of only black lines and shapes. These figures 
should have no shades or half-tones of gray, only black and white.}

\subsubsection{Author photos}
{Head and shoulders shots of authors that appear at the end of our papers. }

\subsubsection{Tables}
{Data charts which are typically black and white, but sometimes include 
color.}

\begin{table}[h]
\caption{Units for Magnetic Properties}
\label{table}
\setlength{\tabcolsep}{3pt}
\begin{tabular}{|p{25pt}|p{75pt}|p{115pt}|}
\hline
Symbol& 
Quantity& 
Conversion from Gaussian and \par CGS EMU to SI $^{\mathrm{a}}$ \\
\hline
$\Phi $& 
magnetic flux& 
1 Mx $\to  10^{-8}$ Wb $= 10^{-8}$ V$\cdot $s \\
$B$& 
magnetic flux density, \par magnetic induction& 
1 G $\to  10^{-4}$ T $= 10^{-4}$ Wb/m$^{2}$ \\
$H$& 
magnetic field strength& 
1 Oe $\to  10^{3}/(4\pi )$ A/m \\
$m$& 
magnetic moment& 
1 erg/G $=$ 1 emu \par $\to 10^{-3}$ A$\cdot $m$^{2} = 10^{-3}$ J/T \\
$M$& 
magnetization& 
1 erg/(G$\cdot $cm$^{3}) =$ 1 emu/cm$^{3}$ \par $\to 10^{3}$ A/m \\
4$\pi M$& 
magnetization& 
1 G $\to  10^{3}/(4\pi )$ A/m \\
$\sigma $& 
specific magnetization& 
1 erg/(G$\cdot $g) $=$ 1 emu/g $\to $ 1 A$\cdot $m$^{2}$/kg \\
$j$& 
magnetic dipole \par moment& 
1 erg/G $=$ 1 emu \par $\to 4\pi \times  10^{-10}$ Wb$\cdot $m \\
$J$& 
magnetic polarization& 
1 erg/(G$\cdot $cm$^{3}) =$ 1 emu/cm$^{3}$ \par $\to 4\pi \times  10^{-4}$ T \\
$\chi , \kappa $& 
susceptibility& 
1 $\to  4\pi $ \\
$\chi_{\rho }$& 
mass susceptibility& 
1 cm$^{3}$/g $\to  4\pi \times  10^{-3}$ m$^{3}$/kg \\
$\mu $& 
permeability& 
1 $\to  4\pi \times  10^{-7}$ H/m \par $= 4\pi \times  10^{-7}$ Wb/(A$\cdot $m) \\
$\mu_{r}$& 
relative permeability& 
$\mu \to \mu_{r}$ \\
$w, W$& 
energy density& 
1 erg/cm$^{3} \to  10^{-1}$ J/m$^{3}$ \\
$N, D$& 
demagnetizing factor& 
1 $\to  1/(4\pi )$ \\
\hline
\multicolumn{3}{p{251pt}}{Vertical lines are optional in tables. Statements that serve as captions for 
the entire table do not need footnote letters. }\\
\multicolumn{3}{p{251pt}}{$^{\mathrm{a}}$Gaussian units are the same as cg emu for magnetostatics; Mx 
$=$ maxwell, G $=$ gauss, Oe $=$ oersted; Wb $=$ weber, V $=$ volt, s $=$ 
second, T $=$ tesla, m $=$ meter, A $=$ ampere, J $=$ joule, kg $=$ 
kilogram, H $=$ henry.}
\end{tabular}
\label{tab1}
\end{table}

\subsection{Multipart figures}
Figures compiled of more than one sub-figure presented side-by-side, or 
stacked. If a multipart figure is made up of multiple figure
types (one part is lineart, and another is grayscale or color) the figure 
should meet the stricter guidelines.

\subsection{File Formats For Graphics}\label{formats}
Format and save your graphics using a suitable graphics processing program 
that will allow you to create the images as PostScript (PS), Encapsulated 
PostScript (.EPS), Tagged Image File Format (.TIFF), Portable Document 
Format (.PDF), Portable Network Graphics (.PNG), or Metapost (.MPS), sizes them, and adjusts 
the resolution settings. When 
submitting your final paper, your graphics should all be submitted 
individually in one of these formats along with the manuscript.

\subsection{Sizing of Graphics}
Most charts, graphs, and tables are one column wide (3.5 inches/88 
millimeters/21 picas) or page wide (7.16 inches/181 millimeters/43 
picas). The maximum depth a graphic can be is 8.5 inches (216 millimeters/54
picas). When choosing the depth of a graphic, please allow space for a 
caption. Figures can be sized between column and page widths if the author 
chooses, however it is recommended that figures are not sized less than 
column width unless when necessary. 

There is currently one publication with column measurements that do not 
coincide with those listed above. Proceedings of the IEEE has a column 
measurement of 3.25 inches (82.5 millimeters/19.5 picas). 

The final printed size of author photographs is exactly
1 inch wide by 1.25 inches tall (25.4 millimeters$\,\times\,$31.75 millimeters/6 
picas$\,\times\,$7.5 picas). Author photos printed in editorials measure 1.59 inches 
wide by 2 inches tall (40 millimeters$\,\times\,$50 millimeters/9.5 picas$\,\times\,$12 
picas).

\subsection{Resolution }
The proper resolution of your figures will depend on the type of figure it 
is as defined in the ``Types of Figures'' section. Author photographs, 
color, and grayscale figures should be at least 300dpi. Line art, including 
tables should be a minimum of 600dpi.

\subsection{Vector Art}
In order to preserve the figures' integrity across multiple computer 
platforms, we accept files in the following formats: .EPS/.PDF/.PS. All 
fonts must be embedded or text converted to outlines in order to achieve the 
best-quality results.

\subsection{Color Space}
The term color space refers to the entire sum of colors that can be 
represented within the said medium. For our purposes, the three main color 
spaces are Grayscale, RGB (red/green/blue) and CMYK 
(cyan/magenta/yellow/black). RGB is generally used with on-screen graphics, 
whereas CMYK is used for printing purposes.

All color figures should be generated in RGB or CMYK color space. Grayscale 
images should be submitted in Grayscale color space. Line art may be 
provided in grayscale OR bitmap colorspace. Note that ``bitmap colorspace'' 
and ``bitmap file format'' are not the same thing. When bitmap color space 
is selected, .TIF/.TIFF/.PNG are the recommended file formats.

\subsection{Accepted Fonts Within Figures}
When preparing your graphics IEEE suggests that you use of one of the 
following Open Type fonts: Times New Roman, Helvetica, Arial, Cambria, and 
Symbol. If you are supplying EPS, PS, or PDF files all fonts must be 
embedded. Some fonts may only be native to your operating system; without 
the fonts embedded, parts of the graphic may be distorted or missing.

A safe option when finalizing your figures is to strip out the fonts before 
you save the files, creating ``outline'' type. This converts fonts to 
artwork what will appear uniformly on any screen.

\subsection{Using Labels Within Figures}

\subsubsection{Figure Axis labels }
Figure axis labels are often a source of confusion. Use words rather than 
symbols. As an example, write the quantity ``Magnetization,'' or 
``Magnetization M,'' not just ``M.'' Put units in parentheses. Do not label 
axes only with units. As in Fig. 1, for example, write ``Magnetization 
(A/m)'' or ``Magnetization (A$\cdot$m$^{-1}$),'' not just ``A/m.'' Do not label axes with a ratio of quantities and 
units. For example, write ``Temperature (K),'' not ``Temperature/K.'' 

Multipliers can be especially confusing. Write ``Magnetization (kA/m)'' or 
``Magnetization (10$^{3}$ A/m).'' Do not write ``Magnetization 
(A/m)$\,\times\,$1000'' because the reader would not know whether the top 
axis label in Fig. 1 meant 16000 A/m or 0.016 A/m. Figure labels should be 
legible, approximately 8 to 10 point type.

\subsubsection{Subfigure Labels in Multipart Figures and Tables}
Multipart figures should be combined and labeled before final submission. 
Labels should appear centered below each subfigure in 8 point Times New 
Roman font in the format of (a) (b) (c). 

\subsection{File Naming}
Figures (line artwork or photographs) should be named starting with the 
first 5 letters of the author's last name. The next characters in the 
filename should be the number that represents the sequential 
location of this image in your article. For example, in author 
``Anderson's'' paper, the first three figures would be named ander1.tif, 
ander2.tif, and ander3.ps.

Tables should contain only the body of the table (not the caption) and 
should be named similarly to figures, except that `.t' is inserted 
in-between the author's name and the table number. For example, author 
Anderson's first three tables would be named ander.t1.tif, ander.t2.ps, 
ander.t3.eps.

Author photographs should be named using the first five characters of the 
pictured author's last name. For example, four author photographs for a 
paper may be named: oppen.ps, moshc.tif, chen.eps, and duran.pdf.

If two authors or more have the same last name, their first initial(s) can 
be substituted for the fifth, fourth, third$\ldots$ letters of their surname 
until the degree where there is differentiation. For example, two authors 
Michael and Monica Oppenheimer's photos would be named oppmi.tif, and 
oppmo.eps.

\subsection{Referencing a Figure or Table Within Your Paper}
When referencing your figures and tables within your paper, use the 
abbreviation ``Fig.'' even at the beginning of a sentence. Do not abbreviate 
``Table.'' Tables should be numbered with Roman Numerals.

\subsection{Submitting Your Graphics}
Because IEEE will do the final formatting of your paper,
you do not need to position figures and tables at the top and bottom of each 
column. In fact, all figures, figure captions, and tables can be placed at 
the end of your paper. In addition to, or even in lieu of submitting figures 
within your final manuscript, figures should be submitted individually, 
separate from the manuscript in one of the file formats listed above in 
Section \ref{formats}. Place figure captions below the figures; place table titles 
above the tables. Please do not include captions as part of the figures, or 
put them in ``text boxes'' linked to the figures. Also, do not place borders 
around the outside of your figures.

\subsection{Color Processing/Printing in IEEE Journals}
All IEEE Transactions, Journals, and Letters allow an author to publish color figures on IEEE Xplore at no charge, and automatically convert them to grayscale for print versions. In most journals, figures and tables may alternatively be printed in color if an author chooses to do so. Please note that this service comes at an extra expense to the author. If you intend to have print color graphics, you will have the opportunity to indicate this in the Author Gateway and will be contacted by PubOps to confirm the charges. Online-only journals will have their figures appear in color, free of charge 

\section{Conclusion}
A conclusion section is not required. Although a conclusion may review the 
main points of the paper, do not replicate the abstract as the conclusion. A 
conclusion might elaborate on the importance of the work or suggest 
applications and extensions. 

\appendices

Appendixes, if needed, appear before the acknowledgment.

\section*{References and Footnotes}

\subsection{References}
References need not be cited in text. When they are, they appear on the
line, in square brackets, inside the punctuation. Multiple references are
each numbered with separate brackets. When citing a section in a book,
please give the relevant page numbers. In text, refer simply to the
reference number. Do not use ``Ref.'' or ``reference'' except at the
beginning of a sentence: ``Reference [3] shows \textellipsis .'' Please do not use
automatic endnotes in \textit{Word}, rather, type the reference list at the end of the
paper using the ``References'' style.

Reference numbers are set flush left and form a column of their own, hanging
out beyond the body of the reference. The reference numbers are on the line,
enclosed in square brackets. In all references, the given name of the author
or editor is abbreviated to the initial only and precedes the last name. Use
them all; use \textit{et al}.\ only if names are not given.
Abbreviate conference titles. When citing IEEE Transactions,
provide the issue number, page range, volume number, year, and/or month if
available. When referencing a patent, provide the day and the month of
issue, or application. References may not include all information; please
obtain and include relevant information. Do not combine references. There
must be only one reference with each number. If there is a URL included with
the print reference, it can be included at the end of the reference.

Other than books, capitalize only the first word in a paper title, except
for proper nouns and element symbols. For papers published in translation
journals, please give the English citation first, followed by the original
foreign-language citation. See the end of this document for formats and
examples of common references. For a complete discussion of references and
their formats, see the IEEE style manual at 
https://\discretionary{}{}{}journals.ieeeauthorcenter.ieee.org/\discretionary{}{}{}create-your-ieee-journal-article/\discretionary{}{}{}create-the-text-of-your-article/\discretionary{}{}{}ieee-editorial-style-manual/.


\subsection{Footnotes}
Number footnotes separately in superscript numbers.\footnote{It is recommended that footnotes be avoided (except for 
the unnumbered footnote with the receipt date on the first page). Instead, 
try to integrate the footnote information into the text.} Place the actual 
footnote at the bottom of the column in which it is cited; do not put 
footnotes in the reference list (endnotes). Use letters for table footnotes 
(see Table \ref{table}).

\section*{Submitting Your Paper for Review}           

\subsection{Review Stage Using IEEE ScholarOne Manuscripts}

Contributions to the Transactions, Journals, and Letters may be submitted electronically on IEEE ScholarOne Manuscripts. You can get help choosing the correct publication for your manuscript as well as find their corresponding peer review site using the tools listed at http://\discretionary{}{}{}www.ieee.org/\discretionary{}{}{}publications\_standards/\discretionary{}{}{}publications/\discretionary{}{}{}authors/\discretionary{}{}{}authors\_submission.html. Once you have chosen your publication and navigated to the IEEE ScholarOne Manuscripts site, you may log in with your IEEE web account. If there is none, please create a new account. After logging in, go to your Author Center and click ``Start New Submission.''

Along with other information, you will be asked to select the manuscript type from the journal's pre-determined list of options. Depending on the journal, there are various steps to the submission process; please make sure to carefully answer all of the submission questions presented to you. At the end of each step you must click ``Save and Continue''; just uploading the paper is not sufficient. After the last step, you should see a confirmation that the submission is complete. You should also receive an e-mail confirmation. For inquiries regarding the submission of your paper on IEEE ScholarOne Manuscripts, please contact oprs-support@ieee.org or call +1 732 465 5861.

IEEE ScholarOne Manuscripts will accept files for review in various formats. There is a ``Journal Home'' link on the log-in page of each IEEE ScholarOne Manuscripts site that will bring you to the journal's homepage with their detailed requirements; please check these guidelines for your particular journal before you submit.

\subsection{Final Stage Using IEEE ScholarOne Manuscripts}
Upon acceptance, you will receive an email with specific instructions
regarding the submission of your final files. To avoid any delays in
publication, please be sure to follow these instructions. Most journals
require that final submissions be uploaded through IEEE ScholarOne Manuscripts,
although some may still accept final submissions via email. Final
submissions should include source files of your accepted manuscript, high
quality graphic files, and a formatted pdf file. If you have any questions
regarding the final submission process, please contact the administrative
contact for the journal.

In addition to this, upload a file with complete contact information for all
authors. Include full mailing addresses, telephone numbers, fax numbers, and
e-mail addresses. Designate the author who submitted the manuscript on
IEEE ScholarOne Manuscripts as the ``corresponding author.'' This is the only
author to whom proofs of the paper will be sent.

\subsection{Copyright Form}
Authors must submit an electronic IEEE Copyright Form (eCF) upon submitting 
their final manuscript files. You can access the eCF system through your 
manuscript submission system or through the Author Gateway. You are 
responsible for obtaining any necessary approvals and/or security 
clearances. For additional information on intellectual property rights, 
visit the IEEE Intellectual Property Rights department web page at 
\underline{http://www.ieee.org/publications\_standards/publications/rights/}\discretionary{}{}{}\underline{index.html}.

\section*{IEEE Publishing Policy}
The general IEEE policy requires that authors should only submit original 
work that has neither appeared elsewhere for publication, nor is under 
review for another refereed publication. The submitting author must disclose 
all prior publication(s) and current submissions when submitting a 
manuscript. Do not publish ``preliminary'' data or results. The submitting 
author is responsible for obtaining agreement of all coauthors and any 
consent required from employers or sponsors before submitting an article. 
The IEEE Transactions and Journals Department strongly discourages courtesy 
authorship; it is the obligation of the authors to cite only relevant prior 
work.

The IEEE Transactions and Journals Department does not publish conference 
records or proceedings, but can publish articles related to conferences that 
have undergone rigorous peer review. Minimally, two reviews are required for 
every article submitted for peer review.

\section*{Acknowledgment}

The preferred spelling of the word ``acknowledgment'' in American English is 
without an ``e'' after the ``g.'' Use the singular heading even if you have 
many acknowledgments. Avoid expressions such as ``One of us (S.B.A.) would 
like to thank $\ldots$ .'' Instead, write ``F. A. Author thanks $\ldots$ .'' In most 
cases, sponsor and financial support acknowledgments are placed in the 
unnumbered footnote on the first page, not here.

\section*{References}

\def\refname{\vadjust{\vspace*{-2.5em}}} %Please don't do this in a real paper.

\noindent\textit{Basic format for books:}

\noindent J. K. Author, ``Title of chapter in the book,'' in {\it Title of His Published Book, x}th ed. City of Publisher,
(only U.S. State), Country: Abbrev. of Publisher, year, ch. $x$, sec. $x$,\break pp.~{\it xxx--xxx.}

{\it Examples:}{\vadjust{\vspace*{-2.5em}}}
\begin{thebibliography}{00}
\bibitem{bib1} G. O. Young, ``Synthetic structure of industrial plastics,'' in {\it Plastics,}2$^{\mathrm{nd}}$ ed., vol. 3, J. Peters, Ed. New York, NY, USA: McGraw-Hill, 1964, pp. 15--64.
\bibitem{bib2} W.-K. Chen, {\it Linear Networks and Systems.}Belmont, CA, USA: Wadsworth, 1993, pp. 123--135.
\end{thebibliography}

\noindent {\it Basic format for periodicals:}

\noindent J. K. Author, ``Name of paper,'' {\it Abbrev. Title of Periodical}, vol. {\it x, no}. $x, $pp{\it . xxx-xxx,}Abbrev. Month, year, DOI.
 {10.1109.} {{\it XXX}} {.123456}.

{\it Examples:}{\vadjust{\vspace*{-2.5em}}}

\begin{thebibliography}{00}\leftskip1pc
\bibitem{bib3} J. U. Duncombe, ``Infrared navigation---Part I: An assessment of feasibility,'' {\it IEEE Trans. Electron Devices}, vol. ED-11, no. 1, pp. 34--39, Jan. 1959, doi:.  {10.1109/TED.2016.2628402}.
\bibitem{bib4} E. P. Wigner, ``Theory of traveling-wave optical laser,''
{\it Phys. Rev}., vol. 134, pp. A635--A646, Dec. 1965, doi:  {10.1109.} {{\it XXX}} {.123456}.
\bibitem{bib5} E. H. Miller, ``A note on reflector arrays,'' {\it IEEE Trans. Antennas Propagat}., to be published.
\end{thebibliography}

\noindent {\it Basic format for reports:}

\noindent J. K. Author, ``Title of report,'' Abbrev. Name of Co., City of Co., Abbrev.
State, Country, Rep. {\it xxx}, year.

{\it Examples:}
\begin{thebibliography}{00}\leftskip1pc
\bibitem{bib6} E. E. Reber, R. L. Michell, and C. J. Carter, ``Oxygen absorption in the earth's atmosphere,'' Aerospace Corp., Los Angeles, CA, USA, Tech. Rep. TR-0200 (4230-46)-3, Nov. 1988.
\bibitem{bib7} J. H. Davis and J. R. Cogdell, ``Calibration program for the 16-foot antenna,'' Elect. Eng. Res. Lab., Univ. Texas, Austin, TX, USA, Tech. Memo. NGL-006-69-3, Nov. 15, 1987.
\end{thebibliography}

\noindent {\it Basic format for handbooks:}

\noindent {\it Name of Manual/Handbook, x} ed., Abbrev. Name of Co., City of Co., Abbrev. State, Country, year, pp.
{\it xxx-xxx.}

{\it Examples:}{\vadjust{\vspace*{-2.5em}}}

\begin{thebibliography}{00}\leftskip1pc
\bibitem{bib8} {\it Transmission Systems for Communications}, 3rd ed., Western Electric Co., Winston-Salem, NC, USA, 1985, pp. 44--60.
\bibitem{bib9} {\it Motorola Semiconductor Data Manual}, Motorola Semiconductor Products Inc., Phoenix, AZ, USA, 1989.
\end{thebibliography}

\noindent {\it Basic format for books (when available online):}

\noindent J. K. Author, ``Title of chapter in the book,'' in {\it Title of Published Book}, $x$th ed. City of
Publisher, State, Country: Abbrev. of Publisher, year, ch.$x$, sec. $x$, pp.
{\it xxx--xxx}. [Online]. Available:  {http://www.web.com}




{\it Examples:}{\vadjust{\vspace*{-2.5em}}}

\begin{thebibliography}{00}\leftskip1pc
\bibitem{bib10} G. O. Young, ``Synthetic structure of industrial plastics,'' in Plastics, vol. 3, Polymers of Hexadromicon, J. Peters, Ed., 2nd ed. New York, NY, USA: McGraw-Hill, 1964, pp. 15-64. [Online]. Available:  {http://www.bookref.com}.
\bibitem{bib11} {\it The Founders' Constitution}, Philip B. Kurland and Ralph Lerner, eds., Chicago, IL, USA: Univ. Chicago Press, 1987. [Online]. Available:  {http://press-pubs.uchicago.edu/founders/}
\bibitem{bib12} The Terahertz Wave eBook. ZOmega Terahertz Corp., 2014. [Online]. Available:  {http://dl.z-thz.com/eBook/zomega\_ebook\_pdf\_1206\_sr.pdf}. Accessed on: May 19, 2014.
\bibitem{bib13} Philip B. Kurland and Ralph Lerner, eds., {\it The Founders' Constitution.}Chicago, IL, USA: Univ. of Chicago Press, 1987, Accessed on: Feb. 28, 2010, [Online] Available:  {http://press-pubs.uchicago.edu/founders/}
\end{thebibliography}

\noindent {\it Basic format for journals (when available online):}

\noindent J. K. Author, ``Name of paper,'' {\it Abbrev. Title of Periodical}, vol. $x$, no. $x$, pp. {\it xxx-xxx}, Abbrev. Month, year.
Accessed on: Month, Day, year, doi:  {10.1109.} {{\it
XXX}} {.123456}, [Online].

{\it Examples:}{\vadjust{\vspace*{-2.5em}}}

\begin{thebibliography}{00}\leftskip1pc
\bibitem{bib14} J. S. Turner, ``New directions in communications,'' {\it IEEE J. Sel. Areas Commun}., vol. 13, no. 1, pp. 11-23, Jan. 1995. DOI.  {10.1109.} {{\it XXX}} {.123456}.
\bibitem{bib15} W. P. Risk, G. S. Kino, and H. J. Shaw, ``Fiber-optic frequency shifter using a surface acoustic wave incident at an oblique angle,'' {\it Opt. Lett.}, vol. 11, no. 2, pp. 115--117, Feb. 1986, doi: {10.1109.} {{\it XXX}} {.123456}.
\bibitem{bib16} P. Kopyt {\it \textit{et al.}, ``}Electric properties of graphene-based conductive layers from DC up to terahertz range,'' {\it IEEE THz Sci. Technol.,}to be published, doi:  {10.1109/TTHZ.2016.2544142}.
\end{thebibliography}

\noindent {\it Basic format for papers presented at conferences (when available online):}

\noindent J.K. Author. (year, month). Title. presented at abbrev. conference title.
[Type of Medium]. Available: site/path/file
\\
\\
\\
{\it Example:}{\vadjust{\vspace*{-2.5em}}}

\begin{thebibliography}{00}\leftskip1pc
\bibitem{bib17} PROCESS Corporation, Boston, MA, USA. Intranets: Internet technologies deployed behind the firewall for corporate productivity. Presented at INET96 Annual Meeting. [Online]. Available:  {http://home.process.com/Intranets/wp2.htp}
\end{thebibliography}

\noindent {\it Basic format for reports and handbooks (when available online):}

\noindent J. K. Author. ``Title of report,'' Company. City, State, Country. Rep. no.,
(optional: vol./issue), Date. [Online] Available:\\
{site/path/file}\\

{\it Examples:}{\vadjust{\vspace*{-2.5em}}}

\begin{thebibliography}{00}\leftskip1pc
\bibitem{bib18} R. J. Hijmans and J. van Etten, ``Raster: Geographic analysis and modeling with raster data,'' R Package Version 2.0-12, Jan. 12, 2012. [Online]. Available:  {http://CRAN.R-project.org/package$=$raster} {}
\bibitem{bib19} Teralyzer. Lytera UG, Kirchhain, Germany [Online]. Available: http://www.lytera.de/Terahertz\_THz\_Spectroscopy.php?id$=$home, Accessed on: Jun. 5, 2014
\end{thebibliography}

\noindent {\it Basic format for computer programs and electronic documents (when available online):}

\noindent Legislative body. Number of Congress, Session. (year, month day). {\it Number of bill or resolution}, {\it Title}. [Type
of medium]. Available: site/path/file

\noindent {\bf {\it NOTE:}} ISO recommends that capitalization follow the accepted
practice for the language or script in which the information is given.



{\it Example:}{\vadjust{\vspace*{-2.5em}}}

\begin{thebibliography}{00}\leftskip1pc
\bibitem{bib20} U.S. House. 102nd Congress, 1st Session. (1991, Jan. 11). {\it H. Con. Res. 1, Sense of the Congress on Approval of Military Action}. [Online]. Available: LEXIS Library: GENFED File: BILLS
\end{thebibliography}

\noindent {\it Basic format for patents (when available online):}

\noindent Name of the invention, by inventor's name. (year, month day). Patent Number  [Type
of medium]. Available:  {site/path/file}

{\it Example:}{\vadjust{\vspace*{-2.5em}}}

\begin{thebibliography}{00}\leftskip1pc
\bibitem{bib21} Musical toothbrush with mirror, by L.M.R. Brooks. (1992, May 19). Patent D 326 189 [Online]. Available: NEXIS Library: LEXPAT File: DES
\end{thebibliography}


\noindent {\it Basic format for conference proceedings (published):}

\noindent J. K. Author, ``Title of paper,'' in {\it Abbreviated Name of Conf.}, City of Conf., Abbrev. State (if
given), Country, year, pp. {\it xxxxxx.}

{\it Example:}{\vadjust{\vspace*{-2.5em}}}

\begin{thebibliography}{00}\leftskip1pc
\bibitem{bib22} D. B. Payne and J. R. Stern, ``Wavelength-switched passively coupled single-mode optical network,'' in {\it Proc. IOOC-ECOC,}Boston, MA, USA,  1985,
pp. 585--590, doi:  {10.1109.} {{\it XXX}} {.123456}.
\end{thebibliography}

{\it Example for papers presented at conferences (unpublished):}{\vadjust{\vspace*{-2.5em}}}

\begin{thebibliography}{00}\leftskip1pc
\bibitem{bib23} D. Ebehard and E. Voges, ``Digital single sideband detection for interferometric sensors,'' presented at the {\it 2nd Int. Conf. Optical Fiber Sensors,} Stuttgart, Germany, Jan. 2-5, 1984.
\end{thebibliography}

\noindent {\it Basic format for patents:}

\noindent J. K. Author, ``Title of patent,'' U.S. Patent {\it x xxx xxx}, Abbrev. Month, day, year.

{\it Example:}{\vadjust{\vspace*{-2.5em}}}

\begin{thebibliography}{00}\leftskip1pc
\bibitem{bib24} G. Brandli and M. Dick, ``Alternating current fed power supply,'' U.S. Patent 4 084 217, Nov. 4, 1978.
\end{thebibliography}

\noindent {\it Basic format} {\it for theses (M.S.) and dissertations (Ph.D.):}

\noindent a) J. K. Author, ``Title of thesis,'' M.S. thesis, Abbrev. Dept., Abbrev.
Univ., City of Univ., Abbrev. State, year.

\noindent b) J. K. Author, ``Title of dissertation,'' Ph.D. dissertation, Abbrev.
Dept., Abbrev. Univ., City of Univ., Abbrev. State, year.

{\it Examples:}{\vadjust{\vspace*{-2.5em}}}

\begin{thebibliography}{00}\leftskip1pc
\bibitem{bib25} J. O. Williams, ``Narrow-band analyzer,'' Ph.D. dissertation, Dept. Elect. Eng., Harvard Univ., Cambridge, MA, USA, 1993.
\bibitem{bib26} N. Kawasaki, ``Parametric study of thermal and chemical nonequilibrium nozzle flow,'' M.S. thesis, Dept. Electron. Eng., Osaka Univ., Osaka, Japan, 1993.
\end{thebibliography}

\noindent {\it Basic format for the most common types of unpublished references:}

\noindent a) J. K. Author, private communication, Abbrev. Month, year.

\noindent b) J. K. Author, ``Title of paper,'' unpublished.

\noindent c) J. K. Author, ``Title of paper,'' to be published.

{\it Examples:}{\vadjust{\vspace*{-2.5em}}}

\begin{thebibliography}{00}\leftskip1pc
\bibitem{bib27} A. Harrison, private communication, May 1995.
\bibitem{bib28} B. Smith, ``An approach to graphs of linear forms,'' unpublished.
\bibitem{bib29} A. Brahms, ``Representation error for real numbers in binary computer arithmetic,'' IEEE Computer Group Repository, Paper R-67-85.
\end{thebibliography}

\noindent {\it Basic formats for standards:}

\noindent a) {\it Title of Standard}, Standard number, date.

\noindent b) {\it Title of Standard}, Standard number, Corporate author, location, date.

{\it Examples:}{\vadjust{\vspace*{-2.5em}}}

\begin{thebibliography}{00}\leftskip1pc
\bibitem{bib30} IEEE Criteria for Class IE Electric Systems, IEEE Standard 308, 1969.
\bibitem{bib31} Letter Symbols for Quantities, ANSI Standard Y10.5-1968.
\end{thebibliography}

{\it Article number in~reference examples:}{\vadjust{\vspace*{-2.5em}}}

\begin{thebibliography}{00}\leftskip1pc
\bibitem{bib32} R. Fardel, M. Nagel, F. Nuesch, T. Lippert, and A. Wokaun, ``Fabrication of organic light emitting diode pixels by laser-assisted forward transfer,'' {\it Appl. Phys. Lett.}, vol. 91, no. 6, Aug. 2007, Art. no. 061103, doi:  {10.1109.} {{\it XXX}} {.123456}.
\bibitem{bib33} J. Zhang and N. Tansu, ``Optical gain and laser characteristics of InGaN quantum wells on ternary InGaN substrates,'' {\it IEEE Photon. J.}, vol. 5, no. 2, Apr. 2013, Art. no. 2600111, doi:  {10.1109.} {{\it XXX}} {.123456}.
\end{thebibliography}

{\it Example when using \textit{et al.}:}{\vadjust{\vspace*{-2.5em}}}

\begin{thebibliography}{00}\leftskip1pc
\bibitem{bib34} S. Azodolmolky~{{et al.}}, Experimental demonstration of an impairment aware network planning and operation tool for transparent/translucent optical networks,''~{\it J. Lightw. Technol.}, vol. 29, no. 4, pp. 439--448, Sep. 2011,doi:  {10.1109.} {{\it XXX}} {.123456}.
\end{thebibliography}

\noindent {\it Basic format for datasets:}

\noindent Author,  Date, Year. ``Title of Dataset,'' distributed by Publisher/Distributor, http://url.com (or if DOI is used, end with a period)

{\it Example:}{\vadjust{\vspace*{-2.5em}}}

\begin{thebibliography}{00}\leftskip1pc
\bibitem{bib35} U.S. Department of Health and Human Services, Aug. 2013, ``Treatment Episode Dataset: Discharges (TEDS-D): Concatenated, 2006 to 2009,'' U.S. Department of Health and Human Services, Substance Abuse and Mental Health Services Administration, Office of Applied Studies, doi: 10.3886/ICPSR30122.v2.
\end{thebibliography}

\noindent {\it Basic format for code:}

\noindent Author,  Date published or disseminated, Year. ``Complete title, including ed./vers.\#,'' distributed by Publisher/Distributor, http://url.com (or if DOI is used, end with a period)

{\it Example:}{\vadjust{\vspace*{-2.5em}}}

\begin{thebibliography}{00}\leftskip1pc
\bibitem{bib36} T. D'Martin and S. Soares, 2019, ``Code for Assessment of Markov Decision Processes in Long-Term Hydrothermal Scheduling of Single-Reservoir Systems (Version 1.0),'' Code Ocean, doi: \_1.24433/CO.7212286.v1
\end{thebibliography}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{a1.png}}]{First A. Author} (Fellow, IEEE) and all authors may include 
biographies. Biographies are
often not included in conference-related papers.
This author is an IEEE Fellow. The first paragraph
may contain a place and/or date of birth (list
place, then date). Next, the author’s educational
background is listed. The degrees should be listed
with type of degree in what field, which institution,
city, state, and country, and year the degree was
earned. The author’s major field of study should
be lower-cased.

The second paragraph uses the pronoun of the person (he or she) and
not the author’s last name. It lists military and work experience, including
summer and fellowship jobs. Job titles are capitalized. The current job must
have a location; previous positions may be listed without one. Information
concerning previous publications may be included. Try not to list more than
three books or published articles. The format for listing publishers of a book
within the biography is: title of book (publisher name, year) similar to a
reference. Current and previous research interests end the paragraph.

The third paragraph begins with the author’s title and last name (e.g.,
Dr. Smith, Prof. Jones, Mr. Kajor, Ms. Hunter). List any memberships in
professional societies other than the IEEE. Finally, list any awards and work
for IEEE committees and publications. If a photograph is provided, it should
be of good quality, and professional-looking.
\end{IEEEbiography}

\begin{IEEEbiographynophoto}{Second B. Author,} photograph and biography not available at the
time of publication.
\end{IEEEbiographynophoto}

\begin{IEEEbiographynophoto}{Third C. Author Jr.} (Member, IEEE), photograph and biography not available at the
time of publication.
\end{IEEEbiographynophoto}

\end{document}
