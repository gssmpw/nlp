\section{Related Work}
\label{sec2}
\begin{figure*}[h]
    \centering
    \includegraphics[width=1\linewidth]{Fig2.pdf}
    \label{fig2}
    \caption{A Role-Based and Process-Oriented Framework for ECG Multimodal QA Generation}
\end{figure*}

With the rapid advancement of LLMs and multimodal learning, automated ECG interpretation has progressed beyond conventional classification tasks to encompass clinical reasoning and dialog-based medical QA. A number of high-quality multimodal datasets have emerged, aiming to bridge the gap between AI and real-world clinical decision-making, especially in the context of ECG-based diagnosis.

One of the earlier efforts, ECG-QA~\cite{bib1}, proposed a question-answering framework for ECG interpretation. However, its structure is limited to single-turn QA with fixed question templates, lacking the capacity to incorporate contextual or longitudinal clinical reasoning. Later, ECG-Chat~\cite{bib2} introduced a large-scale ECG-language model trained on 19k diagnostic ECG samples, demonstrating the promise of integrating signal data with natural language. Nonetheless, it remains constrained in handling conversational dynamics and rare disease scenarios.

To enhance generalizability and clinical applicability, several studies have explored fusing ECG signals with text, images, and structured data within multimodal frameworks. For instance, GEM~\cite{bib3} proposed a multimodal diagnostic model that combines visual and temporal ECG information for more robust and explainable reasoning. Similarly, MedGemini~\cite{bib4} offered a family of multimodal medical foundation models capable of processing ECG, imaging, and structured EHRs for comprehensive clinical understanding.

Recent efforts have also extended ECG modeling from classification to natural language report generation and QA. ECG-ReGen~\cite{bibReGen} presented a retrieval-augmented framework that integrates self-supervised ECG representation learning with large language models to generate clinical reports and perform diagnostic question answering. While promising, it primarily supports single-turn interactions and lacks support for conversational reasoning or patient-specific context.

In parallel, benchmarking efforts based on large-scale ECG datasets have also advanced. One notable example is the PTB-XL~\cite{bibPTBXL} dataset, which provides a publicly available, richly annotated 12-lead ECG corpus for various diagnostic tasks. Accompanied by rigorous benchmarking and analysis across ResNet- and Inception-based deep learning models, PTB-XL has contributed significantly to structured performance evaluation in ECG classification. However, while valuable for model training and general benchmarking, such datasets remain focused on static classification tasks and lack support for conversational modeling or contextual reasoning.

Moreover, the diagnostic value of ECG-based QA systems is inherently tied to signal quality. A comprehensive review~\cite{bibQualityReview} summarized recent advances in automated ECG quality assessment, underscoring the importance of reliable signal input for downstream AI-driven analysis. Incorporating signal quality metrics into future conversational benchmarks remains a valuable research direction.

Concurrently, the need to evaluate medical LLMs in realistic diagnostic settings has led to the development of interactive and multi-agent environments such as AgentClinic~\cite{bib5} and MedQA-CS~\cite{bib6}. While these benchmarks enable evaluation across diverse clinical domains, they are often focused on textual data or radiology tasks, leaving ECG interpretation underrepresented in the multimodal conversational AI landscape.

Additionally, datasets like Human3.6M~\cite{bibH36M1,bibH36M2} (pose-text), AudioSet~\cite{bibAudioSet} (audio-text), and LuoJiaHOG~\cite{bibLuoJiaHOG} (image-text) have demonstrated success in other domains, furthering the potential of multimodal learning. These datasets pair different modalities, including motion, sound, and visual data, with textual descriptions, providing valuable insights into how multimodal models can integrate diverse data sources for improved performance.

ECG-Expert-QA addresses these limitations by introducing the first open-source benchmark to support multi-turn ECG diagnostic QA. By integrating real clinical records with systematically synthesized cases across 12 diagnostic tasks and 47,211 QA pairs, it supports rich clinical contexts, rare conditions, and temporal disease progression patterns. Its design enables comprehensive evaluation of medical LLMs in terms of diagnostic accuracy, clinical reasoning, and knowledge integration, setting a new standard for conversational ECG AI systems.