%%IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%*************************************************************************


% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. The IEEE's font choices and paper sizes can   ***
% *** trigger bugs that do not appear when using other class files.       ***                          ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/


% Please refer to your journal's instructions for other
% options that should be set.
%\documentclass[border={10pt}]{standalone}
%\documentclass[journal,11pt,onecolumn,draftclsnofoot,]{IEEEtran}
%\documentclass[journal]{IEEEtran}
\documentclass[lettersize,journal]{IEEEtran}
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[journal]{../sty/IEEEtran}





% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/pkg/ifpdf
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
%\usepackage{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of the IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/pkg/cite
% The documentation is contained in the cite.sty file itself.






% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at: 
% http://www.ctan.org/pkg/graphicx
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/pkg/epslatex
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). The IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex





% *** MATH PACKAGES ***
%
%\usepackage{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics.
%
% Note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/amsmath





% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as the IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/pkg/algorithms
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/pkg/algorithmicx




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/array


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.




% *** SUBFIGURE PACKAGES ***
%\ifCLASSOPTIONcompsoc
%  \usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
%\else
%  \usepackage[caption=false,font=footnotesize]{subfig}
%\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a larger sans serif font
% than the serif footnote size font used in traditional IEEE formatting
% and thus the need to invoke different subfig.sty package options depending
% on whether compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/pkg/subfig




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure.
% Be aware that LaTeX2e kernels dated 2015 and later have fixltx2e.sty's
% corrections already built into the system in which case a warning will
% be issued if an attempt is made to load fixltx2e.sty as it is no longer
% needed.
% The latest version and documentation can be found at:
% http://www.ctan.org/pkg/fixltx2e


%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/stfloats
% Do not use the stfloats baselinefloat ability as the IEEE does not allow
% \baselineskip to stretch. Authors submitting work to the IEEE should note
% that the IEEE rarely uses double column equations and that authors should try
% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty
% packages (also by Sigitas Tolusis) as the IEEE does not format its papers in
% such ways.
% Do not attempt to use stfloats with fixltx2e as they are incompatible.
% Instead, use Morten Hogholm'a dblfloatfix which combines the features
% of both fixltx2e and stfloats:
%
% \usepackage{dblfloatfix}
% The latest version can be found at:
% http://www.ctan.org/pkg/dblfloatfix




%\ifCLASSOPTIONcaptionsoff
%  \usepackage[nomarkers]{endfloat}
% \let\MYoriglatexcaption\caption
% \renewcommand{\caption}[2][\relax]{\MYoriglatexcaption[#2]{#2}}
%\fi
% endfloat.sty was written by James Darrell McCauley, Jeff Goldberg and 
% Axel Sommerfeldt. This package may be useful when used in conjunction with 
% IEEEtran.cls'  captionsoff option. Some IEEE journals/societies require that
% submissions have lists of figures/tables at the end of the paper and that
% figures/tables without any captions are placed on a page by themselves at
% the end of the document. If needed, the draftcls IEEEtran class option or
% \CLASSINPUTbaselinestretch interface can be used to increase the line
% spacing as well. Be sure and use the nomarkers option of endfloat to
% prevent endfloat from "marking" where the figures would have been placed
% in the text. The two hack lines of code above are a slight modification of
% that suggested by in the endfloat docs (section 8.4.1) to ensure that
% the full captions always appear in the list of figures/tables - even if
% the user used the short optional argument of \caption[]{}.
% IEEE papers do not typically make use of \caption[]'s optional argument,
% so this should not be an issue. A similar trick can be used to disable
% captions of packages such as subfig.sty that lack options to turn off
% the subcaptions:
% For subfig.sty:
% \let\MYorigsubfloat\subfloat
% \renewcommand{\subfloat}[2][\relax]{\MYorigsubfloat[]{#2}}
% However, the above trick will not work if both optional arguments of
% the \subfloat command are used. Furthermore, there needs to be a
% description of each subfigure *somewhere* and endfloat does not add
% subfigure captions to its list of figures. Thus, the best approach is to
% avoid the use of subfigure captions (many IEEE journals avoid them anyway)
% and instead reference/explain all the subfigures within the main caption.
% The latest version of endfloat.sty and its documentation can obtained at:
% http://www.ctan.org/pkg/endfloat
%
% The IEEEtran \ifCLASSOPTIONcaptionsoff conditional can also be used
% later in the document, say, to conditionally put the References on a 
% page by themselves.




% *** PDF, URL AND HYPERLINK PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/url
% Basically, \url{my_url_here}.
\usepackage{mathtools}
\usepackage{blindtext}
\usepackage{enumitem}
\usepackage{bbm, dsfont}
\usepackage{color}
\usepackage{xcolor}
\usepackage{tcolorbox}
\usepackage{balance}
\usepackage{amsthm}
\usepackage{nicematrix,tikz}
\usepackage{bm}
%\let\proof\relax
%\let\endproof\relax
\usepackage[utf8]{inputenc}
\usepackage{booktabs,caption}
\usepackage[flushleft]{threeparttable}
\usepackage[english]{babel}
%\usepackage[dvipsnames]{xcolor}
\usepackage{amssymb}
\usepackage[utf8]{inputenc} 
\usepackage[T1]{fontenc}
\usepackage{dblfloatfix}
\usepackage{url}
\usepackage{ifthen}
\usepackage{cite}
%\usepackage[dvips]{graphics}
\usepackage{graphicx}
\usepackage{psfrag}
\usepackage{blkarray}
\usepackage{amsmath}
%\usepackage[cmex10]{amsmath} % Use the [cmex10] option to ensure complicance
% with IEEE Xplore (see bare_conf.tex)
% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )

\renewcommand\qedsymbol{$\blacksquare$}
\newcommand\scalemath[2]{\scalebox{#1}{\mbox{\ensuremath{\displaystyle #2}}}}
% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}
\captionsetup[figure]{name={Fig.},labelsep=period}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}
\def\vv{{\mathsf d}}
\def\vv{{\mathsf v}}
\def\nn{\nonumber}

\def\red{\textcolor{red}}
\def\blue{\textcolor{blue}}
\def\green{\textcolor{green}}

\usepackage{accents}
\newcommand{\dbtilde}[1]{\accentset{\approx}{#1}}

\newcommand{\indsize}{\scriptsize}
\newcommand{\colind}[2]{\displaystyle\smash{\mathop{#1}^{\raisebox{.5\normalbaselineskip}{\indsize #2}}}}
\newcommand{\rowind}[1]{\mbox{\indsize #1}}

% \usepackage[font=small, skip=5pt]{caption} % Reduce caption spacing
% \setlength{\textfloatsep}{5pt} % Reduce space above/below floats
% \setlength{\floatsep}{5pt} % Reduce space between floats
% \setlength{\intextsep}{5pt} % Reduce space for inline floats
\setlength{\textfloatsep}{5pt} % Adjust the value as needed
\setlength{\parskip}{0pt} % No extra space between paragraphs
% \setlength{\parindent}{15pt} % Standard paragraph indentation

\setlength{\abovedisplayskip}{4pt} % Space before equations
\setlength{\belowdisplayskip}{4pt} % Space after equations
\setlength{\abovedisplayshortskip}{2pt} % Space before short equations
\setlength{\belowdisplayshortskip}{2pt} % Space after short equations
% Adjust spacing before and after subsections
%\usepackage{titlesec} % Allows customization of section spacing
%\titlespacing{\subsection}{0pt}{*0.5}{*0.2} % {left}{before}{after}


\allowdisplaybreaks
\begin{document}
%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
\title{
Capacity of the Binary Energy Harvesting Channel
 \thanks{
 The material in this paper was presented in part at the
 56th Annual Allerton Conference on Communication etc., Monticello, IL, USA, October 2018, and at the IEEE International Symposium on Information Theory, Los Angeles, CA, USA, June 2020.}
} 
\author{
    \IEEEauthorblockN{Eli Shemuel\IEEEauthorrefmark{1}, Oron Sabag\IEEEauthorrefmark{2}, Haim Permuter\IEEEauthorrefmark{1}}\\
    \IEEEauthorblockA{\IEEEauthorrefmark{1}Ben-Gurion University} 
    \IEEEauthorblockA{\IEEEauthorrefmark{2}The Hebrew University of Jerusalem}
}
% \author{%
%   \IEEEauthorblockN{Eli Shemuel}
%   \IEEEauthorblockA{Ben-Gurion University                  }
%   \and
%   \IEEEauthorblockN{Oron Sabag}
%   \IEEEauthorblockA{The Hebrew University of Jerusalem}
              
%   \and                
%   \IEEEauthorblockN{Haim Permuter}
%   \IEEEauthorblockA{Ben-Gurion University
%                     }
% }

% \author{%
%   \IEEEauthorblockN{Eli Shemuel}
%   \IEEEauthorblockA{Ben-Gurion University\\
%                     els@post.bgu.ac.il}
%   \and
%   \IEEEauthorblockN{Oron Sabag}
%   \IEEEauthorblockA{The Hebrew University of Jerusalem\\                    oron.sabag@mail.huji.ac.il}
              
%   \and                
%   \IEEEauthorblockN{Haim Permuter}
%   \IEEEauthorblockA{Ben-Gurion University\\
%                     haimp@bgu.ac.il}
% }
%\author{Eli Shemuel, Oron Sabag, Haim H. Permuter}
%
%
% author names and IEEE memberships
% note positions of commas and nonbreaking spaces ( ~ ) LaTeX will not break
% a structure at a ~ so this keeps an author's name from being broken across
% two lines.
% use \thanks{} to gain access to the first footnote area
% a separate \thanks must be used for each paragraph as LaTeX2e's \thanks
% was not built to handle multiple paragraphs
%
%\author{Eli~Shemuel, Oron~Sabag, Haim~H.~Permuter}
%{Eli~Shemuel,~\IEEEmembership{Student Member,~IEEE,}
        %,~\IEEEmembership{Senior Member,~IEEE}% <-this % stops a space
%\thanks{A part of this work was presented in Allerton 2018 \cite{shemuel2018finite}. Manuscript received **April 19, 2005; revised **August 26, 2015.}

% note the % following the last \IEEEmembership and also \thanks - 
% these prevent an unwanted space from occurring between the last author name
% and the end of the author line. i.e., if you had this:
% 
% \author{....lastname \thanks{...} \thanks{...} }
%                     ^------------^------------^----Do not want these spaces!
%
% a space would be appended to the last name and could cause every name on that
% line to be shifted left slightly. This is one of those "LaTeX things". For
% instance, "\textbf{A} \textbf{B}" will typeset as "A B" not "AB". To get
% "AB" then you have to do: "\textbf{A}\textbf{B}"
% \thanks is no different in this regard, so shield the last } of each \thanks
% that ends a line with a % and do not let a space in before the next \thanks.
% Spaces after \IEEEmembership other than the last one are OK (and needed) as
% you are supposed to have spaces between the names. For what it is worth,
% this is a minor point as most people would not even notice if the said evil
% space somehow managed to creep in.



% The paper headers
%\markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2015}%
%{Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for IEEE Journals}
% The only time the second header will appear is for the odd numbered pages
% after the title page when using the twoside option.
% 
% *** Note that you probably will NOT want to include the author's ***
% *** name in the headers of peer review papers.                   ***
% You can use \ifCLASSOPTIONpeerreview for conditional compilation here if
% you desire.




% If you want to put a publisher's ID mark on the page you can do it like
% this:
%\IEEEpubid{0000--0000/00\$00.00~\copyright~2015 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.

% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}

% make the title area
\maketitle
% As a general rule, do not put math, special symbols or citations
% in the abstract or keywords.

%\begin{abstract}
%\end{abstract}

% Note that keywords are not normally used for peerreview papers.
% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle

\newtheorem{question}{Question}
\newtheorem{claim}{Claim}
\newtheorem{guess}{Conjecture}
\newtheorem{definition}{Definition}
\newtheorem{fact}{Fact}
\newtheorem{assumption}{Assumption}
\newtheorem{theorem}{Theorem}
%\newenvironment{proof}{\textit{Proof}:}{\hfill\qedsymbol}
\newtheorem{lemma}{Lemma}
\newtheorem{ctheorem}{Corrected Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{proposition}{Proposition}
\newtheorem{remark}{Remark}
\newtheorem{example}{Example}

\def\cS{{\mathcal S}}
\def\cX{{\mathcal X}}
\def\cU{{\mathcal U}}
\def\cQ{{\mathcal Q}}
\def\cY{{\mathcal Y}}


% The very first letter is a 2 line initial drop letter followed
% by the rest of the first word in caps.
% 
% form to use if the first word consists of a single letter:
% \IEEEPARstart{A}{demo} file is ....
% 
% form to use if you need the single drop letter followed by
% normal text (unknown if ever used by the IEEE):
% \IEEEPARstart{A}{}demo file is ....
% 
% Some journals put the first two words in caps:
% \IEEEPARstart{T}{his demo} file is ....
% 
% Here we have the typical use of a "T" for an initial drop letter
% and "HIS" in caps to complete the first word.


% you can choose not to have a title for an appendix
% if you want by leaving the argument blank

% use section* for acknowledgment

%\bibliographystyle{IEEEtran}
%\bibliography{IEEEabrv,ref}

% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://mirror.ctan.org/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
%\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)

% biography section
% 
% If you have an EPS/PDF photo (graphicx package needed) extra braces are
% needed around the contents of the optional argument to biography to prevent
% the LaTeX parser from getting confused when it sees the complicated
% \includegraphics command within an optional argument. (You could create
% your own custom macro containing the \includegraphics command to make things
% simpler here.)
%\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{mshell}}]{Michael Shell}
% or if you just want to reserve a space for a photo:

% if you will not have a photo at all:

% insert where needed to balance the two columns on the last page with
% biographies
%\newpage

% You can push biographies down or up by placing
% a \vfill before or after them. The appropriate
% use of \vfill depends on what kind of text is
% on the last page and whether or not the columns
% are being equalized.
\begin{abstract}
The capacity of a channel with an energy-harvesting (EH) encoder and a finite battery remains an open problem, even in the noiseless case. %A key instance of this setting is the binary EH channel (BEHC), 
A key instance of this scenario is the binary EH channel (BEHC), where the encoder has a unit-sized battery and binary inputs. 
%Special attention has been devoted to a key instance of this scenario with a unit-sized battery and 
%binary channel inputs, commonly referred to as . 
Existing capacity expressions for the BEHC are not computable, motivating this work, which determines the capacity to any desired precision via convex optimization. By modeling the system as a finite-state channel with state information known causally at the encoder, we derive single-letter lower and upper bounds using auxiliary directed graphs, termed $Q$-graphs. These $Q$-graphs exhibit a special structure with a finite number of nodes, $N$, enabling the formulation of the bounds as convex optimization problems. As $N$ increases, the bounds tighten and converge to the capacity with a vanishing gap of $O(N)$. For any EH probability parameter $\eta\in \{0.1,0.2, \dots, 0.9\}$, we compute the capacity with a precision of ${1e-6}$, outperforming the best-known bounds in the literature. Finally, we extend this framework to noisy EH channels with feedback, and present numerical achievable rates for the binary symmetric channel using a Markov decision process.
%An MDP formulation enables one to numerically compute and analytically derive achievable rates. We also derive a single-letter lower bound on the capacity, 
%For the well-known binary energy-harvesting channel with a unit-sized battery, we identify a structure of $Q$-graphs, the achievable rates of which are shown to outperform the best achievable rates known in the literature. 
\end{abstract}
\begin{IEEEkeywords}
channel capacity, channels with feedback, convex optimization, energy harvesting, finite-state channel, $Q$-graphs.
\end{IEEEkeywords}
%\vfill

% Can be used to pull up biographies so that the bottom of the last one
% is flush with the other column.
%\enlargethispage{-5in}
\section{Introduction}
%A well-known problem in which the state is input-dependent is
%with a finite battery 
Energy-harvesting (EH) communication models \cite{OptimalEnergyManagement2010,ozel2012optimal,UlkusYang12_energy_harvesting_comm12,Uluku12_Energy_harvesting_BC,AWGNchannelUnderTimeVarying2011,AchievingAWGNCapacity2012,tutuncuoglu2012optimum,mao2017CapAn,jog2014energy,Dong2015NearOptimal,Shaviv2016CapEHCfiniteBattery,tutuncuoglu2013binary,tutuncuoglu2014improved,tutuncuoglu2017binary} have gained significant attention due to their critical role in enabling sustainable and autonomous wireless communication systems. These models find applications in a wide range of emerging technologies, where transmitters harvest energy from the environment either for immediate use or to be stored in a battery for future transmissions, such as low-power wireless sensor networks \cite{visser2013rf,ma2017experimental} and modern unmanned aerial vehicle communications \cite{10452297_2024,EnergyEfficientUAV2020}.
%are motivated by many emerging wireless systems, such as low-power wireless sensor networks \cite{hasan2024energy,valenta2013rf,ma2017experimental,chen2024research,mishra2024synergistic,nguyen2020energy}, 

Typically, EH models describe communication channels where the encoder is powered by a battery that is charged by an exogenous energy source according to an energy arrival process, as depicted in Fig.~\ref{fig:energyharvesting} (with or without output feedback). The channel inputs are constrained by the remaining energy stored in the battery. These unprecedented constraints differ fundamentally from conventional channels with average or peak power constraints, making the capacity analysis of EH models particularly difficult. The capacity has only been determined 
in extreme cases, such as when no battery is present \cite{AWGNchannelUnderTimeVarying2011} or when the battery is infinitely large \cite{AchievingAWGNCapacity2012}. However, for the intermediate scenarios of any finite-sized, non-zero battery, introduced in \cite{tutuncuoglu2012optimum} and further studied in \cite{mao2017CapAn,jog2014energy,Dong2015NearOptimal,Shaviv2016CapEHCfiniteBattery, tutuncuoglu2013binary,tutuncuoglu2014improved,tutuncuoglu2017binary}, the capacity remains an open problem.
%\cite{Uluku12_Energy_harvesting_BC} cite it!!!
% When the
% battery is unlimited, or zero, the capacity has been determined,
% but it remains unknown for a finite non-zero battery. In this
% paper we assume that the harvested energy at each time, the
% total battery storage, and the transmitter signal energy at each
% time can be quantized to the same unit (i.e., the same energy
% interval)
% When the
% battery is unlimited, or zero, the capacity has been determined,
% but it remains unknown for a finite non-zero battery.
% In this paper, we assume that the energy harvested at each
% time, the total battery storage, and the transmitter signal energy
% at each time can be quantized to the same unit (i.e., the
% same energy interval). Under this assumption, we show that
% the capacity can be described using the Verdu-Han general ´
% framework [7].
% \cite{AchievingAWGNCapacity} studies the capacity of channels with energy harvesting transmitters with an infinite-sized battery. It shows that the capacity of an
% AWGN with an infinite-sized battery at the energy harvesting
% encoder is equal to the capacity with an average power
% constraint equal to the average recharge rate. This reference
% proposes save-and-transmit and best-effort-transmit schemes,
% both of which are capacity achieving when the battery size
% is unbounded.
% The exact capacity of the EH model has remained an open problem for any finite, non-zero size of the battery, 
\begin{figure}[t]
 \begin{center}
 \begin{psfrags}
     \psfragscanon
     \psfrag{M}[][][1]{$M$}
     \psfrag{N}[][][1]{$\hat{M}$}
     \psfrag{E}[][][1]{$E_i$}
     \psfrag{B}[][][1]{Battery\hspace{-0.7cm}}
     \psfrag{T}[][][0.98]{Encoder}
     \psfrag{C}[][][1]{Channel}
     \psfrag{R}[][][1]{Decoder}
     \psfrag{X}[][][1]{$X_i$}
     \psfrag{Y}[][][1]{$Y_i$}
      \psfrag{F}[][][1]{$Y_{i-1}$\hspace{-0.7cm}}
   %  \hspace{2cm}
   % State with memory\\
   % \hspace{2cm}
   % e.g. $S^{i-1}$
     %{\\}
 	 %\psfrag{tag}[][][<scale>]{Latex Text}
 \includegraphics[scale=0.38]{Figures/EH_FB.eps}
 %\includegraphics[scale=0.8]{FIG_causal_FSC.eps}
 \caption{The energy-harvesting model with a finite battery.} \label{fig:energyharvesting}
 \psfragscanoff
 \end{psfrags}
 \end{center}
 \end{figure}

The capacity of a finite battery EH model with an AWGN channel is approximated within a constant gap in \cite{Dong2015NearOptimal,Shaviv2016CapEHCfiniteBattery}.
In \cite{jog2014energy}, this scenario is studied with a deterministic energy arrival process, and computable bounds on the capacity are obtained by calculating
the volume of feasible input vectors. Multi-letter capacity expressions are provided in \cite{mao2017CapAn} for a discrete memoryless channel (DMC) using the Verdú-Han framework \cite{Verdu94}, and in \cite{Shaviv2016CapEHCfiniteBattery} for any memoryless channel. However, these capacity expressions are difficult to evaluate due to the optimization over an infinite number of Shannon strategies \cite{shannon1958channels}. 
Recognizing the complexity of the finite-battery EH model's capacity problem, \cite{tutuncuoglu2013binary} introduces a simplified binary EH model, inspired by \cite{Popovski}, called the \textit{binary EH channel} (BEHC), to make progress toward deriving a computable capacity expression.

The BEHC, as its name suggests, operates with binary alphabets. After each channel use, energy is harvested in binary amounts -- either $0$ or $1$ unit. The battery is binary, meaning it is unit-sized and can be either empty or full, and the channel inputs are binary as well. Transmitting a $1$ requires one energy unit per channel use, whereas transmitting a $0$ consumes no energy. Thus, although the encoder can transmit a $0$ at any time, transmitting a $1$ is only possible when the battery is charged. Viewing the battery state as the channel state naturally defines the BEHC as a channel with state information (SI) that is available causally to the encoder but not to the decoder. However, the capacity cannot be achieved using the Shannon strategy \cite{shannon1958channels}, as the state process is not independent and identically distributed (i.i.d.) over time. Instead, the state evolution is affected by the full history of the channel inputs,
%the inputs n all past channel inputs up to the last transmitted $1$ (this will be clarified later in the paper),
which may exhibit infinite memory. The potential for infinitely large memory in the SI, which is unknown to the decoder, makes the capacity problem challenging even in the noiseless case. This complexity arises because the uncertainty that causes communication errors stems from the state evolution, which incorporates the random EH process and imposes intricate constraints on the inputs. For instance, when the decoder receives a noiseless output 
$0$, it cannot determine whether the encoder intended to transmit a $0$ or was constrained to do so due to an empty battery.
 
The problem of the BEHC with an i.i.d. Bernoulli($\eta$) energy arrival process and noiseless channel is studied in \cite{tutuncuoglu2013binary,tutuncuoglu2014improved,tutuncuoglu2017binary}.
Throughout this paper, and as in these works, the BEHC refers to the noiseless channel scenario unless otherwise stated. They demonstrate that the BEHC is equivalent to a timing channel, analogous to the telephone signaling channel in \cite{anantharam1996bits,EntropyTimingCap,ITcap_queues}, by modeling the time differences between consecutive $1$s transmitted through the channel. In the equivalent representation, a single-letter expression for the capacity of the BEHC is derived. 
However, evaluating this expression remains difficult because it involves an auxiliary random variable (RV) with infinite cardinality, and its capacity-achieving distribution is unknown. Based on specific choices of distributions, analogous to lattice coding for the timing channel, \cite{tutuncuoglu2017binary} provides numerical achievable rates. Additionally, two upper bounds are proposed: one using a genie-aided method and the other quantifying the leakage of SI to the decoder \cite{TavanBitsThrBuf}. Numerical results are evaluated for various EH probability parameters $\eta\in \{0.1,0.2, \dots, 0.9\}$, showing a small gap between the lower and upper bounds. This gap is larger for small $\eta$ values and decreases as $\eta$ increases.
%Asymptotically, as the probability of the process goes to zero, it was shown that their bounds were tight.
%We will see that, even in this simple model, unavailability of the battery state to the receiver, memory of the state in time, and the fact that the state evolves based on the previous channel inputs, render the problem challenging
%%%%%%
%of both \cite{mao2017CapAn} and \cite{tutuncuoglu2017binary}
%are difficult to evaluate due to optimizing auxiliary random variable (RVs) with unbounded cardinality. 
%challenging due to the memory in the state, the lack of battery state information at the receiver, and the inter-dependence of the battery state and the channel inputs
%%%%%%%%%%%
%\cite{Dong2015NearOptimal} finds approximations to the capacity of the energy harvesting channel within a constant gap of $2.58$ bits/channel use, with more general results presented in \cite{Shaviv2016CapEHCfiniteBattery}
%%%%%%%%%%%
%For a deterministic energy harvesting profile, \cite{jog2014energy} provides a lower bound on the capacity
% 
% In this paper, we consider a special case of the BEHC
% with no channel noise. Even in this special case, finding
% the capacity is challenging due to the memory in the state,
% the lack of battery state information at the receiver, and the
% inter-dependence of the battery state and the channel inputs.
%Reference [2] views harvested energy as a causally known state, and combines the results of Shannon on channels with causal state at the transmitter \cite{shannon1958channels}
%References \cite{Dong2015NearOptimal,jog2014energy,Shaviv2016CapEHCfiniteBattery} consider an additive white Gaussian noise (AWGN) channel and obtain bounds on the capacity. References \cite{mao2017CapAn,tutuncuoglu2013binary,tutuncuoglu2014improved,tutuncuoglu2017binary} treat a DMC.
%Mao and Hassibi \cite{mao2017CapAn} derived a multi-letter expression for the capacity that is hard to compute. Such a DMC with an input constraint can be modeled as an equivalent FSC without an input constraint. Additionally, viewing the energy remained in the battery as a channel state known causally at the encoder makes the EH model with feedback a special case of our setting.

In this work, we also consider the BEHC and provide a solution for practically computing the capacity with any desired precision using convex optimization. First, recall that the remaining energy in the battery serves as the channel state, which is causally known at the encoder and has memory. Consequently, we view the BEHC as a finite-state channel (FSC), where the memory is encapsulated in the channel state. The BEHC is a special case of FSCs with feedback and SI causally available at the encoder, as introduced in \cite{shemuel2024finite} and depicted in Fig.~\ref{fig:setting}.
Since the channel is noiseless, feedback does not increase the capacity, allowing us to utilize known results from \cite{shemuel2024finite}. They derive the feedback capacity of the general setting as the directed information between auxiliary RVs with
memory to the channel outputs, and provide computable lower bounds via the $Q$-graph method \cite{Sabag_UB_IT} and a Markov decision process (MDP) formulation. 

Here, we construct a single-letter lower bound for the BEHC based on a specific $Q$-graph with $N$ nodes, which has a special structure given $N$. Although a $Q$-graph upper bound is not derived for the general setting in \cite{shemuel2024finite} due to the lack of cardinality bound on the auxiliary RV, we successfully derive a computable single-letter $Q$-graph upper bound for the BEHC. Similar to the lower bound, the upper bound is established on a specific $N$-node $Q$-graph with a special structure. Furthermore, we formulate both the $Q$-graph lower and upper bounds as convex optimization problems, making them computable via convex optimization algorithms. We show that the sequences of these bounds coincide and converge to the capacity of the BEHC as $N$ approaches infinity, with the gap between them scales as $O(N)$. This enables us to compute the capacity with any desired precision. For any EH parameter $\eta\in \{0.1,0.2, \dots, 0.9\}$, we compute the capacity with a precision of ${1e-6}$ and compare it to the tightest numerical bounds known in the literature from \cite{tutuncuoglu2017binary}. Our results outperform the previous bounds across all $\eta$ values, with particularly significant improvements for smaller $\eta$. 

\begin{figure}[t]
\begin{center}
\begin{psfrags}
    \psfragscanon
    \psfrag{E}[][][1]{$M$}
    \psfrag{S}[][][1]{\begin{tabular}{@{}l@{}}
    $\; S^{i-1}$
    \end{tabular}}
    %{\\}
    \psfrag{A}[\hspace{2cm}][][1]{Encoder}
	 \psfrag{F}[\hspace{1cm}][][1]{$X_i$}
	 \psfrag{B}[\hspace{2cm}]{{$P(s_i,y_i|x_i,s_{i-1})$}}
	 \psfrag{G}[][][1]{$Y_i$}
	 \psfrag{C}[\hspace{2cm}][][1]{Decoder}
	 \psfrag{K}[][][1]{$\hat{M}$}
	 \psfrag{H}[\hspace{2cm}][][1]{$Y_i$}
	 \psfrag{D}[\hspace{2cm}][][0.95]{Unit-Delay}
	 \psfrag{J}[\vspace{2cm}\hspace{2cm}][][1]{$Y_{i-1}$}
	 \psfrag{L}[\hspace{2cm}][][1]{Finite-State Channel}
	 \psfrag{I}[][][1]{}
	 %\psfrag{tag}[][][<scale>]{Latex Text}
\includegraphics[scale=0.63]
{Figures/fsc_s_enc_fb.eps}
%\includegraphics[scale=0.8]{FIG_causal_FSC.eps}
\caption{
FSC with feedback and SI available causally to the encoder. At time $i$, the current state~$S_{i-1}$ influences output~$Y_i$.}
\label{fig:setting}
\psfragscanoff
\end{psfrags}
\end{center}
\end{figure}

Additionally, we extend the BEHC framework to noisy channels with feedback and demonstrate that the MDP formulation in \cite{shemuel2024finite} can be applied to any DMC with feedback. We apply the value iteration algorithm (VIA) to numerically evaluate achievable rates for the BEHC over a binary symmetric channel (BSC). To the best of our knowledge, no computable bounds on the feedback capacity have previously been derived in the literature for an EH model with a noisy channel.

The rest of the paper is structured as follows:
Section~\ref{sec:Com_Setup} defines the communication setup. Section~\ref{sec:MainResults} presents the main results. Section~\ref{sec:EHconvex} describes the convex optimization approach used to calculate the lower and upper bounds on the capacity. Section~\ref{sec:proof_main_theorems} provides proofs of the main results. Section~\ref{sec:Noisy} presents numerical achievable rates for the BEHC with a BSC. Finally, Section~\ref{sec:conclusions} concludes the paper.
% Section ???\ref{sec:EH} presents achievable rates for the EH model using the DP and the Q$Q$-graph methods.
%and how using the Q$Q$-graph method leads to outperforming the achievable rates known in the literature.
% if you will not have a photo at all:
% Section~???\ref{sec:problem_def} defines the notation used in this paper and the setting, and provides preliminaries on DP and Q$Q$-graphs. Section~\ref{sec:main_results} presents the main results. Section~???\ref{sec:dpQgraph} focuses on the DP and Q$Q$-graph methods. In Section~???\ref{sec:examples}, we provide several FSCs and study their feedback capacity. Section~???\ref{sec:proof_cap} proves our main result.
% %of unifilar FSCs are studied and the capacity of the probabilistic symmetric POST channel is derived.
% %. Section~???\ref{sec:qgraph} focuses on the Q$Q$-graph lower bound on the capacity.
%  %by using either the DP or the Q$Q$-graph tools.
% Finally, Section~???\ref{sec:conclusions} concludes this work.
\section{Preliminaries and the Channel Model}
\label{sec:Com_Setup}

\subsection{Notation and Definitions}
Lowercase letters denote sample values (e.g., $x,y$), and uppercase letters denote discrete RVs (e.g., $X,Y$). Subscripts and superscripts denote vectors as follows: $x_i^j=(x_i,x_{i+1},\dots,x_j)$ and $X_i^j=(X_i,X_{i+1},\dots,X_j)$ for $1\leq i \leq j$, whereas $x^n\triangleq x_1^n$ and $X^n\triangleq X_1^n$. The notation $\mathbf{0}^N$ denotes a vector of length \( N \) with all entries equal to $0$.
Calligraphic symbols (e.g., $\cX,\cY$) represent alphabets, with $|\cX|$ indicating the alphabet's cardinality. For two RVs $X,Y$, the probability mass function (PMF) of $X$ is expressed as $P(X=x)$, while the conditional PMF given $Y=y$ is written as $P(X=x|Y=y)$, and their joint PMF is denoted by $P(X=x,Y=y)$. For these PMFs, the shorthands $P(x), P(x|y)$ and $P(x,y)$ are used. The indicator function $\mathbbm{1}(b)$, given a binary condition $b$, equals $1$ if $b$ is true and $0$ otherwise.
% \begin{align}
%     \mathbbm{1}(b)\triangleq\begin{cases}
%                     1, & \text{if } {b} \nn\\
%             0, & \text{otherwise}	  
%     \end{cases} \nn.
% \end{align}
Given two integers $n$ and $m$ such that $n \le m$, the discrete interval $[n:m]$ is defined as $\triangleq \{n,n+1,\dots,m\}$. 
For any $\alpha \in [0,1]$, we define $\bar{\alpha}=1-\alpha$.
%$\emptyset$ denotes a "don't care" bit, i.e., either of $\{0,1\}$.

Logarithms are in base $2$; hence, entropy is measured in bits. The binary entropy function is defined by $H_2(p)\triangleq -p \log p -\bar{p} \log \bar{p}, \; p\in [0,1]$.
The \textit{directed information} between $X^N$ and $Y^N$
%,introduced by Massey \cite{massey1990causality},
is defined as $I(X^n\rightarrow Y^n)\triangleq \sum_{i=1}^{n} I(X^i;Y_i|Y^{i-1})$.
% \begin{equation}
% I(X^n\rightarrow Y^n|S)\triangleq \sum_{i=1}^{n} I(X^i;Y_i|Y^{i-1},S).
% \end{equation}
The \textit{causal conditioning distribution} %(see \cite{PermuterWeissmanGoldsmith09})
%introduced in \cite{Kramer03,PermuterWeissmanGoldsmith09},
is defined as $P(x^n||y^{n-1})\triangleq \prod_{i=1}^n P(x_i|x^{i-1},y^{i-1})$. 
% \begin{equation}
    % P(x^n||y^{n-1},s_0)\triangleq \prod_{i=1}^n P(x_i|x^{i-1},y^{i-1},s_0). 
% \end{equation}

A \textit{FSC} is defined by finite alphabets for its input ($\cX$), output ($\cY$), and state ($\cS$) variables. At each time $i$, the corresponding symbols are $X_i,Y_i$ and $S_{i-1}$, and the channel, being time-invariant, is governed by $P_{S^+,Y|X,S}$, satisfying 
\begin{equation}
P(s_i,y_i|x^i,s_0^{i-1},y^{i-1}) = P_{S^+,Y|X,S}(s_i,y_i|x_i,s_{i-1}).
\label{eq:BasicFSCMarkov}
\end{equation}
%Throughout this paper, we assume that the initial state $s_0$ is available both to the encoder and to the decoder. 

\begin{definition}[Connectivity]\label{def:connectivity} \cite[Def.~2]{Permuter06_trapdoor_submit}
A FSC is called \textit{strongly connected} if for all $s',s \in \cS$ there exists an integer $T(s)$ and input distribution of the form $\{P(x_i|s_{i-1})\}_{i=1}^{T(s)}$ that may depend on $s$, such that $\sum_{i=1}^{T(s)} P(S_i=s|S_0=s')>0$.
\end{definition}

\vspace{-0.3cm}
\subsection{Preliminary on Quantized-graphs}
\label{subsection:prelQgraph}
The $Q$-graph is a method for deriving single-letter lower and upper bounds on the capacity of a setting with feedback from a multi-letter expression. This technique is established on \textit{Quantized-graphs} ($Q$-graphs), which are directed, connected graphs with a finite number of nodes $|\cQ|$. Each node represents a unique value $q \in \cQ$ and is associated with exactly $|\cY|$ outgoing edges, each labeled with a distinct symbol from $\cY$. By definition, starting from an initial node, $q_0$, and given an output sequence, $y^i$, traversing the corresponding labeled edges uniquely determines a final node, $q_i$. This transition process is governed by a time-invariant function, $g:\cQ\times\cY\to\cQ$, where the next node is determined by the current node and the channel output.
Recursively, the final node after $i$ steps is given by the function composition
$g^i(q_0,y^i) = g(g^{i-1}(q_0,y^{i-1}),y_i)$. A visual instance of a $Q$-graph is illustrated in Fig.~\ref{fig:q_graph_ex}.
%For any $q,q^+ \in \cQ$ and $y \in Y$, there exists an edge $q \to q^+$ with label $y$ if $q^+=g(q,y)$.
\begin{figure}[t]
\centering
    \psfrag{Q}[][][1]{$Y=0$}
    \psfrag{E}[][][1]{$Y=1$}
    \psfrag{F}[][][1]{$Y=2$}
    \psfrag{O}[][][1]{$Y=0/1/2$}
    \psfrag{L}[][][1]{$Q=1$}
    \psfrag{H}[][][1]{$Q=0$}
    \includegraphics[scale = 0.5]{Figures/QgraphExample.eps}
    \caption{An instance of a $2$-node $Q$-graph with ternary output.}
    \label{fig:q_graph_ex}
\end{figure}

\vspace{-0.15cm}
\subsection{The Energy-Harvesting Communication Model}
%are interested in finding the capacity of the noiseless EH channel with a unit-sized battery.
%**Viewing the current battery state at the encoder as the state naturally makes the model a special problem of the general setting depicted in Fig.~\ref{fig:setting}.
We consider the EH model depicted in Fig.~\ref{fig:energyharvesting} with a unit-sized battery, i.e., it can store at most $B_{max}=1$ energy unit. At time $i$,
the current battery state is represented by
$S_{i-1}\in \{0,1\}$, which is causally known to the encoder but remains unknown to the decoder, and $X_i\in\{0,1\}$ denotes the transmitted input symbol during channel use $i$. The encoder is always capable of transmitting $X_i=0$ regardless of the battery state. However, sending $X_i=1$ is only feasible when the battery is charged ($S_{i-1}=1$). Thus, if the battery is empty ($S_{i-1}=0$), the input is constrained to $X_i=0$. After each transmission, the encoder harvests an energy unit according to an i.i.d. arrival process $E_i\sim$\text{Bern}($\eta$), where $\eta\in [0,1]$ denotes the success probability. 
If $S_{i-1}=0$, a successfully harvested energy unit charges the battery to $S_i=1$, enabling a future transmission of $X=1$. However, if $S_{i-1}=1$, the harvested energy is lost as it cannot be used without being stored.\footnote{This scenario is referred to as the \textit{transmit first model} in \cite{tutuncuoglu2017binary}, as the encoder first transmits $X_i$ and then harvests energy $E_i$, such that the harvested energy is not immediately available for transmission. A \textit{harvest first} model, in which the order is reversed (harvest then transmit), is considered in \cite{mao2017CapAn}.}
For simplicity, and without loss of generality (see \cite[Prop.~1]{Shaviv2016CapEHCfiniteBattery}), we assume that the battery starts in an empty state, i.e., $S_0=0$.
%It is assumed that the battery size, the energy required for a transmission and the harvested energy at each time are all multiples of the same unit of energy.
% In this paper we assume that the harvested energy at each time, the total battery storage, and the transmitter signal energy at each time can be quantized to the same unit (i.e., the same energy interval
%For the binary EH model with a unit-sized battery
%\cite{tutuncuoglu2013binary,tutuncuoglu2014improved,tutuncuoglu2017binary}, %i.e. $B_{\text{max}}=1, E_i \in \{0,1\}$, %all variables, including the battery size, are from the binary alphabet.
The evolution of the battery state is governed by the equation
\begin{equation}
S_{i}=\min\{S_{i-1}-X_i + E_i,1\}, \label{eq:BEHC_state_evolution}
\end{equation}
where $X_i = 0$ if $S_i = 0$ (or equivalently, $X_i\le S_{i-1}$).
%(similarly to \cite[Eq.~(1)]{tutuncuoglu2017binary}).

Our primary focus is on the noiseless channel case, where $Y_i=X_i$ holds for all $i$. This case is referred to as the \textit{noiseless BEHC}, or simply \textit{BEHC}. Additionally, we consider a generalized scenario with noise: a DMC $P(y|x)$ in the presence of feedback, referred to as a \textit{noisy BEHC}.
%for a given time-invariant channel $P(y|x)$ with the Markov property $P(y_i|x^i,s_0^{i-1},y^{i-1},m) = P_{Y|X}(y_i|x_i)$,
%for any $(x^i,s_0^{i-1})$ that satisfy the mentioned energy constraint, i.e., $x_j=0$ if $s_{j-1}=0$ for any $j\in [1:i]$.
%and we call it either \textit{the noiseless binary EH channel} (BEHC) when $Y=X$, or \textit{a noisy BEHC} for a given $P(y|x)$ time-invariant channel with the Markov property $P(y_i|x^i,s_0^{i-1},y^{i-1},m) = P_{Y|X}(y_i|x_i)$,
%\begin{equation}
%P(y_i|x^i,s_0^{i-1},y^{i-1},m) = P_{Y|X}(y_i|x_i),
%\label{eq:BasicMarkov}
%\end{equation}
At time $i$, the encoder observes not only the message $M$ and the current battery state $S_{i-1}$, but also the output feedback $Y_{i-1}$. 
% The encoder's mapping at time $i$ is denoted as 
% $\zeta_i:\mathcal{M} \times \cS_{0}^{i-1} \times \cY^{i-1} \to \cX$,
% while the decoder's mapping, which assigns a message estimate $\hat{m}\in \mathcal{M}$ or an error message e to each received output sequence $y^n$, is given by $\hat{m}: \cY^n \to \mathcal{M} \cup \{\text{e}\}$. A $(2^{nR},n)$ code is defined as a pair of encoding and decoding mappings with a message set $\mathcal{M}=[1:\ceil{2^{nR}}]$, where $M$ is uniformly distributed over $\mathcal{M}$.
The definitions of 
a \textit{$(2^{nR},n)$ code}, the \textit{average probability of error}, \textit{achievable rate} and \textit{capacity} are standard (see \cite{shemuel2024finite}). The capacity of the noiseless BEHC is denoted by $C_{\text{BEHC}}$ throughout this paper.

Determining $C_{\text{BEHC}}$ in a computable form for any harvesting parameter $\eta$ remains an open problem, and resolving it is the primary objective of this work. The challenge arises from the intricate constraints at the encoder due to \eqref{eq:BEHC_state_evolution}, while the decoder remains unaware of the battery state, even when receiving noiseless outputs. Since feedback does not increase the capacity of any noiseless channel model, $C_{\text{BEHC}}$ corresponds to a special case of noisy BEHCs with feedback. A noisy BEHC with feedback can be viewed as a strongly connected FSC $P_{S^+,Y|X,S}$ with feedback and SI available causally at the encoder, as illustrated in Fig.~\ref{fig:setting}, and specified~by
\begin{align}
&P_{S^+,Y|X,S}(s_i,y_i|x_i,s_{i-1}) =P(y_i|x_i)P(s_i|x_i,s_{i-1}) \nonumber\\
&=P(y_i|x_i)\sum\nolimits_{j\in\{0,1\}} P(E_i=j)P(s_i|x_i,s_{i-1},E_i=j) \nonumber\\
&=P(y_i|x_i)\big(
\bar{\eta} \mathbbm{1}\{s_i=s_{i-1}-x_i\} + \eta \mathbbm{1}\{s_i=1\}\big), \label{eq:stateEvolutionEH}
\end{align}
where $x_i\le s_{i-1}$, and in the noiseless channel scenario, the FSC $P_{S^+|X,S}$ is characterized by
\begin{align}
P_{S^+|X,S}(s_i=0|x_i,s_{i-1}) & =\bar{\eta} \mathbbm{1}\{x_i=s_{i-1}\}.
%\bar{\eta} \mathbbm{1}\{s_i=s_{i-1}-x_i\} + \eta \mathbbm{1}\{s_i=1\}.
\label{eq:stateEvolutionNoiselessEH}
\end{align}
%We further aim to demonstrate the computability of achievable rates for noisy BEHCs with feedback.
% We note that for this case the capacity with and without feedback are identical because the output is, in particular, a function of the input, hence it is known to the encoder. As a result, here we may derive new achievable rates and compare them to the tightest bounds on the capacity known in the literature \cite{tutuncuoglu2017binary}, where the setting does not include feedback.
%\begin{equation}
%    S_{i}=\min\{S_{i-1}-X_i + E_i,B_{max}\}.
%\end{equation}
%We assume that the battery is initially empty, i.e. S0 = 0 with probability one. As shown in [6, Proposition 1], the initial battery state does not alter the capacity. Therefore this assumption
The capacity of such general FSC $P_{S^+,Y,X,S}$ is denoted by $C_{\text{fb-csi}}$, and is studied in \cite{shemuel2024finite}.

\vspace{-0.28cm}
\subsection{A Lower Bound on the Feedback Capacity of FSCs With SI Available Causally at the Encoder}
In \cite{shemuel2024finite}, a single-letter lower bound on $C_{\text{fb-csi}}$ is derived using $Q$-graphs, forming the foundation of our main results.
% \begin{theorem}(\cite[Th.~1]{shemuel2024finite}):
% \label{theorem:capDI}
% The feedback capacity of a strongly connected FSC with SI known causally at the encoder is given by $C_{\text{fb-csi}}&=\lim_{n \to \infty} \frac{1}{n}  \max_{P(u^n||y^{n-1})} I(U^n \to Y^n)$, where $\{U_i\}_{i \ge 1}$ are auxiliary RVs with $|\cU|=|\cX|^{|\cS|}$, and the joint distribution is given by 
% \begin{align}
%     \label{eq:joint_distDI}
%     &P(x^n,s_0^n,y^n,u^n)=P_{S_0}(s_0)P(u^n||y^{n-1}) \nn\\ &\times \prod_{i=1}^n\mathbbm{1}\{x_i = f(u_i,s_{i-1})\}P_{S^+,Y|X,S}(s_i,y_i|x_i,s_{i-1})
%     \text{.}
% \end{align}
% Each $u\in \cU$ corresponds to a unique function within the set $\{f_u (s):\cS \to \cX \}$, where each function is given by $f_u(s)\triangleq f(u,s)$.
% \end{theorem}
%%%%%%%%%%%%%%%%%%%%%%%%%% Alternative expression U_i, U_{i-1}
% The feedback capacity $C_{\text{fb-csi}}$ can also be expressed as follows.
% \begin{theorem}(\cite[Th.~2]{shemuel2024finite})
% \label{theorem:capTrunc}
% The feedback capacity of a strongly connected FSC with SI known causally at the encoder is given by
% \begin{align}
% C_{\text{fb-csi}} = &\lim_{n \to \infty} \frac{1}{n}  \max_{\substack{\{P(u_i|u_{i-1},y^{i-1})\}_{i=1}^n\\x_i=f(u_i,s_{i-1})}} \sum_{i=1}^{n} I(U_i,U_{i-1};Y_i|Y^{i-1}),  \label{eq:capTrunc}
% \end{align}
% where $\{U_i\}_{i \ge 1}$ are auxiliary RVs, and the joint distribution is given by 
% \begin{align}
%     \label{eq:joint_distTrunc}
%     &P(x^n,s_0^n,y^n,u^n)=P_{S_0}(s_0)\prod_{i=1}^n
%     P(u_i|u_{i-1},y^{i-1}) \mathbbm{1}\{x_i = f(u_i,s_{i-1})\}P_{S^+,Y|X,S}(y_i,s_i|x_i,s_{i-1}) \text{.}
% \end{align}
% \end{theorem}
Given a $Q$-graph, an auxiliary RV $U$ with a specified cardinality $|\cU|$, a strategy function $f: \mathcal U \times \mathcal S \to \mathcal X$ and an input distribution $P(u^+|u,q)$, the transition matrix $P(s^+,u^+,q^+|s,u,q)$ is defined~as
\begin{align}
\label{eq:suq_transition}
& P(s^+,u^+,q^+|s,u,q) = \sum\nolimits_{x,y} P(u^+|u,q) \mathbbm{1}\{x=f(u^+,s)\} \nn\\
&\quad \times \mathbbm{1}\{ q^+=g(q,y) \}  P_{S^+,Y|X,S}(s^+,y|x,s).
\end{align}
% \begin{align}
% \label{eq:suq_transition}
% & P(s^+,u^+,q^+|s,u,q) = \sum_{y} P(u^+|u,q) \nn\\
% &\quad \times \mathbbm{1}\{ q^+=g(q,y) \}  P_{S^+,Y|X,S}(s^+,y|f(u^+,s),s).
% \end{align}
The set $\mathcal{P}_{\pi}$ denotes all $P(u^+|u,q)$ that induce a transition matrix \eqref{eq:suq_transition} with a unique stationary distribution over $(S,U,Q)$, denoted by $\pi(s,u,q)$. 
%An input distribution $P(u^+|u,q) \in \mathcal{P}_\pi$ is said to be an \textit{aperiodic} if its corresponding $(S,U,Q)$-graph is aperiodic.
An input distribution $P(u^+|u,q)\in \mathcal{P}_\pi$ is said to be \textit{BCJR-invariant} if it satisfies the Markov chain
\begin{align}
 (U^+,S^+)-Q^+-(Q,Y). \label{eq:BCJR_Def}
\end{align}
\begin{theorem}(\!\!\cite[Th.~5]{shemuel2024finite}):
 \label{theorem:qgraph_LB}
For any $Q$-graph, given a fixed finite cardinality $\left|\mathcal{U}\right|$ ($U^+,U \in \cU$) and a function $f: \mathcal U \times \mathcal S \to \mathcal X$, the feedback capacity is lower bounded by
\begin{align}\label{eq:Theorem_Lower}
C_{\text{fb-csi}}\geq I(U^+,U;Y|Q),
\end{align}
for all $P(u^+|u,q)\in\mathcal{P}_\pi$ that are BCJR-invariant \eqref{eq:BCJR_Def}, where the joint distribution is given by
% \begin{align}
% \label{eq:Qgraph_joint_dist}
%   &P(s,u,q,x,s^+,y,u^+,q^+) = \pi(s,u,q) P(u^+|u,q) 
%   \\ &\quad \times \mathbbm{1}\{x = f(u^+,s)\} P_{S^+,Y|X,S}(s^+,y|x,s) \mathbbm{1}\{q^+ = g(q,y)\}. \nn
% \end{align}
\begin{align}
\label{eq:Qgraph_joint_dist}
  P&_{S,U,Q,X,Y,S^+,U^+,Q^+} = \pi_{S,U,Q} P_{U^+|U,Q} \mathbbm{1}\{X = f(U^+,S)\} \nn\\ &\quad \times P_{S^+,Y|X,S} \mathbbm{1}\{Q^+ = g(Q,Y)\}. 
\end{align}
\end{theorem}

For the policy's auxiliary RVs $U,U^+$, we use the notations $U^{(Q=q)},U^{+(U=u,Q=q)}$ throughout the paper to denote them conditioned on $Q=q$ or $(U=u,Q=q)$, respectively, i.e., $U \mid Q=q$ and $U^+ \mid (U=u,Q=q)$.

\section{Main Results}
\label{sec:MainResults}
In this section, we present our main results. 
%Since the EH model with feedback can be viewed as a special case of the general setting in Chapter \ref{chapter:p1} and illustrated in Fig.~\ref{fig:setting}, we can utilize the computation methods for bounding the capacity. In particular,
Using the $Q$-graph method, we derive sequences of single-letter convex lower and upper bounds on $C_{\text{BEHC}}$, which converge to the capacity as their limits coincide. Each term in these sequences is defined using the auxiliary RVs $(U,U^+,Q,Q^+)$, whose cardinality is $N+1$. To emphasize their dependence on $N$, we denote them as $(U^{(N)},U^{+(N)},Q^{(N)},Q^{+(N)})$, though we often omit the superscript $^{(N)}$ for simplicity.

We begin with the following theorem, which establishes the lower bound sequence based on Theorem~\ref{theorem:qgraph_LB}.
% \begin{theorem}[Convex Lower Bound]
% \label{thr:EH_cvx_lower}
% Given an integer $N>0$, the capacity of the BEHC can be lower bounded by a convex optimization problem with the objective function given in the RHS of
% \begin{align}
% C_{\text{BEHC}}\geq I(U^+,U;Y|Q^{(N)}), \label{eq:EH_LB_cvx}
% \end{align}
% where the $Q$-graph is illustrated in Fig.~\ref{fig:EH_LB_Graph}, $|\cQ|=|\cU|= N+1$, $f(u^+,s)$ is fixed, and the optimization variables are a joint distribution $P_{S,U,Q^{(N)},X,S^+,Y,U^+,Q^+}$ distributed as \eqref{eq:Qgraph_joint_dist}. The convex optimization problem serves as a monotonically increasing sequence that converges to $C_{\text{BEHC}}$.
% \end{theorem}
\begin{figure}[t]
\resizebox{\columnwidth}{!}{
\begin{psfrags}
    \psfragscanon
    \psfrag{E}[][][0.8]{$M$}
    \psfrag{A}[\hspace{2cm}][][1]{$Q=0$}
    \psfrag{B}[\hspace{2cm}][][1]{$Q=1$}
    \psfrag{C}[\hspace{2cm}][][1]{$Q=2$}
    \psfrag{G}[\hspace{2cm}][][0.75]{}
    \psfrag{D}[\hspace{2cm}][][0.65]{$Q=N-1$}
    \psfrag{E}[\hspace{2cm}][][1]{$Q=N$}
    \psfrag{F}[\hspace{2cm}][][0.75]{$X=0$}
    \psfrag{M}[\hspace{2cm}][][1]{$...$}
    %\psfrag{L}[\hspace{2cm}][][1]{......\hspace{-0.7cm}}
    \psfrag{H}[][][0.75]{$X=1$\hspace{0.1cm}}
    \psfrag{I}[\hspace{2cm}][][1]{$...$\hspace{0cm}}
    \psfrag{J}[\hspace{2cm}][][0.75]{$X=1$\hspace{-0.5cm}}
    \psfrag{M}[\hspace{2cm}][][0.75]{$X=1$\hspace{-0.30cm}}
    \psfrag{K}[\hspace{2cm}][][0.75]{$X=1$\hspace{0cm}}
    \psfrag{N}[\hspace{2cm}][][0.75]{$X=0$\hspace{0.5cm}}
    \psfrag{L}[\hspace{2cm}][][0.75]{$X=0,1$\hspace{0cm}}
    \includegraphics[scale=0.65]{Figures/eh_LB.eps}
\end{psfrags}
}
\caption{\small A $Q$-graph used for lower bounding $C_{\text{BEHC}}$ in Theorem~\ref{thr:EH_cvx_lower}.}
\label{fig:EH_LB_Graph}
\end{figure}

\begin{theorem}[Convex Lower Bound]
\label{thr:EH_cvx_lower}
For any integer $N\ge0$, the capacity of the BEHC is lower bounded by
\begin{align}
C_{\text{BEHC}}\geq I(U^{+(N)},U^{(N)};X\mid Q^{(N)}), \label{eq:EH_LB_cvx}
\end{align}
where $Q^{(N)}$ is defined on the vertices of the $Q$-graph illustrated in Fig.~\ref{fig:EH_LB_Graph}, with $|\cQ^{(N)}|= N+1$, the auxiliary RV sets are given by
\begin{align}           
 &\mspace{-10mu}\cU^{(Q=q)}\mspace{-5mu}=\mspace{-5mu}[0\mspace{-2mu}:\mspace{-2mu}q], \cU^{+(U=u,Q=q)}\mspace{-5mu}=\mspace{-5mu}\{0,u\mspace{-5mu}+\mspace{-5mu}1\}, \forall q\mspace{-2mu}\in\mspace{-2mu}[0\mspace{-2mu}:\mspace{-2mu}N\mspace{-3mu}-\mspace{-3mu}1], \mspace{-12mu}\label{eq:Uset_initialNodes}\\
 &\mspace{-10mu}\cU^{(Q=N)}=[0:N],  \cU^{+(Q=N)}=\{0\}, \label{eq:Uset_LastNode}    
\end{align}
and the joint distribution $P_{S,U,Q,X,S^+,U^+,Q^+}$ is given by \eqref{eq:Qgraph_joint_dist}, where $Y=X$. The FSC characterization $P_{S^+|X,S}$ is given in \eqref{eq:stateEvolutionNoiselessEH},
and the function $f: \mathcal U \times \mathcal S \to \mathcal X$ is defined as
\begin{align}
     f(U^+,S)&=S \mathbbm{1}\{U^+=0\}.
    % \begin{cases}
    %         1, & \text{if } S=1 \text{ and } U^+=0\\
    %         0, & \text{otherwise}
		  %  \end{cases},
          \label{eq:x_behc}
\end{align}
The induced marginal $\pi_{S|U,Q}$ remains constant for any choice of policy $P_{U^+|U,Q}$, given \eqref{eq:Uset_initialNodes}–\eqref{eq:Uset_LastNode}, and is given by
% $\pi_{S|U,Q}$ given by 
%The special structure of the joint distribution induces the following important relation:
\begin{align}
\label{eq:s_given_uq_lb}
\pi(S=0|u,q)=\bar{\eta}^{u+1}, \quad \forall q\in \cQ, \forall u\in \mathcal{U}^{(Q = q)}.
\end{align}
%The convex optimization problem serves as a monotonically increasing sequence that converges to $C_{\text{BEHC}}$.
Optimizing over $P_{S,U,Q,X,S^+,U^+,Q^+}$ can be formulated as a convex optimization problem, detailed in Eq.~\eqref{optProb_EH_LB}, Section~\ref{subsec:EHconvex_lower}. Moreover, the optimized lower bound in \eqref{eq:EH_LB_cvx} converges to $C_{\text{BEHC}}$ as $N\to \infty$.
%The sequence of the lower bounds parameterized by $N$ is monotone increasing and converges to the capacity.
\end{theorem}

%The characterization of $f(u^+,s)$ and the structure of $P(u^+|u,q)$ are given in the proof of Theorem \ref{thr:EH_cvx_lower} in Section \ref{subsec:EHconvex_lower}.
The proof of Theorem \ref{thr:EH_cvx_lower} is provided in Section~\ref{sec:QgraphLB}. Note that $|\cU^{(Q=q)}|$ increases by $1$ with $q$. In general, $|\cU^{(N)}|=|Q^{(N)}|=N+1$. Furthermore, \eqref{eq:Uset_initialNodes}–\eqref{eq:Uset_LastNode} imply that the policy $P_{U^+|U,Q}$ is subject to
%particular subset of $\mathcal{P}_\pi$:
\begin{align}
P(u^+ \mid u, q) &= 0, \quad \forall q \in [0:N-1], \; \forall u \in \mathcal{U}^{(Q = q)}, \nonumber \\
&\quad \forall u^+ \in \mathcal{U}^{+(Q = q)} \setminus \{0, u+1\}, \label{eq:policy_first_nodes} \\
P_{U^+ \mid U, Q}(0 \mid u, N) &= 1, \quad \forall u \in \mathcal{U}^{(Q = N)}. \label{eq:policy_last_node}
\end{align}
In particular, this policy structure, combined with \eqref{eq:x_behc}, always attempts to transmit $X=1$ at node $Q=N$, with success determined by whether the battery is charged. While the BCJR constraint of the policy in its general form \eqref{eq:BCJR_Def} translates into non-linear equality constraints, Theorem~\ref{thr:EH_cvx_lower} demonstrates that the $Q$-graph lower bound on $C_{\text{BEHC}}$ in \eqref{eq:EH_LB_cvx} can be formulated as a convex optimization problem. 
%Although the BCJR-invariant constrains required for the general $Q$-graph lower bound in Theorem \ref{theorem:qgraph_LB} are generally non-linear constraints, they are innately satisfied in the convex optimization of Theorem \ref{thr:EH_cvx_lower} such that no additional constraint is required for them. The convex optimization problem of Theorem~\ref{thr:EH_cvx_lower} is fully detailed in Section~\ref{subsec:EHconvex_lower}.

We now turn to the upper bound sequence. Although a general single-letter $Q$-graph upper bound on $C_{\text{fb-csi}}$ was not derived in \cite{shemuel2024finite} due to the absence of a general cardinality bound on $\cU$, the following theorem establishes a sequence of such bounds specifically for $C_{\text{BEHC}}$, with a finite $\cU$ and a $Q$-graph modified from Fig.~\ref{fig:EH_LB_Graph} by a single edge.

\begin{figure}[t]
\resizebox{\columnwidth}{!}{
\begin{psfrags}
    \psfragscanon
    \psfrag{E}[][][0.8]{$M$}
    \psfrag{A}[\hspace{2cm}][][1]{$Q=0$}
    \psfrag{B}[\hspace{2cm}][][1]{$Q=1$}
    \psfrag{C}[\hspace{2cm}][][1]{$Q=2$}
    \psfrag{G}[\hspace{2cm}][][0.75]{$X=0$}
    \psfrag{D}[\hspace{2cm}][][0.65]{$Q=N-1$}
    \psfrag{E}[\hspace{2cm}][][1]{$Q=N$}
    \psfrag{F}[\hspace{2cm}][][0.75]{$X=0$}
    \psfrag{M}[\hspace{2cm}][][1]{$...$}
    %\psfrag{L}[\hspace{2cm}][][1]{......\hspace{-0.7cm}}
    \psfrag{H}[][][0.75]{$X=1$\hspace{0.1cm}}
    \psfrag{I}[\hspace{2cm}][][1]{$...$\hspace{0cm}}
    \psfrag{J}[\hspace{2cm}][][0.75]{$X=1$\hspace{-0.5cm}}
    \psfrag{M}[\hspace{2cm}][][0.75]{$X=1$\hspace{-0.30cm}}
    \psfrag{K}[\hspace{2cm}][][0.75]{$X=1$\hspace{0cm}}
    \psfrag{N}[\hspace{2cm}][][0.75]{$X=0$\hspace{0.5cm}}
    \psfrag{L}[\hspace{2cm}][][0.75]{$X=1$\hspace{0cm}}
    \includegraphics[scale=0.65]{Figures/eh_Graph.eps}
\end{psfrags}
}
\caption{\small A $Q$-graph for upper bounding $C_{\text{BEHC}}$ in Theorem~\ref{thr:EH_cvx_upper}.}
\label{fig:EH_UB_Graph}
\end{figure}

\begin{theorem}[Convex Upper Bound]%[$Q$-graph Upper Bound]
\label{thr:EH_cvx_upper}
%\label{thr:EH_Qgraph_ub}
For any integer $N\ge0$, the capacity of the BEHC is upper bounded by 
\begin{align}
C_{\text{BEHC}}\leq \sup_{P(u^+|u,q)\in\mathcal{P}_\pi} I(U^{+(N)},U^{(N)};X \mid Q^{(N)}), \label{eq:EH_UB_cvx}
\end{align}
where $Q^{(N)}$ is defined on the vertices of the $Q$-graph illustrated in Fig.~\ref{fig:EH_UB_Graph}, with $|\cQ^{(N)}|= N+1$, the auxiliary RV sets are given by \eqref{eq:Uset_initialNodes} and
\begin{align}
    &\cU^{(Q=N)}=[0:N], \; \cU^{+(Q=N)}=\{0,1\}, \label{eq:Uset_LastNodeUB}
\end{align}
the joint distribution is given by
\begin{align}
\label{eq:Qgraph_joint_distUB}
  &P_{S,U,Q,X,S^+,U^+,Q^+}= \pi_{S,U,Q} P_{U^+|U,Q} \mathbbm{1}\{X = f(U^+,S)\}  \nn\\
  & \times P_{S^+|X,S,Q} \mathbbm{1}\{Q^+  = g(Q,X)\}, \\    
  &\mspace{-10mu}P_{S^+|X,S,Q}\mspace{-4mu} \triangleq \mspace{-4mu}
  \begin{cases}
     \mathbbm{1}\{S^+=1\}, & \text{if } Q\mspace{-4mu}\in\mspace{-4mu}\{N\mspace{-4mu}-\mspace{-4mu}1,\mspace{-3mu}N\}, \mspace{-4mu}X\mspace{-4mu}=\mspace{-4mu}0\\    
    %\scriptstyle \bar{\eta} \mathbbm{1}\{s^+=s-x\} + \eta \mathbbm{1}\{s^+=1\},
    P_{S^+|X,S} \text{ in \eqref{eq:stateEvolutionNoiselessEH}}
    & \text{otherwise}
   \end{cases}, \label{eq:modifiedChannelLaw}   
\end{align}
and the function $f(U^+,S)$ is defined in \eqref{eq:x_behc}.
The induced marginal $\pi_{S|U,Q}$ is constant for any choice of policy $P_{U^+|U,Q}$, given \eqref{eq:Uset_initialNodes} and \eqref{eq:Uset_LastNodeUB}, and is given by
\begin{align}    
%\end{align}        
%The special structure of the joint distribution induces
%\begin{align}
%\label{eq:batteryRelation1_UB}
\mspace{-15mu} \pi(S=0|u,q)&=
   \begin{cases}
      \bar{\eta}^{u+1},  &\mspace{-10mu} \forall q\in [0:N\mspace{-5mu}-\mspace{-5mu}1], \forall u\in \cU^{(Q=q)}\\
     %\label{eq:batteryRelation2_UB}
      0, &\mspace{-10mu}  q=N, \forall u\in \cU^{(Q=N)}
   \end{cases}. \label{eq:s_given_uq_upper}
\end{align}
Optimizing over the joint distribution $P_{S,U,Q,X,S^+,U^+,Q^+}$ can be formulated as a convex optimization problem, as detailed in Eq.~\eqref{optProb_EH_UB}, Section~\ref{subsec:EHconvex_upper}. Moreover, the upper bound in \eqref{eq:EH_UB_cvx} converges to $C_{\text{BEHC}}$ as $N\to \infty$.
%is determined by the FSC law $P_{S^+,Y|X,S}$ and the $Q$-graph as follows
\end{theorem}

The proof of Theorem~\ref{thr:EH_cvx_upper} is provided in Section~\ref{sec:QgraphUB}.
The key difference between Theorem~\ref{thr:EH_cvx_lower} and Theorem~\ref{thr:EH_cvx_upper} lies in the treatment of the last node $Q=N$. Specifically, in Fig.~\ref{fig:EH_LB_Graph}, the outgoing edge $'X=0'$ from $Q=N$ leads to $Q=0$, whereas in Fig.~\ref{fig:EH_UB_Graph}, it loops back to $Q=N$. Furthermore, \eqref{eq:Uset_LastNodeUB} implies that the policy $P_{U^+|U,Q}$ in Theorem~\ref{thr:EH_cvx_upper} is subject to 
\begin{align}
    P_{U^+|U,Q}(u^+|u,N)=0, \forall u\in \cU^{(Q=N)}, \forall u^+\in [2:N], \label{eq:policy_last_node_UB}
\end{align}
rather than \eqref{eq:policy_last_node} in the lower bound. Unlike the lower bound, the upper bound policy is neither obligated to attempt transmitting $X=1$ in the last node nor satisfies the BCJR property \eqref{eq:BCJR_Def} (particularly at $Q=N$, as can be verified). Furthermore, $P_{S^+|X,S,Q}$ in \eqref{eq:modifiedChannelLaw}, which replaces $P_{S^+|X,S}$ from Theorem~\ref{thr:EH_cvx_lower} in both the joint distribution and the transition matrix \eqref{eq:suq_transition}, differs at the last node $Q^+\mspace{-3mu}=\mspace{-3mu}N\mspace{-3mu}=\mspace{-3mu}g(Q\mspace{-3mu}=\mspace{-3mu}N-1,X\mspace{-3mu}=\mspace{-3mu}0)$, where $S^+\mspace{-3mu}=\mspace{-3mu}1$. This implies that when $Q\mspace{-3mu}=\mspace{-3mu}N$, reached after $N$ consecutive zero inputs, the battery is charged with probability~$1$, as also indicated in Eq.~\eqref{eq:s_given_uq_upper}. Thus, for any integer $N\ge0$, we implicitly consider a scenario whose capacity upper bounds $C_{\text{BEHC}}$, and aids in proving Theorem~\ref{thr:EH_cvx_upper}. 
Notably, an upper bound based on a Markov $Q$-graph of order $N$ could also be established. However, we focus on the $Q$-graph depicted in Fig.~\ref{fig:EH_UB_Graph}, since its number of nodes scales linearly with $N$ rather than exponentially.
% \begin{theorem}[Convex Upper Bound]
% \label{thr:EH_cvx_upper}
% Given an integer $N>0$, the upper bound on the capacity of the BEHC in \eqref{eq:EH_UB_cvx} can be formulated as a convex optimization problem, where the optimization variables are a joint distribution $P_{S,U,Q,X,S^+,Y,U^+,Q^+}$ distributed as \eqref{eq:Qgraph_joint_distUB}. %The convex optimization problem is a monotonically decreasing sequence that converges to $C_{\text{BEHC}}$.
% \end{theorem}

The combination of Theorems~\ref{thr:EH_cvx_lower} and~\ref{thr:EH_cvx_upper} enables the evaluation of $C_{\text{BEHC}}$ using convex optimization algorithms, with a decreasing gap between the lower and upper bounds as $N$ increases. For any harvesting probability $\eta\in \{0.1,0.2,\dots,0.9\}$, we evaluate $C_{\text{BEHC}}(\eta)$ with a precision of ${1e-6}$ using CVX\footnote{\textit{CVX} is a MATLAB-based modeling system for convex optimization. We provide MATLAB code for evaluating the convex lower and upper bounds detailed in \eqref{optProb_EH_LB} and \eqref{optProb_EH_UB}, respectively, for any parameter $\eta$ and integer $N>0$. This code is openly accessible at the following \textit{GitHub} repository:\\ \url{https://github.com/Eli-BGU/Energy-Harvesting-CVX}.}. Our results are compared to the tightest bounds reported in the literature from \cite{tutuncuoglu2017binary}.
The results are summarized in Table~\ref{table:noiseless_BEHC_full}, where our lower and upper bounds are denoted by \textit{$Q$-graph LB} and \textit{$Q$-graph UB}, respectively. We also present the number of nodes required to achieve the specified precision, $|\cQ^{(N)}|=N+1$. Notably, smaller harvesting probabilities $\eta$ require a larger $N$ to achieve the same precision. This trend arises because, for larger $\eta$, when the encoder intends to transmit $N$ consecutive zero inputs, the battery is more likely to be charged afterward, and 
conversely, for smaller $\eta$, it is less likely. For all investigated values of $\eta$, our bounds outperform the best-known results in the literature. While the results from \cite{tutuncuoglu2017binary} are near-tight for larger values of $\eta$, our method provides accurate solutions across all regimes of $\eta$.

\begin{table}[t]
\caption{Bounds on the capacity of the BEHC with i.i.d. harvesting probability $E_i\sim\text{Bern}(\eta)$.}
\label{table:noiseless_BEHC_full}
\centering
 \begin{tabular}{|c||c||c|c|c||c|}
 \hline
 $\eta$ & \textit{LB} \cite{tutuncuoglu2017binary}& ${Q\text{-graph LB}}$ & $|\cQ^{(N)}|$ & ${Q\text{-graph UB}}$ & \textit{UB} \cite{tutuncuoglu2017binary}\\
 \hline
 \textbf{0.1} & 0.2317 & \textbf{0.234150} & 100 &\textbf{0.234151} & 0.2600\\
 \hline
 \textbf{0.2} & 0.3546 & \textbf{0.360193} & 70 & \textbf{0.360194} & 0.3871\\
 \hline
 \textbf{0.3} & 0.4487 & \textbf{0.457051} & 40 & \textbf{0.457052} & 0.4740\\
 \hline
 \textbf{0.4} & 0.5297 & \textbf{0.538820} & 30 & \textbf{0.538821} & 0.5485\\
 \hline
 %0.61064
 \textbf{0.5} & 0.6033 & \textbf{0.610944} & 20 & \textbf{0.610945} & 0.6164\\
 \hline
 \textbf{0.6} & 0.6729 & \textbf{0.678468} & 16 & \textbf{0.678469} & 0.6807\\ 
 \hline
 \textbf{0.7} & 0.7403 & \textbf{0.743533} & 12 & \textbf{0.743534} & 0.7442\\ 
 \hline
 \textbf{0.8} & 0.8088 & \textbf{0.810034} & 8 & \textbf{0.810035} & 0.8101\\
 \hline
 \textbf{0.9} & 0.8845 & \textbf{0.884596} & 7 & \textbf{0.884597} & 0.8846\\
 \hline
 \end{tabular}
\end{table}

\section{Convex Optimization Formulations}
\label{sec:EHconvex}
In this section, we formulate the convex optimization problems for the lower and upper bounds on $C_{\text{BEHC}}$ provided in Theorem~\ref{thr:EH_cvx_lower} and Theorem~\ref{thr:EH_cvx_upper}, and we prove their convexity.

\subsection{Convex Lower Bound on $C_{\text{BEHC}}$}
\label{subsec:EHconvex_lower}
Prior to outlining the convex optimization problem corresponding to the lower bound sequence in Theorem~\ref{thr:EH_cvx_lower}, we first describe its $Q$-graph with $|\cQ|=N+1, N>0$, as illustrated in Fig.~\ref{fig:EH_LB_Graph}. For all nodes $Q\in\cQ=[0:N]$, the outgoing edge $'X=1'$ leads to node $Q=0$. For any node $Q\in [0: N-1]$, the outgoing edge $'X=0'$ leads to node $Q^+=Q+1$. However, for the last node $Q=N$, the outgoing edge $'X=0'$ leads back to the initial node $Q=0$. 

Having described the $Q$-graph, we now define the optimization variables. These variables correspond to the joint PMF $\cS \times \cU \times \cQ \times \cX \times \cS \times \cU \times \cQ$, where $\cU=\cQ$, and they are distributed according to \eqref{eq:Qgraph_joint_dist}. The policy $P_{U^+|U,Q}$ is constrained by \eqref{eq:policy_first_nodes}–\eqref{eq:policy_last_node}, and the strategy function is specified in \eqref{eq:x_behc}.
% \begin{align}
% \label{eq:x_behc}
%   f(U^+,S)=\begin{cases}
%       1, & S=1 \text{ and } U^+=0\\
%       0, & \text{otherwise}
% 		  \end{cases},
% \end{align}
%   & \forall u\in [0:N]: \; P_{U^+|U,Q}(0|u,N)=1; 
% \begin{align}
%   &\forall q\in \{0: N-1\}, \; \forall u\in [0:q], \; \forall u^+\in [1:q+1] \backslash \ (u+1): \; P_{U^+|U,Q}(u^+|u,q)=0, \label{eq:policy_first_nodes}\\
%   & \forall u\in [0:N]: \; P_{U^+|U,Q}(0|u,N)=1; \label{eq:policy_last_node} %LB
%   %&P(U^+=u^+|U=N,Q=N)=0, \quad \text{ if } u^+ \ne 0,N. UB 
% \end{align}
This policy structure, which is general for any given integer $N>0$ determining the $Q$-graph size, implies that the cardinalities $|\cU^{(Q=q)}|=q+1$ and $|\cU^{+(Q=q)}|=q+2$ increase with $q\in [0:N-1]$, and specifically $|\cU^{+(U=u,Q=q)}|=2$ for any $u$, while for the last node $|\cU^{(Q=N)}|=N+1, |\cU^{+(Q=N)}|=1$. These cardinalities are consistent with \eqref{eq:Uset_initialNodes}–\eqref{eq:Uset_LastNode}.
%. For each $q\in [0:N-1]$: $|\cU|=q+1, |\cU^+|=q+2$, while for $q=N$: $|\cU|=N+1, |\cU^+|=1$. 
%For visual demonstration of the general policy structure, we refer the reader to Fig.~\ref{fig:policy} in the end of this section.
Based on the given joint PMF, we define the optimization variables as $\vv \mspace{-3mu}\triangleq\mspace{-3mu} P_{S,U,Q,X,S^+,U^+,Q^+}$, representing its non-zero elements.
%Based on the described joint distribution, the optimization variables are denoted as $\vv \triangleq P_{S,U,Q,X,S^+,U^+,Q^+}$. With some abuse of notation, $\vv$ refers only to the non-zero elements of the joint distribution.

Next, we define four sets of functions that serve as the constraints for the convex optimization problem.
%P(s|u,q)$ is fixed. 

\subsubsection{Stationary Distribution} 
Since $\vv$ has a stationary distribution over $(\cS, \cU, \cQ)$, the marginal distributions of $(S,U,Q)$ and $(S^+,U^+,Q^+)$ must be equal. That is, $P_{S,U,Q}(\tilde{s},\tilde{u},\tilde{q})=P_{S^+,U^+,Q^+}(\tilde{s},\tilde{u},\tilde{q})$ for all $(\tilde{s},\tilde{u},\tilde{q})\in \cS \times \cU^{(Q=\tilde{q})} \times \cQ$. Thus, for each realization $(\tilde{s},\tilde{u},\tilde{q})$, we define a constraint function:
\begin{align}\label{eq:constraints_stationary}
&f_i(\vv)  \triangleq \mspace{-10mu} \sum\limits_{s^+,u^+,q^+}\mspace{-20mu} P_{S,U,Q,X,S^+,U^+,Q^+}(\tilde{s},\mspace{-3mu}\tilde{u},\mspace{-3mu}\tilde{q},\mspace{-3mu}f(u^+,\mspace{-3mu}\tilde{s}),\mspace{-3mu}s^+,\mspace{-3mu}u^+,\mspace{-3mu}q^+) \nn\\ &\quad -\sum\nolimits_{s,u,q}P_{S,U,Q,X,S^+,U^+,Q^+}(s,\mspace{-3mu}u,\mspace{-3mu}q,\mspace{-3mu}f(\tilde{u},\mspace{-3mu}s),\mspace{-3mu}\tilde{s},\mspace{-3mu}\tilde{u},\mspace{-3mu}\tilde{q}),
\end{align}
where $i = 1,\dots,|\cS|\mspace{-5mu} \left(1+2+\cdots+(N+1)\right)\mspace{-5mu}=\mspace{-5mu}(N+1)(N+2)$ indexes all realizations. Note that the constraint functions in \eqref{eq:constraints_stationary} are linear in $\vv$.

\subsubsection{FSC Law}
The following constraint functions set
enforces that $\vv$ adheres to the FSC $P_{S^+|X,S}$ Markov chain $S^+-(X,S)-(U,Q,U^+)$, implied by \eqref{eq:Qgraph_joint_dist}, i.e., 
\begin{align}
&P\left(s,u,q,X=f(u^+,s),s^+,u^+\right)=P(s,u,q,u^+) \nn\\
&\quad \times P_{S^+|X,S}\left(s^+|f(u^+,s),s\right), \quad \forall s,u,q,s^+,u^+, \nn
\end{align}
%$P_{S,U,Q,X,S^+,U^+,Q^+}=P_{S,U,Q,U^+}\mathbbm{1}\{X=f(U^+,S))\} \mathbbm{1}\{Q^+=g(Q,X)\}P_{S^+|X,S}$.
Correspondingly, the constraint functions are defined as
\begin{align}\label{eq:constraints_channelLaw}
&f_i(\vv) \triangleq 
P\left(s,u,q,f(u^+,s),s^+,u^+,g(q,f(u^+,s))\right)-\\ 
& \sum\nolimits_{\tilde{s}^+} \mspace{-8mu} P\big(s,\mspace{-3mu}u,\mspace{-3mu}q,\mspace{-3mu}f\mspace{-2mu}(u^+\mspace{-3mu},\mspace{-3mu}s),\mspace{-3mu}\tilde{s}^+\mspace{-3mu}\mspace{-3mu},\mspace{-3mu}u^+\mspace{-3mu},\mspace{-3mu}g(q,\mspace{-3mu}f\mspace{-2mu}(u^+,s))\big) \mspace{-3mu} P\mspace{-2mu}\big(s^+\mspace{-2mu}|f\mspace{-2mu}(u^+\mspace{-3mu},\mspace{-3mu}s),\mspace{-3mu}s\big), \nn
\end{align}
%\mspace{-3mu}
for $i \mspace{-3mu}= \mspace{-3mu}(N\mspace{-3mu}+\mspace{-3mu}1)(N\mspace{-3mu}+\mspace{-3mu}2)\mspace{-3mu}+\mspace{-3mu}1,\dots,(N\mspace{-3mu}+\mspace{-3mu}1)(N\mspace{-3mu}+\mspace{-3mu}2)\mspace{-3mu}+\mspace{-3mu}2(N\mspace{-3mu}+\mspace{-3mu}1)(2N\mspace{-3mu}+\mspace{-3mu}1)\\=\mspace{-3mu}(N\mspace{-3mu}+\mspace{-3mu}1)(5N\mspace{-3mu}+\mspace{-3mu}4)$ that indexes all realizations. Note that $P_{S^+|X,S}$ remains constant for any choice of $\vv$, as defined in~\eqref{eq:stateEvolutionEH}.
Hence, the constraint functions in \eqref{eq:constraints_channelLaw} are linear in~$\vv$.

\subsubsection{Policy} 
The following constraint functions set enforces that $\vv$ adheres to the policy Markov chain $U^+-(U,Q)-S$ implied by \eqref{eq:Qgraph_joint_dist}, which can also be written as 
\begin{align}
{P(s,\mspace{-3mu}u,\mspace{-3mu}q,\mspace{-3mu}u^+\mspace{-3mu})}/{P(u,\mspace{-3mu}q,\mspace{-3mu}u^+\mspace{-3mu})}\mspace{-3mu}=\mspace{-3mu}P(s|u^+\mspace{-3mu},\mspace{-3mu}u,\mspace{-3mu}q)\mspace{-3mu}=\mspace{-3mu}\pi (s|u,\mspace{-3mu}q), \, \forall s,\mspace{-3mu}u,\mspace{-3mu}q,\mspace{-3mu}u^+\mspace{-3mu}. \nn   
\end{align} 
Since $|\cS|=2$ and $\pi(S=0|u,q)=1\mspace{-3mu}-\mspace{-3mu}\pi(S=1|u,q)$, the Markov chain holds if and only if $P(s=0,u,q,u^+)=\pi(S=0|u,q)P(u,q,u^+)$ for all $(u,q,u^+)$.
The corresponding constraint functions are given by
{
\setlength{\jot}{0pt}
\begin{align}\label{eq:constraints_policy}
& \mspace{-15mu} f_i(\vv) \triangleq \mspace{-5mu}
 \sum\nolimits_{\tilde{s}^+} \mspace{-5mu}P\mspace{-5mu}\left(s, \mspace{-3mu} u, \mspace{-3mu} q, \mspace{-3mu} f(u^+, \mspace{-3mu} s),\tilde{s}^+, \mspace{-3mu} u^+, \mspace{-3mu} g(q, \mspace{-3mu} f(u^+, \mspace{-3mu} s))\right)\mspace{-3mu}- \\&\mspace{-5mu}\sum\nolimits_{\tilde{s},\tilde{s}^+}\mspace{-5mu} P\mspace{-5mu} \left(\tilde{s},u,q,f(u^+,s),\tilde{s}^+,u^+,g(q,f(u^+,s))\right) \pi(s|u,q), \nn
\end{align}
}
where $s=0$, and the realization index $i$ spans from $i = (N\mspace{-3mu}+\mspace{-3mu}1)(5N\mspace{-3mu}+\mspace{-3mu}4)\mspace{-3mu}+\mspace{-3mu}1,\dots,(N\mspace{-3mu}+\mspace{-3mu}1)(5N\mspace{-3mu}+\mspace{-3mu}4)\mspace{-3mu}+\mspace{-3mu}(N\mspace{-3mu}+\mspace{-3mu}1)^2
  =(N\mspace{-3mu}+\mspace{-3mu}1)(6N\mspace{-3mu}+\mspace{-3mu}5)$. Since $\pi_{S|U,Q}$ remains constant for any choice of $\vv$, as given in \eqref{eq:s_given_uq_lb}, the constraint functions in \eqref{eq:constraints_policy} are linear in $\vv$.

\subsubsection{PMF} The final constraint function enforces that the PMF of $\vv$ is valid, i.e.,
\begin{align}\mspace{-3mu}
\label{eq:constraint_PMF}
  f_K(\vv)\mspace{-5mu}&\triangleq \mspace{-25mu} \sum_{s,u,q,s^+,u^+}\mspace{-25mu} P(s,\mspace{-3mu}u,\mspace{-3mu}q,\mspace{-3mu}f(u^+,\mspace{-3mu}s),\mspace{-3mu}s^+,\mspace{-3mu}u^+,\mspace{-3mu}g(q,\mspace{-3mu}f(u^+,\mspace{-3mu}s))\mspace{-3mu}  -\mspace{-3mu}  1,
\end{align}
where $K\mspace{-4mu} \triangleq \mspace{-4mu} (N\mspace{-4mu}+\mspace{-4mu}1)(6N\mspace{-4mu}+\mspace{-4mu}5)\mspace{-4mu}+\mspace{-4mu} 1\mspace{-4mu}=\mspace{-4mu}6N^2\mspace{-4mu}+\mspace{-4mu}11N\mspace{-4mu}+\mspace{-4mu}6$. Note that the constraint function in \eqref{eq:constraint_PMF} is linear $\vv$.

Using the constraint functions in \eqref{eq:constraints_stationary}–\eqref{eq:constraint_PMF}, we formulate the following convex optimization problem for lower bounding $C_{\text{BEHC}}$, consisting of $K$ linear constraints. 

\begin{tcolorbox}[colframe=black,colback=white, sharp corners,colbacktitle=white,coltitle=black,boxrule=0.45pt]
\vspace{-2mm}
\underline{Convex optimization problem formulation for the lower }\\\underline{bound sequence on the capacity of the BEHC, $C_{\text{BEHC}}$:}
\begin{align}\label{optProb_EH_LB}
& \underset{\vv}{\text{maximize}} &f_0(\vv)& \triangleq I(U^{+(N)},U^{(N)};X \mid Q^{(N)}) \nn\\
& \quad \quad \quad \text{s.t.} &f_i(\vv) & = \underline{0}, \; i = 1,\dots,K, \nn\\
& & \vv & \succeq \underline{0},
\end{align}
where $f_i(\vv)$ are detailed in \eqref{eq:constraints_stationary}–\eqref{eq:constraint_PMF}.
\vspace{-2mm}
\end{tcolorbox}

\begin{lemma}
\label{lemma:eh_lb_step_2}
For any integer $N\ge0$, Problem~\eqref{optProb_EH_LB}, corresponding to the $N+1$-node $Q$-graph for the lower bound, illustrated in Fig.~\ref{fig:EH_LB_Graph}, is a convex optimization problem. 
\end{lemma}


\begin{proof}
Since all constraint functions $f_i(\vv), i\in[1:K]$ are linear, it remains to show that the objective function $f_0(\vv)=H(X|Q)-H(X|U^+,U,Q)$ is concave in~$\vv$.

On the one hand, $-H(X|Q)\mspace{-4mu}=\mspace{-4mu}\sum_{x,q} P(q,x) \log \frac{P(q,x)}{P(q)}$ is the relative entropy, which is convex. Hence, $H(X|Q)$ is concave. On the other hand, $\mspace{-4mu}H(X\mspace{-2mu}|U^+\mspace{-6mu}=\mspace{-6mu}u^+,U\mspace{-6mu}=\mspace{-6mu}u,Q\mspace{-6mu}=\mspace{-6mu}q)$ is constant because $P(x|u^+,u,q)\mspace{-4mu}=\mspace{-4mu}\sum_{s} \pi(s|u,q) \mathbbm{1}\{x\mspace{-4mu}=\mspace{-4mu}f(u^+,s)\}$,
where $\pi(s|u,q)$ is constant, as given in \eqref{eq:s_given_uq_lb} and proved in Section~\ref{sec:QgraphLB}. This implies that $H(X|U^+,U,Q)$ is linear in $\vv$. Therefore, the objective function $f_0(\vv)$ is concave in $\vv$.
\end{proof}
%% !!!!!!!!!!!!!!!!!!
% The following theorems refer to the convexity, monotony of the optimization problem in \eqref{optProb_EH_LB} and convergence of the optimization problem sequence to $C_{\text{BEHC}}$.
%% !!!!!!!!!!!!!!!!!!
% \begin{theorem}
% \label{thorem:BEHC_convex_lb_formulation}
% Given any integer $N>0$, \eqref{optProb_EH_LB} related to the $N+1$ sized $Q$-graph in Fig.~\ref{fig:EH_LB_Graph} is a convex optimization problem. 
% \end{theorem}

\subsection{Convex Upper Bound on $C_{\text{BEHC}}$}
\label{subsec:EHconvex_upper}
The convex optimization problem for the upper bound closely resembles that of the lower bound in~\eqref{optProb_EH_LB}, but notable differences arise in the handling of the last node $Q=N$ in the $Q$-graph depicted in Fig.~\ref{fig:EH_UB_Graph}. Specifically, the outgoing edge $'X=0'$ from node $Q=N$ loops back to itself. The optimization variables are now the non-zero elements of the joint PMF $P_{S,U,Q,X,S^+,U^+,Q^+}$ in \eqref{eq:Qgraph_joint_distUB}, denoted by $\tilde{\vv}$.
Recall that this joint distribution includes $P_{S^+|X,S,Q}$ instead of $P_{S^+|X,S}$. The strategy function follows \eqref{eq:x_behc}, and the policy $P_{U^+|U,Q}$ satisfies \eqref{eq:policy_first_nodes}, while also adhering to \eqref{eq:policy_last_node_UB} instead of \eqref{eq:policy_last_node}. This general policy structure, applicable for any given $N$, induces the same auxiliary RV sets as in the lower bound, except that $|\cU^{+(Q=N)}|=2$. Moreover, $S \mid Q=N$ is always $1$, resulting in a cardinality of $1$.
% Given any integer $N>0$, consider the $Q$-graph in Fig.~\ref{fig:EH_UB_Graph}. The optimization variables are chosen as a joint distribution $\cS \times \cU \times \cQ \times \cX \times \cY \times \cS \times \cU \times \cQ$ with $|\cU|=|\cQ|$,
% distributed as 
% \begin{align}
% \label{eq:Qgraph_joint_distribution_EH_modified}
%   &P(s,u,q,x,s^+,y,u^+,q^+) \nn\\&= \pi(s,u,q) P(u^+|u,q) \mathbbm{1}\{x = f(u^+,s)\} P_{S^+,Y|X,S,Q}(s^+,y|x,s,q) \mathbbm{1}\{q^+ = g(q,y)\},
% \end{align}

These differences result in modifications to the FSC law constraints \eqref{eq:constraints_channelLaw} and the policy constraints \eqref{eq:constraints_policy}, as detailed below.
While the stationary distribution constraints \eqref{eq:constraints_stationary} and the PMF constraint \eqref{eq:constraint_PMF} remain unchanged (with $\tilde{\vv}$ replacing $\vv$), their indexing is updated.
The stationary distribution constraints now span up to $(N+1)^2$, and the PMF constraint indexing is adjusted based on the indexing of the following modified constraints.

\subsubsection{Modified FSC Law Constraint Functions}
The following constraint functions set
endorces that $\tilde{\vv}$ adheres to the Markov chain $S^+-(X,S,Q)-(U,U^+)$ implied by \eqref{eq:Qgraph_joint_distUB}. Accordingly, the FSC law constraint functions in \eqref{eq:constraints_channelLaw} are modified by replacing $P_{S^+|X,S}$ with $P_{S^+|X,S,Q}$. The redefined constraints are expressed as
\begin{align}\label{eq:constraints_channelLaw_modified}
&f_i(\tilde{\vv}) \triangleq 
P\left(s,u,q,f(u^+,s),s^+,u^+,g(q,f(u^+,s))\right)-\\ 
& \sum\nolimits_{\tilde{s}^+} \mspace{-10mu} P\mspace{-2mu} \big(s,\mspace{-4mu}u,\mspace{-4mu}q,\mspace{-4mu}f(u^+\mspace{-4mu},\mspace{-4mu}s),\mspace{-4mu}\tilde{s}^+\mspace{-4mu}\mspace{-4mu},\mspace{-4mu}u^+\mspace{-4mu},\mspace{-4mu}g(q,\mspace{-4mu}f(u^+,s))\big) P\mspace{-2mu}\big(s^+\mspace{-2mu}|f(u^+\mspace{-4mu},\mspace{-4mu}s),\mspace{-4mu}s,\mspace{-4mu}q\big), \nn
\end{align}
for $i \mspace{-3mu}=\mspace{-3mu} (N\mspace{-3mu}+\mspace{-3mu}1)^2\mspace{-3mu}+\mspace{-3mu}1,\dots,\mspace{-3mu} (N\mspace{-3mu}+\mspace{-3mu}1)^2\mspace{-3mu}+4(N\mspace{-3mu}+\mspace{-3mu}1)^2 =5(N\mspace{-3mu}+\mspace{-3mu}1)^2$, indexing all $(s,u,q,s^+,u^+)$. Note that $P_{S^+|X,S,Q}$ remains constant for any choice of $\tilde{\vv}$, as given in \eqref{eq:modifiedChannelLaw}. Hence, the modified constraint functions in \eqref{eq:constraints_channelLaw_modified} are linear in~$\tilde{\vv}$.

\subsubsection{Modified Policy Constraint Functions}
The constraint functions for the policy resemble those in \eqref{eq:constraints_policy}, but with
$\pi_{S|U,Q}$ given by \eqref{eq:s_given_uq_upper} instead of \eqref{eq:s_given_uq_lb}. Furthermore, the realization index $i$ spans from 
$i \mspace{-3mu}=\mspace{-3mu} 5(N\mspace{-3mu}+\mspace{-3mu}1)^2\mspace{-3mu}+\mspace{-3mu}1,\dots,5(N\mspace{-3mu}+\mspace{-3mu}1)^2\mspace{-3mu}+\mspace{-3mu}(N\mspace{-3mu}+\mspace{-3mu}1)(N\mspace{-3mu}+\mspace{-3mu}2)=(N\mspace{-3mu}+\mspace{-3mu}1)(6N\mspace{-3mu}+\mspace{-3mu}7)$, since there are $N\mspace{-3mu}+\mspace{-3mu}1$ additional realizations due to $|\cU^{+(Q=N)}|\mspace{-3mu}=\mspace{-3mu}2$. 
The modified $\pi_{S|U,Q}$ remains constant for any choice of ${\vv}$; thus, the modified policy constraint functions are linear in~$\tilde{\vv}$.

Finally, $\tilde{K}\triangleq (N\mspace{-3mu}+\mspace{-3mu}1)(6N\mspace{-3mu}+\mspace{-3mu}7)\mspace{-3mu}+\mspace{-3mu}1=6N^2\mspace{-3mu}+\mspace{-3mu}13N\mspace{-3mu}+\mspace{-3mu}8$ indexes the last constraint function (PMF) and replaces $K$. Incorporating these modifications, we formulate the following convex optimization problem to establish a sequence of computable upper bounds on $C_{\text{BEHC}}$.

\begin{tcolorbox}[colframe=black,colback=white, sharp corners,colbacktitle=white,coltitle=black,boxrule=0.45pt]
\vspace{-2mm}
\underline{Convex optimization problem formulation for the upper}\\
\underline{bound sequence on the capacity of the BEHC, $C_{\text{BEHC}}$:}
\begin{align}\label{optProb_EH_UB}
& \underset{\tilde{\vv}}{\text{maximize}} &f_0(\tilde{\vv})& \triangleq I(U^{+(N)},U^{(N)};X \mid Q^{(N)}) \nn\\
& \quad \quad \quad \text{s.t.} &f_i(\tilde{\vv}) & = \underline{0}, \; i = 1,\dots,\tilde{K}, \nn\\
& & \tilde{\vv} & \succeq \underline{0},
\end{align}
where $f_i(\tilde{\vv})$ are detailed in \eqref{eq:constraints_stationary}, 
\eqref{eq:constraints_channelLaw_modified}, \eqref{eq:constraints_policy}, and \eqref{eq:constraint_PMF}.
\vspace{-2mm}
\end{tcolorbox}

% The following theorems refer to the convexity, monotony of the optimization problem in \eqref{optProb_EH_UB} and convergence of the optimization problem sequence.
\begin{lemma}
\label{lem:BEHC_convex_ub_formulation}
For any integer $N\ge0$, Problem~\eqref{optProb_EH_UB}, corresponding to the $N+1$-node $Q$-graph for the upper bound, illustrated in Fig.~\ref{fig:EH_UB_Graph}, is a convex optimization problem. 
\end{lemma}

The proof of Lemma~\ref{lem:BEHC_convex_ub_formulation} follows identically to that of Lemma~\ref{lemma:eh_lb_step_2},
with the sole distinction that $\pi(s|u,q)$ is constant, as given by \eqref{eq:s_given_uq_upper} rather than \eqref{eq:s_given_uq_lb}, which is proved in Section~\ref{sec:QgraphUB}.
% \begin{theorem}
% \label{theorem:monoton_UB}
%      The sequence $\{b_N\}$ is monotonically decreasing.
% \end{theorem}
% \begin{theorem}
% \label{theorem:convergence_UB}
%     The sequence $\{b_N\}$ converges to the capacity of the BEHC, $C_{\text{BEHC}}$.
% \end{theorem}

%The proofs of Theorems~\ref{theorem:variation_cvx}, \ref{theorem:monoton_UB} and~\ref{theorem:convergence_UB} are omitted here due to length limitation.
%!!!!!!!!!!
% The proof of Theorem \ref{thr:EH_cvx_upper} is a direct consequence of Theorems~\ref{lem:BEHC_convex_ub_formulation}, \ref{theorem:variation_cvx}, \ref{theorem:monoton_UB} and~\ref{theorem:convergence_UB}.

\section{Proofs of Main Results}
\label{sec:proof_main_theorems}
This section provides the proofs of Theorems~\ref{thr:EH_cvx_lower} and~\ref{thr:EH_cvx_upper}, which constitute the core contribution of this work.

\subsection{Proof of Theorem \ref{thr:EH_cvx_lower} -- Lower Bound on \(C_{\text{BEHC}}\)}
\label{sec:QgraphLB}
Theorem~\ref{thr:EH_cvx_lower} is based on Theorem~\ref{theorem:qgraph_LB} and is proved in four parts.
First, the policy's compliance with the BCJR constraints is established in Lemma~\ref{lemma:eh_lb_BCJR}.
Second, the marginal \(\pi_{S|U,Q}\) is shown to satisfy \eqref{eq:s_given_uq_lb}, as detailed in Lemma~\ref{lem:marginal_s_given_uq}. Third, the convexity of the lower bound \eqref{eq:EH_LB_cvx} is demonstrated in Lemma~\ref{lemma:eh_lb_step_2}, Section~\ref{subsec:EHconvex_lower}. Finally, the convergence of the lower bound sequence to \(C_{\text{BEHC}}\) is proved in Theorem~\ref{thr:convergence}, Section~\ref{subsec:ProofThrConvergence}, alongside the convergence of the upper bound sequence.

%\subsection{Proof of Theorem \ref{theorem:BCJR_cvx}}
\begin{lemma}
\label{lemma:eh_lb_BCJR}    
Assuming the auxiliary RV sets given in \eqref{eq:Uset_initialNodes}–\eqref{eq:Uset_LastNode}, any policy satisfies the BCJR constraints \eqref{eq:BCJR_Def}.    
\end{lemma}
\begin{proof}
\label{appendix:ProofBCJR_CVX}
The BCJR constraint \eqref{eq:BCJR_Def} can alternatively be expressed as follows (see \cite[Eq.~12]{shemuel2024finite}):
%The proof is based on the $Q$-graph lower bound in Theorem~\ref{theorem:qgraph_LB}, thus it is remained to show that the BCJR-invariant property constraints hold considering the joint distribution with the chosen $Q$-graph in Fig.~\ref{fig:EH_LB_Graph}.
\begin{align}
&\pi_{U^+,S^+|Q^+} (u^+,s^+|q^+) \label{eq:BCJR_check}\\
&=\frac{\sum_{u,s}\pi(u,s|q)P(u^+|u,q) \mathbbm{1}\{x = f(u^+,s)\} P(s^+|x,s)}{\sum_{u,u'^+,s} \pi(u,s|q)P(u'^+|u,q)\mathbbm{1}\{x = f(u'^+,s)\}}. \nn 
\end{align}
To prove the claim, we need to show that for any $(u^+,s^+, q^+)$, all $(q,x)$ satisfying $q^+ = g(q,x)$ yield the same result in the RHS of \eqref{eq:BCJR_check}. Thus, it suffices to verify this property for nodes with more than one incoming edge. In the $Q$-graph of Fig.~\ref{fig:EH_LB_Graph}, the only such node is $Q^+=0$, which arises in two cases:
\\\textit{Case (I):} $g(q,X=1)=0, \quad \forall q\in \cQ^{(N)}$.
\\\textit{Case (II):} $g(Q=N,X=0)=0$.\\
Since $\cU^{(Q=0)}=\{0\}, |\cS|=2$, it suffices to verify \eqref{eq:BCJR_check} for these two cases when $(u^+,s^+)=(0,0)$ only.

For Case (I), the RHS of \eqref{eq:BCJR_check} when $(u^+,s^+)=(0,0)$ is 
%(all $q$ and $x=1$, \eqref{eq:BCJR_check} when  equals
%For $Q^+=0$, all incoming edges but one are labeled by $'Y=1'$ only. For these edges considering any $q\in\cQ^{(N)}$,
\begin{align}
\label{eq:BCJR_ev_eh1}
%&P(U^+=0,S^+=0|Q^+=0)\nn\\
&\frac{\sum_{u,s}\mspace{-5mu}\pi(u,\mspace{-3mu}s|q)P(U^+\mspace{-6mu}=\mspace{-4mu}0|u,\mspace{-3mu}q)
\mspace{-3mu}\mathbbm{1}\mspace{-3mu}\{1 \mspace{-5mu}=\mspace{-5mu} f(0,\mspace{-3mu}s)\} P_{S^+|X,S}(0|1,\mspace{-3mu}s)}{\sum_{u,u'^+,s} \pi(u,s|q)P(U^+\mspace{-5mu}=\mspace{-5mu}u'^+|u,\mspace{-3mu}q)
\mathbbm{1}\{1 \mspace{-5mu}=\mspace{-5mu} f(u'^+\mspace{-3mu},\mspace{-3mu}s)} \nn\\
&\stackrel{(a)}=\frac{\sum_{u}\pi(u,S=1|q)P(U^+=0|u,q)P_{S^+|X,S}(0|1,1)}{\sum_{u} \pi(u,S=1|q)P(U^+=0|u,q)}
 \nn\\
&\stackrel{(b)}=\frac{\sum_{u}\pi(u,S=1|q)P(U^+=0|u,q)\bar{\eta}}{\sum_{u} \pi(u,S=1|q)P(U^+=0|u,q)}
 =\bar{\eta}.
\end{align}
Step (a) follows from \eqref{eq:x_behc}; %which implies that only arguments with $U^+=0,S=1$ are non-zero;
Step (b) follows from \eqref{eq:stateEvolutionNoiselessEH}.
%A similar calculation for all $q\in \cQ^{(N)}$ and $x=1$, 
%the RHS of \eqref{eq:BCJR_check} when $(u^+,s^+)=(0,0)$ is

For Case (II), the RHS of \eqref{eq:BCJR_check} when $(u^+,s^+)=(0,0)$ is
%For $q=N$ and $x=0$, \eqref{eq:BCJR_check} when $(u^+,s^+)=(0,0)$ is
%For the single incoming edge $'Y=0'$ from node $Q=N$ to node $Q^+=0$, a %similar calculation to \eqref{eq:BCJR_ev_eh} holds,
\begin{align}
\label{eq:BCJR_ev_eh2}
%&P(U^+=0,S^+=0|Q^+=0)\nn\\
& \frac{\sum_{u,s}\mspace{-4mu}\pi(u,\mspace{-3mu}s|q)\mspace{-2mu} P_{U^+|U,Q}(0|u,N)
\mathbbm{1}\mspace{-3mu}\{0 \mspace{-2mu}=\mspace{-5mu} f(0,\mspace{-3mu} s)\}P_{S^+|X,S}(0|0, \mspace{-3mu}s)}{\sum_{u,u'^+,s} \pi(u, \mspace{-3mu} s|q)P_{U^+|U,Q}(u'^+|u,\mspace{-3mu}N)
\mathbbm{1}\mspace{-3mu}\{0 \mspace{-3mu}=\mspace{-2mu} f(u'^+,\mspace{-3mu} s)\}
} \nn\\
& \stackrel{(a)}=\frac{\sum_{u,s}\mspace{-3mu} \pi(u,s|q)
\mathbbm{1}\{0 = f(0,s)\}
P_{S^+|X,S}(0|0,s)}{\sum_{u,s} \pi(u,s|q)\mathbbm{1}\{0 = f(0,s)\}}
 \nn\\ 
 & \stackrel{(b)} = \frac{\sum_{u}\pi(u,S=0|q)\mathbbm{1}\{0 = f(0,0)\} \bar{\eta}}{\sum_{u,s} \pi(u,s|q)
\mathbbm{1}\{0 = f(0,s)\}}\stackrel{(c)}=\bar{\eta}.
%&\stackrel{(b)}=\frac{\sum_{u}P(u,s|Q=N)P(U^+=0|u,q)}{\sum\limits_{u} P(u,s|Q=N)P(U^+=0|u,q)} \nn\\
\end{align}
Step (a) follows from \eqref{eq:policy_last_node}; %\eqref{eq:x_behc} and $Y=X$ which imply that only arguments with $U^+=0,S=1$ are non-zero;
Step (b) follows from \eqref{eq:stateEvolutionNoiselessEH}; Step (c) follows from \eqref{eq:x_behc}, which implies that $x=s$ when $u^+=0$. Therefore, \eqref{eq:BCJR_ev_eh1} equals \eqref{eq:BCJR_ev_eh2}, completing the proof.
\end{proof}
%Similar calculation to \eqref{eq:BCJR_ev_eh1} for all $q\in \cQ^{(N)}$ and $x=1$, when $(u^+,s^+)=(0,1)$, provides that the RHS of \eqref{eq:BCJR_check} equals $\eta$. Furthermore, similar calculation to \eqref{eq:BCJR_ev_eh2} for $q=N$ and $x=0$, when $(u^+,s^+)=(0,1)$ also provides that the RHS of \eqref{eq:BCJR_check} equals $\eta$. In conclusion, all the BJCR-invariant constraints are satisfied and\\$\pi_{U,S|Q} (0,0|0)=1-\eta, \quad \pi_{U,S|Q}(0,1|0)=\eta$.
%The proof of Theorem \ref{theorem:BCJR_cvx} is given in Appendix~\ref{appendix:ProofBCJR_CVX}.

\begin{lemma}
\label{lem:marginal_s_given_uq}
For any policy, assuming the auxiliary RV sets in \eqref{eq:Uset_initialNodes}–\eqref{eq:Uset_LastNode}, the marginal \(\pi_{S|U,Q}\) is constant and given by %\eqref{eq:s_given_uq_lb} 
\begin{align}
\label{eq:s_given_uq_lb_induction}
\pi(S=0|u,q)=\bar{\eta}^{u+1}, \quad \forall q\in \cQ, \forall u\in \mathcal{U}^{(Q = q)}.
\end{align}
\end{lemma}
\begin{proof}
We prove the statement by induction.\\ 
\textit{Base case:} For all $q\in \cQ$, when $u=0$ we have:
\begin{align}
    &\pi_{S|U,Q}(0|0,q)\stackrel{(a)}=\pi_{S^+|U^+,Q^+}(0|0,q) \nn\\
    &=\mspace{-3mu}\sum\nolimits_{s,x \mid s\ge x} P_{S,X,S^+|U^+,Q^+}(0,s,x|0,q)\nn\\
    &=\mspace{-3mu}\sum\nolimits_{s,x \mid s\ge x} \mspace{-7mu} P_{S,X|U^+,Q^+}(s,x|0,q) P_{S^+|S,X}(0|s,f(0,s))\mspace{-3mu}\stackrel{(b)}=\mspace{-3mu}\bar{\eta}. \nn
\end{align}
Step (a) follows from the stationarity of the distribution over $(S,U,Q)$. Step (b) follows from \eqref{eq:stateEvolutionNoiselessEH} and \eqref{eq:x_behc}, which imply that $P_{S^+|S,X}(0|s,f(0,s))=\bar{\eta}$.\\
\textit{Inductive hypothesis:} Suppose the statement holds for all $u \in \mathcal{U}^{(Q = q)}\setminus \{0,q\}, q \in \cQ \setminus \{0,N\}$. I.e., $\pi(S\mspace{-2mu}=\mspace{-2mu}0|u,\mspace{-2mu}q)\mspace{-2mu}=\mspace{-2mu}\bar{\eta}^{u+1}$.\\
%(recall that there is no $u>0$ in $\mathcal{U}^{(Q = 0)}$). 
% \label{eq:s_given_uq_lb}
% \pi(S=0|u,q)=\bar{\eta}^{u+1}, \quad \forall q\in \cQ, \forall u\in \mathcal{U}^{(Q = q)}.
% \end{align}
\textit{Inductive step:} For $\tilde{u}= u+1, \tilde{q}= q+1$, we show that $\pi_{S|U,Q}(0|\tilde{u},\tilde{q})=\bar{\eta}^{\tilde{u}+1}$. Expanding:
\begin{align}
    &\pi_{S|U,Q}(0|\tilde{u},\tilde{q})\stackrel{(a)}=\pi_{S^+|U^+,Q^+}(0|\tilde{u},\tilde{q}) \nn\\    
    &\stackrel{(b)}=P_{S^+|U^+,Q^+,U,Q,X}(0|\tilde{u},\tilde{q},\tilde{u}-1,\tilde{q}-1,0) \nn\\
    &\stackrel{(c)}=P_{S,S^+|U^+,Q^+,U,Q,X}(0,0|\tilde{u},\tilde{q},\tilde{u}-1,\tilde{q}-1,0) \nn\\
    &\stackrel{(d)}=P_{S|U,Q}(0|\tilde{u}-1,\tilde{q}-1)P_{S^+|X,S}(0|0,0) 
    \nn \\    &\stackrel{(e)}=\bar{\eta}^{\tilde{u}}\bar{\eta}=\bar{\eta}^{\tilde{u}+1}. \nn    
\end{align}
Step (a) follows from the stationary distribution.
Step (b) follows since when $\tilde{u}\ne 0$, it implies $X=f(\tilde{u},s)=0, \forall s$. Moreover, $U^+=\tilde{u}, Q^+=\tilde{q}$ are reachable only from $U=\tilde{u}-1, Q=\tilde{q}-1$. Step (c) follows because if $X=0, S^+=0$, it is impossible for $S=1$ due to \eqref{eq:stateEvolutionNoiselessEH}. Step (d) follows from the Markov property of $P_{S^+|X,S}$ and the Markov chain $S-(U,Q)-(U^+,Q^+,X)$. Its proof is omitted for brevity, but follows from  
%the Markov chain $U^+-(U,S)-S$ implied by 
\eqref{eq:Qgraph_joint_dist} and Bayes' rule. Step (e) follows from the inductive hypothesis and \eqref{eq:stateEvolutionNoiselessEH}. Thus, the statement is proven.
%Appendix~\ref{appendix:lemma_s_uq_Markov} and .
% The induced marginal $\pi_{S|U,Q}$ remains constant for any choice of policy $P_{U^+|U,Q}$ considering \eqref{eq:Uset_initialNodes}–\eqref{eq:Uset_LastNode}, and is given by
% \begin{align}
% \label{eq:s_given_uq_lb}
% \pi(S=0|u,q)=\bar{\eta}^{u+1}, \quad \forall q\in \cQ, \forall u\in \mathcal{U}^{(Q = q)}.
% \end{align}
   
\end{proof}
%Theorem~\ref{thr:convergence}
%altogether with the
% and that \eqref{optProb_EH_LB} is a convex optimization. These two steps are stated in the following Lemma~\ref{lemma:eh_lb_BCJR} and Lemma~\ref{lemma:eh_lb_step_2}.

% \begin{theorem}
% \label{theorem:monoton_LB}
%      The sequence $\{a_N\}$ is monotonically increasing.
% \end{theorem}

% \begin{proof}
%     Given any integer $N$, we need to show that $a_N \le a_{N+1}$. Denote $\tilde{N}\triangleq N+1$ and consider $a_{\tilde{N}}$. The $Q$-graph of $a_{\tilde{N}}$ has $|\cQ^{(N)}|=\tilde{N}+1$ nodes, and by taking the policy of node $Q=\tilde{N}-1$ to be 
% \begin{align}
%    \forall u\in [0:\tilde{N}-1]: \; P_{U^+|U,Q}(0|u,\tilde{N}-1)=1,     
% \end{align}
% we obtain that $P(Q^+=0|Q=\tilde{N}-1)=1$, that is, node $Q=\tilde{N}$ is never reached. This induces the same $(N+1)$-node $Q$-graph of $a_N$ with its same policy. In other words, we showed a specific policy that achieves $a_N$, hence $a_{\tilde{N}} \ge a_N$ as required.
% \end{proof}

 %\label{eq:policy_first_nodes}\\
 %   & \forall u\in [0:N]: \; P_{U^+|U,Q}(0|u,N)=1, \label{eq:%policy_last_node} %LB
% !!!!!!!!!!!!!!!!!!
% \begin{theorem}
% \label{theorem:convergence_LB}
%     The sequence $\{a_N\}$ converges to the capacity of the BEHC, $C_{\text{BEHC}}$.
% \end{theorem}
% !!!!!!!!!!!!!!!!!!
% !!!!!!!!!!!!!!!!!!
% The proof of Theorem \ref{theorem:convergence_LB} will be given in the end of this section.
%The proofs of Theorems~\ref{theorem:monoton_LB} and~\ref{theorem:convergence_LB} are omitted here due to length limitation.

\subsection{Proof of Theorem \ref{thr:EH_cvx_upper} -- Upper Bound on $C_{\text{BEHC}}$}
\label{sec:QgraphUB}
To establish the upper bound, we analyze a variation of the BEHC, whose capacity exceeds $C_{\text{BEHC}}$. Specifically, for any integer $N\ge0$, consider a modified BEHC where transmitting $N$ consecutive zero channel inputs automatically charges the battery, i.e., $P(S_i=1|X_{i-N}^{i-1}=\mathbf{0}^N)=1$.
%after each transmission $E_i\sim$\text{Bern}($\eta$), but upon transmitting $N$ consecutive zero channel inputs, a unit energy is harvested with probability $1$ (instead of $\eta$), i.e., $P(E_i=1| X_{i-N}^{i-1}=\mathbf{0}^N)=1$,
%That is, $P(S_i=1|Y_{i-N}^{i-1}=\mathbf{0}^N)=1$.
We refer to this variation as the \textit{boosted BEHC($N$)}. 
Let $\bar{C}_{\text{BEHC}}(N)$ denote the capacity of the boosted BEHC($N$). Then, it follows that $C_{\text{BEHC}} < \bar{C}_{\text{BEHC}}(N) , \, \forall N\ge0$.
% \begin{align}
%     C_{\text{BEHC}} < \bar{C}_{\text{BEHC}}(N) , \quad \forall N\ge0.
% \end{align}
To prove Theorem~\ref{thr:EH_cvx_upper}, we demonstrate that the RHS of \eqref{eq:EH_UB_cvx} serves as an upper bound for $\bar{C}_{\text{BEHC}}(N)$ for any $N$.
% %, i.e.,
% \begin{align}
% \bar{C}_{\text{BEHC}}(N) \leq \sup_{P(u^+|u,q)\in\mathcal{P}_\pi} I(U^+,U;Y|Q^{(N)}), \label{eq:C_var_UB}
% \end{align}
% where the joint distribution is \eqref{eq:Qgraph_joint_distUB} and the associated $Q$-graph is the one given in Fig.~\ref{fig:EH_UB_Graph}.
%%%%%%% Important for myself!!!!
% However, as opposed to the lower bound in Theorem~\ref{theorem:qgraph_LB}, there is no single-letter $Q$-graph upper bound on the feedback capacity of strongly connected FSCs with SI known causally at the encoder.
%The difficulty in deriving such single-letter upper bound stems from the fact that the Markov chain $X_i-(U_i,U_{i-1},Q^{(N)}_{i-1}(X^{i-1}))-X^{i-1}$ does not hold for all $Q$-graphs and from the absence of general cardinality bound on $|\cU|$
%%%%%%
%%%%%%%%%%%%%%%%% IMPORTANT
% While (Oron's upper bound theorem) was derived for all $Q$-graphs, in our general scenario it 
% --- I should move it to a discussion of preivous results above---
% The difficulty in deriving such single-letter upper bound stems from th
% the fact that the Markov 
% $Q$-graphs and from the absence of general cardinality bound on $|\cU|$ in the single-letter expression shown in the RHS of \eqref{eq:EH_UB_cvx}. Yet, we show that for the boosted BEHC($N$), the Markov chain holds for the $Q$-graph in Fig.~\ref{fig:EH_UB_Graph} and that $|\cU|\le N+1$.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{enumerate}
%     \item the strategy function is likewise fixed as in \eqref{eq:x_behc};  
%     \item $P_{S^+,Y|X,S,Q^{(N)}}$ characterizes the modified channel law which now depends on $Q$ and is defined as \eqref{eq:modifiedChannelLaw};
%     \item the policy is chosen likewise to satisfy \eqref{eq:policy_first_nodes}, but \eqref{eq:policy_last_node} featuring the last node is modified to satisfy
% % \begin{align}
% %     & \forall u\in [0:N], \forall u^+\in [2:N]: \; P_{U^+|U,Q}(u^+|u,N)=0. \label{eq:policy_last_node_UB}
% % \end{align}
% \end{enumerate}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% The special structure of the joint distribution induces
% \begin{align}
% \label{eq:batteryRelation1_UB}
% \pi(S=0|u,q)=\bar{\eta}^{u+1}=P(S=0|u), \quad \forall q\in [0:N-1], \forall u\in [0:q],
% \end{align}
% similarly to \eqref{eq:s_given_uq_lb}, but for the last node $Q=N$ we have
% \begin{align}
% \label{eq:batteryRelation2_UB}
%  P(S=0|u,Q^{(N)}=N)=0, \forall u\in [0:N].
% \end{align}
%due to the assumption of the new variation of the BEHC which implies that the battery is full with probability $1$ after $N$ consecutive zero outputs. 
% \begin{proof}[Proof of Theorem \ref{thr:EH_Qgraph_ub}]
%     %\label{theorem:qgraph_UB}
% In this proof, we show  that $\bar{C}_{\text{BEHC}}(N)$ is upper bounded by the RHS of 
% \eqref{eq:EH_UB_cvx}. 
% The proof is given in the appendix.
% \end{proof}
%In this subsection, we prove Theorem \ref{thr:EH_Qgraph_ub}.
%Here, we aim to show a convex optimization problem that characterizes $\bar{C}_{\text{BEHC}}(N)$ itself, thereby upper bounding $C_{\text{BEHC}}$.
The proof consists of three main steps, formulated in Lemma~\ref{lem:C_bar_FSC}, Theorem~\ref{thr:FSC_Qgraph_UB} and Theorem~\ref{thr:cardinality} below. 

Observe that the FSC property \eqref{eq:BasicFSCMarkov} with the original state space does not hold for the boosted BEHC($N$). For instance, consider the case $N\mspace{-3mu}=\mspace{-3mu}2$: 
\begin{align}
&P\mspace{-3mu}\left(S_i\mspace{-3mu}=\mspace{-3mu}1|(X_{i-1},\mspace{-3mu} X_{i})\mspace{-3mu}=\mspace{-3mu}(0,\mspace{-3mu} 0),\mspace{-3mu} (S_{i-2}, \mspace{-3mu} S_{i-1})\mspace{-3mu}=\mspace{-3mu}(0,\mspace{-3mu} 0)\right)\mspace{-3mu}=\mspace{-3mu}1 \nn\\
&> P(S_i\mspace{-3mu}=\mspace{-3mu}1|(X_{i-1},X_{i})\mspace{-3mu}=\mspace{-3mu}(1,0),(S_{i-2},S_{i-1})\mspace{-3mu}=\mspace{-3mu}(1,0)). \nn
\end{align}
% \begin{align}
%     &\scriptstyle P\left(S_i=1|(X_{i-1},X_{i})=(0,0),(S_{i-2},S_{i-1})=(0,0)\right)=1 \nn\\
%     &\scriptstyle \quad > P(S_i=1|(X_{i-1},X_{i})=(1,0),(S_{i-2},S_{i-1})=(1,0)).    
% \end{align}
This demonstrates a dependence on $X_{i-1}, S_{i-2}$, violating the FSC property. Nevertheless, in the first step, we show that the boosted BEHC($N$) can be reformulated as a FSC by introducing a new state definition.

\begin{lemma}[Step 1]
\label{lem:C_bar_FSC}
The boosted BEHC($N$) can be reformulated as a strongly connected FSC $P_{\tilde{S}^+,Y|X,\tilde{S}
}$ with a new state $\tilde{S}_i \triangleq (S_i,Q_i^{(N)}
)$ and initial state $(s_0,q_0)=(0,0)$, given by
%initial state $\tilde{s}_0\triangleq (s_0,q_0)$
%where $Q^{(N)}$ is a RV defined on the vertices of the $Q$-graph illustrated in Fig.~\ref{fig:EH_UB_Graph} with $|\cQ^{(N)}^{(N)}|= N+1$.
\begin{align}
\label{eq:NewFSC}
    &P_{\tilde{S}^+,Y|X,\tilde{S}}((s_i,q_i),y_i|x_i,(s_{i-1},q_{i-1}))
    %&P_{\tilde{S}^+,Y|X,\tilde{S}}(\tilde{s}_i,y_i|x_i,\tilde{s}_{i-1})=P((s_i,q_i),y_i|x_i,(s_{i-1},q_{i-1})) \nn \\
    =\mathbbm{1}\{y_i=x_i\} \nn\\& \times P_{S^+|X,S,Q^{(N)}}(s_i|x_i,s_{i-1},q_{i-1}) \mathbbm{1}\{q_i=g(q_{i-1},x_i)\},
\end{align}
where $P_{S^+|X,S,Q^{(N)}}$ is given in \eqref{eq:modifiedChannelLaw},
% \begin{align}
%    &P_{S^+|X,S,Q^{(N)}}(s_i|x_i,s_{i-1},q_{i-1})\nn\\
%    &=\begin{cases}
%     \scriptstyle \mathbbm{1}\{s_i=1\}, & \scriptstyle \text{if } q_{i-1}=N-1 \text{ and } x_i=0      \\
%     \scriptstyle \bar{\eta} \mathbbm{1}\{s_i=s_{i-1}-x_i\} + \eta \mathbbm{1}\{s_i=1\}, & \scriptstyle \text{otherwise}  
%    \end{cases},     \label{eq:S_evol_UB_step1} 
% \end{align}
and $g(q_{i-1},x_i)$ corresponds to the transitions of the $N\mspace{-3mu}+\mspace{-3mu}1$-node $Q$-graph in Fig.~\ref{fig:EH_UB_Graph}.
% \begin{align}
%     g(q_{i-1},x_i)&= \begin{cases}
%     0, & x_i=1\\            
%     q_{i-1}+1, &x_i=0, q_{i-1}\ne N \\
%     N, &x_i=0, q_{i-1}= N    
%    \end{cases}. \label{eq:qi_proof_step1}  
% \end{align}
\end{lemma}

Having established that the boosted BEHC($N$) can be reformulated as a strongly connected FSC, the next step 
%use previous results on the feedback capacity of strongly connected FSCs with SI known causally at the encoder
is to derive a single-letter $Q$-graph upper bound on $\bar{C}_{\text{BEHC}}(N)$, which is a specific instance of the general $C_{\text{fb-csi}}$, based on Fig.~\ref{fig:EH_UB_Graph}.
%Theorem~\ref{thr:FSC_Qgraph_UB} is valid in particular for $\bar{C}_{\text{BEHC}}($N$)$:
Notably, unlike the general $Q$-graph lower bound in Theorem~\ref{theorem:qgraph_LB}, no general $Q$-graph upper bound on $C_{\text{fb-csi}}$ has been established in \cite{shemuel2024finite}, primarily due to the lack of a cardinality bound on $\cU$. 
%and here we derive a single-letter $Q$-graph upper bound for $\bar{C}_{\text{BEHC}}(N)$.

\begin{theorem}[Step 2]
\label{thr:FSC_Qgraph_UB}
The capacity of the boosted BEHC($N$) is upper bounded by
\begin{align}
\label{eq:lemmaOron}
    \bar{C}_{\text{BEHC}}(N) &\le  \sup I(U^{+(N)},U^{(N)};X|Q^{(N)}),
\end{align}
where the RV $Q^{(N)}$ is defined on the vertices of the $Q$-graph illustrated in Fig.~\ref{fig:EH_UB_Graph}, 
the joint distribution is given by \eqref{eq:Qgraph_joint_distUB}
in which $P_{S^+|X,S,Q^{(N)}}$ is defined by \eqref{eq:modifiedChannelLaw},
and the supremum is taken with respect to
${\{f: \mathcal U  \times \mathcal S \to \mathcal X \; | \; f(u^+,0)=0 \; \forall u^+ \}, \; P(u^+|u,q) \in \mathcal{P}_\pi}$. 
% \begin{align}
% \label{eq:Qgraph_joint_distUB_proof_step1}
%   &P(s,u,q,x,s^+,u^+,q^+) \nn\\&= \pi(s,u,q) P(u^+|u,q) \mathbbm{1}\{x = f(u^+,s)\} \nn\\ & \quad \times P_{S^+|X,S,Q^{(N)}}(s^+|x,s,q) \mathbbm{1}\{q^+ = g(q,x)\},
%   % &P_{S^+|X,S,Q^{(N)}}(s^+|x,s,q) \triangleq 
%   % \begin{cases}
%   %   \mathbbm{1}\{s^+=1\}, & \text{if } q=N-1 \text{ and } x=0\\          
%   %   \bar{\eta} \mathbbm{1}\{s^+=s-x\} + \eta \mathbbm{1}\{s^+=1\}, & \text{otherwise}
%   %  \end{cases},
% \end{align}
%\eqref{eq:S_evol_UB_step1}, 
%and $\mathcal{P}_\pi$ is the set of input distributions $P(u^+|u,q)$ that induce a unique stationary distribution on $(S,U,Q)$, i.e., $\pi_{S,U,Q}(s,u,q)=\pi_{S^+,U^+,Q^+}(s,u,q), \; \forall s,u,q$.
%$Q$ is a RV defined on the vertices of Fig.~\ref{fig:EH_UB_Graph}, and 
%where $\cU_i=\cU$ i   
\end{theorem}

%This upper bound holds for noisy channels although we focus on the noiseless channel here.
In the third step, we demonstrate that the supremum of \eqref{eq:lemmaOron} is achieved with $f(u^+,s)$ and the set of policies $P_{U^+|U,Q}$ specified in Theorem~\ref{thr:EH_cvx_upper} with a finite cardinality $|\cU|$. 
%the $N+1$-sized $Q$-graph in Fig.~\ref{fig:EH_UB_Graph} with 
%yield a valid computable!!!! upper bound from Ineq.~\eqref{eq:lemmaOron} 
% and the joint distribution given in \eqref{eq:Qgraph_joint_distUB} rather than \eqref{eq:Qgraph_joint_dist}.
%{eq:C_var_UB}.

\begin{theorem}[Step 3]
\label{thr:cardinality}
The supremum in \eqref{eq:lemmaOron} can be achieved with $\cU=\cQ=[0:N]$, a strategy function $f(u^+,s)$ specified as in \eqref{eq:x_behc},
% \begin{align}
%     f(U^+,S)&=\begin{cases}
%             1, & \text{if } S=1 \text{ and } U^+=0\\
%             0, & \text{otherwise}
% 		   \end{cases}, \label{eq:x_behc_proof}
% \end{align}
and a policy $P_{U^+|U,Q} \in \mathcal{P}_\pi$ subject to
% \begin{align}
% P(u^+ \mid u, q) &= 0, \quad \forall q \in [0:N-1], \; \forall u \in \mathcal{U}^{(Q = q)}, \nonumber \\
% &\quad \forall u^+ \in \mathcal{U}^{+(Q = q)} \setminus \{0, u+1\}, \label{eq:policy_first_nodesProof} \\
%  P_{U^+|U,Q}(u^+|u,N)&=0, \forall u\in \cU^{(Q=N)}, \forall u^+\in [2:N], \label{eq:policy_last_node_UBProof} 
% \end{align}
\begin{align}     
    &P(u^+|u,q)=0, \quad \forall q\in [0: N-1],  \forall u\in [0:q], \nn\\ &\quad \forall u^+\in [1:q+1] \backslash \ (u+1), \label{eq:policy_first_nodes_proof}\\
    &P(u^+|u,Q=N)=0, \forall u\in [0:N], \forall u^+\in [2:N], \label{eq:policy_last_node_UB_proof}     
    %&P(U^+=u^+|U=N,Q=N)=0, \quad \text{ if } u^+ \ne 0,N.    UB 
\end{align}
yielding the following auxiliary sets: 
\begin{align}
     &\mspace{-10mu}\cU^{(Q=q)}\mspace{-5mu}=\mspace{-5mu}[0\mspace{-2mu}:\mspace{-2mu}q], \cU^{+(U=u,Q=q)}\mspace{-5mu}=\mspace{-5mu}\{0,u\mspace{-5mu}+\mspace{-5mu}1\}, \forall q\mspace{-2mu}\in\mspace{-2mu}[0\mspace{-2mu}:\mspace{-2mu}N\mspace{-3mu}-\mspace{-3mu}1], \mspace{-12mu}\label{eq:Uset_initialNodesProof} \\
     &\mspace{-10mu} \cU^{(Q=N)}=[0:N], \; \cU^{+(Q=N)}=\{0,1\}. \label{eq:Uset_lastNodeProofUB}
\end{align}
Furthermore, the induced marginal $\pi_{S|U,Q}$ is given by \eqref{eq:s_given_uq_upper}.
% $\forall q\mspace{-3mu}\in\mspace{-3mu} [0\mspace{-3mu}:\mspace{-3mu}N\mspace{-3mu}-\mspace{-3mu}1]$: $|\cU^{(Q=q)}|=q+1, |\cU^{+(Q=q)}|=q+2$, while for the last node $|\cU^{(Q=N)}|=N+1, |\cU^{+(Q=N)}|=2$. 
%$|\cU^{(q)}|=q+1, \; \forall q\in[0:N]; \; |\cU^{(q)+}|=q+2, \; q \in [0:N-1]; \; |\cU^+_{q=N}|=2$. 
%For the strategy function in \eqref{eq:x_behc}, any policy satisfying \eqref{eq:policy_first_nodes_proof} \red{and \eqref{eq:policy_last_node_UB_proof} is in $\mathcal{P}_\pi$,}
% \begin{align}    
%\end{align}        
%The special structure of the joint distribution induces
%\begin{align}
%\label{eq:batteryRelation1_UB}
% \pi(S=0|u,q)&=
%    \begin{cases}
%      \bar{\eta}^{u+1},  &\forall q\in [0:N-1], \forall u\in [0:q]\\
%      %\label{eq:batteryRelation2_UB}
%      0, & \text{if } q=N, \forall u\in [0:N]
%    \end{cases}. \label{eq:s_given_uq_upper_proof}
% \end{align}
\end{theorem} 
In conclusion, Steps~1-3 proved below establish the single-letter upper bound on $C_{\text{BEHC}}$ presented in Theorem~\ref{thr:EH_cvx_upper}. Moreover, Lemma~\ref{lem:BEHC_convex_ub_formulation}, shown in Section~\ref{subsec:EHconvex_upper}, formulates \eqref{eq:lemmaOron} as a convex optimization problem. Thus, after proving these three steps, the final task to complete the proof of Theorem~\ref{thr:EH_cvx_upper} is to establish its convergence to $C_{\text{BEHC}}$, as provided in Section~\ref{subsec:ProofThrConvergence}.

% \subsection{Interpretation}
% \label{subsection:interpretation}
% Disabled.
% Interestingly, this general structure has an intuitive interpretation. node $Q=0$ corresponds to the situation where the battery of the encoder is empty with the highest probability. All edges with $Y=1$ enter this node, meaning the encoder transmitted $X=1$. Consequently, the battery is empty, and after this transmission it is charged with probability $\eta$. Hence, for node $Q=0$ the decoder knows that the battery at the encoder is full with probability $P(S=1|Q=0)=\eta$. From the last time that the decoder received $Y=1$, the longer the sequence of $Y=0$ the greater the probability that the battery at the encoder is full; i.e. the \textit{i}'th node represents a probability $p_i$ that the battery is full, and $p_0=\eta<p_1<p_2<...<p_{N-1}<p_N$. Because the structure is constrained by the finite size of $N+1$ nodes, node $Q=N$ has an inside edge to itself. As $N\to\infty$ then $p_N\to 1$ because node $N$ means that the encoder never spent any energy by transmitting $X=1$, so $\forall \eta>0$ the probability that the battery is empty is 
%  $\lim_{N \to \infty} P(S=0|Q=N)=0$.

%The capacity of the noiseless case has been investigated in several papers \cite{ulukus2015energy,ozel2015fundamental,tutuncuoglu2017binary}, but its capacity is still unknown.

%!!!!!!!!!!!!!!!!!!!!
% \begin{theorem}
% \label{theorem:variation_cvx}
%     For any integer $N>0$, the element $b_N$ is the capacity of the BEHC with the harvesting variation after $N$ consecutive zero outputs, that is 
%     \begin{align}
%         b_N =  \bar{C}_{\text{BEHC}}.
%     \end{align}
% \end{theorem}
% The proof is established by arguing that the LHS and the RHS of \eqref{eq:BOUNDS} are equal due to the fact that the BCJR invariant property is innately satisfied, thus 
% \begin{align}
%  \bar{C}_{\text{BEHC}}(N) = \max_{\{P(u^+|u,q)\}\in\mathcal{P}_{\pi}} I(U^+,U;Y|Q^{(N)}).
% \end{align}
%!!!!!!!!!!!!!!!!!!!!

\subsubsection{Proof of Lemma~\ref{lem:C_bar_FSC} (Step 1)} 
%We can define a FSC $P_{\tilde{S}^+,Y|X,\tilde{S}}$ with a
In the new state $\tilde{S}_i\triangleq (S_i,Q_i^{(N)})$, the auxiliary RV $Q_i^{(N)}$, defined on the vertices of the $Q$-graph in Fig.~\ref{fig:EH_UB_Graph},
%is an auxiliary RV with cardinality $|\cQ|=N+1$. It 
maps the previous $N$ inputs to a final node. Specifically, $Q_{i-1}: \cX^N \to \cQ$, where $\cQ=[0:N]$ and $Q_{i-1}=g(x_{i-N}^{i-1})$. The function $g(x_{i-N}^{i-1})$ determines the resulting node as the length of consecutive $0'$s following the last $'1'$ in the previous $N$ inputs, with $g(\mathbf{0}^N)=N$. Thus, % we have%, as reflected by Fig.~\ref{fig:EH_UB_Graph}. %Alternatively, $g(\cdot)$ can be determined recursively by its previous value and the new input as given in~\eqref{eq:qi_proof_step1}.
%. Alternatively, $g(\cdot)$ can be determined recursively by its previous value and the new input as given in~\eqref{eq:qi_proof_step1}.
% \begin{table}[t]
% \caption{Function $g(\cdot)$ for general $N$.}
% \centering
% \label{table:gN}
% \begin{tabular}{||c|c|c|c||c||}
% \hline \hline
% $x_{i-N}$ & $x_{i-(N-1)}$ & $\cdots$ & $x_{i-1}$ & $q_{i-1}=g(x_{i-N}^{i-1})$ \\
% \hline \hline
% $0$ & $0$ & $\cdots$ & $0$ & $N$ \\
% \hline
% $1$ & $0$ & $\cdots$ & $0$ & $N-1$ \\
% \hline
% $\emptyset$ & $1$ & $\cdots$ & $0$ & $N-2$ \\
% \hline
% $\vvots$ & $\vvots$ & $\ddots$ & $\vvots$ & $\vvots$ \\
% \hline
% $\emptyset$ & $\emptyset$ & $\cdots$ & $1$ & $0$ \\
% \hline \hline
% \end{tabular}
% \end{table}
%From the definition of the new state, it follows that 
% its state evolution is
% \begin{align}
% \label{eq:NewFSC2}
%     P_{\tilde{S}^+,Y|X,\tilde{S}}(\tilde{s}_i,y_i|x_i,\tilde{s}_{i-1})&=P(y_i,(s_i,q_i)|(s_{i-1},q_{i-1}),x_i) \nn \\
%     &=\mathbbm{1}\{y_i=x_i\} P(s_i|x_i,s_{i-1},q_{i-1}) \mathbbm{1}\{q_i=g(q_{i-1},y_i)\} \nn\\
% &= P_{S^+,Y|X,S,Q^{(N)}}(s_i,y_i|x_i,s_{i-1},q_{i-1})\mathbbm{1}\{q_i=g(q_{i-1},y_i)\},
% \end{align}
%$S_i$ evolves as in the original BEHC FSC characterization $P_{S^+,Y|X,S}$ \eqref{eq:stateEvolutionEH} when $q^+\ne N$, and $S_i=1$ for $q^+=N$, i.e., 
\begin{align}
    &P(\tilde{s}_i,y_i|x^i,\tilde{s}_0^{i-1},y^{i-1},m)\nn\\
    &=P\left((s_i,q_i),y_i|x^i,(s_0^{i-1},q_0^{i-1}(x^{i-1})),y^{i-1},m\right) \nn\\
    &= P_{S^+|X,S,Q^{(N)}}\left(s_i|x_i,s_{i-1},q_{i-1}(x_{i-N}^{i-1})\right ) \nn\\ &\quad \times \mathbbm{1}\{q_i=g\left(q_{i-1}(x_{i-N}^{i-1}),x_i\right)\}  \mathbbm{1}\{y_i=x_i\} \nn\\
    &= P_{\tilde{S}^+,Y|X,\tilde{S}}(\tilde{s}_i,y_i|x_i,\tilde{s}_{i-1}),
\end{align}
where $P_{S^+|X,S,Q^{(N)}}$ is given in \eqref{eq:modifiedChannelLaw}.
%the evolution of $S$ is the same as of the original BEHC FSC characterization $P_{S^+|X,S}$ except for consecutive $N$ zero inputs, i.e.,
% \begin{align}
% \label{eq:S_evol_UB}
%     & P_{S^+|X,S,Q^{(N)}}(s_i|x_i,s_{i-1},q_{i-1})
%     \nn\\&=\begin{cases}
%     \scriptstyle 1, & \scriptstyle q_{i-1}=N-1 \text{ and } x_i=0      \\
%     \scriptstyle \bar{\eta} \mathbbm{1}\{s_i=s_{i-1}-x_i\} + \eta \mathbbm{1}\{s_i=1\}, & \scriptstyle \text{otherwise}    
%    \end{cases}.    
% \end{align}
This demonstrates that the FSC property~\eqref{eq:BasicFSCMarkov} holds with the new state. %i.e., $P(\tilde{s}_i,y_i|x^i,\tilde{s}_0^{i-1},y^{i-1},m) = P_{\tilde{S}^+,Y|X,\tilde{S}}(\tilde{s}_i,y_i|x_i,\tilde{s}_{i-1})$.
The connectivity property in Definition~\eqref{def:connectivity} is satisfied because any initial state $\tilde{s}'$ can be driven to any desired state $\tilde{s}$ with positive probability by transmitting the appropriate length of consecutive zeros. In conclusion, the boosted BEHC($N$) can be formulated as a strongly connected FSC with state $\tilde{S}$. \qed
%The recursive mapping in \eqref{eq:qi_proof_step1} can be represented as the $Q$-graph in Fig.~\ref{fig:EH_UB_Graph} with $|\cQ|=N+1$.
%Furthermore, $P_{S^+|X,S,Q^{(N)}}$ given in \eqref{eq:S_evol_UB} is the same one given in \eqref{eq:modifiedChannelLaw}.

Steps $2$ and $3$ incorporate an auxiliary RV 
$K_{i-1}^{(N)}\mspace{-3mu}\in\mspace{-3mu}[i\mspace{-3mu}-\mspace{-3mu}N\mspace{-3mu}:\mspace{-3mu}i\mspace{-3mu}-\mspace{-3mu}1]$ for any $i \ge N$, defined as the index of the last input '$1$' among the previous $N$ inputs, $X_{i-N}^{i-1}$, if such an input exists (\textit{Case $(i)$}), i.e., $X_{K_{i-1}}^{i-1}\mspace{-3mu}=\mspace{-3mu}(1,\mathbf{0}^{i-1-{K_{i-1}}})$. Otherwise, 
if $X_{i-N}^{i-1}=\mathbf{0}^N$ (\textit{Case $(ii)$}), we define $K_{i-1}\mspace{-3mu}\triangleq\mspace{-3mu} i\mspace{-3mu}-\mspace{-3mu}(N\mspace{-3mu}+\mspace{-3mu}1)$. 
For simplicity, we omit the superscript $^{(N)}$ and refer to $K_{i-1}$, while recalling that it depends on $N$.
Notice that there is a one-to-one mapping between $K_{i-1}$ and $Q_{i-1}$, given by $K_{i-1}\mspace{-3mu}=\mspace{-3mu}i\mspace{-3mu}-\mspace{-3mu}(Q_{i-1}\mspace{-3mu}+\mspace{-3mu}1)$.

We now proceed to prove Step 2.

\subsubsection{Proof of Theorem~\ref{thr:FSC_Qgraph_UB} (Step 2)}
Our starting point is the converse result in \cite[Eq.~(42)]{shemuel2024finite}, which states that given a fixed sequence of $(2^{nR},n)$ codes for a FSC $P_{S^+,Y|X,S}$ with feedback and SI available causally to the encoder, as depicted in Fig~\ref{fig:setting}, the achievable rate $R$ must satisfy
\begin{align}
\label{eq:UBepsilonN}
R-\epsilon_n&\le \frac{1}{n} \max_{P(u^n||y^{n-1})}
 I(U^n \to Y^n),
\end{align}
where $\epsilon_n\to 0$ as $n \to \infty$, $U_i$ indexes all possible strategies $\{f_u (s):\cS \to \cX \}$, and the joint distribution is given by
\begin{align}
    \label{eq:joint_distDI_prf}
    &P(s_0^n,u^n,x^n,y^n)=P(s_0)P(u^n||y^{n-1}) \nn\\
    &\times \prod\nolimits_{i=1}^n \mspace{-6mu}\mathbbm{1}\{x_i \mspace{-3mu}=\mspace{-3mu} 
    f_{u_i}(s_{i-1})\}P_{S^+,Y|X,S}(s_i,\mspace{-3mu} y_i|x_i,\mspace{-3mu} s_{i-1})
    \text{.}
\end{align}
%In the first part of the proof, we consider the objective of the RHS of \eqref{eq:UBepsilonN} for any $n$ and joint distribution for the boosted BEHC($N$) which
By Lemma~\ref{lem:C_bar_FSC}, the boosted BEHC($N$) is a FSC $P_{\tilde{S}^+|X,\tilde{S}}$ with state $\tilde{S}_{i-1}=(S_{i-1},Q^{(N)}_{i-1})$. Applying \eqref{eq:UBepsilonN} to this FSC with the joint distribution \eqref{eq:joint_distDI_prf}, where 
$Y=X$ and $\tilde{S}_{i-1}$ is used instead of $S_{i-1}$, we obtain the joint distribution
\begin{align}    
    &P(\tilde{s}_0^n,u^n,x^n)=P_{\tilde{S}_0}(\tilde{s}_0)P(u^n||x^{n-1}) \nn\\
    &\quad \times \prod\nolimits_{i=1}^n\mathbbm{1}\{x_i = \tilde{f}_{u_i}(\tilde{s}_{i-1})\}  P_{\tilde{S}^+|X,\tilde{S}}(\tilde{s}_i|x_i,\tilde{s}_{i-1}) \nn\\
    &\stackrel{(*)}=P_{S_0,Q^{(N)}_0}(s_0,q_0) P(\hat{u}^n||x^{n-1}) \prod\nolimits_{i=1}^n \mspace{-6mu}\mathbbm{1}\{x_i = f_{\hat{u}_i}(s_{i-1})\} \nn\\ & \times P_{S^+|X,S,Q^{(N)}}(s_i|x_i,s_{i-1},q_{i-1})  \mathbbm{1}\{q_i=g(q_{i-1},x_i)\}
    \text{.} \label{eq:joint_boosted}
\end{align}
Here, the mappings $\tilde{f}:\tilde{\cS} \to \cX$ are subject to the battery constraint, i.e., $\tilde{f}_{u_i}(s_{i-1}=0,q_{i-1})=0 \;\forall u_i, q_{i-1}$. Step ($*$) holds because $x_i = \tilde{f}_{u_i}(s_{i-1},q_{i-1})$ can be rewritten as a time-invariant function $f_{\hat{u}_i}(s_{i-1})$ with another auxiliary RV $\hat{u}_i\triangleq(u_i,q_{i-1})$, preserving the causal conditioning and the objective at any time $i$:
\begin{align}
    I(U^i;Y_i|X^{i-1})\mspace{-3mu}&=\mspace{-3mu}I(U^i,Q^{i-1};Y_i|X^{i-1})\mspace{-3mu}=\mspace{-3mu}I(\hat{U}^i;Y_i|X^{i-1}), \nn\\
    P(u_i|u^{i-1},\mspace{-3mu}x^{i-1})\mspace{-3mu}&=\mspace{-3mu}P(u_i|u^{i-1},\mspace{-3mu}q^{i-2},\mspace{-3mu}x^{i-1}) \mathbbm{1}\mspace{-3mu}\{q_{i-1}\mspace{-3mu}=\mspace{-3mu}g(x_{i-N}^{i-1})\}\nn\\
    \mspace{-3mu}&=\mspace{-3mu}P(\hat{u}_i|\hat{u}^{i-1},\mspace{-3mu}x^{i-1}). \nn
\end{align}
This follows because $q_{i-1}$ is a deterministic function of $x^{i-1}_{i-N}$. Furthermore, the Markov chain $\hat{U}_i-(\hat{U}^{i-1},Y^{i-1})-S^{i-1}$ holds similarly to the original Markov chain involving $U_i$. Consequently, considering the battery constraint ($X_i \le S_{i-1}$), \eqref{eq:joint_boosted} implies that any achievable rate must satisfy
\begin{align}
\label{eq:UBepsilonN_boosted}
R-\epsilon_n&\le \frac{1}{n} \max_{P(\hat{u}^n||x^{n-1})} I(\hat{U}^n \to X^n),
\end{align}
where the mapping $f_{\hat{u}^+}(s)$ is constrained by $f_{\hat{u}^+}(0)=0, \forall \hat{u}^+$.
%\{f: \hat{\mathcal U}  \times \mathcal S \to \mathcal X \; | \; f(\hat{u}^+,0)=0 \; \forall \hat{u}^+ \}
%where the joint distribution is
%the RHS of \eqref{eq:UBepsilonN} is a valid upper bound where the joint distribution \eqref{eq:joint_distDI_prf} is simplified to
%was formulated in the proof of Lemma~\ref{lem:C_bar_FSC} as a FSC $P_{\tilde{S}^+,Y|X,\tilde{S}}$ with state $\tilde{S}=(S,Q^{(N)})$ known causally at the encoder. 
%where ($*$) follows because $x_i = f(u_i,s_{i-1},q_{i-1})$ can be mapped identically by another function and auxiliary RV $\tilde{f}(\tilde{u}_i,s_{i-1}), \tilde{u}_i\triangleq(u_i,q_{i-1})$.
%, the cardinality of the auxiliary RV is $|\cU_i|= |\cX|^{|\cQ^{(N)}|}=2^{N+1}$ (instead of $|\cX|^{|\cS||\cQ^{(N)}|}$). %, as there are only $2$ feasible strategies out of $4$ in the set $\{f_u (s):\cS \to \cX \}$ (see Table~\ref{table:strategies}).

%$I(U^i;Y_i|X^{i-1})=I(U^i,Q^{(N)}^{i-1};Y_i|X^{i-1})=I(\tilde{U}^i;Y_i|X^{i-1})$, which follows because $q_{i-1}$ is a deterministic function of $x^{i-1}_{i-N}$. Furthermore, the causal conditioning is also preserved, i.e., $P(u_i|u^{i-1},x^{i-1})=P(u_i,q_{i-1}|u^{i-1},q^{i-2},x^{i-1})=P(\tilde{u}_i|\tilde{u}^{i-1},x^{i-1})$.
%where ($*$) follows because $q^n$ is uniquely determined given $x^{n}$, thus $x_i = f(u_i,s_{i-1},q_{i-1})$ is considered as $x_i = f(u_i,s_{i-1})$. 
%and $|\cU_i|= |\cX|^{|\tilde{\cS}|}$,
% \begin{align}
%     R-\epsilon_n &\leq  \frac{1}{n}  \max_{\substack{\{P(u_i|u_{i-1},y^{i-1})\}_{i=1}^n\\x_i=f(u_i,s_{i-1})}} \sum_{i=1}^{n} I(U_i,U_{i-1};Y_i|Y^{i-1}),  \label{UB:prev_paper_ub}
%     %\max_{P(u^n||y^{n-1}),{\{x_i=f(u_i,s_{i-1})\}}_{i=1}^n} \frac{1}{n} I(U^n \to Y^n),   \label{UB:prev_paper}
% \end{align}
% where $\epsilon_n\to 0$ as $n \to \infty$, the joint distribution is given by \eqref{eq:joint_distTrunc}, and $|\cU_i|= {\left(|\cX|^{|\cS|}\right)}^i$, where $U_i$ encompasses all possible strategies $\{f_u (s):\cS \to \cX \}$ until time $i$.

The remainder of the proof comprises two parts.
In the first part, we upper bound the objective in \eqref{eq:UBepsilonN_boosted} by a single-letter expression that depends on $n$. In the second part, we take the limit as $n\to \infty$ to derive the single-letter upper bound in~\eqref{eq:lemmaOron}. 
%\eqref{eq:Theorem_Upper}. 

For the first part, consider the directed information in \eqref{eq:UBepsilonN_boosted}:
\begin{align}
\label{eq:step_EH}
    &\frac{1}{n} \sum\nolimits_{i=1}^n I(\hat{U}^i;X_i|X^{i-1})  \nn\\
    &\stackrel{(a)}\le \frac{1}{n} \sum\nolimits_{i=1}^n H(X_i|Q_{i-1}^{(N)})-H(X_i|\hat{U}^i,X^{i-1},Q_{i-1}^{(N)})
    \nn\\
    &\stackrel{(b)}= \frac{1}{n} \sum\nolimits_{i=1}^n H(X_i|Q_{i-1}^{(N)})-H(X_i|\hat{U}_{K_{i-1}^{(N)}}^i,Q^{(N)}_{i-1})
    \nn\\     
    &\stackrel{(c)}= \frac{1}{n} \sum\nolimits_{i=1}^n I(\tilde{U}^{(N)}_i,\tilde{U}^{(N)}_{i-1};X_i|Q_{i-1}^{(N)})
    \nn\\     
    &\stackrel{(d)}\le I(\tilde{U}^{+(N)},\tilde{U}^{(N)};X|Q^{(N)}) \nn\\
    &\stackrel{(e)}\le \max_{P \in \mathcal{D}_{\frac{1}{n}}}  I(\tilde{U}^{+(N)},\tilde{U}^{(N)};X|Q^{(N)}).
\end{align}
Step (a) follows from $Q_{i-1}\triangleq g(X_{i-N}^{i-1})$ as defined by the $Q$-graph in Fig.~\ref{fig:EH_UB_Graph}, and because conditioning reduces entropy. %Table~\ref{table:gN};
Step (b) follows from Lemma~\ref{lem:boosted_markov} in Appendix~\ref{appendix:lemma_boosted}, which establishes the Markov chain $X_i-(\hat{U}_{K_{i-1}}^i,Q_{i-1})-(\hat{U}^{K_{i-1}-1},X^{i-1})$.
Step (c) introduces the auxiliary RV $\tilde{U}^{(N)}_i \mspace{-3mu} \triangleq \mspace{-3mu}\hat{U}_{K_{i-1}+1}^i$ and similarly $\tilde{U}^{(N)}_{i-1} \mspace{-3mu} \triangleq \mspace{-3mu}\hat{U}_{K_{i-1}}^{i-1}$. %(with cardinality $|\tilde{\cU}|=2^{N+1}$);
We revisit Step (c) in the next theorem by redefining $\tilde{U}_i^{(N)}$ with the auxiliary RV sets $\cU^{(Q=q)}, q\in \cQ$ given in \eqref{eq:Uset_initialNodes}, \eqref{eq:Uset_LastNodeUB}, such that the joint distribution in Step (d) is in the form of \eqref{eq:Qgraph_joint_distUB}. 
Step (d) applies Jensen's inequality, leveraging the concavity of $I(\tilde{U}^+,\tilde{U};X|Q)$ with respect to the joint distribution $P_{S,\tilde{U},Q,X,S^+,\tilde{U}^+,Q^+}$ (see Lemma~\ref{lem:BEHC_convex_ub_formulation}, Section~\ref{subsec:EHconvex_upper}) given by 
\begin{align}
&P(s,\tilde{u},q,x,s^+,\tilde{u}^+,q^+)=\overline{P}(s,\tilde{u},q)\overline{P}(\tilde{u}^+|\tilde{u},q)\nn\\ &\times \mspace{-3mu}\mathbbm{1}\mspace{-2mu}\{x\mspace{-3mu}=\mspace{-3mu}f(\tilde{u}^+\mspace{-4mu},\mspace{-3mu}s')\} P_{S^+|X,S,Q}(s^+\mspace{-3mu}|x,\mspace{-3mu}s,\mspace{-3mu}q) \mathbbm{1}\mspace{-2mu}\{q^+\mspace{-3mu}=\mspace{-3mu}g(q,\mspace{-3mu}x)\}, \nn\\
%the joint distribution inducing $I_q(U;Y|Q)$ to be 
%P(s,u,q) \tilde {P}(u^+|u,q)
&\overline{P}(s,\tilde{u},q) \triangleq \frac{1}{n} \sum\nolimits_{i=1}^n P_{S_{i-1},\tilde{U}_{i-1},Q_{i-1}}(s,\tilde{u},q), \label{eq:averageDist_EH}\\
%&=\frac{1}{n}\sum_{i=1}^n P_{S_{i-1},U_{i-1-N}^{i-1},Q_{i-1}}(s,\tilde{u},q)P_{U_{i-N}^i|U_{i-1-N}^{i-1},Q_{i-1}}(\tilde{u}^+|\tilde{u},q)\nn\\
%&=P(s,\tilde{u},q)P(\tilde{u}^+|\tilde{u},q),   \\ 
&\overline{P}(\tilde{u}^+|\tilde{u},q) \triangleq \frac{1}{n} \sum\nolimits_{i=1}^n P_{\tilde{U}_{i}|\tilde{U}_{i-1},Q_{i-1}}(\tilde{u}^+|\tilde{u},q).  \label{eq:averagePolicyDist_EH}
\end{align}
% where $U$ has the same cardinality as of all $U_i$, i.e., $|\cX|^{|\cS|}$, and
%$\cU$ is defined identically to all $\cU_i$, i.e., it is the set of all strategies mapping $s'$ to $x$ by the deterministic function $f$,
Step (e) defines $\mathcal{D}_{\epsilon}$, the set
of distributions satisfying
\begin{align}
&\mathcal{D}_{\epsilon} \triangleq \{ P_{S,\tilde{U},Q}(s,\tilde{u},q)P_{\tilde{U}^+|\tilde{U},Q}(\tilde{u}^+|\tilde{u},q) \in \mathcal{P}_{\cS \times \tilde{\cU} \times \cQ \times \tilde{\cU}} :\nn\\
&|P_{S,\tilde{U},Q}(s,\mspace{-4mu}\tilde{u},\mspace{-4mu}q) \mspace{-4mu}-\mspace{-4mu} \sum\nolimits_{s',\tilde{u}',q',x} \mspace{-15mu} P_{S,\tilde{U},Q}(s'\mspace{-2mu},\mspace{-4mu}\tilde{u}',\mspace{-4mu}q')P_{\tilde{U}^+|\tilde{U},Q}(\tilde{u}|\tilde{u}'\mspace{-3mu},\mspace{-4mu}q') \nn\\
&\mspace{-4mu}\times \mspace{-6mu} \mathbbm{1}\mspace{-3mu}\{x\mspace{-5mu}=\mspace{-5mu}f(\mspace{-2mu}\tilde{u},\mspace{-4mu}s')\} \mspace{-2mu}P_{S^+|X,S,Q}(s|x,\mspace{-4mu}s',\mspace{-4mu}q')  \mspace{-3mu} \mathbbm{1}\mspace{-3mu}\{q\mspace{-4mu}=\mspace{-4mu}g(\mspace{-2mu}q'\mspace{-3mu},\mspace{-4mu}x)\} |\mspace{-4mu}\le\mspace{-4mu} \epsilon, \mspace{-2mu}\forall s,\mspace{-4mu}\tilde{u},\mspace{-4mu}q \}. \nn
\end{align}
%where $Q_{f^*}(y|u,q)$ is fixed and given by \eqref{eq:y-uyProof}.
%and we show that 
For any codebook of length $n$, its induced averaged distribution $\overline{P}(s,\tilde{u},q,\tilde{u}^+)=\overline{P}(s,\tilde{u},q)\overline{P}(\tilde{u}^+|\tilde{u},q)$ lies in $\mathcal{D}_{\frac{1}{n}}$, as shown by the following derivation
using Definitions \eqref{eq:averageDist_EH}–\eqref{eq:averagePolicyDist_EH}:
%$|\overline{P}(s,\mspace{-4mu}\tilde{u},\mspace{-4mu}q) \mspace{-4mu}-\mspace{-4mu} \sum\nolimits_{s',\tilde{u}',q',x} \mspace{-5mu} \overline{P}(s'\mspace{-2mu},\mspace{-4mu}\tilde{u}',\mspace{-4mu}q')\overline{P}(\tilde{u}|\tilde{u}'\mspace{-3mu},\mspace{-4mu}q')\mspace{-4mu} \mathbbm{1}\mspace{-3mu}\{x\mspace{-5mu}=\mspace{-5mu}f(\mspace{-2mu}\tilde{u},\mspace{-4mu}s')\}P(s|x,\mspace{-4mu}s',\mspace{-4mu}q')  \mspace{-3mu} \mathbbm{1}\mspace{-3mu}\{q\mspace{-4mu}=\mspace{-4mu}g(\mspace{-2mu}q'\mspace{-3mu},\mspace{-4mu}x)\} |\mspace{-4mu}\le\mspace{-4mu} \frac{1}{n}, \mspace{-2mu}\forall s,\mspace{-4mu}\tilde{u},\mspace{-4mu}q$.
%%%%%%%%
%$\scriptstyle|P_{S,\tilde{U},Q}(s,u,q)- \sum_{s',\tilde{u}',q',x} P_{S,\tilde{U},Q}(s',\tilde{u}',q')P_{\tilde{U}^+|\tilde{U},Q}(\tilde{u}|\tilde{u}',q'), \mathbbm{1}\{x=f_{\tilde{u}}(s')\} P_{S^+|X,S,Q}(s|x,s',q') \mathbbm{1}\{q=g(q',x)\} | \le \frac{1}{n}$ for all $s,\tilde{u},q$, 
\begin{align}
    &\Big|\overline{P}_{S,\tilde{U},Q}(s,\mspace{-3mu}\tilde{u},\mspace{-3mu}q) \mspace{-4mu}-\mspace{-5mu} \sum\nolimits_{s',\mspace{-1mu}\tilde{u}',\mspace{-1mu}q',\mspace{-1mu}x}\mspace{-5mu} \overline{P}_{S,\tilde{U},Q}\mspace{-2mu}(s',\mspace{-2mu}\tilde{u}',\mspace{-2mu}q') \overline{P}_{\tilde{U}^+|\tilde{U},Q}\mspace{-2mu}(\tilde{u}|\tilde{u}',\mspace{-2mu}q') \nn\\ &  \quad \times  \mathbbm{1}\{x=f(\tilde{u},s')\} P_{S^+|X,S,Q}(s|x,s',q') \mathbbm{1}\{q=g(q',x)\} \Big|\nn\\
    &= \frac{1}{n} \Big| \sum\nolimits_{i=1}^{n} \big[ P_{S_{i-1},\tilde{U}_{i-1},Q_{i-1}}(s,\tilde{u},q) \nn\\
    & \;-\mspace{-4mu}\sum\nolimits_{s',\tilde{u}',q',x} \mspace{-5mu}
    P_{S_{i-1},\tilde{U}_{i-1},Q_{i-1}}(s',\mspace{-3mu}\tilde{u}',\mspace{-3mu}q')
    P_{\tilde{U}_i|\tilde{U}_{i-1},Q_{i-1}}\mspace{-3mu}(\tilde{u}|\tilde{u}',\mspace{-3mu}q')\nn\\ &  \quad \times\mathbbm{1}\{x=f(\tilde{u},s')\} P(s|x,s',q') \mathbbm{1}\{q=g(q',x)\} \big] \Big|\nn\\         
    &= \frac{1}{n} \Big| \sum\nolimits_{i=2}^{n} \big[ P_{S_{i-1},\tilde{U}_{i-1},Q_{i-1}}(s,\tilde{u},q)     
    \nn\\ &  \;
    -\mspace{-4mu} \sum\nolimits_{s',\tilde{u}',q',x}  
    \mspace{-5mu} P_{S_{i-1},\tilde{U}_{i-1},Q_{i-1}}(s',\mspace{-3mu}\tilde{u}',\mspace{-3mu}q')P_{\tilde{U}_i|\tilde{U}_{i-1},Q_{i-1}}\mspace{-2mu}(\tilde{u}|\tilde{u}',\mspace{-3mu}q') \nn\\ &  \quad \times \mathbbm{1}\{x=f(\tilde{u},s')\} P(s|x,s',q') \mathbbm{1}\{q=g(q',x)\} 
    \big] \nn\\
    &  \quad + P_{S_{0},\tilde{U}_{0},Q_{0}}(s,\tilde{u},q) - P_{S_{n},\tilde{U}_{n},Q_{n}}(s,\tilde{u},q)\Big| \le \frac{1}{n}, 
\end{align}
% \[\frac{1}{n} |P_{Y_0}(\tilde{y})-\sum\nolimits_{q,u} P_{Y_{n-1},U_n,Y_n}(q,u,\tilde{y})| \le \frac{1}{n} \max \{ P_{Y_0}(\tilde{y}),\sum\nolimits_{q,u} P_{Y_{n-1},U_n,Y_n}(q,u,\tilde{y})\} \le \frac{1}{n},
% \]
where the inequality step follows since for any $i \in [1:n]$:
\begin{align}
& \sum\nolimits_{s',\tilde{u}',q',x}         
    \mspace{-5mu} P_{S_{i-1},\tilde{U}_{i-1},Q_{i-1}}(s',\tilde{u}',q')P_{\tilde{U}_i|\tilde{U}_{i-1},Q_{i-1}}\mspace{-3mu}(\tilde{u}|\tilde{u}',q')
    \nn\\ & \times \mspace{-5mu} \mathbbm{1}\mspace{-3mu}\{x\mspace{-4mu}=\mspace{-5mu}f(\tilde{u},s')\} \mspace{-2mu} P(\mspace{-2mu}s|x,s',q') \mspace{-2mu}\mathbbm{1}\mspace{-2mu}\{q\mspace{-5mu}=\mspace{-5mu}g(\mspace{-2mu}q',\mspace{-3mu}x\mspace{-2mu})\} \mspace{-5mu}=\mspace{-5mu} P_{S_{i},\tilde{U}_{i},Q_{i}}\mspace{-3mu}(\mspace{-2mu}s,\mspace{-3mu}\tilde{u},\mspace{-3mu}q\mspace{-2mu}). \nn
\end{align}
%%%%%%%%%%%
% due to
This completes the first part of the proof. %For simplicity of notation, we use $u$ instead of $\tilde{u}$ for the remainder of the proof.

In the second part of the proof, we derive from \eqref{eq:step_EH}:
\begin{align}
    &\bar{C}_{\text{BEHC}}(N) \nn\\
    &\le \lim_{n \to \infty} \max_{{\{P(\tilde{u}_i|\tilde{u}_{i-1},q_{i-1})\}}_{i=1}^n} \mspace{-4mu}
 \frac{1}{n}  \mspace{-4mu}\sum\nolimits_{i=1}^n \mspace{-4mu}I(\tilde{U}^{(N)}_i\mspace{-3mu},\tilde{U}^{(N)}_{i-1};X_i|Q_{i-1}^{(N)})
    \nn\\
     & \le\lim_{n\to \infty}
    \max_{P \in \mathcal{D}_{\frac{1}{n}}} I(\tilde{U}^{+(N)},\tilde{U}^{(N)};X|Q^{(N)}) \nn\\
    & \stackrel{(a)}= \max_{P \in \mathcal{D}_{0}} I(\tilde{U}^{+(N)},\tilde{U}^{(N)};X|Q^{(N)}) \nn\\
    & \stackrel{(b)} = \max_{P(\tilde{u}^+|\tilde{u},q)}  I(\tilde{U}^{+(N)},\tilde{U}^{(N)};X|Q^{(N)}). \label{eq:step_stationary_EH}
\end{align}
Step (a) follows because any $P\in \mathcal{D}_0$ satisfies $P\in \cap_{n=1}^{\infty} \mathcal{D}_{\frac{1}{n}}$, as $\frac{1}{n}>0, \; \forall n\in \mathbb{N}$. 
%monotonically decreasing in $n$, 
Conversely, any $P\in \cap_{n=1}^{\infty} \mathcal{D}_{\frac{1}{n}}$ satisfies $P\in \mathcal{D}_0$
because $\frac{1}{n}$ monotonically decreases as $n\to \infty$. Thus,
$\lim_{n\to \infty} \mathcal{D}_{\frac{1}{n}} = \mathcal{D}_{0}$.
Step (b) follows from the fact that $\mathcal{D}_{0}$ represents the set of all distributions $P(\tilde{u},q,u^+)P(s|\tilde{u},q)$ inducing a stationary distribution on $(S,\tilde{U},Q)$, i.e., 
\begin{align}
    \mspace{-5mu} P_{S,\tilde{U},Q}\mspace{-2mu}(\mspace{-3mu}s,\mspace{-4mu}\tilde{u},\mspace{-4mu}q\mspace{-2mu})\mspace{-4mu}=\mspace{-4mu}P_{S^+,\tilde{U}^+\mspace{-4mu},Q^+}\mspace{-2mu}(\mspace{-3mu}s,\mspace{-4mu}\tilde{u},\mspace{-4mu}q\mspace{-2mu}), \forall s,\mspace{-4mu} \tilde{u},\mspace{-4mu} q\mspace{-4mu} \in \mspace{-4mu} \cS \mspace{-4mu} \times \mspace{-4mu} \tilde{\cU}^{(Q=q)} \mspace{-6mu} \times \mspace{-6mu} \cQ. \label{eq:stationarity_step_proof}
\end{align}
Recall that since $\cS, \cQ^{(N)}$ are finite, and $\tilde{\cU}$ will be shown to be finite in the next theorem, %is also finite ($|\cU|=2^{N+1}$),
there always exists a stationary distribution $\pi(s,\tilde{u},q)$ (not necessarily unique) for any $P(\tilde{u}^+|\tilde{u},q)$. Thus, $\mathcal{D}_{0}$ is non-empty.

Finally, the proof concludes by renaming $\tilde{U}$ in place of $U$ and demonstrating that the maximization in Step (b) can be restricted to $\mathcal{P}_\pi$, the set of $P(u^+|u,q)$ distributions that induce a unique stationary distribution. This conclusion follows from the connectivity (Definition~\ref{def:connectivity}) of the boosted BEHC($N$), and can be proven using the approach outlined in \cite[Lem.~1]{NOST}. \qed
%%%%%%%%%%%
%Note that the proof establishes a general cardinality bound $|\tilde{\cU}|\le 2^{N+1}$. However, in the next step we prove the particular cardinality bound for each node: $|\tilde{\cU}_q|\le q+1$.
%%%%%%%
% \begin{align}
% \label{eq:step_EH}
%     \frac{1}{n} \sum_{i=1}^n I(U^i;Y_i|Y^{i-1}) &\stackrel{(a)}\le \frac{1}{n} \sum_{i=1}^n H(Y_i|Q_{i-1})-H(Y_i|U^i,Y^{i-1})
%     \nn\\
%     &\stackrel{(b)}= \frac{1}{n} \sum_{i=1}^n H(Y_i|Q_{i-1})-H(Y_i|U_{i-N}^i,Q_{i-1})
%     \nn\\     
%     &\stackrel{(c)}\le I(U^+,U;Y|Q^{(N)}) \nn\\
%     &\stackrel{(d)}\le \max_{P \in \mathcal{D}_{\frac{1}{n}}}  I(U^+,U;Y|Q^{(N)}),
% \end{align}
% where in
% \begin{enumerate}[label={(\alph*)}]
% \item $Q_{i-1}\triangleq g(Y_{i-N}^{i-1})$ as given in Table~\ref{table:gN}, and the step follows since conditioning reduces entropy;
% \item follows from Lemma~\ref{lem:boosted_markov}, which is given in Appendix~\ref{appendix:lemma_boosted};
% \item the joint distribution on the RHS is\\$P(s,u,q,x,s^+,y,u^+,q^+)=P(s,u,q,u^+) \mathbbm{1}\{x=f(u^+,s')\} P_{S^+,Y|X,S}(s^+,y|x,s) \mathbbm{1}\{q^+=g(q,y)\}$, in which
% %the joint distribution inducing $I_q(U;Y|Q^{(N)})$ to be 
% \begin{align}
% %P(s,u,q) \tilde {P}(u^+|u,q)
% P(s,u,q,u^+) &\triangleq \frac{1}{n} \sum_{i=1}^n P_{S_{i-1},U_{i-N}^{i-1},Q_{i-1},U_{i-N}^i}(s,u,q,u^+)\nn\\
% &=\frac{1}{n}\sum_{i=1}^n P_{S_{i-1},U_{i-N}^{i-1},Q_{i-1}}(s,u,q)P_{U_{i-N}^i|U_{i-N}^{i-1},Q_{i-1}}(u^+|u,q)\nn\\
% &=P(s,u,q)P(u^+|u,q), \label{eq:averageDist_EH}  
% \end{align}
% % where $U$ has the same cardinality as of all $U_i$, i.e., $|\cX|^{|\cS|}$, and
% %$\cU$ is defined identically to all $\cU_i$, i.e., it is the set of all strategies mapping $s'$ to $x$ by the deterministic function $f$,
% and this step follows from the fact that
% $I(U^+,U;Y|Q^{(N)})$ is concave in the joint distribution $P_{S,U,Q^{(N)},X,S^+,Y,U^+,Q^+}$ as given by the proof of Lemma~\ref{lem:BEHC_convex_ub_formulation};
% \item the notation $\mathcal{D}_{\epsilon}$ denotes the set
% \begin{align}
% &\mathcal{D}_{\epsilon} \triangleq \{ P_{S,U,Q^{(N)}}(s,u,q)P_{U^+|U,Q}(u^+|u,q) \in \mathcal{P}_{\cS \times \cU \times \cQ^{(N)} \times \cU} :\nn\\
% &\scriptstyle|P_{S,U,Q^{(N)}}(s,u,q)- \sum_{s',u',q',x,y} P_{S,U,Q^{(N)}}(s',u',q')P_{U^+|U,Q}(u|u',q')\mathbbm{1}\{x=f(u,s')\} P_{S^+,Y|X,S}(y,s|x,s') \mathbbm{1}\{q=g(q',y)\} |\le \epsilon, \forall s,u,q \},
% \end{align}
% %where $Q_{f^*}(y|u,q)$ is fixed and given by \eqref{eq:y-uyProof}.
% %and we show that 
% where for any codebook of length $n$, its induced probability, $P(s,u,q,u^+)$, lies in $\mathcal{D}_{\frac{1}{n}}$, i.e., 
% $\scriptstyle|P_{S,U,Q^{(N)}}(s,u,q)- \sum_{s',u',q',x,y} P_{S,U,Q^{(N)}}(s',u',q')P_{U^+|U,Q}(u|u',q'), \mathbbm{1}\{x=f(u,s')\} P_{S^+,Y|X,S}(y,s|x,s') \mathbbm{1}\{q=g(q',y)\} | \le \frac{1}{n}
% $ for all $s,u,q$, because by using the definition of $P(s,u,q,u^+)$ given in \eqref{eq:averageDist_EH} we obtain
% \begin{align}
%     &\scriptstyle|P_{S,U,Q^{(N)}}(s,u,q)- \sum_{s',u',q',x,y} P_{S,U,Q^{(N)}}(s',u',q')P_{U^+|U,Q}(u|u',q')\mathbbm{1}\{x=f(u,s')\} P_{S^+,Y|X,S}(y,s|x,s') \mathbbm{1}\{q=g(q',y)\} |\nn\\
%     =&\scriptstyle \frac{1}{n} | \sum_{i=1}^{n} \left[ P_{S_{i-1},U_{i-1},Q_{i-1}}(s,u,q)- \sum_{s',u',q',x,y}         
%     P_{S_{i-1},U_{i-1},Q_{i-1}}(s',u',q')P_{U_i|U_{i-1},Q_{i-1}}(u|u',q')\mathbbm{1}\{x=f(u,s')\} P(y,s|x,s') \mathbbm{1}\{q=g(q',y)\} \right] |\nn\\         
%     =&\scriptstyle \frac{1}{n} | \sum_{i=2}^{n} \left[ P_{S_{i-1},U_{i-1},Q_{i-1}}(s,u,q)- \sum_{s',u',q',x,y}         
%     P_{S_{i-1},U_{i-1},Q_{i-1}}(s',u',q')P_{U_i|U_{i-1},Q_{i-1}}(u|u',q')\mathbbm{1}\{x=f(u,s')\} P(y,s|x,s') \mathbbm{1}\{q=g(q',y)\} 
%     \right] \nn\\
%     &\scriptstyle \quad + P_{S_{0},U_{0},Q_{0}}(s,u,q) - P_{S_{n},U_{n},Q_{n}}(s,u,q)|\nn\\             
%     \le& \scriptstyle \frac{1}{n}, 
% \end{align}
% % \[\frac{1}{n} |P_{Y_0}(\tilde{y})-\sum_{q,u} P_{Y_{n-1},U_n,Y_n}(q,u,\tilde{y})| \le \frac{1}{n} \max \{ P_{Y_0}(\tilde{y}),\sum_{q,u} P_{Y_{n-1},U_n,Y_n}(q,u,\tilde{y})\} \le \frac{1}{n},
% % \]
% which follows from $\scriptstyle \sum_{s',u',q',x,y}         
%     P_{S_{i-1},U_{i-1},Q_{i-1}}(s',u',q')P_{U_i|U_{i-1},Q_{i-1}}(u|u',q')\mathbbm{1}\{x=f(u,s')\} P(y,s|x,s') \mathbbm{1}\{q=g(q',y)\} =P_{S_{i},U_{i},Q_{i}}(s,u,q)$ for any $i \in [1:n]$. %%%%%%%%%%%
% % due to
% \end{enumerate}
% This completes the first part of the proof. In the second part of the proof, we obtain from \eqref{eq:step_EH} 
% \begin{align}
%     &\lim_{n \to \infty} \max_{{\{P(u_i|u_{i-1},q_{i-1})\}}_{i=1}^n}
%  \frac{1}{n}  \sum_{i=1}^n I(U_i,U_{i-1};Y_i|Q_{i-1})
%     \nn\\
%      &\quad \le\lim_{n\to \infty}
%     \max_{P \in \mathcal{D}_{\frac{1}{n}}} I(U^+,U;Y|Q^{(N)}) \nn\\
%     & \quad \stackrel{(a)}= \max_{P \in \mathcal{D}_{0}} I(U^+,U;Y|Q^{(N)}) \nn\\
%     & \quad \stackrel{(b)} = \max_{P(u^+|u,q)}  I(U^+,U;Y|Q^{(N)}),
% \end{align}
% where 
% \begin{enumerate}[label={(\alph*)}]
% \item follows since any $P\in \mathcal{D}_0$ satisfies $P\in \cap_{n=1}^{\infty} \mathcal{D}_{\frac{1}{n}}$ due to the fact that $\frac{1}{n}$ is positive for all $n$,
% %monotonically decreasing in $n$, 
% and vice versa, i.e., any $P\in \cap_{n=1}^{\infty} \mathcal{D}_{\frac{1}{n}}$ satisfies $P\in \mathcal{D}_0$
% because $\frac{1}{n}$ monotonically decreases in $n$; hence,
% $\lim_{n\to \infty} \mathcal{D}_{\frac{1}{n}} =  \mathcal{D}_{0}$;
% \item follows since $\mathcal{D}_{0}$ implies the set of all $P(s,u,q)P(u^+|u,q)$ that have a stationary distribution of $(S,U,Q^{(N)})$, i.e., $P_{S,U,Q^{(N)}}(s,u,q)=P_{S^+,U^+,Q^+}(s,u,q), \forall s,u,q$; recall that since $\cS, \cQ^{(N)}$ are finite and $\cU$ is claimed to be finite in Theorem~\ref{thr:cardinality}, there always exists a stationary $\pi(s,u,q)$ distribution (not necessarily unique) with regard to any $P(u^+|u,q)$, thus $\mathcal{D}_{0}$ is non-empty.
% \end{enumerate}
% This concludes the proof \qed

We conclude this section with the proof of Step 3.

%\eqref{eq:stateEvolutionEH}
\subsubsection{Proof of Theorem~\ref{thr:cardinality} (Step 3)} 
%In this step, we prove validity of the policy structure and the cardinality bound for each node.
We establish the cardinality bounds on $|\tilde{\cU}|$ and $|\tilde{\cU}^+|$, which are identical by the stationary distribution $\pi(s,\tilde{u},q)$ \eqref{eq:stationarity_step_proof}. We also validate the optimality of the policy structure by analyzing the transformations of the auxiliary RVs $U \Rightarrow \hat{U}\Rightarrow \tilde{U}$ throughout the proof of Theorem~\ref{thr:FSC_Qgraph_UB}. 
%In Inequality~\eqref{eq:UBepsilonN}, $U_i$ indexes the set of functions (strategies) $\{f_u (\tilde{s}):\tilde{\cS} \to \cX \}$, and it was proved in \cite[App.~I]{shemuel2024finite} that $|\cU|\le|\cX|^{|\cS|}$.
Initially, the set of strategies satisfies $|\cU|\le|\cX|^{|{\cS}| |\cQ^{(N)}|}$, where $|\cQ^{(N)}|\mspace{-3mu}=\mspace{-3mu}N\mspace{-2mu}+\mspace{-2mu}1$. In \eqref{eq:joint_boosted}, Step~$(*)$ introduces $\hat{u}_i\triangleq(u_i,q_{i-1})$, mapping $x_i=f_{\hat{u}_i}(s_{i-1})$, which reduces $|\hat{\cU}|$ to $|\cX|^{|\cS|}$. 
% We note that the proof of the cardinality bound $|\cX|^{|\cS|}$ holds also for $|\hat{\cU}|$ regarding the boosted BEHC($N$) by modifying $P(y_i|u^{i},y^{i-1})$ in \cite[Eq.~(67)]{shemuel2024finite} to
% \begin{align}
% &P(x_i|u^{i},x^{i-1})\nn\\
% &= \sum\limits_{\tilde{s}_{i-1}}  
%     \frac{\sum\limits_{\tilde{s}^{i-2}} \left(\prod_{j=1}^{i-1} \mathbbm{1}\{x_j=f(u_j,\tilde{s}_{j-1})\} P_{\tilde{S^+}|X,\tilde{S}}(\tilde{s}_j|x_j,\tilde{s}_{j-1}) \right)
% \mathbbm{1}\{x_i=f(u_i,\tilde{s}_{i-1})\}
% }{\sum\limits_{\tilde{s}^{i-2}}\left(\prod_{j=1}^{i-2} \mathbbm{1}\{x_j=f(u_j,\tilde{s}_{j-1})\} P_{\tilde{S^+}|X,\tilde{S}}(\tilde{s}_j|x_j,\tilde{s}_{j-1})\right) 
% \mathbbm{1}\{x_{i-1}=f(u_{i-1},\tilde{s}_{i-2})\}} \nn\\ %\label{eq:yGivenPast2} 
% &= \sum\limits_{\tilde{s}_{i-1}}  
% \frac{\sum\limits_{\tilde{s}^{i-2}} \left(\prod_{j=1}^{i-1} \mathbbm{1}\{x_j=\tilde{f}(\hat{u}_j,s_{j-1})\} P_{\tilde{S^+}|X,\tilde{S}}(\tilde{s}_j|x_j,\tilde{s}_{j-1}) \right)
% \mathbbm{1}\{x_i=\tilde{f}(\hat{u}_i,s_{i-1})\}
% }{\sum\limits_{\tilde{s}^{i-2}}\left(\prod_{j=1}^{i-2} \mathbbm{1}\{x_j=\tilde{f}(\hat{u}_j,s_{j-1})\} P_{\tilde{S^+}|X,\tilde{S}} (\tilde{s}_j|x_j,\tilde{s}_{j-1})\right) 
% \mathbbm{1}\{x_{i-1}=\tilde{f}(\hat{u}_{i-1},s_{i-2})\}} \nn\\
% &=P(x_i|\hat{u}^{i},x^{i-1}),
% \label{eq:yGivenPast2} 
% \end{align}
% %where
% % \begin{enumerate}[label={(\alph*)}]
% % \item follows from the definition of $\tilde{s_j}\triangleq (s_j,q_j)$ and 
% % \end{enumerate}
% where $P_{\tilde{S}^+|X,\tilde{S}}$ is given in \eqref{eq:NewFSC}; which follows due to the definitions of $\tilde{s}_i\triangleq (s_{i-1},q_{i-1}), \hat{u}_i$ and because $q_{i-1}$ is a deterministic function of $x^{i-1}_{i-N}$.
%%%%%%%
%\cite[App.~I]{shemuel2024finite} that 
%\cite[Eq.~(42)]{shemuel2024finite}
%where ($*$) follows because $x_i = f(u_i,s_{i-1},q_{i-1})$ can be mapped identically by another function and auxiliary RV $\tilde{f}(\tilde{u}_i,s_{i-1}), \tilde{u}_i\triangleq(u_i,q_{i-1})$ such that the causal conditioning and the objective are preserved at any time $i$, i.e.,
%%%%%%%%
%$q_{i-1}$ is a deterministic function of $x^{i-1}_{i-N}$.
However, due to the battery constraint $X_i\mspace{-3mu}=\mspace{-3mu}0$ when $S_{i-1}\mspace{-3mu}=\mspace{-3mu}0$, the cardinality of this auxiliary RV further reduces to $|\hat{\cU}_i|\mspace{-3mu}=\mspace{-3mu} |\cX|^{|\cS|-1}\mspace{-3mu}=\mspace{-3mu}2$, as there are only two feasible strategies out of four in the set $\{f_{\hat{u}} (s):\cS \to \cX \}$, as shown in Table~\ref{table:strategies_EH}. Specifically, strategy $\hat{u}^+\mspace{-3mu}=\mspace{-3mu}a$ corresponds to always transmitting $X\mspace{-3mu}=\mspace{-3mu}0$, regardless of the battery state, while strategy $\hat{u}^+\mspace{-3mu}=\mspace{-3mu}b$ represents an attempt to transmit $X\mspace{-3mu}=\mspace{-3mu}1$, which succeeds only if the battery is charged.

%(see Table~\ref{table:strategies_EH}).
%For the remaining of the proof, we replace $\hat{U}$ with $\tilde{U}$.
%$q_{i-1}$ is a deterministic function of $x^{i-1}_{i-N}$.

% \begin{table}[h]
% %\caption{All the strategies of binary input and binary state alphabets, $\cS=\cX=\{0,1\}$. For the BEHC, only strategies $\hat{u}=a$ and $\hat{u}=b$ are feasible, as strategies $\hat{u}=c$ and $\hat{u}=d$ are illegal.}
% \centering
% \label{table:strategies_EH}
% \begin{tabular}[b]{||c|c|c|c||}
% \hline \hline
% $x=f_{\hat{u}^+}(s)$ & $s=0$ & $s=1$ \\
% \hline \hline
% $\hat{u}^+=a$ \text{ (feasible)} & $0$ & $0$ \\
% \hline
% $\hat{u}^+=b$ \text{ (feasible)}& $0$ & $1$ \\
% \hline
% $\hat{u}^+=c$ \text{ (infeasible)}& $1$ & $0$ \\
% \hline
% $\hat{u}^+=d$ \text{ (infeasible)}& $1$ & $1$ \\
% \hline \hline
% \end{tabular}
% \end{table}

\begin{table}[t]
\centering % Center the table in the flow of the paper
\begin{tabular}[b]{||l|c|c||} % Left-align the first column
\hline \hline
$x=f_{\hat{u}^+}(s)$ & $s=0$ & $s=1$ \\
\hline \hline
$\hat{u}^+=a$ (feasible -- always transmit $'0'$) & $0$ & $0$ \\
\hline
$\hat{u}^+=b$ (feasible -- attempt to transmit $'1'$) & $0$ & $1$ \\
\hline
$\hat{u}^+=c$ (infeasible) & $1$ & $0$ \\
\hline
$\hat{u}^+=d$ (infeasible) & $1$ & $1$ \\
\hline \hline
\end{tabular}
\caption{Feasibility of strategies under the battery constraint.}
\label{table:strategies_EH}
\end{table}

Now, we revisit Step (c) in \eqref{eq:step_EH}, which states
\begin{align}
    I(\hat{U}_{K_{i-1}}^i; X_i | Q_{i-1})    
    \mspace{-3mu}=\mspace{-3mu}I(\tilde{U}_i,\tilde{U}_{i-1};X_i|Q_{i-1}), \,i\in[1\mspace{-3mu}:\mspace{-3mu}n], \label{eq:StepC_revisit}
\end{align}
where $\tilde{U}_{i-1} \mspace{-3mu} \triangleq \mspace{-3mu}\hat{U}_{K_{i-1}}^{i-1}$, and redefine $\tilde{U}_{i-1}$ to establish the auxiliary sets in 
\eqref{eq:Uset_initialNodesProof}–\eqref{eq:Uset_lastNodeProofUB}.
% \eqref{eq:policy_first_nodes_proof} and \eqref{eq:policy_last_node_UB_proof}.
%In the next theorem, we revisit Step (c) by redefining $\tilde{U}_i^{(N)}$ with the auxiliary RV sets $\cU^{(Q=q)}, q\in \cQ$ given in \eqref{eq:Uset_initialNodes}–\eqref{eq:Uset_LastNodeUB}.
We analyze each of the two cases corresponding to whether $Q_{i-1}\mspace{-3mu}\in\mspace{-3mu} [0\mspace{-3mu}:\mspace{-3mu}N\mspace{-3mu}-\mspace{-3mu}1]$ or $Q_{i-1}\mspace{-3mu}=\mspace{-3mu}N$.
%, separately.

For Case $(i)$ ($X_{{K}_{i-1}}^{i-1}=(1,\mathbf{0}^{i-1-{K_{i-1}}})$), corresponding to nodes $Q_{i-1}\mspace{-3mu}\in\mspace{-3mu} [0\mspace{-3mu}:\mspace{-3mu}N\mspace{-3mu}-\mspace{-3mu}1]$, we employ the following lemma.

%denote by $k\in[i-N:i-1]$ the index of the last output '$1$' among the previous $N$ outputs $y_{i-N}^{i-1}$ (assuming it exists)
\begin{lemma}
\label{lem:tryTransmit1}
For the boosted BEHC($N$) at time $i$, given $x^{i-1}$ such that $x_{i-N}^{i-1}\mspace{-3mu}\neq \mspace{-3mu}\mathbf{0}^N$, and given $\hat{u}^i$ containing a component $\hat{u}_l\mspace{-4mu}=\mspace{-4mu}b$ for some time index $l\in [k_{i-1}(x_{i-N}^{i-1}):i\mspace{-3mu}-\mspace{-3mu}1]$, 
%in Table~\ref{table:strategies_EH}),
the following holds: 
$P(x_i|\hat{u}^i,x^{i-1})=P(x_i|\hat{u}_l^i,q_{i-1}(x_{i-N}^{i-1}))$.
%such that $f_{\hat{u}_l}(s_{l-1}=1)=1$,
%input distribution $\{P(u_i|u^{i-1},y^{i-1})\}_{i=1}^N$ of the form in which there
%\begin{align}
%    P(x_i|\hat{u}^i,x^{i-1})=P(x_i|\hat{u}_l^i,x_{l}^{i-1},q_{i-1}(x_{i-N}^{i-1})). \label{eq:yGivenPastEH}
%\end{align}
\end{lemma}

The proof of Lemma~\ref{lem:tryTransmit1} appears in Appendix~\ref{Appendix:prf_lem_tryTransmit1}. From this lemma, we deduce that for any time $i$, node $q_{i-1} \in [0:N-1]$ and vector $\hat{u}^i$, we only need to consider the maximal time index $l\in [k\mspace{-3mu}:\mspace{-3mu}i\mspace{-3mu}-\mspace{-3mu}1]$ for which $\hat{u}_l=b$, while all previous strategies $\hat{u}^{l-1}$ can be ignored as they do not affect $I(\hat{U}_{K_{i-1}}^i; X_i | Q_{i-1}=q_{i-1})$.
%LHS of \eqref{eq:StepC_revisit}.
Thus, defining $m \mspace{-3mu}\triangleq \mspace{-3mu} \max \{l \mspace{-3mu} \in  \mspace{-3mu} [k_{i-1}\mspace{-3mu} :\mspace{-3mu} i\mspace{-3mu}-\mspace{-3mu}1] \;| \; \hat{u}_{l}\mspace{-3mu}=\mspace{-3mu} b \}$, we obtain $\hat{u}_m^{i-1}\mspace{-3mu}=\mspace{-3mu}(b,\mspace{-2mu}a,\mspace{-2mu}a,\mspace{-2mu}\dots,\mspace{-2mu}a)$. Due to the support of $m$, there are $i\mspace{-3mu}-\mspace{-3mu}k_{i-1}=q_{i-1}+1$ possible combinations for $\hat{u}_m^{i-1}$. Hence, we establish a one-to-one mapping 
to $[0\mspace{-3mu}:\mspace{-3mu}q_{i-1}]$ via $\tilde{u}_{i-1}\mspace{-3mu}\triangleq \mspace{-3mu} i\mspace{-3mu} -\mspace{-3mu} 1\mspace{-3mu} -\mspace{-3mu} m$. We then define $\tilde{u}_i=\mathbbm{1}\{\hat{u}_i=a\} (\tilde{u}_{i-1}\mspace{-3mu}+\mspace{-3mu}1)$, and map $X_i$ via
\begin{align}
     \tilde{f}(\tilde{U}_i,S_{i-1})&=S_{i-1} \mathbbm{1}\{\tilde{U}_i^+=0\}.
    % \begin{cases}
    %         1, & \text{if } S_{i-1}=1 \text{ and } \tilde{U}_i=0\\
    %         0, & \text{otherwise}
		  %  \end{cases}.
    \label{eq:f_tilde}
\end{align}
Since there is a one-to-one mapping between $\hat{U}_{K_{i-1}}^i$ and $(\tilde{U}_i,\tilde{U}_{i-1})$ given $q_{i-1}$, we obtain $I(\hat{U}_{K_{i-1}}^i; X_i | Q_{i-1}=q_{i-1})=I(\tilde{U}_i,\tilde{U}_{i-1}; X_i | Q_{i-1}=q_{i-1})$.
This establishes $\tilde{\cU}=[0:N]$ and \eqref{eq:policy_first_nodes_proof}, and yields the auxiliary sets in \eqref{eq:Uset_initialNodesProof}.
%$\tilde{u}_i=i-\max \{j\in [k:i] \;| \; f_{\hat{u}_{j}}(s_{j-1}=1)=1\}$
%as shown in Table~\ref{table:uMarkov}
% with cardinality dependent on the node $q_{i-1} \in [0:N-1]$, i.e., 
% \begin{align}
% |\tilde{\cU}^{(Q=q)+}|&\triangleq |\tilde{\cU}_{i}|=i-k+1=q_{i-1}+2, \nn\\
% |\tilde{\cU}^{(Q=q)}|&\triangleq |\tilde{\cU}_{i-1}|=i-k=q_{i-1}+1, \nn
% \end{align}
% \begin{align}
%      f(U^+,S)&=S \mathbbm{1}\{U^+=0\} ,
%     % \begin{cases}
%     %         1, & \text{if } S=1 \text{ and } U^+=0\\
%     %         0, & \text{otherwise}
% 		  %  \end{cases},
%           \label{eq:x_behc}
% \end{align}
%$\tilde{f}(\tilde{u},s)=$ is given by \eqref{eq:x_behc}
% Furthermore, assume $\tilde{u}_{i-1}\mspace{-3mu}\mspace{-3mu}=\mspace{-3mu}\mspace{-3mu}j, j\in [0:i-k]$. If $f(\tilde{u}_i,s_{i-1}\mspace{-3mu}=\mspace{-3mu}1)\mspace{-3mu}=\mspace{-3mu}0$, we obtain $\tilde{u}_{i}\mspace{-3mu}=\mspace{-3mu}j+1$. Otherwise if $f(u_i,s_{i-1}\mspace{-3mu}=\mspace{-3mu}1)\mspace{-3mu}=\mspace{-3mu}1$, we obtain $\tilde{u}_{i}\mspace{-3mu}=\mspace{-3mu}0$. This establishes \eqref{eq:policy_first_nodes_proof} and \eqref{eq:Uset_initialNodesProof} 
% %which forbid transitions to any $\tilde{U}^+ \ne 0, \tilde{U}+1$.

For Case $(ii)$ ($X_{i-N}^{i-1}\mspace{-3mu}=\mspace{-3mu}\mathbf{0}^N$), corresponding to $Q_{i-1}\mspace{-3mu}=\mspace{-3mu}N$, we obtain
$P\mspace{-1mu}(x_i|\hat{u}^i\mspace{-3mu},\mspace{-3mu}x^{i-N-1}\mspace{-3mu},\mspace{-3mu}x_{i-N}^{i-1}\mspace{-4mu}=\mspace{-4mu}\mathbf{0}^N)\mspace{-4mu}=\mspace{-4mu}P\mspace{-1mu}(x_i|\hat{u}_i\mspace{-1mu},\mspace{-2mu}q_{i-1}\mspace{-2mu}=\mspace{-2mu}N)$ (see derivation in \eqref{eq:opt2}, Appendix~\ref{appendix:lemma_boosted}). Thus, $I(\hat{U}_{K_{i-1}}^i; X_i | Q_{i-1}=N)=\mspace{-3mu}I(\hat{U}_{i};X_i|Q_{i-1}\mspace{-3mu}=\mspace{-3mu}N)$, as only $\hat{U}_i$ is relevant. Consequently, we redefine $\tilde{u}_i=\hat{u}_i$, relabeling $(a,b) \mspace{-3mu} \Leftrightarrow \mspace{-3mu}(0,1)$, without requiring $\tilde{u}_{i-1}$. That is, we set $|\tilde{\cU}^{(Q=N)+}\mspace{-2mu}|\mspace{-6mu}\triangleq\mspace{-6mu} 2$ and $|\tilde{\cU}^{(Q=N)}\mspace{-2mu}|\mspace{-6mu}=\mspace{-6mu}0$.
%We note that although for $(q\mspace{-3mu}=\mspace{-3mu}N-1,x\mspace{-3mu}=\mspace{-3mu}0)$, we have $q^+\mspace{-3mu}=\mspace{-3mu}g(q,x)\mspace{-3mu}=\mspace{-3mu}N$ and $|\tilde{U}^{+(Q=N-1)}|\mspace{-3mu}=\mspace{-3mu}|\tilde{U}^{(Q=N)}|\mspace{-3mu}=\mspace{-3mu}2$, we intentionally set $|\tilde{U}^{(Q=N-1)+}|\mspace{-3mu}=\mspace{-3mu}|\tilde{U}^{(Q=N)}|\mspace{-3mu}=\mspace{-3mu}N+1$ to establish stronger similarity with the lower bound  in Theorem~\ref{thr:EH_cvx_lower}. This ensures that the difference between the lower and upper bounds arises only at the last node $Q\mspace{-3mu}=\mspace{-3mu}N$, a key observation used in Theorem~\ref{thr:convergence}. This establishes \eqref{eq:policy_last_node_UB_proof} and \eqref{eq:Uset_lastNodeProofUB}.
However, we set $\tilde{\cU}^{(Q=N)}\mspace{-3mu}=\mspace{-3mu}[0\mspace{-3mu}:\mspace{-3mu}N]$ to align with $\tilde{\cU}^{+(Q=N-1)}\mspace{-3mu}=\mspace{-3mu}[0\mspace{-3mu}:\mspace{-3mu}N]$, since node $Q^+\mspace{-3mu}=\mspace{-3mu}N$ is reached by $g(Q\mspace{-3mu}=\mspace{-3mu}N\mspace{-3mu}-\mspace{-3mu}1,X\mspace{-3mu}=\mspace{-3mu}0)$. Additionally, stationarity on $(S,\tilde{U},Q)$ must be preserved. Hence, \eqref{eq:policy_last_node_UB_proof} and \eqref{eq:Uset_lastNodeProofUB} are established.

Considering the redefinitions of $\tilde{U}_i,\tilde{U}_{i-1}$ in both cases, along with mapping $X_i$ via \eqref{eq:f_tilde}, we conclude that \eqref{eq:StepC_revisit} holds. The resulting joint distribution $P(s_0^n,\tilde{u}^n,x^n,q^n)$ is given by 
\begin{align}
&P_{S_0,Q_0}(s_0,q_0) \prod\nolimits_{i=1}^n P(\tilde{u}_i|\tilde{u}_{i-1},q_{i-1}) \mspace{-2mu}\mathbbm{1}\{x_i = \tilde{f}(\tilde{u}_i,s_{i-1})\nn\\ &\times P_{S^+|X,S,Q}(s_i|x_i,s_{i-1},q_{i-1})  \mathbbm{1}\{q_i=g(q_{i-1},x_i)\}
    \text{.} \label{eq:joint_boostedProofCard}
     %P_{S_0,Q^{(N)}_0}(s_0,q_0) \prod\nolimits_{i=1}^n P(\tilde{u}_i|\tilde{u}_{i-1},q_{i-1}) \mspace{-6mu}\mathbbm{1}\{x_i = \tilde{f}(\tilde{u}_i,s_{i-1})P_{S^+|X,S,Q^{(N)}}(s_i|x_i,s_{i-1},q_{i-1})  \mathbbm{1}\{q_i=g(q_{i-1},x_i)\}
    %\text{.} \label{eq:joint_boosted}
\end{align}
Thus, the policy structure and auxiliary RV sets are established, and \eqref{eq:step_EH}, Step (c), follows with \eqref{eq:joint_boostedProofCard}.
%\eqref{eq:Qgraph_joint_distUB} given $Q$ (renaming $\tilde{U}$ by $U$) 

To maintain consistency with the theorem's notation, we rename $\tilde{U}_i, \tilde{f}$ by $U_i, f$, respectively. It remains to show that the policy structure induces the marginal $\pi_{S|U,Q}$ in \eqref{eq:s_given_uq_upper}. The proof follows by induction, as in Lemma~\ref{lem:marginal_s_given_uq}, except for the last node $Q=N$, because for any $Q\in[0:N-1]$, the conditionals $P_{S^+|,X,S,Q}$ and $P_{S^+|X,S}$ are equal by \eqref{eq:modifiedChannelLaw}. 

Node $Q^+=N$ is reached either by $g(Q\mspace{-3mu}=\mspace{-3mu}N-1,X\mspace{-3mu}=\mspace{-3mu}0)$ or by $g(Q\mspace{-3mu}=\mspace{-3mu}N,X\mspace{-3mu}=\mspace{-3mu}0)$. Hence, for any $u$:
\begin{align}
    &P_{S|U,Q}(0|u,N)\stackrel{(a)}=P_{S^+|U^+,Q^+}(0|u,N)\nn\\
    &\stackrel{(b)}=P_{S^+|U^+,Q^+,X}(0|u,N,0) \nn\\
    &=\sum\nolimits_{q\in\{N-1,N\},s} P_{Q,S,S^+|U^+,Q^+,X}(q,s,0|u,N,0)\nn\\
    &\stackrel{(c)}=\mspace{-22mu}\sum_{q\in\{N-1,N\},s} \mspace{-27mu}P_{Q,S|U^+,Q^+,X}(q,s|u,N,\mspace{-3mu}0)P_{S^+|X,S,Q}(0|0,\mspace{-3mu}s,\mspace{-3mu}q)\mspace{-3mu}\stackrel{(d)}= 0. \nn 
\end{align}
Step (a) follows from the stationary distribution. Step (b) follows because $Q^+=N$ is reached only by $X=0$, as it has no incoming edge $'X=1'$. Step (c) follows from the Markovity of $P_{S^+|X,S,Q}$ implied by the joint distribution \eqref{eq:modifiedChannelLaw}. Step (d) follows from 
\eqref{eq:modifiedChannelLaw}.
% For $u=0$, we obtain for any $q \ne N$:
% $\pi_{S|U,Q}(0|0,q)=\bar{\eta}$ (see explicit derivation in \eqref{eq:P_s_given_uy} in Appendix~\ref{Appendix:prf_lem_tryTransmit1}).
% Assume for any $u>0$ and $q \ne N$:
% $\pi(S=0|u,q)= \bar{\eta}^{u+1}$. We obtain for any $q^+\ne N$: $P_{S^+|U^+,Q^+}(0|u+1,q^+)= \pi_{S|U,Q}(0|u,q) \bar{\eta}=\bar{\eta}^{u+2}=\bar{\eta}^{u^+ +1}$. For $q=N$ we readily have 
% $N$ consecutive zeros outputs, thus $\pi_{S|U,Q}(0|u,N)=0, \forall u$. 
This yields $P_{S|U,Q}(0|u,N)=0$, completing the proof.
\qed

\begin{remark}
    The proof of Theorem~\ref{thr:cardinality} provides an intuitive interpretation of the auxiliary RV $U_{i-1}$: it represents the time elapsed since the encoder last \textit{attempted} to transmit a $'1'$, with success depending on whether the battery was charged. The policy $P_{U_i|U_{i-1},Q_{i-1}}(u^+|u,q), u^+\in \{0,u+1\}$ describes the likelihood of attempting another $'1'$ ($u^+=0$) versus deliberately transmitting a $'0'$ ($u^+=u+1$). This aligns with $\pi(S=0|u,q)$ in \eqref{eq:s_given_uq_upper}: if the last attempt was $u$ steps ago, the probability of an empty battery is $\bar{\eta}^{u+1}$, unless all $N$ previous transmission were zeros, ensuring a charged battery.
\end{remark}

We now proceed to prove the convergence of both the upper and lower bound sequences to $C_{\text{BEHC}}$.
%To conclude the proof  The proof of convergence of the upper bound is provided in Section~\ref{subsec:ProofThrConvergence}.

%where $f(u^+,s)$ is given by \eqref{eq:x_behc}, the policy $P(u^+|u,q)$ is subject to \eqref{eq:policy_first_nodes} and \eqref{eq:policy_last_node_UB}, altogether inducing constant marginal $\pi_{S|U,Q}$ given by
%$\tilde{U}_i \triangleq U_{i-l+1}^i$
%and thereby \eqref{eq:yGivenPastEH} follows.
%(Ineq. \eqref{eq:step_EH} and Ineq. \eqref{eq:step_stationary_EH} follow with the same steps as before, yet with the new auxiliary RV defined).
%which follows from the transformation $k=i-(1+q_{i-1})$.

\subsection{Proof of Bounds Convergence to 
\label{subsec:ProofThrConvergence}
$C_{\text{BEHC}}$}
To complete the proofs of Theorems~\ref{thr:EH_cvx_lower} and~\ref{thr:EH_cvx_upper}, we introduce the following theorem.

\begin{theorem}
\label{thr:convergence}
The sequences of solutions to the convex optimization problems for the lower and upper bounds converge to $C_{\text{BEHC}}$ as $N$ tends to infinity.
\end{theorem}

Let $a_N$ and $b_N$ denote the solutions of the convex optimization problems for the lower and upper bounds in \eqref{optProb_EH_LB} and \eqref{optProb_EH_UB}, respectively. Thus, we aim to prove that 
\begin{align}
\label{eq:thrConvergence}
    \lim_{N\to \infty} a_N= C_{\text{BEHC}}=\lim_{N\to \infty} b_N.
\end{align}

\begin{proof}
We divide the proof into two parts. In the first part, we prove that both sequences converge to the same limit denoted by $J$, i.e.,
\begin{align}
    \lim_{N\to \infty} a_N = \lim_{N\to \infty} b_N=J. \label{eq:conv_part1}
\end{align}
In the second part of the proof, for any $N\ge0$, we have 
$a_N \le C_{\text{BEHC}} \le \bar{C}_{\text{BEHC}}(N) \le b_N$,
and define the constant sequence $c_N\triangleq C_{\text{BEHC}}$ to obtain $a_N \le c_N \le b_N$.
Taking the limit as $N\to \infty$, and using \eqref{eq:conv_part1} alongside the squeeze theorem, we find $\lim_{N\to\infty} c_N=J$. Since $c_N$ is constant, it follows that 
$\lim_{N\to\infty} c_N=C_{\text{BEHC}}$, which implies $J=C_{\text{BEHC}}$, thus concluding the proof. Therefore, it remains to prove \eqref{eq:conv_part1}.

For the sequence of solutions to the convex optimization problems \eqref{optProb_EH_UB} for the lower bound, we denote $\underline{I}_{\underline{P}^*}(U^{+(N)},U^{(N)};X|Q^{(N)})= a_N$, representing the conditional mutual information induced by an optimal joint distribution $\underline{P}^*_{S,U,Q,X,S^+,U^+,Q^+}$.
Similarly, for the sequence of solutions to the convex optimization problems \eqref{optProb_EH_UB} for the upper bound, we denote $\bar{I}_{\bar{P}^*}(U^{+(N)},U^{(N)};X|Q^{(N)})=b_N$,
representing the conditional mutual information induced by an optimal joint distribution $\bar{P}^*_{S,U,Q,X,S^+,U^+,Q^+}$. We construct a joint distribution $\tilde{P}_{S,U,Q^,X,S^+,U^+,Q^+}$
that satisfies all the constraints for a valid lower bound in Theorem~\ref{thr:EH_cvx_lower}.
%, i.e., is distributed according to \eqref{eq:Qgraph_joint_dist}, subject to Constraints \eqref{eq:constraints_stationary}–\eqref{eq:constraint_PMF} of the lower bound optimization problem \eqref{optProb_EH_LB}, 
$\tilde{P}$ induces the lower bound $\underline{I}_{\tilde{P}}(U^{+(N)},U^{(N)};X|Q^{(N)})$, which represents the conditional mutual information based on the $N+1$ sized $Q$-graph in Fig.~\ref{fig:EH_LB_Graph}. The construction mirrors the policy of $\bar{P}^*$, which corresponds to the upper bound, for any node $q\in [0:N-1]$, i.e.,
\begin{align}
    \label{eq:mimicPolicy}
    \mspace{-9mu} \tilde{P}_{U^+|U,Q}(\mspace{-2mu}u^+\mspace{-4mu}|u,\mspace{-4mu}q)\mspace{-4mu}=\mspace{-4mu} \bar{P}^*_{U^+|U,Q}(u^+\mspace{-4mu}|u,\mspace{-4mu}q),  q\mspace{-4mu}\in\mspace{-4mu} [0\mspace{-4mu}:\mspace{-4mu}N\mspace{-4mu}-\mspace{-4mu}1],\mspace{-4mu} \,\forall u,\mspace{-4mu} u^+\mspace{-4mu}, 
\end{align}
while for the last node, $Q=N$, it satisfies Constraint \eqref{eq:policy_last_node}, ensuring a valid joint distribution for the lower bound. This construction induces that $\tilde{P}(x,u,u^+|q)=\bar{P}^*(x,u,u^+|q)$ for all $q \in [0:N-1]$ because
\begin{align}
\label{eq:joint_mimicUB}
    & \tilde{P}(x,u,u^+|q)=\sum\nolimits_{s} \tilde{P}(s,x,u,u^+|q) \nn\\
    &=\sum\nolimits_{s} \tilde{P}(u|q) \pi_{S|U,Q}(s|u,q)\tilde{P}(u^+|u,q) \mathbbm{1}\{x = f(u^+,s)\}     \nn\\
    &\stackrel{(*)}=\sum\nolimits_{s} \bar{P}^{*}(u|q) \pi_{S|U,Q}(s|u,q)\bar{P}^{*}(u^+|u,q) \mathbbm{1}\{x = f(u^+,s)\}     \nn\\
    & =\bar{P}^*(x,u,u^+|q), 
\end{align}
where Step $(*)$ follows from \eqref{eq:mimicPolicy}, the fact that both distributions share the same function $f(U^+,S)$ as given in \eqref{eq:x_behc}, and the same conditional stationary distribution $\pi_{S|U,Q}(s|u,q)$ for all nodes $q\ne N$ (see \eqref{eq:s_given_uq_lb} vs. \eqref{eq:s_given_uq_upper}). Furthermore, by induction, it holds that
\begin{align}
\label{eq:inductionClaimUgivenQ}
\tilde{P}(u|q)=\bar{P}^{*}(u|q), \forall q \in [0:N-1]    
\end{align}
\textit{Base case:} For $q=0$. we have $\tilde{P}_{U|Q}(0|0)=1=\bar{P}^{*}_{U|Q}(0|0)$ because
$\pi_{U,S|Q}(0,0|0)=\bar{\eta}, \quad \pi_{U,S|Q}(0,1|0)=\eta$, as shown in the proof of Lemma~\ref{lemma:eh_lb_BCJR}. This results applies to any policy of the lower bound in Theorem~\ref{thr:EH_cvx_lower} and, in particular, holds for both $\tilde{P}$ and $\bar{P}^{*}$.\\ \textit{Inductive Hypothesis:
}: Assume that 
\begin{align}
\label{eq:inductionHypothesisMimic}
    \tilde{P}(u|q)=\bar{P}^{*}(u|q), q\in [0:N-2], \quad \forall u\in [0:q].
\end{align}
\textit{Inductive Step}: We need to prove that
\begin{align}
\tilde{P}_{U|Q}(u|q+1)=\bar{P}^{*}_{U|Q}(u|q+1), \quad \forall u \in [0:q+1]. \label{eq:inductionStep}
\end{align}
From the stationarity on $(S,U,Q)$, which holds for both $\tilde{P}$ and $\bar{P}^{*}$, it follows that $\pi_{U^+|Q^+} (u|q)=\pi_{U|Q}(u|q), \forall u,q$. Thus, proving \eqref{eq:inductionStep} reduces to showing
\begin{align}
\mspace{-9mu} \tilde{P}_{U^+|Q^+}\mspace{-3mu}(u^+|q\mspace{-3mu}+\mspace{-3mu}1)\mspace{-4mu}=\mspace{-4mu}\bar{P}^{*}_{U^+|Q^+}\mspace{-3mu}(u^+|q\mspace{-3mu}+\mspace{-3mu}1), \forall u^+ \mspace{-4mu}\in\mspace{-4mu} [0:q\mspace{-3mu}+\mspace{-3mu}1]. 
\label{eq:inductionAlternativeStep}
\end{align}
For both $Q$-graphs of lower and upper bounds in Fig.~\ref{fig:EH_LB_Graph} and Fig.~\ref{fig:EH_UB_Graph}, node $Q^+=q+1 \in [1:N-1]$ is reached solely by node $Q=q$ with input $X=0$, i.e. $g(q,0)$. Hence, the joint distribution 
$\tilde{P}_{U^+,S^+|Q^+}(u^+,s^+|q+1)$ is given by
%(u^+,s^+, q, x)$
%hence, for $Q^+=q+1$ and for $(u^+,s^+, q, x)$
%\tilde{P}_{U^+|Q^+}(u|q+1)
\begin{align} 
&\frac{\sum_{u,s}
\mspace{-15mu} \tilde{P}(u|q)\pi(s|u,\mspace{-3mu} q)
\tilde{P}(u^+\mspace{-3mu} |u, \mspace{-3mu} q) \mspace{-3mu} \mathbbm{1}\mspace{-3mu} \{0 \mspace{-3mu} = \mspace{-5mu} f(u^+\mspace{-3mu} ,\mspace{-3mu}s)\} \mspace{-3mu} P_{S^+|X,S}\mspace{-3mu}(s^+\mspace{-3mu} |0, \mspace{-3mu} s)}{\sum_{u,u'^+,s} \tilde{P}(u|q)\pi(s|u, \mspace{-3mu} q)\tilde{P}(u'^+|u, \mspace{-3mu} q)\mspace{-3mu} \mathbbm{1}\mspace{-3mu} \{0 = f(u'^+, \mspace{-3mu} s)\}}, \nn
\end{align}
which equals the following expression due to the induction hypothesis \eqref{eq:inductionHypothesisMimic}, Eq.~\eqref{eq:mimicPolicy}, and the fact that both $\tilde{P}$ and $\bar{P}^*$ share the same $f(U^+,S)$, $\pi_{S|U,Q}$ and $P_{S^{+}|X,S}$ for all $Q\ne N$: 
\begin{align}
&\frac{\sum_{u,s}\mspace{-16mu} \bar{P}^*(u|q)\pi(\mspace{-3mu} s|u, \mspace{-3mu} q)\mspace{-3mu} \bar{P}^*\mspace{-3mu} (u^+\mspace{-3mu} |u, \mspace{-3mu}q) \mathbbm{1}\{0 \mspace{-3mu}=\mspace{-3mu} f(u^+\mspace{-5mu} ,\mspace{-3mu}s)\} P_{S^+|X,S}(s^+\mspace{-3mu}|0,\mspace{-3mu}s)}{\sum_{u,u'^+,s} \bar{P}^*(u|q)\pi(s|u,\mspace{-3mu} q) \bar{P}^*(u'^+\mspace{-3mu} |u, \mspace{-3mu} q)\mathbbm{1}\{0 \mspace{-3mu} =\mspace{-3mu}  f(u'^+,\mspace{-3mu} s)\}} \nn\\
&=\bar{P}^*_{U^+,S^+|Q^+}(u^+,s^+|q+1). \nn
\end{align}
Consequently, the marginals are also equal, i.e., \eqref{eq:inductionAlternativeStep} holds. Thus, \eqref{eq:inductionClaimUgivenQ} holds as well, and \eqref{eq:joint_mimicUB} is established.

Now, we evaluate the difference between the bounds
\begin{align}
    &0 \le b_N-a_N \nn\\
    & =\mspace{-3mu}\bar{I}_{\bar{P}^*}(U^{+(N)}\mspace{-3mu},U^{(N)};X|Q^{(N)})\mspace{-3mu}-\mspace{-3mu}\underline{I}_{\underline{P}^*}(U^{+(N)}\mspace{-3mu},U^{(N)};X|Q^{(N)})  \nn\\
    & \stackrel{(a)}\le \mspace{-3mu} \bar{I}_{\bar{P}^*}(U^{+(N)}\mspace{-3mu},U^{(N)};X|Q^{(N)})\mspace{-3mu}-\mspace{-3mu}\underline{I}_{\tilde{P}}(U^{+(N)}\mspace{-3mu},U^{(N)};X|Q^{(N)})  \nn\\
    & =\mspace{-8mu}\sum_{q=0}^{N-1} \mspace{-8mu} \bar{P}^*_{Q}(q) \bar{I}_{\bar{P}^*}\mspace{-3mu} (U^+\mspace{-5mu} ,\mspace{-3mu} U; \mspace{-3mu} X | Q\mspace{-3mu} = \mspace{-3mu} q) \mspace{-4mu} + \mspace{-4mu} \bar{P}^*_{Q}(\mspace{-2mu} N \mspace{-2mu}) \bar{I}_{\bar{P}^*}(U^+\mspace{-5mu} ,U ; \mspace{-2mu}X|Q\mspace{-4mu} = \mspace{-4mu}N\mspace{-2mu}) \nn\\ & - \mspace{-9mu} \sum_{q=0}^{N-1} \mspace{-7mu}\tilde{P}_{Q}(q)\mspace{-1mu} \underline{I}_{\tilde{P}}(U^+\mspace{-3mu},U;\mspace{-2mu} X |Q\mspace{-3mu}=\mspace{-3mu}q) \mspace{-5mu}-\mspace{-5mu} \tilde{P}_{Q}(\mspace{-2mu}N\mspace{-2mu}) \underline{I}_{\tilde{P}}(U^+\mspace{-3mu},\mspace{-2mu} U;\mspace{-2mu} X|Q\mspace{-3mu}=\mspace{-3mu}N) \nn\\
    & \stackrel{(b)}=\sum\nolimits_{q=0}^{N-1} \left(\bar{P}^*_{Q}(q) -\tilde{P}_{Q}(q)\right) \bar{I}_{\tilde{P}}(U^+,U;X|Q=q)+ \nn\\
    & \quad \mspace{-6mu} \bar{P}^*_{Q}(N) \bar{I}_{\bar{P}^*}(U^+\mspace{-3mu} ,\mspace{-3mu}U;X|Q\mspace{-3mu}=\mspace{-3mu} N) \mspace{-4mu}-\mspace{-4mu} \tilde{P}_{Q}(N) \underline{I}_{\tilde{P}}(U^+\mspace{-3mu},\mspace{-3mu}U;X|Q\mspace{-3mu}=\mspace{-3mu} N) \nn\\
    &\stackrel{(c)}\le \mspace{-3mu} \bar{P}^*_{Q}(N) \bar{I}_{\bar{P}^*}(U^+,U;X|Q\mspace{-3mu}=\mspace{-3mu}N) \mspace{-5mu}\stackrel{(d)}=\mspace{-5mu} \bar{P}^*_{Q}(N) H_2(\bar{P}^*_{U^+|Q}(0|N)) \nn\\
    & \stackrel{(e)}\le \frac{H_2(\bar{P}^*_{U^+|Q}(0|N))}{N \bar{P}^*_{U^+|Q}(0|N) +1} \le \max_{p\in[0,1]} \frac{H_2(p)}{N p +1}.    \label{eq:gap}
\end{align}
Step~(a) follows since $\tilde{P}$ satisfies the conditions for the lower bound optimization problem, but may not be optimal.
Step~(b) utilizes \eqref{eq:joint_mimicUB} 
to evaluate $\underline{I}_{\tilde{P}}(U^+,U;X|Q\mspace{-3mu}=\mspace{-3mu}q)\mspace{-3mu}=\mspace{-3mu}\bar{I}_{\bar{P}^*}(U^+,U;X|Q\mspace{-3mu}=\mspace{-3mu}q)$ for $q \in [0:N-1]$. %due to the fact that each mutual information is induced by the joint distribution $P(u,u^+,x|q)$.
Step~(c) relies on demonstrating that 
\begin{align}
\label{eq:statLastNode_mimic}
    \bar{P}^*_{Q}(q) \le \tilde{P}_{Q}(q), \quad \forall q\in [0:N-1].
\end{align}
To establish this, we observe that the transition probabilities $P_{Q^+|Q}(q^+|q)$ from node $q\in [0:N-1]$ to node $q^+\in [0:N]$ are identical for $\bar{P}^*$ and $\tilde{P}$. This follows from $\tilde{P}(x|q)\mspace{-3mu}=\mspace{-3mu}\bar{P}^*(x|q)$, as implied by \eqref{eq:joint_mimicUB}, and the fact that the $Q$-graphs for both the lower and upper bounds are identical, except at node $Q\mspace{-3mu}=\mspace{-3mu}N$. At node $Q\mspace{-3mu}=\mspace{-3mu}N$, however, there is a key distinction: the self-loop in $\bar{P}^*$ is absent in $\tilde{P}$, i.e.,
$\tilde{P}_{Q^+|Q}(N|N)=0\le \bar{P}^*_{Q^+|Q}(N|N)$. In contrast, $\tilde{P}_{Q^+|Q}(0|N)\ge \bar{P}^*_{Q^+|Q}(0|N)$. As a result, the corresponding stationary distributions satisfy 
$\bar{P}^*_{Q}(N) \ge \tilde{P}_{Q}(N)$ and the necessary Inequality \eqref{eq:statLastNode_mimic}.
%since $\bar{P}^*(Q^+=N|Q=N)\ge 0$ while $\tilde{P}(Q^+=N|Q=N)=0$.
Step~(d) follows from $\bar{I}_{\bar{P}^*}(U^+,U;X|Q\mspace{-3mu}=\mspace{-3mu}N)\mspace{-3mu}=\mspace{-3mu}H_{\bar{P}^*}(X|Q\mspace{-3mu}=\mspace{-3mu}N)-H_{\bar{P}^*}(X|U^+,U,Q\mspace{-3mu}=\mspace{-3mu}N)$ and simplifying each entropy. Specifically, using \eqref{eq:x_behc} and \eqref{eq:s_given_uq_upper}, the latter entropy simplifies to
$\mspace{-3mu}H_{\bar{P}^*}(X|S\mspace{-3mu}=\mspace{-3mu}1,U^+\mspace{-3mu}=\mspace{-3mu}0,U,Q\mspace{-3mu}=\mspace{-3mu}N)\mspace{-3mu}=\mspace{-3mu}0$.
%which $H_{\bar{P}^*}(X|U^+,U,Q\mspace{-3mu}=\mspace{-3mu}N)\mspace{-3mu}=\mspace{-3mu}H_{\bar{P}^*}(X|S\mspace{-3mu}=\mspace{-3mu}1,U^+\mspace{-3mu}=\mspace{-3mu}0,U,Q\mspace{-3mu}=\mspace{-3mu}N)\mspace{-3mu}=\mspace{-3mu}0$ because $\bar{P}^*(X\mspace{-3mu}=\mspace{-3mu}1|S\mspace{-3mu}=\mspace{-3mu}1,U^+\mspace{-3mu}=\mspace{-3mu}0,U,Q\mspace{-3mu}=\mspace{-3mu}N)\mspace{-3mu}=\mspace{-3mu}1$ by \eqref{eq:x_behc} and \eqref{eq:s_given_uq_upper}.
Step~(e) follows from the fact that $\bar{P}^*_{Q}(N) \le \bar{P}'_{Q}(N)$, where $\bar{P}'$ is a joint distribution distributed as \eqref{eq:Qgraph_joint_distUB} with the upper bound $Q$-graph in Fig.~\ref{fig:EH_UB_Graph}, similar to $\bar{P}^*$, but constructed to maximizes the $Q$-graph stationary distribution at node $Q\mspace{-3mu}=\mspace{-3mu}N$ compared to $\bar{P}^*$. The distribution $\bar{P}'$ is constructed by setting $\bar{P}'_{U^+|U,Q}(0|u,q)\mspace{-3mu}=\mspace{-3mu}0$ for any $q\in[0:N-1], u\in \cU^{(Q=q)}$, thereby ensuring $\bar{P}'_{Q^+|Q}(q+1|q)\mspace{-3mu}=\mspace{-3mu}1$. At the last node, however, $\bar{P}'$ mirrors $\bar{P}^*$ by setting $\bar{P}'_{U^+|Q}(0|N)\mspace{-3mu}=\mspace{-3mu}\bar{P}^*_{U^+|Q}(0|N)$. The resulting stationary distribution of the last node, induced by the policy of $\bar{P}'$, is calculated as $\bar{P}'_{Q}(N)\mspace{-3mu}=\mspace{-3mu} (N \bar{P}^*_{U^+|Q}(0|N)+1)^{-1}$.

Finally, taking the limit as $N\to \infty$ in \eqref{eq:gap}, we obtain
\begin{align}
    0 \le \lim_{N\to \infty} b_N-a_N \le \lim_{N\to \infty} \max_{p\in[0,1]} \frac{H(p)}{N p +1} \mspace{-3mu}=\mspace{-3mu}0. 
\end{align}
Consequently, \eqref{eq:conv_part1} holds, thereby concluding the proof.
% Denote the sequence of the solutions to the convex optimization problems \eqref{optProb_EH_LB} for the lower bound by
% \begin{align}
%  a_N \triangleq \underline{I}_{\bar{d}^*}(U^+,U;X|Q)    
% \end{align}
% where $\underline{I}_{\bar{d}^*}(U^+,U;X|Q)$ is the conditional mutual information induced by an optimal joint distribution $\bar{d}^*$. 
% \begin{align}\label{eq:BOUNDS}
% a_N &\triangleq \max_{\substack{\{P(u^+|u,q)\}\in\mathcal{P}_{\text{BCJR}} \\x=f(u^+,s)}}
% I(U^+,U;X|Q) \nn\\
% &\le C_{\text{BEHC}} \le \bar{C}_{\text{BEHC}}(N) \nn\\
% &\le \max_{\{P(u^+|u,q)\}\in\mathcal{P}_{\pi}} I(U^+,U;X|Q) \triangleq b_N, 
% \end{align}
% where the joint distributions of $a_N$ and of $b_N$ are \eqref{eq:Qgraph_joint_dist} and \eqref{eq:Qgraph_joint_distUB}, respectively, with the $Q$-graphs in Fig.~\ref{fig:EH_LB_Graph} and Fig.~\ref{fig:EH_UB_Graph}, respectively.    
%     % \begin{align}
%     % a_N = \le C_{\text{BEHC}}\le =b_N.
%     % \end{align}
\end{proof}

\begin{remark}    
In \eqref{eq:gap}, we derive an upper bound on the gap between the upper and lower bounds:
\begin{align}
    b_N-a_N \le \max_{p\in [0,1]}\frac{H(p)}{N p +1} \triangleq \psi(N)=O(N). \label{eq:gapPrecision}
\end{align}
Notably, this upper bound is independent of $\eta$, and guarantees
that the precision $\psi(N)$ for calculating $C_{\text{BEHC}}$ can be achieved without explicitly computing $a_N$ or $b_N$ using convex optimization algorithms. E.g., with $N=10,000$, a precision of $\psi(10,000)=0.0010432 \approx 1e-3$ is guaranteed for all values of $\eta$. Yet, from Table~\ref{table:noiseless_BEHC_full}, we observe that as $\eta$ increases, a smaller $N$ required to achieve a given precision, and vice versa. While the upper bound provided by $\psi(N)$ is generally loose, especially for large values of $\eta$, its independence from $\eta$ ensures that a desired precision can still be achieved by appropriately selecting $N$, even when $\eta$ is very small.
\end{remark}

\begin{remark}
    The convex optimization problems in \eqref{optProb_EH_LB} and \eqref{optProb_EH_UB} involve $O(N^2)$ constraints. While increasing $N$ improves precision per \eqref{eq:gapPrecision}, it also incurs quadratic space complexity, making large $N$ computationally demanding. The main computational burden lies in constructing the constraints, whereas solving the optimization problem itself, once set, is relatively fast using a convex optimization algorithm.
\end{remark}
%%%%%%%%%%
%Finally, the proof of Theorem~\ref{thr:EH_cvx_lower} is a direct consequence of Theorem~\ref{theorem:qgraph_LB}, Lemma~\ref{lemma:eh_lb_BCJR} and Theorem~\ref{thr:convergence}. Furthermore, the proof of Theorem~\ref{thr:EH_cvx_upper} is concluded by Theorem~\ref{thr:FSC_Qgraph_UB}, Lemma~\ref{lem:C_bar_FSC} and Theorem~\ref{thr:convergence}.
%%%%%%%
% \begin{figure}[t]
%     \centering
%     \qquad
%     \subfloat[\centering $Q=0$]{{
%     \begin{tikzpicture}
%    [%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%         box/.style={rectangle,draw=black,thick, minimum size=0.666666666666667 cm},
%     ]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%     \draw[step=0.666666666666667 cm,black,thick] (0,0) grid (2/1.5,1/1.5);
%     \foreach \xtick in {0,...,1} {\pgfmathsetmacro\result{\xtick} \node at (\xtick/1.5+0.5/1.5,1.2/1.5) {};}%{\pgfmathprintnumber{\result}}; }
%     \foreach \ytick in {0} {\pgfmathsetmacro\result{\ytick} \node at (-.2/1.5,-\ytick/1.5+0.5/1.5) {\pgfmathprintnumber{\result}}; }
% \node[] at (0,1.7/1.5){$U \backslash U^+$};  

% \node[] at (0.5/1.5,1.3/1.5){0};  
% \node[] at (1.5/1.5,1.3/1.5){1};  

% \end{tikzpicture} 
%     }}%
%     \qquad \qquad \quad \;
%     \subfloat[\centering $Q=1$]{{
% \begin{tikzpicture}
%    [%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%         box/.style={rectangle,draw=black,thick, minimum size=0.666666666666667 cm},
%     ]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%     \draw[step=0.666666666666667 cm,black,thick] (0,0) grid (3/1.5,2/1.5);
%     \foreach \xtick in {0,...,2} {\pgfmathsetmacro\result{\xtick} \node at (\xtick/1.5+0.5/1.5,2.2/1.5) {};}%{\pgfmathprintnumber{\result}}; }
%     \foreach \ytick in {0,...,1} {\pgfmathsetmacro\result{\ytick} \node at (-.2/1.5,1/1.5-\ytick/1.5+0.5/1.5) {\pgfmathprintnumber{\result}}; }
% \node[] at (0,2.7/1.5){$U \backslash U^+$};  

% \node[] at (0.5/1.5,2.3/1.5){0};  
% \node[] at (1.5/1.5,2.3/1.5){1};  
% \node[] at (2.5/1.5,2.3/1.5){2};  

% \node[box,fill=blue ] at (1/1.5+0.5/1.5,0+0.5/1.5){};
% \node[box,fill=blue ] at (2/1.5+0.5/1.5,1/1.5+0.5/1.5){};
% \end{tikzpicture} 
%     }}%
%     %\qquad
%     \\
%     \qquad  \qquad \quad
%     \subfloat[\centering $Q=2$]{{
% \begin{tikzpicture}
%    [%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%         box/.style={rectangle,draw=black,thick, minimum size=0.666666666666667 cm},
%     ]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%     \draw[step=0.666666666666667 cm,black,thick] (0,0) grid (4/1.5,3/1.5);
%     \foreach \xtick in {0,...,3} {\pgfmathsetmacro\result{\xtick} \node at (\xtick/1.5+0.5/1.5,3.2/1.5) {};}%{\pgfmathprintnumber{\result}}; }
%     \foreach \ytick in {0,...,2} {\pgfmathsetmacro\result{\ytick} \node at (-.2/1.5,2/1.5-\ytick/1.5+0.5/1.5) {\pgfmathprintnumber{\result}}; }
% \node[] at (0,3.7/1.5){$U \backslash U^+$};  

% \node[] at (0.5/1.5,3.3/1.5){0};  
% \node[] at (1.5/1.5,3.3/1.5){1};  
% \node[] at (2.5/1.5,3.3/1.5){2};  
% \node[] at (3.5/1.5,3.3/1.5){3};  

% \node[box,fill=blue ] at (1.5/1.5,0.5/1.5){};  
% \node[box,fill=blue ] at (2.5/1.5,0.5/1.5){};  
% \node[box,fill=blue ] at (1.5/1.5,1.5/1.5){};  
% \node[box,fill=blue ] at (3.5/1.5,1.5/1.5){};  
% \node[box,fill=blue ] at (2.5/1.5,2.5/1.5){};  
% \node[box,fill=blue ] at (3.5/1.5,2.5/1.5){}; 
% \end{tikzpicture} 
%     }}
%     %\\
%     %
%     \subfloat[\centering $Q=q<N$]{{
% \begin{tikzpicture}
%    [%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%         box/.style={rectangle,draw=black,thick, minimum size=0.666666666666667 cm},
%     ]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%     % \draw[step=1cm,black,thick] (0,0) grid (5,4);
%     % \foreach \xtick in {0,...,4} {\pgfmathsetmacro\result{\xtick} \node at (\xtick+0.5,5.2) {}; 
%     % }%{\pgfmathprintnumber{\result}}; }
%     % \foreach \ytick in {0,...,3} {\pgfmathsetmacro\result{\ytick} \node at (-.2,4-\ytick+0.5){};
%     % }%{\pgfmathprintnumber{\result}}; }
%     \draw[step=0.666666666666667 cm,black,thick] (0,0) grid (5/1.5,4/1.5);
%     \foreach \xtick in {0,...,4} {\pgfmathsetmacro\result{\xtick} \node at (\xtick/1.5+0.5/1.5,5.2/1.5) {}; 
%     }%{\pgfmathprintnumber{\result}}; }
%     \foreach \ytick in {0,...,3} {\pgfmathsetmacro\result{\ytick} \node at (-.2/1.5,4/1.5-\ytick/1.5+0.5/1.5){};
%     }%{\pgfmathprintnumber{\result}}; }
% \node[] at (0.5/1.5,4.3/1.5){0};  
% \node[] at (1.5/1.5,4.3/1.5){1};  
% \node[] at (3/1.5,4.3/1.5){$\cdots$};  
% \node[] at (4.5/1.5,4.3/1.5){q+1};  

% \node[] at (-0.2,3.5/1.5){0};  
% \node[] at (-0.2,2.5/1.5){1};  
% \node[] at (-0.2,1.5/1.5){$\vvots$};  
% \node[] at (-0.2,0.5/1.5){q};  
% \node[] at (-0.2,4.7/1.5){$U \backslash U^+$};  

%  \node[box,fill=blue ] at (1.5/1.5,0.5/1.5){};
%  \node[box,fill=blue ] at (1.5/1.5,1.5/1.5){};
%  \node[box,fill=blue ] at (1.5/1.5,2.5/1.5){};

% \node[box,fill=blue ] at (2.5/1.5,0.5/1.5){};
% \node[box,fill=blue ] at (2.5/1.5,1.5/1.5){};
% \node[box,fill=blue ] at (2.5/1.5,3.5/1.5){};

% \node[box,fill=blue ] at (3.5/1.5,0.5/1.5){};
% \node[box,fill=blue ] at (3.5/1.5,2.5/1.5){};
% \node[box,fill=blue ] at (3.5/1.5,3.5/1.5){};

% \node[box,fill=blue ] at (4.5/1.5,1.5/1.5){};
% \node[box,fill=blue ] at (4.5/1.5,2.5/1.5){};
% \node[box,fill=blue ] at (4.5/1.5,3.5/1.5){};

% \end{tikzpicture} 
%     }}%
%     \\
% LB: \subfloat[\centering $Q=N$]{{
% \begin{tikzpicture}
%    [%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%         box/.style={rectangle,draw=black,thick, minimum size=0.666666666666667 cm},
%     ]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%     \draw[step=0.666666666666667 cm,black,thick] (0,0) grid (5/1.5,5/1.5);
%     \foreach \xtick in {0,...,4} {\pgfmathsetmacro\result{\xtick} \node at (\xtick/1.5+0.5/1.5,5.2/1.5) {}; 
%     }%{\pgfmathprintnumber{\result}}; }
%     \foreach \ytick in {0,...,3} {\pgfmathsetmacro\result{\ytick} \node at (-.2/1.5,4/1.5-\ytick/1.5+0.5/1.5){};
%     }%{\pgfmathprintnumber{\result}}; }
% \node[] at (0.5/1.5,5.3/1.5){0};  
% \node[] at (1.5/1.5,5.3/1.5){1};  
% \node[] at (3/1.5,5.3/1.5){$\cdots$};  
% \node[] at (4.5/1.5,5.3/1.5){N};  

% \node[] at (-0.2,4.5/1.5){0};  
% \node[] at (-0.2,3.5/1.5){1};  
% \node[] at (-0.2,2.1/1.5){$\vvots$};  
% \node[] at (-0.2,0.5/1.5){N};  
% \node[] at (-0.2,5.7/1.5){$U \backslash U^+$};  

% \node[box,fill=blue ] at (1.5/1.5,0.5/1.5){};
% \node[box,fill=blue ] at (1.5/1.5,1.5/1.5){};
% \node[box,fill=blue ] at (1.5/1.5,2.5/1.5){};
% \node[box,fill=blue ] at (1.5/1.5,3.5/1.5){};
% \node[box,fill=blue ] at (1.5/1.5,4.5/1.5){};

% \node[box,fill=blue ] at (2.5/1.5,0.5/1.5){};
% \node[box,fill=blue ] at (2.5/1.5,1.5/1.5){};
% \node[box,fill=blue ] at (2.5/1.5,2.5/1.5){};
% \node[box,fill=blue ] at (2.5/1.5,3.5/1.5){};
% \node[box,fill=blue ] at (2.5/1.5,4.5/1.5){};

% \node[box,fill=blue ] at (3.5/1.5,0.5/1.5){};
% \node[box,fill=blue ] at (3.5/1.5,1.5/1.5){};
% \node[box,fill=blue ] at (3.5/1.5,2.5/1.5){};
% \node[box,fill=blue ] at (3.5/1.5,3.5/1.5){};
% \node[box,fill=blue ] at (3.5/1.5,4.5/1.5){};

% \node[box,fill=blue ] at (4.5/1.5,0.5/1.5){};
% \node[box,fill=blue ] at (4.5/1.5,1.5/1.5){};
% \node[box,fill=blue ] at (4.5/1.5,2.5/1.5){};
% \node[box,fill=blue ] at (4.5/1.5,3.5/1.5){};
% \node[box,fill=blue ] at (4.5/1.5,4.5/1.5){};
% \end{tikzpicture} 
%     }}% \quad 
%     \;
% UB: \subfloat[\centering $Q=N$]{{
% \begin{tikzpicture}
%    [%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%         box/.style={rectangle,draw=black,thick, minimum size=0.666666666666667 cm},
%     ]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%     \draw[step=0.666666666666667 cm,black,thick] (0,0) grid (5/1.5,5/1.5);
%     \foreach \xtick in {0,...,4} {\pgfmathsetmacro\result{\xtick} \node at (\xtick/1.5+0.5/1.5,5.2/1.5) {}; 
%     }%{\pgfmathprintnumber{\result}}; }
%     \foreach \ytick in {0,...,3} {\pgfmathsetmacro\result{\ytick} \node at (-.2/1.5,4/1.5-\ytick/1.5+0.5/1.5){};
%     }%{\pgfmathprintnumber{\result}}; }
% \node[] at (0.5/1.5,5.3/1.5){0};  
% \node[] at (1.5/1.5,5.3/1.5){1};  
% \node[] at (3/1.5,5.3/1.5){$\cdots$};  
% \node[] at (4.5/1.5,5.3/1.5){N};  

% \node[] at (-0.2,4.5/1.5){0};  
% \node[] at (-0.2,3.5/1.5){1};  
% \node[] at (-0.2,2.1/1.5){$\vvots$};  
% \node[] at (-0.2,0.5/1.5){N};  
% \node[] at (-0.2,5.7/1.5){$U \backslash U^+$};  

% % \node[box,fill=blue ] at (1.5/1.5,0.5/1.5){};
% % \node[box,fill=blue ] at (1.5/1.5,1.5/1.5){};
% % \node[box,fill=blue ] at (1.5/1.5,2.5/1.5){};
% % \node[box,fill=blue ] at (1.5/1.5,3.5/1.5){};

% \node[box,fill=blue ] at (2.5/1.5,0.5/1.5){};
% \node[box,fill=blue ] at (2.5/1.5,1.5/1.5){};
% \node[box,fill=blue ] at (2.5/1.5,2.5/1.5){};
% \node[box,fill=blue ] at (2.5/1.5,3.5/1.5){};
% \node[box,fill=blue ] at (2.5/1.5,4.5/1.5){};

% \node[box,fill=blue ] at (3.5/1.5,0.5/1.5){};
% \node[box,fill=blue ] at (3.5/1.5,1.5/1.5){};
% \node[box,fill=blue ] at (3.5/1.5,2.5/1.5){};
% \node[box,fill=blue ] at (3.5/1.5,3.5/1.5){};
% \node[box,fill=blue ] at (3.5/1.5,4.5/1.5){};

% \node[box,fill=blue ] at (4.5/1.5,0.5/1.5){};
% \node[box,fill=blue ] at (4.5/1.5,1.5/1.5){};
% \node[box,fill=blue ] at (4.5/1.5,2.5/1.5){};
% \node[box,fill=blue ] at (4.5/1.5,3.5/1.5){};
% \node[box,fill=blue ] at (4.5/1.5,4.5/1.5){};
% \end{tikzpicture} 
% }}
%     \caption{Illustrations of $P(u^+|u,q)$ for various values of $Q\in [0:N]$. A blue box means $P(u^+|u,q)=0$; a white box means a non-zero value. The structure of the policy $P(u^+|u,q)$ for $Q\in [0:N-1]$ is the same for both the lower and upper bounds. Their policy structure differs for node $Q=N$ only, as given on the bottom LHS and RHS, respectively. For the lower bound, this policy structure satisfies the BCJR constraints innately.}
%     \label{fig:policy}%
% \end{figure}

\section{Achievable Rates for Noisy BEHCs}
\label{sec:Noisy}
% In this section, we use the $Q$-graph and the DP tools in order to derive new achievable rates for the BEHC, for which the exact capacity has remained an open problem. 
% Similarly, for the $Q$-graph tool, the channel law simplifies to 
% \begin{align}
% P(s^+,y|x,s)&=P(y|x,s)\big(\bar{\eta} \mathbbm{1}\{s^+=\min\{s-x,1\}\} +\eta \mathbbm{1}\{s^+=\min\{s-x + 1,1\}\}\big).
% \end{align}
% We focus on the noiseless case and the noisy case separately.
%Hence, Theorems \ref{theorem:MDP} and \ref{theorem:lower} presented, serve as lower bounds for the noiseless channel. We would like to emphasize that both Theorems hold for any EH parameters, including the battery size and the channel law.
%**with the same assumptions of the abstraction of Tutuncuoglu \textit{et. al} \cite{tutuncuoglu2013binary,tutuncuoglu2014improved,tutuncuoglu2017binary}, and compare them to their lower and upper bounds. Even though they did not assume a presence of feedback, such a comparison is feasible because feedback does not affect the capacity of a noiseless channel. Practically, a noiseless binary channel is considered with a unit-sized battery at the encoder that harvests an energy unit due to an independent and identically distributed (i.i.d.) over time Bernoulli process. We also use our DP framework to derive lower bounds on a noisy EH model with feedback.
% \begin{table}[h]
% \caption{Bounds on the capacity of the BEHC with harvesting probability distributed i.i.d. $E_i\sim$\text{Bern}($\eta$).}
% \label{table:noiseless_BEHC_full}
% \centering
%   \begin{tabular}{|c||c|c||c|}
%   \hline
%   Harvesting prob. $(\eta)$ & $R_{\text{$Q$-graph}}$ & \textit{LB} \cite{tutuncuoglu2017binary} & \textit{UB} \cite{tutuncuoglu2017binary}\\
%   \hline
%   \textbf{0} & 0 & 0 & 0\\
%   \hline
%   \textbf{0.1} &\textbf{0.234625} & 0.2317 & 0.2600\\
%   \hline
%   \textbf{0.2} & \textbf{0.360} & 0.3546 & 0.3871\\
%   \hline
%   \textbf{0.3} & \textbf{0.45539} & 0.4487 & 0.4740\\
%   \hline
%   \textbf{0.4} & \textbf{0.53813} & 0.5297 & 0.5485\\
%   \hline
%   %0.61064
%   \textbf{0.5} & \textbf{0.61125} & 0.6033 & 0.6164\\
%   \hline
%   \textbf{0.6} & \textbf{0.67835} & 0.6729 & 0.6807\\ 
%   \hline
%   \textbf{0.7} & \textbf{0.74352} & 0.7403 & 0.7442\\ 
%   \hline
%   \textbf{0.8} & \textbf{0.81003} & 0.8088 & 0.8101\\
%   \hline
%   \textbf{0.9} & \textbf{0.88459} & 0.8845 & 0.8846\\
%   \hline
%   \textbf{1} & 1 & 1 & 1\\
%   \hline
%   \end{tabular}
% \end{table}
In this section, we consider noisy BEHCs.
Using the MDP formulation from \cite[Th.~4]{shemuel2024finite} and the $Q$-graph lower bound in Theorem~\ref{theorem:qgraph_LB}, achievable rates for BEHCs with any DMC and feedback can be evaluated numerically. As an example, we apply the VIA for the MDP to the BSC with $\left|\mathcal{U}\right|\mspace{-3mu}=\mspace{-3mu}2$ and the same $f(u^+,s)$ as in Eq. \eqref{eq:x_behc}. The numerical results, summarized in Table~\ref{table:BSC_BEHC}, show achievable rates as a function of the EH parameter $\eta$ and the channel crossover probability $p$. Notably, for $\eta \mspace{-3mu}=\mspace{-3mu}1$, the achievable rate matches the capacity of the standard BSC (with/ without feedback), given by $1\mspace{-3mu}-\mspace{-3mu}H(p)$.

\begin{table}[t]
\caption{Achievable rates for the BEHC$(\eta)$ over a BSC($p$)}
\label{table:BSC_BEHC}
\begin{center}
\begin{tabular}{|c||c|c|c|c|c|}
\hline 
%\mathbf{$\eta$ \textbackslash  $p$} & \textbf{0.1} & \textbf{0.2} & \textbf{0.3} & \textbf{0.4} & \textbf{0.5}\\
\multicolumn{1}{|c||}{\textbf{$\bm{\eta} \backslash \mathbf{p}$}} & \textbf{0.1} & \textbf{0.2} & \textbf{0.3} & \textbf{0.4}\\
\hline \hline
\textbf{0.1} & 0.0724 & 0.0331 & 0.0132 & 0.0032\\
\hline
\textbf{0.2} & 0.1437 & 0.0639 & 0.0280 & 0.0068\\
\hline
\textbf{0.3} & 0.2106 & 0.1044 & 0.0433 & 0.0105\\
\hline
\textbf{0.4} & 0.2737 & 0.1397 & 0.0588 & 0.0143\\
\hline
\textbf{0.5} & 0.3271 & 0.1730 & 0.0736 & 0.0180\\
\hline
\textbf{0.6} & 0.3725 & 0.2022 & 0.0872 & 0.0214\\
\hline
\textbf{0.7} & 0.4105 & 0.2261 & 0.0994 & 0.0243\\
\hline
\textbf{0.8} & 0.4486 & 0.2445 & 0.1074 & 0.0268\\
\hline
\textbf{0.9} & 0.4870 & 0.2608 & 0.1136 & 0.0284\\
\hline
\textbf{1} & 0.5310 & 0.2781 & 0.1187 & 0.0290 \\
\hline
\end{tabular}
\end{center}
\end{table}
%%%%%%%%%%%%%%%%%%%

More generally, the lower bound computation techniques from \cite{shemuel2024finite} extend to EH models with any DMC, arbitrary finite input/output alphabets, any battery size, and any EH process $E_i$. However, establishing computable upper bounds for general EH models remains challenging, as it requires proving a finite cardinality bound for the auxiliary RV $U$, as done for the BEHC.

\section{Conclusion}
\label{sec:conclusions}
This work resolves the open problem of computing the capacity of the BEHC, a challenge stemming from the encoder's intricate constraints, the decoder's lack of state knowledge, and the requirement for infinitely large memory to track consecutive zero inputs in the full history until they are followed by a $'1'$, which resets the tracking. While prior capacity expressions in the literature are multi-letter and uncomputable, we establish that the capacity can be determined to any desired precision via the $Q$-graph method and convex optimization.
%However, we derive sequences of lower and upper $Q$-graph bounds formulated as convex optimization problems, whose limits converge to the capacity.

Future research may explore coding schemes for the BEHC, refine upper bounding techniques for broader EH models, and extend the approach to large alphabets or continuous-state EH systems.

\appendices

\section{Proof of the Markov Chain 
$X_i-(\hat{U}_{K_{i-1}}^i,Q_{i-1})-(\hat{U}^{K_{i-1}-1},X^{i-1})$}
%$X_i-(\hat{U}_{i-N}^i,Q_{i-1} )-(\hat{U}^{i-N-1},X^{i-1})$
\label{appendix:lemma_boosted}
\begin{lemma}
\label{lem:boosted_markov}
For the boosted BEHC($N$), the following holds for any time~$i$
\begin{align}
        P(x_i|\hat{u}^i,x^{i-1})=P(x_i|\hat{u}_{k_{i-1}}^i,q_{i-1}(x_{i-N}^{i-1})), \quad \forall \hat{u}^i,x^i,\label{eq:boosted_markov}
        %P(x_i|\hat{u}^i,x^{i-1})=P(x_i|\hat{u}_{i-N}^i,q_{i-1}(x_{i-N}^{i-1})), \quad \forall \hat{u}^i,x^i,\label{eq:boosted_markov}
\end{align}
where $q_{i-1}$ is a vertex of the $Q$-graph illustrated in Fig.~\ref{fig:EH_UB_Graph}.% reached by inputs $x_{i-N}^{i-1}$.
\end{lemma}

\begin{proof}
We address each of the two cases separately. For Case $(i)$, i.e., $X_{K_{i-1}}^{i-1}\mspace{-4mu}=\mspace{-4mu}(1,\mathbf{0}^{i-1-{K_{i-1}}})$, we have $S_{K_{i-1}-1}\mspace{-4mu}=\mspace{-4mu}1$ and
\begin{align}
    P(x_i|\hat{u}^i,\mspace{-3mu}x^{i-1})\mspace{-4mu}& \stackrel{(a)}=\mspace{-4mu}P(x_i|\hat{u}^i,x^{i-1},s_{k_{i-1}-1}\mspace{-4mu}=\mspace{-4mu}1,q_{i-1},k_{i-1}) \nn\\
    &\stackrel{(b)}=P(x_i|\hat{u}_{k_{i-1}}^i,x_{k_{i-1}}^{i-1},s_{k_{i-1}-1}\mspace{-4mu}=\mspace{-4mu}1, q_{i-1},k_{i-1}) \nn\\    
    %P(x_i|\hat{u}_{k}^i,x_{k}^{i-1},s_{k-1}\mspace{-4mu}=\mspace{-4mu}1, q_{i-1}(x_{i-N}^{i-1}),k(x_{i-N}^{i-1})) \nn\\
    & \stackrel{(c)}=P(x_i|\hat{u}_{k_{i-1}}^i, q_{i-1},k_{i-1}). \label{eq:opt1}
    %P(y_i|\hat{u}_{i-N}^i,y_{i-N}^{i-1},)
\end{align}
%in Table~\ref{table:gN}
Step (a) follows because $q_{i-1}$ and $k_{i-1}$ are deterministic functions of $x_{i-N}^{i-1}$. In Step (b), the Markov chain follows from an identical derivation of \cite[Lemma~21]{PermuterWeissmanGoldsmith09} that includes $\hat{U}^i$ in the conditioning. Step (c) follows since $x_{k_{i-1}}^{i-1}$ and $s_{{k_{i-1}}-1}$ are deterministic functions of $k_{i-1}$.

For Case $(ii)$, i.e., $x_{i-N}^{i-1}\mspace{-4mu}=\mspace{-4mu}\mathbf{0}^N$, we have $S_{i-1}\mspace{-4mu}=\mspace{-4mu}1$ and
\begin{align}
    &P(x_i|\hat{u}^i,\mspace{-3mu}x^{i-N-1},\mspace{-3mu}x_{i-N}^{i-1}\mspace{-4mu}=\mspace{-4mu}\mathbf{0}^N) \mspace{-4mu}=\mspace{-4mu}P(x_i|\hat{u}_i,\mspace{-3mu}s_{i-1}\mspace{-4mu}=\mspace{-4mu}1,\mspace{-3mu} q_{i-1}\mspace{-4mu}=\mspace{-4mu}N) \nn\\
    %&=P_{Y|X,S}(y_i|f(\hat{u}_i,s_{i-1}=1),s_{i-1}=1) \nn\\
    &=\mathbbm{1}\{x_i\mspace{-4mu}=\mspace{-4mu}f(\hat{u}_i,\mspace{-3mu}s_{i-1}\mspace{-4mu}=\mspace{-4mu}1)\}
    =P(x_i|\hat{u}_i,\mspace{-3mu}q_{i-1}=N). \label{eq:opt2}
    %P(y_i|\hat{u}_{i-N}^i,\mspace{-3mu}y_{i-N}^{i-1},\mspace{-3mu})
\end{align}
Consequently, from \eqref{eq:opt1} and \eqref{eq:opt2}, we obtain for any $\hat{u}^i,x^i$:
\begin{align}
    P(x_i|\hat{u}^i,\mspace{-3mu}x^{i-1})&=P(x_i|\hat{u}_{k_{i-1}}^i,\mspace{-3mu}q_{i-1}(x_{i-N}^{i-1}),\mspace{-3mu} k_{i-1}(x_{i-N}^{i-1})) \nn\\
    &=P(x_i|\hat{u}_{k_{i-1}}^i,\mspace{-3mu}q_{i-1}(x_{i-N}^{i-1})),
%   P(x_i|\hat{u}^i,\mspace{-3mu}x^{i-1})&\stackrel{(a)}=P(x_i|\hat{u}_{i-N}^i,\mspace{-3mu}q_{i-1}(x_{i-N}^{i-1}),\mspace{-3mu} k_{i-1}(x_{i-N}^{i-1})) \nn\\
%&\stackrel{(b)}=P(x_i|\hat{u}_{i-N}^i,\mspace{-3mu}q_{i-1}(x_{i-N}^{i-1})).
\end{align}
%Step (a) follows because the minimum possible value of $K_{i-1}$ is $i-N$.
where the last step follows from the one-to-one mapping between $k_{i-1}(x_{i-N}^{i-1})$ and $q_{i-1}(x_{i-N}^{i-1})$.
\end{proof}

%\subsection{Visualization of the Policy $P(u^+|u,q)$ for the lower and upper bounds}
%\label{sec:appendix_EH_visualization_policy}
\section{Proof of Lemma \ref{lem:tryTransmit1}}
\label{Appendix:prf_lem_tryTransmit1}
Throughout the proof, we denote the special component by 
$\tilde{\hat{u}}_l=b$, where $l\in [k_{i-1}\mspace{-3mu}:\mspace{-3mu} i\mspace{-3mu}-\mspace{-3mu}1]$, i.e., $f_{\tilde{\hat{u}}_l}(s_{l-1}\mspace{-4mu}=\mspace{-4mu}1)\mspace{-4mu}=\mspace{-4mu}1$, and any vector containing it by $\tilde{\hat{u}}_m^j\triangleq (\hat{u}_m,\dots,\hat{u}_{l-1},\tilde{\hat{u}}_l,\hat{u}_{l+1},\dots,\hat{u}_j), m \le l \le j$. We aim to prove that
%\begin{align}
$P(x_i|\tilde{\hat{u}}^i,x^{i-1})\mspace{-4mu}=\mspace{-4mu}P(x_i|\tilde{\hat{u}}_l^i,q_{i-1})$, assuming $x_{i-N}^{i-1}\mspace{-3mu}\neq \mspace{-3mu}\mathbf{0}^N$. %\label{eq:yGivenPastEH1}
%\end{align}
%such $u^i$ by $\tilde{\hat{u}}^i\triangleq (\hat{u}_1,\dots,\hat{u}_{l-1},\tilde{\hat{u}}_l,\hat{u}_{l+1},\dots,\hat{u}_i)$.
\begin{proof}
Assume there exists an input $'1'$ among $x_{i-N}^{i-1}$. Then, 
\begin{align}
    & P(x_i|\tilde{\hat{u}}^i,x^{i-1})=\sum\nolimits_{s_l} P(s_l|\tilde{\hat{u}}^i,x^{i-1})P(x_i|\tilde{\hat{u}}^i,x^{i-1},s_l) \nn\\
    & \stackrel{(a)}= \frac{\sum_{s_l} P(\hat{u}_{l+1}^{i},x_{l+1}^{i-1}|\tilde{\hat{u}}^l,x^l,s_l) P(s_l|\tilde{\hat{u}}^l,x^l) P(x_i|\tilde{\hat{u}}^i,x^{i-1},s_l)}{\sum_{s_l} P(\hat{u}_{l+1}^{i},x_{l+1}^{i-1},s_l|\tilde{\hat{u}}^l,x^l)}  \nn\\
    & = \frac{\sum_{s_l} P(\hat{u}_{l+1}^{i},x_{l+1}^{i-1}|\tilde{\hat{u}}^l,x^l,s_l) P(s_l|\tilde{\hat{u}}^l,x^l) P(x_i|\tilde{\hat{u}}^i,x^{i-1},s_l)}{\sum_{s_l} P(\hat{u}_{l+1}^{i},x_{l+1}^{i-1}|\tilde{\hat{u}}^l,x^l,s_l) P(s_l|\tilde{\hat{u}}^l,x^l)} \nn\\
    & \stackrel{(b)}=\frac{\sum\limits_{s_l}  P(s_l|\tilde{\hat{u}}^l\mspace{-3mu},\mspace{-3mu}x^l)  \mspace{-10mu}\prod\limits_{j=l+1}^{i} \mspace{-10mu}P(\hat{u}_j|\tilde{\hat{u}}^{j-1}\mspace{-3mu},\mspace{-3mu}x^{j-1}) P(x_j|\tilde{\hat{u}}^j\mspace{-3mu},\mspace{-3mu}x^{j-1}\mspace{-3mu},\mspace{-3mu}q_{l}\mspace{-3mu},\mspace{-3mu}s_l) }{\sum\limits_{s_l}  P(s_l|\tilde{\hat{u}}^l\mspace{-3mu},\mspace{-3mu}x^l)  \mspace{-10mu} \prod\limits_{j=l+1}^{i} \mspace{-10mu} P(\hat{u}_j|\tilde{\hat{u}}^{j-1}\mspace{-3mu},\mspace{-3mu}x^{j-1})  \mspace{-10mu} \prod\limits_{j=l+1}^{i-1} \mspace{-10mu} P(x_j|\tilde{\hat{u}}^j\mspace{-3mu},\mspace{-3mu}x^{j-1}\mspace{-3mu},\mspace{-3mu}q_{l}\mspace{-3mu},\mspace{-3mu}s_l) }  \nn\\
    & \stackrel{(c)}= \frac{\sum_{s_l} P(s_l|\tilde{\hat{u}}^l,x^l) \prod_{j=l+1}^{i} P(x_j|\hat{u}_{l+1}^j,x_{l+1}^{j-1},q_{l},s_l)}{\sum_{s_l} P(s_l|\tilde{\hat{u}}^l,x^l) \prod_{j=l+1}^{i-1} P(x_j|\hat{u}_{l+1}^j,x_{l+1}^{j-1},q_l,s_l)}  \nn\\ 
    & \stackrel{(d)}= \frac{\sum_{s_l} P(s_l|q_{l-1},x_l) \mspace{-5mu}\prod_{j=l+1}^{i} \mspace{-5mu} P(x_j|\hat{u}_{l+1}^j,x_{l+1}^{j-1},q_{l},s_l)}{\sum_{s_l} P(s_l|q_{l-1},x_l) \mspace{-10mu} \prod_{j=l+1}^{i-1} \mspace{-5mu}P(x_j|\hat{u}_{l+1}^j,x_{l+1}^{j-1},q_l,s_l)}     
    % &=\frac{P(s_k=0|\tilde{\hat{u}}^k,x^k) \prod_{j=k+1}^{i} P(x_j|\hat{u}_{k+1}^j,x_{k+1}^{j-1},q_k,s_k=0)% new line
    % +P(s_k=1|\tilde{\hat{u}}^k,x^k) \prod_{j=k+1}^{i} P(x_j|\hat{u}_{k+1}^j,x_{k+1}^{j-1},q_k,s_k=1)%
    % }
    % {P(s_k=0|\tilde{\hat{u}}^k,x^k) \prod_{j=k+1}^{i-1} P(x_j|\hat{u}_{k+1}^j,x_{k+1}^{j-1},q_{k},s_k=0)
    % +P(s_k=1|\tilde{\hat{u}}^k,x^k) \prod_{j=k+1}^{i-1} P(x_j|\hat{u}_{k+1}^j,x_{k+1}^{j-1},q_{k},s_k=1) %
    % } \nn\\    &\stackrel{(d)}=\frac{\bar{\eta} \prod_{j=k+1}^{i} P(x_j|\hat{u}_{k+1}^j,x_{k+1}^{j-1},q_{k},s_k=0)
    % +\eta \prod_{j=k+1}^{i} P(x_j|\hat{u}_{k+1}^j,x_{k+1}^{j-1},q_{k},s_k=1)%
    % }
    % {\bar{\eta} \prod_{j=k+1}^{i-1} P(x_j|\hat{u}_{k+1}^j,x_{k+1}^{j-1},q_k,s_k=0)
    % +\eta \prod_{j=k+1}^{i-1} P(x_j|\hat{u}_{k+1}^j,x_{k+1}^{j-1},q_k,s_k=1) %
    % }
    . \label{eq:yGivenPastEH_last}
\end{align}
Step (a) follows from Bayes' rule and the law of total probability. Step (b) follows from the Markov chain $\hat{U}_j-(\hat{U}^{j-1},X^{j-1})-\tilde{S}^{j-1}$ for any time $j$, implied by \eqref{eq:joint_boosted}, and the fact that $q_{l}$ is a function of $(x_{l-N+1}^{l})$. In Step (c), $\prod_{j=l+1}^{i} P(\hat{u}_j|\tilde{\hat{u}}^{j-1},x^{j-1})$ cancels out, and it follows from the claim that the Markov $P(x_j|\tilde{\hat{u}}^j,x^{j-1},s_l)=P(x_j|\hat{u}_{l+1}^j,x_{l+1}^{j-1},s_l) 
$ holds for any $j>l$. This claim follows directly from an identical derivation of \cite[Lemma~21]{PermuterWeissmanGoldsmith09}, with $\hat{U}^j$ in the conditioning. Step (d) holds by
\begin{align}
& P(s_l\mspace{-3mu}=\mspace{-3mu}0|\tilde{\hat{u}}^l,\mspace{-3mu}x^l)=\mspace{-7mu}\sum\nolimits_{s_{l-1}}\mspace{-17mu} P(s_{l-1}|\tilde{\hat{u}}^l,\mspace{-3mu}x^l) P(s_l\mspace{-3mu}=\mspace{-3mu}0|\tilde{\hat{u}}^l,\mspace{-3mu} x^l, \mspace{-3mu} s_{l-1}, \mspace{-3mu} q_{l-1}) \nn\\
& \stackrel{(*)}=\sum\nolimits_{s_{l-1}} 
P(s_{l-1} | \tilde{\hat{u}}^l, x^l)
\bar{\eta} \quad=\bar{\eta}, \nn %\stackrel{(**)}=P(s_l=0|q_{l-1}(x_{i-N}^{l-1}),x_l),  \nn  
\end{align}
where $(*)$ follows from \eqref{eq:modifiedChannelLaw} and the assumptions: 
\begin{itemize}
    \item $x_{i-N}^{i-1} \neq \mathbf{0}^N$, which implies $q_{l-1} \neq N$.
    \item $f_{\tilde{\hat{u}}_l}(s_{l-1} = 1) = 1$, which implies $x_l = s_{l-1}$.
\end{itemize}
%%%%%%%%
%$x_{i-N}^{i-1}\mspace{-4mu}\neq \mspace{-4mu}\mathbf{0}^N$, implying $q_{l-1}\mspace{-3mu} \ne\mspace{-3mu}  N$, and $f_{\tilde{\hat{u}}_l}(s_{l-1}\mspace{-3mu}=\mspace{-3mu}1)\mspace{-3mu}=\mspace{-3mu}1$, implying $x_l\mspace{-3mu}=\mspace{-3mu}s_{l-1}$. 
%%%%%%
%$(**)$ follows because the minimum possible value of $K_{i-1}$ is $i-N$.
%\eqref{eq:stateEvolutionEH}. 

Since $\hat{u}^{l-1},x^{l-1}$ do not appear in \eqref{eq:yGivenPastEH_last}, it follows that $P(x_i|\tilde{\hat{u}}^i,x^{i-1})=P(x_i|\tilde{\hat{u}}_{l}^i,x_{l+1}^{i-1})$. From Lemma~\ref{lem:boosted_markov} and the assumption that $l\ge k_{i-1}$, we also have:\\
$ P(x_i|\tilde{\hat{u}}^i,x^{i-1})=P(x_i|\tilde{\hat{u}}_{k_{i-1}}^i,q_{i-1}(x^{i-1}_{i-N})).$ Since these two expressions are equal, the proof is complete.
% \begin{align}
%     &P(x_i|\tilde{\hat{u}}^i,x^{i-1})=P(x_i|\tilde{\hat{u}}_{k_{i-1}}^i,q_{i-1}).
% \end{align}
\end{proof}
%%%%%%%%%%%%%%%%%
% \begin{proof}
% On the one hand, from Lemma~\ref{lem:boosted_markov} and the assumption that $l\ge k_{i-1}$,
% \begin{align}
%     &P(x_i|\tilde{\hat{u}}^i,x^{i-1})=P(x_i|\tilde{\hat{u}}_{k_{i-1}}^i,q_{i-1}).
% \end{align}
% One the other hand, we expand
% \begin{align}
%     &P(x_i|\tilde{\hat{u}}^i,x^{i-1})=\sum\nolimits_{s_l} P(s_l|\tilde{\hat{u}}^i,x^{i-1}) P(x_i|\tilde{\hat{u}}^i,x^{i-1},s_l) \nn\\
%     &\stackrel{(a)}=\sum\nolimits_{s_l} P(s_l|\tilde{\hat{u}}^i,x^{i-1}) P(x_i|\hat{u}_{l+1}^i,x_{l+1}^{i-1},s_l) \nn\\    &\stackrel{(b)}=\bar{\eta}P(x_i|\tilde{\hat{u}}_{l+1}^i\mspace{-2mu},\mspace{-2mu}q_{i-1}\mspace{-2mu},\mspace{-2mu}s_l\mspace{-2mu}=\mspace{-2mu}0)\mspace{-2mu}+\mspace{-2mu}\eta P(x_i|\tilde{\hat{u}}_{l+1}^i\mspace{-2mu},\mspace{-2mu}q_{i-1}\mspace{-2mu},\mspace{-2mu}s_l\mspace{-2mu}=\mspace{-2mu}1) 
%      %P(s_l|\tilde{\hat{u}}_{k_{i-1}}^i,x^{i-1})P(x_i|\tilde{\hat{u}}^i,x^{i-1},s_l) \nn\\
%     %&\stackrel{(a)}=\sum\nolimits_{s_l} P(s_l|\tilde{\hat{u}}^i,x^{i-1})P(x_i|\hat{u}_{l+1}^j,x_{l+1}^{j-1},q_l,s_l)       
%     %& P(x_i|\tilde{\hat{u}}^i,x^{i-1})=\sum\nolimits_{s_l} P(s_l|\tilde{\hat{u}}^i,x^{i-1})P(x_i|\tilde{\hat{u}}^i,x^{i-1},s_l) \nn\\
%     %&\stackrel{(a)}=\sum\nolimits_{s_l} P(s_l|\tilde{\hat{u}}^i,x^{i-1})P(x_i|\hat{u}_{l+1}^j,x_{l+1}^{j-1},q_l,s_l)
%     . \label{eq:yGivenPastEH_last}
% \end{align}
% Step (a) follows from the claim that the Markov chain 
% \begin{align}
% \label{eq:LemHaimState}
%     P(x_j|\tilde{\hat{u}}^j,x^{j-1},s_l)=P(x_j|\hat{u}_{l+1}^j,x_{l+1}^{j-1},s_l) 
% \end{align} holds for any $j>l$. This claim follows directly from an identical derivation of \cite[Lemma~21]{PermuterWeissmanGoldsmith09}, with $\hat{U}^j$ in the conditioning. Step (b) follows from simplifying $P(s_l|\tilde{\hat{u}}^i,x^{i-1})$ using Bayes' rule as
% \begin{align}
%     &\frac{P(\hat{u}_{l+1}^i,x_{l+1}^{i-1}|\tilde{\hat{u}}^l,x^l,s_l)P(s_l|\tilde{\hat{u}}^l,x^{l-1})}{P(\hat{u}_{l+1}^i,x_{l+1}^{i-1}|\tilde{\hat{u}}^l,x^{l-1})}= \nn\\
%     &\frac{\prod\nolimits_{j=l+1}^{i}P(\hat{u}_j|\tilde{\hat{u}}^{j-1},x^{j-1},s_l)P(x_j|\hat{u}^j,x^{j-1},s_l)}{\prod\nolimits_{j=l+1}^{i}P(\hat{u}_j|\tilde{\hat{u}}^{j-1},x^{l-1})P(x_j|\hat{u}^j,x^{j-1})}P(s_l|\tilde{\hat{u}}^l,x^{l-1})   \nn\\
%     &=P(s_l|\tilde{\hat{u}}^l,x^{i-1})   \nn    
%     ,    
% \end{align}
% where the last step follows from the Markov chain $\hat{U}_j-\hat{U}^{j-1},X^{j-1}-X^{j-1}$
% implied from
% eq:LemHaimState
%%%%%%%%%%%%%%%%%%%
% \begin{align}
% & P(s_l=0|\tilde{\hat{u}}^l,x^l)=\mspace{-7mu}\sum\nolimits_{s_{l-1}}\mspace{-16mu} \mspace{-9mu} P(s_{l-1}|\tilde{\hat{u}}^l\mspace{-2mu},\mspace{-2mu}x^l) P(s_l|\tilde{\hat{u}}^l\mspace{-2mu},\mspace{-2mu}x^l\mspace{-2mu},\mspace{-2mu}s_{l-1}\mspace{-2mu},\mspace{-2mu}q_{l-1}(x_{k_{i-1}}^{l-1})) \nn\\
% & \stackrel{(*)}=(1 \mspace{-3mu}-\mspace{-3mu} \mathbbm{1}\{q_{l-1} \mspace{-3mu}=\mspace{-3mu} N\mspace{-3mu}-\mspace{-3mu}1,x_l \mspace{-3mu}=\mspace{-3mu} 0\})
% \sum\nolimits_{s_{l-1}} \mspace{-11mu}
% P(s_{l-1} | \tilde{\hat{u}}^l, x^l)
% \bar{\eta} \nn\\
% & =(1 \mspace{-3mu}-\mspace{-3mu} \mathbbm{1}\{q_{l-1} \mspace{-3mu}=\mspace{-3mu} N\mspace{-3mu}-\mspace{-3mu}1,x_l \mspace{-3mu}=\mspace{-3mu} 0\})
% \bar{\eta} \nn\\
% &\stackrel{(**)}=P(s_l\mspace{-3mu}=\mspace{-3mu}0|q_{l-1}(x_{i-N}^{l-1}),x_l),  \label{eq:P_s_given_uy}  
% \end{align}
% where $(*)$ follows from the assumption that $f_{\tilde{\hat{u}}_l}(s_{l-1}\mspace{-3mu}=\mspace{-3mu}1)\mspace{-3mu}=\mspace{-3mu}1$ and from \eqref{eq:modifiedChannelLaw}; $(**)$ follows because the minimum possible value of $K_{i-1}$ is $i-N$.
% %\eqref{eq:stateEvolutionEH}. 
%\end{proof}

% \section{Lemma...}
% The following lemma describes the relationship between the capacity expression of the general setting in Theorem \ref{theorem:capDI} and the increasing memory required in the $Q$-graphs of size $|\cQ|=N+1$.

% \begin{lemma}
% \label{lem:tryTransmit1_receive1}
% For the BEHC and the boosted BEHC($N$), for any time $i$ given outputs $y^{i-1}$ such that $y_k=1$ for some $k \in [1:i-1]$
% \begin{align}
%     P(y_i|u^i,y^{i-1})=P(y_i|u_k^i,y_k^{i-1}). \label{eq:yGivenPastEH_receive1}
% \end{align}
% \end{lemma}

% \begin{proof}
% By the law of total probability 
% \begin{align}
%     &P(y_i|\tilde{u}^i,y^{i-1}) \nn\\
%     &=\sum_{s_k} P(s_k|\tilde{u}^i,y^{i-1})P(y_i|s_k,\tilde{u}^i,y^{i-1}) \nn\\
%     &\stackrel{(a)}=\sum_{s_k} P(s_k|\tilde{u}^{i-1},y^{i-1})P(y_i|s_k,\tilde{u}^i,y^{i-1}) \nn\\
%     &\stackrel{(b)}= \frac{\sum_{s_k} P(u_{k+1}^{i-1},y_{k+1}^{i-1}|\tilde{u}^k,y^k,s_k) P(s_k|\tilde{u}^k,y^k) P(y_i|s_k,\tilde{u}^i,y^{i-1})}{\sum_{s_k} P(u_{k+1}^{i-1},y_{k+1}^{i-1},s_k|\tilde{u}^k,y^k)}  \nn\\
%     &= \frac{\sum_{s_k} P(u_{k+1}^{i-1},y_{k+1}^{i-1}|\tilde{u}^k,y^k,s_k) P(s_k|\tilde{u}^k,y^k) P(y_i|s_k,\tilde{u}^i,y^{i-1})}{\sum_{s_k} P(u_{k+1}^{i-1},y_{k+1}^{i-1}|\tilde{u}^k,y^k,s_k) P(s_k|\tilde{u}^k,y^k)}  \nn\\
%     &\stackrel{(c)}= \frac{\sum_{s_k} \prod_{j=k+1}^{i-1} P(u_j|\tilde{u}^{j-1},y^{j-1}) P(y_j|\tilde{u}^j,y^{j-1},s_k) P(s_k|\tilde{u}^k,y^k) P(y_i|s_k,\tilde{u}^i,y^{i-1})}{\sum_{s_k} \prod_{j=k+1}^{i-1} P(u_j|\tilde{u}^{j-1},y^{j-1}) P(y_j|\tilde{u}^j,y^{j-1},s_k) P(s_k|\tilde{u}^k,y^k)}  \nn\\
%     &\stackrel{(d)}= \frac{\sum_{s_k} \prod_{j=k+1}^{i-1} P(y_j|u_{k+1}^j,y_{k+1}^{j-1},s_k) P(s_k|\tilde{u}^k,y^k) P(y_i|s_k,\tilde{u}_{k+1}^i,y_{k+1}^{i-1})}{\sum_{s_k} \prod_{j=k+1}^{i-1} P(y_j|u_{k+1}^j,y_{k+1}^{j-1},s_k) P(s_k|\tilde{u}^k,y^k)}  \nn\\    &=\frac{\left(\splitdfrac{\prod_{j=k+1}^{i-1} P(y_j|u_{k+1}^j,y_{k+1}^{j-1},s_k=0) P(s_k=0|\tilde{u}^k,y^k) P(y_i|s_k=0,\tilde{u}_{k+1}^i,y_{k+1}^{i-1})}% new line
%     {+\prod_{j=k+1}^{i-1} P(y_j|u_{k+1}^j,y_{k+1}^{j-1},s_k=1) P(s_k=1|\tilde{u}^k,y^k) P(y_i|s_k=1,\tilde{u}_{k+1}^i,y_{k+1}^{i-1})}%
%     \right)}
%     {\left(\splitdfrac{\prod_{j=k+1}^{i-1} P(y_j|u_{k+1}^j,y_{k+1}^{j-1},s_k=0) P(s_k=0|\tilde{u}^k,y^k)}% new line
%     {+\prod_{j=k+1}^{i-1} P(y_j|u_{k+1}^j,y_{k+1}^{j-1},s_k=1) P(s_k=1|\tilde{u}^k,y^k)}%
%     \right)} \nn\\    &\stackrel{(e)}=\frac{\left(\splitdfrac{\prod_{j=k+1}^{i-1} P(y_j|u_{k+1}^j,y_{k+1}^{j-1},s_k=0) P(y_i|s_k=0,\tilde{u}_{k+1}^i,y_{k+1}^{i-1})\bar{\eta}}% new line
%     {+\prod_{j=k+1}^{i-1} P(y_j|u_{k+1}^j,y_{k+1}^{j-1},s_k=1) P(y_i|s_k=1,\tilde{u}_{k+1}^i,y_{k+1}^{i-1})\eta}%
%     \right)}
%     {\left(\splitdfrac{\prod_{j=k+1}^{i-1} P(y_j|u_{k+1}^j,y_{k+1}^{j-1},s_k=0) \bar{\eta}}% new line
%     {+\prod_{j=k+1}^{i-1} P(y_j|u_{k+1}^j,y_{k+1}^{j-1},s_k=1) \eta}%
%     \right)}, \label{eq:yGivenPastEH_last_receive1}
% \end{align}
% where
% \begin{enumerate}[label={(\alph*)}]
% \item follows from the Markov chain $U_i-(U^{i-1},Y^{i-1})-S^{i-1}$;
% \item follows from Bayes' rule and the law of total probability;
% \item follows from the Markov chain $U_j-(U^{j-1},Y^{j-1})-S^{j-1}$;
% \item follows since $\prod_{j=k+1}^{i-1} P(u_j|u^{j-1},y^{j-1})$ cancels out and because
% \begin{equation}
% P(y_j|u^j,y^{j-1},s_k)=P(y_j|u_{k+1}^j,y_{k+1}^{j-1},s_k), \quad \forall j>k,
% \end{equation}
% for any FSC with SI known cauaslly at the encoder (see \cite{PermuterWeissmanGoldsmith09} Appendix VI for a proof of a similar claim with $X$ instead of $U$);
% %\item follows since $\cS=\{0,1\}$;
% \item follows because
% \begin{align}
% P(s_k=0|\tilde{u}^k,y^k)&=\sum_{s_{k-1}} P(s_{k-1}|\tilde{u}^k,y^k) P(s_k=0|\tilde{u}^k,y^k,s_{k-1}) \nn\\
% &\stackrel{(*)}=\sum_{s_{k-1}} P(s_{k-1}|\tilde{u}^k,y^k) \bar{\eta} \nn\\
% &=\bar{\eta} \nn
% \end{align}
% due to the assumption that $y_k=1$ and from \eqref{eq:stateEvolutionEH}. 
% \end{enumerate}
% Finally, \eqref{eq:yGivenPastEH_last} holds since $u^{k-1},y^{k}$ do not appear in \eqref{eq:yGivenPastEH_last} as it follows from the assumption on $\tilde{u}_k$. 
% \end{proof}

%\int_{\tilde{u}^i \text{ of same mappings $x^i$}}
% that's all folks
\bibliographystyle{IEEEtran}
\bibliography{ref}
\clearpage
\setcounter{totalnumber}{1}  % Limit total floats on a page to 1

\end{document}