This section describes the implementation of \proposedsystem (See ~\autoref{fig:system_design}) on top of Sinfonia~\cite{satyanarayanan2022sinfonia}, a Kubernetes based open-source orchestrator for edge-native applications. Our implementation adds $\sim$4k SLOC to the  Sinfonia system and is available as open source \emph{(URL blinded)}. 
%Lastly, we describe our simulation environment used in large-scale experiments.

\subsection{\proposedsystem Prototype}
\noindent Our \proposedsystem consists of the following components that we added to Sinfonia.

\noindent\textbf{Telemetry Service}:
Our telemetry service is integrated into Sinfonia’s telemetry, where it collects static (e.g., location and IP address) attributes and real-time  (e.g., utilization) metrics. Real-time metrics are collected based on Prometheus monitoring stack\cite{Prometheus}. We augment Sinfonia's monitoring with the following metrics:
\begin{enumerate}[leftmargin=*]
    \item \textbf{Power Monitoring}: We measure the power consumption of CPU servers using RAPL~\cite{david2010rapl}, and we leveraged Prometheus's DGCM exporter for GPUs~\cite{nvidia_dcgm_exporter_github}. 

    \item \textbf{Carbon Intensity}: We integrate a carbon intensity service that replays historical traces from Electricity Maps~\cite{electricity-map} and uses the traces to provide real-time and forecast carbon intensity.

    \item \textbf{Carbon Monitoring}: We implement carbon monitoring based on energy usage and the carbon intensity of the selected edge sites, where we account for the base power (if the server is turned on) and applications energy usage. 
    
    \item \textbf{End-to-end latency}: In addition to latency across sites, we recorded end-to-end latency between users and their deployed applications.

\end{enumerate}

\noindent\textbf{Profiling Service}: 
We implement an application profiling service that collects the application's performance metrics, such as latency, power consumption, resource demands, and other crucial information, to make accurate placement decisions across available resources. Our profiling service can be replaced with performance models that statically analyze the applications and predict the latency and energy consumption~\cite{paleo, cai2017neuralpower}.

\noindent\textbf{Placement Service}:
Lastly, we implement our placement policy (\autoref{alg:algorithm}) on sinfonia as a matching policy. The placement policy utilizes the system's real-time metrics, static attributes of different edge sites, and workload profiles to determine optimal placements and server activation.
Our implementation batches deployment requests (e.g., every 5 minutes) and solves the optimization problem per application batch using the Google OR-Tools~\cite{ortools}. We demonstrate the effectiveness of our approach in ~\autoref{sec:eval_overhead}. 
%, an interface to the branch and cut solver.
%It gets the configuration of applications and edge data centers and outputs the placement and power management decisions to the edge orchestrator. 
%\noindent\textbf{Edge Orchestrator}: 
%The placement decision are then utilized by the orchestrations service that 
%We leverage Sinfonia, which is built on top of Kubernetes~\cite{kubernetes}, to deploy and manage server states. 
After computing the placement decisions, we utilize Sinfonia's orchestration capabilities to initiate the deployment sequence (Sinfonia RECIPE), which contains the necessary Kubernetes deployment files and helm charts, to the destination servers or activate servers if necessary and inform the client(s) of the destination's address. Note that although Sinfonia and our system are packed with fault-tolerance and reconfiguration capabilities, evaluating them is beyond the scope of this paper.







% \noindent\textbf{Telemetry Service}: \proposedsystem collects real-time performance, resource, and energy consumption telemetry from edge data centers. These telemetries are based on Prometheus monitoring stacks\cite{Prometheus}. We augmented Prometheus with power monitoring capabilities, where we measure the power consumption of CPU servers using RAPL~\cite{david2010rapl}, and for GPU servers, we leveraged Prometheus's DGCM exporter~\cite{nvidia_dcgm_exporter_github}. 
% Our telemetries are integrated into Sinfonia’s dynamic attributes and updated periodically. Lastly, we record end-to-end latency between users and their applications.

% \noindent\textbf{Carbon Intensity Service}: We built our carbon intensity service by integrating historical traces from Electricity Maps~\cite{electricity-map} and using the traces to provide real-time and forecast carbon intensity.


%Additionally, we include latency data based on round-trip time (RTT) measurements from WonderNetwork. Finally, alongside the telemetries, we implement an application profiling service that gathers application performance metrics, such as latency, power consumption, resource demands, and other crucial information for accurate placement decisions across available resources. 


% \noindent\textbf{Placement Service}: We implement the placement service with Python, and the carbon-aware placement optimization is solved using the Google OR-Tools~\cite{ortools}, an interface to the branch and cut solver. It gets the configuration of applications and edge data centers and outputs the placement and power management decisions to the edge orchestrator. 

% The \proposedsystem placement service encompasses the carbon-aware placement and power management decisions across different edge data centers. As we detailed in ~\autoref{sec:design_algorithm}, 
% the placement service integrates its knowledge of the carbon intensity, resource requirements, and load to compute the decisions that optimize the total carbon emissions while ensuring that resource and performance constraints are strictly met. 
% We implement \autoref{alg:algorithm} using Python and Google OR-Tools~\cite{ortools}. 


% \noindent\textbf{Edge Orchestrator}: We leverage Sinfonia, which is built on top of Kubernetes~\cite{kubernetes}, to deploy and manage server states. After getting the placement decisions, Sinfonia initiates the deployment sequence (Sinfonia RECIPE) to the destination servers or activates servers if necessary and informs the client(s) of the destination's address.  Note that although Sinfonia is packed with fault-tolerance and reconfiguration capabilities, evaluating them is beyond the scope of this paper.

\subsection{\proposedsystem Edge Simulator}
In addition to the prototype of \proposedsystem, we developed a simulator for larger-scale evaluations 
that is not feasible using an edge testbed.
Our simulator supports
simulating diverse edge settings with dynamic workloads and heterogeneous servers. This simulator represents the components of Sinfonia and follows the same decision process and metrics, where we implement our proposed carbon-aware placement policy and other baseline policies using Google OR-Tools~\cite{ortools}. The \proposedsystem simulator is implemented in Python using $\sim$2k SLOC.

%Sinfonia consists of three tiers: Tier 1, the cloud; Tier 2, edge sites; and Tier 3, end devices. Tiers 1 and 2 are Kubernetes clusters (K3s) equipped with Prometheus-based monitoring stacks. Tier 1 serves as the control plane, receiving offloading (placement) requests from Tier 3, selecting appropriate edge sites in Tier 2,  and responding with placement decisions. It maintains a CloudletTable that stores both static (e.g., location) and dynamic (e.g., resource utilization) attributes of Tier 2 servers, allowing for placement decisions based on predefined matches (policies). Sinfonia's flexibility to support customer matches allowed us to seamlessly integrate \proposedsystem's carbon-aware placement policy. 

% We built our Carbon Intensity Service (see ~\autoref{sec:design_arch}) by integrating historical traces from Electricity Maps~\cite{electricity-map} and using the traces to provide real-time and forecasted carbon intensity for edge servers in Tier 2. We map each edge data center to a carbon zone by utilizing its geographic coordinates. 





% In addition, we augmented the telemetries in Sinfonia (e.g., utilization, response time) with power monitoring capabilities. To measure power consumption of CPU servers using RAPL~\cite{david2010rapl}, while for GPU servers, we leveraged Prometheus's DGCM exporter~\cite{nvidia_dcgm_exporter_github}. These energy metrics, along with the carbon intensity data, were integrated into Sinfonia’s dynamic attributes and periodically updated from Tier 2 to Tier 1 (default interval of 15 seconds). All dynamic metrics are stored in the CloudletTable in Tier 1 for informed placement decision-making. To include the latency data, we utilized the round-trip time (RTT) measurements from WonderNetwork, stored in a configuration file.

% We implemented \autoref{alg:algorithm} as a new \emph{matcher} in Sinfonia's Tier 1. When a new application arrives (i.e., a Sinfonia RECIPE), the matcher batches applications, utilizes the available resources and solves the ILP using Google OR-Tools~\cite{ortools}. After mapping the application to servers,  Sinfonia initiates the deployment sequence and informs the client(s) (Tier 3) of the destination's address. Note that, although Sinfonia is packed with fault-tolerance and reconfiguration capabilities, evaluating them are beyond the scope of this paper. 



