\section{The \App{} Dataset}
\label{section:dataset}
\App{} is built following the MATRES \cite{ning-etal-2018-multi} approach; however, instead of annotating events only in consecutive sentences, the annotation is \textit{complete}, covering all event pairs across the entire document.
\App{} consists of a set of 30 English news summaries, written by humans (\url{Newser.com}), derived from the Multi-News dataset \cite{fabbri-etal-2019-multi}. We select summaries that portray large events, as these are rich in meaningful event mentions. Each summary contains a set of event mentions, with every pair assigned one of the following relations: \textit{before}, \textit{after}, \textit{equal}, or \textit{vague}.
We now describe \App{}'s annotation process (§\ref{section:background:annot-process}) along with dataset statistics (§\ref{section:dataset:statistics}).

% \subsection{Structure and Task}
% \label{section:background:struct}
% Notably, the temporal relation graph derived from each document is \textit{complete}.

% \App{} consists of a set of documents, where each document contains a set of event mentions, with every pair assigned one of the relations: \textit{before}, \textit{after}, \textit{equal}, or \textit{vague} denoting a relation that cannot be resolved from the provided text. Notably, the temporal graph derived from each document is \textit{complete}.

% \cite{do-etal-2012-joint, chambers-etal-2014-dense, ning-etal-2018-multi}. TRE is formulated as follows: given a document with a set of event mentions highlighted within it, the task is to determine the temporal relations between these events. 


\subsection{Annotation Process}
\label{section:background:annot-process}

% We built \App{} to ensure temporal relations are annotated exhaustively between all event pairs in a document. However, the complexity of this task, especially for non-expert annotators (see §\ref{section:background:datasets}), makes full annotation hard to achieve.
% Our motivation in developing \App{} is to create a resource where temporal relations are annotated exhaustively between all event pairs within a document. However, the complexity of exhaustively labeling such a resource (as discussed in §\ref{section:background:datasets}), particularly for non-expert annotators, makes this task nearly infeasible.

%every event pair and the linguistic challenges involved in annotating temporal relations \cite{naik-etal-2019-tddiscourse, rogers-etal-2024-narrativetime} make it nearly impossible for human annotators to keep up.  

%Luckily, EventFull \cite{eirew2024eventfullcompleteconsistentevent}, a recently published annotation tool, automates aspects of temporal relation annotation. This significantly reduces complexity, making the process more manageable for annotators while ensuring full event pair coverage, consistency, and preventing contradictions.\footnote{EventFull is designed to support multiple event-event relations, including temporal, causal, and coreference; we customized it to focus exclusively on temporal relation annotation.}

% For event selection, we follow the MATRES annotation guidelines, considering only events that are ``actual'' (e.g., \textit{they \underline{won} the game}). Events that are ``non-actual'', such as intentional, negated, recurring, conditional, or wishful (e.g., \textit{I wish they \underline{win} the game}) are excluded from the annotation process.
% Additionally, during annotation, only the starting time of events is considered when establishing temporal relations. 
%rather than their ending times. Finally, we adopted the same set of temporal relations as MATRES: \textit{before}, \textit{after}, \textit{equal}, and \textit{vague}.

For the annotation process, we hired three annotators, all non-expert native English speakers and either undergraduate or graduate students. We instruct annotators to follow the MATRES annotation guidelines, considering only ``actual'' events (e.g., \textit{they \underline{won} the game}). Events that are ``non-actual'', such as intentional, negated, recurring, conditional, or wishful (e.g., \textit{I wish they \underline{win} the game}), are excluded from annotation. Additionally, only the starting time of events is considered when establishing temporal relations.


The actual annotation was done on 30 news summaries, each containing approximately 500 words. The annotators used the EventFull annotation tool \cite{eirew2024eventfullcompleteconsistentevent}, with all events in each document already highlighted. These events were extracted using the event detection method proposed by \citet{cattan-etal-2021-cross-document}, which identifies all types of events (actual and non-actual) and extracts an average of 60 event mentions per document, forming the initial set of events. We follow the same annotation protocol as proposed in EventFull.\footnote{The complete annotation guidelines are available within the EventFull annotation tool.} First, the annotation process begins with the selection of 15 to 18 of the most salient ``actual'' events from each story.\footnote{\citet{eirew2024eventfullcompleteconsistentevent} found that beyond 18 events, annotation becomes challenging for non-expert annotators. This event reduction aligns with previous efforts to decrease annotation workload by limiting the number of events considered \cite{chambers-etal-2014-dense, ning-etal-2018-multi, tan-etal-2024-set}.} After selecting these events, each document was annotated for temporal relations (\textit{before}, \textit{after}, \textit{equal}, or \textit{vague}) by all three annotators. Finally, majority voting was used to determine the final relation, and in cases of disagreement, the relation was labeled as \textit{vague}. Further details about the annotators, time and cost are provided in Appendix~\ref{appx:annot-costs}.


% \input{tables/agreement}


\subsection{Dataset Statistics and Comparison}
\label{section:dataset:statistics}

Table~\ref{tab:stats} summarizes the \App{} dataset's statistics. Overall, the final annotated version of \App{} consists of 30 documents, corresponding to 470 event mentions and 3,483 relations. 
Appendix~\ref{append:additional-figures}, Table~\ref{tab:stats_all} presents the statistics of prominent datasets for the temporal relation extraction
task alongside \App{}. 

The agreement among our annotators averaged 0.72 kappa \cite{kappa-1973}, corresponding to substantial agreement and is comparable to that of TB-Dense \cite{chambers-etal-2014-dense} (0.56$\kappa$–0.64$\kappa$), NarrativeTime \cite{rogers-etal-2024-narrativetime} (0.68$\kappa$), TDD-Manual \cite{naik-etal-2019-tddiscourse} (0.69$\kappa$), and MATRES \cite{ning-etal-2018-multi} (0.84$\kappa$). Additionally, to verify annotation accuracy, one of the authors re-annotated 50 random pairs, with 46 matching the majority vote of the annotators, further confirming the high quality of the annotations.

% Notably, both MATRES \cite{ning-etal-2018-multi} and \App{} define temporal relations based on event start times without considering duration, using four relation types (\textit{before}, \textit{after}, \textit{equal}, and \textit{vague}). In contrast, TimeBank-Dense \cite{chambers-etal-2014-dense} and NarrativeTime \cite{rogers-etal-2024-narrativetime} annotate six and seven relation types, respectively, incorporating \textit{includes} and \textit{is-included} and considering both start and end times as well as event duration. These distinctions create different challenges for models, with TimeBank-Dense and NarrativeTime presenting a more complex task. This difference is also reflected in the distinct relation distributions between the two groups, as shown in Table~\ref{tab:stats}.

% , and in contrast to all other relevant datasets (listed in Appendix~\ref{append:additional-figures}, Table~\ref{tab:stats_all})

Finally, as mentioned, our motivation for developing \App{} was to provide complete annotations within each document, similar to NarrativeTime (§\ref{section:background:datasets}). However, in datasets such as MATRES and TB-Dense, where annotation is complete only between consecutive sentences, event pairs may be inferred through transitivity rules. The extent to which this automatic inference scales, however, remains unclear. To investigate this, we analyze the NarrativeTime dataset by considering all relations within the same sentence or between consecutive sentences. We then apply a transitive algorithm to infer additional relations and assess how many can be recovered beyond a single sentence. Our analysis shows that while some long-distance relations are recovered, most inferred relations remain within close proximity and occur infrequently. This finding highlights the importance of exhaustive annotation in constructing more complete and accurate story timelines. The full analysis is provided in Appendix~\ref{appx:trans-rels}.

