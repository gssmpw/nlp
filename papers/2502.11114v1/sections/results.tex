\section{Results}
\label{section:results}



% \begin{figure}[t!]
% \centering
% \includegraphics[width=\columnwidth]{figures/nt_cs_not.png}
% \caption{Model performance on the NarrativeTime dataset when evaluating predictions on different sets of relations: (1) relations only between events in consecutive sentences, (2) relations only between events in non-consecutive sentences, and (3) relations across the entire document.}
% \label{fig:nt-cs-not}
% \end{figure}

% \begin{figure}[t!]
% \centering
% \includegraphics[width=\columnwidth]{figures/omni_cs_not.png}
% \caption{Model performance on the \App{} dataset when evaluating predictions on different sets of relations: (1) relations only between events in consecutive sentences, (2) relations only between events in non-consecutive sentences, and (3) relations across the entire document.}
% \label{fig:omni-cs-not}
% \end{figure}

% \begin{figure}[t!]
% \centering
% \includegraphics[width=\columnwidth]{figures/per_rel_nt.png}
% \caption{\alone{TBD-Add}}
% \label{fig:per-rel-nt}
% \end{figure}

% \begin{figure}[t!]
% \centering
% \includegraphics[width=\columnwidth]{figures/per_rel_omni.png}
% \caption{\alone{TBD-Add}}
% \label{fig:per-rel-omni}
% \end{figure}


Our results are presented in Table~\ref{tab:models-perform},  with \textit{supervised} SOTA pairwise models in the upper section and our \textit{zero-shot} GPT-4o results in the lower section. Overall, our ZSL-GlobalConsistency approach (§\ref{section:model}) outperforms the CoT baseline \cite{yuan-etal-2023-zero} by a large margin across all datasets except TB-Dense (see §\ref{section:results:quality} for further analysis of TB-Dense). On dense datasets, NT-6 and \App{}, ZSL-GlobalConsistency achieves competitive performance compared to the supervised RoBERTa model (74.5 vs. 73.6 for \App{} and 58.4 vs. 59.3 for NT-6), while requiring no training data. The supervised SOTA Bayesian model performs better than our approach on these datasets, but notably it depends not only on training data but also on a substantial external common-sense knowledge base, which may not be applicable for many domains and languages.
This positions ZSL-GlobalConsistency as an appealing zero-shot alternative for temporal relation extraction in domains lacking labeled training data or comprehensive knowledge bases.


\input{tables/costs}


% which is considered a strong model for the temporal relation extraction task. These results suggest that for both the four-relation and six-relation tasks, this approach is a viable alternative for dense relation extraction in domains lacking training data.
 
Table~\ref{tab:costs} demonstrates, over the OmniTemp dataset, the effectiveness of our approach in terms of time, cost, and temporal consistency of the generated graphs, comparing it to the prior zero-shot CoT baseline. 
To assess effectiveness, we evaluate each method by measuring the average per document for: (1) generation time, (2) cost, calculated using OpenAI’s billing system, and (3) transitive consistency. The latter is evaluated by applying a transitive closure algorithm \cite{warsheall-1962} and counting transitive contradictions—relations that violate the transitivity constraints defined by \citet{ning-etal-2018-joint}.
% and counting transitive contradictions\footnote{Transitive contradictions are identified by counting relations that violate the transitivity constraints defined by \citet{ning-etal-2018-joint}}. 
The results show that all the evolving versions of our method are more cost- and time-efficient than the baseline CoT method. Moreover, the temporal graphs generated by all our methods are significantly more consistent than those produced by the baseline, reinforcing that prompting the LLM to generate the full graph in one step enables more effective use of global information.  
Each refinement of our method—from a simple zero-shot prompting approach to our final version, ZSL-GlobalConsistency—leads to improvements in both consistency and accuracy, which are key aspects of temporal relation extraction, while also increasing efficiency compared to the baseline.


\input{figures/graphs_pair_dist}

% We proceed with providing deeper observations through qualitative analysis.


\subsection{Qualitative Analysis}
\label{section:results:quality}

\textbf{Event Mentions Count.}
We examine how the number of events in a document affects the performance of our ZSL-Timeline method. Our hypothesis is that models encoding global information are influenced by the number of events, as they process more information at once. In contrast, pairwise methods, which consider one event pair at a time, are likely less affected.  
In Figure~\ref{fig:event-buckets}, we group MATRES and TB-Dense documents into subsets of increasing event counts.\footnote{The other datasets we experimented with, have a limited number of events per instance.}  
Overall, performance declines as the number of events increases, supporting our hypothesis. However, in the TB-Dense curve, there is a sharper drop for documents with more than 25 events, deviating from the trend. A closer look reveals that these additional TB-Dense documents contain mostly \textit{vague} relations. Further analysis, summarized in Figure~\ref{fig:per-rel-expr}, indicates that ZSL-Timeline struggles with \textit{vague} relations, particularly in the TB-Dense and MATRES corpora. Notably, the \textit{vague} relation is particularly challenging and often associated with annotator disagreement \cite{chambers-etal-2014-dense}.
% We observed that the Model performance declines as the number of events increases, as illustrated in Figure~\ref{fig:event-buchets}.\footnote{The drop in TB-Dense graph for the sample with fewer than 25 events, occurs because the added documents from ``bucket-20'' to ``bucket-25'' contain mostly \textit{vague} relations, which GPT-4o performs poorly on TB-Dense (see \ref{fig:per-rel-expr}). Beyond this, performance degradation follows the same pattern.} Both MATRES and TB-Dense containing more events than the NT-6 and \App{}. Additionally, it is safe to assume that in \textit{pairwise} supervised methods, where the model consider one pair at a time, the number of events in the document has little impact. 


\input{figures/graphs_rels}


\textbf{Event Pair Distance.}
From Table \ref{tab:models-perform} we learn that our initial version ZSL-Global outperforms the CoT baseline on MATRES but underperforms on \App{}. 
This difference may arise from the two annotation styles of MATRES and \App{}, with the former restricting the distance between events to at most \textit{one} sentence, while the latter imposes no such limitation. To explore this, we evaluate CoT, ZSL-Global, and ZSL-Timeline on three subsets of \App{} and NT-6: the full dataset, only event pairs where the distance between them is at most one sentence (consecutive-sentences), and only event pairs where the distance between them is greater than one sentence (non-consecutive-sentences). See Figure~\ref{fig:cs-sent-expr}.\footnote{For a fair comparison, we compare CoT to our methods before applying global consistency, isolating its performance from that achieved through transitive constraints, whose effectiveness depends on the quality of the input relations.}


%ZSL-Global and ZSL-Timeline use a single-turn prompting approach, while ZSL-SelfConsistency and ZSL-GlobalConsistency extend this by interacting with the LLM multiple times. To ensure a fair comparison with the baseline CoT method, which also relies on a single LLM call, we include only the single-call methods in this evaluation.

Our findings show that on the four-relation \App{} dataset, the CoT baseline performs consistently across all sentence distances, while ZSL-Global performs significantly better on consecutive-sentence relations, with a 10-point gap compared to non-consecutive ones. This discrepancy explains why ZSL-Global improves performance on MATRES, which is annotated within consecutive sentences, but underperforms on \App{}, which spans entire documents (Table~\ref{tab:models-perform}).

Interestingly, the opposite trend occurs in the more challenging six-relation NT-6 dataset, where the CoT baseline performs much better on consecutive-sentence relations, with a 8-point gap compared to performance on the non-consecutive-sentences subset. Noteworthy, in both cases ZSL-Timeline helps mitigating these issues—especially in NT-6—leading to overall improvements across entire documents. We stress that these findings highlight the need for document-level annotations for reliable evaluation of temporal relation classification, especially in zero-shot settings where models cannot rely (not realistically) on distribution patterns in the annotations.

% Our findings indicate that on the four-relation \App{} dataset, the CoT baseline performs consistently across the dataset, regardless of the sentence distance between events, whereas ZSL-Global performs significantly better on relations in consecutive sentences, exhibiting a 10-point gap compared to relations in non-consecutive sentences. This discrepancy explains the performance differences observed between MATRES (which is annotated within consecutive sentences) and \App{} (which is annotated across the entire document). 

% Interestingly, the opposite trend is observed in the more challenging six-relation NarrativeTime dataset, where the CoT baseline performs much better in consecutive sentences than those in non-consecutive sentences (with a 10 points gap). Notably, in both cases, the ZSL-Timeline approach helps mitigate these issues—particularly noticeable in NarrativeTime—leading to overall performance improvements across the entire documents in both datasets. This highlights the importance of resources annotated across the entire document for fair evaluation,  especially in zero-shot setting where the model cannot learn annotation distribution or biases. %Additionally, the performance gap between consecutive and non-consecutive sentences may partially explain the differences in overall performance observed between MATRES and TimeBank-Dense, compared to \App{} and NarrativeTime.

% We observe the variation in results between the CoT baseline method and the ZSL-Global method when evaluating on datasets annotated only for relations between consecutive sentences compared to those annotated across the entire document. For instance, on MATRES, performance improves when moving from CoT to ZSL-Global, whereas on \App{}, performance declines. To understand these results, we conducted an experiment evaluating CoT, ZSL-Global, and ZSL-Timeline on three slices of the \App{} and NarrativeTime datasets: (1) the entire dataset, (2) only relations between events in consecutive sentences, and (3) only relations between events in non-consecutive sentences (illustrated in Figure~\ref{fig:cs-sent-expr}).


\textbf{Label Inconsistency.}
The performance gap between our methods and the supervised models varies across datasets, being more pronounced in MATRES and TB-Dense than in NT-6 and OmniTemp. To better understand this gap, we analyze the ZSL-Timeline performance per label, grouping datasets with similar label categories and comparing them, as shown in Figure~\ref{fig:per-rel-expr}. Our ZSL-Timeline method performs significantly worse on MATRES and TB-Dense than on OmniTemp and NT-6.
%The four datasets are grouped into two charts: one for six-label datasets (TB-Dense and NT-6) and one for four-label datasets (MATRES and OmniTemp). Our ZSL-Timeline method performs significantly worse on MATRES and TB-Dense than on OmniTemp and NT-6.

To investigate this further, we examine label consistency in documents and event pairs shared between TB-Dense and MATRES, which annotated the same corpus. There are 983 such event pairs. While these datasets follow different annotation guidelines, certain labels should remain consistent. For instance, if an event pair is labeled \textit{equal} in TB-Dense—indicating that both the start and end times of the two events are the same—then the relation should also be \textit{equal} in MATRES. Measuring consistency across the four shared relations, we find strong agreement for \textit{before} and \textit{after}, with \textit{before} being the most consistently annotated. However, significant inconsistencies were evident in \textit{vague} and \textit{equal}. Detailed results are provided in Appendix~\ref{appx:consist-eval}.  
%To investigate this further, we examined label consistency in documents shared between MATRES and TB-Dense. There are 35 such documents. While these datasets follow different annotation guidelines, certain labels should remain consistent. For instance, if an event pair is labeled \textit{vague} in MATRES—indicating uncertainty about which event started first—it should also be labeled \textit{vague} in TB-Dense. Measuring consistency across the four shared relations, we find strong agreement for \textit{before} and \textit{after}, with \textit{before} being the most consistently annotated. However, significant inconsistencies were evident in \textit{vague} and \textit{equal}. Detailed results are provided in Appendix~\ref{appx:consist-eval}.  
Since in zero-shot settings the model is not trained on a dataset, it does not learn dataset-specific biases. The annotation inconsistency between MATRES and TB-Dense may partly explain the performance drop on these datasets, particularly for \textit{vague} and \textit{equal} relations, as well as the lower performance on \textit{after} compared to \textit{before}.
This analysis, along with the pair distance analysis, raises a broader question of whether the evaluation of zero-shot approaches on TB-Dense and MATRES is sufficiently reliable. 

In conclusion, our method outperforms the previous zero-shot approach, especially on the two more reliable datasets with complete and consistent annotations.


% In zero-shot settings, where the model cannot learn dataset-specific annotation biases, inconsistencies in labeling the same relation across datasets may exacerbate evaluation ambiguity. Accordingly, MATRES and TB-Dense annotate subsets of the same documents and event pairs; we analyzed the consistency between jointly annotated pairs. For a fair comparison, we employ a pattern that ensures an unbiased evaluation across both datasets, independent of event start/end times and duration considerations. For example, if a relation is labeled as \textit{vague} in MATRES, it suggests that the starting time between the event pair is unclear, thus, should also be labeled as \textit{vague} in TB-Dense. Conversely, if a relation is labeled as \textit{before} in TB-Dense based on both start and end times of the event, it necessitates that this relation should also be \textit{before} in MATRES (detailed further in Appendix~\ref{appx:consist-eval}). We measured consistency across the four shared relations and found strong agreement for both \textit{before} and \textit{after} relations, with \textit{before} being the most consistently annotated. However, we observed inconsistencies in both the \textit{vague} and \textit{equal} relations.
% This may explain in part the performance on MATRES and TB-Dense, particularly for \textit{vague} and \textit{equal} relations, as well as the lower preformence on \textit{after} compared to \textit{before} (Illustrated in Figure~\alone{ref to figure}). This might attribute the performance differences between our method and the supervised methods when comparing MATRES and TimeBank-Dense (TB-Dense) to NarrativeTime and OmniTemp.


% We observe notable performance differences between our method and the supervised methods when comparing MATRES and TimeBank-Dense (TB-Dense) to NarrativeTime and OmniTemp. Our analysis identifies two factors that might explain this: (1) Model performance declines as the number of events increases (see Figure~\ref{fig:event-buchets}),\footnote{The drop in TB-Dense graph for the sample with fewer than 25 events, occurs because the added documents from ``bucket-20'' to ``bucket-25'' contain mostly \textit{vague} relations, which GPT-4o struggles to resolve (\alone{add by relation figure and reference it}) on TB-Dense. Beyond this, performance degradation follows the same pattern.} with both MATRES and TB-Dense containing more events than the other two datasets. Additionally, it is safe to assume that in \textit{pairwise} supervised methods, where the model consider one pair at a time, the number of events in the document has little impact. 


% \footnote{Providing the model with explicit instructions on annotation criteria, such as considering only the starting time for MATRES and the starting time, ending time, and duration for TimeBank-Dense, does not improve performance.}

% Lastly, we observe that GPT-4o using our global method struggles with documents containing many events, leading to a performance gap with the supervised approach on MATRES and TimeBank-Dense. To investigate this, we created two subsets—MATRES (small) and TimeBank-Dense (mid)—by removing documents with over 20 events in MATRES and over 40 in TimeBank-Dense. Our results show that this improves performance, narrowing the gap to the supervised method by 10 points on MATRES and 4 points on TimeBank-Dense.
