\section{Background}
\label{section:background}
This section provides relevant background on datasets and zero-shot methods for the temporal relation extraction task.


\subsection{Temporal Relation Extraction Datasets}
\label{section:background:datasets}

The temporal relation extraction task aims to determine the temporal order between pre-extracted events in a text \cite{pustejovsky2003timeml}. For fair and unbiased model evaluation, datasets should provide gold labels for all event pairs or, at a minimum, be randomly sampled from the full set. However, most existing datasets for temporal relation extraction provide only partial annotation due to the complexity and cost of the process. As a result, the two most widely used datasets, MATRES \cite{ning-etal-2018-multi} and TimeBank-Dense (TB-Dense) \cite{chambers-etal-2014-dense}, annotate only relations between events in consecutive sentences.


Recently, the NarrativeTime project \cite{rogers-etal-2024-narrativetime} released a comprehensive, expert re-annotation of the TB-Dense corpus, covering all possible event pairs. The dataset includes seven relation types: \textit{before}, \textit{after}, \textit{includes}, \textit{is-included}, \textit{equal}, \textit{overlap}, and \textit{vague}. Temporal relations are established based on event start times, end times, and durations. Notably, the \textit{vague} relation indicates that the temporal relation cannot be determined from the provided context or where annotators disagree, and it is crucial for \textit{complete} annotation, as it confirms that the pair was considered during annotation.

While NarrativeTime provides an exhaustively annotated dataset, it follows a complex annotation guidelines similar to TB-Dense. MATRES refined these guidelines by considering only event start times and reducing the label set to \textit{before}, \textit{after}, \textit{equal}, and \textit{vague}, improving inter-annotator agreement while offering an alternative and appealing setting for the task. However, MATRES is not exhaustively annotated. To bridge this gap, we develop \App{}, a dataset that follows the refined MATRES scheme while ensuring complete coverage of \textit{all} event pairs across entire texts. Further details are provided in §\ref{section:dataset}.


%However, there remains a need for additional exhaustively annotated resources that offer alternative settings for the task, particularly those aligned with the prominent MATRES dataset—covering \textit{before}, \textit{after}, \textit{equal}, and \textit{vague} relations while considering only event start times. To address this, we developed \App{}, a complementary dataset to MATRES that includes relations between all events across entire texts (further detailed in §\ref{section:dataset}).

%While NarrativeTime provides a fully annotated dataset, it follows more complex guidelines that assign labels based on both event start and end times, include non-actual events, and use six or more relation types. The authors of MATRES refined these guidelines to simplify annotation by considering only event start times and reducing the label set to \textit{before}, \textit{after}, \textit{equal}, and \textit{vague}, leading to higher inter-annotator agreement. However, MATRES is not exhaustively annotated. To bridge this gap, we develop \App{}, a fully annotated dataset that follows the refined MATRES guidelines while ensuring complete coverage of event relations across entire texts. Further details are provided in §\ref{section:dataset}.



\input{tables/stats_omni}


% Nevertheless, the aforementioned resources have been invaluable in advancing supervised models for the task. Notably, they can be categorized into two levels of challenge for the task: (1) MATRES and TCR, which consider a smaller set of relations—\textit{before}, \textit{after}, and \textit{equal}, with MATRES additionally including the \textit{vague} relation—while relying only on event start times to establish temporal relations, representing the ``easier'' task; and (2) TimeBank-Dense and TDDiscourse, which present a more challenging TRE task by encompassing six relation types, including \textit{includes} and \textit{is-included}, and considering event start times, end times, and durations to determine relations.

% Recently, the NarrativeTime project \cite{rogers-etal-2024-narrativetime} have released a comprehensive re-annotating of the TimeBank-Dense corpus, covering relations between all possible event pairs. The dataset includes seven relation types: \textit{before}, \textit{after}, \textit{includes}, \textit{is-included}, \textit{equal}, \textit{overlap}, and \textit{vague}, where \textit{vague} denotes a relation that cannot be resolved from the provided text. To establish temporal relations, their annotation considers event start times, end times, and durations. 


% and TimeLine \cite{alsayyahi-batista-navarro-2023-timeline}, TimeLine, on the other hand, approached annotation by first assigning each event to a time interval and then using an automated process to determine temporal relations based on these intervals. However, this method produces a relation distribution heavily skewed toward the \textit{vague} relation (constituting 60\% of the relations) and very few equal relations (less then 1\%), which lacks temporal information.



\subsection{Zero-Shot Methods}
\label{section:background:models}

% The temporal relation extraction (TRE) task has traditionally relied on local pairwise methods, where the model processes one event pair at a time. This approach may stem from the lack of gold annotations (§\ref{section:background:datasets}) required for global modeling. 
% Furthermore, TRE has predominantly relied on supervised approaches \cite{huang-etal-2023-classification, tan-etal-2023-event, niu-etal-2024-contempo}, which require training datasets that are challenging to obtain or create \cite{naik-etal-2019-tddiscourse}, especially for new domains lacking annotated resources.

Recent advancements in LMs offer an opportunity to leverage their vast knowledge for zero-shot approaches \cite{10.5555/3600270.3601883}, enabling solutions without training data \cite{zhao-etal-2023-pre}. However, few studies have explored LLMs for temporal relation extraction in zero-shot settings. The most notable one is by \citet{yuan-etal-2023-zero}, who applied a simple zero-shot chain-of-thought (ZS-CoT) method, where the model is asked about each relation for a given pair until it answers ``yes''. Another effort by \citet{chan-etal-2024-exploring} experimented with prompt engineering and in-context learning. Both methods employed a pairwise approach and achieved suboptimal results on the MATRES and TB-Dense datasets. Additionally, the pairwise approach makes these methods cost- and time-inefficient. 

% Finally, determining the temporal relation between two events first requires understanding their temporal order. Several studies have demonstrated that planning before solving \cite{wang-etal-2023-plan} helps improve LLM performance in zero-shot across a range of tasks.

%Ultimately, deeming unsupervised methods as suboptimal for temporal relation extraction \cite{alsayyahi-batista-navarro-2023-timeline, wei-etal-2024-llms, niu-etal-2024-contempo, chan-etal-2024-exploring}.

One of our goals in this work is to provide a more efficient and effective alternative to pairwise approaches by processing the entire document globally in a single step (see §\ref{section:model}).

