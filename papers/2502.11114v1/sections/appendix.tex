\section{Experimental Details}
\label{appx:experiment:details}

For all supervised model experiments, we follow the experimental setup of \citet{tan-etal-2023-event}. To this end, we conducted a grid search to determine the optimal hyperparameters and embedding dimensionality for each test. Each training episode was run for 50 epochs on a single A100 GPU,\footnote{Experiment GPU time varies depending on the size of the training set, ranging from 1 to 20 hours for a full training episode.} with the best-performing epoch on the development set selected for evaluation. For the GPT-4o experiments, we use `gpt-4o-2024-08-06' version through OpenAI API. In all experiments, we provide the model with all event pairs combinations, and evaluate on the available gold labels. For the MATRES and TimeBank-Dense (TB-Dense) datasets, we evenly divide the set of pairs in documents containing more than 20 events. In TB-Dense, for documents exceeding 40 events, we further group the pairs into sets of 100.
Finally, In cases the generation missed pairs or is malformed, we regenerate the document or its respective split. For transitive constraint optimization, we employ the Gurobi Optimizer \cite{gurobi}.


\section{Further Details on Reported Results}
\label{appx:main-res-details}
We provide further details on the results presented in Table~\ref{tab:models-perform}. For the supervised models—RoBERTa, Bayesian, and Bayesian + Constraints—we report the best results achieved following a hyperparameter search (further detailed in Appendix~\ref{appx:experiment:details}). 
For the CoT experiment, we conducted a single evaluation run for each dataset and used this result. Constructing an ensemble or computing the mean for this experiment across multiple runs was beyond our budget. Additionally, our model results are substantially higher, making further aggregation unnecessary. 
In Table~\ref{tab:results-std}, we report the results for ZSL-Global and ZSL-Timeline, presenting the mean result obtained from five generations along with the standard deviation. For ZSL-SelfConsistency and ZSL-GlobalConsistency, we conducted a single run for each experiment, similar to CoT, as these experiments are more costly, and the observed standard deviation does not justify the additional expense.


\input{tables/results_std}

\input{tables/datasets_split}


\section{Label Inconsistency Evaluation}
\label{appx:consist-eval}
We describe the \textit{Label Inconsistency} experiment detailed in §\ref{section:results:quality}. MATRES \cite{ning-etal-2018-multi} and TB-Dense \cite{chambers-etal-2014-dense} annotate the same set of 35 documents but follow different annotation schemes. MATRES considers only event start times to determine temporal order, while TB-Dense accounts for event start times, end times, and durations.

To isolate this difference, we define the following ground truth for each relation: (1) If a pair is marked as \textit{vague} in MATRES, meaning the event start time is unclear, the same pair should also be \textit{vague} in TB-Dense since both the start time and duration are uncertain. (2) If a pair in TB-Dense is annotated as \textit{before}, \textit{after}, or \textit{equal} based on both start and end times, the corresponding MATRES annotation should reflect the same relation when considering only event start times. Figure~\ref{fig:venn-diag} presents our findings in terms of label consistency and inconsistency between the two datasets.


\section{Annotation Costs and Time}
\label{appx:annot-costs}
For the annotation process of \App{} (detailed in §\ref{section:background:annot-process}), we hired three student annotators (two males and one female) to label temporal relations between event pairs.
Their location will be revealed once anonymity requirements are lifted.
The total annotation time for \App{}, including onboarding, amounted to 85 hours, with each worker paid $\$15$ per hour (which is considered a fair market value in their region). %Each document was annotated independently by all three annotators using the EventFull annotation tool.


\section{Filling Transitive Relations}
\label{appx:trans-rels}
As discussed in §\ref{section:dataset:statistics}, to assess the coverage achievable by inferring transitive relations in resources annotated only with consecutive sentences, we extracted from NarrativeTime only the relations between event pairs in consecutive sentences. We then applied a transitive closure algorithm \cite{warsheall-1962} to construct additional relations and compared the results with the original set of relations. Figure~\ref{fig:nt_sentdiff} presents the experimental results.


\section{Formal Description of ZSL-GlobalConsistency}
\label{appx:formal-zsl-global}
ZSL-GlobalConsistency is formulated as follows: we run the ZSL-Timeline method five times on each input as described in §\ref{section:model}, generating five temporal graphs per document, denoted as  
\(
G = \{g_1, \dots, g_5\}
\)
where each \( g_n \) represents a labeled directed graph parsed from the DOT-language output. Each graph consists of a set of predicted event-pair relations:  
\(
g_n = \{p_{12}, p_{13}, \dots, p_{23}, p_{24}, \dots, p_{nm}\}
\)
where each relation \( p_{ij} \) is represented as a one-hot vector over the six relation types. We then sum these vectors element-wise across all five graphs and normalize them to obtain a single distribution per event pair:  
\(
d_{ij} = \frac{1}{5} \sum_{n=1}^{5} p_{ij}^{(n)}
\)
where each \( d_{ij} \) represents the normalized label distribution for the event pair \( (e_i, e_j) \). Instead of selecting the most frequent relation via majority voting, we apply the transitive constraints optimization algorithm, which returns a temporally consistent graph. We call this final method ZSL-GlobalConsistency (Figure~\ref{fig:figure1}).  



\section{Dataset Licenses and Sources}
In our experiments, we use the following commonly used datasets for evaluating the temporal relation extraction task: MATRES \cite{ning-etal-2018-multi}, provided without a license; TimeBank-Dense \cite{chambers-etal-2014-dense}, provided without a license; and NarrativeTime \cite{rogers-etal-2024-narrativetime}, provided under the MIT license. 
Additionally, \App{} uses summaries from the Multi-News corpus \cite{fabbri-etal-2019-multi}, which is distributed under a custom license that permits free academic use.
All datasets were downloaded from official repositories, and used appropriately. \App{} will also be released under a free-to-use academic license.


\section{Adjustments to the NarrativeTime Dataset}
\label{append:nt-further-details}
The NarrativeTime (NT) dataset, introduced in §\ref{section:background:datasets}, features seven relation types, including the six from TB-Dense and the \textit{overlap} relation. Our temporal consistency algorithm relies on Allen’s transitivity laws \cite{ALLEN1984123}, which require each relation type to have a symmetric counterpart (e.g., if event \textit{A} occurs \textit{before} event \textit{B}, then \textit{B} must occur \textit{after} \textit{A}). However, the \textit{overlap} relation in NT lacks a symmetric counterpart, making it incompatible for transitive consistency methods. 
%Additionally, the number of \textit{overlap} relations in the dataset is relatively small, further justifying its exclusion. 
Therefore, before using NT, we exclude event pairs labeled with the \textit{overlap} relation.
% The NarrativeTime (NT) dataset \cite{rogers-etal-2024-narrativetime} is already discussed in §\ref{section:background:datasets}. Before using NT, we exclude event pairs annotated with the \textit{overlap} relation, as its symmetric counterpart is missing from the dataset. This inconsistency conflicts with transitive constraint rules \cite{ALLEN1984123, ning-etal-2018-joint} and complicates our evaluation of temporal consistency (§\ref{section:model:pipeline}).  
Additionally, NT documents contain an average of 50 event mentions per document, corresponding to approximately 1,100 relations, which makes them difficult to process with LLMs due to context length limitations. Handling such documents requires segmenting them and making individual calls to the model for each segment, which increases costs, as discussed in §\ref{section:model}. To avoid segmentation and reduce costs, we randomly select 18 events per document from the test set, along with all their associated relations. The choice of 18 events was based on empirical observations, as it represents the maximum number that can typically fit within the model's context window without requiring segmentation. This reduction is not applied to the training set, which we use to fine-tune the supervised models. We refer to this pre-processed version as NT-6, as it retains only six relation types.


\section{Additional Experiment Tables and Figures}
\label{append:additional-figures}
Table~\ref{tab:stats_all} presents a comparison between common datasets used for evaluating models on the temporal relation task alongside \App{}. Table~\ref{tab:dataset_all} presents the split statistics of these datasets. Figure~\ref{fig:zsl-global-prompt} presents an example of the ZSL-Global prompt. Figure~\ref{fig:timeline-output} presents an example of the generated timeline using the ZSL-Timeline approach.


\begin{figure*}[t]
    \centering
    \subfloat[Before]{\includegraphics[width=0.40\linewidth]{figures/venn_before.png}\label{fig:venn:first}}
    \hfill
    \subfloat[After]{\includegraphics[width=0.40\linewidth]{figures/venn_after.png}}
    \hfill
    \subfloat[Vague]{\includegraphics[width=0.40\linewidth]{figures/venn_vague.png}}\label{fig:venn:third}
    \hfill
    \subfloat[Equal]{\includegraphics[width=0.40\linewidth]{figures/venn_equal.png}}
    \label{fig:venn:fourth}
    \caption{Label Inconsistency: Each group, A and B, represents MATRES and TimeBank-Dense respectively. The intersecting area indicates consistency in label annotation between the two datasets, with the number of such pairs highlighted in the middle, while the non-intersecting areas represent pairs assigned different labels in each dataset.}
    \label{fig:venn-diag}
\end{figure*}



\begin{figure*}[t!]
\centering
\includegraphics[width=0.7\textwidth]{figures/zsl-global.png}
\caption{An example of the ZSL-Global prompt.}
\label{fig:zsl-global-prompt}
\end{figure*}

\begin{figure*}[t!]
\centering
\includegraphics[width=\textwidth]{figures/timeline_output.png}
\caption{An example of a generated output when GPT-4o is prompted using the ZSL-Timeline method (with the Markdown format retained from the original output). The full event list is generated; however, it is trimmed (indicated by ``...'') in this example to ensure the output fits within the figure.}
\label{fig:timeline-output}
\end{figure*}


\input{tables/stats}


\begin{figure*}[t!]
\centering
\includegraphics[width=\textwidth]{figures/nt_sentdiff.png}
\caption{Illustration of the achieved relation distance after applying transitive closure in resources annotated only between consecutive sentences. The blue bars represent the original set of relations in NarrativeTime, which is exhaustively annotated between all events. The orange bars represent the version created by considering only relations between events in consecutive sentences. The green bars represent the set of relations after applying a transitive algorithm to infer additional relations.}
\label{fig:nt_sentdiff}
\end{figure*}
