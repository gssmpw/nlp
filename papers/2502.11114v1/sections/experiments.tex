\section{Experimental Setting}
\label{section:experiment}

We describe the datasets and models used in our experiments. Technical details are in Appendix~\ref{appx:experiment:details}.
% In this section, we detail our experimental setup, as presented in Table~\ref{tab:models-perform}, including the dataset used for evaluation (§\ref{section:experiment:datasets}), and baseline models (§\ref{section:experiment:baselines}). 




\subsection{Datasets}
\label{section:experiment:datasets}


In our experiments, we use our own \App{} and three additional datasets: MATRES, TB-Dense, and NarrativeTime.TCR \cite{ning-etal-2018-joint} and TDD-Manual \cite{naik-etal-2019-tddiscourse}, two additional datasets for the TRE task, are excluded from our experiments as they omit the \textit{vague} relation. Since we generate relations for all possible event pairs, the \textit{vague} label is essential to avoid forcing incorrect relations when context is insufficient (further details on these datasets are presented in Appendix~\ref{append:additional-figures}). 
Below, we provide details on the datasets used in our experiments. 
For our own \App{}, we use the first 10 documents as the test set and the remaining documents as the training set, while for all other datasets, we follow their predefined splits.


\textbf{MATRES.} In MATRES, only events within consecutive sentences are annotated. The dataset includes four relation types: \textit{before}, \textit{after}, \textit{equal}, and \textit{vague}, with temporal relations determined based on event start times. 
% We conduct experiments on MATRES twice: once using the full test set, referred to as \textit{all}, and once using a subset of the test documents containing 20 or fewer event mentions, referred to as \textit{small}. This subset allows us to assess model performance on documents with fewer events, based on our observation that performance varies with the number of event mentions in a document, as further discussed in §\ref{section:results}.

\textbf{TB-Dense.} Similar to MATRES, only events within consecutive sentences are annotated in the TB-Dense dataset. It includes six relation types, the four from MATRES plus \textit{includes} and \textit{is-included}. Temporal relations are determined based on event start and end times as well as their duration. 
% Like before, to assess model performance on documents with fewer events, we experiment with TB-Dense twice, once using the full test set, referred to as \textit{TB-Dense (all)}, and once with a subset of the test set containing only documents with 30 or fewer event mentions, referred to as \textit{TB-Dense (mid)}.


\textbf{NT-6.} The NarrativeTime (NT) dataset, previously introduced in §\ref{section:background:datasets}, features seven relation types, including the six from TB-Dense and the \textit{overlap} relation. 
However, we exclude the \textit{overlap} relation as it is incompatible for the transitive consistency methods, given that the symmetric counterpart was not annotated.
Additionally, NT documents contain an average of 50 events, corresponding to 1,200 relations, per document. This poses challenges for LLMs due to context length limitations. To address this, we randomly select only 18 events per document.  
Further details on these decisions are provided in Appendix~\ref{append:nt-further-details}. We refer to this dataset as NT-6 as it retains only six relations.

% Our temporal consistency algorithm relies on Allen’s transitivity laws \cite{ALLEN1984123}, which require each relation type to have a symmetric counterpart (e.g., if event \textit{A} occurs \textit{before} event \textit{B}, then \textit{B} must occur \textit{after} \textit{A}). However, the \textit{overlap} relation in NT lacks a symmetric counterpart, making it incompatible for transitive consistency methods. 
% %Additionally, the number of \textit{overlap} relations in the dataset is relatively small, further justifying its exclusion. 
% Therefore, before using NT, we exclude event pairs labeled with the \textit{overlap} relation.
% The NarrativeTime (NT) dataset \cite{rogers-etal-2024-narrativetime} is already discussed in §\ref{section:background:datasets}. Before using NT, we exclude event pairs annotated with the \textit{overlap} relation, as its symmetric counterpart is missing from the dataset. This inconsistency conflicts with transitive constraint rules \cite{ALLEN1984123, ning-etal-2018-joint} and complicates our evaluation of temporal consistency (§\ref{section:model:pipeline}).  
% Additionally, NT documents contain an average of 50 event mentions per document, corresponding to approximately 1,100 relations, which makes them difficult to process with LLMs due to context length limitations. Handling such documents requires segmenting them and making individual calls to the model for each segment, which increases costs, as discussed in §\ref{section:model}. To avoid segmentation and reduce costs, we randomly select 18 events per document from the test set, along with all their associated relations. The choice of 18 events was based on empirical observations, as it represents the maximum number that can typically fit within the model's context window without requiring segmentation. This reduction is not applied to the training set, which we use to fine-tune the supervised models. We refer to this pre-processed version as NT-6, as it retains only six relation types.



% \begin{itemize}
%     \item \textbf{MATRES (all):} Represents the full test set of the MATRES dataset \cite{ning-etal-2018-multi}. Events are annotated only between consecutive sentences and support four relation types: \textit{before}, \textit{after}, \textit{equal}, and \textit{vague}. Temporal relations are determined based on event start times.
    
%     \item \textbf{MATRES (small):} A subset of the MATRES test set containing documents with 20 or fewer event mentions. This subset was created to evaluate the model performance on documents with a smaller set of events, following our observation that performance varies depending on the number of event mentions within a document, as further discussed in §\ref{section:results}.

%     \item \textbf{TB-Dense (all)}: The full TimeBank-Dense dataset \cite{chambers-etal-2014-dense}. Events are annotated only between consecutive sentences and supports six relation types: \textit{before}, \textit{after}, \textit{equal}, \textit{includes}, \textit{is-included}, and \textit{vague}. Temporal relations are determined based on event start and end times as well as their duration.

%     \item \textbf{TB-Dense (mid)}: Similar to the smaller MATRES subset, we created a subset of the TimeBank-Dense test set, selecting documents with 30 or fewer event mentions.

%     \item \textbf{NT (6):} Our version of the NarrativeTime dataset \cite{rogers-etal-2024-narrativetime} (detailed in §\ref{section:background:datasets}). We introduce two modifications to the original dataset: (1) We remove events linked by the \textit{overlap} relation, as its symmetric counterpart is not annotated in the dataset, making it incompatible with transitive constraint rules \cite{ALLEN1984123, ning-etal-2018-joint}. This adjustment is necessary since one of our goals is to evaluate temporal consistency (§\ref{section:model:pipeline}). (2) NarrativeTime consists of documents containing up to 70 events and 2,000 relations. Due to context length limitations, such documents require segmentation, as detailed in §\ref{section:model:global}. To avoid segmentation and reduce costs, we randomly select 18 events per document from the test set, along with all their relations. This reduction is not applied to the training set.

%     % \item \textbf{NT (6-cs):} To evaluate model performance on short-distance relations, we created a version of NarrativeTime by filtering \textit{NT (6-all)} to include only relations between events in consecutive sentences. \alone{This should be replace with TB-Dense}
% \end{itemize}



\subsection{Baseline and State-of-the-Art Models}
\label{section:experiment:baselines}

We compare our zero-shot methods with four models, reproducing state-of-the-art (SOTA) supervised models and a zero-shot chain-of-thought (ZS-CoT) baseline method.

\textbf{Bayesian \cite{tan-etal-2023-event}.} 
Bayesian-Translation is the current publicly available state-of-the-art pairwise model for temporal relation extraction. It leverages a COMET-BART encoder \cite{Hwang2020COMETATOMIC2O} and a graph translation model \cite{Balazevic2019MultirelationalPG} to incorporate prior knowledge from the ATOMIC commonsense knowledge base, refining event representations for relational embedding learning. Additionally, it employs a Bayesian framework to estimate the uncertainty of the learned relations.

% uses the RoBERTa-large \cite{zhuang-etal-2021-robustly} encoder to represent event pairs and a graph translation model \cite{Balazevic2019MultirelationalPG} to learn relational embeddings. The model applies a Bayesian framework to estimate the uncertainty of the learned relations.

%\textbf{Bayesian \cite{tan-etal-2023-event}.} A strong pairwise model for temporal relation extraction, similar in architecture to the above RoBERTa baseline. However, it replaces the RoBERTa encoder with a COMET-BART encoder \cite{Hwang2020COMETATOMIC2O}, and integrate prior knowledge from the ATOMIC commonsense knowledge base.

% \textbf{RoBERTa \cite{tan-etal-2023-event}.} A pairwise model that uses the RoBERTa-large \cite{zhuang-etal-2021-robustly} encoder to represent event pairs and a graph translation model \cite{Balazevic2019MultirelationalPG} to learn relational embeddings. The model applies a Bayesian framework to estimate the uncertainty of the learned relations.


\textbf{RoBERTa \cite{tan-etal-2023-event}.} A strong pairwise model for temporal relation extraction, similar in architecture to the Bayesian model described above, but replacing the COMET-BART encoder with a RoBERTa-large encoder \cite{zhuang-etal-2021-robustly}. Unlike the Bayesian model, it learns relational embeddings without relying on prior knowledge from external sources. We use this model as it represents a strong, purely supervised approach, allowing for a direct comparison without the influence of external knowledge.



\textbf{Bayesian + Constraints.}  
We extend the Bayesian model with the transitive constraints optimization algorithm \cite{ning-etal-2018-joint}, the same algorithm used in our ZSL-GlobalConsistency method, applying it at inference time to enable a more direct comparison with our self and global consistency methods.


% \textbf{Bayesian + Constraints.} 
%An extended version of the Bayesian model that applies transitive constraints optimization at inference time, following \cite{ning-etal-2018-joint}.
% An extension of the Bayesian-Trans model, where transitive constraints optimization is applied at inference, following \cite{ning-etal-2018-joint}.


\textbf{CoT \cite{yuan-etal-2023-zero}.} 
As a baseline model, we re-implemented the CoT model \cite{yuan-etal-2023-zero} using GPT-4o, replacing the original implementation, which used ChatGPT. To the best of our knowledge, this is the strongest zero-shot approach for temporal relation extraction.

For evaluation, we report the F1 score on all datasets following the definition in \cite{ning-etal-2019-improved}, where the \textit{vague} relation is excluded from true positive predictions. 

\input{tables/main_results}


% We reproduced the results from the Zero-Shot Chain-of-Thought (CoT) method using GPT-4o.\footnote{\citet{yuan-etal-2023-zero} reported results on ChatGPT.}


% \begin{figure}[t]
%     \centering
%     \subfloat[\App{}]{\includegraphics[width=0.90\linewidth]{figures/omni_cs_not.png}\label{fig:cs-sent-expr:first}}
%     \vfill
%     \subfloat[NarrativeTime]{\includegraphics[width=0.90\linewidth]{figures/nt_cs_not.png}\label{fig:cs-sent-expr:second}}
%     \caption{Performance when evaluating on different sets of relations: (1) relations only between events in consecutive sentences, (2) relations only between events in non-consecutive sentences, and (3) relations across the entire document.}
%     \label{fig:cs-sent-expr}
% \end{figure}


% \begin{figure}[t]
%     \centering
%     \subfloat[NarrativeTime Vs. TimeBank-Dense]{\includegraphics[width=0.90\linewidth]{figures/per_rel_nt_tbd.png}\label{fig:per-rel-expr:first}}
%     \vfill
%     \subfloat[\App{} Vs. MATRES]{\includegraphics[width=0.90\linewidth]{figures/per_rel_omni_matres.png}\label{fig:per-rel-expr:second}}
%     \caption{Measuring the f1 score of the ZSL-Timeline method per relation between two datasets with similar annotation schemes. The relations are denoted as follows: A = after, B = before, I = includes, II = is-included, E = equal, and V = vague.}
%     \label{fig:per-rel-expr}
% \end{figure}


