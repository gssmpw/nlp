\section{Introduction}
\label{intro}
%Temporal relation extraction (TRE) is a fundamental task in natural language processing (NLP) that involves identifying the temporal relations between targeted events. Significant efforts have been dedicated to developing datasets \cite{gast-etal-2016-enriching, rogers-etal-2024-narrativetime} and models \cite{huang-etal-2023-classification, niu-etal-2024-contempo} for detecting such relations. The outputs of these models have been applied to various downstream tasks, including recent advancements in event forecasting \cite{Ma2023ContextawareEF}, misinformation detection \cite{lei-huang-2023-identifying}, and treatment timeline extraction \cite{yao-etal-2024-overview}.


Temporal relation extraction (TRE) is a fundamental task in natural language processing (NLP) that has been instrumental in various downstream tasks, including recent advancements in event forecasting \cite{Ma2023ContextawareEF}, misinformation detection \cite{lei-huang-2023-identifying}, and medical treatment timeline extraction \cite{yao-etal-2024-overview}. 

TRE is formulated as follows: given a text with event mentions marked within it, identify all the temporal relations between these events. Accordingly, and ideally, a dataset for evaluating TRE models should consist of annotated relations between all pairs of events. However, annotating temporal relations is highly challenging \cite{pustejovsky-stubbs-2011-increasing}, and \textit{complete} annotation -- where all possible event pairs in a document are labeled -- has traditionally been considered unfeasible for human annotators \cite{naik-etal-2019-tddiscourse}. To manage this complexity, most datasets include labels for only a subset of event pairs, applying filtering methodologies such as restricting annotations to events within consecutive sentences \cite{chambers-etal-2014-dense, ning-etal-2018-multi} or creating temporal relation annotations through automated processes \cite{naik-etal-2019-tddiscourse, alsayyahi-batista-navarro-2023-timeline}. 
However, such restrictions can lead to unreliable model assessments, failing to accurately reflect a model’s ability to capture long-range relations, or reinforce biases introduced by automated annotation techniques.
% Others did not provide systematic annotation protocols to ensure completeness \cite{pustejovsky-etal-2003, wang-etal-2022-maven}, leading to criticism of their coverage \cite{pustejovsky-stubbs-2011-increasing, rogers-etal-2024-narrativetime}.
Furthermore, incomplete annotations and the lack of global coverage have led the field to primarily focus on developing \textit{pairwise} methods \cite{wen-ji-2021-utilizing, zhou-etal-2022-rsgt}, where a model extracts temporal relations between a single event pair at a time. 
Yet, such methods overlook the document's global temporal structure, resulting in inconsistencies in the output temporal graph \cite{wang-etal-2020-joint}, and are computationally inefficient, requiring $O(n^2)$ inference requests to predict all temporal relations across $n$ given events.
%However, such methods fail to incorporate a global view of the document’s temporal structure, disregarding the broader temporal graph that emerges from event interactions. Additionally, these approaches are computationally inefficient, requiring $O(n^2)$ inference requests to predict all temporal relations within a document with $n$ event mentions.

%Furthermore, incomplete annotations and the lack of global coverage have led the field to primarily focus on developing \textit{pairwise} methods \cite{wen-ji-2021-utilizing, zhou-etal-2022-rsgt}, where a model extracts temporal relations between a single event pair at a time. 
%However, such methods overlook the document's global temporal structure, leading to inconsistent temporal graph outcomes \cite{wang-etal-2020-joint}, and are computationally inefficient, requiring $O(n^2)$ inference requests to predict all temporal relations in a document with $n$ events.


\input{figures/figure1}

Despite these challenges, TRE has seen significant progress in the development of supervised models \cite{tan-etal-2023-event, niu-etal-2024-contempo}. However, current utilization of LLMs remains limited, particularly in zero-shot settings \cite{10.5555/3600270.3601883}.
%The most successful approaches leverage external knowledge \cite{wang-etal-2020-joint}, commonsense reasoning \cite{tan-etal-2023-event}, and knowledge distilled from LLMs \cite{niu-etal-2024-contempo, Chen2024PromptBasedET}. 
%However, the utilization of LLMs—which have proven remarkably effective in computing both factual and common-sense knowledge \cite{liu-etal-2022-generated}—remains limited, particularly in zero-shot settings \cite{10.5555/3600270.3601883}. 
The only existing studies \cite{yuan-etal-2023-zero, chan-etal-2024-exploring} have employed local pairwise prompting strategies, yielding suboptimal results while also being time- and cost-inefficient. Consequently, the application of LLMs to TRE has been widely regarded as ineffective \cite{wei-etal-2024-llms, niu-etal-2024-contempo, chan-etal-2024-exploring}.



% The implications of incomplete annotation can be categorized into two main issues: 
% First, the lack of global coverage of relations has propelled the field to primarily focus on developing \textit{pairwise} methods \cite{wen-ji-2021-utilizing, zhou-etal-2022-rsgt, tan-etal-2023-event, niu-etal-2024-contempo}, where the model is provided with a single event pair at a time to extract the relation. However, such methods lack a global perspective, particularly the broader view of the temporal graph that can be derived from a given document and its events. Additionally, they are computational ineffective requiring $O(n^2)$ inference request to predict all relations in a given document.
% Second, evaluating models on resources with biased annotation may lead to unfair assessments. For example, it may not accurately reflect a model's ability to capture long-range relations or could reinforce annotation biases introduced by the automatic processes used in dataset creation.

% \input{figures/figure1}

% Furthermore, studies dedicated to building supervised models have demonstrated the benefits of leveraging external knowledge \cite{wang-etal-2020-joint}, incorporating common-sense knowledge \cite{tan-etal-2023-event}, and utilizing knowledge distilled from large language models (LLMs) \cite{niu-etal-2024-contempo, Chen2024PromptBasedET} to advance the field.

% % Prior work has explored various strategies to enhance temporal relation extraction (TRE). Some approaches incorporate external knowledge sources to improve model reasoning \citep{wang-etal-2020-joint}, while others leverage common-sense knowledge bases to refine event understanding \citep{tan-etal-2023-event}. Additionally, recent work has focused on distilling knowledge from LLMs to enhance relation extraction without extensive training data \citep{niu-etal-2024-contempo, Chen2024PromptBasedET}. However, despite these advances, most methods rely on \textit{pairwise event classification}, limiting their ability to capture \textit{global} document-level consistency. Our approach addresses this limitation by leveraging LLMs to generate the entire temporal graph in a single step, enforcing global coherence through self-consistency and transitive constraints.


% Nevertheless, the utilization of LLMs—which have proven remarkably effective in computing both common knowledge and common-sense knowledge \cite{liu-etal-2022-generated}—has been rather limited for solving the TRE task, particularly in the zero-shot setting \cite{10.5555/3600270.3601883}. The most notable efforts in this settings \cite{yuan-etal-2023-zero, chan-etal-2024-exploring} have employed local pairwise approaches using ChatGPT, yielding suboptimal performance while also being time- and cost-inefficient. Consequently, the application of LLMs to the TRE task has been widely regarded as ineffective \cite{alsayyahi-batista-navarro-2023-timeline, wei-etal-2024-llms, niu-etal-2024-contempo, chan-etal-2024-exploring}.


In response, we demonstrate how to move beyond pairwise approaches by using LLMs. We introduce a novel zero-shot method that generates the entire temporal graph \textit{globally} in a single step (illustrated in Figure~\ref{fig:figure1} and explained in §\ref{section:model}). 
We then extend this basic zero-shot approach in two major ways. First, we prompt the model to ``think'' by asking it to summarize the timeline of the given events in free-form language before generating the requested temporal classification labels for all event pairs. Second, we collect label distributions by running the model multiple times and then apply a global constraints algorithm that considers these distributions to produce a final globally-optimal graph of relations.
We show that our method significantly outperforms the existing zero-shot pairwise approach across most datasets while being more efficient, as shown in §\ref{section:results}. Our findings demonstrate that, contrary to previous research, LLMs in zero-shot settings may be a valid alternative to supervised models for temporal relation extraction.


Additionally, to address the incompleteness of temporal relation datasets, we develop \textit{\App{}}, a new dataset that incorporates temporal relations between all pairs of targeted events to support unbiased evaluation (§\ref{section:dataset}).\footnote{The \App{} dataset will be made publicly available.} Our subsequent analysis further highlights the importance of datasets with complete pairwise annotation for reliable evaluation of temporal relation graph generation (§\ref{section:results:quality}).

To recap, this research makes the following three contributions: 
(1) A novel global zero-shot approach for generating consistent temporal relation graphs.  
(2) A new dataset with complete temporal relation annotations.
(3) A thorough analysis highlighting the importance of datasets with complete annotations for fair evaluation of zero-shot approaches for TRE.
