\section{Zero-Shot Temporal Graph Generation}
\label{section:model}
% The landscape of modeling approaches for temporal relation extraction has been dominated by the pairwise approach. In this approach, the model receives a single pair at a time, extracts the temporal relation for that pair, and only then proceeds to the next pair. Applying pairwise approaches using LLMs has yielded significantly lower performance compared to task-specific trained models and has been widely regarded as ineffective for solving this task \cite{alsayyahi-batista-navarro-2023-timeline, wei-etal-2024-llms, niu-etal-2024-contempo, chan-etal-2024-exploring}.

% However, given the vast knowledge embedded in contemporary LLMs, their multi-task performance, common-sense reasoning capabilities, and ability to process extensive text with instructions, we were motivated to revisit the zero-shot setting using a new global approach. In this section, we describe the three methods we investigated.

% Given the extensive knowledge encoded in modern LLMs, their impressive multi-task capabilities, apply common-sense reasoning, and process large volumes of text with instructions, we were inspired to reevaluate the zero-shot setting using an approach that leverages global information. This section outlines the three methods we explored.

% \input{tables/transfere}

In this section we describe our three zero-shot learning (ZSL) methods, ZSL-Global (§\ref{section:model:global}), ZSL-Timeline (§\ref{section:model:pipeline}), and ZSL-Pipeline (§\ref{section:model:pipeline}). For all our experiments we use GPT-4o which we access via the OpenAI API\footnote{\url{https://platform.openai.com}}.


\subsection{ZSL-Global}
\label{section:model:global}
\alone{Start with referring to the prompt example in the appendix}.
Our first method takes a step toward transitioning from a pairwise to a global approach, generating all temporal relations for a document in a single pass. 
To achieve this, we define three key objectives: (1) ensuring the model generates relations for all event pairs without omissions, (2) ensuring the generated relations are structured in a way that can be reliably parsed, and (3) handling long documents with many events, potentially by segmenting them to stay within the model’s token limit.

Therefore, we include the full set of pairs as part of the prompt to GPT-4o and explicitly instruct the model to generate relations only for these pairs. This addresses objectives (1) and (3) above, since it allows us to also divide the pairs into groups to meet the model's token limit. 
Second, to address objective (2), we instruct the model to generate relations as a graph, with events as nodes and relations as links, and format the output in the DOT language \cite{Gansner2006DrawingGW}. 
%We found that using the DOT language led GPT-4o to consistently generate valid outputs that we could easily parse into a structured representation, while also consolidating the required relations into a single block and minimizing unrequested explanations.

To summarize, our prompt begins with a concise instruction about the task, following the zero-shot approach proposed by \citet{yuan-etal-2023-zero}. We then include the entire document, with event mentions highlighted using the `<>' brackets along with explicit event identifiers (e.g., `<attack(7)>') so the model can uniquely identify and reference them. Finally, we append the set of event pairs for which the model is required to extract temporal relations.

% Our choice of using the DOT format was based on the observation that GPT-4o consistently generated this format without errors while ensuring that the required relations were produced within a single block. This approach minimized excessive output, such as unrequested explanations between relations.

% To conclude, we construct our prompt as follows: Similar to the zero-shot method proposed by \cite{yuan-etal-2023-zero}, we first provide the model with a brief instruction on the task. Following this, we present the entire document with event mentions highlighted in it using `<>' symbols, along with explicit event identifiers to help the model distinguish between events (e.g., `<attack(7)>'). Finally, the prompt concludes with the set of event pairs for which the model is required to extract temporal relations.


\subsection{ZSL-Timeline}
\label{section:model:timeline}
Figure~\ref{fig:figure1}-[1] shows an example of the ZSL-Timeline prompt.
We observed that, with the zero-shot prompt, the model sometimes generates relations that contradict the text and certain temporal indicators. However, when asked to explain the temporal relation of a specific event pair it initially misclassified, GPT-4o can provide the correct event sequence. Moreover, when prompted to regenerate the relation based on its own explanation, the model produces the correct relation.  

Building on this, the Timeline method operates in two phases. In the first phase, the model is instructed to generate a timeline of events in an unstructured, free-form manner. In the second phase, using this timeline, the model determines the temporal relations between all event pairs.

% Furthermore, requesting the model to first generate the timeline and then derive the relations from it resolves such errors. 


\subsection{ZSL-Pipeline}
\label{section:model:pipeline}
% Our final approach (illustrated in Figure~\ref{fig:figure1}) aims to mitigate the model's inconsistent generation. Like any deep learning model, LLMs are inherently stochastic (\alone{cite something here}), which can lead to slight performance variations and inconsistencies between generations, particularly in challenging pairs where the temporal relation is unclear from the given text.  

% To address this, we generate five separate outputs for each document using our Timeline approach, denoted as \( G = \{g_1, g_2, g_3, g_4, g_5\} \), where each \( g_n \) represents a single graph which we parsed from the generated DOT-language output. Each Graph \( g_n \) consists of all pairwise predictions, \( Pairs = \{p_{12}, p_{13}, p_{14}, \dots, p_{nm}\} \), where each pairwise relation \( p_{ij} \) is represented as a one-hot vector over the relation classes: before, after, includes, is-included, equal, and vague. Following this, we sum and normalize all $Pairs$ from all $G$'s into a single vector \(Dist = \{d_{12}, d_{13}, d_{14}, \dots, d_{nm}\}\), such that each $d_{ij}$ is the normalized distribution vector over the relations for this pair. Following this, we apply the transitive constraints optimization algorithm proposed by \citet{ning-etal-2018-joint} to construct the final temporal graph.

Our final approach is illustrated in Figure~\ref{fig:figure1}. Since LLMs are stochastic, they may produce different outputs for the same input, especially when it is relatively hard to predict the label of a specific event pair.  

Therefore, we use the Timeline approach from the previous section by repeatedly prompting the LLM to generate five outputs per document. These outputs are denoted as \( G = \{g_1, g_2, g_3, g_4, g_5\} \), where each \( g_i \) corresponds to a single graph parsed from the DOT-language output. We parse the output by representing each graph \( g_i \) as a set of predicted event-pair relations, \( \{p_{12}, p_{13}, \dots, p_{23}, p_{24}, \dots, p_{nm}\} \), where each relation \( p_{ij} \) is encoded as a one-hot vector over the six relation types. We then sum these vectors element-wise across all five graphs and normalize them to obtain a set of single vectors \( \{d_{12}, d_{13}, \dots, d_{23}, d_{24}, \dots, d_{nm}\} \). Each \( d_{ij} \) represents the normalized label distribution over relation types for the corresponding event pair. These label distribution vectors are then used as input to the transitive constraints optimization algorithm proposed by \citet{ning-etal-2018-joint} to construct the final temporal graph.


Finally, for comparison, since the method above can also be used to determine relations through majority voting over the normalized vector $d_{ij}$, we present a version of this pipeline where, instead of applying transitive constraints, we apply $argmax(d_{ij})$ to choose the most likely relation for each event pair.

% \input{tables/mat_omni}

% \input{tables/tb_nt}

