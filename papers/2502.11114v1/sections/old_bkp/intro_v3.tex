\section{Introduction}
\label{intro}
Temporal relation extraction (TRE) is a fundamental task in natural language processing (NLP) that involves identifying the temporal relations between targeted events. Significant efforts have been dedicated to developing datasets \cite{gast-etal-2016-enriching, rogers-etal-2024-narrativetime} and models \cite{huang-etal-2023-classification, niu-etal-2024-contempo} for detecting such relations. The outputs of these models have been applied to various downstream tasks, including recent advancements in event forecasting \cite{Ma2023ContextawareEF}, misinformation detection \cite{lei-huang-2023-identifying}, and treatment timeline extraction \cite{yao-etal-2024-overview}.


TRE is formulated as follows: given a text with event mentions marked within it, identify all the temporal relations between these events. Ideally, a dataset for evaluating TRE models should consist of annotated relations between all pairs of events. However, annotating temporal relations is highly challenging \cite{pustejovsky-stubbs-2011-increasing}, and \textit{complete} annotation—where all possible event pairs in a document are labeled—has traditionally been considered unfeasible for human annotators \cite{naik-etal-2019-tddiscourse}. To manage this complexity, many datasets include labels for only a subset of event pairs, applying filtering methodologies such as restricting annotations to events within consecutive sentences \cite{chambers-etal-2014-dense} or creating temporal relation annotations through automated processes \cite{alsayyahi-batista-navarro-2023-timeline}.
% Others did not provide systematic annotation protocols to ensure completeness \cite{pustejovsky-etal-2003, wang-etal-2022-maven}, leading to criticism of their coverage \cite{pustejovsky-stubbs-2011-increasing, rogers-etal-2024-narrativetime}.

The implications of incomplete annotation can be categorized into two main issues: First, the lack of global coverage of relations has led the field to primarily focus on developing \textit{pairwise} methods \cite{wen-ji-2021-utilizing, zhou-etal-2022-rsgt}, where a model extracts temporal relations between a single event pair at a time. 
However, such methods overlook the document's global temporal structure and are computationally inefficient, requiring $O(n^2)$ model requests to predict all temporal relations in a document with $n$ event mentions.
%However, such methods fail to incorporate a global view of the document’s temporal structure, disregarding the broader temporal graph that emerges from event interactions. Additionally, these approaches are computationally inefficient, requiring $O(n^2)$ inference requests to predict all temporal relations within a document with $n$ event mentions.
Second, evaluating models on datasets with annotations limited to consecutive sentences or with automatically generated annotations can lead to unfair assessments, fail to accurately reflect a model’s ability to capture long-range relations \cite{alsayyahi-batista-navarro-2023-timeline}, or reinforce biases introduced by automated annotation techniques.


\input{figures/figure1}

Despite these challenges, TRE remains an active research area, with significant progress in developing supervised models \cite{tan-etal-2023-event, niu-etal-2024-contempo, Chen2024PromptBasedET}. However, the utilization of LLMs remains limited, particularly in zero-shot settings \cite{10.5555/3600270.3601883}.
%The most successful approaches leverage external knowledge \cite{wang-etal-2020-joint}, commonsense reasoning \cite{tan-etal-2023-event}, and knowledge distilled from LLMs \cite{niu-etal-2024-contempo, Chen2024PromptBasedET}. 
%However, the utilization of LLMs—which have proven remarkably effective in computing both factual and common-sense knowledge \cite{liu-etal-2022-generated}—remains limited, particularly in zero-shot settings \cite{10.5555/3600270.3601883}. 
The only existing studies \cite{yuan-etal-2023-zero, chan-etal-2024-exploring} have employed local pairwise prompting strategies with ChatGPT, yielding suboptimal results while also being time- and cost-inefficient. Consequently, the application of LLMs to TRE has been widely regarded as ineffective \cite{alsayyahi-batista-navarro-2023-timeline, wei-etal-2024-llms, niu-etal-2024-contempo, chan-etal-2024-exploring}.



% The implications of incomplete annotation can be categorized into two main issues: 
% First, the lack of global coverage of relations has propelled the field to primarily focus on developing \textit{pairwise} methods \cite{wen-ji-2021-utilizing, zhou-etal-2022-rsgt, tan-etal-2023-event, niu-etal-2024-contempo}, where the model is provided with a single event pair at a time to extract the relation. However, such methods lack a global perspective, particularly the broader view of the temporal graph that can be derived from a given document and its events. Additionally, they are computational ineffective requiring $O(n^2)$ inference request to predict all relations in a given document.
% Second, evaluating models on resources with biased annotation may lead to unfair assessments. For example, it may not accurately reflect a model's ability to capture long-range relations or could reinforce annotation biases introduced by the automatic processes used in dataset creation.

% \input{figures/figure1}

% Furthermore, studies dedicated to building supervised models have demonstrated the benefits of leveraging external knowledge \cite{wang-etal-2020-joint}, incorporating common-sense knowledge \cite{tan-etal-2023-event}, and utilizing knowledge distilled from large language models (LLMs) \cite{niu-etal-2024-contempo, Chen2024PromptBasedET} to advance the field.

% % Prior work has explored various strategies to enhance temporal relation extraction (TRE). Some approaches incorporate external knowledge sources to improve model reasoning \citep{wang-etal-2020-joint}, while others leverage common-sense knowledge bases to refine event understanding \citep{tan-etal-2023-event}. Additionally, recent work has focused on distilling knowledge from LLMs to enhance relation extraction without extensive training data \citep{niu-etal-2024-contempo, Chen2024PromptBasedET}. However, despite these advances, most methods rely on \textit{pairwise event classification}, limiting their ability to capture \textit{global} document-level consistency. Our approach addresses this limitation by leveraging LLMs to generate the entire temporal graph in a single step, enforcing global coherence through self-consistency and transitive constraints.


% Nevertheless, the utilization of LLMs—which have proven remarkably effective in computing both common knowledge and common-sense knowledge \cite{liu-etal-2022-generated}—has been rather limited for solving the TRE task, particularly in the zero-shot setting \cite{10.5555/3600270.3601883}. The most notable efforts in this settings \cite{yuan-etal-2023-zero, chan-etal-2024-exploring} have employed local pairwise approaches using ChatGPT, yielding suboptimal performance while also being time- and cost-inefficient. Consequently, the application of LLMs to the TRE task has been widely regarded as ineffective \cite{alsayyahi-batista-navarro-2023-timeline, wei-etal-2024-llms, niu-etal-2024-contempo, chan-etal-2024-exploring}.


In response, this work makes the following contributions to address these challenges.
Our first contribution demonstrates how to move beyond pairwise approaches by using LLMs. We introduce a novel zero-shot method that generates the entire temporal graph \textit{globally} in a single step, as illustrated in Figure~\ref{fig:figure1} and further explained in §\ref{section:model}. 
We extend this basic zero-shot approach in two ways. First, we prompt the model to ``think'' by summarizing the timeline of events in free-form language before generating the labels. Second, we collect label distributions by running the model multiple times and apply a global constraints algorithm that incorporates these distributions to produce the final graph of relations.
We show that our method significantly outperforms the existing zero-shot pairwise approach across most datasets while being more efficient, as shown in §\ref{section:results}. Our findings suggest that, contrary to common belief, LLMs in zero-shot settings can be an alternative to supervised models for temporal relation extraction.


Our second contribution addresses the incompleteness of temporal relation datasets and the need for a dataset that incorporates global label information for unbiased evaluation. To this end, we developed \textit{\App{}}, a new dataset structured similarly to MATRES \cite{ning-etal-2018-multi} but covering temporal relations between all events in the texts (further detailed in §\ref{section:dataset}).\footnote{The \App{} dataset will be made publicly available under the MIT license.} Finally, our analysis highlights the importance of complete annotated datasets for a fair evaluation of zero-shot methods (further discussed in §\ref{section:results:quality}).
