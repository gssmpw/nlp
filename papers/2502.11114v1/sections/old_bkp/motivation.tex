\section{Motivation}
\begin{itemize}
    \item Motivation for the event temporal graph task
    \item Existing studies approach the task in two steps:
    \begin{itemize}
        \item First, extract the events
        \item Then, extract the relations
    \end{itemize}
    \item Most previous work used a classification approach, where the model is trained and tested on pairs of events to extract the relation between them.
    \item This approach is sub-optimal performance-wise, as the model needs to check all pairs.
    \item A recent approach (\alone{"Neural Language Modeling for Contextualized Temporal Graph Generation" and "Set-Aligning Framework for Auto-Regressive Event Temporal Graph Generation"}) formulated the task as an end-to-end temporal graph generation task. The model input is a document, and the output is the generated temporal relation graph.
    \item While these approaches leverage the full potential of LLMs, the authors used training datasets that are sparse and only capture verbal events.
    \item This holds several disadvantages:
    \begin{itemize}
        \item Events can also be non-verbal; however, the model will not extract them.
        \item The authors created heuristics for filtering "important" events, which are not general and may not fit every task/application.
        \item Sparse temporal event relation dataset. While testing a model on a sparse graph is justified (we can only check against the gold relations),
        \item When training, this means the model cannot learn relation distribution of temporal relation graphs (where every pair should have a relation). Even worse, using the methods in these papers, the model will penalized (\alone{need to verify this}) for extracting relations that are correct but not part of the gold labels.
    \end{itemize}
    \item To address these limitations, in this work, we create a new task and dataset. Our contributions are as follows:
    \begin{itemize}
        \item Many works select different approaches for event selection (while no definitive definition of what is an important or significant event). However, event detection models are quite effective at extracting both verbal and non-verbal events. To support flexibility for the model to be trained on events considered important for a specific task or application, we formulate the task so that the input includes the selected events (extracted manually or using an event detection tool).
        \item To support the training of models on dense graphs and extract the full event-event temporal relation, we released a dataset exhaustively annotated by trained annotators.
        \item \alone{I'm here} to show that our approach indeed increases the ability of models to learn the full distribution of events, we fine-tuned an LLM using our dataset and tested it on other datasets (MATRES/TBD) \alone{TBD - Check if there are results published on the relation extraction task \textbf{without} the event detection}.
    \end{itemize}
\end{itemize}
