\section{Introduction}
\label{intro}
Temporal relation extraction (TRE) is a fundamental task in natural language processing (NLP) that involves identifying the temporal relations between targeted events. Significant efforts have been dedicated to developing datasets \cite{chambers-etal-2014-dense, gast-etal-2016-enriching, ning-etal-2018-multi} and models \cite{huang-etal-2023-classification, tan-etal-2023-event, niu-etal-2024-contempo} for detecting such relations. The outputs of these models have been applied to various downstream tasks, including recent advancements in event forecasting \cite{Ma2023ContextawareEF}, misinformation detection \cite{lei-huang-2023-identifying}, and treatment timeline extraction \cite{yao-etal-2024-overview}.


TRE is formulated as follows: given a text with event mentions marked within it, identify all the relations between these events. Thus, ideally, a dataset for evaluation will consist of annotated relations between all pair. However, annotating temporal relations is highly challenging \cite{pustejovsky-stubbs-2011-increasing}, and \textit{exhaustive} annotation has traditionally been considered unfeasible for human annotators \cite{naik-etal-2019-tddiscourse}. To manage this complexity, many datasets limit annotations using constraints, such as restricting annotations only to events between consecutive sentences \cite{chambers-etal-2014-dense, ning-etal-2018-multi, ning-etal-2018-joint} or employing automated methods \cite{naik-etal-2019-tddiscourse, alsayyahi-batista-navarro-2023-timeline}. Others did not provide systematic annotation protocols to ensure completeness \cite{pustejovsky-etal-2003, wang-etal-2022-maven}, leading to criticism of their coverage \cite{pustejovsky-stubbs-2011-increasing, rogers-etal-2024-narrativetime}.


The implications of incomplete annotation can be categorized into two main issues: 
First, the lack of global coverage of relations has propelled the field to primarily focus on developing \textit{pairwise} methods \cite{wen-ji-2021-utilizing, zhou-etal-2022-rsgt, tan-etal-2023-event, niu-etal-2024-contempo}, where the model is provided with a single event pair at a time to extract the relation. However, such methods lack a global perspective, particularly the broader view of the temporal graph that can be derived from a given document and its events. Additionally, they are computational ineffective requiring $O(n^2)$ inference request to predict all relations in a given document.
Second, evaluating models on resources with biased annotation may lead to unfair assessments. For example, it may not accurately reflect a model's ability to capture long-range relations or could reinforce annotation biases introduced by the automatic processes used in dataset creation.

\input{figures/figure1}

Furthermore, studies dedicated to building supervised models have demonstrated the benefits of leveraging external knowledge \cite{wang-etal-2020-joint}, incorporating common-sense knowledge \cite{tan-etal-2023-event}, and utilizing knowledge distilled from large language models (LLMs) \cite{niu-etal-2024-contempo, Chen2024PromptBasedET} to advance the field.

% Prior work has explored various strategies to enhance temporal relation extraction (TRE). Some approaches incorporate external knowledge sources to improve model reasoning \citep{wang-etal-2020-joint}, while others leverage common-sense knowledge bases to refine event understanding \citep{tan-etal-2023-event}. Additionally, recent work has focused on distilling knowledge from LLMs to enhance relation extraction without extensive training data \citep{niu-etal-2024-contempo, Chen2024PromptBasedET}. However, despite these advances, most methods rely on \textit{pairwise event classification}, limiting their ability to capture \textit{global} document-level consistency. Our approach addresses this limitation by leveraging LLMs to generate the entire temporal graph in a single step, enforcing global coherence through self-consistency and transitive constraints.


Nevertheless, the utilization of LLMs—which have proven remarkably effective in computing both common knowledge and common-sense knowledge \cite{liu-etal-2022-generated}—has been rather limited for solving the TRE task, particularly in the zero-shot setting \cite{10.5555/3600270.3601883}. The most notable efforts in this settings \cite{yuan-etal-2023-zero, chan-etal-2024-exploring} have employed local pairwise approaches using ChatGPT, yielding suboptimal performance while also being time- and cost-inefficient. Consequently, the application of LLMs to the TRE task has been widely regarded as ineffective \cite{alsayyahi-batista-navarro-2023-timeline, wei-etal-2024-llms, niu-etal-2024-contempo, chan-etal-2024-exploring}.


In response, this research makes the following contributions to address the above concerns: First, to move beyond pairwise approaches and capitalize on the advances made by LLMs, we introduce a novel zero-shot method that generates the entire temporal graph \textit{globally} in a single step (illustrated in Figure~\ref{fig:figure1} and detailed in §\ref{section:model}). We demonstrate that our method significantly outperforms the previous zero-shot pairwise approach across most datasets and is competitive with supervised models, highlighting that LLMs, in zero-shot settings, can be an effective and efficient alternative to supervised models, especially in domains where training data is unavailable (further detailed in §\ref{section:results}).


Our second contribution addresses the incompleteness of temporal relation resources and our aspiration to provide a dataset that incorporates global label information. To that end, we leveraged the new EventFull annotation tool \cite{eirew2024eventfullcompleteconsistentevent}, designed to facilitate exhaustive event-event relation annotation. Using this tool, we developed a new dataset, \textit{\App{}}, constructed following the same annotation guidelines as MATRES but covering temporal relations between all events in the texts (further detailed in §\ref{section:dataset}).\footnote{The \App{} dataset will be publicly available.} Finally, through analysis, we demonstrate the importance of densely annotated resources for the fair evaluation of zero-shot methods (further discussed in §\ref{section:results:quality}).


% Recently, with a similar motivation to ours, NarrativeTime \cite{rogers-etal-2024-narrativetime} conducted the most extensive re-annotation of the TimeBank-Dense corpus, where \textit{all} relations between \textit{all} event pairs are judged and annotated. NarrativeTime and TimeBank-Dense share a similar task scope, encompassing the same set of temporal relations and adhering to similar annotation guidelines. However, there remains a need for additional exhaustively annotated resources that present alternative settings for the task, such as those aligned with the prominent MATRES dataset. To address this, we developed a new dataset, \textit{\App{}}, constructed using the same annotation guidelines as MATRES but considering relations between all events in the texts (further detailed in §\ref{section:dataset}). Finally, through analysis, we demonstrate the importance of densely annotated resources for the fair evaluation of zero-shot methods (see §\ref{section:results:quality}).

% Evaluating unsupervised methods incapable to learn this annotation bias, questions the validity and fairness of the evaluation, given that all pairs should be considered at inference.

% As a compromise to this intrinsic complexity, the creators of TimeBank-Dense \cite{chambers-etal-2014-dense} suggested annotating all relations exhaustively only between events in consecutive sentences. This approach was later adopted by MATRES \cite{ning-etal-2018-multi}. As a consequence, the two most widely used datasets for evaluating the task are annotated only for event relations within consecutive sentences.

% Such studies dedicated to building supervised models have demonstrated the benefits of leveraging external knowledge \cite{wang-etal-2020-joint}, incorporating common-sense knowledge \cite{tan-etal-2023-event}, and utilizing knowledge distilled from large language models (LLMs) \cite{niu-etal-2024-contempo, Chen2024PromptBasedET} to advance the field.

% Furthermore, an exhaustively annotated dataset for evaluating models on the TRE task has long been an aspiration. 

% Only very recently have efforts begun to address this challenge \cite{rogers-etal-2024-narrativetime, eirew2024eventfullcompleteconsistentevent}. is conducted almost exclusively on datasets consisting of non-complete biased annotation, for instance, the most adopted datasets for the task TB-Dense \cite{chambers-etal-2014-dense} and MATRES \cite{ning-etal-2018-multi}, annotate only relations between events in consecutive sentences. Such annotation decisions became more crusial when evaluating unsupervised methods 

% The focus of current modeling and prompting approaches on the pairwise method may be attributed in part to the incomplete annotations of temporal relations in datasets for the task. For example, the most widely adopted TRE datasets, TB-Dense \cite{chambers-etal-2014-dense} and MATRES \cite{ning-etal-2018-multi}, annotate only relations between events in consecutive sentences, while the more recent TDDiscourse \cite{naik-etal-2019-tddiscourse} extends TB-Dense by incorporating randomly selected long-range relations. Although this partially mitigates the limitation of annotating only short-distance relations, the resulting dataset remains relatively sparse (as presented in Table~\ref{tab:stats}). Indeed, this lack of globally annotated gold examples constrains modeling approaches to more local methods.


% In response to the above, in this research we strive to make contribution in two aspects, first, to advance beyond the pairwise approaches, and present a zero-shot method that is comparable with supervised task specific models. To that end, we propose a new pipeline approach (illustrated in Figure~\ref{fig:figure1}) that consist of a new zero-shot, chain-of-thought prompting approach, that generates the entire temporal graph globally in one generation. Then, given that this approach is significantly more cost and time effective then pairwise approach, and observing that such graph generations can be both inconsistent between generations and consist of temporal inconsistencies (for example, generating a relation triplet saying; A precede B, B precedes C, and C precedes A). To create a more robust result and consistent graph, we generate five temporal graphs for each document, then join the predictions from the different generation to form a probability matrix that represent the document graph joint probabilities for each generation. Then given this distribution, we apply a transitive constraints optimization algorithm \cite{ning-etal-2018-joint} to build the final graph output (further detailed in §\ref{section:model}). We show that this pipeline approach significantly outperform prior zero-shot methods across most datasets, and is segnificantly more effective.



% Since this approach is significantly more cost- and time-efficient than pairwise methods, we also address its inherent challenges—namely, inconsistency between generations and temporal contradictions (e.g., generating creates an inconsistent timeline stating that A precedes B, B precedes C, and C also precedes A). To enhance robustness and consistency, we generate five temporal graphs for each document and aggregate the predictions into a probability matrix representing joint probabilities across generations. We then apply a transitive constraint optimization algorithm \cite{ning-etal-2018-joint} to construct the final, temporal consistencies, graph output (as detailed in §\ref{section:model}). Our results demonstrate that this pipeline significantly outperforms the prior zero-shot method across most datasets we evaluated on, while on some datasets present comparable results with supervised models trained for the task (see §\ref{section:results}).
