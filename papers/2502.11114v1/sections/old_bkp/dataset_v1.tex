\section{The \App{} Dataset}
\label{section:dataset}
In this section, we provide a detailed description of \App{}, including its structure (§\ref{section:background:struct}), annotation process (§\ref{section:background:annot-process}), dataset statistics and quality, and a comparison with previously released datasets for this task.


\subsection{Dataset Structure and Task}
\label{section:background:struct}
\App{} consists of a set of documents, where each document represents a \textit{complete} temporal graph with event mentions as the graph nodes and temporal relations—\textit{before}, \textit{after}, \textit{equal}, and \textit{vague}—as the graph edges. Notably, the temporal graph in \App{} is complete, meaning that every possible pair of event mentions is labeled with a relation, including transitive relations (but excluding symmetric relations, which can be easily inferred from the reverse relation of the pair).


Our goal in developing \App{} is to assist models in training and testing on the traditional temporal relation extraction (TRE) task \cite{do-etal-2012-joint, chambers-etal-2014-dense, ning-etal-2018-multi}. TRE is formulated as follows: given a document with a set of event mentions (graph nodes) highlighted within it, the output should be the set of temporal relations (edges) between these events. Following this formulation, \App{} is not designed to support the event extraction task, and events are assumed to be pre-extracted through an offline process.


\subsection{\App{} Annotation Process}
\label{section:background:annot-process}
For the annotation process, we hired three non-expert annotators, all native English speakers and either first-degree students or graduates. To assist the annotators, we utilized the recently released EventFull annotation tool \cite{eirew2024eventfullcompleteconsistentevent}, designed to facilitate complete and exhaustive event-event relation annotation (covering temporal, causal, and coreference relations, with an optional event selection phase). This tool offers several advantages: 1) It reduces the annotation load by automatically filling temporal relations that can be inferred from previously made annotations. 2) It prevents temporal relation inconsistencies by alerting annotators when contradictory temporal annotations are made. 3) It guides annotators through the annotation process and ensures that the task is not completed until all relations have been annotated. We customized EventFull to focus exclusively on the event selection and temporal relation annotation steps.

Using the tool, annotators first underwent 2-3 training iterations on 2-3 test documents to ensure a thorough understanding of the tasks. The actual annotation was conducted on 30 news documents, each approximately 500 words. To extract the initial set of event mentions, we employed the event detection method proposed by \citet{cattan-etal-2021-cross-document}, which extracted an average of 60 event mentions per document. Since exhaustively annotating such a resource is highly demanding for non-expert annotators, we followed the MATRES methodology \cite{ning-etal-2018-multi}. MATRES distinguishes event mentions by their temporal characteristics—events that are actual or ``anchorable in time'' (e.g., they \textit{\underline{won}} the game) are included, while wishful, intentional, or conditional events (e.g., I \textit{wish} they \textit{\underline{win}} the game) are excluded. For establishing the temporal relations between events, this methodology only consider the starting time of the events.

Following this, annotators were instructed, using the tool, to first select the 15-18 most salient events from each story that are also anchorable in time. After event selection, each document was annotated by the three annotators for the temporal relations between all pairs of the selected events. Finally, majority voting was used to determine the relation, and in cases of disagreement, the relation was labeled as \textit{vague}.


% we conducted the following annotation protocol. We first split the 30 documents into three document sets each with 10 documents (i.e., \{X, Y, Z\}), each set we designated to two different annotators from the three annotators. Given a set (e.g., set \{X\}), the third annotator which is not assign to this set is required to identifying ``anchorable'' event mentions on each document in the set. On average, 35 ``anchorable'' events were identified per document, from which the annotator is requested to identify the 15-18 most salient events. Then, for the temporal annotation, the two annotators the set is designated to them are instructed to first read the article and annotate the most obvious temporal relations by clicking on the relevant edges in the accompanying graph visualization. This approach has proven effective in helping them better understand the underlying story and avoid simple mistakes. Since this initial pass usually yields only a few annotations, they then make a second pass to address the remaining pairs. Once done, the third annotator is required to annotate only the pairs that the two annotators did not agree on, without seeing their annotation. Once done, one of the paper authors reviewed the annotation in scrutiny and in case of disagreement set the relation to vague. At the end of the annotation, we measure Inter-Annotator Agreement (IAA) between the three annotators \alone{Add the agreement table and point to it}.

\input{tables/agreement}

\input{tables/stats}

\subsection{Dataset Statistics and Quality}
\label{section:}
The statistics of the \App{} dataset are presented in Table~\ref{tab:stats}, alongside the most commonly used resources for training and evaluating temporal relation extraction models. The final annotated set of \App{}, consisting of 30 documents, includes 470 event mentions and 3,483 relations. Notably, using the MATRES methodology results in a dataset with a relation distribution similar to that of MATRES. In contrast, datasets such as TBD, TDD-Man, and NarrativeTime, which annotate six relation types and consider both the starting and ending times of events, exhibit different relation distributions.

Table~\ref{tab:dense} present the sparsity of these dataset, NarrativeTime and \App{}, are the only publicly available datasets to present complete annotations, while the other resources are rather sparsely annotated as expected given that MATRES, TBD and TCR only consider events between consecutive sentences and TDD-Man randomly selected pairs for annotations. Comparing to MATRES, while \App{} is of much smaller scale, per document, it is much more denser.

Finally, Table~\ref{tab:annot_agree} presents the annotation agreement achieved by our three annotators compared with MATRES, TBD, and NarrativeTime. We observe that the agreement is on par with the levels achieved on these resources. Furthermore, to verify the accuracy of the annotations, one of the authors randomly selected 50 pairs from the documents for re-annotation. The resulting annotations showed agreement on 46 relations with the majority voting and 4 disagreements between the author and the majority voting of the annotators.
