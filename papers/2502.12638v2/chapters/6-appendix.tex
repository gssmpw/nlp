\section{Limitations and More Future Works}
NExT-Mol has several limitations that have not been addressed due to our limited computational resources and other technical challenges. We outline these limitations below:

\textbf{Explore Generalization of 3D Conformer Prediction to Unseen Scaffolds.} Table~\ref{tab:scaffold} shows that DMT-B's performance drop significantly on test molecules with unseen scaffolds in the training set. While our proposed transfer learning using MoLlama's pretrained 1D representations can mitigate this issue, there is still room for improvement. Future work could explore advanced generalization techniques and the integration of chemical inductive biases to enhance performance on unseen scaffolds. Additionally, developing a more comprehensive evaluation benchmark with a stricter scaffold split would provide deeper insights into model generalization. We leave these for future research.

\textbf{Explore Randomized SELFIES Data Augmentation in Pretraining.} Although randomized SELFIES augmentation shows promising results when fine-tuning MoLlama for 1D molecule generation, we do not use this augmentation technique during pretraining due to our limited computational resources. We believe applying this technique in pretraining could lead to different outcomes. We leave this exploration for future work.

\textbf{Explore Pretrained Molecular Large LM with Bi-directional Self-Attention.} MoLlama uses causal self-attention, where each token can only attend to previous tokens. While this approach is a good fit for 1D molecule generation, it constrains MoLlama's potential for molecule representation learning. To mitigate this issue, we have attached a bi-directional self-attention layer after MoLlama (\cf Figure~\ref{fig:integrate}). However, a more natural solution would be to use a molecular LM with built-in bi-directional self-attention. Due to resource constraints, we do not pursue this, and existing works are often limited in scale~\citep{Chemformer,zheng2024bert}. We hope this work draws more attention to this area and encourages the development of more foundation models for biochemistry.

\textbf{Explore NExT-Mol for Struture-based Molecule Generation.} We do not explore NExT-Mol for structure-based molecule generation~\citep{zhang2023molecule} due to the limited scope of this work. However, NExT-Mol could be extended for this task by conditioning the generation process on the structural embeddings of target pockets, potentially using techniques like cross-attention, adaptive layer normalization, or soft-prompting~\citep{PrefixTuning}. We leave this for future works.

\textbf{Limited Exploration on Diffusion Guidance.} Our DMT model utilizes i.i.d. sampling, without exploring advanced sampling method like classifier guidance~\citep{ClassifierGuidance} and particle guidance~\citep{ParticleGuidance}. However, particle guidance demonstrates that a well-tuned guidance method can improve the conformer prediction by 10\% precision. This is because the 3D molecular conformational space is large, and a guidance method with appropriate chemical inductive bias can improve the sampling efficiency. We leave this exploration as a future work.

\textbf{Computational Cost when Incorporating MoLlama for 3D Conformer Prediction.} Incorporating MoLlama, a large LM with 960M parameters, increases training time. For example, training DMT-B alone (55M parameters) takes 52 seconds per epoch on an A100 GPU, while DMT-B \textit{with} MoLlama takes 210 seconds. We mitigated this problem by using a pretrained DMT-B, instead of training it from scratch, to reduce the training epochs when incorporating MoLlama. Yet, we will need improvement when transferring 1D representations from a large LM.

\textbf{Quadratic Memory Complexity of DMT's Pair Representation.} This pair representation incurs an additional $O(N^2)$ GPU memory cost than the standard transformer, compared to the standard transformer's $O(N)$ memory complexity when using FlashAttention, where N is the node number of molecular graphs. While we encountered no memory issues on the GEOM-DRUGS dataset (molecules with hundreds of nodes), this could be a bottleneck for molecules with thousands of nodes. Potential solutions include smaller batch sizes and model parallelism.

\textbf{More Future Works.}
In future, we plan to build multi-modal foundation models~\citep{liu2024prott,3dmolm} with NExT-Mol as the essential backbone for 3D molecule generation~\citep{TEDMol}, in order to support tasks like chemical reaction prediction~\citep{liu2024reactxt,shi2023relm} and drug-drug interaction prediction~\citep{moltc}. This is to support the broad application of LLMs for scientific discovery~\citep{sciassess,patentfinder} and multi-modal LLMs~\citep{li2024laso}.
We will also explore model editing techniques~\citep{fang2025alphaedit} to locate the influential parameters and update its knowledge according to the application requirement, and explore causal interventional methods~\citep{li2021interventional} to find the rationale part~\citep{li2023transformer,li2023discovering,li2023redundancy,li2022equivariant,li2022invariant} of 2D molecules that influence 3D conformer prediction.



\section{More Experimental Results}
\subsection{Ablation Study}
\textbf{Ablating MoLlama Pretraining.} As Table~\ref{tab:mollamapretrain} shows, pretraining significantly improves MoLlama's performances on the 1D distribution similarity metrics of SNN, Scaf and FCD, but slightly decreases novelty score (V\&U\&N). This may be because the model without pretraining prefers a more random sampling, increasing the novelty but reducing the similarity to the desired molecule distribution. Pretraining does not significantly influence stability and validity measures, because they are mostly guaranteed by the SELFIES representations.

\begin{table}[h]
\centering
\small
\caption{Ablation study for the MoLlama pretraining for 1D molecule generation on the GEOM-DRUGS dataset.}\label{tab:mollamapretrain}
\setlength{\tabcolsep}{4pt}
\begin{tabular}{lccccccccc} \toprule
Method                   & FCD$\downarrow$ & \multicolumn{1}{l}{AtomStable} & \multicolumn{1}{l}{MolStable} & V\&C           & V\&U           & V\&U\&N        & SNN            & Frag           & Scaf            \\ \midrule
MoLlama                  & \textbf{0.334} & \textbf{1.000}                 & \textbf{0.999}                & \textbf{1.000} & \textbf{0.999} & 0.945          & \textbf{0.529} & \textbf{0.999} & \textbf{0.552}   \\
\textit{w/o} pretraining & 0.586           & \textbf{1.000}                 & 0.995                         & \textbf{1.000} & \textbf{0.999} & \textbf{0.974} & 0.495          & \textbf{0.999} & 0.534          \\ \bottomrule
\end{tabular}
\end{table}


\begin{table}[h!]
\centering
\small
\caption{Ablating random rotation augmentation for 3D conformer prediction on GEOM-QM9.}\label{tab:rot_aug}
\begin{tabular}{lcccccccc} \toprule
                            & \multicolumn{2}{c}{COV-R (\%)$\uparrow$} & \multicolumn{2}{c}{AMR-R $\downarrow$} & \multicolumn{2}{c}{COV-P (\%)$\uparrow$} & \multicolumn{2}{c}{AMR-P $\downarrow$} \\ \cmidrule(lr){2-3} \cmidrule(lr){4-5}  \cmidrule(lr){6-7} \cmidrule(lr){8-9}
Method                     & Mean               & Median              & Mean               & Median            & Mean               & Median              & Mean               & Median            \\\midrule
DMT-B                      & \textbf{95.2}      & \textbf{100.0}      & \textbf{0.090}     & \textbf{0.036}    & \textbf{93.8}      & \textbf{100.0}      & \textbf{0.108}     & \textbf{0.049}    \\
\textit{w/o} rand rot aug. & \textbf{95.2}      & \textbf{100.0}      & 0.095              & 0.040              & 93.3               & \textbf{100.0}      & 0.113              & 0.053             \\ \bottomrule
\end{tabular}
\end{table}

\textbf{Random Rotation Augmentation.} Table~\ref{tab:rot_aug} shows that DMT benefits from random rotation augmentations. Unlike MCF~\citep{MCF}, which relies on fixed canonical rotations, this is a key improvement because real data may be out-of-distribution and do not follow canonical rotations.

\subsection{Molecule Property Prediction Results for MoLlama}
\begin{table}[t]
\centering
\small
\setlength{\tabcolsep}{2pt}
\caption{Molecule property regression results on four MoleculeNet datasets~\citep{MoleculeNet}. Baseline results are from~\citep{MolPROP}. Lower$\downarrow$ is better.}\label{tab:mpp}
\begin{tabular}{lcccc}\toprule
Method                & FreeSolv (RMSE)            & ESOL (RMSE)                 & Lipo (RMSE)                 & QM7 (MAE)                  \\ \midrule
\multicolumn{3}{l}{\textbf{Supervised Learning Methods}}             & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} \\
RF~\citep{MolCLR}            & 2.03{\scriptsize ±0.22}           & 1.07{\scriptsize ±0.19}            & 0.88{\scriptsize ±0.04}            & 122.7{\scriptsize ±4.2}            \\
SVM~\citep{MolCLR}           & 3.14{\scriptsize ±0.00}           & 1.50{\scriptsize ±0.00}            & 0.82{\scriptsize ±0.00}            & 156.9{\scriptsize ±0.0}            \\ \midrule
\multicolumn{3}{l}{\textbf{Supervised GNN-based Methods}}             & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} \\
GCN~\citep{kipf2017semi}           & 2.87{\scriptsize ±0.14}           & 1.43{\scriptsize ±0.05}            & 0.85{\scriptsize ±0.08}            & 122.9{\scriptsize ±2.2}            \\
GATv2~\citep{GATv2}         & 3.14{\scriptsize ±0.00}           & 1.41{\scriptsize ±0.00}            & 0.89{\scriptsize ±0.00}            & 113.3{\scriptsize ±0.0}            \\
GIN~\citep{xu2019powerful}           & 2.76{\scriptsize ±0.18}           & 1.45{\scriptsize ±0.02}            & 0.85{\scriptsize ±0.07}            & 124.8{\scriptsize ±0.7}            \\
SchNet~\citep{schutt2018schnet}        & 3.22{\scriptsize ±0.76}           & 1.05{\scriptsize ±0.06}            & 0.91{\scriptsize ±0.10}            & 74.2{\scriptsize ±6.0}             \\
3D Infomax~\citep{3DInfomax}    & 2.23{\scriptsize ±0.26}           & 0.95{\scriptsize ±0.04}           & 0.74{\scriptsize ±0.01}           & -                    \\
MGCN~\citep{MGCN}          & 3.35{\scriptsize ±0.01}           & 1.27{\scriptsize ±0.15}            & 1.11{\scriptsize ±0.04}            & 77.6{\scriptsize ±4.7}             \\
D-MPNN~\citep{D-MPNN}        & 2.18{\scriptsize ±0.91}           & 0.98{\scriptsize ±0.26}            & 0.65{\scriptsize ±0.05}            & 105.8{\scriptsize ±13.2}           \\ \midrule
\multicolumn{3}{l}{\textbf{Pretrained GNN-based Methods}} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} \\
Pretrain-GNN~\citep{pretrain_gnn}  & 2.83{\scriptsize ±0.12}           & 1.22{\scriptsize ±0.02}            & 0.74{\scriptsize ±0.00}            & 110.2{\scriptsize ±6.4}            \\
MolCLR~\citep{MolCLR}        & 2.20{\scriptsize ±0.20}           & 1.11{\scriptsize ±0.01}            & 0.65{\scriptsize ±0.08}            & 87.2{\scriptsize ±2.0}             \\ \midrule
\multicolumn{3}{l}{\textbf{LM-based Methods}}              & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} \\
ChemBERTa-2~\citep{chemberta-2}   & 2.047{\scriptsize ±0.00}          & 0.889{\scriptsize ±0.00}           & 0.798{\scriptsize ±0.00}           & 172.8{\scriptsize ±0.00}           \\
MolPROP~\citep{MolPROP}       & 1.70{\scriptsize ±0.09}           & 0.777{\scriptsize ±0.02}           & 0.733{\scriptsize ±0.02}           & 151.8{\scriptsize ±10.0}           \\
MoLlama, ours       & \textbf{1.59{\scriptsize ±0.04}}  & \textbf{0.740{\scriptsize ±0.01}}  & \textbf{0.627{\scriptsize ±0.01}}  & \textbf{63.5{\scriptsize ±1.6}}   \\ \bottomrule
\end{tabular}
\end{table}


\textbf{Experimental Settings.} To evaluate MoLlama's capabilities beyond 1D molecule generation, we apply it to molecular property prediction tasks~\citep{simsgt,RGCL}, highlighting the quality of its molecular representations. Following the setup in \citep{MolPROP}, we fine-tune MoLlama on four MoleculeNet~\citep{MoleculeNet} datasets: FreeSolv, ESOL, Lipo, and QM7. We adopt the same experimental settings and dataset splits as \citep{MolPROP}, reporting mean performance and standard deviation over 10 random seeds. 
Following Section~\ref{sec:integrate}, we attach a single-layer bi-directional self-attention layer after MoLlama to improve its encoding ability. After that, we apply a linear layer on the mean embedding of all molecule tokens for property prediction. For each run, MoLlama is trained for 100 epochs, with test performance selected based on the validation dataset. We use a fixed learning rate of 1e-4 with the AdamW optimizer, and fine-tune MoLlama using LoRA~\citep{lora} (LoRA $r=8$ and $\alpha=32$) applied to all linear layers of the model. 
Following Section~\ref{sec:1d-generation}, we apply the random SELFIES augmentation during training. During inference, we use the average prediction values of 20 differently augmented SELFIES as the final prediction.

\textbf{Observation.} As shown in Table~\ref{tab:mpp}, MoLlama significantly outperforms baseline methods, achieving relative improvements of 6.5\%, 4.7\%, 3.5\%, and 16.9\% on the FreeSolv, ESOL, Lipo, and QM7 datasets, respectively. Notably, our baselines include LM-based, GNN-based, and pretrained GNN-based methods, and MoLlama's better performance demonstrates its advantages derived from the extensive pretraining.


\begin{table}[t]
\small
\centering
\caption{Incorporating MoLlama's 1D representations to improve DMT's 3D conformer prediction.}\label{tab:app_integrate}
\setlength{\tabcolsep}{4pt}
\begin{tabular}{llcccccccc}\toprule
    &          & \multicolumn{2}{c}{COV-R (\%)$\uparrow$} & \multicolumn{2}{c}{AMR-R $\downarrow$} & \multicolumn{2}{c}{COV-P (\%)$\uparrow$} & \multicolumn{2}{c}{AMR-P $\downarrow$} \\ \cmidrule(lr){3-4} \cmidrule(lr){5-6}  \cmidrule(lr){7-8} \cmidrule(lr){9-10}
Dataset                     & Method   & Mean               & Median             & Mean              & Median           & Mean               & Median             & Mean              & Median           \\ \midrule
\multirow{2}{1.5cm}{GEOM-QM9}   & DMT-B    & 95.2               & \textbf{100.0}     & 0.090             & \textbf{0.036}   & 93.8               & \textbf{100.0}     & 0.108             & 0.049            \\
    & +MoLlama & \textbf{95.6}      & \textbf{100.0}     & \textbf{0.083}    & \textbf{0.036}   & \textbf{94.2}      & \textbf{100.0}     & \textbf{0.097}    & \textbf{0.044}   \\ \bottomrule
\end{tabular}
\end{table}

\begin{table}[t]
\centering
\small
\caption{Performances of 3D conformer prediction on the GEOM-DRUGS dataset.}\label{tab:pc_sampler}
\setlength{\tabcolsep}{3pt}
\begin{tabular}{lcccccccc}\toprule
          & \multicolumn{2}{c}{COV-R (\%)$\uparrow$} & \multicolumn{2}{c}{AMR-R $\downarrow$} & \multicolumn{2}{c}{COV-P (\%)$\uparrow$} & \multicolumn{2}{c}{AMR-P $\downarrow$} \\ 
          \cmidrule(lr){2-3} \cmidrule(lr){4-5}  \cmidrule(lr){6-7} \cmidrule(lr){8-9}
Model                     & Mean               & Median             & Mean              & Median           & Mean               & Median             & Mean              & Median           \\ \midrule
DMT-B, PC samp., snr=0.2                   & 85.3             & 91.5   & 0.398              & 0.372  & 66.5             & 69.2   & 0.633              & 0.560                \\
DMT-B, PC samp., snr=0.3                   & 85.5             & 91.2   & 0.396              & 0.370  & 67.6             & 71.5   & 0.623              & 0.546                \\
DMT-B, PC samp., snr=0.4                  & 73.8             & 79.9   & 0.535              & 0.501  & 68.0             & 72.1   & 0.621              & 0.548            \\ \bottomrule   
\end{tabular}
\end{table}

\revision{
\begin{table}[t]
\small
\centering
\revision{
\caption{\revision{DMT-B's 3D conformer prediction performances on the GEOM-DRUGS dataset when using different noise schedulers at inference time.}\label{tab:schedule}}
\begin{tabular}{lcccccccc} \toprule
                    & \multicolumn{2}{c}{COV-R (\%) $\uparrow$}   & \multicolumn{2}{c}{AMR-R $\downarrow$}      & \multicolumn{2}{c}{COV-P (\%) $\uparrow$}   & \multicolumn{2}{c}{AMR-P $\downarrow$}      \\\cmidrule(lr){2-3} \cmidrule(lr){4-5}  \cmidrule(lr){6-7} \cmidrule(lr){8-9}
Noise schedule   & Mean                 & Median               & Mean                 & Median               & Mean                 & Median               & Mean                 & Median               \\\midrule
linear           & 62.7 & 62.7 & 0.648 & 0.634 & 60.3 & 60.6 & 0.726 & 0.624 \\
cosine, original & 85.4                 & 92.2                 & 0.401                & 0.375                & 65.2                 & 67.8                 & 0.642                & 0.577                \\
polynomial       & 84.9                 & 91.7                 & 0.454                & 0.421                & 64.5                 & 66.2                 & 0.685                & 0.619 \\ \bottomrule
\end{tabular}}
\end{table}}

\revision{
\begin{table}[t]
\small
\centering
\revision{
\caption{\revision{DMT-B's 3D conformer prediction performances on the GEOM-DRUGS dataset when using different batch sizes.}\label{tab:bs}}
\begin{tabular}{lcccccccc} \toprule
            & \multicolumn{2}{c}{COV-R (\%) $\uparrow$} & \multicolumn{2}{c}{AMR-R $\downarrow$} & \multicolumn{2}{c}{COV-P (\%) $\uparrow$} & \multicolumn{2}{c}{AMR-P $\downarrow$} \\ \cmidrule(lr){2-3} \cmidrule(lr){4-5}  \cmidrule(lr){6-7} \cmidrule(lr){8-9}
Batch size & Mean               & Median               & Mean              & Median             & Mean               & Median               & Mean              & Median             \\\midrule
128        & 85.5               & 92.4                 & 0.395             & 0.366              & 65.1               & 68.0                 & 0.644             & 0.575              \\
256, original        & 85.4               & 92.2                 & 0.401             & 0.375              & 65.2               & 67.8                 & 0.642             & 0.577              \\
512        & 85.1               & 92.0                 & 0.410             & 0.377              & 64.9               & 67.7                 & 0.645             & 0.582              \\\bottomrule
\end{tabular}}
\end{table}}

\subsection{3D Molecular Conformer Prediction}\label{app:conformer}
Table~\ref{tab:app_integrate} presents the results of integrating MoLlama's pretrained 1D representations into DMT-B for 3D conformer prediction, using the same experimental setup as Table~\ref{tab:1d_improve_3d}. The results demonstrate that MoLlama's pretrained representations can enhance DMT-B's performance.


\subsection{Influence of Hyperparameters}
\textbf{More Results on the Predictor-Corrector Sampler.} Table~\ref{tab:conformer} reports DMT-B's performance with the predictor-corrector sampler with the hyperparameter of snr=0.3. To provide a more comprehensive analysis, Table~\ref{tab:pc_sampler} presents results for additional hyperparameter settings.

\textbf{Different Noise Schedules at Inference Time.} We test DMT-B's robustness to different noise schedulers at inference, using two representative options: the linear~\citep{DDPM} and polynomial~\citep{EDM} schedulers. The original noise scheduler, based on the cosine function, follows~\citep{nichol2021improved}. In this study, we use the existing DMT-B checkpoint without retraining the model with these new schedulers, so the results are suboptimal.

\begin{figure}[h!]
    \centering
    \vspace{-2mm}
    \includegraphics[trim={0cm 0.5cm 0cm 0.5cm},clip,width=0.8\linewidth]{figures/amr_vs_step.pdf}
    \vspace{-3mm}
    \caption{Effect of sampling steps on AMR$\downarrow$ for 3D conformer prediction using DMT-B.}
    \label{fig:sampling_steps}
    \vspace{-4mm}
\end{figure}

\textbf{Observation.} As shown in Table~\ref{tab:schedule}, the polynomial scheduler achieves performance close to the cosine scheduler, likely because their curve shapes are similar. However, the linear scheduler results in a significant performance drop, suggesting that retraining DMT-B with the linear scheduler is necessary to achieve better results.

\textbf{Sampling Steps.} We evaluate 3D conformer prediction performance given different sampling steps.


\textbf{Observation.} As shown in Figure~\ref{fig:sampling_steps}, we observe an improving trend in AMR for both recall and precision as the sampling steps increase from 5 to 100. The most significant improvements occur between 5 and 20 steps, with diminishing returns beyond 50 steps. This indicates that our model can half the inference cost by trading off a small amount of performance.
% Beyond 20 steps, the rate of improvement slows down, suggesting that a balance between computational cost and performance can be achieved with an intermediate number of steps.
% Interestingly, the recall AMR consistently remains lower than the precision AMR across all sampling steps. This suggests that our model is generally better at generating structures that cover the ground truth conformations (recall) than at ensuring all generated conformations are close to the ground truth (precision).

\textbf{The Influence of Batch Size to 3D Conformer Prediction.} We evaluate the performance of DMT-B with different batch sizes. The original batch size of 256 was chosen to maximize GPU utilization. To assess the impact of batch size, we tested two variations: (1) reducing the batch size to 128, and (2) increasing it to 512 using gradient accumulation.

\textbf{Observation.} As shown in Table~\ref{tab:bs}, the performance with a 512 batch size is slightly worse than the original model. This is likely due to underfitting caused by fewer training steps. We keep the number of training epochs the same as the original experiment (256 batch size), therefore the larger batch size results in fewer gradient updates, leading to reduced model performance. Other than this observation, using the 128 batch size does not lead to significant difference than the original model.


\begin{figure}[t!]
    \centering
    \includegraphics[width=.6\textwidth]{figures/time_comparison.pdf}
    \caption{\revision{Comparison of conformer generation time on the test set of the GEOM-Drugs dataset using various methods.}}
    \label{fig:time_comparison}
\end{figure}
    
\subsection{Computational Time Comparison}
We conducted a time comparison between our model and representative baselines for conformer generation on the test set of the GEOM-Drugs dataset, which includes 1000 molecules. The baselines include the OpenEye Omega~\citep{OmegaSoftware}, TD w/ PG~\citep{ParticleGuidance}, and xTB\footnote{https://xtb-docs.readthedocs.io/en/latest/}. The results are shown in Figure~\ref{fig:time_comparison}. 

These experiments were performed on a platform with an 8-core Intel Xeon Processor@2.90GHz CPU and an NVIDIA A100 GPU and the time is measured in minutes and seconds. Please note that the Omega and xTB are run on the CPU only, while DMT and Mollama are run on the GPU. So the results may vary depending on the hardware. 

% To thoroughly evaluate the computational performance of NExT-Mol in the context of 3D conformer generation, we conducted a series of experiments comparing it with several baseline models, including OpenEye Omega~\citep{Omega} and Particle Guidance~\citep{ParticleGuidance}.These experiments were performed on a platform with an 8-core CPU and an NVIDIA A100 GPU. The key findings are summarized below:

% \textbf{OpenEye Omega is the fastest}: OpenEye Omega stands out as the fastest method for generating conformers, with an average generation time of approximately 0.1 seconds per molecule. However, the conformers generated by Omega often require further geometric optimization using xTB to achieve stable structures. When including the time required for geometric optimization with xTB, the total time for generating and optimizing conformers using Omega significantly increases. This highlights that geometric optimization, even with fast semi-empirical methods, can become a bottleneck in the conformer generation process.

% \textbf{DMT-B is faster than DMT-L}: As expected, the smaller model size of DMT-B results in faster generation times compared to DMT-L. DMT-B generates conformers in approximately pne third of the time required by DMT-L. This demonstrates the trade-off between model size and computational efficiency in the context of 3D conformer generation.

% \textbf{Integrating MoLlama increases computational time}: The inclusion of MoLlama, a large language model with 960 million parameters, into DMT increases the computational costs. Specifically, NExT-Mol, which integrates MoLlama, takes around 0.4 seconds per molecule, reflecting the additional overhead of the large language model.

% \paragraph{Conclusion:}
% NExT-Mol achieves competitive performance in terms of computational efficiency. Although it is not yet as fast as OpenEye Omega, NExT-Mol can generate high-quality 3D conformers with the potential for further optimization to enhance speed.

\revision{
\subsection{3D Molecular Stability Performance}
We do not report the 3D molecule stability metric~\citep{EDM} in the main part of this work, because this metric presents a significant limitation on the GEOM-DRUGS dataset, showing only 2.8\% for the ground truth training set. We present the results here for backup purposes.

\begin{table}[t]
\centering
\small
\caption{\revision{3D Molecule stability performances. * denotes our reproduced results.}}
\begin{subtable}[t]{0.45\linewidth}
\centering
\caption{\revision{GEOM-DRGUS dataset.}}
\revision{
\begin{tabular}{lc}\toprule
3D-Metric                    & MolStable                    \\ \midrule
{\color[HTML]{808080} Train} & {\color[HTML]{808080} 0.028} \\
EDM                          & 0.002                        \\
JODO                         & 0.010                        \\
MiDi*                        & 0.003                        \\
EQGAT                        & 0.025                        \\
NExT-Mol, ours               & \textbf{0.027}              \\ \bottomrule
\end{tabular}}
\end{subtable}
\begin{subtable}[t]{0.45\linewidth}
\centering
\caption{\revision{QM9-2014 dataset.}}
\revision{
\begin{tabular}{lc}\toprule
3D-Metric                    & MolStable                    \\ \midrule
{\color[HTML]{808080} Train} & {\color[HTML]{808080} 0.953} \\
% E-NF                         & 0.045                        \\
G-SchNet                     & 0.681                        \\
G-SphereNet                  & 0.134                        \\
EDM                          & 0.817                        \\
MDM                          & 0.896                        \\
JODO                         & 0.934                        \\
MiDi*                        & 0.842                        \\
EQGAT                        & 0.889                        \\
NExT-Mol, ours               & \textbf{0.946}              \\ \bottomrule
\end{tabular}}
\end{subtable}
\end{table}
}

\revision{\subsection{More Visualizations}}
\textbf{Visualization of Random Samples.}
Visualizations of complete molecules sampled from NExT-Mol on GEOM-Drugs and QM9 are shown in Figure \ref{fig:examples-drugs} and Figure \ref{fig:examples-qm9}, respectively.
These samples are randomly selected to illustrate the diversity and effectiveness of our model.
The visualization includes 1D SELFIES sequences, 2D molecular graphs, and 3D conformers highlighting the spatial arrangement of atoms within the molecules.
Notably, in the complex GEOM-Drugs dataset, NExT-Mol demonstrates its robustness by consistently generating molecules without disconnected components and effectively preserving the stable geometric planes of aromatic ring structures.
These visualizations not only demonstrate the fidelity of the molecules generated by NExT-Mol with 1D SELFIES sequences along with 3D spatial coordinates, but also emphasize the ability of our model to produce stable and chemically valid conformers accommodating a wide range of molecular weights.
% This capability underscores the model's potential for producing chemically valid and structurally sound conformers, making it a valuable tool for applications in drug discovery and molecular design.




\begin{figure}[t]
\centering
\small
\setlength{\tabcolsep}{1pt}
\renewcommand{\arraystretch}{-10} % General space between rows (1 standard)
\setlength\extrarowheight{-10pt}
\begin{tabular}[t]{ccc}
\begin{subfigure}[t]{.3\linewidth}
\centering
\includegraphics[width=1\linewidth]{figures/appendix_vis/499_gt_3.png}
\vspace{-10mm}
\caption{Ground truth.}
\vspace{-10mm}
\end{subfigure}
& \begin{subfigure}[t]{.3\linewidth}
\centering
\includegraphics[width=1\linewidth]{figures/appendix_vis/499_dmt.png}
\vspace{-10mm}
\caption{DMT-B's prediction (RMSD = 0.90).}
\vspace{-10mm}
\end{subfigure}
& \begin{subfigure}[t]{.3\linewidth}
\centering
\includegraphics[width=1\linewidth]{figures/appendix_vis/499_dmt_llama.png}
\vspace{-10mm}
\caption{DMT-B + MoLlama's prediction (RMSD = 0.05).}
\vspace{-10mm}
\end{subfigure}\\
\begin{subfigure}[t]{.3\linewidth}
\centering
\includegraphics[width=1\linewidth]{figures/appendix_vis/688_gt_4.png}
\vspace{-15mm}
\caption{Ground truth.}
\vspace{-10mm}
\end{subfigure}\hfill
& \begin{subfigure}[t]{.3\linewidth}
\centering
\includegraphics[width=1\linewidth]{figures/appendix_vis/688_dmt.png}
\vspace{-15mm}
\caption{DMT-B's prediction (RMSD = 0.87).}
\vspace{-10mm}
\end{subfigure}\hfill
& \begin{subfigure}[t]{.3\linewidth}
\centering
\includegraphics[width=1\linewidth]{figures/appendix_vis/688_dmt_llama.png}
\vspace{-15mm}
\caption{DMT-B + MoLlama's prediction (RMSD = 0.06).}
\vspace{-10mm}
\end{subfigure} \\
\begin{subfigure}[t]{.3\linewidth}
\centering
\includegraphics[width=1\linewidth]{figures/appendix_vis/864_gt_5.png}
\vspace{-15mm}
\caption{Ground truth.}
\vspace{-10mm}
\end{subfigure}\hfill
& \begin{subfigure}[t]{.3\linewidth}
\centering
\includegraphics[width=1\linewidth]{figures/appendix_vis/864_dmt.png}
\vspace{-15mm}
\caption{DMT-B's prediction (RMSD = 0.84).}
\vspace{-10mm}
\end{subfigure}\hfill
& \begin{subfigure}[t]{.3\linewidth}
\centering
\includegraphics[width=1\linewidth]{figures/appendix_vis/864_dmt_llama.png}
\vspace{-15mm}
\caption{DMT-B + MoLlama's prediction (RMSD = 0.07).}
\vspace{-30mm}
\end{subfigure} \\
\begin{subfigure}[t]{.3\linewidth}
\centering
\includegraphics[width=1\linewidth]{figures/appendix_vis/892_gt.png}
\vspace{-15mm}
\caption{Ground truth.}
\vspace{-30mm}
\end{subfigure}\hfill
& \begin{subfigure}[t]{.3\linewidth}
\centering
\includegraphics[width=1\linewidth]{figures/appendix_vis/892_dmt.png}
\vspace{-15mm}
\caption{DMT-B's prediction (RMSD = 0.86).}
\vspace{-30mm}
\end{subfigure}\hfill
& \begin{subfigure}[t]{.3\linewidth}
\centering
\includegraphics[width=1\linewidth]{figures/appendix_vis/892_dmt_llama.png}
\vspace{-15mm}
\caption{DMT-B + MoLlama's prediction (RMSD = 0.07).}
\vspace{-10mm}
\end{subfigure} \\
\end{tabular}
\caption{\revision{Visualization of 3D conformers. From left-to-right, we have the ground truth conformer, the conformer predicted by DMT-B, and the conformer predicted by DMT-B+MoLlama. For each model, we select the predicted conformer with the least RMSD to the ground truth.}\label{fig:vis_conformer_more}}
\end{figure}
    
\revision{\textbf{Visualization of 3D Conformer Prediction.} To gain more insights on how transfer learning using MoLlama's 1D representations can improve 3D conformer prediction, we present more visualizations of 3D conformer prediction in Figure~\ref{fig:vis_conformer_more}. The samples are selected from the test set of GEOM-DRUGS with unseen scaffolds in the training set. 
}



\begin{figure}[t]
    \centering
    \small
    \includegraphics[width=\linewidth]{figures/random_samples_DRUGS.png}
    \caption{Visualization of random samples generated by NExT-Mol trained on GEOM-DRUGS.}
    \label{fig:examples-drugs}
\end{figure}
\begin{figure}[t]
    \centering
    \small
    \includegraphics[width=\linewidth]{figures/random_samples_QM9.png}
    \caption{Visualization of random samples generated by NExT-Mol trained on QM9-2014.}
    \label{fig:examples-qm9}
\end{figure}

\section{Further Details on Methodology}\label{app:method_detail}
\subsection{1D Molecule Generation with Molecular Llama LM}\label{app:mollama}
\textbf{Data Preparation.} Following~\citep{Chemformer}, we collect 1.8 billion molecules from the ZINC-15 database~\citep{ZINC15}, significantly more than the 100 million molecules used in previous studies~\citep{Chemformer,MolGen}. We keep only molecules with molecular weight$\leq$500 Daltons and LogP$\leq$5~\citep{flynn1980substituent}, and transform them into SELFIES~\citep{SELFIES} sequences. After canonicalizing the SELFIES and removing hydrogen atoms, the dataset contains 90 billion tokens. We further filter the molecules in the valid and test sets of the GEOM-QM9 and GEOM-DRUGS datasets~\citep{GEOM} and randomly sampled 1\% of the remaining data as the validation set.

\begin{table}[t]
\small\centering
\caption{Hyperparameter for pretraining MoLlama.}\label{tab:mollma_hyp}
\begin{tabular}{lc|lc} \toprule
hidden   size           & 2048 & hidden act   & silu     \\
intermediate size       & 5632 & batch size   & 512      \\
max position embeddings & 512  & warmup steps & 2000     \\
num attention heads     & 32   & min lr       & 4.00E-05 \\
num hidden layers       & 22   & init lr           & 4.00E-04 \\
num key value heads     & 4    & weight decay & 1.00E-01 \\
n query groups          & 4    & grad clip    & 1.0        \\ \bottomrule
\end{tabular}
\end{table}



\textbf{Randomized SELFIES Augmentation Details.} In order to generate randomized SELFIES, we first generate the randomized SMILES~\citep{SMILES}, and transform the SMILES into SELFIES.
We follow~\citep{RandomSmiles} for the implementation details of random SMILES, and use a restricted random sampling of SMILES.
Similarly, we also generate canonical SELFIES by transforming canonical SMILES.

\textbf{Pretraining Details.} We train MoLlama from scratch for 1D molecule generation using a next-token prediction objective. The code and hyperparameters are based on~\citep{TinyLlama}, utilizing Flash-Attention~\citep{flashattention2} and FSDP~\citep{FSDP} for faster training. We use a max context length of 512, concatenating multiple SELFIES sequences into the same context, with any overflow trimmed and used in the next context. We use the AdamW optimizer and a scheduler with linear warmup and cosine decay. The key parameters are included in Table~\ref{tab:mollma_hyp}. We train the model for 555k global steps. The training was done on 4 NVIDIA A100-40G GPUs and took approximately two weeks. The training log is shown in Figure~\ref{fig:mollama_log}.

\revision{\textbf{On the Advantages of Acheving 100\% Validity beyond Validity Itself.} We employ the 1D SELFIES representation for LM training. Here we elaborate on the other advantages beyond 100\% validity, which are also crucial for real-world applications:
\begin{itemize}[leftmargin=*]
\item \textbf{Improving validity could improve other 2D metrics, like SNN, Frag, and Scaf.} These metrics measure the distributional similarity of 2D molecular structures of valid molecules. If a model still generate invalid molecules, it is likely the model does not capture the true target distribution, which contain only valid molecules. 100\% validity helps the model learn from and sample from the valid molecular structures, which is essential for molecule generation tasks. This is demonstrated by our improved FCD, SNN, Frag, and Scaf metrics in Table~\ref{tab:denovo}.
\item \textbf{Improving validity could improve 3D geometry learning.} The improved validity also leads to better learning of 3D molecular geometry, because it grounds 3D structure prediction on valid 2D structures. Other joint 2D and 3D prediction methods~\citep{JODO, MiDi} can easily encounter invalid 2D structures when sampling 3D structures, therefore leads to worse 3D structure prediction. This is demonstrated by NExT-Mol's significant improvements in geometry similarity metrics (\eg bond angle and bond length) in Table~\ref{tab:denovo}. 
\end{itemize}
}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/mollama_log/train_val_perplexity.pdf}
    \caption{Visualization of MoLlama's training and validation PPL log during pretraining.}
    \label{fig:mollama_log}
\end{figure}

\subsection{3D Conformer Prediction with Diffusion Molecular Transformer}
\label{app:dmt}


\textbf{Diffusion Process.} Here we elaborate on the details of our diffusion process. Following~\citep{nichol2021improved, JODO}, we use the cosine scheduler controlling the noise scale for the diffusion process:
% the hyperparameters controling the noise scale of diffusion are defined below:
\begin{align}
\bar{\alpha}_t = \frac{f(t)}{f(0)}, \quad f(t) = \cos\left( \frac{t+s}{1+s}\cdot \frac{\pi}{2}\right),
\end{align}
where $t\in (0,1]$ is the time step, and $s$ is a hyperparameter empirically set to $0.008$, following~\citep{nichol2021improved}.




Our pseudo codes for training and sampling are shown in Algorithm~\ref{algo:1} and Algorithm~\ref{algo:2} below. Following~\citep{DDPM}, we have the following hyperparameters used in the pseudo-codes for training and sampling:
\begin{align}
\alpha^{(t)} = \bar{\alpha}^{(t)} / \bar{\alpha}^{(t-1)}, \quad \sigma^{(t)} = \sqrt{1-\alpha^{(t)}}.
\end{align}

\begin{algorithm}
\caption{Training}\label{algo:1}
\begin{algorithmic}[1]
\STATE $t \sim \mathcal{U}(0, 1]$ \hfill \COMMENT {\underline{Sample a time step}}
\STATE $G^{(0)} = (\mathbf{x}^{(0)}, \mathbf{h}, \mathbf{e}) \sim \text{Training Set}$ \hfill \COMMENT {\underline{Sample a 3D molecule}}
\STATE $\mathbf{x}^{(0)} \leftarrow \mathbf{x}^{(0)} - \bar{\mathbf{x}}^{(0)}$ \hfill \COMMENT {\underline{Centering molecule coordinates}}
\STATE $\mathbf{x}^{(0)} \leftarrow \mathbf{x}^{(0)}R$, where $R\in SO(3)$ is randomly sampled \hfill \COMMENT {\underline{Random rotation augmentation}}
\STATE $\boldsymbol{{\epsilon}}^{(t)} \sim \mathcal{N} (\mathbf{0}|\mathbf{I})$
\STATE $\mathbf{x}^{(t)} = \sqrt{\bar{\alpha}^{(t)}}\mathbf{x}^{(0)} + \sqrt{1-\bar{\alpha}^{(t)}}\boldsymbol{{\epsilon}}^{(t)}$ \hfill \COMMENT {\underline{Forward diffusion}}
\STATE $G^{(t)} \leftarrow (\mathbf{x}^{(t)}, \mathbf{h}, \mathbf{e})$ % \hfill \COMMENT {\underline{Obtain the molecule with perturbed coordinates}}
% \STATE $\hat{x}_0 \leftarrow \text{KabschAlign}(x_0, x_t)$ \hfill \COMMENT {\underline{Align the GT coordinates to the generated coordinates}}
\STATE Minimize loss $\mathcal{L}=||\boldsymbol{{\epsilon}}^{(t)} - \text{DMT}(G^{(t)}, t)||_2^2$
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{Sampling 3D Conformers}\label{algo:2}
\begin{algorithmic}[1]
\REQUIRE time steps $\{t_i\}^M_{i=1}$, a 2D molecular graph $G_{\text{2D}} \leftarrow (\mathbf{h}, \mathbf{e})$
\STATE $\mathbf{x}^{(t_1)} \sim \mathcal{N}(\mathbf{0}, \mathbf{I})$ \hfill \COMMENT {\underline{Set the initial noise conformer}}
\FOR {$i \leftarrow 1$ to $M$}
    \STATE $t \leftarrow t_{i-1}$, $s \leftarrow t_i$ \hfill \COMMENT {\underline{Set time step}}
    \STATE $G^{(t)} \leftarrow (\mathbf{x}^{(t)}, \mathbf{h}, \mathbf{e})$
    \STATE $\mathbf{z} \sim \mathcal{N}(\mathbf{0}, \mathbf{I})$ if $i<M$ else $\mathbf{z}=\mathbf{0}$
    \STATE $\mathbf{x}^{(s)} = \frac{1}{\sqrt{\alpha_t}}\left(\mathbf{x}^{(t)}-\frac{1-\alpha^{(t)}}{\sqrt{1-\bar{\alpha}^{(t)}}}\text{DMT}(G^{(t)}, t)\right) + \sigma^{(t)}\mathbf{z}$ \hfill \COMMENT {\underline{Update conformer}}
\ENDFOR
\RETURN $\mathbf{x}^{(M)}$
\end{algorithmic}
\end{algorithm}

% \begin{algorithm}
% \caption{Sampling RNA 3D Conformers}
% \begin{algorithmic}[1]
% \REQUIRE time steps $\{t_i\}^M_{i=1}$, RNA's primary structure $\mathbf{h}$, and secondary structure $\mathbf{s}$
% \STATE $\mathbf{x}^{(t_1)} \sim \mathcal{N}(\mathbf{0}, \mathbf{I})$ \hfill \COMMENT {\underline{Set the initial noise conformer}}
% \FOR {$i \leftarrow 1$ to $M$}
%     \STATE $t \leftarrow t_{i-1}$, $s \leftarrow t_i$ \hfill \COMMENT {\underline{Set time step}}
%     \STATE $\mathbf{z} \sim \mathcal{N}(\mathbf{0}, \mathbf{I})$ if $i<M$ else $\mathbf{z}=\mathbf{0}$ 
%     % \STATE $\epsilon^{(t)} = (1+w)\text{R-Trans}(\mathbf{x}^{(t)}, t, \mathbf{h}, \mathbf{s}) - w \text{R-Trans}(\mathbf{x}^{(t)}, t, \mathbf{h})$
%     \STATE $\epsilon^{(t)} = \text{R-Trans}(\mathbf{x}^{(t)}, t, \mathbf{h}, \mathbf{s})$
%     \STATE $\mathbf{x}^{(s)} = \frac{1}{\sqrt{\alpha_t}}\left(\mathbf{x}^{(t)}-\frac{1-\alpha^{(t)}}{\sqrt{1-\bar{\alpha}^{(t)}}} \epsilon^{(t)}\right) + \sigma^{(t)}\mathbf{z}$ \hfill \COMMENT {\underline{Update conformer}}
% \ENDFOR
% \RETURN $\mathbf{x}^{(M)}$
% \end{algorithmic}
% \end{algorithm}

% \subsubsection{The Neural Architecture of Diffusion Molecule Transformer}
\textbf{RMHA.} Here we define the multi-head version of RMHA. Similar to the single-head version, we first generate the queries, keys, and values for atom representation $\mathbf{H}$, and generate the queries and values for pair representation $\mathbf{E}$:

\begin{tabular}{p{6.5cm}p{6.5cm}}
\begin{equation}
[\mathbf{Q}; \mathbf{K}; \mathbf{V}] = [\mathbf{W}_{q}; \mathbf{W}_{k}; \mathbf{W}_{v}] \Trans{\mathbf{H}},
\end{equation}
&
\vspace{-5mm}
\begin{equation}
[\mathbf{Q}^E;\mathbf{V}^E] = \tanh([\mathbf{W}_{eq};\mathbf{W}_{ev}] \Trans{\mathbf{E}}),
\end{equation}
\end{tabular}

Subsequently, we define the Relational-Attention (R-Attention) module, which is the combination of \Eqref{eq:c} and \Eqref{eq:d}:
\begin{align}
 \mathbf{O} & = \text{R-Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}, \mathbf{Q}^{E}, \mathbf{V}^E), \\
 \text{where } \mathbf{O}_i &= \sum_{j=1}^{N} a_{i,j}(\mathbf{V}^E_{i,j}\odot \mathbf{V}_j), \\
 a_{i,j}& =\softmax_{j}(\frac{(\mathbf{Q}^E_{i,j}\odot \mathbf{Q}_i)\Trans{\mathbf{K}}_j}{\sqrt{d}}).
\end{align}

After this, the muli-head version of RMHA can be written as:
\begin{align}
 \text{RMHA}(\mathbf{Q}, \mathbf{K}, \mathbf{V}, \mathbf{Q}^{E}, \mathbf{V}^E) & = \text{Concat}(\mathbf{O}^{1}, ..., \mathbf{O}^{h}) \mathbf{W}_o \\
\text{where } \mathbf{O}^{f} & = \text{R-Attention}(\mathbf{W}_{qf}\mathbf{Q}, \mathbf{W}_{kf}\mathbf{K}, \mathbf{W}_{vf}\mathbf{V}, \mathbf{W}_{eqf}\mathbf{Q}^{E}, \mathbf{W}_{evf}\mathbf{V}^E),
\end{align}
where $h$ is the number of head; $f\in[1,h]$; $\mathbf{W}_o$ is the linear projector combining outputs of different heads; and $\mathbf{W}_{qf}$, $\mathbf{W}_{kf}$, and $\mathbf{W}_{vf}$ are linear projectors for the $f$-th head of atom representations; and $\mathbf{W}_{eqf}$ and $\mathbf{W}_{eqf}$ are linear projectors for the $f$-th head of the pair representation.

\subsection{MoLlama Representations Improve DMT's 3D Conformer Prediction}
\textbf{Details of SELFIES-to-Atom Mapping.} The mapping process is not straightforward with existing software, so we have to manually code a significant portion. For details on the full implementation, please refer to our code. In brief, the SELFIES software provides a mapping between SELFIES and SMILES tokens, and RDKit gives the atom order when generating SMILES. We manually convert this atom order into a mapping between SMILES and atom indices, then combine the SELFIES-to-SMILES and SMILES-to-atom mappings into the SELFIES-to-atom mapping. Additionally, we handle missing hydrogen atoms in both SMILES and SELFIES during the mapping process.

\revision{
\textbf{Rationale behind Transfer Learning between 1D Molecule Sequences and 3D Conformers.} The final goal of this transfer learning is to leverage the billion-scale 1D/2D molecule dataset to improve the 3D conformer prediction performance, which is constrained by limited 3D data. For clarity, we decompose the rationale into the following chain of arguments: 
\begin{itemize}[leftmargin=*]
\item \textbf{3D conformers are theoretically governed by 2D molecular graphs under quantum mechanics (QM).} 3D molecular properties and structures are fundamentally rooted in QM. Using (approximated) QM-based methods, like DFT, we can accurately predict 3D conformers from 2D molecular graphs, though at high computational cost. This establishes the critical role of 2D representations in determining 3D structures.
\item \textbf{3D conformer prediction  relies on high quality 2D molecule representations.} Deep learning models predict 3D conformers from 2D graphs, and their performance is heavily influenced by the quality of 2D molecular representations. Transfer learning can enhance 2D molecular representations, as demonstrated by prior works~\citep{pretrain_gnn,GraphMVP,GraphMAE}. 
\item \textbf{1D molecular representations can be converted to 2D molecular representations, and contribute to 3D prediction.} 1D molecule sequences encode the same information as 2D molecular graphs, and the 1D to 2D transformation can be achieved by deterministic toolkit, like RDkit. Leveraging RDkit and our proposed cross-modal projector (\cf Section~\ref{sec:integrate}), we can transform 1D molecular representations to 2D molecular representations, and therefore contribute to the 3D prediction. We have demonstrated this improvement in Table~\ref{tab:1d_improve_3d}, where using the pretrained 1D representations improve 3D conformer prediction.
\item \textbf{1D pretraining scales more effectively than 2D.} Given the billion-scale 1D/2D molecule dataset, we mostly prioritize the scalability when selecting the pretraining method. After literature review, we find that 1D LM-based pretraining methods, like Llama~\citep{Llama-2} and BERT~\citep{BERT}, are extensively demonstrated for scalability and effectiveness. Therefore, we opt to 1D pretraining instead of 2D pretraining.
\end{itemize}
}

% \subsubsection{Inference-Time Dropout}
% \textbf{Inference-Time Dropout Improves Conformer Diversity.} An effective conformer predictor should capture a wide range of 3D conformers for a single molecule by ensuring high sample diversity. While diffusion models start from random noise and generate different outputs each time, they are still insufficiently diverse for 3D conformer prediction~\citep{ParticleGuidance}. For improvement, we apply dropout~\citep{dropout} during both training and inference to introduce more randomness and improve diversity. Specifically, we add dropout to the attention scores in RMHA and the MLPs in our DMT, as shown in Figure~\ref{fig:dmt}. The intuition is that inference-time dropout generates different views (augmentations) for the same input molecule~\citep{SimCSE}, thus increasing sampling diversity. The dropout ratios can be tuned during inference to tradeoff between precision and recall.

% % Compared to previous works~\citep{torsion,ParticleGuidance} that leverages external tools or extra gradient guidance to improve conformer diversity of small molecules (see Appendix~\ref{app:dmt}), our method is simple and effective and does not incur any additional computational cost.

% \textbf{Comparison to Prior Methods.} To ensure sample diversity, torsional diffusion~\citep{torsion} uses RDKit to sample diverse draft conformers to start with. Take a step further, Particle Guidance~\citep{ParticleGuidance} applies gradient guidance to maximize the distance among a group of conformers during sampling. In contrast to these methods, which rely on external tools or additional gradient guidance, our approach—using inference-time dropout—is simpler and incurs no additional computational cost. Similarly, AlphaFold3~\citep{AF3} improves sample diversity through MSA resampling to create different augmentations of the same protein, but this method is not applicable to small molecules.



\section{Experimental Details}
\label{app:expdetail}

\revision{
\subsection{Baselines}
Here we present a brief introduction for the baselines used in our experiments. We categorize baselines by their benchmarks.

\textbf{\textit{De Novo} and Conditional 3D Molecule Generation.} 
\begin{itemize}[leftmargin=*]
\item G-SchNet~\citep{GSchNet}: G-SchNet autoregressively generates 3D molecules by considering molecular symmetries through the SchNet~\citep{schutt2018schnet}
\item G-SphereNet~\citep{GSphereNet}: G-SphereNet autoregressively generates 3D molecules, in which each step determines the atom type, bond length, angle, and torsion angles.
% \item E-NF~\citep{ENF}: E-NF enhances the normalizing flow with 3D equivariance by incorporating EGNN~\citep{EGNN} for 3D molecule generation.
\item EDM~\citep{EDM}: EDM pioneers the diffusion methods for 3D molecue generation. It constructs a diffusion model with the EGNN~\citep{EGNN} architecture and the VDM diffusion process~\citep{VDM}.
\item MDM~\citep{MDM}: MDM is a diffusion model for 3D molecule generation. Through a specialized edge construction module, it leverages both global interatomic interactions and local interatomic interactions for 3D modeling.
\item CDGS~\citep{CDGS} and JODO~\citep{JODO}: CDGS is a diffusion model for 2D molecular graph generation. It models discrete vairables (\eg atom types and bond types) using one-hot encoding and applies a continous diffusion for generative modeling. JODO extends CDGS by studying joint 2D and 3D molecule generation. It features a RMHA module for enhanced relational molecular graph modeling.
\item MiDi~\citep{MiDi}: MiDi is a joint 2D and 3D diffusion model for 3D molecule generation. It leverages two diffusion processes of discrete diffusion~\citep{DiGress} and continous diffusion~\citep{EDM} for the corresponding data types in a molecule.
\item EQGAT-diff~\citep{EQGATDiff}: EQGAT-diff modified the EQGAT~\citep{EQGAT} architecture for joint 2D and 3D molecular generation. EQGAT is based on the Tensor Field Networks~\citep{TFN} to achieve 3D rotational and translational equivariance.
\item GeoLDM~\citep{GeoLDM}: GeoLDM explores the idea of latent diffusion model~\citep{LDM} for 3D molecule generation. 
\item EEGSDE~\citep{EEGSDE}: EEGSDE explores conditional 3D molecule generation with diffusion guidance by an energy function.
\item MolGPT~\citep{MolGPT}: MolGPT is a decoder-only molecule LM pretrained on 1D SMILES sequences.
\item MolGen~\citep{MolGen}: MolGen is an encoder-decoder molecular LM pretrained on 1D SELFIES sequences. Following~\citep{T5}, it is pretrained and evaluated using a span-corruption objective.
\end{itemize}

\textbf{3D Conformer Prediction.}
\begin{itemize}[leftmargin=*]
\item OMEGA~\citep{OMEGA}: OpenEye OMEGA is a commercial software that employs a combination of fragment-based methods and torsional sampling, guided by empirical force fields or customized energy functions, to predict 3D conformers.
\item GeoMol~\citep{GeoMol}: GeoMol is an SE(3)-invariant model for 3D conformer prediction. In the first step, it predicts the bond angles and bond lengths for all the neighbors of each non-terminal atom. Next, it assembles the local structures together by predicting their torsion angles.
\item GeoDiff~\citep{GeoDiff}: GeoDiff is a diffusion model that leverages a roto-translational equivariant GNN for 3D conformer prediction.
\item Torsional Diffusion~\citep{torsion}: Torsional diffusion is a diffusion model defined on the dihedral angles of 3D molecules. It samples seed conformers using RDkit, and applies diffusion only on the dihedral angles of molecular bonds, while fixing the bond lengths and bond angles.
\item Particle Guidance~\citep{ParticleGuidance}: Particle guidance is a diffusion guidance method designed to improve the sampling diversity compared to the vanilla i.i.d. sampling. It modifies torsional diffusion's sampling process for 3D conformer prediction, without changing its training process.
\item MCF~\citep{MCF}: MCF explores the power of scaling law for 3D conformer prediction. Instead of following prior works and leveraging a neural architecture with built-in 3D equivariance, it scales up a general-purpose transformer, and demonstrates strong performances.
\end{itemize}
}

\subsection{DMT Configurations}
\textbf{Hyperparameter.} Table~\ref{tab:dmt_hyp} shows the key hyperparameters used for training the DMT-B and DMT-L models. Other hyperparameters, like batch size and training epochs, are separately listed for each task in the following sections.

\textbf{Features.} We use the same atom features and pair features as~\citep{torsion}. For the GEOM-DRUGS dataset, the atom feature has 74 dimensions; for the QM9-2014 and GEOM-QM9 datasets, the atom feature has 44 dimensions. The bond feature has 4 dimensions.

\begin{table}[t]
\centering
\small
\caption{Hyperparameters of the DMT-B and DMT-L models.}\label{tab:dmt_hyp}
\begin{tabular}{lcc} \toprule
                        & \multicolumn{1}{l}{DMT-B} & \multicolumn{1}{l}{DMT-L} \\ \midrule
n layers               & 10                        & 12                        \\
atom hidden size       & 512                       & 768                       \\
atom intermediate size & 2048                      & 3072                      \\
pair hidden size       & 128                       & 192                       \\
pair intermediate size & 512                       & 768                       \\
n heads                & 8                         & 8                         \\
total params           & 55M                       & 150M                     \\
optimizer              & \multicolumn{2}{c}{AdamW}                             \\
init lr                & \multicolumn{2}{c}{1.00E-04}                          \\
min lr                 & \multicolumn{2}{c}{1.00E-05}                          \\
warmup lr              & \multicolumn{2}{c}{1.00E-06}                          \\
warmup steps           & \multicolumn{2}{c}{1000}                              \\
weight decay           & \multicolumn{2}{c}{0.05}                              \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Task: \textit{De Novo} Molecule Generation}
\label{app:3d_generation}
For \textit{De Novo} molecule generation, we separately train NExT-Mol for the GEOM-DRUGS and the QM9-2014 datasets. This process involve training both the MoLlama and DMT of NExT-Mol.

% The training was conducted using 4 NVIDIA A100-80GB GPUs, with configurations adjusted based on the dataset.

\textbf{MoLlama Settings.} For QM9-2014, we use a batch size of 512 and train for 100 epochs, while for GEOM-DRUGS, we use a batch size of 256 and train for 20 epochs. For sampling, we employ a sampling temperature of 1.0 and, beam size of 1, and we sample 10,000 molecules for evaluation. We use the AdamW optimizer and a learning rate scheduler with linear warmup and cosine decay. The optimizer hyperparameters are as follows: init\_lr=1e-4, min\_lr=1e-5, warmup\_lr=1e-6, warmup\_steps=1000, and weight\_decay=0.05.
% We utilize a restricted random sampling for SELFIES augmentations to enhance the model's generalization ability, following~\cite{RandomSmiles}. 

\textbf{DMT Settings.} We use a dropout rate of 0.1 for QM9-2014 and 0.05 for GEOM-DRUGS. Following~\citep{JODO}, we select only the conformer with the lowest energy for training on the GEOM-DRUGS dataset. For both datasets, we train DMT-B for 1000 epochs. The batch size for QM9-2014 is  2048 and the batch size for GEOM-DRUGS is 256.

\revision{
\textbf{Details on the Evaluation Metrics.} We use the MMD distance when computing the distributional similarity of bond lengths, bond angles, and dihedral angles. Note that, we do not perform Kekulization and Sanitization when computing molecule and atom stability for 2D and 3D molecules. We use canonicalized SMILES for both the generated molecules and the training dataset when computing novelty and uniqueness of molecules. All the baselines are consistently evaluated under the same setting above.
}
% To optimize the learning process, we train our models with the AdamW optimizer and implemented a dynamic learning rate adjustment strategy. Specifically, we use a linear warmup followed by a cosine decay schedule.

\subsection{Task: Conditional Molecule Generation}
\label{app:condition}

\textbf{Details for Adapting NExT-Mol for Conditional Generation.}
For conditional molecule generation on the QM9-2014 dataset, we modify the NExT-Mol architecture to incorporate property-specific information into both the MoLlama language model and the DMT conformer prediction model.
This approach allows us to generate molecules with desired properties in both 1D sequence and 3D structure spaces.
\begin{itemize}[leftmargin=*]
    \item \textbf{Condioning MoLlama.} We implement a condition MLP to encode property information into a soft prompt.
 This MLP consists of two linear layers with a GELU activation function in between.
 It transforms a single property value into a 4-token sequence embedding, each token having the same dimensionality as the model's hidden size.
 The resulting soft prompt is prepended to the input sequence embeddings of SELFIES before being fed into the language model.
 We adjust the attention mask accordingly to ensure the model attends to these conditional tokens.
    % We integrate the conditional information into the time embedding of the diffusion process.
    \item \textbf{Condioning DMT.} We use an MLP to process the property value, followed by a linear projection to match the time embedding dimension.
 This processed condition is then added to the time embedding, allowing the diffusion process to be guided by the desired property throughout the denoising steps.
    % This approach allows for flexible conditioning on single or multiple properties by adjusting the output dimension of the final linear layer.
\end{itemize}


\textbf{MoLlama Setting.}
For conditional molecule generation, we train MoLlama with a batch size of 256 for 100 epochs on the QM9-2014 dataset.
We use a sampling temperature of 1.0, beam size of 5, and we sample 10,000 molecules for evaluation of each desired property.

\textbf{DMT Setting.}
For the DMT-B model, we train with a batch size of 512 for 1000 epochs on the QM9-2014 dataset.
We employ a dropout rate of 0 with 100 sampling steps for evaluation.

The optimizer and learning rate schedule are consistent with the \textit{de novo} generation task, using AdamW with a linear warmup followed by cosine decay.
We train the conditional generation model for six different quantum properties using the same optimization strategy as in the \textit{de novo} generation task.
Each model is trained on 4 NVIDIA A100-80GB GPUs.

\subsection{Task: 3D Conformer Prediction}
\label{app:conformer}

\textbf{Training Details.} We elaborate the training details for each of the three training stages in Section~\ref{sec:integrate}.
\begin{itemize}[leftmargin=*]
\item \textbf{Stage 1: DMT Training.} For GEOM-QM9, we train the DMT-B model for \revision{2000 epochs} with a batch size of 2048. For GEOM-DRUGS, we train both the DMT-B and DMT-L models for \revision{3000 epochs} with batch size 256. Note that, for each epoch, we randomly sample a 3D conformer for each molecule, but not enumerate all the 3D conformers of that molecule. The resulting models (\ie DMT-B and DMT-L) are used directly for evaluation in Table~\ref{tab:conformer}. 
\item \textbf{Stage 2: Projector Warmup.} For both datasets, we train only the LoRA weights of MoLlama, and the cross-modal projector for 10 epochs. The pretrained weights of DMT and MoLlama are frozen throughout the process.
\item \textbf{Stage 3: Integrated Fine-tuning.} For both datasets, we train the integrated model for 500 epochs. We train the LoRA weight of MoLlama, the cross-modal pojector, and the DMT model. The pretrained weights of MoLlama are frozen throughout the process.
\end{itemize}
% The 3D conformer prediction task utilized our Diffusion Molecular Transformer (DMT) model in a two-step process. The training process ran for a maximum of 20,000 epochs with batch sizes of 256 for both datasets.

% Conformer quality was evaluated using 3D metrics every 50 epochs, with 100 sampling steps used for GEOM-DRUGS and 500 for GEOM-QM9 during conformer generation.

\textbf{Evaluation.} Following~\citep{MCF,torsion}, we use the dataset split of 243473/30433/1000 for GEOM-DRUGS and 106586/13323/1000 for GEOM-QM9, provided by~\citep{GeoMol}. For a molecule with $K$ ground truth conformers, we generate $2K$ conformers as predictions.

\textbf{Evaluation Metrics.} Let $\{C_l^*\}_{l\in[1,L]}$ be the $L$ predicted conformers and let $\{C_k\}_{k\in[1,K]}$ be the $K$ ground truth conformers. The evaluation metrics AMR-R (AMR-Recall) and COV-R (COV-Recall) can be formally defined as follows:
\begin{align}
 \text{COV-R} & := \frac{1}{L} |\{l\in [1..L]: \exists k\in [1..K], \text{RMSD}(C_k, C_l^*)<\delta\}|, \\
 \text{AMR-R} & := \frac{1}{L} \sum_{l\in [1..L]} \min_{k\in [1..K]} \text{RMSD}(C_k, C_l^*),
\end{align}
where $\delta$ is a threshold that is set to $0.75\r{A}$ for GEOM-DRUGS and set to $0.5\r{A}$ for GEOM-QM9, following~\citep{MCF,torsion}. AMR-P (AMR-Precision) and COV-P (COV-Precision) can be similarly defined by swapping the ground truth conformers and predicted conformers.



% \section{Discussion on NExT-Mol's Design Choices}
% 2D molecule generator is worse than 1D molecule generator
% Why (JODO, MIDI) is small? It is hard to scaleup 2D molecule generator (cite relevant works), easy to scale-up 1D molecule generator
% Given a 1D molecule generator, it requires a model that combines 1D generator and 3D information.
