\section{3D Diffusion Meets 1D LM for 3D Molecule Generation}
\textbf{NExT-Mol for 3D Molecule Generation.} NExT-Mol is a foundation model that generates 3D molecules with a two-step method: initially generating the 1D molecule sequence (a subset of a 3D molecule) using the MoLlama LM and subsequently predicting its 3D conformer using the DMT diffusion model.
Here we begin by introducing the MoLlama for 1D molecule generation and then proceed to DMT. Finally, we detail the transfer learning method to incorporate MoLlama's 1D representation to enhance DMT's 3D conformer prediction. Appendix~\ref{app:method_detail} includes implementation details.
% We describe the key insights of our method, and more details are in 

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{figures/dmtv2}
\vspace{-5mm}
\caption{Overview of DMT's neural architecture. \textbf{(a)} DMT is a diffusion model learning to denoise random Gaussian perturbations $\boldsymbol{\epsilon}$ applied on the 3D coordinates of atoms. \textbf{(b)} DMT relies on the RMHA module to iteratively update atom representations $\mathbf{H}$ and pair representations $\mathbf{E}$.}
\label{fig:dmt}
\vspace{-6mm}
\end{figure}

\vspace{-1mm}
\subsection{1D Molecule Generation with Molecular Llama LM}\label{sec:1d-generation}
\vspace{-1mm}
% Here we briefly introduce the data preparation, neural architecture, and data augmentation used for training MoLlama. More details are in Appendix~\ref{app:mollama}.

% \textbf{Data Preparation.} Following~\citep{Chemformer}, we collect 1.8 billion molecules from the ZINC-15 database~\citep{ZINC15}, significantly more than the 100 million molecules used in previous studies~\citep{Chemformer,MolGen}. We keep only molecules with molecular weight$\leq$500 Daltons and LogP$\leq$5~\citep{flynn1980substituent}, and transform them into SELFIES~\citep{SELFIES} sequences. After canonicalizing the SELFIES and removing hydrogen atoms, the dataset contains 90 billion tokens. We further filter the molecules in the valid and test sets of the GEOM-QM9 and GEOM-DRUGS datasets~\citep{GEOM} and randomly sampled 1\% of the remaining data as the validation set.

\textbf{Data Preparation.} Following~\citep{Chemformer}, we collect 1.8 billion molecules from the ZINC-15 database~\citep{ZINC15}, significantly more than the 100 million molecules used in previous studies~\citep{Chemformer,MolGen}. We preprocess the molecules to transform them into SELFIES and perform data filtering to avoid overlap with the downstream datasets. The resulting dataset contains 90 billion SELFIES tokens. 

% keep only molecules with molecular weight$\leq$500 Daltons and LogP$\leq$5~\citep{flynn1980substituent}, and transform them into SELFIES~\citep{SELFIES} sequences. After canonicalizing the SELFIES and removing hydrogen atoms, the dataset contains 90 billion tokens. We further filter the molecules in the valid and test sets of the GEOM-QM9 and GEOM-DRUGS datasets~\citep{GEOM} and randomly sampled 1\% of the remaining data as the validation set.


\textbf{Pretraining MoLlama.} Our MoLlama is a 960M parameter LM with the popular decoder-only Llama-2~\citep{Llama-2} architecture. We pretrain it from scratch for 1D molecule generation with the next-token prediction objective. The pretraining takes 555K global steps, processing 145 billion tokens, which amounts to approximately 1.6 passes through the pretraining dataset.

% Following~\citep{TinyLlama}, we use Flash-Attention~\citep{flashattention2} and FSDP~\citep{FSDP} to speed up training.

\textbf{Randomized SELFIES Augmentation.} We use randomized SELFIES as data augmentations during fine-tuning MoLlama for 1D molecule generation. A molecule can have multiple valid SELFIES, because they are generated by traversing the 2D molecular graph in different orders. Randomized SELFIES are generated by traversing in random orders. This approach improves sample diversity and mitigates overfitting compared to using the canonical traversal order~\citep{RandomSmiles}. The intuition is that the atoms in a molecule are inherently unordered, therefore an ideal LM should generate different orderings of the same molecule with equal likelihood. 



\vspace{-1mm}
\subsection{3D Conformer Prediction with Diffusion Molecular Transformer}
\vspace{-1mm}
Here we elaborate on the three key components of our proposed DMT: (1) the diffusion process governing the training and inference; (2) the neural architecture; and (3) the rotation augmentation. 
% Here we elaborate on their key insights and leave more details to Appendix~\ref{app:dmt}.

% 我其实只需要 argue equivariance 没有用就行了，不能说所有的 inductive bias 都没有用，那我就自我矛盾了。
\textbf{Diffusion Process.} A molecule $G=(\mathbf{x}, \mathbf{h}, \mathbf{e})$ is represented by its 3D coordinates $\mathbf{x}\in \mathbb{R}^{N\times 3}$, atom features $\mathbf{h}\in \mathbb{R}^{N\times d_1}$ (\eg atom types), and pair features $\mathbf{e}\in \mathbb{R}^{N\times N\times d_2}$ (\eg chemical bonds), where $N$ is the number of atoms and $d_1$ and $d_2$ are the feature dimensions. For 3D conformer prediction, we use a continuous-time diffusion model~\citep{VDM} that denoises a molecule's 3D coordinates $\mathbf{x}$ based on its atom and pair features. As Figure~\ref{fig:dmt}a shows, in the forward diffusion process, noises are gradually applied to the original 3D coordinates $\mathbf{x}^{(0)}=\mathbf{x}$ such that $q(\mathbf{x}^{(t)}|\mathbf{x}^{(0)}) = \mathcal{N}(\mathbf{x}^{(t)};\sqrt{\bar{\alpha}^{(t)}}\mathbf{x}^{(0)},(1-\bar{\alpha}^{(t)})\mathbf{I})$, where $t\in (0,1]$ is the diffusion's time-step, and $\bar{\alpha}^{(t)}$ is a hyperparameter controlling the noise scale at the $t$ step. Based on the reparameterization trick~\citep{DDPM}, we can sample $\mathbf{x}^{(t)} = \sqrt{\bar{\alpha}^{(t)}}\mathbf{x}^{(0)} + \sqrt{1-\bar{\alpha}^{(t)}}\boldsymbol{\epsilon}^{(t)}$, where $\boldsymbol{\epsilon}^{(t)}\sim \mathcal{N}(\mathbf{0},\mathbf{I})$. Given the perturbed coordinates $\mathbf{x}^{(t)}$, DMT is trained to predict the noise $\boldsymbol{\epsilon}^{(t)}$ by minimizing the MSE loss $\mathcal{L}=\|\boldsymbol{\epsilon}^{(t)}-\text{DMT}(G^{(t)}, t)\|^2_2$, where $G^{(t)}=(\mathbf{x}^{(t)}, \mathbf{h}, \mathbf{e})$.
After training, DMT can be employed for 3D conformer prediction through ancestral sampling~\citep{DDPM}.
% Following~\cite{EDM}, we apply the continuous-time diffusion from the Variational Diffussion Model~\cite{VDM}.


\textbf{Neural Architecture.} As Figure~\ref{fig:dmt}b illustrates, DMT adopts Relational Multi-Head Self-Attention (\textbf{RMHA})~\citep{JODO} and adaptive layernorm (adaLN)~\citep{Film,DiT}. adaLN replaces the learnable scale and shift parameters in standard layernorm~\citep{ba2016layer} with adaptive ones that are generated from the condition embedding $\mathbf{C}$, which combines the time-step and optionally a desired chemical property.
For simplicity, we omit adaLNs in discussion below.

The philosophy behind DMT's neural architecture generally follows the ``bitter lesson'' recently revealed by MCF~\citep{MCF} that large scalable models outperform domain-specific inductive biases. Notably, MCF shows that it is unnecessary to have an architecture of built-in 3D equivariance for conformer prediction. However, MCF is limited to employing a lossy representation of 2D molecular structures and overlooks bond information, by relying on the top-k eigenvectors of the graph Laplacian~\citep{maskey2022generalized} to represent 2D molecular graphs. To address this issue, DMT retains the full information of 2D molecular graphs in its atom representation $\mathbf{H}\in \mathbb{R}^{N\times d}$ and pair representation $\mathbf{E}\in \mathbb{R}^{N\times N \times d}$, and then applies RMHA to learn and distinguish the 2D graph structures.
Specifically, the atom representations $\mathbf{H}$ are initialized by concatenating the atom features $\mathbf{h}$ and the perturbed 3D coordinates $\mathbf{x}^{(t)}$, the pair representations $\mathbf{E}$ are initialized by concatenating the pair features $\mathbf{e}$ and the distances between each atom pair. $\mathbf{H}$ and $\mathbf{E}$ are then iteratively refined by RMHA. The single-head RMHA is defined below with the multi-head version in Appendix~\ref{app:dmt}:
\vspace{-3mm}

\begin{tabular}{p{6.5cm}p{6.5cm}}
% \vspace{0.01mm}
\begin{equation}\label{eq:a}
[\mathbf{Q}; \mathbf{K}; \mathbf{V}] = [\mathbf{W}_{q}; \mathbf{W}_{k}; \mathbf{W}_{v}] \Trans{\mathbf{H}},
\end{equation}
&
\vspace{-5mm}
\begin{equation}\label{eq:b}
[\mathbf{Q}^E;\mathbf{V}^E] = \tanh([\mathbf{W}_{eq};\mathbf{W}_{ev}] \Trans{\mathbf{E}}),
\end{equation} \\
\vspace{-12mm}
\begin{equation}\label{eq:c}
a_{i,j}=\softmax_{j}(\frac{(\mathbf{Q}^E_{i,j}\odot \mathbf{Q}_i)\Trans{\mathbf{K}}_j}{\sqrt{d}}),
\end{equation}
&
\vspace{-10.5mm}
\begin{equation}\label{eq:d}
\mathbf{O}_i = \sum_{j=1}^{N} a_{i,j}(\mathbf{V}^E_{i,j}\odot \mathbf{V}_j),
\end{equation}
\end{tabular}

\vspace{-4mm}
\noindent where $\odot$ denotes element-wise product; $\softmax_j$ denotes softmax along the $j$ dimension; linear projectors $\mathbf{W}_q$, $\mathbf{W}_k$, and $\mathbf{W}_v$ generate queries, keys, and values for atom representations, $\mathbf{W}_{eq}$ and $\mathbf{W}_{ev}$ generate queries and values for pair representations; $\mathbf{O}_i$ is RMHA's output for the $i$-th atom; and $\mathbf{Q}^E_{i,j},\mathbf{V}^E_{i,j}\in \mathbb{R}^{d}$ are the query and value for the atom pair representation $(i,j)$.

% RMHA extends the standard Self-Attention by modifying its query $\mathbf{Q}_i$ and value $\mathbf{V}_j$: they are multiplied by the pair representation's query $\mathbf{Q}_{i,j}^E$ and value $\mathbf{V}_{i,j}^E$. In this way, the atom representation $\mathbf{H}$ is adaptively informed by the structural and interaction information in the pair representations $\mathbf{E}$. After RMHA, $\mathbf{O}_i$ is passed to an MLP to update the atom representation $\mathbf{H}_i$, and the linear combination of $\mathbf{O}_i$ and $\mathbf{O}_j$ is used to update the pair representation $\mathbf{E}_{i,j}$. As Figure~\ref{fig:dmt}b illustrates, residual connections and adaLNs are included for improved performance.

RMHA uses the pair-level query $\mathbf{Q}^E_{ij}$ and key $\mathbf{V}^E_{ij}$ of $\mathbf{E}$ to modify the atom-level query $\mathbf{Q}_i$ and value $\mathbf{V}_j$ through element-wise multiplication ($\odot$), enabling RMHA to fully incorporate pair representations. Specifically, the pair $\mathbf{E}$ affects attention scores via $(\mathbf{Q}^E_{ij} \odot \mathbf{Q}_i) \mathbf{K}_j^\top$, and affects the aggregated attention values via $\mathbf{V}^E_{ij} \odot \mathbf{V}_j$. In this way, the output $\mathbf{O}$ is adaptively informed by the structural and interaction information in $\mathbf{E}$. After RMHA, $\mathbf{O}_i$ is passed to an MLP to update the atom representation $\mathbf{H}_i$, and the linear combination of $\mathbf{O}_i$ and $\mathbf{O}_j$ is used to update the pair representation $\mathbf{E}_{i,j}$. As Figure~\ref{fig:dmt}b illustrates, residual connections and adaLNs are included for improved performance.

% \textbf{Data Augmentation.} We use inference-time dropout to improve the sample diversity of conformers and use random rotation augmentation to cultivate DMT's equivariance to rotated input.
% \begin{itemize}[leftmargin=*]
% \item \textbf{Inference-Time Dropout Improves Conformer Diversity.} We apply dropout~\citep{dropout} during both training and inference to introduce more randomness and improve the sample diversity. Specifically, we add dropout to the attention scores in RMHA and the MLPs in the DMT. The intuition is that inference-time dropout generates different views (augmentations) for the same molecule~\citep{SimCSE}, thus increasing sample diversity. Dropout ratios can be tuned during inference to tradeoff between precision and recall. Compared to previous works~\citep{torsion,ParticleGuidance} that use external tools or extra gradient guidance to improve diversity (see Appendix~\ref{app:dmt}), our method is simpler and incurs no additional computational cost.
\textbf{Random Rotation Augmentation.} Following AlphaFold3~\citep{AF3}, we apply the same random rotation augmentation on both the input 3D coordinates ($\mathbf{x}^{(t)}$) and the target 3D coordinates ($\mathbf{\boldsymbol{\epsilon}}^{(t)}$) to help DMT obtain equivariance to rotated inputs by learning. While~\citep{MCF} report decreased performance given random rotations, DMT benefits from it, potentially due to the improved neural architecture.
% \end{itemize}


% \subsubsection{Diffusion Training and Inference}




\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/integrate}
    \vspace{-1mm}
    \caption{Transfer learning between MoLlama's 1D representations and DMT's 3D prediction. \textbf{(a)} A cross-modal projector bridges the gap between MoLlama and DMT. \textcolor{gray}{Grey H} atoms have no corresponding SELFIES tokens, and are replaced by a learnable token.
    \textbf{(b)} Transfer learning's three training stages. Snowflake \includegraphics[width=0.02\textwidth]{figures/snowflake.pdf} denotes frozen parameters while flame \includegraphics[width=0.02\textwidth]{figures/flame.pdf} denotes trainable ones.}
    \label{fig:integrate}
    \vspace{-5mm}
\end{figure}

\vspace{-1.5mm}
\subsection{MoLlama Representations Improve DMT's 3D Conformer Prediction}
\label{sec:integrate}
\vspace{-2mm}
We explore the transfer learning between molecular 1D sequences and 3D conformers. As Figure~\ref{fig:integrate} illustrates, we leverage MoLlama's pretrained representation to improve DMT's 3D conformer prediction. This is achieved by our cross-modal projector and the corresponding training strategy.

\textbf{Cross-Modal Projector.} Following~\cite{liu2023molca}, DMT uses a projector to leverage MoLlama for atom representation, addressing two challenges: (1) MoLlama uses causal self-attention, where each token only perceives preceding tokens, limiting the representation quality; and (2) SELFIES tokens do not map directly to individual atoms. Mitigating the first issue, we feed MoLlama's SELFIES representations into a single-layer bi-directional self-attention~\citep{Transformer}, expanding the receptive field of SELFIES tokens. Further, we program the SELFIES-to-atom mapping using the SELFIES and RDKit software.
For atoms corresponding to multiple SELFIES tokens, we obtain its representation by mean pooling; for hydrogen atoms without corresponding SELFIES tokens, we use a learnable token as a replacement. The output of the SELFIES-to-atom mapping is then fed into an MLP and concatenated with DMT's original atom representations for 3D conformer prediction.

\textbf{Training Strategy.} As Figure~\ref{fig:integrate}b illustrates, to save computation, we fine-tune a pretrained DMT to incorporate MoLlama representations, instead of training a new DMT from scratch using MoLlama representations. Throughout the process, MoLlama uses LoRA tuning~\citep{lora} to save memory. The training strategy consists of three stages. In the first stage, we train a standalone DMT without MoLlama \yuan{until convergence}. In the second stage, we attach MoLlama and the cross-modal projector to the pretrained DMT, keeping the DMT parameters frozen, and train for 10 epochs to warmup the random parameters in the projector and LoRA. This step prevents the gradients from the random parameters from distorting the pretrained DMT parameters~\citep{kumar2022finetuning}. In the final stage, we fine-tune the entire integrated model \yuan{until convergence}. 

% \textbf{SELFIES Representation.}
When incorporating MoLlama representations into DMT, we find that canonical SELFIES performs better than randomized SELFIES. This may be because bridging the gap between 1D MoLlama and 3D DMT is challenging, and using the fixed canonical representations leads to faster convergence.



% \textbf{Training Strategy of Tranfer Learning.} As Figure~\ref{fig:integrate} illustrates, we fine-tune a pretrained DMT to incorporate MoLlama representations, instead of training a new DMT from scratch using MoLlama representations, to save computation. MoLlama uses LoRA tuning~\citep{lora} throughout the training to save memory. Our training strategy can be outlined as: \textbf{Stage 1: DMT Training.} We train a standalone DMT without MoLlama \yuan{until convergence}; \textbf{Stage 2: Projector Warmup.} We attach MoLlama and the cross-modal projector to the pretrained DMT with frozen parameters and train for 10 epochs. This is to warmup the random parameters in the projector and LoRA to avoid discrupting the pretrained DMT parameters~\citep{kumar2022finetuning}; \textbf{Stage 3: Integrated Fine-tuning.} We fine-tune the entire integrated model \yuan{until convergence.} % (500 epochs)