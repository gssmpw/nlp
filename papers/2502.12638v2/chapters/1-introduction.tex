% \vspace{-1mm}
\section{Introduction}
% \vspace{-1mm}
% context
% level1: Molecule design is important. Deep learning is showing good performance in generating 2d molecular graphs. However, 2D molecular graphs is still gapped when being used in various applications, such as xxx. it therefore inspires the generation of 3D molecules.
% Molecule discovery is crucial for designing new drugs and materials. To efficiently navigate the astronomical chemical space of molecules, generative deep learning methods have been extensively explored. These methods~\citep{JT-VAE, DiGress} have achieved promising performances when generating 2D molecular graphs, which depict the atoms and the chemical bonds connecting the atoms. While 2D molecular structures provide valuable insights in chemical synthesis, the missing of 3D conformer information limits their application scope. For example, knowing the 3D molecular conformer is important for structure-based drug design~\citep{zhang2023complete}, predicting molecular quantum chemical properties~\citep{UniMol}, and molecular dynamic simulation~\citep{hansson2002molecular}.

Molecule discovery is crucial for designing new drugs and materials. To efficiently navigate the astronomical chemical space of molecules, generative deep learning methods have been extensively explored. While promising progress has been made in generating 2D molecular graphs~\citep{JT-VAE, DiGress},
recent research has shifted toward 3D molecule generation due to its broader application scope.
For example, understanding the 3D molecular geometry is crucial for structure-based drug design~\citep{zhang2023molecule}, prediction of molecular quantum chemical properties~\citep{UniMol}, and molecular dynamic simulation~\citep{hansson2002molecular}.




% 动机 1是为了 validity，基于已知的 valid 的分子，我们的 3D 模型能够取得更好的表现
% 动机 2是为了 利用大量的预训练数据
% 动机 3是为了 rethink 这个 task
% 动机 4是之前的模型的transformer architecture 不够好，我的一个重要贡献是 transformer architecture，这需要 highlight。而且这中间也有很多 trick，比如 rotation augmentation，比如不用 equivariant network，比如 scalingup，再比如使用 align loss。具体说不好，就是他们坚持用了 equivariance 结构，但是这个结构是没有意义的。但是我又不能直接和他们比，因为我不是直接做 3D分子生成的，我是先做1D 分子生成，再做 3Dconformer 的。

% review
% level2: in this work, we study 3d molecule generation, which generates both 2D molecular structures as well as the 3D conformer of molecules
% 这里的很多东西其实都是不重要的，我唯一要说的话，就是现在的 3D validity 还不够好，还可以提升，然后从 1D 提升是一个很好的思路，因为 grounding on existing valid Molecule。另一个思路，是提出一种新的 transformer 结构，因为之前的方法的效果都不好，因为缺少一个好的 transformer，我们的效果相比于 MCF 也是有提升的，因为我们更好地利用了 relational 信息，我们的方法是无损的对 graph structure 的表示，并且获得了更好的效果。
% The task of 3D molecule generation mitigates the gap by predicting 3D molecular conformers along with the 2D molecular graphs~\citep{EDM}.

%
% 3D molecule generation is to is to predict 3 molecular conformers along with their 2D graphs~\citep{EDM}. These generated 3D molecules are typically typically evaluated by the molecular validity and stability to test their adherence to the chemical valency rules. The emerging 3D diffusion models~\citep{MiDi,MUDiff,JODO} have improved thimproved these metrics, due to their advantages on modeling continuous 3D conformers, yet still occasionally generate invalid molecules. Conversely, generating valid molecular SELFIES~\citep{SELFIES} sequences, which represent 2D molecular graphs as linear strings, using a 1D Language Model (LM) is significantly easier~\citep{MolGen,moses}. This is because SELFIES is a robust representation that ensures 100\% chemical validity of its described molecules, and LMs are scalable sequence learners~\citep{GPT3}. To utilize these advantages of SELFIES and LMs for 3D molecule generation, a natural solution is to use a \yuan{3D diffusion model}~\citep{torsion} to predict 3D conformers of molecules generated by a 1D SELFIES-based LM. Nonetheless, to the best of our knowledge, few prior research has thoroughly explored this incorporation for 3D molecule generation.

% 3D molecule generation is to predict 3D molecular conformers along with their 2D graphs~\citep{EDM}. These generated 3D molecules are typically evaluated by the molecular validity and stability to test their adherence to the chemical valency rules. Recent advancements in 3D diffusion models~\citep{MiDi,MUDiff,JODO} have improved these metrics by better modeling continuous 3D conformers, yet still generate invalid molecules.
% % Conversely, generating valid molecular SELFIES~\citep{SELFIES} sequences, which represent 2D molecular graphs as linear strings, using a 1D Language Model (LM) is significantly easier~\citep{MolGen,moses}. This is because SELFIES is a robust representation that ensures 100\% chemical validity of its described molecules, and LMs are scalable sequence learners~\citep{GPT3}.
% Conversely, it is significantly easier to
% generate valid molecular SELFIES~\citep{SELFIES} sequences using a 1D Language Model (LM)~\citep{MolGen,moses}. SELFIES represent 2D molecular graphs as linear strings. They ensure 100\% chemical validity of its described molecules.
% Inspired by these earlier studies, a natural solution for generating valid 3D molecules is to incorporate a 1D SELFIES-based LM into a 3D diffusion model~\citep{torsion}, thus leveraging the chemical validity of 1D representations while addressing the challenge of predicting accurate 3D molecular conformers.
% % To utilize these advantages of SELFIES and LMs for 3D molecule generation, a natural solution is to use a \yuan{3D diffusion model}~\citep{torsion} to predict 3D conformers of molecules generated by a 1D SELFIES-based LM.
% Nonetheless, to the best of our knowledge, few prior research has thoroughly explored this for 3D molecule generation.

3D molecule generation aims to predict 3D molecular conformers along with their 2D graphs~\citep{EDM}. These generated 3D molecules are typically evaluated based on their molecular validity and stability, ensuring adherence to the chemical valency rules. Recent advancements in 3D diffusion models~\citep{MiDi,MUDiff,JODO} have improved these metrics by better modeling continuous 3D conformers, yet they still occasionally generate invalid molecules.
% Conversely, generating valid molecular SELFIES~\citep{SELFIES} sequences, which represent 2D molecular graphs as linear strings, using a 1D Language Model (LM) is significantly easier~\citep{MolGen,moses}. This is because SELFIES is a robust representation that ensures 100\% chemical validity of its described molecules, and LMs are scalable sequence learners~\citep{GPT3}. 
% % Conversely, it is significantly easier to 
% % generate valid molecular SELFIES~\citep{SELFIES} sequences using a 1D Language Model (LM)~\citep{MolGen,moses}. SELFIES represent 2D molecular graphs as linear strings. They ensure 100\% chemical validity of its described molecules.
This validity issue hinders distribution learning of valid  molecular structures, like pharmacophoric functional groups. For improvement, we draw inspiration from 1D molecule generation \citep{MolGen,moses} studies, which reliably ensure 100\% validity. By representing 2D molecular graphs as linear strings of SELFIES \citep{SELFIES}, these approaches typically leverage 1D language models (LMs) for 2D molecule generation. Due to SELFIES' inherent robustness, the generated molecules are guaranteed to be 100\% valid.
Inspired by these studies, a natural solution for improving 3D molecule generation is to incorporate a 1D SELFIES-based LM into a 3D diffusion model~\citep{torsion}, thus leveraging the chemical validity of 1D representations while improving 3D conformer prediction.
% addressing the challenge of predicting accurate 3D conformers.
% To utilize these advantages of SELFIES and LMs for 3D molecule generation, a natural solution is to use a \yuan{3D diffusion model}~\citep{torsion} to predict 3D conformers of molecules generated by a 1D SELFIES-based LM. 
To our best knowledge, few prior research has thoroughly explored this incorporation for 3D molecule generation.


% Additionally, SELFIES sequences can be effectively learned and sampled using a language model (LM) \citep{MolGen}.
% However, SELFIES cannot describe 3D conformers, therefore cannot be directly applied to 3D molecule generation.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.75\linewidth]{figures/nextmol.pdf}
    \vspace{-1mm}
    \caption{Overview of our NExT-Mol foundation model for 3D molecule generation. NExT-Mol consists of three key components: (1) MoLlama, a large LM for generating 1D molecule sequences; (2) DMT, a diffusion model to predict 3D conformers from the 1D sequences; and (3) NExT-Mol leverages transfer learning to enhance DMT's 3D prediction with MoLlama's 1D representations.}
    \label{fig:nextmol}
    \vspace{-3mm}
\end{figure}


% level 3：我们探索一种简单的办法，并且想办法 maximize 他的效果。
To bridge the research gap above, we explore a two-step solution for 3D molecule generation: initially generating a 1D molecule (a subset of a 3D molecule) using an LM and subsequently predicting its 3D conformer with a diffusion model.
% , as illustrated in Figure xx. 
Here we focus on three key strategies --- scaling up 1D molecular LMs, refining the architecture of 3D diffusion models, and utilizing transfer learning between 1D and 3D modeling --- to resolve the following three challenges faced by prior studies:
\vspace{-2mm}
% To enhance its performance, we focus on scaling up the model size, refining its neural architecture, and leveraging transfer learning. Upon reviewing previous works, we identify three key challenges: 
\begin{itemize}[leftmargin=*]
\item \textbf{The Development of An Effective 1D Molecular LM.} This can be done by training an autoregressive transformer LM~\citep{Transformer} on a large SELFIES corpus. However, existing studies have the following limitations: some use non-autoregressive pretraining, rendering them unsuitable for \textit{de novo} generation~\citep{MolGen,Chemformer,RegressionTransformer,SELFormer}; some do not have 100\% validity~\citep{MolGPT}; and others are constrained by small model sizes and employ non-transformer architectures, limiting their scalability~\citep{moses,LIMO,RandomSmiles,JT-VAE}.
% may not scaleup effectively given the non-transformer architectures.
% \item \textbf{The Design of A Powerful 3D Diffusion Model.} This is to accurately generate the 3D conformers for the 1D molecules generated by the 1D molecular LM in the earlier step. However, existing works either exhibit limited performance~\citep{torsion,ParticleGuidance,GeoDiff,GeoMol} or are not open-source~\citep{MCF}, preventing their adaptation for future research. Moreover, the neural architecture in the prominent MCF study \citep{MCF} can be improved by leveraging the full information of 2D molecular graphs.
\item \textbf{The Design of A Powerful 3D Diffusion Model.} This is to accurately generate the 3D conformers for the 1D molecules generated by the 1D molecular LM in the earlier step. Existing works can be improved by adopting scalable architectures~\citep{torsion,ParticleGuidance,GeoDiff,GeoMol} or leveraging the full information of 2D molecular graphs~\citep{MCF}.
\item \textbf{Transfer Learning between 1D Molecule Sequences and 3D Conformers.} It has the potential to offer a significant improvement to 3D conformer prediction, given the greater availability of 1D sequences compared to high-accuracy 3D conformers, which are typically derived by expensive physics-based computations. For example, ZINC22~\citep{ZINC22} now includes over 54.9 billion 1D sequences
% \footnote{\url{https://cartblanche22.docking.org/}}
and GEOM~\citep{GEOM} holds only 37 million 3D conformers. Although this 1D to 3D transfer learning was successfully applied to 3D protein structure prediction~\citep{ESM2,OmegaFold}, similar methods remain mostly unexplored for small molecules, indicating a significant research opportunity.
\end{itemize}
\vspace{-1mm}

% level4: what we study
To address the challenges above, we propose a foundation model -- \textbf{NExT-Mol}: 3D Diffusion Meets 1D Language Modeling for 3D Molecule Generation, as illustrated in Figure~\ref{fig:nextmol}. NExT-Mol consists of three key components: (1) To achieve effective autoregressive 1D molecule generation, we pretrain a \underline{Mo}lecular \underline{Llama} LM (\textbf{MoLlama})~\citep{Llama-2,TinyLlama} on a large collection of 1.8B SELFIES sequences. 
This extensive pretraining empowers MoLlama to effectively capture the desired 1D/2D molecular patterns (\eg scaffolds and fragments) in downstream datasets, laying a strong foundation for the subsequent 3D conformer prediction.
% making it highly effective in generating 1D molecules for subsequent 3D conformer prediction.
% and making it highly effective in generating 1D molecules for subsequent 3D conformer prediction.
(2) To achieve high-accuracy 3D conformer prediction, we introduce a novel diffusion model -- \underline{D}iffusion \underline{M}olecular \underline{T}ransformer (\textbf{DMT}). DMT combines the power of a scalable neural architecture~\citep{MCF} and retains the full information of 2D molecular graphs by incorporating the Relational Multi-Head Self-Attention~\citep{JODO} that extends the standard self-attention by incorporating pair information describing atomic interactions.
% , which is useful for attention.
% the ability to incorporate chemical inductive bias, \yuan{such as bond information},
% Interestingly, DMT features a data augmentation strategy based on dropout~\citep{dropout} to improve sampling diversity.
We show that DMT achieves leading performance for 3D conformer prediction, surpassing prior works by 1.1\% COV-R on GEOM-DRUGS. Further, it accurately reveals the 3D structures of MoLlama-generated 1D molecules, providing a 26\% relative gain in 3D FCD and significant improvements in geometric similarity and stability on GEOM-DRUGS. (3) We show that transfer learning between 1D molecular sequences and 3D conformers improves conformer prediction by 1.3\% COV-R on GEOM-DRUGS. This improvement is driven by transfering MoLlama's pretrained 1D representations, which encode rich molecular knowledge, to DMT for better molecular representation. The 1D-to-3D modality gap in transfer learning is bridged by our proposed cross-modal projector and the corresponding training strategy~\citep{llava}.


Collectively, our NExT-Mol foundation model is a versatile multi-task learner, and demonstrates leading performances for \textit{de novo} 3D molecule generation, conditional 3D molecule generation, and 3D conformer prediction on the GEOM-DRUGS, GEOM-QM9~\citep{GEOM} and QM9-2014~\citep{QM9} datasets. 
% It is also straightforward to adapt NExT-Mol to random 3D and 1D generation. 
The strong performance highlights NExT-Mol's effectiveness and its potential impact as a foundation model in the field. We further present extensive ablation studies to demonstrate the significance of each component of NExT-Mol.

% on the model design choices and evaluation metrics (\eg molecule stability) for 3D molecule generation and conformer prediction.

% Our contributions can be summarized as follows:
% \begin{itemize}[leftmargin=*]
% \item \textbf{The NExT-Mol Foundation Model.} \yuan{NExT-Mol effectively integrates two models for 3D molecule generation:
% MoLlama for 1D molecule generation and DMT for 3D conformer prediction. Both models are tailored for their tasks with optimized architectures and training strategies (\eg \yuan{relational-attention for 3D diffusion and extensive 1D pretraining dataset}). Further, NExT-Mol demonstrates effective transfer learning between MoLlama's 1D representations and DMT's 3D prediction. These models will be made publicly available to support future research.}
% \item \textbf{State-of-the-Art Performances Across Various Tasks.} NExT-Mol demonstrates state-of-the-art performance across diverse tasks, including \textit{de novo} 3D molecule generation, 3D conformer prediction, and conditional 3D molecule generation targeting quantum chemical properties, evidenced by its performance on the GEOM-QM9 and GEOM-DRUGS datasets~\citep{GEOM}. The strong performance highlights NExT-Mol's effectiveness and its potential impact as a foundation model in the field.
% \item \yuan{\textbf{Analysis on 3D Molecule Generation.}} We present \yuan{extensive in-depth analysis and ablation studies} on the model design choices and evaluation metrics (\eg molecule stability) for 3D molecule generation and conformer prediction. Our findings reveal that \yuan{xxx.}
% \end{itemize}

