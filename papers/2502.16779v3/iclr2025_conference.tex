
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{algorithm}
\usepackage{algcompatible}
\usepackage{booktabs} 
\usepackage{multirow} 
\usepackage[export]{adjustbox}


\usepackage{enumitem}



\title{Unposed Sparse Views Room Layout Reconstruction in the Age of Pretrain Model}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

% \author{Antiquus S.~Hippocampus, Natalia Cerebro \& Amelie P. Amygdale 
% % \thanks{ Use footnote for providing further information
% % about author (webpage, alternative address)---\emph{not} for acknowledging
% % funding agencies.  Funding acknowledgements go at the end of the paper.} \\
% Department of Computer Science\\
% Cranberry-Lemon University\\
% Pittsburgh, PA 15213, USA \\
% \texttt{\{hippo,brain,jen\}@cs.cranberry-lemon.edu} \\
% \And
% Ji Q. Ren \& Yevgeny LeNet \\
% Department of Computational Neuroscience \\
% University of the Witwatersrand \\
% Joburg, South Africa \\
% \texttt{\{robot,net\}@wits.ac.za} \\
% \AND
% Coauthor \\
% Affiliation \\
% Address \\
% \texttt{email}
% }



\author{
Yaxuan Huang$^{1\ast}$ \quad Xili Dai$^{2\ast}$ \quad Jianan Wang$^{3}$ \quad Xianbiao Qi$^{4}$
\quad Yixing Yuan$^{1}$ \\
\textbf{\quad Xiangyu Yue$^{5\dagger}$} \\
\textsuperscript{1}Hong Kong Center for Construction Robotics, The Hong Kong University of Science and Technology\; \\
\textsuperscript{2}The Hong Kong University of Science and Technology (Guangzhou) \; \\
\textsuperscript{3}Astribot\; 
\textsuperscript{4}Intellifusion Inc.\; \\
\textsuperscript{5}MMLab, The Chinese University of Hong Kong\; \\ 
% \and
% \url{https://github.com/justacar/Plane-DUSt3R}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\newcommand{\xd}[1]{{\color{black}{#1}}}
\newcommand{\ours}{Plane-DUSt3R}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle
\renewcommand{\thefootnote}{} 
\footnotetext{$\ast$ Equal contribution, $\dagger$ Corresponding author}


\begin{figure}[h]
\includegraphics[width=0.96\textwidth]{figs/pdf_pic.pdf}
\centering
\caption{We present a novel method for estimating room layouts from a set of unconstrained indoor images. Our approach demonstrates robust generalization capabilities, performing well on both in-the-wild datasets \citep{zhou2018stereo} and out-of-domain cartoon \citep{weber2023toon3d} data.}
\label{fig:teaser}
\end{figure}


\begin{abstract}
Room layout estimation from multiple-perspective images is poorly investigated due to the complexities that emerge from multi-view geometry, which requires muti-step solutions such as camera intrinsic and extrinsic estimation, image matching, and triangulation. However, in 3D reconstruction, the advancement of recent 3D foundation models such as DUSt3R has shifted the paradigm from the traditional multi-step structure-from-motion process to an end-to-end single-step approach.
To this end, we introduce \ours{}, a novel method for multi-view room layout estimation leveraging the 3D foundation model DUSt3R. \ours{} incorporates the DUSt3R framework and fine-tunes on a room layout dataset (Structure3D) with a modified objective to estimate structural planes. By generating uniform and parsimonious results, \ours{} enables room layout estimation with only a single post-processing step and 2D detection results. 
Unlike previous methods that rely on single-perspective or panorama image, \ours{} extends the setting to handle multiple-perspective images. Moreover, it offers a streamlined, end-to-end solution that simplifies the process and reduces error accumulation.
Experimental results demonstrate that \ours{} not only outperforms state-of-the-art methods on the synthetic dataset but also proves robust and effective on in the wild data with different image styles such as cartoon. Our code is available at: \href{https://github.com/justacar/Plane-DUSt3R}{https://github.com/justacar/Plane-DUSt3R}

% Multiple-perspective images room layout reconstruction is poorly investigated due to the tedious and cumbersome steps that emerge from multi-view geometry, e.g camera intrinsic/extrinsic estimation, image matching, and triangulation. However, with the advancement of the current 3D foundation model DUSt3R, a paradigm shift has occurred in the 3D reconstruction realm, moving from a multi-step Structure-from-motion approach to an end-to-end single-step reconstruction without error accumulation and instability.
% To this end, this paper employs the 3D foundation model DUSt3R in the room layout reconstruction task, naming it \ours{}. As the name suggests, \ours{} incorporates the DUSt3R framework and plane representation, then fine-tunes on a room layout dataset (Structure3D). With the uniform and parsimonious results of \ours{}, we can obtain the final room layout with only a single post-processing step and 2D detection results. Compared to previous room layout reconstruction methods, it relaxes the setting from a single perspective/panorama image to multiple perspective images. Moreover, \ours{} solves the task in an end-to-end paradigm without cumbersome steps and accumulated errors. Finally, experiments show that our \ours{} not only surpasses the state-of-the-art on the synthesis dataset Structure3D but also demonstrates the robustness and performs well in real-world cases.
\end{abstract}

\section{INTRODUCTION}
3D room layout estimation aims to predict the overall spatial structure of indoor scenes, playing a crucial role in understanding 3D indoor scenes and supporting a wide range of applications. For example, room layouts could serve as a reference for aligning and connecting other objects in indoor environment reconstruction~\citep{nie2020total3dunderstanding}. Accurate layout estimation also aids robotic path planning and navigation by identifying passable areas~\citep{mirowski2016learning}. Additionally, room layouts are essential in tasks such as augmented reality (AR) where spatial understanding is critical. Therefore, 3D room layout estimation has attracted considerable research attention with continued development of datasets \citep{zheng2020structured3d,wang2022psmnet} and methods \citep{yang2022learning,stekovic2020general,wang2022psmnet} over the past few decades.

% empowering various applications such as augmented reality [\xd{cite}] and robot localization [\xd{cite}]. Man-made indoor environments exhibit unique structural characteristics and priors [\xd{cite}], such as orthogonal, parallel, and adjacent relationships among ceilings, floors, and walls. These properties allow 3D room layout estimation methods to directly derive 3D results by leveraging these particularities, thereby avoiding the tedious and complex steps of multi-view geometry reconstruction (\textit{\textit{\textit{e.g.}}}, camera intrinsic/extrinsic estimation, image matching, \textit{etc}.) [\xd{cite}]. For example, under the Manhattan assumption, methods like [\xd{cite}] can estimate 3D room layouts from a single perspective or panorama image. Over the past few decades, 3D room layout estimation has attracted considerable attention. The continued development of datasets [\xd{cite}] and methods [\xd{cite}] further drives progress in this area.



Methods for 3D room layout estimation \citep{zhang2015large,hedau2009recovering,yang2019dula} initially relied on the Manhattan assumption with a single perspective or panorama image as input. Over time, advancements \citep{stekovic2020general} have relaxed the Manhattan assumption to accommodate more complex settings, such as the Atlanta model, or even no geometric assumption at all. Recently, \cite{wang2022psmnet} introduced a ``multi-view'' approach, capturing a single room with two panorama images, marking the first attempt to extend the input from a single image to multiple images. 
Despite this progress, exploration in this direction remains limited, hindered by the lack of well-annotated multi-view 3D room layout estimation dataset.
% The challenges can be attributed to two main factors: 1) the lack of annotated multi-view 3D room layout estimation dataset, and 2) the inherent difficulties and challenges of multi-view 3D room layout estimation task.

Currently, multi-view datasets with layout annotations are very scarce. Even the few existing datasets, such as Structure3D \citep{zheng2020structured3d}, provide only a small number of perspective views (typically ranging from 2 to 5). This scarcity of observable views highlights a critical issue: wide-baseline sparse-view structure from motion (SfM) remains an open problem. Most contemporary multi-view methods \citep{wang2022psmnet,hu2022mvlayoutnet} assume known camera poses or start with noisy camera pose estimates. Therefore, solving wide-baseline sparse-view SfM would significantly advance the field of multi-view 3D room layout estimation. The recent development of large-scale training and improved model architecture offers a potential solution. While GPT-3 \citep{brown2020language} and Sora \citep{videoworldsimulators2024} have revolutionized NLP and video generation, DUSt3R \citep{wang2024dust3r} brings a paradigm shift for multi-view 3D reconstruction, transitioning from a multi-step SfM process to an end-to-end approach. DUSt3R demonstrates the ability to reconstruct scenes from unposed images, without camera intrinsic/extrinsic or even view overlap. For example, with two unposed, potentially non-overlapping views, DUSt3R could generate a 3D pointmap while inferring reasonable camera intrinsic and extrinsic, providing an ideal solution to the challenges posed by wide-baseline sparse-view SfM in multi-view 3D room layout estimation.

% Even more, it brings a paradigm shift for multi-view reconstruction problem from a multi-step SfM approach to an end-to-end single step reconstruction without error accumulation. In other words, with two-views input, DUSt3R could  

% In this paper, we employ DUSt3R to solve multi-view 3D room layout estimation task. DUSt3R uses pointmaps as its representation for scenes, preserving the mapping relationship from pixels to scene points. However, pointmaps lack explicit holistic structure information. To extract this information, we refer to single-view layout reconstruction methods. These methods reconstruct room layouts by detecting planes, estimating their 3D parameters, and inferring adjacency relationships between walls. They demonstrate the ability to predict plane parameters under occlusion. 

In this paper, we employ DUSt3R to tackle the multi-view 3D room layout estimation task. Most single-view layout estimation methods~\citep{yang2022learning} follow a two-step process: 1) extracting 2D \& 3D information, and 2) lifting the results to a 3D layout with layout priors. 
% For example, \cite{yang2022learning} extracts 2D line \& plane, and predicts 3D plane normal \& offsets from the input single view. Then, lifting the 2D plane to 3D layout with 3D information (3D plane normal \& offsets) and layout priors (assumption of one ceiling, one floor, and orthogonal to the rest walls). 
When extending this approach to multi-view settings, an additional step is required: establishing geometric primitive correspondence across multi-view before the 3D lifting step. 
% Hence, multi-view room layout estimation consists of 3 steps, 1) 2D geometry primitive detection, 2) multi-view geometry primitive correspondence, and 3) 3D lifting. 
Given the limited number of views in existing multi-view layout datasets, this correspondence-establishing step essentially becomes a sparse-view SfM problem. Hence, incorporating a single-view layout estimation method with DUSt3R to handle multi-view layout estimation is a natural approach. However, this may introduce a challenge: independent plane normal estimation for each image fails to leverage shared information across views, potentially reducing generalizability to unseen data in the wild. To this end, we adopt DUSt3R to solve correspondence establishement and 3D lifting simultaneously, 
which jointly predict plane normal and lift 2D detection results to 3D. Specifically, we modify DUSt3R to estimate room layouts directly through dense 3D point representation (pointmap), focusing exclusively on structural surfaces while ignoring occlusions. This is achieved by retraining DUSt3R with the objective to predict only structural planes, the resulting model is named \ours{}. However, dense pointmap representation is redundant for room layout, as a plane can be efficiently represented by its normal and offset rather than a large number of 3D points, which may consume significant space. 
To streamline the process, we leverage well-established off-the-shelf 2D plane detector to guide the extraction of plane parameters from the pointmap. We then apply post-processing to obtain plane correspondences across different images and derive their adjacency relationships.

Compared to existing room layout estimation methods, our approach introduces the first pipeline capable of unposed multi-view (perspective images) layout estimation. Our contributions can be summarized as follows:
\begin{enumerate}[leftmargin=*]
\item We propose an unposed multi-view (sparse-view) room layout estimation pipeline. To the best of our knowledge, this is the first attempt at addressing this natural yet underexplored setting in room layout estimation.
% We also give the analysis and definition of this task in Section 3.1 which could be a good starting point. 
\item The introduced pipeline consists of three parts: 1) a 2D plane detector, 2) a 3D information prediction and correspondence establishment method, \ours{}, and 3) a post-processing algorithm. The 2D detector was retrained with SOTA results on the Structure3D dataset (see Table~\ref{tab:2D_detect_compa}). The \ours{} achieves a $5.27\%$ and $5.33\%$ improvement in RRA and mAA metrics, respectively, for the multi-view correspondence task compared to state-of-the-art methods (see Table~\ref{tab:multiview_compa}). 
\item In this novel setting, we also design several baseline methods for comparison to validate the advantages of our pipeline. Specifically, we outperform the baselines by 4 projection 2D metrics and 1 3D metric respectively (see Table~\ref{tab:method-comparsion}). Furthermore, our pipeline not only performs well on the Structure3D dataset (see Figure~\ref{fig:qualitative}), but also generalizes effectively to in-the-wild datasets \citep{zhou2018stereo} and scenarios with different image styles such as cartoon style (see Figure~\ref{fig:teaser}).
\end{enumerate}

% Understanding indoor scenes is crucial for downstream applications like augmented reality and robotics. This task, though complex, benefits from the nature of man-made environments. These spaces often contain holistic structures combining geometric primitives such as lines and planes, orthogonal relationships, and connectivity. And room layout reconstruction is one of the key tasks for indoor scene understanding. Recent research on layout reconstruction has primarily focused on single-view inputs, which provide limited information about the overall scene. While some techniques leverage panoramic images or point clouds for more comprehensive information, acquiring these inputs often requires specialized equipment or time-consuming processes. Our goal is to reconstruct indoor layout from simple RGB images, allowing users to capture room structures with just a few photos. To achieve this, we propose an unconstrained sparse perspective images room layout reconstruction method using hybrid representations.


% Traditional multi-view 3D reconstruction pipelines like Structure from Motion (SfM)\citep{schoenberger2016sfm} and Multi-View Stereo (MVS)\citep{schoenberger2016mvs} involve solving complex sub-steps such as finding point correspondences, solving triangulations, and estimating camera parameters. Each step introduces errors that accumulate. These pipelines often fail in challenging scenarios with limited views or insufficient overlap areas. 
% However, a recent study, DUSt3R\citep{wang2024dust3r}, demonstrates the ability to reconstruct scenes from unconstrained images without camera pose, intrinsics, or even any overlap. DUSt3R effectively solves the unconstrained multi-view reconstruction problem. It uses pointmaps as its representation for scenes, preserving the mapping relationship from pixels to scene points. However, pointmaps lack explicit holistic structure information. To extract this information, we refer to single-view layout reconstruction methods. These methods reconstruct room layouts by detecting planes, estimating their 3D parameters, and inferring adjacency relationships between walls. They demonstrate the ability to predict plane parameters under occlusion. 

% Inspired by this, we adopt a step-by-step approach to extract planes. We modified DUSt3R to reconstruct room layouts directly as dense 3D points, focusing only on structural surfaces and ignoring occlusions. We achieve this by retraining DUSt3R using the depth of structural planes as our ground truth. We call this modified version \ours{}. While we now have dense 3D points representing the scene, this information is redundant, as we can simply represent a plane with its normal and offset instead of numerous 3D points that occupy large amounts of space. Since many mature plane detection methods exist, we utilize an off-the-shelf 2D plane detector to obtain plane detections, which guide the extraction of plane parameters from the pointmap. We then perform post-processing to obtain plane correspondences across different images and derive their adjacent relationships. We believe that the multi-view structural reconstruction problem can be divided into three sub-tasks: 1. detection (identifying walls, ceilings, and floors), 2. plane parameter estimation, and 3. correspondence. We consider the detection task to be more of a semantic problem that can be treated separately. The latter two tasks are fundamentally tied to 3D geometry and should be learned together, which are simultaneously solved by \ours{} with all the points aligned into the same coordinates. Thus, we have constructed a complete pipeline that represents the scene with both dense pointmaps and "primitives + relationships." To the best of our knowledge, this is the first pipeline that enables multi-view layout reconstruction on unposed RGB images.

% While it may seem intuitive to propose a pipeline that simply combines a single-view layout construction method with DUSt3R to achieve global layout reconstruction—where the single-view method handles sub-tasks 1 and 2, and DUSt3R provides poses for correspondences. This straightforward combination presents two significant challenges. First, estimating plane parameters independently for each image fails to leverage shared information across views, resulting in inconsistent outputs. We provide detailed information on this issue in our experiment \ref{sec:experiment-comparison}. We believe that joint learning across multiple views can improve performance by providing mutual constraints. Second, the generalizability of 3D plane parameter estimation is often limited, leading to poor reconstruction results on unseen data. In contrast, our method preserves the remarkable generalizability of DUSt3R.

% Our contributions for this paper can be summarized as follows:
% \begin{enumerate}[leftmargin=*]
% \item We establish the first pipeline for multi-view room layout reconstruction on sparse RGB images.
% \item We introduce a hybrid representation for scene understanding that integrates dense point-based information with sparse geometric primitives.
% \item Our trained model demonstrates robust performance on in-the-wild data.
% \end{enumerate}

\section{Related Work}
% \xd{squeeze to half}

\paragraph{Layout estimation.}
Most room layout estimation research focuses on single-perspective image inputs. \cite{stekovic2020general} formulates layout estimation as a constrained discrete optimization problem to identify 3D polygons. \cite{yang2022learning} introduces line-plane constraints and connectivity relations between planes for layout estimation, while \cite{sun2019horizonnet} formulates the task as predicting 1D layouts.  
Other studies, such as \cite{zou2018layoutnet}, propose to utilize monocular 360-degree panoramic images for more information.
% For example, \citep{zou2018layoutnet} propose a layout estimation algorithm that is suitable for both perspective and panoramic images. 
Several works extend the input setting from single panoramic to multi-view panoramic images, \textit{e.g.} \cite{wang2022psmnet} and \cite{hu2022mvlayoutnet}.
However, there is limited research addressing layout estimation from multi-view RGB perspective images. \cite{howard2019thinking} detects and regresses 3D piece-wise planar surfaces from a series of images and clusters them to obtain the final layout, but this method requires posed images. The most related work is \cite{jin2021planar}, which focuses on a different task: reconstructing indoor scenes with planar surfaces from wide-baseline, unposed images. It is limited to two views and requires an incremental stitching process to incorporate additional views.

\paragraph{Holistic scene understanding.}
Traditional 3D indoor reconstruction methods are widely applicable but often lack explicit semantic information. To address this limitation, recent research has increasingly focused on incorporating holistic scene structure information, enhancing scene understanding by improving reasoning about physical properties, mostly centered on single-perspective images. Several studies have explored the detection of 2D line segments using learning-based detectors~\citep{zhou2019end, pautrat2021sold2, dai2022fully}. However, these approaches often struggle to differentiate between texture-based lines and structural lines formed by intersecting planes. Some research has focused on planar reconstruction to capture higher-level information \citep{liu2018planenet,yu2019single,liu2019planercnn}. Certain studies \citep{huang2018holistic,nie2020total3dunderstanding,sun2021hohonet} have tackled multiple tasks alongside layout reconstruction, such as depth estimation, object detection, and semantic segmentation.
% For instance, \cite{huang2018holistic} introduces a parsed graph representation to capture the contextual relations between objects
% and room layout. \cite{nie2020total3dunderstanding} reconstructs room layouts alongside object bounding boxes and meshes.
% % , providing a more comprehensive understanding of indoor scenes.
% \cite{sun2021hohonet}  simultaneously addresses layout reconstruction, depth estimation, and semantic segmentation. 
Other works operate on constructed point maps; for instance, \cite{yue2023connecting}  reconstructs floor plans from density maps by predicting sequences of room corners to form polygons. SceneScript \citep{avetisyan2024scenescript} employs large language models to represent indoor scenes as structured language commands.

% However, few works focus on multi-view RGB perspective image structural reconstruction. \citep{jin2021planar}  presents the most relevant work, reconstructing indoor scenes with planar surfaces from sparse, unposed views. Nevertheless, it is limited to two views and requires incremental stitching for additional views. Their method often results in independent and incomplete planar segments due to occlusions. Our approach addresses these challenges by jointly processing multiple images to produce more complete layout planes with relational constraints.

\paragraph{Multi-view pose estimation and reconstruction.}
 The most widely applied pipeline for pose estimation and reconstruction on a series of images involves SfM \citep{schoenberger2016sfm} and MVS \citep{schoenberger2016mvs}, which typically includes steps such as feature mapping, finding correspondences, solving triangulations and optimizing camera parameters. Most mainstream methods build upon this paradigm with improvements on various aspects of the pipeline. However, recent works such as DUSt3R \citep{wang2024dust3r} and MASt3R \citep{leroy2024grounding} propose a reconstruction pipeline capable of producing globally-aligned pointmaps from unconstrained images. This is achieved by casting the reconstruction problem as a regression of pointmaps, significantly relaxing input requirements and establishing a simpler end-to-end paradigm for 3D reconstruction.


\section{METHOD}

\begin{figure}[h]
\includegraphics[width=\textwidth]{figs/pdf_pic_1.pdf}
\centering
\caption{Our multi-view room layout estimation pipeline. It consists of three parts: 1) a 2D plane detector $f_1$, 2) a 3D information prediction and correspondence establishment method \ours{} $f_2$, and 3) a post-processing algorithm $f_3$.
}
\vspace{-1.5em}
\label{fig:pipline}
\end{figure}

In this section, we formulate the layout estimation task, transitioning from a single-view to a multi-view scenario. We then derive our multi-view layout estimation pipeline as shown in Figure~\ref{fig:pipline} (Section~\ref{sec:formulation}). Our pipeline consists of three parts: a 2D plane detector $f_1$, a 3D information prediction and correspondence establishment method \ours{} $f_2$ (Section~\ref{sec:f2}), and a post-processing algorithm $f_3$ (Section~\ref{sec:f3}).

\subsection{Formulation of the Multi-View Layout Estimation Task}
\label{sec:formulation}

We begin by revisiting the single-view layout estimation task and unifying the formulation of existing methods. Next, we extend the formulation from single-view to multiple-view setting, providing a detailed analysis and discussion focusing on the choice of solutions. Before formulating the layout estimation task, we adopt the ``geometric primitives + relationships'' representation from \cite{zheng2020structured3d} to model the room layout. 

\textbf{Geometric Primitives.}
\begin{enumerate}[leftmargin=*]
    \item[-] \textbf{Planes:} The scene layout could be represented as a set of planes $\{\displaystyle \mP_1, \displaystyle \mP_2\, \ldots \}$ in 3D space and their corresponding 2D projections $\{p_1,p_2,\ldots\}$ in images. Each plane is parameterized by its normal $\displaystyle \vn \in \displaystyle \sS^2$ and offset $\displaystyle d$.  For a 3D point $\displaystyle \vx \in \displaystyle \R^3$ lying on the plane, we have $ \displaystyle \vn^T\displaystyle \vx+\displaystyle d = 0$. 

    \item[-] \textbf{Lines \& Junction Points:} In 3D space, two planes intersect at a 3D line, three planes intersect at a 3D junction point. We denote the set of all 3D lines/junction points in the scene as $\{ \mL_1, \mL_2\, \ldots \}$/$\{ \mJ_1, \mJ_2\, \ldots \}$ and their corresponding 2D projections as $\{l_1,l_2,\ldots\}$/$\{j_1,j_2,\ldots\}$ in images.
\end{enumerate}
\textbf{Relationships.}
\begin{enumerate}[leftmargin=*]
    \item[-] \textbf{Plane/Line relationships:} An adjacent matrix $ \mW_{p}/\mW_{l} \in \{0,1\}$ is used to model the relationship between planes/lines. Specifically, $ \mW_{p}(i,j)=1$ if and only if  $ \mP_i$ and $ \mP_j$ intersect along a line; otherwise, $ \mW_{p}(i,j)=0$. Similarly to plane relationship, $\mW_{l}(i,j)=1$ if and only if $ \mL_i$ and $ \mL_j$ intersect at a certain junction, otherwise, $\mW_{l}(i,j)=0$. 
    
\end{enumerate}

The pipeline of single-view layout estimation methods \citep{liu2019planercnn,yang2022learning,liu2018planenet,stekovic2020general} can be formulated as:
\begin{equation}
\begin{aligned}
\mathcal{I} \xrightarrow{\hspace{2mm} f_1 \hspace{2mm}} \{2D,3D\} \xrightarrow{\hspace{2mm} f_3 \hspace{2mm}} \{\bm P,\bm L,\bm J,\bm W\},
\label{eqn:formula1}
\end{aligned}
\end{equation}

where $f_1$ is a function that predicts 2D and 3D information from the input single view. Generally speaking, the final layout result $\{\bm P,\bm L,\bm J,\bm W\}$ can be directly inferred from the outputs of  $f_1$. However, errors arising from $f_1$ usually adversely affect the results. Hence, a refinement step that utilizes prior information about room layout is employed to further improve the performance. Therefore, $f_3$ typically encompasses post-processing and refinement steps where the post-processing step generates an initial layout estimation, and the refinement step improves the final results. 

For instance, \cite{yang2022learning} chooses the HRnet network~\citep{wang2020deep} as $f_1$ backbone to extract 2D plane $p$, line $l$, and predict 3D plane normal $\bm n$ and offset $d$ from the input single view. After obtaining the initial 3D layout from the outputs of $f_1$, the method reprojects the 3D line to a 2D line $\hat{l}$ on the image and compares it with the detected line $l$ from $f_1$. $f_3$ minimizes the error $\|\hat{l}-l\|_2^2$ to optimize the 3D plane normal. In other words, it uses the better-detected 2D line to improve the estimated 3D plane normal. In contrast, \cite{stekovic2020general} uses a different approach: its $f_1$ predicts a 2.5D depth map instead of a 2D line $l$ and uses the more accurate depth results to refine the estimated 3D plane normal. 
% Additionally, \cite{liu2019planercnn,liu2018planenet} follow equation (\ref{eqn:formula1}) but predict different 2D information and different refinement strategies. 
Among the works that follow the general framework of~\ref{eqn:formula1}~\citep{liu2019planercnn,liu2018planenet}, \cite{yang2022learning} stands out as the best single-view perspective image layout estimation method without relying on the Manhattan assumption. Therefore, we present its formulation in equation (\ref{eqn:formula11}) and extend it to multi-view scenarios.
\begin{equation}
\begin{aligned}
\mathcal{I} \xrightarrow{\hspace{2mm} f_1 \hspace{2mm}} \{p, l, \bm n, d\} \xrightarrow{\hspace{2mm} f_3 \hspace{2mm}} \{\bm P,\bm L,\bm J,\bm W\},
\label{eqn:formula11}
\end{aligned}
\end{equation}

% Our framework, depicted in Figure \ref{fig:pipline}, consists of a dense reconstruction module $\mathcal{F}$, a 2D detector $\mathcal{D}$, and a primitive extraction module $\mathcal{P}$.


In room layout estimation from unposed multi-view images, two primary challenges aris: 1) camera pose estimation, and 2) 3D information estimation from multi-view inputs. Camera pose estimation is particularly problematic given the scarcity of annotated multi-view layout dataset.
% Even the few existing multi-view datasets only have a limited number of views (\textit{\textit{e.g.}} Structure3D \cite{zheng2020structured3d} has views ranging from 2 to 5). This leads it to a critical issue: wilde-baseline sparse view camera pose estimation which is still an open problem.
Thanks to the recent advancements in 3D vision with pretrain model, this challenge could be effectively bypassed: DUSt3R \citep{wang2024dust3r} has demonstrated the ability to reconstruct scenes from unposed images without requiring camera intrinsic or extrinsic, and even without overlap between views.
% For example, with two unposed or even no overlap views input, DUSt3R could give the 3D pointmap output with reasonable camera intrinsic and extrinsic which is a perfect solution to address the pain point of wide-baseline sparse-view camera pose estimation. 
Moreover, the 3D pointmap generated from DUSt3R can provide significantly improved 3D information, such as plane normal and offset, compared to single-view methods \citep{yang2022learning} (see Table~\ref{tab:method-comparsion} of experiment section). Therefore, DUSt3R represents a critical advancement in extending single-view layout estimation to multi-view scenarios. Before formulating the multi-view solution, we first present the key 3D representation of DUSt3R: the pointmap $\mX$ and the camera pose $\mT$. The camera pose $\mT$ is obtained through global alignment, as described in the DUSt3R \citep{wang2024dust3r}).



% In this work, we introduce a hybrid representation for holistic 3D scene understanding, combining both dense and sparse scene representations.  Inspired by DUSt3R\citep{wang2024dust3r}, we adopt 3D pointmap as the dense scene representation. 

% \begin{figure}
%      \centering
%      \begin{subfigure}[b]{0.23\textwidth}
%          \centering
%          \includegraphics[width=\textwidth]{figs/representation/pointmap.jpeg}
%          \caption{}
%          \label{fig:ori}
%      \end{subfigure}
%      \hfill
%      \begin{subfigure}[b]{0.23\textwidth}
%          \centering
%          \includegraphics[width=\textwidth]{figs/representation/line_jun.jpeg}
%          \caption{}
%      \end{subfigure}
%      \hfill
%      \begin{subfigure}[b]{0.23\textwidth}
%          \centering
%          \includegraphics[width=\textwidth]{figs/representation/plane.jpeg}
%          \caption{}
%      \end{subfigure}
%      \hfill
%      \begin{subfigure}[b]{0.23\textwidth}
%          \centering
%          \includegraphics[width=\textwidth]{figs/representation/relationship.jpeg}
%          \caption{}
%      \end{subfigure}
%         \caption{}
%         \label{representation}
% \end{figure}

% \textbf{Dense Mapping.} 
\begin{enumerate}[leftmargin=*]
    \item[-] \textbf{Pointmap $\mX$:}  Given a set of RGB images $\{ \displaystyle \mathcal{I}_{1}, \ldots, \displaystyle \mathcal{I}_{n}\} \in \displaystyle \R^{H\times W \times3} $, captured from distinct viewpoints of the same indoor scene, we associate each image $\displaystyle \mathcal{I}_{i}$ with a canonical pointmap $\displaystyle \mX_i  \in \displaystyle \R^{H\times W \times3}$. The pointmap represents a one-to-one mapping from each pixel $(u,v)$ in the image to a corresponding 3D point in the world coordinate frame: $(u, v)\in \R^2 \mapsto \displaystyle \mX(u,v) \in \R^3$.
    % $\displaystyle \mX:\displaystyle \R^2 \mapsto \displaystyle \R^3;(u, v) \mapsto \displaystyle \mX(u,v) $.
    
    \item[-] \textbf{Camera Pose $\mT$:} Each image $\displaystyle \mathcal{I}_i $ is associated with a camera-to-world pose $\displaystyle \mT_i\in SE(3).$
\end{enumerate}

Now, the sparse-view layout estimation problem can be formulated as shown in equation (\ref{eqn:formula2}) 
\begin{equation}
\begin{aligned}
\{\mathcal{I}_1,\mathcal{I}_2,\ldots\} \xrightarrow{\hspace{2mm} f_1,f_2 \hspace{2mm}} \{p,l,\bm X,\bm T\} \xrightarrow{\hspace{2mm} f_3 \hspace{2mm}} \{\bm P,\bm L,\bm J,\bm W\}.
\label{eqn:formula2}
\end{aligned}
\end{equation}

In this work, we adopt the HRnet backbone from \cite{yang2022learning} as $f_1$. 
In the original DUSt3R~\citep{wang2024dust3r} formulation, the ground truth pointmap $\displaystyle \mX^{obj}$ represents the 3D coordinates of the entire indoor scene. In contrast, we are interested in plane pointmap $\displaystyle \mX^p$ that represents the 3D coordinates of structural plane surfaces, including walls, floors, and ceilings. This formulation intentionally disregards occlusions caused by non-structural elements, such as furniture within the room. Our objective is to predict the scene layout pointmap without occlusions from objects, even when the input images contain occluding elements.  For simplicity, any subsequent reference to $\displaystyle \mX$ in this paper refers to the newly defined plane pointmap $\displaystyle \mX^p$. We introduce \ours{} as $f_2$ and directly infer the final layout via $f_3$ without the need for any refinement.  



% While the pointmap preserves detailed geometric information and maintains dense image-to-3D mapping relationships, it's memory-intensive and contains a certain level of noise.
% So the pointmap only serves as an intermediate representation. We further extract the “primitive + relationship” \citep{zheng2020structured3d} representation upon plane pointmap $\displaystyle \mX^p$ as the final sparse scene representation. We consider three fundamental geometric primitives and two key relationships between them.



% \textbf{Geometric Primitives.}
% \begin{enumerate}[leftmargin=*]
%     \item[-] \textbf{Planes:} The scene layout could be represented as a set of planes $\{ \displaystyle \mP_1, \displaystyle \mP_2\, \ldots \}$. Each plane is parameterized by its normal $\displaystyle \vn \in \displaystyle \sS^2$ and offset $\displaystyle d$.  For a 3D point $\displaystyle \vx \in \displaystyle \R^3$ lying on the plane, we have $ \displaystyle \vn^T\displaystyle \vx+\displaystyle d = 0$.

%     \item[-] \textbf{Lines:} The intersection of two planes in 3D space generates a 3D line. We denote the set of all 3D lines in the scene as  $\{ \displaystyle \mL_1, \displaystyle \mL_2\, \ldots \}$.

%     \item[-] \textbf{Junction Points:}
%     The intersection of two lines in 3D space generates a 3D junction point.  We denote the set of all 3D junction points in the scene as  $\{ \displaystyle \mJ_1, \displaystyle \mJ_2\, \ldots \}$.
% \end{enumerate}
% \textbf{Relationships.}
% \begin{enumerate}[leftmargin=*]
%     \item[-] \textbf{Plane relationships:} An adjacent matrix $\displaystyle \mW_{p} \in \{0,1\}$ is adapted to model the relationship between planes. Specifically, if and only if  $\displaystyle \mP_i$ and $\displaystyle \mP_j$ intersect at a certain line, $\displaystyle \mW_{p}(i,j)=1$, and $\displaystyle \mW_{p}(i,j)=0$ otherwise.
    
%     \item[-] \textbf{Line relationships:} An adjacent matrix $\displaystyle \mW_{l} \in \{0,1\}$ is adapted to model the relationship between lines. Specifically, if and only if $\displaystyle \mL_i$ and $\displaystyle \mL_j$ intersect at a certain junction, $\displaystyle \mW_{l}(i,j)=1$, and $\displaystyle \mW_{l}(i,j)=0$ otherwise.
% \end{enumerate}

% Our framework, depicted in Figure \ref{fig:pipline}, consists of a dense reconstruction module $\mathcal{F}$, a 2D detector $\mathcal{D}$, and a primitive extraction module $\mathcal{P}$.



% \begin{equation}
% \begin{aligned}
% \mathcal{I} & \xrightarrow{\hspace{2mm} f_1\hspace{2mm}}  \{X,T\} \\
% \mathcal{I} & \xrightarrow{\hspace{2mm} f_2\hspace{2mm}}  \{p,l\} \\
% \{X,T\},  \{p,l\} & \xrightarrow{\hspace{2mm} f_3 \hspace{2mm}} \{P,L,J,W\}. \\
% \end{aligned}
% \label{eqn:formula1}
% \end{equation}

% \begin{equation}
% \begin{aligned}
% \mathcal{I} \xrightarrow{\hspace{2mm} f_0 \hspace{2mm}} \{X^{obj}\}
% \label{eqn:formula2}
% \end{aligned}
% \end{equation}

% \begin{equation}
% \begin{aligned}
% \mathcal{I} \xrightarrow{\hspace{2mm} f_1,f_2 \hspace{2mm}} \{\bm X,\bm T,p,l\} \xrightarrow{\hspace{2mm} f_3 \hspace{2mm}} \{\bm P,\bm L,\bm J,\bm W\}
% \label{eqn:formula2}
% \end{aligned}
% \end{equation}


\subsection{$f_2$: Plane-based DUSt3R}
\label{sec:f2}
% To obtain global plane pointmap $\displaystyle \mX$, we follow the work of DUSt3R \citep{wang2024dust3r}. The entire process can be divided into two parts. First, the model estimates pointmaps and confidence maps for each pair of images. Second, post-processing is performed to jointly optimize all the estimated point maps, obtaining global point maps expressed in the world coordinate system.

The original DUSt3R outputs pointmaps that capture all 3D information in a scene, including furniture, wall decorations, and other objects. However, such excessive information introduces interference when extracting geometric primitives for layout prediction, such as planes and lines.
% Moreover, the plane is the most important primitive. 
To obtain a structural plane pointmap $\mX$, we modify the data labels from the original depth map (Figure~\ref{fig:depth} (a)) to the \textbf{structural plane depth map} (Figure~\ref{fig:depth} (b)), and then retrain the DUSt3R model. This updated objective guides DUSt3R to predict the pointmap of the planes while ignoring other objects. The original DUSt3R does not guarantee output at a metric scale, so we also trained a modified version of \ours{} that produces \textbf{metric-scale} results. 
%\xd{Though MASt3R\citep{leroy2024grounding}, an improved version of DUSt3R, exists, we chose to fine-tune our model based on DUSt3R instead. This decision was made because MASt3R enhances DUSt3R by introducing an image matching task, designed to establish pixel-to-pixel correspondences across different images of the same scene. However, our goal is to predict room layouts while disregarding non-structural elements. We aim to estimate planes behind occluded areas and obtain correspondences between planes, not image pixels.}

% \subsubsection{Pairwise pointmaps retrival}


\begin{figure}[h]
\includegraphics[width=0.99\textwidth]{figs/arch_2.png}
\centering
\caption{\ours{} architecture remains identical to DUSt3R. The transformer decoder and regression head are further fine-tuned on the occlusion-free depth map (see Figure~\ref{fig:depth}).}
\label{dust3r-model}
\vspace{-0.5cm}
\end{figure}


Given a set of image pairs $\sP = \{ (\displaystyle \mathcal{I}_{i}, \mathcal{I}_{j}) \mid i \neq j, 1 \leq i,j \leq n, \mathcal{I} \in \R^{H\times W \times3}\}$, for each image pair, the model comprises two parallel branches. As shown in Figure ~\ref{dust3r-model}, the detail of the architecture can be found in Appendix~\ref{app:dust3r}.
% As shown in Figure ~\ref{dust3r-model}, each branch is responsible for processing one image. The two images are first encoded in a Siamese manner with weight-sharing ViT encoder~\citep{dosovitskiy2020image} producing two latent features $ \mF_1, \mF_2$. These features are then passed thorough two identical decoders, which continuously share information via cross-attention mechanisms, enabling the model to learn the relative geometric relationships between the two images. Finally, the DPT~\citep{ranftl2021vision} head regresses the pointmap $\mX \in \R^{H\times W \times 3}$ and the confidence map  $C \in \R^{H \times W}$, based on the concatenated features from different layers of the decoder tokens. 
The regression loss function is defined as the scale-invariant Euclidean distance between the normalized predicted and ground-truth pointmaps: $l_{regr}(v,i) = \|\frac{1}{z}\displaystyle \mX_i^{v}-\frac{1}{\bar{z}}\bar{\displaystyle \mX}_i^{v} \|_2^2$, where view $v \in \{1,2\}$ and $i$ is the pixel index. The scaling factors $z$ and $\bar{z}$ represent the average distance of all corresponding valid points to the origin. In addition, by incorporating the confidence loss, the model implicitly learns to identify regions that are more challenging to predict. As in DUSt3R \citep{wang2024dust3r}, the confidence loss is defined as: $\mathcal{L}_{\mathrm{conf}}=\sum_{v\in\{1,2\}}\sum_{i\in\mathcal{D}^v}C_i^{\upsilon,1}\ell_{\mathrm{regr}}(v,i)-\alpha\log C_i^{\upsilon,1}$, where $\mathcal{D}^v\subseteq\{1,\dots, H\}\times\{1, \dots, W\}$ are sets of valid pixels on which the ground truth is defined.

\paragraph{Structural plane depth map.} The Structure3D dataset provides ground truth plane normal and offset, allowing us to re-render the plane depth map at the same camera pose (as shown in Figure~\ref{fig:depth}).  We then transform the structural plane depth map $D^p$ to pointmap $\mX^v$ in the camera coordinate frame $v$. This transformation is given by $ \mX^{v}_{i,j} =  \mK^{-1}[i \mD_{i,j}^p, j \mD_{i,j}^p, \mD_{i,j}^p]^\top$, where $ \mK \in \R^{3\times 3}$ is the camera intrinsic matrix. Further details of this transformation can be found in \cite{wang2024dust3r}.


\paragraph{Metric-scale.} In the multi-view setting, scale variance is required, which differs from DUSt3R. To accommodate this, we modify the regression loss to bypass normalization for the predicted pointmaps when the ground-truth pointmaps are metric. Specifically, we set $z:=\bar{z}$ whenever the ground-truth is metric, resulting in the following loss function $l_{regr}(v,i) = \| \mX_i^{v}-\bar{ \mX}_i^{v} \|_2^2/\bar{z}$.

% The original DUSt3R couldn't guarantee output at a metric scale, so we also trained a modified version of \ours{} that produces metric-scale results. The key change we made was setting $z:=\bar{z}$.

% \begin{equation}
%     l_{regr}(v,i) = \|\frac{1}{z}\displaystyle \mX_i^{v}-\frac{1}{\bar{z}}\bar{\displaystyle \mX}_i^{v} \|_2^2
% \end{equation}

% where view $v \in \{1,2\}$ and $i$ is the pixel index. The scaling factors $z$ and $\bar{z}$ represent the average distance of all corresponding valid points to the origin. 
% However, the original DUSt3R couldn't guarantee output at a metric scale, so we make a change $z:=\bar{z}$ which was introduced in MASt3R \citep{leroy2024grounding} to eliminate the metric scale problem.
% % The original DUSt3R couldn't guarantee output at a metric scale, so we also trained a modified version of \ours{} that produces metric-scale results. The key change we made was setting $z:=\bar{z}$.
% Finally, by introducing the regression loss in confidence loss, the model could implicitly learn how to identify regions that are more challenging to predict compared to others. Same as in DUSt3R \citep{wang2024dust3r}:

% \begin{equation}
%  \mathcal{L}_{\mathrm{conf}}=\sum_{v\in\{1,2\}}\sum_{i\in\mathcal{D}^v}C_i^{\upsilon,1}\ell_{\mathrm{regr}}(v,i)-\alpha\log C_i^{\upsilon,1}
% \end{equation}

% To obtain the ground-truth pointmaps $\displaystyle \mX^{v,1}$ , we first transform the ground truth depthmap $\displaystyle \mD \in \R^{H\times W}$ into a pointmap $\displaystyle \mX^{v}$ express in the camera coordinate of $v$ by $\displaystyle \mX^{v}_{i,j} = \displaystyle \mK^{-1}[i\displaystyle \mD_{i,j}, j\displaystyle \mD_{i,j},\displaystyle \mD_{i,j}]^\top$ with camera intrinsic matrix $\displaystyle \mK \in \R^{3\times 3}$. Then we obtain $\displaystyle \mX^{v,1}$ by $\displaystyle \mX^{v,1} = \displaystyle \mT_1^{-1}\displaystyle \mT_v\displaystyle h(\mX^v)$ with $\displaystyle \mT_1, \displaystyle \mT_v \in \R^{3\times 4}$ the camera-to-world poses and $h$ being the homogeneous transformation.
% \textit{The biggest difference of our method and DUSt3R, is that we used the depth of plane as the ground truth depthmap $\displaystyle \mD$ instead of the normal depth, as shown in Fig \ref{fig:depth}.}

% move to appendix
% Given a set of RGB images 
% $\{ \displaystyle \mI_{1}, \displaystyle \mI_{2}, \ldots, \displaystyle \mI_{n}\} \in \displaystyle \R^{H\times W \times3} $, we first pair them to create a set of image pairs $\displaystyle \sP = \{ (\displaystyle \mI_{i}, \displaystyle \mI_{j}) \mid i \neq j, 1 \leq i,j \leq n\}$. For each image pair $(\displaystyle \mI_{i}, \displaystyle \mI_{j}) \in \displaystyle \sP$, the model estimates two point maps $\displaystyle \mX_{i,i}, \displaystyle \mX_{j,i}$, along with their corresponding confidence maps $\displaystyle \mC_{i,i}, \displaystyle \mC_{j,i}$. Specifically, both pointmaps are expressed in the camera coordinate system of $\displaystyle \mI_{i}$, which implicitly accomplishes dense 3D reconstruction.

% The model consists of two parallel branches, as shown in Fig ~\ref{dust3r-model}, each branch responsible for processing one image. The two images are first encoded in a Siamese manner with weight-sharing ViT encoder\citep{dosovitskiy2020image} to produce two latent features $\displaystyle \mF_1, \displaystyle \mF_2$: $\displaystyle \mF_i = \operatorname{Encoder}(\displaystyle \mI_i)$.
% Next, $\displaystyle \mF_1, \displaystyle \mF_2$ are fed into two identical decoders that continuously share information through cross-attention mechanisms. By leveraging cross-attention mechanisms, the model is able to learn the relative geometric relationships between the two images. Specifically, for each encoder block:
% \begin{equation} 
%     \begin{aligned}
%         \displaystyle \mG_{1,i} = \operatorname{DecoderBlock}_{1,i}(\displaystyle \mG_{1,i-1},\displaystyle \mG_{2,i-1}),  \\
%         \displaystyle \mG_{2,i} = \operatorname{DecoderBlock}_{2,i}(\displaystyle \mG_{1,i-1},\displaystyle \mG_{2,i-1})
%     \end{aligned}
% \end{equation}
% where $\mG_{1,0}:= \displaystyle \mF_1, \mG_{2,0}:= \displaystyle \mF_2$.
% Finally, the DPT\citep{ranftl2021vision} head regresses the pointmap and confidence map from the concatenated features of different layers of the decoder tokens:
% \begin{equation}
%     \begin{aligned}
%         \displaystyle \mX_{1,1}, \displaystyle \mC_{1,1} = \operatorname{Head}_1(\displaystyle\mG_{1,0}, \displaystyle\mG_{1,1},\dots, \displaystyle\mG_{1,B}) \\
%         \displaystyle \mX_{2,1}, \displaystyle \mC_{2,1} = \operatorname{Head}_2(\displaystyle\mG_{2,0}, \displaystyle\mG_{2,1},\dots,\displaystyle\mG_{2,B} )
%     \end{aligned}
% \end{equation}
% where $B$ is the number of decoder blocks.
% The regression loss function is defined as the scale-invariant Euclidean distance between the normalized predicted and ground-truth pointmaps:
% \begin{equation}
%     l_{regr}(v,i) = \|\frac{1}{z}\displaystyle \mX_i^{v,1}-\frac{1}{\bar{z}}\bar{\displaystyle \mX}_i^{v,1} \|_2^2
% \end{equation}
% where $v \in \{1,2\}$ and $i$ is the pixel index. The scaling factors $z$ and $\bar{z}$ represent the average distance of all corresponding valid points to the origin. The original DUSt3R couldn't guarantee output at a metric scale, so we also trained a modified version of \ours{} that produces metric-scale results. The key change we made was setting $z:=\bar{z}$.
% By introducing the regression loss in confidence loss, the model could implicitly learn how to identify regions that are more challenging to predict compared to others. Same as in DUSt3R \citep{wang2024dust3r}:
% \begin{equation}
%  \mathcal{L}_{\mathrm{conf}}=\sum_{v\in\{1,2\}}\sum_{i\in\mathcal{D}^v}C_i^{\upsilon,1}\ell_{\mathrm{regr}}(v,i)-\alpha\log C_i^{\upsilon,1}
% \end{equation}
% To obtain the ground-truth pointmaps $\displaystyle \mX^{v,1}$ , we first transform the ground truth depthmap $\displaystyle \mD \in \R^{H\times W}$ into a pointmap $\displaystyle \mX^{v}$ express in the camera coordinate of $v$ by $\displaystyle \mX^{v}_{i,j} = \displaystyle \mK^{-1}[i\displaystyle \mD_{i,j}, j\displaystyle \mD_{i,j},\displaystyle \mD_{i,j}]^\top$ with camera intrinsic matrix $\displaystyle \mK \in \R^{3\times 3}$. Then we obtain $\displaystyle \mX^{v,1}$ by $\displaystyle \mX^{v,1} = \displaystyle \mT_1^{-1}\displaystyle \mT_v\displaystyle h(\mX^v)$ with $\displaystyle \mT_1, \displaystyle \mT_v \in \R^{3\times 4}$ the camera-to-world poses and $h$ being the homogeneous transformation.
% \textit{The biggest difference of our method and DUSt3R, is that we used the depth of plane as the ground truth depthmap $\displaystyle \mD$ instead of the normal depth, as shown in Fig \ref{fig:depth}.}


\begin{figure}[h]
     \centering
     \begin{subfigure}[b]{0.4\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/depth_ori.png}
         \caption{The original DUSt3R depth map.}
         
     \end{subfigure}
     % \hfill
     \begin{subfigure}[b]{0.4\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/depth_plane.png}
         \caption{The \ours{} depth map.}
     \end{subfigure}
    \caption{The (a) original DUSt3R depth map and (b) occlusion removed depth map.}
    \vspace{-1em}
    \label{fig:depth}
\end{figure}


% \subsection{2D Plane and Line Detection}
% Now that we have established a one-to-one mapping from each pixel to the corresponding 3D point, we can leverage off-the-shelf 2D plane detection methods to facilitate the detection and localization of individual planes in the 3D space. Specifically, we adopt NonCuboidRoom~\citep{yang2022learning} as the plane detector. Given a RGB image $\displaystyle \mI_{i}$, the network could detect the vertical lines formed by the intersection of two adjacent walls and 2D planes.  A line $l_j$ is represented by its angle $\theta_j$ of the line inclination and point  $\displaystyle \vc_j = ( x_j,y_j)$ lying on the line, i.e., $x \cos\theta + y \sin\theta +b = 0$, where $b=x_j \cos\theta + y_j \sin\theta$.
%  Each 2D plane $p_i$ is represented in bounding box format with its center position $\displaystyle \vc_i = ( x_i,y_i)$ and size $\displaystyle \vs_i = (w_i,h_i)$. And classified as one of the following: wall, ceiling, or floor.  We sort all detected walls in ascending order based on the x-coordinates of their plane centers in image space. Then, we calculate potential intersection regions, by comparing the bounding boxes of adjacent walls in image space. Defined as the left boundary and right boudary$(x^l, x^r)$, where $x^l<x^r$.  These intersection regions will be used in the following section to determine whether two walls that appear adjacent in the image are physically connected in 3D space. We refer readers to \citep{yang2022learning} for more details on how to calculate the intersection regions.



\subsection{$f_3$: Post-Processing}
\label{sec:f3}

In this section, we introduce how to combine the multi-view plane pointmaps $\mX$ and 2D detection results  $p,l$ to derive the final layout $\{\bm P,\bm L,\bm J,\bm W\}$. For each single view $\mathcal{I}_{i}$, we can infer a partial layout result $\{\Tilde{\mP}_i,\Tilde{\mL}_i,\Tilde{\mJ}_i,\Tilde{\mW}_i\}=g_1(\mX_i,p^i,l^i)$ from the single view pointmaps $\mX_i$ and 2D detection results $p^i,l^i$ through a post-process algorithm $g_1$ in camera coordinate. Then, a correspondence-establish and merging algorithm $g_2$ combines all partial results to get the final layout $\{\bm P,\bm L,\bm J,\bm W\}=g_2(\{\Tilde{\mP}_1,\Tilde{\mL}_1,\Tilde{\mJ}_1,\Tilde{\mW}_1\},\ldots)$.  

% In this section, we introduce how to combine the multi-view plane pointmaps $\mX$ and 2D detection results  $p,l$ to derive the final layout $\{\bm P,\bm L,\bm J,\bm W\}$. We begin with single-view layout estimation and then extend the approach to a multi-view setting. For single-view layout estimation, we build on the NonCuboid~\citep{yang2022learning} approach, modelling rooms as a combination of one ceiling, one floor, and multiple vertical walls. Once the relationships between walls are established, we can combine them with the floor and ceiling to generate the full room layout. Thus, for the single-view case, we focus on two key challenges: determining the 3D parameters of each plane and identifying whether two adjacent walls are physically connected. As for multi-view reconstruction, our main task is to establish correspondences between planes across different images. We aim to assign each plane a global ID, ensuring that planes from different images representing the same real-world surface share the same ID.

% Following NonCuboid\citep{yang2022learning}, we assume that the room layout consists of a single ceiling, a single floor, and several adjacent vertical walls. Based on this assumption, we can infer that non-adjacent walls in the image must be physically separate in 3D space. So we only need to focus on determining whether adjacent walls connect. By combining the wall relationships with the known floor and ceiling, we can generate the room layout. Since we are working on multi-view room layout reconstruction problem, it is essential to establish correspondences between planes across different images. Our goal is to assign a global ID to each plane, ensuring that planes from different images belonging to the same real-world plane share the same global ID. We will first start by inferring the 3D room layout for each single image, then merge them together to obtain the complete room layout.

\paragraph{Single-view room layout estimation $g_1$.} For an image $\mathcal{I}_{i}$, $g_1$ mainly addresses two tasks: 1) lifting 2D planes to 3D camera coordinate space with 3D normal from pointmap $\mX_i$, and 2) inferring the wall adjacency relationship. We follow the post-processing procedure in \cite{yang2022learning} but with two improvements. First, the plane normal $\bm n$ and offset $d$ are inferred from $\mX_i$ instead of directly regressed by network $f_1$. The points from $\mX_i$ which belong to same plane are used to calculate $\bm n$ and $d$. Second, with the better 3D information pointmap $\mX_i$ we can better infer pseudo wall adjacency through the depth consistency of inferred plane intersection $\mL$ (inferred from 2D plane $p$) and predicted line region $\mL$ (extracted from the region of $\mX_i$). In our experiments, the depth consistency tolerance $\epsilon_1$ is set to 0.005.  

% For an image $\mathcal{I}_{i}$, we have the plane pointmap $\mX_{i}$, from which we can estimate the normal and offset for each point in $\displaystyle \mX_i$. Since the pointmap represents only the structural planar surfaces in the scene, each point should theoretically belong to a single plane and share its normal and offset. Given that each bounding box corresponds to a distinct detected plane, we can determine the plane's normal and offset by assuming that the 3D point at the center of the 2D bounding box lies on that plane. 
% Following NonCuboid~\citep{yang2022learning}, we define a potential intersection line region for each pair of adjacent walls in the image space. To determine the connectivity between two adjacent walls, we first calculate the 3D line $L$ formed by their intersection. Let $\displaystyle \sP$ be the set of 3D points mapped from the intersection bounding box using $\displaystyle \mX^p_i$. We then compute the minimum distance $d$ between $L$ and each point in $\displaystyle \sP$. If $d$ is smaller than a predefined threshold $\epsilon_1$, we conclude that the two walls are physically connected in 3D space. In our experiments,
% $\epsilon_1$ is set to 0.005. For cases where the walls are not physically connected, we introduce an additional virtual plane defined by the camera center and the occlusion line, to ensure all walls are physically connected in 3D space. 

\paragraph{Multi-view room layout estimation $g_2$.} 
\label{sec:mv-reconstruction}
\begin{figure}
     \centering
     \begin{subfigure}[b]{0.21\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/bird-view/ori.png}
         \caption{Projected Lines}
         \label{fig:ori}
     \end{subfigure}
     % \hfill
     \begin{subfigure}[b]{0.21\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/bird-view/rotated.png}
         \caption{Rotated Lines}
         \label{fig:rotated}
     \end{subfigure}
     % \hfill
     \begin{subfigure}[b]{0.21\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/bird-view/calibrated.png}
         \caption{Aligned Lines}
         \label{fig:calibrated}
     \end{subfigure}
     % \hfill
     \begin{subfigure}[b]{0.21\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/bird-view/layout.png}
         \caption{Correspondance}
         \label{fig:layout}
     \end{subfigure}
        \caption{(a) Planes are projected onto the x-z plane as 2D line segments. (b) The scene is rotated so that line segments are approximately horizontal or vertical. (c) Line segments are classified and aligned to be either horizontal or vertical. (d) Merged planes are shown, with segments belonging to the same plane indicated by the same color and index.}
        \label{PlaneMerge}
\vspace{-1.6em}
\end{figure}

% The goal of multi-view layout estimation is similar to that of single-view: we need to estimate 3D parameters for each plane and determine the relationships between adjacent planes. However, in a multi-view setting, we must ensure that each plane represents a unique physical plane in 3D space. The main challenge in multi-view reconstruction is that the same physical plane may appear in multiple images, causing duplication. Our task is to identify which planes correspond to the same physical plane across different images and merge them, keeping only one representation for each unique plane. 

Based on the results of $g_1$, $g_2$ uses the global alignment of DUSt3R (refer to appendix~\ref{app:dust3r}) to get the camera pose $\mT_i$ for each image $\mathcal{I}_{i}$. Then, we can transform all partial layout results $(\{\Tilde{\mP}_1,\Tilde{\mL}_1,\Tilde{\mJ}_1,\Tilde{\mW}_1\},\ldots)$ to the same coordinate space. In this coordinate space, we establish correspondence for each plane, then merge and assign a unique ID to them. 

Since we allow at most one floor and one ceiling detection per image,  we simply average the parameters from all images to obtain the final floor and ceiling parameters. As for walls, we assume all walls are perpendicular to both the floor and ceiling. To simplify the merging process, we project all walls onto the x-z plane defined by the floor and ceiling. This projection reduces the problem to a 2D space.
making it easier to identify and merge corresponding walls.
Figure \ref{PlaneMerge} illustrates the entire process of merging walls. Each wall in an image is denoted as one line segment, as shown in Figure \ref{fig:ori}. We then rotate the scene so that all line segments are approximately horizontal or vertical, as depicted in \ref{fig:rotated}. In Figure \ref{fig:calibrated}, each line segment is classified and further rotated to be either horizontal or vertical, based on the assumption that all adjacent walls are perpendicular to each other. 

Figure \ref{fig:layout} shows the final result after the merging process. The merging process could be regarded as a classical \textbf{Minimum Cut problem}. In Figure \ref{fig:layout}, all line segments can be regarded as a node in a graph, two nodes have a connection if and only if they satisfy two constraints. 1) they belong to the same categories (vertical or horizontal). 2) they do not appear in the same image. 3) they are not across with the other category of node. Finally, the weight of each connection is settled as the Euclidean distance of their line segment centers. Based on this established graph, the merging results are the optimal solution of the minimum cut on this graph.  The detail of the merging process can be found in Algorithm~\ref{alg:plane-correspondance} of Appendix.

% Figure \ref{fig:layout} shows the final result after the merging process. The merging of line segments is performed separately for vertical and horizontal lines. Here, we focus on the process of merging horizontal lines, while the procedure for vertical lines follows a similar approach.
% The line clustering algorithm can be formulated as a constrained optimization problem:
% Given a set of horizontal lines $L = \{l_1, l_2, \ldots, l_n\}$, where each line  $l_i$ is defined by its position $v_i$ and its range $[min_i, max_i]$, along with a set of vertical lines $L_v$, and an image ID  $id_i$, the objective is to find a set of clusters $C = \{C_1, C_2, \ldots, C_k\}$ which minimizes $\left|C\right|$, the number of clusters, with the following constraints:
% \begin{align*}
%     % &\text{minimizes} \left|C\right|  \\
%    % & \text{subject to:}  \\
%   &  1.\  \forall C_i \in C, \forall l_a \in C_i : (D(v_a, center(C_i)) \leq \delta \text{ AND } O(l_a, C_i) \geq \tau)) \text{ OR } \neg I(l_a, L_{v}) \\
% &2.\  \forall C_i \in C, \forall l_a, l_e \in C_i : id_a \neq id_e
% \end{align*}
%  In constraint 1, the proximity criterion is used to ensure that lines within a cluster are close to each other, where $\delta$ is the maximum allowed distance. $D(v_a, center(C_i))$ stands for the distance between line $l_a$ and the center of cluster $C_i$. The overlap criterion is used to guarantee a minimum level of spatial overlap between a line and its cluster, where $\tau$ is the minimum required overlap ratio.
% \begin{equation}
%     O(l_a, C_i) = \frac{\text{Intersection\_length}(l_a, \text{union}(C_i)) }{(max_a - min_a) }
% \end{equation}
% $I(l_a, L_{v})$ is true if any vertical line intersects $l_a$ . Our algorithm handles T-shaped configurations in two ways:
% 1) It allows lines that don not meet the proximity and overlap criteria to form a cluster if no vertical lines intersect them. 2) It separates lines into different clusters if intersecting vertical lines exist, thereby forming the T-shaped configuration. The image uniqueness constraint 2 prevents lines from the same image from being included in the same cluster.

% Once clustering is complete, we assign a unique global ID to each cluster. We then average the plane parameters within the same cluster as the final parameters of the corresponding 3D plane. The 3D plane inherits the adjacency relationships estimated from the single-view setting.

% move to appendix
% \begin{algorithm}[h]
% \small
% 	\caption{Merge Plane}
% 	\label{alg:plane-correspondance}
% 	\begin{algorithmic}[1]
% 		\REQUIRE vertical lines, horizontal lines
%             \STATE Sort $verticalLines$ by x-axis value
%             \STATE Initialize $clusters$ with the first segment.
%             \FOR{each segment  in $verticalLines[1,:]$}
%                 \STATE $found$ $\gets$ False
%                 \FOR{each $cluster$ in $clusters$}
%                     \IF{$lines.image\_id$ in $cluster.image\_id$}
%                         \STATE continue
%                     \ENDIF
%                     \IF{distance($line$, $cluster.centroid$)$<proximity\_threshold$}
%                         \IF{overlap($line$, $cluster.centroid$)$>overlap\_threshold$}
%                             \STATE Insert $line$ into $cluster$
%                             \STATE $found$ $\gets$ True
%                             \STATE break
%                         \ENDIF
%                         \IF{ not intersect($line$, $cluster$, $horizontalLines$, $margin$)}
%                             \STATE Insert $line$ into $cluster$
%                             \STATE $found$ $\gets$ True
%                             break
%                         \ENDIF
%                     \ENDIF
                
%                 \ENDFOR
%             \ENDFOR
%             \IF{$found ==$ False}
%                 \STATE Create a new cluster with $line$
%                 \STATE Append the new cluster to $clusters$
%             \ENDIF
%     \ENSURE Clusters
% 	\end{algorithmic}
% \end{algorithm}

\section{EXPERIMENTS}
\label{experiments}


\subsection{Settings.}

\textbf{Dataset.} Structured3D \citep{zheng2020structured3d} is a synthetic dataset that provides a large collection of photo-realistic images with detailed 3D structural annotations. Similar to \cite{yang2022learning}, the dataset is divided into training, validation, and test sets at the scene level, comprising 3000, 250, and 250 scenes, respectively. Each scene consists of multiple rooms, with each room containing 1 to 5 images captured from different viewpoints. To construct image pairs that share similar visual content, we retain only rooms with at least two images. Within each room, images are paired to form image sets. Ultimately, we obtained 115,836 image pairs for the training set and 11,030 image pairs for the test set. For validation, we assess all rooms from the validation set. For rooms that only have one image, we duplicate that image to form image pairs for pointmap retrieval. In the subsequent inference process, we retain only one pointmap per room.

\textbf{Training details.} During training, we initialize the model with the original DUSt3R checkpoint. We freeze the encoder parameters and fine-tune only the decoder and DPT heads. Our data augmentation strategy follows the same approach as DUSt3R, using input resolution of $512 \times 512$. We employ the AdamW optimizer \citep{loshchilov2017decoupled} with a cosine learning rate decay schedule, starting with a base learning rate of 1e-4 and a minimum of 1e-6. The model is trained for 20 epochs, including 2 warm-up epochs, with a batch size of 16. We train two versions \ours{}, one with metric-scale loss and the other one without it. We name the metric-scale one as \textbf{\ours{} (metric)} and the other one as \textbf{\ours{}} .

\textbf{Evaluation.} Following the task formulation in equation (\ref{eqn:formula2}), our evaluation protocol consists of three parts to assess $f_1$, $f_2$, and the overall performance, respectively. 
\begin{itemize}[leftmargin=*]
    \item For the 2D information extraction module $f_1$,  we use the same metric as \cite{yang2022learning} for comparison: \textbf{Intersection over Union (IoU)}, \textbf{Pixel Error (PE)}, \textbf{Edge Error (EE)}, and \textbf{Root Mean Square Error (RMSE)}. 
    \item For the multi-view information extraction module $f_2$, we report the \textbf{Relative Rotation Accuracy (RRA)} and \textbf{Relative Translation Accuracy (RTA)} for each image pair to evaluate the relative pose error. We use a threshold of $\tau=15$ to report RTA@15 and RRA@15 (The comprehensive results of different thresholds can be seen in Table~\ref{tab:threshold} of Appendix C.1). Additionally, we calculate the \textbf{mean Average Accuracy (mAA30)}, defined as the area under accuracy curve of the angular differences at $min$(RRA@30, RTA@30).
    \item Finally, for evaluating the overall layout estimation, we employ \textbf{3D precision} and \textbf{3D recall} of planes as metrics. A predicted plane is considered matched with a ground truth plane if and only if the angular difference between them is less than \xd{10}\textdegree and the offset difference is less than 0.15m. Each ground truth plane can be matched only once.
\end{itemize}
% For 2D information extraction module $f_1$,  we used the same metric as \citep{yang2022learning} for comparison: \textbf{Intersection over Union (IoU)}, \textbf{Pixel Error (PE)}, \textbf{Edge Error (EE)}, and \textbf{Root Mean Square Error (RMSE)}. 
% For multi-view information extraction $f_2$, we report the \textbf{Relative Rotation Accuracy (RRA)} and \textbf{Relative Translation Accuracy (RTA)} for each image pair to evaluate the relative pose error and select a threshold $\tau=15$ to report RTA@15 and RRA@15. Additionally, we calculate the \textbf{mean Average Accuracy (mAA30)}, defined as the area under the accuracy curve of the angular differences at $min$(RRA@30, RTA@30).
% Finally, for the evaluation of the overall layout results, we employ \textbf{3D precision} and \textbf{3D recall} of planes as the metrics. A predicted plane is considered matched with a ground truth plane if and only if the angular difference between them is less than 15\textdegree and the offset difference is less than 0.15m. Each ground truth plane can be matched only once.

% Since the reconstruction pipeline does not involve utilizing any real-world camera model, the final reconstruction is scale-ambiguity. For the purpose of validation, we align the output reconstruction with the ground truth camera coordinate system. We evaluate the performance on both single image level and room level. For single-image evaluation, we obtain 2D layout segmentation by reprojecting the 3D reconstruction result back to the image. We used the same metric as \citep{stekovic2020general} for comparison: \textbf{Intersection over Union (IoU)}, \textbf{Pixel Error (PE)}, \textbf{Edge Error (EE)}, and \textbf{Root Mean Square Error (RMSE)}.
\begin{figure}
    \centering
          \includegraphics[width=0.19\linewidth,valign=c]{figs/qualitative/case3/rgb_rawlight_0.jpg}
     \includegraphics[width=0.19\linewidth,valign=c]{figs/qualitative/case3/rgb_rawlight_1.jpg} 
      \includegraphics[width=0.19\linewidth,valign=c]{figs/qualitative/case3/rgb_rawlight_2.jpg}
       \includegraphics[width=0.19\linewidth,valign=c]{figs/qualitative/case3/noncuboid_result.png}
      \includegraphics[width=0.19\linewidth,valign=c]{figs/qualitative/case3/result.png}\\
      \includegraphics[width=0.19\linewidth,valign=c]{figs/qualitative/case6/rgb_rawlight_0.jpg}
     \includegraphics[width=0.19\linewidth,valign=c]{figs/qualitative/case6/rgb_rawlight_1.jpg} 
      \includegraphics[width=0.19\linewidth,valign=c]{figs/qualitative/case6/rgb_rawlight_2.jpg}
      \includegraphics[width=0.19\linewidth,valign=c]{figs/qualitative/case6/noncuboid_result.png}
      \includegraphics[width=0.19\linewidth,valign=c]{figs/qualitative/case6/result.png}\\
    \caption{Qualitative results on Structure3D testing set. The first 3 columns are input views, the fourth and fifth columns are layout results of Noncuboid+MASt3R and our pipeline respectively. Due to space limitations, we refer reader to appendix for more complete results. }
    \vspace{-2em}
    \label{fig:qualitative}
\end{figure}



\textbf{Baselines.} As this work is the first attempt at 3D room layout estimation from multi-view perspective images, there are no existing baseline methods for direct comparison. Therefore, we design two reasonable baseline methods. We also compare our $f_1$ and $f_2$ with other methods of the same type.
\begin{itemize}[leftmargin=*]
    \item Since we use Noncuboid \citep{yang2022learning} as our $f_1$, we not only compare it against the baselines from their paper \citep{liu2019planercnn,stekovic2020general}, but also retrain it with better hyper-parameters obtained through grid search. 
    \item \xd{For $f_2$ (\ours{}), we compare it to recent data-driven image matching DUSt3R \citep{wang2024dust3r} and MASt3R \citep{leroy2024grounding}. }
    \item Finally, for the overall multi-view layout baseline, we design two methods: 1) Noncuboid with ground truth camera poses and 2) Noncuboid with MASt3R. In this context, we introduce the fusion of MASt3R~\citep{leroy2024grounding} and NonCuboid~\citep{yang2022learning} as our baseline method. MASt3R further extends DUSt3R, enabling it to estimate camera poses at a metric scale from sparse image inputs. We employ the original NonCuboid method to obtain single-view layout reconstruction. Next, we utilize the predicted camera poses to unify all planes from their respective camera poses into a common coordinate system. For instance, we designate the coordinate system of the first image as the world coordinate system. We then perform the same operation as described in Sec \ref{sec:mv-reconstruction} to achieve the final multi-view reconstruction. The Noncuboid with ground truth camera poses is introduced as an ablation study to eliminate the effects of inaccurate pose estimation. The experimental setup is the same as the Noncuboid with MASt3R pipeline, except for the use of ground truth camera poses instead of poses estimated by MASt3R.
\end{itemize}

% \begin{enumerate}[leftmargin=*]
%     \item[-] \textbf{Intersection over Union (IoU):} It measures the overlap between the predicted 2D plane segmentation and the ground truth. 
%     \item[-] \textbf{Pixel Error (PE):} It calculates the pixel-wise error between the predicted 2D plane segmentation and the ground truth.
%     \item[-] \textbf{Edge Error (EE):} It's the symmetrical Chamfer distance between the predicted layout boundary and the ground truth
%     \item[-] \textbf{Root Mean Square Error (RMSE):} It measures the difference in depth maps between the predicted layout and the ground truth.
% \end{enumerate}


% For the evaluation at the room level, we employ precision and recall of planes as the metrics. A predicted plane is considered matched with a ground truth plane if and only if the angular difference between them is less than 15 \textdegree and the offset difference is less than 0.15m. Each ground truth plane can be matched only once.




\begin{figure}
    \centering
    \includegraphics[width=0.16\linewidth]{figs/consistency_comparison/case1/layout_ours.png}
     \includegraphics[width=0.16\linewidth]{figs/consistency_comparison/case2/layout_ours.png} 
      \includegraphics[width=0.16\linewidth]{figs/consistency_comparison/case3/layout_ours.png} 
      \includegraphics[width=0.16\linewidth]{figs/consistency_comparison/case4/layout_ours.png}
      \includegraphics[width=0.16\linewidth]{figs/consistency_comparison/case5/layout_ours.png}\\
    \includegraphics[width=0.16\linewidth]{figs/consistency_comparison/case1/layout.png}
     \includegraphics[width=0.16\linewidth]{figs/consistency_comparison/case2/layout.png}
     \includegraphics[width=0.16\linewidth]{figs/consistency_comparison/case3/layout.png}
     \includegraphics[width=0.16\linewidth]{figs/consistency_comparison/case4/layout.png}
     \includegraphics[width=0.16\linewidth]{figs/consistency_comparison/case5/layout.png}
    \caption{Birdview of multi-view 3D planes aligned to the same coordinate. The first row shows 5 cases of our pipeline results after post-processing step. The second row is the results of Noncuboid+MASt3R. Line segments of the same color indicate that they belong to the same plane.}
    \label{fig:layout-comparison}
    
\end{figure}


\begin{table}[]
    \centering
    \scriptsize
    \caption{Quantitative results on Structure3D dataset. }
    \label{tab:method-comparsion}
    \begin{tabular}{lcccccc} 
        \toprule
         \multicolumn{1}{c}{\textbf{Method}}& \textbf{re-IoU(\%)}$\uparrow$& \textbf{re-PE(\%) }$\downarrow$&\textbf{re-EE}$\downarrow$&\textbf{re-RMSE}$\downarrow$ &\textbf{3D-precision(\%)}$\uparrow$ & \textbf{3D-recall(\%)}$\uparrow$\\ 
         \hline
         Noncuboid + MASt3R  & 74.51 & 8.57 &  12.72& 0.4831 & 37.00 & 43.39\\
         Noncuboid + GT pose & \underline{75.93} & \underline{7.97} & 11.37 & 0.4457 & 46.96 & \textbf{50.59}\\ 
         Ours (metric) & 75.34&  8.60& \underline{10.83} & \underline{0.4388} & \underline{48.98} & 45.35\\ 
         Ours (aligned) & \textbf{76.84} &  \textbf{7.82}& \textbf{9.53} & \textbf{0.4099} & \textbf{52.63} & \underline{48.37}\\ 
    \bottomrule
    \end{tabular}
    \vspace{-2em}
\end{table}

\subsection{Multi-view Room Layout Estimation Results}
\label{sec:experiment-comparison}

In this section, we compare our multi-view layout estimation pipeline with two baseline methods, both qualitatively and quantitatively. Additionally, we conduct experiments to verify the effectiveness of our pipeline components $f_1$ 2D detector and $f_2$ \ours{}. 


\paragraph{Layout results comparison.} Table~\ref{tab:method-comparsion} and Figure~\ref{fig:qualitative} present quantitative and qualitative comparisons of our pipeline with two baseline methods. Ours (metric) and Ours (aligned) in Table~\ref{tab:method-comparsion} refer to the methods from our pipeline using \ours{} (metric) and \ours{}, respectively. 
% \xd{The aligned version indicates that we align the output reconstruction with the ground truth camera coordinate system during validation. This alignment is necessary because \ours{} output is scale-ambiguous, and we require the ground truth scale to obtain reasonable reprojection results.} 
The first 4 metrics (re-IoU, re-PE, re-EE, and re-RMSE)  are calculated similarly to their 2D counterparts (IoU, PE, EE, and RMSE), except that the predicted 2D results are reprojected from the estimated multi-view 3D layout. Compared with the baseline methods, \ours{} achieves superior 3D plane normal estimations compared to Noncuboid's single-view plane normal estimation, even when using ground truth camera pose (Noncuboid + GT pose). Figure~\ref{fig:layout-comparison} further demonstrates that \ours{} could predict accurate and robust 3D information with sparse-view input. 




\begin{table}[h]
\centering
\scriptsize
\caption{\xd{Comparison with data-driven image matching approaches. }}
\label{tab:multiview_compa}
\begin{tabular}{cl@{\ \ }c@{\ \ }c@{\ \ }c@{\ \ }c@{\ \ }c@{\ \ }c@{\ \ }c@{\ \ }c}
\toprule
\multicolumn{2}{c}{\multirow{2}[3]{*}{Methods}} & \multicolumn{1}{c}{RealEstate10K} & \multicolumn{3}{c}{Structured3D} & \multicolumn{3}{c}{CAD-estate} \\
\cmidrule(lr){3-3} \cmidrule(lr){4-6} \cmidrule(lr){7-9}
& & mAA@30 & RRA@15 & RTA@15 & mAA@30 & RRA@15 & RTA@15 & mAA@30\\
\midrule
 \multirow{1}{ 1em}{(a)} & DUSt3R \citep{wang2024dust3r} & 61.2 & 89.44 & 85.00 & 76.13 & 99.88 & 84.82 & 76.38 \\
 & MASt3R \citep{leroy2024grounding} & 76.4 & 92.94 & 89.77 & 85.34 & 99.94 & 99.00 & 95.29 \\
 \midrule
 \multirow{2}{ 1em}{(b)} &  \ours{} (metric)  & - & 98.21 & 96.66 & 90.67 & 94.61 & 70.52 & 61.48 \\
 & \ours{} (aligned) & - & 97.95 & 96.59 & 91.80 & 94.96 & 73.74 & 64.21 \\
\bottomrule
\end{tabular}
\vspace{-2em}
\end{table}




% \vspace{-1em}
\paragraph{3D information prediction and correspondence-established method \ours{} $f_2$.}\xd{ Table~\ref{tab:multiview_compa} shows the comparison results of our \ours{} (part (b) in Table~\ref{tab:multiview_compa}), recent popular data-driven image matching approaches (part (a) in Table~\ref{tab:multiview_compa}) in  RealEstate10K \citep{zhou2018stereo},  Structure3D \citep{zheng2020structured3d}, and  CAD-Estate \citep{rozumnyi2023estimatinggeneric3droom} datasets. Note that in part(b), results on RealEstate10K dataset are not provided since our model is specifically designed to predict room structural elements, while RealEstate10K contains outdoor scenes that may lead to prediction failures. Instead, we utilize CAD-Estate dataset, which is derived from RealEstate10K with additional room layout annotations, as a more suitable benchmark for comparison. The results of parts (a) and (b) on three datasets show the advancements of MASt3R, not only in traditional multi-view datasets (RealEstate10K,CAD-Estate), but also in sparse-view dataset (Structure3D).} \ours{} could get a better performance compared to the previous SOTA MASt3R. One arguable point is that \ours{} is obviously better since it is fine-tuned on Strucutre3D. That is the message we want to convey. DUSt3R/MASt3R are the SOTAs in both multi-view and saprse-view camera pose estimation tasks. After our improvements (section~\ref{sec:f2}) and fine-tuning, \ours{} could get 5.33 points better on the sparse-view layout dataset.  



% To the best of our knowledge, there is no existing work that addresses a problem similar to ours. In this context, we introduce the fusion of MASt3R~\citep{leroy2024grounding} and NonCuboid~\citep{yang2022learning} as our baseline method. MASt3R further extends DUSt3R, enabling it to estimate camera poses at a metric scale from sparse image inputs. We employ the original NonCuboid method to obtain single-view layout reconstruction. Next, we utilize the predicted camera poses to unify all planes from their respective camera poses into a common coordinate system. For instance, we designate the coordinate system of the first image as the world coordinate system. We then perform the same operation as described in Sec \ref{sec:mv-reconstruction} to achieve the final multi-view reconstruction. We also compare our results against three single-view methods: (i) Planar R-CNN~\citep{howard2019thinking}, (ii)  RaC~\citep{stekovic2020general}, and  (iii) NonCuboid~\citep{yang2022learning}.

% Our model's performance does not surpass these three single-view methods for several reasons. Firstly, our performance is partially dependent on the original NonCuboid method, as it forms part of our pipeline. Secondly, we are addressing a multi-view problem and perform inference without any prerequisite information about the ground-truth pose or actual camera parameters which is much more challenging than single-view problems. Our method outperforms the baseline approach, likely due to our implementation of multi-view reconstruction prior to extracting 3D plane parameters. While the baseline first extracts parameters separately and then unifies them into a single coordinate system. Our approach appears to yield more consistent results, as shown in Fig \ref{fig:layout-comparison}.




\begin{table}[]
    \centering
    \footnotesize
    \caption{2D detectors comparison on Structure3D dataset. 
    % Our retrained Noncuboid method performs best among these methods.
    }
    \label{tab:2D_detect_compa}
    \begin{tabular}{lcccc} 
        \toprule
         \multicolumn{1}{c}{\textbf{Method}}& \textbf{IoU(\%)}$\uparrow$& \textbf{PE(\%) }$\downarrow$&\textbf{EE}$\downarrow$&\textbf{RMSE}$\downarrow$ \\ 
         \hline
          Planar R-CNN \citep{liu2019planercnn} & 79.64  & 7.04  &\underline{6.58} &0.4013 \\
          Rac \citep{stekovic2020general} &76.29  & 8.07 & 7.19& 0.3865 \\
          Noncuboid \citep{yang2022learning} & \underline{79.94} & \underline{6.40} & 6.80 & \underline{0.2827}\\
          Noncuboid (re-trained) & \textbf{80.18} & \textbf{6.13} & \textbf{6.41} & \textbf{0.2631}\\
        \bottomrule
        % \vspace{-1cm}
    \end{tabular}
\end{table}

\vspace{-1em}
\paragraph{Comparison of 2D detectors ($f_1$).} We retrain the Noncuboid method with a more thorough hyper-parameter grid search, resulting in an improved version. Table~\ref{tab:2D_detect_compa} compares its results with other baseline methods from \cite{yang2022learning}.

\vspace{-1em}
\paragraph{Comparison of various input views.} We experiment the impact of different number of input views in Appendix C.2. The results in Table~\ref{tab:input_view} show a general improvement trend as the number of views increases.

\vspace{-1em}
\subsection{Generalizability to Unknown and Out-of-Domain Data}
% Trained on diverse datasets, Dust3r demonstrates remarkable generalization capabilities when faced with unseen data. By freezing the encoder during the training process, our model effectively preserves this generality, allowing it to maintain its performance on in-the-wild data. This phenomenon has also been observed in \cite{revaud2024sacreg}
Figure~\ref{fig:teaser} and~\ref{burger} also demonstrate the generalizability of our pipeline. It not only performs well on the testing set of Structure3D (Figure~\ref{fig:qualitative}), but also generalizes well to new datasets, such as RealEstate10K (Figure~\ref{fig:in-the-wild} shows examples from this dataset). Furthermore, our pipeline proves effective even with data in the wild as shown in appendix in Figure~\ref{friends},\ref{burger}. We also experiment our pipeline on the dataset CAD-estate (see Appendix E for details).

\begin{figure}
    \centering
    % \includegraphics[width=0.19\linewidth,valign=c]{figs/in-the-wild/case1/1.png}
    %  \includegraphics[width=0.19\linewidth,valign=c]{figs/in-the-wild/case1/2.png} 
    %   \includegraphics[width=0.19\linewidth,valign=c]{figs/in-the-wild/case1/3.png}
    %   \includegraphics[width=0.19\linewidth,valign=c]{figs/in-the-wild/case1/case1_noncuboid_result.png}
    %   \includegraphics[width=0.19\linewidth,valign=c]{figs/in-the-wild/case1/case1_result.png}
    %   \\
    \includegraphics[width=0.19\linewidth,valign=c]{figs/in-the-wild/case2/1.png}
     \includegraphics[width=0.19\linewidth,valign=c]{figs/in-the-wild/case2/2.png} 
      \includegraphics[width=0.19\linewidth,valign=c]{figs/in-the-wild/case2/3.png}
       \includegraphics[width=0.19\linewidth,valign=c]{figs/in-the-wild/case2/case2_noncuboid_result.png}
      \includegraphics[width=0.19\linewidth,valign=c]{figs/in-the-wild/case2/case2_result.png}
       \\
    \includegraphics[width=0.19\linewidth,valign=c]{figs/in-the-wild/case3/1.png}
     \includegraphics[width=0.19\linewidth,valign=c]{figs/in-the-wild/case3/2.png} 
      \includegraphics[width=0.19\linewidth,valign=c]{figs/in-the-wild/case3/3.png}
       \includegraphics[width=0.19\linewidth,valign=c]{figs/in-the-wild/case3/case3_noncuboid_result.png}
      \includegraphics[width=0.19\linewidth,valign=c]{figs/in-the-wild/case3/case3_result.png}
       \\
    \caption{Qualitative results on in-the-wild data \citep{zhou2018stereo}. The first three columns are input views, the fourth column is the layout results of Noncuboid+MASt3R. The rightmost column shows the predicted plane pointmap with the extracted wireframe drawn in red.
    }
    \vspace{-0.7cm}
    \label{fig:in-the-wild}
\end{figure}







\section{Conclusion}
This paper introduces the first pipeline for multi-view layout estimation, even in sparse-view settings. The proposed pipeline encompasses three components: a 2D plane detector, a 3D information prediction and correspondence establishment method, and a post-processing algorithm. 
As the first comprehensive approach to the multi-view layout estimation task, this paper provides a detailed analysis and formulates the problem under both single-view and multi-view settings. Additionally, we design several baseline methods for comparison to validate the effectiveness of our pipeline. Our approach consistently outperforms the baselines on both 2D projection and 3D metrics. Furthermore, our pipeline not only performs well on the synthetic Structure3D dataset, but generalizes effectively to in-the-wild datasets and scenarios with different image styles such as the cartoon style.


% \subsubsection*{Author Contributions}
%  Yaxuan Huang and Xili Dai contributed equally to this work as co-first authors.
% If you'd like to, you may include  a section for author contributions as is done
% in many journals. This is optional and at the discretion of the authors.

\subsubsection*{Acknowledgments}
This work is partially supported by the Hong Kong Center for Construction Robotics (Hong Kong ITC-sponsored InnoHK center), the National Natural Science Foundation of China (Grant No. 62306261),  CUHK Direct Grants (Grant No. 4055190), and The Shun Hing Institute of Advanced Engineering (SHIAE) No. 8115074.
% This work was funded by the Hong Kong Center for Construction Robotics (Hong Kong ITC-sponsored InnoHK center)
% Use unnumbered third level headings for the acknowledgments. All
% acknowledgments, including those to funding agencies, go at the end of the paper.


\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}

\newpage
\appendix

\section{DUSt3R Details}
\label{app:dust3r}

Given a set of RGB images 
$\{ \displaystyle \mI_{1}, \displaystyle \mI_{2}, \ldots, \displaystyle \mI_{n}\} \in \displaystyle \R^{H\times W \times3} $, we first pair them to create a set of image pairs $\displaystyle \sP = \{ (\displaystyle \mI_{i}, \displaystyle \mI_{j}) \mid i \neq j, 1 \leq i,j \leq n\}$. For each image pair $(\displaystyle \mI_{i}, \displaystyle \mI_{j}) \in \displaystyle \sP$, the model estimates two point maps $\displaystyle \mX_{i,i}, \displaystyle \mX_{j,i}$, along with their corresponding confidence maps $\displaystyle \mC_{i,i}, \displaystyle \mC_{j,i}$. Specifically, both pointmaps are expressed in the camera coordinate system of $\displaystyle \mI_{i}$, which implicitly accomplishes dense 3D reconstruction.

The model consists of two parallel branches, as shown in Fig ~\ref{dust3r-model}, each branch responsible for processing one image. The two images are first encoded in a Siamese manner with weight-sharing ViT encoder\citep{dosovitskiy2020image} to produce two latent features $\displaystyle \mF_1, \displaystyle \mF_2$: $\displaystyle \mF_i = \operatorname{Encoder}(\displaystyle \mI_i)$.
Next, $\displaystyle \mF_1, \displaystyle \mF_2$ are fed into two identical decoders that continuously share information through cross-attention mechanisms. By leveraging cross-attention mechanisms, the model is able to learn the relative geometric relationships between the two images. Specifically, for each encoder block:
\begin{equation} 
    \begin{aligned}
        \displaystyle \mG_{1,i} = \operatorname{DecoderBlock}_{1,i}(\displaystyle \mG_{1,i-1},\displaystyle \mG_{2,i-1}),  \\
        \displaystyle \mG_{2,i} = \operatorname{DecoderBlock}_{2,i}(\displaystyle \mG_{1,i-1},\displaystyle \mG_{2,i-1})
    \end{aligned}
\end{equation}
where $\mG_{1,0}:= \displaystyle \mF_1, \mG_{2,0}:= \displaystyle \mF_2$.
Finally, the DPT\citep{ranftl2021vision} head regresses the pointmap and confidence map from the concatenated features of different layers of the decoder tokens:
\begin{equation}
    \begin{aligned}
        \displaystyle \mX_{1,1}, \displaystyle \mC_{1,1} = \operatorname{Head}_1(\displaystyle\mG_{1,0}, \displaystyle\mG_{1,1},\dots, \displaystyle\mG_{1,B}) \\
        \displaystyle \mX_{2,1}, \displaystyle \mC_{2,1} = \operatorname{Head}_2(\displaystyle\mG_{2,0}, \displaystyle\mG_{2,1},\dots,\displaystyle\mG_{2,B} )
    \end{aligned}
\end{equation}
where $B$ is the number of decoder blocks.
The regression loss function is defined as the scale-invariant Euclidean distance between the normalized predicted and ground-truth pointmaps:
\begin{equation}
    l_{regr}(v,i) = \|\frac{1}{z}\displaystyle \mX_i^{v,1}-\frac{1}{\bar{z}}\bar{\displaystyle \mX}_i^{v,1} \|_2^2
\end{equation}
where $v \in \{1,2\}$ and $i$ is the pixel index. The scaling factors $z$ and $\bar{z}$ represent the average distance of all corresponding valid points to the origin. The original DUSt3R couldn't guarantee output at a metric scale, so we also trained a modified version of \ours{} that produces metric-scale results. The key change we made was setting $z:=\bar{z}$.
By introducing the regression loss in confidence loss, the model could implicitly learn how to identify regions that are more challenging to predict compared to others. Same as in DUSt3R \citep{wang2024dust3r}:
\begin{equation}
 \mathcal{L}_{\mathrm{conf}}=\sum_{v\in\{1,2\}}\sum_{i\in\mathcal{D}^v}C_i^{\upsilon,1}\ell_{\mathrm{regr}}(v,i)-\alpha\log C_i^{\upsilon,1}
\end{equation}
To obtain the ground-truth pointmaps $\displaystyle \mX^{v,1}$ , we first transform the ground truth depthmap $\displaystyle \mD \in \R^{H\times W}$ into a pointmap $\displaystyle \mX^{v}$ express in the camera coordinate of $v$ by $\displaystyle \mX^{v}_{i,j} = \displaystyle \mK^{-1}[i\displaystyle \mD_{i,j}, j\displaystyle \mD_{i,j},\displaystyle \mD_{i,j}]^\top$ with camera intrinsic matrix $\displaystyle \mK \in \R^{3\times 3}$. Then we obtain $\displaystyle \mX^{v,1}$ by $\displaystyle \mX^{v,1} = \displaystyle \mT_1^{-1}\displaystyle \mT_v\displaystyle h(\mX^v)$ with $\displaystyle \mT_1, \displaystyle \mT_v \in \R^{3\times 4}$ the camera-to-world poses and $h$ being the homogeneous transformation.
% \textit{The biggest difference of our method and DUSt3R, is that we used the depth of plane as the ground truth depthmap $\displaystyle \mD$ instead of the normal depth, as shown in Fig \ref{fig:depth}.}


\paragraph{Global Alignment}
For global alignment, we aim to assign a global pointmap and camera pose for each image.
First, the average confidence scores of each pair of images are utilized as the similarity scores. A higher value of confidence implies a stronger visual similarity between the two images.  These scores are employed to construct a Minimum Spanning Tree, denoted as $\displaystyle \gG(\displaystyle \gV, \displaystyle \gE)$, where each vertex $\displaystyle \gV$ corresponding to an image in the input set and each edge $e = (n,m) \in \displaystyle \gE$ indicates that images $\displaystyle \mI_{n}$ and $\displaystyle \mI_{m}$ share significant visual content.
We aim to find globally aligned point maps$\{\chi^n \in \displaystyle \R^{H\times W \times3} \}$ and a transformation $\displaystyle \mT_i \in \displaystyle \R^{3\times4}$ than transform the prediction into the world coordinate frame. To do this, for each image pair $e = (n,m) \in \displaystyle \gE$ we have two point maps $\displaystyle \mX^{n,n}, \displaystyle \mX^{m,n}$ and  their confidence maps $\displaystyle \mC^{n,n}, \displaystyle \mC^{m,n}$. For simplicity, we use the annotation $\displaystyle \mX^{n,e} := \displaystyle \mX^{n,n}, \displaystyle \mX^{m,e} := \displaystyle \mX^{m,n}$. Since $\displaystyle \mX^{n,e}$ and $\displaystyle \mX^{m,e}$ are in the same coordinate frame, $\displaystyle \mT_e := \displaystyle \mT_n$ should align both point maps with the world-coordinate. We then solve the following optimization problem:
\begin{equation}
    \chi^*=\arg\min_{\chi,T,\sigma}\sum_{e\in\mathcal{E}}\sum_{v\in e}\sum_{i=1}^{HW}C_i^{v,e}\left\|\chi_i^v-\sigma_eT_eX_i^{v,e}\right\|_2^2.
\end{equation}
where $v\in e$ means $v$ can be either $n$ or $m$ for the pair $e$ and $\sigma_e$ is a positive scaling factor . To avoid the trivial solution where $\sigma_e = 0$, we ensure that $\prod_e\sigma_e=1$

\section{$f_3$ algorithm}

The goal of multi-view layout estimation is similar to that of single-view: we need to estimate 3D parameters for each plane and determine the relationships between adjacent planes. However, in a multi-view setting, we must ensure that each plane represents a unique physical plane in 3D space. The main challenge in multi-view reconstruction is that the same physical plane may appear in multiple images, causing duplication. Our task is to identify which planes correspond to the same physical plane across different images and merge them, keeping only one representation for each unique plane. 

Since we allow at most one floor and one ceiling detection per image,  we simply average the parameters from all images to obtain the final floor and ceiling parameters. As for walls, we assume all walls are perpendicular to both the floor and ceiling. To simplify the merging process, we project all walls onto the x-z plane defined by the floor and ceiling. This projection reduces the problem to a 2D space, making it easier to identify and merge corresponding walls.
Figure \ref{PlaneMerge} illustrates the entire process of merging walls. Each wall in an image is denoted as one line segment, as shown in Figure \ref{fig:ori}. We then rotate the scene so that all line segments are approximately horizontal or vertical, as depicted in \ref{fig:rotated}. In Figure \ref{fig:calibrated}, each line segment is classified and further rotated to be either horizontal or vertical, based on the assumption that all adjacent walls are perpendicular to each other. 

\begin{algorithm}[h]
\small
	\caption{Merge Plane}
	\label{alg:plane-correspondance}
	\begin{algorithmic}[1]
		\REQUIRE vertical lines, horizontal lines
            \STATE Sort $verticalLines$ by x-axis value
            \STATE Initialize $clusters$ with the first segment.
            \FOR{each segment  in $verticalLines[1,:]$}
                \STATE $found$ $\gets$ False
                \FOR{each $cluster$ in $clusters$}
                    \IF{$lines.image\_id$ in $cluster.image\_id$}
                        \STATE continue
                    \ENDIF
                    \IF{distance($line$, $cluster.centroid$)$<proximity\_threshold$}
                        \IF{overlap($line$, $cluster.centroid$)$>overlap\_threshold$}
                            \STATE Insert $line$ into $cluster$
                            \STATE $found$ $\gets$ True
                            \STATE break
                        \ENDIF
                        \IF{ not intersect($line$, $cluster$, $horizontalLines$, $margin$)}
                            \STATE Insert $line$ into $cluster$
                            \STATE $found$ $\gets$ True
                            break
                        \ENDIF
                    \ENDIF
                
                \ENDFOR
            \ENDFOR
            \IF{$found ==$ False}
                \STATE Create a new cluster with $line$
                \STATE Append the new cluster to $clusters$
            \ENDIF
    \ENSURE Clusters
	\end{algorithmic}
\end{algorithm}

\section{Additional Quantitative Results}

\subsection{Performance Under Varying Thresholds}
To give a more complete view of the performance of the \ours{} method, we present the results under various threshold settings (the threshold of both camera's translation and rotation) in Table~\ref{tab:threshold}. 


\subsection{Performance Under input views}
The impact of varying input views on performance is presented in Table~\ref{tab:input_view}. For each input views, we select this number of views from the 5 views as input and only test on rooms that have all 5 views to eliminate potential bias from room complexity variations.  The results show a general improvement trend as the number of views increases.


\begin{table}[]
    \centering
    \scriptsize
    \caption{\xd{Quantitative results with different thresholds on Structure3D dataset.} }
    \label{tab:threshold}
    \begin{tabular}{ccc} 
        \toprule
         \textbf{Threshold(Translation \& Rotation)} &\textbf{3D-precision(\%)}$\uparrow$ & \textbf{3D-recall(\%)}$\uparrow$\\ 
         \hline
         0.1m, 5\textdegree & 34.11 & 31.66 \\
         0.15m, 10\textdegree & 52.63 & 48.37   \\ 
         0.2m, 15\textdegree & 64.64 & 59.53 \\ 
         0.4m, 30\textdegree & \textbf{82.75} & \textbf{76.13}\\ 
    \bottomrule
    \end{tabular}
\end{table}

\begin{table}[]
    \centering
    \scriptsize
    \caption{\xd{Quantitative results with different input views on Structure3D dataset. }}
    \label{tab:input_view}
    \begin{tabular}{ccccccc} 
        \toprule
         \multicolumn{1}{c}{\textbf{Input views}}& \textbf{re-IoU(\%)}$\uparrow$& \textbf{re-PE(\%) }$\downarrow$&\textbf{re-EE}$\downarrow$&\textbf{re-RMSE}$\downarrow$ &\textbf{3D-precision(\%)}$\uparrow$ & \textbf{3D-recall(\%)}$\uparrow$\\ 
         \hline
         2 & 75.02 & 8.72 & 8.70 & 0.4148 & 53.19 & 42.60 \\
        3 & 75.29 & 8.53 & 8.56 & 0.3596 & 54.43 & 47.97 \\
        4 & 75.55 & 8.39 & \textbf{8.55} & 0.3463 & 54.91 & 49.44 \\
        5 & \textbf{75.57} & \textbf{8.35} & 8.59 & \textbf{0.3422} & \textbf{55.02} & \textbf{49.59} \\
    \bottomrule
    \end{tabular}
\end{table}

\section{Additional Qualitative Results}
Figure~\ref{fig:qualitative-appendix} showcases more visualization of our method on the Structured3D dataset, while Figure~\ref{fig:fail-case} presents failed cases. To demonstrate real-world applicability, we present results on in-the-wild images in Figure~\ref{friends} and Figure~\ref{burger}.

\begin{figure}
    \centering
    \includegraphics[width=0.15\linewidth,valign=c]{figs/qualitative/case1/rgb_rawlight_0.jpg}
     \includegraphics[width=0.15\linewidth,valign=c]{figs/qualitative/case1/rgb_rawlight_1.jpg} 
      \includegraphics[width=0.15\linewidth,valign=c]{figs/qualitative/case1/rgb_rawlight_2.jpg}
      \includegraphics[width=0.15\linewidth,valign=c]{figs/qualitative/case1/rgb_rawlight_4.jpg}
      \includegraphics[width=0.15\linewidth,valign=c]{figs/qualitative/case1/result.png}
      \includegraphics[width=0.15\linewidth,valign=c]{figs/qualitative/case1/result_wireframe.png}\\
    \includegraphics[width=0.15\linewidth,valign=c]{figs/qualitative/case2/rgb_rawlight_0.jpg}
     \includegraphics[width=0.15\linewidth,valign=c]{figs/qualitative/case2/rgb_rawlight_1.jpg} 
      \includegraphics[width=0.15\linewidth,valign=c]{figs/qualitative/case2/rgb_rawlight_2.jpg}
      \includegraphics[width=0.15\linewidth,valign=c]{figs/qualitative/case2/rgb_rawlight_3.jpg}
      \includegraphics[width=0.15\linewidth,valign=c]{figs/qualitative/case2/result.png}
      \includegraphics[width=0.15\linewidth,valign=c]{figs/qualitative/case2/result_wireframe.png}\\
\includegraphics[width=0.15\linewidth,valign=c]{figs/qualitative/case3/rgb_rawlight_0.jpg}
     \includegraphics[width=0.15\linewidth,valign=c]{figs/qualitative/case3/rgb_rawlight_1.jpg} 
      \includegraphics[width=0.15\linewidth,valign=c]{figs/qualitative/case3/rgb_rawlight_2.jpg}
      \includegraphics[width=0.15\linewidth,valign=c]{figs/qualitative/case3/rgb_rawlight_3.jpg}
      \includegraphics[width=0.15\linewidth,valign=c]{figs/qualitative/case3/result.png}
      \includegraphics[width=0.15\linewidth,valign=c]{figs/qualitative/case3/result_wireframe.png}\\
      \includegraphics[width=0.15\linewidth,valign=c]{figs/qualitative/case4/rgb_rawlight_0.jpg}
     \includegraphics[width=0.15\linewidth,valign=c]{figs/qualitative/case4/rgb_rawlight_1.jpg} 
      \includegraphics[width=0.15\linewidth,valign=c]{figs/qualitative/case4/rgb_rawlight_2.jpg}
      \includegraphics[width=0.15\linewidth,valign=c]{figs/qualitative/case4/rgb_rawlight_3.jpg}
      \includegraphics[width=0.15\linewidth,valign=c]{figs/qualitative/case4/result.png}
      \includegraphics[width=0.15\linewidth,valign=c]{figs/qualitative/case4/result_wireframe.png}\\
      \includegraphics[width=0.15\linewidth,valign=c]{figs/qualitative/case6/rgb_rawlight_0.jpg}
     \includegraphics[width=0.15\linewidth,valign=c]{figs/qualitative/case6/rgb_rawlight_1.jpg} 
      \includegraphics[width=0.15\linewidth,valign=c]{figs/qualitative/case6/rgb_rawlight_2.jpg}
      \includegraphics[width=0.15\linewidth,valign=c]{figs/qualitative/case6/rgb_rawlight_3.jpg}
      \includegraphics[width=0.15\linewidth,valign=c]{figs/qualitative/case6/result.png}
      \includegraphics[width=0.15\linewidth,valign=c]{figs/qualitative/case6/result_wireframe.png}\\
      \includegraphics[width=0.15\linewidth,valign=c]{figs/qualitative/case7/0_rgb_rawlight.jpg}
     \includegraphics[width=0.15\linewidth,valign=c]{figs/qualitative/case7/1_rgb_rawlight.jpg} 
      \includegraphics[width=0.15\linewidth,valign=c]{figs/qualitative/case7/2_rgb_rawlight.jpg}
      \includegraphics[width=0.15\linewidth,valign=c]{figs/qualitative/case7/3_rgb_rawlight.jpg}
      \includegraphics[width=0.15\linewidth,valign=c]{figs/qualitative/case7/result.png}
      \includegraphics[width=0.15\linewidth,valign=c]{figs/qualitative/case7/result_wireframe.png}\\
      \includegraphics[width=0.15\linewidth,valign=c]{figs/qualitative/case8/0_rgb_rawlight.jpg}
     \includegraphics[width=0.15\linewidth,valign=c]{figs/qualitative/case8/1_rgb_rawlight.jpg} 
      \includegraphics[width=0.15\linewidth,valign=c]{figs/qualitative/case8/2_rgb_rawlight.jpg}
      \includegraphics[width=0.15\linewidth,valign=c]{figs/qualitative/case8/3_rgb_rawlight.jpg}
      \includegraphics[width=0.15\linewidth,valign=c]{figs/qualitative/case8/result.png}
      \includegraphics[width=0.15\linewidth,valign=c]{figs/qualitative/case8/result_wireframe.png}\\
    \caption{\xd{Qualitative results on Structure3D testing set. The 5-th column is our result visualized with pointcloud, the last column is the result shown in pure wireframe }}
    \vspace{-0.5em}
    \label{fig:qualitative-appendix}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.16\linewidth,valign=c]{figs/qualitative/fail/case1/rgb_rawlight_0.jpg}
     \includegraphics[width=0.16\linewidth,valign=c]{figs/qualitative/fail/case1/rgb_rawlight_1.jpg} 
      \includegraphics[width=0.16\linewidth,valign=c]{figs/qualitative/fail/case1/rgb_rawlight_2.jpg}
      \includegraphics[width=0.16\linewidth,valign=c]{figs/qualitative/fail/case1/rgb_rawlight_3.jpg}
      \includegraphics[width=0.16\linewidth,valign=c]{figs/qualitative/fail/case1/result.png}
      \includegraphics[width=0.16\linewidth,valign=c]{figs/qualitative/fail/case1/result_wireframe.png}\\
      \includegraphics[width=0.16\linewidth,valign=c]{figs/qualitative/fail/case2/0_rgb_rawlight.jpg}
     \includegraphics[width=0.16\linewidth,valign=c]{figs/qualitative/fail/case2/1_rgb_rawlight.jpg} 
      \includegraphics[width=0.16\linewidth,valign=c]{figs/qualitative/fail/case2/2_rgb_rawlight.jpg}
      \includegraphics[width=0.16\linewidth,valign=c]{figs/qualitative/fail/case2/3_rgb_rawlight.jpg}
      \includegraphics[width=0.16\linewidth,valign=c]{figs/qualitative/fail/case2/result.png}
      \includegraphics[width=0.16\linewidth,valign=c]{figs/qualitative/fail/case2/result_wireframe.png}\\
       \includegraphics[width=0.16\linewidth,valign=c]{figs/qualitative/fail/case3/0_rgb_rawlight.jpg}
     \includegraphics[width=0.16\linewidth,valign=c]{figs/qualitative/fail/case3/1_rgb_rawlight.jpg} 
      \includegraphics[width=0.16\linewidth,valign=c]{figs/qualitative/fail/case3/2_rgb_rawlight.jpg}
      \includegraphics[width=0.16\linewidth,valign=c]{figs/qualitative/fail/case3/3_rgb_rawlight.jpg}
      \includegraphics[width=0.16\linewidth,valign=c]{figs/qualitative/fail/case3/result.png}
      \includegraphics[width=0.16\linewidth,valign=c]{figs/qualitative/fail/case3/result_wireframe.png}\\
    \caption{\xd{Failed case on Structure3D testing set. The first 4 columns are input views, the 5-th column is our result visualized with pointcloud, the last column is the result shown in pure wireframe.}}
    \label{fig:fail-case}
\end{figure}

\begin{figure}
     \centering
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/out_domain/friends/chandler-apartment.png}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/out_domain/friends/friends-overnight-joey-chandler-apartment.jpeg}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/out_domain/friends/result.png}
     \end{subfigure}
        \caption{We provide qualitative
results on in-the-wild data.}
        \label{friends}
\end{figure}

\begin{figure}
     \centering
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/out_domain/burger/1.png}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/out_domain/burger/2.jpg}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/out_domain/burger/result.png}
     \end{subfigure}
        \caption{We provide qualitative
results on out of domain cartoon data \citep{weber2023toon3d}. }
        \label{burger}
\end{figure}

\section{Evaluation result on CAD-Estate dataset}

We conducted an additional evaluation on the CAD-Estate dataset \cite{rozumnyi2023estimatinggeneric3droom}. CAD-Estate is derived from RealEstate10K dataset\cite{zhou2018stereo} and provides generic 3D room layouts from 2D segmentation masks. Due to differences in annotation standards between CAD-Estate and Structured3D, we selected a subset of the original data that aligns with our experimental setup. Our method and Structured3D assume a single floor, single ceiling, and multiple walls configuration. In contrast, CAD-Estate includes scenarios with multiple ceilings (particularly in attic rooms) and interconnected rooms through open doorways, whereas Structured3D treats doorways as complete walls. To ensure a fair comparison, we sampled 100 scenes containing 469 images that closely match Structure3D's annotation style. Each scene contains 2 to 10 images.

Since CAD-Estate only provides 2D segmentation annotations without 3D information, we report performance using 2D metrics: IoU and pixel error. While CAD-Estate's label classes include ["ignore", "wall", "floor", "ceiling", "slanted"], we only focus on wall, floor, and ceiling classes. We utilize the dataset's provided intrinsic parameters for reprojection during evaluation. Results are reported for both "Noncuboid + GT pose" and "Plane-DUSt3R (metric)" in Table~\ref{tab:cad}. We visualize our results in Figure~\ref{fig:cad-1} and Figure~\ref{fig:cad-2}

\begin{table}[h]
    \centering
    \scriptsize
    \caption{\xd{Quantitative results with on CAD-estate dataset.} }
    \label{tab:cad}
    \begin{tabular}{ccc} 
        \toprule
          \textbf{Method}& \textbf{re-IoU(\%)}$\uparrow$& \textbf{re-PE(\%) }$\downarrow$\\ 
         \hline
         Noncuboid + GT pose & 55.99 & 20.33 \\
         Ours (metric) & \textbf{63.14} & \textbf{15.15}   \\ 
         
    \bottomrule
    \end{tabular}
\end{table}

\begin{figure}[h]
     \centering
     \begin{subfigure}[b]{0.73\textwidth}
         \centering
          \includegraphics[width=0.24\linewidth,valign=c]{figs/cad/case1/1.jpg}
          \includegraphics[width=0.24\linewidth,valign=c]{figs/cad/case1/2.jpg}
          \includegraphics[width=0.24\linewidth,valign=c]{figs/cad/case1/3.jpg}
          \includegraphics[width=0.24\linewidth,valign=c]{figs/cad/case1/4.jpg} \\
          \includegraphics[width=0.24\linewidth,valign=c]{figs/cad/case1/gt_1.png}
          \includegraphics[width=0.24\linewidth,valign=c]{figs/cad/case1/gt_2.png}
          \includegraphics[width=0.24\linewidth,valign=c]{figs/cad/case1/gt_3.png}
          \includegraphics[width=0.24\linewidth,valign=c]{figs/cad/case1/gt_4.png} \\
          \includegraphics[width=0.24\linewidth,valign=c]{figs/cad/case1/pred_1.png}
          \includegraphics[width=0.24\linewidth,valign=c]{figs/cad/case1/pred_2.png}
          \includegraphics[width=0.24\linewidth,valign=c]{figs/cad/case1/pred_3.png}
          \includegraphics[width=0.24\linewidth,valign=c]{figs/cad/case1/pred_4.png}
         \caption{}
         
     \end{subfigure}
     % \hfill
     \begin{subfigure}[b]{0.21\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/cad/case1/result.png}\\
         \includegraphics[width=\textwidth]{figs/cad/case1/result_wireframe.png}
         \caption{}
     \end{subfigure}
    \caption{\xd{Visualization of results from the CAD-Estate dataset. (a) Input views are shown in the top row, followed by CAD-Estate's ground-truth segmentation in the middle row, and our predicted segmentation in the bottom row. (b) Our 3D reconstruction results displayed with point clouds (top row) and wireframe renderings (bottom row).}}
    \vspace{-1em}
    \label{fig:cad-1}
\end{figure}

\begin{figure}[h]
     \centering
     \begin{subfigure}[b]{0.73\textwidth}
         \centering
          \includegraphics[width=0.24\linewidth,valign=c]{figs/cad/case2/1.jpg}
          \includegraphics[width=0.24\linewidth,valign=c]{figs/cad/case2/2.jpg}
          \includegraphics[width=0.24\linewidth,valign=c]{figs/cad/case2/3.jpg}
          \includegraphics[width=0.24\linewidth,valign=c]{figs/cad/case2/4.jpg} \\
          \includegraphics[width=0.24\linewidth,valign=c]{figs/cad/case2/gt_1.png}
          \includegraphics[width=0.24\linewidth,valign=c]{figs/cad/case2/gt_2.png}
          \includegraphics[width=0.24\linewidth,valign=c]{figs/cad/case2/gt_3.png}
          \includegraphics[width=0.24\linewidth,valign=c]{figs/cad/case2/gt_4.png} \\
          \includegraphics[width=0.24\linewidth,valign=c]{figs/cad/case2/pred_1.png}
          \includegraphics[width=0.24\linewidth,valign=c]{figs/cad/case2/pred_2.png}
          \includegraphics[width=0.24\linewidth,valign=c]{figs/cad/case2/pred_3.png}
          \includegraphics[width=0.24\linewidth,valign=c]{figs/cad/case2/pred_4.png}
         \caption{}
         
     \end{subfigure}
     % \hfill
     \begin{subfigure}[b]{0.21\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/cad/case2/result.png}\\
         \includegraphics[width=\textwidth]{figs/cad/case2/result_wireframe.png}
         \caption{}
     \end{subfigure}
    \caption{\xd{Visualization of results from the CAD-Estate dataset. (a) Input views are shown in the top row, followed by CAD-Estate's ground-truth segmentation in the middle row, and our predicted segmentation in the bottom row. (b) Our 3D reconstruction results displayed with point clouds (top row) and wireframe renderings (bottom row).}}
    \vspace{-1em}
    \label{fig:cad-2}
\end{figure}

\end{document}
