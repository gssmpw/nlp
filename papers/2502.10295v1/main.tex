%%%%%%%% ICML 2024 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{subcaption}
\usepackage{multirow}
% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2024} with \usepackage[nohyperref]{icml2024} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2024}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2024}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


\usepackage{comment}

\renewcommand{\topfraction}{0.95}
\renewcommand{\textfraction}{0.05}
\renewcommand{\floatpagefraction}{0.95}
\usepackage{placeins} % If using FloatBarrier


\newcommand{\andre}[1]
{{\color{red}\textbf{[AM: #1]}}}
\newcommand{\mario}[1]
{{\color{teal}\textbf{[MF: #1]}}}
\newcommand{\sophia}[1]
{{\color{green!80!black}\textbf{[SS: #1]}}}
\newcommand{\antonio}[1]
{{\color{green!60!black}\textbf{[AF: #1]}}}
\newcommand{\sweta}[1]
{{\color{orange!60!black}\textbf{[SA: #1]}}}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Fenchel-Young Variational Learning}

\begin{document}

\twocolumn[
\icmltitle{Fenchel-Young Variational Learning}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2024
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Sophia Sklaviadis}{yyy,xxx}
\icmlauthor{Sweta Agrawal}{yyy}
\icmlauthor{Antonio Farinhas}{yyy,xxx}
\icmlauthor{Andre Martins}{yyy,xxx,zzz,comp}
\icmlauthor{Mario Figueiredo}{yyy,xxx,comp}
% \icmlauthor{Firstname6 Lastname6}{sch,yyy,comp}
% \icmlauthor{Firstname7 Lastname7}{comp}
%\icmlauthor{}{sch}
% \icmlauthor{Firstname8 Lastname8}{sch}
% \icmlauthor{Firstname8 Lastname8}{yyy,comp}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{yyy}{Instituto de Telecomunicacoes, Lisbon, Portugal}
\icmlaffiliation{xxx}{Instituto Superior Técnico, Universidade de Lisboa, Lisbon, Portugal}
\icmlaffiliation{zzz}{Unbabel, Lisbon, Portugal}
\icmlaffiliation{comp}{Lisbon ELLIS Unit (LUMLIS), Lisbon, Portugal}
% \icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

\icmlcorrespondingauthor{Sophia Sklaviadis}{ssklaviadis@gmail.com}
% \icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

% Statistical learning criteria can be viewed from a unifying, abstract variational perspective: find a distribution of the unknown variables/parameters that strikes a balance between minimizing an empirical risk and fitting a regularizer that expresses prior knowledge. \andre{I don't like this first sentence; it's long and not to the point. I suggest ``Many statistical learning objectives can be seen from a variational perspective as finding a distribution that strikes a balance between an empirical risk and a regularizer.''}
% In this paper, we broaden this perspective by considering general classes of entropic regularizers, which are at the core of Fenchel-Young losses. 
% The proposed framework is thus termed Fenchel-Young variational learning. 
% We describe alternating minimization and gradient backpropagation algorithms to compute (or lower bound) a new Fenchel-Young evidence, which enables fitting a wider class of models to data. 
% \andre{how about describing this instead as ``In this paper, we broaden this perspective by considering general classes of  regularizers associated with a Fenchel-Young loss. Our proposed method---dubbed \textit{Fenchel-Young variational learning}---includes as key ingredients a new notion of Fenchel-Young free energy, evidence, and posterior. We derive alternating minimization and gradient backpropagation algorithms to compute (or lower bound) this generalized evidence, which enables fitting a wider class of models to data.}
% We use our framework to derive new generalized FY-variants of classical machine learning algorithms: expectation-maximization (EM), yielding a new FYEM algorithm; and a new FY-variational autoencoder (FYVAE) \andre{I also don't like this sentence, which is mixing algorithms (EM) with models (VAE). I am also not sure we are deriving a new algorithm---we're applying alternating minimization to minimize our variational objective and the result is a EM-like algorithm. I suggest ``This leads to generalized FY-variants of classical machine learning algorithms (such as a FY expectation-minimization algorithm) and latent variable models (such as a FY variational autoencoder)''}. 
\begin{abstract}

From a variational perspective, many statistical learning criteria involve seeking a distribution that balances empirical risk and regularization. In this paper, we broaden this perspective by introducing a new general class of variational methods based on Fenchel-Young (FY) losses, treated as divergences that generalize (and encompass) the familiar Kullback-Leibler divergence at the core of classical variational learning. Our proposed formulation---\textit{FY variational learning}---includes as key ingredients new notions of FY free energy, FY evidence, FY evidence lower bound, and FY posterior.
We derive alternating minimization and gradient backpropagation algorithms to compute (or lower bound) the FY evidence, which enables learning a wider class of models than previous variational formulations. 
This leads to generalized FY variants of classical algorithms, such as an FY expectation-maximization (FYEM) algorithm, and latent-variable models, such as an FY variational autoencoder (FYVAE).
Our new methods are shown to be empirically competitive, often outperforming their classical counterparts, and most importantly, to have qualitatively novel features. For example, FYEM has an adaptively sparse E-step, while the FYVAE can support models with sparse observations and sparse posteriors. 
  
\end{abstract}

\section{Introduction}
\label{submission}
Much of modern machine learning can be formulated as a variational (optimization) problem with the goal of minimizing a regularized empirical risk over a training set, where the regularizer encourages the solution \textit{(e.g.}, learned parameters) to have certain properties. In Bayesian formulations, all the unknowns (e.g., parameters to be learned and latent/hidden variables) are seen as random variables and learning becomes synonymous with finding the conditional distribution  of the unknowns given the observed data.
% From a probabilistic perspective, this estimate can be seen as a \textit{maximum likelihood} (ML) estimate (in the absence of regularization, by interpreting the loss as the negative log-likelihood of the parameters, given the observed data) or a \textit{maximum a posteriori} (MAP) estimate, if some regularizer (e.g., weight decay) is used \andre{maybe delete this sentence so that we can go straight to the point}. 
% This \textit{empirical risk minimization} (ERM) approach does not provide uncertainty quantification of the parameter estimate, motivating research using Bayesian inference tools in large-scale machine learning problems \citep{pmlr-v235-papamarkou24b} \andre{we also don't do any UQ in our paper; consider removing this sentence as well, so that the second sentence goes straight to Zellner}. \mario{I agree; let's just remove this first sentence.}
Inspired by and based on the variational formulation of Bayesian inference due to \citet{Zellner1988} and the generalized loss posteriors of \citet{bissiri2016general}, \citet{JMLR:v24:22-0291} unify diverse, well-established Bayesian inference algorithms under a common formal umbrella: finding the (generalized) posterior distribution of the unknown parameters or latent variables, 
% \andre{might be confusing to talk about ``parameters'' here}
% \mario{I  think it is more confusing to talk about latent variables, as all models have parameters but not all have latent variables; in fact, in the basic Khan and Rue formulation, there are only parameters, no latent variables.} 
% \sophia{I added the latent var disjunction -- we've been talking about whether or not it's a good idea to reset the theory as generalizing Bayesian inference on global/local parameters and Andre said since the experiments are local param stick with latent vars} 
in some allowed family, that strikes a balance between yielding a low expected risk and staying close (in a Kullback-Leibler divergence sense) to some given distribution (a prior). This formulation reduces to standard Bayesian learning if the expected risk is simply the expected negative log-likelihood, and the family of allowed distributions contains all possible distributions. 

In this paper, we extend the variational formulations of (generalized) posterior inference studied along related lines by \citet{bissiri2016general}, \citet{knoblauch2019generalized} and \citet{JMLR:v24:22-0291}, by considering alternative optimality criteria that exploit the class of Fenchel-Young losses \citep{Blondel2020, martins2022sparse}. We use Fenchel-Young losses as divergences between prior and variational posterior distributions. Our estimation methodology strictly generalizes those of \citet{knoblauch2019generalized} and \citet{JMLR:v24:22-0291}, leading to a wider class of statistical families. We consider in detail models with sparse posteriors, among the expansive range of scenarios made possible within the proposed Fenchel-Young variation learning setting.  

The Kullback-Leibler term in the standard variational formulation of posterior inference is equivalent to an entropic regularizer  that favors posterior distributions of high Shannon entropy \citep{Jordan1999}. Our broader formulation is obtained by replacing the Shannon entropy with more general families, such as the Tsallis $\alpha$-entorpies
% and Rényi entropies \andre{maybe skip Rényi? we don't do anything with them and there are some nonconvexity annoyances}\mario{I agree, let's drop the mention to Rényi.} 
\citep{Blondel2020}. This generalization allows us to obtain 
% both heavy-tailed and 
sparse (limited support) discrete and continuous posteriors. In one class of cases, including fixed exponential family priors, the posterior support can be strictly smaller than that of the prior and the likelihood function, which may be of interest when it is important to concentrate the attention of the posterior on a strict subset of the set in which it is defined. 
% \sophia{I'm not sure about this sentence bc it seems to be skipping cases where the prior itself is deformed or conjugate to deformed}

In \S\ref{sec:fy_variational}, we propose a generalized \textit{Fenchel-Young variational learning} (FYVL) framework, including a natural EM-style algorithm for parameter estimation. We also describe a gradient backpropagation approach for amortized inference. Both of these estimation methods can be used to fit arbitrary generative models that may be specified as instances of FYVL. Notably, the proposed FYVL framework brings with it new generalized versions of classical objects in variational learning: FY free energy, FY evidence, FY posterior, and FY ELBO (evidence lower bound). 


In \S\ref{sec:experiments} we derive and experiment with three new models, two of which illustrate cases of sparse posteriors: FYEM algorithms for \textit{Gaussian mixture model} (GMM) estimation and two FY {variational autoencoders} (FYVAE), one for images and the other for documents. The FYEM algorithm for GMM recovers the classical EM when the FY loss uses the Shannon entropy as the regularizer, and a new version with an adaptively sparse E-step, when the regularizer is the Tsallis $\alpha$-entropy, for $\alpha>1$. In the FYVAEs, we obtain the best performance when we combine a sparse posterior with a sparse observation model. We also experiment with a neural variational document model (NVDM; \cite{miao2016neural}) in which we tune the sparsity of the observation model, where we use a Fenchel-Young loss in place of a negative log-likelihood, combined with a common Gaussian latent variable. Our code will be publicly released upon acceptance. 
 
% \sophia{q-gaussian}

\section{Background}

\subsection{Definitions and Notation}
We use capital letters to represent random variables: $X$ for observations and $Z$ for latent variables. These random variables each take values $x \in \mathcal{X}$ and $z \in \mathcal{Z}$, which can be discrete or continuous spaces. We denote by $p_Z(z)$ and $p_{X|Z}(x | z)$ the \textit{prior} and the \textit{likelihood function} (seen as a function of $z$, for given $x$), respectively, and by $p_X(x)$ and $p_{Z|X}(z | x)$ the evidence and the posterior distribution, respectively, related through Bayes law: 
\begin{align}\label{eq:bayes}
p_{Z|X}(z | x) = \frac{p_{X|Z}(x | z) p_Z(z)}{p_X(x)}.
\end{align}
The space of probability densities or mass functions over a set $\mathcal{S}$ is denoted as $\Delta(\mathcal{S})$ (the probability simplex, for finite $\mathcal{S}$).  For $r\!\in\! \Delta(\mathcal{Z})$, $H(r) = \mathbb{E}_{Z\sim r} \left[-\log r(Z)\right]$ is the Shannon (differential, in the continuous case) entropy \citep{Cover}. For $r,s \in \Delta(\mathcal{Z})$, the Kullback-Leibler divergence (KLD) between $r$ and $s$ is given by $D_{\text{\footnotesize KL}}(r\|s) = \mathbb{E}_{Z\sim r} [\log (r(Z) / s(Z))]$; it is guaranteed that $D_{\text{\footnotesize KL}}(r\|s) \geq 0$, and zero if and only if $r = s$ (everywhere, for discrete $\mathcal{Z}$; almost everywhere, for continuous $\mathcal{Z}$) \citep{Cover}. Finally, we define the support of some $r \in  \Delta(\mathcal{Z})$ as supp$(r) =\{z \in \mathcal{Z}: \; r(z) > 0\}$.

\subsection{Variational View of Bayesian Learning}
As shown by \citet{Zellner1988} (see also the recent note by \citet{Thanh2021}), the posterior $p_{Z|X}$ and the evidence $p_X$ 
% \sophia{clarify why evidence} 
in \eqref{eq:bayes} can be expressed in variational form as the distribution in $\Delta(\mathcal{Z})$ that is optimally compatible with: (\textit{i}) the prior $p_Z$, in a  Kullback-Leibler divergence sense; and (\textit{ii}) the expected likelihood $p_{X | Z}$. Formally, 
\begin{align}
   \lefteqn{p_{Z|X}(\cdot | x) }  \nonumber \\
    &= \arg\!\! \max_{q \in \Delta(\mathcal{Z})} \underset{Z\sim q}{\mathbb{E}} \bigl[ \log  p_{X|Z}(x | Z) \bigr] \! - \!D_{\text{\footnotesize KL}}(q \| p_Z)  \nonumber  \\
    & = \arg\!\! \max_{q \in \Delta(\mathcal{Z})} \underset{Z\sim q}{\mathbb{E}} \bigl[ \log p_{X,Z}(x , Z)\bigr] \! + \!  H(q). \label{eq:BLR1}
\end{align}
The optimal value of the objective in this variational problem (achieved by $p_{Z|X}(\cdot|x)$) is the log-evidence $\log p_X(x)$, \textit{i.e.},
\[
\underset{Z\sim p_{Z|X}(\cdot|x)} {\mathbb{E}} \bigl[ \log p_{X,Z}(x , Z)\bigr] + H\bigl( p_{Z|X}(\cdot|x)\bigr) = \log p_X(x). 
\]

In \textit{variational inference} (VI) \citep{Jordan1999,Blei2017}, the goal is to approximate the posterior and bound the evidence (when these are intractable), by confining $q$ to a tractable family $\mathcal{Q} \subset \Delta(\mathcal{Z})$, rather than to the complete space $\Delta(\mathcal{Z})$. In this case, 
\begin{equation}    
q_x = \arg\max_{q\in\mathcal{Q}} \underset{Z\sim q}{\mathbb{E}} \bigl[ \log p_{X,Z}(x , Z)\bigr]  +   H(q), \label{eq:VI}
\end{equation}
and the optimal value of the variational objective is the ELBO (\textit{evidence lower bound}):
\[
\underset{Z\sim q_x } {\mathbb{E}} \bigl[ \log p_{X,Z}(x , Z)\bigr] + H\bigl( q_x \bigr) \leq \log p_X(x).
\]
The negative of the objective function in \eqref{eq:VI} is often called the \textit{variational free energy} (VFE).

\citet{JMLR:v24:22-0291} use the extension of the variational formulation by \citet{bissiri2016general} to generalized posteriors in order to derive, as special cases, much of the core catalog of modern machine learning algorithms and to propose new ones. This extension consists of replacing the negative log-likelihood with an arbitrary loss function $\ell(x; Z)$, yielding 
\begin{align}
    q_x^\star & =  \arg\min_{q\in\mathcal{Q}} \underset{Z\sim q}{\mathbb{E}} \left[  \ell(x; Z) \right]  + D_{\text{\footnotesize KL}}(q \| p_Z) \label{eq:BLR3} \\    
     & =  \arg\min_{q\in\mathcal{Q}} \underset{Z\sim q}{\mathbb{E}} \left[  \ell(x; Z) - \log p_Z(Z) \right] - H(q), \label{eq:BLR4} 
\end{align}
where the notation $q_x^\star$ highlights the dependency on the observations $x$.
Choosing $\ell(x; z) = -\log p_{X|Z}(x | z)$ and $\mathcal{Q} = \Delta(\mathcal{Z})$ recovers \eqref{eq:BLR1} as a particular case, with $q_x^\star = p_{Z|X}(\cdot | x).$  Furthermore, \citet{JMLR:v24:22-0291} restrict $\mathcal{Q}$ to be a regular minimal exponential family, i.e., $\mathcal{Q} = \{ q(z) = h(z) \exp\bigl( \langle \lambda, t(z) \rangle\bigr) / Z(\lambda) , \mbox{for } \lambda \in \Lambda \}$, where $\Lambda$ is the parameter space, $t$ is a canonical sufficient statistic, $h$ is the base measure (independent of $\lambda$), and $Z(\lambda)$ is the partition function.  


\subsection{Fenchel-Young Losses}
A parallel, previously unrelated, line of research concerns \textit{Fenchel-Young (FY) losses} \citep{Blondel2020}, a class of losses that generalize the KLD and can be seen as mixed-space variants of Bregman divergences \citep[\S 3.2]{amari2016information,Blondel2020}. These losses have been used in supervised classification problems, but not in the context of variational inference or latent variable models. 

\begin{comment}
\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{fy.png}
    \caption{\andre{this fig would need some work and it's introducing some notation / concepts we don't use in the paper. since we're beyond the page the limit I'd remove the figure} When $\Omega$ is the Shannon entropy $D_\Omega$ is the KL divergences. 
    % \andre{we should also consider having a figure/diagram that explains both our method and the key ingredients, $\Omega$, $q$, $\eta$, etc.}
    }
    \label{}
\end{figure}
\end{comment}

Let $\Omega: \Delta(\mathcal{Z}) \rightarrow \bar{\mathbb{R}}$ be a lower semi-continuous (l.s.c.) proper convex function, and let   
$\Omega^*$ be its Fenchel-Legendre conjugate%
\footnote{Given a function $f:\mathcal{U}\rightarrow \bar{\mathbb{R}}$, its Fenchel conjugate $f^*:\mathcal{U}^*\rightarrow \bar{\mathbb{R}}$ is defined as $f^*(x) = \sup_{u\in\mathcal{U}} (\langle x,u\rangle - f(u)),$ where $\mathcal{U}^*$ is the dual space of $\mathcal{U}$. If $f$ is convex l.s.c., biconjugation holds: $f^{**} = f$. If $\mathcal{U}$ is compact or $f$ is continuous, sup may be replaced with max, because the supremum is achieved. In this case, assuming differentiability, the maximizing argument in the definition of $f^*$ is $\arg\max_{u\in\mathcal{U}} (\langle x,u\rangle - f(u)) = \nabla f^*(x)$.} %
\citep{Bauschke,Borwein}. 
Given a scoring function $\eta: \mathcal{Z} \rightarrow \mathbb{R}$, 
and a distribution $q \in \Delta(\mathcal{Z})$, 
the FY loss induced by $\Omega$ 
is a measure of how ``incompatible'' $\eta$ and $q$ are. It is written as 
\begin{align}\label{eq:LOmega}
L_\Omega(\eta; q) := \Omega^*(\eta) - \mathbb{E}_{Z \sim q}[\eta(Z)] + \Omega(q).
\end{align}
An FY loss is always non-negative, due to the Fenchel-Young inequality \citep{Bauschke}; it is zero if and only if $q = \nabla \Omega^*(\eta)$. In this context, the gradient map $\nabla \Omega^*$ is called \textit{$\Omega$-regularized prediction map} \citep{Blondel2020} and it is given by
\begin{equation}
    \nabla \Omega^*(\eta) = \arg\max_{q \in \Delta(\mathcal{Z})} \mathbb{E}_{Z \sim q}[\eta(Z)] - \Omega(q). \label{eq:gradientmap}
\end{equation}
For example, in the finite domain case, $\mathcal{Z}= \{ 1, ..., K\}$, the scoring function defines the elements of the vector of label scores (or logits) $\eta = ( \eta_1, ..., \eta_K)$. Different choices of regularizers $\Omega$ recover well-known maps: for $\Omega(q)= - H(q)$, we obtain $\nabla \Omega^*(\eta) = \mbox{softmax}(\eta)$ and $L_\Omega(\eta; q) = D_\mathrm{KL}(q \| \mbox{softmax}(\eta))$; for $\Omega(q)= \frac{1}{2}\|q\|_2^2$, $\nabla \Omega^*(\eta) = \mbox{sparsemax}(\eta)$, which corresponds to the Euclidean projection of $\eta$ onto $\Delta(\mathcal{Z})$ \citep{Martins2016}. Whereas supp $(\mbox{softmax}(\eta)) = \mathcal{Z}$, it may happen that supp$(\mbox{sparsemax}(\eta))$ is a strict subset of $\mathcal{Z}$, \textit{i.e.}, the distribution in question may have sparse support. 

An apt choice for $\Omega$ is the \textit{generalized Tsallis negentropy} (negative entropy) \citep{tsallis2009introduction}, defined as 
\begin{align}\label{eq:tsallis}
    \Omega_\alpha(q) := \left\{
    \begin{array}{ll}
       \frac{\displaystyle \mathbb{E}_{Z \sim q}[q(Z)^{\alpha-1} - 1]}{\alpha(\alpha-1)} , & \text{if $\alpha \ne 1$,} \\
       \mathbb{E}_{Z \sim q}[\log q(Z)],
         & \text{if $\alpha = 1$,} 
    \end{array}
    \right.
\end{align}
where $\alpha>0$ is a scalar parameter. Tsallis negentropies are continuous on $\alpha$ and they recover Shannon's negentropy for $\alpha=1$. The $\Omega_\alpha$-regularized prediction map $\nabla \Omega_\alpha^*$ is called $\alpha$-entmax \citep{peters2019sparse} and recovers softmax, for $\alpha=1$, $\nabla \Omega_1^*(\eta) = \mbox{softmax}(\eta)$, and sparsemax, for $\alpha=2$, $\nabla \Omega_2^*(\eta) = \mbox{sparsemax}(\eta)$.



% : 
% \begin{equation}
% \begin{split} 
% \alpha\mbox{-entmax}(\eta(Z)) = \arg\max_{q \in \Delta(\mathcal{Z})} 
% \end{split} 
% \end{equation}



\section{Fenchel-Young Variational Learning}\label{sec:fy_variational}
\subsection{Generalized Variational Posteriors}
The variational formulation in \eqref{eq:BLR1} shows explicitly that the Bayesian posterior maximizes the expectation of the log of the joint distribution plus the entropy, revealing that a maximum entropy criterion is at the heart of the classical Bayes rule. In this section, we generalize this formulation by combining variational inference and Fenchel-Young losses, which are used to generalize the KDL. We extend the maximum Shannon entropy regularization in \eqref{eq:BLR1} by using wider families of entropies (that contain Shannon's as a particular case), such as Tsallis (neg)entropy. 
% \andre{Suggestion: rephrase the previous sentence as: ``In this section, we show how we can generalize this framework by combining variational inference and Fenchel-Young losses \S\ref{sec:fy}.'' In the discussion / conclusions we can also comment ``We presented our contribution as generalizing variational inference by replacing entropy and the cross-entropy loss with an arbitrary convex function $\Omega$ and its corresponding Fenchel-Young loss $L_\Omega$, effectively combining these two ideas. We can equivalently regard our contribution from the oppositive perspective, as expanding the scope of Fenchel-Young losses---originally proposed in the context of fully observed models---to latent variable models.}

Substituting $-H(q)$ in the variational formulation of Bayes's law in \eqref{eq:BLR4} with a generic $\Omega(q)$ amounts to generalizing the KLD between the variational posterior and the prior $p_Z$ with a Fenchel-Young regularizer induced by $\Omega$. 

% For example, if the prior belongs to the exponential family $\log p_Z(z) \propto \langle \lambda, t(z) \rangle$ its scoring function $\eta(z) = \langle \lambda, t(z)\rangle$ is used to generate the corresponding $\Omega$-regularized prediction map $\hat q_\Omega (\eta)$ and its associated FY loss $L_\Omega(\eta; q)$ \andre{this example also takes time to parse. if we need to invoke exponential families perhaps we can have this notation with suff stats and canonical parameters early on in the background (I am not sure we need it, though)}. 

% with $\Omega^*$ being the Fenchel conjugate of $\Omega$. If the prior belongs to an exponential family, the scoring function is $\eta(z) = \langle \lambda, t(z)\rangle$ and $\Omega^*$ is the cumulant function \citep{wainwright2008graphical}. 

% The fact that the first argument of this FY loss is the log-prior shows that the scoring function introduced above can be seen as an analog to a log-prior, although in general, it may lack that probabilistic interpretation. If the prior belongs to an exponential family, the scoring function is $\eta(z) = \langle \lambda, t(z)\rangle$ and $\Omega^*$ is the cumulant function \citep{wainwright2008graphical}. 

% The preferred way is to number definitions, propositions, lemmas, etc. consecutively, within sections, as shown below.
% \begin{definition}
% \label{def:inj}
% A function $f:X \to Y$ is injective if for any $x,y\in X$ different, $f(x)\ne f(y)$.
% \end{definition}
% Using \cref{def:inj} we immediate get the following result:
% \begin{proposition}
% If $f$ is injective mapping a set $X$ to another set $Y$, 
% the cardinality of $Y$ is at least as large as that of $X$
% \end{proposition}
% \begin{proof} 
% Left as an exercise to the reader. 
% \end{proof}
% \cref{lem:usefullemma} stated next will prove to be useful.
% \begin{lemma}
% \label{lem:usefullemma}
% For any $f:X \to Y$ and $g:Y\to Z$ injective functions, $f \circ g$ is injective.
% \end{lemma}
% \begin{theorem}
% \label{thm:bigtheorem}
% If $f:X\to Y$ is bijective, the cardinality of $X$ and $Y$ are the same.
% \end{theorem}
% An easy corollary of \cref{thm:bigtheorem} is the following:
% \begin{corollary}
% If $f:X\to Y$ is bijective, 
% the cardinality of $X$ is at least as large as that of $Y$.
% \end{corollary}
% \begin{assumption}
% The set $X$ is finite.
% \label{ass:xfinite}
% \end{assumption}
% \begin{remark}
% According to some, it is only the finite case (cf. \cref{ass:xfinite}) that is interesting.
% \end{remark}

We propose a Fenchel-Young variational learning (FYVL) generalization of Bayesian inference by introducing a novel general class of posterior distributions of the following form: 
\begin{align}
        q_x^\star  &:=  \arg\min_{q \in \mathcal{Q}} \underset{Z\sim q}{\mathbb{E}}  \left[\ell(x; Z)\right]  + L_\Omega(\eta; q) \label{eq:fylr0} \\
        & \, = 
        \arg\min_{q \in \mathcal{Q}} \underset{Z\sim q}{\mathbb{E}} \left[  \ell(x; Z) - \eta(Z) \right] + \Omega(q). \label{eq:fylr}
\end{align}

% \begin{align}
%         q_x^\star  &:=  \arg\min_{q \in \mathcal{Q}} \underset{Z\sim q}{\mathbb{E}}  \left[\ell(x; Z)\right]  + L_\Omega(\eta; q) \label{eq:fylr0} \\
%         & \, = 
%         \arg\min_{q \in \mathcal{Q}} \underset{Z\sim q}{\mathbb{E}} \left[  \ell(x; Z) - \eta(Z) \right] + \Omega(q). \label{eq:fylr}
% \end{align}

\begin{comment}
It is common for the loss $\ell$ and/or the score $\eta$ to depend on parameters that also need to be estimated/learned from the observations $x$. In that case, $\Omega^*({ \eta})$ may not be constant, and therefore it is left in \eqref{eq:fylr} \andre{this justification is premature, I would probably remove it. Note that $\Omega^*({ \eta})$ is always a constant wrt $q$ (it does not contribute to the E step), we only need this term when we add another minimization wrt to $\theta$ and $\eta$ in the M-step of EM}. 
\end{comment}

As mentioned by \citet[\S 3.2]{Blondel2020}, $L_\Omega$ is intimately linked to Bregman divergences, and can be seen as a ``mixed space'' variant of those divergences (although Fenchel-Young losses are more general since they do not require $\Omega$ to be a Legendre-type function). We can thus regard \eqref{eq:fylr0} as replacing the KLD regularizer in \eqref{eq:BLR3} with a Bregman-like divergence. 


\subsection{Fenchel-Young Free Energy, Evidence, Evidence Lower Bound, and Posterior}
By analogy with \eqref{eq:VI}, the objective function in \eqref{eq:fylr0} can be seen as a \textit{Fenchel-Young variational free energy} (FYVFE),
\begin{align}\label{eq:fy_free_energy}
    F_{\ell, \Omega}(q, \eta ; x) = \underset{Z \sim q}{\mathbb{E}} \left[\ell(x; Z)\right] + L_{\Omega}(\eta; q ),
\end{align}
its minimum as a (negative) \textit{Fenchel-Young evidence},
\begin{align}\label{eq:fy_evidence}
    J_{\ell, \Omega}(x,\eta) = \min_{q\in\mathcal{Q}}  F_{\ell, \Omega}(q,\eta; x) = F_{\ell, \Omega}(q_x^\star, \eta ; x),
\end{align}
and its minimizer $q_x^\star$ as the \textit{Fenchel-Young posterior}. 

% \begin{align}\label{eq:fy_free_energy}
%     F_{\ell, \Omega}(q, \eta ; x) = \underset{Z \sim q}{\mathbb{E}} \left[\ell(x; Z)\right] + L_{\Omega}(\eta; q ),
% \end{align}

By construction, $-J_{\ell, \Omega}(x,\eta) \ge  -F_{\ell, \Omega}(q,\eta; x)$, for any $q \in \Delta(\mathcal{Z}),$ thus $-F_{\ell, \Omega}(q,\eta; x)$ is a \textit{Fenchel-Young evidence lower bound} (FY-ELBO).


% its minimum as a (negative) \textit{Fenchel-Young evidence},
% \begin{align}\label{eq:fy_evidence}
%     J_{\ell, \Omega}(x,\eta) = \min_{q\in\mathcal{Q}}  F_{\ell, \Omega}(q,\eta; x) = F_{\ell, \Omega}(q_x^\star, \eta ; x),
% \end{align}
% and its minimizer $q_x^\star$ as the \textit{Fenchel-Young posterior}. 
% \andre{in the eq we have $*$ and here we have $\star$, we need to uniformize. I generally prefer $\star$ for minimizers and $*$ for conjugacy, which is also what we used in \cite{Blondel2020}.} 
% By construction, $-J_{\ell, \Omega}(x,\eta) \ge  -F(q,\eta; x), \quad \forall q \in \Delta(\mathcal{Z}),$ thus we have a \textit{Fenchel-Young evidence lower bound} (FY-ELBO).
%When $\Psi$ and $\Omega$ are both negative Shannon entropies, $-J_{\Psi, \Omega}(x)$ and $ -F(p; x)$ reduce respectively to the regular evidence, $\log p(x)$, and the Bayesian posterior, $p(z | x)$. 
For $\mathcal{Q}=\Delta(\mathcal{Z})$, $\Omega(q)=\Omega_1(q)$ (Shannon negentropy), and $\ell(x; z) = -\log p_{X|Z}(x|z)$, the Fenchel-Young evidence becomes the regular evidence and the Fenchel-Young posterior recovers the Bayesian posterior $p_{Z|X}(z|x)$. 

It is often useful to include an additional parameter $\beta$ weighting the KLD regularizer in \eqref{eq:BLR3} or the Fenchel-Young loss in \eqref{eq:fy_free_energy}, as in the $\beta$-version of variational auto-encoders ($\beta$-VAE) proposed by \citet{Higgins2016betaVAELB}. The $\beta$-version of FYVL relies on a generalization of the FYVFE:
\begin{align}\label{eq:fy_free_energybeta}
    F_{\ell, \Omega, \beta}(q, \eta ; x) = \underset{Z \sim q}{\mathbb{E}} \left[\ell(x; Z)\right] + \beta L_{\Omega}(\eta; q ).
\end{align}
In the sequel, omitting $\beta$ means assuming $\beta=1$. Parameter $\beta$ can be seen from an information-theoretic perspective as controlling a rate-distortion trade-off \citep{alemi2018information}. 



\subsection{Fenchel-Young EM (FYEM) Algorithm}\label{sec:FYEM}
The loss function is commonly parameterized, and thus we write it as $\ell(x; z, \theta)$, where $\theta$ is a vector of parameters to be learned from observed data. Suppose we have a tuple of $N$ observations $x = (x_1, ..., x_N)$, associated to which there is a corresponding collection of latent variables  $z = (z_1, ..., z_N)$, and their variational distributions $q = (q_1, ..., q_N)$, where $z_i \in \mathcal{Z}$ and $q_i\in \Delta(\mathcal{Z})$. %Finally, $\eta$ may also be unknown and estimated from the observations. 
The learning problem then corresponds to minimizing the FYVFE \eqref{eq:fy_free_energy} w.r.t. $q$ and $\theta$: 
\begin{align}
    \hat{q}, \hat{\theta} &= \arg\min_{q, \theta} \sum_{i=1}^N F_{\ell_\theta, \Omega}(q_i, \eta; x_i)  \label{eq:joint_minimization} \\
    &\hspace{-1cm} = \arg\min_{q, \theta} \sum_{i=1}^N \Bigl( \underset{Z_i \sim q_i}{\mathbb{E}} \left[\ell(x_i; Z_i, \theta)\right] + L_{\Omega}(\eta; q_i ) \Bigr). \nonumber
\end{align}
%\andre{maybe add $\hat{\eta}$ to the LHS in the eq above; and write $F_{\theta, \Omega}$ to make $\theta$ explicit in the RHS}

%Our objective is to minimize \eqref{eq:fy_elbo1} with respect to $\theta$ and ${q}$. 
Alternating minimization w.r.t. to $q$ and $\theta$ leads to an EM-style algorithm: the E-step computes the distributions of the latent variables (as in standard EM, each $q_i$ can be obtained independently of the others), whereas the M-step updates the estimates of $\theta$. In the E-step, given some current $\hat\theta$, combining \eqref{eq:fy_free_energy} and \eqref{eq:LOmega}, leads to  
\begin{align}\label{eq:e_step}
    \hat{q}_i^{\mbox{\scriptsize new}} &= \arg\min_{q_i} \underset{Z_i \sim q_i}{\mathbb{E}} \left[\ell(x_i; Z_i,  \hat{\theta})\right] + L_{\Omega}(\eta; q_i ) \nonumber\\
    &= \arg\max_{q_i} \underset{Z_i \sim q_i}{\mathbb{E}} \left[ \eta(Z_i) - \ell(x_i; Z_i,  \hat{\theta} ) \right] - \Omega(q_i) \nonumber\\
 &=\arg\min_{q_i} L_\Omega(\eta - \ell(x_i; \cdot,  \hat{\theta}); q )  \nonumber \\
    &= \nabla \Omega^*(\eta - \ell(x_i; \cdot,  \hat{\theta})), 
\end{align}
for $i=1,...,N$, where $\nabla \Omega^*$ is the gradient map in \eqref{eq:gradientmap} (\textit{e.g.}, softmax or another $\alpha$-entmax transformation). The M-step consists in solving 
\begin{equation}
    \hat{\theta}^{\mbox{\scriptsize new}} = \arg\min_\theta \sum_{i=1}^N \underset{Z_i\sim \hat{q}_i^{\mbox{\scriptsize new}}}{\mathbb{E}}[\ell(x_i;  Z_i, \theta)],
\end{equation}
which depends on the particular form of $\ell(x; Z, \theta)$.  

If it is desired to estimate the prior scoring function $\eta$, we can also update $\eta$ in the M-step according to 
\begin{align}
       \hat{\eta}^{\mbox{\scriptsize new}} 
       % \hat q_\Omega [\hat{\eta}^{\mbox{\scriptsize new}}]& 
       &= \arg\min_\eta \sum_{i=1}^N L_{\Omega}\bigl( \eta; \hat{q}_i^{\mbox{\scriptsize new}}\bigr)\nonumber\\
       & = \arg\min_\eta N \; \Omega^*(\eta) - \sum_{i=1}^N \underset{Z_i \sim \hat{q}_i^{\mbox{\scriptsize new}}}{\mathbb{E}} [\eta(Z_i)] \nonumber\\ 
    & = \arg\max_\eta  \underset{Z \sim  \bar{q}}{\mathbb{E}} [\eta(Z)] - \Omega^*(\eta) \nonumber \\
    & \in \partial \Omega(\bar{q}),
\end{align}
where $\bar{q} = (1/N)\sum_{i=1}^N \hat{q}_i^{\mbox{\scriptsize new}}$ and $\partial \Omega$ is the subdifferential of $\Omega$ \citep{Bauschke}. 
% \andre{the argmin is not unique, the gradient is only one of many minimizers. it would be more rigorous to write $\hat{\eta}^{\mbox{\scriptsize new}}  \in \partial \Omega(\bar{q})$, where }
% \sophia{are we sure that omitting $\Omega(q)$ is ok on the second line?} \andre{yes, the minimization is wrt $\eta$, independent of $q$}
The third equality results from the fact that the average of expectations under a set of probability (mass or density) functions is the expectation under the average of these functions. The final set inclusion is a standard result similar to the one in footnote 1 for the general case of possibly non-differentiable $\Omega$. 

If $\ell(x_i, z_i, \theta) = -\log \mathcal{N}(x_i; \mu_{z_i}, \Sigma_{z_i})$ are Gaussian negative log-likelihoods and $\nabla \Omega^*$ is the softmax transformation, we recover the classical EM algorithm for GMM. In the next section, we generalize it by considering other regularizers, namely that for which $\nabla \Omega^*$ is $\alpha$-entmax, which yields a new adaptively sparse EM algorithm for GMMs. 


\subsection{Gradient Backpropagation}

In two of the experiments discussed below, we optimize the FY-ELBO in an amortized inference setting \citep{Gershman}, where the variational distribution is parametrized by an encoder network that takes $x$ as the input, as in variational autoencoders \citep{Kingma2014}. We denote the resulting variational distribution by $q_\phi(z | x)$, where $\phi$ are the inference network parameters. Instead of minimizing \eqref{eq:joint_minimization} through alternating minimization, we use end-to-end gradient backpropagation (combined with a reparametrization trick or with a score function estimator) to update the encoder parameters $\phi$, the decoder parameters $\theta$, and, optionally, any parameters associated with $\eta$. 


\section{Applications and Experiments}\label{sec:experiments}
In this section, we illustrate Fenchel-Young variational learning through experiments with different models and applications.  
We begin by applying the FYEM algorithm to GMM-based clustering, showing that it recovers the standard and hard versions of EM for particular choices of regularizer, in addition to a new \textit{sparse}\footnote{This new sparse variant of EM is a different instance of FYVL than those proposed by \citet{dukkipati2016mixture} and \citet{inoue2013q}, which are standard mixtures of what are commonly called $q$-Gaussians by \citet{naudts2009q} and $\beta$-Gaussians by \citet{martins2022sparse}.} \textit{EM} variant (\S\ref{sec:fyem_gmm}).
%, \textit{i.e.} they have a discrete exponential family latent variable and a deformed exponential family observation model ($q$-Gaussian).}  variant. 
% \andre{discuss this in a related work section instead of in this footnote? this seems more relevant to the NVDM model actually} \sophia{it's about clustering}
In \S\ref{sec:FYVAE_beta}, we use FYVL to develop VAEs with finite posterior support and apply them to MNIST image data with both sparse and standard multivariate Bernoulli observation models, and to Fashion MNIST data with sparse and standard Categorical observation models. Finally, we combine a Gaussian prior with a sparse observation model into an FY variant of the $\beta$-VAE to handle sparse observations in a neural variational document model (\S\ref{sec:fy_VAE_doc}).

% \begin{itemize}
% \item  We use the FYEM algorithm for GMM-based clustering, and show that FYEM recovers the standard and hard versions of EM for particular choices of regularizer, in addition to a new variant which we call \textit{sparse EM}.\footnote{The proposed sparse EM is a different instance of FYVL than $q$-GMMs \citep{dukkipati2016mixture,inoue2013q}, which have a discrete exponential family latent variable and a deformed exponential family observation model ($q$-Gaussian). \andre{discuss this in a related work section instead of in this footnote? this seems more relevant to the NVDM model actually} \sophia{it's about clustering}}

% \item We apply subspace clustering based on mixtures of \textit{probabilistic principal component analyzers} (MPPCA), learned via sparse-EM, to motion segmentation in video sequences.

% \item $q$-Gaussian

% \item  We combine a Gaussian prior is combined with a sparse observation model into an FY variant of the $\beta$-VAE to support sparse observations in a neural variational document model. 
% \end{itemize}

\subsection{FYEM for GMM} \label{sec:fyem_gmm}
\paragraph{Standard EM.} 
The FYEM algorithm described in $\S$\ref{sec:FYEM} recovers standard EM  for GMMs given the following choices: 
\begin{itemize}
\item $x_1,...,x_N \in \mathbb{R}^d$ are $d$-variate observations and the latent variables $z_1, ... z_N \in \mathcal{Z} = \{1,...,K\}$  indicate to which component each observation $x_i$ belongs;  
\item $q_i = \bigl( q_i(1),...,q_i(K) \bigr) \in\! \Delta(\mathcal{Z})$ is the variational approximation of the distribution of $z_i$;
\item $\eta  \in \mathbb{R}^K$ contains the logarithms of the prior probabilities of the $K$ components, denoted $\pi_1, ..., \pi_K$, \textit{i.e.}, $\eta_1 = \log \pi_1, ..., \eta_K = \log \pi_K$; 
\item $\ell(x_i, z, \theta) = -\log \mathcal{N}(x_i; \mu_{z}, \Sigma_{z})$, for $z=1,...,K$, where $\mu_{z}$ and $\Sigma_{z}$ are, respectively, the mean and covariance of component $z$.
\item $\Omega(q) = -H(q)$, thus $\nabla \Omega^* = $ softmax.
\end{itemize}

The E-step in \eqref{eq:e_step} then becomes 
\begin{align}
    \hat{q}_{i}^{\mbox{\scriptsize new}}(z) &=\mbox{softmax}\bigl(\eta_z + \log \mathcal{N}(x_i; \hat{\mu}_{z}, \hat{\Sigma}_{z})\bigr) \label{eq:standradEstep}\\
    & = \pi_z \; \mathcal{N}(x_i; \hat{\mu}_z, \hat{\Sigma}_z) \biggl( \sum_{j=1}^K \pi_j \; \mathcal{N}(x_i; \hat{\mu}_{j}, \hat{\Sigma}_{j})\biggr)^{-1}\! , \nonumber
\end{align}
for $i=1,...,N,$ $z=1,...,K$, where $\hat{\mu}_z$, $\hat{\Sigma}_z$ are the current parameter estimates. This is the E-step of standard EM  for GMM estimation \citep{Figueiredo2002}. The M-step corresponds to maximum weighted log-likelihood estimation,  yielding, for $z = 1, ..., K$,
\begin{align*}
    \hat{\mu}_z^{\mbox{\scriptsize new}} & = \frac{\sum\limits_{i=1}^N \hat{q}_{i}^{\mbox{\scriptsize new}}(z)\; x_i}{\sum\limits_{i=1}^N \hat{q}_{i}^{\mbox{\scriptsize new}}(z)} \\
    \hat{\Sigma}_{z}^{\mbox{\scriptsize new}} & = \frac{\sum\limits_{i=1}^N \hat{q}_{i}^{\mbox{\scriptsize new}}(z)\; (x_i - \hat{\mu}_z^{\mbox{\scriptsize new}}) (x_i - \hat{\mu}_z^{\mbox{\scriptsize new}})^\top }{\sum\limits_{i=1}^N \hat{q}_{i}^{\mbox{\scriptsize new}}(z)}\\
    \hat{\eta}_z^{\mbox{\scriptsize new}} & = \log \sum\limits_{i=1}^N \hat{q}_{i}^{\mbox{\scriptsize new}}(z) - \log N. 
\end{align*}
% In Table 1 updated the component weights are also updated to their maximum likelihood estimates: for $z=1,...,K$,
% \[
% \hat{\pi}_z^{\mbox{\scriptsize new}} = \frac{1}{n} \sum_{i=1}^n \hat{q}_{i,z}^{\mbox{\scriptsize new}}.
% \]

\begin{figure*}[h!]
\centering
\includegraphics[width=\linewidth]{clustering_results.pdf}
\caption{
Comparison of the standard, sparse, and hard versions of EM for GMM-based clustering. Data points are colored by their true labels, with \textcolor{gray}{\sf x} markers indicating outliers (unlabeled). Black ellipses represent the estimated clusters (level curves of the GMM components), while colored ellipses denote the ground-truth ones.}
\label{fig:clustering_comparison}
\end{figure*}

\paragraph{Hard EM.}
Setting $\Omega(q) = 0$, \textit{i.e.}, removing the regularizer, yields a different E-step:  
\begin{equation*}
    \hat{q}_{i}^{\mbox{\scriptsize new}} \! = \mbox{argmax}\bigl(\pi_1 \; \mathcal{N}(x_i; \hat{\mu}_1, \hat{\Sigma}_1),...,\pi_K \; \mathcal{N}(x_i; \hat{\mu}_K, \hat{\Sigma}_K)\bigr), 
\end{equation*}
where $\mbox{argmax}(u_1,...,u_K)$ of a tuple of $K$ numbers returns a vector with $1/m$ in the entries with the same indices as the $m$ maxima and zero everywhere else. If the maximum is unique, this is just the one-hot representation of the maximizer. This hard version of EM coincides with the \textit{classification EM} (CEM) algorithm proposed by \citet{Celeux}. The M-step is the same as in standard EM. 

\paragraph{Sparse EM.}
Finally, setting $\Omega(q) = \Omega_\alpha(q)$, the Tsallis negentropy \eqref{eq:tsallis} with $\alpha >1 $, yields the new sparse E-step: 
% \andre{this is wrong, we should *not* have  $\log \pi_i$ here, instead it should be $\eta = \nabla \Omega_\alpha(\pi) = \frac{\pi^{\alpha - 1}}{\alpha - 1}$, so the update should be
\begin{align} \label{eq:sparseEstep}
    {\hat{q}_{i}^{\mbox{\scriptsize new}}} 
     = \alpha\mbox{-entmax}\Bigl( & \frac{\pi_1^{\alpha - 1}}{\alpha-1} + \log \bigl(\mathcal{N}(x_i; \hat{\mu}_1, \hat{\Sigma}_1)\bigr),..., \nonumber  \\ 
 &\frac{\pi_K^{\alpha - 1}}{\alpha-1} + \log\bigl(\mathcal{N}(x_i; \hat{\mu}_K, \hat{\Sigma}_K\bigr)\Bigr).
\end{align}
% }
% \begin{align} \label{eq:sparseEstep}
%     {\hat{q}_{i}^{\mbox{\scriptsize new}}} 
%      = \alpha\mbox{-entmax}\Bigl( & \log \bigl(\pi_1 \mathcal{N}(x_i; \hat{\mu}_1, \hat{\Sigma}_1)\bigr),..., \nonumber  \\ 
%  &\log\bigl(\pi_K \mathcal{N}(x_i; \hat{\mu}_K, \hat{\Sigma}_K\bigr)\Bigr).
% \end{align}
The case of 2-entmax is called \textit{sparsemax} and corresponds to the Euclidean projection of its vector argument onto the probability simplex \citep{Martins2016}. For $\alpha > 1$, the output of $\alpha\mbox{-entamax}$ can have both zero and non-zero components. When $\alpha = 1$, Tsallis $\alpha$-negentropy becomes Shannon's entropy and we recover the standard EM algorithm,  where all the components of $\hat{q}_i^{\mbox{\scriptsize new}}$ are strictly positive. 

The key novel feature of sparse EM is the possible presence of zeros in $\hat{q}_i$, meaning that sample $x_i$ will not contribute to the update of the parameter estimates of the corresponding components. The E-step in standard EM does not yield zero probabilities, thus all samples affect the parameter estimates of all the components. In hard EM, each sample only affects the parameters of the ``closest" component, as is the case of $K$-means clustering (with the important difference that $K$-means clustering does not estimate component covariances). 


\paragraph{Experimental Results and Discussion.} 
We generate $1000$ samples uniformly from four bivariate Gaussians with means at $[-1, -1]$, $[0, 0]$, $[1, 1]$, and $[1, -1]$. The components overlap significantly, with covariance matrices $0.11\, I$, $0.5\, I$, $0.7\, I$, and $0.9\, I$. Additionally, we sample $100$ uniformly distributed random samples in $[-3, 3]^2$ to simulate the presence of outliers, yielding a total of $1100$ data points. We compare the three EM variants described---standard EM, hard EM, and sparse EM---on unsupervised clustering of this dataset. The number of components in each model is set to $4$ and the initial parameters (means and covariances) are randomly initialized from a uniform distribution in $[0,\, 0.1]$. Each algorithm runs $200$ iterations, which we verified is more than enough to satisfy a tight convergence criterion.  

% \mario{Running a fixed 200 iterations requires some justification; is it true that in all cases the algorithms had essentially converged?} \sophia{we haven't been checking for convergence but by 200 iterations there is not change in the cluster assignments -- we can verify better but even after 50 the synthetic data model is ``converged"}

We evaluate the quality of unsupervised clustering of each algorithm using three metrics: adjusted mutual information \citep{Vinh}, adjusted Rand index \citep{Rand}, and silhouette score \citep{Rousseeuw}. Each model is trained and evaluated across five different random seeds, and the average and standard deviation of the metrics are reported in Table~\ref{tab:synthetic-clustering}, showing that sparse EM outperforms standard and hard EM in two of three standard metrics (see also Figure~\ref{fig:clustering_comparison} for an illustration).


\begin{table}[t]
\caption{\label{tab:synthetic-clustering}
Clustering quality metrics (higher is better) for GMM-based clustering using the standard, hard, and sparse (ours) versions of EM; $\alpha =2 $ in sparse EM. Values are averaged over 5 random seeds.  % for each model.
%All models' means and standard deviations are randomly initialized from U(0,1). The ground truth synthetic data are 1000 samples drawn uniformly from 4 bivariate Normal, overlapping clusters with different means and variances plus 100 outliers. %\sophia{percent of outliers, plus minus outliers, breakdown point, stopping criterion, how much are centers moving plot over time}
} 
\vspace{0.3cm}
\resizebox{\linewidth}{!}{\begin{tabular}{llll}
\toprule
   \textsc{E-Step} & \textsc{Adjusted MI} & \textsc{Adjusted RI} & \textsc{Silhouette Sc.} \\
\midrule
   Standard EM  & .606 $\pm$ .012 & \textbf{.531} $\pm$ .005 & .345 $\pm$ .032 \\
   Hard EM  & .537 $\pm$ .088 & .348 $\pm$ .043 & .207 $\pm$ .128 \\
   Sparse EM  & \textbf{.636} $\pm$ .017 & .476 $\pm$ .037 & \textbf{.393} $\pm$ .041\\
\bottomrule
\end{tabular}}
\end{table}

\subsection{Fenchel-Young $\beta$-VAE for Images}\label{sec:FYVAE_beta}
In a classical VAE application by \citet{Kingma2014}, a Gaussian variational posterior and a $\mathcal{N}(0, I)$ prior are combined with a multivariate Bernoulli observation model, representing the pixels of MNIST digit images. The objective maximized by amortized VI w.r.t. $\theta$ (decoder parameters) and $\phi$ (encoder parameters), for a set of $N$ images is %, each with $d$ pixels, is
% \begin{equation*}
% J(\phi, \theta) \! = \! \sum\limits_{i=1}^N \sum_{j=1}^d \underset{Z\sim q_{i,j}}{\mathbb{E}} \bigl[\log p_{\theta}(x_{i,j} | Z) \bigr] + \beta D_{\text{\small KL}}(q_{i,j} \| p_Z),
% \end{equation*}
% where $q_{i,j} =  \mathcal{N}\bigl(\mu_{\phi}(x_{i, j}), \mbox{diag}\bigl(\sigma^2_{\phi}(x_{i,j}) \bigr) \bigr)$ and 
% \begin{equation*}
\begin{equation}
J(\phi, \theta) \! = \! \sum\limits_{i=1}^N  \underset{Z\sim q_{i}}{\mathbb{E}} \bigl[\log p_{\theta}(x_{i} | Z) \bigr] + \beta D_{\text{\small KL}}(q_{i} \| p_Z), \label{eq:enc_dec}
\end{equation}
where $q_{i} =  \mathcal{N}\bigl(\mu_{\phi}(x_{i}), \mbox{diag}\bigl(\sigma^2_{\phi}(x_{i}) \bigr) \bigr)$ and 
\begin{equation*}
p_{\theta}(x_{i} | z_{i}) \sim \textnormal{Bernoulli}(\sigma( f_{\theta}( z_{i}));
\end{equation*}
$\sigma$ denotes the sigmoid function, and $f_{\theta}$ a neurally parametrized function.

We experiment with truncated parabola $\xi$-Gaussian\footnote{The designation used by \citep{martins2022sparse} is $\beta$-Gaussian, but in this paper we use $\beta$ elsewhere.} \citep{martins2022sparse} finite-support variational posteriors, combined with two decoders. The same multivariate Bernoulli decoder described in the previous paragraph and a product of binary $\alpha$-entmax deformed exponential family distributions. The FY-ELBO VAE objective is 
\begin{equation} 
J (\phi, \theta)  = \sum\limits_{i=1}^N \underset{Z\sim q_i}{\mathbb{E}} \left[   L_\Psi(\theta(Z); x_i) \right] + \beta L_\Omega (\eta_{\mathcal{N}(0, \textnormal{I})} ; q_i), \nonumber
\end{equation}
for different $\Psi$ (corresponding to the observation model's $\alpha$ which is equivalent to the multivariate Bernoulli decoder when $\alpha = 1$), and different $\Omega$ (corresponding to the hyperparameter $\xi$ of the latent $\xi$-Gaussian). For the derivation and reparametrization of the $\xi$-Gaussian, see the work by \citet{martins2022sparse}.

\paragraph{Experimental Results and Discussion.} 
We train VAEs with two-layer (512, 256) neural networks for both encoder and decoder, with a symmetric design. All models are trained for 50 epochs using a batch size of 64 and the Adam optimizer with a learning rate of $5 \times 10^{-5}$. The entropic regularizer is weighted by a factor of $\beta = 0.01$ which prevents posterior collapse. 

We report the $\ell_1$ reconstruction error, \textit{i.e.} the absolute distance between the model's reconstructed samples and the original input images, evaluated on the test split for both datasets. The results are presented in Table~\ref{tab:beta-Gaussian-vae}.  The first row corresponds to a standard VAE with a Gaussian latent variable. The Biweight and Epanechnikov densities are retrieved by setting the Tsallis entropy $\alpha = 3/2$ (1/2-Gaussian) and $\alpha = 2$ (0-Gaussian), respectively. The best reconstruction errors are obtained with latent $\xi$-Gaussian encoders combined with 2-entmax observation models. We provide visualisations of samples obtained from all encoder-decoder combinations in Fig.~\ref{fig:sample_vae} of Appendix \ref{sec:mnist}. 
% \sweta{I added some samples from each VAE's in Fig~\ref{fig:sample_vae}. Not sure if we plan to include them. if yes, we might need to add a reference here.}


% \mario{The Biweight and Epanechnikov latent variable models are not explained in the text; this is needed.}

% Biweight a = 3/2, b = 2-a = 1/2
% Gaussian a = 1, b = 1
% Epanechnikov a = 2, b = 0 
\begin{table}[t]
    \centering
     \resizebox{\linewidth}{!}{
    \begin{tabular}{p{3cm}p{3cm}rr}
    \toprule
     { \textsc{Observation}}   &   { \textsc{Latent Variable}} & \textsc{Mnist} & \textsc{FashionMnist} \\
         \midrule
       \multirow{3}{*} {\parbox{3cm}{Multivariate Bernoulli}}   & Gaussian       & 13.272 & 34.192 \\
           & Biweight      & 12.061 & 32.690 \\
           & Epanechnikov   & 12.111 & 32.482 \\
\hline
 \multirow{3}{*}{\parbox{3cm} {Product of $\Omega_2$ sparse binary distributions}}  & Gaussian  & 12.610 & 32.986 \\
           & Biweight       & 10.326 & \textbf{25.985} \\
           & Epanechnikov   & \textbf{9.183}  & 26.186 \\
        \bottomrule
    \end{tabular}}
     \caption{VAE $\ell_1$ reconstruction error on MNIST and Fashion MNIST datasets.}
    \label{tab:beta-Gaussian-vae}
\end{table}

\subsection{Fenchel-Young $\beta$-VAE for Documents}\label{sec:fy_VAE_doc}
% In classical VAEs the variational posterior is commonly assumed to be a Gaussian, to facilitate simple reparametrized stochastic gradients \citep{Kingma2014}. Further, the observation model belongs to an exponential family, and both the observation model and variational posterior are neurally parametrized. We experiment with an extension of the classical VAE, using Fenchel-Young losses, thereby incorporating more flexible observation models that can directly handle sparsity in the empirical distribution. 

% \subsubsection{Sparse Support Document Model}
%\textbf{Compact Support Document Model. } 
Similarly with the sparsemax observation model combined with a Gaussian latent variable in the previous section, we experiment with a FY $\beta$-VAE applied to unsupervised word embeddings to directly handle the sparsity of the empirical distribution. In this experiment we focus on tuning the level of sparsity in the observation model. 
% Our compact support observation model FY VAE outperforms \cite{miao2016neural}'s neural variational document model (NVDM) baseline in absolute reconstruction error. 
The baseline \textit{neural variational document model} (NVDM) proposed by \citet{miao2016neural} is a standard VAE (with prior $p_Z = \mathcal{N}(0, I)$): it models the joint probability of observed word counts (or proportions) in a document $x_i$, $i =1, ..., N$, and of continuous latent semantic embeddings $z_i$, by optimizing the same objective as \eqref{eq:enc_dec} and replacing the multivariate Bernoulli with a multinomial observation model,
\begin{align*}
% &J(\phi, \theta) = \sum\limits_{i=1}^N \underset{Z\sim q}{\mathbb{E}} \left[-  \log  f(x_i | Z; \theta) \right] + \beta D_{\text{\small KL}}(q_i \| p_Z) \\  
% &Z \sim \mathcal{N}\Bigl(\mu_{\phi}(x_i), \mbox{diag}\bigl(\sigma^2_{\phi}(x_i) \bigr) \Bigr) \nonumber \\
&p_\theta(x_i | z_i) \sim \textnormal{Multinomial}(\textnormal{softmax}( W z_i)), \nonumber
\end{align*}
where $\theta = W \in \mathbb{R}^{|D| \times K}$, $K$ is the dimension of the latent multivariate Gaussian, and $|D|$ is the size of the observed vocabulary.
% ; $\mu_{\phi}$ and $\sigma^2_{\phi}$ are neurally parameterized functions, with parameters $\phi$, from the observation space to the parameters of the distribution of the latent variable $Z$. 
The model parameters are learned by stochastic gradient backpropagation. 

Our FY $\beta$-VAE model uses the same multivariate Gaussian latent $q$ and prior $p_Z$, combined with a FY observation model $L_\Psi(\theta(z); x)$:
\begin{align} 
J (\phi, \theta)  = \sum\limits_{i=1}^N \underset{Z\sim q}{\mathbb{E}} \left[   L_\Psi(\theta(Z); x_i) \right] + \beta D_{\text{\footnotesize KL}}(q_i \| p_Z), \nonumber
\end{align}
where $\Psi$ is the Tsallis negentropy, dom($\Psi) =\Delta_{|D|-1}$, and $\alpha$-entmax replaces softmax in the $L_\Psi$ decoder.
% \begin{equation} 
% f(x | z) \sim \textnormal{Multinomial}(\textnormal{sparsemax}( W z))\bigr. \nonumber
% \end{equation} 
Unlike the standard VAE formulation, the deformed exponential family distribution parametrized by $\theta(z)$ can assign zero probability to vocabulary categories that do not occur in a given document, directly modelling the sparse empirical distribution.

% \subsubsection{} 
\textbf{Experimental Results and Discussion.} We use the \textit{20NewsGroups}\footnote{\url{http://qwone.com/~jason/20Newsgroups/}} dataset, which consists of 11,314 training samples and 7,531 test samples (documents, represented as bags of words). We assume a decoder parameterizing a sparse discrete distribution over the vocabulary generates all the words in a document independently, given a continuous Gaussian latent variable $Z$ (i.e. a dense, continuous document representation). Following \citet{miao2016neural}, we assume that each dimension in the latent space represents a ``topic'' (which is a simplification of the classical discrete latent variable topic model of \cite{blei2003latent}). 

The first 1,000 test samples serve as the validation set for hyperparameter tuning. We perform a search over entmax $\alpha$ (controlling decoder's sparsity), learning rate, batch size, number of epochs, and the number of topics using Optuna \citep{optuna2019} across 100 trials for both the baseline and modified NVDM models. Finally, we rerun the experiment with the best hyperparameters using five different random seeds and report the reconstruction error (RE) defined by the $\ell_1$ distance between the empirical data distribution and the learned observation model distribution. 
%\sophia{best hyperparameters for reproducibility}

Our sparse support observation model FYVAE outperforms the NVDM baseline method \citep{miao2016neural}  by 10.68\% in absolute reconstruction error, as reported in Table~\ref{tab:topic-model}. 

% \begin{table}[H]
% \centering
% \caption{Average values over 5 seeds for each model. Reconstruction Error (RE) normalized by the number of examples in the test set (6505).
% \begin{tabular}{l l l}
%         & NVDM          & FY VAE        \\ \hline
% Average RE & 1.4984 {\scriptsize  ± 0.0009} & \textbf{1.3384} {\scriptsize ± 0.0008}
% \end{tabular}%
% }
% \label{tab:topic-model}
% \end{table}

\begin{table}[t]
\centering
\begin{tabular}{lr}
\toprule
      \textsc{Model}  & \textsc{Average RE} \\
      \midrule
      NVDM     & 1.4984 ± 0.0009 \\     
      FY VAE   & \textbf{1.3384} ± 0.0008 \\
      \bottomrule
\end{tabular}%
\caption{Average reconstruction error over 5 seeds for each model normalized by the number of examples in the test set (6505).}
\label{tab:topic-model}
\end{table}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\linewidth]{coherence-30.pdf}
    \caption{Topic coherence scores for the standard NVDM and the FY VAE models. }
    \label{fig:coherence}
\end{figure}

Figure~\ref{fig:coherence} shows average coherence scores, measured by the pairwise similarity between embedding representations of the top-$k$ words for each latent ``topic" dimension, using a sentence-transformer model \citep{reimers-gurevych-2019-sentence}.\footnote{We use the \texttt{all-mpnet-base-v2} model, which is available at \url{huggingface.co/sentence-transformers/all-mpnet-base-v2}.} FYVAE consistently achieves better or similar coherence scores across varying values of $k$ (number of words) than the NVDM model, confirming that the latent document representations learned by FYVAE are more semantically coherent and better aligned with meaningful word groupings.


\section{Conclusions}
%We present our contribution as generalizing variational inference, by replacing the entropy term and the cross-entropy term, respectively, with an arbitrary convex function $\Omega$ and its corresponding Fenchel-Young loss/divergence $L_\Omega$, effectively combining the variational view of Bayesian infrence with the class of Fenchel-Young losses. We can equivalently regard our contribution from the oppositive perspective, as expanding the scope of Fenchel-Young losses---originally proposed in the context of fully observed models---to latent variable models.

We introduced the \textit{Fenchel-Young variational learning} (FYVL) framework by exploiting general classes of entropic regularizers to obtain sparse discrete and continuous posterior distributions. FYVL extends classical Bayesian inference and variational inference by expanding the class of variational posteriors. We define and lower bound the Fenchel-Young variational free energy (FYVFE) and describe EM-style and amortized variational inference algorithms for estimating arbitrary generative models that may be specified as instances of FYVL. 

We instantiate and experiment with FYVL on three experiments on classical machine learning problems. We derive and experiment with a sparse E-step EM algorithm for Gaussian mixture models, obtaining improved clustering quality compared with the classical and hard versions of EM on a synthetic dataset with overlapping clusters and outliers. In our FY variational autoencoder experiments with continuous finite support posteriors, we obtain the best performance when combining a latent $\xi$-Gaussian posterior with a sparse support observation model. Finally, we obtain significantly improved reconstruction error in neural variational document modeling of the \textit{20Newsgroups} dataset by using a Fenchel-Young observation model with sparse support.

In future work we are interested in exploring heavy-tailed posteriors, conjugate FY families, and non-parametric sampling of FY posteriors. 


% Acknowledgements should only appear in the accepted version.
% \section*{Acknowledgements}

% \textbf{Do not} include acknowledgements in the initial version of
% the paper submitted for blind review.

% If a paper is accepted, the final camera-ready version can (and
% usually should) include acknowledgements.  Such acknowledgements
% should be placed at the end of the section, in an unnumbered section
% that does not count towards the paper page limit. Typically, this will 
% include thanks to reviewers who gave useful comments, to colleagues 
% who contributed to the ideas, and to funding agencies and corporate 
% sponsors that provided financial support.

\section*{Impact Statement}

% Authors are \textbf{required} to include a statement of the potential 
% broader impact of their work, including its ethical aspects and future 
% societal consequences. This statement should be in an unnumbered 
% section at the end of the paper (co-located with Acknowledgements -- 
% the two may appear in either order, but both must be before References), 
% and does not count toward the paper page limit. In many cases, where 
% the ethical impacts and expected societal implications are those that 
% are well established when advancing the field of Machine Learning, 
% substantial discussion is not required, and a simple statement such 
% as the following will suffice:

% ``
This paper presents work whose goal is to advance the field of 
\textit{machine learning} (ML) at a fundamental level. There are many potential societal consequences of our work, as of any ML work, none of which we feel must be specifically highlighted here.
% ''

% The above statement can be used verbatim in such cases, but we 
% encourage authors to think about whether there is content which does 
% warrant further discussion, as this statement will be apparent if the 
% paper is later flagged for ethics review.


% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
\nocite{langley00}

\bibliography{references}
\bibliographystyle{icml2024}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{Experiment details}

% You can have as much text here as you want. The main body must be at most $8$ pages long.
% For the final version, one more page can be added.
% If you want, you can use an appendix like this one.  

% The $\mathtt{\backslash onecolumn}$ command above can be kept in place if you prefer a one-column appendix, or can be removed if you prefer a two-column appendix.  Apart from this possible change, the style (font size, spacing, margins, page numbering, etc.) should be kept the same as the main body.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{comment}\subsection{Mixture of Probabilistic PCA}\label{sec:mppca}
\paragraph{MPPCA Model.}
The classical \textit{mixture of probabilistic PCA} (MPPCA) model introduced by \citet{tipping1999mixtures} is simply a GMM with a ``low-rank plus identity" structure for the covariance matrices of the components,
\[
\Sigma_z = W_z\, W_z^\top + \sigma_z^2 I, \;\;\; \mbox{for $z=1,...,K$},
\]
where each $W_z\in\mathbb{R}^{d\times r}$ and $r<d$. The E-step of all the versions of EM derived above remain unchanged. However, in the M-step, the updates of the estimates of $\mu_z$ and $\Sigma_z$ are different, with the update of the estimate of $\Sigma_z$ being replaced by that of $W_z$ and $\sigma_z^2$; we omit the details, which can be found in the work of \citet{tipping1999mixtures}.

\paragraph{Experimental Results and Discussion.} We consider one of the classical applications of MPPCA: subspace clustering for motion segmentation of video sequences \citep{VidalTron}. Each mixture component captures a different motion subspace across the sequence and each sample $x_i \in \mathbb{R}^d$ corresponds to stacking the pixels of the $i$-th frame into a $d$-dimensional vector. 

Our experiments follow previous work by \citet{xu2023hemppcat} and \citet{vidal2011subspace}. We use the standard Hopkins 155 dataset \citep{tron2007benchmark}, in which each video has up to 4 subspace components. Our baselines are MPPCA via standard EM \citep{tipping1999mixtures} and K-planes subspace (KSS) clustering \citep{kambhatla1997dimension, bradley2000k}. We split the dataset into training (72\%), validation (8\%), and testing (20\%) subsets for each sequence. As in previous work by \citet{xu2023hemppcat}, Gaussian noise is added to the data, scaling it based on the maximum energy of all trajectories. The average classification error on the test split is reported in Table~\ref{tab:video-segmentaton}, showing that sparse EM achieves comparable average error to standard EM, with both EM variants outperforming KSS.

We ran 10 Optuna trials to fit MPPCA with noise 0.001; average absolute $\ell_1$ reconstruction error was 0.133. With the same noise, we ran 20 Optuna trials varying entmax $\alpha$ between 0 and 3; the best value of $\alpha$ was 2.0 with a reconstruction error of 0.139. 
% MPPCA 
% 0.1333994302241213 and parameters: {'noise': 0.001}
% SMPPCA - varies alpha (1, 3) for the best noise from MPPCA
% 0.13855755636895453 'alpha_ent': 2.0
% SMPPCA - varies alpha (0,1) for the best noise from MPPCA
% 0.18052208454940255 'alpha_ent': 0.8

% \begin{table}[H]
% \centering
% \caption{Average classification error (\%) ± standard deviation over the Hopkins 155 dataset.  }
% \begin{tabular}{llll}
%  &
%   \textsc{KSS} &
%    \textsc{Standard EM} &
%    \textsc{Sparse  EM } \\ \hline
% \begin{tabular}[c]{@{}c@{}} Mean Classification Error \end{tabular} &
%   \begin{tabular}[c]{@{}l@{}}19.30 { ± 17.64}\end{tabular} &
%   \begin{tabular}[c]{@{}l@{}}\textbf{16.20} { ± 16.19} \end{tabular} &
%   \begin{tabular}[c]{@{}l@{}}16.64 { ± 16.36}\end{tabular}
% \end{tabular}%
% \label{tab:video-segmentaton}
% \end{table}

\begin{table}[H]
\centering
\caption{Average classification error (\%) ± standard deviation over the Hopkins 155 dataset.}
\begin{tabular}{l c}
\toprule
\textsc{E-Step} & \textsc{Average Classification Error} \\
\midrule
{KSS}          & 19.30 ± 17.64 \\
{Standard EM}  & \textbf{16.20} ± 16.19 \\
{Sparse EM}    & 16.64 ± 16.36 \\
 \bottomrule
 \end{tabular}
\label{tab:video-segmentaton}
\end{table}


% \mario{Isn't this example somewhat pointless? OK, we can use sparse EM with MPPCA, which is obvious, we try it on some specific task and gain nothing w.r.t. standard EM. Why do we need to report this?} \sophia{appendix? we had thought that it's a good fit for the subspace clustering problem}

% \subsection{FYEM for GMM}

% \begin{table}[H]
% \caption{
% Clustering quality metrics (higher is better) for GMM-based clustering using the standard, hard, and sparse (ours) versions of EM. Values are averaged over 5 random seeds. Entmax $\alpha=2$ for all the experiments in this table. All models' means and standard deviations are randomly initialized from U(0,1). The ground truth synthetic data are 1000 samples drawn uniformly from 4 bivariate Normal, overlapping clusters with different means and variances plus 100 outliers. %\sophia{percent of outliers, plus minus outliers, breakdown point, stopping criterion, how much are centers moving plot over time}
% } 
% %\label{tab:synthetic-clustering}
% \resizebox{0.5\columnwidth}{!}{%
% \begin{tabular}{llccc}
% \hline
% \textbf{Prior} &
%   \textbf{Method} &
%   \textbf{\begin{tabular}[c]{@{}c@{}}Adjusted \\ MI\end{tabular}} &
%   \textbf{\begin{tabular}[c]{@{}c@{}}Adjusted \\ Rand Index\end{tabular}} &
%   \textbf{\begin{tabular}[c]{@{}c@{}}Silhouette \\ Score\end{tabular}} \\ \hline
% \begin{tabular}[c]{@{}l@{}}Learnable \\ $\log p_z$\end{tabular} &
%   Sparse EM &
%   \textbf{\begin{tabular}[c]{@{}c@{}}$\textbf{0.6702} \\ \pm 0.0152$\end{tabular}} &
%   \begin{tabular}[c]{@{}c@{}}$0.6679 \\ \pm 0.0246$\end{tabular} &
%   \textbf{\begin{tabular}[c]{@{}c@{}}$\textbf{0.4422} \\ \pm 0.0133$\end{tabular}} \\
%  &
%   Hard EM &
%   \begin{tabular}[c]{@{}c@{}}$0.5374 \\ \pm 0.0877$\end{tabular} &
%   \begin{tabular}[c]{@{}c@{}}$0.3480 \\ \pm 0.0432$\end{tabular} &
%   \begin{tabular}[c]{@{}c@{}}$0.2074 \\ \pm 0.1277$\end{tabular} \\
%  &
%   GMM &
%   \begin{tabular}[c]{@{}c@{}}$0.6056 \\ \pm 0.0123$\end{tabular} &
%   \begin{tabular}[c]{@{}c@{}}$0.5305 \\ \pm 0.0049$\end{tabular} &
%   \begin{tabular}[c]{@{}c@{}}$0.3454 \\ \pm 0.0319$\end{tabular} \\ \hline
% \begin{tabular}[c]{@{}l@{}}Learnable \\ $p_z$\end{tabular} &
%   Sparse EM &
%   \begin{tabular}[c]{@{}c@{}}$0.6360 \\ \pm 0.0167$\end{tabular} &
%   \begin{tabular}[c]{@{}c@{}}$0.4763 \\ \pm 0.0372$\end{tabular} &
%   \begin{tabular}[c]{@{}c@{}}$0.3929 \\ \pm 0.0413$\end{tabular} \\ \hline
% \begin{tabular}[c]{@{}l@{}}Fixed \\ (uniform)\end{tabular} &
%   Sparse EM &
%   \begin{tabular}[c]{@{}c@{}}$0.6694 \\ \pm 0.0153$\end{tabular} &
%   \textbf{\begin{tabular}[c]{@{}c@{}}$\textbf{0.6681} \\ \pm 0.0245$\end{tabular}} &
%   \begin{tabular}[c]{@{}c@{}}$0.4422 \\ \pm 0.0136$\end{tabular} \\
%  &
%   Hard EM &
%   \begin{tabular}[c]{@{}c@{}}$0.6462 \\ \pm 0.0159$\end{tabular} &
%   \begin{tabular}[c]{@{}c@{}}$0.6312 \\ \pm 0.0243$\end{tabular} &
%   \begin{tabular}[c]{@{}c@{}}$0.4217 \\ \pm 0.0138$\end{tabular} \\
%  &
%   GMM &
%   \begin{tabular}[c]{@{}c@{}}$0.5439 \\ \pm 0.0040$\end{tabular} &
%   \begin{tabular}[c]{@{}c@{}}$0.4854 \\ \pm 0.0043$\end{tabular} &
%   \begin{tabular}[c]{@{}c@{}}$0.3301 \\ \pm 0.0073$\end{tabular} \\ \hline
% \end{tabular}%
% }
% \end{table}


\end{comment}


\subsection{MNIST and FashionMNIST Samples}\label{sec:mnist}

% \andre{Can we add some text here poitning to the figure? otherwise it looks like an empty section}
Figure~\ref{fig:sample_vae} presents 64 samples generated from the different FY VAE models compared in Table ~\ref{tab:beta-Gaussian-vae}.  We decode random latent vectors and apply either sigmoid or 2-entmax activation, depending on the model configuration. The images' clarity and structure improve when using Latent $\xi$-Gaussian encoders combined with 2-entmax observation models.

\begin{figure}[ht]
\centering
\begin{subfigure}
  \centering
  \includegraphics[width=0.30\linewidth]{figs/sigmoid_normal_1.0_1.00001_fmnist_best.png} 
\end{subfigure}
\begin{subfigure}
  \centering
  \includegraphics[width=0.30\linewidth]{figs/sigmoid_q-normal_1.0_1.5_fmnist_best.png} 
\end{subfigure}
\begin{subfigure}
  \centering
  \includegraphics[width=0.30\linewidth]{figs/sigmoid_q-normal_1.0_2.0_fmnist_best.png} 
\end{subfigure}
\begin{subfigure}
  \centering
  \includegraphics[width=0.30\textwidth]{figs/entmax_normal_2.0_1.0_fmnist_best.png} 
\end{subfigure}
\begin{subfigure} 
  \centering
  \includegraphics[width=0.30\textwidth]{figs/entmax_q-normal_2.0_1.5_fmnist_best.png} 
\end{subfigure}
\begin{subfigure}
  \centering
  \includegraphics[width=0.30\textwidth]{figs/entmax_q-normal_2.0_2.0_fmnist_best.png} 
\end{subfigure}
\caption{Samples from different VAEs. Top: Categorical decoders. Bottom: 2-entmax decoders. Left: Gaussian encoders. Middle: Biweight encoders. Right: Epanechnikov encoders. }
\label{fig:sample_vae}
\end{figure}

\subsection{Fenchel-Young $\beta$-VAE for Documents}
For the baseline NVDM model over 100 Optuna trials the hyperparameters that yield the best reconstruction error of 1.484 reported in Table 4 are batch size = 64, $\beta = 0.01$, learning rate = $5 \times 10^{-5}$, number of alternating epochs updating the decoder and encoder parameters = 1, number of training epochs = 500, dimension of the latent Gaussian = 50. For the Fenchel-Young $\beta$-VAE over 100 Optuna trials the hyperparameters that yield the best reconstruction error of 1.384 reported in Table 4 are batch size = 1024, $\beta = 0.01$, learning rate $= 5 \times 10^{-5}$, number of alternating epochs updating the decoder and encoder parameters = 3, number of training epochs = 500, dimension of the latent Gaussian = 50. 

% Softmax
% Best trial:
%   Value: 1478.730712890625
%   Params: 
%     alpha_ent: 1.6851816060492335
%     batch_size: 64
%     kld_weight: 0.01
%     learning_rate: 5e-05
%     n_alternating_epoch: 1
%     n_epoch: 500
%     n_topics: 50

% Entmax
% Best trial:
%   Value: 1310.6749267578125
%   Params: 
%     alpha_ent: 1.3134782000900973
%     batch_size: 1024
%     kld_weight: 0.01
%     learning_rate: 5e-05
%     n_alternating_epoch: 3
%     n_epoch: 500
%     n_topics: 50


% \section{Otherwise}


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
