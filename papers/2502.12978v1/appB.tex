\section{Selection Events of the Deep Learning Models}
\label{app:selection_events_of_dnn}
%
We explain the selection events regarding the deep learning model that transforms an image instance $\bm x_i \in \RR^d$ to a latent feature vector $\bm{z}_i \in \RR^{\tilde{d}}$.
%
We consider a deep learning model that consists of sequential piecewise-linear functions (e.g., convolution, ReLU activation, max pooling, and up-sampling).
% 
Obviously, the composite function of those piecewise-linear functions maintains its piecewise-linear nature.
%
Thus, within a specific real space in $\RR^d$, the deep learning model simplifies to a linear function, which can be expressed as:
%
\begin{equation}
 \cA_{\rm{DL}}(\bm{x}_i) 
 = \bm{B} + \bm{W}\bm{x}_i
 \;\;\; \text{if } \bm{x}_i \in \cP,
\end{equation}
%
where $\bm B \in \RR^{\tilde{d}}$ and $\bm W \in \RR^{\tilde{d}}$ represent the bias and weight matrices, and $\cP \subseteq \RR^d$ is a polytope where $\cA_{\rm{DL}}$ acts as a linear function.
%
The polytope can be characterized by a set of linear inequalities.
%
For details on computing these linear inequalities, see \citet{katsuoka2025si4onnx}.
% 
Let the selection event denote the set of polytopes for all instances in $\bm{Y}$:
%
\begin{equation}
 \cD_{\bm{Y}} := \{\cP \mid \bm{X}_i \in \bm{Y}, \bm{X}_i \in \cP\}.
\end{equation}
% 
For $k$NNAD using feature representations from the deep learning model, we can compute the selective $p$-value by adding the conditioning $\cD_{\bm{Y}} = \cD_{\bm{y}}$ into Eq.\eqref{eq:selective_pvalue}, as follows:
%
\begin{equation}
 p_{\rm selective}
 :=
 \PP_{\rm H_0}
 \left(
 T(\bm Y) \ge T(\bm y)
 \middle|
 \cD_{\bm{Y}} = \cD_{\bm{y}},
 \cN_{\bm{Y}} = \cN_{\bm{y}},
 \cK_{\bm{Y}} = \cK_{\bm{y}},
 \cS_{\bm{Y}} = \cS_{\bm{y}},
 \cQ_{\bm{Y}} = \cQ_{\bm{y}}
 \right).
\end{equation}
