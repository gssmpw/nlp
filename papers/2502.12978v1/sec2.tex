\section{Problem Setup}
\label{sec:setup}
%
In this section, we present the problem setup\footnote{In \S\ref{sec:relatedWorks}, we will discuss the scope, limitations, and potential extensions of the problem setup.}.
%
The proposed Stat-$k$NNAD method consists of two-stages as illustrated in Fig.~\ref{fig:two-stages}.
%
\begin{figure}[htb]
 \begin{center}
  \igr{0.40}{Fig/Fig1.pdf}
  \caption{Schematic illustration of Stat-$k$NNAD: In Stage 1, anomalies are screened using $k$NNAD. In Stage 2, statistical significance is assessed via $p$-values, and instances with $p$-values below a significance level $\alpha$ (e.g., 5\%) are identified as anomalies. We apply $k$NNAD in both the original feature space and the latent space from pretrained deep learning models.}
  \label{fig:two-stages}
 \end{center}
\end{figure}
%
\subsection{Dataset and Its Statistical Model}
\label{subsec:statistical_model_data}
%
In semi-supervised AD problems, the available training dataset consists only of the set of normal instances.
%
Let $\bm{x}_1, \ldots, \bm{x}_n \in \RR^d$ represent the set of $d$-dimensional feature vectors for $n$ normal training instances, where $n$ is the number of instances.
%
To formulate semi-supervised AD as a statistical hypothesis test, we interpret these feature vectors as realizations of random vectors $\bm{X}_1, \ldots, \bm{X}_n \in \RR^d$.
%
The true signal vector of $\bm{X}_i \in \RR^d$ is denoted as $\bm{s}_i \in \RR^d$ for $i \in [n]$, where $[n]$ represents the set of natural numbers up to $n$.
%
We do not assume any prior knowledge or assumptions about true signal vectors $\{\bm{s}_i\}_{i \in [n]}$.
%
Denoting the additive noise for normal training instances as $\bm \veps_1, \ldots, \bm \veps_n \in \RR^d$, the random vector $\bm X_i$ is represented as
\begin{equation}
 \bm{X}_i = \bm{s}_i + \bm{\veps}_i, \quad i \in [n].
 \label{eq:statistical_model}
\end{equation}
%
To conduct statistical inference, we assume the noise vectors $\bm{\veps}_i$ follow a Gaussian distribution with the mean vector $\bm{0}$ and the covariance matrix $\Sigma \in \RR^{d \times d}$.
%
It is assumed that the covariance matrix $\Sigma$ is either known or estimable using independent data --- a reasonable assumption in semi-supervised AD problems where a sufficiently large number of normal instances are available.

\subsection{Statistical Test Formulation}
\label{subsec:kNNAD-test}
%
Let the feature vector of a test instance be denoted as $\bm{x}^{\rm test}$ and its corresponding random version as $\bm X^{\rm test}$. 
%
We assume $\bm X^{\rm test} = \bm{s}^{\rm test} + \bm \veps^{\rm test}$ in the same way as Eq.\eqref{eq:statistical_model}, where $\bm{s}^{\rm test}$ is the (unknown) true signal and $\bm \veps^{\rm test}$ is the Gauusian noise with covariance $\Sigma$.
%
In this paper, we focus on the $k$-nearest neighbor approach as the choice of anomaly detection algorithm.
%
In $k$NNAD, the $k$ normal training instances closest to $\bm{x}^{\rm test}$ are selected from the $n$ available training instances.
%
We denote the set of the $k$ nearest neighbor instances as $\cN \subset [n]$.
%
The details of the $k$NNAD approach for the 1st-stage anomaly screening are described in \S\ref{sec:kNNAD}.

The problem of determining whether the test instance $\bm x^{\rm test}$ is can be done based on whether the true signals of the selected $k$ normal training instances $\{\bm s_i\}_{i \in \cN}$ and that of the test instance $\bm s^{\rm test}$ differ significantly.
%
Let the vector obtained by averaging the (unknown) true signal vectors of the selected $k$ normal training instances 
\begin{align}
 \bar{\bm s}^{\text{$k$NN}} := \frac{1}{k} \sum_{i \in \cN} \bm s_i.
\end{align}
%
The $k$NNAD can be considered as a statistical test with the following null hypothesis ${\rm H}_0$ and alternative hypothesis ${\rm H}_1$:  
\begin{align}
 \label{eq:H0_H1}
 {\rm H}_0: \bm s^{\rm test} = \bar{\bm s}^{\text{$k$NN}}
 ~~~{\text{v.s.}}~~~
 {\rm H}_1: \bm s^{\rm test} \neq \bar{\bm s}^{\text{$k$NN}}.
\end{align}
%
The null hypothesis ${\rm H}_0$ states that the mean true signal of the $k$ nearest normal training instances equals the true signal of the test instance, while the alternative hypothesis ${\rm H}_1$ asserts they are different.
%
By performing a statistical test on these hypotheses, the false detection probability of an anomaly can be quantified using $p$-values.

To solve the statistical test in Eq.~\eqref{eq:H0_H1} and compute the $p$-value, the difference between $\bar{\bm s}^{\text{$k$NN}}$ and $\bm s^{\rm test}$ must be estimated from the observed data.
%
As a reasonable test statistic for the hypothesis test in Eq.~\eqref{eq:H0_H1}, we consider:  
\begin{align}
 \label{eq:test_statistic}
 T(\bm X^{\rm test}, \bm X_1, \ldots, \bm X_n) := \left\| \bm X^{\rm test} - \bar{\bm X}^{\text{$k$NN}} \right\|_1, 
\end{align}
where $\bar{\bm X}^{\text{$k$NN}} = \frac{1}{k} \sum_{i \in \cN} \bm X_i$, and $\| \cdot \|_1$ indicates the $L_1$ norm.
%
The $p$-value for quantifying the statistical significance of the test instance $\bm x^{\rm test}$ is defined as the probability of observing the test statistic greater than or equal to the observed test-statistic $T(\bm x^{\rm test}, \bm x_1, \ldots, \bm x_n)$ under the null hypothesis ${\rm H}_0$ in Eq.~\eqref{eq:H0_H1}.
%
Let $\alpha \in (0, 1)$ represent the significance level (e.g., $\alpha = 0.05$).
%
If test instances with $p$-values less than $\alpha$ are declared as anomalies, the probability of false identification can be controlled to remain below $\alpha$.
%
This enables semi-supervised anomaly detection with guaranteed statistical significance.  
%
The details of $p$-value computation, which is our main contribution, are described in \S\ref{sec:SI}.

\subsection{The $k$NNAD in Latent Feature Space}
%
For detecting anomalies in complex data such as images, signals, and text, effective feature extraction before AD is crucial.
%
In particular, using latent features from deep learning models enhances AD~\cite{li2021cutpaste,chalapathy2019deep,bergman2020deep}.
%
This study applies $k$NNAD in both the original feature space and latent feature spaces from pretrained deep learning models.
%
Hereafter, we consider semi-supervised anomaly detection for images.

With a slight abuse of notation, let us consider a set of $n$ normal images, each with $d$ pixels, denoted as $\bm x_1, \ldots, \bm x_n$.
%
As discussed earlier, these observed images are realizations of random images $\bm X_1, \ldots, \bm X_n$, where each $\bm X_i$ is modeled as an (unknown) true pixel value vector $\bm s_i$ with additive Gaussian noise $\bm \veps_i$, as described in Eq.~\eqref{eq:statistical_model}.  
%
To obtain suitable image features, we assume the availability of a pretrained deep learning model (e.g., those trained on benchmark classification tasks such as ImageNet).  

We define the transformation of an image $\bm x_i \in \mathbb{R}^d$ into a latent feature vector (e.g., the feature representations from the layer preceding the final layer) as $\cA_{\rm DL}: \mathbb{R}^d \ni \bm x_i \mapsto \bm z_i \in \mathbb{R}^{\tilde{d}}$ where $\bm z_i \in \mathbb{R}^{\tilde{d}}$ is the extracted $\tilde{d}$-dimensional feature vector.  
%
The $k$NNAD is then performed on the latent vectors $\bm z^{\rm test} = \cA_{\rm DL}(\bm x^{\rm test}), \quad \bm z_i = \cA_{\rm DL}(\bm x_i), \quad i \in [n]$.
%
For images, where neighboring pixel values often exhibit similarity, we use the following modified test statistic:  
$T_{\rm image}(\bm X^{\rm test}, \bm X_1, \ldots, \bm X_n) = \frac{1}{d} \sum_{j \in [d]} X^{\rm test}_{\cdot j} - \frac{1}{d} \sum_{j \in [d]} \bar{X}^{\text{$k$NN}}_{\cdot j}$.

