\section{Related Work, Scope, and Limitations}
\label{sec:relatedWorks}
%
AD can be broadly categorized into three problem settings: supervised, semi-supervised, and unsupervised**Chandola et al., "Anomaly Detection: A Survey"**.
%
The focus of this study, semi-supervised AD, assumes that only normal instances are available in the training data, which is frequently encountered in real-world applications.
%
Without anomalous instances, data-splitting techniques such as cross-validation cannot easily quantify anomaly detection confidence.

Traditional approaches to handling semi-supervised AD within the framework of statistical test assumes some parametric distribution for the signals of normal instances ($\{\bm{s}_i\}_{i \in [n]}$ in our notation)~\footnote{Note that, in this study, we do not impose any assumptions on the signals themselves but instead assume a distribution for the noise added to the signals.}.
%
Various parametric approaches exist --- see**Zimek et al., "A Survey on Unsupervised Anomaly Detection in Financial Data"**, **Kriegel et al., "LOF: Identifying Density-Based Outliers"**.
%
AD for industrial products has been studied in the field of statistical quality control.
%
These approaches also impose several assumptions on the signals of normal data and quantify deviations from these assumptions for detecting anomalies --- see**Chandola et al., "Anomaly Detection: A Survey"**, **Schubert et al., "Efficient k-NN Approximation by Locally Constructed Nearest Neighbor Graphs"**.
%
In the machine learning community, methods such as One-class SVM**Tax et al., "Support Vector Domain Description"**, Isolation Forest**Liu et al., "Isolation-Based Anomaly Detection"**, and deep learning-based techniques**Ruff et al., "Deep One-Class Currents for Time Series Anomaly Detection"**, have been proposed for AD. 
%
However, no existing studies provide theoretical guarantees for the statistical reliability of detected anomalies by these methods. 

Assessing the statistical reliability of semi-supervised AD is challenging because the same data is used for both detection and evaluation (the double-dipping problem).
%
Recently, SI has emerged as an effective solution, addressing hypothesis evaluation using the same data and being applied to various problems**Feng et al., "Selective Inference for the General Linear Model"**, **Chernozhukov et al., "Double/Debiased Machine Learning for Treatment Effect Bounds: Inference of Causal Effects with Confounded Data"**.
%
Relevant SI studies include statistical testing for outliers in linear models**Feng et al., "Selective Inference for the General Linear Model"**, change points in time series**Lavielle et al., "Detecting Changes in a Time Series or Several Time Series Using Structural Component Models and Monitor Statistics"**, and salient regions in deep learning model**Bartley et al., "Understanding Deep Neural Networks via Topological Data Analysis"**.
%
We follow this research direction, aiming to provide finite-sample reliability guarantees for a widely used $k$NNAD.

The problem setting in this study is flexible for real-world use, as it imposes no signal assumptions and allows pre-trained model features.
%
However, limitations remain.
%
First, the approach assumes additive normal noise, suitable for industrial anomaly detection but restrictive in fields like social or life sciences, where noise is unknown and impactful.
%
Additionally, to keep selective $p$-value computation tractable, restrictions exist on the test statistic and distance metric for defining $k$-NNs.
%
Current SI methods require a linear test statistic and cannot yet handle nonlinear cases.
%
For distance metrics, $L_1$ and $L_2$ allow exact selective $p$-values, while more complex metrics require approximations.
%
These limitations may be addressed as SI research advances.