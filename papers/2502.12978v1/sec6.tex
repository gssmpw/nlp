
\section{Numerical Experiments}
\label{sec:experiments}
%
In this \red{section}, we demonstrate that the proposed method exhibits high power \red{(true positive rate)} while controlling the \red{type I error rate (false positive rate) below} the significance level compared to other methods. 
%
First, experiments are conducted on synthetic datasets, followed by similar experiments on \red{two types of} real datasets.
%
\red{All experiments are conducted with a significance level $\alpha=0.05$.}

\subsection{Methods Comparison}
\label{sec:methods}
%
\red{In the experiments on synthetic datasets and tabular datasets, we compare the proposed method (\texttt{Stat-kNNAD}) with three other methods: \texttt{w/o-pp}, \texttt{naive}, and \texttt{bonferroni}.}
%
\red{Subsequently, in the experiments on image datasets, we additionally compare two further methods: \texttt{opA1} and \texttt{opA2}.}

\begin{itemize}
  \item \texttt{w/o-pp}: An ablation study that excludes the parametric programming technique described in~\S\ref{subsec:computing_selective_p}. 
  \item \texttt{naive}: This method uses a classical $z$-test without conditioning, i.e., we compute the naive $p$-value as $p_\mathrm{naive} = \mathbb{P}_{\mathrm{H}_0}(|T(\bm{Y})| \geq |T(\bm{y})|)$.
  \item \texttt{bonferroni}: This is a method to control the type I error rate by using the Bonferroni correction. There are \red{$\tbinom{n}{k}$} ways to choose the neighbors $\mathcal{N}$, then we compute the Bonferroni corrected $p$-value as $p_\mathrm{bonferroni} = \min(1, \red{\tbinom{n}{k}} \cdot p_\mathrm{naive})$.
  \red{
    \item \texttt{OpA1}: Another ablation study that excludes the selection events for $k$NNAD (i.e., $\mathcal{N}_{\bm{Y}}$, $\mathcal{K}_{\bm{Y}}$, and $\mathcal{S}_{\bm{Y}}$).
    \item \texttt{OpA2}: Another ablation study that excludes the selection events for DNN (i.e., $\mathcal{D}_{\bm{Y}}$ in Appendix~\ref{app:selection_events_of_dnn}).
  }
\end{itemize}




\subsection{Synthetic Datasets}
\label{subsec:experiment_of_synthetic_data}
%
To evaluate the type I error rate, we \red{changed} the training dataset size $n \in \left\{100, 200, 500, 1000\right\}$ and set the data dimension $d=2$. The number of neighbors $k$ was either fixed at 1 or adaptively selected in a data-driven manner from $\left\{1,2,5,10\right\}$.
%
See Appendix~\ref{app:appC1} for results when $d$ and $k$ are changed.
%
For each configuration, we iterated \red{1,000} experiments.
%
In each \red{iteration}, we generated test instance $\bm{x}^{\text{test}} \sim  \mathcal{N}(0, \bm{I}_{d})$ and train instances $\bm{x}_i \sim  \mathcal{N}(0, \bm{I}_{d})$ for $i \in [n]$.
%
To evaluate the power, we changed the signal strength $\delta \in \left\{1, 2, 5, 10\right\}$ and set $d=2$, $n=100$. 
%
\red{We also evaluate two settings of $k$, identical to those used above.}
%
See Appendix~\ref{app:appC2} for results when $d$, $k$, and $n$ are changed.
%
For each configuration, we iterated \red{1,000} experiments.
%
In each \red{iteration}, we generated test instance $\bm{x}^{\text{test}} \sim  \mathcal{N}(\delta, \bm{I}_{d})$ and train instances $\bm{x}_i \sim  \mathcal{N}(0, \bm{I}_{d})$ for $i \in [n]$.
%

%
The results of type I error rate are shown in Figure~\ref{fig:synthetic_fpr}.
%
The \texttt{Stat-kNNAD}, \texttt{w/o-pp}, and \texttt{bonferroni} successfully controlled the type I error rate under the significance level, whereas the \texttt{naive} could not.
%
Because the \texttt{naive} failed to control the type I error rate, we no longer consider its power.
%
The results of power are shown in right side of Figure~\ref{fig:synthetic_power}.
%
Among the methods that controlled the type I error rate, the \texttt{Stat-kNNAD} has the highest power.
% %
% The reduced power of the \texttt{w/o-pp} compared to the \texttt{Stat-kNNAD} can be attributed to its inherent conditioning on more information.
% %
% This problem is known as \emph{over-conditioning} in the context of SI.
% %
% The notably low power of the \texttt{bonferroni} is consistent with the understanding that such classical methods are often too conservative for the large-scale problems considered in this study.
%
\begin{figure}[H]
  \centering
  \begin{minipage}[b]{0.48\linewidth}
      \centering
      \includegraphics[width=0.98\linewidth]{Fig/fpr/zinkou_knn_kchoose.pdf}
      \subcaption{\red{Fixed $k=1$}}
  \end{minipage}
  \begin{minipage}[b]{0.48\linewidth}
      \centering
      \includegraphics[width=0.98\linewidth]{Fig/fpr/zinkou_knn_kchoose.pdf}
      \subcaption{\red{Adaptively Selected $k$}}
  \end{minipage}
  \caption{
    \red{
    Results of type I error rate when changing the dataset size $n$.
    %
    % The left side shows the results for the fixed $k$ setting, while the right side shows the results for the adaptive selection of $k$.
    %
    Our proposed method (\texttt{Stat-kNNAD}), the ablation study (\texttt{w/o-pp}), and the Bonferroni method (\texttt{bonferroni}) successfully control the type I error rate across all settings.  
    %
    The results of the \texttt{bonferroni} are almost zero, because it is too conservative.
    %
    However, the naive method (\texttt{naive}) fails.
    }
  }
  \label{fig:synthetic_fpr}
\end{figure}
%
\begin{figure}[H]
  \centering
  \begin{minipage}[b]{0.48\linewidth}
      \centering
      \includegraphics[width=0.98\linewidth]{Fig/power/zinkou_knn_pwr.pdf}
      \subcaption{\red{Fixed $k=1$}}
  \end{minipage}
  \begin{minipage}[b]{0.48\linewidth}
      \centering
      \includegraphics[width=0.98\linewidth]{Fig/power/zinkou_knn_pwr_kchoose.pdf}
      \subcaption{\red{Adaptively Selected $k$}}
  \end{minipage}
  \caption{
      Power when changinge signal strength $\delta$.
      %
      % \red{
      % The left side shows the results for the fixed $k$ setting, while the right side shows the results for the adaptive selection of $k$.
      % }
      %
      Our proposed method (\texttt{Stat-kNNAD}) outperformed other methods in all settings.
  }
  \label{fig:synthetic_power}
\end{figure}


%
% \begin{figure}[h]
%   {
%       \begin{minipage}[b]{0.49\linewidth}
%           \centering
%           \includegraphics[width=\linewidth]{Fig/fpr/zinkou_knn.pdf}
%           % \subcaption{Fixed $k$}  
%       \end{minipage}
%       \begin{minipage}[b]{0.49\linewidth}
%           \centering
%           \includegraphics[width=\linewidth]{Fig/fpr/zinkou_knn_kchoose.pdf}
%           % \subcaption{Adaptively selected $k$}
%       \end{minipage}
%       \subcaption{Type I Error Rate}
%   }
%   {
%       \begin{minipage}[b]{0.49\linewidth}
%           \centering
%           \includegraphics[width=\linewidth]{Fig/power/zinkou_knn_pwr.pdf}
%           % \subcaption{op1}
%       \end{minipage}
%       \begin{minipage}[b]{0.49\linewidth}
%           \centering
%           \includegraphics[width=\linewidth]{Fig/power/zinkou_knn_pwr_kchoose.pdf}
%           % \subcaption{op1}
%       \end{minipage}
%       \subcaption{Power}
%   }
%   \caption{
%       Type I Error Rate when changing the number of samples (left side) and Power when changing the true coefficient (right side).
%       %
%       % Only our proposed and the oc method are able to control the type I error rate and our proposed method has much higher power than the oc method, in all settings for all types of pipelines.
%       \red{The proposed method (\texttt{Stat-kNNAD}), the ablation study (\texttt{w/o-pp}), and the Bonferroni method (\texttt{bonferroni}) successfully control the type I error rate across all settings and pipeline types.}
%       %
%       \red{Among the methods that control the type I error rate, the proposed method has the highest power across all settings and pipeline types.}
%       % across all settings and pipeline types.
%       %
%       % Our proposed method has much higher power than the oc method in all settings for all types of pipelines.
%   }
%   \label{fig:main_results}
% \end{figure}
% %


\subsection{\red{Real Datasets I: Tabular Data}}
\label{subsec:experiment_of_tabular_data}
%
We conducted evaluations using 10 \red{tabular real-world datasets}. These datasets reflect various real-world problems from different domains. The datasets used in our experiments are listed in Appendix~\ref{app:appC3}.
%
Only numerical features from each dataset were used in the experiments. 
%
{The datasets vary in dimensionality, ranging from 4 to 10 dimensions.}
%
For the number of neighbors $k$, we conducted experiments under two conditions: a fixed setting where $k=1$, and an adaptive selection setting where $k$ was chosen from \red{$\left\{1, 2, 5, 10\right\}$} based on the data.
%
Before conducting the experiments, All datasets are \red{standarized} with each feature having mean 0 and variance 1.
%
The results of the type I error rate and power are shown in Figure~\ref{fig:tabular_results}. 
%
The \texttt{Stat-kNNAD} method outperformed the other methods in terms of power, while controlling the type I error rate \red{below} the significance level.
%





\subsection{\red{Real Datasets II: Image Data}}
\label{subsec:experiment_of_image_data}
%
In this experiment, we used the \red{MVTec AD} dataset~\red{\cite{bergmann2019mvtec,bergmann2021mvtec}} and all experiments are conducted in the latent space.
%
The dataset consists of 15 classes, and we chose 10 classes for the experiments which seem to follow a normal distribution.
%
The datasets used in our experiments are listed in Appendix~\ref{app:appC4}.
%
Before conducting the experiments, All datasets are \red{standarized} with each feature having mean 0 and variance 1.
%
\red{
In this experiment, we employed a ResNet model as a feature extractor.
%
This model was pre-trained by \citet{bergman2020deep}. on the ImageNet dataset for kNNAD in the latent space.
}
% In this experiment, we used a deep learning model called DN2~\cite{bergman2020deepnearestneighboranomaly}.
% %
% We used a ResNet feature extractor that was pretrained on the Imagenet dataset.
%
% The input image was divided into $30 \times 30$ patches, and the patches with positions corresponding to the test data were trained as $n$ training patches.
%
\red{As a preprocessing step, the original image, which has a size of 900 × 900, was divided into 30 × 30 patches, and the patch was used as the test instance.
%
For the training instances, we used 100 patches from the same position as the test instance.
%
We set the number of neighbors $k=1$.
}
%
The results of the type I error rate and power are shown in Figure~\ref{fig:MVtec_results}. 
%
The \texttt{Stat-kNNAD} method outperformed the other methods in terms of power, while controlling the type I error rate \red{below} the significance level.
%
\begin{figure}[H]
  {
      \centering
      \includegraphics[width=\linewidth]{Fig/fpr/legend.pdf}
  }
  {
      \centering
      \includegraphics[width=0.8\linewidth]{Fig/fpr/real_knn.pdf}
      \subcaption{Type I Error Rate for \red{Fixed} $k=1$}
  }
  {
      \centering
      \includegraphics[width=0.8\linewidth]{Fig/fpr/real_knn_kchoose.pdf}
      \subcaption{Type I Error Rate for Adaptively Selected $k$}
  }

  \caption{
    \red{
      Results of the experiments on tabular datasets.
      % 上の２枚はType I Error Rate, 下の２枚はPower
      The two figures show the type I error rate.
      %
      The proposed method (\texttt{Stat-kNNAD}) controlled the type I error rate below the significance level across all datasets.
      %
      The type I error rate of the \texttt{bonferroni} are almost zero, because it is too conservative.
    }
  }
   \label{fig:tabular_results}
\end{figure}

\begin{figure}[H]
  {
      \centering
      \includegraphics[width=0.8\linewidth]{Fig/power/real_knn.pdf}
      \subcaption{Power for \red{Fixed} $k=1$}
  }
  \vspace*{5mm}
  {
      \centering
      \includegraphics[width=0.8\linewidth]{Fig/power/real_knn_kchoose.pdf}
      \subcaption{Power for Adaptively Selected $k$}
  }

  \caption{
    \red{
      Results of the experiments on tabular datasets.
      % 上の２枚はType I Error Rate, 下の２枚はPower
      The two figures show the power.
      %
      The proposed method (\texttt{Stat-kNNAD}) outperformed the other methods in terms of power, across all datasets.
      %
      The power of the \texttt{bonferroni} are almost zero, because it is too conservative.
    }
  }
   \label{fig:tabular_results2}
  
\end{figure}


\begin{figure}[H]
  {
    \centering
    \includegraphics[width=\linewidth]{Fig/fpr/legend2.pdf}
  }
  {
      \centering
      \includegraphics[width=\linewidth]{Fig/fpr/real_dn2.pdf}
      \subcaption{Type I Error Rate \red{Results on Image Data}}
  }
  {
      \centering
      \includegraphics[width=\linewidth]{Fig/power/real_dn2.pdf}
      \subcaption{Power \red{Results on Image Data}}
  }

  \caption{\red{
    Results of the experiments on image datasets.
    %
    The proposed method (\texttt{Stat-kNNAD}) outperformed the other methods in terms of power, while controlling the type I error rate below the significance level.
    %
    The type I error rate and power of the \texttt{bonferroni} are almost zero, because it is too conservative.
    }}
   \label{fig:MVtec_results}
\end{figure}
