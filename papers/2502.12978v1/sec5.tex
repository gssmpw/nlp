\section{Related Work, Scope, and Limitations}
\label{sec:relatedWorks}
%
AD can be broadly categorized into three problem settings: supervised, semi-supervised, and unsupervised~\cite{mehrotra2017anomaly,ramaswamy2000efficient,breunig2000lof}.
%
The focus of this study, semi-supervised AD, assumes that only normal instances are available in the training data, which is frequently encountered in real-world applications.
%
Without anomalous instances, data-splitting techniques such as cross-validation cannot easily quantify anomaly detection confidence.

Traditional approaches to handling semi-supervised AD within the framework of statistical test assumes some parametric distribution for the signals of normal instances ($\{\bm{s}_i\}_{i \in [n]}$ in our notation)~\footnote{Note that, in this study, we do not impose any assumptions on the signals themselves but instead assume a distribution for the noise added to the signals.}.
%
Various parametric approaches exist --- see \citet{barnett1994outliers}.
%
AD for industrial products has been studied in the field of statistical quality control.
%
These approaches also impose several assumptions on the signals of normal data and quantify deviations from these assumptions for detecting anomalies --- see \citet{montgomery2020introduction}.
%
In the machine learning community, methods such as One-class SVM~\citep{scholkopf2001estimating} and Isolation Forest~\citep{liu2008isolation}, and deep learning-based techniques~\citep{chalapathy2019deep,bergman2020deep,li2021cutpaste}, have been proposed for AD. 
%
However, no existing studies provide theoretical guarantees for the statistical reliability of detected anomalies by these methods. 

Assessing the statistical reliability of semi-supervised AD is challenging because the same data is used for both detection and evaluation (the double-dipping problem).
%
Recently, SI has emerged as an effective solution, addressing hypothesis evaluation using the same data and being applied to various problems~\citep{lee2015evaluating,yang2016selective,suzumura2017selective,hyun2018exact,rugamer2020inference,tanizaki2020computing,das2021fast,rugamer2022selective,gao2022selective,le2024cad}.
%
Relevant SI studies include statistical testing for outliers in linear models~\cite{chen2019valid,tsukurimichi2022conditional}, change points in time series~\cite{duy2020computing,hyun2021post,jewell2022testing,shiraishi2024selective}, and salient regions in deep learning model~\cite{duy2020quantifying,daikivalid,shiraishi2024statistical,miwa2024statistical,katsuoka2024statistical}.
%
We follow this research direction, aiming to provide finite-sample reliability guarantees for a widely used $k$NNAD.

The problem setting in this study is flexible for real-world use, as it imposes no signal assumptions and allows pre-trained model features.
%
However, limitations remain.
%
First, the approach assumes additive normal noise, suitable for industrial anomaly detection but restrictive in fields like social or life sciences, where noise is unknown and impactful.
%
Additionally, to keep selective $p$-value computation tractable, restrictions exist on the test statistic and distance metric for defining $k$-NNs.
%
Current SI methods require a linear test statistic and cannot yet handle nonlinear cases.
%
For distance metrics, $L_1$ and $L_2$ allow exact selective $p$-values, while more complex metrics require approximations.
%
These limitations may be addressed as SI research advances.





