\section{Statistical Test for Anomaly Candidates}
\label{sec:SI}
%
In this section, we describe a statistical test for calculating $p$-values for potential anomalies identified in the 1st stage.
%
In the 2nd stage, anomalies are identified by selecting only candidates with $p$-values smaller than the significance level $\alpha$ (e.g., 0.05), allowing for appropriate control of the false detection probability.

\subsection{Main Selection Events}
%
The core idea of SI is to conduct statistical inference based on conditional distribution of the test-statistic conditional on the hypothesis selection event.
%
In our problem, it is necessary to consider two selection events to account for the following two facts: 
%
\begin{description}
 %
 \item[SE1] The test statistic in Eq.\eqref{eq:test_statistic} depends on the selection of $k$-nearest neighbors.
       %
 \item[SE2] The anomaly candidates are selected because the anomaly score in Eq.\ref{eq:anomaly_definition} is grater than the threshold $\theta$.
       %
\end{description}

Before discussing these two selection events, we introduce some additional notations. 
%
Let us denote the $(1+n)d$-dimensional vector obtained by concatenating the test instance $\bm x^{\rm test}$ and $n$ training instance $\bm x_1, \ldots, \bm x_n$, all of which are $d$-dimensional vectors, as  
\begin{align}
\bm y = {\rm vec} \left( \bm x^{\rm test}, \bm x_1, \ldots, \bm x_n \right) \in \RR^{(1+n)d},
\end{align}
where ${\rm vec}$ is the operation that concatenates multiple vectors into a single column vector.  
%
Similarly, the $(1+n)d$-dimensional vector obtained by concatenating $1+n$ $d$-dimensional random vectors is denoted as  
\begin{align}
\bm Y = {\rm vec} \left( \bm X^{\rm test}, \bm X_1, \ldots, \bm X_n \right) \in \RR^{(1+n)d}.
\end{align}
%
With these notations, we rewrite the test statistic in Eq.\eqref{eq:test_statistic} as $T(\bm Y) = \left\| \bm X^{\rm test} - \bar{\bm X}^{\text{$k$NN}} \right\|_1$.

\paragraph{SE1: Selection event for $k$ neighbors}
%
Let the index of the $k$th nearest neighbor in the observed data be denoted as $(k)$.
%
The event that specific $k$ training instances are selected as the $k$-nearest neighbors of the test example is expressed as  
\begin{align}
 \label{eq:k_neighbors_condition}
 {\rm dist}(\bm X^{\rm test}, \bm X_{(\bar{k})}) \le {\rm dist}(\bm X^{\rm test}, \bm X_{(\underline{k})})
\end{align}
for all $(\bar{k}, \underline{k}) \in \{1, \ldots, k\} \times \{k+1, \ldots, n\}$.
%
With a slight abuse of notations, we denote the set of indices for the $k$ nearest neighbors of the test instance $\bm X^{\rm test}$ and $n$ training instances $\bm X_1, \ldots, \bm X_n$ sampled from the statistical model in \S\ref{subsec:statistical_model_data} as $\cN_{\bm Y}$ (in \S\ref{sec:setup}, we only denote this as $\cN$).
%
Hereafter, the conditions in Eq.\eqref{eq:k_neighbors_condition} is represented as $\cN_{\bm Y} = \cN_{\bm y}.$

\paragraph{SE2: Selection event for anomaly screening}
%
Since the statistical test in the 2nd stage is performed only on test instances selected in the 1st-stage anomaly screening, it is essential to consider the selection events associated with it.
%
A test instance is selected in the 1st-stage if its anomaly score, as defined in Eq.~\eqref{eq:anomaly_definition}, exceeds a threshold $\theta$.
%
Because the anomaly score in Eq.~\eqref{eq:anomaly_definition} is calculated based on the $k$-the nearest neighbor instance, the condition on the $k$-the nearest neighbor instance must also be incorporated.
%
Specifically, by conditioning on
\begin{align}
 \label{eq:event2_cond1}
 {\rm dist}(\bm X^{\rm test}, \bm X_{(k)})
 \ge
 {\rm dist}(\bm X^{\rm test}, \bm X_{(k^\prime)})
\end{align}
for $k^\prime = 1, \ldots, k-1$, and 
\begin{align}
 \label{eq:event2_cond2}
 {\rm dist}(\bm X^{\rm test}, \bm X_{(k)})
 \le
 {\rm dist}(\bm X^{\rm test}, \bm X_{(k^\prime)})
\end{align}
for $k^\prime = k+1, \ldots, n$, we can consider only cases where the $k$-the nearest neighbor is the same as the observed case.
%
Furthermore, the condition for the anomaly score is written as
\begin{align}
 \label{eq:event2_cond3}
 \log {\rm dist}\left(\bm X^{\rm test}, \bm X_{(k)} \right) - \frac{\log k}{d} \ge \theta.
\end{align}
%
With the conditions in Eqs.\eqref{eq:event2_cond1}-\eqref{eq:event2_cond3}, we can characterize the selection event that the test case $\bm X^{\rm test}$ is selected as an anomaly candidate in the 1st-stage anomaly screening.
%
Hereafter, these conditions are collectively represented as $\cK_{\bm Y} = \cK_{\bm y}.$

\paragraph{SI with main selection events}
%
The SI taking into account the above two types of selection events is performed based on the sampling distribution of the following conditional test-statistic: 
\begin{align}
 \label{eq:cond_dist1}
 T(\bm Y) \mid \left\{\cN_{\bm Y} = \cN_{\bm y}, \cK_{\bm Y} = \cK_{\bm y} \right\}.
\end{align}
%
Performing statistical inference based on the conditional test-statistic in Eq.\eqref{eq:cond_dist1} means that we consider only cases where the randomness of the data $\bm Y$ satisfies $\cN_{\bm Y} = \cN_{\bm y}$ and $\cK_{\bm Y} = \cK_{\bm y}$, which enables us to circumvent the selection bias associated with the above two selection events.

\subsection{Additional Selection Events}
%
To make the computation of SI tractable, it is common in the SI literature to introduce additional selection events besides the main selection events mentioned above.
%
In our problem, it is necessary to introduce the following two additional selection events:
%
\begin{description}
 %
 \item[SE3] A selection event to make the computation of the $L_1$ norm in the test statistic tractable.
 %	     
 \item[SE4] A selection event related to the sufficient statistic for the nuisance component.
 %
\end{description}
%
We note that introducing these additional selection events does not affect the control of the false detection probability, but tends to reduce the power (true detection probability) of the test.

\paragraph{SE3: Selection event for $L_1$ norm.}
%
SI can be applied when the test statistic $T(\bm Y)$ can be expressed as a linear function of the data $\bm Y$.
%
In our problem, the test statistic $T(\bm Y)$ can be expressed as a linear function of $\bm Y$ by introducing additional conditions.
%
Specifically, to remove the absolute value operator in the definition of $L_1$ norm, we fix the sign of each dimension by condition, which can be written as
\begin{align}
\label{eq:event3}
 {\rm sgn}
(
X^{\rm test}_{\cdot j}
-
\bar{X}^{\text{$k$NN}}_{\cdot j}
)
=
{\rm sgn}
(
 x^{\rm test}_{\cdot j} 
-
\bar{x}^{\text{$k$NN}}_{\cdot j}
)
\end{align}
for all $j \in [d]$. 
%
Together with the condition $\cN_{\bm Y} = \cN_{\bm y}$, the test statistic $T(\bm Y)$ can be expressed as a linear function of $\bm Y$ as  
\begin{align}
 T(\bm Y) = \bm \eta^\top \bm Y
\end{align}
using a certain vector $\bm \eta \in \RR^{(1+n)d}$.
%
Hereafter, the condition in Eq.\eqref{eq:event3} is represented as $\cS_{\bm Y} = \cS_{\bm y}.$

\paragraph{SE4: Selection event for nuisance component.}
%
Finally, to make SI tractable, it is necessary to condition on the sufficient statistic of the nuisance component of the test statistic $T(\bm Y) = \bm \eta^\top \bm Y$.
%
Specifically, the nuisance parameter of the test statistic is expressed as
\begin{align}
 \cQ_{\bm Y}
 :=
 \left(
 I_{(1+n)d}
 -
 \frac{
 \tilde{\Sigma} \bm \eta \bm \eta^\top
 }{
 \bm \eta^\top \tilde{\Sigma} \bm \eta
 }
 \right) \bm Y,
\end{align}
where
$\tilde{\Sigma} \in \RR^{(1+n)d \times (1+n)d}$
is a block-diagonal matrix with $\Sigma$ in each $d \times d$ diagonal block.
%
The conditioning on the nuisance component $\cQ_{\bm Y}$ is a standard practice of SI literature to make the computation tractable~\footnote{
For example, $\cQ_{\bm Y}$ corresponds to $\bm z$ defined in \S\ref{sec:relatedWorks}, Eq.(5.2) in the seminal SI paper \citet{lee2016exact}.
}
%
Hereafter, we denote this selection event as $\cQ_{\bm Y} = \cQ_{\bm y}$.

\subsection{Selective $p$-values}
%
By conditioning on
$\cN_{\bm Y} = \cN_{\bm y}$,
$\cK_{\bm Y} = \cK_{\bm y}$,
$\cS_{\bm Y} = \cS_{\bm y}$,
and 
$\cQ_{\bm Y} = \cQ_{\bm y}$,
we can derive the exact sampling distribution of the test statistic $T(\bm Y)$ under null distribution ${\rm H}_0$, which enables us to compute the valid $p$-value.
%
\begin{definition}[Selective $p$-values]
 The selective $p$-value for a test instance $\bm x^{\rm test}$ is defined as
 \begin{align}
  \label{eq:selective_pvalue}
  p_{\rm selective}
  :=
  \PP_{\rm H_0}
  \left(
  T(\bm Y) \ge T(\bm y)
  \middle |
  \begin{gathered}
   \cN_{\bm{Y}} = \cN_{\bm{y}},\\
   \cK_{\bm{Y}} = \cK_{\bm{y}},\\
   \cS_{\bm{Y}} = \cS_{\bm{y}},\\
   \cQ_{\bm{Y}} = \cQ_{\bm{y}}
  \end{gathered}
  \right).
 \end{align}
\end{definition}
%
The selective $p$-values in Eq.\eqref{eq:selective_pvalue} correctly quantifies the false detection probability as formally stated in the following theorem.
%
\begin{theorem}
 \label{theo:main}
 The selective $p$-values defined in Eq.\eqref{eq:selective_pvalue} satisfies
 \begin{align}
  \label{eq:theorem_a}
  \PP_{\rm H_0}
  \left(
  p_{\rm selective}
  \le
  \alpha
  \middle |
  \begin{gathered}
   \cN_{\bm{Y}} = \cN_{\bm{y}},\\
   \cK_{\bm{Y}} = \cK_{\bm{y}},\\
   \cS_{\bm{Y}} = \cS_{\bm{y}},\\
   \cQ_{\bm{Y}} = \cQ_{\bm{y}}
  \end{gathered}
  \right)
  =
  \alpha,
  ~
  \forall \alpha \in (0, 1).
 \end{align}
 %
 Furthermore, the property in Eq.\eqref{eq:theorem_a} indicates the selective $p$-values are valid in the (unconditional) marginal sampling distribution, i.e., 
 \begin{align}
  \label{eq:theorem_b}
  \PP_{\rm H_0}
  \left(
  p_{\rm selective}
  \le
  \alpha
  \right)
  =
  \alpha,
  ~
  \forall \alpha \in (0, 1).
 \end{align}
\end{theorem}
%
The proof of Theorem~\ref{theo:main} is given in Appendix~\ref{app:proof_theo_main}.

\subsection{Selection Event for Data-driven selection of $k$}
%
In the case of the data-driven option for determining the number of neighbors $k$, its effect must also be appropriately considered as a selection event.  
%
For example, consider the scenario where $k_1, \ldots, k_K$ are candidate values for $k$, and the candidate that maximizes the anomaly score in Eq. \eqref{eq:anomaly_definition} is selected.  
%
Let the selected $k \in \{k_1, \ldots, k_K\}$ be denoted as $k^*$.
%
Then, the selection event is simply given by $\log {\rm dist}(\bm x^{\rm test}, \bm x_{(k^*)}) - \frac{\log k^*}{d} \ge \log {\rm dist}(\bm x^{\rm test}, \bm x_{(k_t)}) - \frac{\log k_t}{d}, \forall t \in [K]$.
%
In the case of data-driven option to determine $k$, in addition to the four selection events mentioned above, this event must also be incorporated as an additional condition.

\subsection{Selection Event for Deep Learning Models}
%
When using $k$NNAD with feature representations from a pre-trained deep learning model, the influence of the model should be considered as a selection event.
%
SI for deep learning has been discussed in prior studies, and tools like the software developed by \citet{katsuoka2025si4onnx} facilitate the analysis of selection events in these models.
%
In this study, we employ methods from earlier research to calculate selective $p$-values, taking into account selection events related to deep learning models.
%
The basic idea in these methods involves decomposing the model into components and representing each as a piecewise linear function.
%
For example, operations in a CNN such as convolution, ReLU activation, max pooling, and up-sampling are represented as piecewise linear functions.
%
In the experiment, we utilize the feature representation of a CNN model pre-trained on the ImageNet database.
%
This model is represented precisely as a composition of piecewise linear functions, facilitating accurate computation of selective $p$-values through parametric programming.
%
Details on the selection events concerning the deep learning model are discussed further in Appendix~\ref{app:selection_events_of_dnn}.

\subsection{Computing Selective $p$-values}
\label{subsec:computing_selective_p}
%
Calculating selective $p$-values is complex, but we effectively use methods from existing SI research.
%
We specifically use the parametric programming (pp)-based method from previous studies~\citep{sugiyama2020more,le2021parametric,duy2022more}.
%
In SI, statistical inference is based on the probability measure within the subspace $\mathcal{Z}$ of the data space $\mathbb{R}^{(1+n)d}$ where selection event conditions are met.
%
By conditioning on the selection event for the nuisance component, $\mathcal{Q}_{\bm{Y}} = \mathcal{Q}_{\bm{y}}$, $\mathcal{Z}$ reduces to a one-dimensional subspace.
%
The selection events are formulated as unions of intersections of linear or quadratic inequalities, suitable when using $L_1$ or $L_2$ distances for $k$-nearest neighbors.
%
$\mathcal{Z}$ consists of finite number of intervals along a line in the $(1+n)d$-dimensional space, and the pp-based method systematically enumerates all intervals that meet these conditions.

Since the noise is Gaussian, the test statistic $T(\bm{Y})$ under the null hypothesis ${\rm H}_0$ follows a one-dimensional truncated Gaussian distribution within the subspace $\mathcal{Z}$, comprising finite intervals along a line.
%
The selective $p$-value is calculated as the tail probability of this truncated distribution.
%
Early SI research often simplified calculations by assuming $\mathcal{Z}$ as a single interval under additional conditions, which still controls the false detection probability but reduces detection power.
%
In our problem, a similar simplification can be considered by enforcing $\mathcal{Z}$ to be a single interval.
%
In the experiments in \S\ref{sec:experiments}, we conduct an ablation study comparing this simple approach (denoted as {\tt w/o-pp}) as one of the baselines.


