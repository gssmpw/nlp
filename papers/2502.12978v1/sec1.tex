\section{Introduction}
\label{sec:intro}
%
In this study, we consider semi-supervised anomaly detection (AD) using the $k$-nearest neighbor (NN) approach~\citep{breunig2000lof,ramaswamy2000efficient,mehrotra2017anomaly}.
%
Semi-supervised AD detects anomalies using only normal training instances.
%
In many practical cases, such as industrial AD, anomalous instances are rare, making semi-supervised AD essential.
%
We focus on the $k$-nearest neighbor anomaly detection ($k$NNAD) among various semi-supervised AD methods.
%
The $k$NNAD approach is simple yet effective, offering flexibility, minimal data assumptions, and adaptability to different distance metrics.

An important challenge in semi-supervised AD is quantifying the reliability of detected anomalies~\citep{barnett1994outliers,chandola2009anomaly,montgomery2020introduction}.
%
Without anomalous training instances in training, estimating detection accuracy is challenging.
%
Furthermore, modeling anomaly distributions is difficult since similar anomalies may not occur repeatedly.
%
To address this issue, we formulate semi-supervised $k$NNAD as a statistical test to quantify false AD probability using $p$-values.
%
If the $p$-values are accurately calculated and, anomalies with $p$-values below a desired significance level (e.g., 5\%) can be detected, ensuring that the detected anomalies are statistically significantly different from normal instances in the specified significance level.

However, a critical challenge emerges when formulating semi-supervised AD as a statistical test.
%
The primary issue is conducting both detection and testing of anomalies on the same data, which makes accurate $p$-value calculation intractable. 
%
In traditional statistics, selecting and evaluating a hypothesis on the same data causes selection bias in $p$-values, leading to inaccuraciesâ€”a problem known as \emph{double dipping}~\citep{breiman1992little,kriegeskorte2009circular,benjamini2020selective}.
%
In semi-supervised AD, since only normal instances are available, both the detection and evaluation must rely on the same data.
%
Thus, a naive statistical test formulation cannot avoid the double dipping issue.

To address this issue, we employ \emph{Selective Inference (SI)}, a statistical framework gaining attention in the past decade~\citep{fithian2014optimal,taylor2015statistical,lee2016exact}.
%
SI ensures valid statistical inferences after data-driven selection of hypotheses by correcting selection biases, ensuring accurate \( p \)-values and confidence intervals.
%
SI was originally designed to assess feature selection reliability, enabling accurate significance evaluation even when selection and evaluation use the same dataset~\citep{lee2016exact,tibshirani2016exact,duy2022more}.
%
The key principle of SI is to perform statistical inference conditioned on the selected hypothesis.
%
By using conditional probability distributions, SI effectively mitigates selection bias from double dipping.
%
In this study, we propose \emph{Statistically Significant $k$NNAD (Stat-$k$NNAD)}, a method that performs statistical hypothesis test conditioned on anomalies detected by the $k$NNAD algorithm.
%
The Stat-$k$NNAD offers theoretical guarantees and precise quantification of false anomaly detection probability.

Our contributions are summarized as follows.
%
First, we formulate semi-supervised AD using $k$NNAD as a statistical test within the SI framework, enabling accurate reliability quantification of detected anomalies.
%
While $k$NNAD is widely used, no existing method theoretically and accurately quantifies the false identification probability of detected anomalies.
%
Second, to enable conditional inference for $k$NNAD, we decompose it into tractable selection events (linear or quadratic inequalities) within the SI framework, Notably, applying $k$NNAD in deep learning-based latent spaces requires representing complex deep learning operations in a tractable form, posing a significant technical challenge (details in \S~\ref{sec:SI}). 
%
Finally, through experiments on various datasets and industrial product AD, we validate the effectiveness of Stat-$k$NNAD. Specifically, for industrial product images, we show that applying $k$NNAD in the latent space of a pretrained CNN effectively addresses practical challenges.
%
A more comprehensive discussion on related work, as well as the scope and limitations of this work, is presented in \S\ref{sec:relatedWorks}.
