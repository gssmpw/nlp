\section{Anomaly Screening by $k$-NN}
\label{sec:kNNAD}
%
This section outlines the first anomaly screening stage~\footnote{
%
Note that this anomaly screening approach is already well-known and does not contain any novel technical aspects.
}.

\paragraph{The $k$-nearest neighbor anomaly detection ($k$NNAD)}
%
In $k$NNAD, the distance between the test instance $\bm x^{\rm test}$ and its $k$-th nearest normal training instance among $n$ instances $\{\bm{x}_i\}_{i \in [n]}$ is used as a criterion.  
%
We denote the $k$-th nearest normal training instance as $\bm{x}_{(k)}$ and the distance between $\bm x^{\rm test}$ and $\bm x_{(k)}$ as ${\rm dist}(\bm{x}^{\rm test}, \bm{x}_{(k)})$.  
%
Since the choice of $k$ affects the distance magnitude, we adopt the following well-known anomaly score~\cite{mehrotra2017anomaly}:  
\begin{align}
 \label{eq:anomaly_definition}
 a(\bm x^{\rm test}) = \log {\rm dist}\left(\bm x^{\rm test}, \bm x_{(k)} \right) - \frac{\log k}{d}, 
\end{align}
where the first term represents the log-scale distance, and the second term adjusts for the influence of $k$'s selection~\footnote{The choice of Eq.~\eqref{eq:anomaly_definition} is based on certain assumptions and heuristics in the literature, but its details are beyond the scope of this paper. For further information, refer to \citet{mehrotra2017anomaly}.}.  
%
In the first anomaly screening stage, if Eq.~\eqref{eq:anomaly_definition} exceeds a certain threshold $\theta$, the test instance $\bm x^{\rm test}$ is selected as a candidate anomaly.  
%
The threshold $\theta$ is typically set based on the empirical distribution of anomaly scores among normal instances.

\paragraph{Selection of $k$}
%
The choice of $k$ greatly affects the results in $k$NNAD.
%
Users can set $k$ based on domain knowledge or experience.
%
However, when domain knowledge is limited or data is complex, a systematic approach is needed.
%
In semi-supervised AD, unlike supervised learning such as $k$-NN classification or regression, it is not possible to determine $k$ through data splitting.
%
A data-driven method to select $k$ calculates the anomaly score for various $k$ values per test instance $\bm x^{\rm test}$, choosing the $k$ that maximizes this score.
%
In the Stat-$k$NNAD method, whether $k$ is chosen specifically or determined through this heuristic, the false detection probability is controlled.
