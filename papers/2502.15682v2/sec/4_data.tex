
\section{Best Practice for Data Curation \& Training}

The recent visual-language foundation models are often trained on massive numbers (billions) of paired image-caption samples, with considerable computing resources. Here, we explore a `student friendly' \emph{best practice} for data curation that enables improving large-scale visual-language models with limited resources.
Specifically, there are two major challenges to be addressed:
(i) training with a large batch size is challenging, due to limitation on GPU memory; (ii) training on billions of samples is prohibitively expensive on computation cost. 
In Section~\ref{sec:hard_mining}, we discuss the strategy of global hard sample mining to make the training more effective with a small batch size. 
In Section~\ref{sec:large_training_data}, we detail the procedure for selecting and curating an image-text training dataset with maximum information.


\subsection{Global Hard Sample Mining}
\label{sec:hard_mining}

Training CLIP, SigLIP and BLIP-2 often requires a large batch size,
as this increases the chances of having hard training samples, 
and leads to improved contrastive and discriminative capability of the model. Here, we adopt a strategy of global hard sample mining to group hard samples in batches to make training with small batch size more effective. 

More specifically, for each pair of image-text pair $I_i$ and $T_i$, we compute their image and text features using pre-trained CLIP image and text encoders; 
then we group a batch by collecting other image-text pairs whose images have high CLIP feature similarity scores with the \emph{reference text} $T_i$. 
Examples of our generated training batches are in Figure~\ref{fig:hard_sample}. Assuming the training batch size is $B$ and the original dataset has $N$ image-text pairs, the algorithm gives us $B \times N$ training samples grouped by batches. In practice, we train our model on a random subset of it. 

\begin{figure}[t]
	\centering
\includegraphics[height=0.5\linewidth]{images/hard_sample.pdf}
\vspace{-5mm}
\caption{
\textbf{Examples of generated training batches via global hard sample mining.} 
For each row, the first sample is used to group other samples. 
Captions for row 1~(from left to right): `a wooden table with no base'; `a wooden table with a couple of folding legs on it'; 
`a table that has a metal base with an olive wood top'; 
`small table outdoors sitting on top of the asphalt'. 
Captions for row 2 (from left to right): 
`a huge body of blue ice floats in a mountain stream'; 
`the big chunk of glacier is falling off of the cliff'; 
`there is a broken piece of glass that has been broken from the ground'; `a body of water surrounded by a forest near a mountain'. 
It can be observed that the images and captions are very similar to each other, and significantly more close than images and captions in a random batch. 
} 
\vspace{-3mm}
\label{fig:hard_sample}
\end{figure}

\subsection{Selection and Curation of Large-Scale Dataset}
\label{sec:large_training_data}

In the literature, a number of large-scale image-text training datasets have been introduced, such as CC3M~\cite{sharma2018conceptual}, DataComp~\cite{gadre2024datacomp}, etc. 
A recent effort~\cite{vasu2024mobileclip} utilises large-scale pre-trained image captioning models to generate synthetic captions for DataComp images, providing more information for training. 
Experiments~\cite{vasu2024mobileclip} show that training CLIP on the generated DataCompDR12M dataset achieves better performance than on DataComp1B, although only 1\% data samples are used. However, in our case, even using DataCompDR12M to train our models still takes prohibitive time (about 200 GPU days) to train ELIP-B on the 12M data with A6000/A40 GPUs. 


To accelerate the ELIP-B training, we adopt the batch selection strategy,
that construct batches by learnability, as inspired by JEST~\cite{evans2024data}. Specifically, we run both ELIP-B (the learner) and the pre-trained BLIP-2 model (the reference model) on the grouped batches by our global hard sample mining strategy as in Section~\ref{sec:hard_mining}. We therefore select the batches with the top 10\% highest learnability, where the learnability of a batch is calculated as the difference between the losses of our model and the reference model.


\begin{figure}[t]
	\centering
\includegraphics[height=0.5\linewidth]{images/ood_benchmark.pdf}
\vspace{-2mm}
\caption{
\textbf{Examples of the out-of-distribution benchmarks.} Occluded COCO is on the left, and  ImageNet-R is on the right. For both benchmarks, the positive images contain the object described by the text query while the negative images do not contain the object. We display positive images in the first row and negative images in the second row.
For Occluded COCO, the target object in the image is occluded, making it more difficult to be retrieved. For example, for the text query \emph{Bicycle} in Occluded COCO, positive images have an occluded bicycle (highlighted in dashed box) while negative images do not have a bicycle in it; for the text query \emph{Goldfish} in ImageNet-R, positive images have goldfish while negative images do not have goldfish.
} 
\vspace{-3mm}
\label{fig:ood_benchmark}
\end{figure}


\begin{table*}[t]
    \centering
    \tabcolsep=0.10cm
    \begin{tabular}{c|cccc|cccccccc}
    \toprule 

     \multirow{2}{*}{Setting} &\multirow{2}*{\thead{Architecture}} &\multirow{2}*{\thead{Training \\ Dataset}}
     &\multirow{2}*{\thead{Hard Sample \\ Mining}} 
     &\multirow{2}*{\thead{Multiple \\ Prompts}}
     & \multicolumn{3}{c}{COCO} & \multirow{2}*{Avg.} & \multicolumn{3}{c}{Flickr}& \multirow{2}*{Avg.} \\
     \cline{6-8} \cline{10-12}
     & & & & & R@1 & R@5 & R@10 &  & R@1 & R@5 & R@10 & \\
    
    \midrule
    $\mathbb{A}$ & CLIP & - & &  & 40.2 & 66.0 & 75.6 & 60.6 & 67.6 & 88.3 & 93.0 & 83.0 \\ 
    $\mathbb{B}$ & ELIP-C & CC3M~\cite{sharma2018conceptual} & & & 40.7 & 66.2 & 76.1 & 61.0 & 68.8 & 88.9 & 93.8 & 83.8 \\ 
    $\mathbb{C}$ & ELIP-C & CC3M~\cite{sharma2018conceptual} & \checkmark & & 41.8 & 67.5 & 77.5 & 62.3 & 69.5 & 89.7 & 94.1 & 84.4 \\ 
    $\mathbb{D}$ &ELIP-C & DataCompDR~\cite{vasu2024mobileclip} & \checkmark  & & 44.2 & 70.0 & 79.5 & 64.6 & 71.3 & 90.6 & 94.4 & 85.4 \\ 
    $\mathbb{E}$ &ELIP-C & DataCompDR~\cite{vasu2024mobileclip} & \checkmark  & \checkmark & \textbf{45.6} & \textbf{71.1} & \textbf{80.4} & \textbf{65.7} & \textbf{72.3} & \textbf{90.6} & \textbf{94.7} & \textbf{85.9} \\ 
    \bottomrule
    \end{tabular}
    \caption{\textbf{Ablation study on ELIP-C} for choice of training dataset, hard sample mining, and number of prompt vectors generated.
    }
    \label{tab:ablation}
\end{table*}


\begin{table*}[h]
    \centering
    \tabcolsep=0.1cm
    \begin{tabular}{lccccccccc}
    \toprule
 \multirow{2}*{Model} &\multirow{2}*{Year} & \multicolumn{3}{c}{COCO} & \multirow{2}*{Average} & \multicolumn{3}{c}{Flickr}& \multirow{2}*{Average} \\
\cmidrule(lr){3-5} \cmidrule(lr){7-9}
& & Recall@1 & Recall@5 & Recall@10 &  & Recall@1 & Recall@5 & Recall@10 & \\
\midrule 

\emph{CLIP}~\cite{radford2021learning,openclip}& 2021 & 40.16 & 65.95 & 75.62 & 60.58 & 67.56 & 88.34 & 93.00 & 82.97\\ 
    \emph{ELIP-C(Ours)} & - & \textbf{45.61} & \textbf{71.08} & \textbf{80.43} & \textbf{65.71} & \textbf{72.30} & \textbf{90.62} & \textbf{94.68} & \textbf{85.87} \\ 

\midrule
\emph{SigLIP}~\cite{zhai2023siglip}& 2023 & 54.21 & 76.78 & 84.24 & 71.74 & 82.96 & 96.10 & 98.04 & 92.37 \\ 
    \emph{ELIP-S(Ours)} & - & \textbf{61.03} & \textbf{82.62} & \textbf{88.70} & \textbf{77.45} & \textbf{87.62} & \textbf{98.16} & \textbf{99.16} & \textbf{94.98} \\ 

\midrule
\emph{SigLIP-2}~\cite{tschannen2025siglip2}& 2025  & 56.87 & 78.79 & 85.49 & 73.72 & 83.94 & 96.62 & 98.20 & 92.92 \\ 
    \emph{ELIP-S-2(Ours)} & - & \textbf{62.91} & \textbf{83.86} & \textbf{89.70} & \textbf{78.82} & \textbf{87.74} & \textbf{97.96} & \textbf{98.94} & \textbf{94.88} \\ 
    
\midrule
    \emph{BLIP-2*}~\cite{li2023blip}& 2023 & 68.25 & 87.72 & 92.63 & 82.87 & 89.74 & 98.18 & 98.94 & 95.62\\ 
    Q-Pert.(E)*~\cite{sogi2024object} & 2024 & 68.34 & 87.76 & 92.63 & 82.91 & 89.82 & 98.20 & 99.04 & 95.69\\ 
    Q-Pert.(D)*~\cite{sogi2024object} & 2024 & 68.35 & 87.72 & 92.65 & 82.91 & 89.86 & 98.20 & 99.06 & 95.71\\ 
    \emph{ELIP-B(Ours)*} & - & \textbf{68.41} & \textbf{87.88} & \textbf{92.78} & \textbf{83.02}& \textbf{90.08} & \textbf{98.34} & \textbf{99.22} & \textbf{95.88}\\
    \bottomrule
    \end{tabular}
    \caption{
\textbf{Comparison with recent state-of-the-art methods.} Top: CLIP-based models; Middle: SigLIP-based models; Bottom: BLIP-2-based models. 
ELIP-C/ELIP-S brings a significant \textbf{zero-shot} performance boost of CLIP/SigLIP architectures, and ELIP-B outperforms the state-of-the-art BLIP-2 model. 
    Results for models without * are zero-shot, whereas results for models with * are only zero-shot on Flickr as the BLIP-2 model has been fine-tuned on COCO and the * models are based on BLIP-2. 
    However, our method brings an improvement over BLIP-2 on both benchmarks when trained on DataCompDR.
    }
    \label{tab:compare_sota}
\end{table*}


\section{Evaluation Datasets}

We evaluate our models on both the standard text-to-image retrieval benchmarks, COCO~\cite{lin2014microsoft} and Flickr~\cite{plummer2015flickr30k} (Section~\ref{sec:standard_benchmark}), as well as on out-of-distribution benchmarks (Section~\ref{sec:ood_benchmark}) that we newly set up.

\subsection{Standard Benchmarks}
\label{sec:standard_benchmark}


\noindent \textbf{COCO}~\cite{lin2014microsoft},
is a large-scale dataset for studying object detection, segmentation, and captioning. In terms of captioning, each image is annotated with 5 different captions. Previous works use the test split of 5,000 images and 25,010 captions for the evaluation of text-to-image retrieval. 

\vspace{2pt} \noindent \textbf{The Flickr30k Dataset}~\cite{plummer2015flickr30k} contains images collected from Flickr, together with 5 reference sentences provided by human annotators. The test set for text-to-image retrieval consists of 1,000 images and 5,000 captions.

\vspace{2pt} \noindent \textbf{Evaluation Metrics.} 
We adopt the standard metrics for assessing retrieval performance,
namely, Recall@1, Recall@5 and Recall@10. Recall@k denotes the proportion of relevant images that are successfully retrieved within the top-$k$ results for each text query. 



\subsection{Out-of-Distribution Benchmarks}
\label{sec:ood_benchmark}

To evaluate a model's capability for text-to-image retrieval in out-of-distribution~(OOD) scenarios, we set up two new benchmarks for text-based image retrieval.
Specifically, \emph{Occluded COCO} focuses on the retrieval of occluded objects, and \emph{ImageNet-R} emphasizes retrieving objects from various unusual domains such as cartoons, sketches, etc. 
Figure~\ref{fig:ood_benchmark} shows examples from the Occluded COCO and ImageNet-R benchmarks.

\vspace{2pt}\noindent \textbf{Occluded COCO} 
is curated with annotations from~\cite{lee2022instance}, 
with the method as described in~\cite{zhan2022tri}, where the occlusion relationship is utilised to collect images containing occluded objects. 
This dataset aims to evaluate the model's performance on retrieving images with occluded target objects against images that do not contain the target object.

\vspace{2pt} \noindent \textbf{ImageNet-R} is generated using annotations from~\cite{hendrycks2021many} and aims to examine the model's performance for retrieval across various domains, 
for example, art, cartoons, deviantart, graffiti, embroidery, graphics, origami, paintings, patterns, plastic objects, plush objects, sculptures, sketches, tattoos, toys, and video games.


\vspace{2pt} \noindent \textbf{Evaluation Metrics.} 
Here, we use mAP as the evaluation metric. This is because there might be multiple positive images for each text query.

