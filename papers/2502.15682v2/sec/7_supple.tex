

\onecolumn


\appendix
\section*{Appendix}

\section{Additional Implementation Details}
\label{sec:sup_implement_detail}

In this section, we provide additional implementation details regarding: the choice of $k$ for re-ranking in Section~\ref{sec:supple_choice_of_k} as mentioned in Section~7; the general experiments in Section~\ref{sec:sup_implement_detail_general} as mentioned in Section~5.1 and Section~7; fine-tuning experiments in Section~\ref{sec:sup_implement_detail_finetune} as mentioned in Section~7.2; and the Occluded COCO benchmark in Section~\ref{sec:sup_implement_detail_occluded} as mentioned in Section~6.2.


\subsection{Choice of $k$ for re-ranking}
\label{sec:supple_choice_of_k}


\begin{table*}[h]
    \centering
    \tabcolsep=0.25cm
    \begin{tabular}{cccccc}
    
    
\toprule
 CLIP & & & & & \\
\midrule 
Dataset & Top-20 & Top-50 & Top-100 & Top-200 & Top-500 \\

COCO & 84.25 & 93.30 & 97.67 & 99.12 & 99.65 \\
Flickr & 95.90 & 98.32 & 99.24 & 99.82 & 99.98 \\

\midrule

Dataset & Top-100 & Top-200 & Top-500 & Top-1000 & Top-2000 \\

Occluded COCO & 65.31 & 72.00 & 81.14 & 87.66 & 93.71 \\
ImageNet-R & 57.33 & 77.83 & 91.13 & 95.05 & 97.18 \\

\bottomrule



\toprule
 SigLIP & & & & & \\
\midrule 
Dataset & Top-20 & Top-50 & Top-100 & Top-200 & Top-500 \\

COCO & 90.14 & 96.07 & 98.77 & 99.44 & 99.78 \\
Flickr & 98.98 & 99.64 & 99.82 & 99.92 & 100.00 \\

\midrule

Dataset & Top-50 & Top-100 & Top-200 & Top-500 & Top-1000 \\

Occluded COCO & 68.84 & 75.90 & 81.18 & 86.96 & 91.94 \\
ImageNet-R & 39.22 & 67.46 & 90.46 & 98.14 & 99.04 \\

\bottomrule


\toprule
 SigLIP-2 & & & & & \\
\midrule 
Dataset & Top-20 & Top-50 & Top-100 & Top-200 & Top-500 \\

COCO & 91.10 & 96.59 & 98.85 & 99.55 & 99.80 \\
Flickr & 99.34 & 99.80 & 99.92 & 99.94 & 100.00 \\

\midrule

Dataset & Top-50 & Top-100 & Top-200 & Top-500 & Top-1000 \\

Occluded COCO & 70.69 & 79.57 & 85.67 & 91.28 & 95.34 \\
ImageNet-R & 39.39 & 67.74 & 90.90 & 98.24 & 99.10 \\

\bottomrule




\toprule
 BLIP-2 & & & & & \\
\midrule 
Dataset & Top-5 & Top-10 & Top-20 & Top-50 & Top-100 \\

COCO & 86.08 & 91.85 & 95.96 & 98.80 & 99.63 \\
Flickr & 97.64 & 99.06 & 99.52 & 99.82 & 99.86 \\

\midrule

Dataset & Top-50 & Top-100 & Top-200 & Top-500 & Top-1000 \\

Occluded COCO & 68.67 & 77.40 & 82.12 & 88.17 & 92.67 \\
ImageNet-R & 35.97 & 61.21 & 82.81 & 92.88 & 94.99 \\

    \bottomrule


    
    \end{tabular}
    \caption{\textbf{Recall @ Different $k$ for CLIP, SigLIP and BLIP-2 initial ranking.} See text for a more detailed discussion.}
    \label{tab:choice_k}
\end{table*}



In terms of the selection of the top-$k$ values for re-ranking as mentioned in Section~7, the value of $k$ is chosen such that the recall at that $k$ is high in the original ranking, and also so that the inference is fast. 
For ELIP-C, $k$ is set to 100 for COCO and Flickr, 500 for Occluded COCO, and 1000 for ImageNet-R; for ELIP-S and ELIP-S-2, $k$ is set to 100 for COCO and Flickr, 500 for Occluded COCO, and 200 for ImageNet-R; for ELIP-B, $k$ is set to 20 for COCO and Flickr, 100 for Occluded COCO, and 200 for ImageNet-R. 
More specifically, as in Table~\ref{tab:choice_k}, for 
CLIP, the recall@100 for COCO/Flickr is over 95\%, recall@500 for Occluded COCO is over 80\%, recall@1000 for ImageNet-R is over 95\%; For SigLIP/SigLIP-2, the recall@100 for COCO/Flickr is over 95\%, recall@500 for Occluded COCO is over 85\%, recall@200 for ImageNet-R is over 90\%; for BLIP-2, the recall@20 for COCO/Flickr is over 95\%, recall@100 for Occluded COCO is over 75\%, recall@200 for ImageNet-R is over 80\%. 
We have experimented with increasing $k$ for re-ranking, but there is no significant performance boost while the computational cost is much higher.

\subsection{Implementation Details for General Experiments}
\label{sec:sup_implement_detail_general}

As mentioned in Section~5.1, in practice we use a random subset of 6M samples for training ELIP-C/ELIP-S and a random subset of 1M samples for training ELIP-B. For ELIP-B, we first select the samples with the top 10\% highest learnability as described in Section~5.2, and then train the ELIP-B architecture with the selected training samples. For ELIP-C, we use the default backbone (ViT-B) given limited computing resources and for the ablations, while for ELIP-S and ELIP-B we use the best backbone (ViT-G for BLIP-2 and SigLIP-2, ViT-SO400M for SigLIP) to show the effectiveness of our method over the state-of-the-art model. 
As we use large backbones for ELIP-S and it increases GPU memory, for every training batch we take 10 samples out of the batch of 40 in the ELIP-C training dataset.

The implementation of the MLP Mapping Network is three linear layers with a GELU between each pair of layers. We expand the output dimension to be $n$ times when we generate $n$ tokens and then divide the generated vector into $n$ tokens. 
We have also investigated other architectures for the Mapping Network, such as transformer layers, different numbers of layers of MLP, and linear network, but there is no significant performance advantage found. We therefore use the current architecture which is simple but effective.

\subsection{Implementation Details for Fine-Tuning on COCO and ImageNet}
\label{sec:sup_implement_detail_finetune}

As mentioned in Section~7.2, we have further fine-tuned our models (pre-trained on DataCompDR) on COCO and ImageNet, respectively. 
For both datasets, we first group the hard samples in batches using the method described in Section~5.1. 
The captions for the images are category names, 
and we ensure that in a batch one category can only appear once, 
so that there will not be noise in training. 
We fine-tune ELIP-C, ELIP-S, and ELIP-B with an initial learning rate of $1e-5$ for 10k iterations. 
Although there is still a domain gap between COCO and Occluded COCO, ImageNet, and ImageNet-R, we find that even fine-tuning on COCO and ImageNet can already bring a boost to the performance on Occluded COCO and ImageNet-R as the caption domain is closer. It is not feasible to fine-tune on Occluded COCO and ImageNet-R because Occluded COCO has very few data samples and ImageNet-R is designed for evaluation.



\subsection{Implementation Details for Setting up the Occluded COCO Benchmark}
\label{sec:sup_implement_detail_occluded}

As mentioned in Section~6.2, we set up the Occluded COCO benchmark using the annotations from~\cite{lee2022instance}. More specifically, for each COCO category, we collect two lists - one for the images containing occluded object of such category, and the other for the images not having object of such category.
For images containing occluded object of such category, 
we select the ones that meet the following criteria: 
1) The image contains object of the category; 2) All instances of such category are occluded by at least one other instance, \emph{i.e.,} have an occludee-occluder relationship with another instance in the annotations of~\cite{lee2022instance}. The idea of collecting images containing occluded instances is similar to~\cite{zhan2022tri}, which utilises the occlusion relationship. 






\clearpage
\section{Additional Qualitative Results}
\label{sec:sup_qualitative}

In this section, we provide more qualitative results as mentioned in Section~7 of the main paper, including: 
1) Qualitative comparison between the rankings of CLIP and ELIP-C on COCO (Figure~\ref{fig:supple_qualitative_clip1}), Flickr (Figure~\ref{fig:supple_qualitative_clip1-2}), Occluded COCO (Figure~\ref{fig:supple_qualitative_clip2}) and ImageNet-R (Figure~\ref{fig:supple_qualitative_clip2-2}); 
2) Qualitative comparison between the rankings of SigLIP/SigLIP-2 and ELIP-S/ELIP-S-2 on COCO (Figure~\ref{fig:supple_qualitative_siglip1}), Flickr (Figure~\ref{fig:supple_qualitative_siglip1-2}), Occluded COCO (Figure~\ref{fig:supple_qualitative_siglip2}) and ImageNet-R (Figure~\ref{fig:supple_qualitative_siglip2-2}); 
3) Qualitative comparison between the rankings of BLIP-2 and ELIP-B on COCO (Figure~\ref{fig:supple_qualitative_blip1}), Flickr (Figure~\ref{fig:supple_qualitative_blip1-2}), Occluded COCO (Figure~\ref{fig:supple_qualitative_blip2}) and ImageNet-R (Figure~\ref{fig:supple_qualitative_blip2-2}); 
4) Attention maps for CLIP and ELIP-C (Figure~\ref{fig:supple_attention_clip} and Figure~\ref{fig:supple_attention_clip-2}); 
5) Attention maps for SigLIP/SigLIP-2 and ELIP-S/ELIP-S-2(Figure~\ref{fig:supple_attention_siglip} and Figure~\ref{fig:supple_attention_siglip-2}); 
6) Attention maps for BLIP-2 and ELIP-B (Figure~\ref{fig:supple_attention_blip} and Figure~\ref{fig:supple_attention_blip-2}).


\clearpage
\subsection{Qualitative Comparison of CLIP and ELIP-C}

Figure~\ref{fig:supple_qualitative_clip1} shows more qualitative comparison between the CLIP initial ranking and ELIP-C re-ranking on COCO. 

\begin{figure*}[h]
	\centering
\includegraphics[height=0.75\linewidth]{images/supple_qualitative_clip1.pdf}
	\caption{
\textbf{Qualitative Comparison on COCO for CLIP and ELIP-C.} 
For each example, we show both top-5 rankings (from left to right) and highlight the ground truth image in a black dashed box.
In the first and second examples (from top to bottom), the text queries are `Two young men playing a game of soccer', and `A man cuts into a small cake with his sharp knife', and the ground truth images are not in top-5 images of the CLIP initial ranking, but ranked top-1 in our ELIP-C re-ranking. 
	} 
	\label{fig:supple_qualitative_clip1}
	\end{figure*}


\clearpage
Figure~\ref{fig:supple_qualitative_clip1-2} shows more qualitative comparison between the CLIP initial ranking and ELIP-C re-ranking on Flickr. 


\begin{figure*}[h]
	\centering
\includegraphics[height=0.75\linewidth]{images/supple_qualitative_clip1-2.pdf}
	\caption{
\textbf{Qualitative Comparison on Flickr for CLIP and ELIP-C.} For each example, we show both top-5 rankings (from left to right) and highlight the ground truth image in a black dashed box.
In the first example, the text query is `A man in blue overalls and red shirt holding a chainsaw', and the ground truth image ranks top-3 in the initial ranking but top-1 in our re-ranking. In the second example, the text query is `A muzzled greyhound dog wearing yellow and black is running on the track', and the ground truth image is not in top-5 images of the CLIP initial ranking, but ranked top-1 in our ELIP-C re-ranking. 
	} 
	\label{fig:supple_qualitative_clip1-2}
	\end{figure*}


\clearpage
Figure~\ref{fig:supple_qualitative_clip2} displays qualitative comparison of CLIP initial ranking and our ELIP-C re-ranking on Occluded COCO. 

\begin{figure*}[h]
	\centering
\includegraphics[height=0.75\linewidth]{images/supple_qualitative_clip2.pdf}
	\caption{
\textbf{Qualitative Comparison on Occluded COCO for CLIP and ELIP-C.} We display the top-5 retrieved images. Negative samples (errors) are highlighted in an orange solid box. 
For the first example, the text query is `bowl' and CLIP confuses it with `frisbee', `toilet', and `plate' in top-5 retrieved images; for the second example, the text query is `boat' and CLIP confuses it with `surfboard' in top-5 retrieved images.  
	} 
	\label{fig:supple_qualitative_clip2}
	\end{figure*}


\clearpage
Figure~\ref{fig:supple_qualitative_clip2-2} displays qualitative comparison of CLIP initial ranking and our ELIP-C re-ranking on ImageNet-R. 

\begin{figure*}[h]
	\centering
\includegraphics[height=0.75\linewidth]{images/supple_qualitative_clip2-2.pdf}
	\caption{
\textbf{Qualitative Comparison on ImageNet-R for CLIP and ELIP-C.} We display examples at the top-100 rankings where there is a difference between the two models, \emph{i.e.,} one model retrieves a positive sample while the other model retrieves a negative sample, as the majority of the top-100 retrieves samples are positive. \emph{Generally, ELIP-C retrieves more positive samples in top-100 images than CLIP}. Negative samples (errors) are highlighted in an orange solid box. 
For the first example, the text query is `hen' and CLIP retrieves some `duck's in top-100 retrieved images; for the second example, the text query is `baseball player' and CLIP retrieves some players for other ball games in the top-100 retrieved images while our ELIP-C retrieves a basketball player in top-100 images.  
	} 
	\label{fig:supple_qualitative_clip2-2}
	\end{figure*}

\clearpage
In conclusion, these results demonstrate that, though the initial CLIP model can roughly retrieve relevant images, it is less able to distinguish fine-grained differences between the top retrieved images, whereas our re-ranking model further improves the discrimination between the ground truth and hard negative images, achieving a better retrieval.








\clearpage
\subsection{Qualitative Comparison of SigLIP/SigLIP-2 and ELIP-S/ELIP-S-2}

Figure~\ref{fig:supple_qualitative_siglip1} shows more qualitative comparison between the SigLIP/SigLIP-2 initial ranking and ELIP-S/ELIP-S-2 re-ranking on COCO. 

\begin{figure*}[h]
	\centering
\includegraphics[height=0.75\linewidth]{images/supple_qualitative_siglip1.pdf}
	\caption{
\textbf{Qualitative Comparison on COCO for SigLIP and ELIP-S (top), SigLIP-2 and ELIP-S-2 (bottom).} For each example, we show both top-5 rankings (from left to right) and highlight the ground truth image in a black dashed box.
In the first (SigLIP v.s. ELIP-S) and second (SigLIP-2 v.s. ELIP-S-2) examples (from top to bottom), the text queries are `A small blue plane sitting on top of a field', and `Assortment of doughnuts and other snack items on a serving tray', and the ground truth images are not in top-5 images of the SigLIP/SigLIP-2 initial ranking, but ranked top-1 in our ELIP-S/ELIP-S-2 re-ranking. 
	} 
	\label{fig:supple_qualitative_siglip1}
	\end{figure*}


\clearpage
Figure~\ref{fig:supple_qualitative_siglip1-2} shows more qualitative comparison between the SigLIP/SigLIP-2 initial ranking and ELIP-S/ELIP-S-2 re-ranking on Flickr. 


\begin{figure*}[h]
	\centering
\includegraphics[height=0.75\linewidth]{images/supple_qualitative_siglip1-2.pdf}
	\caption{
\textbf{Qualitative Comparison on Flickr for SigLIP and ELIP-S (top), SigLIP-2 and ELIP-S-2 (bottom).} For each example, we show both top-5 rankings (from left to right) and highlight the ground truth image in a black dashed box.
In the first example, the text query is `A man wearing bathing trunks is parasailing in the water', and the ground truth image ranks top-3 in the SigLIP initial ranking but top-1 in our ELIP-S re-ranking. In the second example, the text query is `A group of mountain bikers race each other down a dirt hill', and the ground truth image ranks top-3 in the SigLIP-2 initial ranking, but ranked top-1 in our ELIP-S-2 re-ranking. 
	} 
	\label{fig:supple_qualitative_siglip1-2}
	\end{figure*}


\clearpage
Figure~\ref{fig:supple_qualitative_siglip2} displays qualitative comparison of SigLIP/SigLIP-2 initial ranking and our ELIP-S/ELIP-S-2 re-ranking on Occluded COCO. 


\begin{figure*}[h]
	\centering
\includegraphics[height=0.75\linewidth]{images/supple_qualitative_siglip2-1.pdf}
	\caption{
\textbf{Qualitative Comparison on Occluded COCO for SigLIP and ELIP-S (top), SigLIP-2 and ELIP-S-2 (bottom).} 
Similar to Figure~\ref{fig:supple_qualitative_clip2-2}, we display examples at the top-100 rankings where there is a difference between the two models, \emph{i.e.,} one model retrieves a positive sample while the other model retrieves a negative sample. \emph{Generally, ELIP-S/ELIP-S-2 retrieves more positive samples in top-100 images than SigLIP/SigLIP-2}. Negative samples (errors) are highlighted in an orange solid box. 
For the first example, the text query is `bowl' and SigLIP confuses it with `roller skating rink', `plate', and `frisbee' in the top-100 retrieved images, while ELIP-S confuses it with `cup'; for the second example, the text query is `TV' and SigLIP-2 confuses it with `cell phone', 'laptop' and 'remote control' in top-100 retrieved images.} 

	 
	\label{fig:supple_qualitative_siglip2}
	\end{figure*}


\clearpage
Figure~\ref{fig:supple_qualitative_siglip2-2} displays qualitative comparison of SigLIP/SigLIP-2 initial ranking and our ELIP-S/ELIP-S-2 re-ranking on ImageNet-R. 


\begin{figure*}[h]
	\centering
\includegraphics[height=0.75\linewidth]{images/supple_qualitative_siglip2-2.pdf}
	\caption{
\textbf{Qualitative Comparison on ImageNet-R for SigLIP and ELIP-S (top), SigLIP-2 and ELIP-S-2 (bottom).} We display examples at the top-100 rankings where there is a difference between the two models, \emph{i.e.,} one model retrieves a positive sample while the other model retrieves a negative sample, as the majority of the top-100 retrieves samples are positive. \emph{Generally, ELIP-S/ELIP-S-2 retrieves more positive samples in top-100 images than SigLIP/SigLIP-2}. Negative samples (errors) are highlighted in an orange solid box. 
For the first example, the text query is `duck' and SigLIP retrieves some `goose's in top-100 retrieved images while our ELIP-S retrieves a `goose' in top-100 images; for the second example, the text query is `centipede' and SigLIP-2 retrieves some other insects in top-100 retrieved images.  
	} 
	\label{fig:supple_qualitative_siglip2-2}
	\end{figure*}

\clearpage
In conclusion, these results demonstrate that, though the initial SigLIP/SigLIP-2 model can roughly retrieve relevant images, it is less able to distinguish fine-grained differences between the top retrieved images, whereas our re-ranking model further improves the discrimination between the ground truth and hard negative images, achieving a better retrieval.









\clearpage
\subsection{Qualitative Comparison of BLIP and ELIP-B}

Figure~\ref{fig:supple_qualitative_blip1} shows more qualitative comparison between the BLIP-2 ranking and ELIP-B re-ranking on COCO. 

\begin{figure*}[h]
	\centering
\includegraphics[height=0.75\linewidth]{images/supple_qualitative_blip1.pdf}
	\caption{
\textbf{Qualitative Comparison on COCO for BLIP-2 and ELIP-B.} For each example, we show both top-5 rankings (from left to right) and highlight the ground truth image in a black dashed box.
In the first and second examples, the text queries are `There is dust coming out of the catcher's glove as a boy prepares to bat' and `Park scene, park bench, light pole and building in background, probably city park area', and the ground truth images are ranked top-5 / top-4 respectively by BLIP-2, but ranked top-1 by ELIP-B. 
	} 
	\label{fig:supple_qualitative_blip1}
	\end{figure*}



\clearpage
Figure~\ref{fig:supple_qualitative_blip1-2} shows more qualitative comparison between the BLIP-2 ranking and ELIP-B re-ranking on Flickr. 

\begin{figure*}[h]
	\centering
\includegraphics[height=0.75\linewidth]{images/supple_qualitative_blip1-2.pdf}
	\caption{
\textbf{Qualitative Comparison on Flickr for BLIP-2 and ELIP-B.} For each example, we show both top-5 rankings (from left to right) and highlight the ground truth image in a black dashed box.
In the first and second examples, the text queries are `A brown dog waits for the frisbee to come down before catching it' and `A crowd is present at a bar', and the ground truth images rank top-2 in the BLIP-2 ranking but top-1 in our ELIP-B ranking.
	} 
	\label{fig:supple_qualitative_blip1-2}
	\end{figure*}



\clearpage
Figure~\ref{fig:supple_qualitative_blip2} displays qualitative comparison of BLIP-2 initial ranking and our ELIP-B re-ranking on Occluded COCO. 


\begin{figure*}[h]
	\centering
\includegraphics[height=0.75\linewidth]{images/supple_qualitative_blip2.pdf}
	\caption{
\textbf{Qualitative Comparison on Occluded COCO for BLIP-2 and ELIP-B.} Similar to Figure~\ref{fig:supple_qualitative_clip2-2}, we display examples at the top-100 rankings where there is a difference between the two models, \emph{i.e.,} one model retrieves a positive sample while the other model retrieves a negative sample. \emph{Generally, ELIP-B retrieves more positive samples in top-100 images than BLIP-2}. Negative samples (errors) are highlighted in an orange solid box. 
For the first example, the text query is `cell phone' and BLIP-2 confuses it with `remote control' and `camera' in the top-100 retrieved images, while ELIP-B confuses it with `keyboard'; for the second example, the text query is `knife' and BLIP-2 confuses it with `scissors', `fruit peeler' and `pizza cutter' in top-100 retrieved images, while ELIP-B confuses it with `scissors'.  
	} 
	\label{fig:supple_qualitative_blip2}
	\end{figure*}



\clearpage
Figure~\ref{fig:supple_qualitative_blip2-2} displays qualitative comparison of BLIP-2 initial ranking and our ELIP-B re-ranking on ImageNet-R. 



\begin{figure*}[h]
	\centering
\includegraphics[height=0.75\linewidth]{images/supple_qualitative_blip2-2.pdf}
	\caption{
\textbf{Qualitative Comparison on ImageNet-R for BLIP-2 and ELIP-B.} Similarly, we display examples at the top-100 rankings where there is a difference between the two models, \emph{i.e.,} one model retrieves a positive sample while the other model retrieves a negative sample. \emph{Generally, ELIP-B retrieves more positive samples in top-100 images than BLIP-2}. Negative samples (errors) are highlighted in an orange solid box. 
For the third example, the text query is `hammerhead' and BLIP-2 retrieves some `hammer's in the top-100 retrieved images; for the fourth example, the text query is `mitten' and BLIP-2 retrieves some other objects made of similar material of mitten in top-100 retrieved images while our ELIP-B makes a similar mistake but the total number of errors are lower. 
	} 
	\label{fig:supple_qualitative_blip2-2}
	\end{figure*}


\clearpage
In conclusion, these results demonstrate that the ELIP-B re-ranking model improves the discrimination capability between the positive and hard negative images, achieving a better retrieval performance than the original BLIP-2 model.

    





    

    
\clearpage
\subsection{Attention Map of CLIP and ELIP-C}


Figure~\ref{fig:supple_attention_clip} shows the cross-attention map of \texttt{[CLS]} token on patch tokens for both CLIP and our ELIP-C on COCO (Rows 1-2) and Flickr (Rows 3-4). We display the cases where the image matches the text query on the left and the images that do not match the text query on the right. The visualisation is based on the codebase of~\cite{selvaraju2017grad}, where a warmer color represents a higher activation.



\begin{figure*}[h]
	\centering
\includegraphics[height=0.55\linewidth]{images/supple_attention_map_clip.pdf}
	\caption{
\textbf{Visualisation of attention map of CLIP and ELIP-C} shows the cross-attention map of \texttt{[CLS]} token on patch tokens for both CLIP and our ELIP-C on COCO and Flickr. Left: image matches the text query; Right: image does not match the text query. COCO: Rows 1-2; Flickr: Rows 3-4.
It can be observed that when the image matches the text query, our generated visual prompt vectors can effectively boost the selection of image features relevant to the text query: 
For example, when the text query is `A person wearing a banana headdress and necklace'~(Row 1), comparing the attention map of CLIP and ELIP-C, we can observe ELIP-C enables more attention on the banana headdress and necklace; when the text query is `A cute cat laying down in a sink'~(Row 2), we note that ELIP-C gives significantly more attention on the cat; when the text query is `A black and white dog is running in a grassy garden surrounded by a white fence'~(Row 3), it can be noted that ELIP-C largely increases the attention on the black and white dog and the white fence; when the text query is `A group of young people are listening to a man in a blue shirt telling them about Brazil'~(Row 4), it can be noted that ELIP-C makes the attention to focus more on the man in a blue shirt who is talking, as well as the Brazil flag and people listening.
The difference is not significant if the image does not match the query (Columns 4-6). 
	} 
	\label{fig:supple_attention_clip}
	\end{figure*}



\clearpage
Figure~\ref{fig:supple_attention_clip-2} shows the cross-attention map of \texttt{[CLS]} token on patch tokens for both CLIP and our ELIP-C on Occluded COCO (Rows 1-2) and ImageNet-R (Rows 3-4). We display the cases where the image matches the text query on the left and the images that do not match the text query on the right. The visualisation is based on the codebase of~\cite{selvaraju2017grad}, where a warmer color represents a higher activation.




\begin{figure*}[h]
	\centering
\includegraphics[height=0.55\linewidth]{images/supple_attention_map_clip-2.pdf}
	\caption{
\textbf{Visualisation of attention map of CLIP and ELIP-C} shows the cross-attention map of \texttt{[CLS]} token on patch tokens for both CLIP and our ELIP-C on Occluded COCO and ImageNet-R. Left: image matches the text query; Right: image does not match the text query. Occluded COCO: Rows 1-2; ImageNet-R: Rows 3-4.
It can be observed that when the image matches the text query, our generated visual prompt vectors can effectively boost the selection of image features relevant to the text query: 
For example, when the text query is `banana'~(Row 1), it can be observed that ELIP-C enables more concentration of attention on the banana rather than other areas; when the text query is `baseball bat'~(Row 2), we observe that ELIP-C brings more attention on the baseball bat; when the text query is `baboon'~(Row 3) and `ant'~(Row 4), ELIP-C makes the attention to focus more on the baboon and ant respectively.
The difference is not significant if the image does not match the query (Columns 4-6). 	} 
	\label{fig:supple_attention_clip-2}
	\end{figure*}






\clearpage
\subsection{Attention Map of SigLIP/SigLIP-2 and ELIP-S/ELIP-S-2}


Figure~\ref{fig:supple_attention_siglip} shows the cross-attention map of \texttt{[CLS]} token on patch tokens for both SigLIP/SigLIP-2 and our ELIP-S/ELIP-S-2 on COCO (Rows 1 and 3) and Flickr (Rows 2 and 4). We display the cases where the image matches the text query on the left and the images that do not match the text query on the right. 
The visualisation is based on the codebase of~\cite{selvaraju2017grad}, where a warmer color represents a higher activation.



\begin{figure*}[h]
	\centering
\includegraphics[height=0.55\linewidth]{images/supple_attention_map_siglip.pdf}
	\caption{
\textbf{Visualisation of attention map of SigLIP/SigLIP-2 and ELIP-S/ELIP-S-2} shows the cross-attention map of \texttt{[CLS]} token on patch tokens for both SigLIP/SigLIP-2 and our ELIP-S/ELIP-S-2 on COCO and Flickr. Top: SigLIP v.s. ELIP-S; Bottom: SigLIP-2 v.s. ELIP-S-2. Left: image matches the text query; Right: image does not match the text query. COCO: Rows 1 and 3; Flickr: Rows 2 and 4. 
It can be observed that when the image matches the text query, our generated visual prompt vectors can effectively boost the selection of image features relevant to the text query: 
For example, when the text query is `A basket ball player is posing in front of a basket'~(Row 1), comparing the attention map of SigLIP and ELIP-S, we can observe ELIP-S enables more attention on the basketball, the basket, and the player; when the text query is `A black dog is slowly crossing a fallen log that is outstretched over a stream of water'~(Row 2), we note that ELIP-S gives significantly more attention on the black dog and the log; when the text query is `A dark skinned child getting ready to be pushed on a swing'~(Row 3), it can be noted that ELIP-S-2 largely increases the attention on the child and the swing; when the text query is `A boy is hanging out of the window of a yellow taxi'~(Row 4), it can be noted that ELIP-S-2 makes the attention to focus more on the boy out of the window.
The difference is not significant if the image does not match the query (Columns 4-6). 
	} 
	\label{fig:supple_attention_siglip}
	\end{figure*}



\clearpage
Figure~\ref{fig:supple_attention_siglip-2} shows the cross-attention map of \texttt{[CLS]} token on patch tokens for both SigLIP/SigLIP-2 and our ELIP-S/ELIP-S-2 on Occluded COCO (Rows 1 and 3) and ImageNet-R (Rows 2 and 4). We display the cases where the image matches the text query on the left and the images that do not match the text query on the right. 
The visualisation is based on the codebase of~\cite{selvaraju2017grad}, where a warmer color represents a higher activation.




\begin{figure*}[h]
	\centering
\includegraphics[height=0.55\linewidth]{images/supple_attention_map_siglip-2.pdf}
	\caption{
\textbf{Visualisation of attention map of SigLIP/SigLIP-2 and ELIP-S/ELIP-S-2} shows the cross-attention map of \texttt{[CLS]} token on patch tokens for both SigLIP/SigLIP-2 and our ELIP-S/ELIP-S-2 on Occluded COCO and ImageNet-R. Top: SigLIP v.s. ELIP-S; Bottom: SigLIP-2 v.s. ELIP-S-2. Left: image matches the text query; Right: image does not match the text query. Occluded COCO: Rows 1 and 3; ImageNet-R: Rows 2 and 4.
It can be observed that when the image matches the text query, our generated visual prompt vectors can effectively boost the selection of image features relevant to the text query: 
For example, when the text query is `bird'~(Row 1), it can be observed that ELIP-S enables more concentration of attention on the bird; when the text query is `beagle'~(Row 2), we observe that ELIP-S brings more attention on the beagle; when the text query is `book'~(Row 3) and `chow chow'~(Row 4), ELIP-S-2 makes the attention to focus more on the book and chow chow respectively.
The difference is not significant if the image does not match the query (Columns 4-6). 
	} 
	\label{fig:supple_attention_siglip-2}
	\end{figure*}
    


\clearpage
\subsection{Attention Map of BLIP-2 and ELIP-B}



Figure~\ref{fig:supple_attention_blip} shows the cross-attention map of the query tokens at the ITM head on patch tokens for both BLIP-2 and our ELIP-B on COCO (Rows 1-2) and Flickr (Rows 3-4). There are 32 query tokens in total, and we take the average value of the 32 query tokens to visualise the attention map. The attention map represents image features at which locations are selected and concentrated. We display the situations where the image matches the text query on the left, and where the images do not match the text query on the right. The visualisation is based on the codebase of~\cite{selvaraju2017grad}, where a warmer color represents a higher activation.



\begin{figure*}[h]
	\centering
\includegraphics[height=0.55\linewidth]{images/supple_attention_map_blip.pdf}
	\caption{
\textbf{Visualisation of attention map of BLIP-2 and ELIP-B} shows the cross-attention map of the query tokens at the ITM head on patch tokens for both BLIP-2 and our ELIP-B on COCO and Flickr. There are 32 query tokens in total, and we take the average value of the 32 query tokens to visualise the attention map. The attention map represents image features at which locations are selected and concentrated.
Left: image matches the text query; Right: image does not match the text query. COCO: Rows
1-2; Flickr: Rows 3-4. 
It can be observed that when the image matches the text query, our generated visual prompt vectors can effectively boost the selection of image features relevant to the text query: 
For example, when the text query is `A baseball player holding a bat while standing in a field'~(Row 1), comparing the attention map of BLIP-2 and ELIP-B we can observe ELIP-B enables more attention on the baseball bat; 
when the text query is `A brown and white dog wearing a neck tie'~(Row 2), we note that ELIP-B gives more attention on the tie and the dog; 
when the text query is `A bunch of beer pull tabs at a bar with Christmas lights on the ceiling'~(Row 3), it can be noted that ELIP-B largely increases the attention on the Christmas lights on the ceiling and the bunch of beer pull tabs; 
when the text query is `A father and daughter holding a young tree upright, ready to be planted, as the son stands to their side wielding a shovel'~(Row 4), it can be noted that ELIP-B makes the attention focus more on the father, daughter, son and tree.  
The difference is not significant if the image does not match the query (Columns 4-6). 
	} 
	\label{fig:supple_attention_blip}
	\end{figure*}





\clearpage
Figure~\ref{fig:supple_attention_blip-2} shows the cross-attention map of the query tokens at the ITM head on patch tokens for both BLIP-2 and our ELIP-B on Occluded COCO (Rows 1-2) and ImageNet-R (Rows 3-4). There are 32 query tokens in total, and we take the average value of the 32 query tokens to visualise the attention map. The attention map represents image features at which locations are selected and concentrated. We display the situations where the image matches the text query on the left and where the images do not match the text query on the right. The visualisation is based on the codebase of~\cite{selvaraju2017grad}, where a warmer color represents a higher activation.



    
    


\begin{figure*}[h]
	\centering
\includegraphics[height=0.55\linewidth]{images/supple_attention_map_blip-2.pdf}
	\caption{
\textbf{Visualisation of attention map of BLIP-2 and ELIP-B} shows the cross-attention map of the query tokens at the ITM head on patch tokens for both BLIP-2 and our ELIP-B on Occluded COCO and ImageNet-R. There are 32 query tokens in total, and we take the average value of the 32 query tokens to visualise the attention map. The attention map represents image features at which locations are selected and concentrated.
Left: image matches the text query; Right: image does not match the text query. Occluded COCO: Rows 1-2; ImageNet-R: Rows 3-4. It can be observed that when the image matches the text query, our generated visual prompt vectors can effectively boost the selection of image features relevant to the text query: 
For example, when the text query is `pizza'~(Row 1), it can be observed that ELIP-B enables more concentration of attention on the pizza; 
when the text query is `surfboard'~(Row 2), we observe that ELIP-B brings more attention on the surfboard; 
when the text query is `bee'~(Row 3) and `basketball'~(Row 4), ELIP-B makes the attention to focus more on the bee and basketball respectively. 
The difference is not significant if the image does not match the query (Columns 4-6). 
	} 
	\label{fig:supple_attention_blip-2}
	\end{figure*}








\clearpage
\section{Ablation Study for Different Number of Generated Visual Prompt Tokens}
\label{sec:sup_implement_detail}


\begin{table*}[h]
    \centering
    \tabcolsep=0.25cm
    \begin{tabular}{ccccccccc}
    \toprule
 \multirow{2}*{Settings} & \multirow{2}*{\#Generated Tokens} & \multicolumn{3}{c}{COCO} &  \multicolumn{3}{c}{Flickr}& \\
\cmidrule(lr){3-5} \cmidrule(lr){6-8}
& &  Recall@1 & Recall@5 & Recall@10  & Recall@1 & Recall@5 & Recall@10 \\
\midrule 


$\mathbb{A}$ & 0 & 40.2 & 66.0 & 75.6 & 67.6 & 88.3 & 93.0 \\ 
$\mathbb{B}$ & 1 & 44.2 & 70.0 & 79.5 & 71.3 & 90.6 & 94.4  \\ 
$\mathbb{C}$ & 2 & 44.7 & 70.2 & 79.8 & 71.4 & 90.5 & 94.1  \\ 
$\mathbb{D}$ & 5 & 45.2 & 70.5 & 80.0 & 72.4 & 90.5 & 94.6  \\ 
$\mathbb{E}$ & 10 & 45.6 & 71.1 & 80.4 & 72.3 & 90.6 & 94.7  \\ 


    \bottomrule
    \end{tabular}
    \caption{\textbf{Ablation study on number of generated visual prompt tokens.} Following Table~1 of the main paper, we use CLIP as the baseline for the ablation study. 0 refers to the CLIP baseline and a non-zero number refers to our ELIP-C architecture with different numbers of generated visual prompt vectors.}
    \label{tab:ablation_num_token}
\end{table*}

As mentioned in Section~7.1, here we study the performance as the number of generated visual prompt tokens are varied. The results are shown in Table~\ref{tab:ablation_num_token}, and it can be observed that generating multiple tokens, \emph{e.g.,} 2 tokens (Setting $\mathbb{C}$), 5 tokens (Setting $\mathbb{D}$) or 10 tokens (Setting $\mathbb{E}$), brings an improvement compared with generating only 1 token (Setting $\mathbb{B}$), and Setting $\mathbb{E}$ performs best. Therefore, we generate 10 tokens in all experiments. 



\clearpage
\section{Further Ablation Study for ELIP-B}
\label{sec:sup_ablation_elip_b}



\begin{table*}[h]
    \centering
    \tabcolsep=0.1cm
    \begin{tabular}{cccccccccc}
    \toprule
 \multirow{2}*{Settings} &\multirow{2}*{Fine-tune ITM Head} & \multirow{2}*{JEST Selection} & \multicolumn{3}{c}{COCO} &  \multicolumn{3}{c}{Flickr}& \\
\cmidrule(lr){4-6} \cmidrule(lr){7-9}
& & & Recall@1 & Recall@5 & Recall@10  & Recall@1 & Recall@5 & Recall@10 \\
\midrule 


$\mathbb{A}$ & - & - & 68.25 & 87.72 & 92.63 & 89.74 & 98.18 & 98.94 \\ 
$\mathbb{B}$ & \checkmark &  & 68.26 & 87.86 & 92.78 & 89.80 & 98.26 & 99.14 \\ 
$\mathbb{C}$ &  & \checkmark & 67.35 & 87.38 & 92.53 & 89.08 & 98.06 & 99.08 \\ 
$\mathbb{D}$ & \checkmark & \checkmark & \textbf{68.41} & \textbf{87.88} & \textbf{92.78} & \textbf{90.08} & \textbf{98.34} & \textbf{99.22} \\ 


    \bottomrule
    \end{tabular}
    \caption{\textbf{Ablation study of ELIP-B} in terms of \emph{training data selection inspired by JEST~\cite{evans2024data}} and \emph{fine-tuning ITM head}. Setting $\mathbb{A}$ is the baseline BLIP-2. Setting $\mathbb{B}$ is trained on a random subset of the same size of Settings $\mathbb{C}$ and $\mathbb{D}$. It can be observed that both the strategies of fine-tuning the ITM head and training on selected subset by JEST contribute to the improvement of our ELIP-B.}
    \label{tab:supple_blip_ablation}
\end{table*}



In this section, we have further conducted an ablation study on ELIP-B to show the effectiveness of selecting the training data using the strategy inspired by JEST~\cite{evans2024data} as described in Section~5.2, as well as also fine-tuning the ITM head as in Figure~3 of the main paper, as these are two unique things we only applied to BLIP-2.


In Table~\ref{tab:supple_blip_ablation}, we can observe that: 1) Comparing Settings $\mathbb{B}$ and $\mathbb{D}$, we can note the effectiveness of training on JEST selected training batches compared with selecting batches randomly; 2) Comparing Settings $\mathbb{C}$ and $\mathbb{D}$, we can infer that it is important to also fine-tune the lightweight ITM head as the input features to the ITM head have been changed in ELIP-B.





\clearpage
\section{Efficiency of ELIP Pre-Training}
\label{sec:sup_efficiency}



\begin{table}[h]
    \centering
    \tabcolsep=0.7cm
    \begin{tabular}{cccccc}
    \toprule 
    Model & GPU Hours & \#GPU & BS & FLOPS & $\Delta$FLOPS \\
    \midrule

CLIP & 10736 A100 & 176 $\times$ A100 & 33792 &33.7G & -  \\
ELIP-C  & 144 A40 & 2 $\times$  A40 & 40 &35.8G & 2.1G \\
\midrule
SigLIP & 1152 TPUv4 & 16 $\times$ TPUv4 & 32768 & 604.5G & -    \\
ELIP-S & 144 A40 & 2 $\times$  A40 & 10 &613.3G & 8.8G   \\
\midrule
SigLIP-2 & * & 2048 $\times$ TPUv5e & 32768 &1.31T & -    \\
ELIP-S-2 & 144 A40 & 2 $\times$  A40 & 10 &1.34T &  0.03T   \\
\midrule
BLIP-2 & 2304 A100 & 16 $\times$  A100 & 1680 & 1.33T & -   \\
ELIP-B & 48 A40 & 2 $\times$ A40 & 12 &1.35T & 0.02T \\

    \bottomrule
    \end{tabular}
    \caption{\textbf{Efficiency of ELIP pre-training} compared with original pre-training. Our method significantly improves the efficiency on training time, GPU requirement, and batch size, introducing only a marginal increase on FLOPS for our trainable MLP mapping network. *SigLIP-2 original training time is not provided in their paper.}
    \label{tab:supple_efficiency}
\end{table}



As mentioned in Section~7, in the table below we show a comparison of our method to the original pre-training of CLIP, SigLIP, SigLIP-2, and BLIP-2 in terms of training time, GPU requirement, batch size, and FLOPS. It can be observed that our method significantly improves the efficiency on training time, GPU requirement, and batch size, while only introducing a marginal increase on FLOPS which results from our trainable MLP mapping network.