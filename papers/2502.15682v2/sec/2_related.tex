\section{Related Work}
\label{sec:related_work}

\noindent \textbf{Text-to-Image Retrieval}  
is a fundamental and much researched task in cross-modal learning~\cite{lee2018stacked,chen2020imram,ji2021step,faghri2017vse++,zhang2022negative,radford2021learning,li2021align,chen2020uniter,wang2023image,yu2022coca,chen2024internvl,chen2020interclass,chen2021learning,chen2023vilem,chun2021probabilistic,diao2021similarity,engilberge2018deep,gu2018look,huang2017instance,ji2019saliency,karpathy2014deep,kim2023improving,li2019visual,liu2019focus,liu2020graph,song2019polysemous,thomas2020preserving,wang2020consensus,wang2018learning,wang2019camp,wang2023multilateral,wei2020universal,wei2020multi,yan2021discrete,zeng2022learning,zhang2022show,zhang2020context,zhang2018deep,zheng2019towards}.
Large vision language models such as CLIP~\cite{radford2021learning,openclip}, ALIGN~\cite{jia2021scaling}, BLIP-2~\cite{li2023blip}, SigLIP~\cite{zhai2023siglip} and SigLIP-2~\cite{tschannen2025siglip2} that have powerful zero-shot capabilities have now become the de facto method for open-set text-based image retrieval.
The most recent work~\cite{sogi2024object} gives a slight improvement over BLIP-2 by incorporating the output of an object detector or annotations of detection bounding boxes. This succeeds in overcoming the failure cases where small but semantically important objects in an image are not properly understood by the model. We compare to this model and show superior performance. 


\noindent \textbf{CIR and Universal Retrieval.}  
In composed image
retrieval~(CIR)~\cite{liu2023CIR,gu2023compodiff,baldrati2023zero,ventura2024covr}, 
the query is specified by a composition of an image and text, with the
text specifying how the image should be changed. For example, the query image may be of a dog lying down, and the query text may be `playing with a ball'. This composed query defines
the target image to be retrieved from the gallery. This differs from our task, where the query is specified
only by text, and the text alone defines the target image to be retrieved from the gallery.  A more general setting is `universal retrieval'~\cite{wei2023uniir,liu2025LamRA} 
where the query can be a combination of image,
text, and instruction;  and the target can be image alone, text alone, or image and text.


\vspace{2pt}
\noindent \textbf{Post-Retrieval Re-ranking.}
For single modality image retrieval, where the query is an image, there has been a series of works that have re-ranked the top-$k$ images from an initial ranking via classical computer vision algorithms, such as `query expansion', `geometric verification', or a combination of the two~\cite{jegou2008hamming,philbin2007object,chum2011total,chum2007total,tolias2014visual,Arandjelovic12}, as well as via learning-based algorithms~\cite{cao2020unifying,hausler2021patch,el2021training,tan2021instance,Bhalgat23}. 
Re-ranking algorithms have been relatively less explored in text-to-image retrieval~\cite{yanagi2019text,qu2023learnable,long2024cfir}. \cite{miech2021thinking} introduced a method for computing the similarity score between an image and a text query by estimating the log-likelihood of the text conditioned on the image. While this approach has demonstrated strong performance, it remains computationally expensive both during training and inference, making it a {\em slow} process.
Our paper also focuses on the re-ranking stage -- developing a more powerful version of visual-language foundation models to give a better ranking of images that are  hard to distinguish by the original retrieval model.




\vspace{2pt}
\noindent \textbf{Multi-Modal Datasets.} 
To obtain multi-modal foundation models with a strong capability of generalisation, it is important to train them on large-scale multi-modal datasets.
Therefore, in recent years, there has been a significant increase in the number and scale of multimodal vision-language datasets that provide image-text pairs, such as
COCO~\cite{lin2014microsoft}, SBU~\cite{ordonez2011im2text}, Conceptual Captions~\cite{sharma2018conceptual}, LAION~\cite{schuhmann2022laion}, DataComp~\cite{gadre2024datacomp}. The increase in the size of multi-modal datasets enables the training of more powerful visual-language foundation models. More recently, DataCompDR~\cite{vasu2024mobileclip} utilises prior knowledge from large-scale pre-trained image captioning models to generate synthetic captions for DataComp images which results in less noisy captions than the datasets collected from the web, such as the original DataComp dataset. In our paper, we have experimented with training our model using Conceptual Captions~\cite{sharma2018conceptual} and DataCompDR~\cite{vasu2024mobileclip}.


\vspace{2pt}
\noindent \textbf{Multi-Modal Data Curation.} 
It is essential to conduct data curation for multi-modal datasets as it enables more efficient and effective training, especially in situations where the resources are limited.
There have been continuous efforts in data curation, such as offline example-level data pruning~\cite{gadre2024datacomp,kakaobrain2022coyo-700m,changpinyo2021conceptual,jia2021scaling,fang2023data,xu2023demystifying,hessel2021clipscore,mahmoud2024sieve}, offline cluster-level data pruning~\cite{abbas2023semdedup,abbas2024effective,sorscher2022beyond,campbell2018bayesian,har2004coresets}, and online data curation with model-based scoring~\cite{evans2023bad,lin2024rho,loshchilov2015online,mindermann2022prioritized}. 
The most recent work, JEST~\cite{evans2024data}, utilises a pair of learner model and reference model to select batches of data that are learnable by the model but have not yet been learned. This inspired us to select the most efficient batches to train our architecture on BLIP-2. Another series of works related to us is hard negative mining, which has been both explored in classical metric learning~\cite{bucher2016hard,harwood2017smart,mishchuk2017working,simo2015discriminative,wu2017sampling,xuan2020hard} and modern contrastive learning~\cite{robinson2020contrastive,tian2021divide}.




