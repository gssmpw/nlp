\section{Conclusion}
\label{sec:conclusion}

In this paper, we introduced {\em Enhance Language-Image Pre-training (ELIP)}, a method to improve visual-language foundation models for text-based image retrieval. ELIP is a simple plug-and-play modification to pre-trained visual-language foundation models that significantly improves their zero-shot performance. Furthermore, the mapping network can be fine-tuned to
efficiently adapt these models to OOD datasets, leading to still further improvements. We have also demonstrated, by visualizing the attention maps, that ELIP enables the image encoder to attend to more relevant details.

Future work could apply ideas similar to ELIP to enhance generative Multimodal Large Language Models by introducing more effective text-guided visual attention and encoding for both decoder-only architectures~\cite{liu2023visual} and cross-attention-based architectures~\cite{alayrac2022flamingo}. 