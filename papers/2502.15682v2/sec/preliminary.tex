\section{Preliminaries}
\label{sec:preliminary}


\noindent \textbf{Re-Ranking in Image Retrieval.} 
Given an input query, the goal of a retrieval system is to rank all instances in a dataset $\Omega = \{I_1, \dots, I_n \}$, based on their relevance to the query. 
In the case of text-to-image retrieval, the query is specified by text~($T$), and the ideal outcome is a set~($\hat{\Omega}$), with the relevant images being ranked higher than those that are not. In general, an effective retrieval system proceeds in two stages: the first stage provides an initial ranking in a fast and efficient manner, while the second stageâ€”referred to as re-ranking--refines this ranking by recomputing the relevance scores between the text query and each of the top-$k$ ranked candidates with a more powerful (and usually more expensive) ranking model. 
The $k$ is selected such that in general there is a high recall for all the relevant images. In this paper, our novelty lies on the second stage, 
that aims to re-rank the top-$k$ candidates from the first stage results. 

\vspace{3pt}
\noindent \textbf{Visual Prompt Tuning~(VPT)}~\cite{jia2022visual} is a method of enhancing the ViT image encoder by inserting additional learnable prompts into the transformer layers. It enables efficient adaptation of ViT, requiring  only the few parameters of the learnable prompts to be trained. VPT has two different variants -- \emph{VPT-Shallow} and \emph{VPT-Deep}. \emph{VPT-Shallow} only inserts the additional visual prompts into the first Transformer layer, whereas for \emph{VPT-Deep}, prompts are introduced at every transformer layer's input space. We insert our generated set of visual prompt vector into the first transformer layer of ViT, which is similar to \emph{VPT-Shallow}.

