\section{The ELIP Architecture}
\label{sec:mapping_network}

In this section, we describe the ELIP text-to-visual prompt mapping network, 
that can be efficiently applied to adapt the commonly used CLIP/SigLIP architectures as well as the more sophisticated BLIP-2 architectures for re-ranking. We first introduce the architecture of the network in Section~\ref{sec:network}, and the training/inference strategy in Sections~\ref{sec:arch-clip} and~\ref{sec:arch-blip} respectively. 
We refer to the network applied to CLIP as \emph{ELIP-C}, applied to SigLIP/SigLIP-2 as \emph{ELIP-S/ELIP-S-2}, and applied to BLIP-2 as \emph{ELIP-B}.

\begin{figure}[t]
	\centering
\includegraphics[height=0.45\linewidth]{images/arch-1.pdf}
\vspace{-2mm}
	\caption{
\textbf{Architecture of ELIP-C / ELIP-S.}  
At training time, a batch of text-image pairs is fed into the architecture.
The text feature is mapped to the visual embedding space as a set of prompt vectors via the MLP mapping network and then guides the encoding of the image feature. 
We use color coding for the {\color{orange} \texttt{[CLS]} token}, 
{\color{cyan} patch tokens}, and {\color{blue} generated visual tokens} from text. The architecture is trained with InfoNCE loss (for ELIP-C) and Sigmoid loss (for ELIP-S/ELIP-S-2), to align the text feature with the corresponding re-computed image feature.} 
\vspace{-3mm}
\label{fig:arch-1}
\end{figure}

\subsection{Text-Guided MLP Mapping Network}
\label{sec:network}

Here, we propose a mapping network that projects the embedding of the text query into a set of prompt vectors within the visual embedding space. This set of prompt vectors is then incorporated as additional tokens into the first layer of the Vision Transformer (ViT) image encoder, used to re-compute the visual embeddings:
\begin{align*}
&[t_p^1, \ \dots, \ t_p^m, \ t_{\texttt{CLS}}] \ = \ \Phi_{\text{t}}(T)\\
&v \ = \ \Phi_{\text{v}}([x_p^1, \ \dots, \ x_p^n, \ x_{\texttt{CLS}}; \ \psi_{\text{map}}(t_{\texttt{CLS}})])
\end{align*}
where $T$ denotes the query text, which is first encoded with a pre-trained, frozen text encoder~($\Phi_{\text{t}}(\cdot)$) into $m+1$ embeddings. 
The \texttt{[CLS]} token is further fed into a \textbf{trainable} mapping network to generate the prompt vectors, which are concatenated with the $n+1$ image embeddings~($[x_p^1, \dots, x_p^n, x_{\texttt{CLS}}]$), and passed into the pre-trained, frozen visual encoder~($\Phi_{\text{v}}(\cdot)$). 
The MLP Mapping Network consists of 3 layers of linear layers with a GELU between every two linear layers. 
The ELIP architecture is shown in Figure~\ref{fig:arch-1} and Figure~\ref{fig:arch-2}.



\subsection{Training and Testing ELIP-C/ELIP-S}
\label{sec:arch-clip}

\noindent \textbf{Text-Guided Contrastive Training.}
At training time, we compute the dot product between the \texttt{[CLS]} token embedding of the text query~($t_{\texttt{CLS}}$) and the re-computed image features guided by the query text, {\em i.e.}, $\{v_1, \dots, v_b\}$~($b$ denotes the batch size). For ELIP-C, we train with the standard InfoNCE loss on the batches; 
For ELIP-S/ELIP-S-2, we train with pairwise Sigmoid loss. 
In Section~\ref{sec:hard_mining}, we provide more details on the batch selection scheme via global hard sample mining.

\vspace{2pt} \noindent \textbf{Re-Ranking at Inference Time.} 
At inference time, for each text query, 
we first compute the similarity scores between the visual-language embedding, computed by the original CLIP/SigLIP model, to obtain an initial ranking of all images. We then select the top-$k$ candidates for further re-ranking, where, the visual features are re-computed by incorporating the prompted vectors from the mapping network. The final ranking is obtained via the dot product of the re-computed image features and the text feature.

\begin{figure}[t]
\centering
\includegraphics[height=0.45\linewidth]{images/arch-2.pdf}
\vspace{-2mm}
\caption{\textbf{Architecture of ELIP-B.} Similar to the architecture on CLIP/SigLIP, the \emph{MLP Mapping Network} maps the text feature to the visual embedding space. The only difference is the text-guided image features are further fed into the Q-Former to cross-attend the input text and then passed through the Image-Text Matching (ITM) Head to predict whether the image and text match or not. As the input image features to the ITM head have been changed, we also fine-tune the ITM head, which is a lightweight MLP network. The network is fed pairs of text and positive/negative image features at training time and is trained with binary cross entropy loss.}
\vspace{-3mm}
\label{fig:arch-2}
\end{figure}

\subsection{Training and Testing ELIP-B}
\label{sec:arch-blip}

Figure~\ref{fig:arch-2} illustrates the application of our architecture on BLIP-2. The only difference with that described for CLIP-type models is that BLIP-2 re-ranking does not use a dual encoder, rather, the image and text encoders attend to each other. However, the purpose of our mapping network, and its training, are essentially unchanged.


\vspace{2pt}
\noindent \textbf{Text-Guided Image-Text Matching Loss.}
At training time, we feed the text query ($T$) and the re-computed image features with the query text as prompts, \emph{i.e.,$\{v_+, v_-\}$}~($v_+$ denotes the positive image and $v_-$ denotes the negative image), into the Q-Former, and then to an Image-Text Matching (ITM) Head to predict a score indicating whether the text and image match or not. The output of the ITM head is trained with binary cross entropy loss. 


\vspace{2pt}
\noindent \textbf{Inference Time Re-Ranking.}
For each text query, we first compute the similarity scores between the visual-language embedding, computed by the original BLIP-2 image and text encoders, to obtain an initial ranking of all images. We then select the top-$k$ candidates for further re-ranking, where, the visual features are re-computed by incorporating the prompted vectors from the mapping network. The final ranking is obtained via the sum of the initially computed similarity score and the score predicted by the ITM head based on the re-computed image features and text query.









