\section{Introduction}
\label{sec:intro}
This paper considers the problem of text-to-image retrieval, 
that aims to rank image instances based on their relevance to a text query. 
Effective retrieval generally includes two stages: the first stage provides an initial ranking in a fast and efficient manner, while the second stageâ€”referred to as re-ranking--refines this ranking by re-computing the relevance scores between the text query and each of the top-ranked candidates with a more expensive model.

Recent advances in text-to-image retrieval have primarily focused on the first stage. Notable models, such as CLIP~\cite{radford2021learning} and ALIGN~\cite{jia2021scaling}, leverage contrastive learning~\cite{oord2018representation} on large-scale image-text pairs to learn joint representations, demonstrating impressive generalization capabilities for cross-modal retrieval tasks.


Our primary contribution here focuses on the second stage of the retrieval pipeline, namely, the re-ranking. Specifically, our goal is to enhance the performance of off-the-shelf vision-language foundation models, so that they can be re-purposed for re-ranking the top-$k$ candidates from the fast retrieval process. 
The approach we develop, termed as {\em Enhanced Language-Image Pre-training} ({\bf ELIP}), requires only a few trainable parameters, and the training can be conducted efficiently with `student friendly' resources and data. 
We demonstrate that ELIP can boost the performance of the pre-trained CLIP~\cite{radford2021learning}, SigLIP~\cite{zhai2023siglip}, SigLIP-2~\cite{tschannen2025siglip2} and BLIP-2~\cite{li2023blip} for cross-modal retrieval.

To achieve this goal, we first introduce a lightweight, text-guided visual prompting module. As illustrated in Figure~\ref{fig:teaser}, a query text is mapped to a set of visual prompt vectors~\cite{jia2022visual}, that are then concatenated with the \texttt{[CLS]} and patch embeddings of the image encoder. These augmented embeddings are then passed into the frozen vision encoder to re-compute the image representation. The resulting image embedding is aware of the text conditioning and this enhances its performance in re-ranking.

As a second contribution, we address the two major challenges in training large vision-language models: first, data size -- to enable a strong generalisation capability it is necessary to train on millions or billions of images, but this is expensive; second, the batch size -- to enhance the model's discriminative capability it is important to train at a large batch size, but that requires a large number of GPUs. 
We develop here a best practice by introducing strategies to select and curate a training dataset with maximum information, and group hard samples together in a batch to make training with small batch size effective.

To assess the re-ranking performance of our proposed ELIP models, 
we experiment on the standard COCO~\cite{lin2014microsoft} and Flickr30k~\cite{plummer2015flickr30k} text-to-image retrieval benchmarks. 
As a further challenge, we also evaluate the generalisation of the ELIP-boosted models on out-of-distribution domains. To do so, we repurpose the Occluded COCO~\cite{lee2022instance} and ImageNet-R~\cite{hendrycks2021many} datasets to be used for text-to-image retrieval benchmarks. 


In summary, we have made the following four contributions:
\emph{First}, we propose a novel architecture to improve text-based image retrieval on large pre-trained vision-language models, including the most popular CLIP/SigLIP architectures 
and the state-of-the-art BLIP-2 architecture.
\emph{Second}, we propose a best practice for training our architecture efficiently with limited resources.
\emph{Third}, to evaluate the generalisation capability of text-to-image retrieval models to different out-of-distribution domains, we set up two new benchmarks of text-to-image retrieval, \emph{Occluded COCO} and \emph{ImageNet-R}. 
\emph{Fourth}, and most significantly, we demonstrate that ELIP  {\em substantially} improves the image retrieval performance of CLIP and SigLIP architectures, and outperforms the state-of-the-art BLIP-2 architecture. Furthermore, it provides an efficient method
to adapt these architectures to OOD datasets, again giving a tremendous boost with CLIP, SigLIP, SigLIP-2, and BLIP-2.  
