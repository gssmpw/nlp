\section{Experiment}
\label{sec:experiment}

\noindent \textbf{Implementation Details.}
Due to computational resource constraints, 
we train the ELIP-C model with a batch size of 40, 
the ELIP-S model with a batch size of 10, 
and the ELIP-B model with a batch size of 12. 
The initial learning rate is set to $1 \times 10^{-3}$ for ELIP-C, ELIP-S, and ELIP-S-2, and $1 \times 10^{-5}$ for ELIP-B. 
All models are trained on the DataCompDR dataset by default, 
with additional experiments conducted on the smaller CC3M dataset for ablation studies. Training is performed on two A6000 or A40 GPUs. 
For re-ranking, we select the top-$k$ samples based on the dataset and model: for ELIP-C, $k$ is set to 100 for COCO and Flickr, 500 for Occluded COCO, and 1000 for ImageNet-R; for ELIP-S and ELIP-S-2, $k$ is set to 100 for COCO and Flickr, 500 for Occluded COCO, and 200 for ImageNet-R; for ELIP-B, $k$ is set to 20 for COCO and Flickr, 100 for Occluded COCO, and 200 for ImageNet-R. 
The value of $k$ is chosen to ensure high recall in the original ranking while maintaining fast inference. 
Compared to the original pre-training approach of CLIP, SigLIP and BLIP-2, our method significantly improves training efficiency in terms of reduced training time, GPU requirements, and batch size, with only a marginal increase in FLOPS introduced by the trainable MLP mapping network. Further details are provided in the appendix.


\subsection{Results on COCO and Flickr Benchmarks}
\label{sec:results_coco_flickr}

\noindent \textbf{Ablation Study.}
In Table~\ref{tab:ablation}, we evaluate the contributions of different components of the ELIP framework for CLIP. 
A comparison between Settings $\mathbb{A}$ and $\mathbb{B}$ highlights the effectiveness of the ELIP-C boost over the original CLIP. 
The comparison between Settings $\mathbb{B}$ and $\mathbb{C}$ demonstrates the importance of hard sample mining when training with a small batch size. Settings $\mathbb{C}$ and $\mathbb{D}$ show the benefit of training on larger datasets with less noisy captions. Finally, the comparison between Settings $\mathbb{D}$ and $\mathbb{E}$ reveals that generating multiple visual prompts ({\em e.g.}, 10 prompts in this study) is more beneficial than generating a single prompt. Further ablation studies on the number of generated prompts are detailed in the appendix.


\vspace{2pt} \noindent \textbf{Comparison with State-of-the-Art.}
As shown in Table~\ref{tab:compare_sota}, we compare our models~(ELIP-C, ELIP-S, ELIP-S-2, and ELIP-B) with prior state-of-the-art methods. When trained on DataCompDR12M, our method demonstrates zero-shot performance improvements for CLIP, SigLIP, SigLIP-2, and BLIP-2 on the COCO and Flickr benchmarks. Notably, ELIP-B outperforms the most recent work~\cite{sogi2024object}, establishing a new state-of-the-art for text-to-image retrieval on the BLIP-2 backbone. Furthermore, our ELIP-S, when applied to SigLIP and SigLIP-2, achieves performance comparable to BLIP-2. 



\vspace{2pt}
\noindent \textbf{Recall Top-$k$ Curves.}
Figure~\ref{fig:curve_recall} (right) presents the Recall@Top-$k$ curves for the original CLIP model and our ELIP-C on the COCO benchmark. The curves are generated by plotting the Recall values across various Top-$k$ thresholds. Notably, there is a significant performance gap between the two models, demonstrating that ELIP-C re-ranking consistently improves text-to-image retrieval performance across different $k$ values.


\begin{figure}[t]
\centering
\includegraphics[height=0.4\linewidth]{images/curve_recall.pdf}
\vspace{-7mm}
\caption{
\textbf{Before/after comparisons.} Left: Precision-Recall curves for  Occluded COCO retrieval, comparing SigLIP-2 initial rankings to the re-rankings given by ELIP-S-2. Right:  Recall Top-$k$ curves for COCO retrieval, comparing  CLIP initial rankings to the re-rankings given by ELIP-C. 
} 
\label{fig:curve_recall}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[height=1.35\linewidth]{images/qualitative.pdf}
\vspace{-4mm}
\caption{
\textbf{Qualitative comparison between CLIP initial ranking and ELIP-C re-ranking.} COCO: Columns 1–2; Flickr: Columns 3–4. 
The ground-truth image for each query is highlighted with a dashed box, with the top-5 retrieved images shown. For the COCO query ``A large wooden pole with a green street sign hanging from it'', CLIP ranks a non-wooden pole as top-1, while ELIP-C correctly re-ranks the large wooden pole to top-1. For the Flickr query ``A man wearing bathing trunks is parasailing in the water'', CLIP ranks a wakeboarding person as top-1, whereas ELIP-C accurately re-ranks the parasailing man wearing bathing trunks to top-1.} 
\label{fig:qualitative}
\vspace{-4mm}
\end{figure}

\vspace{2pt}
\noindent \textbf{Qualitative Results.}
Figure~\ref{fig:qualitative} provides a qualitative comparison between the initial rankings produced by the CLIP model and the re-ranked results obtained with ELIP-C on the COCO~(left) and Flickr~(right) benchmarks. In both cases, ELIP-C significantly improves the rankings by elevating the ground-truth image (highlighted with a dashed box) to rank 1. Additional qualitative results are provided in the appendix.


\begin{figure*}[h]
\centering
\includegraphics[height=0.3\linewidth]{images/attention_map.pdf}
\vspace{-2pt}
\caption{
\textbf{Visualisation of attention maps} comparing the cross-attention maps of the \texttt{[CLS]} token on patch tokens for CLIP and ELIP-C. Left: image matches the text query; Right: image does not match the text query. For matched queries, ELIP-C enhances attention on image features relevant to the text. 
For example: (Row 1) ELIP-C focuses more on the giant tennis racket and the young woman for the query ``A young woman holding a giant tennis racket''; 
(Row 2) ELIP-C highlights the cake, baby, and shirt for the query ``A baby in plaid shirt eating a frosted cake''.
Differences are minimal when the image does not match the query (Columns 4–6). 
} 
\label{fig:attention_map}
\end{figure*}



\vspace{2pt} \noindent \textbf{Visualisation of Attention Map.} 
Figure~\ref{fig:attention_map} visualizes the cross-attention maps of the \texttt{[CLS]} token on patch tokens for both CLIP and ELIP-C on COCO. When the image matches the text query (left of Figure~\ref{fig:attention_map}), our generated visual prompt vectors effectively enhance the selection of image features relevant to the query. This improvement can be attributed to ELIP-C's early fusion approach, which integrates text features at the beginning of the image encoder, enabling the model to produce image embeddings more closely aligned with the query text. The visualizations provide strong evidence supporting this hypothesis.

\begin{table}[h]
    \centering
    \tabcolsep=0.15cm
    \begin{tabular}{c|ccc}
    \toprule 
    Model & \thead{Occluded \\ COCO} & ImageNet-R & Average  \\ 
    \midrule
    CLIP & 47.47 & 76.01 & 61.74 \\ 
    ELIP-C~(zero-shot) & 48.89 & 76.81 & 62.85 \\ 
    ELIP-C~(fine-tuned) & \textbf{59.88} & \textbf{81.44} & \textbf{70.66} \\ 
    
    

    \midrule
    SigLIP & 61.74 & 92.11 & 76.93 \\ 
    ELIP-S~(zero-shot) & 64.58 & 92.42 & 78.50 \\ 
    ELIP-S~(fine-tuned) & \textbf{71.99} & \textbf{92.86} & \textbf{82.43} \\ 
    
    \midrule
    SigLIP-2 & 66.40 & 92.66 & 79.53 \\ 
    ELIP-S-2~(zero-shot) & 67.42 & 92.74 & 80.08 \\ 
    ELIP-S-2~(fine-tuned) & \textbf{76.10} & \textbf{94.00} & \textbf{85.05} \\ 

    \midrule
    BLIP-2 & 62.73 & 82.31 & 72.52 \\ 
    ELIP-B~(zero-shot) &  63.40 & 82.99 & 73.20 \\ 
    ELIP-B~(fine-tuned) & \textbf{70.49} & \textbf{83.68} & \textbf{77.09} \\ 
    
    \bottomrule
    \end{tabular}
    \caption{\textbf{mAP Results on OOD datasets.} 
    ELIP prompting achieves notable zero-shot improvements for CLIP, SigLIP series, and BLIP-2. These gains are further amplified through fine-tuning on relevant datasets. 
    For example, to adapt to the Occluded COCO, the ELIP model is fine-tuned on COCO. Similarly, fine-tuning on ImageNet adapts ELIP to ImageNet-R. These results demonstrate ELIP's capability for efficiently adapting the models to new datasets. 
    }
    \label{tab:ood_result}
\end{table}

\subsection{Results on OOD Benchmarks}
\label{sec:ood_result}

The results on the out-of-distribution (OOD) benchmarks are presented in Table~\ref{tab:ood_result}. ELIP achieves notable {\em zero-shot} improvements across all models on the OOD benchmarks, Occluded COCO and ImageNet-R, highlighting the strong generalization capabilities of the ELIP models. 
The performance can be improved further by fine-tuning the mapping network on suitable datasets (the image and text encoders are frozen).
Since it is not feasible to fine-tune on Occluded COCO~(very few data samples) and ImageNet-R~(evaluation only),
for Occluded COCO retrieval we fine-tune on the original COCO dataset, and for ImageNet-R retrieval we fine-tune on ImageNet. As can be seen in Table~\ref{tab:ood_result}  by this fine-tuning the performance of all the models is significantly boosted further. This demonstrates that fine-tuning ELIP enables efficient adaptation of the models to new datasets. 
The significant difference ELIP makes is also illustrated in Figure~\ref{fig:curve_recall} (left). 
Please refer to the appendix on the fine-tuning.


