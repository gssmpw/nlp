% ICCV 2025 Paper Template

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
% \usepackage{iccv}              % To produce the CAMERA-READY version
% \usepackage[review]{iccv}      % To produce the REVIEW version
\usepackage[pagenumbers]{iccv} % To force page numbers, e.g. for an arXiv version

% Import additional packages in the preamble file, before hyperref
\input{preamble}

% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, 
% e.g. with the file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete *.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you should be clear).
\definecolor{iccvblue}{rgb}{0.21,0.49,0.74}
\usepackage[pagebackref,breaklinks,colorlinks,allcolors=iccvblue]{hyperref}

%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\paperID{***} % *** Enter the Paper ID here
\def\confName{ICCV}
\def\confYear{2025}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{ELIP: Enhanced Visual-Language Foundation Models for Image Retrieval}

%%%%%%%%% AUTHORS - PLEASE UPDATE
\author{%
  Guanqi Zhan$^{1*}$, Yuanpei Liu$^{2*}$,
  Kai Han$^2$, Weidi Xie$^{1,3}$, Andrew Zisserman$^1$\\
    $^1$VGG, University of Oxford\quad\quad
    $^2$The University of Hong Kong \quad\quad
    $^3$Shanghai Jiao Tong University\\
  \texttt{\{guanqi,weidi,az\}@robots.ox.ac.uk} \\
  \texttt{ypliu0@connect.hku.hk}~~~~
  \texttt{kaihanx@hku.hk} \\
}




\begin{document}


\twocolumn[{
    \vspace{-30pt}
    \renewcommand\twocolumn[1][]{#1}
    \maketitle
    \centering
    \vspace{-10pt}
 \includegraphics[height=0.45\linewidth]{images/teaser.pdf}   
 \vspace{-8mm}
   \captionof{figure}{
\textbf{The ELIP architecture.}
{\em Left}: We propose a novel architecture that can be applied to pre-trained and frozen vision-language foundation models, such as CLIP, SigLIP, SigLIP-2 and BLIP-2, to enhance their text-to-image retrieval performance. 
The \emph{key idea} is to use the text query to define a set of visual prompt vectors that are incorporated into the image encoder to make it aware
of the query when generating the embedding. An MLP maps from the text space to the visual space of the input to the ViT encoder. The architecture is lightweight, and our data curation strategies enable efficient and effective training with limited resources.
{\em Right}: In this retrieval example from the COCO benchmark, the top-$k$ ($k$=100) images are re-ranked by our ELIP model for the text query: `People on bicycles ride down a busy street'. The ground truth image matching the query is not in the top-5 ranked images in the initial CLIP ranking, but is ranked top-1 (highlighted in the dashed box) by the re-ranking. 
   }
    \label{fig:teaser}
    \vspace{20pt}
    }
    ]

\def\thefootnote{*}\footnotetext{Equal contribution.}\def\thefootnote{\arabic{footnote}}

\input{sec/0_abstract}    
\input{sec/1_intro}
\input{sec/2_related}
\input{sec/preliminary}
\input{sec/3_architecture}
\input{sec/4_data}
\input{sec/5_experiment}
\input{sec/6_conclusion}


\paragraph{Acknowledgements. } 
This research is supported by EPSRC Programme Grant VisualAI EP$\slash$T028572$\slash$1, a Royal Society
Research Professorship RP$\backslash$R1$\backslash$191132, a China Oxford Scholarship and the Hong Kong Research Grants Council -- General
Research Fund (Grant No.: 17211024).
We thank Minghao Chen, Jindong Gu, Zhongrui Gui, Zhenqi He, Jo√£o Henriques, Zeren Jiang, Zihang Lai, Horace Lee, Kun-Yu Lin, Xianzheng Ma, Christian Rupprecht, Ashish Thandavan, Jianyuan Wang, Kaiyan Zhang, Chuanxia Zheng and Liang Zheng for their help and support for the project.




{
    \small
    \bibliographystyle{ieeenat_fullname}
    \bibliography{main}
}


\input{sec/7_supple}


\end{document}
