\chapter{Bayesian Neural Networks in the Feature Learning Regime}
While providing qualitative lessons on deep learning, the infinite over-parametrization or GP limit has several shortcomings. The first, concerns feature learning, a loosely defined term reflecting changes in the internal representations generated by a neural network. There is much evidence for feature learning in realistic networks which, as shown below, is absent in the infinite width limit. Perhaps the most convincing evidence for feature learning is the success of transfer learning, a technique in which one takes several trained layers of DNN, installs new layers on top of these, and trains these new layers on a related but different task. This means that the first, pre-trained, layers generated some useful representation. 

The GP/infinite-width limit is often a less powerful inference model. For instance, finitely over-parametrized CNNs which are the only set of competitive models in which a careful comparison with GP was made, shows that GPs underperform \citep{novak2018bayesian,lee2020finite} by around 5\% depending on whether or not one uses pooling. Furthermore, one can analytically establish the superiority of finite networks over GPs in various shallow network settings \citep{arous2021online,Maillard2020,Luca2024}. Finally, as we will discuss, computing perturbative corrections to the GP limit reveals that these are not controlled by one over the width/channels, but rather by that number times an extensive scale proportional to the amount of data \citep{seroussi2023separation,Hanin2023}. Thus, the GP limit is an uncontrolled approximation for most realistic scenarios. 

In this chapter, we consider the theory of feature learning, focusing on the case of fully-trained/equilibrated DNNs trained using Langevin dynamics. The equilibrium/Bayesian setting contains much of the richness of finite networks but simplifies things substantially, as we do not have to worry about transient non-equilibrium effects. We would first derive an interacting field theory governing the network outputs and preactivations. We would show how the GP limit is obtained from this theory at infinite width/channels, and why there can be no representation learning in this limit. We then consider three ways of dealing with interactions: Perturbation theory, Kernel Scaling, and Kernel Adaptation. The first approach provides information on the scaling required to be in the GP limit, as well as educated guesses for another relevant scaling regime - the mean-field scaling regime. The second provides an accurate quantitative account for the average predictor in a learning regime in which notable feature learning effects occur, however, the DNN is still in the same sample complexity class as GPR. Finally, the kernel-adaptation approach, accurate in mean-field scaling and semi-accurate in standard scaling, further tracks the internal representations directly and allows exploring regimes in which the DNN is in a different complexity class than GPR. 

\section{Multilayer actions}
Here we derive an explicit field theory description of finite DNNs. In the field-theory spirit of this review, our goal is to integrate or marginalize over all weights and obtain an equivalent description of the partition function in terms of fields only. Specifically, the field governing network output ($f(x)$) and those governing pre-activations per layer per neuron ($h^{(l)}_c(x)$). 

A basic tool we would use to derive this action are functional-delta-functions, commonly used in field theory \citep{MoritzBook} along with their Fourier representation. A functional-delta-function, $\delta[f]$, is the path-integral analog of standard delta functions. Formally it obeys the following relation w.r.t. a $Df$ functional integration  
\begin{align}
\int Df \delta[f-g] S[f] &= cS[g] 
\end{align}
where $S[f]$ is any functional. 

Similarly to a usual delta function, $\delta[..]$ can be written in Fourier form as followed
\begin{align}
\delta[f-g] &= \int D\tilde{f} e^{i \int d\mu_x \tilde{f}(x)(f(x)-g(x))}
\end{align}
where it can be verified that for any discretization of the path integral into multidimensional integrals as done in Sec. \ref{Sec:PathIntegrals}, it becomes a multidimensional delta function \footnote{albeit with different $c$ constant which we, as said, justly ignore.}. We refer to the field $\tilde{f}(x)$ as $f(x)$'s helping/auxiliary field.  

Next, we consider the partition function for the Boltzmann distribution (or Bayesian posterior) and apply the above as needed to obtain an action in the weights, which we may then integrate out. The formalism is general; however, let us introduce it for a three-layer FCN network for clarity. Similarly to Sec. \ref{Sec:NNGP_On_Data}, we consider a Langevin-trained network with weight decay and unnormalized-MSE loss which leads to a Boltzmann distribution as $t \rightarrow \infty$. The partition function, or equivalently the normalization of this distribution, is then     
\begin{align}
Z &= \int \dif {a} \;\dif{w^{(1)}} \;\dif{w^{(0)}} \; e^{-\frac{1}{2}\left[N|{a}|^2+N|{w^{(1)}}|^2+d |{w^{(0)}}|^2\right]- \frac{1}{T}\sum_{\mu=1}^P L(z_{{\theta}}(x_{\mu}),y_{\mu})}\\ \nonumber 
z_{\theta}(x) &= \sum_{i,j=1}^N \sum_{k=1}^d a_i \sigma\left(w^{(1)}_{ij} \sigma(w^{(0)}_{jk} x_k)\right)
\end{align}
 where $L(a,b)$ can be any point-wise loss (e.g. $(a-b)^2$). We would also introduce the shorthand notation $|{\theta}|^2$ capturing the first three terms in the above action. We further choose above the variance of individual weights ($a_i,w^{(1)}_i,w^{(0)}_i$) to be one over the fan-in without the data term. Taking a Bayesian perspective, this implies that in the prior all preactivations are order $1$ as they are for a network initialized by the  Kaiming/He initialization.

 
Evaluating averages under the above action is difficult because of the many weight variables appearing non-linearly in the loss via $z_{ {\theta}}(x)$. To circumvent this we introduce a field governing the network outputs by inserting $\int Df \delta[f-z_{{\theta}}]=1$ and going to the Fourier representation of the delta functional. Namely, 
\begin{align}
Z &= \int \dif {a} \; \dif {w^{(1)}} \; \dif{w^{(0)}} \int Df \; D\tilde{f} \; e^{-\frac{|{\theta}|^2}{2}- \sum_{\mu} L(f(x_{\mu}),y_{\mu})+i \int d\mu_x \tilde{f}(x) (f(x)-z_{{\theta}(x)})}
\end{align}
At this point the readout layer weights (${a}$) can already be integrated out however the others are still non-Gaussian. To remedy this, we proceed by introducing the pre-activations of the pen-ultimate layer by placing the following constant factor in the partition function  $\prod_{i=1}^N \int D h_i^{(1)}\delta[h_i^{(1)}-\sum_{j,k}w^{(1)}_{ij} \sigma(w^{(0)}_{jk}x_k)]$ using again the delta functional identity. This then yields 
\begin{align}
Z&=\int \dif{a} \; \dif {w^{(1)}} \; \dif{w^{(0)}} \; Df \; D\tilde{f} \; D{h}^{(1)}D{\tilde{h}}^{(1)} \; e^{-S} \\ \nonumber 
S &= \frac{|{\theta}|^2}{2}+\frac{1}{T}\sum_{\mu=1}^P L(f(x_{\mu}),y_{\mu})-i \int \dif\mu_x \tilde{f}(x) \Big(f(x)-\sum_i a_i \sigma(h^{(1)}_i(x))\Big) \\ \nonumber 
&- i\sum_i \int \dif \mu_x \tilde{h}^{(1)}_i \Big(h^{(1)}_i(x) - \sum_{jk} w^{(1)}_{ij} \sigma(w^{(0)}_{jk }x_k)\Big)
\end{align}
at this point both the ${a}$ and ${w}^{(1)}$ integrations are Gaussian, but ${w}^{(0)}$ is still non-Gaussian. We may again negotiate this difficulty by placing a factor involving the $h^{(0)}_j(x)=\sum_{k}w^{(0)}_{jk}x_k$ however since, unlike hidden pre-activations, this is just a simple linear transformation on the input weights, it is of little practical use. At this point, we may integrate out only the ${a}$ and ${w}^{(1)}$ weights and obtain our field theory description of a 3-layer FCN (before data averaging) 
\begin{align}
\label{Eq:FCN3}
Z &= \int \int \dif {{w}^{(0)}}Df \; D\tilde{f} \; D{h}^{(1)} \; D{\tilde{h}}^{(1)} \; e^{-S_{\text{FCN3}}} \\ \nonumber 
S_{\text{FCN3}} &= \frac{d |{ w^{(0)}}|^2}{2}+\sum_{\mu} L(f(x_{\mu}),y_{\mu}) \\ \nonumber
&- i\int \dif\mu_x \Big[\tilde{f}(x) f(x)+\sum_i
\tilde{h}^{(1)}_i(x) h^{(1)}_i(x)\Big] \\ \nonumber 
&+\frac{1}{2N^{(1)}} \sum_i \left(\int \dif\mu_x \tilde{f}(x) \sigma(h^{(1)}_i(x))\right)^2\\ \nonumber
&+ \frac{1}{2N^{(0)}}\sum_{ij}\left( \int \dif \mu_x \tilde{h}^{(1)}_i(x) \sigma({w}^{(0)}_{j} \cdot {x})\right)^2
\end{align}
where we also allowed layer-dependent width ($N^{(l)}$). 

Next, we point out several properties of this action, starting from the {\bf fluctuating kernels' interpretation.} Indeed, one can readily integrate all the helping fields that appear in quadratic form by completing the squares. For instance, doing so on $\tilde{f}$, would yield a non-linearity of the form $\frac{1}{2}\int \dif\mu_x \dif\mu_{x'} f(x) f(x') \tilde{K}^{-1}(x,x')$ where $\tilde{K}$, the fluctuating kernel, is given by $N^{-1}\sum_{i=1}^N \sigma(h^{(1)}_i(x))\sigma(h^{(1)}_i(x'))$. The average of this latter quantity, at $N\rightarrow \infty$, was the kernel defined in Sec. \ref{Sec:InfiniteRandomDNNs}. Thus, finite-width effects are like having fluctuating, upstream weight-dependent kernels controlling the outputs. Notwithstanding, for subsequent approximations having the helping fields, and avoiding inversion of fluctuating kernels such as $\tilde{K}$, would prove useful. 

Next, we recover the {\bf GPR limit} at large $N^{(l)}$. The cleanest argument involves taking the limit sequentially downstream: Consider integrating out $w^{(0)}$ at very large $N^{(0)}$ via a cumulant-like expansion ($\log(\int \dif x P(x) e^{O(x)})=\langle O(x) \rangle_{P(x)}+\frac{1}{2} \text{Var}[O(x)]+.. $) with $x$ being all $w^{(0)}$'s, $O(x)$ the $w^{(0)}$-dependent random-variable defined by the last term in $S_{\text{FCN3}}$, $P(x)$ being the probability measure defined by the first term in the action. Discarding constants in partition functions, as usual, one obtains the action $S_{\text{FCN3}}$ with all $w^{(0)}$ dependence removed and additional terms of the form 
\begin{align}
&\left \langle \frac{1}{2N^{(0)}}\sum_{ij}\left( \int \dif \mu_x \tilde{h}^{(1)}_i(x) \sigma({w}^{(0)}_{j} \cdot {x})\right)^2 \right \rangle_{{\cal N}(0,d^{-1}I_{d\times d} \otimes I_{N^{(0)}\times N^{(0)}};{w}^{(0)})}  \\ \nonumber 
&+\frac{1}{2}Var\left[ \frac{1}{2N^{(0)}}\sum_{ij}\left( \int d\mu_x \tilde{h}^{(1)}_i(x) \sigma(\bm{w}^{(0)}_{j} \cdot \bm{x})\right)^2 \right]_{{\cal N}(0,d^{-1}I_{d\times d} \otimes I_{N^{(0)}\times N^{(0)}};\bm{w}^{(0)})}+... \\ \nonumber 
&= \frac{1}{2}\sum_{i=1}^{N^{(1)}} \left \langle \left( \int \dif \mu_x \tilde{h}^{(1)}_i(x) \sigma({w}^{(0)}_{j=1} \cdot {x})\right)^2 \right \rangle_{{\cal N}(0,d^{-1} I_{d\times d} \otimes I_{N^{(0)}\times N^{(0)}};{w}^{(0)})}  \\ \nonumber 
&+\frac{1}{2 N^{(0)}} Var\left[\sum_{i=1}^{N^{(1)}}\left( \int d\mu_x \tilde{h}^{(1)}_i(x) \sigma(\bm{w}^{(0)}_{j=1} \cdot {x})\right)^2 \right]_{{\cal N}(0,d^{-1}I_{d\times d} \otimes I_{N^{(0)}\times N^{(0)}};{w}^{(0)})}+...
\end{align}
where we used the independence of different ${w}^{(0)}_j$ under the Gaussian measure used in the cumulant expansion. At $N^{(0)}\rightarrow \infty$, only the first term above survives, and yields $\frac{1}{2}\sum_i \int \dif\mu_x \dif\mu_{x'} \tilde{h}_i^{(1)}(x) K^{(1)}(x,x') \tilde{h}_i^{(1)}(x')$, where $K^{(1)}(x,x')$ is the latent kernel of the input layer (see  Eq. \ref{Eq:NNGP_K_1}) which also coincides with above average over of $w^{(0)}_{j=1}$. As $\tilde{h}^{(1)}$ now appears only to quadratic order in the action, one can integrate it using square completion, and obtain $S_{\text{FCN3}}$ at $N^{(0)}\rightarrow \infty$ given by  
\begin{align}
S_{\text{FCN3},N^{(0)}\rightarrow \infty} &= \sum_{\mu} L(f(x_{\mu}),y_{\mu}) - i\int \dif\mu_x \Big[\tilde{f}(x) f(x)+\sum_i
\tilde{h}^{(1)}_i(x) h^{(1)}_i(x)\Big] \\ \nonumber 
&+\frac{1}{2N^{(1)}} \sum_i \left(\int \dif\mu_x \tilde{f}(x) \sigma(h^{(1)}_i(x))\right)^2 \\ \nonumber &+ \frac{1}{2}\sum_i \int \dif \mu_x \dif \mu_{x'} h_i^{(1)}(x) [K^{(1)}]^{-1}(x,x') h_i^{(1)}(x')
\end{align}
with all $\tilde{h}^{(1)},w^{(0)}$-dependent terms removed. The process can then be repeated, namely, perform a cumulant expansion in the second to last term of the above action under the Gaussian measure induced by the new kernel term (last term above) and integrating out $\tilde{f}$ to get a kernel term for $f(x)$. This step would recover $S_{\GP}$ (Eq. \ref{Eq:Z_FieldTheoryOnData}) as the action for $f$. We note by passing that, under the additional approximations used below, one can also establish this result using the weaker requirement of $N^{(l)}=N \rightarrow \infty$. 

The above derivation of the GP limit makes precise the notion that it is indeed a {\bf lazy learning} \citep{chizat2018lazy} limit --- one without any useful feature learning. Specifically, considering the first step detailed above, we find that the effect of $w^{(0)}$ on downstream variables ($h^{(1)},f$) is completely dataset agnostic as $N^{(0)}\rightarrow \infty$. Thus, if some dataset-dependent information gets encoded in the input weights, it has no consequence on the downstream processing of the network or predictions. The same then holds for the subsequent layer.   

\subsection{Dataset averaging}
Here we consider a necessary step towards solving any such theory analytically namely {\it dataset averaging}. Since all the more complex terms involving interactions between layers are dataset-agnostic \footnote{As discussed, around Eq. \ref{Eq:RKHS}, despite appearances the action is largely independent of $d\mu_x$.}, this amounts to repeating the manipulations of Sec. \ref{Sec:AveragedGPR} with the RKHS term replaced with these more complex terms. The result is the following replica field theory with $Q$ copies of each field and variable whose action is  
\begin{align}
\label{Eq:BarSFCN3}
&\bar{S}_{\text{FCN3}}= \sum_{a,j}\frac{d |{ w_{a,j}^{(0)}}|^2}{2} - P\int \dif \mu_x e^{-\sum_{a=1}^Q L(f_a(x),y(x))} \\ \nonumber &- \sum_{a=1}^Q i\int d\mu_x [\tilde{f}_a(x) f_a(x)+\sum_i
\tilde{h}^{(1)}_{ai}(x) h^{(1)}_{ai}(x)] \\ \nonumber 
&+\frac{1}{2N} \sum_{i,a} \left(\int \dif\mu_x \tilde{f}_a(x) \sigma(h^{(1)}_{ai}(x))\right)^2 + \frac{1}{2N}\sum_{ij}\left( \int \dif \mu_x \tilde{h}^{(1)}_{ai}(x) \sigma({w}^{(0)}_{a,j} \cdot {x})\right)^2
\end{align}
on which one can readily apply the approximations carried out in the second chapter. In this chapter, our focus is on finite-width effects. We thus consider the simplest approximation for the finite-sample effects, namely the EK approximation, and further, take the first order in Taylor expansion of the MSE loss term $- P\int \dif \mu_x e^{-\sum_{a=1}^Q L(f_a(x),y(x))} \approx - P + P\int \dif \mu_x \sum_{a=1}^Q L(f_a(x),y(x))$, which can, alternatively to the derivation in Sec. \ref{Sec:AveragedGPR} using Poisson averages, be regarded as the lowest order cumulant expansion of fluctuations of the loss. Accordingly, we obtain
\begin{align}
&\bar{S}_{\text{FCN3,EK}}= \sum_{a,j}\frac{d |{ w_{a,j}^{(0)}}|^2}{2}+P\int \dif\mu_x \frac{(f_a(x) - y(x))^2}{2\kappa^2}\\ \nonumber &- \sum_{a=1}^Q i\int \dif \mu_x [\tilde{f}_a(x) f_a(x)+\sum_i
\tilde{h}^{(1)}_{ai}(x) h^{(1)}_{ai}(x)] \\ \nonumber 
&+\frac{1}{2N} \sum_{i,a} \left(\int \dif \mu_x \tilde{f}_a(x) \sigma(h^{(1)}_{ai}(x))\right)^2 + \frac{1}{2N}\sum_{ij}\left( \int \dif \mu_x \tilde{h}^{(1)}_{ai}(x) \sigma({w}^{(0)}_{a,j} \cdot {x})\right)^2
\end{align}
To illustrate the power of our method, in the following, we will do concrete computations for a simpler case of 2-layer FCN action given, following similar manipulation to the 3-layer case, by 
\begin{align}
\label{Eq:S_FCN2_nonEK}
&\bar{S}_{\text{FCN2}}= -P\int \dif\mu_x e^{-\frac{(f_a(x)-y(x))^2}{2\kappa^2}} - \sum_{a=1}^Q i\int \dif \mu_x \tilde{f}_a(x) f_a(x) \\ \nonumber 
&+\frac{1}{2N} \sum_{i,a} \left(\int \dif\mu_x \tilde{f}_a(x) \sigma({w}^{(0)}_{a,i} \cdot {x})\right)^2 + \frac{d |{ w_{a,i}^{(0)}}|^2}{2}
\end{align}
which in the EK limit gets replaced by  
\begin{align}
\label{Eq:S_FCN2}
&\bar{S}_{\text{FCN2,EK}}= P\int \dif\mu_x \frac{(f_a(x)-y(x))^2}{2\kappa^2} - \sum_{a=1}^Q i\int \dif \mu_x \tilde{f}_a(x) f_a(x) \\ \nonumber 
&+\frac{1}{2N} \sum_{i,a} \left(\int \dif\mu_x \tilde{f}_a(x) \sigma({w}^{(0)}_{a,i} \cdot {x})\right)^2 + \frac{d |{ w_{a,i}^{(0)}}|^2}{2}
\end{align}

\section{Approximation Scheme: Perturbation theory}
We next discuss several approximate, yet asymptotically exact, ways of dealing with the interaction terms in the above action.  

Given that our action becomes Gaussian at $N\rightarrow \infty$, it is natural to consider it as the starting point of a $1/N$ perturbative expansion. Such an expansion can shed further light on the required conditions to be close to the GP limit. We would find that these conditions are rarely met in practice since each term in the series involves extensive summations over $P$, making the expansion poorly convergent unless $P \ll  N$ (as typical values for $N$ are $O(10^3)$ for FCN and $O(10^2)$ for CNNs). Interestingly, though, leading order perturbation theory appears to provide several insights about the correct scaling of hyperparameters (e.g. $\mu$P scaling as proposed by \cite{yang2022tensorprogramsvtuning} and also \cite{dinan2023effectivetheorytransformersinitialization}). This effectiveness is yet to be explained theoretically but seems to imply some correlation between $P \ll N$ and $P \gg N$ behavior of realistic neural networks.  

We turn to the technicalities of such an expansion focusing, for simplicity, on the case of $\bar{S}_{\text{FCN2,EK}}$ then sketch the generalization for deeper networks. Following the previous derivation of the GP limit, we may write our action as 
\begin{align}
&\bar{S}_{\text{FCN2,EK}}= \bar{S}_{0} + U \\ \nonumber 
&\bar{S}_{0} =  \frac{d|{w^{(0)}}|^2}{2}+\sum^Q_{a=1} P\int \dif \mu_x \frac{(f_a(x)-y(x))^2}{2\kappa^2} -  i\int \dif\mu_x \tilde{f}_a(x) f_a(x) \\ \nonumber &+\sum_{a=1}^Q \frac{1}{2} \int \dif\mu_x \dif\mu_{x'} \tilde{f}_a(x) K^{(1)}(x,x') \tilde{f}_a(x') \\ \nonumber 
&U = \bar{S}_{\text{FCN2,EK}}-\bar{S}_0 \\ \nonumber 
= & \frac{1}{2N} \sum_{a} \int \dif\mu_x \dif\mu_{x'} \tilde{f}_a(x) \sum_{i=1}^N\left[\sigma({w}^{(0)}_{a,i} \cdot {x})\sigma({w}^{(0)}_{a,i} \cdot {x'})-K^{(1)}(x,x')\right]\tilde{f}_a(x')
\end{align}
 
A first simplification, which is a natural outcome of working in the EK limit, is that all replica modes are decoupled. Hence, the replica limit is trivial here (see Sec. \ref{Sec:Replicas}) and we may remove replica indices and treat the action as a regular field theory. 

As the interaction involves the helping field, we require its average and correlations under the Gaussian theory $\bar{S}_{0}$. A simple computation yields 
\begin{align}
\label{Eq:tilde_f_averages_EK}
&\langle \tilde{f}(x) \rangle_{\bar{S}_0} = i\frac{P}{\kappa^2} \langle f(x)-y(x) \rangle_{\bar{S}_0} \equiv  i\frac{P}{\kappa^2} \langle f(x)-y(x) \rangle_{\GPR,\EK} \\ \nonumber
&\langle \tilde{f}(x)\tilde{f}(x')\rangle_{\bar{S}_0,\con} = [K^{(1)}+(\kappa^2/P) \hat{I}]^{-1}(x,x') \equiv \tilde{K}^{-1}(x,x') 
\end{align}
where $\hat{I}$ is the identity operator on function space. We find that the average $\tilde{f}(x)$ is proportional to the discrepancy in GPR prediction using the kernel $K^{(1)}(x,x')$ within the EK approximation \footnote{We note by passing that had we not done dataset averaging, this average would have been  $i\kappa^{-2} 
\sum_{\mu} \delta(x-x_{\mu}) \langle f(x_{\mu})-y(x)\rangle_{\GPR}$. This allows one to recover the fixed dataset case from our average dataset computations}. Furthermore, by adding a source term that couples $\tilde{f}$ to the action and taking $\log$-derivatives w.r.t to the source to obtain averages, one can show that the average of $\tilde{f}$ is proportional to the discrepancy of the average predictor also under the full interacting theory. 

As would soon be apparent, the leading order correction in $1/N$ is obtained from a 2nd order perturbative expansion in $U$. Following Sec. \ref{Sec:PT}, this is given by 
\begin{align}
\langle f(x_*) - y(x_*) &\rangle_{\bar{S}_{\text{FCN2,EK}}} = \langle f(x_*) - y(x_*)\rangle_{\text{GPR,EK}} \\ \nonumber &-i\frac{\kappa^2}{P} \langle \tilde{f}(x_*) U \rangle_{\bar{S}_0,\con}-i\frac{\kappa^2}{P}\frac{1}{2}\langle \tilde{f}(x_*) U^2 \rangle_{\bar{S}_0,\con} 
\end{align}
The term linear in $U$ vanishes due to the independent average of the Gaussian input weight under which $\langle \sigma({w}^{(0)}_{a,i} \cdot {x})\sigma({w}^{(0)}_{a,i} \cdot {x'})\rangle_{\bar{S}_0}=K^{(1)}(x,x')$. 
The second term involves the variance of $\sigma({w}^{(0)}_{a,i} \cdot {x})\sigma({w}^{(0)}_{a,i} \cdot {x'})$ which is non-vanishing. Taking this average and summing over the width index, one obtains  
\begin{align}
&\frac{1}{2}\langle \tilde{f}(x_*) U^2 \rangle_{\bar{S}_0,\con} = \int \dif\mu_{x_1} \dif\mu_{x_2} \dif\mu_{x_3}\dif\mu_{x_4} \\ \nonumber & \frac{1}{8N}\left\langle \tilde{f}(x_*)   \tilde{f}_a(x_1) \tilde{f}_a(x_2) \tilde{f}_a(x_3) \tilde{f}_a(x_4)\right\rangle_{\bar{S}_0,\con} \\ \nonumber 
& \times\left[\langle \sigma({w} \cdot {x_1}) \sigma({w} \cdot {x_2})\sigma({w} \cdot {x_3}) \sigma({w} \cdot {x_4}) \rangle_{w} -K^{(1)}(x_1,x_2)K^{(1)}(x_3,x_4)  \right]  \\ \nonumber 
=& \int d\mu_{x_1} \dif\mu_{x_2} \dif\mu_{x_3}\dif\mu_{x_4} \frac{1}{24}\left\langle \tilde{f}(x_*)   \tilde{f}_a(x_1) \tilde{f}_a(x_2) \tilde{f}_a(x_3) \tilde{f}_a(x_4)\right\rangle_{\bar{S}_0,\con} \\ \nonumber 
& \times\left[3\langle \sigma({w} \cdot {x_1}) \sigma({w} \cdot {x_2})\sigma({w} \cdot {x_3}) \sigma({w} \cdot {x_4}) \rangle_{w} -K^{(1)}(x_1,x_2)K^{(1)}(x_3,x_4)[3]  \right]/N
\end{align}
where $\langle ... \rangle_w = \langle ... \rangle_{{\cal N}(0,d^{-1}I_{d\times d};w)}$ and we used to interchangeability of $x_1,x_2,x_3,x_4$ in the connected average to symmetrize the variance term. To this end we introduce the pair-symmetrizer notation  $g(x_1,x_2,x_3,x_4)[3]=g(x_1,x_2,x_3,x_4)+g(x_1,x_3,x_3,x_4)+g(x_1,x_4,x_2,x_3)$. 

We next relate the factor appearing in the bottom line above, to the fourth cumulant associated with the output of a random network (For a more general discussion of this structure and its relation with the Edgeworth expansion, see Ref. \cite{naveh2021predicting}). Since the random network output is $z_{\theta}(x)=\sum_{i=1}^N a_i \sigma({w}_i \cdot x)$ and made of $N$ independent variables, the cumulants are just $N$ times those of any specific $i$. The fourth cumulant is therefore  
\begin{align}
&u(x_1\ldots x_4) \equiv \\ \nonumber
&N \langle a \sigma({w} \cdot {x}_1) a \sigma({w} \cdot {x}_2) a \sigma({w} \cdot {x}_3) a \sigma({w} \cdot x_4) \rangle_{P_0(a,{w})} - N^{-1} K(x_1,x_2)K(x_3,x_4)[3] \\ \nonumber 
&= \left[3\langle  \sigma({w} \cdot {x}_1)  \sigma({w} \cdot {x}_2)  \sigma({w} \cdot {x}_3)  \sigma({w} \cdot {x}_4) \rangle_{w} - K(x_1,x_2)K(x_3,x_4)[3]\right]/N
\end{align}
where $P_0(a,{w}) =  {\cal N}(0,N^{-1};a),{\cal N}(0,d^{-1} I_{d\times d};w)$ leading to a $1/N$ scaling of all the above terms. The above expression generalizes to any neural network, even deeper FCNs or CNNs, by simply replacing the last line above with the fourth cumulant of the desired random network.

Next, we resolve the above connected average over the $\tilde{f}$'s to obtain 
\begin{align}
&\nonumber -i\left\langle \tilde{f}(x_*)   \tilde{f}_a(x_1) \tilde{f}_a(x_2) \tilde{f}_a(x_3) \tilde{f}_a(x_4)\right\rangle_{\bar{S}_0,\con}  \\  
&=  -\tilde{K}^{-1}_{x_*,x_1} \frac{P^3\Delta(x_2)\Delta(x_3)\Delta(x_4)}{\kappa^{6}} \otimes \left[\sum_{i=1}^4 \{x_1 \leftrightarrow x_i\} \right] \\ \nonumber
&+\tilde{K}^{-1}_{x_*,x_1}\left[\frac{P\Delta(x_2)}{\kappa^2}\tilde{K}^{-1}_{x_3,x_4}\otimes[1+\{x_2 \leftrightarrow x_3\}+\{x_2 \leftrightarrow x_4\}]\right] \\ \nonumber 
&\otimes \left[\sum_{i=1}^4 \{x_1 \leftrightarrow x_i\}\right]
\end{align}
where $\Delta(x_1)=\langle f(x_1) - y(x_1)\rangle_{\GPR,\EK}$  and we have further introduced the variables swap notation ($\otimes [\{x_i \leftrightarrow x_j\}]$) implying for instance $A(x_1,x_2,x_3) \otimes [1+\{x_1 \leftrightarrow x_2 \}+\{x_1 \leftrightarrow x_3\}]=A(x_1,x_2,x_3)+A(x_2,x_1,x_3)+A(x_3,x_2,x_1)$.

Next, we require a concrete expression for $u(x_1 \ldots x_4)$ to complete the computation. To this end, we first note that for FCNs, due to the rotation symmetry of the kernel, it must obey the ansatz 
\begin{align}
u(x_1 \ldots x_4)\propto u(|x_1|\ldots |x_4|,x_1 \cdot x_2,x_1 \cdot x_3,\ldots ,x_3 \cdot x_4)
\end{align}
Next we make the reasonable assumption that the dimension of $x_i$'s together with the data measure implies that typically $|x_i|^2 = O(1)$ and $x_i \cdot x_{j\neq i}=O(1/\sqrt{d_{\eff}})$, with $d_{\eff}\gg 1$. Further,  taking, for simplicity, $|x_i|^2=1$, and an antisymmetric network ($z_{\theta}(x)=-z_{\theta}(-x)$) we obtain the following series expansion for $u(x_1...x_4)$
\begin{align}
u(x_1 \ldots x_4) = u_1[(x_1 \cdot x_2) (x_3 \cdot x_4)][3]/N+O(\frac{1}{d_{\eff}^2 N}) 
\end{align}
where we kept the leading $O(1/d_{\eff})$ contribution and ignored higher order anti-symmetric (e.g. $(x_1 \cdot x_2)^3(x_3 \cdot x_4)$) terms \footnote{A more careful approximation is to model those quartic and higher terms as white noise \cite{Cui2023} when sampling from $d\mu_x$, however in our context this would not affect the average predictor.}. Underlying this expansion is the assumption that $u(...)$ has $O(1)$ Taylor coefficient, such as $u_1$ above, when viewed as a function of $x_i \cdot x_j$. This is consistent with the fact that, when normalizing properly, $u(x,x,x,x)$ being a 4th cumulant of the network outputs, is order $1$. Hence, it would require delicate cancellations if its Taylor coefficients were $O(d_{eff})$. We further note by passing that the above becomes exact for deep linear networks, which show many of the phenomena associated with generic deep linear networks (e.g. their training dynamics is non-linear \citep{saxe2013exact}, they exhibit feature learning \citep{LiSompolinsky2021,seroussi2023separation}, some linear CNNs can show sample complexity separation from GPR \citep{naveh2021self}).  Indeed the above argument, also related to the Gaussian equivalence principle \citep{Cui2023}, can be seen as the source of this effective linear behaviour.  

Let us focus on the most interesting $1/N$ correction involving three $\Delta$ factors or equivalently three $y$ factors. Whereas the linear in $y$ terms can still be absorbed into some task/target agnostic complex redefinition of the GPR kernel, the cubic term represents some initial form of feature learning or interaction between features. Gathering all relevant contributions, it amounts to the following correction to the average discrepancy (or predictor) 
\begin{align}
\label{Eq:1stOrderCorrection}
&\frac{u_1 P^2}{24 N\kappa^4}\int \dif \mu_{x_1}... \dif\mu_{x_4} 4 \tilde{K}^{-1}_{x_*,x_1} \Delta(x_2)\Delta(x_3)\Delta(x_4)[(x_1 \cdot x_2) (x_3 \cdot x_4)[3]] = \\ \nonumber 
& \frac{3u_1P^2}{6 N\kappa^{4}}\int  \dif\mu_{x_1} \dif\mu_{x_2} \tilde{K}^{-1}_{x_*,x_1} (x_1 \cdot x_2)\Delta(x_2) \int  \dif\mu_{x_3}  \dif\mu_{x_4} \Delta(x_3) (x_3 \cdot x_4) \Delta(x_4)
\end{align}
where the factor of $4$ came from the 4 equivalent exchanges implied by $\otimes [\sum_{i=1}^4 \{ x_1 \leftrightarrow x_i\}]$.

Analyzing the above term, we first note that, despite appearances, it is non-divergent as $\kappa^2 \rightarrow 0$ since it follows from taking Eq. (\ref{Eq:EK_Av_Pred}) back to real space and subtracting $y$ that  $P\kappa^{-2} \Delta(x) =\int  \dif\mu_{x'} \tilde{K}^{-1}(x,x')y(x')$. In fact, it vanishes, which is, however, an artefact of working with the EK approximation beyond its validity regime. 

More troubling, for our perturbative approach, is the appearance of $P^2/N$ and the fact that other $P$ dependencies can only appear through the discrepancy in GPR prediction. Assuming $\Delta(x)$ falls off as $P^{-\alpha_D/2}$, we require $\alpha_D > 4/3$ for this correction to be small compared to $f(x_*)$ (which we assume is $O(1)$). Reference \cite{Bahri2024Explaining}, studied $\alpha_D$ in various settings and found $\alpha_D\approx 1$ in variance-limited regimes and $\alpha_D \in [0.26,0.58]$ in resolution-limited regimes. Both imply that just scaling up $P$, keeping $N$ fixed, would place us outside the perturbative regime. More advanced scaling can be considered. A reasonable choice is to scale instead the number of parameters and $P$ together which implies, for deep networks, $N \propto \sqrt{P}$. For $\alpha_D=1$, the perturbative correction would not scale. However, for $\alpha_D<1$, it would still diverge as we scale up the model. 


%Assuming we are in the regime of impressive yet non-perfect learning, say test loss $\propto \Delta(x)^2$ dropped by a factor of $10$, the $\Delta(x)^3$-power would be of the order of $10^{-3}$ as well as perhaps factors of $1/d_{\eff}$ coming from the last factor above, which would need to compete with $P^2/N$. As a rough estimate, note that for CIFAR-10, $d_{\eff}$, estimated by retaining ~80\% of the is $O(10)$, $P=50000$ (CIFAR-10 training set) and FCNs yield a factor $10$ drop factor in test/validation MSE loss \cite{lee2020finite}, we can estimate that perturbative treatment is valid when $N \gg 50000^2/(10 \cdot 10^{1.5}) \approx 10^7$. In contrast, typical FCN widths are $O(1000)$. Similarly, using an NTK type GP limit, Fig. S5 of Ref. \cite{lee2019wide} shows that for such toy datasets, even with $P=4096$, having $N=2028$ still generates noticeable corrections to the corresponding GP limit. Moreover, at a fixed width, the discrepancy between the actual DNN and the GP grows at least linearly with dataset size (see Fig. S5 of that work). \IS{Not sure I follow the estimate} 

A more precise analytical statement can be carried out for deep linear networks, in which case, given our normalization here that $|x|=1$ and not $d$ as before, $K(x,x')=x \cdot x'$. Noting in addition that $\frac{P}{\kappa^2}\Delta(x)=[[K+\kappa^2 I/P]^{-1}y](x)$ we find the following simplified expression \footnote{The correct interpretation for operator inverses in this rank-deficient case is to use pseudo-inverses, namely inverses which project out the null-space of the operator.}
\begin{align}
\frac{3u_1}{6 N}\Delta(x_*) \sum_k y_k^2 \frac{\lambda_k}{(\lambda_k+\kappa^2/P)^2}
\end{align}
where $\lambda_k$ are, as always, the eigenvalues of the kernel w.r.t. the data measure, and $y_k$ is the decomposition of the target on these modes. 

Consequently, we find the following $1/N$ correction for a deep linear network 
\begin{align}
\langle f(x_*) - y(x_*) &\rangle_{\bar{S}_{\text{FCN2,EK}}} &= \Delta(x_*)\left[1+\frac{3 u_1}{6N} \sum_k y_k^2 \frac{\lambda_k}{(\lambda_k+\kappa^2/P)^2} \right]
\end{align}
To estimate the magnitude of this correction, we first note that our normalization in the EK limit implies that for a normalized target ($y(x)=O(1)$), $\sum_k y_k^2=O(1)$. To make further progress, we should make some assumptions on the spectrum. The simplest is Gaussian iid data, in which case $\lambda_k=1/d$. Here for $\kappa^2=1$ and $P$ being of the same order of $d$ (intermediate learning), we find that $P^2/(dN)\propto d/N$, in line with the heuristic estimation above. At high performance ($P \gg d$) we obtain $d/N$ as the control parameter for the relative correction in the discrepancy. However, as the discrepancy vanishes for $P \gg d$, this is not easily noticeable in practice. Similar assessments can be carried in the more realistic case where $\lambda_k = k^{-1-\alpha}$ and $y^2_k = O(k^{-1-\alpha})$, which can arise for example by considering power law covariance structure on the data and uniform discrepancies at initialization. In this case, taking again $\kappa^2=1$ for clarity, the unlearnable modes ($k > k_T$) where $k_T$ is defined by $\lambda_{k_T}P=1 \Rightarrow k_T = P^{1/(1+\alpha)}$ would dominate the relative correction leading to a $\frac{P^2}{N} \sum_{k>k_{T}} y_k^2 \lambda_k \approx \frac{P^2}{N} k^{-1-2\alpha}_{T}=\frac{P^{\frac{1}{1+\alpha}}}{N}$-factor which is again extensive in $P$. 

As a comment to field theorists, we stress that our perturbative approach, and others', \citep{roberts2021principles} are strict low-order perturbation theories. As such they do not involve partial resummations (e.g. self-energy correction, one-loop corrections, etc.), commonly used in field theory. Indeed we have no RG-based justification for keeping only a subset of corrections, although it would be interesting to develop such a reasoning \citep{howard2024}. 


\section{Application: Hyperparameter transfer}
\label{Sec:Hyper}
Hyper-parameter transfer is a technique by which hyper-parameter tuning (e.g. choice of weight-decays, widths, depth, learning rates, etc.) is done by brute-force grid scans on small models (e.g. fewer parameters, smaller dataset, shorter training times). Subsequently, the resulting optimal hyper-parameters are re-scaled (transferred) appropriately to larger models.    

A recently proposed prescription for the above re-scaling is the consistent-limit prescription \citep{yang2022tensorprogramsvtuning,bordelon2023depthwise}. Here one scales hyperparameters, for example, the width, such that they do not affect the small $P$ behavior, where one may use perturbation theory. This ensures that, within perturbation theory, scaling up the width one remains with an optimal choice of hyperparameters for the small $P$ model. One then makes the working {\it assumption} that this remains a good choice for the model at larger $P$. This assumption is supported by the standard practice of tuning other hyperparameters such as learning rates, momentum, and weight decays based on early training times (and hence effectively small $ P $). However, why this optimal choice for small $P$ remains optimal for larger $P$ is non-trivial from a theoretical perspective. Still, if we adopt this viewpoint, we should scale parameters such that leading order perturbation theory remains invariant under scaling.

To this end, we consider our obtained perturbative correction to the predictor (Eq. \ref{Eq:1stOrderCorrection}) and note that it scales as $1/N$. Following said logic, we wish to change other quantities to cancel out this $1/N$ dependence. Taking cues from the deep linear case, we note that if we scale down both $\lambda_k$ and $\kappa^2$ by $1/N$ we obtain an $N$-independent result. Such a change can be obtained mechanistically, by dividing the network's readout layer weights by $\sqrt{N}$ and, viewing it as mimicking gradient noise, dividing Langevin noise variance by $1/N$, then scaling down all weight-decay terms by $1/\chi$ to maintain the variance despite the reduced noise. In the context of gradient flow and NTK dynamics, where weight-decay and noise are absent, $1/\sqrt{N}$ scaling down of the outputs is known as mean-field scaling which had some practical successes \citep{yang2022tensorprogramsvtuning,dinan2023effectivetheorytransformersinitialization}. In our Langevin training case, the re-scaling prescription is then to optimize the model's $\kappa^2$, weight-decays ($\gamma$), and $N$ at small $P,N$, then take $N,\kappa^2,\gamma \rightarrow SN,\kappa^2/S,\gamma/S$ for the larger $P$ experiments. The choice of scaling factor $S$, can be loosely chosen such that the overparmaterization ratio (number of parameters over the number of data points, which goes roughly as $N^2/P$) remains fixed (i.e. $P\rightarrow S^2 P$).    


\section{Approximation Scheme: Kernel Scaling}
A shortcoming of the previous perturbative approach is that it appears uncontrolled in various relevant settings. Furthermore, it seems unlikely that one can describe qualitatively new phenomena, such as feature learning, by perturbing away from a theory with no feature learning. This motivates us to consider non-perturbative approaches. 

One such approximation scheme, yielding surprisingly simple outcomes argues that under various circumstances non-perturbative on the average predictor can be absorbed into a single scalar variable governing the scale of the kernel. This approach is based on two approximations. The first was introduced in \cite{LiSompolinsky2021} in the context of deep linear networks, and the second approximation in \cite{ariosto2022statistical} which extends the former to non-linear networks. Here we derive a variant of this approach, where we also average over the data and work within the replicated field theory formalism. We also focus on the case of two-layer networks (i.e. Eq. \ref{Eq:S_FCN2_nonEK}).

Our starting point is thus Eq. \ref{Eq:S_FCN2_nonEK} where our first aim is to integrate out the read-in layer weights (${w}^{(0)}$). As one can verify in Eq. \ref{Eq:S_FCN2}, for deep linear networks ($\sigma(x)=x$) this amounts to a simple Gaussian integration. However, when $\sigma(..)$ is non-linear, direct integration of ${w}^{(0)}$ becomes intractable. To this end, Ref. \cite{ariosto2022statistical} suggests we treat terms such as $\int \dif \mu_x \tilde{f}(x) \sigma({w}^{(0)} \cdot x)$ as a Gaussian random variable under the probability measure induced by the weight decay part on the action (${\cal N}(0,I/d;w^{(0)})$) \footnote{Focusing for simplicity on anti-symmetric activation functions it also has zero mean.}. 
% w.r.t. ${w}^{(0)}$ drawn from the distribution induced by the Gaussian piece of the action, namely the $d|\bm{w}^{(0)}|^2/2$ term
This Gaussianity assumption is analogous to the central limit theorem if we view the integral over $\mu_x$ as a ``summation'' on sufficiently many weakly correlated variables under the ${\cal N}(0,I/d;w^{(0)})$ ensemble, namely the set of random variables $\tilde{f}(x) \sigma({w}^{(0)} \cdot x)$ one obtains for different $x$ points. We next see what theory this approximation leads to, then flesh out one typical scenario in which this works very well and another, with strong feature learning, where it fails. 

Focusing only on the ${w}^{(0)}$ relevant terms in Eq. \ref{Eq:S_FCN2_nonEK}, we have the following integral to perform \footnote{where we again allowed ourselves the freedom to add or remove constant factors partition functions, specifically here the normalization of the Gaussian distribution of ${w}^{(0)}$.}
\begin{align}
Z_w &\equiv \int \dif {w}^{(0)} e^{-\frac{1}{2N} \sum_{i,a} \left(\int \dif\mu_x \tilde{f}_a(x) \sigma({w}^{(0)}_{a,i} \cdot {x})\right)^2} {\cal N}(0,I/d;{w}^{(0)})
\end{align}
Following Ref. \cite{ariosto2022statistical} we 
% flesh out the above Gaussianity assumption by 
introduce the variables $q_{ai}=\int \dif \mu_x \tilde{f}_a(x) \sigma({w}^{(0)}_{a,i} \cdot {x})$ by inserting a product of the following term$$\int \dif q_{ai} \delta(q_{ai}-\int \dif\mu_x \tilde{f}_a(x) \sigma({w}^{(0)}_{a,i} \cdot {x}))$$ such that,
\begin{align}
... &= \int \dif q  e^{-\frac{1}{2N} \sum_{i,a} q^2_{ai}} \prod_{ai} \int \dif{w}_{ai}^{(0)} {\cal N}(0,I/d;{w}_{ai}^{(0)})
\\ \nonumber 
&\cdot \delta\left(q_{ai}-\int \dif\mu_x \tilde{f}_a(x) \sigma({w}^{(0)}_{a,i} \cdot {x})\right) \\ \nonumber 
&\stackrel{\text{Gaussianity}}{\approx} \int \dif q  e^{-\frac{1}{2N} \sum_{i,a} q^2_{ai}} \prod_{ai} {\cal N}(0,C[\tilde{f}_a];q_{ai}) = ... \\ \nonumber 
C[\tilde{f}_a] &= \int  \dif {w}_{ai}^{(0)} {\cal N}(0,I/d;{w}_{ai}^{(0)})\left(\int  \dif\mu_x \tilde{f}_a(x) \sigma({w}^{(0)}_{a,i} \cdot {x})\right)^2 \\ \nonumber 
&= \int  \dif\mu_x  \dif\mu_{x'} \tilde{f}_a(x) K^{(1)}(x,x') \tilde{f}_a(x')
\end{align}
where in the last equality we used Eq. \ref{Eq:NNGP_K_1} which defines the kernel. Performing the remaining Gaussian integration is straightforward and yields 
\begin{align}
Z_w &= \prod_{a=1}^Q \left(\int \dif q  e^{-\frac{1}{2N} \sum_{i=1,a} q^2_{ai}} {\cal N}[0,C[\tilde{f}_a];q_{a,i=1}]\right)^N = e^{-\sum_a\frac{N}{2}\log(1+C[\tilde{f}_a]/N)}
\end{align}

Thus we obtain the following approximation for Eq. \ref{Eq:S_FCN2_nonEK} following this approximate integrating out procedure of the input layer weights  
\begin{align}
&\bar{S}_{\text{FCN2}} \approx -P\sum_a \int \dif \mu_x e^{-\frac{(f_a(x)-y(x))^2}{2\kappa^2}} - \sum_{a=1}^Q i\int \dif\mu_x \tilde{f}_a(x) f_a(x) \\ \nonumber 
&+\sum_a\frac{N}{2}\log(1+C[\tilde{f}_a]/N)
\end{align}

Our second approximation is a mean-field type approximation for the above log term. The underlying assumption is that, $C[\tilde{f}_a]$ which can be re-written in $K^{(1)}$'s eigenfunction basis as $\sum_k \lambda_k \tilde{f}^2_{a,k}$, is a sum of many positive contributions and hence weakly fluctuating. Introducing the mean-field average of $C[\tilde{f}_a]$ under the above action which we denote by $C_{\MF}$), Taylor expanding the log to first order in $\Delta C[\tilde{f}_a]=C[\tilde{f}_a]-C_{\MF}$, and neglecting constant contribution to the action we obtain the following action 
\begin{align}
\label{Eq:S_FCN2_Rotondo}
&\bar{S}_{\text{FCN2,Scaling}}= -P\int \dif \mu_x e^{-\sum_a \frac{(f_a(x)-y(x))^2}{2\kappa^2}} - \sum_{a=1}^Q i\int \dif\mu_x \tilde{f}_a(x) f_a(x) \\ \nonumber 
&+\sum_a \frac{ \int \dif\mu_{x} \dif \mu_{x'}\tilde{f}_a(x)K^{(1)}(x,x')\tilde{f}_a(x')}{2 (1+ C_{\MF}/N)}
\end{align}
several comments are in order: {\bf (i)} Upon integrating out $\tilde{f}$ using simple square completion formulas, the above action coincides with that of data-average GPR (Eq. \ref{Eq:IntroducingSBar}) with a scaled kernel $K(x,x')=K^{(1)}(x,x')/(1+C_{\MF}/N)$. This is why it is often understood as a kernel scaling approximation \citep{ariosto2022statistical}, although the output variance does not simply scale \citep{rubin2024a} \footnote{This may perplex the keen reader given the above action, however, mean-field actions may not reliably predict variances. Specifically, adding source term ($\alpha$) which couples to $f$, one finds that $C_{\MF}$ contains an additional $O(\alpha^2)$ contribution which comes into play only when taking a 2nd derivative w.r.t. $\alpha$ to obtain the variance.} 
{\bf (ii)} at $N \rightarrow \infty$, we retrieve the standard GP-limit. {\bf (iii)} As far as the average predictor goes, scaling down the kernel is equivalent to scaling up the ridge ($\kappa^2$). Hence, we again find a simple renormalization of the ridge, as in Sec. \ref{SSec:Canatar}. Moreover, at vanishing ridge, the simple GP-limit ($N \rightarrow \infty$ with $d,P$ kept fixed) yields the same average predictor.

For non-vanishing ridge, one cannot avoid computing $C_{\MF}$. This requires the average and mean of $\tilde{f}_a$ under the above mean-field theory. To this end, we introduce a source term $-i\sum_a \int \dif x \; \alpha(x) \tilde{f}_a(x)$ in the action such that $\delta_{\alpha(x)}\log(Z_{\text{FCN2,Scaling}})|_{\alpha=0}$ and $\delta_{\alpha(x)}\delta_{\alpha(x')}\log(Z_{\text{FCN2,Scaling}})|_{\alpha=0}$ give the average and variance ($Cov$) of $\tilde{f}$ under $\bar{S}_{\text{FCN2,Scaling}}$. To obtain concrete expressions for these averages, we go back to $\bar{S}_{\text{FCN2}}$ (Eq. \ref{Eq:S_FCN2_nonEK}) and note that integrating $\tilde{f}$, in the presence of such source terms, yields a modified RKHS term ($\frac{1}{2} \int ({f}(x)+i\frac{\alpha(x)}{p_{\data}(x)}) K^{-1}(x,x') ({f}(x')+i\frac{\alpha(x')}{p_{\data}(x')})$ where $\dif\mu_x = p_{\data}(x)\dif x$. Taking the above functional derivatives at $\alpha=0$ we obtain 
\begin{align}
&\langle \tilde{f}_a(x) \rangle_{\bar{S}_{\text{FCN2}}} = i \int \dif\mu_x K^{-1}(x,x') \langle f_a(x') \rangle_{\bar{S}_{\text{FCN2}}} \\ \nonumber 
&Cov(\tilde{f}_a(x), \tilde{f}_a(x')) = K^{-1}(x,x')\\ \nonumber &-\int \dif \mu_z \dif \mu_{z'} K^{-1}(x,z) Cov_{\bar{S}_{\text{FCN2}}}(f_a(x),f_a(y)) K^{-1}(z',x') 
\end{align}
The above can be verified to agree with Eq. \ref{Eq:tilde_f_averages_EK} in the EK limit. By replacing the above averages with respect to the exact action, with those with respect to our mean-field approximation ($S_{\text{FCN2,Scaling}}$) we obtain the following self-consistency equation for $C_{\MF}$ namely 
\begin{align}
C_{\MF} &= \int \dif \mu_x \dif\mu_{x'} \langle \tilde{f}_a(x) \tilde{f}_a(x') \rangle_{\bar{S}_{\text{FCN2,Scaling}}} K^{(1)}(x,x') 
\end{align}
where $C_{\MF}$ appears on the r.h.s. via $\bar{S}_{\text{FCN2,Scaling}}$ which is itself $C_{\MF}$ dependent. Using the approximation scheme of Ref. \cite{Canatar2021} for GPR together with the spectral decomposition of $K(x,x')$ ($\lambda_k,\phi_k(x)$) and using Eqs. \ref{eq:barf},\ref{eq:f_k_f_k_connected} we obtain 
\begin{align}
\label{Eq:CMF_Consistency}
\frac{C_{\MF}}{(1+\frac{C_{\MF}}{N})} &= \sum_k \left( \frac{\lambda_k}{\lambda_k + \kappa^2_{\eff}/P} - \frac{P}{\kappa^4_{\eff}} \frac{\lambda_k}{(P/\kappa_{\eff}^2 \lambda_k+1)^2}D - y_k^2 \frac{\lambda_k}{(\lambda_k+\kappa_{\eff}^2/P)^2} \right)
\end{align}
where the first two terms on the r.h.s. come from the covariance of $\tilde{f}$ and the last one from the squared average. Also, $\kappa_{\eff},D$ are those obtained from Eqs. \ref{Eq:CanatarEffectiveRidge},\ref{Eq:D} with $K(x,x')=K^{(1)}(x,x')/(1+C_{\MF}/N)$ where we identify the kernel (down) scaling factor 
\begin{align}
{\cal Q}\equiv (1+C_{\MF}/N)
\end{align}
This self-consistent equation yields highly accurate predictions regarding the network test output, as can be seen in Fig. \ref{fig:scaling_output} for a linear network trained on a single index linear teacher. Accurate predictions have been obtained for nonlinear networks and on real datasets in \cite{RotondoFCN2}.  The above r.h.s. contains three distinct contributions. The first positive term accounts for GPR fluctuations, and roughly counts the number of learnable modes. The two other terms are associated with the second moment of the average predictor, across data-set draws. All these different contributions are extensive in the input dimension or $P$, consistent with our previous result that $N \gg P$ is required for the GP-limit (see also \cite{Hanin2023} for a proof of this and extension to infinite depth). 

\begin{figure*}[t]
\vskip -0.2in
\begin{centering}
\includegraphics[width=1\textwidth]{Chapters/figures/scaling_output.jpg}\vskip -0.1in\caption{Here we consider a single hidden layer linear network trained on a linear single index target, and compare the theoretical predictions of the kernel scaling approximation for the network output, as well those of the NNGP. We study two measures for the network output: (a) Learnability which we define as $\frac{f\cdot  y}{ y \cdot y},$ which corresponds to the proportion of the target learned by the network, as well as (b) mean squared test error. Network Parameters:
$d=50,N=1000$, each experimental point corresponds to an ensemble of $\sim$30 networks trained on different data seeds. Each network was trained until there was no visible change to the learnability, loss or hidden layer weight variance.}
\label{fig:scaling_output}
\par\end{centering}
\vskip -0.2in
\end{figure*}

A simple limit of the above formula is when the original/bare $\kappa^2$ goes to zero, as in this case the kernel scaling factor (${\cal Q}$) affects both $\lambda_k$ and $\kappa_{eff}$ the same. Specifically, both $\lambda_k$ and $\kappa_{\eff}^2$ get scaled down by ${\cal Q}\equiv (1+C_{\MF}/N)$, from their GP value since the latter by being a sum of the former. Using the notation $\lambda_{k}^{(1)} = \lambda_k {\cal Q},\ \kappa^{2}_{(1),\eff} = \kappa_{\eff}^{2}{\cal Q}$, where $\lambda_{k}^{(1)}$ are the eigenvalues of the GP kernel $K^{(1)}$ and $\kappa^{2}_{(1),\eff}$ is the GP solution to Eq. \ref{Eq:CanatarEffectiveRidge}, we can take out this ${\cal Q}$ scale, resolving all $C_{\MF}$ and $N$ dependencies of the r.h.s. we obtain a simple quadratic equation for ${\cal Q}$
\begin{align}
\frac{N({\cal Q}-1)}{{\cal Q}} &\stackrel{{\scriptscriptstyle \kappa\rightarrow0}}{=} \sum_k \frac{\lambda_{k}^{(1)}}{\lambda_{k}^{(1)} + \kappa^{2}_{(1),\eff}/P} - \frac{{\cal Q}\lambda_{k}^{(1)}D }{P(\lambda_{k}^{(1)}+\kappa^{2}_{(1),\eff}/P)^2}- \frac{{\cal Q} y_k^2  \lambda_{k}^{(1)}}{(\lambda_{k}^{(1)}+\kappa^{2}_{(1),\eff}/P)^2}
\end{align}
where we also note that $D$ is independent of ${\cal Q}$. 

Let us flesh out the usefulness of such ``scale learning" at different bias (average-predictor) variance (GPR fluctuations) trade-offs. When the target is large \footnote{which can also be seen as a form of mean-field scaling, namely taking the variance of $a_i$'s to be   $O(1/N)$ time some small number.}, the loss in the strict GP-limit will mainly depend on average predictor (i.e. resolution-limited regime). In the above, the two negative, target-dependent, terms would dominate the r.h.s. We denote the sum (taken as positive) by $Y$ which is $O(y^2)$ and ignore the first term on the r.h.s. Using this notation the above equation can be written as $\frac{Y}{N}{\cal Q}^2+{\cal Q}-1=0$, yielding, in our limit of large target (specifically $\frac{Y}{N}\gg 1$), ${\cal Q} \approx \sqrt{N/Y} \ll 1$. Thus, the kernel scales up to partially approach the scale of the target, still however leaving the bias term dominant due to the rather slow $O(\sqrt{Y})$ kernel-scaling. In the opposite regime, of a small target, the first positive term on the r.h.s. dominates. Based on \ref{Sec:ScalingLaws}, this term, which counts the number of learnable modes, would scale as $P$. Making an order of magnitude analysis, we thus have ${\cal Q}-1={\cal Q} \frac{O(P)}{N}-{\cal Q}^2 \frac{Y}{N}$, implying at $Y \ll |O(P)/N-1|$, ${\cal Q} = O(P)/Y$. Thus, the kernel is scaled down, to meet the magnitude of the target, thereby making the variance contribution to the loss comparable to the bias. Notably, this happens in a setting where, GP-wise, we expect the variance to be strongly dominant. 

{\bf Revisiting the Gaussianity assumption.} We turn to verify the Gaussianity assumption underlying this analysis. Notably this assumption is equivalent to saying that under a typical draw of $\tilde{f}(x)$, $\int \dif \mu_x \tilde{f}_a(x) \sigma({w}^{(0)}_{a,i} \cdot {x})$ is, to a good approximation, a linear function of ${w}^{(0)}$ as this is the most general Gaussian variable under ${\cal N}(0,I/d;w^{(0)})$. Focusing again on the case of a large target (or mean-field scaling) $\tilde{f}(x)$ would be weakly fluctuating around its mean. We thus aim to  assess how close the next expression is to a linear function of $w^{(0)}$
\begin{align}
\int \dif\mu_x \langle \tilde{f}_a(x)\rangle_{S_{\text{FCN2}}} \sigma({w}^{(0)}_{a,i} \cdot {x}) &\propto \int \dif\mu_x [\langle f(x) \rangle_{S_{\text{FCN2}}} - y(x)] \sigma({w}^{(0)}_{a,i} \cdot {x}) =... 
\end{align}

For concreteness, let us next consider an FCN network with data taken from a uniform measure on the hypersphere. In this case, we may decompose $y(x)$ as $\sum_l \phi_l(x) y_l$ where $\phi_l(x)$ is the sum of all spherical Harmonics of order $l$ in $y(x)$ and $y_l$ is the norm of their coefficients. We thus obtain  
\begin{align}
... &= \sum_{l=0}^{\infty} \frac{\kappa^2_{\eff}/P}{\lambda_l + \kappa^2_{\eff}/P} y_l \int \dif\mu_x \phi_l(x) \sigma({w}^{(0)}_{a,i} \cdot {x})
\end{align}
Taking $\sigma(..)$ to be linear, the r.h.s can only be a linear function of ${w}^{(0)}$ and the contribution of $l>1$ is exactly zero. Taking $\sigma(..)$ to be $\text{Erf}$, and focusing on $\phi_{l=1}(x)$ which we may write as ${w}_* \cdot x$, the above integral is $\sqrt{\pi/2} ({w}_* \cdot {w}_{a,i}^{(0)})/\sqrt{1+2|{w}_{a,i}^{(0)}|^2}$ \citep{rubin2024grokking}. Thus at large input dimension, where the fluctuations of $|{w}_{a,i}^{(0)}|$ become negligible, we indeed obtain a contribution linear in ${w}^{(0)}$, in support of the Gaussianity assumption. However, for a non-linear target and at $P$ large enough the $l=0,l=1$ contribution to the discrepancy would be highly learnable and thus strongly suppressed compared to $l=2$ and above. Focusing on $l=2$, we then formally Taylor expand the activation function to infinite order, and note that the zeroth and first-order terms in this expansion are fully spanned by the $l=0,l=1$ spherical Harmonics. Consequently, these $O(1),O(w^{(0)})$ contributions disappear from the integral. The remaining contributions, say $l=2$ one, would give rise to $(w^* \cdot w^{(0)}_{a,i})^2$ or higher terms \citep{rubin2024grokking} which are clearly non-Gaussian random variables under ${\cal N}(0,I/d;w^{(0)})$. Examining the original argument for Gaussianity, in the light of this result, the culprit is that $\tilde{f}_a(x)\sigma({w}^{(0)} \cdot x)$ becomes correlated across different $x$ values, thereby undermining the central limit theorem which relies on having no or weak correlations between the summed random variables.  

We conclude that the assumption underlying kernel scaling breaks down when the correlation between linear functions and the discrepancy over the dataset measure becomes negligible compared to those of higher-order functions. This is likely to be the case when the linear components of the target have been almost fully learned, as one expects for small $\kappa^2$ and $P \gg d$. Taking Gaussian iid data, mean-field scaling, and $P \propto d$ one indeed finds that kernel-scaling fails namely, actual finite neural networks can learn the higher (quadratic, cubic, etc...) components of the target 
\citep{abbe2021staircasepropertyhierarchicalstructure,naveh2021self,  Cui2023,arous2021online,rubin2024a}. In contrast, GPR with the scaled kernel requires $P \propto d^l$ to learn the $l$'th target component. At finite ridge, this poorer scaling stems from the fact that the learnability of an $l$'th spherical Harmonics goes as  $\kappa^{-2}_{\eff}P\lambda_l < \kappa^{-2} P\lambda_l \propto P d^{-l}$. At zero-ridge, this can be argued for in more detail based on our formulas. Instead, we suffice ourselves with the following heuristic argument: GPR with the scaled kernel is still rotationally invariant as is the Gaussian iid data. As such its Gaussian prior favors all $l=2$ spherical harmonics the same. Furthermore, due to spectral bias, information learned from the $l=1$ piece of the target cannot affect the $l=2$ piece. As there are $O(d^2)$ $l=2$ functions, all equally likely, $n=O(d^2)$ data points are needed to determine their coefficients. 

We note that an actual DNN can work differently. Specifically, it can use the knowledge obtained when learning the $l=1$ component of the target, and use that to emphasize certain $l=2$ Spherical Harmonics. This form of assisted (or staircase \cite{abbe2021staircasepropertyhierarchicalstructure}) learning can be tracked via the adaptive kernel approach which we next discuss.   

\section{Approximation Scheme: Kernel Adaptation}
\label{Sec:KernelAda}
An essential strategy of statistical physics is to describe complex systems in terms of a few order parameters. In the previous analysis this order parameter was ${\cal Q}$, the scaling factor of the kernel which, for deeper networks, would become a set of scaling factors (see Refs. \cite{LiSompolinsky2021,ariosto2022statistical} where this is made explicit). Here we consider a richer set of order parameters meant to capture a richer set of phenomena-- the layer-wise kernels themselves. Instead of obtaining a non-linear equation for the scaling factor (i.e. Eq. \eqref{Eq:CMF_Consistency}) we would obtain an equation for these kernels. This would have the clear disadvantage of being more complex but the advantage of describing how internal representation develops and captures beyond-GPR effects in DNNs, leading to a new sample complexity class, as explained below. 

{\bf Related approaches.} This line of thought has been explored concurrently by several groups with various similarities and differences. The equilibrium statistical mechanics approach was used in \citep{naveh2021self,seroussi2023separation} and can also be viewed as a first-principles statistical mechanics formalization of the ideas of kernel flexibility of Refs. \cite{aitchison2019bigger,aitchison2021deepProcess} and a generalization of the latter to non-linear networks. Analogous dynamical approaches appear in Refs. \citep{yang2022tensorprogramsvtuning,bordelon2022self}, but focused on on-data results and avoided some of the approximation we take below (specifically, Gaussian or Gaussian mixture approximation for pre-activations) and traded those with numerics, otherwise they essentially do the same approximation (layer-wise decoupling using average kernel). Unique to Refs. \citep{naveh2021self,seroussi2023separation} is the observation that in many scenarios (but not all \citep{seroussi2023separation,rubin2024grokking}), pre-activation statistics are Gaussian or mixtures of Gaussians \cite{rubin2024grokking}. It should be noted that unlike \citep{seroussi2023separation}, where the transition from on-data to data-average was carried in an ad-hoc manner \footnote{Specifically the ``$q$-factors" of that work which coincide with the more careful treatment carried here.}-- this review takes a full data-averaged approach and avoids using any data-sized matrices.  

The kernel adaptation approach relies on one main approximation (mean-field layer decoupling) and an additional Gaussianity approximation, which often applies yet needs case-by-case analysis. We first provide an overview of these approximations and then demonstrate them on a simple model. 

{\bf Mean-field decoupling between layers.} In multi-layer actions, e.g. Eq. \eqref{Eq:BarSFCN3}, we will treat terms such as $\frac{1}{N}\sum_{i=1}^N \sigma(h_{ia}^{(l)}(x))\sigma(h_{ia}^{(l)}(y))$ as weakly fluctuating and replace them by their mean-field average and keep only leading order dependence in their fluctuations. By the same token, we would treat terms such as $\frac{1}{N}\sum_{i=1}^N\tilde{h}^{(l)}_{ia}(x)\tilde{h}^{(l)}_{ia}(y)$ as weakly fluctuating (see Eq. \ref{Eq:FCN3}). In addition, we will treat $\tilde{f}_a(x) \tilde{f}_a(y)$ as weakly fluctuating even though it is not a sum over many variables. 

As we would show below, this approximation is justified under mean-field scaling. We introduce such scaling in our action by dividing $\sigma_a^2$ by an additional $\chi \gg 1$ factor which suppresses $\tilde{f}$ fluctuations \footnote{An alternative approach which avoids averaging over the $\tilde{f}$-fields is to introduce kernel-fields using delta-functions, and apply the Grtner-Ellis theorem \cite{fischer2024critical} which yields the same result.} Taking $\chi=N$ this scaling of parameters is consistent with the mean-field scaling as introduced in Sec. \ref{Sec:Hyper}. When working on-data rather than in the data-averaged/field-theory scenario, as we have done so far, $\chi \gg 1$ (let alone $\chi=N$) is alone controls this approximation. However, as in this review we also take into account the dataset ensemble, we further require that dataset-induced fluctuations do not dominate the mean of $\tilde{f} \tilde{f}$. As we will see below, this is typically the case when overall performance is good and feature learning in consistent between different draws of the same dataset. 

As shown in Ref. \cite{seroussi2023separation}, the above mean-field approximation leads to a set of decoupled layer-wise actions that ``see'' each other through averages of the aforementioned term/operators. In the presence of nonlinear activations function these layer-wise  actions are non-Gaussian requiring additional approximation such as VGA (variational Gaussian approximation) or saddle point or in some cases involving phase transitions, an approximation via a mixture of Gaussians \cite{rubin2024grokking}. 

To demonstrate the various aspects of this approximation scheme, we will consider both 2-layer FCNs and CNNs with non-overlapping convolution windows via the following network 
\begin{align} \label{Eq:2_layer_FCN}
    f({x}) &= \sum_{i=1}^{N_w} \sum_{c=1}^{C} a_{ic} \sigma\left({w}_c \cdot {x}_{i}\right) 
\end{align}
where ${x} \in {\mathbb{R}}^{d}$ with $d = N_w S$ and ${w}_c, {x}_{i} \in {\R}^{S}$. The vector ${x}_{i}$ is given by the $iS,..,(i+1)S-1$ coordinates of ${x}$. 
The dataset consists of $\{ {x}_{\mu} \}_{\mu=1}^{n}$ i.i.d. samples, each sample ${x}_\mu$ is a centered Gaussian vector with covariance $I_d$. The FCN case is obtained by setting $N_w=1$. Notably $C$ now takes the role of $N$ of the previous sections as controlling the overparametrization.   

As before, we train this DNN using GD+noise/Langevin training. However, we also introduce the $\chi$, mean-field scaling factor, to the readout layer weights in the following manner. First, we define $\kappa^2/\chi$ as the noise rather than $\kappa^2$. Next, we tune the weight-decay such that without any data, $a_{ic} \sim {\mathcal N}(0,\sigma_\text{a}^{2}(N_w C)^{-1} \chi^{-1})$ and $[{w}_c]_s \sim {\mathcal N}(0,\sigma_\text{w}^2 S^{-1})$. This is set up such that at the GP limit ($C \rightarrow \infty$), the mean predictor is unaffected by $\chi$ whereas fluctuations are reduced by a factor of $\chi$. 

Before delving into a detailed analysis, let us flesh out three main questions one can address using such a model: {\bf Weight-sharing and laziness.} Focusing for simplicity on $\sigma(x)=x$, one can show, by direct computation, that the GP kernel associated with this network coincides with that of a linear FCN. Clearly, this limit does not reflect the presence of convolutional patches ($i$ index) and misses out on weight-sharing. Similar mismatches between CNNs and their lazy limits were first noted in Ref. \cite{novak2019neural} and used as a qualitative explanation for the poorer performance of the lazy regime. Thus, feature learning corrections play a qualitative role here in restoring the weight-sharing and local connectivity structure of CNN. {\bf Sample complexity.} The next question concerns sample complexity, a concept used to describe the scaling of $n$ with $d_{\inn}$ required for good learning. For $C=O(1)$, and a linear target of the form $y_{\mu} = \sum_{i} a^*_i ({w}^* \cdot {x}_{\mu,i})$ where $a^*_i \sim {\mathcal N}(0,1/N_w)$ and $w^*_s \sim {\mathcal N}(0,1/S)$, $n$ of the order of the number of parameters ($O((N_w+S)C)=O(\sqrt{d_{\inn}})$) is enough to fit the target. We expect the sample complexity (the dependence of $n$ on $d_{\inn}$) of a CNN to be better than that of an FCN ($O(d_{\inn})$) or similarly the lazy limit of that same CNN. {\bf Assisted learning and staircase function.} Another qualitative mechanism \citep{abbe2021staircasepropertyhierarchicalstructure,dandi2023twolayerneuralnetworkslearn} present even in the FCN ($N_w=1$) case, is that simple target components (e.g. linear ones such as $w^* \cdot x$) can focus the network's attention along particular directions in input space (e.g. ${w}^*$) thus enabling one to learn non-linear components at $n=O(d_{\inn})$ even though the lazy limit puts the associated eigenvalues at $1/d^{2}_{\inn}$ or less. Thus, the linear component may assist (or act as a ``staircase'') in learning the more complex parts of the target, something that a GP cannot do, at least not within the eigenlearning framework. 

Our starting point is the equivalent of \ref{Eq:BarSFCN3} for our CNN 
\begin{align}
\label{Eq:S_CNN2}
&\bar{S}_{\text{CNN2}}= -P\int \dif\mu_x e^{-\sum_{a=1}^Q \frac{(f_a(x)-y(x))^2}{2\kappa^2/\chi}} - \sum_{a=1}^Q i\int \dif\mu_x \tilde{f}_a(x) f_a(x) \\ \nonumber 
&+\frac{1}{2CN_w \chi} \sum_{c,a,i}^{C,Q,N_w} \left(\int \dif\mu_x \tilde{f}_a(x) \sigma({w}_{c,a} \cdot {x}_i)\right)\left(\int \dif\mu_{x'} \tilde{f}_a(x') \sigma({w}_{c,a} \cdot {x}'_i)\right) \\ \nonumber &+ \sum_{c,a}^{C,Q}\frac{S |{ w_{c,a}}|^2}{2}
\end{align}
where we replaced ${w}^{(0)}$ by ${w}$ for compactness. 

Our first approximation here is a mean-field approximation wherein we first rewrite $\tilde{f}_a(x) \tilde{f}_a(x')$ in terms of its average value ($A(x,x')$ assuming replica symmetry) and fluctuation namely $\tilde{f}_a(x) \tilde{f}_a(x')=A(x,x')+\Delta A_a(x,x')$ where $\Delta A_a(x,x') = \tilde{f}_a(x) \tilde{f}_a(x')-A(x,x')$ and $(CN_w)^{-1}\sum_{c,i=1}^{C,N_w}\sigma({w}_{a,i} \cdot {x}_i)\sigma({w}_{a,i} \cdot {x}'_i)=K({x},{x}')+\Delta K({x},{x}')$ where $K({x},{x}')+\Delta K({x},{x}')=(CN_w)^{-1}\sum_{c,i=1}^{C,N_w}\sigma({w}_{a,i} \cdot {x}_i)\sigma({w}_{a,i} \cdot {x}'_i)-K({x},{x}')$ and then expand the action up to linear order in $\Delta K,\Delta A$. Ignoring irrelevant constant additive terms in the action, we obtain 
\begin{align}
\bar{S}_{\text{CNN2}} &= S_{\text{readout}}+S_{w}+O(\Delta^2) \\ \nonumber 
S_{\text{readout}} &= -P\int \dif\mu_x e^{-\sum_{a=1}^Q \frac{(f_a(x)-y(x))^2}{2\kappa^2/\chi}} - \frac{1}{\chi}\sum_{a=1}^Q i\int \dif\mu_x \tilde{f}_a(x) f_a(x) \\ \nonumber &+ \frac{1}{2}\sum_{a=1}^Q \int \dif\mu_x \dif\mu_{x'} K(x,x') \tilde{f}_a(x) \tilde{f}_a(x') \\ \nonumber 
S_{w} &= \frac{1}{2CN_w \chi} \sum_{c,a,i}^{C,Q,N_w} \int \dif\mu_x \dif\mu_{x'} A(x,x') \sigma({w}_{a,i} \cdot {x}_i)\sigma({w}_{a,i} \cdot {x}'_i) + \frac{S |{ w_{c,a,i}}|^2}{2}
\end{align}
where $A(x,x')$ and $K(x,x')$ should be determined self-consistently under the above action via their definitions as averages. As advertised, as a result of the mean-field decoupling, two decouple actions have been obtained, one for the read-out layer and one for the input layer. These "see" each other only through the average/mean-field quantities $K(x,x')$ and $A(x,x')$.

Examining $S_{\text{readout}}$, one notes that integrating out the Gaussian $\tilde{f}$ fields, we find that it coincides with the dataset averaged GP action having a ridge given by $\kappa^2/\chi$ and a kernel given by $K(x,x')/\chi$. Thus, we can readily apply the approximations for GP inference of Chapter \ref{Sec:AveragedGPR} to this action. In particular, as far as the average predictor goes, we would find that it behaves effectively as a quadratic theory with $\kappa^2/\chi$ replaced by $\kappa^2_{\eff}/\chi$. Moreover, the fields can be rescaled such that $\chi$ multiplies $y$ thus leading to GPR with $\kappa^2_{\eff}$ ridge, $K(x,x')$ as the kernel, and $y(x)\chi$ as the target. 

Turning to $S_{\text{readout}}$, it clearly describes $C$ times $N_w$ times $Q$ iid weight distributions associated with each ${w}_{c,a,i}$. We may then focus on just one representative weight which we would simply denote by ${w}$. 

Solving the model now amounts to computing the values of $K(x,x')$ (which would depend on $A(x,x')$ appearing in $S_w$) and $A(x,x')$ (which would depend on $K(x,x')$) and requiring self-consistency leading, in general, to a closed operator equation relating $A(x,x')$ and $K(x,x')$. Similar results follow for deeper networks, where $K(x,x')$ would generalize to layer-wise kernels ($K^{(l)}(x,x')$) and $A^{(l)}(x,x')$ to layer-wise auxiliary field with $l$ being the layer number. For simple datasets, such as those considered below, these operators change in only a few relevant directions, simplifying the analysis considerably.  

Let us first consider the case of linear activation ($\sigma(x)=x$) where the action for ${w}$, following our mean-field decoupling above, is Gaussian and appears in quadratic form as  
\begin{align}
S_w &= \frac{1}{2} \sum_{a,i}^{Q,N_w} {w}_{a,i}^T \left[\frac{1}{{C} N_w \chi} \int \dif\mu_x \dif\mu_{x'} A(x,x') {x}'_i {x}^{T}_i+I_{S \times S}S\right] {w}_{a,i} 
\end{align}
where, notably, all different channel indices ($i$) and replica indices ($a$) came out decoupled (i.e. going from $S_w$ to probabilities via $e^{-S_w}\propto P(w)$, the latter is a product of iid distributions across these indices). As we shall soon see, $A(x,x')$ goes as {\it minus} the square discrepancy and hence acts to stretch the distribution of $w$'s along directions which overlap with the discrepancy. In other words, the weight covariance adapts to align with the error in predictions. Another observation is that $A(x,x') \propto \chi^2$, hence taking mean-field scaling in the sense of $\chi = N$, one loses the $N$ dependence, explaining why in this parameterization feature learning persists regardless of $N$. 

Next we note that given our target ($y_{\mu} = \sum_{i} a^*_i \sigma({w}^* \cdot {x}_{\mu,i})$) and choice of data measure (${x} \sim {\cal N}(0,I_{N_w S \times N_w S})$) our original action (Eq. \ref{Eq:S_CNN2}), is symmetric to any rotation of the $w_i$'s along the $O(S-1)$ directions orthogonal to $w^*$. Consequently, the $S \times S$ covariance matrix of $w_i$, must be of the form $c_{\perp} I + (c_*-c_{\perp}) {w}^* [{w}^*]^T$ for some two scalar $c_{\perp},c_*$, as this is the most general (real, symmetric) matrix which is invariant under the above $O(S-1)$ symmetry. As a result 
\begin{align}
\label{Eq:AdaptedKernel1}
K(x,x')&=N_w^{-1} \sum_{i=1}^{N_{w}} \langle ({w} \cdot x_i) ({w} \cdot x'_i) \rangle_{S(w)} \\ \nonumber &= N_{w}^{-1} \sum_{i=1}^{N_{w}} (x'_i)^T [ c_{\perp} I + (c_*-c_{\perp}) {w}^* [{w}^*]^T] x_i 
\end{align}
at which point we obtain {\bf an intuitive explanation} for the extra learning abilities of DNNs: The discrepancy in predictions (embodied by the above $A(x,x')$) can generate a new non-NNGP like kernel which can distinguish between target relevant and target irrelevant directions whenever $c_* > c_{\perp}$. Next, we proceed by computing this $c_*$ and seeing its quantitative effect. 


Given our simple Gaussian measure for $x$, this kernel can be diagonalized as follows: $N$-fold degenerate eigenfunctions associated with function which depend only on the teacher direction on each patch namely,  
\begin{align}
\int \dif\mu_{x'} K(x,x') ({w}^* \cdot x_i) &= \frac{c_* |{w}^*|^2}{N_w} ({w}^* \cdot x_i) \equiv \lambda_* ({w}^* \cdot x_i)
\end{align}
whereas all other linear functions are degenerate with an eigenvalue $c_{\perp}/N_w$. 

Having expressed the action of the weights and $K(x,x')$ in terms of $c_{\perp},c_*$ we turn to compute the GPR implied by $S_{\text{readout}}$ with the above $K(x,x')$ and deduce $A(x,x')$ thereby closing our equations for $c_{\perp},c_*$. Since all eigenfunctions are Gaussian in the sense of Sec. \ref{SSec:Canatar} we may use the approximation therein to obtain 
\begin{align}
\bar{f}_{a}(x) &= \frac{\frac{c_* |{w}^*|^2}{N_w}}{\frac{c_* |{w}^*|^2}{N_w}+\kappa^2_{\eff}/P} y(x) 
\end{align}
where we also used the fact that $y(x)$ is a superposition of the above $N$-fold degenerate manifold. Focusing on the statistics of $\tilde{f}_a(x)$, relevant for $A(x,x')$, one can show by introducing a source field which couples to $\tilde{f}_a(x)$, integrating over $\tilde{f}_a$, and taking derivatives w.r.t. to that source that  
\begin{align}
\bar{\tilde{f}}_a(x) &= i \chi \int \dif \mu_{x'} K^{-1}(x,x') \bar{f}_a(x') = \frac{i\chi}{\frac{c_* |{w}^*|^2}{N_w}+\kappa^2_{\eff}/P} y(x) \\ \nonumber 
\langle \tilde{f}_a(x)\tilde{f}_a(x')\rangle_{\con} &= \chi^2 \int \dif \mu_{z} \dif\mu_{z'} K^{-1}(x,z) K^{-1}(x,z') \langle f_a(z)f_a(z') \rangle_{\con}\\ \nonumber &-\chi K^{-1}(x,x')
\end{align}
where $ \langle f_a(z)f_a(z') \rangle_{\con}$ is given, in the eigenfunctions basis, by Eq. \eqref{eq:f_k_f_k_connected}. Working at large $\chi$, we focus on the dominant, $O(\chi^2)$ contributions to $A(x,x')$ given by 
\begin{align}
A(x,x') &= \bar{\tilde{f}}_a(x) \bar{\tilde{f}}_a(x')+\chi^2 \sum_k \frac{\phi_k(x) \phi_k(x')}{\lambda_k^2} \left(\frac{P}{\kappa^2_{\eff}}+\lambda_k^{-1}\right)^{-2} \frac{P}{\kappa^4_{\eff}}D 
\end{align}
where $D$ is defined in Eq. (\ref{Eq:D}). 
Anticipating that the network would learn the target well (i.e. $\lambda_k$ associated with $y(x)$ would be much larger than $\kappa_{\eff}^2/P$), one can show that the second term which reflects dataset fluctuations should be suppressed by $\kappa^2_{\eff}/P$ compared to the first and hence negligible. Conveniently $A(x,x')$ can now be written as $y(x)y(x')(\lambda_* + \kappa_{\eff}^2/P)^{-2}$ which, placed in the quadratic form in $S(w)$ yields the mean-field self-consistency equation 
\begin{align}
[c_{\perp} I + (c_*-c_{\perp}) {w}^* [{w}^*]^T]^{-1} &= c_{\perp}^{-1} I + (c^{-1}_*-c_{\perp}^{-1}) {w}^* [{w}^*]^T \\ \nonumber 
&= -\frac{\chi}{CN_w} \sum_i |a^*_i|^2 {w}^* [{w}^*]^T (\lambda_* + \kappa_{\eff}^2/P)^{-2} + S I. 
\end{align}
By projecting both sides of this equation on any direction orthogonal to $w^*$, we obtain $c_{\perp}=S^{-1}$, i.e. no feature learning along orthogonal directions to the target \footnote{Had we kept the dataset fluctuation terms, proportional to $D$, we would find an insignificant increase in variance in those directions when the target is well learnable.}. Focusing for clarity on the case where $|{w}^*|=|{a}^*|=1$, we obtain the following equation for $c_*$ 
\begin{align}
c^{-1}_* &= S - \frac{\chi}{CN_w}\frac{1}{(\frac{c_*}{N_w}+ \kappa_{\eff}^2/P)^2}  
\end{align}
Assuming like before that $\lambda_* \gg \kappa^2_{\eff}/P$, this simplifies to $c^{-1}_* = S - \frac{\chi}{C} \frac{N_w}{c^2_*}$ implying $c^2_*- S^{-1}c_* - \frac{\chi N_w}{C S}=0$. 

Let us analyze the above result. Taking first a mean-field scaling of the type $\chi=N_w$ and $N_w=S$, the above equation implies $c_* = 1 + O(1/S)$ and therefore $\lambda_* = 1/N_w + O(1/d_{\inn})$ whereas $\lambda_{\perp}=1/d_{\inn}$. This $O(S)$ inflation of the eigenvalue along the teacher direction implies that $P=O(N_w)$ samples are sufficient to learn the target well instead of $O(N_w S)$ as in the lazy regime or for an FCN \footnote{The fact that an FCN cannot do better than their lazy limit in this linear setting is implied by the kernel-scaling result}. Taking instead $\chi$ to be larger but finite and $N_w \propto S \propto C$, one still obtains a good approximation (though not asymptotically exact). In this milder form of mean-field scaling, we find, $c_* = \sqrt{ \frac{\chi N_w}{SC}}$ hence $\lambda_* = \sqrt{\chi/(SN_w C})$ implying $P=O(\sqrt{SN_w C})=O(d_{\inn}^{3/4})$ samples are enough. This is again a better sample complexity class compared to the lazy limit or an FCN though not as good as that of mean-field scaling. The change in sample complexity relative to the lazy limit can be observed in Fig. \ref{fig:sample_complexity}, where this approach predicts that the network will indeed learn the target at $P=O(d_{\inn}^{3/4})$ in agreement with experiment and in contrast to the GP predictions. We define learnability as the components of the target learned by the network, given by- $\frac{ f \cdot y}{ y \cdot y}$, so that learnability=1 implies perfectly learning the target. 


% MH read until here 2025-02-18



\begin{figure*}[t]
\vskip 0.2in
\begin{centering}
\includegraphics[width=0.8\textwidth]{Chapters/figures/sample_complexity.png}\caption{Learnability of linear CNNs as a function of $P$. We take $S,N,C \propto \alpha$, and consider different $\alpha$ scales of these parameters. Here the network is observed to learn the target at $P\propto d^{3/4}$, regardless of the parameter scale, as opposed to the GP predictions which predict learning at $P\propto d$. Parameters: $\chi=100$, $N=10\alpha,S=50\alpha,C=1000\alpha$.}
\label{fig:sample_complexity}
\par\end{centering}
\vskip -0.2in
\end{figure*}

Finally, we comment on $\kappa^2_{\eff}$. Since the readout layer coincides with the data-averaged GP action, $\kappa^2_{\eff}$ obeys Eq. \ref{Eq:CanatarEffectiveRidge} w.r.t. to the adapted kernel $K(x,x')$. In general, this adds an additional equation which must be solved (typically numerically) together with that for $c_*$. For large $\kappa^2$ (compared to the MSE), this can be avoided using the EK approximation for $S_{\text{readout}}$. For small or intermediate values of $\kappa^2$ one notes that $K(x,x')$ contains a single outlier eigenvalue which is well-learnable and typically well-learnable eigenvalues do not contribute to $\kappa^2_{\eff}$ . Hence, $\kappa^2_{\eff}$ can be approximated by its value w.r.t. to the non-adapted kernel, thereby decoupling it from the equation for $c_*$. Moreover, in our simple setting explicit expressions for $\kappa^2_{\eff}$ are given in Ref. \cite{Canatar2021}.   

{\bf Non-linear networks.} Let us next generalize this result to non-linear activations, such as $\sigma(x)=\text{Erf}(x)$. Repeating the previous analytical workflow, symmetry would still play the same role, implying that $c_*,c_{\perp}$ are the only relevant parameters to track. The main complication here compared to the linear case is that the action $S_w$, is non-Gaussian and hence it is unclear how to calculate $K(x,x')$, a quantity which includes averages w.r.t. $S_w$. Another, more technical complication, is that the spectrum of $K(x,x')$ becomes harder to diagonalize. Both these issues turn out to be subleading for large enough $S$, where due to concentration phenomena in the mean-field regime, we may replace $|{x}_i|^2$ and $|{w}|^2$ by their mean. 

Turning to the first issue, let us assume and later verify the assumption self-consistently, that $A(x,x')$ maintains the previous form ($A(x,x') \propto y(x) y(x')$). Using the fact that, 
\begin{align}
\label{Eq:Useful_Integral}
\int \dif x {\cal N_w}(0,I_{S\times S};x) \Erf({w} \cdot x) {v}\cdot x= \frac{2}{\sqrt{\pi}} \frac{{w} \cdot {v}}{\sqrt{1+2|{w}|^2}}
\end{align}
we obtain the following expression for $S_w$ given the above ansatz for $A(x,x')$
\begin{align}
\frac{1}{2CN_w \chi} \frac{1}{(c_*/N_w+\kappa^2_{\eff}/P)^2} \sum_i \frac{4}{\pi} \frac{({w}_i \cdot {w}^*)^2}{1+2|{w}_i|^2}.
\end{align}
A general approximate tool for handling such non-quadratic action is the Variational Gaussian Approximation (VGA), directly analogous to the Hartree-Fock approximation in physics. This was used in Ref. \cite{seroussi2023separation}, based on the general heuristic argument that actions which involve many similar degrees of freedom with all-to-all interactions often admit a good Gaussian approximation. Here we can show this in more concrete terms, as the source of all non-quadratic behavior here is $|{w}_i|^2=\sum_{k=1}^S [{w}_i]_k^2$ appearing in the denominator. Being a sum over many ($S$) degrees of freedom it is reasonable to replace it by its mean ($|{w}_i|^2=1$) and neglect fluctuations entirely (we will revisit this approximation when we take $\chi = O(N_w)$). Following this we obtain a quadratic action and the only effect of the non-linearity on $S_w$ is the extra factor of $\frac{4}{3\pi} \approx 0.42$ accompanying the ${w}^*$ dependent term in the action compared to the linear case. We may thus readily compute $K(x,x')$ within this VGA/mean-field approximation. Specifically, using a direct application of the computation leading to Eq. \ref{Eq:ErfKernel} within each patch we obtain 
\begin{align}
K(x,x') &= \frac{2}{\pi N_w} \sum_i \sin^{-1} \left[\frac{{x}^T_i \Sigma {x}'_i}{\sqrt{1+2 {x}_i^T \Sigma {x}_i}\sqrt{1+2 [{x}'_i]^T \Sigma {x}'_i}} \right] 
\end{align}
where, again due to $O(S-1)$ symmetry, $\Sigma = c_{\perp}I + (c_*-c_{\perp}) {w}^* [{w}^*]^T$. The above is a sum of 2-layer FCN Erf network NNGP kernel acting on each patch. 

To obtain an equation for $c_{\perp},c_*$, what remains is to diagonalize the above kernel in order to obtain the discrepancy ($\bar{\tilde{f}}_a(x)$) given $\Sigma$, which, as assumed earlier, would be proportional to $y(x)$. Since these patch-wise kernels all commute under our Gaussian data measure, we may diagonalize each one separately and reconstruct the full spectrum. Focusing on a single patch or equivalently on the $N=1$ case, we note that we may undo the Gaussian average over ${w}$ and combine it with the previous integral to obtain 
\begin{align}
&\int \dif x'_1 {\cal N}(0,I_{S \times S};x'_1) \frac{2}{\pi} \sin^{-1} \left[\frac{{x}^T_1 \Sigma {x}'_1}{\sqrt{1+2 {x}_1^T \Sigma {x}_1}\sqrt{1+2 [{x}'_1]^T \Sigma {x}'_1}} \right] {v} \cdot x'_1 \\ \nonumber 
&= \int \dif x'_1 {\cal N}(0,I_{S \times S};x'_1)\int \dif {w} {\cal N}(0,\Sigma;w) \Erf({w} \cdot x_1)\Erf({w} \cdot x'_1) { v}\cdot x'_1 \\ \nonumber 
&= \int \dif {w} {\cal N}(0,\Sigma;w) \Erf({w} \cdot x_1)\frac{2}{\sqrt{\pi}} \frac{{w} \cdot { v}}{\sqrt{1+2|{w}|^2}} = ... 
\end{align}
which apart from having $\Sigma \neq I_{S \times S}$ is of the same form as that in our Erf integral. Redefining ${w}'=\sqrt{\Sigma}^{-1} w$, where $\sqrt{\Sigma}$ is the matrix having the same eigenvectors as $\Sigma$ but with square root eigenvalues, we have that ${w}' \sim {\cal N}(0,I_{S \times S})$, consequently,  
\begin{align}
... &= \int \dif {w}' {\cal N}(0,I_{S\times S};{w}') \Erf({w}' \cdot [\sqrt{\Sigma} x_1]) \frac{2}{\sqrt{\pi}} \frac{{w}' \cdot [\sqrt{\Sigma} { v}]}{\sqrt{1+2{w'}^T \Sigma {w}'}} \\ \nonumber 
&\approx \int \dif{w}' {\cal N}(0,I_{S\times S};{w}') \Erf({w}' \cdot [\sqrt{\Sigma} x_1]) \frac{2}{\sqrt{\pi}} \frac{{w}' \cdot [\sqrt{\Sigma}{ v}]}{\sqrt{1+2\Tr[\Sigma]}}=...
\end{align}
where we used again the mean-field approximation on $|{w}|^2=[{w}']^T \Sigma {w'}\approx \Tr[\Sigma]$ which is justified provided $\Sigma$ does not have a dominating (larger by an $O(S)$ factor) eigenvalue. We may now readily carry out our Erf integral and use again our mean-field treatment of norms to obtain 
\begin{align}
... &= \frac{4}{3\pi \sqrt{1+2\Tr[\Sigma]}\sqrt{1+2x_1^T \Sigma x_1}} {v}^T \Sigma x_1 \approx \frac{4}{3\pi(1+2\Tr[\Sigma])} {v}^T \Sigma x_1  
\end{align}
We thus arrive at the simple conclusion that at large $S$, ${v} \cdot x_1$ with ${v}$ chosen along eigenvectors of $\Sigma$ are approximate eigenfunctions of the kernel. Thus we again find, like in the linear case, that $\bar{\tilde{f}}_a(x) \propto y(x)$ and hence $A(x,x')\propto y(x)y(x')$, only we a slightly different proportionality factor compared to the linear case.  Solving the equations for $c_*$ numerically, we indeed observe that the amplification factor (namely $\lambda_*/\lambda_{\perp}$) as well as the learnability only minorly differs between the linear and non-linear networks, as evident in Fig. \ref{fig:amp_fact_cnn}.  

\begin{figure*}[t]
\vskip -0.8in
\begin{centering}
\includegraphics[width=1\textwidth]{Chapters/figures/non_lin_comp.jpg}
\caption{In this figure we compare a linear network trained on a single index linear teacher, with an Erf network trained on a cubic single index teacher ($y(x)=w_* \cdot x +0.1 H_3(w_*\cdot x)$, where $H_3$ is the third Hermite polynomial). The ratio between the teacher direction eigenvalue of the kernel to the eigenvalues corresponding to orthogonal directions for the Erf and linear networks is shown in panels (a) and (b) respectively. In panels (c), (d) the learnability ($f\cdot y/y\cdot y$) is shown for the Erf and linear network respectively.  Network parameters:
$\chi=100$, $N_w=1,5,10,S=50,C=1000$.}
\label{fig:amp_fact_cnn}
\par\end{centering}
\vskip -0.2in
\end{figure*}

Thus, apart from some $O(1)$ factors, we obtain the same result as in the linear case for $\chi \gg 1$ and expect an $P=O(N\sqrt{S})$ complexity. Notably, since the outlier in $\Sigma$ has eigenvalue $c_* \propto 1/\sqrt{S}$ our mean-field replacement of $|w|^2$ and $x_1^T \Sigma x_1$ are justified at large $S$ and in fact $\Tr[\Sigma]\approx 1$. However, taking $\chi = O(N)$, we previously found $c_* \propto 1$ which implies that the contribution of $w$ fluctuations along $w^*$ is as dominant as those along all other directions combined. Failing this consistency check means we may only assume the norm is self-averaging along directions orthogonal to the target. If so one may try the approximation $|w|^2 \approx (S-1) c_{\perp}+(w \cdot {w}^*)^2=\frac{S-1}{S}+(w \cdot {w}^*)^2$ and $x_1^T \Sigma x_1 = \frac{S-1}{S}+c_* (x_1 \cdot {w}^*)^2$, complicating the diagonalization process in general due to creation of ``harmonics'' or powers of $y(x)$ in $A(x,x')$. We can control this extra complication by softening the mean-field scaling by taking, say, $\chi = N/100$, leading to a similar sample complexity class as in the linear case. 


\subsubsection{Some relations between Kernel-Scaling and Kernel-Adaptation}
Finally, we establish some similarities and distinctions between the kernel-scaling and kernel-adaptation approaches. The technique we refer to as kernel-scaling was first derived in Ref. \cite{LiSompolinsky2021}, for deep linear networks and was called ``The Backpropagating Kernel Renormalization''. The latter is exact in the scaling limit $P\propto N_w \propto d \rightarrow \infty$, for fixed depth \citep{Hanin2023}. The kernel-adaptation approach is only accurate up to $1/\chi$ corrections, due to the mean-field decoupling of the last layer (though often quite accurate even for moderate $\chi$, see \cite{rubin2024a}). Still, for mean-field scaling ($\chi=O(N)$) it also becomes exact. In fact, using an on-data formulation of the kernel-adaptation approach \cite{rubin2025kernels}, one can show that the equations for the predictor coming from both these approaches coincide. 

Turning to non-linear FCNs, we recall that Ref. \cite{ariosto2022statistical} extended ``The Backpropagating Kernel Renormalization'' by approximating $\int \dif \mu_x \tilde{f}(x) \sigma(w^{(0)} \cdot x)$ 
in Eq. \ref{Eq:S_FCN2} as a Gaussian variable. This is exact for linear networks \footnote{Technically, they did so in an on-data formulation whereas we work here with a data-averaged formulation, but it is the same approximation.}, and provides a good approximation when the summand is weakly correlated in the $\mu$ index. In standard scaling and iid data, this approach appears asymptotically exact in harmony with the fact that FCNs in standard scaling and such data behave effectively as linear networks \citep{Cui2023}. However, going to mean-field scaling, finite FCNs are in a different sample complexity class than their GP counterparts, via, for instance, the aforementioned staircase mechanisms \citep{abbe2021staircasepropertyhierarchicalstructure,Paccolat_2021}. The kernel-scaling approximation, which leaves us in the infinite-width/GP sample complexity class cannot accommodate such effects. %Taking a teacher-student setting as done in this review, the issue in the kernel-scaling approach arises from the fact that $t_{\mu}$, whose average value is the discrepancy, loses its linear in $(w^* \cdot x_{\mu})$ component, as the linear component of the target becomes almost perfectly learnable. The remaining non-linear dependence, when considered in $\sum_{\mu} t_{mu} \sigma(w \cdot x_{\mu})$, leads to a non-Gaussian $(w^* \cdot w)^2$ term. Thus the sum of many variables, in this case, does not lead to a Gaussian behavior.   

At least for FCNs with weakly non-linear target functions and for CNNs (as shown above, see also Ref. \cite{naveh2021self}), kernel-adaptation can accurately predict this change in sample complexity class, limited however, to large $\chi$. 

Kernel-scaling has been adapted to CNNs \citep{RotondoCNN1,bassetti2024featurelearningfinitewidthbayesian}, at the price of having a vector order parameter associated with the scaling of each convolutional patch, however, it remains to be seen whether this extension is capable of predicting changes to sample complexity, as shown in Sec. \ref{Sec:KernelAda}. 



\subsubsection{Equivalence of kernel scaling and kernel adaptation in limited settings}
Consider a two-layer linear neural network ($f(x) = \sum_c a_c (w^T_c x)$), trained to equilibrium using Langevin dynamics with mean-field scaling. Specifically, we choose the posterior covariance with no data (``the prior'') to have weight variances $\sigma_{\rm a}^2=N^{-1} \chi^{-1},\sigma_{\rm w}^2=d^{-1}$ where $\chi \gg 1$. 

As shown in Ref. \cite{seroussi2023separation} following same approximations (mean-field decoupling between layers, and variational Gaussian approximation on the resulting decoupled layer-wise actions) \footnote{Specifically, Eq. (8) and noting that for a linear network, in that notation, $G(..)=..$, $\frac{\partial [Q_f]_{\mu \nu}}{\partial \Sigma_{ss'}}=[x_{\mu}]_s [x_{\nu}]_{s'}$} the equation for the average discrepancy vector on the training set ($t \in {\rm R}^P$, analogous to $\bar{\tilde{f}}$ in our data-averaged formalism)  is 
\begin{align}
t = \left[[K^{-1} - \frac{\chi}{N} t t^T]^{-1}+(\kappa^2/P) I\right]^{-1} y
\end{align}
This can be understood as the standard GPR equation for the train discrepancy with $[K^{-1} - \frac{\chi}{N} t t^T]^{-1}$ playing the role of the (adapted) kernel. Noting that $w^{*} \cdot x_{\mu}=y_{\mu}$ and that, when we average over data, due to symmetry, $ t \propto y$ and $K^{-1} \propto (x \cdot x')$ on the space of linear functions, we find that this discrete kernel is consistent with Eq. \ref{Eq:AdaptedKernel1}.

Performing a Woodbury identity on the EoS yielding the equivalent form 
\begin{align}
\label{Eq:EoS}
t = \left[K+\frac{\chi}{N}[1-(\chi/N)t^T K t]^{-1} Ktt^TK +(\kappa^2/P) I\right]^{-1} y
\end{align}

Multiplying both sides by $\left[K+\frac{\chi}{N}[1-(\chi/N)t^T K t]^{-1} Ktt^TK +(\kappa^2/P) I\right]$ one finds 
\begin{align}
\left[K+\frac{\chi}{N}[1-(\chi/N)t^T K t]^{-1} Ktt^TK +(\kappa^2/P) I\right] t &= y \\ \nonumber 
K t +\frac{\chi}{N}[1-(\chi/N)t^T K t]^{-1} Ktt^TK t +(\kappa^2/P) t &= y \\ \nonumber 
K t\left[1 +\frac{\chi}{N}[1-(\chi/N)t^T K t]^{-1} t^TK t\right] + (\kappa^2/P) t &= y \\ \nonumber 
K t\left[\frac{1-(\chi/N)t^T K t+\frac{\chi}{N}t^TK t}{1-(\chi/N)t^T Kt}\right] + (\kappa^2/P) t &= y \\ \nonumber
K t\left[\frac{1}{1-(\chi/N)t^T Kt}\right] + (\kappa^2/P) t &= y \\ \nonumber
[\bar{Q} K+(\kappa^2/P)]t &= y \\ \nonumber
t = [\bar{Q} K+(\kappa^2/P)&]^{-1}y \\ \nonumber
\end{align}
where $\bar{Q}=[1-(\chi/N)t^T K t]^{-1}$, hence we obtain the on-data version of the kernel scaling interpretation only without the fluctuation contribution to $\bar{Q}$. Indeed taking a saddle on Eq. (7) of Ref. \citep{ariosto2022statistical} (which agrees with \citep{LiSompolinsky2021} for linear networks) and omitting the $Tr\log$ term associated with GP fluctuations, as it is negligible in the mean-field limit, one arrives at the same result. 

Turning to standard scaling, at least for FCN, this fluctuation correction is non-negligible. Interestingly, including leading saddle point corrections to kernel adaptation, as done in Ref. \cite{rubin2025kernels} accounts for much of its effect. However, while kernel adaptation can be adapted to this FCN + standard scaling setting, it is much more natural to use kernel scaling. Turning to regimes where FCN and CNNs perform better than their GP counterparts, kernel adaptation and its variants which replace the variational Gaussian approximation with a variational Gaussian mixture approximation \citep{rubin2024grokking}-- become more adequate. Still, there is much to be understood in the phase diagram of feature learning. For instance, regimes at which strong channel/channel or width-index/width-index correlations appear as one approaches the underparametrized regime \citep{bricken2023monosemanticity}.    


