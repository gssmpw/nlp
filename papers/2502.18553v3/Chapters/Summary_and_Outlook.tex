\chapter{Summary and Outlook}
Besides being the formalism underlying the standard model, field theory serves a major role in physics as a unifying ground to discuss theories of large scale interacting systems. This has enabled fruitful exchanges between condensed matter, statistical mechanics, and high energy communities \citep{Fradkin_2024}, and allows approximations, and other computational techniques, to be compared and shared via one common language. The goal of this review was to portray the possibility and potential benefits of extending such field-theory language to the deep learning domain. For instance, by showing how the approximation of Ref. \cite{Canatar2021}, derived for GPR, integrates rather seamlessly into feature learning formalisms or how Wilsonian RG can be applied to deep learning. 

Considering the above aim, and due to limitations of space and knowledge, various interesting and impactful approaches to deep learning have been, unfortunately, sidelined. Next, we partially remedy this state of affairs by reviewing and establishing links with some of those. 

\section{Other perspectives}

{\bf NTK and its hierarchy.} Following the conceptual simplicity and rigorous nature of the NTK result \cite{Jacot2018}, several authors have developed $1/N$ expansions \cite{roberts2021principles,huang2019dynamicsdeepneuralnetworks,dyer2019asymptotics} around the NTK result. These track the evolution of the NTK kernel in terms of higher-order tensors associated with neural network output and gradients. They can be considered analogues to the perturbative expansion introduced in Sec. \ref{Sec:PT}, having the capacity (and complexity) to track dynamical effects, conditioned on where one may truncate this expansion. In addition, \cite{roberts2021principles} provide a different application of RG in deep learning, by viewing the recursion relations defining network kernels as forms of a discrete RG transformation.In this context, we also mention several works that study Bayesian and random infinitely wide neural networks of large depth in particular depth which is of the order of the width and found an interesting regime of feature learning associated with the depth \cite{Hanin2023,hanin2024bayesian}.

{\bf Neural network field theory.} The interpretation we followed here, viewing neural networks as field theories, was also taken in the context of ``neural network field theories'' \citep{Halverson2021,Demirtas2023} (see also \cite{naveh2021predictingoutputsfinitedeepV1}) though with an emphasis on the ability of neural network priors (or random DNNs) to act as simulators or regulators of actual field theories one encounters in physics. Specifically, Ref. \cite{Demirtas2023}, provides techniques to engineer random weights or neural network priors to mimic physical actions such as a $\phi^4$ theory.

{\bf One step SGD.} Several authors studied feature learning effects in neural networks after one or several $O(1)$/giant steps of SGD \citep{ba2022highdimensional,damian2022neuralnetworkslearnrepresentations,dandi2023twolayerneuralnetworkslearn}, and found that these can change sample complexity classes, as we also discussed in the context of kernel adaptation. Others used such an approach to deduce the scaling of dataset-sampling-induced noise in representation learning with $P$ \citep{Paccolat_2021}, and use this scaling to rationalize the behavior of learning curves. As discussed in Sec. \ref{Sec:Hyper} the success of these approaches seems to rely on strong correlations between network behavior near initialization and close to equilibrium. 


{\bf Narrow Neural Networks with Gaussian data.} Several authors study networks of the type $g(Wx)$ where $g$ is a non-linear function of an $N$ dimensional vector $Wx$ (also known as a multi-index model for $N>1$), $W$ is a $N \times d$ trainable weight matrix, and $x$ is Gaussian iid data and a similar type of teacher network with weight matrix $W_* \in R^{N_* \times d}$. This approach can capture aspects of deeper neural networks by taking $g$ to be the function generated by a neural network with fixed second layer weights (typically set to 1). Following \citet{saad1995dynamics,biehl1994line, biehl1995learning, saad1995exact} considering online SGD at a suitably defined vanishing learning rate, one finds a deterministic dynamics for the order parameters $Q = W W^T$ and $R=W W_*^T$ which can be calculated explicitly for activation function such as $g=Erf$. This provides an efficient low-dimensional description of the dynamics at large $d$ and $N,N_*=O(1)$ \citep{saad1995dynamics, goldt2019dynamics,wang2019solvable, arnaboldi2023highdimensional,ben2022high,damian2023smoothing,Bruno,dandi2024benefits, collinswoodfin2023hitting}. The same $Q,R$ order parameters can be used in Bayesian inference context leading, via saddle-point on the action of the Bayesian partition function, to a set of non-linear matrix equations on $Q,R$ (see recent review by \citet{cui2024high} and Refs. therein) which can be solved efficiently using Approximate Message Passing algorithms. 

{\bf Distributional Dynamics.} Considering a two-layer neural network of infinite width and finite input dimension, $d$, at mean-field scaling learning can be recast into a PDE governing the probability flow \citep{MeiMeanField2018,sirignano2020mean,rotskoff2022trainability} on the distribution of $a_i,w_i$ (read-out and input weights per neuron) which enjoys some elegant properties. This approach was also seminal in promoting the notion of mean-field scaling which is related to maximum adaptive scaling  \citep{yang2019scaling,yang2022tensorprogramsvtuning}.

%{\bf Dynamics of online SGD. ** OLD**} Several works have analyzed the dynamics of online SGD in which at each iteration one sample a fresh independent sample and shows that it converges to a deterministic limit over a certain class of functions. The dynamic is tractable for shallow DNN in two main regimes: (a) two layer neural network of infinite width and finite input dimension, $d$, see for example, \cite{MeiMeanField2018,chizat2019lazy,sirignano2020mean,rotskoff2022trainability} who derived a PDE limit in parameter space for the SGD process. (b) Two layer neural networks of finite width and input dimensions of the same order as number of samples/iterations. The original work leading this approach was by \cite{saad1995dynamics,biehl1994line, biehl1995learning, saad1995exact}. 
%These works showed that the dynamics of one-pass SGD on multi-index models can be described by a set of ODEs for Gaussian normal data. 
%These results have been rigorously proven and extended to other models under the isotropic Gaussian assumption \cite{goldt2019dynamics,wang2019solvable, arnaboldi2023highdimensional,ben2022high,damian2023smoothing,Bruno,dandi2024benefits}. Recently, there are also extensions to structured data i.e. for data generated by non-identity covariance matrix \cite{CollinsWoodfinPaquette01,balasubramanian2023high,goldt2022gaussian,Yoshida, Goldt,collinswoodfin2023hitting}. 
%Extensions to
%multi-pass SGD with small mini-batches \cite{PPAP01} as well as momentum \cite{LeeChengPaquettePaquette} and adaptive stochastic methods \cite{collins2024high} have also been studied. 
%Several other works also characterized the class of functions and the effect of initialization, which can be learned by SGD introducing the concept of the \textit{information exponent} \cite{ben2022high,damian2023smoothing,Bruno,dandi2024benefits}. 
%Other high-dimensional limits of SGD and gradient descent 
%leading to a different class of more evolved 
%dynamics for shallow neural networks also exist \cite{mignacco2020dynamical,gerbelot2022rigorous,celentano2021highdimensional,chandrasekher2021sharp,sarao2020complex,martin2024impact}, or at the infinite width limit \cite{bordelon2022learning}. 
%These limit though interesting in many setting are harder to analyze analytically, 
% For example, for multi-pass SGD at the proportionate batch limit (when the size of the batch is proportional to the dataset size) \cite{mignacco2020dynamical,gerbelot2022rigorous} as well as gradient descent dynamics \cite{celentano2021highdimensional,chandrasekher2021sharp,bordelon2022learning}. 
%Most of these works utilize a technique known as dynamical mean field theory, which yields a system of integro-differential equations for the covariate of the iterates. 



%BACKMATTER SEE DOCUMENTATION
\backmatter  % references, restarts sample

\printbibliography