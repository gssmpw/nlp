\chapter{Field theory approach to dynamics}
\label{Sec:MSRDJ}
So far, our focus has been on equilibrium/ infinite-training-time/Bayesian aspects of deep learning. In physics, the equilibrium state is typically more universal and tractable than out-of-equilibrium ones. It also serves as a starting point for various near-equilibrium approaches, making it a sensible scientific starting point. Notwithstanding, sometimes one is interested in dynamical properties. Several relevant examples are Grokking \citep{power2022grokking,rubin2024grokking} where the network goes from a memorization phase to a generalization phase after a long, nearly steady-state behaviour or the issue of early stopping, where some networks show better performance at finite training time rather than in their equilibrium state. %or (iii) ReLU sparsification \cite{Cizat}, where specialization of ReLU neurons occurs at very long time scales. 

Conveniently, via field-theoretical approaches to dynamics, the transition from equilibrium to dynamics does not come at a big formal cost. As shown below, much of our formalism carries through to dynamics at the cost of introducing additional time “indices” to equilibrium quantities. One may find field-theoretical approaches to dynamics in the works of \citep{bordelon2022self, Bordelon2023Dynamics, mignacco2020dynamical}. Bordelon et. al. use the MSRDJ formalism to identify, for given preactivation and gradients, an effective time-dependent Gaussian process which characterize the noise due to initialization and the Langevin dynamics noise. They then use this noise distribution in an integral equation governing the preactivations and outputs. This leads to a set of self-consistency equations ensuring that the noise distribution results in the same preactivations it was predicated upon. 
The resulting set of self-consistency equations differs from those derived later in this chapter and characterizes the dynamics in terms of a different and larger set of order parameters. Mignacco et al. focus on networks with a single trainable layer and Gaussian input data, analyzing dynamics under actual stochastic gradient descent (SGD). Our approach diverges by focusing on the stochastic updating process driven by Langevin dynamics, and focusing on the dynamics of more than one trainable layer. %This approach leads us to an integral equation governing network output evolution. 

Below we take the modest task of demonstrating this technique on a two layer network and tracking the transition from NTK-type dynamics to NNGP equilibrium behaviour \citep{avidan2023connectingntknngpunified}. As far as kernel adaptation and feature learning goes, we would only point to similarities with previous Bayesian actions. We further note by passing that the MSRDJ formalism fits perfectly with data averaging as the MSRDJ partition function is constant in the data and hence does not require any introduction of replicas (e.g. \cite{lindner2023theory}).

\section[MSRDJ for one hidden layer]{MSRDJ field theory for a one hidden layer non-linear neural network}
\label{chap:MSR main}
In the following section, we describe the dynamics of a single hidden layer non-linear fully connected network (FCN) using the MSRDJ formalism. Extensions to deeper networks are straightforward. Besides deriving the MSRDJ partition function, we further obtain an integral equation for the dynamics of the mean discrepancy of the network's output in the infinite width limit. We further show how this integral equation recovers as a limiting case both the dynamics governed by the NN-NTK mapping \citep{lee2019wide} and the equilibrium distribution governed by the NNGP mapping \citep{lee2017deep}.

\section{Problem setup}
We consider a two-layer FCN similar to the one defined in Equation \ref{Eq:2_layer_FCN} while keeping track of the training-time dependence of the fields.

\begin{equation}
f(t;x_\mu) =  \sum_{c=1}^{C}\frac{1}{\sqrt{C}} a_{c}(t) \sigma\left(\frac{1}{\sqrt{S}}{w}_c(t) \cdot {x}_{\mu}\right) 
\end{equation}

 In the spirit of kernel adaptation (see Section \ref{Sec:KernelAda}) we consider the FCN to be of width $C$ and the dataset to consists of $\{ {x}_{\mu} \}_{\mu=1}^{P}$ where ${w}_c, {x}_{\mu} \in {\R}^{S}$. In section \ref{Sec:KernelAda}, we scale $a_{ic}$ and $w_c$ such that their variance is of the order of $\frac{1}{NC}$ and $\frac{1}{S}$ respectively. However, unlike section \ref{Sec:KernelAda}, it is preferable here to explicitly scale the network output while taking the variance of $a_c(t)$ and $w_c(t)$ at initialization to be of order of $\frac{1}{\chi}$ and $O(1)$ respectively. This approach facilitates tracking terms that vanish at infinite width. 
We take the loss function
\begin{gather}
    \cL(t) =\cL_{\MSE}(t)+\frac{1}{2}\frac{\kappa^2 }{\sigma_{\rm a}^2 \eta}  a^2(t) +\frac{1}{2}\frac{\kappa^2 }{\sigma_{\rm w}^2\chi\eta} w^2(t)\\
    \cL_{\MSE}(t)=\frac{1}{2} \sum\limits_{\mu=1}^P (f(t;{x}_\mu)-y_\mu)^2
\end{gather}
and consider the time-continuous Langevin dynamics where the network weights evolve under noisy gradient descent with respect to the aforementioned loss:
\begin{gather} \label{dynamics_TB}
    \frac{\dif a_c }{\dif t}=-\eta \partial_{a_c}\cL(t)+{\xi}^{\rm a}_{c}(t) \\  \nonumber
    \frac{\dif w_{ci} }{\dif t}=-\eta \partial_{w_{ci}}\cL(t)+{\xi}^{\rm w}_{ci}(t)
\end{gather}
where $\xi^{\rm a}\in \R^C, \xi^{\rm w}\in \R^{C\times S}$ are some additive Gaussian noise with the following statistics,
\begin{equation}
    \langle \xi^{\rm a}_c \rangle=0, \langle \xi^{\rm a}_{c}(t) \xi^{\rm a}_{c'}(t')\rangle=\frac{\eta\kappa^2}{\chi}\delta_{cc'}\delta(t-t')
\end{equation}
and
\begin{equation}
    \langle \xi^{\rm w}_{ci}\rangle=0,\langle \xi^{\rm w}_{ci}(t) \xi^{\rm w}_{c'i'}(t')\rangle=\frac{\eta\kappa^2}{\chi} \delta_{cc'}\delta_{ii'}\delta(t-t').
\end{equation}
We initialize $a_c(0)\sim\cN(0,\frac{\sigma_{\rm a}^2}{\chi})$ and $w_{ci}(0)\sim\cN(0,\sigma_{\rm w}^2)$ with both $\sigma_{\rm a},\sigma_{\rm w}=O(1)$. %This condition aligns with the equilibrium distribution of the weights when training without data.

% It is worth noting the difference between Langevin dynamics and stochastic gradient descent (SGD). Unlike Langevin, in SGD one chooses uniformly at random a subset of the dataset for each training step, which one can model as adding noise to the gradients calculated on the full dataset. The distribution of this noise may differ from the one realized in the Langevin case. In the work by \citep{mignacco2021effective}, may imply ...
\section{MSRDJ - User interface}
Field theory and stochastic differential equations (SDEs) both examine distributions of functions and, thus, are naturally connected. The MSRDJ formalism links them by interpreting an SDE like $\bm{L}[\phi]=\chi(t)$, where $\bm{L}[\phi]$ is a possibly non-linear differential operator and $\chi(t)$ is white noise, as a functional delta-function. Given the comprehensive reviews available (e.g. \cite{MoritzBook}), we concentrate on offering a minimal ``user interface'', notably on crafting the field theory related to neural network training. To this end, consider an SDE of the form:
\begin{gather} \label{consider this sde}
\frac{\dif \phi(t)}{\dif t} = f(\phi(t)) +\chi(t), \\  \nonumber
\phi(0^+)=\phi_0 \quad \quad \phi_0\sim P_0 \\  \nonumber
\langle\chi(t)\rangle=0\quad\quad\langle\chi(t)\chi(t')\rangle=\delta(t-t')\sigma^2
\end{gather}
where $f$ is a deterministic non-linear function typically related to the loss function, and $P_0$ is the initial distribution of $\phi(t)$. This SDE is framed as the limit $h\rightarrow 0$ of the dynamics on a discrete time lattice $\phi_n=x(t_n)$,  $t_n=n h$ where $n\in\{1,\dots,M\}$ and $t_0 = 0$.
Under the Itô convention, the drift term depends only on the previous state $\phi_{n-1}$, granting the discrete update rule:
\begin{equation}\label{Ito difference equation}
\phi_{n} = \phi_{n-1} + f(\phi_{n-1}) h +\chi_n.
\end{equation}
We can express the joint probability distribution of the set $\{\phi_n\}$, i.e., a discrete path, by taking the average of the product of delta functions, which enforce the update rule at each discrete time step. This average can be written as:
\begin{align}
P[\{ \phi_n \}_{n=1}^M|\phi_0] = &\prod\limits_{n=1}^M  \int_{-\infty}^{\infty} \frac{\dif \chi_n}{\sqrt{2\pi \sigma^2 h}} \exp\left( -\frac{(\chi_n)^2}{2 \sigma^2 h} \right)\int_{-i\infty}^{i\infty} \frac{\dif \tilde{\phi}_n}{2\pi i}\notag\\&\exp\left( \tilde{\phi}_n\left[ \phi_n-\phi_{n-1} - f(\phi_{n-1} )h - \chi_n \right] \right)
\end{align}
then, by integrating out the noise and in the continuum limit (i.e. $h\rightarrow 0$) the probability for a path $\phi(t)$ is
\begin{gather}
P[\phi(t)|\phi(0^+)] = \int \cD_{2\pi i}{\tilde{\phi}}\ \  e^{-S} \notag\\
S=\int\limits_0^t  -\frac{\sigma^2}{2}\tilde{\phi}(t')^2-\tilde{\phi}(t')\left[\partial_{t'}\phi(t')-f(\phi(t'))\right]\dif t'
\end{gather}
where we define $\int\cD_{2\pi i}\tilde{\phi} = \lim\limits_{h\rightarrow 0}\prod\limits_{n=1}^M\int\limits_{-i\infty}^{i\infty}\frac{d\tilde{\phi}_n}{2\pi i}$ and the partition function under MSRDJ becomes
\begin{gather}
    Z=\int \cD{\phi\ \cD_{2\pi i}\tilde{\phi}}\ d\phi(0^+)\ e^{-S+\log(P(\phi(0^+))} \nonumber\\
    S=\int\limits_0^t - \frac{\sigma^2}{2}\tilde{\phi}(t')^2-\tilde{\phi}(t')\left[\partial_{t'}\phi(t')-f(\phi(t'))\right]\dif t'.
\end{gather}
Thus, to apply MSRDJ to an SDE, one transforms $\frac{d\phi(t)}{\dif t} = f(\phi(t)) +\chi(t)$ into $S=\int\limits_0^t - \frac{\sigma^2}{2}\tilde{\phi}(t')^2-\tilde{\phi}(t')\left[\partial_{t'}\phi(t')-f(\phi(t'))\right]\dif t'$.

\section{Condensed Notation}
It is useful to introduce the following notation. Consider $x,y$ as the time vectors, whose entries are $x(t),y(t)$ then we define their dot product as $x^T y=\int_0^t x(t')y(t') \dif t'$. In this convention, the transpose acts on both the time index and the vector index for vectors and matrices (vector index being the index in the normal sense), i.e. $x^Ty = \int_{0}^t \sum\limits_{i}  x_i(t') y_i(t') \dif t'$.
As another shorthand we shall use the Hadamard product $\odot$ to act time-wise, meaning $M(t,s)N(t,s)=\bm{M}\odot\bm{N}$.

\section{MSRDJ partition function}
In condensed notation the MSRDJ partition function corresponding to equations \ref{dynamics_TB} reads as
 
\begin{align}
Z=
\int_{\tilde{w},w,\tilde{a},a} 
e^{-S-S_{\text{initial}}}\notag
\end{align}
Where
\begin{align}
&S=
\sum\limits_c\left(-\frac{\eta\kappa^2}{2\chi}\tilde{a}_{c}^T\tilde{a}_{c} 
- \tilde{a}_{c}^T[\bm{L}_{\rm a} a_{c} 
+ \eta \partial_{a_c} \cL_{MSE}]\right) \\ \nonumber
&+\sum\limits_{ci}\left(- \frac{\eta\kappa^2}{2\chi} \tilde{w}_{ci}^T \tilde{w}_{ci} 
- \tilde{w}_{ci}^T [\bm{L}_{\rm w} w_{ci}+\eta \partial_{w_{ci}} \cL_{MSE}] \right)
\end{align}
and
\begin{equation}
S_{\text{initial}} = \sum\limits_c\frac{\chi}{2\sigma_{\rm a}^2} a_c(0)^2+\sum\limits_{ci}\frac{1}{2\sigma_{\rm w}^2}w_{ci}(0)^2
\end{equation}

We define $\bm{L}_{\rm a}\equiv\partial_t+\frac{\eta \kappa^2}{\sigma_{\rm a}^2}$ and $\bm{L}_{\rm w}\equiv\partial_t+\frac{\eta\kappa^2}{\sigma_{\rm w}^2\chi}$ .

It should be noted that the action resulting from the MSRDJ formalism is highly non-linear as it contains terms such as $\tilde{a}^T\sigma(w\cdot x_\mu)$ with $\sigma$ being a general non-linear function. To elevate some of this difficulty, we draw upon the strategy used in works such as  \cite{Jacot2018}, where significant progress was made by shifting the description of the network from its weights to its output or output discrepancy. The success of this transition suggests that the network output provides a more natural and fundamental representation of the system's dynamics, similar to the use of generalized coordinates in classical mechanics.
Following their example, we adopt a description based on network output discrepancy.

We define $f_\mu(t)=\sum\limits_{c=1}^{C}a_{c}(t) \frac{1}{\sqrt{C}}\sigma( \frac{1}{\sqrt{S}}w_c(t)\cdot x_{\mu})$. We incorporate $f$ into the action, by introducing the identity $1=\prod\limits_{\mu=1}^{P}\int \cD f_\mu \delta(f_\mu(t)-\sum\limits_{c=1}^{C}a_{c}(t) \frac{1}{\sqrt{C}}\sigma( \frac{1}{\sqrt{S}}w_c(t)\cdot x_{\mu}))$, which enforces the definition of $f_\mu(t)$. This has the effect of introducing $\tilde f_\mu(t)$ the auxiliary output field. 
Finally, using $\frac{\partial \cL(t)}{\partial f(t;x_\mu)}=f_\mu(t)-y_\mu$ and applying the chain rule we obtain:

\begin{align}    
&Z=
\int_{\tilde{w},w,\tilde{a},a,\tilde{f},f} 
e^{-S-S_{\text{initial}}}  \\ \nonumber
&S=
-\frac{\eta\kappa^2}{2\chi}\sum\limits_{c}\tilde{a}_{c}^T\tilde{a}_{c} 
-\sum\limits_{c}\tilde{a}_{c}^T\bm{L}_{\rm a} a_{c}
-\frac{\eta\kappa^2}{2\chi} \sum\limits_{c,i}\tilde{w}_{ci}^T \tilde{w}_{ci} -\sum\limits_{c,i} \tilde{w}_{ci}^T \bm{L}_{\rm w} w_{ci}\\ \nonumber
&-\sum\limits_{c,\mu}\tilde{a}_{c}^T[\eta f_{\mu} \frac{1}{\sqrt{C}}\sigma(\frac{1}{\sqrt{S}}w_{c}\cdot x_{\mu})-\eta y_{\mu}\frac{1}{\sqrt{C}}\sigma(\frac{1}{\sqrt{S}}w_{c}\cdot x_{\mu})] \\ \nonumber
&-\sum\limits_{c,i,\mu}\tilde{w}_{ci}^T [\eta f_{\mu} a_{c} \frac{1}{\sqrt{CS}}\sigma'(\frac{1}{\sqrt{S}}  w_{ci} x_{\mu;i}) x_{\mu;i} -\eta y_{\mu} a_{c} \frac{1}{\sqrt{CS}}\sigma'(\frac{1}{\sqrt{S}} w_{ci} x_{\mu;i}) x_{\mu;i}]\\ \nonumber
&-\sum\limits_{\mu}\tilde{f}_{\mu}^T[f_{\mu}- \sum\limits_{c}a_{c} \frac{1}{\sqrt{C}}\sigma( \frac{1}{\sqrt{S}}w_c\cdot x_{\mu})]
\end{align}
This action is quadratic in the fields $\tilde{a},a$, which allows us to integrate out these fields. The term $\exp(- \sum\limits_c\frac{\chi}{2\sigma_{\rm a}^2} a_c(0)^2)$ induces an initial condition on the time correlation (propagator) of $a_c$, thus determining it uniquely:
$a_{c}\sim \cN(\bar{a}_{c},\bm{\Sigma})$ where $\bar{a}_{c}(t)=-\eta \int\limits_0^t   e^{-\frac{\eta\kappa^2}{\sigma_{\rm a}^2}(t-t')} \sum\limits_{\nu=1}^{P} \frac{1}{\sqrt{CS}}\sigma(\frac{1}{\sqrt{S}}w_{c}(t') \cdot x_{\nu}) (f_{\nu}(t')-y_\nu) \dif t'$ and $\bm{\Sigma}(t,t')=\frac{\sigma_{\rm a}^2}{2\chi} e^{-\frac{\eta\kappa^2}{\sigma_{\rm a}^2}|t-t'|}$.
Leaving us with the following expression for the MSRDJ partition function

\begin{align}\label{Eq:S_f_w_MSRDJ}
    &Z=
    \int_{\tilde{w},w,\tilde{f},f}e^{-S-\sum\limits_{ci}\frac{1}{2\sigma_{\rm w}^2}w_{ci}(0)^2} \\ \nonumber
    &S=-\frac{\eta\kappa^2}{2\chi}\sum\limits_{c,i} \tilde{w}_{ci}^T \tilde{w}_{ci} -\sum\limits_{c,i} \tilde{w}_{ci}^T \bm{L}_{\rm w} w_{ci} \\ \nonumber
   & - \sum\limits_{c,i,\mu}\tilde{w}_{ci}^T [\eta (f_{\mu}-y_\mu) \bar{a}_{c} \frac{1}{\sqrt{CS}}\sigma'(\frac{1}{\sqrt{S}} w_{ci} x_{\mu;i}) x_{\mu;i}] \\ \nonumber
    &-\sum\limits_{c,\mu}\tilde{f}_{\mu}^T[f_{\mu}-\bar{a}_c \frac{1}{\sqrt{C}}{\sigma}(\frac{1}{\sqrt{S}} w_c\cdot x_{\mu})] \\ \nonumber
    &-\sum\limits_{c,\mu,\nu}\Bigg[-\tilde{f}_{\mu}^T
  \frac{1}{\sqrt{C}}\sigma^T(\frac{1}{\sqrt{S}} w_c\cdot x_{\mu})
+\\ \nonumber&
\sum\limits_{i}\tilde{w}_{ci}^T
\eta (f_{\mu}-y_\mu)^T \frac{1}{\sqrt{CS}}\sigma'^T(\frac{1}{\sqrt{S}} w_{ci} x_{\mu;i}) x_{\mu;i}\Bigg] 
\bm{\Sigma}\\ \nonumber
&\Bigg[-  \frac{1}{\sqrt{C}}\sigma(\frac{1}{\sqrt{S}} w_c\cdot x_{\nu}) \tilde{f}_{\nu}
+
\sum\limits_{j}\tilde{w}_{cj}
\eta  \frac{1}{\sqrt{CS}} \sigma'( \frac{1}{\sqrt{S}} w_{cj} x_{\nu;j}) (f_{\nu}-y_\nu) x_{\nu;j}\Bigg] 
\end{align}
At this juncture, we draw the reader's attention to the resemblances between Equation \ref{Eq:S_FCN2_nonEK} and Equation \ref{Eq:S_f_w_MSRDJ}. Both equations encompass terms like $\sum_\mu(\tilde{f}_\mu\sigma(w_c\cdot x_\mu))^2$ or, alternatively, in the limit where sampling the discrete dataset transitions to integrating over the data measure, expressed as $\int d\mu_x(\tilde{f}(x)\sigma(w_c\cdot x))^2$. However, the key distinction lies in the presence of differential operators in Equation \ref{Eq:S_f_w_MSRDJ}, which act on the temporal indices of $f$ and $w$.
\section{Infinite width limit}
Our next goal is to study the infinite width limit of the above action and retrieve the NTK and NNGP behavior. At this point, we turn to two key approximations, enabling the integration of the $\tilde{w},w$ fields.
The first comes by when focusing on the following part of the action.
$S_{\rm w}=-\frac{\eta\kappa^2}{2\chi} \tilde{w}_{ci}^T \tilde{w}_{ci} 
    - \tilde{w}_{ci}^T [\bm{L}_{\rm w} w_{ci}+\eta (f_{\mu}-y_\mu) \bar{a}_{c} \frac{1}{\sqrt{CS}}\sigma'(\frac{1}{\sqrt{S}} w_{ci} x_{\mu;i}) x_{\mu;i}] -\sum\limits_{ci}\frac{1}{2\sigma_{\rm w}^2}w_{ci}(0)^2$
which is composed of two contributions to the evolution of the $w$ field, one $\tilde{w}_{ci}^T\bm{L}_{\rm w} w_{ci}$ and the other $\tilde{w}_{ci}^T\eta (f_{\mu}-y_\mu) \bar{a}_{c} \frac{1}{\sqrt{CS}} \sigma'(\frac{1}{\sqrt{S}} w_{ci} x_{\mu;i}) x_{\mu;i}$. In this second contribution, the field $\tilde{w}_{ci}$ experiences interactions that are suppressed by $\frac{1}{C}$ which is apparent by $\bar{a}_c\sim\frac{1}{\sqrt{C}}$, which in the infinite width limit become negligible.
This simplification leaves us with:
\begin{equation}
    S_{\rm w}=\sum\limits_{c,i}\left(\frac{\eta\kappa^2}{2\chi} \tilde{w}_{ci}^T \tilde{w}_{ci} + \tilde{w}_{ci}^T \bm{L}_{\rm w} w_{ci}-\frac{1}{2\sigma_{\rm w}^2}w_{ci}(0)^2\right)
\end{equation}



And the partition function
\begin{align}
    &Z=
\left\langle
    \int_{\tilde{f},f}
    e^{S_f} 
    \right\rangle_{S_{\rm w}}
   \\ \nonumber
   &S_f=\sum\limits_{\mu}\tilde{f}_{\mu}^T[f_{\mu}-\frac{1}{\sqrt{C}} \bar{a}_c\cdot \sigma(\frac{1}{\sqrt{S}} w_c\cdot x_{\mu})] 
   \\ \nonumber&+\!\sum\limits_{c,\mu,\nu}\left[\!-\tilde{f}_{\mu}^T
\frac{1}{\sqrt{C}}  \sigma^T(\frac{1}{\sqrt{S}} w_c\cdot x_{\mu})
+
\sum\limits_{i}\tilde{w}_{ci}^T
\eta (f_{\mu}-y_\mu)^T\frac{1}{\sqrt{CS}} \sigma'^T(\frac{1}{\sqrt{S}} w_{ci} x_{\mu;i}) x_{\mu;i}\!\right] 
\\ \nonumber &\bm{\Sigma}
\left[-\frac{1}{\sqrt{C}}  \sigma(\frac{1}{\sqrt{S}} w_c\cdot x_{\nu}) \tilde{f}_{\nu}
+
\sum\limits_{j}\tilde{w}_{cj}
\eta  \frac{1}{\sqrt{CS}} \sigma'(\frac{1}{\sqrt{S}} w_{cj} x_{\nu;j}) (f_{\nu}-y_\nu) x_{\nu}\right]
\end{align}
Here, the second approximation arises. We apply cumulant expansion for $S_f$ under $e^{-S_{\rm w}}$, keeping order zero terms in $\frac{1}{C}$, we obtain the following:
\begin{gather}
Z=\int_{\tilde{f},f}
    e^{-S_{f}}
    \\ \nonumber
    S_{f}=\sum\limits_{\mu,\nu}\bigg(\!-\tilde{f}_{\mu}^T[f_{\mu}+
    \eta(\bm{L}^{-1}_{\rm a}\odot\bm{\Phi}(x_\mu,x_\nu))f_{\nu}-\eta(\bm{L}^{-1}_{\rm a}\odot\bm{\Phi}(x_\mu,x_\nu))y_{\nu}]\bigg.\\ \nonumber
     -\eta (f_{\mu}-y_\mu)^T(\bm{\Sigma}_{\rm a}\odot\bm{L}^{-1}_{\rm w}\odot\bm{\Phi}'(x_\mu,x_\nu) K^x({x_\mu,x_\nu})) \tilde{f}_{\nu}
     \\ \nonumber\bigg.\!- \tilde{f}_{\mu}^T (\bm{\Sigma}_{\rm a}\odot\bm{\Phi}(x_\mu,x_\nu)) \tilde{f}_{\nu}\bigg)
\end{gather}
where
\begin{equation}
    \bm{\Phi}(t,t';x_\mu,x_\nu)=\langle\sum\limits_{c,i,j}\frac{1}{C}\sigma(\frac{1}{\sqrt{S}}w_{ci}(t) x_{i\mu})\sigma(\frac{1}{\sqrt{S}}w_{cj}(t)x_{j\nu})\rangle_{S_{\rm w}}
\end{equation}
and
\begin{equation}
\bm{\Phi}'(t,t';x_\mu,x_\nu)=\langle\sum\limits_{c,i,j}\frac{1}{CS}\sigma'(\frac{1}{\sqrt{S}}w_{ci}(t) x_{i\mu})\sigma'(\frac{1}{\sqrt{S}}w_{cj}(t)x_{j\nu})\rangle_{S_{\rm w}}    
\end{equation}

The above action is quadratic in $\tilde{f},f$ , allowing us to take the average of $f_\mu(t)$, resulting in the following integral equation for the average of the network's output (mean predictor)
\begin{align} \label{delta=-y-int}
  &f_{\mu}(t)=-\int\limits_0^t \sum\limits_{\nu=1}^{P} \eta\left(  e^{-\frac{\eta\kappa^2}{\sigma_{\rm a}^2}(t-t')}\bm{\Phi}(t,t';x_\mu,x_\nu)\right. \\ \nonumber
  &\left.+ \frac{\sigma_{\rm a}^2}{2\chi}e^{-\frac{\eta\kappa^2}{\sigma_{\rm a}^2}|t-t'|}e^{-\frac{\eta\kappa^2}{\sigma_{\rm w}^2\chi}(t-t')} (\bm{\Phi}'(t,t';x_\mu,x_\nu)  K^x({x_\mu,x_\nu}))\right)
  (f_{\nu}(t')-y_\nu) \dif t' 
\end{align}
Where $K^x(x_\mu, x_\nu)=x_\mu\cdot x_\nu$.
In our upcoming discussion, we will demonstrate how Equation \ref{delta=-y-int} serves as an interpolation between the Neural Tangent Kernel (NTK) behavior during initial time periods and the Neural Network Gaussian Process (NNGP) behavior as it approaches equilibrium. At this juncture, it is pertinent to mention that the work by \cite{avidan2023connectingntknngpunified} in which they derive an integral equation for the average predictor by employing a Markov proximal approach.
They show that in the vanishing learning rate limit, their approach converges to gradient descent + noise. Their result for the mean predictor converges to the expression above at the vanishing learning rate limit.
\clearpage
\section{Recovering NTK dynamics}
To recover the NTK description for the neural network's output, we take the $t$ derivative of Equation \ref{delta=-y-int}.
 \begin{align}
       &\dot{f}_\mu(t)=-\sum\limits_{\nu}\eta\left( \bm{\Phi}(t,t;x_\mu,x_\nu)+\frac{\sigma_{\rm a}^2}{2\chi}(\bm{\Phi}'(t,t;x_\mu,x_\nu) K^x(x_\mu,x_\nu))\right) (f_\nu(t)-y_\nu)\notag\\ 
       & -\sum\limits_{\nu}\!\int\limits_0^t\left(
       {\eta^2}\frac{\eta\kappa^2}{\sigma_{\rm a}^2} e^{-\frac{\kappa^2}{\sigma_{\rm a}^2}(t-t')}\bm{\Phi}(t,t';x_\mu,x_\nu)+ \right.\notag\\
       &\left. (\frac{\kappa^2}{\sigma_{\rm a}^2}+\frac{\kappa^2}{\sigma_{\rm w}^2\chi}){\eta^2} \frac{\sigma_{\rm a}^2}{2\chi} e^{-\frac{\eta\kappa^2}{\sigma_{\rm a}^2}|t-t'|} e^{-\frac{\eta\kappa^2}{\sigma_{\rm w}^2\chi}(t-t')}(\bm{\Phi}'(t,t';x_\mu,x_\nu)  K^x(x_\mu,x_\nu))
       \right) \notag\\
       &\phantom{\left. (\frac{\kappa^2}{\sigma_{\rm a}^2}+\frac{\kappa^2}{\sigma_{\rm w}^2\chi}){\eta^2} \frac{\sigma_{\rm a}^2}{2\chi} e^{-\frac{\eta\kappa^2}{\sigma_{\rm a}^2}|t-t'|} e^{-\frac{\kappa^2}{\sigma_{\rm w}^2\chi}(t-t')}dsgoirhs\right)}(f_\nu(t')-y_\nu) \dif t'\notag\\
       &-\sum\limits_{\nu}\!\int\limits_0^t\left(
       {\eta^2} e^{-\frac{\eta\kappa^2}{\sigma_{\rm a}^2}(t-t')}(\bm{\Phi}'(t,t';x_\mu,x_\nu) K^x(x_\mu,x_\nu))\frac{\kappa^2}{2\chi} e^{-\frac{\eta\kappa^2}{\sigma_{\rm w}^2\chi}|t-t'|}
       \right.\notag\\
       &+{\eta^2}\frac{\sigma_{\rm a}^2}{2\chi}e^{-\frac{\eta\kappa^2}{\sigma_{\rm a}^2}|t-t'|}e^{-\frac{\eta\kappa^2}{\sigma_{\rm w}^2\chi}(t-t')}(\bm{\Phi}''(t,t';x_\mu,x_\nu) K^x(x_\mu,x_\nu) K^x(x_\mu,x_\nu))\notag\\
       &\phantom{+{\eta}\frac{\sigma_{\rm a}^2}{2\chi}e^{-\frac{\eta\kappa^2}{\sigma_{\rm a}^2}|t-t'|}e^{-\frac{\kappa^2}{\sigma_{\rm w}^2\chi}(t-t')}asga}\left.\frac{\kappa^2}{2\chi} e^{-\frac{\eta\kappa^2}{\sigma_{\rm w}^2\chi}|t-t'|}
       \right)(f_\nu(t')-y_\nu) \dif t' 
    \end{align}
it is now apparent that either by using $\int\limits_0^t g(t') \dif t'=t\ g(0)+\mathcal{O}(t)$ under $t\ll\frac{\eta\kappa^2}{\sigma_m^2}$ where we take $\sigma_m=\min(\sigma_{\rm a},\sigma_{\rm w})$ or under the limit $T\rightarrow0$ the expression above reduces to:
\begin{equation}
    \dot{f}_\mu(t)=-\sum\limits_{\nu}\eta\left(\bm{\Phi}(t,t;x_\mu,x_\nu)+\bm{\Phi}'(t,t;x_\mu,x_\nu) K^x(x_\mu,x_\nu) \right) (f_\nu(t)-y_\nu)
\end{equation}
however, since the time dependence of $\bm{\Phi}(t,t'),\bm{\Phi}'(t,t')$ comes through as a function of the time correlate $\bm{\Sigma}(t,t')$ which has the properties $\bm{\Sigma}(t,t)=\bm{\Sigma}(0,0)$ both $\bm{\Phi}(t,t)=\bm{\Phi}(0,0)$ and $\bm{\Phi}'(t,t)=\bm{\Phi}'(0,0)$
finally
\begin{gather}
    \bm{\Phi}(0,0;x_\mu,x_\nu)+ \bm{\Phi}'(0,0;x_\mu,x_\nu) K^x(x_\mu,x_\nu)=\notag\\\left\langle\bm{\nabla}_a f(0,x_\mu)\cdot\bm{\nabla}_af(0,x_\nu)+\bm{\nabla}_w f(0,x_\mu)\cdot\bm{\nabla}_w f(0,x_\nu)\right\rangle_{S_{\rm w}}=\Theta_0
\end{gather}
This is $\langle\sum_{\alpha} \partial_{\theta_{\alpha}}f_0(x_{\mu}) \partial_{\theta_{\alpha}}f_0(x_{\nu})\rangle_{S_{\rm w}}$ namely the Neural Tangent Kernel (NTK) in the infinite width limit averaged under random initializations, as it was defined in Equation \ref{Eq:NTK}. 
These, together with the initial condition $f_{\mu}(0)=-y_{\mu}$ gotten by setting $t=0$ in Equation \ref{delta=-y-int}, result in the same dynamical description given by the NN-NTK mapping.

\section{Equilibrium and the NNGP limit}\label{Result 3}
This section recovers the mean of the network output at equilibrium as predicted by the NN-NNGP mapping.
Consider Equation \ref{delta=-y-int}, and rewrite  $e^{-\frac{\eta\kappa^2}{\sigma_{\rm a}^2}(t-t')}$ in the first term under the integral as $\frac{\sigma_{\rm a}^2}{\eta\kappa^2}\partial_{t'}[ e^{-\frac{\eta\kappa^2}{\sigma_{\rm a}^2}(t-t')}]$ then integrating by parts. Finally setting $\sigma=\sigma_{\rm a}=\sigma_{\rm w}$ leaves us with:
\begin{align}
  f_{\mu}(t)=&-\left.\frac{\sigma^2}{\kappa^2} e^{-\frac{\eta\kappa^2}{\sigma^2}(t-t')}\sum\limits_{\nu}\! \bm{\Phi}(t,t';x_\mu,x_\nu) (f_{\nu}(t')-y_{\nu}) \right|_{t'=0}^{t}\notag\\&+\sum\limits_{\nu}\!\int\limits_0^t\!  \left(\frac{\sigma^2}{\kappa^2}  e^{-\frac{\eta\kappa^2}{\sigma^2}(t-t')} \bm{\Phi}(t,t';x_\mu,x_\nu)\dot{f}_{\nu}(t') \right) \dif t' \label{the discrepancy equation}
\end{align}
at the limit $t\rightarrow\infty$ the term
\begin{equation} \label{eq:integral term}
    I=\int\limits_0^t  \left( \frac{\sigma^2}{\kappa^2}  e^{-\frac{\eta\kappa^2}{\sigma^2}(t-t')} \sum\limits_{\nu}\!\bm{\Phi}(t,t';x_\mu,x_\nu)\dot{f}_{\nu}(t') \right) \dif t'
\end{equation}
vanishes.
Leaving us with
\begin{equation}
    f_{\GP}(x_\sigma)=\sum\limits_{\mu,\nu}K^{\GP}_{\sigma,\mu}
    [(K^{\GP}+\kappa^2 I)^{-1}]_{\mu,\nu}
    y_\nu
\end{equation}
where $K^{\GP}_{\mu,\nu}=\lim\limits_{t\rightarrow\infty}\sigma^2\bm{\Phi}(t,t;x_\mu,x_\nu)$ and $ f_{\GP}(x_\sigma)=\lim\limits_{t\rightarrow\infty} f_\sigma(t)$ this is Equation \ref{Eq:GPR} for the mean predictor.
