\chapter{Infinite networks in standard scaling}

One of the surprising empirical findings about neural networks is the fact that increasing the number of DNN parameters has, typically, a beneficial effect on performance. Historically, this was noted as soon as 1995 \citep{Breiman2018ReflectionsAR}, but made more notable following the recent rise of DNNs by \cite{Neyshabur2014,Zhang2016}. Namely, despite having orders of magnitude more parameters than training points to fit, DNNs generalize (fit/predict previously unseen data points) quite well. This stands in contrast to the intuition that such DNN would find one random weight configuration out of many to fit the training data and, as each such configuration is expected to give a different predictor, what would be the chance that this random choice ended up the right one? 

This discussion motivates us to consider infinitely over-parametrized DNNs, such as those discussed in Sec. \ref{Sec:InfiniteRandomDNNs}, as these embody the above issue most clearly. In addition, one may hope this limit would also describe a well-performing regime of DNNs. However, as we shall see, this description often overshoots the best-performing regime which occurs at finite overparametrization (see 
 \cite{novak2018bayesian} for an empirical comparison)\footnote{Or at infinite overparametrization in so-called mean-field scaling \cite{yang2022tensorprogramsvtuning,MeiMeanField2018}}. Nevertheless, the description obtained in this limit has three advantages: {\bf (i)} It is simple and insightful, {\bf (ii)} It correlates with actual network performance \citep{novak2018bayesian,lee2019wide} thereby allowing us to rationalize architecture choices, {\bf (iii)} It serves as an analytical starting point for studying more advanced, finitely over-parametrized regimes.

\section{The DNN-NNGP mapping for trained networks}
\label{Sec:NNGP_On_Data}
Consider a DNN trained for infinitely long using the Langevin dynamics discussed in Sec. \ref{Sec:TrainingProtocols}. Let us first focus on DNN outputs ($z(x)$) on $P$ training points ($x_1...x_P$) and one test point ($x_0$). Furthermore, for simplicity, we focus on scalar outputs. Consider the partition function associated with the equilibrium distribution of the weights found in Eq. (\ref{Eq:BoltzmannDistWeights}) (i.e. $P(\theta) \propto e^{-L(\theta)/T}$). As common in physics, instead of working with the probability (which implies carrying normalization factors and the dummy variable $\theta$), we work with its normalization factor also known as the partition function 
\begin{align}
Z &= \int d\theta e^{-L(\theta)/T}
\end{align}
where we use the following  unnormalized-MSE+weight-decay loss 
\begin{align}
L(\theta) &= \frac{1}{2}\sum_{\mu=1}^P (z_{\theta}(x_{\mu})-y(x_{\mu}))^2 + T \sum_{\alpha} \frac{\theta^2_{\alpha}}{2\sigma^2_{\alpha}}
\end{align}
and allowed different weight decay factors for each weight, to be determined soon. We often denote 
\begin{align}
\kappa^2 &= T
\end{align}
where $\kappa$ is referred to as the ridge parameter. 

We comment that the above distribution over network parameters has the following Bayesian interpretation. Consider the statistical model  $P((x,y)|\theta) \propto e^{-(z_{\theta}(x)-y)^2/(2\kappa^2)}$, with $\theta$ being the model parameters. This model can be interpreted as a neural network with an output corrupted by Gaussian noise of std $\kappa$. The distribution in Eq. \ref{Eq:BoltzmannDistWeights} coincides with the Bayesian posterior of the above model given a Gaussian iid prior given by $\theta_{\alpha} \sim {\cal N}(0,\sigma^2_{\alpha})$. This is a particular link between the physical notion of equilibrium and Bayesian inference. 

Next, we wish to simplify the above distribution. To this end, we note that the complex dependence on $\theta$ enters only through the network outputs one may write 
\begin{align}
\label{Eq:Derivation1}
Z &= \int df_0..df_P \int d\theta \Pi_{\mu=0}^P \; \delta(f_{\mu}-z_{\theta}(x_{\mu}))  e^{-L(\theta)/T} \\ \nonumber 
&= \int df_0..df_P \; e^{-\frac{1}{2T} \sum_{\mu=1}^P (f_{\mu}-y(x_{\mu}))^2}\int d\theta \; e^{-\sum_{\alpha} \frac{\theta^2_{\alpha}}{2\sigma^2_{\alpha}}} \Pi_{\mu=0}^P \; \delta(f_{\mu}-z_{\theta}(x_{\mu}))  
\end{align}
where we denote by $f_\mu = f(x_\mu)$. Viewing the last term as a function of $f_0..f_P$, one finds that apart from an inconsequential normalization factor, it is really the definition of the probability density induced on $f_0..f_P$ by a DNN with random Gaussian weights. Denoting this probability by $P_0(f_0..f_P)$ we thus find 
\begin{align}
Z &= \int df_0..df_P e^{-\frac{1}{2T} \sum_{\mu=1}^P (f_{\mu}-y(x_{\mu}))^2} P_0(f_0..f_P)
\end{align}
we comment that, reading off a probability density for $f_0..f_P$ by removing the integrals and normalizing by $Z$, one obtains the formula for the Bayesian posterior with additive Gaussian measurement noise and $P_0$ as a prior in function space  \citep{Welling2011,cohen2021learning}. Thus, our results carry through to Bayesian Inference and Bayesian neural networks.

So far our manipulations have been exact. Taking the limit of infinite overparametrization, as discussed in Sec. \ref{Sec:InfiniteRandomDNNs}, we found that the outputs of the DNN are Gaussian. Hence, in this limit $P_0$ simplifies to a centered Gaussian distribution with covariance matrix $K(x_{\mu},x_{\nu})$. Denote this $(P+1) \times (P+1)$ matrix simply by $K$, we find the following simple result in the standard-over-parametrized (SOP) limit 
\begin{align}\label{eq:S_SOP}
Z_{\text{SOP}} &= \int df_0..df_P \; e^{-\frac{1}{2T} \sum_{\mu=1}^P (f_{\mu}-y(x_{\mu}))^2} e^{-\frac{1}{2} \sum_{\mu \nu=0}^P f_{\mu} [K^{-1}]_{\mu \nu} f_{\nu}}
\end{align}
Namely a Gaussian partition function. 

Calculating observables under this partition function, using block matrix inversion lemmas, yields the well-known Gaussian Process Regression (GPR) formulas \citep{Rasmussen2005},  namely 
\begin{eqnarray}
\label{Eq:GPR} \\ \nonumber
&\langle f(x_0) \rangle_{Z_{\text{SOP}}} &= \sum_{\mu \nu=1}^P K(x_0,x_{\nu}) [(K_D+TI)^{-1}]_{\nu \mu} y(x_{\mu}) \\ \nonumber 
&\langle f^2(x_0) \rangle_{Z_{\text{SOP}}} - \langle f(x_0) \rangle^2_{Z_{\text{SOP}}} &= K(x_0,x_0) \\ \nonumber
& & - \sum_{\mu \nu=1}^P K(x_0,x_{\nu}) [(K_D+T I)^{-1}]_{\nu \mu} K(x_{\mu},x_0) 
\end{eqnarray}
where $K_D$ is the $P \times P$ matrix with elements $[K_D]_{\mu>0,\nu>0}=K(x_{\mu},x_{\nu})$. 

Despite the difficulty of inverting $K_D$, which may be extremely large in practice, the above formula provides concrete predictions for the average output of a trained DNN and its fluctuation under the ensemble of DNN induced by the Langevin dynamics. Experimentally, the average value can be obtained by training many networks using different realizations of the gradient noise and initial conditions, and averaging the output over this set of DNNs. Such a technique (ensembling) is used in practice for small datasets when training is not too costly. 

The above result also sheds light on why over-parametrization is, to the very least, not strictly detrimental to performance. Indeed in this limit, we have obtained GPR--- a known and respectful way of doing inference. One can still argue that the strong weight decay we required, coming from the fact that  $\sigma^2_{\alpha}$ scales one over fan-in, is sufficiently restrictive to counter the addition of parameters. Here one should note that having weight decay such that weights are limited to a scale of $1/\sqrt{N}$ is not very restrictive on the size of the resulting functions. Indeed if such weights are summed coherently, they can still reach a diverging magnitude of $\sqrt{N}$. However, to counter this most sharply, together with potential concerns about using fully equilibrated DNNs, we next discuss a way of obtaining a similar GPR result without using weight decay or assuming equilibrium, via the celebrated Neural Tangent Kernel result \citep{jacot2018neural}. 

\section{The DNN-NTK mapping for trained networks}
Consider next a DNN trained with Gradient Flow and unnormalized MSE loss and no weight decay. Following Refs. \cite{Jacot2018,lee2019wide} let us study the dynamics on the outputs $f(x_0)..f(x_P)$ induced by the dynamics on the weights via 
\begin{align}
\frac{\dif f_t(x_{\mu})}{\dif t} &= \sum_{\alpha} \partial_{\theta_{\alpha,t}}f(x_{\mu}) \frac{\dif \theta_{\alpha,t}}{\dif t} \\ \nonumber 
\frac{\dif \theta_{\alpha,t}}{\dif t} &= - \gamma \partial_{\theta_{\alpha}} L = -  \gamma  \sum_{\nu=1}^P [f_t(x_{\nu})-y(x_{\nu})] \partial_{\theta_{\alpha}} f_t(x_{\nu})
\end{align}
where we have kept the dependence of $f_t(x_{\nu})$ on the weights implicit. Note that the second equation defines Gradient Flow with the said loss. Joining these two equations and defining the time-dependent Neural Tangent Kernel (NTK) $\Theta_t(x_\mu, x_\nu)$, we obtain 
\begin{align}\label{Eq:NTK}
\frac{\dif f_t(x_{\mu})}{\dif t} &= -  \gamma \sum_{\nu=1}^P \Theta_t(x_{\mu},x_{\nu}) [f_t(x_{\nu})-y(x_{\nu})]  \\ \nonumber 
\Theta_t(x_{\mu},x_{\nu}) &= \sum_{\alpha} \partial_{\theta_{\alpha}}f_t(x_{\mu}) \partial_{\theta_{\alpha}}f_t(x_{\nu})
\end{align}

So far our manipulations have been exact, the important insight is that $\Theta_t(x_{\mu},x_{\nu})$, the NTK, becomes independent of time and independent of initialization at infinite over-parametrization. This surprising result is related to the aforementioned possibility of generating diverging functions using coherent sums of weights. More accurately, minor but coherent $1/N$ (i.e. one over width) changes to the $1/\sqrt{N}$-sized random weights are sufficient to generate $O(1)$ functions. Given such vanishingly small changes throughout training, it is reasonable to expect that a leading order Taylor expansion of $f(x_{\mu};\theta_t)$ around the initial $\theta_0$ value is enough \citep{lee2019wide}. If so we have that throughout training  $f_{t}(x_{\mu})\approx f_{0} (x_{\mu})+\sum_{\alpha}(\theta_{t,\alpha}-\theta_{0,\alpha})\partial_{\theta_{\alpha}}f_{0}(x_{\mu})$. Plugging this into the above definition of $\Theta_t(x_{\mu},x_{\nu})$ and noting that all $\theta_{\alpha}$ dependence of $f_t(x_{\mu})$ now comes from the $\theta_{\alpha,t}$ factor, yields a time-independent NTK evaluated at the initial weights which we denote by $\Theta(x_{\mu},x_{\nu})$. Having $N \rightarrow \infty$ equivalent parameters being summed over, it is also natural to expect this quantity to concentrate/self-average and become independent of the particular draw of initial weights. These are all, of course, heuristic arguments and proofs can be found in Ref. \cite{jacot2018neural}. 

Given fixed $\Theta(x_{\mu},x_{\nu})$, the dynamics is now governed by a linear ODE in $P+1$ variables whose solution is  
\begin{align}
f_{t}(x_{\mu>0}) &= \sum_{\nu=1}^P [e^{-\gamma \Theta_D t}]_{\mu \nu}
 [y(x_{\nu})-f_{0}(x_{\nu})]
\end{align}
where $\Theta_D$ is the matrix obtained by taking the NTK only between training points. Turning to the test point ($x_0$) we can place the above solution within the differential equation to obtain
\begin{align}
\partial_t f_{t}(x_0) &= -\gamma \sum_{\mu=1}^P \Theta(x_0,x_{\mu}) \left[\sum_{\nu=1}^P [e^{-\gamma \Theta_D t}]_{\mu \nu}
 [y(x_{\nu})-f_{0}(x_{\nu})]-y(x_{\mu})\right] 
\end{align}
Integrating both sides from $t=0$ to $\infty$ one obtains the outputs after training infinitely long on a test point $x_0$, namely
\begin{align}
\label{Eq:NTKInfiniteTime}
f_{\infty}(x_0) &= \sum_{\mu \nu=1}^P \Theta(x_0,x_{\mu}) [\Theta_D^{-1}]_{\mu \nu} y(x_{\nu}) + I_0 \\ \nonumber
I_0 &= f_0(0)-\sum_{\mu \nu=1}^P \Theta(x_0,x_{\mu}) [\Theta_D^{-1}]_{\mu \nu} f_{0}(x_{\nu})
\end{align}
Ignoring for the moment $I_0$, the first line appears as GPR with zero ridge ($\kappa^2=0$, ridgeless) using however the NTK kernel ($\Theta$) rather than $K$, the NNGP kernel. The second line then appears as the discrepancy in using such ridgeless GPR in predicting the test point's initial value given the train points' initial values. Notably averaging over an ensemble of initialization seeds, the $I_0$ contribution vanishes and one is left with the first line alone. 

Thus, the NTK approach leads to GPR formula for the predictor/average. We thus obtain a similar result to the DNN-NNGP correspondence described in the previous chapter. Nonetheless, the variance contribution (related to moments of $I_0$) differs from that in the previous section. We note by passing that GPR with the NTK kernel is very well suited to predict functions generated at random by the DNN with that NTK. Hence, for complex target functions requiring large $P$, we generally expect the $I_0$ contribution to be quite small.  

\section{Field theory of Gaussian Process Regression (GPR) - on data}
We next wish to rephrase some of the above GPR results as a field theory. Even putting aside the powerful machinery offered by field theory, a more concrete motivation here is to avoid the spurious dependence on data of the functional prior term in the action in Eq. \eqref{eq:S_SOP}. Such a dependence obfuscates the natural symmetries of the action and makes the process we would soon undertake -- averaging over datasets, cumbersome. 

Accordingly, we wish to write a field theory which would reproduce the statistics for $f(x_0)..f(x_P)$ but allow us to extend it to any sample ($x$). This is important for understanding generalization. To achieve this, we follow Sec. \ref{Sec:PathIntegrals} and take a limit where we gradually extend the set of points $x_0..x_P$ to a much denser set $x_1..x_n$ but make sure $x_1..x_P$ are always a subset of this denser set so that we can still write the loss term. Finally, we take $n\rightarrow \infty$ and replace sums with integrals, matrices with operators, and vectors with functions while making sure we track $n$ factors correctly.  The result which we would soon verify independently is 
\begin{align}
\label{Eq:Z_FieldTheoryOnData}
Z &= \int Df e^{-S_{\GP}} \\ \nonumber
S_{\GP} &= \frac{1}{2} \int d\mu_x d\mu_y f(x) K^{-1}(x,y) f(y) + \frac{1}{2\kappa^2}\sum_{\mu=1}^P (f(x_{\mu})-y(x_{\mu}))^2
\end{align}
where $K^{-1}(x,y)$ is the inverse operator to $K(x,y)$ under the $d\mu_x$ measure as defined in Sec. \ref{Sec:Intro_Measure}.

Let us demonstrate how we reproduce the original discrete posterior  ($Z_{\textrm{SOP}}$ of Eq. \eqref{eq:S_SOP}) from this one. To this end, we introduce the variables $g_0..g_P$, force them to equal to $f(x_0)..f(x_P)$ using delta functions, and integrate out $f$ to obtain the partition function for such $g_0..g_P$. The latter enables us to calculate any statistical property $f(x_0)..f(x_P)$ and would match, up to inconsequential factors, the original discrete $Z$. Specifically, 
\begin{align}
Z &= \int dg_0..dg_P \int Df  e^{-S_{\GP}} \prod_{\nu=0}^P \delta(g_{\nu}-f(x_{\nu})) \\ \nonumber 
&= \int dt_0..dt_P \; dg_0..dg_P \int Df \exp\biggl\{-S_{\GP}+i\sum_{\nu=0}^P t_{\nu}(g_{\nu}-f(x_{\nu})) \biggl\} \\ \nonumber
&= \notag \int (...) \int Df \exp\biggl\{-\frac{1}{2} \int d\mu_x d\mu_y f(x) K^{-1}(x,y) f(y) \\&- \frac{1}{2\kappa^2}\sum_{\mu=1}^P (g_{\mu}-y(x_{\mu}))^2+i\sum_{\nu=0}^P t_{\nu}(g_{\nu}-f(x_{\nu})) \biggl\}
\end{align}
where the ellipsis $(...)$ means copying the appropriate expression from the previous line which here is $dt_0..dt_P \; dg_0..dg_P$. We also rewrote the delta function as an exponential. To obtain our original discrete $Z$, we next integrate over $Df$, using a square completion of the form $f(x) \rightarrow f(x)+i\sum_{\nu} K(x,x_{\nu})t_{\nu}$ which yields (ignoring, as always, constant factors)  
\begin{align}
\notag Z &= \int dt_0..dt_P \; dg_0..dg_P \exp\biggl\{-\frac{1}{2} \sum_{\mu \nu=0}^P t_{\mu} K(x_{\mu},x_{\nu})t_{\nu} \\&- \frac{1}{2\kappa^2}\sum_{\mu=1}^P (g_{\mu}-y(x_{\mu}))^2+i\sum_{\nu=0}^P t_{\nu}g_{\nu}\biggl\}\end{align}
where integrating out the $t$ variables yields $Z$ in the discrete case with $f_{\mu}$ replaced by $g_{\mu}$'s, thereby establishing our claim. For the NTK case, following Eq. \ref{Eq:NTKInfiniteTime}, we simply take $K=\Theta$ and $\kappa^2\to 0$, however obtaining fluctuations ($I_0$) and time dependence requires some more work (see Ch. \ref{Sec:MSRDJ}).

To conclude this section, we discuss some properties of Eq. (\ref{Eq:Z_FieldTheoryOnData}). As we have seen in Sec. \ref{Sec:PathIntegrals}, the first term in $S_{\GP}$ reflects the prior induced on function space by a random DNN. The keen reader may notice an oddity, which is that the prior should be data-measure agnostic. This requirement is indeed obeyed in Eq. \ref{Eq:Z_FieldTheoryOnData}, though implicitly, based on the aforementioned fact that the RKHS norm is measure independent \footnote{Thus measure here plays a somewhat similar role to a Gauge field in physics, at least as far as the prior is concerned, the data-term explicitly breaks this invariance of the theory}. 

Interestingly, the source of the first term is entropic: Indeed it is not difficult to show that, had we replaced the Gaussian iid distribution of each $\theta_{\alpha}$ with a uniform distribution with the same variance, the same kernel would be reproduced at infinite $N$. Considering for simplicity the discrete case, $P_0(f_0..f_P)$ (defined in Eq. (\ref{Eq:Derivation1})) with this Gaussian-to-uniform replacement, one finds that 
\begin{align}
\frac{1}{2} \sum_{\mu \nu}&f(x_{\mu}) [K^{-1}]_{\mu \nu} f(x_{\nu}) = -\log(P_0(f_0..f_P)) + \const \\ \nonumber 
&\stackrel{N \rightarrow \infty}{=} -\log\left(V_{\theta}\int_{\text{intervals}} \dif \theta \prod_{\nu=0}^P \delta(f_{\mu} - z_{\theta}(x_{\nu}))\right) + \const
\end{align}
where $V_{\theta}$ is the total weight space volume encompassed by the uniform measure of the weights. Notably, 
the final line is the entropy/log-volume of weight configurations within intervals set by the uniform measure which yield the outputs $f_0..f_P$. This view carries on the field-theory setting, where it would correspond to the entropy of weights yielding the function $f(x)$. Viewing weights as micro-states and output as the macro-state, the above l.h.s. is proportional to the Boltzmann entropy associated with the macro-state $f(x)$. The second term, in $S_{\GP}$ the data-term, can then be thought of as an energy. Much like in physics, the Gaussian Process (i.e. the partition function $Z$) would seek a $\kappa^2=T$ compromise between entropy and energy, favoring energy at low $T$ and entropy at high $T$. Entropy then acts as a regulating force, pushing the field away from the training data and choosing $f(x)$ which extrapolates between the data points such that it has low entropy. 

This more regular and ``orderly" behavior induced by entropy is analogous to the order-through-disorder effect exhibited by some physical systems \citep{Villain1980}. Note also that this type of entropy is largely a property of the neural network and the relevant volume in weight space and not the training algorithm. Nonetheless, it is likely to affect all training algorithms, as more common-in-weight-space functions are likely to be discovered first by most noisy gradient based optimization methods. 

The fact that entropy acts as a regulator offers a way of rationalizing the original puzzle regarding over-parametrization. Indeed, the absurdity came from the fact that many weight solutions exist, which yield zero train loss. This ambiguity in choosing weights is just what entropy measures. The fact that entropy regulates the solution implies that weight configurations which generate functions with smaller RKHS norms (i.e. are favored by the entropy term) are far more abundant in weight space. Hence during training, dynamics is biased by this abundance to find those smaller-RKHS solutions and goes to high-RKHS/low-entropy solution only given sufficient data. In typical real world setting the eigenvalues of the kernel are very multiscale creating a strong differentiation between functions \footnote{This is not the case for some artificial kernels and datasets, say those with a degenerate spectrum, leading to overfitting issues near the interpolation threshold (e.g. \cite{Canatar2021})}. Given the empirical fact that such GPR works quite well \citep{lee2019wide}, one concludes that this bias toward low RKHS norm, should also be the bias towards more favourable functions, performance-wise. In contrast, highly over-fitting solutions which match the training data but do something erratic outside the training set, have a very low phase space in weight space and hence are much more difficult to find during training. In Sec. \ref{Sec:Symmetries}, we flesh out this bias for the case of an FCN, and show that it favors low degree polynomials. 

\section{Field theory of GPR - data-averaged}
\label{Sec:AveragedGPR}
Looking to make precise analytical statements on the GPR formula, or equivalently on the above field theory, is made difficult by the randomness induced by the specific draw of dataset. This randomness spoils any potential symmetry of the problem and requires us to invert a specific, dataset-dependent, $P\times P$ matrix ($K_D$) to obtain predictions. We thus wish to average over many such different random draws of $P$ points thereby smoothening dataset draw specific effects and restoring potential symmetries.  

Following Sec. \ref{Sec:Replicas}, we focus on the quenched averaged free energy written using the replica identity namely 
\begin{align}
\label{Eq:ReplicatedZGPR}
\langle \log(Z) \rangle_{x_{1..P}} &= \lim_{Q\to 0}\left\langle \frac{Z^Q-1}{Q} \right\rangle_{x_{1..P}} 
% = Q^{-1}\left\langle Z^Q[\alpha] \right\rangle_{x_{1..P}} - Q^{-1} 
\end{align}
where we denote by $\langle A(x_1..x_P) \rangle_{x_{1..P}} \equiv \int d\mu_{x_1}..d\mu_{x_P} A(x_1..x_P)$ for some function of the training points $A(...)$. Focusing on the average of $Z^Q[\alpha]$, one convenient fact is that the prior term in $S_{\GP}$ does not depend on variables being averaged ($x_1..x_P$). This is one of the advantages of working with the $S_{\GP}$ action compared to the discrete one.  Consequently, we find 
\begin{align}
\label{eq:quenched_ZQ}
&\langle Z^Q\rangle_{x_{1..P}} = \int D f_1..Df_Q e^{-\frac{1}{2} \sum_{a=1}^Q \int d\mu_x d\mu_y f_a(x) K^{-1}(x,y) f_a(y)}  \\ \nonumber 
&\times \left\langle e^{-\frac{1}{2\kappa^2}\sum_{a=1}^Q \sum_{\mu=1}^P [f_a(x_{\mu})-y(x_{\mu})]^2}\right\rangle_{x_{1..P}} \\ \nonumber 
&= (...)\times \left(\int d\mu_x e^{-\frac{1}{2\kappa^2}\sum_{a=1}^Q [f_a(x)-y(x)]^2}\right)^P 
\end{align}
at this point, we can already take the log of the data term and get a non-local data term. A slightly more elegant solution which yields a local loss term is to raise the data term to the exponent by noticing the following algebraic identity   
\begin{align}
\label{eq:poisson_averaged_data_term}
&\exp\left(-\tilde{P}+\tilde{P} \int d\mu_x e^{-\frac{1}{2\kappa^2}\sum_{a=1}^Q [f_a(x)-y(x)]^2}\right) \\ \nonumber 
&= e^{-\tilde{P}}\sum_{n=0}^{\infty} \frac{\tilde{P}^n}{n!} \left(\int d\mu_x e^{-\frac{1}{2\kappa^2}\sum_{a=1}^Q [f_a(x)-y(x)]^2}\right)^n
\end{align}
and that the r.h.s. now appears as the data term we need, however, averaged over $P$ (played by $n$ in the above). Thinking about $P$ as a Poisson random variable with mean $\tilde{P}$. At large $\tilde{P}$, fluctuations in the Poisson-averaged data term are of the order $\sqrt{\tilde{P}}$ and so negligible compared to the average. Setting $\tilde{P}=P$ and combining \ref{eq:poisson_averaged_data_term} with \ref{eq:quenched_ZQ}, we obtain, 
\begin{align}
\label{Eq:IntroducingSBar}
&\langle Z^Q[\alpha]\rangle_{x_{1..P}} \underset{P \gg 1}{\approx} e^{-P}\sum_{n=0}^{\infty} \frac{P^n}{n!} \langle Z^Q[\alpha]\rangle_{x_{1..n}} \\ \nonumber 
&e^{-P}\sum_{n=0}^{\infty} \frac{P^n}{n!} \langle Z^Q[\alpha]\rangle_{x_{1..n}} \equiv Z_Q[\alpha] = \int Df_1..Df_Q e^{-\bar{S}_{\GP}[f_1..f_Q]} \\ \nonumber 
&\bar{S}_{\GP} = \frac{1}{2} \sum_{a=1}^Q \int d\mu_x d\mu_y f_a(x) K^{-1}(x,y) f_a(y) - P \int d\mu_x e^{-\frac{1}{2\kappa^2}\sum_{a=1}^Q [f_a(x)-y(x)]^2} \\ \nonumber 
&+ P - \int dx \; \alpha(x) \sum_{a=1}^Q f_a(x)
\end{align}
where we also included a source ($\alpha(x)$) for computing dataset averaged moments of $f(x)$ \footnote{Specifically, we introduce an $\int dx \alpha(x) f(x)$ term in the original, un-replicated action, which then leads to the above term in the replicated action.}.  We thus obtained a field theory of replicas governing the dataset-averaged free energy associated with GPR. This can either be thought of as an approximation to averaging over all datasets with $P$ elements drawn from $p(x)$ or as an exact result associated with averaging over all datasets drawn from $p(x)$ with the number of elements drawn from a Poisson distribution with average $P$ \footnote{We note by passing one can circumvent this Poisson averaging by exponentiating $[\int d\mu_x e^{-\frac{1}{2\kappa^2}\sum_{a=1}^Q [f_a(x)-y(x)]^2}]^P$, the resulting data-term is non-local but still manageable and leads to the same predictions at large $P$}. A similar approach was taken in \cite{malzahn2001variational,cohen2021learning}.  As common when taking quenched averages, the original Gaussian theory became non-Gaussian/interacting following the averaging procedure. To obtain, say, the dataset averaged value of the average GPR predictor at a point $x_0$, namely the dataset average of the first line in Eq. (\ref{Eq:GPR}), one needs to calculate (see Eq. \ref{Eq:ReplicatedZGPR}) 
\begin{align}
&\left\langle \left \langle f(x_0) \right \rangle_{S_{GP},D=[x_1..x_n]}  \right\rangle_{x_1..x_n;n\sim \text{Poisson}(P)} = \lim_{Q\rightarrow 0}\partial_{\alpha(x_0)} \left[Q^{-1}Z_Q[\alpha]-Q^{-1} \right]|_{\alpha=0} \\ \nonumber 
&= \lim_{Q\rightarrow 0} Q^{-1} \partial_{\alpha(x_0)}Z_Q|_{\alpha=0} = ...
\end{align}
Given the relevant setting here, where all replicas of the field would play a similar role (i.e. replica symmetric), the division by $Q$ in the above formula would be cancelled by $\sum_{a=1}^Q$ accompanying $\alpha(x)$. Noting in addition that in the replica limit the action vanishes and hence $Z_{Q\rightarrow 0}=1$ we find 
\begin{align}
... &= \lim_{Q\rightarrow 0} Q^{-1} \partial_{\alpha(x_0)}Z_Q|_{\alpha=0} &= \lim_{Q\rightarrow 0} Q^{-1} Z^{-1}_Q\partial_{\alpha(x_0)}Z_Q|_{\alpha=0} = \lim_{Q\rightarrow 0}\langle f_{a}(x_0) \rangle_{\bar{S}_{GP}},
\end{align}
where $a$ is any specific replica index. 
The above two equations provide a concrete connection between dataset averages of GP predictors and standard averages under $\bar{S}_{GP}$. 

Next, we present several approximation techniques for evaluating the average of $f_a(x)$ under the above field theory. 
\subsection{Equivalent Kernel}
\label{Sec:EK}
The simplest approximation method, yielding the so-called Equivalent Kernel (EK) result \citep{Silverman1984}, is to assume $(f_a(x)-y(x))^2$ is much smaller than $\kappa^2$ on the measure $d\mu_x$. Roughly speaking, this should be correct when the typical MSE loss is smaller than $\kappa^2$ as expected to happen at large enough $P$ and fixed $\kappa^2$. More formally, we can think of EK as the asymptotic limit of taking $P\rightarrow \infty$ with $P/\kappa^2$ kept fixed (e.g. more data points but also more noise to balance the amount of information). Concretely, we approximate 
\begin{align}
\label{Eq:Approx_S_GP}
\bar{S}_{\GP} &\approx \bar{S}_{\GP,\text{EK}} = \frac{1}{2} \sum_{a=1}^Q \int d\mu_x d\mu_y f_a(x) K^{-1}(x,y) f_a(y) \\ \nonumber  &- P \int d\mu_x \left(1- \frac{1}{2\kappa^2}\sum_{a=1}^Q [f_a(x)-y(x)]^2 + O(\kappa^{-4})\right)  
+ P  \\ \nonumber 
&= \frac{1}{2} \sum_{a=1}^Q \int d\mu_x d\mu_y f_a(x) K^{-1}(x,y) f_a(y) +\frac{P}{2\kappa^2} \int d\mu_x [f_a(x)-y(x)]^2  \\ \nonumber 
\end{align}
where we dropped the $\alpha(x)$ source term. 
Notably, the above action is Gaussian and does not contain any coupling between replicas. Following this, we can focus on a single replica and move to the eigenfunction basis of $K(x,y)$ ($\phi_k(x)$) to obtain the following simple ``diagonal'' action per replica
\begin{align}
\label{Eq:EK_Av_Pred}
\bar{S}_{\text{GP,EK}}&= \sum_{k=1}^{\infty} \frac{f_k^2}{2 \lambda_k} + \frac{(f_k-y_k)^2}{2\kappa^2}
\end{align}
where $f_k = \int d\mu_x \phi_k(x) f(x)$ (and similarly with $y_k$). This then yields the following approximation for the dataset averaged GPR predictor 
\begin{align}
\langle f_a(x) \rangle_{\bar{S}_{\GP}} &= \sum_k \frac{\lambda_k}{\lambda_k + \frac{\kappa^2}{P}} y_k \phi_k(x) + O(\frac{Loss^2}{\kappa^4})
\end{align}
whereby $Loss^2$ we mean more formally the typical value of $(f_a(x)-y(x))^2$ which is of the order of the MSE loss unless some outliers appear. We thus estimate that when the mean loss ($Loss$) is much smaller than the ridge parameter, the GPR predictor behaves as a linear high-pass spectral filter on the target. Namely, eigenfunctions with $\lambda_k P \gg \kappa^2$, can be copied into the predictor and eigenfunctions with $\lambda_k P \ll \kappa^2$ get discarded. 

Turning to variances, there are three different types here: {\bf (i)} Taking the variance per dataset according to GPR (i.e. the second line of \ref{Eq:GPR} giving us fluctuations induced by the gradient noise and, more importantly at low $\kappa$, by the remaining randomness in the weights which minimize the loss) and averaging it over datasets {\bf (ii)} Taking the GPR predictor (i.e. first line in \ref{Eq:GPR}) and taking its variance under different dataset draws, {\bf (iii)} The sum of these two. This third variance is the most natural one since it reflects the total variance contribution ($V$) to the dataset averaged loss namely, 
\begin{align}
&\int d\mu_x \langle \langle (f(x)-y(x))^2 \rangle_{S_{\GP}} \rangle_{x_1..x_P} = B + V \\ \nonumber 
B &=  \int d\mu_x (\langle \langle f(x) \rangle_{S_{\GP}} \rangle_{x_1..x_P}-y(x))^2 \\ \nonumber 
V &=\int d\mu_x \langle \langle f^2(x) \rangle_{S_{\GP}} \rangle_{x_1..x_P} - \int d\mu_x \langle\langle  f(x) \rangle_{S_{\GP}} \rangle_{x_1..x_P}^2 
\end{align}
By adding another source term which couples to $\sum_{a=1}^{Q} f_a^2(x)$, one can show that $V$ in our theory is $\langle \int d\mu_x (f(x)-\bar{f}(x))^2 \rangle_{\bar{S}_{\text{GP,EK}}}$ or equivalently $\sum_k \langle (f_k-\bar{f}_k)^2 \rangle_{\bar{S}_{\text{GP,EK}}}$ and therefore given by 
\begin{align}
V &\stackrel{\text{EK}}{\approx} \sum_k \frac{1}{\lambda_k^{-1}+\frac{P}{\kappa^{2}}} = \frac{\kappa^2}{P} \sum_k \frac{\lambda_k}{\lambda_k+\frac{\kappa^{2}}{P}}
\end{align}
which can be roughly interpreted as the number of learnable modes (i.e. modes with $\lambda_k P > \kappa^2$) over the effective number of data points ($P/\kappa^2$). We note that the same result would have been obtained had we just taken a second derivative w.r.t. to the  $\alpha$ source term, thereby obtaining type (i)  contribution. We thus find a qualitative limitation of the EK limit which is that it neglects type (ii), dataset-choice-induced, variances. This could have been anticipated since it is exact in the limit of infinite $P$ (with $P/\kappa^2$ fixed) where all the randomness induced by the choice of dataset disappears. Another issue with this limit is that it breaks down in the ridgeless limit $\kappa^2$ (e.g. for NTK). Notwithstanding, it is accurate in its regime of validity, which is quite a sensible regime (small loss over ridge). Furthermore, as we shall see later, other, more advanced approximation techniques which are valid at vanishing ridge end up looking like EK with an effective ridge parameter. 

The EK approximation thus exposes a central qualitative aspect of the problem, which is that learning is done diagonally in kernel eigenfunctions and there is no ``cross-talk'' between eigenfunctions. Let us delve into this point a bit more. Since the GPR predictor is linear in the target, its dataset average must also be some linear functional of the target. However, while this linear functional could be diagonalized leading to some spectral filtering of the target, this spectrum and its associated eigenfunctions, did {\it not} have to coincide with the eigenbasis of the kernel or be independent of $P$ and $\kappa^2$. The fact that for all $P$, it is the kernel eigenfunctions and spectrum which control the induced filter on the target, is sometimes referred to as eigenlearning \citep{simon2021} or spectral bias.
As we shall shortly see, while at low input dimensions and low ridge, eigenlearning may break down, it appears to be quite robust at large effective input dimension, which is the more common use case of GPR in the context of deep learning.   


\subsection{Perturbative treatment}
A straightforward approach to improve upon the previous EK limit is to view it as a perturbative expansion in $1/\kappa^2$ and take the next to leading order, namely the $1/\kappa^4$ contribution. Specifically, this means splitting $\bar{S}_{\GP}$  into its free part (last line in Eq. \ref{Eq:Approx_S_GP}) plus the following interaction term 
\begin{align}
U &= -\frac{P}{4 \kappa^4} \int d\mu_x \left(\sum_{a=1}^Q [f_a(x)-y(x)]^2\right)^2
\end{align}
To compute the first moment of the data averaged GPR predictor under this perturbative correction one needs to integrate over $Q$ path integrals in replica space. Performing this calculation with the limit $Q\to 0$, it emerges that simply picking one replica is sufficient. 

Using leading perturbation theory approach (Sec. \ref{Sec:PT}) one then obtains (\cite{cohen2021learning}, Appendix I)

\begin{align}
&\langle f_a(x_0) \rangle_{\bar{S}_{\GP}}= \langle f(x_0) \rangle_{\bar{S}_{\text{GP,EK}}} \\ \nonumber &
-\frac{P}{\kappa^4} \sum_{ijk}  \frac{\frac{\kappa^2}{P}}{\frac{\kappa^2}{P} + \lambda_i} y_i \phi_j(x_0) V_{j} V_k\left(\int d\mu_x \phi_i(x)\phi_j(x)\phi^2_k(x)\right) + O(\kappa^{-6}) \\ \nonumber 
V_i &= \frac{1}{\lambda_i^{-1} + \frac{P}{\kappa^2}}.
\end{align}

To familiarize ourselves with the above expression we note that: {\bf (i)} The factor $\frac{\frac{\kappa^2}{P}}{\frac{\kappa^2}{P} + \lambda_i} y_i$, is related to the bias component of the loss of $i$'th mode according to EK, hence the correction increases if performance according to EK is poor. {\bf (ii)} $V_k$ is the variance contribution of the $k$'th mode, hence the correction increase is the variance contribution to the loss is high according to EK. {\bf (iii)} Non-linear correlations between eigenfunctions enter this correction and may, together with the $y_i \phi_j(x_0)$ non-diagonal factor, generate cross-talk between features. 

Interestingly, given a high dimensional input measure, one can argue that this cross-talk vanishes. Here, we assume $\phi_k(x)$ to be a sum over many ($O(d)$ or higher) different components of $x$ at different powers. If so, it is reasonable to treat $\phi_k(x)$, via the central limit theorem, as a Gaussian random variable over $p(x)$. Assuming for simplicity $\int d\mu_x \phi_k(x)=0$ (i.e. centered Gaussian), the above non-linear correlation simplifies via Wick's theorem to 
\begin{align}
&\int d\mu_x \phi_i(x)\phi_j(x)\phi^2_k(x) \stackrel{d \gg 1}{\approx} \int d\mu_x \phi_i(x)\phi_j(x) \int d\mu_x \phi_k^2(x) \\ \nonumber 
&+ 2\int d\mu_x \phi_i(x)\phi_k(x) \int d\mu_x \phi_j(x) \phi_k(x) = \delta_{ij}+\delta_{ik} \delta_{jk}
\end{align}
consequently, $i=j$ and cross-talk is eliminated. However, at low dimensions cross talk is possible, see for example \citep{Tomasini2022,howard2024}. Removing the $\int d\mu_x \phi_k(x)=\bar{\phi}_k=0$ restriction, cross-talk re-emerges since even for $i\neq j \neq k$ the integral would be $-\bar{\phi}_i\bar{\phi}_j(1+\bar{\phi}^2_k)$. In practice, however, $\bar{\phi}_j$ decays very quickly with $k$ and practically vanishes for $j\gg 1$. \footnote{Indeed $\bar{\phi}_j$ is also the overlap of the constant function with the $\phi_j(x)$ feature. Typically, the constant function is easy to learn hence spanable by the top kernel eigenfunctions and thus orthogonal to the rest.} At the same time, the terms related to the loss (items (i) and (ii)) above, typically vanish for $j,i=O(1)$ and large $P$, hence, in practice, cross-talk may be ignored. 

\subsection{Effective ridge treatment}
\label{SSec:Canatar}
In the case of small gradient noise (i.e. small $\kappa^2$) relevant, in particular, to NTK one cannot rely on the above perturbative treatment. Instead, several authors \citep{Tsigler2020,cohen2021learning,Canatar2021} noted that, at high input dimensions, the fluctuations of the unlearnable/low kernel eigenvalues simply generate an effective ridge parameter for the learnable kernel eigenvalues. Here we will re-derive the most general of those (\cite{Canatar2021, Sollich2001}) using our field theory approach. 

The main underlying assumption of Ref. \cite{Canatar2021} is that $f_a(x)-y(x)$, appearing within the interaction term in Eq. (\ref{Eq:IntroducingSBar}), is Gaussian when viewed as a random variable over $p(x)$. More specifically, a $Q$-variate Gaussian, as there could be correlations between different replicas. This {\it Gaussian Discrepancy Assumption} follows from our previous Gaussian-eigenfunctions assumption but is generally a weaker requirement. We further assume, for simplicity, that $\int d\mu_x [f_a(x)-y(x)]=0$, which amounts to taking an anti-symmetric DNN or assuming that we quickly learn the constant component in $y(x)$ perfectly without any fluctuations (indeed it is typically easy to learn). Introducing the discrepancy variables $\Delta_a$ via the identity $1=\int d\Delta_a  \delta(\Delta_a-f_a(x)+y(x))$ we find 
\begin{align}
\label{Eq:PoissonInteraction}
&P \int d\mu_x e^{-\frac{1}{2\kappa^2}\sum_{a=1}^Q [f_a(x)-y(x)]^2} \\ \nonumber 
&= P \int d\mu_x \prod_{a=1}^Q \left[\int d\Delta_a \delta(\Delta_a-f_a(x)+y(x))\right]e^{-\frac{1}{2\kappa^2}\sum_{a=1}^Q [f_a(x)-y(x)]^2}\\ \nonumber 
&\approx P\int d\Delta_{1}..d\Delta_Q \; {\cal N}(0,C;\Delta) \; e^{-\frac{\Delta^T \Delta}{2 \kappa^2}} 
= P e^{-\frac{1}{2} \Tr\log(I+ \kappa^{-2} C)} = ...\\ \nonumber 
C_{ab} &= \int d\mu_x \; [f_a(x)-y(x)][f_b(x)-y(x)] = \sum_{k=1}^{\infty} [f_{k,a}-y_k][f_{k,b}-y_k]
\end{align}
where the approximation in the second line is exact for Gaussian features,
$I$ is an identity matrix in replica space ($I_{ab}=\delta_{ab};a,b\in 1..Q$), in the second equality we view $\int d\mu_x \prod \delta(\Delta_a - f_a(x)-y(x))$ as the joint distributions of the $\Delta_a$'s which we assume is Gaussian, in the third equality we used standard Gaussian integration formulas together with the identity $\det B = e^{\text{Tr} \log B}$, and in the last equality we used a spectral decomposition and the orthogonality of features/eigenfunctions. 

Since $C_{ab}$, the replica correlations of discrepancy over the data measure, are made up of a contribution of many independent $k$-modes, it makes sense to try a mean-field approximation. Specifically, following our previous section  \ref{ssec:mean_field}, we define a mean-field kernel $C=C_{\MF}+C-C_{\MF}\equiv C_{\MF}+\Delta C$, plug this in the above term, and expand to leading order in $\Delta C$. Using the Taylor expansion $e^{-\frac{1}{2}\Tr\log(M+X)}=e^{-\frac{1}{2} \Tr\log(M)}(1-\frac{1}{2}\Tr[M^{-1}X])+O(X^2)$ with $X=\Delta C$ and $M=I+\kappa^{-2} C_{\MF}$ yields 
\begin{align}
&... = \frac{Pe^{-\frac{1}{2}\Tr\log(I+\kappa^{-2}C_{\MF})}}{2} \Tr[(\kappa^2 I+C_{\MF})^{-1}\Delta C] \\ \nonumber &= \frac{Pe^{-\frac{1}{2}\Tr\log(I+\kappa^{-2}C_{\MF})}}{2} \sum_k \sum_{ab=1}^Q [f_{k,a}-y_k][(\kappa^2 I+C_{\MF})^{-1}]_{ab}[f_{k,b}-y_k] + \const \\ \nonumber 
&=|_{Q \rightarrow 0}\frac{P}{2} \sum_k \sum_{ab=1}^Q [f_{k,a}-y_k][(\kappa^2 I+C_{\MF})^{-1}]_{ab}[f_{k,b}-y_k] \end{align}
where in the last equality we ignored the irrelevant additive constant term and, anticipating the zero replica limit, took $\Tr\log(I+\kappa^{-2}C_{\MF})=O(Q)$ to zero upfront. 

The resulting data term is conveniently quadratic in the fields and diagonal in the kernel eigenmodes. Consequently, computing averages according to this theory (i.e. Eq. \ref{Eq:IntroducingSBar} with the interaction term replaced by the above) is straightforward apart from resolving the coupling between replicas. Expecting a replica symmetric solution, we may write $C_{\MF}=A I+ D \Gamma$ where $\Gamma_{ab}=1$. Noting that $\Gamma^2=Q\Gamma$ and $(a I + b \Gamma)(a^{-1} I - \frac{b}{a^2} \Gamma)=I-Q\frac{b^2}{a^2} \Gamma$, we obtain the following action which coincides with the original one in the replica limit ($Q\rightarrow 0$) 
\begin{align}
&\bar{S}_{\GP,\MF} = \sum_k \sum_{a=1}^Q \frac{f_{a,k}^2}{2\lambda_k} \\ \nonumber &+ \frac{P}{2} \sum_k \sum_{ab=1}^Q [f_{k,a}-y_k]\left[(\kappa^2+A)^{-1} I+\frac{D}{(\kappa^2+A)^2}\Gamma\right]_{ab}[f_{k,b}-y_k] 
\end{align}
Using similar matrix inversion formula, dropping $Q$ factors, and denoting $\kappa_{\text{eff}}^2=\kappa^2+A$ one finds 
\begin{align}\label{eq:barf}
\langle f_{k,a}\rangle_{\bar{S}_{\GP,\MF}} &= \frac{\lambda_k}{\lambda_k+\kappa^2_{\text{eff}}/P} y_k
\end{align}
which coincides with EK prediction using an effective ridge $k^2_{\text{eff}}$. For the variance, we obtain, 
\begin{align}\label{eq:f_k_f_k_connected}
\langle f_{a,k} f_{b,k} \rangle_{\bar{S}_{\GP,\MF},\con} &= \left( \frac{P} {\kappa^{2}_{\eff}} + \lambda_k^{-1} \right)^{-1} \delta_{ab} + \left( \frac{P} {\kappa^{2}_{\eff}} + \lambda_k^{-1} \right)^{-2}\frac{P}{\kappa^{4}_{\eff}} D 
\end{align}
which contains an EK-like contribution to the variance (the first term) along with a new contribution that reflects the aforementioned type (ii) contributions to the variances missed by EK. 

What remains is to obtain $A,D$ which determine $C_{\MF}$ or namely to solve the self-consistency equation
\begin{align}
&[AI+D\Gamma]_{ab} = [C_{\MF}]_{ab} = \sum_k \langle [f_{k,a}-y_k][f_{k,b}-y_k] \rangle_{\bar{S}_{\GP,\MF}} \\ \nonumber 
&= \sum_k \langle f_{a,k} f_{b,k} \rangle_{\bar{S}_{\GP,\MF},\con} 
+ \sum_k [\langle f_{k,a}\rangle_{\bar{S}_{\GP,\MF}}-y_k][\langle f_{k,b}\rangle_{\bar{S}_{\GP,\MF}}-y_k]. 
\end{align}
Plugging Eqs. \ref{eq:barf},\ref{eq:f_k_f_k_connected} in the above, we note that contribution proportional to $I$ on the r.h.s. comes solely from the variance term and yields 
\begin{align}
\label{Eq:CanatarEffectiveRidge}
A &= \sum_k \left(\frac{P} {\kappa^{2}_{\eff}}+\lambda_k^{-1}\right)^{-1} \Rightarrow \kappa_{\eff}^2 =  \kappa^2 + \sum_k \left(\frac{P} {\kappa^{2}_{\eff}}+\lambda_k^{-1}\right)^{-1}
\end{align}
which has a simple interpretation: The effective ridge is the bare ridge ($\kappa^2$) together with the posterior-variance (i.e. $V_k$) of all modes in the presence of the effective ridge and $P$. Turning to $D$ we find two contributions proportional to $\Gamma$ on the r.h.s. yielding 
\begin{align}
\label{Eq:D}
\left(1-\sum_k \left( \frac{P}{\kappa_{\eff}^2} + \lambda_k^{-1} \right)^{-2}\frac{P}{\kappa^{4}_{\eff}} \right)D &= \sum_k \left(\frac{\lambda_k}{\lambda_k+\kappa^2_{\eff}/P} y_k\right)^2
\end{align}
which, given $\kappa^2_{\eff}$, is a simple linear equation for $D$. It can also be shown that the first factor on the l.h.s. is always larger than zero.

\subsection{Renormalization group treatment}
Last, we review a recent approach \citep{howard2024} which utilizes renormalization group ideas to tackle the same problem. The renormalization group (RG) is a powerful analytical technique used extensively in physics. At its core, it is a greedy-algorithm for performing path integrals: First, we integrate over a small set of field modes. These should be chosen wisely so that their integration/marginalizing is simple. A common choice is modes with small variance, allowing one to treat all interaction terms involving such modes via perturbation theory. The result of this partial integration is a different representation of the partition function, which nonetheless reproduces the correct statistics for all the remaining field modes. This new partition function can be described by a slightly different action (renormalized/effective action) coming from the feedback of those modes we integrated out on the remaining ones. Interestingly, in many cases, we can track the change (flow) of this effective action as we progressively integrate-out more and more field modes. The resulting effective action is often simpler than the original one, thus providing a computational advantage. These flows of actions may also be contracting in the space of effective actions, which is the underlying mechanism of universality in physics. Here we shall use this RG approach to reproduce some of the results of the previous section. 

Our starting point is the following form of $\bar{S}_{\GP}$ obtained from Eq. \ref{Eq:PoissonInteraction} we limit the summation over eigenmodes to some potentially huge but finite cut-off ($\Lambda$)
\begin{align}
\label{Eq:SLambda}
&\bar{S}_{\GP,\Lambda} = \sum_{k=0}^{\Lambda} \sum_{a=1}^Q \frac{f_{a,k}^2}{2\lambda_k} +P e^{-\frac{1}{2} \Tr\log\left(\kappa^2 I+ \sum_{k=1}^{\Lambda} (\bm{f_k}-\bm{y_k})(\bm{f_k}-\bm{y_k})^T\right)} \\ \nonumber 
Z_{\Lambda} &=  \int \left[\prod_{a=1}^Q df_{1,a}..df_{a,\Lambda}\right] e^{-\bar{S}_{\GP,\Lambda}}
\end{align}
where we used the {\it Gaussian Discrepancy Assumption} (Eq. \ref{Eq:PoissonInteraction}), took the replica limit upfront by pulling out a $\Tr\log(\kappa^{-2} I)$ term from the exponent, we further sort $k$ such that $\lambda_k$ come in descending order and also introduce the replica space vector notation $[\bm{f_k}]_a=f_{k,a}$ and $[\bm{y_k}]_a=y_k.$ 
% as well as a cut-off ($\Lambda$) on the number of modes which we consider to be extremely large. 
Next, we consider integrating out $f_{k,a}$ modes with $k = \Lambda$, assuming that $\lambda_{\Lambda},y^2_{\Lambda}\ll \kappa^2$. To this end we split the interaction term into its so-called lesser part (i.e. involving only modes we keep i.e. $k < \Lambda$) and its greater part which involves the modes we are about to integrate over
\begin{align}
e^{-\frac{1}{2}\Tr\log\left(\kappa^2 I+ \sum_{k=1}^{\Lambda-1} (\bm{f_k}-\bm{y_k})(\bm{f_k}-\bm{y_k})^T+(\bm{f_{\Lambda}}-\bm{y_{\Lambda}})(\bm{f_{\Lambda}}-\bm{y_{\Lambda}})^T\right)} &= ...
\end{align}
Knowing that the first term in the action limits $f_{a,\Lambda}^2$ to order $\lambda_{\Lambda} \ll \kappa^2$ and assuming also that $y^2_{\Lambda} \ll \kappa^2$ it is reasonable to treat $(\bm{f_{\Lambda}}-\bm{y_{\Lambda}})(\bm{f_{\Lambda}}-\bm{y_{\Lambda}})^T$ to leading order in perturbation theory namely, 
\begin{align}
\label{Eq:RGTaylorExpansion}
... &= e^{-\frac{1}{2} \Tr\log\left(\kappa^2 I+ \sum_{k=1}^{\Lambda-1} (\bm{f_k}-\bm{y_k})(\bm{f_k}-\bm{y_k})^T\right)}[1 + U] \\ \nonumber 
U &= \Tr\left[\left(\kappa^2 I+ C_{\Lambda-1}
% \sum_{k=1}^{\Lambda-1} (\bm{f_k}-\bm{y_k})(\bm{f_k}-\bm{y_k})^T
\right)^{-1}(\bm{f_{\Lambda}}-\bm{y_{\Lambda}})(\bm{f_{\Lambda}}-\bm{y_{\Lambda}})^T \right]
\\\nonumber 
&C_{\Lambda-1} = \sum_{k=1}^{\Lambda-1} (\bm{f_k}-\bm{y_k})(\bm{f_k}-\bm{y_k})^T
\end{align}
Since $U$ is proportional to the highest, GP modes ($f_{\Lambda}$) which is small in average and variance compared to $\kappa^2$ and the highest target mode which we assume is similarly small compared to $\kappa^2$, we may  
 perform the integration over $d \bm{f_{\Lambda}}$ to order $O(U)$ and re-exponentiate the $O(U)$ contribution to obtain 
\begin{align}
&Z_{\Lambda-1} = \int \left[\prod_{a=1}^Q df_{1,a}..df_{a,\Lambda-1}\right] e^{-\bar{S}_{\GP,\Lambda-1}} \\ \nonumber 
&\bar{S}_{\GP,\Lambda-1} = \sum_{k=1}^{\Lambda-1} \sum_{a=1}^Q \frac{f_{a,k}^2}{2\lambda_k} + Pe^{-\frac{1}{2} \Tr\log\left(\kappa^2 I+ \sum_{k=1}^{\Lambda-1} (\bm{f_k}-\bm{y_k})(\bm{f_k}-\bm{y_k})^T\right)}\times \\ \nonumber 
&\left[1+\Tr\left[\left(\kappa^2 I+ C_{\Lambda-1}\right)^{-1}\langle (\bm{f_{\Lambda}}-\bm{y_{\Lambda}})(\bm{f_{\Lambda}}-\bm{y_{\Lambda}})^T\rangle_{{\bm f_{\Lambda}} \sim {\cal N}(0,\lambda_{\Lambda}I_Q)} \right]\right]
\end{align}
where $\langle ... \rangle_{\bm{f}_{\Lambda} \sim {\cal N}(0,\lambda_{\Lambda}I_Q)}$ comes from integrating over/out the ${\bm f}_{\Lambda}$ using the action in Eq. \ref{Eq:SLambda} with the interaction appearing in \ref{Eq:RGTaylorExpansion} at $U=0$, where it becomes a single Gaussian field. This averaging yields $\lambda_{\Lambda} I + y^2_{\Lambda} \Gamma$ where $\Gamma_{ab}=1$. Next we use the fact that to leading order in $U$ we may undo the Taylor expansion of $e^{-\frac{1}{2}\Tr\log(..)}$ we obtain 
\begin{align}
\bar{S}_{\GP,\Lambda-1} = \sum_{k=1}^{\Lambda-1} \sum_{a=1}^Q \frac{f_{a,k}^2}{2\lambda_k} + Pe^{-\frac{1}{2} \Tr\log\left((\kappa^2+\lambda_{\Lambda}) I+ y^2_{\Lambda}\Gamma + \sum_{k=1}^{\Lambda-1} (\bm{f_k}-\bm{y_k})(\bm{f_k}-\bm{y_k})^T\right)}
\end{align}
where we recognize the appearance of a renormalized ridge ($\kappa^2_{\RG,\Lambda-1}=\kappa^2+\lambda_{\Lambda}$). 
This process can be repeated recursively, reducing $\Lambda$ to some $\Lambda'$, until we reach the point our perturbative assumption breaks down, namely $\lambda_{\Lambda'} \not \ll \kappa^2_{\RG,\Lambda'}$ or $y_{\Lambda'}^2 \not \ll \kappa^2_{\RG,\Lambda'}$. However, provided this does not happen too soon when we reach $\Lambda'$ we find an effective ridge ($\kappa^2_{\RG}$) equal to 
\begin{align}
\label{Eq:Effective_Ridge_RG}
\kappa_{\RG}^2 &= \kappa^2_{\RG,\Lambda'} =   \kappa^2+\sum_{k>\Lambda'}^{\Lambda}\lambda_k 
\end{align}
where it is sensible to choose $\Lambda'$ as the index associated with the minimal unlearnable mode, given $\kappa_{\RG}^2$. Thus, similar to Eq. \ref{Eq:CanatarEffectiveRidge}, determining what is $\Lambda'$ requires solving a non-linear equation. Here it is given by the above equation together with the requirement $P\lambda_{\Lambda'}/\kappa_{\RG}^2 = \epsilon$, where $\epsilon \ll 1$ is some arbitrary threshold value defining what we mean by an unlearnable mode.  

It is interesting to compare the above result with Eq. \ref{Eq:CanatarEffectiveRidge}. In both equations the contribution of unlearnable modes ($\kappa^2_{\eff/\RG}/P \ll \lambda_k$), to, $k_{\eff/\RG}^2$ is identical. Furthermore, stopping our RG when the above condition is violated, then using Eq. \ref{Eq:CanatarEffectiveRidge} with $\kappa^2=\kappa_{\RG}^2$ would lead to the same final result. However, provided $\kappa^2_{\RG}$ becomes large enough, we can simply carry on directly using EK or perhaps EK along with a perturbative correction. The latter approach was used in Ref. \cite{cohen2021learning}, although using a very different heuristic justification which was also limited to infinite fully connected networks, and led to good prediction for ridgeless GPR with the NTK. In Ref. \cite{howard2024}, it was also generalized to non-Gaussian $f_a(x)-y(x)$, where evidence for cross-talk at low dimensions and beyond spectral-bias effects appeared again. Interestingly, this entered as a $x$-dependent effective ridge parameter in the effective action.   

{\bf Related works.} Finally, we review several other works involving RG and neural networks based on several criteria (i) What is the scale or what is being integrated out? (ii) What is the microscopic (UV) theory on which one is applying RG? (iii) Is the induced RG flow tractable, or does it make some quantities more tractable? 

An early line of work sought to augment (or interpret) the action of a layer in deep neural networks so that it coarse-grains the data distribution, thus drawing an analogy with RG \cite{mehta2014exact,Koch_Janusz_2018,Beny:2013pmv}. Similarly, the action of a diffusion model can be interpreted as inverting an RG flow on the data distribution \cite{Cotler_2023}. The UV theories here are thus the data distribution and the IR theory consists of the relevant features in the data for the layer action and white noise data for the diffusion action. Another line of work \cite{Erbin_2022,erbin2023functional} 
views the distribution on functions induced by a random DNN as the UV theory, and uses RG formulations to treat non-Gaussian (finite-width) effects \cite{Halverson:2020trp, Grosvenor:2021eol,Roberts_2022} 
in this distribution. A route for RG where network depth is coarse-grained was put forward in Ref. \cite{Lee2024DynamicNeurons}. 
Another set of works relating RG flows and Bayesian Inference view both as forms of augmenting a probability distribution  \cite{berman2022dynamics,berman2022inverse,Berman_2023,berman2024ncoder,howard2024bayesianrgflowneural}. In this approach however, reducing data generates the RG flow, so the true data-generated model (or target function) is the UV theory, while the  IR theory is a random DNN, so that observables that are consistently reproduced along the flow are the most unlearnable modes.
To the best of our understanding, these works have yet to analytically predict the generalization performance of neural networks. Finally, an earlier work \cite{Bradde_2017} models the data distributions, irrespective of any neural network, as a field theory performs RG by removing large PCA components. Trading PCA with kernel-PCA, both that work and the current work share the same notion of UV and IR modes, however focus on very different ensembles/probability distributions. In the analysis above, the UV theory is the distribution over outputs generated by an ensemble of trained neural networks at a finite amount of data, while the IR is the theory for the learnable modes. To generate an RG flow, we integrate out unlearnable output modes and track their renormalizing effect on the learnable modes.

\subsection{Revisiting the optimization problem}
At this point we can offer a quantitative solution to one of the fundamental questions in deep learning--- given that so many zero train loss minima exist, and that DNNs are generally capable of learning any function \citep{HORNIK1989,zhang2021understanding}, what biases them during optimization to choose the well-generalizing ones? 

Technically, we have so far seen that NTK dynamics leads to an output which is the sum of two terms: (i) The average GPR predictor using NTK kernel with zero ridge and (ii) the $I_0$ fluctuation term which corresponds to the average GPR predictor using the NTK kernel trying to learn the initial output of the trained DNN. The first term will exhibit a good learning curve (drop of error with $P$) if the target is supported on kernel eigenvalues ($\lambda$) obeying $\lambda \gg \int d\mu_x K(x,x)/P > \sigma_{\eff}^2/P$. The second would be small if the outputs at initialization show similar support. 

A more tangible viewpoint of how implicit bias is generated by the dynamics can be obtained as follows. Revisiting the NTK section, one notes that the NTK kernel is proportional to $J^T(x) J(x')$ where $J_{\alpha}(x) = \partial_{\theta_{\alpha}} z_{\theta}(x)$. Thus the NTK $P$-by-$P$ matrix' eigenvalues are the square of the Singular Values of the generally rectangular matrix $[J^T]_{\mu \alpha}=\partial_{\theta_{\alpha}}z_{\theta}(x_{\mu})$. Specifically, we can associate a weight-space singular vector $v$ of $J^T$, with an eigenvector of the NTK matrix ($\phi_{\mu}$) via $\phi = J^T v$, as one can verify algebraically. Next, we note that as $P$ grows large, one generally expects these matrix eigenvalues to be $P$ times the corresponding NTK operator eigenvalues w.r.t. to the data measure (indeed one can show that the discrete eigenvalue equation tends to the operator eigenvalue equation in this limit). Similarly, we expect matrix eigenvectors would be close to NTK-operator eigenfunctions sampled on the data. We thus associate a high singular vector of $J^T$ ($v_{\alpha}$) with a high-eigenvalue eigenfunction of the NTK operator ($\phi(x)$), via the $P \rightarrow \infty$ limit of $\phi =J^T v$. Finally, viewing $J^T$ as the linear Taylor expansion coefficient of network outputs around the initial weights, having $v$ with a large eigenvalue implies that a small motion in weight space along this direction produces a relatively large change in output along this functional direction. Thus we need to travel less in weight space to generate functions associated with high NTK-operator eigenvalues. This biases the optimization procedure towards loss minima which utilize these types of functions.  

\section{Application: Symmetries and spectral bias}
\label{Sec:Symmetries}
One of the main lessons from the previous section is that over-parametrized DNNs in standard scaling \footnote{i.e. pre-activations and outputs being $O(1)$ at initialization or according to the prior as considered so far}, often \footnote{At high input dimension or large ridge.} have an implicit bias (a.k.a. spectral bias) towards learning function associated with high eigenvalues of the kernel. Thus, if we had the diagonalization data for the kernel  $(\lambda_k,\phi_k(x))$ on the true data measure and the true structure of the target $y(x)$ for all $x$, we could predict the training dynamics, the learning curves, and the test loss. Conveniently, all the complex dynamics involved in doing inference resulted in a linear high-pass filter in this $\phi_k(x)$ basis. 

We begin with a discussion putting these results in practical context. The obvious elephant in the room is that the above diagonalization data is analytically intractable and computationally costly. Furthermore, there is a conceptual difficulty which is that, while we can readily obtain concrete expressions for the kernel \citep{lee2019wide}, the data measure is only partially accessible to us, as we only see the finite dataset. We can gather more data points if needed, but this still falls short of a concrete expression for $p(x)$, the true distribution of the data. In practice, however, the high and learnable $\lambda_k$'s are well resolved by the finite data available (i.e. would change negligibly if we add more data). Furthermore, corresponding eigenvectors can be seen as discrete samples of $\phi_k(x)$. \footnote{These can be extended into functions using GPR with $K$ as the kernel and $\phi_k(x)$ as the target.} Thus, we can get numerical access to the dominant kernel eigenmodes at an $O(P^3)$ (matrix diagonalization) computational cost. The more problematic issue is that there is no real computational or interpretation-wise benefit in diagonalizing a kernel versus doing actual GPR. In fact, we mainly replaced the complexity of understanding DNN training with the complexities of characterizing a set of functions  ($\phi_k(x)$) in high dimension. 

However, the picture is also not as stark as the above implies. For instance, ({\bf i}) considering toy target functions and toy datasets \citep{cohen2021learning,Canatar2021,lavie2024towards,yang2019fine}, $\phi_k(x)$'s can be understood analytically, and spectral bias becomes tractable. As such toy data models capture qualitative aspects of real-world DNN behavior (e.g. effects of depth on over-fitting \citep{yang2019fine}, relating power laws in performance to power laws in the data \citep{Bahri2024Explaining}, and relations between architecture choices, symmetries in the data, and performance \citep{novak2018bayesian})--- spectral-bias provides qualitative explanations for phenomena occurring in real world data. As we improve the theoretical modeling of data \citep{Francesco2024}, we may expect this portfolio of applications to increase, with spectral-bias providing the link between data, kernel, and performance.  ({\bf ii}) Turning from predictions to bounds, Ref. \cite{lavie2024symmetric} derived a learnability lower bound on the number of samples ($P_*$) involved in learning target functions which are sums of $O(1)$ different $\phi_k(x)$'s. Importantly, the $\phi_k(x)$ can be derived from any convenient, $p_{\text{ideal}}(x)$ while the learnability bound applies to any $p(x)$. This allows, for instance, to bound the time it would take a transformer in the NTK regime acting on standard Language data to learn ``copying heads'' \citep{olsson2022context}, an important target component which aids in-context learning. 

The above discussion motivates the need to diagonalize kernels on relatively simple data measures, as means of understanding behavior on real-world data. This has worked out for datasets which are uniform on the hypersphere \citep{Azevedo2015EigenvaluesOD}, hypercube \citep{yang2019fine}, or for sequence data that is permutation invariant \citep{lavie2024towards}. Here we explain in some detail the case of the hypersphere using the language of representation theory, which carries through to the other two cases as well. 

Consider a fully connected DNN with $x \in \R^{d_{\text{in}}}$ and $p(x)$ obeying $p(x)=p(Ox)$ where $O$ is any orthogonal transformation from the orthogonal group ${\rm O}(d_{\text{in}})$ represented by a rotation matrix $O$. As discussed briefly in Sec. \ref{Sec:Intro_Measure}, an FCN kernel together with such a measure constitutes a symmetric operator ($\hat{K}$) acting on function space. Next, we promote $O \in {\rm O}(d_{\text{in}})$ to a linear operator on function space via $\hat{O} f(x)\equiv f(Ox)$ as well as $\hat{O} K(x,y)=K(Ox,y)$ and $K(x,y)\hat{O}=K(x,yO)=K(x,O^T y)$. These definitions together with the symmetry of $K$ ($K(Ox,Oy)=K(x,y)$) imply that the commutation relation $[\hat{K},\hat{O}]\equiv \hat{K}\hat{O}-\hat{O}\hat{K}$ vanishes. When two matrices/operators commute, they preserve each other's eigenspaces. When an operator $\hat{K}$ commutes with an entire group (e.g. ${\rm O}(d_{\text{in}})$) it preserves its {\it irreducible representations} (irreps), which we next define. 

An irreducible representation $R_{\alpha}$ of a group (where $\alpha$ enumerates all possible irreps) associated with a subspace of dimension $d_{\alpha}$, is {\bf (1)} A representation: a set of $d_{\alpha} \times d_{\alpha}$ unitary matrices, one for each element $O \in {\rm O}(d_{\text{in}})$, which obey the algebra of ${\rm O}(d_{\text{in}})$ (i.e. the matrix representing $OO'$ equal the matrix representing $O$ times the matrix representing $O'$.) {\bf (2)} Irreducible: There is no unitary transformation that would transform all representing matrices $O$ to a block diagonal form with identical block structure and more than one block. A block diagonal matrix could be decomposed into two matrices, each acting on one of the subblocks; it would be reducible in this sense. Namely, there is no smaller subspace of this ${\R}^{d_{\alpha}}$-space on which these matrices could be defined. 

The benefit of irreducible representations is that {\bf (1)} function space can be split into a 
linear sum of the spaces associated with each irreducible representation. {\bf (2)} If each irreducible representation ($R_{\alpha}$) appears only once in the previous linear sum, $\hat{K}$ is block diagonal and proportional to the identity matrix within each such subspace \footnote{The latter follows from Schur's lemma, using the linear mapping $\hat{K}-\lambda I$, where $\lambda$ is any one of $K$'s eigenvalues.}. %{\bf (3)} If an irreducible representation ($R_{l}$) appears $m$ times in the above sum, then a unitary transform acting on this joint $R^{m d_{\alpha}}$-space can be found such that $\hat{K}$ acting on this space consists of $m$-blocks of size $R^{d_{\alpha}}$ and proportional to the identity matrix within each block. 
As would be elaborated upon in the example below, since $\lambda_k \geq 0$ and $\sum_k \lambda_k=\int d\mu_x K(x,x)=O(1)$, we have a "spectral budget". Irreps of size $d_{\alpha}$, having degenerate eigenvalues, must be associated with eigenvalues of the order $O(1)/d_{\alpha}$ to respect this spectral budget. 
Last we note that representation theory is a well-developed mathematical field \citep{fulton_representation_2004} with many concrete results and practical tools which enable one to find concrete data on these irreducible representations. 

Focusing back on our FCNs, let us take $p(x)=V_{d_{\text{in}}}^{-1}\delta(|x|-1)$ where $V_{d_{\text{in}}}$ is the volume of a hypersphere in $d_{\text{in}}$-dimension. In this case, the space of functions on this hypersphere is spanned by irreducible representations known as hyper-spherical harmonics. Representations are labelled by $\alpha \in {\cal N}_+$, and correspond to homogeneous polynomials of degree $\alpha$ which are in the null-space of the Laplacian operator \citep{Frye2012}. For instance, $\alpha=0$ has $d_{\alpha=0}=1$ and correspond to the constant function, $\alpha=1$ has $d_{\alpha=1}=d_{\text{in}}$ and corresponds to all linear functions, $\alpha=2$ has $d_{\alpha=2}=(d+2)(d-1)/2$ and correspond to the polynomials $x_i x_j$ with $i \neq j$ as well as $x^2_i-|x|^2/d_{\text{in}}$ for $i \in [1..d_{\text{in}}-1]$. One can manually verify that these indeed obey the above requirement of an irreducible representation. More generally, $d_{\alpha}=\frac{2\alpha+d_{\text{in}}-2}{\alpha+d_{\text{in}}-2}\binom{\alpha+d_{\text{in}}-2}{\alpha}$ which scales as $d_{\text{in}}^{\alpha}$ for $\alpha \ll d_{\text{in}}$. Since each such irrep appears only once when spanning the space of functions on the hypersphere, we immediately have that $\hat{K}$ is given by $\lambda_{\alpha}$ within each $d_{\alpha}$ block. 

Next, we can relate the degeneracy or dimension of the block associated with an irrep to its kernel eigenvalue $\lambda_{\alpha}$. Indeed, working with a normalized kernel we take $K(x,x)=O(1)$\footnote{Within an EK approach, the meaningful scale for the average predictor is $K(x,x)/\kappa^2$. Within the effective ridge approach, the ridge would scale as the kernel, hence the scale has no impact on the average predictor and can just as well be taken to $1$} and consequently $\Tr[K]=\int \dif \mu_x K(x,x)=\int \dif \mu_x \lambda_k \phi_k(x) \phi_k(x)=\sum_k \lambda_k = O(1)$. Since $K(x,y)$ represents a covariance, it is a positive semi-definite operator and hence $\lambda_k \geq 0$. Consequently, we have that, within each irrep block $d_{\alpha} \lambda_{\alpha}\leq \Tr[K] \leq O(1)$. Thus, $\lambda_{\alpha}$ scale as $1/d_{\alpha}$. 

We thus obtain an important insight regarding inference with FCNs on high dimensional spaces. Let us analyze this from the effective ridge viewpoint. Say we wish to learn a target function $y(x)$ which is a degree $q$ polynomial in the span of the $\alpha=q$ representation (i.e. a degree $q$ polynomial which vanishes under the Laplacian). For $P$ at which $y(x)$ starts becoming learnable, we can use Eq. \ref{Eq:Effective_Ridge_RG} with $\Lambda'$ set to the first index associated with the $\alpha=q$ irrep, to determine $\kappa_{\eff}^2$. For such $P$, it would be of the scale $d_{\alpha}\lambda_{\alpha}=O(1)$ or larger, where $\lambda_{\alpha}$ is the eigenvalue associated with the $\alpha=q$ representation. Since $\lambda_{\alpha}=O(d_{\text{in}}^{-q})$ we need $P_* > O(\kappa_{\eff}^2/d_{\text{in}}^{-q}) \approx O(d_{\text{in}}^q)$ samples to start learning such modes. Thus FCNs a have strong spectral bias towards extrapolating between training points using low-order polynomials.

Turning to real-world data, consider taking some datasets like imageNet and normalizing all image vectors to have a fixed norm. Since this only puts a single constraint on the image (and also just implies an overall change of brightness) we expect this to have a minor effect of performance. Interestingly, the above results for $P_*$ hold for any FCN even if the data measure is not uniform. Following Ref. \cite{lavie2024symmetric}, the main change for an $O(1)$ target is that the above condition on $P_*$ becomes $P_* > d_{\text{in}}^q[\int V_{d_{\text{in}}}^{-1} \delta(x- |x|)  y^2(x)] /[\int \dif \mu_x y(x)]$. Thus, provided that the normalization of $y(x)$ under the uniform/idealized measure does not scale very differently than under the true measure, the same scaling of $P_*$ with the degree of the polynomial holds. Though imageNet is far from being uniform on a hyper-sphere, it is highly plausible that low order polynomials such as, say, the red-color of the middle pixel times the blue-color of two of its adjacent pixels, would have the same norm under both. This, however, implies, that such a simple local filter would require $P_*$ equal to the dimension of ImageNet to the third power, which is astronomical. Indeed, however, FCN do not yield good results on ImageNet, CNN do. Taking CNN with non-overlapping convolutional-kernel windows, one obtains a much smaller symmetry group proportional to rotation within each patch \citep{naveh2021self}. These smaller groups would have $d_{\alpha}$ scaling as the patch dimension (typically 27) to the power of the polynomial rank, making learning such local features feasible with $27^3 \approx 20k$ data points. The first steps towards applying a similar approach to Transformers, taking the idealized dataset measure as permutation invariant in the sequence index, have been taken in Ref. \cite{lavie2024towards}.  


\section{Application: Scaling laws}
\label{Sec:ScalingLaws}
A question of central practical importance is how to predict the performance of a large trained model, requiring expensive computing, based on the training of smaller, cheaper, ones. An interesting empirical observation \citep{kaplan2020scalinglawsneurallanguage} found in training large language models is that performance as a function of model size and training set size, is well approximated by a sum of two power-law decays (a scaling law) specifically 
\begin{align}
{Loss}(P,N_{\text{params}}) &= \left[ \left(\frac{N_0}{N_{\text{params}}}\right)^{\frac{\alpha_{N}}{\alpha_P}}+\left(\frac{P_0}{P} \right) \right]^{\alpha_{P}}
\end{align}
where $N_{\text{params}}$ is the number of model parameters, $N_0,P_0$ are fitting constants, and $\alpha_P$ ($\alpha_N$) are the exponents associated with the decay w.r.t $P$ at $N_{\text{params}}
\rightarrow \infty$ (w.r.t. $N_{\text{params}}$ at $P \rightarrow \infty$). 

While making analytical predictions on realistic large language models (LLMs) is challenging, those models too have a GPR limit when the number of attention heads goes to infinity. One can therefore wonder to what extent can scaling laws can be explained using the tools developed in this chapter. This has been studied in Ref. \cite{Bahri2024Explaining}, under the reasonable assumption that the spectrum of the kernel operator is power-law distributed namely 
\begin{align}
\lambda_k &= \lambda_1 k^{-1-\alpha} \,\,\,\ \alpha > 0 
\end{align}
This assumption is loosely related to Zipf's law scaling of word frequencies in natural languages or power-law decay of PCA data on images and can be verified experimentally. We also assume that the target is similarly distributed namely that $y^2_k \approx y_1^2 k^{-1-\alpha}$. 

Next, we show how the power-law $\alpha$ is related to $\alpha_P$ in the large ridge regime (EK limit) and in the vanishing ridge regime.  
To this end we introduce the notion of a threshold mode $k_T$ that distinguishes the learnable modes $k < k_T$ from those that are not learnable $k \ge k_T$. A reasonable choice for this mode is given by the point where $\kappa^2/P = \lambda_{k_T}$ at which the learnability factor in \ref{Eq:EK_Av_Pred} is $1/2$. So $k_T = (P/\kappa^2)^{1/(1+\alpha}$.

Given our assumption on the target, we can estimate the loss that is dominated by the discrepancies of the unlearnable modes ($\sim y_k$) via $\sum_{k > k_T}^{\infty} y^2_k \approx \int_{k_T}^{\infty} \dif k \; k^{-(1+\alpha)} \propto k_T^{-\alpha} \propto P^{-\alpha/(1+\alpha)}$. Therefore, we find that $\alpha_P=\alpha/(1+\alpha)$. 

Next, we consider how the effective noise approach (specifically in the RG approximation) changes this scaling for $\kappa^2=0$. After integrating out all modes down to some $\Lambda'$ cutoff, we obtain $\kappa^2_{\text{RG}}=\sum_{\Lambda'}^{\infty} k^{-(1+\alpha)} \approx \alpha^{-1} (\Lambda')^{-\alpha}$. The RG flow remains tractable, in the sense of maintaining $\lambda_{\Lambda'}/\kappa^2_{\RG,\Lambda'} \ll 1$, as we have $\kappa^2_{\text{RG},\Lambda'} \propto (\Lambda')^{-\alpha} \gg \lambda_{\Lambda'}=(\Lambda')^{-1-\alpha}$. At this point, we undertake a self-consistent assumption that the growth of $\kappa_{\text{RG},\Lambda'}^2$ puts us sufficiently close to the EK limit so that we can evaluate the threshold for learnable modes via $P=\kappa_{\text{eff}}^2(k_T)/\lambda_{k_T}\approx \alpha^{-1} k_T^{-\alpha}/k_T^{-(1+\alpha)}=\alpha^{-1}k_T$. This leads to a reasonable result which shows that the number of learnable modes scales linearly with the dataset size. Evaluating the MSE loss by resumming all unlearnable modes of $y_k$, we now obtain $\sum_{k > k_T}^{\infty} y^2_k \approx \int_{k_T}^{\infty} \dif k \; k^{-(1+\alpha)} \propto k_T^{-\alpha} \propto P^{-\alpha}$, which is the data exponent $\alpha_P$ (there $\alpha_D$) found in \cite{Bahri2024Explaining} using asymptotic properties of the hypergeometric function obtained from the explicit solution of the self-consistent equations of \cite{Canatar2021} (namely Eqs \ref{eq:barf},\ref{Eq:CanatarEffectiveRidge}).
\begin{figure}
    \centering
    \includegraphics[trim=1.8cm 0 5cm 6cm, clip, width=1\textwidth]{Chapters/figures/RG_figs.pdf}
    \vspace{-2cm}
    \caption{Gaussian Processes Regression on four 10k binary CIFAR and MNIST datasets, at $\kappa^2=1e-8$. Experimental results (dots) match well both the effective ridge theory and the RG theory. In the latter, we took $0.01$-learnability as marking the RG cut-off. We comment that results are similarly accurate for $T=0.001$ and $T=0.1$. The Equivalent Kernel estimator is expected to become accurate when the loss reaches the scale of $\kappa^2$, explaining its poor performance in the shown range of $P$. }
    \label{fig:RG_CIFAR_MNIST}
\end{figure}

Next we return to verify the closeness of the system to the EK limit. We highlight here that the EK approximation comes from the Taylor expansion of $\int d\mu_x e^{-(f(x)-y(x))^2/2\kappa^2}$. Therefore, we need to compare $\kappa_{\text{eff}}^2$ with the mean MSE loss that we have obtained. The relation $\kappa_{\text{eff}}^2=\alpha^{-1} k_T^{-\alpha}$ places $\kappa^2_{\text{eff}}$ at the same scale as the loss thus placing the system at the threshold of the EK approximation, where it is expected to provide a correct order of magnitude estimate. 

This exposes direct relations between power-laws in the data (the exponent $\alpha$) and power-laws in the learning curve via spectral bias. Thus, at least at a qualitative level, reduces the mystery of scaling-laws in DNN performance to that of scaling-laws in data. It further exemplifies a level of universality since, all the information required to model the complex and unknown target function and data measure is one parameter, namely $\alpha$. It also suggests that a finer understanding of scaling laws (e.g., addressing the differences found empirically between $\alpha_N$ and $\alpha_P$ which the GPR approach fails to capture) might be within our analytical grasp. 


