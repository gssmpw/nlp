\chapter{Preliminaries}
\label{c-Preliminaries} In this chapter we establish notation, manage expectations, and give a basic introduction to the analytical tools we require. The tools introduced are quite diverse, spanning math and physics topics. Rather than aiming to make the reader confident in them, the purpose here is to give the minimal user interface required for the analysis of trained neural networks carried out in later chapters. Readers looking to establish their knowledge of Replicas, Path-Integrals, and Gaussian Processes can consider the following introductory books   \cite{MezardBook,schulman1996,Rasmussen2005}.

\section{Motivation}
The goal of this review is to demonstrate how field theory and statistical physics provide a rich framework that may, in the future, accommodate a unified theory of deep learning. This then brings into mind questions concerning the need for such a theory and its general contours. 

In traditional scientific disciplines, simple universal results are celebrated. Snell's law describing light scattering, Carnot's optimal engine efficiency, and the universal scaling of magnetic clusters in Ising-like transitions lead to concrete and universal predictions. Unlike naturally occurring systems, virtual ones, such as deep neural networks, have been post-selected by software engineers to adapt to various complex learning settings. They are therefore more akin to biological systems than to physical systems, where complexity is a fact of life rather than an obfuscation of a simple underlying phenomenon.  

To further illustrate this point, let us imagine one finds something similar to Schr√∂dinger's equation (a linear partial different equation) as a universal description of a trained transformer network. This would be very convenient to theorists, as we have various analytical and numerical tools to analyze linear differential equations. However, it would also mean that a transformer is equivalent to a linear model. However, linear models were studied extensively in machine learning and did not lead to transformer-grade performance. It is therefore difficult to believe that such a linear theory can exist. This fleshes out a tension between analytical solutions which, by virtue of being called ``solutions'', should be computationally simple or ``linear'' and a theory capable of tracking deep learning.

The above explainability paradox is not new to physicists studying complex systems and various workarounds exist based on averaging, universality, dimensional reduction, modularity, and asymptotic limits. For instance, while SAT-3 is an NP-hard problem, various computational techniques borrowed from spin-glass theory can accurately predict thresholds, in terms of the ratio of variables and clauses/constraints, at which large-scale SAT-3 instances become easy to solve. These theories escape the paradox of finding an analytical solution to an NP-hard problem by averaging over ensembles of problems instead of solving a particular SAT-3 instance. Still, they provide useful statistical rules for determining whether a typical instance is solvable. This is also an example of dimensional reduction since out of the many parameters defining a SAT-3 instance, only a single quantity, the above mentioned variable to clauses ratio, controls hardness in large-scale SAT-3 problems \citep{MezardBook}. Similarly, Random Matrix Theory \citep{Potters_Bouchaud_2020} provides accurate statistical information on the spectrum large matrices (e.g. Wigner's semi-circle law) while circumventing the $O(n^3)$ computational difficulty of diagonalizing $n$ dimensional matrices. This example also showcases universality, as various changes to the statistics of random matrix elements still lead to the same distribution law for eigenvalues. 

In building a theory of deep learning, one needs to negotiate these difficulties and tensions. One should also be wary of a common pitiful in which the analytical ``solutions" offered by theories involve equations which require expensive numerical solutions, potentially more complex than training the networks themselves. 

Optimism can however be drawn from various successful recent outcomes, some of which are described in detail in this review. In particular {\bf (i)} A linear description of highly overparametrized DNNs (the {\it GP limit}) which, despite describing a less powerful regime of deep learning, provides insights on fundamental questions about overparametrization and allows rationalizing architecture choices. {\bf (ii)} Analytically inspired recipes for {\it hyper-parameter transfer} between small and large networks, which are based on solutions of the very short time dynamics yet appear to apply throughout training. {\bf (iii)} The empirical observation of {\it scaling laws} where, based on only two constants (and faith in the scaling law), one can predict the performance of ChatGPT and other real-world networks aiding the allocation of computation resources. We shall further explain how these predictions can be derived within our field-theory framework. 


\section{Notation}
The following notation will be used throughout this review. 
\begin{center}
\begin{tabular}{ |c|c||c|c| } 
 \hline
 Width & $N$ & Number of channels & $C$ \\ 
 Input dimension & $d$ & Training set size & $P$ \\ 
 Data indices & $\mu,\nu,\chi,\xi$ & Neuron indices & $i,j$ \\ 
 Feature indices & $k,q$ & Replica indices & $a,b$ \\ 
 Learnable parameters & $\theta$ & Readout layer weights & $a_i$ \\ 
 Hidden layer weights & $w^{(l)}_{ij}$ & Learning rate & $\eta$ \\ 
 Ridge parameter & $\kappa^2$ & Weight variances & $\sigma_a^2,\sigma_{w^{(l)}}^2$ \\
 Training points & $x_{\mu}$ & Test point & $x_*$ or $x_0$  \\
 NNGP Kernel & $K_{\mu \nu}$ & NTK Kernel & $\Theta_{\mu \nu}$ \\Regression Target & $y(x)$,$y_{\mu}$ & Data measure & $\dif\mu_x = \dif x \; p(x)$ \\
 Gaussian Dis. in $x$ & ${\cal N}(\mu,\Sigma;x)$ & Pre-activations & $h_{i,\mu}$, \; $h_i(x)$ \\
 Network outputs & $f_{\mu}$,$f(x)$ & Determinant & \text{det}(..) \\
 Activation function & $\sigma(...)$ & $\kappa_{\mu_1...\mu_n}$ & $n$'th cumulant \\
 \hline
\end{tabular}
\end{center}

\section{Gaussian Integrals}
Multivariate Gaussian integrals are central to field theory. Here we recall the definition of those and several useful identities, in particular Wick's theorem and square completion. 

We denote a multivariate Gaussian distribution of $w \in R^d$ with mean $\mu$ and covariance matrix $\Sigma$, by $\mathcal{N}[\mu,\Sigma;w]$ where 
\begin{align}
\mathcal{N}[\mu,\Sigma;w] &\equiv \frac{1}{\sqrt{(2\pi)^d \det(\Sigma)}} e^{-\frac{1}{2} [w-\mu]^T \Sigma^{-1} [w-\mu]}
\end{align}

{\bf Marginalization.} Given a Gaussian $\mathcal{N}[\mu,\Sigma;(w_1..w_d)]$, the marginalzied probability distribution for $w'=(w_1..w_{d'<d})$ is easily written in terms of the $d'\times d'$ matrix $\Sigma'_{ij}\equiv\Sigma_{ij}$ ($i,j \in [1..d']$) 
\begin{align}
\int dw_{d'+1}..dw_d \mathcal{N}[\mu,\Sigma;w] &= \mathcal{N}[\mu',\Sigma';w']
\end{align}
where $\mu'=(\mu_1...\mu_{d'})$. 

{\bf Expectation values.} Any finite moment of a Gaussian random variable can be calculated using Wick's-Isserlis' theorem. Denoting the moment by $w_{i_1}...w_{i_n}$, we consider all partitions of the indices $i_1...i_n$ as follows: We first split the indices into one set $A$, then the remaining indices into all possible pairs. Each of these possible joint splits is denoted by $A,P$. The n-th moment can be expressed as the sum of all such splits, 
\begin{align}
\label{Eq:WickWithAverage}
\int \prod_{j=1}^n dw_{i_j} \mathcal{N}[\mu,\Sigma;w] w_{i_1}...w_{i_n} &= \sum_{A,P \in Splits} [\prod_{a \in A} \mu_a][\prod_{p \in P} \Sigma_{p_1 p_2}] 
\end{align}
where $p$ is the set of pairs in $P$. For instance, say we have the moment $w_1 w_2 w_3 w_4$ we find 
\begin{align}
&\int \prod_{j=1}^4dw_j \mathcal{N}[\mu,\Sigma;w] w_{1}w_{2}w_{3}w_4 = \mu_1 \mu_2 \mu_3 \mu_4 \\ \nonumber 
&+ \mu_1 \mu_2 \Sigma_{34} + \mu_1 \mu_3 \Sigma_{24} + \mu_1 \mu_4 \Sigma_{23} + \mu_2 \mu_3 \Sigma_{14} + \mu_2 \mu_4 \Sigma_{13} + \mu_3 \mu_4 \Sigma_{12} \\ \nonumber
&+ \Sigma_{12}\Sigma_{34}+\Sigma_{13}\Sigma_{24}+\Sigma_{14}\Sigma_{23}
\end{align}
The common use case for this formula is when $\mu=0$, where it amounts to splitting the indices into all possible pairs. 

{\bf Square completion.} We often encounter integrals of the following form, say when using Fourier transforms (moment generating function)
\begin{align}
\int dw \mathcal{N}[0,\Sigma;w] e^{-k^T w} &= \int dw \frac{1}{\sqrt{(2\pi)^d ||\Sigma||}} e^{-\frac{1}{2} w^T \Sigma^{-1} w-k^T w} = e^{\frac{1}{2} k^T \Sigma k}
\end{align}
where we completed the square (absorbed $k^T w$ into the quadratic piece) and used the freedom to shift the integration counter. Used with complex $k$ ($k \rightarrow ik$, one recognizes that the above l.h.s. is the Fourier transform of a Gaussian, thereby yielding the familiar result that the Fourier transform of a multivariate Gaussian is an (unnormalized) multivariate Gaussian with the inverse covariance matrix.


\section{Network Definition and Training Protocol}
\label{Sec:TrainingProtocols}
Here we define, very briefly, the mechanical (or algorithm) aspects of deep learning. A more thorough introduction appears in Ref. \cite{nielsenneural}.  

For simplicity, we focus our exposition on fully connected networks (FCNs) with $L$ layers and $N_l$ neurons in each layer. This is the simplest deep neural networks (DNNs) architecture, we will later comment on generalizations to Convolutional Neural Networks (CNNs) and Transformers. We denote by $x \in R^d$ the input of a network, $z_i^{(l)}(x)$ denotes output of the $l$'th layer on the $i$'th neuron, and take $z_i^{l=0}(x)=x_i$. The network is defined recursively by 
\begin{align}
z^{(l)}_{i}(x) &= \sigma(h_{i}^{(l)}(x)+b^{(l)}_i) \\ \nonumber 
h^{(l)}_{i}(x) &= \sum_{j} W^{(l)}_{ij} z^{(l-1)}_j(x) 
\end{align}
where $W^{(l)}_{ij}$ and $b^{(l)}_i$ are respectively the weights and biases (which we often refer to just as weights) and $\sigma(...)$ is the activation function. We encapsulate all weights and biases into a vector $\theta_{\alpha}$ where $\alpha$ is an abstract index spanning all the variables defining the network. The network's output is $f(x)=h^{(L)}(x)$

Considering a supervised learning problem where given a dataset $\mathcal{D} =\{(x_\mu, y_\mu)\}_{\mu=1}^P$ of input and output pairs where the task is either regression or classifications of new examples not in the data set. To achieve this task, most DNNs are trained to minimize a scalar quantity known as the loss function ($\cL$) given by a sum over all training points. 

{\bf MSE loss.} This loss is a common choice for regression problems 
\begin{align}
\mathcal{L} &= P^{-1}\sum_{\mu=1}^P |f(x_{\mu})-y_{\mu}|^2
\end{align}
where $P$ is the number of training points, $y_{\mu} \in R^{d_o}$ (where $d_o$ is the output dimension) is the regression target function (or vector function for $d_o>1$). Alternatively, one may treat $y(x)$ as a one-hot encoding of categorical labels taking $d_o$ distinct values via $[y(x)]_i = \delta_{i c(x)}$ where $c(x) \in [1..d_o]$ assigns a serial number for the category of $x$. Last, in binary classification problems, one can take $d_o=1$ and $y(x)=c(x)$ where $c(x) \in 
\{+1,-1\}$. 

{\bf Cross entropy loss.} This loss is another common choice relevant to classification problems. Here we consider again $f(x)$ as a vector of dimension $d_o$ and write the loss as  
\begin{align}
\cL &= -\sum_{\mu=1}^P \left[\log(e^{-f_{c(x_{\mu})}(x_{\mu})})-\log(\sum_{l=1}^{d_o} e^{-f_o(x_{\mu})})\right]
\end{align}

{\bf Weight decay.} To both these losses, one may add weight decay terms of the form $\gamma_{\alpha}\theta^2_{\alpha}$, to regulate the overall weight norm. 

Many DNNs perform better in the over-parametrized regime where the number of network parameters exceeds the number of training points. For networks with scalar outputs ($d_o=1$), fitting perfectly a single data-point can be seen as placing a single constraint in weight space. Hence in the over-parametrized setting, finding $\theta$'s which fit the training set perfectly is an unconstrained optimization problem. As appreciated even before the recent rise of deep learning \citep{Breiman2018ReflectionsAR} and later supported by various theoretical works  \citep{Dauphin2014,Choromanska2014,Jacot2018} such optimization problems tend to be simple. While different optimization algorithms (e.g. Gradient descent, stochastic Gradient descent (SGD), Adam \cite{kingma2017adammethodstochasticoptimization}) may excel at bringing us faster to the minima or provide some implicit bias towards wider/better minima, the success of deep learning seems to be more generic than the peculiarities of each algorithm. Hence, we find it reasonable to focus on two relatively tractable algorithms.

{\bf Gradient flow.} Is a continuum idealization of the more standard discrete Gradient Descent algorithm. In deep learning jargon, it corresponds to full batch Gradient Descent at vanishing step-size/learning-rate. It is described by the partial differential equation 
\begin{align}
\frac{\dif}{\dif t} \theta_{\alpha}&= -\eta\partial_{\theta_{\alpha}} \cL
\end{align}
where $\eta$ is the learning rate. While in discrete gradient descent, $\eta$ may induce qualitative effects in standard/discrete gradient descent \citep{lewkowycz2020large}, in the above, continuum idealization, it only sets the time scale and may be absorbed into a re-definition of $t$. We note by passing that some effects of finite learning rate can be represented with the continuum description in the form of augmenting the loss with additional gradient terms \citep{Barrett2020,Smith2021}. 

{\bf Langevin dynamics} \citep{Welling2011,williams1996computing}. Another tractable form of dynamics is the following stochastic differential equation 
\begin{align}
\frac{\dif}{\dif t} \theta_{\alpha}&= -\eta\partial_{\theta_{\alpha}} \cL + \sqrt{2 T \eta} \xi_{\alpha}(t) 
\end{align}
where the gradient noise, $\xi_{\alpha}(t)$, is an an uncorrelated Gaussian process obeying $\langle \xi_{\alpha}(t) \rangle = 0, \langle \xi_{\alpha}(t) \xi_{\beta}(t') \rangle = \delta(t-t') \delta_{\alpha \beta}$ where $\delta(t)$ is the Dirac delta function. Unlike gradient flow, the above dynamics loosely mimic the stochastic nature of the dynamics. Using standard results from the theory of Langevin equations, the distribution induced on weight space by this dynamics as $t \rightarrow \infty$ (i.e. equilibrium distribution) is given by \cite{gardiner2010stochastic}
\begin{align}
\label{Eq:BoltzmannDistWeights}
P_{\infty}(\theta) &\propto e^{-\cL/T}
\end{align}

Several doubts could be raised regarding the ability of this process to reach equilibrium at reasonable times. As argued earlier, the ruggedness of the loss landscape is less of a concern in the over-parametrized regime, however diffusion in the high-dimensional near zero loss manifold may potentially result in slow equilibration for some observables, as well as possible entropy barriers. In practice, one finds good support for ergodicity, at least for relevant observables such as network outputs and pre-activations. In particular relaxation times for the loss are of a similar scale as relaxation times for various macroscopic network observable (i.e. involving a summation of many statistically equivalent parts) see for example \cite{naveh2021predicting}. In addition, \cite{LiSompolinsky2021, ariosto2022statistical,naveh2021predicting} show that theoretical descriptions offered by $P_{\infty}(\theta)$ provides accurate predictions on DNNs trained for a reasonable amount of epochs when taking into account the use of full-batch and low learning rates. 

\section{Infinite Random Neural Networks}
\label{Sec:InfiniteRandomDNNs}
Neural networks are, at face value, a very complex ansatz for functions. Grasping the statistical relations between weights and outputs is generally a complex task. Here we discuss some simplifications which occur in the limit of highly over-parametrized neural networks, which would be useful in the next chapters.  

As DNNs, per given weights, define a function, random DNNs define a distribution over functions. Interestingly, for all relevant DNN architectures, there is a suitable infinite-parameter limit where the latter distribution becomes Gaussian (more formally a Gaussian Process or a free field theory). For FCNs, this entails taking the width of all layers to infinity \citep{neal1996priors}, for CNNs the number of channels \citep{novak2018bayesian}, and for transformers the number of heads \citep{hron2020infinite}. Though it would turn out that this limit is rarely reached in practice, it does turn out to be an insightful viewpoint which can be extended to more realistic scenarios.

Consider a DNN with uncorrelated random weights with sub-Gaussian tail, e.g. each weight and bias is i.i.d Gaussian random variable $W_{ij}^{(l)}\sim \mathcal{N}(0, \sigma^2_w)$ and biases $b_i^{(l)}\sim \mathcal{N}(0, \sigma^2_b)$. Starting from the most upstream layer, the input layer, $h^{(0)}_i(x) = w_i \cdot x$, one finds that $h^{(0)}_i(x)$, per given $x$, are uncorrelated random variables. Consequently, by induction $h_i^{(l)}(x)=\sum_{j=1}^N W^{(l)}_{ij} \sigma(h^{(l-1)}_{j}(x))$ appear as a sum of $N$ uncorrelated random variables. Thus, for reasonable choice of $\sigma(..)$ and properly normalized $W^{(1)}$, the preactivation $h_i^{(1)}(x)$, per $x$, would be uncorrelated Gaussian variables. The proper normalization is to take a variance of $\sigma_w^2 = 1/N$ for $W^{(1)}_{ij}$ (more generally one over the fan-in or number of signals coming in), which is a common way to initialize DNNs. 

Following the above argument, we see that the output of a neural network as $N\rightarrow \infty$, per $x$, is a Gaussian random variable. Let us obtain the variance of this random variable, and at the same time, the correlations between the outputs of the network on two different data points. Repeating the previous line of argument, the two dimensional vector random variable $[h^{(0)}_i(x_1),h^{(0)}_i(x_2)]$ is a two-dimensional multivariate Gaussian since it is linear in $w_i \in R^d$ which we assume is Gaussian. The correlations $\langle h^{(0)}_i(x_{\alpha}) h^{(0)}_{j}(x_{\beta})\rangle_{w_i,w_j \sim {\cal N}(0,d^{-1}I_{d\times d}]}=\delta_{ij} d^{-1} (x_{\alpha} \cdot x_{\beta})$, with $\alpha,\beta \in \{1,2\}$. We denote the resulting distribution for $[h^{(0)}_i(x_{1}),h^{(0)}_i(x_{2})]$, per any neuron index $i$, as ${\cal N}(0,K^{(0)})$, namely a two-dimensional centered Gaussian distribution with a covariance matrix $K^{(0)}=d^{-1} \begin{pmatrix}
|x_1|^2 & x_1 \cdot x_2 \\
x_2 \cdot x_1 & |x_2|^2
\end{pmatrix}$

Continuing downstream, the vector of random variables $[h^{(1)}_i(x_1),h^{(1)}_i(x_2)]$ can be written as 
\begin{align}
[h^{(1)}_i(x_1),h^{(1)}_i(x_2)] &= \sum_{j=1}^N W^{(1)}_{ij} [\sigma(h_j^{0}(x_1)),\sigma(h_j^{0}(x_2))]
\end{align}
is given as a summation of $N$ uncorrelated 2d vector random variables. Hence, it is again Gaussian by the vector version of the central limit theorem. Furthermore, by definition, there is no linear correlation between different $i$'s (due to $W_{ij}^{(1)}$), which for Gaussian variables means no correlation at all. Thus its distribution is fully determined by its average and correlation specifically on any specific $i$. The average $\langle [h^{(1)}_i(x_1),h^{(1)}_i(x_2)] \rangle_{W^{(1)}_{ij} \sim {\cal N}(0,N^{-1}I_{N \times N}),w_{1..N} \sim {\cal N}[0,d^{-1} I_{d\times d}]}$ is easily seen to be zero, and the $2 \times 2$ correlations matrix $K^{(1)}_{\mu \nu}$ given by 
\begin{align}
\label{Eq:NNGP_K_1}
K^{(1)}_{\mu \nu} &= \left\langle h^{(1)}_i(x_{\mu})h^{(1)}_i(x_{\nu}) \right\rangle_{W^{(1)}_{ij} \sim {\cal N}(0,N^{-1}I_{N \times N}),w_{1..N} \sim {\cal N}[0,d^{-1} I_{d\times d}] } \\ \nonumber &= \left\langle h^{(1)}_i(x_{\mu})h^{(1)}_i(x_{\nu}) \right\rangle_{W^{(1)}_{ij} \sim {\cal N}(0,N^{-1}I_{N \times N}),h^{(0)} \sim {\cal N}(0,K^{(0)})} \\ \nonumber 
&= \frac{1}{N}\sum_{j=1}^N \int d h^{(0)}_{j}(x_\mu) d h^{(0)}_{j}(x_\nu) \; {\cal N}(0,K^{(1)};h_{j}^{(0)}(x_.)) \; \sigma(h^{(0)}_j(x_{\mu}))\sigma(h^{(0)}_j(x_{\nu})) \\ \nonumber 
&= \int d h^{(0)}_1(x_\mu) d h^{(0)}_1(x_\nu) \; {\cal N}(0,K^{(0)};h^{(0)}) \; \sigma(h^{(0)}_{1}(x_{\mu})) \sigma(h^{(0)}_{1}(x_{\nu}))
\end{align} 
where in passing between the first and second line, we used the fact that $h^{(1)}$ depends on the $w_{1..N}$ only through $h^{(0)}$, to replace the average under the inputs weights with an average over $h^{(0)}$. The resulting two-dimensional integral turns out to be solvable for various activation functions in particular ReLU \citep{Saul2009,lee2017deep} and Erf \citep{williams1996computing}. For instance, in the latter case one obtains 
\begin{align}
\label{Eq:ErfKernel}
K^{(1)}_{\mu \nu}  \equiv K^{(1)}(x_{\mu},x_{\nu})  &= \frac{2}{\pi} \sin^{-1}\left[\frac{2 K^{(0)}_{\mu \nu}}{\sqrt{1+2 K^{(0)}_{\mu \mu}}\sqrt{1+2K^{(0)}_{\nu \nu}}}\right]
\end{align}
More generally, the above reasoning now extends naturally to the downstream layer, with $(1)\rightarrow(2)$ and $(0) \rightarrow (1)$. Thus we have a concrete formula for the correlations between the outputs on $x_1,x_2$ for an arbitrarily deep ReLU or Erf network. 

Next, we wish to extend this to any finite number of points ($x_1...x_{P'}$). At large enough $N$ (in particular $N \gg P'$) each pre-activation as well as the output remains a multivariate $P'$-dimensional Gaussian and hence solely defined by the correlation between each two data-points--- the same correlation described by the above formula. Thus as $N\rightarrow \infty$, we find that the distribution induced by the DNN on function space is such that when examined on any finite set of points--- it behaves as a multivariate Gaussian with a kernel $K^{(L)}_{\mu \nu}$. In physics, we call such distributions over functions free field theories. The advantage of the latter, a less rigorous but practically exact viewpoint, is that it transcends the mathematical description based on the awkward notion of all possible finite sets of points, to the simple physical notion of a continuum. It also offers a convenient starting point for analyzing the effect of non-linearities via perturbation theory, saddle point, and mean-field approaches, as discussed below. 

\section{Operator algebra on spaces with measures}
\label{Sec:Intro_Measure}
As is common with Gaussian distributions, the eigenvectors and eigenvalues of the covariance matrix play a useful role. As alluded to in the above section, our real object of interest is an extreme case of a multivariate Gaussian distribution, wherein instead of considering a Gaussian distribution over vectors, we consider a Gaussian distribution over an infinite-dimensional vector or equivalently a smooth function. 

To substantiate this viewpoint and explain what are the coefficients of this infinite vector, consider the following eigenvalue problem 
\begin{align}
\label{Eq:EigenvalueEquation}
\int \dif x' p(x') K(x,x') \phi_{k}(x') &= \lambda_k \phi_k(x)  
\end{align}
where $p(x')$ is some distribution over inputs and we shall soon denote $\dif x \; p(x)=\dif \mu_x$. Given that (i) $p(x')$ decays sufficiently fast at infinity, (ii) $K(x,x')=K(x',x)$, and (iii) $\int \dif \mu_{x'} \dif\mu_x K(x,x') g(x) g(x') \geq 0$ for all real functions $g(x)$ with finite norm ($\int \dif \mu_x g^2(x) < \infty$) we have that (a) the above eigenvalue problem yields a discrete set of solutions $(\lambda_k,\phi_k(x))$ with $\lambda_k$ being non-negative real numbers (b) $\phi_k(x)$ may be chosen normalized and orthogonal namely $\int \dif\mu_x \phi_{k}(x) \phi_{k'}(x)=\delta_{kk'}$ (c) $K(x,x')=\sum_{k=1}^{\infty} \lambda_k \phi_k(x) \phi_k(x')$ (Mercer's theorem). (d) $\phi_k(x)$ define a complete basis for function space namely we may express any $g(x)$ via $\sum_k g_k \phi_k(x)$ where $g_k = \int \dif\mu_x \phi_k(x) g(x)$. 

Notably, kernels of DNNs obey requirements $(ii),(iii)$. Concerning point $(i)$ above, input datasets, typically have a natural scale and decay to zero beyond some value (e.g., the brightness of a pixel cannot be larger than 255, or the input amplitude of a microphone cannot exceed 150db). 

Next, we define several additional linear algebra notions on kernels. In particular, when all $\lambda_k > 0$, the inverse kernel $K^{-1}(x,x')$ can be defined and is given by 
\begin{align}
K^{-1}(x,x') &= \sum_k \lambda_k^{-1} \phi_k(x) \phi_{k}(x')  
\end{align}
Noting further that $\int \dif x' p(x') \sum_k \phi_k(x) \phi_k(x') g(x')= g(x)$ we find that 
\begin{align}
\sum_k \phi_k(x) \phi_{k}(x') &= \delta(x-x')/p(x') 
\end{align}
using the above we find an equivalent definition of $K^{-1}(x,x')$
\begin{align}
\label{Eq:Alt_Inv}
\int \dif\mu_{x'} K(x,x') K^{-1}(x',x'') &= \delta(x-x'')/p(x) 
\end{align}

Notably while $K(x,x')$ is given and independent of the measure $p(x)$, $K^{-1}(x,x')$ does depend on it, yet in a predictable way: Given $K^{-1}(x,x')$ defined w.r.t. the measure $p(x)$ and $\tilde{K}^{-1}(x,x')$ defined w.r.t. the measure $\tilde{p}(x)$ we find
\begin{align}
\frac{p(x)}{\tilde{p}(x)}K^{-1}(x,x') \frac{p(x')}{\tilde{p}(x')}&= \tilde{K}^{-1}(x,x')
\end{align}
which can be verified via 
\begin{align}\int &\dif\tilde{\mu}_{x'}K(x,x') \tilde{K}^{-1}(x',x'')=\int \dif\tilde{\mu}_{x'}K(x,x')\frac{p(x')}{\tilde{p}(x')} K^{-1}(x',x'')\frac{p(x'')}{\tilde{p}(x'')}\\ \nonumber &=\int \dif\mu_{x'}K(x,x') K^{-1}(x',x'')\frac{p(x'')}{\tilde{p}(x'')}=\frac{\delta(x-x'')}{\tilde{p}(x'')}\end{align} where $\dif\tilde{\mu}_x=\dif x \; \tilde{p}(x)$. We thus find that $\tilde{K}^{-1}$ obeys Eq. \ref{Eq:Alt_Inv}.  

An interesting corollary of this is that the so-called Reproducing Kernel Hilbert Space (RKHS) norm of a function $g(x)$ 
\begin{align}
\label{Eq:RKHS}
|g|_K^2 \equiv \int \dif\mu_x \dif\mu_{x'} g(x) g(x') K^{-1}(x,x')
\end{align}
is in fact the same for any two measures with the same support, loosely speaking we mean that $\infty>p(x)/\tilde{p}(x) > 0$ for any $x$. For more details, see \cite{Rasmussen2005} section 6.1 and references therein.

\section{ Symmetries: Equivariance and Invariance} Kernels of DNNs are typically highly symmetric objects. For instance, as can be inferred from our previous computation for the kernel-- any DNN with a fully-connected input layer would have a kernel which depends only on $x \cdot x',|x|,|x'|$. As a result, it would be symmetric to rotations of the input. More formally let $O$ be some symmetry action on $x$ ($d \times d$ rotation matrix in the above example), we say that a kernel is equivariant under $O$ when $K(x,x')=K(Ox,Ox')$. We say that the kernel is invariant under $O$ when $K(x,Ox')=K(Ox,x')=K(x,x')$. Intuitively, equivariance means that the RKHS norm of a function ($f(x)$) is equal to that of $f(Ox)$ where invariance means non-symmetric functions are outside-RKHS/inexpressible using that kernel. Note also that invariance implies equivariance. 

We comment that these notions of equivariance and invariance coincide with those used in DNN nomenclature (e.g. \cite{novak2018bayesian}). Indeed, equivariance means that the action of $O \in O(d)$ on $x$ has some non-trivial action ($\tilde{O} \in O(N)$) on the $c-$index vector $h^{(l)}_{.}(x)$ namely that $h^{(l)}_{.}(Ox)=\tilde{O}h^{(l)}_{.}(x)$ whereas invariance means $\tilde{O}$ is the identity. Using the definition of the kernel of the $l$'th layer as the average of $[h^{(l)}_{.}(x)]^T h^{(l)}_{.}(x')$ (over weights) one recovers the above definitions for symmetries which are a subset of the orthogonal group (e.g. permutations, reflections, rotations on convolutional patches). \footnote{Scale invariance, present in ReLU networks without biases, has a different manifestation which does not fall strictly within the above two categories, specifically $K(x,cx')=cK(x,x')$).} 

We comment that one can map this notion of symmetry to that used in quantum mechanics. Specifically, define the operator associated with $O$ ($\tilde{O}$) via the action $\tilde{O} f(x)=f(Ox)$. This then implies the right action $OK(x,x')=K(Ox,x')$. Next using the action of $K$ on function ($\int \dif\mu_{x'} K(x,x') f(x')$) we find the left action via $\int \dif\mu_{x'}K(x,x') \tilde{O}f(x')=\int \dif\mu_{x'}K(x,x')f(Ox')=\int \dif\mu_{O^T x'}K(x,O^T x')f(x')$. Conditioned on the measure being symmetric ($\dif\mu_{O^T x'}=\dif\mu_{x'}$) we find $K(x,x')\tilde{O}=K(x,O^T x')$ \footnote{For non-symmetric measure the ratio $\dif\mu_{x'}/\dif\mu_{O^T x'}=p(x')/p(O^T x')$ appears as a factor} and therefore $\tilde{O} K(x,x') \tilde{O}^T=K(Ox,Ox')=K(x,x')$ as the definition of a symmetry which coincides with that used in quantum mechanics, when viewing $K$ as the Hamiltonian. 

Much like in quantum physics, symmetries have direct implications on the eigenfunctions and eigenvalues of a kernel. However, at least in non-relativistic physics, we often assume that space itself possesses all symmetries. Here the situation is different since the measure, $\dif\mu_x=\dif x \; p(x)$ need not be symmetric. In case it is, namely when $p(x)=p(Ox)$, we find the following simplifications: Equivariance implies that eigenfunctions can be grouped into irreducible representations (irrep) of the symmetry group (e.g. the rotation group for an FCN). All eigenfunctions which are part of the irrep would have the same eigenvalue. Invariance implies equivariance, hence the above conclusions hold but in addition, one finds that all non-trivial irreps of the symmetry have a zero eigenvalue. Given that irreps for standard symmetry groups are well known (e.g. spherical-harmonics for $O(3)$ and hyper-spherical harmonics for $O(d)$), these considerations simplify the diagonalization of kernels when both kernel and measure are symmetric. For instance, any DNN having a fully-connected first layer together with a measure that is uniform on a hyper-sphere in input space, yields an exactly diagonalizable kernel with hyper-spherical harmonics as eigenfunctions (e.g. \cite{cohen2021learning}). 

\section{Essentials of path integrals}
\label{Sec:PathIntegrals}
Path integrals are an extreme version of multivariable integrals and an underlying formalism for field theory \cite{schulman1996}. Multivariable integrals are needed when we integrate over functions having a multi-variable argument. Path integrals are required when we are integrated over functionals-- functions which take a function as their argument.  

In physics, we often encounter functionals-- say the elastic energy associated with the height configuration/field ($f(x,y)$) of a 2d membrane which we denote as $E[f]$ (from now on, we use square brackets (e.g. $A[..]$) to denote $A$ as functional and angular brackets if $A$ is a single or multi-variable function). At equilibrium, statistical physics tells us that the probability of finding the membrane at height configuration $f(x,y)$ is {\it proportional to} $e^{-E[f]/(k_B T)}$ where $T$ is the temperature and $k_B$ is Boltzmann's constant. However, to discuss the probability density of $f$ ($P[f]$) or calculate averages such as the mean value of $f$ at $(x_0,y_0)$, we need to integrate over all possible $f$ configurations. Developing a formal way of doing so, especially in cases where $E[f]$ is not a simple quadratic form in $f$, is the main purpose of path integrals. Alongside this, path integrals provide a unified formalism for statistical field theory, stochastic processes, quantum mechanics, and quantum field theory and aid the transfer of knowledge between these diverse fields. 

Turning to deep learning, it is natural to consider the output of the network $f({x})$, where ${x\in \mathbb{R}^d}$ is the network input, in analogy with the height/field configuration of the membrane. Analogous to the Boltzmann distribution $e^{-E[f]/(k_bT)}$, we may now consider several natural distributions on the output function, $f$, of the network: {\bf (i)} The prior distribution i.e. the distribution of $f$ induced by choosing suitably normalized random Gaussian weights, {\bf (ii)} The posterior distribution i.e. the distribution of $f$ induced by the Bayesian posterior (i.e. the distribution obtained multiplying the prior distribution by the probability of seeing the training set as the output of the model and normalizing the product), or {\bf (iii)} The distribution induced on $f$ by training an ensemble of neural networks, with different weights drawn from the initialization ensemble and different algorithmic noise. 

Path integrals can be formally defined as an infinite-variable limit of multi-variable integrals. The most common way is to discretize the space (${x}$) over some fine grid. Concretely, let us assume ${x}$ does not exceed some $[-L,L]^d$ hypercube and allow each real coordinate $[{x}]_i$ to take on only the values $-L + n/\Lambda$ with $n$ spanning the integers in the interval $[0,2 \floor*{L \Lambda}]$. Limiting space in this manner, $f$ is equivalent to a vector, $f_{\Lambda}$ of dimension $d_{\Lambda}=(2 \floor*{\Lambda L})^d$ with each coordinate of $f_{\Lambda}$ marking its value on one point on the grid. Next, we need to map our functional of interest (say $E[f]$), into a multivariate function $E(f_{\Lambda})$. The standard way of doing so is to trade derivatives with discrete derivatives and integrals with summations. \footnote{In cases where the typical configurations of $f$ are not smooth beyond some small length scale, as is the case in stochastic processes or quantum field theories with Gauge fields, the ambiguities of how to define discrete derivatives matter. However, in our case, they do not, since as we shall soon see our $E[f]$ involves integrals and not derivatives.} Last we consider some quantity of interest, say $f({x}_0)$, quantize ${x}_0$ to the nearby grid point (${x}_{g0}$), and calculate its average via $\int \dif^{d_{\Lambda}} e^{-E(f)} f({x}_{g0})/\int \dif^{d_{\Lambda}} e^{-E(f)}$. Finally, we take the limit of $\Lambda,L \rightarrow \infty$ and treat that as the path integral. In practice, this underlying machinery is kept under the hood, and, following some general identities of path integrals we may derive in this manner, we work directly in the continuum and make no reference to a grid. 

Let us demonstrate this procedure for the case where $f$ is the output of a network with random weights in the infinite-width limit. Let us further enumerate the grid points with a single index (${x}_p$, $p \in 1..d_{\Lambda}$). Four relevant objects here are the network kernel matrix, or correlations matrix between outputs, $K_{pp'}=K({x}_p,{x}_{p'})$ and its inverse $K^{-1}_{pp'}$ as well as the operator versions of these namely $K({x},{x}')$ and $K^{-1}({x},{x}')$, which we assume are all smooth function.  

%*** Place this paragraph under some "for specialist", or under RG section. *** As we are about to replace discrete sums with integrals, let us discuss the smoothness of the above operators. Being a correlation of network outputs, the finite input sensitivity implied by the smoothness of network activations implies that $K({x},{x}')$ is a smooth function of its arguments. For its inverse, it is less clear. In practice, we shall assume it is. A more careful treatment, which we only sketch, is the following: Note that using an operator exponential (analogous to a matrix exponential) we can write $K^{-1} = \int_0^{\infty} dt e^{-K t}$. Next we can define a regulated inverse $\tilde{K}^{-1} = \int_0^{T} dt e^{-K t}$ which, as can be seen by diagonalizing both sides and performing the integral, effectively cuts-off eigenvalues of $K$ which are much smaller than $1/T$ from appearing in the spectrum of $\tilde{K}^{-1}$. Since $t$ is finite, one may now Taylor expand $e^{-Kt} = I - Kt + \frac{1}{2} KK t^2 + ..$ where, say, $KK$, is the operator $(KK)(x,y)=\int \dif\mu_z K(x,z)K(z,y)$. Notably each such $K,KK,KKK,...$ operator inherits the smoothness of $K(x,y)$ and so, being a finite sum of smooth functions $\tilde{K}^{-1}$ is smooth at any finite $T$. We thus keep $T$ finite in all manipulations below and take it to infinity after taking the limit of an infinitely fine grid. Zooming back out, this exposes our focus on observables which have a finite limit as we take $T$ to be infinite. Equivalently, we may compute statistical properties of quantities which do not have singular dependence on low $K$ eigenvalues (e.g. we avoid $f K^{-n} f,n \geq 1$) as these may diverge at large $T$. Still, such quantities are quite unnatural, and so this is not a serious reason for concern. Typically, using the path-integral formalism for such quantities would yield an infinite result thereby ``warning us'' that we are misusing it. 

We turn to obtain the path integral formulation of a Gaussian Process as a limit of an infinitely fine discretization grid. Based on the smooth assumptions discussed above we have that 
\begin{align}
\lambda \phi_{\lambda}(x_b) &= \int \dif\mu_z K(x_b,z) \phi_{
\lambda}(z) \approx d_{\Lambda}^{-1} \sum_{a=1}^{d_{\Lambda}} K(x_b,z_a) \phi(z_a) \\ \nonumber
\phi_{\lambda}(x_b) &= \int \dif\mu_z \dif\mu_w K^{-1}(x_b,z)K(z,w) \phi_{
\lambda}(w) \\ \nonumber 
&\approx d_{\Lambda}^{-2} \sum_{a,c=1}^{d_{\Lambda}} K^{-1}(x_b,z_a) K(z_a,w_c) \phi(w_c) 
\end{align}
where $z_a, a \in 1..d_{\Lambda}$ goes over all grid points as does $w_c$, $x_b$ is some grid point, and factors of $d_{\Lambda}$ come from turning an integral to a sum. The first equation shows that the eigenfunctions of the operator $K(x,z)$, when sampled on the grid, are the eigenvectors of the $d_{\Lambda} \times d_{\Lambda}$ matrix $K_{ba}=K(x_b,z_a)$ however with eigenvalues multiplied by $d_{\Lambda}$. This is what we mean by  asymptotic association $K(x,z) \rightarrow d_{\Lambda}^{-1}K(x_a,z_b)$. The second equation then implies that the matrix obtained by the discretization of the operator $K^{-1}(x,z)$ acts as the matrix inverse of $K_{ab}$ up to a factor of $d_{\Lambda}^2$, namely $K^{-1}(x_a,z_b) \rightarrow d_{\Lambda}^{2}[K^{-1}]_{ab}$. Given these associations we may write the log probability of the GP prior as  
\begin{align}
&\sum_{ab} f(x_a) [K^{-1}]_{ab} f(x_b) \rightarrow \sum_{ab} d_{\Lambda}^{-2}  f(x_a) K^{-1}(x_a,x_b) f(x_b) \\ \nonumber  &\approx \int \dif\mu_x \dif\mu_{x'} f(x) K^{-1}(x,x') f(x') + Const\\ \nonumber 
\end{align}
We thus find the following path integral formulation of the probability induced on function space by an infinitely over-parameterized DNN 
\begin{align}
P[f] &= Z^{-1} e^{-\frac{1}{2} \int \dif\mu_x \dif\mu_{x'} \,f(x) K^{-1}(x,x') f(x')}
\end{align}
where the normalization factor, also known as a partition function, is given by the following path integral over the function space $f$ ($\int Df$)
\begin{align}
Z &= \int Df  e^{-\frac{1}{2} \int \dif\mu_x \dif\mu_{x'} \, f(x) K^{-1}(x,x') f(x')}
\end{align}
where $\int Df$, the path integral over $f$, is defined via the limit $Df = \lim_{L,\Lambda\to \infty}\prod_{i=1}^{d_{\Lambda}} \dif f_i$. 

Borrowing physics computation style, we would, from now on, focus mainly on $\log(Z)$ together with so-called source terms ($\alpha(x)$ dependent terms), as our main object of interest. Specifically, we will define 
\begin{align}
Z[\alpha] &= \int Df \; e^{-S[f,\alpha]}\\ \nonumber 
S[f,\alpha] &= \frac{1}{2} \int \dif\mu_x \dif\mu_{x'}\, f(x) K^{-1}(x,x') f(x')+\int \dif x \; \alpha(x) f(x)
\end{align}
where we also introduced, borrowing jargon from field-theory, the ``action" (or negative log probability) $S[f,\alpha]$. Next we introduce the functional derivative $\delta_{\alpha(x)}$ which obeys 
\begin{align}
\delta_{\alpha(x)} \alpha(y) &= \delta(x-y)
\end{align}
to calculate the statistics of $f(x)$ under $P[f]$ we use the chain rule as such 
\begin{align}
&\delta_{\alpha(x_0)} \log(Z[\alpha])|_{\alpha=0} = Z^{-1} \delta_{\alpha(x_0)} Z[\alpha]\mid_{\alpha=0} = \\ \nonumber  &Z^{-1} \int Df  e^{-\frac{1}{2} \int \dif\mu_x \dif\mu_{x'} \,f(x) K^{-1}(x,x') f(x')} \delta_{\alpha(x_0)}\int \dif x \alpha(x) f(x) \mid_{\alpha=0} \\ \nonumber 
&= Z^{-1} \int Df  e^{-\frac{1}{2} \int \dif\mu_x \dif\mu_{x'} \, f(x) K^{-1}(x,x') f(x')}\int \dif x \delta(x_0-x) f(x) \\ \nonumber 
&= Z^{-1} \int Df  e^{-\frac{1}{2} \int \dif\mu_x \dif\mu_{x'} \, f(x) K^{-1}(x,x') f(x')}f(x_0) \\ \nonumber
&= \int Df P[f] f(x_0)
\end{align}
where the second term in the exponential vanishes at $\alpha=0$.

Similarly, one can show that second order functional derivatives provide the second cumulants (correlations) namely  
\begin{align}
\delta_{\alpha(x_0)}\delta_{\alpha(x_1)} \log(Z[\alpha])|_{\alpha=0} &= \int Df f(x_0) f(x_1) -  \int Df f(x_0) \int Df f(x_1)
\end{align}

\noindent we comment that $\log(Z[\alpha])$ can be understood as a path integral version of a multivariate cumulant generating function (which however omits an inconsequential constant, since $\log(Z[0])$ need not be $1$). In physics $-\log(Z)$ times $k_B T$ is known as the free energy. 

One useful identity by which one can evaluate averages as the one above is 
\begin{align}
Z[\alpha] &= \int Df  e^{-\frac{1}{2} \int \dif\mu_x \dif\mu_{x'} f(x) K^{-1}(x,x') f(x')+\int \dif x \alpha(x) f(x)} \\ \nonumber 
&= \left[\int Df' e^{-\frac{1}{2} \int \dif\mu_x \dif\mu_{x'} f'(x) K^{-1}(x,x') f'(x')}\right] e^{\frac{1}{2} \int \dif\mu_x \dif\mu_{x'} \alpha(x) K(x,x') \alpha(x')} \\ \nonumber 
&= e^{\frac{1}{2} \int \dif\mu_x \dif\mu_{x'} \alpha(x) K(x,x') \alpha(x')+Z[0]}
\end{align}
where we perform square-completion defining $f'(x) = f(x)-\int \dif\mu_{x'} K(x,x') \alpha(x')$ and in the penultimate line we used freedom in defining the integration contour to cancel $\alpha$ dependence of the functional integral on the 2nd line. A corollary of this identity is that the correlation between $f(x_0)$ and $f(x_1)$ under the path integral is 
\begin{align}
&\delta_{\alpha(x_0)} \delta_{\alpha(x_1)}\log(Z[\alpha]) = \delta_{\alpha(x_0)} \delta_{\alpha(x_1)}\log\left( 
e^{\frac{1}{2} \int \dif\mu_x \dif\mu_{x'} \alpha(x) K(x,x') \alpha(x')+Const} \right) \\ \nonumber 
&= \delta_{\alpha(x_0)} \delta_{\alpha(x_1)} \left( \frac{1}{2} \int \dif\mu_x \dif\mu_{x'} \alpha(x) K(x,x') \alpha(x')\right) = \frac{1}{2}[K(x_0,x_1)+K(x_1,x_0)] \\ \nonumber 
&=K(x_0,x_1)
\end{align}
which can also be viewed as a sanity check for this formalism. Notably, since we shall always be interested in evaluating such functional derivatives at $\alpha=0$, we kept this requirement implicit in the above and from now on.

Another useful identity is the spectral representation of the path integral. First note that due to the orthogonality of eigenfunction under the measure (see below Eq. (\ref{Eq:EigenvalueEquation})) we have that
\begin{align}
 \int \dif\mu_x \dif\mu_{x'} f(x) K^{-1}(x,x') f(x') &= \sum_{k=1}^{\infty} f_k \lambda_k^{-1} f_k \\ \nonumber 
f_k &= \int \dif\mu_x f(x) \phi_{\lambda_k}(x)
\end{align}
with $\lambda_k, \phi_{\lambda_k}(x)$ be the eigenvalues and eigenfunction of the kernel $K$. Further, since we do not need to pay attention to constant factors multiplying ($Z[\alpha]$), as those would anyway be lost in any functional derivative of $\log(Z[\alpha])$ we may ignore the Jacobian (which is anyways one for any finite discretization), associated with changing the integration variable from $Df$ to $\Pi_{k=1}^{\infty}d f_k$ and obtain 
\begin{align}
Z[\alpha] &\propto \int \Pi_k \dif f_k \; e^{-\frac{1}{2}\sum_{k=1}^{\infty} f_k \lambda_k^{-1} f_k + \int \dif\mu_x \alpha(x) f(x)} \\ \nonumber 
&= \int \Pi_k \dif f_k \; e^{-\frac{1}{2}\sum_{k=1}^{\infty} f_k \lambda_k^{-1} f_k + \sum_{k=1}^{\infty} \alpha_k f_k}
\end{align}
where, in the last line, we used again the orthogonality relations together with the completeness relation $f(x) = \sum_k f_k \phi_k(x)$ and $\alpha(x) = \sum_k \alpha_k \phi_k(x)$.

\section{Approximating path integrals}
Evaluating path integrals or multivariable integrals is generally impossible apart from the cases where the action (negative log probability) is at most quadratic in the integration variables (i.e. Gaussian). The typical action would, however, be non-Gaussian, requiring an extra set of tools. Here we will cover three basic tools: Perturbation theory, mean-field theory, and saddle point methods.  

\subsection{Perturbation theory}
\label{Sec:PT}
Consider an action of the form 
\begin{align}
S[f] &= S_0[f] + U[f]
\end{align}
where $S_0[f]$ is quadratic in $f$ (e.g. $\int \dif\mu_x \dif\mu_y f(x) K^{-1}(x,y) f(y)$) and $U[f]$, henceforth the {\it interaction}, is non-quadratic, and, for simplicity, has some finite power of $f$ (e.g. $\int \dif\mu_{x_1}..\dif\mu_{x_4}U(x_1..x_4) f(x_1)..f(x_4)$). We assume the Gaussian part of the action ($S_0[f]$) is solved so we know the mean and variances under that action 
\begin{align}
\langle f(x) \rangle_{S_0} &= \bar{f}(x) \\ \nonumber 
\langle [f(x)-\bar{f}(x)] [f(y)-\bar{f}(y)] \rangle_{S_0} &= K(x,y)
\end{align}
We wish to evaluate the average value of some observable (e.g. $f(x)f(y)$) under that action as a series expansion in $U(x_1..x_4)$. 
The answer, up to the second order, can be formally written as a Taylor expansion of the interaction term exponent
\begin{align}
\langle f(x) f(y) \rangle &= \frac{Z_{U=0}}{Z}\langle f(x) f(y) e^{-U[f]} \rangle_{S_0} = \langle f(x) f(y) \rangle_{S_0} - \langle f(x) f(y) U[f] \rangle_{S_0,\con} \\ \nonumber &+ \frac{1}{2!} \langle f(x) f(y) U[f] U[f]\rangle_{S_0,\con} - ...
\end{align}
where we introduced connected average notation ($\langle .. \rangle_{S_0,\con}$), which accounts for the normalization of the partition function, and is defined as follows (cf. \citep{K_hn_2018} App. A.): Noting that the averages are under Gaussian distribution, one first considers all terms contributing to this average according to Eq. (\ref{Eq:WickWithAverage}). Next, one omits the ``disconnected'' contributions to the sum in that equation. A disconnected contribution is one in which one or both of the following two conditions are met: {\bf (i)} The $f$ factor in the observable are not paired through $P$ with any $f$ factors in the interaction term. {\bf (ii)} At least one of the $U[f]$ factors are not paired with $f$ factors in the observable or in another $U[f]$. The sum of all remaining terms (ones in which all the different $U[f]$ terms as well as the observable are connected through pairs) constitute the connected average. For instance, in the above example, assuming for simplicity that $\bar{f}(x)=0$, we obtain 
\begin{align}
&\langle f(x) f(y) U[f] \rangle_{S_0,\con} = \int \dif\mu_{x_1}..d \mu_{x_4} U(x_1..x_4) \langle f(x) f(y) f(x_1)..f(x_4) \rangle_{S_0,\con} \\ \nonumber 
&\langle f(x) f(y) f(x_1)f(x_2)f(x_3)f(x_4) \rangle_{S_0,\con} = K(x,x_1)K(y,x_2) K(x_3,x_4)\\ \nonumber 
&+ K(x,x_1)K(y,x_3) K(x_2,x_4) + K(x,x_1)K(y,x_4) K(x_3,x_3) \\ \nonumber &+ K(x,x_2)K(y,x_1) K(x_3,x_4) + K(x,x_2)K(y,x_3) K(x_1,x_4) \\ \nonumber 
&+ K(x,x_2)K(y,x_4) K(x_1,x_3) +  K(x,x_3)K(y,x_1) K(x_2,x_4)\\ \nonumber 
&+ K(x,x_3)K(y,x_2) K(x_1,x_3) +  K(x,x_3)K(y,x_4) K(x_1,x_2) \\ \nonumber 
&+ K(x,x_4)K(y,x_1)K(x_2,x_3) +  K(x,x_4)K(y,x_2) K(x_1,x_3)\\ \nonumber 
&+  K(x,x_4)K(y,x_3) K(x_1,x_2)
\end{align}
which can be simplified into a single term times $12$, in the typical case where $U(x_1..x_4)$ is permutation symmetric in four arguments. 

Notwithstanding, going to higher orders order and/or having $\bar{f}\neq 0$ quickly generates many terms and becomes intractable. Perturbation theory is thus mainly useful in the following three circumstances: {\bf (i)} To perfect an already nearly-accurate result at zeroth order {\bf (ii)} As means of estimating when the zeroth order approximation starts breaking down {\bf (iii)} In scenarios which we will encounter, partial re-summations-- summing a certain simple subset of pairing choices to infinite order in $U[f]$, may allow tracking strong $U[f]$ effects accurately. For instance, it can be checked that keeping only pairings in which two $f$'s from each $U[f]$ get paired with themselves amounts to an effective change of $K(x,y)$. However, whether this subset of pairings dominates over all the exponentially many other pairings, would depend on the particularities of the problem. 

\subsection{Mean-field}
\label{ssec:mean_field}
A different technique that can capture strong $U[f]$ effects is taking a mean-field. The idea here is to identify a certain combination of $f$ within the interaction term which is likely to be weakly fluctuating under the path-integral. Typically, these would be coherent (equal sign) sums/integrals over many weakly correlated variables. Next, one replaces these with their expected mean value. This expected mean value is the value of this combination in the theory in which this replacement was made. Typically, the replacement should be such that it results in a tractable theory under which this mean value can be calculated. 

Let us demonstrate this seemingly circular logic in action through a concrete example. Consider an interaction of the form 
\begin{align}
U[f] &= u\left(\int \dif\mu_{x_1} f^2(x_1)\right)\left(\int \dif\mu_{x_2} f^2(x_2)\right) 
\end{align}
Next, one recognizes that both terms are an integration of a positive variable ($f^2(x)$) and hence coherent. Furthermore, within the free ($S_0$) theory, each such term is $\sum_{k=1}^{\infty}
f_k^2$. In cases where many $f_k$'s contribute significantly to the sum, we expect the mean to be much larger than the variance. Following this, we make the replacement 
\begin{align}
U[f] &= u\left[\left\langle \left(\int \dif\mu_{x_2} f^2(x_2)\right) \right\rangle_{S_{\MF}}+\Delta[f]\right]^2 \\ \nonumber 
\Delta[f] &= \int \dif\mu_{x_2} f^2(x_2)-\left\langle \left(\int \dif\mu_{x_2} f^2(x_2)\right) \right\rangle_{S_{\MF}}
\\ \nonumber 
S_{\MF} &\equiv S_0 + 2 u \Delta[f]\left\langle \left(\int \dif\mu_{x_2} f^2(x_2)\right) \right\rangle_{S_{\MF}} + \const  \\ \nonumber 
&= S_0 + 2 u \int d \mu_x f^2(x)\left\langle \left(\int \dif\mu_{x_2} f^2(x_2)\right) \right\rangle_{S_{\MF}} + \const 
\end{align}
where by $\langle ...\rangle_{S_{\MF}}$ is an expectation with respect to the measure generated by the above mean-field action $S_{\MF}$ (which neglects $O(\Delta^2)$ contributions), and therefore will soon be evaluated self consistently.  The mean-field approximation is to neglect the $\Delta[f]^2$ contribution and accordingly use the mean-field action ($S_{\MF}$) given by the original action without this term. Neglecting $\Delta[f]^2$ is justified as subtracting the average makes $\Delta[f]$ an incoherent sum (i.e. $\sum_k f_k^2 -\langle f^2_k \rangle_{S_{\MF}}$). As $f_k$'s are uncorrelated under $S_{\MF}$ and, as assumed, many $k$ modes contribute $\Delta[f]^2$ would be smaller by a factor of one over the effective number of contributing modes. Of course, for this to hold, we should subtract the correct average--- the average under the mean-field action. We turn to obtain this quantity. 

Since $S_0[f]$ was assumed Gaussian and the mean-field contribution of the interaction to the action is quadratic, $S_{\MF}[f]$ remains Gaussian and given by 
\begin{align}
S_{\MF} &= \frac{1}{2} \int \dif\mu_{x_1} \dif\mu_{x_2} f(x_1) K^{-1}(x_1,x_2) f(x_2) \\ \nonumber 
&- \int \dif\mu_{x_1} \dif\mu_{x_2} f(x_1) K^{-1}(x_1,x_2) \bar{f}(x_2) + 2u M \int \dif\mu_x f^2(x) 
\end{align}
where $K(x_1,x_2)$ and $\bar{f}(x_1)$ are, respectively, the covariance and mean which define $S_0$ and $M$, the mean-field, equals $\left\langle \left(\int \dif\mu_{x_2} f^2(x_2)\right) \right\rangle_{S_{\MF}}$. We may also write the last term as $2u M \int \dif\mu_{x_1} \dif\mu_{x_2} f(x_1) f(x_2) \delta(x_1-x_2)/p_{data}(x_1)$. By a standard splitting of the second moment, we have 
\begin{align}
\label{Eq:MF_1}
&\left\langle \left(\int \dif\mu_{x_2} f^2(x_2)\right) \right\rangle_{S_{\MF}} = \int \dif\mu_{x_2}  [K_{\MF}(x_2,x_2)+ \langle f(x_2) \rangle_{S_{\MF}}^2] 
\end{align}
where $K_{\MF}$, the covariance of the mean-field action, is the inverse operator to $K^{-1}(x_1,x_2)+4 u M \delta(x_1-x_2)/p(x_1)$. To obtain this operator, we first note that $\delta(x_1-x_2)/p(x_1)$ acts as the identity operator ($I$) w.r.t. the data measure since $\int \dif\mu_{x_2} \delta(x_1-x_2)/p(x_1) \phi(x_2)=\phi(x_1)$. Hence, we find that it simply shifts the eigenvalue of $K^{-1}$ (i.e. $\lambda_k^{-1}$) by $4uM$. We thus obtain 
\begin{align}
\label{Eq:KMF_Spectral}
K_{\MF}(x,y) &= \sum_{k=1}^{\infty} \phi_k(x) \phi_k(y) [\lambda_k^{-1}+4uM]^{-1}
\end{align}
alternatively, using the fact that the identity operator commutes with any other operator, or by algebraic manipulations of the above formula, we equivalently find 
\begin{align}
K_{\MF} &= K [I+4 uM K]^{-1} = [I+4 uM K]^{-1} K
\end{align}
Next using square completion on $S_{\MF}$ we further find 
\begin{align}
\langle f(x) \rangle_{S_{\MF}} &= \int \dif\mu_y \dif\mu_z K_{\MF}(x,y) K^{-1}(y,z) \bar{f}(z) 
&= \int \dif\mu_z [K_{\MF}K^{-1}](x,z) \bar{f}(z) \\ \nonumber 
&= \int \dif\mu_z [I+4uMK]^{-1}(x,z) \bar{f}(z)
\end{align}
one can verify that these two results reproduce averages under $S_0$ for $u=0$. 

Having obtained an approximate Gaussian theory for $f(x)$ and fleshed out its correlation functions and averages, what remains is to determine the parameter $M$. Plugging the last several results into Eq. (\ref{Eq:MF_1}) we obtain the following {\it self-consistency} equation for $M$ 
\begin{align}
M &= \int \dif\mu_{x} K_{\MF}(x,x)+\left(\int \dif\mu_{z} [I+4uMK]^{-1}(x,z) \bar{f}(z)\right)^2
\end{align}
To simplify matters, let us next assume $\bar{f}(z)$ is some eigenfunction ($\phi_q(z)$) of $K$. Turning to the spectral representation of $K_{\MF}$ (Eq. \ref{Eq:KMF_Spectral}) and using the fact that now $\int \dif\mu_{z} [I+4uMK]^{-1}(x,z) \bar{f}(z)=\phi_q(z) (1+4uM \lambda_q)^{-1}$ we obtain an  algebraic equation for $M$  
\begin{align}
M &= \sum_{k=1}^{\infty}[ \lambda_k^{-1} + 4u M]^{-1}+ [1+4uM \lambda_q]^{-2}
\end{align}
which can be easily solved on a computer. Seeking to do pure analytics, we note that typically the largest $\lambda_k$ ($\lambda_{k=1}$) is order $1$ ($O(1)$) and also $\sum_{k=1}^{\infty} \lambda_k=\int \dif\mu_x K(x,x)\equiv \Tr[K]$. Thus for $u \ll 1$, we may Taylor expand the above equation to leading order in $M$ which gives 
\begin{align}
M &= \Tr[K]+1 - 4uM \sum_k \lambda_k^2 -8 u M \lambda_q \Rightarrow M &= \frac{\Tr[K]+1}{1+4u(\sum_k \lambda_k^2+2\lambda_q)}
\end{align}
which, as aforementioned, holds for sufficiently small $u$. 

We briefly comment that, in general, when solving self-consistency equations, one may find several plausible (i.g. non-imaginary, non-exploding, having the allowed sign) solutions for $M$. One should then compare the free energy of these solutions ($\log(\int Df e^{-S_{\MF}[f]})$ \footnote{Here one should be careful to include all the constant contributions to the action, such $uM^2$, we so far discarded}) and choose the one with the higher free energy. If one obtains that just near $u=u_c$ several solutions co-exist with the same free energy-- this marks a {\it phase transition} (provided that our mean-field treatment is accurate namely, that $\Delta^2[f]$ is indeed negligible). If these different solutions for $M$ coincide at $u=u_c$ we call it a {\it second-order phase transition} or a {\it continuous phase transition}. If the solutions are distinct at $u=u_c$, we call it a {\it first-order phase transition}.  

We also note that the mean-field can be improved using perturbation theory in the $\Delta^2[f]$ term we neglected taking $S_0$, in the perturbation theory context, to be our $S_{\MF}$. Showing that such corrections are small, is also a way of testing the accuracy of the mean-field treatment or assessing which scales control it. 

\subsection{Saddle point}
Another useful approximation which contains mean-field as a particular case is the {\it saddle point approximation}. It applies in cases where a large parameter appears in the action and controls its scale. For instance 
\begin{align}
Z &= \int Df e^{-P \tilde{S}[f]}
\end{align}
where $\tilde{S}[f]$ is some potentially non-linear action. Let us assume that the global minima of $\tilde{S}[f]$ as a function of $f$ is unique and obtained by some real-valued function $f_0(x)$. The saddle-point approximation then amounts to expanding the action to quadratic order around $f_0(x)$ namely  
\begin{align}
Z &= e^{-P\tilde{S}[f_0]} \int Df e^{-P \int \dif x \dif  y \; \left(\delta_{f(x)} \delta_{f(y)} \tilde{S}[f_0]\right) (f(x)-f_0(x))(f(y)-f_0(y))}
\end{align}
the exactness of this approximation at large $P$ stems from (I) our assumption that $\tilde{S}[f]$ is some smooth functional which is independent of $P$ and hence analytic even as $P\rightarrow 0$. (II) As we found a global minimum, $P\delta_{f(x)} \delta_{f(y)} \tilde{S}[f_0]$ is a positive definite operator, with eigenvalues scaling as $O(P)$ hence the contributions of $f(x)$ such that $f(x)-f_0(x) \gg 1/\sqrt{P}$ to the path integral are exponentially suppressed making the Taylor expansion exact at $P \rightarrow \infty$. 

In more elaborate scenarios, the minimum value of $S[f]$ may come out imaginary. Typically, the above formula still holds as is however, verifying it becomes more subtle. In particular, it requires analytically continuing the path integral to imaginary functions (imaginary $f_k$'s) and making sure that an integration contour exists that goes through the minima along its convex direction and obeys the path-integral boundary conditions (negative real infinity to positive real infinity). In general, this is quite hard to be certain of, however, if this happens to hold, the above approximation is again exact. Thus one can view the above as generating a concrete hypothesis that should be tested experimentally. 

{\bf Example.} Let us revisit the previous mean-field setup but handle it through a saddle point. To this end, one first notes that the action has no such explicit $P$ factor outside. However, we flesh out this factor using the following Fourier identity trick, namely 
\begin{align}
Z &= \int_{-\infty}^{\infty} dM \int_{-\infty}^{\infty} d\tilde{M}\int Df e^{i \tilde{M}(M-\int \dif\mu_x f^2(x))} e^{-S[f_0]-uM^2} 
\end{align}
where we ignored constant factors that are independent of the integration variables. Next, we note that the $f$ appears quadratically in the action and hence we may integrate over it yielding 
\begin{align}
Z &= \int_{-\infty}^{\infty} dM \int_{-\infty}^{\infty} d\tilde{M} e^{i \tilde{M} M-uM^2 - S(\tilde{M})} \\ \nonumber 
S(\tilde{M}) &= -\log \left( \int Df e^{-S_0[f]-i\tilde{M} \int \dif\mu_x f^2(x)}\right)
\end{align}
Using again square completion identities on $S_0[f]$ to extract the $\bar{f}(x)$ related contribution as well as using $\int Df e^{-\frac{1}{2} \int \dif\mu_x \int \dif\mu_y f(x) K^{-1}(x,y) f(y)}=\Pi_{k} (2\pi \lambda_k)^{1/2}$ and using the identity $\log(\text{det}(A))=\Tr \log(A)$ we find 
\begin{align}
S(\tilde{M}) &= \frac{1}{2}\Tr\log(K^{-1}+2i \tilde{M} I)-\frac{1}{2}
\int \dif\mu_x 
\int \dif\mu_y \bar{f}(x) [K^{-1}+2i\tilde{M}I]^{-1}(x,y) \bar{f}(y)  \\ \nonumber 
&= \frac{1}{2}\sum_k \log(\lambda_k^{-1}+2i\tilde{M}) \\ \nonumber 
&-\frac{1}{2}\int \int[K^{-1}\bar{f}](x) [K^{-1}+2i\tilde{M}I]^{-1}(x,y) [K^{-1}\bar{f}](y)
\end{align}
up to a constant factor. Next requiring that the $M$ integration is extremized we obtain $i \tilde{M}=2uM$, similarly from the requirement for the $\tilde{M}$ integration to be at its saddle we obtain
\begin{align}
&\partial_{i\tilde{M}
}\left(i \tilde{M}M - S(\tilde{M}) \right) = 0 \\ \nonumber 
&\Rightarrow  M =  \sum_k (\lambda_k^{-1}+2i \tilde{M})^{-1}+\int \dif\mu_x \dif\mu_y [K^{-1}\bar{f}](x)[K^{-1}+2i\tilde{M}I]^{-2} [K^{-1}\bar{f}](y) \\ \nonumber 
&\stackrel{\text{saddle}}{=} \sum_k (\lambda_k^{-1}+4uM)^{-1}+\int \dif\mu_x \dif\mu_y \bar{f}(x)[I+4uM K]^{-2} \bar{f}(y)\end{align}
where we recognize our previous mean-field equations for $M$. We here used the identity $\partial_ {A_{ij}} \log \det(A) = [A^{-1}]_{ij}$.

Let us re-examine some of the subtleties we glanced over in the above computation. First is the lack of a clear large parameter controlling the scale of the action. To flesh it out, let us consider a case where the spectrum of $K(x,y)$ consists of $d$ degenerate modes with eigenvalue $1/d$ \footnote{This degeneracy and scaling would be the case for a linear fully connected with a uniform data measure on the $d$-dim hypersphere}. In which case $\sum_k \log(\lambda_k^{-1} + i 2\tilde{M})=d \log(1 + i 2\tilde{M}/d)+d\log(d)$ which at large $d$ (i.e. many contributing modes) contains a large pre-factor (and even becomes more and more smooth). Turning to $M$, we note that the action is already quadratic and hence Taylor expanding it in $M$ to second order is exact and not part of the saddle point approximation. 

The second issue is that the saddle point for $\tilde{M}$ is imaginary, while the integration contour is real. As aforementioned, this requires analytically continuing the $\tilde{M}$ integration to the complex plane, such that it goes through the saddle along the concave direction. This can be done and a more detailed treatment, using large deviation theory and in particular in this case, the G\"{a}rtner Ellis theorem [\cite{touchette2009large}]. 
 
\section{The Replica Method}
\label{Sec:Replicas}
Often the action or equivalently the partition function depends on some unknown random parameters.  In machine learning, these could be the model parameters and/or the specific choice of dataset. In such settings, we wish to compute averages/observables given a particular choice of random parameters but then average these over the distribution of parameters (also known as {\it quenched average}). We thus have two ensembles in mind, the first concerns the actual degrees of freedom of the system and the second is the ensemble of model parameters. The Replica Method [e.g.  \cite{MezardBook}] is a technique which helps us treat these two ensembles on equal footing, effectively treating parameters as additional degrees of freedom. This then allows us to use standard techniques from field theory to treat the effects of disorder/parameter-fluctuations.

To illustrate the method, let us take the simplest system possible with one degree of freedom ($f$) and one parameter ($m$) which we wish to average observables over. Specifically, consider a simple non-centred Gaussian in the scalar variable $f$ whose action is given by 
\begin{align}
S(f;m) &= \frac{1}{2} f^2 - m f
\end{align}
The parameter $m$ is not known; rather, its distribution is known to be $N[0,\sigma^2;m]$. Consider some observable, say the average $f$, we may then ask what is the expectation value of this second moment under the above distribution for $m$ namely 
\begin{align}
\int dm {\cal N}(\bar{m},\sigma^2;m) \langle f \rangle_{S(f;m)}
\end{align}
This type of average, an average over action parameters taken after computing expectation value, is known in physics as a {\it quenched average}. More generally, $f$ would represent the configuration space of a statistical mechanics model and $m$ some set of random parameters in the action.  

In our simple model, the above-quenched average can be computed straightforwardly. Specifically since $\langle f \rangle_{S(f;m)}=m$ we find 
\begin{align}
\int dm {\cal N}(\bar{m},\sigma^2;m) m = \bar{m}
\end{align}

Crucially, however, we typically do not know how to solve the action for a given $m$. Indeed, our ability to perform computations often hinges on symmetry and random parameters generally spoil these helpful symmetries. Consequently, it is useful to find a formulation where fluctuations in parameter and fields/integration-variables are treated on more equal footing.  

To set the stage for the replica method, let us first express the desired expectation value via the partition function for a given $m$, using a source field $\alpha$,
\begin{align}
Z(\alpha;m) &= \int \dif f e^{-\frac{1}{2}f^2 + mf + \alpha f} \\ \nonumber 
\bar{f}(m) &= \partial_{\alpha}\log(Z(\alpha;m))|_{\alpha=0}
\end{align}
We note that by taking higher derivatives w.r.t. to $\alpha$, we can obtain all the statistics of $f$. Consequently, our main focus turns to the free energy, since its dependence on $\alpha$ encodes the entire statistics. Moreover, since averaging w.r.t. $m$ and taking derivatives w.r.t. $\alpha$ commute, we can obtain the quenched average of all cumulants of $f$ from the quenched average of the free energy namely $\int dm {\cal N}(\bar{m},\sigma^2;m) \log(Z(\alpha;m))$. 

To compute the latter average, we note the identity  $\log(x)=\lim_{Q \rightarrow 0}(x^Q-1)/Q$. We may thus write 
\begin{align}
\int dm {\cal N}(\bar{m},\sigma^2;m)\log(Z(\alpha;m)) &\equiv \langle\log(Z(\alpha;m))\rangle_{m} \\ \nonumber 
&= \lim_{Q \rightarrow 0} \left \langle \frac{Z(\alpha;m)^Q - 1}{Q}
\right \rangle_{m} \end{align}
where, for quenched averages, we denote the variable being averaged by the subscript of the angular brackets. 

Next, we shall compute the r.h.s. for all positive integer $Q$ and analytically continue the result to the positive reals so that we can take the limit. The uniqueness of such an analytical continuation, is unfortunately unclear. For instance, for any function we find one may add the function $\sin(\pi Q)$ as it is zero on integer $Q$'s. Fortunately, the simplest analytical continuation, which is to treat $Q$ as a real variable, is typically sufficient. 

Let us perform this procedure on the above example. Splitting $m$ into its mean part ($\bar{m}$) and fluctuation part ($m-\bar{m}$) we obtain  
\begin{align}
\notag \left\langle Z(\alpha;m)^Q 
\right\rangle_{m} &= \frac{1}{\sqrt{2\pi \sigma^2}}\int dm \int \dif f_1..\dif f_Q \; e^{-\frac{(m-\bar{m})^2}{2\sigma^2}-\sum_{a=1}^Q \left[\frac{f_a^2}{2}-[(m-\bar{m})+(\alpha+\bar{m})] f_a\right]}\\  &= ...
\end{align}
where each of the resulting $Q$ copies of $f$ is referred to as a replica of $f$. As advertised, for a given positive integer $Q$, this can be viewed as the partition function of $Q+1$ degrees of freedom, one of them being the action parameter. 

In general the replicas are decoupled for given parameter $m$, however integrating over $m$ generates a coupling between them namely
\begin{align}
... &= \int \dif f_1..\dif f_Q e^{-\sum_{a=1}^Q \left[\frac{f_a^2}{2}-(\alpha+\bar{m}) f_a\right]+\sigma^2 \sum_{ab=1}^Q \frac{f_a f_b}{2}} = ...
\end{align}
Notably the quadratic terms in the action can be written as $\frac{1}{2}\bm{f}^T [I-\sigma^2\Gamma] \bm{f}$, where $\bm{f}$ is a vector in replica space and $\Gamma$ is the all-ones matrix ($\Gamma_{ab}=1$). The $\Gamma$ matrix is rank one, with a single non-zero eigenvalue $Q$. Thus, the above Gaussian integral is non-convergent when $\sigma^2 \geq 1/Q$. Since we wish to obtain the behavior as $Q \rightarrow 0$, we do not concern ourselves with this issue and proceed with calculating it assuming $Q$ or $\sigma^2$ are small enough. This yields 
\begin{align}
... &= (\sqrt{2\pi})^Q e^{-\frac{1}{2}\Tr\log([I-\sigma^2 \Gamma])+\frac{(\alpha+\bar{m})^2}{2} \bm{1}^T [I + \sigma^2\Gamma]^{-1} \bm{1}} \\ \nonumber 
&= (\sqrt{2\pi})^Q e^{-\frac{1}{2}\log(1-Q\sigma^2)+\frac{(\alpha+\bar{m})^2}{2} \frac{Q}{1+Q\sigma^2}}\end{align}
where $\bm{1}$ is the all-ones vector in replica space. 

Next, we perform analytical continuation simply by treating $Q$ as a real variable, taking $Q
\rightarrow 0$, and using a Taylor expansion to yield 
\begin{align}
\lim_{Q \rightarrow 0} \left \langle \frac{Z(\alpha;m)^Q - 1}{Q}
\right \rangle_{m}  &= \lim_{Q \rightarrow 0}  \frac{e^{\frac{1}{2} Q\sigma^2+\frac{(\alpha+\bar{m})^2}{2}Q}-1}{Q} = \frac{1}{2}[\sigma^2+(\alpha+\bar{m})^2]
\end{align}
Next, we recall that the r.h.s. is just the quenched averaged free energy, hence taking its derivatives w.r.t. $\alpha$ should recover that quenched averaged cumulants. Indeed taking one derivative yields $\bar{f}=\bar{m}$, twice yields $1$ (the variance of $f$ which is unaffected by $m$). Further derivatives yield zero since, regardless of $m$, the distribution is Gaussian and its higher cumulants vanish. 
