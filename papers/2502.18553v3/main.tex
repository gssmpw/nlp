% !Mode:: "TeX:DE:UTF-8:Main"
%
%
%JOURNAL CODE  SEE DOCUMENTATION

\documentclass[examplefnt,biber]{nowfnt} % creates the journal version, needs biber version
%wrapper for book and ebook are created automatically.

\usepackage{graphicx}
\usepackage{subcaption}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{bm}
% \usepackage{soul}
\usepackage{cancel}
\usepackage[normalem]{ulem}

\usepackage{amsfonts}
\usepackage{mathtools}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}
% a few definitions that are *not* needed in general:
\newcommand{\ie}{\emph{i.e.}}
\newcommand{\eg}{\emph{e.g.}}
\newcommand{\etc}{\emph{etc}}
\newcommand{\now}{\textsc{now}}
\newcommand{\Tr}{\text{Tr}}
\newcommand{\MF}{\text{MF}}
\newcommand{\GP}{\text{GP}}
\newcommand{\inn}{\text{in}}
\newcommand{\GPR}{\text{GPR}}
\newcommand{\MSE}{\text{MSE}}
\newcommand{\EK}{\text{EK}}
\newcommand{\RG}{\text{RG}}
\newcommand{\eff}{\text{eff}}
\newcommand{\con}{\text{con}}
\newcommand{\Erf}{\text{Erf}}
\newcommand{\const}{\text{const}}
\newcommand{\dif}{\text{d}}
\newcommand{\data}{\text{data}}
\newcommand{\cL}{\mathcal{L}}
\newcommand{\cD}{\mathcal{D}}
\newcommand{\cN}{\mathcal{N}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\IS}[1]{\textcolor{blue}{{#1}}}
\newcommand{\NR}[1]{\textcolor{green}{{#1}}}
%ARTICLE TITLE
\title{Applications of Statistical Field Theory in Deep Learning}


%ARTICLE SUB-TITLE
\subtitle{A gentle introduction}


%AUTHORS FOR COVER PAGE 
% separate authors by \and, item by \\
% Don't use verbatim or problematic symbols.
% _ in mail address should be entered as \_
% Pay attention to large mail-addresses ...

%if there are many author twocolumn mode can be activated.
%\booltrue{authortwocolumn} %SEE DOCUMENTATION
%\maintitleauthorlist{
%Alet Heezemans \\
%now publishers, Inc. \\
%alet.heezemans@nowpublishers.com
%\and
%Mike Casey \\
%now publishers, Inc. \\
%mike.casey@nowpublishers.com
%}

%ISSUE DATA AS PROVIDED BY NOW
\issuesetup
{%
 copyrightowner={Z.~Ringel, I.~Seroussi, M.~Helias},
 volume        = xx,
 issue         = xx,
 pubyear       = 2025,
 isbn          = xxx-x-xxxxx-xxx-x,
 eisbn         = xxx-x-xxxxx-xxx-x,
 doi           = 10.1561/XXXXXXXXX,
 firstpage     = 1, %Explain
 lastpage      = 18
 }

%BIBLIOGRAPHY FILE
%\addbibresource{sample.bib}
%\addbibresource{testfnt.bib}
\addbibresource{sample-now.bib}

\usepackage{mwe}

%AUTHORS FOR ABSTRACT PAGE
\author[1]{Ringel,Zohar}
\author[1]{Rubin, Noa}
\author[1]{Mor, Edo}
\author[2,3]{Helias, Moritz}
\author[4]{Seroussi, Inbar}

\affil[1]{The Hebrew University of Jerusalem, Jerusalem, Israel.}
\affil[2]{Jülich Research Centre, Jülich, Germany.}
\affil[3]{RWTH Aachen University, Aachen, Germany.}
\affil[4]{Tel Aviv University, Tel Aviv, Israel.}

\articledatabox{\nowfntstandardcitation}

\begin{document}

\makeabstracttitle

\begin{abstract}
Deep learning algorithms have made incredible strides in the past decade, yet due to their complexity, the science of deep learning remains in its early stages. Being an experimentally driven field, it is natural to seek a theory of deep learning within the physics paradigm. As deep learning is largely about learning functions and distributions over functions, statistical field theory, a rich and versatile toolbox for tackling complex distributions over functions (fields) is an obvious choice of formalism. Research efforts carried out in the past few years have demonstrated the ability of field theory to provide useful insights on generalization, implicit bias, and feature learning effects. Here we provide a pedagogical review of this emerging line of research.    
\end{abstract}

\input{Chapters/Preliminaries}
\input{Chapters/Infinite_networks_in_standard_scaling}
\input{Chapters/Bayesian_Neural_Networks_in_the_Feature_Learning_Regime}
\input{Chapters/Field_theory_approach_to_dynamics}
\input{Chapters/Summary_and_Outlook}

%BACKMATTER SEE DOCUMENTATION
\backmatter  % references, restarts sample

%\printbibliography

\end{document}