\section{Insufficiency of Local Regularization in the Transductive Model}\label{Section:local-regularization-fails}

We are now equipped to prove the primary result of the paper: There exists a learnable hypothesis class $\Hotp$ which cannot be transductively learned by any local regularizer (\cref{Theorem:local-regularization-fails}). We begin by defining the class $\Hotp$ which witnesses this separation. Notably, $\Hotp$ is intimately related to secret sharing techniques, including the \emph{one-time pad}. More precisely, each hypothesis $h \in \Hotp$ is parameterized by two strings $A, B \in \{0, 1\}^*$, and $h(i) = (0,A)$ or $h(i) = (1,B)$ depending upon the $i$th bit of $A \oplus B$. When a training set $S$ contains two distinct labels, learning is trivial: only the ground truth function attains zero training error. When the data distribution places full measure on a single label, however,  correctly predicting an unseen test point $j$ requires a local regularizer which favors functions mapping $j$ to that same label. (And, owing to the structure of the one-time pad, knowledge of $A \oplus B$ at the training points and complete knowledge of $A$ reveals little information regarding $A \oplus B$ at an unseen test point.) Using a coupling argument, we demonstrate that no local regularizer can do so simultaneously for all labels. 

Recall that for binary strings $A, B \in \{0, 1\}^n$, $A \oplus B$ denotes their entrywise XOR. We set $\sigma_0(A) = \{i \in [n] : A(i) = 0\}$ and likewise $\sigma_1(A) = \{i \in [n] : A(i) = 1\}$. In the event that $|\sigma_0(A)| = |\sigma_1(A)|$, $A$ is said to be \emph{balanced}. 

\begin{definition}\label{Definition:secret-sharing-class}
Let $\CX = \N$ and $\CY = \{0, 1\} \times \{0, 1\}^*$. For each $d \in \N$ and $A, B \in \{0, 1\}^d$, define 
\begin{align*}
h_{A, B} \colon \CX &\longrightarrow \CY \\
x &\longmapsto \begin{cases} (0, A) & (A \oplus B)(x \bmod d) = 0, \\ (1, B) & (A \oplus B)(x \bmod d) = 1. \end{cases} 
\end{align*} 
Then the hypothesis class $\Hotp \subseteq \CY^\CX$ is defined as
\[ \Hotp = \Big \{ h_{A, B} : A, B \in \{0, 1\}^*, |A| = |B|, A \oplus B \text{ is balanced} \Big \}.  \] 
\end{definition}

\begin{remark}
For those familiar with the \emph{first Cantor class} of \citet{DS14}, denoted $\Cantor$, note that $\Hotp$ can be seen as generalizing this class. In particular, $\Cantor$ is defined by ``glueing together" the classes $\{ \CH_d \}_{d \in \N}$, where $\CH_d \subseteq \CX_d^{\CY_d}$, $\CX_d$ is a set of size $d$, $\CY_d = 2^{\CX_d} \cup \{*\}$, and 
\[ \CH_d = \big \{ h_A : A \subseteq \CX_d, |A| = d / 2 \big \} \]
where 
$h_A(x) = A$ if $x \in A$ and $*$ otherwise. 
Each such $\CH_d$ can equivalently be defined in the language of Definition~\ref{Definition:secret-sharing-class} by identifying each $A \subseteq \CX_d$ with its characteristic vector, setting $h_A := h_{A, 1^d}$, and considering the relabeling $(1, 1^d) \mapsto *$, $(0, A) \mapsto A$. 
\end{remark}

\begin{lemma}
$\Hotp$ is a GBDLS class.
\end{lemma}
\begin{proof}
It is immediate from Definition~\ref{Definition:secret-sharing-class} that each $h \in \CH$ has $|\im(h)| = 2$, meaning $\CH$ is generalized binary. Furthermore, if $f \in \CH$ is such that $\im(f) = \{(0, A), (1, B)\}$ then it must be that $f = h_{A, B}$. Thus $\Hotp$ has distinct label sets. 
\end{proof}

We first equate the task of learning with a local regularizer to learning with a local regularizer which is \emph{locally injective}, i.e., injective on $\CH$ for each fixed choice of test point $x \in \CX$. 

\begin{definition}
A local regularizer $\psi \colon \CH \times \CX \to \R_{\geq 0}$ is \defn{locally injective} if $\psi(\arg, x)$ is injective for each $x \in \CX$. That is, $\psi(h, x) \neq \psi(h', x)$ for any $x \in \CX$ and $h \neq h' \in \CH$. 
\end{definition}

The following lemma establishes that for countable hypothesis classes $\CH$, their local regularizers $\psi$ can be assumed to be locally injective (i.e., to totally order $\CH$ at each location $x \in \CX$, rather than merely partially order). Note that in this case, $\psi$ induces a unique learner, as there is no ``tie-breaking" left to its induced learners. 

\begin{lemma}\label{Lemma:locally-injective-regularizer}
Let $\CH \subseteq \CY^\CX$ be a countable hypothesis class. Then $\CH$ can be learned by a local regularizer if and only if it can be learned by one which is locally injective. 
\end{lemma}
\begin{proof}
The backward direction is immediate. For the forward, let $\psi$ be a local regularizer which learns $\CH$, meaning that each of its induced learners succeeds on $\CH$. Recall that for each $x \in \CX$, $\psi( \cdot, x)$ defines a (strict) partial order over $\CH$, i.e., with $h < h'$ if $\psi(h, x) < \psi(h', x)$. For each such $x$, let $\overline{\psi(\cdot, x)}$ be any completion of this partial order into a total ordering. As $\CH$ is countable, $\overline{\psi(\cdot, x)}$ can be embedded into the real numbers. Define $\overline{\psi}(\cdot, x)$ using such an embedding, and consider the unique learner $\CA$ induced by $\overline{\psi}$. As $\overline{\psi}$ acts as a completion of $\psi$ at each $x \in \CX$, then $\CA$ is also a learner induced by $\psi$. Thus $\CA$ learners $\CH$, by our assumption on $\psi$, meaning $\overline{\psi}$ learns $\CH$. 
\end{proof}

\begin{figure}[t] 
\centering
\input{figure.tikz}
\vspace{-40 pt}
\caption{Depiction of the learning problems $\{(S_i,h^*_i)\}_{i \in [4]}$ in \Cref{Theorem:local-regularization-fails}, with training sets $S_i$ circumscribed in blue. Each row corresponds to one of the learning problems $(S_i,h^*_i)$. Crucially, success of a local regularizer $\psi$ on each learning problem imposes the ordering relations on the right-hand side, which collectively produce a cycle.} \label{Figure:1}
\label{figure:bipartite}
\end{figure}

\begin{theorem}\label{Theorem:local-regularization-fails}
$\Hotp$ is a learnable hypothesis class, but cannot be transductively learned by any local regularizer.
\end{theorem}
\begin{proof}
Fix a local regularizer $\psi$ and $d \in \N$. By Lemma~\ref{Lemma:locally-injective-regularizer}, we may assume that $\psi$ is locally injective, inducing a unique learner $\CA$. Using a probabilistic argument, we will demonstrate that there exists a transductive learning instance $(S, h^*)$ with $|S| = 2d$ for which $\psi$ incurs error at least $\frac{1}{4}$. To this end, define the following independent random variables: 
\begin{itemize}
    \itemsep 0em
    \item $C$ is drawn uniformly at random from all balanced strings of length $2d$. 
    \item $A$ is drawn uniformly at random from $\{0, 1\}^{2d}$. 
    \item $m_0$ and $m_1$ are drawn uniformly at random from $[d]$. 
\end{itemize}
Further, define $\xtest$ to be the index of the $m_0$-th 0 entry in $C$, and $\xfool$ to be the index of the $m_1$-th 1 entry in $C$. (Note that such entries exist for any value of $m_0, m_1 \in [d]$, owing to the fact that $C$ is balanced.) 

We now define four random variables $\{T_i\}_{i \in [4]}$ based upon the previous variables. Each one is parameterized by a tuple $(S_i, h^*_i)_{i \in [4]}$ and measures the performance of $\CA$ at the test point $\xtest$ when trained the datapoints in $S_{i} \setminus \{\xtest\}$ labeled by $h^*_i$, the ground truth function. The $S_i$ and $h^*_i$ are defined as follows: 

\begin{itemize}
    \item[(1.)] Let $h^*_{1} = h_{A, C \oplus A}$, $S_1 = \sigma_0(C)$. Thus 
    \[  T_1 = \Big[\CA\big(S_1 \setminus \{\xtest\}, h^*_1\big)(\xtest) \neq h^*_1(\xtest) \Big],  \]
    where $\CA\big(S_1 \setminus \{\xtest\}, h^*_1\big)$ denotes the output of $\CA$ when trained on the dataset consisting of the points in $S_1 \setminus \{\xtest\}$ labeled by $h^*_1$. 
    
    \item[(2.)] Let $h^*_2 = h_{A, \overline{C} \oplus A}$, where $\overline{C}$ equals $C$ with its $\xtest$-th and $\xfool$-th entries each flipped.\footnote{Note that $\overline{C}$ is balanced, as $\xtest$ corresponds to a 0 entry in $C$ and $\xfool$ to a 1 entry.} Let $S_2 = \sigma_1(\overline{C}) = \sigma_1(C) \cup \{\xtest\} \setminus \{\xfool\}$. Thus 
    \[ T_2 = \big[\CA\big(S_2 \setminus \{\xtest\}, h^*_2\big)(\xtest) \neq h^*_2(\xtest) \big]. \]  
    
    \item[(3.)] Let $h^*_3 = h_{\overline{A}, C \oplus \overline{A}}$ where $\overline{A}$ denotes $A$ with its $\xtest$-th and $\xfool$-th entries each flipped. Let $S_3 = \sigma_0(C) = S_1$. Thus 
    \[ T_3 = \big[\CA\big(S_3 \setminus \{\xtest\}, h^*_3\big)(\xtest) \neq h^*_3(\xtest) \big]. \]  
    
    \item[(4.)] Let $h^*_4 = h_{\overline{A}, \overline{C} \oplus \overline{A}}$, $S_4 = \sigma_1(\overline{C}) = S_2$. Thus 
    \[ T_4 = \big[\CA\big(S_4 \setminus \{\xtest\}, h^*_4\big)(\xtest) \neq h^*_4(\xtest) \big]. \]  
\end{itemize}

\noindent It remains to prove the following lemma. 
\vspace{-0.7 cm}
\begin{quote}
\begin{lemma}\label{Lemma:internal-main-theorem}
Each random variable $T_i$, for $i \in [4]$, is distributed as the  error $\psi$ incurs on the (randomly-chosen) transductive learning instance $(S_i,h^*_i)$. Furthermore, for any value of the variables $C$, $A$, $m_0$, and $m_1$, the local regularizer $\psi$ must err at $\xtest$ in at least one of the instances $(S_i,h^*_i)_{i \in [4]}$. (That is, $\max_i T_i = 1$.)   
\end{lemma}
\begin{proof}
The first claim amounts to demonstrating that for each $i \in [4]$, conditioned upon $h^*_i$ and $S_i$, $\xtest$ is distributed uniformly randomly across $S_i$. For $i \in \{1, 3\}$, this is immediate. For $i \in \{2, 4\}$, recall that $S_2 = S_4 = \sigma_1(C) \cup \{\xtest\} \setminus \{\xfool\}$, and that $\xtest$ and $\xfool$ are independent and uniformly random elements of $\sigma_0(C)$ and $\sigma_1(C)$, respectively. Further, $C$ is chosen uniformly at random from the set of all balanced $2d$-strings. 
Then, conditioned upon $\overline{C}$, the probability that $\xtest$ takes the value of a given $s \in S = \sigma_1(\overline{C})$ is proportional to the cardinality of 
\begin{align*}
\Big\{ (\h{C}, \widehat{x}_{\mathrm{fool}}) &: s \in \sigma_0(\h{C}), \widehat{x}_{\mathrm{fool}} \in \sigma_1(\h{C}), \h{C} \text{ is balanced}, \h{C} \oplus e(s) \oplus e(\widehat{x}_{\mathrm{fool}}) = \overline{C} \Big\} \\
&= \Big\{ (\h{C}, \widehat{x}_{\mathrm{fool}}) : s \in \sigma_0(\h{C}), \widehat{x}_{\mathrm{fool}} \in \sigma_1(\h{C}), \h{C} \text{ is balanced}, \h{C} = \overline{C} \oplus e(s) \oplus e(\widehat{x}_{\mathrm{fool}}) \Big\}  \\
&= \Big\{ (\h{C}, \widehat{x}_{\mathrm{fool}}) : s \in \sigma_1(\overline{C}), \widehat{x}_{\mathrm{fool}} \in \sigma_0(\overline{C}), \h{C} \text{ is balanced}, \h{C} = \overline{C} \oplus e(s) \oplus e(\widehat{x}_{\mathrm{fool}}) \Big\}  \\
&= \Big\{ (\h{C}, \widehat{x}_{\mathrm{fool}}) : \widehat{x}_{\mathrm{fool}} \in \sigma_0(\overline{C}), \h{C}  = \overline{C} \oplus e(s) \oplus e(\widehat{x}_{\mathrm{fool}}) \Big\} \\
&\cong \Big\{ \widehat{x}_{\mathrm{fool}} : \widehat{x}_{\mathrm{fool}} \in \sigma_0(\overline{C}) \Big\}.
\end{align*}
The first equality follows from the observation that $\h{C} \oplus e(s) \oplus e(\widehat{x}_{\mathrm{fool}}) = \overline{C}$ is symmetric in $\h{C}$ and $\overline{C}$. The second equality rephrases the conditions on $s$ and $\widehat{x}_{\mathrm{fool}}$ in terms of $\overline{C}$, rather than $\h{C}$. The third equality employs the fact that $s \in \sigma_1(\overline{C})$ by definition, and that any $\h{C}$ satisfying the remaining conditions is automatically balanced. Thus the cardinality of this set does not depend upon $s$, completing the argument. 

For the second claim, note that $h_1^*(\xtest) \neq h_2^*(\xtest)$, as $C(\xtest) = 0$ yet $\overline{C}(\xtest) = 1$. However, $h_2^*$ attains zero empirical error on $S_1 \setminus \{\xtest\}$, as for any $s \in S_1  \setminus \{\xtest\}$,
\begin{align*}
h_1^*(s) = h_{A, C \oplus A}(s) = (0, A) = h_{A, \overline{C} \oplus A}(s) = h_2^*(s),
\end{align*}
where the first and third equalities use the fact that $s \in S_1 \setminus \{\xtest\} \subseteq \sigma_0(C) \cap \sigma_0(\overline{C})$. Thus, in order for $\psi$ to correctly classify task (1.), it must be that $\psi(h^*_1, \xtest) < \psi(h^*_2, \xtest)$. Likewise, $h_2^*(\xtest) \neq h_3^*(\xtest)$ because $C(\xtest) = 0 \neq 1 =  \overline{C}(\xtest)$, yet $h_3^*$ attains zero empirical error on $S_2 \setminus \{\xtest\}$, as $S_2 \setminus \{\xtest\} \subseteq \sigma_1(C) \cap \sigma_1(\overline{C})$ and $\overline{C} \oplus A = C \oplus \overline{A}$. Thus, for $\psi$ to correctly classify task (1.) would require that $\psi(h^*_2, \xtest) < \psi(h^*_3, \xtest)$. 

Invoke this reasoning twice more with $h^*_3$ and $h^*_4$; see \Cref{Figure:1}. In particular, $h^*_3(\xtest) \neq h^*_4(\xtest)$ as $C(\xtest) \neq \overline{C}(\xtest)$, yet $S_3 \setminus \{\xtest\} = \sigma_0(C) \setminus \{\xtest\} \subseteq \sigma_0(C) \cap \sigma_0(\overline{C})$. Thus $\psi$'s success on $T_3$ relies upon $\psi(h^*_3, \xtest) < \psi(h^*_4, \xtest)$. Finally, $h^*_4(\xtest) \neq h^*_1(\xtest)$, but $S_4 \setminus \{\xtest\} = \sigma_1(\overline{C}) \setminus \{\xtest\} \subseteq \sigma_1(C) \cap \sigma_1(\overline{C})$ and $C \oplus A = \overline{C} \oplus \overline{A}$. Thus success of $\psi$ on $T_4$ imposes the final requirement that $\psi(h^*_4, \xtest) < \psi(h^*_1, \xtest)$. It follows immediately that $\psi$ cannot succeed on all at once. 
\end{proof}
\end{quote}

By the second claim of Lemma~\ref{Lemma:internal-main-theorem}, the error $\psi$ incurs at $\xtest$, on average over the instances $(S_i, h^*_i)_{i \in [4]}$, is $\frac{1}{4}\sum_{i=1}^4 \E [T_i] \geq \frac{1}{4}$.  By the first claim of Lemma~\ref{Lemma:internal-main-theorem}
and a use of the probabilistic method, this implies the existence of a single transductive learning instance (on $2d$ points) for which $\psi$ incurs transductive error at least $\frac{1}{4}$. Conclude by recalling that $d$ was chosen arbitrarily. 
\end{proof}
