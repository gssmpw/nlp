\section{Preliminaries}\label{Section:Preliminaries}

\subsection{Notation}

For a natural number $d \in \N$, we denote $[d] = \{1, \ldots, d\}$. For $Z$ a set, we use $Z^*$ to denote the set of all finite sequences in $Z$, i.e., $Z^* = \bigcup_{n \in \N} Z^n$. In particular, $\{0,1\}^*$ denotes the set of all finite binary strings. For $A, B \in \{0, 1\}^d$ binary strings of equal length, $A \oplus B$ refers to their entrywise XOR. We use $\sigma_0(A)$ and $\sigma_1(A)$ to refer to the entries at which $A$ takes the value of 0 or 1, respectively. That is, $\sigma_0(A) = \big\{ i \in [d] : A(i) = 0 \big\}, \sigma_1(A) = \big\{ i \in [d] : A(i) = 1 \big\}$.
We use $e(i)$ to denote the $i$th standard basis vector, whose length will always be clear from context (i.e., the vector with a 1 in its $i$th entry and zeroes elsewhere). 
For a statement $P$, $[P]$ denotes the Iverson bracket of $P$, i.e., $[P] = 1$ when $P$ is true and 0 otherwise. For a function $f: A \to B$, we let $\im(f) = \{f(a) : a \in A\}$ denote the image of $f$ in $B$.

\subsection{Learning Theory}

Unlabeled datapoints $x$ are drawn from a \defn{domain} $\CX$ and labeled by an element of the \defn{label set} $y \in \CY$. Pairs $(x, y) \in \CX \times \CY$ are referred as labeled datapoints or \emph{examples}. A \defn{training set} or training sequence is a tuple of labeled datapoints $S \in (\CX \times \CY)^n$. When clear from context, we will refer to labeled and unlabeled datapoints simply as \emph{datapoints}. A function $f \colon \CX \to \CY$ is a \defn{classifier} or \defn{predictor}. A collection of classifiers $\CH \subseteq \CY^\CX$ is referred to as a \defn{hypothesis class}; one of its elements $h \in \CH$ is a \defn{hypothesis}. Throughout the paper we employ the 0-1 loss function $\ell_{0-1} \colon \CY \times \CY \to \R_{\geq 0}$ defined by $\ell_{0-1}(y, y') = \big[ y \neq y' \big]$.
When $S = (x_i, y_i)_{i \in [n]}$ is a training set and $f \in \CY^\CX$ a classifier, the average loss incurred by $f$ on $S$ is referred to as its \defn{empirical risk} and denoted $L_S(f)$. That is, 
\[ L_S(f) = \frac{1}{n} \sum_{i \in [n]} \ell_{0-1} \big( f(x_i), y_i \big). \]

A \defn{learner} is a function from training sets to classifiers, e.g., $\CA : (\CX \times \CY)^* \to \CY^\CX$. We focus on learning in the realizable case, for which the purpose of a learner is to deduce the behavior of a \emph{ground truth} function $h^* \in \CH$ given its behavior on a finite training set. More precisely, we adopt the transductive model of learning, as employed by \citet{haussler1994predicting} and originally introduced by \citet{vapnik1974theory} and \citet{vapnik1982estimation}. 

\begin{definition}
The \defn{transductive model} of learning is that in which the following sequence of steps take place: 
\begin{enumerate}
    \item An adversary selects a collection of $n$ unlabeled datapoints $S  = (x_{i})_{i\in [n]}$ and a hypothesis $h^* \in \CH$. 
    \item The unlabeled datapoints $S$ are displayed to the learner. 
    \item An index $j \in [n]$ is selected at uniformly at random. The learner then receives the training set $(x_i, h^*(x_i))_{i \neq j}$. 
    \item The learner is prompted to predict the label of $x_j$, i.e., $h^*(x_j)$.
\end{enumerate}
\end{definition}

We refer to the choice of $S$ and $h^*$ parameterizing the above steps as a \emph{transductive instance}, and often collect this information into a pair, i.e., $(S, h^*)$. The \defn{transductive error} incurred by a learner on an instance $(S, h^*)$ is its average loss at the test point $x_j$, over the uniformly random choice of $j \in [n]$, i.e.,
\[ L^{\Trans}_{S, h^*}(\CA) = \frac 1n \sum_{i \in [n]} \big[ \CA(S_{-i}, h^*)(x_i) \neq h^*(x_i) \big], \]
where $\CA(S_{-i}, h^*)(x_i)$ denotes $\CA$'s output on the sample consisting of datapoints in $S_{-i} = S \setminus x_i$ labeled by $h^*$. 
Naturally, a class is transductively learnable if it can be learned to vanishingly small error on increasingly large datasets. 

\begin{definition}\label{Definition:trans-learning}
A hypothesis class $\CH \subseteq \CY^\CX$ is \defn{transductively learnable} if there exists a learner $\CA$ and function $m \colon (0, 1) \to \N$ such that for any $\epsilon \in (0, 1)$ and $h^* \in \CH$, if $S \in \CX^*$ has length $|S| \geq m(\epsilon)$, then 
\[ L_{S, h^*}^{\Trans}(\CA) \leq \epsilon. \]
\end{definition}

The pointwise minimal function $m$ such that there exists a learner $\CA$ satisfying \Cref{Definition:trans-learning} is referred to as the \emph{transductive sample complexity} of learning $\CH$, denoted $m_{\Trans, \CH}$, and learners which attain this sample complexity are said to be \emph{optimal}. (For our purposes, however, it will suffice to consider the property of learnability, without emphasizing sample complexities.) Notably, the transductive model of learning bears intimate connections with the Probably Approximately Correct (PAC) model of learning \citep{valiant1984theory}, which considers underlying probability distributions and requires learners to perform well on  randomly drawn test points when trained on i.i.d.\ training points. In particular, there is an equivalence between sample complexities in both models (up to a logarithmic factor), and techniques from transductive learning have recently been employed to establish the first characterizations of learnability for both multiclass classification and realizable regression \citep{brukhim2022characterization, attias2023optimal}. 
We defer a dedicated discussion of the PAC model to \Cref{Section:PAC-difficulties}.

In the landscape of learning, perhaps the most fundamental learners are given by ERM and SRM, which operate by selecting a hypothesis $h$ in the underlying class $\CH$ with lowest empirical risk (possibly balanced with an inductive bias over hypotheses in $\CH$, in the case of SRM). 

\begin{definition}\label{Definition:ERM}
A learner $\CA$ is an \defn{empirical risk minimization} (ERM) learner for a class $\CH$ if for all samples $S$, 
\[ \CA(S) \in \argmin_{h \in \CH} L_{S}(h). \] 
\end{definition}

\begin{definition}\label{Definition:SRM}
Let $\CH$ be a hypothesis class. A \defn{regularizer} for $\CH$ is a function $\psi: \CH \to \R_{\geq 0}$.
A learner $\CA$ is a \defn{structural risk minimization} (SRM) learner for a class $\CH$ if there exists a regularizer $\psi$ such that for all samples $S$, 
\[ \CA(S) \in \argmin_{h \in \CH} \Big( L_{S}(h) + \psi(h) \Big). \]
\end{definition}

\subsection{Local Regularization}

In realizable binary classification, ERM learners are known to succeed on all learnable classes, and furthermore to attain nearly-optimal sample complexity \citep{vapnik1974theory,BEHW89,ehrenfeucht1989general}. For multiclass learning over arbitrary label sets, however, these learning strategies are known to fail as a consequence of \citet[Theorem~1]{DS14}. In particular, \citet{DS14} establish that there exist multiclass problems which can only be learned by \defn{improper learners}: learners which may emit classifiers outside of the underlying hypothesis class $\CH$. (Note that ERM and SRM learners, in being phrased as arg min's over $\CH$, are necessarily \defn{proper}.) More recently, \citet{asilisunderstanding} expanded upon this result by demonstrating that there exist learnable multiclass problems which cannot be learned by any \emph{aggregation} of a finite number of proper learners, ruling out such strategies as majority voting of ERM learners, which has recently found success in binary classification \citep{aden2024majority,hogsgaardmany}. 

As such, multiclass learning in full generality requires different algorithmic blueprints than ERM and SRM. Perhaps the simplest such blueprint which has been considered is that of \emph{local regularization}, as introduced by \citet{asilis2024regularization}. Put simply, local regularization augments the regularizer --- usually a function $\psi \colon \CH \to \R_{\geq 0}$ --- to additionally receive an unlabeled datapoint as input, i.e., $\psi \colon \CH \times \CX \to \R_{\geq 0}$. The unlabeled datapoint is taken to be the test datapoint at which the learner is tasked with making a prediction. At each test datapoint $x \in \CX$, the learner then takes the behavior of the hypothesis $\widehat{h}$ with minimal value of $\psi(\widehat{h}, x)$, subject to $L_S(\widehat{h}) = 0$. 

This bears two crucial advantages over classical regularization.
\begin{itemize}
    \item[1.] Local regularization models the \emph{location-dependent} complexity of a hypotheses. It may be that $h$ acts as a simple function on a region $U \subseteq \CX$ yet as a complex function on $V \subseteq \CX$ (and vice versa for $h' \in \CH$). A local regularizer $\psi$ can model this behavior as $\psi(h, u) < \psi(h', u)$ for $u \in U$ and $\psi(h, v) > \psi(h', v)$ for $v \in V$. A classical regularizer, in contrast, is obligated to assign each of $h$ and $h'$ with a single value encoding their global complexity. 

    \item[2.] Local regularizers can induce improper learners, by emitting classifiers that ``stitch together" various hypotheses of $\CH$. In the previous case, for instance, one can imagine a learner $\CA$ induced by a local regularizer which takes the behavior of $h$ on $U \subseteq \CX$ and of $h'$ on $V \subseteq \CX$. 
\end{itemize}

\begin{definition}[{\cite{asilis-open-problem}}]\label{Definition:local-regularizer}
A \defn{local regularizer} for a hypothesis class $\CH$ is a function $\psi \colon \CH \times \CX \to \R_{\geq 0}$. A learner $\CA$ is said to be \defn{induced} by $\psi$ if for all samples $S$ and datapoints $x \in \CX$, 
\[ \CA(S)(x) \in \Big\{ h(x) : h \in \argmin_{h \in L_{S}^{-1}(0)} \psi(h, x) \Big\}. \]
We say that $\psi$ \defn{transductively learns} $\CH$ if all learners it induces are transductive learners for $\CH$. 
\end{definition}

\begin{remark}
Restricting to the set of hypotheses which incur zero empirical error in \Cref{Definition:local-regularizer}, rather than minimizing the sum of hypotheses' empirical error and regularization value, has the effect of simplifying the analysis of regularization. Furthermore, simply normalizing a regularizer to take outputs in the range $[0, \nicefrac{1}{n}]$ has the effect of equating the two perspectives for samples of size at most $n$. (Though this procedure is not uniform with respect to all possible sample sizes.) 
\end{remark}

\citet{asilis-open-problem} posed the open problem of whether local regularization is sufficiently expressive to learn all multiclass problems possible. 

\begin{open}[{\cite{asilis-open-problem}}]\label{Open-problem}
In multiclass classification, can all learnable hypothesis classes be learned by a local regularizer? If so, with optimal (or nearly optimal) sample complexity?
\end{open}

In Theorem~\ref{Theorem:local-regularization-fails} we resolve Open Problem~\ref{Open-problem} for the transductive model of learning, by exhibiting a learnable class which cannot be transductively learned by any local regularizer. In \Cref{Section:PAC-difficulties} we discuss the possibility of extending this result to the PAC model, and describe some of the challenges involved. 

\subsection{Secret Sharing}

We now provide a brief review of topics in \emph{secret sharing}, as the proof of our primary result, Theorem~\ref{Theorem:local-regularization-fails}, employs a hypothesis class whose structure is intimately related to concepts from the field.

\defn{Secret sharing} refers to the fundamental task of distributing private information, a \emph{secret}, among a group of \emph{players}. A successful solution consists of a technique for distributing information from a single \emph{dealer} to each player in such a manner that any individual player is incapable of recovering the secret, yet it can be revealed when the group works in concert. The information revealed to each player individually is referred to as a \emph{share}. In the general setting, there are $n$ players and the secret should be recoverable by any group of $t$ cooperating players, but not by any smaller group. (More precisely, any smaller group should not even be able to deduce partial information concerning the secret.) A solution to this problem is referred to as a $(t, n)$-threshold scheme.\footnote{In an even-more-general setting, the problem is defined by a collection of \emph{accessor subsets}, i.e., sets of players that should be able to deduce the secret when cooperating \citep{ito1989secret}.}

The study of secret sharing dates to the work of \citet{shamir1979share} and \citet{blakley1979safeguarding}; for a contemporary introduction, see \citet{beimel2011secret} or \citet{krenn2023introduction}. \citet{shamir1979share} designed an elegant $(t, n)$-threshold scheme which proceeds as follows: Let $q > n$ be a sufficiently large prime to contain all possible secrets, $k \in [q]$ the secret, and select $a_1, \ldots, a_{t-1}$ uniformly at random from $[q]$. Lastly, define the polynomial $P(x) = k + \sum_{i=1}^{t-1} a_i x^i \mod q$, and distribute to the $j$th player the share $(j, P(j))$. Then the cooperation of any $t$ players suffices to reveal $P$ (and thus $k$) owing to Lagrange's interpolation theorem, yet any smaller collection of players can reveal no information concerning $k = P(0)$ owing to the uniformly random choices of $a_1, \ldots, a_{t-1}$. 

For our purposes, however, it will suffice to consider the basic case of $t = n = 2$, for which there is a strikingly simple secret-sharing method referred to as the \defn{one-time pad} (OTP) in cryptography. Namely, given a secret $C \in \{0, 1\}^n$, the one-time pad selects a string $A$ uniformly at random from $\{0, 1\}^n$, and distributes to player one the share $A$ and to player two the share $A \oplus C$. Individually, then, each player witnesses a uniformly random string of length $n$, yet the XOR of their shares is precisely the secret, $A \oplus (A \oplus C) = C$. In \Cref{Section:local-regularization-fails}, we design a hypothesis class inspired by the one-time pad, for which --- roughly speaking --- each hypothesis $h$ is represented by a secret and each of its two possible outputs reveals one share of the secret. Crucially, then, the information of one of $h$'s secrets (i.e., its behavior on one half of the domain) maintains the identity of its remaining secret completely opaque (i.e., its behavior on the remaining half of the domain).\footnote{Strictly speaking, for the hypothesis class we construct, one of $h$'s ``secrets" and its behavior on a region of the domain can reveal partial information concerning its other ``secret."}
