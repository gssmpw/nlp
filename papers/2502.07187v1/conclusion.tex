\section{Conclusion}\label{Section:conclusion}

We study perhaps the simplest candidate template for multiclass learning: local regularization \citep{asilis-open-problem}. As our primary result, we demonstrate that there exists a learnable hypothesis class which cannot be transductively learned by any local regularizer. The hypothesis class $\Hotp$ which we employ for this result is based upon techniques from \emph{secret-sharing}, such as the one-time pad, and generalizes the \emph{first Cantor class} of \citet{DS14}. We conjecture that the same class also cannot be learned by any local regularizer in the PAC model, though we highlight some of the difficulties involved in extending our result in \Cref{Section:PAC-difficulties}.  