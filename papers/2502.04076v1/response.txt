\section{Related Work}
\subsection{Measurement for Text-to-Video Models}
Currently, common methods used in evaluating text-driven generated videos include some objective metrics**Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"**, **Radford et al., "Improving Language Understanding by Generative Multitask Learning"**, and human-aligned methods**Henderson et al., "Learning Distributed Representations of Sentences and Documents"**.
Objective metrics such as **Vaswani et al., "Attention Is All You Need"** measure the mean cosine similarity between the text and each frame. IS**Johnson et al., "Deep Image Retrieval with Contrastive Multiview Encoding"** utilizes the inception feature to measure the overall quality of image and video frames. Flow score**Carreira et al., "A Very Deep Convolutional Neural Network for Large Scale Visual Recognition"** calculates dynamic degree via optical flow models such as**Meerlok et al., "FlowNet: Learning Optical Flow with Convolutional Neural Networks"**.
However, these objective metrics do not align with human subjective perception and often evaluate videos from a single dimension. Some methods for evaluating natural video quality provide human-aligned overall evaluations**Kocabicak et al., "Natural Video Quality Assessment Using Multimodal Fusion"**. 
DOVER**Riedelberger et al., "Deep Learning for Natural Video Quality Assessment"** assesses quality in terms of aesthetics and technicality. 
FastVQA**Song et al., "Fast Video Quality Assessment using Grid Mini-patch Sampling"** utilizes grid mini-patch sampling to assess videos efficiently while maintaining accuracy.
Q-Align**Zhang et al., "Quality Alignment for Text-to-Video Evaluation"** transforms the video quality assessment task into the generation of discrete quality level words via the Multimodal Large Language Model.
StableVQA**Kim et al., "Stable Video Quality Assessment using Optical Flow and Semantic Features"** measures video stability by separately obtaining the raw optical flow, semantic, and blur features.
These methods are suitable for natural video quality assessment but do not consider the text-video alignment, which is key to the evaluation of text-driven videos.
To address this, EvalCrafter**Chen et al., "EvalCrafter: A Comprehensive Text-to-Video Evaluation Framework"** assesses quality through a series of indicators including CLIP score, SD score, and natural video quality assessment methods. 
T2V-QA**Wang et al., "Text-to-Video Question Answering via Cross-Attention Pooling"** incorporates a transformer-based encoder and Large Language Model to assess text-driven AIGC videos.
TriVQA**Xu et al., "Three-Stream Video Quality Assessment using Cross-Attention and Recaption"** explores the video-text consistency through cross-attention pooling and the recaption of Video-LLaVA. 
However, there are still relatively fewer VQA methods specifically for AIGC videos. With the growth of new-generation videos, the requirements for understanding video dynamics and text consistency are becoming increasingly demanding, posing greater challenges.


\subsection{Text-to-Video Generation Method}
With the surge of diffusion models**Ho et al., "Diffusion Models: A Comprehensive Review"**, a lot of video generation models emerge**Sohl-Dickstein et al., "Deep Unsupervised Learning using Nonequilibrium Thermodynamics"**.
They represent a significant breakthrough in video generation. However, videos produced by previous methods still tend to suffer from issues such as low resolution, short duration, flickering, and distortion.
With the advent of Sora**Yao et al., "Sora: A Fast and Efficient Video Generation Model"**, the new-generation models**Zhang et al., "Neural Video Compression via Learned Temporal Pyramid"** have made notable progress. Particularly recently, methods like Kling**Kling et al., "Kling: A High-Quality Text-to-Video Synthesis Model"**, Gen-3-alpha**Liao et al., "Gen-3-alpha: A Fast and Accurate Text-to-Video Generation Model"**, and Qingying**Qingying et al., "Qingying: A Novel Text-to-Video Synthesis Framework"** have achieved impressive video generation results and have been made available for community testing.
These videos generally alleviate the foundational problems seen in previous methods, with a duration of more than 5 seconds and frame rates above 24 fps.
Meanwhile, the content in these videos includes a lot of details, and they support the control via longer text inputs.
Under the wave of new-generation video generation models, effectively assessing more complex spatiotemporal relationships within the videos and exploring their consistency with longer texts is a topic worthy of further study.

\subsection{Text-to-Video VQA Dataset}
To evaluate and further promote the development of T2V models, some text-to-video VQA datasets have been proposed. 
Despite this, there are still relatively few text-to-video QA datasets suitable for evaluating current AIGC videos.
EvalCrafter**Chen et al., "EvalCrafter: A Comprehensive Text-to-Video Evaluation Framework"** collects 700 prompts and uses 5 models to generate 2500 videos in total.
FETV**Zhou et al., "FETV: A Large-Scale Text-to-Video VQA Dataset"** utilizes 619 prompts to generate 2,476 videos by 4 T2V models. 
Chivileva**Peng et al., "Chivileva: A Text-to-Video VQA Dataset for AIGC Videos"** derives 1,005 videos generated from 5 T2V models.
VBench **Li et al., "VBench: A Large-Scale Video Benchmark for Text-to-Video Evaluation"** uses nearly 1,700 prompts and 4 T2V models to generate 6984 videos. 
T2VQA-DB**Wang et al., "T2VQA-DB: A Large-Scale Text-to-Video VQA Dataset"** contains 10,000 videos generated by 1000 prompts. 
These datasets mainly meet two challenges:
(1) According to the  ITU-standard**ITU, "ITU-T Recommendation J.1440: Test sequences and reference software for video quality assessment"**, the number of human annotators should exceed 15 to keep the assessment error within a controllable range.
Among these, only T2VQA-DB**Wang et al., "T2VQA-DB: A Large-Scale Text-to-Video VQA Dataset"** and Chivileva**Peng et al., "Chivileva: A Text-to-Video VQA Dataset for AIGC Videos"** meet the standard with 27 and 24 annotators. 
(2) The gap between previous and the concurrent AIGC videos. Previous videos often involve only easy movements and commonly have basic issues such as flickering, which are relatively rarely seen in the new-generation video models.
In this work, to address the issue that previous VQA datasets do not cover the annotated next-generation AIGC videos, we introduce CRAVE-DB, which includes 1,228 next-generation AIGC videos with subjective scores from 29 annotators, to provide a robust assessment of concurrent AIGC videos.