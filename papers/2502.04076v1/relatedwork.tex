\section{Related Work}
\subsection{Measurement for Text-to-Video Models}
Currently, common methods used in evaluating text-driven generated videos include some objective metrics~\cite{clip,fvd,is} and human-aligned methods~\cite{pickscore,trivqa,t2vqa}.
Objective metrics such as CLIP-score~\cite{clip} measure the mean cosine similarity between the text and each frame. IS~\cite{is} utilizes the inception feature to measure the overall quality of image and video frames. Flow score~\cite{vbench} calculates dynamic degree via optical flow models such as~\cite{raft,skflow}.
However, these objective metrics do not align with human subjective perception and often evaluate videos from a single dimension. Some methods for evaluating natural video quality provide human-aligned overall evaluations~\cite{dover,fastvqa,stablevqa}. 
DOVER~\cite{dover} assesses quality in terms of aesthetics and technicality. 
FastVQA~\cite{fastvqa} utilizes grid mini-patch sampling to assess videos efficiently while maintaining accuracy.
Q-Align~\cite{wu2023qalign} transforms the video quality assessment task into the generation of discrete quality level words via the Multimodal Large Language Model.
StableVQA~\cite{stablevideo} measures video stability by separately obtaining the raw optical flow, semantic, and blur features.
These methods are suitable for natural video quality assessment but do not consider the text-video alignment, which is key to the evaluation of text-driven videos.
To address this, EvalCrafter~\cite{evalcrafter} assesses quality through a series of indicators including CLIP score, SD score, and natural video quality assessment methods. 
T2V-QA~\cite{t2vqa} incorporates a transformer-based encoder and Large Language Model to assess text-driven AIGC videos.
TriVQA~\cite{trivqa} explores the video-text consistency through cross-attention pooling and the recaption of Video-LLaVA. 
However, there are still relatively fewer VQA methods specifically for AIGC videos. With the growth of new-generation videos, the requirements for understanding video dynamics and text consistency are becoming increasingly demanding, posing greater challenges.


\subsection{Text-to-Video Generation Method}
With the surge of diffusion models~\cite{stablediffusion,ddpm}, a lot of video generation models emerge~\cite{make-a-video,lavie,modelscope,svd,videocrafter, opensora, opensora-plan}.
They represent a significant breakthrough in video generation. However, videos produced by previous methods still tend to suffer from issues such as low resolution, short duration, flickering, and distortion.
With the advent of Sora~\cite{sora}, the new-generation models~\cite{hunyuan, luma, hailuo,tongyi,pika,cogvideox} have made notable progress. Particularly recently, methods like Kling~\cite{kling}, Gen-3-alpha~\cite{gen3}, and Qingying~\cite{ying} have achieved impressive video generation results and have been made available for community testing.
These videos generally alleviate the foundational problems seen in previous methods, with a duration of more than 5 seconds and frame rates above 24 fps.
Meanwhile, the content in these videos includes a lot of details, and they support the control via longer text inputs.
Under the wave of new-generation video generation models, effectively assessing more complex spatiotemporal relationships within the videos and exploring their consistency with longer texts is a topic worthy of further study.

\subsection{Text-to-Video VQA Dataset}
To evaluate and further promote the development of T2V models, some text-to-video VQA datasets have been proposed. 
Despite this, there are still relatively few text-to-video QA datasets suitable for evaluating current AIGC videos.
EvalCrafter~\cite{evalcrafter} collects 700 prompts and uses 5 models to generate 2500 videos in total.
FETV~\cite{liu2023fetv} utilizes 619 prompts to generate 2,476 videos by 4 T2V models. 
Chivileva~\cite{chivileva2023measuring} derives 1,005 videos generated from 5 T2V models.
VBench ~\cite{huang2023vbench} uses nearly 1,700 prompts and 4 T2V models to generate 6984 videos. 
T2VQA-DB~\cite{kou2024subjectivealigneddatasetmetrictexttovideo} contains 10,000 videos generated by 1000 prompts. 
These datasets mainly meet two challenges:
(1) According to the  ITU-standard~\cite{itu}, the number of human annotators should exceed 15 to keep the assessment error within a controllable range.
Among these, only T2VQA-DB~\cite{kou2024subjectivealigneddatasetmetrictexttovideo} and Chivileva~\cite{chivileva2023measuring} meet the standard with 27 and 24 annotators. 
(2) The gap between previous and the concurrent AIGC videos. Previous videos often involve only easy movements and commonly have basic issues such as flickering, which are relatively rarely seen in the new-generation video models.
In this work, to address the issue that previous VQA datasets do not cover the annotated next-generation AIGC videos, we introduce CRAVE-DB, which includes 1,228 next-generation AIGC videos with subjective scores from 29 annotators, to provide a robust assessment of concurrent AIGC videos.