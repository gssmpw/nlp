@inproceedings{evalcrafter,
  title={Evalcrafter: Benchmarking and evaluating large video generation models},
  author={Liu, Yaofang and Cun, Xiaodong and Liu, Xuebo and Wang, Xintao and Zhang, Yong and Chen, Haoxin and Liu, Yang and Zeng, Tieyong and Chan, Raymond and Shan, Ying},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={22139--22149},
  year={2024}
}

@article{tvge,
  title={Cvpr 2023 text guided video editing competition},
  author={Wu, Jay Zhangjie and Li, Xiuyu and Gao, Difei and Dong, Zhen and Bai, Jinbin and Singh, Aishani and Xiang, Xiaoyu and Li, Youzeng and Huang, Zuwei and Sun, Yuanxi and others},
  journal={arXiv preprint arXiv:2310.16003},
  year={2023}
}

@inproceedings{raft,
  title={Raft: Recurrent all-pairs field transforms for optical flow},
  author={Teed, Zachary and Deng, Jia},
  booktitle={Computer Vision--ECCV 2020: 16th European Conference, Glasgow, UK, August 23--28, 2020, Proceedings, Part II 16},
  pages={402--419},
  year={2020},
  organization={Springer}
}

@article{skflow,
  title={Skflow: Learning optical flow with super kernels},
  author={Sun, Shangkun and Chen, Yuanqi and Zhu, Yu and Guo, Guodong and Li, Ge},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={11313--11326},
  year={2022}
}

@article{iebench,
  title={IE-Bench: Advancing the Measurement of Text-Driven Image Editing for Human Perception Alignment},
  author={Sun, Shangkun and Qu, Bowen and Liang, Xiaoyu and Fan, Songlin and Gao, Wei},
  journal={arXiv preprint arXiv:2501.09927},
  year={2025}
}

@InProceedings{ntire_vqa,
    author    = {Liu, Xiaohong and Min, Xiongkuo and Zhai, Guangtao and Li, Chunyi and Kou, Tengchuan and Sun, Wei and Wu, Haoning and Gao, Yixuan and Cao, Yuqin and Zhang, Zicheng and Wu, Xiele and Timofte, Radu and Peng, Fei and Fu, Huiyuan and Ming, Anlong and Wang, Chuanming and Ma, Huadong and He, Shuai and Dou, Zifei and Chen, Shu and Zhang, Huacong and Xie, Haiyi and Wang, Chengwei and Chen, Baoying and Zeng, Jishen and Yang, Jianquan and Wang, Weigang and Fang, Xi and Lv, Xiaoxin and Yan, Jun and Zhi, Tianwu and Zhang, Yabin and Li, Yaohui and Li, Yang and Xu, Jingwen and Liu, Jianzhao and Liao, Yiting and Li, Junlin and Yu, Zihao and Guan, Fengbin and Lu, Yiting and Li, Xin and Motamednia, Hossein and Hosseini-Benvidi, S. Farhad and Mahmoudi-Aznaveh, Ahmad and Mansouri, Azadeh and Gankhuyag, Ganzorig and Yoon, Kihwan and Xu, Yifang and Fan, Haotian and Kong, Fangyuan and Zhao, Shiling and Dong, Weifeng and Yin, Haibing and Zhu, Li and Wang, Zhiling and Huang, Bingchen and Saha, Avinab and Mishra, Sandeep and Gupta, Shashank and Sureddi, Rajesh and Saha, Oindrila and Celona, Luigi and Bianco, Simone and Napoletano, Paolo and Schettini, Raimondo and Yang, Junfeng and Fu, Jing and Zhang, Wei and Cao, Wenzhi and Liu, Limei and Peng, Han and Yuan, Weijun and Li, Zhan and Cheng, Yihang and Deng, Yifan and Li, Haohui and Qu, Bowen and Li, Yao and Luo, Shuqing and Wang, Shunzhou and Gao, Wei and Lu, Zihao and Conde, Marcos V. and Timofte, Radu and Wang, Xinrui and Chen, Zhibo and Liao, Ruling and Ye, Yan and Wang, Qiulin and Li, Bing and Zhou, Zhaokun and Geng, Miao and Chen, Rui and Tao, Xin and Liang, Xiaoyu and Sun, Shangkun and Ma, Xingyuan and Li, Jiaze and Yang, Mengduo and Xu, Haoran and Zhou, Jie and Zhu, Shiding and Yu, Bohan and Chen, Pengfei and Xu, Xinrui and Shen, Jiabin and Duan, Zhichao and Asadi, Erfan and Liu, Jiahe and Yan, Qi and Qu, Youran and Zeng, Xiaohui and Wang, Lele and Liao, Renjie},
    title     = {NTIRE 2024 Quality Assessment of AI-Generated Content Challenge},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops},
    month     = {June},
    year      = {2024},
    pages     = {6337-6362}
}

@inproceedings{vbench,
  title={Vbench: Comprehensive benchmark suite for video generative models},
  author={Huang, Ziqi and He, Yinan and Yu, Jiashuo and Zhang, Fan and Si, Chenyang and Jiang, Yuming and Zhang, Yuanhan and Wu, Tianxing and Jin, Qingyang and Chanpaisit, Nattapol and others},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={21807--21818},
  year={2024}
}

@article{t2vqa,
  title={Subjective-Aligned Dateset and Metric for Text-to-Video Quality Assessment},
  author={Kou, Tengchuan and Liu, Xiaohong and Zhang, Zicheng and Li, Chunyi and Wu, Haoning and Min, Xiongkuo and Zhai, Guangtao and Liu, Ning},
  journal={arXiv preprint arXiv:2403.11956},
  year={2024}
}

@inproceedings{tune-a-video,
  title={Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation},
  author={Wu, Jay Zhangjie and Ge, Yixiao and Wang, Xintao and Lei, Stan Weixian and Gu, Yuchao and Shi, Yufei and Hsu, Wynne and Shan, Ying and Qie, Xiaohu and Shou, Mike Zheng},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={7623--7633},
  year={2023}
}

@inproceedings{t2v-zero,
  title={Text2video-zero: Text-to-image diffusion models are zero-shot video generators},
  author={Khachatryan, Levon and Movsisyan, Andranik and Tadevosyan, Vahram and Henschel, Roberto and Wang, Zhangyang and Navasardyan, Shant and Shi, Humphrey},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={15954--15964},
  year={2023}
}

@inproceedings{instructpix2pix,
  title={Instructpix2pix: Learning to follow image editing instructions},
  author={Brooks, Tim and Holynski, Aleksander and Efros, Alexei A},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={18392--18402},
  year={2023}
}

@inproceedings{controlnet,
  title={Adding conditional control to text-to-image diffusion models},
  author={Zhang, Lvmin and Rao, Anyi and Agrawala, Maneesh},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={3836--3847},
  year={2023}
}

@article{longclip,
    title={Long-CLIP: Unlocking the Long-Text Capability of CLIP},
    author={Beichen Zhang and Pan Zhang and Xiaoyi Dong and Yuhang Zang and Jiaqi Wang},
    journal={arXiv preprint arXiv:2403.15378},
    year={2024}
}

@inproceedings{controlvideo,
  title={ControlVideo: Training-free Controllable Text-to-video Generation},
  author={Zhang, Yabo and Wei, Yuxiang and Jiang, Dongsheng and ZHANG, XIAOPENG and Zuo, Wangmeng and Tian, Qi},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024}
}

@inproceedings{fatezero,
  title={Fatezero: Fusing attentions for zero-shot text-based video editing},
  author={Qi, Chenyang and Cun, Xiaodong and Zhang, Yong and Lei, Chenyang and Wang, Xintao and Shan, Ying and Chen, Qifeng},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={15932--15942},
  year={2023}
}

@inproceedings{rave,
  title={Rave: Randomized noise shuffling for fast and consistent video editing with diffusion models},
  author={Kara, Ozgur and Kurtkaya, Bariscan and Yesiltepe, Hidir and Rehg, James M and Yanardag, Pinar},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={6507--6516},
  year={2024}
}

@inproceedings{tokenflow,
  title={TokenFlow: Consistent Diffusion Features for Consistent Video Editing},
  author={Geyer, Michal and Bar-Tal, Omer and Bagon, Shai and Dekel, Tali},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024}
}

@inproceedings{flatten,
  title={FLATTEN: optical FLow-guided ATTENtion for consistent text-to-video editing},
  author={Cong, Yuren and Xu, Mengmeng and Chen, Shoufa and Ren, Jiawei and Xie, Yanping and Perez-Rua, Juan-Manuel and Rosenhahn, Bodo and Xiang, Tao and He, Sen and others},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024}
}

@inproceedings{pnp,
  title={Plug-and-play diffusion features for text-driven image-to-image translation},
  author={Tumanyan, Narek and Geyer, Michal and Bagon, Shai and Dekel, Tali},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={1921--1930},
  year={2023}
}

@inproceedings{fresco,
  title={FRESCO: Spatial-Temporal Correspondence for Zero-Shot Video Translation},
  author={Yang, Shuai and Zhou, Yifan and Liu, Ziwei and Loy, Chen Change},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={8703--8712},
  year={2024}
}




@inproceedings{pix2video,
  title={Pix2video: Video editing using image diffusion},
  author={Ceylan, Duygu and Huang, Chun-Hao P and Mitra, Niloy J},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={23206--23217},
  year={2023}
}

@article{pickscore,
  title={Pick-a-pic: An open dataset of user preferences for text-to-image generation},
  author={Kirstain, Yuval and Polyak, Adam and Singer, Uriel and Matiana, Shahbuland and Penna, Joe and Levy, Omer},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={36652--36663},
  year={2023}
}

@article{fvd,
  title={Towards accurate generative models of video: A new metric \& challenges},
  author={Unterthiner, Thomas and Van Steenkiste, Sjoerd and Kurach, Karol and Marinier, Raphael and Michalski, Marcin and Gelly, Sylvain},
  journal={arXiv preprint arXiv:1812.01717},
  year={2018}
}

@inproceedings{rerender-a-video,
    title = {Rerender A Video: Zero-Shot Text-Guided Video-to-Video Translation},
    author = {Yang, Shuai and Zhou, Yifan and Liu, Ziwei and and Loy, Chen Change},
    booktitle = {ACM SIGGRAPH Asia Conference Proceedings},
    year = {2023},
}

@inproceedings{lpips,
  title={The unreasonable effectiveness of deep features as a perceptual metric},
  author={Zhang, Richard and Isola, Phillip and Efros, Alexei A and Shechtman, Eli and Wang, Oliver},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={586--595},
  year={2018}
}

@inproceedings{stablevqa,
  title={Stablevqa: A deep no-reference quality assessment model for video stability},
  author={Kou, Tengchuan and Liu, Xiaohong and Sun, Wei and Jia, Jun and Min, Xiongkuo and Zhai, Guangtao and Liu, Ning},
  booktitle={Proceedings of the 31st ACM International Conference on Multimedia},
  pages={1066--1076},
  year={2023}
}

@inproceedings{fastvqa,
  title={Fast-vqa: Efficient end-to-end video quality assessment with fragment sampling},
  author={Wu, Haoning and Chen, Chaofeng and Hou, Jingwen and Liao, Liang and Wang, Annan and Sun, Wenxiu and Yan, Qiong and Lin, Weisi},
  booktitle={European conference on computer vision},
  pages={538--554},
  year={2022},
  organization={Springer}
}

@inproceedings{dover,
  title={Exploring video quality assessment on user generated contents from aesthetic and technical perspectives},
  author={Wu, Haoning and Zhang, Erli and Liao, Liang and Chen, Chaofeng and Hou, Jingwen and Wang, Annan and Sun, Wenxiu and Yan, Qiong and Lin, Weisi},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={20144--20154},
  year={2023}
}

@inproceedings{simplevqa,
  title={A deep learning based no-reference quality assessment model for ugc videos},
  author={Sun, Wei and Min, Xiongkuo and Lu, Wei and Zhai, Guangtao},
  booktitle={Proceedings of the 30th ACM International Conference on Multimedia},
  pages={856--865},
  year={2022}
}

@inproceedings{clip,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={International conference on machine learning},
  pages={8748--8763},
  year={2021},
  organization={PMLR}
}

@article{trivqa,
  title={Exploring aigc video quality: A focus on visual harmony, video-text consistency and domain distribution gap},
  author={Qu, Bowen and Liang, Xiaoyu and Sun, Shangkun and Gao, Wei},
  journal={arXiv preprint arXiv:2404.13573},
  year={2024}
}

@article{emu,
  title={Video Editing via Factorized Diffusion Distillation},
  author={Singer, Uriel and Zohar, Amit and Kirstain, Yuval and Sheynin, Shelly and Polyak, Adam and Parikh, Devi and Taigman, Yaniv},
  journal={arXiv preprint arXiv:2403.09334},
  year={2024}
}

@article{neu,
  title={Neutral Editing Framework for Diffusion-based Video Editing},
  author={Yoon, Sunjae and Koo, Gwanhyeong and Hong, Ji Woo and Yoo, Chang D},
  journal={arXiv preprint arXiv:2312.06708},
  year={2023}
}

@inproceedings{stablevideo,
  title={Stablevideo: Text-driven consistency-aware diffusion video editing},
  author={Chai, Wenhao and Guo, Xun and Wang, Gaoang and Lu, Yan},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={23040--23050},
  year={2023}
}

@inproceedings{videop2p,
  title={Video-p2p: Video editing with cross-attention control},
  author={Liu, Shaoteng and Zhang, Yuechen and Li, Wenbo and Lin, Zhe and Jia, Jiaya},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={8599--8608},
  year={2024}
}
@InProceedings{Lu_2024_CVPR,
    author    = {Lu, Yiting and Li, Xin and Li, Bingchen and Yu, Zihao and Guan, Fengbin and Wang, Xinrui and Liao, Ruling and Ye, Yan and Chen, Zhibo},
    title     = {AIGC-VQA: A Holistic Perception Metric for AIGC Video Quality Assessment},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops},
    month     = {June},
    year      = {2024},
    pages     = {6384-6394}
}
@inproceedings{li2022blip,
      title={BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation}, 
      author={Junnan Li and Dongxu Li and Caiming Xiong and Steven Hoi},
      year={2022},
      booktitle={ICML},
}
@InProceedings{Text2Video-Zero,
    author    = {Khachatryan, Levon and Movsisyan, Andranik and Tadevosyan, Vahram and Henschel, Roberto and Wang, Zhangyang and Navasardyan, Shant and Shi, Humphrey},
    title     = {Text2Video-Zero: Text-to-Image Diffusion Models are Zero-Shot Video Generators},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    month     = {October},
    year      = {2023},
    pages     = {15954-15964}
}
@article{wang2023modelscope,
  title={Modelscope text-to-video technical report},
  author={Wang, Jiuniu and Yuan, Hangjie and Chen, Dayou and Zhang, Yingya and Wang, Xiang and Zhang, Shiwei},
  journal={arXiv preprint arXiv:2308.06571},
  year={2023}
}
@Misc{zeroscope,
    author = {S. Sterling},
    title = {zeroscope-v2},
    year={2023}
}

@InProceedings{Esser_2023_ICCV,
    author    = {Esser, Patrick and Chiu, Johnathan and Atighehchian, Parmida and Granskog, Jonathan and Germanidis, Anastasis},
    title     = {Structure and Content-Guided Video Synthesis with Diffusion Models},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    month     = {October},
    year      = {2023},
    pages     = {7346-7356}
}

@article{modelscope,
  title={Modelscope text-to-video technical report},
  author={Wang, Jiuniu and Yuan, Hangjie and Chen, Dayou and Zhang, Yingya and Wang, Xiang and Zhang, Shiwei},
  journal={arXiv preprint arXiv:2308.06571},
  year={2023}
}

@InProceedings{vebench,
  title={VE-bench: Subjective-aligned benchmark suite for text-driven video editing quality assessment},
  author={Sun, Shangkun and Liang, Xiaoyu and Fan, Songlin and Gao, Wenxu and Gao, Wei},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  year={2024}
}

@InProceedings{huang2023vbench,
     title={VBench: Comprehensive Benchmark Suite for Video Generative Models},
     author={Huang, Ziqi and He, Yinan and Yu, Jiashuo and Zhang, Fan and Si, Chenyang and Jiang, Yuming and Zhang, Yuanhan and Wu, Tianxing and Jin, Qingyang and Chanpaisit, Nattapol and Wang, Yaohui and Chen, Xinyuan and Wang, Limin and Lin, Dahua and Qiao, Yu and Liu, Ziwei},
     booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
     year={2024}
 }
@article{wang2023lavie,
  title={LAVIE: High-Quality Video Generation with Cascaded Latent Diffusion Models},
  author={Wang, Yaohui and Chen, Xinyuan and Ma, Xin and Zhou, Shangchen and Huang, Ziqi and Wang, Yi and Yang, Ceyuan and He, Yinan and Yu, Jiashuo and Yang, Peiqing and others},
  journal={arXiv preprint arXiv:2309.15103},
  year={2023}
}
@techreport{sora,
    author = {Brooks, Tim and Peebles, Bill and Holmes, Connor and DePue, Will and Guo, Yufei and Jing, Li and Schnurr, David and Taylor, Joe and Luhman, Troy and Luhman, Eric and Ng, Clarence and Wang, Ricky and Ramesh, Aditya},
    title = {Video generation models as world simulators},
    institution = {OpenAI},
    year = {2024}
}
@misc{kou2024subjectivealigneddatasetmetrictexttovideo,
      title={Subjective-Aligned Dataset and Metric for Text-to-Video Quality Assessment}, 
      author={Tengchuan Kou and Xiaohong Liu and Zicheng Zhang and Chunyi Li and Haoning Wu and Xiongkuo Min and Guangtao Zhai and Ning Liu},
      year={2024},
      eprint={2403.11956},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2403.11956}, 
}

@article{bt500,
  title={Methodology for the Subjective Assessment of
the Quality of Television Pictures ITU-R Recommendation},
  author={Int.Telecommun.Union},
  journal={Tech. Rep.},
  year={2000},
}

@misc{rankloss,
      title={Learning to Rank for Blind Image Quality Assessment}, 
      author={Fei Gao and Dacheng Tao and Xinbo Gao and Xuelong Li},
      year={2019},
      eprint={1309.0213},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@software{opensora-plan,
  author       = {PKU-Yuan Lab and Tuzhan AI etc.},
  title        = {Open-Sora-Plan},
  month        = apr,
  year         = 2024,
  publisher    = {GitHub},
  doi          = {10.5281/zenodo.10948109},
  url          = {https://doi.org/10.5281/zenodo.10948109}
}


@software{opensora,
  author = {Zangwei Zheng and Xiangyu Peng and Tianji Yang and Chenhui Shen and Shenggui Li and Hongxin Liu and Yukun Zhou and Tianyi Li and Yang You},
  title = {Open-Sora: Democratizing Efficient Video Production for All},
  month = {March},
  year = {2024},
  url = {https://github.com/hpcaitech/Open-Sora}
}

@article{zhu2023deep,
  title={Deep learning for video-text retrieval: a review},
  author={Zhu, Cunjuan and Jia, Qi and Chen, Wei and Guo, Yanming and Liu, Yu},
  journal={International Journal of Multimedia Information Retrieval},
  volume={12},
  number={1},
  pages={3},
  year={2023},
  publisher={Springer}
}

@InProceedings{Wu_2024_CVPR,
    author    = {Wu, Haoning and Zhang, Zicheng and Zhang, Erli and Chen, Chaofeng and Liao, Liang and Wang, Annan and Xu, Kaixin and Li, Chunyi and Hou, Jingwen and Zhai, Guangtao and Xue, Geng and Sun, Wenxiu and Yan, Qiong and Lin, Weisi},
    title     = {Q-Instruct: Improving Low-level Visual Abilities for Multi-modality Foundation Models},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2024},
    pages     = {25490-25500}
}
@misc{wu2024openendedvisualqualitycomparison,
      title={Towards Open-ended Visual Quality Comparison}, 
      author={Haoning Wu and Hanwei Zhu and Zicheng Zhang and Erli Zhang and Chaofeng Chen and Liang Liao and Chunyi Li and Annan Wang and Wenxiu Sun and Qiong Yan and Xiaohong Liu and Guangtao Zhai and Shiqi Wang and Weisi Lin},
      year={2024},
      eprint={2402.16641},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2402.16641}, 
}
@article{Villegas2022PhenakiVL,
    title   = {Phenaki: Variable Length Video Generation From Open Domain Textual Description},
    author  = {Ruben Villegas and Mohammad Babaeizadeh and Pieter-Jan Kindermans and Hernan Moraldo and Han Zhang and Mohammad Taghi Saffar and Santiago Castro and Julius Kunze and D. Erhan},
    journal = {ArXiv},
    year    = {2022},
    volume  = {abs/2210.02399}
}
@inproceedings{NUWA,
author = {Wu, Chenfei and Liang, Jian and Ji, Lei and Yang, Fan and Fang, Yuejian and Jiang, Daxin and Duan, Nan},
title = {N\"{U}WA: Visual Synthesis Pre-training for&nbsp;Neural visUal World creAtion},
year = {2022},
isbn = {978-3-031-19786-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-19787-1_41},
doi = {10.1007/978-3-031-19787-1_41},
booktitle = {Computer Vision – ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part XVI},
pages = {720–736},
numpages = {17},
location = {Tel Aviv, Israel}
}
@article{liu2023fetv,
  title   = {FETV: A Benchmark for Fine-Grained Evaluation of Open-Domain Text-to-Video Generation},
  author  = {Yuanxin Liu and Lei Li and Shuhuai Ren and Rundong Gao and Shicheng Li and Sishuo Chen and Xu Sun and Lu Hou},
  year    = {2023},
  journal = {arXiv preprint arXiv: 2311.01813}
}

@article{is,
  title={Improved techniques for training gans},
  author={Salimans, Tim and Goodfellow, Ian and Zaremba, Wojciech and Cheung, Vicki and Radford, Alec and Chen, Xi},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}

@inproceedings{make-a-video,
  title={Make-A-Video: Text-to-Video Generation without Text-Video Data},
  author={Singer, Uriel and Polyak, Adam and Hayes, Thomas and Yin, Xi and An, Jie and Zhang, Songyang and Hu, Qiyuan and Yang, Harry and Ashual, Oron and Gafni, Oran and others},
  year={2023},
  booktitle={The Eleventh International Conference on Learning Representations}
}

@article{lavie,
  title={Lavie: High-quality video generation with cascaded latent diffusion models},
  author={Wang, Yaohui and Chen, Xinyuan and Ma, Xin and Zhou, Shangchen and Huang, Ziqi and Wang, Yi and Yang, Ceyuan and He, Yinan and Yu, Jiashuo and Yang, Peiqing and others},
  journal={arXiv preprint arXiv:2309.15103},
  year={2023}
}

@article{videocrafter,
  title={Videocrafter1: Open diffusion models for high-quality video generation},
  author={Chen, Haoxin and Xia, Menghan and He, Yingqing and Zhang, Yong and Cun, Xiaodong and Yang, Shaoshu and Xing, Jinbo and Liu, Yaofang and Chen, Qifeng and Wang, Xintao and others},
  journal={arXiv preprint arXiv:2310.19512},
  year={2023}
}



@article{svd,
  title={Stable video diffusion: Scaling latent video diffusion models to large datasets},
  author={Blattmann, Andreas and Dockhorn, Tim and Kulal, Sumith and Mendelevitch, Daniel and Kilian, Maciej and Lorenz, Dominik and Levi, Yam and English, Zion and Voleti, Vikram and Letts, Adam and others},
  journal={arXiv preprint arXiv:2311.15127},
  year={2023}
}


@ARTICLE{chipQA,
  author={Ebenezer, Joshua Peter and Shang, Zaixi and Wu, Yongjun and Wei, Hai and Sethuraman, Sriram and Bovik, Alan C.},
  journal={IEEE Transactions on Image Processing}, 
  title={ChipQA: No-Reference Video Quality Prediction via Space-Time Chips}, 
  year={2021},
  volume={30},
  number={},
  pages={8059-8074},
  keywords={Quality assessment;Video recording;Prediction algorithms;Optical distortion;Visualization;Distortion;Databases;Video quality assessment;natural video statistics;human visual system},
  doi={10.1109/TIP.2021.3112055}}



@article{wu2023qalign,
  title={Q-Align: Teaching LMMs for Visual Scoring via Discrete Text-Defined Levels},
  author={Wu, Haoning and Zhang, Zicheng and Zhang, Weixia and Chen, Chaofeng and Li, Chunyi and Liao, Liang and Wang, Annan and Zhang, Erli and Sun, Wenxiu and Yan, Qiong and Min, Xiongkuo and Zhai, Guangtai and Lin, Weisi},
  journal={arXiv preprint arXiv:2312.17090},
  year={2023},
  institution={Nanyang Technological University and Shanghai Jiao Tong University and Sensetime Research},
}

@misc{xu2023imagereward,
      title={ImageReward: Learning and Evaluating Human Preferences for Text-to-Image Generation},
      author={Jiazheng Xu and Xiao Liu and Yuchen Wu and Yuxuan Tong and Qinkai Li and Ming Ding and Jie Tang and Yuxiao Dong},
      year={2023},
      eprint={2304.05977},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{mochi,
      title={Mochi 1},
      author={Genmo Team},
      year={2024},
      publisher = {GitHub},
      journal = {GitHub repository},
      howpublished={\url{https://github.com/genmoai/models}}
}

@article{viclip,
  title={InternVid: A Large-scale Video-Text Dataset for Multimodal Understanding and Generation},
  author={Yi Wang and Yinan He and Yizhuo Li and Kunchang Li and Jiashuo Yu and Xin Jian Ma and Xinyuan Chen and Yaohui Wang and Ping Luo and Ziwei Liu and Yali Wang and Limin Wang and Y. Qiao},
  journal={ArXiv},
  year={2023},
  volume={abs/2307.06942},
  url={https://api.semanticscholar.org/CorpusID:259847783}
}

@article{itu,
  title={Methodology for the subjective assessment of the quality of television pictures},
  author={Series, B},
  journal={Recommendation ITU-R BT},
  volume={500},
  number={13},
  year={2012},
  publisher={International Telecommunication Union Switzerland, Geneva}
}

@inproceedings{cogvideo,
  title={CogVideo: Large-scale Pretraining for Text-to-Video Generation via Transformers},
  author={Hong, Wenyi and Ding, Ming and Zheng, Wendi and Liu, Xinghan and Tang, Jie},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2023}
}

@article{chivileva2023measuring,
  title={Measuring the quality of text-to-video model outputs: Metrics and dataset},
  author={Chivileva, Iya and Lynch, Philip and Ward, Tomas E and Smeaton, Alan F},
  journal={arXiv preprint arXiv:2309.08009},
  year={2023}
}

@article{prompt,
  title={Diffusion model-based image editing: A survey},
  author={Huang, Yi and Huang, Jiancheng and Liu, Yifan and Yan, Mingfu and Lv, Jiaxi and Liu, Jianzhuang and Xiong, Wei and Zhang, He and Chen, Shifeng and Cao, Liangliang},
  journal={arXiv preprint arXiv:2402.17525},
  year={2024}
}

@misc{pixeldance,
  author={ByteDance},
  title = {PixelDance Pro},
  howpublished = "\url{https://jimeng.jianying.com/}",
  year = {2024}, 
}

@article{caldelli2021optical,
  title={Optical Flow based CNN for detection of unlearnt deepfake manipulations},
  author={Caldelli, Roberto and Galteri, Leonardo and Amerini, Irene and Del Bimbo, Alberto},
  journal={Pattern Recognition Letters},
  volume={146},
  pages={31--37},
  year={2021},
  publisher={Elsevier}
}

@article{spacy,
author = {Honnibal, Matthew and Montani, Ines and Van Landeghem, Sofie and Boyd, Adriane},
doi = {10.5281/zenodo.1212303},
title = {spaCy: Industrial-strength Natural Language Processing in Python},
year = {2020}
}

@misc{hunyuan,
      title={HunyuanVideo: A Systematic Framework For Large Video Generative Models}, 
      author={Tencent Hunyuan},
      year={2024},
      archivePrefix={arXiv preprint arXiv:2412.03603},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2412.03603}, 
}

@inproceedings{somethingsomething,
  title={The" something something" video database for learning and evaluating visual common sense},
  author={Goyal, Raghav and Ebrahimi Kahou, Samira and Michalski, Vincent and Materzynska, Joanna and Westphal, Susanne and Kim, Heuna and Haenel, Valentin and Fruend, Ingo and Yianilos, Peter and Mueller-Freitag, Moritz and others},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={5842--5850},
  year={2017}
}

@article{sharegpt,
    title={InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks},
    author={Chen, Zhe and Wu, Jiannan and Wang, Wenhai and Su, Weijie and Chen, Guo and Xing, Sen and Zhong, Muyan and Zhang, Qinglong and Zhu, Xizhou and Lu, Lewei and Li, Bin and Luo, Ping and Lu, Tong and Qiao, Yu and Dai, Jifeng},
    journal={arXiv preprint arXiv:2312.14238},
    year={2023}
}

@article{uniformer,
  title={Uniformer: Unifying convolution and self-attention for visual recognition},
  author={Li, Kunchang and Wang, Yali and Zhang, Junhao and Gao, Peng and Song, Guanglu and Liu, Yu and Li, Hongsheng and Qiao, Yu},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume={45},
  number={10},
  pages={12581--12600},
  year={2023},
  publisher={IEEE}
}

@inproceedings{stablediffusion,
  title={High-resolution image synthesis with latent diffusion models},
  author={Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj{\"o}rn},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={10684--10695},
  year={2022}
}

@article{ddpm,
  title={Denoising diffusion probabilistic models},
  author={Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={6840--6851},
  year={2020}
}

@inproceedings{agarwal2020detecting,
  title={Detecting deep-fake videos from appearance and behavior},
  author={Agarwal, Shruti and Farid, Hany and El-Gaaly, Tarek and Lim, Ser-Nam},
  booktitle={2020 IEEE international workshop on information forensics and security (WIFS)},
  pages={1--6},
  year={2020},
  organization={IEEE}
}

@article{kinetics,
  title={The kinetics human action video dataset},
  author={Kay, Will and Carreira, Joao and Simonyan, Karen and Zhang, Brian and Hillier, Chloe and Vijayanarasimhan, Sudheendra and Viola, Fabio and Green, Tim and Back, Trevor and Natsev, Paul and others},
  journal={arXiv preprint arXiv:1705.06950},
  year={2017}
}



@inproceedings{streamflow,
  title={StreamFlow: Streamlined Multi-Frame Optical Flow Estimation for Video Sequences},
  author={Sun, Shangkun and Liu, Jiaming and Li, Thomas H and Li, Huaxia and Liu, Guoqing and Gao, Wei},
  booktitle={Advances in neural information processing systems},
  year={2024}
}

@article{t2vcompbench,
  title={T2V-CompBench: A Comprehensive Benchmark for Compositional Text-to-video Generation},
  author={Sun, Kaiyue and Huang, Kaiyi and Liu, Xian and Wu, Yue and Xu, Zihan and Li, Zhenguo and Liu, Xihui},
  journal={arXiv preprint arXiv:2407.14505},
  year={2024}
}

@article{bvqa,
  title={Blindly assess quality of in-the-wild videos via quality-aware pre-training and motion perception},
  author={Li, Bowen and Zhang, Weixia and Tian, Meng and Zhai, Guangtao and Wang, Xianpei},
  journal={IEEE Transactions on Circuits and Systems for Video Technology},
  volume={32},
  number={9},
  pages={5944--5958},
  year={2022},
  publisher={IEEE}
}


@article{easyanimate,
  title={EasyAnimate: A High-Performance Long Video Generation Method based on Transformer Architecture},
  author={Xu, Jiaqi and Zou, Xinyi and Huang, Kunzhe and Chen, Yunkuo and Liu, Bo and Cheng, MengLi and Shi, Xing and Huang, Jun},
  journal={arXiv preprint arXiv:2405.18991},
  year={2024}
}

@article{cogvideox,
  title={Cogvideox: Text-to-video diffusion models with an expert transformer},
  author={Yang, Zhuoyi and Teng, Jiayan and Zheng, Wendi and Ding, Ming and Huang, Shiyu and Xu, Jiazheng and Yang, Yuanming and Hong, Wenyi and Zhang, Xiaohan and Feng, Guanyu and others},
  journal={arXiv preprint arXiv:2408.06072},
  year={2024}
}

@inproceedings{patchvq,
  title={Patch-vq:'patching up'the video quality problem},
  author={Ying, Zhenqiang and Mandal, Maniratnam and Ghadiyaram, Deepti and Bovik, Alan},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={14019--14029},
  year={2021}
}

@inproceedings{hps,
  title={Human preference score: Better aligning text-to-image models with human preference},
  author={Wu, Xiaoshi and Sun, Keqiang and Zhu, Feng and Zhao, Rui and Li, Hongsheng},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={2096--2105},
  year={2023}
}

@article{hpsv2,
  title={Human preference score v2: A solid benchmark for evaluating human preferences of text-to-image synthesis},
  author={Wu, Xiaoshi and Hao, Yiming and Sun, Keqiang and Chen, Yixiong and Zhu, Feng and Zhao, Rui and Li, Hongsheng},
  journal={arXiv preprint arXiv:2306.09341},
  year={2023}
}

@article{vgeneval,
  title={The Dawn of Video Generation: Preliminary Explorations with SORA-like Models},
  author={Zeng, Ailing and Yang, Yuhang and Chen, Weidong and Liu, Wei},
  journal={arXiv preprint arXiv:2410.05227},
  year={2024}
}



@misc{kling,
  author={Kuaishou},
  title = {Kling},
  howpublished = "\url{https://kling.kuaishou.com/}",
  year = {2024}, 
}

@misc{gen3,
  author={Runway},
  title = {Gen-3},
  howpublished = "\url{https://runwayml.com/blog/introducing-gen-3-alpha/}",
  year = {2024}, 
}

@misc{vidu,
  author={Shengshu},
  title = {Vidu},
  howpublished = "\url{https://www.vidu.studio/create}",
  year = {2024}, 
}

@misc{ying,
  author={Zhipu},
  title = {Ying},
  howpublished = "\url{https://chatglm.cn/video}",
  year = {2024}, 
}



@article{gpt4,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@misc{pika,
  author={Pika Labs},
  title = {Pika 1.5},
  howpublished = "\url{https://pika.art}",
  year = {2024}, 
}

@misc{tongyi,
  author={Ali Tongyi},
  title = {Wanxiang Video},
  howpublished = "\url{https://tongyi.aliyun.com/wanxiang/videoCreation}",
  year = {2024}, 
}

@misc{hailuo,
  author={MiniMax},
  title = {Hailuo AI},
  howpublished = "\url{https://hailuoai.com/video}",
  year = {2024}, 
}

@misc{seaweed,
  author={ByteDance},
  title = {Seaweed Pro},
  howpublished = "\url{https://jimeng.jianying.com/}",
  year = {2024}, 
}

@misc{luma,
  author={LumaLabs},
  title = {Dream Machine},
  howpublished = "\url{https://lumalabs.ai/dream-machine}",
  year = {2024}, 
}




