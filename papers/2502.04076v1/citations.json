[
  {
    "index": 0,
    "papers": [
      {
        "key": "clip",
        "author": "Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others",
        "title": "Learning transferable visual models from natural language supervision"
      },
      {
        "key": "fvd",
        "author": "Unterthiner, Thomas and Van Steenkiste, Sjoerd and Kurach, Karol and Marinier, Raphael and Michalski, Marcin and Gelly, Sylvain",
        "title": "Towards accurate generative models of video: A new metric \\& challenges"
      },
      {
        "key": "is",
        "author": "Salimans, Tim and Goodfellow, Ian and Zaremba, Wojciech and Cheung, Vicki and Radford, Alec and Chen, Xi",
        "title": "Improved techniques for training gans"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "pickscore",
        "author": "Kirstain, Yuval and Polyak, Adam and Singer, Uriel and Matiana, Shahbuland and Penna, Joe and Levy, Omer",
        "title": "Pick-a-pic: An open dataset of user preferences for text-to-image generation"
      },
      {
        "key": "trivqa",
        "author": "Qu, Bowen and Liang, Xiaoyu and Sun, Shangkun and Gao, Wei",
        "title": "Exploring aigc video quality: A focus on visual harmony, video-text consistency and domain distribution gap"
      },
      {
        "key": "t2vqa",
        "author": "Kou, Tengchuan and Liu, Xiaohong and Zhang, Zicheng and Li, Chunyi and Wu, Haoning and Min, Xiongkuo and Zhai, Guangtao and Liu, Ning",
        "title": "Subjective-Aligned Dateset and Metric for Text-to-Video Quality Assessment"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "clip",
        "author": "Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others",
        "title": "Learning transferable visual models from natural language supervision"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "is",
        "author": "Salimans, Tim and Goodfellow, Ian and Zaremba, Wojciech and Cheung, Vicki and Radford, Alec and Chen, Xi",
        "title": "Improved techniques for training gans"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "vbench",
        "author": "Huang, Ziqi and He, Yinan and Yu, Jiashuo and Zhang, Fan and Si, Chenyang and Jiang, Yuming and Zhang, Yuanhan and Wu, Tianxing and Jin, Qingyang and Chanpaisit, Nattapol and others",
        "title": "Vbench: Comprehensive benchmark suite for video generative models"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "raft",
        "author": "Teed, Zachary and Deng, Jia",
        "title": "Raft: Recurrent all-pairs field transforms for optical flow"
      },
      {
        "key": "skflow",
        "author": "Sun, Shangkun and Chen, Yuanqi and Zhu, Yu and Guo, Guodong and Li, Ge",
        "title": "Skflow: Learning optical flow with super kernels"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "dover",
        "author": "Wu, Haoning and Zhang, Erli and Liao, Liang and Chen, Chaofeng and Hou, Jingwen and Wang, Annan and Sun, Wenxiu and Yan, Qiong and Lin, Weisi",
        "title": "Exploring video quality assessment on user generated contents from aesthetic and technical perspectives"
      },
      {
        "key": "fastvqa",
        "author": "Wu, Haoning and Chen, Chaofeng and Hou, Jingwen and Liao, Liang and Wang, Annan and Sun, Wenxiu and Yan, Qiong and Lin, Weisi",
        "title": "Fast-vqa: Efficient end-to-end video quality assessment with fragment sampling"
      },
      {
        "key": "stablevqa",
        "author": "Kou, Tengchuan and Liu, Xiaohong and Sun, Wei and Jia, Jun and Min, Xiongkuo and Zhai, Guangtao and Liu, Ning",
        "title": "Stablevqa: A deep no-reference quality assessment model for video stability"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "dover",
        "author": "Wu, Haoning and Zhang, Erli and Liao, Liang and Chen, Chaofeng and Hou, Jingwen and Wang, Annan and Sun, Wenxiu and Yan, Qiong and Lin, Weisi",
        "title": "Exploring video quality assessment on user generated contents from aesthetic and technical perspectives"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "fastvqa",
        "author": "Wu, Haoning and Chen, Chaofeng and Hou, Jingwen and Liao, Liang and Wang, Annan and Sun, Wenxiu and Yan, Qiong and Lin, Weisi",
        "title": "Fast-vqa: Efficient end-to-end video quality assessment with fragment sampling"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "wu2023qalign",
        "author": "Wu, Haoning and Zhang, Zicheng and Zhang, Weixia and Chen, Chaofeng and Li, Chunyi and Liao, Liang and Wang, Annan and Zhang, Erli and Sun, Wenxiu and Yan, Qiong and Min, Xiongkuo and Zhai, Guangtai and Lin, Weisi",
        "title": "Q-Align: Teaching LMMs for Visual Scoring via Discrete Text-Defined Levels"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "stablevideo",
        "author": "Chai, Wenhao and Guo, Xun and Wang, Gaoang and Lu, Yan",
        "title": "Stablevideo: Text-driven consistency-aware diffusion video editing"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "evalcrafter",
        "author": "Liu, Yaofang and Cun, Xiaodong and Liu, Xuebo and Wang, Xintao and Zhang, Yong and Chen, Haoxin and Liu, Yang and Zeng, Tieyong and Chan, Raymond and Shan, Ying",
        "title": "Evalcrafter: Benchmarking and evaluating large video generation models"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "t2vqa",
        "author": "Kou, Tengchuan and Liu, Xiaohong and Zhang, Zicheng and Li, Chunyi and Wu, Haoning and Min, Xiongkuo and Zhai, Guangtao and Liu, Ning",
        "title": "Subjective-Aligned Dateset and Metric for Text-to-Video Quality Assessment"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "trivqa",
        "author": "Qu, Bowen and Liang, Xiaoyu and Sun, Shangkun and Gao, Wei",
        "title": "Exploring aigc video quality: A focus on visual harmony, video-text consistency and domain distribution gap"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "stablediffusion",
        "author": "Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj{\\\"o}rn",
        "title": "High-resolution image synthesis with latent diffusion models"
      },
      {
        "key": "ddpm",
        "author": "Ho, Jonathan and Jain, Ajay and Abbeel, Pieter",
        "title": "Denoising diffusion probabilistic models"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "make-a-video",
        "author": "Singer, Uriel and Polyak, Adam and Hayes, Thomas and Yin, Xi and An, Jie and Zhang, Songyang and Hu, Qiyuan and Yang, Harry and Ashual, Oron and Gafni, Oran and others",
        "title": "Make-A-Video: Text-to-Video Generation without Text-Video Data"
      },
      {
        "key": "lavie",
        "author": "Wang, Yaohui and Chen, Xinyuan and Ma, Xin and Zhou, Shangchen and Huang, Ziqi and Wang, Yi and Yang, Ceyuan and He, Yinan and Yu, Jiashuo and Yang, Peiqing and others",
        "title": "Lavie: High-quality video generation with cascaded latent diffusion models"
      },
      {
        "key": "modelscope",
        "author": "Wang, Jiuniu and Yuan, Hangjie and Chen, Dayou and Zhang, Yingya and Wang, Xiang and Zhang, Shiwei",
        "title": "Modelscope text-to-video technical report"
      },
      {
        "key": "svd",
        "author": "Blattmann, Andreas and Dockhorn, Tim and Kulal, Sumith and Mendelevitch, Daniel and Kilian, Maciej and Lorenz, Dominik and Levi, Yam and English, Zion and Voleti, Vikram and Letts, Adam and others",
        "title": "Stable video diffusion: Scaling latent video diffusion models to large datasets"
      },
      {
        "key": "videocrafter",
        "author": "Chen, Haoxin and Xia, Menghan and He, Yingqing and Zhang, Yong and Cun, Xiaodong and Yang, Shaoshu and Xing, Jinbo and Liu, Yaofang and Chen, Qifeng and Wang, Xintao and others",
        "title": "Videocrafter1: Open diffusion models for high-quality video generation"
      },
      {
        "key": "opensora",
        "author": "Zangwei Zheng and Xiangyu Peng and Tianji Yang and Chenhui Shen and Shenggui Li and Hongxin Liu and Yukun Zhou and Tianyi Li and Yang You",
        "title": "Open-Sora: Democratizing Efficient Video Production for All"
      },
      {
        "key": "opensora-plan",
        "author": "PKU-Yuan Lab and Tuzhan AI etc.",
        "title": "Open-Sora-Plan"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "sora",
        "author": "Brooks, Tim and Peebles, Bill and Holmes, Connor and DePue, Will and Guo, Yufei and Jing, Li and Schnurr, David and Taylor, Joe and Luhman, Troy and Luhman, Eric and Ng, Clarence and Wang, Ricky and Ramesh, Aditya",
        "title": "Video generation models as world simulators"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "hunyuan",
        "author": "Tencent Hunyuan",
        "title": "HunyuanVideo: A Systematic Framework For Large Video Generative Models"
      },
      {
        "key": "luma",
        "author": "LumaLabs",
        "title": "Dream Machine"
      },
      {
        "key": "hailuo",
        "author": "MiniMax",
        "title": "Hailuo AI"
      },
      {
        "key": "tongyi",
        "author": "Ali Tongyi",
        "title": "Wanxiang Video"
      },
      {
        "key": "pika",
        "author": "Pika Labs",
        "title": "Pika 1.5"
      },
      {
        "key": "cogvideox",
        "author": "Yang, Zhuoyi and Teng, Jiayan and Zheng, Wendi and Ding, Ming and Huang, Shiyu and Xu, Jiazheng and Yang, Yuanming and Hong, Wenyi and Zhang, Xiaohan and Feng, Guanyu and others",
        "title": "Cogvideox: Text-to-video diffusion models with an expert transformer"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "kling",
        "author": "Kuaishou",
        "title": "Kling"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "gen3",
        "author": "Runway",
        "title": "Gen-3"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "ying",
        "author": "Zhipu",
        "title": "Ying"
      }
    ]
  },
  {
    "index": 21,
    "papers": [
      {
        "key": "evalcrafter",
        "author": "Liu, Yaofang and Cun, Xiaodong and Liu, Xuebo and Wang, Xintao and Zhang, Yong and Chen, Haoxin and Liu, Yang and Zeng, Tieyong and Chan, Raymond and Shan, Ying",
        "title": "Evalcrafter: Benchmarking and evaluating large video generation models"
      }
    ]
  },
  {
    "index": 22,
    "papers": [
      {
        "key": "liu2023fetv",
        "author": "Yuanxin Liu and Lei Li and Shuhuai Ren and Rundong Gao and Shicheng Li and Sishuo Chen and Xu Sun and Lu Hou",
        "title": "FETV: A Benchmark for Fine-Grained Evaluation of Open-Domain Text-to-Video Generation"
      }
    ]
  },
  {
    "index": 23,
    "papers": [
      {
        "key": "chivileva2023measuring",
        "author": "Chivileva, Iya and Lynch, Philip and Ward, Tomas E and Smeaton, Alan F",
        "title": "Measuring the quality of text-to-video model outputs: Metrics and dataset"
      }
    ]
  },
  {
    "index": 24,
    "papers": [
      {
        "key": "huang2023vbench",
        "author": "Huang, Ziqi and He, Yinan and Yu, Jiashuo and Zhang, Fan and Si, Chenyang and Jiang, Yuming and Zhang, Yuanhan and Wu, Tianxing and Jin, Qingyang and Chanpaisit, Nattapol and Wang, Yaohui and Chen, Xinyuan and Wang, Limin and Lin, Dahua and Qiao, Yu and Liu, Ziwei",
        "title": "VBench: Comprehensive Benchmark Suite for Video Generative Models"
      }
    ]
  },
  {
    "index": 25,
    "papers": [
      {
        "key": "kou2024subjectivealigneddatasetmetrictexttovideo",
        "author": "Tengchuan Kou and Xiaohong Liu and Zicheng Zhang and Chunyi Li and Haoning Wu and Xiongkuo Min and Guangtao Zhai and Ning Liu",
        "title": "Subjective-Aligned Dataset and Metric for Text-to-Video Quality Assessment"
      }
    ]
  },
  {
    "index": 26,
    "papers": [
      {
        "key": "itu",
        "author": "Series, B",
        "title": "Methodology for the subjective assessment of the quality of television pictures"
      }
    ]
  },
  {
    "index": 27,
    "papers": [
      {
        "key": "kou2024subjectivealigneddatasetmetrictexttovideo",
        "author": "Tengchuan Kou and Xiaohong Liu and Zicheng Zhang and Chunyi Li and Haoning Wu and Xiongkuo Min and Guangtao Zhai and Ning Liu",
        "title": "Subjective-Aligned Dataset and Metric for Text-to-Video Quality Assessment"
      }
    ]
  },
  {
    "index": 28,
    "papers": [
      {
        "key": "chivileva2023measuring",
        "author": "Chivileva, Iya and Lynch, Philip and Ward, Tomas E and Smeaton, Alan F",
        "title": "Measuring the quality of text-to-video model outputs: Metrics and dataset"
      }
    ]
  }
]