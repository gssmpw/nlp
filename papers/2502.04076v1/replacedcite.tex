\section{Related Work}
\subsection{Measurement for Text-to-Video Models}
Currently, common methods used in evaluating text-driven generated videos include some objective metrics____ and human-aligned methods____.
Objective metrics such as CLIP-score____ measure the mean cosine similarity between the text and each frame. IS____ utilizes the inception feature to measure the overall quality of image and video frames. Flow score____ calculates dynamic degree via optical flow models such as____.
However, these objective metrics do not align with human subjective perception and often evaluate videos from a single dimension. Some methods for evaluating natural video quality provide human-aligned overall evaluations____. 
DOVER____ assesses quality in terms of aesthetics and technicality. 
FastVQA____ utilizes grid mini-patch sampling to assess videos efficiently while maintaining accuracy.
Q-Align____ transforms the video quality assessment task into the generation of discrete quality level words via the Multimodal Large Language Model.
StableVQA____ measures video stability by separately obtaining the raw optical flow, semantic, and blur features.
These methods are suitable for natural video quality assessment but do not consider the text-video alignment, which is key to the evaluation of text-driven videos.
To address this, EvalCrafter____ assesses quality through a series of indicators including CLIP score, SD score, and natural video quality assessment methods. 
T2V-QA____ incorporates a transformer-based encoder and Large Language Model to assess text-driven AIGC videos.
TriVQA____ explores the video-text consistency through cross-attention pooling and the recaption of Video-LLaVA. 
However, there are still relatively fewer VQA methods specifically for AIGC videos. With the growth of new-generation videos, the requirements for understanding video dynamics and text consistency are becoming increasingly demanding, posing greater challenges.


\subsection{Text-to-Video Generation Method}
With the surge of diffusion models____, a lot of video generation models emerge____.
They represent a significant breakthrough in video generation. However, videos produced by previous methods still tend to suffer from issues such as low resolution, short duration, flickering, and distortion.
With the advent of Sora____, the new-generation models____ have made notable progress. Particularly recently, methods like Kling____, Gen-3-alpha____, and Qingying____ have achieved impressive video generation results and have been made available for community testing.
These videos generally alleviate the foundational problems seen in previous methods, with a duration of more than 5 seconds and frame rates above 24 fps.
Meanwhile, the content in these videos includes a lot of details, and they support the control via longer text inputs.
Under the wave of new-generation video generation models, effectively assessing more complex spatiotemporal relationships within the videos and exploring their consistency with longer texts is a topic worthy of further study.

\subsection{Text-to-Video VQA Dataset}
To evaluate and further promote the development of T2V models, some text-to-video VQA datasets have been proposed. 
Despite this, there are still relatively few text-to-video QA datasets suitable for evaluating current AIGC videos.
EvalCrafter____ collects 700 prompts and uses 5 models to generate 2500 videos in total.
FETV____ utilizes 619 prompts to generate 2,476 videos by 4 T2V models. 
Chivileva____ derives 1,005 videos generated from 5 T2V models.
VBench ____ uses nearly 1,700 prompts and 4 T2V models to generate 6984 videos. 
T2VQA-DB____ contains 10,000 videos generated by 1000 prompts. 
These datasets mainly meet two challenges:
(1) According to the  ITU-standard____, the number of human annotators should exceed 15 to keep the assessment error within a controllable range.
Among these, only T2VQA-DB____ and Chivileva____ meet the standard with 27 and 24 annotators. 
(2) The gap between previous and the concurrent AIGC videos. Previous videos often involve only easy movements and commonly have basic issues such as flickering, which are relatively rarely seen in the new-generation video models.
In this work, to address the issue that previous VQA datasets do not cover the annotated next-generation AIGC videos, we introduce CRAVE-DB, which includes 1,228 next-generation AIGC videos with subjective scores from 29 annotators, to provide a robust assessment of concurrent AIGC videos.