@article{chivileva2023measuring,
  title={Measuring the quality of text-to-video model outputs: Metrics and dataset},
  author={Chivileva, Iya and Lynch, Philip and Ward, Tomas E and Smeaton, Alan F},
  journal={arXiv preprint arXiv:2309.08009},
  year={2023}
}

@inproceedings{clip,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={International conference on machine learning},
  pages={8748--8763},
  year={2021},
  organization={PMLR}
}

@article{cogvideox,
  title={Cogvideox: Text-to-video diffusion models with an expert transformer},
  author={Yang, Zhuoyi and Teng, Jiayan and Zheng, Wendi and Ding, Ming and Huang, Shiyu and Xu, Jiazheng and Yang, Yuanming and Hong, Wenyi and Zhang, Xiaohan and Feng, Guanyu and others},
  journal={arXiv preprint arXiv:2408.06072},
  year={2024}
}

@article{ddpm,
  title={Denoising diffusion probabilistic models},
  author={Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={6840--6851},
  year={2020}
}

@inproceedings{dover,
  title={Exploring video quality assessment on user generated contents from aesthetic and technical perspectives},
  author={Wu, Haoning and Zhang, Erli and Liao, Liang and Chen, Chaofeng and Hou, Jingwen and Wang, Annan and Sun, Wenxiu and Yan, Qiong and Lin, Weisi},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={20144--20154},
  year={2023}
}

@inproceedings{evalcrafter,
  title={Evalcrafter: Benchmarking and evaluating large video generation models},
  author={Liu, Yaofang and Cun, Xiaodong and Liu, Xuebo and Wang, Xintao and Zhang, Yong and Chen, Haoxin and Liu, Yang and Zeng, Tieyong and Chan, Raymond and Shan, Ying},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={22139--22149},
  year={2024}
}

@inproceedings{fastvqa,
  title={Fast-vqa: Efficient end-to-end video quality assessment with fragment sampling},
  author={Wu, Haoning and Chen, Chaofeng and Hou, Jingwen and Liao, Liang and Wang, Annan and Sun, Wenxiu and Yan, Qiong and Lin, Weisi},
  booktitle={European conference on computer vision},
  pages={538--554},
  year={2022},
  organization={Springer}
}

@article{fvd,
  title={Towards accurate generative models of video: A new metric \& challenges},
  author={Unterthiner, Thomas and Van Steenkiste, Sjoerd and Kurach, Karol and Marinier, Raphael and Michalski, Marcin and Gelly, Sylvain},
  journal={arXiv preprint arXiv:1812.01717},
  year={2018}
}

@misc{gen3,
  author={Runway},
  title = {Gen-3},
  howpublished = "\url{https://runwayml.com/blog/introducing-gen-3-alpha/}",
  year = {2024}, 
}

@misc{hailuo,
  author={MiniMax},
  title = {Hailuo AI},
  howpublished = "\url{https://hailuoai.com/video}",
  year = {2024}, 
}

@InProceedings{huang2023vbench,
     title={VBench: Comprehensive Benchmark Suite for Video Generative Models},
     author={Huang, Ziqi and He, Yinan and Yu, Jiashuo and Zhang, Fan and Si, Chenyang and Jiang, Yuming and Zhang, Yuanhan and Wu, Tianxing and Jin, Qingyang and Chanpaisit, Nattapol and Wang, Yaohui and Chen, Xinyuan and Wang, Limin and Lin, Dahua and Qiao, Yu and Liu, Ziwei},
     booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
     year={2024}
 }

@misc{hunyuan,
      title={HunyuanVideo: A Systematic Framework For Large Video Generative Models}, 
      author={Tencent Hunyuan},
      year={2024},
      archivePrefix={arXiv preprint arXiv:2412.03603},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2412.03603}, 
}

@article{is,
  title={Improved techniques for training gans},
  author={Salimans, Tim and Goodfellow, Ian and Zaremba, Wojciech and Cheung, Vicki and Radford, Alec and Chen, Xi},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}

@article{itu,
  title={Methodology for the subjective assessment of the quality of television pictures},
  author={Series, B},
  journal={Recommendation ITU-R BT},
  volume={500},
  number={13},
  year={2012},
  publisher={International Telecommunication Union Switzerland, Geneva}
}

@misc{kling,
  author={Kuaishou},
  title = {Kling},
  howpublished = "\url{https://kling.kuaishou.com/}",
  year = {2024}, 
}

@misc{kou2024subjectivealigneddatasetmetrictexttovideo,
      title={Subjective-Aligned Dataset and Metric for Text-to-Video Quality Assessment}, 
      author={Tengchuan Kou and Xiaohong Liu and Zicheng Zhang and Chunyi Li and Haoning Wu and Xiongkuo Min and Guangtao Zhai and Ning Liu},
      year={2024},
      eprint={2403.11956},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2403.11956}, 
}

@article{lavie,
  title={Lavie: High-quality video generation with cascaded latent diffusion models},
  author={Wang, Yaohui and Chen, Xinyuan and Ma, Xin and Zhou, Shangchen and Huang, Ziqi and Wang, Yi and Yang, Ceyuan and He, Yinan and Yu, Jiashuo and Yang, Peiqing and others},
  journal={arXiv preprint arXiv:2309.15103},
  year={2023}
}

@article{liu2023fetv,
  title   = {FETV: A Benchmark for Fine-Grained Evaluation of Open-Domain Text-to-Video Generation},
  author  = {Yuanxin Liu and Lei Li and Shuhuai Ren and Rundong Gao and Shicheng Li and Sishuo Chen and Xu Sun and Lu Hou},
  year    = {2023},
  journal = {arXiv preprint arXiv: 2311.01813}
}

@misc{luma,
  author={LumaLabs},
  title = {Dream Machine},
  howpublished = "\url{https://lumalabs.ai/dream-machine}",
  year = {2024}, 
}

@inproceedings{make-a-video,
  title={Make-A-Video: Text-to-Video Generation without Text-Video Data},
  author={Singer, Uriel and Polyak, Adam and Hayes, Thomas and Yin, Xi and An, Jie and Zhang, Songyang and Hu, Qiyuan and Yang, Harry and Ashual, Oron and Gafni, Oran and others},
  year={2023},
  booktitle={The Eleventh International Conference on Learning Representations}
}

@article{modelscope,
  title={Modelscope text-to-video technical report},
  author={Wang, Jiuniu and Yuan, Hangjie and Chen, Dayou and Zhang, Yingya and Wang, Xiang and Zhang, Shiwei},
  journal={arXiv preprint arXiv:2308.06571},
  year={2023}
}

@software{opensora,
  author = {Zangwei Zheng and Xiangyu Peng and Tianji Yang and Chenhui Shen and Shenggui Li and Hongxin Liu and Yukun Zhou and Tianyi Li and Yang You},
  title = {Open-Sora: Democratizing Efficient Video Production for All},
  month = {March},
  year = {2024},
  url = {https://github.com/hpcaitech/Open-Sora}
}

@software{opensora-plan,
  author       = {PKU-Yuan Lab and Tuzhan AI etc.},
  title        = {Open-Sora-Plan},
  month        = apr,
  year         = 2024,
  publisher    = {GitHub},
  doi          = {10.5281/zenodo.10948109},
  url          = {https://doi.org/10.5281/zenodo.10948109}
}

@article{pickscore,
  title={Pick-a-pic: An open dataset of user preferences for text-to-image generation},
  author={Kirstain, Yuval and Polyak, Adam and Singer, Uriel and Matiana, Shahbuland and Penna, Joe and Levy, Omer},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={36652--36663},
  year={2023}
}

@misc{pika,
  author={Pika Labs},
  title = {Pika 1.5},
  howpublished = "\url{https://pika.art}",
  year = {2024}, 
}

@inproceedings{raft,
  title={Raft: Recurrent all-pairs field transforms for optical flow},
  author={Teed, Zachary and Deng, Jia},
  booktitle={Computer Vision--ECCV 2020: 16th European Conference, Glasgow, UK, August 23--28, 2020, Proceedings, Part II 16},
  pages={402--419},
  year={2020},
  organization={Springer}
}

@article{skflow,
  title={Skflow: Learning optical flow with super kernels},
  author={Sun, Shangkun and Chen, Yuanqi and Zhu, Yu and Guo, Guodong and Li, Ge},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={11313--11326},
  year={2022}
}

@techreport{sora,
    author = {Brooks, Tim and Peebles, Bill and Holmes, Connor and DePue, Will and Guo, Yufei and Jing, Li and Schnurr, David and Taylor, Joe and Luhman, Troy and Luhman, Eric and Ng, Clarence and Wang, Ricky and Ramesh, Aditya},
    title = {Video generation models as world simulators},
    institution = {OpenAI},
    year = {2024}
}

@inproceedings{stablediffusion,
  title={High-resolution image synthesis with latent diffusion models},
  author={Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj{\"o}rn},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={10684--10695},
  year={2022}
}

@inproceedings{stablevideo,
  title={Stablevideo: Text-driven consistency-aware diffusion video editing},
  author={Chai, Wenhao and Guo, Xun and Wang, Gaoang and Lu, Yan},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={23040--23050},
  year={2023}
}

@inproceedings{stablevqa,
  title={Stablevqa: A deep no-reference quality assessment model for video stability},
  author={Kou, Tengchuan and Liu, Xiaohong and Sun, Wei and Jia, Jun and Min, Xiongkuo and Zhai, Guangtao and Liu, Ning},
  booktitle={Proceedings of the 31st ACM International Conference on Multimedia},
  pages={1066--1076},
  year={2023}
}

@article{svd,
  title={Stable video diffusion: Scaling latent video diffusion models to large datasets},
  author={Blattmann, Andreas and Dockhorn, Tim and Kulal, Sumith and Mendelevitch, Daniel and Kilian, Maciej and Lorenz, Dominik and Levi, Yam and English, Zion and Voleti, Vikram and Letts, Adam and others},
  journal={arXiv preprint arXiv:2311.15127},
  year={2023}
}

@article{t2vqa,
  title={Subjective-Aligned Dateset and Metric for Text-to-Video Quality Assessment},
  author={Kou, Tengchuan and Liu, Xiaohong and Zhang, Zicheng and Li, Chunyi and Wu, Haoning and Min, Xiongkuo and Zhai, Guangtao and Liu, Ning},
  journal={arXiv preprint arXiv:2403.11956},
  year={2024}
}

@misc{tongyi,
  author={Ali Tongyi},
  title = {Wanxiang Video},
  howpublished = "\url{https://tongyi.aliyun.com/wanxiang/videoCreation}",
  year = {2024}, 
}

@article{trivqa,
  title={Exploring aigc video quality: A focus on visual harmony, video-text consistency and domain distribution gap},
  author={Qu, Bowen and Liang, Xiaoyu and Sun, Shangkun and Gao, Wei},
  journal={arXiv preprint arXiv:2404.13573},
  year={2024}
}

@inproceedings{vbench,
  title={Vbench: Comprehensive benchmark suite for video generative models},
  author={Huang, Ziqi and He, Yinan and Yu, Jiashuo and Zhang, Fan and Si, Chenyang and Jiang, Yuming and Zhang, Yuanhan and Wu, Tianxing and Jin, Qingyang and Chanpaisit, Nattapol and others},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={21807--21818},
  year={2024}
}

@article{videocrafter,
  title={Videocrafter1: Open diffusion models for high-quality video generation},
  author={Chen, Haoxin and Xia, Menghan and He, Yingqing and Zhang, Yong and Cun, Xiaodong and Yang, Shaoshu and Xing, Jinbo and Liu, Yaofang and Chen, Qifeng and Wang, Xintao and others},
  journal={arXiv preprint arXiv:2310.19512},
  year={2023}
}

@article{wu2023qalign,
  title={Q-Align: Teaching LMMs for Visual Scoring via Discrete Text-Defined Levels},
  author={Wu, Haoning and Zhang, Zicheng and Zhang, Weixia and Chen, Chaofeng and Li, Chunyi and Liao, Liang and Wang, Annan and Zhang, Erli and Sun, Wenxiu and Yan, Qiong and Min, Xiongkuo and Zhai, Guangtai and Lin, Weisi},
  journal={arXiv preprint arXiv:2312.17090},
  year={2023},
  institution={Nanyang Technological University and Shanghai Jiao Tong University and Sensetime Research},
}

@misc{ying,
  author={Zhipu},
  title = {Ying},
  howpublished = "\url{https://chatglm.cn/video}",
  year = {2024}, 
}

