
\subsection{Proprioceptive Inputs}
\subsubsection{Inertial Measurement Unit} The \ac{imu} is assumed to be rigidly attached to the robot, the robot body frame $\bodyframe$ is defined as the \ac{imu} coordinate frame, and the world frame is defined as $\mathcal{W}$. The $\ac{imu}$ provides the 3D accelerometer measurements $\accmeas$ and 3D gyroscope measurements $\gyromeas$ which are modelled as:
\begin{subequations}
\begin{align}
    \accmeas&=\accbody + \mathbf{b}_{\mathbf{a}} + \mathbf{q}^{-1}\mathbf{g}_{\mathcal{W}} + \mathbf{n}_{\mathbf{a}}\\
    \gyromeas&=\gyrobody + \mathbf{b}_{\boldsymbol{\omega}} + \mathbf{n}_{\boldsymbol{\omega}}
\end{align}
\end{subequations}
where $\accbody$ is the linear acceleration of the robot in the frame $\bodyframe$, $\mathbf{b}_\mathbf{a}$ is the acceleration bias, modelled as a random walk $\dot{\mathbf{b}_\mathbf{a}}\sim\mathcal{N}(\mathbf{0}, \Sigma_{\mathbf{b}_{\mathbf{a}}})$ and $\mathbf{n}_{\mathbf{a}}$ is the noise $\mathbf{n}_{\mathbf{a}}\sim\mathcal{N}\left(\boldsymbol{0},\Sigma_{\mathbf{a}}\right)$. The orientation of the robot is defined by $\mathbf{q}$ as a map from $\mathcal{B}$ to $\mathcal{W}$ and $\mathbf{g}_{\mathcal{W}}$ is the gravity vector aligned with $Z$ axis of the frame $\mathcal{W}$. Similarly, $\gyrobody$ denotes the angular velocity of the robot in $\bodyframe$, $\mathbf{b}_{\boldsymbol{\omega}}$ denotes the gyroscope bias modelled as a random walk $\dot{\mathbf{b}_{\boldsymbol{\omega}}}\sim\mathcal{N}\left(\mathbf{0}, \Sigma_{\mathbf{b}_{\boldsymbol{\omega}}}\right)$ and $\mathbf{n}_{\boldsymbol{\omega}}$ denotes the noise $\mathbf{n}_{\boldsymbol{\omega}}\sim\mathcal{N}\left(\mathbf{0}, \Sigma_{\boldsymbol{\omega}}\right)$.

\subsubsection{Motor Commands} Given a robot consisting of a total of $J$ rigidly attached bidirectional thrusters, whose input motor commands are $\left\{u_{j}: j\in\left[1, J\right]\right\}$, the combined thruster commands are expressed as a vector $\mathbf{u}_{J}=\left[u_{1}, u_{2},..., u_{J}\right]$. In the proposed method we use the actuator commands from the onboard autopilot.
\subsubsection{Battery Voltage}
The battery voltage is used to mitigate the dependence of thrust generated by the robot and the input motor command. The battery input is denoted as $k_{v}$ which is the instantaneous voltage of the battery.
\subsubsection{Network Input}
The input modalities described above are stacked into a $7+J$ channel vector at every time step $t$.
\begin{equation}
\mathbf{p}_{t}=\left[\accmeas,\gyromeas, \mathbf{u}_{J}, k_{v}\right]_{t}
\end{equation}

\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{PNG/DeepVLOverview2.png}
    % \caption{DeepVL network architecture considering \ac{imu}, motor commands and battery voltage as inputs proprioceptive inputs to the network, followed by the details of the network architecture and the training details for the model.}
    \caption{DeepVL method overview with \ac{imu}, motor commands and battery voltage as proprioceptive inputs to the ensemble of recurrent neural networks. The output velocity and covariance alongside the relative barometric depth measurement are then used in an \ac{ekf} for robot state estimation.}
    \label{fig:deepvlarchitecture}
\end{figure}


\subsection{Network Architecture} Motivated by the light-weight architecture of \ac{gru}~\cite{cho-etal-2014-learning}, their ability to process the latest input as it arrives and propagate the temporal contexts using hidden states, we use it as the core temporal recurrent backbone. The network, outlined in Figure~\ref{fig:deepvlarchitecture},
takes the current propriocecptive vector $\mathbf{p}_{t}$ as an input of dimension $7+J$ channels to the first layer of the network, followed by 3 layers of the \acp{gru} with hidden layer dimension of $40$. The output of the last \ac{gru} layer is passed through $50\%$ dropout followed by two fully connected layers each of dimension of $40\times3$ to obtain the $3D$ outputs of the predicted robot-centric velocity $\velpred$ and corresponding uncertainty $\velunc$. Let $\theta$ be the model parameters, and $\mu_{\theta}(\cdot)$ denote the network as a function, then the model at time $t$ can be defined as:

\begin{equation}
    \velpred_{t}, \velunc_{t}, \mathbf{h}_{t}=\mu_{\theta}\left(\mathbf{p}_{t}, \mathbf{h}_{t-1}\right)
\end{equation}
where $\mathbf{h}_{t-1}$ is the \ac{gru} hidden state from the last time step $(t-1)$. Similar to TLIO \cite{liuTLIOTightLearned2020a} we use \ac{mse} to train the network in the beginning until the predictions stabilize and then use \ac{gnll} to further supervise the predictions and uncertainty. The \ac{mse} loss for a batch of size $n$ is defined as:

\begin{equation}
    \mathcal{L}_{\mathrm{\ac{mse}}}(\vel,\velpred)=\frac{1}{n}\sum_{i=1}^{n}\|\vel_{i} - \velpred_{i}\|^{2}
\end{equation}
where $\vel=\{\vel_{i}\}_{i\leq{n}}$ are the robot-centric linear velocity used as supervision. Further, the \ac{gnll} is defined as:

\begin{equation}
    \mathcal{L}_{\mathrm{\ac{gnll}}}(\vel, \sigmapred, \velpred)=\frac{1}{n}\sum_{i=1}^{n}\left(\frac{1}{2}\log{|{\sigmapred_{i}}|}+\frac{1}{2}\|\vel_{i}-\velpred_{i}\|^{2}_{\sigmapred_{i}}\right)
\end{equation}
 where $\sigmapred=\{\sigmapred_{i}\}_{i\leq{n}}$ is the covariance matrix for the term. Similar to TLIO \cite{liuTLIOTightLearned2020a} we assume a diagonal covariance matrix and define $\sigmapred\left(\velpred\right)=\textrm{diag}\left(e^{2\velunc_{x}}, e^{2\velunc_{y}}, e^{2\velunc_{z}}\right)$. Since the supervision velocity is in the frame $\bodyframe$, the principle axes of the predicted covariance is along the \ac{imu} axes.
 
\subsection{Ensemble Predictive Uncertainty} The light-weight nature of the proposed model allows to use an ensemble of models to further enhance the predictions before integrating them into state estimation. We use the predictive uncertainty as described in \cite{lakshminarayananSimpleScalablePredictive2017} based on an ensemble of the neural network models. Hence, for an ensemble of $M$ networks, with $\theta_{m}$ as the parameters, the output is a mixture of Gaussians $\mathrm{M}^{-1}\Sigma\mathcal{N}\left(\velpred_{m}, \sigmapred_{m}\right)$ with mean as:
\begin{equation}
    \velpred_{*}=\frac{1}{\mathrm{M}}\sum_{m=1}^{M}\velpred_{m}
\end{equation}
 and the covariance as:
 \begin{equation}
     \sigmapred_{*}=\frac{1}{\mathrm{M}}\sum_{m=1}^{M}\left(\sigmapred_{m}+\velpred_{m}^{2}\right)-\velpred_{*}^{2}.
 \end{equation}

To incorporate the uncertainty, we train the ensemble of $\mathrm{M}$ networks and $\velpred_{*}$ is used as the velocity update in the \ac{ekf} with $\sigmapred_{*}$ as the covariance.


\subsection{Network Implementation Details}
The network contains $28$k trainable parameters. It is trained on sequences each having a length of $15$ seconds (i.e. $300$ data points) with a batch size of $128$. A total of $\approx120$k sequences are randomly sampled from the collected data to form the training set and $\approx10$k distinct sequences (not in the training data) are used as a validation set. The Adam optimizer is used with a learning rate of $0.001$, along with a multi-step rate scheduler at $1500$, $2500$, $3500$ with gamma of $0.2$. The training is started with \ac{mse} loss, while after $3000$ iterations we switch the loss function to \ac{gnll} and the training is stopped at $4000$ iterations. Furthermore, for the ensemble predictive uncertainty, we use an ensemble of $M=8$ networks. All input and outputs other than for uncertainty are normalized to achieve 0 mean and 1.0 standard deviation. The total training takes an hour to train the network on an NVIDIA RTX $3080$ GPU. The network can run both on GPU and CPU with an inference time \SI{<5}{\milli\second} on an Orin AGX. This efficient result is attributed to the single input, single output implementation where the \ac{gru} hidden states propagate the temporal context.
