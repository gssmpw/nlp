\subsection{Training Dataset Collection}
We used the BlueROV \ac{rov} further integrating an AlphaSense Core Research 5 Camera-\ac{imu} module to collect the data for training. The Alphasense module integrates the BMI085 \ac{imu} running at \SI{200}{\hertz}, and $5$ Sony IMX-287 cameras running at \SI{20}{\hertz}. We use a Pixhawk6X as the autopilot and Orin AGX \SI{32}{\giga\byte} as the onboard computer. The motor commands and the battery percentage are measured at \SI{20}{\hertz} from the autopilot via the MAVLink protocol. The Alphasense module is synchronized with the Orin AGX computing system via PTP, while the autopilot is connected via Ethernet and runs time synchronisation via MAVROS. To maximize the performance of the reference odometry, we use 4 of the cameras (the fifth is looking upwards and thus not used), namely the front-facing stereo pair (inclined by \SI{16}{\degree}) and the left and right side facing monocular cameras. \ac{reaqrovio}~\cite{SinghRCMinRovio2024} with $N=30$ features ($\ac{vio}_{30}$) is used to estimate the odometry and the robot-centric velocity of the vehicle, considering the data from all four cameras and the \ac{imu}, and its output is treated as ground-truth to supervise the network's training and assess the performance of the method in the evaluations presented further in this section. The intrinsics and extrinsics calibration for all cameras is performed in air and the refractive camera model in \cite{SinghRCMinRovio2024} adapts the cameras to the water by using a fixed value for the refractive index i.e. $1.33$ for the freshwater environments and $1.34$ for seawater environments respectively. 

We collected data in the laboratory pool of NTNU's Marine Cybernetics lab (MC-lab) and in the Trondheim Fjord. The dimensions of the MC-lab pool are $\SI{40}{\meter}\times\SI{6.45}{\meter}\times\SI{1.5}{\meter}$, while the average depth of the experiment site in the Fjord is \SI{6}{\meter}. The robot is piloted manually to perform a diverse set of motions in order to cover most of the state space, and such that both low- and high-frequency motions are experienced. Particularly, the trajectories involve velocities uniformly ranging from $0$ to \SI{\approx 0.8}{\meter/\second}, in all linear directions. The majority of the data was collected in the pool of MC-lab, which is about 1.5 meters deep thus extended vertical motions could not be captured. The dataset includes 3 hours and 40 minutes from the MC-lab and 20 minutes in the Trondheim Fjord, with the latter allowing for extended variations in $Z$ and also including long-duration static motions. In the MC-lab, the robot was piloted to move in all lateral directions for varying amounts, and the motion patterns were varied at random instances. Overall the key insight in data collection is to acquire a representative sample set of the motions a robot can undergo with the feasible limits. To further extend the envelope, we also change the attitude of the robot to random orientations using the ``Attitude Hold'' mode of the onboard autopilot. The robot was connected to the ground-station computer via a tether cable which is used for high-level communication and telemetry. It was ensured that the tether was tension-free to avoid any unaccounted force on the robot. The collected data shall be releasd in \url{https://github.com/ntnu-arl/underwater-datasets}. 

\subsection{Evaluation Dataset} For the evaluation of the proposed method we collected $6$ trajectories of varying lengths and coverage in the Fjord as showcased in Figure \ref{fig:collective_plot} (trajectory $1$ to $6$). Additionally, we collect $2$ trajectories ($7$ and $8$) in the indoor pool (MC-lab).

\subsection{Detailed Evaluation on a representative trajectory}
We present a detailed result on the trajectory $5$ collected in the Fjord. The length of the trajectory is \SI{235}{\meter} and the maximum linear velocity of the robot is \SI{0.72}{\meter/\second} while the average velocity is \SI{0.52}{\meter/\second}. Its total duration is \SI{440}{\second}, and it consists of three laps starting from the origin and reaching the endpoint as shown in Figure \ref{fig:detailed_results}(a). First, the robot descends from the surface, and moves along the wall, while it is then piloted to move along the seabed overall ensuring that in general there are visual features such that the result of \ac{reaqrovio} with all $4$ cameras ($\ac{vio}_{30}$) can be considered a reasonable ground-truth. The executed trajectory is shown in Figure \ref{fig:detailed_results}. We compare the odometry estimates from $\ac{deepvl}_{0}$ (i.e. with no camera feed) and show that it achieves a Relative Position Error (\ac{rpe}) \ac{rmse} of \SI{0.33}{\meter} over deltas of \SI{10}{\meter} (\SI{\approx3}{\percent}). We also present the result for $\ac{deepvl}_{1}$  from a monocular camera against $\ac{vio}_{1}$. In the present trajectory, the estimates of $\ac{vio}_{1}$ diverges. We also compare the \ac{imu} biases estimated by $\ac{deepvl}_{0}$ over time against the reference ground-truth odometry estimation and the bias convergence is verified. Furthermore, we compare the trace of uncertainty predicted by the network $\sigmapred$ and predicted from the ensemble of the networks $\sigmapred_{*}$. The former is consistently overconfident about the velocity predictions. The effect of network uncertainty is further discussed in Section~\ref{subsection:ablation}. Last, we present the \ac{rpe} over time which also validates the enhancement in the odometry performance from $\ac{deepvl}_{0}$ to $\ac{deepvl}_{1}$.
\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{PDF/RPEcombined.pdf}
    \caption{Aggregate \ac{rpe} over all trajectories with monocular (left) and stereo (right) camera to analyze the effect of incrementally increasing the number of maximum features used in \ac{vio} with and without the integration of \ac{deepvl}.}
    \label{fig:aggregate_rpe}
\end{figure}
\subsection{Collective evaluation on all trajectories}
We further evaluate the proposed method on the full dataset of $8$ trajectories (Figure~\ref{fig:collective_plot}). Firstly, we evaluate $\ac{deepvl}_{0}$ and the average $\ac{rpe}$ over all trajectories over deltas of \SI{10}{\meter} is \SI{0.39}{\meter}. We further vary the number of features $N$ ranging from $(1, 2, 4, 8)$ in both $\ac{vio}_{N}$ and $\ac{deepvl}_{N}$. Figure \ref{fig:aggregate_rpe} shows the plot of \ac{rpe} values and corresponding standard deviation for $\ac{deepvl}_{N}$ and $\ac{vio}_{N}$ for both the front-left monocular and the stereo pair of the Alphasense camera. Likewise, Table~\ref{table:average_rpe_mono_stereo} summarizes \ac{rpe} values across the dataset. As shown, the \ac{rpe} for $\ac{deepvl}_{N}$ is consistently lower than $\ac{vio}_{N}$ for the same number of features.

\begin{table}[]
\caption{Table presenting the average of the \ac{rpe} for varying visual features for monocular and stereo camera configuration. The symbol 'X' indicates that a method diverges. The \ac{rpe} was calculated over delta of \SI{10}{\meter}.}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{|l|lllll|}
\hline
             & \multicolumn{5}{l|}{Maximum Visual Features (N)}                                                     \\ \hline
\textbf{}    & \multicolumn{1}{l|}{0} & \multicolumn{1}{l|}{1}           & \multicolumn{1}{l|}{2}           & \multicolumn{1}{l|}{3}           & 4           \\ \hline
$\ac{deepvl}_{N}$(Mono) & \multicolumn{1}{l|}{0.393} & \multicolumn{1}{l|}{0.260} & \multicolumn{1}{l|}{0.226} & \multicolumn{1}{l|}{0.193} & 0.188 \\ \hline
$\ac{vio}_{N}$ (Mono)& \multicolumn{1}{l|}{-} & \multicolumn{1}{l|}{X} & \multicolumn{1}{l|}{0.489} & \multicolumn{1}{l|}{0.241} & 0.209  \\ \hline
$\ac{deepvl}_{N}$(Stereo)  & \multicolumn{1}{l|}{0.393} & \multicolumn{1}{l|}{0.229} & \multicolumn{1}{l|}{0.158} & \multicolumn{1}{l|}{0.135} & 0.134 \\ \hline
$\ac{vio}_{N}$  (Stereo)& \multicolumn{1}{l|}{-} & \multicolumn{1}{l|}{X} & \multicolumn{1}{l|}{0.222} & \multicolumn{1}{l|}{0.158} & 0.154 \\ \hline
\end{tabular}%
}\label{table:average_rpe_mono_stereo}
\end{table}

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{PDF/Ablation.pdf}
    \caption{Ablation results for comparison between the odometry results from the proposed network, the proposed network with \ac{gnll} uncertainty and a network that only uses motor commands as inputs.} 
    \label{fig:ablation}
\end{figure}

\subsection{Ablation Study}\label{subsection:ablation}
To evaluate the proposed model, we conduct the following ablations: `Case$1$': We first replace the proposed ensemble-based predictive uncertainty by a single model \ac{gnll}-based uncertainty. It is observed that the former achieves superior \ac{rpe} scores for odometry compared to the latter. `Case$2$': We evaluate the network that is only trained on the motor command data as input to analyse the contribution of the motor commands in the proposed network. Figure \ref{fig:ablation} shows the \ac{rpe} for both the above cases and the proposed model for every trajectory, while Table \ref{table:ablation} shows the average \ac{rpe} for all trajectories. The proposed  model with the \ac{imu}, motor commands and battery inputs performs superior compared to the model trained only on motor inputs. Similarly, we train a model only with \ac{imu} measurements as the input and evaluate the resulting odometry performance where the average (over all $8$ trajectories) \ac{rpe} is \SI{3.90}{\meter} compared to \SI{0.39}{\meter} for the proposed network, and \SI{0.48}{\meter} for the network trained only with motor commands. These ablations verify the use of the ensemble-based approach, and provide insight into the complementary nature of \ac{imu} measurements and motor command inputs to the model.

\subsection{Closed loop position control verification}
We use the presented method to perform state estimation onboard the robot in the MC-lab and command it to follow a \SI{2}{\meter}-wide square path through closed-loop control (Figure~\ref{fig:position_control}). The state estimation does not include any visual features and instead relies exclusively on \ac{imu}, motor data, battery readings, and barometer measurements through \ac{deepvl} as in Sections~\ref{sec:deepvl},~\ref{sec:vio}. This experiment demonstrates that the proposed method a) allows robot autonomy to perform at a functional level in the case of complete loss of vision and b) generalizes on motions that have not been in the training data. Specifically, here the network experiences closed-loop position control data which typically contain different frequency content compared to human steering. 

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{PNG/PositionControl.png}
    \caption{The plot on the left showing topdown view of the online estimated position with \ac{deepvl} running onboard the robot which is used for following position set-points along the vertices of a square of edge length \SI{2}{\meter}. The two plots on right show the estimated position vs. time. The ``Ground-Truth'' is estimated offline using \ac{vio}.}
    \label{fig:position_control}
\end{figure}

\begin{table}[]
\caption{Table presenting the average of the \ac{rpe} for the ablation of the network.}
%\resizebox{\columnwidth}{!}{%
\begin{tabular}{|l|l|l|l|l|}
\hline
\textbf{RPE}    & Proposed & Case1  & Case2 & IMU Only \\ \hline
\textbf{Avg RMSE (m)} & 0.393        & 0.454       & 0.482      & 3.907    \\ \hline
\textbf{Avg STD (m)}  & 0.181        & 0.211       & 0.185      & 2.031    \\ \hline
\end{tabular}%
%}
\label{table:ablation}
\end{table}
