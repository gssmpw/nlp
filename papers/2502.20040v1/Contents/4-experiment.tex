\section{Experimental setups}
In the section, we present the experiment datasets, experimental configurations, evalution metrics and comparison methods. 
% The implementations and demos of the proposed method are publicly available\footnote{https://audio.westlake.edu.cn}.

\subsection{Dataset}

\subsubsection{Speech enhancement training dataset} The proposed model is trained with synthetic noisy/clean speech pairs. Reverberant speech signals are generated by convolving source speech signals with RIRs, then added with noise signals. Clean speech signals are generated by convolving source speech signals with the direct-path part of RIRs.

In this work we conduct speech enhancement for both Mandarin Chinese and English, and we will evaluate our model on five different datasets (as will be shown later). We attempt to train the model once and test it on all the datasets, which will reflect the general capability of the model under various situations. To do this, we collect data (in terms of source speech, RIRs and noise) from multiple public datasets, and form a training set with sufficient speech quality and environment/device diversity. 


\textbf{Source speech}: Source speech signals are collected from 6 datasets, including AISHELL I \cite{bu2017aishell1}, AISHELL II \cite{du2017aishell2} and THCHS30 \cite{wang2015thchs} for Chinese, and EARS \cite{richter2024ears}, VCTK \cite{Yamagishi2019VCTK} and DNS I challenge \cite{reddy2020dns1} for English. 
% The mixing ratio and speech quality of two languages of datasets are controlled when organizing the dataset. 
For each language, about 200 hours of high-quality speech data are selected from the original datasets, based on the raw score of DNSMOS P.835 \cite{reddy2022dnsmos835}. The selection thresholds are set to 3.6 and 3.5 for Chinese and English data, respectively. Except that the entire EARS training set is included, since EARS involves various emotional speech that cannot be well evaluated by the DNSMOS.
The amount of data selected from each dataset is summarized in Table~\ref{tab: e-speech-dur}.

\input{Contents/Tables/E-speech-dur-table}

\textbf{RIR}: 
We use real-measured RIRs from multiple public datasets \cite{eaton2015ace, jeub2009air, prawda2022ARNI, carlo2021dechorate, Hadad2014MultiChannel, james2016naturalreverb, Kinoshita2013REVERB, nakamura2000rwcp}. Table~\ref{tab: e-rirs} shows the statistics of these RIR datasets. For all (even multi-channel) datasets, all RIRs are used, except that we uniformly sampled 1,000 RIRs from the very large original ARNI dataset. 

The reverberation time, i.e. $T-{60}$, of RIRs mostly lie in the range of $(0, 1.5)$ seconds, except for a few rooms in AIR \cite{jeub2009air} and NaturalReverb \cite{james2016naturalreverb}. The distribution of the $T_{60}$s are shown in Fig.~\ref{fig: t60-dist}. 
Besides the wide distribution range of $T_{60}$s, these RIRs also have large diversity in terms of environments and measuring devices. For example, NaturalReverb \cite{james2016naturalreverb} is recorded in 271 spaces encountered by humans during daily life, AIR \cite{jeub2009air} is measured with a dummy head for binaural applications, RWCP \cite{nakamura2000rwcp} use a Head-Torso as source speaker, etc. 
When synthesizing reverberant speech, 80\% source speech samples are convolved with a randomly selected RIR, while there is no RIR convolution for the rest 20\% samples to account for the near-field applications where reverberation is negligible. 
\input{Contents/Tables/E-rirs}
\input{Contents/Images/E-rir-dist}

\textbf{Noise}:
Speech and noise are mixed with a random signal-to-noise ratio (SNR) between -5 dB and 20 dB. We use the noise signals from the DNS challenge \cite{reddy2021dns3} and the RealMAN dataset \cite{Bing2024RealMAN}. The DNS challenge dataset has about 181 hours of noise sampled from AudioSet \cite{audioset} and Freesound. The RealMAN dataset has 106 hours of ambient noise recorded in 31  daily life scenes, including various indoor, semi-outdoor, outdoor and transportation scenes.  

\subsubsection{Speech enhancement evaluation datasets}
The speech enhancement performance of the proposed model is evaluated on five datasets. For Chinese, we use the public test set (static speaker) of the RealMAN dataset \cite{Bing2024RealMAN}. For English, the evaluation is conducted on 4 different datasets: (1) the CHiME4 challenge `isolated\-1ch\-track' test set \cite{vincent2016chime4}, (2) the one-channel test set of REVERB \cite{Kinoshita2013REVERB}, (3) the test set of DNS I challenge \cite{reddy2020dns1}, (4) the public EARS blind test dataset \cite{richter2024ears}. 


\subsubsection{ASR evaluation datasets}
The ASR performance of the proposed model is evaluated on three  datasets. For Chinese, RealMAN (static) \cite{Bing2024RealMAN} is  evaluated, with the pre-trained WenetSpeech ASR model provided in ESPNet \footnote{https://github.com/espnet/espnet/tree/master/egs2}. For English, CHiME4 \cite{vincent2016chime4} and REVERB \cite{Kinoshita2013REVERB} are evaluated, with their respective pre-trained ASR models obtained using ESPNet. Note that, the ASR models used in this work have the same STFT and mel-frequency configurations, thence one CleanMel model can be used for all of them. 


\subsection{Configurations}

\input{Contents/Tables/E-model-size}

\subsubsection{Data configurations}
\label{sec: dataset-data-preprocessing}
The sampling rate of all data is set to 16 kHz. STFT is applied using Hanning window with a length of 512 samples (32 ms) and a hop size of 128 and 256 samples (8 and 16 ms) for the offline and online models, respectively. The offline model has a finer temporal resolution than the online model since it is used for ASR in this work and its temporal resolution is aligned with the ASR models. However, we empirically found that, compared to the 16-ms hope size, the 8-ms hope size does not benefit much for the speech enhancement performance. The number of Mel frequencies is set to $F_\text{Mel}=80$ for the frequency range of 0-8 kHz. The same STFT implementation (ESPNet implementation\footnote{https://github.com/espnet/espnet/blob/master/espnet2/layers/stft.py})
is used for the CleanMel networks, neural vocoders and ASR models, to avoid configuration mismatch. The natural logarithm (base of $e$) is used.

\subsubsection{Network configurations}

Follow \cite{quan2024spatialnet}, the kernel size of the T-Conv1d in the input module and the F-GConv1d layers in the cross-band block are both set to 5. As shown in Fig.~\ref{fig: model-arch}, in narrow-band block, forward-only and forward/backward Mamba layers are set for online and offline processing, respectively. We set up two model scales for the offline models, referred to as CleanMel-{S} and CleanMel-{L}. And the online model scale is set approximately to CleanMel-{S}. The configurations are shown in Table~\ref{tab: model-size}. The depth $L$ of online models are set to twice the one of corresponding offline models to have the similar model size. Due to the different setups of STFT hop size, the computational complexity, i.e. FLOPs, of offline models are roughly twice the one of corresponding online models.


\subsubsection{Training and inference setups}
For CleanMel, AdamW optimizer \cite{loshchilov2018AdamW} with an initial learning rate of $10^{-3}$ is used for training. The learning rate exponentially decays with $\text{lr} \leftarrow 0.001 \times 0.99^{\text{epoch}}$. Gradient clipping is applied with a gradient norm threshold 10. The batch size are set to 32. Training samples are synthesized in an on-the-fly manner, and 100,000 samples are considered as one training  epoch. The CleanMel-{S} and CleanMel-{L} models are trained by 100 and 150 epochs, respectively.
% All models are trained to convergence for 150 epochs. 
Afterward, we average the model weights of the last 10 epochs as the final model for inference. 

For training the Vocos neural vocoder \cite{siuzdak2024vocos}, we synthesized 400,000 (direct-path) clean speech samples with both English and Chinese data used for CleanMel training. The training configurations keep unchanged as in its original work. For ASR evaluation, as already mentioned, the pre-trained ASR models obtained using in ESPNet are used.  


\subsection{Evaluation metrics}
Speech enhancement performance is evaluated with Perceptual Evaluation of Speech Quality (PESQ) \cite{rix2001pesq}, DNSMOS P.808 \cite{reddy2021dnsmos808} and P.835 \cite{reddy2022dnsmos835}, where the background, signal and overall scores for P.835 are all reported. Word Error Rate (WER) and Character Error Rate (CER) are used to evaluate English and Chinese ASR performances, respectively.


\subsection{Comparison models}
We compare with five advanced speech enhancement models, which were all claimed in their original papers to be able to conduct joint speech denoising and dereverberation, including (i) FullSubNet \cite{hao2021fullsubnet} is a LSTM-based full-band and sub-band fusion network originally proposed for online speech denoising, and extended to speech dereverberation in \cite{zhou2023rts}. For offline processing, we change the uni-directional LSTMs to be bi-directional. (ii) Demucs \cite{defossez20demucs} is an online speech enhancement model that operates directly on waveforms with a U-net. (iii) VoiceFixer \cite{liu22voicefixer} also performs Mel-spectrogram enhancement (using a ResUNet) and generates the waveform using a neural vocoder, but is developed only for offline speech enhancement. (iv) StoRM \cite{Lemercier2023storm} is a powerful diffusion-based offline speech enhancement system. (v) SpatialNet \cite{quan2024spatialnet} and oSpatialNet \cite{quan2024oSpatialNet} perform speech enhancement in the STFT linear-frequency domain, and offer the backbone network for the proposed CleanMel model. In SpatialNet and oSpatialNet, self-attention (and temporal convolution) and Mamba are adopted for learning the narrow-band spatial information in an offline and an online manner, respectively. 
For offline processing, besides comparing with the original SpatialNet, we also implemented a variant of it with bi-directional Mamba for narrow-band blocks (same architecture with the proposed method), which is referred to as SpatialNet-Mamba,  and serves as the linear-frequency baseline for the proposed Mel-frequency model.
Note that, SpatialNet and oSpatialNet were originally proposed for multi-channel speech enhancement, and this work is the first one to fully evaluate and analyze the capability of them for single-channel speech enhancement. 

To conduct fair comparisons, we re-train the FullSubNet, StoRM and oSpatialNet/SpatialNet models using the same training data utilized for the proposed model. However, we found that it is not easy to re-train the Demucs and Voicefixer models. In \cite{defossez20demucs}, different data augmentation strategies are applied when training Demucs on different datasets. Voicefixer \cite{liu22voicefixer} was designed for 44.1 kHz signals. Re-training Demucs and VoiceFixer request careful data engineering or hyperparameter search. Therefore, we use the pre-trained checkpoints of Demucs and VoiceFixer to perform speech enhancement on only English data. For Demucs, we use the `dns64' model provided by the authors \footnote{https://github.com/facebookresearch/denoiser}. For VoiceFixer, we use the default pre-trained model provided in the open-source package \footnote{https://github.com/haoheliu/voicefixer}. Following the VoiceFixer default inference setup, test waveforms are first up-sampled to 44.1 kHz to perform speech enhancement and then down-sampled back to 16 kHz for evaluation.




