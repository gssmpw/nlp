\section{Introduction}

\IEEEPARstart{T}HIS work studies single-channel speech enhancement using deep neural networks (DNNs), to improve both speech quality and automatic speech recognition (ASR) performance. 
A large class of speech enhancement methods employ DNNs to map from noisy and reverberant speech to corresponding clean speech, conducted either in time domain \cite{Luo2019Conv, Defossez2020Real} or in time-frequency domain \cite{Xiong2022Spectro, hu2020DCCRN, li2022glance}. These methods can efficiently suppress noise, but not necessarily improve ASR performance due to the speech artifacts/distortions caused by speech enhancement networks \cite{iwamoto2022artifacts}. In \cite{kinoshita2020improving}, it was found that time-domain enhancement is more ASR-friendly than frequency-domain enhancement. A time domain progressive learning method is proposed in \cite{nian2022progressive}, which also shows the superiority of time domain speech enhancement, and the progressive learning mechanism is very effective for robust ASR by mitigating the over suppression of speech. In \cite{yang2023robustASR, yang2024towards}, ASR performance is largely improved by decoupling frontend enhancement and backend recognition. In \cite{yang2024towards}, it is shown that an advanced time-frequency domain network, i.e. CrossNet, can even outperform the time domain ARN network.

Speech enhancement in Mel-frequency domain, or similarly in
rectangular bandwidth (ERB) domain, has been developed under various contexts in the literature. Mel-frequency and ERB bands model human speech perception of spectral envelope and signal periodicity, within which speech enhancement is more perceptually and computationally efficient than within linear-frequency domain or time domain. In \cite{valin2018hybrid,valin2020perceptually, schroter23deepfilter}, spectral envelope enhancement is performed within the ERB bands, and then applying pitch filtering \cite{valin2018hybrid, valin2020perceptually} or deep filtering \cite{schroter23deepfilter} to recover the enhanced speech. 
Sub-band networks \cite{Xiong2022Spectro,li2019narrow} and full-band/sub-band fusion networks, i.e. FullSubNet \cite{hao2021fullsubnet, zhou2023rts}, have been recently proposed and achieved outstanding performance. However, separately processing sub-bands in the linear-frequency domain leads to a large computational complexity. To reduce the number of sub-bands and thus the computational complexity, Fast FullSubNet \cite{hao2022fast} and the work of \cite{kothapally2023deep} proposed to perform sub-band processing in the Mel-frequency domain, and then transform back to linear frequency with a joint post-processing network. In \cite{liu22voicefixer, tian2023diffusion}, speech enhancement is directly conducted in the Mel-frequency domain and then a separate neural vocoder is used to recover speech waveform. These methods improve speech enhancement capability by alleviating the burden of enhancing full-band speech details and also by leveraging the powerful full-band speech recovery capacity of advanced neural vocoder, as a result, achieve higher speech quality compared to their linear-frequency counterparts. 

In this work, we propose CleanMel, a single-channel Mel-spectrogram enhancement network for improving both speech quality and ASR performance. Different from the previous works \cite{valin2018hybrid,valin2020perceptually,schroter23deepfilter,hao2022fast} that performing speech enhancement in the ERB or Mel domain, and then applying a joint pitch filtering or deep filtering to obtain the enhanced speech, this work decouples the Mel-spectrogram enhancement and post-processing parts by targeting the enhancement network with clean Mel-spectrogram. The enhanced Mel-spectrogram can be directly used for ASR, or transformed back to waveform with a separate neural vocoder as is done in \cite{liu22voicefixer}. Compared to linear-frequency spectrogram or time-domain waveform, Mel-frequency presents speech in a more compact and less-detailed way (but still perceptually efficient) and has a lower feature dimension (number of frequencies) from the perspective of machine learning, which would result in lower the prediction error. % of enhancing Mel-spectrogram would be lower. 
This is beneficial for both speech quality improvement and ASR: (i) Neural vocoders have been extensively studied in the field of Text-to-Speech, and are capable of efficiently transforming Mel-spectrogram back to time-domain waveform. Therefore, the low-error property of Mel-spectrogram enhancement can be hopefully maintained by the neural vocoders and higher speech quality can be achieved. (ii) From the perspective of ASR, there seems no need to first recover the less-accurate full-band speech details and then compress to Mel-frequency. A direct and more-accurate Mel-spectrogram estimation would be preferred.  

We adapt the network architecture of our previous proposed (online) SpatialNet \cite{quan2024spatialnet,quan2024oSpatialNet} with some modifications to better accommodate Mel-spectrogram enhancement.
SpatialNet is composed of interleaved cross-band and narrow-band blocks originally proposed for processing multichannel STFT frequencies. The narrow-band block processes STFT frequencies independently to learn the spatial information presented in narrow-band (one frequency), such as the convolutive signal propagation and the spatial correlation of noise. And the cross-band block was designed for learning the across-frequency dependencies of narrow-band information. 
As for single-channel Mel-spectrogram enhancement in this work, the narrow-band block processes Mel frequencies independently to also learn the (single-channel) convolutive signal propagation of target speech, which is crucial not only for conducting dereverberation of target speech but also for discriminating between target speech and interfering signals. The cross-band block is now reinforced and utilized to learn the full-band spectral pattern in the Mel-frequency domain.  

In addition, we have studied several critical issues when decoupling the Mel-spectrogram enhancement front-end and ASR/Vocoder back-ends. i) We have systematically studied and compared different learning targets that can be used for Mel-spectrogram enhancement, including logMel mapping, Mel ratio masking and the clipping issue of logMel; ii) We have developed a data normalization scheme to align the signal levels of cascaded front-end and back-end models; iii) We have developed an online neural vocoder to enable online speech enhancement. 

Experiments are conducted on five public datasets (four English and one Chinese) for speech denoising and dereverberation individually or jointly. Importantly, we adopt a more realistic evaluation setup: from multiple data sources of clean speech, real-measured room impulse responses (RIRs) and noise signals, we collected and organized a relatively large-scale training set, based on which we train the network for once and directly test it on all the five test sets. Experiments show that the proposed model achieves the state-of-the-art (SOTA) speech enhancement performance in term of speech perceptual quality.
Moreover, on top of various pre-trained and advanced ASR models, the proposed model prominently improves the ASR performance on all datasets.
These results demonstrate that our trained models have the potential to be directly employed to real applications. 