\section{Mel-spectrogram Enhancement}
In this work, we propose to enhance the Mel-spectrograms, which then can be directly fed into an ASR model, or transformed to waveforms with a neural vocoder. 

\subsection{Learning Target: Clean Mel-spectrogram}
\label{sec: learning-target}
The power-based or magnitude-based Mel-spectrogram of the target speech $X(f,t)$, denoted as $X_{\text{mel}}(f_{\text{mel}},t)$, can be obtained by weighted summing the squared magnitude $|X(f,t)|^2$ or magnitude $|X(f,t)|$ over frequencies with the triangle weight functions of Mel filterbanks,  where $f_{\text{mel}}\in\{1,...,F_{\text{mel}}\}$ denotes the index of Mel-frequency. Our preliminary experiments showed that using  power-based or magnitude-based Mel-spectrograms achieve similar enhancement performance, thence we can choose either of them according to the Mel setup of ASR or neural vocoder backend models. In this work, we use the power-based Mel-spectrogram according to the setup of commonly-used ASR models. Then, the logMel-spectrogram, namely the logarithm of $X_{\text{mel}}(f_{\text{mel}},t)$ 
\begin{align}
\label{eq:logmel}
X_{\text{logmel}}(f_{\text{mel}},t)=\log(\max\{X_{\text{mel}}(f_{\text{mel}},t), \epsilon\})
\end{align}
can be taken as the input feature of ASR or neural Vocoder backends. The base of logarithm ($e$ or 10) should be consistent to the one of back-ends as well, while $e$ is used in this work. 
The Mel-spectrogram is clipped with a small value of $\epsilon$ to avoid applying logarithm to close-zero values. 

Normally, $\epsilon$ is set to be very small, e.g. 1e-10, to maintain complete speech information. However, in our preliminary experiments, we found that very small speech values are not very informative for both ASR and neural vocoder, and thus can be clipped without harming performance. Moreover, since those small values are highly contaminated by noise or reverberation, the prediction error of them could be very large.
For these reasons, we set $\epsilon$ to a relatively large value, e.g. 1e-5 (when the maximum value of time domain signal is normalized. The signal normalization methods will be presented in Section~\ref{sec:normalization}). Fig.~\ref{fig: cleantarget} gives an example of our target logMel-spectrogram, in which about 40\% TF bins are clipped. 

In this work, we evaluate two different learning targets as for Mel-spectrogram enhancement. 

\textbf{LogMel mapping}: The clean logMel-spectrogram can be directly predicted with the network. The training loss is set to the mean absolute error (MAE) loss between the predicted and the clean logMel-spectrogram, namely 
\begin{align}
\begin{aligned}
    \mathcal{L}_{\text{MAE}}
    =\frac{1}{F_{\text{mel}}T}\sum_{f_{\text{mel}}=1}^{F_\text{mel}}\sum_{t=1}^T  |\hat{X}_{\text{logmel}}(f_{\text{mel}},t) - X_{\text{logmel}}(f_{\text{mel}},t)|,     
\end{aligned}
\label{eq: loss}
\end{align}
where $\hat{X}_{\text{logmel}}(f_{\text{mel}},t)$ is the network output.  

\textbf{Mel ratio mask}: Ratio mask is one type of popular learning targets for speech magnitude enhancement  \cite{wang2018supervised}. For each time-mel-frequency bin, the Mel ratio mask is defined as 
\begin{align}
    M(f_\text{mel}, t) = \text{min} \left( \sqrt{\frac{X_{\text{mel}}(f_\text{mel},t)}{Y_\text{mel}(f_\text{mel}, t)}}, 1 \right),
\end{align}
where $Y_\text{mel}(f_\text{mel}, t)$ denotes the power level of noisy Mel-spectrogram. The square root function transforms the power domain to the magnitude domain. The $\text{min}(\cdot)$ function rectifies the mask into the range of $[0,1]$. 
The mean squared error (MSE) of the ratio mask is taken as the training loss, namely
\begin{align}
    \mathcal{L}_{\text{MRM}} = \frac{1}{F_{\text{mel}}T}\sum_{f_{\text{mel}}=1}^{F_\text{mel}}\sum_{t=1}^T (M(f_\text{mel}, t)-\hat{M}(f_\text{mel}, t))^2,
\end{align}
where $\hat{M}(f_\text{mel}, t)$ denotes the model prediction of ${M}(f_\text{mel}, t)$. Then, the enhanced logMel-spectrogram can be obtained as
\begin{align}
    \hat{X}_\text{logmel}(f_\text{mel}, t) = \text{log}(\max\{\hat{M}(f_\text{mel}, t)^2Y_\text{mel}(f_\text{mel}, t), \epsilon\}).
\end{align}



\input{Contents/Images/M-logmel-spec}

\subsection{The CleanMel Network} 
\label{sec: model-arch}
\input{Contents/Images/M-model-architecture}

Fig.~\ref{fig: model-arch} shows the network architecture. The proposed network takes as input (the real ($\mathcal{R}(\cdot)$) and imaginary ($\mathcal{I}(\cdot)$) parts of) the STFT of microphone recording, i.e. $Y(f,t)$, denoted as $\mathbf{y}$:
\begin{align}
\label{eq:feature}
\mathbf{y}[f,t,:]=[\mathcal{R}(Y(f,t)), \mathcal{I}(Y(f,t))]\in \mathbb{R}^2,
\end{align}
where $[:]$ represents to take values of one dimension of a tensor.
The network is composed of an input layer, interleaved cross-band and narrow-band blocks first in the linear-frequency domain and then in the Mel-frequency domain, a Mel-filterbank, and finally a Linear output layer. The input layer conducts temporal convolution on $\textbf{y}$ with a kernel size of 5, obtaining the hidden representation with the dimensions of $F\times T \times H$.
Then one cross-band block and one narrow-band block process the hidden tensor in linear-frequency. Mel-filterbank (with triangle weight functions) transforms the frequency dimension from $F$ linear frequencies to $F_\text{mel}$ Mel frequencies, which is realized with a non-trainable matrix multiplication. Then $L$ interleaved cross-band and narrow-band blocks process the tensor in Mel-frequency. 
After the final narrow-band block, the output Linear layer transforms $H$-dim to 1-dim as either the enhanced logMel-spectrogram or the Mel ratio mask. Note that Sigmoid activation is applied for predicting Mel ratio mask. 
 

\subsubsection {Narrow-band block}
\label{sec: arch-narrow-band}
As shown in Eq.~(\ref{eq:stft}), the time domain convolution can be decomposed as frequency-independently narrow-band convolutions, while the latter has much smaller complexity compared to the former in terms of the order of room filters. Therefore, modeling the narrow-band convolution would be much more efficient than modeling the time domain convolution. The convolution model of target speech provides not only necessary information for dereverberation of the target speech but also discriminative information between the target speech and other interfering sources. In addition, in narrow-band, non-stationary speech and stationary noise can be well discriminated by testing the signal stationarity. For these reasons, we propose the narrow-band network, which processes frequencies independently along the time dimension, and all frequencies share the same network. 

The narrow-band convolution in Eq.~(\ref{eq:stft}) is defined between the complex-valued STFT coefficients of source speech and room filter. Thence we process the complex-valued STFT coefficients of noisy signal (in a hidden space), instead of other features such as magnitude, to retain the convolution correlation. We first process in (the finer) linear-frequency with one narrow-band block to fully exploit the convolution information, then after Mel-filterbanks, we process in (the coarser) Mel-frequency with more narrow-band blocks.

The narrow-band network is composed of Mamba blocks \cite{gu2023mamba}. Mamba is a recently proposed network architecture based on structured state space sequence models, which was shown very efficient for learning both short-term and long-term dependencies of sequential data. 
Besides short-term correlations of signals, there exist some long-term dependencies should be exploited for narrow-band speech enhancement. For example, the convolution model is time invariant in a very long period of time for static speakers. Moreover, Mamba has a linear computational complexity w.r.t time, which is suitable for streaming processing of long audio signals. 
Specifically, one narrow-band block consists of a forward Mamba for online processing and an optional backward Mamba (averaging with forward output) for offline processing. 


\subsubsection{Cross-band block}
\label{sec: arch-cross-band}
The cross-band block is used to learn full-band/cross-band dependencies of signals. In the original SpatialNet \cite{quan2024spatialnet}, the cross-band block is designed for learning the linear relationship of inter-channel features (e.g. inter-channel phase different) across frequencies. In this work, single channel does not have such inter-channel information. Instead, the cross-band block can learn the full-band spectral pattern of signals in the linear/Mel-frequency domain, which is also critical information for (especially single-channel) speech enhancement. The cross-band block processes frames independently along the frequency dimension, and all frames share the same network.

Specifically, we adopt the original cross-band block as in SpatialNet, which is composed of cascaded frequency convolutional layer (F-GConv1d), across-frequency linear layers (F-Linear) and a second frequency convolution layer. The frequency convolutional layers perform 1-D convolution along frequency to learn correlations between adjacent frequencies. The across-frequency linear layer processes all frequencies together to learn full-band dependencies. 
One cross-band block is first applied in the linear-frequency domain to learn detailed full-band correlations. To reduce the model complexity, the hidden dimension $H$ is compressed to a much smaller dimension, such as $H/12$, and then each hidden dimension is independently processed by the across-frequency linear layer with a complexity of $F^2$. After Mel-filterbank, the cross-band blocks learn full-band correlations across Mel frequencies, where the model complexity is largely reduced from $F^2$ to $F_\text{mel}^2$. Correspondingly, the hidden dimension is remained as $H$ (no longer compressed) to reinforce the capability of full-band learning.  
We set all the Mel-frequency cross-band blocks to share the same across-frequency linear layers.

\subsection{Back-ends}
\label{sec: model-backends}

At the inference stage, CleanMel is followed by either an ASR model or a neural vocoder. The ASR model and neural vocoder are separately trained with the CleanMel network. 

\subsubsection{ASR}
At inference, the enhanced logMel-spectrogram is directly fed to an already-trained ASR system, without performing any fine-tuning or joint-training.
Different ASR systems may have different configurations in STFT settings, number of mel frequencies and base of logarithm. To seamlessly integrate the enhanced logMel-spectrogram into one ASR model, our CleanMel would adopt the same configurations as the ASR model.   
The training cost of CleanMel is not very high, so it can be easily re-trained for a new ASR system, especially for those large-scale ASR systems and already-deployed ASR systems. 
In this work, we only conduct offline ASR combined with offline CleanMel.

\subsubsection{Neural vocoder}
\label{sec: model-vocos}
The vocoder we adopt in this work is Vocos \cite{siuzdak2024vocos}, a recently proposed  Generative Adversarial Network (GAN)-based neural vocoder. The generator of Vocos predicts the STFT coefficients of speech at frame level and then generates waveform through inverse STFT. Vocos uses the multiple-discriminators and multiple-losses proposed in HiFi-GAN \cite{kong2020hifi}, but it significantly improves the computational efficiency compared to HiFi-GAN that directly generates waveform at sample level. In this work, to unify the front-end and back-end processing, we have made several necessary modifications to Vocos as follows:
\begin{itemize}
\item The magnitude-based Mel-spectrogram of original Vocos is modified as power-based to be consistent with the front-end and ASR models, where the two cases were shown to achieve similar performance in our preliminary experiments. 
\item The sampling rate of signals, the STFT configurations and the number of Mel-frequencies of the Vocos are adjusted according to the setup of front-end and ASR models. 
\item The original Vocos is designed for offline processing, as it employs non-causal convolution layers. To enable online processing, we modified Vocos to be causal by substituting each non-causal convolution layer with its causal version. 
% As far as we know, this is the first time to try online neural vocoder in the filed. 
The online vocoder still performs quite well. In addition, to reduce the computational complexity of online processing, the 75\% STFT overlap of original Vocos is reduced to be 50\% overlap, which still achieves comparable performance.
\end{itemize}
The offline and online Vocos are used to work with the offline and online CleanMel, respectively. Vocos models are trained with our direct-path target speech $x(n)$.

\subsection{Signal Normalization}
\label{sec:normalization}
When separate front-end and back-end models are cascaded, signal normalization should be performed not only to facilitate the training of respective models but also to align the signal level of cascaded models. For offline processing, the normalization method used in Vocos is also applied for CleanMel. Specifically, a random gain is applied to time domain signal to ensure that the maximum level of the resulting signal lies within the range of -1 to -6 dBFS. This normalization manner ensures the maximum level of sample values is close to and smaller than 1, thence the generated waveform can be directly played with full volume and without clipping effect. For CleanMel, we apply this normalization to noisy signal and utilize the same gain of noisy signal to the corresponding clean target signal. In this way, the enhanced Mel-spectrogram can be directly fed to Vocos. When applying this time-domain normalization, the clip value $\epsilon$ for computing logMel-spectrogram is set to 1e-5 for both CleanMel and Vocos. As for ASR, ASR models normally have a separate Mel-spectrogram normalization operation, which will be applied to re-normalize the enhanced Mel-spectrogram. 

For online processing, the time domain normalization method is no longer applicable. Instead, an online STFT-domain normalization is used for both CleanMel and Vocos. Specifically, for CleanMel, the noisy and target speech are normalized in the STFT domain as $\tilde{Y}(f,t)=Y(f,t) / \mu(t)$ and $\tilde{X}(f,t)=X(f,t) / \mu(t)$, where $\mu(t)$ is a recursively calculated meanvalue of STFT magnitude of $Y(f,t)$, i.e. $\mu(t)=\alpha \mu(t-1)+(1- \alpha) \frac{1}{F} \sum_{f=0}^{F-1} |Y(f,t)|$. The smoothing weight is set to $\alpha=\frac{K-1} {K+1}$, by which the recursive smoothing is equivalent to using a $K$-long rectangle smoothing window. When training Vocos using target speech signal $x(n)$, we still first apply the time domain normalization mentioned above, then apply an extra online normalization.  Specifically, $\mu(t)$ is computed with and also applied to $X(f,t)$, and then the corresponding logMel-spectrogram is computed as the input of Vocos generator. Accordingly, the output of Vocos generator (before applying inverse STFT) would be an estimation of normalized $X(f,t)$. To go back to the signal level of time domain normalization, the recursive normalization factor $\mu(t)$ is multiplied back to the estimation of normalized $X(f,t)$ and then applying inverse STFT, after which Vocos losses (including Mel loss and discriminator losses) are computed. At inference, online normalization is applied to the noisy input, and the enhanced logMel-spectrogram is directly fed into Vocos.  The recursive normalization factor computed with the noisy input is multiplied to the estimated STFT coefficients by Vocos and then applying inverse STFT to obtain the final waveform which is time-domain normalized and can be directly played. When applying this online normalization, the clip value $\epsilon$ for computing logMel-spectrogram is set to 1e-4 for CleanMel and the input of Vocos generator.