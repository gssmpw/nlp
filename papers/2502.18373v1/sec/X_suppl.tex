\clearpage
% \setcounter{page}{1}
\vspace{-1em}

\section{Data Access}
The MultiEgoView dataset, its structural description, and usage information can be found here: \url{https://siplab.org/projects/EgoSim}. We will release EgoSim's code to facilitate future research and data generation. An overview of EgoSim's rich customization options can be found in Table \ref{tab:egosimfeatures}. An overview of the diversity of our scenes is shown in Figure \ref{fig:example_images}.
\begin{figure}[h]
    \centering
    \caption{An excerpt of our example images from 24 locations across 4 scenes.}
    \begin{tabular}{ccc} % Create a 2-column table for images
        \includegraphics[width=0.25\linewidth]{imgs/dataset/example_images/001.jpg} & \includegraphics[width=0.25\linewidth]{imgs/dataset/example_images/002.jpg} & \includegraphics[width=0.25\linewidth]{imgs/dataset/example_images/img_PosePrior_03101_socket1_0_1716847595942376000.jpg} \\
        \includegraphics[width=0.25\linewidth]{imgs/dataset/example_images/006.jpg} & \includegraphics[width=0.25\linewidth]{imgs/dataset/example_images/007.jpg} & \includegraphics[width=0.25\linewidth]{imgs/dataset/example_images/009.jpg} \\
        \includegraphics[width=0.25\linewidth]{imgs/dataset/example_images/013.jpg} & \includegraphics[width=0.25\linewidth]{imgs/dataset/example_images/015.jpg} & \includegraphics[width=0.25\linewidth]{imgs/dataset/example_images/img_ACCAD_Male1General_c3d_socket1_0_1716851322142031300.jpg} \\
        \includegraphics[width=0.25\linewidth]{imgs/dataset/example_images/img_KIT_6_socket6_0_1708556598159407400.jpg} & \includegraphics[width=0.25\linewidth]{imgs/dataset/example_images/img_KIT_200_socket1_0_1708661944481718700.jpg} & \includegraphics[width=0.25\linewidth]{imgs/dataset/example_images/img_KIT_424_socket1_0_1708591813742400800.jpg} \\

        % Add more rows as needed
    \end{tabular}
    \label{fig:example_images}
\end{figure}


\section{Model Complexity and Ablation Studies}
Our multiview transformer is ViT-based and has 114M trainable parameters and requires roughly 1.7GB VRAM for inference. With an input/output window of up to 5 seconds, the inference time is 17.6ms on an RTX 4090 with a batch size of 1, making the system real time capable. Training with 6 cameras and a batch size of 12 increases VRAM demands to 20GB. 

\subsection{Analysis of Cameras}
Previous work utilized varying numbers of body-worn cameras \cite{shiratori2011motion, yi2023egolocate, liEgoBodyPoseEstimation2023aug, luoDynamicsRegulatedKinematicPolicy2022oct}. 
In our ablation study (Table \ref{tab:cam_setup}), we demonstrate the benefits of using more cameras. In this ablation study, we use scenes (1) and (2) of our dataset. Our multiview transformer achieves the lowest global-MPJPE with six cameras. Even with fast-moving cameras (see Section \ref{sec:joints}) attached to knees and writs, our method accurately recovers body pose, though global translation error increases. Using only head and wrist cameras results in higher pose errors, particularly in leg and foot movements ($0.068/0.106/0.145m$ root-aligned foot position error for the three configurations respectively). 
The use of additional cameras also leads to more pronounced and active motions. The model tends to average poses over sequences rather than capturing rapid movements. This is evidenced by a decreased jerk with fewer cameras, a finding further supported by qualitative analysis.

This highlights the advantage of additional cameras, especially for accurately estimating limb poses, even when attached to fast-moving mounting points. Thus, showing that we do not require cameras on stable positions, e.g. head or pelvis.

\begin{table*}
\centering
\caption{Details of EgoSim's features that allow simulating complex scenarios for body-worn sensors in egocentric settings. These features are especially useful in contexts where data is scarce and data collection poses significant challenges or requires extensive time.}
\label{tab:egosimfeatures}
\begin{tabular}{p{0.25\linewidth} p{0.7\linewidth}} % two columns with specified widths
\toprule
\textbf{EgoSim Feature} & \textbf{Description} \\
\midrule
Avatar skeletal mesh options & Compatible with SMPL \cite{SMPL:2015}, SMPL+H \cite{MANO:SIGGRAPHASIA:2017}, SMPL-X \cite{SMPL-X:2019}, and custom skeletal meshes via the FBX format \\
\midrule
Avatar motion capabilities & Supports MoCap data \cite{amass, bedlam} as well as synthetic motions \\
\midrule
Camera customization & Adjustable image resolution, field of view (FOV), and auto exposure settings including speed, bias, brightness limits, and spring system between body and camera for realistic motion simulation of nonrigid camera mounting \\
\midrule
Image noise and distortion & Customizable noise intensity and horizontal bump distortion \\
\midrule
Environmental settings & Support for various environments including indoor and outdoor settings, diverse weather conditions, and lighting variations based on Unreal Engine \cite{unrealengine} marketplace \\
\midrule
Egocentric and external camera integration & Support for both egocentric cameras attachable to different body parts and stationary external cameras to facilitate third-person perspective captures. \\
\bottomrule
\end{tabular}
\vspace*{0.25cm}
\end{table*}
\begin{table}[t]
    \centering
    \caption{Results of different camera setups. Adding more cameras yields better pose prediction. Training and evaluation were conducted on synthetic scenes (1) and (2).}
    \vspace{1mm}
    
    \begin{tabular}{@{}lrrrrrr@{}}
         &  Global & PA- & & & \\
        cameras &  MPJPE & MPJPE & MTE & MRE & MJAE & Jerk \\
        \midrule
        all six & 0.18 & 0.041 & 0.14 & 0.334 & 9.3 & 21.7 \\
        wrists \& knees & 0.238 & 0.051 & 0.197 & 0.376 & 11.1 & 19.4 \\
        head \& wrists & 0.293 & 0.06 & 0.243 & 0.454 & 11.6 & 14.6 \\
        head & 0.345 & 0.0823 & 0.286 & 0.452 & 14.7 & 0.9 \\
    \end{tabular}
    \label{tab:cam_setup}
\end{table}


\subsection{Analysis of Scenes}
Within the main paper, we investigated the sim-to-real transfer of our model. Here, we investigate the model's ability to transfer its knowledge between scenes. 
Table \ref{tab:scene_transfer} shows that there is a significant rise in pose prediction error when transferring scenes. Interestingly the model is generally able to predict the root pose (low MTE) and lower body pose of the avatar while the arms are badly predicted, as confirmed by a qualitative inspection. The trend is similar when training on just 1 scene and evaluating on the scene (2) and when training on (1) and (2) and evaluating on the very diverse scenes (3) and (4).  \\
This indicates the opportunity for future scene generation to improve the dataset's generalizability. As we will publish EgoSim upon acceptance, future research can tailor the synthetic scenes to achieve maximal performance in the target domain.

\begin{table}[t]
    \centering
    \caption{Results of scene transfer.}
    \vspace{1mm}
    
    \begin{tabular}{@{}llrrrrrr@{}}
         & &  Global & PA- & & & \\
        Train Scene & Eval Scene &  MPJPE & MPJPE & MTE & MRE  \\
        \midrule
        Downtown (1) & Downtown (1) & 0.18 & 0.043 & 0.15 & 0.270 \\
        Downtown (1) & CAB (2) & 0.37 & 0.143 & 0.28 & 0.466 \\
        Synthetic (1) \& (2) & Synthetic (1) \& (2) & 0.18 & 0.041 & 0.14 & 0.334 \\
         
         Synthetic (1) \& (2) & Synthetic (3) \& (4) & 0.42 & 0.148 & 0.34 & 0.47\\
         % all scenes & Synthetic (1) \& (2) & 0.15 & 0.039 & 0.12 & 0.271 \\
    \end{tabular}
    \label{tab:scene_transfer}
\end{table}

\section{Analysis of Camera Positions}\label{sec:joints}
Limb-based cameras, such as those mounted on wrists or knees, experience higher velocities, accelerations, and jerks, making them harder to track and localize. As shown in Table \ref{tab:speeds_multiegoview}, the head and pelvis are the most stable mounting points, with the least movement. In contrast, wrists have the highest average acceleration due to rapid arm movements during activities like walking.
Knees follow slightly behind as they are mostly steady for all standing motions. 
Overall, MultiEgoView offers many body camera positions with varying stability. 

\begin{table}[]
    \centering
    \caption{Statistics about the movements of the real-world data of MultiEgoView. The head and pelvis offer the most stable positions on the body, while wrists and knees experience much higher average accelerations and changes of acceleration.}
    \begin{tabular}{l|ccc}
         Joints & mean velocity ($m/s$) & mean acceleration ($m/s^2$) & mean jerk ($m/s^3$) \\ 
         \hline
         Head & $0.53 \pm 0.12$ & $2.37 \pm 1.80$ & $113.46 \pm 192.24$ \\
         Pelvis & $0.48 \pm 0.12$ & $2.39 \pm 1.77$ & $119.46 \pm 192.24$ \\
         Wrists & $0.83 \pm 0.15$ & $4.95 \pm 2.28$ & $163.4 \pm 241.90$ \\
         Knees & $0.61 \pm 0.12$ & $3.58 \pm 1.69$ & $162.4 \pm 187.18$ \\
    \end{tabular}
    \label{tab:speeds_multiegoview}
\end{table}


\section{Data Recording Procedure}
Participation in the data recording was entirely voluntary. Participants were required to sign a consent form for both the data recording and the subsequent publication of the data. They retained the right to withdraw their consent for recording and publication at any time before or during the data collection process. The names and identities of the participants will remain confidential and undisclosed. As a token of appreciation, participants received a small gift for their involvement in the study.

Upon obtaining informed consent, participants were given a brief overview of the recording procedure and the specific movements required for the study. The recording session commenced from a standardized starting position, followed by a brief calibration process. Cameras were then activated and synchronized by a clap. Participants performed the prescribed movements within a predefined area, executing them in a sequential order. To introduce variability in both position and camera perspectives, participants were instructed to take one to five steps between each movement repetition. 
The recording session concluded with participants returning to the initial starting position. A recording session took on average 10:28 minutes with a standard deviation of 1:50 minutes, up to 3 sessions were recorded per participant. 

\section{Data Annotation}
To gain insights into the semantics of human movement, we manually annotated the real-world recordings following the categories from BABEL \cite{punnakkalBABELBodiesAction2021jun}. 
BABEL densely annotated the majority of the AMASS dataset with action labels. Annotators identified segments and assigned labels to these segments. 
The raw, language-based annotations were then categorized into action categories. Building on the BABEL framework, we included commonly found movements from BABEL in our recordings, see Table \ref{tab:motions}.

Our annotations cover the entire sequence from the starting position to the return to the starting position. 
During the recording, participants performed different movements sequentially, often walking a few steps between motions. These intermediary steps were not annotated as separate segments unless they exceeded a few steps.

Most action classes are featured for around 6 minutes in the dataset. 
Walking is the most prominent as participants often walk between different movements. 
MultiEgoView features a wide coverage of different movements from leg and arm motions to sports activities, making it an ideal resource for evaluating BABEL-based systems on real-life data.

% \begin{table}[h!]
% \centering
% \caption{Overview of different movements in the real-world data of MultiEgoView. MultiEgoView covers a wide range of 35 different movements.}
% \begin{tabular}{ll|ll}
% Motion & Time (min) & Motion & Time (min) \\ \hline
% jump & 8.45 & stretch body left right & 6.81 \\ 
% walk & 31.04 & wave arm & 6.59 \\ 
% a pose & 7.84 & bicep curls & 6.68 \\ 
% kick ball & 7.65 & elbow to opposite knee & 6.49 \\ 
% throw ball & 8.15 & raise left/right arm & 6.55 \\ 
% stand & 10.09 & arms in front of chest & 6.64 \\ 
% dribble ball & 6.86 & squats & 6.25 \\ 
% side steps back and forth & 9.06 & t pose & 9.95 \\ 
% aim with hand & 7.53 & arms over head & 6.23 \\ 
% rotate arms & 7.96 & walk backward & 6.34 \\ 
% move arms to front & 6.31 & balance step feet in one line & 6.91 \\ 
% lunge with arms to the side & 8.27 & pick something up one arm & 7.13 \\ 
% lunge & 5.99 & pick something up both arms & 4.81 \\ 
% punch the air in front & 6.61 & blow kiss & 5.07 \\ 
% walk with extended arms & 6.70 & bow & 4.88 \\ 
% swing tennis racket & 7.36 & crouch down & 5.41 \\ 
% arms to face & 6.23 & jumping jacks & 4.33 \\ 
% stretch arms left and right & 6.03 & \textbf{Total} & 259.75 \\ 
% \end{tabular}

% \label{tab:motions}
% \end{table}

\begin{table}[h!]
\centering
\caption{Overview of different movements in the real-world data of MultiEgoView. MultiEgoView covers a wide range of 35 different movements.}
\begin{tabular}{ll|ll}
Motion & Fraction (\%) & Motion & Fraction (\%) \\ \hline
jump & 3.18 & stretch body left right & 2.56 \\ 
walk & 11.68 & wave arm & 2.48 \\ 
A-pose & 2.95 & bicep curls & 2.51 \\ 
kick ball & 2.88 & elbow to opposite knee & 2.44 \\ 
throw ball & 3.07 & raise left/right arm & 2.46 \\ 
stand & 3.80 & arms in front of chest & 2.50 \\ 
dribble ball & 2.58 & squats & 2.35 \\ 
side steps & 3.41 & T-pose & 3.75 \\ 
aim with hand & 2.84 & arms over head & 2.34 \\ 
rotate arms & 3.0 & walk backward & 2.38 \\ 
move arms to front & 2.38 & balance step feet in one line & 2.60 \\ 
lunge with arms to the side & 3.11 & pick something up one arm & 2.69 \\ 
lunge & 2.26 & pick something up both arms & 1.81 \\ 
punch the air in front & 2.49 & blow kiss & 1.91 \\ 
walk with extended arms & 2.53 & bow & 1.83 \\ 
swing tennis racket & 2.77 & crouch down & 2.04 \\ 
arms to face & 2.34 & jumping jacks & 1.63 \\ 
stretch arms left and right & 2.27 & \\ 
\end{tabular}
\label{tab:motions}
\end{table}

\section{Ethical Considerations}

EgoSim's high-fidelity simulation of camera footage addresses several ethical implications associated with motion capture, particularly in real-world settings.
Motion capture is afflicted by privacy concerns for recorded individuals, especially given the need for larger-scale capture of representative human data with diverse participants.
Our simulator mitigates this by synthesizing data from realistic avatars whose appearances can be flexibly adjusted while expressing behavior based on actual human motion.
% , which reduces the need for recording real people.
This not only preserves individual privacy but also allows the creation of diverse datasets that include a wide range of ethnic backgrounds, which is crucial for the effective generalization of learned algorithms.
Consequently, our simulator provides a valuable tool for advancing real-world perception inference while respecting ethical considerations.

Body-worn cameras capture extensive environmental details, offering the potential for simultaneous ego-body and environment understanding. Capturing data with more cameras always opens up more opportunities for surveillance. 
In our case, the amount of cameras results in the unintended exposure of individuals in the proximity of the participant to data recording. MultiEgoView's focus is on the ego-body, therefore we minimize the exposure of other people in the dataset by selecting a recording area with a limited number of passersby. Additionally, to protect the privacy of bystanders, we automatically detected and blurred all faces using deface \cite{ORBHDDeface2024jun}. Examples of blurred images are shown in Figure \ref{fig:privacy_mev}.

In conclusion, we have addressed data-related concerns by compensating participants, obtaining signed consent for data recording, preserving privacy through face blurring, and ensuring no personal information is disclosed. We believe our work will not result in any harmful consequences or negative societal impact.
\begin{figure}
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=\linewidth]{imgs/dataset/privacy_blue_1.png}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=\linewidth]{imgs/dataset/privacy_blur_2.png}
\end{subfigure}
\caption{MultiEgoView typically does not feature many people in the field of view. We preserve people's privacy by automatically blurring their faces.}
\label{fig:privacy_mev}
\end{figure}

\section{License, Data Accessibility and Maintenance}
The data including its documentation will be released under the CC BY-NC-SA license and is available at \url{https://siplab.org/projects/EgoSim}. The dataset is composed of PNG images and CSV files, which are in open and widely used formats, ensuring ease of access and usability. Ground truth joint poses of the synthetic data in the SMPL-X format can be obtained via the AMASS website. Detailed explanations on how to read and utilize the dataset are provided on the hosted website. Upon acceptance, the code for EgoSim and our method will be released on GitHub under the GPL-3.0 license. The dataset and code will be hosted on ETH servers, ensuring long-term preservation and availability. The authors confirm that the data was collected consensually and bear all responsibility for any rights violations related to the dataset.


\endinput

\section{Global Position}
The real-world data recording was conducted using a 17 IMU full-body Xsens motion capture suit. Although this system offers precise pose and position estimations, its global positioning can exhibit some drift due to the inherent limitations of IMU-based systems. Recording sessions ranged from 6:58 to 13:40 minutes in duration (mean = 10:28 minutes, SD = 1:50 minutes). Participants consistently started and concluded the recording at the same location.

The mean drift in global translation observed across sessions varied from 0.2 to 3.6 meters (mean = 1.44 meters, SD = 0.99 meters). This quantifies the drift and demonstrates that, while present, it has a limited impact on the overall data. The average deviation of the system from the starting position across entire sessions was only 1.44 meters.

These results indicate that our ground truth measurements are highly reliable. While not entirely free from error, the observed drift is minimal and does not significantly affect the validity of the data. Thus, the measurements can be treated as nearly perfect for practical purposes.

Participants were allowed to move within an area during the recording session. Thus, the recorded movements show natural patterns. An example trajectory is visualized in Figure \ref{fig:movement_trajectory}.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{imgs/trajectory.png}
    \caption{Participants were able to move freely in a big area leading to natural movements. The drift of our recording system is minimal.}
    \label{fig:movement_trajectory}
\end{figure}

% \section{Rationale}
% \label{sec:rationale}
% % 
% Having the supplementary compiled together with the main paper means that:
% % 
% \begin{itemize}
% \item The supplementary can back-reference sections of the main paper, for example, we can refer to \cref{sec:intro};
% \item The main paper can forward reference sub-sections within the supplementary explicitly (e.g. referring to a particular experiment); 
% \item When submitted to arXiv, the supplementary will already included at the end of the paper.
% \end{itemize}
% % 
% To split the supplementary pages from the main paper, you can use \href{https://support.apple.com/en-ca/guide/preview/prvw11793/mac#:~:text=Delete%20a%20page%20from%20a,or%20choose%20Edit%20%3E%20Delete).}{Preview (on macOS)}, \href{https://www.adobe.com/acrobat/how-to/delete-pages-from-pdf.html#:~:text=Choose%20%E2%80%9CTools%E2%80%9D%20%3E%20%E2%80%9COrganize,or%20pages%20from%20the%20file.}{Adobe Acrobat} (on all OSs), as well as \href{https://superuser.com/questions/517986/is-it-possible-to-delete-some-pages-of-a-pdf-document}{command line tools}.



\section{Comparison between EgoSim and existing synthetic datasets of humans}
Existing synthetic datasets of humans could be classified into two primary categories: those supporting third-person camera perspectives and those favoring egocentric views. Notable datasets for third-person perspectives include AGORA \cite{patel2021agora}, BEDLAM \cite{bedlam}, and SynBody \cite{yang2023synbody}. AGORA \cite{patel2021agora} and BEDLAM \cite{bedlam} are limited to single-camera setups, whereas SynBody \cite{yang2023synbody} incorporates four cameras, making it particularly useful for multi-view tasks. However, these datasets are not suitable for egocentric scenarios where the camera moves with the wearer, introducing unique challenges. This movement varies depending on the body part where the camera is mounted and the wearer's motion, aspects not covered in these datasets. Furthermore, occlusions caused by the camera wearer's body parts entering the field of view are another element not addressed in third-person view datasets.

In contrast, the second category, comprising egocentric datasets like xR-EgoPose \cite{tome2019xr} and UnrealEgo \cite{unrealengine}, features head-mounted cameras. xR-EgoPose \cite{tome2019xr} utilizes a fisheye camera focused downwards to capture the wearer, offering a limited view of the surrounding environment. UnrealEgo \cite{unrealengine} similarly employs a stereo fisheye camera with a downward orientation, focusing primarily on the wearer.

EgoSim stands out in the realm of synthetic datasets by offering the ability to simulate scenarios where cameras can be attached to any body part, capturing either the wearer or the environment. Unlike other egocentric synthetic datasets, EgoSim does not limit its focus to single characters and uniquely facilitates the simulation of multiple cameras mounted on various body parts of multiple avatars. A groundbreaking aspect of EgoSim is its physical simulation of camera movement, crucial for adapting to real-world scenarios where the camera's attachment to the body is often loose, depending on the body-worn device and its mounting position.

Moreover, EgoSim introduces IMU simulation, a capability not previously available in other synthetic datasets. This advancement allows for the exploration of multi-sensor tasks prevalent in real-world scenarios. More detailed comparison between EgoSim and other synthetic datasets is provided in \Cref{table:1}.

%amass
%example of egocentrc

\begin{table*}[t]

            \centering

            \resizebox{\textwidth}{!}{

            \begin{booktabs}{colspec = {cccccccc}, cell{1}{2}={c=4}{c}, cell{1}{6}={c=2}{c}, row{Z} = {LightGray}}

                        \toprule

                        & Modality & &  & & Camera Perspective & \\

                        \cmidrule[lr]{2-5} \cmidrule[lr]{6-7}

                         & RGB & Depth  & Normal & Segmentation & Third-person & Egocentric & \#Avatars & \#Cameras & IMU Support & Camera Placement & Camera Movement \\

                        \midrule

                        AGORA \cite{patel2021agora} & \checkmark & - & - & - & \checkmark & - & Multiple & \centering 1 & \centering - & \centering Environment & \centering Rigid\\
                        
                        BEDLAM \cite{bedlam} & \checkmark & \checkmark & - & \checkmark & \checkmark & - & Multiple & \centering 1 &\centering - & \centering Environment & \centering Rigid\\
                        SynBody \cite{yang2023synbody} & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & - & Multiple & \centering Multiple & \centering - &\centering Environment & \centering Rigid\\
                        \midrule
                        xR-EgoPose \cite{tome2019xr} & \checkmark & \checkmark & - & -& - & \checkmark & Single & \centering 1 & \centering - & \centering Head & \centering Rigid\\                           

                        UnrealEgo \cite{akada2022unrealego} & \checkmark & \checkmark & - & - & - & \checkmark & Single & \centering 2 &  \centering - & \centering Head & \centering Rigid\\                     

                         \textbf{EgoSim}  & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & Multiple & \centering Multiple & \centering \checkmark & \centering Any body part & \centering Physical simulation\\

                        \bottomrule

            \end{booktabs}

            } 

                \caption{Comparison between EgoSim and existing synthetic dataests of humans.}              

                \label{table:1}

\end{table*}


\section{Details of Ego Dataset}

\begin{table}[t]
            \centering
            \resizebox{\columnwidth}{!}{
            % {
            \begin{booktabs}{colspec = {lcccc}, row{Z}={LightGray}}
            \toprule
            Dataset & Capture Location & Camera Location & \#Views & Size  \\
            \midrule
            Panoptic Studio \cite{panoptic} & Indoor & Environment & 480 & 736K\\
            \midrule
            You2Me \cite{ng2020you2me} & Indoor& Head & 1& 150K\\
            HPS \cite{guzov2021human} & Indoor& Head & 1& 320K\\
            EgoBody \cite{zhang2022egobody} & Indoor& Head & 1 & 374K\\
            EgoHumans \cite{khirodkar2023egohumans} & In/Outdoor& Head & 4 & 410K\\
            \textbf{Ego Dataset} & In/Outdoor& Head and Pelvis & Up to 10& 740K\\
            \bottomrule
            \end{booktabs}
            }
            \caption{Comparison between Ego Dataset and other existing egocentric datasets.}
            \label{tab:2}
\end{table}
We utilized EgoSim to generate the Ego Dataset, a unique large-scale egocentric dataset characterized by renderings from cameras mounted on both the head and pelvis of multiple avatars. This positioning distinguishes the Ego Dataset from other egocentric datasets listed in \Cref{tab:2}, which primarily feature head-mounted cameras. The Ego Dataset further differentiates itself by offering a wide range of settings, including both indoor and outdoor environments. Its motion data is derived from real-world Motion Capture (MoCap) datasets, ensuring a comprehensive variety of movements. Additional specifics regarding the dataset's generation are provided below.

\paragraph{Body shapes and motions}
To achieve realistic avatar motion simulation, we employ MoCap data, specifically using animation files from \cite{bedlam}. This guarantees a diversity of motions and body shapes for the avatars. Moreover, to create longer motion sequences, we concatenate animations for each avatar. This is particularly beneficial for future research applications such as tracking, where longer data sequences are crucial. The concatenation process involves aligning the root positions of two consecutive motions. Specifically, we match the last frame of one animation with the first frame of the next. To ensure seamless transitions between consecutive motions, we generate $10$ intermediate frames using Spherical Linear Interpolation (slerp) \cite{slerp}, thereby facilitating smooth and continuous motion sequences.
\paragraph{Textures and environments}
The avatar textures are obtained from the open-source assets provided by \cite{bedlam}. This ensures a diverse range of skin tones and cloth textures for the avatars, contributing to the realism and variety in our simulations. Additionally, the environments we use are sourced from open-source 3D environments available in the Unreal Engine marketplace \cite{unrealengine}. This integration allows for a wide selection of realistic settings, enhancing the overall quality and applicability of our dataset. 
\paragraph{Avatar placement}
To ensure optimal placement of avatars within the environment while avoiding collisions, we utilize a bin packing algorithm \cite{binpacking}.  
\section{Additional visual examples of EgoSim features}
\subsection{Motion blur}
EgoSim supports high frame rate rendering, essential for generating realistic motion blur. An example is shown in \Cref{fig:blur}.
\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{imgs/375.png}
    \caption{Example of blurred images generated by EgoSim. We first render sequence with $3000$ FPS and then average each $100$ frames resulting in $30$ FPS videos with realistic motion blur.}
    \label{fig:blur}
\end{figure}
\subsection{Camera parameters}
EgoSim offers support for configurable image resolution and lens field of view (FOV). An illustration of varying FOV settings can be seen in \Cref{fig:FOV}. Additionally, EgoSim supports adjustable camera noise. An example is shown in \Cref{fig:noise}.
\begin{figure}[ht]
\centering
% First row with two figures
\begin{subfigure}[b]{0.49\linewidth}
    \includegraphics[width=\linewidth]{imgs/FOV/70.png}
    \caption*{FOV $=70^{\circ}$}
    \label{fig:image1}
\end{subfigure}
\hfill % This will add some space between the two figures
\begin{subfigure}[b]{0.49\columnwidth}
    \includegraphics[width=\columnwidth]{imgs/FOV/90.png}
    \caption*{FOV $=90^{\circ}$}
    \label{fig:image2}
\end{subfigure}

% Second row with two figures
\begin{subfigure}[b]{0.49\columnwidth}
    \includegraphics[width=\columnwidth]{imgs/FOV/130.png}
    \caption*{FOV $=130^{\circ}$}
    \label{fig:image3}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.49\columnwidth}
    \includegraphics[width=\columnwidth]{imgs/FOV/150.png}
    \caption*{FOV $=150^{\circ}$}
    \label{fig:image4}
\end{subfigure}

\caption{Example of different lens field of view simulated by EgoSim}
\label{fig:FOV}
\end{figure}

\begin{figure}[ht]
\centering
% First row with two figures
\begin{subfigure}[b]{0.49\linewidth}
    \includegraphics[width=\linewidth]{imgs/FOV/110.png}
    \caption*{Without noise}
    \label{fig:image1}
\end{subfigure}
\hfill % This will add some space between the two figures
\begin{subfigure}[b]{0.49\columnwidth}
    \includegraphics[width=\columnwidth]{imgs/FOV/img_Char1_socket1_0_1700916815167696000.png}
    \caption*{With noise}
    \label{fig:image2}
\end{subfigure}

\caption{Example of image noise simulated by EgoSim}
\label{fig:noise}
\end{figure}

\subsection{IMU simulation}
EgoSim includes IMU simulation capabilities. \Cref{fig:imuacc} and \Cref{fig:imuang} illustrate examples of simulated linear acceleration and angular velocity, respectively.

\begin{figure*}[t!]
    \centering
    \begin{minipage}{0.45\textwidth}
        \includegraphics[width=\columnwidth]{imgs/IMU/acc.pdf}
    \caption{An example of simulated linear acceleration.}
    \label{fig:imuacc}
    \end{minipage}
    \begin{minipage}{0.45\textwidth}
        \includegraphics[width=\columnwidth]{imgs/IMU/ang_v.pdf}
    \caption{An example of simulated angular velocity.}
    \label{fig:imuang}
    \end{minipage}
\end{figure*}

\section{Author Statement}
The authors bear all responsibility in case of violation of rights.



% \begin{figure}[t]
%     \centering
%     \includegraphics[width=\columnwidth]{imgs/IMU/acc.pdf}
%     \caption{Example of simulated linear acceleration.}
%     \label{fig:imuacc}
% \end{figure}
% \begin{figure}[t]
%     \centering
%     \includegraphics[width=\columnwidth]{imgs/IMU/ang_v.pdf}
%     \caption{Example of simulated angular velocity}
%     \label{fig:imuang}
% \end{figure}



