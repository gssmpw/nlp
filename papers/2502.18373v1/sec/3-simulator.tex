\begin{figure}[b]
  %
    \centering
    \includegraphics[width=\textwidth]{imgs/renders-ch.pdf}
    \caption{EgoSim renders multiple modalities:
    (a)~RGB, (b)~depth, (c)~normals, (d)~semantic labels.}
    \label{fig:multimodal}
\end{figure}



\section{EgoSim Simulation Platform}
\label{sec:simulator}

\begin{table}[]
    \centering
    \caption{Comparison of previous datasets for egocentric 3D human pose estimation.}
    \vspace{1mm}
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{rccccccccc}
        Dataset & Mo2Cap2 \cite{xuMo2Cap2RealtimeMobile2019jan} & xR-EgoPose \cite{tome2019xr} & EgoCap \cite{rhodin2016egocap} & EgoGlass \cite{zhao2021egoglass} & UnrealEgo \cite{akada2022unrealego} & EgoBody \cite{zhang2022egobody} & ARES \cite{liEgoBodyPoseEstimation2023aug} & MultiEgoView (ours)\\ 
        \midrule
        Head camera type &  fisheye & fisheye & wide stereo & stereo & fisheye stereo & front facing & fisheye stereo & front facing \\
        sees body & \tick & \tick & \tick & \tick & \tick & \cross & \cross &partly\\
        Hand camera & \cross & \cross & \cross & \cross & \cross & \cross & \cross &$2\times$\\
        Leg camera & \cross & \cross & \cross & \cross & \cross & \cross & \cross & $2\times$\\
        Pelvis camera & \cross & \cross & \cross & \cross & \cross & \cross & \cross & \tick \\
        Image generation & composite & Maya & composite & real & Unreal Engine & real & Replica & Unreal Engine\\
        Image quality & low & high & real & green screen & high & real & high & high\\
        Environment & In- \& Outdoor & Mostly Indoor & Indoor & Indoor & In- \& Outdoor & Indoor & Indoor & Outdoor\\
        Dataset Size & $530k$ & $383k$ & $2 \times 41$k & $2 \times 170$k & $2 \times 450$k & $220k$ & $1.2M$ & $6 \times 12.9M$\\
        Real data & \cross & \cross & \tick & \tick & \cross & \tick & \cross & \tick $6\times520k$ \\
        Motion Diversity & mid & low & low & low & high & high & high & high \\
    \end{tabular}
    }
    \label{tab:dataset_comparison}
\end{table}



EgoSim is designed for body-worn camera simulation.
We extend Microsoft's AirSim simulator~\cite{shah2018airsim} integrated within the Unreal Engine~\cite{unrealengine} to leverage its flexibility and realistic output renders (e.g.,~\cite{bedlam, yang2023synbody}).
Specifically, we augment the platform with the capability of simulating \emph{body-worn} cameras during realistic human motion, generating dynamic changes in camera motion that correspond to a person's movements, including potentially irregular, rough, and non-smooth moments.

% We provide the details of the features supported by EgoSim below. These features are essential for both efficiency and user-friendliness of EgoSim in simulating wearable sensors.

% EgoSim incorporates the following key features:

\paragraph{Simulating images.}
EgoSim renders footage through Unreal Engine's cinematic camera~\cite{pueyo2020cinemairsim} for realistic images.
The camera model and noise parameters are adjustable. 
EgoSim supports simultaneously rendering multiple modalities (Figure~\ref{fig:multimodal}), including RGB, depth, normal maps, and semantic segmentation masks.
These modalities are complementary and can serve as input to various computer vision tasks in the future.

\paragraph{Simulating physical attachment and motion artifacts.}
A key feature of EgoSim is the consideration of camera attachment to account for motion artifacts during simulation.
Since body-worn cameras are non-rigidly mounted, often coupled to clothing or strapped to the limbs like a smartwatch, the loose attachments can lead to slip and drag in the camera's position and orientation.
EgoSim simulates these using \href{https://dev.epicgames.com/documentation/en-us/unreal-engine/using-spring-arm-components-in-unreal-engine}{spring arm} mounts that connect the avatar's body and the virtual cameras. 
We demonstrate that spring-damper systems as a camera mounting model help to realistically capture the effects of loose camera attachment as found in the real world (Section~\ref{sec:spring_damper}).

% A key feature of EgoSim is the addition of a realistic camera attachment model.
% Since body-worn cameras are non-rigidly mounted, often coupled to clothing or strapped to the limbs like a smartwatch, the loose attachments can lead to slip and drag in the cameras' positions.
% % relative location may slip and their movement relative to the body site is additionally affected by drag, notably during looser attachment.
% EgoSim, therefore, dynamically simulates the camera positions and drag motions using a physical spring controller that connects the avatar bodies and the cameras.
% We model the dynamics of this connection using a mass-spring-damper system~\cite{Thornton2003-of} as
% $m\ddot{x} + k_v \dot{x} + k_s x = 0$.
% The spring stiffness $k_s$ and damping factor $k_v$ can be adjusted to support the simulation of diverse sensor attachment scenarios. 

\paragraph{Simulating diverse environments.}
EgoSim benefits from the vast selection of indoor and outdoor environments available in Unreal and previous work, e.g., \cite{bedlam}. As shown in Figure~\ref{fig:overall}, it can render both, large, realistic hand-modeled scenes and scanned scenes that closely resemble their real-world counterparts. The used scenes are in wide open spaces where motion capture is traditionally hard to perform.
Additional details about EgoSim's features are provided in the appendix Table \ref{tab:egosimfeatures}.
% Table~\ref{tab:egosimfeatures}. 

% \paragraph{Simulating Inertial Measurement Units (IMU).}
% To complement camera recordings and serve IMU-based pose estimation methods or combinations thereof, EgoSim can additionally synthesizes 3D acceleration and 3D gyroscope signals for each sensor.
% At each time step $t$, the IMU's angular velocity $\omega_t$ and linear acceleration $a$ are estimated as
% $\omega_{t} = \frac{\theta_t - \theta_{t-1}}{dt} + \eta_\omega  \quad$, and $\quad a_{t} = \frac{x_{t-1}+x_{t+1}-2 x_{t}}{dt^2} - g + \eta_a$.
% Here, $\theta$ is virtual IMU's angular rotation, $x$ is its position, $dt$ is the time interval between two consecutive measurements, and $g$ is the gravitational acceleration constant.
% In addition, $\eta_\omega \sim \mathcal{N}(0, r_{\omega}I)$ and $\eta_\omega \sim \mathcal{N}(0, r_{a}I)$ are white noise variables with adjustable hyperparameters $r_{\omega}$ and $r_{a}$ representing measurement errors for angular velocity and linear acceleration. 


\paragraph{Synchronizing multi-sensor and multi-person recordings.}
Synchronizing multiple cameras poses challenges in real-world recordings, yet it is straightforward to generate synchronized multi-modal data in EgoSim while obtaining ground-truth characteristics of the environment or avatars.
In addition EgoSim is capable of simulating and rendering data from \emph{across} multiple avatars and to obtain corresponding ground-truth poses and camera positions.
EgoSim supports a flexible number of sensors, sensor characteristics, and attachment locations---independently for each avatar. 
