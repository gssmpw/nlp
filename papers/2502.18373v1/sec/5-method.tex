\section{Baseline Method: Wearable Multi-Camera Body Pose Estimation}\label{sec:method}

\label{sec:pose_estimation}
To demonstrate the benefits of \dataset{}, we trained a neural network to estimate 3D ego body poses using multiple body-worn cameras. The input to the network consists of the aligned video sequences \( \bm{X} \in [0,1]^{C \times F \times 3 \times H \times W} \), with \( F \) frames from \( C \) body-attached cameras. Based on these inputs, the network predicts a pose \( \bm{\hat{p}}_i \) for each input frame $i$.

\subsection{Network architecture}
Our network is a Vision Transformer Model based on Sparse Video Tube ViTs~\cite{piergiovanni2023rethinking}.
We extract feature vectors from each input video using a sparse view tokenizer SVT with a shared interpolated kernel.
The extracted feature vectors from the sparse tube tokenizers are then added to their fixed spatio-temporal position encoding \( \bm{\kappa}_{p} \) and their learnable view encoding \( \bm{\kappa}_{v,c} \) per camera $c$.

\begin{equation}
    \bm{v}_{c} = \mathrm{SVT}(\bm{X}_c, \bm{W}) + \bm{\kappa}_{p} + \bm{\kappa}_{v,c}, \quad \text{ where } \bm{W} \text{ are the shared weights of the kernel.}
\end{equation}
The resulting feature vectors for the different cameras $\bm{v}^c$ are concatenated with the pose token $\bm{\phi}_{j} = \bm{\tau}(j) + \bm{\psi}, j \in[0, F-1]$, where $\bm{\psi}$ is a trainable pose token and $\bm{\tau}$ is a sinusoidal positional encoding.
The resulting token sequence is then processed using a Vision Transformer Encoder.
\begin{equation}
    \{\bm{z}_{0}, \ldots, \bm{z}_{F-1}\} = \mathrm{ViT}(\text{concat}(\bm{\phi}_0, ..., \bm{\phi}_{F-1}, \bm{v}_{0}, ..., \bm{v}_{c-1}))
\end{equation}
Based on each embedded pose token $\bm{z}$, we obtain the 6D representation \cite{zhouContinuityRotationRepresentations2019jun} of the SMPL pose parameters $\bm{\theta}$, the 6D relative rotation $\bm{R}_{r}$, and 3D relative translation of the root $\bm{t}{r}$ with respect the previous frame.
\begin{equation}
\hat{\bm{\theta}} = W_{\theta}\bm{z}, \quad \hat{\bm{R}}_r = W_{R}\bm{z}, \quad  \hat{\bm{t}}_{r} = W_{t}\bm{z}
\end{equation}

To improve generalization, the network is trained to predict the pose difference, i.e., the relative root pose with respect to the previous pose, instead of directly predicting global root poses.

Using Forward Kinematics, we obtain the global body pose $\bm{p}$ with respect to the starting pose.
\begin{equation}
    \{\bm{\hat{p}}_{0}, \ldots, \bm{\hat{p}}_{F-1}\} = \mathrm{FK}_{\theta}(\bm{\theta}, \hat{\bm{R}}_{g}, \hat{\bm{t}}_{g}, \beta), \text{where}  \quad\hat{\bm{R}}_{g}, \hat{\bm{t}}_{g} = \mathrm{FK}_{g}(\hat{\bm{R}}_{r}, \hat{\bm{t}}_{r}) 
\end{equation}
Where $\beta$ are the shape parameters of the SMPL-X model \cite{SMPL-X:2019} for a given person.

We use 4 tubes with the following configurations: \(16 \times 16 \times 16\) with stride \((12, 48, 48)\) and offset \((0, 0, 0)\), \(24 \times 6 \times 6\) with stride \((12, 32, 32)\) and offset \((8, 12, 12)\), \(12 \times 24 \times 24\) with stride \((24, 48, 48)\) and offset \((0, 28, 28)\), and \(1 \times 32 \times 32\) with stride \((12, 64, 64)\) and offset \((0, 0, 0)\). The pose embedding parameter is initialized using the Kaiming uniform distribution~\cite{heDelvingDeepRectifiers2015feb}, and the pose token is initialized using the Normal distribution.

\subsection{Loss function}
We supervise the network with the following loss function:
\begin{equation}
    \mathcal{L} = \lambda_{\theta}\mathcal{L}_{\theta} +  \lambda_{p}\mathcal{L}_{p} +  \lambda_{v}\mathcal{L}_{v} +  \lambda_{t_{r}}\mathcal{L}_{t_{r}} +  \lambda_{R_{r}}\mathcal{L}_{R_{r}} +  \lambda_{t_{g}}\mathcal{L}_{t_{g}} + \lambda_{R_{g}}\mathcal{L}_{R_{g}}  + \lambda_{z}\mathcal{L}_{z}
\end{equation}

The angle loss $\mathcal{L}_{\theta}$ encourages the model to learn the SMPL angles $\bm{\theta}$, while the joint position loss $\mathcal{L}_{p}$ forces the predicted joint positions through forward kinematics to be close to the ground-truth joint positions.
This way, both the local and the accumulated errors are considered.
\begin{equation}
    \mathcal{L}_{\theta} = |\bm{\theta}_{\text{6D}}- \bm{\hat{\theta}}_{\text{6D}}|_{1}\quad \textrm{and} \quad \mathcal{L}_{p} = |\bm{p} - \hat{\bm{p}}|_{1},
\end{equation}
where \textit{6D} indicates the six-dimensional representation of the rotation matrices~\cite{zhouContinuityRotationRepresentations2019jun}. 
For the root pose, we penalize both the relative and absolute translation and orientation error accumulated through the kinematic chain, 
\begin{equation}
\begin{split}
    \mathcal{L}_{R_{r}} = |\bm{R}_{r, \text{6D}} - \bm{\hat{R}}_{r, \text{6D}}|_{1}\quad \textrm{and} \quad \mathcal{L}_{t_{r}} = |\bm{t}_{r} - \bm{\hat{t}}_{r}|_{1} \\
    \mathcal{L}_{R_{g}} = \lVert \hat{\bm{R}}_g\bm{R}^{-1}_{g} - I \rVert_{2} \quad \textrm{and} \quad \mathcal{L}_{t_{g}} = |\bm{t}_{g} - \hat{\bm{t}}_{g}|_{1}
\end{split}
\end{equation}

To encourage the model to estimate more expressive motions accurately, we add a velocity loss $\mathcal{L}_{v}$.
We also regularize the embedded pose token $\bm{z}$ using an $l_2$-regularization term $\mathcal{L}_{z}$.
\begin{equation}
    \mathcal{L}_v = | (\bm{p}_i - \bm{p}_{i-1}) - (\hat{\bm{p}}_i - \hat{\bm{p}}_{i-1})|_1 \quad \text{and} \quad \mathcal{L}_{z} = \lVert \bm{z}\rVert_{2}
\end{equation}

We set $\lambda_{\theta}=10$,  $\lambda_{p}=25$,  $\lambda_{v}=40$,  $\lambda_{t_r}=25$, $\lambda_{R_r}=15$, $\lambda_{t_g}=1$, $\lambda_{R_g}=0.025$, and $\lambda_{z}=0.0005$.