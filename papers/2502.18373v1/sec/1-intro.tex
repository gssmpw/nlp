\section{Introduction}
\label{sec:intro}


% The structure of the intro to me would seem to be (2+3 could perhaps be combined)

% 1. paragraph: Multi camera research is super important, synchronized cameras, great tasks and progress
% 2. paragraph: Increasingly mobile applications exist that require computer vision methods. Popular and there's much research on it, but often only one camera and thus limited tasks.
% 3. paragraph: people are starting collect wearable footage (Aria, wearable go pros) to tackle these tasks. some become possible through this, such as x, y, z
% 4. paragraph: however, this is laborious and some of the devices are not widely available, which hinders progress. in fact, tasks like a, b, c require large datasets of long scenarios in outside environments, which is near impossible to do manually
% 5. we propose ... rough overview
% 6. some more features (optional, perhaps integrated with 5)
% 7. we demonstrate EgoSim's capabilities in X tasks
% 8. first task: quick description + results
% 9. second task: ...
% 10. Collectively, our contributions are...
% This could also list other downstream tasks that this would be suitable for.

% The collaborative potential of multiple sensors opens up new dimensions for enhancing our interactions and understanding of both people and our surroundings. For instance, combining multiple wearable camera data enables global human pose estimation of people we interact with, providing a deeper insight into their body language and gestures. This facilitates better communication and enhances our ability to interpret social cues in diverse settings. Furthermore, the fusion of sensor information can significantly contribute to better environmental awareness. This is particularly valuable in tasks like navigation, where real-time data from various sensors aids in creating comprehensive and accurate spatial maps. 


The newest generation of AI-based personal devices evidently requires an understanding of the world from a user's perspective to provide meaningful context.
For example, Meta's Ray-Ban glasses~\cite{metaglass}, Hu.ma.ne AI pin~\cite{aipin}, or the glasses demoed at Google I/O 2024 all share the wearer's perspective to analyze their surroundings.
Such emerging devices in addition to existing immersive Mixed Reality platforms have further spurred research efforts on egocentric perception tasks~\cite{zhang2022egobody, khirodkar2023egohumans}.
% cameras in the form factor of a virtual reality headset or augmented reality glasses have obvious benefits in terms of being aligned with users' vision and are thus currently driven by industry,

\begin{figure}
    \centering
    % \includegraphics[width=8cm]{imgs/dataset/rayan_data_rec.drawio.pdf}
    % \includegraphics[width=\textwidth]{imgs/teaseregosim.pdf}
    \includegraphics[width=\textwidth]{imgs/teaser-ch.pdf}
    \caption{(Left)~Our dataset \emph{MultiEgoView} contains 5\,hours of egocentric real-world footage from 6~body-worn GoPro cameras and ground-truth 3D body poses from an Xsens motion capture suit as well as 119\,hours of simulated footage in high-fidelity virtual environments on the basis of real motion capture data and associated 3D body poses.
    (Right)~Our method estimates ego poses from video data alone, here visualized inside the scanned 3D scene.}
    \label{fig:teaser}
\end{figure}


While head-worn cameras have primarily been used for localization~\cite{liEgoBodyPoseEstimation2023aug,yi2023egolocate}, they are ideally positioned to simultaneously capture the wearer's arm motions, for example, to estimate upper body poses~\cite{ohkawa2023assemblyhands, jiang2024egoposer, ahuja2021coolmoves} or detect user input from hand poses and actions~\cite{xiao2018mrtouch, streli2023structured}. 
For egocentric pose estimation, previous work has commonly used head-mounted fisheye cameras pointing down~\cite{xuMo2Cap2RealtimeMobile2019jan, tome2019xr, wangSceneAwareEgocentric3D2023jun}, which can capture much of the upper body.
This promise has spurred interest in egocentric pose estimation, for which several real~\cite{zhang2022egobody, liEgoBodyPoseEstimation2023aug, wangSceneAwareEgocentric3D2023jun, akada2022unrealego, guzovHumanPOSEitioningSystem2021juna, grauman2022ego4d, graumanEgoExo4DUnderstandingSkilled2024apr,xuMo2Cap2RealtimeMobile2019jan, tome2019xr} and synthetic~\cite{liEgoGenEgocentricSynthetic2024apr,akada2022unrealego,tome2019xr} datasets have been collected. The advantage of synthetic data has been demonstrated for simultaneous localization and mapping (SLAM~\cite{tartanair, wang2020synthetic,rukhovich2019estimation}), 3D reconstruction~\cite{straub2019replica, lin2022capturing}, and human mesh recovery~(HMR~\cite{patel2021agora, bedlam, yang2023synbody}). %, demonstrating state-of-the-art results on real images when exclusively trained on synthetic data~\cite{bedlam}.


For more comprehensive capture of body motion, prior work has used motion-capture suits~\cite{trumble2017total} or individual body-worn motion sensors~\cite{huang2018deep, yi2021transpose, yi2022physical, armani2024ultra}, where learned methods predict 3D body poses from up to a set of inertial sensors as input.
These sensor ensembles provide rich information about the various limb motions and enable fine-grained pose estimation.
However, estimates from motion sensors alone suffer from drift and struggle with tracking global positions, for which previous work has added head-worn cameras~\cite{yi2023egolocate,guzovHumanPOSEitioningSystem2021juna, jiang2022avatarposer, jiang2024manikin} to complement inertial motion cues.


Considering the ongoing miniaturization of camera technology, there is promise in further augmenting on-body tracking methods with camera sensors, for example, to remove the occlusion of lower body parts and extend the coverage of the environment~\cite{wangSceneAwareEgocentric3D2023jun}. 
Indeed, Shiratori et al.'s pioneering effort to track 3D body poses in the wild from multiple body-worn cameras in 2011 predates many learning-based methods~\cite{shiratori2011motion} and demonstrated the potential of the richer modality that is videos for human motion tasks.
In addition, body-worn cameras, such as those on the wrists~\cite{kim2012digits,maekawaWristSenseWristwornSensor2012mar,ohnishi2016recognizing,li2020mobile} or legs~\cite{shiratori2011motion} benefit from their proximity to the point of interest during human activity or hand-object interaction. 
The use of multiple cameras mitigates the effect of occlusion and provides multiple vantage points of the ego-body, surrounding people, and the environment.
Extensive research on integrating multi-view data (e.g.,~\cite{cleveland2006principles, furukawa2015multi, wang2021multi}), albeit typically from static third-person perspectives, has shown benefits for navigation~\cite{bonin2008visual}, 3D reconstruction~\cite{goesele2007multi}, and pose estimation~\cite{tu2020voxelpose, MVP}. 
% capability to compensate for the scarcity of real-world data.
% Consequently, developing realistic synthetic datasets for multiple wearable sensors emerges as a promising strategy to overcome challenges linked to privacy issues, varied human interactions, and the demand for extensive labeling.

In this paper, we introduce \emph{EgoSim}, a multi-view body-worn camera simulator designed for human motion tasks. 
We also present \emph{MultiEgoView}, a dataset that comprises rendered footage simulated from existing human motion data and novel complementary real-world recordings  (Figure~\ref{fig:teaser}). 
We demonstrate the benefit of body-worn cameras and our simulator with the example of ego-body pose estimation using an end-to-end trained vision-only model.
% 
Our contributions in this dataset paper are:

\begin{enumerate}[leftmargin=*,nosep]
\item EgoSim, an easy-to-use, adaptable, and highly realistic simulator for multiple body-worn cameras that uses real human motion as input.
Camera positions on the body and their intrinsics can be configured flexibly, and EgoSim renders a range of useful modalities.
% EgoSim also simulates the forces acting on body-worn cameras to include realistic motion artifacts in rendered footage.
EgoSim also simulates the attachment of body-worn cameras realistically via a spring arm to include motion artifacts.

\item MultiEgoView, a 119-hour video dataset of one or more avatars that perform natural motions and activities based on AMASS~\cite{amass} in four virtual environments with reference 3D body poses.
We contribute a novel 5-hour real-world dataset with 13 participants who wore 6 GoPro cameras with 3D body reference poses (from Xsens~\cite{XSens}) and dense human activity classes (BABEL~\cite{punnakkalBABELBodiesAction2021jun}).

\item A learning-based multi-view method for end-to-end 3D pose estimation tasks from video.
We analyze the sim2real gap based on our dataset and show the benefits of simulated data.
\end{enumerate}


Taken together, we believe that EgoSim---alongside other emerging simulators (e.g., for faces~\cite{li2017learning} and scene interactions of human bodies~\cite{liEgoGenEgocentricSynthetic2024apr, hassan2021populating, zhao2023synthesizing, zhang2022wanderings, delp2007opensim}, and hands~\cite{zhang2023artigrasp, braun2023physically})---will contribute to advancing open research on egocentric perception tasks.


% the evaluation of the previous tasks on the data synthesized by EgoSim shows its potential to produce adequate realistic datasets for wearable device research.
% Through its synthesized data, our adapted network architectures could perform in complex egocentric scenarios \TODO{with usable/... results}.
% Therefore, we believe that EgoSim, alongside other emerging simulators in the field (e.g., for faces~\cite{li2017learning} and scene interactions of human bodies~\cite{hassan2021populating, zhao2023synthesizing, zhang2022wanderings, delp2007opensim} and hands~\cite{zhang2023artigrasp, braun2023physically}), can significantly contribute to advancing open research on wearable sensors and egocentric tasks, offering more flexibility on environment and sensor placement.

 

%-------------------------------------------------------------------------
% \subsection{Language}

% All manuscripts must be in English.

% \subsection{Dual submission}

% Please refer to the author guidelines on the \confName\ \confYear\ web page for a
% discussion of the policy on dual submissions.

% \subsection{Paper length}
% Papers, excluding the references section, must be no longer than eight pages in length.
% The references section will not be included in the page count, and there is no limit on the length of the references section.
% For example, a paper of eight pages with two pages of references would have a total length of 10 pages.
% {\bf There will be no extra page charges for \confName\ \confYear.}

% Overlength papers will simply not be reviewed.
% This includes papers where the margins and formatting are deemed to have been significantly altered from those laid down by this style guide.
% Note that this \LaTeX\ guide already sets figure captions and references in a smaller font.
% The reason such papers will not be reviewed is that there is no provision for supervised revisions of manuscripts.
% The reviewing process cannot determine the suitability of the paper for presentation in eight pages if it is reviewed in eleven.

% %-------------------------------------------------------------------------
% \subsection{The ruler}
% The \LaTeX\ style defines a printed ruler which should be present in the version submitted for review.
% The ruler is provided in order that reviewers may comment on particular lines in the paper without circumlocution.
% If you are preparing a document using a non-\LaTeX\ document preparation system, please arrange for an equivalent ruler to appear on the final output pages.
% The presence or absence of the ruler should not change the appearance of any other content on the page.
% The camera-ready copy should not contain a ruler.
% (\LaTeX\ users may use options of \texttt{cvpr.sty} to switch between different versions.)

% Reviewers:
% note that the ruler measurements do not align well with lines in the paper --- this turns out to be very difficult to do well when the paper contains many figures and equations, and, when done, looks ugly.
% Just use fractional references (\eg, this line is $087.5$), although in most cases one would expect that the approximate location will be adequate.


% \subsection{Paper ID}
% Make sure that the Paper ID from the submission system is visible in the version submitted for review (replacing the ``*****'' you see in this document).
% If you are using the \LaTeX\ template, \textbf{make sure to update paper ID in the appropriate place in the tex file}.


% \subsection{Mathematics}

% Please number all of your sections and displayed equations as in these examples:
% \begin{equation}
%   E = m\cdot c^2
%   \label{eq:important}
% \end{equation}
% and
% \begin{equation}
%   v = a\cdot t.
%   \label{eq:also-important}
% \end{equation}
% It is important for readers to be able to refer to any particular equation.
% Just because you did not refer to it in the text does not mean some future reader might not need to refer to it.
% It is cumbersome to have to use circumlocutions like ``the equation second from the top of page 3 column 1''.
% (Note that the ruler will not be present in the final copy, so is not an alternative to equation numbers).
% All authors will benefit from reading Mermin's description of how to write mathematics:
% \url{http://www.pamitc.org/documents/mermin.pdf}.

% \subsection{Blind review}

% Many authors misunderstand the concept of anonymizing for blind review.
% Blind review does not mean that one must remove citations to one's own work---in fact it is often impossible to review a paper unless the previous citations are known and available.

% Blind review means that you do not use the words ``my'' or ``our'' when citing previous work.
% That is all.
% (But see below for tech reports.)

% Saying ``this builds on the work of Lucy Smith [1]'' does not say that you are Lucy Smith;
% it says that you are building on her work.
% If you are Smith and Jones, do not say ``as we show in [7]'', say ``as Smith and Jones show in [7]'' and at the end of the paper, include reference 7 as you would any other cited work.

% An example of a bad paper just asking to be rejected:
% \begin{quote}
% \begin{center}
%     An analysis of the frobnicatable foo filter.
% \end{center}

%    In this paper we present a performance analysis of our previous paper [1], and show it to be inferior to all previously known methods.
%    Why the previous paper was accepted without this analysis is beyond me.

%    [1] Removed for blind review
% \end{quote}


% An example of an acceptable paper:
% \begin{quote}
% \begin{center}
%      An analysis of the frobnicatable foo filter.
% \end{center}

%    In this paper we present a performance analysis of the  paper of Smith \etal [1], and show it to be inferior to all previously known methods.
%    Why the previous paper was accepted without this analysis is beyond me.

%    [1] Smith, L and Jones, C. ``The frobnicatable foo filter, a fundamental contribution to human knowledge''. Nature 381(12), 1-213.
% \end{quote}

% If you are making a submission to another conference at the same time, which covers similar or overlapping material, you may need to refer to that submission in order to explain the differences, just as you would if you had previously published related work.
% In such cases, include the anonymized parallel submission~\cite{Authors14} as supplemental material and cite it as
% \begin{quote}
% [1] Authors. ``The frobnicatable foo filter'', F\&G 2014 Submission ID 324, Supplied as supplemental material {\tt fg324.pdf}.
% \end{quote}

% Finally, you may feel you need to tell the reader that more details can be found elsewhere, and refer them to a technical report.
% For conference submissions, the paper must stand on its own, and not {\em require} the reviewer to go to a tech report for further details.
% Thus, you may say in the body of the paper ``further details may be found in~\cite{Authors14b}''.
% Then submit the tech report as supplemental material.
% Again, you may not assume the reviewers will read this material.

% Sometimes your paper is about a problem which you tested using a tool that is widely known to be restricted to a single institution.
% For example, let's say it's 1969, you have solved a key problem on the Apollo lander, and you believe that the 1970 audience would like to hear about your
% solution.
% The work is a development of your celebrated 1968 paper entitled ``Zero-g frobnication: How being the only people in the world with access to the Apollo lander source code makes us a wow at parties'', by Zeus \etal.

% You can handle this paper like any other.
% Do not write ``We show how to improve our previous work [Anonymous, 1968].
% This time we tested the algorithm on a lunar lander [name of lander removed for blind review]''.
% That would be silly, and would immediately identify the authors.
% Instead write the following:
% \begin{quotation}
% \noindent
%    We describe a system for zero-g frobnication.
%    This system is new because it handles the following cases:
%    A, B.  Previous systems [Zeus et al. 1968] did not  handle case B properly.
%    Ours handles it by including a foo term in the bar integral.

%    ...

%    The proposed system was integrated with the Apollo lunar lander, and went all the way to the moon, don't you know.
%    It displayed the following behaviours, which show how well we solved cases A and B: ...
% \end{quotation}
% As you can see, the above text follows standard scientific convention, reads better than the first version, and does not explicitly name you as the authors.
% A reviewer might think it likely that the new paper was written by Zeus \etal, but cannot make any decision based on that guess.
% He or she would have to be sure that no other authors could have been contracted to solve problem B.
% \medskip

% \noindent
% FAQ\medskip\\
% {\bf Q:} Are acknowledgements OK?\\
% {\bf A:} No.  Leave them for the final copy.\medskip\\
% {\bf Q:} How do I cite my results reported in open challenges?
% {\bf A:} To conform with the double-blind review policy, you can report results of other challenge participants together with your results in your paper.
% For your results, however, you should not identify yourself and should not mention your participation in the challenge.
% Instead present your results referring to the method proposed in your paper and draw conclusions based on the experimental comparison to other results.\medskip\\

% \begin{figure}[t]
%   \centering
%   \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
%    %\includegraphics[width=0.8\linewidth]{egfigure.eps}

%    \caption{Example of caption.
%    It is set in Roman so that mathematics (always set in Roman: $B \sin A = A \sin B$) may be included without an ugly clash.}
%    \label{fig:onecol}
% \end{figure}

% \subsection{Miscellaneous}

% \noindent
% Compare the following:\\
% \begin{tabular}{ll}
%  \verb'$conf_a$' &  $conf_a$ \\
%  \verb'$\mathit{conf}_a$' & $\mathit{conf}_a$
% \end{tabular}\\
% See The \TeX book, p165.

% The space after \eg, meaning ``for example'', should not be a sentence-ending space.
% So \eg is correct, {\em e.g.} is not.
% The provided \verb'\eg' macro takes care of this.

% When citing a multi-author paper, you may save space by using ``et alia'', shortened to ``\etal'' (not ``{\em et.\ al.}'' as ``{\em et}'' is a complete word).
% If you use the \verb'\etal' macro provided, then you need not worry about double periods when used at the end of a sentence as in Alpher \etal.
% However, use it only when there are three or more authors.
% Thus, the following is correct:
%    ``Frobnication has been trendy lately.
%    It was introduced by Alpher~\cite{Alpher02}, and subsequently developed by
%    Alpher and Fotheringham-Smythe~\cite{Alpher03}, and Alpher \etal~\cite{Alpher04}.''

% This is incorrect: ``... subsequently developed by Alpher \etal~\cite{Alpher03} ...'' because reference~\cite{Alpher03} has just two authors.

% \begin{figure*}
%   \centering
%   \begin{subfigure}{0.68\linewidth}
%     \fbox{\rule{0pt}{2in} \rule{.9\linewidth}{0pt}}
%     \caption{An example of a subfigure.}
%     \label{fig:short-a}
%   \end{subfigure}
%   \hfill
%   \begin{subfigure}{0.28\linewidth}
%     \fbox{\rule{0pt}{2in} \rule{.9\linewidth}{0pt}}
%     \caption{Another example of a subfigure.}
%     \label{fig:short-b}
%   \end{subfigure}
%   \caption{Example of a short caption, which should be centered.}
%   \label{fig:short}
% \end{figure*}
