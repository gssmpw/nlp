

\section{Experiments}
\label{sec:experiments}
We empirically study the effectiveness of MultiEgoView for egocentric body pose estimation.
Following the BABEL-60 split \cite{punnakkalBABELBodiesAction2021jun} (60\%/20\%/20\%), sequences of synthetic data are divided into segments of up to 5 seconds.
The baseline model directly takes inputs from all six cameras, normalizes the images to the ImageNet mean, and downsamples them to 224\,$\times$\,224 pixels at 10\,fps.
We accelerate the training process with a pre-trained sparse tube tokenizer on UCF101~\cite{yrDanielcodeTubeViT2024may, soomroUCF101Dataset1012012dec}. 
The model is trained using the Adam optimizer with a learning rate of \(1 \times 10^{-4}\) on an Nvidia GeForce RTX 4090 with a batch size of 12 for 135k steps, taking around 3 days.

For real data, we use a random 80\%/20\% split with the same 5-second chunking and training parameters.
We also conduct a cross-participant evaluation, using 10 participants for training and 3 for testing, to demonstrate the model's generalization ability.

\subsection{Quantitative metrics}

We evaluate our model on a series of metrics using the body joints of the SMPLX model as follows:

\begin{description}[align=right,leftmargin=3.1cm,labelindent=2.9cm]
\item[Global MPJPE ($m$)] Evaluates the mean $l_{2}$-norm between predicted and ground truth joint positions, punishing both pose and global position errors.

\item[PA-MPJPE ($m$)] Assesses pose estimation accuracy after aligning joint positions up to a similarity transform, isolating pure pose errors.

\item[MTE ($m$)] Mean Translation Error measures the mean $l_{2}$-norm of global root translation errors, indicating global translation accuracy.
\item[MRE] Mean Rotation Error reports global orientation error using $\lVert R\hat{R}^{-1} - I \rVert$.

\item[MJAE (Â°)] Mean Joint Angle Error compares predicted joint angle errors in degrees without considering forward kinematic chain errors.

\item[Jerk ($m/s^3$)] Measures the smoothness of the predicted movement, indicating temporal continuity and naturalness of motion.
\end{description}

\subsection{Evaluation results}

% \paul{already reads like a discussion.}

% \jiaxi{explain the setup: what is 20\%, 20\% real and what is Real(cross-participant). \\also explain why training and testing on Real gave the best Jerk}
Table~\ref{tab:results_table} shows the results of our multi-view pose transformer when trained on MultiEgoView. 
Training on synthetic data shows a low PA-MPJPE, implying a very good pose estimation. 
The slightly higher global MPJPE error arises due to a worse estimation of the root translation and rotation. 
The combination of synthetic and real data in MultiEgoView is crucial, as direct sim-2-real and training solely on real data fails to achieve accurate pose estimation. Pretraining on synthetic data followed by fine-tuning on real data improves the global MPJPE by 3.1-4 times and also lowers the PA-MPJPE by at least 2.7\,cm, indicating a knowledge transfer of pose understanding from the large synthetic dataset to the real-world data. 
Even with a reduced fine-tuning train split of 20\%, the network predicts accurate poses, though with a 8.8\% increase in translation error. 
This showcases the benefit of synthetic data in improving pose estimation on scarce real training data.
The results of the cross-participant evaluation lag behind the others. Indicating that more diversity could be required to obtain stable cross-participant results. 

The visualization of the predicted poses in Figure \ref{fig:predicted_poses} confirms the quantitative metrics. 
Generally, the model estimates the pose accurately. The biggest errors typically occur in the fast-moving limbs, as seen in the right column of Figure \ref{fig:predicted_poses} where the model does mistakenly detect an arm movement. 
The pose outputs of the model are spatial-temporally smooth, which is also reflected in low jerk values (Table \ref{tab:results_table}). Generally, the evaluations yielding lower Jerk indicate less active predictions that do not fully capture the full range and speed of the gt-motions (gt-jerk on eval set is 29.3) but still look natural.  
The model's weak point is the higher error global root position estimates. 
We attribute this weakness in global transformation prediction to two factors: 1) The model predicts the relative transformation between each frame, simplifying training by focusing on neighboring frames' transformations. However, small errors in relative prediction quickly accumulate in the forward kinematics. 2) The model lacks a method-based grounding of global position (e.g., through SLAM or SfM), making its transformation prediction reliant on learned environmental understanding.

Overall, MultiEgoView, with its synthetic and real-world data, shows its utility by enabling sim-to-real transfer learning.
Our results also show that this would not be possible with just synthetic data or the amount of real data captured, validating the benefit of our simulator.

\begin{table}[t]
    \centering
    \caption{Results of our method on MultiEgoView, showing the benefit of our simulator.}
    \vspace{1mm}
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{@{}llrrrrrr@{}}
        Method & & Global & PA- & & & \\
        trained on & evaluated on &  MPJPE $\downarrow$ & MPJPE $\downarrow$ & MTE $\downarrow$ & MRE $\downarrow$ & MJAE $\downarrow$ & Jerk \\
        \midrule
        % Synthetic Synthetic (2 scenes) & Synthetic (2 scenes) & 0.18 & 0.041 & 0.14 & 0.334 & 9.3 & 21.7 \\
        % \textbf{NEW} Synthetic (24 scenes) & Synthetic (2 scenes) & 0.15 & 0.039 & 0.12 & 0.271 & 9.0 & 22.2 \\
        Synthetic & Synthetic & 0.16 & 0.040 & 0.13 & 0.272 & 9.1 & 21.9 \\
        % \textbf{NEW} Synthetic (2 scenes) & Synthetic (new 22 scenes) & 0.42 & 0.148 & 0.34 & 0.47 & 25.7 & 15.4 \\
        % Synthetic Synthetic (2 scenes) & Real & 1.02 & 0.203 & 0.98 & 1.081 & 33.9 & 15.8 \\
        Synthetic & Real & 0.77 & 0.119 & 0.71 & 0.947 & 29.0 & 20.9 \\
        Real & Real & 1.23 & 0.087 & 0.79 & 1.030 & 16.4 & 1.5 \\[.75mm]
        
        % Synthetic Fine Tuned & synthetic & 0.46 & 0.14 & 0.36 & 0.499 & 25.2 & 14.52 \\
        \emph{with fine-tuning:} \\
        % Synthetic 2 + 20\% Real & Real & 0.44 & 0.060 & 0.41 & 0.562 & 13.1 & 15.9 \\
        Synthetic + 20\% Real & Real & 0.40 & 0.056 & 0.37 & 0.504 & 12.8 & 15.4 \\
        %Synthetic 2 + 80\% real & Real & 0.36 & 0.047 & 0.34 & 0.486 & 10.5 & 16.7 \\
        Synthetic + 80\% real & Real & 0.33 & 0.044 & 0.31 & 0.415 & 10.2 & 16.7 \\
        % Synthetic 2 + 10 real participants & Real (cross-participant) & 0.52 & 0.068 & 0.49 & 0.651 & 17.2 & 13.5 \\
        Synthetic + 10 real participants & Real (cross-participant) & 0.35 & 0.060 & 0.32 & 0.557 & 16.6 & 18.3 \\
    \end{tabular}
    }
    \label{tab:results_table}
\end{table}

\begin{figure}[b]
    \centering
    \includegraphics[width=\textwidth]{imgs/results/egosim_results-ch2.pdf}
    \caption{Visualization of our results obtained from our multi-view egocentric pose estimator on real-world data. The change of color denotes different timestamps.}
    \label{fig:predicted_poses}
\end{figure}

% \begin{table}[]
%     \centering
%     \begin{tabular}{cr|rrrrr}

%  Method & Eval on &Ang Err &Mesh Err&MPJPE&PA-MPJPE&Trans Err\\
% \midrule
% DIP & & & & & &\\
% TransPose & & & & & &\\
% PIP & & & & & &\\
% Disney & & & & & &\\
% Unreal-Ego & & & & & &\\
% HPS & & & & & &\\
% SceneAware HPS & & & & & &\\
% Ours & & & & & &\\
% \end{tabular}
%     \caption{Pose estimation from wearable sensors.}
%     \label{tab:my_label}
% \end{table}

\subsection{Spring Damper}\label{sec:spring_damper}
Body-worn cameras experience motion artifacts, especially when mounted on limbs that move quickly, due to non-rigid mounting points.
EgoSim models these motion artifacts via a \href{https://dev.epicgames.com/documentation/en-us/unreal-engine/using-spring-arm-components-in-unreal-engine}{Spring Arm}.
We demonstrate that a spring-damper system approximates real camera motion better than a rigid mount.
For that, we used an \href{https://www.optitrack.com/cameras/primex-13/}{OptiTrack motion capture system} to track both the attached body and the GoPro that we loosely attached to the body.
Using an \href{https://www.optitrack.com/cameras/primex-13/}{OptiTrack motion capture system}, we tracked both the body and a loosely attached GoPro. 
Results show the spring-damper model yields a lower mean position error (1.98 cm) compared to the rigid model (2.35 cm), highlighting its effectiveness.