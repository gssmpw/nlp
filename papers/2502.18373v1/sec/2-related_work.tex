\section{Related Work}
\label{sec:relatedwork}


% The advancements in deep learning-based methods have prompted a notable shift in the emergence of a new paradigm focused on developing simulators and curating real-world data. Notably, Microsoft Airsim \cite{shah2018airsim}, tailored specifically for car and drone simulation, been successfully used for developing and testing algorithms in in various domains, including reinforcement learning and robotics. The datasets synthesized using AirSim \cite{tartanair, midair}, exhibit a remarkable level of realism, have played a significant role in advancing autonomous driving and aerial vehicle technologies by facilitating the development of algorithms for Simultaneous Localization and Mapping (SLAM) and Visual Odometry (VO). However, while AirSim and similar simulators \cite{dosovitskiy2017carla} have proven invaluable, they lack a focus on human-centric tasks. Only recently, Habitat 3 was released, aiming to address human-robot interaction tasks. However, its scope is confined to indoor environments and it falls short in terms of flexibility for attaching sensors to various body parts, as well as in facilitating multi-person interactions. In contrast, our simulator is the first to specifically focus on the simulation of wearable sensors, and it uniquely supports multi-character interactions in both indoor and outdoor settings. This broadens the scope for research and development in wearable sensor technology, allowing for more comprehensive and diverse studies in this field.

% Real datasets with pose anotations are usually captuted from third person view cameras. The datasets with accurate annotations are captured in indoor environments usually with motion capture setups (cite panoptic. As motion capture in outdoor is challenging the real datasets in outdoor setting provide sudo annotations. Synthetic datasets can provide groudtruth annotations. with realistic rendering, they can be a good replacement of the real datasets. Synthetic datasets with mocap motions and realistic textures such as  \cite{bedlam} has shown that networks optimized on synthetic datasets can reach state of the art results on real data.

% Egocentric datasets, mainly focus on action recognition or hand-object interactions. Such datasets do not provide annotations about the pose of the camera wearer and the people in the scene. Only a few Egocentric datasets provide pose annotations. Mo2Cap2~\cite{xu2019mo}, xR-EgoPose~\cite{tome2019xr} target a single head-mounted camera for egocentric pose estimation of camera wearer but there is no interaction or pose label for the people in the environment. EgoCap~\cite{rhodin2016egocap}, EgoGlass~\cite{zhao2021egoglass}, UnrealEgo~\cite{akada2022unrealego} target stereo camera setup mounted on head or glasses.
% Similarly, these datasets provide only the pose of the wearer.
% HPS~\cite{guzov2021human} provides the recording for multiple IMUs and a head-mounted camera for estimating the pose of camera wearer.
% There is no interaction with other people. You2Me~\cite{ng2020you2me} provides human poses of two interacting people from the captures of a chest mounted camera.
% EgoBody~\cite{zhang2022egobody} provides the pose of two inetracting people from the head-mounted camera.
% Egohumans~\cite{khirodkar2023egohumans} provides the pose of up to $4$ people interacting with each other in both indoor and outdoor scenes from wearable glasses.
% Aria digital twin~\cite{pan2023aria}, a dataset for aria glasses limited to wearable camera and indoors.
% Ego4D~\cite{grauman2022ego4d} provides large dataset from head-mounted cameras that could be useful for various tasks such as social interaction and hand object interaction but doesn't provide dense 3D annotation of the pose of camera wearer and the people in the scene.   

% \TODO{restructure related work, add citations, build stronger link to methods and problems that could benefit from this simulator}

% Real datasets with pose anotations are usually captuted from third person view cameras. The datasets with accurate annotations are captured in indoor environments usually with motion capture setups (cite panoptic. As motion capture in outdoor is challenging the real datasets in outdoor setting provide sudo annotations. Synthetic datasets can provide groudtruth annotations. with realistic rendering, they can be a good replacement of the real datasets. Synthetic datasets with mocap motions and realistic textures such as  \cite{bedlam} has shown that networks optimized on synthetic datasets can reach state of the art results on real data.
% Real datasets with pose anotations are usually captuted from third person view cameras. The datasets with accurate annotations are captured in indoor environments usually with motion capture setups (cite panoptic. As motion capture in outdoor is challenging the real datasets in outdoor setting provide sudo annotations. Synthetic datasets can provide groudtruth annotations. with realistic rendering, they can be a good replacement of the real datasets. Synthetic datasets with mocap motions and realistic textures such as  \cite{bedlam} has shown that networks optimized on synthetic datasets can reach state of the art results on real data.
% in outdoor setting provide sudo annotations. Synthetic datasets can provide groudtruth annotations. with realistic rendering, they can be a good replacement of the real datasets. Synthetic datasets with mocap motions and realistic textures such as  \cite{bedlam} 


\paragraph{Synthetic datasets and simulators.}

The advancement of deep learning in recent years has necessitated larger and more varied datasets that can be acquired using simulated data. Visual synthetic data proved its benefits in many fields such as human mesh recovery \cite{bedlam}, visual-inertial odometry~\cite{minoda2021viode}, visual SLAM~\cite{rukhovich2019slam,teed2021droidslam}, and human pose estimation~\cite{varol2017learnng, bedlam}. Microsoft AirSim~\cite{shah2018airsim} stands out as one of the most effective simulators. It has facilitated the creation of photo-realistic datasets such as TartanAir~\cite{tartanair}, optimized for Visual SLAM tasks, and Mid-air~\cite{midair}, designed for low-altitude drone flights.
So far, AirSim \cite{shah2018airsim} and other simulators \cite{dosovitskiy2017carla} fall short in tasks centered on human dynamics, such as 3D human pose estimation or multi-actor interactions. Only recently, the Habitat\,3~\cite{puig2023habitat3} simulator targets human-robot interaction tasks and progresses in this area but offers limited configuration for sensor placement and environmental diversity. EgoGen as a novel human-centered simulator demonstrates promise by focusing on human motion synthesis \cite{liEgoGenEgocentricSynthetic2024apr}.
Traditionally, datasets simulate cameras either statically or with smooth movements. Such datasets fail to generalize to egocentric scenarios where the camera's position dynamically changes in relation to the wearer's movements.
EgoSim advances this field by being specifically designed for human-centric research with wearable cameras that follow the natural non-smooth movements within the human body. It uniquely supports complex multi-character interactions in varied environments, both indoor and outdoor, enabling more comprehensive and diverse studies in this field. 



\paragraph{Human motion datasets.}
In controlled settings, multiple third-person view cameras and motion capture equipment offer accurate ground-truth data ~\cite{amass,panoptic,bhatnagar2022behave, huang2022contact, hassan2019ambiguities, cai2022humman}.
Fitting 3D body models~\cite{SMPL:2015,SMPL-X:2019,STAR:2020} to point cloud marker sets~\cite{amass} or using RGBD camera data can provide ground-truth poses.
However, the complexity of these setups mostly limits their scalability to indoor environments~\cite{rhodin2016egocap, zhao2021egoglass, zhang2022egobody}.
Pseudo-ground truth pose annotations can overcome these limitations for outdoor environments.
Several methods use 2D keypoints~\cite{andriluka2014humanpose, iqbal2017posetrack, martinmartin2021jrdb}, which are easy to label at a large scale, but provide 2D constraints only on the human pose. Alternatively, fitting 3D body models such as SMPL~\cite{SMPL:2015} to images provides pseudo-ground truth parameters~\cite{moon2022neuralannot,kolotouros2019reconstruct,joo2020posefitting}.
% Networks trained on this may inherit biases from this ground-truth computation, however.
% 
You2Me~\cite{ng2020you2me} and EgoBody~\cite{zhang2022egobody} capture human pose data for interacting individuals using head-mounted cameras in indoor settings.
% However, these datasets are constrained to interactions between two individuals and indoor settings, limiting the potential range of motion and actions.
Recently, Egohumans~\cite{khirodkar2023egohumans} has expanded the scope to include up to four interacting individuals in both indoor and outdoor settings.
%However, there remains a gap in large-scale datasets for scenarios involving interactions with more than four people. Additionally, there is a lack of datasets where cameras are mounted on various body parts, not just the head. 
Meanwhile, larger datasets like Ego4D~\cite{grauman2022ego4d, graumanEgoExo4DUnderstandingSkilled2024apr} offer extensive data from head-mounted cameras for tasks such as social interaction and hand-object interaction, but they lack data from additional body-worn cameras. % and comprehensive 3D pose annotations for both the camera wearer and others in the scene.
The recently published Nymeria dataset~\cite{maNymeriaMassiveCollection2024sep} addresses this gap partially and includes real-world videos from wrist-mounted cameras.
Our real-world MultiEgoView dataset further extends to a setup with six body-worn cameras with additional sensors at the knees and pelvis.
To overcome limitations in real-world datasets, realistic synthetic datasets offer an alternative that offers diversity and quality ground truth annotations\cite{bedlam, yang2023synbody, akada2022unrealego}.
Our work expands on this approach by introducing a configurable simulator tailored to body-worn sensors, with adjustable parameters for lighting, scene, and camera placement.
EgoSim complements real-world datasets like Nymeria by enabling the rendering of synthetic images from adjustable body-worn cameras based on their captured motion sequences. 

% EgoSim further complements existing real-world datasets such as Nymeria offering the opportunity for additional synthetic dataaugmentation by giving opportunity to import and render motion sequences in unseen environements.
% EgoSim further complements existing real-world datasets such as Nymeria by enabling diverse multi-modal camera rendering from real motion data.

\paragraph{Egocentric perception.}
Wearable cameras serve as the primary input for research on egocentric perception tasks.
Currently, real and synthetic egocentric datasets mainly feature head-mounted sensors.
Some systems~\cite{xu2019mo, wangSceneAwareEgocentric3D2023jun, tome2019xr} use a single head-mounted, body-facing, fisheye camera to estimate 3D ego-body pose, while others rely on a stereo configuration ~\cite{rhodin2016egocap, zhao2021egoglass, akada2022unrealego}. 
Head-mounted, body-facing cameras benefit from capturing visible joints in image space to aid ego-body pose estimation.
Other methods recover the 3D pose from non-body-facing cameras. HPS~\cite{guzov2021human} integrates multiple body-worn IMUs with camera-based localization using structure from motion. 
Kinpoly~\cite{luoDynamicsRegulatedKinematicPolicy2022oct} recovers the whole body pose from a front-facing camera using physics simulation with reinforcement learning, while EgoEgo~\cite{liEgoBodyPoseEstimation2023aug} combines SLAM with a diffusion model to recover the ego-body pose.
AvatarPoser~\cite{jiang2022avatarposer} and its subsequent work~\cite{jiang2024egoposer, jiang2024manikin} predict full-body poses based on head and hand poses tracked by commercial mixed reality devices.
HOOV~\cite{streli2023hoov} extends hand tracking beyond the field of view of head-mounted cameras using inertial signals captured at the wrist.

So far, egocentric datasets have mainly focused on head-mounted cameras that either point down toward the body \cite{akada2022unrealego, wangSceneAwareEgocentric3D2023jun, xuMo2Cap2RealtimeMobile2019jan} or forward~\cite{liEgoBodyPoseEstimation2023aug, guzovHumanPOSEitioningSystem2021juna,  yuanEgoPoseEstimationForecasting2019aug}, often designed for specific devices~\cite{khirodkar2023egohumans, graumanEgoExo4DUnderstandingSkilled2024apr, zhang2022egobody, zhaoEgoBody3MEgocentricBody2024}.
Our work extends egocentric datasets to multiple body-worn cameras by providing an adaptable simulation platform and a real-world dataset of six body-worn cameras.

% Primarily focused on action recognition or hand-object interactions, these datasets only provide dense 3D pose annotations for the camera wearer, and do not feature interactions or tracking of other humans in a scene.


\endinput
% Wearable cameras and sensors present an alternative to stationary multi-camera setups, enabling research in novel egocentric tasks. Currently, egocentric datasets predominantly feature head-mounted sensors. 

% Egocentric datasets, mainly focus on action recognition or hand-object interactions.
% Such datasets do not provide annotations about the pose of the camera wearer and the people in the scene. Only a few Egocentric datasets provide pose annotations. Mo2Cap2~\cite{xu2019mo}, xR-EgoPose~\cite{tome2019xr} target a single head-mounted camera for egocentric pose estimation of camera wearer but there is no interaction or pose label for the people in the environment.
% EgoCap~\cite{rhodin2016egocap}, EgoGlass~\cite{zhao2021egoglass}, UnrealEgo~\cite{akada2022unrealego} target stereo camera setup mounted on head or glasses.
% Similarly, these datasets provide only the pose of the wearer.
% HPS~\cite{guzov2021human} provides the recording for multiple IMUs and a head-mounted camera for estimating the pose of camera wearer.
% There is no interaction with other people. You2Me~\cite{ng2020you2me} provides human poses of two interacting people from the captures of a chest mounted camera.
% EgoBody~\cite{zhang2022egobody} provides the pose of two interacting people from the head-mounted camera.
% Egohumans~\cite{khirodkar2023egohumans} provides the pose of up to $4$ people interacting with each other in both indoor and outdoor scenes from wearable glasses.
% Aria digital twin~\cite{pan2023aria}, a dataset for aria glasses limited to wearable camera and indoors.
% Ego4D~\cite{grauman2022ego4d} provides large dataset from head-mounted cameras that could be useful for various tasks such as social interaction and hand object interaction but does not provide dense 3D annotation of the pose of camera wearer and the people in the scene.   