\section{Discussion}
\label{discussion}
% Egocentric datasets with only head-mounted camera benefit from capturing the whole body but face multiple challenges in predicting the lower body due to self-occlusions and lower resolution caused by increased distance. 
% Additionally, these datasets must balance the focus of the field of view between the body and the environment, which is most relevant for interaction. 

While egocentric pose estimation has been well explored, in prior implementations head-mounted cameras faces challenges such as self-occlusion, reduced resolution for lower body reconstruction, and lack of environmental information.
Here, multiple body-worn cameras can mitigate these by providing dynamic, multi-view perspectives that simultaneously capture the environment and the body and, more importantly, the interaction between our hands and legs with the surroundings.
% As our predominant interaction with the world is via the hands, wrist-mounted cameras should show a promising path for human interaction understanding.

EgoSim, together with MultiEgoView, is a first stepping stone to deepen our understanding of human activity from body-worn cameras at various locations.
We showcase the usefulness of MultiEgoView for ego pose estimation with our learned video-based end-to-end multi-view model.
Our findings show that ego pose can effectively be estimated from several body-mounted cameras and EgoSim's rendered data helps obtain better pose estimation in sim-to-real scenarios.

\noindent\textbf{Limitations of EgoSim.}
\label{sec:limitations}
Our current simulator has some limitations that will be addressed in future iterations.
First, although our data includes multi-human scenarios, individual avatar animations are sampled independently from AMASS. These animations, while physically plausible, do not account for interactions with other humans or objects, limiting the study of such interactions.
Additionally, our system currently features only four scenes, which can be extended to improve the generalization.
Lastly, while our simulator supports high-fidelity rendering, improvements in graphics and neural rendering methods~\cite{kerbl20233d} are expected to reduce the simulation-to-real gap further.

\noindent\textbf{Future research on \dataset{}.}  
While the pose estimation capabilities of our multi-view transformer trained using EgoMultiView are convincing on real-world data (PA-MPJPE < 5 cm), there is still room for improvement in the global position and orientation estimation of the root, especially for long sequences, where cumulative errors in root position become more pronounced. Future research directions could consider integrating low-drift camera localization methods, such as SLAM \cite{teed2021droidslam}, or image-based localization via structure from motion \cite{colmap}, to achieve more stable global translation and orientation. 
Moreover, our current experiments only utilize RGB data. Future research could leverage \dataset{} to enhance inertial-based pose estimation~\cite{huang2018deep}, depth estimation using monocular or multiple cameras~\cite{zhao2020monocular}, and semantic scene classification~\cite{dai20183dmv}, all of which are supported by the ground-truth annotations provided by our simulator.

% However, it lacks scenarios rich in human-human and human-object interaction in which body-worn cameras provide benefits on paper. Extending MultiEgoView with additional in- and outdoor scenes and making the avatars more interaction-centered can lead to a more complete understanding of human activity recognition using body-worn cameras.



% While the pose estimation of our multi-view transformer is accurate (low PA-MPJPE), the global position and orientation estimation of the root lag behind. 
% Here, future research directions can consider adjusting low drift camera localization methods such as SLAM \cite{teed2021droidslam} or image-based localization via structure from motion \cite{colmap} to ground the global translation and orientation stable in the world. 

% \xintong{maybe mention multiegoview only has rgb images but not depth } yes will do thanks!!

\section{Conclusion}
We have proposed EgoSim, an egocentric multi-view simulator for body-worn cameras that generates multiple data modalities to support emerging wearable motion capture and method development.
Using EgoSim, we partially generated MultiEgoView, the first dataset that complements existing head-focused egocentric datasets with synchronized footage from six cameras worn at other locations on the body, simulated from accurate and real human motion and artifacts.
We complement MultiEgoView's 119\,hours of synthetic data with 5\,hours of actual recordings from 6 body-worn GoPro cameras and 13 participants during a wide range of motions and activities in the wild with annotated 3D body poses and classification labels to bridge the gap between simulation and real-world data.

% While the tasks we investigated primarily involved multiple cameras, the utility of our simulator extends beyond just these applications. It can also be beneficial for tasks involving multiple IMU sensors, and any combination of wearable cameras and IMU sensors. This versatility showcases the wide range of potential uses for our simulator in various research contexts.

In the wake of the emerging area of vision-based method development from one or more body-worn sensors, we believe that our release of EgoSim and MultiEgoView will be a useful resource for future work to increase our understanding of human activities and interactions in the real world.
