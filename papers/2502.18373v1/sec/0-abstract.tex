% \twocolumn[{
%     \maketitle
%     \renewcommand\twocolumn[1][]{#1}
%     \centering
%     \vspace{-0.5em}
%     \begin{minipage}{0.985\textwidth}
%         \centering
%         \includegraphics[trim=000mm 000mm 000mm 000mm, clip=False, width=\linewidth]{imgs/final_2.pdf}
%     \end{minipage}
%     \vspace{-0.5 em}
%     \captionof{figure}{{\bf EgoSim} is a novel simulator for synthesizing egocentric visual-inertial data with rich 3D ground-truth annotations.
%     Configurable to produce a wide array of temporally consistent animations featuring multi-person interactions in diverse outdoor settings, EgoSim captures synchronized sequences of RGBD images and inertial data frames from virtual sensors attached to any avatar's body part within the simulated scene. 
%     This opens avenues for advancing research in egocentric vision by addressing existing limitations in accessible datasets.
%     }
%     \label{fig:teaser}
%     \vspace{2.2em}
% }]

\begin{abstract}
Research on egocentric tasks in computer vision has mostly focused on head-mounted cameras, such as fisheye cameras or embedded cameras inside immersive headsets.
We argue that the increasing miniaturization of optical sensors will lead to the prolific integration of cameras into many more body-worn devices at various locations.
This will bring fresh perspectives to established tasks in computer vision and benefit key areas such as human motion tracking, body pose estimation, or action recognition---particularly for the lower body, which is typically occluded.

In this paper, we introduce \emph{EgoSim}, a novel simulator of body-worn cameras that generates realistic egocentric renderings from multiple perspectives across a wearer's body.
A key feature of EgoSim is its use of real motion capture data to render motion artifacts, which are especially noticeable with arm- or leg-worn cameras.
In addition, we introduce \emph{MultiEgoView}, a dataset of egocentric footage from six body-worn cameras and ground-truth full-body 3D poses during several activities:
119\,hours of data are derived from AMASS motion sequences in four high-fidelity virtual environments, which we augment with 5\,hours of real-world motion data from 13 participants using six GoPro cameras and 3D body pose references from an Xsens motion capture suit.

We demonstrate EgoSim's effectiveness by training an end-to-end video-only 3D pose estimation network.
Analyzing its domain gap, we show that our dataset and simulator substantially aid training for inference on real-world data.

EgoSim code \& MultiEgoView dataset: \scalebox{0.95}[1]{\textbf{\color{magenta}{\url{https://siplab.org/projects/EgoSim}}}}


\end{abstract}

\endinput

showcasing its potential for advancing pose estimation techniques using multi-egocentric video inputs.
, which will spark a need for extensive egocentric datasets.

% previous

Despite the recent popularity of tasks that focus on wearable sensor data, such as full-body pose and motion estimation or human-object interaction, method development is limited by the availability of larger datasets, especially those with body-worn (multi-)camera captures.
To this end, we introduce \emph{EgoSim}, the first simulation platform specifically tailored for body-worn cameras for multi-person scenarios.
EgoSim realistically simulates RGBD camera input and associated inertial measurements.
EgoSim can animate multiple avatars in a scene with natural articulated motions and translations, which are sampled from AMASS and concatenated to represent realistic human motion---for the wearer of the simulated cameras as well as observed avatars in the environment.
A key feature of our simulator is its capability of attaching multiple sensors across one or more avatar bodies, resulting in a detailed and diverse data collection that is synchronized across the simulated setup.
EgoSim additionally produces normal maps and semantic segmentation masks. 
We validate EgoSim for developing and benchmarking algorithms on wearable sensor applications in two downstream tasks:
1) Multi-view multi-person 3D pose estimation, adapted for body-worn cameras, and 2) Benchmarking camera localization algorithms for body-mounted cameras.
In both, EgoSim's synthetic data addresses the lack of egocentric datasets and accurate ground truth, which will enable the development of new methods and tasks.

