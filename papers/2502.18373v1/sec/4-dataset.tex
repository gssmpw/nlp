\section{\dataset{} Dataset}\label{sec:Dataset}

% \begin{itemize}
%     \item explain our dataset and what we generate
%     \item maybe mention the amount of textures and skin tones textures that we use (from bedlam)
%     \item own pose, camera poses, imu, other models joint poses, reference to amass timestamps for smpl parameters
%     \item explain the usecases this dataset can have, ego pose estimation, slam, localization, multiview other person pose estimation
%     \item explain real-world dataset in the wild in front of CAB (maybe piggy bag on Laamar by saying how there is a CAB scan and one can use global localization)
%     \item create a better connection to laamar :)
    
% \end{itemize}

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{imgs/example_figure-ch.pdf}
    \caption{Example RGB renders produced by EgoSim and included in our \dataset dataset. Qualitatively, the simulated scan (d, e, f) and real data (g, h, i) look similar.
    Both simulated scenes (Scene 1: a, b, c; Scene 2: d, e, f) offer high-fidelity environments.
    The pelvis provides a stable view of the environment, whereas wrist and knee cameras typically move quickly and capture artifacts.}
    \label{fig:overall}
\end{figure}

\dataset{} contributes a sizeable and synchronized dataset of RGB data from six body-worn cameras, along with ground-truth body poses and activity annotations.
Our dataset includes real and synthetic data, providing a challenging and interesting testbed for training and benchmarking body-pose estimation, activity classification, dynamic camera localization, and mapping algorithms.


\paragraph{Synthetic data generation.} 
Using EgoSim, we rendered a dataset of 77.4\,M RGB images corresponding to 119.4\,hours captured by six virtual cameras on a virtual avatar.
Images were rendered with a 118째 field of view (FOV) at a resolution of 640\,$\times$\,360 and a framerate of 30\,fps.
Cameras were attached to the head, pelvis, wrists, and knees, facing outwards to capture both the environment and parts of the wearer's body.
This considerably extends the focus of prior work on head-mounted cameras~\cite{xuMo2Cap2RealtimeMobile2019jan, tome2019xr, rhodin2016egocap, akada2022unrealego} and better resembles emerging wearable platforms devices~\cite{zhao2021egoglass, grauman2022ego4d, zhang2022egobody}.
To ensure realistic motions, we animated avatars using motion capture sequences from AMASS~\cite{amass}, converted to FBX format for EgoSim support \cite{SMPL-X:2019}.
We randomly varied avatar appearances in terms of skin color and clothing texture using BEDLAM's assets~\cite{bedlam}.
Our dataset features 24 locations across 4 scenes: (1)~a \href{https://www.unrealengine.com/marketplace/en-US/product/6bb93c7515e148a1a0a0ec263db67d5b}{hand-built virtual outdoor environment of a city}, (2) the front courtyard of a university building that we scanned using \href{https://poly.cam/}{Polycam}, with an accurate public point cloud scan and structure-from-motion model available~\cite{sarlinLaMARBenchmarkingLocalization2022}, (3) \href{https://www.fab.com/listings/474a0598-ed86-40b6-baa1-c801d96ef4ab}{Downtown city with skyscrapers} and (4) \href{https://www.fab.com/listings/11cc2abb-126c-4452-9fe4-6f2381d96544}{a park with sport courts, lawn, vegetation and water}.
Each scene includes up to four simultaneously animated avatars to increase diversity and support multi-view multi-human pose estimation~\cite{khirodkar2023egohumans, aso2021portable}.
In addition to the RGB data, we provide ground-truth camera and 3D avatar poses, as well as simulated accelerometer and gyroscope readings from all six cameras.

\paragraph{Real-world data collection.}

% To generate realistic motions with body-worn cameras we utilize motion sequences from AMASS and render them with EgoSim. 
% To this end, we convert the smpl-x amass sequences and convert them to fbx format, which can be imported and rendered by EgoSim while keeping the body shape of the original amass sequence.
% Additionally, we randomize the skin colors, and clothing textures taken from bedlam \cite{bedlam} of the avatars for each render sequence to increase the diversity of the dataset. 
% The scenes are generated at a resolution of 640x360 at 30 FPS with a FOV of 118째 with no spring used.
% We utilized two scenes to generate the data: first a \href{https://www.unrealengine.com/marketplace/en-US/product/6bb93c7515e148a1a0a0ec263db67d5b}{hand-built outdoor environment of a city} and secondly a scanned and reconstructed front court of a university building via \href{https://poly.cam/}{polycam}.
% This scene also benefits from LaAMAR \cite{sarlinLaMARBenchmarkingLocalization2022} as it covers the same area and offers ground truth global position data, a point cloud scan, and a structure from the motion model for image localization. 
% The body-worn cameras are attached to the head, wrists (in the direction of fingers), pelvis, and knees. 
% MultiEgoView distinguishes itself from other egocentric datasets not only by the body-worn cameras but also by having no camera facing the ego body. 
% This makes pose estimation particularly challenging. 
% To make the dataset interesting not only for ego pose estimation each scene was populated with 4 characters in parallel. 
% MultiEgoView contains ground truth poses of the joints, cameras, and the IMU (accelerometer and gyroscope) readings of all six cameras attached to the main character.
% Obtaining the original smpl parameters of a frame is possible via the saved timestamp of the animation.
% MultiEgoView uses cm as units and the coordinate frame defined by Unreal Engine (x front, y right, z up).


We captured a dataset of $\sim$5\,hours in the real world using six GoPro cameras (5$\times$HERO~10, 1$\times$HERO~9)~\cite{GoPro}, worn at the same body locations as in our simulation.
We recruited 13 participants from places around our institution for this collection, who consented to participation and data recording.
The study considers ETH ethics guidelines and Participants received a small gratuity for their time.
Data was recorded in the same university front courtyard that was scanned for the synthetic environment (2), using GoPros set to a resolution of 1080p at 30\,fps and a horizontal FOV of 118째.
The 13 participants (4 female, 9 male, ages 21--30, mean\,=\,26.4) were recruited from our institution, with heights ranging from 160--190\,cm (mean\,=\,176.1, SD\,=\,9.5) and weights from 50--94\,kg (mean\,=\,69.6, SD\,=\,13.2).
After providing consent (see Appendix for details), participants were equipped with a full-body Xsens~\cite{XSens} motion capture suit for ground-truth pose capture.
Following an initial calibration, participants performed a block of 35 different activities featuring the most common motions from AMASS according to the BABEL annotation~\cite{punnakkalBABELBodiesAction2021jun}.
For a full list of activities, see our appendix Table \ref{tab:motions}.
Each block lasted about 10 minutes, with participants repeating the block 1--3 times. To synchronize the GoPro camera with Xsens, participants clap at the beginning of each recording and match the camera with extracted SMPL poses. We compute shape parameters from the body measurement of participants with Virtual Caliper~\cite{pujades2019virtual}.

All sequences across real and synthetic data are labeled with activity classes from BABEL~\cite{punnakkalBABELBodiesAction2021jun}.



% To verify systems real data in companion with ground truth is required.
% Therefore, we recorded 13 participants in the same University front court that we recreated in the simulation. 
% This allows for an in-the-wild evaluation of ego pose estimation in addition to an understanding of EgoSim's sim-2-real capabilities. 
% Participants were asked to wear an Xsens motion capture suite with 17 IMUs; additionally, 6 GoPROs were mounted identically as in EgoSim.
% After a calibration of the motion capture suit the participants would be asked to perform a selected range of 36 different motions (walk, t-pose, squats, rotate arms, play tennis, ...). 
% These were inspired by the most common motion classes in amass according to the BABEL annotation \cite{punnakkalBABELBodiesAction2021jun}. 
% The participants would repeat the data recording 1-3 times and each recording session lasted roughly 10 minutes. 
% The data was recorded in 1080p at 30 FPS with a horizontal FOV of 118째.

% As our motions are AMASS-based MultiEgoView can not only be used for ego pose and multi-view pose estimation but also for activity classification with the help of BABEL \cite{punnakkalBABELBodiesAction2021jun}.
% Due to the availability of ground truth position both in the synthetic and real-world data, MultiEgoView can also be used for localization (especially when utilizing a subset of CAB LaMAR \cite{sarlinLaMARBenchmarkingLocalization2022}) or SLAM. 
% In total MultiEgoView contains roughly 76h of synthetic video from 6 cameras each totaling more than 50 million frames with ground truth data. 
% In addition to the vast amount of synthetic video Multi-Ego-View also contains 13 participants ($\mu_{height} = 176\text{cm}, \sigma_{height} = 9.46\text{cm}, \mu_{weight}=69.6 \text{kg}, \sigma_{weight}=13.1\text{kg}, \mu_{age}=26.5 \text{years}, \sigma_{age}=2.61 \text{years}$, male to female ratio=4:13) recorded in the wild totaling $\sim$5\,hours).

