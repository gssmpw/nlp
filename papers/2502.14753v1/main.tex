\documentclass[10pt, letter, onecolumn]{main}

% \usepackage{csquotes}
% \usepackage[bibstyle=nature,citestyle=numeric-comp,
%             natbib=true,backend=biber,maxbibnames=5,
%             giveninits=false,sorting=none,defernumbers=true]{biblatex}
% \usepackage[superscript,biblabel]{cite}
% note: sorting=none gives refs in order they appear
\usepackage[pdftex]{graphicx}
\usepackage{pifont}
\usepackage{amssymb}
\usepackage{multirow}
\usepackage{array}
\usepackage{dblfloatfix}
\usepackage{titlesec}
\usepackage[numbers,sort&compress]{natbib}
\usepackage{nameref}
\usepackage{varioref}
% \usepackage{hyperref}
\usepackage{etoc}


% \addbibresource{main.bib}
% \renewcommand*{\bibfont}{\linespread{0.8}\footnotesize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% user added packages / commands

\setlength{\abovecaptionskip}{10pt plus 3pt minus 2pt} % add spacing b/w figure and table
\newlength{\cboxlength}
\settowidth{\cboxlength}{7.1 $\pm$ 13.2} % The widest entry

% for nice-looking up arrows
\usepackage{amsmath,xparse,mleftright}
\NewDocumentCommand{\up}{som}{%
  \IfBooleanTF{#1}
    {\upext{#3}}
    {#3\IfNoValueTF{#2}{\mathord}{#2}\uparrow}%
}
\NewDocumentCommand{\upext}{m}{%
  \mleft.\kern-\nulldelimiterspace#1\mright\uparrow
}

\usepackage{xcolor} % For text color
\usepackage{soul} % For highlighting

\usepackage{adjustbox}
\usepackage[most]{tcolorbox}
\usepackage{float}
\usepackage{xspace}
\usepackage[symbol]{footmisc}
\usepackage{lineno}

\tcbset{
  aibox/.style={
    width=394.18663pt,
    top=10pt,
    colback=white,
    colframe=black,
    colbacktitle=black,
    enhanced,
    center,
    attach boxed title to top left={yshift=-0.1in,xshift=0.15in},
    boxed title style={boxrule=0pt,colframe=white,},
  }
}
\newtcolorbox{AIbox}[2][]{aibox,title=#2,#1}

% preliminaries
\ifdefined\red
    \renewcommand{\red}[1]{\textcolor{red}{#1}}
\else
    \newcommand{\red}[1]{\textcolor{red}{#1}}
\fi
\ifdefined\blue
    \renewcommand{\blue}[1]{\textcolor{blue}{#1}}
\else
    \newcommand{\blue}[1]{\textcolor{blue}{#1}}
\fi
% \newcommand{\hhll}[1]{\sethlcolor{yellow}\hl{#1}} % highlights on
\newcommand{\hhll}[1]{#1} % highlights off
\newcommand{\hhllred}[1]{\sethlcolor{red}\hl{#1}} 

% small caps formatting of model names
\newcommand{\flantfive}{\textsc{FLAN-T5}}
\newcommand*{\eg}{e.g.\@\xspace}

% \usepackage{todonotes}

% \newcommand{\conditionalprintbibliography}[1]{
%   \begingroup
%     \def\do#1{\ifcsname blx@bsee@#1\endcsname\else
%       \global\cslet{blx@bsee@#1}\relax
%       \printbibliography[heading=subbibintoc, segment=#1, resetnumbers=false]
%     \fi}
%     \do{#1}
%   \endgroup
% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\maya}[1]{{\noindent\color{blue}[Maya: #1]}}
\newcommand{\name}[1][]{\textsc{CompRx}}

\makeatletter
\renewcommand\AB@authnote[1]{}
\renewcommand\AB@affilnote[1]{}
\makeatother

\usepackage{anyfontsize} % allows for precise control over title font size
\usepackage{titlesec}
\titleformat{\section}{\normalfont\Large\bfseries}{\thesection}{1em}{#1}

% -------------  Title  ---------------------- 
\title{{\fontsize{16.5pt}{15.5pt}\selectfont MedVAE: Efficient Automated Interpretation of Medical Images with Large-Scale Generalizable Autoencoders}}


% -------------  Authors ----------------------

\author[]{Maya Varma$^{1,*}$, Ashwin Kumar$^{1,*}$, Rogier van der Sluijs$^{1,*}$, Sophie Ostmeier$^{1}$, Louis Blankemeier$^{1}$, Pierre Chambon$^{1}$, Christian Bluethgen$^{1}$, Jip Prince$^{2}$, Curtis Langlotz$^{1}$, Akshay Chaudhari$^{1}$}

\affil{\footnotesize{$^1$Stanford Center for Artificial Intelligence in Medicine and Imaging, Stanford University, Palo Alto, CA, USA. $^2$UMC Utrecht, Utrecht, Netherlands}}

% \affil{}
\renewcommand{\correspondingauthor}[1]{$\ast$~Equal contributions.}
\usepackage{xcolor}
\usepackage{hyperref}

\hypersetup{
    colorlinks=true,   % Enables colored links
    urlcolor=blue      % Ensures URLs are blue
}
\usepackage[noabbrev,capitalize]{cleveref}


\begin{document}
\begin{abstract}
Medical images are acquired at high resolutions with large fields of view in order to capture fine-grained features necessary for clinical decision-making. Consequently, training deep learning models on medical images can incur large computational costs. In this work, we address the challenge of downsizing medical images in order to improve downstream computational efficiency while preserving clinically-relevant features. We introduce \textit{MedVAE}, a family of six large-scale 2D and 3D autoencoders capable of encoding medical images as downsized latent representations and decoding latent representations back to high-resolution images. We train MedVAE autoencoders using a novel two-stage training approach with 1,052,730 medical images. Across diverse tasks obtained from 20 medical image datasets, we demonstrate that (1) utilizing MedVAE latent representations in place of high-resolution images when training downstream models can lead to efficiency benefits (up to 70x improvement in throughput) while simultaneously preserving clinically-relevant features and (2) MedVAE can decode latent representations back to high-resolution images with high fidelity. Our work demonstrates that large-scale, generalizable autoencoders can help address critical efficiency challenges in the medical domain. Our code is available at \href{https://github.com/StanfordMIMI/MedVAE}{https://github.com/StanfordMIMI/MedVAE}. 
\end{abstract}

\maketitle


\vspace{10mm}
% \begin{refsegment}
% \defbibfilter{notother}{not segment=\therefsegment}
\nolinenumbers
\input{sections/1_intro}
\clearpage
\input{sections/2_results}
\clearpage
\input{sections/3_discussion}
\clearpage
\input{sections/5_method}
\clearpage

\subsection*{Acknowledgments}
MV is supported by graduate fellowship awards from the Department of Defense (NDSEG), the Knight-Hennessy Scholars program at Stanford University, and the Quad program. AK is supported by graduate fellowships from Tau Beta Pi and the Knight-Hennessy Scholars program at Stanford University. RS was supported by the Rubicon fellowship of the Dutch National Research Council (NWO). This work was supported in part by NIH grants R01 HL155410, R01 HL157235, by AHRQ grant R18HS026886, and by the Gordon and Betty Moore Foundation. CL is supported by the Medical Imaging and Data Resource Center (MIDRC), which is funded by the National Institute of Biomedical Imaging and Bioengineering (NIBIB) under contract 75N92020C00021 and through The Advanced Research Projects Agency for Health (ARPA-H). AC is supported by NIH grants R01 HL167974, R01HL169345, R01 AR077604, R01 EB002524, R01 AR079431, P41 EB027060, AY2AX000045, and 1AYSAX0000024-01; and NIH contracts 75N92020C00008 and 75N92020C00021. 

\subsection*{Author contributions}
M.V., A.K., and R.S. designed the study, constructed models, and performed technical evaluations. R.S., C.B., and J.P. carried out the reader study. M.V., A.K., R.S., S.O., L.B., and P.C. collected data and analyzed model performance. All authors contributed to technical discussions and the drafting and revision of the manuscript. C.L. and A.C. supervised, funded, and guided the research.

\clearpage
\nolinenumbers
% \setlength\bibitemsep{3pt}
\bibliographystyle{plain} 
\bibliography{main}      
% \end{refsegment}
\input{sections/6_appendix}

\end{document}
