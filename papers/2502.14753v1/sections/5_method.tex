\section{Methods}
\subsection{Background}
In this section, we provide background information on autoencoders. 

2D autoencoding methods can be formulated as follows. We begin with a training dataset $\mathcal{D} = \{x_i\}_{i=1}^N$ consisting of $N$ high-resolution input images $x_i \in \mathcal{X}$. Each high-resolution image $x_i$ has dimensions $H \times W$ with $B$ channels, which can be expressed as $x_i \in \mathbb{R}^{H \times W \times B}$. An autoencoding method learns an encoding function $g: \mathcal{X} \rightarrow \mathcal{Z}$, where $\mathcal{Z}$ represents a low-dimensional latent space and $z_i \in \mathcal{Z}$ represents the downsized latent representation corresponding to the input $x_i$. Let $f$ represent the downsizing factor applied to the 2D area of the image; then, the latent representation $z_i$ can be expressed as $z_i \in \mathbb{R}^{(H/(\sqrt{f}) \times (W/\sqrt{f}) \times C}$, where $C$ is a pre-specified number of latent channels. Autoencoding methods also learn a decoding function $h: \mathcal{Z} \rightarrow \hat{\mathcal{X}}$, which reconstructs the image $\hat{x_i}$ from the latent representation $z_i$. The encoding and decoding functions $g$ and $h$ are optimized in an end-to-end manner with the goal of maximizing perceptual similarity between $x_i$ and $\hat{x_i}$.

3D autoencoding methods follow a similar formulation, where each image $x_i$ represents a 3D volume with dimensions $H \times W \times S$ with $B$ channels. Here, the downsizing factor $f$ is applied to the 3D volume of the image; as a result, the latent representation $z_i$ can be expressed as $z_i \in \mathbb{R}^{(H/(\sqrt[3]{f}) \times (W/\sqrt[3]{f}) \times (S/(\sqrt[3]{f}) \times C}$, where $C$ is a pre-specified number of latent channels. 

\subsection{Curating a large-scale training dataset}
We first collect a large-scale, open-source training dataset $\mathcal{D}$ for training medical image autoencoders. We incorporate diverse modalities and anatomical features in order to ensure that trained autoencoders gain proficiency in processing the wide variety of diagnostic features that occur in medical images. Our dataset consists of 1,021,356 2D images and 31,374 3D images obtained from 19 multi-institutional, open-source datasets.

2D images include chest X-rays and FFDMs, selected because (a) chest X-rays are well-studied with large amounts of publicly-available data and (b) FFDMs are a challenging class of images due to large dimensions and the presence of fine-grained features critical for diagnoses (e.g. microcalcifications). We collect images from two chest X-ray datasets and six FFDM datasets~\cite{johnson2019mimic,feng2021candid,jeong2022emory,sorkhei2021csaw,rsnamammo,nguyen2022vindrmammo,moreira2012inbreast,cai2023online}. 

3D images include head MRIs, knee MRIs, and high-resolution whole-body (head, neck, abdomen, chest, lower limb) CTs. We selected these datasets since (a) head MRIs/CTs are a commonly obtained examination, and (b) high-resolution CTs tend to contain subtle features and consume large amounts of storage. These images were curated from four T1- and T2-weighted head MRI datasets (14,296), one knee MRI dataset (3,564), two head/neck CT datasets (10,156), two whole-body CT datasets (1,434), and two chest CT datasets (1,924)~\cite{jack2008alzheimer,dagley2017harvard,insel2020a4,lamontagne2019oasis,bien2018deep,hooper2021impact,chilamkurthy2018development,wasserthal2023totalsegmentator,ji2022amos,armato2011lung,stanfordaimi_coca_2024}.

\subsection{Training autoencoders for medical images}
In this section, we discuss our two-stage approach for training generalizable autoencoders for medical images. Motivated by prior work on natural images~\cite{rombach2022high}, we elect to use variational autoencoders (VAEs) as our backbone. In the first stage of training, we optimize for reconstruction quality by maximizing perceptual similarity between the input image $x$ and the reconstructed image $\hat{x}$. Whereas existing works train autoencoders solely using this approach, the medical image domain introduces the added complexity of subtle, fine-grained features required for clinical interpretation of images; thus, we introduce a second stage of training, where the latent representation space $\mathcal{Z}$ is refined with continued fine-tuning. Our approach is intended to explicitly preserve diverse clinically-relevant features in both latent representations and reconstructed images. In total, the MedVAE family includes four 2D VAEs and two 3D VAEs trained with various downsizing factors.

\textbf{Stage 1: Training Base Autoencoders} (Fig.~\ref{fig:method}a). 
We begin by performing base training of the autoencoders using the collected 2D images in order to optimize the quality of reconstructions $\hat{x}$. In line with prior work~\cite{rombach2022high}, each MedVAE autoencoder learns an encoder and decoder (corresponding to functions $g$ and $h$) end-to-end using a fully convolutional VAE. Each MedVAE autoencoder accepts single-channel, high-resolution medical images $x_i$ as input, applies function $g$ to transform the input to a downsized latent representation $z_i$, and then applies function $h$ to reconstruct the original image $\hat{x_i}$. MedVAE models are characterized by two hyperparameters: $f$, which represents the downsizing factor applied to the 2D area of the input image, and $C$, which describes the number of channels included in the latent representation. For instance, given an input image $x_i$ of size $H \times W \times 1$, a MedVAE model with $f = 16$ and $C = 3$ would generate a latent representation $z_i$ of size $(H/4) \times (W/4) \times 3$, downsizing the image area by 16x and adding two additional channels. The reconstructed image $\hat{x_i}$ would be of size $H \times W \times 1$.

In order to learn functions $g$ and $h$, the VAE is trained to maximize the similarity between $x_i$ and $\hat{x_i}$ using a perceptual loss term~\cite{lpips} and a patch-based adversarial objective~\cite{isola2018patchgan}. Additionally, in order to ensure preservation of clinically-relevant features within the reconstructed image, we introduce a domain-specific embedding consistency loss based on BiomedCLIP, a pretrained vision-language foundation model trained on a large corpus of paired medical image-text data~\cite{zhang2023biomedclip}. During training, we apply an $L_2$ penalty between BiomedCLIP embeddings corresponding to the input image $x_i$ and the reconstructed image $\hat{x_i}$. This loss function is inspired by prior work on developing autoencoders for chest X-rays~\cite{lee2023llmcxr}. Finally, in addition to the loss functions listed above, a KL-divergence penalty is applied to the latent sample in order to pull latents towards a standard normal; the penalty is assigned a low weight of 1e-6. 

We use the above loss functions and the curated dataset of one million 2D images to train the following four base autoencoders, trained across various downsizing factors and latent channels. Implementation details for each base model is described below:
\begin{itemize}
\item \textbf{2D Base Autoencoder (Stage 1) with $f=16$ and $C=1$}: This autoencoder yields latent representations $z_i$ of size $(H/4) \times (W/4) \times 1$. Stage 1 training is performed from scratch. The VAE is trained solely with the perceptual loss, the KL-divergence penalty, and the BiomedCLIP embedding consistency loss for the first 3125 steps; then, the patch-based adversarial objective is applied. We train for 100K steps using 8 NVIDIA A100 GPUs and a batch size of 32. 
\item \textbf{2D Base Autoencoder (Stage 1)  with $f=16$ and $C=3$}: This autoencoder yields latent representations $z_i$ of size $(H/4) \times (W/4) \times 3$. We first initialize the VAE with weights from a previously-developed natural image autoencoder (KL-VAE)~\cite{rombach2022high}. Then, we perform Stage 1 training using LoRA~\cite{hu2021lora} with rank=4 applied to all 2D convolutional layers. We train with all four loss functions for 50k steps using 8 A100 GPUs and a batch size of 32. 
\item \textbf{2D Base Autoencoder (Stage 1)  with $f=64$ and $C=1$}: This autoencoder yields latent representations $z_i$ of size $(H/8) \times (W/8) \times 1$. Stage 1 training is performed from scratch. The VAE is trained solely with the perceptual loss, the KL-divergence penalty, and the BiomedCLIP embedding consistency loss for the first 3125 steps; then, the patch-based adversarial objective is applied. We train for 100K steps using 8 NVIDIA A100 GPUs and a batch size of 32. 
\item \textbf{2D Base Autoencoder (Stage 1)  with $f=64$ and $C=4$}: This autoencoder yields latent representations $z_i$ of size $(H/8) \times (W/8) \times 4$. We first initialize the VAE with weights from a previously-developed natural image autoencoder (KL-VAE)~\cite{rombach2022high}. Then, we perform Stage 1 training using LoRA~\cite{hu2021lora} with rank=4 applied to all 2D convolutional layers. We train with all four loss functions for 50k steps using 8 A100 GPUs and a batch size of 32. 
\end{itemize}
 

\textbf{Stage 2: Preserving Clinically-Relevant Features Across Modalities} (Fig.~\ref{fig:method}b). After performing base training of the autoencoders using the collected 2D images, we introduce a second stage of training intended to further refine the latent space such that clinically-relevant features are preserved across various modalities. 

In the context of 2D imaging modalities, the second training stage takes the form of a lightweight fine-tuning procedure designed to maximize consistency in clinically-relevant features between the input image and the latent representation. Our key insight here is that image embeddings generated by BiomedCLIP~\cite{zhang2023biomedclip} can effectively capture clinically-relevant features in 2D medical images, suggesting utility as a guidance mechanism during training. We freeze all parameters in the encoder and decoder of the VAE. During training, the input image $x_i$ is passed through the frozen VAE encoder to generate the latent representation $z_i$; then, $z_i$ is passed through a series of lightweight, trainable projection layers, which yield an output representation $\Bar{z_i}$ with the same size as $z_i$. Let the function $b(\cdot)$ represent the BiomedCLIP embedding function. We optimize the projection layer weights using a domain-specific embedding consistency loss, which takes the form of an $L_2$ loss between $b(x_i)$ and $b(\Bar{z_i})$. All downstream evaluations of latent representation quality are performed with the projected latent $\Bar{z_i}$. We perform Stage 2 training using the curated 2D training dataset with one million images. Our procedure yields four 2D MedVAE autoencoders with various downsizing factors and number of latent channels:
\begin{itemize}
\item \textbf{2D MedVAE with $f=16$ and $C=1$}: The projection layers generate $\Bar{z_i}$ of size $(H/4) \times (W/4) \times 1$. Stage 2 training is performed for 50K steps using 8 NVIDIA A100 GPUs and a batch size of 32. 
\item \textbf{2D MedVAE with $f=16$ and $C=3$}: The projection layers generate $\Bar{z_i}$ of size $(H/4) \times (W/4) \times 3$. Stage 2 training is performed for 50K steps using 8 NVIDIA A100 GPUs and a batch size of 32. 
\item \textbf{2D MedVAE with $f=64$ and $C=1$}: The projection layers generate $\Bar{z_i}$ of size $(H/8) \times (W/8) \times 1$. Stage 2 training is performed for 60K steps using 8 NVIDIA A100 GPUs and a batch size of 32. 
\item \textbf{2D MedVAE with $f=64$ and $C=4$}: The projection layers generate $\Bar{z_i}$ of size $(H/8) \times (W/8) \times 4$. Stage 2 training is performed for 50K steps using 8 NVIDIA A100 GPUs and a batch size of 32. 
\end{itemize}

In the context of 3D imaging modalities (e.g. CT scans, MRIs), the second training stage involves lifting the 2D VAE architecture to 3D using a kernel centering inflation strategy~\cite{zhang2022adapting}; we then continue training with 3D images. We note here that using external 2D medical foundation models like BiomedCLIP to enforce feature consistency is inadequate for 3D settings. As a result, we instead implement a training procedure focused on maximizing perceptual similarity, analogous to 2D stage 1 training. We train the 3D autoencoders using random cubic patches of size $64 \times 64 \times 64$. The perceptual loss and the patch-based adversarial objective are calculated per-slice, with the final loss term computed as the mean across all slices in the volume. Following such a training strategy, a 3D MedVAE model with $f = 64$, $C = 1$, and input image $x_i$ of size $H \times W \times S \times 1$ would generate a latent representation $z_i$ of size $(H/4) \times (W/4) \times (S/4) \times 1$, downsizing the volume by 64x. We perform Stage 2 training using the curated dataset of 31,374 3D images. Our procedure yields two 3D MedVAE autoencoders across various downsizing factors: 
\begin{itemize}
\item \textbf{3D MedVAE with $f=64$ and $C=1$}: The latent representations $z_i$ are of size $(H/4) \times (W/4) \times (S/4) \times 1$. We initialize the VAE with weights from 2D Base Autoencoder (Stage 1) with $f=16$ and $C=1$. We then train the VAE for 35K steps using 4 NVIDIA A6000 GPUs and a batch size of 32.
\item \textbf{3D MedVAE with $f=512$ and $C=1$}: The latent representations $z_i$ are of size $(H/8) \times (W/8) \times (S/8) \times 1$. We initialize the VAE with weights from 2D Base Autoencoder (Stage 1) with $f=64$ and $C=1$. We then train the VAE for 140K steps using 1 NVIDIA A6000 GPU and a batch size of 8. Both 3D MedVAEs are trained for the same number of steps when accounting for batch size. 
\end{itemize}

We analyze the effects of each stage of training on latent representation quality in Appendix Table~\ref{table:ablations2d} and Appendix Table~\ref{table:ablations3d}. 


\subsection{Evaluating latent representations}
We evaluate the quality of latent representations $z$ with a set of eight clinically-relevant CAD tasks, which directly evaluate the preservation of clinically-relevant features in 2D and 3D images (Appendix Table~\ref{table:clssummary}). For each CAD task, we measure the difference in classification performance between models trained using latent representations and those trained using original, high-resolution images; this serves as an indicator of latent quality by directly measuring the retention of important diagnostic features. These evaluations also provide insights into potential performance gains afforded by training downstream models directly on MedVAE latent representations rather than high-resolution images.

Below, we provide implementation details for each 2D CAD task.
\begin{enumerate}
    \item  \textbf{Malignancy Detection:} We evaluate the quality of FFDM latent representations on a binary malignancy detection task, which involves predicting the presence or absence of a malignancy. We use images from the Chinese Mammography Dataset (CMMD), which includes a total of 5202 deidentified FFDMs from 1775 patients~\cite{cai2023online, cmmddata}. CMMD includes labels indicating the presence of masses and calcifications as well as biopsy-confirmed labels indicating benign and malignant findings. We assigned 80\% of patients to the training set (1420 patients with 2982 images) and the remaining 20\% to the test set (355 patients with 762 images). The average size of an FFDM after preprocessing was $1999.2 \times 793.9 \times 1$. In order to maintain consistent sizing, we downsized each FFDM to $1024 \times 512 \times 1$ using bicubic interpolation. 
    \item \textbf{Calcification Detection:} We evaluate the quality of FFDM latent representations on a binary calcification detection task, which involves identifying the presence or absence of breast calcifications. We use the CMMD dataset, described in detail above~\cite{cmmddata, cai2023online}. We preprocessed the CMMD dataset by assigning 80\% of patients to the training set (1420 patients with 4156 images) and 20\% of patients to the test set (355 patients with 1046 images). 
    \item \textbf{BI-RADS Classification:} We evaluate the quality of FFDM latent representations on Breast Imaging Reporting and Data System (BI-RADS) classification. We use images from the VinDR-Mammo dataset, which includes a total of 20,000 deidentified FFDMs from 5000 studies collected from Hanoi Medical University Hospital and Hospital 108 in Vietnam~\cite{nguyen2022vindrmammo}. BI-RADS scores evaluate the likelihood of cancer on an integer scale from 0 to 6~\cite{nguyen2022vindrmammo}. We use the provided data splits for VinDR-Mammo, which assign 16,000 images to the training set and 4000 images to the test set. There are no images with BI-RADS scores of 0 or 6. The average size of an FFDM after preprocessing was $2607.3 \times 948.6 \times 1$. In order to maintain consistent sizing across the dataset, we downsized each X-ray to $1024 \times 512 \times 1$. 
    \item \textbf{Bone Age Prediction:} We evaluate the quality of musculoskeletal X-ray latent representations on a bone age prediction task. We use images from the RSNA Bone Age dataset, which includes 14,036 hand radiographs collected from Children’s Hospital Colorado and Lucile Packard Children’s Hospital at Stanford University~\cite{rsnaboneage}. We use the provided data splits for the RSNA Bone Age dataset, which assign 12,611 images to the training set and 1425 images to the test set.  The average size of a musculoskeletal X-ray after preprocessing was $1665.4 \times 1319.8 \times 1$. In order to maintain consistent sizing across the dataset, we downsized each X-ray to $1024 \times 1024 \times 1$.
    \item \textbf{Pediatric Wrist Fracture Detection:} We evaluate the quality of musculoskeletal X-ray latent representations on a binary wrist fracture detection task. We use images from the GRAZPEDWRI-DX dataset, which includes a total of 20,327 deidentified images from 6,091 patients collected at University Hospital Graz in Austria~\cite{Nagy2022wristfrac}. We preprocessed the GRAZPEDWRI-DX dataset by first using provided labels to remove all samples with metal hardware and casts, which may exhibit spurious correlations with the target labels. We then assigned 75\% of patients to the training set (4281 patients with 10,511 images) and the remaining 25\% to the test set (1428 patients with 3602 images). The average size of a musculoskeletal X-ray after preprocessing was $987.8 \times 537.7 \times 1$. In order to maintain consistent sizing across the dataset, we resized each X-ray to $1024 \times 512 \times 1$.
\end{enumerate}

We perform each 2D CAD task listed above using a pretrained HRNet\_w64 neural network implemented in the \texttt{timm} Python package~\cite{wang2020hrnet,timm}. HRNets are a type of convolutional neural network adapted for classification of high-resolution images. We preprocess latent representations by applying the mean operation across the channel dimension if more than one channel is present. We train the HRNet on 2 A100 GPUs using supervised linear probing with one output class. We train for 100 epochs using a batch size of 256, an AdamW optimizer~\cite{adamw} with an initial learning rate of 1e-4, and cross-entropy loss. Classification performance is measured on the test set using the final model checkpoint. We report AUROC for binary classification tasks and Macro AUROC for multi-class classification tasks. 

Below, we provide implementation details for each 3D CAD task.
\begin{enumerate}
    \item \textbf{Spine Fracture Detection:} We evaluate the quality of Spine CT latent representations on a binary spine fracture detection task. We use images from the VerSe 2019 dataset~\cite{loffler2020vertebral}, which includes 160 high-resolution, 1-mm isotropic or in sagittal 2-mm to 3-mm series of 1-mm in-plane resolution, spine CT images. The training, validation, and testing split (50/25/25) was maintained from the original dataset. The final size of a volume after preprocessing was $224 \times 224 \times 160$. 
    \item \textbf{Head Fracture Detection:} We evaluate the quality of head CT latent representations on a binary head fracture detection task. We use images from the CQ500 dataset~\cite{chilamkurthy2018development}, which includes 378 head CT images. This dataset was curated by the Centre for Advanced Research in Imaging, Neurosciences, and Genomics (CARING) in New Delhi, India. Images were divided into training and testing sets following an 80/20 split. The final size of a volume after preprocessing was $224 \times 224 \times 44$. 
    \item \textbf{ACL and Meniscal Tear Detection:} We evaluate the quality of knee MRI latent representations on a binary ACL or meniscal tear detection task. We use images from the MRNet dataset~\cite{bien2018deep}, which includes 1250 sagittal knee MRI scans performed at Stanford University Medical Center between 2001-2012. A positive label in this context may indicate the presence of an ACL tear, a meniscal tear, or both simultaneously. The dataset was split into a training and test set (95/5). The final size of a volume after preprocessing was $56 \times 256 \times 256$.
\end{enumerate}

We perform each 3D CAD task listed above using the MONAI SEResNet-152~\cite{hu2018squeeze} architecture. We implemented a weighted sampling strategy for the head fracture detection and ACL and meniscal tear detection tasks due to class imbalance. We trained the SEResNet-152 on an A6000 GPU using supervised linear probing with 1 output class. We trained for 100 epochs with a batch size of 20 for latents, a batch size of 10 for the original images, an AdamW optimizer~\cite{adamw} with an initial learning rate of 1e-4, and binary cross-entropy loss. Classification performance (AUROC) is measured on the test set using the final model checkpoint.

\subsection{Evaluating reconstructed images}
We evaluate the quality of reconstructions $\hat{x}$ using both automated and manual perceptual quality evaluations. Perceptual quality assessments measure information loss resulting from the autoencoding process by comparing the original image to the reconstructed (decoded) image. These evaluations quantify the extent to which the encoding and subsequent decoding process retains relevant features.

For 2D images, we evaluate full-image perceptual quality on chest X-rays, FFDMs, and musculoskeletal X-rays; we also evaluate fine-grained perceptual quality on musculoskeletal X-rays. Chest X-rays are obtained from CANDID-PTX~\cite{feng2021candid} and MIMIC-CXR~\cite{johnson2019mimic}; FFDMs are obtained from RSNA Mammography~\cite{rsnamammo}, VinDR-Mammo~\cite{nguyen2022vindrmammo}, CSAW-CC~\cite{sorkhei2021csaw}, EMBED~\cite{jeong2022emory}, CMMD~\cite{cai2023online}, and INBreast~\cite{moreira2012inbreast}; musculoskeletal X-rays are obtained from GRAZPEDWRI-DX~\cite{Nagy2022wristfrac}. We compute two standard perceptual quality metrics: PSNR and MS-SSIM. For 2D fine-grained perceptual quality evaluations, we extract 7677 images containing fractures from GRAZPEDWRI-DX, and we use bounding boxes provided by the authors to isolate the region of the fracture~\cite{Nagy2022wristfrac}. We then compute PSNR scores on these regions.

For 3D full-volume perceptual quality evaluations, we evaluate full-image perceptual quality on head MRIs, head CTs, abdomen CTs, whole-body CTs, lung CTs, and knee MRIs. Head MRIs are obtained from Alzheimer's Disease Neuroimaging Initiative (ADNI)~\cite{jack2008alzheimer}, Harvard Aging Brain Study (HABS)~\cite{dagley2017harvard}, A4 dataset~\cite{insel2020a4}, and Open Access Series of Imaging Studies (OASIS) brain dataset~\cite{lamontagne2019oasis}; head CTs are obtained from CQ500~\cite{chilamkurthy2018development}; whole-body CTs are obtained from TotalSegmentator dataset~\cite{wasserthal2023totalsegmentator}; abdomen CTs are obtained from the Abdominal Multi-Organ Segmentation (AMOS) dataset~\cite{ji2022amos}; lung CTs are obtained from LIDC-IDRI~\cite{armato2011lung}; and knee MRIs are obtained from MRNet~\cite{bien2018deep}. For each volume, a center crop of volume dimensions $160 \times 160 \times 160$ was extracted. For the AMOS and CQ500 datasets, the crop region was expanded to dimensions $320 \times 320 \times 160$ to include both soft-tissue and bony features. We compute two standard perceptual quality metrics: PSNR and MS-SSIM. 

For manual evaluations of reconstructed image quality, we perform a reader study with 3 radiologists. Each expert reader is presented with a pair of chest X-rays, consisting of an original high-resolution image $x$ on the left and a reconstructed image $\hat{x}$ on the right (Appendix Fig.~\ref{fig:readerstudyui}). A total of 50 unique chest X-rays with fractures, randomly sampled from CANDID-PTX, are selected and presented in a randomized order~\cite{feng2021candid}. The reader study poses three distinct questions on image fidelity, preservation of clinically-relevant features, and the presence of artifacts. Each question is scored based on a 5-point Likert scale ranging between -2 and 2. Below, we provide additional details on each of these questions: 
\begin{enumerate}
    \item \textbf{Image Fidelity:} This question aims to assess how closely the reconstructed CXR image resembles the original image in terms of image fidelity considering the overall similarity, level of detail preservation, and visual quality. A higher rating indicates a closer resemblance to the original image, while a lower rating implies a greater deviation or degradation.
    \item \textbf{Preservation of clinically-relevant features:} This question evaluates the extent to which the reconstructed chest X-rays image preserves the diagnostic information present in the original image given the clarity and visibility of anatomical structures, abnormalities, and other important diagnostic features. A higher rating indicates a greater preservation of diagnostic information, while a lower rating suggests a significant loss that may affect the accuracy of diagnosis.
    \item \textbf{Presence of Artifacts:} This question focuses on the presence and impact of artifacts in the reconstructed chest X-ray. Artifacts can include image distortions, noise, blurring, or other visual anomalies (ie. hallucinations) that are not present in the original image. A higher rating suggests less or no interference from artifacts, while a lower rating suggests a greater occurrence of artifacts.
\end{enumerate}


\subsection{Statistical analysis}
For latent representation evaluations, we report classification performance using AUROC, calculated using the \texttt{torchmetrics} library. We report mean and standard deviations across three runs with different random seeds. For automated perceptual quality evaluations on 2D images, we calculate PSNR and MS-SSIM on a random sample of 1000 images for each image type; we report mean and standard deviations across four runs with different random seeds. For automated perceptual quality evaluations on 3D images, we calculate PSNR and MS-SSIM on a single random sample of 100 images for each image type. For manual perceptual quality evaluations with expert readers, we report mean scores and 95\% confidence intervals across three readers.




