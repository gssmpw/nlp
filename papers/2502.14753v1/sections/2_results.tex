\section{Results}
\subsection{Training MedVAE autoencoders}

Autoencoding methods are capable of encoding high-resolution images as downsized latent representations. For a given 2D input image with dimensions $H \times W$ with $B$ channels, an autoencoding method will output a downsized latent representation of size $H/(\sqrt{f}) \times (W/\sqrt{f}) \times C$. Here, $f$ represents the downsizing factor applied to the 2D area of the image and $C$ represents a pre-specified number of latent channels. 3D autoencoding methods follow a similar formulation, where input images are 3D in nature with dimensions $H \times W \times S$ with $B$ channels. Here, the downsizing factor $f$ is applied to the 3D volume of the image; as a result, the latent representation will have dimensions $(H/(\sqrt[3]{f}) \times (W/\sqrt[3]{f}) \times (S/(\sqrt[3]{f}) \times C$. Autoencoding methods are also capable of decoding latent representations back to reconstructed high-resolution images. 

We aim to develop large-scale, generalizable medical image autoencoders capable of preserving diverse clinically-relevant features in both latent representations and reconstructions. To this end, we first collect a large-scale training dataset with 1,021,356 2D images and 31,374 3D images curated from 19 multi-institutional, open-source datasets~\cite{johnson2019mimic,feng2021candid,jeong2022emory,sorkhei2021csaw,rsnamammo,nguyen2022vindrmammo,moreira2012inbreast,cai2023online,jack2008alzheimer,dagley2017harvard,insel2020a4,lamontagne2019oasis,bien2018deep,hooper2021impact,chilamkurthy2018development,wasserthal2023totalsegmentator,ji2022amos,armato2011lung,stanfordaimi_coca_2024}. Images are obtained from two chest X-ray datasets, six full-field digital mammogram (FFDM) datasets, four T1- and T2-weighted head magnetic resonance imaging (MRI) datasets, one knee MRI dataset, two head/neck CT datasts, two whole-body CT datasets, and two chest CT datasets.

% ******* Figure ********
\begin{figure}[ht]
\centering
\includegraphics[width=\textwidth, trim=0 0 0 0]{figures/method.pdf}
\caption{\textbf{Overview of training pipeline and evaluation tasks for MedVAE, a suite of large-scale autoencoders for medical images.} \textbf{a,} We train MedVAE autoencoders using a two-stage process. The first stage involves training base autoencoders using 2D images. \textbf{b,} The second stage of training aims to further refine the latent space such that clinically-relevant features are preserved across modalities. We introduce separate training procedures for 2D images (e.g. X-rays, mammograms) and 3D volumes (e.g. CT scans, MRI). \textbf{c,} We evaluate medical image autoencoders with respect to latent representation quality, storage and efficiency benefits arising from using latent representations rather than high-resolution images in downstream CAD pipelines, and reconstructed image quality.}
\label{fig:method}
\end{figure}


We then utilize this dataset to train a family of generalizable autoencoders for medical images. Motivated by prior work on natural images~\cite{rombach2022high}, we utilize variational autoencoders (VAEs) as the model backbone. We perform model training using a novel two-stage training scheme designed to optimize quality of latent representations and decoded reconstructions. Specifically, the first stage involves training base autoencoders using 2D images (Fig.~\ref{fig:method}a); we maximize the perceptual similarity between input images and reconstructed images using a perceptual loss~\cite{lpips}, a patch-based adversarial objective~\cite{isola2018patchgan}, and a domain-specific embedding consistency loss. Whereas existing works on autoencoders train using only this stage, the medical image domain introduces the added complexity of subtle, fine-grained features required for clinical interpretation; thus, we introduce a second stage of training, which aims to further refine the latent space such that clinically-relevant features are preserved across various modalities (Fig.~\ref{fig:method}b). Specifically, in the context of 2D imaging modalities (e.g. X-rays, mammograms), the second training stage takes the form of a lightweight training approach that leverages the embedding space of BiomedCLIP, a recently-developed medical vision-language foundation model~\cite{zhang2023biomedclip}, to enforce feature consistency between input images and latent representations. In the context of 3D imaging modalities (e.g. CT scans, MRI), the second training stage involves lifting the 2D autoencoder architecture to 3D and performing continued fine-tuning with 3D images. In Appendix Table~\ref{table:ablations2d} and Appendix Table~\ref{table:ablations3d}, we analyze the effects of each stage of training on latent representation quality. In total, the MedVAE family includes 4 large-scale 2D autoencoders and 2 large-scale 3D autoencoders trained with various downsizing factors $f$ and latent channels $C$. 

In order to assess the capabilities of MedVAE, we evaluate the extent to which latent representations and reconstructed images generated by MedVAE autoencoders can contribute to downstream storage and efficiency benefits while simultaneously preserving clinically-relevant features (Fig.~\ref{fig:method}c). Specifically, we assess (1) whether downsized latent representations can effectively replace high-resolution images in CAD pipelines while maintaining performance; (2) whether latent representations can reduce storage requirements and improve downstream efficiency; and (3) whether decoded reconstructions effectively preserve clinically-relevant features necessary for radiologist interpretation. 

\subsection{Latent representation quality}


\begin{table*}[t]
% \scriptsize

\centering
\resizebox{\linewidth}{!}{%
{%\renewcommand{\arraystretch}{1.2}%
\begin{tabular}{ lcccccccc }
\toprule
\textbf{}
& \multicolumn{2}{c}{\textbf{}}
& \multicolumn{5}{c}{\textbf{AUROC} $\uparrow$}
& \textbf{}
\\
\cmidrule(l{3pt}r{0pt}){4-8}
\cmidrule(l{3pt}r{0pt}){9-9}
% "
\textbf{Method}
& \textbf{$f$}
& \textbf{$C$}
& \small Malignancy
& \small Calcification
& \small BI-RADS
& \small Bone Age
& \small Wrist Fracture
& Average
\\ 
 & & &  \small (FFDM) & \small  (FFDM) & \small  (FFDM) &  (X-ray) &  (X-ray)
\\
\midrule
\small High-Resolution & 1 & 1 & \textbf{66.1$_{\pm0.5}$} & \textbf{62.4$_{\pm0.6}$} & \textbf{63.4$_{\pm0.1}$} & \textbf{80.2$_{\pm0.1}$}  & \textbf{73.7$_{\pm0.0}$} & \textbf{69.2}\\

\midrule
\small Nearest & 16 & 1   &  65.5$_{\pm0.1}$ & 59.7$_{\pm0.3}$ & 62.4$_{\pm0.1}$ & \textcolor{blue}{81.6$_{\pm0.1}$}  & 70.5$_{\pm0.0}$ & 67.9\\
\small Bilinear & 16 & 1    &65.5$_{\pm0.1}$ & 58.1$_{\pm0.3}$ & 61.1$_{\pm0.2}$ & \textcolor{blue}{81.6$_{\pm0.0}$}  & \textbf{71.2$_{\pm0.1}$} & 67.5 \\
\small Bicubic & 16 & 1   &  65.5$_{\pm0.4}$ & 58.5$_{\pm0.5}$ & 61.1$_{\pm0.0}$ & \textcolor{blue}{81.8$_{\pm0.2}$}  & 71.1$_{\pm0.1}$ & 67.6\\
\small KL-VAE & 16 & 3   & 59.7$_{\pm0.2}$ & 59.1$_{\pm0.3}$ & 58.5$_{\pm0.1}$ & 74.3$_{\pm0.1}$ & 64.5$_{\pm0.1}$ & 63.2\\
\small VQ-GAN & 16 & 3   & 57.4$_{\pm0.3}$  & 58.2$_{\pm0.4}$ & 62.3$_{\pm0.1}$ & 79.1$_{\pm0.2}$ & 65.8$_{\pm0.1}$ & 64.6\\
\small 2D MedVAE & 16 & 1   &  63.6$_{\pm0.6}$ & \textcolor{blue}{\textbf{63.9$_{\pm0.4}$}} & \textcolor{blue}{\textbf{65.3$_{\pm0.2}$}} & \textcolor{blue}{\textbf{84.6$_{\pm0.1}$}} & 70.3$_{\pm0.1}$ & \textcolor{blue}{\textbf{69.5}}\\
\small 2D MedVAE & 16  & 3 &  \textcolor{blue}{\textbf{66.1$_{\pm0.2}$}} &   61.7$_{\pm0.2}$ & 62.3$_{\pm0.1}$ & \textcolor{blue}{82.1$_{\pm0.1}$}  &   70.6$_{\pm0.1}$ & 68.6\\
\midrule

\small Nearest & 64 & 1    & 63.0$_{\pm0.1}$ & 58.8$_{\pm0.2}$ & 60.0$_{\pm0.2}$ & 72.1$_{\pm0.0}$  & 65.1$_{\pm0.1}$ & 63.8	\\
\small Bilinear & 64 & 1   & 61.5$_{\pm0.3}$ & 56.9$_{\pm0.4}$ & \textbf{61.3$_{\pm0.1}$} & 72.8$_{\pm0.5}$  & \textbf{67.9$_{\pm0.1}$} & 64.1\\
\small Bicubic & 64 & 1   & 61.2$_{\pm0.5}$ & 57.6$_{\pm0.4}$ & 61.1$_{\pm0.1}$ & 72.8$_{\pm0.2}$  & 67.9$_{\pm0.2}$ & 64.1\\
\small KL-VAE & 64 & 4   & 62.2$_{\pm0.7}$ &  55.8$_{\pm0.4}$ & 56.8$_{\pm0.1}$ & 65.7$_{\pm0.0}$ & 58.8$_{\pm0.0}$ & 59.9\\
\small VQ-GAN & 64 & 4    & 64.5$_{\pm0.5}$ & 57.3$_{\pm0.3}$ &  56.6$_{\pm0.1}$ & 67.6$_{\pm0.1}$  & 61.6$_{\pm0.2}$ & 61.5 \\
\small 2D MedVAE & 64 & 1  & 59.0$_{\pm0.3}$ & \textbf{59.4$_{\pm0.7}$} & 60.7$_{\pm0.1}$ & \textbf{73.5$_{\pm0.2}$} & 64.3$_{\pm0.1}$ & 63.4\\
\small 2D MedVAE & 64 & 4   & \textbf{64.9$_{\pm0.2}$} &  58.5$_{\pm0.3}$ & 60.6$_{\pm0.0}$ & 73.0$_{\pm0.2}$ & 66.7$_{\pm0.1}$ & \textbf{64.7}\\

\bottomrule
\end{tabular}
}
}
\caption{\textbf{Evaluating latent representation quality with 2D CAD tasks.} We evaluate the 2D MedVAE autoencoders on five 2D CAD tasks, and we report the mean AUROC and standard deviation across three random seeds. We compare MedVAE with three interpolation methods (nearest, bilinear, bicubic) and two natural image autoencoders (KL-VAE and VQ-GAN). Here, $f$ represents the downsizing factor applied to the 2D area of the input image and $C$ represents the number of latent channels. The best performing models on each task are bolded. We highlight methods that perfectly preserve clinically-relevant features (i.e. performance equals or exceeds performance when training with high-resolution images) in \textcolor{blue}{\textbf{blue}}.}
\label{table:image_classification}
\vspace{-1mm}
\end{table*}


\begin{table*}[ht]
% \scriptsize
\centering
{%
\begin{tabular}{ lcccccc }
\toprule
\textbf{}
& \multicolumn{2}{c}{\textbf{}}
& \multicolumn{3}{c}{\textbf{AUROC} $\uparrow$}
& \textbf{}
\\
\cmidrule(l{3pt}r{0pt}){4-6}
\cmidrule(l{3pt}r{0pt}){7-7}
\textbf{Method}
& \textbf{$f$}
& \textbf{$C$}
& \small Spine Fractures
& \small Skull Fractures
& \small Knee Injury
& Average
\\ 
 & & &  \small (CT) & \small (CT) & \small (MRI) &
\\
\midrule
\small High-Resolution & 1 & 1  & \textbf{82.9$_{\pm2.2}$} & \textbf{63.9$_{\pm6.3}$} & \textbf{69.9$_{\pm0.6}$} & \textbf{72.2}\\

\midrule
\small Bicubic & 64 & 1  &  77.3$_{\pm4.1}$ & \textcolor{blue}{64.8$_{\pm4.0}$} & 66.4$_{\pm2.3}$ & 69.5\\
\small KL-VAE & 64 & 3   & 68.8$_{\pm2.1}$ & 40.7$_{\pm9.1}$ & 63.9$_{\pm8.2}$  &  57.8\\
\small VQ-GAN & 64 & 3 & 73.2$_{\pm2.0}$  & 75.5$_{\pm14.8}$ & 63.6$_{\pm10.5}$ &   70.8 \\
\small 3D MedVAE & 64 & 1  & \textcolor{blue}{\textbf{83.7$_{\pm2.8}$}} & \textcolor{blue}{\textbf{87.0$_{\pm7.3}$}} & \textbf{68.4$_{\pm2.4}$} & \textcolor{blue}{\textbf{79.7}}\\
\midrule
\small Bicubic & 512 & 1 & \textbf{72.3$_{\pm2.2}$} & 38.4$_{\pm24.5}$ & \textbf{59.4$_{\pm2.5}$} & 56.7\\
\small KL-VAE & 512 & 4  & 67.7$_{\pm3.9}$ & 42.6$_{\pm4.0}$ & 50.9$_{\pm5.1}$ & 53.7\\
\small VQ-GAN & 512 & 4  & 68.9$_{\pm7.0}$ & 30.6$_{\pm12.5}$ & 57.4$_{\pm5.0}$ & 52.3 \\
\small 3D MedVAE & 512 & 1  & 72.0$_{\pm3.8}$ & \textbf{49.1$_{\pm19.8}$} & 58.2$_{\pm1.7}$ & \textbf{59.8} \\
\bottomrule
\end{tabular}
}
\caption{\textbf{Evaluating latent representation quality with 3D CAD tasks.} We evaluate the 3D MedVAE autoencoders on three 3D CAD tasks, and we report the mean AUROC and standard deviation across three random seeds. We compare MedVAE with one interpolation method (bicubic) and two natural image 2D autoencoders (KL-VAE and VQ-GAN). For 2D baselines, we stitch 2D latent representations together across slices such that the size of the 2D latent representation matches those generated by 3D models. Here, $f$ represents the downsizing factor applied to the 3D volume of the input image and $C$ represents the number of latent channels. The best performing models on each task are bolded. We highlight methods that perfectly preserve clinically-relevant features (i.e. performance equals or exceeds performance when training with high-resolution volumes) in \textcolor{blue}{\textbf{blue}}.}
\label{table:3D_latent_cls}
\vspace{-1mm}
\end{table*}





We first evaluate whether clinically-relevant features are preserved in MedVAE latent representations. To this end, we measure the extent to which latent representations can serve as drop-in replacements for high-resolution input images in CAD pipelines \textit{without} any customization or modifications to CAD model architectures. 

We evaluate latent representation quality using the following 8 CAD tasks: malignancy detection on 2D FFDMs~\cite{cai2023online}, calcification detection on 2D FFDMs~\cite{cai2023online}, BI-RADS prediction on 2D FFDMs~\cite{nguyen2022vindrmammo}, bone age prediction on 2D X-rays~\cite{rsnaboneage}, fracture detection on 2D wrist X-rays~\cite{Nagy2022wristfrac}, fracture detection on 3D spine CTs~\cite{loffler2020vertebral}, fracture classification on 3D head CTs~\cite{chilamkurthy2018development}, and anterior cruciate liagment (ACL) and meniscal tear detection on 3D sagittal knee MRIs~\cite{bien2018deep}. In order to perform each of these CAD tasks, a model must rely on fine-grained, clinically-relevant features.% we quantitatively evaluate this by selecting tasks where using naive image downsizing techniques (i.e. interpolation methods) contributes to degraded performance when compared to using high-resolution images. 

For each CAD task, we train a classifier (HRNet~\cite{wang2020hrnet} in 2D settings and SEResNet~\cite{hu2018squeeze} in 3D settings) on a training set consisting of latent representations. We then measure the difference in classification performance between models trained directly on latent representations and models trained using original, high-resolution images; this serves as an indicator of latent representation quality (e.g. a small performance difference indicates that the downsizing approach preserves diagnostic features). We compute AUROC for all binary tasks and macro AUROC for all multi-class tasks. We train each classifier with three random seeds, and we report results as mean AUROC $\pm$ standard deviation.

We compare MedVAE with two categories of image downsizing methods: (1) interpolation methods (nearest, bilinear, and bicubic), which are the de-facto gold standard for medical image downsizing as demonstrated by the quantity of prior work leveraging this approach~\cite{wantlin2023benchmd, Varma2019, convirt, Huang_2021_ICCV, miura2021improving, Tiu2022}, and (2) recently-introduced large-scale natural image autoencoders (KL-VAE and VQ-GAN)~\cite{rombach2022high}. Due to the fact that prior work on developing large-scale 3D autoencoders has been limited, we compare our 3D MedVAE models with 2D methods by stitching 2D latent representations together across slices such that the size of the 2D latent representation matches those generated by 3D models. 

We provide results for 2D CAD tasks in Table \ref{table:image_classification} and 3D CAD tasks in Table \ref{table:3D_latent_cls}. Our results demonstrate that the MedVAE training approach yields high-quality latent representations for both 2D and 3D images. At a downsizing factor of $f=16$, 2D MedVAE perfectly preserves clinically-relevant features on four out of five 2D classification tasks. Similarly, at a downsizing factor of $f=64$, 3D MedVAE perfectly preserves relevant clinical information on two out of three 3D classification tasks (spine and skull CT fracture detection). In these cases, performance equals or exceeds performance when training with original, high-resolution images. The average performance of 2D MedVAE with $f=16$ and 3D MedVAE with $f=64$ across all tasks also exceeds the average performance when training with high-resolution images.  

We observe that MedVAE consistently outperforms the natural image autoencoders KL-VAE and VQ-GAN on all classification tasks, demonstrating the utility of the MedVAE training procedure. On average across the five 2D classification tasks, 2D MedVAE demonstrates a 10.0\% improvement over KL-VAE at a downsizing factor of $f=16$ and a 8.0\% improvement at a downsizing factor of $f=64$. Similar trends are noted for VQ-GAN. %, with MedVAE demonstrating a 7.6\% improvement at $f=16$ and a 5.2\% improvement at $f=64$.
In particular, 2D MedVAE outperforms KL-VAE and VQ-GAN on the two musculoskeletal tasks (bone age prediction and wrist fracture detection) despite the fact that no musculoskeletal radiographs are used during MedVAE training; this suggests effective generalization to other types of medical images. On average across the three 3D tasks, 3D MedVAE demonstrates a 37.9\% improvement over KL-VAE at a downsizing factor of $f=64$ and a 11.4\% improvement over KL-VAE at a downsizing factor of $f=512$. Our findings suggest that 3D training of autoencoders leads to high-quality latent representations due to preservation of volumetric information (e.g. fractures spanning multiple slices), particularly at $f=64$. These findings are corroborated by results in Appendix Table~\ref{table:2d3dcad}, which compares performance of 2D MedVAE and 3D MedVAE on 3D CAD tasks.

Additionally, we note that MedVAE outperforms the interpolation methods across most tasks, but interpolation methods are a competitive baseline. Overall, our findings suggest that our MedVAE training procedure yields downsized latent representations that can be used as drop-in replacements for high-resolution input images in CAD pipelines. 

\subsection{Storage and efficiency benefits of latent representations}

Next, we evaluate the extent to which downsized MedVAE latent representations can reduce storage requirements and improve downstream efficiency of CAD pipelines when compared to high-resolution input images. Using a 2D high-resolution network (HRNet\_w64) and 3D squeeze-excitation network (SEResNet-152) as our base CAD architectures, we report latency, throughput, and maximum batch size. Latency is the time (in milliseconds) to perform a forward pass of the network on one batch. Throughput is the number of samples that can be evaluated by the network in one second. Finally, we report the maximum batch size (in powers of 2) for a forward pass that will fit on a single A100 GPU (2D) and an A6000 GPU (3D). We assume a high-resolution input image size of $1024 \times 1024$ with 1 channel for 2D settings and an input volume size of $256 \times 256 \times 256$ with 1 channel for 3D settings.

% ******* Figure ********
\begin{figure}[ht]
\centering
\includegraphics[width=\textwidth, trim=0 0 0 0]{figures/efficiency.pdf}
\caption{\textbf{CAD model efficiency.} Here, we compare the efficiency of CAD models trained with downsized latent representations to CAD models trained with high-resolution images. $f$ represents the downsizing factor applied to the 2D area or 3D volume of the input image. We report latency (in milliseconds), throughput (in samples per second), and the maximum batch size (in powers of 2) that will fit on one GPU.}
\label{fig:efficiency}
\end{figure}

Results are provided in Figure \ref{fig:efficiency}. We demonstrate that training CAD models directly on downsized latent representations can lead to large improvements in model efficiency. In the 2D setting, we observe that as the downsizing factor increases to $f=64$, the latency decreases by 69x, the throughput increases by 70x, and the maximum batch size increases by 32x. In the 3D setting, as the downsizing factor increases to $f=512$, the latency decreases by 62x, the throughput increases by 55x, and the maximum batch size increases by 512x. Storage costs decrease proportionally with the downsizing factor (i.e. 64x for 2D and 512x for 3D).

\subsection{Reconstructed image quality}

We evaluate whether clinically-relevant features are preserved in reconstructed images using both automated and manual perceptual quality evaluations. These evaluations quantify the extent to which the encoding and subsequent decoding processes retain relevant features.

For automated evaluations, we use perceptual metrics to compare reconstructed images with the original inputs. We report peak signal-to-noise ratio (PSNR) and the multi-scale structural similarity index measure (MS-SSIM). For 2D evaluations, we measure perceptual quality on X-rays~\cite{feng2021candid,johnson2019mimic}; FFDMs~\cite{jeong2022emory,sorkhei2021csaw,rsnamammo,nguyen2022vindrmammo,moreira2012inbreast,cai2023online}; and musculoskeletal X-rays~\cite{Nagy2022wristfrac}. In addition to full-image evaluations, we additionally include a fine-grained perceptual quality assessment, where we extract regions containing wrist fractures by using bounding boxes~\cite{Nagy2022wristfrac}; then, the original region and reconstructed region are compared using perceptual metrics. For 3D evaluations, we compute metrics on brain MRIs~\cite{jack2008alzheimer,dagley2017harvard,insel2020a4,lamontagne2019oasis}; head CTs~\cite{chilamkurthy2018development}; abdomen CTs~\cite{ji2022amos}; CTs from a wide range of anatomies~\cite{wasserthal2023totalsegmentator}; lung CTs~\cite{armato2011lung}; and knee MRIs~\cite{bien2018deep}.


\begin{table*}[t]
% \scriptsize
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{lccccccccc}
\toprule
\textbf{Method}
& \textbf{$f$}
& \textbf{$C$}
& \multicolumn{2}{c}{\textbf{Mammograms}}
& \multicolumn{2}{c}{\textbf{Chest X-rays}}
& \multicolumn{2}{c}{\textbf{Musculoskeletal X-rays}}
& \multicolumn{1}{c}{\textbf{Wrist X-rays (FG)}}
\\
\cmidrule(l{3pt}r{0pt}){4-5}
\cmidrule(l{3pt}r{0pt}){6-7}
\cmidrule(l{3pt}r{0pt}){8-9}
\cmidrule(l{3pt}r{0pt}){10-10}
% "
& 
&
& \small PSNR $\uparrow$
& \small MS-SSIM $\uparrow$
& \small PSNR $\uparrow$
& \small MS-SSIM $\uparrow$
& \small PSNR $\uparrow$
& \small MS-SSIM $\uparrow$
& \small PSNR $\uparrow$
\\ 
\midrule
\small Nearest & 16 & 1    &  25.95$_{\pm0.06}$  & 0.846$_{\pm0.00}$  & 29.87$_{\pm0.04}$ & 0.942$_{\pm0.00}$   &  24.06$_{\pm0.02}$  & 0.890$_{\pm0.00}$  & 26.11$_{\pm0.02}$\\
\small Bilinear & 16 & 1    & 30.18$_{\pm0.07}$  & 0.936$_{\pm0.00}$ & 34.23$_{\pm0.03}$ & 0.981$_{\pm0.00}$ & 28.75$_{\pm0.02}$  & 0.959$_{\pm0.00}$ & 30.92$_{\pm0.03}$ \\
\small Bicubic & 16 & 1    & 31.69$_{\pm0.07}$  & 0.961$_{\pm0.00}$ & 35.48$_{\pm0.03}$ & 0.989$_{\pm0.00}$ & 30.18$_{\pm0.02}$  & 0.974$_{\pm0.00}$ & 32.65$_{\pm0.04}$  \\
\small KL-VAE & 16 & 3  & 36.11$_{\pm0.07}$  & 0.989$_{\pm0.00}$ & 41.45$_{\pm0.04}$ & 0.996$_{\pm0.00}$ & 38.29$_{\pm0.03}$  &0.992$_{\pm0.00}$  & 36.55$_{\pm0.03}$\\
\small VQ-GAN & 16 & 3  &   35.55$_{\pm0.07}$  & 0.986$_{\pm0.00}$ & 37.80$_{\pm0.03}$ & 0.995$_{\pm0.00}$ & 36.41$_{\pm0.02}$  & 0.990$_{\pm0.00}$  & 34.19$_{\pm0.04}$\\
\small 2D MedVAE  & 16 & 1  & 32.34$_{\pm0.07}$ &  0.969$_{\pm0.00}$ & 38.44$_{\pm0.02}$ & 0.990$_{\pm0.00}$ & 33.97$_{\pm0.03}$ & 0.973$_{\pm0.00}$  &  31.97$_{\pm0.03}$ \\
\small 2D MedVAE  & 16 & 3  & \textbf{37.57}$_{\pm0.08}$ &  \textbf{0.993}$_{\pm0.00}$ & \textbf{43.55 }$_{\pm0.02}$& \textbf{0.997}$_{\pm0.00}$ & \textbf{39.41}$_{\pm0.04}$ & \textbf{0.994}$_{\pm0.00}$  &  \textbf{37.61}$_{\pm0.02}$  \\
\midrule

\small Nearest & 64 & 1   & 22.46$_{\pm0.05}$  & 0.669$_{\pm0.00}$ & 26.22$_{\pm0.03}$ & 0.858$_{\pm0.00}$ & 19.93$_{\pm0.02}$  & 0.756$_{\pm0.00}$  & 22.14$_{\pm0.04}$  \\
\small Bilinear & 64 & 1    & 26.81$_{\pm0.06}$  & 0.837$_{\pm0.00}$ & 31.18$_{\pm0.03}$ & 0.949$_{\pm0.00}$ & 24.89$_{\pm0.01}$  & 0.898$_{\pm0.00}$ & 27.12$_{\pm0.03}$  \\
\small Bicubic & 64 & 1    & 27.84$_{\pm0.06}$  & 0.874$_{\pm0.00}$ & 32.09$_{\pm0.03}$ & 0.962$_{\pm0.00}$  & 25.92$_{\pm0.01}$  & 0.922$_{\pm0.00}$  & 28.54$_{\pm0.03}$ \\
\small KL-VAE & 64 & 4    & 31.88$_{\pm0.07}$  & 0.959$_{\pm0.00}$ &36.37$_{\pm0.01}$ &0.987$_{\pm0.00}$ &33.49$_{\pm0.02}$ &0.966$_{\pm0.00}$ & 31.04$_{\pm0.03}$ \\
\small VQ-GAN & 64 & 4   & 30.13$_{\pm0.06}$  & 0.938$_{\pm0.00}$ & 34.87$_{\pm0.02}$ & 0.980$_{\pm0.00}$ & 32.00$_{\pm0.02}$  & 0.953$_{\pm0.0}$  & 29.92$_{\pm0.02}$ \\
\small 2D MedVAE  & 64 & 1   & 28.00$_{\pm0.07}$ & 0.872$_{\pm0.00}$ & 31.92$_{\pm0.04}$ &  0.962$_{\pm0.00}$ & 28.27$_{\pm0.02}$&  0.917$_{\pm0.00}$  & 28.03$_{\pm0.01}$\\
\small 2D MedVAE  & 64 & 4  & \textbf{33.13}$_{\pm0.07}$  & \textbf{0.969}$_{\pm0.00}$ & \textbf{38.88}$_{\pm0.03}$ &  \textbf{0.990}$_{\pm0.00}$ & \textbf{34.73}$_{\pm0.02}$&  \textbf{0.972}$_{\pm0.00}$& \textbf{32.30}$_{\pm0.02}$\\
\bottomrule
\end{tabular}
}
\caption{\textit{Evaluating reconstruction quality on 2D datasets.} We evaluate 2D MedVAE with perceptual quality metrics on mammograms and chest X-rays, which we classify as \textit{in-distribution}, since the MedVAE training set includes mammograms and chest X-rays. We also evaluate MedVAE on musculoskeletal X-rays and wrist X-rays (fine-grained), which we classify as \textit{out-of-distribution}. Here, $f$ represents the downsizing factor applied to the 2D area of the input image and $C$ represents the number of latent channels. The best performing models are bolded. We calculate PSNR and MS-SSIM using a random sample of 1000 images for each image type; we report mean and standard deviations across four runs with different random seeds.}
\label{table:perceptualid}
\end{table*}




\begin{table*}[t]
% \scriptsize
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{lcccccccccccccc}
\toprule
\textbf{Method}
& \textbf{$f$}
& \textbf{$C$}
& \multicolumn{2}{c}{\textbf{Brain MRIs}}
& \multicolumn{2}{c}{\textbf{Head CTs}}
& \multicolumn{2}{c}{\textbf{Abdomen CTs}}
& \multicolumn{2}{c}{\textbf{TS CTs}}
& \multicolumn{2}{c}{\textbf{Lung CTs}}
& \multicolumn{2}{c}{\textbf{Knee MRIs}}
\\
\cmidrule(l{3pt}r{0pt}){4-5}
\cmidrule(l{3pt}r{0pt}){6-7}
\cmidrule(l{3pt}r{0pt}){8-9}
\cmidrule(l{3pt}r{0pt}){10-11}
\cmidrule(l{3pt}r{0pt}){12-13}
\cmidrule(l{3pt}r{0pt}){14-15}
% "
& 
&
& \small PSNR $\uparrow$
& \small MS-SSIM $\uparrow$
& \small PSNR $\uparrow$
& \small MS-SSIM $\uparrow$
& \small PSNR $\uparrow$
& \small MS-SSIM $\uparrow$
& \small PSNR $\uparrow$
& \small MS-SSIM $\uparrow$
& \small PSNR $\uparrow$
& \small MS-SSIM $\uparrow$
& \small PSNR $\uparrow$
& \small MS-SSIM $\uparrow$
\\ 
\midrule
\small Bicubic & 16 & 1 & 29.27 & 0.975 & 36.21 & 0.996 & 33.81 & 0.989 & 27.33 & 0.972 & 28.00 & 0.973 & 26.37 & 0.986 \\
\small KL-VAE & 16 & 3 & 33.23 & {\textbf{0.994}} & 47.65 & {\textbf{1.000}} & 43.51 & 0.998 & 34.14 & 0.994 & 32.62 & {\textbf{0.989}} & 31.31 & {\textbf{0.998}} \\
\small VQ-GAN & 16 & 3 & 32.72 & 0.992 & 42.87 & 0.999 & 40.85 & 0.997 & 33.55 & 0.993 & 32.20 & {\textbf{0.989}} & 30.75 & 0.997 \\
\small 2D MedVAE  & 16 & 1 & 29.48 & 0.980 & 39.71 & 0.997 & 33.45 & 0.983 & 29.70 & 0.983 & 28.40 & 0.973 & 27.38 & 0.990 \\
\small 2D MedVAE  & 16 & 3 & {\textbf{33.99}} & {\textbf{0.994}} & {\textbf{48.56}} & {\textbf{1.000}} & {\textbf{44.95}} & {\textbf{0.999}} & {\textbf{34.83}} & {\textbf{0.995}} & {\textbf{33.34}} & {\textbf{0.989}} & {\textbf{31.52}} & 0.997 \\
\small 3D MedVAE  & 64 & 1 & 29.52 & 0.983 & 39.03 & 0.999 & 36.61 & 0.993 & 31.35 & 0.987 & 28.79 & 0.975 & 28.25 & 0.994 \\
\midrule
\small Bicubic & 64 & 1 & 26.25 & 0.911 & 30.11 & 0.980 & 28.84 & 0.955  & 24.24 & 0.914 & 24.40 & 0.928 & 24.11 & 0.956  \\
\small KL-VAE & 64 & 3 & 29.32 & {\textbf{0.977}} & 40.95 & 0.997 & 38.07 & {\textbf{0.995}} & 29.85 & 0.982 & 28.83 & 0.974 & 27.68 & {\textbf{0.993}} \\
\small VQ-GAN & 64 & 3 & 27.43 & 0.967 & 39.02 & 0.997 & 36.25 & 0.991 & 27.47 & 0.972 & 26.66 & 0.964 & 25.95 & 0.990 \\
\small 2D MedVAE  & 64 & 1 & 25.66 & 0.920 & 33.10 & 0.988 & 29.51 & 0.967  & 24.50 & 0.922 & 24.39 & 0.933 & 24.48 & 0.973 \\
\small 2D MedVAE  & 64 & 3 & {\textbf{29.34}} & 0.976 & {\textbf{41.98}} & {\textbf{0.999}} & {\textbf{39.49}} & {\textbf{0.995}} & {\textbf{30.35}} & {\textbf{0.984}} & {\textbf{29.59}} & {\textbf{0.977}} & {\textbf{28.05}} & {\textbf{0.993}} \\
\small 3D MedVAE  & 512 & 1 & 26.23 & 0.937 & 30.85 & 0.991 & 29.47 & 0.960 & 26.34 & 0.949 & 24.76 & 0.934 & 24.36 & 0.977 \\

\bottomrule
\end{tabular}
}
\caption{\textit{Evaluating reconstruction quality on 3D datasets.} We evaluate 3D MedVAE with perceptual quality metrics on head MRIs, head CTs, abdomen CTs, various high-resolution CTs (TS), lung CTs, and knee MRIs. $f$ represents the downsizing factor applied to the input volume and $C$ represents the number of latent channels. The best performing models are bolded. We compare 3D MedVAE with several 2D methods, including 2D MedVAE, KL-VAE, and VQ-GAN.}
\label{table:3dperceptual}
\end{table*}


In Table \ref{table:perceptualid}, we compare 2D MedVAE with interpolation methods and large-scale natural image autoencoders across four types of 2D images. We find that 2D MedVAE achieves the highest perceptual quality across all evaluated image types. In particular, our evaluations with wrist X-rays explore generalization of MedVAE to unseen anatomical features; notably, MedVAE achieves the highest PSNR scores on this task, despite the fact that MedVAE was not trained on musculoskeletal X-rays. We also note a general trend that increasing the number of latent channels $C$ improves perceptual quality of the reconstructed image. 

In Table \ref{table:3dperceptual}, we compare MedVAE with interpolation methods and large-scale natural image autoencoders across six types of 3D volumes. Due to the absence of existing large-scale 3D autoencoder baselines, we compare our 3D MedVAE models with 2D methods by performing downsizing on individual 2D slices and then stitching slices together to form the reconstructed 3D volume. We again find that MedVAE reconstructions demonstrate superior perceptual quality when compared to baselines. In particular, 2D MedVAE achieves the highest perceptual quality across almost all evaluated image types, despite the fact that no MRI or CT slices were included in the 2D MedVAE training set. We also observe that 3D MedVAE achieves competitive performance, despite utilizing a significantly higher downsizing factor than comparable 2D methods (i.e. downsizing across all three dimensions rather than just two). In Appendix Table~\ref{table:decoder}, we compare 3D MedVAE with a model referred to as 2D MedVAE-Decoder, which has a comparable downsizing factor $f$. The 2D MedVAE-Decoder model performs downsizing on individual 2D slices, which are then stitched and interpolated together to form a latent representation of equivalent size to the 3D MedVAE model; we then perform fine-tuning of the decoder using our curated dataset of 3D volumes. The superiority of 3D MedVAE to the 2D MedVAE-Decoder approach demonstrates the utility of 3D training of autoencoders, which enables the model to capture important volumetric patterns. 

% ******* Figure ********
\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth, trim=0 0 0 0]{figures/readerstudy.pdf}
\caption{\textbf{Manual perceptual quality evaluations with expert readers.} We report the mean scores from three expert readers on three criteria: fidelity, preservation of relevant features, and artifacts. We compare 2D MedVAE with ($f=16,C=3$) and ($f=64,C=4$) with bicubic interpolation, a standard and widely-used approach for downsizing medical images. Error bars represent 95\% confidence intervals.}
\label{fig:readerstudy}
\end{figure}


Qualitative reader studies by domain experts are critical for ensuring clinical usability of developed methods. We supplement our automated evaluations of reconstructed image quality with a manual reader study. Each reader is presented with a pair of chest X-rays, consisting of an original high-resolution image on the left and a reconstructed image on the right. A total of 50 unique chest X-rays with fractures, randomly sampled from CANDID-PTX, are selected and presented in a randomized order~\cite{feng2021candid}. The reconstructed images are scored on a 5-point Likert scale ranging from -2 to 2 based on three main criteria: image fidelity, preservation of diagnostic features, and the presence of artifacts. Our study involved three radiologists as expert readers. We compared 2D MedVAE with bicubic interpolation, a standard and widely-used approach for downsizing medical images (Figure~\ref{fig:readerstudy}).

For manual evaluations of reconstructed image quality, readers rated image fidelity for 2D MedVAE to be 2.8 points higher than bicubic interpolation averaged across the two downsizing factors. 2D MedVAE also better preserved clinically-relevant features (2.8 points). Artifacts (e.g. blurring, hallucinations) were more frequent in interpolated images (2.6 points), which severely suffered from blurring artifacts with increasing downsizing factors. In summary, our results suggest that 2D MedVAE better preserves diagnostic features than interpolation. In Figure \ref{fig:qualitative}, we provide qualitative examples of a reconstructed chest X-ray and a reconstructed T1-weighted brain MRI slice. 


% ******* Figure ********
\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth, trim=0 0 0 0]{figures/qualitative.pdf}
\caption{\textbf{Qualitative examples of reconstructed medical images.} The top section provides qualitative examples of a reconstructed chest X-ray. The bottom section provides qualitative examples of a reconstructed brain MRI slice. Residual figures show pixel-level differences between reconstructed images and original, high-resolution images; brighter colors represent larger differences.}
\label{fig:qualitative}
\end{figure}