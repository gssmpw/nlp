\begin{table}[t]
\resizebox{0.95\linewidth}{!}{
\begin{tabular}{@{}lcccc@{}}
\toprule
\multirow{2}{*}{\textbf{Configuration}} & \textbf{Language Model}           & \textbf{Vision-Language} & \textbf{Instruction Tuning}          & \multirow{2}{*}{\textbf{Instruction Tuning}} \\
                                        & \textbf{(Continued) Pre-training} & \textbf{Pre-training}    & \textbf{(Vision-Language Alignment)} &                                              \\ \midrule
ViT Init.                               & -                                 & SigLIP-Large             & from Stage 2                         & from Stage 3                                 \\
LLM Init.                               & Phi-2                             & -                        & from Stage 1                         & from Stage 3                                 \\
VL Projector init.                      & -                                 & -                        & random                               & from Stage 3                                 \\
Image Resolution                        & -                                 & $518^{2}$                & $518^{2}$                            & $518^{2}$                                    \\
ViT sequence length                     & -                                 & 1,024                    & 1,024                                & 1,024                                        \\
LLM sequence length                     & 4,096                             & -                        & 4,096                                & 4,096                                        \\
Optimizer                               & \multicolumn{4}{c}{AdamW}                                                                                                                          \\
Optimizer hyperparameter                & \multicolumn{4}{c}{$\beta_{1} = 0.9$, $\beta_{2} = 0.98$, $eps = 1e-6$}                                                                            \\
Peak learning rate                      & 2e-5                              & 5e-4                     & 1e-4                                 & 1e-5                                         \\
Learning rate schedule                  & \multicolumn{4}{c}{cosine decay}                                                                                                                   \\
Weight decay                            & 0.1                               & 0.2                      & 0.1                                  & 0.1                                          \\
Gradient clip                           & \multicolumn{4}{c}{1.0}                                                                                                                            \\
Training epochs                         & 3                                 & 20                       & 3                                    & 4                                            \\
Warm-up ratios                          & 0.05                              & 0.05                     & 0.05                                 & 0.05                                         \\
Global batch size                       & 1,024                             & 512                      & 512                                  & 256                                          \\
Gradient Acc.                           & \multicolumn{4}{c}{1}                                                                                                                              \\
Numerical precision                     & \multicolumn{4}{c}{bfloat16}                                                                                                                       \\
DeepSpeed                               & ZoRO-2                            & -                        & ZoRO-2                               & ZoRO-3                                       \\ \bottomrule
\end{tabular}}
\caption{Training hyperparameters of CheXagent.}
\label{table:hyperparameters}
\end{table}