\section{Related Work}
% Traditional approaches to music style transformation have primarily focused on controlling individual elements such as rhythm, harmony, or melody. For example,  **Elgqvist, "Style Transfer for Pop Piano"** enables style transfer for pop piano pieces with customizable attributes like rhythmic intensity, while  **Serrà, "Harmonizing Melodies in the Style of Bach"** focus on harmonizing melodies in the style of Bach. Similarly, models like  **Cazaly, "Variations and Arrangements with Control"** and  **Odom, "Generating Variations or Arranging Accompaniments"** concentrate on generating variations or arranging accompaniments with control over specific features, such as voice density. In contrast, our approach introduces a unified model that simultaneously controls rhythm, harmony, and melody, adapting them to the target genre while maintaining stylistic coherence.

% Genre style conversion, a task closely related to our work, has been explored in studies like  **Huang, "Music Language Model with Edit Model"**, which integrates a music language model with an edit model for monophonic melody style conversion. CycleGAN  **Lee, "CycleGAN for Symbolic Polyphonic Piano Pieces"** and Diffusion-based  **Vahdat, "Diffusion-based Approach to Genre Conversion"** approaches , such as , apply CycleGAN for symbolic polyphonic piano pieces. More recent works on infilling and prompt continuation, like  **Sturm, "Infilling and Prompt Continuation Tasks"** and  **Serra, "Prompt Completion Methods"**, share similar goals to our model. While these methods focus on specific elements of style transfer, they do not offer the same level of holistic control over the entire composition. Our approach stands out by using an iterative generation framework that enables users to adjust the degree of style transfer and structural similarity to the original composition, offering a more flexible and expressive approach.

% While early works such as  **Balkenius, "Generating Expressive Renditions from MIDI"** focused on generating expressive renditions from MIDI note events, and more recent studies like  **Grachten, "Expressive Performances from Symbolic Scores"** and  **Srivastava, "Score-to-Performance Methods"** focused on generating expressive performances from symbolic scores, these approaches primarily model variations within the same genre, often in classical music, where the score is typically performed as written. In contrast, our work addresses the challenge of cross-genre improvisation, where the performance is not a mere reproduction of the score but an expressive transformation that adapts to the target genre. This approach provides full control over the degree of expressiveness, distinguishing it from traditional score-to-performance methods and offering greater flexibility for creative and stylistic adaptation.

Music style transformations come in various forms, depending on how style is defined, how music is represented, and what the model is conditioned on. In this paper, we focus on expressive and controllable improvisational style in the symbolic domain, achieved through modifications to the rhythm, harmony, and melody of a solo piano performance conditioned on the target genre. Previous studies have focused on these tasks separately. For example,  **Koehn, "Style Transfer for Long Pop Piano Pieces"** addresses style transfer for long pop piano pieces, allowing users to customize musical attributes such as rhythmic intensity and polyphony at the bar level. Models such as  **Serrà, "Harmonizing Melodies in the Style of Bach Chorales"** harmonize a melody in the style of Bach chorales. Similarly,  **Huang, "FolkDuet: Fusing Chinese Melodies with Musical Counterpoint"** proposes FolkDuet to fuse Chinese melodies with musical counterpoint in real time.  **Miotto, "FaderNets for Generating Musical Variations"** introduces FaderNets for generating musical variations by adjusting low-level attributes using latent regularization and feature disentanglement while enabling high-level feature control (e.g., arousal) through a VAE framework.  **Papadopoulos, "Controllable Accompaniment Arrangement Task"** focus on arranging accompaniments for solo piano melodies, offering control over aspects such as voice density. Studies such as  **Grachten, "Extending Controllable Accompaniment Arrangement Task to Multi-track Generation"** extend the controllable accompaniment arrangement task to multi-track generation. However, all of these approaches use highly quantized piano encodings, limiting the model's ability to produce expressive renditions.

An expressive performance rendition in symbolic music should be capable of human-like characteristics in timing, dynamics, and articulations, which go beyond the rigid quantization of notes.  **Hadjeres, "Generating Expressive Performance Renditions by Modelling Midi Streams"** was among the first works to generate expressive performance renditions by modelling a stream of midi note events. Recently, generating expressive performance renditions from symbolic score has been the focus of several works such as  **Srivastava, "Expressive Performances from Symbolic Scores"**. Among these,  **Hadjeres, "Controllable and Expressive Polyphonic Piano Variations"** is capable of generating controllable and expressive polyphonic piano variations, but is limited to short fragments of music. 

Several works have explored genre style conversion, which is more closely related to this paper.  **Huang, "Unsupervised Monophonic Melody Style Conversion"**, explores unsupervised monophonic melody style conversion using a statistical framework that integrates a music language model with an edit model, discovering musical styles through unsupervised grammar induction.  **Lee, "CycleGAN for Genre Conversion in Symbolic Polyphonic Piano Pieces"** pioneered the use of GANs for genre conversion in symbolic polyphonic piano pieces by adapting the CycleGAN  framework with genre classifier discriminators.  **Vahdat, "Enhanced CycleGAN Approach to Genre Conversion"**, enhanced this approach by incorporating a beta-VAE  for better disentanglement of style and content. Recently,  **Koehn, "Multi-instrument Genre Conversion Using Architectural Modifications"** and  **Hadjeres, "Advances in Multi-instrument Genre Conversion"**, have advanced multi-instrument genre conversion through architectural and loss function-related modifications to the CycleGAN and VAE frameworks. However, these GAN-based approaches face training instability, potential mode collapse, and challenging parameter tuning. Their piano-roll representations also limit expressive performance capabilities and lack user-controlled style conversion intensity, both key aspects of our work. Recent works on infilling and prompt continuation tasks, including  **Sturm, "Infilling and Prompt Continuation Tasks"**, also relate to our model's capabilities.