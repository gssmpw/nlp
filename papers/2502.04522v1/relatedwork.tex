\section{Related Work}
% Traditional approaches to music style transformation have primarily focused on controlling individual elements such as rhythm, harmony, or melody. For example, \cite{wu2023musemorphose} enables style transfer for pop piano pieces with customizable attributes like rhythmic intensity, while \cite{hadjeres2017deepbach, huang2019counterpoint} focus on harmonizing melodies in the style of Bach. Similarly, models like \cite{tan2020music} and \cite{zhao2021accomontage} concentrate on generating variations or arranging accompaniments with control over specific features, such as voice density. In contrast, our approach introduces a unified model that simultaneously controls rhythm, harmony, and melody, adapting them to the target genre while maintaining stylistic coherence.

% Genre style conversion, a task closely related to our work, has been explored in studies like \cite{nakamura2019unsupervised}, which integrates a music language model with an edit model for monophonic melody style conversion. CycleGAN \cite{brunner2018symbolic, fu2020transfer} and Diffusion-based \cite{brunner2018symbolic, fu2020transfer} approaches  , such as , apply CycleGAN for symbolic polyphonic piano pieces. More recent works on infilling and prompt continuation, like and \cite{lv2023getmusic}, share similar goals to our model. While these methods focus on specific elements of style transfer, they do not offer the same level of holistic control over the entire composition. Our approach stands out by using an iterative generation framework that enables users to adjust the degree of style transfer and structural similarity to the original composition, offering a more flexible and expressive approach.

% While early works such as \cite{performance-rnn-2017} focused on generating expressive renditions from MIDI note events, and more recent studies like \cite{jeong2019graph} and \cite{lenz2024pertok} focused on generating expressive performances from symbolic scores, these approaches primarily model variations within the same genre, often in classical music, where the score is typically performed as written. In contrast, our work addresses the challenge of cross-genre improvisation, where the performance is not a mere reproduction of the score but an expressive transformation that adapts to the target genre. This approach provides full control over the degree of expressiveness, distinguishing it from traditional score-to-performance methods and offering greater flexibility for creative and stylistic adaptation.

Music style transformations come in various forms, depending on how style is defined, how music is represented, and what the model is conditioned on. In this paper, we focus on expressive and controllable improvisational style in the symbolic domain, achieved through modifications to the rhythm, harmony, and melody of a solo piano performance conditioned on the target genre. Previous studies have focused on these tasks separately. For example, \cite{wu2023musemorphose} addresses style transfer for long pop piano pieces, allowing users to customize musical attributes such as rhythmic intensity and polyphony at the bar level. Models such as \cite{hadjeres2017deepbach,huang2019counterpoint} harmonize a melody in the style of Bach chorales. Similarly, \cite{jiang2020counterpoint} proposes FolkDuet to fuse Chinese melodies with musical counterpoint in real time. \cite{tan2020music} introduces FaderNets for generating musical variations by adjusting low-level attributes using latent regularization and feature disentanglement while enabling high-level feature control (e.g., arousal) through a VAE framework. \cite{zhao2021accomontage,wu2024generating} focus on arranging accompaniments for solo piano melodies, offering control over aspects such as voice density. Studies such as \cite{lv2023getmusic,ren2020popmag,zhaostructured,wu2023c2,min2023polyffusion,cifka2020groove2groove} extend the controllable accompaniment arrangement task to multi-track generation. However, all of these approaches use highly quantized piano encodings, limiting the model's ability to produce expressive renditions.

An expressive performance rendition in symbolic music should be capable of human-like characteristics in timing, dynamics, and articulations, which go beyond the rigid quantization of notes. \cite{performance-rnn-2017} was among the first works to generate expressive performance renditions by modelling a stream of midi note events. Recently, generating expressive performance renditions from symbolic score has been the focus of several works such as \cite{jeong2019graph,jeong2019virtuosonet,xiao2024music,borovik2023scoreperformer,maezawa2019rendering,lenz2024pertok,colton2024automatic}. Among these, \cite{lenz2024pertok} is capable of generating controllable and expressive polyphonic piano variations, but is limited to short fragments of music. 

Several works have explored genre style conversion, which is more closely related to this paper. \cite{nakamura2019unsupervised} explores unsupervised monophonic melody style conversion using a statistical framework that integrates a music language model with an edit model, discovering musical styles through unsupervised grammar induction. \cite{brunner2018symbolic} pioneered the use of GANs for genre conversion in symbolic polyphonic piano pieces by adapting the CycleGAN \cite{zhu2017unpaired} framework with genre classifier discriminators. \cite{fu2020transfer} enhanced this approach by incorporating a beta-VAE \cite{higgins2017beta} for better disentanglement of style and content. Recently, \cite{sulaiman2022genre} and \cite{ding2022steelygan} have advanced multi-instrument genre conversion through architectural and loss function-related modifications to the CycleGAN and VAE frameworks. However, these GAN-based approaches face training instability, potential mode collapse, and challenging parameter tuning. Their piano-roll representations also limit expressive performance capabilities and lack user-controlled style conversion intensity, both key aspects of our work. Recent works on infilling and prompt continuation tasks, including \cite{min2023polyffusion,lv2023getmusic,thickstun2023anticipatory,chang2021variable,hadjeres2021piano}, also relate to our model's capabilities.