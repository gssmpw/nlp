\section{Gemini Defense Mechanism \label{cha:appendixD}}

\begin{figure*}[htbp]
    \centering
    \includegraphics[width =\textwidth]{img/category.png}
    \caption{Gemini Defense Mechanism: category:HARM\_CATEGORY\_DANGEROUS\_CONTENT}
    \label{fig:HARM_CATEGORY_DANGEROUS_CONTENT}
    \Description[<short description>]{<long description>}
\end{figure*}

\begin{itemize}
    \item Multi-level security policies
    \begin{itemize}
        \item Google Gemini's content generation process has multiple built-in security policies to detect and filter potential violations, sensitive or high-risk content. These security policies are usually based on a combination of deep learning models and symbolic rules to evaluate the nature of user input or generated content in real time. Once the system detects that the request contains sensitive or dangerous information that exceeds the threshold, the corresponding interception mechanism will be activated.
        \item In the ValueError shown in the screenshot, the judgment of dangerous content has reached the MEDIUM level, while the risk of other categories (such as sex, hate speech, harassment) is NEGLIGIBLE. When the system determines that the "dangerous content" exceeds a certain "tolerance" based on the policy, it chooses to directly reject or terminate the generation process.
    \end{itemize}
    
    \item finish\_reason and security score
    \begin{sloppypar}
    \begin{itemize}
        \item In the return structure of Google Gemini, finish\_reason is usually used to indicate the reason for the termination of model generation. Common values include normal termination by the user, completion of model generation, policy rejection (or ban), etc.
        \item The error shown in finish\_reason=3 often means forced termination/interception by security policy. In this case, the model does not return the actual text `Part', resulting in an error of `no content available' when calling methods such as response.text.
        \item The corresponding safety\_ratings is the result of the system's probability assessment on each predefined content risk category (for example: HARM\_CATEGORY\_SEXUALLY\_EXPLICIT, HARM\_CATEGORY\_HATE\_SPEECH, HARM\_CATEGORY\_HARASSMENT, HARM\_CATEGORY\_DANGEROUS\_CONTENT, etc.). Each category has a corresponding probability value (from "NEGLIGIBLE" to "HIGH"), and if any category reaches or exceeds the internal warning line, the system will execute a rejection strategy.
    \end{itemize}
    \end{sloppypar}
\end{itemize}

This multi-layer defense strategy uses semantic analysis and probability threshold judgment to terminate generation when a request that does not meet security standards or is high-risk is detected, avoiding the leakage of potential violation information. For unauthorized researchers, this phenomenon shows that Google Gemini's review system covers multiple categories of risks and implements real-time intervention in the generation stage.