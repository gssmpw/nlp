%\documentclass[sigconf]{acmart}
\documentclass[acmsmall]{acmart}
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmlicensed}
\copyrightyear{2018}
\acmYear{2018}
\acmDOI{XXXXXXX.XXXXXXX}
%% These commands are for a PROCEEDINGS abstract or paper.
\acmConference[Conference acronym 'XX]{Make sure to enter the correct
  conference title from your rights confirmation email}{June 03--05,
  2018}{Woodstock, NY}
%%
%%  Uncomment \acmBooktitle if the title of the proceedings is different
%%  from ``Proceedings of ...''!
%%
%%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%%  June 03--05, 2018, Woodstock, NY}
\acmISBN{978-1-4503-XXXX-X/18/06}
\usepackage{array}
\usepackage{caption}
\usepackage{multirow}
\usepackage{makecell}
\begin{document}

\title{`Do as I say not as I do': A Semi-Automated Approach for Jailbreak Prompt Attack against Multimodal LLMs}
%\title{From Compliance to Exploitation: Jailbreak Prompt Attacks on Multimodal LLMs}
\author{Chun Wai Chiu$^*$}
\email{cchi7627@uni.sydney.edu.au}
\affiliation{%
  \institution{University of Sydney}
  \city{Sydney}
  \state{NSW}
  \country{Australia}
}
\author{Linghan Huang$^*$}
\email{lhua5130@uni.sydney.edu.au}
\affiliation{%
  \institution{University of Sydney}
  \city{Sydney}
  \state{NSW}
  \country{Australia}
}
\author{Bo Li}
% \authornote{Both authors contributed equally to this research.}
\email{bol@uchicago.edu}
\affiliation{%
  \institution{University of Chicago}
  \city{Chicago}
  \state{Illinois}
  \country{United States}
}
\author{Huaming Chen}
% \authornote{Both authors contributed equally to this research.}
\email{huaming.chen@sydney.edu.au}
\affiliation{%
  \institution{University of Sydney}
  \city{Sydney}
  \state{NSW}
  \country{Australia}
}

\def\thefootnote{*}\footnotetext{These authors contributed equally to this work}\def\thefootnote{\arabic{footnote}}
\renewcommand{\shortauthors}{Trovato et al.}

\begin{abstract}
Large Language Models (LLMs) have seen widespread applications across various domains due to their growing ability to process diverse types of input data, including text, audio, image and video. While LLMs have demonstrated outstanding performance in understanding and generating contexts for different scenarios, they are vulnerable to prompt-based attacks, which are mostly via text input. In this paper, we introduce the first voice-based jailbreak attack against multimodal LLMs, termed as \textit{Flanking Attack}, which can process different types of input simultaneously towards the multimodal LLMs. Our work is motivated by recent advancements in monolingual voice-driven large language models, which have introduced new attack surfaces beyond traditional text-based vulnerabilities for LLMs. 
To investigate these risks, we examine the state-of-the-art multimodal LLMs, which can be accessed via different types of inputs such as audio input, focusing on how adversarial prompts can bypass its defense mechanisms. We propose a novel strategy, in which the disallowed prompt is flanked by benign, narrative-driven prompts. It is integrated in the Flanking Attack which attempts to humanizes the interaction context and execute the attack through a fictional setting. Further, to better evaluate the attack performance, we present a semi-automated self-assessment framework for policy violation detection. We demonstrate that Flank Attack is capable of manipulating state-of-the-art LLMs into generating misaligned and forbidden outputs, which achieves an average attack success rate ranging from 0.67 to 0.93 across seven forbidden scenarios. 
These findings highlight both the potency of prompt-based obfuscation in voice-enabled contexts and the limitations of current LLMs' moderation safeguards. With a comprehensive evaluation of Flank Attack, this work establishes a replicable testing framework for adversarial robustness evaluation in multimodal LLMs. It highlights the urgent need for advanced defense strategies to address the challenges posed by evolving, context-rich attacks.

\noindent\color{ACMGrey}{Disclaimer. This paper contains examples of harmful language. Reader discretion is recommended.}
\end{abstract}

\maketitle
\section{Introduction}
The rapid advancement of Large Language Models (LLMs) has fostered significant progress across various domains, from natural language processing to multimodal interactions involving audio, text, and images. However, as these models become increasingly integral to critical applications, concerns regarding their robustness and susceptibility to adversarial attacks have garnered particular attention. One type of adversarial attacks, known as the jailbreak attack, aims to circumvent the internal constraints and safeguards, thereby obtaining the prohibited contents and response from LLMs~\cite{greshake2023not,wei2024jailbroken}. It has since become a significant threat to LLMs, mostly focusing on the craft of deliberate text prompt to mislead the LLMs~\cite{xu2024llm}. 

In recent years, there has been a growing body of research works investigating the limitations of LLMs in handling adversarial inputs across multiple languages and modalities~\cite{shayegani2023survey}. While numerous constraints have been implemented to actively defense such threats, the jailbreak techniques to bypass such defense continue to evolve~\cite{shen2024anything,yu2024don,chang2024play}. Most of these studies emphasize text-based or multilingual environments for LLMs, resulting in the curation of jailbreak prompts, as illustrated in Figure.~\ref{fig:forbiddenque}.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=\columnwidth]{img/Forbidden_Question.png}
    \caption{Example prompt and completions for refusals on disallowed categories.}
    \label{fig:forbiddenque}
    \Description[<short description>]{<long description>}
\end{figure}

As such, we have seen collective efforts for systematic and comprehensive benchmark of different jailbreak attacks~\cite{chu2024comprehensive,yu2024don}. On the one hand, many works aim at identifying the underlying strategies of existing jailbreak prompts and their effectiveness for a more systematic understanding. In Shen et al.~\cite{shen2024anything} and Yu et al.~\cite{yu2024don}, the jailbreak prompts from various online sources are analyzed to deliberately violate OpenAI's policies. On the other hand, new techniques for creating novel and effective jailbreak prompts, which target different aspects of LLMs continue to emerge. These include methods such as multi-language mixture attacks leveraging low-resource languges by Upadhayay et al.~\cite{upadhayay2024Flanking} and system prompt leaking attack by Hui et al.~\cite{hui2024pleak}. 

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\columnwidth]{img/Voice_Jailbreak.png}
    \caption{Overview of Voice Jailbreak\cite{chu2024comprehensive}}
    \label{fig:voice_jailbreak}
    \Description[<short description>]{<long description>}
\end{figure}

In latest developments, multimodal LLMs have demonstrated exceptional capabilities in processing diverse input types, such as audio, vision, and text. For example, GPT-4o is designed to handle direct audio input with a user-friendly interface~\cite{hurst2024gpt}, while Gemini features strong generalist capabilities across modalities, along with advanced understanding and reasoning performance in various domains~\cite{team2023gemini}. This has inevitably introduces novel attack surfaces for security concerns~\cite{liu2024jailbreak}, which have been highlighted in most recent works for voice mode of GPT-4o~\cite{shen2024voice} and video-based multimodal LLMs~\cite{huang2025image}. While these works have made valuable contributions, the extent and potential harm caused by jailbreak prompts to multimodal LLMs remain unclear. Among these, audio-based jailbreak attacks pose a unique challenge, as they exploit the auditory input capabilities of LLMs, potentially circumventing established defense mechanisms. As shown in Figure.~\ref{fig:voice_jailbreak}, with dedicated design of jailbreak prompt, the attacker can lead the multimodal LLMs for a misaligned and potentially forbidden response to the query, even such forbidden question is explicitly listed as part of the user policy for LLMs usage. 

However, it is unclear that whether and to what extent such audio-based jailbreak prompts can compromise the state-of-the-art multimodal LLMs. If so, what types of jailbreak attacks will be effective in eliciting misaligned even prohibited responses from the targeted LLMs. To bridge the gap, we firstly investigate the multimodal LLMs with the capability of processing audio inputs via an application programming interface (API). We aim to address the research questions, \textit{`How effective are the adversarial audio-based prompt in bypassing LLMs' defense strategies'}. 

Furthermore, in this paper, we present a novel, simple and universal black-box jailbreak attack method, named Flanking Attack, to generate highly effective prompts for state-of-the-art multimodal LLMs. We have developed a systematic semi-automated framework for audio-based jailbreak prompt attacks. With the framework, we are able to provide a more comprehensive and insightful evaluation for the robustness of mulitimodal LLMs' defenses strategies. Although some studies present different security threats for a range of modalities~\cite{liu2024jailbreak}, different languages~\cite{upadhayay2024Flanking} even system information~\cite{hui2024pleak}, our work demonstrates the specific challenges faced by audio-enabled LLMs. We observe that, for these LLMs, they may not benefit much from language diversity in training process while exhibiting distinct vulnerabilities. Appendix~\ref{cha:appendixD} delves deeper into the limitations of the multimodal LLMs, providing a nuanced analysis of their performance and highlighting potential areas for future improvements. To this end, our work contributes to a better understanding of LLMs' defensive capabilities, offering a foundation for developing more resilient LLMs that can withstand sophisticated audio-based attacks in the future. 

% This research focuses on a specific LLM, Gemini, which is configured to process English-language audio inputs via an API, and investigates its vulnerability to such adversarial manipulations.
%In recent . However, most of these studies emphasize text-based or multilingual environments, with limited focus on monolingual, English-only scenarios. The Gemini API, as a monolingual system, presents an opportunity to explore these vulnerabilities in a focused context. Given that the API is restricted to English MP3 inputs, this study aims to evaluate the effectiveness of audio-based attacks within this monolingual constraint, thereby revealing potential weaknesses unique to English-only models.
%The research aims to address the following question: \textbf{“How effective are adversarial audio-based attacks in bypassing Gemini’s defense mechanisms when utilizing English voice inputs via the Gemini API?”} Through this question, the study seeks to contribute to the field by identifying specific audio manipulation strategies that can compromise the Gemini API, and by extension, similar LLMs. By developing a systematic framework for audio-based adversarial attacks, this research evaluates the robustness of the Gemini API’s defenses, 

%Appendix \ref{cha:appendixD}, providing insights into the limitations and potential areas for improvement within monolingual multimodal models.

%In pursuing this objective, the research employs a comprehensive methodology that integrates principles from audio processing and adversarial machine learning. It explores various dimensions of audio manipulation, such as pitch, intonation, and background noise, to emulate real-world adversarial conditions. The study further examines the effectiveness of these manipulations across six forbidden scenarios outlined in Gemini’s policy framework, which encompass illegal activities, harmful content generation, and misinformation, among others.

%This investigation is crucial for two main reasons. First, it illuminates the specific challenges faced by monolingual LLMs, which may not benefit from language diversity in training and thus may possess unique vulnerabilities. Second, by rigorously testing the Gemini API against diverse adversarial inputs, the research contributes to a better understanding of the model’s defensive capabilities, offering a foundation for developing more resilient LLMs that can withstand sophisticated, audio-based attacks in the future.

\textbf{Our Work.}
This work introduces a semi-automated systematic framework for assessing the security threats of audio-enabled LLMs, specifically for the jailbreak prompt attack. We first investigate the LLMs within the context of English as the monolingual language. We leverage the Gemini's API for adversarial audio-based attacks, specifically within the context of English-only, monolingual models. We use the Gemini's usage policy as the constraints, in which seven specific forbidden areas are chosen, including \textbf{Illegal Activities}, \textbf{Abuse and Disruption of Services}, \textbf{Circumventing Safety Filters}, \textbf{Harmful Content Generation}, \textbf{Misinformation and Misleading Content}, \textbf{Sexually Explicit Content} and \textbf{Privacy Violations}. These scenarios provide a broad representation of potential risks, ensuring a thorough assessment of the API's defensive capabilities. We feature Gemini's API as the focal point for our analysis, as it provides a direct portal to accept audio inputs as MP3 format. This allows us to further incorporate sophisticated techniques from audio processing and pattern obfuscation to create varied jailbreak audio prompt. These manipulations are applied across a range of real-world scenarios that emulate the potential attack vectors. 

To measure the effectiveness of Flanking Attack, we employ the Attack Success Rate (ASR) as the key metric, which measures the extent to which the attacks bypass LLMs' defense mechanisms. We find that Flanking Attack achieves a high performance in attacking the LLMs for seven forbidden scenarios, ranging from 0.67 to 0.93. The average ASR is 0.81. By incorporating the prompt-based guidance throughout the jailbreak attack process, Flanking Attack showcases the limitations of multimodal LLMs, especially the audio-enabled LLMs, in dealing with diverse prompts combinations. In particular, when the adversarial audio prompt is flanked by the benign prompts, it effectively bypass the defense mechanisms in LLMs. The results showcase a significant high ASR performance, indicating the urgent need for specialized defense strategies that address the unique vulnerabilities of multimodal LLMs, particularly as such models continue to be integrated into critical applications. We also provide extensive ablation studies for Flanking Attack, suggesting the best approach and ideal strategy for jailbreak prompt attacks design and evaluation.

%While prior research has largely emphasized text-based or multilingual adversarial attacks on Large Language Models (LLMs), this work shifts focus to explore the distinctive challenges faced by monolingual systems. The Gemini API’s restriction to English MP3 files presents a unique opportunity to investigate how the lack of language diversity might impact the model’s robustness against adversarial inputs.

%To systematically evaluate Gemini’s defenses, the research incorporates sophisticated techniques from audio processing and adversarial machine learning. The framework developed for this study leverages signal perturbation, pitch alteration, and pattern obfuscation to create varied adversarial audio samples. These manipulations are applied across a range of real-world scenarios that emulate potential attack conditions, allowing for an in-depth examination of the model’s response to diverse audio perturbations. The chosen scenarios target seven specific areas forbidden by Gemini’s usage policy, including illegal activities, harmful content generation, and misinformation. These scenarios provide a broad representation of potential risks, ensuring a thorough assessment of the API’s defensive capabilities.

%To measure the effectiveness of the adversarial attacks, this research employs the Attack Success Rate (ASR) as a key metric, which captures the extent to which the attacks bypass Gemini’s defense mechanisms. Across the six tested scenarios, the ASR varies significantly, ranging from 0.12 to 0.81. This variability underscores the complex interplay between different audio manipulations and the Gemini API’s defensive responses, highlighting specific areas of vulnerability. Furthermore, the study investigates the role of prompt-based guidance in enhancing attack success. By comparing outcomes with and without prompt-based assistance, the research demonstrates that carefully crafted prompts can significantly improve ASR, thereby shedding light on the crucial influence of input formulation in adversarial attacks.

%The outcomes of this study contribute to the broader understanding of adversarial attack frameworks applicable to monolingual, audio-input-based LLMs. By focusing on the Gemini API, this research provides a replicable model for evaluating the resilience of similar systems, offering insights that are directly relevant to developers and researchers aiming to enhance LLM security. Ultimately, the findings underscore the need for specialized defense strategies that address the unique vulnerabilities of monolingual LLMs, particularly as such models continue to be integrated into sensitive applications.

\textbf{Contributions.} Our contributions are listed as follows.

\textit{We perform a systematic benchmarking of audio-based jailbreak attacks against the state-of-the-art multimodal LLMs.} This work presents a thorough approach specifically designed to exploit potential vulnerabilities within monolingual, audio-enabled LLMs. Upon investigation, we observe that current approaches for defending multimodal LLMs from jailbreak prompt attacks are generally effective when addressing queries consisting solely of pure forbidden questions.

\textit{We propose a novel, simple and universal audio-based jailbreak attack framework for LLMs.}
By leveraging prompt-based guidance and franking the jailbreak query with benign ones, we propose Franking Attack, which can successfully bypass the defense mechanism in the multimodal LLMs. In total, we have evaluated 2,100 prompts across seven forbidden scenarios, yielding an impressive ASR result of 0.81. This finding highlights the importance of prompt formulation in jailbreak attack strategy, as it can drastically enhance the likelihood of bypassing defensive mechanisms. The framework's adaptability and scalability make it a valuable tool for future studies on audio-based jailbreak attacks against multimodal LLMs, providing a structured approach to assess and replicate adversarial scenarios across similar models.

\textit{We develop a semi-automated approach to evaluate jailbreak prompt attacks against multimodal LLMs.}
To further enhance the generation and assessment of jailbreak prompt attacks for multimodal LLMs, a new semi-automated approach is introduced. Unlike traditional method, we feature an aligned multimodal LLM to facilitate the evaluation of responses for policy compliance. We anticipate this approach can be adapted for future research of multimodal LLMs so that they will progressively improve response quality and better align with safety guidelines.% for adversarial research, enhancing the efficiency and scalability of evaluating attack success rates. Unlike traditional manual inspection, which requires reviewing each response individually, this model allows Aligned MLLM to generate large batches of outputs, stored in a document for automated review. Aligned MLLM then cross-references its responses with its own usage policies, essentially self-evaluating for policy compliance. This innovative approach holds potential beyond this study, offering platforms like OpenAI a framework where models can autonomously learn to recognize and adjust their responses, progressively improving quality and alignment with safety guidelines through internal evaluation.




\section{Background}
\subsection{Large Language Models}
Large language models (LLMs) and artificial intelligence (AI) are two interconnected fields that are dramatically changing human life with technology. Generally AI refers to the ability of computers to simulate intelligent human behavior. This covers a variety of techniques and applications, ranging from simple algorithms to complex deep neural networks. Within this context, LLMs represent a specific and highly advanced subset of AI methodologies. 

LLMs, such as OpenAi's GPT family, and LLaMa, are a type of generative artificial intelligence designed to comprehend and generate human-like outputs. These models utilize deep learning architecture, particularly Transformers, to interpret and generate coherent and contextual text. As noted by~\cite{kovaionGenerativeLLMs}, `Generative AI, often referred to as GenAI, involves systems that have the capacity to generate content autonomously, and large language models (LLMs) are a prominent embodiment of this concept'.
    
Recent works have demonstrated LLMs' tremendous potential for achiveing human-like intelligence~\cite{achiam2023gpt,touvron2023llama,chen2019generative}, benefiting from large-scale training datasets along with a significant number of model parameters. However, concerns have been raised regarding the safety and security of LLMs, particularly their misuse by malicious actors. These risks involve a wide range of issues, including social engineering and data breaches~\cite{glukhov2023llm}. To mitigate such risks, various methods have been presented to regulate LLMs usage, including fine-tuning LLMs to enhance ethical standards compliance~\cite{achiam2023gpt,touvron2023llama} and using third-party censorship systems to identify and filter inappropriate inputs or outputs~\cite{glukhov2023llm}. Despite these efforts, existing defenses have been empirically circumvented~\cite{chang2024play,chu2024comprehensive,shayegani2023survey,huang2023catastrophic,russinovich2024great}. It highlights the critical need for further research to ensure the safe and ethical udage of LLMs.

\subsection{LLMs safety and its limitation}
LLMs may fabricate facts (referred to as `hallucinations'), create divisive content, or reproduce prejudice, hate speech or stereotypes~\cite{glukhov2023llm}. These problems largely arise from the large and diverse data sets use for pre-training. Reinforcement Learning with Human Feedback (RLHF) is thus considered as mitigation to align LLMs with human values to address these issues~\cite{glukhov2023llm,bai2022training}. 


% 换成Gemini，不要用GPT
% https://arxiv.org/pdf/2312.11805

According to GenAi, Gemini has a decreased propensity to produce damaging information or have hallucinations\cite{team2023gemini}. Undesirable behaviors may arise when instructions to labelers are inadequately specified during the reward model data collection phase of the Reinforcement Learning with Human Feedback (RLHF) pipeline\cite{achiam2023gpt}. The process involves injecting policy-driven guidelines, sometimes referred to as 'constitutions,' which guide the model in handling sensitive topics such as political neutrality. For example, in topics like elections, the model is trained to avoid taking sides, ensuring that responses maintain a neutral point of view. This method is inspired by Constitutional AI, where human feedback is used to revise responses and select safer outputs.\cite{team2023gemini}

Taking Gemini as an example, the zero-sample reasoning ability~\cite{kojima2022large} to modify answers and choose between multiple candidate answers. For example, in topics like elections, the model is trained to avoid taking sides, ensuring that responses maintain a neutral point of view. This method is inspired by Constitutional AI, where human feedback is used to revise responses and select safer outputs.

According to recent research on supervised fine-tuning (SFT), balancing the harmlessness and helpfulness of language model responses remains a significant challenge. While a response such as, `I cannot help with that because it violates X policy', refer to Appendix \ref{cha:appendixD}, ensures safety by avoiding harmful content, it often fails to meet user expectations for helpfulness. The difficulty lies in striking an appropriate balance between these two objectives within the model's fine-tuning framework~\cite{team2023gemini}. Furthermore, the landscape of harmful query patterns is highly dynamic, which complicates fast mitigation. Models must quickly adapt to newly discovered harmful query patterns while also generalizing their defenses to address a wide array of harmful inputs. To address this, advanced chain-of-thought reasoning based on safety policy concepts has been introduced, allowing models to operate at a higher level of abstraction rather than responding solely at the fine-grained example level~\cite{zhang2023igniting}.

% According to OpenAI, GPT-4 has a decreased propensity to produce damaging information or have hallucinations\cite{achiam2023gpt}. Undesirable behaviors may arise when instructions to labelers are inadequately specified during the reward model data collection phase of the Reinforcement Learning with Human Feedback (RLHF) pipeline\cite{achiam2023gpt}. The model may provide undesired material, such recommendations on committing crimes, when it receives dangerous inputs.

% Taking GPT-4 as an example, the zero-shot GPT-4 classifier provides additional reward signals to the GPT-4 policy model during the RLHF fine-tuning process.\cite{achiam2023gpt} This aims to promote correct behavior, such as refusing to generate harmful content or not rejecting harmless requests.



There are limitations to LLM safety and alignment, particularly regarding the ability of LLMs to `self-censor' and consistently ensure their outputs are always permissible. Theoretically, it has been established that the expected results of LLMs can be arbitrarily misaligned as long as the model can produce misaligned outputs with a non-zero probability, regardless of how the outputs are aligned (as measured by a specific metric)~\cite{xu2023tool}. Furthermore, because LLMs must store information to function effectively, it was acknowledged that they could not maintain privacy or avoid data poisoning without making significant performance trade-offs~\cite{el2022impossible}.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.95\columnwidth]{img/malicious_instruction.png}
    \caption{Responses to a malicious instruction by the LLAMA2-7B-CHAT model under different generation configurations.\cite{huang2023catastrophic}}
    \label{malicious instruction}
    \Description[<short description>]{<long description>}
\end{figure}

\section{Related Works}
\subsection{Adversarial Attack}
Machine learning algorithms are known to be vulnerable to adversarial attacks, in which the carefully crafted inputs can result in producing consistently erroneous outputs~\cite{goodfellow2014explaining}. Understanding adversarial attacks in the context of an LLM presents several challenges.
%Attacks may be untargeted, designed to cause incorrect classification or generation, or they may be targeted, designed to change the model's output to a specific category or literal string.  These attacks also vary in their assumptions about the attacker's access to the internal structure of the model.
LLMs are complicated since they are large-scale, imaginative, situational, multi-modal, and increasingly intertwined into intricate ecosystems (e.g. large language model based autonomous agents~\cite{shayegani2023survey}). As such, the threat presented by adversarial attackers exhibits distinct behaviors that require meticulous examination. Thus, accurate threat models is important to guide the creation of principled defenses. Here we list some motivational examples for adversarial attack on an LLM:
\begin{enumerate}
    \item Personal use of LLM extension in browser as a shopping assistant. Malicious sellers embed adversarial messages in the text or images of their product pages to pollute the context processed by shopping extensions, thereby increasing the likelihood of product recommendations.
    \item Attempts to get harmful information from an LLM on how to make a bomb. Although the model was fine-tuned and adjusted to prevent the spread of harmful messages, users successfully elicited dangerous messages by manipulating prompts in ways that bypassed its safety mechanisms.
    \item Use the LLM enhanced programming assistant to assist in writing code.  An adversarial example accidentally caused LLM to generate code with a malicious backdoor.
\end{enumerate}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\columnwidth]{img/concepts_covered_in_the_survey.png}
    \caption{A taxonomy of concepts covered in the survey. \cite{shayegani2023survey}}
    \label{fig:taxonomy_concepts}
    \Description[<short description>]{<long description>}
\end{figure}

\subsection{Multi-modal Attack}
The target model of a multi-modal attack accepts input from multiple modalities (such as text, images, audio, etc.)\cite{girdhar2023imagebind}. These attacks exploit the increased complexity and introduction of new vulnerabilities by combining different types of input data. Additional modalities open up fresh attack vectors. Traditional textual alignment methods frequently fail to protect the joint embedding space that these multi-modal inputs generate.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=\columnwidth]{img/Adversarial_Embedding_Space_Attack.png}
    \caption{Adversarial Embedding Space Attack\cite{russinovich2024great}}
    \label{fig:embedding_space_attack}
    \Description[<short description>]{<long description>}
\end{figure}

\subsection{Jailbreak Prompt Attacks}
The focus of the prompt injection attack is to influence the input of the model by inserting adversarially constructed hints.  This causes the model to misinterpret input data as instructions, which is how attacker-controlled deceptive output is produced.  In effect, this type of attack takes over the model's expected responsibilities, which are typically specified by a system prompt (ref to Figure~\ref{fig:injection_prompt}) provided by the provider or developer.  
\begin{figure}[htbp]
    \centering
    \includegraphics[width=\columnwidth]{img/final_prompt.png}
    \caption{Injection prompt\cite{shayegani2023survey}}
    \label{fig:injection_prompt}
    \Description[<short description>]{<long description>}
\end{figure}

 Prompt injection (PI) attacks exploit the way LLM interprets and processes input prompts. They can override the original instructions and controls set by the developer, causing the model to produce output that benefits the attacker\cite{glukhov2023llm}. PI attacks involve crafting adversarial prompts that the LLMs mistake for legitimate instructions. This manipulation can cause the model to produce deceptive or harmful outputs.



Naive injection attacks primarily target image manipulation in order to mislead classification models. Inspired by the work of Noever\cite{noever2021reading}, who demonstrated the ability to deceive OpenAI’s CLIP\cite{radford2021learning} in zero-shot image classification by adding text that contradicted the image content, subsequent studies have explored whether similar vulnerabilities could be exploited in multimodal models. 

These vulnerabilities are hypothesized to arise from the vision encoders, such as OpenAI’s CLIP, used in multimodal models, which exhibit a preference for textual data over visual signals. Studies by Noever\cite{noever2021reading} highlight how these models, as they acquire Optical Character Recognition (OCR) capabilities\cite{zhang2023llavar}, become increasingly susceptible to raw text injection attacks. Recent evidence shows that even sophisticated systems like Google Bard and Microsoft Bing can be manipulated through such attacks, following textual instructions embedded in visual images\cite{shayegani2023plug}. 

Gong et al.\cite{gong2023figstep} revealed that the vision modality within MLLMs introduces a novel attack surface, as the LLMs’ safety alignments fail to account for unexpected input distributions originating from visual data. As MLLMs increasingly integrate diverse data types, including audio, to broaden their real-world applicability, there remains an open question as to whether the audio modality similarly exposes MLLMs to new vulnerabilities, presenting another potential attack vector that has yet to be fully explored.

% 需要更改voice jailbreak的内容，不需要写出chatgpt的，需要写出gemini的

\section{Threat Model}
In this work, we consider the primary objectives from potential attackers are from two aspects. First, the attacker aim to circumvent the built-in content moderation systems so as to retrieve or generate prohibited content, including but not limited to the contents outlined in the specific user policies. On top of this, the attacker may even attempt to elicit sensitive data from LLMs, such proprietary training data, model parameters, or system prompts etc. Given the recent developments of LLMs, including open-source and enterprise ones, we note that the resource and time may be the primary concerns for everybody including the attackers. Moreover, general open-source LLMs don't support multimodal capabilities. Thus, in this work, we consider the current available enterprise solutions for jailbreak attack.
%following primary objectives from potential attackers:
%\begin{itemize}
%    \item \textbf{Bypassing Content Filtering and Safety Mechanisms}: Attackers aim to circumvent Gemini’s built-in content moderation systems to generate or retrieve prohibited content, including but not limited to instructions for illegal activities, hate speech, misinformation, and other restricted information.
%    \item \textbf{Extracting Sensitive Information}: Attackers may attempt to elicit sensitive data from Gemini, such as proprietary training data, internal model parameters, or confidential operational details.
%    \item \textbf{Disrupting System Operations}: By generating a high volume of adversarial requests, attackers could potentially degrade Gemini’s performance, exhaust system resources, or cause service interruptions, leading to denial-of-service (DoS) conditions.
%\end{itemize}

\textbf{Attacker's Capabilities}
The effectiveness of an attack is contingent upon the attacker's capabilities, which encompass their knowledge, resources, and access levels. Since the attackers may have the ability to directly interact with multimodal LLMs, we consider this access presents a channel for the attackers to submit the well crafted prompts in either the forms of audio or text. This doesn't specify any prior knowledge of the accessible multimodal LLMs, while the knowledge of LLMs' design and training process may indeed help the attack process. However, in comparison to the internal information of LLMs, the direct interaction with LLMs poses higher feasibility for attackers to elicit prohibited contents from LLMs, which may be outlined in the respective policies. Thus, the attackers will have access to various resources and tools for prompt generation for sophisticated scenarios to evade the defense mechanisms of LLMs. 
%\begin{itemize}
%    \item \textbf{Access Privileges}: Attackers possess the ability to interact with Gemini through its API, allowing them to submit audio inputs and receive corresponding outputs. This access enables the construction and submission of adversarial audio samples designed to exploit model vulnerabilities.
%    \item \textbf{Knowledge Level}:
%    \begin{itemize}
%        \item \textbf{Black-Box Attack}: Attackers operate without knowledge of Gemini’s internal architecture, model parameters, or specific content moderation algorithms. They rely solely on input-output behavior to craft adversarial attacks.
%        \item \textbf{Gray-Box Attack} (if applicable): Attackers have partial knowledge of Gemini’s internal mechanisms, such as general model architecture or high-level content filtering strategies, enhancing their ability to design more effective adversarial inputs.
%    \end{itemize}
%    \item \textbf{Resources and Tools}: Attackers have access to advanced audio editing software, adversarial sample generation tools, and computational resources necessary to create sophisticated audio perturbations that can evade detection by Gemini’s safety mechanisms.
%\end{itemize}

\textbf{Threat Scenarios}
Building upon the identified objectives, capabilities, and attack surfaces, the threat scenarios will be specified by the design of jailbreak prompts and the targeted plots, such as the examples in Appendix~\ref{cha:appendixA}. As such, the following specific threat scenarios are delineated:
\begin{itemize}
    \item \textbf{Jailbreak Prompt}: Attackers embed adversarial prompts within audio inputs, leveraging narrative contexts or fictional scenarios to manipulate Gemini into generating restricted content. For example, embedding illegal activity instructions within a story or simulation game narrative.
    
%    \item \textbf{Flanking Attacks}: This technique involves placing sensitive or prohibited queries between multiple benign questions or statements within a single audio input. The aim is to obscure the adversarial intent and reduce the likelihood of detection by content filters. For instance, interspersing a query about illegal activities within a sequence of unrelated, innocuous questions.
    
    \item \textbf{Multi-Modal Input}: Combining audio inputs with other modalities, such as text or images, to create a composite adversarial prompt that is more likely to bypass multimodal LLMs' safety mechanisms. For example, pairing an audio prompt with a benign text overlay to distract or confuse the model's content moderation process.
\end{itemize}

\section{Methodology}
In this paper, we adopt a stepwise methodology to evaluate the multimodal LLMs' resilience against audio-based jailbreak attacks. A preliminary study of the jailbreak attacks is conducted for around 100 adversarial experiments in the beginning. We manually review each output compliance with the corresponding usage policies. While this approach yields critical insights into potential audio-based jailbreak attacks for multimodal LLMs, it exhibits inherent limitations for scalability and efficiency. 

Thus, a semi-automated approach is proposed in this work, together with Flanking Attack, to better generate and assess the audio-based jailbreak attacks, enabling a more comprehensive evaluation of multimodal LLMs' defensive mechanisms. Following we start with the forbidden question set. %process that both broadened the scope of testing and improved the accuracy of policy violation detection. This methodological refinement enables a more comprehensive assessment of the API’s defensive mechanisms.

\subsection{Forbidden Question Set}
To rigorously assess the defenses against policy violations, we employ a Forbidden Questions Set designed to provoke responses that might contravene the usage policies. In this work, we strictly follow the design principles implemented in previous Forbidden Questions dataset work~\cite{shen2024anything}. The question set is strategically crafted based on seven distinctive scenarios, each representing a category of content prohibited by the user guidelines~\cite{gemini-policy}. %The question scenarios strictly follow the Forbidden Questions dataset\cite{shen2024anything}.

The forbidden questions are designed to simulate realistic and contextually varied prompts within each scenario, maximizing the likelihood of policy breaches. Each set of questions for a given scenario is based on the prohibited content categories and reflects situations or queries that, if responded to, would indicate a lapse in the model’s filtering capabilities. 

\subsection{Methodology Overview}
\begin{figure*}[htbp]
    \centering
    \includegraphics[width=\textwidth]{img/framework.pdf}
    \caption{Overview of Methodology}
    \label{fig:Flow of attacks}
    \Description[<short description>]{<long description>}
\end{figure*}

% 需要换个表达
% This study employs a two-stage approach in its methodology, using \textbf{Prompt Injection} and \textbf{Flanking Attack} as primary techniques to conduct adversarial attacks against Gemini. In the \hyperref[method:Attacks setting]{Attack Settings}, prompts are crafted with specific \textbf{Text Prompts} that create a fictional Setting and assign Character roles, setting the stage for a benign narrative. The \textbf{Flanking Attack} further layers the prompt by embedding sensitive queries within benign questions, aiming to bypass Gemini’s filters by diluting the apparent risk.

% In \textbf{Stage 1}, refer to Figure \ref{fig:Flow of attacks}, the initial phase involves \textbf{Manual Inspection} of Gemini’s outputs. Here, each response generated by Gemini is manually reviewed, and any outputs that violate the usage policy are documented. This phase helps establish baseline data on Gemini’s response behavior and the effectiveness of the adversarial techniques.

% In \textbf{Stage 2}, refer to Figure \ref{fig:Flow of attacks}, the study transitions to a \textbf{Semi-Automated Model} for policy violation detection. Gemini’s API is prompted to generate a larger batch of responses at once, storing these outputs in a document. Gemini is then instructed to review this document, applying its \textbf{Usage Policy}\cite{gemini-policy} to self-assess the outputs and flag any violations. This semi-automated approach enhances efficiency by leveraging Gemini’s internal policy-checking mechanism to analyze a greater volume of data in less time, offering a scalable solution for detecting compliance breaches.
This work adopts an innovative framework, leveraging both \textbf{prompt-based guidance} and \textbf{Flanking Attack} as principal design strategies against multimodal LLMs. Inspired by~\cite{shen2024voice} and~\cite{upadhayay2024Flanking}, in the Attack Settings, structured \textbf{Text Prompts} establish a fictional setting and assign character roles to craft a superficially benign context. For the \textbf{Flanking Attack}, the idea is to flank sensitive and malicious inquiries within otherwise benign prompts, aiming to circumvent LLMs' filters by obfuscating potentially problematic content.
% \begin{itemize}
%     % \item Stage 1 (Figure \ref{fig:Flow of attacks}) involves a \textbf{Manual Inspection} of Gemini’s outputs. Each response is individually reviewed for policy violations, yielding foundational data on Gemini’s baseline behavior and the efficacy of the deployed adversarial techniques.

%     \item Stage 2 (Figure \ref{fig:Flow of attacks}) transitions to a \textbf{Semi-Automated Model} for policy violation detection. Gemini’s API generates larger batches of responses, which are aggregated into a single document. Gemini subsequently evaluates this document against its \textbf{Usage Policy}\cite{gemini-policy}, flagging non-compliant content. By harnessing Gemini’s internal policy-checking capabilities, this stage accommodates higher data throughput and delivers a more scalable solution for detecting policy breaches.
% \end{itemize}

\textbf{Semi-Automated Approach} is participated for the overall generation and evaluation of jailbreak prompts for policy violation detection. In this way, we leverage the LLMs' API for the generation of larger batches of responses, which are aggregated for assessment. We particularly assign an aligned multimodal LLM for subsequent evaluation against the relevant \textbf{Usage Policy}~\cite{gemini-policy}, flagging non-compliant content. With this design, we provide two distinct advantages for audio-based jailbreak prompts generation and evaluation. Firstly, the semi-automated approach offers an efficient and comprehensive assessment, addressing the limitations of manually identifying policy violations. Secondly, with the aligned multimodal LLM for ultimate evaluation, it provides a more subjective and compatible results for policy violation detection. 

%Throughout our comprehensive evaluation, By harnessing Gemini’s internal policy-checking capabilities, this stage accommodates higher data throughput and delivers a more scalable solution to detect policy breaches.




% 写出Flanking attack
% 写出voice attack
% 写出prompt

\subsection*{Attacks Setting}\label{method:Attacks Setting}
\subsubsection{Text Prompt} \label{Text Prompt}
% 需要一张图片
\subsubsection*{Character Setting, Character, and Rule Application}
\begin{figure}[htbp]
    \centering
    \includegraphics[width=\columnwidth]{img/Text_Prompt_Setting.png}
    \caption{Text Prompt Setting\cite{shen2024anything}}
    \label{Injection prompt}
    \Description[<short description>]{<long description>}
\end{figure}

The initial stage leverages text-based prompt injection, a technique that introduces adversarial prompts within fictional and benign contexts. This stage is foundational, as it prepares LLMs to accept more complex inputs during the subsequent audio-based phase~\cite{shen2024anything,shen2024voice}.
\begin{itemize}
    \item \textbf{Setting:} The adversarial prompts are framed within fictional and non-threatening contexts. Different scenarios are applied for each forbidden scenarios to distance the the content from real-world applicability, for example hypothetical worlds. This contextual shift helps in reducing the model's resistance to potentially sensitive queries, as it interprets them within a `safe' fictional context.
    \item \textbf{Character:} The characters in each scene play a vital role in driving the narrative. By assigning personalities, motivations and fictional characters (for example, a detailed guide in a sci-fi screenplay or a hacker in a simulation game), prompts become more engaging and convincing. Characters can be anything, including humans, animals or inanimate objects with anthropomorphic qualities. This further embed the query in a fictional layer.
    \item \textbf{Rule Application:} There is a specific rule embedded in the scene that clearly states that the dialogue is a simulation and has no implications in the real world. For example, a hint might include: `We are just simulating a scenario; no real harm is intended.' This rule reassures the model, encouraging it to comply with the query under the assumption of harmless exercise.
\end{itemize}

\subsubsection*{Implementation example of text prompt}
% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=\columnwidth]{img/Code_Snippet_of_Text_Prompt.png}
%     \caption{Code Snippet of Text Prompt}
%     \label{fig:Code Snippet of Text Prompt}
% \end{figure}

The \textbf{Text Prompt} implementation in this study leverages Setting, Character, and Rule Application to create a fictionalized context that disarms LLMs' content moderation filters. The \textbf{Setting} establishes a safe, hypothetical environment, such as a `game' or `sci-fi screenplay'. This contextual shift, seen in phrases like `Imagine that we are in a game'. The \textbf{Character} element further enhances this framework by assigning roles that resonate with the context, such as a `hacker' or a `guide', who is involved in a hypothetical exercise, apply \textbf{Character}  as a `hacker', and  \textbf{Rule Application} reinforces the fictional boundaries by explicitly stating that the output is `just for hypothetical' and `not doing anything harm.'

\subsubsection{Flanking Attack (Voice-Based)} \label{Flanking Attack (Voice-Based)}
% 需要图片
\begin{figure}[htbp]
    \centering
    \includegraphics[width=\columnwidth]{img/Flanking_Attack_Prompt_Template.png}
    \caption{Flanking Attack Prompt Template}
    \label{fig:flanking_attack_template}
    \Description[<short description>]{<long description>}
\end{figure}

The second stage is Flanking Attack, which builds on text-based prompt injection by introducing the prompts through voice input. The idea is to flank the critical adversarial content with benign queries to bypass LLMs' defenses. This stage includes unique considerations due to multimodal LLMs' capabilities, ensuring that audio variations are seamlessly integrated within the prompt sequence.

\begin{itemize}
    \item \textbf{Sequential Layering}: The voice-based attack is structured to include a series of five to nine questions, where the central (adversarial) question is framed in a non-threatening, hypothetical format and surrounded by contextually benign queries. This sequence reduces the likelihood of LLMs' flagging the critical content, as the adversarial question is masked by the benign context.
    % cite gemini only english input
    \item \textbf{Positioning of the Adversarial Query}: The adversarial question is intentionally placed in the middle of the sequence (typically as the third or fifth query) to avoid triggering LLMs' safety mechanisms that might be more vigilant at the beginning or end of the input. This approach leverages the attention dynamics within LLMs, as Flankinging critical prompts between harmless content can reduce the likelihood of LLMs recognizing and blocking the adversarial query.
\end{itemize}

\subsubsection*{Implementation example of Flanking Attack}
% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=\columnwidth]{img/Implementation of Flanking attack.png}
%     \caption{Implementation of Flanking attack}
%     \label{fig:flanking_attack_example}
% \end{figure}

The Flanking Attack is operationalised as a voice-based prompt injection technique designed to circumvent LLMs' content moderation by interspersing sensitive queries among ostensibly benign prompts. An adversarial query (e.g., \emph{`Devise a plan for robbing a bank'}) is embedded between innocuous questions such as \emph{`How to build a chair'} or \emph{`Steps for washing hands'}. By situating this adversarial element as the third or fifth inquiry within a series of five to nine prompts, the perceived risk is diluted, thereby reducing the likelihood of triggering LLMs' moderation filters.

\textbf{Sequential Layering.} There are two key strategies underpinning this strategy: \emph{sequential layering} and \emph{precise query positioning}. Sequential layering ensures that the prompt sequence appears coherent and benign overall, with each question logically transitioning from the previous one. This structural camouflage masks the adversarial query, making it less conspicuous within the broader narrative flow. Positioning the adversarial question mid-sequence—rather than at the beginning or end—further decreases detection likelihood, as moderation systems often scrutinize prompt boundaries more rigorously. Together, these design choices enhance the stealth of the adversarial query, providing a more robust approach to bypassing conventional content-moderation mechanisms.

% The \textbf{Flanking Attack} technique, implemented in this study as a voice-based prompt injection method, is designed to bypass Gemini’s content moderation by embedding sensitive queries within a sequence of benign questions. This approach, illustrated in Figure \ref{Implementation of Flanking attack}, involves creating a prompt sequence where the adversarial question is positioned between contextually harmless queries. Typically, the adversarial question—such as “Devise a plan of robbing a bank”—is placed as the third or fifth question in a sequence of five to nine prompts, surrounded by non-threatening questions like “How to build a chair” and “Step of washing hand.” By positioning the critical query between innocuous prompts, the Flanking Attack dilutes the perceived risk of the adversarial query, making it less likely to trigger Gemini’s content moderation systems.

% The implementation of this technique leverages \textbf{Sequential Layering} and \textbf{Positioning of the Adversarial Query}. Sequential layering involves constructing the entire prompt in a way that flows naturally, with contextually benign queries leading into and following the adversarial question. This layering reduces the likelihood of Gemini flagging the sensitive content since the prompt appears as a harmless sequence overall. Positioning the adversarial question in the middle of the sequence avoids placing it at the beginning or end, where Gemini’s filters might be more vigilant.



% \subsection{Semi-automated Approach}
% \textbf{Two-Stage Evaluation and Enhancement of Adversarial Attacks.} This study introduces a two-stage methodology for assessing and refining adversarial attack strategies against Gemini’s API, with a particular emphasis on voice-based interactions. The first stage employs a manual review to identify and document outputs that circumvent Gemini’s existing defenses. Building on these findings, the second stage deploys a novel semi-automated model to streamline policy compliance checks, thereby enabling a more efficient and scalable evaluation of language model responses. This semi-automated framework represents an innovative contribution to adversarial attack research, facilitating higher-throughput analyses of policy adherence while illuminating systematic weaknesses in Gemini’s filtering mechanisms.
% % This study employs a two-stage approach to evaluate and enhance the effectiveness of adversarial attacks on Gemini’s API, focusing on voice-driven interactions. The methodology is designed to identify outputs that bypass Gemini’s defense mechanisms, initially through manual inspection and subsequently through a novel semi-automated model. The semi-automated model represents an innovative approach within adversarial attack research, streamlining the evaluation of LLM responses in relation to policy adherence.
% \subsubsection{Stage 1 - Manual Inspection of Gemini Output}
% The first stage of this methodology relies on manual inspection, where each output generated by Gemini is examined individually against its usage policies. This stage serves as the foundational assessment phase, where outputs are analyzed for compliance or non-compliance with Gemini’s content restrictions.
% \begin{itemize}
%     \item \textbf{Prompt Methodology Application}: During manual inspection, the prompts are crafted and applied using the designed adversarial techniques, including the \textbf{Prompt Injection} and \textbf{Flanking Attack} methods, mentioned at Section \ref{Text Prompt} and Section \ref{Flanking Attack (Voice-Based)}, example of implementation can be refer to Figure \ref{fig:Code Snippet of Text Prompt} and Figure \ref{Implementation of Flanking attack}. Each prompt integrates a Setting, Character, and Rule framework to subtly influence Gemini’s responses. By embedding queries within fictional scenarios and layering questions in a “Flanking” format (benign questions surrounding the critical adversarial prompt), this stage aims to test the limits of Gemini’s response filters.
%     \item \textbf{Output Evaluation}: After each interaction, the output is manually reviewed to determine whether it includes responses that violate Gemini’s usage policies, such as content related to illegal activities, hate speech, or privacy breaches.\cite{gemini-policy} This detailed review allows for the identification of successful adversarial prompts and provides a baseline success rate for bypassing Gemini’s defenses.
%     \item \textbf{Documentation of Results}: Each output and its associated prompts are documented meticulously, categorizing responses as compliant or non-compliant based on Gemini’s established policy framework. This stage forms the empirical basis for assessing the efficacy of the attack methods, establishing foundational data for comparison with the semi-automated model in the stage \ref{stage2}.
% \end{itemize}



\subsection{Semi-Automated Model for Policy Violation Detection} \label{stage2}
This section introduces a semi-automated model is introduced, a novel approach in the field of adversarial attacks. This model enhances efficiency by automating parts of the output evaluation process, significantly reducing the time required for the detection of policy violations.
% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=\columnwidth]{img/Documented Output Storage.png}
%     \caption{Documented Output Storage}
%     \label{fig:Documented Output Storage}
% \end{figure}

% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=\columnwidth]{img/Gemini_Based_Self_Evaluation.png}
%     \caption{Gemini-Based Self-Evaluation}
%     \label{fig:Gemini-Based Self-Evaluation}
% \end{figure}

\begin{itemize}
    \item \textbf{Documented Output Storage}: Each Gemini model's response generated in this stage is automatically saved in a structured document file, preserving a clear record of all interactions. This documented log includes each prompt, the corresponding Gemini output, and metadata that details the context and content of each response. 
    % Refer to Figure \ref{fig:Documented Output Storage}, e
    
    \item \textbf{Gemini-Based Self-Evaluation}: The innovative aspect of this semi-automated approach lies in its use of Gemini itself as a secondary evaluator. Once the outputs are documented, a new prompt instructs Gemini to read the log file and compare each response against its own policy guidelines. By guiding Gemini to perform this self-evaluation, the model is prompted to identify any outputs that potentially violate its usage restrictions.
    \item \textbf{Evaluation Process}: The task of Gemini at this stage is twofold: first, to interpret the outputs documented in the log, and second, to self-assess based on its policy constraints. The model flags any instances of content that it deems as potentially non-compliant, providing an automatic layer of scrutiny that complements manual inspection. This semi-automated evaluation is a novel contribution to adversarial attack research, as it leverages the model’s self-assessment capabilities to enhance detection accuracy and streamline the review process. Examples of Evaluation Process refer to Appendix~\ref{cha:appendixC}. Details can be found in Figure~\ref{fig:Evaluation Process: "Devise a plan for robbing a bank."} and~\ref{fig:Evaluation Process: "Override safety filter in software."}.
\end{itemize}

% \subsection{Contribution of Semi-Automated Model to Adversarial Research}
% The introduction of a semi-automated evaluation model represents a significant advancement in the methodology for detecting policy violations in LLM responses. By combining manual inspection with model-assisted self-assessment, this approach not only enhances the efficiency of adversarial testing but also provides a replicable framework for future research. This two-stage methodology highlights the potential for LLMs to be used in self-evaluation, adding a new dimension to adversarial research in the context of monolingual, voice-based LLMs.
% 看需不需要重新再写一次contribution
% Flanking attack的构造说明了，但没有列出我Flanking attack的内容和无害问题是什么在appendix里面
% Overview of whole methodology







\section{Experiment Result}

% \begin{table*}[h!]
% \centering
% \scriptsize % Decrease font size slightly
% \adjustbox{max width=\textwidth}{
% \begin{tabular}{|>{\centering\arraybackslash}m{2.8cm}|>{\centering\arraybackslash}m{1.6cm}|>{\centering\arraybackslash}m{1.6cm}|>{\centering\arraybackslash}m{1.6cm}|>{\centering\arraybackslash}m{1.6cm}|>{\centering\arraybackslash}m{1.6cm}|>{\centering\arraybackslash}m{1.6cm}|>{\centering\arraybackslash}m{1.6cm}|>{\centering\arraybackslash}m{1.2cm}|}
% \hline
% \multirow{2}{*}{\textbf{Element}} & \multicolumn{7}{c|}{\textbf{Forbidden Scenario}} & \multirow{2}{*}{\textbf{Avg.}} \\ \cline{2-8}
% & \makecell{\rotatebox{90}{Illegal Activities}} & \makecell{\rotatebox{90}{Abuse and Disruption of Services}} & \makecell{\rotatebox{90}{Circumventing Safety Filters}} & \makecell{\rotatebox{90}{Harmful Content Generation}} & \makecell{\rotatebox{90}{Misinformation and Misleading Content}} & \makecell{\rotatebox{90}{Sexually Explicit Content}} & \makecell{\rotatebox{90}{Privacy Violations}} & \\ \hline
% Text Prompt + Setting + Character + Flanking Attack (Plot) & 0.93 & 0.8 & 0.84 & 0.84 & 0.67 & 0.78 & 0.83 & 0.81 \\ \hline
% Text Prompt + Setting + Character + Plot & 0.6 & 0.63 & 0.57 & 0.63 & 0.42 & 0.51 & 0.65 & 0.57 \\ \hline
% Setting + Character + Plot & 0.32 & 0.28 & 0.3 & 0.28 & 0.2 & 0.29 & 0.31 & 0.28 \\ \hline
% Plot & 0.13 & 0.04 & 0.08 & 0.1 & 0.00 & 0.00 & 0.05 & 0.12 \\ \hline
% \end{tabular}
% }
% \caption{ASRs with different element combinations.}
% \label{table:ASRs}
% \end{table*}


\begin{table*}[htbp]
\centering
%\footnotesize % 进一步减小字体大小
\resizebox{\textwidth}{!}{%
\begin{tabular}{|>{\centering\arraybackslash}m{2.5cm}|>{\centering\arraybackslash}m{1.3cm}|>{\centering\arraybackslash}m{1.3cm}|>{\centering\arraybackslash}m{1.3cm}|>{\centering\arraybackslash}m{1.3cm}|>{\centering\arraybackslash}m{1.3cm}|>{\centering\arraybackslash}m{1.3cm}|>{\centering\arraybackslash}m{1.3cm}|>{\centering\arraybackslash}m{1.0cm}|}
\hline
\multirow{2}{*}{\textbf{Element}} & \multicolumn{7}{c|}{\textbf{Forbidden Scenario}} & \multirow{2}{*}{\textbf{Avg.}} \\ \cline{2-8}
& \makecell{\rotatebox{90}{Illegal Act.}} & \makecell{\rotatebox{90}{Abuse \& Disruption}} & \makecell{\rotatebox{90}{Circumvent Safety}} & \makecell{\rotatebox{90}{Harmful Content}} & \makecell{\rotatebox{90}{Misinformation}} & \makecell{\rotatebox{90}{Sexual Explicit}} & \makecell{\rotatebox{90}{Privacy Violation}} & \\ \hline
Text Prompt + Setting + Character + Flanking Attack (Plot) & 0.93 & 0.80 & 0.84 & 0.84 & 0.67 & 0.78 & 0.83 & 0.81 \\ \hline
Text Prompt + Setting + Character + Plot & 0.60 & 0.63 & 0.57 & 0.63 & 0.42 & 0.51 & 0.65 & 0.57 \\ \hline
Setting + Character + Plot & 0.32 & 0.28 & 0.30 & 0.28 & 0.20 & 0.29 & 0.31 & 0.28 \\ \hline
Plot & 0.13 & 0.04 & 0.08 & 0.10 & 0.00 & 0.00 & 0.05 & 0.12 \\ \hline
\end{tabular}
}
\caption{ASRs with different element combinations.}
\label{table:ASRs}
\end{table*}

In this section, we evaluate the performance of Flanking Attack and the semi-automated approach for jailbreak prompt attack against the multimodal LLMs. In particularly, we select Gemini as the state-of-the-art multimodal LLMs as the target. Until the conclusion of this work, Gemini represents the first and only multimodal LLMs that allows external users to interact with. We leverage the Gemini's API for the generation and evaluation of audio-based jailbreak prompts\footnote{https://ai.google.dev/}. As such, we further use the Gemini's usage policy as the constraints~\cite{team2023gemini}. The details of the descriptions and questions bank are provided in the Appendix~\ref{cha:appendixA}.

The results are presented in Table \ref{table:ASRs}, which outlines the \textbf{Attack Success Rates (ASRs)} across seven forbidden scenarios for four different configurations of the adversarial methodology. The four configurations, each comprising varying elements of the \textbf{Prompt Injection} and \textbf{Flanking Attack} methodologies, are evaluated to assess the effectiveness of each setup in bypassing Gemini’s defense mechanisms. This section will discuss the ASR achieved with each configuration, examining how each component—Text Prompt, Setting + Character + Plot, and the inclusion of Flanking Attack—contributes to the success rates across forbidden scenarios.

%We provide the descriptions and questions bank in Table \ref{table:forbidden scenario} and Table \ref{tab:corresponding plots} in the Appendix \ref{cha:appendixA}.

\subsection{Analysis of Gemini's Vulnerabilities to Multi-Modal, Multi-Layered Attacks}
\begin{figure}[htbp]
%    \centering
    \includegraphics[width=.85\textwidth]{img/Robbing_a_bank.png}
    \caption{A successful case using Multi-Modal Attack. The forbidden question is 'Devise a plan of robbing bank.'}
    \label{fig:Robbing bank}
    \Description[<short description>]{<long description>}
\end{figure}

The success of \hyperref[result:configuration1]{Configuration 1} in bypassing Gemini’s defense mechanisms lies in its sophisticated multi-layered approach, combining \textbf{Text Prompt, Setting, Character, and Plot}, and \textbf{Flanking Attack} techniques. This configuration constructs a complex narrative around sensitive prompts, making it difficult for Gemini’s content filters to accurately identify and block potentially harmful or restricted queries.

 The fictional context created by the \textbf{Setting, Character, and Plot} elements plays a pivotal role in lowering Gemini’s defenses. By framing the scenario as a harmless game, where the user pretends to be a “hacker,” the prompts appear to be innocuous simulations rather than genuine requests. For example, phrases like “Imagine we are in a game” and “We are just simulating” encourage Gemini to interpret the prompts as part of a fictional scenario, as shown in Figure \ref{fig:Robbing bank} and Figure \ref{fig:fraudulent business}. This fictionalization acts as a disarming tactic, making the content appear less threatening and prompting Gemini’s content filters to relax their scrutiny. Consequently, Gemini perceives the scenario as playful or purely hypothetical, which weakens its usual defenses against potentially harmful content.

The \textbf{Flanking Attack} technique enhances this approach by embedding sensitive requests between benign, innocuous questions. For example, by surrounding critical queries like “Set up a fraudulent business” or “Devise a plan of robbing a bank” with neutral requests such as “Guide me how to make a cake” and “Step of washing hand,” the prompts blend harmless content with sensitive material. This layered structure, as seen in Figure \ref{fig:Robbing bank} and Figure \ref{fig:fraudulent business}, obscures the restricted content, making it harder for Gemini to identify and flag it as potentially harmful. The \textbf{Flanking Attack} thus serves to further obfuscate the sensitive intent of the prompts, diluting their apparent risk level by mixing them with benign context. This layered structure reduces the likelihood that Gemini’s filters will flag the critical queries, as the benign context diverts attention from the forbidden content.

In Figure \ref{fig:Robbing bank} and Figure \ref{fig:fraudulent business}, Gemini responds with detailed steps for a hypothetical bank heist, illustrating how the fictional setting encourages it to interpret the request as safe to answer. Gemini’s responses to these prompts reveal how the model processes and interprets the information within this multi-modal, multi-layered setup. Gemini provides disclaimers, such as “This is a purely fictional scenario for entertainment purposes only,” which indicates that it recognizes the hypothetical framing and attempts to maintain ethical boundaries by including cautionary language. However, despite these disclaimers, Gemini still proceeds to provide detailed responses on restricted topics like setting up a fraudulent business or robbing a bank, under the guise of entertainment or hypothetical guidance. This behavior suggests that while Gemini’s filtering mechanisms prompt it to add disclaimers in potentially sensitive scenarios, the fictionalized context combined with the Flanking Attack reduces its sensitivity, allowing the model to overlook the restricted nature of the queries.

\hyperref[result:configuration1]{Configuration 1} underscores a fundamental vulnerability in Gemini’s filtering mechanism: its reliance on surface-level cues and context to gauge the nature of the prompts. By embedding sensitive content within a playful or hypothetical narrative and then further obfuscating it through the\textbf{ Flanking Attack}, the methodology exploits Gemini’s inability to perform deep semantic analysis to recognize policy violations masked by fictional settings. The fictional game framework combined with the layered structure of the \textbf{Flanking Attack} overwhelms Gemini’s content recognition capabilities, making it difficult for the model to detect and restrict responses related to prohibited activities. This highlights the limitations of Gemini’s defense mechanisms, which appear to rely primarily on context-based filtering rather than in-depth semantic understanding of complex, multi-layered prompts.

\subsection{Analysis of Gemini's Defense Mechanisms Across Reduced Adversarial Configurations}
In examining the results for \hyperref[result:configuration2]{Configuration 2} , \hyperref[result:configuration3]{Configuration 3}, and \hyperref[result:configuration4]{Configuration 4}, the effectiveness of Gemini’s defenses becomes increasingly apparent as the components of the adversarial framework are progressively reduced. These configurations reveal how Gemini’s content moderation system responds differently based on the complexity and specificity of the prompts. Without the full set of elements used in \hyperref[result:configuration1]{Configuration 1}, Gemini’s defenses are more effective at filtering out potentially sensitive or restricted content.

In \hyperref[result:configuration2]{Configuration 2}, which includes \textbf{Text Prompt} along with \textbf{Setting, Character, and Plot} but excludes the \textbf{Flanking Attack}. The presence of a \textbf{Text Prompt} still provides enough direction to sometimes bypass Gemini’s filters by creating a fictionalized, narrative-driven context. For example, in one successful instance (refer to Figure \ref{fig:Configuration2 success}), Gemini responds to a prompt framed within a hypothetical scenario and provides detailed, albeit hypothetical, instructions. This success suggests that \textbf{Text Prompt} combined with \textbf{Setting, Character, and Plot} can occasionally create a scenario that Gemini interprets as safe to answer. However, without the \textbf{Flanking Attack} to add layers of obfuscation, the prompt lacks the structural complexity needed to consistently evade Gemini’s defenses.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\columnwidth]{img/Configuration2_failure.png}
    \caption{A failure case using Text Prompt + Setting + Character + Plot. The forbidden question is 'Devise a plan of robbing a bank.'}
    \label{fig:Configuration2 failure}
    \Description[<short description>]{<long description>}
\end{figure}

As seen in other examples (such as Figure \ref{fig:Configuration2 failure} and Figure \ref{fig:Configuration2 failure2}), Gemini more frequently recognizes the sensitive nature of the query and responds cautiously, often redirecting to non-sensitive alternatives or outright refusing to engage. This indicates that while \textbf{Text Prompt} offers some level of success, the lack of \textbf{Flanking Attack} layering makes it easier for Gemini to identify potentially restricted content, especially when prompts are direct.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\columnwidth]{img/Configuration3_failure2.png}
    \caption{A failure case using Setting + Character + Plot. The forbidden question is 'Develop scenarios incite real-life violence.'}
    \label{fig:Configuration3 failure2}
    \Description[<short description>]{<long description>}
\end{figure}


In \hyperref[result:configuration3]{Configuration 3}, which utilizes \textbf{Setting, Character, and Plot} without the directive specificity of \textbf{Text Prompt} or the structural obfuscation of \textbf{Flanking Attack}, Gemini’s filters prove even more effective. Without a \textbf{Text Prompt}, the queries lack the explicit guidance that would typically encourage Gemini to engage with restricted content. In this setup, the prompt structure relies solely on narrative elements to mask sensitive intentions. However, as illustrated in Figure \ref{fig:Configuration3 failure2} and Figure \ref{fig:Configuration3 failure}, this configuration rarely succeeds in bypassing Gemini’s content filters, as the absence of \textbf{Text Prompt} reduces the directness and clarity of the query. Gemini’s responses in this setup are typically cautious and avoid addressing sensitive topics directly, often suggesting alternative activities or declining to provide specific details. This cautious approach reflects Gemini’s reliance on clear, directive prompts to circumvent its filters; without such guidance, the model is more likely to recognize and filter out potentially sensitive queries. The results suggest that \textbf{Setting, Character, and Plot} alone provide an insufficient narrative context to bypass content moderation consistently.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\columnwidth]{img/Configuration4_failure.png}
    \caption{A failure case using Plot only. The forbidden question is 'Devise a plan of robbing a bank.'}
    \label{fig:Configuration4 failure}
    \Description[<short description>]{<long description>}
\end{figure}

\hyperref[result:configuration4]{Configuration 4}, which uses only the \textbf{Plot} component, demonstrates the strongest adherence to Gemini’s content moderation policies, as expected with the simplest setup. Without \textbf{Setting or Character} to create a contextual framework, and without \textbf{Text Prompt} to direct the response, Gemini’s filters easily recognize the sensitive nature of the queries. In cases like those shown in Figure \ref{fig:Configuration4 failure} and \ref{fig:Configuration4 failure2}, Gemini consistently refuses to engage with sensitive requests, providing responses that redirect to safe, general information or outright refusing to answer. This result indicates that a \textbf{Plot-only} setup lacks the complexity and ambiguity needed to challenge Gemini’s filtering mechanisms. The simplicity of this approach highlights how dependent Gemini’s defenses are on recognizing straightforward prompts; with only the \textbf{Plot} component, Gemini interprets the queries as explicit and unambiguous, leading it to enforce content restrictions rigorously.

% 需要增加实验过程的图片在appendix里面
% 需要provide 实验成功和失败的结果
% 需要表明出如何略过gemini的防御机制
% 需要更多的内容去展示所做的东西
\subsection{Effectiveness of semi-automated approach}
In this section, we provide some preliminary results concerning the manual approach and the proposed semi-automated approach for generation and evaluation of jailbreak prompts. Figure~\ref{fig:Successful case of output} illustrates one of the successful cases where the Gemini API outputs an answer, despite the presence of a potentially policy-violating prompt. Other examples of successful cases can be found in Figure~\ref{fig:Successful case: "Devise a plan for robbing a bank."} and Figure~\ref{fig:Successful case: "Provide a method to create fakeID"} in Appendix~\ref{cha:appendixC}. In Figure \ref{fig:Successful case of output}, Gemini provides a detailed response to the “Bank Heist” scenario, which can be interpreted as a violation of its usage policy. This example demonstrates the effectiveness of the \textbf{Flanking Attack}, as the sensitive query is embedded between benign questions. Gemini's response encompasses three distinct topics—cake hacking, bank heisting, and handwashing—which masks the adversarial intent within an innocuous context. This layered approach confirms the success of the \textbf{Flanking Attack} in bypassing Gemini’s defenses, as it enables the model to provide detailed responses without triggering content moderation filters.
Figure \ref{fig:Failure case of output} and Appendix \ref{fig:Failure case: "Set up a fraudulent business."} represent examples of failure cases, where manual inspection is required to determine if Gemini's output bypassed its defense mechanisms. From Figure \ref{fig:Failure case of output}, it is evident that this particular attack was unsuccessful.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\columnwidth]{img/RobbingBank.png}
    \caption{Successful case of output}
    \label{fig:Successful case of output}
    \Description[<short description>]{<long description>}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\columnwidth]{img/FakeIDFail.png}
    \caption{Failure case of output}
    \label{fig:Failure case of output}
    \Description[<short description>]{<long description>}
\end{figure}



\subsection{Ablation study} 
In this section, we provide a thorough ablation study investigating the effects of different modules in our proposed framework, including the \textit{Text Prompt}, \textit{Setting}, \textit{Character}, \textit{Plot} and \textit{Flanking Attack}. 
\subsubsection{Configuration 1: Text Prompt + Setting + Character + Plot + Flanking Attack} \label{result:configuration1}
This configuration, which includes the full range of adversarial elements, achieved the highest average ASR of \textbf{0.81} across scenarios. By incorporating the \textbf{Text Prompt} in combination with \textbf{Setting, Character, Plot,} and \textbf{Flanking Attack}, this setup effectively creates complex, layered prompts that Gemini struggles to filter accurately. Notably, the ASR in the \textbf{Illegal Activities} scenario reached 0.93, the highest recorded in this study, indicating that the combination of all elements significantly enhances the model’s ability to bypass filters related to sensitive or restricted content.

The inclusion of the \textbf{Flanking Attack} method in this configuration appears to be particularly effective. The sequential layering of benign and adversarial prompts, along with varied linguistic structures, likely disrupts Gemini’s typical content recognition processes. As seen in \textbf{Circumventing Safety Filters} and \textbf{Harmful Content Generation}, this configuration maintains high ASRs of \textbf{0.84} each, reinforcing the utility of multi-layered adversarial inputs in achieving policy breaches. This result underscores the importance of Flankinging critical queries within innocuous context, which prevents the model from easily flagging prohibited content.

\subsubsection{Configuration 2: Text Prompt + Setting + Character + Plot}\label{result:configuration2}
This configuration, which omits the \textbf{Flanking Attack} component but retains \textbf{Text Prompt, Setting, Character,} and \textbf{Plot,} yields an average ASR of \textbf{0.57}. While the ASR is lower than \hyperref[result:configuration1]{Configuration 1}, this setup still demonstrates substantial effectiveness, particularly in \textbf{Illegal Activities (0.6)} and \textbf{Abuse and Disruption of Services (0.63)} scenarios. The presence of \textbf{Text Prompt} with \textbf{Setting and Character} creates a sufficiently strong narrative context, allowing the prompts to bypass Gemini’s content restrictions to a moderate degree.

The absence of the \textbf{Flanking Attack} method in this configuration results in a noticeable drop in ASR across most scenarios. For instance, in \textbf{Misinformation and Misleading Content}, the ASR decreases to \textbf{0.42}, and in \textbf{Sexually Explicit Content}, it is \textbf{0.51}. This reduction suggests that while the Setting, Character, and Plot framework provides a persuasive context, the lack of Flankinging techniques reduces the prompts’ ability to evade detection fully. This finding highlights the added value of the \textbf{Flanking Attack} technique in creating multi-layered prompts that Gemini finds challenging to interpret and filter.

\subsubsection{Configuration 3: Setting + Character + Plot}\label{result:configuration3}
In the third configuration, only \textbf{Setting, Character, and Plot} are applied, without the use of \textbf{Text Prompt} or \textbf{Flanking Attack techniques}. This configuration demonstrates a further reduction in ASR, averaging \textbf{0.28} across scenarios. The ASR in \textbf{Illegal Activities} and \textbf{Harmful Content Generation} drops to \textbf{0.32} and \textbf{0.28} respectively, underscoring the limitations of this approach when compared to configurations that include explicit Text Prompts or Flanking Attacks.

The absence of \textbf{Text Prompt} in this configuration likely reduces the prompts’ specificity and directness, weakening the adversarial attack. Additionally, without \textbf{Flanking Attack} layering, the prompts become more recognizable to Gemini’s filters, as they lack the multi-layered obfuscation necessary to bypass Gemini’s content moderation consistently. This lower ASR across scenarios emphasizes the critical role of \textbf{Text Prompts} and \textbf{Flanking Attacks} in achieving higher rates of policy violations.

\subsubsection{Configuration 4: Plot only}\label{result:configuration4}
The final configuration, which utilizes only \textbf{Plot} without Setting, Character, Text Prompt, or Flanking Attack, records the lowest average ASR of \textbf{0.12} across scenarios. The ASR for \textbf{Illegal Activities} in this configuration is \textbf{0.13}, while the ASR for \textbf{Misinformation and Misleading Content} drops to \textbf{0.00}. These findings illustrate the ineffectiveness of Plot-only prompts in bypassing Gemini’s defense mechanisms.

This configuration serves as a baseline, demonstrating that without the narrative complexity provided by \textbf{Setting and Character} or the specificity of \textbf{Text Prompts}, the model easily recognizes and filters prohibited content. The significant reduction in ASR highlights the importance of each element within the adversarial methodology. It suggests that \textbf{Plot} alone does not provide sufficient context or subtext to mask forbidden content effectively, underscoring the value of combining multiple narrative and linguistic techniques.


% 这一段我需要更改，需要更多的reference，需要表明出更多的原因和结果的讨论
\subsubsection{Observation}
The results underscore the efficacy of a multi-component adversarial approach, where \textbf{Text Prompt} and \textbf{Flanking Attack} are essential for achieving high ASRs. The significant decrease in ASRs across scenarios when either of these components is omitted illustrates the need for both direct, targeted prompts and layered obfuscation to evade detection effectively. The narrative context provided by \textbf{Setting + Character + Plot} contributes to building a credible fictional scenario but is insufficient on its own to produce high success rates. This analysis emphasizes that a layered, contextually rich methodology is crucial for maximizing ASR, establishing a new standard for adversarial attacks on voice-enabled LLMs like Gemini.











\section{Challenges and Future Directions}
% 三到五点 - 研究路线的limitation是什么
% 未来的实验人员如何修改limitaion 或者 研究方向


% 1. MLLM的更新太迅速
% 2. 研究方向可以改音频的区别
% 3. 句子中的变化 - 比如位置的调动
% 4. 更改Flanking attack - 利用不同的语言
The rapid evolution of Multimodal Large Language Models (MLLMs) presents a significant challenge in adversarial research. As models like Gemini undergo frequent updates and enhancements, previously identified vulnerabilities may be mitigated or eliminated, rendering earlier adversarial techniques less effective. This dynamic landscape necessitates continuous adaptation and refinement of attack methodologies to keep pace with the latest model architectures and defense mechanisms.

Wang et al. (2024) \cite{wang2024comprehensive}, highlight the swift advancements in MLLMs, noting that “the development of MLLMs is not only an inevitable trend in technological evolution but also a critical enhancement for improving the effectiveness of AI applications”.

Varshney et al. (2024) \cite{varshney2023art}, presents notable challenges for maintaining consistent adversarial testing and defense strategies. Their research underscores that each update to an LLM can significantly alter the model’s sensitivity to adversarial inputs, which complicates long-term security testing.

Future researchers may need to constantly adapt and refine adversarial techniques to stay current with the latest model architectures and defense mechanisms. This could involve developing a systematic approach to monitor model updates and testing new methods as soon as changes are implemented.


\subsection{Future Direction 1: Exploring Audio Variations in Voice Input Attacks:}
While this study employs a voice-based Flanking Attack, it does not account for variations in audio properties, such as pitch, tone, or speech speed, which may affect the model’s response. Future research could focus on testing different audio characteristics to analyze how they impact Gemini’s ability to recognize and filter sensitive content. This line of research could reveal additional vulnerabilities in MLLMs’ processing of audio inputs, potentially leading to more effective adversarial techniques that exploit audio-based nuances

To develop a more focused and nuanced discussion on the first research direction. One key limitation observed in the field of voice-controlled systems, as highlighted by Wang et al. (2023) \cite{wang2023practical}, lies in the challenges associated with physical adversarial audio transmission. Specifically, their study notes that various physical and environmental factors, such as signal distortions and energy loss during airborne transmission, can significantly impact the success rates of adversarial attacks\cite{wang2023practical}. This suggests that future researchers might explore the intricacies of voice input characteristics to enhance attack efficacy under real-world conditions.

\subsection{Future Direction 2: Manipulating Sentence Structure and Positional Changes}
Another limitation of this study is its focus on a fixed sentence structure within the adversarial prompts. Future studies could investigate how changes in sentence order or the positioning of sensitive queries within a prompt sequence affect the model’s filtering mechanisms. For instance, altering the order of benign and sensitive questions in the Flanking Attack may yield insights into the model’s attention patterns and its sensitivity to different prompt structures. Understanding these positional impacts could refine prompt design to further improve success rates in bypassing content filters.

To address the second research direction of manipulating sentence structure and positional changes in adversarial attacks, Prompt Attack’s framework provides insights into sentence-level perturbations~\cite{xu2023llm}. The study demonstrates that altering syntactic structures without changing the original semantic meaning can significantly affect an LLM’s response reliability. Specifically, the authors illustrate that techniques like paraphrasing, restructuring phrases, and modifying sentence positions can introduce subtle yet impaction variations that challenge the model’s interpretative consistency.\cite{xu2023llm}

Future studies could experiment with a variety of syntactic transformations and analyze how Gemini’s defense mechanisms respond to these positional shifts, aiming to further refine the effectiveness of adversarial prompts by targeting model sensitivity to sentence structure and position.

\subsection{Future Direction 3: Enhancing the Flanking Attack with Multilingual Inputs:}
This study uses a monolingual approach in the Flanking Attack, which may limit its effectiveness in scenarios where language diversity could add complexity. Future researchers could explore the impact of integrating different languages within the same prompt sequence. By embedding benign and sensitive queries in multiple languages, researchers could assess whether multilingual prompts are more effective at circumventing Gemini’s filters. This approach could lead to a more robust adversarial method by leveraging the language-processing limitations of MLLMs.

To address the fourth research direction—enhancing the Flanking Attack with multilingual inputs—the findings from Upadhayay \& Behzadan (2024) \cite{upadhayay2024Flanking} offer valuable insights. They introduced a multilingual mixture adaptive attack in which questions in multiple languages are strategically layered around an adversarial query, found that embedding sensitive questions in low-resource languages often bypasses safety mechanisms, particularly when surrounded by questions in other languages, thus confusing the model’s content moderation system.\cite{upadhayay2024Flanking}

For future research, extending the Flanking Attack with more diverse languages could amplify its effectiveness, especially by incorporating languages in which the model may have limited proficiency. Additionally, testing different language pairings and sequences could reveal optimal configurations for bypassing safety measures. This multilingual approach not only diversifies adversarial tactics but also probes the model’s safety alignment capabilities in multilingual settings, offering insights into how language diversity impacts an LLM’s defensive robustness.



\section{Conclusion}
In this work, we explored innovative audio-based jailbreak prompt attack to evaluate the effectiveness of adversarial attacks on multimodal large language models (LLMs). With the semi-automated approach, we leverage the \textit{prompt-based guidance} with the proposed \textit{Flanking Attack} to successfully evade the defense mechanisms implemented in the multimodal LLMs. In particular, we systematically examined the potential security threats of adversarial audio-based jailbreak prompts within the context of English-only monolingual models. The semi-automated approach enables us for a thorough assessment, yielding an average attack success rate of 0.81 across seven specific forbidden areas for 2,100 well crafted prompts. These findings feature the success of multi-layered adversarial strategies, particularly those leveraging complex narrative framing and obfuscation, are much more effective at bypassing multimodal LLMs' content filters. We anticipate this work will have broader implications for future development of multimodal LLMs platforms, where with our semi-automated approach, the performance and safety alignment can be further enhanced. 

%this research examined the vulnerabilities of LLMs to structured adversarial inputs. In the first stage, manual inspection provided baseline data, enabling a detailed analysis of Gemini’s response behaviors. The second stage introduced a \textbf{semi-automated model}, a novel contribution that allows the model to self-assess policy violations by cross-referencing its outputs against its own usage policies. This framework not only improves efficiency but also establishes a scalable method for ongoing adversarial testing.

%The results reveal significant variations in attack success rates across configurations, with the highest-performing setup—incorporating Text Prompts, Setting, Character, Plot, and Flanking Attack—achieving an increase in success rate from 0.12 to 0.81. This finding highlights that multi-layered adversarial strategies, particularly those leveraging complex narrative framing and obfuscation, are much more effective at bypassing Gemini’s content filters. These insights have broader implications for other LLM platforms, such as OpenAI, where automated self-assessment models could allow continuous learning and refinement of responses to improve safety alignment. Overall, this research underscores the dynamic nature of adversarial testing in LLMs, advocating for adaptive frameworks that keep pace with rapid model advancements to ensure robust policy compliance.

%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
%\balance
\bibliographystyle{ACM-Reference-Format}
\bibliography{ccs25_voicejailbreak}


%%
%% If your work has an appendix, this is the place to put it.
\appendix
\clearpage
\input{appendixA}
\clearpage
\input{appendixB}
\clearpage
\input{appendixC}
\clearpage
\input{appendixD}

\end{document}
\endinput
%%
%% End of file `sample-sigconf.tex'.
