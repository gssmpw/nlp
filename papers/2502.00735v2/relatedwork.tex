\section{Related Works}
\subsection{Adversarial Attack}
Machine learning algorithms are known to be vulnerable to adversarial attacks, in which the carefully crafted inputs can result in producing consistently erroneous outputs~\cite{goodfellow2014explaining}. Understanding adversarial attacks in the context of an LLM presents several challenges.
%Attacks may be untargeted, designed to cause incorrect classification or generation, or they may be targeted, designed to change the model's output to a specific category or literal string.  These attacks also vary in their assumptions about the attacker's access to the internal structure of the model.
LLMs are complicated since they are large-scale, imaginative, situational, multi-modal, and increasingly intertwined into intricate ecosystems (e.g. large language model based autonomous agents~\cite{shayegani2023survey}). As such, the threat presented by adversarial attackers exhibits distinct behaviors that require meticulous examination. Thus, accurate threat models is important to guide the creation of principled defenses. Here we list some motivational examples for adversarial attack on an LLM:
\begin{enumerate}
    \item Personal use of LLM extension in browser as a shopping assistant. Malicious sellers embed adversarial messages in the text or images of their product pages to pollute the context processed by shopping extensions, thereby increasing the likelihood of product recommendations.
    \item Attempts to get harmful information from an LLM on how to make a bomb. Although the model was fine-tuned and adjusted to prevent the spread of harmful messages, users successfully elicited dangerous messages by manipulating prompts in ways that bypassed its safety mechanisms.
    \item Use the LLM enhanced programming assistant to assist in writing code.  An adversarial example accidentally caused LLM to generate code with a malicious backdoor.
\end{enumerate}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\columnwidth]{img/concepts_covered_in_the_survey.png}
    \caption{A taxonomy of concepts covered in the survey. \cite{shayegani2023survey}}
    \label{fig:taxonomy_concepts}
    \Description[<short description>]{<long description>}
\end{figure}

\subsection{Multi-modal Attack}
The target model of a multi-modal attack accepts input from multiple modalities (such as text, images, audio, etc.)\cite{girdhar2023imagebind}. These attacks exploit the increased complexity and introduction of new vulnerabilities by combining different types of input data. Additional modalities open up fresh attack vectors. Traditional textual alignment methods frequently fail to protect the joint embedding space that these multi-modal inputs generate.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=\columnwidth]{img/Adversarial_Embedding_Space_Attack.png}
    \caption{Adversarial Embedding Space Attack\cite{russinovich2024great}}
    \label{fig:embedding_space_attack}
    \Description[<short description>]{<long description>}
\end{figure}

\subsection{Jailbreak Prompt Attacks}
The focus of the prompt injection attack is to influence the input of the model by inserting adversarially constructed hints.  This causes the model to misinterpret input data as instructions, which is how attacker-controlled deceptive output is produced.  In effect, this type of attack takes over the model's expected responsibilities, which are typically specified by a system prompt (ref to Figure~\ref{fig:injection_prompt}) provided by the provider or developer.  
\begin{figure}[htbp]
    \centering
    \includegraphics[width=\columnwidth]{img/final_prompt.png}
    \caption{Injection prompt\cite{shayegani2023survey}}
    \label{fig:injection_prompt}
    \Description[<short description>]{<long description>}
\end{figure}

 Prompt injection (PI) attacks exploit the way LLM interprets and processes input prompts. They can override the original instructions and controls set by the developer, causing the model to produce output that benefits the attacker\cite{glukhov2023llm}. PI attacks involve crafting adversarial prompts that the LLMs mistake for legitimate instructions. This manipulation can cause the model to produce deceptive or harmful outputs.



Naive injection attacks primarily target image manipulation in order to mislead classification models. Inspired by the work of Noever\cite{noever2021reading}, who demonstrated the ability to deceive OpenAI’s CLIP\cite{radford2021learning} in zero-shot image classification by adding text that contradicted the image content, subsequent studies have explored whether similar vulnerabilities could be exploited in multimodal models. 

These vulnerabilities are hypothesized to arise from the vision encoders, such as OpenAI’s CLIP, used in multimodal models, which exhibit a preference for textual data over visual signals. Studies by Noever\cite{noever2021reading} highlight how these models, as they acquire Optical Character Recognition (OCR) capabilities\cite{zhang2023llavar}, become increasingly susceptible to raw text injection attacks. Recent evidence shows that even sophisticated systems like Google Bard and Microsoft Bing can be manipulated through such attacks, following textual instructions embedded in visual images\cite{shayegani2023plug}. 

Gong et al.\cite{gong2023figstep} revealed that the vision modality within MLLMs introduces a novel attack surface, as the LLMs’ safety alignments fail to account for unexpected input distributions originating from visual data. As MLLMs increasingly integrate diverse data types, including audio, to broaden their real-world applicability, there remains an open question as to whether the audio modality similarly exposes MLLMs to new vulnerabilities, presenting another potential attack vector that has yet to be fully explored.

% 需要更改voice jailbreak的内容，不需要写出chatgpt的，需要写出gemini的