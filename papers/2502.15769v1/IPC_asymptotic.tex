\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[margin=20mm]{geometry}
\usepackage{amsmath,amssymb}
\usepackage{algorithm,algorithmic}
\usepackage{graphicx}
\usepackage{bm}
\usepackage{color}
\usepackage{comment}
\usepackage{subcaption}
\usepackage{authblk}
\usepackage{hyperref}

\newcommand{\wt}{\widetilde}
\newcommand{\changed}[1]{{\sf\color[rgb]{1,0,0}{#1}}}
\newcommand{\com}[1]{{\sf\color[rgb]{0,0,1}{#1}}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\title{Asymptotic evaluation of the information processing capacity in reservoir computing}
\author{Yohei Saito \thanks{\href{mailto:saito.yohei450@mail.kyutech.jp}{saito.yohei450@mail.kyutech.jp}}}
\affil{Graduate School of Life Science and Systems Engineering, Kyushu Institute of Technology}
\date{}

\begin{document}
\maketitle
\begin{abstract}
The squared error normalized by the target output is known as the information processing capacity (IPC) and is used to evaluate the performance of reservoir computing (RC). 
Since RC aims to learn the relationship between input and output time series, we should evaluate the IPC for infinitely long data rather than the IPC for finite-length data. 
To evaluate the IPC for infinitely long data using the IPC for finite-length data, we use an asymptotic expansion of the IPC and the least-squares method. 
% However, simply calculating the IPC for sufficiently long data obscures the deviation from the limit value obtained for infinite data length. 
% We evaluate the IPC for infinitely long data using an asymptotic expansion of the IPC and the least squares method. 
Then, we show the validity of our method by numerical simulations. 
\begin{comment}
RC の性能評価では教師出力で規格化した二乗誤差が用いられており、IPC と呼ばれている。
RC の目的は入出力関係の学習なので、有限長のデータに対する IPC ではなく、無限長データに対する IPC を推定する必要がある。
しかし十分に長いデータの IPC を計算するだけでは、無限データ長で得られる極限値とのずれは曖昧なままである。
我々は IPC の漸近展開と最小二乗法のフィッティングを用いて無限長データに対する IPC を評価した。
また得られた式を数値シミュレーションを用いて検証し、極限値との残差を確認した。 
\end{comment}
\end{abstract}
\section{Introduction}
\begin{comment}
・時系列データ学習の重要性。RNNによる時系列データの近似と学習の困難さ。LSTMによる困難さの解消。しかし学習時間の長さは依然問題。
・学習時間を短縮したESNの解説。さらにより一般的なRCの説明。RCの長所と短所。
・RCの性能指標としてのIPCとその利点。単純な推定法の問題点。どのように解消するか？
・論文の構成。
\end{comment}
Since many kinds of data, e.g. video, audio, and stock prices, have time correlation, machine learning of time-series data is an important issue. 
% Since time correlation appears in many kinds of data, e.g. video, audio, and stock prices, machine learning of time-series data is an important issue. 
Recurrent neural networks (RNNs) can store past input by recursively connecting hidden nodes~\cite{rumelhart1986learning} and can approximate the relationship between input and output time series with arbitrary accuracy~\cite{schafer2006recurrent}. 
Backpropagation through time (BPTT) is mainly used to train RNNs, but it is difficult to optimize network parameters due to the gradient vanishing or the gradient explosion~\cite{bengio1994learning}. 
Many variants of RNNs, such as LSTM~\cite{hochreiter1997long} and GRU~\cite{cho2014learning}, have been proposed to solve the difficulty of training and have been very successful. 
However, BPTT calculations become slower for longer training data. 

An echo state network (ESN)~\cite{jaeger2004harnessing} is a kind of RNNs, which can finish training quickly by fixing the recurrent connections at the initial value and optimizing only the linear transformation of the readout layer. 
Not limited to neural networks, a linear combination of nonlinear dynamical systems can be used to approximate the relationship between input and output time series and is called a reservoir computing (RC) system~\cite{tanaka2019recent}. 
RC systems can also approximate the relationship between input and output time series with arbitrary accuracy~\cite{grigoryeva2018echo, gonon2019reservoir}. 
% The property in which the output gradually becomes less dependent on past inputs is called the fading memory property (FMP) (for a strict definition, see Ref.~\cite{grigoryeva2018echo}). 
% The input-output series correspondence with the FMP can be accurately approximated by RC\cite{grigoryeva2018echo, gonon2019reservoir}. 
% Since an RC system has a finite number of hidden nodes, there is a limit to the actual approximation accuracy.
Since no optimization is performed other than the linear transformation, an RC system is often inferior in performance to LSTM and other methods. 
However, it has the advantage that training finishes quickly by only calculating the pseudoinverse matrix. 
Therefore, it is more convenient than LSTM and other methods in situations where the target to be optimized changes frequently. 

The performance of an RC system is evaluated by the mean squared error or the squared error normalized by the target output, and the latter is called the information processing capacity (IPC)~\cite{dambre2012information, kubota2021unifying}. 
The IPC ranges between $0$ and $1$, representing the approximation accuracy. 
Since the goal of RC is to learn the relationship between input and output time series, it is necessary to evaluate the IPC for infinitely long data, not the IPC for finite-length data. 
The simplest method of estimating the IPC for infinitely long data is to calculate the IPC for sufficiently long data. 
However, their difference remains and should be removed as much as possible. 
In this paper, we estimate the IPC for infinitely long data using the asymptotic expansion of the IPC and the least-squares method. 

This paper is organized as follows. 
Section 2 reviews RC and its performance index, the IPC. 
In Section 3, we derive the asymptotic form of the IPC, and in Section 4, we show the validity of our method by numerical simulations. 
Section 5 summarizes the paper. 

\begin{comment}
動画、音声、株価など時間相関を持つデータは多いので、時系列データの学習は重要な課題である。
RNN は隠れ層を再帰的に結合させることで記憶効果を持たせた NN であり\cite{rumelhart1986learning}、入出力系列の対応関係を任意精度で近似できることが示されている（普遍性）\cite{schafer2006recurrent}。
RNN の学習には主に BPTT が用いられるが、勾配消失や勾配爆発により最適なネットワークパラメータの探索は難しい\cite{bengio1994learning}。
学習の困難さを解消するため LSTM\cite{hochreiter1997long}, GRU\cite{cho2014learning} など RNN の変種は多く提案され大きな成功を収めているが、BPTT は学習データが長くなるほど計算が遅くなる。
一方、学習の難しい再帰結合は初期値のまま固定し、読み取り層の線形変換のみ最適化することで学習を高速に終わらせる RNN も考案され、ESN と呼ばれている\cite{jaeger2004harnessing}。
NN に限らずより一般的な非線形ダイナミクスの線形結合で時系列データを近似する系は RC と呼ばれる\cite{tanaka2019recent}。
出力が過去の入力に徐々に依存しなくなっていく性質は fading memory property (FMP) と呼ばれているが（厳密な定義は文献 \cite{grigoryeva2018echo} 参照）、RC は FMP をもつ入出力系列の対応関係を任意精度で近似できることが示されている\cite{grigoryeva2018echo, gonon2019reservoir}。
RC の隠れノードは有限個なので実際の近似精度に限界はある。
また読み取り層以外の最適化は行わないため、LSTM などと比べて性能面で劣ることは多い。
しかし疑似逆行列計算のみで学習が終わるため非常に速いという利点がある。
そのため最適化する対象が頻繁に変わるような状況では LSTM などより便利である。
 
RC の性能評価には平均二乗誤差のほか、IPC と呼ばれる教師出力で規格化した二乗誤差も用いられている\cite{dambre2012information, kubota2021unifying}。
RC の目標は時系列データの入出力対応関係の学習なので、性能を評価するときは有限長データに対する IPC ではなく、無限長データに対する IPC を用いる必要がある。
無限長データに対する IPC のもっとも単純な推定法は、十分長いデータを使って IPC を計算することだが、データ長が無限の極限値との差がどの程度なのかはあいまいである。
本研究では漸近展開を用いて極限値との差を評価した。
さらにフィッティングを用いて無限長データに対する IPC を推定することで、より正確な評価が可能になった。
この論文は以下のように構成される。
2節で RC 及び性能指標である IPC のレビューを行う。
3節で IPC の漸近形を導出し、4節で数値シミュレーションを用いた確認をする。
5節で論文の要約を行う。
\end{comment}



\section{Review of RC and the IPC}  % review of the IPC
\begin{comment}
・レザバーとは何か軽く説明。オルテガを参考にして書く。
・レザバーのダイナミクス、コスト関数と最適化法を式で書く。
・普遍性がある。性能はそこそこだが、計算はすぐ終わる。
\end{comment}
The dynamics of RC is expressed as follows. 
\begin{align}
 x_t =& f(x_{t-1}, u_t),
 \label{eq:reservoir_nodes} \\
 y_t =& w^\top x_t + b,
 \label{eq:reservoir_output}
\end{align}
Here, $u_t \in \mathbb{R}^{d_0}, x_t \in \mathbb{R}^{d_1}, y_t \in \mathbb{R}^{d_2}$ are the values of the input, the hidden nodes, and the reservoir output at time $t$, respectively. 
To simplify the notation, we rewrite $(x_t^\top, 1)^\top$ as $x_t$, and $(w^\top, b)^\top$ as $w$, and Eq.~(\ref{eq:reservoir_output}) is expressed by 
\begin{align}
 y_t =& w^\top x_t.
 \label{eq:reservoir_output_2}
\end{align}
To obtain $x_t$ from Eq.~(\ref{eq:reservoir_nodes}), the values of the hidden nodes at a certain time $-\tau \, (< t)$ are required as the initial value. 
The initial value is not optimized in RC. 
Instead, we employ $f$, which reduces the dependence on the initial value through the recursive equation Eq.~(\ref{eq:reservoir_nodes}), and take a sufficiently large $\tau$. 
Furthermore, in most cases, $f$ reduces the dependence on the past input. 
For example, the hidden node values of the ESN, a typical example of RC systems, are given by~\cite{jaeger2002tutorial} 
\begin{align}
 x_t =& \tanh(v_1^\top x_{t-1} + v_2^\top u_{t-1} + c).
 \label{eq:ESN_nodes}
\end{align}

The cost function for an RC system is usually given by the mean squared error between the reservoir output sequence $(y_1, \ldots, y_T)$ and the target output sequence $(\hat{y}_1, \ldots, \hat{y}_T)$ for the input sequence $(\ldots, u_1, \ldots, u_T)$, 
\footnote{Although not used in this paper, a regularization term for $w$ may be added to the cost function.}
\begin{align}
 \frac{1}{T} \sum_{t=1}^T \|\hat{y}_t - y_t\|^2 = \frac{1}{T} \sum_{t=1}^T \|\hat{y}_t - w^\top x_t\|^2. 
 \label{eq:RC_cost}
\end{align}
The linear transformation of the readout layer, $w$, is optimized by 
\begin{align}
 w_T = \argmin_w \frac{1}{T} \sum_{t=1}^T \|\hat{y}_t - w^\top x_t\|^2.
 \label{eq:RC_optimize}
\end{align}
In addition to the mean squared error, a quantity called the IPC, defined below, 
\begin{align}
 1 - \frac{\min_w \frac{1}{T} \sum_{t=1}^T \|\hat{y}_t - w^\top x_t\|^2}{\frac{1}{T} \sum_{t=1}^T \|\hat{y}_t\|^2},
 \label{eq:finite_IPC}
\end{align}
is also used as a performance index for RC. 
Due to the normalization by the target output, the IPC ranges from $0$ to $1$, representing the accuracy of the approximation. 
Since the purpose of RC is to learn the relationship between input and output time series from a finite-length input/output set (training data), the actual performance is given by Eq.~(\ref{eq:finite_IPC}) with $T \to \infty$. 
Therefore, we should estimate the IPC for infinitely long training data from the IPC for finite-length training data. 
The simplest estimation method is to use sufficiently long training data. 
However, this method has the problem that we cannot evaluate the deviation from the limit value. 


\begin{comment}
RC のダイナミクスは次のように表される。
\begin{align}
x_t =& f(x_{t-1}, u_t),
\label{eq:reservoir_nodes} \\
y_t =& w x_t + b,
\label{eq:reservoir_output}
\end{align}
ここで、$u_t \in \mathbb{R}^{d_0}, x_t \in \mathbb{R}^{d_1}, y_t \in \mathbb{R}^{d_2}$ はそれぞれ時刻 $t$ における入力値、隠れノードの値、RC の出力値である。
表記を簡略化するため、$(x_t^\top, 1)^\top$ を $x_t$, $(w^\top, b)^\top$ を $w$ と書き直すと、式 (\ref{eq:reservoir_output}) は次のように表される。
\begin{align}
y_t =& w x_t.
\label{eq:reservoir_output_2}
\end{align}
式(\ref{eq:reservoir_nodes}) を用いるにはある時刻 $-\tau \, (< t)$ における隠れノードの値が初期値として必要になる。
RC では初期値を最適化することはなく、過去の入力ほど隠れノードの値に影響しなくなる性質(FMP)を満たすように $f$ を選び、初期値依存性をなくす。
例えば RC の代表例である ESN では以下のように与えられる\cite{jaeger2002tutorial}。
\begin{align}
x_t =& \tanh(v_1 x_{t-1} + v_2 u_{t-1} + c). 
\label{eq:ESN_nodes}
\end{align}

RC のコスト関数は通常、入力列 $(\ldots, u_1, \ldots, u_T)$ に対するレザバー出力列 $(y_1, \ldots, y_T)$ と教師出力列 $(\hat{y}_1, \ldots, \hat{y}_T)$ の間の平均二乗誤差によって与えられる。
\footnote{この論文では使用しないが、コスト関数に $w$ に対する正則化項を追加することもある。}
\begin{align}
\frac{1}{T} \sum_{t=1}^T \|\hat{y}_t - y_t\|^2
 = \frac{1}{T} \sum_{t=1}^T \|\hat{y}_t - w x_t\|^2.
\label{eq:RC_cost}
\end{align}
% 隠れ層のパラメータ $V_1, V_2, c$ は初期値に固定され、
読み出し層の線形変換 $w$ は次のように最適化される。
\begin{align}
w_T = \argmin_w \frac{1}{T} \sum_{t=1}^T \|\hat{y}_t - w x_t\|^2.
\label{eq:RC_optimize}
\end{align}
平均二乗誤差の他、以下で定義されるIPCと呼ばれる量
\begin{align}
1 - \frac{\min_w \frac{1}{T} \sum_{t=1}^T \|\hat{y}_t - w x_t\|^2}{\frac{1}{T} \sum_{t=1}^T \|\hat{y}_t\|^2}, 
\label{eq:finite_IPC}
\end{align}
も RC の性能指標として使用される。
教師出力による正規化のため、IPC のとる範囲は$0$から$1$になり、性能の良しあしを直感的に理解しやすいという利点がある。
RC の目的は有限長の入出力セット（訓練データ）から入力と出力の対応関係を学習することなので、実際の性能を反映するのは式(\ref{eq:finite_IPC})で $T \to \infty$ としたものである。
そのため有限長の訓練データに対する IPC から、無限長のデータセットに対する IPC を推定する必要がある。
最も簡単な推定方法は入力列を長くすることだが、この方法では極限値からのずれを評価できないという問題がある。
\end{comment}




\section{Asymptotic expansion of the IPC}
\begin{comment}
・性能はただの２乗ノルムではなく、正規化された２乗ノルムを使う方がよい。理由は Dumbre 参考。
・total IPC の上限や、既存の推定法について紹介。Dumbre 参考。
\end{comment}
In this section, we estimate the IPC for infinitely long data using the asymptotic expansion. 
First, we summarize the RC dynamics by referring to Ref.~\cite{dambre2012information, grigoryeva2018echo, gonon2019reservoir}. 
From Eq.~(\ref{eq:reservoir_nodes}), the values of the hidden nodes of the RC system at time $t$ are determined by the initial value $x_{-\tau}$ and the input sequence $(u_{-\tau}, \ldots, u_t)$. 
As we have explained, the RC system is usually designed to reduce dependence on the initial value and the past input. 
Hence, taking $\tau \to \infty$, we can consider that $x_t$ is determined only by the input sequence $(u_{t - s})_{s=0}^\infty$. 
Due to time-independence of Eq.~(\ref{eq:reservoir_nodes}), if we give the same input sequence at two different times, $(u_{t-s})_{s=0}^\infty = (v_{t'-s})_{s=0}^\infty$, the hidden node values at $t$ with the input sequence $(u_{t-s})_{s=0}^\infty$ and those at $t'$ with $(v_{t'-s})_{s=0}^\infty$ are the same. 
This means that the values of the hidden nodes are determined by a sequence of real numbers, and the hidden nodes of the RC system can be considered as the mapping from a real number sequence (input sequence) to a real vector (hidden node variables). 
To simplify the notation, in the following, the input sequence $(u_{t - s})_{s=0}^\infty$ is represented as $u'_t$, and ${\cal U} \subseteq (\mathbb{R}^{d_0})^{\mathbb{N}}$ denotes the set of input sequences. 
We express the target output function as $\hat{y} \colon {\cal U} \to \mathbb{R}^{d_2}$ and the reservoir function as $x \colon {\cal U} \to \mathbb{R}^{d_1+1}$. 
We assume the components of $x$, namely, $x_1, \ldots, x_{d}, x_{d+1}$, are linearly independent. 

Next, we consider the stochasticity of the input sequence. 
Following Ref.~\cite{dambre2012information, gonon2019reservoir}, we assume that $U = (U_t)_{t \in \mathbb{Z}}$ is a stationary ergodic process. 
We use the notation $U'_t = (U_s)_{s \leq t}$, and in particular $U' = (U_t)_{t \leq 0}$. 
The target output sequence is given by $\{\hat{y}(U'_1), ..., \hat{y}(U'_T), ..., \hat{y}(U'_{T+T'})\}$, and we divide it into training data $\{ (U'_t, \hat{y}(U'_t)) \}_{t=1}^T$ and test data $\{ (U'_t, \hat{y}(U'_t)) \}_{t=T+1}^{T+T'}$. 
We assume $T' = O(T)$ in this paper. 
The readout layer is optimized by the training data, 
\begin{align}
 w_T =& \min_w \frac{1}{T} \sum_{t=1}^T \|\hat{y}(U'_t) - w^\top x(U'_t)\|^2,
 \label{eq:w_MLE}
\end{align}
where we use the notation $\|x(U'_t)\|^2 = x(U'_t)^\top x(U'_t)$. 
The IPCs for the training and the test data at $w_T$ is called the training IPC and the test IPC, respectively, which are given by 
\begin{align} 
 C_T =& 1 - \frac{\frac{1}{T} \sum_{t=1}^T \|\hat{y}(U'_t) - w_T^\top x(U'_t)\|^2}{\frac{1}{T} \sum_{t=1}^T \|\hat{y}(U'_t)\|^2} 
  = 1 - \frac{l_T(w_T)}{\mu_T}, 
  \label{eq:IPC_train} \\ 
 C'_{T, T'} =& 1 - \frac{\frac{1}{T'} \sum_{t=T+1}^{T+T'} \|\hat{y}(U'_t) - w_T^\top x(U'_t)\|^2}{\frac{1}{T'} \sum_{t=T+1}^{T+T'} \|\hat{y}(U'_t)\|^2}
 = 1 - \frac{l'_{T'}(w_T)}{\mu_{T'}}. 
 \label{eq:IPC_test}
\end{align}
For later convenience, we express the numerators and denominators of Eqs.~(\ref{eq:IPC_train}) and (\ref{eq:IPC_test}) as 
\begin{align}
 l_T(w) =& \frac{1}{T} \sum_{t=1}^T \|\hat{y}(U'_t) - w^\top x(U'_t)\|^2,
 \label{eq:reservoir_norm_train} \\
 l'_{T'}(w) =& \frac{1}{T'} \sum_{t=T+1}^{T+T'} \|\hat{y}(U'_t) - w^\top x(U'_t)\|^2,
 \label{eq:reservoir_norm_test} \\
 \mu_T =& \frac{1}{T} \sum_{t=1}^T \|\hat{y}(U'_t)\|^2.
 \label{eq:teacher_norm}
\end{align}

Now, we derive the asymptotic forms of the training and the test IPC. 
In this paper, the IPC for infinitely long training data is called the true IPC $C_0$, and $w$ at that time is called the true parameter $w_0$. 
Since RC and the optimized value $w_T$ can be considered a linear regression model and its optimal parameter, the asymptotic theory of linear regression models in Ref.~\cite{white2014asymptotic} can be applied. 
First, we introduce the notation, 
\begin{align} 
 l(w, U'_t) =& \|\hat{y}(U'_t) - w^\top x(U'_t)\|^2, \\
 % f(w, U') =& \frac{1}{2} \|\hat{y}(U') - w x(U')\|^2 - \frac{1}{2} \|\hat{y}(U') - w_0 x(U')\|^2, 
 \mu(U'_t) =& \|\hat{y}(U'_t)\|^2, 
\end{align}
and assume 
\begin{align}
 \mathbb{E}[l(w, U')] < \infty, \ \ 
 \mathbb{E}[\mu(U')] < \infty. 
\end{align}
Then, from the law of large numbers, $l_T(w)$ converges in probability to $l(w) = \mathbb{E}[l(w, U')]$ for $T \to \infty$, and $\mu_T$ to $\mu_0 = \mathbb{E}[\mu(U')]$. 
We find that $C_0$ and $w_0$ can be expressed by 
\begin{align}
 C_0 =& 1 - \frac{\min_w l(w)}{\mu_0} = 1 - \frac{l(w_0)}{\mu_0},
 \label{eq:true_IPC} \\
 w_0 =& \argmin_w l(w).
 \label{eq:true_parameter}
\end{align}
Next, we introduce $\zeta(U'_t) = \mu(U'_t) - \mu_0$ and define the following quantities, 
\begin{align}
 I_\infty(w) 
 =& \frac{1}{4} \sum_{t=0}^\infty {\rm Cov}(\nabla_w l(w, U'_0), \nabla_w l(w, U'_t)^\top)
 + \frac{1}{4}\sum_{t=1}^\infty {\rm Cov}(\nabla_w l(w, U'_0), \nabla_w l(w, U'_{-t})^\top), 
 % =& \sum_{t=0}^\infty {\rm Cov}(\nabla_w f(w, U'_0), \nabla_w f(w, U'_t)^\top)|_{w = w_0} + \sum_{t=1}^\infty {\rm Cov}(\nabla_w f(w, U'_0), \nabla_w f(w, U'_{-t})^\top)|_{w = w_0}, 
  \label{def:I_inf} \\ 
 J =& \frac{1}{2} \frac{\partial^2 l(w)}{\partial w \partial w}\biggr|_{w = w_0}, \ \ 
 J_{ij, kl} = \mathbb{E}[x_i(U') \, x_k(U') \, \delta_{jl}], 
  \label{def:J} \\
 V_{\zeta, \infty} 
  =& \sum_{t=0}^\infty {\rm Cov}(\zeta(U'_0), \zeta(U'_t)) 
  + \sum_{t=1}^\infty {\rm Cov}(\zeta(U'_0), \zeta(U'_{-t})), 
  \label{def:V_mu_inf} \\
 C_{l, \zeta}(w) 
  =& \sum_{t=0}^\infty {\rm Cov}(l(w, U'_0) \, , \zeta(U'_t)) 
  + \sum_{t=1}^\infty {\rm Cov}(l(w, U'_0) \, , \zeta(U'_{-t})), 
  \label{def:l_zeta_cov} \\ 
 V_{l, \infty}(w) 
  =& \sum_{t=0}^\infty {\rm Cov}(l(w, U'_0) \, , l(w, U'_t))
  + \sum_{t=1}^\infty {\rm Cov}(l(w, U'_0) \, , l(w, U'_{-t})). 
  \label{def:l_auto_cov}
\end{align}
We further assume that $I_\infty(w), J, V_{\zeta, \infty}, C_{l,\zeta}$(w), and $V_{l, \infty}(w)$ are finite matrices. 
Below, we express $I_\infty(w_0), C_{l,\zeta}(w_0)$, and $V_{l, \infty}(w_0)$ as $I_\infty, C_{l,\zeta}$, and $V_{l, \infty}$, respectively. 
% The true parameter $w_0$ of RC is uniquely determined, so it is a regular statistical model.
% Also, $w_T$ in equation (\ref{eq:w_MLE}) can be considered as the maximum likelihood estimate using a normal distribution as the likelihood function.
% Referring to the literature on regular statistical models, \com{(What to see?)}, $w_T$ probabilistically converges to the true parameter $w_0$,
Then, the following quantities, 
\begin{align}
 \xi_T 
  =& \frac{1}{2} J^{-1/2} \nabla_w \frac{1}{\sqrt{T}} \sum_{t=1}^T (l(w, U'_t) - l(w)) \Bigr|_{w=w_0}
  = \frac{1}{2 \sqrt{T}} J^{-1/2} \sum_{t=1}^T \nabla_w l(w_0, U'_t) \, , \\
 \xi'_{T'} 
  =& \frac{1}{2} J^{-1/2} \nabla_w \frac{1}{\sqrt{T'}} \sum_{t=T+1}^{T+T'} (l(w, U'_t) - l(w)) \Bigr|_{w=w_0}
  = \frac{1}{2 \sqrt{T'}} J^{-1/2} \sum_{t=T+1}^{T+T'} \nabla_w l(w_0, U'_t) \, ,
\end{align}
converge in distribution,  
\begin{align}
 \xi_T \xrightarrow{d}& {\cal N}(0, J^{-1/2} I_\infty J^{-1/2}),
  \label{eq:xi_converge} \\
 \xi'_{T'} \xrightarrow{d}& {\cal N}(0, J^{-1/2} I_\infty J^{-1/2}). 
  \label{eq:xi'_converge}
\end{align}
Following Ref.~\cite{watanabe2009algebraic}, the asymptotic forms can be derived from the mean-value theorem. 
From the mean-value theorem, there exists $c \in [0, 1]$ which satisfies 
\begin{align}
 0 =& \nabla_w l_T(w_T) = \nabla_w l_T(w_0) + \nabla_w^2 l_T(w') \, (w_T - w_0), 
 \label{eq:MVT_MLE_params} \\
 w' =& c w_T + (1 - c) w_0. 
\end{align}
Substituting $\nabla_w l_T(w_0) = 2 (J / T)^{1/2} \xi_T$ and $\nabla_w^2 l_T(w') = J + o_p(1)$ into Eq.~(\ref{eq:MVT_MLE_params}), we obtain the asymptotic form of $w_T$, 
\begin{align}
 w_T = w_0 - (T J)^{-1/2} \, \xi_T + o_p\left( \frac{1}{\sqrt{T}}\right).
  \label{eq:MLE_params_convergence}
\end{align}
The third and higher order derivatives of $l_T$ is $0$. 
Hence, applying the mean-value theorem to Eqs.~(\ref{eq:reservoir_norm_train}), we find that 
% there exists $a \in [0, 1]$ which satisfies 
\begin{align}
 l_T(w_0) =& l_T(w_T) + \nabla_w l_T(w_T)^\top (w_0 - w_T) + \frac{1}{2} (w_0 - w_T)^\top \, \nabla_w^2 l_T(w_T) \, (w_0 - w_T). 
 % l_T(w_0) =& l_T(w_T) + \nabla_w l_T(w_T)^\top (w_0 - w_T) + \frac{1}{2} (w_0 - w_T)^\top \, \nabla_w^2 l_T(w^*) \, (w_0 - w_T), 
  \label{eq:MVT_train_error}
 % w^* =& a w_0 + (1 - a) w_T. 
\end{align}
Substituting $\nabla_w l_T(w_T) = 0$, $\nabla_w^2 l_T(w_T) = 2 J$ and Eq.~(\ref{eq:MLE_params_convergence}) into Eq.~(\ref{eq:MVT_train_error}), we obtain 
\begin{align}
 l_T(w_T) = l_T(w_0) - \frac{1}{T} \|\xi_T\|^2 + o_p\left(\frac{1}{T}\right). 
 \label{eq:train_error_asymp}
\end{align}
Similarly, applying the mean-value theorem to Eq.~(\ref{eq:reservoir_norm_test}), we find that 
% there exists $a' \in [0, 1]$ which satisfies 
\begin{align}
 l'_{T'}(w_T) =& l'_{T'}(w_0) + \nabla_w l'_{T'}(w_0)^\top (w_T - w_0) + \frac{1}{2} (w_T - w_0)^\top \, \nabla_w^2 l'_{T'}(w_0) \, (w_T - w_0). 
  \label{eq:MVT_test_error} 
 % w^{**} =& a' w_0 + (1 - a') w_T. 
\end{align}
Substituting $\nabla_w l'_{T'}(w_0) = 2(J / T')^{1/2} \xi'_{T'} + o_p(1/\sqrt{T'})$, $\nabla_w^2 l'_{T'}(w_0) = 2 J$ and Eq.~(\ref{eq:MLE_params_convergence}) into Eq.~(\ref{eq:MVT_test_error}), we obtain 
\begin{align} 
 l'_{T'}(w_T) 
  =& l'_{T'}(w_0) + \frac{1}{T} \|\xi_T\|^2 + \frac{2}{\sqrt{T T'}} \, \xi_T^\top \, \xi'_{T'} + o_p\left( \frac{1}{T} \right) \, . 
  \label{eq:test_error_asymp} 
\end{align}
From the central limit theorem~\cite{heyde1974central}, the denominators of Eqs.~(\ref{eq:IPC_train}) and (\ref{eq:IPC_test}) converge in distribution,  
\begin{align}
 \sqrt{T} \zeta_T 
  =& \sqrt{T} (\mu_T - \mu_0) \xrightarrow{d} {\cal N}(0, V_{\zeta, \infty}).
  \label{eq:mu_converge}
\end{align}
% where $\zeta(U'_t) = \|\hat{y}(U'_t)\|^2 - \mu_0$. 
Substituting Eqs.~(\ref{eq:train_error_asymp}), (\ref{eq:test_error_asymp}) and (\ref{eq:mu_converge}) into Eqs.~(\ref{eq:IPC_train}) and (\ref{eq:IPC_test}), we obtain the asymptotic forms of the training and the test IPC, 
\begin{align} 
 C_T =& 1 - \frac{1}{\mu_0} \left[ l_T(w_0) - \frac{l_T(w_0) \zeta_T}{\mu_0} + \frac{l_T(w_0) \zeta_T^2}{\mu_0^ 2} - \frac{\|\xi_T\|^2}{T}\right] + o_p\left(\frac{1}{T}\right), 
  \label{eq:asymp_train_IPC} \\ 
 C'_{T,T'} =& 1 - \frac{1}{\mu_0} \left[ l'_{T'}(w_0) - \frac{l'_{T'}(w_0) \zeta_{T'}}{\mu_0} + \frac{l'_{T'}(w_0) \zeta_{T'}^2}{\mu_0^2} + \frac{\|\xi_T\|^2}{T} + \frac{2 \xi_T^\top \xi'_{T'}} {\sqrt{T T'}} \right] + o_p\left(\frac{1}{T}\right).
 \label{eq:asymp_test_IPC}
\end{align}
Taking expectations for the training and the test data, the mean and the variance of the training IPC are given by 
\begin{align} 
 \mathbb{E}[C_T] 
  =& 1 - \frac{l(w_0)}{\mu_0} + \frac{1}{T} \left[ \frac{C_{l, \zeta}}{\mu_0^2} - \frac{l(w_0) \, V_{\zeta, \infty}}{\ mu_0^3} + \frac{{\rm Tr}(I_\infty J^{-1})}{\mu_0} \right] + o\left( \frac{1}{T} \right) \, , 
  \label{eq:asymp_train_IPC_mean} \\ 
 \mathbb{V}[C_T] 
  =& \frac{1}{T} \left[ \frac{V_{l, \infty}}{\mu_0^2} + \frac{l(w_0)^2 \, V_{\zeta, \infty}}{\mu_0^4} - \frac{2}{\mu_0^3} \, l(w_0) \, C_{l, \zeta} \right] + o\left( \frac{1}{T} \right) \, .
 \label{eq:asymp_train_IPC_var} 
\end{align}
% Here, $l(w, U'_t) = \|\hat{y}(U'_t) - w x (U'_t)\|^2$. 
Similarly, the mean and the variance of the test IPC are given by
\begin{align} 
 \mathbb{E}[C'_{T, T'}] 
  =& 1 - \frac{l(w_0)}{\mu_0} + \frac{1}{T} \left[ \frac{T}{T'} \, \frac{C_{l, \zeta}} {\mu_0^2} - \frac{T}{T'} \, \frac{l(w_0) \, V_{\zeta, \infty}}{\mu_0^3} - \frac{{\rm Tr}(I_\infty J^{-1})}{\mu_0} \right] + o\left( \frac{1}{T} \right) \, , 
  \label{eq:asymp_test_IPC_mean} \\ 
 \mathbb{V}[C'_{T, T'}] 
  =& \frac{1}{T} \left[ \frac{T}{T'} \, \frac{V_{l, \infty}}{\mu_0^2} + \frac{T}{T'} \, \frac{l(w_0)^2 \, V_{\zeta, \infty}}{\mu_0^4} - \frac{T}{T'} \, \frac{2}{\mu_0^3} \, l(w_0) \, C_{l, \zeta} \right] + o\left( \frac{1}{T} \right) \, .
  \label{eq:asymp_test_IPC_var}
\end{align}
Note that we have assumed $T' = O(T)$. 
From Eqs.~(\ref{eq:asymp_train_IPC_mean}) and (\ref{eq:asymp_test_IPC_mean}), we find that both means approach the true IPC, $C_0 = 1 - l(w_0) / \mu_0$, in $O(1/T)$. 
To estimate the true IPC, we first approximate the expectation values, $\mathbb{E}[C_T]$ and $\mathbb{E}[C'_{T, T'}]$, by the sample mean at each $T$. 
Then we ignore the term $o(1/T)$ in Eqs.~(\ref{eq:asymp_train_IPC_mean}) and (\ref{eq:asymp_test_IPC_mean}) and use the least squares method to estimate the true IPC (see Appendix). 
Our method can be applied to RC systems that satisfy the assumption that $U$ is a stationary ergodic process, $\mathbb{V}[l(w, U')] < \infty$, and $I_\infty(w), J, V_{\zeta, \infty}, C_{l,\zeta}(w)$, and $V_{l, \infty}(w)$ are finite matrices. 
In the next section, we apply this method to several models. 

Finally, we note that fitting the variances by Eqs.~(\ref{eq:asymp_train_IPC_var}) and (\ref{eq:asymp_test_IPC_var}) fails when the true IPC is $0$. 
In this case, the true parameter is $w_0 = 0$, and we find $l(w_0, U') = \|\hat{y}(U')\|^2$. 
Thus, we obtain 
\begin{align}
 {\rm Cov}(l(w_0, U'_0), l(w_0, U'_t)) = {\rm Cov}(l(w_0, U'_0), \|\hat{ y}(U'_t)\|^2)
 = {\rm Cov}(l(w_0, U'_0), \zeta(U'_t)) = {\rm Cov}(\zeta(U'_0) , \zeta(U'_t)).
\end{align}
That is, $V_{l, \infty} = V_{\zeta, \infty} = C_{l, \zeta}$. 
Since the coefficients of the $1/T$ terms in Eqs.~(\ref {eq:asymp_train_IPC_var}) and (\ref{eq:asymp_test_IPC_var}) vanish, fitting the coefficients of the $1/T$ terms fails. 
In the next section, we will show an example of the failure. 


\begin{comment}
% An empirical process central limit theorem for dependent non-identically distributed random variables
% https://homepage.ntu.edu.tw/~ckuan/pdf/et01/et_Ch6.pdf
この節では、漸近展開を使用して無限長データに対する IPC を評価する。
まず、文献 \cite{grigoryeva2018echo, gonon2019reservoir} を参照して RC ダイナミクスを整理する。
式(\ref{eq:reservoir_nodes})から、時刻 $t$ における RC の隠れノードの値 $x_t$ は、時刻 $-\tau \, (< t)$ における隠れノードの初期値 $x_{-\tau}$ と、時刻 $t$ までの入力シーケンス $(u_{-\tau}, \ldots, u_t)$ によって決定される。
初期値依存性を除くため、RC は通常 FMP を持つように設計されている。
そのため $\tau$ を十分大きくとれば、$x_t$ は入力列 $(u_{t - s})_{s=0}^\infty$ のみで決定されると考えてよい。
式(\ref{eq:reservoir_nodes})のダイナミクスには時間依存性がないため、入力列を与えれば時刻に依らず隠れノードの値が決まる。
実数列を与えれば隠れノードの値が決まるので、RC の隠れノードを実数列（入力列）から実数ベクトルへの写像と見なすことができる。
表記を簡単にするため、以下では入力列 $(u_{t - s})_{s=0}^\infty$ を $u'_t$ と表し、入力列の集合は ${\cal U} \subseteq (\mathbb{R}^{d_0})^{\mathbb{N}}$ と表す。
また、教師出力関数を $\hat{y} \colon {\cal U} \to \mathbb{R}^{d_2}$ とし、レザバー関数を $x \colon {\cal U} \to \mathbb{R}^{d_1+1}$ と書く。

文献\cite{dambre2012information}を参考にして無限長データについて整理する。
% 確率空間 $(\Omega, {\cal F}, P)$ と確率過程 $U = (U_t)_{t \in \mathbb{Z}}, U_t \colon \Omega \to \mathbb{R}^{d_0}$ を考え、$U$ はエルゴード定常過程であると仮定する。
エルゴード定常過程 $U = (U_t)_{t \in \mathbb{Z}}$ を考える。
また $U'_t := (U_s)_{s \leq t}$ とし、特に $U' = (U_t)_{t \leq 0}$ と表す。
教師出力列 $\{\hat{y}(U'_1), ..., \hat{y}(U'_T), ..., \hat{y}(U'_{T+T'})\}$ を取り、訓練データ $D_t := \{ (U'_t, \hat{y}(U'_t)) \}_{t=1}^T$ とテストデータ $D'_{T'} := \{ (U'_t, \hat{y}(U'_t)) \}_{t=T+1}^{T+T'}$ に分割する。
ただし $T' = O(T)$ とする。
読み取り層は訓練データを使用して最適化される。
\begin{align}
w_T =& \min_w \frac{1}{T} \sum_{t=1}^T \|\hat{y}(U'_t) - w x(U'_t)\|^2. 
 \label{eq:w_MLE}
\end{align}
$w_T$ を用いたときの訓練データに対する IPC を訓練 IPC と呼び、テストデータに対する IPC をテスト IPC と呼ぶ。
\begin{align}
C_T
=& 1 - \frac{\frac{1}{T} \sum_{t=1}^T \|\hat{y}(U'_t) - w_T x(U'_t)\|^2}{\frac{1}{T} \sum_{t=1}^T \|\hat{y}(U'_t)\|^2}
= 1 - \frac{l_T(w_T)}{\mu_T},
\label{eq:IPC_train} \\
C'_{T, T'}
=& 1 - \frac{\frac{1}{T'} \sum_{t=T+1}^{T+T'} \|\hat{y}(U'_t) - w_T x(U'_t)\|^2}{\frac{1}{T'} \sum_{t=T+1}^{T+T'} \|\hat{y}(U'_t)\|^2}
= 1 - \frac{l'_{T'}(w_T)}{\mu_{T'}}.
\label{eq:IPC_test}
\end{align}
ここで式(\ref{eq:IPC_train}), (\ref{eq:IPC_test})の分子と分母を次のように表した。
\begin{align}
l_T(w) =& \frac{1}{T} \sum_{t=1}^T \|\hat{y}(U'_t) - w x(U'_t)\|^2,
\label{eq:reservoir_norm_train} \\
l'_{T'}(w) =& \frac{1}{T'} \sum_{t=T+1}^{T+T'} \|\hat{y}(U'_t) - w x(U'_t)\|^2,
\label{eq:reservoir_norm_test} \\
\mu_T =& \frac{1}{T} \sum_{t=1}^T \|\hat{y}(U'_t)\|^2.
\label{eq:teacher_norm}
\end{align}

ここから訓練IPCとテストIPCの漸近形を導出する。
この論文では、無限長の訓練データを用いて計算された IPC を真の IPC $C_0$ と呼び、そのときの $w$ を真のパラメータと呼び、$w_0$ と表す。
RC および推定値 $w_T$ は線形回帰モデルとその最適パラメータとみなせるので、
線形回帰モデルの漸近論\cite{stock1987asymptotic, wasserman2006all}?が適用できる。

大数の法則から $T \to \infty$ で $l_T(w)$ は $l(w) = \mathbb{E}[\|\hat{y}(U') - w x(U')\|^2]$ に $\mu_T$ は $\mu_0 = \mathbb{E}[\{\hat{y}(U')\|^2]$ に確率収束するので、以下のように表せる。
\begin{align}
C_0 =& 1 - \frac{\mathbb{E}[\|\hat{y}(U') - w_0 x(U')\|^2]}{\mathbb{E}[\|\hat{y}(U')\|^2]}
= 1 - \frac{l(w_0)}{\mu_0},
\label{eq:true_IPC} \\
w_0 =& \argmin_w \mathbb{E}[\|\hat{y}(U') - w x(U')\|^2]. 
 \label{eq:true_parameter} 
\end{align}
また $w_0 = \mathbb{E}[\hat{y}(U') x(U')^\top] \mathbb{E}[x(U') x(U')^\top]^{-1}$ と書ける。 
% RC の真のパラメータ $w_0$ は一意に決まるので、正則な統計モデルである。
% また式(\ref{eq:w_MLE})の $w_T$ は尤度関数に正規分布を用いた最尤推定値とみなせる。
% 正則統計モデルの文献を参照すると \com{(何を見る？)}、$w_T$ は確率的に真のパラメータ $w_0$ に収束し、

以下の量を定義する。
\begin{align}
f(w, U') 
 =& \frac{1}{2} \|\hat{y}(U') - w x(U')\|^2 - \frac{1}{2} \|\hat{y}(U') - w_0 x(U')\|^2, \\
I_\infty 
 =& \sum_{t=-\infty}^\infty {\rm Cov}(\nabla_w f(w, U'_0) \nabla_w f(w, U'_t)^\top)|_{w = w_0}, 
  \label{def:I_inf} \\
J =& \frac{1}{2} \frac{\partial^2 l(w)}{\partial w \partial w}\biggr|_{w = w_0}. 
\label{def:J}
\end{align}
もし $I_\infty$ が有限なら、
\begin{align}
 \xi_T =& J^{-1/2} \nabla_w \frac{1}{\sqrt{T}} \sum_{t=1}^T (f(w, U'_t) - \mathbb{E}[f(w, U'_t)]) \, , \\
 \xi'_{T'} =& J^{-1/2} \nabla_w \frac{1}{\sqrt{T'}} \sum_{t=T+1}^{T+T'} (f(w, U'_t) - \mathbb{E}[f(w, U'_t)]) \, , 
\end{align}
は以下のように法則収束する。
\begin{align}
\xi_T \xrightarrow{d}& {\cal N}(0, J^{-1/2} I_\infty J^{-1/2}),
\label{eq:xi_converge} \\
\xi'_{T'} \xrightarrow{d}& {\cal N}(0, J^{-1/2} I_\infty J^{-1/2}),
\label{eq:xi'_converge}
\end{align}
すると $w_T$ は以下の漸近形を持つ。
\begin{align}
w_T = w_0 - (T J)^{-1/2} \, \xi_T + o_p\left( \frac{1}{\sqrt{T}}\right). 
\label{eq:MLE_params_convergence}
\end{align}
従って、訓練データとテストデータに対する式(\ref{eq:reservoir_norm_train}), (\ref{eq:reservoir_norm_test})の漸近形は次のように記述できる。
\begin{align}
l_T(w_T) =& l_T(w_0) - \frac{1}{T} \|\xi_T\|^2 + o_p\left( \frac{1}{T} \right) \, ,
\label{eq:train_error_asymp} \\
l'_{T'}(w_T) =& l'_{T'}(w_0) + \frac{1}{T} \|\xi_T\|^2 + \frac{2}{\sqrt{T T'}} \, \xi_T^\top \, \xi'_{T'} + o_p\left( \frac{1}{T} \right) \, .
\label{eq:test_error_asymp}
\end{align}
また
\begin{align}
V_{\zeta, \infty} =& \sum_{t=-\infty}^\infty {\rm Cov}(\zeta(U'_0), \zeta(U'_t)), 
\label{def:V_mu_inf}
\end{align}
が有限とすると、中心極限定理\cite{heyde1974central}から、教師出力は
\begin{align}
\sqrt{T} \zeta_T =& \sqrt{T} (\mu_T - \mu_0) \xrightarrow{d} {\cal N}(0, V_{\zeta, \infty}),
\label{eq:mu_converge} 
\end{align}
と法則収束する。ただし $\zeta(U'_t) = \|\hat{y}(U'_t)\|^2 - \mu_0$. 
これらの式から、訓練 IPC とテスト IPC の漸近形は次のように記述できる。
\begin{align}
C_T =&
1 - \frac{1}{\mu_0} \left[ l_T(w_0) - \frac{l_T(w_0) \zeta_T}{\mu_0} + \frac{l_T(w_0) \zeta_T^2}{\mu_0^2} - \frac{\|\xi_T\|^2}{T}\right] + o_p\left(\frac{1}{T}\right),
\label{eq:asymp_train_IPC} \\
C'_{T,T'} =&
1 - \frac{1}{\mu_0} \left[ l'_{T'}(w_0) - \frac{l'_{T'}(w_0) \zeta_{T'}}{\mu_0} + \frac{l'_{T'}(w_0) \zeta_{T'}^2}{\mu_0^2} + \frac{\|\xi_T\|^2}{T} + \frac{2 \xi_T^\top \xi'_{T'}}{\sqrt{T T'}} \right] + o_p\left(\frac{1}{T}\right),
\label{eq:asymp_test_IPC}
\end{align}
データセット $D_T, D'_{T'}$ に対して期待値をとると、訓練 IPC の平均と分散は次のように与えられる。
\begin{align}
\mathbb{E}[C_T]
=& 1 - \frac{l(w_0)}{\mu_0}
+ \frac{1}{T} \left[ \frac{C_{l, \zeta}}{\mu_0^2} - \frac{l(w_0) \, V_{\zeta, \infty}}{\mu_0^3}
+ \frac{{\rm Tr}(I_\infty J^{-1})}{\mu_0} \right] + o\left( \frac{1}{T} \right) \, ,
\label{eq:asymp_train_IPC_mean} \\
\mathbb{V}[C_T]
=& \frac{1}{T} \left[ \frac{V_{l, \infty}}{\mu_0^2} + \frac{l(w_0)^2 \, V_{\zeta, \infty}}{\mu_0^4} - \frac{2}{\mu_0^3} \, l(w_0) \, C_{l, \zeta} \right] + o\left( \frac{1}{T} \right) \, ,
 \label{eq:asymp_train_IPC_var} \\
 C_{l, \zeta} =& \sum_{t=-\infty}^\infty {\rm Cov}(l(w_0, U'_0) \, , \zeta(U'_t)) \, ,
\label{def:l_zeta_cov} \\
V_{l, \infty} =& \sum_{t=-\infty}^\infty {\rm Cov}(l(w_0, U'_0) \, , l(w_0, U'_t)) \, ,
\label{def:l_auto_cov}
\end{align}
ここで $l(w, U'_t) = \|\hat{y}(U'_t) - w x(U'_t)\|^2$. 
同様にテスト IPC の平均値と分散は次のように与えられる。
\begin{align}
\mathbb{E}[C'_{T, T'}]
=& 1 - \frac{l(w_0)}{\mu_0} + \frac{1}{T} \left[ \frac{T}{T'} \, \frac{C_{l, \zeta}}{\mu_0^2} - \frac{T}{T'} \, \frac{l(w_0) \, V_{\zeta, \infty}}{\mu_0^3}
- \frac{{\rm Tr}(I_\infty J^{-1})}{\mu_0} \right] + o\left( \frac{1}{T} \right) \, ,
\label{eq:asymp_test_IPC_mean} \\
\mathbb{V}[C'_{T, T'}]
=& \frac{1}{T} \left[ \frac{T}{T'} \, \frac{V_{l, \infty}}{\mu_0^2} + \frac{T}{T'} \, \frac{l(w_0)^2 \, V_{\zeta, \infty}}{\mu_0^4} - \frac{T}{T'} \, \frac{2}{\mu_0^3} \, l(w_0) \, C_{l, \zeta} \right] + o\left( \frac{1}{T} \right) \, .
\label{eq:asymp_test_IPC_var}
\end{align}
式 (\ref{eq:asymp_train_IPC_mean}) および (\ref{eq:asymp_test_IPC_mean}) から、両方の平均値が $O(1/T)$ で真の IPC $C_0 = 1 - l(w_0) / \mu_0$ に近づくことが分かる。
期待値をデータセットのサンプル平均で近似すれば、$o(1/T)$ の項を無視できる $T$ のデータに最小二乗法などのフィッティングを使用することで真の IPC を推定できる (付録を参照)。
次節では数値シミュレーションを用いて推定法の確認を行う。

最後に、真の IPC が $0$、つまり $l(w_0) = \mu_0$ のときは、式 (\ref{eq:asymp_train_IPC_var}) と (\ref{eq:asymp_test_IPC_var}) で与えられる分散のフィッティングが失敗することを説明する。
真のパラメータは一意なので $w_0 = 0$ であり、$l(w_0, U') = \|\hat{y}(U')\|^2$ となるので、次式が得られる。
\begin{align}
{\rm Cov}(l(w_0, U'_0), l(w_0, U'_t)) = {\rm Cov}(l(w_0, U'_0), \|\hat{y}(U'_t)\|^2)
= {\rm Cov}(l(w_0, U'_0), \zeta(U'_t)) = {\rm Cov}(\zeta(U'_0), \zeta(U'_t)).
\end{align}
つまり $V_{l, \infty} = V_{\zeta, \infty} = C_{l, \zeta}$ となるので、式(\ref{eq:asymp_train_IPC_var})と式(\ref{eq:asymp_test_IPC_var})の$1/T$の項は$0$であり、IPC の分散は$O(1/T)$よりも速く減衰することが分かる。
\end{comment}

\begin{comment}
In this section, we use an asymptotic expansion to evaluate the IPC for infinitely long data. 
First, we summarize RC dynamics with reference to Ortega. 
The value of hidden nodes in RC at time $t$, $x_t$, is determined by the value of hidden nodes at $-\tau$, which are sufficiently past from $t$, and the input sequence $(u_{-\tau}, \ldots, u_t)$ up to time $t$. 
Due to the FMP, $x_t$ can be considered to be determined from the input sequence $(u_{t - s})_{s=1}^\infty$. (There may be a better setting. See Ortega.)
Considering in the same way in each time, we can regard the hidden node in RC as a mapping from a real sequence to a real vector. 
For simplicity, below we will represent the input sequence $(u_{t - s})_{s=1}^\infty$ as $u'_t$, and the set of input sequences will be ${\cal U} \subseteq (\mathbb{R}^{d_0})^{\mathbb{N}}$. 
We will also define the target output function as $\hat{y} \colon {\cal U} \to \mathbb{R}^{d_2}$, and the hidden node function as $x \colon {\cal U} \to \mathbb{R}^{d_1+1}$. 

We will organize the handling of infinitely long data with reference to Dambre. 
Consider the probability space $(\Omega, {\cal F}, P)$ and the stochastic process $U = (U_t)_{t \in \mathbb{Z}}, U_t \colon \Omega \to \mathbb{R}^{d_0}$, and assume that it is an ergodic stationary process. 
Let $U'_t = (U_s)_{s \leq t}$, and in particular denote $U' = (U_t)_{t \leq 0}$. 
Take the target output sequence $\{\hat{y}(U'_1), ..., \hat{y}(U'_T), ..., \hat{y}(U'_{T+T'})\}$. 
We divide it into training data, $D_t := \{ (U'_t, \hat{y}(U'_t)) \}_{t=1}^T$, and test data, $D'_{T'} := \{ (U'_t, \hat{y}(U'_t)) \}_{t=T+1}^{T+T'}$. 
We assume that $T' / T$ does not diverge as $T \to \infty$, that is, $T' = O(T)$. 
The optimization of the read-out layer is performed using the training data. 
The IPC calculated for the training data will be called the training IPC, and the IPC calculated for the test data will be called the test IPC, 
\begin{align}
 w_T =& \min_w \frac{1}{T} \sum_{t=1}^T \|\hat{y}(U'_t) - w x(U'_t)\|^2 \, , \\
 C(D_T) 
  =& 1 - \frac{\frac{1}{T} \sum_{t=1}^T \|\hat{y}(U'_t) - w_T x(U'_t)\|^2}{\frac{1}{T} \sum_{t=1}^T \|\hat{y}(U'_t)\|^2}
  = 1 - \frac{l(w_T; D_T)}{\mu(D_T)},
 \label{eq:IPC_train} \\
 C'(D_T, D'_{T'}) 
  =& 1 - \frac{\frac{1}{T'} \sum_{t=T+1}^{T+T'} \|\hat{y}(U'_t) - w_T x(U'_t)\|^2}{\frac{1}{T'} \sum_{t=T+1}^{T+T'} \|\hat{y}(U'_t)\|^2}
  = 1 - \frac{l(w_T; D'_{T'})}{\mu(D'_{T'})}.
 \label{eq:IPC_test}  
\end{align}
We denote the numerator and denominator as 
\begin{align}
 l(w; D_T) =& \frac{1}{T} \sum_{t=1}^T \|\hat{y}(U'_t) - w x(U'_t)\|^2, 
  \label{eq:reservoir_norm} \\
 \mu(D_T) =& \frac{1}{T} \sum_{t=1}^T \|\hat{y}(U'_t)\|^2.
  \label{eq:teacher_norm}
\end{align}
In this paper, the IPC calculated using infinitely long training data is called the true IPC, and $w$ at that time is called the true parameter and written as $w_0$, 
\begin{align}
 C_0 =& 1 - \frac{\mathbb{E}[\|\hat{y}(U') - w_0 x(U')\|^2]}{\mathbb{E}[\|\hat{y}(U')\|^2]}
  = 1 - \frac{l_0}{\mu_0}, 
  \label{eq:true_IPC} \\
 w_0 =& \argmin_w \mathbb{E}[\|\hat{y}(U') - w x(U')\|^2], 
  \label{eq:true_parameter} \\
 \mu_0 =& \mathbb{E}[\mu(U'_0)].
  \label{def:mu_0} 
\end{align}

Referring to some literature, the maximum likelihood estimate $w_T$ converges to the true parameter in probability as 
\begin{align}
 w_T = w_0 - (T J)^{-1/2} \, \xi_T + o_p\left( \frac{1}{\sqrt{T}}\right), 
 \label{eq:MLE_params_convergence}
\end{align}
and the asymptotic forms of the training error and test error can be written as 
\begin{align}
 l(w_T; D_T) =& l(w_0; D_T) - \frac{1}{T} \|\xi_T\|^2 + o_p\left( \frac{1}{T} \right) \, , 
 \label{eq:train_error_asymp} \\
 l(w_T; D'_{T'}) =& l(w_0; D'_{T'}) + \frac{1}{T} \|\xi_T\|^2 + \frac{2}{\sqrt{T T'}} \, \xi_T^\top \, \xi'_{T'} + o_p\left( \frac{1}{T} \right) \, . 
 \label{eq:test_error_asymp} 
\end{align}
Here, $\xi_T, \xi'_{T'}$ are random variables which satisfy 
\begin{align}
 \xi_T \xrightarrow{d}& {\cal N}(0, J^{-1/2} I_\infty J^{-1/2}), 
  \label{eq:xi_converge} \\
 \xi'_{T'} \xrightarrow{d}& {\cal N}(0, J^{-1/2} I_\infty J^{-1/2}), 
  \label{eq:xi'_converge}
\end{align}
where $I_\infty$ and $J$ are define by 
\begin{align}
 I_\infty =& \sum_{t=-\infty}^\infty I_t, \\
  \label{def:I_inf}
 I_t =& {\rm Cov}(\nabla_w f(w; U'_0) \nabla_w f(w; U'_t)^\top)|_{w = w_0}, \\
 f(w; U') =& \frac{1}{2} \|\hat{y}(U') - w x(U')\|^2 - \frac{1}{2} \|\hat{y}(U') - w_0 x(U')\|^2, \\
 J_{ij} =& \frac{1}{2} \frac{\partial^2 l(w; D_T)}{\partial w_i \partial w_j}\biggr|_{w = w_0}. 
  \label{def:J}
\end{align}
From the central limit theorem, the target output converges to 
\begin{align}
 \sqrt{T} \zeta(D_T) =& \sqrt{T} (\mu(D_T) - \mu_0) \xrightarrow{d} {\cal N}(0, V_{\zeta, \infty}), 
  \label{eq:mu_converge} \\
 V_{\zeta, \infty} =& \sum_{t=-\infty}^\infty {\rm Cov}(\zeta(U'_0), \zeta(U'_t)). 
  \label{def:V_mu_inf}
\end{align}
Using the above equations, the training IPC and test IPC can be written as 
\begin{align}
 C(D_T) =& 
 1 - \frac{1}{\mu_0} \left[ l(w_0; D_T) - \frac{l(w_0; D_T) \zeta(D_T)}{\mu_0} + \frac{l(w_0; D_T) \zeta(D_T)^2}{\mu_0^2} - \frac{\|\xi_T\|^2}{T}\right] + o_p\left(\frac{1}{T}\right), 
 \label{eq:asymp_train_IPC} \\
 C'(D_T, D'_{T'}) =& 
 1 - \frac{1}{\mu_0} \left[ l(w_0; D'_{T'}) - \frac{l(w_0; D'_{T'}) \zeta(D'_{T'})}{\mu_0} + \frac{l(w_0; D'_{T'}) \zeta(D'_{T'})^2}{\mu_0^2} + \frac{1}{T} \|\xi_T\|^2 + \frac{2}{\sqrt{T T'}} \xi_T^\top \xi'_{T'} \right] + o_p\left(\frac{1}{T}\right), 
 \label{eq:asymp_test_IPC} 
\end{align}
The mean and variance of the train IPC are given by 
\begin{align}
 \mathbb{E}[C(D_T)]
 =& 1 - \frac{l_0}{\mu_0} 
 + \frac{1}{T} \left[ \frac{C_{l, \zeta}}{\mu_0^2} - \frac{l_0 \, V_{\zeta, \infty}}{\mu_0^3} 
 + \frac{{\rm Tr}(I_\infty J^{-1})}{\mu_0} \right] + o\left( \frac{1}{T} \right) \, , 
  \label{eq:asymp_train_IPC_mean} \\
 \mathbb{V}[C(D_T)] 
 =& \frac{1}{T} \left[ \frac{V_{l, \infty}}{\mu_0^2} + \frac{l_0^2 \, V_{\zeta, \infty}}{\mu_0^4} - \frac{2}{\mu_0^3} \,  l_0 \, C_{l, \zeta} \right] + o\left( \frac{1}{T} \right) \, , 
  \label{eq:asymp_train_IPC_var} \\
 C_{l, \zeta} =& \sum_{t=-\infty}^\infty {\rm Cov}(l(w_0; U'_0) \, , \zeta(U'_t)) \, , 
  \label{def:l_zeta_cov} \\
 V_{l, \infty} =& \sum_{t=-\infty}^\infty {\rm Cov}(l(w_0; U'_0) \, , l(w_0; U'_t)) \, , 
  \label{def:l_auto_cov}
\end{align}
and those of the test IPC are given by 
\begin{align}
 \mathbb{E}[C'(D_T, D'_{T'})] 
 =& 1 - \frac{l_0}{\mu_0} + \frac{1}{T} \left[ \frac{T}{T'} \, \frac{C_{l, \zeta}}{\mu_0^2} - \frac{T}{T'} \, \frac{l_0 \, V_{\zeta, \infty}}{\mu_0^3} 
 - \frac{{\rm Tr}(I_\infty J^{-1})}{\mu_0} \right] + o\left( \frac{1}{T} \right) \, , 
  \label{eq:asymp_test_IPC_mean} \\
 \mathbb{V}[C'(D_T, D'_{T'})] 
 =& \frac{1}{T} \left[ \frac{T}{T'} \, \frac{V_{l, \infty}}{\mu_0^2} + \frac{T}{T'} \, \frac{l_0^2 \, V_{\zeta, \infty}}{\mu_0^4} - \frac{T}{T'} \, \frac{2}{\mu_0^3} \,  l_0 \, C_{l, \zeta} \right] + o\left( \frac{1}{T} \right) \, . 
  \label{eq:asymp_test_IPC_var}
\end{align}
From Eqs.~(\ref{eq:asymp_test_IPC_mean}) and (\ref{eq:asymp_test_IPC_mean}), we find that both mean values approach the true IPC at $O(1/T)$. 
If we approximate the expectation values with the sample mean, we can estimate the true IPC using fitting such as the least squares method (see Appendix). 
The fitting of Eqs.~(\ref{eq:asymp_train_IPC_var}) and (\ref{eq:asymp_test_IPC_var}) may fail in special cases. 
For instance, suppose the true IPC is $0$, that is, $l_0 = \mu_0$. 
Since the projection is unique, we find that $w_0 = 0$ and $l(w_0; U') = \|\hat{y}(U')\|^2$ and obtain 
\begin{align}
 {\rm Cov}(l(w_0; U'_0), l(w_0; U'_t)) = {\rm Cov}(l(w_0; U'_0), \|\hat{y}(U'_t)\|^2)
 = {\rm Cov}(l(w_0; U'_0), \zeta(U'_t)) = {\rm Cov}(\zeta(U'_0), \zeta(U'_t)), 
\end{align}
which means $V_{l, \infty} = V_{\zeta, \infty} = C_{l, \zeta}$. 
Then, we find that the $O(1/T)$ terms of Eqs.~(\ref{eq:asymp_train_IPC_var}) and (\ref{eq:asymp_test_IPC_var}) are $0$, and asymptotic variance decays faster than $O(1/T)$. 
\end{comment}


\section{Numerical simulation}
\begin{comment}
 ・小さいが厳密に IPC や漸近形の各項を計算できる系で試す。既存手法との比較？
 ・現実的なサイズの RC でルジャンドル多項式の IPC を計算。
 ・NARMA10 で確認。
 ・グラフは平均値に分散のエラーバーを付けたものと、分散自体のグラフも欲しい。
\end{comment}
First, we show the effectiveness of our method in a system where each term of the asymptotic forms except for $o(1/T)$ can be precisely calculated. 
Next, we estimate the true IPCs in systems where the target outputs are given by Legendre polynomials. 
Finally, we apply our method to the estimation of the true IPC for the NARMA10 task.

\begin{comment}
まず厳密に IPC の漸近形の各項を計算できる系で推定法の有効性を確認する。
次にルジャンドル多項式による教師出力で IPC の推定を行い、最後にNARMA10タスクのIPC推定に適用する。
\end{comment}

\begin{comment}
まず厳密に計算できる系で、導出した式を確認する。
隠れノード1つのレザバーで入出力関数は1時刻遅れ入力を用いる。
各 T で n 回試行を行い、平均値と分散を実数と両対数でプロットした。
（平均値・分散のグラフともに、分散のエラーバーを付けておく。また厳密式との比較なので、T, n ともに大きめに取っておく。）
大きい T でほぼ理論通りに 1 / T で下がっているのが分かる。
また訓練 IPC, テスト IPC ともに真の IPC へ 1 / T で漸近する様子もわかる。

次に N = 100 のレザバーでいくつかのルジャンドル多項式の IPC を評価した。
（完全系という指標なので、厳密系と同程度の T, n でよい。）
既存手法と比べて…という点が優れている？
またレザバーのベンチマークとしてよく使われている NARMA10 で IPC の推定も行った。
（おもちゃベンチマークなので、T, n はそこそこに抑えておく。）
直交多項式以外でも機能していることが分かる。
\end{comment}

 \subsection{A simple model}
\begin{comment}
・小さいが厳密に IPC や漸近形の各項を計算できる系で試す。
・多分ほぼ一致する。
\end{comment}
In this model, $U_t$ is a random variable from ${\rm Uniform}(-1, 1)$. 
The hidden node of the RC system and the target output are given by 
\begin{align}
 x(U') = \sum_{s=0}^\infty 2^{-s} \, U_{-s}, \ \
 \hat{y}(U') = 1 + x(U').
\end{align}
The mean squared error is 
\begin{align}
 l(w) = \mathbb{E}[( \hat{y}(U') - w \, x(U'))^2] = \frac{4}{9} \, (w-1)^2 + 1 . 
\end{align}
Thus, the true parameter is $w_0 = 1$, and the minimum value of the mean squared error is $l(w_0) = 1$. 
The true ICP is 
\begin{align}
 C_0 = 1 - \frac{l(w_0)}{\mu_0} = \frac{4}{13}. 
\end{align}
The other terms in the asymptotic expansions are 
\begin{align}
C_{l, \zeta} = 0, \ \ V_{l, \infty} = 0, \ \ V_{\zeta, \infty} = \frac{6992}{1215}, \ \
I_\infty = \frac{4}{3}, \ \ J = \frac{4}{9}. 
\end{align}

We performed a numerical simulation. 
Training and test data were generated $10000$ times for each data length $T$, and the training and the test IPC were calculated. 
Their means and variances were modeled as 
\begin{align}
 C(T) = a + \frac{b}{T} , \ \ C'(T, T') = a + \frac{c}{T'} , \ \
 V(T) = \frac{d}{T} , \ \ V'(T, T') = \frac{d}{T'},
\end{align}
and $a, b, c, d$ were estimated using the least squares method. 
The exact and estimated values of the asymptotic expansion are shown in Table \ref{table:result_simple_model}.
\begin{table}[h]
 \centering
 \caption{The ground truth and the estimated values of the coefficients of the asymptotic forms are shown. We can find that the estimated values matched well to the ground truth.}
 \label{table:result_simple_model}
 \begin{tabular}{|c|c|c|c|c|}
  \hline
    & a & b & c & d \\ \hline
   ground truth & 4/13 & 1839/10985 & 66606/10985 & 188784/142805 \\ \hline
   estimation & 0.3076476776827442 & 0.1685955855549023 & 5.9262712596285585 & 1.3418518658017338 \\ \hline
 \end{tabular}
\end{table}
We find that the least-squares estimation is successful. 
We plot the means and variances of the IPC in Fig.~\ref{fig:small_sys_IPC} and find that they are almost on the asymptote up to the term $1/T$. 
Next, we plotted on the log-log scale to verify the dominant $T$-dependence. 
To remove the constant term, we subtracted $C_0$ from the means of the IPCs. 
(After subtracting $C_0$, the 18th item in the mean of the training IPC was negative, and we removed it from the log-log scale.) 
Most of the samples are on the asymptote up to the term $1/T$. 
Although the means of the training IPC fluctuate more, this is likely due to a lack of samples. 


\begin{comment}
$U'$ を ${\rm Uniform}(-1, 1)$ から独立に生成したサンプル列とし、RC の隠れ層と教師出力をそれぞれ 
\begin{align}
 x(u') = \sum_{s=0}^\infty 2^{-s} \, u_{-s}, \ \ 
 \hat{y}(u') = 1 + x(u'), 
\end{align}
とする。
% これは隠れノード1つの RC で $v_1 = 1/2, v_2 = 1, c = 0$ とすることに対応する。
% このとき以下の式が成り立つ。
% \begin{align}
% & \mathbb{E}[x(U')] = 0 \, , \ \
% \mathbb{E}[x(U')^2] = \frac{4}{9} \, , \ \
% \mathbb{E}[x(U')^4] = \frac{304}{675} \, , \\
% & \mathbb{E}[x(U'_0) \, x(U'_t)]
% = \frac{4}{9} \cdot 2^{-|t|} \, , \\
% & \mathbb{E}[x(U'_0) \, x(U'_t)^2] = \mathbb{E}[x(U'_0)^2 \, x(U'_t)] = 0 \, , \\
% & \mathbb{E}[x(U'_0)^2 \, x(U'_t)^2] = \frac{16}{81} + 4^{-|t|} \cdot \frac{512}{2025} \, , \\
% & \mathbb{E}[x(U') \, \hat{y}(U')] = \frac{4}{9} \, , \ \
% \mu_0 = \mathbb{E}[\hat{y}(U')^2] = \frac{13}{9} \, .
% \end{align}
% すると平均二乗誤差は
このとき平均二乗誤差は
\begin{align}
l(w) = \mathbb{E}[( \hat{y}(U') - w \, x(U'))^2] = \frac{4}{9} \, (w-1)^2 + 1 \, ,
\end{align}
で与えられるため、真のパラメータは $w_0 = 1$ であり、平均二乗誤差の最小値は $l(w_0) = 1$ となる。
したがって、真の ICP は
\begin{align}
C_0 = 1 - \frac{l(w_0)}{\mu_0} = \frac{4}{13}
\end{align}
となる。
漸近展開の他の項は
% \begin{align}
% C_{l, \zeta} =& \sum_{t=-\infty}^\infty {\rm Cov}(l(w_0; U'_0), \zeta(U'_t)) = 0, \\
% V_{l, \infty} =& \sum_{t=-\infty}^\infty {\rm Cov}(l(w_0; U'_0), l(w_0; U'_t)) = 0, \\
% V_{\zeta, \infty} =& \sum_{t=-\infty}^\infty {\rm Cov}(\zeta(U'_0), \zeta(U'_t))
% = \frac{6992}{1215}, \\
% I_\infty =& \sum_{t=-\infty}^\infty \mathbb{E}[x(U'_0) x(U'_t)] = \frac{4}{3}, \\
% J =& \frac{1}{2} \frac{\partial^2 l}{\partial w^2}(w_0) = \mathbb{E}[x(U')^2] = \frac{4}{9}.
% \end{align}
\begin{align}
C_{l, \zeta} = 0, \ \ V_{l, \infty} = 0, \ \ V_{\zeta, \infty} = \frac{6992}{1215}, \ \ 
I_\infty = \frac{4}{3}, \ \ J = \frac{4}{9}, 
\end{align}
となる。
次にシミュレーションを行う。
% $T = 100, 200, \ldots, 1000, 2000, \ldots, 10000$, $T' = 2 T$ とした。
各 $T$ で $10000$ 回データを生成し、訓練 IPC とテスト IPC を計算した。
それらの平均値と分散を
\begin{align}
 C(T) = a + \frac{b}{T} , \ \ C'(T, T') = a + \frac{c}{T'} , \ \
 V(T) = \frac{d}{T} , \ \ V'(T, T') = \frac{d}{T'}, 
\end{align}
とおき、最小二乗法で $a, b, c, d$ の推定を行った。
漸近展開の理論値と推定値は表\ref{table:result_simple_model}のようになった。
\begin{table}[h]
 \centering
 \caption{ground truth and estimations}
 \label{table:result_simple_model}
 \begin{tabular}{|c|c|c|c|c|}
  \hline
    & a & b & c & d \\ \hline
   ground truth & 4/13 & 1839/10985 & 66606/10985 & 188784/142805 \\ \hline
   estimation & 0.3076476776827442 & 0.1685955855549023 & 5.9262712596285585 & 1.3418518658017338 \\ \hline
 \end{tabular}
\end{table}
最小二乗法による推定はうまくいっているといえる。
% 理論値は $a = 4/13, b = 1839/10985, c = 66606/10985, d = 188784/142805$, 
% 推定値は $a = 0.3076476776827442, b = 0.1685955855549023, c = 5.9262712596285585, d = 1.3418518658017338$ となり、推定は成功した。
IPC の平均と分散をプロットすると（図\ref{fig:small_sys_IPC}）、ほぼ $1/T$ の項までの漸近線の上に乗っていることが分かる。
両対数プロットすると訓練 IPC の平均値はばらつきが多くなったが、サンプル数の不足によるものと考えられる。
（訓練 IPC の平均値では、18個目の訓練データで $\log$ の引数が負になったので、両対数グラフから除いた。）
\end{comment}

\begin{figure}[htbp]
  \begin{tabular}{cc}
    \begin{minipage}[t]{0.4\hsize}
      \centering
      \includegraphics[keepaspectratio, scale=0.4]{asymp_paper/small_sys_from_exact_terms/IPC_means.eps}
      \subcaption{Mean of IPCs in the simple model}
      \label{fig:small_sys_IPC_mean}
    \end{minipage} & 
    \begin{minipage}[t]{0.4\hsize}
      \centering
      \includegraphics[keepaspectratio, scale=0.4]{asymp_paper/small_sys_from_exact_terms/IPC_means_log.eps}
      \subcaption{Mean of IPCs after removal of the constant term on a log-log scale in the simple model}
      \label{fig:small_sys_IPC_mean_log}
    \end{minipage} \\
    \begin{minipage}[t]{0.4\hsize}
      \centering
      \includegraphics[keepaspectratio, scale=0.4]{asymp_paper/small_sys_from_exact_terms/IPC_vars.eps}
      \subcaption{Variance of IPCs in the simple model}
      \label{fig:small_sys_IPC_var}
    \end{minipage} &
    \begin{minipage}[t]{0.4\hsize}
      \centering
      \includegraphics[keepaspectratio, scale=0.4]{asymp_paper/small_sys_from_exact_terms/IPC_vars_log.eps}
      \subcaption{Variance of IPCs on a log-log scale in the simple model}
      \label{fig:small_sys_IPC_var_log}
    \end{minipage} \\
  \end{tabular}
  \caption{The means and the variances of the training and the test IPC obtained by simulation in the simple system, and the theoretical lines obtained by the asymptotic expansions are plotted against $T$. Although the means of the training IPC fluctuate in the log-log plot, most samples are roughly on the theoretical lines, which indicates the effectiveness of the estimation.}
  % \caption{単純な系においてシミュレーションで得た訓練IPCおよびテストIPCの平均値と分散、および漸近展開で得た理論線を $T$ に対してプロットした。訓練 IPC の平均値のばらつきはやや大きいが、すべてのデータはおおむね理論線の上に乗っており、漸近線の正しさが分かる。}
\label{fig:small_sys_IPC}
\end{figure}




\begin{comment}
Let $U'$ be a sample sequence from ${\rm Uniform}(-1, 1)$, and $x(u') = \sum_{s=0}^\infty 2^{-s} \, u_{-s}\, , \hat{y}(u') = 1 + x(u')$. 
Then, we find that 
\begin{align}
 & \mathbb{E}[x(U')] = 0 \, , \ \ 
 \mathbb{E}[x(U')^2] = \frac{4}{9} \, , \ \ 
 \mathbb{E}[x(U')^4] = \frac{304}{675}  \, , \\
 & \mathbb{E}[x(U'_0) \, x(U'_t)] 
 = \frac{4}{9} \cdot 2^{-|t|} \, , \\
 & \mathbb{E}[x(U'_0) \, x(U'_t)^2] = \mathbb{E}[x(U'_0)^2 \, x(U'_t)] = 0 \, , \\
 & \mathbb{E}[x(U'_0)^2 \, x(U'_t)^2] = \frac{16}{81} + 4^{-|t|} \cdot \frac{512}{2025} \, , \\
 & \mathbb{E}[x(U') \, \hat{y}(U')] = \frac{4}{9} \, , \ \ 
 \mu_0 = \mathbb{E}[\hat{y}(U')^2] = \frac{13}{9} \, . 
\end{align}
Since the mean squared error is given by 
\begin{align}
 l(w) = \mathbb{E}[( \hat{y}(U') - w \, x(U'))^2] = \frac{4}{9} \, (w-1)^2 + 1 \, , 
\end{align}
the true parameter is $w_0 = 1$ and the minimum of the mean squared error is $l_0 = 1$. 
Thus, the true ICP is 
\begin{align}
 C_0 = 1 - \frac{l_0}{\mu_0} = \frac{4}{13}. 
\end{align}
Other components of the asymptotic expansions are 
\begin{align}
 C_{l, \zeta} =& \sum_{t=-\infty}^\infty {\rm Cov}(l(w_0; U'_0), \zeta(U'_t)) = 0, \\
 V_{l, \infty} =& \sum_{t=-\infty}^\infty {\rm Cov}(l(w_0; U'_0), l(w_0; U'_t)) = 0, \\
 V_{\zeta, \infty} =& \sum_{t=-\infty}^\infty {\rm Cov}(\zeta(U'_0), \zeta(U'_t))
 = \sum_{t=-\infty}^\infty \left[ \mathbb{E}(\zeta(U'_0) \, \zeta(U'_t)) - \mathbb{E}[\zeta(U'_0)]^2 \right] = \frac{6992}{1215}, \\
 I_\infty =& \sum_{t=-\infty}^\infty I_t = \sum_{t=-\infty}^\infty \mathbb{E}[x(U'_0) x(U'_t)] = \frac{4}{3}, \\
 J =& \frac{1}{2} \frac{\partial^2 l}{\partial w^2}(w_0) = \mathbb{E}[x(U')^2] = \frac{4}{9}. 
\end{align}
\end{comment}


\begin{comment}
A sample approximation of the mean squared error is 
\begin{align}
 l(w; D_T) =& \frac{1}{T} \sum_{t=1}^T (\hat{y}(U'_t) - w x(U'_t))^2
  = \frac{1}{T} \sum_{t=1}^T [(w-1) \, x(U'_t) - 1]^2, 
\end{align}
and we find 
\begin{align}
 w_T =& \argmin_w l(w; D_T) = 1+ \frac{\left<x\right>_{D_T}}{\left<x^2\right>_{D_T}} \, , \\
 l(w_T; D_T) 
  =& 1 - \frac{\left<x\right>_{D_T}^2}{\left<x^2\right>_{D_T}} 
  = l_0(w) - \frac{\left<x\right>_{D_T}^2}{\left<x^2\right>_{D_T}} 
  = l(w_0; D_T) - \frac{\left<x\right>_{D_T}^2}{\left<x^2\right>_{D_T}} \, , 
  \label{eq:exact_train_error} \\
 C(D_T) =& 1 - \frac{l(w_T; D_T)}{\left<\hat{y}^2\right>_{D_T}} \, . 
  \label{eq:exact_train_IPC}
\end{align}
where $<\cdot>_{D_T}$ is the time average such as 
\begin{align}
 \left<x\right>_{D_T} := \frac{1}{T} \sum_{t=1}^T x(U'_t) \, . 
\end{align}
We take a sample mean and variance of Eq.~(\ref{eq:exact_train_error}). 

\begin{align}
 V_\infty =& \mathbb{V}[X(U'_1)] + 2 \sum_{t=1}^\infty {\rm Cov}(X(U'_1), X(U'_{t+1})) 
 = \mathbb{V}[X(U'_1)] + 2 \sum_{t=1}^\infty \mathbb{E}[(X(U'_1) \, X(U'_{t+1})] \nonumber \\
 =& \mathbb{V}[X(U'_1)] + 2 \sum_{t=1}^\infty \mathbb{E}[(X(U'_1) \, 2^{-t} \, X(U'_1)]
 = \mathbb{V}[X(U'_1)] + \mathbb{E}[(X(U'_1)^2] \, \sum_{t=1}^\infty 2^{1-t} \nonumber \\
 =& 3 \mathbb{V}[X(U'_1)] = \frac{4}{3} \, . 
\end{align}

The sample mean and variance of Eq.~(\ref{eq:exact_train_error}) is 
\begin{align}
 E_{\rm M} =& \mathbb{E}[L_n(\hat w)] = L(w_0) - \frac{1}{2} \frac{\frac{1}{n} V_\infty}{\mathbb{E}[X(U'_1)^2]} + o\left(\frac{1}{n}\right) \nonumber \\
 =& L(w_0) - \frac{1}{2n} \frac{3 \mathbb{V}[X(U'_1)]}{\mathbb{V}[X(U'_1)]} + o\left(\frac{1}{n}\right)
 = L(w_0) - \frac{3}{2n} + o\left(\frac{1}{n}\right) \, , \\
 V_{\rm M} =& \mathbb{V}[L_n(\hat w)] 
 = \mathbb{V}\left[ L(w_0) - \frac{1}{2} \frac{\left< X(U') \right>_n^2}{\left< (X(U'))^2 \right>_n} \right] 
 = \frac{1}{4} \mathbb{V}\left[ \frac{\left< X(U') \right>_n^2}{\left< (X(U'))^2 \right>_n} \right] \nonumber \\
 =& \frac{1}{4 \, \mathbb{E}[X(U')^2]^2} \, \mathbb{V}\left[ \frac{1}{n^2} \left(\sum_{i=1}^n X(U'_i)\right)^2 \right] + o\left(\frac{1}{n^3}\right) 
 = \frac{1}{4 n^4 \, \mathbb{E}[X(U')^2]^2} \, \mathbb{V}\left[ \left(\sum_{i=1}^n X(U'_i)\right)^2 \right] + o\left(\frac{1}{n^3}\right) \nonumber \\ . 
 =& \frac{1}{4 n^4 \, \mathbb{E}[X(U')^2]^2} \, \left[ n \, \mathbb{V}[ X(U'_1) ] + 2 (n-1) \, {\rm Cov}(X(U'_1), X(U'_2)) 
 + \cdots + 2 {\rm Cov}(X(U'_1), X(U'_n)) \right] + o\left(\frac{1}{n^3}\right) \nonumber \\
 =& \frac{V_\infty}{4 n^3 \, \mathbb{E}[X(U')^2]^2} + o\left(\frac{1}{n^3}\right)
 = \frac{27}{16 \, n^3} + o\left(\frac{1}{n^3}\right) \, . 
\end{align}
From $I_t = \mathbb{E}[X(U'_1) \, X(U'_{t+1})] = \mathbb{E}[X(U'_1) \, 2^{-t} \, X(U'_1)] = 2^{-t} \, I$, the asymptotic form can be written as 
\begin{align}
 E_{\rm M, asym} =& L(w_0) - \frac{3}{2n} + o\left(\frac{1}{n}\right) \, , \\
 V_{\rm iid, asym} =& 0 + o\left(\frac{1}{n}\right) = o\left(\frac{1}{n}\right) \, , 
\end{align}
and we find that these are consistent to the exact form up to $O(1/n)$. 


In case that $U'_1, \ldots, U'_n$ are i.i.d. samples, from the central limit theorem, $\sqrt{n} \, \left<X(U')\right>_n$ converges in law to ${\cal N}(0, \mathbb{V}[X(U')])$ as $n \to \infty$. 
Therefore, we obtain 
\begin{align}
 E_{\rm iid} =& \mathbb{E}[L_n(\hat w)] = L(w_0) - \frac{1}{2n} \frac{\mathbb{V}[X(U')]}{\mathbb{E}[X(U')^2]} + o\left(\frac{1}{n}\right) \nonumber \\
 =& L(w_0) - \frac{1}{2n} \frac{\mathbb{E}[X(U')^2]}{\mathbb{E}[X(U')^2]} + o\left(\frac{1}{n}\right)
 = L(w_0) - \frac{1}{2n} + o\left(\frac{1}{n}\right) \, , \\
 V_{\rm iid} =& \mathbb{V}[L_n(\hat w)] 
 = \mathbb{V}\left[ L(w_0) - \frac{1}{2} \frac{\left< X(U') \right>_n^2}{\left< (X(U'))^2 \right>_n} \right] 
 = \frac{1}{4} \mathbb{V}\left[ \frac{\left< X(U') \right>_n^2}{\left< (X(U'))^2 \right>_n} \right] \nonumber \\
 =& \frac{1}{4} \mathbb{V}\left[ \frac{\frac{1}{n^2} \sum_{i=1}^n X(U'_i)^2 + \frac{1}{n^2} \sum_{j \neq k} X(U'_j) \, X(U'_k)}{ \mathbb{E}[X(U')^2] \, \left\{ 1 + \frac{\left<X(U')^2\right>_n - \mathbb{E}[X(U')^2]}{\mathbb{E}[X(U')^2]} \right\} } \right] 
 = \frac{1}{4 \mathbb{E}[X(U')^2]^2} \, \mathbb{V}\left[ \frac{1}{n^2} \sum_{i=1}^n X(U'_i)^2 \right] + o\left(\frac{1}{n^3}\right) \nonumber \\
 =& \frac{1}{4n^3} \, \frac{\mathbb{V}[X(U')^2]}{\mathbb{E}[X(U')^2]^2} + o\left(\frac{1}{n^3}\right)
 = \frac{8}{25 n^3} + o\left(\frac{1}{n^3}\right) \, . 
\end{align}
From $I = J = \mathbb{E}[(X(U'))^2] = \frac{4}{9}$, the asymptotic form is 
\begin{align}
 E_{\rm iid, asym} =& L(w_0) - \frac{1}{2n} + o\left(\frac{1}{n}\right) \, , \\
 V_{\rm iid, asym} =& \frac{1}{n} \mathbb{V}\left[\frac{1}{2}\right] + o\left(\frac{1}{n}\right) = o\left(\frac{1}{n}\right) \, , 
\end{align}
and we find that these are consistent to the exact form up to $O(1/n)$. 



Next, we evaluate the test error. 
We denote a sample mean of the test data as 
\begin{align}
 \left< X(U') \right>_m := \frac{1}{m} \sum_{i=n+1}^{n+m} X(U'_i) \, . 
\end{align}
Then, as $n \to \infty$, it converges in law to 
\begin{align}
 \begin{pmatrix}
  \left< X(U') \right>_n \\
  \left< X(U') \right>_m \\
 \end{pmatrix}
 \xrightarrow{d}
 {\cal N} \left( 
 \begin{pmatrix}
  0 \\
  0 \\
 \end{pmatrix} \, ,
 \begin{pmatrix}
  \frac{1}{n} \, I_\infty & 0 \\
  0 & \frac{1}{m} \, I_\infty \\
 \end{pmatrix} \right) \, . 
\end{align}
(Since the correlation decays at $o(1/n)$, the mean of the correlation between training set and test set converges to $0$.)
Using this relationship, we find that the test error is 
\begin{align}
 L'_m(\hat w) =& - \frac{1}{m} \sum_{i=n+1}^{n+m} \ln p(G(U'_i) \mid U'_i, \hat w) \nonumber \\
  =& \frac{1}{2} \ln (2 \pi) 
 + \frac{1}{2} \left< (X(U'))^2 \right>_m \, \left\{ (\hat w - 1) - \frac{\left< X(U') \right>_m}{\left< (X(U'))^2 \right>_m} \right\}^2
 - \frac{1}{2} \frac{\left< X(U') \right>_m^2}{\left< (X(U'))^2 \right>_m} + \frac{1}{2} \nonumber \\ 
  =& L(w_0) + \frac{1}{2} \frac{ \left< X(U')^2 \right>_m \, \left< X(U') \right>_n^2 }{ \left< X(U')^2 \right>_n^2 }
 - \frac{ \left< X(U') \right>_m \, \left< X(U') \right>_n }{ \left< X(U')^2 \right>_n } \nonumber \\
  =& L'_m(w_0) + \frac{1}{2} \frac{ \left< X(U')^2 \right>_m \, \left< X(U') \right>_n^2 }{ \left< X(U')^2 \right>_n^2 }
 - \frac{ \left< X(U') \right>_m \, \left< X(U') \right>_n }{ \left< X(U')^2 \right>_n } \, . 
\end{align}
In case that the data set is an i.i.d. samples, the mean and variance is 
\begin{align}
 E'_{\rm iid} =& \mathbb{E}[L'_m(\hat w)] 
  = L(w_0) + \frac{1}{2} \mathbb{E} \left[ \frac{ \left< X(U')^2 \right>_m \, \left< X(U') \right>_n^2 }{ \left< X(U')^2 \right>_n^2 } \right]
  - \mathbb{E}\left[ \frac{ \left< X(U') \right>_m \, \left< X(U') \right>_n }{ \left< X(U')^2 \right>_n } \right] \nonumber \\
  =& L(w_0) + \frac{1}{2} \, \frac{\mathbb{E}[X(U')^2]}{\mathbb{E}[X(U')^2]^2} \, \mathbb{E} \left[ \left< X(U') \right>_n^2 \right]
  - \frac{1}{ \mathbb{E}[X(U')^2] } \, \mathbb{E}\left[ \left< X(U') \right>_m \, \left< X(U') \right>_n \right] 
  + o\left(\frac{1}{n}\right) \nonumber \\
  =& L(w_0) + \frac{1}{2} \, \frac{1}{\mathbb{E}[X(U')^2]} \, \frac{1}{n} \, \mathbb{V}[X(U')] + o\left(\frac{1}{n}\right) 
  = L(w_0) + \frac{1}{2n} + o\left(\frac{1}{n}\right) \, , \\
 V'_{\rm iid} =& \mathbb{V}[L'_m(\hat w)] 
  = \mathbb{V} \left[ \frac{1}{2} \frac{ \left< X(U')^2 \right>_m \, \left< X(U') \right>_n^2 }{ \left< X(U')^2 \right>_n } 
  - \frac{ \left< X(U') \right>_m \, \left< X(U') \right>_n }{ \left< X(U')^2 \right>_n } \right] \nonumber \\
  =& \mathbb{V} \left[ \frac{1}{2} \frac{ \left< X(U')^2 \right>_m \, \left< X(U') \right>_n^2 }{ \left< X(U')^2 \right>_n } \right]
  + \mathbb{V} \left[ \frac{ \left< X(U') \right>_m \, \left< X(U') \right>_n }{ \left< X(U')^2 \right>_n } \right] \nonumber \\
  &- 2 \, {\rm Cov} \left( \frac{1}{2} \frac{ \left< X(U')^2 \right>_m \, \left< X(U') \right>_n^2 }{ \left< X(U')^2 \right>_n } \, , 
           \frac{ \left< X(U') \right>_m \, \left< X(U') \right>_n }{ \left< X(U')^2 \right>_n } \right) \nonumber \\
  =& \frac{1}{4} \, \frac{ \mathbb{E}[X(U')^2]^2 }{ \mathbb{E}[X(U')^2]^2 } \, \mathbb{V} \left[ \left< X(U') \right>_n^2 \right]
  + \frac{1}{ \mathbb{E}[X(U')^2]^2 } \, \mathbb{V} \left[ \left< X(U') \right>_m \, \left< X(U') \right>_n \right] \nonumber \\
  &- \frac{ \mathbb{E}[X(U')^2]^2 }{ \mathbb{E}[X(U')^2]^4 } \, {\rm Cov} \left( \left< X(U') \right>_n^2 \, , 
           \left< X(U') \right>_m \, \left< X(U') \right>_n \right)
  + o\left(\frac{1}{n}\right) \nonumber \\
  =& \frac{1}{4} \, \mathbb{V} \left[ \left< X(U') \right>_n^2 \right]
  + o\left(\frac{1}{n}\right) 
  = \frac{1}{4} \, \mathbb{V} \left[ \frac{1}{n^2} \left( \sum_{i=1}^n X(U'_i) \right)^2 \right]
  + o\left(\frac{1}{n}\right) 
  = \frac{1}{4 n^4} \, \mathbb{V} \left[ \left( \sum_{i=1}^n X(U'_i) \right)^2 \right]
  + o\left(\frac{1}{n}\right) \nonumber \\
  =& \frac{1}{4 n^4} \, \left( n \, \mathbb{V} [X(U')] \right) + o\left(\frac{1}{n}\right) 
  = \frac{1}{9 n^3} + o\left(\frac{1}{n}\right) \, . 
\end{align}
The asymptotic form is 
\begin{align}
 E'_{\rm iid, asym} =& L(w_0) + \frac{1}{2n} + o\left(\frac{1}{n}\right) \, , \\
 V'_{\rm iid, asym} =& 0 + o\left(\frac{1}{n}\right) = o\left(\frac{1}{n}\right) \, , 
\end{align}
and we find that these are consistent to the exact form up to $O(1/n)$. 

Next, suppose that the data set is sampled from Markov chain. 
We take mean and variance for the data set and obtain 
\begin{align}
 E'_{\rm M} =& \mathbb{E}[L'_m(\hat w)] 
  = L(w_0) + \frac{1}{2} \mathbb{E} \left[ \frac{ \left< X(U')^2 \right>_m \, \left< X(U') \right>_n^2 }{ \left< X(U')^2 \right>_n^2 } \right]
  - \mathbb{E}\left[ \frac{ \left< X(U') \right>_m \, \left< X(U') \right>_n }{ \left< X(U')^2 \right>_n } \right] \nonumber \\
  =& L(w_0) + \frac{1}{2} \, \frac{\mathbb{E}[X(U')^2]}{\mathbb{E}[X(U')^2]^2} \, \mathbb{E} \left[ \left< X(U') \right>_n^2 \right]
  - \frac{1}{ \mathbb{E}[X(U')^2] } \, \mathbb{E}\left[ \left< X(U') \right>_m \, \left< X(U') \right>_n \right] 
  + o\left(\frac{1}{n}\right) \nonumber \\
  =& L(w_0) + \frac{1}{2} \, \frac{1}{\mathbb{E}[X(U')^2]} \, \mathbb{E}\left[ \frac{1}{n^2} \left( \sum_{i=1}^n X(U'_i) \right)^2 \right] + o\left(\frac{1}{n}\right) \nonumber \\
  =& L(w_0) + \frac{1}{2} \, \frac{1}{\mathbb{E}[X(U')^2]} \, 
  \left[ \frac{1}{n} \, \mathbb{E}[X(U'_1)^2] + \frac{2(n-1)}{n^2} \, \mathbb{E}[X(U'_1) \, X(U'_2)] + \cdots + \frac{2}{n^2} \, \mathbb{E}[X(U'_1) \, X(U'_n)] \right] + o\left(\frac{1}{n}\right) \nonumber \\
  =& L(w_0) + \frac{1}{2 n} \, \frac{V_\infty}{\mathbb{E}[X(U')^2]} + o\left(\frac{1}{n}\right) 
  = L(w_0) + \frac{3}{2 n} + o\left(\frac{1}{n}\right) \, , \\
 V'_{\rm M} =& \mathbb{V}[L'_m(\hat w)] 
  = \mathbb{V} \left[ \frac{1}{2} \frac{ \left< X(U')^2 \right>_m \, \left< X(U') \right>_n^2 }{ \left< X(U')^2 \right>_n } 
  - \frac{ \left< X(U') \right>_m \, \left< X(U') \right>_n }{ \left< X(U')^2 \right>_n } \right] \nonumber \\
  =& \mathbb{V} \left[ \frac{1}{2} \frac{ \left< X(U')^2 \right>_m \, \left< X(U') \right>_n^2 }{ \left< X(U')^2 \right>_n } \right]
  + \mathbb{V} \left[ \frac{ \left< X(U') \right>_m \, \left< X(U') \right>_n }{ \left< X(U')^2 \right>_n } \right] \nonumber \\
  &- 2 \, {\rm Cov} \left( \frac{1}{2} \frac{ \left< X(U')^2 \right>_m \, \left< X(U') \right>_n^2 }{ \left< X(U')^2 \right>_n } \, , 
           \frac{ \left< X(U') \right>_m \, \left< X(U') \right>_n }{ \left< X(U')^2 \right>_n } \right) \nonumber \\
  =& \frac{1}{4} \, \frac{ \mathbb{E}[X(U')^2]^2 }{ \mathbb{E}[X(U')^2]^2 } \, \mathbb{V} \left[ \left< X(U') \right>_n^2 \right]
  + \frac{1}{ \mathbb{E}[X(U')^2]^2 } \, \mathbb{V} \left[ \left< X(U') \right>_m \, \left< X(U') \right>_n \right] \nonumber \\
  &- \frac{ \mathbb{E}[X(U')^2]^2 }{ \mathbb{E}[X(U')^2]^4 } \, {\rm Cov} \left( \left< X(U') \right>_n^2 \, , 
           \left< X(U') \right>_m \, \left< X(U') \right>_n \right)
  + o\left(\frac{1}{n}\right) \nonumber \\
  =& \frac{1}{4} \, \mathbb{V} \left[ \left< X(U') \right>_n^2 \right]
  + \frac{1}{ \mathbb{E}[X(U')^2]^2 } \, \mathbb{V} \left[ \left< X(U') \right>_m \, \left< X(U') \right>_n \right] \nonumber \\
  &- \frac{1}{ \mathbb{E}[X(U')^2]^2 } \, {\rm Cov} \left( \left< X(U') \right>_n^2 \, , 
           \left< X(U') \right>_m \, \left< X(U') \right>_n \right)
  + o\left(\frac{1}{n}\right) \nonumber \\
  =& \frac{1}{4} \, \mathbb{V} \left[ \frac{1}{n^2} \left( \sum_{i=1}^n X(U'_i) \right)^2 \right]
  + o\left(\frac{1}{n}\right) 
  = \frac{1}{4 n^4} \, \mathbb{V} \left[ \left( \sum_{i=1}^n X(U'_i) \right)^2 \right]
  + o\left(\frac{1}{n}\right) \nonumber \\
  =& \frac{1}{4 n^4} \, \left[ n \, \mathbb{V} [X(U'_1)] + 2(n-1) \, {\rm Cov}(X(U'_1), X(U'_2)) 
 + \cdots + 2 \, {\rm Cov}(X(U'_1), X(U'_n)) \right]
  + o\left(\frac{1}{n}\right) \nonumber \\
 =& \frac{V_\infty}{4 n^3} + o\left(\frac{1}{n}\right)
 = \frac{1}{3 n^3} + o\left(\frac{1}{n}\right) \, . 
\end{align}
The asymptotic form is 
\begin{align}
 E'_{\rm M, asym} =& L(w_0) + \frac{3}{2n} + o\left(\frac{1}{n}\right) \, , \\
 V'_{\rm iid, asym} =& 0 + o\left(\frac{1}{n}\right) = o\left(\frac{1}{n}\right) \, , 
\end{align}
and we find that these are consistent to the exact form up to $O(1/n)$. 
\end{comment}

 \subsection{Legendre polynomials}
\begin{comment}
・先行研究に倣って直交多項式のIPCを計算。
・IPC ~ 1 の例と IPC ~ 0 の例を示し、評価の正しさを見る。
\end{comment} 
The RC system approximates a mapping from the input to output time series by the linear combination of nonlinear dynamics. 
To capture the set of the functions that RC can produce, it is useful to evaluate the IPCs of the orthogonal polynomials~\cite{dambre2012information, kubota2021unifying}. 
In this subsection, the input sequence $U'$ is sampled independently from ${\rm Uniform}(-1, 1)$, and the target output is the product of Legendre polynomials, 
\begin{align}
\hat{y}(U'_t) = \prod_{i=1}^\infty L_{s_i}(u_{t-i}), 
\end{align}
where, $L_{s_i}$ is the $s_i$-th Legendre polynomial, and the number of nonzero elements in $(s_i)_{i=1}^\infty$ is finite. 

We employed two target outputs. 
One was a short-term linear task $\hat{y}_t = L_1(u_{t-1})$, which is easy to approximate with the ESN, and the other was a long-term nonlinear task $\hat{y}_t = L_{15}(u_{t-5})$, which is difficult to approximate. 
The number of hidden nodes was $d_1 = 100$, and the ESN parameters, $v_1, v_2, c$, were randomly chosen from the normal distribution. 
The spectral radius of $v_1$ was set to $0.9$, and the proportion of nonzero elements was set to $0.7$. 
For each $T$, we generated data $1000$ times and calculated the training and the test IPC. 
Then, we estimated the asymptotic parameters. 
Figs.~\ref{fig:legendre1_IPC} and \ref{fig:legendre15_IPC} show the results when the first- and 15th-order Legendre polynomials were used for the target outputs, respectively. 
The means and the variances of the true IPC for the first-order polynomial task are roughly on the asymptotic curve, indicating that the estimation was successful. 
In the 15th-order polynomial task, the asymptote fitted the mean IPC samples well, and the estimated true IPC was almost $0$. 
The variances were significantly off the asymptote in the log-log graph, which reflects our mention in Section 3 that when the true IPC is $0$, the variance approaches $0$ faster than $1/T$. 


\begin{comment}
RC は隠れノード関数の線形和で関数を近似するため、RC がどのような関数を近似できるかは、$L^2({\cal U}, \mu)$ の各直交多項式の IPC を計算することで評価できる\cite{dambre2012information, kubota2021unifying}。
入力列 $U'$ を ${\rm Uniform}(-1, 1)$ から独立にサンプルし、出力をルジャンドル多項式の積
\begin{align}
\hat{y}(U'_t) = \prod_{i=1}^\infty L_{s_i}(u_{t-i}),
\end{align}
とする。ここで、$L_{s_i}$ は $s_i$ 次ルジャンドル多項式であり、$\{s_i\}_{i=1}^\infty$ のうち $0$ でない要素は有限個とする。
ここでは教師信号として、ESN で近似が容易な短時間の線形タスク $y_t = L_1(u_{t-1})$ と、近似が難しい長時間の非線形タスク $y_t = L_{15}(u_{t-5})$ を使用する。
隠れノード数は $d_1 = 100$ とし、ESN パラメータ $v_1, v_2, c$ は正規分布を用いてランダムにとった。
特に $v_1$ はスペクトル半径を $0.9$ とし、$0$ でない要素の割合は $0.7$ とした。
% $T = 200, 300, \ldots, 1000, 2000, \ldots, 10000, 20000, \ldots, 100000$, $T' = T$ とた。
各 $T$ で $1000$ 回データを生成し、訓練 IPC とテスト IPC の計算を行った。
続いてそれらの平均値と分散から最小二乗法で漸近形のパラメータ推定をした。
教師出力にルジャンドル1次式を用いたものは図\ref{fig:legendre1_IPC}, ルジャンドル15次式を用いたものは図\ref{fig:legendre15_IPC}となった。
1次式のほうは平均値、分散ともに推定した曲線状に概ね乗っており、推定がうまく行えていることが分かる。
15次式のほうは平均値のフィッティングはうまく行えたが、真の IPC の推定値がほぼ $0$ となっており、そのため分散の結果は両対数グラフで大きくずれていることが分かる。
3節で指摘したように、真の IPC が $0$ のときは分散は $1/T$ より速く $0$ に近づくことが分かる。
\end{comment}

\begin{figure}[htbp]
  \begin{tabular}{cc}
    \begin{minipage}[t]{0.4\hsize}
      \centering
      \includegraphics[keepaspectratio, scale=0.4]{asymp_paper/legendre1/IPC_means.eps}
      \subcaption{Mean of IPCs for the 1st order polynomial task}
      \label{fig:legendre1_IPC_mean}
    \end{minipage} & 
    \begin{minipage}[t]{0.4\hsize}
      \centering
      \includegraphics[keepaspectratio, scale=0.4]{asymp_paper/legendre1/IPC_means_log.eps}
      \subcaption{Mean of IPCs after removal of the constant term on a log-log scale for the 1st order polynomial task}
      \label{fig:legendre1_IPC_mean_log}
    \end{minipage} \\
    \begin{minipage}[t]{0.4\hsize}
      \centering
      \includegraphics[keepaspectratio, scale=0.4]{asymp_paper/legendre1/IPC_vars.eps}
      \subcaption{Variance of IPCs for the 1st order polynomial task}
      \label{fig:legendre1_IPC_var}
    \end{minipage} &
    \begin{minipage}[t]{0.4\hsize}
      \centering
      \includegraphics[keepaspectratio, scale=0.4]{asymp_paper/legendre1/IPC_vars_log.eps}
      \subcaption{Variance of IPCs on a log-log scale for the 1st order polynomial task}
      \label{fig:legendre1_IPC_var_log}
    \end{minipage} \\
  \end{tabular}
  \caption{The means and the variances of the IPC using uniformly distributed input and Legendre first-order output were plotted, along with the asymptote estimated from them. The simulation results were mostly on the theoretical line, indicating the effectiveness of the estimation.}
  % \caption{一様分布入力とルジャンドル1次出力を学習データとしたIPCの平均値および分散と、それらから二乗誤差を用いて推定した漸近線をプロットした。シミュレーション結果はおおむね理論線上にあり、推定結果の正しさが分かる。}
\label{fig:legendre1_IPC}
\end{figure}


\begin{figure}[htbp]
  \begin{tabular}{cc}
    \begin{minipage}[t]{0.4\hsize}
      \centering
      \includegraphics[keepaspectratio, scale=0.4]{asymp_paper/legendre15/IPC_means.eps}
      \subcaption{Mean of IPCs for the 15th order polynomial task}
      \label{fig:legendre15_IPC_mean}
    \end{minipage} & 
    \begin{minipage}[t]{0.4\hsize}
      \centering
      \includegraphics[keepaspectratio, scale=0.4]{asymp_paper/legendre15/IPC_means_log.eps}
      \subcaption{Mean of IPCs after removal of the constant term on a log-log scale for the 15th order polynomial task}
      \label{fig:legendre15_IPC_mean_log}
    \end{minipage} \\
    \begin{minipage}[t]{0.4\hsize}
      \centering
      \includegraphics[keepaspectratio, scale=0.4]{asymp_paper/legendre15/IPC_vars.eps}
      \subcaption{Variance of IPCs for the 15th order polynomial task}
      \label{fig:legendre15_IPC_var}
    \end{minipage} &
    \begin{minipage}[t]{0.4\hsize}
      \centering
      \includegraphics[keepaspectratio, scale=0.4]{asymp_paper/legendre15/IPC_vars_log.eps}
      \subcaption{Variance of IPCs on a log-log scale for the 15th order polynomial task}
      \label{fig:legendre15_IPC_var_log}
    \end{minipage} \\
  \end{tabular}
  \caption{The means and the variances of the IPC using uniformly distributed input and 15th-order Legendre output are plotted, along with the asymptote estimated from them. Although the means are roughly on the theoretical line, the variances are significantly off. Since the true IPC is nearly $0$, we can find that the variance decays faster than $1/T$. (In this graph, it decays at approximately $1/T^2$.)}
  % \caption{一様分布入力とルジャンドル15次出力を学習データとしたIPCの平均値および分散と、それらから二乗誤差を用いて推定した漸近線をプロットした。平均値はおおむね理論線通りだが、分散は大きくずれている。真の IPC がほぼ $0$ なので、分散は $1/T$ より速く減衰しているのが分かる。（このグラフではほぼ $1/T^2$ で減衰している。）}
\label{fig:legendre15_IPC}
\end{figure}

\begin{comment}
The performance of RC can be evaluated by the IPCs of the orthogonal polynomials of input sequence space. 
We employ input sequence as $u_t \sim {\rm Uniform}(-1, 1)$ and the output as 
\begin{align}
  y_t = \prod_{i=1}^\infty L_{s_i}(u_{t-i}), 
\end{align}
where $L_{s_i}$ is the $s_i$-th order of the Legendre polynomial and finite numbers of $s_i$ are nonzero. 
We employ $y_t = L_1(u_{t-1}) = u_{t-1}$ and $y_t = L_{15}(u_{t-5})$ as the teacher signals. 
We can estimate the true IPC by the least square method. 
\end{comment}


 \subsection{NARMA10}
\begin{comment}
・RCのベンチマークとしてよく用いられるNARMA10を使う。
・複雑なタスクでも使えることを見る。
\end{comment}
The NARMA10 task is widely used as a benchmark for RC~\cite{atiya2000new}. 
The input sequence is sampled independently from the uniform distribution. 
The output sequence of the NARMA10 is given by 
\begin{align}
 \hat{y}_t 
  = \alpha y_{t-1} + \beta y_{t-1} \sum_{i=1}^{10} y_{t-i} + \gamma u_t u_{t-9} + \delta.
\end{align}
We used ${\rm Uniform}(0, 0.2)$ as the input distribution and $(\alpha, \beta, \gamma, \delta) = (0.3, 0.05, 1.5, 0.1)$ as NARMA10 parameters. 
The number of hidden nodes was $d_1 = 100$, and the ESN parameters $v_1, v_2, c$, were randomly chosen from the normal distribution. 
The spectral radius of $v_1$ was set to $0.9$, and the proportion of non-zero elements was set to $0.7$. 
For each $T$, data were generated $1000$ times, and the training and the test IPC were calculated. 
Then, we estimate the asymptotic parameters. 
The results are shown in Fig.~\ref{fig:narma10_IPC}. 
We can see that both the means and variances are almost on the asymptote. 

\begin{comment}
NARMA10 タスクは RC のベンチマークタスクとして広く使用されている\cite{atiya2000new}。
入力列は ${\rm Uniform}(0, 0.2)$ から独立にサンプルした。
NARMA10 の出力 $\hat{y}_t$ は以下で与えられる。
\begin{align}
\hat{y}_t = \alpha y_{t-1} + \beta y_{t-1} \sum_{i=1}^{10} y_{t-i} + \gamma u_t u_{t-9} + \delta,
\end{align}
ここで NARMA10 パラメーターとして $(\alpha, \beta, \gamma, \delta) = (0.3, 0.05, 1.5, 0.1)$ を使用した。
% $T = 200, 300, \ldots, 1000, 2000, \ldots, 10000, 20000, \ldots, 100000$, $T' = T$ とした。
各 $T$ で $1000$ 回データを生成し、訓練 IPC とテスト IPC の計算を行った。
続いてそれらの平均値と分散から最小二乗法で漸近形のパラメータ推定をした。
結果は図\ref{fig:narma10_IPC}のようになった。
平均値、分散ともに漸近線にほぼ乗っていることが分かる。
\end{comment}

\begin{figure}[htbp]
  \begin{tabular}{cc}
    \begin{minipage}[t]{0.4\hsize}
      \centering
      \includegraphics[keepaspectratio, scale=0.4]{asymp_paper/narma10/IPC_means.eps}
      \subcaption{Mean of IPCs for the NARMA10 task}
      \label{fig:narma10_IPC_mean}
    \end{minipage} & 
    \begin{minipage}[t]{0.4\hsize}
      \centering
      \includegraphics[keepaspectratio, scale=0.4]{asymp_paper/narma10/IPC_means_log.eps}
      \subcaption{Mean of IPCs after removal of the constant term on a log-log scale for the NARMA10 task}
      \label{fig:narma10_IPC_mean_log}
    \end{minipage} \\
    \begin{minipage}[t]{0.4\hsize}
      \centering
      \includegraphics[keepaspectratio, scale=0.4]{asymp_paper/narma10/IPC_vars.eps}
      \subcaption{Variance of IPCs for the NARMA10 task}
      \label{fig:narma10_IPC_var}
    \end{minipage} &
    \begin{minipage}[t]{0.4\hsize}
      \centering
      \includegraphics[keepaspectratio, scale=0.4]{asymp_paper/narma10/IPC_vars_log.eps}
      \subcaption{Variance of IPCs on a log-log scale for the NARMA10 task}
      \label{fig:narma10_IPC_var_log}
    \end{minipage} \\
  \end{tabular}
  \caption{The means and the variances of the IPC obtained from the NARMA10 task and the asymptote estimated from them are plotted. The simulation results are almost on the theoretical lines.}
  % \caption{NARMA10タスクで得られたIPCの平均値および分散と、それらから二乗誤差を用いて推定した漸近線をプロットした。平均値、分散ともにシミュレーション結果はほぼ理論線の上に乗っており、この推定法はベンチマークタスクにも使用できることが分かる。}
\label{fig:narma10_IPC}
\end{figure}


\begin{comment}
The NARMA10 task is a widely used benchmark task for RC. 
We employ input sequence as $u_t \sim {\rm Uniform}(0, 0.2)$, and the output of the NARMA10, $y_t$ as 
\begin{align}
  y_t = \alpha y_{t-1} + \beta  y_{t-1} \sum_{i=1}^{10} y_{t-i} + \gamma u_t u_{t-9} + \delta, 
\end{align}
where we use the NARMA10 parameter $(\alpha, \beta, \gamma, \delta) = (0.3, 0.05, 1.5, 0.1)$. 
We can also estimate the true IPC for this task. 
\end{comment}


\section{Summary}
\begin{comment}
・RC の説明と長所と短所の説明。
・性能指標としての IPC と既存の推定法の紹介。
・既存手法の問題点を漸近形で解決。
・計算の重さは今後の課題。
\end{comment}
RNNs and their variants such as LSTM~\cite{hochreiter1997long} and GRU~\cite{cho2014learning} can approximate the relationship between the input and the output time series. 
However, the gradient learning takes a long time as data length becomes longer. 
RC is a system that approximates the relationship between input and output time series by the linear combination of nonlinear dynamics and finishes training quickly by optimizing only the linear combination. 

The performance of RC is evaluated by the mean squared error or the squared error normalized by the target output. 
The latter is called the IPC~\cite{dambre2012information, kubota2021unifying}. 
To know the approximation accuracy of the relationship between input and output time series, we should evaluate the IPC for infinitely long data, not the IPC for finite-length data. 
Simply evaluating the IPC with long data cannot remove the difference from the limit value. 
We propose a method to estimate the IPC for infinitely long data by using the asymptotic expansions of the training and the test IPC and the least-squares method. 
Then, we show the validity of our method by numerical simulations. 
Although we used ESNs in the demonstrations, our method can be applied to RC systems that satisfies the finiteness of the covariance matrices, as mentioned in the last second paragraph of Section 3. 

Our method can estimate the IPC more accurately than simply extending the data length. 
However, it is computationally heavy due to multiple trials with long data. 
This is a trade-off with the estimation accuracy. 
% Hence, we would like the designer to choose an appropriate method based on the length of the data that can be obtained. 
A computationally less costly method for evaluating the IPC is our future work. 


\begin{comment}
RNN は入出力系列の対応関係を任意の精度で近似できるが\cite{schafer2006recurrent}、
再帰結合の最適化は勾配消失や勾配爆発により難しい\cite{bengio1994learning}。
そのためそれらの問題を解消した LSTM\cite{hochreiter1997long} や GRU\cite{cho2014learning} など多くの変種が提案されてきたが、長いデータに対する勾配学習の遅さは解消できていない。
RC は読み取り層の線形変換のみ学習することで、ある程度の性能を確保しつつ、学習を高速に終えられる系である\cite{jaeger2004harnessing, tanaka2019recent}。
% 性能は初期値の選び方に大きく左右されてしまうが、学習は非常に高速に行える利点がある。
RC の性能評価には平均二乗誤差のほかに、規格化された二乗誤差に由来する IPC も用いられている\cite{dambre2012information, kubota2021unifying}。
入出力対応関係の近似精度を知るには有限長データに対する IPC ではなく、無限長データに対する IPC を評価しなければならないが、単純に長いデータで評価するだけでは、極限値との差分が曖昧なままになる。
本研究は IPC の漸近展開を用いて極限値との差を明確にし、さらにフィッティングを用いて極限値を推定する方法を提案した。
今回は ESN に適用したが、この推定法は ESN 以外の RC にも適応できるので、ぜひ使っていただきたい。
この推定は単にデータ長を伸ばすより正確に IPC を推定できるとはいえ、長いデータで何度も試行する必要があるので、計算が重いのが欠点である。
これは推定精度とのトレードオフなので、取得できるデータの長さと合わせて設計者側が適切に選んでいただきたい。
どのようなトレードオフになっているかは今後の課題である。
\end{comment}


\section*{Acknowledgement}
% 本研究は…。
% また森江先生及び窪田博士には有益なコメントをいただき感謝する。
This paper is based on results obtained from a project, JPNP16007, commissioned by the New Energy and Industrial
Technology Development Organization (NEDO). 
We appreciate Professor Takashi Morie and Doctor Tomoyuki Kubota for their fruitful comments. 

\appendix
% \section{Evaluation of the IPC}
\begin{comment}
 本文でごまかした導出を書く必要があるかもしれない。
 特に定常仮定の中心極限定理部分。
 もし書く必要があるなら、単独では無理なので、誰かに協力を頼む。
\end{comment}

\section{Least mean square}
Let $f_1(n) = a + \frac{b_1}{n}$ and $f_2(n) = a - \frac{b_2}{n}$ be the asymptotic mean IPCs for the training and the test data, respectively, 
and $\{(n_{i,1}, g_{i,1})\}_{i=1}^N$ and $\{(n_{i,2}, g_{i,2})\}_{i=1}^N$ are the mean IPCs for the training and the test data at each data length. 
We determine $a, b_1, b_2$ by the least mean square of the cost function below, 
\begin{align}
 L(a, b, b') = \frac{1}{2} \sum_{i=1}^N \left[ \left(a + \frac{b_1}{n_{i, 1}} - g_{i,1}\right)^2 + \left(a - \frac{b_2}{n_{i, 2}} - g_{i,2}\right)^2\right]. 
\end{align}
Using the notation, 
\begin{align}
 & \alpha_1 = \sum_{i=1}^N \frac{1}{n_{i,1}}, \ \ 
 \alpha_2 = \sum_{i=1}^N \frac{1}{n_{i,2}}, \ \ 
 \beta_1 = \sum_{i=1}^N \frac{1}{n_{i,1}^2}, \ \ 
 \beta_2 = \sum_{i=1}^N \frac{1}{n_{i,2}^2}, \\
 & s_1 = \sum_{i=1}^N g_{i,1}, \ \ 
 s_2 = \sum_{i=1}^N g_{i,2}, \ \ 
 t_1 = \sum_{i=1}^N \frac{g_{i,1}}{n_{i,1}}, \ \ 
 t_2 = \sum_{i=1}^N \frac{g_{i,2}}{n_{i,2}}, 
\end{align}
we can obtain $a, b, b'$ from 
\begin{align}
 \begin{pmatrix}
  2 N & \alpha_1 & - \alpha_2 \\
  \alpha_1 & \beta_1 & 0 \\
  \alpha_2 & 0 & - \beta_2 \\
 \end{pmatrix} \, 
 \begin{pmatrix}
  a \\
  b_1 \\
  b_2 \\
 \end{pmatrix} = 
 \begin{pmatrix}
  s_1 + s_2 \\
  t_1 \\
  t_2 \\
 \end{pmatrix} .
\end{align}
In the same way, let $f'_1(n) = \frac{d}{n}$ and $f'_2(n) = \frac{d}{n}$ are the asymptotic IPC variances for the training and the test data, respectively, and $\{(n_{i,1}, g'_{i,1})\}_{i=1}^N$ and $\{(n_{i,2}, g'_{i,2})\}_{i=1}^N$ are the IPC variances at each data length. 
We determine $d$ by the least mean square, 
\begin{align}
 L'(d) = \frac{1}{2} \sum_{i=1}^N \left[ \left(\frac{d}{n_{i,1}} - g'_{i,1}\right)^2 + \left(\frac{d}{n_{i,2}} - g'_{i,2}\right)^2\right], 
\end{align}
and obtain 
\begin{align}
 d = \frac{\sum_{i=1}^N \left(\frac{g'_{i,1}}{n_{i,1}} + \frac{g'_{i,2}}{n_{i,2}}\right)}{\sum_{i=1}^N \left(\frac{1}{n_{i,1}^2} + \frac{1}{n_{i,2}^2}\right)} = \frac{t'_1 + t'_2}{\beta_1 + \beta_2}, 
\end{align}
where 
\begin{align}
 t'_1 = \sum_{i=1}^N \frac{g'_{i,1}}{n_{i,1}}, \ \ 
 t'_2 = \sum_{i=1}^N \frac{g'_{i,2}}{n_{i,2}}. 
\end{align}

\begin{comment}
\section{Evaluation of a simple model}
 \begin{align}
 \mathbb{E}[X(U')] =& \int X(u') \, p_u(du') = \sum_{s=0}^\infty 2^{-s} \, \int_{-1}^1 u_{-s} \, \frac{du_{-s}}{2} = 0 \, , \\
 \mathbb{E}[X(U')^2] =& \int (X(u'))^2 \, p_u(du') = \sum_{s=0}^\infty 4^{-s} \, \int_{-1}^1 (u_{-s})^2 \, \frac{du_{-s}}{2} = \frac{4}{9} \, , \\
 \mathbb{E}[X(U')^4] =& \int (X(u'))^4 \, p_u(du') = \int \left(\sum_{s=0}^\infty 2^{-s} u_{-s}\right)^4 \, p_u(du') \nonumber \\
 =& \int \left[ \sum_{s=0}^\infty 16^{-s} (u_{-s})^4 + \frac{1}{2} \, \frac{4!}{2! \, 2!} \sum_{s=0}^\infty \sum_{t \neq s} 4^{-s} (u_{-s})^2 4^{-t} (u_{-t})^2 \right] \, p_u(du') \nonumber \\
 =& \sum_{s=0}^\infty 16^{-s} \int_{-1}^1 (u_{-s})^4 \, \frac{du_{-s}}{2} 
 + 3 \sum_{s=0}^\infty \sum_{t \neq s} 4^{-s} 4^{-t} \int_{-1}^1 \int_{-1}^1 (u_{-s})^2 \, (u_{-t})^2 \, \frac{du_{-s}}{2} \, \frac{du_{-t}}{2} \nonumber \\
 =& \frac{1}{5} \sum_{s=0}^\infty 16^{-s} 
 + \frac{1}{3} \sum_{s=0}^\infty \sum_{t \neq s} 4^{-s} 4^{-t} \nonumber \\
 =& \frac{1}{5} \, \frac{16}{15} + \frac{1}{3} \left( \sum_{s, t} 4^{-s} 4^{-t} - \sum_{s=0}^\infty 4^{-s} 4^{-s} \right) 
 = \frac{1}{5} \, \frac{16}{15} + \frac{1}{3} \left( \frac{16}{9} - \frac{16}{15} \right) = \frac{304}{675}  \, , \\
 \mathbb{E}[X(U') \, G(U')] =& \mathbb{E}[X(U')] + \mathbb{E}[X(U')^2] = \frac{4}{9} \, , 
\end{align}
In case of $t \geq 0$, 
\begin{align}
  & \mathbb{E}[x(U'_0)^2 \, x(U'_t)] 
 = \int \left( \sum_{s=0}^\infty 2^{-s} u_{-s} \right)^2 \, \left( \sum_{s'=0}^\infty 2^{-s'} u_{t-s'} \right) \, p_u(du') \nonumber \\
 =& \int \left( \sum_{s=0}^\infty 2^{-s} u_{-s} \right)^2 \, \left( 2^{-t} \sum_{s'=0}^\infty 2^{-s'} u_{-s'} + \sum_{s'=0}^{t-1} 2^{-s'} u_{t-s'} \right) \, p_u(du') \nonumber \\
 =& 2^{-t} \, \mathbb{E}[x(U'_0)^3] + \int \left( \sum_{s=0}^\infty 2^{-s} u_{-s} \right)^2 \, \left( \sum_{s'=0}^{t-1} 2^{-s'} u_{t-s'} \right) \, p_u(du') 
 = 0 + 0 = 0 \, , 
\end{align}
In case of $t < 0$, using the time translation invariance and $t' = - t > 0$, 
we find that 
\begin{align}
 \mathbb{E}[x(U'_0)^2 \, x(U'_t)] = \mathbb{E}[x(U'_{-t})^2 \, x(U'_0)] 
  = \mathbb{E}[x(U'_{t'})^2 \, x(U'_0)] \, .
\end{align}
Therefore, it is sufficient to evaluate the below correlation for $t > 0$. 
\begin{align}
  & \mathbb{E}[x(U'_0) \, x(U'_t)^2] 
 = \int \left( \sum_{s=0}^\infty 2^{-s} u_{-s} \right) \, \left( \sum_{s'=0}^\infty 2^{-s'} u_{t-s'} \right)^2 \, p_u(du') \nonumber \\
 =& \int \left( \sum_{s=0}^\infty 2^{-s} u_{-s} \right) \, \left( 2^{-t} \sum_{s'=0}^\infty 2^{-s'} u_{-s'} + \sum_{s'=0}^{t-1} 2^{-s'} u_{t-s'} \right)^2 \, p_u(du') \nonumber \\
 =& 2^{-2t} \, \mathbb{E}[x(U'_0)^3] 
 + 2 \cdot 2^{-t} \, \mathbb{E}[x(U'_0)^2] \int \left( \sum_{s'=0}^{t-1} 2^{-s'} u_{t-s'} \right) \, p_u(du')
 + \mathbb{E}[x(U'_0)] \int \left( \sum_{s'=0}^{t-1} 2^{-s'} u_{t-s'} \right)^2 \, p_u(du') \nonumber \\
 =& 0 + \mathbb{E}[x(U'_0)^2] \cdot 0 + 0 \cdot \int \left( \sum_{s'=0}^{t-1} 2^{-s'} u_{t-s'} \right)^2 \, p_u(du') = 0 \, .
\end{align}
Hence, we obtain 
\begin{align}
 \mathbb{E}[x(U'_0)^2 \, x(U'_t)] = \mathbb{E}[x(U'_0) \, x(U'_t)^2] = 0 \, . 
\end{align}

\begin{align}
  & \mathbb{E}[x(U'_0)^3 \, x(U'_t)] 
 = \int \left( \sum_{s=0}^\infty 2^{-s} u_{-s} \right)^3 \, \left( \sum_{s'=0}^\infty 2^{-s'} u_{t-s'} \right) \, p_u(du') \nonumber \\
 =& \int \left( \sum_{s=0}^\infty 2^{-s} u_{-s} \right)^3 \, \left( 2^{-t} \sum_{s'=0}^\infty 2^{-s'} u_{-s'} + \sum_{s'=0}^{t-1} 2^{-s'} u_{t-s'} \right) \, p_u(du') \nonumber \\
 =& 2^{-t} \, \mathbb{E}[x(U'_0)^4] + \int \left( \sum_{s=0}^\infty 2^{-s} u_{-s} \right)^3 \, \left( \sum_{s'=0}^{t-1} 2^{-s'} u_{t-s'} \right) \, p_u(du') = 2^{-t} \, \mathbb{E}[x(U'_0)^4] \, ,
\end{align}
\begin{align}
  & \mathbb{E}[x(U'_0) \, x(U'_t)^3] 
 = \int \left( \sum_{s=0}^\infty 2^{-s} u_{-s} \right) \, \left( \sum_{s'=0}^\infty 2^{-s'} u_{t-s'} \right)^3 \, p_u(du') \nonumber \\
 =& \int \left( \sum_{s=0}^\infty 2^{-s} u_{-s} \right) \, \left( 2^{-t} \sum_{s'=0}^\infty 2^{-s'} u_{-s'} + \sum_{s'=0}^{t-1} 2^{-s'} u_{t-s'} \right)^3 \, p_u(du') \nonumber \\
 =& 2^{-3t} \, \mathbb{E}[x(U'_0)^4] + 3 \cdot 2^{-t} \int \left( \sum_{s=0}^\infty 2^{-s} u_{-s} \right)^2 \, \left( \sum_{s'=0}^{t-1} 2^{-s'} u_{t-s'} \right)^2 \, p_u(du') \nonumber \\
 =& 2^{-3t} \, \mathbb{E}[x(U'_0)^4] + 3 \cdot 2^{-t} \int \left( \sum_{s=0}^\infty 2^{-s} u_{-s} \right)^2 \, \left( \sum_{s'=0}^{t-1} 2^{-2s'} {u_{t-s'}}^2 \right) \, p_u(du') \nonumber \\
 =& 2^{-3t} \, \mathbb{E}[x(U'_0)^4] + 3 \cdot 2^{-t} \, \mathbb{E}[x(U'_0)^2] \, \left( \mathbb{E}[U^2] \, \sum_{s'=0}^{t-1} 2^{-2s'} \right) \nonumber \\
 =& 2^{-3t} \, \mathbb{E}[x(U'_0)^4] + \frac{4}{3} \cdot 2^{-t} \, (1 - 2^{-2t}) \, \mathbb{E}[x(U'_0)^2] \, , 
\end{align}
In case of $t \geq 0$, 
\begin{align}
  & \mathbb{E}[x(U'_0)^2 \, x(U'_t)^2] 
 = \int \left( \sum_{s=0}^\infty 2^{-s} u_{-s} \right)^2 \, \left( \sum_{s'=0}^\infty 2^{-s'} u_{t-s'} \right)^2 \, p_u(du') \nonumber \\
 =& \int \left( \sum_{s=0}^\infty 2^{-s} u_{-s} \right)^2 \, \left( 2^{-t} \sum_{s'=0}^\infty 2^{-s'} u_{-s'} + \sum_{s'=0}^{t-1} 2^{-s'} u_{t-s'} \right)^2 \, p_u(du') \nonumber \\
 =& 2^{-2t} \, \mathbb{E}[x(U'_0)^4] + \mathbb{E}[x(U'_0)^2] \, \int \left( \sum_{s'=0}^{t-1} 2^{-s'} u_{t-s'} \right)^2 \, p_u(du') \nonumber \\
 =& 2^{-2t} \, \mathbb{E}[x(U'_0)^4] + \mathbb{E}[x(U'_0)^2] \, \int \left( \sum_{s'=0}^{t-1} 2^{-2s'} u_{t-s'}^2 \right) \, p_u(du') \nonumber \\
 =& 2^{-2t} \, \mathbb{E}[x(U'_0)^4] + \mathbb{E}[x(U'_0)^2] \, \mathbb{E}[U^2] \sum_{s'=0}^{t-1} 2^{-2s'} \nonumber \\
 =& 2^{-2t} \, \mathbb{E}[x(U'_0)^4] + \frac{4}{9} \, (1 - 2^{-2t}) \, \mathbb{E}[x(U'_0)^2] \, . 
\end{align}
In case of $t < 0$, from 
\begin{align}
 \mathbb{E}[x(U'_0)^2 \, x(U'_t)^2] = \mathbb{E}[x(U'_{-t})^2 \, x(U'_0)^2]
 = \mathbb{E}[x(U'_{t'})^2 \, x(U'_0)^2], 
\end{align}
we find that it is sufficient to evaluate $\mathbb{E}[x(U'_{t'})^2 \, x(U'_0)^2]$ for $t' = - t > 0$. 

Other components of the asymptotic expansions are 
\begin{align}
 & {\rm Cov}(l(w_0; U'_0) \zeta(U'_t)) = \mathbb{E}[l(w_0; U'_0) \zeta(U'_t)]
 = \mathbb{E}[(\hat{y}(U'_0) - w_0 x(U'_0))^2 (\hat{y}(U'_t)^2 - \mu_0)]
 = \mathbb{E}[\hat{y}(U'_t)^2 - \mu_0] = 0, \\
 & C_{l, \zeta} = \sum_{t=-\infty}^\infty {\rm Cov}(l(w_0; U'_0) \zeta(U'_t)) = 0. 
\end{align}
\begin{align}
 & {\rm Cov}(l(w_0; U'_0) l(w_0; U'_t)) = \mathbb{E}[l(w_0; U'_0) l(w_0; U'_t)] - 1
 = \mathbb{E}[(\hat{y}(U'_0) - w_0 x(U'_0))^2 (\hat{y}(U'_t) - w_0 x(U'_t))^2] - 1 = 0, \\
 & V_{l, \infty} = \sum_{t=-\infty}^\infty {\rm Cov}(l(w_0; U'_0) l(w_0; U'_t)) = 0. 
\end{align}
\begin{align}
 & {\rm Cov}(\zeta(U'_0) \zeta(U'_t)) = \mathbb{E}[\zeta(U'_0) \zeta(U'_t)]
 = \mathbb{E}[(\hat{y}(U'_0)^2 - \mu_0) (\hat{y}(U'_t)^2 - \mu_0)]
 = \mathbb{E}[\hat{y}(U'_t)^2 \hat{y}(U'_0)^2] - \mu_0^2 \nonumber \\
 =& \mathbb{E}[x(U'_0)^2 \, x(U'_t)^2] + 4 \mathbb{E}[x(U'_0) \, x(U'_t)] + 2 \mathbb{E}[x(U'_0)^2 ] + 1 - \mu_0^2 
 = \frac{512}{2025} \cdot 4^{-|t|} + \frac{16}{9} \cdot 2^{-|t|} , \\
 & V_{\zeta, \infty} = \sum_{t=-\infty}^\infty {\rm Cov}(\zeta(U'_0) \zeta(U'_t)) = \frac{6992}{1215} . 
\end{align}
\begin{align}
 & I_t = {\rm Cov}((\hat{y}(U'_0) - w_0 x(U'_0)) \, x(U'_0) \, (\hat{y}(U'_t) - w_0 x(U'_t)) \, x(U'_t)) \nonumber \\
 &= {\rm Cov}(x(U'_0) \, x(U'_t))
 = \mathbb{E}[x(U'_0) \, x(U'_t)]
 = 2^{-|t|} \, \mathbb{E}[x(U')^2], \\
 & I_\infty = \sum_{t=-\infty}^\infty I_t = 3 \, \mathbb{E}[x(U')^2] = \frac{4}{3}. 
\end{align}
\begin{align}
 J = \frac{\partial^2 l}{\partial w^2}(w_0) = \frac{8}{9}. 
\end{align}
\end{comment}

\begin{thebibliography}{10}

\bibitem{rumelhart1986learning}
D.~E. Rumelhart, G.~E. Hinton, and R.~J. Williams.
\newblock Learning representations by back-propagating errors.
\newblock {\em Nature}, 323(6088):533--536, 1986.

\bibitem{schafer2006recurrent}
A.~M. Sch{\"a}fer and H.~G. Zimmermann.
\newblock Recurrent neural networks are universal approximators.
\newblock In {\em Artificial Neural Networks--ICANN 2006, Part I}, pages
  632--640. Springer, 2006.

\bibitem{bengio1994learning}
Y.~Bengio, P.~Simard, and P.~Frasconi.
\newblock Learning long-term dependencies with gradient descent is difficult.
\newblock {\em IEEE Transactions on Neural Networks}, 5(2):157--166, 1994.

\bibitem{hochreiter1997long}
S.~Hochreiter.
\newblock Long short-term memory.
\newblock {\em Neural Computation}, 9(8):1735--1780, 1997.

\bibitem{cho2014learning}
K.~Cho, B.~v. Merrienboer, C.~Gulcehre, D.~Bahdanau, F.~Bougares, H.~Schwenk,
  and Y.~Bengio.
\newblock Learning phrase representations using rnn encoder-decoder for
  statistical machine translation.
\newblock {\em arXiv preprint arXiv:1406.1078}, 2014.

\bibitem{jaeger2004harnessing}
H.~Jaeger and H.~Haas.
\newblock Harnessing nonlinearity: Predicting chaotic systems and saving energy
  in wireless communication.
\newblock {\em Science}, 304(5667):78--80, 2004.

\bibitem{tanaka2019recent}
G.~Tanaka, T.~Yamane, J.~B. H{\'e}roux, R.~Nakane, N.~Kanazawa, S.~Takeda,
  H.~Numata, D.~Nakano, and A.~Hirose.
\newblock Recent advances in physical reservoir computing: A review.
\newblock {\em Neural Networks}, 115:100--123, 2019.

\bibitem{grigoryeva2018echo}
L.~Grigoryeva and J.-P. Ortega.
\newblock Echo state networks are universal.
\newblock {\em Neural Networks}, 108:495--508, 2018.

\bibitem{gonon2019reservoir}
L.~Gonon and J.-P. Ortega.
\newblock Reservoir computing universality with stochastic inputs.
\newblock {\em IEEE Transactions on Neural Networks and Learning Systems},
  31(1):100--112, 2019.

\bibitem{dambre2012information}
J.~Dambre, D.~Verstraeten, B.~Schrauwen, and S.~Massar.
\newblock Information processing capacity of dynamical systems.
\newblock {\em Scientific Reports}, 2(1):514, 2012.

\bibitem{kubota2021unifying}
T.~Kubota, H.~Takahashi, and K.~Nakajima.
\newblock Unifying framework for information processing in stochastically
  driven dynamical systems.
\newblock {\em Physical Review Research}, 3(4):043135, 2021.

\bibitem{jaeger2002tutorial}
H.~Jaeger.
\newblock Tutorial on training recurrent neural networks, covering BPPT,
  RTRL, EKF and the echo state network approach. 
\newblock {\em GMD-Forschungszentrum Informationstechnik}, volume~5, 2002.

\bibitem{white2014asymptotic}
H.~White.
\newblock {\em Asymptotic Theory for Econometricians}.
\newblock Academic Press, 2014.

\bibitem{watanabe2009algebraic}
S.~Watanabe.
\newblock {\em Algebraic Geometry and Statistical Learning Theory}, volume~25.
\newblock Cambridge University Press, 2009.

\bibitem{heyde1974central}
C.~C. Heyde.
\newblock On the central limit theorem for stationary processes.
\newblock {\em Zeitschrift f{\"u}r Wahrscheinlichkeitstheorie und Verwandte
  Gebiete}, 30(4):315--320, 1974.

\bibitem{atiya2000new}
A.~F. Atiya and A.~G. Parlos.
\newblock New results on recurrent network training: unifying the algorithms
  and accelerating convergence.
\newblock {\em IEEE Transactions on Neural Networks}, 11(3):697--709, 2000.

\end{thebibliography}


% \bibliographystyle{unsrt}
% \bibliographystyle{abbrv}
% \bibliography{biblio}

\end{document}