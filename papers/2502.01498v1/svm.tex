\section{Printed Bespoke Sequential SVMs}
\label{sec:svm}
\begin{figure}[!t]
    \centering
    \includegraphics[width=\columnwidth]{figures/sequential_svm-idx.pdf} \vspace{-4ex}
    \caption{\blue{Overview of our proposed sequential \gls{svm} circuits.
    They consist of three main components memory, SVM compute engine and control unit.}}
    \label{fig:architecture}\vspace{-3ex}
\end{figure}

\gls{svm} is a supervised learning algorithm able to classify data by determining the optimal hyperplane for separating classes in a high-dimensional feature space.
\glspl{svm} are effective in handling small, high-dimensional datasets, offering robustness against overfitting.
In brief, \glspl{svm} compute a number of support vectors
%, one for each pair of classes,
and based on the obtained results, determine the class with the most wins.
In printed hardware, linear kernels are typically preferred within support vectors, to maximize area efficiency.
Existing implementations~\cite{Mubarik:MICRO:2020:printedml, Armeniakos:TCAD2023:cross} design fully parallel bespoke circuits, where model parameters are hardwired into the circuit implementation, eliminating the need for costly sequential elements in \gls{pe}, but requiring a hardware multiplier for each \gls{svm} weight.
In contrast, our work focuses on designing sequential \glspl{svm} to minimize the required arithmetic units, making it crucial, nevertheless, to optimize the cost of memory and registers.
Notably, in CMOS technologies, a D Flip-flop's area-equivalent is $4$ NAND gates, whereas in EGFET, this number rises to $6$--a $50$\% increase.

An abstract overview of our proposed sequential SVM circuit is shown in~\cref{fig:architecture}.
It consists of three main components, i.e., parameter storage, control, and compute, all working in sync to fold the entire \gls{svm} prediction over a single \gls{mac} unit. 
Each iteration involves fetching the respective support vector from memory (as determined by the control unit) and computing it in the support vector engine (a multi-cycle operation over a single \gls{mac}).
The result is fed back to the control unit, which either advances the computation (intermediate step) or selects the winning class (final step).
Importantly, our design choices for implementing our sequential \gls{svm} circuits target area efficiency.
Nevertheless, since power consumption in EGFET circuits is mostly static and internal (short-circuit), minimizing area also effectively reduces power.

\subsection{SVM Training} \label{sec:train}
Two established strategies for SVM multi-class classification are OvO and OvA (One-vs-All)~\cite{bishop2006pattern:ovo_ova}.
In OvO, each binary classifier distinguishes between two classes, requiring $\frac{n(n-1)}{2}$ classifiers for $n$ classes.
OvA uses $n$ classifiers, each separating one class from the rest.
OvO requires storing more support vectors but works with smaller subsets of training data, avoiding accuracy loss from imbalanced datasets.
Prioritizing high accuracy and relying on the inherent area efficiency of our sequential implementation, we choose OvO.
For the examined datasets (Section~\ref{sec:eval}), using quantized support vectors, OvO delivers $8.7$\% higher accuracy, on average, compared to OvA.


%Training is performed using scikit-learn's LinearSVC class, with randomized parameter optimization, to train a classifier for each possible class pair.
%Inputs are normalized to $[0,1]$, and training/testing is done with a random $80$\%/$20$\% split.
Training is performed using scikit-learn's LinearSVC class to train a classifier (support vector) for each possible class pair.
Inputs are truncated to $4$-bit fixed-point, and post-training, weights and biases are quantized with min-max linear scaling to the lowest precision that results in negligible accuracy loss (within $0.5$\%).
Additionally, SVM inference is profiled to determine the minimum precision needed for the support vector engineâ€™s accumulator.
The Verilog description for each trained SVM is automatically generated using code templates.
The extracted precisions and SVM parameters are stored in a configuration file.
The support vector engine is the same, with only precision adjustments, while a unique control unit is generated for each model.

\subsection{Support Vector Storage}\label{sec:memory}
Sequential architectures must store model parameters in memory (weights and biases in our \glspl{svm}).
A printed \gls{rom} is a prominent choice for this.
A compact \gls{rom} was proposed and fabricated in~\cite{Bleier:ISCA:2020:printedml}, outperforming other state-of-the-art designs.
It uses a crossbar architecture, where cross-points are shorted by printing conductive material (such as PEDOT:PSS) to represent bit values.
By varying the geometry of the conductive material, multi-bit values can be stored in a single printed dot.
An analog-to-digital converter (ADC) is needed to read stored values.
Considering the cost of ADCs and ROM cells in~\cite{Bleier:ISCA:2020:printedml}, we deduce that storing \mbox{$2$-bit} values is optimal for reading $2$-bit to $8$-bit words as for our model parameters.
This assumes the use of one to four \mbox{$2$-bit} ADCs to maintain reasonable performance.
Since our design uses a single-MAC \gls{svm} engine, we need to process only a single model parameter (weight or bias) per cycle.
One support vector is stored per crossbar row.
Based on the selected support vector (from the control unit), the respective row is activated. 
Then, a set of columns, depending on the precision of the model parameters, are activated each cycle.
A counter (see Section~\ref{sec:engine}) that selects the appropriate parameter is used as the column index.
Nevertheless, such an architecture is highly vulnerable to printing variations, which can alter stored parameters and lead to significant accuracy degradation~\cite{Zhao:DATE2023}. 

As a more robust alternative, we propose the use of bespoke MUXs.
The MUX input data are hardwired to the model parameters, with the row and column indexes described above serving as the input select signals.
While this approach offers improved robustness, it is less dense than ROM storage.
For example, for the Pendigits dataset (see Section\ref{sec:eval}) that features $45$ support vectors with $18$ $8$-bit parameters each, the area, power, and latency of the ROM are $2.4$cm$^2$, $1.9$mW, and $15$ms, respectively.
The corresponding values for MUX-based storage are $5.0$cm$^2$, $5.6$mW, and $19$ms, respectively.
Although less hardware-efficient, the overhead of the MUX-based storage is not prohibitive, especially considering that this example represents the worst-case difference among all datasets examined.
Therefore, we choose bespoke MUX-based storage for our SVMs to prioritize accuracy and robustness.



% \noindent\textbf{MAC-Based Computational Engine:}
\subsection{Single MAC Support Vector Engine} \label{sec:engine}
Our control unit makes decisions by evaluating one support vector at a time.
Hence, we design a support vector engine to compute the corresponding output:
\begin{equation}
    y = \begin{cases}
        1 & \text{if}\, \sum_{i=1}^{m} w_i x_i + b \geq 0 \\
        0 & \text{otherwise,}
    \end{cases}
\label{eq:compute_mac}
\end{equation}
where $w_i$ and $b$ are the support vector's weights and bias, and $x_i$ are the input activations.
Striving for area efficiency, we fold the computation of \eqref{eq:compute_mac} over a single MAC unit.

As shown in \cref{fig:architecture}, a register stores the accumulation result, whose size is minimized based on partial sum profiling during SVM training.
A second small register is used to store a counter, which serves as the column index for the memory and for synchronization.
In the first cycle, the bias is fetched, initializing the accumulation register.
In each subsequent cycle, a weight is read from memory, multiplied by the respective input, and the product is accumulated.
Once all model parameters are processed, the engine outputs a ready signal, providing the inverted sign of the accumulation.

\begin{figure}[!t]
    \centering
    % \includegraphics[width=\columnwidth]{example-image-b}
    \includegraphics[width=.9\columnwidth]{figures/sequential_svm_example-cropped.pdf}
    \caption{Example execution of our sequential \glspl{svm}. $4$ classes and $6$ input features/weights are considered. Support vectors corresponding to selected nodes are shown on the right, along with the classification output of the support vector engine. The output class is predicted in $3\times 7=21$ cycles. Fixed point values are represented by integers.
    }
    \label{fig:example}\vspace{-4ex}
\end{figure}


% \noindent\textbf{Control Unit:}
\subsection{Control Unit} \label{sec:control_unit}
OvO computes a support vector for each pair of classes, with the class attaining the highest score being selected as the SVM output.
In~\cite{Armeniakos:DATE2022:axml,Armeniakos:TCAD2023:cross}, all support vectors are computed in parallel, and a voter circuit determines the winning class.
While this approach requires a voter, a sequential implementation also requires a log$_2\frac{n(n-1)}{2}$ bit counter and additional registers to track the winning class and its score.
To minimize register use, we implement OvO as a decision-directed acyclic graph (DDAG)~\cite{ovo}--specifically, a binary decision tree--for our control \gls{fsm}, requiring only a small register of log$_2\frac{n(n-1)}{2}$ bits to store the FSM state.
%A software implementation of OvO as a DDAG is detailed in~\cite{ovo}, where each state represents the current winning class.
Each DDAG node (FSM state) represents the current winning class, and is linked to a support vector that compares this class against a new class, yet to be considered.
FSM selects the corresponding support vector by its row index in the memory.
%According to its row index, the support vector is fetched from memory.
After $m+1$ cycles (where $m$ is the number of input features), the support vector is computed in our engine, and the winner of this comparison is determined.
The newly identified winning class becomes the current winner, prompting the DDAG to transition to the right if the new class prevails, or to the left otherwise.

The hardware implementation of the FSM is straightforward.
It uses a hardcoded row index per state and only requires a MUX to select between two predefined next states based on the support vector engine's output.
\cref{fig:example} presents a detailed example of our sequential execution and FSM flow.
The FSM has $\frac{n(n-1)}{2}$ states (the number of OvO support vectors) but requires only $n-1$ support vector evaluations, resulting in a total of $(n-1) \times (m+1)$ cycles for each classification.


% The binary decision tree is implemented via a lightweight \gls{fsm} for our control unit.
% Each node of the tree corresponds to a binary classifier, distinguishing between two selected classes.
% The binary decision is produced after $m+1$ cycles within our computational engine, as described in \cref{eq:compute_mac}.
% Given the predicted binary class at a given node, the decision tree progresses left or right, traversing through a single path and multiple support vectors, until it reaches a leaf node, where the final class decision is made.
% We observe that our approach does not necessitate a costly voting circuit at its output.
% Overall, for $n$-class classification, only $n$ iterations are required to compute the output class.
% By leveraging the lightweight binary classifiers and their small associated kernels from the OvO approach, this hierarchical structure minimizes both area and power consumption, which are key requirements in printed devices.




% An example of the algorithmic flow of our bespoke sequential \gls{svm} implementation and its \gls{fsm}-based control unit can be seen in \cref{fig:example}.
% Assuming $5$-class classification and $6$ input features (weights) per input (support) vector, \cref{fig:example} presents a step-by-step walkthrough of each stage of our tree-based \gls{svm}.
% The root node fetches the $S_{1,5}$ classifier from our \gls{mux}-based memory to distinguish between classes $1$ and $5$.
% The computational engine produces a positive sum, indicating that class $5$ prevails, signaling a move on the right towards lower levels of the tree.
% A path is formed as each binary classification produces a dominant class, to finally reach at the leaf nodes.
% Without the need for any voting mechanism, class $4$ is predicted after a total of $n\times m=30$ cycles.
% \cref{fig:example} showcases the simplicity and effectiveness of our sequential \gls{svm} implementation in correctly classifiying input data.
