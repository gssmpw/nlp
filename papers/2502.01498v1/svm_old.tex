\section{Printed Bespoke Sequential SVMs}
\label{sec:svm}
\begin{figure}[!t]
    \centering
    \includegraphics[width=\columnwidth]{figures/architecture.png}
    % \includegraphics[width=\columnwidth]{figures/sequential_svm.drawio-cropped.pdf}
    \caption{Schematic overview of our proposed bespoke sequential \gls{svm} circuit architecture.}
    \label{fig:architecture}
\end{figure}

In this section, we present our methodology for designing printed sequential \glspl{svm} for \gls{ml} classification.
% Generic intro to SVMs
\gls{svm} is a supervised learning algorithm able to classify data by determining the optimal hyperplane for separating classes in a high-dimensional feature space.
\glspl{svm} are effective for handling small, high-dimensional datasets, offering robustness against overfitting, and their use of kernel functions, makes them suitable for bespoke printed circuit implementations.

Typically in printed hardware, \glspl{svm} bespoke circuits follow a fully-parallel architecture~\cite{Mubarik:MICRO:2020:printedml, Armeniakos:DATE2022:axml}.
They comprise of a number of multipliers, equal to the number of input features, responsible for multiplying input features with trained coefficients/weights.
Multiplication results are then added and allocated to the nearest class using comparators and encoders~\cite{Mubarik:MICRO:2020:printedml}.
As a result, fully-parallel architectures employ one \gls{mac} unit for each \gls{mac} operation.
Both input features and weights are stored in registers.

In our work, we follow a novel, alternative direction and propose printed bespoke sequential \glspl{svm} architectures, targeting to elevate their area efficiency by designing them in finer granularity.
An overview of our proposed design can be seen in \cref{fig:architecture}.
It consists of three separate units, for memory, control and compute, all working in sync to orchestrate fetching and storing of support vectors, and the efficient calculation of output classes over multiple steps.
Each iteration involves fetching a support vector from memory (as determined by the control unit), and performing a \gls{mac} operation in the compute unit, which either advances the output calculation (intermediate step) or results in the selection of the winning class (final step).
Importantly, the design choices for implementing our designs are taken with area efficiency serving as the guiding principle, as will be explained below.
Overall, our sequential \gls{svm} architectures are designed to adhere to strict area and power constraints imposed by \gls{pe}, by far surpassing the efficiency of parallel models.


% \noindent\textbf{Support Vector \& Weight Storage:}
\subsection{Support Vector \& Weight Storage}
\label{sec:memory}
Sequential architectures face the challenge of storing input features and weights in memory, which is absent in parallel implementations.
The most prominent choices for our memory unit are registers, memristor-based \acrshortpl{rom} or \gls{mux}-based units.
In order to determine the optimal design choice, we conducted a thorough evaluation for each option, assessing their area and power consumption.
\TODO{Evaluation of memory units?}
We observe, \red{\dots XX}.
Thus, we conclude that using \glspl{mux} for data storage is the more efficient choice.
In each iteration, our control unit broadcasts a request for a given support vector from the memory unit.
As we describe below, our support vectors are essentially binary classifiers, trained to distinguish between a selected pair of classes.
After providing the correct memory address, the requested support vector is extracted from the \gls{mux}, to be then forwarded to the computational engine.
Note, all data is stored in low-precision fixed-point arithmetic, complying with \gls{pe} standards.




% \noindent\textbf{MAC-Based Computational Engine:}
\subsection{MAC-Based Computational Engine}
\label{sec:compute}
Striving for ultra-low area and power consumption, we aim to reduce the area footprint of our computational unit, which is tasked with performing cumbersome \gls{mac} operations.
Specifically, we fold the computations of our entire \gls{svm} model over a single \gls{mac} unit, exploiting hardware re-use to its maximum potential and significantly simplifying the hardware implementation of our bespoke circuits.

Our computational engine receives a chosen support vector as input, along with the input features vector, both of equal size.
In each cycle, one feature and weight are propagated to the \gls{mac} unit in order to compute the dot product of the two vectors.
The output $y$ of our computational unit at any given cycle is described by the following formula:
\begin{equation}
    y = \begin{cases}
        1 & \text{if}\, \sum_{i=1}^{m} w_i x_i \geq 0 \\
        0 & \text{otherwise,}
    \end{cases}
\label{eq:compute_mac}
\end{equation}
where $w_i$ are the support vector' weights and $x_i$ are the input activations ($m$ in total).
As seen in \cref{eq:compute_mac}, the computational procedure lasts for $m$ cycles ($m+1$ accounting for the bias).
Once an auxiliary counter signals that all products are formed and accumulated, the sign of the generated sum is checked (i.e., its MSB).
The sign value directly dictates the predicted class of the given input vector, according to the evaluated support vector, i.e., binary classifier.
Ultimately, the binary output of our computational engine determines the subsequent steps towards predicting the output class within our control unit.


% \noindent\textbf{Control Unit:}
\subsection{Control Unit}
\label{sec:control_unit}
\TODO{This can be reduced if it is common knowledge}

In the process of designing a control unit for an \gls{svm}-based system with constrained resources, such as \gls{pe}-based systems, two conventional options stand out as more prominent for multi-class classification: the One-vs-One (OvO) and One-vs-All (OvA) strategies~\cite{bishop2006pattern:ovo_ova}.
In the OvO approach, each binary classifier distinguishes between two classes, resulting in \( \frac{n(n-1)}{2} \) classifiers for $n$ classes, while the OvA method uses $n$ classifiers, each separating one class from the others.
As observed, OvO involves training more classifiers, however it deals with smaller subsets of the training data, as only two classes are considered per classifier, and thus avoids accuracy degradation due to imbalanced datasets.
While these methods are effective in standard multi-class classification tasks, kernel-based methods such as \glspl{svm} strongly favor the OvO strategy, since they scale poorly in proportion to training dataset size.
The kernel matrices formed with OvO are also much smaller, and therefore faster and more efficient to train.
This is particularly advantageous for \gls{pe}, since area efficiency is of paramount importance when deploying support vectors in hardware.

For these reasons, we opt for OvO as our \gls{svm} decision-making strategy and control unit, and improve it in terms of area efficiency. 
Specifically, we implement a hierarchical decision-tree-based \gls{svm} approach, which alleviates the need for a voting mechanism, thus further simplifying the implementation.
The binary decision tree is implemented via a lightweight \gls{fsm} for our control unit.
Each node of the tree corresponds to a binary classifier, distinguishing between two selected classes.
The binary decision is produced after $m+1$ cycles within our computational engine, as described in \cref{eq:compute_mac}.
Given the predicted binary class at a given node, the decision tree progresses left or right, traversing through a single path and multiple support vectors, until it reaches a leaf node, where the final class decision is made.
We observe that our approach does not necessitate a costly voting circuit at its output.
Overall, for $n$-class classification, only $n$ iterations are required to compute the output class.
By leveraging the lightweight binary classifiers and their small associated kernels from the OvO approach, this hierarchical structure minimizes both area and power consumption, which are key requirements in printed devices.



\begin{figure}[!t]
    \centering
    % \includegraphics[width=\columnwidth]{example-image-b}
    \includegraphics[width=\columnwidth]{figures/sequential_svm_example-cropped.pdf}
    \caption{Example of the algorithmic flow of our bespoke sequential \glspl{svm} and their \gls{fsm}-based control unit. $5$ classes and $6$ input features/weights are considered. Support vectors corresponding to selected nodes are shown on the right, along with the binary classification outcome from the computational engine. The output class is predicted $n\times m=30$ cycles, without redundant voting mechanisms.
    }
    \label{fig:example}
\end{figure}
An example of the algorithmic flow of our bespoke sequential \gls{svm} implementation and its \gls{fsm}-based control unit can be seen in \cref{fig:example}.
Assuming $5$-class classification and $6$ input features (weights) per input (support) vector, \cref{fig:example} presents a step-by-step walkthrough of each stage of our tree-based \gls{svm}.
The root node fetches the $S_{1,5}$ classifier from our \gls{mux}-based memory to distinguish between classes $1$ and $5$.
The computational engine produces a positive sum, indicating that class $5$ prevails, signaling a move on the right towards lower levels of the tree.
A path is formed as each binary classification produces a dominant class, to finally reach at the leaf nodes.
Without the need for any voting mechanism, class $4$ is predicted after a total of $n\times m=30$ cycles.
\cref{fig:example} showcases the simplicity and effectiveness of our sequential \gls{svm} implementation in correctly classifiying input data.