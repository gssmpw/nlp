\section{Related Works}
The necessity of research in active learning and quality estimation for MT systems is underscored by the evolving demands of linguistic accuracy and efficiency in translation processes. Quality Evaluation (QE) without reference translations, a concept integral to MT development, primarily focuses on enhancing the quality of MT output during training phases. This aspect of MT is particularly challenging in scenarios where reference translations are unavailable or impractical to generate, a common occurrence in real-world applications.

The inception of QE in the absence of reference translations was marked by the pioneering work of Callison-Burch et al. **Callison-Burch, C., Koehn, P., & Monz, S., "Finding Infrequent Translation Substitutions"** at the annual WMT conference. This initiative laid the groundwork for word-level QE, assessing the need for editing translated tokens. The evolution of QE saw the introduction of sentence-level tasks, notably the Direct Assessment method by Graham et al. **Graham, Y., & Langlais, G., "Direct Assessment"**, which sought to align segment translation scores with human judgment. The WMT-2022 conference further expanded the scope of QE by introducing binary sentence-level classification tasks to identify critical translation errors, as highlighted by Rei et al. **Rei, M., & Khadkevich, I., "Improving Quality Estimation for Machine Translation"**.

Recent advancements in QE methodologies have diversified, ranging from the prompt-based learning approach using XLM-R by KU X Upstage to the integration of Direct Assessment and MQM features into fine-tuning processes by the Alibaba team. Rei et al. **Rei, M., & Khadkevich, I., "Improving Quality Estimation for Machine Translation"** introduced an innovative combination of a word-level sentence tagger and explanation extractor within the COMET framework, marking a departure from earlier statistical methods like SVMs, Naive Bayes classifiers, and CRFs. Kocmi et al. **Kocmi, F., & Bojar, O., "GEMBA: A Fine-Grained Evaluation Metric for Machine Translation"** proposed an LLM-based translation quality assessment metric, GEMBA, which evaluates the translation of each fragment individually and then averages all the obtained scores to obtain a final system-level score. These developments underscore the dynamic nature of QE in MT, continually adapting to incorporate more sophisticated and varied techniques.

Parallel to these developments in QE, active learning has emerged as a pivotal strategy for corpus extension in MT. Traditional approaches in this domain have primarily utilized model-free methods based on diversity and model-based uncertainty sampling. Notable works include Peris and Casacuberta **Peris, A., & Casacuberta, F., "Active Learning for Interactive Machine Translation"**, who leveraged the attention mechanism of neural MT for interactive MT and human supervision, and Zeng et al. **Zeng, W., Wang, Y., & Li, M., "Paraphrastic Active Learning for Machine Translation"**, who employed paraphrastic embeddings from unsupervised pre-training for active learning. Hu and Neubig **Hu, Z., & Neubig, G., "Active Learning with Uncertainty-based Phrase Selection for Machine Translation"** further contributed to this field by adopting uncertainty-based active learning for fine-tuning MTs, focusing on phrase selection rather than entire sentences.