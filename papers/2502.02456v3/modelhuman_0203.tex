% 
% Annual Cognitive Science Conference
% Sample LaTeX Paper -- Proceedings Format
% 

% Original : Ashwin Ram (ashwin@cc.gatech.edu)       04/01/1994
% Modified : Johanna Moore (jmoore@cs.pitt.edu)      03/17/1995
% Modified : David Noelle (noelle@ucsd.edu)          03/15/1996
% Modified : Pat Langley (langley@cs.stanford.edu)   01/26/1997
% Latex2e corrections by Ramin Charles Nakisa        01/28/1997 
% Modified : Tina Eliassi-Rad (eliassi@cs.wisc.edu)  01/31/1998
% Modified : Trisha Yannuzzi (trisha@ircs.upenn.edu) 12/28/1999 (in process)
% Modified : Mary Ellen Foster (M.E.Foster@ed.ac.uk) 12/11/2000
% Modified : Ken Forbus                              01/23/2004
% Modified : Eli M. Silk (esilk@pitt.edu)            05/24/2005
% Modified : Niels Taatgen (taatgen@cmu.edu)         10/24/2006
% Modified : David Noelle (dnoelle@ucmerced.edu)     11/19/2014
% Modified : Roger Levy (rplevy@mit.edu)     12/31/2018



%% Change "letterpaper" in the following line to "a4paper" if you must.

\documentclass[10pt,letterpaper]{article}

\usepackage{cogsci}
\usepackage{booktabs}
\cogscifinalcopy % Uncomment this line for the final submission 


\usepackage{pslatex}
\usepackage{apacite}
\usepackage{float} % Roger Levy added this and changed figure/table
                   % placement to [H] for conformity to Word template,
                   % though floating tables and figures to top is
                   % still generally recommended!
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{enumitem}

%\usepackage[none]{hyphenat} % Sometimes it can be useful to turn off
%hyphenation for purposes such as spell checking of the resulting
%PDF.  Uncomment this block to turn off hyphenation.


%\setlength\titlebox{4.5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 4.5cm (the original size).
%%If you do, we reserve the right to require you to change it back in
%%the camera-ready version, which could interfere with the timely
%%appearance of your paper in the Proceedings.

\title{Model Human Learners:\\Computational Models to Guide Instructional Design}
% \title{A Computational Model of Human Learning from Tutoring}
 
\author{{\large \bf Christopher J. MacLellan (cmaclell@gatech.edu)} \\
  School of Interactive Computing\\
  Georgia Institute of Technology\\Atlanta, GA 30308 USA}

\begin{document}

\maketitle

\begin{abstract}
Instructional designers face an overwhelming array of design choices, making it challenging to identify the most effective interventions.
To address this issue, I propose the concept of a {\it Model Human Learner}, a unified computational model of learning that can aid designers in evaluating candidate interventions.
This paper presents the first successful demonstration of this concept, showing that a computational model can accurately predict the outcomes of two human A/B experiments---one testing a problem sequencing intervention and the other testing an item design intervention.
It also demonstrates that such a model can generate learning curves without requiring human data and provide theoretical insights into why an instructional intervention is effective.
These findings lay the groundwork for future Model Human Learners that integrate cognitive and learning theories to support instructional design across diverse tasks and interventions.

%This paper explores this idea in the context of designing an intelligent tutor for fraction arithmetic, demonstrating that a simulated student model can predict human A/B experimental study outcomes and learning curve trends. 
%I also explore the model’s account for why a tutor design that interleaves fraction practice is better than one that blocks practice—interleaving leads to more student mistakes that counter-intuitively provide the data students need to induce the correct knowledge.
%The paper concludes with potential extensions to the proposed Model Human Learner, highlighting how recent AI and ML advancements enable exciting new possibilities.

\textbf{Keywords:} 
Artificial Intelligence; Education; Learning; Skill acquisition and learning; Symbolic computational modeling
\end{abstract}


\section{Introduction}
In their seminal paper, \citeA{Card:1986vx} propose the concept of a {\it Model Human Processor}, a unified information-processing model that codifies psychological theories to predict human performance in user interfaces.
They argue that interface designers could use this kind of model to make more informed design decisions that improve usability.
In this paper, I propose an analogous concept: a {\it Model Human Learner}, a unified computational model that codifies cognitive and learning theories to predict human learning in instructional systems. 
I argue that instructional designers could use this kind of model to make more informed instructional design decisions that improve pedagogical effectiveness.

An analysis by \citeA{Koedinger:2013hp} suggests that instructional designers face a crisis of choice---design spaces are combinatorial, containing trillions of possible configurations.
How can instructional designer determine the best choices?
Currently, they rely on A/B experiments to navigate this vast space, but human studies are costly and time consuming.
Moreover, each A/B experiment provides only a single bit of information, meaning instructional designers are essentially playing a losing game of 20 questions with nature \cite{Newell:1973tt}. 

Computational models of learning, such as those proposed by \citeA{Maclellan:2017thesis} and \citeA{weitekamp2023computational}, offer a potential solution.
Unlike abstract, mathematical models of learning, such as the Additive Factors Model \cite{Cen:2009ve} or Bayesian Knowledge Tracing \cite{Corbett:1994ux}, which fit functions to performance data, computational models of learning are {\it mechanistic}. 
Similar to cognitive architectures \cite{Langley:2008ka}, these models use artificial intelligence and machine learning to simulate how knowledge is updated in response to practice and how performance evolves over time.
I argue that these models can realize the Model Human Learner concept, letting instructional designers simulate A/B experiments to test alternative interventions and identify the most promising ones before conducting costly human studies.

Prior research has explored several applications of computational models of learning.
For example, \citeA{Li:2013vd} investigated their use for discovering cognitive models, while
\citeA{Matsuda:2011wf} explored how they can promote ``learning-by-teaching,'' where students learn by instructing simulated students.
\citeA{maclellan2022domain} and \citeA{weitekamp2020interaction} examined their applications for tutor authoring.
Lastly, others have studied their utility for theory testing \cite{rachatasumrit2023content,Lee:2009ty,NanLi:2010wi}.

More recently, researchers have proposed using these models to predict how different instructional design choices will impact learning \cite{MacLellan:2016tqa,maclellan2023optimizing}.
While these studies have generated reasonable predictions about experimental effects, {\bf none have yet been compared with human outcomes.}
This critical next step is the focus of this paper.
Specifically, I apply a computational model of learning to predict the outcomes of two human A/B experiments. By comparing these predictions to previously collected human data, I demonstrate that computational models of learning can:
\begin{itemize}[itemsep=-3pt, topsep=3pt]
    \item Successfully predict the main effects of two human experiments---one evaluating a problem sequencing intervention and the other testing an item design intervention;
    \item Generate learning curve predictions that closely align with human learning curve trends, without training on human data first; and
    \item Provide theoretical insights into why specific interventions work, challenging a prior  hypothesis by \citeA{Lee:2015gb} and suggesting a novel explanation.
\end{itemize}

% In the previous chapter, I showed that apprentice learner models can support tutor authoring. However, it is still difficult for developers to build tutoring systems that are effective for learning. Fortunately, one branch of Educational Data Mining (EDM) research leverages data to improve our theoretical understanding of how people learn \cite{Romero:2010du,Baker:2014de} and this developers can leverage this understanding to guide tutor design. Analogous to how data from the Large Hadron Collider, which was motivated by a strong theory, can be used to gain insights into physical laws, educational data can provide insights into the mechanisms underlying student learning. Although early EDM work explores this idea \cite{Anderson:1989kg,Ur:1995wr}, recent EDM research trends center on using statistical methods to discover the domain skills students learn within tutors (i.e., knowledge-component models) and estimating which skills students know based on their performance (i.e., knowledge tracing) \cite{Baker:2014de}. While these research directions are important, the availability of educational data makes the EDM community well poised to contribute substantially towards our theoretical understanding of human learning. 

% Although many widely used predictive models of learning, such as Bayesian Knowledge Tracing \cite{Corbett:1994ux} and Additive Factors Model \cite{Cen:2006th}, rely on existing theories of human learning, such as the {\it chunking theory of learning} \cite{Newell:1981uk}, researchers rarely apply these techniques to educational data with the aim of improving psychological theory. Further, there are a number of barriers to using educational data for this purpose. First, many EDM approaches are only loose approximations of the theories on which they are based. For example, the Additive Factors Model predicts that improvements in human performance will follow a single logistic function, whereas many previous theories states that the improvements should follow a power function \cite{Heathcote:2000dz}. Second, EDM models do not reflect the current state of learning theory. For example, recent studies of skill acquisition actually suggest that improvements should follows three distinct power functions, one for each phase of cognitive skill acquisition \cite{Tenison:2015el}, rather than a single logistic function. This disconnect between theories and models makes it difficult to draw inferences about the underlying theories given the fit of models to data. By more tightly connecting EDM models to psychological theory, we can leverage educational data to improve our understanding of the mechanisms behind human learning and, in turn, use these theories to improve predictions of student behavior. 

%\begin{figure*}[t!]
%\begin{center}
%     \includegraphics[width=0.75\textwidth]{Images/theory-data-loop.png}
%  \caption{A depiction of how psychological theories, models, and behavior relate. Researchers use theories to generate models, which simulate behaviors that researchers compare to human behaviors. Researchers then use behavioral differences to inform future models and theories.}
%  \label{fig:theory-data-loop}
%  \end{center}
%\end{figure*}

% To more tightly link a theory to models, researchers can develop computational theories \cite{Newell:1973tt}, which describe the mechanisms that produce observed phenomena. Within this paradigm, a model presents as a specific implementation of these mechanisms that can be executed to simulate behavior, which then can be compared with observed behavior to test both the model and the underlying theory. The objective is not only to explain or ``fit'' relationships to observed data, but also, to predict relationships before data are observed. Figure \ref{fig:theory-data-loop} shows the iterative relationship between theories, models, and behaviors. I argue that this approach should be central to the field of EDM.

% In the current chapter, I demonstrate the use of apprentice learner models for theory-driven prediction of student behavior. Unlike prior models of student performance, such as Additive Factors Model and its variants, apprentice learner models explain the mechanisms students use to acquire new knowledge from instruction. This mechanistic description lets one simulate learner behavior within an instructional environment and use the results to predict human behavior. Rather than arriving at a general conclusion, say that students learn differently from positive and negative feedback, this approach lets one explore possible explanations for the mechanisms driving these results.

% Leveraging this capability, I use the Apprentice Learner Architecture to test two alternative theories of learning, in order to show how the architecture also supports theory development. The first, which corresponds roughly to \textsc{SimStudent}'s theory of learning \cite{Li:2014hr}, posits that humans learn when to apply skills {\it non-incrementally}; i.e., they relearn to classify the applicability of skills in light of all available training data. Although this theory seems implausible for lifelong learning, there is some evidence that, at least for the duration of a single equation solving tutoring session, it explains some errors that humans make \cite{Lee:2009ty}. In contrast, the second theory posits that humans learn the conditions for skill application {\it incrementally}; i.e., they update how they classify skills given only the most recent training datum. This second theory seems more psychologically plausible and better aligns with other computational theories of learning \cite{Feigenbaum:1984tp,Langley:1985wm,Fisher:1993vz}. 

% To test these theories, I instantiated two models within the architecture: the \textsc{Decision Tree} model and the \textsc{Trestle} model. The key difference is that the first uses a non-incremental decision-tree learner \cite{Quinlan:1986cv,scikit-learn} for when-learning, whereas the other uses Trestle \cite{MacLellan:2016ue}, an incremental categorization tree learner. In all other respects the models are identical---they have the same prior knowledge and implementations for the other components.\footnote{See chapter 3 for more details on these models.} This variation is reasonably simple and not particularly contentious, but prior models of human learning have adopted the assumption that humans learn when to apply skills non-incrementally \cite{Li:2014hr}, so it seemed like a good initial test for of the general research approach. 

% To evaluate these models and test the theories underlying them, I use them to predict human behavior in a fraction arithmetic tutor that manipulated the order students received problems \cite{Patel:ho4PITKp}. Given this predicted behavior, I assess which models are capable of predicting the main instructional effects and which have better agreement with the human behavior. Using this approach, I provide evidence to support two claims:

% \begin{enumerate}
% \item Both models predict the main experimental effects of the problem ordering manipulation.
% \item The incremental \textsc{Trestle} model better explains human behavior than the non-incremental \textsc{Decision Tree} model. 
% \end{enumerate}

% Given these initial computational models and my data-driven theory development approach (see Figure \ref{fig:theory-data-loop}), my ultimate goal is to develop a model that is consistent with available educational data sets, such as those found in DataShop \cite{Koedinger:2010tj} and other similar repositories. The development of such a model is possible under the assumption that differences in students' behavior are primarily due to differences in their experiences and prior knowledge \cite{Lee:2009ty}, rather than differences in the mechanisms they use for learning.\footnote{Under this view, individuals might have slightly different parameters for the same mechanisms; for example, individuals might have different thresholds for how widely or deeply they search when explaining an example.} 

% To pursue this goal, the general research program consists of generating different models of human learning using the Apprentice Learner Architecture, then connected them to the same intelligent tutoring systems that logged the data found on Datashop, and finally comparing the the behavior of these models in these tutors to human behavior. By analyzing differences between the models and humans, researchers can revise the underlying models and theories to reduce these differences and repeat the process. The following sections describe one iteration of this general research program, focusing on the fraction arithmetic tutor and one data set it collected. They describe how to stimulate student learning in this tutor and use simulation data to predict the human behavior. Additionally, they demonstrate the use of human data for testing which model (and theory) is better supported by the data. Finally, they discuss the implications of my results and potential directions for research. 

%\subsection{A/B Experiment Limitations}
% \subsection{The Model Human Processor}
% \subsubsection{CogTool as the actual instantiation}

%\section{Apprentice Learning}
%Humans learn new skills in a variety of ways. One format in which they regularly engage is {\it apprentice learning}, or learning from the examples and feedback provided by expert instructors. From the perspective of the learner, there are practical benefits to this approach over more formal schooling methods, such as lectures. In particular, prior studies of human apprenticeship find that it situates learned skills within their context of use, whereas lecture-based formats typically abstract context away---making it harder for students to later retrieve and apply learned skills in practice \cite{Collins:1tN8cWpe}. Additionally, this paradigm is often easier for instructors. Notably, skill knowledge is typically tacit and thus difficult to verbally communicate \cite{Clark:1996wh}. In contrast, apprenticeship facilitates a more natural transference of skills because demonstrating and coaching skills is often much easier for teachers than formalizing and describing them. 

%These benefits make apprentice learning a natural approach for skill transfer. Authoring tools equipped with this capability should let developers efficiently author tutors with examples and feedback, rather than having to formalize and program knowledge directly into a tutor, which is time consuming and requires programming expertise \cite{Matsuda:2014dd}. Additionally, tutoring systems utilize this format of instruction; i.e., they provide students with immediate correctness feedback on their problem-solving steps and examples\footnote{Examples are often referred to as {\it bottom-out hints} within the tutoring literature because they are usually provided to students as the final hint, in a sequence of hints, for a given problem-solving step.} when they are stuck \cite{VanLehn:2006wp}. Thus, developers should be able to use models of human learning to stimulate student interactions with their tutors, in order to test the pedagogical effectiveness of these systems before deployment.

%\subsection{The Problem}
%In order to build a model that supports tutor authoring and that simulates students, the first step is to formally define the learning problem. The starting point for my formulation is VanLehn's \citeyear{VanLehn:2006wp} characterization that apprentice learning within tutors consists primarily of two loops: an outer loop, in which an instructor selects problems for a student to attempt, and an inner loop, in which students attempt steps and instructors provide hints and feedback. Figure \ref{fig:interaction-patterns} further divides these behaviors into five distinct interactions between the tutor and tutee. The first interaction (I1) corresponds to the outer loop, in which a tutor selects steps for a tutee to perform. Note that \citeA{VanLehn:2006wp} originally characterized the outer loop as selecting problems, not steps, but the current characterization is slightly more general. In particular, a tutor that selects steps can achieve identical behavior to a tutor that select problems---by always selecting the successive steps within a problem. However, it also allows for tutors that can assign students to work on specific steps within problems, potentially skipping some steps. The other four interactions (I2a, I2b, I3a, and I3b) correspond to the inner loop. In particular, in response to receiving a step, the student can either attempt it (I2a) or request an example of how to perform it correctly (I2b). The tutor then either provides feedback on the student's attempt (I3a) or an example (I3b), depending on the student's action. At the end of this sequence (I1 $\rightarrow$ I2 $\rightarrow$ I3), the tutor typically begins interaction again by providing the student with a new step (e.g., if they got the last step correct) or the same step again (e.g., if their attempt was incorrect and the tutor wants to give them another chance). 


%\begin{figure}[t!]
%\begin{center}
%     \includegraphics[width=0.4\textwidth]{Images/interaction_patterns.png}
%  \caption{The five apprentice-learning interactions. First the tutor provides the learner with a step to attempt (I1). In response the learner either attempts to solve the step (I2a) or requests an demonstration (I2b). If the learner makes an attempt, then the tutor provides feedback on its correctness (I3a). In the case of a an example request, the tutor provides a demonstration of the step (I3b). Note, the solid lines denote actions performed by the tutor and the dashed lines denote actions performed by the tutee.}
%  \label{fig:interaction-patterns}
%  \end{center}
%\end{figure}

%A key characteristic of these interactions is that they frame apprentice learning as an {\it incremental} and {\it interactive} process. In other words, learners are given training examples and feedback sequentially over the course of their problem solving, rather than in a single batch up front. Further, the steps, feedback, and examples they receive are chosen by a tutor in response to their actions. For example, instructors only provide feedback on the steps a student actually attempts, and, ideally, they select steps that challenge and advance a student's knowledge, rather than those that they already know how to solve. Together, these characteristics enable tutors to {\it personalize} each student's instruction---continuously assessing their skill knowledge and focusing attention on skills that have the most room for improvement. This personalization is hypothesized to be one of the key reasons that tutoring systems are so effective for human learning \cite{VanLehn:2006wp}.

%In order to build a model that supports these interactions, the next step is to define the structure of the information exchanged. During the first interaction, the learner is provided with a description of a step that the tutor has selected. Although many representations for this description are possible, the current work takes the stance that this description is provided in a {\it relational representation}, such as first-order logic. Figure \ref{fig:tutor-relational-representation} shows an example of a step in a Chinese character tutor and a relational description. 

%An important assumption of the current work is that the learner is not {\it explicitly} provided with problem-solving goals (i.e., a partial description of a desirable state).\footnote{More generally, goals are not modeled as explicit cognitive structure in this work, but future work should investigate how explicit goals can be incorporated into the current theoretical framework.} However, there is always an implicit goal during apprentice learning that the student should attempt to take actions that receive positive feedback and avoid actions that receive negative feedback. Additionally, information about what a learner should do (i.e., goal information) is typically embedded in the state description. For example, in Figure \ref{fig:tutor-relational-representation} the student is tasked with translating the provided Chinese character into English, and the accompanying state description contains this information. I make this assumption because novice students typically are not aware of explicit goals a priori, so they must get this information from the provided problem statement. Thus, in addition to learning new skills, students must learn the mappings between the information available in the state description and their learned skills.

%\begin{figure}[t!]
%\begin{center}
%     \includegraphics[width=.4\textwidth]{Images/tutor_relational_representation.png}
%  \caption{A visual representation of a step in the Chinese tutor and its accompanying relational description.}
%  \label{fig:tutor-relational-representation}
%  \end{center}
%\end{figure}

%Once a learner receives a description of a step to work on, they respond with either an attempt or a request for an example. When making an attempt, the learner specifies the action that should be taken within the tutor, which for the current work I represent using a Selection-Action-Input (SAI) triple \cite{Ritter:1996vb}. For example, given the step in Figure \ref{fig:tutor-relational-representation}, the student might respond with (sai f3 UpdateTextField ``small''), which specifies that interface element f3 should be updated with the text ``small'' (the correct English translation for ``\includegraphics[height=\fontcharht\font`\B]{Images/small_character.png}''). Typically, when the tutor receives an SAI, it checks its correctness\footnote{The current work assumes that correctness is Boolean, but in theory it could be a continuous value.} and responds with a Step-SAI-Feedback triple, which contains the relational description of the current step, the SAI from the student, and the correctness feedback on this SAI. In situations where the learner requests an example, the tutor also responds with an Step-SAI-Feedback triple, but the SAI is one that is generated by the tutor, and the feedback is marked as correct (assuming the tutor is providing a correct example). By specifying that the response from the tutor for both the feedback and examples has the same format (a Step-SAI-Feedback triple), the learner can process them in similar ways. In particular, receiving feedback on a correct step is analogous to receiving the same correct step as an example. Also, it is worth noting that even though the current work is theoretically committed to apprentice learning as an incremental and interactive process, the current formulation supports non-incremental and non-interactive instruction as well. In particular, an instructor could provide a student with multiple examples, either positive or negative, at any point within, or outside, of the typical interactive loops. 

%One final point that is worth discussing is that steps {\it are} the granularity at which tutors provide feedback, but step size is not necessarily consistent across tutors. At one extreme, tutors might present problems as a single step and only provide feedback on the final answers. For example, a tutor might present a student with the problem $3x + 2 = 11$ and only provide correctness feedback when they give the correct final answer ($x=3$). In this case, a student might need to take multiple intermediate, non-tutored, actions in order to produce the final answer. In this scenario, when a student gets feedback, they are faced with the {\it credit-assignment} problem \cite{Sleeman:1982ty}; more specifically, they need to determine which of their intermediate actions are responsible for their answer's correctness. At the other extreme, a tutor might present problems with steps for each intermediate problem-solving action and provide feedback on each. In this scenario, the student is effectively faced with a supervised learning problem and does not have to handle credit assignment.

% There is an added complication that what constitutes a single action might be different between learners. For example, updating the answer field with the word ``small'' in the step shown in Figure \ref{fig:tutor-relational-representation} might be a single action for most English students. However, for students less experienced with English keyboards, this might require multiple actions for inputting the individual letters. In general, \citeA{VanLehn:2006wp} hypothesizes that decreasing the grain size of steps is better for students learning because it reduces the amount of credit assignment that a student must perform. However, a super-fine grain size is inefficient because it requires students to process more feedback for domain-independent skills, such as typing, and draws attention and feedback away from domain skills, such as Chinese translation. In practice, most tutors assume basic prior knowledge for skills like typing and choose a step size that usually consists of one to two actions, depending on a student's prior knowledge. 

%\subsection{Prior Work}

%Given this formulation of the problem, I next turn to the literature and review prior models that conform to it. While there are a number of modeling efforts that fit at least some aspect of the current formulation, reviewing all of these works is beyond the scope of this thesis. Instead, I focus on orthogonal methods within the literature and discuss prototypical examples that leverage these approaches in order to situate the current work. It is important to note that many of these examples do not attempt to model {\it human} learning, but they all utilize apprentice learning in some form.

%The predominant view within the literature is that problem-solving consists of heuristic search through a problem space \cite{newell1972human}, which is typically defined using a STRIPS-like representation \cite{Fikes:1972wp}. Within this representation, a problem space is defined by:

%\begin{itemize}
%\item a state representation (i.e., relations and features) that describe possible states;
%\item operators, which present as IF-THEN rules for moving from one state to another; and
%\item goals, which can appear explicitly as partial descriptions of desirable states or implicitly as black-box tests for assessing whether a particular state achieves the goals.
%\end{itemize}

%\noindent Given this view, prior work characterizes apprentice learning as the translation of problem-solving traces (i.e., particular paths through a problem space) and feedback on them into knowledge of the structure of the underlying problem space and how best to search it. 

%Although most work can be unified under this characterization, the prior work differs on the source and format of the problem-solving traces used for learning. One common assumption is that complete traces---consisting of all state-operator pairs---are provided by experts in one batch upfront. Despite the restrictiveness of this requirement, many systems adopt it. For example, Abbeel and Ng \citeyear{Abbeel:2004gn} explore an approach for learning to solve a variety of common toy AI problems given anywhere from 100-100,000 complete expert traces for each problem up front. However, not all systems subscribe to these restrictions. In particular, some systems, such as LEAP \cite{Mitchell:1985ud} and ODYSSEUS \cite{Wilkins:1986to}, relax the constraint that all of the traces must be provided in one batch. Instead, these systems learn incrementally by observing human experts over the course of their normal problem solving. Other systems further relax the requirement that problem-solving traces must be complete. For example, ICARUS \cite{Li:2009ws} is capable of learning from traces that lack some of the state-operator pairs, some of the operators (i.e., that contain only the intermediate states), or both, by filling in the gaps using means-ends problem solving. Taken to the extreme, this problem-solving approach, and others like it \cite{Fikes:1972wp,Laird:1987up,Anderson:1993up}, enable systems to generate their own problem-solving traces.

% Most systems that get their traces from experts assume that the latter also provide the feedback on them. In particular, expert traces are usually implicitly assumed to be correct, although it is possible for experts to also explicitly label traces as positive or negative. When systems generate their own traces, however, there is some variation in the source of feedback. Systems that possess explicit descriptions of problem-solving goals often provide their own feedback by labeling traces that achieve the goals as correct and those that fail (e.g., dead ends or loops) as incorrect \cite{Langley:1985wm}. When systems lack explicit knowledge of goals, they must instead rely on implicit goal tests (i.e., experts or other black-box sources) to label generated traces. One challenge faced by these systems is that navigating a problem space without any knowledge of the goals can be difficult, if not impossible. To overcome this challenge, many systems \cite{Nason:2005cq,Martinez:2015hd,Tadepalli:2004wp} leverage feedback on earlier problem solving to direct search to more promising parts of the space.

% Despite variety in the origin and presentation of traces and feedback, once they are in hand the prior literature has coalesced around a number of methods for translating them into new problem-solving knowledge. In particular, methods exist for learning operator preference knowledge, learning heuristic operator conditions, learning composite operators, and even learning new primitive operators. I will review each in turn.

% One of the most common types of knowledge extracted from solution traces is operator preference knowledge, which describes the operators that should be preferred during search. In particular, at each state in the problem space, an agent must decide which of its operators to execute. If the agent is informed and makes good choices, then it will quickly achieve its goals. However, when an agent is uninformed, then it will spend time exploring unfruitful portions of the problem space and often fail to solve difficult problems in a reasonable amount of time. To combat this challenge, prior work explores how to leverage correct and incorrect traces to learn which operators are preferable in each state. In particular, most systems decompose traces into their constituent state-operator-feedback triples, which become training data for learning \cite{Sleeman:1982ty}. Systems using this approach typically label state-operators along successful paths as correct and those that lead to failures, or sometimes are just off path, as incorrect.

% Given these training data, one common type of learned knowledge is operator preference relations, which specify preferences over potential operators, such as which to accept or reject, which are good or bad, or which should be tried first (e.g., try Operator-1 before Operator-2). ELM \cite{Brazdil:1978uj} is an early example of a system that learns ordering relations by analyzing which operators occur before others in experts' algebra and arithmetic problem-solving traces. One limitation of this early approach was that learned relations were oblivious to state information, often resulting in contradictions (i.e., Operator-1 $>$ Operator-2 in some states, but Operator-1 $<$ Operator-2 in others). Other systems, such as Soar \cite{Laird:1987up}, correct this limitation by learning preferences (modeled as selection and rejection rules) that condition on the current situation (the current problem space, goal, operator, and state in Soar). Another common type of learned knowledge is operator preference functions, which specify numeric values (typically reward or cost) over states, operators, or state-operator pairs that determine the order to expand operators during search. This approach is commonly used in reinforcement-learning systems, such as the work of Abbeel and Ng \citeyear{Abbeel:2004gn}, which fits a reward function to state-operator-feedback triples extracted from expert traces, or Soar-RL \cite{Nason:2005cq}, which utilizes {\it Q-learning} to learn a reward function from its own problem solving.

% Another kind of knowledge commonly extracted from solution traces is heuristic conditions, which constrain, beyond their original, legality conditions, the number of situations in which operators apply---effectively reducing the branching factor of the search. For example, when playing chess, legality conditions on an agent's move operator express when they {\it can} move a piece. In contrast, the heuristic conditions express when they {\it should} move a piece. The acquisition of this type of knowledge is similar to that of preference knowledge in that solution traces are decomposed into constituent state-operator-feedback triples, which are then used for supervised learning. For example, given these triples, LEX \cite{Mitchell:1983ub} applies version-space learning and SAGE \cite{Langley:1985wm} applies discrimination learning to discover heuristics for a variety of domains, ranging from Towers of Hanoi to symbolic integration.

% Like preference knowledge, heuristic conditions direct search to more promising parts of the space. However, unlike preferences, they completely exclude operators from consideration at the onset, which tends to improve search efficiency because evaluating heuristic conditions is often much faster than computing preferences over all legally-applicable operators (and each of their bindings). A key limitation of heuristic conditions, though, is that they can sometimes be over constraining. In particular, incorrect heuristic conditions can make it impossible for systems to generate their own correct problem-solving traces. Some systems, such as SAGE, try to avoid this problem by biasing learning towards more general, rather than more specific, heuristic conditions, so that operators are only constrained when there is sufficient evidence for doing so. Additionally SAGE and other systems, such as ACT \cite{Anderson:1982tz}, retain more general rules for use when none of the more specific ones apply. This problem is also somewhat alleviated in circumstances where correct expert traces are available because the training data they contain can provide the basis for correcting overly-specific conditions.

% In addition to learning preference knowledge and heuristic conditions, which change how an agent searches a problem space, many researchers have investigated how agents might acquire new operators, which change the underlying structure of the space they search. Within this area, macro-operator learning, or learning operators composed from other operators, is one of the more common approaches. Systems that engage in this kind of learning compile sequences of operators that appear in successful problem-solving traces into new operators that can be used in subsequent search. Many systems, such as STRIPS \cite{Fikes:1972wp}, ODYSSEUS \cite{Wilkins:1986to}, and GENESIS \cite{Money:2017tc} utilize explanation-based learning to construct macro-operators from successful problem solving. These approaches analyze a given sequence of operators and determine the minimal conditions that must be met for successful execution of the entire sequence. Learned operators are more efficient to execute than their constituents because condition matching only needs to happen once up front, rather than once for each constituent. Other approaches, such as chunking in Soar \cite{Laird:1987up}, knowledge compilation in ACT \cite{Anderson:1982tz}, and hierarchical task network learning in ICARUS \cite{Li:2009ws}, use different approaches and representations, but ultimately serve a similar purpose of learning compositional operator structures. In general, learned macro-operators enable agents to take bigger steps across the problem space, essentially decreasing the search depth needed to find a solution. However, sometimes the cost of using new operators outweighs their benefit resulting in worse performance (e.g., new operators increase the branching factor of search and make pattern matching more expensive), something known as the {\it utility problem} \cite{Minton:1990bc}.

% One limitation of macro-operators learning is that it requires sufficient knowledge of primitive operators. Most systems assume all primitives are provided, but some researchers have explored techniques for directly acquiring new primitives from expert\footnote{Generally, it is difficult for an agent to learn primitives from its own problem-solving traces because it does not have the knowledge necessary to generate them in the first place, although it may be possible.} problem-solving traces. In general, the work in this areas centers on analyzing the differences between subsequent states in a trace in order to induce new operators. For example, ALEX \cite{Neves:1985ws} learns new primitive operators by searching for built-in symbol manipulating functions (e.g., insert-after or string-concatenate) that explain the differences between states. Once it finds functions to explain the differences---which constitute the THEN part of learned rules---it applies condition learning techniques to discover the IF part. It is important to note that this technique is analogous to macro-operator learning where the agent has primitive operators to represent built-in symbol manipulating functions; however, the approach is slightly different in that it maintains a clear distinction between domain operators and functions, similar to VanLehn's \citeyear{Vanlehn:1999tm} {\it overly-general} operators, and only falls back on the later when necessary. An alternative approach is to directly learn new functional mappings between states. For example, KnoMic \cite{vanLent:2001jf} learns new primitive air-combat operators by applying specific-to-general learning to the differences between states in expert traces (along with specific-to-general learning to acquire conditions on these operators). One limitation of this technique is that it requires complete expert traces that include information about which operators are being applied in each state, so training data can be appropriately grouped for learning. \textsc{CASCADE} \cite{Vanlehn:1999tm} overcomes this limitation by only learning primitive operators with effects that correspond exactly to observed differences between states (i.e., effects are not generalized). Expert operator annotations are not required in this case because all pairs of subsequent states that exhibit the same differences are assumed to be applications of the same primitive operator. \citeA{Vanlehn:1999tm} provides evidence that the combination of simple primitive operator learning with macro-operator learning explains much, although not all,\footnote{Humans regularly engage in reflective behaviors to test, debug, and optimize their learned rules, something \textsc{CASCADE} did not model.} of the learning observed in human physics students.

% Collectively, this prior research, which often centers on the specific learning subproblems, begins to form an overall picture of how an agent might engage in apprentice learning. In particular, an expert provides an a learner with problem-solving traces and feedback, the learner generates their own traces and feedback, or some combination of the two. Once a learner has these training data, they can learn four kinds of knowledge: 

%\begin{enumerate}
%\item primitive operators, which grow the problem space (increasing branching factor);
%\item macro-operators, which enable an agent to take bigger steps through the problem space (decreasing search depth, but increasing branching factor);
%\item heuristic conditions, which specify when an agent should, or should not, consider operators during search (decreasing the branching factor); and
%\item operator preferences, which guide search to more promising portions of the problem space (typically decreasing overall search).
%\end{enumerate}

%As these kinds of knowledge have complementary benefits for problem solving, many systems exist that learn some combination of these knowledge types. One relevant subset of this work centers on modeling students' buggy problem-solving knowledge \cite{Vanlehn:1983wf}. Although these systems do not model human learning, they uses apprentice learning to discover knowledge that explains human behavior. For example, \textsc{ACM} \cite{LangIey:1984uh} discovers heuristics that explain students' erroneous multi-column subtraction behavior. This system is provided with a collection of overly-general operators---operators that could hypothetically explain a student's behavior, but that only have minimal legality conditions, such as only type conditions on inputs---and traces of a student's incorrect problem solving. It then searches for additional heuristic conditions on the overly-general rules that constrain their application to the situations observed in the provided traces. These more constrained rules constitute a model of the buggy knowledge that generated the provided traces. Langley and Ohlsson \citeyear{LangIey:1984uh} show that this system is capable of discovering heuristics that account for many of the common subtraction bugs exhibited by human students.

% Another line of work centers on human learning. Prototypical examples in this line include \textsc{CASCADE} \cite{VanLehn:1991we} and \textsc{STEPS} \cite{Ur:1995wr}, which models students learning physics from self explanation and from a physics tutoring system. Both systems make a distinction between domain rules (for problem solving) and overly-general rules (for explaining provided traces, which are correct in this case) and use explanation-based learning to compile the latter into the former (i.e., they learn domain-specific macro-operators). However, there are some differences between the two systems. In particular, \textsc{CASCADE} can learn primitive operators with constant effects by studying examples, but \textsc{STEPS} cannot. They also differ in how they direct their problem solving; \textsc{CASCADE} uses preference knowledge it learns from examples, whereas \textsc{STEPS} learns heuristic conditions from its problem-solving traces.

% Another major system in this line of work is \textsc{SimStudent} \cite{Lee:2009ty,Matsuda:2011wf,Li:2013vd,MacLellan:2015tw}, which models human learning across multiple tutor domains. Like the other systems, it makes a distinction between functions (i.e., overly-general rules) and skills (i.e., domain rules), and it discovers new skills by explaining an expert's steps using functions and compiling these explanations into new skills (i.e., it learns domain-specific macro-operators, as in \textsc{CASCADE} and \textsc{STEPS}). \textsc{SimStudent} also learns conditions on these skills, but, unlike the other systems, it makes a distinction between learning retrieval patterns (relational conditions that specify how relevant elements relate to other elements in the state) and feature conditions (non-relational conditions over elements bound in these retrieval patterns). This distinction is both practical and theoretical in that separating relational and non-relational conditions lets it leverage specialized learning approaches for each and to employ separate learning biases for each (retrieval pattern learning is specific-to-general and feature condition learning is general-to-specific). 

% These systems each represent major progress towards modeling humans in apprentice learning contexts. However, they each perform only a subset of the four types of learning and they implement different approaches for each. What is needed is a theory that unifies the different types of learning and provides a means of testing which implementations best align with human behavior. Additionally, the generality of these prior systems remains unclear. In particular, how much prior knowledge needs to be authored to employ these systems in new domains? For example, \textsc{CASCADE} and \textsc{STEPS} focus mainly on physics, but do these models also explain behavior in language or engineering domains? If so, how much additional prior knowledge must a researcher (or teacher or domain expert) develop in order to apply them?

% \textsc{SimStudent} has been applied to multiple tutoring domains and, consequently, might have a better claim to generality. However, each \textsc{SimStudent} model possesses a specialized set of domain-specific prior knowledge, suggesting that a would-be user needs to author additional content for their domain. \citeA{Li:2013un} investigates how to automatically discover prior knowledge using unsupervised learning, but these approaches require access to training data in one batch upfront, which may not be available for novel domains. Given this limitation, it seems reasonable for researchers to apply these techniques to learn domain-general prior knowledge, such as how to parse English sentences or mathematical formulas, but it seems unlikely that teachers, or other non-technical domain experts, could apply them to authoring domain-specific knowledge. It seems more feasible for non-technical users to hand author the prior knowledge than to collect/clean-up domain-specific training data and apply unsupervised learning to them; although, this is an open empirical question. In contrast, while \textsc{CASCADE} and \textsc{STEPS} focus mainly on physics, they suggest that the combination of access to overly-general prior knowledge, heuristic condition learning, preference knowledge learning, macro-operator learning, and primitive operator learning (learning primitives with constant effects) might be sufficient to model human learning across a wide range of domains. I investigate these ideas in the current work.

\section{The Computational Model}
This study employs a computational model from the {\it Apprentice Learner Architecture} \cite{Maclellan:2017thesis,weitekamp2020interaction,maclellan2022domain}, which integrates mechanisms from prior models of human learning, including ACM \cite{LangIey:1984uh}, CASCADE \cite{VanLehn:1991we}, STEPS \cite{Ur:1995wr}, and SimStudent \cite{Li:2013vd}. 

Specifically, I use the {\it Trestle} model,\footnote{For a full description, which is beyond the scope of the current paper, see \citeA{maclellan2022domain}.} which provides an account for how skills are incrementally acquired from a mixed combination of worked examples and correctness feedback.
When presented with a problem, such as the fraction arithmetic problem shown in Figure~\ref{fig:fractions-tutor}, the model matches previously learned skills against the current state. If any skills match, it executes the one with the highest utility to generate a step. If no skills match, which is common early in learning, then the model requests a demonstration from the tutor. For example, in Figure~\ref{fig:fractions-tutor}, the tutor might demonstrate placing a 6 in the lower left denominator conversion box. The model, equipped with basic arithmetic primitives for adding, subtracting, multiplying, and dividing, searches for a sequence of mental operations (i.e., a procedure) that explains this demonstration. 
It might identify that multiplying the two given fraction denominators  produces the demonstrated value.
The agent then generalizes this procedure, removing specific values to form a reusable skill.
It then applies separate learning mechanisms to identify the conditions for applying the skill and to update its utility.
When the agent uses this skill on subsequent problems and receives feedback from the tutor, it further refines the skill's conditions and utility.

%\subsection{Provide a general description and cite other papers}
%\subsection{The Theory-Driven Approach to Computational Modeling}
%\subsection{Connection to the Common Model of Cognition}

%\begin{figure}[t] % 't' positions the figure at the top of the page
%    \centering
%    \begin{subfigure}[b]{0.47\textwidth} % %'b' aligns the baseline of the subfigure
%        \includegraphics[width=\textwidth]{Images/FractionArithmeticTutorHuman.png}
%        % \caption{Caption for Human Tutor}
%    \end{subfigure}
%    \hfill % Adds horizontal space between the figures
%    \begin{subfigure}[b]{0.47\textwidth} % Adjust width as needed
%        \includegraphics[width=\textwidth]{Images/FractionArithmeticTutor.png}
%        % \caption{Caption for AI Tutor}
%    \end{subfigure}
%    \caption{The fraction arithmetic tutor interface used by the human students (left) and the isomorphic, machine-readable interface used by the simulated agents (right). If the current fractions need to be converted to a common denominator, then students must check the ``I need to convert these fractions before solving'' box before performing the conversion. If they do not need to be converted, then they enter the result directly in the answer fields (without using the conversion fields).} % Optional overall caption
%    \label{fig:fractions-tutor}
%\end{figure}

\section{Study 1: Fraction Arithmetic Tutor}

\subsection{Human Data}

\begin{figure}[t] % 't' positions the figure at the top of the page
    \includegraphics[width=0.47\textwidth]{Images/fractions-UI.png}
    \caption{Fraction arithmetic tutor interfaces.} % Optional overall caption
    \label{fig:fractions-tutor}
\end{figure}

To evaluate this model's ability to predict human learning outcomes, I used data from the {\it Fraction Addition and Multiplication} dataset accessed via DataShop \cite{Koedinger:2010tj}.\footnote{https://pslcdatashop.web.cmu.edu/DatasetInfo?datasetId=1190} \citeA{Patel:ho4PITKp} collected these data during an experiment to examine whether blocking or interleaving fraction arithmetic problems leads to better learning.

The dataset includes tutor data from 79 students solving 24 fraction addition problems (ten with same denominators and 14 with different denominators) and 24 fraction multiplication problems using the tutor interface shown in Figure~\ref{fig:fractions-tutor}. 
%Several features of the tutor are notable.
Students had to check the ``I need to convert these fractions before solving'' box to reveal the conversion fields. For fraction addition requiring conversion, only the cross multiplication strategy (multiplying denominators) was accepted; alternative strategies were marked incorrect.
After training, students took a posttest within the tutor, which consisted of four fraction multiplication and four fraction addition problems (two with same denominators and two with different). No hints or feedback were provided during the posttest.

Students were randomly assigned to one of two conditions. In the blocked condition, problems were presented in three sequential blocks: fraction addition (same denominators), fraction addition (different denominators), and finally, fraction multiplication. The order of the problems within each block was randomized. In the interleaved condition, problems were presented in a fully randomized order. 
This experiment aimed to test whether interleaving enhances learning compared to the blocking approach recommended by the Common Core State Standards. \citeA{Patel:ho4PITKp} found that students in the blocked condition performed better during training, while those in the interleaved condition performed better on the posttest. This suggests that although interleaved practice results in errors during training, it ultimately leads to greater long-term learning. The goal of this study is to determine whether the computational model can accurately predict this main experimental effect.

\begin{figure}[t] % 't' positions the figure at the top of the page
    \centering
    \begin{subfigure}[b]{0.47\textwidth} % 'b' aligns the baseline of the subfigure
        \includegraphics[width=\textwidth]{Images/Fractions-Tutor-Overall-Bar.png}
        %\caption{Caption for Tutor Overall}
        %\label{fig:fractions-overall}
    \end{subfigure}
    \hfill % Adds horizontal space between the figures
    \begin{subfigure}[b]{0.47\textwidth} % Adjust width as needed
        \includegraphics[width=\textwidth]{Images/Fractions-Posttest-Bar.png}
        %\caption{Caption for Posttest}
        %\label{fig:fractions-posttest}
    \end{subfigure}
    \caption{Fraction arithmetic accuracy on tutor and posttest problems with 95\% confidence intervals (CIs).} % Optional overall caption
    \label{fig:fractions-overall}
\end{figure}

\subsection{Simulation and Analysis Methods}
To simulate human behavior, I created an instance of the model (an agent) for each student and connected it to the machine-readable version of the tutor, shown on the right in Figure \ref{fig:fractions-tutor}.
This isomorphic tutor, developed using Cognitive Tutor Authoring Tools \cite{aleven2006cognitive}, provided each agent with the same sequence of problems that its corresponding human student received. Importantly, agents were not constrained to take the same actions as their human counterparts. Thus, a tutor was necessary to provide the agents with appropriate hints and feedback during training, as the log data alone was insufficient for conducting the simulation. 

\begin{figure}[t] % 't' positions the figure at the top of the page
    \centering
    \begin{subfigure}[b]{0.47\textwidth} % 'b' aligns the baseline of the subfigure
        % \includegraphics[width=\textwidth]{Images/Fractions-Blocked-LC-Field.png}
        \includegraphics[width=\textwidth]{Images/Fractions-Blocked-Error-By-Problem-full.png}
        %\caption{Caption for Blocked}
        %\label{fig:fractions-blocked-lc}
    \end{subfigure}
    \hfill % Adds horizontal space between the figures
    \begin{subfigure}[b]{0.47\textwidth} % Adjust width as needed
        % \includegraphics[width=\textwidth]{Images/Fractions-Interleaved-LC-Field.png}
        \includegraphics[width=\textwidth]{Images/Fractions-Interleaved-Error-by-Problem-full.png}
        %\caption{Caption for Interleaved}
        %\label{fig:fractions-interleaved-lc}
    \end{subfigure}
    \caption{Fraction arithmetic learning curves with 95\% CIs. The blocked tutor transitions types at problems 11 and 25.} % Optional overall caption
    \label{fig:fractions-learning-curve}
\end{figure}

One key technical limitation of the isomorphic tutor was that it did not support hidden fields. In the human tutor, the fields for converting fractions remained hidden until the student checked the ``I need to convert these fractions before solving'' box, whereas in the machine-readable tutor, these fields were always visible. As a result, simulated students could make errors---such as attempting to convert fractions before selecting the conversion box---that were impossible in the human tutor. However, preliminary simulations showed that agents rarely made such errors.

After training, the human students took a posttest to assess their fraction arithmetic knowledge. This assessment was administered directly within the tutor with hints and correctness feedback disabled. Students could enter any values in the text fields and they could check the conversion box at any time, which revealed the conversion fields regardless of whether conversion was necessary. At any point, they could press the done button and advance to the next problem. \citeA{Patel:ho4PITKp} evaluated students based on the percentage of problems solved correctly. The simulated agents also completed the posttest within the tutor. During the posttest, hints and feedback were disabled. Agents were marked as correct only if they completed all steps of a problem correctly, in which case they advanced to the next problem. If they made any mistakes or requested a hint, the problem was immediately marked incorrect and they were moved to the next problem. 
%As with the human students, I evaluated simulated agents based on the percentage of problems they solved correctly.

To statistically evaluate the effect of instructional condition on both humans and agents, I conducted mixed-effect logistic regression analyses. 
During data preparation, I identified that seven students had corrupted posttest data and one additional student had corrupted tutor and posttest data.
After removing these cases, the dataset included tutor data from 78 students and posttest data for 71 students.
The agent simulations were based on the tutor data, so they produced tutor and posttest predictions for the 78 students with complete tutor data.

To assess tutor performance, I modeled correctness as the dependent variable, including fixed effects for condition, problem type (add same, add different, and multiply), and problem count (number of prior problems of the same type). I also included an interaction between problem type and count to capture different learning rates for problem types. For human data, I incorporated a random intercept for each student to account for individual differences. Since all agents had identical prior fraction knowledge (none), no random effect was included. For posttest performance, I again modeled problem correctness as the dependent variable, with instructional condition and problem type as fixed effects and a random effect for student. Problem count was excluded, as students did not receive any feedback during testing, so learning was not expected. Unlike in training, agents had varying prior knowledge at posttest based on their training, so the regression included a random effect for student.

\begin{table*}[ht]
\small
\centering
\caption{Mixed-effect regression models fit to humans and agents fraction arithmetic tutor and posttest data.}
%\small
\begin{tabular}{l|c|c||c|c}
\toprule
\textbf{Fixed Effects} & \textbf{Human Tutor} & \textbf{Model Tutor} & \textbf{Human Posttest} & \textbf{Model Posttest}\\ 
% & \textbf{(OR [95\% CI])} & \textbf{(OR [95\% CI])} & \textbf{(OR [95\% CI])} & \textbf{(OR [95\% CI])}\\ 
\midrule
Intercept (Add Diff) & 0.35 [0.21, 0.57]* & 0.12 [0.08, 0.17]* & 1.43 [0.73, 2.81] & 0.12 [0.06, 0.22]* \\
Condition (Interleaved) & 0.50 [0.28, 0.89]* & 0.40 [0.34, 0.47]* & 2.35 [1.0, 5.52]* & 7.75 [4.06, 14.78]* \\
Type: Add Same & 4.54 [2.76, 7.45]* & 5.59 [3.48, 9.04]* & 4.42 [2.28, 8.59]* & 1.43 [0.82, 2.48] \\
Type: Mult & 10.39 [6.92, 15.62]* & 7.49 [4.99, 11.37]* & 4.23 [2.42, 7.40]* & 38.08 [19.73, 73.47]* \\
Type Count & 1.16 [1.12, 1.20]* & 1.39 [1.33, 1.45]* & --- & --- \\
Type: Add Same × Count & 1.17 [1.07, 1.27]* & 0.93 [0.87, 1.00] & --- & --- \\
Type: Mult × Count & 0.94 [0.91, 0.98]* & 0.85 [0.81, 0.89]* & --- & --- \\
\toprule
\textbf{Random Effects} &  & & & \\ 
\midrule
$\sigma^2$ & 3.29 & --- & 3.29 & 3.29 \\ 
$\tau_{00}$ (Student) & 1.47 & --- & 2.00 & 0.79 \\ 
Intraclass Correlation & 0.31 & --- & 0.38 & 0.19 \\
N (Student) & 78 & --- & 71 & 78 \\
\toprule
Observations & 3559 & 3559 & 567 & 624 \\ 
Marginal/Conditional $R^2$ & 0.254 / 0.484 & --- & 0.100 / 0.441 & 0.499/0.596\\ 
$R^2$ (Tjur) & --- & 0.278 & --- & --- \\ 
\bottomrule
\end{tabular}\\
\textit{Note.} The fixed effect entries are have the format: Odds Ratio [95\% Confidence Interval].
* denotes $p < .05$.
\label{tab:fractions_regression}
\end{table*}

\subsection{Results}
\subsubsection{Main Effect of Condition}
The overall tutor and posttest performance for both humans and agents are shown in Figure~\ref{fig:fractions-overall}.
The results of the regression analysis are shown in Table~\ref{tab:fractions_regression}.
Mirroring the prior findings of \citeA{Patel:ho4PITKp}, I find that while humans have lower tutor performance in the interleaved condition (odds ratio: 0.50, $p<0.05$), 
they have higher posttest performance (odds ratio: 2.35, $p<0.05$). 
My analyses shows that agents exhibit this same effect.
Agents in the interleaved condition have lower tutor performance (odds ratio: 0.40, $p<0.05$), but higher posttest performance (odds ratio: 7.75, $p<0.05$).
Beyond the main effect of condition, the model exhibits most of the other effects, and direction of effects, that are present in the human data.
There are two exceptions: the effect of practice on the add same problem performance and the intercept on the posttest. 
However, in both situations either the human or model regression is not significant, suggesting that more data is needed. 

%These results support my claim that the model can predict the effect of a problem ordering manipulation; i.e., that humans in the blocked condition will do better in the tutor, but worse on the posttest. The results also indirectly support the models' underlying assumption that student behavior is primarily a factor of their experiences and prior knowledge, rather than difference in their learning and performance mechanisms.

% These results support the use of the model as a preliminary tool for predicting the effects of tutor design choices before conducting more costly human experiments.

\subsubsection{Learning Curves}

I plotted the errors on each problem, averaging across students within condition. 
The resulting learning curves are shown in Figure~\ref{fig:fractions-learning-curve}.
Unlike models that generate predictions by fitting a function to the student data, such as the Additive Factors Model \cite{Cen:2006th,MacLellan:2015to}, these learning curves are {\it parameter-free} \cite{weitekamp2019toward} predictions based solely on task structure.
% There are several characteristics of these curves worth noting. 
There is a large difference between the models and humans on earlier opportunities.
This suggests many students have prior fraction knowledge---their average first problem error was 53.8\%.
In contrast, the agents have zero prior fraction knowledge---they have an average first problem error of 100\%.
Despite this difference, the model does surprisingly well at {\it qualitatively} capturing the main effects.
For example, both humans and agents exhibit high error rates when transitioning from one problem type to another in the blocked condition and have similar asymptotic error.

\subsection{Discussion}
These results show that the model can successfully predict the main experimental effects of a fraction arithmetic problem ordering manipulation.
% In my first analysis, I show that the model predicts the main experimental effects of a problem ordering manipulation. I showed that, like the humans, both models have better tutor performance in the blocked condition, but better posttest performance in the interleaved condition. 
They also show that the model can generate reasonable parameter-free learning curve predictions. These results are a clear example of how tutor A/B experiment results can be predicted in a completely theory-driven way using a computational model of learning.

One caveat is that the model only qualitatively predicts the experimental effects. It does not accurately predict the absolute tutor and posttest scores for each student (or their average) because it does not currently account for prior fractions knowledge.
This is particularly noticeable in earlier practice opportunities, where the model predicts that human performance should be much worse than what is observed.
As previously mentioned, these differences are due to prior fraction arithmetic skills that students bring to the tutoring system, which are not accounted for in the current model. The model is essentially predicting what human performance would look like if the students did not have any prior fraction arithmetic skills. While these exaggerated error rates might be useful for detecting transitions between skills, such as when using learning curve analysis to develop knowledge-component models \cite{Corbett:1994ux}, they also suggest an opportunity for improvement. 

Some researchers have started to explore different ways to account for this knowledge.
\citeA{weitekamp2019toward} propose statistically estimating how much
previous practice each student has had with each type of problem and pretraining agents on a comparable number of problems to initialize prior knowledge.
Alternatively, \citeA{maclellan2023optimizing} suggest iteratively adjusting each agent's prior fraction knowledge to minimize the differences between agent and human learning trajectories.
Both approaches require human data, limiting their use when testing novel designs;
%to tasks where such data is available.
% However, once tuned instructional designers can use these models to predict counterfactual performance for alternative instructional choices \cite{maclellan2023optimizing}.
more research is needed to better account for prior knowledge in this case.


%Also, the model seems to exhibit similar asymptotic error in the interleaved condition.
%There are some differences worth highlighting.
%Also, we see that in the blocked condition, the agents quickly converge to lower error rates than the humans in the second and third blocks.
%\citeA{Patel:ho4PITKp} found evidence that human students experience negative interference effects on this task.
%I hypothesize that the human students have to overcome prior knowledge that negatively interferes with their ability to learn to add fractions with different denominators (the second block) whereas agents do not have prior knowledge that interferes.


% I computed the error on each learner's first attempts of each step and generate overall learning curves using these values. In particular, I labeled each step in both the human and simulated data sets by the skill, or knowledge component \cite{Koedinger:2010tr}, that it exercised. In this case, I labeled them by the field that was being updated (which roughly corresponds to the application of a particular skill) for each problem type.\footnote{This model, called the ``Field'' model, is currently the best fitting knowledge-component model on DataShop.} Then, for each first attempt (the first time performing a particular step on a problem), I computed the prior number of opportunities a student has had to exercise the same skill and plotted the average error on first attempts (across all skills) by the number of prior practice opportunities.

%The result of this process is the three learning curves shown in the left graph of Figure \ref{fig:fraction-lc-plus-residuals}: one for the humans and one for each model. Unlike models that generate predictions by fitting a function to the student data, such as the Additive Factors Model, the learning curves generated by these models constitute {\it parameter-free} predictions of the human students' performance. There are a number of characteristics of these curves that are worth noting. First, there is a large difference between the models and humans on earlier opportunities. This suggests that many human students have prior knowledge of how to perform fraction arithmetic---they get more than 50\% of the steps correct on the first practice opportunity. In contrast, the simulated agents start without any fraction arithmetic skills. However, by the fifth practice opportunity the performance of the humans and the models has begun to converge. Further, after the eighth opportunity, the \textsc{Decision Tree} model consistently outperforms the humans, whereas the \textsc{Trestle} model more closely approximates the human performance. This difference in performance between the two models at the higher practice opportunities is due to the different when-learning implementations and suggests that the \textsc{Trestle} model is a better approximation of the human behavior. 

%To ensure that these results are not simply a side effect of averaging over students and steps, the right graph of Figure \ref{fig:fraction-lc-plus-residuals} shows the model residuals plotted by opportunity. Model residuals were generated by subtracting each model's prediction (for a particular student on a particular opportunity of a particular skill) from the corresponding human student's performance. These residuals represent the average agreement between the humans and models on specific items. The key result of this analysis is that it agrees with my initial analysis of the learning curves. Mainly, both models are different from the humans on the first five practice opportunities, but the \textsc{Trestle} model better agrees on the higher opportunities (i.e., the residuals are closer to zero). 


%Given the human and simulated learning curve data, I next turned to evaluating which of the two models better accounts for variation in the human behavior. For this evaluation, I performed a mixed-effects regression analysis, which showed that the \textsc{Trestle} model better explains the variation in student performance across students, skills, and opportunities. This latter result supports the theory that humans learn the conditions under which skills apply incrementally, rather than non-incrementally and provides an example of how my general research approach supports theory testing and revision.

\section{Study 2: Box and Arrows Tutor}

\subsection{Human Data}

As a second test of whether a computational model can  predict the results of a human experiment, I used the {\it Box and Arrow Tutor Data (Turk Study)} dataset accessed via DataShop.\footnote{https://pslcdatashop.web.cmu.edu/DatasetInfo?datasetId=1123}
\citeA{Lee:2015gb} collected these data from Mechanical Turk as part of an experiment to investigate how different instructional choices effect student learning of problem-solving rules.
This experiment used the box and arrows tutor shown in Figure~\ref{fig:box-and-arrows-tutor}, which presents learners with two types of problems: (1) easy problems, where students enter the solution to the arithmetic problem from the first row into the empty box in the second row (e.g., $22/11=2$ in the right example from the figure), and (2) hard problems, where the student enters a value in second row that makes the lower arithmetic problem equal the value the base of the arrow points to (e.g., $7-4 = 3$ in the left example from the figure).
The easy problems were designed to be obvious, leveraging students' prior knowledge, whereas the hard problems were designed to be non-obvious, requiring students to discover the rule.

The data was collected from 202 students solving 32 box and arrow problems (16 easy and 16 hard).
The main experimental manipulation explored whether students perform better when receiving constrained or unconstrained problems. The constrained problems were designed so only the correct procedure produced a whole-number answer; other procedures (e.g., applying the easy procedure to a hard problems) produced a fractional answer. In contrast, unconstrained problems produced whole-number answers for incorrect procedures too. Students received no explicit instruction, but they did receive correctness feedback on answers.

\citeA{Lee:2015gb} frame learning on this task as a search process where students generate and test rules to see if they produce correct results.
They also hypothesized that constrained problems bias students towards the correct procedure because they make the correct procedure easier to compute than the incorrect procedure (working with whole numbers is easier than fractional numbers).
They found that students in the constrained condition had lower error rates on both easy and hard problems than those in the unconstrained condition.

My model is a direct instantiation of \citeA{Lee:2015gb}'s theory of learning; mainly, agents learn by generating and testing rules, making use of those that work and discontinuing use of those that do not.
However, working with whole and fractional numbers are equally difficult for the agents, so we can use them to directly test \citeA{Lee:2015gb}'s hypotheses for why constrained problems aid learning.
%Thus, the current study explores whether we can replicate \citeA{Lee:2015gb}'s main experimental effect.
If the agents exhibit the same main effect as the humans, then it provides further evidence that computational models of learning can predict experimental outcomes.
Such a finding would, however, be direct evidence against \citeA{Lee:2015gb}'s hypothesis for why constrained problems make learning easier.
 
\subsection{Simulation and Analysis Method}

I used the same simulation procedure as study 1, creating an agent for each student and connecting it to a machine-readable version of the box and arrows tutor, shown in Figure~\ref{fig:box-and-arrows-tutor} to simulate learning.
An analysis of the human data showed that the majority of human students solved the easy problems correctly on the first attempt. In contrast, only a few students got the  hard problems correct on the first try because they are specifically designed to prevent prior knowledge use. Thus, for my analysis, I trained the agents on all the problems, but only analyzed their performance on the hard problems, because my model does not take into account prior knowledge, which seems to play a major role in solving easy problems.\footnote{Half of the simulated agents received prior training on 16 easy problems; I pre-trained the agents in case there was positive or negative transfer from the easy problems to the hard ones.} The focus on hard problems makes it possible to compare agents and humans in a situation where they have matched prior knowledge. This subset of the human data contained 202 students solving 16 hard problems, for a total of 3,232 observations.

To evaluate the effect of instructional condition, I conducted a mixed-effect logistic regression analysis similar to the one used for the fractions task. I modeled problem correctness as the dependent variable, including fixed effects for condition and problem count and a random effect for student. Unlike the fractions analysis, this regression model did not include an effect for problem type because all problems were of of the same type (hard). This mixed-effect regression model has a close correspondence to the ANOVA analysis conducted by \citeA{Lee:2015gb}. When applying this regression analysis to the agents, the random effect for student was removed because the agents all have identical initial conditions.

\subsection{Result}

\begin{figure}[t] % 't' positions the figure at the top of the page
    \includegraphics[width=0.47\textwidth]{Images/box-and-arrow-UI.png}
    \caption{Box and arrows tutor interfaces.} % Optional overall caption
    \label{fig:box-and-arrows-tutor}
\end{figure}

\begin{figure}[t] % 't' positions the figure at the top of the page
    \includegraphics[width=0.47\textwidth]{Images/box-and-arrows-overall.png}
        %\caption{Caption for Blocked}
        %\label{fig:fractions-blocked-lc}
    \caption{Overall box and arrow accuracy with 95\% CIs.}
    %The error bars denote 95\% confidence intervals and significant differences were determined via a mixed-effect regression analysis.} % Optional overall caption
    \label{fig:box-and-arrow-overall}
\end{figure}

\begin{figure}[t] % 't' positions the figure at the top of the page
    \centering
    \begin{subfigure}[b]{0.47\textwidth} % 'b' aligns the baseline of the subfigure
        % \includegraphics[width=\textwidth]{Images/Fractions-Blocked-LC-Field.png}
        \includegraphics[width=\textwidth]{Images/box-and-arrows-lc-constrained.png}
        %\caption{Caption for Blocked}
        %\label{fig:fractions-blocked-lc}
    \end{subfigure}
    \hfill % Adds horizontal space between the figures
    \begin{subfigure}[b]{0.47\textwidth} % Adjust width as needed
        % \includegraphics[width=\textwidth]{Images/Fractions-Interleaved-LC-Field.png}
        \includegraphics[width=\textwidth]{Images/box-and-arrows-lc-unconstrained.png}
        %\caption{Caption for Interleaved}
        %\label{fig:fractions-interleaved-lc}
    \end{subfigure}
    \caption{Box and arrow learning curves with 95\% CIs.} % Optional overall caption
    \label{fig:box-and-arrow-learning-curve}
\end{figure}

\subsubsection{Main Effect of Condition}
Figure~\ref{fig:box-and-arrow-overall} shows the accuracy of humans and agents across the two conditions.
Similar to the fractions study, it shows the model and human performance are {\it qualitatively} similar.
%Similar to the fractions study, agents exhibit the same experimental effect as the humans. 
I find that \citeA{Lee:2015gb}'s findings apply to just the hard problems: students in the unconstrained condition are less likely to be correct (odds ratio: 0.17, 95\% CI: [0.08, 0.37], $p<0.05$).
I find that agents exhibit the same effect: those trained with unconstrained problems are also less likely to be correct (odds ratio: 0.46, 95\% CI: [0.37, 0.56], $p<0.05$).

This task is also uniquely well suited for comparing the {\it quantitative} performance between agents and humans.
In contrast to the fractions task, where humans could apply prior knowledge, this task was intentionally designed to inhibit prior knowledge use.
Thus, the agents and humans start with the same initial conditions.
Figure~\ref{fig:box-and-arrow-overall} shows that humans have a constrained accuracy of $19.1\%$ ($SD=0.39$) and agents have a similar constrained accuracy of $19.6\%$ ($SD=0.40$). Also, humans have an unconstrained accuracy of $8.8\%$ ($SD=0.28$) while agents have an unconstrained accuracy of $10\%$ ($SD=0.30$).
\vskip 6pt

\subsubsection{Learning Curves}
Figure~\ref{fig:box-and-arrow-learning-curve} shows the learning trajectories of the agents and humans, averaging error across students within each condition at each problem.
In contrast to the fractions results, we see a much shallower rates of learning for both the agents and humans, suggesting that the model is able to capture general trends in learning (rapid learning in fractions, but more limited learning here).
This is notable because the model is not trained on or fit to the human data; these predictions are generated based entirely on the structure of the task and the sequence of the items.
They could have been generated prior to collecting any human data.

%Given these overall results, I next looked at testing both models' capabilities for predicting the effects of instructional manipulations. In the previous chapter, I showed that both models predict the effect of problem ordering on tutor and posttest performance in fraction arithmetic. Of the new data sets, the only testable instructional manipulation was in boxes and arrows, which varied whether students received constrained or unconstrained training problems. On constrained problems, the correct rule produces an integer value, but the incorrect rule produces a non-integer value. For the unconstrained problems, however, both correct and incorrect rules produce integer values. \citeA{Lee:2015gb} found that students learn better when presented with constrained problems across both the easy and hard problems. To test if my models predict this effect, I first tested to ensure this effect holds with the humans for just the hard problems as I did not analyze performance on the easy problems. To test for this effect, I used a mixed-effect beta regression analysis \cite{Bates:2015cf} with fixed effects for condition (constrained or unconstrained) and practice opportunity, a random intercept and slope (for practice opportunity) for each skill, and a random intercept for each student.\footnote{The R lmer4 regression formula is correctness $\sim$ condition $+$ kc $*$ opportunity $+$ (1 $+$ opportunity|student).} This analysis, which is similar to the repeated-measures ANOVA used by \citeA{Lee:2015gb}, shows that the effect of condition holds just for the hard problems as well ($z=-4.376$, $p<0.001$). Next, I applied the same statistical test to both of the simulated data sets, with the exception that I dropped the random effects for students because all simulated students are the same. This analysis shows that the incremental version of the model successfully predicts this effect $z=-8.870$, $p < 0.001$, but the batch version does not $z=-0.051$, $p>0.95$.

\subsection{Discussion}

These findings provide a second demonstration of the model accurately predicting the main effect of a human experiment.
They also showcase again its ability to generate learning curves that reasonably align with human learning trajectories.
Although the study 1 and 2 tasks are both math related, they employ different instructional manipulations---a problem ordering manipulation and an item design manipulation.
This provides some evidence for the model's general ability to guide instructional design for varying kinds of interventions.

The model can also facilitate testing of different learning theories and hypotheses.
My model directly instantiates \citeA{Lee:2015gb}'s  theory of learning as search through a space of rules; its ability to generate data that matches the human data provides additional evidence in support of this theory.
Surprisingly, however, the results provide evidence against \citeA{Lee:2015gb}'s hypothesis that constrained problems aid learning by making the correct procedures easier to compute.
Computing with whole or fractional numbers is equally difficult for agents, but they still exhibits the main effect, so constrained problems must aid agent learning in another way.

I hypothesize that the benefit of constrained problems is less about ease of computation and more about lower procedural ambiguity. 
To satisfy the property that only correct procedures yield whole-number solutions, constrained problems often have only a single (correct) candidate procedure that is consistent with the correct answer.
In contrast, unconstrained problems often have multiple candidate procedures (some incorrect) that are consistent.\footnote{For example, the unconstrained hard problem in Figure~\ref{fig:box-and-arrows-tutor} has three candidates: $7-3=4$ (correct), $2+2=4$, and $2 \times 2=4$. The comparable constrained problem (see Figure 3 in \citeNP{Lee:2015gb}) has only a single (correct) candidate procedure.}
As a result, both the model and humans are more likely to select the correct procedure on constrained problems, which aids their learning and performance.
More research is needed to test this hypothesis, but if true, it would suggest a distinctively different strategy to designing problems to promote learning---items should be designed so there is only a single (correct) candidate procedure that is consistent with the correct answer.
% Beyond providing evidence to support my the use of my model for predicting how different instructional choices will affect human learning and performance, our results suggest the power of my approach for testing learning theories and guiding instructional design.

%Although the earlier results suggest that the \textsc{Decision Tree} model has better performance, this result suggest that the \textsc{Trestle} model better predicts human behavior. Interestingly, \citeA{Lee:2015gb} hypothesize that students do better with constrained problems because the correct rule is easier to compute. However, correct and incorrect rules are equally easy for both models, suggesting that it is not the difficulty of computation that is driving these results. Instead, my findings suggest that problem ordering and interference, due to different distractor numbers and operators in the unconstrained problems, are responsible. The \textsc{Decision Tree} model is less affected by problem ordering and interference, so it does not show this effect.

%\section{Related Work}
%\subsection{Simulated Students}
%\subsubsection{Tutor Authoring}
%\subsubsection{Cognitive Model Discovery}
%\subsubsection{Learning Companions}

\section{Conclusion}
To my knowledge, this paper presents the first evidence that a computational model of learning can successfully predict the main effects observed in multiple human A/B experiments.
It also demonstrates that this models can generate reasonable predictions of human learning trajectories and offer theoretical insights into the effectiveness of specific instructional interventions.
These findings suggest that computational models of learning could operate as Model Human Learners, supporting instructional designers in evaluating and identifying the most pedagogically effective instructional designs.
More research is needed to expand the Model Human Learner concept to additional tasks and interventions, but this work represents a preliminary proof of concept that lays the groundwork for a broader research program.

% \section{Model Human Learner Extensions}
%\subsubsection{Personalized Models}
%\subsubsection{Counterfactual Prediction}
%\subsubsection{Moving Beyond Tutors}

% The results of this study have been encouraging, but they do not mean that the Apprentice Learner Architecture and the models cast within it offer a complete account of human learning. However, they do suggest the framework is flexible enough to support new hypotheses about learning and provides ways to test these hypotheses. In future work, I plan to explore several variations of the current theory and invite the community to extend it to explain other phenomena.

% One affordance of the Apprentice Learner Architecture is that it facilitates a search among alternative theories and models. Not unlike existing techniques for searching the space of domain models \cite{Cen:2006th}, a search among alternative apprentice learner models would let researchers explore several hypotheses of human learning. For example, it is questionable whether how-, where-, when-, and which-learning are the correct combination of internal components. It may be that the \textsc{Trestle} model's when-learning approach is sufficient for both where- and when-learning, which would suggest that the current distinction between where- and when-learning is unnecessary and that a single learning component might produce more human-like simulated data. Alternatively, it could be argued that the architecture is incomplete because it has a fixed set of relational knowledge, rather than updating its relational knowledge given new experiences. This argument implies that a learning component for updating relational knowledge, effectively a what-learner, should be included in the architecture \cite{Li:2014hr}. Beyond merging or adding architectural components, each component could be instantiated using different approaches. For example, the current where-learning approach carries out effectively no generalization, in that it memorizes the field names used in positive examples, but future models might explore more human-like approaches, such as learning heuristic conditions in a general-to-specific fashion \cite{Langley:1985wm}. Exploring all of these possibilities could be framed as a search task over different models within the architecture to find those that generate the most human-like behavior. 

% This chapter compared model and human error rates, but the Apprentice Learner Architecture allows for finer-grained evaluation. Rather than comparing simulated and human learners on whether they performed a step correctly, researchers could compare learners in terms of their literal response on a step. This type of comparison would support the evaluation of theories of student misconceptions and lets researchers explore how misconceptions affect the particular responses students make, similar to the prior work on modeling student bugs \cite{LangIey:1984uh,Vanlehn:1983wf}. Similarly, in this study I only compared performance on first attempts, because this is a common convention in EDM, but the high-fidelity simulation data would let me examine learner behavior beyond the first attempt. Ultimately, a unified theory of apprentice learning should account for all of the behaviors learners exhibit on their path to mastery.

% As I have stated previously, I view the current state of the Apprentice Learner Architecture as incomplete. There are several aspects of learning that the architecture does not try to explain, such as the effects of delayed feedback \cite{Schmidt:1992ex}, the impacts of metacognition \cite{Aleven:2006wb}, and the behavior of collaborative learners \cite{Olsen:2015vt}. Crucially, however, the architecture is not fundamentally incompatible with these ideas. For example, which-learning, which updates skills' utilities, might utilize a reinforcement learning scheme to back-propagate correctness from delayed feedback. The role of metacognition might be accounted for with a more nuanced variation of a recognize-act cycle that takes into account metacognitive decisions. Finally, instantiating multiple apprentice learner models within the same environment and allowing them to generate demonstrations for each other could serve as an initial computational model of collaborative learning. These are just a few examples of how the structure of the architecture could be augmented to incorporate and test additional learning theories. 

% In conclusion, this chapter provided an initial proof of concept for my proposed research program (see Figure \ref{fig:theory-data-loop}) and showed that the Apprentice Learner Architecture supports both theory-driven prediction of human behavior and data-driven theory development. Additionally, it constituted a first step towards a complete computational theory of apprentice learning. Not only do I believe that EDM can improve our fundamental theories of learning, but that it is uniquely positioned to do so. Using my proposed approach, researchers can use every tutored learning data set in the canon of EDM to test and advance learning theories. I hope that other EDM researchers will also see the potential of the Apprentice Learner Architecture and the proposed research paradigm, and I look forward to working with them to further develop our collective understanding of human learning.

%%
%% The acknowledgments section is defined using the "acks" environment
%% (and NOT an unnumbered section). This ensures the proper
%% identification of the section in the article metadata, and the
%% consistent spelling of the heading.

%\small
\section{Acknowledgments}
This work grew directly out of my PhD thesis \cite{Maclellan:2017thesis}. As a result, it would not have been possible without the members of my thesis dissertation committee (Ken Koedinger, Pat Langley, Vincent Aleven, and John Anderson) who provided me with invaluable feedback, guidance, and support. This work has been generously funded in part from several sources, including a Graduate Training Grant awarded to Carnegie Mellon University by the Department of Education (\#R305B090023 and \#R305A090519), by the Pittsburgh Science of Learning Center, which is funded by the NSF (\#SBE-0836012), two National Science Foundation Awards (\#DRL-0910176 and \#DRL-1252440), a DARPA award (\#HR00111990055), and the NSF National AI Institutes Program (\#2247790 and \#2112532). The views, opinions and/or findings expressed are those of the author and should not be interpreted as representing the official views or policies of the Department of Defense or the U.S. Government.  I would also like to thank Rony Patel, Hee Seung Lee, and the PSLC DataShop team for providing access to the data used in this paper. Finally, this work would also not have been possible without my collaborator Erik Harpstead, who was integral in first articulating the idea of a computational model of human learning in our original paper on the topic \cite{MacLellan:2016tqa}.

\bibliographystyle{apacite}

\setlength{\bibleftmargin}{.125in}
\setlength{\bibindent}{-\bibleftmargin}

\bibliography{references}


\end{document}

