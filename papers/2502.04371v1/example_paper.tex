%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
% \usepackage{subfigure} % 注释
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% add
\usepackage{csquotes}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{threeparttable}
\usepackage{multirow}
\usepackage{colortbl}
\definecolor{mycolor}{RGB}{182, 198, 233}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{PerPO: Perceptual Preference Optimization via Discriminative Rewarding}

\begin{document}


\widowpenalty=10000
\clubpenalty=10000


\twocolumn[
\icmltitle{PerPO: Perceptual Preference Optimization via Discriminative Rewarding}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Zining Zhu}{equal,1}
\icmlauthor{Liang Zhao}{equal,2}
\icmlauthor{Kangheng Lin}{equal,3}
\icmlauthor{Jinze Yang}{1}
\icmlauthor{En Yu}{4}
\icmlauthor{Chenglong Liu}{1}
\icmlauthor{Haoran Wei}{2}
\icmlauthor{Jianjian Sun}{2}
\icmlauthor{Zheng Ge}{2}
\icmlauthor{Xiangyu Zhang}{2}
%\icmlauthor{}{sch}
% \icmlauthor{Firstname8 Lastname8}{sch}
% \icmlauthor{Firstname8 Lastname8}{yyy,comp}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{1}{University of Chinese Academy of Sciences}
\icmlaffiliation{2}{StepFun}
\icmlaffiliation{3}{Beijing University of Posts and Telecommunications}
\icmlaffiliation{4}{Huazhong University of Science and Technology}

\icmlcorrespondingauthor{Zining Zhu}{zhuzining20@mails.ucas.ac.cn}
\icmlcorrespondingauthor{Liang Zhao}{zhaoliang02@stepfun.com}
% \icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
% \icmlkeywords{Machine Learning, ICML}
\icmlkeywords{MLLMs, Perception, RLHF, DPO}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

% \printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
This paper presents \textbf{Per}ceptual \textbf{P}reference \textbf{O}ptimization (\textbf{PerPO}), a perception alignment method aimed at addressing the visual discrimination challenges in generative pre-trained multimodal large language models (MLLMs). To align MLLMs with human visual perception process, PerPO employs discriminative rewarding to gather diverse negative samples, followed by listwise preference optimization to rank them. By utilizing the reward as a quantitative margin for ranking, our method effectively bridges generative preference optimization and discriminative empirical risk minimization. PerPO significantly enhances MLLMs' visual discrimination capabilities while maintaining their generative strengths, mitigates image-unconditional reward hacking, and ensures consistent performance across visual tasks. This work marks a crucial step towards more perceptually aligned and versatile MLLMs. We also hope that PerPO will encourage the community to rethink MLLM alignment strategies. 

% Our code is available at \url{https://github.com/ningerhhh/PerPO}.
\end{abstract}

\vspace{-1.7em}

\section{Introduction}

\vspace{-0.2em}

The success of \textit{next token generation}~\citep{radford2018improving, radford2019language} has reignited the pursuit of artificial general intelligence (AGI). Representative methods~\citep{brown2020language,claude} have achieved non-trivial advancements in both creative generation~\citep{creation,generation} and logical reasoning~\citep{gptmath1,gptmath}. Recently, they have also demonstrated exceptional multimodal capabilities~\citep{achiam2023gpt,gpt4o}, achieving remarkable results in various generative visual tasks~\citep{evaluation_gpt4v,wen2024road}.

\vspace{-0.1em}

However, visual discrimination tasks have emerged as the Achilles' heel of these multimodal large language models (MLLMs)~\citep{li2024groundinggpt, got, slowperception}. These tasks, which require minimal reasoning and yield deterministic answers—such as \enquote{provide the position of the person}, as illustrated in Figure~\ref{fig1:subfig1}—often leave these powerful models quite \enquote{nearsighted}, or even \enquote{blind}. Could it be that \textit{generative models fundamentally \textbf{struggle with visual discrimination tasks} that are simple for a child?}

\vspace{-0.1em}

Despite efforts~\citep{yu2023merlin,wei2023vary} to address this issue by incorporating discriminative tasks into generative pre-training, results often remain suboptimal, compromising core linguistic abilities. This paper approaches the problem from an \textbf{\textit{alignment}} perspective. We argue that \textit{performance deficiencies in pre-trained models with basic competencies stem primarily from misalignment}. In practice, existing MLLMs lack alignment with perceptual objectives—a fundamental expectation for such models. Recent methods~\citep{sun2023aligning,zhao2023beyond} using Direct Preference Optimization (DPO)~\citep{rafailov2024direct} aim for low-hallucination, high-accuracy outputs but often fall into image-unconditional reward hacking~\citep{RH}, a phenomenon where text preferences are optimized without engaging with visual input. Consequently, a truly perception-oriented alignment becomes necessary.

\vspace{-0.1em}

In this paper, we propose a simple yet effective approach: \textbf{Per}ceptual \textbf{P}reference \textbf{O}ptimization (PerPO) via \textit{discriminative rewards}. Our method aims to align with the human coarse-to-fine visual perception process, which starts broadly and then refines: generating multiple hypotheses around the objective truth, and gradually narrowing down to the best hypothesis as rewards increase~\citep{visual_perception}. To simulate this process, PerPO builds on empirical risk minimization~\citep{ERM1,ERM2}, initially defining the reward as the error between model predictions and the ground truth. Meanwhile, through Best-of-N~\citep{charniak2005coarse} validation in Figure~\ref{fig1:subfig2}, we observe the remarkable consistency between this reward and visual discriminative ability, also revealing the untapped discriminative potential within MLLMs.


\begin{figure*}[t]
    \centering
    \begin{subfigure}[b]{0.31\textwidth}
        \centering
        \setlength{\abovecaptionskip}{15pt}
        \includegraphics[width=\textwidth]{figs/111-7.pdf}
        \caption{Generative vs. Discriminative} 
        \label{fig1:subfig1}
    \end{subfigure}\hfill
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/1-2.pdf}
    \caption{PerPO vs. Other methods}
    \label{fig1:subfig2}
    \end{subfigure}\hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \setlength{\abovecaptionskip}{10pt}
        \includegraphics[width=\textwidth]{figs/1-3.pdf}
        
    \caption{PerPO vs. DPO}
    \label{fig1:subfig3}
    \end{subfigure}
    % \caption{(a) PerPO performs perception alignment improve the visual discrimination ability of the model. (b) The changes in RefCOCOg with increasing list size for SFT, DPO, PerPO, and Best-of-N. (c) Compared to DPO, PerPO shows a larger performance difference with and without image conditions.}
    \caption{(a) Examples of visual generative and discriminative tasks. (b) Performance comparison in RefCOCOg~\citep{2016Generation} with increasing list size for SFT, DPO, PerPO, and Best-of-N. (c) Performance comparison of PerPO and DPO with and without image input across different benchmarks. Notably, PerPO shows a greater performance gap, highlighting a strong reliance on image conditioning.}
    \label{fig:overall}
\end{figure*}

\vspace{-0.1em}

Centered on such \textbf{discriminative rewards}, PerPO first enables \textbf{the acquisition of more negative samples}, which refer to model-generated responses that deviate from the ground truth, with "negative" defined relative to the discriminative truth. This strategy aims to fully exploit the inherent \textit{scalability} of discriminative rewards, facilitating efficient learning from diverse negative samples without human annotation. By employing \textbf{a listwise preference optimization}, PerPO is better suited to learning the relative relationships among different samples—i.e., ranking sequences of samples, which \textit{more effectively captures image-conditioned preference patterns}. As Figure~\ref{fig1:subfig3} confirms, PerPO significantly suppresses optimization toward image-unconditioned reward hacking. Meanwhile, to compensate for the \textit{uncertainty} introduced by preference ranking, we treat the \textbf{reward itself as a quantitative margin} for anchoring the ranking. We demonstrate both theoretically and empirically that PerPO effectively combines generative preference optimization with discriminative empirical risk minimization. This ultimately ensures consistent modeling across visual generation and discrimination tasks.






% \vspace{-0.5em}

Our contributions are summarized as follows: 

% \vspace{-0.8em}

%\begin{enumerate}
\begin{itemize}

    \vspace{-0.5em}
    
    \item We highlight, for the first time, the capability dilemma of generative MLLMs in visual discrimination tasks. To address this, we propose PerPO to align with the human perception process, enhancing both visual discrimination and generation abilities.

    \vspace{-0.2em}

    
    % and human preference alignment.
    \item Technically, we first introduce a scalable discriminative reward, which is consistent with the theoretical basis of ERM. Using the reward, we can obtain more negative samples for effective penalization.

    \vspace{-0.2em}
    
    \item Building on this, a listwise approach to preference optimization facilitates learning the ranking relationships of diverse negative samples and mitigates image-unconditional reward hacking.

    \vspace{-0.2em}
    
    \item Further, using the reward itself as a margin to anchor uncertainty in ranking is theoretically and experimentally proven to harmonize visual perception and generation performance.
%\end{enumerate}
\end{itemize}

\vspace{-1.0em}

\section{Preliminaries}

\vspace{-0.1em}

\textbf{Best-of-N} sampling~\citep{charniak2005coarse, nakano2021webgpt}, also known as rejection sampling, involves generating N candidate solutions and selecting the one that scores highest according to a proxy reward. This method leverages the natural \textit{variability}~\citep{temperaturesampling} in LLM responses, effectively finding the best output from a pool of possibilities. By picking the top-scoring candidate, Best-of-N increases the likelihood of identifying the correct answer, enhancing the problem-solving capabilities~\citep{deepseekcode} of LLMs and making them more reliable and accurate~\citep{bai2022constitutional}.

\vspace{-0.1em}

\textbf{Direct Preference Optimization} (DPO)~\citep{rafailov2024direct} surpasses Best-of-N by utilizing an \textit{implicit reward} derived from reinforcement learning objectives. DPO employs the LLM for both reward learning and proposal generation, fine-tuning the model to better align with human preferences. This integration improves the model's relevance and quality, pushing the boundaries of LLM performance. Formally, given pairwise preference data $(x, y^+, y^-)$, where $y^+$ is preferred over $y^-$ with respect to prompt $x$, the reward objective is defined as:

\vspace{-0.5em}

\begin{equation}
\scriptsize{
r(x,y)=\beta \log \frac{\pi_{\theta} \left( y|x \right)}{\pi_{\text{ref}} \left( y|x \right)} + Z\left( q \right)
}
\label{dpo-reward}
\end{equation}

\vspace{-0.5em}

where $\pi_{\theta}$ is the model being optimized, $\pi_{\text{ref}}$ is the reference model, $Z\left( q \right)$ is a partition function, and $\beta$ is a hyperparameter controlling the deviation between $\pi_{\theta}$ and $\pi_{\text{ref}}$. By reparameterizing the Bradley-Terry (BT) model~\citep{1952Rank}, DPO's objective can be expressed as:



\vspace{-0.5em}

\begin{tiny} 
\begin{equation}
\mathcal{L}_{\text{DPO}} (\theta ) = - \mathbb{E}_{(x, y^{+},  y^{-})  \! \sim \!\mathcal{D}} \left[ \log \! \sigma \! \left(  \! \beta \! \left( \! \log \frac{\pi_{\theta} \left( y^{+} | \! x \right)}{\pi_{\text{ref}} \left( y^{+} | \! x \right)} \!  - \!  \log \! \frac{\pi_{\theta} \! \left( y^{-} \! | x \right)}{\pi_{\text{ref}} \left( y^{-} \! | x \right)} \right) \right) \right]
\end{equation}
\end{tiny}

\vspace{-0.5em}




where $\sigma$ is the sigmoid function, and $\mathcal{D}$ is the preference dataset. This objective encourages the model to assign higher probabilities to preferred completions.



\textbf{From pairwise to listwise preference}, LiPO~\citep{liu2024lipo} extends DPO to handle ranked lists of responses $Y = \{y_1, ..., y_n\}$. It employs the pairwise logistic ranking loss~\citep{2005Learning} for sequence optimization. Specifically, each response is assigned a predicted score, which is defined as follows. To simplify notation, we use $R_{*}$ to represent these scores.

\vspace{-0.5em}

\begin{equation}\scriptsize
\left\{ R_{1},...,R_{n} \right\} =\left\{ \log \frac{\pi_{\theta} \left( y_{1}|x \right)}{\pi_{\text{ref}} \left( y_{1}|x \right)} ,..., \log \frac{\pi_{\theta} \left( y_{n}|x \right)}{\pi_{\text{ref}} \left( y_{n}|x \right)} \right\}
\end{equation}

\vspace{-0.5em}

Additionally, each response is associated with a ranking level $\psi =\left\{ \psi_{1} ,...,\psi_{n} \right\}$, which determines the sample's role in training: higher-ranked responses serve as positive samples, while lower-ranked ones are negative. The listwise ranking objective, in both its basic form and advanced variant (LiPO-$\lambda$), is defined as:


\vspace{-0.5em}

\begin{scriptsize} 
\begin{equation}
\mathcal{L}_{\text{LiPO}} (\theta )=-\mathbb{E}_{(x,Y,\psi )\sim \mathcal{D}} \left[\sum_{\psi_{i} >\psi_{j}} \Delta_{i,j} \log \sigma \left(\beta (R_{i}-R_{j})\right)\right] 
\end{equation}
\end{scriptsize} 

\vspace{-0.5em}

In the basic version of LiPO, $\Delta_{i,j}=1$ for all $i$ and $j$. In the advanced variant, $\Delta_{i,j}$, the Lambda weight, adjusts preference pair weighting based on ranking levels.




Both methods enable efficient preference-based fine-tuning. LiPO offers more nuanced optimization by considering the relative rankings of multiple completions. These approaches align language models with human preferences without needing explicit reward modeling or reinforcement learning techniques.


\vspace{-0.5em}

\section{PerPO: Perceptual Preference Optimization}
\label{perpo}

\vspace{-0.5em}


Motivated by the contrast between MLLMs' prowess in generative tasks~\citep{evaluation_gpt4v,wen2024road} and their struggles in visual discrimination~\citep{li2024groundinggpt,qu2024rio}, we aim to bridge this gap. 
% We propose that explicit perception alignment could alleviate this issue.
We posit that this issue could be alleviated by an explicit perception alignment.
% primarily stems from a lack of explicit perception alignment. 
Therefore, we employ preference optimization to simulate the human innate, coarse-to-fine visual perception process~\citep{visual_perception}. As we will detail, we utilize the value of the model's prediction error relative to the visual ground truth as a reward signal. By maximizing the exploitation of this reward, we can effectively activate the model's inherent visual discrimination capability.



\textbf{A simple reward aligns well with visual discrimination.} Visual models, when trained effectively with supervised learning, tend to produce generalizable and reliable predictions. In visual discrimination tasks, the quality of generated responses can often be measured using specific evaluation criteria. This indicates that the discrepancy between model predictions and ground truths can serve as a highly accurate and effective reward in visual discrimination tasks.



To substantiate this, Figure~\ref{fig1:subfig2} shows the effects of Best-of-N~\citep{charniak2005coarse, nakano2021webgpt}, SFT, DPO~\citep{rafailov2024direct}, and PerPO on RefCOCOg~\citep{2016Generation}. Among them, Best-of-N selects the answer with the highest reward, SFT uses the ground truth, DPO chooses the pair answers with the largest reward discrepancy, and PerPO incorporates all answers. Notably, Best-of-N grows with $N$, achieving 50\% improvement at $N=20$, demonstrating consistency between discriminative reward and model performance. In addition, DPO, trained on largest-margin pairs, surpasses SFT at $N=8$, indicating the reward's efficacy in sample selection. 




\textbf{More negative samples and listwise optimization boost visual perception.}
Methods like PPO~\citep{schulman2017proximal, instructgpt} and LiPO~\citep{liu2024lipo} highlight the importance of diverse preference sample sequences in RL optimization. Generally, a sufficiently varied and systematically ordered set of negative samples helps the model rectify deficiencies incrementally and learn true preferences from rankings. Discriminative rewards, which require no human annotation, scale efficiently and enhance the impact of diverse negative samples for MLLMs.
This is corroborated by Figure~\ref{fig1:subfig2}, where PerPO's performance improves with increasing $N$. Table~\ref{tab:3} further compares PerPO and DPO performance as $N$ increases, validating the superiority of listwise over pairwise optimization.

Meanwhile, recent studies show that human alignment in MLLMs doesn't effectively extend to visual conditions~\citep{wang2024mdpo}, suggesting a form of image-unconditional reward hacking~\citep{RH}. Our comparative analysis of DPO and PerPO, with and without image input (Figure~\ref{fig1:subfig3}), reveals that PerPO exhibits superior gains with visual information. This indicates PerPO's optimization is more dependent on visual conditions. We attribute this robustness to the precision of discriminative reward and the strength of listwise optimization. For MLLMs, this implies that visual input engagement is crucial for accurate pattern identification.

\textbf{Your reward is secretly the perfect margin.}
Rewards often lack absolute values or have unclear magnitudes. Previous methods have addressed this by adding margins~\citep{meng2024simpo} or constructing imbalanced rankings~\citep{pro}, with their success arising from non-uniform objectives that create smoother optimization spaces~\citep{smooth}. As mentioned earlier, discriminative rewards provide a measured deterministic signal, we can directly use them as margins to align with the expected target. 





Concretely, we use the absolute value of the reward itself as the weight for the sequence. Formally, we define \begin{small}$\left\{ \hat{R}_{1},...,\hat{R}_{n} \right\}=\left\{ f(x,y_1),...,f(x,y_n) \right\}$\end{small} to denote discriminative reward scores, where $\hat{R}_i$ is derived by evaluating the discrepancy (denoted by $f$) between sequence samples $Y$ and ground truth $x$. Based on them, we define the reward weight ${w_{ij}}$ for any pair of responses $\left( x,\  y_{i},\  y_{j} \right)$ as:



\vspace{-0.5em}

\begin{equation}\scriptsize
w_{ij}=\frac{\left( \hat{R}_{i}-\hat{R}_{j} \right)^{\gamma}}{\sum_{\hat{R}_{i}>\hat{R}_{j}} \left( \hat{R}_{i}-\hat{R}_{j} \right)^{\gamma}}
\label{marge}
\end{equation}

\vspace{-0.5em}

where $\gamma$ is a scale factor. Notably, a norm design mitigates numerical impacts from varied discriminative rewards, enhancing model training robustness.





\textbf{The PerPO objective.} 
PerPO maximizes the ranking objective using discriminative reward scores to accurately measure response rankings. Leveraging these deterministic scores as the personalization reward weight for listwise preference amplifies the differences between distinct responses.
Ultimately, the ranking optimization objective of our PerPO is defined as:

\vspace{-0.5em}

\begin{scriptsize} 
\begin{equation}
\mathcal{L}_{\text{PerPO}} (\theta )=-\mathbb{E}_{(x,Y)\sim \mathcal{D}} [\sum_{\hat{R}_{i}>\hat{R}_{j}} w_{ij}\log \sigma (\beta (R_{i}-R_{j}))]
\label{perpo-eq}
\end{equation}
\end{scriptsize} 

\vspace{-0.5em}

Overall, PerPO's listwise optimization intensifies penalties on negative samples, mitigating image-unconditional reward hacking, while refining performance through adaptive pairwise optimization based on discriminative rewards.

\textbf{Theoretically, PerPO is a listwise ERM.} A natural question is: \textit{why don't we directly optimize discriminative rewards?} In other words, why not perform empirical risk minimization directly on MLLM? Interestingly, when we adjust the order of the discriminative reward margin and preference optimization objective in Eq~\ref{perpo-eq}, we have


\vspace{-0.8em}

\begin{scriptsize}
\begin{multline}
\mathcal{L}_{\text{PerPO}} (\theta ) = -\mathbb{E}_{(x,Y)\sim \mathcal{D}} \Bigg[ \sum_{\hat{R}_{i}>\hat{R}_{j}} \log \sigma (\beta (R_{i}-R_{j})) \cdot  \\
\frac{\left( \hat{R}_{i}-\hat{R}_{j} \right)^{\gamma}}{\sum_{\hat{R}_{i}>\hat{R}_{j}} \left( \hat{R}_{i}-\hat{R}_{j} \right)^{\gamma}} \Bigg]
\label{full-perpo}
\end{multline}
\end{scriptsize}

\vspace{-0.8em}


We can consider a simplified scenario where $\gamma$ equals 1 and $\sum_{\hat{R}_{i}>\hat{R}_{j}} \left( \hat{R}_{i}-\hat{R}_{j} \right)^{\gamma}$ is treated as a constant.  
Formally, this can be expressed as:



\begin{scriptsize}
\begin{equation}
\mathcal{L}_{\text{PerPO}} (\theta ) = -{\mathbb{E}}_{(x,Y)\sim \mathcal{D}} \left[ \sum\limits_{\hat{R}_{i}} \phi(R_i) \cdot \hat{R}_{i} \right],
\end{equation}
\end{scriptsize}
\begin{scriptsize}
\begin{equation}
\text{where } \phi(R_i) = \sum\limits_{\hat{R}_{i} >\hat{R}_{m}} \log \sigma (\beta (R_{i}-R_{m})) - \sum\limits_{\hat{R}_{i} <\hat{R}_{n}} \log \sigma (\beta (R_{n}-R_{i})).
\end{equation}
\end{scriptsize}



Fundamentally, PerPO can be characterized as a \textit{supervised discriminative learning} paradigm, or more specifically, a \textit{listwise empirical risk minimization} (ERM). At its core, we optimize discriminative rewards that measure the total distance between all rollouts and the unique ground truth. 
For each rollout $R_{i}$, all $R_{m}$ smaller than it in discriminative reward $\hat{R}$ form a coefficient in the preference optimization objective, while all $R_{n}$ larger than it construct an opposite coefficient in this objective. 
This metric, as a difficulty measure in the RL optimization space, functions similarly to a \textbf{focal} term~\citep{lin2017focal}, dynamically adjusting the optimization of discriminative rewards.
In essence, PerPO represents a synergistic optimization framework that harmonizes \textit{\textbf{discriminative rewards with latent RL rewards}} derived from DPO~\citep{rafailov2024direct}, effectively unifying \textit{\textbf{generative and discriminative learning}} paradigms.




\begin{table*}[t]
\renewcommand{\arraystretch}{1.0}
  \centering
  \caption{Performance comparison of SFT, DPO, and PerPO in object grounding and image understanding. \textbf{Bolding} indicates optimal performance, \underline{underlining} indicates sub-optimal performance.}
  \setlength{\tabcolsep}{1.2mm}{
    \begin{threeparttable} 
    % \scriptsize
    \small
    \begin{tabular}{l | c c c c c c c c| c c c c }
    \toprule

   	\multirow{2}{*}{Methods} & \multicolumn{3}{c} {RefCOCO} & \multicolumn{3}{c} {RefCOCO+} & \multicolumn{2}{c |} {RefCOCOg} &
    \multirow{2}{*}{LLaVA\textsuperscript{W}} &  \multicolumn{2}{c} {MMHalBench} & \multirow{2}{*}{POPE} \\

    \cline{2-4}
    \cline{5-7}
    \cline{8-9}
    \cline{11-12}
    & val & testA & testB & val & testA & testB & val & test &  & Score $\uparrow$ & HalRate $\downarrow$ & \\
    
 
     \midrule

      LLaVA-v1.5-7B &  50.0  & 59.9 &  43.3  &  45.8  &  55.2  & 34.6   &  49.4   &  49.3   & 61.8   &  2.11  &  \textbf{0.54}  & 86.1 \\
      
     + SFT &  59.4  & 66.6   &  49.2  &  52.0  &  61.1  & 40.2   &  54.9   &  54.7   & \underline{62.0} &  \underline{2.16}  &  0.61  &  86.1 \\
     + DPO &  \underline{60.6}  &  \underline{67.8}  &  \underline{50.5}  &  \underline{53.3}  & \underline{62.1}   & \underline{41.4}   &  \underline{55.9}   &  \underline{55.1}   & 61.3 &  2.08  &  0.62  & 86.3 \\
     \rowcolor{mycolor!30} + PerPO &  \textbf{63.8} &  \textbf{70.6}  &  \textbf{54.4}  &  \textbf{57.3}  & \textbf{65.9}   & \textbf{46.9}   &  \textbf{60.0}   &  \textbf{59.6}   &  \textbf{64.0} & \textbf{2.26}  &  \underline{0.57}  & \textbf{86.5}  \\
     

     \midrule
     LLaVA-NEXT-7B &  84.9  &  90.5  &  77.3  &  \underline{77.6}  & 86.8   & 67.0   &  80.7   &  80.3   & 72.7   &  \underline{2.79}  & \underline{0.48}    & \underline{87.5}  \\
     + SFT &  84.6  &  90.3  &  77.1  & 77.5   &  86.5  &  67.4  &  \underline{81.3}   &  80.2   &  75.0 & 2.57   & \underline{0.48}   &  \textbf{87.6}  \\
     + DPO  &  \underline{85.5}  &  \underline{90.8}  &  \underline{78.8}  &  \textbf{78.1}  &  \underline{86.9}  &  \underline{68.0}  &  81.0   &  \underline{81.1}   &   \underline{77.6}  & 2.69  &   0.49  &  \underline{87.5} \\  
     % + LiPO&  &     &  &    &  2.81   & 0.46   &    &  &  79.45  &  82.45 \\  
     \rowcolor{mycolor!30} + PerPO &  \textbf{86.7}  &  \textbf{91.3}  &  \textbf{81.0}  &  69.4  &  \textbf{87.3}  &  \textbf{70.1}  &  \textbf{82.4}   &  \textbf{82.4}   &   \textbf{81.2}  & \textbf{2.81}   &  \textbf{0.46}  &  \textbf{87.6}   \\
    
    \midrule
    LLaVA-OneVision & 73.6 & 82.6 & 63.8 & 69.4 & 79.5 & 58.2 & 71.1 & 70.8 & 79.7 & 2.70 & 0.41 & 88.3 \\
    + SFT & 74.7 & 83.7 & 65.4 & 70.3 & 80.8 & 59.1 & 72.1 & 71.7 & 77.9 & 2.73 & 0.40 & 88.1 \\
    + DPO & \underline{79.5} & \underline{86.5} & \underline{71.1} & \underline{74.6} & \underline{83.4} & \underline{64.5} & \underline{76.3} & \underline{76.1} & \underline{80.1} & \underline{2.75} & \underline{0.39} & \underline{88.4} \\
    \rowcolor{mycolor!30} + PerPO & \textbf{82.2} & \textbf{88.1} & \textbf{75.6} & \textbf{77.3} & \textbf{85.3} & \textbf{68.4} & \textbf{79.6} & \textbf{79.9} & \textbf{83.3} & \textbf{2.82} & \textbf{0.37} & \textbf{88.8} \\
    
    
    \bottomrule
    \end{tabular}

    \end{threeparttable}}

  \label{tab:1}%
\end{table*}%


\begin{table*}[t]
\renewcommand{\arraystretch}{1.0}
  \centering
  \caption{Performance comparison of SFT, DPO, and PerPO in dense OCR and image understanding. \textbf{Bolding} indicates optimal performance, \underline{underlining} indicates sub-optimal performance.}
  \setlength{\tabcolsep}{0.55mm}{
    \begin{threeparttable} 
    \small
    \begin{tabular}{l | c  c c  c c c | c c c c  }
    \toprule

         	\multirow{2}{*}{Methods} & 	\multirow{2}{*}{Edit Dist $\downarrow$} & 	\multirow{2}{*}{~F1 $\uparrow$~} & 	\multirow{2}{*}{~Prec$\uparrow$~} &  \multirow{2}{*}{~Rec $\uparrow$} & 	\multirow{2}{*}{BLEU $\uparrow$} & 	\multirow{2}{*}{METEOR $\uparrow$} & \multirow{2}{*}{LLaVA\textsuperscript{W}} &  \multicolumn{2}{c} {MMHalBench} & \multirow{2}{*}{POPE} \\

         \cline{9-10}
    &  &  &  &  &  &  &  &  Score $\uparrow$ & HalRate $\downarrow$ & \\
 
 \midrule
     LLaVA-Next-25k-7B &  0.67   & 0.47  & 0.71  & 0.37  &  0.16  &  0.28 &  \textbf{68.9}  &  2.79  &  0.42    &  89.0  \\

     + SFT &  0.66   & 0.47  &  \underline{0.72} &  0.38  &  0.17  & 0.29  &  67.8  &    2.85  &  0.42    &   89.0 \\

     + DPO &  \underline{0.61}   &  \underline{0.51}  &  \textbf{0.73} & \underline{0.41}  &  \underline{0.20}  & \underline{0.32}  &  68.3  &  \textbf{2.95}  &  \underline{0.40}     &  89.0  \\

     \rowcolor{mycolor!30} + PerPO&  \textbf{0.58}   & \textbf{0.54}  &  \textbf{0.73} & \textbf{0.44}  &  \textbf{0.23}  & \textbf{0.36}  &  \underline{68.4}  &   \underline{2.92}   &  \textbf{0.39}    & 89.0   \\
         \midrule

     
     LLaVA-Next-50k-7B &    0.64  & 0.51  & \underline{0.74} & 0.41  & 0.18  & 0.31  &  70.2  &    2.97 &  \underline{0.36}   &   89.6 \\

     + SFT& 0.62   & 0.52 & \underline{0.74}  & 0.42  &  0.20 & 0.32   &  \underline{69.8}  &   \textbf{3.15}    &  \textbf{0.34}     &   \underline{89.9}   \\
     + DPO&  \underline{0.60}  & \underline{0.54}  &  \textbf{0.75}  &  \underline{0.43} & \underline{0.21}  &  \underline{0.33}  &   69.2   &    \underline{3.10}  &     \underline{0.36}  &  \textbf{90.0}      \\
     \rowcolor{mycolor!30} + PerPO& \textbf{0.56}   &  \textbf{0.56}  & \textbf{0.75}  & \textbf{0.46}  & \textbf{0.24} & \textbf{0.36}   &   \textbf{71.5}  &   3.00    &   \underline{0.36}    &   \textbf{90.0}  \\


    \bottomrule
    \end{tabular}

    \end{threeparttable}}

  \label{tab:2}%
\end{table*}%


\vspace{-0.5em}

\section{Experiments}

% \vspace{-0.5em}

\subsection{Implemental Details}

\vspace{-0.5em}

\textbf{Data construction.}We construct listwise preference data for two visual discriminative tasks: object grounding and dense OCR. Discriminative rewards are calculated using Intersection over Union (IoU) for object grounding and edit distance for dense OCR.
For object grounding, we derive the corpus from RefCOCO~\citep{2016Modeling}, RefCOCO+~\citep{2016Modeling}, and RefCOCOg~\citep{2016Generation}. We sample an equal amount of data from each dataset and perform 20 samplings per instruction using the model at a temperature of 0.5. The resulting preference data are then filtered based on the data margin, defined as the difference between the maximum and minimum discriminative rewards within a list of responses. By setting the margin to 0.8, we retain 3,000 high-quality samples.
For dense OCR, we use page-level OCR data from Fox~\citep{liu2024focus}, employing edit distance instead of IoU for rewarding. Setting the margin to 0.04 yields a dataset of 1,800 samples.







\textbf{Models and training settings.} 
We adopt LLaVA-v1.5-7B~\citep{liu2023improvedllava} as the base model, integrating CLIP-ViT-L-336px~\citep{clip} and Vicuna-7B-v1.5~\citep{vicuna, llavav1.5}. All experiments are conducted using DeepSpeed ZeRO stage-3, applying LoRA~\citep{lora} for fine-tuning. The training setup includes a batch size of 8 and a learning rate of 5e-6 with the AdamW optimizer. Training is completed on 8 GPUs in approximately 1.5 hours. To further validate our approach, we utilize LLaVA-Next-7B~\citep{liu2024llavanext} and LLaVA-OneVision~\citep{llavaonevision} for object grounding task. However, these models demonstrate limited efficacy in the dense OCR task, due to a lack of sufficient training data. To address this, we construct page OCR datasets of varying sizes (25k, 50k), combining them with the original 780k instruction tuning data to train LLaVA-Next-*k-7B. The models employ SigLIP-400M~\citep{siglip} as the visual encoder and Qwen2-7B~\citep{quen2} as the language model.




\begin{figure*}[t]
    \centering
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/data_margin1_c.pdf}
        \caption{Data Margin} 
        \label{fig2:subfig1}
    \end{subfigure}
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/data_size-2_c.pdf}
        \caption{Data Size}
        \label{fig2:subfig2}
    \end{subfigure}
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/beta_c.pdf}
        \caption{Hyperparameter $\beta$} 
        \label{fig2:subfig3}
    \end{subfigure}
    \caption{Analysis of training data quality, quantity, and hyperparameter $\beta$ (a) Performance across different data margins. (b) Performance across different data sizes. (c) Performance across different $\beta$ values in the loss function.
    }
    \label{fig2:Ablation}
    % \vspace{-5mm}
\end{figure*}



\textbf{Evaluation benchmarks.}
We conduct a comprehensive assessment of PerPO across various multimodal benchmarks. Using LLaVA\textsuperscript{W}~\citep{liu2023improvedllava}, we evaluate the general capabilities of multimodal models. To assess perceptual robustness, we employ hallucination metrics from MMHalBench~\citep{sun2023aligning} and POPE~\citep{li2023evaluating}.
For object grounding, we utilize the RefCOCO, RefCOCO+, and RefCOCOg datasets, with AP@50 as the evaluation metric. In the dense OCR scenario, we use Fox's proprietary dataset, measuring performance with Edit Distance, F1-score, Precision, Recall, BLEU~\citep{2002BLEU}, and METEOR~\citep{2005METEOR}. Meanwhile,  Appendix~\ref{app:2} provides additional metrics for evaluating the model's performance in general visual tasks. This comprehensive evaluation provides valuable insights into PerPO's capacity in addressing multimodal challenges.

\vspace{-0.5em} 


\subsection{Performance Comparison}


\textbf{PerPO performs well across various visual discriminative tasks.}
To demonstrate PerPO's effectiveness, we evaluate SFT, DPO and our PerPO on different model baselines across various downstream tasks.
As shown in Table~\ref{tab:1}, PerPO consistently outperforms SFT and DPO across benchmarks, revealing a superiority of listwise preference optimization to pointwise (SFT) and pairwise (DPO).
On LLaVA-v1.5-7B, PerPO significantly boosts the object grounding capacity, with relative gains of 3.42\%, 8.18\%, and 5.58\% on RefCOCO, RefCOCO+, and RefCOCOg, respectively. 
On stronger baselines LLaVA-NEXT-7B and LLaVA-OneVision, PerPO also delivers consistent improvements, demonstrating its cross-model generalizability.
PerPO similarly demonstrates its superiority in the highly applicable dense OCR scenario. Table~\ref{tab:2} illustrates this by showing significant reductions in edit distance on two baselines (13.4\% in LLaVA-Next-25k-7B and 14.3\% in LLaVA-Next-50k-7B, respectively). This highlights, first, PerPO's cross-task generalizability, and second, its higher data utilization efficiency compared to SFT and DPO.

\textbf{PerPO also improves general image understanding ability.}
As demonstrated in Table~\ref{tab:1} and Table~\ref{tab:2}, PerPO exhibits substantial improvements in general image understanding (LLaVA\textsuperscript{W}) and image hallucination mitigation (MMHalBench and POPE). In addition, we evaluated more general vision metrics in Appendix~\ref{app:2}, including MM-Vet, MM-Bench,  MMMU and VQAv2. Experiments indicates that despite PerPO's singular focus on aligning perceptual processes, it effectively generalizes to broader image comprehension domains, and in fact, deepens image cognition.



\vspace{-0.5em}


\subsection{Ablation Study}

\textbf{Training data statistical analysis.}
Training data plays a crucial role in preference optimization. We conduct a comprehensive statistical analysis, focusing on data quality and quantity. Quality is assessed by the margin, defined as the difference between the highest and lowest discriminative scores within a list. As shown in Figure \ref{fig2:subfig1}, the experimental results are influenced by the margin. A balanced performance for both LLaVA\textsuperscript{W} and RefCOCO+ is achieved with the margin of 0.8 to 1.0. Figure \ref{fig2:subfig2} indicates that RefCOCO+ improves with larger data size, while LLaVA\textsuperscript{W} declines. Optimal performance occurs at 3k samples.



\textbf{Hyperparameter $\beta$ in PerPO loss.}
DPO loss includes a hyperparameter $\beta$, which controls the model's sensitivity to differences between candidate responses. A higher $\beta$ increases the model's focus on subtle distinctions in outputs, while a lower $\beta$ allows for greater tolerance of minor deviations. During training, $\beta$ also affects the model's rate of assimilating human preferences, with an optimal value ensuring stable learning progression. This parameter, also applied in our PerPO method, underwent several experimental iterations. As shown in Figure \ref{fig2:subfig3}, the best performance was achieved with $\beta$ set to 0.1. More ablation studies in the training process can be found in Appendix \ref{app:4}.




\begin{figure*}[t]
    \centering
    
    \begin{subfigure}[b]{0.92\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/visvis2.pdf}
    \end{subfigure}
    \caption{Relative performance (Left, Human users as judge) and comparative showcases (Right) with and without PerPO alignment across different tasks.}
    \label{fig3}
\end{figure*}



\begin{table*}[t]
\renewcommand{\arraystretch}{1.0}
  \centering
  \caption{Performance comparison of PerPO and DPO for different sample sizes $N$.} 
  \setlength{\tabcolsep}{1.5mm}{
    \begin{threeparttable} 
    \small
    \begin{tabular}{c | c  c c  c c  |c  c c c c }
    \toprule
	$N$ & Methods &   RefCOCO+ & RefCOCOg & LLaVA\textsuperscript{W} & POPE    & Methods &  RefCOCO+ & RefCOCOg& LLaVA\textsuperscript{W} &POPE   \\
    \midrule

  2  &    DPO  &  50.9 &  54.0  &  60.1  &  86.2 &   PerPO  & 55.4 &  57.3  & \textbf{65.9}  &  86.3 \\

 4 &    DPO  &  52.2 &  54.6 & 60.6   & \textbf{86.3}   &  PerPO  &  56.2 &  58.6   & 61.2  &   \textbf{86.5} \\
    
8  &    DPO  & 52.6  &  \underline{55.2}  &  \underline{62.4} & 86.2  &    PerPO & \underline{57.0} &  59.3   & 62.1 &  \underline{86.4} \\
     
     
 12 &    DPO  &  \underline{52.7}  &  \textbf{55.4}  & \textbf{62.6}  &  86.2 & PerPO & \textbf{57.4}  &   \underline{59.4}   & 63.1 &  \textbf{86.5} \\
    
 20 &    DPO  & \textbf{52.9}  &  \textbf{55.4}  & 61.2  &  86.2  & PerPO &  \textbf{57.4} &  \textbf{59.7}  & \underline{64.7} &  \textbf{86.5} \\

    \bottomrule
    \end{tabular}

    \end{threeparttable}}

  \label{tab:3}%
\end{table*}%




% \vspace{-1.5em}

\vspace{-0.5em}

\section{In-depth Analysis}

\subsection{Impact of Discriminative Reward in PerPO}

\textbf{Discriminative reward aligns well with perception.}
We conducted a comparative analysis of Best-of-N, SFT, DPO, and PerPO on object grounding task, using IoU as discriminative reward. To explore upper-bound performance, we calculated Best-of-N using test set ground truth, while other methods utilized the train set. Sampling was performed at temperature 0.5 from a moderately capable model. As shown in Figure~\ref{fig1:subfig1}, Best-of-N's logarithmic performance trend with increasing samples validates the reward's effectiveness in aligning with perception performance in an oracle scenario. Meanwhile, the enhanced gains of DPO and PerPO at higher $N$ values confirm the accuracy of reward-based sample selection or ranking, highlighting the potential of reward-guided approaches for model improvement.


\textbf{Discriminative reward also aligns well with human.}
To assess PerPO's user alignment, we employed both GPT-4o and human users to compare models before and after PerPO alignment from multiple perspectives. We uniformly sampled 500 questions from open-ended datasets like LLaVA\textsuperscript{W}, RefCOCO, and Page-ocr in Fox, and evaluated relative performance, considering response accuracy, instruction adherence, and hallucination reduction. A more detailed description of the evaluation can be found in Appendix~\ref{app:3}. Figure~\ref{fig3} (left) shows that the PerPO-aligned model achieved a higher win rate, with significant improvements in different datasets. Therefore, enhancing perception not only aligns better with human preferences but also boosts user experience due to stronger visual capabilities and more efficient optimization.

% \vspace{-1em}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.45\textwidth]{figs/perpo_loss_c.pdf}
    \caption{Performance of different $\gamma$ values in PerPO loss.}
    \label{fig:gama}
\end{figure}

% \vspace{-1em}

\subsection{Impact of More Negative Samples}

\textbf{More negative supervisions help discrimination.} Figure~\ref{fig1:subfig2} illustrates the asymptotic growth of DPO and PerPO under increased sampling, preliminarily validating the value of negative samples. We further conduct a comprehensive comparison between PerPO and DPO across multiple benchmarks including RefCOCO+, RefCOCOg, LLaVA\textsuperscript{W}, and POPE, examining performance disparities at varying sample sizes $2, 4, 8, 12, 20$. In Table~\ref{tab:3}, observations reveal that increased sampling consistently led to improved performance across diverse metrics. Moreover, PerPO demonstrated more pronounced absolute performance and performance gains relative to DPO. This confirms the role of negative sample supervision in visual preference optimization. Notably, as sampling size $N$ increases, performance gains saturate, indicating a loss of negative sample diversity. Thus, mining more diverse negative samples is critical and will be pursued in future work.


\textbf{Listwise preference optimization helps prevent image-unconditional reward hacking.}
As discussed in Section~\ref{perpo}, we compared the preference optimization results of DPO and PerPO with and without image input on RefCOCOg and LLaVA\textsuperscript{W}. PerPO shows significant performance gains over DPO with image input, demonstrating that PerPO's optimization is more reliant on visual conditions, and hence helps prevent such reward hacking.



\vspace{-0.5em}

\subsection{Impact of Discriminative Margin.}

\textbf{Reward itself serves as the perfect margin.}
As shown in Eq~\ref{perpo-eq}, we introduce a coefficient $\gamma$ to finely modulate the influence of the differential discriminative rewards on the corresponding sample pairs. It can be seen that when $\gamma = 0$, PerPO simplifies to LiPO. When $\gamma \neq 0$, unlike LiPO balanced ranking, PerPO can emphasize inter-sample distinctions, facilitating more targeted optimization. 
Our ablation study on $\gamma$ parameter, presented in Figure~\ref{fig:gama}, shows that the model achieves optimal performance at $\gamma$ = 0.5, highlighting the effectiveness of our personalized weighting strategy in improving model performance.

\vspace{-0.5em}

\subsection{Further Analysis}

\textbf{PerPO aims to unlock the model's full potential.}
PerPO's effectiveness seems to depend on the capability level of the model. Comparing SFT and PerPO performance on models trained with varying amounts of OCR data (0k, 25k, 50k), we found that PerPO's advantage emerges only as the model's capabilities mature. Figure~\ref{fig:perpo_vs_sft} shows that with weak or no dense OCR capabilities, PerPO and SFT perform similarly. However, as the model approaches capability saturation, the area of the light blue region increases significantly, indicating that PerPO outperforms SFT. To sum up, SFT is crucial for imparting basic capabilities, whereas PerPO is key to unlocking the model's full potential in later stages.



\textbf{Qualitative analysis.}
To qualitatively analyze the effectiveness of PerPO, as shown in Figure~\ref{fig3} (right), we present two cases highlighting the differences before and after applying PerPO. The first case involves the object grounding task of locating a glass behind a hamburger. Initially, the model focuses on the hamburger, but after alignment, it correctly identifies the glass. The second case is to ask what the other people arounding the man cooking in the image are doing. Without PerPO, the model would mistakenly think they are watching the man prepare the food and observing his cooking techniques, while the model with PerPO would answer that the people around are socializing and they are enjoying outdoor event and the food being prepared on the grill. PerPO not only improves the accuracy of visual recognition tasks such as object detection, but also reduces hallucinations and enhances visual perception capabilities.


\begin{figure}[t]
    \centering
    \includegraphics[width=0.45\textwidth]{figs/sft_vs_perpo_cc.pdf}
    \caption{Comparison of PerPO and SFT across different dense OCR levels.}
    \label{fig:perpo_vs_sft}
\end{figure}



\vspace{-0.5em}

\section{Related Work}


\textbf{Reinforcement Learning from Human Feedback (RLHF).} 
RLHF~\citep{christiano2017deep, 2020Learning} is a crucial technique for aligning Large Language Models (LLMs) with human preferences, comprising both reward model-based and model-free methods. In PPO~\citep{schulman2017proximal, instructgpt}, an auxiliary reward model is cultivated first and then used to optimize the policy. Conversely, DPO~\citep{rafailov2024direct} directly leverages preference data for policy optimization, offering a streamlined yet effective pathway for alignment. To mitigate overfitting, IPO~\citep{azar2024general} incorporates a regularization term. KTO~\citep{ethayarajh2024kto} and DPOP~\citep{pal2024smaug} optimize the relative gain of outputs, bypassing the need for pairwise data. sDPO~\citep{kim2024sdpo} uses multi-stage training for better alignment. ORPO~\citep{hong2403orpo} and SimPO~\citep{meng2024simpo} adopt reference-free reward formulations to simplify alignment. Despite impressive results, these methods rely on labeled perference data, limiting their generalizability. In contrast, PerPO uses a discriminative reward mechanism, allowing data scaling without extra costs and enhancing model performance across diverse domains.




\textbf{Multimodal Large Language Models (MLLMs).} 
MLLMs~\citep{liu2024visual, yu2023merlin, zhu2024self, dreamllm, text-to-audio, video-llava} integrate various data modalities into a unified framework, enabling more sophisticated content understanding and generation. Vision-Language Models (VLMs) are a prominent example, aligning visual encoders with LLMs to connect different modal information. Recently, MLLMs have been evolving to enhance reliability and incorporate ethical considerations, aiming to align their outputs with human values~\citep{amirloo2024understanding, yu2024rlhf, xu2024survey}. LLaVA-RLHF~\citep{sun2023aligning} leverages supplementary factual information to enhance the reward model, mitigating vulnerabilities like reward hacking. HA-DPO~\citep{zhao2023beyond} reframes hallucination as a preference task, introducing an efficient pipeline for generating high-quality, consistent sample pairs. Additionally, mDPO~\citep{wang2024mdpo} balances language and image preferences, reducing the over-emphasis on textual inputs. Nevertheless, these models focus on reasoning and reducing hallucinations, they often struggle with discriminative tasks requiring minimal analysis and concise answers. PerPO, however, can enhance models' visual comprehension abilities through discriminative rewards. 





\textbf{Generative and Discriminative.} AI's landscape is shaped by discriminative tasks, which classify and predict~\citep{distask1, distask2, detr}, and generative tasks, which create and innovate~\citep{radford2018improving, radford2019language}. Traditionally distinct, these tasks are now converging in the era of MLLMs. Hybrid applications, such as conversational agents~\citep{brown2020language, agent2, agent1} that understand and generate text or autonomous vehicles~\citep{auto1, auto3, auto2} that recognize objects and make decisions, exemplify this trend. Discriminative tasks are increasingly tackled through generative modeling, yielding impressive results in areas like mathematical reasoning~\citep{mathz1, mathz2} and multimodal inference~\citep{inferz1, inferz2}. However, current MLLM architectures face limitations in visual discrimination due to the absence of negative reinforcement. PerPO addresses this shortcoming by optimizing perceptual ordered preferences from discriminative rewards, effectively bridging the gap between MLLMs' generative prowess and their discriminative capabilities in visual tasks.


\vspace{-0.5em}

\section{Conclusion}

In this paper, we highlight the limitations of Multimodal Large Language Models (MLLMs) in visual discrimination tasks, such as object recognition and dense OCR. We propose Perceptual Preference Optimization (PerPO), a framework that enhances MLLMs' visual discrimination capabilities through discriminative rewarding. PerPO constructs perceptual ordered preferences based on prediction deviations, optimizing performance without extensive human annotations. 
Experiments demonstrate significant improvements in MLLMs' performance and output robustness. 
Our method bridges generative and discriminative learning, advancing towards more comprehensive AI systems.






% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
\nocite{langley00}

\bibliography{example_paper}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{Comprehensive Assessment of PerPO}



\subsection{General Visual Capacity Assessment}
\label{app:2}
% You may include other additional sections here.

Our method enhances model perception by employing discriminative rewards in specific tasks like object grounding and dense OCR. To thoroughly evaluate PerPO's capabilities on general visual tasks, we included diverse benchmarks in Table~\ref{tab:01}, such as \textbf{MM-Vet~\citep{mmvet}, MM-Bench~\citep{mmbench}, MMMU~\citep{mmmu}, VQAv2~\citep{vqav2}, and LLaVA\textsuperscript{W}~\citep{liu2023improvedllava}}. The results clearly demonstrate a significant advantage over SFT and DPO, confirming PerPO's superior efficacy.


\textbf{MM-Vet} stands as a preeminent multimodal evaluation metric, critically assessing models across six dimensions: recognition, OCR, knowledge, language generation, spatial reasoning, and mathematical computation. Detailed evaluation results within MM-Vet are presented in Table~\ref{tab:02}. Obviously, our method excels across multiple tasks, indirectly suggesting an enhancement in the model's perceptual capabilities.



\textbf{MM-Bench} is designed to systematically evaluate multimodal models on a range of vision-language tasks with emphasis on robustness, reasoning, and generalization. It often focuses on benchmarks that highlight deficiencies in current vision-language systems. Detailed evaluation criteria and associated tasks span domains like captioning, VQA, and multimodal reasoning.


\textbf{MMMU} stands for multimodal multitask understanding, encompassing datasets and benchmarks tailored to models capable of performing multiple tasks. It is a concept designed to focus on advanced perception and reasoning with domain-specific knowledge, emphasizing flexibility and comprehension across various visual and linguistic scenarios.

\textbf{VQAv2} is a dataset for visual question answering, addressing issues like biases in earlier datasets. It contains pairs of images and questions with answers verified by human annotators, ensuring higher reliability and reducing the tendency of models to exploit statistical patterns in the dataset.


\textbf{LLaVA\textsuperscript{W}} evaluates multimodal large language models on real-world, unstructured inputs like everyday photos and screenshots. It focuses on tasks such as visual question answering, reasoning, and conversational understanding, using human and AI feedback to assess accuracy and relevance. This benchmark emphasizes practical robustness in diverse, open-world applications.



\begin{table*}[!htbp]
\renewcommand{\arraystretch}{1.2}
  \centering
  \caption{Performance comparison of SFT, DPO, and PerPO on general visual benchmarks.}  
  \setlength{\tabcolsep}{2.0mm}{
    \begin{threeparttable} 
    \small
    \begin{tabular}{l c c c c c}
    \toprule
    Methods & MM-Vet & MM-Bench & MMMU & VQAv2 & LLaVA\textsuperscript{W} \\
    \hline
    LLaVA-v1.5-7B & 32.9 & 62.3 & 35.7 & 78.5 & 61.8 \\
    + SFT & 31.0 & 62.5 & 36.7 & 78.6 & 62.0 \\
    + DPO & 31.2 & 62.3 & 36.0 & 78.4 & 61.3 \\
    \rowcolor{mycolor!30}  + PerPO & \textbf{33.3} & \textbf{62.8} & \textbf{37.0} & \textbf{78.8} & \textbf{64.0} \\
     \bottomrule
    \end{tabular}

    \end{threeparttable}}

  \label{tab:01}
\end{table*}



\begin{table*}[!htbp]
\renewcommand{\arraystretch}{1.2}
  \centering
  \caption{Performance comparison of SFT, DPO, and PerPO on MM-Vet.}
  \setlength{\tabcolsep}{2.0mm}{
    \begin{threeparttable}
    \small
    \begin{tabular}{l c c c c c c c}
    \toprule
    Methods & Rec & Ocr & Know & Gen & Spat & Math & Overall \\
    \hline
    
    LLaVA-v1.5-7B & 44.9 & 26.7 & \textbf{22.9} & 21.5 & 25.6 & 7.7 & 32.9 \\
    + SFT & 43.8 & 25.6 & 16.7 & 20.6 & 24.9 & 7.7 & 31.0 \\
    + DPO & 43.5 & 24.6 & 19.5 & 22.5 & 24.5 & 7.7 & 31.2 \\
   \rowcolor{mycolor!30}  + PerPO & \textbf{45.1} & \textbf{29.3} & 19.5 & \textbf{23.0} & \textbf{26.8} & \textbf{12.7} & \textbf{33.3} \\
    
     \bottomrule
    \end{tabular}

    \end{threeparttable}}

  \label{tab:02}
\end{table*}



\subsection{GPT-4o and Human users Assessment}
\label{app:3}

We conducted a comparative analysis of models before and after PerPO alignment, utilizing assessments from GPT-4o and human users across three dimensions: response accuracy (RA), instruction adherence (IA), and hallucination reduction (HaR). The test dataset comprises 500 samples sourced from multiple public datasets. Ultimately, we derived the win rates for PerPO across individual datasets in Table~\ref{tab:05}. The results indicate that the evaluations of GPT-4o and humans yield relatively consistent outcomes.




\begin{table*}[t]
\renewcommand{\arraystretch}{1.2}
  \centering
  \caption{The evaluation of GPT-4o and Human users.}
  \setlength{\tabcolsep}{2.0mm}{
    \begin{threeparttable}
    \small
    \begin{tabular}{l|c|c|c}
    \toprule

 & LLaVA\textsuperscript{W} & RefCOCO & Page-ocr \\
\hline
Win rate as judged by GPT-4o & 56\% & 72\% & 71\% \\
Win rate as judged by Human users & 59\% & 76\% & 71\% \\
     \bottomrule
    \end{tabular}

    \end{threeparttable}}

  \label{tab:05}
\end{table*}



\vspace{-0.5em}

\textbf{GPT-4o prompt template.}
The prompt used to compare the responses before and after applying PerPO is illustrated in Figure~\ref{fig11}.

\vspace{-0.5em}

\begin{figure}[htpb]
    \centering
    
    \begin{subfigure}[b]{0.9\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/gpt1.pdf}
    \end{subfigure}
    \caption{The prompt for comparing the responses before and after applying PerPO.}
    \label{fig11}
\end{figure}


\vspace{-0.5em}

\textbf{Human users.} We invited 20 experts and scholars specializing in computer vision, natural language processing, and human-computer interaction to provide independent assessments. For each question, we calculated the average scores in terms of response accuracy, instruction adherence, and hallucination reduction. The winning response was determined based on the magnitude of these average scores. Finally, we aggregated evaluations from 20 expert assessors to determine PerPO's overall win rate.

\vspace{-0.5em}

\section{More Ablation Studies}
\label{app:4}




\begin{table*}[t]
\renewcommand{\arraystretch}{1.0}
\centering
\caption{Analysis of LoRA training strategy.}
  \setlength{\tabcolsep}{0.9mm}{
    \begin{threeparttable}
    \begin{tabular}{ c c | c  c c  c  c }
    \toprule
	$r$ &  $\alpha$ &  ~RefCOCO~ & RefCOCO+~ & RefCOCOg  & LLaVA\textsuperscript{W} & POPE \\
    \midrule
     64 & 128 &  62.9  &  57.0 & 59.5  & 62.2 &  86.4  \\
     128 &  256 &  63.4  & 57.2  & 59.7  &  62.8 &  86.4 \\
     256 & 512 &  63.7  & 57.6  &  60.0 & 64.1 & 86.5   \\
     512 & 1024 &  \underline{64.4}  & \underline{58.2}  &  \underline{60.3} &  \textbf{64.6}  & \textbf{86.7}  \\
     \rowcolor{mycolor!30} 1024 & 2048 &  \textbf{65.8} & \textbf{59.6}  &  \textbf{61.5} & \underline{64.2}  &  \underline{86.6}  \\
    \bottomrule
    \end{tabular}
    \end{threeparttable}}
\label{tab:lora}
\end{table*}

\textbf{LoRA training strategy.}
The calibration of hyperparameters $r$ and $\alpha$ in LoRA training illustrates the balance between specialized learning and general competence in fine-tuning. Higher $r$ values enhance task-specific knowledge acquisition but carry the risk of catastrophic forgetting, while $\alpha$ controls the magnitude of weight updates. As demonstrated in Table \ref{tab:lora}, the horizontal and vertical axes represent the values of LLaVA\textsuperscript{W} and RefCOCO, respectively. As $r$ increases, the model's performance shows an upward trend. Our experiments with PerPO, conducted at $r=128$ and $\alpha = 256$, prioritize computational efficiency over maximizing performance, in order to reduce resource consumption. This approach underscores the trade-off between theoretical optimization and computational constraints in applied machine learning.





\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
