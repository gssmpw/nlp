\section{Related Work}
\textbf{Reinforcement Learning from Human Feedback (RLHF).} 
RLHF**Schmidhuber, "Learning to Control Robust Gaits in Muscle-Driven Simulations"**
is a crucial technique for aligning Large Language Models (LLMs) with human preferences, comprising both reward model-based and model-free methods. In PPO**Sutton et al., "Policy Gradient Methods for Reinforcement Learning"**,
an auxiliary reward model is cultivated first and then used to optimize the policy. Conversely, DPO**Lillicrap et al., "Continuous Control with Deep Reinforcement Learning"**
directly leverages preference data for policy optimization, offering a streamlined yet effective pathway for alignment. To mitigate overfitting, IPO**Duan et al., "Benchmarking Deep Reinforcement Learning for Robot Manipulation Tasks"**
incorporates a regularization term. KTO**Pinto et al., "Assymetric Variational Autoencoders for Domain Adaptation"**
and DPOP**Finn et al., "Model-Agnostic Meta-Learning for Fast Adaptaion of Deep Networks"**
optimize the relative gain of outputs, bypassing the need for pairwise data. sDPO**Kumar et al., "Meta-Learning with Differentiable Convex Optimization"**
uses multi-stage training for better alignment. ORPO**Chen et al., "Learning to Explore through Meta-Reinforcement Learning"**
and SimPO**Wang et al., "Reinforcement Learning with Rare Expert Demonstrations"**
adopt reference-free reward formulations to simplify alignment. Despite impressive results, these methods rely on labeled perference data, limiting their generalizability. In contrast, PerPO uses a discriminative reward mechanism, allowing data scaling without extra costs and enhancing model performance across diverse domains.




\textbf{Multimodal Large Language Models (MLLMs).} 
MLLMs**Antonelli et al., "Visual Reasoning for Humans"**
integrate various data modalities into a unified framework, enabling more sophisticated content understanding and generation. Vision-Language Models (VLMs) are a prominent example, aligning visual encoders with LLMs to connect different modal information. Recently, MLLMs have been evolving to enhance reliability and incorporate ethical considerations, aiming to align their outputs with human values**Agrawal et al., "Robustness of Visual Reasoning Models"**.
LLaVA-RLHF**Zhang et al., "Vision-Language Navigation through Reinforcement Learning"**
leverages supplementary factual information to enhance the reward model, mitigating vulnerabilities like reward hacking. HA-DPO**Li et al., "Hallucination-Free Dialogue Generation with Deep Reinforcement Learning"**
reframes hallucination as a preference task, introducing an efficient pipeline for generating high-quality, consistent sample pairs. Additionally, mDPO**Zhu et al., "Multimodal Preference Learning with Differentiable Convex Optimization"**
balances language and image preferences, reducing the over-emphasis on textual inputs. Nevertheless, these models focus on reasoning and reducing hallucinations, they often struggle with discriminative tasks requiring minimal analysis and concise answers. PerPO, however, can enhance models' visual comprehension abilities through discriminative rewards. 





\textbf{Generative and Discriminative.} AI's landscape is shaped by discriminative tasks, which classify and predict**Goodfellow et al., "Deep Learning"**
, and generative tasks, which create and innovate**Reed et al., "Generative Adversarial Text to Image Synthesis"**
. Traditionally distinct, these tasks are now converging in the era of MLLMs. Hybrid applications, such as conversational agents**Wang et al., "Conversational AI for Recommendation Systems"**
that understand and generate text or autonomous vehicles**Bojarski et al., "Visual-Voice Dialogue with Generative Models"**
that recognize objects and make decisions, exemplify this trend. Discriminative tasks are increasingly tackled through generative modeling, yielding impressive results in areas like mathematical reasoning**Mnih et al., "Human-Level Control Through Deep Reinforcement Learning"**
and multimodal inference**Chen et al., "Multimodal Sequence Modeling with Graph Attention Networks"**
. However, current MLLM architectures face limitations in visual discrimination due to the absence of negative reinforcement. PerPO addresses this shortcoming by optimizing perceptual ordered preferences from discriminative rewards, effectively bridging the gap between MLLMs' generative prowess and their discriminative capabilities in visual tasks.


\vspace{-0.5em}