\section{Related Work}
\textbf{Reinforcement Learning from Human Feedback (RLHF).} 
RLHF~\citep{christiano2017deep, 2020Learning} is a crucial technique for aligning Large Language Models (LLMs) with human preferences, comprising both reward model-based and model-free methods. In PPO~\citep{schulman2017proximal, instructgpt}, an auxiliary reward model is cultivated first and then used to optimize the policy. Conversely, DPO~\citep{rafailov2024direct} directly leverages preference data for policy optimization, offering a streamlined yet effective pathway for alignment. To mitigate overfitting, IPO~\citep{azar2024general} incorporates a regularization term. KTO~\citep{ethayarajh2024kto} and DPOP~\citep{pal2024smaug} optimize the relative gain of outputs, bypassing the need for pairwise data. sDPO~\citep{kim2024sdpo} uses multi-stage training for better alignment. ORPO~\citep{hong2403orpo} and SimPO~\citep{meng2024simpo} adopt reference-free reward formulations to simplify alignment. Despite impressive results, these methods rely on labeled perference data, limiting their generalizability. In contrast, PerPO uses a discriminative reward mechanism, allowing data scaling without extra costs and enhancing model performance across diverse domains.




\textbf{Multimodal Large Language Models (MLLMs).} 
MLLMs~\citep{liu2024visual, yu2023merlin, zhu2024self, dreamllm, text-to-audio, video-llava} integrate various data modalities into a unified framework, enabling more sophisticated content understanding and generation. Vision-Language Models (VLMs) are a prominent example, aligning visual encoders with LLMs to connect different modal information. Recently, MLLMs have been evolving to enhance reliability and incorporate ethical considerations, aiming to align their outputs with human values~\citep{amirloo2024understanding, yu2024rlhf, xu2024survey}. LLaVA-RLHF~\citep{sun2023aligning} leverages supplementary factual information to enhance the reward model, mitigating vulnerabilities like reward hacking. HA-DPO~\citep{zhao2023beyond} reframes hallucination as a preference task, introducing an efficient pipeline for generating high-quality, consistent sample pairs. Additionally, mDPO~\citep{wang2024mdpo} balances language and image preferences, reducing the over-emphasis on textual inputs. Nevertheless, these models focus on reasoning and reducing hallucinations, they often struggle with discriminative tasks requiring minimal analysis and concise answers. PerPO, however, can enhance models' visual comprehension abilities through discriminative rewards. 





\textbf{Generative and Discriminative.} AI's landscape is shaped by discriminative tasks, which classify and predict~\citep{distask1, distask2, detr}, and generative tasks, which create and innovate~\citep{radford2018improving, radford2019language}. Traditionally distinct, these tasks are now converging in the era of MLLMs. Hybrid applications, such as conversational agents~\citep{brown2020language, agent2, agent1} that understand and generate text or autonomous vehicles~\citep{auto1, auto3, auto2} that recognize objects and make decisions, exemplify this trend. Discriminative tasks are increasingly tackled through generative modeling, yielding impressive results in areas like mathematical reasoning~\citep{mathz1, mathz2} and multimodal inference~\citep{inferz1, inferz2}. However, current MLLM architectures face limitations in visual discrimination due to the absence of negative reinforcement. PerPO addresses this shortcoming by optimizing perceptual ordered preferences from discriminative rewards, effectively bridging the gap between MLLMs' generative prowess and their discriminative capabilities in visual tasks.


\vspace{-0.5em}