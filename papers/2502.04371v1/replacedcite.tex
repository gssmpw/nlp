\section{Related Work}
\textbf{Reinforcement Learning from Human Feedback (RLHF).} 
RLHF____ is a crucial technique for aligning Large Language Models (LLMs) with human preferences, comprising both reward model-based and model-free methods. In PPO____, an auxiliary reward model is cultivated first and then used to optimize the policy. Conversely, DPO____ directly leverages preference data for policy optimization, offering a streamlined yet effective pathway for alignment. To mitigate overfitting, IPO____ incorporates a regularization term. KTO____ and DPOP____ optimize the relative gain of outputs, bypassing the need for pairwise data. sDPO____ uses multi-stage training for better alignment. ORPO____ and SimPO____ adopt reference-free reward formulations to simplify alignment. Despite impressive results, these methods rely on labeled perference data, limiting their generalizability. In contrast, PerPO uses a discriminative reward mechanism, allowing data scaling without extra costs and enhancing model performance across diverse domains.




\textbf{Multimodal Large Language Models (MLLMs).} 
MLLMs____ integrate various data modalities into a unified framework, enabling more sophisticated content understanding and generation. Vision-Language Models (VLMs) are a prominent example, aligning visual encoders with LLMs to connect different modal information. Recently, MLLMs have been evolving to enhance reliability and incorporate ethical considerations, aiming to align their outputs with human values____. LLaVA-RLHF____ leverages supplementary factual information to enhance the reward model, mitigating vulnerabilities like reward hacking. HA-DPO____ reframes hallucination as a preference task, introducing an efficient pipeline for generating high-quality, consistent sample pairs. Additionally, mDPO____ balances language and image preferences, reducing the over-emphasis on textual inputs. Nevertheless, these models focus on reasoning and reducing hallucinations, they often struggle with discriminative tasks requiring minimal analysis and concise answers. PerPO, however, can enhance models' visual comprehension abilities through discriminative rewards. 





\textbf{Generative and Discriminative.} AI's landscape is shaped by discriminative tasks, which classify and predict____, and generative tasks, which create and innovate____. Traditionally distinct, these tasks are now converging in the era of MLLMs. Hybrid applications, such as conversational agents____ that understand and generate text or autonomous vehicles____ that recognize objects and make decisions, exemplify this trend. Discriminative tasks are increasingly tackled through generative modeling, yielding impressive results in areas like mathematical reasoning____ and multimodal inference____. However, current MLLM architectures face limitations in visual discrimination due to the absence of negative reinforcement. PerPO addresses this shortcoming by optimizing perceptual ordered preferences from discriminative rewards, effectively bridging the gap between MLLMs' generative prowess and their discriminative capabilities in visual tasks.


\vspace{-0.5em}