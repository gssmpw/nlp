\documentclass[11pt]{article}

%% The amssymb package provides various useful mathematical symbols
\usepackage{amssymb}
\usepackage{lipsum}
\usepackage[table]{xcolor}
\usepackage{xcolor}
\usepackage{booktabs} % For better looking tables
\usepackage{tabularx}
\usepackage{amsmath}
\usepackage{float}
\usepackage{placeins}
\usepackage{url}
\usepackage[pdftex]{graphicx}
\usepackage{caption,subcaption}
\usepackage{hyperref}
\usepackage[numbers]{natbib}
\bibliographystyle{plainnat}
%% You might want to define your own abbreviated commands for common used terms, e.g.:
\newcommand{\kms}{km\,s$^{-1}$}
\newcommand{\msun}{$M_\odot$}

\title{Finding Optimal Trading History in Reinforcement Learning for Stock Market Trading}

\author{Sina Montazeri\thanks{University of North Texas, Denton, Texas 76207, USA}
\and Haseebullah Jumakhan\thanks{Ajman University, Ajman, United Arab Emirates}
\and Amir Mirzaeinia\thanks{University of North Texas, Denton, Texas 76207, USA}}

\date{\today}

\begin{document}
\maketitle

\begin{abstract}
This paper investigates the optimization of temporal windows in Financial Deep Reinforcement Learning (DRL) models using 2D Convolutional Neural Networks (CNNs). We introduce a novel approach to treating the temporal field as a hyperparameter and examine its impact on model performance across various datasets and feature arrangements. We introduce a new hyperparameter for the CNN policy, proposing that this temporal field can and should be treated as a hyperparameter for these models. We examine the significance of this temporal field by iteratively expanding the window of observations presented to the CNN policy during the deep reinforcement learning process. Our iterative process involves progressively increasing the observation period from two weeks to twelve weeks, allowing us to examine the effects of different temporal windows on the model's performance. This window expansion is implemented in two settings. In one setting, we rearrange the features in the dataset to group them by company, allowing the model to have a full view of company data in its observation window and CNN kernel. In the second setting, we do not group the features by company, and features are arranged by category. Our study reveals that shorter temporal windows are most effective when no feature rearrangement to group per company is in effect. However, the model will utilize longer temporal windows and yield better performance once we introduce the feature rearrangement. To examine the consistency of our findings, we repeated our experiment on two datasets containing the same thirty companies from the Dow Jones Index but with different features in each dataset and consistently observed the above-mentioned patterns. The result is a trading model significantly outperforming global financial services firms such as the Global X Guru by the established Mirae Asset.
\end{abstract}

\tableofcontents

%% main text

\input{1-Intro}

\input{2-Lit-Rev}

\input{2.1.hypothesis}

\input{3-Methodology}

\input{4-Eval}

\input{5-application}

\input{6-sum-con}

% \section*{Acknowledgements}
% Thanks to ...

%% The Appendices part is started with the command \appendix;
%% appendix sections are then done as normal sections
% \appendix

% \section{Appendix title 1}
%% \label{}

% \section{Appendix title 2}
%% \label{}

%% If you have bibdatabase file and want bibtex to generate the
%% bibitems, please use
%%
\section{Declaration of generative AI and AI-assisted technologies in the writing process}
% \textbf{Declaration of generative AI and AI-assisted technologies in the writing process}
During the preparation of this work the author(s) used ChatGPT  as writing assistant to draft text, improving clarity , proofreading, language refinement, and saving time. After using this tool/service, the author(s) reviewed and edited the content as needed and take(s) full responsibility for the content of the publication.


% \bibliographystyle{IEEEtran} % We choose the "plain" reference style
% \begin{thebibliography}{24}

% \begin{thebibliography}{}

\bibliography{7-biblio}

% \bibitem{parallel_drl_stock_trading}
% Wang, Xiaoyang, et al. "A parallel multi-module deep reinforcement learning algorithm for stock trading." Neurocomputing 439 (2021): 23-34. Elsevier. DOI: 10.1016/j.neucom.2021.01.128. URL: https://www.sciencedirect.com/science/article/pii/S0925231221005233.

% \bibitem{novel_drl_stock_trading}
% Zhang, Wei, et al. "A novel Deep Reinforcement Learning based automated stock trading system." Expert Systems with Applications 184 (2021): 115548. Elsevier. DOI: 10.1016/j.eswa.2021.115548. URL: https://www.sciencedirect.com/science/article/pii/S0957417423033031.

% \bibitem{market_sentiment_drl_stock_trading}
% Huang, Xin, Haoran Tan, and Qing Xu. "Market sentiment-aware deep reinforcement learning approach for stock trading." Procedia Computer Science 181 (2021): 1147-1155. Elsevier. DOI: 10.1016/j.procs.2021.01.296. URL: https://www.sciencedirect.com/science/article/pii/S2215098621000070.

% \bibitem{drl_end_to_end_stock_trading}
% Liu, Qi, et al. "Deep Reinforcement Learning Based End-to-End Stock Trading Strategy." Procedia Computer Science 184 (2021): 320-326. Elsevier. DOI: 10.1016.j.procs.2021.01.280. URL: https://www.sciencedirect.com/science/article/pii/S2215098621000197.

% \bibitem{adaptive_drl_stock_trading}
% Li, Bo, Liang Zhang, Rui Sun, and Zhaoyu Sun. "Adaptive stock trading strategies with deep reinforcement learning." ScholarX Journal 27.3 (2021): 451-467. URL: https://scholar.xjtlu.edu.cn/en/publications/adaptive-stock-trading-strategies-with-deep-reinforcement-learning.

% \bibitem{Liu2020}
% Liu, Xiao-Yang, et al. "FinRL-Meta: Market Environments and Benchmarks for Data-Driven Financial Reinforcement Learning." Deep Reinforcement Learning Workshop, 34th Conference on Neural Information Processing Systems (NeurIPS2020), 2020. Vancouver, Canada. arXiv:2011.09607v2 [q-fin.TR]. DOI: 10.48550/arXiv.2011.09607. URL: https://doi.org/10.48550/arXiv.2011.09607.

% \bibitem{Vyawahare2020}
% Vyawahare, Abides, et al. "ABIDES-Gym: Gym Environments for Multi-Agent Discrete Event Simulation and Application to Financial Markets." Proceedings of the IEEE Conference on Business Informatics (CBI), 2020.

% \bibitem{Montazeri2023}
% S. Montazeri, A. Mirzaeinia, H. Jumakhan, and A. Mirzaeinia, "CNN-DRL for Scalable Actions in Finance," presented at the 10th Annual Conf. on Computational Science \& Computational Intelligence, 2024. [Online]. Available: https://doi.org/10.48550/arXiv.2401.06179

% \bibitem{Montazeri2024}
% S. Montazeri, A. Mirzaeinia, and A. Mirzaeinia, "CNN-DRL with Shuffled Features in Finance," presented at the 10th Annual Conf. on Computational Science \& Computational Intelligence (CSCI'23), 2024. [Online]. Available: https://doi.org/10.48550/arXiv.2402.03338

% \bibitem{Zhang2019}
% Zhang, Wei, et al. "FinRL-Podracer: High Performance and Scalable Deep Reinforcement Learning for Quantitative Finance." Deep Reinforcement Learning Workshop, 34th Conference on Neural Information Processing Systems (NeurIPS2020), 2019.

% \bibitem{Tsay2010}
% Tsay, Ruey S. "Analysis of Financial Time Series." John Wiley \& Sons, 2010.

% \bibitem{Kumar2007}
% Kumar, P. R., and V. Ravi. "Bankruptcy prediction in banks and firms via statistical and intelligent techniques – A review." European Journal of Operational Research 180.1 (2007): 1-28.

% \bibitem{Atsalakis2009}
% Atsalakis, G. S., and K. P. Valavanis. "Surveying stock market forecasting techniques – Part II: Soft computing methods." Expert Systems with Applications 36.3 (2009): 5932-5941.

% \bibitem{Mnih2015}
% Mnih, Volodymyr, et al. "Human-level control through deep reinforcement learning." Nature 518.7540 (2015): 529-533.

% \bibitem{Tsantekidis2017}
% Tsantekidis, Avraam, et al. "Forecasting stock prices from the limit order book using convolutional neural networks." Proceedings of the IEEE Conference on Business Informatics (CBI), 2017.

% \bibitem{courbariaux2016binarized}
% Courbariaux, M., Hubara, I., Soudry, D., El-Yaniv, R., \& Bengio, Y. (2016). Binarized neural networks: Training deep neural networks with weights and activations constrained to +1 or −1. arXiv preprint arXiv:1602.02830.

% \bibitem{dai2021coatnet}
% Dai, Z., Liu, H., Le, Q. V., \& Tan, M. (2021). Coatnet: Marrying convolution and attention for all data sizes. arXiv preprint arXiv:2106.04803.

% \bibitem{chen2019compressing}
% Chen, K., \& Huo, Q. (2019). Compressing CNN–DBLSTM models for OCR with teacher–student learning and Tucker decomposition. Pattern Recognition, 96, 106957. Elsevier.

% \bibitem{dai2016rfcn}
% Dai, J., Li, Y., He, K., \& Sun, J. (2016). R-FCN: Object detection via region-based fully convolutional networks. IEEE Transactions on Pattern Analysis and Machine Intelligence. IEEE.

% \bibitem{chollet2017xception}
% Chollet, F. (2017). Xception: Deep learning with depthwise separable convolutions. arXiv preprint arXiv:1610.02357.

% \bibitem{LeCun2015}
% LeCun, Yann, Yoshua Bengio, and Geoffrey Hinton. "Deep learning." Nature 521.7553 (2015): 436-444.

% \bibitem{Krizhevsky2012}
% Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. "ImageNet classification with deep convolutional neural networks." Advances in Neural Information Processing Systems 25 (2012).

% \bibitem{Goodfellow2016}
% Goodfellow, Ian, Yoshua Bengio, and Aaron Courville. "Deep Learning." MIT Press, 2016.

% \bibitem{He2016}
% He, Kaiming, et al. "Deep residual learning for image recognition." Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016: 770-778.

% \bibitem{Simonyan2014}
% Simonyan, K., \& Zisserman, A. (2014). Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556.

% \bibitem{Szegedy2015}
% Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke, V., \& Rabinovich, A. (2015). Going deeper with convolutions. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 1-9).

% \end{thebibliography}

% \end{thebibliography}

\end{document}

\endinput
%%
%% End of file `elsarticle-template-harv.tex'.