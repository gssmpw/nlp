\section{Further Experiment Setup} \label{sec:further_experiment_setup}

This section provides additional details on the setup of the experiment:

\paragraph{Dataset Statistic}
We provide the detailed dataset statistics in Table \ref{tab:data_statistic}.
\begin{table}[h]
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{lcccc}
\toprule
\textbf{Datasets (Subjects)} & Train & Validation & Test & Score Range\\
\midrule
ASAP 1 (Science)    & 1,337 & 331  & 554 & 0-3 \\
ASAP 2 (Science)    & 1,018 & 252  & 426 & 0-3 \\
ASAP 5 (Biology)    & 1,436 & 359  & 598 & 0-3 \\
ASAP 6 (Biology)    & 1,437 & 359  & 599 & 0-3 \\ 
Private 1 (Biology) & 440   & 89   & 254 & 0-4 \\
Private 2 (Biology) & 358   & 72   & 196 & 0-3 \\
\bottomrule
\end{tabular}}
\caption{\small \textbf{Dataset statistics.}}
\label{tab:data_statistic}
\end{table}

\paragraph{Classification Baseline} The input to the text classifier consists of concatenated question-related information (including the question prompt, key answer elements, and marking rubric) along with the student answer, separated by newlines. The classifier is trained to predict scores. Following previous studies, we trained a separate model for each dataset and evaluated it using the original test splits~\cite{bert_classifer_aes}. We employed DeBERTa-v3-large as the base pre-trained language model~\cite{debertav3}. The reported results are averaged over five runs with different random seeds (210, 102, 231, 314, 146). The hyper-parameter settings are provided in Table~\ref{tab:hyperparameters_classification}.

\begin{table}[h]
\small
\centering
\begin{tabular}{|l|c|}
\hline
\textbf{Hyperparameter} & \textbf{Value} \\
\hline
Learning Rate          & 2e-5          \\
Batch Size             & 16            \\
Epochs                 & 15            \\
Warmup Steps           & 100           \\
Weight Decay           & 0.1           \\
Optimizer              & Adam          \\
Adam Epsilon           & 1e-8          \\
\hline
\end{tabular}
\caption{\small\textbf{Classification hyper-parameters setting.}}
\label{tab:hyperparameters_classification}
\end{table}

\paragraph{Generative Baselines} For generative baselines, the input to the model comprises the question context and student answers, with the model generating assessment rationales in textual form. The results are averaged over three runs with different random seeds. Unlike prior work~\cite{li_emnlp2024}, we conducted full parameter training using bfloat16 precision. All generative models were trained using the LLaMA-factory framework~\cite{zheng-etal-2024-llamafactory}. The hyper-parameter settings are provided in Table~\ref{tab:hyperparameters_generative}.

\begin{table}[h]
\small
\centering
\begin{tabular}{|l|c|c|}
\hline
\textbf{Hyperparameter} & \textbf{SFT} & \textbf{DPO} \\
\hline
Learning Rate          & 1e-5     & 1e-5        \\
Batch Size             & 4        & 4           \\
Gradient Accumulation  & 4        & 4           \\
Epochs                 & 4.0      & 3.0         \\
Warmup Ratio           & 0.1      & 0.1         \\
LR Scheduler Type      & cosine   & cosine     \\
Optimizer              & Adam     & Adam       \\
Adam Epsilon           & 1e-8     & 1e-8        \\
DPO ftx                & -        & 0.5         \\
DPO $\beta$            & -        & 0.1         \\
\hline
\end{tabular}
\caption{\small\textbf{Generative hyper-parameters setting.}}
\label{tab:hyperparameters_generative}
\end{table}

\paragraph{API Use for Synthetic Data Generation} We utilized \texttt{gpt-4-turbo}~\cite{gpt4} as the LLM to generate synthetic reflection data~\cite{li2023overprompt}, as described in \textsection{\ref{sec:method_data_curation}}. All inference parameters were kept at their default values. The prompt template is presented in Figure \ref{box:prompt_template}.

\begin{figure}
\begin{tcolorbox}[
    colback=gray!10,      %
    colframe=gray!80,     %
    title=Template Prompt for Generate Reflection,
    fonttitle=\bfseries,  %
    rounded corners,
    boxrule=0.5mm,        %
    width=\linewidth
]
\scriptsize
Here is an incorrect assessment rationale for the student answer:\\
\text{[Student Answer]}:\{student\_answer\}\\
Incorrect Rationale: \{reject\_rationale\}\\
This wrong rationale missed the following key elements:\\
- \{idx\}: The student didn't answer the "{key\_element[idx]}" but the incorrect rationale wrongly assessed the student mentioned it.\\
- \{idx\}: The student answered the "{key\_element[idx]}" but the incorrect rationale wrongly assessed the student didn't mention it.\\
Please construct a **reflection guidance** that\\
1. point out the incorrectly assessed key elements,\\
2. guide the model to reflect on the mistakes for generating a better assessment rationale,\\
3. pretend you are talking with an assessor using pronouns like "you",\\
4. By the end of the guidance ask the model to reflect or revise based on the feedback and retry or regenerate the rationale.\\
Output the guidance in JSON format:\{ "guidance": "..." \}
\end{tcolorbox}
\caption{\textbf{The Prompt Template for Contrastive Reflection Synthesis.}}
\label{box:prompt_template}
\end{figure}


\paragraph{\texttt{DARS} Framework} We trained both the Reasoner and Critic models using full parameters training with bfloat16 precision. All models were evaluated using greedy decoding. Except for the scaling experiment, all results were averaged over three different runs. The hyper-parameter settings are provided in Table~\ref{tab:hyperparameters_ours}. We train the Reasoner and Critic models using synthetic data we generated, as introduced in our methodology part. All those models are solely trained on the original train split, as shown in Table \ref{tab:data_statistic}. The validation split was only used to select the best checkpoint, and the Test split was never seen by the model until the evaluation.

\begin{table}[h]
\small
\centering
\begin{tabular}{|l|c|}
\hline
\textbf{Hyperparameter} & \textbf{Value} \\
\hline
Learning Rate          & 2e-5      \\
Batch Size             &           \\
- Model Size $\leq$ 8B & 16        \\
- Model Size $>$ 8B    & 8         \\
Gradient Accumulation  &           \\
- Model Size $\leq$ 8B & 1         \\
- Model Size $>$ 8B    & 2         \\
Epochs                 & 1.0       \\
Warmup Ratio           & 0.05      \\
Weight Decay           & 0.02      \\
LR Scheduler Type      & cosine    \\
Optimizer              & Adam      \\
Adam Epsilon           & 1e-8      \\
\hline
\end{tabular}
\caption{\small\textbf{\texttt{DARS} framework hyper-parameters settings.}}
\label{tab:hyperparameters_ours}
\end{table}

\paragraph{API Use for GPT-4-turbo Critic Baseline} We utilized \texttt{gpt-4-turbo-2024-04-09}~\cite{gpt4} as the Critic LLM to generate reflection data. The temperature is set as 0.7 and the maximum token generation is limited to 1,024. The prompt template is presented in Figure \ref{box:critic_template}.

\begin{figure}
\begin{tcolorbox}[
    colback=gray!10,      %
    colframe=gray!80,     %
    title=Prompt Template for GPT-4-turbo,
    fonttitle=\bfseries,  %
    rounded corners,
    boxrule=0.5mm,        %
    width=\linewidth,
    label={box:prompt}
]
\scriptsize
Given the provided assessment of the student's answer, generate constructive and actionable feedback to help the assessment model improve their response. The feedback should:\\
	1.	Highlight Areas for Improvement: Point out specific aspects where the model can enhance their assessment, such as accuracy, completeness, clarity, or structure.\\
	2.	Provide Actionable Suggestions: Offer clear, practical steps the model can take to address identified weaknesses and improve their understanding.\\
Please generate feedback based on these guidelines to guide the model in refining their response effectively.\\
If the assessment seems good enough, please output ``\text{[STOP]}'' to indicate the end of the feedback.
\end{tcolorbox}
\caption{\small\textbf{Prompt template for GPT-4-turbo as critic.}}
\label{box:critic_template}
\end{figure}

\paragraph{Base Models, Computational Environment, and Inference Setup} In this study, we utilized six different models downloaded from HuggingFace Transformers~\footnote{https://huggingface.co/}. We adhered to the licensing terms of all involved models. meta-llama/Llama-3.2-3B-Instruct (LLaMA 3B), meta-llama/Llama-3.1-8B-Instruct (LLaMA 8B) from~\cite{llama3}, and Qwen/Qwen2.5-3B-Instruct (Qwen 3B), Qwen/Qwen2.5-7B-Instruct (Qwen 7B), Qwen/Qwen2.5-14B-Instruct (Qwen 14B), Qwen/Qwen2.5-32B-Instruct (Qwen 32B) from~\cite{qwen2.5,qwen2_5_report}. 

All generative models were trained using either 4 $\times$ A100 80G or 4 $\times$ H100 GPUs. 

To ensure reproducibility, all evaluations are done using zero-shot prompting with greedy decoding and a temperature of 0. Inference of LLMs is carried out using vLLM~\cite{kwon2023efficient}. We utilized the same prompt templates and score extractor as released by~\cite{li_emnlp2024}. Prompt templates for ASAP 1 (Figure \ref{box:error_1}), ASAP 2 (Figure \ref{box:correct_2}), ASAP 5 (Figure \ref{box:main_example}), and ASAP 6 (Figure \ref{box:error_gpt4o}) can also be found in each case studies.

\paragraph{Manual Evaluation Setup} We randomly sampled 20 instances from each dataset and manually examined the reflection and refinement generated. The outputs were derived from a single run using the LLaMA 3B Reasoner and LLaMA 3B Critic model, as reported in Table \ref{tab:main_assessment_comparison}. The annotations were conducted by the authors of this paper. We categorized the errors using the following schema.

\paragraph{Evaluation on Critic's Reflection} Errors in the Critic model's reflections were classified as follows:
\begin{itemize}[leftmargin=*,noitemsep,topsep=0pt]
\item \textbf{Correct Reflection}: The Critic model accurately identified errors in the previous assessment, ensuring faithfulness to both the student's answer and the question content.
\item \textbf{Incorrect Reflection}: The Critic model either misinterpreted the meaning of the student's answer or the scope of key answer elements, leading to incorrect identification of errors or the identification of errors that were not coherent to the given content.
\end{itemize}

\paragraph{Evaluation on Reasoner's Refinement} We classify the error made by the Reasoner model in refinement into the following three categories:
\begin{itemize}[leftmargin=*,noitemsep,topsep=0pt]
\item \textbf{Correct Refinement}: The situation the Reasoner model successfully refined its previous mistakes based on the Critic's reflection.
\item \textbf{Wrong Refinement Obeyed Reflection}: The situation Reasoner model made an error because it faithfully followed the Critic's wrong reflection.
\item \textbf{Wrong Refinement Ignored Reflection}: The situation in which the Reasoner model introduced a new error, deviating from the Critic's reflection.
\end{itemize}

