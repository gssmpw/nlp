\section{Experiments} \label{sec:experiments}

\subsection{Experimental Setup}
\paragraph{Datasets} We use two data sources, consisting of a total of six different datasets, for our experiments: (1) The Hewlett Foundation Short Answer Scoring (ASAP) dataset~\cite{asap-aes}, which contains short essay responses across science and biology topics. (2) A private dataset comprising student responses to biology exam questions, where human-assigned scores are provided from a reputable examination service. We present dataset statistics in Table \ref{tab:data_statistic}.



\begin{table*}
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{ccccccccccccccccccc}
\toprule
\multirow{2}{*}{\textbf{Methods}} & \multicolumn{3}{c}{\textbf{Classification Baseline}} & \multicolumn{9}{c}{\cellcolor{gray!20}\textbf{Generative Baselines} (\emph{Reasoner Only})} &  \multicolumn{6}{c}{\cellcolor{gray!20}\textbf{Dual-Model Reasoning Framework}}\\ 
\cmidrule(r){2-4} \cmidrule(l){5-13} \cmidrule(l){14-19} 
& \multicolumn{3}{c}{PLM Classifier} & \multicolumn{3}{c}{SFT} & \multicolumn{3}{c}{DPO} & \multicolumn{3}{c}{(\texttt{DARS}) Reasoner only} & \multicolumn{3}{c}{Reflect w/ GPT-4} &  \multicolumn{3}{c}{\textbf{(\texttt{DARS}) Reflect w/ Critic}}\\ 
\cmidrule(l){1-1} \cmidrule(l){2-4} \cmidrule(l){5-13} \cmidrule(l){14-19} 
\textbf{Datasets} & ACC & F1 & \textbf{QWK} & ACC & F1 & \textbf{QWK} & ACC & F1 & \textbf{QWK} & ACC & F1 & \textbf{QWK}  & ACC & F1 & \textbf{QWK} & ACC$^{\dagger,*}$ & F1$^{\dagger,*}$ & \textbf{QWK}$^{*}$\\
\cmidrule(r){1-1} \cmidrule(r){2-4} \cmidrule(l){5-7} \cmidrule(l){8-10} \cmidrule(l){11-13} \cmidrule(l){14-16} \cmidrule(l){17-19} 
\textbf{ASAP 1}    & 0.7767 & 0.7805 & 0.8528 & 0.6968 & 0.7073 & \textbf{0.8277} & 0.6895 & 0.5655 & 0.8051 & 0.6480 & 0.6606 & 0.8073 & 0.5181 & 0.5106 & 0.6349 & \textbf{0.7274} & \textbf{0.7315} & 0.8100 \\
\textbf{ASAP 2}    & 0.6798 & 0.6817 & 0.8187 & \textbf{0.7324} & \textbf{0.7468} & \textbf{0.8420} & 0.6761 & 0.6783 & 0.8033 & 0.6925 & 0.7074 & 0.8136 & 0.5869 & 0.5636 & 0.6532 & 0.7136 & 0.7303 & 0.8277 \\
\textbf{ASAP 5}    & 0.8625 & 0.6055 & 0.8187 & 0.8495 & 0.5600 & 0.8203 & 0.8612 & 0.6449 & 0.8001 & 0.8545 & 0.5424 & 0.7766 & 0.8177 & 0.5119 & 0.6340 & \textbf{0.8645} & \textbf{0.6303} & \textbf{0.8326} \\
\textbf{ASAP 6}    & 0.8891 & 0.6118 & 0.8426 & 0.8314 & 0.5513 & 0.7273 & 0.8314 & 0.5420 & 0.7522 & 0.8280 & 0.5628 & 0.7232 & 0.8130 & 0.4265 & 0.4754 & \textbf{0.8648} & \textbf{0.5988} & \textbf{0.8016} \\
\textbf{Private 1} & 0.6787 & 0.6784 & 0.8853 & 0.5236 & 0.5197 & 0.8082 & 0.5236 & 0.4670 & 0.8196 & 0.5551 & 0.5584 & 0.8221 & 0.4134 & 0.3407 & 0.6018 & \textbf{0.5709} & \textbf{0.5653} & \textbf{0.8253} \\
\textbf{Private 2} & 0.6224 & 0.6355 & 0.8385 & 0.5459 & 0.5377 & 0.7004 & 0.5561 & 0.5600 & 0.7599 & 0.5765 & 0.5752 & 0.7604 & 0.5357 & 0.5219 & 0.7688 & \textbf{0.6071} & \textbf{0.6059} & \textbf{0.7705} \\
\cmidrule(r){1-1} \cmidrule(r){2-4} \cmidrule(l){5-7} \cmidrule(l){8-10} \cmidrule(l){11-13} \cmidrule(l){14-16} \cmidrule(l){17-19} 
\textbf{Overall}  & 0.7515 & 0.6656 & 0.8428 & 0.6966 & 0.6038 & 0.7877 & 0.6897 & 0.5763 & 0.7900 & 0.6925 & 0.6011 & 0.7839 & 0.6141 & 0.4792& 0.6280 & \textbf{0.7247} & \textbf{0.6437} & \textbf{0.8113} \\ 
\bottomrule
\end{tabular}
}
\vspace{-3mm}
\caption{\textbf{Comparison of assessment performance across baseline and Reasoner only preference optimization methods.} Generative methods are indicated with a \colorbox{gray!20}{gray background}. All methods were reproduced or trained using the same LLaMA 3B model as the base. We highlighted the highest values for ACC ($\uparrow$), F1 Score ($\uparrow$), and QWK ($\uparrow$) among generative methods in \textbf{bold}. The overall performance is calculated as the average across all datasets. Symbols $\dagger$ and $*$ indicate statistical significance compared to SFT and DPO by each metric, respectively.}
\label{tab:main_assessment_comparison}
\vspace{-4mm}
\end{table*}

\paragraph{Evaluation Metrics}
We evaluate the assessment performance using Accuracy (ACC), macro F1 (F1), and Quadratic Weighted Kappa (QWK)\footnote{QWK is often considered as the main metric for ASAS.}.

\paragraph{Baselines}
We compare with four baselines: 
\begin{itemize}[leftmargin=*,noitemsep,topsep=0pt,label={}]
\item \textbf{PLM Classifier}: A text classifier built on a pre-trained \texttt{Deberta-v3-large} model~\cite{debertav3} and fine-tuned on various datasets.
\item \textbf{SFT}: A Reasoner-only, supervised fine-tuning baseline trained with datasets released by~\citep{li_emnlp2024} (e.g, takes \textcircled{\raisebox{-0.3pt} {\scriptsize1}} as input, predicts \textcircled{\raisebox{-0.3pt} {\scriptsize2}}).
\item \textbf{DPO}: A DPO approach that performed preference optimization with synthetic reasoning preference data as presented in~\citep{li_emnlp2024} (e.g, takes \textcircled{\raisebox{-0.3pt} {\scriptsize1}} as input, optimize \textcircled{\raisebox{-0.3pt} {\scriptsize4}}$\succ$\textcircled{\raisebox{-0.3pt} {\scriptsize2}}). The base model used is the SFT baseline. 
\item \textbf{Dual w/ GPT-4} A dual-model inference time VRL baseline~\cite{dong-etal-2024-pace}, where Reasoner is trained within our framework, and \texttt{gpt-4-turbo} is used as the Critic to give verbal reflection instructions (e.g,  \textcircled{\raisebox{-0.3pt} {\scriptsize3}}\&\textcircled{\raisebox{-0.3pt} {\scriptsize5}} are generated by GPT-4).
\end{itemize}
We provide further details about the experiment setup in \textsection{\ref{sec:further_experiment_setup}}.

\subsection{Overall Comparison} \label{sec:main_exps}
In this section, we provide a comprehensive evaluation of both scoring performance and rationale quality. As shown in Table \ref{tab:main_assessment_comparison}, we compare our dual-model reasoning framework (\texttt{DARS}) against four baselines, including both classification and generative approaches. All methods, including ours, were trained using the same LLaMA 3B model. Our results indicate that \emph{\textbf{``two heads are better than one''} -- our framework overcomes the data scarcity issue, maintains balanced improvements across all evaluation metrics and outperforms state-of-the-art Reasoner-only preference optimization method}. Furthermore, our Critic model proves to be more effective than a GPT-4 prompting baseline, highlighting its ability to provide more specialized and precise reflection to guide the Reasoner model.

\paragraph{Classifier Baseline} PLM Classifier serves as a strong baseline as it is directly fine-tuned on student answer scoring data. While it exhibits strong performance across all metrics, the \emph{classification approach lacks explainability}, as it only generates scores without providing rationales for its decisions.


\paragraph{Single Reasoner Baselines} The single reasoner baselines, including SFT and DPO, aim to improve explainability by generating rationales for scoring decisions. However, these methods generally underperform compared to classification-based approaches, particularly on the private datasets, where \emph{data scarcity presents a major challenge}. The preference optimization method consistently shows modest improvements over the SFT base model in terms of QWK scores. However, \emph{these improvements come at the cost of declines in F1 (-4\%) and ACC scores (-1\%)}, suggesting a tendency to overfit to preference annotations~\cite{chowdhury2024provably,mitchell2023note}. Moreover, the \emph{implicit preference optimization process lacks transparency}, making the Reasoner-only DPO approach less reliable.

\paragraph{Dual w/ GPT-4 Baseline} We also evaluate a dual-model variant where GPT-4-turbo serves as the Critic to generate reflection-based instructions for refinement. However, after multiple refinements (\texttt{DARS} Reasoner only vs. Reflect w/ GPT-4-turbo), performance significantly declined across all datasets and evaluation metrics. This indicates that despite GPT-4-turbo’s strong general capabilities, \emph{it struggles to produce specialized and precise reflections for refining the Reasoner's output}\footnote{Detailed case studies are provided in Appendix \ref{sec:gpt4o_case_study}.}.


\paragraph{Our Framework} \texttt{DARS} \emph{demonstrates significant improvements from the initial to the final iteration across all datasets, highlighting the efficacy of dual model reasoning, and test-time rationale refinement}. The \texttt{DARS} Reasoner only performance is measured on the Reasoner's \emph{first-pass predictions} (e.g., Reasoner predicts \textcircled{\raisebox{-0.3pt} {\scriptsize2}} based on \textcircled{\raisebox{-0.3pt} {\scriptsize1}}), while the Reflect w/ Critic results are generated from \texttt{DARS}, i.e., %
the final refined Reasoner output before the loop is terminated by the Critic model (e.g.,\textcircled{\raisebox{-0.3pt} {\scriptsize4}}). Compared to the preference optimization baseline (SFT to DPO), our framework (Reasoner only to Reflect w/ Critic) not only \textbf{\emph{outperforms on average ACC, F1, and QWK scores}} but also \textbf{\emph{maintains a balanced enhancement across all metrics even under data scarcity}} (improved 5\% for ACC, 11\% for F1, and 2\% for QWK). Compared with GPT-4-turbo as the Critic, our Critic model more effectively reflects on wrongly assessed rationales and guides the Reasoner outputs to be closer to the oracle labels (18\%-34\% better in metrics). Specifically, Reflect w/ Critic surpasses the Reasoner only assessment result across all datasets and metrics (3\%-7\% improvement). Statistically, Reflect w/ Critic significantly outperforms the state-of-the-art baselines (SFT and DPO)\footnote{A one-tailed t-test yielded a \emph{p}-value of $\leq 0.05$, indicating statistical significance.}. 



 \begin{figure}[ht]
\centering
\includegraphics[width=\linewidth]{figures/gpt4_dars_qwk_performance.png}
\caption{\small \textbf{Performance and completion rate in iterations.}}
\vspace{-2mm}
\label{fig:iteration}
\end{figure}

To show the effectiveness of our Critic model in reflection and determine when to stop, as illustrated in Figure \ref{fig:iteration}, we visualize the performance trend and completion rate comparison between \texttt{DARS}’s iterative reasoning process and GPT-4-turbo as the Critic model. Our method requires only two iterations to achieve a significant improvement over iteration 0, which represents the Reasoner’s initial predictions. In contrast, GPT-4-turbo takes nearly four iterations to reach termination, and shows a clear trend of performance degradation across all metrics as the iterations progress.




\paragraph{Quality Evaluation for Reflection}
To further analyze the transparency and correctness of the generated reflections, we conducted a human evaluation of the Critic-Reasoner interactions. We assessed the quality of the Critic's reflections and the subsequent Reasoner's refinements. The evaluation results are visualized in Figure \ref{fig:human_evaluation}. %
\begin{figure}[ht]
\centering
\includegraphics[width=0.9\linewidth]{figures/donut_charts_refined.png}
\caption{\textbf{Qualitative analysis on reflection and refinement.}}
\label{fig:human_evaluation}
\vspace{-3mm}
\end{figure}

Our findings indicate that the Critic model accurately identified assessment errors in 64\% of cases, effectively localizing errors in scoring rationales. This aligns with previous observations~\cite{tyen-etal-2024-llms}, which suggest that LLMs can correct errors when provided with proper error localization. However, in 36\% of cases, the Critic's reflections were inaccurate, often due to misinterpretation of the student's answer and the scope of the key answer elements. Such inaccuracies had cascading effects: in 34\% of cases, the Critic's incorrect guidance misled the Reasoner, leading to further wrong assessments. We also observed that in 3\% of instances, the Reasoner ignored the Critic's feedback (despite correct or incorrect) %
and still produced erroneous outcomes.%
These results indicate that our Reasoner can follow the Critic's guidance 97\% of the time for refinement. Overall, \emph{these results highlight the critical role of a strong Critic for generating explainable, verbal reflection instructions, so that the Reasoner could effectively refine its predictions}. Further error analysis (\textsection{\ref{sec:our_detail_error_analysis}}) and case studies (\textsection{\ref{sec:our_case_study}}) are provided in the
Appendix.

\subsection{Scaling Law for Dual-Model Framework} \label{sec:scaling_law}
Given that our Reasoner and Critic models are trained independently, we study the effect of model size on the performance of \texttt{DARS} using four Qwen model variants (3B, 7B, 14B, and 32B)~\cite{qwen2.5}. We trained each model using identical datasets, training procedures, and hyper-parameters, resulting in a total of 16 distinct Reasoner and Critic combinations.
\begin{figure}[!ht]
\centering
\includegraphics[width=0.8\linewidth]{figures/heat_maps.png} %
\caption{\small \textbf{Scaling experiment for reasoner and critic.}}
\label{fig:scaling_law}
\vspace{-4mm}
\end{figure}

\noindent We present the overall final performance and performance improvements\footnote{Performance improvement is expressed as a percentage increment compared to the Reasoner only's performance.} in Figure \ref{fig:scaling_law}. Unlike observations in prior studies~\citep{welleck2023generating, akyurek-etal-2023-rl4f, paul-etal-2024-refiner}, \textbf{\emph{our findings suggest that increasing the Critic's size}} (horizontal direction, left to right) \textbf{\emph{leads to greater performance gains (ACC and QWK), more so than increasing the Reasoner's size}} (vertical direction, bottom to top). This suggests that a larger Critic provides more precise evaluation and reflection, which the Reasoner relies upon for refinement\footnote{See \textsection{\ref{sec:compare_critic_sizes}} for case studies.}. Although larger Critic models generally improve F1 scores, this trend is not as pronounced, which is due to imbalances in dataset sizes and label distributions\footnote{Significant label imbalances in some datasets may cause the Reasoner to modify initially ``correct'' minority label categories, thereby affecting the overall F1 trend.}.

\subsection{Ablation Studies on \texttt{DARS}}
\paragraph{Can the Reasoner Refine Effectively Without Strong Task Capability?}
To investigate whether the Reasoner can perform refinement without a strong task capability, we trained two ``weak'' Reasoners with Qwen 3B and LLaMA 3B with weaker rationale training data\footnote{We characterized the data as weaker data for two reasons: (1) the rationales were sourced from ChatGPT, whereas the current training data was curated using GPT-4; (2) a previous study \cite{li_emnlp2024} shows models trained on this dataset exhibit significantly low and imbalance performance.}, following \citet{li_emnlp2023}.
As shown in Figure \ref{fig:weak_reasoner}, all the \texttt{DARS} frameworks with a ``weak'' Reasoner dropped more than 10\% in overall performance across all metrics, even with access to high-quality refinement data and a strong Critic model. %
This result shows that \textbf{\emph{without a strong task capability, the Reasoner cannot perform refinement effectively}}.
\begin{figure}[!h]
\centering
\vspace{-2mm}
\includegraphics[width=0.95\linewidth]{figures/weak_reasoner.png}
\vspace{-2mm}
\caption{\small \textbf{\texttt{DARS} refine with ``weak'' Reasoner model.}}
\label{fig:weak_reasoner}\vspace{-6mm}
\end{figure}

\paragraph{Does Refinement Ability Benefit Reasoner's Task Capability?}
To further investigate the impact of refinement data on task performance, we trained two models: LLaMA 3B w/o Refinement and LLaMA 8B w/o Refinement by excluding the multi-turn reflection refinement data from the Reasoner's training sets.
We report the Reasoner-only's performance in Figure \ref{fig:refine_unseen}.
We observe that evaluation result for Reasoner's w/o refinement models dropped nearly 5\% in all metrics compared with including refinement data, \textbf{\emph{indicating the error correction data (e.g. training the model to refine from errors) can boost the Reasoner’s task capability}}.
This observation align closely with previous findings~\cite{tong-etal-2024-llms,kamoi-etal-2024-llms}.
We also show that reflection data can effectively regulate preference optimization training in \textsection{\ref{sec:rationale_dpo}}.
\begin{figure}[!h]
\centering
\vspace{-2mm}
\includegraphics[width=0.95\linewidth]{figures/refine_unseen.png}
\vspace{-2mm}
\caption{\small \textbf{Ablation on the refinement data for Reasoner.}}
\label{fig:refine_unseen}\vspace{-4mm}
\end{figure}


\paragraph{Can a Single Model Perform Both Reasoning and Reflection?}
We explore whether merging the training data of both the Reasoner and Critic to train a single model could enables effective self-reflection. We trained two self-reflection models Qwen 3B (Self) and LLaMA 3B (Self). Figure \ref{fig:self_reflection} shows a significant decline in the iterative refinement process, with a negative performance improvement rate. This unified model struggles to accurately determine when to terminate the refinement process and failed to provide useful reflection instructions. These findings align with prior observations~\cite{Huang2023LargeLM}, suggesting that \textbf{\emph{``two heads are better than one''--a single model cannot effectively balance both reasoning and critique}}. %
\begin{figure}[!h]
\centering
\vspace{-2mm}
\includegraphics[width=0.95\linewidth]{figures/self_reflection.png}
\vspace{-2mm}
\caption{\small\textbf{Combine dual-model into a single one.}}
\label{fig:self_reflection}\vspace{-6mm}
\end{figure}

\subsection{Generalization Studies}



\paragraph{Can Critic Effectively Reflect on Unseen Questions?} In Figure \ref{fig:unseen_question}, we evaluate the ability of the Critic model to generalize to unseen questions. To do this, we %
trained two versions of Critic: one with exposure to our private datasets (Critic Seen) and one without (Critic Unseen). We use LLaMA 3B as the base model. Our results reveal that the Critic Unseen model, \textbf{\emph{despite its lack of exposure to all datasets, still enhances the Reasoner’s original assessments}} (+1\% in QWK), albeit with slightly reduced effectiveness compared to the Critic Seen model (-3\% in QWK). These findings show that %
the Critic can still provide meaningful feedback even when it has not been explicitly trained on new data.%
\begin{figure}[!h]
\centering
\vspace{-2mm}
\includegraphics[width=0.95\linewidth]{figures/unseen_question.png}
\vspace{-2mm}
\caption{\small \textbf{\texttt{DARS} Critic reflects on unseen questions.}}
\label{fig:unseen_question}\vspace{-6mm}
\end{figure}


\paragraph{Adaptability Beyond Model Sizes and Architectures} 
Figure \ref{fig:generalization}(a) illustrates our exploration of the performance across various base models, including LLaMA 3B, 8B and Qwen 3B, 7B. The results show minimal variance in performance no matter across different model sizes and architectures, demonstrating that our \textbf{\emph{training method is highly adaptable and not restricted to a specific model architecture or size}}. Furthermore, Figure \ref{fig:generalization}(b)  explores the feasibility of using different base models for the Reasoner and Critic at inference time, such as pairing a Qwen Reasoner with a LLaMA Critic. Our findings indicate  \textbf{\emph{consistent performance irrespective of model combinations}}. This highlights the robustness of our framework, due to its \emph{use of text} for effective interactions between the Critic and Reasoner components.
\begin{figure}[!h]
\centering
\vspace{-2mm}
\includegraphics[width=0.90\linewidth]{figures/combined.png}
\vspace{-2mm}
\caption{\small \textbf{Generalization analysis on size, architecture and inference combinations.}}
\label{fig:generalization}\vspace{-6mm}
\end{figure}











