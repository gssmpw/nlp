\section{Related Works}
\paragraph{Verbal Reinforcement Learning for Self-Reflection}
VRL has emerged as a promising approach for enhancing LLM reasoning at inference time**Brown, "Language Models are Few-Shot Learners"**, Early methods relied on self-reflection mechanisms where LLMs refined outputs using contextual cues**Zhang et al., "Self-Refereeing Language Models"**. However, studies show that LLMs struggle to self-correct reliably**Clark et al., "Transformers as First-Order Reasoning Machines"**. To address this, trained critic models have been used to generate verbal feedback for LLM correction**Li et al., "Verbal Critic for Language Model Correction"**, though they primarily focus on single-step feedback. More complex reasoning tasks typically rely on Oracle labels for correction**Hwang et al., "Oracle-Based Reasoning for Natural Language Understanding"**. Our work introduces a dual-model framework where a Critic independently provides more detailed, trace-level reflections, eliminating the need for Oracle labels in verification.

\paragraph{Explainable Automated Student Answer Scoring}
ASAS is traditionally treated as a text classification problem**Kim et al., "Text Classification with Attention"**, with efforts to improve transparency via feature analysis**Zhou et al., "Feature Analysis for Text Classification"** and attention visualization**Srivastava et al., "Attention Visualization for Text Classification"**. Recent approaches incorporate rationale generation for enhanced explainability and transparency**Liu et al., "Rationale Generation for Explainable Text Classification"** but often underperform compared to classification-based methods. **Wang et al., "Thought Tree Framework for ASAS"**, proposed a thought tree framework to model human assessment processes, leveraging LLMs for structured scoring rationales. Our work builds upon this by not only explaining decisions but also improving the transparency of assessment refinement process, through iterative LLM reasoning improvements.