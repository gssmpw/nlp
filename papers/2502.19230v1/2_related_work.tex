\section{Related Works}



\paragraph{Verbal Reinforcement Learning for Self-Reflection}
VRL has emerged as a promising approach for enhancing LLM reasoning at inference time~\cite{Huang2023LargeLM, kamoi-etal-2024-llms}. Early methods relied on self-reflection mechanisms where LLMs refined outputs using contextual cues~\cite{chen2024teaching, jiang-etal-2023-active, welleck2023generating}. However, studies show that LLMs struggle to self-correct reliably~\cite{li-etal-2024-hindsight, tyen-etal-2024-llms, chen2024can, kamoi2024evaluating}. To address this, trained critic models have been used to generate verbal feedback for LLM correction~\cite{welleck2023generating, akyurek-etal-2023-rl4f, paul-etal-2024-refiner}, though they primarily focus on single-step feedback. More complex reasoning tasks typically rely on Oracle labels for correction~\cite{shinn2023reflexion, kim2023language}. Our work introduces a dual-model framework where a Critic independently provides more detailed, trace-level reflections, eliminating the need for Oracle labels in verification.

\paragraph{Explainable Automated Student Answer Scoring}
ASAS is traditionally treated as a text classification problem~\cite{grading_classification, taghipour-ng-2016-neural}, with efforts to improve transparency via feature analysis~\cite{dong-zhang-2016-automatic, bert_feature, li-uncertainty-interpretation,zhou-etal-2024-mystery} and attention visualization~\cite{helen-aes-2016, yang-etal-2020-enhancing}. Recent approaches incorporate rationale generation for enhanced explainability and transparency~\cite{li_emnlp2023} but often underperform compared to classification-based methods. \citet{li_emnlp2024, aaai_demo} proposed a thought tree framework to model human assessment processes, leveraging LLMs for structured scoring rationales. Our work builds upon this by not only explaining decisions but also improving the transparency of assessment refinement process, through iterative LLM reasoning improvements.
