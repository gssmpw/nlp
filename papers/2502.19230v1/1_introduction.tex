








 












\section{Introduction}
Large Language Models (LLMs) %
often struggle with complex reasoning tasks that require nuanced intermediate steps and explicit feedback~\cite{li_emnlp2023}.
Recent advancements in preference optimization such as Direct Preference Optimization (DPO,~\citealt{dpo}) present a promising approach to align the modelâ€™s reasoning outputs with human preferences~\cite{pang2024iterativereasoningpreferenceoptimization,step_dpo}.
These methods typically train LLMs as a \emph{Single Reasoner} using preference pairs to learn from implicit rewards. %
While effective, they are often insufficient to provide explicit feedback to explain \textbf{why} one response is preferred over another \cite{dpo_drawback,sampo,chowdhury2024provably} and require human labels.


\begin{figure}[t]
\centering
\includegraphics[width=0.8\linewidth]{figures/intro_new2.pdf}
\vspace{-2mm}
\caption{\textbf{LLM evaluation and refinement.} (a) Existing inference-time LLM reflection methods use the same model for reflection and refinement, often producing \textbf{vague feedback} that hinders effective refinement; %
(b) Our \texttt{DARS} framework trains two distinct models for reflection and refinement on synthetic error correction data, enabling more \textbf{actionable critiques} and improving refinement for explainable student answer scoring.} %
\label{fig:intro}
\vspace{-6mm}
\end{figure}
A distinct line of research has aimed to improve reasoning with Verbal Reinforcement Learning (VRL)~\cite{shinn2023reflexion, Kumar2024TrainingLM, Snell2024ScalingLT}.
These studies generally follow a paradigm of alternating between two generative steps within the same model to improve reasoning, without requiring additional training.
While these methods can generate explicit reasoning reflection by iteratively refining reasoning errors through verbal critique~\cite{wei-jie-etal-2024-interpretable}, they still face challenges in accurately identifying errors and providing actionable critiques for reasoning traces \cite{kamoi-etal-2024-llms} (Figure~\ref{fig:intro} (a)). This difficulty arises from a \textbf{system-level} conflict inherent to LLMs-their dual roles in both reflecting on errors and refining them~\cite{Huang2023LargeLM}.


A crucial aspect of self-reflection and refinement in LLMs is their ability to detect and correct errors. However, at the \textbf{data-level}, datasets explicitly annotated with error detection and correction traces are scarce, making it difficult to train models that can systematically identify and address their own mistakes~\cite{liu2024best}. To address these challenges, we focus on Automated Student Answer Scoring (ASAS), a task requiring complex reasoning to compare student answers with key answer elements and derive scores based on marking rubrics. To overcome the \textbf{data-level} challenge, we propose a \emph{contrastive reflection synthesis} pipeline (\textsection{\ref{sec:method_data_curation}}) that generates precise verbal reflection data (i.e., error correction instructions) by analyzing discrepancies in multi-step reasoning paths.

Our method leverages structured thought trees to formalize assessment rationales. Given a student response and key answer elements, we construct a tree through progressive comparisons, producing binary decisions (i.e., presence or absence of each element in student's answer). By contrasting discrepancy between assessed paths in this tree, we can systematically identify mismatches in key element assessments to signal potential errors in rationales. These differences then serve as structured prompts for an LLM to generate explicit error correction instructions as verbal reflection (e.g., ``\emph{The answer did not mention the duration each sample was allowed to dry, but your rationale mistakenly credited the student for mentioning it}.'').

Furthermore, to solve the \textbf{system-level} role conflict, we %
propose \texttt{DARS}, a \emph{\underline{D}u\underline{a}l-model \underline{R}eflective \underline{S}coring framework} (\textsection{\ref{sec:reasoning_framework}}) with specialized, trained \textit{Reasoner} and \textit{Critic} models that perform refinement and reflection, respectively. Our \textit{Critic} innovatively integrates both process reward modeling (providing detailed reflection for reasoning steps) and outcome reward modeling (validating overall reasoning outcome correctness) \emph{without relying on human labels} (Figure \ref{fig:intro} (b)). %
Our framework shows effectiveness in refining LLM reasoning process\footnote{Our synthetically generated training data and source code will be made available for reproducibility.}.%

In summary, our contributions are threefold:
\begin{enumerate}[leftmargin=*,noitemsep,topsep=0pt]
    \item We propose an effective \textbf{\emph{contrastive reflection synthesis}} pipeline to generate error detection and correction instructions as verbal feedback  %
    from binary reasoning preference pairs.
    \item Built on the synthetic reflection data, we propose a \textbf{\emph{dual-model reasoning framework} \texttt{DARS}} consisting of a \emph{Reasoner} and a \emph{Critic} to perform more effective inference-time VRL.
    \item We have several novel empirical insights:
    \begin{itemize}[leftmargin=*,noitemsep,topsep=0pt]
        \item Our dual-model reasoning framework \textbf{\emph{outperforms}} %
        single Reasoner-based preference optimisation and maintained balanced performance across all metrics even in data scarcity. %
        \item Human evaluation confirms that Critic-generated reflection provides \textbf{\emph{actionable guidance}} that the Reasoner could \textbf{\emph{reliably follow}}.
        \item Contrary to prior findings, our experiments show that \textbf{\emph{increasing the size of the Critic model leads to better results than scaling the Reasoner}}.
    \end{itemize}
\end{enumerate}
