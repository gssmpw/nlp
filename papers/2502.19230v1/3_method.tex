
\section{Preliminary}
Existing ASAS systems primarily aim to automate teachers' complex reasoning processes on the assessment of short answer questions, typically operating within a classification paradigm~\cite{grading_classification,yue-aes-2017}. These systems take various contextual input, including \emph{question prompts}, \emph{key answer elements} (e.g., keywords or phrases that qualify for marks), \emph{marking rubrics} (e.g., criteria for assigning scores), and \emph{student responses}, and are trained to predict a \emph{score} as output.

Given a specific question, the dataset can be represented as \( D = \{(x_i, y_i)\}_{i=1}^{N} \), where \( x_i \) denotes a student's response and \( y_i \) represents the corresponding score assigned by human assessors. For each question, let \( \mathcal{K} = \{k_j\}_{j=1}^{M} \) represent the set of key answer elements, where \( M \) is the number of distinct elements expected in a complete answer. The scoring process can be formalized using a question-specific scoring function \( f_r(\cdot) \), which determines the final score based on the extend to which student's response includes the required elements:
\begin{equation}
y_i = f_r(\mathbf{v}(x_i, \mathcal{K})), \label{formula:assessment_problem_definenation}
\end{equation}
where \( \mathbf{v}(x_i, \mathcal{K}) \in \mathbb{R}^{M} \) is a multi-hot vector indicating the presence of each key element \( k_j \in \mathcal{K} \) in the student response \( x_i \).
This coverage vector is then mapped to the final score through \( f_r \).

\citet{li_emnlp2024} proposed a structured thought tree to imitate the human assessment process (as illustrated in Figure \ref{fig:thoughtTree}) %
and generate assessment rationales. We define thought trees as \( \mathcal{T}=\{\text{path}_l\}_{l=1}^{d} \), where each \( \text{path}_l \) represents a structured decision path, capturing a unique combination of binary assessments for key answer elements (e.g., Figure \ref{fig:thoughtTree}). Each path is encoded as:
\begin{equation}
    \hat{\mathbf{v}}(\mathcal{Z}_l) = [z_1^{(l)}, z_2^{(l)}, \dots, z_M^{(l)}],
\end{equation}
where \( z_j^{(l)} \in \{0,1\} \) indicates whether the \( j^\text{th} \) key element is correctly included or not. Prior work assumes that paths leading to a correct score is the \emph{preferred} or \emph{chosen} path ($\text{path}_l^{\text{chosen}}$), while paths resulting in an incorrect score is the \emph{rejected} path ($\text{path}_{l}^{\text{reject}}$). The rationales \( r_{\text{chosen}} \) and \( r_{\text{reject}} \) are then derived by summarizing the intermediate decisions along their respective paths. %

\begin{figure}[ht]
\centering
\includegraphics[width=0.8\linewidth]{figures/synthethic_data.pdf} 
\vspace{-2mm}
\caption{\small \textbf{Illustration of a thought tree and the identification of discrepancies in reasoning paths.}}
\vspace{-3mm}
\label{fig:thoughtTree}
\end{figure}

\begin{figure*}[t]
\centering
\includegraphics[width=\linewidth]{figures/framework.pdf}
\vspace{-6mm}
\caption{\textbf{(Left)}: Example conversation between the trained Reasoner and Critic. \textbf{(Right)}: Illustration of our \texttt{DARS} framework. A detailed explanation of this example is provided within training details in \textsection{\ref{sec:training}}. The context related to the Reasoner's initial mistake is highlighted in \textcolor{blue}{blue}, while the refinement is marked in \textcolor{red}{red}. Some question context is omitted in \textcircled{\raisebox{-0.3pt} {\scriptsize1}}, but the full example can be found in \textsection{\ref{sec:figure_full_example}}. Here, \textcircled{\raisebox{-0.3pt} {\scriptsize1}} represents the framework’s input, while the final response from the Reasoner before the Critic’s termination (\textcircled{\raisebox{-0.3pt} {\scriptsize4}}) serves as the framework’s output.}
\label{fig:framework}
\vspace{-4mm}
\end{figure*}



\section{\texttt{DARS}: Dual-Model Reflective Scoring} %
\subsection{Contrastive Reflection Synthesis}\label{sec:method_data_curation}



Existing approaches to enhancing LLM reasoning capabilities often rely on preference optimisation methods~\cite{Lu2024StepControlledDL, step_value_emnlp2024}, which are based on datasets annotated using binary pairwise comparisons under the Bradley-Terry model~\cite{bt}. However, these methods typically lack transparency, as they do not explain \textbf{why} one response is preferred over another.

This opacity makes it difficult for humans to understand or refine the reasoning process of LLMs. Furthermore, generating synthetic verbal reflections remains a challenge. Simple prompting of LLMs (e.g., GPT-4) often results in vague, superficial, or unhelpful rationales~\cite{Yin2024RelativePO, Jiang2024BridgingAM}\footnote{Empirical experiment analysis for this is provided in \textsection{\ref{sec:main_exps}}.}.
To address this, \textbf{\emph{we propose a pipeline that generates meaningful reflections based on a structured thought tree to explain why \( r_{\text{reject}} \) is less preferable than \( r_{\text{chosen}} \)}}.

\subsubsection*{Step 1: Identify Discrepancy in Reasoning Paths}
We first obtain a thought tree for each student's response \(x_i\), and analyze the discrepancy between rationale preference pairs by comparing the differences between their corresponding multi-hot vectors (as shown in Figure \ref{fig:thoughtTree}):
\begin{equation}
    \Delta\mathbf{v} = \hat{\mathbf{v}}(\mathcal{Z}_{l}^{\text{chosen}}) - \hat{\mathbf{v}}(\mathcal{Z}_{l}^{\text{reject}}),
\end{equation}
where $\Delta\mathbf{v}$ is the difference vector highlighting discrepancy between $\text{path}_l^{\text{chosen}}$ and $\text{path}_l^{\text{reject}}$. Each dimension $\Delta_j$ in $\Delta\mathbf{v}$ corresponds to a key element, specifically:
\[
\small
\Delta_j = 
\begin{cases} 
1 & \text{if decision for } k_j \text{ changed from 0 to 1}, \\ 
-1 & \text{if decision for } k_j \text{ changed from 1 to 0}, \\ 
0 & \text{if decision is the same}.
\end{cases}
\]
For each key element where \( \Delta_j \neq 0 \), we construct a hint prompt\footnote{A detailed prompt template is provided in \textsection{\ref{sec:further_experiment_setup}}.} that highlights the differences in the intermediate assessment decisions (e.g. \(r_{\text{reject}}\) missed $k_j$ that the student has already included):
\begin{equation}
    \text{hint}_{\Delta\mathbf{v}} = \text{Prompt}(\Delta\mathbf{v}, K).
\end{equation}

\subsubsection*{Step 2: Generate Synthetic Reflections}
After identifying discrepancies and constructing the hint prompt, we prompt an LLM (e.g., GPT-4-turbo) to generate a verbal reflection between the preference pair \( r_{\text{reject}} \) and \( r_{\text{chosen}} \):
\begin{equation}
    r_{\text{reflect}} = \texttt{LLM}_{\theta}(x_i, r_{\text{reject}}, r_{\text{chosen}}, \text{hint}_{\Delta\mathbf{v}}),
    \label{eq:r_pref}
\end{equation}
where the LLM synthesizes a verbal reflection to guide another LLM in refining its reasoning by transitioning from the incorrect rationale $r_{\text{reject}}$ to the correct rationale $r_{\text{chosen}}$.
\subsection{Dual-Model Reasoning} \label{sec:reasoning_framework}
We propose an on-policy dual-model framework comprising a \textbf{Reasoner} (\(\mathcal{R}\)) and a \textbf{Critic} (\(\mathcal{C}\)), where \(\mathcal{C}\) provides explicit verbal reflections to guide \(\mathcal{R}\)’s reasoning process. Unlike traditional reward models, which rely on scalar scores or implicit preference rankings, our approach incorporates verbal reflections to iteratively improve the outputs of \(\mathcal{R}\). 

However, since verbal reflection is inherently non-differentiable, we use a sampling-based approach to train the Reasoner and Critic models independently, transforming it into an off-policy process. During inference, these models collaborate through an iterative VRL process. Our framework not only enhances transparency in the ASAS task but also improves the reasoning process. Figure~\ref{fig:framework} illustrates our framework with an example illustrating how the Reasoner progressively refines its reasoning based on the Critic’s detailed reflection.

\subsection*{Training Reasoner and Critic Models} \label{sec:training}
Build on the synthetic reflection data generated in \textsection{\ref{sec:method_data_curation}}, we create diverse data combinations to train \(\mathcal{R}\) and \(\mathcal{C}\) on refinement and reflection capabilities:

\paragraph{Reasoner (\(\mathcal{R}\))}
The training data for the Reasoner is designed to include two capabilities:
\begin{enumerate}[leftmargin=*,noitemsep,topsep=0pt]
    \item \textbf{Task Capability}: \(\mathcal{R}\) takes \textcircled{\raisebox{-0.3pt} {\scriptsize1}} (question context and student answer) as input, and predicts \textcircled{\raisebox{-0.3pt} {\scriptsize2}}  (an initial assessment, e.g., $r_{\text{reject}}$ or $r_{\text{chosen}}$).
    \item \textbf{Refinement}: \(\mathcal{R}\) takes \textcircled{\raisebox{-0.3pt} {\scriptsize1}} \& \textcircled{\raisebox{-0.3pt} {\scriptsize2}} (history of assessment, e.g., $r_{\text{reject}}$), with \textcircled{\raisebox{-0.3pt} {\scriptsize3}} (\(\mathcal{C}\) generated reflection) as input, and predict \textcircled{\raisebox{-0.3pt} {\scriptsize4}} (an refined assessment, e.g., $r_{\text{chosen}}$).
\end{enumerate}
\vspace{-2mm}
\paragraph{Critic (\(\mathcal{C}\))}
The training data for the Critic is designed to include two capabilities:
\begin{enumerate}[leftmargin=*,noitemsep,topsep=0pt]
\item \textbf{Reflection}: If the assessment is incorrect, \(\mathcal{C}\) is trained to take previous assessment histories (e.g., \textcircled{\raisebox{-0.3pt} {\scriptsize1}}-\textcircled{\raisebox{-0.3pt} {\scriptsize2}} or \textcircled{\raisebox{-0.3pt} {\scriptsize1}}-\textcircled{\raisebox{-0.3pt} {\scriptsize4}}) as input, and predict \textcircled{\raisebox{-0.3pt} {\scriptsize3}} (a reflection instruction $r_{\text{reflect}}$ for \(\mathcal{R}\)) as output.
\item \textbf{When to Stop}: \(\mathcal{C}\) takes \(\mathcal{R}\)'s previous assessment outcome (e.g., $r_{\text{reject}}$ or $r_{\text{chosen}}$), either from single-round \textcircled{\raisebox{-0.3pt} {\scriptsize1}}-\textcircled{\raisebox{-0.3pt} {\scriptsize2}} or multi-rounds \textcircled{\raisebox{-0.3pt} {\scriptsize1}}-\textcircled{\raisebox{-0.3pt} {\scriptsize4}} as input, and validate the correctness of the assessment. If the assessment is correct, \(\mathcal{C}\) predict \textcircled{\raisebox{-0.3pt} {\scriptsize5}}, a special token that signals the termination of the reasoning loop and outputs the final assessment generated by \(\mathcal{R}\).
\end{enumerate}
Our Critic model is innovatively trained to perform both \emph{process reward modelling (Reflection})~\cite{lightman2024lets, wang-etal-2024-math} and \emph{outcome reward modelling (When to Stop)}~\cite{instruct_following}, without the need for comparisons with oracle labels~\cite{shinn2023reflexion, kim2023language} or manually setting the maximum iterations to terminate the refinement process~\cite{selfrefine, li-etal-2024-hindsight}.
\vspace{-2mm}
\subsection*{Inference-Time Iterative Refinement}
Once the Reasoner and Critic models are trained, they could collaborate to refine the assessment rationale at inference time through iterative conversations. At each iteration step \( t \), \(\mathcal{R}\) generates an assessment trajectory \(\hat{y}_r^{0}, \hat{y}_r^{1}, ...,\hat{y}_r^{T}\): 
\vspace{-2mm}
\begin{equation}
\begin{aligned}
&\textbf{Initialization:} \quad \hat{y}_r^0 = \mathcal{R}\bigl(x_i\bigr) \\
&\textbf{Iterative~Reflection:} \quad \\
&\begin{cases}
\hat{y}_r^{(t+1)} = \mathcal{R}\Bigl(\hat{y}_r^{t},\mathcal{C}(\hat{y}_r^{t})\Bigr), 
& \small\text{if } \mathcal{C}(\hat{y}_r^{t})=\mathrm{Reflection},\\
\hat{y}_r^T = \hat{y}_r^{t},
& \small\text{if } \mathcal{C}(\hat{y}_r^{t})=\mathrm{[STOP]}.
\end{cases}
\end{aligned}\notag
\label{eq:reasoner_critic_update}
\end{equation}
\noindent
Here, \(\mathcal{C}(\cdot)\) checks the correctness of \(\hat{y}_r^{t}\). If refinement is needed, it generates a verbal reflection for \(\mathcal{R}\) to refine \(\hat{y}_r^{t}\), otherwise, a ``Terminate'' signal is triggered, and the final assessment \(\hat{y}_r^T\) is taken as the output.
