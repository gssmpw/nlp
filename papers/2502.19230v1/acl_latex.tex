\pdfoutput=1

\documentclass[11pt]{article}

\usepackage[final]{acl}

\usepackage{times}
\usepackage{latexsym}

\usepackage[T1]{fontenc}

\usepackage[utf8]{inputenc}

\usepackage{microtype}

\usepackage{inconsolata}

\usepackage{graphicx}

\usepackage{soul}
\definecolor{lightblue}{rgb}{0.68, 0.85, 0.9}
\sethlcolor{lightblue}

\usepackage{amsmath,amssymb,amsthm,amsfonts}
\usepackage{dsfont}

\usepackage{geometry}
\usepackage{booktabs}
\usepackage{makecell}
\usepackage{multirow}

\usepackage[most]{tcolorbox}
\usepackage{fontawesome5}
\definecolor{yellow}{HTML}{F6BD60}
\usepackage{multicol}

\definecolor{hilight}{HTML}{000000}
\sethlcolor{hilight}

\usepackage{rotating}
\usepackage{tabularx}

\usepackage{algorithm}
\usepackage{algpseudocode}

\usepackage{colortbl}

\usepackage[most]{tcolorbox}

\usepackage{enumitem} %

\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}
\theoremstyle{remark}
\newtheorem{remark}{Remark}
            




\linespread{0.98}


\title{Two Heads Are Better Than One: Dual-Model Verbal Reflection at Inference-Time}




\author{Jiazheng Li$^1$,\quad Yuxiang Zhou$^1$,\quad Junru Lu$^4$,\quad Gladys Tyen$^5$\thanks{Now at Google DeepMind.},\\
{\bf Lin Gui$^1$,\quad Cesare Aloisi$^2$,\quad Yulan He$^{1,3}$}\\ 
  $^1$King's College London\quad\quad$^2$AQA\quad\quad$^3$The Alan Turing Institute\\$^4$Tencent YouTu Lab\quad\quad $^5$University of Cambridge\\
\texttt{caloisi@aqa.org.uk, junrulu@tencent.com, gladys.tyen@cl.cam.ac.uk},\\
\texttt{\{jiazheng.li, yuxiang.zhou, lin.gui, yulan.he\}@kcl.ac.uk}}



\begin{document}
\maketitle

\begin{abstract}
Large Language Models (LLMs) often struggle with complex reasoning scenarios. While preference optimization methods enhance reasoning performance through training, they often lack transparency in why one reasoning outcome is preferred over another. Verbal reflection techniques improve explainability but are limited in LLMs' critique and refinement capacity. To address these challenges, we introduce a contrastive reflection synthesis pipeline that enhances the accuracy and depth of LLM-generated reflections. We further propose a dual-model reasoning framework within a verbal reinforcement learning paradigm, decoupling inference-time self-reflection into specialized, trained models for reasoning critique and refinement. Extensive experiments show that our framework outperforms traditional preference optimization methods across all evaluation metrics. Our findings also show that ``two heads are better than one'', demonstrating that a collaborative Reasoner-Critic model achieves superior reasoning performance and transparency, compared to single-model approaches.
\end{abstract}

\input{1_introduction}
\input{2_related_work}
\input{3_method}
\input{4_experiment}


\section{Conclusion and Discussion}
We proposed a novel approach to enhance inference-time reasoning in LLMs through a dual-model framework. Our approach introduces a contrastive reflection synthesis pipeline, which generates verbal reflections that significantly improved reasoning explainability. Our framework, consisting of a dedicated Reasoner and Critic, enables effective reasoning refinement without relying on oracle labels. %
Moreover, our carefully designed training process equips both models with capabilities that extend beyond task-specific reasoning. The Reasoner not only solves problems but also learns to refine its reasoning based on feedback, while the Critic not only identifies errors but also learns when to stop, ensuring more efficient reasoning improvement. This capacity aligns with reasoning LLM advances seen in models like DeepSeek-R1 and OpenAI’s O1, where inference-time reflection enables iterative, self-correcting reasoning. Although our experiments focus on ASAS, the adaptability of the thought tree and the reflection synthesis process make it possible to extend our framework to other complex reasoning tasks in future work.



\section*{Acknowledgments}
This work was supported in part by the UK Engineering and Physical Sciences Research Council through a Turing AI Fellowship (grant no. EP/V020579/1, EP/V020579/2). JL is funded by a PhD scholarship provided by AQA. We thank Hainiu Xu and Ruobing Wang for their advice on formatting for this paper.

\section*{Limitations}
This study has two primary limitations. First, the training process requires substantial computational resources. While our framework minimizes the need for future retraining, the SFT training for both the Reasoner and Critic involves additional data points to enhance the model’s various capabilities, leading to higher training FLOPs than single Reasoner approaches. Second, the generalizability of our framework to tasks beyond ASAS remains unexplored. Although we conducted a comprehensive evaluation across six datasets, our focus was predominantly on the ASAS task. Future work should investigate the applicability of the proposed framework to a broader range of tasks.

\section*{Ethics Statement}
This study utilized both public and private datasets of anonymized student responses, none of which contain sensitive or personally identifiable information. We thoroughly reviewed the LLM's outputs and did not identify any instances of harmful content or exposure to personal information. Nevertheless, before deploying our framework in high-stakes examination settings, experts must carefully evaluate its assessment decisions and the underlying rationales to ensure reliability and fairness.

\bibliography{custom}

\clearpage

\appendix
\setcounter{table}{0}
\renewcommand{\thetable}{A\arabic{table}}
\setcounter{figure}{0}
\renewcommand{\thefigure}{A\arabic{figure}}

\input{5_experiment_setup}
\input{7_further_exps}

\end{document}
