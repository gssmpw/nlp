
\documentclass[10pt]{article} % For LaTeX2e
\usepackage[preprint]{tmlr}
% If accepted, instead use the following line for the camera-ready submission:
%\usepackage[accepted]{tmlr}
% To de-anonymize and remove mentions to TMLR (for example for posting to preprint servers), instead use the following:
%\usepackage[preprint]{tmlr}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}

\usepackage{multirow}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[pdftex]{graphicx}
\usepackage{subcaption}
\usepackage{tabularx}
\usepackage{siunitx}
\usepackage{array}
\usepackage{natbib}

\usepackage{adjustbox}
\usepackage[framemethod=TikZ]{mdframed}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{enumitem}
\usepackage{booktabs}
\usepackage{fontawesome5}
\usepackage{xcolor}
\usepackage{makecell}
\usepackage{amsmath, amsthm}
\usepackage{lineno}

\newtheorem{definition}{Definition}

\usepackage{array}
\newcolumntype{L}[1]{>{\raggedright\arraybackslash}p{#1}}
\newcolumntype{C}[1]{>{\centering\arraybackslash}m{#1}}
\newcolumntype{R}[1]{>{\raggedleft\arraybackslash}p{#1}}

\mdfdefinestyle{takeawaybox}{
    backgroundcolor=gray!30,
    shadow=false,
    innerleftmargin=5pt,
    innerrightmargin=5pt,
    roundcorner=10pt,
    skipabove=12pt
}


\title{Taxonomy, Opportunities, and Challenges of\\Representation Engineering for Large Language Models}

% Authors must not appear in the submitted version. They should be hidden
% as long as the tmlr package is used without the [accepted] or [preprint] options.
% Non-anonymous submissions will be rejected without review.

\author{
    Jan Wehner$^{1\footnotemark[1]}$\hspace{3pt},\quad Sahar Abdelnabi$^2$,\quad Daniel Tan$^{3}$,\quad David Krueger$^{4}$,\quad Mario Fritz$^{1}$\\
    \\
    $^{1}$\textnormal{\textit{CISPA Helmholtz Center for Information Security}}\\
    $^{2}$\textnormal{\textit{Microsoft}}\\
    $^{3}$\textnormal{\textit{University College London}}\\
    $^{4}$\textnormal{\textit{Mila, University of Montreal}}
    }

\footnotetext[1]{
Correspondence to jan.wehner@cispa.de
}

% The \author macro works with any number of authors. Use \AND 
% to separate the names and addresses of multiple authors.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\def\month{MM}  % Insert correct month for camera-ready version
\def\year{YYYY} % Insert correct year for camera-ready version
\def\openreview{\url{https://openreview.net/forum?id=XXXX}} % Insert correct link to OpenReview for camera-ready version


\begin{document}


\maketitle

\begin{abstract}
Representation Engineering (RepE) is a novel paradigm for controlling the behavior of LLMs. Unlike traditional approaches that modify inputs or fine-tune the model, RepE directly manipulates the model's internal representations. As a result, it may offer more effective, interpretable, data-efficient, and flexible control over models' behavior. We present the first comprehensive survey of RepE for LLMs, reviewing the rapidly growing literature to address key questions: What RepE methods exist and how do they differ? For what concepts and problems has RepE been applied? What are the strengths and weaknesses of RepE compared to other methods? To answer these, we propose a unified framework describing RepE as a pipeline comprising representation identification, operationalization, and control. We posit that while RepE methods offer significant potential, challenges remain, including managing multiple concepts, ensuring reliability, and preserving models' performance. 
Towards improving RepE, we identify opportunities for experimental and methodological improvements and construct a guide for best practices. 
\end{abstract}

\section{Introduction}
Prompting and fine-tuning are common and effective methods for controlling the behavior of Large Language Models (LLMs). But recently, a new paradigm for controlling LLMs inspired by interpretability research has emerged: Representation Engineering (RepE). Instead of adapting the inputs or training the weights towards outputs, Representation Engineering controls the LLMs' behavior by manipulating its internal representations. For this, it first identifies how a human-understandable concept is represented in the network's activations. Next, it uses that knowledge to steer the model's representations, thus, controlling its behavior (see Figure \ref{fig:intro_figure}). 
\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.7\linewidth]{figures/Intro_figure.pdf}
    \caption{Representation Engineering first \textbf{identifies} how a concept is represented in the activation space of the model and then steers that representation to \textbf{control} the model's behavior.}
    \label{fig:intro_figure}
\end{figure}

By tapping into models' representations, RepE offers two main advantages: (1) \textbf{Improve understanding}: RepE identifies how human-understandable concepts are represented in the models' activation space; steering that representation can verify whether the representation has the expected influence on the outputs. (2) \textbf{Control}: RepE is promising as a powerful tool to control the behavior, personality, encoded beliefs, and performance of an LLM, allowing us to make models behave in safe and desired ways. Since no training is required, RepE can be cheaper, more data efficient, and more flexible to different users and situations than other methods while causing less deterioration to the model's performance. 

Initial work on Activation Steering \citep{turner2024steeringlanguagemodelsactivation,li2023inference} was built on the assumption that concepts are represented as linear directions in the activation space of LLMs \citep{park2024the}. These methods focused on the difference in activations for inputs that are positive or negative with regards to the concept. They identify a vector that captures the model's representation of a concept and can be added to the activations on new inputs to modulate the intensity of the concept. Since then, a range of new RepE methods have been proposed. 
For example, such methods identify representations by finding interventions that lead to desired outputs \citep{cao2024personalized} or learning internal features in an unsupervised fashion \citep{templeton2024scaling}. They go beyond static and linear representations \citep{qiu2024spectraleditingactivationslarge} by employing different operators than a vector \citep{postmus2024steeringlargelanguagemodels}. And they experiment with new functions for steering activations \citep{singh2024representation}, as well as modifying the weights of the model by adding adapters \citep{wu2024reftrepresentationfinetuninglanguage}.

However, the literature on RepE lacks an overview and there exist many unanswered questions and conceptual confusions. In our work, we systematically answer the following questions.

\vspace{-10pt}
\begin{itemize}[itemsep=0pt,leftmargin=15pt]
    \item \textbf{What is Representation Engineering?} We provide the first unification of different approaches to RepE with a framework that describes them as pipelines that identify, operationalize, and control representations.
    
    \item \textbf{What RepE methods exist and how do they differ?} We discuss and contrast different methods for each step in this pipeline.

    \item \textbf{How has RepE been applied?} We describe how RepE has been applied to areas such as AI Safety, Ethics, and Interpretability and show which concepts are more amendable to be controlled by RepE.

    \item \textbf{How does it compare to other methods?} We contrast RepE to related methods and provide a meta-survey of comparisons between RepE and fine-tuning, prompting, and decoding-based methods.

    \item \textbf{Why does it work?} We suggest reasons why RepE works so effectively.

    \item \textbf{What are strengths and weaknesses of RepE?} We provide a list of advantages and challenges of RepE. 

    \item \textbf{What opportunities for future research are there?} We suggest broad themes and concrete ideas for improvements and outline opportunities for strengthening the field.
\end{itemize}
\vspace{-10pt}

To answer these questions we conduct a thorough literature review, collecting detailed information from $>$130 papers. 
Although previous surveys on Mechanistic Interpretability \citep{ferrando2024primer,bereska2024mechanistic} and frameworks for causal interpretability \citep{mueller2024questrightmediatorhistory,geiger2024causalabstractiontheoreticalfoundation} have touched on RepE, there is no dedicated survey of this research area. A survey is especially pressing since over 100 papers have been released within the last year without a systematization of the literature. In addition to Activation Steering, we include methods that modify the weights, identify concept representations through optimizing for outputs, and are not based on vectors. We focus on methods that apply control on intermediate representations of a model without fully replacing them, thus excluding soft-prompting, decoding-based methods, and Activation Patching. For a detailed description of the inclusion criteria and literature search process, see Appendix \ref{sec:methodology}. 

We conclude that future work needs to build ways to benchmark RepE methods to enable thorough comparisons. In addition, we identify opportunities in considering representations that are non-linear, multi-concept, have interactions between layers or trajectories over time. Furthermore, there are a number of promising directions for new applications, and improved concept identification and control methods.
To make RepE usable in practice, we will need to see improvements in its ability for multi-concept steering, long-form generation, reliability, Out-of-Distribution (OOD) robustness, and maintenance of general capabilities.

% \vspace{20pt}

\setcounter{tocdepth}{2}
\tableofcontents
\newpage

\section{What is Representation Engineering?}
\label{sec:definition_goals}
Representation Engineering (RepE) is a class of techniques that: \textbf{Manipulate representations of a model in order to control its behavior with regard to a concept}.

To achieve this, a pipeline with two steps is necessary: First, we identify how the targeted concept is represented within the model. Secondly, we use that information to steer the model's representations on new inputs. In this context, a concept can be understood broadly as any human-understandable feature like behaviors, tasks, entities, pieces of knowledge, or personality traits. RepE assumes that such concepts are represented in LLMs. Representations are the internal structures or encodings that a model uses to capture and process information about a concept.

The goal of \textbf{Representation Identification} is to produce a \textbf{concept operator} that accurately captures the model's representation of a concept. The concept operator is an object, for example, a vector, with a specific intended relationship to the model's actual concept representation. This bakes in assumptions about the geometry and location of concept representations.

The concept operator is then used for \textbf{Representation Control} by steering the representations of the model. For this a \textbf{steering function} is employed that uses the concept operator to manipulate the activations or weights of the model. If the concept operator accurately reflects the model's representations and the steering function is effective, this will steer the model's representations and, thus, control the outputs of the model.

RepE is also sometimes referred to as Activation Engineering or Activation Steering. However, we chose the term Representation Engineering to emphasize that representations can also be controlled by modifying the weights of a model.

\subsection{Illustrative Examples}
To illustrate RepE, we provide an example inspired by \citet{zou_representation_2023}, where RepE is applied to make an LLM give more truthful answers. To identify how truthfulness is represented, inputs are provided that instruct the model to be honest or to lie. Next, the activations for these inputs are collected. Then, the difference between activations for honest and dishonest inputs is calculated. This produces a vector that denotes a direction in the activation space which represents honesty. 
This vector can be used to guide the training of a LoRA module that generates activations more aligned with the truthfulness direction. This effectively steers the model to give more truthful answers.

In another example inspired by \citet{cao2024personalized}, RepE is applied to align the outputs of an LLM with the preferences of a human. For this, they find how activations need to be changed to lead to desired outputs. Given a dataset of desired and undesired outputs, a vector is optimized that can be added to the activations during a forward pass to make it more likely for the desired output to be generated. When processing new inputs, that vector can be added to the activations, thus, steering the model towards human preferences.

\subsection{Advantages of Representation Engineering}
\textbf{Sample Efficient.} RepE can be effective with a low number of training examples~\citep{wang2024model}. Furthermore, some RepE methods do not require a labeled dataset (see Section \ref{subsec:pipelines}). These properties make it easier and cheaper to employ RepE.

\textbf{Flexible.} The steering of RepE can be turned on or off flexibly. It is also possible to dynamically adjust the steering per request, thus enabling personalization and context-dependent control~\citep{cao2024personalized,stickland2024steeringeffectsimprovingpostdeployment,10.1145/3626772.3657819,chen2024designing,lucchetti2024activation}.

\textbf{Low Impact on Capabilities.}
As shown in Section \ref{subsec:comparing}, RepE tends to not strongly degrade the capabilities of the model~\citep{panickssery2024steeringllama2contrastive,stickland2024steeringeffectsimprovingpostdeployment,10.1145/3626772.3657819,qiu2024spectraleditingactivationslarge,van2024extending}. Thus, it is possible to use RepE practically in production without sacrificing model quality.

\textbf{Precise Control.} RepE offers direct control over the representations of a concept. This makes it possible to apply precise steering at the granularity of single concepts.

\textbf{Efficient During Inference.} Methods for Representation Control come at a negligible increase in computational cost during inference~\citep{li2023inference}. Most methods do not require a large number of additional parameters, and none require additional inference steps.

\textbf{Causally Verifiable Interpretability.} By observing the effect of steering a representation on the output, we can verify that the identified representation causes the expected change~\citep{marks_geometry_2023,arora2024causalgym}. This can make us more confident that the right representation for a concept was found.

\textbf{Tacit Interpretability.} By giving users access to knobs they can turn to influence internal, human-understandable representations, RepE can provide tacit understanding to non-expert users about the inner workings of LLMs \citep{chen2024designing}.

\textbf{Advantages over Other Methods.} Our meta-survey in Section \ref{subsec:comparing} indicates that RepE tends to be more effective at controlling models' behavior than alternative methods like prompting, fine-tuning, or decoding-based methods.

\textbf{Compatible with Other Methods.} RepE can be used in addition to other methods for stronger control (see Section \ref{subsec:comparing}). This enables the use of RepE in a defense-in-depth approach, where multiple imperfect methods are combined to achieve strong safety guards.

\begin{mdframed}[style=takeawaybox]
\textbf{Takeaway: What is Representation Engineering?} \newline
RepE manipulates the representations of a model in order to control its behavior w.r.t. a concept. This is done by identifying how a concept is represented within the model and then using that knowledge to steer the representations.
It allows for effective, efficient, flexible, and precise control over models' behavior while largely retaining quality. 
\end{mdframed}

\section{Framework and Notation}
\label{sec:framework}
Building on the background and definitions in Section~\ref{sec:definition_goals}, we propose a framework that classifies RepE pipelines based on the method they use for Representation Identification, Representation Control and how they operationalize representations (see Figure \ref{fig:taxonomy}). Additionally, we provide a unifying formalization of RepE that showcases the function signatures of the steps of RepE pipelines. For an overview of the notation, refer to Table \ref{tab:notation}.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.7\linewidth]{figures/framework_new.pdf}
    \caption{Framework of Representation Engineering pipelines. One Representation Identification method is used to identify a concept operator. Representations are operationalized by assuming a geometry of representations. The concept operator is used to steer the weights or activations of the model.
    }
    \label{fig:taxonomy}
\end{figure}

\begin{table}[ht]
    \caption{Explanation of Notation for ease. Large letters $C,S,X,Y,A,O$ refer to a set of the corresponding elements.}
    \label{tab:notation}
    \begin{center}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{lll}
        \toprule
        Notation & Name & Explanation\\
        \midrule
        $c$ & Target concept & The concept we want to control \\
        $\mathcal{M}$ & Model & The LLM on which RepE is applied\\
        $s$ & String/set of strings & The text or set of texts \\
        $x\in S$ & Input & String given as inputs to the model\\
        $y\in S$ & Output & String outputted by the model\\
        $s^{c^{+/-/0}}$ & Positive/negative/neutral string & Text that is positively/negatively/neutrally related to the target concept \\
        $w_l$ & Weights & Weights of model $M$ at layer $l$\\
        $a_l,m,p$ & Activations & Activations at layer $l$, model component $m$, and token position $p$\\
        $\text{RI}$ & Representation Identification & Methods that output the concept operator \\
        $\text{RC}$ & Representation Control & Methods that steer the model's representations\\
        $o^c$ & Concept operator & An object denoting the model's representation of the concept\\
        $F$ & Steering function & A function that modifies the weights or activations using $O^c$\\
         \bottomrule
    \end{tabular}}
    \end{center}
\end{table}

\subsection{Representation Identification}
Representation Identification (RI) is the first step in the RepE pipeline. It aims to find out how the concept is represented in the LLM's activations. For this, an RI method needs to provide a stimulus related to the concept, collect the model's activations, and calculate the representation of the concept from the collected activations. The result is a concept operator that denotes the model's representation of the concept.
\newpage
\begin{definition}
Representation Identification \(\text{RI}\) is a method
\[
\text{RI}(M,S)\rightarrow o^c
\]
that takes 
\begin{itemize}
    \item a model \(\mathcal{M}\)
    \item a set of strings \(S\) that have a positive, negative, neutral, or an unknown relationship to the concept:\\
        \(
        S = S^{c^+} \cup S^{c^-} \cup S^{c^\varnothing} \cup S^{c^?},
        \)
\end{itemize}
and returns a concept operator \(o^c\) that denotes the concept's representation.
\end{definition}

Methods for Representation Identification need to carry out three steps. First, they need to \textit{provide a stimulus} related to the concept. These are strings used as inputs or ground-truth outputs that have a positive, negative, neutral, or unknown relationship to the concept. This means that they exemplify or contain the concept, serve as counterexamples to the concept, are unrelated to the concept, or that their relationship to the concept is unknown. This relationship is usually predefined by humans, e.g., as labels in a dataset. However, in some cases, it is evaluated dynamically during the execution of the method. It is important that these strings actually activate the desired concept's representation, so it is present in the activations. 
Secondly, they \textit{collect activations} of the model related to these strings. 
\begin{definition}
    The activation collection function $Act$
    \[
        Act(\mathcal{M},X^c,l,m,p)\rightarrow A_{l,m,p}^c
    \]
    extracts activations of model $\mathcal{M}$ for inputs $X$ at layer $l$, model component $m$, and token position $p$
\end{definition}
Crucially, our knowledge about the strings' relationship to the concept lets us infer whether the concept representation is present in the activations for that input. 
Thirdly, they \textit{calculate the concept operator} from these collected activations. This can range in complexity from a simple arithmetic operation to a large training run.

There are three broad categories of approaches to Representation Identification. Methods that employ \textit{Input Reading} provide inputs related to the concept, read out their activations, and then calculate the operator from the resulting sets of activations. Those that do \textit{Output Optimization} utilize a way of scoring outputs with regard to the concept to optimize the activations accordingly.
A concept operator can then be derived from the optimized activations. 
Finally, other methods employ \textit{Unsupervised Feature Learning}, where they first learn internal features from activations in an unsupervised way. We can identify the feature corresponding to the targeted concept by studying inputs and outputs related to that feature. 


\subsection{Operationalizing Representations}
To be able to identify and control representations, RepE pipelines need to operationalize how the model represents the concept. 
This introduces assumptions about the structure and geometry the model uses to represent the concept in the activations. For example, concepts could be represented as linear directions in the activation space \citep{park2024the} or as non-linear manifolds. Furthermore, a concept operator $o^c$ is necessary to denote how the model represents the concept. $o^c$ results from the Representation Identification step and is used in Representation Control. It can take various forms and differs in its intended relationship to the model's actual representation of the concept.


\subsection{Representation Control}
Representation Control (RC) is the second step in a RepE pipeline. RC methods should steer the representation of a concept in order to control the model's behavior w.r.t. that concept. For this, they employ a steering function $F$ that uses the concept operator $o^c$ to modify the model's activations or weights. While $F$ only describes a mathematical operation, RC encapsulates the whole process of where, when, and how $F$ is applied. Employing RC should result in controlled outputs $y^c$ that are more aligned with the concept $c$ than an originally uncontrolled output $y^{\text{org}}$. 

\begin{definition}
    Representation Control RC
    \[
    \text{RC}(F,o^c,\mathcal{M},x)\rightarrow y^c
    \]
    applies the steering function \(F\) and concept operator \(o^c\) to control the representations of model \(\mathcal{M}\) on inputs \(x\). This leads to outputs \(y^c\) that have been controlled with regard to \(c\).
\end{definition}

To steer the representations of a model, we can either modify its activations during inference or adapt its weights. 
Activations of unseen inputs can be adapted during inference by shifting them using the concept operator.

\begin{definition}
    An activation steering function
    \[
    F^a(a_l, o^c_l) \rightarrow a_l^c
    \]
    adapts the activations \(a_{l}\) at layer \(l\), using \(o^c_l\) to attain the steered activations \(a^c_{l}\).
    \label{def:f_a}
\end{definition}

In addition, we can modify weights to steer the activations they output in desired ways. 

\begin{definition}
    A weight steering function
    \[
    F^w(w_l, o^c_l) \rightarrow w_l^c
    \]
    modifies the weights \(w_l\) using \(o^c_l\) so that
    \[
        w_l^c(a_{l-1}) \rightarrow a_l^c
    \]
    the modified weights \(w_l^c\) produce steered activations \(a_l^c\).
\end{definition}

Some steering functions apply simple operations, while others require retraining of the model. Furthermore, some RC methods are adaptive to the present context, while others always apply the same effect.
Weight steering functions only need to be applied once, while activation steering functions need to be applied repeatedly on every new input. Notably, the line between activation and weight steering functions can be blurry. For example, in an MLP layer, adding a steering vector to the activations is equivalent to adding it to the bias term.


\begin{mdframed}[style=takeawaybox]
\textbf{Takeaway: Framework \& Notation} \newline
RepE is a 2-step pipeline: 
\vspace{-10pt}
\begin{enumerate}[leftmargin=20pt,itemsep=-3pt]
    \item \textbf{Representation Identification} finds a \textbf{concept operator} that denotes the model's representation of the concept.
    \item \textbf{Representation Control} applies a \textbf{steering function} that uses the concept operator to modify the activations or weights of the model.
\end{enumerate}
\vspace{-10pt}
Further, RepE pipelines make assumptions about representations to \textbf{operationalize} them into a concept operator.
\end{mdframed}

\section{Representation Identification}
\label{sec:identifying}
Representation Identification (RI) aims to find a concept operator that denotes how the model represents the target concept in its activations. 
Methods can be based on Input Reading, Output Optimization, or Unsupervised Feature Learning. This section separately dives into these categories and identifies differences of methods within the categories.

\subsection{Input Reading}
\label{subsec:input}
RI methods that fall into the \textit{Input Reading} category provide inputs related to the concept, read out their activations, and finally calculate the operator from the resulting sets of activations.

\renewcommand{\arraystretch}{1.2}
\begin{table}[ht]
    \caption{Steps and differences between different \textbf{Input Reading} methods for \textbf{Representation Identification}.}
    \label{tab:overview_ir}
    \begin{center}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{p{0.25\linewidth} p{0.83\linewidth}}
        \toprule
        \textbf{Steps in Input Reading} & \textbf{How do methods differ?} \\
        \midrule
        \multirow{4}{*}{\hyperref[subsubsec:contructing_inputs]{1. Constructing Inputs}} & \textbf{Source} of inputs (existing datasets, manually written, or synthetically generated) \\
        & \textbf{Relationship} to the concept (positive, neutral, unrelated, or unknown) \\
        & \textbf{Differences} between contrastive sets (paired with concept-related difference, or unpaired) \\
        & \textbf{Prompting template} (pre-prompt concept-related instructions, A/B choices) \\
        \addlinespace
        \multirow{3}{*}{\hyperref[subsubsec:reading_activations]{2. Collecting Activations}} & \textbf{Layer} (all layers, or concept-related layers only)\\
        & \textbf{Model component} (residual stream, output of MLP or attention heads)\\
        & \textbf{Token position} (End of sequence, all tokens, or concept-related tokens) \\
        \addlinespace
        \multirow{2}{*}{\hyperref[subsubsec:calculate_operator]{\parbox{\linewidth}{3. Calculating the Concept Operator}}} & What \textbf{calculation}? (class-mean, probes, or components after dimensionality reduction) \\
        & Other \textbf{techniques} (refining via concept-related neurons, project to other spaces)\\
        \bottomrule
    \end{tabular}}
    \end{center}
\end{table}
\renewcommand{\arraystretch}{1.0}


\subsubsection{Constructing Inputs} 
\label{subsubsec:contructing_inputs}
Input Reading methods aim to activate the concept representation by providing inputs $S$ that are related to the concept. To do that, one needs to attain a relevant dataset, divide it into groups that differ in their relation to the concept, and put the inputs into a prompting format that successfully activates the concept's representation.

\textbf{Source of Inputs.} The data can be gathered from an existing dataset, manually created, or synthetically generated. A majority of papers use existing datasets like TruthfulQA \citep{lin2022truthfulqa} or ToxiGen \citep{hartvigsen2022toxigen}. Researchers can use the labels in the dataset to separate examples into different groups. Multiple papers also use manually created datasets \citep{li2024the,marks_geometry_2023}, which require human effort but may allow for more flexible and rigorous experimentation. Furthermore, it is also possible to prompt an LLM to synthetically generate the inputs. This is an efficient and flexible approach that allows targeting concepts for which no dataset exists. However, one might introduce confounders based on the LLM's biases and errors.

\textbf{Relationship to the Concept.} Typically, inputs are divided into multiple sets that differ in their relation to the concept. Inputs can be positive $s^{c^+}$, negative $s^{c^-}$, or neutral $s^{c^\varnothing}$ if they exemplify or contain the concept, serve as a counterexample, or are unrelated to the concept, respectively. Inputs can be classified based on whether they mention a certain topic, e.g., by checking for wedding-related keywords \citep{turner2024steeringlanguagemodelsactivation} or containing a specific word \citep{makelo2025evaluating}. More abstractly, inputs can differ in whether they exemplify a concept, e.g., being written in a specific style \citep{konen2024style}. Lastly, inputs can be divided based on the model's response to them, like classifying inputs based on whether the model answers them correctly \citep{yang-etal-2024-enhancing,hjer2025improving}.

\textbf{Prompting Template.} Next, the inputs are often formatted into a prompt that activates the concept's representations. Multiple papers use an A/B-choice format consisting of a question, two answer choices designated with “A:” and “B:”, and the letter corresponding to the answer \citep{panickssery2024steeringllama2contrastive}. Here, it is important to randomize the order, to not introduce the letter or position as a confounder. Another common template adds pre-prompts to a question that instruct the model towards or against the concept \citep{zou_representation_2023}.

\textbf{Differences Between Contrastive Sets.} Some methods construct contrastive pairs of inputs. These are derived from the same example but transformed into two contrasting inputs. For example, one can use the same question but attach a different pre-prompt \citep{zou_representation_2023} or answer to it \citep{panickssery2024steeringllama2contrastive}. \citet{deng2025rethinking} employ a matched pair trial design where pairs of inputs differ only in the roles assigned to them and are guaranteed to lead to opposite behaviors.
In other methods, the positive and negative inputs are unrelated to each other since they are not derived from the same example \citep{wu2024reftrepresentationfinetuninglanguage,xu2024uncoveringsafetyriskslarge,beaglehole2025aggregateconquerdetectingsteering}. Generally, contrastive pairs are preferable since they better isolate the target concept from other factors that could differ between the sets of inputs. However, for some settings, contrastive pairs can be harder to obtain.

\subsubsection{Collecting Activations}
\label{subsubsec:reading_activations}
The provided inputs are fed into the model to perform a forward pass for which the activations $\text{Act}(\mathcal{M},X^c,l,m,p)\rightarrow A_{l,m,p}^c$ are recorded at a specific layer $l$, model components $m$, and token positions $p$. It is possible to find the best options for these hyperparameters by testing the effectiveness of the resulting concept operator \citep{arditi2024refusallanguagemodelsmediated}.

\textbf{Layer.} It is a common practice to collect the activations from all layers and subsequently compute a concept operator for each layer. Alternatively, one can reduce computational cost by focusing on layers that are more likely to contain the concept's representation \citep{ghandeharioun2024whosaskinguserpersonas}.

\textbf{Model Component.} There are multiple distinct places within a transformer block at which activations can be collected. The most common ones are the residual stream after the MLP or the attention layer, the outputs of individual attention heads, and the outputs of an MLP or attention layer. Analyzing the MLP or attention heads gives insight into the computations done at that layer, while focusing on the residual stream is akin to analyzing the intermediate memory of a program.

\textbf{Token Position.} Some methods read the activations at a specific token position that has special significance towards the concept, commonly, the last token \citep{scalena2024multiproperty} or the end-of-sequence token \citep{rozanova-etal-2023-interventional}, the token containing an answer \citep{panickssery2024steeringllama2contrastive}, or the token mentioning the concept. Other methods collect activations for all tokens and then take their average to serve as a representation of the sequence \citep{li2024implicit}.

\subsubsection{Calculating the Concept Operator} 
\label{subsubsec:calculate_operator}
The activation collection step results in sets of activations that have different relationships to the concept. These can be used to calculate the concept operator. 
Assuming that we are given a positive and a negative set of activations $A^+_{l,m,p}, A^-_{l,m,p}$ with $n$ samples each, we need a function $f(A^+_{l,m,p}, A^-_{l,m,p}) \rightarrow o^c_{l,m}$ that calculates the concept operator.

\textbf{Difference in Means.} Taking the difference between the classes of activations can identify the direction that captures their differences. Difference-in-Means (DiM) is a popular method that simply calculates the average for both classes and subtracts the negative from the positive one \citep{panickssery2024steeringllama2contrastive,jorgensen2024improving,ball2024understandingjailbreaksuccessstudy}. This is equivalent to taking the average of the difference between pairs of examples $o^c=\frac{1}{n}\sum_{a^+,a^- \in A^+,A^-}a^+ - a^-$ . Earlier papers took the difference between a single pair of inputs \citep{turner2024steeringlanguagemodelsactivation,liu2023aligning}, however, it has been found that taking the difference across larger datasets leads to higher-quality operators \citep{chu2024causal,jorgensen2024improving}. \citet{arora2024causalgym} employ an unsupervised method that discovers 2 clusters of activations through $k$-means and subtracts their means.
\citet{im2025unifiedunderstandingevaluationsteering} show that DiM is optimal at identifying a vector that matches negative examples to positive ones. However, these vectors are not suited to steering positive examples where they lead to large performance reductions.

\textbf{Probes.} Probes are classifiers that predict whether activations belong to the positive or negative class $f_\theta(a) \rightarrow \{c^+,c^-\}$\citep{alain2018understandingintermediatelayersusing}. The weights $\theta$ of a probe should capture the concept's representations. Many papers employ logistics regression classifiers as probes and use the learned weight vector as the concept operator \citep{chen2024designing,10.1145/3626772.3657819,wang2024model}. Contrast Consistent Search identifies a linear probe that retains logical consistency
$P(c|x_i)=1-P(\text{not }c|x_i)$ \citep{burns2023discovering}. Others train multiple orthogonal probes and combine them through a weighted sum \citep{Chen_Sun_Jiao_Lian_Kang_Wang_Xu_2024}. Multiple papers use MLP-based probes, which are more expressive \citep{hoscilowicz2024nonlinearinferencetimeintervention,singh2024representation}. \citet{wang2024adaptive} use k-means clustering on the difference between positive and negative pairs to then train one linear probe per cluster, thus discovering multiple aspects of the concept. \citet{xu2024uncoveringsafetyriskslarge} find a vector that can be added to activations to prevent a linear probe from detecting the concept. However, \citet{im2025unifiedunderstandingevaluationsteering} critique that probes only capture the direction and not the magnitude necessary to match positive and negative examples.
\citet{beaglehole2025aggregateconquerdetectingsteering} train a probe that learns non-linear features from the activations, before deriving a matrix that captures the directions in activation space to which the probe is most sensitive. The top eigenvectors of these matrices are then used as concept operators. Lastly, \citet{nguyen2025multiattributesteeringlanguagemodels} train concept vectors to match the negative to positive activations distribution using a Maximum Mean Discrepancy loss that captures higher order differences in distributions like variance.

\textbf{Dimensionality Reduction.} Dimensionality Reduction techniques can help to identify the most important information about the concept. It is common to apply Principal Component Analysis to the difference between the two sets of activations $PCA(A^+-A^-)=\lambda_1,\lambda_2,...$ before selecting the first principle component as the concept operator $o^c=\lambda_1$ \citep{zou_representation_2023,adila2024discovering}. However, the direction with the highest variance does not necessarily capture the shift between positive and negative examples \citep{im2025unifiedunderstandingevaluationsteering}. 
Other papers use Singular Value Decomposition to identify the most important components of each group of inputs \citep{adila2024discovering}, their difference \citep{tlaie2024exploring,ma2025dressing}, or their covariance matrix \citep{qiu2024spectraleditingactivationslarge}. \citet{xiao2024enhancing} criticize that dimensionality reduction techniques lose crucial information and instead train a Gaussian Mixture Model to model the distribution of positive and negative activations.

\textbf{Additional Techniques.} 
In addition to these techniques, multiple papers attempt to identify neurons or attention heads which are more relevant to the concept and then only calculate the operator from those. The idea is to find a minimal steering intervention that causes fewer side effects by assuming that the concept's representation is only encoded in the selected neurons. \citet{li2023inference} and \citet{ma2025dressing} only intervene on the attention heads where a probe achieves high accuracy, and \citet{todd2024function} focuses on attention heads that have a causal effect on a specified task.
Other papers find important neurons by calculating attribution scores per neuron \citep{wu-etal-2024-mitigating-privacy}, by selecting ones that exhibit low variance on the difference between sets \citep{li2024rethinkingjailbreakinglensrepresentation} or by identifying causally influential nodes through Activation Patching \citep{xiao2024enhancing}. Another technique first projects activations into a different space before deriving an operator. For example, \citet{zhang2024truthxalleviatinghallucinationsediting} train an autoencoder that projects activations into a space where they are easily separable.

\subsection{Output Optimization}
\label{subsec:output}
Representation Identification methods that perform Output Optimization provide information about the concept by scoring outputs. This is done by first providing inputs and collecting their activations and outputs. Secondly, a loss function is constructed by evaluating the outputs with regard to the concept. Thirdly, an optimization algorithm is used to iteratively optimize the activations to achieve a high concept score.

\begin{table}[ht]
    \caption{Steps and differences between different \textbf{Output Optimization} methods for \textbf{Representation Identification}.}
    \label{tab:overview_oo}
    \begin{center}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{p{0.25\linewidth} p{0.78\linewidth}}
        \toprule
        \textbf{\makecell[l]{Steps In Output\\Optimization}} & \textbf{How do methods differ?} \\
        \midrule
        \multirow{4}{*}{\hyperref[subsubsec:coll_act]{1. Collecting Activations}} 
        & \textbf{Source} of inputs (existing datasets, manually written, or synthetically generated)\\
        & \textbf{Layer} (all layers, or concept-related layers only)\\
        & \textbf{Model component} (residual stream, output of MLP or attention heads)\\
        & \textbf{Token position} (End of sequence, all tokens, or concept-related tokens) \\
        \addlinespace
        \addlinespace
        \multirow{3}{*}{\hyperref[subsubsec:output:scoring]{2. Output Scoring}} 
        & \textbf{Output Format} (token, sentence, or long-form generation)\\
        & \textbf{Scoring} (dataset or scoring function)\\
        & \textbf{Loss Function} (Cross-Entropy, KL-divergence, regularization)\\
        \addlinespace
        \multirow{2}{*}{\hyperref[subsubsec:optimizing_activations]{3. Optimizing Activations}} 
        & \textbf{Optimization Algorithm} (SGD, iterative sampling)\\
        & \textbf{Deriving Concept Operator} (combined training, individual operators, averaging)\\
        \bottomrule
    \end{tabular}}
    \end{center}
\end{table}


\subsubsection{Collecting Activations}
\label{subsubsec:coll_act}
Output Optimization methods provide some inputs $X$ and collect the resulting activations that will be optimized $\text{Act}(\mathcal{M},X^c,l,m,p)\rightarrow A_{l,m,p}^c$ at a specific layer $l$, model components $m$, and token positions $p$.

\textbf{Source of Inputs.} Inputs $X$ can be taken from existing datasets \citep{wu2024reftrepresentationfinetuninglanguage}, or be generated manually or synthetically (as in Section \ref{subsubsec:contructing_inputs}). While they do not need a specific relationship to the concept, they are usually meant to produce outputs that are relevant to the concept. The inputs can be a query \citep{yin2024lofit,cao2024personalized,zeng2024privacyrestore} or a sentence that the model should transform \citep{subramani_etal_2022_extracting,konen2024style}.

\textbf{Layers, Model Components, and Token Positions.} Next, the activations of the model that will be optimized are chosen. Activations are usually collected at every layer from the attention heads, residual stream, or MLP or attention layer outputs. However, some papers focus on attention heads for which a linear probe achieves high accuracy \citep{yin2024lofit,zeng2024privacyrestore}.
Furthermore, we need to decide at which token positions the optimization will be applied. While some papers only focus on the activations during generation \citep{konen2024style}, others use the activations on input tokens \citep{cai2024selfcontrol}, a combination of input and output activations \citep{wu2024reftrepresentationfinetuninglanguage}, or activations of tokens with specific syntactic relevance to the concept, e.g. for the token naming a person \citep{hernandez2024inspecting}.

\subsubsection{Output Scoring}
\label{subsubsec:output:scoring}
The model generates outputs $M(X) \rightarrow Y$ that are scored according to the concept $\text{Score}_c(Y)$, which is used as a loss function $\mathcal{L}_{\text{Score}}(Y)$.

\textbf{Output Format.} Based on the input, we can let the model generate a single token \citep{hernandez2024inspecting}, a sentence \citep{subramani_etal_2022_extracting}, or a long-form generation \citep{cao2024personalized,wu2024advancing,wu2024reftrepresentationfinetuninglanguage}. 

\textbf{Scoring.} These outputs are now evaluated on how much they align with the concept. This provides information about the concept to the optimization process. 
The outputs can be evaluated by comparing them to ground-truth answers from a dataset or through a scoring function. 
A score can be derived from the log-probability the LLM assigns to the ground-truth token \citep{hernandez2024inspecting,wu2024reftrepresentationfinetuninglanguage} or sentence \citep{subramani_etal_2022_extracting,yin2024lofit} or from the difference in log-probabilities between a desired and undesired answer \citep{cao2024personalized,zeng2024privacyrestore}.
A scoring function can judge how much an output aligns with the concept. \citet{cai2024selfcontrol} ask an LLM whether the output is aligned with the concept and use its token probabilities on ``yes'' and ``no'' as a score.

\textbf{Loss Function.} The score is then transformed into a loss function. The Cross-Entropy loss is popular \citep{subramani_etal_2022_extracting,konen2024style,wu2024advancing,wu2024reftrepresentationfinetuninglanguage}. \citet{yin2024lofit} add a regularization term to induce sparsity and \citet{hernandez2024inspecting} additionally minimize the KL-divergence compared to the unsteered token distribution to minimize unwanted side-effects. Other papers adapt previously developed loss functions for preference optimization like DPO \citep{cao2024personalized} or ORPO \citep{zeng2024privacyrestore}. Lastly, \citet{wu2025axbenchsteeringllmssimple} suggest ReFT-r1, which optimizes a joint objective for the probing accuracy and steering effectiveness of a concept.

\subsubsection{Optimizing Activations}
\label{subsubsec:optimizing_activations}
A concept operator is optimized to modify activations so that they lead to outputs that score highly on the loss function $o^c=\text{argmin}_{o^c} \mathcal{L}_{\text{Score}}(\mathcal{M}(F(A,o^c))$.

\textbf{Optimization Algorithm.} After we have attained a loss function with regards to the concept, we can employ an optimization algorithm to adapt the activations to minimize that loss. The most common method for this is using Gradient Descent with the AdamW optimizer \citep{subramani_etal_2022_extracting,hernandez2024inspecting,yin2024lofit,cao2024personalized,wu2024advancing,wu2024reftrepresentationfinetuninglanguage}. \citet{cai2024selfcontrol} propose an iterative algorithm that samples outputs before backpropagating to a steering vector that makes the highest scoring output more likely.

\textbf{Derive Concept Operator.} The concept operator is optimized for the loss function using the optimization algorithm, while the activations and weights are kept frozen. This concept operator should combine information about the concept from different outputs so that it generalizes to unseen examples. For a set of inputs and outputs, we can train one shared concept operator using SGD over batches of inputs and evaluated outputs \citep{yin2024lofit,wu2024reftrepresentationfinetuninglanguage}. \citet{subramani_etal_2022_extracting} instead trains one concept operator for each input. However, this concept operator fails to generalize to different inputs. \citep{konen2024style} overcome this by first deriving one concept operator per input before averaging them into a generalized concept operator.

\subsection{Unsupervised Feature Learning}
\label{subsec:unsupervised}

Representation Identification methods that perform Unsupervised Feature Learning (UFL) first learn sparse features in the model's activations and then identify which concepts they belong to. This is done by first collecting a set of activations, applying a learning algorithm that discovers features in the activation space, and then identifying which human-understandable concept a feature corresponds to.
In contrast to other RI approaches, UFL methods do not set out to find one specific concept representation but identify a large set of concepts from which concepts of interest can be chosen.

\textbf{UFL methods.}
The dominant paradigm in UFL are Sparse Autoencoders (SAEs) \citep{templeton2024scaling,huben2023sparse}. They are motivated by the observation that a model represents more features than it has directions, thus forcing the model to represent features in superposition \citep{elhage2022toy}. SAEs aim to disentangle these representations by learning a dictionary of monosemantic directions that represent sparse concepts. This dictionary receives the LLM's activations and is trained to accurately reconstruct them while only using a small set of its features at a time. 

Deep Causal Transcoding (DCT) \citep{mach2024mechanistically, mack2024deep} attempts to find a minimal perturbation in the activations of a layer that causes a large, predictable change in a later layer. As such, DCT might have applicability for eliciting latent behaviors like backdoors or unknown capabilities.

\renewcommand{\arraystretch}{1.2}
\begin{table}[ht]
    \caption{Steps and difference between \textbf{Unsupervised Feature Learning} methods for \textbf{Representation Identification}.}
    \label{tab:overview_sfl}
    \begin{center}
    \begin{tabular}{l l}
        \toprule
        \textbf{\makecell[l]{Steps in Unsupervised\\Feature Learning}} & \textbf{How do methods differ?} \\
        \midrule
        \multirow{2}{*}{\hyperref[subsubsec:ufl_collect]{1. Collecting Activations}}
        & \textbf{Inputs} (large-general, small-specific datasets)\\
        & \textbf{Location} (residual stream, attention/MLP layer outputs) \\
        \addlinespace
        \multirow{2}{*}{\hyperlink{subsubsec:feature_discovery}{2. Discovering Features}} 
        & \textbf{Goal} (disentangle features, or find meaningful perturbations)\\
        & \textbf{Loss Function} (reconstruction and sparsity, consistent large change)\\
        & \textbf{Learned Objects} (feature dictionary, diverse steering vectors) \\
        \addlinespace
        \multirow{2}{*}{\hyperref[subsubsec:concept_identification]{3. Identifying Concepts}} 
        & \textbf{Identification} (highly activating, probing accuracy, mutual information)\\
        & \textbf{Labels} (LLM-as-a-judge, existing dataset)\\        
        \bottomrule
    \end{tabular}
    \end{center}
\end{table}
\renewcommand{\arraystretch}{1.0}

\subsubsection{Collecting Activations}
\label{subsubsec:ufl_collect}
UFL methods provide inputs $X$ and collect activations $\text{Act}(\mathcal{M},X^c,l,m,p)\rightarrow A_{l,m,p}^c$ at a specific layer $l$, model components $m$, and token positions $p$.

\textbf{Inputs.}
SAEs employ large datasets to collect many activations. The dataset does not have to be specifically related to the concept of interest. However, SAEs require much larger datasets than other RI methods. This inefficiency partially stems from the fact that SAEs learn many features, while other methods focus on a single concept.
DCT uses a much smaller set of inputs to collect activations. The inputs can be specific to a domain or have wide coverage and do not require a specific relationship to the concepts of interest.

\textbf{Location.}
The activations for SAEs are usually collected from the residual stream but have also been taken from the outputs of the attention layers \citep{kissane2024interpreting} and MLP layers \citep{bricken2023towards}. DCT collects activations from the residual stream.
DCT uses a much smaller set of inputs to collect activations from the residual stream. 


\subsubsection{Discovering Features}
\label{subsubsec:feature_discovery}
An unsupervised learning algorithm is applied to discover features $o$ in the activations $A$.

\textbf{Goal.}
SAEs are trained to take the model's activations, project them into a higher dimensional and sparse space, and then project them back. The goal is to disentangle the representations of concepts that are superimposed on each other. Since the dimensionality of the SAE is much larger than that of the LLM, it can represent each concept as a monosemantic direction while retaining the same information as the LLM. 

The key idea of DCT is that perturbations along semantically meaningful directions at one layer will cause large, consistent, and predictable changes at later layers, whereas the effect of meaningless perturbations is inconsistent and quickly diminishes over the layers.

\textbf{Loss.}
The SAE is trained to have a low reconstruction loss and to use sparse features. The reconstructed activations should be very similar to the original activations, and any feature in the SAE should only activate on a few inputs.

In DCT, a set of vectors is optimized over a set of inputs so that adding them to the activations at an early layer causes a large change in later layers. The vectors are optimized to have effects that a shallow MLP model can predict with high accuracy. Furthermore, orthogonality between vectors is enforced to find a diverse set of features.

\textbf{Learned Objects.}
SAEs learn an MLP that projects from the LLM's activations into the SAE space and one MLP that projects back. The directions in the SAEs space can be seen as features in this learned dictionary. Ideally, they should denote representations of concepts within the LLM and can be used as steering vectors.
DCT produces vectors that can be added to the model's activations to achieve specific downstream effects. They can be used directly as concept operators.

\subsubsection{Identifying Concepts}
\label{subsubsec:concept_identification}
Lastly, one needs to identify which concepts $c$ the learned features $o$ correspond to, thus deriving the concept operator $o^c$. This requires a method for identifying the features and a source of information about the concept.

\textbf{Identification.}
One can inspect the inputs and outputs on which the feature activates most strongly. However, \citet{durmus2024featuresteering} find that the context in which a feature fires does not always predict its effect. Alternatively, one can increase the value of the feature and observe the effect on the model's outputs.
Other methods identify feature directions that have a high mutual information with the target concept \citep{zhao2024steeringknowledgeselectionbehaviours}, optimize a collection of features to lead to desired outputs \citep{kharlapenko2024extracting}, or select the features with the highest probing accuracy for a concept \citep{wu2025axbenchsteeringllmssimple}

\textbf{Labels.}
Early works viewed highly activating inputs and outputs manually to determine the corresponding concept. Current work leverages automated interpretability by using LLMs to make these judgments.
Other methods require a pre-existing dataset labeled for the concept. For example, \citet{wu2025axbenchsteeringllmssimple} build a dataset of positive and negative examples for a concept to determine the probing accuracy of features.

\subsubsection{Applying SAEs}
Once one has used SAEs to identify a feature that represents the targeted concept, the direction of that feature can be used as a concept operator. However, using that concept operator for steering requires one to translate every activation into the SAE space and back. Therefore, there have been attempts to use SAE features to improve steering vectors in the LLMs activations that were identified by other methods. \texttt{\mbox{\citet{chalnev2024improvingsteeringvectorstargeting}}} derive steering vectors that activate desired SAE features. \citet{conmy2024activation} decompose a steering vector into SAE features and remove features that seem unrelated to the desired concepts. However, this is challenging, as the steering vectors fall outside of the SAEs training distribution and contain negative feature directions which are not accommodated in the SAE \citep{mayne2024can}. \citet{kharlapenko2024extracting} address this by optimizing the SAE reconstruction of the steering vector to achieve good downstream performance while incentivizing sparsity, leading to cleaner reconstructions.

Notably, SAEs have recently been extended to discover circuits of features implemented over adjacent layers \citep{dunefsky2024transcodersinterpretablellmfeature} or multiple layers \citep{lindsey2024sparse}. Furthermore, there has been continued progress for better SAE architectures \citep{rajamanoharan2024improving,rajamanoharan2024jumpingaheadimprovingreconstruction}. This progress will hopefully lead to better methods for identifying and steering LLMs. 

Many human-understandable concepts, such as famous people or scam emails, have been identified through SAEs \citep{templeton2024scaling}. However, the method does not guarantee to find a specific concept of interest. An engineer might be interested in the representation for honesty but fail to find one in the SAE. Furthermore, while many papers motivate SAEs with the ability to steer representations, few of them actually evaluate whether the discovered features allow for effective Representation Control. To improve the applicability of SAEs for feature steering, it should be a standard to evaluate the effects of features on models' outputs.

\begin{mdframed}[style=takeawaybox]
\textbf{Takeaway: Representation Identification} \\
To identify the representation of a concept, methods need to provide a stimulus related to the concept, collect the model's activations, and calculate a concept operator from them. There are three types of methods:
\vspace{-10pt}
\begin{enumerate}[leftmargin=20pt,itemsep=-3pt]
    \item \textbf{Input Reading}: Construct sets of inputs related to the concept, collect their activations, and calculate the concept operator from the sets of activations.
    \item \textbf{Output Optimization}: Collects activations and outputs, scores the outputs according to the concept, and optimizes a concept operator for that score.
    \item \textbf{Unsupervised Feature Learning}: Collect activations, discover features in the activations with an unsupervised learning algorithm, and identify which concepts the features correspond to.
\end{enumerate}
\end{mdframed}


\section{Representations Operationalization}
\label{sec:operator}
RepE assumes that the model represents the concept of interest in its activation space. However, it is unclear how exactly the model's representations are structured. RepE methods for identification and control bake in assumptions about the geometry of representations used by the model. These representations are operationalized in RepE methods with a concept operator that has a specific shape and intended relationship to the models' representation of the concept.

\begin{table}[ht]
    \caption{Differences between approaches for operationalizing representations (the output of the \textbf{Representation Identification} step). }
    \label{tab:overview_otr}
    \begin{center}
    \begin{tabular}{p{0.3\linewidth} p{0.65\linewidth}}
        \toprule
        \textbf{\makecell[l]{Aspects of Representation\\Operationalization}} & \textbf{How do methods differ?} \\
        \midrule
        \multirow{1}{*}{\hyperref[subsec:assumed_geometry]{Assumed Geometry}} & 
        \textbf{Features as} Linear Direction, Fuzzy clusters or Non-linear manifolds\\
        \addlinespace
        \multirow{3}{*}{\hyperref[subsec:concept_operator]{Choice of Concept Operator}} 
        & \textbf{Shape} (Vector, Matrix, Vector + Matrix)\\
        & \textbf{Relationship} with concept representation (Exact match, Capture covariance, Capture principled direction and variance)\\
        \bottomrule
    \end{tabular}
    \end{center}
\end{table}

\subsection{Assumed Geometry}
\label{subsec:assumed_geometry}
Assumptions about the geometry of internal representations are included in the choice of RI method and steering function. For example, if the weights of a linear probe are used as a concept operator, it will not be able to pick up on non-linear aspects of a concept's representation. Similarly, if an activation steering function only increases along a linear direction, it would fail if the concept was represented along a non-euclidean space.

There is an ongoing discussion in the interpretability community about the geometry that transformer models use to represent internal features. The Linear Representation Hypothesis poses that concepts are represented as linear directions in the activation space, and their intensity is denoted by the magnitude of the activations in that direction \citep{park2024the}. 
Although this hypothesis has some merits and, if true, would make RepE easier, it has also been criticized (see \autoref{sec:why} for discussion). Thus, RepE methods with alternative feature geometries have been proposed. They assume representations to be fuzzy, clusters \citep{postmus2024steeringlargelanguagemodels}, or non-linear manifolds \citep{qiu2024spectraleditingactivationslarge}. Other possibilities for feature geometries include circles \citep{engels2025not}, onions \citep{csordas2024recurrent}, non-linear paths, or subsets of a vector space. 

\subsection{Concept Operators}
\label{subsec:concept_operator}
The concept operator is the object that denotes the model's representation of the concept. As the output of RI and the input for RC, it is the linking element between the two parts of the RepE pipeline. Concept operators can take different forms like vectors, matrices, or combinations thereof. Furthermore, they differ in the relationship they are intended to have to the model's representation.

\textbf{Vectors.}
The most common type of concept operator is a vector $o^c \in \mathcal{R}^{d}$, where $d$ is the dimensionality of the activation space. Often, this is used by methods that assume that representations are linear directions in the model's activation space, which a vector can describe. Thus, the concept operator vector should be as similar as possible to that direction.
Other methods follow \citet{li2023inference} and employ one vector per attention head~\citep{10.1145/3626772.3657819,wang2024adaptive,li2024destein}.
Furthermore, some methods denote a concept representation through a set of vectors that represent different aspects of the concept \citep{Chen_Sun_Jiao_Lian_Kang_Wang_Xu_2024} or serve different functions \citep{wu2024advancing}. \citet{cai2024selfcontrol} use a set of vectors that were optimized to steer activations towards specific outputs to train a LoRA-adapter that produces similarly steered activations.

\textbf{Matrix.}
A range of matrices have been used as concept operators $o^c \in \mathcal{R}^{d\times d}$. 
\citet{postmus2024steeringlargelanguagemodels} operate on the assumption that features are fuzzy clusters in the activations. They use a positive semi-definite matrix called a conceptor that should capture the principle directions and variances. This allows to more precisely steer complex representations since conceptors also capture the underlying correlations between activations. \citet{xiao2024enhancing} two Gaussian Models that respectively model the distribution of positive and negative activations. This retains information contained in the distribution of activations while allowing activations to be mapped from one distribution to the other.
\citet{qiu2024spectraleditingactivationslarge} employ 2 matrices that capture covariance with positive and negative activations, respectively. These matrices can be used to project new activations to maximise their covariance with positive demonstrations and minimize covariance with negative demonstrations.
Lastly, \citet{rozanova-etal-2023-interventional} learn a Nullspace Projection Matrix $o^c$ which combines nullspaces that are orthogonal to the feature direction
$o^ca=(I-(o_0^c, ..., o_n^c))a \rightarrow a^c$. This captures information in the activations that a probe could use to predict the concept of interest. Thus, the matrix can be used to project out information about the concept.

\textbf{Matrix + Vector. } Some methods employ both a matrix and a vector to denote the concept representation, most commonly to perform an affine transformation where the activations are multiplied with the matrix and then added to the vector \citep{dong2024contrans}.
\citet{singh2024representation} and \citet{avitan2024interventionlensrepresentationsurgery} find that matching the mean and covariance of negative and positive activations allows one to steer the model's representations. Thus, they derive a matrix that captures the covariance of the concept's representation and a vector that captures its mean. This allows to steer the representation with an affine function. \citet{wu2024reftrepresentationfinetuninglanguage} extend this by learning an additional matrix that serves to project the activations in and out of a space where the affine transformation is more effective. \citet{hernandez2024inspecting} employ a matrix to amplify directions of an attribute that are relevant to a concept entity and a bias term that better fits the transformed attribute into the activations. This assumes representations of entities and attributes to be represented linearly.
Alternatively, \citet{pham2024householderpseudorotationnovelapproach} assume that representations are directions of activations from the origin and that the magnitude along that direction reflects the intensity of the representation. They use the weights vector of a probe that separates the positive and negative regions in the activation space and an MLP that can predict how to adjust the angle of an activation vector.


\begin{mdframed}[style=takeawaybox]
\textbf{Takeaways: Representation Operationalization}
\vspace{-10pt}
\begin{enumerate}[leftmargin=20pt,itemsep=-3pt]
    \item \textbf{Representaiton Geometry}: RepE methods make implicit assumptions about the geometry of representations. Many assume that concepts are represented as linear directions, but alternative perspectives exist. 
    \item \textbf{Concept Operator}: Most commonly, the concept operator is a vector denoting a linear direction. Others use matrices to capture principled directions, covariances or all linearly available information about a concept.
\end{enumerate}
\end{mdframed}

\section{Representation Control}
\label{sec:control}
The goal of Representation Control (RC) is to steer the representations of the LLM in order to control its outputs. For this, it employs a \textbf{steering function} that makes use of the concept operator to modify the weights or activations of the model. Additionally, RC methods decide when and where to apply the steering functions.
The choice of concept operator $o^c$ and steering function $F$ determine the effect of RC and bake in assumptions about the geometry of model representations.

Throughout the section, we will describe the proposed steering functions along with the concept operators they use. We describe what effect the steering function has on the activation space and how this is supposed to control the representations of the model.


\subsection{Modifying Activations}
Activations can be controlled at inference time with an activation steering function $F_a(a_l, o^c_l) \rightarrow a_l^c$ (see Definition \ref{def:f_a}). For a given input, we read the activations at an intermediate layer $a_l$ during the forward pass, perform some operation $F$ on it using the operator $o^c_l$, and then use the modified activations $a^c_l$ as input for the next layer. By steering the activations, we can control the model's output with regard to the concept.

\begin{table}[h!]
    \caption{Differences between \textbf{Modifying Activations} methods for \textbf{Representation Control}.}
    \label{tab:overview_ma}
    \begin{center}
    \begin{tabular}{p{0.24\linewidth} p{0.71\linewidth}}
        \toprule
        \makecell[l]{\textbf{Aspects of} \\\textbf{Modifying Activations}} & \textbf{How do methods differ?} \\
        \midrule
        \multirow{6}{*}{\hyperref[subsec:steering_fns]{Steering Functions}} & \textbf{Operation} (Linear Addition, Vector rejection, Affine transformation, ...) \\
        & \textbf{Concept Operator} (vector, matrix, vector+matrix)\\
        & \textbf{Assumed Geometry} (linear directions, mean\&covariance matching) \\
        & Intended Effect on \textbf{Activation Space} (scale direction, remove information, rotate direction)\\
        & Intended Effect on \textbf{Representations} (increase/decrease intensity, remove)\\
        \addlinespace
        \multirow{3}{*}{\hyperref[subsubsec:location]{Location \& Time}} & \textbf{Layer \& Model component} (single or multiple layer(s) \& component(s))\\
        & \textbf{Token position} (every token, one token before generation, specific token)\\
        & \textbf{Input dependent} (independent, switch on/off, adjust intervention strength)\\
        \bottomrule
    \end{tabular}
        \end{center}
\end{table}

\subsubsection{Activation Steering Functions} \label{subsec:steering_fns}
\textbf{Linear Addition.} This simply adds a vector $o^c\in \mathbb{R}^d$ to the original activations $a_l + \lambda o_l^c \rightarrow a^c_l$, where the factor $\lambda$ determines the steering strength. The model's representation is assumed to be a linear direction, and the concept operator is a vector that should denote that direction. Linear Addition simply increases the magnitude of activation along the direction, which should increase the intensity of the concept in the model's representations.

Linear Addition is the most popular steering function. Improving on this simple function, some papers renormalize the activations after the addition to a normal magnitude, thus avoiding reductions in quality \citep{rütte2024a,adila2024discovering,liu2024incontext,leong-etal-2023-self,Chen_Sun_Jiao_Lian_Kang_Wang_Xu_2024}. Other methods employ a set of vectors that are applied to individual attention heads \citep{li2024destein,li2023inference}. Following Differential Privacy, \citet{zeng2024privacyrestore} adds noise to the vector to protect privacy.

Multiple papers attempt to add multiple steering vectors at once to steer multiple concepts or multiple aspects of one concept. This can be done by simply adding all vectors at once \citep{wang2024adaptive,scalena2024multiproperty} or combining them in a weighted sum \citep{Chen_Sun_Jiao_Lian_Kang_Wang_Xu_2024,zeng2024privacyrestore}. However, this leads to interference between concepts and increases negative impact on the LLM's quality (see \autoref{par:multi_concept_control}). \citet{van2024extending} avoid this by adding different vectors at different layers. \citet{adila2024discovering} decompose the different vectors into their orthonormal basis and use those for steering.

\textbf{Vector Rejection.} The vector $o^c$ can be rejected $a_l-\frac{a_l\cdot o^c_l}{o^c_l \cdot o^c_l}o^c_l \rightarrow a^c_l$, by first projecting $a_l$ onto $o^c_l$ before subtracting. This removes the component of $a_l$ that is parallel to $o^c_l$, leaving us with the part of $a_l$ that is perpendicular to $o^c_l$. Assuming linear feature directions, this removes the influence of the concept $c$ in the representations \citep{arditi2024refusallanguagemodelsmediated,deng2025rethinking}. Equivalently, vector scaling $a_l+\frac{a_l\cdot o^c_l}{o^c_l \cdot o^c_l}o^c_l \rightarrow a^c_l$ amplifies the components of the activations that are aligned with the concept operator \citep{chu2024causal}.

\textbf{Nullspace Projection.} This employs a Nullspace projection matrix $o^c$ that captures information that a probe would use to classify the concept in the activations. Multiplying activations with this matrix removes the information of that concept $o^c_la_l\rightarrow a^c_l$ \citep{rozanova-etal-2023-interventional}, thus removing the influence of that concept on the model's outputs.

\textbf{Soft Projection.} This multiplies the activations with a conceptor matrix (discussed in Section~\ref{subsec:concept_operator}) that encodes the principal directions and variances of a concept set of activation vectors. This scales the activation vector along the directions relevant to the concept while not completely overwriting the non-concept directions. Thus, it allows for a more nuanced control over representations \citep{postmus2024steeringlargelanguagemodels}.

\textbf{Rotation.}
\citet{pham2024householderpseudorotationnovelapproach} argue that rotating activations into the appropriate direction from the origin is superior to shifting the direction directly since rotations achieve the desired direction while retaining the magnitude of activations. For computational efficiency, they approximate such a rotation through a reflection followed by an adjustment of the angle. Undesired activations are reflected along a hyperplane into the desired region of the activation space. Next, a learned MLP determines how to adjust the angle of the reflected activations to attain the desired activation.

\textbf{Affine Transformations.}
Affine Transformations for activation steering employ a matrix $o^{c,M}_l$ and a vector $o^{c,v}_l$ to control the activation $a_l$ through an affine transformation $o^{c,M}_la_l + o^{c,v}_l\rightarrow a^c_l$. 
\citet{singh2024representation} apply affine steering to match the mean and covariance of two representations. They prove this as an optimal steering function since it guards against linear probes for that concept, thus removing any linearly available information about the concept. Similarly, \citet{xiao2024enhancing} recenter and then rescale activations to fit into the distribution of positive activations.
Affine Steering has been used to manipulate facts the model associates with an entity \citep{hernandez2024inspecting}, project a steering vector into the activation space of another model \citep{dong2024contrans}, or to create counterfactuals with regard to sensitive attributes \citep{avitan2024interventionlensrepresentationsurgery}.

\textbf{Projecting Before Editing.}
Another approach is to project activations into a space that allows for better editing. \citet{qiu2024spectraleditingactivationslarge} first project into a non-linear space, before they project activations into directions that co-vary maximally with positive demonstrations and minimally with negative ones.
\citet{arora2024causalgym} project different activations onto the feature vector to then subtract them without influencing other features.

In section \ref{subsec:unsupervised} we discussed that SAEs can learn vectors that correspond to interpretable concepts. However, these vectors are in the representation space of the SAE. If one wants to use them to steer the normal model, it is possible to project the model's activations into the SAE space, apply the steering on the feature of interest, and then project them back. However, this comes with larger decreases in capabilities due to the reconstruction error and at higher computational cost. 


\subsubsection{Location \& Time}
\label{subsubsec:location}
\textbf{Layers and Components.} Representation Control methods can modify the activations at different layers or components. While some methods only steer the activations at a single layer and component \citep{panickssery2024steeringllama2contrastive,zhang2024betterangelsmachinepersonality,cao2024personalized,ball2024understandingjailbreaksuccessstudy}, others modify activations on many or all layers and components \citep{konen2024style,chen2024designing,liu2024ctrla,adila2024can,qiu2024spectraleditingactivationslarge}. The optimal location for steering can be found by measuring steering effectiveness \citep{zhang2024betterangelsmachinepersonality}, probing accuracy \citep{zhang2024truthxalleviatinghallucinationsediting}, inspecting the projection of each layer's steering vector to the vocabulary \citep{cao2024nothing}, or specific loss functions \citep{wang2023backdoor,adila2024can}.

\textbf{Tokens.} Furthermore, one could steer at different token positions. Some papers decide to steer at every token position during generation \citep{bortoletto2024benchmarking,adila2024can,wang2023backdoor}. While this can be more effective \citep{subramani_etal_2022_extracting}, it also increases the impact on capabilities. Others only steer at the last token of the query, hoping that the steering effects propagate throughout the generation \citep{wang2024adaptive,adila2024discovering,li2024rethinkingjailbreakinglensrepresentation}. Some papers chose to steer at or before a token that gives a prediction or answer \citep{paulo2024does,lucchetti2024activation}.

\textbf{Dynamic Activation Steering.}
\label{par:dynamic_steering}
For most activation steering functions, the intervention does not depend on the input.
This is suboptimal since some inputs might require different steering directions or strengths. Dynamically adapting activation steering to the present context is an emerging theme within RepE since it can increase steering effectiveness and reduce unwanted side effects.

Multiple papers decide whether steering is necessary for an input based on the cosine similarity between the activations and the steering vector \citep{adila2024can,wang2024inferaligner}, between the activations and the average of steered activations \citep{ma2025dressing} or depending on the output of a probe given the activations \citep{stickland2024steeringeffectsimprovingpostdeployment,li2024destein,pham2024householderpseudorotationnovelapproach,nguyen2025multiattributesteeringlanguagemodels}. Similarly, the intervention decision can be made separately for each layer or component \citep{zhang2024uncoveringlatentchainthought,simhi2024constructing}.
Instead of a binary decision for intervention, other papers modulate the strength of the intervention. This can be based on cosine similarity \citep{cao2024nothing}, KL-divergence between steered and unsteered output \citep{scalena2024multiproperty}, the L2-norm of positive and negative activations \citep{leong-etal-2023-self}, or by trying different strengths and evaluating the outputs \citep{fatahi-bayat-etal-2024-enhanced}. When multiple operators are available, the weight assigned to each operator can be derived dynamically \citep{zeng2024privacyrestore,wang2024adaptive}. Lastly, the operation itself can be dynamic, like using an MLP to predict the angle by which the activations are rotated \citep{pham2024householderpseudorotationnovelapproach}.


\subsection{Modifying Weights}
The representations of a model can be steered by modifying the weights with a weight steering function $F^w(w_l, o^c_l) \rightarrow w_l^c$.


\begin{table}
    \caption{Differences between \textbf{Modifying Weights} methods for \textbf{Representation Control}.}
    \label{tab:overview_mw}
    \begin{center}
    \begin{tabular}{l l}
        \toprule
        \textbf{Approaches to Modifying Weights} & \textbf{How do methods differ?} \\
        \midrule
        \multirow{2}{*}{\hyperref[subsubsec:training_weights]{Training weights}} & \textbf{Training Target} (activations and/or outputs)\\
        & \textbf{Trained weights} (low-rank adapter, all weights)\\
        \addlinespace
        \multirow{1}{*}{\hyperref[subsubsec:editing_weigths]{Editing weights}} & \textbf{Operation} (weight orthogonalization, addition)\\
        \bottomrule
    \end{tabular}
    \end{center}
\end{table}

\subsubsection{Training Weights}
\label{subsubsec:training_weights}
The weights can be modified through fine-tuning. This training is either aimed at producing desired activations or optimizes for specific outputs. Furthermore, fine-tuning methods differ in which weights or adapters they train.

\textbf{Training Towards Desired Activations.} Weights $w_l$ can be trained to produce desired activations $a^c_l$ by a weight steering function. A dataset of desired activations can be gathered by applying an activation steering function $F_a(a_l,o^c)\rightarrow a_l^c$.
\citet{zou_representation_2023} optimize a low-rank adapter (LoRA) to generate activations with low L2-distances to steered activations. 
Similarly, \citet{cai2024selfcontrol} train a LoRA adapter on a dataset of optimally steered activations so that the resulting activations are similar to the optimally steered activations.
Alternatively, \citet{zhang2024generalconceptualmodelediting} design an Adversarial Training setup where a LoRA adapter is trained to generate activations that a discriminator classifies as aligned with the concept.

\textbf{Training Representations without a Concept Operator.}
Some weight steering functions do not rely on a concept operator $o^c$. Instead, they train weights to produce activations similar to random noise.
\citet{zou2024improving,li2024the,rosati2024representation} fine-tune an LLM so that activations on harmful inputs or information that should be unlearned are close to noise. This should remove harmful representations from the model.

\textbf{Training Towards Desired Outputs.}
Multiple methods propose to fine-tune a small number of added parameters towards a dataset of desired outputs. \citet{wu2024reftrepresentationfinetuninglanguage} propose ReFT, which trains an adapter to bring the activations into a subspace where a linear projection and addition are applied.
\citet{yin2024lofit} fine-tune only the bias terms of selected attention heads. \citet{wu2024advancing} train an adapter consisting of a scaling and a bias vector.
While these methods are inspired by RepE and aim to control the model's representations, they are also end-to-end parameter-efficient fine-tuning methods like LoRA adapters \citep{hu2022lora}. 
This is shown by \citet{jiang2024a}, who draw a theoretical connection between LoRA and ReFT and unify them into a meta-algorithm for model efficient fine-tuning.
This highlights that weight steering functions that optimize for specific outputs can be seen as being on a spectrum between RepE and fine-tuning.

\textbf{Training Towards Desired Activations and Outputs.}
Other methods optimize both for desired activations and desired outputs. \citet{ackerman2024representation} optimize weights to produce activations with high cosine similarity to the concept operator vector while also retaining a low cross-entropy loss in the outputs. \citet{yu2024robustllmsafeguardingrefusal} repeatedly ablate the vector that represents refusal while training the model to refuse harmful answers. This should force the model to learn more robust representations of that concept. \citet{stickland2024steeringeffectsimprovingpostdeployment} mitigate performance loss by training a LoRA adapter that minimizes KL-divergence in outputs when a steering vector is applied to the outputs of the original unsteered model.


\subsubsection{Editing Weights}
\label{subsubsec:editing_weigths}
It is also possible to edit the weights directly without training.
\citet{arditi2024refusallanguagemodelsmediated} apply weight orthogonalization $w_l-o^c_lo^{c^T}_lw_l\rightarrow w^c_l$ to the MLP weight matrix that writes to the residual stream. This modifies the component weights to never write in the direction $o^c_l$. If $o^c_l$ captures the direction with which the model represents a concept, this operation removes the concept from the model's representations.
\citet{wang2024model} simply add the weights of a probe $o^c$ to a few selected row vectors within the gated projection matrix of the MLP weights. This shifts activations in the hidden state of the MLP away from undesirable regions and towards desirable regions.

\begin{mdframed}[style=takeawaybox]
\textbf{Takeaway: Representation Control}\\
Representations can be controlled by steering the activations or weights. 
\vspace{-10pt}
\begin{enumerate}[leftmargin=20pt,itemsep=-3pt]
    \item \textbf{Activations} can be steered during inference by addition, rejection, or projection, thus increasing, removing, or adapting the concept representation. Activation steering can be applied at different tokens and dynamically adjusted for new inputs.
    \item \textbf{Weights} can be steered by training them to generate desired activations and outputs or by editing components of weight matrices.
\end{enumerate}
\end{mdframed}


\section{Practical Representation Engineering Pipelines}
In this section, we describe some prototypical RepE pipelines and classify how they fit into our taxonomy. We compare them according to selected criteria. In addition, we gather empirical evidence to answer which design choices in RepE work best.

\subsection{Prototypical RepE Pipelines}
\label{subsec:pipelines}

In Table \ref{tab:methods}, we classify RepE pipelines according to which strategy they employ for Representation Identification, the shape of their operator, and whether they control activations or weights. We score Representation Identification methods on whether they require a dataset labeled according to the concept and whether they require expensive optimization. Additionally, we score whether the method is restricted to linear representations or can capture non-linear ones. Furthermore, we score whether controlling the model requires retraining of its weights and whether the operation to control the model is dependent on the given inputs. Lastly, we determine the types of concepts the method was initially designed to engineer. However, this does not mean that they are not able to engineer other concepts.
We select these RepE pipelines to show a diversity of approaches while preferring highly-cited papers that were published at top conferences.

\begin{table}
    \caption{Prototypical methods for RepE classified according to our taxonomy and the criteria they meet.}
    \label{tab:methods}
    \begin{center}  
    \resizebox{\linewidth}{!}{
    \begin{tabular}{m{2.4cm}| m{1.4cm}C{1.8cm}C{0.8cm} m{2cm}m{1.5cm} m{1.4cm}C{1.3cm}C{1.3cm} | m{2.4cm}}
    \toprule
    \multicolumn{1}{l}{Method} & \multicolumn{3}{c}{Representation Identification}  & \multicolumn{2}{c}{Representations Operationalization} & \multicolumn{3}{c}{Representation Control} & \multicolumn{1}{l}{\makecell[l]{Originally\\proposed for}} \\
    \cmidrule(lr){1-1} \cmidrule(lr){2-4} \cmidrule(lr){5-6} \cmidrule(lr){7-9} \cmidrule(lr){10-10}
    Name & Type & \multicolumn{1}{l}{\makecell[l]{Works with\\unlabeled\\dataset?}} & \multicolumn{1}{l}{\makecell[l]{Optimi-\\zation\\Free?}} & \multicolumn{1}{l}{\makecell[l]{Concept\\Operator}} & \multicolumn{1}{l}{\makecell[l]{Allows\\non-linear\\representations?}} & Type & \multicolumn{1}{l}{\makecell[l]{Training-\\free?}} & \multicolumn{1}{l}{\makecell[l]{Context-\\adjusted\\steering?}} & \\
    \midrule
    CAA \citep{panickssery2024steeringllama2contrastive} & Input Reading & \textcolor{red}{\faTimesCircle} & \textcolor{green}{\faCheckCircle} & Vector & \textcolor{red}{\faTimesCircle} & Activations & \textcolor{green}{\faCheckCircle} & \textcolor{red}{\faTimesCircle} &  \makecell[l]{High-level\\behaviors}\\
    \addlinespace[0.5em]
    LoRRA \citep{zou_representation_2023} & Input Reading & \textcolor{green}{\faCheckCircle} & \textcolor{green}{\faCheckCircle} & Vector & \textcolor{red}{\faTimesCircle} & Weights & \textcolor{red}{\faTimesCircle} & \textcolor{red}{\faTimesCircle} & \makecell[l]{High-level\\behaviors} \\
    \addlinespace[0.5em]
    ITI \citep{li2023inference} & Input Reading & \textcolor{red}{\faTimesCircle} & \textcolor{red}{\faTimesCircle} & Vectors & \textcolor{red}{\faTimesCircle} & Activations & \textcolor{green}{\faCheckCircle} & \textcolor{red}{\faTimesCircle} & Truthfulness\\
    \addlinespace[0.5em]
    SAE \citep{templeton2024scaling} & Unsupervised Feature Learning & \textcolor{green}{\faCheckCircle} & \textcolor{red}{\faTimesCircle} & Vector & \textcolor{red}{\faTimesCircle} & Activations & \textcolor{green}{\faCheckCircle} & \textcolor{red}{\faTimesCircle} & Abstract features \\
    \addlinespace[0.5em]
    LoReFT \citep{wu2024reftrepresentationfinetuninglanguage} & \makecell[l]{Output\\Optimization} & \textcolor{green}{\faCheckCircle} & \textcolor{red}{\faTimesCircle} & 2 Matrices + vector & \textcolor{red}{\faTimesCircle} & \makecell[l]{Activations/\\Weights} & \textcolor{red}{\faTimesCircle} & \textcolor{green}{\faCheckCircle} & Improving task performance\\
    \addlinespace[0.5em]
    BiPO \citep{cao2024personalized} & \makecell[l]{Output\\Optimization} & \textcolor{red}{\faTimesCircle} & \textcolor{red}{\faTimesCircle} & Vector & \textcolor{red}{\faTimesCircle} & Activations & \textcolor{green}{\faCheckCircle} &  \textcolor{red}{\faTimesCircle} & \makecell[l]{Aligning to\\preferences} \\
    \addlinespace[0.5em]
    SEA \citep{qiu2024spectraleditingactivationslarge} & Input Reading & \textcolor{red}{\faTimesCircle} & \textcolor{green}{\faCheckCircle} & 2 matrices & \textcolor{green}{\faCheckCircle} & Activations & \textcolor{green}{\faCheckCircle} & \textcolor{green}{\faCheckCircle}  & \makecell[l]{High-level\\behaviors} \\
    \addlinespace[0.5em]
    MiMiC \cite{singh2024representation} & Input Reading & \textcolor{red}{\faTimesCircle} & \textcolor{green}{\faCheckCircle} & Matrix + vector & \textcolor{red}{\faTimesCircle} & Activations & \textcolor{green}{\faCheckCircle} & \textcolor{red}{\faTimesCircle} & Removing bias \& Toxicity\\
    \bottomrule
    \end{tabular}}        
    \end{center} 
\end{table}

\textbf{Contrastive Activation Addition (CAA).} 
CAA \citep{panickssery2024steeringllama2contrastive} provides positive and negative examples of a behavior, collects their activations, and takes the mean difference between the 2 sets of activations to calculate a vector. This vector is used as a concept operator that is added to the activations during inference, thus steering the model with regard to high-level behavioral characteristics.\\

\textbf{Low-Rank Representation Adaptation (LoRRA).} 
LoRRA \citep{zou_representation_2023} identifies the operator by adding positive or negative pre-prompts to inputs, collecting their activations, and taking the first principle component of the difference between the two sets of activations.
Instead of directly adding the vector to the activations, they train a LoRA adapter to output activations that are similar to adding the vector to the original activations. This can steer a range of high-level concepts, including honesty, emotions, and ethical values.

\textbf{Inference-Time Intervention (ITI).} 
ITI \citep{li2023inference} inputs questions and answers that do or do not correspond to the concept. Then, a linear probe is trained on the activations of each attention head to predict whether the answer was or was not related to the concept. They select the attention heads with the highest probing accuracy and use the weight vector of the probe as a concept operator. During inference, these vectors are added to the activations of their respective attention heads. This method was first proposed to make models more truthful.

\textbf{Sparse Auto-Encoder (SAE).} 
SAEs \citep{templeton2024scaling} are trained in an unsupervised fashion on a large dataset of activations to learn internal features. Activations are projected into a wider feature space, which is optimized so that features are sparse and can be used to reconstruct the original activations. We can identify the concept a feature corresponds to by investigating inputs or outputs for which the feature is highly activated. These features correspond to vectors in the embedding space of the SAE and can be used to stimulate the model's activations. SAEs find a wide range of concepts at different levels of abstraction.

\textbf{Low-rank Linear Subspace Representation Finetuning (LoReFT).} 
LoReFT \citep{wu2024reftrepresentationfinetuninglanguage} learns an adapter that intervenes with the activations during a forward pass. The adapter first projects the activations into a linear subspace, edits the activations with an affine transformation, and projects them back. The outputs of the adapter are added to the original activations. For this, a low-rank matrix, projection matrix and bias vector are trained to minimize the cross-entropy loss on a dataset that exemplifies the concept. This method was proposed to improve performance on specific downstream tasks.

\textbf{Bi-directional Preference Optimization (BiPO).}
BiPO \citep{cao2024nothing} optimize a vector so that adding the vector to the activations makes the desired outputs more and the undesired outputs less likely. Subtracting the vector should have the opposite effect. After optimizing with Gradient Descent, we get a vector that represents the preferences in the provided dataset. During inference, the model's representations are controlled by adding the vector to the activations. This was proposed to efficiently align LLMs with human preferences. 

\textbf{Spectral Editing of Activations (SEA).} 
SEA \citep{qiu2024spectraleditingactivationslarge} collects activations for a neutral question and for corresponding positive and negative responses. By applying Singular Value Decomposition on the covariance matrices between negative and neutral, as well as positive and neutral activations, they identify a positive and negative editing matrix. During inference, they can select directions in the activations that co-vary with the editing matrices and prune negative directions while retaining positive ones. They extend this to non-linear directions by first transforming the activations into a non-linear space where the editing can be performed before transforming the edited activations back. While SEA is not designed for particular types of concepts, it was initially used to steer truthfulness and bias.

\textbf{Minimally Modified Counterfactual (MiMiC).}
MiMiC \citep{singh2024representation} divides activations into a positive and negative set based on an MLP probe trained to detect the concept. They then derive an affine steering function that matches the mean and the covariance of the two sets of activations while causing minimal distortion. On new activations, this function performs a projection and translation, steering it towards the target concept while preserving key statistical properties. It was initially proposed to control representations of toxicity and bias in sensitive concepts.

\subsection{Which Methods Work Better?}
In this section, we summarize previously reported results that compare different methods. While this section gathers existing empirical evidence for comparison between RepE methods and implementations, we often cannot make conclusive statements about the superiority of one method. This is because of a lack of unbiased comparisons (see Section \ref{subsec:building} for discussion).

\subsubsection{What Works for Representation Identification?}
\textbf{Is it more effective to use Input Reading or Output Optimization? - Depends on the concept.}\\
Multiple papers claim that their method based on output optimization improves on input reading methods \citep{yin2024lofit,cai2024selfcontrol,cao2024personalized,ackerman2024representation}. However, we lack evidence for the contrary, which is likely because papers that propose input reading techniques did not compare their methods to output optimization methods.
Two unbiased comparisons find that an input reading method is better at steering the sentiment of text \citep{konen2024style} and that output optimization methods control the concrete topics more effectively~\citep{wu2025axbenchsteeringllmssimple}.

\textbf{Are steering features from Sparse Auto-Encoders better than other RI methods? - Possibly a combination.}\\
In \citet{zhao2024steeringknowledgeselectionbehaviours} and \citet{makelo2025evaluating}, an SAE-based steering is more effective than input-reading-based RI methods.
In contrast,\citet{chalnev2024improvingsteeringvectorstargeting} find that an input-reading-based RI method leads to more effective steering than SAE features. However, results in \citet{chalnev2024improvingsteeringvectorstargeting} and \citet{kharlapenko2024extracting} suggest that SAEs can be used to make input-reading-based concept operators more effective. Finally, evaluations on the AxBench~\citep{wu2025axbenchsteeringllmssimple} find that SAEs perform worse at concept detection and steering than LoReFT and class means but better than methods using probing or dimensionality reduction.

\textbf{Which prompting format works best? - Unclear.}\\
\citet{braun2025understanding} compare control effectiveness for seven prompt settings varying in whether an answer token is appended, whether an instruction is prepended, and whether few-shot demonstrations are included. While the setting that only appends an answer token has a slightly larger average control effectiveness, they conclude that no prompt type clearly outperforms the other. 
\citet{wang2023backdoor} find that choice prompts, where the model has to select answer A or B, outperform free-form answers. This is likely because the choice prompt concentrates the relevant context into a single token and thus more effectively triggers the concept representation. The lack of principled evaluations of optimal prompting formats for eliciting representations makes this an important area of further study.

\textbf{At which token positions should activations be read out? - Unclear.}\\
Some evidence suggests that averaging the activations of multiple tokens is better than just using the last token's activations \citep{hoscilowicz2024nonlinearinferencetimeintervention}. Furthermore, focusing on the last token given by the user can be more effective than taking activations from an appended post-prompt like “I think the \{attribute\} of this user is” \citep{chen2024designing}. \citet{braun2025understanding} do not find a clear difference between reading activations at the last token of the question or at the token containing the answer.

\textbf{Which function is best for calculating the difference between sets of activations? - Difference-in-Means.}\\
In Input Reading, functions like Difference-in-Means (DiM), linear probes, Principle Component Analysis, or Contrast Consistent Search \citep{burns2023discovering} are used to identify the concept representation from contrasting sets of activations. This has been experimentally studied in 7 papers \citep{li2023inference,marks_geometry_2023,rütte2024a,xu2024exploring,arora2024causalgym,wu2025axbenchsteeringllmssimple,im2025unifiedunderstandingevaluationsteering}. In 6 out of 7 papers Difference-in-Means leads to the strongest performance. The next best seems to be taking the weight vector of a linear probe. PCA and CCS tend to perform worse. 
A fair comparison between DiM, PCA, and classifier-based methods finds that DiM is best at positively and negatively steering multiple-choice questions, equals the performance of probes at steering long-form generations, but leads to the largest performance deterioration on positive examples \citep{im2025unifiedunderstandingevaluationsteering}.
Lastly, Distributed Alignment Search (DAS) \citep{pmlr-v236-geiger24a} was only tested once in which it performed the strongest, highlighting that DAS remains underexplored.
Besides Input Reading, it is advantageous to apply mean-centering to the steered activations as shown by \citet{jorgensen2024improving}.

\subsubsection{What Works for Representation Operationalization?}
\textbf{Are matrices or vectors better concept operators? - Matrix more effective but more costly.}\\
Multiple works that propose using matrices instead of vectors as concept operators find that they improve steering effectiveness \citep{postmus2024steeringlargelanguagemodels,rajendran2024learning,pham2024householderpseudorotationnovelapproach}. Results from \citet{zou_representation_2023} are more ambivalent, finding that a matrix slightly outperforms a vector-based concept operator, but is clearly outperformed by a vector that is derived from additional forward passes for every new input. While matrices can be more effective, they can also come at an increased cost \citep{postmus2024steeringlargelanguagemodels} or reduced output quality \citep{rajendran2024learning}.

\textbf{Are linear or non-linear concept operators better? - Non-linear.}\\
While most methods perform linear operations on the representations, the direct comparison between linear and non-linear methods favors the non-linear ones. Non-linear Spectral Editing of Activations (SEA) outperforms linear SEA \citep{qiu2024spectraleditingactivationslarge}, and non-linear ITI is more effective than ITI, although this comes at a higher cost in capabilities \citep{hoscilowicz2024nonlinearinferencetimeintervention}.

\subsubsection{What works for Representation Control?}
\textbf{Is it more effective to modify weights or activations? - Unclear.}\\
This question is still open. \citet{wang2024model} find that their method for controlling model weights is more effective than intervening on activations while leading to a larger reduction in output quality. However, a broader evaluation of that question is necessary.

\textbf{Which steering function should be used for modifying activations? - Unclear.}\\
\citet{zou_representation_2023} find that a piece-wise operation is more effective than linear addition. \citet{chu2024causal} find that projection outperforms a product, which again is more effective than linear addition. \citet{luo2024paceparsimoniousconceptengineering} find that linear addition is more effective than orthogonal projections at a higher cost in capabilities. Linear addition remains the most popular activation steering function despite some other steering functions outperforming it. This warrants further investigation comparing different operations. \citet{krasheninnikov2024steering} study different activation steering functions in a toy classification setup. They find that more expressive steering functions have a higher performance ceiling and can control more complex tasks, but simpler steering functions are effective with lower amounts of data.

\textbf{At which token positions can activations be steered most effectively? - Apply steering at every inference step.}\\
Overall, it is more effective to continuously apply RepE at every inference step than to only apply it at the first token \citep{subramani_etal_2022_extracting}. This is because the steering effect diminishes throughout the generation \citep{scalena2024multiproperty}. However, only intervening on the first token can still be very effective \citep{subramani_etal_2022_extracting} and has a lower impact on output quality \citep{scalena2024multiproperty}.

\textbf{Which component of the transformer should be steered? - Unclear.}\\
RepE often modifies activations in the residual stream, the attention or the MLP block, but it is unclear which component is most effective to steer. Existing evaluations only find that steering at the LM head or embedding layer is ineffective \citep{subramani_etal_2022_extracting} and that steering both the attention and MLP blocks is more effective than only intervening on one of them \citep{zhang2024truthxalleviatinghallucinationsediting}.

\textbf{Should only one or many layers be controlled? - Many layers.}\\
It is generally assumed that intervening on all layers at once leads to more effective steering at a higher cost in the general capabilities of the model. However, to the best of our knowledge, there are no experiments proving this. Notably, many concepts can be most effectively steered in middle layers \citep{panickssery2024steeringllama2contrastive,im2025unifiedunderstandingevaluationsteering}.

\section{What Concepts Can Be Controlled with Representation Engineering?}
\subsection{Types of Concepts}
\label{subsec:types}
To illuminate how RepE can be applied, we cluster the concepts that have been controlled using RepE into six categories of concepts. Table \ref{tab:concept_categories} showcases that most RepE papers steer high-level behavioral concepts and that RepE can be successfully applied to all types of concepts. 

\begin{table}[h]
    \caption{The number of papers that attempt to, successfully or not, steer a type of concept.}
    \label{tab:concept_categories}
    \begin{center}
    \begin{tabular}{l c c}
    \toprule
        Type of Concept & Success & Failure \\
        \midrule
        High-level behavioral characteristics & 56 & 8 \\
        Tasks & 11 & 4 \\
        Language and style & 10 & 0  \\
        Knowledge and beliefs & 9 & 1 \\
        Content & 8 & 0 \\
        Values, goals, and ethical principles & 7 & 1 \\
        Linguistic or grammatical features & 6 & 0 \\
        \bottomrule
    \end{tabular}        
    \end{center}
\end{table}

\textbf{High-level behavioral characteristics} like harmfulness or honesty are the most popular category of concepts controlled via RepE. These concepts are fairly abstract, may be context-dependent and cannot easily be specified. They are in reference to the behavior exemplified by the model.

\textbf{Tasks} a model should carry out, including tasks like reasoning, classification and associations can be induced by RepE. Such concepts specify an input-output function, for example, by finding a function vector that triggers the execution of a task \citep{todd2024function}.

\textbf{Language and style} of a text, such as sentiment, style, genre, or language can be controlled with RepE. Such concepts are high-level properties of the generated text.

\textbf{Knowledge and beliefs} encoded in a model about the world can be controlled with RepE. Such concepts reference what the model assumes to be true while generating text. This includes changes to the model's factual knowledge or its situational beliefs (see Section \ref{subsec:knowledge}).

\textbf{Content} of the model's outputs are amendable to RepE, like steering it to focus on a specific topic and contain or not contain some contents. These contents are often not exact text, but higher-level concepts like ``talking about weddings'' or ``social security numbers''. For example, the Concept500 dataset~\citep{wu2025axbenchsteeringllmssimple} was designed for evaluating RepE and contains positive and negative examples for 500 concrete topical concepts. Such concepts are properties of the generated text.

\textbf{Values, goals, and ethical principles} that are encoded in an LLM have been adjusted with RepE. This refers to overarching objectives and principles according to which the LLM is making its decisions. It has been applied to implementing human preferences into the LLM's behavior~\citep{cao2024personalized} or adapting which ethical theories are used for decision-making~\cite{tlaie2024exploring}.

\textbf{Linguistic or grammatical features} like verb conjugations or the gender of nouns are controlled in multiple papers. Such concepts are specific to language, and their presence can easily be verified in the generated text. While controlling them is not of great interest itself, they are used as a concept for interpretability~\citep{hao-linzen-2023-verb} or to evaluate RepE methods~\citep{arora2024causalgym}.

\subsection{Commonly Controlled Concepts}
This section showcases concepts for which RepE has been applied most often. For this, we selected all steered concepts in the surveyed papers. Table \ref{tab:success_concepts} shows the concepts which have been controlled in $\geq 5$ papers and how many of the experiments were successful or failed in steering that concept. Here, success is defined as providing a significant improvement in the measure of the concept compared to the uncontrolled model.

\begin{table}[h]
    \caption{Commonly steered concepts along with the amount of experiments where the concept was or was not successfully controlled.}
    \label{tab:success_concepts}
    \begin{center}
    \begin{tabular}{l c c}
    \toprule
        Target Concept & Success & Failure \\
        \midrule
        Truthfulness & 17 & 1 \\
        Harmfulness & 17 & 0  \\
        Toxicity & 16 & 0 \\
        Fairness & 11 & 0 \\
        Refusal & 10 & 1 \\
        Sentiment & 7 & 0 \\
        Privacy & 5 & 0 \\
        \bottomrule
    \end{tabular}        
    \end{center}
\end{table}

The most common concepts are Truthfulness, Harmfulness, and Toxicity. This is because they are of practical importance in the context of LLM chatbots and because RepE seems well suited to control them, but also because early seminal papers experimented on these concepts and later work followed.
Furthermore, we see a very high success rate for all concepts. However, this statistic should be viewed critically since experiments that fail to control a concept are less likely to be published. Nevertheless, it does provide evidence that these concepts can be controlled effectively.

\begin{mdframed}[style=takeawaybox]
\textbf{Takeaway: What concepts have been controlled with RepE?}\\
There are different types of concepts that can be controlled with RepE, among which high-level behavioral concepts like Truthfulness, Harmfulness, and Fairness are the most popular.
\end{mdframed}

\section{Applications of Representation Engineering}
\label{sec:applications}
After describing some popular concepts that can be effectively steered with RepE, this section describes concrete problems for which RepE has been applied.

\subsection{AI Safety}
\label{subsec:safety}
AI Safety aims to prevent harm caused by or with AI models. In the context of LLMs this often refers to preventing outputs that are harmful to the user, the use of LLMs for harmful applications, or untruthful and hallucinated statements. We also include the prevention of private data leakage and the refusal behavior of LLMs as safety concerns. Furthermore, the field studies threats of future advanced AI models such as deception or self-improvement in current LLMs.

\textbf{Reducing Harmfulness.} 
Harmfulness is a broad category describing outputs that can cause some harm to users, other individuals, or society. It includes more granular categories, such as toxic outputs, wrong information, or private data leakage. To control harmfulness in general, one needs to identify and steer harmful representations~\citep{cai2024selfcontrol,zou_representation_2023,scalena2024multiproperty,chu2024causal,beaglehole2025aggregateconquerdetectingsteering,deng2025rethinking}. This approach can be combined with other safety techniques such as safe-decoding in a defense-in-depth approach \citep{banerjee2024safeinfercontextadaptivedecoding}. RepE can control harmfulness in multi-modal LLMs \citep{wang2024inferaligner}, and harmful representations identified in one language can be transferred to another \citep{xu2024exploring}. Furthermore, the effect of controlling the model's exhibited personality traits on the harmfulness of its outputs has been explored using RepE \citep{ghandeharioun2024whosaskinguserpersonas,zhang2024betterangelsmachinepersonality}.

Toxicity is a more specific concept referring to offensive, aggressive, or inappropriate outputs. RepE has been used to decrease toxicity \citep{wang2024model,qian2024tracingtrustworthinessdynamicsrevisiting,singh2024representation,luo2024paceparsimoniousconceptengineering,jorgensen2024improving,pham2024householderpseudorotationnovelapproach,li2024destein,turner2024steeringlanguagemodelsactivation,chu2024causal,nguyen2025multiattributesteeringlanguagemodels}. 
Concept operators for toxicity have been transferred from smaller to larger models \citep{dong2024contrans}. However, by increasing the intensity of the toxicity representation, RepE can also be used to make models more toxic and induce attacks on LLMs \citep{wang2023backdoor}.

A range of datasets are used to operationalize harm with AdvBench \citep{zou2023universaltransferableadversarialattacks}, HarmfulQA \citep{bhardwaj2023redteaminglargelanguagemodels} and HarmEval \citep{banerjee2024safeinfercontextadaptivedecoding} being among the most popular.
RepE methods tend to prevent harmful outputs more effectively than prompting or fine-tuning-based safeguards \citep{wang2024inferaligner,cao2024nothing,li2024rethinkingjailbreakinglensrepresentation,yu2024robustllmsafeguardingrefusal}.

\textbf{Preventing and Triggering Jailbreaking, Refusal, and Backdoors.} 
Training LLMs to refuse to answer harmful requests is a key mechanism for ensuring that they do not produce harmful and toxic outputs. To stress-test these safety mechanisms, jailbreaks have been developed that can circumvent this refusal behavior \citep{yi2024jailbreakattacksdefenseslarge}. By directly manipulating the internal mechanism that triggers refusal, RepE can serve as a very potent technique to avoid overrefusal or ensure that refusals are effective and appropriate.

\citet{wang2023backdoor} were the first to show that adding a steering vector during inference undermines safety by reducing truthfulness, making outputs more biased and increasing toxicity and harmfulness. This was followed by a range of work that uses RepE to jailbreak models \citep{wang2024model,ball2024understandingjailbreaksuccessstudy,xu2024uncoveringsafetyriskslarge,stickland2024steeringeffectsimprovingpostdeployment,li2024rethinkingjailbreakinglensrepresentation,tran2024initial,zhang2024generalconceptualmodelediting}.

\citet{arditi2024refusallanguagemodelsmediated} find that there is a single direction in the activations that mediates whether refusal behavior is triggered or not. \citet{yu2024robustllmsafeguardingrefusal} ablate the refusal direction while performing safety training, forcing the model to learn more robust refusal behavior. Furthermore, RepE has been used to stop the LLM from refusing to answer benign queries \citep{cao2024nothing,xiao2024enhancing}.

Lastly, RepE has been used to identify backdoors through UFL \citep{mach2024mechanistically} and trigger backdoors by inducing the presence of the backdoor trigger \citep{price2024future}.

\textbf{Making LLMs Truthful.} 
A model is truthful if it outputs true answers. Getting models to give truthful and honest answers is crucial for their performance, but it also increases the trust we can place in them.

\citet{li2023inference} were the first to steer truthfulness using a RepE method. \citet{marks_geometry_2023} find linear probes that can steer the model to treat false statements as true and vice-versa. Overall, RepE methods seem to be particularly suited for steering this concept, often outperforming fine-tuning \citep{ackerman2024representation,Chen_Sun_Jiao_Lian_Kang_Wang_Xu_2024,qian2024tracingtrustworthinessdynamicsrevisiting,qiu2024spectraleditingactivationslarge,li2023inference,liu2024ctrla} although it is sometimes outperformed by prompting \citep{Chen_Sun_Jiao_Lian_Kang_Wang_Xu_2024,li2023inference,wang2024adaptive}. The initial methods have been extended to encapsulate multiple aspects of truthfulness \citep{wang2024adaptive,Chen_Sun_Jiao_Lian_Kang_Wang_Xu_2024}, steer non-linear representations \citep{hoscilowicz2024nonlinearinferencetimeintervention} or steer with different intensities before selecting the most truthful output with a probe \citep{fatahi-bayat-etal-2024-enhanced}.
TruthfulQA is the most commonly used dataset \citep{lin2022truthfulqa}.

Relatedly, hallucinations have been mitigated or induced with RepE \citep{wang2024adaptive,simhi2024constructing,zhang2024truthxalleviatinghallucinationsediting,zhang2024generalconceptualmodelediting,panickssery2024steeringllama2contrastive,beaglehole2025aggregateconquerdetectingsteering}. \citet{simhi2024constructing} find that hallucinations can be more effectively identified from activation before the model answers and more effectively controlled in the activations of attention heads.

\textbf{Increasing Privacy.} 
LLMs are trained on large amounts of uncurated text from the internet that contains private information, which could be leaked by the LLM.

To mitigate these privacy concerns, RepE has been used to prevent private data leakage \citep{zhang2024betterangelsmachinepersonality,qian2024tracingtrustworthinessdynamicsrevisiting,cai2024selfcontrol}. \citet{wu-etal-2024-mitigating-privacy} find that securing specific private data can increase the leakage of other private information, which they successfully suppress by controlling privacy-sensitive neurons. \citet{zeng2024privacyrestore} remove private information from the inputs and apply steering vectors to restore performance. In the tested setups, these techniques can even improve over Differential Privacy methods \citep{zeng2024privacyrestore,wu-etal-2024-mitigating-privacy}.

\textbf{Avoiding Future Risks.} 
The AI Safety community has hypothesized future risks from advanced AI models, and researchers are now attempting to study key aspects of these risks in LLMs. 

A deceptive system could lie to humans about its intentions. While it is possible to make models more truthful in general, \citet{clymer2024poserunmaskingalignmentfaking} are not able to uncover and steer deception. However, sycophancy, which is the tendency of LLMs to adjust their responses to what the user wants to hear, has been successfully controlled \citep{stickland2024steeringeffectsimprovingpostdeployment,panickssery2024steeringllama2contrastive,templeton2024scaling}, although others fail to reproduce this \citep{paulo2024does,van2024extending}. 
AI systems might develop misaligned motivations, but this could be mitigated by engineering the representations of their goals \citep{mini2023understandingcontrollingmazesolvingpolicy}. RepE has controlled an LLMs' exhibited desire for survival \citep{panickssery2024steeringllama2contrastive}, power \citep{zou_representation_2023}, wealth \citep{van2024extending}, and short-term rewards \citep{panickssery2024steeringllama2contrastive,van2024extending}.
Furthermore, \citet{panickssery2024steeringllama2contrastive} control corrigibility and willingness to coordinate with other AIs against humans.
Additionally, understanding the capabilities of a model is important for estimating the risk it poses. \citet{mach2024mechanistically} propose a method that could elicit undiscovered capabilities through unsupervised feature learning. 

\subsection{Ethics}
\label{subsec:ethics}
Many ethical questions surround the development and application of LLMs. These include concerns about societal biases and the fairness of models, aligning the LLM to the values of its users, and encoding ethical reasoning into LLMs.

\textbf{Improving Fairness.} 
Since LLMs are trained on text from the internet, they are prone to encode and amplify existing societal biases. An LLM might reproduce stereotypes, not be representative in its outputs, or treat members from different social groups differently. By identifying how and where specific biases are represented, RepE can aid in uncovering, showcasing, and mitigating such biases.

RepE methods have been used to identify representations of bias-sensitive attributes such as gender, age, or race and control them to reduce the bias in outputs \citep{pham2024householderpseudorotationnovelapproach,chu2024causal,wang2023backdoor,nguyen2025multiattributesteeringlanguagemodels}. RepE has been used to control the sentiment expressed towards social groups \citep{luo2024paceparsimoniousconceptengineering} or how often a group is associated with a specific disease \citep{zou_representation_2023}. 
\citet{singh2024representation} identify representations for gender and dialect bias and removes the bias by steering all inputs to the same gender or dialect class. Furthermore, \citet{durmus2024featuresteering} use SAEs to identify features for gender, age, and political bias. They also find a “Neutrality” and “Multiple Perspectives” feature that reduce bias across many dimensions of bias.

Additionally, RepE can be used to steer the representation of protected attributes to create counterfactual outputs, which can surface biases and thus be used for auditing the model. \citet{chen2024designing} do this by controlling the model's belief about the user’s age, gender, educational level, and socioeconomic status, thus uncovering how the model responds differently to different users. \citet{avitan2024interventionlensrepresentationsurgery} intervene in representations of gender to create counterfactual generations.

Commonly used datasets for identifying the representations of biases include BOLD \citep{10.1145/3442188.3445924}, BBQ \citep{parrish-etal-2022-bbq}, or BiasInBios \citep{10.1145/3287560.3287572} datasets.

\textbf{Aligning with human preferences.} 
Pre-trained LLMs do not necessarily act according to the preferences and values of their users. To mitigate this, LLMs are commonly fine-tuned with methods like RLHF or DPO, where humans give feedback to align the model with their preferences. Instead of fine-tuning the weights, we could also use RepE to align with human preferences.

\citet{cao2024personalized} train a steering vector using a DPO-based loss that makes it more likely for the model to produce desired responses. It is also possible to use synthetically generated preference data to identify and steer relevant models' representations \citep{liu2023aligning,adila2024can}.


\textbf{Changing Ethical Reasoning.} 
To aid in making LLMs act ethically, it is important they understand human ethical reasoning developed over millennia. RepE can help to improve general ethical reasoning \citep{pham2024householderpseudorotationnovelapproach}. It can also steer the model towards specific moral theories like commonsense morality, utilitarianism, or deontological reasoning \citep{tlaie2024exploring,zou_representation_2023,xu2024exploring}.


\subsection{Knowledge Editing}
\label{subsec:knowledge}
LLMs encode a wealth of knowledge in their representations. However, some of that knowledge might be wrong, outdated, or dangerous. Thus, we would like to be able to remove and edit knowledge. Relatedly, LLMs can encode certain beliefs about the world or other systems that can be useful to interpret and edit.

\textbf{Editing Factual Associations.} 
Model editing methods like ROME~\citep{NEURIPS2022_6f1d43d5} have successfully changed factual knowledge stored in LLMs. Similarly, \citet{zou_representation_2023} were able to make an LLM output that the Eiffel Tower is located in Rome. \citet{hernandez2024inspecting} propose REMEDI, which can detect and edit the attributes associated with an entity through its activations. Furthermore, it is possible to use RepE to improve the ability of an LLM to reason with newly provided information \citep{yin2024lofit}.
\citet{zhao2024steeringknowledgeselectionbehaviours} do not edit factual associations but rather control whether an LLM uses its learned parametric knowledge or the contextual knowledge provided in the prompt to answer a question.  Relatedly, it is possible to detect and steer how much the model reproduces memorized text from the training dataset \citep{zou_representation_2023}.

\textbf{Unlearning.} 
Unlearning is concerned with removing harmful or unwanted knowledge in a model. \citet{li2024the} propose Representation Misdirection for Unlearning (RMU) that unlearns hazardous knowledge by pushing harmful representations towards a random vector while retaining harmless ones. However, \citet{xu2024uncoveringsafetyriskslarge} find that RMU and other unlearning methods are not robust against harmful steering vectors. \citet{farrell2024applying} use SAE steering to unlearn hazardous knowledge but find that it is not yet a competitive unlearning method. 
\citet{rozanova-etal-2023-interventional} are able to remove information about specific features. These mixed results indicate that RepE might become a useful tool for unlearning but currently still has weaknesses. 

\textbf{Modulating Situational Awareness.} 
During inference, LLMs may hold beliefs about the situation they are currently in. These could be facts about the environment or other agents, which RepE can potentially identify and control. \citet{chen2024designing} identify and control the models' beliefs about the age, gender, educational level, and socioeconomic status of the user it is currently interacting with. Similarly, \citet{ghandeharioun2024whosaskinguserpersonas} control the model's inferred impression of the user's personas in order to elicit harmful responses. \citet{price2024future} change what the model believes the current year is. Lastly, RepE has been used to improve the model's capability to reason about the beliefs of other agents in Theory of Mind tasks \citep{bortoletto2024benchmarking,zhu2024language}.

\subsection{Task Execution}
\label{subsec:task}
Making LLMs carry out specific tasks is key to making them practically useful. This is usually approached through fine-tuning on task examples, by providing few-shot examples, or by giving precise instructions. Recently, 
RepE has also found applications to this by editing the representations to control which task is executed in what way.

\citet{todd2024function} identify a function vector (FV) that captures an ICL task by finding attention heads that have a causal effect on that task. In contrast, \citet{liu2024incontext} find an in-context vector (ICV) from the activations on contrastive few-shot prompting inputs. \citet{brumley2024comparing} compare these two methods and find that ICVs are better able to steer behavioral tasks, while FVs control functional tasks more effectively. Furthermore, FVs generalize better and deteriorate model quality less.
\citet{jorgensen2024improving} improve the ICV methodology by adding mean centering and \citet{li2024implicit} apply inner and momentum optimization to the in-context vector. These works indicate that In-Context Learning partially works by shifting the representations to the correct task. Additionally, it is possible to identify representations that trigger the model to carry out Chain-of-Thought reasoning without being prompted to do so \citep{zhang2024uncoveringlatentchainthought}.

\subsection{Controlled Text Generation}
\label{subsec:controlled_generation}
Controlled text generation involves generating text with specific attributes or constraints, such as sentiment, style, or topic. The challenge lies in balancing the generation of coherent, natural language while maintaining control over these characteristics. By shifting internal representations, RepE allows control over the desired attributes.

\textbf{Controlling Sentiment.} 
Sentiment is the emotional tone or opinion of a text. Researchers have used RepE to control how negative or positive the tone of generated text is \citep{turner2024steeringlanguagemodelsactivation,konen2024style} and what emotions are expressed in it \citep{cai2024selfcontrol,konen2024style,zou_representation_2023}. Commonly used datasets are GoEmotions \citep{demszky-etal-2020-goemotions} and the Yelp sentiment dataset.

\textbf{Controlling Personality.} 
It is also possible to shift the personality traits exemplified by the model. This can also indirectly influence other properties. \citet{zhang2024betterangelsmachinepersonality} steer personality traits of a model according to the MBTI-scale and study the effects on its safety properties. \citet{weng2024controllm} are able to steer the model towards OCEAN personality traits. By moderating the personalities, they are able to improve reasoning, enhance conversational capacity, and reduce sycophancy. Furthermore, it is possible to control the level of honesty and creativity displayed by the model with RepE \citep{rütte2024a}.

\textbf{Controlling Language, Style, and Genre.} 
\citet{10.1145/3626772.3657819} use input pairs from different languages to improve coherence in cross-lingual information retrieval and \citet{scalena2024multiproperty} find that safety steering vectors derived in English transfer to other languages. RepE can control the style of generated text, like writing Shakespearean or Chinese text \citep{konen2024style,beaglehole2025aggregateconquerdetectingsteering,ma2025dressing} and its genre, like fantasy, sci-fi, or sports \citep{jorgensen2024improving}. It can also steer generation towards specific topics \citep{makelo2025evaluating,templeton2024scaling,turner2024steeringlanguagemodelsactivation}.

\subsection{Performance}
\label{subsec:performance}
RepE has been applied to improve the performance of LLMs on general capabilities and specific tasks.

\textbf{Improving Reasoning.} 
Improving the reasoning abilities of LLMs is an important frontier in NLP research to which RepE has been applied \citep{cai2024selfcontrol,wu2024reftrepresentationfinetuninglanguage,yin2024lofit,hjer2025improving}. 
RepE has been used to identify the representations underlying the Chain-of-Thought (CoT) mechanism and to control the model to produce CoT reasoning without being prompted to do so \citep{hu2024hopfieldian,zhang2024uncoveringlatentchainthought}. However, an attempt to steer the faithfulness of CoT reasoning with regard to the actual decisions made by the LLM was unsuccessful \citep{tanneru2024on}. Additionally, RepE has been used to improve social reasoning abilities \citep{bortoletto2024benchmarking,zhu2024language}.

\textbf{Other Performance Improvements.} 
The capability of LLMs for Natural Language Understanding and Generation tasks has been improved through RepE \citep{wu2024reftrepresentationfinetuninglanguage,wu2024advancing}.
\citet{van2024extending} use RepE for improving the quality of general code and of python-specific code and \citet{lucchetti2024activation} apply RepE to achieve type predictions that are more robust to irrelevant changes in the code.
Furthermore, RepE has been used to make the model provide equivalent answers for semantically equivalent queries \citep{yang-etal-2024-enhancing} and reduce its bias to the ordering of examples and answer options \citep{adila2024discovering}.
\citet{10.1145/3626772.3657819} improve the quality of a multi-lingual information retrieval system by steering for coherence and accuracy. Lastly, \citet{rahn2024controlling} identify that uncertainty in the decision-making of an LLM agent is related to the entropy of its activations, which can be steered to achieve better exploration behavior.

\subsection{Interpretability}
\label{subsec:interpretability}
Interpretability aims to help humans understand the internal processes of AI models. RepE has helped to identify how human-understandable concepts are represented and to study the impact of representations on outputs. 

\textbf{Finding Linear Representations.} 
If a linear vector detects a concept with high accuracy and controls the concept effectively, this is evidence that this direction is how the model represents the concept. Thus, researchers have claimed to find linear representations of concepts such as Truthfulness \citep{marks_geometry_2023}, Refusal \citep{arditi2024refusallanguagemodelsmediated}, models' encoded beliefs about the current year \citep{price2024future}, subject number for conjugations \citep{hao-linzen-2023-verb}, and Harmfulness \citep{xu2024uncoveringsafetyriskslarge}. These results also provide some evidence for the Linear Representation Hypothesis \citep{park2024the}. However, we should retain appropriate skepticism about evidence provided by RepE about the linearity of concept representations (see Section \ref{subsubsec:challeges_RI}).

\textbf{Other Insights.} 
Aside from interpreting the shape of representations, RepE has been used to shed light on many phenomena in LLMs.

\citet{wolf2024tradeoffsalignmenthelpfulnesslanguage} identify a trade-off between helpfulness and safety.
\citet{qian2024tracingtrustworthinessdynamicsrevisiting} use RepE to investigate how safety-critical concepts emerge throughout pre-training. 
\citet{rozanova-etal-2023-interventional} study the representations of safety concepts in different languages. They find them to be similar and see that English safety concepts can be used to steer other languages. Other papers investigate the inner mechanisms of CoT reasoning \citep{hu2024hopfieldian} and the impact of removing a concept \citep{rozanova-etal-2023-interventional}.

However, RepE is not a perfect tool for interoperability, and results can easily be overclaimed. \citet{wang2024does} specifically criticize the idea that editing representations at a location which leads to the desired output change actually provides evidence that the concept representation is localized there. This is because they see that optimal edits at random points are also very effective.

\subsection{Representation Engineering outside LLMs}
\label{subsec:image}

\subsubsection{Image Generation}
A large number of previous works studied controlling image generation by operating on learned representations. \citet{upchurch2017deep} showed that image editing can be done by linear interpolation in deep feature space between images with a certain attribute and images without that attribute, given the hypothesis that ConvNets linearize the manifold of natural images. Rich structures also appear in unsupervised representation learning of Generative Adversarial Networks (GANs). This can enable controlled generation by simple vector arithmetic~\citep{radforddcgan2016} or non-linear traversal~\citep{wang2021hijack} operations on latent vectors to manipulate semantic concepts. Concept classifiers for attributes can be used to disentangle the representations of attributes, thus allowing us to constrain the optimization in a GAN to find directions that maximally affect the concept attribute~\citep{wang2021hijack}. \citet{shen2020interpreting} and \citet{wu2021stylespace} showed that the latent space of StyleGAN2 can be disentangled so that each dimension captures at most one attribute. Others methods train GANs with contrastive learning to learn disentangled representations~\citep{shoshan2021gan}. Recently, such concept disentanglement in the activation space has been attempted in LLMs by training Sparse Auto-Encoders (SAEs)~\citep{huben2023sparse}. 

\textbf{Modulated Features.} Controlled image generation and editing is able to control attributes of a human, such as age, facial hair, eyewear, headwear, pose, facial expressions, and other global features, such as illumination and artistic styles~\citep{shoshan2021gan,bhattad2024stylitgan}. \citet{collins2020editing} perform spatially localized edits based on a concept image. \citet{jahaniansteerability} showed that activations can be steered to control camera movements.

\textbf{Analogy to LLMs.} Some techniques in controlled image generation resemble those in RepE for LLMs. Contastive learning to promote disentanglement of features~\citep{shoshan2021gan} uses contrastive examples where an attribute is or is not present, which is similar to Input Reading with contrastive examples. Other methods use a concept classifier~\citep{jahaniansteerability,wang2021hijack,chen2016infogan} to identify trajectories in the latent space that steer the concept, which resembles identifying a concept operator through Output Optimization. Similar to RepE methods using Unsupervised Feature Learning (see Section \ref{subsec:unsupervised}), \citet{voynov2020unsupervised} discover directions corresponding to changes in a concept by having humans interpret the effect of the changes. \citet{dong2024towards} take inspiration from RepE for LLM safety to derive steering vectors that can be projected out to increase the safety of generated images.

\subsubsection{Games}
\label{subsec:other_areas}
RepE has been used to interpret the internal world models learned by Transformer models that were trained in an autoregressive fashion on board games. \citet{nanda-etal-2023-emergent} find that Othello-GPT represents the color of tiles linearly, and steering that vector affects which moves the model believes to be legal or not. \citet{karvonen2024emergent} conduct similar experiments for a Chess-GPT model, where they find that individual pieces and the skill level of players are linearly represented and can be steered. These works indicate that autoregressive Transformers can learn to implicitly represent world models.
\citet{mini2023understandingcontrollingmazesolvingpolicy} identify a vector that successfully steers the goal pursued by a policy trained to solve mazes. This is promising since it could allow us to directly identify and alter the goals pursued by capable AI Agents.



\begin{mdframed}[style=takeawaybox]
\textbf{Takeaway: Applications}\\
\vspace{-10pt}
RepE has been used in
\begin{itemize}[leftmargin=10pt,itemsep=-3pt]
    \item \textbf{AI Safety} to reduce harmfulness, increase truthfulness, and control refusal behavior.
    \item \textbf{Interpretability} to confirm the presence and shape of concept representations.
    \item \textbf{AI Ethics} to reduce societal bias, align LLM behavior with human preferences, and control ethical reasoning. 
    \item \textbf{Knowledge Editing} to change or unlearn knowledge and adapt model encoded beliefs.
    \item Other areas such as controlling task execution and text generation and improving general model performance.
\end{itemize}
\end{mdframed}

\section{Comparing RepE to Other Methods}
\label{sec:related_areas}

\subsection{Feature Identification and Steering}
\label{subsec:other_feature}
There is a range of methods that aim to identify activations in the model related to a target concept, which can then be used to steer the LLM's internal processes or output behavior. 

\textbf{Activation Patching (AP).} In AP, the activations for some neurons on a specific input are replaced with different activations derived from other inputs. By observing the counterfactual outputs, researchers can study the causal relationship between internal representations and outputs. In contrast, RepE modifies the existing activations instead of fully replacing them. RepE also focuses less on a causal understanding of individual neurons and more on usefully steering behavior.

\textbf{Model Editing (ME).} ME aims to make targeted modifications to the model weights that alter specific knowledge or behavior while preserving overall performance \citep{NEURIPS2022_6f1d43d5,meng2023massediting,fang2024alphaeditnullspaceconstrainedknowledge}. After identifying relevant parameters, one can make precise changes and verify their impact on the model's outputs. In contrast to RepE, ME does not directly attempt to identify representations in the activations space and instead only finds tweaks to the model parameters that bring the desired change.

\textbf{Probing.} In Probing, a classifier is trained to predict the occurrence of a concept in the model's activations. The derived classifier can also be used to adapt the activations during inference. Section \ref{par:dynamic_steering} explains that a probe can be used to dynamically decide the strength of the steering function. Furthermore, high scores of a probe on safety-relevant concepts, like task drift \citep{abdelnabi2024you} or deceptive reasoning \citep{macdiarmid2024sleeperagentprobes,goldowskydill2025detectingstrategicdeceptionusing}, can trigger a shutdown of the model or engage a more expensive safety procedure. While probing can be part of Representation Identification, it is usually not designed to exert control on model behavior.

\textbf{Causal Abstraction (CA).} In CA, a causal model for a task is specified, and an attempt is made to align the internal activations of a model with this high-level model \citep{geiger2024causalabstractiontheoreticalfoundation,pmlr-v236-geiger24a}. If intervening on the causal model has similar effects as intervening on the network representations, it can be said to be a good abstraction. In principle, this can allow us to identify and manipulate model representations. The focus of this method lies on its applications to interpretability, while RepE is also applied to control the LLM's behavior.

\textbf{Concept Erasure (CE).} CE attempts to identify and remove a concept feature from a model's representations. This requires identifying where the feature is represented and then removing it from the activations or weights. Indeed, some RepE methods attempt to erase concepts from the model representations \citep{li2024the,rozanova-etal-2023-interventional}.

\textbf{Mechanistic Interpretability (MI).} MI is not a specific method, but a field of study that aims to reverse engineer the mechanisms learned by Neural Network into human-understandable algorithms. Although RepE partially grew out of MI, \citet{zou_representation_2023} present it as a counterproposal. MI takes a bottom-up view by studying individual neurons and their connections into circuits. 
While such approaches can explain simple mechanisms well, they struggle to scale to higher-level concepts and processes. In contrast, RepE takes a top-down view by interpreting how high-level concepts are implemented as patterns in the representational space spanned by a large population of neurons. However, in practice, there is no clear delineation between these approaches.

\subsection{Meta-study comparing to Other Methods For Behavior Control}
\label{subsec:comparing}
Other methods for controlling the behavior of LLMs include prompting, fine-tuning, and decoding-based methods. We describe them, contrast them to RepE, and conduct a meta-survey that compares them to RepE according to their effectiveness and impacts on the model's capabilities.

For every paper, our meta-survey compares whether the proposed RepE approach or the compared approach works better for each paper. If available, we also compare which approach retains higher general capabilities and how many samples were used. When an experiment in a paper was run for multiple tasks, we averaged the results over the tasks. When there are multiple compared methods from the same category of approach, we compare to the best-performing one. Lastly, we want to caution the reader that most available comparisons come from RepE papers, thus, biasing the results in favor of RepE.


\subsubsection{Prompting}
\label{subsubsec:prompting}
Changing the (system-)prompt given to an LLM is the most straightforward and commonly used way to steer the behavior of a model. However, it does so without any changes to weights or activations and does not provide any interpretability benefits. Furthermore, RepE does not take up space in the context window, while the additional tokens from prompting, especially when using many in-context examples, increases the computational cost.
Similar to RepE, soft-prompting modifies the model in a continuous embedding space. It does so by operating on the input embeddings, whereas RepE modifies internal activations or weights. However, soft-prompting does not have the goal of identifying the representation of a concept.

\textbf{Empirical Comparison.}
There are 24 papers (listed in Appendix \ref{app:compare_list}) that provide experiments comparing the effectiveness between RepE and a prompting method. Across the surveyed papers, RepE is more effective than prompting at steering the target concept in $75\%$ of the cases (see Table \ref{tab:RepE_vs_other_methods}). Furthermore, it often had less impact on the general capabilities of the model. Notably, AxBench~\citep{wu2025axbenchsteeringllmssimple} conducts a thorough comparison between multiple RepE methods and prompting and finds prompting to perform better at detecting and steering concrete content-related concepts.

\begin{table}
    \caption{We compare RepE to prompting, fine-tuning, and decoding by the amount of papers where it is better, equal, or worse at controlling a concept or retaining capabilities.}
    \label{tab:RepE_vs_other_methods}
    \begin{center}
    \begin{tabular}{l c c c c c c}
        \toprule
         RepE vs & \multicolumn{2}{c}{Prompting} & \multicolumn{2}{c}{Fine-tuning} & \multicolumn{2}{c}{Decoding} \\
         \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7}
         & \makecell[c]{Control\\Effectiveness} & \makecell[c]{Capability\\Retention} & \makecell[c]{Control\\Effectiveness} & \makecell[c]{Capability\\Retention} & \makecell[c]{Control\\Effectiveness} & \makecell[c]{Capability\\Retention}\\
         \midrule
         better & 18 & 4 & 18 & 8 & 5 & 3 \\
         equal & 0 & 1 & 3 & 2 & 0 & 0 \\
         worse & 7 & 2 & 5 & 1 & 2 & 0 \\
         \bottomrule
    \end{tabular}        
    \end{center}
\end{table}



\subsubsection{Fine-tuning}
\label{subsubsec:fine-tuning}
Fine-tuning modifies the weights of a pre-trained model by further training it on a specific dataset or reward signal. While fine-tuning does influence the model's representations, it does not do so in a manner that is targeted to identifying and controlling representations of specific concepts. Instead, it applies some uninterpretable, non-sparse changes to the weights that lead to the desired outputs. 
Unlike RepE, fine-tuning does not allow for precise and isolated control of concepts of interest.
Furthermore, many RepE methods provide continuous control, where a concept can be increased or decreased to the desired intensity. 
As mentioned in Section \ref{subsubsec:training_weights}, there is an overlap between fine-tuning and RepE methods that do output optimization and apply a weight steering function.

\textbf{Empirical Comparison.}
There are 25 papers (listed in Appendix \ref{app:compare_list}) that provide experiments comparing the effectiveness between RepE and a fine-tuning method. Hereby RepE methods such as ReFT \citep{wu2024reftrepresentationfinetuninglanguage} and BIPO \cite{cao2024personalized} that are adjacent to fine-tuning are counted towards RepE. Table \ref{tab:RepE_vs_other_methods} shows that RepE steers the target concept more effectively in $72\%$ of comparisons, while fine-tuning is only better in $20\%$ of cases. A majority of comparisons find that fine-tuning deteriorates general model capabilities more heavily in most cases. Figure \ref{fig:RepE_fine_tuning} lightly indicates that RepE might be more sample effective, outperforming fine-tuning in every comparison under $500$ samples.


\begin{figure}[h]
    \centering
    \includegraphics[width=0.45\linewidth]{figures/RepE_vs_fine_tuning_bar.pdf}
    \caption{The number of papers using a number of samples where RepE or fine-tuning offer more effective control.}
    \label{fig:RepE_fine_tuning}
\end{figure}

\subsubsection{Decoding-based Methods}
\label{subsubsec:decoding}
Decoding-based approaches steer the behavior of LLMs by shifting the token probabilities during the decoding step. Contrastive Decoding takes two sets of token probabilities, contrasts them against each other, and consequently shifts the token probabilities \citep{li-etal-2023-contrastive}. Often, this means taking predictions from two different models, one of which is better at the desired behavior. Alternatively, contrastive predictions can be taken from the same model on contrastive inputs. 

Similar to RepE methods, this method steers the behavior of the model during inference time. However, contrastive decoding requires multiple forward passes per generated token. Furthermore, it only superficially shifts token probabilities and does not tap into the internal representations of LLMs.

\textbf{Empirical Comparison.}
There are 7 papers (listed in Appendix \ref{app:compare_list}) that provide experiments comparing the effectiveness between RepE and a decoding-based method. RepE-based methods outperform decoding-based methods in $71\%$ of cases and have much less impact on general model capabilities.

\subsubsection{Combining Methods} 
One great advantage of RepE is that it works well in combination with other approaches. All four papers (listed in Appendix \ref{app:compare_list})  that evaluate this find that combining RepE with a prompting, fine-tuning, or decoding-based method delivers more effective control than any of them alone \citep{banerjee2024safeinfercontextadaptivedecoding,stickland2024steeringeffectsimprovingpostdeployment,li2023inference,wang2024adaptive}. This is typically tested in papers where RepE does not outperform the respective baseline method.

\begin{mdframed}[style=takeawaybox]
\textbf{Takeaway: Comparing RepE to Other Methods}\\
Other methods for identifying and editing concept representations, like Activation Patching or Concept Erasure, are less applicable for controlling output behaviors or have some overlap with RepE. Our meta-survey shows that RepE offers more effective control at a lower decrease of capabilities compared to prompting, fine-tuning, and decoding-based approaches.
\end{mdframed}

\section{Why Does It Work?}
\label{sec:why}
It is somewhat surprising that relatively simple operations on the representations of a model allow us to control high-level behavior while not destroying capabilities. While there are specific reasons for the effectiveness of different methods, this section offers some general explanations for this phenomenon.

\textbf{LLMs already represent human-understandable concepts.} LLMs seem to represent some human-understandable concepts like refusal or honesty. This makes it possible to tap into those representations and change their behavior. Thus, RepE does not need to learn the concept representations anew but simply identifies and promotes them.

\textbf{Models have structured representations.} 
LLMs represent concepts in a structured way. RepE can uncover and exploit this structure. For example, concepts might be represented as linear directions in the activation space of LLMs. This Linear Representation Hypothesis (LRH) \citep{park2024the} implies that scaling the direction corresponding to a concept increases the intensity of that concept. Many RepE methods are explicitly inspired by LRH or implicitly built on it. If the LRH is true, it explains the success of many RepE methods.

The plausibility of the LRH has been an area of much discussion in the interpretability community. In fact, some features have been proven to be represented linearly in autoregressive models trained for board games \citep{nanda-etal-2023-emergent,karvonen2024emergent} and in LLMs. Many concepts in LLMs have been decoded through linear probes \citep{li2024implicit,abdou-etal-2021-language,grand2018semanticprojectionrecoveringhuman}, and the success of many Representation Engineering methods is itself evidence for linear representations of high-level concepts. On the other hand, \citet{engels2025not} find non-linear features, such as circular representations for the days of the week.

\textbf{Resilience to activation manipulation.} Neural Networks are able to remain functional after their activations are manipulated. For example, adding small random perturbations to the activations does not strongly affect model performance~\citep{reagen2018ares}. This resilience might be attributable to dropout during training. Furthermore, the concept operator is derived from previous activations of the same network, which could mean that the manipulated activations still remain in the usual distribution of activations. This makes it possible to retain general capabilities.

\textbf{Control over intermediate variables.} Neural Networks can be seen as programs where activations are intermediate memory containing variables and are functions that perform operations on these variables. Changing variables in that memory changes the further computation and, ultimately, the output. When RepE operates on representations of concepts in the activations, this is similar to editing semantically meaningful variables in the intermediate memory of a Neural Network.

\begin{mdframed}[style=takeawaybox]
\textbf{Takeaway: Why it works}\\
Models represent human-understandable concepts in structured, possibly linear ways. By manipulating these latent variables, we can influence the model's later computations. LLMs remain functional since they are somewhat resilient to internal manipulations.
\end{mdframed}

\section{Challenges in Representation Engineering}
\subsection{Empirical Weaknesses}
\label{subsec:weaknesses}
\textbf{Multi-Concept Control.} 
\label{par:multi_concept_control}
Applying RepE for multiple concepts at once reduces the effectiveness of steering \citep{van2024extending,scalena2024multiproperty} and leads to larger reductions in model capabilities \citep{scalena2024multiproperty}. This might be because applying multiple edits can lead to interference between concept representations \citep{van2024extending}. An exception is that combining specific vectors derived by BiPO \cite{cao2024personalized} seems to increase their effectiveness. Improving multi-concept control is recognised as one of the major challenges in RepE \citep{cao2024personalized,postmus2024steeringlargelanguagemodels,van2024extending,dong2024contrans,chu2024causal}.
While there are proposals to steer different concepts in different layers \citep{van2024extending}, this limits the number of concepts that can be steered simultaneously. Furthermore, \citet{scalena2024multiproperty} dynamically adjust the strength of interventions which reduces the negative effects of multi-concept control. Further, there are attempts to control broader representations, like ``human preferences'', that encapsulate multiple desired concepts \cite{liu2023aligning}. \citet{nguyen2025multiattributesteeringlanguagemodels} improve multi-concept steering by sparsely applying each steering vector only at some tokens and enforcing orthogonality between steering vectors to reduce interference. Lastly, \citet{beaglehole2025aggregateconquerdetectingsteering} report progress on multi-concept steering based on non-linear feature learning.

\textbf{Long-Form Generation.} While RepE can effectively control short answers, there is a lack of evidence for its ability to control long-form generations and multi-turn conversations. Indeed, there is preliminary evidence that operators identified from a training set with short answers do not effectively steer long generations \mbox{\citep{pres2024towards}}. 
This calls for a thorough evaluation of new RepE methods for longer generations. Recent work has started to address this problem by retaining the natural distribution of activations \citep{cao2024personalized} and dynamically adjusting the representation control throughout a generation \citep{scalena2024multiproperty}.

\textbf{Out-of-Distribution Generalization.} To effectively steer a concept, our concept operator should generalize from its training data to different contexts, prompt templates, or different generation tasks. Additionally, if a concept operator does not generalize across different situations, we cannot argue that we have found a general representation of the concept. However, this still presents a challenge for current RepE pipelines, which become less effective when the system prompt differs between the training and test set \citep{tan2024analyzing} and when the operator is identified \textit{after} the answer but employed \textit{before} the answer \citep{simhi2024constructing}. Evaluating and improving the ability of RepE methods for OOD generalization will be crucial \citep{mach2024mechanistically,zou_representation_2023,li2023inference,ackerman2024representation}.

\textbf{Deterioration of Capabilities.} RepE generally leads to a reduction in general language modeling capabilities and the quality and diversity of generated text \citep{scalena2024multiproperty,stickland2024steeringeffectsimprovingpostdeployment,zhang2024generalconceptualmodelediting}. Aside from the quality of the generated text, \citet{park2024measuring} points out that steered models can be worse at instruction following. While this reduction is rather small, it still represents an important cost to employing RepE that should be reduced.
Increasing the strength of the intervention reduces the capabilities~\citep{wu2025axbenchsteeringllmssimple,durmus2024featuresteering}. This effect could appear because RepE disrupts the underlying structure of a model's activations \citep{zhang2024generalconceptualmodelediting}. Thus, finding improvements to reduce or better navigate the trade-off between control effectiveness and general model capabilities will be crucial \citep{li2024the,wu-etal-2024-mitigating-privacy,wang2024adaptive}. Efforts to solve this problem include fine-tuning the model to prevent reductions in quality through RepE \citep{stickland2024steeringeffectsimprovingpostdeployment}, moderating the intervention strength to only steer when necessary \citep{li2024destein}, and more precisely localising the concept representations \citep{wang2024inferaligner}. Others attempt to mitigate this by retaining the underlying structure of the LLM \citep{zhang2024generalconceptualmodelediting}, keeping the magnitude of activations consistent \citep{pham2024householderpseudorotationnovelapproach}, or providing an output-based goal to retain text quality \citep{xu2024uncoveringsafetyriskslarge}. 

\textbf{Learning Specific Concept Representations.} Ideally, RepE would steer a concept without influencing other, unrelated concepts. However, in practice, RepE methods struggle to isolate a concept representation from other concepts. For example, steering towards happiness reduces refusal rates for harmful requests \citep{zou_representation_2023}, steering away from harmfulness makes the model less helpful \citep{wolf2024tradeoffsalignmenthelpfulnesslanguage}, and steering gender awareness can increase age-based biases~\citep{durmus2024featuresteering}. 

\textbf{Learning Complete Concept Representations.} We would like RepE to identify a concept operator which represents the concepts with all its aspects and in different contexts. A concept like Honesty is multi-faceted and is applied differently depending on the context. However, identifying such complex representations is challenging and likely requires a large coverage of aspects and contexts in the training data, as well as a concept operator that is expressive enough. \citet{Chen_Sun_Jiao_Lian_Kang_Wang_Xu_2024} address this by learning multiple orthogonal steering vectors that are supposed to capture different aspects of a concept.

\textbf{Unreliability.} RepE methods are generally unreliable and tend to fail in three ways. Firstly, they are sensitive to the hyperparameters chosen. Often, a small seemingly insignificant change in hyperparameters can cause significant performance decreases \citep{zhang2024generalconceptualmodelediting, adila2024discovering}. This indicates that the chosen RepE setup is not robust. Secondly, there are often concepts that cannot be successfully steered. While it is common for papers to mainly report on the successful applications of their method, more neutral investigations often find concepts that can not be successfully steered \citep{tan2024analyzing,tanneru2024on}. \citet{braun2025understanding} provide evidence that some concepts are not effectively steered because the difference between positive and negative activations is not consistent and not linearly separable. Lastly, even for interventions that are successful in steering the average behavior, there are inputs for which the steering effect is negative \citep{tan2024analyzing,stickland2024steeringeffectsimprovingpostdeployment}. This can especially be the case for examples that were already positive with respect to the concept \citep{im2025unifiedunderstandingevaluationsteering}. This unreliability hinders the deployment of RepE methods.

\textbf{Unreliable Interpretability.} RepE is often used as a tool to interpret LLM representations, but it is questionable whether they provide strong evidence or are largely misleading. Firstly, a failure to identify an effective linear operator for a concept does not prove that the concept is not linearly represented in the model. It could merely be a failure of the method to find the relevant directions. Secondly, it has been shown that finding an effective intervention in specific layers or attention heads does not prove that the concept is actually localized there \citep{tan2024analyzing}. Thirdly, there can be multiple different vectors that successfully steer the concept. Consequently, a direction that steers the model is not necessarily the single direction that represents the concept \citep{subramani_etal_2022_extracting}.

\textbf{Requires Access to Models' Internals.} RepE methods rely on access to models' internals and activations. This limits who is able to create and employ RepE methods. \citet{xu2024uncoveringsafetyriskslarge} partially circumvent this by using RepE on white-box models to find safety vectors, use them to derive attack prompts, and transfer those attack prompts to a black-box model.

\textbf{Computational Cost.} Although RepE is highly efficient during inference and cheap to train, multiple papers aim to improve the computational efficiency of deriving or employing RepE methods. This can be done by removing the need to perform expensive hyperparameter tuning \citep{xu2024uncoveringsafetyriskslarge} or expensive iterative optimization \citep{qiu2024spectraleditingactivationslarge,jorgensen2024improving}. The additional operations necessary to manipulate activations during inference can be avoided by instead manipulating weights \citep{wang2024model}.

\subsection{Principled Challenges}
In addition to these previous general challenges and weaknesses, we further break down the challenges according to each step in the pipeline. 

\subsubsection{Challenges in Representation Identification}
\label{subsubsec:challeges_RI}

\textbf{Spuriously Correlated Concepts.} Current RI methods are mostly not able to disentangle concepts that are correlated. As a result, the identified concept operator will also capture and steer the spuriously correlated concept~\citep{tan2024analyzing}. Concepts can be correlated because they often occur together in the inputs, because the output scoring evaluates both highly or because their representations tend to be activated at the same time.
This presents a major hurdle for precisely identifying a concept representation. Sparse Autoencoders might be able to address this by identifying disentangling concept representations.

\textbf{Concept Misspecification.} Representation Identification forces us to specify the target concept by providing a scoring function or inputs related to the concept. Through these, we are hoping to elicit the model's representation of this concept. However, if the concept specification differs from the intended concepts, the identified concept operator will not be accurate. \citet{deng2025rethinking} point out that RI methods based on contrastive pre-prompts implicitly assume that the model actually follows the pre-prompt.

Furthermore, even when the specified inputs or scoring functions are accurate, they might activate other representations than the ones for the desired concept. For example, we might instruct a model to be honest or dishonest and hope this activates the model representations of ``I will be honest/dishonest''. However, for a model that was trying to deceive, such an input might instead activate a representation for ``The human wants me to be honest/dishonest'', which is importantly different from the intended concept.

Additionally, models might use concepts that are not understandable for humans. They might use an ontology that differs from ours or discover new concepts. In this case, it would be difficult to specify and steer these concepts.

\textbf{Interference from Superposition.} Interpretability research has shown that LLMs are polysemantic since they represent more features than they have neurons \citep{elhage2022toy}. Thus, networks represent features in superposition, meaning that features are not all orthogonal to each other, which results in interference between features. In practice, this will mean that controlling a concept representation will also steer some other concepts. Thus, concept operators are not specific to a concept and have side effects on unrelated concepts.
\citet{nguyen2025multiattributesteeringlanguagemodels} attempt to reduce interference by enforcing orthogonality between concept vectors, thus improving multi-concept steering.

\textbf{Assumptions on Available Data.} RepE methods often only work when specific data is available. Firstly, it is not always possible to provide contrastive inputs that describe the behavior and its opposite \citep{jorgensen2024improving,cai2024selfcontrol,postmus2024steeringlargelanguagemodels}. Secondly, many methods rely on expensive human-annotated ground-truth data \citep{adila2024discovering,adila2024can}. Lastly, methods are not designed to handle noisy and biased data \citet{adila2024can}.

\textbf{Reliance on Models' Own Representations.} RepE methods tap into the existing representations of a model. Consequently, a concept cannot be controlled with RepE if the network did not learn a representation for it. 

\textbf{Interpretability Lacks Ground Truth.} The challenge of developing methods that find representations related to a concept is very difficult because there is no ground truth for where and how the model represents a concept. Thus, we cannot know whether we have identified an accurate representation or what the semantic significance of the concept operator is \citep{herrmann2024standardsbeliefrepresentationsllms}.

\subsubsection{Challenges in Representation Operationalization}
\textbf{Assumptions About Models' Representations.} Methods for identifying concept operators come with assumptions about the shape and geometry of representations in the models' activations. 
Common assumptions about representations are that concepts are represented as linear direction \citep{qiu2024spectraleditingactivationslarge}, that representations do not change throughout a sequence of inputs, that they are the same across context, that they are localized in a single layer, and that they do not rely on interactions between layers. However, if the true form of the representation does not match these assumptions, it will lead to unsuccessful or less effective Representation Identification and Control.

These assumptions have been criticized \citep{luo2024paceparsimoniousconceptengineering,chu2024causal}. Assumptions have been changed and weakened by shifting from a point-in-space to a direction-magnitude view \citep{pham2024householderpseudorotationnovelapproach}, adapting the strength and direction of steering to the context \citep{wang2024adaptive}, or applying non-linear operations \citep{qiu2024spectraleditingactivationslarge,hoscilowicz2024nonlinearinferencetimeintervention}.

An additional challenge is that a feature might not have a single representation. For example, \citep{gw2024writecode} find 800 orthogonal vectors that steer ``write code'' and \citep{mack2024deep} finds 200 independent vectors with which a model represents that a request is harmless.
While this might be due to weaknesses in the specific methods or because of wrong assumptions about the representations geometry, it could also mean that some concepts are represented in multiple ways inside a model. This challenges traditional notions in RepE, where it is assumed that there is one representation of the concept.

\subsubsection{Challenges in Representation Control}
\textbf{Shifting Activations off Their Natural Distribution.} Manipulating the weights and activations of a model can shift the representations off of their natural distribution. In turn, this can lead to a deterioration of LLMs' capabilities since later computations in the model might be disturbed by the unnatural shift in activations. While steering was found to only cause small shifts in the distribution \citep{van2024extending}, this effect could be detrimental when the strength of intervention is increased or many concepts are steered at the same time.

\begin{mdframed}[style=takeawaybox]
\textbf{Takeaway: Challenges in Representation Engineering}\\
RepE struggles with steering multiple concepts at once, controlling long-form generations, and providing a reliable steering effect. These weaknesses arise because of challenges like the inability to disentangle spuriously correlated concepts, to posit correct assumptions about models' representations, and to retain the models' natural distribution of activations.
\end{mdframed}

\section{Opportunities for Future Research}
\label{sec:open_problems}

\subsection{Opportunities to Improve Representation Identification}
\textbf{Refining Identified Operators.} The identified concept operator could be improved to make it better suited for control. For example, an operator identified through Input Reading could be refined by fine-tuning it to produce desired outputs or by decomposing it with an SAE and removing unrelated features. In general, there is promise in developing pipelines that combine different RI methods. For example, \citet{wu2025axbenchsteeringllmssimple} suggest that finding concept operators by jointly learning for concept detection and steering can be more effective.

\textbf{Data-centric RepE.} Most efforts for improving RI focus on new calculations for deriving the concept operator. However, recent work indicates that the quality of a concept operator is strongly dependent on the quality of the dataset it is trained on \citep{tan2024analyzing,braun2025understanding} and improves when it is trained on more data (see Appendix \ref{subsec:samples}). Akin to data-centric approaches to train better models \citep{zha2025datacentric}, a focus on enhancing the quality and quantity of data could be key to improving RI. Furthermore, detailed investigations of the properties of datasets that lead to strong steering performance could unlock important insights.

\textbf{Better Methods for Scoring Outputs.} Currently, most Output Optimization methods simply compare outputs to a ground-truth text. There are more accurate and flexible ways for scoring the alignment of outputs with a concept. One could use a human judge, LLM-as-a-judge, a trained reward model, or specific context-dependent metrics. Furthermore, one could use the performance in an environment, like rewarding actions in a multi-agent negotiation setup based on the final agreement \citep{abdelnabi2024cooperation}.

\textbf{Combine with Other Interpretability Methods.} 
As described in Section \ref{subsec:other_feature}, there exist other methods that aim to identify representations of concepts in LLMs. While these might not produce suitable concept operators themselves, they could provide information about the representation that can be leveraged for Representation Identification. At least, it would be interesting to compare the identified representations from different methods for the same concept.

\textbf{Finding and Analyzing Failure Modes.} We hypothesize that spuriously correlations between concepts and concept misspecification pose challenges for effective Representation Identification. However, there is no empirical evidence for this yet. Exploring such theoretically plausible failure modes could uncover new weaknesses to be addressed by improved RI methods.

\textbf{Automated Interpretability.} 
LLMs can judge which concepts a feature corresponds to by looking at highly activating inputs and outputs. This is commonly done in Unsupervised Feature Learning to identify concept operators. A similar approach could be used to refine concept operators identified by other RI methods. An LLM can automatically judge the inputs and outputs activating for multiple concept operators. % This one needs some more work

\subsection{Opportunities to Improve Representation Operationalization}
\textbf{Expanding on Linear Representations.} 
Most RepE pipelines assume that concepts are represented as linear directions. However, in practice, representations might be non-linear. As noted previously, there is early work on RepE for non-linear steering, but more exploration of non-linear representations is necessary. 

\textbf{Context-Dependent Control.} Current RepE methods assume that the representation of a concept is the same for every context and thus apply the same intervention independent of the inputs. However, the model might represent a concept differently in different contexts. There is already work that dynamically adjusts the strength or composition of concept operators. However, future work could learn separate concept operators for a range of situations or train a small model that can predict the right concept operator to apply in a new situation.

\textbf{Modeling Trajectories of Representations.} Current RepE methods assume that representations of a concept remain static throughout a generation. However, it could also be that the representation of a concept changes over time. In that case, representations of interests are better modeled as trajectories of activations over inference steps.

\textbf{Inter-Layer Dependencies.} 
Recent work \citep{lindsey2024sparse} shows that representations of some concepts can be spread over multiple layers. Current RepE methods are not capable of capturing these inter-layer dependencies. To address this, RI methods could concurrently optimize multiple concept operators at different layers.

\textbf{Modeling Interactions Between Concept Representations.} 
Current RepE methods aim to find an isolated representation of a single concept. But this misses any interactions and relationships between multiple concepts. For example, there could be a specific representation $o^{A\wedge B}$ if concepts $A$ and $B$ are both present, or there could be conditional representations $o^{B|A}$ of $B$ given that $A$ is present. Thus, attempting to identify interacting representations between concepts could uncover more rich concepts the model is using.

\textbf{More Complex Operators.}
Current operators are mostly relatively simple. Perhaps more complex operators could capture more comprehensive and nuanced representations. It might be possible to train a Neural Network to predict a concept operator for the given inputs. Furthermore, an ensemble of concept operators could be trained on different portions of the data or different clusters of activations to capture different aspects of a concept.

\textbf{Feature Geometry as a Hyperparameter.} 
Current work starts by assuming the geometry of the concept representation and then aims to identify the concept representation adhering to that geometry. By trying out multiple geometries for one concept and then measuring their effectiveness, we can expect to find the geometry that most suits the specific concept representation.
Additionally, RepE would benefit from further understanding the geometry of concepts in Transformer Language Models, which is already an active field of study \citep{jiang2024on,csordas2024recurrent}.

\subsection{Opportunities to Improve Representation Control}
\textbf{Combining Weight and Activation Steering Functions.} 
Representations are an interplay between weights and activations. Thus, it could be beneficial to develop methods that control weights and activations together. Naively, one could apply RepE for a concept to the weights and also apply it to the activations. Furthermore, it could be interesting to strengthen or weaken certain representations while training the model, thus guiding which representations are learned by the model.

\textbf{Complex Steering Sequences.} 
By applying steering for different sequences at different time points in a generation, RepE can be used to steer more complex behaviors or ensure that LLMs abide by a predefined pattern. For example, the providers of an LLM might want it to answer code-related questions by first steering for ``detailed reasoning'' while the problem is being analyzed, then for ``not containing errors'' while new code suggestions are generated, and lastly for ``friendliness'' while the model asks for clarifications.

\textbf{More expressive steering functions.}
\citet{krasheninnikov2024steering} find in a toy setting that more expressive steering functions, whose operators have more parameters, can reach a higher performance ceiling and control more complex tasks while requiring more data. This indicates there is promise in developing more expressive steering functions to push the Pareto frontier of steering effectiveness and data-efficiency.

\textbf{Controlling Learning Processes with RepE.}
\citet{yu2024robustllmsafeguardingrefusal} repeatedly ablate the refusal feature to learn more robust refusal mechanisms. This highlights that steering during training can be used to influence the representations that are being learned by the model. Aside from ablating features, representations can be positively steered potentially to make them more salient in the learning process.
On the other hand, learning representations in specific ways can provide concept operators to steer model behavior. For example, \citet{cloud2024gradientroutingmaskinggradients} use Gradient Routing to control where in the network specific capabilities are localized, thus enabling to unlearn specific capabilities during inference. 

\subsection{Possible Applications of Representation Engineering}
After outlining current applications in Section \ref{sec:applications}, we now suggest some further problems that could be addressed with RepE.

\textbf{Agent Goals.} RepE could be used to control the goals pursued by an LLM Agent. Previous work achieved this for RL agents \citep{mini2023understandingcontrollingmazesolvingpolicy}, but the growing popularity of LLM agents trained with Reinforcement Learning \citep{openai2024openaio1card} makes it timely to apply RepE to steer their goals. Controlling an agent away from dangerous and towards desirable goals could be a simple but effective technique for mitigating misalignment in LLM Agents. 
RepE might be uniquely capable of adjusting agent goals compared to prompting. An AI system that intrinsically values a goal is instrumentally incentivized to retain that goal~\citep{bostrom2012superintelligent}, even when it is prompted to change its goal. However, RepE might be able to directly edit the goal the AI is pursuing by steering its own representations.

\textbf{Studying In-Context Learning.} Finding out how and why in-context learning (ICL) works is a hotly debated topic. By controlling representations, it might be possible to steer the hypothesis class ICL uses and thus make progress on understanding the patterns and algorithm enabling ICL. 

\textbf{Preventing Deception.} 
AI systems behaving as if they are aligned with human interests while secretly pursuing different goals is a key worry in the AI Safety community. 
RepE could provide unique control over models' behavior when the AI is attempting to deceive the user~\citep{park2024ai}.
In this case, it will not suffice to instruct the model to be truthful. However, by directly accessing the model's own representations and thus influencing its ``thought process'', RepE might be able to control it to be more honest.

\textbf{Controlling Coooperation.} As LLM agents become more ubiquitous, they will often interact and negotiate with other LLM agents. Through RepE, it might be possible to control how agents behave and cooperate in multi-agent scenarios.

\textbf{Value Engineering.} RepE could be used to identify specific ethical values to let users personalize their LLM agent. For example, a user could specify which beings the AI should give moral consideration to, which moral theory it should apply, or what its stance on specific issues should be.

\textbf{Red Teaming.} RepE could be used when red-teaming and evaluating LLMs. Firstly, for open-weight models, which can easily be controlled towards harmful outcomes through RepE, it should be standard to also apply RepE during red-teaming. Secondly, for closed-weight models, applying RepE to jailbreak a model could serve as a worst-case safety failure that can be evaluated \citep{che2024model}.

\textbf{Studying the Development of Concept Representations.} 
RepE could be used to identify and steer representations for the same concept in different stages of training or different models, thus giving insights about when representations of a concept emerge and how they change.
One could use RepE to study how concepts evolve throughout pre-training and how they are influenced by various post-training methods or how concepts emerge across models of different sizes in the same family.


\subsection{Building a More Rigorous Science of Representation Engineering}
\label{subsec:building}
The field of Representation Engineering is young and does not yet stand on a strong scientific foundation. To enable progress in the field, we point out the opportunity to make research into RepE more rigorous and to provide a better grounding for the field.

\subsubsection{Best Practices for Evaluating RepE Methods}
\label{subsubsec:best_practices}
\textbf{Better Measurements of Controllability.} It is, in general, hard to evaluate whether the outputs of an LLM were successfully steered towards a concept. Many papers, therefore, employ LLMs to judge how much an output corresponds to a concept. While this can be a convenient and effective metric \citep{zheng2023judging}, using LLMs-as-a-judge is known to be biased and sometimes inaccurate \citep{fu-etal-2024-gptscore,zheng2023judging}. Alternatively, many studies rely on existing benchmarks, which largely only include short answers, thus ignoring the evaluation of more free- and long-form generation.

\textbf{Reporting Changes in Models' Quality.} Only a minority (~35\%) of papers report the change in models' capabilities after applying Representation Engineering. Since we know that RepE can reduce these capabilities, it is important to have a standard of reporting measures of LLMs' quality so that we can compare the merits of each method. These could include performance on popular performance benchmarks or changes in perplexity \cite{braun2024sober}.

\textbf{Measuring Out-of-Distribution Generalisation.} As noted earlier, measuring how effectively an operator is able to control an LLM in cases that are different from those where the operator was identified is an open challenge. However, most papers only measure performance on distributions that are similar to the training data, which leads to overly optimistic results.

\textbf{Comparing to SOTA Methods.}
When proposing a new method, its effectiveness should be compared to the current state-of-the-art method. However, often papers only compare to early methods that have since been surpassed.

\textbf{Survivorship Bias.} It is rare for papers that propose new methods for Representation Engineering to demonstrate concepts or situations in which control is not effective. However, papers that set out to neutrally evaluate the method often find weaknesses or limitations not obvious from the original paper. This practice can give false perceptions and stifle progress in the field. Thus, we encourage authors to report on null results.

\textbf{Reporting Variance.} \citet{tan2024analyzing} rightly point out that many papers only report on average steerability. However, it is essential to showcase the distribution of results so readers see whether a method provides reliable steerability.

\textbf{Testing on Different Model Sizes.} Appendix \ref{subsec:models} finds that most RepE experiments are conducted on models between 3-10 billion parameters. To prove the general effectiveness of a RepE method, it is necessary to evaluate it on models of different sizes. Specifically, larger models need to be tested to prove that the method scales to frontier models.


\subsubsection{Benchmarking RepE Pipelines}
There is currently no benchmark that provides a way to comprehensively evaluate the effectiveness of a RepE pipeline. Thus we are currently unsure which pipeline is most effective in which situations. A benchmark can also serve as a guide for the future development of RepE methods by providing clarity about the effectiveness of methods and being a target to aim for. Such unified evaluations can also surface previously undiscovered limitations of methods \citep{brumley2024comparing}.

Such a benchmark should cover a range of concepts of different types since different RepE methods might steer different concepts more effectively \citep{brumley2024comparing}. Hereby, the focus should lie on use cases where RepE is foreseen to be practically useful, such as improving truthfulness, changing goals, adapting situational awareness, or removing societal biases. Furthermore, evaluations should cover multiple models, test the influence of dataset size, and show the impact of different steering strengths. Additionally, it should specifically evaluate the weaknesses of current RepE methods (see Section \ref{subsec:weaknesses}), thus putting a spotlight on improvements on these challenges. Of course, this benchmark should make sure to follow best practices outlined above (Section \ref{subsubsec:best_practices}. 

Such a benchmark could also provide a testing ground for rigorous comparison. One challenge in developing a RepE benchmark lies in providing fair comparisons between RepE methods that have different requirements, assumptions, and goals. They might have different requirements on the available data, require different amounts of compute, or were developed to control different kinds of concepts.

AxBench \citep{wu2025axbenchsteeringllmssimple} is a benchmark that evaluates 9 different RepE methods on 500 concepts. However, all concepts are of a similar type, testing whether a concrete, narrow topic is contained in a text. Extensions to this benchmark should consider different types of concepts. \citet{im2025unifiedunderstandingevaluationsteering} conduct an evaluation of different Input Reading methods for Representation Identification. However, they only focus on high-level behaviors and always use a vector as a concept operator and Linear Addition as a steering function. An extension could consider different concepts and methods for Representation Control.

\subsubsection{Groundwork for RepE}
\textbf{Developing Theoretical Frameworks.}
The field of RepE is almost entirely based on experimental evidence and lacks a clear theoretical framework. 
Fields such as Adversarial ML and Machine Unlearning have greatly benefited from a shared theoretical framework that formulates the settings and objectives \citep{cao2015unlearning}.  
Such a theoretical framework could clearly state the problem RepE attempts to solve and provide criteria for success.
For example, current theoretical work on RepE has found that difference-in-means is optimal at identifying a vector that shifts negative to positive examples \citep{im2025unifiedunderstandingevaluationsteering,singh2024representation}.

\textbf{Causality and RepE.}
RepE aims to identify representations that are causally related to a concept of interest and apply interventions that cause predictable changes in output behavior. Taking a causal perspective on RepE, the treatment could be the relation of a string to the concept or the intervention applied to the representations, and the effect could be the change in activations or in output behavior. Frameworks from causality, such as Structural Causal Models (SCMs) based on Directed Acyclic Graphs (DAGs) \citep{pearl2014probabilistic} and the Potential Outcomes (PO) framework \citep{rubin1974estimating} could provide a theoretical grounding for RepE. 

A causal graph describing the computations of the model allows for the application of the do-calculus. This would enable us (1) to find minimal interventions that have the desired steering effect via the back door criterion, (2) to estimate the causal effect of representations despite the presence of unobserved confounders via the front door rule and (3) to distinguish interventions that have a direct or indirect effect on the desired outputs.
However, this comes with multiple assumptions and requires one to identify how patterns in the activations of a network relate to nodes in the causal graph. 
Indeed, such an attempt has been made by formalizing \citep{geiger2024causalabstractiontheoreticalfoundation} and conducting \citep{pmlr-v236-geiger24a} the search for a DAG of high-level concepts that aligns with the neuron-level activity in the network.

Alternatively, using the PO framework does not require DAG and instead leverages covariates. This allows the use of tools such as \textit{propensity score matching} or \textit{doubly robust estimation} to assess the causal effect of 
modifications to representation and thus enables causal adjustments of the interventions.
However, this assumes that confounding factors can be adequately controlled using observed covariates.

Lastly, causality can inform the design of input datasets from which concept operators are derived \citet{deng2025rethinking}. This can help to better isolate the causal mechanism from confounders and improve ood generalization.

\textbf{Studying RepE on Toy Setting.}
\citet{krasheninnikov2024steering} develop a simple classification task as a toy setting to run controlled experiments evaluating RepE methods for tasks of different complexities. Such toy settings can help us to gain a deeper understanding how and why RepE works. They can also serve as testbeds to evaluate the strengths and weaknesses of different RepE methods. Such toy setups could be used to study the impact of model size, data quality and assumed feature geometries.

\textbf{Building a Library of Concept Representations.}
An ambitious effort could aim to map out a library of concepts and their representation in a specific model. This could serve as a valuable resource for later study. For example, by using this database, it might be possible to train models that can predict the concept operator for new concepts. Furthermore, such a database could serve as a baseline for the development of new RepE methods.

\textbf{Finding Best Practices.} Practitioners using RepE want to easily know which methodology is most effective. However, currently, it is not clear which method should be used practically. Finding best practices for using RepE and then describing them in an easily accessible form could greatly increase the adoption of this technique.

\textbf{Developing Tooling for RepE.} Great software tools for fine-tuning, such as the Transformers library \citep{wolf-etal-2020-transformers}, make it easy to use this technique, thus boosting its adoption. Developing dedicated tooling, such as APIs or libraries, for RepE could be similarly valuable. An example of such work is Gemma Scope \citep{neuronpedia} which allows novices to analyze and steer SAE features derived from Gemma-2-2b.

\begin{mdframed}[style=takeawaybox]
\textbf{Takeaway: Opportunities for Future Research}\\
Future RepE methods can expand restrictive notions of representations, combine multiple RI methods and attempt more complex interventions. Untapped applications lie in steering goals and values of agents or preventing deceptive behavior. To progress the field of RepE, thorough evaluations and best practices are needed.
\end{mdframed}


\section{Conclusion}
\label{sec:conclusion}
Representation Engineering (RepE) is a novel paradigm for controlling and interpreting Large Language Models by manipulating their internal representations. This survey has provided a comprehensive overview of RepE methods, focusing on different methods to identify, operationalize, and control the representations of the model. By synthesizing insights from over 100 recent studies, we have highlighted key advancements, methodologies, and challenges in this rapidly evolving field.

Our analysis reveals that most RepE methods identify representations by providing contrasting inputs and controlling them by adding a vector to the activations. Furthermore, many approaches are based on the Linear Representation Hypothesis.
These methods have been applied to AI Safety, Ethics, and Interpretability by controlling concepts such as Harmfulness, Fairness, and Truthfulness.

We find that RepE tends to be more effective at a lower cost to generation quality compared to prompting, fine-tuning, and decoding-based methods. However, challenges remain in ensuring reliability, robustness, and quality, especially for tasks requiring long-form generation and multi-concept control. Issues such as spuriously related concepts, assumptions about feature geometries, and distributional shifts caused by RepE must be addressed. 
Further research should prioritize the development of more rigorous ways of evaluating and comparing RepE methods. Furthermore, researchers should explore representations that are non-linear, developing over time, spanning layers, and interacting with each other.

We believe RepE will prove to have important advantages compared to other control methods. RepE might be especially advantageous when there is a need to control not only the output of the model but also the internal processes by which it arrives at these outputs. As such, it is likely that RepE will become a common tool for engineers to control and for researchers to study LLMs.

\subsubsection*{Acknowledgments}
We are grateful for thoughtful feedback from Ruta Binkyte, Dmitrii Krasheninnikov, Ivaxi Sheth, Sarath Sivaprasad, Alex Cloud and Simon C. Marshall.


%% The file named.bst is a bibliography style file for BibTeX 0.99c
%\bibliographystyle{named}
\bibliography{main}
\bibliographystyle{tmlr}


\appendix


\section{Meta-survey}
\label{sec:meta}
In order to get a better feeling for how research on RepE is conducted, we conducted a meta-survey where we gathered which models and sample sizes were used. Furthermore, we collect statistics about the papers including authors and publication dates. This meta-survey only includes papers published before 31.8.2024.

\subsection{Commonly Used Models}
\label{subsec:models}
The most popular models are the base and respective instruction-tuned models of Llama-2-7B, Llama-2-13B, Mistral-7B and Llama-3-8B.

We find that a majority of experiments ($164\text{ out of }229$) are performed on base models that have not been trained with RLHF, while $65$ experiments are conducted on preference-tuned models. While experimental results in \citet{dong2024contrans,rütte2024a} find a larger effect from applying RepE to base models than to instruction-tuned models, \citet{bortoletto2024benchmarking,wang2024adaptive} find no clear relationship between instruction-tuning and steering effectiveness.

Figure \ref{fig:model_size} shows that a majority of models to which RepE has been applied on lie between $3-10$ billion parameters. This is underlined by the fact that 9 out of the 10 most popular models for RepE are all between $7-13$ billion parameters. Furthermore, 8 of them belong to the Llama family of models or are derived from Llama models. This calls for more diversity in which model experiments are performed. However, the model scale might not be a crucial factor for the effectiveness of RepE. Across 4 papers that compare the steerability of models across different numbers of parameters within the same model family, none find a clear relationship between scale and steerability \citep{arditi2024refusallanguagemodelsmediated,xu2024exploring,arora2024causalgym,bortoletto2024benchmarking}. 
Furthermore, only \citet{templeton2024scaling} are able to experiment on a proprietary, state-of-the-art model, thus leaving open questions about how effective RepE is at larger scales and for the most capable models. 

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.7\linewidth]{figures/model_size.pdf}
    \caption{The number of experiments applying RepE to a model with specific numbers of parameters.}
    \label{fig:model_size}
\end{figure}

Almost all papers focus on transformer language models, while \citet{paulo2024does} find that CAA can also be applied to RNNs by steering the in-layer activations or the state-space. Furthermore, \citet{adila2024discovering} apply RepE to a vision-language model. This indicates that RepE can be an architecture-agnostic method.

\subsection{Number of Samples}
\label{subsec:samples}
We note how many samples were used during Representation Identification to derive the operator. Figure \ref{fig:num_samples} shows that most papers use between 100 and 1000 samples, showcasing that RepE can be relatively sample efficient. However, as with fine-tuning and ICL, RepE benefits from using more samples. \citet{qiu2024spectraleditingactivationslarge}, \citet{yin2024lofit}, and \citet{wang2024adaptive} find that the effectiveness of steering improves as the number of samples for Representation Identification increases. However, \citet{adila2024discovering} find no such relationship in their unsupervised method. Lastly, \citet{krasheninnikov2024steering} find that more expressive steering functions, whose concept operators have more parameters and that contain more operations, work less well with a low amount of data but can better leverage high amounts of data.

There is no general recommendation on the necessary number of samples, which depends on the model, target concept and method used.
Building more sample-efficient methods and identifying the scaling laws of RepE are promising avenues of inquiry.
 
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.6\linewidth]{figures/num_samples.pdf}
    \caption{Amount of experiments that use a certain amount of samples.}
    \label{fig:num_samples}
\end{figure}


\subsection{Statistics about Publications on Representation Engineering}
\label{subsec:statistics}

\textbf{Applying vs. Improving.} A majority of papers set out to make methodological improvements to RepE (60 out of 87), while 22 mostly focus on applying RepE to solve a problem. Only 3 papers aim to evaluate RepE methods.

\textbf{Academic vs. Industry Papers.} Work on RepE is mostly done by academics, with 58 out of 87 papers only featuring academic co-authors. Another 19 papers are collaborations between academia and industry, while only 8 papers are solely authored by industry researchers.

\textbf{Publication Date.} We take the date of the initial publication of a paper and plot them in Figure \ref{fig:pub_date}. Here, we count the first time a paper was published. If a paper was first uploaded on arXiv and later published in a conference, we count the date of the initial arXiv submission.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{figures/publications_chart.pdf}
    \caption{Linechart with publications over time.}
    \label{fig:pub_date}
\end{figure}

\begin{mdframed}[style=takeawaybox,nobreak=true]
\textbf{Takeaway: Meta-survey}
\begin{itemize}[leftmargin=10pt,itemsep=2pt]
    \item \textbf{Models}: A majority of models used have between 3-10 billion parameters. According to current literature, there appears to be no relationship between RepE effectiveness and model scale.
    \item \textbf{Samples}: More samples make RepE more effective.
    \item \textbf{Publication statistics}: A majority of papers on RepE come from academics. The literature is very new but growing rapidly.
\end{itemize}
\end{mdframed}

\section{Survey Methodology}
\label{sec:methodology}
We conducted a structured survey that resulted in a new taxonomy and summary of existing applications but also serves as a meta-study that compares RepE to other methods and gathers interesting meta-statistics.

\subsection{Literature Search}
\label{subsec:search}
Firstly, we started with 3 seminal papers \citep{zou_representation_2023,turner2024steeringlanguagemodelsactivation,li2023inference} and conducted a forward search. Secondly, we used the combinations of search terms \textit{\{“Activation Steering”, “Representation Engineering”, “Concept Activation Vectors”, “Linear Probe + Steering”\}} + \textit{\{“Language Model”, “LLM”, “Neural Network”\}}. Thirdly, the citations of relevant papers were checked. 

To distinguish papers on RepE from related methods, we adopt five criteria for inclusion and point to papers that were excluded by this criterion.
\begin{itemize}
    \item \textbf{Operates with LLMs.} This excludes the sizeable literature on using steering vectors in image generation models like GANs and Diffusion models, which we describe in Section \ref{subsec:image}.
    \item  \textbf{Identifies a Concept Representation.} A representation related to the target concept needs to be identified instead of only training the network.
    \item \textbf{Controls Models' Behavior.} This excludes work that solely focuses on interpreting LLMs without modifying representations.
    \item \textbf{Performs Post-hoc Representation Identification.} We focus on identifying representation after training instead of retraining the network to produce desired concept representations.
    \item \textbf{Steers the Intermediate Representations.} We focus on representation steering instead of shifting token embeddings or logits.
\end{itemize}

This exhaustive literature search was conducted in September of 2024 and only covers papers published until 31.8.2024. We include many RepE papers published after that date but do not guarantee a complete coverage of later publications.

\subsection{Extracting Information from Papers}
\label{subsec:extracted_information}
While surveying papers, we extracted key information upon which the survey was built. A full list of the information we extract can be found in Appendix \ref{app:extracted_information}.

Regarding the methodology, we developed the taxonomy after a first non-exhaustive read of the literature. On a second, exhaustive read, we classified where methods fall in our taxonomy but also recorded many methodological details that allowed us to map the space of choices when applying RepE.
Furthermore, we identify which target concepts were engineered and what problem the paper attempted to solve, thus letting us cluster papers into categories of applications.
We noted any experimental comparisons with other RepE methods or other methods for controlling LLMs' behavior. Additionally, we gathered experimental details like the model, dataset, and sample size used.
Furthermore, we extracted meta-information like the date and venue of publication and association of the authors. To gather views on RepE, we recorded citations where authors expressed how they see RepE as useful, how they think it works, what weaknesses they see, and which future work would be valuable. See \autoref{app:extracted_information} for all information we extracted from the papers.

We used the assistance of Claude-3.5-Sonnet to extract information from papers. While this sometimes sped up the process of identifying key information, it is not sufficiently reliable to be trusted to retrieve information faithfully. Thus, we read every paper ourselves and confirmed the correctness of any LLM-generated answers before including them in this survey.

\section{Information Extracted from Papers}
\label{app:extracted_information}

\textbf{General.}
\begin{enumerate}
    \item Name of the method
    \item Short summary of the paper
    \item Does it improve Representation Engineering methods or apply it to a problem?
    \item Which area and problem does it apply RepE to?
    \item Open problems/future work
    \item Publication month and year
    \item Venue
    \item Are the authors from academia, industry or a mix?
\end{enumerate}

\textbf{Identification.}
\begin{enumerate}
    \item Is it an application of a previous method? Which one?
    \item Description of the identification method
    \item Is the information about the concept obtained by reading from inputs, optimizing for outputs, or optimizing for an internal loss function?
    \item How are concept-related activations elicited?
    \item How is the concept vector identified from activations?
    \item Is the method supervised or unsupervised?
    \item What data was used for identification?
    \item An example of the prompting format
    \item At which layer, in which component, and for which token are activations read?
\end{enumerate}

\textbf{Control.}
\begin{enumerate}
    \item Description of the control method
    \item How does it engineer activations?
    \item Does it change activations or weights?
    \item At which layer, in which component, and for which token does engineering take place?
    \item Is the change in activations static or dynamically adjusted per input?
    \item Aside from control, does the paper use the concept vector for detection?
\end{enumerate}

\textbf{Concepts.}
\begin{enumerate}
    \item Which concepts were identified and steered?
    \item Which concepts were successfully steered?
    \item Which concepts were not successfully steered?
\end{enumerate}

\textbf{Experimental Details.}
\begin{enumerate}
    \item Is there an experimental comparison to non-RepE methods? If yes, which method is superior?
    \item Is there an experimental comparison to other RepE methods? If yes, which method is superior?
    \item How many examples were used for identification?
    \item Which datasets was the control method evaluated on?
    \item Which models were experimented on?
    \item Summary of interesting results

\end{enumerate}

Furthermore, we noted \textbf{arguments related to RepE} that were only contained in a few papers:
\begin{enumerate}
    \item How does RepE relate to other methods/areas?
    \item Why does RepE work?
    \item What is the theory/impact/promises/justification of RepE?
    \item What weaknesses does RepE have?
    \item What are principled advantages/disadvantages between RepE Methods?
\end{enumerate}


\section{Papers that compare RepE to Prompting, Fine-tuning and Decoding-based methods}
\label{app:compare_list}
To allow for reproducibility of our meta-study in Section \ref{subsec:comparing}, we list the papers from which we gathered the data about experimental comparisons between RepE and prompting, fine-tuning and decoding-based methods.

\textbf{Prompting.} \citet{zhang2024generalconceptualmodelediting,cai2024selfcontrol,konen2024style,arditi2024refusallanguagemodelsmediated,ghandeharioun2024whosaskinguserpersonas,zhang2024uncoveringlatentchainthought,Chen_Sun_Jiao_Lian_Kang_Wang_Xu_2024,stickland2024steeringeffectsimprovingpostdeployment,10.1145/3626772.3657819,qiu2024spectraleditingactivationslarge,leong-etal-2023-self,li2024rethinkingjailbreakinglensrepresentation,luo2024paceparsimoniousconceptengineering,cao2024nothing,scalena2024multiproperty,koo2024smitinselfmonitoredinferencetimeintervention,wang2024inferaligner,liu2024incontext,li2024implicit,liu2024ctrla,wang2024adaptive,li2024dialogue}

\textbf{Fine-tuning.} \citet{lucchetti2024activation,li2024destein,yang-etal-2024-enhancing,price2024future,pham2024householderpseudorotationnovelapproach,liu2024incontext,wang2024inferaligner,koo2024smitinselfmonitoredinferencetimeintervention,wu-etal-2024-mitigating-privacy,yu2024robustllmsafeguardingrefusal,leong-etal-2023-self,qiu2024spectraleditingactivationslarge,10.1145/3626772.3657819,stickland2024steeringeffectsimprovingpostdeployment,qian2024tracingtrustworthinessdynamicsrevisiting,Chen_Sun_Jiao_Lian_Kang_Wang_Xu_2024,zhang2024truthxalleviatinghallucinationsediting,liu2023aligning,wang2024model,ackerman2024representation,li2024the,yin2024lofit,cao2024personalized,wu2024advancing,wu2024reftrepresentationfinetuninglanguage}


\textbf{Decoding-based methods.} \citet{li2024dialogue,li2024destein,cao2024nothing,banerjee2024safeinfercontextadaptivedecoding,leong-etal-2023-self,qiu2024spectraleditingactivationslarge,zhang2024truthxalleviatinghallucinationsediting}

\textbf{Combination.} \citet{wang2024adaptive,li2023inference,stickland2024steeringeffectsimprovingpostdeployment,Chen_Sun_Jiao_Lian_Kang_Wang_Xu_2024}

\end{document}

