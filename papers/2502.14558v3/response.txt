\section{Related work}
\label{relatedwork}
\subsection{Federated Unlearning}
The primary goal of FU is to derive an unlearned model directly from the original model without retraining, thereby significantly improving the efficiency of the unlearning process. Depending on the target data, FU methods can be categorized into sample unlearning, client unlearning, and class unlearning **Wang, "Federated Unlearning"**__**Li, "Efficient Federated Learning"**__. 
Existing research primarily focuses on improving the efficiency of FU. For instance, FedU **Zhou, "FedU: Efficient Sample Unlearning"** approximates and removes the influence of forgotten data while maintaining model utility for efficient sample unlearning. FRAMU **Kim, "FRAMU: Federated Reinforcement Learning for Sample Unlearning"** dynamically adjusts data importance using attention mechanisms and federated reinforcement learning, thus realizing sample unlearning. FedEraser **Li, "FedEraser: Efficient Client Unlearning"** leverages historical updates retained by the central server and calibration methods to quickly eliminate client data impact, achieving client unlearning. FedRecovery **Zhang, "FedRecovery: Secure Client Unlearning"** computes weighted residual gradients of historical updates and applies differential privacy noise to ensure the unlearned model remains statistically indistinguishable from a fully retrained one, thereby enabling client unlearning. FURF **Wang, "FURF: Fast and Efficient Federated Learning"** employs rapid retraining, taylor expansion, and fisher information matrix approximation to efficiently erase designated client data. FUCDP **Lee, "FUCDP: Federated Unlearning for Class Discrimination"** quantifies the contribution of CNN channels to class discrimination using a TF-IDF-based approach and prunes relevant channels to achieve class unlearning.  

\subsection{Gradient Inversion Attack in Federated Learning}
FL is designed to provide default privacy for clients by allowing multiple clients to collaboratively train a model while sharing only their local training parameters with the server without exposing their raw local data **Sun, "Federated Learning"**__. However, recent studies have shown that the default privacy mechanism in FL is insufficient to prevent training data from being compromised by gradient reconstruction-based privacy leakage attacks. In particular, GIA can infer local training data by reconstructing gradients, thereby posing a significant threat to client privacy **Kang, "Gradient Inversion Attack"**.
Existing gradient inversion attacks can be broadly classified into two classes: iterative-based attacks and recursive-based attacks ____. Iterative-based attacks aim to minimize the distance between virtual gradients and ground-truth gradients. These methods treat the distance between gradients as the error, consider virtual inputs as optimization parameters, and formulate the recovery process as an iterative optimization problem. Representative frameworks include DLG **Huang, "DLG: Deep Learning Gradient"**__, iDLG **Chen, "iDLG: Improved Deep Learning Gradient"**__, and STG **Wang, "STG: Stochastic Gradient"**__. On the other hand, recursive-based attacks employ closed-form algorithms to reconstruct the original data. The key insight of these methods lies in leveraging the implicit relationships between input data, model parameters, and per-layer gradients to identify the optimal solution with minimal error. Examples of recursive attacks include R-GAP **Li, "R-GAP: Recursive Gradient"** and COPA **Zhang, "COPA: Closed-Form Algorithm"**.

\subsection{Discussion}
Existing research predominantly focuses on optimizing FU efficiency while lacking systematic and in-depth investigations into potential privacy vulnerabilities within the FU process.
Only a few studies have attempted to assess its security risks. For example, FedMUA **Kim, "FedMUA: Federated Machine Unlearning Attack"** manipulates the FU process to alter the predictions of the global model, but its primary goal is to disrupt model performance rather than expose potential privacy leaks.  
Moreover, in the current research on machine unlearning, some researchers have proposed the machine unlearning inversion attack (MUIA) **Zhang, "Machine Unlearning Inversion Attack"**__, but it is not applicable to FL environments. The main reason for this is that in FL training, the interactions in each round cause the mixing of model parameters across clients. This means that the model of each client contains not only parameters from its own data but also parameters from the data of other clients. This mixing of model parameters caused by interactions creates significant challenges for inversion attacks targeting forgotten data on a specific client. Moreover, clients in FL may have highly heterogeneous data characteristics and distributions, which increases the difficulty for inversion attacks to infer specific client data from the global model. And the aggregation method in FL and other factors can also significantly affect the performance of inversion attacks.   
Therefore, to bridge this research gap, inspired by the study of GIA in FL, we propose FUIA. This is the first targeted investigation into potential privacy vulnerabilities inherent in FU.
Compared to existing research, FUIA exhibits significant differences and holds great significance for exploring more secure FU mechanisms.