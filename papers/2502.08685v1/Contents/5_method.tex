\begin{figure*}[h]
    \centering
    \includegraphics[scale=0.6]{Figs/Framework_Graph.pdf}
    \caption{The architecture of the whole DVR framework. The left side of the figure is batch samples from behavior data as the input of the framework. The framework consists of the inner optimization for the recommendation model and the outer optimization of the data valuator. The yellow and blue glowing two-bend arrows mean the backward propagation to optimize the recommendation model and reinforcement learning to optimize the data valuator, respectively.}
    \label{fig:framework}
\end{figure*}

\section{Methodology}
In this section, we first introduce the data valuator for explainable data valuation that evaluates the data quality by calculating the Shapley value from the game-theoretic perspective. Then, we present the metric adapter that achieves metric adaption for both differentiable and non-differentiable metrics by reinforcement learning in an end-to-end manner.

\subsection{Data Valuator}

Our intuition for explainable data valuation is that a sample's data quality is related to its impact on model performance. That is, samples that contribute more to model performance are considered high-quality data, whereas those with less impact are deemed low-quality data. Therefore, the sample's contribution to the model performance can be viewed as the explanation of the data valuation. 

To incorporate the above intuition, we propose to measure the contribution of each training sample $(u, i, j)$ to the model performance as the data value $w_{uij}$. As samples work together to impact the model's performance collectively, we contend that considering the training sample collectively is better than measuring them in isolation. Based on game theory, the Shapley value~\cite{winter2002shapley} is a popular technique that computes the players' contribution to the outcome by the average marginal contribution considering all possible coalitions. Therefore, we treat each sample as the player cooperating with others and contributing to model performance.

Given the large volume of training samples in the recommender system, the computational complexity associated with computing all potential sample coalitions with the original Shapley value is prohibitively high. To reduce computational complexity, we focus on computing the Shapley value of samples within the training batch data, which is represented as $\mathcal{B}= \{(u_1, i_1, j_1), (u_2, i_2, j_2), ..., (u_{n_b}, i_{n_b}, j_{n_b})\}$. The Shapley value for sample $h_m =(u_m, i_m, j_m)\in \mathcal{B}$ is formulated as:
\begin{equation}
\small
\begin{aligned}
\phi\left(h_m\right)=\sum_{\mathcal{S}\subseteq \mathcal{B}\setminus \{h_m\}}\frac{\left|\mathcal{S}\right|!\left(n_b-\left|\mathcal{S}\right|-1\right)!}{n_b!}\left[v\left(\mathcal{S}\cup \{h_m\}\right)-v\left(\mathcal{S}\right)\right],
\end{aligned}
\end{equation}
where $v(\mathcal{S})$ is the value function that computes the model performance with the sample set $\mathcal{S}$ as the input. Many data valuation methods~\cite{ghorbani2019data} utilize retraining techniques to calculate $v(\mathcal{S})$. However, due to the large computational consumption, it is challenging to apply the retraining-based methods to the scenario of the recommender system.

For the scenario of the recommender system, we employ a customized DNN consisting of Harsanyi Interaction to compute the Shapley value effectively. Specifically, Harsanyi interaction $\mathcal{S}$ represents the sample coalition contributing to the model performance collectively. Each Harsanyi interaction $\mathcal{S}$ makes a specific numerical contribution, denoted by $I(\mathcal{S})$, to the model performance. Based on prior research~\cite{harsanyi1982simplified}, the Shapley value is proved to be calculated as:
\begin{equation}
    \label{eq:interaction}
    \begin{aligned}
        \phi(h_m)=\sum_{\mathcal{S}\subseteq \mathcal{B}:\mathcal{S}\ni h_m}\frac1{|\mathcal{S}|}I(\mathcal{S}).
    \end{aligned}
\end{equation} The customized DNN is specified as $w_{\mathcal{B}}(\Lambda)=\mathcal{G}(\mathcal{B};\Lambda)$, where $\Lambda$ is a set of learnable parameters.

\subsubsection{Sample Encoder}
We first employ the sample encoder to encode each sample within the training batch to obtain the sample representation. For the training sample $h_m\in \mathcal{B}$, we use the following network get the representation of $h_m$:
\begin{equation}
    \begin{aligned}
        \hat{\mathbf{e}}_{m} &= \text{ReLU}(W_1\cdot  \mathbf{e}_{m}+b_1),\\
        \mathbf{h}_{m} &= \text{Tanh}(W_2 \cdot \hat{\mathbf{e}}_{m}+b_2),
    \end{aligned}
\end{equation}
where the input $\mathbf{e}_{m}=[\mathbf{p}_{u_m}; \mathbf{q}_{i_m}; \mathbf{q}_{j_m}]\in \mathbb{R}^{3d}$ is the concatenation of the representation of user $u_m$, item $i_m$, and item $j_m$. $W_1\in \mathbb{R}^{d \times 3d}$, $W_2\in \mathbb{R}^{d}$, $b_1\in \mathbb{R}^{d}$, and $b_2\in \mathbb{R}$. We take $\mathbf{h}_{m}$ as the sample representation.

\subsubsection{Batch Valuator}
To compute the contribution of each training sample to the model performance, the batch valuator is achieved by a customized DNN, which takes batch samples as input to predict the model performance. 

Referring to HarsanyiNet~\cite{chen2023harsanyinet}, the customized DNN is designed by considering the AND relationship between the children nodes of the neuron. Since the customized DNN contains $L$ stacked Harsanyi blocks, we focus on the $m$-th neuron in the $l$-th block. Given the children set $\mathcal{C}_m^{(l)}$, the neural activation $\mathbf{z}_m^{(l)}$ of the neuron $(l, m)$ is computed by applying the AND operation on the Linear network:
\begin{equation}
    \begin{aligned}
        \hat{\mathbf{z}}_m^{(l)}&=\mathbf{A}_m^{(l)}\cdot (\textstyle{\sum_m^{(l)}}\cdot \mathbf{z}^{(l-1)}),\\
        \mathbf{z}_m^{(l)}&=\text{ReLU}(\hat{\mathbf{z}}_m^{(l)}\cdot\prod_{(l,m)\in\mathcal{C}_{m}^{(l)}}\mathbbm{1}(\mathbf{z}_{m}^{(l)}\neq 0)).
    \end{aligned}
\end{equation}
 The input of the $l$-th block are neurons from the last block, denoted as $\mathbf{z}^{(l-1)}=[\mathbf{z}_1^{(l-1)},...,\mathbf{z}_m^{(l-1)}...,\mathbf{z}_{M^{(l-1)}}^{(l-1)}]$ where $M^{(l-1)}$ is the number of neurons in the $(l-1)$-th block. Especially, for the first block, the input neurons are $\mathbf{z}^{(0)}=[\mathbf{h}_1, ...,\mathbf{h}_m,... \mathbf{h}_{n_b}]$. The children set $C_m^{(l)}$ is implemented as a trainable binary diagonal matrix $\textstyle{\sum_m^{(l)}}\in \{0,1\}^{M^{(l-1)}\times M^{(l-1)}}$, which selects children nodes of the neuron $(l, m)$ from the neurons in the last blocks. $\mathbf{A}_m^{(l)}\in \mathbb{R}^{M^{(l-1)}}$ denotes the weight vector. 
 
\subsubsection{Prediction and Optimization}
Neurons in multiple layers of blocks are used to predict the model performance:
\begin{equation}
    \begin{aligned}
        \hat{y} = \sum_{l=1}^{L}(\mathbf{v}^{(l)})^{T}\mathbf{z}^{(l)},
    \end{aligned}
\end{equation}
where $\mathbf{v}^{(l)}=[\mathbf{v}^{(l)}_1, ..., \mathbf{v}^{(l)}_{M^{(l)}}]\in \mathbb{R}^{M^{(l)}}$ denotes the weight vector. $\mathbf{z}^{(l)}=[\mathbf{z}^{(l)}_1, ..., \mathbf{z}^{(l)}_{M^{(l)}}]$ denotes neurons in the $l$-th block. $\hat{y}$ is the prediction value of model performance. We use batch loss to represent the model performance. Then, the model is trained by the mean squared error of predicted value $\hat{y}$ and the batch BPR loss. The loss function is denoted as:
\begin{equation}
\label{eq:mse}
    \begin{aligned}
        \mathcal{L}_\text{MSE}(\mathcal{B};\Lambda) = (\hat{y} - \sum_{(u,i,j) \subset \  \mathcal{B}}\mathcal{L}_{\text{BPR}}(u,i,j;\Theta))^2.
    \end{aligned}
\end{equation}

\subsubsection{Calculation of Shapley Value}
As proved in the prior method~\cite{chen2023harsanyinet}, through the Harsanyi Interaction represented by neurons in the customized DNN, we implement the formula \ref{eq:interaction} to compute Shapley value $\phi(h_m)$ as:
\begin{equation}
    \label{eq:svcal}
    \begin{aligned}
        \mathcal{\hat{C}}_m^{(l)} &:= \cup_{(l,m)\in \mathcal{C}_m^{(l)}}\mathcal{\hat{C}}_{m}^{(l)}, \quad s.t. \mathcal{\hat{C}}_{m}^{(1)}:=\mathcal{C}_m^{(1)},\\
    \phi(h_m)&=\sum_{l=1}^L\sum_{m=1}^{M^{(l)}}\frac1{|\mathcal{\hat{C}}_m^{(l)}|}\mathbf{v}_{m}^{(l)}\mathbf{z}_{m}^{(l)}\mathbbm{1}(\mathcal{\hat{C}}_m^{(l)}\ni h_m).
    \end{aligned}
\end{equation}

\subsection{Metric Adapter}
\subsubsection{The Bilevel Optimization}
We utilize the assigned Shapley value for training sample selection. For the training batch $\mathcal{B}$, the probability of sampling is denoted as $\pi(\mathcal{B},\mathbf{s};{\Lambda}) = \prod_{i=1}^{|\mathcal{B}|}[\hat{w}_{uij}({\Lambda})^{s_{uij}} \cdot (1-\hat{w}_{uij}({\Lambda}))^{1-s_{uij}}]$. In order to calculate the sampling probability, we normalize the Shapley value as $\hat{w}_{uij}$. The selection vector is dentoed as $\mathbf{s}=[s_{uij}]^{|\mathcal{B}|}$ where $s_{uij}$ is the parameterized Bernoulli variable with the probability $\hat{w}_{uij}$ to be 1 and $1-\hat{w}_{uij}$ to be 0. If $s_{uij}=1/0$, the sample  $(u,i,j)$ is selected/not selected for training the recommendation model.

After data selection, we use the selected data to train the recommendation model and data valuator in a bilevel manner. The bi-level optimization is formulated as:
\begin{equation}
    \begin{aligned}
    &\min_{\Lambda}\mathbb{E}_{s \sim \pi(\mathcal{B},\cdot;{\Lambda})} \mathcal{R}(\hat{\mathcal{D}}, f(\Theta^*(\Lambda))) + \mathcal{L}_\text{MSE}(\mathcal{B};\Lambda) \\
    &\text{s.t.}\Theta^{*}(\Lambda)=\mathop{\arg\min}_{\Theta}\mathbb{E}_{s \sim \pi(\mathcal{B},\cdot;{\Lambda})} \mathcal{L}_{\text{BPR}}(u,i,j;\Theta).       
    \end{aligned}
    \label{eq:bilevel2}
\end{equation}
The recommendation evaluator $\mathcal{R}(\hat{\mathcal{D}}, f(\Theta^*(\Lambda)))$ means that $\mathcal{R}(\cdot)$ takes the test data $\hat{\mathcal{D}}$ and recommendation model $f(\Theta^*(\Lambda))$ as input to measure the metric $R$. In the outer optimization, the data valuator $\mathcal{G}(\mathcal{B};\Lambda)$ is optimized by the performance metric and the MSE loss. In the inner optimization, the recommendation model is optimized by the BPR loss of selected training samples.

\subsubsection{Reinforced Metric Adaption}
The sampled BPR loss and MSE loss can be optimized with backprop gradients~\cite{amari1993backpropagation} because they are differentiable. The challenge arises from the fact that many metrics are non-differentiable, which poses difficulties for optimization of the metric in the outer optimization of Eq. (\ref{eq:bilevel2}):
\begin{equation}
    \begin{aligned}
        \hat{l}(\Lambda)=\mathbb{E}_{s \sim \pi(\mathcal{B},\cdot;{\Lambda})} \mathcal{R}(\hat{\mathcal{D}}, f(\Theta^*(\Lambda))).
    \end{aligned}
    \end{equation}

To handle this situation, we propose to use REINFORCE algorithm to optimize non-differentiable metrics. The optimization gradient $\nabla_{\Lambda}\hat{l}(\Lambda)$ can be computed directly as:
\begin{equation}
    \label{eq:metricpg}
    \small
    \begin{aligned}
        &\nabla_{\Lambda}\hat{l}(\Lambda)
        &=\mathbb{E}_{s\sim \pi(\mathcal{B},\cdot;{\Lambda})}[\mathcal{R}(\hat{\mathcal{D}}, f(\Theta^*(\Lambda)))\cdot \nabla_{\Lambda}log(\pi(\mathcal{B},s;{\Lambda}))],
    \end{aligned}
\end{equation}
where $\mathcal{R}(\hat{\mathcal{D}}, f(\Theta^*(\Lambda)))\nabla_{\Lambda}log(\pi(\mathcal{B},s;{\Lambda}))$ is the policy gradient of $ \nabla_{\Lambda}\hat{l}(\Lambda)$, which can gudide the data valuator to identify samples that are beneficial for the metric. In the Eq. \ref{eq:metricpg}, $\nabla_{\Lambda}log(\pi(\mathcal{B},s;{\Lambda}))$ can be further computed as:
\begin{equation}
    \small
    \begin{aligned}
    \label{eq:lambda}
        &\nabla_{\Lambda}log(\pi(\mathcal{B},s;{\Lambda}))
        \\&=\nabla_{\Lambda}\sum_{(u,i,j)\in \mathcal{B}} log(\hat{w}_{uij}(\Lambda)^{s_{uij}}\cdot(1-\hat{w}_{uij}(\Lambda))^{1-s_{uij}})
        \\&=\sum_{(u,i,j)\in \mathcal{B}} s_{uij}\nabla_{\Lambda}log(\hat{w}_{uij}(\Lambda))+(1-s_{uij})\nabla_{\Lambda}log(1-\hat{w}_{uij}(\Lambda)).
    \end{aligned}
\end{equation}
Therefore, given the optimum $\Theta^{*}(\Lambda)$ of the recommendation model, we can update the parameter $\Lambda$ of the data valuator as:
\begin{equation}
    \begin{aligned}
    \label{eq:pge}
        \Lambda \longleftarrow \Lambda - \eta [\mathcal{R}(\hat{\mathcal{D}}, f(\Theta^*(\Lambda)))\nabla_{\Lambda}log(\pi(\mathcal{B},s;{\Lambda})].
    \end{aligned}
\end{equation}
It is clear that Eq. (\ref{eq:pge}) does not involve any implicit differentiation because its component $\mathcal{R}(\hat{D}, f(\Theta^*(\Lambda)))$ and $log(\pi(\mathcal{B},s;{\Lambda})$ can be computed via forward propagation and model evaluation without the need for backpropagation. Therefore, we can update $\Lambda$ via Eq. (\ref{eq:pge}) very efficiently. 
Hence, we can solve our bilevel optimization problem by iteratively optimizing the objective functions of both the recommender and the data valuator. 