\section{Related Work}
\subsection{Data valuation}
Originating from game theory, many data valuation methods based on Shapley value have been proposed. Data Shapley \cite{ghorbani2019data} is commonly used for feature attribution tasks, where the prediction performance of all possible subsets is considered to compute the marginal improvement in performance as the data value. However, this method decouples data valuation from predictor model training, limiting their overall performance due to the lack of joint optimization.
Different from the prior works, DVRL \cite{yoon2020data} directly models the data values using learnable neural networks. For training the data value estimator, DVRL utilizes a reinforcement learning approach coupled with a sampling process. DVRL demonstrates model-agnostic behavior and can be applied even to non-differentiable target objectives. Notably, the learning process is conducted jointly for the data value estimator and the associated predictor model, leading to exceptional outcomes across all considered use cases. To efficiently compute the Shapley value of input variables in a deep learning model, HarsanyiNet utilizes the intermediate neurons the network to capture Harsanyi Interaction~\cite{chen2023harsanyinet}, which allows for the precise computation of the Shapley value.

\subsection{Data Valuation for Recommendation}
Aim at making better use of positive pairs, TCE and RCE \cite{10.1145/3437963.3441800} propose a reweighting method to set user-item pairs with larger loss values as noises so that it assigns lower weights to those positive user-item pairs and reduce their training impact. To evaluate the negative samples, the earlier methods use negative sampling to subsample some unobserved items as negatives based on the predefined sampling distributions, such as uniform and popularity distribution~\cite{10.5555/3367243.3367349,caselles2018word2vec, 10.1145/3097983.3098202, gantner2012personalized,zhu2022gain}. To improve the ability to adapt to different model states and users, the adaptive sampling method is proposed to solve this problem by devising additional measures to select hard samples\cite{10.1145/2556195.2556248, 10.1145/3366423.3380187}. Several works also use auxiliary information to guide the negative sampling process, such as knowledge-graph\cite{10.1145/3366423.3380098, 10.5555/3367243.3367349}. Considering the value of both the positive and negative items jointly on a triplet level, TIL \cite{wu2022adapting} formulates the problem of learning data values as a bilevel optimization task, enabling adaptive learning of the data values for training triplets.