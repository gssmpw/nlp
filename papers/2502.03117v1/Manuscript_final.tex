% ========================
% 확인 사항 (Final Checklist)
% 모든 수식 번호 순서 맞는지 확인 -> 
% 모든 u_n, u_0 --> \omega_n 으로 바뀌었는지 확인 중. (Appendix 등) -> 
% 코멘트 전부 반영 여부 확인 -> 
% 모든 where 쉼표/수식 점 확인 -> 
% 모든 그림 번호/참조 확인 -> 
% 모든 표 번호/참조 확인 -> 
% 모든 수식 참조 맞게 하는지 확인 -> 
% 모든 Thm/Lem/Cor/Rem/Def 번호/참조 확인 -> 
% 모든 레퍼런스/참조 확인 -> 
% 모든 약어 선언 확인 ->  
% ========================



%% bare_jrnl.tex
%% V1.4b
%% 2015/08/26
%% by Michael Shell
%% see http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.8b or later) with an IEEE
%% journal paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/pkg/ieeetran
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall the IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%*************************************************************************


% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. The IEEE's font choices and paper sizes can   ***
% *** trigger bugs that do not appear when using other class files.       ***                          ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/


%% ================= Docmument Class =====================
\documentclass[journal]{IEEEtran} %OFFICIAL
%\documentclass[10pt,times,mathptm,psfig,twocolumn]{article}

%\documentclass[journal, onecolumn]{IEEEtran} % one-column

% CL
% \documentclass[journal,comsoc]{IEEEtran} % CL

% TVT
% \documentclass[journal,10pt]{IEEEtran} % TVT (double-column)

% TCOM
% \documentclass[journal,draftcls,onecolumn,12pt,twoside]{IEEEtranTCOM} % one-column, official(wide space)
% \documentclass[journal,draftcls,onecolumn,12pt,twoside]{IEEEtran} % one-column, unofficial(narrow space)
% \documentclass[journal,twocolumn,twoside]{IEEEtranTCOM} % two-column, official
%\documentclass[journal,twocolumn,twoside]{IEEEtran} % two-column, unofficial

%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[journal]{../sty/IEEEtran}


%% ============== Useful LaTeX Packages ===================
% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)
\usepackage{verbatim}
\usepackage{epsfig,scalefnt,multirow}
\usepackage{url}
\usepackage{ifthen}
\usepackage{mathtools}
\usepackage{cite}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{caption}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{epstopdf}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{cases}
\usepackage{xcolor}
\usepackage{multicol}
\usepackage{stfloats}
\usepackage{blindtext}
\usepackage{caption}
\usepackage{subcaption}

% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/pkg/ifpdf
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
%\usepackage{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of the IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/pkg/cite
% The documentation is contained in the cite.sty file itself.



%\begin{comment}
%a
%v
%c
%\end{comment}



% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at: 
% http://www.ctan.org/pkg/graphicx
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/pkg/epslatex
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). The IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex





% *** MATH PACKAGES ***
%
%\usepackage{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics.
%
% Note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/amsmath

% Calligraphic uppercase
\def\cA{{\mathcal{A}}} \def\cB{{\mathcal{B}}} \def\cC{{\mathcal{C}}} \def\cD{{\mathcal{D}}}
\def\cE{{\mathcal{E}}} \def\cF{{\mathcal{F}}} \def\cG{{\mathcal{G}}} \def\cH{{\mathcal{H}}}
\def\cI{{\mathcal{I}}} \def\cJ{{\mathcal{J}}} \def\cK{{\mathcal{K}}} \def\cL{{\mathcal{L}}}
\def\cM{{\mathcal{M}}} \def\cN{{\mathcal{N}}} \def\cO{{\mathcal{O}}} \def\cP{{\mathcal{P}}}
\def\cQ{{\mathcal{Q}}} \def\cR{{\mathcal{R}}} \def\cS{{\mathcal{S}}} \def\cT{{\mathcal{T}}}
\def\cU{{\mathcal{U}}} \def\cV{{\mathcal{V}}} \def\cW{{\mathcal{W}}} \def\cX{{\mathcal{X}}}
\def\cY{{\mathcal{Y}}} \def\cZ{{\mathcal{Z}}} \def\cz{{\mathcal{z}}}

\def\argmin{\mathop{\mathrm{argmin}}}
\def\argmax{\mathop{\mathrm{argmax}}}
\def\diag{\mathop{\mathrm{diag}}}
\def\inf{\mathop{\mathrm{in}}}
\def\outf{\mathop{\mathrm{out}}}
\def\trace{\mathop{\mathrm{tr}}}
\def\dim{\mathop{\mathrm{dim}}}
\def\Re{\mathop{\mathrm{Re}}}
\def\Im{\mathop{\mathrm{Im}}}

\newcommand{\Ei}{{\mathrm{Ei}}}

\def\bDelta{{\pmb{\Delta}}} \def\bdelta{{\pmb{\delta}}}
\def\bSigma{{\pmb{\Sigma}}} \def\bsigma{{\pmb{\sigma}}}
\def\bPhi{{\pmb{\Phi}}} \def\bphi{{\pmb{\phi}}}
\def\bGamma{{\pmb{\Gamma}}} \def\bgamma{{\pmb{\gamma}}}
\def\bOmega{{\pmb{\Omega}}} \def\bomega{{\pmb{\omega}}}
\def\bTheta{{\pmb{\Theta}}} \def\btheta{{\pmb{\theta}}}
\def\bepsilon{{\pmb{\epsilon}}} \def\bPsi{{\pmb{\Psi}}}
\def\obH{\overline{\bH}}\def\obw{\overline{\bw}} \def\obz{\overline{\bz}}\def\bmu{{\pmb{\mu}}}
\def\b0{{\pmb{0}}}\def\bLambda{{\pmb{\Lambda}}} \def\oc{\overline{\bc}}

% Bold
\def\ba{{\mathbf{a}}} \def\bb{{\mathbf{b}}} \def\bc{{\mathbf{c}}} \def\bd{{\mathbf{d}}}
\def\bee{{\mathbf{e}}} \def\bff{{\mathbf{f}}} \def\bg{{\mathbf{g}}} \def\bh{{\mathbf{h}}}
\def\bi{{\mathbf{i}}} \def\bj{{\mathbf{j}}} \def\bk{{\mathbf{k}}} \def\bl{{\mathbf{l}}}
\def\bm{{\mathbf{m}}} \def\bn{{\mathbf{n}}} \def\bo{{\mathbf{o}}} \def\bp{{\mathbf{p}}}
\def\bq{{\mathbf{q}}} \def\br{{\mathbf{r}}} \def\bs{{\mathbf{s}}} \def\bt{{\mathbf{t}}}
\def\bu{{\mathbf{u}}} \def\bv{{\mathbf{v}}} \def\bw{{\mathbf{w}}} \def\bx{{\mathbf{x}}}
\def\by{{\mathbf{y}}} \def\bz{{\mathbf{z}}} \def\bxb{\bar{\mathbf{x}}} \def\bone{\mathbf{1}}

\def\bA{{\mathbf{A}}} \def\bB{{\mathbf{B}}} \def\bC{{\mathbf{C}}} \def\bD{{\mathbf{D}}}
\def\bE{{\mathbf{E}}} \def\bF{{\mathbf{F}}} \def\bG{{\mathbf{G}}} \def\bH{{\mathbf{H}}}
\def\bI{{\mathbf{I}}} \def\bJ{{\mathbf{J}}} \def\bK{{\mathbf{K}}} \def\bL{{\mathbf{L}}}
\def\bM{{\mathbf{M}}} \def\bN{{\mathbf{N}}} \def\bO{{\mathbf{O}}} \def\bP{{\mathbf{P}}}
\def\bQ{{\mathbf{Q}}} \def\bR{{\mathbf{R}}} \def\bS{{\mathbf{S}}} \def\bT{{\mathbf{T}}}
\def\bU{{\mathbf{U}}} \def\bV{{\mathbf{V}}} \def\bW{{\mathbf{W}}} \def\bX{{\mathbf{X}}}
\def\bY{{\mathbf{Y}}} \def\bZ{{\mathbf{Z}}}

\DeclarePairedDelimiter\norm{\lVert}{\rVert}

\def\red{\textcolor{red}}
\def\blue{\textcolor{blue}}






% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}

\IEEEoverridecommandlockouts
\begin{document}
%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
\title{Meta-Learning-Based People Counting and Localization Models Employing CSI from Commodity WiFi NICs
\thanks{This work was supported in part by the Ministry of Science and ICT (MSIT), Korea, under the Information Technology Research Center (ITRC) support program (IITP-2025-2020-0-01787) supervised by the Institute of Information \& Communications Technology Planning \& Evaluation (IITP) and by Institute of Information \& Communications Technology Planning \& Evaluation (IITP) under 6G·Cloud Research and Education Open Hub (IITP-2025-RS-2024-00428780) grant funded by the Korea government (MSIT). (\textit{Jihoon Cha and Hwanjin Kim contributed equally to this work.})}
\thanks{J. Cha and J. Choi are with the School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon 34141, South Korea (e-mail: charge@kaist.ac.kr; junil@kaist.ac.kr).
	
Hwanjin Kim is with the School of Electronics Engineering, Kyungpook National University, Daegu 41566, South Korea (e-mail:
hwanjin@knu.ac.kr).}}
%
%
% author names and IEEE memberships
% note positions of commas and nonbreaking spaces ( ~ ) LaTeX will not break
% a structure at a ~ so this keeps an author's name from being broken across
% two lines.
% use \thanks{} to gain access to the first footnote area
% a separate \thanks must be used for each paragraph as LaTeX2e's \thanks
% was not built to handle multiple paragraphs
%

\author{Jihoon Cha, \IEEEmembership{Graduate Student Member,~IEEE}, Hwanjin Kim, \IEEEmembership{Member,~IEEE}, \\ and Junil Choi, \IEEEmembership{Senior Member,~IEEE}}


% make the title area
\maketitle

% As a general rule, do not put math, special symbols or citations
% in the abstract or keywords.
\begin{abstract}
% This paper focuses, considers
In this paper, we consider people counting and localization systems exploiting channel state information (CSI) measured from commodity WiFi network interface cards (NICs). While CSI has useful information of amplitude and phase to describe signal propagation in a measurement environment of interest, CSI measurement suffers from offsets due to various uncertainties. Moreover, an uncontrollable external environment where other WiFi devices communicate each other induces interfering signals, resulting in erroneous CSI captured at a receiver. In this paper, preprocessing of CSI is first proposed for offset removal, and it guarantees low-latency operation without any filtering process. Afterwards, we design people counting and localization models based on pre-training. To be adaptive to different measurement environments, meta-learning-based people counting and localization models are also proposed. Numerical results show that the proposed meta-learning-based people counting and localization models can achieve high sensing accuracy, compared to other learning schemes that follow simple training and test procedures.
%\\\\
%Numerical results reveal that
%\\\\ 
%Finally, 
\end{abstract}

% Note that keywords are not normally used for peerreview papers.
\begin{IEEEkeywords}
	People counting, localization, deep neural network (DNN), model-agnostic meta-learning (MAML), commodity WiFi network interface cards (NICs).
\end{IEEEkeywords}






% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle

\section{Introduction}\label{sec1}
\IEEEPARstart{P}ROMISING techniques such as smart home and Internet of things are gradually realized in our daily lives, as electronic devices and even furniture can access to a unified network with wireless communications. For such devices to cooperatively provide personalized services for their surrounding users, wireless human sensing in indoor environments becomes critical \cite{Teixeira:2010,Liu:2019,Liu:2020a,Liu:2020b}. Based on the idea that the signals affected by objects and people record on-site information, human sensing has been studied by using various methods. Camera-based methods employ cameras to recognize human activities \cite{Yang:2003,Aggarwal:2011,Ke:2013}. They do not operate to dark and blind spots and have to handle privacy issues though. Radar-based systems take advantage of wide bandwidth to guarantee high resolution of sensing, and they can extract useful information even in a blind manner \cite{Adib:2015,Lien:2016,Alizadeh:2019}. However, deploying specialized hardware is not preferred as a cost- and power-effective solution in a daily situation. On the contrary, WiFi-based sensing exploits WiFi access points (APs), which are widely deployed in advance, without any additional hardware deployment.

Human sensing with WiFi signals was studied for various purposes, e.g., occupancy and people counting \cite{Khan:2023,Soltanaghaei:2020,Zhao:2019}, gesture and activity recognition \cite{Wang:2017a,Jiang:2020,Yousefi:2017,Bu:2020,Niu:2021,Arshad:2019}, and localization \cite{Noelia:2021,Foliadis:2021,Qian:2017,Qian:2018}, and some researches considered them simultaneously \cite{Tan:2019,Choi:2021,Yan:2021}. All these tasks are based on WiFi channel measurements, for which commodity WiFi network interface cards (NICs) are usually employed. NICs can provide two kinds of channel information from the measurement, namely, received signal strength indicator (RSSI) and channel state information (CSI) \cite{Liu:2020a}. Both of them describe the environment with moving objects and static obstacles, but CSI provides larger dimensional information than RSSI. CSI contains phase information of channels for multiple subcarriers as well as their amplitudes. Since the amplitude and phase information is distorted by offsets, it is required to reduce the offsets to fully exploit CSI \cite{Xie:2019, Wang:2017c}.

Some studies investigated the relation between the signal propagation and the formation of CSI and then estimated the parameters such as angle of arrival, Doppler frequency shift, and time of flight \cite{Qian:2017,Qian:2018,Tan:2019,Niu:2021}. They were able to provide geometrical information of people serving as reflector or blockage, but such parameter-based analyses become infeasible in more complicated situations. Instead, as a high-performance computing system can manage to process a massive amount of data, deep neural network (DNN) is considered as another practical solution to handle sensing problems \cite{Yousefi:2017,Zhao:2019,Wang:2017b,Yan:2021}. With conducting only simple preprocessing, DNN can employ entire CSI as model inputs with minimizing potential loss of channel information by parameter extraction.

While DNN can be utilized to solve the human sensing problem in ordinary indoor environments, its performance is usually dependent of the domains where and when the training and test samples are collected. In other words, although a DNN model achieves very high accuracy for a certain place and time duration, it may not work in different environments.
%(Even in the same place, the newly deployed WiFi access points (APs) can make additional interference, and existing ones also communicate with changing their carrier frequencies.)
Since DNN commonly uses a huge number of samples for training, repeated training for new environments is time-consuming. In this case, transfer learning can adapt the pre-trained model to a new environment with a small number of training samples \cite{Khan:2023,Noelia:2021,Soltanaghaei:2020,Bu:2020,Arshad:2019}.

In this paper, we propose preprocessing of CSI measured from commodity WiFi NICs. This process is to conduct offset removal with simple and low-latency operation and does not require any filtering. As baseline, we explain pre-training-based people counting and localization models with the preprocessed CSI. To achieve high adaptability to test environments different from training environments, we propose meta-learning-based people counting and localization models. The proposed model is based on the model-agnostic meta-learning (MAML) algorithm, which is universally employed by adapting the learned model to each of specialized tasks \cite{Finn:2017}. Compared to other DNN structures, the proposed model using only a small number of adaptation samples after pre-training can achieve high accuracy for different test environments. Through experiments in several environments, we validate the applicability of meta-learning-based model to solve the human sensing problem with CSI measured by commodity WiFi NICs.

The paper is organized as follows. In Section \ref{sec2}, we briefly describe a WiFi signal transmission model, explain a limitation of parameter-based approach, and introduce our human sensing process. Including a base module for learning models, pre-training-based people counting and localization models are introduced as preliminaries in Section \ref{sec3}. In Section \ref{sec4}, we elaborate on preprocessing of CSI that reduces CSI distortion to derive distinct features as model inputs for learning, then propose the meta-learning-based scheme for people counting and localization. In Section \ref{sec5}, we first analyze the computational complexity of the proposed meta-learning-based model, and then present the experimental setup and results to evaluate its performance in comparison to other DNN models. We finally make the conclusion in Section \ref{sec6}.

\textbf{Notations:} Lower and upper boldface letters represent respectively column vectors and matrices. $\bA^{\mathrm{T}}$ denotes the transpose of a matrix $ \bA $, and $\bA^{\dagger}$ is the pseudo-inverse matrix of $ \bA $. The $ (m,n) $-th element of $ \bA $ is denoted by $ [\bA]_{(m,n)} $. $ [\bA]_{(m,:)} $ and $ [\bA]_{(:,n)} $ represent the $ m $-th row and $ n $-th column of $ \bA $, respectively. $\mathbf{1}_N$ is used for the $N\times 1$ all one vector.  $ \Re\{z\} $ and $ \Im\{z\} $ represent the real and imaginary parts of a complex value $ z \in \mathbb{C}$ where $ \mathbb{C} $ is the set of all complex~numbers. $ \lvert z\rvert $ and $ \angle (z) $ are the absolute value and angle of $ z\in \mathbb{C} $, respectively. 

\section{System Model}\label{sec2}
We first explain the signal transmission model with WiFi NICs and erroneous CSI induced by uncontrollable factors. After commenting on the limitation to employ a conventional parameter-based approach for the erroneous CSI, we present overall process of proposed people counting and localization models.

\subsection{WiFi Signal Transmission Model}\label{sec2-1}
Two commodity WiFi NICs connected to personal computers (PCs) serve as a transmitter and a receiver as in Fig. \ref{fig1}. The receiver with $ M_\mathrm{r} $ antennas measures CSI for $ K $ subcarriers based on each packet sent from the transmitter with $ M_\mathrm{t} $ antennas. The PC equipped with the receiver sends the measured CSI to a server, which conducts people counting and localization exploiting preprocessed~CSI.

The WiFi CSI of interest for the $ n $-th packet, the $ k $-th subcarrier, and the $ m $-th spatial link for $ {m \in \{1,2,\cdots,M_\mathrm{r}M_\mathrm{t}(\triangleq M)\}}$ can be modeled as
\begin{align}\label{channel}
h(n,k,m) &= \sum_{\ell=0}^{L_0-1}h_\ell(n,k,m)\notag\\
&=\sum_{\ell=0}^{L-1} \alpha_\ell(n,k,m) e^{-j2\pi f_k \tau_\ell(n,k,m)},
\end{align}
where\footnote{While the number of propagation paths can be different with $n$, $k$, and $m$ in practice, we assume it is the same for all paths to simplify the notation.} $ L_0 $ is the number of propagation paths by reflectors, $ h_\ell(n,k,m) $ is the channel corresponding to the $ \ell $-th path, and $ \alpha_\ell(n,k,m) $ represents the channel attenuation. The $ k $-th subcarrier frequency $ f_k $ is expressed~as
\begin{align}\label{subcarrier frequency}
f_k=f_c+\Delta f_k,
\end{align}
where $ f_c $ is the carrier frequency, and $ \Delta f_k $ is the frequency difference between $ f_k $ and $ f_c $. The propagation delay for the $ \ell $-th path $ \tau_\ell (n,k,m) $ is defined~as
\begin{align}\label{propagation delay}
\tau_\ell(n,k,m)=\bar{\tau}_\ell+\frac{f_{D,\ell}}{f_k}\Delta t_n+\frac{\Delta d_{m_\mathrm{r}} \sin\theta_\ell + \Delta d_{m_\mathrm{t}} \sin\phi_\ell}{v_\mathrm{c}},
\end{align}
where $ \bar{\tau}_\ell $ is the reference delay for the $ \ell $-th path, and $ f_{D,\ell} $ is the Doppler frequency shift by the $ \ell $-th moving reflector. The time duration for movement is denoted by $ \Delta t_n $, and $v_\mathrm{c}$ represents the speed of light. For the $ m $-th spatial link, the spatial difference between the $ m_\mathrm{r} $-th receive antenna and the reference receive antenna is represented by $ \Delta d_{m_\mathrm{r}} $, and $ \theta_\ell $ is the angle of arrival at the receiver. Similarly, the spatial difference and the angle of departure for the $ m_\mathrm{t} $-th transmit antenna are denoted by $ \Delta d_{m_\mathrm{t}} $ and $ \phi_\ell $, respectively.

\begin{figure}
	\centering
	\includegraphics[width=0.95\columnwidth]{Fig/Fig1.eps}
	\caption{Structure of a CSI measurement system using commodity WiFi NICs for people counting and localization.} \label{fig1}
\end{figure}

There are two main factors inducing the CSI measured at the receiver to be erroneous: interference and offsets. For an indoor environment, there can be other WiFi APs and routers for general use. Assuming that they operate with the same or adjacent channel where the WiFi NICs communicate, co-channel interference is added to the CSI of interest. Reflectors outside the measurement environment also influence signal propagation arriving at the receiver. The interfering CSI by $ J $ other WiFi devices can be written as
\begin{align}\label{interference}
h_I(n,k,m) &= \sum_{j=1}^{J}\sum_{\ell=0}^{L_j-1}h_{j,\ell}(n,k,m)\notag\\
&=\sum_{j=1}^{J}\sum_{\ell=0}^{L_j-1} \alpha_{j,\ell}(n,k,m) e^{-j2\pi f_k \tau_{j,\ell}(n,k,m)},
\end{align}
where $ L_j $ is the number of propagation paths for each $ j\in\{1,2,\cdots,J\} $, and $ \alpha_{j,\ell}(n,k,m) $ and $ \tau_{j,\ell}(n,k,m) $ are the channel attenuation and the propagation delay for the interfering signals, respectively.

In addition to the co-channel interference, the CSI is distorted due to the offsets by imperfection of transceivers and their asynchronization. The erroneous CSI measured at the receiver can be modeled as \cite{Xie:2019, Wang:2017c, Speth:1999}
\begin{align}\label{erroneous CSI}
\tilde{h}(n,k,m) =&e^{j2\pi\left(\left(\epsilon_{b}+\epsilon_{s}(n)\right)k+\epsilon_c(n) \right)}\notag\\
&\times\left(h(n,k,m)+h_I(n,k,m)\right)+\epsilon_a(n,k),
\end{align}
where $ \epsilon_b $ and $ \epsilon_{s}(n) $ are the phase offsets from the packet boundary detection uncertainty and the sampling time difference of transceivers, respectively. The carrier frequency difference of transceivers causes the phase offset $ \epsilon_{c}(n) $, and $ \epsilon_a(n,k) $ is additional complex-valued noise. The two offsets $ \epsilon_b $ and $ \epsilon_{s}(n) $ induce subcarrier phase rotation proportional to $ k $, and the degree of rotation also changes steadily in time by $ \epsilon_{s}(n) $ and~$ \epsilon_{c}(n) $.

\subsection{General Sensing Process}
One direct approach for human sensing with CSI is to estimate the signal parameters indicating the existence and movement of the reflectors such as the propagation delay, the Doppler frequency shift, and the angle of arrival. However, existing methods based on this approach are only valid in a single-person environment. They are not proper for multiple-person scenarios, which are the focus in this paper, since it is almost infeasible to separately estimate the signal parameters for each person of interest. Moreover, CSI with co-channel interference and offsets makes it difficult to exploit parameter-based approaches without complicated preprocessing.

\begin{figure}
	\centering
	\includegraphics[width=0.95\columnwidth]{Fig/Fig2.eps}
	\caption{A remote server conducts preprocessing of CSI and people counting and localization. CSI preprocessing includes offset removal of amplitudes and phases for each packet, and a learning model derives a sensing result based on the training and test sample sets of preprocessed CSI.} \label{fig2}
\end{figure}

In this paper, we configure the people counting and localization process depicted in Fig. \ref{fig2}. Erroneous CSI conveyed to the server is preprocessed to remove offsets, followed by a learning model to derive a sensing result through training and test phases. The detailed CSI preprocessing procedure will be explained in Section \ref{sec4-1}. While a basic learning model is depicted in Fig. \ref{fig2}, we will adopt adaptive learning models with a convolutional neural network (CNN) in Sections \ref{sec3} and \ref{sec4-2}. Note that each task of people counting and localization is separately optimized depending on specific use of human sensing. Although we employ the same feature of input data and CNN structure, the two tasks are independently conducted by computing loss functions with different labeling. The localization task can also result in a different output form with either classification or regression, which will be specified in Section \ref{sec3}.

\section{Preliminaries}\label{sec3}

In this section, we first describe a CNN structure as a base module, which is commonly employed for adaptive learning models. As a baseline model, we introduce pre-training-based people counting and localization models to handle the situation that the characteristics of CSI samples largely depend on measurement environments. It is assumed that CSI preprocessing is applied to both the CNN module and the baseline model discussed in this section.

%In this section, we explain the structure of pre-training-based people counting and localization models as baseline. We define preprocessing of CSI for model inputs, followed by the convolutional neural network (CNN) as a base module. To handle the situation that the characteristics of CSI samples largely depend on a measurement environment, we introduce pre-training and adaptation to the people counting and localization \textcolor{red}{models.}

\begin{figure*}
	\centering
	\includegraphics[width=1.5\columnwidth]{Fig/Fig3.eps}
	\caption{A basic CNN module for the proposed people counting and localization models.} \label{fig3}
\end{figure*}

\subsection{CNN Module for Classification}\label{sec3-1}

We employ a simple CNN module to operate rapidly, which is enough to achieve high classification accuracy, as shown in Section \ref{sec5-2}.  In Fig. \ref{fig3}, the preprocessed CSI matrices $ \widehat{\bH}_{n,\mathrm{A}} $, $ \widehat{\bH}_{n,\mathrm{Re}}, $ and $ \widehat{\bH}_{n,\mathrm{Im}} $ for each $ n $ are exploited as a CNN module input whose dimension is $ K\times M\times 3 $. The CNN module consists of $ Q $ convolutional layers, fully connected (FC) layers including a flatten layer, a softmax layer, and a output layer to derive a prediction result. We define the output vector of the softmax layer as
\begin{align}\label{CNN softmax output}
\hat{\bp}_n = \left[\hat{p}_{n,0},\hat{p}_{n,1},\cdots,\hat{p}_{n,C-1}\right]^\mathrm{T},
\end{align}
where $ \hat{p}_{n,c} $ is the likelihood of $ c $-th class. For people counting, each class indicates the number of people in a room. Classification can also be applied to localization by dividing a room into $ C $ sectors, and each sector is labeled by ${c \in \{0,1,\cdots, C-1\}}$. The input-output relationship of classification is expressed as
\begin{align}\label{CNN input output}
\hat{\bp}_n = f_{\text{Cla}, \boldsymbol{\Theta}}\left(\widehat{\bH}_{n,\mathrm{A}}, \widehat{\bH}_{n,\mathrm{Re}}, \widehat{\bH}_{n,\mathrm{Im}}\right),
\end{align}
where $ \boldsymbol{\Theta} $ denotes the model parameters of the CNN. Prediction is finally conducted by computing
\begin{align}\label{CNN output}
\hat{c}_n = \argmax_{c\in \{0,1,\cdots,C-1\}}\hat{p}_{n,c},
\end{align}
at the output layer.

\subsection{CNN Module for Regression}\label{sec3-2}
Classification is an effective approach for localization; however, labeling sectors is neither directly related to physical locations of people nor enough to predict precise locations. Accordingly, we also consider the CNN module for regression, and a softmax layer is not included in this module.
The input of CNN module is the same as the classification model in Section \ref{sec3-1}. The output is the location of $N_L$ people, which is defined as
\begin{align}
\hat{\bd}_n=[\hat{\bd}_{n,1}^\mathrm{T},\cdots,\hat{\bd}_{n,l}^\mathrm{T},\cdots,\hat{\bd}_{n,N_L}^\mathrm{T}]^\mathrm{T},
\end{align}
where $\hat{\bd}_{n,l}=(\hat{x}_{n,l}, \hat{y}_{n,l})^\mathrm{T}$ is the predicted location of $l$-th person. The input-output relationship of regression is given as
\begin{align}\label{CNN input output2}
\hat{\bd}_n = f_{\text{Reg}, \boldsymbol{\Theta}}\left(\widehat{\bH}_{n,\mathrm{A}}, \widehat{\bH}_{n,\mathrm{Re}}, \widehat{\bH}_{n,\mathrm{Im}}\right).
\end{align}
We will verify performance of localization using each of classification and regression models in Section \ref{sec5-2}.
% Moreover, incorrect precision is equally weighted regardless of a physical distance from a true location.


\subsection{Basic Learning Process}\label{sec3-3}
We group the preprocessed CSI matrices for each phase of learning. The CSI sample set is defined as
\begin{align}\label{matrix set}
\cH_s=\left\{\left(\widehat{\bH}_{n,\mathrm{A}}, \widehat{\bH}_{n,\mathrm{Re}}, \widehat{\bH}_{n,\mathrm{Im}}\right) \big| n\in \cI_s \triangleq \cI(I_s,N_s) \right\}.
\end{align}
The received packet index set of the matrices is denoted by $ \cI(I_s,N_s) $ expressed as
\begin{align}\label{index set}
\cI(I_s,N_s)=\{n \mid I_s\leq n < I_s + N_s\},
\end{align} where $ I_s $ is the initial index of measurement, and $ N_s $ is the number of samples. If we employ the basic model in Fig. \ref{fig2}, learning process consists of simple training and test phases, and their corresponding sample sets $\cH_\text{train},\cH_\text{test}$ and index sets $\cI_\text{train},\cI_\text{test}$ can be defined. With $\cH_\text{train}$, the model parameters $ \boldsymbol{\Theta} $ is optimized to minimize a loss function, and two kinds of functions can be defined for classification and regression.

For classification, the categorical crossentropy is employed as the loss function. It measures the difference between the detected likelihood vector $ \hat{\bp}_n $ and the one-hot vector $ \bee_n=[e_{n,0},e_{n,1},\cdots,e_{n,C-1}]^\mathrm{T} $ that indicates the true value of number of people or sector index. The loss function is~computed~as
\begin{align}\label{categorical crossentropy}
\text{CE}(\cH_\text{train} ; \boldsymbol{\Theta}) = -\frac{1}{N_\text{train}}\sum_{n\in \cI_\text{train}}\sum_{c=0}^{C-1}e_{n,c}\log {\hat{p}_{n,c}}.
\end{align}
For regression, the loss function is the mean squared error (MSE) between the true location $ \bd_{n,l} $ and predicted location $ \hat{\bd}_{n,l} $, which is defined as
\begin{align}\label{mean square error}
\text{MSE}(\cH_\text{train} ; \boldsymbol{\Theta}) = \frac{1}{N_\text{train}}\sum_{n\in \cI_\text{train}}\sum_{l=1}^{N_L}\norm{\bd_{n,l}-\hat{\bd}_{n,l}}^2.
\end{align}
We define the loss function according to classification and regression as follows:
\begin{align}\label{loss function}
\text{Loss}(\cH_\text{train} ; \boldsymbol{\Theta})=\begin{cases}\text{CE}(\cH_\text{train} ; \boldsymbol{\Theta})~ &\text{for classification},\\
\text{MSE}(\cH_\text{train} ; \boldsymbol{\Theta})~&\text{for regression.}
\end{cases}
\end{align}
The optimized model is finally evaluated with the preprocessed CSI samples in the test set $\cH_\text{test}$.


\subsection{Pre-Training-Based People Counting and Localization}\label{sec3-4}
\begin{figure}
	\centering
	\includegraphics[width=0.7\columnwidth]{Fig/Fig4.eps}
	\caption{A pre-training-based model using preprocessed CSI as model inputs. The pre-training-based model employs a simple CNN module, whose parameters are optimized through pre-training and adaptation phases.} \label{fig4}
\end{figure}

Since the model parameters $ \boldsymbol{\Theta} $ are learned by using the training samples and employed to validate accuracy with test samples, training and test sets should be related in terms of domain and task \cite{Pan:2010}. Practically, the training set consists of outdated samples, and the test set is lately collected. Although the CSI of interest $ h(n,k,m) $ can be made consistent by maintaining the measurement environment of interest, external WiFi devices induce the interfering CSI $ h_I(n,k,m) $ \cite{Zheng:2017,Huang:2020}, which easily varies in time due to the uncontrollable change of external environments. Therefore, adaptation is required before test where adaptation and test samples are more related than~pre-training~ones.

A pre-training-based model with preprocessed CSI is depicted in Fig. \ref{fig4}.  The pre-training CSI set is denoted by $ \cH_\text{ptr} $ with the index set $ \cI_\text{ptr} $, and the adaptation set $ \cH_\text{adpt} $ consists of the samples for $ n\in\cI_\text{adpt}$. The loss function can be written~as
\begin{align}\label{pre loss function}
\text{Loss}(\cH_s ; \boldsymbol{\Theta})=\begin{cases}\text{CE}(\cH_s ; \boldsymbol{\Theta})~ &\text{for classification},\\
\text{MSE}(\cH_s ; \boldsymbol{\Theta})~&\text{for regression,}
\end{cases}
\end{align}
with equivalent definitions of \eqref{categorical crossentropy}, \eqref{mean square error}\blue{,} and \eqref{loss function} but for ${s \in \{\text{ptr},\text{adpt}\}}$.

The parameters $ \boldsymbol{\Theta} $ are first optimized with the samples in $ \cH_\text{ptr} $ during the pre-training phase with an adaptive moment estimation (ADAM) optimizer to minimize the loss function $\text{Loss}(\cH_\text{ptr} ; \boldsymbol{\Theta})$. Afterwards, regarded as an initial point of adaptation, the model parameters are adjusted by using $ \cH_\text{adpt} $. This parameter optimization during the adaptation phase is conducted with the ADAM optimizer to minimize $\text{Loss}(\cH_\text{adpt} ; \boldsymbol{\Theta})$. The pre-training-based model is evaluated with the preprocessed CSI samples in the test set $ \cH_\text{test} $. Note that the index sets do not share the same elements for each learning phase, i.e., $ {\cI_s \cap \cI_{s'} = \varnothing} $ for $ s\neq s' $ where $ s,s'\in\{\text{ptr},\text{adpt},\text{test}\} $.

%We assume $ I_{\text{ptr}} \ll I_{\text{adpt}}  $, i.e., the pre-training is conducted long before the adaptation to verify the effect of adaptation.

%With only tens of adaptation samples, \red{the pre-trained model can be adjusted with the adaptation samples and achieve high counting accuracy for the test samples. However, the parameters after pre-training are specialized in that measurement environment, and re-training them in adaptation phase} may slowly occur for the largely changed environment.

%Moreover, the uncontrollable change of the external environment should be handled in the case that the measurement of CSI is based on the signal propagation from existing WiFI APs. Surrounding WiFi APs communicate with devices through certain orthogonal frequency-division multiplexing channels, and they choose one of the channels in a way of avoiding interference caused by crowded channel use of other APs. Since the receiver does not always measure the CSI in the same frequency

\section{Proposed CSI Preprocessing and Meta-Learning-Based Scheme}\label{sec4}
In this section, we first propose the CSI preprocessing for the raw data to address the indistinguishable profiles of the input data caused by offset. Additionally, to mitigate performance degradation caused by discrepancies between training and test environments, we develop the meta-learning-based people counting and localization schemes that enable adaptive learning and generalization in diverse scenarios.

\subsection{CSI Preprocessing}\label{sec4-1}

\begin{figure}
	\centering
	\includegraphics[width=0.95\columnwidth]{Fig/Fig5.eps}
	\caption{Procedure of CSI preprocessing. From erroneous CSI, a distinct feature of a given environment is extracted by reducing randomly generated offsets.} \label{fig5}
\end{figure}

We propose preprocessing of CSI for each packet to improve learning efficiency. Raw data of CSI including offsets leads to indistinguishable profiles of real and imaginary values of CSI over different classes, which degrades learning performance. To derive distinct features that are consistent for the same class but different from other classes, we propose effective preprocessing for learning models, as depicted in Fig. \ref{fig5}.


%Different from existing methods, there are no additional denoising and filtering processes that require CSI stacking in time, and thereby this preprocessing can guarantee low latency operation.
When the $ n $-th packet is captured at the receiver, the measured CSI conveyed to the server is expressed as $ \Re\left\{\tilde{h}(n,k,m)\right\} $ and $ \Im\left\{\tilde{h}(n,k,m)\right\} $ for $ k\in \{1,2,\cdots,K\} $ and $ m\in \{1,2,\cdots,M\} $. For preprocessing, we construct CSI as a $ K\times M $ complex-valued matrix, whose $ (k , m) $-th element is written as
\begin{align}\label{CSI matrix}
\left[\widetilde{\bH}_{n}\right]_{(k,m)} =\Re\left\{\tilde{h}(n,k,m)\right\} + j\Im\left\{\tilde{h}(n,k,m)\right\}.
\end{align}
The CSI matrix $ \widetilde{\bH}_{n} $ can be separated as its amplitudes and phases for all elements. We define these separated matrices as $ \widetilde{\bH}_{n,\mathrm{A}} $ and $ \widetilde{\bH}_{n,\mathrm{P}} $ where each element is expressed as
\begin{align}\label{CSI amplitude phase}
\left[\widetilde{\bH}_{n,\mathrm{A}}\right]_{(k,m)} &= \left\lvert\left[\widetilde{\bH}_n\right]_{(k,m)}\right\rvert,\\
\left[\widetilde{\bH}_{n,\mathrm{P}}\right]_{(k,m)} &= \angle{\left(\left[\widetilde{\bH}_n\right]_{(k,m)}\right)}.
\end{align}

Phase offset removal is a general procedure for human sensing using CSI. As seen in \eqref{erroneous CSI}, there are several factors causing the phase offsets, which vary with time and frequency. Most of previous studies handled the issue by using only the amplitudes of CSI \cite{Wang:2017a,Jiang:2020,Wang:2017b}. However, to keep the information of signal propagation in \eqref{propagation delay}, we employ both the phases and amplitudes of CSI. Since a general DNN benefits from a large number of training samples, we are also motivated to conserve the number of CSI samples without stacking over packets for filtering.

We conduct phase offset removal over subcarriers for each packet. Recalling the exponent of phase offsets $\left(\epsilon_{b}+\epsilon_{s}(n)\right)k+\epsilon_c(n)$ in \eqref{erroneous CSI}, it is observed that the phase rotation occurs by the slope $ {\epsilon_{b}+\epsilon_{s}(n)} $ proportional to the subcarrier index $ k $ and the constant $ \epsilon_c(n) $ for each packet [27], [28]. Because this random phase rotation varies with packets, for each of $ M $ spatial links, we first conduct linear regression for unwrapped phases of CSI $ [\widetilde{\bH}_{n,\mathrm{P}}]_{(:,m)} $ over subcarriers and subtract the linear component from the raw phases.
%In \eqref{erroneous CSI}, it is observed that the phase rotation occurs by the slope $ {\epsilon_{b}+\epsilon_{s}(n)} $  proportional to the subcarrier index $ k $ and the constant $ \epsilon_c(n) $. For each of $ M $ spatial links, we first conduct linear regression for unwrapped phases of CSI $ [\widetilde{\bH}_{n,\mathrm{P}}]_{(:,m)} $ over subcarriers and subtract the linear component from the raw phases.
Specifically, we first define a $ K\times 2 $ auxiliary matrix for linear regression as
\begin{align}\label{Auxiliary matrix}
\bX = \begin{bmatrix}
1&1&\cdots&1\\1&2&\cdots&K
\end{bmatrix}^\mathrm{T}.
\end{align}
We then compute the parameter vector including the regression component as
\begin{align}\label{parameter vector}
\boldsymbol{\beta}_{n,m}=\bX^\dagger \left[\widetilde{\bH}_{n,\mathrm{P}}\right]_{(:,m)}.
\end{align}
Removing the linear component with $ \boldsymbol{\beta}_{n,m} $ for each spatial link $ m $, the column vector of phase matrix is derived as
\begin{align}
\left[\overline{\bH}_{n,\mathrm{P}}\right]_{(:,m)}= \left[\widetilde{\bH}_{n,\mathrm{P}}\right]_{(:,m)}-\bX\boldsymbol{\beta}_{n,m}.
\end{align}
There still exists the constant offset for all subcarriers, which can be reduced by subtracting the phase at the reference subcarrier $ k_0 $ to all phases of the $ K $ subcarriers. The phase offset removal is finally conducted as
\begin{align}\label{CSI phase offset}
\widehat{\bH}_{n,\mathrm{P}} = \overline{\bH}_{n,\mathrm{P}} - \mathbf{1}_K\left[\overline{\bH}_{n,\mathrm{P}}\right]_{(k_0,:)}.
\end{align}

In addition to phase offsets, there also exist amplitude offsets by uncertainty in power control \cite{Xie:2019}. While previous works, e.g., \cite{Zhao:2019,Choi:2021}, conducted filtering process for amplitudes of CSI of consecutive packets, we normalize the amplitudes over subcarriers for each packet and spatial link, written as
\begin{align}\label{CSI amplitude offset}
\left[\widehat{\bH}_{n,\mathrm{A}}\right]_{(:,m)} = 
\frac{\left[\widetilde{\bH}_{n,\mathrm{A}}\right]_{(:,m)}}{\frac{1}{K}\sum_{k=1}^K\left[\widetilde{\bH}_{n,\mathrm{A}}\right]_{(k,m)}}.
\end{align}
Based on $ \widehat{\bH}_{n,\mathrm{A}} $ and $ \widehat{\bH}_{n,\mathrm{P}} $, we construct a complex-valued preprocessed CSI matrix and separate it into real and imaginary parts to derive two real-valued matrices, whose elements are~expressed~as
\begin{align}\label{CSI processed}
\left[\widehat{\bH}_{n,\mathrm{Re}}\right]_{(k,m)} = \Re\left\{{\left[\widehat{\bH}_{n,\mathrm{A}}\right]_{(k,m)} e^{j \left[\widehat{\bH}_{n,\mathrm{P}}\right]_{(k,m)}}}\right\},\\
\left[\widehat{\bH}_{n,\mathrm{Im}}\right]_{(k,m)} = \Im\left\{{\left[\widehat{\bH}_{n,\mathrm{A}}\right]_{(k,m)} e^{j \left[\widehat{\bH}_{n,\mathrm{P}}\right]_{(k,m)}}}\right\}.
\end{align}
For each packet, we employ these preprocessed matrices $ \widehat{\bH}_{n,\mathrm{A}} $, $ \widehat{\bH}_{n,\mathrm{Re}}, $ and $ \widehat{\bH}_{n,\mathrm{Im}} $ as CNN model inputs.\footnote{The real, imaginary, and amplitude of channels are used to the CNN inputs for the performance improvements in \cite{Ahmet:2020}.} Different from existing methods, there are no additional denoising and filtering processes that require CSI stacking over packets, which enables to exploit a large number of CSI samples separately by packet. To verify the impact of CSI preprocessing, the detailed performance comparison with the raw CSI data is presented in Section \ref{sec5-2}.



\subsection{Meta-Learning-Based People Counting and Localization Models}\label{sec4-2}

\begin{figure}
	\centering
	\includegraphics[width=0.95\columnwidth]{Fig/Fig6.eps}
	\caption{The proposed meta-learning-based people counting and localization models. The meta-training phase is to learn the way how well the model is adaptive to each of various environments. The meta-adaptation phase exploits only a few adaptation samples from a new environment.} \label{fig6}
\end{figure}

Pre-trained models explained in Section \ref{sec3-4} are used as a straightforward approach to people counting and localization. However, when the test environment differs from the training environment, these pre-trained models may suffer from considerable performance loss. This is because the model parameters can still be dependent on the training environment even after adaptation. To mitigate this issue, we propose the meta-learning-based people counting and localization models employing MAML for better initialization and adjustment of model parameters in meta-adaptation and meta-test phases. As depicted in Fig. \ref{fig6}, the proposed learning model consists of meta-training phase that includes multiple pre-training and validation processes, meta-adaptation phase, and meta-test phase. The proposed model equivalently adopts the CNN structure and the CSI preprocessing explained in Sections \ref{sec3-1}, \ref{sec3-2}, and \ref{sec4-1}. However, during the meta-training phase, the model parameters learn a general way the model can be easily adapted to any measurement environments. The parameters are consequently optimized into more generalized ones and can become adequately adjusted for the meta-adaptation and meta-test environment.

For the CSI grouping in the meta-learning-based model with the definition of CSI sample set in \eqref{matrix set}, several pre-training sets $ \cH_{\text{ptr}(u)} $ for $ {u \in \{1,2,\cdots,U\}} $ are chosen as $ U $ specific tasks. Each of the sets includes the CSI samples corresponding to the index set $ \cI_{\text{ptr}(u)} $. Similarly, we define the validation sets $ \cH_{\text{val}(u)} $ and the corresponding index sets $ \cI_{\text{val}(u)} $ for $ u \in \{1,2,\cdots,U\} $. The CSI sample sets for meta-adaptation and meta-test phases are denoted by $ \cH_\text{adpt} $ and $ \cH_\text{test} $ same as in Section~\ref{sec3-4}. Meanwhile, similar to \eqref{pre loss function}, the loss function in the meta-learning-based model can be expressed as
\begin{align}\label{inner loss function}
\mathrm{Loss}\left(\cH_s;\boldsymbol{\Theta}_t\right)=\begin{cases}\text{CE}(\cH_s ; \boldsymbol{\Theta}_t)~ &\text{for classification},\\
\text{MSE}(\cH_s ; \boldsymbol{\Theta}_t)~&\text{for regression,}
\end{cases}
\end{align}
for $ {s\in \bigcup_{u=1}^U \{\text{ptr}(u),\text{val}(u)\}\cup\{\text{adpt}\}, t\in \{0,1,\cdots,U\}} $.

During the meta-training phase, the proposed model optimizes the meta-learner parameters $ \boldsymbol{\Theta}_\mathrm{0} $ such that the task-specific parameters $ \boldsymbol{\Theta}_u $ can become appropriately adaptive for each $u \in \{1,2,\cdots,U\}$. As in Fig. \ref{fig6}, the meta-training phase has two updates: inner-loop update and outer-loop update. For the first iteration, the model parameters $ \boldsymbol{\Theta}_u $ are set to the randomly initialized parameters $ \boldsymbol{\Theta}_\mathrm{0} $. In the inner-loop update, the pre-training is conducted with  $ \cH_{\text{ptr}(u)} $ to minimize the loss function $ \mathrm{Loss}\left(\cH_{\text{ptr}(u)};\boldsymbol{\Theta}_u\right) $ in \eqref{inner loss function} where a stochastic gradient descent (SGD) optimizer is utilized
\begin{align}
\boldsymbol{\Theta}_u\leftarrow \boldsymbol{\Theta}_u-\alpha \nabla_{\boldsymbol{\Theta}_u} \mathrm{Loss}\left(\cH_{\text{ptr}(u)};\boldsymbol{\Theta}_u\right),
\end{align}
with the inner-loop rate $\alpha$.

In the outer-loop update, based on the task-specific parameters $ \boldsymbol{\Theta}_u$ optimized for each of $ U $ tasks, $ \boldsymbol{\Theta}_\mathrm{0} $ is updated with the ADAM optimizer to minimize the total loss function. It is defined as
\begin{align}\label{meta learning loss function}
\text{Loss}_\text{tot}(\boldsymbol{\Theta}_0) =\sum_{u=1}^{U} \text{Loss}\left(\cH_{\text{val}(u)};\boldsymbol{\Theta}_u\right),
\end{align}
with the outer-loop rate $\beta$ by evaluating the model with the validation samples in $ \cH_{\text{val}(u)} $.
%\begin{align}
%\boldsymbol{\Theta}_0\leftarrow \boldsymbol{\Theta}_0-\beta \nabla_{\boldsymbol{\Theta}_0} \mathrm{Loss}\left(\boldsymbol{\Theta}_0\right).
%\end{align}
The updated parameters $ \boldsymbol{\Theta}_\mathrm{0} $ are employed as new initial values of $ \boldsymbol{\Theta}_u $ for the next iteration, and the optimization of $ \boldsymbol{\Theta}_\mathrm{0} $ is progressed through the following pre-training and validation phases. By conducting the process with a few iterations, $ \boldsymbol{\Theta}_\mathrm{0} $ is gradually optimized.

After the meta-training phase, the initial model parameters are adjusted to minimize $ \mathrm{Loss}\left(\cH_\text{adpt};\boldsymbol{\Theta}_\mathrm{0}\right) $ during the meta-adaptation phase with the SGD optimizer as
\begin{align}
\boldsymbol{\Theta}_0\leftarrow \boldsymbol{\Theta}_0-\alpha \nabla_{\boldsymbol{\Theta}_0} \mathrm{Loss}\left(\cH_{\text{adpt}};\boldsymbol{\Theta}_0\right).
\end{align}
This adaptive model is finally evaluated with the CSI samples in $ \cH_{\text{test}} $ during the meta-test phase. As in Section \ref{sec3-4}, the defined index sets among different learning phases and tasks are disjoint each other, i.e., $ {\cI_s \cap \cI_{s'} = \varnothing} $ for $ s\neq s' $ where ${ s,s'\in\bigcup_{u=1}^U \{\text{ptr}(u),\text{val}(u)\}\cup\{\text{adpt},\text{test}\}} $.

\section{Complexity Analysis and Experimental Results}\label{sec5}
 In this section, we first analyze the computational complexity of the proposed scheme and the CSI preprocessing. Then, we evaluate the performance of the people counting and localization models in the experimental results.

\subsection{Complexity Analysis}\label{sec5-1}
In this subsection, we analyze the computational complexity of the meta-learning-based model, the pre-trained model, the transfer-learning-based model, which are denoted as Meta, Pre, and TL, respectively, and the CSI preprocessing with the Big-O notation \cite{Hunger:2005}. The computational complexity of Meta is divided into three phases, which are training, adaptation, and testing phases. In the training phase, the computational complexity is given by \cite{Mizutani:2001, Taghavi:2019, Kim:2021, Kim:2023}
\begin{align}
&C_\text{Meta-train}\notag \\&=\mathcal{O}\Bigg(N_\text{epoch}N_\text{meta-train}\Bigg(\sum_{q=1}^{Q}N_{\text{ker}}C_{\text{in},q}C_{\text{out},q}+\sum_{i=1}^LD_iD_{i+1}\notag\\ &+D_{L+1}C\Bigg)\Bigg)\notag\\
&\stackrel{(a)}{=}\mathcal{O}\big(N_\text{epoch}N_\text{meta-train}\big(N_\text{ker}\big(C_0N_f+(Q-1)N_f^2\big)+C_QN_d\notag\\
&+(L-1)N_d^2+N_dC\big)\big)\notag \\
&\stackrel{(b)}{=}\mathcal{O}\left(N_\text{epoch}N_\text{meta-train}\left(N_\text{ker}QN_f^2+LN_d^2+N_dC\right)\right)\notag\\
&\stackrel{(c)}{=}\mathcal{O}\left(N_\text{epoch}N_\text{meta-train}\left(N_\text{ker}QN_f^2+LN_d^2\right)\right),
\end{align} where $N_\text{epoch}$ is the number of epoch, $N_\text{meta-train}$ is the number of meta-training samples, $Q$ is the number of the convolutional layers, $N_\text{ker}$ is the size of kernel, $C_{\text{in},q}$ is the size of the $q$-th kernel input, $C_{\text{out},q}$ is the size of the $q$-th kernel output, $L$ is the number of FC layers, $D_i$ is the number of the $i$-th nodes, $N_f$ is the number of filters, $C_Q$ is the number of nodes in the flatten layer, and $N_d$ is the number of nodes in FC layers. Note that (a) is from $C_{\text{in},1}=C_0$ where $C_0=3$ since we use amplitude, real, and imaginary matrices as the inputs, $C_{\text{in},q}=N_f$ for $q>1$, $C_{\text{out},q}=N_f$, $D_1=C_Q$, and $D_i=N_d$, (b) comes from $N_f\gg C_0$ and $C_Q\simeq N_d$, and (c) is derived by $N_d\gg C$. Similar to the above derivation, the complexity of adaptation phase with the number of gradient steps $N_\text{gr}$ and the number of adaptation samples $N_\text{adpt}$ is given as
\begin{align}
C_\text{Meta-adaptaion}{=}\mathcal{O}\left(N_\text{gr}N_\text{adpt}\left(N_\text{ker}QN_f^2+LN_d^2\right)\right),
\end{align} and the complexity of testing phase is given by 
\begin{align}
C_\text{Meta-test}{=}\mathcal{O}(N_\text{test}(N_\text{ker}QN_f^2+LN_d^2)).    
\end{align} The total complexity of Meta then becomes
\begin{align}
C_\text{Meta}{=}&\mathcal{O}\big((N_\text{epoch}N_\text{meta-train}+N_\text{gr}N_\text{adpt}+N_\text{test})\notag \\
&\cdot\big(N_\text{ker}QN_f^2+LN_d^2\big)\big).
\end{align}

We also analyze the computational complexity of Pre and TL, which are our benchmarks in Section \ref{sec5-2}. In the training phase, the complexity of Pre can be obtained by
\begin{align}
C_\text{Pre-train}{=}\mathcal{O}\left(N_\text{epoch}N_\text{pre-train}\left(N_\text{ker}QN_f^2+LN_d^2\right)\right),
\end{align} where $N_\text{pre-train}$ is the number of training samples of Pre. Then, the total complexity of Pre is given by
\begin{align}
C_\text{Pre}{=}&\mathcal{O}\big((N_\text{epoch}N_\text{pre-train}+N_\text{gr}N_\text{adpt}+N_\text{test})\notag \\
&\cdot\big(N_\text{ker}QN_f^2+LN_d^2\big)\big).
\end{align}

In the TL method, it consists of three phases, which are training, fine-tuning, and testing phases. The complexity in training phase of TL can be expressed as
\begin{align}
C_\text{TL-train}{=}\mathcal{O}\left(N_\text{epoch}N_\text{tl-train}\left(N_\text{ker}QN_f^2+LN_d^2\right)\right),
\end{align} where $N_\text{tl-train}$ is the number of training samples of TL. In the fine-tuning phase, we assume that only the FC layers in the CNN module are fine-tuned in the TL framework. Therefore, the complexity in fine-tuning phase is given by
\begin{align}
C_\text{TL-fine}{=}\mathcal{O}\left(N_\text{gr}N_\text{adpt}LN_d^2\right).
\end{align}
Considering the complexity in testing phase, the total complexity of TL is finally given as
\begin{align}
C_\text{TL}{=}&\mathcal{O}\big((N_\text{epoch}N_\text{tl-train}+N_\text{test})\left(N_\text{ker}QN_f^2+LN_d^2\right)\notag \\&+N_\text{gr}N_\text{adpt}LN_d^2\big).
\end{align}

It is clearly observed that the total complexities of Meta, Pre, and TL are almost the same since $N_\text{epoch}N_\text{train} \gg N_\text{gr}N_\text{adpt}>N_\text{test}$. If we assume the number of training samples for each scheme is the same, the total complexities of all schemes can be approximated as $\mathcal{O}\left(N_\text{epoch}N_\text{train}\left(N_\text{ker}QN_f^2+LN_d^2\right)\right)$.

Moreover, we analyze the complexity of the CSI preprocessing. Since the operations in (\ref{Auxiliary matrix})-(\ref{CSI amplitude offset}) scale linearly with $K$, the total complexity across $M$ antennas and $N_\text{pck}$ packets becomes $\mathcal{O}(N_\text{pck}MK)$. The complexity of CSI preprocessing is much smaller than that of NN frameworks since $N_\text{epoch}N_\text{train}N_f^2\gg N_\text{pck}MK$. The computational complexities of Meta, Pre, TL, and the CSI preprocessing are summarized in Table \ref{table1}.

\begin{table*}[t!]
	% increase table row spacing, adjust to taste
	\centering
	\renewcommand{\arraystretch}{1.25}
	\captionsetup{justification=centering}
	\captionsetup{labelsep=newline}
	\caption{Computational complexities of Meta, Pre, TL, and CSI preprocessing}
	\resizebox{\linewidth}{!}{%
		\begin{tabular}{l l l l}
			\toprule
			\text{Scheme} & Phase & \text{Complexity} & \text{Total complexity} \\
			\hline
			\multirow{3}{*}{Meta}& \multicolumn{1}{l}{Train} & \multicolumn{1}{l}{$\mathcal{O}\left(N_\text{epoch}N_\text{meta-train}\left(N_\text{ker}QN_f^2+LN_d^2\right)\right)$} & \multirow{3}{*}{$\mathcal{O}\left((N_\text{epoch}N_\text{meta-train}+N_\text{gr}N_\text{adpt}+N_\text{test})\left(N_\text{ker}QN_f^2+LN_d^2\right)\right)$} \\ %\cline{2-3}
			& \multicolumn{1}{l}{Adaptation} & \multicolumn{1}{l}{$\mathcal{O}\left(N_\text{gr}N_\text{adpt}\left(N_\text{ker}QN_f^2+LN_d^2\right)\right)$} & \\  %\cline{2-3}
			& \multicolumn{1}{l}{Test} & \multicolumn{1}{l}{$\mathcal{O}\left(N_\text{test}\left(N_\text{ker}QN_f^2+LN_d^2\right)\right)$} & \\ \hline			
			\multirow{3}{*}{Pre}& \multicolumn{1}{l}{Train} & \multicolumn{1}{l}{$\mathcal{O}\left(N_\text{epoch}N_\text{pre-train}\left(N_\text{ker}QN_f^2+LN_d^2\right)\right)$} & \multirow{3}{*}{$\mathcal{O}\left((N_\text{epoch}N_\text{pre-train}+N_\text{gr}N_\text{adpt}+N_\text{test})\left(N_\text{ker}QN_f^2+LN_d^2\right)\right)$} \\ %\cline{2-3}
			& \multicolumn{1}{l}{Adaptation} & \multicolumn{1}{l}{$\mathcal{O}\left(N_\text{gr}N_\text{adpt}\left(N_\text{ker}QN_f^2+LN_d^2\right)\right)$} & \\  %\cline{2-3}
			& \multicolumn{1}{l}{Test} & \multicolumn{1}{l}{$\mathcal{O}\left(N_\text{test}\left(N_\text{ker}QN_f^2+LN_d^2\right)\right)$} & \\ \hline
			\multirow{3}{*}{TL}& \multicolumn{1}{l}{Train} & \multicolumn{1}{l}{$\mathcal{O}\left(N_\text{epoch}N_\text{tl-train}\left(N_\text{ker}QN_f^2+LN_d^2\right)\right)$} & \multirow{3}{*}{$\mathcal{O}\left((N_\text{epoch}N_\text{tl-train}+N_\text{test})\left(N_\text{ker}QN_f^2+LN_d^2\right)+N_\text{gr}N_\text{adpt}LN_d^2\right)$} \\ %\cline{2-3}
			& \multicolumn{1}{l}{Fine-tune} & \multicolumn{1}{l}{$\mathcal{O}\left(N_\text{gr}N_\text{adpt}LN_d^2\right)$} & \\  %\cline{2-3}
			& \multicolumn{1}{l}{Test} & \multicolumn{1}{l}{$\mathcal{O}\left(N_\text{test}\left(N_\text{ker}QN_f^2+LN_d^2\right)\right)$} & \\ \hline		
			\multirow{1}{*}{CSI preprocessing} & \multicolumn{1}{l}{-} &\multicolumn{1}{l}{$\mathcal{O}\left(N_\text{pck}MK\right)$} & \multirow{1}{*}{-} \\ 		
			\bottomrule
	\end{tabular}}
	{\label{table1}}
\end{table*}

\subsection{Experimental Results}\label{sec5-2}
CSI measurement is conducted in two rooms, i.e., a small room ($3$ m $\times$ $5$ m) and a large room ($8$ m $\times$ $8$ m), and an open space (larger than $14$ m $\times$ $13$ m) depicted in Fig. \ref{fig7:figures}. We use the WiFi NIC (Atheros AR$9380$) with the carrier frequency $f_c=2.437$ GHz, sampling frequency $f_s=1$ kHz, number of subcarriers $K=52$, and number of spatial links $M=9$ with transceivers with three antennas $ (M_\mathrm{t}=M_\mathrm{r}=3) $. With the measured CSI, the people counting and localization models use the NVIDIA Quadro RTX $8000$ GPU and TensorFlow $2.1$ in the experimental results. For the CNN model, we use $Q=5$ convolutional layers consisting of $N_f=64$ filters with a kernel size of $5\times5$. The input dimension for CNN is $K\times M\times 3=52 \times 9 \times 3$. For the FC layers in CNN module, we use $L_\text{peo}=2$ layers with $N_d=256$ nodes for the people counting model and $L_\text{loc}=4$ layers with $N_d=256$ nodes for the localization model. In the MAML algorithm, we set the number of epochs $N_\text{epoch}=20$, the batch size $V=64$, the inner-loop rate $\alpha=10^{-1}$, and the outer-loop rate $\beta=10^{-4}$. All system parameters are summarized in Table \ref{table2}.

\begin{figure}[t]
	\centering
	\begin{subfigure}[c]{0.5\textwidth}
		\centering
	  $\vcenter{\hbox{\includegraphics[width=0.4\textwidth]{Fig/Fig7a1.eps}}}$%
		\hspace{0.5em}
		$\vcenter{\hbox{\includegraphics[width=0.5\textwidth]{Fig/Fig7a2.eps}}}$
		\caption{Small room.}
	\vspace{0.5em}
		\label{fig7:first}
	\end{subfigure}
	%\vskip\baselineskip
	\begin{subfigure}[c]{0.5\textwidth}
		\centering
		$\vcenter{\hbox{\includegraphics[width=0.4\textwidth]{Fig/Fig7b1.eps}}}$%
		\hspace{0.5em}
		$\vcenter{\hbox{\includegraphics[width=0.5\textwidth]{Fig/Fig7b2.eps}}}$
		\caption{Large room.}
	\vspace{0.5em}
		\label{fig7:second}
	\end{subfigure}
    %\vskip\baselineskip
	\begin{subfigure}[c]{0.5\textwidth}
		\centering
		$\vcenter{\hbox{\includegraphics[width=0.4\textwidth]{Fig/Fig7c1.eps}}}$%
		\hspace{0.5em}
		$\vcenter{\hbox{\includegraphics[width=0.5\textwidth]{Fig/Fig7c2.eps}}}$
		\caption{Open space.}
		\label{fig7:third}
	\end{subfigure}
	\caption{Experimental environments for CSI measurement.}
	\label{fig7:figures}
\end{figure}

 In this paper, we use accuracy as a performance metric for classification, defined as
\begin{align}
\text{Accuracy} = \frac{1}{N_\text{test} }\sum_{n\in \cI_\text{test}}\delta_{\hat{c}_n,c_n},
\end{align}
where $ \delta_{\hat{c}_n,c_n} $ is the Kronecker delta function to count the number of correct predictions as $ \hat{c}_n=c_n $ for the test samples in $ \cH_\text{test} $.
Also, we use root mean squared error (RMSE) as a performance metric for regression
\begin{align}
%\text{RMSE}=\red{\sqrt{\frac{1}{N_\text{test}}\sum_{n\in \cI_\text{test}}\cE_{n}^2}},
\text{RMSE}=\sqrt{\frac{1}{N_\text{test}N_L}\sum_{n\in \cI_\text{test}}\sum_{l=1}^{N_L}\left\|\bd_{n,l}-\hat{\bd}_{n,l}\right\|^2}.
\end{align}
In the experimental results, we compare following algorithms:
\begin{itemize}
 	\item \textbf{Pre:} pre-train the model in the training phase then re-train in the adaptation phase without CSI preprocessing.
	\item \textbf{TL:} transfer-learning-based model for people counting \cite{Khan:2023} and localization \cite{Noelia:2021} without CSI preprocessing. The model is pre-trained in the training phase, then only the FC layers of the CNN module are fine-tuned in the adaptation phase.
 	\item \textbf{Meta:} proposed meta-learning-based model without CSI preprocessing.
\item \textbf{Pre-CSI:} pre-trained model with CSI preprocessing.
\item \textbf{TL-CSI:} transfer-learning-based model with CSI preprocessing.
\item \textbf{Meta-CSI:} proposed meta-learning-based model with CSI preprocessing.
\end{itemize}




\begin{table}[t]
	\renewcommand{\arraystretch}{1.2}
	\captionsetup{justification=centering}
	\captionsetup{labelsep=newline}
	\caption{System parameters}
	\centering
	\label{table2}
	\begin{tabular}{l  l}
		\toprule
		Parameter & Value \\
		\midrule
		WiFi NIC &  Atheros AR$9380$\\
		Carrier frequency & $ 2.437 $ GHz\\
		Sampling frequency  & {$ 1 $ kHz}\\
		Number of subcarriers & $ 52 $ \\
		Number of spatial links  & $ 9 $\\
		Number of epochs & $ 20 $ \\
		Batch size & 64 \\
		Inner-loop rate & $10^{-1}$ \\
		Outer-loop rate & $10^{-4}$ \\
		\bottomrule
	\end{tabular}
\end{table}

\begin{figure}[t]
	\centering
	\begin{subfigure}{0.32\textwidth}
		\includegraphics[width=\textwidth]{Fig/Fig8a.eps}
		\caption{Small room.}
		\label{fig8:first}
	\end{subfigure}
	\hspace{1em}
	%\hfill
	\begin{subfigure}{0.32\textwidth}
		\includegraphics[width=\textwidth]{Fig/Fig8b.eps}
		\caption{Large room.}
		\label{fig8:second}
	\end{subfigure}
	\hspace{1em}
    %\hfill
	\begin{subfigure}{0.32\textwidth}
		\includegraphics[width=\textwidth]{Fig/Fig8c.eps}
		\caption{Open space.}
		\label{fig8:third}
	\end{subfigure}
	\caption{People counting accuracy vs. number of adaptation samples.}
	\label{fig8:figures}
\end{figure}

\begin{figure}[t]
	\centering
	\begin{subfigure}{0.3\textwidth}
		\includegraphics[width=\textwidth]{Fig/Fig9a.eps}
		\caption{Small room.}
	\vspace{1em}
		\label{fig9:first}
	\end{subfigure}
    \hspace{1em}
	\begin{subfigure}{0.3\textwidth}
		\includegraphics[width=\textwidth]{Fig/Fig9b.eps}
		\caption{Large room.}
	\vspace{1em}
		\label{fig9:second}
	\end{subfigure}
    \hspace{1em}
	\begin{subfigure}{0.3\textwidth}
		\includegraphics[width=\textwidth]{Fig/Fig9c.eps}
		\caption{Open space.}
		\label{fig9:third}
	\end{subfigure}
	\caption{Confusion matrix of Meta-CSI for people counting when $N_\text{adpt}=50$.}
	\label{fig9:figures}
\end{figure}


\begin{figure}[t]
	\centering
	\begin{subfigure}[]{0.33\textwidth}
		\includegraphics[width=\textwidth]{Fig/Fig10a.eps}
		\caption{Small room.}
		\label{fig10:first}
	\end{subfigure}
	%\hspace{1em}
	%\hfill
	\begin{subfigure}[]{0.33\textwidth}
		\includegraphics[width=\textwidth]{Fig/Fig10b.eps}
		\caption{Large room.}
		\label{fig10:second}
	\end{subfigure}
    %\hfill
    %\hspace{1em}
    \begin{subfigure}[]{0.33\textwidth}
		\includegraphics[width=\textwidth]{Fig/Fig10c.eps}
		\caption{Open space.}
		\label{fig10:third}
	\end{subfigure}
	\caption{Localization accuracy vs. number of adaptation samples.}
	\label{fig10:figures}
\end{figure}

\begin{figure}[t]
	\centering
	\begin{subfigure}{0.30\textwidth}
		\includegraphics[width=\textwidth]{Fig/Fig11a.eps}
		\caption{Small room.}
	\vspace{1em}
		\label{fig11:first}
	\end{subfigure}
	\hspace{1em}
	%\hfill
	\begin{subfigure}{0.30\textwidth}
		\includegraphics[width=\textwidth]{Fig/Fig11b.eps}
		\caption{Large room.}
	\vspace{1em}
		\label{fig11:second}
	\end{subfigure}
    \hspace{1em}
    \begin{subfigure}{0.30\textwidth}
		\includegraphics[width=\textwidth]{Fig/Fig11c.eps}
		\caption{Open space.}
		\label{fig11:third}
	\end{subfigure}
	\caption{Confusion matrix of Meta-CSI for localization when $N_\text{adpt}=50$.}
	\label{fig11:figures}
\end{figure}

To evaluate the people counting model, we use 1000 training samples and 500 test samples for each class. We exploit 10000 training samples and 5000 test samples for the localization model. Also, we use the same number of adaptation samples $N_\textrm{adpt}$ for the pre-trained model, transfer-learning-based model, and meta-learning-based model for fair comparison.

\subsubsection{People Counting Model}
We consider the people counting problem with $C=6,11,$ and $16$ for the small room, large room, and open space, respectively. We set total $C-1$ positions for each environment, and locate an arbitrary number of people among the positions uniformly. In Fig. \ref{fig8:figures}, we plot the accuracy of pre-trained, transfer-learning-based, and meta-learning-based people counting models according to the number of adaptation samples. The models without CSI preprocessing show degraded performance with a small number of adaptation samples. Especially, the accuracy for the open space does not increase with $N_\text{adpt}$ at all. Longer and fewer propagation paths in such a wider environment make offsets more dominant. However, after CSI preprocessing, Meta-CSI clearly outperforms the other models for all environments. For the small and large rooms, we obtain higher than 95$\%$ accuracy of the people counting model with 20 adaptation samples in Meta-CSI. Even in the open space setting, Meta-CSI surpasses 80$\%$ accuracy with 50 adaptation samples. Thus, Meta-CSI can give accurate performance with a small number of adaptation samples.

\begin{figure}[tbp]
\centering
\begin{subfigure}{0.35\textwidth}
	\includegraphics[width=\textwidth]{Fig/Fig12a.eps}
	\caption{Same location.}
	\label{fig12:first}
\end{subfigure}
\hspace{1em}
%\hfill
\begin{subfigure}{0.35\textwidth}
	\includegraphics[width=\textwidth]{Fig/Fig12b.eps}
	\caption{Different location.}
	\label{fig12:second}
\end{subfigure}
\caption{RMSE in large room vs. number of adaptation samples.}
\label{fig12:figures}
\end{figure}

\begin{figure}[tbp]
\centering
\begin{subfigure}{0.35\textwidth}
	\includegraphics[width=\textwidth]{Fig/Fig13a.eps}
	\caption{Same location.}
	\label{fig13:first}
\end{subfigure}
\hspace{1em}
%\hfill
\begin{subfigure}{0.35\textwidth}
	\includegraphics[width=\textwidth]{Fig/Fig13b.eps}
	\caption{Different location.}
	\label{fig13:second}
\end{subfigure}
\caption{RMSE in open space vs. number of adaptation samples.}
\label{fig13:figures}
\end{figure}

We also plot the confusion matrix of Meta-CSI for people counting when $N_\text{adpt}=50$ in Fig. \ref{fig9:figures}. The confusion matrix is the classification table; each row of matrix represents the true values, while each column represents the predicted values. Each entry of the matrix expresses the prediction ratio of each class. For both the small room and large room, the accuracy of each class is nearly 100$\%$. Moreover, the accuracy is close to 90$\%$ in the open space.

\subsubsection{Localization Model (Classification)}\label{localization model classification} 
We also consider the localization problem for single person as classification. The small room is equally divided into $C=4\times2=8$ sectors, and the large room and open space are divided into $C=4\times4=16$ sectors. We assume that only one person can stand at the center of each sector, and the size of sectors is set to $1.2$ m $\times$ $1.2$ m. Similar to the people counting model, we exploit the meta-learning algorithm to classify each sector. In Fig. \ref{fig10:figures}, we show the accuracy of localization models according to the number of adaptation samples. Although Meta and the baselines without CSI preprocessing show inconsistent accuracy for different environments, Meta-CSI steadily has the best performance. Fig. \ref{fig11:figures} depicts the confusion matrix of Meta-CSI for localization when $N_\text{adpt}=50$. The confusion matrices show that the accuracy of each class is almost 100$\%$ in both the small room and large room, while it is about 90$\%$ in the open space. For incorrect prediction, however, the minimum localization error cannot be smaller than a sector size. For example, the error for large room is larger than $1.2$ m since we set each sector size as $1.2$ m $\times$ $1.2$ m. Therefore, we employ the regression model for more accurate localization in Section~\ref{localization model regression}.

%However, for incorrect prediction the minimum localization error cannot be smaller than a sector size. For example, the error for large room is larger than $2$ m since we set each sector size as $2$ m $\times$ $2$ m. Therefore, we employ the regression model for more accurate localization in Section \ref{localization model regression}.

\subsubsection{Localization Model (Regression)} \label{localization model regression}
Different from Section \ref{localization model classification}, we consider a two-person localization problem in the large room and open space, and the output of learning models is the location of two people, i.e., $N_L=2$. We set two different scenarios, i.e., same location and different location. The same location implies the locations of people of the training dataset and the test dataset are the same. The different location implies a more practical case that the locations of people of the training dataset and the test dataset~are~different. Specifically, we collect the training samples where the two people are located among the designated positions with uniform spacing of $1.2$ m, and the test datasets for the different location do not include the positioning cases in the training dataset.

Fig. \ref{fig12:figures} shows the RMSEs of localization models with CSI preprocessing according to the number of adaptation samples in the large room. Meta-CSI outperforms Pre-CSI and TL-CSI, and the RMSE value of Meta-CSI is less than 0.2 m with $N_\text{adpt}\geq 20$. Also, even in the different location scenario, the proposed Meta-CSI has moderate performance with a small number of adaptation~samples.

In contrast to the large room results, Fig. \ref{fig13:figures} indicates that the RMSE values in the open space scenario are generally higher, likely due to increased signal fluctuations and environmental interference. Despite these more challenging conditions, all three methods show a gradual decrease in RMSE as the number of adaptation samples increases. In particular, Meta-CSI retains the lowest RMSE curve throughout the adaptation process, which demonstrates its robustness in more demanding environments. 

\begin{figure}[tbp]
\centering
\begin{subfigure}{0.35\textwidth}
	\includegraphics[width=\textwidth]{Fig/Fig14a.eps}
	\caption{Same location.}
	\label{fig14:first}
\end{subfigure}
\hspace{1em}
%\hfill
\begin{subfigure}{0.35\textwidth}
	\includegraphics[width=\textwidth]{Fig/Fig14b.eps}
	\caption{Different location.}
	\label{fig14:second}
\end{subfigure}
\caption{CDF of localization error in large room with $N_\textrm{adpt}=50$.}
\label{fig14:figures}
\end{figure}

Fig. \ref{fig14:figures} reveals the cumulative distribution functions (CDFs) of localization error with the number of adaptation samples $N_\textrm{adpt}=50$. 
Note that the localization error is defined as 
\begin{align}
\text{Localization error}=\frac{1}{N_L}\sum_{l=1}^{N_L}\left\|\bd_{n,l}-\hat{\bd}_{n,l}\right\|.
\end{align}The figure clearly shows that Meta-CSI has superior performance compared to the other models. In the same location, $90 \%$ CDF levels of Meta-CSI, TL-CSI, and Pre-CSI  are $0.1452$ m, $0.1684$ m, and $0.2871$ m, respectively. In the different location, $90\%$ CDF levels of Meta-CSI, TL-CSI, and Pre-CSI are $0.1469$ m, $0.2077$ m, and $0.3443$ m. Although the localization errors in the different location are slightly higher than those in the same location, Meta-CSI performs well in the different location. This result shows that the proposed Meta-CSI can be used in practical scenarios.

\begin{figure}[tbp]
\centering
\begin{subfigure}{0.35\textwidth}
	\includegraphics[width=\textwidth]{Fig/Fig15a.eps}
	\caption{Same location.}
	\label{fig15:first}
\end{subfigure}
\hspace{1em}
%\hfill
\begin{subfigure}{0.35\textwidth}
	\includegraphics[width=\textwidth]{Fig/Fig15b.eps}
	\caption{Different location.}
	\label{fig15:second}
\end{subfigure}
\caption{CDF of localization error in open space with $N_\textrm{adpt}=50$.}
\label{fig15:figures}
\end{figure}

As shown in Fig. \ref{fig15:figures}, the open space environment yields larger overall localization errors compared to the large room, even with 50 adaptation samples. At the 90$\%$ CDF level, the localization errors for Meta-CSI, TL-CSI, and Pre-CSI are 0.7701 m, 0.9267 m, and 0.9737 m, respectively, in the same location scenario, and 0.8442 m, 0.8993 m, and 1.052 m, respectively, in the different location scenario. Despite the challenging nature of the open space environment, Meta-CSI still gives the lowest overall error distribution. These findings highlight the importance of robust adaptation techniques in open spaces, where multipath effects can substantially impact localization accuracy.

%\begin{figure}[t]
%	\centering
%	\includegraphics[width=0.95\columnwidth]{Fig9_1.eps}
%	\caption{CDF of localization error in same location with $N_\textrm{ad}=100$ } \label{Fig9}
%\end{figure}

%\begin{figure}[t]
%	\centering
%	\includegraphics[width=0.95\columnwidth]{Fig9_2.eps}
%	\caption{CDF of localization error in different location with $N_\textrm{ad}=100$ } \label{Fig10}
%\end{figure}

\section{Conclusion and Discussion}\label{sec6}
In this paper, we configured the people counting and localization systems employing CSI from the commodity WiFi NICs. The preprocessing of CSI was proposed to remove the amplitude and phase offsets from erroneous CSI. The proposed meta-learning-based model optimizes its parameters into general ones for human sensing, which makes the model adaptive to different measurement environments. The experimental results revealed that the proposed meta-learning-based model with CSI preprocessing has notable gain over the benchmark models with a small amount of adaptation samples in both people counting and localization problems.
%We believe that the proposed meta-learning-based model can be exploited for accurate human sensing using the commodity WiFi NICs.

While the proposed scheme demonstrates its ability for adaptive people counting and localization in wide spaces without relying on extensive datasets, the limitation and challenges remain. Labeling training data to distinguish various cases involving people and locations is a manual and exhaustive task. Additionally, nonlinear offset components in the measured CSI, which vary across time-frequency resources, still exist. To enhance scalability, the proposed meta-learning-based model requires improvement with a larger scale of data collection in different environments and with high computing resources for fast training and adaptation processes. As a possible future work, we can adopt unsupervised learning for the proposed model without data labeling, and CSI preprocessing for offset removal can also be improved by treating its nonlinear components.

% use section* for acknowledgment
%\section*{Acknowledgment}


%The authors would like to thank...


% Can use something like this to put references on a page
% by themselves when using endfloat and the captionsoff option.
\ifCLASSOPTIONcaptionsoff
  \newpage
\fi



% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://mirror.ctan.org/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%\bibliography{bibtex/bib/IEEEexample}
\bibliography{refs_all}

% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)
%\begin{thebibliography}{1}
%\bibitem{IEEEhowto:kopka}
%  0.5em minus 0.4em\relax Harlow, England: Addison-Wesley, 1999.
%\end{thebibliography}








% biography section
% 
% If you have an EPS/PDF photo (graphicx package needed) extra braces are
% needed around the contents of the optional argument to biography to prevent
% the LaTeX parser from getting confused when it sees the complicated
% \includegraphics command within an optional argument. (You could create
% your own custom macro containing the \includegraphics command to make things
% simpler here.)
%\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{mshell}}]{Michael Shell}
% or if you just want to reserve a space for a photo:

%\begin{IEEEbiography}{Michael Shell}
%Biography text here.
%\end{IEEEbiography}

% if you will not have a photo at all:
%\begin{IEEEbiographynophoto}{John Doe}
%Biography text here.
%\end{IEEEbiographynophoto}

% insert where needed to balance the two columns on the last page with
% biographies
%\newpage

%\begin{IEEEbiographynophoto}{Jane Doe}
%Biography text here.
%\end{IEEEbiographynophoto}

% You can push biographies down or up by placing
% a \vfill before or after them. The appropriate
% use of \vfill depends on what kind of text is
% on the last page and whether or not the columns
% are being equalized.

%\vfill

% Can be used to pull up biographies so that the bottom of the last one
% is flush with the other column.
%\enlargethispage{-5in}



% that's all folks
\end{document}


