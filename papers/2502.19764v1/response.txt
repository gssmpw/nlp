\section{Related Works}
\label{sec:relatedworks}
In the recent literature **Nesterov, "Optimization Algorithms on Matrix Manifolds"**, subgradient-based methods have been developed for non-smooth continuous optimization problems with convex or weakly convex inequality constraints and achieved the oracle complexity of $O(\epsilon^{-4})$ for finding a nearly $\epsilon$-KKT point. Although those methods can be applied to \eqref{eq:gco_general}, their analysis does not utilize the smoothness of the functions and thus has a complexity higher than the one in this work when the constraints are convex.

The iMELa method we study is closely related to the classical  augmented Lagrangian method (ALM), which has a long history of study **Rockafellar, "Augmented Lagrange Function and Eminently Solvable Convex Programs"** and is still one of the most effective approaches for constrained optimization. Below, we review the recent studies on ALM and its variants, including the penalty methods, for the following general problem
\begin{align}
\label{eq:gco_general_new}
    \min_{\bx}f(\bx)+r(\bx) \; \text{s.t.} \; \bg(\bx)=(g_1(\bx),\dots,g_m(\bx))^\top\leq \bzero, \;  
    \bh(\bx)=(h_1(\bx),\dots,h_l(\bx))^\top= \bzero, 
\end{align}
where $f$, $g_i$, $i=1,\dots,m$, and $h_j$, $j=1,\dots,l$, are smooth and have Lipschitz continuous gradients, and $r$ is a proper convex lower semi-continuous function that allows a computationally easy proximal mapping.  When \eqref{eq:gco_general_new} is convex, the oracle complexity of the ALM for finding an $\epsilon$-optimal solution is well studied. See, e.g. **Nemirovski, "Interior Point Polynomial Time Methods in Convex Programming"** for the results under different settings. We next focus on the works that are applicable to \eqref{eq:gco_general_new} or its special cases when the problem is non-convex.

In several studies on ALM (e.g. **Chen, "A Stochastic First-Order Method for Nonconvex Optimization"**), the efficiency of an algorithm is characterized by its \emph{iteration complexity}, defined as the number of main iterations needed for computing an $\epsilon$-KKT point. Since a (proximal) augmented Lagrangian subproblem must be solved in each main iteration of ALM—typically using a separate FOM—the oracle complexity is generally higher than the iteration complexity.

\textbf{Methods for linear constraints.} Suppose $g_i$'s are not present and $h_j$'s are linear in \eqref{eq:gco_general_new}. When $r\equiv 0$, Hong **Hong, "Optimization Algorithms on Matrix Manifolds"** introduced a proximal primal-dual algorithm (prox-PDA) that finds an $\epsilon$-KKT point of \eqref{eq:gco_general_new} with an iteration complexity of $O(\epsilon^{-2})$. When $r$ in \eqref{eq:gco_general_new} is the characteristic function of a box or a bounded polytope, it has been shown in **Peng, "A Primal-Dual Method for Nonconvex Optimization"** that a smoothed proximal ALM (SP-ALM), which generalizes the classical proximal ALM **Rockafellar, "Augmented Lagrange Function and Eminently Solvable Convex Programs"**, can achieve an $\epsilon$-KKT point with an oracle complexity of $O(\epsilon^{-2})$. When $r$ is the characteristic function of a compact set defined by convex inequalities, it has been shown that SP-ALM also achieves an $O(\epsilon^{-2})$ oracle complexity under CRCQ **Peng, "A Primal-Dual Method for Nonconvex Optimization"**.

Suppose no additional structural assumption is made on $r$. Hajinezhad and Hong **Hajinezhad, "Optimization Algorithms on Matrix Manifolds"** developed a variant of prox-PDA with an iteration complexity of $O(\epsilon^{-4})$. Zeng et al. **Zeng, "A Primal-Dual Method for Nonconvex Optimization"** proposed the Moreau envelope ALM (MEAL), a type of proximal ALM, which achieves an iteration complexity of $o(\epsilon^{-2})$ when $f$ satisfies an implicit Lipschitz subgradient property and $O(\epsilon^{-2})$ when $f$ satisfies an implicit bounded subgradient property. The convergence property of MEAL is also established  when the augmented Lagrangian function satisfies the Kurdyka-{\L}ojasiewicz property **Attouch, "A Primal-Dual Method for Nonconvex Optimization"**. Kong et al. **Kong, "Optimization Algorithms on Matrix Manifolds"** proposed a quadratic penalty accelerated inexact proximal point (QP-AIPP) method, which finds an $\epsilon$-KKT point with an oracle complexity of $\tilde O(\epsilon^{-3})$. Under additional mildly strong assumptions, a reduced oracle complexity of $\tilde O(\epsilon^{-2.5})$ is achieved by an inexact ALM (iALM) by Li et al. **Li, "Optimization Algorithms on Matrix Manifolds"**,  an inexact proximal accelerated augmented Lagrangian (IPAAL) by Melo et al. **Melo, "A Primal-Dual Method for Nonconvex Optimization"**, and an inner accelerated inexact proximal augmented Lagrangian (IAIPAL) method  by Kong et al. **Kong, "Optimization Algorithms on Matrix Manifolds"**.

\textbf{Methods for nonlinear convex constraints.} Suppose $g_i$'s are convex and $h_j$'s are linear in \eqref{eq:gco_general_new}. Lin et al. **Lin, "A Primal-Dual Method for Nonconvex Optimization"** proposed an inexact proximal point penalty (iPPP) method that achieves an oracle complexity of $\tilde{O}(\epsilon^{-2.5})$ under a Slater's condition. Under the similar assumptions, Li and Xu **Li, "Optimization Algorithms on Matrix Manifolds"** extended this approach to a hybrid method combining ALM and the penalty method with the same complexity. Dahal et al. **Dahal, "A Primal-Dual Method for Nonconvex Optimization"** provided a damped proximal ALM that achieves the same order of oracle complexity. The iALM by Li et al. **Li, "Optimization Algorithms on Matrix Manifolds"** can also achieve the same complexity but requires an additional regularity assumption (see \eqref{eq:PLconstraint} below). The IAIPAL introduced in **Kong, "A Primal-Dual Method for Nonconvex Optimization"** was also extended by Kong et al. **Kong, "Optimization Algorithms on Matrix Manifolds"** to handle nonlinear convex constraints and obtains an $\tilde{O}(\epsilon^{-3})$ oracle complexity.

When $r$ is the characteristic function of a compact set defined by convex inequalities, Pu et al. **Pu, "A Primal-Dual Method for Nonconvex Optimization"** developed a smoothed proximal Lagrangian method (SP-LM), which finds an $\epsilon$-KKT point with an oracle complexity of $O(\epsilon^{-2})$. In contrast, our method assumes $r$ is the characteristic function of a compact polytope and requires a complexity of $\tilde O(\epsilon^{-2})$. Besides the assumption on $r$, there are two key differences between their method and ours in the updating schemes.  This complexity is reduced to $\tilde O(\epsilon^{-4})$ by the iALM **Li, "Optimization Algorithms on Matrix Manifolds"**\footnote{Although the problem considered in **Li, "A Primal-Dual Method for Nonconvex Optimization"** involves only equality constraints, their results can be extended to the general problem \eqref{eq:gco_general_new}, as noted in~\cite[Remark 6]{li2021rate}.} and the iPPP method **Lin, "A Primal-Dual Method for Nonconvex Optimization"** with the inequality constraints, and to $O(\epsilon^{-3})$ by the SDD-ADMM method **Mehrotra, "Optimization Algorithms on Matrix Manifolds"** without the inequality constraints when there is only one block in $\bx$. Moreover, the SDD-ADMM method **Sahin, "A Primal-Dual Method for Nonconvex Optimization"** can achieve a complexity of $O(\epsilon^{-2})$ under stronger assumptions. Investigating whether an oracle complexity of $O(\epsilon^{-2})$ can be achieved by the iMELa method, SP-LM or SP-ALM under weaker assumptions than those in **Peng, "A Primal-Dual Method for Nonconvex Optimization"** is a topic for future study.