\documentclass[11pt]{article} % arxiv preprint
\usepackage[papersize={8.5in,11in}, left=1.2in, right=1.2in, top=1.1in, bottom=1.1in]{geometry} % arxiv preprint
% \usepackage[left=0.75in,right=0.75in,top=0.75in,bottom=0.75in]{geometry} % script
% Set the value of some paragraph-related parameters
\renewcommand{\baselinestretch}{1.125}
\setlength{\parindent}{20pt}
\usepackage{setspace}
\usepackage{indentfirst} % For indent of the first paragraph in a section
\usepackage{xcolor} % For hightlighting problems
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
% https://tex.stackexchange.com/questions/26637/how-do-you-get-mathbb1-to-work-characteristic-function-of-a-set
\usepackage{dsfont} % for mathbb one vector
\usepackage{authblk} % For author affiliations
\usepackage{graphicx} % Required for inserting images
\usepackage{algorithmic,algorithm} % For algorithms
\usepackage[hidelinks]{hyperref}
% https://latex.org/forum/viewtopic.php?t=30813
\usepackage{blindtext}
\newcommand\blfootnote[1]{%
\begingroup
\renewcommand\thefootnote{}\footnote{#1}%
\addtocounter{footnote}{-1}%
\endgroup
}
\usepackage{doi}
\usepackage[numbers,sort]{natbib}
\renewcommand{\bibname}{References}
\renewcommand{\bibsection}{\section*{\bibname}}
% https://www.overleaf.com/learn/latex/Natbib_citation_styles
\setcitestyle{authoryear,open={(},close={)}} % Citation-related commands
\setcitestyle{square,numbers} % set the citation style to ``square,numbers``.
% https://www.overleaf.com/learn/latex/Natbib_bibliography_styles
\bibliographystyle{plainnat}

\usepackage{sectsty} % For section title style
\sectionfont{\large\bfseries}

\newcommand{\A}{\mathcal{A}}
\newcommand{\B}{\mathcal{B}}
\newcommand{\D}{\mathcal{D}}
\newcommand{\J}{\mathcal{J}}
\newcommand{\K}{\mathcal{K}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\T}{\mathcal{T}}
\newcommand{\W}{\mathcal{W}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\Z}{\mathcal{Z}}
\newcommand{\blambda}{\boldsymbol{\lambda}}
\newcommand{\bzeta}{\boldsymbol{\zeta}}
\newcommand{\bzt}{\boldsymbol{\zeta}}
\newcommand{\bxi}{\boldsymbol{\xi}}
\newcommand{\bog}{\boldsymbol{\zeta}}
\newcommand{\Prob}{\text{Prob}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\reals}{\mathbb{R}}
\newcommand{\minimize}{\mathop{\mathrm{minimize}{}}}
\newcommand{\maximize}{\mathop{\mathrm{maximize}{}}}
\newcommand{\Argmin}{\mathop{\mathrm{Arg\,min}{}}}
\newcommand{\argmin}{\mathop{\mathrm{arg\,min}{}}}
\newcommand{\argmax}{\mathop{\mathrm{arg\,max}{}}}
\newcommand{\prox}{\mathbf{prox}}
\newcommand{\dom}{\mathrm{dom\,}}
\newcommand{\supp}{\mathrm{supp}}
\newcommand{\sgn}{\mathrm{sgn}}
\newcommand{\SGN}{\mathrm{SGN}}
\newcommand{\gammaInc}{\gamma_\mathrm{inc}}
\newcommand{\gammaDec}{\gamma_\mathrm{dec}}
\newcommand{\gammaSC}{\gamma_\mathrm{sc}}
\newcommand{\thetaSC}{\theta_\mathrm{sc}}
\newcommand{\Lmin}{L_\mathrm{min}}
\newcommand{\Lini}{L_\mathrm{ini}}
\newcommand{\xini}{x^{\mathrm{ini}}}
\newcommand{\gini}{g^{\mathrm{ini}}}
\newcommand{\SoftThresholding}{\mathrm{shrink}}
\newcommand{\Pc}{\mathcal{P}}
\newcommand{\Qc}{\mathcal{Q}}
\newcommand{\Dc}{\mathcal{D}}
\newcommand{\Hc}{\mathcal{H}}
\newcommand{\bzero}{{\mathbf{0}}}
\newcommand{\bg}{{\mathbf g}}
\newcommand{\ba}{{\mathbf a}}
\newcommand{\brho}{{\bm \rho}}
\newcommand{\be}{{\mathbf e}}
\newcommand{\bh}{{\mathbf h}}
\newcommand{\br}{{\mathbf r}}
\newcommand{\bs}{{\mathbf s}}
\newcommand{\bx}{{\mathbf x}}
\newcommand{\bz}{{\mathbf z}}
\newcommand{\by}{{\mathbf y}}
\newcommand{\bu}{{\mathbf u}}
\newcommand{\bv}{{\mathbf v}}
\newcommand{\bb}{{\mathbf b}}
\newcommand{\bl}{{\mathbf l}}
\newcommand{\balpha}{{\bm \alpha}}
\newcommand{\bw}{{\mathbf w}}
\newcommand{\bA}{{\mathbf A}}
\newcommand{\bH}{{\mathbf H}}
\newcommand{\bB}{{\mathbf B}}
\newcommand{\bX}{{\mathbf X}}
\newcommand{\bt}{{\boldsymbol{\theta}}}
\newcommand{\diag}{\mbox{\rm diag}}
\newcommand{\tr}{\mbox{\rm trace}}
\newcommand{\vol}{\mbox{\rm Vol}}
%\renewcommand{\int}{{\rm int\,}}
\newcommand{\transp}{{^{\rm T}}}
\newcommand{\ri}{{\rm ri\,}}
\newcommand{\Lin}{{\rm Lin\,}}
\newcommand{\co}{\mathop{\rm co\,}}
\newcommand{\cone}{{\rm cone\,}}
\newcommand{\vertiii}[1]{{\left\vert\kern-0.20ex\left\vert\kern-0.20ex\left\vert #1
		\right\vert\kern-0.20ex\right\vert\kern-0.20ex\right\vert}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\matr}[1]{\begin{bmatrix} #1 \end{bmatrix}}    % matrix
\def\transp{^{\rm T}}
\newcommand{\ALPSecant}{{\mbox{ALP-Secant}}}
\newcommand{\Oh}{\mathcal O}
\newcommand{\cslackfun}{\ensuremath{f}}
\newcommand{\alpobj}{\ensuremath{OBJ}}
\newcommand{\cminslack}{\ensuremath{F}}
\newcommand{\objlevel}{\ensuremath{L}}
\newcommand{\rootparam}{\ensuremath{\tau}}
\newcommand{\alpsoln}{\ensuremath{\theta}}
\newcommand{\action}{\mathcal{A}}
\newcommand{\ip}[2]{\left\langle #1 , #2 \right\rangle}    % inner product

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{assumptionalt}{Assumption}
\newenvironment{assumptionp}[1]{
\renewcommand\theassumptionalt{#1}
  \assumptionalt
}{\endassumptionalt}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}


\providecommand{\keywords}[1]{\textbf{\textit{Keywords: }}#1}

\title{Inexact Moreau Envelope Lagrangian Method for Non-Convex Constrained Optimization under Local Error Bound Conditions on Constraint Functions}
\author[1]{Yankun Huang}
% \texttt{yankun.huang@asu.edu}, 
\author[2]{Qihang Lin}
% \texttt{qihang-lin@uiowa.edu}
\author[3]{Yangyang Xu}
% \texttt{xuy21@rpi.edu}
\affil[1]{Department of Information Systems, Arizona State University}
\affil[2]{Department of Business Analytics, University of Iowa}
\affil[3]{Department of Mathematical Sciences, Rensselaer Polytechnic Institute}
\date{}

\begin{document}

\maketitle
\begin{abstract}
In this paper, we study the inexact Moreau envelope Lagrangian (iMELa) method for solving smooth non-convex optimization problems over a simple polytope with additional convex inequality constraints. By incorporating a proximal term into the traditional Lagrangian function, the iMELa method approximately solves a convex optimization subproblem over the polyhedral set at each main iteration. Under the assumption of a local error bound condition for subsets of the feasible set defined by subsets of the constraints, we establish that the iMELa method can find an $\epsilon$-Karush-Kuhn-Tucker point with $\tilde O(\epsilon^{-2})$ gradient oracle complexity. 
\end{abstract}

\begin{keywords}
non-convex constrained optimization, Lagrangian method, first-order methods, error bound conditions
\end{keywords}

\section{Introduction}

\blfootnote{\texttt{yankun.huang@asu.edu}\textsuperscript{1}, \texttt{qihang-lin@uiowa.edu}\textsuperscript{2}, and \texttt{xuy21@rpi.edu}\textsuperscript{3}.}In this paper, we study first-order methods (FOMs) for nonlinear constrained optimization problems formulated as 
\begin{align}
\label{eq:gco_general}
    f^*\equiv \min_{\bx\in\X}f(\bx) \quad \text{s.t.} \quad \bg(\bx)=(g_1(\bx),\dots,g_m(\bx))^\top\leq \bzero,
\end{align}
where $\X\subset\mathbb{R}^n$ is a bounded polytope that allows a computationally easy projection operator, $f$ is smooth but not necessarily convex, and $g_i$ for $i=1,\dots,m$ are smooth and convex. Since \eqref{eq:gco_general} is non-convex, computing its optimal or $\epsilon$-optimal solution %of \eqref{eq:gco_general} 
is in general intractable. Therefore, the objective of this study is to compute an $\epsilon$-Karush-Kuhn-Tucker ($\epsilon$-KKT) point of \eqref{eq:gco_general} (see Definition~\ref{dfn:epsilon_KKT}) using an FOM. 


The FOM studied in this paper is the inexact Moreau envelope Lagrangian (iMELa) method (see Algorithm~\ref{alg:imela}). This method is related to the proximal augmented Lagrangian method (ALM), a technique with a long history~\cite{hestenes1969multiplier,powell1969method,rockafellar1973multiplier,rockafellar1973dual,rockafellar1974augmented,rockafellar1976augmented} that remains an active area of research~\citep{lin2022complexity,kong2019complexity,kong2020efficient,melo2020iteration,kong2023iteration-SIOPT,kong2023iteration-MathOR,zhang2020proximal,zhang2022global,zhang2022iteration,pu2024smoothed,zeng2022moreau}. Following the literature, we measure the computational efficiency of the iMELa method by its first-order \emph{oracle complexity}, which is the number of gradient evaluations of $f$ or $g_i$ for $i=1,\dots,m$ required to reach an $\epsilon$-KKT point. 

Under a Slater's condition and a local error bound condition on the constraint set (Assumption~\ref{assume:draft_regular_cond_lm}), we show that the iMELa method can find an $\epsilon$-KKT point for \eqref{eq:gco_general} with complexity $\tilde{O}(\epsilon^{-2})$.\footnote{Here and in the rest of the paper, any logarithmic factor of $\epsilon$ is omitted in notation $\tilde{O}(\cdot)$.} This complexity matches the optimal complexity of an unconstrained problem~\cite{carmon2020lower,carmon2021lower} up to a logarithmic factor and thus is also (nearly) optimal for a constrained problem. The algorithmic design and the complexity analysis of the iMELa method are motivated by~\cite{zhang2020proximal,zhang2022global,zhang2022iteration}, where the authors studied similar algorithms that also have complexity $O(\epsilon^{-2})$ but under different assumptions. See Section~\ref{sec:relatedworks} for comparisons with those works in details.

Besides the complexity result, this work contributes to understanding the interaction between the convergence property of the iMELa method and the local error bound conditions of the constraints. Error bound condition has a long history of study for its connection to constraint qualifications and its impacts on the convergence of optimization algorithms (see, e.g.~\cite{lewis1998error,luo1994error,pang1997error,li1997abadie}). 
Unlike existing works, where the complexity analysis relies on different constraint qualifications, e.g. Mangasarian-Fromovitz constraint qualification (MFCQ)~\cite{jia2022first}, linear independence constraint qualification (LICQ)~\cite{pu2024smoothed} and constant rank constraint qualification (CRCQ)~\cite{zhang2022iteration}, our analysis leverages the local error bound property which is weaker than most constraint qualification conditions~\cite{li1997abadie}. As a result, the complexity result in this work applies to the cases which are not covered by the existing methods.  



\section{Related Works}
\label{sec:relatedworks}
In the recent literature~\cite{boob2023stochastic, ma2020quadratically,jia2022first,huang2023oracle,liu2025single}, subgradient-based methods have been developed for non-smooth continuous optimization problems with convex or weakly convex inequality constraints and achieved the oracle complexity of $O(\epsilon^{-4})$ for finding a nearly $\epsilon$-KKT point. Although those methods can be applied to \eqref{eq:gco_general}, their analysis does not utilize the smoothness of the functions and thus has a complexity higher than the one in this work when the constraints are convex. 

The iMELa method we study is closely related to the classical  augmented Lagrangian method (ALM), which has a long history of study~\cite{hestenes1969multiplier,powell1969method,rockafellar1973multiplier,rockafellar1973dual,rockafellar1974augmented,rockafellar1976augmented} and is still one of the most effective approaches for constrained optimization. Below, we review the recent studies on ALM and its variants, including the penalty methods, for the following general problem
\begin{align}
\label{eq:gco_general_new}
    \min_{\bx}f(\bx)+r(\bx) \; \text{s.t.} \; \bg(\bx)=(g_1(\bx),\dots,g_m(\bx))^\top\leq \bzero, \;  
    \bh(\bx)=(h_1(\bx),\dots,h_l(\bx))^\top= \bzero, 
\end{align}
where $f$, $g_i$, $i=1,\dots,m$, and $h_j$, $j=1,\dots,l$, are smooth and have Lipschitz continuous gradients, and $r$ is a proper convex lower semi-continuous function that allows a computationally easy proximal mapping.  When \eqref{eq:gco_general_new} is convex, the oracle complexity of the ALM for finding an $\epsilon$-optimal solution is well studied. See, e.g.~\cite{lan2016iteration,xu2021iteration,xu2021first,he2010acceleration,liu2019nonergodic} for the results under different settings. We next focus on the works that are applicable to \eqref{eq:gco_general_new} or its special cases when the problem is non-convex.  

In several studies on ALM (e.g.~\cite{hong2016decomposing,hajinezhad2019perturbed,zeng2022moreau}), the efficiency of an algorithm is characterized by its \emph{iteration complexity}, defined as the number of main iterations needed for computing an $\epsilon$-KKT point. Since a (proximal) augmented Lagrangian subproblem must be solved in each main iteration of ALM—typically using a separate FOM—the oracle complexity is generally higher than the iteration complexity.  

\textbf{Methods for linear constraints.} Suppose $g_i$'s are not present and $h_j$'s are linear in \eqref{eq:gco_general_new}. When $r\equiv 0$, Hong~\cite{hong2016decomposing} introduced a proximal primal-dual algorithm (prox-PDA) that finds an $\epsilon$-KKT point of \eqref{eq:gco_general_new} with an iteration complexity of $O(\epsilon^{-2})$. When $r$ in \eqref{eq:gco_general_new} is the characteristic function of a box or a bounded polytope, it has been shown in~\cite{zhang2020proximal,zhang2022global} that a smoothed proximal ALM (SP-ALM), which generalizes the classical proximal ALM~\cite{rockafellar1976augmented}, can achieve an $\epsilon$-KKT point with an oracle complexity of $O(\epsilon^{-2})$. When $r$ is the characteristic function of a compact set defined by convex inequalities, it has been shown that SP-ALM also achieves an $O(\epsilon^{-2})$ oracle complexity under CRCQ~\cite{zhang2022iteration}. 

Suppose no additional structural assumption is made on $r$. Hajinezhad and Hong~\cite{hajinezhad2019perturbed} developed a variant of prox-PDA with an iteration complexity of $O(\epsilon^{-4})$. Zeng et al.~\cite{zeng2022moreau} proposed the Moreau envelope ALM (MEAL), a type of proximal ALM, which achieves an iteration complexity of $o(\epsilon^{-2})$ when $f$ satisfies an implicit Lipschitz subgradient property and $O(\epsilon^{-2})$ when $f$ satisfies an implicit bounded subgradient property. The convergence property of MEAL is also established  when the augmented Lagrangian function satisfies the Kurdyka-{\L}ojasiewicz property~\cite{zeng2022moreau}. Kong et al.~\cite{kong2019complexity,kong2020efficient} proposed a quadratic penalty accelerated inexact proximal point (QP-AIPP) method, which finds an $\epsilon$-KKT point with an oracle complexity of $\tilde O(\epsilon^{-3})$. Under additional mildly strong assumptions, a reduced oracle complexity of $\tilde O(\epsilon^{-2.5})$ is achieved by an inexact ALM (iALM) by Li et al.~\cite{li2021rate},  an inexact proximal accelerated augmented Lagrangian (IPAAL) by Melo et al.~\cite{melo2020iteration}, and an inner accelerated inexact proximal augmented Lagrangian (IAIPAL) method  by Kong et al.~\cite{kong2023iteration-SIOPT}. 

\textbf{Methods for nonlinear convex constraints.} Suppose $g_i$'s are convex and $h_j$'s are linear in \eqref{eq:gco_general_new}. Lin et al.~\cite{lin2022complexity} proposed an inexact proximal point penalty (iPPP) method that achieves an oracle complexity of $\tilde{O}(\epsilon^{-2.5})$ under a Slater's condition. Under the similar assumptions, Li and Xu~\cite{li2021augmented} extended this approach to a hybrid method combining ALM and the penalty method with the same complexity. Dahal et al.~\cite{dahal2023damped} provided a damped proximal ALM that achieves the same order of oracle complexity. The iALM by Li et al.~\cite{li2021rate} can also achieve the same complexity but requires an additional regularity assumption (see \eqref{eq:PLconstraint} below). The IAIPAL introduced in~\cite{kong2023iteration-SIOPT} was also extended by Kong et al.~\cite{kong2023iteration-MathOR} to handle nonlinear convex constraints and obtains an $\tilde{O}(\epsilon^{-3})$ oracle complexity. 

When $r$ is the characteristic function of a compact set defined by convex inequalities, Pu et al.~\cite{pu2024smoothed} developed a smoothed proximal Lagrangian method (SP-LM), which finds an $\epsilon$-KKT point with an oracle complexity of $O(\epsilon^{-2})$. In contrast, our method assumes $r$ is the characteristic function of a compact polytope and requires a complexity of $\tilde O(\epsilon^{-2})$. Besides the assumption on $r$, there are two key differences between their method and ours in the updating schemes. First, their method projects the dual variables onto a compact, artificially constructed set,  whereas our method only needs to project the dual variables onto $\mathbb{R}^m_+$. Second, their method only uses a single loop but ours employs a double-loop structure. More importantly, their method relies on a Slater-like condition and a local LICQ condition at all KKT points, whereas we assume a Slater's condition and a local error bound condition on each active subset of the constraint set (see Assumption~\ref{assume:draft_regular_cond_lm}). We will show (see Proposition~\ref{thm:LICQ_implies_regular_cond_lm}) that, if the local LICQ condition in~\cite{pu2024smoothed} holds over all KKT points, our local error bound assumption also holds. However, the converse is not true. Thus, our work extends the complexity analysis of SP-LM to the case not covered in~\cite{pu2024smoothed}.  

\textbf{Methods for nonconvex constraints.} Suppose $g_i$'s are non-convex or $h_j$'s are nonlinear in \eqref{eq:gco_general_new}. In this case, even finding a feasible solution for \eqref{eq:gco_general_new} is generally intractable. When $r$ is the characteristic function of a box and $\bg\equiv\bzero$,~\cite{curtis2015adaptive,curtis2016adaptive} proposed an augmented Lagrangian trust-region method with the global asymptotic convergence guarantee. Their methods may produce an infeasible stationary point due to the non-convex constraints. In general, additional assumptions are needed to obtain a feasible stationary point. For example, assuming a (nearly) feasible solution can be easily obtained, the iPPP method~\cite{lin2022complexity} and a scaled dual descent alternating direction method of multipliers (SDD-ADMM) proposed by Sun and Sun~\cite{sun2024dual} can find an $\epsilon$-KKT point with an oracle complexity of $O(\epsilon^{-4})$. Another common assumption made in literature is the regularity condition, which assumes that there exists $\zeta>0$ such that
\begin{align}
\label{eq:PLconstraint}
    \zeta\sqrt{\|[\bg(\bx)]_+\|^2+\|\bh(\bx)\|^2}\leq \text{dist}\left(-\nabla\bg(\bx)[\bg(\bx)]_+-\nabla\bh(\bx)\bh(\bx),\partial r(\bx)\right)
\end{align}
for any $\bx\in\text{dom}(r)$. When \eqref{eq:PLconstraint} holds and $g_i$'s are not present, Sahin et al. \cite{sahin2019inexact} showed that iALM can find an $\epsilon$-KKT point with a complexity of $\tilde O(\epsilon^{-4})$.  This complexity is reduced to $\tilde O(\epsilon^{-3})$ by the iALM~\cite{li2021rate}\footnote{Although the problem considered in~\cite{li2021rate} involves only equality constraints, their results can be extended to the general problem \eqref{eq:gco_general_new}, as noted in~\cite[Remark 6]{li2021rate}.} and the iPPP method~\cite{lin2022complexity} with the inequality constraints, and to $O(\epsilon^{-3})$ by the SDD-ADMM method~\cite{sun2024dual} without the inequality constraints when there is only one block in $\bx$. Moreover, the SDD-ADMM method~\cite{sun2024dual} can achieve a complexity of $O(\epsilon^{-2})$ under stronger assumptions. Investigating whether an oracle complexity of $O(\epsilon^{-2})$ can be achieved by the iMELa method, SP-LM or SP-ALM under weaker assumptions than those in~\cite{li2021rate} is a topic for future study.

\section{Preliminaries}
\label{sec:preliminaries}
%Let $\|\cdot\|$ be the $\ell_2$-norm, $\|\cdot\|_1$ be the $\ell_1$-norm and $\|\cdot\|_\infty$ be the $\ell_\infty$-norm. 
Let $\|\cdot\|$ be the $\ell_2$-norm and $\|\cdot\|_1$ be the $\ell_1$-norm. For a differentiable function $h$ on $\mathbb{R}^n$, we say $h$ is \emph{$L$-smooth ($L\geq0$)} on $\X$ if 
%\begin{align*}
	%$h(\bx)\leq h(\bx')+\langle\nabla h(\bx),\bx-\bx'\rangle+\frac{L }{2}\|\bx-\bx'\|^2$ 
$   \|\nabla h(\bx) - \nabla h(\bx')\| \leq L \|\bx-\bx'\|$
%\end{align*}
for any $(\bx,\bx')\in\X\times\X$, \emph{$\mu$-strongly convex ($\mu\geq0$)} on $\X$ if 
%\begin{align*}
$	h(\bx)\geq h(\bx')+\langle\nabla h(\bx'),\bx-\bx'\rangle+\frac{\mu }{2}\|\bx-\bx'\|^2$ 
%\end{align*}
for any $(\bx,\bx')\in\X\times\X$, and \emph{$\rho$-weakly convex} ($\rho\geq0$) on $\X$ if
%\begin{align*}
$	h(\bx)\geq h(\bx')+\langle\nabla h(\bx'),\bx-\bx'\rangle-\frac{\rho}{2}\|\bx-\bx'\|^2$ 
%\end{align*}
for any $(\bx,\bx')\in\X\times\X$. Note that the $L$-smoothness of $h$ implies %is equivalent to 
that %$\nabla h(\bx)$ is $L$-Lipschitz continuous and 
$h$ is $L$-weakly convex but $h$ can have a smaller weak convexity constant.

For a closed convex set $\B$, we denote its normal cone at $\bx$ by $\mathcal{N}_\B(\bx)$. %and its relative interior by $\text{relint}(\B)$.
Let $\delta_{\B}(\bx)$ be the zero-infinity characteristic function of $\B$, i.e., $\delta_{\B}(\bx)=0$ if $\bx\in \B$ and $+\infty$ otherwise. Let $\text{proj}_{\B}(\cdot)$ be the projection mapping onto $\B$ and $\text{dist}(\bx,\A):=\min_{\by\in \A}\|\bx-\by\|$ for a set $\A$. Define $[\cdot]_+:=\max\{\cdot,0\}$. Denote $[m]=\{1,\dots,m\}$ and $[l]=\{1,\dots,l\}$.
%$\max\{\cdot,\cdot\}$ takes the maximum between scalars or function values. Denote $[\cdot]_+=\max\{\cdot,0\}$. 
%and $\mathbb{R}^m$ as the set of $m$-dimensional vectors having all non-negative components.
%We say a point $\bx$ is $\epsilon$-feasible if $\bx\in\X$ and $\bg(\bx)\leq\epsilon$. Let $\delta_{\X}(\bx)$ be the zero-infinity characteristic function of set $\X$, $\text{proj}_{\X}(\cdot)$ be the projection mapping to $\X$, and $\text{dist}(\bx,\A):=\min_{\blambda\in \A}\|\bx-\blambda\|$ for set $\A$.

%Recall that in problem~\eqref{eq:gco_general} we assume that $\X$ is a polytope. According to Definition 4.2.5 in Chapter A of~\citep{hiriart2001fundamentals}, we can write in the matrix form where $\X:=\big\{\bx:\langle\bx,a_j\rangle\leq b_j,\;j=1,2,\dots,k\big\}=\big\{\bx\in\mathbb{R}^n:\bA\bx\leq\bb\big\}$. Note that $A\in\mathbb{R}^{k\times n}$ has rows $a_j^\top$ $j=1,\dots,k$ and $b=(b_1,b_2,\dots,b_k)\in\mathbb{R}^{k}$.

We make the following assumptions on~\eqref{eq:gco_general} throughout this paper.
\begin{assumption}
\label{assume:draft}
The following statements hold:
\begin{itemize}
    %\setlength\itemsep{6pt}
    \item[A.] $\X=\big\{\bx\in\mathbb{R}^n:\bA\bx\leq \bb\big\}$ with $\bA\in\mathbb{R}^{l\times n}$ and $\bb\in\mathbb{R}^{l}$ is compact with diameter $D_\X:=\max_{\bx,\bx'\in\X}\|\bx-\bx'\|<+\infty$.
    \item[B.] $\min_{\bx\in\X}f(\bx)\geq \underline{f}$ for some $\underline{f}\in\mathbb{R}$.
    \item[C.] $f$ and $g_i$, $i\in[m]$, are $L$-smooth, and each $g_i$ is convex.
    \item[D.] $\max_{\bx\in\X}\{\|\nabla f(\bx)\|\}\leq B_f$ for some $B_f\geq0$. % $B_f\in\mathbb{R}$.
    \item[E.] $\max_{\bx\in\X}\max\{\|\bg(\bx)\|,\|\nabla \bg(\bx)\|\}\leq B_g$ for some $B_g\geq0$. % $B_g\in\mathbb{R}$.
    \item[F.] (Slater's condition) There exists $\bx_{\textup{feas}}\in\X$ such that $g_i(\bx_{\textup{feas}})<0$, $i\in[m]$. 
\end{itemize}
\end{assumption}


We denote the Lagrangian function of \eqref{eq:gco_general} by
\begin{align}
\label{eq:Lagrange}
    \mathcal{L}(\bx,\blambda): =f(\bx)+\sum_{i=1}^m \lambda_i g_i(\bx),
\end{align}
where $\blambda=(\lambda_1,\dots,\lambda_m)^\top$ is a vector of Lagrangian multipliers. A point $\bx^*\in\X$ is called a Karush-Kuhn-Tucker (KKT) point of \eqref{eq:gco_general} if there exists $\blambda^*$ %a vector of Lagrangian multipliers 
%$\blambda^*=(\lambda_1^*,\dots,\lambda_m^*)^\top\in\mathbb{R}^m$ 
such that %the following conditions hold:
\begin{align*}
    \text{stationarity: }&~ -\nabla f(\bx^*) -\textstyle\sum_{i=1}^m \lambda_i^*\cdot\nabla g_i(\bx^*)\in \N_\X(\bx^*),\\
    \text{feasibility: }&~g_i(\bx^*)\leq0,~ \lambda_i^*\geq0,\;i\in[m],\\
    \text{complementary slackness: }&~\textstyle\sum_{i=1}^m\lambda_i^* g_i(\bx^*)=0.
\end{align*}
Typically, a KKT point can only be approached by an algorithm as a limit point of its iterates. Within a finite number of iterations, an algorithm may only generate an \emph{$\epsilon$-KKT} point for a tolerance $\epsilon > 0$, which %only approximately satisfies the conditions above and 
is defined as follows.
\begin{definition}
\label{dfn:epsilon_KKT}
A point $\bx\in\X$ is an $\epsilon$-KKT point of problem \eqref{eq:gco_general} if there exists $\blambda\geq\bzero$ such that 
\begin{align}
\label{dfn:epsilon_stat}
    \epsilon\text{-stationarity: }&~%\text{dist}\Big(\nabla f(\bx)+\nabla \bg(\bx)^\top\blambda,-\N_\X(\bx)\Big)\leq\epsilon,\\
    \text{dist}(-\nabla f(\bx) -\textstyle\sum_{i=1}^m \lambda_i\cdot \nabla g_i(\bx), \N_\X(\bx))\leq\epsilon,\\
    \label{dfn:epsilon_feas}
    \epsilon\text{-feasibility: }&~
    \|[\bg(\bx)]_+\|\leq\epsilon,\\
    \label{dfn:epsilon_comple_slack}
    \epsilon\text{-complementary slackness: }&~\textstyle\sum_{i=1}^m |\lambda_ig_i(\bx)|\leq\epsilon.
\end{align}
\end{definition}

For any $I_g\subseteq[m]$ and $I_{A}\subseteq[l]$, we define an active feasible subset as
%\begin{small}
\begin{equation}
\label{eq:regular_cond_eq_ineq_sys}    
    \mathcal{S}(I_g, I_A)  :=
    \left\{\bu\in\X:
    \begin{array}{l}
    ~g_i({\bu})=0,\;i\in I_g,\;g_i({\bu})\leq 0,\;i\in[m]\backslash I_g,\\
    ~[\bA{\bu}-\bb]_j=0,\;j\in I_A,\;[\bA{\bu}-\bb]_j\leq0,\;j\in[l]\backslash I_A
\end{array}
\right\}.
\end{equation}
%\end{small}
When $\mathcal{S}(I_g, I_A)\neq\emptyset$, we call $\mathcal{S}(I_g, I_A)$ the active feasible subset induced by %the active constraints indexed by 
$I_g$ and $I_A$. For any $\bx\in\X$, let 
$$
J_g(\bx):=\{i\in[m]:  g_i(\bx)=0\}\text{  and  }J_A(\bx):=\{j\in[l]: [\bA\bx-\bb]_j=0\}
$$ 
be the index sets of the constraints active at $\bx$. Let $\X^*$ denote the set of KKT points of \eqref{eq:gco_general}. We assume the constraints of  \eqref{eq:gco_general} satisfy a uniform local error bound condition near each KKT point.
%\textcolor{green}{$\bx^*$ on each ``face'' of the sets of the active constraints at $\bx^*$}. 
\begin{assumption}[Local Error Bound Condition]
\label{assume:draft_regular_cond_lm}
There exist $\delta>0$ and $\gamma>0$ such that, for any $\bx\in\X$, if $\|\bx-\bx^*\|\leq \delta$ for some $\bx^*\in\X^*$, it holds that
%\begin{small}
\begin{align}
\label{eq:regular_cond_gamma_lm}
    \textup{dist}^2 (\bx,\mathcal{S}(I_g, I_A))\leq \gamma\left(
    \begin{array}{l}
    \sum_{i\in I_g} |g_i(\bx)|^2
    +\sum_{i\in[m] \backslash I_g} ([g_i(\bx)]_+)^2\\
    +\sum_{j\in I_A}\left|[\bA\bx-\bb]_j\right|^2
    +\sum_{j\in[l] \backslash I_A} \big(\left[[\bA\bx-\bb]_j\right]_+\big)^2
    \end{array}
    \right),
%= \gamma \cdot \mathrm{dist}^2\big(\bg(\bx), \mathcal{N}_{\mathbb{R}_+^m}(\blambda)\big).
\end{align}
%\end{small}
for any $I_g\subset J_g(\bx^*)$ and $I_A\subset J_A(\bx^*)$.
\end{assumption}
Inequality \eqref{eq:regular_cond_gamma_lm} means the set $\mathcal{S}(I_g, I_A)$ defined %by the equalities and inequalities as 
in \eqref{eq:regular_cond_eq_ineq_sys} satisfies a local linear error bound with $\delta$ and $\gamma$ uniform for all $\bx$. 
We show that Assumption~\ref{assume:draft_regular_cond_lm} holds when the constraints of \eqref{eq:gco_general} satisfy the local linear independence constraint qualification (LICQ) assumed in \cite[Assumption 2.2]{pu2024smoothed}, namely, there exists $\zeta>0$ such that, for any $\bx^*\in\X^*$, 
\begin{align}
\label{eq:LICQ_sigmamin}
    \sigma_{\min}\big(\big[\nabla \bg_{J_g(\bx^*)}(\bx^*), \bA_{ J_A(\bx^*)}^\top\big]\big)\geq \zeta,
\end{align}
where $\sigma_{\min}(\cdot)$ denotes the smallest singular value. See the following proposition whose proof is provided in Appendix~\ref{sec:LICQ_implies_regular_cond_lm}.
\begin{proposition}
\label{thm:LICQ_implies_regular_cond_lm}
If the constraints of \eqref{eq:gco_general} satisfy the local LICQ in \eqref{eq:LICQ_sigmamin}, then Assumption~\ref{assume:draft_regular_cond_lm} holds, but not vice versa.
\end{proposition}

Throughout the paper, we assume that $p$ is a constant satisfying $p>L$. Following~\cite{zhang2020proximal,zhang2022global,zhang2022iteration}, given any $\bz\in\X$, we define 
\begin{align}
    \label{eq:v(z)}    
    &\qquad v(\bz):=\min_{\bx\in\X, \,g_i(\bx)\leq0,\,i\in[m]} \left\{f(\bx)+\frac{p}{2}\|\bx-\bz\|^2\right\},\\
    \label{eq:x(z)}
    \begin{split}
        &\qquad \bx(\bz):= \argmin_{\bx\in\X, \,g_i(\bx)\leq0,\,i\in[m]}\left\{f(\bx)+\frac{p}{2}\|\bx-\bz\|^2\right\}, \\
        &\text{and }\blambda(\bz) \text{ is a corresponding vector of Lagrangian multipliers}.
    \end{split}
\end{align}
%Here, the term $\frac{p}{2}\|\bx-\bz\|^2$ is called the smoothed proximal term~\cite{zhang2020proximal,zhang2022global,zhang2022iteration}. 
Note that \eqref{eq:v(z)} is a convex optimization problem as $p>L$ and $v(\bz)$ is differentiable by Danskin's Theorem (see, e.g.~\cite[Proposition B.25]{bertsekas1999nonlinear}) with
%By Assumption~\ref{assume:draft}A, set $\{\bx\in\X:\bg(\bx)\leq\bzero\}$ is bounded. Thus, we get from Danskin's theorem (see, e.g.~\cite[Proposition B.25]{bertsekas1999nonlinear} or~\cite[Proposition 4.5.1]{bertsekas2003convex}) that
\begin{align}
\label{eq:nablav}
    \nabla v(\bz)=p(\bz-\bx(\bz)).
\end{align}
Like \eqref{eq:Lagrange}, the Lagrangian function of \eqref{eq:v(z)} is
\begin{align}
\label{eq:imela}
    \mathcal{L}_p (\bx,\bz,\blambda) :=f(\bx)+ \sum_{i=1}^m\lambda_i g_i(\bx)+\frac{p}{2}\|\bx-\bz\|^2.
\end{align}
Similar to the notations in~\cite{zhang2020proximal,zhang2022global,zhang2022iteration}, we define
\begin{align}
    \label{eq:d(lambda,z)}    
    d(\blambda,\bz):=%&~
    \min_{\bx\in\X} \mathcal{L}_p (\bx,\bz,\blambda),\quad % \left\mathcal{L}_p (\bx,\bz,\blambda) =f(\bx)+\sum_{i=1}^m\lambda_i\cdot g_i(\bx)+\frac{p}{2}\|\bx-\bz\|^2\right\},\\ %K(\bx,\bz,y),\\
    %\label{eq:x(lambda,z)}    
    \bx(\blambda,\bz):=% ~
    \argmin_{\bx\in\X} \mathcal{L}_p (\bx,\bz,\blambda).%K(\bx,\bz,y),\\
\end{align}
% At the end of this section, we present the strong duality between the quadratically regularized problems~\eqref{eq:v(z)} and~\eqref{eq:d(lambda,z)} for completion. Apparently $\mathcal{L}_p(\bx,\bz,\blambda)\geq d(\blambda,\bz)$ for $\forall \blambda\geq\bzero,\bz\in\X$. Since $p>L$, we get that $f(\bx)+\frac{p}{2}\|\bx-\bz\|^2$ is $(p-L)$-strongly convex for $\bx\in\X$. Fix some $\bz\in\X$. By Assumption~\ref{assume:draft}F, we have the Slater's condition for problem~\eqref{eq:v(z)}. Thus, we get from~\cite[Corollary 29.1.4]{rockafellar1970convex} that there exists $\blambda(\bz)\geq\bzero$ such that the following KKT conditions hold at $\bx(\bz)\in\X$:
% \begin{align*}
%     \text{stationarity: }&~\nabla_\bx f(\bx(\bz))+\textstyle\sum_{i=1}^m [\blambda(\bz)]_i\cdot\nabla_\bx g_i(\bx(\bz))\\
%     &+p(\bx(\bz)-\bz)\in-\N_\X(\bx),\\
%     \text{feasibility: }&~ g_i(\bx(\bz))\leq0,~[\blambda(\bz)]_i\geq0,\;i=1,2,\dots,m,\\
%     \text{complementary slackness: }&~[\blambda(\bz)]_i\cdot g_i(\bx(\bz))=0,\;i=1,2,\dots,m,
% \end{align*}
% where $\blambda(\bz)$ is a vector of Lagrangian multipliers. By~\cite[Theorem 28.3]{rockafellar1970convex}, we know that the KKT conditions above hold for $(\bx(\bz),\blambda(\bz))$ if and only if $(\bx(\bz),\blambda(\bz))$ is a saddle-point of $\mathcal{L}_p(\bx,\bz,\blambda)$, i.e.,
% \begin{align*}
%    \mathcal{L}_p(\bx(\bz),\bz,\blambda) \leq\mathcal{L}_p (\bx(\bz),\bz,\blambda(\bz))=v(\bz) \leq\mathcal{L}_p(\bx,\bz,\blambda(\bz))
% \end{align*}
% for $\forall\bx\in\X$ with $g_i(\bx)\leq0$, $i=1,2,\dots,m$, $\forall\blambda\geq\bzero$, where the equality comes from~\cite[Theorem 28.4]{rockafellar1970convex}. By the saddle-point condition, we get from~\eqref{eq:d(lambda,z)} that
% \begin{align*}
%     d(\blambda,\bz)=\min_{\bx\in\X} \mathcal{L}_p(\bx,\bz,\blambda)
%     \leq\min_{\substack{\bx\in\X, ~g_i(\bx)\leq0 \\ i=1,2,\dots,m}} \mathcal{L}_p(\bx,\bz,\blambda)
%     \leq\mathcal{L}_p (\bx(\bz),\bz,\blambda)\leq v(\bz)
% \end{align*}
% for $\forall\blambda\geq\bzero$. This yields the weak duality $d(\blambda,\bz)\leq v(\bz)$, $\forall\blambda\geq\bzero$. By~\cite[Theorem 28.4]{rockafellar1970convex} again, we have that the condition for $\bx(\bz)$ in~\eqref{eq:x(z)} holds if and only if $\bx(\bz)$ minimizes $\mathcal{L}_p(\bx,\bz,\blambda(\bz))$ over $\X$, i.e., $v(\bz)=\mathcal{L}_p (\bx(\bz),\bz,\blambda(\bz))=d(\blambda(\bz),\bz)$ for this $\blambda(\bz)\geq\bzero$. Therefore, we get the strong duality:
By the strong duality (see, e.g.~\cite[Section 28-30]{rockafellar1970convex}), it holds that
\begin{align}
\label{eq:str-dual}
    v(\bz)= \max_{\blambda\geq\bzero} d(\blambda,\bz).
\end{align} 


\section{Inexact Moreau Envelope Lagrangian Method}

We present the inexact Moreau envelope Lagrangian (iMELa) method in Algorithm~\ref{alg:imela}, for finding an $\epsilon$-KKT point of~\eqref{eq:gco_general}. At iteration $t$, we first obtain $\blambda^{(t+1)}$ by performing a projected gradient ascent step with respect to $\blambda$ on function $\mathcal{L}_p(\bx^{(t)},\bz^{(t)},\blambda)$ at $\blambda=\blambda^{(t)}$ with a step-size $\tau_t$. Then, given the convex subproblem $\min_{\bx\in\X}\mathcal{L}_p(\bx,\bz^{(t)},\blambda^{(t+1)})$, the optimal solution of which denoted by $\tilde{\bx}^{(t+1)}:= \bx(\blambda^{(t+1)},\bz^{(t)})$ in the rest of the paper for simplicity of notation, we approximately solve it by computing its $\epsilon_t$-stationary point with $\epsilon_t\in(0,1)$, which is a point $\bx^{(t+1)}\in\X$ satisfying
\begin{align}
\label{eq:subroutine_optimality_imela} 
    \text{dist}\big( -\nabla_\bx\mathcal{L}_p(\bx^{(t+1)},\bz^{(t)},\blambda^{(t+1)}),\N_\X(\bx^{(t+1)})\big)\leq\epsilon_t.
\end{align}
The point 
$\bx^{(t+1)}$ can be found by various FOMs, %for unconstrained optimization, 
for example, the algorithms in~\cite[Section 5.1.1]{nesterov2013gradient} and \cite[(2.2.22)]{nesterov2018lectures}. Moreover, condition \eqref{eq:subroutine_optimality_imela} can be numerically verified and thus used as a stopping condition for solving the subproblem, provided that $\N_\X(\bx)$ has a simple structure. At last, we update $\bz$ %$\bz^{(t)}$ to $\bz^{(t+1)}$ 
by taking a convex combination of $\bz^{(t)}$ and $\bx^{(t+1)}$ with a weight $\theta_t\in[0,1]$, which ensures $\bz^{(t)}\in\X,\, \forall\,t\geq0$.  

\begin{algorithm}[ht]
\setstretch{1.125}
\caption{Inexact Moreau Envelope Lagrangian (iMELa) Method}
\label{alg:imela}
\begin{algorithmic}[1]
    \STATE {\bfseries Input:} total number of iterations $T$, proximal parameter $p>0$, step-sizes $\{\tau_t,\theta_t\}_{t\geq0}$.
    \STATE {\bfseries Initialization:} $\bx^{(0)}=\bz^{(0)}\in\X$, $\blambda^{(0)}=\bzero$.
    \FOR{iteration $t=0,1,\cdots,T-1$}
    \STATE Set $\lambda_i^{(t+1)} =[\lambda_i^{(t)}+\tau_t\cdot g_i(\bx^{(t)})]_+,\; \forall\,i\in[m]$.
    \STATE Compute $\bx^{(t+1)}\approx\tilde{\bx}^{(t+1)}:= \argmin_{\bx\in\X}\mathcal{L}_p(\bx,\bz^{(t)},\blambda^{(t+1)})$ that satisfies \eqref{eq:subroutine_optimality_imela}.
    \STATE \vskip 0.5pt Set $\bz^{(t+1)}=\bz^{(t)}+\theta_t\cdot(\bx^{(t+1)}-\bz^{(t)})$.
    \ENDFOR
    %\STATE {\bfseries Output:} $\bx^{(s)}$ with index $s$ sampled uniformly randomly from $\{0,1,\dots,T\}$.
\end{algorithmic}
\end{algorithm}

Condition \eqref{eq:subroutine_optimality_imela} holds if and only if there exists $\bv^{(t+1)}\in\mathbb{R}^n$ such that $\|\bv^{(t+1)}\|\leq\epsilon_t$ and $\bxi^{(t+1)}\in\N_\X(\bx^{(t+1)})$ such that
\begin{align}
\label{eq:subroutine_optimality_imela_equiv} 
   \nabla_\bx\mathcal{L}_p (\bx^{(t+1)},\bz^{(t)},\blambda^{(t+1)})=&-\bxi^{(t+1)}-\bv^{(t+1)}.
\end{align}
This implies that for any $\bx\in\X$,
\begin{align}
\label{eq:normalconext}
\langle\nabla_\bx\mathcal{L}_p (\bx^{(t+1)},\bz^{(t)},\blambda^{(t+1)})+\bv^{(t+1)},\bx-\bx^{(t+1)}\rangle=
    \langle\bxi^{(t+1)},\bx^{(t+1)}-\bx\rangle\geq 0.
\end{align}
In addition, %condition \eqref{eq:subroutine_optimality_imela} implies that $\bx^{(t+1)}$ is an $\frac{\epsilon_t^2}{2(p-L)}$-optimal solution for the subproblem $\min_{\bx\in\X}\mathcal{L}_p(\bx,\bz^{(t)},\blambda^{(t+1)})$. In fact, 
since $\mathcal{L}_p$ is $(p-L)$-strongly convex in $\bx$, we have
%\begin{small}
\begin{align}
\nonumber
    &~\mathcal{L}_p (\bx^{(t+1)},\bz^{(t)},\blambda^{(t+1)}) - \mathcal{L}_p (\tilde{\bx}^{(t+1)},\bz^{(t)},\blambda^{(t+1)})\\
    \nonumber
    \leq&-\langle\nabla_\bx\mathcal{L}_p (\bx^{(t+1)},\bz^{(t)},\blambda^{(t+1)}), \tilde{\bx}^{(t+1)}-\bx^{(t+1)}\rangle %\textstyle 
    -\frac{p-L}{2}\|\tilde{\bx}^{(t+1)}-\bx^{(t+1)}\|^2\\
    \nonumber
    \leq&%\mathcal{L}_p (\bx(\blambda^{(t+1)},\bz^{(t)}),\bz^{(t)},\blambda^{(t+1)}) +
    ~\langle\bv^{(t+1)}, \tilde{\bx}^{(t+1)}-\bx^{(t+1)}\rangle %\\\nonumber
    %&
    %\textstyle 
    -\frac{p-L}{2}\|\tilde{\bx}^{(t+1)}-\bx^{(t+1)}\|^2\\ %\nonumber
    \leq&  %\mathcal{L}_p (\bx(\blambda^{(t+1)},\bz^{(t)}),\bz^{(t)},\blambda^{(t+1)})+
    %\textstyle 
    ~\frac{1}{2(p-L)}\|\bv^{(t+1)}\|^2%\\
    \label{eq:subroutine_optimality_imela_corollary}
    \leq %~&  %\mathcal{L}_p (\bx(\blambda^{(t+1)},\bz^{(t)}),\bz^{(t)},\blambda^{(t+1)})+
    \frac{\epsilon_t^2}{2(p-L)},
\end{align}
%\end{small}
where %$\tilde{\bx}^{(t+1)} = \bx(\blambda^{(t+1)},\bz^{(t)})$, 
the second inequality is by \eqref{eq:normalconext} at $\bx=\tilde{\bx}^{(t+1)}$ and the third one is by Young's inequality. 

The convergence analysis for Algorithm~\ref{alg:imela} relies on a uniform upper bound on $\{\|\blambda^{(t)}\|\}_{t\ge 0}$, which we present below. 
\begin{lemma}
\label{lmm:bound_lambda_lm}
The sequence $\{\blambda^{(t)}\}_{t\geq0}$ generated by Algorithm~\ref{alg:imela} satisfies
\begin{align}
\label{eq:bound_lambda_lm}
    \|\blambda^{(t)}\|\leq M_{\blambda}:=\max\Big\{2\overline{\tau}B_g,\frac{2\overline{\tau}(C_{\blambda}+\overline{\tau}B_g^2)}{2\underline{\tau}\min_{i\in[m]}[-g_i(\bx_{\textup{feas}})]}\Big\},~\forall\,t\geq0,
\end{align}
%for $\forall\,t\geq0$, 
where $\overline{\tau}=\sup_{t\geq0}\tau_t$, $\underline{\tau}=\inf_{t\geq0}\tau_t$, and $C_{\blambda}:=(B_f+p D_{\X}+1)\cdot D_{\X}$.
\end{lemma}
\begin{proof}
%This proof is mainly suggested by~\cite{xu2024personalcommunication1}. 
Let $\bv^{(t+1)}$ be the vector from \eqref{eq:subroutine_optimality_imela_equiv}. Recall that $\bx_{\text{feas}}\in\X$ is from Assumption~\ref{assume:draft}F. By \eqref{eq:normalconext} at $\bx=\bx_{\text{feas}}$, we have
\begin{align}
    \label{eq:subroutine_epsilon_lm}
    \langle\nabla_\bx \mathcal{L}_p (\bx^{(t+1)},\bz^{(t)},\blambda^{(t+1)})+\bv^{(t+1)},\bx^{(t+1)}-\bx_{\text{feas}}\rangle\leq 0.
\end{align}  
Using the convexity of $g_i$'s and $\blambda^{(t+1)}\geq\bzero$, we have
%\begin{small}
\begin{align}
    \nonumber
    \langle\blambda^{(t+1)}, \bg(\bx^{(t+1)})- \bg(\bx_{\text{feas}})\rangle
    %=&~\sum_{i=1}^m \lambda_i^{(t+1)}\left(g_i(\bx^{(t+1)})-g_i(\bx_{\text{feas}})\right)\\
    \leq&~\textstyle\sum_{i=1}^m \langle\lambda_i^{(t+1)}\nabla g_i(\bx^{(t+1)}),\bx^{(t+1)}-\bx_{\text{feas}}\rangle\\   
    \nonumber
    =&~\langle \nabla_\bx\mathcal{L}_p(\bx^{(t+1)},\bz^{(t)},\blambda^{(t+1)})+\bv^{(t+1)},\bx^{(t+1)}-\bx_{\text{feas}}\rangle\\   
    \nonumber
    &+\langle \nabla f(\bx^{(t+1)})+p(\bx^{(t+1)}-\bz^{(t)})+\bv^{(t+1)},\bx_{\text{feas}}-\bx^{(t+1)}\rangle\\   
    \nonumber
    \leq&~\langle \nabla f(\bx^{(t+1)})+p(\bx^{(t+1)}-\bz^{(t)})+\bv^{(t+1)},\bx_{\text{feas}}-\bx^{(t+1)}\rangle\\
    % \nonumber
    % \leq&~\big(\|\nabla f(\bx^{(t+1)})\|+p\cdot\|\bx^{(t+1)}-\bz^{(t)}\|+\|\bv^{(t+1)}\|\big)\cdot\|\bx_{\text{feas}}-\bx^{(t+1)}\|\\
    \label{eq:lambda_result_C_lambda}
    \leq&~(B_f+p D_{\X}+1)\cdot D_{\X}=C_{\blambda}.
\end{align}
%\end{small}
where the second inequality is by \eqref{eq:subroutine_epsilon_lm} and the last one is from Assumption~\ref{assume:draft}A and D, and the fact that $\|\bv^{(t+1)}\|\leq\epsilon_t<1$. Since $\blambda^{(0)}=\bzero$ in Algorithm~\ref{alg:imela}, \eqref{eq:lambda_result_C_lambda} also holds for $t=-1$. So, $\langle\blambda^{(t)}, \bg(\bx^{(t)})- \bg(\bx_{\text{feas}})\rangle\leq C_{\blambda},~\forall\,t\geq0$, or equivalently, 
\begin{align}
\label{eq:lambda_result_C_lambda_equiv}
   \langle\blambda^{(t)}, \bg(\bx^{(t)})\rangle\leq C_{\blambda}-\langle\blambda^{(t)}, - \bg(\bx_{\text{feas}})\rangle,~\forall\,t\geq0.
\end{align}
%for $t\geq0$.

We then prove the result in~\eqref{eq:bound_lambda_lm} by induction. It clearly holds for $t=0$ since $\blambda^{(0)}=\bzero$. 
Suppose that the inequality in \eqref{eq:bound_lambda_lm} holds %for $\blambda^{(t)}$ 
for some $t\geq0$. By %the fact that 
$\|\bg(\bx_{\text{feas}})\|\leq B_g$ (see Assumption~\ref{assume:draft}E) and $\overline{\tau}=\sup_{t\geq0}\tau_t$, we have $\|\tau_t \bg(\bx_{\text{feas}})\|\leq\overline{\tau} B_g\leq M_{\blambda}/2$. Since $\|\blambda^{(t)}\|\leq M_{\blambda}$, we have
\begin{align}
%    \blambda_*^{(t)} =
    \nonumber
    \|\blambda^{(t)}\|^2 -2\tau_t\langle\blambda^{(t)}, - \bg(\bx_{\text{feas}})\rangle
    \leq&~\max_{\blambda\geq\bzero ,\,\|\blambda\|\leq M_{\blambda}} \left\{\|\blambda\|^2-2\tau_t\langle\blambda,-\bg(\bx_{\text{feas}})\rangle\right\}\\
    \label{eq:bound_lambda_t_lm}
    =&~\max_{\theta\in[0,1],\, \blambda\geq\bzero,\,\|\blambda\|= M_{\blambda}} \left\{\|\theta\blambda\|^2-2\theta\tau_t\langle\blambda,-\bg(\bx_{\text{feas}})\rangle\right\}.
\end{align} 
Fixing any $\blambda$ satisfying $\blambda\geq\bzero $ and $\|\blambda\|= M_{\blambda}$, consider the maximization in \eqref{eq:bound_lambda_t_lm} only over $\theta\in[0,1]$. Since $\tau_t\langle\blambda,-\bg(\bx_{\text{feas}})\rangle/\|\blambda\|^2\leq \|\tau_t \bg(\bx_{\text{feas}})\|/M_{\blambda}\leq1/2$,  $\theta=1$ is an optimal solution for \eqref{eq:bound_lambda_t_lm}. As a consequence,
\begin{align}
\nonumber
    \|\blambda^{(t)}\|^2 -2\tau_t\langle\blambda^{(t)}, - \bg(\bx_{\text{feas}})\rangle
    \leq&~\max_{\blambda\geq\bzero ,\,\|\blambda\|= M_{\blambda}} \left\{\|\blambda\|^2-2\tau_t\langle\blambda,-\bg(\bx_{\text{feas}})\rangle\right\}\\
    \label{eq:bound_lambda_M_lm}
    =&~%\textstyle 
    M_{\blambda}^2 -2\tau_t M_{\blambda} \cdot\min_{i\in[m]}[-g_i(\bx_{\text{feas}})].
\end{align} 
%where the last inequality is because $\overline{\tau}=\sup_{t\geq0}\tau_t$.

According to the updating equation of $\blambda^{(t+1)}$ in Step 4 of Algorithm~\ref{alg:imela}, we have 
\begin{align}
\nonumber
    \|\blambda^{(t+1)}\|^2 
    =&~\textstyle\sum_{i=1}^m ([\lambda_i^{(t)}+\tau_t\cdot g_i(\bx^{(t)})]_+)^2\\
    \nonumber
    \leq&~\textstyle\sum_{i=1}^m (\lambda_i^{(t)}+\tau_t\cdot g_i(\bx^{(t)}))^2%\nonumber
    =\|\blambda^{(t)}\|^2 +2\tau_t\langle\blambda^{(t)}, \bg(x^{(t)})\rangle +\tau_t^2\|\bg(x^{(t)})\|^2\\
    \nonumber
    \leq&~\|\blambda^{(t)}\|^2 +2\tau_t\big(C_{\blambda}-\langle\blambda^{(t)}, - \bg(\bx_{\text{feas}})\rangle\big) +\tau_t^2\|\bg(x^{(t)})\|^2\\
    \nonumber
    \leq&~%\textstyle 
    M_{\blambda}^2+2\overline{\tau} C_{\blambda}-2\underline{\tau} M_{\blambda}\cdot\min_{i\in[m]}[-g_i(\bx_{\text{feas}})] +\overline{\tau}^2B_g^2%\\   
    %\nonumber
    %\leq&~M_{\blambda}^2,
    \leq M_{\blambda}^2,
\end{align} 
where the second inequality is from \eqref{eq:lambda_result_C_lambda_equiv}, and the last one is from \eqref{eq:bound_lambda_M_lm}, the definitions of $\overline{\tau}$, $\underline{\tau}$ and $M_{\blambda}$ in the settings. %By induction, $\|\blambda^{(t)}\|^2 \leq M_{\blambda}^2$ for $\forall\,t\geq0$, which implies \eqref{eq:bound_lambda_lm}.
\eqref{eq:bound_lambda_lm} then follows by the inductive result.
\end{proof}

By~\eqref{eq:imela}, Lemma~\ref{lmm:bound_lambda_lm}, and $\|\blambda\|_1\leq\sqrt{m}\|\blambda\|\leq\sqrt{m}M_{\blambda}$, $\mathcal{L}_p(\bx,\bz,\blambda^{(t)})$ is $(p-L)$-strongly convex and $K$-smooth in $\bx$ over $\X$ for any $t\ge0$, where
\begin{align}
\label{eq:al_Lipschitz_cont_grad_lm}
    K:=&~L+L \sqrt{m} M_{\blambda}+p.
\end{align}
%Applying this $K$ to~\eqref{eq:x(lambda,z)} and~\eqref{eq:subroutine_optimality} , we get from 
Hence, by~\cite[Corollary 1]{nesterov2013gradient}, to ensure \eqref{eq:subroutine_optimality_imela},  it suffices to ensure
\begin{equation}
\label{eq:x_primal_descent_lm}
    \mathcal{L}_p (\bx^{(t+1)},\bz^{(t)},\blambda^{(t+1)}) -\mathcal{L}_p (\tilde{\bx}^{(t+1)},\bz^{(t)},\blambda^{(t+1)})\leq\frac{\epsilon_t^2}{4K}.
\end{equation}
Suppose that the algorithm in~\cite[Section 5.1.1]{nesterov2013gradient} with the initial point set to $\bx^{(t)}$ is applied to solve $\min_{\bx\in\X}\mathcal{L}_p(\bx,\bz^{(t)},\blambda^{(t+1)})$ for $k_t$ steps to produce $\bx^{(t+1)}$. According to \eqref{eq:x_primal_descent_lm}, Assumption~\ref{assume:draft}A and~\cite[Theorem 6]{nesterov2013gradient}, \eqref{eq:subroutine_optimality_imela} % will hold if 
% after $k_t$ steps as long as
% \begin{align*}
%    %\frac{(K-(p-L)) \left\|\bx(\blambda^{(t+1)},\bz^{(t)})-\bx^{(t)}\right\|^2}{2\left(1+\sqrt{\frac{p-L}{2(K-(p-L))}}\right)^{2(k_t-1)}}
%     \frac{(K-(p-L)) D_\X^2}{2\left(1+\sqrt{\frac{p-L}{2(K-(p-L))}}\right)^{2(k_t-1)}}
%    \leq\frac{\epsilon_t^2}{4K}.
% \end{align*}
% In other words, % \eqref{eq:subroutine_optimality_imela} 
can be ensured with complexity
\begin{small}
\begin{align}
\label{eq:subroutine_complexity_lm}
    %&~1+\frac{1}{2}\left[\ln\left(1+ \sqrt{\frac{p-L}{2(K-(p-L))}}\right)\right]^{-1} \ln\left(\frac{(K-(p-L))\left\|\bx(\blambda^{(t+1)},\bz^{(t)})-\bx^{(t)}\right\|^2}{\epsilon_t^2/(4K)}\right)\\
    %&~k_t\sim1+
    k_t\sim\frac{1}{2}\left[\ln\left(1+ \sqrt{\frac{p-L}{2(K-(p-L))}}\right)\right]^{-1} \ln\left(\frac{(K-(p-L))D_\X^2}{\epsilon_t^2/(4K)}\right)
    =O\left(\sqrt{\frac{K}{p-L}}\ln(\epsilon_t^{-1})\right).
    %\label{eq:subroutine_complexity_lm}
    %= &~O\left(\sqrt{\frac{K}{p-L}}\ln(\epsilon_t^{-1})\right).
\end{align}
\end{small}Similar complexity is needed by other accelerated FOMs, e.g. the one in~\cite[(2.2.22)]{nesterov2018lectures}.




\section{Convergence Analysis for the iMELa Method}
For simplicity of notation, let 
$%\begin{align*}
    \sigma=\frac{p-L}{p}.
$  %\end{align*}
The following lemma from~\cite[Lemma 3.5]{zhang2020proximal} characterizes the Lipschitz continuity of $\bx(\bz)$ in~\eqref{eq:x(z)} and $\bx(\blambda,\bz)$ in~\eqref{eq:d(lambda,z)} under Assumption~\ref{assume:draft}.
\begin{lemma}
\label{lmm:Lipschitz_continuity_lm}
%Let $\sigma=\frac{p-L}{p}.$ 
For any $\bz,\bz'\in\X$ and $\blambda\geq\bzero$, we have
\begin{equation}
\label{eq:Lipschitz_continuity_x(z)}
\begin{aligned}
    \|\bz-\bz'\|\geq&~\sigma\|\bx(\bz)-\bx(\bz')\|,\\ 
%\label{eq:Lipschitz_continuity_x(lambda,z)-lm}
    \|\bz-\bz'\|\geq&~\sigma\|\bx(\blambda,\bz)-\bx(\blambda,\bz')\|.
\end{aligned}
\end{equation}
\end{lemma}

\iffalse
\begin{proof}
Let $f(\bx,\bz)=f(\bx)+\frac{p}{2}\|\bx-\bz\|^2$. Then we have
\begin{align*}
    f(\bx(\bz),\bz')-f(\bx(\bz'),\bz')
    =&~(f(\bx(\bz),\bz)-f(\bx(\bz'),\bz))-(f(\bx(\bz'),\bz')-f(\bx(\bz'),\bz))\\
    &+(f(\bx(\bz),\bz')-f(\bx(\bz),\bz))\\
    =&~(f(\bx(\bz),\bz)-f(\bx(\bz'),\bz))-\frac{p}{2}(\|\bx(\bz')-\bz'\|^2-\|\bx(\bz')-\bz\|^2)\\
    &+\frac{p}{2}(\|\bx(\bz)-\bz'\|^2-\|\bx(\bz)-\bz\|^2)\\
    =&~(f(\bx(\bz),\bz)-f(\bx(\bz'),\bz))+p\langle\bx(\bz')-\bx(\bz),\bz'-\bz\rangle\\
    \leq&~-\frac{p-L}{2}\|\bx(\bz')-\bx(\bz)\|^2+p\langle\bx(\bz')-\bx(\bz),\bz'-\bz\rangle
\end{align*}
where the last inequality is from the $(p-L)$-strong convexity of problem~\eqref{eq:x(z)} and~\cite[Theorem 2.1.8]{nesterov2018lectures}. On the other hand, again by the strong convexity, we have
\begin{align*}
    f(\bx(\bz),\bz')-f(\bx(\bz'),\bz')\geq\frac{p-L}{2}\|\bx(\bz)-\bx(\bz')\|^2.
\end{align*}
Hence, we have
\begin{align*}
    -(p-L)\|\bx(\bz)-\bx(\bz')\|^2+p\langle\bx(\bz')-\bx(\bz),\bz'-\bz\rangle\geq0,
\end{align*}
which by the Cauchy-Schwarz inequality further implies the first inequality in~\eqref{eq:Lipschitz_continuity_x(z)}.

Recall from $p>L$ that $\mathcal{L}_p$ in~\eqref{eq:imela} is $(p-L)$-strongly convex in $\bx$ over $\X$. Then we have
\begin{small}
\begin{align*}
    &~\mathcal{L}_p (\bx(\blambda,\bz),\bz',\blambda)-\mathcal{L}_p (\bx(\blambda,\bz'),\bz',\blambda)\\
    =&~(\mathcal{L}_p (\bx(\blambda,\bz),\bz,\blambda)-\mathcal{L}_p (\bx(\blambda,\bz'),\bz,\blambda))-(\mathcal{L}_p (\bx(\blambda,\bz'),\bz',\blambda)-\mathcal{L}_p (\bx(\blambda,\bz'),\bz,\blambda))\\
    &+(\mathcal{L}_p (\bx(\blambda,\bz),\bz',\blambda)-\mathcal{L}_p (\bx(\blambda,\bz),\bz,\blambda))\\
    =&~(\mathcal{L}_p (\bx(\blambda,\bz),\bz,\blambda)-\mathcal{L}_p (\bx(\blambda,\bz'),\bz,\blambda))-\frac{p}{2}(\|\bx(\blambda,\bz')-\bz'\|^2-\|\bx(\blambda,\bz')-\bz\|^2)\\
    &+\frac{p}{2}(\|\bx(\blambda,\bz)-\bz'\|^2-\|\bx(\blambda,\bz)-\bz\|^2)\\
    =&~(\mathcal{L}_p (\bx(\blambda,\bz),\bz,\blambda)-\mathcal{L}_p (\bx(\blambda,\bz'),\bz,\blambda))+p\langle\bx(\blambda,\bz')-\bx(\blambda,\bz),\bz'-\bz\rangle\\
    \leq&~-\frac{p-L}{2}\|\bx(\blambda,\bz')-\bx(\blambda,\bz)\|^2+p\langle\bx(\blambda,\bz')-\bx(\blambda,\bz),\bz'-\bz\rangle
\end{align*}
\end{small}where the last inequality is from the $(p-L)$-strong convexity of problem~\eqref{eq:d(lambda,z)} and~\cite[Theorem 2.1.8]{nesterov2018lectures}. On the other hand, again by the strong convexity, we have
\begin{align*}
    \mathcal{L}_p (\bx(\blambda,\bz),\bz',\blambda)-\mathcal{L}_p (\bx(\blambda,\bz'),\bz',\blambda)\geq\frac{p-L}{2}\|\bx(\blambda,\bz)-\bx(\blambda,\bz')\|^2.
\end{align*}
Hence, we have
\begin{align*}
    -(p-L)\|\bx(\blambda,\bz)-\bx(\blambda,\bz')\|^2+p\langle\bx(\blambda,\bz')-\bx(\blambda,\bz),\bz'-\bz\rangle\geq0,
\end{align*}
which by the Cauchy-Schwarz inequality further implies the second inequality in~\eqref{eq:Lipschitz_continuity_x(z)}.
\end{proof}
\fi

\iffalse
Similar to~\cite[Lemma 3.10]{zhang2020proximal} and~\cite[Lemma B.1]{zhang2022global}, we immediately obtain the following primal error bounds by setting $\blambda=\blambda^{(t)}$, $\bz=\bz^{(t)}$ and $\bz'=\bz^{(t+1)}$ in Lemma~\ref{lmm:Lipschitz_continuity_lm}.
\begin{lemma}
\label{lmm:primal_errorbound}
The sequence $\{\bx^{(t)},\blambda^{(t)},\bz^{(t)}\}_{t\geq0}$ generated by Algorithm~\ref{alg:imela} satisfies that
\begin{align}
\label{eq:Lipschitz_continuity_x(z)_t-lm}
    \|\bz^{(t+1)}-\bz^{(t)}\|\geq&~\sigma\|\bx(\bz^{(t+1)})-\bx(\bz^{(t)})\|,\\
    \label{eq:Lipschitz_continuity_x(lambda,z)_t-lm}
    \|\bz^{(t+1)}-\bz^{(t)}\|\geq&~\sigma\|\bx(\blambda^{(t+1)},\bz^{(t+1)})-\bx(\blambda^{(t+1)},\bz^{(t)})\|
\end{align}
with $\sigma=\frac{p-L}{p}$.
\end{lemma}
\fi

Similar to the analysis in~\cite{zhang2020proximal,zhang2022global,zhang2022iteration}, consider the potential function
\begin{align}
\label{eq:phi^t_lm}
    \phi^t= \phi(\bx^{(t)},\bz^{(t)},\blambda^{(t)}) :=\mathcal{L}_p (\bx^{(t)},\bz^{(t)},\blambda^{(t)})-2d(\blambda^{(t)},\bz^{(t)})+2v(\bz^{(t)}).
\end{align}
By Assumption~\ref{assume:draft}B, \eqref{eq:v(z)},  
\eqref{eq:d(lambda,z)} and \eqref{eq:str-dual}, it is easy to see that
%\begin{small}
\begin{align}
    \nonumber
    \phi^t=&~
    (\mathcal{L}_p (\bx^{(t)},\bz^{(t)},\blambda^{(t)})-d(\blambda^{(t)},\bz^{(t)}))+(v(\bz^{(t)})-d(\blambda^{(t)},\bz^{(t)}))+v(\bz^{(t)})\\
    \label{eq:phi^t_lowerbound_lm}
    \geq
    &~
    v(\bz^{(t)})\geq \underline{f}.
\end{align}
%\end{small}
Then, we present three descent lemmas similar to the ones in~\cite{zhang2020proximal,zhang2022global,zhang2022iteration}. 
\begin{lemma}
\label{lmm:primal_descent_lm}
The sequence $\{(\bx^{(t)},\blambda^{(t)},\bz^{(t)})\}_{t\geq0}$ generated by Algorithm~\ref{alg:imela} satisfies that
%\begin{small}
\begin{align}
    \nonumber
    &~\mathcal{L}_p (\bx^{(t)},\bz^{(t)},\blambda^{(t)})-\mathcal{L}_p (\bx^{(t+1)},\bz^{(t+1)},\blambda^{(t+1)})\\
    \nonumber
    \geq&~\frac{p-L}{2}\|\bx^{(t)}-\tilde{\bx}^{(t+1)}\|^2 -\langle\blambda^{(t+1)}-\blambda^{(t)},\bg(\bx^{(t)}) \rangle\\
    \label{eq:primal_descent_lm}
    &+\frac{p}{2}(2/\theta_t-1)\|\bz^{(t)}-\bz^{(t+1)}\|^2-\frac{\epsilon_t^2}{2(p-L)}.
\end{align}
%\end{small}
\end{lemma}
\begin{proof}
According to the definition of $\mathcal{L}_p$ in \eqref{eq:imela}, %updating scheme of $\blambda^{(t)}$, 
we have
\begin{align}
    \label{eq:primal_descent_lm_lambda}
    \mathcal{L}_p (\bx^{(t)},\bz^{(t)},\blambda^{(t)})-\mathcal{L}_p (\bx^{(t)},\bz^{(t)},\blambda^{(t+1)})
    =-\langle\blambda^{(t+1)}-\blambda^{(t)},\bg(\bx^{(t)})\rangle.
\end{align}
By the $(p-L)$-strong convexity of $\mathcal{L}_p$ in $\bx$, we have
%\begin{footnotesize}
\begin{align}
\label{eq:primal_descent_lm_x}
    \mathcal{L}_p (\bx^{(t)},\bz^{(t)},\blambda^{(t+1)}) -\mathcal{L}_p (\tilde{\bx}^{(t+1)},\bz^{(t)},\blambda^{(t+1)})
    \geq\frac{p-L}{2}\|\bx^{(t)}-\tilde{\bx}^{(t+1)}\|^2.
\end{align}
Moreover, we have from Step 6 of Algorithm~\ref{alg:imela} that
\begin{align}
    \nonumber
    &~\mathcal{L}_p (\bx^{(t+1)},\bz^{(t)},\blambda^{(t+1)})-\mathcal{L}_p (\bx^{(t+1)},\bz^{(t+1)},\blambda^{(t+1)})\\
    \nonumber
    =&~\frac{p}{2}(\|\bx^{(t+1)}-\bz^{(t)}\|^2-\|\bx^{(t+1)}-\bz^{(t+1)}\|^2)\\
    \nonumber
    =&~\frac{p}{2}\langle\bz^{(t+1)}-\bz^{(t)},(\bx^{(t+1)}-\bz^{(t)})+(\bx^{(t+1)}-\bz^{(t+1)})\rangle\\
    \label{eq:primal_descent_lm_z}
    =&~\frac{p}{2}(2/\theta_t-1)\|\bz^{(t)}-\bz^{(t+1)}\|^2 
    %\geq&~\frac{p}{2\theta_t}\|\bz^{(t)}-\bz^{(t+1)}\|^2
\end{align}
for $\theta_t\leq1$. Combining \eqref{eq:subroutine_optimality_imela_corollary}, \eqref{eq:primal_descent_lm_lambda}, \eqref{eq:primal_descent_lm_x} and \eqref{eq:primal_descent_lm_z}  yields \eqref{eq:primal_descent_lm}.
\end{proof}
\begin{lemma}
\label{lmm:dual_descent}
The sequence $\{(\blambda^{(t)},\bz^{(t)})\}_{t\geq0}$ generated by Algorithm~\ref{alg:imela} satisfies that
\begin{align}
    \nonumber
    d(\blambda^{(t+1)},\bz^{(t+1)})-d(\blambda^{(t)},\bz^{(t)})
    \geq&~\langle\blambda^{(t+1)}-\blambda^{(t)}, \bg(\tilde{\bx}^{(t+1)})\rangle\\
    %\geq&~(y^{(t+1)}-y^{(t)})\cdot g(\bx(y^{(t+1)},\bz^{(t)}))\\
    \label{eq:dual_descent_lm}
    &+\frac{p}{2}\langle\bz^{(t+1)}-\bz^{(t)},\bz^{(t+1)}+\bz^{(t)}-2\bx(\blambda^{(t+1)},\bz^{(t+1)})\rangle.
\end{align}
\end{lemma}
\begin{proof}
By the definition of $\mathcal{L}_p$ in \eqref{eq:imela} and the definition of $d$ in \eqref{eq:d(lambda,z)}, we have
\begin{align*}
    d(\blambda^{(t+1)},\bz^{(t)})-d(\blambda^{(t)},\bz^{(t)})
    =&~\mathcal{L}_p (\tilde{\bx}^{(t+1)},\bz^{(t)},\blambda^{(t+1)})-\mathcal{L}_p (\bx(\blambda^{(t)},\bz^{(t)}),\bz^{(t)},\blambda^{(t)})\\
    \geq&~\mathcal{L}_p (\tilde{\bx}^{(t+1)},\bz^{(t)},\blambda^{(t+1)})-\mathcal{L}_p(\tilde{\bx}^{(t+1)},\bz^{(t)},\blambda^{(t)})\\
    =&~\langle\blambda^{(t+1)}-\blambda^{(t)},\bg(\tilde{\bx}^{(t+1)})\rangle.
\end{align*}

Next, using the same technique, we have
\begin{align*}
    &~ d(\blambda^{(t+1)},\bz^{(t+1)})-d(\blambda^{(t+1)},\bz^{(t)})\\
    =&~\mathcal{L}_p (\bx(\blambda^{(t+1)},\bz^{(t+1)}),\bz^{(t+1)},\blambda^{(t+1)})-\mathcal{L}_p(\tilde{\bx}^{(t+1)},\bz^{(t)},\blambda^{(t+1)})\\
    \geq&~\mathcal{L}_p (\bx(\blambda^{(t+1)},\bz^{(t+1)}),\bz^{(t+1)},\blambda^{(t+1)})-\mathcal{L}_p(\bx(\blambda^{(t+1)},\bz^{(t+1)}),\bz^{(t)},\blambda^{(t+1)})\\
    =&~\frac{p}{2} (\|\bx(\blambda^{(t+1)},\bz^{(t+1)})-\bz^{(t+1)}\|^2-\|\bx(\blambda^{(t+1)},\bz^{(t+1)})-\bz^{(t)}\|^2)\\
    =&~\frac{p}{2} \langle\bz^{(t+1)}-\bz^{(t)},\bz^{(t+1)}+\bz^{(t)}-2\bx(\blambda^{(t+1)},\bz^{(t+1)})\rangle.
\end{align*}
Combining the above inequalities yields~\eqref{eq:dual_descent_lm}.
\end{proof}
\begin{lemma}
\label{lmm:proximal_descent_lm}
The sequence $\{\bz^{(t)}\}_{t\geq0}$ generated by Algorithm~\ref{alg:imela} satisfies that
%\begin{small}
\begin{align}
\label{eq:proximal_descent_lm}
    v(\bz^{(t+1)})-v(\bz^{(t)})
    \leq p\langle\bz^{(t+1)}-\bz^{(t)},\bz^{(t)}-\bx(\bz^{(t)})\rangle
    +\frac{p(1/\sigma+1)}{2}\|\bz^{(t)}-\bz^{(t+1)}\|^2.
\end{align}
%\end{small}
\end{lemma}
\begin{proof}
According to \eqref{eq:nablav} and Lemma~\ref{lmm:Lipschitz_continuity_lm}, for any $\bz$ and $\bz'$ in $\X$, it holds that
\begin{align*}
    \|\nabla v(\bz)-\nabla v(\bz')\|
    =&~\|p(\bz-\bx(\bz))-p(\bz'-\bx(\bz'))\|\\
    \leq&~p(\|\bz-\bz'\|+\|\bx(\bz)-\bx(\bz')\|)
    \leq p(1/\sigma+1)\|\bz-\bz'\|,
\end{align*}
meaning that $v(\bz)$ is $p(1/\sigma+1)$-smooth, which implies \eqref{eq:proximal_descent_lm}.
%By the descent lemma (see, e.g.~\cite[Theorem 2.1.5]{nesterov2018lectures}), we get~\eqref{eq:proximal_descent_lm}.
\end{proof}

Following the analysis in~\cite{zhang2020proximal,zhang2022global,zhang2022iteration}, we then use the above three descent lemmas to show the decrease of the potential function $\phi^t$ in \eqref{eq:phi^t_lm}. 
\begin{proposition}
\label{thm:difference_phi_lm}
Suppose that in Algorithm~\ref{alg:imela}, 
\begin{align*}
    \tau_t=\tau=\frac{p-L}{4B_g^2}\text{ and }\theta_t=\theta \le\frac{p-L}{18p}, ~\forall\,t\geq0.
\end{align*}
%for $t\geq0$. 
The sequence $\{(\bx^{(t)},\blambda^{(t)},\bz^{(t)})\}_{t\geq0}$ generated by Algorithm~\ref{alg:imela} satisfies that
\begin{align}
\nonumber
    \phi^t-\phi^{t+1}\geq&~\frac{p-L}{4}\|\bx^{(t)}-\tilde{\bx}^{(t+1)}\|^2-\frac{\epsilon_t^2}{2(p-L)}-6p\theta\|\tilde{\bx}^{(t+1)}-\bx(\bz^{(t)})\|^2\\
    \label{eq:difference_phi_lm}
    &+\frac{p}{6\theta}\|\bz^{(t)}-\bz^{(t+1)}\|^2
    +\tau\cdot \textup{dist}^2\big( \bg(\tilde{\bx}^{(t+1)}), \mathcal{N}_{\mathbb{R}_+^m}(\blambda^{(t+1)})\big).
\end{align}
\end{proposition}
\begin{proof}
Applying \eqref{eq:primal_descent_lm}, \eqref{eq:dual_descent_lm} and \eqref{eq:proximal_descent_lm} to \eqref{eq:phi^t_lm} and rearranging terms, we have
\begin{small}
\begin{align}
    \nonumber
    \phi^t-\phi^{t+1}=&~\mathcal{L}_p (\bx^{(t)},\bz^{(t)},\blambda^{(t)})-\mathcal{L}_p (\bx^{(t+1)},\bz^{(t+1)},\blambda^{(t+1)})\\
    \nonumber
    &-2(d(\blambda^{(t)},\bz^{(t)})-d(\blambda^{(t+1)},\bz^{(t+1)})) +2(v(\bz^{(t)})-v(\bz^{(t+1)}))\\
    \nonumber
    % \geq&~\frac{p-L}{2}\|\bx^{(t)}-\bx(\blambda^{(t+1)},\bz^{(t)})\|^2-\frac{\epsilon_t^2}{2(p-L)}
    %\nonumber
    % +\frac{p}{2}(2/\theta_t-1)\|\bz^{(t)}-\bz^{(t+1)}\|^2\\
    % \nonumber
    % &-\langle\blambda^{(t+1)}-\blambda^{(t)},\bg(\bx^{(t)})\rangle +2\langle\blambda^{(t+1)}-\blambda^{(t)}, \bg(\bx(\blambda^{(t+1)},\bz^{(t)}))\rangle\\
    % \nonumber
    % &+p\langle\bz^{(t+1)}-\bz^{(t)},\bz^{(t+1)}+\bz^{(t)}-2\bx(\blambda^{(t+1)},\bz^{(t+1)})\rangle\\ \nonumber
    % &+2p\langle\bz^{(t+1)}-\bz^{(t)},\bx(\bz^{(t)})-\bz^{(t)}\rangle-p(1/\sigma+1)\|\bz^{(t)}-\bz^{(t+1)}\|^2\\ \nonumber
    % =&~
    \geq&~\frac{p-L}{2}\|\bx^{(t)}-\tilde{\bx}^{(t+1)}\|^2-\frac{\epsilon_t^2}{2(p-L)}+\frac{p}{2}(2/\theta_t-1)\|\bz^{(t)}-\bz^{(t+1)}\|^2\\
    \nonumber
    &-\langle\blambda^{(t+1)}-\blambda^{(t)},\bg(\bx^{(t)})\rangle +2\langle\blambda^{(t+1)}-\blambda^{(t)}, \bg(\tilde{\bx}^{(t+1)})\rangle\\
    \nonumber
    &+p\langle\bz^{(t+1)}-\bz^{(t)},\bz^{(t+1)}-\bz^{(t)}-2(\bx(\blambda^{(t+1)},\bz^{(t+1)})-\bx(\bz^{(t)}))\rangle\\
    \label{eq:difference_phi_descent_result}
    &-p(1/\sigma+1)\|\bz^{(t)}-\bz^{(t+1)}\|^2.
\end{align}
\end{small}For any $\alpha>0$, by Young's inequality, it holds that
\begin{align}
    \label{eq:difference_phi_alpha}
    2\langle\bz^{(t+1)}-\bz^{(t)},\tilde{\bx}^{(t+1)}-\bx(\bz^{(t)})\rangle
    \leq\|\bz^{(t+1)}-\bz^{(t)}\|^2/\alpha+\alpha\|\tilde{\bx}^{(t+1)}-\bx(\bz^{(t)})\|^2.
\end{align}
By Cauchy-Schwartz inequality and \eqref{eq:Lipschitz_continuity_x(z)}, we have 
\begin{align}
\nonumber
    &~2\langle\bz^{(t+1)}-\bz^{(t)},\bx(\blambda^{(t+1)},\bz^{(t+1)})-\tilde{\bx}^{(t+1)}\rangle\\
    \label{eq:difference_phi_sigma}
    \leq&~2\|\bz^{(t+1)}-\bz^{(t)}\|\cdot\|\bx(\blambda^{(t+1)},\bz^{(t+1)})-\tilde{\bx}^{(t+1)}\|
    \leq(2/\sigma)\cdot \|\bz^{(t+1)}-\bz^{(t)}\|^2.
\end{align}
%where the second inequality is by Lemma~\ref{lmm:Lipschitz_continuity_lm}. 
Applying \eqref{eq:difference_phi_alpha} and \eqref{eq:difference_phi_sigma} to \eqref{eq:difference_phi_descent_result}, we have
%\begin{small}
\begin{align}
    \nonumber
    \phi^t-\phi^{t+1}
    % %\nonumber
    % \geq&~\frac{p-L}{2}\|\bx^{(t)}-\bx(\blambda^{(t+1)},\bz^{(t)})\|^2-\frac{2}{K}\epsilon_t^2-p\alpha\|\bx(\blambda^{(t+1)},\bz^{(t)})-\bx(\bz^{(t)})\|^2\\
    % \nonumber
    % &+p\left(\frac{1}{\theta_t}-1/2+1 -1/\alpha-2/\sigma-(1/\sigma+1)\right)\|\bz^{(t)}-\bz^{(t+1)}\|^2\\
    % \nonumber
    % &+2\left\langle\blambda^{(t+1)}-\blambda^{(t)}, \bg(\bx(\blambda^{(t+1)},\bz^{(t)}))\right\rangle-\langle\blambda^{(t+1)}-\blambda^{(t)},\bg(\bx^{(t)})\rangle\\
    \geq&~\frac{p-L}{2}\|\bx^{(t)}-\tilde{\bx}^{(t+1)}\|^2-\frac{\epsilon_t^2}{2(p-L)}-p\alpha\|\tilde{\bx}^{(t+1)}-\bx(\bz^{(t)})\|^2\\
    \nonumber
    &+p(1/\theta_t-{3/2}-1/\alpha-3/\sigma)\cdot\|\bz^{(t)}-\bz^{(t+1)}\|^2\\
    \label{eq:difference_phi_descent_terms}
    &+2\langle\blambda^{(t+1)}-\blambda^{(t)}, \bg(\tilde{\bx}^{(t+1)})-\bg(\bx^{(t)})\rangle+\langle\blambda^{(t+1)}-\blambda^{(t)},\bg(\bx^{(t)})\rangle.
\end{align}
%\end{small}
According to the updating equation of $\blambda^{(t+1)}$ in Step 4 of Algorithm~\ref{alg:imela}, we have 
\begin{align}
\label{eq:imela_g_lambda_norm}
    -\tau_t \langle\blambda^{(t)},\bg(\bx^{(t)})\rangle \geq -\tau_t\langle\blambda^{(t+1)},\bg(\bx^{(t)})\rangle +\|\blambda^{(t+1)}-\blambda^{(t)}\|^2,&\\
    \label{eq:imela_g_lambda_cone}
    -\blambda^{(t+1)} +\blambda^{(t)}+\tau_t\bg(\bx^{(t)})\in \mathcal{N}_{\mathbb{R}_+^m}(\blambda^{(t+1)}).&
\end{align}
Therefore, we have
%\begin{small}
\begin{align}
    \nonumber
    &~2\langle\blambda^{(t+1)}-\blambda^{(t)}, \bg(\tilde{\bx}^{(t+1)})-\bg(\bx^{(t)})\rangle+\langle\blambda^{(t+1)}-\blambda^{(t)},\bg(\bx^{(t)})\rangle\\  \nonumber
    \geq&~2\langle\blambda^{(t+1)}-\blambda^{(t)}, \bg(\tilde{\bx}^{(t+1)})-\bg(\bx^{(t)})\rangle+\tau_t^{-1}\cdot\|\blambda^{(t+1)}-\blambda^{(t)}\|^2\\\nonumber
    =&-\tau_t\|\bg(\tilde{\bx}^{(t+1)})-\bg(\bx^{(t)})\|^2+ \tau_t^{-1}\|\blambda^{(t+1)}-\blambda^{(t)}+\tau_t(\bg(\tilde{\bx}^{(t+1)})-\bg(\bx^{(t)}))\|^2\\
    \nonumber
    \geq&-\tau_t\|\bg(\tilde{\bx}^{(t+1)})-\bg(\bx^{(t)})\|^2+\tau_t^{-1}\cdot\text{dist}^2\big(\tau_t\bg(\tilde{\bx}^{(t+1)}), \mathcal{N}_{\mathbb{R}_+^m}(\blambda^{(t+1)})\big)\\
    \label{eq:difference_phi_result_tau_lm}
    =&-\tau_t\|\bg(\tilde{\bx}^{(t+1)})-\bg(\bx^{(t)})\|^2+\tau_t\cdot\text{dist}^2\big(\bg(\tilde{\bx}^{(t+1)}), \mathcal{N}_{\mathbb{R}_+^m}(\blambda^{(t+1)})\big),
\end{align}
%\end{small}
where the two inequalities are by \eqref{eq:imela_g_lambda_norm} and \eqref{eq:imela_g_lambda_cone}, respectively, and the last equality is from the structure of $\mathcal{N}_{\mathbb{R}_+^m}(\blambda^{(t+1)})$. 

Applying \eqref{eq:difference_phi_result_tau_lm} and Assumption~\ref{assume:draft}E to \eqref{eq:difference_phi_descent_terms}, we have 
%\begin{small}
\begin{align}
\nonumber
    \phi^t-\phi^{t+1}
    \nonumber
    \geq&\left(\frac{p-L}{2}-\tau_tB_g^2\right)\|\bx^{(t)}-\tilde{\bx}^{(t+1)}\|^2-\frac{\epsilon_t^2}{2(p-L)}-p\alpha\|\tilde{\bx}^{(t+1)}-\bx(\bz^{(t)})\|^2\\
    \nonumber
    &+p(1/\theta_t-{3/2}-1/\alpha-3/\sigma)\cdot\|\bz^{(t)}-\bz^{(t+1)}\|^2\\
    \label{eq:difference_phi_descent_coeffs}
    &+\tau_t\cdot \text{dist}^2\big(\bg(\tilde{\bx}^{(t+1)}), \mathcal{N}_{\mathbb{R}_+^m}(\blambda^{(t+1)})\big).
\end{align}
%\end{small}
Let $\alpha=6\theta$. Recall that $\theta_t=\theta \leq \frac{p-L}{18p}<\frac{1}{18}$ and $\sigma=\frac{p-L}{p}$. We have
\begin{align}
\label{eq:difference_phi_theta_coefficient}
   %\textstyle   
   \frac{1}{\theta_t}-{3/2}-1/\alpha-3/\sigma\geq\frac{1}{2\theta_t}-1/\alpha-3/\sigma=\frac{1}{3\theta}-\frac{3p}{p-L}\ge\frac{1}{6\theta}.
\end{align}
Recall that $\tau_t=\tau=\frac{p-L}{4B_g^2}$. We have $\frac{p-L}{2}-\tau_tB_g^2=\frac{p-L}{4}$. 
% \begin{align}
% \label{eq:difference_phi_tau_coefficient}
%     \frac{p-L}{2}-\tau_tB_g^2=\frac{p-L}{4}.
% \end{align}
Applying this equation, 
\eqref{eq:difference_phi_theta_coefficient}, % \eqref{eq:difference_phi_tau_coefficient} 
and $\alpha=6\theta$ to \eqref{eq:difference_phi_descent_coeffs} leads to \eqref{eq:difference_phi_lm}.
\end{proof}

According to Proposition~\ref{thm:difference_phi_lm}, we notice that to ensure a sufficient decrease of $\phi_t$, we need to further eliminate the negative term $-6p\theta\|\tilde{\bx}^{(t+1)}-\bx(\bz^{(t)})\|^2$ in \eqref{eq:difference_phi_lm}. To do so, we will employ the local error bound condition assumed in Assumption~\ref{assume:draft_regular_cond_lm}. 
\begin{lemma}
\label{thm:nearKKT_impliesnearx} 
%Suppose that Assumption~\ref{assume:draft_regular_cond_lm} holds. Let $\delta$ be from  Assumption~\ref{assume:draft_regular_cond_lm} and $M_{\blambda}$ defined as in \eqref{eq:bound_lambda_lm}.
For any $\delta' >0$, there must exist $R(\delta') > 0$ such that, if 
\begin{equation}
\label{eq:regular_cond_delta_near_lm}
\begin{aligned}  
     \textup{dist}^2\big(-\nabla f(\tilde{\bx}) - \nabla\bg(\tilde{\bx})\blambda, \N_\X(\tilde{\bx})\big) 
    +\textup{dist}^2\big( \bg(\tilde{\bx}), \mathcal{N}_{\mathbb{R}_+^m}(\blambda)\big)  \leq R^2(\delta')
\end{aligned} 
\end{equation}
for some $\tilde\bx\in\X$ and some $\blambda\geq\bzero$ satisfying $\|\blambda\|\leq M_{\blambda}$, there must exist $\bx^*\in\X^*$ such that $\|\tilde\bx-\bx^*\|\leq\delta'$ and $\textup{supp}(\blambda)\subset J_g(\bx^*)\text{ and }J_A(\tilde\bx)\subset J_A(\bx^*)$. 
\end{lemma}
\begin{proof}
We prove this by contradiction. Suppose that the claim does not hold. Then there exists a sequence of $\{(\tilde{\bx}^{(k)},\tilde{\blambda}^{(k)})\}_{k\geq1}\subset\X\times\mathbb{R}_+^m$, $\|\tilde{\blambda}^{(k)}\|\leq M_{\blambda}$, such that
%\begin{small}
\begin{align}
\label{eq:regular_cond_delta_near_lm_claim}
    \text{dist}^2\big(-\nabla f(\tilde{\bx}^{(k)}) - \nabla\bg(\tilde{\bx}^{(k)})\tilde{\blambda}^{(k)}, \N_\X(\tilde{\bx}^{(k)})\big) 
    +\text{dist}^2\big( \bg(\tilde{\bx}^{(k)}), \mathcal{N}_{\mathbb{R}_+^m}(\tilde{\blambda}^{(k)})\big)
    \leq 1/k,
\end{align}
%\end{small}
but for any $k$, there is no $\bx^*\in\X^*$ such that $\|\tilde\bx^{(k)}-\bx^*\|\leq\delta'$ and $\textup{supp}(\tilde\blambda^{(k)})\subset J_g(\bx^*)$ and $J_A(\tilde\bx^{(k)})\subset J_A(\bx^*)$.%$\text{supp}(\tilde{\blambda}^{(k)})\not\subset J_g(\bx^*)\text{ or }J_A(\tilde{\bx}^{(k)})\not\subset J_A(\bx^*)$ for any $k$ and any $\bx^*\in\X^*$
%satisfying $\|\tilde{\bx}^{(k)}-\bx^*\|\leq\delta$. 

Due to the compactness of $\X$ (see Assumption~\ref{assume:draft}A) and the set $\{\blambda\in\mathbb{R}_+^m: \|\blambda\|\leq M_{\blambda}\}$, %and the finiteness of $[m]$ and $[l]$, 
by passing to a subsequence if necessary, there exists some pair $(\tilde{\bx}^*, \tilde{\blambda}^*)\in\X\times\mathbb{R}_+^m$, $\|\tilde{\blambda}^*\|\leq M_{\blambda}$, such that %$\|\tilde{\bx}^{(k)}-\tilde{\bx}^*\|\leq\delta$ for any $k\geq1$, and 
$(\tilde{\bx}^{(k)},\tilde{\blambda}^{(k)})\rightarrow(\tilde{\bx}^*,\tilde{\blambda}^*)$ as $k\rightarrow\infty$. %with $\text{supp}(\tilde{\blambda}^{(k)})= \tilde{I}_g$ and $J_A(\tilde{\bx}^{(k)})=\tilde{I}_A$ for some $\tilde{I}_g\subset [m]$ and $\tilde{I}_A\subset [l]$.  
In addition, by the continuity of $\nabla f(\cdot)$ and $\nabla\bg(\cdot)$ and the outer semi-continuity of $\N_\X(\cdot)$, it follows from \eqref{eq:regular_cond_delta_near_lm_claim} that 
\begin{align*}
    \text{dist}^2\big(-\nabla f(\tilde{\bx}^*) - \nabla\bg(\tilde{\bx}^*)\tilde{\blambda}^*, \N_\X(\tilde{\bx}^*)\big) 
    +\text{dist}^2\big( \bg(\tilde{\bx}^*), \mathcal{N}_{\mathbb{R}_+^m}(\tilde{\blambda}^*)\big)=0,
\end{align*}
which easily implies that $\tilde{\bx}^*\in\X^*$. Also, by the continuity of $\bA(\cdot)-\bb$, if $[\bA\tilde{\bx}^*-\bb]_j < 0$, then $[\bA\tilde{\bx}^{(k)}-\bb]_j < 0$ as well when $k$ is large enough, and thus $J_A(\tilde\bx^{(k)})\subset J_A(\tilde\bx^*)$. Moreover, let $\tilde{I}_g^{(k)}=\text{supp}(\tilde{\blambda}^{(k)})$. Then 
\begin{align*}
    \textstyle
    \sum_{i\in \tilde{I}_g^{(k)}} |g_i(\tilde{\bx}^{(k)})|^2 + \sum_{i\in [m]\backslash\tilde{I}_g^{(k)}} \big([g_i(\tilde{\bx}^{(k)})]_+ \big)^2 
    =\text{dist}^2\big( \bg(\tilde{\bx}^{(k)}), \mathcal{N}_{\mathbb{R}_+^m}(\tilde{\blambda}^{(k)})\big)
    \leq 1/k.
\end{align*}
Now let $k$ be large enough such that for any $i\not\in J_g(\tilde{\bx}^*)$, i.e., $g_i(\tilde{\bx}^*) < 0$, it holds that $g_i(\tilde{\bx}^{(k)}) \leq \frac{1}{2}g_i(\tilde{\bx}^*)$ and $|g_i(\tilde{\bx}^*)|^2 > \frac{4}{k}$. This observation together with the above inequality implies $\text{supp}(\tilde{\blambda}^{(k)})=\tilde{I}_g^{(k)}\subset J_g(\tilde{\bx}^*)$. Hence, we find $\tilde{\bx}^*\in\X^*$ such that $\|\tilde\bx^{(k)}-\tilde\bx^*\|\leq\delta'$ and $\textup{supp}(\tilde\blambda^{(k)})\subset J_g(\tilde\bx^*)\text{ and }J_A(\tilde\bx^{(k)})\subset J_A(\tilde\bx^*)$ when $k$ is large enough. This contradicts to the assumption. Therefore, the conclusion holds.
\end{proof}


Equipped with Lemma~\ref{thm:nearKKT_impliesnearx}, we are able to upper bound $\|\tilde{\bx}^{(t+1)}-\bx(\bz^{(t)})\|^2$ in \eqref{eq:difference_phi_lm} using %$\epsilon_t$ and 
$\text{dist}^2( \bg(\tilde{\bx}^{(t+1)}, \mathcal{N}_{\mathbb{R}_+^m}(\blambda^{(t+1)}))$, when $\tilde{\bx}^{(t+1)}$ is a near-KKT point. %the latter %$\text{dist}^2\left( \bg(\bx(\blambda^{(t+1)},\bz^{(t)})), \mathcal{N}_{\mathbb{R}_+^m}(\blambda^{(t+1)})\right)$ 
%is small enough.
\begin{proposition}
\label{thm:regularity_lm} 
Suppose that Assumption~\ref{assume:draft_regular_cond_lm} holds. %Let $\tilde{\bx}^{(t+1)} = \bx(\blambda^{(t+1)},\bz^{(t)})$. 
Under the same conditions as in Proposition~\ref{thm:difference_phi_lm}, if
\begin{equation}
\label{eq:regular_cond_KKT_near_R}
\begin{aligned}  
     \textup{dist}^2\big(-\nabla f(\tilde{\bx}^{(t+1)}) - \nabla\bg(\tilde{\bx}^{(t+1)})\blambda^{(t+1)}, \N_\X(\tilde{\bx}^{(t+1)})\big) &\\
    +\;\textup{dist}^2\big( \bg(\tilde{\bx}^{(t+1)}), \mathcal{N}_{\mathbb{R}_+^m}(\blambda^{(t+1)})\big)  &\leq R^2(\delta),
\end{aligned} 
\end{equation}
where $\delta$ is from Assumption~\ref{assume:draft_regular_cond_lm} and $R(\cdot)$ is from Lemma~\ref{thm:nearKKT_impliesnearx}, then 
\begin{align}
\label{eq:regularity_result_lm}
%\nonumber  
    \|\bx(\bz^{(t)})-\tilde{\bx}^{(t+1)}\|^2 \leq&~\frac{\gamma K}{p-L}\textup{dist}^2\big( \bg(\tilde{\bx}^{(t+1)}, \mathcal{N}_{\mathbb{R}_+^m}(\blambda^{(t+1)})\big).%\\
    %
    % &~+\frac{2}{(p-L)^2}\left(\frac{2\gamma KB_g^2}{p-L} +1\right)\cdot\epsilon_t^2.
\end{align}
\end{proposition}
\begin{proof}
From \eqref{eq:d(lambda,z)}, we have
$\tilde{\bxi}^{(t+1)}:=-\nabla_\bx\mathcal{L}_p (\tilde{\bx}^{(t+1)},\bz^{(t)},\blambda^{(t+1)})\in \N_\X(\tilde{\bx}^{(t+1)}).$
Since $\mathcal{L}_p$ is $K$-smooth in $\bx$, for any $\bx\in\X$, we have 
%\begin{small}
\begin{align}
\nonumber
    &~\mathcal{L}_p (\bx,\bz^{(t)},\blambda^{(t+1)}) - \mathcal{L}_p (\tilde{\bx}^{(t+1)},\bz^{(t)},\blambda^{(t+1)})\\
    \label{eq:regularity_result_ksi_lm}
    \leq&-\,\langle\tilde{\bxi}^{(t+1)},\bx-\tilde{\bx}^{(t+1)}\rangle
    +\frac{K}{2}\|\bx-\tilde{\bx}^{(t+1)}\|^2.
\end{align}
By the $(p-L)$-strongly convexity of $\mathcal{L}_p$ in $\bx$, we have
\begin{align}
\label{eq:regularity_result_sc_lm}
    \frac{p-L}{2}\|\bx-\tilde{\bx}^{(t+1)}\|^2
    \leq\mathcal{L}_p (\bx,\bz^{(t)},\blambda^{(t+1)})-\mathcal{L}_p (\tilde{\bx}^{(t+1)},\bz^{(t)},\blambda^{(t+1)}).
\end{align} 
Adding \eqref{eq:regularity_result_ksi_lm} and \eqref{eq:regularity_result_sc_lm} with $\bx = \bx(\bz^{(t)})$ shows that, for any $\bx\in\X$,
%\begin{footnotesize}
\begin{align}
\nonumber
    \frac{p-L}{2}\|\bx(\bz^{(t)})-\tilde{\bx}^{(t+1)}\|^2
    \leq&~\mathcal{L}_p (\bx(\bz^{(t)}),\bz^{(t)},\blambda^{(t+1)})-\mathcal{L}_p  (\bx,\bz^{(t)},\blambda^{(t+1)})\\
    \label{eq:regularity_result_ksi_tilde_lm}
    &-\langle\tilde{\bxi}^{(t+1)},\bx-\tilde{\bx}^{(t+1)}\rangle+\frac{K}{2}\|\bx-\tilde{\bx}^{(t+1)}\|^2.
\end{align}

Recall that $\|\blambda^{(t+1)}\|\leq M_{\blambda}$. According to Lemma~\ref{thm:nearKKT_impliesnearx} and \eqref{eq:regular_cond_KKT_near_R}, there must exist $\bx^*\in\X^*$ such that $\|\tilde\bx^{(t+1)}-\bx^*\|\leq\delta$, $I_g^{(t+1)}=\textup{supp}(\blambda^{(t+1)})\subset J_g(\bx^*)$ and $I_A^{(t+1)}=J_A(\tilde\bx^{(t+1)})\subset J_A(\bx^*)$.
Let $\bar{\bx}^{(t+1)}$ be the projection of $\tilde\bx^{(t+1)}$ onto $\mathcal{S}(I_g^{(t+1)}, I_A^{(t+1)})$, so $\textup{dist} (\tilde\bx^{(t+1)},\mathcal{S}(I_g^{(t+1)}, I_{A}^{(t+1)}))=\|\tilde\bx^{(t+1)}-\bar{\bx}^{(t+1)}\|$. As a result, it holds that
\begin{align}
\label{eq:regularity_xbar_g_lm}
    &~g_i(\bar{\bx}^{(t+1)})=0,\;i\in I_g^{(t+1)},\;g_i(\bar{\bx}^{(t+1)})\leq 0,\;i\in[m]\backslash I_g^{(t+1)},\\
\label{eq:regularity_xbar_polyhedral_lm}
    &~[\bA\bar{\bx}^{(t+1)}-\bb]_j=0,\;j\in I_{A}^{(t+1)},\;[\bA\bar{\bx}^{(t+1)}-\bb]_j\leq0,\;j\in[l]\backslash I_{A}^{(t+1)},
\end{align}
%with $I_g^{(t+1)}=\text{supp}(\blambda^{(t+1)})$ and $I_A^{(t+1)}=J_A(\tilde{\bx}^{(t+1)})$, and
By  Assumption~\ref{assume:draft_regular_cond_lm} and the fact that $I_A^{(t+1)}=J_A(\tilde\bx^{(t+1)})$, we have
\begin{align}
\label{eq:regularity_result_norm_dist}
%\nonumber
    \|\bar{\bx}^{(t+1)}-\tilde{\bx}^{(t+1)}\|^2 \leq %&~\gamma\Bigg(\sum_{i\in I_{g}^{(t+1)}} |g_i(\bx^{(t+1)})|^2+	\sum_{i\in[m]\backslash I_{g}^{(t+1)}}( [g_i(\bx^{(t+1)})]_+)^2\\
    % \nonumber
    % &+\sum_{j\in I_{A}^{(t+1)}}\left|[\bA\bx^{(t+1)}-\bb]_j\right|^2 +\sum_{j\in[l]\backslash I_{A}^{(t+1)}} \left(\left[[\bA\bx^{(t+1)}-\bb]_j\right]_+\right)^2\Bigg) \\
    % 
    % =&~
    \gamma\cdot\text{dist}^2 \big(\bg(\tilde{\bx}^{(t+1)}), \mathcal{N}_{\mathbb{R}_+^m}(\blambda^{(t+1)})\big). 
\end{align}

Let $\bA(I_{A}^{(t+1)},:)$ be the matrix formed by the rows of $\bA$ with indices in $I_{A}^{(t+1)}$. 
Since $\tilde{\bxi}^{(t+1)}\in\N_\X(\tilde{\bx}^{(t+1)})$, there exists some $\bw^{(t+1)}\in\mathbb{R}_+^{|I_{A}^{(t+1)}|}$ such that 
%\begin{align*}
    $\tilde{\bxi}^{(t+1)} %=\left[\bA(I_{A}^{(t+1)},:)\right]^\top\bw^{(t+1)},$
    =[\bA(I_{A}^{(t+1)},:)]^\top \bw^{(t+1)},$
%\end{align*}
which, according to \eqref{eq:regularity_xbar_polyhedral_lm}, implies
\begin{align}
\label{eq:regularity_result_ksi_equality}
    \langle\tilde{\bxi}^{(t+1)}, \bar{\bx}^{(t+1)}-\tilde{\bx}^{(t+1)}\rangle=0.
\end{align}
Setting $\bx=\bar{\bx}^{(t+1)}$ in \eqref{eq:regularity_result_ksi_tilde_lm} and applying \eqref{eq:regularity_result_norm_dist} and \eqref{eq:regularity_result_ksi_equality}, %to \eqref{eq:regularity_result_ksi_tilde_lm}, 
we have
%\begin{small}
\begin{align}
    \nonumber
    \frac{p-L}{2}\|\bx(\bz^{(t)})-\tilde{\bx}^{(t+1)}\|^2 
    \leq&~\mathcal{L}_p (\bx(\bz^{(t)}),\bz^{(t)},\blambda^{(t+1)})-\mathcal{L}_p (\bar{\bx}^{(t+1)},\bz^{(t)},\blambda^{(t+1)})\\
    \label{eq:regularity_result_xbar_dist}
    &+\frac{\gamma K}{2}\text{dist}^2\big(\bg(\tilde{\bx}^{(t+1)}), \mathcal{N}_{\mathbb{R}_+^m}(\blambda^{(t+1)})\big).
\end{align}
%\end{small}
By \eqref{eq:x(z)} and \eqref{eq:regularity_xbar_g_lm}, we must have $\lambda_i^{(t+1)}g_i(\bx(\bz^{(t)}))\leq0$ and $\lambda_i^{(t+1)}g_i(\bar{\bx}^{(t+1)})=0$ for $i\in[m]$. This implies
%\begin{footnotesize}
\begin{align}
\nonumber
    &~\mathcal{L}_p (\bx(\bz^{(t)}),\bz^{(t)},\blambda^{(t+1)})-\mathcal{L}_p (\bar{\bx}^{(t+1)},\bz^{(t)},\blambda^{(t+1)})\\
    \label{eq:regularity_result_imela_diff}
    \leq&\left(f(\bx(\bz^{(t)}))+\frac{p}{2}\|\bx(\bz^{(t)})-\bz^{(t)}\|^2\right)-\left(f(\bar{\bx}^{(t+1)})+\frac{p}{2}\|\bar{\bx}^{(t+1)}-\bz^{(t)}\|^2\right)%\\
    %\nonumber
    \leq0,
\end{align}
%\end{footnotesize}
where the last inequality is because $\bar{\bx}^{(t+1)}$ is a feasible solution to the minimization problem in \eqref{eq:x(z)} while
$\bx(\bz^{(t)})$ is its optimal solution. Applying  %\eqref{eq:regularity_result_dist_epsilon_t}  and 
\eqref{eq:regularity_result_imela_diff} to \eqref{eq:regularity_result_xbar_dist} gives the desired result.
\end{proof}

By Proposition~\ref{thm:regularity_lm}, we can bound the negative term $-6p\theta\|\tilde{\bx}^{(t+1)}-\bx(\bz^{(t)})\|^2$ in \eqref{eq:difference_phi_lm} by $\textup{dist}^2( \bg(\tilde{\bx}^{(t+1)}, \mathcal{N}_{\mathbb{R}_+^m}(\blambda^{(t+1)}))$ and choosing an appropriate $\theta$ when \eqref{eq:regular_cond_delta_near_lm} holds. The following bound will be used when \eqref{eq:regular_cond_delta_near_lm} does not hold.

\begin{lemma}
\label{lmm:weak_dual_bound_lm}
%Let $\tilde{\bx}^{(t+1)} = \bx(\blambda^{(t+1)},\bz^{(t)})$. 
With $M_{\blambda}$ defined in \eqref{eq:bound_lambda_lm}, it holds that
\begin{align}
\label{eq:weak_dual_bound_lm}
    \|\tilde{\bx}^{(t+1)} - \bx(\bz^{(t)})\|^2\leq \frac{2M_{\blambda}}{p-L}%\left(M_{\blambda}+\frac{D_\X(B_f+p D_\X)}{\min_{i\in [m]}-g_i(\bx_{\text{feas}})}\right)   
    \textup{dist}\left( \bg(\tilde{\bx}^{(t+1)}, \mathcal{N}_{\mathbb{R}_+^m}(\blambda^{(t+1)})\right). 
\end{align}
\end{lemma}
\begin{proof}
By the $(p-L)$-strong convexity of $\mathcal{L}_p$ in $\bx$, we have 
%\begin{small}
\begin{align*}
    &%\textstyle 
    ~\frac{p-L}{2}\|\bx(\bz^{(t)})-\tilde{\bx}^{(t+1)}\|^2 \leq\mathcal{L}_p (\bx(\bz^{(t)}),\bz^{(t)}, \blambda^{(t+1)}) - \mathcal{L}_p (\tilde{\bx}^{(t+1)},\bz^{(t)},\blambda^{(t+1)}),\\%[1mm]
    &%\textstyle 
    ~\frac{p-L}{2}\|\tilde{\bx}^{(t+1)}-\bx(\bz^{(t)})\|^2\leq\mathcal{L}_p
    (\tilde{\bx}^{(t+1)}, \bz^{(t)}, \blambda(\bz^{(t)})) - \mathcal{L}_p(\bx(\bz^{(t)}),\bz^{(t)}, \blambda(\bz^{(t)})),
\end{align*}
%\end{small}
where $\blambda(\bz^{(t)})$ is a vector of Lagrangian multipliers corresponding to $\bx(\bz^{(t)})$ defined in \eqref{eq:x(z)}. Notice that by the strong duality, $\blambda(\bz^{(t)})\in\argmax_{\blambda\geq\bzero}\mathcal{L}_p(\bx(\bz^{(t)}),\bz^{(t)},\blambda)$. Hence, adding the above two inequalities gives
%\begin{small}
\begin{align}
\nonumber
    %\,\textstyle 
    (p-L)\cdot \|\tilde{\bx}^{(t+1)}- \bx(\bz^{(t)})\|^2
    \nonumber
    \leq&~\mathcal{L}_p (\tilde{\bx}^{(t+1)},\bz^{(t)},\blambda(\bz^{(t)})) - \mathcal{L}_p (\tilde{\bx}^{(t+1)}, \bz^{(t)},\blambda^{(t+1)})\\
    \nonumber
    =&~\langle\blambda(\bz^{(t)})-\blambda^{(t+1)}, \bg(\tilde{\bx}^{(t+1)})\rangle\\
    \nonumber
    \leq&~\textstyle \sum_{i\in\mathrm{supp}(\blambda^{(t+1)})} |\lambda_i(\bz^{(t)}) - \lambda_i^{(t+1)}|\cdot |g_i(\tilde{\bx}^{(t+1)})|\\
    \nonumber
    &+\textstyle \sum_{i\in[m]\backslash \mathrm{supp}(\blambda^{(t+1)})} \lambda_i(\bz^{(t)}) \cdot [g_i(\tilde{\bx}^{(t+1)})]_+\\
    \label{eq:weak_dual_bound_lambda_lm}
    \leq&~\|\blambda(\bz^{(t)}) - \blambda^{(t+1)}\| \cdot \text{dist}\big( \bg(\tilde{\bx}^{(t+1)}), \mathcal{N}_{\mathbb{R}_+^m}(\blambda^{(t+1)})\big),
\end{align}
%\end{small}
where the last inequality follows from the Cauchy-Schwarz inequality.

In addition, by the KKT conditions at $(\bx(\bz^{(t)}),\blambda(\bz^{(t)}))$ in \eqref{eq:x(z)}, we have
%\begin{small}
\begin{align}
\label{eq:x(z)_KKT_stat}
    \textstyle  \bxi:= -\nabla f(\bx(\bz^{(t)})) - \sum_{i=1}^m \lambda_i(\bz^{(t)}) \nabla g_i(\bx(\bz^{(t)})) - p(\bx(\bz^{(t)})-\bz^{(t)}) \in \N_\X(\bx(\bz^{(t)})),&\\
    \label{eq:x(z)_KKT_comple_slack}
\lambda_i(\bz^{(t)}) g_i(\bx(\bz^{(t)})) = 0,\;i\in [m].&
\end{align} 
%\end{small}
Hence, by the convexity of $g_i$'s and $\blambda(\bz^{(t)})\geq\bzero$, it follows that
\begin{align*}
    %&~
    \textstyle\sum_{i=1}^m \lambda_i(\bz^{(t)}) \cdot g_i(\bx_{\text{feas}})%\\
    \quad\geq\;\;\;\; &~\textstyle\sum_{i=1}^m \lambda_i(\bz^{(t)}) \left(g_i(\bx(\bz^{(t)})) + \big\langle \nabla g_i(\bx(\bz^{(t)})), \bx_{\text{feas}}-\bx(\bz^{(t)}) \big\rangle\right)\\
    \overset{ \eqref{eq:x(z)_KKT_stat}, \eqref{eq:x(z)_KKT_comple_slack}}
    {=}&~\big\langle \bxi + \nabla f(\bx(\bz^{(t)})) + p(\bx(\bz^{(t)})-\bz^{(t)}), 
    \bx(\bz^{(t)})-\bx_{\text{feas}}  \big\rangle\\
    \quad\geq\;\;\;\; &~\big\langle\nabla f(\bx(\bz^{(t)})) + p(\bx(\bz^{(t)})-\bz^{(t)}), \bx(\bz^{(t)})-\bx_{\text{feas}}\big\rangle,
\end{align*}
where $\bx_{\text{feas}}$ is given in Assumption~\ref{assume:draft}F, and the last inequality holds because $\bxi\in \N_\X(\bx(\bz^{(t)}))$. Now applying Assumption~\ref{assume:draft}A, D, and F, we obtain
\begin{align*}
    &~\textstyle \|\blambda(\bz^{(t)})\|_1\cdot \min_{i\in[m]}[-g_i(\bx_{\text{feas}})]\\
    %\leq&~\textstyle\sum_{i=1}^m \lambda_i(\bz^{(t)}) \cdot [-g_i(\bx_{\text{feas}})]\\
    \leq&~\langle\nabla f(\bx(\bz^{(t)})) + p(\bx(\bz^{(t)})-\bz^{(t)}), \bx_{\text{feas}}-\bx(\bz^{(t)})\rangle\\
    \leq&~\big(\|\nabla f(\bx(\bz^{(t)}))\|+p\cdot\|\bx(\bz^{(t)})-\bz^{(t)}\|\big) \cdot\|\bx_{\text{feas}}-\bx(\bz^{(t)})\|
    \leq(B_f+p D_{\X})\cdot D_{\X},%\leq C_{\blambda},
\end{align*} 
and thus $\|\blambda(\bz^{(t)})\|\leq \|\blambda(\bz^{(t)})\|_1\leq \frac{(B_f+p D_{\X})\cdot D_{\X}}{\min_{i\in[m]}[-g_i(\bx_{\text{feas}})]} \leq M_{\blambda},$
which together with \eqref{eq:weak_dual_bound_lambda_lm} and \eqref{eq:bound_lambda_lm} gives the desired result. 
\end{proof}

The following result refines \eqref{eq:difference_phi_lm} using \eqref{eq:regularity_result_lm}.
\begin{proposition}
\label{thm:bounded_phi_lm}
Suppose that Assumptions~\ref{assume:draft} and~\ref{assume:draft_regular_cond_lm} hold and, in Algorithm~\ref{alg:imela}, 
\begin{align*}
    \tau_t=\tau=\frac{p-L}{4B_g^2}\text{ and }\theta_t=\theta=\min\left\{\frac{p-L}{18 p},6p\tau,\frac{(p-L)\tau}{12 p\gamma K},\frac{R^2(\delta) (p-L)^2\tau}{2\cdot 12^3 p^3 M_{\blambda}^2}\right\}, \forall\,t\geq0.
\end{align*}
%for $t\geq0$. 
The sequence $\{(\bx^{(t)},\blambda^{(t)},\bz^{(t)})\}_{t\geq0}$ generated by Algorithm~\ref{alg:imela} satisfies that
\begin{align}
\nonumber
    \phi^t-\phi^{t+1}\geq&~\frac{p-L}{4}\|\bx^{(t)}-\bx(\blambda^{(t+1)},\bz^{(t)})\|^2+\frac{p}{12\theta}\|\bz^{(t)}-\bz^{(t+1)}\|^2\\
    \label{eq:bounded_phi_lm}
    &+\frac{\tau}{2} \textup{dist}^2\left( \bg(\bx(\blambda^{(t+1)},\bz^{(t)})), \mathcal{N}_{\mathbb{R}_+^m}(\blambda^{(t+1)})\right) - \frac{p\theta\epsilon_t^2}{12(p-L)^2}.
    %-\frac{\epsilon_t^2}{2(p-L)}.
\end{align}
\end{proposition}
\begin{proof}
By the choices of $\tau_t$ and $\theta_t$, inequality \eqref{eq:difference_phi_lm} holds by %according to 
Proposition~\ref{thm:difference_phi_lm}. We then prove \eqref{eq:bounded_phi_lm} separately for the cases when \eqref{eq:regular_cond_delta_near_lm} is satisfied and when it is not. %In this proof, let $\tilde{\bx}^{(t+1)} = \bx(\blambda^{(t+1)},\bz^{(t)})$.

Case I: \eqref{eq:regular_cond_delta_near_lm} is satisfied. Inequality \eqref{eq:regularity_result_lm} holds by Proposition~\ref{thm:regularity_lm}. Hence, %which implies
%\begin{small}
\begin{align}
\nonumber
    &~\tau\cdot\text{dist}^2\big( \bg(\tilde{\bx}^{(t+1)}, \mathcal{N}_{\mathbb{R}_+^m}(\blambda^{(t+1)})\big)-6p\theta\|\tilde{\bx}^{(t+1)}-\bx(\bz^{(t)})\|^2\\
    \nonumber
    \overset{\eqref{eq:regularity_result_lm}}{\geq}&~\tau\cdot\text{dist}^2\big( \bg(\tilde{\bx}^{(t+1)}, \mathcal{N}_{\mathbb{R}_+^m}(\blambda^{(t+1)})\big)
    -6p\theta\frac{\gamma K}{p-L} \text{dist}^2\big( \bg(\tilde{\bx}^{(t+1)}, \mathcal{N}_{\mathbb{R}_+^m}(\blambda^{(t+1)})\big)\\
    %\nonumber
    %=&~\left(\tau
    % -\frac{24p\theta\gamma K}{p-L}\right)
    % \text{dist}^2\left( \bg(\bx(\blambda^{(t+1)},\bz^{(t)})), \mathcal{N}_{\mathbb{R}_+^m}(\blambda^{(t+1)})\right)
    % -\frac{12p\theta}{(p-L)^2} \left(\frac{2\gamma KB_g^2}{p-L} +1\right)\cdot\epsilon_t^2\\
    \label{eq:bounded_phi_regularity_result_1}
    \geq\;&~(\tau/2)\cdot\text{dist}^2\big( \bg(\tilde{\bx}^{(t+1)}, \mathcal{N}_{\mathbb{R}_+^m}(\blambda^{(t+1)})\big),
\end{align}
%\end{small}
where %the first inequality is by~\eqref{eq:regularity_result_lm} and 
the second inequality is because %$\tau=\frac{p-L}{4B_g^2}$ and 
$\theta\leq\frac{(p-L)\tau}{12 p\gamma K}$. Then applying \eqref{eq:bounded_phi_regularity_result_1} to \eqref{eq:difference_phi_lm} implies \eqref{eq:bounded_phi_lm}.

Case II: condition \eqref{eq:regular_cond_delta_near_lm} is not satisfied. In this case, it holds that
%\begin{small}
\begin{equation}
\label{eq:regular_cond_delta_near_lm_violated}
\begin{aligned}  
    \text{dist}^2\big(-\nabla f(\tilde{\bx}^{(t+1)}) - \nabla\bg(\tilde{\bx}^{(t+1)})\blambda^{(t+1)}, \N_\X(\tilde{\bx}^{(t+1)})\big)& \\
    +\;\text{dist}^2\big( \bg(\tilde{\bx}^{(t+1)}), \mathcal{N}_{\mathbb{R}_+^m}(\blambda^{(t+1)})\big)
    %+\textstyle\sum_{i=1}^m| \lambda_i^{(t+1)} g_i(\tilde{\bx}^{(t+1)})|^2 
    & > R^2(\delta). %\frac{\delta^2}{2}-\frac{B_g^2}{(p-L)^2}\epsilon_t^2.
\end{aligned} 
\end{equation}
By the triangle inequality and $-\nabla_\bx \mathcal{L}_p (\tilde{\bx}^{(t+1)},\bz^{(t)},\blambda^{(t+1)})\in\N_\X(\tilde{\bx}^{(t+1)})$, it holds that
%\begin{small}
\begin{align}
\nonumber
    &~\text{dist}^2\big(-\nabla f(\tilde{\bx}^{(t+1)}) - \nabla\bg(\tilde{\bx}^{(t+1)})\blambda^{(t+1)}, \N_\X(\tilde{\bx}^{(t+1)})\big)\\
    \nonumber
    \leq&~p^2\|\tilde{\bx}^{(t+1)} - \bz^{(t)}\|^2
    \leq2p^2\big(\|\tilde{\bx}^{(t+1)} - \bx^{(t+1)}\|^2 + \|\bx^{(t+1)} - \bz^{(t)}\|^2\big)\\
    \nonumber
    =&~2p^2\big(\|\tilde{\bx}^{(t+1)} - \bx^{(t+1)}\|^2 + (1/\theta^2)\cdot\|\bz^{(t+1)} - \bz^{(t)}\|^2\big)\\
    \label{eq:upper-bd-df}
    \leq&~2p^2\left(\frac{\epsilon_t^2}{(p-L)^2} + \frac{1}{\theta^2}\|\bz^{(t+1)} - \bz^{(t)}\|^2\right),
\end{align}
%\end{small}
where the last inequality follows
from \eqref{eq:subroutine_optimality_imela_corollary} and \eqref{eq:regularity_result_sc_lm} with $\bx = \bx^{(t+1)}$. Plugging
%\eqref{eq:upper-bd-cs} and 
\eqref{eq:upper-bd-df} into \eqref{eq:regular_cond_delta_near_lm_violated}, we obtain
%\begin{small}
\begin{align}
\label{eq:regular_cond_delta_near_lm_violated-2}
    %(M_{\blambda}^2+1)\cdot
    \text{dist}^2\big( \bg(\tilde{\bx}^{(t+1)}), \mathcal{N}_{\mathbb{R}_+^m}(\blambda^{(t+1)})\big)
    +2p^2\left(\frac{\epsilon_t^2}{(p-L)^2} + \frac{1}{\theta^2}\|\bz^{(t+1)} - \bz^{(t)}\|^2\right)  > R^2(\delta). %\frac{\delta^2}{2}-\frac{B_g^2}{(p-L)^2}\epsilon_t^2,
\end{align}
%\end{small}
We then have
%\begin{small}
\begin{align}
\nonumber
    \;&~%\textstyle 
    \frac{p}{6\theta}\|\bz^{(t)}-\bz^{(t+1)}\|^2 + \tau\cdot\text{dist}^2\big( \bg(\tilde{\bx}^{(t+1)}), \mathcal{N}_{\mathbb{R}_+^m}(\blambda^{(t+1)})\big)-6p\theta\|\tilde{\bx}^{(t+1)}-\bx(\bz^{(t)})\|^2\\
    \nonumber
    \overset{\eqref{eq:weak_dual_bound_lm}}{\geq}&~%\textstyle 
    \frac{p}{6\theta}\|\bz^{(t)}-\bz^{(t+1)}\|^2 + \tau\cdot\text{dist}^2\big( \bg(\tilde{\bx}^{(t+1)}), \mathcal{N}_{\mathbb{R}_+^m}(\blambda^{(t+1)})\big)\\
    \nonumber
    &- \frac{12 p\theta M_{\blambda}}{p-L} \text{dist}\big( \bg(\tilde{\bx}^{(t+1)}), \mathcal{N}_{\mathbb{R}_+^m}(\blambda^{(t+1)})\big)\\ 
    \nonumber
    \geq\,\,&~%\textstyle 
    \frac{p}{6\theta}\|\bz^{(t)}-\bz^{(t+1)}\|^2 + \frac{3\tau}{4}\text{dist}^2\big( \bg(\tilde{\bx}^{(t+1)}), \mathcal{N}_{\mathbb{R}_+^m}(\blambda^{(t+1)})\big)- \frac{1}{\tau} \left(\frac{12 p\theta M_{\blambda}}{p-L}\right)^2\\
    \nonumber
    \geq\,\,&~%\textstyle 
    \frac{p}{12\theta}\|\bz^{(t)}-\bz^{(t+1)}\|^2 + \frac{\tau}{2}\text{dist}^2\big( \bg(\tilde{\bx}^{(t+1)}), \mathcal{N}_{\mathbb{R}_+^m}(\blambda^{(t+1)})\big)- \frac{1}{\tau} \left(\frac{12 p\theta M_{\blambda}}{p-L}\right)^2\\
    \nonumber
    &+ \frac{R^2(\delta) \theta}{24p} - \frac{p\theta\epsilon_t^2}{12(p-L)^2}  \\ 
    \label{eq:bounded_phi_regularity_result_2}
    \geq\,\,&~%\textstyle 
    \frac{p}{12\theta}\|\bz^{(t)}-\bz^{(t+1)}\|^2 + \frac{\tau}{2}\text{dist}^2\big( \bg(\tilde{\bx}^{(t+1)}), \mathcal{N}_{\mathbb{R}_+^m}(\blambda^{(t+1)})\big) - \frac{p\theta\epsilon_t^2}{12(p-L)^2},
\end{align}
%\end{small}
where the second inequality follows from the Young's inequality, the third one holds by \eqref{eq:regular_cond_delta_near_lm_violated-2} and $\theta \leq6p\tau$, and the last one results from $\theta\leq\frac{R^2(\delta) (p-L)^2\tau}{2\cdot 12^3 p^3 M_{\blambda}^2}$.  %is then obtained by 
Applying \eqref{eq:bounded_phi_regularity_result_2} to \eqref{eq:difference_phi_lm} gives \eqref{eq:bounded_phi_lm}.
%Combining \eqref{eq:bounded_phi_regularity_result_1} and \eqref{eq:bounded_phi_regularity_result_2} in two cases and then applying to \eqref{eq:difference_phi_lm} yields \eqref{eq:bounded_phi_lm}.
\end{proof}

Building on Proposition~\ref{thm:bounded_phi_lm}, we establish the complexity of Algorithm~\ref{alg:imela} in the following theorem.
\begin{theorem}
\label{thm:main_result}
Suppose that the assumptions in Proposition~\ref{thm:bounded_phi_lm} hold. If  $\epsilon_t=\frac{c}{t+1}$ for any constant $c>0$ and
\begin{small}
\begin{align*}
    T=\left[
    \left(1+\frac{(M_{\blambda}^2+1)B_g^2}{(p-L)^2}\right)\frac{c\pi^2}{3}+\max\left\{\frac{24p}{\theta},\frac{4(M_{\blambda}^2+1)}{\tau}\right\}\left(\phi^0-\underline{f}+\frac{cp\theta\pi^2}{72(p-L)^2}\right)\right]
    \Big/\epsilon^2,
\end{align*}
\end{small}Algorithm~\ref{alg:imela} finds an $\epsilon$-KKT solution for \eqref{eq:gco_general} with oracle complexity $O(\epsilon^{-2}\ln(\epsilon^{-1}))$. 
\end{theorem}
\begin{proof}
Consider any $t\in\{0,1,\dots,T-1\}$. Firstly, by \eqref{eq:subroutine_optimality_imela} and the triangle inequality, we have
%\begin{small}
\begin{align}
\nonumber
    &~\text{dist}\big(-\nabla f(\bx^{(t+1)})-\textstyle\sum_{i=1}^m \lambda_i^{(t+1)}\cdot\nabla g_i(\bx^{(t+1)}), \N_\X(\bx^{(t+1)})\big)\\
    \nonumber
    \leq&~\text{dist}\big(-\nabla_\bx\mathcal{L}_p (\bx^{(t+1)},\bz^{(t)},\blambda^{(t+1)}), \N_\X(\bx^{(t+1)})\big) +p\|\bx^{(t+1)}-\bz^{(t)}\|\\
    %\nonumber
    \label{eq:epsilon_stat_result_imela}
    \leq&~\epsilon_t+p\|\bx^{(t+1)}-\bz^{(t)}\|
    = \epsilon_t+%\frac{p}{\theta}
    (p/\theta)\cdot\|\bz^{(t+1)}-\bz^{(t)}\|.
\end{align}
%\end{small}
Secondly, recalling that $\tilde{\bx}^{(t+1)} = \bx(\blambda^{(t+1)},\bz^{(t)})$, we have
%\begin{small}
\begin{align}
\nonumber
    \|[\bg(\bx^{(t+1)})]_+\|
    \leq&~\|[\bg(\tilde{\bx}^{(t+1)})]_+\| +\|[\bg(\bx^{(t+1)})]_+-[\bg(\tilde{\bx}^{(t+1)})]_+\|\\
    \nonumber
    %\leq&~\|[\bg(\bx(\blambda^{(t+1)},\bz^{(t)}))]_+\| +\|\bg(\bx^{(t+1)}))-\bg(\bx(\blambda^{(t+1)},\bz^{(t)}))\|\\
    %\nonumber
    \leq&~\|[\bg(\tilde{\bx}^{(t+1)})]_+\| +B_g\|\bx^{(t+1)}-\tilde{\bx}^{(t+1)}\|\\
    \label{eq:epsilon_feas_xtilde_imela}
    \leq&~\|[\bg(\tilde{\bx}^{(t+1)})]_+\| +\frac{B_g\epsilon_t}{p-L}%\\    \label{eq:epsilon_feas_result_imela}
    %\leq&~
    \leq\text{dist} \big(\bg(\tilde{\bx}^{(t+1)}), \mathcal{N}_{\mathbb{R}_+^m}(\blambda^{(t+1)})\big) +\frac{B_g\epsilon_t}{p-L},
\end{align}
%\end{small}
where the second inequality is by Assumption~\ref{assume:draft}E, the third one is from~\eqref{eq:subroutine_optimality_imela_corollary} and \eqref{eq:regularity_result_sc_lm} with $\bx = \bx^{(t+1)}$, and the last one holds by $\|[\bg(\tilde{\bx}^{(t+1)})]_+\|= \text{dist}(\bg(\tilde{\bx}^{(t+1)}), \mathbb{R}_-^m)$ and $\mathcal{N}_{\mathbb{R}_+^m}(\blambda^{(t+1)})\subset \mathbb{R}_-^m$.

Lastly, recalling that $I_{g}^{(t+1)}=\text{supp}(\blambda^{(t+1)})$, we have
%\begin{small}
\begin{align}
\nonumber%&~
    \textstyle\sum_{i=1}^m |\lambda_i^{(t+1)}g_i(\bx^{(t+1)})|
    %\\\nonumber
    =&~
    \textstyle\sum_{i\in I_{g}^{(t+1)}} |\lambda_i^{(t+1)}g_i(\bx^{(t+1)})|
    \\
    \nonumber
    \leq&~\textstyle\sqrt{\sum_{i\in I_{g}^{(t+1)}}(\lambda_i^{(t+1)})^2}\sqrt{\sum_{i\in I_{g}^{(t+1)}}g^2_i(\bx^{(t+1)})} \\
    \nonumber
    \leq&~M_{\blambda}\cdot\text{dist} \big(\bg(\bx^{(t+1)}), \mathcal{N}_{\mathbb{R}_+^m}(\blambda^{(t+1)})\big)\\
    \nonumber
    \leq&~M_{\blambda}\cdot\text{dist} \big(\bg(\tilde{\bx}^{(t+1)}), \mathcal{N}_{\mathbb{R}_+^m}(\blambda^{(t+1)})\big) +M_{\blambda}\cdot \|\bg(\bx^{(t+1)})-\bg(\tilde{\bx}^{(t+1)})\|\\ \label{eq:epsilon_comple_slack_result_imela}
    \leq&~M_{\blambda}\cdot\text{dist} \big(\bg(\tilde{\bx}^{(t+1)}), \mathcal{N}_{\mathbb{R}_+^m}(\blambda^{(t+1)})\big) +\frac{M_{\blambda}B_g\epsilon_t}{p-L},
\end{align}
%\end{small}
where the first inequality is by the Cauchy-Schwarz inequality, the second one is by Lemma~\ref{lmm:bound_lambda_lm} and the fact that 
$g^2_i(\bx^{(t+1)})=\text{dist}^2(g_i(\bx^{(t+1)}),\mathcal{N}_{\mathbb{R}_+}(\lambda_i^{(t+1)}))$ for $i\in I_{g}^{(t+1)}$, the third one is by the triangle inequality, and the last one is by \eqref{eq:subroutine_optimality_imela_corollary}, \eqref{eq:regularity_result_sc_lm} with $\bx = \bx^{(t+1)}$, and Assumption~\ref{assume:draft}E.

By summing up \eqref{eq:bounded_phi_lm} for $t=0,1,\dots,T-1$, we have
%\begin{footnotesize}
\begin{align}
\nonumber
   &~\sum_{t=0}^{T-1}\left(\frac{p-L}{4}\|\bx^{(t)}-\tilde{\bx}^{(t+1)}\|^2+\frac{p}{12\theta}\|\bz^{(t)}-\bz^{(t+1)}\|^2
   %\nonumber
   +\frac{\tau}{2} \text{dist}^2\big( \bg(\tilde{\bx}^{(t+1)}), \mathcal{N}_{\mathbb{R}_+^m}(\blambda^{(t+1)})\big) \right)\\
    \leq&~\phi^0-\phi^T+\frac{p\theta}{12(p-L)^2}\sum_{t=0}^{T-1}\epsilon_t^2
    \label{eq:constantC}
    \leq \phi^0-\underline{f}+\frac{cp\theta\pi^2}{72(p-L)^2},
    %-\frac{\epsilon_t^2}{2(p-L)}.
\end{align}
%\end{footnotesize}
where the second inequality is by \eqref{eq:phi^t_lowerbound_lm} and the fact that
\begin{align}
\label{eq:bounded_sum_epsilon_t_imela}
    %\textstyle
    \sum_{t=0}^{T-1}\epsilon_t^2\leq\sum_{t=0}^{\infty}\epsilon_t^2=\frac{c\pi^2}{6}.
\end{align}

Squaring and summing up both sides of \eqref{eq:epsilon_stat_result_imela}, the right inequality in \eqref{eq:epsilon_feas_xtilde_imela} and \eqref{eq:epsilon_comple_slack_result_imela} for $t=0,1,\dots,T-1$ leads to 
%\begin{small}
\begin{align}
\nonumber
    &~\sum_{t=0}^{T-1}
    \left(
    \begin{array}{l}
    \text{dist}^2\big(-\nabla f(\bx^{(t+1)})-\sum_{i=1}^m \lambda_i^{(t+1)}\cdot\nabla g_i(\bx^{(t+1)}), \N_\X(\bx^{(t+1)})\big)\\
    +\|[\bg(\bx^{(t+1)})]_+\|^2
    +\big(\sum_{i=1}^m |\lambda_i^{(t+1)}g_i(\bx^{(t+1)})|\big)^2
    \end{array}
    \right)\\
    \nonumber
    \leq&~\frac{2p^2}{\theta^2} \sum_{t=0}^{T-1}\|\bz^{(t+1)}-\bz^{(t)}\|^2
    +\left(2+\frac{2(M_{\blambda}^2+1)B_g^2}{(p-L)^2}\right) \sum_{t=0}^{T-1}\epsilon_t^2\\
    \nonumber
    &+2(M_{\blambda}^2+1)\cdot \text{dist}^2 \big(\bg(\tilde{\bx}^{(t+1)}), \mathcal{N}_{\mathbb{R}_+^m}(\blambda^{(t+1)})\big)\\
    \label{eq:main_result_constant_bound}
    \leq&~\left(1+\frac{(M_{\blambda}^2+1)B_g^2}{(p-L)^2}\right)\frac{c\pi^2}{3}+\max\left\{\frac{24p}{\theta},\frac{4(M_{\blambda}^2+1)}{\tau}\right\}\left(\phi^0-\underline{f}+\frac{cp\theta\pi^2}{72(p-L)^2}\right),
\end{align}
%\end{small}
where the second inequality is because of \eqref{eq:constantC} and \eqref{eq:bounded_sum_epsilon_t_imela}. According to the choice of $T$, there exists $s\in\{0,1,\dots,T-1\}$ such that
\begin{equation}
\label{eq:main_result_epsilon_KKT}
\begin{aligned}
    &~\text{dist}^2\big(-\nabla f(\bx^{(s+1)}) -\textstyle\sum_{i=1}^m \lambda_i^{(s+1)}\cdot\nabla g_i(\bx^{(s+1)}), \N_\X(\bx^{(s+1)})\big)\\
    &+\|[\bg(\bx^{(s+1)})]_+\|^2 +\big(\textstyle\sum_{i=1}^m |\lambda_i^{(s+1)}g_i(\bx^{(s+1)})|\big)^2~\leq~\epsilon^2.
\end{aligned}
\end{equation}
According to Definition~\ref{dfn:epsilon_KKT},  $\bx^{(s+1)}$ is an $\epsilon$-KKT point of \eqref{eq:gco_general}. By \eqref{eq:subroutine_complexity_lm}, the complexity of the $t$-th iteration of Algorithm~\ref{alg:imela} is $O\left(\sqrt{\frac{K}{p-L}}\ln(\epsilon_t^{-1})\right) = O(\ln(\epsilon^{-1}))$ for $t=0,1,\dots,T-1$. Therefore, the total oracle complexity of Algorithm~\ref{alg:imela} for finding an $\epsilon$-KKT point is $O(\epsilon^{-2}\ln(\epsilon^{-1}))$. 
\end{proof}


\section{Numerical Experiment}
\label{sec:experiment}
We demonstrate the performance of the iMELa method on a fairness-aware classification problem, which is an instance of \eqref{eq:gco_general}. We compare it %the iMELa method 
with the inexact proximal point penalty (iPPP) method~\cite{lin2022complexity}, the smoothed proximal Lagrangian method (SP-LM)~\cite{pu2024smoothed}, and the switching subgradient (SSG) method~\cite{huang2023oracle}. All experiments were conducted using MATLAB 2024a on a computer with an Intel Core i7-7700 CPU (3.60 GHz) and 32~GB of RAM.

Recall that, at its $t$-th iteration, the iMELa method obtains %approximately solves the subproblem
\begin{align}
\label{eq:imela_subroutine}
    \bx^{(t+1)}\approx \argmin_{\bu\in\X}\left\{f(\bu)+{\textstyle\sum_{i=1}^m\lambda_i^{(t+1)}\cdot g_i(\bu)}+\frac{p}{2}\|\bu-\bz^{(t)}\|^2\right\}.
\end{align}
Similarly, at its $t$-th iteration, the iPPP method approximately solves the subproblem
\begin{align}
\label{eq:ippp_subroutine}
    \bx^{(t+1)}\approx \argmin_{\bu\in\X}\left\{f(\bu)+\frac{\rho_t}{2}{\textstyle\sum_{i=1}^m\left(\big[g_i(\bu)]_+\right)^2}+\frac{p_t}{2}\|\bu-\bx^{(t)}\|^2\right\},
\end{align}
%at $t$-th iteration, 
where $\rho_t>0$ is the penalty parameter and $p_t>0$ is the proximal parameter. For a fair comparison, we apply the accelerated projected gradient (APG) method in~\cite[(2.2.22)]{nesterov2018lectures} to approximately solve \eqref{eq:imela_subroutine} and \eqref{eq:ippp_subroutine}. Let $F(\bu)$ be the objective function in \eqref{eq:imela_subroutine} or \eqref{eq:ippp_subroutine} and let $\bu^{(k)}$ be the main iterate generated by the APG method. The APG method is terminated when 
\begin{align}
\label{eq:subroutine_stopping}
    \|\mathcal{G}_{1/\eta_t}(\bu^{(k)})\| :=\big\|\eta_t^{-1}\cdot\big( \bu^{(k)}-T_{1/\eta_t}(\bu^{(k)})\big)\big\|\leq\epsilon_t',
\end{align}
where $T_{1/\eta_t}(\bu^{(k)}):=\text{proj}_{\X} \left(\bu^{(k)}-\eta_t\nabla F(\bu^{(k)})\right)$,
% \begin{align*}
%     T_{1/\eta_t}(\bu^{(k)})
%     :=&~\argmin_{\bu\in\X}\left\{F(\bu^{(k)})+\langle\nabla F(\bu^{(k)}),\bu-\bu^{(k)}\rangle+\frac{1}{2\eta_t}\|\bu-\bu^{(k)}\|^2\right\}\\
%     =&~\text{proj}_{\X} \left(\bu^{(k)}-\eta_t\nabla F(\bu^{(k)})\right),
% \end{align*}
$\eta_t$ is a step-size and $\epsilon_t'$ is a targeted tolerance. Then we take $\bx^{(t+1)}=T_{1/\eta_t}(\bu^{(k)})$ as 
the approximate solution to \eqref{eq:imela_subroutine} or \eqref{eq:ippp_subroutine}. Here, $\mathcal{G}_{1/\eta_t}(\bu^{(k)})$ is known as the gradient mapping of $F(\cdot)$ at $\bu^{(k)}$. By~\cite[(2.15)]{nesterov2013gradient}, \eqref{eq:subroutine_stopping} ensures %that $\bx^{(t+1)}$ satisfies 
$\text{dist}\left( -\nabla F(\bx^{(t+1)}),\N_\X(\bx^{(t+1)})\right)\leq\epsilon_t$ with some $\epsilon_t=O(\epsilon_t')$, i.e., condition \eqref{eq:subroutine_optimality_imela} holds if $F$ is from \eqref{eq:imela_subroutine}.

In addition, at its $t$-th iteration, SP-LM updates $\bx$ by
%\begin{small}
\begin{align}
\label{eq:sp_lm_x_update}
    \bx^{(t+1)} =\text{proj}_{\X} \left(\bx^{(t)}-\eta_t\nabla_{\bx}\left( f(\bx^{(t)})+{\textstyle\sum_{i=1}^m\lambda_i^{(t+1)}\cdot g_i(\bx^{(t)})}+\frac{p}{2}\|\bx^{(t)}-\bz^{(t)}\|^2\right)\right),
\end{align}
%\end{small}
and, at its $t$-th iteration, the SSG method updates $\bx$ by
\begin{small}
\begin{align}
\label{eq:ssg_x_update}
    \bx^{(t+1)}=
    \begin{cases}
        \text{proj}_{\X} (\bx^{(t)}-\eta_t\nabla f(\bx^{(t)}))\text{ if }\max_{i\in[m]}g_i(\bx^{(t)})\leq\epsilon_t, \\
        \text{proj}_{\X} (\bx^{(t)}-\eta_t\bzeta_g^{(t)})\text{ for some }\bzeta_g^{(t)}\in\mathrm{conv}(\{\nabla g_j(\bx^{(t)}):j\in I(\bx^{(t)})\})\text{ otherwise,}
    \end{cases}
\end{align}
\end{small}where $I(\bx):=\{j\in[m]:g_j(\bx)=\max_{i\in[m]}g_i(\bx)\}$.

Next, we define the instance of~\eqref{eq:gco_general} solved in our experiments. Given a feature vector $\ba\in\mathbb{R}^d$ and a class label $b\in\{1,-1\}$, the goal of linear binary classification is to learn a model $\bx\in\mathbb{R}^d$ to predict $b$ based on the score $\bx^\top\ba$ with a larger score indicating a higher chance of a positive label. Suppose that a training set $\D=\{(\ba_i,b_i)\}_{i=1}^n$ is available. A model $\bx$ can be obtained by solving 
%\begin{small}
\begin{equation}
\label{eq:ermL_linear}
    \mathcal{L}^* := \min_{\bx\in\X}\Big\{\mathcal{L}(\bx):=\frac{1}{n}\sum_{i=1}^n \ell(b_i\cdot\bx^\top\ba_i)\Big\},
\end{equation}
%\end{small}
where $\ell(z)=\log(1+\exp(-z))$ and $\X$ is a convex and compact set.

However, solving~\eqref{eq:ermL_linear} only optimizes the classification accuracy of the resulting model but does not guarantee its fairness. Suppose that a data point $\ba$ is classified as positive if $\bx^\top\ba\geq0$ and as negative otherwise. Also, suppose that model $\bx$ is applied to two groups of data, a protected group $\mathcal{D}_p=\{\ba_i^p\}_{i=1}^{n_p}$ and an unprotected group $\mathcal{D}_u=\{\ba_i^u\}_{i=1}^{n_u}$. A measure of the fairness of $\bx$ between $\mathcal{D}_p$ and $\mathcal{D}_u$ is defined as 
%\begin{small}
$$%\begin{equation*}
    %\big|(1/n_p)\cdot \textstyle\sum_{i=1}^{n_p} \mathbb{I}(\bx^\top\ba_i^p\geq0)-(1/n_u)\cdot\sum_{i=1}^{n_u} \mathbb{I}(\bx^\top\ba_i^u\geq0) \big|,
    \Big|\frac{1}{n_p}\sum_{i=1}^{n_p} \mathbb{I}(\bx^\top\ba_i^p\geq0)-\frac{1}{n_u}\sum_{i=1}^{n_u} \mathbb{I}(\bx^\top\ba_i^u\geq0) \Big|,
$$%\end{equation*}
%\end{small}
where $\mathbb{I}(\cdot)$ is the one-zero indicator function. This measure is known as the demographic parity ~\cite{feldman2015certifying}. When its value is small, the model $\bx$ produces similar predicted positive rates in both groups, indicating the fairness of the model. However, this measure is computationally challenging because of the discontinuity of $\mathbb{I}(\cdot)$. Hence, we approximate this measure by a continuous function of $\bx$ defined as
%\begin{small}
\begin{equation}
\label{eq:ROC_linear}
    \mathcal{R}(\bx):=%\Big|
    \frac{1}{n_p}\sum_{i=1}^{n_p} \sigma(\bx^\top\ba_i^p)-\frac{1}{n_u}\sum_{i=1}^{n_u} \sigma(\bx^\top\ba_i^u),
    %\Big|,
\end{equation}
%\end{small}
where $\sigma(z)=\exp(z)/(1+\exp(z))$. 

To obtain a fair $\bx$, we balance the loss $\mathcal{L}(\bx)$ in \eqref{eq:ermL_linear} and the fairness measure $|\mathcal{R}(\bx)|$ in \eqref{eq:ROC_linear} by solving 
\begin{equation}
\label{eq:DPfairnessclassification_linear}
    \min_{\bx\in\X} %\textstyle 
    \frac{1}{2}\big(\mathcal{R}(\bx)\big)^2\text{ s.t. } \mathcal{L}(\bx)\leq \mathcal{L}^*+\kappa,
\end{equation}
where $\kappa$ is the slackness parameter indicating how much we are willing to increase the loss in order to reduce $|\mathcal{R}(\bx)|$ to obtain a more fair model, and $\X$ is the same %set constraint 
as that in \eqref{eq:ermL_linear}. In the experiments, we set $\X=\{\bx\in\mathbb{R}^d:\|\bx\|_1\leq r\}$ to satisfy Assumption~\ref{assume:draft}A. 
%Also note that the SSG method reduces to the switching gradient method in this example according to \eqref{eq:ssg_x_update} and \eqref{eq:DPfairnessclassification_linear}. 

We solve problem \eqref{eq:DPfairnessclassification_linear} on three datasets: \textit{a9a}~\cite{kohavi1996scaling}, \textit{bank}~\cite{moro2014data} and \textit{COMPAS}~\cite{angwin2016compas}.  Details about these datasets are provided in Table~\ref{tbl:data}. Each dataset is splited into two subsets with a ratio of 2 : 1. The larger subset serves as $\mathcal{D}$ in the constraint, while the smaller subset is further partitioned into $\mathcal{D}_p$ and $\mathcal{D}_u$ based on the binary group variable specified in Table~\ref{tbl:data}.

%\vskip -0.2in
\begin{table}[htb]
\vskip 0.1in
\begin{center}
\begin{sc}
%\resizebox{0.5\textwidth}{!}{
\begin{tabular}{c|ccccc}
\hline
Datasets & $n$ & $d$ & Label & Groups \\
\hline
a9a & 48,842 & 123 & Income & Gender\\
Bank & 41,188 & 54 & Subscription & Age\\
COMPAS & 6,172 & 16 & Recidivism & Race\\
%german & 1,000 & 21 & Credit risk & Gender\\
\hline
\end{tabular}
%}
\end{sc}
\end{center}
\vskip -0.05in
\caption{Dataset information. The binary group variable represents males or females in the a9a dataset, users with age within $[25,60]$ and outside $[25,60]$ in the bank dataset, and Caucasian versus non-Caucasian individuals in the COMPAS dataset.}
\label{tbl:data}
%\vskip -0.2in
\end{table}

In the experiments, we first solve \eqref{eq:ermL_linear} by the projected gradient method up to near optimality to obtain a close approximation of $\mathcal{L}^*$ and a solution $\bx_{\text{feas}}$. Specifically, we use a constant step-size of $0.1$ in the projected gradient method and terminate it when it finds a solution $\bx_{\text{feas}}$ satisfying $\text{dist}(-\nabla\mathcal{L}(\bx_{\text{feas}}),\N_\X(\bx_{\text{feas}}))\leq0.001$. Then we set $\kappa=0.001\mathcal{L}^*$ in \eqref{eq:DPfairnessclassification_linear}.

The methods in comparison are initialized at $\bx^{(0)}=\bx_{\text{feas}}$ obtained above. Based on the definitions of $\mathcal{R}$ and $\mathcal{L}$, we can calculate an upper bound of the smooth parameter of the objective and constraint functions in \eqref{eq:DPfairnessclassification_linear} using the data. For the objective function in \eqref{eq:DPfairnessclassification_linear}, we notice that $\mathcal{R}(\bx)\in[0,1]$ for any $\bx\in\mathbb{R}^d$ and, by a derivation similar to~\cite[(75-77)]{huang2023oracle}, $\mathcal{R}(\bx)$ and $\nabla \mathcal{R}(\bx)$ are Lipschitz continuous with constants of
%\begin{small}
\begin{align*}
%\label{eq:ROC_linear_R_alpha_beta} 
    \alpha=\frac{1}{4n_p}\sum_{i=1}^{n_p} \|\ba_i^p\|+\frac{1}{4n_u}\sum_{i=1}^{n_u} \|\ba_i^u\|\text{ and }
    \beta=\frac{1}{4n_p}\sum_{i=1}^{n_p} \|\ba_i^p\|^2+\frac{1}{4n_u}\sum_{i=1}^{n_u} \|\ba_i^u\|^2,
\end{align*}
%\end{small}
respectively. Additionally, it is easy to show that $\frac{1}{2}(\mathcal{R}(\bx))^2$ is $\alpha$-Lipschitz continuous and $(\beta+\alpha^2)$-smooth in $\bx$. For the constraint function in \eqref{eq:DPfairnessclassification_linear}, by the fact that $\ell(z)$ is 1-Lipschitz continuous and $\sigma(z)$ is $\frac{1}{4}$-Lipschitz continuous,  we can show that $\mathcal{L}(\bx)$ and $\nabla\mathcal{L}(\bx)$ are Lipschitz continuous with constants of
%\begin{small}
\begin{align*}
    \gamma=\frac{1}{n}\sum_{i=1}^n\|\ba_i\|
    \text{ and }
    \gamma'=\frac{1}{4n}\sum_{i=1}^n\|\ba_i\|^2,
\end{align*}
%\end{small}
respectively. Therefore, \eqref{eq:DPfairnessclassification_linear} satisfies Assumption~\ref{assume:draft} with any $L\geq\max\{\beta+\alpha^2,\gamma'\}$. After estimating $\max\{\beta+\alpha^2,\gamma'\}$ using data, we set $L$ to 10, 10 and 2.5 on \textit{a9a}, \textit{bank} and \textit{COMPAS}, respectively, and set $p=2L$. In the APG method applied to \eqref{eq:imela_subroutine} and \eqref{eq:ippp_subroutine}, we use a constant step-size $\eta$. On \textit{a9a} and \textit{bank}, we select $\eta$ from $\{0.005,0.01,0.02,0.05\}$; on \textit{COMPAS}, we select $\eta$ from $\{0.02,0.05,0.1,0.2\}$.
The choices of other parameters are uniform across all datasets. For the iMEla method, we select $\tau_t=\tau$ from $\{5,10,20,50\}$ and $\theta_t=\theta$ from $\{0.5,0.75,1\}$, and set $\epsilon_t=\frac{c}{t+1}$ with $c$ selected from $\{1,2,5,10\}$. For the iPPP method,  following~\cite{lin2022complexity}, we set $\rho_t=\rho\sqrt{t+1}$ and $\epsilon_t=\frac{1}{\rho_t(t+1)}$ with $\rho$ selected from $\{200,500,1000,1500\}$. For SP-LM, we select $\eta$ from $\{0.005,0.01,0.02,0.05\}$ on \textit{a9a} and \textit{bank} and from $\{0.02,0.05,0.1,0.2\}$ on \textit{COMPAS}. We then select $\tau_t=\tau$ from $\{5,10,20,50\}$ and $\theta_t=\theta$ from $\{0.5,0.75,1\}$. For the SSG method, following~\cite{huang2023oracle}, we adopt both static and diminishing step-sizes. For the static step-size, we select $\epsilon_t=\epsilon$ from $\{10^{-6}, 2\times10^{-6},5\times10^{-6}, 10^{-5}\}$ and $\eta_t=\eta$ from $\{2\times10^{-4}, 5\times10^{-4},10^{-3}, 2\times10^{-3}\}$. For the diminishing step-size, we select $\epsilon_t=\frac{E_1}{\sqrt{t+1}}$ and $\eta_t=\frac{E_2}{\sqrt{t+1}}$ and select $E_1$ from $\{5\times10^{-5}, 10^{-4},2\times10^{-4}, 5\times10^{-4}\}$ and $E_2$ from $\{0.02,0.05,0.1,0.2\}$. Since the iMELa method, the iPPP method and SP-LM are primal-dual based, their best combination of parameters is chosen as the one that minimizes the smallest value of 
%\begin{small}
\begin{align*}
    &~\text{dist}\big(-\mathcal{R}(\bx^{(s)})\cdot\nabla \mathcal{R}(\bx^{(s)}) -\lambda^{(s)}\cdot\nabla \mathcal{L}(\bx^{(s)}), \N_\X(\bx^{(s)})\big)\\
    &+\big\|\big[\mathcal{L}(\bx^{(s)})-(\mathcal{L}^*+\kappa)\big]_+\big\| + \big|\lambda^{(s)}\big(\mathcal{L}(\bx^{(s)})-(\mathcal{L}^*+\kappa)\big)\big|,
\end{align*}
%\end{small}
among iterations $s=0,1,\dots,T_{\text{tuning}}-1$. Here, $T_{\text{tuning}}$ is the number of iterations for parameter tuning. 
Note that $\blambda^{(t)}$ is directly generated in the iMELa method and SP-LM, but we need to set $\blambda^{(t)}=\rho_{t-1}[\bg(\bx^{(t)})]_+$ for the iPPP method according to Step 5 of Algorithm 1 in~\cite{lin2022complexity}. Since the SSG method does not generate $\blambda^{(t)}$, its best combination of parameters is identified as the one that minimizes the smallest value of $\frac{1}{2}(\mathcal{R}(\bx^{(s)}))^2$ when $\mathcal{L}(\bx^{(s)})\leq \mathcal{L}^*+\kappa+10^{-5}$ among iterations $s=0,1,\dots,T_{\text{tuning}}-1$. For SP-LM and the SSG method, we set $T_{\text{tuning}}=1000$, 5000 and 75000 for \textit{a9a}, \textit{bank} and \textit{COMPAS}, respectively. For a fair comparison, for the iMEla method and the iPPP method, we set $T_{\text{tuning}}$ to be the smallest integer satisfying $\sum_{t=0}^{T_{\text{tuning}}-1}k_t\geq1000\text{, }5000\text{ and }75000$ for \textit{a9a}, \textit{bank} and \textit{COMPAS}, respectively, where $k_t$ is the number of inner steps performed at the $t$-th outer iteration. Once all parameters are chosen, we run SP-LM and the SSG method for $T=4T_{\text{tuning}}$ iterations, and run the iMEla method and the iPPP method for $T$ iterations with $T$ being the smallest integer satisfying $\sum_{t=0}^{T-1} k_t\geq4\sum_{t=0}^{T_{\text{tuning}}-1}k_t$. 

\begin{figure*}[!ht]
     \begin{tabular}{@{}c|ccc@{}}
      & a9a & bank & COMPAS \\
		\hline \vspace*{-0.1in}\\
		\raisebox{10ex}{\small{\rotatebox[origin=c]{90}{Objective}}}
		& \hspace*{-0.06in}\includegraphics[width=0.30\textwidth]{plots/a9a_deterministic_convex_objective_result.eps}
		& \hspace*{-0.06in}\includegraphics[width=0.295\textwidth]{plots/BankMarketing_deterministic_convex_objective_result.eps}
		& \hspace*{-0.06in}\includegraphics[width=0.285\textwidth]{plots/COMPAS_deterministic_convex_objective_result.eps}
          \\
		\raisebox{10ex}{\small{\rotatebox[origin=c]{90}{Infeasibility}}}
		& \hspace*{-0.06in}\includegraphics[width=0.30\textwidth]{plots/a9a_deterministic_convex_feasibility_result.eps}
		& \hspace*{-0.06in}\includegraphics[width=0.295\textwidth]{plots/BankMarketing_deterministic_convex_feasibility_result.eps}
		& \hspace*{-0.06in}\includegraphics[width=0.30\textwidth]{plots/COMPAS_deterministic_convex_feasibility_result.eps}
           \\
        \raisebox{10ex}{\small{\rotatebox[origin=c]{90}{Stationarity}}}
		& \hspace*{-0.06in}\includegraphics[width=0.30\textwidth]{plots/a9a_deterministic_convex_stationarity_result.eps}
		& \hspace*{-0.06in}\includegraphics[width=0.29\textwidth]{plots/BankMarketing_deterministic_convex_stationarity_result.eps}
		& \hspace*{-0.06in}\includegraphics[width=0.285\textwidth]{plots/COMPAS_deterministic_convex_stationarity_result.eps}
          \\
        \raisebox{10ex}{\small{\rotatebox[origin=c]{90}{Complementary Slackness}}}
		& \hspace*{-0.06in}\includegraphics[width=0.30\textwidth]{plots/a9a_deterministic_convex_complementaryslackness_result.eps}
		& \hspace*{-0.06in}\includegraphics[width=0.295\textwidth]{plots/BankMarketing_deterministic_convex_complementaryslackness_result.eps}
		& \hspace*{-0.06in}\includegraphics[width=0.295\textwidth]{plots/COMPAS_deterministic_convex_complementaryslackness_result.eps}
        \end{tabular}
	%\vspace{-0.1in}
	\caption{Performances of $\mathrm{iMELa}$, $\mathrm{SP}$-$\mathrm{LM}$, $\mathrm{iPPP}$ and $\mathrm{SSG}$ with static and diminishing step-sizes vs number of steps on classification problems with demographic parity fairness. } 
    \label{fig:figure_convex_experiment_step}
	\vspace{-0.1in}
\end{figure*}

We compare the methods based on the objective value, infeasibility, stationarity, and complementary slackness (see \eqref{dfn:epsilon_stat}, \eqref{dfn:epsilon_feas} and \eqref{dfn:epsilon_comple_slack}) achieved by $\bx^{(t)}$ and the corresponding Lagrangian multipliers.\footnote{Since the SSG method does not generate a Lagrangian multiplier, we do not report its measures of stationarity and complementary slackness.} 
While the calculations for most of these metrics are straightforward, additional details are provided for computing the stationarity metric. Observe that $\X=\{\bx\in\mathbb{R}^d:\bB\bx\leq r\cdot\mathds{1}\}$, where $\bB\in\mathbb{R}^{2^d\times d}$ is a matrix whose rows are all vectors in $\{1,-1\}^{d}$, and $\mathds{1}$ denotes the all-one vector in $\mathbb{R}^{2^d}$. If $\|\bx\|_1<r$, then $\N_{\X}(\bx)=\{\bzero\}$ so that the stationarity measure in \eqref{dfn:epsilon_stat} reduces to $\|\mathcal{R}(\bx)\cdot\nabla \mathcal{R}(\bx)+\lambda\cdot\nabla \mathcal{L}(\bx)\|$, where $\lambda\geq0$ is the Lagrange multiplier produced by the algorithm to pair with $\bx$. If $\|\bx\|_1=r$, then by~\cite[Example A.5.2.6 (b)]{hiriart2001fundamentals}, there exists an index set of the active constraints at $\bx\in\X$ defined by
\begin{align}
\label{eq:activeset}
    J(\bx)=\{j=1,\dots,2^d:[\bB]_j\bx=r\}
\end{align}
where $[\bB]_j$ denotes the $j$-th row of $\bB$. Then the normal cone to $\X$ at $\bx$ in this case is
\begin{align}
\label{eq:normalcone}
    \N_{\X}(\bx)=\text{cone}\{[\bB]_j^\top:j\in J(\bx)\} =\textstyle\{\sum_{j\in J(\bx)}a_j[\bB_j]^\top:a_j\geq0,\,j\in J(\bx)\}.
\end{align}
Therefore, the stationarity measure in \eqref{dfn:epsilon_stat} can be calculated by solving 
\begin{align*}
    \min\big\{\big\|\mathcal{R}(\bx)\cdot\nabla \mathcal{R}(\bx)+\lambda\cdot\nabla \mathcal{L}(\bx)+\textstyle\sum_{j\in J(\bx)}a_j[\bB_j]^\top\big\|\,\big|\,a_j\geq0,\,j\in J(\bx)\big\}.
\end{align*}
We use \texttt{mpcActiveSetSolver} in MATLAB to solve this quadratic program.

To compare the methods in their oracle complexity, in Figure~\ref{fig:figure_convex_experiment_step}, we plot the four aforementioned performance metrics against the accumulated number of gradient steps performed by the APG method called within the iMEla method and the iPPP method, and the number of gradient steps performed by SP-LM and the SSG method. According to Figure~\ref{fig:figure_convex_experiment_step}, on all the instances studied, the iMELa method performs similarly to SP-LM and outperforms the iPPP method in all performance metrics. This is consistent with our theoretical finding that the iMELa method has an oracle complexity similar to SP-LM but better than the iPPP method. Moreover, the iMELa method and SP-LM reduce the infeasibility quickly and maintains a (nearly) feasible solution throughout the remaining iterations. In addition, we find that for all instances, the number of gradient steps performed by the APG method for solving \eqref{eq:imela_subroutine} within the iMELa method is generally much smaller than that for solving \eqref{eq:ippp_subroutine}, which is the main reason for the better empirical performance of the iMELa method than the iPPP method.

\section{Conclusion}
We analyze the oracle complexity of the iMELa method for finding an $\epsilon$-KKT point in smooth non-convex functional constrained optimization. Each iteration consists of approximately solving a strongly convex subproblem, followed by updates to the Lagrange multipliers and the proximal center. Our results show that the oracle complexity of the iMELa method matches the best known result in the literature, up to a logarithmic factor, under a local error bound condition, which is a strictly weaker assumption than the local LICQ condition required to achieve the best complexity guarantee.

\appendix
\section{Proof of Proposition~\ref{thm:LICQ_implies_regular_cond_lm}}
\label{sec:LICQ_implies_regular_cond_lm}
\begin{lemma}
\label{lmm:bound_KKT_set}
The set $\X^*$ is compact.
\end{lemma}
\begin{proof}
By the structure of $\X$ and the Slater's condition in Assumption~\ref{assume:draft}A and F, $\X^*$ is non-empty (see, e.g.~\cite[Section 3.3]{bertsekas1999nonlinear}). We then prove the lemma by two steps.

First, we show that, for any KKT point $\bx^*\in\X^*$, its any associated vector of Lagrangian multipliers $\blambda^*$ satisfies that $\|\blambda^*\|\leq B_{\blambda}:=\frac{B_f D_{\X}}{\min_{i\in[m]}[-g_i(\bx_{\text{feas}})]}$. By the KKT conditions at $(\bx^*,\blambda^*)$ in \eqref{eq:gco_general}, we have
\begin{align}
\label{eq:x*_KKT_stat_comple_slack}
    \textstyle  \bxi^*:= -\nabla f(\bx^*) - \sum_{i=1}^m \lambda_i^* \nabla g_i(\bx^*) \in \N_\X(\bx^*)\text{ and }
    \lambda_i^* g_i(\bx^*) = 0,\;i\in [m].
\end{align} 
Hence, by the convexity of $g_i$'s and $\blambda^*\geq\bzero$, it follows that
%\begin{small}
\begin{align*}
    \|\blambda^*\|_1\cdot \min_{i\in[m]}[-g_i(\bx_{\text{feas}})]
    \leq&
    -\textstyle\sum_{i=1}^m \lambda_i^* g_i(\bx_{\text{feas}})
    \leq-\textstyle\sum_{i=1}^m \lambda_i^* \left(g_i(\bx^*) + \langle \nabla g_i(\bx^*), \bx_{\text{feas}}-\bx^* \rangle\right)\\
    =&~
    \big\langle \bxi^* + \nabla f(\bx^*), 
    \bx_{\text{feas}}-\bx^*\big\rangle
    \leq
    \big\langle\nabla f(\bx^*), \bx_{\text{feas}}-\bx^*\big\rangle
    %\leq\|\nabla f(\bx^*)\| \cdot\|\bx_{\text{feas}}-\bx^*\|
    \leq B_f D_{\X},
\end{align*}
%\end{small}
where $\bx_{\text{feas}}$ is given in Assumption~\ref{assume:draft}F, the equality holds by~\eqref{eq:x*_KKT_stat_comple_slack}, the third inequality holds by $\bxi^*\in \N_\X(\bx^*)$, and the last one is from Assumption~\ref{assume:draft}A, D, and F. Thus, we obtain $\|\blambda^*\|\leq \|\blambda^*\|_1\leq B_{\blambda}$,
which proves the claimed result.

Second, by Assumption~\ref{assume:draft}A, $\X^*$ is bounded. Hence, it suffices to show that $\X^*$ is closed. Suppose there is a sequence of KKT points and their associated vector of Lagrangian multipliers, denote by $\{(\bar{\bx}^{(k)},\bar{\blambda}^{(k)})\}_{k\geq1}\subset\X^*\times\mathbb{R}_+^m$, such that $\bar{\bx}^{(k)}\rightarrow\bar\bx^*$ as $k\rightarrow \infty$ for some $\bar\bx^*\in\X$. By definition, there exists $\bar{\bxi}^{(k)}\in \N_\X(\bar{\bx}^{(k)})$ such that  
\begin{align*}
    \nabla f(\bar{\bx}^{(k)})+\textstyle\sum_{i=1}^m \bar{\lambda}_i^{(k)}\nabla g_i(\bar{\bx}^{(k)})+\bar{\bxi}^{(k)}=\bzero,
    ~g_i(\bar{\bx}^{(k)})\leq0, ~\bar{\lambda}_i^{(k)} g_i(\bar{\bx}^{(k)})=0, ~\forall\, i\in[m].
\end{align*}
Since $\|\bar{\blambda}^{(k)}\|\leq B_{\blambda}$, passing to a subsequence if necessary, it holds that $\lim_{k\rightarrow \infty}\bar{\blambda}^{(k)}=\bar{\blambda}^*$ for some $\bar{\blambda}^*\geq\bzero$. As $k\rightarrow\infty$, we conclude from the continuity of $\nabla f(\cdot)$ and $\nabla\bg(\cdot)$ and the outer semi-continuity of $\N_\X(\cdot)$ that $\bar{\bxi}^{(k)}\rightarrow\bar{\bxi}^*$ for some $\bar{\bxi}^*\in \N_\X(\bar{\bx}^*)$ and
\begin{align*}
    \nabla f(\bar{\bx}^*)+\textstyle\sum_{i=1}^m \bar{\lambda}_i^*\nabla g_i(\bar{\bx}^*)+\bar{\bxi}^*=\bzero,
    ~g_i(\bar{\bx}^*)\leq0, ~\bar{\lambda}_i^* g_i(\bar{\bx}^*)=0, ~\forall\,i\in[m],
\end{align*}
meaning that $\bar{\bx}^*\in\X^*$, which completes the proof. 
\end{proof}

\begin{lemma}
\label{lmm:nearKKT_activeset}
For any $\vartheta>0$, there exists $r(\vartheta)>0$ such that, if $\bx$ is feasible to \eqref{eq:gco_general} and $\textup{dist}(\bx,\X^*)\leq r(\vartheta)$, then there must exist $\bx^*\in\X^*$ such that $\|\bx-\bx^*\|\leq\vartheta$ and $J_g(\bx)\subset J_g(\bx^*)\text{ and }J_A(\bx)\subset J_A(\bx^*)$.
\end{lemma}
\begin{proof}
We prove this by contradiction. Suppose that the claim does not hold. Then there exists $\vartheta>0$ and a sequence $\{\bx^{(k)}\}_{k\geq1}$ such that $\bx^{(k)}$ is feasible to \eqref{eq:gco_general} and $\text{dist}(\bx^{(k)},\X^*)\leq r_k:=\frac{\vartheta}{k}\leq\vartheta$ but $J_g(\bx^{(k)})\not\subset J_g(\bx^*)\text{ or }J_A(\bx^{(k)})\not\subset J_A(\bx^*)$ for any $k$ and any $\bx^*\in\X^*$
satisfying $\|\bx^{(k)}-\bx^*\|\leq\vartheta$. 
%In other words, there exist $i\in[m]$, $g_i(\bx^{(k)})=0$, $g_i(\bx^*)<0$ or $j\in[l]$, $[\bA\bx^{(k)}-\bb]_j=0$, $[\bA\bx^*-\bb]_j<0$.

Due to the compactness of $\X$ and $\X^*$ (see Lemma~\ref{lmm:bound_KKT_set}),  by passing to a subsequence if necessary, there exists some $\bar{\bx}^*\in\X^*$ such that  %$\|\bx^{(k)}-\bar\bx^*\|\leq\vartheta$ for any $k\geq 1$ and 
$\bx^{(k)}\rightarrow\bar{\bx}^*$ as $k\rightarrow\infty$. By the continuity of $\bg(\cdot)$ and $\bA(\cdot)-\bb$, it holds that for any $i\in[m]$, if $g_i(\bar{\bx}^*)<0$, then $g_i(\bx^{(k)})<0$ as well when $k$ is large enough, and for any $j\in[l]$, if $[\bA\bar{\bx}^*-\bb]_j<0$, then $[\bA\bx^{(k)}-\bb]_j<0$ as well when $k$ is large enough. That means $J_g(\bx^{(k)})\subset J_g(\bar\bx^*)$ and $ J_A(\bx^{(k)})\subset J_A(\bar\bx^*)$ for a large enough $k$. This contradicts to the assumption.  
Therefore, the conclusion holds.
\end{proof}
\begin{lemma}
\label{lmm:nearKKT_LICQ}
%Under the same conditions as in Proposition~\ref{thm:LICQ_implies_regular_cond_lm}, 
Suppose the constraints of \eqref{eq:gco_general} satisfy the local LICQ in \eqref{eq:LICQ_sigmamin}.
Then there exists $\delta'>0$ such that, if $\bx$ is feasible to \eqref{eq:gco_general} and $\|\bx-\bx^*\|\leq\delta'$ for some $\bx^*\in\X^*$, then $\sigma_{\min}\big(\big[\nabla \bg_{I_g}(\bx), \bA_{I_A}^\top\big]\big)\geq\frac{\zeta}{2}$ for any $I_g\subset J_g(\bx^*)$ and $
I_A\subset J_A(\bx^*)$.
\end{lemma}
\begin{proof}
The result follows from \eqref{eq:LICQ_sigmamin} and the Lipschitz continuity of $\nabla \bg(\cdot)$.
\end{proof}
\begin{proof}[Proof of Proposition~\ref{thm:LICQ_implies_regular_cond_lm}]
Let $\delta=\min\left\{\frac{r(\delta')}{2},\frac{\zeta}{2L\sqrt{m}}\right\}$, where $r(\cdot)$ is from Lemma \ref{lmm:nearKKT_activeset} and $\delta'$ is from Lemma~\ref{lmm:nearKKT_LICQ}. Suppose that $\bx\in\X$ and $\|\bx-\bx^*\|\leq \delta$ for some $\bx^*\in\X^*$, we then need to show that there exists $\gamma>0$ such that \eqref{eq:regular_cond_gamma_lm} holds for any $I_g\subset J_g(\bx^*)$ and $I_A\subset J_A(\bx^*)$.
For such $I_g$ and $I_A$, consider the minimization problem 
%\begin{small}
\begin{align}
\label{eq:projbarx}
    \bar{\bx}=\argmin_{\bz} \frac{1}{2}\|\bz-\bx\|^2
    \text{ s.t.}&\begin{array}{l}
    g_i(\bz)=0,\,i\in I_g,\,g_i(\bz)\leq 0,\,i\in[m]\backslash I_g,\\[0.1ex]
    [\bA\bz-\bb]_j=0,\,j\in I_A,\,[\bA\bz- \bb]_j\leq0,\,j\in[l]\backslash I_A.
    \end{array}
\end{align}
%\end{small}
Note that $\bar{\bx}$ is well-defined because the feasible set of \eqref{eq:projbarx} is compact and non-empty as it at least contains $\bx^*$. As a result, $\text{dist} (\bx,\mathcal{S}(I_g, I_A))=\|\bx-\bar{\bx}\|\leq \|\bx-\bx^*\|\leq \delta$ and thus $\text{dist}(\bar{\bx},\X^*)\leq\|\bar{\bx}-\bx^*\|\leq 2\delta \leq r(\delta')$. Since $\bar\bx$ is also feasible to \eqref{eq:gco_general}, by invoking Lemma~\ref{lmm:nearKKT_activeset}, there exists $\bar\bx^*\in\X^*$ such that $\|\bar\bx-\bar\bx^*\|\leq\delta'$ and $J_g(\bar\bx)\subset J_g(\bar\bx^*)$ and $J_A(\bar\bx)\subset J_A(\bar\bx^*)$. By Lemma~\ref{lmm:nearKKT_LICQ} at $\bar\bx$ and $\delta'$, we have $\sigma_{\min}\big(\big[\nabla \bg_{J_g(\bar\bx)}(\bar\bx), \bA_{J_A(\bar\bx)}^\top\big]\big)\geq\frac{\zeta}{2}$. This means \eqref{eq:projbarx} satisfies the LICQ at $\bar\bx$. Hence, there exist a vector of Lagrangian multipliers 
$\bar\blambda\in\mathbb{R}^{m}$ and $\bar\bw\in\mathbb{R}^{l}$ such that 
\begin{align}
\label{eq:KKTprojection_stat}
    &\bar{\bx}-\bx+\textstyle\sum_{i=1}^m \bar\lambda_i \cdot\nabla g_i(\bar{\bx})
    +\textstyle\sum_{j=1}^l  \bar{w}_j\cdot[\bA]_j^\top=\bzero,\\
    \label{eq:KKTprojection_feas}
    &\bar\lambda_i \geq 0,\; i\in[m]\backslash I_g,\quad\bar{w}_j \geq 0,\;j\in[l]\backslash I_{A},\\
    \label{eq:KKTprojection_comple_slack}
    &\bar\lambda_i\cdot g_i(\bar{\bx}) = 0,\; i\in[m]\backslash I_g,\quad\bar{w}_j\cdot [\bA\bar{\bx}-\bb]_j = 0,\;j\in[l]\backslash I_{A}.
\end{align}
Notice $I_g\subset J_g(\bar{\bx})$ and $I_A\subset J_A(\bar{\bx})$.
By \eqref{eq:KKTprojection_comple_slack}, we must have $\bar\lambda_i=0$ for $i\in[m]\backslash J_g(\bar{\bx})$ and $\bar{w}_j=0$ for $j\in[l]\backslash J_A(\bar{\bx})$. Thus, \eqref{eq:KKTprojection_stat} and $\sigma_{\min}([\nabla \bg_{J_g(\bar{\bx})}(\bar{\bx}), \bA_{ J_A(\bar{\bx})}^\top])\geq \textstyle \frac{\zeta}{2}$ imply 
\begin{align}
\label{eq:KKTprojection_zeta}
    \|\bar{\bx}-\bx\|=\left\|\textstyle\sum_{i\in J_g(\bar{\bx}) } \bar\lambda_i \cdot \nabla g_i(\bar{\bx})
    +\textstyle\sum_{j\in J_A(\bar{\bx})} \bar{w}_j\cdot [\bA]_j^\top\right\|\geq \textstyle \frac{\zeta}{2}\sqrt{\rule{0pt}{2ex}\|\bar\blambda\|^2+\|\bar{\bw}\|^2}.
\end{align}
%where the inequality is from the uniform LICQ. 
% adjust the height of sqrt according to
% https://tex.stackexchange.com/questions/455584/how-to-adjust-increase-the-height-of-square-root
Moreover, for $i\in I_g$, by the $L$-smoothness of $g_i$ and the fact that $g_i(\bar{\bx})=0$, we have
\begin{align}
    \nonumber
    \bar\lambda_i\cdot \left\langle\nabla g_i(\bar{\bx}),\bx-\bar{\bx}\right\rangle
    %\leq |\bar\lambda_i|\cdot|\left\langle\nabla g_i(\bar{\bx}),\bx-\bar{\bx}\right\rangle|
    \leq&~|\bar\lambda_i|\cdot|g_i(\bx)-g_i(\bar{\bx})|+\frac{|\bar\lambda_i|L}{2}\|\bx-\bar{\bx}\|^2\\
    \label{eq:lambda_bar_in_I_g}
    =&~ |\bar\lambda_i|\cdot|g_i(\bx)|+\frac{|\bar\lambda_i|L}{2}\|\bx-\bar{\bx}\|^2.
\end{align}
For $i\in[m]\backslash I_g$, using \eqref{eq:KKTprojection_feas}, \eqref{eq:KKTprojection_comple_slack} and the convexity of $g_i$, we have 
\begin{align}
\label{eq:lambda_bar_notin_I_g}
    \bar\lambda_i\cdot\left\langle\nabla g_i(\bar{\bx}),\bx-\bar{\bx}\right\rangle\leq \bar\lambda_i\cdot\left(g_i(\bx)-g_i(\bar{\bx})\right)
    \leq |\bar\lambda_i|\cdot[g_i(\bx)]_+.
\end{align}
Following similar arguments, we have
\begin{align}
\label{eq:w_bar_in_[l]}
    \bar{w}_j\cdot [\bA(\bx-\bar{\bx}) ]_j
    \leq\begin{cases}
        |\bar{w}_j|\cdot|[\bA\bx-\bb ]_j| & \text{ for }j\in I_A\\
        |\bar{w}_j|\cdot [[\bA\bx-\bb ]_j]_+ & \text{ for }j\in[l]\backslash I_{A}.
    \end{cases}
\end{align}
%Following similar arguments, for $j\in I_A$, we have
%\begin{align}
%\label{eq:w_bar_in_I_A}
%    \bar{w}_j\cdot [\bA(\bx-\bar{\bx}) ]_j
%    \leq |\bar{w}_j|\cdot|[\bA\bx-\bb ]_j|,
%\end{align}
%and, for $j\in[l]\backslash I_{A}$, we have
%\begin{align}
%\label{eq:w_bar_notin_I_A}
%    \bar{w}_j\cdot [\bA(\bx-\bar{\bx}) ]_j
%    \leq |\bar{w}_j|\cdot[[\bA\bx-\bb ]_j]_+.
%\end{align}

Taking the inner product of the left-hand side of \eqref{eq:KKTprojection_stat} and $\bar{\bx}-\bx$ and applying \eqref{eq:lambda_bar_in_I_g}, \eqref{eq:lambda_bar_notin_I_g}, and \eqref{eq:w_bar_in_[l]} lead to%\eqref{eq:w_bar_in_I_A} and \eqref{eq:w_bar_notin_I_A} lead to
\begin{small}
\begin{align*}
    \|\bar{\bx}-\bx\|^2
    \leq&\left(\begin{array}{l}
    \sum_{i\in I_g} |\bar\lambda_i|\cdot|g_i(\bx)|
    +\sum_{i\in[m]\backslash I_g} |\bar\lambda_i|\cdot[g_i(\bx)]_+\\[1ex]
    +\sum_{j\in I_{A}} |\bar{w}_j|\cdot\left|[\bA\bx-\bb]_j\right|
    +\sum_{j\in[l]\backslash I_{A}} |\bar{w}_j|\cdot\left[[\bA\bx-\bb]_j\right]_+
    \end{array}
    \right)+\sum_{i\in I_g}\frac{|\bar\lambda_i|L}{2}\|\bx-\bar{\bx}\|^2\\
    \leq&\left(\begin{array}{l}
    \sum_{i\in I_g} |g_i(\bx)|^2
    +\sum_{i\in[m]\backslash I_g} \left([g_i(\bx)]_+\right)^2\\%[-0.5ex]
    +\sum_{j\in I_{A}}\left|[\bA\bx-\bb]_j\right|^2
    +\sum_{j\in[l]\backslash I_{A}} \left(\left[[\bA\bx-\bb]_j\right]_+\right)^2
    \end{array}
    \right)^{1/2}\cdot \sqrt{\|\bar\blambda\|^2+\|\bar{\bw}\|^2}\\
    &+\sqrt{m} \sqrt{\|\bar\blambda\|^2+\|\bar{\bw}\|^2}\cdot\frac{L}{2}\|\bx-\bar{\bx}\|^2.
\end{align*}
\end{small}Recall \eqref{eq:KKTprojection_zeta} and the fact that $\|\bx-\bar{\bx}\|\leq \delta\leq \frac{\zeta}{2L\sqrt{m}}$. The inequality above implies
\begin{small}
\begin{align*}
    \|\bar{\bx}-\bx\|^2
    \leq&\left(\begin{array}{l}
    \sum_{i\in I_g} |g_i(\bx)|^2
    +\sum_{i\in[m]\backslash I_g} \left([g_i(\bx)]_+\right)^2\\%[-0.5ex]
    +\sum_{j\in I_{A}}\left|[\bA\bx-\bb]_j\right|^2
    +\sum_{j\in[l]\backslash I_{A}} \left(\left[[\bA\bx-\bb]_j\right]_+\right)^2
    \end{array}
    \right)^{1/2}\cdot \frac{2\|\bar{\bx}-\bx\|}{\zeta}\\
    &+\sqrt{m}\|\bar{\bx}-\bx\|\cdot\frac{L}{\zeta}\|\bx-\bar{\bx}\|^2\\
    \leq&\left(\begin{array}{l}
    \sum_{i\in I_g} |g_i(\bx)|^2
    +\sum_{i\in[m]\backslash I_g} \left([g_i(\bx)]_+\right)^2\\%[-0.5ex]
    +\sum_{j\in I_{A}}\left|[\bA\bx-\bb]_j\right|^2
    +\sum_{j\in[l]\backslash I_{A}} \left(\left[[\bA\bx-\bb]_j\right]_+\right)^2
    \end{array}
    \right)^{1/2}\cdot \frac{2\|\bar{\bx}-\bx\|}{\zeta}+\frac{1}{2}\|\bx-\bar{\bx}\|^2,
\end{align*}
\end{small}which implies \eqref{eq:regular_cond_gamma_lm} with $\gamma=16/\zeta^2$.
Conversely, consider an instance with an objective $f(\bx)=(x_1+1)^2+x_2^2$ and $\X=\{\bx\in\mathbb{R}^2: 0\leq x_1 \leq 1, -1\leq x_2\leq 1\} $ and a single nonlinear constraint $g(\bx):=-x_1 \leq 0$. Apparently, $\bzero$ is the unique KKT point, but the LICQ does not hold at~$\bzero$. Also, it is easy to see that $J_g(\bzero)=\{1\}$ and $J_A (\bzero)$ is the index corresponding to the constraint $x_1\geq 0$. In addition, either $\mathcal{S}(I_g, I_A) =\{\bx\in\mathbb{R}^2: x_1=0, -1\leq x_2\leq 1\}$ or $\mathcal{S}(I_g, I_A) =\{\bx\in\mathbb{R}^2:0\leq x_1 \leq 1, -1\leq x_2\leq 1\}$ for any $I_g\subset J_g(\bzero)$ and $I_A\subset J_A (\bzero)$. In either case,  \eqref{eq:regular_cond_gamma_lm} holds for some $\gamma\geq0$ for any $\bx\in\X$ according to the Hoffman's error bound~\cite{hoffman1952approximate}. This completes the proof.
\end{proof}

\bibliography{references}
\end{document}