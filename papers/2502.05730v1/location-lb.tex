\section{Lower Bound for Location Estimation of Symmetric Distributions}

In the asymptotic setting, symmetry is a strong enough condition to attain the Fisher information rate \cite{stone1975adaptive}. In contrast, we will show that for any number of samples $n$, there is a symmetric distribution where any estimator $\est(X)$ will incur error arbitrarily larger than the two-point testing rate (even incurring error worse than $\omega_D(C)$ for a constant $C>0$, which is a much weaker goal than the typical $\omega_D(\tilde{\Theta}(\tfrac{1}{n}))$):

\loclb*
\begin{proof}
We first remark that the constants in our theorem statement are semi-arbitrary. 
Additionally, the $\omega_{D}(\frac{1}{10})$ yielded by our construction will be strictly positive, otherwise the theorem could be vacuously true. Let us begin with some intuition for constructing the distribution. Consider the uniform distribution $\unif(\mu-1,\mu+1)$: it is well-known that the optimal error for estimating $\mu$ from $n$ samples is $\Theta(\frac{1}{n})$ (by taking the midpoint of the minimum sample and the maximum sample). Now, consider modifying the uniform distribution by discretizing the domain $[\mu-1,\mu+1]$ it into $T \gg n$ equally-sized buckets, and then for a random half of the buckets we set the density to $0$, while we double the density for the other half of the buckets. Even if we are told the new modified distribution, it does not seem significantly easier to estimate its mean compared to the original uniform distribution. However, the two-point testing rate dramatically changes. For most modified distributions, there will be a large distance between itself and any translation larger than $\frac{2}{T}$, as roughly half of the domain will correspond to $x$ where one translation has density $0$, and the other translation has density $1$.

Our proof will aim to capture a similar intuition, where we discretize the domain into $4T$ buckets, and define a randomly modified version of the uniform distribution over these buckets that is always symmetric. $\mathcal{F}_D$ will be our family of modified distributions, and our goal will be to show that there exists a $D \in \mathcal{F}_D$ where:

\begin{enumerate}
    \item $\omega_D(\frac{1}{10}) \le \frac{1}{T}$
    \item $\min_{\est} \max_\mu \Pr_{X \sim D(x-\mu)^{\otimes n}}[|\est(X)-\mu| \ge \nu \cdot \frac{1}{T}] \ge \frac{1}{4}$
\end{enumerate}

Together, these two properties would imply our entire theorem. It appears simple to hand-design distributions where property (2) holds, but property (1) is more inconvenient (e.g. because any distribution that nearly has some small periodicity will not satisfy this property). Hence, our proof will show the existence of such a $D$ by the probabilistic method: a uniformly random $D$ sampled from $\mathcal{F}_D$ will have both properties with positive probability. For random $D$ from our family, property (1) will be simpler to prove, but property (2) will become slightly more involved.

Let us begin by defining $D_v$, a distribution parameterized by a vector $v \in \{0,1\}^T$. We can think of $D_v$ as having discretized the domain $[-1,+1]$ into $4T$ buckets, and we consider buckets in batches of $4$. Each batch $i \in \{0,\dots,T-1\}$ will consist of two adjacent buckets corresponding to $[i \cdot \frac{1}{T},i \cdot \frac{1}{T} + \frac{1}{2T})$ and $[i \cdot \frac{1}{T} + \frac{1}{2T},(i+1) \cdot \frac{1}{T})$, and the two buckets when mirrored over $0$. Depending on $v_i$, one of the two adjacent buckets will have density $1$ and the other will have density $0$, while the mirrored buckets will have the mirrored values; this will enforce that each batch contains constant probability mass, and that the distribution is symmetric. We formally define the distribution:

\begin{definition}[Modified symmetric uniform distribution]
    Let $v$ be a vector in $\{0,1\}^T$, then $D_v$ is the distribution:
     \begin{align*}
     \D_v(x) = \begin{cases} 
          v_i & |x| \in [i \cdot \frac{1}{T},i \cdot \frac{1}{T} + \frac{1}{2T}) \textrm{ for } i \in \{0,\dots,T-1\}\\
          1-v_i & |x| \in [i \cdot \frac{1}{T} + \frac{1}{2T},(i+1) \cdot \frac{1}{T}) \textrm{ for } i \in \{0,\dots,T-1\}\\
          0 & |x| \ge 1
       \end{cases}
    \end{align*}
\end{definition}

Our family $\mathcal{F}_D$ will be the collection of all $2^T$ possible $D_v$. We will first show that for a uniformly random $D_v$ from $\mathcal{F}_D$, that $\omega_{D_v}(\frac{1}{10})$ is probably small:

\begin{lemma} \label{lemma:twopoint-lookgood} There exists a universal constant $T_0 > 0$ such that for any integer $T \ge T_0$:
    \begin{equation*}
        \Pr_{D_v \sim \mathcal{F}_D}\left[\omega_{D_v}\left(\frac{1}{10}\right) \le \frac{1}{T}\right] \ge \frac{3}{4}
    \end{equation*}
\end{lemma}
\begin{proof}
    Recall how $w_{D_v}(\eps) \triangleq \sup \{|\theta| \, | \, \dhsq(D_v(x),D_v(x-\theta)) \le \eps \}$. Hence, to show our lemma it will be sufficient to show that $\dhsq(D_v(x),D_v(x-\theta)) > \frac{1}{10}$ for $\theta \ge \frac{1}{T}$.

    We first remark that if $|\theta| > \frac{1}{5}+\frac{1}{T}$ then $\dhsq(D_v(x),D_v(x-\theta)) > \frac{1}{10}$ for any value of $v$ and sufficiently large $T$. We may then just focus on obtaining a lower bound for:
    \begin{align*}
        & \min_{|\theta| \in [\frac{1}{T},\frac{1}{5}+\frac{1}{T}]} \dhsq(D_v(x),D_v(x-\theta)) \intertext{Note that since our distributions always have values $D_v(x)$ of $0$ or $1$, then the squared Hellinger distance is equal to the total variation distance:}
        & = \min_{|\theta| \in [\frac{1}{T},\frac{1}{5}+\frac{1}{T}]} \dtv(D_v(x),D_v(x-\theta)) \intertext{Additionally, the total variation distance interpolates linearly between the $\theta$ for the previous multiple of $\frac{1}{T}$ to the next multiple. Accordingly, the distance is at least the distance of the centering from the adjacent multiples:}
        & \ge \min_{|\theta| \in \{\frac{1}{T},\frac{2}{T},\dots, \lceil \frac{T}{5} + 1 \rceil \cdot \frac{1}{T}\}} \dtv(D_v(x),D_v(x-\theta)) 
    \end{align*}
    This shows us that it is sufficient to consider the total variation distance for a bounded number of translations, which will be easier to work with:
    \begin{align*}
        & \Pr_{D_v \sim \mathcal{F}_D}\left[\omega_{D_v}\left(\frac{1}{10}\right) > \frac{1}{T}\right]\\
        & \le \Pr_{D_v \sim \mathcal{F}_D}\left[\min_{|\theta| \in \{\frac{1}{T},\frac{2}{T},\dots, \lceil \frac{T}{5} \rceil \cdot \frac{1}{T} + \frac{1}{T} \}} \dtv(D_v(x),D_v(x-\theta)) \le \frac{1}{10}\right] \intertext{By union bound:}
        & \le \left(2 \cdot \left\lceil \frac{T}{5} + 1 \right\rceil\right) \cdot \max_{|\theta| \in \{\frac{1}{T},\frac{2}{T},\dots, \lceil \frac{T}{5} \rceil \cdot \frac{1}{T} + \frac{1}{T} \}} \cdot \Pr_{D_v \sim \mathcal{F}_D}\left[\dtv(D_v(x),D_v(x-\theta)) \le \frac{1}{10}\right] \numberthis \label{step:translate-prob}
        \end{align*}
        We now bound this probability. For every point $x \in (-1,+1)$, we call $\operatorname{Batch}(x) \triangleq \lfloor |x| \cdot T \rfloor$ the batch of four buckets related to this domain point. Accordingly, the density of $D_{v}(x)$ is only affected by $v_{\operatorname{Batch}(x)}$, and the density of $D_{v}(x-\theta)$ is only affected by $v_{\operatorname{Batch}(x-\theta)}$. Our goal is to examine subsets of the domain where the density of $D_v(x)$ is determined by a disjoint set of coordinates from those that determine the density of $D_v(x-\theta)$. Then, we may hope to lower bound their total variation distance by the sum of i.i.d. random variables.

        Without loss of generality, suppose $\theta > 0$. We will choose subsets of the domain that are to the right of $\theta$, starting with $[\theta,\theta+\frac{1}{T}), [\theta + \frac{1}{T}, \theta + \frac{2}{T}),\dots, [2\theta -\frac{1}{T},2\theta)$. However, starting at $[2\theta,2\theta+\frac{1}{T})$ we observe that this batch for $P_v(x-\theta)$ is the same as the batch of $[\theta,\theta + \frac{1}{T})$ for $P_v(x)$. To avoid this issue, we will choose alternating sets of batches: including the first $\theta T$ segments of length $\frac{1}{T}$, then skipping the next $\theta T$, then including the next $\theta T$, and so on, stopping at $\theta + 1$. Throughout this process, we will include at least $\lceil T/2 \rceil$ segments of length $\frac{1}{T}$, where each segment will either deterministically contribute total variation distance of $\frac{1}{2T}$ (if the segment is not within $(-1,+1)$), or will i.i.d. contribute total variation distance of $0$ or $\frac{1}{2T}$ with equal probability. We may now conveniently bound the probability:
        \begin{align*}
            \cref{step:translate-prob} & \le \left(2 \cdot \left\lceil \frac{T}{5} + 1 \right\rceil\right) \cdot \max_{|\theta| \in \{\frac{1}{T},\frac{2}{T},\dots, \lceil \frac{T}{5} \rceil \cdot \frac{1}{T} + \frac{1}{T} \}} \cdot \Pr_{D_v \sim \mathcal{F}_D}\left[\left(\sum_{k=1}^{\lceil T/2 \rceil} \bern\left(\frac{1}{2}\right) \cdot \frac{1}{2T} \right) \le \frac{1}{10}\right] \\
            & \le \left(\frac{2T}{5} + 4\right) \cdot 2 \cdot \exp\left( \frac{-2 \cdot (1/80)^2}{\lceil \frac{T}{2} \rceil (1/2T)^2}\right)\\
            & \le  \left(\frac{2T}{5} + 4\right) \cdot \exp\left( \frac{-T}{800}\right)
        \end{align*}
        For sufficiently large $T$, this quantity is upper bounded by $\frac{1}{4}$ (or any chosen constant).
\end{proof}

\cref{lemma:twopoint-lookgood} indicated that a random $D_v \sim \mathcal{F}_D$ often has a very optimistic two-point testing lower bound. Next remains to show a minimax-style lower bound for most $D_v$ that implies this is unattainable. Let us define a packing as is typically used in techniques like LeCam's method:

\begin{definition}
    A family of $m$ distributions $P_1,\dots, P_m$ with corresponding parameters $\Theta_1,\dots,\Theta_m$ is an $\eps$-packing of size $m$ if for all $i \ne j$ it holds that $|\theta_i-\theta_j| \ge 2\eps$.
\end{definition}

We now show a general lower bound that applies when random samples from some $P_i$ will often have some $P_j$ where the sample has at least as large of a likelihood. In other words, if samples from some distribution often look at least as likely to be from some other distribution in the packing, it will be difficult to determine which distribution samples come from. We expect this style of lower bound has appeared in many works before:

\begin{lemma}\label{lemma:other-likely}
    Consider an $\eps$-packing of size $m$: $P_1,\dots,P_m$, where each $P_i$ is a distribution supported over $\R^d$. Suppose for all $i \in [m]$ it holds that:
    \begin{equation*}
        \Pr_{X \sim P_i}\left[\left(\max_{j \ne i} P_j(x) \right) \ge P_i(x)\right] \ge \alpha.
    \end{equation*}

    Then, $\min_{\est} \max_{i \in [m]} \Pr_{X \sim P_i, \est}[|\est(X)-\theta_i| \ge \eps] \ge \alpha/2$.
\end{lemma}
\begin{proof}
    \begin{align*}
        & \min_{\est} \max_{i \in [m]} \Pr_{X \sim P_i, \est}[|\est(X)-\theta_i| \ge \eps] \\
        & \ge \frac{1}{m} \cdot \min_{\est} \sum_{i \in [m]} \Pr_{X \sim P_i, \est}[|\est(X)-\theta_i| \ge \eps] \\
        & = \frac{1}{m}   \cdot\min_{\est} \int_{\R^d} \left( \sum_{i \in [m]} \Pr_{\est}[|\est(x)-\theta_i| \ge \eps ] \cdot p_i(x) \right) \diff x \intertext{This minimum over $\est$ is attained for each value of $x$ by the estimator that estimates $\theta_{i^*}$ for $i^* \triangleq \argmax_{i^*} p_{i^*}(x)$:}
        & = \frac{1}{m}  \cdot\int_{\R^d} \left( \left( \sum_{i \in [m]} p_i(x) \right) - \max_{i^*} p_{i^*}(x) \right) \diff x \intertext{For each value of $x$, we may relate this quantity to the total probability from $p_i(x)$ for each $p_i(x)$ that is not the unique maximum:}
        & \ge \frac{1}{m}  \cdot \int_{\R^d} \left( \sum_{i \in [m]} \frac{p_i(x)}{2} \cdot \mathbbm{1}\left[\left(\max_{j \ne i} p_j(x) \right) \ge p_i(x)\right] \right) \diff x\\
        & = \frac{1}{m} \cdot \sum_{i \in [m]} \frac{1}{2} \cdot \Pr_{X \sim P_i}\left[\left( \max_{j \ne i} p_j(x) \right) \ge p_i(x)\right]\intertext{Finally, using the main assumption of our lemma:}
        & \ge \frac{\alpha}{2} \tag*{\qedhere} 
    \end{align*}
    \end{proof}

Now we will show that most $D$ in $\mathcal{F}_D$ have a packing of their translations that satisfies this property. For an integer $m$ (we defer this choice until later), we choose a collection $\theta_1,\dots,\theta_m$ such that:
\begin{enumerate}
    \item For all $i \ne j$ it holds that $|\theta_i - \theta_j| \ge \frac{1}{100nm^2}$
    \item All $\theta_i$ are multiples of $\frac{1}{T}$
    \item All $|\theta_i| \le \frac{1}{100nm}$
    \item $m \ge 2^n \ln(100m)$ \label{item:a}
    \item $\frac{n^2 m}{T} \le \frac{1}{100m}$ \label{item:b}
    \item $\frac{nm^2}{T} \le \frac{1}{50m}$
\end{enumerate}
Later, (1) will dictate how good of a lower bound we can get from this packing, while the remaining properties will enable that it is not possible to estimate which is the true $\theta_i$. We now set parameters to satisfy these properties. Setting $m = 9 \cdot 2^n \cdot n$ will satisfy (4), using $n \ge 1$. Setting $T \ge \lceil 100nm^3 \rceil = \lceil 100 \cdot 9^3 \cdot 2^{3n} \cdot n^4 \rceil$ will satisfy (5) and (6). Finally, if we seek to pack $\theta_i$ such that all $|\theta_i| \le \frac{1}{100nm}$ and all $\theta_i$ are multiples of $\frac{1}{T}$, then there exists such a packing where all $i\ne j$ satisfy $|\theta_i - \theta_j| \ge \frac{2/(100nm)}{m} - \frac{1}{T} \ge \frac{1}{50nm^2} - \frac{1}{100n^2m^2} \ge \frac{1}{100nm^2}$, satisfying (1).

With this packing of $\theta_i$ in hand, for a given $D_v$ we define translations $D_{v,1},\dots,D_{v,m}$, where $D_{v,i}(x) \triangleq D_v(x - \theta_i)$. We prove the crucial property required to use the probabilistic method to invoke \cref{lemma:other-likely}:

\begin{lemma}\label{lemma:rand-has-overlap}
    $\Pr_{D_v \sim \mathcal{F}_D}\left[\min_i \Pr_{X \sim D_{v,i}^{\otimes n}}\left[(\max_{j \ne i} D_{v,j}^{\otimes n}(x) ) \ge D_{v,i}^{\otimes n}(x)\right] \ge \frac{1}{2}\right] \ge \frac{9}{10}$
\end{lemma}
\begin{proof}
    Note that the constant $\frac{1}{2}$ in the lemma statement could be an arbitrary constant in $(0,1)$. Let us focus first on showing this claim for a particular $i$, instead of the minimum $i$:

    \begin{claim}\label{claim:each-i}
        For any $i \in [m]$:
        \begin{equation*}
            \Pr_{D_v \sim \mathcal{F}_D}\left[\Pr_{X \sim D_{v,i}^{\otimes n}}\left[(\max_{j \ne i} D_{v,j}^{\otimes n}(x) ) \ge D_{v,i}^{\otimes n}(x)\right] \ge \frac{1}{2}\right] \ge 1 - \frac{1}{10m}
        \end{equation*}
    \end{claim}
    \begin{proof}
        We will first try relate the desired quantity (a probability over distributions $\mathcal{F} \sim D_v$ that samples from a translation will have some property) to a more natural quantity (the probability of an event jointly over $D_v$ and its samples from a translation $X \sim D_{v,i}^{\otimes n}$):
        \begin{align*}
            & \Pr_{D_v \sim \mathcal{F}_D}\left[\Pr_{X \sim D_{v,i}^{\otimes n}}\left[\left(\max_{j \ne i} D_{v,j}^{\otimes n}(x) \right) \ge D_{v,i}^{\otimes n}(x)\right] \ge \frac{1}{2}\right]\\
            & = 1 - \Ex_{D_v \sim \mathcal{F}_D}\left[\mathbbm{1}\left[ \Pr_{X \sim D_{v,i}^{\otimes n}}\left[\left(\max_{j \ne i} D_{v,j}^{\otimes n}(x) \right) \ge D_{v,i}^{\otimes n}(x)\right] < \frac{1}{2} \right]\right]\\
            & \ge 1 - \Ex_{D_v \sim \mathcal{F}_D}\left[2 \cdot \left( 1 -  \Pr_{X \sim D_{v,i}^{\otimes n}}\left[\left(\max_{j \ne i} D_{v,j}^{\otimes n}(x) \right) \ge D_{v,i}^{\otimes n}(x)\right] \right) \right]\\
            & = 2 \cdot \Pr_{D_v \sim \mathcal{F}_D, X \sim D_{v,i}^{\otimes n}}\left[\left(\max_{j \ne i} D_{v,j}^{\otimes n}(x) \right) \ge D_{v,i}^{\otimes n}(x)\right] - 1 \numberthis \label{step:pr-now}
        \end{align*}
        Now we are analyzing the probability that if we take a random distribution $D_v \sim \mathcal{F}_D$, and we sample from one translation of this distribution $X \sim D_{v,i}^{\otimes n}$, the probability that our sample is at least equally likely to be from some other translation $D_{v,j}^{\otimes n}$ where $i\ne j$.\\
        Our main intuition will be that for $T \gg n,m$, the realization of $D_{v,i}^{\otimes n}$ for most samples will almost be independent of $D_{v,j}^{\otimes n}$ for every $i \ne j$, in the sense that for a sample $x \sim D_{v,i}$ it is only necessary to realize one coordinate of $v$ which may not be the coordinate relevant to the other translation. Moreover, if they were truly independent, then the probability of some $D_{v,j}^{\otimes n}$ also being supported on all the samples is $2^{-n}$, so if $m \gg 2^n$ we might expect our desired property to hold.\\
        Let us try formalize this event of independence: $\mathcal{E}$. For every point $x \in (-1,+1)$, we call $\operatorname{Batch}(x) \triangleq \lfloor |x| \cdot T \rfloor$ the batch of four buckets related to this domain point. Equivalently, the density of $D_{v,i}(x)$ is only affected by $v_{\operatorname{Batch}(x-\theta_i)}$. We refer to $\mathcal{E}$ as the event that for all $a \in [n]$ and $b \in [m]$: (i) all $x_a \in (\theta_b-1,\theta_b+1)$, and (ii) all values of $\operatorname{Batch}(x_a-\theta_b)$ are $nm$ distinct values. For large $T$, we expect $\mathcal{E}$ to almost always occur, so it should not be too lossy to focus on the occurrences of our event that also have $\mathcal{E}$:
        \begin{align*}
            \cref{step:pr-now}& \ge 2 \cdot \Pr_{D_v \sim \mathcal{F}_d, X \sim D_{v,i}^{\otimes n}}\left[ \mathcal{E} \right] \cdot \Pr_{D_v \sim \mathcal{F}_d, X \sim D_{v,i}^{\otimes n}}\left[ \left(\max_{j \ne i} D_{v,j}^{\otimes n}(x) \right) \ge D_{v,i}^{\otimes n}(x)|  \mathcal{E} \right] - 1\intertext{To lower bound the probability of $\mathcal{E}$, we consider a collection of causes why the event may fail. First, some $x_a \notin (\theta_b-1,\theta_b+1)$, has probability at most $\frac{1}{2} \cdot \max_{i,j} |\theta_i - \theta_j| \le \max_j |\theta_j|\le \frac{1}{100nm}$ for a single sample and we union bound to $\frac{1}{100m}$ over all samples. Second, some $x_a$ satisfies $\operatorname{Batch}(x_a-\theta_{b_1})=\operatorname{Batch}(x_a-\theta_{b_2})$ for $b_1\ne b_2$ and $b_1,b_2\in [m]$, has probability at most $\frac{\binom{m}{2}}{2T}$ for a single sample and we union bound to $\frac{m^2n}{2T} \le \frac{1}{100m}$ over all samples. Third, some sample $x_{a_2}$ satisfies $\operatorname{Batch}(x_{a_1}-\theta_{b_1})=\operatorname{Batch}(x_{a_2}-\theta_{b_2})$ for $a_1<a_2$, with $a_1,a_2 \in [n]$ and $b_1,b_2 \in [m]$, has probability at most $\frac{nm}{T}$ for a single sample $x_{a_2}$ and we union bound to $\frac{n^2m}{T} \le \frac{1}{100m}$ over all samples. Combining these:}
            & \ge 2 \cdot \left(1 - \frac{3}{100m}\right) \cdot \Pr_{D_v \sim \mathcal{F}_d, X \sim D_{v,i}^{\otimes n}}\left[ \left(\max_{j \ne i} D_{v,j}^{\otimes n}(x) \right) \ge D_{v,i}^{\otimes n}(x)|  \mathcal{E} \right] - 1\intertext{Observe how the event $\mathcal{E}$ was not actually affected by the realization of $D_v$, it was only affected by which segments of length $\frac{1}{T}$ had samples realized within them, and these have the same joint probabilities for all $D_v \in \mathcal{F}_D$. Moreover, by definition of $\mathcal{E}$, each sample is within the potential support of each $D_{v,j}$, and all values of $\operatorname{Batch}(x_{a}-\theta_{b})$ are distinct, so the events of whether a $D_{v,j}(x) > 0$ for $i \ne j$ are exactly i.i.d. Bernoulli random variables with probability $2^{-n}$:}
            & = 2 \cdot \left(1 - \frac{3}{100m}\right) \cdot \left(1 - \left(1 - 2^{-n} \right)^m \right) - 1\\
            & \ge 2 \cdot \left(1 - \frac{3}{100m}\right) \cdot \left(1 - e^{\frac{-m}{2^n}} \right) - 1 \intertext{Using $m \ge 2^n \ln(100m)$:}
            & \ge 2 \cdot \left(1 - \frac{3}{100m}\right) \cdot \left(1 - \frac{1}{100m} \right) - 1\\
            & \ge 1 - \frac{8}{100m} \tag*{\qedhere} 
        \end{align*}
    \end{proof}
    We may conclude our entire lemma with the $\max_{i}$ quantifier by invoking \cref{claim:each-i} over each of the $m$ translations and using a union bound.
\end{proof}
Combining \cref{lemma:other-likely,lemma:rand-has-overlap}, we obtain:
\begin{corollary}\label{cor:minimax-err}
\begin{equation*}
    \Pr_{D_v \sim \mathcal{F}_D}\left[\min_{\est} \max_{\mu} \Pr_{X \sim D_v(x-\mu)^{\otimes n}, \est}\left[|\est(X)-\theta_i| \ge \frac{1}{200nm^2}\right] \ge \frac{1}{4} \right] \ge \frac{9}{10}
\end{equation*}
\end{corollary}

Finally, by combining \cref{lemma:twopoint-lookgood,cor:minimax-err}, the probabilistic method implies existence of a $D_v$ with the following properties:
\begin{enumerate}
    \item $\omega_{D_v}(\frac{1}{10}) \le \frac{1}{T}$
    \item $\min_{\est} \max_{\mu} \Pr_{X \sim D_v(x-\mu)^{\otimes n}, \est}[|\est(X)-\mu| \ge \frac{1}{200nm^2}] \ge \frac{1}{4}$
\end{enumerate}
This immediately yields our desired result by setting $T$ to be sufficiently large.
\end{proof}






