\section{Lower Bound for Adaptive Location Estimation of Symmetric, Unimodal Distributions}

We now aim to prove that it is not possible to adaptively attain the two-point testing rates if the distribution is only promised to be symmetric and unimodal. In our positive result, we focused on how indicators of intervals witness distance between log-concave mixtures and their translations. Looking inside this proof more, we leveraged how one could roughly threshold the likelihood ratio by looking at an interval of the domain.

In designing our hard instance, we seek to design a distribution where the likelihood ratio with its translation is large in regions that are very spaced apart. Moreover, if we consider a family of such distributions with different spacings, then we hope to show that it is impossible to attain the two-point testing rate. For a more visual depiction, consider the step distribution in \cref{fig:step}, which is a unimodal and symmetric distribution that resembles a collection of steps. Comparing this distribution with a slight translation in \cref{fig:step}, we see that the likelihood is strictly greater than $1$ in regions that are spaced apart. Our lower bound will consist of a family of distributions where the step width is random. Then, we will not know where to look for the spikes in the likelihood ratio. In fact, our proof will proceed by showing that a family of random step distributions is indistinguishable from a triangle with the same center. We then show that triangles have a much larger two-point testing lower bound than any step distribution in our family, concluding our proof.

\adaptivelb*
\begin{proof}
Let us define some relevant distributions in terms of a sample size $n \ge 1$, and parameter $0 < \eps < 1$ where $\frac{1}{2 \eps}$ is an integer. 



\begin{definition}[Triangle Distribution]
     \begin{align*}
     \tri(x) = \begin{cases} 
          1-|x| & |x| \le 1 \\
          0 & |x| > 1 
       \end{cases}
       \end{align*}
\end{definition}

Before we define step distributions, let us define a helper function $s_w(x)$ which defines a function with three steps: 

\begin{definition} The function $s_w(x)$ has $0 \le w < \eps/2$ and is supported on $[0,\eps)$ such that:
    \begin{align*}
     s_w(x) = \begin{cases} 
          0 & 0 \le x<\eps/2-w\\
          \eps/2  & \eps/2-w \le x \le \eps/2+w \\
          \eps & \eps/2+w \le x < \eps
       \end{cases}
    \end{align*}
\end{definition}

Although not important yet, $s_w(x)$ was designed such that if we sample $w \sim \unif(0,\eps/2)$, then its marginal is identical to the line $f(x)=x$ on $[0,\eps)$. We now define the step distribution:

\begin{definition}[Step Distribution] Let $v$ be a vector of length $\frac{1}{2\eps}$, where each $v_i \in [0,\eps/2]$. The parameter $v_i$ informs the length of the $i$-th step:
     \begin{align*}
     \step_v(x) = \begin{cases} 
          1 - (i+1)\eps + s_{v_i}((i+1)\eps - |x|) & |x| \in [i\eps,(i+1)\eps) \textrm{ for } i \in \{0,\dots,\frac{1}{2\eps}-1\}\\
          1-|x| & \frac{1}{2} \le |x| < 1 \\
          0 & |x| \ge 1
       \end{cases}
    \end{align*}
\end{definition}

Now, consider the mixture where we sample $\frac{1}{2\eps}$ i.i.d. variables $v_i \sim \unif(0,\eps/2)$ and then receive $n$ samples from $\step_v(x)$:

\begin{definition}[Mixture of Step Distributions]
\begin{align*}
    \rstep_v(x_1,\dots,x_n) = \Ex_{v_1,\dots,v_n \sim \unif(0,\eps/2)}[\Pi_{i=1}^n[\step_v(x_i)]]
\end{align*}
\end{definition}

With these definitions, we can concretely outline our agenda. Let $\Delta_{\textrm{step}}$ be the largest two-point testing lower bound for any valid step distribution, and $\Delta_{\textrm{tri}}$ be a value such that $\dtv(\tri(x)^{\otimes n},\tri(x-\Delta_{\textrm{tri}})^{\otimes n})$ is small. We will observe that $\Delta_{\textrm{step}} \ll \Delta_{\textrm{tri}}$. Then, if we show $\dtv(\rstep(x_1,\dots,x_n),\tri(x)^{\otimes n})$ is small, this would imply $\dtv(\rstep(x_1,\dots,x_n),\rstep(x_1-\Delta_{\textrm{tri}},\dots,x_n-\Delta_{\textrm{tri}}))$ is small, and thus that for any algorithm there is at least one step distribution where it incurs error $\Delta_{\textrm{tri}}$ with at least constant probability. However, since $\Delta_{\textrm{step}} \ll \Delta_{\textrm{tri}}$, this implies that we cannot attain the two-point testing bound for the step distribution.

The bulk of our effort will be in proving that $\dtv(\rstep(x_1,\dots,x_n),\tri(x)^{\otimes n})$ is small. To do so, we will compute an upper bound on their $\chi^2$ divergence. In an effort to simplify calculations, we will now introduce two modified distributions $\mtri,\rmstep$, that we design to have smaller distance than $\tri,\rstep$ by a data-processing inequality argument: as we show there is a deterministic function $h$ where $h(\mtri)=\tri$ and each $h(\mstep_v)=\step_v$, so $\dtv(\tri^{\otimes n},\rstep^{\otimes n}) = \dtv(h(\mtri)^{\otimes n},h(\rmstep)^{\otimes n}) \le \dtv(\mtri^{\otimes n},\rmstep^{\otimes n})$. Moreover, we design $\mstep$ so that it is easier to work with because each step interval $[i\eps,(i+1)\eps)$ will be identical (as opposed to the original distributions that have different heights). We design

\begin{definition}[Modified Triangle Distribution]

       \begin{align*}
     \mtri(x) = \begin{cases} 
          \frac{1}{2} + (i+1)\eps - |x| & |x| \in [i\eps,(i+1)\eps) \textrm{ for } i \in \{0,\dots,\frac{1}{2\eps}-1\}\\
          1-|x| & \frac{1}{2} \le |x| < 1 \\
          \frac{1}{2}-(i+1)\eps & |x| \in [1+i\eps,1+(i+1)\eps) \textrm{ for } i \in \{0,\dots,\frac{1}{2\eps}-1\}\\
          0 & |x|>\frac{3}{2}
       \end{cases}
    \end{align*}
\end{definition}

\begin{definition}[Modified Step distribution] Let $v$ be a vector of length $\frac{1}{2\eps}$, where each $v_i \in [0,\eps/2]$. The parameter $v_i$ informs the length of the $i$-th step:

    \begin{align*}
     \mstep_v(x) = \begin{cases} 
          \frac{1}{2} + s_{v_i}((i+1)\eps - |x|) & |x| \in [i\eps,(i+1)\eps) \textrm{ for } i \in \{0,\dots,\frac{1}{2\eps}-1\}\\
          1-|x| & \frac{1}{2} \le |x| < 1 \\
          \frac{1}{2} - (i+1)\eps & |x| \in [1+i\eps,1+(i+1)\eps) \textrm{ for } i \in \{0,\dots,\frac{1}{2\eps}-1\}\\
          0 & |x|>\frac{3}{2}
       \end{cases}
    \end{align*}
\end{definition}

We now give our function $h$:

\begin{definition}[Deterministic Mapping $h$]
    \begin{align*}
     h(x) = \begin{cases} 
          x & |x|<1 \textrm{ or } |x| \ge \frac{3}{2}\\
          x-1 & 1 \le x < \frac{3}{2}\\
          x + 1 & -\frac{3}{2}< x \le -1
       \end{cases}
    \end{align*}
\end{definition}

\begin{claim}\label{claim:use-dpi}
    $\dtv(\tri^{\otimes n},\rstep^{\otimes n}) \le \dtv(\mtri^{\otimes n},\rmstep^{\otimes n})$
\end{claim}
\begin{proof}
    Note how $\tri=h(\mtri)$ and $\step_v = h(\mstep_v)$. Thus, 
    $\dtv(\tri^{\otimes n},\rstep^{\otimes n}) = \dtv(h(\mtri)^{\otimes n},h(\rmstep)^{\otimes n}) \le \dtv(\mtri^{\otimes n},\rmstep^{\otimes n})$ by data-processing inequality.
\end{proof}

Now, we bound $\dtv(\mtri^{\otimes n},\rmstep^{\otimes n})$ by analyzing $\dchi(\rmstep^{\otimes n}\| \mtri^{\otimes n})$, via a mostly routine calculation.

\begin{lemma}\label{lemma:hard-lb}
    There exists a universal constant $C>0$ such that, for any $\eps \le \frac{1}{2}$, if $n \le \frac{C}{\eps^{2.5}}$ then $\dtv(\rmstep^{\otimes n},  \mtri^{\otimes n}) \le \frac{1}{10}$.
\end{lemma}
\begin{proof}
Let us define $p_0(x) \triangleq \mtri(x)$, let $p_v(x) \triangleq \mstep_v(x)$, and let $k \triangleq \frac{1}{2\eps}$. Then:
\begin{align*}
    & \dchi(\rmstep^{\otimes n}\| \mtri^{\otimes n})\\
    & = \Ex_{v,w \sim \unif(0,\eps/2)^{k}}\left[\int_{x_1,\dots,x_n} \Pi_{i=1}^n \frac{p_v(x_i)p_w(x_i)}{p_0(x_i)}\right] -1 \\
    & = \Ex_{v,w \sim \unif(0,\eps/2)^{k}}\left[\left(\int_{-\infty}^\infty \frac{p_v(x)p_w(x)}{p_0(x)} dx\right)^n\right] -1 \\
    \intertext{For ease of notation, let us denote $f(v,w) \triangleq \int_{-\infty}^\infty \frac{p_v(x) p_w(x)}{p_0(x)} dx$}
    & = \Ex_{v,w \sim \unif(0,\eps/2)^{k}}\left[f(v,w)^n\right] -1 \numberthis \label{step:chi2-f} \\
    \intertext{We will hence aim to bound $f(v,w)$:}
    & f(v,w) \triangleq \int_{-\infty}^\infty \frac{p_v(x)p_w(x)}{p_0(x)} dx \\
    \intertext{Now, we use the actual values of $p_0$ and $p_v$ to start calculating the integral. Note how $p_0$ and $p_v$ are symmetric around $0$ for all $v$, all $v$ satisfy $p_0(x)=p_v(x)$ for $|x|>\frac{1}{2}$, and $p_0(x)=0$ for $|x|>\frac{3}{2}$.}
    & = 2 \int_{0}^{1/2} \frac{p_v(x) p_w(x)}{p_0(x)} dx + 2 \int_{1/2}^1 p_0(x) dx + 2 \int_1^{3/2} p_0(x) dx\\
    & = 2 \int_{0}^{1/2} \frac{p_v(x) p_w(x)}{p_0(x)} dx + 2 \int_{0}^{1/2} x \, dx + 2 \sum_{i=0}^{k-1} i \eps^2 \\
    & = 2 \int_{0}^{1/2} \frac{p_v(x) p_w(x)}{p_0(x)} dx + \frac{1}{4} +\left(\frac{1}{4}-\eps/2\right) \\
    & = \frac{1}{2} - \eps/2 +  2 \sum_{i=0}^{k-1} \int_0^\eps \frac{(\frac{1}{2}+s_{v_i}(x))(\frac{1}{2}+s_{w_i}(x))}{\frac{1}{2}+x} dx \\
    \intertext{To evaluate this integral, we will separate into the five intervals where $s_{v_i}(x)$ and $s_{w_i}(x)$ are constant. For ease of notation, let $a_i \triangleq \min(v_i,w_i)$ and $b_i \triangleq \max(v_i,w_i)$.}
    & = \frac{1}{2}-\frac{\eps}{2} + 2 \sum_{i=0}^{k-1} \int_0^{\eps/2-b_i} \frac{1/4}{1/2 + x} dx + \int_{\eps/2-b_i}^{\eps/2-a_i} \frac{1/2 (1/2 + \eps/2)}{1/2 + x} dx + \int_{\eps/2-a_i}^{\eps/2+a_i} \frac{(1/2 + \eps/2)^2}{1/2 + x} dx \\
    &+\int_{\eps/2+a_i}^{\eps/2+b_i} \frac{(1/2+\eps/2)(1/2+\eps)}{1/2+x} dx + \int_{\eps/2+b_i}^{\eps} \frac{(1/2+\eps)^2}{1/2+x} dx\\
    & = \frac{1}{2} - \frac{\eps}{2} + 2 \sum_{i=0}^{k-1} \frac{1}{4} \ln\left(\frac{1/2 + \eps/2 - b_i}{1/2}\right) + \left(\frac{1}{4}+\eps/4\right) \ln\left(\frac{1/2 + \eps/2-a_i}{1/2 + \eps/2-b_i}\right) + \left(\frac{1}{2} + \frac{\eps}{2}\right)^2 \ln\left(\frac{1/2 + \eps/2 + a_i}{1/2 + \eps/2 - a_i}\right) \\
    & + \left(\frac{1}{2} + \frac{\eps}{2}\right)\left(\frac{1}{2}+\eps\right)\ln\left(\frac{1/2 + \eps/2 + b_i}{1/2 + \eps/2 + a_i}\right) + \left(\frac{1}{2}+\eps\right)^2 \ln\left(\frac{1/2 + \eps}{1/2 + \eps/2 + b_i}\right)\\
    & = \frac{1}{2} - \frac{\eps}{2} + 2 \sum_{i=0}^{k-1} \frac{1}{4} \ln(2) + \frac{\eps}{4} \ln\left(\frac{1}{1/2 + \eps/2 - b_i}\right) + \left(\frac{\eps}{4}+\frac{\eps^2}{4}\right)\ln\left(\frac{1}{1/2 + \eps/2 - a_i}\right) \\
    & + \left(\frac{\eps}{4} + \frac{\eps^2}{4}\right)\ln\left(\frac{1}{1/2 + \eps/2 + a_i}\right) + \left(\frac{\eps}{4}+\frac{\eps^2}{2}\right)\ln\left(\frac{1}{1/2 + \eps/2 + b_i}\right) + \left(\frac{1}{2}+\eps\right)^2 \ln\left(\frac{1}{2}+\eps\right)\\
    \intertext{Now, we modify to make a later Taylor expansion cleaner (roughly, changing arguments from $\ln(\frac{1}{2}+x)$ to $\ln(1+2x)-\ln(2)$):}
    & = \frac{1}{2} - \frac{\eps}{2} + 2 \sum_{i=0}^{k-1} \frac{1}{4} \ln(2) + \frac{\eps}{4} \left(\ln\left(\frac{1}{1 + \eps - 2b_i}\right)+\ln(2)\right) + \left(\frac{\eps}{4} + \frac{\eps^2}{4}\right) \left(\ln\left(\frac{1}{1+\eps-2a_i}\right) + \ln(2)\right) \\
    &+ \left(\frac{\eps}{4}+\frac{\eps^2}{4}\right)\left(\ln\left(\frac{1}{1+\eps+2a_i}\right)+\ln(2)\right) + \left(\frac{\eps}{4}+\frac{\eps^2}{2}\right)\left(\ln\left(\frac{1}{1 + \eps + 2b_i}\right)+\ln(2)\right) \\
    & + \left(\frac{1}{2}+\eps\right)^2 \left(\ln(1+2\eps)-\ln(2)\right)\\
    & = \frac{1}{2} - \frac{\eps}{2} + 2 \sum_{i=0}^{k-1} \frac{\eps}{4} \ln\left(1-\frac{\eps-2b_i}{1+\eps-2b_i}\right) + \left(\frac{\eps}{4}+\frac{\eps^2}{4}\right)\ln\left(1-\frac{\eps-2a_i}{1+\eps-2a_i}\right) + \left(\frac{\eps}{4}+\frac{\eps^2}{4}\right)\ln\left(1-\frac{\eps+2a_i}{1+\eps+2a_i}\right) \\
    & + \left(\frac{\eps}{4}+\frac{\eps^2}{2}\right) \ln\left(1 - \frac{\eps+2b_i}{1+\eps+2b_i}\right) + \left(\frac{1}{2}+\eps\right)^2 \ln(1+2 \eps)\\
    & = \sum_{i=0}^{k-1} \eps - \eps^2 +\frac{\eps}{2} \ln\left(1-\frac{\eps-2b_i}{1+\eps-2b_i}\right) + \left(\frac{\eps}{2}+\frac{\eps^2}{2}\right)\ln\left(1-\frac{\eps-2a_i}{1+\eps-2a_i}\right) + \left(\frac{\eps}{2}+\frac{\eps^2}{2}\right)\ln\left(1-\frac{\eps+2a_i}{1+\eps+2a_i}\right) \\
    & + \left(\frac{\eps}{2}+\eps^2\right) \ln\left(1 - \frac{\eps+2b_i}{1+\eps+2b_i}\right) + 2\left(\frac{1}{2}+\eps\right)^2 \ln(1+2 \eps) 
\end{align*}

As will soon be more clear, for all values of $v,w$ it will be that case that $f(v,w)\approx 1$. Accordingly, to study $f(v,w)^n-1$, it may be more insightful to analyze $(1+g(v,w))^n$, where $g(v,w)\triangleq f(v,w)-1$. We define the following $g_i(\cdot)$ function so that $\sum_{i=1}^k g_i(v_i,w_i) = g(v,w) = f(v,w)-1$:
\begin{align*}
    g_i(v_i,w_i) & \triangleq -\eps  - \eps^2 +\frac{\eps}{2} \ln\left(1-\frac{\eps-2b_i}{1+\eps-2b_i}\right) + \left(\frac{\eps}{2}+\frac{\eps^2}{2}\right)\ln\left(1-\frac{\eps-2a_i}{1+\eps-2a_i}\right) + \\
    & \left(\frac{\eps}{2} +\frac{\eps^2}{2}\right)\ln\left(1-\frac{\eps+2a_i}{1+\eps+2a_i}\right) + \left(\frac{\eps}{2}+\eps^2\right) \ln\left(1 - \frac{\eps+2b_i}{1+\eps+2b_i}\right) + 2\left(\frac{1}{2}+\eps\right)^2 \ln\left(1+2 \eps\right) \\
    & \intertext{We will now bound $g_i(v_i,w_i)$. Starting with a Taylor expansion that uses $\ln(1+x) \le x - \frac{x^2}{2} + \frac{x^3}{3}$ and $\ln(1-x) \le -x$, then this is valid for $0 < \eps \le \frac{1}{2}$:}
    & \le -\eps - \eps^2 + \frac{\eps}{2} \cdot \frac{2b_i - \eps}{1 + \eps - 2b_i} + \left(\frac{\eps}{2}+\frac{\eps^2}{2}\right) \cdot \frac{2a_i - \eps}{1 + \eps - 2 a_i} + \left(\frac{\eps}{2}+\frac{\eps^2}{2}\right) \cdot \frac{-2a_i - \eps}{1 + \eps + 2a_i} \\
    &+ \left(\frac{\eps}{2}+\eps^2\right)\cdot \frac{- \eps - 2b_i}{1 + \eps + 2b_i} + 2\left(\frac{1}{2}+\eps\right)^2 \cdot \left(2\eps - 2\eps^2 + \frac{8 \eps^3}{3}\right)\\
    \intertext{Note that all terms other than the last are non-positive, as $0 < a_i,b_i \le \frac{\eps}{2}$. Now, we use $\frac{1}{1+z} = 1 - \frac{z}{1+z} \ge (1-z)$ for $z\ge 0$. }
    & \le -\eps - \eps^2 + \frac{\eps}{2} \cdot (2b_i - \eps)(1-\eps) + \left(\frac{\eps}{2}+\frac{\eps^2}{2}\right) \cdot (2a_i - \eps) \cdot (1-\eps) + \left(\frac{\eps}{2}+\frac{\eps^2}{2}\right) \cdot (-2a_i - \eps) \cdot (1-2\eps) \\
    &+ \left(\frac{\eps}{2}+\eps^2\right)\cdot (- \eps - 2b_i)(1-2\eps) + 2\left(\frac{1}{2}+\eps\right)^2 \cdot \left(2\eps - 2\eps^2 + \frac{8 \eps^3}{3}\right)\\
    & = a_i \eps^2 - b_i \eps^2 + \frac{7 \eps^3}{3} + a_i \eps^3 + 4 b_i \eps^3 + \frac{29 \eps^4}{6} + \frac{16 \eps^5}{3} \\
    & \le O(1) \cdot \eps^3 \numberthis \label{step:gi-bound}
\end{align*}

Finally, we show how this enables us to directly bound $\Ex[f(v,w)^n]-1$, picking up from \cref{step:chi2-f}:

\begin{align*}
    & \dchi(\rmstep^n\| \mtri^n) \\
    & = \Ex_{v,w \sim \unif(0,\eps/2)^k}\left[ f(v,w)^n \right] - 1 \\
    & = \Ex_{v,w \sim \unif(0,\eps/2)^k}\left[ (1+g(v,w))^n \right] - 1 \\
    & = \Ex_{v,w \sim \unif(0,\eps/2)^k}\left[ \sum_{j=0}^n \binom{n}{j} g(v,w)^j \right] -1 \\
    & = \Ex_{v,w \sim \unif(0,\eps/2)^k}\left[ \sum_{j=1}^n \binom{n}{j} g(v,w)^j \right] \\
    & = \Ex_{v,w \sim \unif(0,\eps/2)^k}\left[ \sum_{j=2}^n \binom{n}{j} g(v,w)^j \right] + n \Ex_{v,w \sim \unif(0,\eps/2)^k}\left[ g(v,w) \right]\\
    \intertext{Note how $\Ex_{v,w}\left[ g(v,w) \right]=\Ex_{v,w}\left[ f(v,w) \right]-1$ and that we designed our distributions so that $\Ex_v[p_v(x)]=p_0(x)$ for all $x$, and thus $\Ex_{v,w}\left[ g(v,w) \right]=\Ex_{v,w}\left[ f(v,w) \right]-1 = 1-1 = 0$:}
    & = \Ex_{v,w \sim \unif(0,\eps/2)^k}\left[ \sum_{j=2}^n \binom{n}{j} g(v,w)^j \right]\\
    & = \Ex_{v,w \sim \unif(0,\eps/2)^k}\left[ \sum_{j=2}^n \binom{n}{j} \cdot \left( \sum_{i=0}^{k-1} g_i(v_i,w_i) \right)^j \right]\\
    \intertext{Recall how we just used $\Ex_{v,w}[g(v,w)]=0$. As each $g_i(v_i,w_i)$ is i.i.d., then we also know $\Ex_{v,w}[g_i(v_i,w_i)]=0$ for all $i$, and when we expand the previous step, any term with an $i$ appearing exactly once will evaluate to $0$. Let us use that the number of ordered sequences of length $j$ from $k$ elements, that have no element occuring exactly once, is at most $k^{\lfloor j/2 \rfloor} \lfloor j/2 \rfloor^j \le \frac{k^{j/2} j^j}{2^j}$:}
    & \le \Ex_{v,w \sim \unif(0,\eps/2)^k}\left[ \sum_{j=2}^n \binom{n}{j} \cdot \frac{k^{j/2} j^j}{2^j} \cdot \left(\max_{i} |g_i(v_i,w_i)| \right)^j \right]\\
    & \le \Ex_{v,w \sim \unif(0,\eps/2)^k}\left[ \sum_{j=2}^n \left( \frac{en}{j} \right)^j \cdot \frac{k^{j/2} j^j}{2^j} \cdot \left(\max_{i} |g_i(v_i,w_i)| \right)^j \right]\\
    \intertext{Recall our upper bound on $g_i(v_i,w_i)$ from \cref{step:gi-bound}. Also observe that $g_i(v_i,w_i) \ge 0$, as otherwise the distribution corresponding to the $v,w$ that have each entry identical to $v_i,w_i$ would have negative $\chi^2$ divergence with $p_0$, which is impossible. Thus, our upper bound on $g_i(v_i,w_i)$ is also an upper bound on $|g_i(v_i,w_i)|$:}
    & \le \Ex_{v,w \sim \unif(0,\eps/2)^k}\left[ \sum_{j=2}^n \left( \frac{en}{j} \right)^j \cdot \frac{k^{j/2} j^j}{2^j} \cdot \left(O(1) \cdot \eps^3 \right)^j \right]\\
    & = \Ex_{v,w \sim \unif(0,\eps/2)^k}\left[ \sum_{j=2}^n \frac{e^j n^j k^{j/2} O(1)^j \eps^{3j}}{2^j}  \right]\\
    \intertext{Recall $k = \frac{1}{2\eps}$:}
    & = \Ex_{v,w \sim \unif(0,\eps/2)^k}\left[ \sum_{j=2}^n \frac{e^j n^j O(1)^j \eps^{2.5j}}{2^j}  \right]\\
    \intertext{This sum will be upper bounded by at most a constant factor more than its first term, as long as the ratio of consecutive terms is bounded above by, say, $\frac{1}{2}$. The ratio is at most $O(1) n \eps^{2.5}$, so there exists a universal constant $0<C_0<1$ such that if $n \le \frac{C_0}{\eps^{2.5}}$, then this expectation is bounded by:}
    & \le O(1) \cdot n^2 \eps^5
\end{align*}

Thus, we may conclude there is a constant $0<C<1$ such that for any $0< \eps \le \frac{1}{2}$, if $n \le \frac{C}{\eps^{2.5}}$, then $\dchi(\rmstep^{\otimes n},\mtri^{\otimes n}) \le \frac{1}{50}$. Using $\dtv(P,Q) \le \sqrt{\frac{1}{2}\cdot \dchi(P,Q)}$ (e.g. see Section 13.2.1 of \cite{duchinotes}, which also outlines the general technique of this point-mixture lower bound style used in this proof), then:

\begin{equation*}
    \dtv(\rmstep^{\otimes n},\mtri^{\otimes n}) \le \sqrt{\frac{1}{2} \cdot \dchi(\rmstep^{\otimes n},\mtri^{\otimes n})} \le \frac{1}{10} \tag*{\qedhere} 
\end{equation*}
\end{proof}

We now show that it is hard to distinguish the triangle distribution from a translated version with an appropriately chosen translation:

\begin{claim}\label{claim:tri-hard}
    There exists a constant $0<C < 1$ where, if $n \ge 2$ and we let $\Delta_{\textrm{tri}} \triangleq \frac{C}{\sqrt{\log(n) \cdot n}}$, then:
    \begin{equation*}
        \dtv(\tri(x)^{\otimes n}, \tri(x-\Delta_{\textrm{tri}})^{\otimes n}) \le \frac{1}{10}
    \end{equation*}
\end{claim}
\begin{proof}
    \begin{align*}
        & \dtv(\tri(x)^{\otimes n}, \tri(x-\Delta_{\textrm{tri}})^{\otimes n})\\
        & \le \sqrt{2} \cdot \sqrt{\dhsq(\tri(x)^{\otimes n}, \tri(x-\Delta_{\textrm{tri}})^{\otimes n})} \\
        & =\sqrt{2} \cdot \sqrt{1 - (1 - \dhsq(\tri(x), \tri(x-\Delta_{\textrm{tri}})))^n} \intertext{Observe that at least a quarter of the Hellinger distance comes from the domain $[-1,0]$:}
        & \le \sqrt{2} \cdot \sqrt{1 - \left(1 - 2 \cdot \left( \int_{0}^{1-\Delta_{\textrm{tri}}} (\sqrt{\tri(x)}-\sqrt{\tri(x+\Delta_{\textrm{tri}})})^2 \diff x + \int_{1-\Delta_{\textrm{tri}}}^1 \tri(x) \diff x \right)\right)^n}\\
        & \le \sqrt{2} \cdot \sqrt{1 - \left(1 - 2 \cdot \left( \int_{2 \cdot \Delta_{\textrm{tri}}}^{1} (\sqrt{x}-\sqrt{x-\Delta_{\textrm{tri}}})^2 \diff x + \int_0^{2\Delta_{\textrm{tri}}} x \diff x \right)\right)^n}\\
        & \le \sqrt{2} \cdot \sqrt{1 - \left(1 - 2 \cdot \left( \int_{2 \cdot \Delta_{\textrm{tri}}}^{1} (\Delta_{\textrm{tri}}/\sqrt{x-\Delta_{\textrm{tri}}})^2 \diff x + \int_0^{2\Delta_{\textrm{tri}}} x \diff x \right)\right)^n}\\
        & \le \sqrt{2} \cdot \sqrt{1 - \left(1 - 2 \cdot \left( \Delta_{\textrm{tri}}^2 \cdot \ln(1/\Delta_{\textrm{tri}}) + 2 \cdot \Delta_{\textrm{tri}}^2 \right)\right)^n} \intertext{We will choose a sufficiently small $C$ where $\ln(1/\Delta_{\textrm{tri}}) \ge 1$, as it enforced by $n \ge 2$ and $C \le \frac{\sqrt{2}}{e}$:}
        & \le \sqrt{2} \cdot \sqrt{1 - \left(1 - 6 \cdot \frac{C^2}{\log(n) \cdot n} \cdot \ln(\sqrt{\log(n) \cdot n}/C)\right)^n} \intertext{Using $\log(n) \le n$:}
        & \le \sqrt{2} \cdot \sqrt{1 - \left(1 - 6 \cdot \frac{\ln(n/C) C^2}{\log(n) \cdot n}\right)^n} \\
        & \le \sqrt{2} \cdot \sqrt{1 - \left(1 - 6 \cdot \frac{C^2 \cdot (1 + \ln(1/C))}{n}\right)^n}\\
        & = \sqrt{2} \cdot \sqrt{1 - \left(1 - 6 \cdot \frac{C^2 \cdot (1 + \ln(1/C))}{n}\right)^{\left(\frac{n}{6 \cdot C^2 \cdot (1 + \ln(1/C))}\right) \cdot \left({6 \cdot C^2 \cdot (1 + \ln(1/C))}\right)}}\intertext{For sufficiently small $C$ where $\frac{6 \cdot C^2 \cdot (1 + \ln(1/C))}{n} \le \frac{1}{4}$ then:}
        & \le \sqrt{2} \cdot \sqrt{1 - 0.3^{{6 \cdot C^2 \cdot (1 + \ln(1/C))}}} \le \frac{1}{10}
    \end{align*}
    For sufficiently small $C$.
\end{proof}

Together, \cref{claim:use-dpi,lemma:hard-lb,claim:tri-hard} enable us to show a lower bound for the performance of adaptive mean estimation (which we will not yet relate to the two-point testing rate):
\begin{corollary}\label{cor:est-bad}
    There exists some constant $C>0$ such that if $n \le \frac{C}{\eps^{2.5}}$, then any estimator $\est$ must likely incur $\frac{C}{\sqrt{n \log(n)}}$ error for some translation of some step distribution. More formally:
    \begin{equation*}
        \min_{\est} \max_{v \in [0,\eps/2)^{\frac{1}{2\eps}}, \mu \in \R} \Pr_{X \sim \step_v(x-\mu)^{\otimes n},\est}\left[|\est(X)-\mu| \ge \frac{C}{\sqrt{n \log(n)}}\right] \ge \frac{7}{20}
    \end{equation*}
\end{corollary}
\begin{proof}
    Let $C'$ be the constant in \cref{claim:tri-hard}, and consider a testing problem between $\rstep(x)^{\otimes n}$ and $\rstep(x - \Delta_{\textrm{tri}})^{\otimes n}$ where $\Delta_{\textrm{tri}} \triangleq \frac{C'}{n \log(n)}$. Then, if $C < C'/2$, we remark that an estimator which has error at most $\frac{C}{n \log(n)}$ is able to distinguish the testing problem. Hence:
    \begin{align*}
        & \min_{\est} \max_{v \in [0,\eps/2)^{\frac{1}{2\eps}}, \mu \in \R} \Pr_{X \sim \step_v(x-\mu)^{\otimes n},\est}\left[|\est(X)-\mu| \ge \frac{C}{\sqrt{n \log(n)}}\right]\\
        & \ge \frac{1-\dtv(\rstep(x)^{\otimes n},\rstep(x - \Delta_{\textrm{tri}})^{\otimes n})}{2}\\
        & \ge \frac{1}{2} - \frac{1}{2} \cdot \dtv(\rstep(x)^{\otimes n},\tri(x)^{\otimes n}) - \frac{1}{2} \cdot \dtv(\tri(x)^{\otimes n},\tri(x-\Delta_{\textrm{tri}})^{\otimes n}) \\
        & - \frac{1}{2} \cdot \dtv(\rstep(x- \Delta_{\textrm{tri}})^{\otimes n},\tri(x - \Delta_{\textrm{tri}})^{\otimes n}) \intertext{Using \cref{claim:tri-hard}:}
        & \ge \frac{1}{2} - \frac{1}{20} - \frac{1}{2} \cdot \dtv(\rstep(x)^{\otimes n},\tri(x)^{\otimes n}) - \frac{1}{2} \cdot \dtv(\rstep(x- \Delta_{\textrm{tri}})^{\otimes n},\tri(x - \Delta_{\textrm{tri}})^{\otimes n}) \intertext{Using \cref{claim:use-dpi}:}
        & \ge \frac{1}{2} - \frac{1}{20} -  \dtv(\mtri^{\otimes n},\rmstep^{\otimes n}) \intertext{Using \cref{lemma:hard-lb}:}
        & \ge \frac{1}{2} - \frac{1}{20} -  \frac{1}{10} = \frac{7}{20}
    \end{align*}
\end{proof}
All that remains is to analyze the two-point testing rate for step distributions and determine for which $n_0$ is the two-point testing rate for $n_0$ samples still unattainable from $n \gg n_0$ samples given our lower bound from \cref{cor:est-bad}.

\begin{claim}
    For any $\Delta \le \eps/2$, it holds that for all $v \in [0,\eps/2)^{\frac{1}{2\eps}}$:
    \begin{equation*}
        \dhsq(\step_v(x),\step_v(x+\Delta)) \ge \frac{\eps\Delta}{16}
    \end{equation*}
\end{claim}
\begin{proof}
    \begin{align*}
        & \dhsq(\step_v(x),\step_v(x+\Delta)) \\
        & \ge \int_0^{\frac{1}{2}} \left(\sqrt{\step_v(x)} - \sqrt{\step_v(x+\Delta)}  \right)^2 \diff x \\
        & = \sum_{i=0}^{\frac{1}{2\eps}-1} \int_{i\eps}^{(i+1)\eps} \left(\sqrt{\step_v(x)} - \sqrt{\step_v(x+\Delta)}  \right)^2 \diff x \intertext{Using the structure of step functions and that $\Delta \le \eps/2$:}
        & \ge \sum_{i=0}^{\frac{1}{2\eps}-1} \int_{i\eps + \eps/2 + v_i - \Delta}^{i\eps + \eps/2 + v_i} \left(\sqrt{\step_v(x)} - \sqrt{\step_v(x+\Delta)}  \right)^2 \diff x \\
        & \ge \sum_{i=0}^{\frac{1}{2\eps}-1} \int_{i\eps + \eps/2 + v_i - \Delta}^{i\eps + \eps/2 + v_i} \left(\sqrt{\step_v(x)} - \sqrt{\step_v(x) - \eps/2}  \right)^2 \diff x\\
        & \ge \sum_{i=0}^{\frac{1}{2\eps}-1} \int_{i\eps + \eps/2 + v_i - \Delta}^{i\eps + \eps/2 + v_i} \left(\frac{\eps/4}{\sqrt{\step_v(x)}} \right)^2 \diff x \\
        & \ge \sum_{i=0}^{\frac{1}{2\eps}-1} \int_{i\eps + \eps/2 + v_i - \Delta}^{i\eps + \eps/2 + v_i} \left(\frac{\eps/4}{\sqrt{1/2}} \right)^2 \diff x \\
        & = \frac{\eps \Delta}{16}
    \end{align*}
\end{proof}
We remark that the same proof immediately implies the guarantee in terms of $\min(\Delta,\eps/2)$ with no required upper bound on $\Delta$. This enables a lower bound of the Hellinger distance for all translations:
\begin{corollary}\label{cor:modulus-bound}
    For all $v \in [0,\eps/2)^{\frac{1}{2\eps}}$:
    \begin{equation*}
    \dhsq(\step_v(x),\step_v(x+\Delta)) \ge \frac{\eps \cdot \min(\Delta,\eps/2)}{16}
\end{equation*}
This immediately implies that if $\frac{\eps^2}{32} \ge \frac{1}{n_0}$ then:
\begin{equation*}
    \omega_{\step_v}\left(\frac{1}{n_0}\right) \le \frac{16}{\eps \cdot n_0}
\end{equation*}
\end{corollary}

We are finally ready to conclude for which value of $n_0$ must any estimator incur error at least $\nu \cdot \omega_{\step_v}\left(\frac{1}{n_0}\right)$ with constant probability:

\begin{lemma}\label{lemma:related-errs}
    There exists a universal constant $C>0$ such that for any sufficiently large $n$, any value $\nu \ge 1$, and any  estimator $\est$, then there exists a setting of $\eps$ such that $\est$ must incur large error with constant probability for some translation of a step distribution:
    \begin{equation*}
        \min_{\est} \max_{v \in [0,\eps/2)^{\frac{1}{2\eps}}, \mu \in \R} \Pr_{X \sim \step_v^{\otimes n},\est}\left[|\est(X)-\mu| \ge \nu \cdot \omega_{\step_v}\left( \frac{C^{7/5}}{128\nu n^{9/10} \sqrt{\log(n)}} \right)\right] \ge \frac{7}{20}
    \end{equation*}
\end{lemma}
\begin{proof}
    First, we will set $\eps$. It is our intention to use \cref{cor:est-bad}, so we must satisfy $n \le \frac{C}{\eps^{2.5}} \Leftrightarrow \frac{1}{\eps} \ge \left(\frac{n}{C}\right)^{2/5}$. Additionally, we have the constraint that $\frac{1}{2\eps}$ is an integer. For sufficiently large $n$, there will be a satisfying value of $\eps$ where $\frac{1}{\eps} \in [\left(\frac{n}{C}\right)^{2/5}, 2 \cdot \left(\frac{n}{C}\right)^{2/5}]$. 
    
    Given \cref{cor:est-bad}, then it is sufficient to show:
    \begin{align*}
        & \frac{C}{\sqrt{n \log (n)}} \ge \nu \cdot \omega_{\step_v}\left( \frac{1}{n_0} \right) \intertext{It is our goal to see how large $\frac{1}{n_0}$ can be while satisfying this inequality. If we later set parameters such that $\frac{1}{n_0} \le \frac{\eps^2}{32}$, then we may invoke \cref{cor:modulus-bound}. By our choice of $\eps$, this is satisfied as long as $\frac{1}{n_0} \le \frac{1}{128 \cdot \left(\frac{n}{C}\right)^{4/5}}=\frac{C^{4/5}}{128 \cdot n^{4/5}}$:}
        & \impliedby \frac{C}{\sqrt{n \log (n)}} \ge  \frac{16 \nu }{\eps \cdot n_0}\\
        & \Leftrightarrow \frac{C \cdot \eps}{16 \nu \cdot \sqrt{n \log (n)}} \ge  \frac{1}{n_0} \\
        & \impliedby \frac{C \cdot \frac{C^{2/5}}{2 \cdot n^{2/5}}}{16 \nu \cdot \sqrt{n \log (n)}} \ge  \frac{1}{n_0}\\
        & \Leftrightarrow  \frac{C^{7/5}}{32 \nu \cdot n^{9/10} \sqrt{\log (n)}} \ge  \frac{1}{n_0}
    \end{align*}
    Hence, the lemma holds if:
    \begin{equation*}
        \frac{1}{n_0} \le \min\left(\frac{C^{7/5}}{32 \nu \cdot n^{9/10} \sqrt{\log (n)}}, \frac{C^{4/5}}{128 \cdot n^{4/5}} \right) \impliedby \frac{1}{n_0} \le \frac{C^{7/5}}{128 \nu \cdot n^{9/10} \sqrt{\log(n)}}
    \end{equation*}
\end{proof}

The statement of our theorem follows from \cref{lemma:related-errs}.
\end{proof}
