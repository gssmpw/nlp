\section{Related Work}
\textbf{Asymptotic setting. } Location estimation and adaptive location estimation have been more extensively studied in the asymptotic settings: where the distribution $D$ is fixed and then we analyze the performance of estimators as $n \rightarrow \infty$. For location estimation, it is known that the Fisher information rate is attainable: the MLE asymptotically approaches $N(\mu,\tfrac{1}{n\mathcal{I}})$, where $\mathcal{I}$ is the Fisher information of $D$ (e.g. see Chapter 7 of \cite{van2000asymptotic}). For adaptive location estimation, many works have studied estimation under the assumption that $D$ is symmetric (e.g. \cite{stein1956efficient,van1970efficiency,stone1975adaptive,sacks1975asymptotically,beran1978efficient,dalalyan2006penalized}). Stone \cite{stone1975adaptive} showed that the Fisher information rate is asymptotically attainable if $D$ is symmetric.  More recently, Laha \cite{laha2019location} showed that tuning parameters may be avoided for adaptive location estimation of symmetric distributions if $D$ is also log-concave. 

For distributions with infinite Fisher information (e.g. $\unif(\mu - 1, \mu + 1)$, non-smooth distributions), it is perhaps sharper to consider a result of LeCam \cite{lecam1973convergence} who showed the Hellinger distance two-point testing rate is attainable given conditions related to the covering number of the family under the Hellinger metric.


\textbf{Finite-sample setting. } In this setting, we focus on how well the location may be estimated for a particular $D$ and $n$. The work of \cite{gupta2024minimax} showed that for location estimation, variants of the MLE attained minimax optimal guarantees for any $D$ and $n$, yet it does not necessarily reveal what the optimal rate is. The works of \cite{gupta2022finite} and \cite{gupta2023finite} study location estimation and adaptive location estimation, respectively, and show how estimators similar to \cite{stone1975adaptive} are able to attain the \textit{smoothed Fisher information rate}, which is the Fisher information of $D$ convolved with $N(0,r^2)$ (where $r$ is a smoothing parameter that depends on $n$, and they require $D$ is symmetric for adaptive location estimation). For some distributions, this is sufficient to attain guarantees with optimal constant factors. Unfortunately, for other distributions, the smoothing parameter $r$ may be sufficiently large such that too much information is lost. For example, their error guarantees for $\unif(\mu-1,\mu+1)$ are polynomially worse than $\Theta(\frac{1}{n})$.

The balance finding algorithm of \cite{compton2024near} for heteroskedastic mean estimation inspires our estimator. The algorithm looks for an estimate $\muh$ that exhibits a particular kind of balance, where for parameters $w$ and $\Delta$, the number of samples within $w$ to the left of $\muh$ and $w$ to the right of $\muh$ are approximately balanced, yet there is strong imbalance for $\muh \pm \Delta$. In this way, balance finding also leverages interval statistics to inform its estimator. While the balance finding algorithm attains desired guarantees for the distributions in \cref{fig:gaussian,fig:entangled}, it incurs polynomially-suboptimal errors for \cref{fig:unif,fig:semi-circle,fig:add,fig:conv}. Sweep-line techniques similarly enable near-linear time.

The work of \cite{kao2024choosing} focuses on adaptive location estimation with the goal of minimizing the $L_\gamma$ loss for $\gamma \ge 2$, where $\gamma$ is chosen data-dependently (the guarantees are a mix of asymptotic and finite-sample). Their approach is sufficient to enable sharp rates for distributions such as $\tilde{O}(\frac{1}{n})$ for $\unif(\mu-1,\mu+1)$ and $\tilde{O}(n^{-2/3})$ for the semicircle distribution. Their results also extend to the regression setting. In their discussion, they remark how this approach is unable to leverage discontinuities in the interior of the support, such as in \cref{fig:add}, which our results will encompass.

\textbf{Additional related work. } For examples such as \cref{fig:add}, much of the difficulty of adaptive location estimation boils down to determining where the discontinuity in the density occurs. In this sense, it is natural that techniques will be shared with the richly-studied task of density estimation. Focusing on log-concave distributions, it is recently known that the log-concave MLE learns the density within optimal Hellinger distance up to logarithmic factors (for any number of dimensions) \cite{han2016approximation,kim2016global,kur2019optimality}. Most relevant to our work are the techniques of \cite{chan2014efficient}, who (among other results) optimally learn mixtures of log-concave distributions in total variation distance up to logarithmic factors. Their techniques analyze estimates where the number of samples empirically within collections of intervals roughly match the expected number of samples for the estimate. Their analysis uses piecewise-polynomial approximations of log-concave distributions. Later, our work will design an algorithm that also verifies whether indicators of intervals match what is expected given shape-constraints, whose analysis also uses piecewise approximations of log-concave distributions (and a slightly finer notion of matching). This line of prior work is crucially leveraging the notion of $\mathcal{A}_k$ distance, roughly defined as the total variation distance witnessed by the union of $k$ disjoint intervals (also studied, for example, by \cite{devroye2001combinatorial,diakonikolas2014testing,diakonikolas2015optimal,diakonikolas2017near,diakonikolas2019testing,diakonikolas2023testing}). Our work will later focus instead on the \textit{Hellinger distance} witnessed by the union of $k$ disjoint intervals. 

An interesting recent line of work focuses on getting optimal constant-factor dependence on the sub-Gaussian rate (e.g. \cite{catoni2012challenging,lee2022optimal,lee2022optimal2,gupta2024beyond}). In contrast, our work focuses on shape-constrained distributions where we may perform polynomially better than the sub-Gaussian rate (but incur logarithmic-factors of lossiness in our analysis).

For some recent examples (among many) to showcase the influence of the modulus of continuity perspective: \cite{cai2015framework} introduces a local modulus of continuity as a benchmark for estimating convex functions, \cite{duchi2024right} uses the local modulus of continuity (instead with total variation distance) for locally private estimation, and \cite{foster2021statistical} presents an analog of the modulus of continuity for interactive learning.

\input{adaptive-alg}
\input{adaptive-lb}
\input{location-alg}
\input{location-lb}

\input{discussion}