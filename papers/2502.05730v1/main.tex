\pdfoutput=1
\documentclass[11pt,letterpaper]{article}%
\usepackage[utf8]{inputenc}
\usepackage[rflt]{floatflt}
\usepackage{fullpage}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{epsfig}
\usepackage{color}
\usepackage{enumerate}
\usepackage{verbatim}
\usepackage{amsthm,amssymb}
\usepackage{amsmath}
\usepackage{thmtools,thm-restate}
\usepackage{wrapfig}
\usepackage{xspace}
\usepackage{authblk}
\usepackage{amsfonts}
\usepackage{bbm}
\usepackage{algorithm, caption, subcaption}
\usepackage[noend]{algpseudocode}
\usepackage{ulem}
\usepackage{todonotes}
\newcommand{\newtext}[1]{{#1}}

\definecolor{darkgreen}{rgb}{0,0.5,0}
\usepackage{hyperref}
\hypersetup{
    unicode=false,          %
    colorlinks=true,        %
    linkcolor=blue,          %
    citecolor=darkgreen,        %
    filecolor=magenta,      %
    urlcolor=cyan           %
}

  

\usepackage[disableredefinitions,roman]{complexity}
\usepackage[framemethod=tikz]{mdframed}
\global\mdfdefinestyle{myframe}{leftmargin=.75in,rightmargin=.75in,linecolor=black,linewidth=1.5pt,innertopmargin=10pt,innerbottommargin=10pt} 
\usepackage{mdframed}
\usepackage{framed}
\usepackage{nicefrac}




\date{}

\newcommand{\eqdef}{\stackrel{\text{\tiny\rm def}}{=}}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{subclaim}[theorem]{Subclaim}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{infclaim}[theorem]{Informal Claim}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{observation}[theorem]{Observation}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{invariant}{Invariant}
\newtheorem{condition}{Condition}

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}


\usepackage[capitalize, nameinlink]{cleveref}
\crefname{theorem}{Theorem}{Theorems}
\Crefname{lemma}{Lemma}{Lemmas}
\Crefname{alg}{Algorithm}{Algorithms}
\Crefname{claim}{Claim}{Claims}
\Crefname{subclaim}{Subclaim}{Subclaims}
\Crefname{infclaim}{Claim}{Claims}
\Crefname{observation}{Observation}{Observations}
\Crefname{invariant}{Invariant}{Invariants}
\Crefname{algorithm}{Algorithm}{Algorithms}
\Crefname{fact}{Fact}{Facts}



\def\polylog{\operatorname{polylog}}
\def\diag{\operatorname{diag}}
\def\snap{\operatorname{snap}}





\DeclareMathOperator{\sgn}{sgn}



\newcommand{\ee}[1]{{\mathbb E} \left[ #1 \right]}
\newcommand{\var}[1]{{\mathbb Var} \left[ #1 \right]}
\newcommand{\prob}[1]{\mathbb P \left[ #1 \right]}
\newcommand{\eps}{\varepsilon}
\newcommand{\alg}{\mathcal{A}}

\newcommand{\false}{\textsc{false}\xspace}
\newcommand{\true}{\textsc{true}\xspace}


\newcommand{\bbR}{\mathbb{R}}
\newcommand{\bbN}{\mathbb{N}}

\newcommand{\cnt}{\rho}
\newcommand{\muh}{\hat{\mu}}
\newcommand{\nb}{n^{\beta}}
\newcommand{\Ex}{\mathbb{E}}

\renewcommand{\Pr}{\operatorname{{Pr}}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\allowdisplaybreaks[1]



\newcommand{\calD}{\mathcal{D}}
\newcommand{\calF}{\mathcal{F}}
\newcommand{\calN}{\mathcal{N}}
\newcommand{\hk}{\mathcal{H}_k}
\newcommand{\tstar}{\textbf{T*}}
\newcommand{\tprime}{\textbf{T'}}
\renewcommand{\R}{\mathbb{R}}
\newcommand{\tmin}{\tau_{\textrm{min}}}

\newcommand{\dtv}{\mathsf{d_{TV}}}
\newcommand{\dhel}{\mathsf{d_{h}}}
\newcommand{\dhsq}{\mathsf{d_{h}^2}}
\newcommand{\dchi}{\mathsf{d_{\chi^2}}}
\def\bin{\operatorname{Bin}}

\def\tri{\operatorname{Tri}}
\def\mtri{\operatorname{Mod-Tri}}
\def\step{\operatorname{Step}}
\def\mstep{\operatorname{Mod-Step}}
\def\rstep{\operatorname{Rand-Step}}
\def\rmstep{\operatorname{Rand-Mod-Step}}
\def\unif{\operatorname{Unif}}
\def\bern{\operatorname{Bern}}

\def\cgam{C_{\gamma}}
\def\cdist{C_{\operatorname{dist}}}
\def\cround{C_{\operatorname{round}}}
\def\gsmall{\gamma_{\textrm{small}}}
\def\glarge{\gamma_{\textrm{large}}}

\def\est{\hat{\theta}}


\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}


\newcommand*\diff{\mathop{}\!\mathrm{d}}
\newcommand*\Diff[1]{\mathop{}\!\mathrm{d^#1}}



\title{Attainability of Two-Point Testing Rates for Finite-Sample\\ Location Estimation}
\bibliographystyle{alphaurl}
\author[]{Spencer Compton}
\author[]{Gregory Valiant}
\affil[]{Stanford University \authorcr
  \{\tt comptons, valiant\}@stanford.edu}

\begin{document}

\maketitle
\begin{abstract}
    LeCam's two-point testing method yields perhaps the simplest lower bound for estimating the mean of a distribution: roughly, if it is impossible to well-distinguish a distribution centered at $\mu$ from the same distribution centered at $\mu+\Delta$, then it is impossible to estimate the mean by better than $\Delta/2$. It is setting-dependent whether or not a nearly matching upper bound is attainable. We study the conditions under which the two-point testing lower bound can be attained for univariate mean estimation; both in the setting of \textit{location estimation} (where the distribution is known up to translation) and \textit{adaptive location estimation} (unknown distribution). Roughly, we will say an estimate nearly attains the two-point testing lower bound if it incurs error that is at most polylogarithmically larger than the \textit{Hellinger modulus of continuity} for $\tilde{\Omega}(n)$ samples.

    Adaptive location estimation is particularly interesting, as some distributions admit much better guarantees than sub-Gaussian rates (e.g. $\unif(\mu-1,\mu+1)$ permits error $\Theta(\frac{1}{n})$, while the sub-Gaussian rate is $\Theta(\frac{1}{\sqrt{n}})$), yet it is not obvious whether these rates may be adaptively attained by one unified approach. Our main result designs an algorithm that nearly attains the two-point testing rate for mixtures of symmetric, log-concave distributions with a common mean. 
    Moreover, this algorithm runs in near-linear time and is parameter-free. In contrast, we show the two-point testing rate is not nearly attainable even for symmetric, unimodal distributions.

    We complement this with results for location estimation, showing the two-point testing rate is nearly attainable for unimodal distributions, but unattainable for symmetric distributions.

\end{abstract}

\section{Introduction}

Estimating the mean of a distribution $D$ from $n$ samples is a well-studied task, both in the setting of \textit{location estimation} (where $D$ is known up to translation) and \textit{adaptive location estimation} (where $D$ is unknown). While in some settings the typical estimators such as the sample mean/median are near-optimal (e.g. i.i.d. samples from a Gaussian), in many others there are approaches that may perform much better. A classical example is how for the uniform distribution, $\unif(\mu - 1, \mu + 1)$, the sample mean/median will produce an estimate $\muh$ with expected error $\Ex[|\mu - \muh|] = \Theta(\frac{1}{\sqrt{n}})$, while the sample midrange (taking the midpoint between the smallest sample and the largest sample) only incurs error $\Theta(\frac{1}{n})$. Such phenomena naturally raise questions regarding how well the mean of any particular distribution can be learned, as well as when there are separations between the non-adaptive and the adaptive settings.

Perhaps the simplest lower bound for this task is given by LeCam's two-point testing method: if hypothesis testing between $D$ centered at $\mu$ and $D$ centered at $\mu + \Delta$ must fail with constant probability, then any estimator of the mean must incur error at least $\Delta/2$ with constant probability. It is setting-dependent whether or not a nearly matching upper bound is attainable. Our work aims to study the shape-constraints (e.g. symmetric, unimodal, log-concave) under which the two-point testing rate can be attained for the tasks of location estimation and adaptive location estimation. In contrast, distributions have mostly so far been treated on a more case-by-case basis.



\begin{figure}
\centering
\begin{subfigure}{.3\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figs/gaussian.png}
    \caption{Gaussian}\label{fig:gaussian}
\end{subfigure}
    \hfill
\begin{subfigure}{.3\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figs/unif.png}
    \caption{Uniform}\label{fig:unif}
\end{subfigure}
   \hfill
\begin{subfigure}{.3\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figs/semicircle.png}
    \caption{Semicircle: $p(x) \propto \sqrt{1-|x|^2}$}\label{fig:semi-circle}
\end{subfigure}

\begin{subfigure}{.3\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figs/add.png}
    \caption{Mixture of a Gaussian and Uniform distribution}\label{fig:add}
\end{subfigure}
    \hfill
\begin{subfigure}{.3\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figs/conv.png}
    \caption{Convolution of a Gaussian and Uniform distribution}\label{fig:conv}
\end{subfigure}
   \hfill
\begin{subfigure}{.3\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figs/entangled.png}
    \caption{Mixture of two Gaussians with different variances}\label{fig:entangled}
\end{subfigure}
{\caption{Examples of symmetric log-concave densities and mixtures of log-concave densities}
\label{fig:images}}
\end{figure}


\textbf{Examples. } Let us showcase some instances that illustrate interesting behaviors for adaptive location estimation. 
\begin{itemize}
    \item For $n$ samples from a Gaussian $N(\mu,\sigma^2)$, the sample mean/median both incur optimal error of $|\mu - \muh| = \Theta(\frac{\sigma}{\sqrt{n}})$.
    \item For the uniform distribution $\unif(\mu-1\,\mu+1)$, the sample midrange (the midpoint between the smallest and the largest sample) incurs much better error of $\Theta(\frac{1}{n})$. This phenomenon occurs because there is information in the sharp discontinuity: the sample minimum and maximum concentrate within $\Theta(\frac{1}{n})$ of their expectation; the same phenomena enables $\tilde{O}(n^{-2/3})$ error for the semicircle distribution by the sample midrange.
    \item For a mixture $\frac{1}{2} N(\mu,1) + \frac{1}{2} \unif(\mu-1,\mu+1)$ (\cref{fig:add}), the sample midrange would no longer perform optimally, instead incurring error $\Theta({1}/{\sqrt{\log(n)}})$, yet the MLE would still attain $\Theta(\frac{1}{n})$ (as remarked in \cite{kao2024choosing}). This begs the question of when knowing the distribution up to translation (so one can, say, use the MLE) changes the rate dramatically. There are many more examples where rates much better than the sub-Gaussian $\Theta(\frac{\sigma}{\sqrt{n}})$ can be attained.
    \item \label{item:conv-disc} The convolution of the uniform distribution $\unif(\mu-1,\mu+1)$ and the Gaussian distribution $N(\mu,n^{-2\alpha})$ (for a constant $\alpha \in (0,1)$; \cref{fig:conv}) is merely a log-concave distribution, yet the earlier approaches are not sharp: the sample median/median incurs error $\tilde{\Theta}(n^{- 1/2})$, the sample midrange incurs error $\tilde{\Theta}(n^{-\alpha})$, yet our later results would show the optimal error is $\tilde{\Theta}(n^{- 1/2 - \alpha/2 })$ by more carefully leveraging information from the tails. This sharper rate is not obviously attainable from the guarantees of known prior work.
    \item Mixtures of Gaussians with a common mean (even a two-component mixture $w_1 N(\mu, \sigma_1) + (1-w_1) N(\mu, \sigma_2)$ is non-trivial, \cref{fig:entangled}) demonstrate interesting behavior, studied as \textit{entangled mean estimation} or \textit{heteroskedastic mean estimation}, where works \cite{chierichetti2014learning,liang2020learning,yuan2020learning,pensia2022estimating,devroye2023mean,compton2024near} analyzed a collection of algorithms (median, shorth, modal, iterative trimming, and balance finding estimators) and resolved that the optimal rate entails a phase transition \cite{liang2020learning,compton2024near}.
\end{itemize}
  

The examples we presented were all solved by a collection of different estimators, and it is natural to wonder whether a unified approach can adaptively recover near-optimal rates for many distributions. Our main result will design a new algorithm that nearly attains the two-point testing lower bound for all these examples.

\begin{figure}
\centering
\begin{subfigure}{.32\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figs/plot-gauss.png}
    \caption{}\label{fig:plot-gaussian}
\end{subfigure}
    \hfill
\begin{subfigure}{.32\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figs/plot-unif.png}
    \caption{}\label{fig:plot-unif}
\end{subfigure}
   \hfill
\begin{subfigure}{.32\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figs/plot-semicircle.png}
    \caption{}\label{fig:plot-semi-circle}
\end{subfigure}

\vspace{-0.05cm}
\begin{subfigure}{.32\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figs/plot-mix-unif-gauss.png}
    \caption{}\label{fig:plot-add}
\end{subfigure}
    \hfill
\begin{subfigure}{.32\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figs/plot-conv-unif-gauss.png}
    \caption{}\label{fig:plot-conv}
\end{subfigure}
   \hfill
\begin{subfigure}{.32\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figs/plot-mix-gauss.png}
    \caption{}\label{fig:plot-entangled}
\end{subfigure}
{\caption{Performance of our estimator (\cref{algo:fast}) for corresponding distributions in \cref{fig:images}.}
\label{fig:plots}}
\end{figure}

\textit{Simulations. } We examine performance of our estimator on these examples in \cref{fig:plots}, where each point is the average of $500$ tests. Running a short Python implementation\footnote{\url{https://github.com/SpencerCompton/mean-estimation}} of our estimator on $N=10^6$ samples took approximately $40$ seconds on a laptop. We interpret our estimator in \cref{fig:plot-gaussian,fig:plot-unif,fig:plot-semi-circle,fig:plot-add} as behaving similarly to the optimal rates: $\tilde{\Theta}(\frac{1}{\sqrt{n}})$ (\cref{fig:plot-gaussian}), $\tilde{\Theta}(\frac{1}{n})$ (\cref{fig:plot-unif}), $\tilde{\Theta}(\frac{1}{n^{2/3}})$ (\cref{fig:plot-semi-circle}), and $\tilde{\Theta}(\frac{1}{n})$ (\cref{fig:plot-add}). Lagging behind by a multiplicative factor, as in \cref{fig:plot-unif}, is not too surprising as our algorithm and analysis are loose up to polylogarithmic factors. In \cref{fig:plot-conv}, we observe how when $N$ is small relative to the standard deviation of the Gaussian convolution then behavior is similar to the uniform distribution, for larger $N$ there is information in the tail not leveraged by the other estimators, and for $N$ even larger than our simulation then we expect sample median/mean to improve beyond sample midrange and close the gap with our estimator (recall our earlier discussion of \cref{fig:conv}). Finally, in \cref{fig:plot-entangled}, we observe a sharp improvement in performance when $N$ is large enough that our estimator is able to detect the mixture component with smaller weight and standard deviation. Collectively, these simulations give some insight into how we adaptively attain sharper guarantees for many distributions with one estimator.



\textbf{Hellinger modulus of continuity. } We now provide background to introduce the \textit{Hellinger modulus of continuity} which will characterize the two-point testing rate. The \textit{Hellinger distance} is a distance metric on probability distributions:
\begin{definition}[Hellinger distance]
   If $P,Q$ are distributions over the same probability space $\Omega$ with densities $p$ and $q$, then the \textit{squared Hellinger distance} between $P$ and $Q$ is
   \[
        \dhsq(P,Q) = \frac{1}{2}\int_\Omega \left(\sqrt{p(x)} - \sqrt{q(x)}\right)^2
   \]
\end{definition}

Throughout this paper, we may also directly reference the Hellinger distance between probability densities. The Hellinger distance may be related to the total variation distance:
\begin{fact}[e.g. \cite{le2000asymptotics} page 44]\label{fact:tv-hell}
    \[
    \dhsq(P,Q) \le \dtv(P,Q) \le \sqrt{2 \dhsq(P,Q)}
    \]
\end{fact}

The Hellinger distance \textit{tensorizes}, which makes it ideal for studying the sample complexity of hypothesis testing.
\begin{fact}[Tensorization of Hellinger distance; e.g. \cite{le2000asymptotics} page 45]\label{fact:tensor-hell}
Suppose $P,Q$ are distributions over the same probability space $\Omega$, and let $P^{\otimes n}$ and $Q^{\otimes n}$ denote the distribution of $n$ i.i.d. samples from $P$ and $Q$ respectively.
Then
\[
\dhsq(P^{\otimes n},Q^{\otimes n}) = 1 - \left(1-\dhsq(P,Q)\right)^n.
\]
\end{fact}
In particular, as a corollary of \Cref{fact:tv-hell,fact:tensor-hell}, the Hellinger distance is ideal for measuring the sample complexity of hypothesis testing between two distributions.
If $P,Q$ are distributions over the same probability space, then
\begin{equation}
\dtv(P^{\otimes n},Q^{\otimes n}) \ge \left(1 - e^{-n\cdot \dhsq(P,Q)}\right),\label{eq:tvbd}
\end{equation}
so that once $n \sim \frac{1}{\dhsq(P,Q)}$, $n$ samples distinguish between $P$ and $Q$ with at least constant probability.
The second inequality in \Cref{fact:tv-hell} shows that if $n \ll \frac{1}{\dhsq(P,Q)}$, hypothesis testing between $P$ and $Q$ with fewer than $n$ samples is information-theoretically impossible except with vanishing probability.

Since the squared Hellinger distance $\dhsq(P,Q)$ informs the sample complexity of hypothesis testing between $P$ and $Q$, Donoho and Liu \cite{donoho1987geometrizing} introduced the \textit{Hellinger modulus of continuity} that yields often-sharp two-point testing lower bounds. The Hellinger modulus is defined for a functional $T$ and class $\mathbf{F}$ as
\begin{equation*}
    \omega(\eps) \triangleq \sup \{ |T(F_1) - T(F_0)|: \dhsq(F_1,F_0) \le \eps, F_i \in \mathbf{F} \}.
\end{equation*}
For estimating the mean of a distribution $D$, the Hellinger modulus can be instantiated as
\begin{equation*}
    \omega_D(\eps) \triangleq \sup \{ | \mu_1 - \mu_2 |: \dhsq(D_{\mu_1},D_{\mu_2}) \le \eps, \, \, \mu_1,\mu_2 \in \R \},
\end{equation*}
where $D_\mu$ denotes the distribution $D$ centered at $\mu$. Given our earlier background, we see that $\omega_D(\frac{1}{n})$ informs some two-point testing style lower bound, since $D_{\mu_1}$ and $D_{\mu_2}$ will only be distinguishable with constant probability. As immediately explored by Donoho and Liu \cite{donoho1987geometrizing,donoho1991geometrizing2,donoho1991geometrizing3}, it is often possible to nearly attain the Hellinger modulus in statistical estimation tasks. For example, they show in \cite{donoho1991geometrizing2} the Hellinger modulus rate is asymptomatically attainable if $\mathbf{F}$ is convex, $T$ is linear, and $\omega$ is H\"olderian; this style of result is recently furthered in \cite{polyanskiy2019dualizing}. In our setting $T$ is linear, but the main obstacle in employing techniques from such works is that our class $\mathbf{F}$ is not convex. Observe how convex combinations of translations of $D$ are not necessarily a translation of $D$. Similarly, shape-constraints do not form a convex set either; convex combinations of translations of symmetric distributions need not be symmetric. 

We will study the shape-constraints on $D$ under which it is possible to attain error $|\mu - \muh| \le \polylog(n) \cdot \omega_D(\frac{\polylog(n)}{n})$ (our formal statement of results will add dependence on a failure probability $\delta$). This roughly corresponds to error that is polylogarithmically larger than the two-point testing bound for $\frac{n}{\polylog(n)}$ samples.






















    



    


\subsection{Preliminaries} \label{sec:preliminaries}
A probability density $p$ is a $k$-mixture if $p(x) = \sum_{i=1}^k w_i \cdot p_i(x)$, where $w_i \ge 0$, $\sum_{i=1}^k w_i = 1$, and each $p_i$ is a density. It is a $k$-mixture of log-concave distributions if each $p_i$ is log-concave. It is a centered/symmetric mixture if all mixture components are symmetric around a common point. We denote $p_\Delta$ to be the density $p$ shifted to recenter at $\Delta$, meaning $p_\Delta(x) \triangleq p(x-\Delta)$.

 \subsection{Our Contributions}
We present positive and negative results on the attainability of the two-point testing rate, both in the settings of location estimation and adaptive location estimation. We begin with our most interesting finding: the positive result for adaptive location estimation. We follow with our three complementary results that elucidate the landscape of these tasks more broadly. 

\textbf{Attainability for adaptive location estimation.} For mixtures of $k$ symmetric log-concave distributions with the same center, we show that the two-point testing rate is nearly attainable:

\begin{restatable}{theorem}{fastthm}\label{thm:fastthm}
    Suppose $p$ is a probability density that is a centered/symmetric mixture of $k$ log-concave distributions. There exists some universal constant $\cdist \ge 1$, where if
    \begin{equation*}
        \Delta^* \triangleq \omega_p\left(\cdist \cdot \frac{k}{n} \cdot \log(2n/\delta) \cdot \log^2(2n)\right)
    \end{equation*}

    then with probability $1-\delta$ the output $\muh$ of \cref{algo:fast} will satisfy $|\mu - \muh| \le \Delta^*/2$. Moreover, \cref{algo:fast} always runs in $O(n \log(n) \log(\log(n)))$ time.
\end{restatable}



\textit{Brief intuition. } Here is an informal outline of an algorithm that guides our ideas:
\begin{enumerate}
    \item Consider a possible estimate $\muh$ of the true mean $\mu$. 
    \item Test if there is an interval that reveals the true distribution is not symmetric around $\muh$. Precisely, check if there exists an $0 \le a < b$ where the number of samples within $[\muh - b, \muh - a]$ is noticeably different from the number within $[\muh + a, \muh + b]$.
    \item For any $\muh$ that passes this test, hope it is a good estimate of $\mu$.
\end{enumerate}
Nothing is immediately clear about the performance of this algorithm. First, it is not obviously efficient to consider all values of $\muh,a,b$, but we will delay this concern. Notably, it is not clear how good of an estimate $\muh$ must be if it passes these interval tests. For arbitrary symmetric distributions, a $\muh$ passing interval tests can indeed be a poor estimate. Surprisingly, we show that for mixtures of log-concave distributions, $\muh$ is close (in terms of the Hellinger modulus) to $\mu$ with high probability.

We observe that performance of our informal algorithm boils down to the following key question: if $p$ and a translation of $p$ have large Hellinger distance, must there be an interval of the domain where their expected number of samples are noticeably different? This is not true for general $p$, but we will show it holds for $p$ satisfying our assumptions.

Trying to answer this question, we draw connections to \cite{bhatt2021information,pensia2023communication} who show how the Hellinger distance between any two distributions can be approximately preserved by a channel $T$ that outputs an indicator of a threshold of the likelihood ratio: i.e. the indicator of $p(x)/q(x) \ge \tau$ for a well-chosen threshold parameter $\tau \ge 0$. Roughly, if $P$ and $Q$ are easy to distinguish from $n$ samples, then $T(P)$ and $T(Q)$ are easy to distinguish from $\tilde{O}(n)$ samples. From their results, it becomes clear that our key question is essentially resolved if the appropriate likelihood threshold channel can be simulated by an indicator of an interval of the domain (we call this an interval statistic). Later, we show that it is also sufficient to approximately simulate the channel.

In the simpler case of $k=1$, a simple calculation reveals that any likelihood threshold channel is exactly simulated by an interval statistic. This is not true for $k>1$, but with non-trivial analysis involving piecewise-approximations of the densities and likelihood ratios, we are able to show that it is still possible to approximate the channel sufficiently well with an interval statistic.

Eventually, we further refine our approach to permit a near-linear time algorithm that still aligns with the intuition of the informal algorithm we discussed. This gives an efficient algorithm (with no tuning parameters) that we evaluated in \cref{fig:plots} on our examples of \cref{fig:images}.


\textbf{Unattainability for adaptive location estimation.} We show that if the distribution is only promised to be unimodal and symmetric, then such a rate is unattainable:


\begin{restatable}{theorem}{adaptivelb}\label{theorem:adaptive-lb}
    There exists a universal constant $0<C<1$ such that for any $n$ larger than a sufficiently large constant, and $\nu \ge 1$, then for every estimator $\est$ there is a unimodal and symmetric distribution where $\est$ incurs much larger error than the two-point testing rate with constant probability:
    \begin{equation*}
        \min_{\est} \max_{\textrm{unimodal and symmetric $D$,} \, \mu \in \R} \Pr_{X \sim D(x-\mu)^{\otimes n}, \est}\left[|\est(X) - \mu |  \ge \nu \cdot \omega_{D}\left(\frac{C}{\nu \cdot n^{9/10} \sqrt{\log(n)}}\right)\right] \ge \Omega(1)
    \end{equation*}
    Note that the statement has randomness over $\est$ to account for non-deterministic estimators.
\end{restatable}



Notably, the exponent for $n$ is $\frac{9}{10}$ instead of 1. Observe that if we invoke this theorem with $\nu = \log(n)^c$ for any constant $c$, we rule out the possibility of a positive guarantee of the form $\polylog(n) \cdot \omega_D(\tfrac{\polylog(n)}{n})$ since $n^{1/10} > \log^c(n)$ for sufficiently large $n$.

\textit{Brief intuition. } In the proof of our positive result \cref{thm:fastthm}, we crucially leveraged that thresholds of the likelihood ratio of log-concave mixtures and their translations could be well-approximated by interval statistics. For our hard instance, we hope to use a distribution where the likelihood ratio with its translation is large in regions that are very spaced apart, so interval statistics are less helpful because any large interval must contain large fractions of the domain that contain minimal information. Moreover, if we consider a family of such distributions with different spacings, then we expect it will be impossible to find where the likelihood ratio is large. We will show there is no estimator that attains two-point testing rates for all distributions in this family.

More concretely, we consider a \textit{step distribution}, which is a unimodal and symmetric distribution that resembles a collection of steps. Comparing this distribution with a slight translation in \cref{fig:step}, we see that the likelihood ratio is unequal to $1$ in regions that are spaced apart. We carefully study a family of step distributions with different step widths, and show this mixture family is indistinguishable from a triangle distribution (which has a worse two-point testing rate).

 \begin{figure}
\centering
\includegraphics[width=0.5\linewidth]{figs/step-translation.png}
{\caption{Step distribution (black, solid), a slight translation (red, dotted), and the logarithm of their likelihood ratio (purple, solid). Observe the likelihood ratio is unequal to $1$ in disjoint regions.}
\label{fig:step}}
\end{figure}

\textbf{Attainability for location estimation.} On the other hand, we show the two-point testing rate is attainable for location estimation even when the distribution is only promised to be unimodal:



\begin{restatable}{theorem}{localg}\label{thm:loc-alg}
Suppose $p$ is a unimodal probability density with mode $p(0)$, $\sqrt{n} \ge 6 \log(2/\delta)$, and $\delta \in (0,\frac{1}{2})$. There exists some universal constant $\cdist \ge 1$, where if 
\begin{equation*}
    \Delta^* \triangleq \omega_p\left(\cdist \cdot \frac{\log(n/\delta)}{n} \right)
\end{equation*}

then with probability $1-\delta$, the output $\muh$ of our algorithm will satisfy $|\mu - \muh| \le 4  \Delta^*$.
\end{restatable}
We remark that the condition of $\sqrt{n} \ge 6 \log(2/\delta)$ is semi-arbitrary, but our proof does need at least some bound on $\delta$ in relation to $n$.

\textit{Brief intuition. } While the work of \cite{gupta2024minimax} shows that a variant of the MLE attains a form of minimax optimality for this task, it is still not obvious how to directly analyze whether their algorithm attains the two-point testing rate for this task. Thus, we present and analyze a simple approach that attains this guarantee.

For our approach, we use the first $n/2$ samples as candidates for our estimate $\muh$. We prove that with high probability, one of these samples $X_i$ will satisfy that $\dhsq(p_{\mu},p_{X_i}) \le O(1) \cdot \frac{\log(1/\delta)}{n}$. Our hope is to perform likelihood tests between pairs of candidate estimates on batches of samples, and then use the results of these tests to choose a candidate. Unfortunately, it is not immediately clear that likelihood tests with $p_{X_i}$ will perform well, since although $\dhsq(p_{\mu},p_{X_i})$ is small, $X_i$ is still not exactly $\mu$. The main observation we use is that if we purposefully ``underpower'' the likelihood tests, using batches of size at most $O(\frac{n}{\log(n/\delta)})$, then since it is impossible to well-distinguish between $p_{\mu}$ and $p_{X_i}$ from batches of this size, $p_{X_i}$ must perform well on likelihood tests.

We remark that this approach should be fairly straightforward to extend to mixtures of a bounded number of unimodal distributions (not necessarily with the same center) if desired. For our purposes, we primarily desired to show this contrast with the corresponding negative result for unimodal distributions in adaptive location estimation.

\textbf{Unattainability for location estimation.} Finally, we show that if the distribution is only promised to be symmetric, then such a rate is unattainable:

\begin{restatable}{theorem}{loclb}\label{thm:loc-lb}
    For any positive integer $n$ and positive value $\nu$, there exists a distribution $D_{n,\nu}$ that is symmetric around $0$, and for every estimator $\est(X)$, there exists a centering $\mu$ where $\est$ incurs large error with constant probability:
    \begin{equation*}
        \min_{\est} \max_{\mu} \Pr_{X \sim D_{n,\nu}(x-\mu)^{\otimes n}, \est}\left[|\est(X) - \mu |  \ge \nu \cdot \omega_{D_{n,\nu}}\left(\frac{1}{10}\right)\right] \ge \frac{1}{4}
    \end{equation*}
    Note that the statement has randomness over $\est$ to account for non-deterministic estimators.
\end{restatable}

This indicates that location estimation does not get much easier from symmetry alone, as the lower bound is quite strong: by setting $\nu$ as desired, the error gets arbitrarily worse than $\omega_D(\tfrac{1}{10})$, which is already much worse than the $\omega_D(\tfrac{1}{n})$ standard for two-point testing. The constants in our theorem statement are semi-arbitrary, but adding more variables to our theorem does not seem more insightful in our primary goal of showing that the two-point testing rate is not even nearly attainable under just an assumption of symmetry.

\textit{Brief intuition. } Our analysis considers a family of distributions and uses the probabilistic method to conclude that at least one distribution satisfies desired technical properties which enable a type of packing lower bound. Our family of distributions will essentially be uniform distributions $\unif(\mu-1,\mu+1)$ with a random half of regions of their support missing. The family is slightly modified to enforce symmetry constraints. From the details of our construction, these modified distributions should not actually be much easier to estimate than by using the sample midrange for error $\Theta(\frac{1}{n})$, but the two-point testing lower bound will deceptively look much more favorable.

\subsection{Related Work}
\textbf{Asymptotic setting. } Location estimation and adaptive location estimation have been more extensively studied in the asymptotic settings: where the distribution $D$ is fixed and then we analyze the performance of estimators as $n \rightarrow \infty$. For location estimation, it is known that the Fisher information rate is attainable: the MLE asymptotically approaches $N(\mu,\tfrac{1}{n\mathcal{I}})$, where $\mathcal{I}$ is the Fisher information of $D$ (e.g. see Chapter 7 of \cite{van2000asymptotic}). For adaptive location estimation, many works have studied estimation under the assumption that $D$ is symmetric (e.g. \cite{stein1956efficient,van1970efficiency,stone1975adaptive,sacks1975asymptotically,beran1978efficient,dalalyan2006penalized}). Stone \cite{stone1975adaptive} showed that the Fisher information rate is asymptotically attainable if $D$ is symmetric.  More recently, Laha \cite{laha2019location} showed that tuning parameters may be avoided for adaptive location estimation of symmetric distributions if $D$ is also log-concave. 

For distributions with infinite Fisher information (e.g. $\unif(\mu - 1, \mu + 1)$, non-smooth distributions), it is perhaps sharper to consider a result of LeCam \cite{lecam1973convergence} who showed the Hellinger distance two-point testing rate is attainable given conditions related to the covering number of the family under the Hellinger metric.


\textbf{Finite-sample setting. } In this setting, we focus on how well the location may be estimated for a particular $D$ and $n$. The work of \cite{gupta2024minimax} showed that for location estimation, variants of the MLE attained minimax optimal guarantees for any $D$ and $n$, yet it does not necessarily reveal what the optimal rate is. The works of \cite{gupta2022finite} and \cite{gupta2023finite} study location estimation and adaptive location estimation, respectively, and show how estimators similar to \cite{stone1975adaptive} are able to attain the \textit{smoothed Fisher information rate}, which is the Fisher information of $D$ convolved with $N(0,r^2)$ (where $r$ is a smoothing parameter that depends on $n$, and they require $D$ is symmetric for adaptive location estimation). For some distributions, this is sufficient to attain guarantees with optimal constant factors. Unfortunately, for other distributions, the smoothing parameter $r$ may be sufficiently large such that too much information is lost. For example, their error guarantees for $\unif(\mu-1,\mu+1)$ are polynomially worse than $\Theta(\frac{1}{n})$.

The balance finding algorithm of \cite{compton2024near} for heteroskedastic mean estimation inspires our estimator. The algorithm looks for an estimate $\muh$ that exhibits a particular kind of balance, where for parameters $w$ and $\Delta$, the number of samples within $w$ to the left of $\muh$ and $w$ to the right of $\muh$ are approximately balanced, yet there is strong imbalance for $\muh \pm \Delta$. In this way, balance finding also leverages interval statistics to inform its estimator. While the balance finding algorithm attains desired guarantees for the distributions in \cref{fig:gaussian,fig:entangled}, it incurs polynomially-suboptimal errors for \cref{fig:unif,fig:semi-circle,fig:add,fig:conv}. Sweep-line techniques similarly enable near-linear time.

The work of \cite{kao2024choosing} focuses on adaptive location estimation with the goal of minimizing the $L_\gamma$ loss for $\gamma \ge 2$, where $\gamma$ is chosen data-dependently (the guarantees are a mix of asymptotic and finite-sample). Their approach is sufficient to enable sharp rates for distributions such as $\tilde{O}(\frac{1}{n})$ for $\unif(\mu-1,\mu+1)$ and $\tilde{O}(n^{-2/3})$ for the semicircle distribution. Their results also extend to the regression setting. In their discussion, they remark how this approach is unable to leverage discontinuities in the interior of the support, such as in \cref{fig:add}, which our results will encompass.

\textbf{Additional related work. } For examples such as \cref{fig:add}, much of the difficulty of adaptive location estimation boils down to determining where the discontinuity in the density occurs. In this sense, it is natural that techniques will be shared with the richly-studied task of density estimation. Focusing on log-concave distributions, it is recently known that the log-concave MLE learns the density within optimal Hellinger distance up to logarithmic factors (for any number of dimensions) \cite{han2016approximation,kim2016global,kur2019optimality}. Most relevant to our work are the techniques of \cite{chan2014efficient}, who (among other results) optimally learn mixtures of log-concave distributions in total variation distance up to logarithmic factors. Their techniques analyze estimates where the number of samples empirically within collections of intervals roughly match the expected number of samples for the estimate. Their analysis uses piecewise-polynomial approximations of log-concave distributions. Later, our work will design an algorithm that also verifies whether indicators of intervals match what is expected given shape-constraints, whose analysis also uses piecewise approximations of log-concave distributions (and a slightly finer notion of matching). This line of prior work is crucially leveraging the notion of $\mathcal{A}_k$ distance, roughly defined as the total variation distance witnessed by the union of $k$ disjoint intervals (also studied, for example, by \cite{devroye2001combinatorial,diakonikolas2014testing,diakonikolas2015optimal,diakonikolas2017near,diakonikolas2019testing,diakonikolas2023testing}). Our work will later focus instead on the \textit{Hellinger distance} witnessed by the union of $k$ disjoint intervals. 

An interesting recent line of work focuses on getting optimal constant-factor dependence on the sub-Gaussian rate (e.g. \cite{catoni2012challenging,lee2022optimal,lee2022optimal2,gupta2024beyond}). In contrast, our work focuses on shape-constrained distributions where we may perform polynomially better than the sub-Gaussian rate (but incur logarithmic-factors of lossiness in our analysis).

For some recent examples (among many) to showcase the influence of the modulus of continuity perspective: \cite{cai2015framework} introduces a local modulus of continuity as a benchmark for estimating convex functions, \cite{duchi2024right} uses the local modulus of continuity (instead with total variation distance) for locally private estimation, and \cite{foster2021statistical} presents an analog of the modulus of continuity for interactive learning.

\input{adaptive-alg}
\input{adaptive-lb}
\input{location-alg}
\input{location-lb}

\input{discussion}

\section{Acknowledgements}
We would like to thank John Duchi for conversations that introduced the perspective of the Hellinger modulus of continuity. We would like to thank Tselil Schramm for helpful technical discussions and feedback. This work was supported by the National Defense Science \& Engineering Graduate (NDSEG) Fellowship Program, Tselil Schramm's NSF CAREER Grant no. 2143246, and Gregory Valiant's Simons Foundation Investigator Award and NSF award AF-2341890.

\bibliography{ref}
\end{document}
