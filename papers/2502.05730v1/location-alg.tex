\section{Location Estimation for Unimodal Distributions}
We now study location estimation, where the distribution is known up to translation. We will discuss an approach that nearly attains the two-point testing rate for location estimation of unimodal distributions. Suppose the density $p$ is known up to translation ($p(0)$ is the mode of our known density before translation) and let $P_\theta$ denote the distribution with density $p(x-\theta)$. Given that the density is known up to translation, a natural approach would be to compute the MLE among all translations. Indeed, the work of \cite{gupta2024minimax} shows that a variant of the MLE attains a form of minimax optimality for this task. However, it is still not obvious how to directly analyze whether the MLE attains the two-point testing rate for this task. 

Instead, we will analyze a modified version of the MLE. As a warmup, consider the easier task of estimating the mean from a list $L$ candidate means $\theta_1,\dots,\theta_{|L|}$, \textit{where it is promised the true mean $\mu \in L$}. Now, consider a procedure where for each pair $i \ne j$ we compute whether the empirical likelihood of $n$ samples is larger for $\theta_i$ or $\theta_j$. Using folklore results, we could conclude that with probability $\ge 1 - \delta$, the true mean will only lose in comparisons against $\theta_i$ where $\dhsq(P_\mu, P_{\theta_i}) = O(\frac{\log(|L|/\delta)}{n})$. This is sufficient to find an estimate of the mean within $\omega_P(O(\frac{\log(|L|/\delta)}{n}))$. We simply choose the $\theta_i$ that is undefeated (if one exists), or otherwise we choose the $\theta_i$ whose farthest loss is closest to $\theta_i$. This works because if the chosen $\theta_i$ were a poor enough estimate such that $|\mu - \theta_i| \gg \omega_P(O(\frac{\log(|L|/\delta)}{n}))$, then $\theta_i$ would lose to $\mu$ and have a farther loss from it than $\mu$ has.

This warmup shows promise, but does not actually resolve the task where we are not given such a list. An initial idea is to use the first $n/2$ as our list, and then estimate from the latter $n/2$ samples. This is close to working, but does not satisfy the property that the list contains exactly the true mean. Unfortunately, we do not necessarily know anything about the performance of likelihood tests when the mean is close but not exactly correct.

This leads to our main insight: we will purposefully underpower the tests. First, we will conclude that with high probability, one of the first $n/2$ samples $X_i$ satisfies $\dhsq(P_\mu,P_{X_i}) \le O(\frac{\log(1/\delta)}{n})$. Then, we realize that $P_\mu$ and $P_{X_i}$ cannot be well-distinguished from only $\ll \frac{n}{\log(1/\delta)}$ samples. This means that likelihood tests between $P_\mu$ and some $P_{\theta_j}$ from $\ll \frac{n}{\log(1/\delta)}$ samples must perform similarly to likelihood tests between $P_{X_i}$ and $P_{\theta_j}$ by data processing inequality. Accordingly, we employ an approach where we use the first half of samples to get a candidate list, and then use a similar algorithm to the warmup but with purposefully underpowered tests (followed by a boosting step). We prove it nearly attains the two-point testing rate:

\localg*
\begin{proof}
    We remark that the condition of $\sqrt{n} \ge 6 \log(2/\delta)$ is semi-arbitrary, but our proof does require at least some bound on $\delta$ in relation to $n$. We also note that it is valid to argue with statements like ``for $n$ larger than a sufficiently large constant'', because this can be enforced by setting $\cdist$ large to enforce $\cdist \cdot \frac{\log(n/\delta)}{n} > 1$ for small $n$, for which the theorem is vacuous.
    
    The algorithm will begin by using the $n/2$ samples as candidates. Our hope is that at least one of these candidates $X_i$ is sufficiently close to $\mu$ such that $\dhsq(P_\mu,P_{X_i}) = O(\frac{\log(1/\delta)}{n})$. We show a result that $\dhsq(P,P_\Delta)$ lower bounds the probability of samples within $[-\Delta,+\Delta]$:

\begin{lemma}\label{lemma:hel-to-mass}
    Let $P$ be a unimodal distribution with location $0$, and let $P_{\Delta}$ be the distribution shifted by $\Delta$. Then, $\dhsq(P,P_{\Delta}) \le \int_{-\Delta}^{\Delta} p(x) \, dx$
\end{lemma}
\begin{proof}
    \begin{align*}
        & H^2(P,P_\Delta) \triangleq \frac{1}{2} \int (\sqrt{p(x)} - \sqrt{p(x-\Delta)})^2 \\
        & = \frac{1}{2} \int_{-\infty}^0 (\sqrt{p(x)} - \sqrt{p(x-\Delta)})^2 + \frac{1}{2} \int_{\Delta}^\infty (\sqrt{p(x)} - \sqrt{p(x-\Delta)})^2  + \frac{1}{2} \int_{0}^\Delta (\sqrt{p(x)} - \sqrt{p(x-\Delta)})^2 \\
        & \le \frac{1}{2} \int_{-\infty}^0 (\sqrt{p(x)} - \sqrt{p(x-\Delta)})^2 + \frac{1}{2} \int_{\Delta}^\infty (\sqrt{p(x)} - \sqrt{p(x-\Delta)})^2 + \frac{1}{2} \int_{-\Delta}^\Delta p(x)\\
        & = \frac{1}{2}\left( \int_{-\infty}^0 p(x) + \int_{-\infty}^{-\Delta} p(x) - 2\int_{-\infty}^0 \sqrt{p(x)}\sqrt{p(x-\Delta)} \right) \nonumber \\
        & + \frac{1}{2} \left( \int_0^\infty p(x) + \int_\Delta^\infty p(x) - 2\int_0^\infty \sqrt{p(x)}\sqrt{p(x+\Delta)}\right) + \frac{1}{2} \int_{-\Delta}^\Delta p(x) \intertext{Using that $P$ is unimodal:}
        & \le \frac{1}{2}\left( \int_{-\infty}^0 p(x) + \int_{-\infty}^{-\Delta} p(x) - 2\int_{-\infty}^0 p(x-\Delta) \right) \nonumber \\
        & + \frac{1}{2} \left( \int_0^\infty p(x) + \int_\Delta^\infty p(x) - 2\int_0^\infty p(x+\Delta)\right) + \frac{1}{2} \int_{-\Delta}^\Delta p(x) \label{step:use-unimodal}\\
        & = \int_{-\Delta}^{\Delta} p(x) dx \tag*{\qedhere} 
    \end{align*}
\end{proof}

This lets us conclude that with high probability, one of the first $n/2$ samples will be close to $\mu$:

\begin{corollary}\label{cor:cand-good}
    Let $\Delta_1 \ge 0$ be the smallest value such that:
    \begin{equation*}
        \int_{\mu - \Delta_1}^{\mu + \Delta_1} p(x) \diff x \ge \frac{2}{\log(e)} \cdot \frac{\log(2/\delta)}{n}
    \end{equation*}
    Then, with probability at least $1-\delta/2$, one of the first $n/2$ samples will have value $X_i \in [\mu-\Delta_1,\mu+\Delta_1]$. Moreover, for such an $X_i$ it holds that:
    \begin{equation*}
        \dhsq(P_\mu,P_{X_i}) \le \frac{2}{\log(e)} \cdot \frac{\log(2/\delta)}{n}
    \end{equation*}
\end{corollary}
\begin{proof}
    The probability of none of the first $n/2$ samples being in this range is at most:
    \begin{align*}
        & \left(1- \frac{2}{\log(e)} \cdot \frac{\log(2/\delta)}{n}\right)^{n/2} \\
        & = \left(1 - \frac{(2/\log(e)) \cdot \log(2/\delta)}{n}\right)^{\frac{n}{(2/\log(e))\cdot \log(2/\delta)} \cdot \frac{n/2}{n/((2/\log(e)) \cdot \log(2/\delta))}} \\
        & \le \left(\frac{1}{e}\right)^{\ln(2/\delta)} = \delta/2
    \end{align*}
    Additionally, \cref{lemma:hel-to-mass} immediately implies that for any $X_i \in [\mu - \Delta_1,\mu + \Delta_1]$ it holds that $\dhsq(P_\mu,P_{X_i}) \le \frac{2}{\log(e)} \cdot \frac{\log(2/\delta)}{n}$.
\end{proof}

Assuming this event holds, let $X_{i^*}$ be an arbitrary one of the desired samples. With the remaining $n/2$ samples we hope to use likelihood tests of size $n_{\textrm{test}} \triangleq \lfloor C_{\textrm{test}} \cdot \frac{n}{\log(n/\delta)} \rfloor$ for a later-chosen $0<C_{\textrm{test}}<1$. We will show that $P_\mu$ and $P_{X_{i^*}}$ have small total variation distance over $n_{\textrm{test}}$ samples, and then show how this implies likelihood tests with $P_{X_{i^*}}$ will perform well.

\begin{lemma}\label{lemma:underpowered-small-tv}
    There exists a constant $0<C_{\textrm{test}}<1$ such that if $n_{\textrm{test}} \triangleq \lfloor C_{\textrm{test}} \cdot \frac{n}{\log(n/\delta)} \rfloor$ then $\dtv(P_{\mu}^{\otimes n_{\textrm{test}}},P_{X_{i^*}}^{\otimes n_{\textrm{test}}}) \le 0.01$.
\end{lemma}
\begin{proof}
    \begin{align*}
        & \dtv(P_{\mu}^{\otimes n_{\textrm{test}}},P_{X_{i^*}}^{\otimes n_{\textrm{test}}}) \\
        & \le \sqrt{2} \cdot \sqrt{\dhsq(P_{\mu}^{\otimes n_{\textrm{test}}},P_{X_{i^*}}^{\otimes n_{\textrm{test}}})} \\
        & = \sqrt{2} \cdot \sqrt{1 - (1-\dhsq(P_\mu,P_{X_{i^*}}))^{n_{\textrm{test}}}} \\
        & = \sqrt{2} \cdot \sqrt{1 - (1-\dhsq(P_\mu,P_{X_{i^*}}))^{(1/\dhsq(P_\mu,P_{X_{i^*}})) \cdot (n_{\textrm{test}} \cdot \dhsq(P_\mu,P_{X_{i^*}}))}}\intertext{We will assume $\dhsq(P_\mu,P_{X_{i^*}}) \le \frac{1}{4}$ to imply $(1-\dhsq(P_\mu,P_{X_{i^*}}))^{\dhsq(P_\mu,P_{X_{i^*}})} \ge 0.3$. This assumption holds if $\frac{2}{\log(e)} \cdot \frac{\log(2/\delta)}{n} \le \frac{1}{4}$, which is implied by $n \ge 6 \log(2/\delta)$: }
        & \le \sqrt{2} \cdot \sqrt{1 - 0.3^{n_{\textrm{test}} \cdot \dhsq(P_\mu,P_{X_{i^*}})}}\\
        & \le \sqrt{2} \cdot \sqrt{1 - 0.3^{C_{\textrm{test}} \cdot \frac{2}{\log(e)}}} \le 0.01 
    \end{align*}
    For sufficiently small $0<C_{\textrm{test}}<1$.
\end{proof}

We use the folklore fact that likelihood test performance is informed by  total variation distance:
\begin{fact}\label{fact:mu-test-good}
    Consider the task of testing between two distributions $P_1,P_2$. Let $\est_{\textrm{likelihood}}^{P_1,P_2}(X)$ to be the estimator that outputs $1$ if $P_1(X) > P_2(X)$ and $2$ otherwise. Then:
    \begin{equation*}
        \min_{i \in \{1,2\}} \Pr_{X \sim P_i}[\est_{\textrm{likelihood}}^{P_1,P_2}(X) = i] \ge \dtv(P_1,P_2) 
    \end{equation*}
\end{fact}

Now, we show that any sufficiently bad $X_j$ will most likely fail a likelihood test against $X_{i^*}$:
\begin{lemma}
    There exists a constant $\cdist \ge \frac{2}{\log(e)}$ (that is only a function of $C_{\textrm{test}}$) such that if
    \begin{equation*}
        \Delta^* \triangleq \omega_P \left( \cdist \cdot \frac{\log(n/\delta)}{n}\right)
    \end{equation*}

    then, for any $\theta \notin [\mu-2\Delta^*,\mu+2\Delta^*]$ it holds that:
    \begin{equation*}
        \Pr_{X \sim P_\mu^{\otimes n_{\textrm{test}}}}[\est_{\textrm{likelihood}}^{P_{X_{i^*}}^{\otimes n_{\textrm{test}}},P_{\theta}^{\otimes n_{\textrm{test}}}}(X) = 1] \ge 0.98
    \end{equation*}
\end{lemma}
\begin{proof}
    We remark that the constraint $\cdist \ge \frac{2}{\log(e)}$ was chosen to imply that $\Delta^* \ge \Delta_1$ (as long as $n \ge 2$) for convenience. 

    \begin{align*}
        & \Pr_{X \sim P_\mu^{\otimes n_{\textrm{test}}}}[\est_{\textrm{likelihood}}^{P_{X_{i^*}}^{\otimes n_{\textrm{test}}},P_{\theta}^{\otimes n_{\textrm{test}}}}(X) = 1] \\
        & \ge \Pr_{X \sim P_{X_{i^*}}^{\otimes n_{\textrm{test}}}}[\est_{\textrm{likelihood}}^{P_{X_{i^*}}^{\otimes n_{\textrm{test}}},P_{\theta}^{\otimes n_{\textrm{test}}}}(X) = 1] - \dtv\left(\est_{\textrm{likelihood}}^{P_{X_{i^*}}^{\otimes n_{\textrm{test}}},P_{\theta}^{\otimes n_{\textrm{test}}}}(X \sim P_\mu^{\otimes n_{\textrm{test}}}), \est_{\textrm{likelihood}}^{P_{X_{i^*}}^{\otimes n_{\textrm{test}}},P_{\theta}^{\otimes n_{\textrm{test}}}}(X \sim P_{X_{i^*}}^{\otimes n_{\textrm{test}}})\right) \intertext{By data processing inequality:}
        & \ge \Pr_{X \sim P_{X_{i^*}}^{\otimes n_{\textrm{test}}}}[\est_{\textrm{likelihood}}^{P_{X_{i^*}}^{\otimes n_{\textrm{test}}},P_{\theta}^{\otimes n_{\textrm{test}}}}(X) = 1] - \dtv(P_\mu^{\otimes n_{\textrm{test}}},P_{X_{i^*}}^{\otimes n_{\textrm{test}}}) \intertext{By \cref{lemma:underpowered-small-tv}:}
        & \ge \Pr_{X \sim P_{X_{i^*}}^{\otimes n_{\textrm{test}}}}[\est_{\textrm{likelihood}}^{P_{X_{i^*}}^{\otimes n_{\textrm{test}}},P_{\theta}^{\otimes n_{\textrm{test}}}}(X) = 1] - 0.01 \intertext{By \cref{fact:mu-test-good}:}
        & \ge \dtv(P_{X_{i^*}}^{\otimes n_{\textrm{test}}},P_{\theta}^{\otimes n_{\textrm{test}}}) - 0.01 \\
        & \ge \dhsq(P_{X_{i^*}}^{\otimes n_{\textrm{test}}},P_{\theta}^{\otimes n_{\textrm{test}}}) - 0.01 \\
        & = 1 - (1 - \dhsq(P_{X_{i^*}},P_{\theta}))^{n_{\textrm{test}}} - 0.01 \\
        & \ge 1 - e^{-n_{\textrm{test}} \cdot \dhsq(P_{X_{i^*}},P_{\theta})} - 0.01 \intertext{Using that $|X_{i^*} - \theta| > 2 \Delta^* - \Delta_1 \ge \Delta^*$ implies $\dhsq(P_{X_{i^*}}, P_\theta) > \cdist \cdot \frac{\log(n/\delta)}{n}$:}
        & \ge 0.99 - e^{-\lfloor C_{\textrm{test}} \cdot \frac{n}{\log(n/\delta)} \rfloor \cdot \cdist \cdot \frac{\log(n/\delta)}{n}} \intertext{With sufficiently large $n$:}
        & \ge 0.99 - e^{-\cdist \cdot C_{\textrm{test}}/2} \ge 0.98
    \end{align*}
    For sufficiently large $\cdist$.
\end{proof}

We are now ready to argue that with probability $1-\delta$, $X_{i^*}$ passes all likelihood tests against $\theta \notin [\mu-2\Delta^*,\mu+2\Delta^*]$ when we take the majority answer of $k_{\textrm{num-tests}} \triangleq \lfloor \frac{n/2}{\lfloor C_{\textrm{test}} \cdot \frac{n}{\log(n/\delta)} \rfloor} \rfloor$ tests:

\begin{claim}\label{claim:majority-good}
    Consider for each pair of the first $n/2$ samples we take the majority outcome of $k_{\textrm{num-tests}}$ likelihood tests. Then, with probability at least $1-\delta/2$, $X_{i^*}$ has a strict majority against all tested $\theta$ where $\theta \notin [\mu - 2 \Delta^*, \mu + 2 \Delta^*]$. 
\end{claim}
\begin{proof}
    Let $S$ be the set of the first $n/2$ samples that are not in $[\mu - 2 \Delta^*, \mu + 2\Delta^*]$. Then:
    \begin{align*}
        & \Pr_{\textrm{$k_{\textrm{num-tests}}$ groups of $n_{\textrm{test}}$-sized tests}}[X_{i^*} \textrm{ does not have strict majority over all } S] \\
        & \le \frac{n}{2} \cdot \max_{\theta \notin [\mu-2\Delta^*, \mu + 2 \Delta^*]}\Pr_{\textrm{$k_{\textrm{num-tests}}$ groups of $n_{\textrm{test}}$-sized tests}}[X_{i^*} \textrm{ does not have strict majority over } \theta] \\
        & \le \frac{n}{2} \cdot \Pr\left[\sum_{j=1}^{k_{\textrm{num-tests}}} \bern(0.98) \le 0.5\right]\\
        & \le \frac{n}{2} \cdot 2 \cdot \exp\left( - \frac{2 \cdot (0.4 \cdot k_{\textrm{num-tests}})^2}{k_{\textrm{num-tests}}} \right) \\
        & = n \cdot \exp\left( - 0.32 \cdot k_{\textrm{num-tests}} \right)\\
        & \le n \cdot \exp\left( - 0.32 \cdot \left\lfloor \frac{\log(n/\delta)}{2 C_{\textrm{test}}} \right\rfloor \right) \le \delta/2
    \end{align*}
    For sufficiently small $C_{\textrm{test}}$.
\end{proof}

Wrapping up, from our initial $n/2$ samples, our algorithm will choose one sample $X_{j'}$ as our estimate. If there is an undefeated $X_{j'}$ then it will choose this one. Otherwise, it will choose the $j'$ that minimizes the furthest loss:

\begin{equation}
    j' \triangleq \argmin_{j' \in \{1,\dots, n/2\}} \max_{\ell \in \{1,\dots, n/2\} \textrm{ where $X_\ell$ beats $X_{j'}$ }} |X_{j'} - X_\ell| \label{eq:jprime}
\end{equation}

\begin{claim}\label{claim:jprime-good}
    Under the event in \cref{claim:majority-good}, we conclude $X_{j'} \in [\mu - 4 \Delta^*, \mu + 4\Delta^*]$
\end{claim}
\begin{proof}

If there is an undefeated $X_{j'}$ then either $j' = i^*$ or all the first $n/2$ samples are in $[\mu-2\Delta^*,\mu+2\Delta^*]$; in either case, our desired result immediately follows. Otherwise, if no sample is undefeated, let a sample's ``radius'' be the distance from its farthest loss. By \cref{claim:majority-good}, $X_{i^*}$ will have radius at most $\Delta_1 + 2 \Delta^* \le 3 \Delta^*$. For sake of contradiction, suppose $X_{j'} \notin [\mu - 4\Delta^*, \mu + 4 \Delta^*]$. Then, $X_{i^*}$ must beat it, yet their distance is $> 3 \Delta^*$, so this is impossible. Thus, our algorithm incurs error at most $4 \Delta^*$. 
\end{proof}

In summary, our algorithm is as follows:
\begin{itemize}
    \item We use the first $n/2$ samples as a list of candidate estimates. By \cref{cor:cand-good}, we conclude that there is at least one sample $X_{i^*} \in [\mu - \Delta_1, \mu + \Delta_1]$.
    \item For sufficiently large $\cdist$ and sufficiently small $0<C_{\textrm{test}}<1$, we group the remaining $n/2$ samples into $k_{\textrm{num-tests}} \triangleq \lfloor \frac{n/2}{\lfloor C_{\textrm{test}} \cdot \frac{n}{\log(n/\delta)} \rfloor} \rfloor$ tests of size $n_{\textrm{test}} \triangleq \lfloor C_{\textrm{test}} \cdot \frac{n}{\log(n/\delta)} \rfloor$. We also define $\Delta^*$ in terms of $\cdist$.
    \item For each pair of candidate estimates, we perform the  $k_{\textrm{num-tests}}$ likelihood tests, and we say that one of the pair ``wins'' if it has strictly larger likelihood for a strict majority of the tests. By \cref{claim:majority-good}, with probability $1-\delta/2$ (conditioned on the existence of an $X_{i^*}$), $X_{i^*}$ will have a strict majority against any $X_j \notin [\mu - 2\Delta^*, \mu + 2\Delta^*]$. 
    \item We choose our estimate to be $X_{j'}$: the candidate whose furthest loss in the closest as indicated in \cref{eq:jprime} (or the undefeated candidate, if there is one). By \cref{claim:jprime-good}, this estimate will be within $[\mu - 4\Delta^*,\mu+ 4\Delta^*]$.
\end{itemize}
\end{proof}

\textbf{Remarks. } If desired, we expect this same proof method should naturally extend towards an analogous positive result for mixtures of unimodal distributions (not necessarily with the same center). The itemized summary previously stated should still essentially hold. Modifying the first item of the summary, instead show that one of the first $n/2$ samples will be sufficiently close to the mode of one of the mixture components, such that using the translation that overlays the component's mode over the sample will have small Hellinger distance with the correct translation. We avoid this additional complication,  as our motivation is primarily to contrast with our negative result for symmetric, unimodal distributions in the adaptive setting.
