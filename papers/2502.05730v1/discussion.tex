\section{Discussion}

In this work, we studied the conditions under which the two-point testing rate is attainable for the tasks of location estimation and adaptive location estimation. We discuss two interesting avenues:

\textbf{Estimation in higher dimensions. } Our results focus entirely on the 1-dimensional setting. A similar study in higher dimensions could be very interesting. For instance, one could study the attainability of two-point testing rates for adaptive location estimation of unimodal, radially symmetric densities in $d \ge 2$ dimensions. For further inspiration, the earlier-discussed related task of entangled mean estimation demonstrates interesting behavior in higher dimensions. The works of \cite{chierichetti2014learning,pensia2022estimating,compton2024near} studied how the task becomes easier in higher dimensions given  radial symmetry (demonstrating how this is a very strong condition). The very recent work of \cite{diakonikolas2025entangled} studied high-dimensional entangled mean estimation without stringent radial symmetry assumptions (instead studying bounded covariance matrices), encountering different rates and techniques.

\textbf{Adaptive location estimation for more general distributions. } Our main result \cref{thm:fastthm} shows a positive result for adaptive location estimation of log-concave mixtures that are symmetric around a common point.  While our negative result \cref{theorem:adaptive-lb} shows that the two-point testing rate is unattainable for symmetric, unimodal distributions, it still seems quite possible that the rate is attainable for more general distributions than the assumptions of \cref{thm:fastthm}. For example, consider a symmetric mixture of log-concave distributions,
\begin{equation*}
    p(x) = \sum_{i=1}^k \frac{w_i}{2} \cdot (p_i(x - \Delta_i) + p_i(x + \Delta_i)),
\end{equation*}
where each $p_i$ is a log-concave distribution that is symmetric around $0$. One such distribution is the Gaussian mixture $\frac{1}{2} N(\mu - \Delta, \sigma^2) + \frac{1}{2} N(\mu + \Delta,\sigma^2)$ (learning parameters of such a mixture is studied in e.g. \cite{wu2021randomly}). We remark that \cref{algo:ident} would immediately handle this generalization if the technical result of \cref{lemma:intervals} could be appropriately strengthened (the proof contains remarks about where the current method fails to generalize). As a starting point, if one made more stringent assumptions on the log-concave components, such as assuming they are Gaussian, then it seems that the result of \cref{lemma:intervals} would more easily generalize.

