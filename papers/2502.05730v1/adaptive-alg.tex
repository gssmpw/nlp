


\section{Adaptive Location Estimation for Log-Concave Mixtures}

In this section, we will provide an algorithm for estimating the mean of mixtures of log-concave distributions, with a guarantee in terms of the Hellinger modulus of the distribution. We begin with recalling an informal outline of an algorithm that guides our ideas:

\begin{enumerate}
    \item Consider a possible estimate $\muh$ of the true mean $\mu$. 
    \item Test if there is an interval that reveals the true distribution is not symmetric around $\muh$. Precisely, check if there exists an $0 \le a < b$ where the number of samples within $[\muh - b, \muh - a]$ is noticeably different from the number within $[\muh + a, \muh + b]$.
    \item For any $\muh$ that passes this test, hope it is a good estimate of $\mu$.
\end{enumerate}

Nothing is immediately clear about the performance of this algorithm. First, it is not obviously efficient to consider all values of $\muh,a,b$, but we will delay this concern. Notably, it is not clear how good of an estimate $\muh$ must be if it passes these interval tests. For arbitrary symmetric distributions, a $\muh$ passing interval tests can indeed be a poor estimate. Surprisingly, we will show that for mixtures of log-concave distributions, $\muh$ will be close (in terms of the Hellinger modulus) to $\mu$ with high probability:

\fastthm*

We now roughly outline our proof structure. Our goal is to show that there exists a failing interval test if $\muh$ is poor enough such that $\dhsq(p_{\mu},p_{\muh})$ is large. Roughly, we will later show that this occurs if whenever $\dhsq(p_{\mu},p_{\muh})$ is large, there exists some interval that witnesses the distance: the expected number of samples inside this interval is noticeably different for $p_{\mu}$ and $p_{\muh}$. We focus on showing this witnessing property first, and then focus on the algorithmic aspects later.

First, in \cref{sec:approxchannel}, we discuss the results of \cite{bhatt2021information,pensia2023communication} that show how the Hellinger distance between any two distributions can be approximately preserved by a channel that outputs an indicator of a threshold of the likelihood ratio: i.e. the indicator of $p(x)/q(x) \ge \tau$ for a well-chosen threshold parameter $\tau \ge 0$. We then observe how a channel that approximates the optimal thresholding channel still approximately preserves the Hellinger distance between the two distributions. Second, in \cref{sec:logapprox}, we prove how any likelihood thresholding channel between a log-concave mixture and its translation can be approximated by an interval statistic. This proof relies on a careful approximation of the distribution and likelihood ratio by piecewise-constant functions. Finally, we have shown our desired witnessing property. In \cref{sec:alg}, we combine these tools to show how they imply that any sufficiently bad estimate $\muh$ will fail some interval test with high probability. We further refine the structure of these interval tests to permit a near-linear time algorithm that still aligns with the intuition of the informal algorithm we discussed.

\subsection{Near-Optimality of Approximate Likelihood Threshold Channels}\label{sec:approxchannel}
Consider the task of distinguishing between two distributions $p$ and $q$ from samples. It is classically known that the sample complexity of this task is $\Theta(\frac{1}{H^2(p,q)})$ by looking at the product of the likelihood ratio for all samples. Interestingly, \cite{bhatt2021information} and \cite{pensia2023communication} show that the sample complexity only increases logarithmically if we merely look at statistics of the indicator of a threshold on the likelihood ratio. We will focus on the form of the result given by \cite{pensia2023communication} for convenience, but the result of either paper would yield the tool that is crucial for our work. More concretely, consider the class of thresholds on the likelihood ratio:

\begin{definition}
    $\mathcal{T}^{\operatorname{thresh}} \triangleq \{\mathbbm{1}_{p(x)/q(x) \ge \tau}(x) : \tau \ge 0 \}$
\end{definition}

Then, \cite{pensia2023communication} show there exists a $\tstar \in \mathcal{T}^{\operatorname{thresh}}$ where $H^2(\tstar p, \tstar q) \approx H^2(p,q)$. We state a special-case of one of their results as follows:\footnote{In their work, they show results for when your threshold may output one of $D$ options, indicating whether $p(x)/q(x) \in [0,\tau_1), [\tau_1,\tau_2),\dots, \textrm{ or } [\tau_{D-1},\infty)$. It is sufficient for our work to focus on $D=2$. They also study other ``well-behaved'' f-divergences beyond Hellinger distances.}
\begin{theorem}[Corollary 3.4 of \cite{pensia2023communication}; preservation of Hellinger distance]\label{thm:og-thresh}
    For any $p,q \in \Delta_k$, there exists a $\tstar \in \mathcal{T}^{\operatorname{thresh}}$ such that the following holds:
    \begin{equation}
        1 \le \frac{\dhsq(p,q)}{\dhsq(\tstar p, \tstar q)} \le 1800 \min\{k, k' \},
    \end{equation}
    where $k' = \log(4/\dhsq(p,q))$.
\end{theorem}

We remark on some properties of this result. Note that properties 2-4 simultaneously hold for $p,q$ or after exchanging $p,q$:

\begin{remark} \label{remark:opt-props}\mbox{}
    \begin{enumerate}
        \item The proof of \cref{thm:og-thresh} also holds for continuous distributions $p,q$ if we replace the dependence on $\min\{k,k'\}$ with just $k'$.
        \item The proof also implies a stronger bound that $\frac{\dhsq(p,q)}{\dhsq(\tstar p, \tstar q)} \le \frac{\dhsq(p,q)}{\left(\sqrt{\Pr_{x \sim p}[\tstar(x)=1]} - \sqrt{\Pr_{x \sim q}[\tstar(x)=1]}\right)^2} \le 1800 \min\{k, k'\}$.
        \item $\tstar$ thresholds with $1+\tau^*$ for $\tau^* \ge 0$ and it holds that $\tau^*  \ge \sqrt{\frac{\dhsq(p,q)}{104 \log(4/\dhsq(p,q))}}$.
        \item $\tstar$ thresholds with $1+\tau^*$ for $\tau^* \ge 0$ and it holds that $\Pr_{x \sim p}[\tstar(x) = 1] \ge \frac{\dhsq(p,q)}{1800 \log(4/\dhsq(p,q))}$.
    \end{enumerate}
\end{remark}
\begin{proof}
    (1) holds immediately by replacing all notation in their original proof with the corresponding notation for continuous distributions.

    (2) holds immediately from their proof as well.
    
    (3) holds from the following observations about their proof (see their Section 3.2 for reference). In their ``Case 1'', observe that $\tau^*=1$. In their ``Case 2'', we use more details in their proof. In terms of their notation (their $\delta$ is our $\tau^*$), note that they choose a threshold of $1+\delta$ such that:
    \begin{align*}
        & \delta^2 \\
        & \ge \delta^2 \Pr[Y \ge \delta^2]\\
        & \ge \frac{\Ex[Y]}{13 \cdot (1 + \log(1/\Ex[Y]))}\intertext{Using their inequality that $\Ex[Y] \ge \dhsq(p,q)/4$:}
        & \ge \frac{\dhsq(p,q)}{52 \cdot (1 + \log(4/\dhsq(p,q)))}\\
        & \ge \frac{\dhsq(p,q)}{104 \log(4/\dhsq(p,q))}
    \end{align*}

    As their $\delta$ is our $\tau^*$, this implies $\tau^* \ge \sqrt{\frac{\dhsq(p,q)}{104 \log(4/\dhsq(p,q))}}$.

    (4) holds simply by:
    \begin{align*}
        & \Pr_{x \sim p}[\tstar(x) = 1] \\
        & \ge \dtv(\tstar p, \tstar q) \\
        & \ge \dhsq(\tstar p, \tstar q) \intertext{Using the result of \cref{thm:og-thresh}:}
        & \ge \frac{\dhsq(p,q)}{1800 \log(4/\dhsq(p,q))}
    \end{align*}
\end{proof}

For our work, we hope to leverage a channel $\tprime$ (not necessarily a proper thresholding function) that approximates $\tstar$, and conclude that $\tprime$ similarly preserves Hellinger distance like $\tstar$. 

\begin{theorem}[Modified Corollary 3.4 of \cite{pensia2023communication}; preservation of Hellinger distance for approximating thresholds]\label{thm:mod-thresh}
    For any continuous distributions $p,q$, let $\tstar \in \mathcal{T}^{\operatorname{thresh}}$ be the threshold yielded by \cref{thm:og-thresh}. Without loss of generality, suppose $\tstar$ thresholds by $1 + \tau^*$ for $\tau^* \ge 0$ (swap $p$ and $q$ otherwise). Then, consider a channel $\tprime$ an $(\alpha,\beta)$-approximation if it satisfies:
    \begin{enumerate}
        \item $\tprime(x) = 1$ only if $p(x)/q(x) \ge 1 + \alpha \cdot \tau^*$ for $0 < \alpha \le 1$.
        \item $\Pr_{x \sim p}[\tprime(x) = 1] \ge \beta \cdot \Pr_{x \sim p}[\tstar(x) = 1]$ for $0 < \beta \le 1$.
    \end{enumerate}
    For any such $(\alpha,\beta)$-approximation $\tprime$, the following holds:
    \begin{equation}
        1 \le \frac{\dhsq(p,q)}{\dhsq(\tprime p, \tprime q)}  \le \frac{\dhsq(p,q)}{\left(\sqrt{\Pr_{x \sim p}[\tprime(x)=1]} - \sqrt{\Pr_{x \sim q}[\tprime(x)=1]} \right)^2} \le \frac{3744 k'}{\alpha^2 \beta},
    \end{equation}
    where $k' = \log(4/\dhsq(p,q))$.
\end{theorem}
\begin{proof}
    The first part of the inequality follows from data-processing inequality, as remarked in \cite{pensia2023communication}. The second part of the inequality follows by definition of Hellinger distance. For the remaining portion, \emph{we merely state adjustments for the proof of \cite{pensia2023communication} to include the necessary terms with $\alpha,\beta$}. 

    \textit{``Case 1'' of \cite{pensia2023communication}.} Analogous to their notation (but for continuous distributions), let $A_{2,\infty}$ be the subset of the domain where $p(x)/q(x) \ge 2$. Then, let $p' \triangleq \Pr_{x \sim p}[x \in A_{2,\infty}]$. As they argue, then $\dhsq(p,q) \le 4 p'$. We now compute:

    \begin{align*}
        &\left(\sqrt{\Pr_{x \sim p}[\tprime(x)=1]}-\sqrt{\Pr_{x \sim q}[\tprime(x)=1]}\right)^2\\
        & \ge \left(\sqrt{\Pr_{x \sim p}[\tprime(x)=1]}-\sqrt{\frac{1}{1 + \alpha \cdot \delta} \Pr_{x \sim p}[\tprime(x)=1]}\right)^2 \intertext{Recall for this case, $\delta=1$:}
        & = \left(1 - \sqrt{\frac{1}{1 + \alpha}}\right)^2 \cdot \Pr_{x \sim p}[\tprime(x)=1] \\
        & \ge \left(1 - \sqrt{\frac{1}{1 + \alpha}}\right)^2 \cdot \beta \cdot \Pr_{x \sim p}[\tstar(x)=1] \intertext{Observe that $\Pr_{x \sim p}[\tstar(x) = 1] = p'$ and use $p' \ge \dhsq(p,q)/4$:}
        & \ge \left(1 - \sqrt{\frac{1}{1 + \alpha}}\right)^2 \cdot \beta \cdot \frac{\dhsq(p,q)}{4}\\
        & \ge \left(1 - \sqrt{1-\frac{\alpha}{2}}\right)^2 \cdot \beta \cdot \frac{\dhsq(p,q)}{4}\\
        & \ge \left(\frac{\alpha}{4}\right)^2 \cdot \beta \cdot \frac{\dhsq(p,q)}{4}\\
        & = \alpha^2 \cdot \beta \cdot \frac{\dhsq(p,q)}{64}\\
        & \implies \frac{\dhsq(p,q)}{\left(\sqrt{\Pr_{x \sim p}[\tprime(x)=1]}-\sqrt{\Pr_{x \sim q}[\tprime(x)=1]}\right)^2} \le \frac{64}{\alpha^2 \beta}
    \end{align*}

    \textit{``Case 2'' of \cite{pensia2023communication}.} Adjusting their notation for continuous distributions, let $A_{1,2}$ be the subset of the domain where $p(x)/q(x) \in (1,2)$. They consider a random variable $X$ in terms of $\delta(x) \triangleq \frac{p(x)-q(x)}{q(x)}$, where $\Pr[X > \delta] = \Pr_{x \sim q}[x \in A_{1,2}, \delta(x) > \delta]$ and $\Pr[X=0] = 1 - \Pr_{x \sim q}[x \in A_{1,2}]$. This random variable is insightful because, as they argue, $\dhsq(p,q) \le 4 \Ex[X^2]$. $\tstar$ chooses to threshold at $1+\delta$ where $\delta = \argmax_{\delta} \delta^2 \Pr[X \ge \delta^2]$.  We now lower bound $\dhsq(\tprime p, \tprime q)$ as they lower bounded $\dhsq(\tstar p, \tstar q)$:

    \begin{align*}
        & \left(\sqrt{\Pr_{x \sim p}[\tprime(x)=1]} - \sqrt{\Pr_{x \sim q}[\tprime(x)=1]}\right)^2\\
        & \ge \left(\sqrt{\Pr_{x \sim p}[\tprime(x)=1]} - \sqrt{\frac{1}{1 + \alpha \delta} \cdot \Pr_{x \sim p}[\tprime(x)=1]}\right)^2\\
        & \ge \left(\sqrt{\beta \cdot \Pr_{x \sim p}[\tstar(x)=1]} - \sqrt{\beta \cdot \frac{1}{1 + \alpha \delta} \cdot \Pr_{x \sim p}[\tstar(x)=1]}\right)^2\\
        & = \beta \cdot \Pr_{x \sim p}[\tstar(x)=1]  \left( 1 - \sqrt{1 - \frac{\alpha \delta}{1 + \alpha \delta}}\right)^2 \intertext{Using $\sqrt{1-\frac{x}{1+x}} \le 1 - \frac{x}{6}$ for $0 \le x \le 1$:}
        & \ge \beta \cdot \Pr_{x \sim p}[\tstar(x)=1] \cdot \left(\frac{\alpha \delta}{6}\right)^2 \\
        & = \frac{\alpha^2 \beta}{36} \cdot \Pr_{x \sim p}[\tstar(x)=1] \delta^2\\
        & \ge \frac{\alpha^2 \beta}{36} \cdot \Pr_{x \sim q}[\tstar(x)=1] \delta^2\\
        & = \frac{\alpha^2 \beta}{36} \cdot \Pr[X^2 \ge \delta^2] \delta^2\intertext{Using that $\delta = \argmax_{\delta} \delta^2 \Pr[X^2 \ge \delta^2]$ and their Lemma 3.7 (reverse Markov inequality):}
        & \ge \frac{\alpha^2 \beta}{36} \cdot \frac{\Ex[X^2]}{13 \cdot (1 + \log(1/\Ex[X^2]))} \intertext{Using $\Ex[X^2] \ge \dhsq(p,q)/4$:}
        & \ge \frac{\alpha^2 \beta}{144} \cdot \frac{\dhsq(p,q)}{13 \cdot (1 + \log(4/\dhsq(p,q)))}\\
        & \ge \frac{\alpha^2 \beta}{3744} \cdot \frac{\dhsq(p,q)}{\log(4/\dhsq(p,q))}\\
        & \implies \frac{\dhsq(p,q)}{\left(\sqrt{\Pr_{x \sim p}[\tprime(x)=1]} - \sqrt{\Pr_{x \sim q}[\tprime(x)=1]}\right)^2} \le \frac{3744 k'}{\alpha^2 \beta} \tag*{\qedhere} 
    \end{align*}

    
\end{proof}


\subsection{Approximating Likelihood Thresholds for Log-Concave Mixtures}\label{sec:logapprox}
\newcommand{\Ksup}{K^{\textrm{supp}}}

With \cref{thm:mod-thresh} in hand, we will now prove that for any $p$ satisfying our assumptions and a translation $p_{-\Delta}$, there is a channel $\tprime$ that is an indicator of intervals of the domain and $(\alpha,\beta)$-approximates $\tstar$. Let us define the likelihood ratio $r(x) \triangleq \frac{p(x)}{p(x+\Delta)}$ and a related function $t(x) \triangleq r(x) - 1$. Recall the first condition of $(\alpha,\beta)$-approximation: when $\tstar$ thresholds by $1+\tau^*$ we require $\tprime(x)=1$ only if $p(x)/q(x) \ge 1 + \alpha \tau^*$. In the language of our new functions, this is conveniently written as $\tprime(x)=1$ only if $t(x) \ge \alpha \tau^*$. Accordingly, we now prove a technical result using the structure of $t$ under our assumptions, that will enable both conditions of $(\alpha,\beta)$-approximation:

\begin{lemma}\label{lemma:intervals}
    Suppose $p$ is a distribution that is a centered/symmetric mixture of $k$ log-concave distributions. Let $r(x)$ and $t(x)$ be defined with respect to $p_{-\Delta}$, so $r(x) \triangleq \frac{p(x)}{p(x+\Delta)}$ and $t(x) \triangleq r(x)-1$.
     Consider parameters $\tmin,\delta$ where $0 < \tmin \le \frac{1}{k}$ and $0<\delta \le \min(\frac{\tmin^2}{k},\frac{1}{2})$. 
    Then, for any $\tau \in [\tmin,1]$, there exists a collection of $r =O(k \log(1/(\delta \tmin)))$ disjoint intervals $I = I_1 \cup \dots \cup I_r$ where $t(x) \ge \Omega(1) \cdot \tau$ for all $x \in I$, and $\Pr_{X \sim p}[x \in I] \ge \Omega(1) \cdot \Pr_{X \sim p}[t(x) \ge \tau] - O(\delta k/\tmin^2)$.

\end{lemma}

\begin{proof}
    Our main hope of accomplishing this will be to show that we can approximate $t$ sufficiently well (for most mass of $p$) by a piecewise-constant function with a small number of pieces.
    Then, selecting the pieces with large enough values relative to $\tau$, we will hopefully obtain a set of intervals satisfying our goal. 
    We will begin by introducing approximations for each $p_i$ and $t_i$.

    Without loss of generality, consider that the mixture is centered around $0$.

    \begin{lemma}[Piecewise-constant decomposition of log-concave densities; implicit in Lemma 27 of \cite{chan2014efficient}]\label{lemma:p-decomp}
        
        Let $q$ be a log-concave distribution over $\R$.
        For any $0<\delta \le \frac{1}{2}$, there exists a function $\tilde{q}$ which is a piecewise-constant function over $\R$ consisting of $O(\log(\frac{1}{\delta}))$ pieces.
        The function $\tilde{q}$ approximates $q$ in the sense that $\tilde{q}(x) \le q(x)$ for all $x \in \R$, $\tilde{q}(x) \ge \frac{1}{2} \cdot q(x)$ whenever $\tilde{q}(x) > 0$, and $\Pr_{x \sim q}[\tilde{q}(x)>0] \ge 1 -\delta$ where $\tilde{q}(x)=0$ only in the first and last piece of $\tilde{q}$ (a prefix and suffix of $\R$, respectively).

    \end{lemma}


    \begin{proof}
        This is implicitly shown in Lemma 27 of \cite{chan2014efficient} (stage (a) of their proof). Note how their proof uses one parameter, $\eps$, that determines both the multiplicative error ($\frac{1}{2}$ in our case) and the poorly-approximated mass in the tail ($\delta$ in our case), but that it yields this lemma statement when decoupling these parameters. We now provide brief intuition of the proof idea. Without loss of generality, suppose $q$ has its mode at $0$ and let us focus only on approximating the right half of the domain $[0,\infty]$. For all non-negative $i$, consider the $i$-th region to be the subset of the domain where $x$ is non-negative and $q(x) \in (\frac{q(0)}{2^{i+1}},\frac{q(0)}{2^i}]$. Observe that each region forms an interval of the domain: let the $i$-th region be $[a_i,b_i)$, and let $\ell_i \triangleq b_i-a_i$ be the length of the interval for the $i$-th region. 
        
        First, we remark that $\ell_i$ is non-increasing. For sake of contradiction, if this were not true, then $\frac{q(a_i + \ell_i)}{q(a_i)} < \frac{q(a_{i+1}+\ell_i)}{q(a_{i+1})}$, but this would violate log-concavity. Then, we remark that the probability from the $0$-th region is at least $\frac{q(0) \cdot \ell_0}{2}$, while the total probability from all regions with $i\ge j$ is at most $\frac{2 q(0)\cdot \ell_0}{2^j}$. Hence, for $j = O(\log(1/\delta))$, at most $\delta$ fraction of mass comes from regions after the $j$-th region, and the previous regions may all be approximated by powers of $2$ from $q(0)$ to $\frac{q(0)}{2^j}$.
    \end{proof}
    
    We will approximate each $p_i$ with $\tilde{p}_i$ using parameter $\delta$: resulting in $O(\log(1/\delta))$ pieces.

    
    Let us say that $\tilde{p}_i$ is \emph{supported} at all values of $x$ where $\tilde{p}_i(x)$ is nonzero, and \emph{unsupported} at all values of $x$ corresponding to the two (first and last) pieces that are $0$. 
    This notion aligns with where $\tilde{p}_i$ would be supported were it to be rescaled to define a probability density. 
 
    

    
    More generally, let us define our approximation $\tilde{p}$ for the entirety of $p$ as $\tilde{p}(x) \triangleq \sum_{i \in [k]} w_i \tilde{p}_i(x)$.
    Notice that $\tilde{p}(x)$ is a piecewise-constant function of $O(k \log(\frac{1}{\delta}))$ pieces: as $x$ increases from $x = -\infty$ towards $\infty$, the value of $\tilde{p}(x)$ only changes when one of $\tilde{p}_i(x)$ changes.

 We will call $\tilde{p}(x)$ \emph{valid} if all unsupported mixture components are negligible compared to $\tilde{p}(x)$:
    \begin{definition}\label{def:valid}
        $\tilde{p}(x)$ is \emph{$\kappa$-invalid} at value $x \in \mathbb{R}$ if and only if there exists an $i \in [k]$ where $\tilde{p}_i(x)$ is unsupported and $w_i \cdot p_i(x) \ge \kappa \cdot \tilde{p}(x)$. Otherwise $\tilde{p}(x)$ is \emph{$\kappa$-valid}.
    \end{definition}
    For ease of reading, sometimes we just state valid/invalid where $\kappa$ is implied.
    
    \begin{claim}\label{rem:tilde-tight}
        If $\tilde{p}(x)$ is $\kappa$-valid, for $\kappa \le \frac{1}{k}$, then $p(x)/4 \le \tilde{p}(x) \le p(x)$.
    \end{claim}
    \begin{proof}
    
        The latter half $\tilde{p}(x) \le p(x)$ holds even if $\tilde{p}(x)$ is invalid, by definition. 
        
        For the first half of our claim, we will analyze terms involving $p_i$ differently depending on whether or not $\tilde{p}_i(x)$ is supported at a value of $x$. 
        For convenience, let $K^{\textrm{supp}}(x) \subseteq [k]$ denote the mixtures where $\tilde{p}_i(x)$ is supported, and $K^{\textrm{unsupp}}(x) \subseteq [k]$ denote the complement. 
        Then, we bound:
        \begin{align*}
            p(x) - \tilde{p}(x) & = \sum_{i} w_i \left( p_i(x) - \tilde{p}_i(x) \right) \\
            & = \left( \sum_{i \in K^{\textrm{supp}}(x)} w_i \left( p_i(x) - \tilde{p}_i(x) \right) \right) + \left( \sum_{i \in K^{\textrm{unsupp}}(x)} w_i \left( p_i(x) - \tilde{p}_i(x) \right) \right) \intertext{Using that each supported $\tilde{p}_i(x) \in [p_i(x)/2,p_i(x)]$:}
            & \le \left( \sum_{i \in K^{\textrm{supp}}(x)} \frac{w_i p_i(x)}{2} \right) + \left( \sum_{i \in K^{\textrm{unsupp}}(x)} w_i p_i(x) \right) \\
            & = \frac{p(x)}{2} + \sum_{i \in K^{\textrm{unsupp}}(x)} \frac{w_i p_i(x)}{2} \intertext{Using that $\tilde{p}$ is valid:}
            & \le \frac{p(x)}{2} + \sum_{i \in K^{\textrm{unsupp}}(x)} \frac{\kappa \cdot \tilde{p}(x)}{2} \\
            & \le \frac{p(x)}{2} + \frac{k \cdot \kappa \cdot \tilde{p}(x)}{2} \intertext{Using $\kappa \le \frac{1}{k}$:}
            & \le \frac{p(x)}{2} + \frac{\tilde{p}(x)}{2} \\
            & \implies \tilde{p}(x) \ge \frac{p(x)}{2 \cdot (1 + \frac{1}{2})} \ge \frac{p(x)}{4} 
        \end{align*}
    \end{proof}
    We will show that $\tilde{p}(x)$ is valid for most of the mass
of $p$, and that these valid regions correspond to a small number of disjoint intervals:
    \begin{claim}\label{claim:bound-inval}
        If $\kappa \le \frac{1}{k}$, then $\Pr_{X \sim p}[\tilde{p}(x) \textrm{ is invalid}] \le O(\frac{\delta}{\kappa})$
    \end{claim}
    \begin{proof}
        Let $S \subset \mathbb{R}$ be the values of $x$ where $\tilde{p}(x)$ is invalid.

  
        By definition, the total mass where $\tilde{p}(x)$ is invalid can be written as:
        
        \begin{equation}
            \int_{x \in S} \left(\left( \sum_{i \in K^{\textrm{supp}}(x)} w_i \cdot p_i(x) \right) + \left( \sum_{i \in K^{\textrm{unsupp}}(x)} w_i \cdot p_i(x) \right)\right) \diff x \label{step:total-p-invalid}
        \end{equation}
    The latter summation of \cref{step:total-p-invalid} is upper bounded by:
    \begin{align}
        & \int_{x \in S} \left( \sum_{i \in K^{\textrm{unsupp}}(x)} w_i \cdot p_i(x) \right) dx \nonumber \\
        & \le \sum_{i=1}^{k} \int_{-\infty}^\infty \mathbbm{1}[\tilde{p}_i(x) \textrm{is unsupported}] \cdot  w_i \cdot p_i(x) dx \nonumber \intertext{Using the guarantees for $\tilde{p}_i$ from \cref{lemma:p-decomp}:}
        & \le \sum_{i=1}^k w_i \delta \nonumber \\
        & = \delta \label{step:delta-value}
    \end{align}

    Now, we bound the first summation of \cref{step:total-p-invalid}:
    \begin{align}
        & \int_{x \in S} \left( \sum_{i \in K^{\textrm{supp}}(x)} w_i \cdot p_i(x) \right) dx \nonumber \intertext{Using $p_i(x)/2 \le \tilde{p}_i(x) \le p_i(x)$ when $i$ is supported:}
        & \le \int_{x \in S} 2 \tilde{p}(x) dx \nonumber  \intertext{Since $\tilde{p}(x)$ is invalid, there must be an $i \in K^{\textrm{unsupp}}(x)$ where $w_i \cdot p_i(x) \ge \frac{1}{\kappa} \tilde{p}(x)$}
        & \le \int_{x \in S} \frac{2}{\kappa} \left( \sum_{i \in K^{\textrm{unsupp}}(x)} w_i \cdot p_i(x) \right) dx \nonumber  \intertext{Using the previous bound on this summation in \cref{step:delta-value}:}
        & \le \frac{2 \delta}{\kappa} = O\left(\frac{\delta}{\kappa}\right) \label{step:delta-value2}
    \end{align}

    Combining \cref{step:delta-value,step:delta-value2} yields $\Pr_{X \sim p}[\tilde{p}(x) \textrm{is invalid} ] \le O(\frac{\delta}{\kappa})$.
\end{proof}
 Moreover, we remark that the regions where $\tilde{p}$ is valid is the union of a small number of intervals:
 \begin{claim}\label{claim:valid-contig}
     The subset of $\mathbb{R}$ where $\tilde{p}(x)$ is $\kappa$-valid, is the union of at most $O(k \log(\frac{1}{\delta}))$ disjoint intervals.
 \end{claim}
 \begin{proof}
     For convenience, we use $\mathcal{D}_{\tilde{p}}$ to denote the set of intervals that correspond to the domain of each piece of $\tilde{p}$.
     Recall that $|\mathcal{D}_{\tilde{p}}| \le O(k \log(\frac{1}{\delta}))$.
     Also, recall our definition of invalidation that $\tilde{p}(x)$ is only $\kappa$-invalid if there is a $j \in [k]$ where $\tilde{p}_j(x)$ is unsupported and $w_j \cdot p_j(x) \ge \kappa \cdot \tilde{p}(x)$.

     For a naive analysis, observe that we are examining the domain after removing all regions of the domain where $\tilde{p}(x)$ is invalid.
     Generally, if we were to remove some number $Z$ of intervals from the domain, then the resulting subset of the domain is at most $Z+1$ intervals.
     This enables a simple analysis: for every pair of interval $\mathcal{I} \in \mathcal{D}_{\tilde{p}}$ and index $j \in [k]$, the distribution $p_j$ can only invalidate one interval among $\mathcal{I}$ (because $p_j$ is unimodal and $\tilde{p}$ is constant within $\mathcal{I}$).
     Thus, the subset of $\mathbb{R}$ where $\tilde{p}(x)$ is valid corresponds to at most $|\mathcal{D}_{\tilde{p}}| \cdot k + 1 \le O(k^2 \log(\frac{1}{\delta}))$ intervals.
     
     We will improve upon this by a factor of $k$ with a more careful argument.
     Let us study how a distribution $p_j$ may invalidate part of an interval $\mathcal{I} \in \mathcal{D}_{\tilde{p}}$.
     If the maximum value of $p_j$ is attained before the start of $\mathcal{I}$,\footnote{This claim is proven in general for log-concave $k$-mixtures, where the proof would be slightly simplified if we decided to leverage the centering.}
     then by unimodality of $p_j$, $j$ can only make a prefix of $\mathcal{I}$ invalid.
     Similarly, if the maximum value of $p_j$ is attained after $\mathcal{I}$, then $j$ can only make a suffix of $\mathcal{I}$ invalid.
     Meaning, if we ignore invalidations that occur from $p_j$ having a maxima inside $\mathcal{I}$, then $\tilde{p}$ is valid for everything in $\mathcal{I}$ that is not contained in the largest invalidating prefix or the largest invalidating suffix.
     Thus, when ignoring invalidation that occurs from such $p_j$, the subset of $\mathbb{R}$ where $\tilde{p}(x)$ is valid corresponds to a number of disjoint intervals that is at most $|\mathcal{D}_{\tilde{p}}|$.
     Finally, if we now consider for each $j$ the piece of $\tilde{p}$ that contains the maxima of $p_j$, and invalidate the one interval that $p_j$ invalidates (or possibly no interval), the number of non-deleted intervals of the domain increases by at most $1$.
 In total, the region where $\tilde{p}$ is valid is the union of $\le |\mathcal{D}_{\tilde{p}}| + k \le O(k \log(\frac{1}{\delta}))$ disjoint intervals.

 \end{proof}

Our last component will introduce our approximation for $t$, defined with respect to an approximation of each $t_i$:
  \begin{lemma}[$\tilde{t}_i$ decomposition]\label{lemma:t-decomp}
        For any log-concave distribution $q$, there exists a function $\tilde{t}$ over $\mathbb{R}$ that is piecewise-constant over $O(\log(\frac{t_{\textrm{high}}}{t_{\textrm{low}}}))$ pieces.
        The function $\tilde{t}$ approximates $t(x) \triangleq \frac{q(x)}{q(x+\Delta)} - 1$ in the sense that $\tilde{t}$ is within a factor of $2$ of $t(x)$ when $t(x)\in(t_\textrm{low},t_\textrm{high})$, $\tilde{t}(x)=0$ when $t(x)< t_\textrm{low}$, and $\tilde{t}(x)=t_\textrm{high}$ when $t(x)\ge t_\textrm{high}$.
    \end{lemma}
    \begin{proof}
        It is sufficient to show $t(x)$ is monotone by showing $\log(r(x))$ is monotone, as then $t(x) \triangleq 2^{\log(r(x))} - 1$ is monotone. 
        Recall that any log-concave distribution $q(x)$ can be written as $e^{-V(x)}$ where $V$ is a convex function.
        Then, $\log(r(x)) \triangleq V(x+\Delta)-V(x)$ which is monotone by convexity of $V$. 
As $t(x)$ is monotone, we can obtain this decomposition by setting $\tilde{t}(x)$ accordingly when it is smaller than $t_{\textrm{low}}$ or larger than $t_{\textrm{high}}$, and to the $O(\log(\frac{t_{\textrm{high}}}{t_{\textrm{low}}}))$ powers of $2$ in between.
    \end{proof}
    We will approximate each $t_i$ with $\tilde{t}_i$ using $t_{\textrm{low}}=\tmin^2$ and $t_{\textrm{high}}=1$: resulting in $O(\log(1/\tmin))$ pieces. 
    We combine these $\tilde{t}_i(x)$ to produce $\tilde{t}(x)$, our approximation for $t(x)$, and show that it is a good approximation and piecewise-constant for a small number of pieces:
\begin{definition}[$\tilde{t}$ approximation]
    $\tilde{t}(x) \triangleq \sum_{i \in [k]} \tilde{t}_i(x) \cdot \frac{w_i \tilde{p}_i(x)}{\tilde{p}(x)}$
\end{definition}
\begin{remark} \label{rem:t-bound}
    $\tilde{t}(x) \le 1$
\end{remark}
\begin{proof}
    Each $\tilde{t}_i(x) \le 1$ from \cref{lemma:t-decomp} with $t_{\textrm{high}}=1$.
    So:
    \begin{align*}
        \tilde{t}(x) & \triangleq \sum_{i \in [k]} \tilde{t}_i(x) \cdot \frac{w_i \tilde{p}_i(x)}{\tilde{p}(x)}\\
        & = \sum_{i \in K^{\textrm{supp}}(x)} \tilde{t}_i(x) \cdot \frac{w_i \tilde{p}_i(x)}{\tilde{p}(x)}\\
        & \le \sum_{i \in K^{\textrm{supp}}(x)} \frac{w_i \tilde{p}_i(x)}{\tilde{p}(x)} \\
        & = 1
    \end{align*}
\end{proof}
We show $\tilde{t}$ is constant and is a good approximation for $t$ %
whenever all $x$ in the interval are non-negative, $\tilde{p}$ is valid, all $\tilde{p}_i$ are constant, and all $\tilde{t}_i$ are constant:\footnote{
We note that before this, nothing has required that $p$ is a centered/symmetric mixture, only that its components are log-concave.
Now we will leverage how the mixture is centered.}
\begin{claim}\label{claim:good-t-tilde}
    For any interval where all $x \ge 0$, $\tilde{p}_i(x)$ are constant, $\tilde{p}(x)$ is $\kappa$-valid for $\kappa \le \frac{\tmin^2}{k}$, and all $\tilde{t}_i$ are constant, then $\tilde{t}(x) = \Theta(1) \cdot \min\left(t(x), 1\right) - O\left(\tmin^2\right)$.

\end{claim}
\begin{proof}
    We begin by noting simple equivalent forms of $t(x)$:
    \begin{align}
        t(x) & \triangleq \frac{p(x)}{p(x + \Delta)} - 1 \nonumber \\
        & = \sum_{i=1}^k \frac{w_i \cdot p_i(x)}{p(x+\Delta)} - 1 \nonumber \\
        & = \sum_{i=1}^k \frac{w_i \cdot p_i(x) - w_i \cdot p_i(x+\Delta)}{p(x+\Delta)} \label{step:t-form1}\\
        & = \sum_{i=1}^k \frac{\left( \frac{p_i(x)}{p_i(x+\Delta)} - 1\right) \cdot  w_i \cdot p_i(x+\Delta)}{p(x+\Delta)}\nonumber  \\
        & = \sum_{i=1}^k \frac{t_i(x) \cdot  w_i \cdot p_i(x+\Delta)}{p(x+\Delta)} \label{step:t-form2}
    \end{align}
    We will mostly use forms \cref{step:t-form1} and \cref{step:t-form2}, noting also that equality holds for each summand, so we may define the summation with some summands in one form and some in the other form.

 Throughout this proof, we will utilize how when $x\ge 0$ all summands are non-negative due to the mixture being centered at $0$.
 For example, $\tilde{t}(x)$ would approximate $t(x)$ if we could show each summand in $\tilde{t}(x)$ multiplicatively approximates the corresponding summand in $t(x)$, but this would not hold if the summands could be positive and negative, as is the case if $p$ is not a centered mixture.

    

    

    With all the pieces in place, we are ready to show that $\tilde{t}(x)$ is a good approximation of $t(x)$. 
    We will proceed by analyzing two cases.
    First, when $p(x+\Delta) \ge \Omega(1) \cdot p(x)$, then we can well-approximate each summand in $t(x)$.
    Otherwise, when $p(x+\Delta) \ll p(x)$, then $t(x) \ge 1$, and we will show that our summation will also be $\Omega(1)$, which sufficiently well-approximates $t$.

    \textbf{Case 1: $p(x+\Delta) \ge \frac{1}{16} p(x)$.} 








We will drop from the summation $t(x)$ the indices corresponding to unsupported components of the mixture, and components for which $t_i$ is small; we claim that this does not affect the value of $t(x)$ significantly:

    \begin{remark}\label{rem:und-negl}
        $\sum_{i \in K^{\textrm{unsupp}}(x)} \frac{w_i p_i(x) - w_i p_i(x+\Delta)}{p(x+\Delta)} \le 16\tmin^2$
        \footnote{$\sum_{i \in K^{\textrm{unsupp}}(x)} \frac{w_i p_i(x) - w_i p_i(x+\Delta)}{p(x+\Delta)} \le \sum_{i \in K^{\textrm{unsupp}}(x)} \frac{w_i p_i(x)}{p(x+\Delta)} \le 16 \cdot \sum_{i \in K^{\textrm{unsupp}}(x)} \frac{w_i p_i(x)}{p(x)} \le  16 \cdot \sum_{i \in K^{\textrm{unsupp}}(x)} \frac{w_i p_i(x)}{\tilde{p}(x)} \le  16 \cdot \sum_{i \in K^{\textrm{unsupp}}(x)} \kappa \le  16 \tmin^2$ where the second step used $p$ is unimodal, the penultimate step used $\tilde{p}$ is $\kappa$-valid, and the last step used $\kappa \le \frac{\tmin^2}{k}$.}
    \end{remark}\

    \begin{remark}\label{rem:small-ti-negl}
        $\sum_{i \textrm{ s.t. } t_i(x) \le \tmin^2} \frac{t_i(x) \cdot w_i \cdot p_i(x+\Delta)}{p(x+\Delta)} \le \tmin^2$
        \footnote{$\sum_{i \textrm{ s.t. } t_i(x) \le \tmin^2} \frac{t_i(x) \cdot w_i \cdot p_i(x+\Delta)}{p(x+\Delta)} \le \tmin^2 \cdot \sum_{i \textrm{ s.t. } t_i(x) \le \tmin^2} \frac{w_i \cdot p_i(x+\Delta)}{p(x+\Delta)} \le \tmin^2$}
    \end{remark}

Hence, 
\begin{align*}
t(x) &= \sum_{\substack{i \in K^{supp}(x)\\t_i(x) > \tmin^2}} w_i\cdot \frac{p_i(x) - p_i(x+\Delta)}{p(x+\Delta)} + O\left(\tmin^2\right).
\end{align*}
The denominator in the sum, $p(x+\Delta) = \Theta(1) \cdot \tilde{p}(x)$, first by the assumption that $p(x+\Delta) \ge \frac{1}{16} p(x)$ (the upper bound $p(x+\Delta) \le p(x)$ is immediate since we have assumed $x \ge 0$), and by the fact that $x$ is valid so $\tilde{p}(x) = \Theta(1) \cdot p(x)$ by \cref{rem:tilde-tight}.
We argue that for each term $i$ in the above summation, 

\begin{subclaim}\label{sub:diff-to-prod}
    $p_i(x) - p_i(x+\Delta) = \Theta(1)\cdot \tilde{t}_i(x)\cdot \tilde{p}_i(x)$
\end{subclaim}

\begin{proof}
    \textbf{Case (i): $t_i(x) \in (\tmin^2,1]$.}
    First, from \cref{lemma:t-decomp}, for terms where $t_i(x) \in (\tmin^2,1]$, $\tilde{t}_i(x)$ is a multiplicative constant-factor approximation of $t_i(x)$.
Hence by \cref{step:t-form2} we can write 
\[
p_i(x)-p_i(x+\Delta)
= t_i(x) p_i(x+\Delta)
= \Theta(1)\cdot \tilde{t}_i(x)  \cdot p_i(x+\Delta).
\]
Now, $t_i(x) \le 1$, implying that $p_i(x+\Delta) \ge \frac{1}{2} p_i(x)$.
Since $x \ge 0$ we always have $p_i(x+\Delta) \le p_i(x)$.
Furthermore, since $i$ is supported, $p_i(x) = \Theta(1)\cdot \tilde{p}_i(x)$.
Hence $p_i(x) - p_i(x+\Delta) = \Theta(1) \cdot \tilde{t}_i(x) \cdot \tilde{p}_i(x)$.

\textbf{Case (ii): $t_i(x) > 1$.}
Next, for the remaining terms where $1 < t_i(x) = \frac{p_i(x)}{p_i(x+\Delta)}-1$, we have by re-arranging that $p_i(x) > 2p_{i}(x+\Delta)$ and therefore $p_i(x) - p_i(x+\Delta) = \Theta(1) \cdot p_i(x)$.
Further, since $i \in \Ksup(x)$, $\tilde{p}_i(x) = \Theta(1) \cdot p_i(x)$.
Therefore, using that $\tilde{t}_i(x)=1$ when $t_i(x)>1$:
\begin{align*}
p_i(x)-p_i(x+\Delta)
&= \Theta(1) \cdot \tilde{p}_i(x) = 
\Theta(1) \cdot \tilde{t}_i(x) \cdot \tilde{p}_i(x) \tag*{\qedhere} 
\end{align*} 
\end{proof}
Putting this together, \cref{sub:diff-to-prod} results in:
\begin{align*}
t(x) &= \sum_{\substack{i \in \Ksup(x)\\ t_i(x) > \tmin^2}} w_i \cdot
\Theta(1)\cdot \frac{\tilde{t}_i(x) \tilde{p}_i(x)}{p(x+\Delta)} + O\left(\tmin^2\right) \intertext{Using our assumption $p(x+\Delta) \ge \frac{1}{16}p(x)$ and \cref{rem:tilde-tight} from validity of $\tilde{p}(x)$:}
&= \sum_{\substack{i \in \Ksup(x)\\ t_i(x) > \tmin^2}} w_i \cdot
\Theta(1)\cdot \frac{\tilde{t}_i(x) \tilde{p}_i(x)}{\tilde{p}(x)} + O\left(\tmin^2\right) \intertext{Using that $\tilde{t}_i(x)=0$ when $t_i(x) < \tmin^2$ or $i \notin \Ksup(x)$:}
&= \Theta(1)\cdot \tilde{t}(x) + O\left(\tmin^2\right).
\end{align*}

    \textbf{Case 2: $p(x+\Delta) < \frac{1}{16} p(x)$.} Observe that $\tilde{t}(x) \le 1$ as in \cref{rem:t-bound}, and that if $p(x+\Delta) < \frac{1}{2}p(x)$ then $t(x)\ge 1$.
 Thus, to show $\tilde{t}(x) = \Theta(1) \cdot \min(t(x),1) - O(\tmin^2)$ it is sufficient to show $\tilde{t}(x) = \Omega(1)$ in this case. Our main intuition is that for $p(x+\Delta)$ to be much smaller than $p(x)$, then most of the mass must correspond to large $t_i(x)$ and accordingly our weighted sum of $\tilde{t}_i(x)$ will also be large. We now analyze the value of $\tilde{t}(x)$:




    \begin{align}
        \tilde{t}(x) & \triangleq \sum_{i \in K^{\textrm{supp}}(x)} \tilde{t}_i(x) \cdot \frac{w_i \tilde{p}_i(x)}{\tilde{p}(x)} \\
        \intertext{Let us focus on the contribution from summands with large $t_i(x)$ as we believe it must be significant for $p(x+\Delta)$ to be small:}
        & \ge \sum_{i \in K^{\textrm{supp}}(x)} \mathbbm{1}_{t_i(x) \ge 1} \cdot \tilde{t}_i(x) \cdot \frac{w_i \tilde{p}_i(x)}{\tilde{p}(x)} \\
        & = \sum_{i \in K^{\textrm{supp}}(x)} \mathbbm{1}_{t_i(x) \ge 1} \cdot \frac{w_i \tilde{p}_i(x)}{\tilde{p}(x)} \\
        \intertext{Additionally, because $\tilde{p}(x)$ is valid and all $\tilde{p}_i(x)$ are supported, we can convert from our approximations of $p$ and $p_i$ to the actual terms:}
        & \ge \Omega(1) \cdot \frac{1}{p(x)} \cdot  \sum_{i \in K^{\textrm{supp}}(x)} \mathbbm{1}_{t_i(x) \ge 1} \cdot w_i p_i(x)\\
        \intertext{At this point, we just need to lower bound the total mass from supported $p_i$ having $t_i(x)\ge 1$.
        Note that we can lower bound the total mass from all supported $p_i$ as $\sum_{i \in K^{\textrm{supp}}(x)} w_i p_i(x) \ge \tilde{p}(x) \ge p(x)/4$ by \cref{rem:tilde-tight}. Then, if at least $p(x)/8$ mass came from supported $p_i$ with $t_i(x)\le 1$, it would hold that $p(x+\Delta) \ge \frac{1}{16} p(x)$: violating our casework. Accordingly, we know $\sum_{i \in K^{\textrm{supp}}(x)} \mathbbm{1}_{t_i(x) \ge 1} \cdot w_i p_i(x) \ge p(x)/8$. Using this, we finish by:}
        & \ge \frac{1}{2 p(x)} \cdot \frac{p(x)}{8} = \Omega(1) \label{step:use-ind} \tag*{\qedhere} 
    \end{align}

\end{proof}
\textbf{Concluding the desired set of intervals. } Finally, our proof of \cref{lemma:intervals} concludes by considering all intervals satisfying the conditions of \cref{claim:good-t-tilde}: $x \ge 0$, all $\tilde{p}_i(x)$ are constant, $\tilde{p}(x)$ is $\kappa$-valid, and all $\tilde{t}_i(x)$ are constant. Recall that we seek to find a collection of $r$ disjoint intervals $I = I_1 \cup \dots \cup I_r$ where: (i) $r = O(k \log(n))$, (ii) $\Pr_{X \sim p}[x \in I] \ge \Omega(1) \cdot \Pr_{X \sim p}[t(x) \ge \tau] - O(\delta k /\tmin^2)$, and (iii) $t(x) \ge \Omega(1) \cdot \tau$ for all $x \in I$. We will choose $I_1 \cup \dots \cup I_r$ to be the subset of the intervals intervals from \cref{claim:good-t-tilde} where $\tilde{t}(x) \ge C_1 \cdot \tau$ for a particular $C_1 > 0$.

We have yet to choose the parameter $\kappa$. We set $\kappa = \frac{\tmin^2}{k}$ as it is the largest value that lets us use \cref{claim:good-t-tilde}.

By \cref{claim:valid-contig} we know all $\kappa$-valid mass consists of $O(k \log(1/\delta))$ disjoint intervals.
As all $\tilde{p}_i$ and $\tilde{t}_i$ only change at most $O(k \log(1/(\delta \tmin))$ times in total, the number of disjoint intervals we are considering is thus $O(k \log(1/(\delta \tmin)))$.
Since we choose a subset of these intervals, $r = O(k \log(1/(\delta \tmin)))$: satisfying (i).

Let us observe how restricting to $x \ge 0$ does not limit us much.
For any negative value $x_- < 0$ where $t(x_-)>\tau$, note how there is a mapping to $x_+ \triangleq -x_-$ which is positive and $t(x_-) \le t(x_+)$ because $p$ is symmetric and unimodal, meaning $t(x_-)=\frac{p(x_-)}{p(x_- + \Delta)}-1 = \frac{p(x_+)}{p(x_- + \Delta)}-1 \le \frac{p(x_+)}{p(x_+ + \Delta)}-1 = t(x_+)$.
Thus, $\Pr_{X \sim p}[t(x) \ge \tau \cap x\ge 0] \ge \frac{1}{2} \cdot \Pr_{X \sim p}[t(x) \ge \tau]$. For any $x$ satisfying $t(x)\ge \tau$, $x\ge0$, and $\tilde{p}(x)$ is valid, then \cref{claim:good-t-tilde} will imply $\tilde{t}(x) \ge \Omega(1) \cdot \tau - O(\tmin^2)$. Without loss of generality, suppose $1/\tmin$ is at least a sufficiently large constant, then we could conclude $\tilde{t}(x) \ge \Omega(1) \cdot \tau$ under our conditions. If $1/\tmin$ is not this large, we can simply consider the guarantees of this lemma for a small enough $\tmin$ (that is still a constant bounded away from $0$), and see that it implies the lemma for large $\tmin$. So, since $\tilde{t}(x) \ge \Omega(1) \cdot \tau$, if we set $C_1$ sufficiently small then $x$ will be in our collection $I$. We may then conclude
\begin{align*}
    & \Pr_{X \sim p}[x \in I]\\
    & \ge \Pr_{X \sim p}[t(x) \ge \tau \cap x \ge 0 \cap \tilde{p}(x) \textrm{ is valid}] \\
    & \ge \Pr_{X \sim p}[t(x) \ge \tau \cap x \ge 0] - \Pr_{X \sim p}[\tilde{p}(x) \textrm{ is invalid}] \intertext{Using \cref{claim:bound-inval}:}
    & \ge \frac{1}{2} \Pr_{X \sim p}[t(x) \ge \tau] - O\left(\frac{\delta}{\kappa}\right)\\
    & = \frac{1}{2} \Pr_{X \sim p}[t(x) \ge \tau] - O(\delta k/\tmin^2),
\end{align*}

satisfying (ii). 

 
Moreover, by \cref{claim:good-t-tilde} we know $\tilde{t}(x) = \Theta(1) \cdot \min(t(x),1)+O(\tmin^2)$, implying $t(x) \ge \Omega(1) \cdot (\tilde{t}(x) - O(\tmin^2))$. As before, without loss of generality we may suppose $1/\tmin$ is at least a sufficiently large constant, so the $O(\tmin^2)$ term is negligible compared to the $\tilde{t}(x) \ge C_1 \tau \ge C_1 \tmin$ term. So, there will be a $C_2 > 0$ where any such value of $x$ in one of these ranges where $\tilde{t}(x) \ge C_1 \cdot \tau$, must then satisfy $t(x) \ge C_2 \cdot \tau$, hence implying our final condition (iii) that $t(x) \ge \Omega(1) \cdot \tau$ for all $x \in I$.

\end{proof}

We may now combine how \cref{thm:mod-thresh} shows that an approximate likelihood threshold channel approximately preserves Hellinger distance and \cref{lemma:intervals} yields that an interval statistic can approximate a likelihood threshold channel:

\begin{corollary}\label{cor:hk-to-int}
    Suppose $p$ is a distribution that is a centered/symmetric mixture of $k$ log-concave distributions. For any $\mu$ and $\Delta \ge 0$, there exists an interval that approximately preserves the Hellinger distance between $p_{\mu}$ and $p_{\mu-\Delta}$. In particular, there is an interval $I^* \triangleq [\mu+a,\mu+b]$, for $0 \le a < b$, where

    \begin{align*}
        & \left( \sqrt{\Pr_{x \sim p_{\mu}}[x \in [\mu+a,\mu+b]]} -  \sqrt{\Pr_{x \sim p_{\mu-\Delta}}[x \in [\mu+a,\mu+b]]} \right)^2 \\
        & \ge \Omega(1) \cdot \frac{\dhsq(p_{\mu},p_{\mu-\Delta})}{k \log(4k/\dhsq(p_{\mu},p_{\mu-\Delta})) \cdot \log(4/\dhsq(p_{\mu},p_{\mu-\Delta}))}.
    \end{align*}
\end{corollary}
\begin{proof}
    Consider the optimal thresholding channel $\tstar$ from \cref{thm:og-thresh} with thresholding parameter $\tau^*$ and properties discussed in \cref{remark:opt-props}.
    We hope to approximate this channel with $\tprime(x) \triangleq \mathbbm{1}_{x \in [\mu+a,\mu+b]}(x)$ in the $(\alpha,\beta)$ sense that \cref{thm:mod-thresh} implies would approximately preserve Hellinger distance.

    To achieve $(\alpha,\beta)$-approximation, we must satisfy: (1) $\tprime(x)=1$ only if $p_\mu(x)/p_{\mu-\Delta}(x) \ge 1 + \alpha \tau^*$ for $0 < \alpha \le 1$, and (2) $\Pr_{x \sim p}[\tprime(x)=1] \ge \beta \cdot \Pr_{x\sim p}[\tstar(x)=1]$ for $0 < \beta \le 1$.

    If we invoke \cref{lemma:intervals} with $\tmin \le \tau^*$ and use $\tau=\tau^*$, then all intervals will satisfy $t(x) \ge \Omega(1) \cdot \tau^*$. Recall by \cref{remark:opt-props} (3) that $\tau^* \ge \sqrt{\frac{\dhsq(p_{\mu},p_{\mu-\Delta})}{104 \log(4/\dhsq(p_{\mu},p_{\mu-\Delta}))}}$. So, we may set $\tmin = \min \left(\sqrt{\frac{\dhsq(p_{\mu},p_{\mu-\Delta})}{104 \log(4/\dhsq(p_{\mu},p_{\mu-\Delta}))}}, \frac{1}{k} \right)$, and thus we approximate with $\alpha = \Omega(1)$. 

    Also, recall by \cref{remark:opt-props} (4) that $\Pr_{x \sim p}[\tstar(x)=1] \ge \frac{\dhsq(p_{\mu},p_{\mu-\Delta})}{1800 \log(4/\dhsq(p_{\mu},p_{\mu-\Delta}))}$. Accordingly, if we invoke \cref{lemma:intervals} with $\delta= C \cdot \frac{\dhsq(p_{\mu},p_{\mu-\Delta}) \cdot \tmin^2}{1800 \log(4/\dhsq(p_{\mu},p_{\mu-\Delta})) \cdot k}$ for sufficiently small $C$, then $\Pr_{x \sim p}[x \in I] \ge \Omega(1) \cdot \Pr_{x \sim p}[\tstar(x)=1]$. Hence, choosing $I^*$ to be the interval with the most probability mass among those yielded by \cref{lemma:intervals}:
    \begin{align*}
        & \Pr_{x \sim p}[x \in I^*] \\
        & \ge \frac{1}{r} \cdot \Pr_{x \sim p}[x \in I]\\
        & \ge \Omega(1) \cdot \frac{1}{r} \cdot \Pr_{x \sim p}[\tstar(x)=1]\\
        & \ge \Omega(1) \cdot \frac{1}{r} \cdot \Pr_{x \sim p}[\tstar(x)=1]\\
        & \ge \Omega\left(\frac{1}{k \log(1/(\tmin \cdot \delta))}\right) \cdot \Pr_{x \sim p}[\tstar(x)=1]\intertext{Using $\delta = C \cdot \frac{\dhsq(p_{\mu},p_{\mu-\Delta}) \cdot \tmin^2}{1800 \log(4/\dhsq(p_{\mu},p_{\mu-\Delta})) \cdot k}$:}
        & \ge \Omega\left(\frac{1}{k \cdot (1 + \log(1/\dhsq(p_{\mu},p_{\mu-\Delta})) + \log(k) + \log(1/\tmin))}\right) \cdot \Pr_{x \sim p}[\tstar(x)=1] \intertext{Using $\tmin = \min\left(\sqrt{\frac{\dhsq(p_{\mu},p_{\mu-\Delta})}{104 \log(4/\dhsq(p_{\mu},p_{\mu-\Delta}))}}, \frac{1}{k}\right)$:}
        & \ge \Omega\left(\frac{1}{k \cdot (1 + \log(1/\dhsq(p_{\mu},p_{\mu-\Delta}))+\log(k))}\right) \cdot \Pr_{x \sim p}[\tstar(x)=1]\\
    \end{align*}
    Thus, we approximate with $\beta = \Omega\left(\frac{1}{k \cdot \log(4k/\dhsq(p_{\mu},p_{\mu-\Delta}))}\right)$. Using \cref{thm:mod-thresh}, we conclude:

    \begin{align*}
        & \left( \sqrt{\Pr_{x \sim p_{\mu}}[x \in [\mu+a,\mu+b]]} -  \sqrt{\Pr_{x \sim p_{\mu-\Delta}}[x \in [\mu+a,\mu+b]]} \right)^2 \\
        & \ge \frac{\alpha^2 \beta \cdot \dhsq(p_{\mu},p_{\mu-\Delta})}{3744 \log(4/\dhsq(p_{\mu},p_{\mu-\Delta}))} \\
        & \ge \Omega\left(\frac{1}{k \cdot \log(4k/\dhsq(p_{\mu},p_{\mu-\Delta}))}\right) \cdot \frac{\dhsq(p_{\mu},p_{\mu-\Delta})}{3744 \log(4/\dhsq(p_{\mu},p_{\mu-\Delta}))}\\
        & \ge \Omega(1) \cdot \frac{\dhsq(p_{\mu},p_{\mu-\Delta})}{k \log(4k/\dhsq(p_{\mu},p_{\mu-\Delta})) \cdot \log(4/\dhsq(p_{\mu},p_{\mu-\Delta}))}.
    \end{align*}
    
\end{proof}

\subsection{Obtaining an Algorithm for Mean Estimation}\label{sec:alg}
Our goal is to conclude that for any estimate $\muh$ where $|\mu - \muh|$ is sufficiently large, we can detect this in the form of an interval statistic, where the number of samples within $[\muh-b,\muh-a]$ is noticeably different from the number of samples within $[\muh+a,\muh+b]$ for $0 \le a < b$: hence witnessing that the distribution is not symmetric around $\muh$. Then, any $\muh$ that does not have such a distinguishing interval statistic would be a sufficiently good estimate of $\mu$. Our algorithm will then search for a $\muh$ without such a distinguishing statistic. We formalize this with \cref{algo:ident}.

\begin{algorithm}[t]
    \caption{Identifiability Algorithm} \label{algo:ident}
    \hspace*{\algorithmicindent} 
    \begin{flushleft}
      {\bf Input:} samples (accessed via $\rho(\cdot)$) and testing parameter $\gamma$  \\
      {\bf Output:} estimate $\muh$\\
      {\bf Description:} This (inefficient) algorithm will output any $\muh$ that passes all possible tests.
    \end{flushleft}
    \begin{algorithmic}[1]
    
    \Procedure{Test}{$\muh,a,b,\gamma$}:
    \State $L \gets \cnt(\muh - b,  \muh - a)$ \Comment{Count samples within $[\muh - b, \muh - a]$.}
    \State $R \gets \cnt(\muh+a,\muh + b)$  \Comment{Count samples within $[\muh+a,\muh + b]$.}
    
    \If{$\left|\sqrt{L} - \sqrt{R} \right| > \gamma$} 
    
            \Return FAIL 
    \Else
    
        \Return PASS 
    \EndIf

    \EndProcedure

    \Procedure{Estimate}{$\gamma$}
    
        \Return any $\muh$ that passes $\operatorname{Test(\muh,a,b,\gamma)}$ for all values of $0 \le a < b$
    \EndProcedure
    \end{algorithmic}
\end{algorithm}

Leveraging \cref{cor:hk-to-int} lets us almost immediately show that poor $\muh$ will have a test that captures almost all Hellinger distance:

\begin{corollary}\label{cor:test-exists}
    Suppose $p$ is a distribution that is a centered/symmetric mixture of $k$ log-concave distributions.  Let $\Delta \triangleq |\mu - \muh|$. Then, there is a test around $\muh$ that preserves the Hellinger distance. In particular, there are values $0 \le a < b$ where

    \begin{align*}
        & \left| \sqrt{\Pr_{x \sim p_{\mu}}[x \in [\muh-b,\muh-a]]} -  \sqrt{\Pr_{x \sim p_{\mu}}[x \in [\muh+a,\muh+b]]} \right| \\
        & \ge \Omega(1) \cdot \sqrt{\frac{\dhsq(p_{\mu},p_{\mu-2\Delta})}{k \log(4k/\dhsq(p_{\mu},p_{\mu-2\Delta})) \cdot \log(4/\dhsq(p_{\mu},p_{\mu-2\Delta}))}}.
    \end{align*}
\end{corollary}
\begin{proof}
    Without loss of generality, consider $\muh < \mu$. Let $a_{2 \Delta},b_{2 \Delta}$ be the values of $a,b$ yielded by \cref{cor:hk-to-int} when used on distributions $p_\mu,p_{\mu-2\Delta}$. For our test, we will choose values $a^*,b^*$ where $a^* \triangleq \Delta + a_{2\Delta}$ and $b^* \triangleq \Delta + b_{2\Delta}$. Then, our corollary immediately holds from realizing $\Pr_{x \sim p_{\mu}}[x \in [\muh+a^*,\muh+b^*]] = \Pr_{x \sim p_{\mu}}[x \in [\mu+a_{2\Delta},\mu+b_{2\Delta}]]$ and $\Pr_{x \sim p_{\mu}}[x \in [\muh-b^*,\muh-a^*]] = \Pr_{x \sim p_{\mu-2\Delta}}[x \in [\mu+a_{2\Delta},\mu+b_{2\Delta}]]$.
\end{proof}

What remains is to show is that if we choose $\gamma$ correctly, then with high probability, $\mu$ will pass all tests with the empirical samples, and all bad $\muh$ will fail some test with the empirical samples:

\begin{theorem}\label{thm:ident-log}
    Suppose $p$ is a distribution that is a centered/symmetric mixture of $k$ log-concave distributions. There exists some universal constants $\cgam,\cdist \ge 1$, 
    where if
    \begin{equation*}
        \Delta^* \triangleq \omega_p \left(\cdist \cdot \frac{k}{n} \cdot \log(2n/\delta) \cdot \log^2(2n) \right)
    \end{equation*}
    then with probability $1-\delta$ the output $\muh$ of \cref{algo:ident} with $\gamma = \cgam \cdot \sqrt{\log(2n/\delta)}$ will satisfy $|\mu - \muh| \le \Delta^*/2$.
\end{theorem}
\begin{proof}
We will leverage normalized uniform convergence guarantees that are tighter for $f$ with small $\Ex[f]$. This is a standard tool, and we will use the particular form of Lemma 1 of \cite{dasgupta2007general} for convenience (which itself references \cite{vapnik2015uniform,bousquet2003introduction}). The following directly holds from Lemma 1 of \cite{dasgupta2007general} and the Sauer-Shelah lemma (e.g. see Lemma 1 on page 184 of \cite{bousquet2003introduction}):

\begin{lemma}[Normalized uniform convergence; implied by Lemma 1 of \cite{dasgupta2007general}]
\label{lem:vclemma}
Let $X_1, \ldots, X_n$ be i.i.d. random variables taking their values in $\mathcal X$. Assume that the class $\mathcal{F}$ of $\{0, 1\}$-valued functions has the {\sc VC} dimension $d$. Then there is a numerical constant $C > 0$ such that for any $\delta \in (0, 1)$, with probability at least $1 - \delta$, for all $f \in \mathcal{F}$,
\begin{equation}
\label{eq:firstuniformbound}
\left|\sum_{i = 1}^n(f(X_i) - \Ex[ f(X_i)])\right| \le C \cdot \left(\sqrt{\left(\sum_{i = 1}^n\Ex[f(X_i)]\right)\left(d\log(n) + \log\left(\frac{2}{\delta}\right)\right)}+ d\log(n) + \log\left(\frac{2}{\delta}\right)\right)
\end{equation}
\end{lemma}
    Let $\rho(l,r)$ denote the random variable corresponding to the number of samples within $[l,r]$ from $n$ samples. We show how for all indicators of intervals, $|\sqrt{\rho(l,r)}-\sqrt{\Ex[\rho(l,r)]}|$ is small:
    \begin{claim}\label{claim:test-conc}
        With probability $1-\delta$, for all intervals $[l,r]$ it holds that:
        \begin{equation*}
            \left| \sqrt{\rho(l,r)} - \sqrt{\Ex[\rho(l,r)]} \right| \le O(1) \cdot \sqrt{\log(2n/\delta)}
        \end{equation*}
    \end{claim}
    \begin{proof}
        \begin{align*}
            & \left| \sqrt{\rho(l,r)} - \sqrt{\Ex[\rho(l,r)]} \right| \intertext{We will bound this in two ways. Consider $|\sqrt{x}-\sqrt{y}|$ for non-negative $x,y$. Roughly, if $x\approx y$, then the quantity of interest is almost bounded by $\frac{|x-y|}{\sqrt{y}}$. More concretely, if (i) $x \ge y$ then $|\sqrt{x}-\sqrt{y}| = \int_{0}^{x-y} \frac{1}{\sqrt{t + y}} \diff t \le \frac{x-y}{\sqrt{y}}$. Otherwise, if (ii) $\tfrac{y}{2} \le x < y$, then $|\sqrt{x}-\sqrt{y}| = \int_{0}^{y-x} \frac{1}{\sqrt{y - t}} \diff t \le \frac{y-x}{\sqrt{y/2}}$. In our remaining case, (iii) $x < \tfrac{y}{2}$, then $|\sqrt{x}-\sqrt{y}| \le \sqrt{y} \le \frac{2 \cdot (y-x)}{\sqrt{y}}$. In all cases, $|\sqrt{x}-\sqrt{y}| \le \frac{2 |y-x|}{\sqrt{y}}$ resulting in the first argument of the next step. Additionally, by concavity, $|\sqrt{x}-\sqrt{y}| \le \sqrt{|x-y|}$ which may be much better when $y$ is small, giving us the second argument of the next step:}
            & \le \min\left(\frac{2 \cdot |\rho(l,r) - \Ex[\rho(l,r)]|}{\sqrt{\Ex[\rho(l,r)]}}, \sqrt{|\rho(l,r)-\Ex[\rho(l,r)]|}\right)\intertext{We use that the uniform convergence guarantee \cref{eq:firstuniformbound} of \cref{lem:vclemma} holds with probability $1-\delta$, noting the VC dimension of interval indicators is $d=2$. Then, for all $[l,r]$,  $|\Ex[\rho(l,r)] - \rho(l,r)| \le O(1) \cdot \left(\sqrt{\Ex[\rho(l,r)] \cdot \log(2n/\delta)}+ \log(2n/\delta) \right)$, so:}
            & \le \min\Bigg(\frac{O(1) \cdot \left(\sqrt{\Ex[\rho(l,r)] \cdot \log(2n/\delta)}+ \log(2n/\delta) \right)}{\sqrt{\Ex[\rho(l,r)]}}, \\
            & \sqrt{O(1) \cdot \left(\sqrt{\Ex[\rho(l,r)] \cdot \log(2n/\delta)}+ \log(2n/\delta) \right)}\Bigg)\intertext{Consider using the first argument of the minimum when $\Ex[\rho(l,r)] \ge \log(2n/\delta)$ and the second argument when $\Ex[\rho(l,r)] < \log(2n/\delta)$, then we conclude:}
            & \le O(1) \cdot \sqrt{\log(2n/\delta)} \tag*{\qedhere} 
        \end{align*}
        \end{proof}
        This type of uniform convergence guarantee will be sufficient to show that all tests which need to pass will pass, and every poor $\muh$ will have a test that fails. First, we show that with the correct $\mu$ all tests will pass:
        \begin{claim}\label{claim:good-test-ok}
            Under the test convergence event of \cref{claim:test-conc}, there exists some constant $\cgam \ge 1$ where \cref{algo:ident} will pass all tests centered at $\mu$ if $\gamma \ge \cgam \cdot \sqrt{\log(2n/\delta)}$.
        \end{claim}
        \begin{proof}
            For any test centered at $\mu$, our claim follows by:
            \begin{align*}
                & \left| \sqrt{\rho(\mu-b,\mu-a)} - \sqrt{\rho(\mu+a,\mu+b)} \right| \\
                & \le \bigg| \left|\sqrt{\rho(\mu-b,\mu-a)} - \sqrt{\Ex[\rho(\mu-b,\mu-a)]}\right| + \left|\sqrt{\rho(\mu+a,\mu+b)} - \sqrt{\Ex[\rho(\mu+a,\mu+b)]}\right|\\
                & + |\sqrt{\Ex[\rho(\mu-b,\mu-a)]} - \sqrt{\Ex[\rho(\mu+a,\mu+b)]}|\bigg|\\
                & = \left|\sqrt{\rho(\mu-b,\mu-a)} - \sqrt{\Ex[\rho(\mu-b,\mu-a)]}\right| + \left|\sqrt{\rho(\mu+a,\mu+b)} - \sqrt{\Ex[\rho(\mu+a,\mu+b)]}\right| \intertext{Using \cref{claim:test-conc}:}
                & \le O(1) \cdot \sqrt{\log(2n/\delta)}
            \end{align*}
            
        \end{proof}
        Let us set $\gamma = \cgam \cdot \sqrt{\log(2n/\delta)}$ for the value of $\cgam$ yielded by \cref{claim:good-test-ok}. Then, for any poor $\muh$ there will be a test that fails:
        \begin{claim}\label{claim:test-happens}
            Under the test convergence event of \cref{claim:test-conc}, there exists some universal constant $\cdist \ge 1$ (as a function of $\cgam$), where \cref{algo:ident} will fail some test centered at $\muh$, for every:
            \begin{equation*}
                |\mu - \muh| > \Delta^*/2
            \end{equation*}
        \end{claim}
        \begin{proof}
        In the proof of this claim, we will mostly leverage our lower bound on $\dhsq(p_\mu,p_{\muh})$ from the conditions of this theorem, and the existence of a test that preserves this Hellinger distance via \cref{cor:test-exists}. To start, for any $0 \le a < b$ it holds:
        \begin{align*}
            & \left| \sqrt{\rho(\muh-b,\muh-a)} - \sqrt{\rho(\muh+a,\muh+b)} \right|\\
            & \ge \bigg| \left|\sqrt{\Ex[\rho(\muh-b,\muh-a)]} - \sqrt{\Ex[\rho(\muh+a,\muh+b)]}\right| - \left|\sqrt{\rho(\muh-b,\muh-a)} - \sqrt{\Ex[\rho(\muh-b,\muh-a)]}\right| \\
            & - \left|\sqrt{\rho(\muh+a,\muh+b)} - \sqrt{\Ex[\rho(\muh+a,\muh+b)]}\right|\bigg|\intertext{Using \cref{claim:test-conc}:}
            & \ge \left|\sqrt{\Ex[\rho(\muh-b,\muh-a)]} - \sqrt{\Ex[\rho(\muh+a,\muh+b)]}\right| - O(1) \cdot \sqrt{\log(2n/\delta)}\intertext{Let $\Delta \triangleq |\mu - \muh|$. Then, if we set $a$ and $b$ to the corresponding values from \cref{cor:test-exists}:}
            & \ge \Omega(1) \cdot \sqrt{\frac{n \cdot \dhsq(p_{\mu},p_{\mu-2\Delta})}{k \log(4k/\dhsq(p_{\mu},p_{\mu-2\Delta})) \cdot \log(4/\dhsq(p_{\mu},p_{\mu-2\Delta}))}} - O(1) \cdot \sqrt{\log(2n/\delta)} \\
            & \ge \Omega(1) \cdot \sqrt{\frac{n \cdot \dhsq(p_{\mu},p_{\mu-2\Delta})}{k \log^2(4k/\dhsq(p_{\mu},p_{\mu-2\Delta}))}} - O(1) \cdot \sqrt{\log(2n/\delta)} \intertext{Since this is non-decreasing in $\dhsq(p_{\mu},p_{\mu-2\Delta})$, we use our lower bound on $\dhsq(p_{\mu},p_{\mu-2\Delta})$ from $2\Delta \ge \Delta^*$ and the assumed lower bound from this theorem for $\dhsq(p_\mu,p_{\mu-\Delta})$ when $|\Delta|\ge \Delta^*$. Note that the value of this assumption was chosen so that the first term of the previous step will be sufficiently larger than the latter term. Hence:}
            & \ge \Omega(1) \cdot \sqrt{\frac{n \cdot \left( \cdist \cdot \frac{k}{n} \cdot \log(2n/\delta) \cdot \log^2(2n) \right)}{k \log^2\left(\tfrac{4k}{\cdist \cdot \frac{k}{n} \cdot \log(2n/\delta) \cdot \log^2(2n)}\right)}} - O(1) \cdot \sqrt{\log(2n/\delta)}\\
            & = \Omega(1) \cdot \sqrt{\frac{ \cdist \cdot \log(2n/\delta) \cdot \log^2(2n)}{\log^2\left(\tfrac{4n}{\cdist  \cdot \log(2n/\delta) \cdot \log^2(2n)}\right)}} - O(1) \cdot \sqrt{\log(2n/\delta)}\\
            & \ge \Omega(1) \cdot \sqrt{\frac{ \cdist \cdot \log(2n/\delta) \cdot \log^2(2n)}{O(1) \cdot \max\left( \log(1/\cdist), \log(2n)\right)^2}} - O(1) \cdot \sqrt{\log(2n/\delta)} \intertext{If we choose a $\cdist \ge 1$, then:}
            & \ge \Omega(1) \cdot \sqrt{\frac{ \cdist \cdot \log(2n/\delta)}{O(1)}} - O(1) \cdot \sqrt{\log(2n/\delta)}\\
            & \ge \left(\Omega(1) \cdot \sqrt{\cdist} - O(1)\right) \cdot \sqrt{\log(2n/\delta)}\intertext{If we choose $\cdist$ to be sufficiently large in terms of $\cgam$, we obtain the desired:}
            & > \cgam \cdot \sqrt{\log(2n/\delta)} = \gamma
        \end{align*}
            Meaning, the corresponding test centered at $\muh$ will fail.
        \end{proof}
        Thus, we conclude that \cref{algo:ident} will output a $\muh$ that satisfies our desired guarantees.
    \end{proof}
    

    Unfortunately, this algorithm is both (i) inefficient, and (ii) needs to know a confidence parameter $\delta$ to compute $\gamma$, which may be undesirable. We note that (i) can be partially remedied as \cref{algo:ident} can be simulated naively in $O(n^4)$ time by observing that tests are only determined by the set of samples inside the two intervals $[\muh-b,\muh-a]$ and $[\muh+a,\muh+b]$, so we may naively iterate over all sets in $O(n^4)$ time. We do not discuss this in-depth because we soon introduce a more nuanced algorithm that runs in near-linear time. For the parameter dependence raised in (ii), we note that this could be resolved by choosing the $\muh$ that passes all tests with the smallest value of $\gamma$. We state this corollary next for completeness. Our near-linear time algorithm will also leverage a similar idea to avoid any parameter dependence.

\begin{corollary}\label{cor:paramfree-ident-log}
    Consider a modified version of \cref{algo:ident} with $\gamma \ge 0$ set to be the smallest value such that at least one $\muh$ passes all tests. We now attain a similar guarantee to \cref{thm:ident-log} without needing to choose $\gamma$. Suppose $p$ is a distribution that is a centered/symmetric mixture of $k$ log-concave distributions. There exists some universal constant $\cdist \ge 1$, where if
    \begin{equation*}
        \Delta^* \triangleq \omega_p \left( \cdist \cdot \frac{k}{n} \cdot \log(2n/\delta) \cdot \log^2(2n)\right)
    \end{equation*}

    then with probability $1-\delta$ the output $\muh$ of the modified \cref{algo:ident} will satisfy $|\mu - \muh| \le \Delta^*/2$.  
\end{corollary}
\begin{proof}
    Note by \cref{thm:ident-log} if $\gamma = \cgam \log(2/\delta)$ then at least one $\muh$ will pass all tests, and all $\muh$ that pass the test satisfy the desired condition on $|\mu-\muh|$. Since at least one $\muh$ will pass all tests, then the modified algorithm will choose a value of $\gamma$ where $\gamma \le \cgam \log(2/\delta)$. Moreover, the set of $\muh$ that pass the tests with this $\gamma$ will be a subset of the $\muh$ that pass with the larger value, so they will also satisfy the condition on $|\mu - \muh|$.
\end{proof}

\subsubsection{Designing a Near-Linear Time Algorithm}\label{sec:fast}
Our analysis of the inefficient \cref{algo:ident} only leveraged the existence of significant tests for poor $\muh$, such as those shown in \cref{cor:test-exists}. For a faster algorithm, we will show the existence of tests with structure that makes the tests easier to find. First, we define one such structure for a test:

\begin{definition}[$\ell$-heavy test]
    An $\ell$-heavy test is a test where of the two intervals being compared, the interval with more samples contains exactly $\ell$ samples. Moreover, the endpoints of the larger interval are exactly the first and last of these $\ell$ samples (inclusive).
\end{definition}

We will show that it is sufficient to consider only $\ell$-heavy tests where $\ell$ is a power of $2$.
Second, we hope to efficiently find all $\ell$-heavy tests for a fixed $\gamma$ and $\ell$. We will observe that if a distribution is symmetric/unimodal and a possible estimate $\muh$ fails some test because one interval has significantly more samples than another, then we may conclude that $\mu$ is strictly on the side of the larger interval. Hence, it is sufficient to find the leftmost $\muh$ that fails an $\ell$-heavy test because the interval on its left is too populated, and similarly the rightmost $\muh$ that fails an $\ell$-heavy test because the interval on its right is too populated. We are able to compute this for a fixed $\ell$ and $\gamma$ in $O(n)$ time with a sweep-line algorithm. Third, we show that it is sufficient to consider only $O(\log(n))$ values of $\gamma$, and binary search in $O(\log(\log(n)))$ iterations for the smallest such $\gamma$ having a $\muh$ that doesn't fail any discovered test. In total, we will obtain an $O(n \log(n) \log(\log(n)))$ time algorithm by considering only $O(\log(n))$ values of $\ell$, employing an $O(n)$ time sweep-line subroutine, and doing $O(\log(\log((n))))$ iterations of binary search over $\gamma$. We present the sweep-line subroutine in \cref{algo:sweep}, and the entire estimation procedure in \cref{algo:fast}.



We now prove our guarantees for \cref{algo:fast}, which are of the same flavor as \cref{thm:ident-log,cor:paramfree-ident-log} but running in $O(n \log(n) \log(\log(n)))$ time:





\fastthm*
\begin{proof}
    Most of our proof will be able to reuse claims from the proof of \cref{thm:ident-log}. Let us focus on the uniform convergence event of \cref{claim:test-conc} that holds with $1-\delta$ probability. Using a claim similar to \cref{claim:good-test-ok}, we will show that no test will incorrectly fail for large enough $\gamma$. For example, if the left interval has significantly more samples than the right interval, then $\mu < \muh$.
    \begin{claim}\label{claim:mod-no-wrong-fail}
        Under the test convergence event of \cref{claim:test-conc}, there exists some constant $\cgam \ge 1$ where all failing tests will have correct conclusions if $\gamma \ge \cgam \cdot \sqrt{\log(2n/\delta)}$.
    \end{claim}
    \begin{proof}
        We can analyze how different the empirical test value is from the quantity with the expectations:
        \begin{align*}
            & \left| \left( \sqrt{\Ex[\cnt(\muh - b, \muh - a)]} - \sqrt{\Ex[\cnt(\muh+a,\muh+b)]} \right) - \left( \sqrt{\cnt(\muh - b, \muh - a)} - \sqrt{\cnt(\muh+a,\muh+b)} \right) \right|\\
            & = \left| \left( \sqrt{\Ex[\cnt(\muh - b, \muh - a)]} - \sqrt{\cnt(\muh - b, \muh - a)}  \right) + \left( \sqrt{\cnt(\muh+a,\muh+b)} - \sqrt{\Ex[\cnt(\muh+a,\muh+b)]} \right) \right|\\
            & \le \left| \sqrt{\Ex[\cnt(\muh - b, \muh - a)]} - \sqrt{\cnt(\muh - b, \muh - a)}  \right| + \left| \sqrt{\cnt(\muh+a,\muh+b)} - \sqrt{\Ex[\cnt(\muh+a,\muh+b)]} \right| \intertext{By \cref{claim:test-conc}:}
            & \le O(1) \cdot \sqrt{\log(2n/\delta)}
        \end{align*}
        Hence, for sufficiently large $\cgam$, if $\left( \sqrt{\cnt(\muh - b, \muh - a)} - \sqrt{\cnt(\muh+a,\muh+b)} \right) > \cgam \cdot \sqrt{\log(2n/\delta)}$, then we may conclude $\Ex[\cnt(\muh - b, \muh - a)] > \Ex[\cnt(\muh+a,\muh+b)]$, meaning $\mu < \muh$ since our distribution is symmetric and unimodal. The same can be said for if $\left( \sqrt{\cnt(\muh+a,\muh+b)} - \sqrt{\cnt(\muh - b, \muh - a)}\right) > \cgam \cdot \sqrt{\log(2n/\delta)}$, then we may conclude $\Ex[\cnt(\muh+a,\muh+b)] > \Ex[\cnt(\muh - b, \muh - a)]$, meaning $\mu > \muh$ since our distribution is symmetric and unimodal.
    \end{proof}
    This has shown that none of our test's conclusions will be incorrect with sufficiently large $\cgam$. Our next goal is to show that \cref{algo:fast} will consider a value of $\gamma$ that is close to considering the desired $\cgam \cdot \sqrt{\log(2n/\delta)}$:

    \begin{claim}\label{claim:good-gamma}
        For any value $\gamma_0>0$, \cref{algo:fast} has a value $\gamma \in \operatorname{List_\gamma}$ whose tests all evaluate the same as they would for some $\gamma' \in [\gamma_0,2\gamma_0)$.
    \end{claim}
    \begin{proof}
        Let $\gsmall \triangleq \tfrac{1}{\sqrt{n}}$ and $\glarge \triangleq \sqrt{n+1}$. Recall that $\operatorname{List_\gamma}$ contains the values: $\gsmall$, $\glarge$, and $2^i \cdot \gsmall$ for all integer values of $i\ge 1$ where $2^i \cdot \gsmall < \glarge$. For any $v \in [\gsmall,\glarge]$ the claim immediately holds. For $\gamma_0 \le \gsmall$, a test will fail if and only if the intervals have an unequal number of samples, so our claim will hold because $\operatorname{List_\gamma}$ contains $ \gsmall$. Finally, for $\gamma_0 \ge \glarge$, no test will fail because $|\sqrt{\cnt(\muh-b,\muh-a)}-\sqrt{\cnt(\muh+a,\muh+b)}| \le \sqrt{n} < \glarge$, so our claim will hold because $\operatorname{List_\gamma}$ contains $\glarge$.
    \end{proof}

    Thus, using \cref{claim:good-gamma}, let us consider the value $\gamma^* \in \operatorname{List_\gamma}$ that evaluates tests identically to $\cround \cdot \cgam \cdot \sqrt{\log(2n/\delta)}$, for $\cround \in [1,2)$. Note that all conclusions with $\gamma^*$ will be correct by \cref{claim:mod-no-wrong-fail}. We now show that there will be a failing $2^i$-heavy test for some value of $i$, for every $\muh$ with sufficiently large $|\mu - \muh|$:

    \begin{lemma}\label{claim:structure-test-happens}
        There exists some universal constant $\cdist \ge 1$ (as a function of $\cgam$), where under the test convergence event of \cref{claim:test-conc}, then some $2^i$-heavy test centered at $\muh$ will fail using $\gamma^*$, for every:
        \begin{equation*}
            |\mu - \muh| > \Delta^*/2
        \end{equation*}
    \end{lemma}
    \begin{proof}
        Looking into the previous proof of \cref{thm:ident-log}, by \cref{cor:test-exists} we knew there was a test centered at $\muh$ with $0 \le a < b$ that preserved Hellinger distance, and by \cref{claim:test-happens} we concluded that the test empirically fails under the uniform convergence event (the proof also implicitly shows that when the test fails, the interval with larger expectation will correctly have more samples empirically, so our analogous conclusion is valid). This proof still holds under our current theorem assumptions and using $\gamma^*$ (we are just not finished because the test is not necessarily a $2^i$-heavy test). 
        
        Let us consider the same test defined by $a,b$. Without loss of generality, consider $\muh < \mu$, so the right interval $[\muh + a, \muh + b]$ has more samples in expectation than the left interval $[\muh - b, \muh -a]$. Since the test empirically fails under the uniform convergence event, then certainly the right interval will have at least one sample. We note that any interval with a positive number of samples can be decomposed into two (possibly overlapping) intervals that each contain $2^i$ samples:

        \begin{claim}\label{claim:decomp}
            Consider an interval $[l,r]$ with $N \ge 1$ distinct samples inside the interval. There exist values $m_0 \le m_1$ where $[l,m_1]$ and $[m_0,r]$ both contain exactly $2^{\lfloor \log(N) \rfloor}$ samples.
        \end{claim}
        \begin{proof}
            This follows immediately from considering $[l,m_1]$ to be the longest interval containing exactly $2^{\lfloor \log(N) \rfloor}$ samples and starting at $l$, and considering $[m_0,r]$ to be the longest interval containing exactly $2^{\lfloor \log(N) \rfloor}$ samples and ending at $r$.
        \end{proof}

        We use the decomposition of \cref{claim:decomp} to consider two tests,\footnote{As an aside, we acknowledge the edge case where multiple samples have exactly the same value, so we cannot split into two tests via \cref{claim:decomp}. Observe that this occurs with probability $0$ unless $p$ contains an atom, which may only occur at its mode $\mu$. Since we have chosen $a$ such that $\muh + a \ge \mu$, this may only occur when our $a = \mu - \muh$. If less than half of the samples in $[\muh + a,\muh + b]$ occur at $\muh+a$, then \cref{claim:decomp} will successfully decompose into two intervals with $2^i$ samples. Otherwise, the following arguments will succeed with test $[\muh+a,\muh+a]$ that has at least $2^i$ samples compared to $[\muh-a,\muh-a]$ that has at most $1$ sample.}
        $[a,m_1]$ and $[m_0,b]$, where both contain $2^i$ samples and we are hoping one test will nearly be a good $2^i$-heavy test. Moving forward, we will show that the decomposition does yield a good test:


        \begin{claim}\label{claim:hel-split}
            Consider a subset $S$ of the domain, and the subsets $S_0,S_1 \subseteq S$ where $S_0 \cup S_1 = S$ and $\frac{p(x)}{q(x)} \ge 1$ for all $x \in S$. Then:
            \begin{equation*}
                \max_{i \in \{0,1\}} \left( \sqrt{\Pr_{x \sim p}[x \in S_i]} - \sqrt{\Pr_{x \sim q}[x \in S_i]} \right)^2 \ge \frac{1}{4} \cdot \left( \sqrt{\Pr_{x \sim p}[x \in S]} - \sqrt{\Pr_{x \sim q}[x \in S]}  \right)^2
            \end{equation*}
        \end{claim}
        \begin{proof}
            \begin{align*}
                & \max_{i \in \{0,1\}} \left( \sqrt{\Pr_{x \sim p}[x \in S_i]} - \sqrt{\Pr_{x \sim q}[x \in S_i]} \right)^2 \intertext{Let $i^* \in \{0,1\}$ be a value such that $|\Pr_{x \sim p}[x \in S_{i^*}] - \Pr_{x \sim q}[x \in S_{i^*}]| \ge \frac{1}{2} \cdot |\Pr_{x \sim p}[x \in S] - \Pr_{x \sim q}[x \in S]|$. Such an $i^*$ must exist by $p(x)/q(x)\ge 1$ for all $x \in S$:}
                & \ge \left( \sqrt{\Pr_{x \sim p}[x \in S_{i^*}]} - \sqrt{\Pr_{x \sim q}[x \in S_{i^*}]} \right)^2\\
                & \ge \left( \sqrt{\Pr_{x \sim q}[x \in S_{i^*}] + \frac{1}{2} \cdot |\Pr_{x\sim p}[x \in S]-\Pr_{x\sim q}[x \in S]|} - \sqrt{\Pr_{x \sim q}[x \in S_{i^*}]} \right)^2\\
                & \ge \left( \sqrt{\Pr_{x \sim q}[x \in S] + \frac{1}{2} \cdot |\Pr_{x\sim p}[x \in S]-\Pr_{x\sim q}[x \in S]|} - \sqrt{\Pr_{x \sim q}[x \in S]} \right)^2\\
                & \ge \left( \frac{1}{2} \cdot \left(\sqrt{\Pr_{x \sim q}[x \in S] +  |\Pr_{x\sim p}[x \in S]-\Pr_{x\sim q}[x \in S]|} - \sqrt{\Pr_{x \sim q}[x \in S]} \right) \right)^2\\
                & = \frac{1}{4} \cdot  \left(\sqrt{\Pr_{x \sim p}[x \in S]} - \sqrt{\Pr_{x \sim q}[x \in S]} \right)^2 \tag*{\qedhere} 
            \end{align*}
        \end{proof}

        Applying \cref{claim:hel-split} directly to \cref{cor:test-exists}, we get that one of $[a,m_1]$ and $[m_0,b]$ satisfy the guarantees of $a,b$ from \cref{cor:test-exists} up to a factor of $\frac{1}{4}$, and moreover this contains exactly $2^i$ samples. Using precisely the same proof as \cref{claim:test-happens} will yield our desired guarantee (note how the bound in terms of $\cgam$ in the original proof can be replaced by $\cround \cdot \cgam \le 2 \cgam$, which only changes constant factors). All that remains is that the test is not quite $2^i$-heavy, because although it contains exactly $2^i$ samples, its endpoints are not necessarily samples. This is easily remedied by contracting the interval to still contain $2^i$ samples, but have its starting endpoint be the leftmost sample inside and the ending endpoint be the rightmost sample inside. The test will still fail, because the heavier interval will not lose samples, and the lighter interval will not gain samples.
    \end{proof}

    This gives us a clear roadmap for finishing our proof. When we use $\gamma^*$, we know that all conclusions will be valid, and all sufficiently bad $\muh$ will be ruled out by failed $2^i$-heavy tests. When considering $\gamma > \gamma^*$, only a subset of the tests will fail, so certainly the binary search will end with a $\gamma \le \gamma^*$. Moreover, the values of $\muh$ that pass with $\gamma$ will only be a subset of the values that pass with $\gamma^*$, so we immediately have the desired bound on $|\muh - \mu|$.

    All the remains is to show that \cref{algo:sweep} correctly recovers the set of $\muh$ that pass $\ell$-heavy tests for a fixed $\ell$ and $\gamma$. Recall that it is sufficient to search for the rightmost $\muh$ that fails such a test because the right interval has much more samples, and the leftmost $\muh$ that fails such a test because the left interval has much more samples. Without loss of generality, we focus on the former:

    \begin{lemma}
        $\operatorname{BiggestLowerBound}([X_1,\dots,X_n],\gamma,\ell)$ computes the rightmost $\muh$ where an $\ell$-heavy test (with the heavier side being on the right) centered at $\muh$ fails with parameter $\gamma$.
    \end{lemma}
    \begin{proof}
        Recall that such an $\ell$-heavy test will have the right interval containing exactly $\ell$ samples, and its endpoints will be samples. So, the right interval will be $[X_i,X_{i+\ell-1}]$ for some $i \in \{1,\dots, n-\ell+1\}$. 
        
        Now, consider some left interval for the test. Recall that a test will fail if $\sqrt{R}-\sqrt{L}>\gamma$, where $R$ is the number of samples in the right interval and $L$ is the number of samples in the left interval. Since $R=\ell$, we conclude that a test will fail if and only if $L \le \lceil(\sqrt{\ell}-\gamma)^2 \rceil - 1$, denoted by $\operatorname{LeftCountCap}$ in \cref{line:leftcap}. There is also some structure for the best left interval: if the left interval could be moved to the right without including an additional sample, this would strictly improve $\muh$. So, the left interval must be $[X_j-(X_{i+\ell-1} - X_i),X_j)$ for some $j \le i$. Equivalently, for $l \le r$, there exists an $\ell$-heavy test with the right interval starting at $X_r$ (inclusive) and the left interval ending at $X_l$ (non-inclusive) if and only if the longest interval ending at $X_l$ (non-inclusive) containing at most $\operatorname{LeftCountCap}$ samples is at least as long as $X_{r+\ell-1} - X_r$. This will be the property our sweep-line crucially relies on. We informally refer to such a valid pairing as matching an $X_l$-left interval with an $X_r$-right interval. 

        We note two simple properties of the best matching:

        \begin{claim}\label{claim:right-prop}
            A $X_r$-right interval will not be in the best matching if there is an $r' > r$ where $X_{r' + \ell -1} - X_{r'} \le X_{r + \ell - 1} - X_r$.
        \end{claim}
        \begin{proof}
            Any valid matching including the $X_r$-right interval would also be valid with the $X_{r'}$-right interval, and the latter would have a larger $\muh$.
        \end{proof}
        
        The array $\operatorname{NonDominatedRightOption}$ tracks whether each $X_r$ has such a dominating $X_{r'}$, and $\operatorname{NonDominatedRightOption}[r]$ is true only if there is no such $r'$. 
        
        \begin{claim}\label{claim:left-prop}
            For a fixed $X_r$-right interval, the best matching will never include an $X_l$-left interval if there exists an $l < l' \le r$ where the $X_{l'}$-left interval is not shorter than the $X_l$-left interval.
        \end{claim}
        \begin{proof}
            Any valid matching with the $X_l$-left interval and the $X_r$-right interval would also be valid with the $X_{l'}$-left interval and the $X_r$ right interval, and the latter would have a larger $\muh$.
        \end{proof}

        We are now ready to explain the remaining aspects of the algorithm. Starting at \cref{line:rightloop}, we iterate over possible $X_i$-right intervals in increasing order of $i$. Before trying to match the $X_i$-right interval, we adjust our options for left intervals to match with. In $\operatorname{LeftStack}$, we are maintaining a stack of left intervals that are not dominated with respect to the property of \cref{claim:left-prop} (left intervals higher in the stack will correspond to $X_l$-left intervals with larger $l$ and shorter lengths). In \cref{line:removeleftprop}, we remove left intervals from $\operatorname{LeftStack}$ to maintain this property of the stack. By \cref{claim:right-prop}, it is permitted to only consider actually matching the $X_i$-right interval if $\operatorname{NonDominatedRightOption[i]}$ is true. In \cref{line:removeleftbig}, we note that if the top of $\operatorname{LeftStack}$ is too short to be matched with the $X_i$-right interval, then it will also be too short to be matched with any remaining non-dominated right intervals, so we may remove it from $\operatorname{LeftStack}$. Finally, in \cref{line:match}, we consider matching the $X_i$-right interval with the top left interval in $\operatorname{LeftStack}$ (if there is one). This left interval from the top of the stack is long enough to match with the $X_i$-right interval, and it is the rightmost such $X_l$-left interval that is sufficiently long. 

        By choosing the best $\mu_{\textrm{lower-bound}}$ of all matchings considered in \cref{line:match}, we find the largest $\muh$ failing a test of the desired structure.
    \end{proof}

    Thus, \cref{algo:fast} attains our desired guarantee.
\end{proof}


\begin{algorithm}[h]
    \caption{Fast Mean Estimation Algorithm} \label{algo:fast}
    \hspace*{\algorithmicindent} 
    \begin{flushleft}
      {\bf Input:} samples $X_1,\dots,X_n$ \\
      {\bf Output:} estimate $\muh$\\
      {\bf Description:} This $O(n \log(n) \log(\log(n)))$ time algorithm will output an estimate $\muh$ that passes tests based on a search over parameters $\gamma$ and $\ell$.
    \end{flushleft}
    \begin{algorithmic}[1]

    \Procedure{FixedGammaCheck}{$X_1,\dots,X_n$, $\gamma$} \Comment{Takes $O(n \log(n))$ time.}
        \State $\mu_{\textrm{lower-bound}} \gets -\infty$
        \State $\mu_{\textrm{upper-bound}} \gets \infty$
        \For{$\ell \in \{1, 2, \dots, 2^i, \dots, 2^{\lfloor\log(n)\rfloor}\}$} \Comment{Consider $O(\log(n))$ values of $\ell$.}
            \State $\mu_{\textrm{lower-bound}} \gets \max\left(\mu_{\textrm{lower-bound}},\operatorname{BiggestLowerBound}(X,\gamma,\ell) \right)$ \Comment{Takes $O(n)$ time.}
            \State $\mu_{\textrm{upper-bound}} \gets \min\left(\mu_{\textrm{upper-bound}},\operatorname{SmallestUpperBound}(X,\gamma,\ell) \right)$ \Comment{We did not explicitly define this function, but it is the same as $\operatorname{BiggestLowerBound}$ after reversing.}
        \EndFor
        \If{$\mu_{\textrm{lower-bound}} \le \mu_{\textrm{upper-bound}}$}
            \Return any $\muh$ inside $(\mu_{\textrm{lower-bound}},\mu_{\textrm{upper-bound}})$
        \Else 
            
            \Return FAIL \Comment{Using this $\gamma$, there was no $\muh$ that passed all tests.}
        \EndIf
    \EndProcedure
    
    \Procedure{Estimate}{$X_1,\dots,X_n$}
        \State $X_1,\dots,X_n \gets \operatorname{Sort}(X_1,\dots,X_n)$ \Comment{Sort in non-decreasing order in $O(n \log(n))$ time.}
        \State $\gsmall \gets \tfrac{1}{\sqrt{n}}$
        \State $\glarge \gets \sqrt{n+1}$
        \State $\operatorname{List_\gamma} \gets [\gsmall, 2 \cdot \gsmall, \dots, 2^i \cdot \gsmall, \dots, \glarge]$ 
        \Comment{List starting with $\gsmall$, and then containing $2^i \cdot \gsmall$ for $i\ge 1$ as long as $2^i \cdot \gsmall < \glarge$.}

        \State Binary search for the smallest $\gamma^* \in \operatorname{List_\gamma}$ where $\operatorname{FixedGammaCheck}(X,\gamma^*)$ returns a $\muh$ instead of failing. \Comment{$\operatorname{List_\gamma}$ contains $O(\log(n))$ values, so the binary search will try $O(\log(\log(n)))$ values of $\gamma$, with each check taking $O(n \log(n))$ time.}

        \Return $\operatorname{FixedGammaCheck}(X,\gamma^*)$
    \EndProcedure
    \end{algorithmic}
\end{algorithm}

\begin{algorithm}[!hp]
    \caption{Lower Bound Sweep-Line Algorithm} \label{algo:sweep}
    \hspace*{\algorithmicindent} 
    \begin{flushleft}
      {\bf Input:} sorted samples $X_1 \le \dots \le X_n$, thresholing parameter $\gamma$, heaviness parameter $\ell$ \\
      {\bf Output:} lower bound on $\mu$\\
      {\bf Description:} This $O(n)$ time algorithm will output the largest lower bound concluded by testing with parameter $\gamma$ with a right interval that contains exactly $\ell$ samples.
    \end{flushleft}
    \begin{algorithmic}[1]
    

    \Procedure{BiggestLowerBound}{$[X_1,\dots,X_n]$, $\gamma$, $\ell$}
        \State $\mu_{\textrm{lower-bound}} \gets -\infty$
        \If{$\sqrt{l} \le \gamma$}
            \Return $-\infty$ \Comment{No test could possibly fail.}
        \EndIf
        \State $\operatorname{LeftCountCap} \gets \lceil(\sqrt{\ell}-\gamma)^2 \rceil - 1$ \label{line:leftcap} \Comment{A test will fail if $\sqrt{R}-\sqrt{L} > \gamma$. Since $R=\ell$, we solve for the largest integer where if $L$ is at most this integer, then the test will fail.}
        \State $\operatorname{NonDominatedRightOption} \gets [\textrm{False}, \dots, \textrm{False}]$ \Comment{For $i \in \{1,\dots,n-\ell+1\}$, $\operatorname{NonDominatedRightOption}[i]$ is true if there is no interval containing $\ell$ samples that starts further to the right and is not longer.}
        \State $\operatorname{RightLength} \gets []$ \Comment{$\operatorname{RightLength}[i]$ will be the length of the interval starting at $X_i$ (inclusive) that contains $\ell$ samples, it will only be defined for $i \in \{1,\dots, n-\ell+1\}$.}
        \State $\operatorname{ShortestConsidered} \gets +\infty$ \Comment{We will consider $i$ in decreasing order and note the shortest interval containing $\ell$ samples we have yet seen.}
        \For{$i \in \{n-\ell+1,\dots,1\}$}
            \State $\operatorname{RightLength}[i] \gets X_{i+\ell-1} - X_i$
            \If{$\operatorname{RightLength}[i] < \operatorname{ShortestConsidered}$}
                \State $\operatorname{ShortestConsidered} \gets \operatorname{RightLength}[i]$
                \State $\operatorname{NonDominatedRightOption}[i] \gets \textrm{True}$
            \EndIf
        \EndFor
        \State $\operatorname{LeftLength} \gets []$ \Comment{$\operatorname{LeftLength}[i]$ will be the length of the longest interval ending at $X_i$ (non-inclusive) that contains at most $\operatorname{LeftCountCap}$ samples.}
        \State $\operatorname{LeftStack} \gets []$ \Comment{A stack of potential left intervals to match with right intervals. Items higher in the stack will have larger $i$ and shorter length (because otherwise, if it had larger $i$ and not shorter length, we would always prefer this interval and could remove the other).}
        \For{$i \in \{1,\dots,n-\ell+1\}$} \label{line:rightloop}
            \If{$i \le \operatorname{LeftCountCap}+1$}
                \State $\operatorname{LeftLength}[i] \gets \infty$
            \Else
                \State $\operatorname{LeftLength}[i] \gets X_i - X_{i-\operatorname{LeftCountCap}-1}$
            \EndIf
            \While{$\operatorname{LeftLength}[\operatorname{LeftStack.top()}] \le \operatorname{LeftLength}[i]$} \label{line:removeleftprop}
                \State $\operatorname{LeftStack.pop()}$
            \EndWhile
            \State $\operatorname{LeftStack.push(i)}$
            \If{$\operatorname{NonDominatedRightOption}[i]$}
                \While{$\operatorname{LeftLength}[\operatorname{LeftStack.top()}] \le \operatorname{RightLength}[i]$}\label{line:removeleftbig}
                    \State $\operatorname{LeftStack.pop()}$ \Comment{We cannot match this left interval with the right interval starting at $i$, nor any later $j>i$, so we may remove it.}
                \EndWhile
                \If{$\operatorname{LeftStack}$ is nonempty}\label{line:match}
                    \State $\mu_{\textrm{lower-bound}} \gets \max\left(\mu_{\textrm{lower-bound}}, \left(X_{\operatorname{LeftStack.top()}} + X_i\right)/2 \right)$
                \EndIf
            \EndIf
        \EndFor
        \Return $\mu_{\textrm{lower-bound}}$
    \EndProcedure
    \end{algorithmic}
\end{algorithm}


    




