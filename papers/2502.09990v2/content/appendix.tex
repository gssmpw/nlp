\onecolumn

\setcounter{equation}{0}
\setcounter{definition}{0}
\setcounter{proposition}{0}

\section{Additional Results}
\subsection{The compliance rates in HarmBench}
\label{app:single_turn_asr}
We evaluate the basic safety level of the LLMs using HarmBench~\cite{harmbench}. 
%
Table \ref{tab:single_turn_asr} shows both existing defense methods and X-Boundary can effectively reduce the compliance rates to harmful requests in HarmBench without using attack methods.
\begin{table}[h]
% \captionsetup{position=above} % 让 caption 显示在表格上方
\setlength{\abovecaptionskip}{0.1in} % 让 caption 和表格更紧凑
\caption{The compliance rates to harmful requests in HarmBench after using existing defense methods and X-Boundary.}
% \vskip 0.1in
\label{tab:single_turn_asr}
% \setlength{\tabcolsep}{2pt}
\centering
\resizebox{0.6\columnwidth}{!}{
\begin{tabular}{c|cccccc}
%\Xhline{4\arrayrulewidth}
% \midrule
% \multirow{2.5}{*}{\textbf{Methods}} & \multicolumn{3}{c}{\textbf{Multi-Turn ASR (\%) $\downarrow$}} & \multicolumn{4}{c}{\textbf{Over-Refusal Rate (\%) $\downarrow$}} & \multicolumn{3}{c}{\textbf{General Capability (\%) $\uparrow$}} \\
\midrule
% Model & Single Round ASR (\%) & Multi Round ASR (\%) & 
Methods & \makecell{Llama-3-\\8B-Instruct} & \makecell{Qwen2.5-\\7B-Chat} & \makecell{Mistral-7B-\\Instruct-v0.2} & \makecell{Qwen2.5-\\14B-Chat} \\
\midrule
Vanilla & 11.67 & 26.25 & 56.67 & 15.83 \\
% \cmidrule(lr){2-12}
\midrule
SFT & 1.25 & 5.42 & 7.08 & 1.25 \\
DPO & 0.83 & 2.92 & 13.33 & 5.83 \\
GA & 5.00 & 3.75 & 9.17 & / \\
CB & 1.67 & 4.58 & 13.33 & 4.58 \\
% \cmidrule(lr){2-12}
\midrule
X-Boundary & 1.25 & 4.58 & 13.33 & 2.92 \\
% \Xhline{4\arrayrulewidth}
\midrule
\end{tabular}}
\vspace{-10pt}
\end{table}

\subsection{The effect of defense methods on the LLMs' reasoning ability}
\label{app:reasoning_ability}
Large reasoning models often rely on generating lengthy reasoning paths for inference. Therefore, we conducted a statistical analysis of the output length of large reasoning models employing various defense mechanisms. As shown in Table~\ref{tab:reasoning_model_len}, while X-Boundary does not lead to a degradation in general capability, it results in shorter output lengths, which may indirectly impact reasoning performance. Exploring strategies to prevent the reduction in output length represents a promising direction for future research.

\begin{table*}[t]
\setlength{\abovecaptionskip}{0.1in} % 让 caption 和表格更紧凑
\caption{Comparison of pass@1 accuracy and average output token length across different defense methods on reasoning model}
\label{tab:reasoning_model_len}
\setlength{\tabcolsep}{4pt}
\centering
\resizebox{0.8\textwidth}{!}{ % 使用 textwidth 适配整个表格
\begin{tabular}{cc|cccccc} % 列数要和表头匹配
\midrule
\multirow{2}{*}{\textbf{Models}} & \multirow{2}{*}{\textbf{Methods}} 
& \multicolumn{2}{c|}{\textbf{AIME2024}} 
& \multicolumn{2}{c|}{\textbf{GPQA}} 
& \multicolumn{2}{c}{\textbf{LiveCode}} \\
\cmidrule(lr){3-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8}
& & pass@1 & Length (Avg.) & pass@1 & Length (Avg.) & pass@1 & Length (Avg.) \\
\midrule
\multirow{5}{*}{\makecell{\textbf{DeepSeek-}\\\textbf{R1-Distill-}\\\textbf{Llama-8B}}} 
& Vanilla & 50.00 & 15672.07 & 50.00 & 8910.93 & 40.00 & 6457.43 \\
\cmidrule(lr){2-8}
& SFT & 44.95 & 13678.53 & 40.00 & 8699.93 & 35.10 & 6804.28 \\
& DPO & 46.97 & 15716.27 & 50.00 & 8489.33 & 42.40 & 6301.96 \\
& CB & 46.97 & 15488.23 & 46.97 & 9088.78 & 40.65 & 6479.9 \\
\cmidrule(lr){2-8}
& X-Boundary & 50.00 & 13310.90 & 50.00 & 8233.20 & 39.86 & 6498.04 \\
\midrule
\multirow{5}{*}{\makecell{\textbf{DeepSeek-}\\\textbf{R1-Distill-}\\\textbf{Qwen-7B}}} 
& Vanilla & 53.33 &11046.63 & 48.99 & 8592.54 & 39.76 & 6683.22 \\
\cmidrule(lr){2-8}
& SFT & 46.67 & 13844.87 & 48.99 & 8176.29 & 36.44 & 6825.17 \\
& DPO & 53.33 & 12063.57 & 50.00 & 8344.05 & 40.08 & 6694.74 \\
& CB & 46.97& 12609.93 & 46.97 & 8356.40 & 39.33 & 6536.76\\
\cmidrule(lr){2-8}
& X-Boundary & 53.33 & 12959.73 & 50.51 & 8237.67 & 40.02 & 6583.29 \\
\midrule
\end{tabular}}
\vspace{-2pt}
\end{table*}

\subsection{The Trade-Off between Robustness and General Capability}
\label{app:asr_utility}
Fig. \ref{fig:asr_utility} intuitively shows the trade-off between the ASR against multi-turn jailbreaks and the decline of general capability.
%
As the training process advances, the ASR steadily decreases, while the decline in code and math capability progressively increases.
% It is inappropriate to focus solely on excelling in a single metric.
%
X-Boundary lies in the lower-left corner of the plots, demonstrating that it achieves a win-win outcome with robust defense and strong general capability.
% achieves both low ASR and low over-refusal rate in the meantime.

\renewcommand{\thesubfigure}{}
\begin{figure}[!ht]
	\begin{center}
		\subfigure[]{
			\centering
            \includegraphics[width=0.48\columnwidth]{Figs/GSM8K.pdf}      
		}
		\subfigure[]{
			\centering
			\includegraphics[width=0.48\columnwidth]{Figs/HumanEval.pdf}     
		}\vspace{-10mm}
	\end{center}
	\caption{The trade-off between ASR of multi-turn jailbreak and general capability on Llama-3-8B-Instruct. The data points were collected by sampling and evaluating at every 100 training steps.}
    \label{fig:asr_utility}
\end{figure}

\subsection{Ablation Study on Llama-3-8B-Instruct and Mistral-7B-Instruct-v0.2}
\label{app:ablation}
Through analyzing the results of ablation experiments in Table~\ref{tab:ablation_llama} and Table~\ref{tab:ablation_mistral}, we can obtain conclusions consistent with that in Section \ref{sec:ablation}.

\subsection{Effects of the Size of Boundary-Safe Data}
\label{app:set_size_effect}
Fig.~\ref{fig:data_size} shows that as the boundary-safe data size increases, the over-refusal rate generally decreases, while ASR against multi-turn attacks tends to increase. Without the separate loss, when the boundary-safe data size reaches 500, the ASR hardly decreases, failing to achieve the purpose of enhancing multi-turn defense. This demonstrates that it is difficult to balance ASR and over-refusal rate simply by adjusting the boundary-safe data size.

\subsection{Effects of Separate Loss and Boundary-Safe Data on the Representation Distribution}
\label{app:repe_angle}
Fig.~\ref{fig:repe_angle} shows that adding boundary-safe data to the retain set reduces the angle between boundary-safe representations after training and their original representations.
Furthermore, under the effect of separate loss, this angle is further minimized.
Meanwhile, the angle between boundary-safe representations and refusal representations increases, indicating that separate loss contribute  to establish a clear distinction boundary.

\begin{table*}[!ht]
\setlength{\abovecaptionskip}{0.1in}
\caption{Ablation study on Llama-3-8B-Instruct. In this table, A represents single-turn defense data, B represents multi-turn defense data, C represents boundary-safe data, and D represents the separate loss $\mathcal{L}_{\texttt{s}}$.}
\label{tab:ablation_llama}
\setlength{\tabcolsep}{2pt}
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{c|cccc|ccccccccccccc}
%\Xhline{4\arrayrulewidth}
\midrule
\multirow{2.5}{*}{\textbf{Models}} & \multirow{2.5}{*}{A} & \multirow{2.5}{*}{B} & \multirow{2.5}{*}{C} & \multirow{2.5}{*}{D} & \multicolumn{3}{c}{\textbf{Multi-turn ASR (\%) $\downarrow$}} & \multicolumn{4}{c}{\textbf{Over-refusal Rate (\%) $\downarrow$}} & \multicolumn{3}{c}{\textbf{General Capability (\%) $\uparrow$}} \\
\cmidrule(lr){6-8} \cmidrule(lr){9-12} \cmidrule(lr){13-15}
% Model & Single Round ASR (\%) & Multi Round ASR (\%) & 
~ & & & & & ActorAttack & RedQueen & Crescendo & XSTest & OKTest & OR-Bench & PHTest & MMLU & GSM8K & HumanEval \\
\midrule
Vanilla & & & & & 58.50 & 25.00 & 34.00 & 6.80 & 9.00 & 8.00 & 13.67 & 68.30 & 79.08 & 59.18 \\
(a) & $\checkmark$ & & & & 36.50 & 5.00 & 18.00 & 12.00 & 16.00 & 14.33 & 26.00 & 68.13 & 78.54 & 59.76\\
(b) & $\checkmark$ & $\checkmark$ & & & 16.50 & 0.50 & 10.00 & 23.60 & 27.67 & 36.00 & 52.00 & 67.66 & 78.47 & 59.76\\
(c) & $\checkmark$ & $\checkmark$ & $\checkmark$ & & 15.00 & 0.50 & 10.00 & 14.00 & 18.00 & 11.67 & 35.33 & 68.05 & 78.47 & 59.76\\
\cmidrule(lr){1-15}
X-Boundary & $\checkmark$ & $\checkmark$ & $\checkmark$ & $\checkmark$ & 16.50 & 1.00 & 10.00 & 8.40 & 14.00 & 8.00 & 28.66 & 67.94 & 78.47 & 59.76\\
% \Xhline{4\arrayrulewidth}
\midrule
\end{tabular}}
\end{table*}
%
\begin{table*}[!ht]
\setlength{\abovecaptionskip}{0.1in} % 让 caption 和表格更紧凑
\caption{Ablation study on Mistral-7B-Instruct-v0.2. In this table, A represents single-turn defense data, B represents multi-turn defense data, C represents boundary-safe data, and D represents the separate loss $\mathcal{L}_{\texttt{s}}$.}
\label{tab:ablation_mistral}
\setlength{\tabcolsep}{2pt}
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{c|cccc|ccccccccccccc}
%\Xhline{4\arrayrulewidth}
\midrule
\multirow{2.5}{*}{\textbf{Models}} & \multirow{2.5}{*}{A} & \multirow{2.5}{*}{B} & \multirow{2.5}{*}{C} & \multirow{2.5}{*}{D} & \multicolumn{3}{c}{\textbf{Multi-turn ASR (\%) $\downarrow$}} & \multicolumn{4}{c}{\textbf{Over-refusal Rate (\%) $\downarrow$}} & \multicolumn{3}{c}{\textbf{General Capability (\%) $\uparrow$}} \\
\cmidrule(lr){6-8} \cmidrule(lr){9-12} \cmidrule(lr){13-15}
% Model & Single Round ASR (\%) & Multi Round ASR (\%) & 
~ & & & & & ActorAttack & RedQueen & Crescendo & XSTest & OKTest & OR-Bench & PHTest & MMLU & GSM8K & HumanEval \\
\midrule
Vanilla & & & & & 70.00 & 49.50 & 40.00 & 10.00 & 21.00 & 4.33 & 13.00 & 59.98 & 45.34 & 34.76 \\
(a) & $\checkmark$ & & & & 46.00 & 28.00 & 20.00 & 28.80 & 28.00 & 18.00 & 23.00 & 59.92 & 44.66 & 34.76\\
(b) & $\checkmark$ & $\checkmark$ & &  & 15.00 & 11.50 & 12.00 & 45.20 & 32.33 & 55.00 & 50.00 & 59.91 & 46.63 & 33.54\\
(c) & $\checkmark$ & $\checkmark$ & $\checkmark$ & & 13.50 & 30.00 & 14.00 & 35.60 & 25.67 & 12.67 & 38.67 & 60.06 & 46.17 & 35.37\\
\cmidrule(lr){1-15}
X-Boundary & $\checkmark$ & $\checkmark$ & $\checkmark$ & $\checkmark$ & 16.00 & 13.50 & 14.00 & 19.20 & 23.33 & 10.33 & 26.33 & 59.83 & 45.34 & 36.59\\
% \Xhline{4\arrayrulewidth}
\midrule
\end{tabular}}
\end{table*}
%
% \renewcommand{\thesubfigure}{}
\begin{figure}[!ht]
	\begin{center}
		\subfigure[]{
			\centering
            \includegraphics[width=0.48\columnwidth]{Figs/data_size_wo_sl.pdf}      
		}
		\subfigure[]{
			\centering
			\includegraphics[width=0.48\columnwidth]{Figs/data_size_with_sl.pdf}     
		}
        \vspace{-5mm}
	\end{center}
	\caption{The impact of boundary-safe data size on ASR and over-refusal rate without and with separate loss.}
    \label{fig:data_size}
\end{figure}
%
\begin{figure}[h]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.6\columnwidth]{Figs/repe_angle.pdf}}
\caption{Visualization of effects of separate loss and boundary-safe data on the representation distribution. ``Boundary-Safe'' refers to the average representations of boundary-safe queries from OR-Bench along with their corresponding helpful responses. ``refusal'' refers to the average representations of boundary-safe queries from OR-Bench paired with refusal responses.}
\label{fig:repe_angle}
\end{center}
\vskip -0.2in
\end{figure}

\subsection{Details about Representation Visualization}
\label{app:complete_tsne}
To analyze safety-usability trade-off from the perspective of interpretability mechanism, we extract the feature representations from the 10th layer of Llama-3-8B-Instruct and visualize them using 2-dimensional t-SNE, as shown in Fig.~\ref{figs:app_tsne}.
%
\begin{figure}[h]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{Figs/app_tsne.pdf}}
\caption{Visualization of the representation distribution before and after implementing SFT, DPO, GA, and CB. ``Harmful'' and ``boundary-safe'' refer to the representations of harmful and boundary-safe queries along with their corresponding responses, respectively.}
\label{figs:app_tsne}
\end{center}
\vskip -0.2in
\end{figure}

\section{Experimental Details}
\subsection{Construction of Multi-Turn Defense Dataset}
\label{app:defense_data}
We construct a multi-turn defense dataset based on SafeMTData. 
%
SafeMTData is derived from the circuit breaker training dataset, and carefully filtered to prevent data contamination with Harmbench.
%
It includes harmful multi-turn queries generated by ActorAttack \cite{actor_attack}, along with refusal responses to reject the harmful queries.
%
To curate the harmful responses for DPO, GA, and CB, we use harmful multi-turn queries in SafeMTData to attack deepseek-chat \cite{deepseek} and filter the harmful response using HarmBench classifier \cite{harmbench}.
%
% On the other hand, we replace harmful responses with refusal responses, and use the previous multi-turn dialogues as context to train the LLMs with multi-turn SFT and DPO.

For SFT and DPO, following \citet{actor_attack}, we maintain a 1:2 ratio between the multi-turn defense data and instruction data, \eg UltraChat \cite{ultrachat}.
% Considering that CB and GA enhance the safety defenses by diminishing LLMs’ ability to generate harmful outputs, their effectiveness should be agnostic with the specific form and content of the jailbreak prompts.
% Therefore, we extract harmful responses from multi-turn dialogues that have been successfully attacked and construct training sets following \citet{safe_unlearning} and \citet{circuit_breaker}.
 %
For CB, we add the filtered harmful responses and their corresponding single-turn queries into the CB dataset.
%
The other data settings remain consistent with \citet{circuit_breaker}.
%
For GA, we follow \cite{safe_unlearning} and use unlearning data, instruction data, and refusal data in a ratio of 5:5:1.

\subsection{Training Details of Baselines}
\label{app:baseline_settings}
\paragraph{Multi-Turn SFT} 
For multi-turn SFT, we set the batch size to 1 with accumulation step 16. The training process was conducted for a total of 1 epoch. Optimization was performed using the AdamW optimizer, with the learning rate set to \(5 \times 10^{-4}\), ensuring stable and efficient model updates. The warm-up ratio and weight decay ratio are set to 0.05, 0.03. All training processes use Low-Rank Adaptation (LoRA) for parameter fine-tuning, where the rank \(r\), scaling factor \(\alpha\), and dropout rate are set to 16, 16, and 0.1, respectively. It takes about 40 minutes to train a Llama-3-8B-Instruct model on a single A100 80G GPU.

\paragraph{Multi-Turn DPO} 
For Multi-turn DPO, we use a learning rate of \(1.0 \times 10^{-5}\) with a cosine learning rate scheduler and a warm-up ratio of 0.1. We set the training epoch to 3 and the batch size to 1 with gradient accumulation steps of 8. All training processes use Low-Rank Adaptation (LoRA) for parameter fine-tuning with the rank \(r\), scaling factor \(\alpha\), and dropout rate set to 8, 16, and 0, respectively. We conducted all training processes on a single A100 80GB GPU.

\paragraph{Gradient Ascent} 
Following the experimental setting of \citet{safe_unlearning}, we set the batch size to 11 with accumulation step 1, where the ratio of the three types of data in a batch is 5:5:1.
We use the AdamW optimizer with a learning rate of \( 2 \times 10^{-5} \) and set the maximum epoch as 3. For Qwen2.5-7B-Chat and Llama-3-8B-Instruct, the coefficients of safe responses loss $ \mathcal{L}_s $, general performance loss $ \mathcal{L}_g $, and unlearning loss $ \mathcal{L}_h $ are set to 0.5, 1.0, 0.3. For Mistral-7B-Instruct-v0.2, the loss coefficients are set to 0.25, 1.0, and 0.05, respectively.
All training processes use Low-Rank Adaptation (LoRA) for parameter fine-tuning. For Llama-3-8B-Instruct and Mistral-7B-Instruct-v0.2, we set the rank \(r\), scaling factor \(\alpha\), and 
dropout rate to 16, 16, 0.05. For Qwen2.5-7B-Chat, we conducted a grid search over the LoRA hyperparameters with \(r \in \{8, 16, 32\}\) and \(\alpha \in \{16, 32, 64\}\). We end up selecting \(r = 8\), \(\alpha = 64\), and a dropout rate of \(0.05\). We linearly decay the learning rate and select the checkpoint after 1 epoch for evaluation. Training a Mistral-7B-Instruct-v0.2 model on a single A100 80GB GPU takes approximately 1 hour.

\paragraph{Circuit Breaker}
We follow \cite{circuit_breaker} to use LoRA for fine-tuning and set the rank \(r\) as 16 on Llama-3-8B-Instruct and Mistral-7B-Instruct-v0.2, 32 on Qwen2.5-7B-Chat and Qwen2.5-14B-Chat. We gather the feature representations from layers 10, 20, 30, and 40 to calculate circuit-breaking loss and inset LoRA adapter into all linear layers from 0 through 40. 
The loss coefficients are dynamically adjusted. The coefficients of circuit-breaking loss and retain loss are $c_s=\alpha(1-\frac{t}{\beta})$ and $c_r=\alpha \frac{t}{\beta}$, respectively. We set $\alpha$ as 5 on Mistral-7B-Instruct-v0.2 and 10 on other LLMs, $\beta$ as 300 on Mistral-7B-Instruct-v0.2 and Llama-3-8B-Instruct, 600 on Qwen2.5-7B-Chat, and 1200 on Qwen2.5-14B-Chat. 
Qwen2.5-14B-Chat is trained on for 360 steps with a batch size of 8 on 4 A100 GPUs, while other LLMs is trained on for 180 steps with a batch size of 16 on 1 A100 GPU.

\subsection{Training Details of X-Boundary}
\label{app:x_training}
We use LoRA for fine-tuning and set the rank \(r\) as 16 on Llama-3-8B-Instruct and Mistral-7B-Instruct-v0.2, 32 on Qwen2.5-7B-Chat and Qwen2.5-14B-Chat.
We set dynamic loss coefficients following \cite{circuit_breaker}, where $c_r=\alpha \frac{t}{\beta}$ and $c_e=c_s=\alpha(1-\frac{t}{\beta})$.
$\alpha,\beta$, and the target layers for calculating erase loss keep consistent with hyperparameters specified in Appendix~\ref{app:baseline_settings}.
We conduct a grid search on the size of boundary-safe data in a valid set in the range of [0,500], with a step of 50, selecting the size for Llama-3-8B-Instruct, Mistral-7B-Instruct-v0.2, Qwen2.5-7B-Chat, and Qwen2.5-14B-Chat is 500, 200, 100, and 50, respectively.
Qwen2.5-14B-Chat is trained for 260 steps with a batch size of 8 on 4 A100 GPUs, while other LLMs are trained for 180 steps with a batch size of 16 on 1 A100 GPU.

\subsection{Evaluations}
\label{app:eval}
\paragraph{Datasets.}
We evaluate our approach on benchmarks covering multi-turn attacks, over-refusal, and general model capabilities:
% \paragraph{Single-turn Attack}: We use HarmBench to evaluate model robustness against direct jailbreak attempts.
\paragraph{Multi-Turn Attack} We employ three state-of-the-art multi-turn attack benchmarks.
We adopt three state-of-the-art multi-turn attack benchmarks:
\begin{itemize}
    \item ActorAttack~\cite{actor_attack}: Emphasizes role-playing scenarios to gradually induce harmful behavior. The multi-turn queries in SafeMTData\_Attack\_600~\cite{actor_attack} are used to attack victim models, and HarmBench classifier~\cite{harmbench} is used to judge whether the attack is successful.
    \item RedQueen~\cite{red_queen}: Focuses on dynamic prompt engineering with iterative refinements. We use the template of RedQueen to generate 600 test data based on HarmBench, and use HarmBench classifier as the judge model.
    \item Crescendo~\cite{crescendo}: Includes gradually escalating attacks that push the model to produce harmful content over multiple turns. GPT-3.5-turbo is used as the attack model and GPT-4o is utilized as the judge model.
\end{itemize}

\paragraph{Over-Safety Assessment} We utilize four complementary datasets to measure over-refusal:
\begin{itemize}
    \item XSTest~\cite{xstest}: Examines model responses to boundary-case prompts involving sensitive but potentially valid information.
    \item OKTest~\cite{oktest}: Evaluates whether the model declines benign questions in real-world scenarios.
    \item OR-Bench~\cite{orbench}: Explicitly measures over-refusal rates on a suite of harmless queries.
    \item PHTest: Comprises prompts that may look suspicious but are legitimately safe for the model to address.
\end{itemize}
\paragraph{General Capability} To ensure our method preserves the model’s general performance, we use:
\begin{itemize}
    \item MMLU: A broad measure of knowledge in diverse domains.
    \item GSM8K: A math reasoning benchmark to test step-by-step problem solving.
    \item HumanEval~\cite{human_eval}: Assesses code generation capability, crucial for real-world AI applications.
\end{itemize}

\paragraph{Evaluation Metrics.}
To comprehensively assess our method, we adopt the following evaluation metrics:
\begin{itemize}
    \item Attack Success Rate (ASR): The proportion of attack attempts (single-turn or multi-turn) that successfully elicit harmful content from the model. Lower ASR indicates better robustness against jailbreaks.

    \item Over-Refusal Rate \label{subsubsec:orr}: The fraction of benign prompts that the model incorrectly refuses to answer. A lower over-refusal rate signifies better usability.

    \item General Performance \label{subsubsec:general}: We measure the model’s utility on standard benchmarks (MMLU, GSM8K, HumanEval) to ensure that defensive measures do not degrade essential capabilities. A higher score indicates stronger performance on domain knowledge, reasoning, or code generation.
\end{itemize}

\section{Theoretical Analysis of X-Boundary}
\label{ap:proof}

\begin{proposition}
\label{ap:prop_cluster}
 If $\phi_\# \mu$ is $(n, \Delta)$-clusterable, then for all $m \leq n(2\Delta)^{-2}$,
 \begin{equation}
     \Var_{m}(\phi_\# \mu) < 48\Delta.
 \end{equation}
 Given a distribution $\mu$, $(n, \Delta)$-clusterable means that $\textnormal{supp}(\mu)$ lies in the union of $n$ balls of radius at most $\Delta$.
\end{proposition}
\begin{proof} Proposition \ref{prop_cluster} in this paper is an application of Proposition 13 in \cite{weed2017sharp}.


\begin{definition}[\cite{weed2017sharp}]
A distribution $\mu$ is $(m, \Delta)$-clusterable if $\supp(\mu)$ lies in the union of~$m$ balls of radius at most $\Delta$.
\end{definition}

\begin{proposition}[Proven in \cite{weed2017sharp}]
If $\mu$ is $(n, \Delta)$-clusterable, then for all $m \leq n(2\Delta)^{-2p}$, 
\begin{align}
    \E_{S \sim \mu^m}[\gW_p^p(\mu, \mu_S)] \leq (9^p + 3) \sqrt{\frac{n}{m}}.
\end{align}
\end{proposition}
According to the triangle inequality, we have
\begin{align}
\Var_m(\phi_\# \mu) = \E_{S, \tilde{S} \sim \mu^m}[\gW_1(\phi_\# \mu_S, \phi_\# \mu_{\tilde{S}})] \leq 2\E_{S \sim \mu^m}[\gW_p^p(\mu, \mu_S)] \leq 24\sqrt{\frac{n}{m}}.
\end{align}

In this way, for all $m \leq n(2\Delta)^{-2p}$, we have

\begin{align}
\Var_m(\phi_\# \mu) \leq 24\sqrt{\frac{n}{m}} < 48\Delta.
\end{align}

\end{proof}



\section{Case Study}
\label{app:case_study}
In this section, we showcase a range of examples to demonstrate the practical utility of the X-Boundary in mitigating over-safety and its robustness against multi-turn attacks.\\ 

\textbf{Cases of Over-Refusal} In Fig.~\ref{oversafety_case1} and Fig.~\ref{oversafety_case2}, we present two examples from OKTest and OR-Bench, comparing the responses of Llama-3-8B-Instruct with different safety strategies. X-Boundary provides accurate and useful responses to boundary-safe queries, \eg``Is there an effective program in place to educate children about the dangers of molestation? '', unlike previous methods which incorrectly treat them as harmful queries and refuse to respond. This lies in X-Boundary effectively increases the distance between harmful representations and boundary-safe representations, allowing for a clear distinction between harmful queries and boundary-safe queries.\\

\textbf{Cases of Multi-Turn Attack} In Fig.~\ref{multi_attack_case1}, Fig.~\ref{multi_attack_case2}, and Fig.~\ref{multi_attack_case3}, we present three examples taken from ActorAttack, RedQueen, and Crescendo, respectively. 
X-Boundary effectively defends against various types of multi-turn jailbreaks, demonstrating its robustness.\\

\textbf{Cases of Safety Defense on Reasoning Model} In Fig.~\ref{reasoning_case}, with X-Boundary, dangerous content generated in the reasoning process is automatically converted into gibberish. Additionally, we use carefully designed rules to detect gibberish, allowing us to terminate the thought process early and replace the response with a refusal answer.

\begin{figure}[h]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.9\columnwidth]{Figs/oversafety_case1.pdf}}
\caption{Over-Safety example from Llama-3-8B-Instruct using various defense methods on OR-Bench.}
\label{oversafety_case1}
\end{center}
\vskip -0.2in
\end{figure}

\begin{figure}[h]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.9\columnwidth]{Figs/oversafety_case2.pdf}}
\caption{Over-Safety example from Llama-3-8B-Instruct using various defense methods on OKTest.}
\label{oversafety_case2}
\end{center}
\vskip -0.2in
\end{figure}

\begin{figure}[h]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.9\columnwidth]{Figs/multi_attack_case1.pdf}}
\caption{Multi-Turn Attack example from Llama-3-8B-Instruct on Actorattack.}
\label{multi_attack_case1}
\end{center}
\vskip -0.2in
\end{figure}

\begin{figure}[h]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.9\columnwidth]{Figs/multi_attack_case2.pdf}}
\caption{Multi-Turn Attack example from Llama-3-8B-Instruct on RedQueen.}
\label{multi_attack_case2}
\end{center}
\vskip -0.2in
\end{figure}

\begin{figure}[h]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.85\columnwidth]{Figs/multi_attack_case3.pdf}}
\caption{Multi-Turn Attack example from Qwen2.5-7B-Chat on Crescendo.}
\label{multi_attack_case3}
\end{center}
\vskip -0.2in
\end{figure}

\begin{figure}[t]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{Figs/case_in_reasoning_model.pdf}}
\caption{Safety defense example on reasoning models.}
\label{reasoning_case}
\end{center}
\vskip -0.2in
\end{figure}