\begin{table*}[t]
% \captionsetup{position=above} % 让 caption 显示在表格上方
\setlength{\abovecaptionskip}{0.1in} % 让 caption 和表格更紧凑
\caption{Comparison of existing defense methods and X-Boundary.}
% \vskip 0.1in
\label{tab:main_results}
\setlength{\tabcolsep}{2pt}
\centering
\resizebox{2\columnwidth}{!}{
\begin{tabular}{cc|cccccccccccc}
%\Xhline{4\arrayrulewidth}
\midrule
\multirow{2.5}{*}{\textbf{Models}} & \multirow{2.5}{*}{\textbf{Methods}} & \multicolumn{3}{c}{\textbf{Multi-Turn ASR (\%) $\downarrow$}} & \multicolumn{4}{c}{\textbf{Over-Refusal Rate (\%) $\downarrow$}} & \multicolumn{3}{c}{\textbf{General Capability (\%) $\uparrow$}} \\
\cmidrule(lr){3-5} \cmidrule(lr){6-9} \cmidrule(lr){10-12}
% Model & Single Round ASR (\%) & Multi Round ASR (\%) & 
& ~ & ActorAttack & RedQueen & Crescendo & XSTest & OKTest & OR-Bench & PHTest & MMLU & GSM8K & HumanEval \\
\midrule
\multirow{6}*{\makecell{\textbf{Llama-3-}\\\textbf{8B-Instruct}}} & Vanilla & 58.50 & 25.00 & 34.00 & 6.80 & 9.00 & 8.00 & 13.67 & 68.30 & 79.08 & 59.18 \\
\cmidrule(lr){2-12}
~ & SFT & 19.50 & \textbf{0.50} & \textbf{8.00} & 27.20 & 42.33 & 22.00 & 57.33 & 68.17 & 76.19 & 54.27 \\
~ & DPO & 17.50 & 5.00 & 14.00 & 20.00 & 28.33 & 17.33 & 41.00 & 68.01 & 75.59 & 58.54 \\
~ & GA & 38.50 & 1.50 & 12.00 & 10.80 & 15.00 & 13.33 & 35.33 & \textbf{68.25} & 77.86 & \textbf{62.20} \\
~ & CB & \textbf{16.50} & \textbf{0.50} & 10.00 & 23.60 & 27.67 & 36.00 & 52.00 & 67.66 & 78.47 &59.76 \\
\cmidrule(lr){2-12}
~ & X-Boundary & \textbf{16.50} & 1.00 & 10.00 & \textbf{8.40} & \textbf{14.00} & \textbf{8.00} & \textbf{28.67} & 67.94 & \textbf{78.70} & 59.76 \\
\midrule
\multirow{6}*{\makecell{\textbf{Qwen2.5}\\\textbf{-7B-Chat}}} & Vanilla & 76.00 & 39.50 & 62.00 & 6.00 & 19.33 & 1.67 & 5.60 & 74.26 & 80.67 & 81.71 \\
\cmidrule(lr){2-12}
~ & SFT & 21.00 & 6.00 & 18.00 & 46.00 & 57.67 & 29.33 & 53.67 & 74.30 & 76.42 & 77.44 \\
~ & DPO & 38.00 & 12.00 & 24.00 & 21.60 & 25.67 & 11.67 & 32.33 & 73.63 & \textbf{80.97} & 80.49 \\
~ & GA & 38.00 & 21.00 & \textbf{12.00} & 58.40 & 70.00 & 67.67 & 85.33 & \textbf{74.58} & 80.43 & 79.27 \\
~ & CB & \textbf{15.50} & \textbf{5.50} & \textbf{12.00} & 20.60 & 26.00 & 34.00 & 43.67 & 74.21 & 80.36 & \textbf{81.10} \\
\cmidrule(lr){2-12}
~ & X-Boundary & 17.50 & 7.50 & 16.00 & \textbf{10.40} & \textbf{16.67} & \textbf{5.33} & \textbf{15.00} & 74.17 & 80.52 & \textbf{81.10} \\
% \Xhline{4\arrayrulewidth}
\midrule
\multirow{6}*{\makecell{\textbf{Mistral-7B}\\\textbf{-Instruct-v0.2}}} & Vanilla & 70.00 & 49.50 & 40.00 & 10.00 & 21.00 & 4.33 & 13.00 & 59.98 & 45.34 & 34.76 \\
\cmidrule(lr){2-12}
~ & SFT & 37.50 & 22.00 & 18.00 & 53.60 & 42.00 & 29.33 & 58.67 & 58.94 & 41.55 & 27.44 \\
~ & DPO & 44.50 & 19.00 & 28.00 & 25.20 & 38.67 & 20.33 & 37.67 & 58.79 & 43.21 & 34.76 \\
~ & GA & 24.00 & \textbf{9.00} & \textbf{10.00} & 38.40 & 50.67 & 35.67 & 71.33 & 60.13 & 45.64 & 34.76 \\
~ & CB & \textbf{15.00} & 11.50 & 12.00 & 45.20 & 32.33 & 55.00 & 50.00 & \textbf{59.91} & \textbf{46.63} & 33.54 \\
\cmidrule(lr){2-12}
~ & X-Boundary & 16.00 & 13.50 & 14.00 & \textbf{19.20} & \textbf{23.33} & \textbf{10.34} & \textbf{26.33} & 59.83 & 45.34 & \textbf{36.59} \\
% \Xhline{4\arrayrulewidth}
\midrule
\end{tabular}}
\vspace{-10pt}
\end{table*}

\section{Comparison of Existing Defense Methods Against Multi-Turn Jailbreaks}
To the best of our knowledge, we are the \textbf{first} to conduct a \textbf{comprehensive evaluation} of classic defense approaches against multi-turn jailbreaks, considering both defense robustness and impact on usability.
%
Although previous studies~\cite{actor_attack, red_queen} have employed Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO) in multi-turn defense scenarios, many other defense methods effective against single-turn jailbreaks, such as Gradient Ascent (GA) and Circuit Breaking (CB), have been overlooked and have not been thoroughly compared.
% Meanwhile, Gradient Ascent (GA) and Circuit Breaking (CB) have primarily demonstrated effectiveness against various single-turn jailbreak attacks.}
%
In Section~\ref{sec:comparison}, we outline the process of constructing training data, reproducing SFT and DPO, and adapting GA and CB for multi-turn defense scenarios.
%
In Section~\ref{sec:existing_eval}, we present and analyze the evaluation results, \textbf{revealing the shortcomings of existing defense methods in balancing robustness and usability.}

\subsection{Adaption and Evaluation of Single-Turn Defense Methods Against Multi-Turn Jailbreaks}
% Implementation and Evaluation of Existing Defense Methods in Multi-turn Scenarios.}}
\label{sec:comparison}
We compare the following defense methods against multi-turn jailbreak \cite{actor_attack, red_queen} on Llama-3-8B-Instruct \cite{llama3-instruct}, Qwen2.5-7B-Chat \cite{qwen2.5}, and Mistral-7B-Instruct-v0.2 \cite{mistral7b}:
\begin{itemize}
  \item SFT \cite{actor_attack}: fine-tuning LLMs using harmful queries as inputs and refusal answers as supervised labels directly.
  \item DPO \cite{dpo, red_queen}: aligning LLMs using harmful queries as inputs, harmful answers as rejected responses, and refusal answers as chosen responses.
  \item GA \cite{safe_unlearning, eraser}: unlearning harmful knowledge by training with gradient ascent optimization methods .
  \item CB \cite{circuit_breaker}: remapping the representations of harmful knowledge to desired targeted representations.
\end{itemize}

\textbf{Construct multi-turn defense datasets.} 
% We can categorize the above methods into two types: input-based and output-based.
% The former defends by training LLMs to recognize the characteristics of harmful inputs and refuse to respond, while the latter focuses on diminishing the ability of LLMs to generate harmful outputs. 
% Therefore, the training data for input-based methods require complete multi-turn dialogues as context, whereas the training data for output-based methods only need to include harmful outputs and their corresponding single-turn queries.}
We construct the multi-turn defense training datasets based on SafeMTData \cite{actor_attack}.
%
SafeMTData consists of 1680 safe multi-turn dialogues for the safety alignment of LLMs in multi-turn interactions.
%
For SFT, we directly exploit SafeMTData as a multi-turn training dataset following \citet{actor_attack}.
%
For DPO, we curate harmful responses to the harmful multi-turn queries in SafeMTData and constructed a multi-turn preference dataset following \citet{red_queen}.
%
For CB and GA, to remove harmful knowledge that could be elicited through multi-turn attacks, we add pairs of harmful queries from SafeMTData along with the curated harmful responses into their respective defense training datasets.~\cite{safe_unlearning,circuit_breaker}
%
More details about data construction are illustrated in Appendix~\ref{app:defense_data} and training settings of these methods are listed in Appendix~\ref{app:baseline_settings}.

\textbf{Evaluation of defense robustness.}
%
To evaluate the robustness of these defense methods, we test them against three SOTA multi-turn jailbreak attacks: ActorAttack~\cite{actor_attack}, RedQueen~\cite{red_queen} and Crescendo~\cite{crescendo}.
%
The evaluation metric used is the Attack Success Rate (ASR), defined as the proportion of attack attempts that successfully elicit harmful content from the LLMs. 
%
A lower ASR indicates greater robustness against multi-turn jailbreak attacks.
%
We also evaluate the compliance rates of LLMs with harmful requests in HarmBench and the results are listed in the Appendix \ref{app:single_turn_asr}.
% To evaluate the defense performance, we test attack success rate (ASR) on the corresponding testing set of ActorAttack XX, and use harmbench testset to construct multi-turn attack prompts by RedQueen \cite{red_queen} and Crescendo \cite{crescendo}. 

\textbf{Evaluation of usability.}
We evaluate the impact of defense methods on the usability of LLMs from two perspectives: over-refusal and the decline of general capability.
%
Over-refusal is evaluated using XSTest \cite{xstest}, OKTest \cite{oktest}, OR-Bench \cite{orbench}, and PHTest \cite{phtest}.
%
The corresponding evaluation metric is the over-refusal rate, which measures the proportion of benign prompts that the model incorrectly refuses to answer. 
%
A lower over-refusal rate indicates better usability.
%
General capability, including general knowledge, mathematics, and coding ability, is evaluated using MMLU \cite{mmlu}, GSM8K \cite{gsm8k} and HumanEval \cite{human_eval}, respectively.
%
Please see Appendix~\ref{app:eval} for more details about evaluation.

\subsection{Experimental Results and Analysis}
\label{sec:existing_eval}
\textbf{Existing methods fail to strike a balance between robustness and usability.}
%
Table~\ref{tab:main_results} shows that existing methods can effectively reduce the ASR of multi-turn jailbreaks after training with the aforementioned data.
% Among these methods, multi-turn SFT and CB demonstrate outstanding defense performance on both Llama3-8B and Qwen2.5-7B.
However, SFT, DPO, and GA even tend to severely compromise general capabilities when achieving good performance, commonly referred to as the ``alignment tax'' \cite{alignment_tax}. 
%
For instance, SFT results in a 3\%-7\% decrease in both the mathematical and the coding abilities of the three models.
%
Moreover, all of these methods lead to a significant increase in the over-refusal rate.

\textbf{An excessively high over-refusal rate makes the ASR unreliable.}
%
In PHtest, the over-refusal rate of Llama-3-8B increases from 13.67\% to more than 40\% on all methods.
%
The over-refusal rates after GA are more than 50\% on four benchmarks.
% Additionally, we find that multi-turn defenses exacerbate the over-refusal problem to a greater extent compared with single-turn defense. Please see \ref{} for more discussions.
%
The high over-refusal rate reflects that these methods cannot precisely distinguish harmful queries and build effective defense mechanisms for them. 
%
Instead, they simply reduce the ASR of multi-turn attacks by indiscriminately rejecting input queries, which is not trustworthy and undermines the model’s usability in real-world scenarios.
%
Therefore, it is necessary to analyze the cause of over-refusal and propose a more precise defense method to mitigate it while preserving robustness against multi-turn jailbreak.

\section{X-Boundary: Optimize Exact Boundary to Balance Defense Robustness and Usability}
In this section, we propose X-Boundary to balance robustness against multi-turn jailbreaks and usability by explicitly formulating the distinction boundary.
%
Section~\ref{sec:formulation} analyzes the essential mechanism of decline in usability. Section~\ref{sec:loss} introduces the optimization objective of X-Boundary. Section~\ref{sec:theoretical} theoretically proves that X-Boundary may ease the learning difficulty and contribute to fast learning.
% can accelerate the convergence process. 

\subsection{The Imprecise Distinction Boundary of Existing Multi-Turn Defense Methods.}
\label{sec:formulation}
% \textbf{The decline in usability is caused by an imprecise distinction boundary.}
%
\textbf{Notations.} Give an input data point $x$, $\mathcal{R}_{\mathcal{M}} \left(x \right)$ denotes its feature representations encoded by LLMs $\mathcal{M}$.
%
$\{x_i\}_{i=1}^N$ and $\{\mathcal{R}_{\mathcal{M}} \left(x_i \right)\}_{i=1}^N$ denote a set of multiple data points and representations, respectively.
%
In particular, $x_i^h$ represents a harmful Query and its corresponding harmful Answer (QA pair), while $x_i^r$ denotes the refusal response to the harmful query $x_i^h$.
%
$x_i^s$ and $x_i^b$ denote a safe QA pair and a boundary-safe QA pair, respectively, where the answer is both safe and helpful.
% x is a sequence of tokens, a vector; R is a matrix (token_len * hidden_size) 

\textbf{Analysis of safety-usability trade-off from the perspective of interpretability mechanism.} Existing defense methods~\cite{circuit_breaker,zou2023representation} typically improve the adversarial robustness of LLMs by intervening harmful feature representations $\{\mathcal{R}_{\mathcal{M}} \left(x_i^h \right)\}_{i=1}^N$.
% $S_h=\{(q_i^h, a_i^h)\}$
% yielded by harmful Questions and harmful Answer (QA) pair $x_h$.
% Questions and Answer (QA) pairs
Specifically, SFT~\cite{decoupled_sft} and CB~\cite{circuit_breaker} remap harmful representations to refusal representations $\mathcal{R}_{\mathcal{M}} \left(x_i^r \right)$.
% \wrt refusal response $x_i^r$
% \textcolor{red}{incoherent representations} or
%
In this process, these methods implicitly train LLMs to learn a boundary that distinguishes harmful representations and safe representations $\{\mathcal{R}_{\mathcal{M}} \left(x_i^s \right)\}_{i=1}^N$.
%
However, Fig.~\ref{figs:baseline_tsne} shows that \textbf{the boundary learned through this implicit training is imprecise}, with some boundary-safe representations $\{\mathcal{R}_{\mathcal{M}} \left(x_i^b \right)\}_{i=1}^N$ mixed with harmful representations rather than being clearly distinguished.
% it is difficult for LLMs to learn an accurate boundary through this implicit training, as shown in .
%
In this way, these boundary-safe representations are mistakenly treated as harmful ones, leading LLMs to refuse the corresponding boundary-safe queries and ultimately reducing usability.
% As a result, LLMs refuse to answer the corresponding boundary-safe queries, leading to a decline in usability.}
% Fig. \ref{figs:repe_move} shows that there exist boundary-safe QA pairs $\{x_i^b\}_{i=1}^N$ whose representations $\{\mathcal{R}_{\mathcal{M}} \left(x_i^b \right)\}_{i=1}^N$ are mixed with $\{\mathcal{R}_{\mathcal{M}} \left(x_i^h \right)\}_{i=1}^N$, making it challenging to define an erasure boundary that exactly separates $\{\mathcal{R}_{\mathcal{M}} \left(x_i^h \right)\}_{i=1}^N$ and $\{\mathcal{R}_{\mathcal{M}} \left(x_i^b \right)\}_{i=1}^N$.
% As a result, partial $\{\mathcal{R}_{\mathcal{M}} \left(x_i^b \right)\}_{i=1}^N$ are included in the erasure region and are unintentionally disrupted, causing LLMs to refuse the corresponding boundary-safe queries.}
% We refer to these part of safe representations as boundary safe representations.
% As a result, these boundary safe representations are divided into the erasure region and are unintentionally disrupted.
% Therefore, LLMs will refused to answer.
%
\begin{figure}[t]
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{Figs/t-sne.pdf}}
\caption{Visualization of the representation distribution after implementing SFT, DPO, GA, and CB. ``Harmful'' and ``boundary-safe'' refer to the representations of harmful and boundary-safe queries along with their corresponding responses, respectively.} 
\label{figs:baseline_tsne}
\end{center}
\vskip -0.45in
\end{figure}
\subsection{Explicit Formulation for Distinction Boundary}
% Optimization Objective of X-Boundary}
\label{sec:loss}
\textbf{We propose X-Boundary to explicitly formulate the distinction boundary between safe and harmful representations.}
% \paragraph{We propose X-Boundary to achieve an erasure boundary exactly divide safe representations and harmful representations.}
%
The key idea is to push harmful representations far away from boundary-safe representations through explicit loss function, such that harmful representations can be effectively and precisely erased without disrupting safe ones.
%
In this way, a balance between defense robustness and LLM usability can be achieved.

% based on CB to achieve win-win outcome between robust defense and minor over-refusal.
% boundary safe的定义
Specifically, we construct a separate set $D_\texttt{s}$ for separating harmful and boundary-safe representations, an erase set $D_\texttt{e}$ to contain harmful knowledge that should be erased, and a retain set $D_\texttt{r}$ for preserving safe knowledge related to the usability of LLMs. 
% To this end, the dataset used in X-boundary can be partitioned into the Erase Set $S_\texttt{e}$, the Retain Set $S_\texttt{r}$, and the Separate Set $S_\texttt{s}$.
%
To this end, $D_\texttt{r}$ includes safe QA pairs $\{x_i^s\}_{i=1}^N$, boundary-safe QA pairs $\{x_i^b\}_{i=1}^N$, and refusal responses to harmful queries $\{x_i^r\}_{i=1}^N$.
% boundary-safe 和 safe的区别
% $x_s, x_b, x_r \in S_\texttt{r}$.
%
$D_\texttt{e}$ consists of harmful QA pairs: $D_\texttt{e} = \{x_i^h\}_{i=1}^N$.
%
$D_\texttt{s}$ contains pairs of $x_b$ and $x_r$: $D_\texttt{s} = \{\left(x_i^b, x_i^r\right)\}_{i=1}^N$.
% To this end, we initially collect a Boundary Set $S_\texttt{boundary}$ containing pairs of boundary safe queries and responses $x_b$.
\begin{figure}[t]
\begin{center}
\centerline{\includegraphics[width=0.9\columnwidth]{Figs/loss_function.pdf}}
% \vskip -0.1in
\caption{Illustration of representation manipulation in X-Boundary for a clear distinction boundary.}
\label{fig:loss_function}
\end{center}
\vskip -0.3in
\end{figure}

\textbf{To explicit formulate a precise distinction boundary,} we propose separate loss $\mathcal{L}_\texttt{s}$ to increase the distance between harmful representations $\{\mathcal{R}_{\mathcal{M}_\theta} \left(x_i^h \right)\}_{i=1}^N$ and boundary-safe representations $\{\mathcal{R}_{\mathcal{M}_\theta} \left(x_i^b \right)\}_{i=1}^N$.
%
Since most $\{\mathcal{R}_{\mathcal{M}_\theta} \left(x_i^h \right)\}_{i=1}^N$ will be remapped to $\{\mathcal{R}_{\mathcal{M}_\theta} \left(x_i^r \right)\}_{i=1}^N$ due to the following erasure operation, we can separate them by directly optimizing $\mathcal{R}_{\mathcal{M}_\theta}\left(x_i^r \right)$ to be orthogonal to $\mathcal{R}_{\mathcal{M}_\theta}\left(x_i^b \right)$ as shown in Fig.~\ref{fig:loss_function}:
\begin{equation}
    \mathcal{L}_\texttt{s} = \frac{1}{\left|S_s\right|}\sum_{i=1}^{\left|S_s\right|}\texttt{ReLU} \left(\texttt{cos}\left(\mathcal{R}_{\mathcal{M}_\theta} \left(x_i^r \right), \mathcal{R}_\mathcal{M} \left(x_i^b \right) \right) \right) 
\end{equation}

\textbf{To establish robust defense against multi-turn attacks,} we utilize erase loss $L_{e}$ to erase the representations of harmful QA pairs in $D_\texttt{e}$.
% 
$L_{e}$ optimizes $\mathcal{R}_{\mathcal{M}_{\theta}} \left(x_i^h \right)$ to be orthogonal to their original representations $\mathcal{R}_{\mathcal{M}_{\texttt{ref}}} \left(x_i^h \right)$ following \cite{circuit_breaker}:
\begin{equation}
    \mathcal{L}_\texttt{e} =\frac{1}{\left|D_e\right|}\sum_{i=1}^{\left|D_e\right|}\texttt{ReLU} \left(\texttt{cos}\left(\mathcal{R}_{\mathcal{M}_{\theta}} \left(x_i^h \right), \mathcal{R}_{\mathcal{M}_{\texttt{ref}}} \left(x_i^h \right)\right) \right) 
\end{equation}
% \mathbb{E}_{x \sim S_{\text{e}}}
where $x_i$ represents a sample in retain set ($x_i \in D_r$), and $\mathcal{R}_{\mathcal{M}_{\theta}}$ and $\mathcal{M}_{\texttt{ref}}$ denote the model under training and the reference model before training, respectively.
% The accompanying losses $\mathcal{L}_\texttt{boundary}$ for the datasets are reroute loss $\mathcal{L}_\texttt{reroute}$, retain loss $\mathcal{L}_\texttt{retain}$, and separate loss $\mathcal{L}_\texttt{separate}$.
% Retain term keeps boundary safe representations close to other safe representations, while term2 pushes harmful representations further away from boundary safe representations.
% $\mathcal{L}_\texttt{reroute}$ optimizes $\mathcal{R}_{\mathcal{M}_{\theta}} \left(x_h \right)$ to be orthogonal to their original representations $\mathcal{R}_{\mathcal{M}_{\texttt{ref}}} \left(x_h \right)$:
%
% The retain term $\mathcal{L}_\texttt{retain}$ maintains $\mathcal{R}_{\mathcal{M}_{\theta}} \left(x_s \right)$ and $\mathcal{R}_{\mathcal{M}_{\theta}} \left(x_b \right)$.

\textbf{To preserve usability of LLMs,} we use retain loss $\mathcal{L}_\texttt{r}$ to maintain safe representations of data points in $S_\texttt{r}$.
%
$\mathcal{L}_\texttt{r}$ minimizes the $\ell_2$ distance between trained representations and their original representations:
% \begin{equation}
%     \mathcal{L}_\texttt{retain} = \left\| \mathcal{R}_{\mathcal{M}_\theta} \left(x_b \right) - \mathcal{R}_\mathcal{M} \left(x_b \right) \right\|_2
% \end{equation}
\begin{equation}
\mathcal{L}_\text{r} = \frac{1}{\left|S_r\right|}\sum_{i=1}^{\left|S_r\right|} \left\| \mathcal{R}_{\mathcal{M}_\theta} \left(x_i \right) - \mathcal{R}_\mathcal{M} \left(x_i \right) \right\|_2
\end{equation}
%
Notably, to maintain the existing refusal mechanism of LLMs, refusal responses $x_r$ to harmful queries are added into $S_\texttt{r}$.
%
Therefore, most $\{\mathcal{R}_\mathcal{M} \left(x_h \right)\}_{i=1}^N$ are finally optimized to refusal representations $\{\mathcal{R}_\mathcal{M} \left(x_r \right)\}\}_{i=1}^N$ under the joint effect of $\mathcal{L}_\texttt{e}$ and $\mathcal{L}_\texttt{r}$.

In summary, the overall loss function is a weighted combination of the three aforementioned loss functions:
\begin{equation}
    \mathcal{L}=c_r\mathcal{L}_r + c_e\mathcal{L}_e + c_s\mathcal{L}_s
\end{equation}
where $c_r$, $c_e$ and $c_s$ are adaptive loss coefficients following \cite{circuit_breaker, adaptive_loss}.
%
With the above optimization objective, X-Boundary can perform fine-grained optimization in the representation space to \textbf{strike a balance between defense robustness and the usability of LLMs}.
%
\begin{algorithm}[tb]
\caption{The optimization process of X-Boundary}
\label{alg:example}
\begin{algorithmic}[1]
    \REQUIRE Original frozen model $\mathcal{M}_{\texttt{ref}}$, model $\mathcal{M}_{\theta}$ with parameters $\theta$ to be optimized, a function $\mathcal{R}$ that extracts representation from a model on a batch of inputs, a erase dataset $\mathcal{D}_e$, a retain dataset $\mathcal{D}_r$, a boundary dataset $D_b$, number of optimization steps $T$, hyperparameters $\alpha$ and~$\beta$, batch size $n$ 
   \begin{spacing}{1.2}
   \FOR{$t=1$ {\bfseries to} $T$}
   % \STATE $c_s = \alpha  (1 - \frac{t}{2T})$, $c_r = \alpha \frac{t}{2T}$ \COMMENT{Example Coefficient Schedule}
   \STATE Sample $\{x_i\}_{i=1}^n \sim \mathcal{D}_r$, $\{x_i^h\}_{i=1}^n \sim \mathcal{D}_e$
   \STATE Sample $\{(x_i^b,x_i^r)\}_{i=1}^n \sim \mathcal{D}_b$ 
   \STATE $c_r=\alpha \frac{t}{\beta}$, $c_e=c_s=\alpha(1-\frac{t}{\beta})$
   \STATE $\mathcal{L}_\text{r} = \frac{1}{n}\sum_{i=1}^{n} \left\| \mathcal{R}_{\mathcal{M}_\theta} \left(x_i \right) - \mathcal{R}_\mathcal{M} \left(x_i \right) \right\|_2$ % \COMMENT{Retain Loss}
   \STATE {\small $\mathcal{L}_\texttt{e} =\frac{1}{n}\sum_{i=1}^{n}\texttt{ReLU} \left(\texttt{cos}\left(\mathcal{R}_{\mathcal{M}_{\theta}} \left(x_i^h \right), \mathcal{R}_{\mathcal{M}_{\texttt{ref}}} \left(x_i^h \right)\right) \right)$} % \COMMENT{Erase Loss}
   \STATE {\small $\mathcal{L}_\texttt{s} = \frac{1}{n}\sum_{i=1}^{n}\texttt{ReLU} \left(\texttt{cos}\left(\mathcal{R}_{\mathcal{M}_\theta} \left(x_i^r \right), \mathcal{R}_{\mathcal{M}_{\texttt{ref}}} \left(x_i^b \right) \right) \right)$} % \COMMENT{Separate Loss}
   \STATE $\mathcal{L}=c_r\mathcal{L}_r + c_e\mathcal{L}_e + c_s\mathcal{L}_s$ % \COMMENT{Loss to be Optimized}
   \STATE Update parameters $\theta$ to minimize $\mathcal{L}$
   \ENDFOR
   \end{spacing}
   % \STATE \textbf{return} the optimized parameters $\theta$
   % \UNTIL{$noChange$ is $true$}
\end{algorithmic}
\end{algorithm}

\subsection{Theoretical Analysis of X-Boundary}
\label{sec:theoretical}
In this subsection, we theoretically analyze the convergence rate of LLM from the perspective of the optimal transport theory \cite{solomon2020k, chuang2021measuring, weed2017sharp}. Specifically, we theoretically prove that X-boundary enables a faster learning speed of feature learning, which is verified in Fig.~\ref{fig:loss_plot}.

\textbf{Preliminaries: optimal transport and $k$-variance.} Wasserstein distance measures the distance between probability distributions on a metric space. Let $\mu$ and $\nu \in \Prob(\sR^d)$ denote two probability measures, the definition of $p$-Wasserstein distance with Euclidean cost function is
\begin{equation}
    \gW_p(\mu, \nu) = \inf_{\pi \in \Pi(\mu, \nu)} \left( \E_{(H,Q) \sim \pi} \|H-Q\|^p\right)^{1/p},
\end{equation}
where $\Pi(\mu, \nu) \subseteq \Prob(\sR^d \times \sR^d)$ represent the set of measure couplings and $\mu$ and $\nu$ denote their marginals, respectively. From the perspective of optimal transport, Wasserstein distances indicate the minimal cost of transforming the distribution $\mu$ to $\nu$. Typically, the Earth Mover distance is equivalent to the 1-Wasserstein distance. 

\begin{definition}[Wasserstein-$1$ $k$-variance]
Given a probability measure $\mu \in \Prob(\sR^d)$ and a parameter $k \in \sN$, the \emph{Wasserstein-$1$ $k$-variance} is given as
\begin{equation}
    \Var_{k}(\mu) =  \E_{S, \tilde{S} \sim \mu^k} \left[ \gW_1(\mu_S, \mu_{\tilde{S}} ) \right],
\end{equation}
where $\mu_S = \frac{1}{k}\sum_{i=1}^k \delta_{x_i}$ for $x_i \overset{\textnormal{i.i.d.}}{\sim} \mu $.
\end{definition}

$k$-variance measures structural properties of distribution beyond variance based on Wasserstein distances \cite{solomon2020k}. We theocratically analyze the learning trend of DNNs' feature representations, which can be measured by the convergence rate of $k$-variance following \cite{weed2017sharp, solomon2020k}.

%Following the Wasserstein-2 $k$-variance analysis in \cite{solomon2020k}, we apply bounds by \citet{weed2017sharp} to demonstrate the fast convergence rate of Wasserstein-1 $k$-variance when the support is clusterable. 

\begin{proposition} (Proven in Appendix~\ref{ap:proof})
\label{prop_cluster}
 If $\phi_\# \mu$ is $(n, \Delta)$-clusterable, then for all $m \leq n(2\Delta)^{-2}$,
 \begin{equation}
     \Var_{m}(\phi_\# \mu) < 48\Delta.
 \end{equation}
 Given a distribution $\mu$, $(n, \Delta)$-clusterable means that $\textnormal{supp}(\mu)$ lies in the union of $n$ balls of radius at most $\Delta$.
\end{proposition}

 Proposition~\ref{prop_cluster} indicates that $\Var_{m}(\phi_\# \mu)$ is bounded by the radius $\Delta$, reflecting the concentration of the feature distribution. In this way, the proposed X-Boundary enables more clustered features (the smaller radius $\Delta$) and a faster learning speed (the smaller $k$-variance $\Var_{m}(\phi_\# \mu)$). 

\textbf{Experimental Verification.} Fig.~\ref{fig:loss_plot} verifies that X-Boundary enables a faster learning speed of the training process. To this end, we fine-tune Llama-3-8B-Instruct and Qwen2.5-7B-Chat following settings in Section~\ref{sec:comparison}. Specifically, we set 0.1 and 0.55 of the training loss as thresholds to judge whether the training process has converged for Llama-3-8B-Instruct and Qwen2.5-7B-Chat, respectively. Based on this, Fig.~\ref{fig:loss_plot} indicates that the proposed X-Boundary accelerates the converging process of 26.47\% and 18.29\% on Llama-3-8B-Instruct and Qwen2.5-7B-Chat, respectively.
%
\begin{figure}[t]
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{Figs/final_loss_plot1.pdf}}
\vskip -0.1in
\caption{The training curves of X-Boundary and without X-Boundary on Llama-3-8B-Instruct and Qwen2.5-7B-Chat.}
\label{fig:loss_plot}
\end{center}
\vskip -0.4in
\end{figure}