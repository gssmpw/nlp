\section{Introduction}

\begin{figure*}[h]
\begin{center}
\centerline{\includegraphics[width=2.0\columnwidth]{Figs/repe_move0.pdf}}
\caption{Illustration of the representation distinction boundary and the trade-off between multi-turn defense performance and over-refusal of existing defense methods and X-Boundary. }
\label{figs:repe_move}
\end{center}
\vskip -0.2in
\end{figure*}

As large language models (LLMs) have demonstrated impressive abilities \cite{gpt4, llama3-instruct, internlm, rh20t_p} and are increasingly deployed in diverse real-world applications \cite{minedreamer, agent_survey}, their security vulnerabilities have raised growing concern. 
One of the most significant security threats is ``jailbreaks'', where deliberately designed prompts are used to elicit harmful responses from LLMs \cite{jailbreak_survey, autodan}.
% 
Although current safety alignment techniques \cite{dpo, decoupled_sft} can defend against various single-turn jailbreaks, multi-turn jailbreaks remain a challenge.
%
Unlike single-turn attacks, multi-turn jailbreaks exploit flexible multi-turn dialogues to bypass the safeguards of LLMs \cite{bju_multi,imposterAI_multi}, making them difficult to detect and defend against.

In this paper, we pioneeringly adapt and comprehensively compare four single-turn defense methods against multi-turn jailbreaks, including supervised fine-tuning (SFT) \cite{actor_attack}, Direct Preference Optimization (DPO) \cite{red_queen}, Gradient Ascent (GA) \cite{safe_unlearning}, and circuit breaking (CB) \cite{circuit_breaker}.
Although these methods can reduce the attack success rate (ASR) of multi-turn jailbreaks, they compromise the usability of LLMs, \ie reducing general capability or leading to severe over-refusal issues.
%
For example, the coding capability relatively reduces about 20\% after SFT and the over-refusal rate exceeds 50\% after GA.
% \footnote{Tested on on Qwen2.5-7B-Chat and Mistral-7B-Instruct-v0.2, respectively. Please see Table~\ref{tab:main_results} for detailed results.}.
%
None of them successfully achieve a balance between defense robustness and usability.
%
Over-refusal means that LLMs may unreasonably reject safe prompts \cite{xstest, oktest, orbench}.
%
For instance, Fig.~\ref{figs:repe_move} shows that LLMs refuse to answer a safe prompt ``What are the harmful effects of meth on the human body?''. 

From the perspective of the internal mechanism of LLMs, establishing a boundary to exactly distinguish harmful and safe feature representations is crucial for achieving the balance between robustness and usability.
% 
This distinction boundary enables harmful representations to be intervened in while safe representations are preserved.
% enables the harmful queries to be rejected and safe queries to be properly responded.}
%
However, existing multi-turn defense methods fail to learn an exact boundary due to the lack of an explicit boundary formulation, as shown in Fig.~\ref{figs:repe_move}.
%
Some boundary-safe queries, \eg ``What are the harmful effects of meth on the human body?'' involve sensitive information, \eg ``meth'', and their representations are close to harmful representations.
%
In this way, these boundary-safe queries are mistakenly treated as harmful and rejected by LLMs.

To strike a balance between defense robustness and usability, we propose an X-Boundary that explicitly formulates the boundary between harmful and safe representations.
% 
Specifically, X-Boundary optimizes the LLM to push harmful representations far away from boundary-safe representations, while keeping trained boundary-safe representations close to their original representations. 
%
In this way, we obtain a precise distinction boundary and these harmful representations are further erased.
%
Experimental results demonstrate that X-Boundary decreases the ASR of multi-turn jailbreaks from 58.5\% to 16.5\% on Llama-3-8B-Instruct, while reducing the over-refusal rate by an average of 20.5\% compared to state-of-the-art (SOTA) method and preserving nearly complete general capability.
%
Additionally, we theoretically analyze the feature learning trend of LLM with X-Boundary from the perspective of optimal transport theory.
%
Theoretical analysis and experimental results indicate that X-Boundary achieves about 22\% improvement in the learning speed.

X-Boundary achieves a win-win outcome with enhanced robustness against multi-turn jailbreaks and minimal decline in usability.
% 
We do not expect X-Boundary can independently address all security threats without collaborating with alignment methods such as SFT and Reinforcement Learning from Human Feedback (RLHF). 
%
Instead, we believe that X-Boundary can provide a more efficient and fine-grained defense for safety-aligned LLMs, ultimately enhancing the prospects of deploying robust AI systems in diverse real-world applications.