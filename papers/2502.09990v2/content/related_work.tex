\section{Related Work}

\paragraph{Multi-turn attack.}
Several studies have explored the safety risks in multi-turn dialogue scenarios~\cite{agent_multi,backdoor_multi}. 
%
For instance, \citet{scaleAI_multi} employs human red teamers to uncover vulnerabilities in LLMs when subjected to multi-turn attacks.
%
\citet{red_queen} crafts 40 multi-turn scenarios in which malicious intent is concealed under the guise of preventing harm.
%
\citet{cosafe_multi}, \citet{bju_multi} and \citet{imposterAI_multi} generate multi-turn jailbreak queries by breaking down the original malicious query into multiple less harmful sub-questions.
%
\citet{actor_attack} captures multi-turn attack clues by modeling a network of semantically linked actors. 
%
\citet{coa} and \citet{crescendo} dynamically adjust the attack query based on the contextual feedback from victim LLMs, gradually steering benign initial queries toward more harmful topics throughout the conversation.
%
In this paper, we evaluate the defense robustness of existing methods and X-Boundary against three types of multi-turn jailbreak attacks: ActorAttack~\cite{actor_attack}, RedQueen~\cite{red_queen}, Crescendo~\cite{crescendo}.
% For a comprehensive evaluation of the robustness of defense methods, we implement three multi-turn jailbreak method to attack ,  

\textbf{Defenses for LLMs.} 
Although defense methods for multi-turn jailbreak attacks are less explored in the literature, some existing approaches have proven effective against various single-turn attacks and have the potential to be adapted for multi-turn scenarios. 
% These methods have the potential to be adapted for multi-turn scenarios.
% alignment with human values~\citep{bai2022training, dpo, ouyang2022training},
These defense methods can be classified into the following categories: training LLMs to refuse harmful queries~\citep{bai2022training, dpo, ouyang2022training, decoupled_sft}, training LLMs to prioritize safe instructions~\citep{lu2024sofa,wallace2024instruction,zhang2023defending}, unlearning and editing harmful knowledge ~\citep{eraser, safe_unlearning, ren2024identifying, cq}, prompt engineering~\citep{xie2023defending, zheng2024prompt}, and implementing input and output guardrails~\citep{ inan2023llama,dubey2024llama} such as jailbreak detection~\citep{hu2024gradient, jain2023baseline} input perturbation~\citep{cao2023defending,robey2023smoothllm,liu2024protecting}. % Specifically, input and output guardrails involve input perturbation~\citep{robey2023smoothllm,cao2023defending,liu2024protecting}, safety decoding~\citep{xu2024safedecoding}, and jailbreak detection~\citep{zhang2024parden,yuan2024rigorllm,phute2023llm,alon2023detecting,jain2023baseline,hu2024gradient}.
%
Several studies ~\citep{li2024wmdp,circuit_breaker,zou2023representation, qian2024towards, zhang2024better} also propose defense methods from the perspective of representation engineering, inspiring us to optimize LLMs in the representation space to strike a balance between defense robustness and LLM usability.

\textbf{Decline in usability caused by defense methods.} % \textbf{Safety-Usability Trade-off}
We assess the impact of defense methods on usability from two aspects: general capability degradation and over-refusal.
%
General capability degradation, commonly known as the ``alignment tax''~\cite{alignment_tax} phenomenon, has garnered widespread attention and has been extensively discussed in technical reports on LLMs~\cite{dubey2024llama,inan2023llama,actor_attack,li2024wmdp,vlsbench}.
%
% \textcolor{brown}{Recent studies discuss the false refusal issue, also referred to as exaggerated safety, over-defensiveness, and overkill.
Over-refusal refers to the unreasonable rejection of safe queries by LLMs~\cite{varshney2023art,zhao2024towards,zou2023representation,arditi2024refusal,cao2024nothing}.
% also referred to as over-defensiveness, exaggerated safety, and overkill.
\citet{bianchi2023safety} discover that excessive safety-tuning makes LLMs refuse entirely safe prompts if they superficially resemble unsafe ones.
% \citet{varshney2023art} find that self-checking-based jailbreak defenses, which prompt the LLM to check its own input and output, significantly increases false refusal rates on some harmless prompt datasets.
%
\citet{xstest}, \citet{oktest}, \citet{orbench}, and \citet{phtest} employ linguistic techniques or automatic pipelines to generate seemingly unsafe prompts for evaluating LLMsâ€™ over-refusal behavior.
%
% \textcolor{blue}{Previous studies have explored several approaches to strike a better trade-off.
% Some other work aims to either mitigate or exacerbate this trade-off. use post safety alignment, 
% For example, \citet{zou2023representation}, \citet{arditi2024refusal}, \citet{stickland2024steering}, and \citet{cao2024nothing} identify the refusal vectors in LLM's representation spaces and steer them to mitigate over-refusal.}
% 
% Nevertheless, whether this trade-off is intrinsic for auto-regressive LLMs is still an open question, and our results show that scaling alone cannot fix it.
% % observe over-generalization where LLMs apply alignment rules when they should not be applied.
% %
% As false refusals gain attention, some work constructs dedicated datasets and conducts a systematic evaluation.
% XStest \citep{xstest} and OKtest \citep{oktest} design specific patterns of false refusal and craft pseudo-harmful prompts manually or with the assistance of LLMs.
% % 
% Given the limited size of these public datasets, \citet{orbench, phtest} develops a pipeline to automatically generate a large-scale false refusal dataset.}
