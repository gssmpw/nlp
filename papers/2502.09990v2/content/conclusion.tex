\section{Conclusion}
%
In this paper, we comprehensively compare existing defense methods in multi-turn attack scenarios and reveal their shortcomings in balancing the robustness of defense and LLM usability. 
%
We analyze this issue from the perspective of LLMs' feature space, and conclude that previous methods fail to learn a precise boundary that distinguishes safe and harmful representations without an explicit formulation.
%
To address this issue, we propose the X-Boundary to push harmful representations away from safe representations through explicit loss functions and obtain a clear distinction boundary. Such distinction boundary enables the consequential removal of harmful representations without disrupting safe ones, thereby achieving a balance between robustness against multi-turn jailbreaks and LLM usability.
% to explicit formulate the distinction boundary to achieve the balance between robustness and usability.
% By doing so, a precise distinction is established, allowing harmful representations to be erased without disrupting safe ones, thus achieving the balance between robustness against multi-turn jailbreaks and the usability of LLMs.
%
%This approach establishes a clear distinction boundary, enabling the removal of harmful representations without disrupting safe ones, thereby achieving a balance between robustness against multi-turn jailbreaks and LLM usability.
%
We think that X-Boundary can offer more efficient and fine-grained defense for LLMs, complementing existing safety alignment techniques and ultimately improving the deployment of robust AI systems in real-world applications.
% \textbf{Limitation and future work.}
