\section{Experiments}
\label{sec:exp}
% We first introduce the implementation details, including datasets and the pipeline with key hyperparameters. 
In this section, we evaluate on multiple text-to-image (T2I) generation and multimodal understanding (MMU) tasks to inspect the efficacy of Show-o Turbo. %, we compare it with Show-o in image generation and multimodal understanding. Some ablation studies are conducted to validate the effectiveness of the designed components.

% \begin{table*}[t]
%     \centering
%     \setlength{\tabcolsep}{2pt} % 调整列间距
%     % 让第一行的标题文字稍微缩小
%     \begin{tabular}{lcc|ccccccc|ccc|c} % 第一列设置为左对齐 (l)
%         \toprule
%         \multicolumn{3}{c|}{\textbf{\small Method}} & \multicolumn{7}{c|}{\textbf{\small GenEval}} & \multirow{2}{*}{\textbf{\small HPS}} & \multirow{2}{*}{\textbf{\small IR}} & \multirow{2}{*}{\textbf{\small CS}} & \multirow{2}{*}{\textbf{\small Time (sec)}} \\
%         \cmidrule(lr){1-10} % 第二道横线只出现在 GenEval 部分下方
%         \textbf{\small Model} & \textbf{\small Steps} & \textbf{\small CFG} & \textbf{\small Avg} & \textbf{\small Two Object} & \textbf{\small Counting} & \textbf{\small Position} & \textbf{\small Colors} & \textbf{\small Single Object} & \textbf{\small Color Attr} &  &  &  &  \\
%         \midrule
%         \multirow{6}{*}{Show-o} & 16 & 10 & 0.59 & 0.69 & 0.48 & 0.17 & 0.86 & 0.98 & 0.38 & 0.25 & 0.74 & 31.0 & 0.44 \\
%          & 8 & 10 & 0.54 & 0.58 & 0.43 & 0.15 & 0.84 & 0.97 & 0.29 & 0.25 & 0.67 & 30.8 & 0.24 \\
%          & 4 & 10 & 0.43 & 0.33 & 0.33 & 0.10 & 0.70 & 0.95 & 0.14 & 0.23 & 0.22 & 30.1 & 0.14 \\
%          & 16 & 5 & 0.59 & 0.69 & 0.48 & 0.17 & 0.86 & 0.98 & 0.38 & 0.25 & 0.64 & 30.9 & 0.44 \\
%          & 8 & 5 & 0.54 & 0.58 & 0.43 & 0.15 & 0.84 & 0.97 & 0.29 & 0.25 & 0.60 & 30.8 & 0.24 \\
%          & 4 & 5 & 0.43 & 0.33 & 0.33 & 0.10 & 0.70 & 0.95 & 0.14 & 0.23 & 0.23 & 30.2 & 0.14 \\
%         \midrule
%         \multirow{2}{*}{Show-o Turbo$^*$} & 4 & 0 & 0.50 & 0.51 & 0.38 & 0.13 & 0.79 & 0.96 & 0.26 & 0.25 & 0.59 & 30.7 & 0.09 \\
%          & 2 & 0 & 0.44 & 0.36 & 0.31 & 0.08 & 0.76 & 0.94 & 0.19 &  0.22 &  0.17 & 30.2 & 0.06 \\
%         \midrule
%         \multirow{2}{*}{Show-o Turbo} & 4 & 0 & 0.52 & 0.66 & 0.30 & 0.10 & 0.80 & 0.96 & 0.31 & 0.25 & 0.71 & 30.9 & 0.09 \\
%          & 2 & 0 & 0.49 & 0.53 & 0.33 & 0.09 & 0.79 & 0.96 & 0.26 & 0.24 & 0.53 & 30.6 & 0.06 \\
%         \bottomrule
%     \end{tabular}
%     \caption{\textbf{Comparison on GenEval and other metrics.} (Show-o Turbo$^*$ is the checkpoint of the first stage.)}%The speed of image generation was measured on a single 4090 GPU. HPS, IR and CS are tested on 800 test prompts from HPD dataset. For both models during sampling, Steps = 4, topk = 200; Steps = 2, topk = 10.
%     \label{tab:merged_method_comparison}
% \end{table*}




% \begin{table*}[t]
%     \centering
%     \setlength{\tabcolsep}{4pt} % 调整列间距
%     \begin{tabular}{lcc|ccccccc|ccc|c}
%         \toprule
%          \multirow{2}{*}{\textbf{\small Model}} & \multirow{2}{*}{\textbf{\small Steps}} & \multirow{2}{*}{\textbf{\small CFG}} & \multicolumn{7}{c|}{\textbf{\small GenEval}} & \multirow{2}{*}{\textbf{\small HPS}} & \multirow{2}{*}{\textbf{\small IR}} & \multirow{2}{*}{\textbf{\small CS}} & \multirow{2}{*}{\textbf{\small Time (sec)}} \\
%         \cmidrule(lr){4-10} % 第二道横线只出现在 GenEval 部分下方
%          &  &  & \textbf{\small AVG} & \textbf{\small TO} & \textbf{\small CT} & \textbf{\small P} & \textbf{\small CL} & \textbf{\small SO} & \textbf{\small CA} &  &  &  &  \\
%         \midrule
%         \multirow{8}{*}{Show-o} & 16 & 10 & 0.591 & 0.692 & 0.478 & 0.165 & 0.859 & 0.978 & 0.378 & 0.254 & 0.739 & 0.310 & 0.44 \\
%          & 8 & 10 & 0.540 & 0.578 & 0.428 & 0.145 & 0.838 & 0.969 & 0.285 & 0.249 & 0.665 & 0.308 & 0.24 \\
%          & 4 & 10 & 0.425 & 0.333 & 0.334 & 0.100 & 0.700 & 0.950 & 0.135 & 0.228 & 0.219 & 0.301 & 0.14 \\
%          & 2 & 10 & 0.206 & 0.046 & 0.140 & 0.033 & 0.330 & 0.678 & 0.010 & 0.169 & -1.257 & 0.254 & 0.08 \\
%          \cmidrule(lr){2-14}
%          & 16 & 5 & 0.571 & 0.631 & 0.469 & 0.155 & 0.846 & 0.994 & 0.333 & 0.253 & 0.642 & 0.309 & 0.44 \\
%          & 8 & 5 & 0.530 & 0.558 & 0.441 & 0.133 & 0.825 & 0.972 & 0.255 & 0.247 & 0.602 & 0.308 & 0.24 \\
%          & 4 & 5 & 0.429 & 0.351 & 0.369 & 0.078 & 0.707 & 0.947 & 0.120 & 0.228 & 0.225 & 0.302 & 0.14 \\
%          & 2 & 5 & 0.229 & 0.068 & 0.122 & 0.023 & 0.378 & 0.763 & 0.020 &  0.182 & -0.917 & 0.263 & 0.08 \\
%         \midrule
%         \multirow{4}{*}{Show-o Turbo$^*$} & 16 & 0 & 0.543 & 0.593 & 0.447 & 0.130 & 0.814 & 0.953 & 0.323 & 0.251 & 0.586 & 0.307 & 0.27 \\
%         & 8 & 0 & 0.518 & 0.518 & 0.400 & 0.123 & 0.809 & 0.972 & 0.285 & 0.250 & 0.597 & 0.307 & 0.15 \\
%         & 4 & 0 & 0.504 & 0.513 & 0.375 & 0.130 & 0.787 & 0.962 & 0.257 & 0.245 & 0.586 & 0.307 & 0.09 \\
%          & 2 & 0 & 0.439 & 0.358 & 0.313 & 0.075 & 0.755 & 0.941 & 0.193 &  0.224 &  0.174 & 0.302 & 0.06 \\
%         \midrule
%         \multirow{4}{*}{Show-o Turbo} & 16 & 0 & 0.562 & 0.689 & 0.366 & 0.140 & 0.814 & 0.991 & 0.373 & 0.258 & 0.752 & 0.310 & 0.27 \\
%         & 8 & 0 & 0.552 & 0.669 & 0.353 & 0.128 & 0.817 & 0.963 & 0.385 &  0.255 & 0.738 & 0.309 & 0.15 \\
%         & 4 & 0 & 0.523 & 0.664 & 0.303 & 0.103 & 0.801 & 0.959 & 0.308 & 0.252 & 0.706 & 0.309 & 0.09 \\
%          & 2 & 0 & 0.494 & 0.530 & 0.334 & 0.093 & 0.787 & 0.959 & 0.260 & 0.240 & 0.529 & 0.306 & 0.06 \\
%         \bottomrule
%     \end{tabular}
%     \caption{\textbf{Comparison of results of different models on GenEval and other metrics.} 
%     Show-o Turbo$^*$ refers to the checkpoint undergoing only the first stage of training. 
%     AVG: average, TO: Two Object, CT: Counting, P: Position, CL: colors, SO: Single Object, CLA: Color Attr. }%The speed of image generation was measured on a single 4090 GPU. HPS, IR and CS are tested on 800 test prompts from HPD dataset. For both models during sampling, Steps = 4, topk = 200; Steps = 2, topk = 10.
%     \label{tab:merged_method_comparison}
% \end{table*}




\begin{table*}[htbp]
    \centering
    \setlength{\tabcolsep}{3pt}
    \begin{threeparttable}
        \begin{tabular}{lc|c|ccc|cc}
            \toprule
            \textbf{Method} & \textbf{Decoding}  & \textbf{Speed (tokens/s) \large$\uparrow$} & \textbf{POPE \large$\uparrow$} & \textbf{MME \large$\uparrow$}  & \textbf{MMMU \large$\uparrow$} & \textbf{Flickr30K \large$\uparrow$}  &\textbf{NoCaps \large$\uparrow$} \\
            \midrule
            \multirow{2}{*}{Show-o} & AR &  40.3 & \textbf{83.2} & \textbf{1042.5}  & 24.6 & \textbf{26.6} & \textbf{38.9} \\
            & Jacobi &  36.9 & \textbf{83.2} & \textbf{1042.5}  & 24.6 & \textbf{26.6} & \textbf{38.9} \\
            \midrule
            Show-o Turbo$^{*}$ & Jacobi & 49.93 & 81.8 & 1003.6 & 25.4 & 20.3 & 29.6 \\
            Show-o Turbo & Jacobi & \textbf{61.1} & 78.4 & 865.8 & \textbf{26.3} & 20.4 & 30.3 \\
            \bottomrule
        \end{tabular}
    \end{threeparttable}%
    \caption{\textbf{Comparison of 512 $\times$ 512 MMU performance on multiple benchmarks.}
    Note that Flickr30K and NoCaps evaluate the ability of image description, and POPE, MME, and MMMU measure question-answering ability.}
    \label{tab:speed_comparison}
\end{table*}

\begin{figure*}[htbp]
    \centering
    \includegraphics[width=\textwidth]{graphs2/mmu.pdf}
    \caption{\small \textbf{
    % Intuitive display of 
     The text sampling trajectory of Show-o Turbo in MMU cases.} Show-o Turbo realizes acceleration by predicting multiple successive tokens in one iteration and correctly guessing the later tokens.}
    \label{fig:pdf_two_columns}
\end{figure*}



% \begin{table*}[t]
%     \centering
%     \scalebox{0.75}{!}{%
%     \begin{tabular}{cccccccccc}
%         \toprule
%         \textbf{Method/GenEval} & \textbf{Steps} & \textbf{CFG} & \textbf{All} & \textbf{Two Object} & \textbf{Counting} & \textbf{Position} & \textbf{Colors} & \textbf{Single Object} & \textbf{Color\_attr} \\
%         \midrule
%         \multirow{3}{*}{Show-o} & 16 & 10 & 0.591 & 0.692 & 0.478 & 0.165 & 0.859 & 0.978 & 0.378\\
%          & 8 & 10 & 0.540 & 0.578 & 0.428 & 0.145 & 0.838 & 0.969 & 0.285\\
%          & 4 & 10 & 0.425 & 0.333 & 0.334 & 0.100 & 0.700 & 0.950 & 0.135 \\
%         \midrule
%         \multirow{2}{*}{Show-o Turbo$^*$} & 4 & 0 & 0.504 & 0.513 & 0.375 & 0.130 & 0.787 & 0.962 & 0.257 \\
%          & 2 & 0 & 0.439 & 0.358 & 0.313 & 0.075 & 0.755 & 0.941 & 0.193 \\
%          \midrule
%         \multirow{2}{*}{Show-o Turbo} & 4 & 0 & 0.523 & 0.664 & 0.303 & 0.103 & 0.801 & 0.959 & 0.308\\
%          & 2 & 0 & 0.494 & 0.530 & 0.334 & 0.093 & 0.787 & 0.959 & 0.260 \\
%         \bottomrule
%     \end{tabular}%
%     }
%     \caption{Comparison of results of different models on GenEval metrics. Show-o Turbo$^*$ is the checkpoint of the first training phase, while Show-o Turbo represents the final model. For both models during sampling, Steps = 4, topk = 200; Steps = 2, topk = 10.}
%     \label{tab:method_comparison}
% \end{table*}

% \begin{table}[b]
%     \centering
%     \scalebox{0.75}{!}{%
%     \begin{threeparttable}
%         \begin{tabular}{ccccccc}
%             \toprule
%             \textbf{Method} & \textbf{Steps} & \textbf{CFG} & \textbf{HPS} & \textbf{IR} & \textbf{CS} & \textbf{Time (sec)} \\
%             \midrule
%             \multirow{6}{*}{Show-o} & 16 & 10 & 0.254 & 0.739 & 31.02 & 0.44 \\
%             & 8 & 10 & 0.249 & 0.665 & 30.84 & 0.24 \\
%             & 4 & 10 & 0.228 & 0.219 & 30.07 & 0.14 \\
%             & 16 & 5 & 0.253 & 0.642 & 30.90 & 0.44 \\
%             & 8 & 5 & 0.247 & 0.602 & 30.81 & 0.24 \\
%             & 4 & 5 & 0.228 & 0.225 & 30.17 & 0.14 \\
%             \midrule
%             \multirow{2}{*}{Show-o Turbo$^{*}$} & 4 & 0 & 0.245 & 0.586 & 30.71 & 0.09 \\
%             \cmidrule(lr){2-7}
%             \multirow{4}{*}{Show-o Turbo} 
%             & 16 & 0 & 0.258 & 0.778 & 30.94 & 0.27 \\
%             & 8 & 0 & 0.254 & 0.708 & 30.89 & 0.15 \\
%             & 4 & 0 & 0.252 & 0.706 & 30.88 & 0.09 \\
%             & 2 & 0 & 0.240 & 0.529 & 30.59 & 0.06 \\
%             \bottomrule
%         \end{tabular}
%     \end{threeparttable}%
%     }
%     \caption{Comparison of results on HPD test prompts.All three scores in this table are based on 800 test prompts from HPD. Show-o Turbo$^*$ indicates the checkpoint of first traing stage. Our show-o Turbo method does not require CFG, uses topk = 10 for 2-step sampling, and uses topk = 200 for 4-step sampling.}
%     \label{tab:hpd_comparison}
% \end{table}

\subsection{Implementation Details}
\noindent \textbf{Datasets.}
% During the training process, three types of data are needed in total. 
We leverage three types of data for the training of Show-o Turbo: 
the captions in the train split of COCO 2017~\cite{lin2014microsoft} and the LLaVA instruction tuning dataset~\cite{liu2024visual} are used to generate the image and text trajectories respectively; the RefinedWeb text dataset~\cite{penedo2023refinedweb} is used to maintain the language modeling ability. 

\noindent \textbf{Training Details.} 
We separate the training process into two stages.
% with the student model initialized using the parameters of the teacher model in each stage.
% In the first stage, we get the multimodal trajectories from the original Show-o as a teacher model with CFG=10 and the both image and text generation trajectory length are 16.   
For 256 resolution, in the first stage, we get image trajectories from the original Show-o with a CFG scale of 10 and $K=16$.
We split each trajectory into 4 segments to train the student model, denoted as Show-o Turbo$^*$. 
% In the second stage, we use the checkpoint from the first stage as the teacher model.
In the second stage, we initialize the teacher and student model using Show-o Turbo$^*$. We sample image trajectories with a CFG scale of 1.5, $K=8$, and the number of segments as 2.
% The sampling setting of the text generation remains the same, except for splitting the trajectories into 2 segments for both tasks.
The text trajectories are collected similarly.
% There are some other important details in the training process not about the stages. 
We employ Jacobi decoding to iteratively produce 16 tokens in each round to finally form lengthy text, which proves to yield good acceleration performance while preserving the generative modeling capabilities~\cite{kou2024cllms}. 
% while the trajectory length in  decrease to 8.
% In order to balance the $\mathcal{L}_{c}^u$ and $\mathcal{L}_{c}^v$, and ensure the constraint effect of $\mathcal{L}_{REG}^u$, we set $\alpha$ to 10 and $\beta$ to 20 based on a lot of attempts.
In terms of loss coefficients, we set $\alpha=10$ according to the relative values of the losses, set $\beta=20$ and $\gamma=100$ according to the ablation study in Table~\ref{tab:combined_comparison_extended}, and set $\delta=2$ following ~\cite{xie2024show}.
% we set $\alpha$ to 10 and $\beta$ to 20 according to the ablation study in Table~\ref{}. 
% Following experience in~\cite{kou2024cllms,xie2024show}, we set $\gamma$ to 100 and $\delta$ to 2, respectively. 
For 512 resolution, we set $K$ to 32, CFG scale to 15 and number of segments to 8 in the first stage, and set $K$ to 16, CFG scale to 1.75 and number of segments to 4 in the second stage. 
Besides, we set $\alpha=10$, $\beta=40$, $\gamma=200$ and $\delta=8$.
We follow the original Show-o regarding the mask schedule.
% And we set $\gamma$ to 100, referring to the experience of text regularization weights in CLLM~\cite{kou2024cllms}, and set $\delta$ to 2, referring to the weight setting of plain text in Show-o~\cite{xie2024show}. 
% During the training, all image trajectories are generated at a resolution of 256. 
We use an AdamW optimizer and 8 RTX 4090 GPUs to train each stage for 18 hours, with a constant learning rate of $10^{-5}$.


% We demonstrate the effectiveness of trajectory segmentation and curriculum learning through a series of experiments.
%\noindent \textbf{Evaluation Details.} We choose to distill the Show-o Turbo from the 256-resolution version of show-o. 
%For MMU task training, the maximum length of each Jacobi decoding is set to 16, the maximum length of the entire generated sequence is 512, and the number of segments of the Jacobi trajectory is set to 4 in the first stage and 2 in the second stage. 
%For T2I task, in the first stage, we use show-o with CFG = 10 for 16-step sampling, and the trajectory is divided into 4 segments. 
%In the second stage, we use the checkpoint of the first stage with CFG = 1.5 for 8-step sampling, and the trajectory is divided into 2 segments. 
%The entire training process was conducted using the AdamW optimizer on 6 NVIDIA 4090 GPUs, with more than 20,000 steps of full parameter training performed at a learning rate of 1e-5 in each stage.
\subsection{Main Results}
\noindent \textbf{Benchmarks.} 
% We evaluate Show-o Turbo on two tasks: image generation and multimodal understanding. 
For {T2I generation}, we evaluate the generated images with Human Preference Score v2 (HPS)~\cite{wu2023human}, ImageReward (IR)~\cite{xu2023imagerewardlearningevaluatinghuman}, and CLIP Score (CS)~\cite{hessel2022clipscorereferencefreeevaluationmetric} metrics, based on test prompts in Human Preference Dataset v2 (HPD)~\cite{wu2023human}. 
Additionally, following Show-o, we evaluate also Show-o Turbo on GenEval~\cite{ghosh2023genevalobjectfocusedframeworkevaluating}. 
For {MMU}, we evaluate Show-o Turbo on the description benchmarks Flickr30K~\cite{flickrentitiesijcv,flickr30k} and  NoCaps~\cite{agrawal2019nocaps} measured by the $bleu_4$ score, 
and calculate the accuracy on question answering benchmarks POPE~\cite{li2023evaluating}, MMEMME~\cite{fu2023mme}, and MMMU~\cite{yue2024mmmu}. 
% Results can be found in Table~\ref{tab:merged_method_comparison} and Table~\ref{tab:speed_comparison}.

\noindent \textbf{Baselines.} 
For {T2I generation}, we compare Show-o Turbo without CFG to Show-o with CFG across various sampling steps to fully demonstrate the effectiveness of our method. 
We consider a CFG scale of 5 and 10 for Show-o following \cite{xie2024show}. 
% (Table~\ref{tab:merged_method_comparison}). 
For {MMU}, we compare Show-o Turbo with the original Show-o in terms of both inference speed and accuracy, where the speed is measured on a single RTX 4090 GPU.

% (Table~\ref{tab:speed_comparison}).
\begin{figure*}[htbp]
    \centering
    \begin{tabular}{cccccccc}
        \multicolumn{4}{c}{\makebox[0.5\textwidth][c]{\textbf{Show-o (CFG=10)}}} & \multicolumn{4}{c}{\makebox[0.5\textwidth][c]{\textbf{Show-o Turbo}}} \vspace{2ex}\\
        % 第一行图片及步骤信息
        \begin{minipage}{0.12\linewidth} 
            \centering
            \textbf{16 Steps} \\
            \includegraphics[width=\linewidth]{showo512-2/o16/test_lmcm_x_photo_0.png}
        \end{minipage} 
        \hspace{-0.15cm} % 这个减小了图片之间的间隙
        \begin{minipage}{0.12\linewidth} 
            \centering
            \textbf{8 Steps} \\
            \includegraphics[width=\linewidth]{showo512-2/o8/test_lmcm_x_photo_0.png}
        \end{minipage} 
        \hspace{-0.15cm}
        \begin{minipage}{0.12\linewidth} 
            \centering
            \textbf{4 Steps} \\
            \includegraphics[width=\linewidth]{showo512-2/o4/test_lmcm_x_photo_0.png}
        \end{minipage} 
        \hspace{-0.15cm}
        \begin{minipage}{0.12\linewidth} 
            \centering
            \textbf{2 Steps} \\
            \includegraphics[width=\linewidth]{showo512-2/o2/test_lmcm_x_photo_0.png}
        \end{minipage} 
        \hspace{0cm}
        \begin{minipage}{0.12\linewidth} 
            \centering
            \textbf{16 Steps} \\
            \includegraphics[width=\linewidth]{showo512-2/t16/test_lmcm_x_photo_0.png}
        \end{minipage} 
        \hspace{-0.15cm}
        \begin{minipage}{0.12\linewidth} 
            \centering
            \textbf{8 Steps} \\
            \includegraphics[width=\linewidth]{showo512-2/t8/test_lmcm_x_photo_0.png}
        \end{minipage} 
        \hspace{-0.15cm}
        \begin{minipage}{0.12\linewidth} 
            \centering
            \textbf{4 Steps} \\
            \includegraphics[width=\linewidth]{showo512-2/t4/test_lmcm_x_photo_0.png}
        \end{minipage} 
        \hspace{-0.15cm}
        \begin{minipage}{0.12\linewidth} 
            \centering
            \textbf{2 Steps} \\
            \includegraphics[width=\linewidth]{showo512-2/t2/test_lmcm_x_photo_0.png}
        \end{minipage} \\

        \multicolumn{8}{c}{\small \textit{A cybernetic owl perched on a neon-lit branch, its mechanical feathers reflecting holographic patterns...}} \\

        \vspace{0.1cm}

        

        \begin{minipage}{0.12\linewidth} 
            \centering
            \includegraphics[width=\linewidth]{showo512-2/o16/test_lmcm_x_photo_1.png}
        \end{minipage} 
        \hspace{-0.15cm} % 这个减小了图片之间的间隙
        \begin{minipage}{0.12\linewidth} 
            \centering
            \includegraphics[width=\linewidth]{showo512-2/o8/test_lmcm_x_photo_1.png}
        \end{minipage} 
        \hspace{-0.15cm}
        \begin{minipage}{0.12\linewidth} 
            \centering
            \includegraphics[width=\linewidth]{showo512-2/o4/test_lmcm_x_photo_1.png}
        \end{minipage} 
        \hspace{-0.15cm}
        \begin{minipage}{0.12\linewidth} 
            \centering
            \includegraphics[width=\linewidth]{showo512-2/o2/test_lmcm_x_photo_1.png}
        \end{minipage} 
        \hspace{0cm}
        \begin{minipage}{0.12\linewidth} 
            \centering
            \includegraphics[width=\linewidth]{showo512-2/t16/test_lmcm_x_photo_1.png}
        \end{minipage} 
        \hspace{-0.15cm}
        \begin{minipage}{0.12\linewidth} 
            \centering
            \includegraphics[width=\linewidth]{showo512-2/t8/test_lmcm_x_photo_1.png}
        \end{minipage} 
        \hspace{-0.15cm}
        \begin{minipage}{0.12\linewidth} 
            \centering
            \includegraphics[width=\linewidth]{showo512-2/t4/test_lmcm_x_photo_1.png}
        \end{minipage} 
        \hspace{-0.15cm}
        \begin{minipage}{0.12\linewidth} 
            \centering
            \includegraphics[width=\linewidth]{showo512-2/t2/test_lmcm_x_photo_1.png}
        \end{minipage} \\

        \multicolumn{8}{c}{\small \textit{A modern electric guitar with a flame maple top, its wood grain catching studio lights...}} \\
        \vspace{0.1cm}



        \begin{minipage}{0.12\linewidth} 
            \centering
            \includegraphics[width=\linewidth]{showo512-2/o16/test_lmcm_x_photo_3.png}
        \end{minipage} 
        \hspace{-0.15cm} % 这个减小了图片之间的间隙
        \begin{minipage}{0.12\linewidth} 
            \centering
            \includegraphics[width=\linewidth]{showo512-2/o8/test_lmcm_x_photo_3.png}
        \end{minipage} 
        \hspace{-0.15cm}
        \begin{minipage}{0.12\linewidth} 
            \centering
            \includegraphics[width=\linewidth]{showo512-2/o4/test_lmcm_x_photo_3.png}
        \end{minipage} 
        \hspace{-0.15cm}
        \begin{minipage}{0.12\linewidth} 
            \centering
            \includegraphics[width=\linewidth]{showo512-2/o2/test_lmcm_x_photo_3.png}
        \end{minipage} 
        \hspace{0cm}
        \begin{minipage}{0.12\linewidth} 
            \centering
            \includegraphics[width=\linewidth]{showo512-2/t16/test_lmcm_x_photo_3.png}
        \end{minipage} 
        \hspace{-0.15cm}
        \begin{minipage}{0.12\linewidth} 
            \centering
            \includegraphics[width=\linewidth]{showo512-2/t8/test_lmcm_x_photo_3.png}
        \end{minipage} 
        \hspace{-0.15cm}
        \begin{minipage}{0.12\linewidth} 
            \centering
            \includegraphics[width=\linewidth]{showo512-2/t4/test_lmcm_x_photo_3.png}
        \end{minipage} 
        \hspace{-0.15cm}
        \begin{minipage}{0.12\linewidth} 
            \centering
            \includegraphics[width=\linewidth]{showo512-2/t2/test_lmcm_x_photo_3.png}
        \end{minipage} \\

        \multicolumn{8}{c}{\small \textit{A small succulent plant in a ceramic pot, its leaves forming a perfect geometric pattern...}} \\
        \vspace{0.1cm}





        \begin{minipage}{0.12\linewidth} 
            \centering
            \includegraphics[width=\linewidth]{showo512-2/o16/test_lmcm_x_photo_5.png}
        \end{minipage} 
        \hspace{-0.15cm} % 这个减小了图片之间的间隙
        \begin{minipage}{0.12\linewidth} 
            \centering
            \includegraphics[width=\linewidth]{showo512-2/o8/test_lmcm_x_photo_5.png}
        \end{minipage} 
        \hspace{-0.15cm}
        \begin{minipage}{0.12\linewidth} 
            \centering
            \includegraphics[width=\linewidth]{showo512-2/o4/test_lmcm_x_photo_5.png}
        \end{minipage} 
        \hspace{-0.15cm}
        \begin{minipage}{0.12\linewidth} 
            \centering
            \includegraphics[width=\linewidth]{showo512-2/o2/test_lmcm_x_photo_5.png}
        \end{minipage} 
        \hspace{0cm}
        \begin{minipage}{0.12\linewidth} 
            \centering
            \includegraphics[width=\linewidth]{showo512-2/t16/test_lmcm_x_photo_5.png}
        \end{minipage} 
        \hspace{-0.15cm}
        \begin{minipage}{0.12\linewidth} 
            \centering
            \includegraphics[width=\linewidth]{showo512-2/t8/test_lmcm_x_photo_5.png}
        \end{minipage} 
        \hspace{-0.15cm}
        \begin{minipage}{0.12\linewidth} 
            \centering
            \includegraphics[width=\linewidth]{showo512-2/t4/test_lmcm_x_photo_5.png}
        \end{minipage} 
        \hspace{-0.15cm}
        \begin{minipage}{0.12\linewidth} 
            \centering
            \includegraphics[width=\linewidth]{showo512-2/t2/test_lmcm_x_photo_5.png}
        \end{minipage} \\

        \multicolumn{8}{c}{\small \textit{A traditional wooden chess piece on a marble board, its polished surface reflecting soft light...}} \\
        \vspace{0.1cm}

        

        \begin{minipage}{0.12\linewidth} 
            \centering
            \includegraphics[width=\linewidth]{showo512-2/o16/test_lmcm_x_photo_10.png}
        \end{minipage} 
        \hspace{-0.15cm} % 这个减小了图片之间的间隙
        \begin{minipage}{0.12\linewidth} 
            \centering
            \includegraphics[width=\linewidth]{showo512-2/o8/test_lmcm_x_photo_10.png}
        \end{minipage} 
        \hspace{-0.15cm}
        \begin{minipage}{0.12\linewidth} 
            \centering
            \includegraphics[width=\linewidth]{showo512-2/o4/test_lmcm_x_photo_10.png}
        \end{minipage} 
        \hspace{-0.15cm}
        \begin{minipage}{0.12\linewidth} 
            \centering
            \includegraphics[width=\linewidth]{showo512-2/o2/test_lmcm_x_photo_10.png}
        \end{minipage} 
        \hspace{0cm}
        \begin{minipage}{0.12\linewidth} 
            \centering
            \includegraphics[width=\linewidth]{showo512-2/t16/test_lmcm_x_photo_10.png}
        \end{minipage} 
        \hspace{-0.15cm}
        \begin{minipage}{0.12\linewidth} 
            \centering
            \includegraphics[width=\linewidth]{showo512-2/t8/test_lmcm_x_photo_10.png}
        \end{minipage} 
        \hspace{-0.15cm}
        \begin{minipage}{0.12\linewidth} 
            \centering
            \includegraphics[width=\linewidth]{showo512-2/t4/test_lmcm_x_photo_10.png}
        \end{minipage} 
        \hspace{-0.15cm}
        \begin{minipage}{0.12\linewidth} 
            \centering
            \includegraphics[width=\linewidth]{showo512-2/t2/test_lmcm_x_photo_10.png}
        \end{minipage} \\

        \multicolumn{8}{c}{\small \textit{A detailed macro shot of a dragonfly perched on a thin blade of grass, its wings iridescent in the sunlight...}} \\
        

        \vspace{0.1cm}

        


        \begin{minipage}{0.12\linewidth} 
            \centering
            \includegraphics[width=\linewidth]{showo512-2/o16/test_lmcm_x_photo_14.png}
        \end{minipage} 
        \hspace{-0.15cm} % 这个减小了图片之间的间隙
        \begin{minipage}{0.12\linewidth} 
            \centering
            \includegraphics[width=\linewidth]{showo512-2/o8/test_lmcm_x_photo_14.png}
        \end{minipage} 
        \hspace{-0.15cm}
        \begin{minipage}{0.12\linewidth} 
            \centering
            \includegraphics[width=\linewidth]{showo512-2/o4/test_lmcm_x_photo_14.png}
        \end{minipage} 
        \hspace{-0.15cm}
        \begin{minipage}{0.12\linewidth} 
            \centering
            \includegraphics[width=\linewidth]{showo512-2/o2/test_lmcm_x_photo_14.png}
        \end{minipage} 
        \hspace{0cm}
        \begin{minipage}{0.12\linewidth} 
            \centering
            \includegraphics[width=\linewidth]{showo512-2/t16/test_lmcm_x_photo_14.png}
        \end{minipage} 
        \hspace{-0.15cm}
        \begin{minipage}{0.12\linewidth} 
            \centering
            \includegraphics[width=\linewidth]{showo512-2/t8/test_lmcm_x_photo_14.png}
        \end{minipage} 
        \hspace{-0.15cm}
        \begin{minipage}{0.12\linewidth} 
            \centering
            \includegraphics[width=\linewidth]{showo512-2/t4/test_lmcm_x_photo_14.png}
        \end{minipage} 
        \hspace{-0.15cm}
        \begin{minipage}{0.12\linewidth} 
            \centering
            \includegraphics[width=\linewidth]{showo512-2/t2/test_lmcm_x_photo_14.png}
        \end{minipage} \\

        \multicolumn{8}{c}{\small \textit{A single, colorful autumn leaf floating on the surface of a calm pond...}} 
        % \\


        
    \end{tabular}

    % \vspace{0.1cm}
    \caption{\textbf{Comparison between Show-o and Show-o Turbo on 512 resolution in T2I generation.} The former crashes in two-step sampling, while the latter maintains good performance.} 
    \label{fig:t2i_comparison}
\end{figure*}

\noindent \textbf{Quantitative Results.} 
Table~\ref{tab:merged_method_comparison} displays the results for T2I generation.
We observe that in 2-8 step sampling, Show-o Turbo comprehensively outperforms Show-o, even without using CFG, particularly at 2 and 4 steps. 
In 16-step sampling, Show-o Turbo is comparable to Show-o on GenEval, 
% and surpasses it on HPS, IR, and CS.
HPS, IR, and CS. 
Moreover, the 4-step sampling of Show-o Turbo without CFG 
% is comparable to 
outperforms 
the 8-step sampling of Show-o, and the results of the 2-step Show-o Turbo fall between the 4-step and 8-step Show-o with CFG, highlighting the effectiveness of our method in acceleration. 
Besides, we can observe that Show-o Turbo outperforms Show-o Turbo$^{*}$, demonstrating the efficacy of curriculum learning. 
% In addition, the results of Show-o without CFG and Show-o Turbo with CFG are provided in the supplementary material.
% Additionally, we test the 16-step Show-o without CFG and observe that its IR drops to $-0.916$, reflecting the reliance of Show-o on CFG. % showing a significant decline compared to the data in the table. 
% We also test the 4-step sampling Show-o Turbo with CFG with a scale of 1 and find that its IR improves to $0.731$, showing that CFG can further enhance Show-o Turbo.
Additionally, we demonstrate the reliance of Show-o on CFG, and find that CFG can further enhance Show-o Turbo, which is shown in Appendix~\ref{sec:cfg_appendix}.


Table~\ref{tab:speed_comparison} shows the performance of Show-o Turbo in MMU tasks.
We evaluate the text tokens generation speed on NoCaps, witnessing a 1.5x speedup on average. 
Besides, we notice that Show-o Turbo achieves comparable description performance to Show-o on Flickr30K and NoCaps.
The slight performance drop also implies a trade-off between the performance and acceleration effect on these tasks. 
Performing distillation with more advanced MMU corpora can be a possible remedy to this. 
% This may result from omitting certain information during the distillation process, presenting an interesting direction for future exploration.
On the other hand, we find that Show-o Turbo maintains strong performance on question-answering benchmarks POPE, MME, and MMMU which rely on one-token responses. 

\noindent \textbf{Qualitative Results.} 
Figure~\ref{fig:t2i_comparison} provides a visual comparison between Show-o with a CFG scale of 10 and Show-o Turbo without CFG across various sampling steps on 512 resolution. 
It can be observed that the images generated by Show-o with 2 steps are collapsed, while our model addresses this issue. 
More results of our Show-o Turbo are provided in Figure~\ref{fig:show}. 
These studies prove that Show-o Turbo has the ability to perform effective sampling with fewer steps. 

Figure~\ref{fig:pdf_two_columns} visualizes the text sampling trajectory of Show-o Turbo for several MMU cases.
As shown, Show-o Turbo can complete the prediction of 16 tokens in fewer than 10 iterations, due to the ability to predict multiple successive tokens in one iteration and correctly guess the later tokens. 
% , which a clear demonstration of Show-o Turbo’s efficiency in text generation. 
% In these two examples, Show-o Turbo completes the prediction of 16 tokens in fewer than 8 iterations. 

We also showcase the performance of Show-o Turbo in image inpainting and extrapolation in Appendix~\ref{sec:Inpainting}.
Show-o Turbo can effectively complete both tasks in just four steps without requiring additional fine-tuning.



% \noindent \textbf{Results.} 
% We include comparison examples for visualization in Figure~\ref{fig:pdf_two_columns} and 
% Figure~\ref{fig:t2i_comparison}, demonstrating the performance of the Show-o Turbo model in image understanding and generation tasks,
% showing our model significantly improves speed and quality compared to Show-o in fewer steps sampling. 
% Table~\ref{tab:merged_method_comparison} displays excellent results on image generation.
% We observe that with the same number of sampling steps, our Turbo version is comparable to the original version on GenEval and significantly surpasses it in HPS, IR, and CS, especially in 2-step and 4-step.
% It can be observed that the 2-step image generation has collapsed (see in Figure~\ref{fig:t2i_comparison}, while our model still maintains a high level of performance,
% demonstrating that it enhances the original's ability to perform effective sampling with fewer steps.
% Moreover, the 4-step sampling of the Turbo version is comparable to the 8-step sampling of the original version, highlighting the effectiveness of our method in acceleration.
% Besides, we can observe that Show-o Turbo outperforms Show-o Turbo$^{*}$, which undergoes only the first stage of training, highlighting the effectiveness of curriculum learning.
% % Show-o Turbo consistently performs well on HPS, IR, CS, and GenEval benchmarks, where the comprehensive score of 4-step sampling without CFG is close to that of 16-step sampling of Show-o when cfg=10, and 2-step sampling is between 4-step and 8-step of Show-o.  
% In terms of multimodal understanding, we present results in Table~\ref{tab:speed_comparison}.
% We assess the convergence speed of Jacobi iteration on the NoCaps benchmark, achieving approximately 1.5 times speedup while still maintaining good performance in question-answering benchmarks such as POPE and MME, as well as description benchmarks such as Flickr30K and NoCaps.


\begin{table}[t]
    \centering
        \setlength{\tabcolsep}{2.5pt}
        \begin{tabular}{l|ccc|cc}
            \toprule
            \textbf{\small Settings} & \textbf{\small \#IT \large$\downarrow$} & \textbf{\small POPE \large$\uparrow$} & \textbf{\small MME \large$\uparrow$} & \textbf{\small IR \large$\uparrow$} & \textbf{\small CS \large$\uparrow$} \\
            \midrule
            \multicolumn{6}{c}{\textbf{Number of Segments}} \\
            \midrule
            4 Segments & \textbf{10.57} & 72.6 & \textbf{803.4} & \textbf{0.586} & \textbf{0.307} \\
            2 Segments & 12.48 & 69.8 & 595.8 & 0.500 & 0.306 \\
            1 Segment  & 11.71 & \textbf{74.1} & 675.3 & 0.270 & 0.304 \\
            \midrule
            \multicolumn{6}{c}{\textbf{Full-parameter Tuning vs. LoRA}} \\
            \midrule
            Full-parameter & \textbf{10.57} & 72.6 & 803.4 & \textbf{0.586} & \textbf{0.307} \\
            LoRA & 13.14 & \textbf{78.1} & \textbf{881.2} & 0.472 & 0.304 \\
            \midrule
            \multicolumn{6}{c}{\textbf{Regularization}} \\
            \midrule
            $\beta=0$, $\gamma=0$ & \textbf{2.85} & 0.0 & 4.91 & -2.278 & 0.184 \\
            $\beta=10$, $\gamma=50$ & 12.71 & \textbf{74.8} & 798.4 & 0.483 & \textbf{0.307} \\
            $\beta=20$, $\gamma=100$ & 10.57 & 72.6 & \textbf{803.4} & \textbf{0.586} & \textbf{0.307} \\
            \bottomrule
        \end{tabular}%
    \caption{\textbf{Ablation studies regarding various aspects on 256 resolution.} \#IT represents the number of iterations required by Jacobi decoding to decode 16 tokens. Refer to the text for more details. }
    \label{tab:combined_comparison_extended}
\end{table}




\subsection{Ablation Studies}
To analyze the influence of each part in our method, we conduct a comprehensive ablation study on 256 resolution in this subsection. 
Unless otherwise specified, we report the results of the model after the first training stage (i.e., Show-o Turbo$^*$) and the T2I generation is done with 4 sampling steps. 
%to demonstrate the effectiveness of each component in Show-o Turbo.

\noindent \textbf{Number of Segments.}
As shown in Table~\ref{tab:combined_comparison_extended}, models trained in two segments and without trajectory segmentation (i.e., using one segment) can exhibit a suboptimal performance and a degraded acceleration effect. % on the MMU task is significantly diminished. 
This result reflects the effectiveness of our trajectory segmentation strategy for improving convergence speed and model performance. 



% \begin{table}[h]
%     \centering
%     \scalebox{0.66}{%
%         \begin{tabular}{c|ccc|cc}
%             \toprule
%             \textbf{Segments} & \textbf{Iterations/16 tokens} & \textbf{POPE} & \textbf{MME} & \textbf{IR} & \textbf{CS} \\
%             \midrule
%             4 & 10.57 & 72.6 & 803.4 & 0.59 & 30.7 \\
%             2 & 12.48 & 69.8 & 595.8 & 0.49 & 30.6 \\
%             1 & 11.71 & 74.1 & 675.3 & 0.27 & 30.4 \\
%             \bottomrule
%         \end{tabular}%
%     }
%     \caption{Comparison of results of different numbers of segments. The left side (\textit{Iterations/16 tokens}, \textit{POPE}, \textit{MME}) represents text evaluations, while the right side (\textit{IR}, \textit{CS}) represents image evaluations.}
%     \label{tab:segment_comparison}
% \end{table}

\noindent \textbf{Full-parameter Training.} 
We study the influence of the training module on Show-o Turbo. 
Table~\ref{tab:combined_comparison_extended} shows the performance of full-parameter training and LoRA trianing~\cite{hu2021lora} on several benchmarks. 
Although the LoRA method maintains good performance on the MMU task, its acceleration effect on MMU and T2I tasks is inferior to that of full-parameter training. 
Therefore, we ultimately opt for full parameter training to balance between accuracy and speed. 

\noindent \textbf{Regularization.} 
As shown in Table~\ref{tab:combined_comparison_extended}, training without regularization constraints (i.e., $\beta=0, \gamma=0$) tends to make the model collapse rapidly. 
Besides, smaller regularization weights can lead to inferior performance, highlighting the importance of regularization in constraining the distribution of Show-o Turbo in training.

% \begin{table}[h]
%     \centering
%     \scalebox{0.66}{%
%         \begin{tabular}{cc|ccc|cc}
%             \toprule
%             \textbf{AR Weight} & \textbf{Reg Weight} & \textbf{Iterations/16 tokens} & \textbf{POPE} & \textbf{MME} & \textbf{IR} & \textbf{CS} \\
%             \midrule
%             0 & 0 & 2.85 & 0 & 4.91 & -2.27 & 18.4 \\
%             5 & 10 & 12.71 & 74.8 & 798.4 & 0.48 & 30.6 \\
%             10 & 20 & 10.57 & 72.6 & 803.4 & 0.59 & 30.7 \\
%             \bottomrule
%         \end{tabular}%
%     }
%     \caption{Comparison of results with different loss weights.}
%     \label{tab:reg_weight_comparison}
% \end{table}

\noindent \textbf{Top-k Sampling.} 
Table~\ref{tab:topk_comparison} shows the results with different sampling strategies.
We observe that top-k significantly improves the performance of Show-o Turbo on both 2-step and 4-step sampling.
In contrast, the benefit of top-k to Show-o is minor. 
This is probably because there is higher uncertainty in the output distribution of Show-o Turbo. 
% This enhancement is likely due to the need to predict more low-confidence regions in a single step when using fewer-step sampling, which leads to higher uncertainty in the logits. 
% Top-$k$ sampling helps mitigate this issue.

\begin{table}[t]
    \centering
    \setlength{\tabcolsep}{5pt}
        \begin{tabular}{lccccccc}
            \toprule
            \textbf{Model} & \textbf{Steps} & \textbf{Top-k} & \textbf{HPS \large$\uparrow$} & \textbf{IR \large$\uparrow$} & \textbf{CS \large$\uparrow$} \\
            \midrule
            \multirow{4}{*}{Show-o Turbo} & 4 & - & 0.245 & 0.621 & 0.306 \\
             & 4 & 200 & \textbf{0.252} & \textbf{0.706} & \textbf{0.309} \\
             \cline{2-6}
            & 2 & - & 0.216 & 0.027 & 0.291 \\
             & 2 & 10 & \textbf{0.240} & \textbf{0.529} & \textbf{0.306} \\
            \midrule
            \multirow{4}{*}{Show-o} & 4 & - & 0.228 & 0.219 & 0.301\\
             & 4 & 200 & \textbf{0.230} & \textbf{0.286} & \textbf{0.302} \\
             \cline{2-6}
            & 2 & - & \textbf{0.169} & \textbf{-1.257} & \textbf{0.254} \\
             & 2 & 10 & 0.168 & -1.263 & \textbf{0.254} \\
            \bottomrule
        \end{tabular}%
    \caption{\textbf{Comparison on sampling strategy on 256 resolution.} Top-k sampling is beneficial to Show-o Turbo compared to regular multinomial samples, but the benefits for the original Show-o are minor. 
    % We evaluate the sampling strategy effect on the image quality generated by Show-o and our Show-o Turbo.
    }
    \label{tab:topk_comparison}
\end{table}




% \begin{table}[h]
%     \centering
%     \scalebox{0.66}{%
%         \begin{tabular}{c|ccc|cc}
%             \toprule
%             \textbf{Method} & \textbf{Iterations/16 tokens} & \textbf{POPE} & \textbf{MME} & \textbf{IR} & \textbf{CS} \\
%             \midrule
%             Full Parameters & 10.57 & 72.6 & 803.4 & 0.59 & 30.7 \\
%             LoRA & 13.14 & 78.1 & 881.2 & 0.47 & 30.4 \\
%             \bottomrule
%         \end{tabular}%
%     }
%     \caption{Comparison of different training methods.}
%     \label{tab:lora_comparison}
% \end{table}

% \begin{figure*}[h]
%     \centering
%     \begin{minipage}[t]{0.2\textwidth}
%         \vspace{0pt} % 使图像区域与文字区域顶部对齐
%         \includegraphics[height=4.5cm]{./graphs/mmu.jpg} 
%     \end{minipage}
%     \hfill
%     \begin{minipage}[t]{0.5\textwidth}
%         \vspace{0pt} % 使文字区域与图像区域顶部对齐
%         \footnotesize 
%         \textbf{User:} What scene does this picture show?\\[1ex]
%         \textbf{Show-o:} \\The image shows a man surfing on a large wave in the ocean. He is skillfully riding the wave on his surfboard, enjoying the thrill of the sport. The wave is quite large, and the surfer is positioned near the top of the wave, showcasing his ability to maintain balance and control while riding the wave.\\[2ex]
%         \textbf{Show-o Turbo:} \\The image captures a surfer riding a wave on a surfboard in the ocean. The surfer is skillfully navigating the wave, and the wave is large and powerful, providing a thrilling and dynamic scene. The surfer is positioned towards the right side of the wave, and the wave is breaking towards the left side of the image. The surfer's position and the wave's size and power create a sense of motion and excitement in the scene.\\[2ex]
%     \end{minipage}
%     \caption{Comparative descriptions of an image by different models.}
%     \label{fig:image_description_comparison}
% \end{figure*}



% \subsection{Limitations and Discussion}
% In this section, we examine the limitations of our proposed approach and suggest potential avenues for future enhancements.

% \noindent \textbf{Fewer Steps.} 
% While we have greatly improved the performance of Show-o Turbo with 2-step and 4-step sampling, the result of one-step generation is still unsatisfactory, indicating a worthwhile area for future exploration.

% \noindent \textbf{Plain Text Distillation.} 
% Show-o Turbo does not incorporate distillation with the plain texts of question-answer pairs, as it primarily focuses on image generation and understanding. 
% Additionally, training at images of 256 resolution limits its applicability to text-based tasks. 
% In the future, we will include fine-tuning with plain text instructions and images of higher resolutions.



% \begin{figure*}[htbp]
%     \centering
%     \includegraphics[width=\textwidth]{graphs2/inex.pdf}    
%     \caption{\small \textbf{Inpainting and extrapolation.} From top to bottom, the images are generated by Show-o Turbo in 2, 4, and 8 sampling steps. Show-o Turbo only need 4 steps to achieve good image inpainting and extrapolation without cfg and extra fine-tuning. }
%     \label{fig:inpainting}
% \end{figure*}