\section{Related Work}
\label{sec:related}
% This section introduces the key developments in multimodal large models and acceleration techniques for diffusion models and LLM.

% \begin{figure}[t]
%   \centering
%   \fbox{\rule{0pt}{4in} \rule{0.9\linewidth}{0pt}} % Increased height to 3 inches
%    %\includegraphics[width=0.8\linewidth]{egfigure.eps}

%    \caption{Comparisons of three types of distillation methods.}
%    \label{fig:intro_work_compare}
% \end{figure}
% \subsection{Consistency Distillation for Continuous Diffusion Models}

\noindent \textbf{Multimodal Large Models.}
There has been much effort in exploring multimodal large models for image generation~\cite{rombach2022high,yang2024cogvideox,podellsdxl,sun2024autoregressive} and understanding~\cite{liu2024llava,bai2023qwen,ye2024mplug,zhu2023minigpt}. 
For image generation tasks, 
text-conditioned Diffusion-based models~\cite{rombach2022high,podellsdxl,song2020score,chen2023pixart,chen2024pixart,li2024hunyuan} gradually remove Gaussian noise in latent space~\cite{rombach2022high} to generate images aligning with prompts. 
% In contrast to the aforementioned approach, LlamaGen~\cite{sun2024autoregressive} attempts to generate the image tokens decoded by VQVAE to get the image.
For multimodal understanding tasks, 
LLaVA family~\cite{liu2024llava,lin2024moe,liu2024visual,liu2024improved,li2024llava,zhu2024llava} employs a vision encoder to encode the image, integrating it with a large language model (LLM) architecture to facilitate image-to-text understanding. 
% Although they have displayed strong results in text-to-image or imgae-to-text task, showing the multimodal understanding ability, it is difficult for them to generate the multimodal results.
Recent unified multimodal models~\cite{wu2023next,zhao2024monoformer,chern2024anole,dong2023dreamllm,wu2024janus} have emerged that aim to handle both image and text tasks simultaneously. For example, 
% NExT-GPT~\cite{wunext} leverages different adaptors and decoders for multimodal data. 
Chameleon~\cite{team2023gemini} 
and Emu3~\cite{wang2024emu3} autoregressively predict the next token on both tasks, 
% including the image tokens which can be decoded by corresponding vision decoder and text tokens, 
while Transfusion~\cite{zhou2024transfusion} combines the autoregressive and continuous diffusion generation methods to handle different tasks. Similar to the Transfusion, Show-o~\cite{xie2024show} applies the autoregressive text generation but uses the discrete diffusion methods in image generation process. 
% \noindent \textbf{Discrete Diffusion Model.}
% % In recent years, continuous diffusion models based on Gaussian noise have achieved considerable success in image generation. 
% % However, 
% Recently, discrete diffusion models~\cite{austin2021structured} have emerged as a powerful alternative to traditional continuous diffusion~\cite{salimans2022progressive}, particularly in the realm of image generation.
% % Unlike traditional denoising diffusion models that operate on a continuous latent space, discrete diffusion models use a classification approach to represent data, add Gaussian noise in the continuous latent space, and train the model to predict and remove this noise. 
% While continuous diffusion models operate on the continuous latent space, discrete ones employ a classification approach to represent data and introduce discrete noise on the discrete space, for example, 
% % D3PM introduces a discrete corruption process, replaces Gaussian noise with a label-based classification framework \cite{austin2023structureddenoisingdiffusionmodels}, and proposes theoretical foundations such as random transfer matrices and absorption matrices. 
% D3PM~\cite{austin2021structured} introduces random transfer matrices and absorption matrices for discrete corruption process and replaces Gaussian noise with a label-based classification framework~\cite{austin2021structured}. 
% Pioneering works such as Mask-Predict~\cite{ghazvininejad2019mask}, MaskGIT~\cite{chang2022maskgit}, ARDM~\cite{hoogeboom2021autoregressive} and UniD3~\cite{hu2022unified} have fully demonstrated the feasibility of discrete models. 
% Drawing inspiration from them, Show-o~\cite{xie2024show} adopts a similar approach to produce efficient and high-quality multimodal outputs.

% \subsection{Consistency Distillation Algorithm}

% \subsubsection{Consistency Distillation for diffusion models}

% \noindent \textbf{Latent Consistency Model (LCM)} 
% \noindent \textbf{Consistency Distillation for Diffusion Models.}
\noindent \textbf{Acceleration of Diffusion Model.}
Diffusion models (DMs), such as Stable Diffusion~\cite{rombach2022high,podellsdxl}, inherently suffer from slow generation speeds due to iterative sampling. 
In this context, numerous acceleration techniques~\cite{salimans2022progressive,liu2023instaflow,sauer2025adversarial,ren2024hyper,chadebec2024flash,yin2024one} have emerged in recent years, 
% such as trajectory-based progressive distillation~\cite{salimans2022progressive}, InstaFlow~\cite{}, and consistency distillation (CM)~\cite{song2023consistency,luo2023latent}.
% Among these, consistency distillation, as a representative of trajectory-based distillation, has achieved significant results in accelerating diffusion. 
with the most influential being the Consistency Model (CM) family~\cite{song2023consistency,luo2023latent}.
This family introduces the concept of consistency, mapping any two points on a trajectory to the same endpoint and supporting fast one-step generation. Subsequent works built on them introduce multi-step consistency~\cite{zheng2024trajectory,heek2024multistep,xie2024mlcm,wang2024phased}.
% and PCM(phase consistency model), which divide the trajectory into several segments, with each segment trained using a consistency objective. 
Their idea of segmentation trajectories reduces the learning difficulty and enhances the effect of consistency distillation. 
However, these methods focus on continuous diffusion models, which makes them inconsistent with the discrete diffusion process. 
More importantly, they do not integrate acceleration for both text and image generation.
% To improve the inherent slow generation speed of diffusion models, consistency models (CM)~\cite{song2023consistency} and latent consistency models (LCM)~\cite{luo2023latent} are introduced to map any two points on the trajectories to the same endpoint and support fast one-step generation.
% Based on CMs, the Multistep Consistency Model (MCM)~\cite{heek2024multistep} split the trajectories into segments, with each segment trained using a consistency target, which reduces the learning difficulty and enhances the effect of consistency distillation through segmentation. 
% However, they focus on continuous diffusion models that do not align with discrete diffusion processes. More importantly, they do not integrate acceleration for both text and image generation.
% Inspired by the consistency model \cite{sauer2023adversarialdiffusiondistillation}, LCM is based on ODE trajectories and obtains straighter DDIM trajectories through consistency distillation, thereby reducing the number of required sampling iterations. 
% Inspired by the consistency model \cite{sauer2023adversarialdiffusiondistillation}, LCM is based on ODE trajectories and obtains straighter DDIM trajectories through consistency distillation, thereby reducing the number of required sampling iterations. 
% However, LCM mainly focuses on continuous diffusion models, does not meet the needs of discrete diffusion processes, and does not integrate text and image acceleration \cite{luo2023latentconsistencymodelssynthesizing}.
% \noindent \textbf{Multistep Consistency Model (MCM)}
% The multistep consistency model (MCM) introduces a multistep consistency target based on the LCM principle and enhances the effect of consistency distillation through segmentation \cite{heek2024multistepconsistencymodels}. The main goal of MCM is to balance sample quality and inference speed by dividing the diffusion process into multiple predefined segments. Each segment is trained using a consistency target, which reduces the learning difficulty and improves the consistency of the model with the consistency target. The segmentation idea of MCM makes its accelerated sampling effect far better than LCM.
% \subsubsection{Consistency Distillation for LLM}

\noindent \textbf{Acceleration of LLMs.} 
The acceleration of LLMs~\cite{kwon2023efficient,dao2022flashattention,song2023powerinfer,cai2024medusa,dao2023flashattention} has been a popular research area, including decreasing the size of KVcache~\cite{liu2024kivi,hooper2024kvquant,lin2024qserve} and the parallel decoding methods~\cite{li2024eagle,li2024eagle1,fu2024break}. 
% StreamingLLM~\cite{xiao2023efficient} accelerates the inference speed by controlling the KVcache dynamically. 
For example, speculative decoding~\cite{leviathan2023fast} involves training a draft model to predict tokens while the LLM verifies them. 
The target LLM may generate multiple tokens in a single inference process, but training such a draft model proves to be challenging.
Given the success of consistency distillation on diffusion models, CLLM~\cite{kou2024cllms} extends this idea to the acceleration of LLMs. 
Based on Jacobi decoding~\cite{song2021accelerating}, CLLM collects Jacobi trajectories during the LLM generation process and then uses consistency distillation principles similar to CM to achieve acceleration. 
% Unlike methods such as LCM, CLLM accelerates convergence by training the difference between the logits output of random points and fixed points on the trajectory, while the former's training target is the consistency loss between two similar points.
% While LCM and MCM leverage consistency loss between two similar points, CLLM accelerates convergence by training the difference between the logits output of random points and fixed points on the trajectory.
It proves that the idea of consistency distillation can effectively accelerate language models, and serves as an inspiration for our work on applying similar strategies in multimodal scenarios.



\setcounter{figure}{1}
\begin{figure*}[ht] % 't' option places the figure at the top of the page
    \centering
    \includegraphics[width=\textwidth,keepaspectratio]{graphs2/trajectory_final.pdf}
    \caption{\textbf{Illustration of the sampling trajectories of text and image tokens in Show-o.} 
    As shown, they both display a denoising pattern. 
    In particular, the trajectory of text generation is yielded by Jacobi Decoding~\cite{santilli2023accelerating}. 
    The black line denotes the unified abstraction of the multimodal trajectory, and the red lines illustrate the objective of our Show-o Turbo---to map an arbitrary point on the sampling trajectory to the endpoint. 
    Note that we omit the trajectory segmentation strategy here for brevity.}
    \label{fig:trajectory_framework}
\end{figure*}
